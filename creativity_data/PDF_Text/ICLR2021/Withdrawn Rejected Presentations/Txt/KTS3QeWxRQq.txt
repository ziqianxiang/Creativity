Under review as a conference paper at ICLR 2021
Quantitative Understanding of VAE as a Non-
linearly Scaled Isometric Embedding
Anonymous authors
Paper under double-blind review
Ab stract
Variational autoencoder (VAE) estimates the posterior parameters (mean and vari-
ance) of latent variables corresponding to each input data. While it is used for
many tasks, the transparency of the model is still an underlying issue. This paper
provides a quantitative understanding of VAE property by interpreting VAE as a
non-linearly scaled isometric embedding. According to the Rate-distortion the-
ory, the optimal transform coding is achieved by using a PCA-like orthonormal
transform where the transform space is isometric to the input. From this analogy,
we show theoretically and experimentally that VAE can be mapped to an implicit
isometric embedding with a scale factor derived from the posterior parameter. As
a result, we can estimate the data probabilities in the input space from the prior,
loss metrics, and corresponding posterior parameters. In addition, the quantitative
importance of each latent variable can be evaluated like the eigenvalue of PCA.
1	Introduction
Variational autoencoder (VAE) (Kingma & Welling, 2014) is one of the most successful generative
models, estimating posterior parameters of latent variables for each input data. In VAE, the latent
representation is obtained by maximizing an evidence lower bound (ELBO). A number of studies
(Higgins et al., 2017; Kim & Mnih, 2018; Lopez et al., 2018; Chen et al., 2018; Locatello et al.,
2019; Alemi et al., 2018; Rolinek et al., 2019) have tried to reveal the property of latent variables.
However, quantitative behavior of VAE is still not well clarified. For example, there has not been
a theoretical formulation of the reconstruction loss and KL divergence in ELBO after optimization.
More specifically, although the conditional distribution pθ(x|z) in the reconstruction loss OfELBO
is predetermined such as the Gaussian or Bernoulli distributions, it has not been discussed well
whether the true conditional distribution after optimization matches the predetermined distribution.
Rate-distortion (RD) theory (Berger, 1971), which is an important part of Shannon information
theory and successfully applied to image compression, quantitatively formulates the RD trade-off
optimum in lossy compression. To realize a quantitative data analysis, Rate-distortion (RD) theory
based autoencoder, RaDOGAGA (Kato et al., 2020), has been proposed with isometric embedding
(Han & Hong, 2006) where the distance between arbitrary two points of input space in a given
metrics is always the same as L2 distance in the isometric embedding space. In this paper, by
mapping VAE latent space to an implicit isometric space like RaDOGAGA on variable-by-variable
basis and analysing VAE quantitatively as a well-examined lossy compression, we thoroughly clarify
the quantitative properties of VAE theoretically and experimentally as follows.
1)	Implicit isometric embedding is derived in the loss metric defined space such that the entropy of
data representation becomes minimum. A scaling factor between the VAE latent space and implicit
isometric space is formulated by the posterior for each input. In the case of β-VAE, the posterior
variance of each dimensional component in the implicit isometric embedding space is a constant
β∕2, which is analogous to the rate-distortion optimal of transform coding in RD theory. As a result,
the reconstruction loss and KL divergence in ELBO can be quantitatively formulated.
2)	From these properties, VAE can provide a practical quantitative analysis of input data. First,
the data probabilities in the input space can be estimated from the prior, loss metric, and poste-
rior parameters. In addition, the quantitative importance of each latent variable, analogous to the
eigenvalue of PCA, can be evaluated from the posterior variance of VAE.
This work will lead the information theoretic generative models in the right direction.
1
Under review as a conference paper at ICLR 2021
2	Related works
2.1	Variational autoencoder and theoretical analysis
In VAE, ELBO is maximized instead of maximizing the log-likelihood directly. Let x ∈ Rm be a
point in a dataset. The original VAE model consists of a latent variable with fixed prior Z 〜p(z)=
N(z; 0, In) ∈ Rn, a parametric encoder Encφ : X ⇒ z, and a parametric decoder Decθ : Z ⇒ X.
In the encoder, qφ(z∣x) = N(z;从⑺，σ⑻)is provided by estimating parameters 从⑺ and σ(χ).
Let Lx be a local cost at data X. Then, ELBO is described by
ELBO=Ex〜p(x)[Ez〜qφ(z∣χ)[logpθ(x|z)] - DκL(qφ(z∣x)∣∣p(z))].	⑴
In Ex〜p(x)[ ∙ ], the first term Ez〜qφ(z∣x)[ ∙ ] is called the reconstruction loss. The second term
Dkl(∙) is a Kullback-Leibler (KL) divergence. Let μj(x), σj(x), and DKLj(X) be j-th dimensional
values of μ(x), σ(x), and KL divergence. Then Dkl(∙) is derived as:
n1
Dkl(∙) = EDKLj(x), where DKLj(X) = 5 (μj(x)2 + σj(x)2 - log σj(x)2 - 1) .	(2)
j=1	2
D(x, X) denotes a metric such as sum square error (SSE) and binary cross-entropy (BCE) as log-
likelihoods of Gaussian and Bernoulli distributions, respectively. In training VAE, the next objective
is used instead of Eq. 1, where β is a parameter to control the trade-off (Higgins et al., 2017).
Lx = Ez〜qφ(z∣x)[D(x, X)]+ βDκL(∙).	(3)
However, it has not been fully discussed whether the true conditional distribution matches the pre-
determined distribution, or how the value of KL divergence is derived after training.
There have been several studies to analyse VAE theoretically. Alemi et al. (2018) introduced the
RD trade-off based on the information-theoretic framework to analyse β-VAE. However, they did
not clarify the quantitative property after optimization. Dai et al. (2018) showed that VAE restricted
as a linear transform can be considered as a robust PCA. However, their model has a limitation for
the analysis on each latent variable basis because of the linearity assumption. Rolinek et al. (2019)
showed that the Jacobian matrix of VAE at each latent variable is orthogonal, which makes latent
variables disentangled implicitly. However, they do not uncover the orthonormality and quantitative
properties because they simplify KL divergence as a constant. Dai & Wipf (2019) also showed that
the expected rate of VAE for the r-dimensional manifold is close to -(r/2) logγ + O(1) at γ → 0
when pθ (X|x) = N(X; x, YIm) holds. The remaining challenge is to clearly figure out what latent
space is obtained at a given dataset, a loss metric, and β in the model.
2.2	Rate-distortion theory, transform coding, and isometric embedding
RD theory (Berger, 1971) formulated the optimal transform coding (Goyal, 2001) for the Gaussian
source with square error metric as follows. Let X ∈ Rm be a point in a dataset. First, the data are
transformed deterministically with the orthonormal transform (orthogonal and unit norm) such as
KarhUnen-LOeVe transform (KLT) (Rao & Yip, 2000). Let Z ∈ Rm be a point transformed from
X. Then, z is entropy-coded by allowing equivalent stochastic distortion (or posterior with constant
variance) in each dimension. A lower bound of a rate R at a distortion D is denoted by R(D).
The derivation of R(D) is as follows. Let zj be the j-th dimensional component of z and σzj2 be
the variance of zj in a dataset. It is noted that σzj 2 is the equivalent to eigenvalues of PCA for the
dataset. Let d be a distortion equally allowed in each dimensional channel. At the optimal condition,
the distortion Dopt and rate Ropt on the curve R(D) is calculated as a function of d:
1m	m
RoPt = 2∑Smax(bg(σzj2/d), 0), DoPt = Emin(d, σ%j2).	(4)
2 j=1	j=1
The simplest way to allow equivalent distortion is to use a uniform quantization (Goyal, 2001). Let
T be a quantization step, and round(∙) be a round function. Quantized value Zj is derived as kT,
where k = round(zj /T). Then, d is approximated by T2/12 as explained in Appendix H.1.
To practically achieve the best RD trade-off in image compression, rate-distortion optimization
(RDO) has also been widely used (Sullivan & Wiegand, 1998). In RDO, the best trade-off is
2
Under review as a conference paper at ICLR 2021
Nonlinear scaling:
Implicit isometric space
SlSo(U 睁)，P WSiso
Dataset [ Input SPaCe ⅞put (U 及加)J L MetriCS D (冗,K
________________I	ImPIiCit isometric embedding
VAE anisometric orthogonal
space Svae (⊂ Rπ), z∈ Svae
P(¾) Fixed Prior P(Zn)
POSteriOr VarialICe
Figure 1: Mapping of VAE to implicit isometric embedding.
___________Prior VariallCe
Posterior with constant variance B/2
achieved by finding a encoding parameter that minimizes a cost L = D + λR at given Lagrange
parameter λ. Recently, deep image compression (Balle et al., 2018) has been proposed. In these
works, instead of an orthonormal transform with sum square error (SSE) metric in the conventional
lossy compression, a deep autoencoder is trained with flexible metrics, such as structural similarity
(SSIM) (Wang et al., 2001) for RDO. Recently, an isometric autoencoder, RaDOGAGA (Kato et al.,
2020) was proposed based on Balle et al. (2018). They proved that the latent space to be isometric to
the input space if the model is trained by RDO using a parametric prior and posterior with constant
variance. By contrast, VAE uses a fixed prior with a variable posterior. In section 3, we explain that
VAE can be quantitatively understood as the rate-distortion optimum as in Eq. 4 by mapping VAE
latent space to implicit isometric embedding on a variable-to-variable basis as in Fig. 1 .
3 Understanding of VAE as a scaled isometric embedding
This section shows the quantitative understanding of VAE. First, we present the hypothesis of map-
ping VAE latent space to an implicit isometric embedding space. Second, we reformulate the ob-
jective of β-VAE for easy analysis. Third, we prove the hypothesis from the minimum condition
of the objective. Then, we show that ELBO can be interpreted as an optimized RDO cost of trans-
form coding where the quantitative properties are well clarified, as well as discuss and correct some
prior theoretical studies. Lastly, we explain the quantitative properties of VAE to validate the theory
including approximations and provide a practical data analysis.
3.1	Hypothesis of mapping VAE to the implicit orthonormal transform
Figure 1 shows the mapping of VAE to the implicit isometric embedding. Assume the data manifold
is smooth and differentiable. Let SinpUt (⊂ Rm) be an input space of the dataset. D(x, X) denotes
a metric for points x, X ∈ SinpUt. Using the second order Taylor expansion, D(x, X + δx) can
be approximated by tδx Gxδx, where Gx and δx are an x dependent positive definite Hermitian
metric tensor and an arbitrary infinitesimal displacement in SinpUt , respectively. The derivations of
Gx for SSE, BCE, and SSIM are shown in Appendix H.2. Next, an implicit isometric embedding
space SIso(⊂ Rm) is introduced like the isometric latent space in RaDOGAGA (Kato et al., 2020),
such that the entropy of data representation is minimum in the inner product space of Gx. Let y and
yj be a point in SIso and its j-th component, respectively. Because of the isometricity, p(X) ' p(y)
will hold. We will also show the posterior variance of each dimensional component yj is a constant
β∕2. In addition, the variance of yj∙ will show the importance like PCA when the data manifold has
a disentangled feature by nature in the metric space of Gx and the prior covariance is diagonal.
Then, SIso is nonlinearly scaled to the VAE’s anisometric orthogonal space SVAE(⊂ Rn) on a
variable-by-variable basis. Let z be a point in SVAE , and zj denotes the j-th component of z . Let
p(yj) and p(zj) be the probability distribution of the j-th variable in SIso and SVAE. Each variable
yj is nonlinearly scaled to zj , such that dzj ∕dyj = p(yj )∕p(zj ) to fit the cumulative distribution.
dzj∙∕dyj∙ is σj∙(x)/,β∕2, the ratio of posterior,s standard deviations for Zj and yj∙, such that KL
divergences in both spaces are equivalent. In addition, dimensional components whose KL diver-
gences are zero can be discarded because such dimensions have no information.
3
Under review as a conference paper at ICLR 2021
3.2	Reformulation of objective to THE form using ∂x∕∂zj and ∂x∕∂Z
We reformulate the objective Lx to the form using ∂x∕∂zj and ∂x∕∂z. Here, the dimensions of X
and z, i.e., m and n, are set as the same. The condition to reduce n is shown in section 3.3.
Reformulation of D(x, X) loss: In accordance with Kato et al. (2020), the loss D(x, X) can be
decomposed into D(X, X) + D(x, X), where X denotes Decθ(μ(x)). The first term D(X, X) is a
distortion between the decoded values of μ(x) with and without noise σ(x). We call this term as
a coding loss. This term is expanded as follows. δX denotes X — X. Then, D(X, X) term can be
approximated by tδX GxδX. Let Xzj be ∂x∕∂zj at Zj = μj(x), and δzj 〜N(0, σ7-(x)) be an added
noise in Zj. Then, δX is approximated by δX ` Pm=I δzj Xzj∙. Because δzj and δzk for j = k are
uncorrelated, the average of D(X, X) over Z 〜qφ(z∣X) can be finally reformulated by
n
Ez~qφ(z∣x) [D(X, X)]' Ez~qφ(z∣x) [tδX GxδX] ` fθj(x) tXzjGχXzj.	(5)
j=1
The second term D(x, X) is a loss between the input data and Decθ(μ(x)). We call this term a
transform loss. We presume VAE is analogous to the Wiener filter (Wiener, 1964; Jin et al., 2003)
where the coding loss is regarded as an added noise. From the Wiener filter theory, the ratio between
the transform loss and coding loss is close to the ratio between the coding loss and the variance of the
input data. The coding loss, approximately nβ∕2 as in Eq. 14, should be smaller than the variance
of the input data to capture meaningful information. Thus the transform loss, usually small, is not
considered in the following discussion. Appendix B explains the detail in a simple 1-dimensional
VAE. We show the exhaustive and quantitative evaluation of coding loss and transform loss in the
toy dataset in appendix E.2 to validate this approximation.
Reformulation of KL divergence: When σj(x)	1, σj(x)2	- logσj(x)2 is observed. For
example, when σj(x)2 < 0.1, we have -(σj(x)2∕ logσj(x)2) < 0.05. in such dimensions, DKLj(x)
can be approximated as Eq. 6 by ignoring the σj(x)2 term and setting p(μj∙(x)) to N(Zj; 0,1):
DKLj(X) ' 2 (μj(x) - log σj(x) - 1) = - log (σj(x) p(μj(x))) -	2	.	(6)
Eq. 6 can be considered as a rate of entropy coding for a symbol with mean μj∙(x) allowing quantiza-
tion noise σj(x)2, as shown in Appendix H.3. Thus, in the dimension with meaningful information,
σj(x)2 is much smaller than the prior variance 1, and the approximation in Eq.6 is reasonable. Let
p(μ(x)) be Qn=ι p(μj∙(x)). p(μ(x)) = p(χ) ∣det(∂x∕∂z)∣ holds where det(∂x∕∂z) is a Jacobian
determinant at Z = μ(x). Let CDKL be a constant 2 log 2πe. Then, DKL(∙) is reformulated by
n	∂X n
Dkl (∙) ' — log (p(μ(x)) Y σj∙(x)) — CDKL ' Tog (P(X) det (^) Y 0j(x)) — Cdkl∙⑺
j=1	Z	j=1
Final objective form: From Eqs. 5 and 7, the objective L0x to minimise is derived as:
n
L0x = X σj(x)2 tXzjGxXzj — βlog p(X)
j=1
(8)
3.3	Proof of the hypothesis
Mapping VAE to implicit isometric embedding: The minimum condition of L0x at X is examined.
Let Xzj be the j-th column vector of a cofactor matrix for Jacobian matrix ∂x∕∂z. Note that
dlog ∣det(∂x∕∂z)∣∕dxzj∙ = Xzj∙∕det(∂x∕∂z) holds as is also used in Kato etal. (2020). Using this
equation, the derivative of L0x by Xzj is described by
Tj^^~ = 2σj(x)2GxXzj - N 十(：/内、Xzj.	(9)
dXzj	j det (∂X∕∂Z)	j
Note thattXzk ∙ Xzj = det(∂x∕∂z) δjd holds by the cofactor,s property. Here, ∙ denotes the dot
product, and δjk denotes the Kronecker delta. By setting Eq. 9 to zero and multiplying tXzk from
the left, the condition to minimize L0x is derived by the next orthogonal form of Xzj :
(2σj(x)2∕β) tXzkGxXzj = δjk.	(10)
4
Under review as a conference paper at ICLR 2021
Here, the diagonal posterior covariance is the key for orthogonality. Next, implicit latent variable
y and its j-th dimensional component yj are introduced. Set yj to zero at zj = 0. The derivative
between y7- and Zj at μj(χ) is defined by
dZjlzj =μj(x) = 2σj σj(x)T	(11)
Xyj denotes ∂x∕∂yj. By applying Xzj = dyj/dzj Xyjto Eq. 10, Xyj shows the isometric property
(Han & Hong, 2006; Kato et al., 2020) in the inner product space with a metric tensor Gx as follows:
tXyjGxXyk = δjk.	(12)
Minimum entropy of implicit isometric representation: Let L0min x be a minimum of L0x at X.
Dmin x and Rminx denote a coding loss and KL divergence in L0min x, respectively. By applying
Eqs. 10-11 and p(zj) = (dyj /dzj) p(yj) to Eqs. 5 and 7, the following equations are derived:
Lmin X = Dmin X + βRmin X , Where Dmin X = ~^~ , Rmin X = - log p(y) -	^	. (13)
Here, Dmin X is derived as P"(β∕2 tXyjGXXyj = nβ∕2, implying each dimensional poste-
rior variance of the implicit isometric variable is a constant β∕2. In addition, exp(-Lmmin ⑦//)=
p(y) exp(Const.) H p(y) ` P(X) will hold in the inner product space of GX from the isometricity.
By averaging Lmin X over X 〜 P(X) and approximating this average by the integration over y 〜
p(y), the global minimum L0G is derived as:
LG = DG + BRg, where DG = nβ, RG = min (— Pp(y) logp(y)dy) - nlog(ene). (14)
2	p(y)	2
The term - P(y) log P(y)dy in RG is the entropy ofy. Thus, the optimal implicit isometric space
is derived such that the entropy of data representation is minimum in the inner product space of GX.
When the data manifold has a disentangled property in the given metric, each yj will capture a
disentangled feature with minimum entropy, as shown in Kato et al. (2020). This is analogous to
PCA for Gaussian data, which gives the disentangled representation with minimum entropy in SSE.
Considering the similarity to the PCA eigenvalues, the variance ofyj will indicate the importance of
each dimension. In the dimensions where the variance of yj is less than β∕2,。外⑦)=1,小心)=0,
and DKLj(x) = 0 will hold. In addition, σj(X)2 tXzjGXXzj will be close to 0 because this needs not
to be balanced with DKLj(x). This is similar to the case in the RD theory in Eq. 4 whereσzj2 is less
than d, meaning no information. As a result, Eqs. 10-14 will not hold here. Thus, latent variables
with variances from the largest to the n-th with DKLj(x) > 0 are sufficient for the representation and
the dimensions with DKLj(x) = 0 can be ignored, allowing the reduction of the dimension n for z.
Some approximations may be slightly violated, however, our analysis still helps to understand VAE.
3.4	Discussion and relationship with prior theoretical studies
First, we show β-VAE optimum as in Eq. 14 can be interpreted as the rate-distortion optimum
(Eq. 4) in RD theory when the uniform distortion d in Eq. 4 is set to β∕2 in the metric defined space.
H(X) = - Jp(x) logp(x) dX denotes a differential entropy for a set X ∈ X; X 〜p(x). For the
1-dimensional Gaussian data X 〜 N(x, 0, σ2), H(X) = 2 log(2πeσ2) holds. Thus, Ropt in Eq. 4
is derived as a difference of the differential entropy between transformed data Z 〜Qj N(Zj; 0, σzj∙)
and uniform distortion D 〜 N(D; 0, dim). RG is also derived as a difference of the differential
entropy between transformed data y 〜 p(y) and uniform distortion D 〜 N(D;0, (β∕2)Im,).
Furthermore, DG in Eq. 14 can be interpreted as Dopt in Eq. 4 by setting d = β∕2. As a result, the
VAE optimal corresponds to the rate-distortion optimal of transform coding in RD theory, and β∕2
is regarded as a variance of the constant distortion equally added to each dimensional component.
Because of the isometricity, the power of distortion (i.e., posterior variance) in the implicit isometric
space is the same as that in the metric defined input space. Thus the conditional distribution after
optimization in the metric defined space is derived as pθ (x|z) = pθ(x∣X) ` N(x; X, (β∕2)I). This
is consistent with the fact that the quality of the reconstructed data becomes worse in larger β.
5
Under review as a conference paper at ICLR 2021
Next, We estimate the reconstruction loss Eqφ(z∣χ)[logpθ(x|z)] and KL divergence Dkl(∙) in β-
VAE and also correct the analysis in Alemi et al. (2018). Let H = -Ep(x) [log p(x)] be a differential
entropy of input data. When β = 1, Alemi et al. (2018) suggest ”the ELBO objective alone (and
the marginal likelihood) cannot distinguish betWeen models that make no use of the latent variable
(autodecoders) versus models that make large use of the latent variable and learn useful representa-
tions for reconstruction (autoencoders),” because the reconstruction loss and KL divergence can be
arbitrary value on the line -Eqφ(z∣χ)[log pθ (x|z)]+ Dkl(∙) = H. Correctly, the reconstruction loss
and KL divergence after optimization are deterministically estimated at any β (including β = 1) as:
Eqφ(z∣x)[logPθ(x|z)] ' -(n∕2)log(β∏e), Dkl(∙) ' Togp(y) - (n/2) log(β∏e).	(15)
The proof is explained in Appendix A.1. Thus ELBO can be estimated as:
ELBO=Ep(χ)[Ez〜qφ(z∣χ)[logpθ(x|z)] - Dkl(∙)] ' Ep(X)[logp(y)] ` Ep(x)[logp(x)]. (16)
As a result, When the objective of β-VAE is optimised, ELBO (Eq. 1) in the original form (Kingma
& Welling, 2014) is approximately equal to the log-likelihood of x, regardless β = 1 or not.
Finally, the predetermined conditional distribution PRp(x∣X) and the true conditional distribution
after optimization prθ(x|x) are examined using β in the input Euclidean space of x. Assume
PRp(x∣X) = N(x; X,σ2I). In this case, the metric D(x, X) is derived as - logPRp(x∣X) =
(l∕2σ2)∣x - X∣2 + Const. From Eq. 13, the following equations are derived:
Eqφ(X∣x) [D(X, X)] = Eqφ(X∣x) h2σ2 |x - X|2] = Eqφ(X∣x) [2σ2 X(Xi- xi)[ ' nβ∕2,	(17)
i
Eqφ(X∣χ) [(xi - Xi)2] ' βσ2.	(18)
Because the variance of each dimension is estimated as βσ2, the true conditional distribution after
optimization is approximated as prθ(x|X) ` N(x; X,βσ21). If β = 1, i.e., the original VAE,
PRp(x∣X) and prθ(x|X) are equivalent as expected. If β = 1, however, pRp(x∣X) and prθ(x|X) are
different. Actually, what β-VAE does is only to scale the variance of the pre-determined conditional
distribution in the original VAE by a factor of β, because β-VAE objective can be rewritten as:
Eqφ(∙)[logN(x； X,σ2I)] - βDκL(∙) = β (Eqφ(∙)[logN(x; X,βσ2I)] - Dkl(∙)) + const. (19)
More detailed discussions about prior works (Higgins et al. (2017); Alemi et al. (2018); Dai et al.
(2018); Dai & Wipf (2019); Tishby et al. (1999); Goyal (2001)) are explained in Appendix A.
3.5	Quantitative properties to validate the theory
This section shows three quantitative properties in VAE with a prior N (z; 0, In), to validate the the-
ory in section 3.3. The second and third properties also provide practical data analysis approaches.
The derivation of equations in the second and third properties are explained in appendix C.
j-th
Norm of Xyj equal to 1: Let e(j) be a vector (0, ∙∙∙ , 1, •…，0) where the j-th dimension is 1, and
others are 0. Let Dj0 (z) be D(Decθ(z), Decθ(z + e(j)))∕2, where denotes a minute value for
the numerical differential. From Eq. 10, the squared norm of Xyj can be numerically evaluated as
the first term of Eq. 20. This value will be equal to 1 at any X and dimension j except DκLj (x) = 0.
22
HJj(X) Dj(Z) ' β (σj(x)	XzjGxxzj) ' XyjGxxyj = 1.
(20)
If observed, the existence of an implicit isometric embedding can be shown because of unit norm
and orthogonality (Rollnek et al., 2019). Eq. 20 also show σj-(χ)2 txz.GxXzj ` 22, implying that a
noise σj(X) added to each dimension of latent variable causes an equal noise β∕2 in the input space.
PCA-like feature: When the data manifold has a disentangled property in the given metric, the
variance of the j -th implicit latent component yj can be roughly estimated as
y y 2p(yj)dyj ` β E、⑸⑻-21.	(21)
2 X 〜p(x)
The average E[σj(X)-2] on the right allows evaluating the quantitative importance of each dimension
in practice, like the eigenvalue of PCA. Note that a dimension whose average is close to 1 implies
DκLj (x) = 0. Such a dimension has no information and is an exceptions of the property in Eq. 20.
6
Under review as a conference paper at ICLR 2021
—MS1) ： E[51]=O,E[512]=1∕6
P(S2): E[ly2]=0, E[522]=2∕3
一p(s3) : E[53]=0,E[532]=8∕3
-4√3∕3 -√2 -1 O 1 √2	8√3∕3
Figure 2: PDFs of three variables to generate a toy dataset.
Estimation of the data probability distribution: First, assume the case m = n. Since the y space
is isometric to the inner product space of Gx , the PDFs in both spaces are the same. The Jacobian
determinant between the input space and inner product space, giving the the ratio of PDFs, is derived
as |Gx 12. We set p(μ(x)) to the prior. Thus, the data probability in the input space can be estimated
by ∣Gx |1 and either the prior/posterior or Lx after training, as the following last two equations:
1	1	m	11
P(X) ` IGx|2p(y) Y IGx|2 P(M(X)) Y σj(x) Y IGx|2 eχp(-βLx).	(22)
j=1
In the case m > n, the derivation of the PDF ratio between the input space and the inner product
space is generally intractable, except for Gx = axIm , where ax is an x-dependent scalar factor. In
this case, the PDF ratio is given by axn/2 . Thus, P(x) can be estimated as follows:
n
P(X) Y axn p(μ(x)) Y σj(x) Y ax 2 exp (一至乙宏)	(23)
j=1
Equations 22 and 23 enable a probability-based quantitative data analysis/sampling in practice.
4	Experiment
We show the experiments of the quantitative properties presented in Section 3.5. First, the results of
the toy dataset are presented. Then, the results of CelebA are shown as a real data example.
4.1	Evaluation of quantitative properties in the toy dataset
The toy dataset is generated as follows. First, three dimensional variables s1, s2, and s3 are sampled
in accordance with the three different shapes of distributions P(s1), P(s2), and P(s3), as shown in
Fig. 2. The variances of s1, s2, and s3 are 1/6, 2/3, and 8/3, respectively, such that the ratio of the
variances is 1:4:16. Second, three 16-dimensional uncorrelated vectors v1, v2, and v3 with L2 norm
1, are provided. Finally, 50, 000 toy data with 16 dimensions are generated by X = Pi3=1 sivi . The
data generation probability P(X) is also set to P(s1)P(s2)P(s3). If our hypothesis is correct, P(yj)
will be close toP(sj). Then, σj(x) Y dzj /dyj = P(yj)/P(zj) will also vary a lot with these varieties
of PDFs. Because the properties presented in Section 3.5 are calculated from σj(x), our theory can
be easily validated by evaluating those properties.
Then, the VAE model is trained using Eq. 1. We use two kinds of the reconstruction loss D(∙, ∙) to
analyze the effect of the loss metrics. The first is the square error loss equivalent to sum square error
(SSE). The second is the downward-convex loss which we design as Eq. 24, such that the shape
becomes similar to the BCE loss as in Appendix H.2:
D(x, X) = ax∣∣x — X∣∣2, where ax = (2/3 + 2 ∣∣x∣∣2/21) and Gx = axIm.	(24)
Here, ax is chosen such that the mean of ax for the toy dataset is 1.0 since the variance of X is
1/6+2/3+8/3=7/2. The details of the networks and training conditions are written in Appendix D.1.
Then the network is trained with two types of reconstruction losses. The ratio of transform loss
to coding loss for the square error loss is 0.023, and that for the downward-convex loss is 0.024.
As expected in section 3.2, the transform losses are negligibly small. Tables 1 and 2 show the
measurements of 2σj∙(x)2Dj(z) (shown as 2σj∙2Dj), Dj(z), and σj∙(x)-2 described in Section
3.5. In these tables, z1, z2, and z3 show acquired latent variables. ”Av.” and ”SD” are the average
and standard deviation, respectively. To begin with, the norm of the implicit orthonormal basis
7
Under review as a conference paper at ICLR 2021
Table 1: Property measurements of the toy
dataset trained with the square error loss.
variable	Zι	z2	z3
2 σj2 Dj	Av.	0.965	0.925	0.972
SD	0.054	0.164	0.098
Dj0 (z)	Av.	0.162	0.726	2.922
SD	0.040	0.466	1.738
σj(X) -2 Av.	3.33e1	1.46e2	5.89e2
(Ratio) Av.	1.000	4.39	17.69
Table 2: Property measurements of the toy
dataset trained with the downward-convex loss.
variable	Zι	z2	z3
2 σj2 Dj	Av.	0.964	0.928	0.978
SD	0.060	0.160	0.088
Dj (Z)	Av.	0.161	0.696	2.695
SD	0.063	0.483	1.573
σj (x)-2^^Av^	3.30e1	1.40e2	5.43e2
(Ratio) Av.	1.000	4.25	16.22
•qo」d PΘI⅛eE耳S山
6 4 2
•qo」d PSeE耳S山
×ιo--
•qo」d PSeE耳S山
4 2
•qo」d PSeE耳S山
⑶ p(μ(χ))	⑹ eχp(-Lx/β)	⑹ aX/2p(μ(χ)) Qjq㈤(d) aX/2 exp(-Lχ/β)
Figure 3: Scattering plots of the data generation probability (x-axis) versus four estimated prob-
abilities (y-axes) for the downward-convex loss. y-axes are (a) p(μ(x)), (b) exp(-Lx/β), (c)
aX/2p(μ(x)) Qj σj(χ), and (d) aX/2 exp(-Lχ/β).
is discussed. In both tables, the values of β2σ(x)j∙2Dj (Z) are close to 1.0 in each dimension as
described in Eq. 23. By contrast, the average of Dj0 (z), which corresponds to txzjGxxzk, is
different in each dimension. Therefore, the derivative of x with zj , the original latent variable of
VAE, is not normalized.
Next, the PCA-like feature is examined. The average of σj(X) -2 in Eq.21 and its ratio are shown
in Tables 1 and 2. Although the average of σj(X)-2 is a rough estimation of variance, the ratio is
close to 1:4:16, i.e., the variance ratio of generation parameters s1, s2, and s3. When comparing
both losses, the ratio of s2 and s3 for the downward-convex loss is somewhat smaller than that for
the square error. This is explained as follows. In the downward-convex loss, |xyj |2 tends to be 1/aX
from Eq. 12, i.e. txyj (axIm) xyk = δjk. Therefore, the region in the inner product space with a
larger norm is shrunk, and the estimated variances corresponding to s2 and s3 become smaller.
Figure 3 shows the scattering plots of the data generation probability p(x) and estimated probabil-
ities for the downward-convex loss. The plots for the square error loss are shown in Appendix E.
Figure 3a shows the plots of p(x) and the prior probabilities p(μ(x)). This graph implies that it is
difficult to estimate p(x) only from the prior. The correlation coefficient shown as ”R” (0.434) is
also low. Figure 3b shows the plots of p(x) and exp(-Lx/β), i.e., the lower bound of likelihood.
The correlation coefficient (0.771) becomes better, but is still not high. Next, Figures 3c and 3d
show the plots of aX/2p(μ(x)) Qj σj(x) and aX/2 exp(-Lx/β) in Eq. 23. These graphs, showing
a high correlation coefficients around 0.91, support that the objective LX in Eq. 3 is optimized in
the inner product space of Gx. In the case of the square error loss, the plots with exp(-Lx/β) also
shows a high correlation coefficient 0.904 because aX is 1, allowing the probability estimation from
LX in Eq. 3. The ablation study with different PDF, losses, and β is shown in Appendix E.
4.2	Evaluations in CelebA dataset
This section evaluates the first and second quantitative properties of VAE trained with the CelebA
dataset 1 (Liu et al., 2015) as an example of real data. This dataset is composed of 202,599 celebrity
images. In use, the images are center-cropped to form 64 × 64 sized images.
1(http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
8
Under review as a conference paper at ICLR 2021
• Estimated Variance
▲ Norm: Average
Norm: Standard Deviation
SD: 0.13
0	10	20	30
Number Oflatentvariables
(descending order of Estimated Variance)
Figure 4: Graph of σj(x) -2
average and 2 σ7∙㈤2Dj (Z) in
VAE for CelebA dataset.
Figure 5: Graph of σj(x) -2
average and ∙∣ σj∙(x)2Dj (z) in
VAE for CelebA dataset with
explicit decomposed loss.
1 一 2 2
6.・4.3.
7 - 1 1
6 7
3 - 1 1
Z - Z Z
一 s≡≡i
=αu!s
.0
H>wfil营 $ H>
φ 9 H> 营 9 φ-
$ φ- φ Xw- φ φ-
国专.&f $_
Figure 6: Dependency of
decoded image changes with
zj = -2 to 2 on the average
of σj(x) .
We use SSIM, which is popular in image compression, as a reconstruction loss. The details of
networks and training conditions are written in Appendix D.2.
Figure 4 shows the averages of σj(x) -2 in Eq.21 as the estimated variances, as well as the average
and the standard deviation of 2 σj∙(x)2Dj (z) in Eq.20 as the estimated square norm of implicit
transform. The latent variables zi are numbered in descending order by the estimated variance. In the
dimensions greater than the 27th, the averages of σj∙(x)-2 are close to 1 and that of 2σj∙(x)2Dj (z)
is close to 0, implying DKL(∙) = 0. Between the 1st and 26th dimensions, the mean and standard
deviation of Ieσj∙(x)2Dj (z) averages are 1.83 and 0.13, respectively. These values seem almost
constant with a small standard deviation; however, the mean is somewhat larger than the expected
value 1. This result implies that the implicit transform can be considered as almost orthonormal by
dividing √1.83 ` 1.35. Thus, the average of σj(x)-2 still can determine the quantitative importance
of each latent variable. This also mean that the added noise to each yj∙ is around 1.83(β∕2). We also
train VAE by the decomposed loss explicitly, where Lx is set to D(x, X) + D(X, X) + βDKL(∙).
Figure 5 shows the result. Here, the mean and standard deviation of 2 σj∙(x)2Dj (z) averages are
0.92 and 0.04, respectively, which suggests almost a unit norm. As a result, the explicit use of
decomposed loss matches the theory better, allowing better analysis. The slight violation of the
norm in the conventional form needs a more exact analysis as a future study.
Figure 6 shows decoder outputs where the selected latent variables are traversed from -2 to 2 while
setting the rest to 0. The average of σj(x) -2 is also shown there. The components are grouped by
the average ofσj(x)-2, such that z1, z2, z3 to the large, z16, z17 to the medium, and z32 to the small,
accordingly. In the large group, significant changes of background brightness, the direction of the
face, and hair color are observed. In the medium group, we can see minor changes such as facial
expressions. However, in the small group, there are almost no changes. This result strongly supports
that the average of σj(x)-2 shows the importance of each latent variable. The traversed outputs for
all the component and results with another conditions are shown in Appendix F.
5 Conclusion
This paper provides a quantitative understanding of VAE by non-linear mapping to an isometric em-
bedding. According to the Rate-distortion theory, the optimal transform coding is achieved by using
PCA/KLT orthonormal transform, where the transform space is isometric to the input. From this
analogy, we show theoretically and experimentally that VAE can be mapped to an implicit isometric
embedding with a scale factor derived from the posterior parameter. Based on this property, we also
clarify that VAE can provide a practical quantitative analysis of input data such as the probability
estimation in the input space and the PCA-like quantitative multivariate analysis. We believe the
quantitative properties thoroughly uncovered in this paper will be a milestone to further advance the
information theory-based generative models such as VAE in the right direction.
9
Under review as a conference paper at ICLR 2021
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fix-
ing a broken ELBO. In Proceedings of the 35th International Conference on Machine Learn-
ing(ICML), pp. 159-168. PMLR, July 2018.
Johannes Balle, Laparra Valero, and Simoncelli Eero P. Density modeling of images using a general-
ized normalization transformation. In Proceedings of the 4t International Conference on Learning
Representations (ICLR), May 2016.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. In Proceedings of the 6th International Conference
on Learning Representations (ICLR), April 2018.
Toby Berger (ed.). Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice
Hall, 1971. ISBN 0137531036.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2610-2620, 2018.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. In International Conference on
Learning Representations (ICLR), May 2019.
Bin Dai, Yu Wang, John Aston, Gang Hua, and David Wipf. Hidden talents of the variational
autoencoder. The Journal of Machine Learning Research, 19:1573-1614, January 2018.
Vivek K Goyal. Theoretical foundations of transform coding. IEEE Signal Processing Magazine,
18:9-21, September 2001.
Qing Han and Jia-Xing Hong. Isometric Embedding of Riemannian Manifolds in Euclidean Spaces.
American Mathematical Society, 2006. ISBN 0821840711.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a
constrained variational framework. In Proceedings of the 5th International Conference on Learn-
ing Representations (ICLR), April 2017.
Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression
rates of deep generative models. In Proceedings of the 37th International Conference on Machine
Learning (ICML), July 2020.
F. Jin, P. Fieguth, L. Winger, and E. Jernigan. Adaptive wiener filtering of noisy images and image
sequences. In IEEE International Conference on Image Processing, Sept 2003.
Keizo Kato, Zhing Zhou, Tomotake Sasaki, and Akira Nakagawa. Rate-distortion optimization
guided autoencoder for generative analysis. In Proceedings of the 37th International Conference
on Machine Learning (ICML), July 2020.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Proceedings of the 35th Interna-
tional Conference on Machine Learning (ICML), pp. 2649-2658, Jul 2018.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2nd
International Conference on Learning Representations (ICLR), Banff, Canada, April 2014.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In Proceedings of the 36th International Conference on Machine
Learning (ICML), volume 97, pp. 4114-4124. PMLR, June 2019.
10
Under review as a conference paper at ICLR 2021
Romain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on auto-
encoding variational bayes. In Advances in Neural Information Processing Systems, pp. 6114-
6125, 2018.
William A. Pearlman and Amir Said. Digital Signal Compression: Principles and Practice. Cam-
bridge University Press, 2011. ISBN 0521899826.
Kamisetty Ramamohan Rao and Pat Yip (eds.). The Transform and Data Compression Handbook.
CRC Press, Inc., Boca Raton, FL, USA, 2000. ISBN 0849336929.
Michal Rollnek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca direc-
tions (by accident). In Proceedings of Computer Vision and Pattern Recognition (CVPR), June
2019.
Gary J. Sullivan and Thomas Wiegand. Rate-distortion optimization for video compression. IEEE
Signal Processing Magazine, 15:74-90, November 1998.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
The 37th annual Allerton Conference on Communication, Control, and Computing, September
1999.
Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simoncelli. Image quality
assessment: from error visibility to structural similarity. IEEE Trans. on Image Processing, 13:
600-612, April 2001.
Norbert Wiener. Extrapolation, Interpolation, and Smoothing of Stationary Time Series. The MIT
Press, 1964. ISBN 978-0-262-73005-1.
11
Under review as a conference paper at ICLR 2021
A Detailed relation to prior works
Firstly, we clarify the the difference between ELBO in Eq. 1 and the objective Lx in Eq. 3 in the
right direction. Then, we discuss the relation to the prior works. We also point out the incorrectness
of some works.
A. 1 Derivation of ELBO with clear and quantitative form
We derive the reconstruction loss and KL divergence terms in ELBO (without β) at x in Eq. 1 when
the objective of β-VAE L0x in Eq. 13 is optimised. The reconstruction loss can be rewritten as
Ez〜qφ(z∣x)[lθgPθ (X∣Z)]
/ qφ(z∣x)logPθ(x∣z)dz
/ qφ(y∣χ)iog pθ (χ∣y)dy.
(25)
Let μy(χ) be a implicit isometric variable corresponding to 仙⑺.Because the posterior variance in
each isometric latent variable is a constant β∕2, qφ(y∣x) ` N(y; μy(χ),(β∕2)In) Will hold. If β∕2
is small, p(X) ` p(x) will hold. Then, the next equation will hold also using isometricity;
pθ(x|z) = pθ(x|y) = pθ(χ∣X) = p(X∣x)p(x)∕p(X) ` p(X∣χ) ` qφ(y∣χ).	(26)
Thus the reconstruction loss is estimated as:
Ez 〜qφ(z∣x)[lθg Pθ(x∣z)]
〜/ N(y; μy(x), (β∕2)In) log N (y; “y⑻, (β∕2)Ln) dy
= -(n∕2) log(βπe).
(27)
From Eq. 13, KL divergence is derived as:
Dkl(∙) = Rminx = — logp(y) - (n∕2)log(β∏e).	(28)
By summing both terms, ELBO at x can be estimated as
ELBO = Ex〜p(x)[Ez〜qφ(z∣x)[logPθ(x|z)] - Dkl(∙)]
` Ex〜p(x)[iogp(y)]
'Ex 〜p(x)[log p(x)].	(29)
As a result, ELBO (Eq. 1) in the original form (Kingma & Welling, 2014) is close to the log-
likelihood of x, regardless β = 1 or not, when the objective of β-VAE (Higgins et al., 2017) is
optimised.
Some of the prior VAE works do not explicitly distinguish between the reconstruction loss
Ex〜p(x∣x)[p(x∣X)] in ELBO and the distortion D(x, X) in the objective Lx by mistake, which
leads to some incorrect discussion.
In addition, there have also been incorrect discussions in some prior works. In ELBO derivation,
they use Ex 〜p(x∣x) [p(χ∣X)] as a reconstruction loss, without discussing what kinds of properties the
distortion probability p(X∣x) should be. In training VAE with a real dataset, by contrast, they use
a predetermined distortion metric D(x, X) like BCE and SSE as a reconstruction loss instead of a
log-likelihood of the distortion probability, without discussing what distortion probability should be
after optimization.
Correctly, the distortion probability p(X∣x) after training is determined by β and the metric as
Pθ (X|x) 'N (X; x, (β∕2)Im) in the metric defined space. Then, by applying pθ (X|x) ` pθ (X|x)
to Eq. 1, the value of ELBO will become log p(y) ' log p(X) regardless β = 1 or not.
If D(x, x + δx) = t δxGxδx + O(∣∣δx∣∣3) is notSSE, by introducing a variable X = Lx TX where
Lx satisfies tLx Lx = Gx, the metric D(∙, ∙) can be replaced by SSE in the Euclidean space of X.
A.2 Relation to Tishby et al. (1999)
The theory described in Tishby et al. (1999) is consistent with our analysis. Tishby et al. (1999) clar-
ified the behaviour of the compressed representation when the rate-distortion trade-off is optimized.
12
Under review as a conference paper at ICLR 2021
X ∈ X denotes the signal space with a fixed probability p(x) and X ∈ X denotes its compressed
representation. Let D(x, X) be a loss metric. Then the rate-distortion trade-off can be described as:
L = I(X; X)+ β0 E	[D(x, X)].
P(X,X)
By solving this condition, they derive the following equation:
p(X∣x) H exp(-β0D(x, X)).
(30)
(31)
As shown in our discussion above, p(X∣x) ` N(X; x, (β∕2)Im) will hold in the metric defined
space from our VAE analysis. This result is equivalent to Eq. 31in their work if D(x, X) is SSE and
β0 is set to β-1, as follows:
P(X|x) h exp(-β0D(X, X))
exp
(—||X - X||2 ∖
I	2(β∕2))
(XN(X; X, (β∕2)Im).
(32)
If D(x, X) is not SSE, the use of the space transformation explained in appendix A.1 will lead to
the same result.
A.3 RELATION TO β-VAE (HIGGINS ET AL., 2017)
In the β-VAE work by Higgins et al. (2017), it is presumed that the objective LX was not mistakenly
distinguished from ELBO. In their work, ELBO equation is modified as:
Ep(X) [ EX〜pφ(X∣x) [qθ (X | x)] - βDKL (∙) ].	(33)
However, they use the predetermined probabilities of pθ(X|x) such as the Bernoulli and Gaussian
distributions in training (described in table 1 in Higgins et al. (2017)). As shown in our appendix
H.2, the log-likelihoods of the Bernoulli and Gaussian distributions can be regarded as BCE and
SSE metrics, respectively. As a result, the actual objective for training in Higgins et al. (2017) is not
Eq. 33, but the objective LX in Eq. 3 using BCE and SSE metrics with varying β. Thus ELBO as
Eq. 1 form will become log p(X) in the BCE / SSE metric defined space regardless β = 1 or not, as
shown in appendix A.1.
Actually, the equation 33 dose not show the log-likelihood of x. When DKL(∙) ‘ 一 logP(X)-
(n∕2) log(βπe) and EX〜p(χ∣χ)[p(x∣X)] ` -(n∕2) log(βπe) are applied, the value of Eq. 33 is
derived as β log p(X) + (β - 1)(n∕2) log(βπe), which is different from the log-likelihood of X if
β 6= 1.
Correctly, what β-VAE really does is only to scale the variance of the pre-determined conditional
distribution in the original VAE by a factor of β. In the case the pre-determined conditional dis-
tribution is Gaussian N(x; X, σ2I), the objective of β can be can be rewritten as a linearly scaled
original VAE objective with a Gaussian N(x; X, βσ2I):
一 一. c . 一	「1	CIX- XI2
Eqφ(∙)[log N (x； x,σ2I)] - βDκL(∙) = Eqφ(∙) - 2log2πσ2---------而— —βDκL(∙)
= β (Eqφ(∙) -2log2πβσ2 - ∣X2-Cx∣2 - DKL(.))
+β log 2πβσ2 - 2 log 2πσ2
=β (Eqφ(∙)[logN(x； X,βσ2I)] - DKL(∙)) + const.
(34)
A.4 Relation to Alemi et al. (2018)
Alemi et al. (2018) discuss the rate-distortion trade-off by the theoretical entropy analysis. Their
work is also presumed that the objective LX was not mistakenly distinguished from ELBO, which
leads to the incorrect discussion. In their work, the differential entropy for the input H, distortion D,
and rate R are derived carefully. They suggest that VAE with β = 1 is sensitive (unstable) because
D and R can be arbitrary value on the line R = H - βD = H - D. Furthermore, they also suggest
13
Under review as a conference paper at ICLR 2021
that R ≥ H, D = 0 at β → 0 and R = 0, D ≥ H at β → ∞ will hold as shown the figure 1 of
their work.
In this appendix, we will show that β determines the value of R and D specifically. We also show
that R ' H - D will hold regardless β = 1 or not.
In their work, these values of H, D, and D are mathematically defined as:
H	≡ -	dx p* (x) log p* (x),	(35)
D	≡ -Z dxp*(x)Z dze(z|x)logd(x|z),	(36)
R	≡	j dx p*(x) j dz e(z∣x) log e,zx，. m(z)	(37)
Here, p*(x) is a true PDF of x, e(z∣x) is a stochastic encoder, e(z∣x) is a decoder, and m(z) is a
marginal probability of z.
Our work allows a rough estimation of Eqs. 35-37 with β by introducing the implicit isometric
variable y as explained in our work.
Using isometric variable y and the relation dz e(z|x) = dy e(y|x), Eq. 36 can be rewritten as:
D
-/dx p*(x) /
dy e(y|x) log d(x|y).
(38)
Let μy be the implicit isometric latent variable corresponding to the mean of encoder output 从⑻.
As discussed in section 3.3, e(y∣x) = N(y; μy, (β∕2)In) will hold. Because of isometricity, the
value of d(x∣y) will be also close to e(y∣x) = N(y; μy, (β∕2)In). Though d(x∣z) must depend
on e(z|x), this important point has not been discussed well in this work. By using the implicit
isometric variable, we can connect both theoretically. Thus, D can be estimated as:
D
'/ dx p*(x) / dy N(y; 〃y, (β∕2)In)logN(y; 〃y, (β∕2)In)
` / dx p*(x) (n log(β∏e)^
n
=2 log(β∏e)
(39)
Second, R is examined. m(y) is a marginal probability of y. Using the relation dz e(z|x) =
dy e(y∣x) and e(z∣x)∕m(z) = (e(y∣x)(dy∕dz))∕(m(y)(dy∕dz)) = e(y∣x)∕m(y), Eq. 37 can
be rewritten as:
R'
/ dx p*(x) /
e(y|x)
dy e(y∣x)log ---.
m(y)
(40)
Because of isometricity, e(y∣x) ` p(x∣x) ` N(x； x, (β∕2)Im) will approximately hold where X
denotes a decoder output. Thus m(y) can be approximated by:
m(y) ` J dx p*(x)e(y∣x) ` J dx p*(x) N(x; x, (β∕2)Im)
(41)
Here, ifβ∕2, i.e., added noise, is small enough compared to the variance of x, a normal distribution
function term in this equation will act like a delta function. Thus m(y) can be approximated as:
m(y) ` J dx p*(x) δ(x — x) ` p*(x).
(42)
In the similar way, the following approximation will also hold.
J dy e(y∣x) log m(y) ` J dy e(y∣x) logp*(x) ` J dx δ(x — x) log p*(x) ` logp*(x) (43)
14
Under review as a conference paper at ICLR 2021
By using these approximation and applying Eqs. 38-39, R in Eq. 37 can be approximated as:
R ` /dx p*(x) / dy e(y∣x) log
'-ZaXp*(x)logp*(x)
n
' H — 2 log(β∏e)
' H-D
e(y∣χ)
p*(x)
dxp*(x) /dy e(y∣x)loge(y∣x)
(44)
As discussed above, R and D can be specifically derived from β. In addition, Shannon lower bound
discussed in Alemi et al. (2018) can be roughly verified in the optimized VAE with clearer notations
using β.
From the discussion above, we presume Alemi et al. (2018) might wrongly treat D in their work.
They suggest that VAE with β = 1 is sensitive (unstable) because D and R can be arbitrary value on
the line R = H - βD = H - D; however, our work as well as Tishby et al. (1999) (appendix A.2)
and Dai & Wipf (2019)(appendix A.5) show that the differential entropy of the distortion and rate,
i.e., D and R, are specifically determined by β after optimization, and R = H - D will hold for
any β regardless β = 1 or not. Alemi et al. (2018) also suggest D should satisfy D ≥ 0 because
D is a distortion; however, we suggest D should be treated as a differential entropy and can be less
than 0 because x is once handled as a continuous signal with a stochastic process in Eqs. 35-37.
Here, D ' (n/2) log(βπe) can be -∞ if β → 0, as also shown in Dai & Wipf (2019). Thus, upper
bound of R at β → 0 is not H, but R = H - (-∞) = ∞, as shown in RD theory for a continuous
signal. Huang et al. (2020) show this property experimentally in their figures 4-8 such that R seems
to diverge if MSE is close to 0.
A.5 Relation to Dai et al. (2018) and Dai & Wipf (2019)
Our work is consistent with Dai et al. (2018) and Dai & Wipf (2019).
Dai et al. (2018) analyses VAE by assuming a linear model. As a result, the estimated posterior is
constant. If the distribution of the manifold is the Gaussian, our work and Dai et al. (2018) give a
similar result with constant posterior variances. For non-Gaussian data, however, the quantitative
analysis such as probability estimation is intractable using their linear model. Our work reveals
that the posterior variance gives a scaling factor between z in VAE and y in the isometric space
when VAE is ideally trained with rich parameters. This is validated by Figures 3c and 3d, where the
estimation of the posterior variance at each data point is a key.
Next, the relation to Dai & Wipf (2019) is discussed. They analyse a behavior of VAE when ideally
trained. For example, the theorem 5 in their work shows that D → (d/2) logγ + O(1) and R →
-(Y∕2)log Y+O(1) hold if Y → +0, where γ, d, and Y denote a variance of d(x∣z), data dimension,
and latent dimension, respectively. By setting Y = β∕2 and d = ^ = n, this theorem is consistent
with R and D derived in Eq. 39 and Eq. 44.
A.6 Relation to transform coding (Goyal, 2001)
We show the optimum condition of VAE shown in Eq. 14 can be mapped to the optimum condition
of transform coding (Goyal, 2001) as shown in Eq. 4. First, the derivation of Eq. 4 is explained
by solving the optimal distortion assignment to each dimension. In the transform coding for m
dimensional the Gaussian data, an input data x is transformed to z using an orthonormal transform
such as KLT/DCT. Then each dimensional component zj is encoded with allowing distortion dj .
Let D be a target distortion satisfying D = Pjm=1 dj . σz2j denotes a variance of each dimensional
component Zj for the input dataset. Then, a rate R can be derived as Pm=I 11 log(σ∣j/dj). By
introducing a Lagrange parameter λ and minimizing a rate-distortion optimization cost L = D+λR,
the optimum condition is derived as:
λopt = 2D/m, dj = D/m = λ°pt∕2.	(45)
This result is consistent with Eq. 14 by setting β = λopt = 2D/m. This implies that L0G in
Eq. 14 is a rate-distortion optimization (RDO) cost of transform coding when x is deterministically
transformed to y in the implicit isometric space and stochastically encoded with a distortion β∕2.
15
Under review as a conference paper at ICLR 2021
B Estimation of the coding loss and transform loss in
1-DIMENSIONAL LINEAR VAE
This appendix estimates the coding loss and transform loss in 1-dimensional linear β-VAE for the
Gaussian data, and also shows that the result is consistent with the Wiener filter. Let x be a one
dimensional data with the normal distribution:
X ∈ R, X 〜N(x; 0, σχ2)	(46)
Let z be a one dimensional latent variable. Following two linear encoder and decoder are provided
with constant parameters a, b, and σz to optimize:
Z = ax + σz E where E 〜N(;0,1),
X = bz.	(47)
First, KL divergence at x, DKLx is derived. Due to the above relationship, we have
p(z) = N(z; 0, (aσx)2).
Using Eq. 6, KL-divergence at x can be evaluated as:
DKLx = — log(σz p(z)) — 1log2πe = — log σχ + a-X——|.
Second, the reconstruction loss at x Dx is evaluated as:
Dx = E [(x — (b(ax + σz E)))2] = ((ab — 1)x)2 + b2σz2 .
e 〜N(a0,1)L'
(48)
(49)
(50)
Then, the loss objective Lx = Dx + βDκLx is averaged over X 〜N(x; 0, σx2), and the objective
L to minimize is derived as:
L = E	[Lx] = (ab — I)2σx2 + b2σz2 + β (— logσz +------^x----3).	(5I)
x〜N(x*,σχ2)	∖	2	2 J
Here, (ab — 1)2σx2 and b2σz2 in the last equation are corresponding to the transform loss DT and
coding loss DC, respectively.
By solving dL/da = 0, dL/db = 0, and dL∕dσz = 0, a , b, and σ% are derived as follows:
a =	1∕σx,
Qx (1 + p∖ - 2β∕σx2
Qz	= J	2 M/
σx ( 1+ p∖ - 2β∕σx2
(52)
From Eq. 52, DT and DC are derived as:
DT
DC
β∕2.
(53)
As shown in section 3.3, the added noise, β∕2, should be reasonably smaller than the data variance
Qx2. If Qx2	β, b and Qz in Eq. 52 can be approximated as:
DT '
(β∕2)2
Qx2
驾De.
Qx2
(54)
As shown in this equation, DT∕DC is small in the VAE where the added noise is reasonably small,
and DT can be ignored.
Next, the relation to the Wiener filter is discussed. We consider an simple 1-dimensional Gaussian
process. Let X 〜N(x; 0, Qlx) be input data. Then, X is scaled by s, and a Gaussian noise n 〜
16
Under review as a conference paper at ICLR 2021
N(n; 0, σn2) is added. Thus, y = s x + n is observed. From the Wiener filter theory, the estimated
value with minimum distortion, X can be formulated as:
X
sσχ2
s2σχ2 + σn2 y.
(55)
In this case, the estimation error is derived as:
4	24	2
E[(x - X)2] = C—2n~~2τ2σx2 + C-2-x—2τ2σn2 =	2 I Ix 2/2、(bn2/s2). (56)
(s2σχ2 + σn2)2	(s2σx2 + σj)2 σx2 + (σn2∕s2)
In the second equation, the first term is corresponding to the transform loss, and the second term is
corresponding to the coding loss. Here the ratio of the transform loss and coding loss is derived as
σn2∕(s2σx2). By appying S = 1∕σx and σn = σz to σn2∕(s2σx2) and assuming σx》β∕2, this
ratio can be described as:
2
σn2
s2σx2
σz2
β∕2_________4__________
σ2(1+ √1- 2β∕σx2)2
(57)
This result is consistent with Eq. 54, implying that optimized VAE and the Wiener filter show similar
behaviours.
C Derivation of quantitative properties in Section 3.5
C.1 Derivation of the estimated variance
This appendix explains the derivation of Eq. 21 in Section 3.5. Here, we assume that zj is mapped
to yj such that yj is set to 0 at zj = 0. We also assume that the prior distribution is N (z; 0, In).
The variance is derived by the subtraction of E[yj]2, the square of the mean, from E[yj2], the square
mean. Thus, the approximations of both E[yj] and E[yj2] are needed.
First, the approximation of the mean E[yj] is explained. Because the cumulative distribution func-
tions (CDFs) ofyj are the same as CDF of zj, the following equations hold:
Z0
-∞
p(yj)dyj
Z0
-∞
p(zj )dzj = 0.5,
p(yj )dyj =	p(zj )dzj = 0.5.
00
(58)
This equation means that the median of the yj distribution is 0. Because the mean and median are
close in most cases, the mean E[yj] can be approximated as 0. As a result, the variance ofyj can be
approximated by the square mean E[yj2].
Second, the approximation of the square mean E[yj2] is explained. The standard deviation of the
posterior σj(x) is assumed as a function of zj, regardless of x. This function is denoted as σj (zj).
For zj ≥ 0, yj is approximated as follows, using Eq. 11 and replacing the average of 1∕σj (z´j) over
z´j = [0,zj] by 1∕σj(zj):
∕zjdyj”,
yj = J0 dj d´j
叵广 ι ,，〜∕β	户.，
V2 J0 σj (Zj )di' V2 σj (Zj) J0 dj
∕β zj
V 2 σj (Zj)
(59)
The same approximation is applied to Zi < 0. Then the square mean of yi is approximated as
follows, assuming that the correlation between σ(Zj)-2 and Zj2 is low:
/ yj 2p(yj )dyj ` 2/ (σj) ) P(Zj )dzj ' 2 / σj (Zj )-2 P(Zj IdZjJ Zj2P(Zj )dzj.	(6O)
Finally, the square mean of yi is approximated as the following equation, using Zj2P(Zj)dZj = 1
and replacing σj(Zj)2 by σj(x)2, i.e., the posterior variance derived from the input data:
/yj2p(yj)dyj ` 2 /σj(Zj)-2P(Zj)dZj ` 2 Z JE(z )[σj(Zj)-2] ` 2 x%X)[σj(χ)-2]. (61)
Although some rough approximations are used in the expansion, the estimated variance in the last
equation seems still reasonable, because σj(x) shows a scale factor between yj and Zj while the vari-
ance of Zj is always 1 for the priorN(Zj; 0, 1). Considering the variance of the prior Zj 2 P(Zj)dZj
in the expansion, this estimation method can be applied to any prior distribution.
17
Under review as a conference paper at ICLR 2021
Figure 7: Projection of the volume element from the implicit orthonormal space to the isometric
space and input space. Vn (∙) denotes n-dimensional volume.
C.2	Derivation of the data probability estimation
This appendix shows the derivation of variables in Eqs. 22 and 23. First, the derivation of Lx for
the input x is described. Then, the PDF ratio between the input space and inner product space is
explained for the cases m = n and m > n.
Derivation of Lx for the input x :
AsshownininEq. 1, Lx is denoted as -Ez〜q@(z\x)[∙ ]+4Dkl( ∙). We approximate Ez〜q1(z|x)[ ∙]
as 1 (D(x, Dec。(μx + σx)) + D(χ, Dec。(μx — σx))), i.e., the average of two samples, instead of
the average over Z 〜qφ(z|x). Dkl( ∙ ) can be calculated from μx and σx using Eq. 2.
The PDF ratio in the case m = n:
The PDF ratio for m = n is aJacobian determinant between two spaces. First, (∂∂y )TGx (∂∂y) = Im
holds from Eq. 12. ∣∂χ∕∂y∣2 |Gx| = 1 also holds by calculating the determinant. Finally, ∣∂χ∕∂y|
is derived as ∣Gχ∣1/2 using ∣∂y∕∂x∣ = ∣∂x∕∂y|-1.
The PDF ratio in the case m > n and Gx = axIm :
Although the strict derivation needs the treatment of the Riemannian manifold, we provide a simple
explanation in this appendix. Here, it is assumed that Dkl(j)(∙) > 0 holds for all j = [1,..n]. If
DκL(j) (∙) = 0 for some j, n is replaced by the number of latent variables with Dkl(j)(∙) > 0.
For the implicit isometric space Siso(⊂ Rm), there exists a matrix Lx such that both y = Lxx and
Gx = tLxLx holds. w denotes a point in Siso, i.e., w ∈ Siso. Because Gx is assumed as axIm in
Section 3.5, Lx = ax1/2Im holds. Then, the mapping function w = h(x) between Sinput and Siso
is defined, such that:
’?(X)	=	^w	=	Lx,	and	h(x(0)) = w(0)	for ∃ x(0) ∈	Sinput and ∃ w(0) ∈ 凡。.(62)
∂x	∂x
Let δx and δw are infinitesimal displacements around x and w = h(x), such that w + δw =
h(x + δx). Then the next equation holds from Eq. 62:
δw = Lxδx	(63)
Let δx(1), δx(2), δw(1), and δw(2) be two arbitrary infinitesimal displacements around x and w =
h(x), such that δw(1) = Lxδx(1) and δw(2) = Lxδx(2). Then the following equation holds, where
• denotes the dot product.
tδx⑴Gxδx⑵=t(Lxδx(I))(Lxδχ(2)) = δw(1) • δw(2)	(64)
This equation shows the isometric mapping from the inner product space for x ∈ Sinput with the
metric tensor Gx to the Euclidean space for w ∈ Siso .
Note that all of the column vectors in the Jacobian matrix ∂x∕∂y also have a unit norm and are
orthogonal to each other in the metric space for x ∈ Sinput with the metric tensor Gx . Therefore,
the m × n Jacobian matrix ∂w∕∂y should have a property that all of the column vectors have a unit
norm and are orthogonal to each other in the Euclidean space.
18
Under review as a conference paper at ICLR 2021
Then n-dimensional space which is composed of the meaningful dimensions from the implicit iso-
metric space is named as the implicit orthonormal space Sortho . Figure 7 shows the projection of
the volume element from the implicit orthonormal space to the isometric space and input space.
Let dVortho be an infinitesimal n-dimensional volume element in Sortho . This volume element is a
n-dimensional rectangular solid having each edge length dyj . Let Vn (dVX) be the n-dimensional
volume of a volume element dVX. Then, Vn (dVortho) = Qjn dyj holds. Next, dVortho is pro-
jected to n dimensional infinitesimal element d%so in Siso by ∂w/∂y. Because of the orthonor-
mality, dViso is equivalent to the rotation / reflection of dVortho, and Vn (dViso) is the same as
Vn (dVortho), i.e., Qjn dyj . Then, dViso is projected to n-dimensional element dVinput in Sinput
by ∂x∕∂w = L-1 = ax-1/2Im. Because each dimension is scaled equally by the scale factor
ax-1/2, Vn(dVinput) = Qjn ax-1/2dyj = ax-n/2 Vn(dVortho) holds. Here, the ratio of the vol-
ume element between Sinput and Sortho is Vn(dVinput)/Vn(dVortho) = ax-n/2. Note that the PDF
ratio is derived by the reciprocal of Vn(dVinput)/Vn(dVortho). As a result, the PDF ratio is derived
as ax n/2 .
D	Details of the networks and training conditions in the
EXPERIMENTS
This appendix explains the networks and training conditions in Section 4.
D. 1 Toy data set
This appendix explains the details of the networks and training conditions in the experiment of the
toy data set in Section 4.1.
Network configurations:
FC(i, o, f) denotes a FC layer with input dimension i, output dimension o, and activate function f.
The encoder network is composed of FC(16, 128, tanh)-FC(128, 64, tahh)-FC(64, 3, linear)×2 (for
μ and σ). The decoder network is composed of FC(3, 64, tanh)-FC(64, 128, tahh)-FC(128, 16,
linear).
Training conditions:
The reconstruction loss D(∙, ∙) is derived such that the loss per input dimension is calculated and
all of the losses are averaged by the input dimension m = 16. The KL divergence is derived as a
summation of DκLj) (∙) as explained in Eq. 2.
In our code, we use essentially the same, but a constant factor scaled loss objective from the original
β-VAE form Lx = D(∙, ∙) + βDκLj)(∙) in Eq. 1, such as:
Lx = λD(∙,∙)+ DκL(j)(∙).	(65)
Equation 65 is essentially equivalent to L = D(∙, ∙) + βDκLj)(∙), multiplying a constant λ = β-1
to the original form. The reason why we use this form is as follows. Let ELBOtrue be the true
ELBO in the sense of log-likelihood, such as E[logp(x)]. As shown in Section 3.3, the minimum
of the loss objective in the original β-VAE form is likely to be a -βELBOtrue + Constant. If we
use Eq. 65, the minimum of the loss objective will be -ELBOtrue + Constant, which seems more
natural form of ELBO. Thus, Eq. 65 allows estimating a data probability from Lx in Eqs. 22 and
23, without scaling Lx by 1∕β.
Then the network is trained with λ = β-1 = 100 using 500 epochs with a batch size of 128. Here,
Adam optimizer is used with the learning rate of 1e-3. We use aPC with CPU Inter(R) Xeon(R) CPU
E3-1280v5@3.70GHz, 32GB memory equipped with NVIDIA GeForce GTX 1080. The simulation
time for each trial is about 20 minutes, including the statistics evaluation codes.
In our experiments, λ orβ-1, i.e., 100, seems somewhat large. This is caused by the use of the mean
square error as a reconstruction loss. In contrast, KL divergence is the sum for the whole image,
which can be thought ofas a rate for the whole image. Considering the number of input dimensions,
β0 = (λ∕16)-1 = 16∕λ = 0.16 is thought of as β in the general form of VAE.
19
Under review as a conference paper at ICLR 2021
D.2 CelebA data set
This appendix explains the details of the networks and training conditions in the experiment of the
toy data set in Section 4.2.
Network configurations:
CNN(w, h, s, c, f) denotes a CNN layer with kernel size (w, h), stride size s, dimension c, and activate
function f. GDN and IGDN 2 are activation functions designed for image compression (Balle et al.,
2016). This activation function is effective and popular in deep image compression studies.
The encoder network is composed of CNN(9, 9, 2, 64, GDN) - CNN(5, 5,2, 64, GDN) - CNN(5, 5,
2, 64, GDN) - CNN(5, 5, 2, 64, GDN) - FC(1024, 1024, SoftplUS) - FC(1024, 32, None) × 2 (for μ
and σ) in encoder.
The decoder network is composed of FC(32, 1024, softplus) - FC(1024, 1024, softplus) - CNN(5, 5,
2, 64, IGDN) - CNN(5, 5, 2, 64, IGDN) - CNN(5, 5, 2, 64, IGDN)-CNN(9, 9, 2, 3, IGDN).
Training conditions:
In this experiment, SSIM explained in Appendix H.2 is used as a reconstruction loss. The recon-
struction loss D(∙, ∙) is derived as follows. Let SSIM be a SSIM calculated from two input images.
Then 1 - SSIM is set to D(∙, ∙). The KL divergence is derived as a summation of DκLj)(•) as
explained in Eq. 2.
We also use the loss form as in Equation 65 in our code. In the case of the decomposed loss, the
loss function Lx is setto λ(D(x, X) + D(X, X)) + Dkl (∙) in our code. Then, the network is trained
with λ = β -1 = 1, 000 using a batch size of 64 for 300,000 iterations. Here, Adam optimizer is
used with the learning rate of 1e-3.
We use a PC with CPU Intel(R) Core(TM) i7-6850K CPU @ 3.60GHz, 12GB memory equipped
with NVIDIA GeForce GTX 1080. The simulation time for each trial is about 180 minutes, includ-
ing the statistics evaluation codes.
In our experiments, λ = β-1 = 1, 000 seems large. This is caused by the use of SSIM. As explained
in Appendix H.2, SSIM is measured for a whole image, and its range is between 0 and 1. The order
of 1 - SSIM is almost equivalent to the mean square error per pixel, as shown in Eq. 74. As
explained in Appendix D.1, KL divergence is thought ofas a rate for the whole image. Considering
the number of pixels in a image, β0 = (λ∕(64 X 64))T = 4096∕λ = 4.096 is comparable to β in
the general form of VAE.
2Google provides a code in the official Tensorflow library (https://github.com/tensorflow/compression)
20
Under review as a conference paper at ICLR 2021
E	Additional results in the toy datasets
E.1	S cattering plots for the square error loss in Section
4.1	Figure 8a shows the plots of p(x) and estimated probabilities for the square error coding loss in
Section 4.1, where the scale factor Qx in Eq. 23 is 1. Thus, both exp(-Lx∕β) andp(μ(x)) Qj σj∙(x)
show a high correlation, allowing easy estimation of the data probability in the input space. In
contrast, p(μ(x)) still shows a low correlation. These results are consistent with our theory.
×10~2	×10~4	×10-5
•qo」d P①看IU-M
qo」d P①看IU-M
qo」d P①看E4s山
(a) p(μ(x))	(b) exp(-Lχ∕β)	(C) p(μ(x)) ∏j σj⑺
Figure 8:	Plots of the data generation probability (x-axis) versus estimated probabilities (y-axes) for
the square error loss. y-axes are (a) p(μ(χ)), (b) exp(-Lχ/β), and (c) p(μ(χ)) Qj。人况).
E.2 ABLATION STUDY USING 3 TOY DATASETS, 3 CODING LOSSES, AND 10 β PARAMETERS.
In this appendix, we explain the ablation study for the toy datasets. We introduCe three toy datasets
and three Coding losses inCluding those used in SeCtion 4.1. We also Change β-1 = λ from 1 to
1, 000 in training. The details of the experimental Conditions are shown as follows.
Datasets: First, we Call the toy dataset used in SeCtion 4.1 the Mix dataset in order to distinguish
three datasets. The seCond dataset is generated suCh that three dimensional variables s1, s2, and s3
are sampled in aCCordanCe with the distributions p(s1), p(s2), and p(s3) in Figure 9. The varianCes
of the variables are the same as those of the Mix dataset, i.e., 1/6, 2/3, and 8/3, respeCtively. We
Call this the Ramp dataset. BeCause the PDF shape of this dataset is quite different from the prior
N(z; 0, I3), the fitting will be the most diffiCult among the three. The third dataset is generated
suCh that three dimensional variables s1, s2, and s3 are sampled in aCCordanCe with the normal
distributions N(s1; 0, 1/6), N(s2; 0, 2/3), and N(s3; 0, 8/3), respeCtively. We Call this the Norm
dataset. The fitting will be the easiest, beCause both the prior and input have the normal distributions,
and the posterior standard deviation, given by the PDF ratio at the same CDF, Can be a Constant.
Coding losses: Two of the three Coding losses is the square error loss and the downward-Convex
loss desCribed in SeCtion 4.1. The third Coding loss is a upward-Convex loss whiCh we design as Eq.
66 suCh that the sCale faCtor ax beComes the reCiproCal of the sCale faCtor in Eq. 24:
D(x, X)= Qxkx — X∣∣2, where Qx = (2/3 + 2 ∣∣xk2/21)Tand Gx = QxIm.	(66)
Figure 10 shows the sCale faCtors ax in Eqs. 24 and 66, where s1 in x = (s1, 0, 0) moves within
±5.
Parameters: As explained in Appendix D.1, λ = 1∕β is used as a hyper parameter. Specifically,
λ = 1, 2, 5, 10, 20, 50, 100, 200, 500, and 1, 000 are used.
Figures 11 - 19 show the property measurements for all combinations of the datasets and coding
losses, with changing λ. In each Figure, the estimated norms of the implicit transform are shown in
the figure (a), the ratios of the estimated variances are shown in the figure (b), and the correlation
coefficients between p(x) and estimated data probabilities are shown in the figure (c), respectively.
First, the estimated norm of the implicit transform in the figures (a) is discussed. In all conditions, the
norms are close to 1 as described in Eq. 20 in the λ range 50 to 1000. These results show consistency
with our theoretical analysis, supporting the existence of the implicit orthonormal transform. The
values in the Norm dataset are the closest to 1, and those in the Ramp dataset are the most different,
which seems consistent with the difficulty of the fitting.
21
Under review as a conference paper at ICLR 2021
一P(Sl): E岛]=0,E岛2]=iq
-p(s2): E[52]=0,E[¾2]=2∕3
—p(s3): E[a3]=0,E[532]=8∕3
------	SIF
-4√3∕3∙ -2√3∕3 -√3∕3	2√3∕3 4√3∕3	8√3∕3
Figure 10: Scale factor ax for the
downward-convex loss and upward-
convex loss.
Figure 9:	PDFs of three variables
to generate a Ramp dataset.
Second, the ratio of the estimated variances is discussed. In the figures (b), Var(zj ) denotes the
estimated variance, given by the average of σj-(2x). Then, Var(z2)/Var(z1) and Var(z3)/Var(z1)
are plotted. In all conditions, the ratios of Var(z2)/Var(z1) and Var(z3)/Var(z1) are close to the
variance ratios of the input variables, i.e., 4 and 16, in the λ range 5 to 500. Figure 20 shows
the detailed comparison of the ratio for the three datasets and three coding losses at λ = 100. In
most cases, the estimated variances in the downward-convex loss are the smallest, and those in
the upward-convex loss are the largest, which is more distinct for Var(z3)/Var(z1). This can be
explained as follows. When using the downward-convex loss, the space region with a large norm
is thought of as shrinking in the inner product space, as described in Section 4.1. This will make
the variance smaller. In contrast, when using the upward-convex loss, the space region with a large
norm is thought of as expanding in the inner product space, making the variance larger. Here, the
dependency of the losses on the ratio changes is less in the Norm dataset. The possible reason is
that data in the normal distribution concentrate around the center, having less effect on the loss scale
factor in the downward-convex loss and upward-convex loss.
Third, the correlation coefficients between p(x) and the estimated data probabilities in the figures
(c) are discussed. In the Mix dataset and Ramp dataset, the correlation coefficients are around
0.9 in the λ range from 20 to 200 when the estimated probabilities axn/2p(μ(x)) Q；=1 σj∙(X)and
axn/2 eχp(-(1∕β)Lx) in Eq. 23 are used. When using p(μ(x)) Qn=I σj∙(X)and exp(-(1∕β)Lx)
in the downward-convex loss and upward-convex loss, the correlation coefficients become worse.
In addition, when using the prior probability p(μ(x)), the correlation coefficients always show the
worst. In the Norm dataset, the correlation coefficients are close to 1.0 in the wider range of λ when
using the estimated distribution in Eq. 23. When using p(μ(x)) Qn=I σj∙(x)and exp(-(1∕β)Lx) in
the downward-convex loss and upward-convex loss, the correlation coefficients also become worse.
When using the prior probability p(μ(x)), however, the correlation coefficients are close to 1 in
contrast to the other two datasets. This can be explained because both the input distribution and the
prior distribution are the same normal distribution, allowing the posterior variances almost constant.
These results also show consistency with our theoretical analysis.
Figure 21 shows the dependency of the coding loss on β for the Mix, Ramp, and Norm dataset using
square the error loss. From DG in Eq. 14 and n = 3, the theoretical value of coding loss is 3β, as
also shown in the figure. Unlike Figs. 11-19, x-axis is β = λ-1 to evaluate the linearity. As expected
in section 3.3, the coding losses are close to the theoretical value where β < 0.1, i.e., λ > 10.
Figure 22 shows the dependency of the ratio of transform loss to coding loss on β for the Mix,
Ramp, and Norm dataset using square the error loss. From Eq. 54, the estimated transform loss is
P3=1 (β∕2)2Nar(Si) = 63β2. Thus the theoretical value is (63β2)/(3β)=骅,as is also shown in
the figure. x-axis is also β = λ-1 like Figure 21. Considering the correlation coefficient discussed
above, the useful range ofβ seems between 0.005-0.05 (20-200 for λ). In this range, the ratio is less
than 0.1, implying the transform loss is almost negligible. As expected in section 3.2 and appendix
B, the ratio is close to the theoretical value where β > 0.01, i.e., λ < 100. For β < 0.01, the
conversion loss is still negligibly small, but the ratio is somewhat off the theoretical value. The
reason is presumably that the transform loss is too small to fit the network.
As shown above, this ablation study strongly supports our theoretical analysis in sections 3.
22
Under review as a conference paper at ICLR 2021
O O O ∩-
QSQ5.0.
2 11
⅛PJ9A< ZI(H)S JO O=UX
.0ʒ
UonPIəɪɪoɔ
(a) Estimated norm 2σj(x)2Dj (Z). (b) Ratio of the estimated
variances Var(z3)/Var(z1) and
Var(z2)/Var(z1)
(c) Correlation coefficient of
the estimated data probability
Figure 11: Property measurements of the Mix dataset using the square error loss. λ is changed from
1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
O 5
L S
Uo 口Pləɪnŋ
(a) Estimated norm 2σj(x)2Dj (Z). (b) Ratio of the estimated	(c) Correlation coefficient of
variances Var(z3)/Var(z1) and the estimated data probability
Var(z2)/Var(z1)
Figure 12: Property measurements of the Mix dataset using the downward-convex loss. λ is changed
from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
.0ʒ
UOnpIəɪɪoo
(a) Estimated norm 2σj∙(x)2Dj (Z). (b) Ratio of the estimated	(C) Correlation coefficient of
variances Var(z3)/Var(z1) and	the estimated data probability
Var(z2)/Var(z1)
Figure 13: Property measurements of the Mix dataset using the upward-convex loss. λ is changed
from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
23
Under review as a conference paper at ICLR 2021
1	10	100	1000
λ=βτ (logarithmic scale)
∙P(μ)	∙G^(0.5)o p(μ)
(a) Estimated norm 2σj(x)2Dj (Z). (b) Ratio of the estimated
variances Var(z3)/Var(z1) and
Var(z2)/Var(z1)
(c) Correlation coefficient of
the estimated data probability
Figure 14: Property measurements of the Ramp dataset using the square error loss. λ is changed
from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
.05
UoWəɪɪoɔ
(a) Estimated norm 2σj∙(x)2Dj (z). (b) Ratio of the estimated	(c) Correlation coefficient of
variances Var(z3)/Var(z1) and	the estimated data probability
Var(z2)/Var(z1)
Figure 15: Property measurements of the Ramp dataset using the downward-convex loss. λ is
changed from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
⅛PJ9>< ZI(K)S JOoDPX
UoyFləɪɪoɔ
(a) Estimated norm 2σj∙(x)2Dj (z). (b) Ratio of the estimated	(c) Correlation coefficient of
variances Var(z3)/Var(z1) and the estimated data probability
Var(z2)/Var(z1)
Figure 16: Property measurements of the Ramp dataset using the upward-convex loss. λ is changed
from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
24
Under review as a conference paper at ICLR 2021
OO O 。 。
0.s S5.C5
2 11
ətlopjəAV Zl(K)trJO O=UH
.05
UoDPɪəɪɪoɔ
(a) Estimated norm 2σj(x)2Dj (z). (b) Ratio of the estimated	(c) Correlation coefficient of
variances Var(z3)/Var(z1) and	the estimated data probability
Var(z2)/Var(z1)
Figure 17: Property measurements of the Norm dataset using the square error loss. λ is changed
from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
OO O 。 。
QLKQ5.6
2 11
⅛BJ9AVZl(K)trJoogUX
O 5
L S
UoWəɪɪoɔ
(a) Estimated norm 2σj(x)2Dj (z). (b) Ratio of the estimated	(c) Correlation coefficient of
variances Var(z3)/Var(z1) and the estimated data probability
Var(z2)/Var(z1)
Figure 18: Property measurements of the Norm dataset using the downward-convex loss. λ is
changed from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
(a) Estimated norm 2σj∙(x)2Dj (z).
⅛PJ9>< ZI(K)S JO.2JPX
.05
uo-lpɪəɪɪoo
(b)	Ratio of the estimated
variances Var(z3)/Var(z1) and
Var(z2)/Var(z1)
(c)	Correlation coefficient of
the estimated data probability
Figure 19:	Property measurements of the Mix dataset using the upward-convex loss. λ is changed
from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
25
Under review as a conference paper at ICLR 2021
4.6
4 2 4
4.4.
(IZ)JE>'CZ)JE>
Downward Square Error Upward
Convex	Convex
Reconstruction Loss
(-Z⅛A⅞Z⅛A
Reconstruction Loss
(b) Var(z3)/Var(z1).
(a) Var(z2)/Var(z1).
Figure 20:	Ratio of the estimated variances Var(z3)/Var(z1) and Var(z2)/Var(z1) for the three
datasets and three coding losses at λ = 100. Var(zj ) denotes the estimated variance, given by the
average of σj-(2x) .
Figure 21: Dependency of Coding Loss on
β for Mix, Norm, and Ramp dataset using
square loss.
Theoretical estimation
→-Mix, MSE
♦Norm, MSE
→-Ramp, MSE
Figure 22: Dependency of Transform loss /
Coding Loss Ratio on β for Mix, Norm, and
Ramp dataset using square loss.
26
Under review as a conference paper at ICLR 2021
F Additional results in CelebA dataset
F.1 Traversed outputs for all the component in the experimental section 4.2
(a) Trained using the conventional loss form. (b) Trained using the decomposed loss form.
Figure 23: Traversed outputs for all the component, changing zj from -2 to 2. The latent variables
zj are numbered in descending order by the estimated variance σj-2 shown in Figures 4 and 5.
27
Under review as a conference paper at ICLR 2021
Figure 23 shows decoder outputs for all the components, where each latent variable is traversed
from -2 to 2. The estimated variance of each yj , i.e., σj-2, is also shown in these figures. The
latent variables zi are numbered in descending order by the estimated variances. Figure 23a is a
result using the conventional loss form, i.e., Lx = D(x, X) + βDKL(∙). The degrees of change
seem to descend in accordance with the estimated variances. In the range where j is 1 from 10,
the degrees of changes are large. In the range j > 10, the degrees of changes becomes gradually
smaller. Furthermore, almost no change is observed in the range j > 27. As shown in Figure 4,
DKL(j) (∙) is close to zero for j > 27, meaning no information. Thus, this result is clearly consistent
with our theoretical analysis in Section
Figure 23b is a result using the decomposed loss form, i.e., Lx = D(x, X) + D(X, X) + βDKL(∙).
The degrees of change also seem to descend in accordance with the estimated variances. When look-
ing at the detail, there are still minor changes even j = 32. As shown in Figure 5, KL divergences
DKL(j) (∙) for all the components are larger than zero. This implies all of the dimensional compo-
nents have meaningful information. Therefore, we can see a minor change even j = 32. Thus, this
result is also consistent with our theoretical analysis.
Another minor difference is sharpness. Although the quantitative comparison is difficult, the de-
coded images in Figure 23b seems somewhat sharper than those in Figure 23a. A possible reason
for this minor difference is as follows. The transform loss D(x, X) serves to bring the decoded
image of μ(x) closer to the input. In the conventional image coding, the orthonormal transform and
its inverse transform are used for encoding and decoding, respectively. Therefore, the input and the
decoded output are equivalent when not using quantization. If not so, the quality of the decoded
image will suffer from the degradation. Considering this analogy, the use of decomposed loss might
improve the decoded images for μ(x), encouraging the improvement of the orthonormality of the
encoder/decoder in VAE.
F.2 Additional experimental result with other condition
In this Section, we provide the experimental results with other condition. We use essentially the
same condition as described in Appendix D.2, except for the following conditions. The bottleneck
size and λ are set to 256 and 10000, respectively. The encoder network is composed of CNN(9,
9, 2, 64, GDN) - CNN(5, 5, 2, 64, GDN) - CNN(5, 5, 2, 64, GDN) - CNN(5, 5, 2, 64, GDN) -
FC(1024, 2048, softplus) - FC(2048, 256, None) × 2 (for μ and σ) in encoder. The decoder network
is composed of FC(256, 2048, softplus) - FC(2048, 1024, softplus) - CNN(5, 5, 2, 64, IGDN) -
CNN(5, 5, 2, 64, IGDN) - CNN(5, 5, 2, 64, IGDN)-CNN(9, 9, 2, 3, IGDN).
Figures 24a and 24b show the averages of σj(x)-2 as well as the average and the standard deviation
of 2 σj∙(x)2Dj (z) in the conventional loss form and the decomposed loss form, respectively. When
using the conventional loss form, the mean of 2σj∙(x)2Dj (z) is 1.25, which is closer to 1 than the
(a) Conventional loss form
Lw(Z)
(b) Decomposed loss form
Figure 24: Graph of σj∙(x)-2 average and ∣σj∙(x)2Dj (Z) in CelebA dataset. The bottleneck size and
λ are set to 256 and 10000, respectively.
Quj-(Z)
28
Under review as a conference paper at ICLR 2021
mean 1.83 in Section 4.2. This suggests that the implicit transform is closer to the orthonormal. The
possible reason is that a bigger reconstruction error is likely to cause the interference to RD-trade
off and a slight violation of the theory, and it might be compensated with a larger lambda. When
using the decomposed loss form, the mean of 2σj∙(x)2Dj (Z) is 0.95, meaning almost unit norm.
These results also support that VAE provides the implicit orthonormal transform even if the lambda
or bottleneck size is varied.
G Additional Experimental Result with MNIST dataset
In this Appendix, we provide the experimental result of Section 4.2 with MNIST dataset3 consists of
binary hand-written digits with a dimension of 768(=28 × 28). We use standard training split which
includes 50,000 data points. For the reconstruction loss, we use the binary cross entropy loss (BCE)
for the Bernoulli distribution. We averaged BCE by the number of pixels.
The encoder network is composed of FC(768, 1024, relu) - FC(1024, 1024, relu) - FC(1024, bot-
tleneck size) in encoder. The decoder network is composed of FC(bottleneck size, 1024, relu) -
FC(1024, 1024, relu) - FC(1024, 768, sigmoid). The batch size is 256 and the training iteration
number is 50,000. In this section, results with two parameters, (bottleneck size=32, λ=2000) and
(bottleneck size=64, λ=10000) are provided. Note that since we averaged BCE loss by the number
of pixels, β in the conventional β VAE is derived by 768∕λ. Then, the model is optimized by Adam
optimizer with the learning rate of 1e-3, using the conventional (not decomposed) loss form.
We use a PC with CPU Intel(R) Core(TM) i7-6850K CPU @ 3.60GHz, 12GB memory equipped
with NVIDIA GeForce GTX 1080. The simulation time for each trial is about 10 minutes, including
the statistics evaluation codes.
Figure 25 shows the averages of σj(x) -2 as well as the average and the standard deviation of
2σj∙(x)2Dj(Z). In both conditions, the means of 2σj∙(x)2Dj(Z) averages are also close to 1 except
in the dimensions where σj(x)-2 is less than 10. These results suggest the theoretical property still
holds when using the BCE loss. In the dimensions where σj∙(x)-2 is less than 10, the ∣σj∙(x)2Dj (Z)
is somewhat lower than 1. The possible reason is that DKL⑴(∙) in such dimension is 0 for some
inputs and is larger than 0 in other inputs. The understanding of the transition region needs further
study.
• VarianCe
ɪ Norm: Average
× Norm: Standard Deviation
F 1E+3
e
⅛ 1E+1
1E+O
o.o
0	10	20	30
Number Oflatentvariables
(descending order OfEstimated Variance)
• Variance
ɪ Norm: Average
× Norm: Standard Deviation
1E+O ~0.0
0	20	40	60
Number Oflatentvariables
(descending order OfEstimated Variance)

(a) bottle neck=32, λ=2000
(b) bottle neck=64, λ=10000
Figure 25: Graph of σj∙(x)-2 average and 2σj∙(x)2Dj (Z) in MNIST dataset.
3http://yann.lecun.com/exdb/mnist/
29
Under review as a conference paper at ICLR 2021
H Derivation/Explanation in RDO-related equation expansions
H.1 Approximation of distortion in uniform quantization
Let T bea quantization step. Quantized values Zj is derived as k T, where k = round(zj/T). Then
d, the distortion per channel, is approximated by
d
(k+1/2)T
=	p(zj)(zj - kT)2 dzj
k (k-1/2)T
(k+1/2)T 1
'∑Tp(kT)	斤(Zj- kT)2 dzj
k	(k-1/2)T T
T2
=适 X TPlkT)
k
T2
` ——.
12
Here, Pk Tp(k T) ' R-∞∞p(zj)dzj
(67)
1 is used. The distortion for the given quantized value is
2	12
also estimated as T /12, because this value is approXimated by J(k 1/2)T T(Zj	k T ) dZj.
H.2 Approximation of reconstruction loss as a quadratic form.
In this appendiX, the approXimations of the reconstruction losses as a quadratic form tδx Gxδx+Cx
are eXplained for the sum of square error (SSE), binary cross entropy (BCE) and Structural Similarity
(SSIM). Here, we have borrowed the derivation of BCE and SSIM from Kato et al. (2020), and add
some eXplanation and clarification to them for convenience. We also describe the log-likelihood of
the Gaussian distribution.
Let X and Xi be decoded sample Decθ (Z) and its i-th dimensional component respectively. δx and
δxi denote x - X and Xi - r^i, respectively. It is also assumed that δx and δxi are infinitesimal. The
details of the approXimations are described as follows.
Sum square error:
In the case of sum square error, Gx is equal to Im . This can be derived as:
mm
X (xi - Xi)2 = X δxi2 = tδxImδx.	(68)
i=1	i=1
Binary cross entropy:
Binary cross entropy is a log likelihood of the Bernoulli distribution. The Bernoulli distribution is
described as:
m
Pθ (x|z) = Y Xixi(I- Xi)(If)	(69)
i=1
Then, the binary cross-entropy (BCE) can be eXpanded as:
m
-logpθ(x|z) = - log Y Xixi(I - Xi)(I-Xi)
i=1
m
= X(-Xi logXi - (1 - Xi)log(i - Xi))
i=1
=	(-Xi log(Xi + δXi) - (1 - Xi) log(1 - Xi - δXi))
i
=X(-Xilog (1+2)-(1-Xi)Iog (1-±))
+ X(-Xi log(Xi) - (1 - Xi) log(1 - Xi)).	(70)
i
30
Under review as a conference paper at ICLR 2021
Figure 26: Graph of 1 (1 +
in the BCE approximation.
Here, the second term of the last equation is a constant Cx depending on x. Using log(1 + x) =
x - x2/2 + O(x3), the first term of the last equation is further expanded as follows:
+ O (δxi3)
+ O (δxi3).
(71)
As a result, a metric tensor Gx can be approximated as the following positive definite Hermitian
matrix:
(
1
2
0
0
1
2
∖
...
...
.
..
(72)
Here, the loss function in each dimension 1 ( — + J ) is a downward-convex function as shown
,	2 x1	1-x1
in Figure 26.
Structural similarity (SSIM):
Structural similarity (SSIM) (Wang et al., 2001) is widely used for picture quality metric, which is
close to human subjective quality. Let SSIM be a SSIM value between two pictures. The range of
the SSIM value is between 0 and 1. The higher the value, the better the quality. In this appendix, we
also show that (1 - SSIM) can be approximated to a quadratic form such as tδx Gxδx.
SSIMN×N(h,v)(x, y) denotes a SSIM value between N × N windows in pictures X and Y , where
x ∈ RN2 and y ∈ RN2 denote N × N pixels cropped from the top-left coordinate (h, v) in the
images X and Y, respectively. Let μx, μy be the averages of all dimensional components in x,
y , and σx , σy be the variances of all dimensional components in x, y in the N × N windows,
respectively. Then, SSIMN ×N(h,v) (x, y) is derived as
SSIMN×n(h,v) (x, y) =	2μxμy 2 ∙	2σxy 2.	(73)
v , 7	μx2 + μy2 σχ2 + σy2
In order to calculate a SSIM value for a picture, the window is shifted in a whole picture and all
of SSIM values are averaged. Therefore, if 1 - SSIMN×N(h,v) (x, y) is expressed as a quadratic
form tδx G(h,v)x δx, (1 - SSIM) can be also expressed in quadratic form tδx Gxδx.
Let δX be a minute displacement of x. μδx and σδx 2 denote an average and variance of all dimen-
sional components in δx, respectively. Then, SSIM between x and x + δx can be approximated
as:
22
SSIMN ×N (h,v) (X, X + δ X) ' 1 - q3x 2 -	2 + O ((Iδ x|/|x|)3) .	(74)
2μx	2σx	`	/
31
Under review as a conference paper at ICLR 2021
Then μδχ2 and σδχ2 can be expressed as
/ 1	1	...	1 ∖
1	1	1	...	1
μδχ2 = tδx Mδx, where M = —2	...	.	,	(75)
N2	..	..	..	..
1	1	...	1
and
σδχ2 = tδx Vδx, where V = EIN - M,	(76)
respectively. As a result, 1-	SSIMN ×N (h,v) (x, x + δx) can be expressed in the following
quadratic form as:
1 - SSIMN×n(h,v)(x, X + δx) ` tδx G(h,v)χδx, where G(h,v)χ = 2μ^2MM + 2σ12 v) ∙
x	x (77)
It is noted that M is a positive definite Hermitian matrix and V is a positive semidefinite Hermitian
matrix. Therefore, G(h,v)x is a positive definite Hermitian matrix. As a result, (1 - SSIM) can be
also expressed in quadratic form tδx Gxδx, where Gx is a positive definite Hermitian matrix.
Log-likelihood of Gaussian distribution:
Gaussian distribution is described as:
mm
Pθ(x|z) = Y 士……=Y 士e-δx*2σ2
i=1 2πσ2	i=1 2πσ2
(78)
where σ2 is a variance as a hyper parameter. Then, the log-likelihood of the Gaussian distribution is
denoted as:
mm
-logPθ(x|z) = - log Y √= e-δxi/2σ = 2σ2 X δx2 + ylog(2∏σ2).	(79)
The first term can be rewritten as (1∕2σ2) tδxImδx. Thus, Gx = (1∕2σ2) Im holds. Cx is derived
as the second term of the last equation in Eq.78.
H.3 Detailed explanation of KL divergence as a rate of entropy coding.
This appendix explains the detail how KL divergence can be interpreted as a rate in the transform
coding. In the transform coding, input data is transformed by an orthonormal transform. Then, the
transformed data is quantized, and an entropy code is assigned to the quantized symbol, such that
the length of the entropy code is equivalent to the logarithm of the estimated symbol probability.
It is generally intractable to derive the rate and distortion of individual symbols in the ideal infor-
mation coding. Thus, we first discuss the case of uniform quantization. Let Pzj and Rzj be the
probability and rate in the uniform quantization coding of Zj 〜 N(zj;0,1). Here, μj(x) and σj(x)2
are regarded as a quantized value and a coding noise after the uniform quantization, respectively. Let
T be a quantization step size. The coding noise after quantization is T2∕12 for the quantization step
size T, as explained in Appendix H.1. Thus, T is derived as T = 2√3σj(x) from σj(x)2 = T2/12.
We also assume σj(x)2《 1. As shown in Fig.27a, Pzj is denoted by R：j；X-T/p(zj)dzj where
p(zj) is N(zj; 0, 1). Using Simpson’s numerical integration method and ex = 1 + x + O(x2)
expansion, Pzj is approximated as:
Pzj ` ^6 (P(Mj(X)- 2) + 4p(μj(x))+p(μj(x) + 2))
Tp(μj(x)) (A	4μj(χ)τ-τ2	-4μj(χ)T-T2∖
= J C + e -8- + e -8-)
` Tp (μj(x))(1- T2/24)
=r∏σj(x) e-(μj(χ)2"2(1 - j~).
(80)
32
Under review as a conference paper at ICLR 2021
(a) Probability Pzj
(b) Approximation of Pzj
Figure 27: Probability for a symbol with mean μ and noise σ2
Using log(1 + X) = X + O(x2) expansion, Rμσ is derived as:
Rzj = - log Pzj ' 2 (μj(x)2 + σj(x)2 - log σj(x)2 - log ∏) = DKLj(X)G) + 2 log ɪ. (8I)
When Rzj and DKLj(x)(∙) in Eq. 2 are compared, both equations are equivalent except a small
constant difference 2 log(πe∕6) ` 0.176 for each dimension. As a result, KL divergence for j-th
dimension is equivalent to the rate for the uniform quantization coding, allowing a small constant
difference.
To make theoretical analysis easier, We use the simpler approximation as Pzj = T p(μj∙(x)) =
2√z3σj∙(x) p(μj∙(x)) instead of Eq.80, as shown in Fig.27b. Then, Rzj is derived as:
Rzj = - log(2√3 σj(x) p(μj(X))) = Eq. 6 + 1log πe.
26
(82)
This equation also means that the approximation of KL divergence in Eq. 6 is equivalent to the rate in
the uniform quantization coding with Pzj = 2χ∕3σj∙(x)p(μj∙(x)) approximation, allowing the same
small constant difference as in Eq. 81. It is noted that the approximation Pzj = 2χ∕3σj∙(x)p(μj∙(x))
in Figure 27b can be applied to any kinds of prior PDFs because there is no explicit assumption for
the prior PDF. This implies that the theoretical discussion after Eq. 6 in the main text will hold in
arbitrary prior PDFs.
Finally, the meaning of the small constant difference 1 log 管 in Eqs. 81 and 82 is shown. Pearlman
& Said (2011) explains that the difference of the rate between the ideal information coding and
uniform quantization is 1 log 管.This is caused by the entropy difference of the noise distributions.
In the ideal case, the noise distribution is known as a Gaussian. In the case the noise variance is
σ2, the entropy of the Gaussian noise is 1 log(σ22πe). For the uniform quantization with a uniform
noise distribution, the entropy is 1 log(σ212). As a result, the difference is just 1 log 管.Because
the rate estimation in this appendix uses a uniform quantization, the small offset 1 log 管 can be
regarded as a difference between the ideal information coding and the uniform quantization. As a
result, KL divergence in Eq. 2 and Eq. 6 can be regarded as a rate in the ideal informaton coding for
the symbol with the mean μj∙ (x)and variance σj∙ (x)2.
33