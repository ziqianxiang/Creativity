Under review as a conference paper at ICLR 2021
Monotonic Robust Policy Optimization
with Model Discrepancy
Anonymous authors
Paper under double-blind review
Ab stract
State-of-the-art deep reinforcement learning (DRL) algorithms tend to overfit in
some specific environments due to the lack of data diversity in training. To mit-
igate the model discrepancy between training and target (testing) environments,
domain randomization (DR) can generate plenty of environments with a sufficient
diversity by randomly sampling environment parameters in simulator. Though
standard DR using a uniform distribution improves the average performance on
the whole range of environments, the worst-case environment is usually neglected
without any performance guarantee. Since the average and worst-case perfor-
mance are equally important for the generalization in RL, in this paper, we propose
a policy optimization approach for concurrently improving the policy’s perfor-
mance in the average case (i.e., over all possible environments) and the worst-case
environment. We theoretically derive a lower bound for the worst-case perfor-
mance of a given policy over all environments. Guided by this lower bound, we
formulate an optimization problem which aims to optimize the policy and sam-
pling distribution together, such that the constrained expected performance of all
environments is maximized. We prove that the worst-case performance is mono-
tonically improved by iteratively solving this optimization problem. Based on the
proposed lower bound, we develop a practical algorithm, named monotonic robust
policy optimization (MRPO), and validate MRPO on several robot control tasks.
By modifying the environment parameters in simulation, we obtain environments
for the same task but with different transition dynamics for training and testing.
We demonstrate that MRPO can improve both the average and worst-case perfor-
mance in the training environments, and facilitate the learned policy with a better
generalization capability in unseen testing environments.
1 Introduction
With deep neural network approximation, deep reinforcement learning (DRL) has extended classical
reinforcement learning (RL) algorithms to successfully solving complex control tasks, e.g., playing
computer games with human-level performance (Mnih et al., 2013; Silver et al., 2018) and continu-
ous robotic control (Schulman et al., 2017). By random exploration, DRL often requires tremendous
amounts of data to train a reliable policy. It is thus infeasible for many tasks, such as robotic control
and autonomous driving, as training in the real world is not only time-consuming and expensive, but
also dangerous. Therefore, training is often conducted on a very limited set of samples, resulting
in overfitting and poor generalization capability. One alternative solution is to learn a policy in a
simulator (i.e., source/training environment) and then transfer it to the real world (i.e., target/testing
environment). Currently, it is impossible to model the exact environment and physics of the real
world. For instance, the physical effects like nonrigidity and fluid dynamics are quite difficult to be
accurately modeled by simulation. How to mitigate the model discrepancy between the training and
target environments remains challenging for the generalization in RL.
To simulate the dynamics of the environment, domain randomization (DR), a simple but effective
method is proposed. It randomizes the simulator (e.g., by randomizing the distribution of environ-
ment parameters) to generate a variety of environments for training the policy in the source domain.
Compared with training in a single environment, recent researches have shown that policies learned
through an ensemble of environment dynamics obtained by DR achieve better generalization per-
formance with respect to the expected return. The expected return is referred to as the average per-
1
Under review as a conference paper at ICLR 2021
formance across all the trajectories sampled from different environments. Since these trajectories,
regardless of their performance, are uniformly sampled, the trajectories with the worst performance
would severely degrade the overall performance.
In contrast, another line of research on the generalization in RL is from the perspective of control
theory, i.e., learning policies that are robust to environment perturbations. Robust RL algorithms
learn policies, also using model ensembles produced by perturbing the parameters of the nominal
model. EPOpt (Rajeswaran et al., 2017), a representative of them, trains policy solely on the worst
performing subset, i.e., trajectories with the worst α percentile of returns, while discarding all the
higher performing trajectories. In other words, it seeks a higher worst-case performance at the cost
of degradation on the average performance. In general, robust RL algorithms may sacrifice perfor-
mance on many environment variants and focus only on environments with the worst performance,
such that the policy learned will not behave very badly in a previously unseen environment.
In this paper, we focus on the generalization issue in RL, and aim to mitigate the model discrepancy
of the transition dynamics between the training and target environments. Considering that both the
average and worst-case performance are equally important for evaluating the generalization capabil-
ity of the policy, we propose a policy optimization approach in which the distribution of the sampled
trajectories are specifically designed for concurrently improving both the average and worst-case
performance. Our main contributions are summarized as follows.
•	For a given policy and a wide range of environments, we theoretically derive a lower bound for the
worst-case expected return of that policy over all the environments, and prove that maximizing this
lower bound (equivalent to maximizing the worst-case performance) can be achieved by solving
an average performance maximization problem, subject to constraints that bound the update step
in policy optimization and statistical distance between the worst and average case environments.
To the best of our knowledge, this theoretical analysis of the relationship between the worst-case
and average performance is reported for the first time, which provides a practical guidance for
updating policies towards both the worst-case and average performance maximization.
•	Trajectories obtained from diverse environments may contribute differently to the generalization
capacity of the policy. Therefore, in face of a huge amount of trajectories, the problem that
which types of trajectories are likely to mostly affect the generalization performance should be
considered. Unlike traditional uniform sampling without the worst-case performance guarantee,
and different from the worst α percentile sampling in which the parameter α is empirically preset,
we propose a criterion for the sampling trajectory selection based on the proposed worst-case and
average performance maximization, with which both the environment diversity and the worst-case
environments are taken into account.
•	Based on the proposed theorem, we develop a monotonic robust policy optimization (MRPO) al-
gorithm to learn the optimal policy with both the maximum worst-case and average performance.
Specifically, MRPO carries out a two-step optimization to update the policy and the distribution of
the sampled trajectories, respectively. We further prove that the policy optimization problem can
be transformed to trust region policy optimization (TRPO) (Schulman et al., 2015) on all possible
environments, such that the policy update can be implemented by the commonly used proximal
policy optimization (PPO) algorithm (Schulman et al., 2017). Finally, we prove that by updating
the policy with the MRPO, the worst-case expected return can be monotonically increased.
•	To greatly reduce the computational complexity, we impose Lipschitz continuity assumptions
on the transition dynamics and propose a practical implementation of MRPO. We then conduct
experiments on five robot control tasks with variable transition dynamics of environments, and
show that MRPO can improve both the average and worst-case performance in the training envi-
ronments compared to DR and Robust RL baselines, and significantly facilitate the learned policy
with a better generalization capability in unseen testing environments.
2 Background
Under the standard RL setting, the environment is modeled as a Markov decision process (MDP)
defined by a tuple < S, A, T, R >. S is the state space and A is the action space. For the con-
venience of derivation, we assume they are finite. T : S × A × S → [0, 1] is the transition
dynamics determined by the environment parameter p ∈ P , where P denotes the environment pa-
2
Under review as a conference paper at ICLR 2021
π(a∣s)
π(aIs)
η(π Ip) ≥ Ln (π Ip) 一 (ι2λY)2 β2,	Ln (nIp) = n(n|P) + Es 〜P∏(∙∣p),a 〜π(∙∣s)
rameter space. For example in robot control, environment parameter could be physical coefficient
that directly affect the control like friction of joints and torso mass. Throughout this paper, by en-
vironment p, we mean that an environment has the transition dynamics determined by parameter
p. R : S × A → R is the reward function. At each time step t, the agent observes the state
St ∈ S and takes an action at ∈ A guided by policy ∏(at∣St). Then, the agent will receive a reward
rt = R(st, at) and the environment shifts from current state st to the next state st+1 with probability
T (st+1 |st, at, p). The goal of RL is to search for a policy π that maximizes the expected cumula-
tive discounted reward η(π |p) = Eτ [G(τ |p)], G(τ |p) = Pt∞=0 γtrt. τ = {st, at,rt, st+1}t∞=0
denotes the trajectory generated by policy π in environment p and γ ∈ [0, 1] is the discount fac-
tor. We can then define the state value function as Vn (S) = E [P∞=o Ykrt+k∣st = s], the ac-
tion value function as Q∏(s, a) = E [P∞=0 γkrt+k |st = s, at = a], and the advantage function as
Aπ(S, a) = Qπ(S, a) - Vπ(S). We denote the state distribution under environment p and policy π
as P∏(s|p) and that at time step t as Pn (s|p). During the policy optimization in RL, by updating the
current policy ∏ to a new policy ∏, Schulman et al. (2015) prove that
An (s, a)
(1)
where λ = max§ |E。〜∏(a∣s)[A∏(s, a)]| is the maximum mean advantage following current policy
∏ and β = maxs DTV(∏(∙∣s)k∏(∙∣s)) is the maximum total variation (TV) distance between ∏ and
∏. The policy,s expected return after updating can be monotonically improved by maximizing the
lower bound in (1) w.r.t. ∏. Based on this and with certain approximation, Schulman et al. (2015)
then propose a algorithm named trust region policy optimization (TRPO) that optimizes ∏ towards
the direction of maximizing L∏ (π), subject to the trust region constraint β ≤ δ.
In standard RL, environment parameter p is fixed without any model discrepancy. While under the
domain randomization (DR) settings, because of the existence of model discrepancy, environment
parameter should actually be a random variable p following a probability distribution P over P .
By introducing DR, the goal of policy optimization is to maximize the mean expected cumulative
discounted reward over all possible environment parameters, i.e., max∏ EP〜P [η(∏∣p)].
In face of model discrepancy, our goal is to provide a performance improvement guarantee for the
worst-case environments, and meanwhile to improve the average performance over all environments.
Lemma 1. Guided by a certain policy π, there exists a non-negative constant C ≥ 0, such that the
expected cumulative discounted reward in environment with the worst-case performance satisfies:
n(n|Pw) - EP〜P [n(n|p)] ≥ -C,	⑵
where environment pw corresponds to the worst-case performance, and C is related to pw and π.
Proof. See Appendix A.1 for details.	□
Theorem 1. In MDPs where reward function is bounded, for any distribution P over P, by updating
the current policy π to a new policy π, the following bound holds:
YEP〜P [e(Pw l|p)]	4|r|maxα
η(π|Pw) ≥ EP〜P [n(n|P)] — 2IrImax-----(1 - γ)2-------(1 - γ)2 ,	(3)
where e(pw∣∣p) ， maxtEs，〜Pt(∙∣pw)Ea〜∏(∙∣so)Dτv(T(s∣s0,a,pw)∣T(s∣s0,a,p)), environment
pw corresponds to the worst-case performance under the current policy π, and α ,
maxt Es，〜Pt(∙∣pw )Dtv (∏(a∣s0 )k∏(a∣s0)).
Proof. See Appendix A.2 for details, and Appendix A.7 for bounded reward function condition. □
In (3), e(pw lp) specifies the model discrepancy between two environments pw andp in terms of the
maximum expected TV distance of their transition dynamics of all time steps in trajectory sampled in
environment pw using policy π, and α denotes the maximum expected TV distance of two policies
along trajectory sampled in environment pw using policy π. In general, the RHS of (3) provides
a lower-bound for the expected return achieved in the worst-case environment pw , where the first
term denotes the mean expected cumulative discounted reward over all environments following the
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Monotonic Robust Policy Optimization
1:	Initialize policy ∏o, uniform distribution of environment parameters U, number of environment
parameters sampled per iteration M, maximum number of iterations N and maximum episode
length T .
2:	for k = 0 to N - 1 do
3:	Sample a set of environment parameters {pi}iM=-0 1 according to U.
4:	for i = 0 to M - 1 do
5:	Sample L trajectories {τi,j }jL=-01 in environment pi using πk .
6:	Determine Pw = arg minpi∈{pi}M-ι PL=-I G(Tij ∣Pi)∕L.
7:	Compute E(Pi,∏k) = Pj=0 GL(Tijp - 2|,|节-；(对房) for environmentpi.
8:	end for
9:	Select the trajectory set T = {τi : E (Pi, πk) ≥ E (Pkw, πk)}.
10:	Use PPO for policy optimization on T to get the updated policy πk+1 .
11:	end for
sampling distribution P , while the other two terms can be considered as penalization on a large
TV distance between the worst-case environment Pw and the average case, and a large update step
from the current policy ∏ to the new policy ∏, respectively. Therefore, by maximizing this lower
bound, we can improve the worst-case performance, which in practice is equivalent to the following
constrained optimization problem with two constraints:
maxEP〜P [η(∏∣p)]	s.t. α ≤ δ1, EP〜P [e(pw∣∣p)] ≤ δ2.	(4)
∏,p
The optimization objective is to maximize the mean expected cumulative discounted reward over all
possible environments, by updating not only the policy ∏, but the environment parameter,s sampling
distribution P. The first constraint imposes a similar trust region to TRPO (Schulman et al., 2017)
that constrains the update step in policy optimization. In addition, we further propose a new trust
region constraint on the sampling distribution P that the TV distance between the worst-case envi-
ronment Pw and average case over P is bounded, such that by achieving the optimization objective
in (4), the worst-case performance is also improved.
To solve the constrained optimization problem in (4), We need to seek for the optimal policy ∏ and
the distribution P of the sampled trajectories. In practice, we carry out a two-step optimization
procedure to simplify the computational complexity. First, we fix the policy by letting ∏ = ∏, and
optimize the objective in (4) w.r.t. the distribution P. In this case, we no longer need to consider the
first constraint on the policy update, and thus can convert the second constraint on the sampling dis-
tribution into the objective with the guidance of Theorem 1, formulating the following unconstrained
optimization problem:
max Ep〜P [E(p, π)],	(5)
where we denote E(p,∏)，η(∏∣p) — 2"|%-*爪)
The first term in E(p, π) indicates policy
∏'s performance in environment p, while the second term measures the model discrepancy between
environment P and Pw . Since the objective function in (5) is linear to P, we can update P by
assigning a higher probability to environment P with higher E(p, ∏). As a consequence, sampling
according to E (p, ∏) would increase the sampling probability of environments with both poor and
good-enough performance, and avoid being trapped in the worst-case environment. Specifically, we
propose to select samples from environment P that meets E(p, ∏) ≥ E(pw, ∏) for the training of
policy ∏, which is equivalent to assigning a zero probability to the other samples.
In the second step, we target at optimizing the policy π with the updated distribution P fixed, i.e.,
the following optimization problem:
maxEP〜P [η(∏∣p)]	s.t. α ≤ δ1
π
(6)
Optimization in (6) can be transformed to a trust region robust policy optimization similar to TRPO
and solve it practically with PPO (refer to Appendix A.3 and Schulman et al. (2017) for more infor-
mation). To summarize, we propose a monotonic robust policy optimization (MRPO) in Algorithm1.
4
Under review as a conference paper at ICLR 2021
At each iteration k, we uniformly sample M environments and run a trajectory for each sampled en-
vironment. For each environment pi, we sample L trajectories {τi,j}jL=1, approximate η(πk |pi) with
PjL=-01 G(τi,j |pi)/L, and determine the worst-case environment pw based on PjL=-01 G(τi,j |pi)/L of
a given set of environments piiM=-0 1, We then optimize the policy with PPO on the selected trajectory
subset T according to E(pi, πk) and E(pkw, πk).
We now formally show that by maximizing the lower bound provided in Theorem1, the worst-case
performance within all the environments can be monotonically improved by MRPO.
Theorem 2. The sequence of policy {π1, π2, . . . , πN} generated by Algorithm1 is guaranteed with
the monotonic worst-case performance improvement, i.e.,
η(πι∖pW) ≤ n(n2|pW) ≤ …≤ η(πNIpN),	⑺
where pkw denotes the parameter of environment with the worst-case performance guided by the
current policy πk at iteration k.
Proof. See Appendix A.4 for details.	口
3	Practical Implementation using Simulator
Motivated by Theorem 1, we propose Algorithm 1 that provably promotes monotonic improvement
for the policy’s performance in the worst-case environment according to Theorem 2. However, The-
orem 1 imposes calculation of (pw kp) that requires the estimation of the expected total variation
distance between the worst-case environment and every other sampled environment at each time
step. Estimation by sampling takes exponential complexity. Besides, in the model-free setting, we
unfortunately have no access to analytical equation of the environment’s transition dynamics. Hence,
computation for the total variation distance between two environments is unavailable. Under the de-
terministic state transition, an environment will shift its state with a probability of one. Hence, we
can set state of one environment’s simulator to the state along the trajectory from the other environ-
ment, and take the same action. We then can compare the next state and compute the total variation
distance step by step. Though feasible, this method requires large computational consumption. In
this section, we propose instead a practical implementation for Algorithm 1.
According to Appendix A.9, we first make a strong assumption that the transition dynamics model
is Lp-Lipschitz in terms of the environment parameter p:
kT (s∖s0, a, p) - T(s∖s0,a,pw)k ≤ Lpkp -pwk.	(8)
Then, we can simplify the calculation of (pw kp) via:
e(PW Ilp) ≤ max Es0~Pt(∙∣pw,π)Ea~π(∙∣s0) 5 X ： Lpkp ― pw k = 5 X ： Lpkp - pw k .	(9)
t	22
ss
It can be seen from the expression of e(pw kp) that it measures the transition dynamics distance be-
tween the worst-case environment pw and a specific environment p. In simulator, the environment’s
transition dynamics would vary with the environment parameters, such as the friction and mass in
robot simulation. Hence the difference between environment parameters can reflect the distance be-
tween the transition dynamics. In addition, if we use in practice the penalty coefficient of e(pw kp)
as recommended by Theorem 1, the subset T would be very small. Therefore, we integrate it with
Lp as a tunable hyperparameter κ, and propose a practical version of MRPO in Algorithm 2 in
Appendix A.6.
4	Experiments
We now evaluate the proposed MRPO in five robot control benchmarks designed for evaluation
of generalization under changeable dynamics. These five environments are modified by the open-
source generalization benchmarks (Packer et al., 2018) based on the complex robot control tasks
introduced in (Schulman et al., 2017). We compare MRPO with two baselines, PPO-DR and PW-
DR, respectively. In PPO-DR, PPO is applied for the policy optimization in DR. In PW-DR, we use
5
Under review as a conference paper at ICLR 2021
Table 1: Range of parameters for each environment.
Environment	ParameterS	Training Range	TeSting Range
Walker2d/Hopper/HalfCheetah	Density Friction	[750, 1250] [0.5, 1.1]	[1, 750] [0.2, 0.5]
HalfcheetahBroadRange	Density	[750, 1250]	[1, 750]
	Friction	[0.2, 2.25]	[0.05, 0.2]
InvertedDoublePendulum	Pole1 length	[0.50, 1.00]	[1.00, 1.10]
	Pole2 length	[0.50, 1.00]	[1.00, 1.10]
Cartpole	Force magnitude	[1.00, 20.00]	10
	Pole length	[0.05, 1.00]	[1.00, 6.00]
	Pole mass	[0.01, 1.00]	[1.00, 6.00]
purely trajectories from the 10% worst-case environments for training and still apply PPO for the
policy optimization. Note that PW-DR is the implementation of EPOpt algorithm proposed in (Ra-
jeswaran et al., 2017) without the value function baseline. Since this value function baseline in
EPOpt is not proposed for the generalization improvement (in fact it could incur a performance im-
provement for all the policy optimization algorithms), we do not adopt it in PW-DR. We utilize two
64-unit hidden layers to construct the policy network and value function in PPO. For MRPO, we use
the practical implementation as described in Algorithm 2. Further note that during the experiments,
we find that environments generating poor performance would be far away from each other in terms
of the TV distance between their transition dynamics with the dimension of changeable parameters
increasing. In other words, a single worst-case environment usually may not represent all the en-
vironments where the current policy performs very poorly. Hence, we choose the 10% worst-case
environments to replace the calculation of E0(pW ,∏k) in Algorithm 2. That is, the trajectories We add
to the subset T should have the E 0(pi, πk) greater than and equal to those of all the 10% Worst-case
environments. From the expression of E0(pi,∏k) = PL=-o1 G(τi,j ∣Pi)∕L - Kkpi - Pw ∣∣ for environ-
ment pi , it can be seen that the trajectory selection is based on a trade-off betWeen the performance
and the distance to the Worst-case environment, as We described in detail in the paragraph under (5).
Our experiments are designed to investigate the folloWing tWo classes of questions:
•	Can MRPO effectively improve the policy’s Worst-case and average performance over the Whole
range of environments during training. Compared to DR, Will the policy’s average performance
degrade by using MRPO? Will MRPO outperform PW-DR in the Worst-case environments?
•	HoW does the performance of trained robust policy using MRPO degrade When employed in
environments With unseen dynamics? And What determines the generalization performance to
unseen environments during training?
4.1	Training Performance with Different Dynamics
The five robot control tasks are as folloWs. (1) Walker2d: control a 2D bipedal robot to Walk;
(2) Hopper: control a 2D one-legged robot to hop as fast as possible; (3) HalfCheetah: control
a 2D cheetah robot to run (Brockman et al., 2016); (4) InvertedDoublePendulum: control a cart
(attached to a tWo-link pendulum system by a joint) by applying a force to prevent the tWo-link
pendulum from falling over; and (5) Cartpole: control a cart (attached to a pole by a joint) by ap-
plying a force to prevent the pole from falling over. In robot control, the environment dynamics
are directly related to the value of some physical coefficients. For example, if sliding friction of the
joints is large, it Will be more difficult for the agent to manipulate the robot compared to a smaller
sliding friction. Hence, a policy that performs Well for a small friction may not by generalized to
the environment With a large friction due to the change of dynamics. In the simulator, by random-
izing certain environment parameters, We can obtain a set of environments With the same goal but
different dynamics (Packer et al., 2018). The range of parameters that We preset for training of each
environment is shoWn in Table 1.
We run the training process for the same number of iterations N sampled from preset range of en-
vironment parameters. At each iteration k, We generate trajectories from M = 100 environments
sampled according to a uniform distribution U. The results are obtained by running each algorithm
6
Under review as a conference paper at ICLR 2021
(a) WaIker2d
(b) Hopper
(c) HaIfCheetah
(g) InvertedDoubIePenduIum
(h) Cartpole
(i) HaIfCheetahBroadRange
(j) InvertedDoublePendulum
(k) Cartpole
Figure 1: Training Curves of average return and 10% worst-Case return.
(l) HalfCheetahBroadRange
Table 2: Average and worst-Case performanCe in training, where W, H, C, CP, I and CB refer to
Walker2d, Hopper, HalfCheetah, Cartpole, InvertedDoublePendulum and HalfCheetahBroadRange.
Algorithm	Average W	Worst W	Average H	Worst H	Average C	Worst C
MRPO DR PW-DR	1930.0 ± 66.7 2170.0 ± 57.9 372.0 ± 291.2	1702.5 ± 185.9 1772.5 ± 162.2 273.0 ± 230.0	2257.4 ± 423.5 2250.0 ± 80.4 1903.3 ± 49.2	2048.0 ± 246.8 1254.0 ± 348.5 1440.0 ± 243.9	2537.0 ± 98.3 2970.0 ± 30.0 1810.0 ± 41.2	2340.0 ± 61.2 2512.5 ± 120.9 1677.5 ± 72.3
Algorithm	Average I	Worst I	Average CP	Worst CP	Average CB	Worst CB
MRPO DR PW-DR	9185.2 ± 165.6 8252.6 ± 439.8 9222.2 ± 173.3	5137.0 ± 1421.1 1354.5 ± 381.2 5450.4 ± 1687.3	998.7 ± 1.7 995.5 ± 5.6 895.4 ± 147.2	988.4 ± 15.4 961.6 ± 48.3 791.4 ± 288.9	2363.3 ± 82.6 2646.7 ± 303.5 1456.7 ± 301.3	2166.7 ± 70.4 1883.3 ± 103.7 1278.7 ± 215.9
with five different random seeds. The average return is Computed over the returns of M sampled en-
vironments at eaCh iteration k. We show the training Curves of Walker2d, Hopper and HalfCheetah,
InvertedDoublePendulum, Cartpole, and HalfCheetahBroadRange, respeCtively, in Figs. 1(a)-1(C)
and 1(g)-1(i). In Figure 1, the solid Curve is used to represent the average performanCe of eaCh algo-
rithm on all the five seeds, while the shaded-area denotes the standard error. It is seen that DR Can
steadily improve the average performanCe on the whole training range as expeCted, while MRPO
does not signifiCantly degrade the average performanCe in all the the tasks. PW-DR, on the other
hand, foCuses on the worst-Case performanCe optimization, leading to an obvious degradation of av-
7
Under review as a conference paper at ICLR 2021
(a) MRPO
583.6
666.8
750.0
(b) PWDR
friction
1.0
84.2
167.4
0.20 0.23 0.27 0.30 0.33 0.37 0.40 0.43 0.47 0.50
friction
(a) MRPO
84.2
167.4
583.6
666.8
750.0
(a) Walker2d
(b) PWDR
friction
417.1
500.3
666.8
750.0
0.05
Aa-Suep
■
0.10 0.12 0.13 0.15 0.17 0.18 0.20
friction
(b) Hopper
(b)PWDR
1.0
84.2
167.4
250.7
417
500.3
583.6
666.8
750.0
ω 333,
0.05 0.07 0.08 0.10 0.12 0.13 0.15 0.17 0.18 0.20
friction
(c) HalfcheetahBroadRange
Figure 2: Heatmap of return in unseen environments on Waler2d, Hopper and Halfcheetah with
policies trained by MRPO, PW-DR and DR in the training environments.
erage performance on Hopper and Halfcheetah and failure on Walker2d. We measure the worst-case
performance by computing the worst 10% performance in all the sampled environments at iteration
k and the corresponding training curves are illustrated in Figs. 1(d)-1(f) and 1(j)-1(l), respectively.
It can be observed that MRPO presents the best worst-case performance on Hopper, Cartpole and
HalfcheetahBroadRange, while DR neglects the optimization on its worst-case performance and
performs badly in these three tasks. PW-DR shows limited improvement of the worst-case perfor-
mance compared to MRPO on Hopper, Halfcheetah, Cartphole and HalfcheetahBroadRange, and
failure on Walker2d. Comparing Figs. 1(f) and 1(l), it can be concluded that the original parameter
range we set for the Halfcheetah task (e.g., the friction range of [0.5, 1.1]) was too narrow to cause
seriously poor performance on the 10% worst-case environments. By enlarging the friction range
from [0.5, 1.1] to [0.2, 2.5] for HalfcheetahBroadRange, the training curves in Fig. 1(l) can clearly
demonstrate that MRPO outperforms the other baselines. The tabular comparison of the average
and worst-case performance achieved during training by different algorithms in different tasks can
be found in Table 2.
4.2	generalization to unseen environments
MRPO has been demonstrated in theoretical analysis to optimize both average and worst-case per-
formance during training. Here, we carry out experiments to show that MRPO can generalize to a
broader range of unseen environments in testing. To this end, we compare the testing performance
on some unseen environments of Walker2d, Hopper and HalfcheetahBroadRange with the best poli-
cies obtained by MRPO, DR and PW-DR from training, with the range of parameters set for testing
showin in Table 1. It is observed that policies all degrade with the decrease of friction, while the
impact of unseen density is not that obvious as the friction. The heatmap of return achieved in all the
testing Hopper environments by each algorithm is shown and compared in Fig. 2. It can be seen that
8
Under review as a conference paper at ICLR 2021
MRPO has better generalization ability to the unseen environments, while DR can hardly generalize
in testing. Compared to PW-DR, MRPO has a broader generalization range, from which We remark
that both the worst-case and average performance during training are crucial for the generalization
to an unseen environment.
5	Related Work
With the success of RL in recent years, plenty of works have focused on how to improve the general-
ization ability for RL. Learning a policy that is robust to the worst-case environment is one strategy.
Based on theory of H∞ control (Zhou et al., 1996), robust RL takes into account the disturbance
of environment parameters and model it as an adversary that is able to disturb transition dynamics
in order to prevent the agent from achieving higher rewards (Morimoto & Doya, 2005). The policy
optimization is then formulated as a zero-sum game between the adversary and the RL agent. Pinto
et al. (2017) incorporate robust RL to DRL method, which improves robustness of DRL in complex
robot control tasks. To solve robust RL problem, robust dynamic programming formulates a ro-
bust value function and proposes accordingly a robust Bellman operator (Iyengar, 2005; Mankowitz
et al., 2020). The optimal robust policy can then be achieved by iteratively applying the robust
Bellman operator in a similar way to the standard value iteration (Sutton & Barto, 2018). Besides,
Rajeswaran et al. (2017) leverage data from the worst-case environments as adversarial samples to
train a robust policy. However, the aforementioned robust formulations will lead to an unstable
learning. What’s worse, the overall improvement of the average performance over the whole range
of environments will also be stumbled by their focus on the worst-case environments. In contrast, in
addition to the worst-case formulation, we also aim to improve the average performance.
For generalization across different state spaces, an effective way is domain adaptation, which maps
different state space to a common embedding space. The policy trained on this common space can
then be easily generalized to a specific environment (Higgins et al., 2017b; James et al., 2019; Am-
mar et al., 2015) through a learned mapping, with certain mapping methods, such as β-VAE (Higgins
et al., 2017a), cGAN (Isola et al., 2017), and manifold alignment (Wang & Mahadevan, 2009).
Function approximation enables RL to solve complex tasks with high-dimensional state and action
spaces, which also incurs inherent generalization issue under supervised learning. Deep neural net-
work (DNN) suffers overfitting due to the distribution discrepancy between training and testing sets.
l2-regularization, dropout and dataset augmentation (Goodfellow et al., 2016) play an significant role
for generalization in deep learning, which have also enabled improvement of policy’s generalization
on some specifically designed environments (Cobbe et al., 2019; Farebrother et al., 2018).
In terms of the theoretical analysis, Murphy (2005) provide a generalization error bound for Q-
learning, where by generalization error they mean the distance between expected discounted reward
achieved by converged Q-learning policy and the optimal policy. Wang et al. (2019) analyze the
generalization gap in reparameterizable RL limited to the Lipschitz assumptions on transition dy-
namics, policy and reward function. For monotonic policy optimization in RL, Schulman et al.
(2015) propose to optimize a constrained surrogate objective, which can guarantee the performance
improvement of updated policy. In the context of model-based RL, Janner et al. (2019); Luo et al.
(2019) formulate the lower bound for a certain policy’s performance on true environment in terms of
the performance on the learned model. It can therefore monotonically improve the performance on
true environment by maximizing this lower bound. Different from this, the proposed MRPO in this
work can guarantee the robustness of the policy in terms of the monotonically increased worst-case
performance, and also improve the average performance.
6	Conclusion
In this paper, we have proposed a robust policy optimization approach, named MRPO, for improv-
ing both the average and worst-case performance of policies. Specifically, we theoretically derived a
lower bound for the worst-case performance of a given policy over all environments, and formulated
an optimization problem to optimize the policy and sampling distribution together, subject to con-
straints that bounded the update step in policy optimization and statistical distance between the worst
and average case environments. We proved that the worst-case performance was monotonically im-
proved by iteratively solving this optimization problem. We have validated MRPO on several robot
control tasks, demonstrating a performance improvement on both the worst and average case envi-
ronments, as well as a better generalization ability to a wide range of unseen environments.
9
Under review as a conference paper at ICLR 2021
References
Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E Taylor. Unsupervised cross-domain
transfer in policy gradient reinforcement learning via manifold alignment. In Twenty-Ninth AAAI
Conference on Artificial Intelligence. Citeseer, 2015.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural lyapunov control. In Advances in Neural
Information Processing Systems,pp. 3245-3254, 2019.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generaliza-
tion in reinforcement learning. In International Conference on Machine Learning, pp. 1282-1289.
PMLR, 2019.
Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in
dqn. arXiv preprint arXiv:1810.00123, 2018.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2017a.
Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot trans-
fer in reinforcement learning. arXiv preprint arXiv:1707.08475, 2017b.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257-280, 2005.
S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, and
K Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-
canonical adaptation networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 12619-12629, 2019.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems, pp. 12519-
12530, 2019.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations, 2019.
Daniel J Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg,
Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy Mann, and Martin Riedmiller. Robust reinforce-
ment learning for continuous control with model misspecification. In International Conference
on Learning Representations, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2):335-359,
2005.
Susan A Murphy. A generalization error for q-learning. Journal of Machine Learning Research, 6
(Jul):1073-1097, 2005.
10
Under review as a conference paper at ICLR 2021
Charles Packer, Katelyn Gao, Jernej Kos, PhiliPP Krahenbuhl, Vladlen Koltun, and DaWn Song.
Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav GuPta. Robust adversarial reinforce-
ment learning. In International Conference on Machine Learning, pp. 2817-2826, 2017.
Aravind RajesWaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. EPoPt: Learning
robust neural netWork policies using model ensembles. 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla DhariWal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Thomas Hubert, Julian SchrittWieser, Ioannis Antonoglou, MattheW Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Richard S Sutton and AndreW G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Chang Wang and Sridhar Mahadevan. Manifold alignment Without correspondence. In IJCAI,
volume 2, pp. 3. Citeseer, 2009.
Huan Wang, Stephan Zheng, Caiming Xiong, and Richard Socher. On the generalization gap in
reparameterizable reinforcement learning. In International Conference on Machine Learning, pp.
6648-6658, 2019.
Kemin Zhou, John Comstock Doyle, Keith Glover, et al. Robust and optimal control, volume 40.
Prentice hall NeW Jersey, 1996.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Lemma 1
Proof. First, We define η(∏∣Pw) - maxp∈p η(∏∣p)，-Co, where Co ≥ 0 depends on ∏ and P.
Then, given a policy π and any environment p ∈ P, and for any non-negative constant C ≥ C0 ≥ 0,
we thus have:
η(∏∣Pw) - Ep~P [η(∏∣p)] ≥ η(∏∣Pw) - maxη(∏∣p) = -Co ≥ -C.	(10)
p∈P
□
A.2 Proof of Theorem 1
Lemma 2. For any two joint distribution P1(x, y) = P1 (x)P1 (y|x) and P2(x, y) = P2(x)P2(y|x)
over x and y, we can bound the total variation distance of them by:
DTV(P1(x,y)kP2(x,y)) ≤ DTV (P1(x)|P2(x)) + maxDTV(P1(y|x)kP2(y|x))	(11)
x
Proof.
DTV (P1(χ,y)kP2(χ,y)) =1 X ∣Pι(χ,y) - P2(χ,y)l	(12)
x,y
=2 X ∣P1(χ)P1(y∣χ) - P2(χ)P2(y∣χ)l	(13)
x,y
=1 X ∣P1(χ)P1(y∣χ) - P1(χ)P2(y∣χ) + P1(χ)P2(y∣χ) - P2(χ)P2(y∣χ)l
x,y
(14)
≤2 XPI(X)|Pi(y|x) - P2(y|x)| + 2 X IPI(X)- P2(x)|	(15)
=Eχ~PιDτv (P1(y∣x)kP2(y∣x)) + DTV (PI(X)∣P2(x))	(16)
□
Lemma 3. Suppose the initial state distributions P1o(s) and P2o(s) are the same. Then the distance
in the state marginal at time step t is bounded as:
DTV(Pt(S)kPf(s)) ≤ tmaxEso^ptDtv(Pi(s∣s0)kP2(s∣s0))	(17)
Proof.
|P1t(s)-P2t(s)|=|XP1(st=s|s0)P1t-1(s0)-XP2(st=s|s0)P2t-1(s0)|	(18)
s0	s0
≤ X |Pi(St = s∣s0)Pt-i(s0) - P2(st = s∣s0)P2-l(s0)I	(19)
s0
=X|P1(st=s|s0)P1t-1(s0)-P2(st=s|s0)P1t-1(s0)	(20)
s0
+P2(st=s|s0)P1t-1(s0) - P2(st = s|s0)P2t-1(s0)|	(21)
≤Eso~Pt-ι∣Pι(s∣s0)- P2(s∣s0)∣ + ]ΓP2(s∣s0)∣Pt-1 (s')- P尸(s0)∣	(22)
s0
12
Under review as a conference paper at ICLR 2021
DTV(Pt(S)kP^(s)) ≤ 1 X ∣Pt(s) - P2(s)l	(23)
s
≤ 1X(Es0 〜p1t-1∣P1(s∣s0) - P2(s∣s0)∣ + X P2(s∣s0)∣Pt-1(s0) - Pt-1(s0)∣
s	s0
(24)
=EsO 〜Pt-1 DTV (P1(s∣s0)kP2(s∣s0)) + DTV (PtT(S0)kP厂1(s0))	(25)
t
≤ X EsO〜Pi-1 DTV (P1(s∣s0)kP2(s∣s0))	(26)
i=1
≤t max EsO 〜PtDTV(P1(s∣s0)∣∣P2(s∣s0))	(27)
□
Theorem 3. (A modified version of Theorem 1.) For any distribution P over P, by updating the
current policy π to a new policy π, the following bound holds:
,-.p^ γ w	1、	γep~p [e(Pwl∣p)]	4|r|maxα
n(n|pw) — EP〜P [n(n|P)] ≥ -2IrImax —@ - 寸---------------@ - 炉,	(28)
where E(PwIlP)，maxt E§，〜Pt(∙∣pw)Ea〜∏(∙F)Dtv(T(s∣s0, a,Pw)∣∣T(s∣s0, a,p)), environment Pw
CorresPonds to the worst-case performance, and ɑ，maxt E§o〜Pt(.∣pw)Dtv(π(a∣s0) ∣∏(a∣s0)).
Proof. We can rewrite the LHS of (2) aS:
η(∏∣Pw) - EP〜P [η(∏∣P)] = η(∏∣Pw) - η(∏∣Pw) + η(∏∣Pw) - EP〜P [η(∏∣P)].
For the laSt two term, we have
EP〜P|n(n|Pw) - n(n|P)| =EP〜P| XYt X(Pt(S, a|Pw) - Pt(S,a|P)R(s, a))|
t s,a
≤ep〜P X γt X IPt(S, a|Pw) - Pt(S, a|P)||R(s, a)|
=2|r|max〉： YtEP~P [DTV(Pt(S, a|Pw)IIPt(s, a|P))] ,	(29)
t
where Pt(s, a∣Pw) = π(a∣s)Pt(s∣Pw) and Pt(s, a∣P) = π(a∣s)Pt(s∣P). ByLemma 2, we have
EP〜P [Dtv(Pt(s,a∣Pw)∣Pt(s,a∣P))]
≤Es〜Ptap®)Dtv(∏(a∣s)k∏(a∣s))+ EP〜P [Dtv(Pt(SIPw)∣Pt(s∣P))] .	(30)
Note that
P(SIS0,Pw) = X T(SIS0, a,Pw)π(aIS0)	(31)
a
P (s∣s0, P) = X T (s∣s0,a, p)∏(a∣s0).	(32)
a
Similar to Lemma 2, we have
DTV(P(SIS0,Pw)IP(SIS0,P))	(33)
=2 XX∣T(s∣s0, a,Pw)∏(a∣s0) -T(s∣s0,a, p)∏(a∣s0)∣	(34)
≤ 2 XX IT(SIS0, a,Pw) - T (SIS0, a, P)Iπ(aIS0) + 2 XXT(s∣s0, a, p)∣π(a∣s0) - ∏(a∣s0)∣
s a	s a	(35)
=Ea 〜∏(∙F)Dτv (T(SIS0,a,Pw )∣T (s∣s0,a, p)) + DTV (∏(a∣s0)k∏(a∣s0))	(36)
13
Under review as a conference paper at ICLR 2021
By Lemma 3, we have
Ep~p [Dtv(Pt(s∣Pw)kPt(s∣p))]
≤tEp~P maxEso~pt(.∣pw)Dtv(P(s∣s0,Pw)∣∣P(s∣s0,p))
≤tEp~P maxEso~pt(.∣pw)Ea~∏(∙F)Dτv(T(s∣s0,a,pw)∣∣T(s∣s0, a,p))
+1 max Es，~pt(.|pw )Dtv (∏(a∣s0)k∏(a∣s0))	(37)
Since E(PIlpw) = maxtEs，~pt(∙∣pw)Ea~∏(.∣sθ)Dτv(T(s∣s0,a,pw)kT(s∣s0,a,p)) and α =
maxt Es'~pt(∙∣pw)Dtv(∏(a∣s0)∣∏(a∣s0)), combining (29), (30) and (37), and referring to Jensen's
inequality, we have
|n(n|pw) - Ep~pη(πIp)I
≤Ep~P∣η(∏∣pw) - η(∏|p)|
≤2IrI
max X γtEp~P h(t + 1) maxEs0~pt(∙∣pw)Dtv(π(a∣s0)∣∏(a∣s0))
t
+t max Es0~p t(∙∣pw)Ea~∏(∙∣s0)Dτv (T (s∣s0,a,Pw )∣∣T (s∣s0,a,p))j
=2IrI
max	γt [t(Ep~p k(pw llp)] + α) + α]
t
=2IrI
max
γEp~P [e(pw kp)]
―Lp — +
(38)
α
O-τF -
With policy ∏ be updated to ∏, η(∏∣pw) ≤ Ep~p [η(∏|p)]. Then,
/ I 、 Ir -Li	Dll	「Yep~p Hpwkp)] l α -
η(πlpw) — Ep~P [n(n|P)] ≥ -2|r|max [-----(1 - γ)2----- +(1 - γ)2
Similar to the derivation of (38) and refer to Janner et al. (2019), we have
n(n|pw) — n(n|pw) ≥ 一丁"；：.
(1 - γ)2
Combining the above results, we end up with the proof
γ-E	γ 1o 「Li 八 OI I	Y ep~p [e(pw kp)]	4|r|maxα
η(πlpw) — Ep~P [n(n|p)] ≥ -2|r|max-------(1 - γ)2-----(1 - γ)2 .
(39)
(40)
(41)
□
A.3 Derivation of Policy Optimization Step
In the policy optimization step, we aim to solve the following optimization problem:
maxEp~ρ [η(∏∣p)]	s.t. α ≤ δι.	(42)
π
Referring to (1), we have:
ep~P [n(n|p)] ≥ EP~P [L∏ (n|p)]-C 2λγ∖2 β2	(43)
(1 — Y)2
We now turn to optimize the RHS of (43) to maximize the objective in (6) under constraints α ≤ δ1:
max Ep~ρ [L∏ (π |p)]—
π
2λγ β2
(1 - Y)2 β
s.t. α ≤ δ1.
(44)
Note that we have:
α = maxEsθ~pt(.∣p )Dtv(π(a∣s0)k∏(a∣s0)) ≤ maxDTV(∏(a∣s0)k∏(a∣s0)) = β. (45)
t	w	s0
Following the approximation in Schulman et al. (2015), (44) can be equivalently transformed to:
Ep~p Es~p∏ (∙∣p),a~∏(∙∣s)
∏⅞)An(s,a)]]	s.t.β≤δ,
(46)
which can be solved using PPO (Schulman et al., 2017).
14
Under review as a conference paper at ICLR 2021
A.4 Proof of Theorem 2
Proof. Denote H(πkk∏k+1) ，maxt旧§，〜pt(∙∣pw)Dtv(∏k(a∣s0)k∏k+1(a∣s0)). Updating ∏k to
πk+1 at each iteration k and following Theorem 1, we have
η(∏k+ι∣pW) ≥ Ep〜Pk+1 [η(∏k+ι∣p) - 2lrlmax叫成)]-4lrmxH"2πk+1).	(47)
w	(1 - γ)2	(1 - γ)2
Since Pk+1 and πk+1 are obtained by maximizing the RHS of (3), we have
EP 〜P k + 1	η(πk+1 |p)	2|r|maxYe(Pkpwy (1-γ )2	]		-	4|r| maxH(∏k k∏k+l)	(48)
					(1 - γ )2	.	
≥EP 〜P k+1	η(πk |p) -	2rmaxYe(Pkpw) - (i-γ)2	.	-	4|r	|maxH(∏kk∏k) (1-y)2	(49)
=EP 〜P k+1	η(πk |p) -	2rmaxYe(Pkpw) ^ (1 - Y)2	.				(50)
From Line 9 in Algorithm 1, the environment selected for training satisfies:
η(∏k|p) - 21mlpW) ≥ η(∏k∣pW) — ⅜(1γ-(PwkpW) = η(∏k∣pW).	(51)
Therefore, combining (47)-(51), we have:
n(nk+i|pW+1) ≈ n(nk+i|pW) ≥ EP〜Pk+1 [η(πk|pW)] = η(πk∖p>W).	(52)
where the approximation is made under the assumption that the expected returns of worst-case
environment between two iterations are similar, which stems from the trust region constraint we
impose on the update step between current and new policies, and can also be validated from experi-
ments.	□
A.5 Empirical Verification of Assumption in Theorem 2
To verify the assumption made in Theorem 2, in Fig. 3, we study how the parameters of environ-
ments with poor performance scatter in the parameter space with different dimensions. Specifically,
we plot the heatmap of return for the range of Hopper environments used for training, achieved by
using MRPO to update the policy between two iterations. It can be validated that at the iteration
k = 300, the poorly performing environments of the two policies before and after the MRPO update
concentrate in the same region, i.e., the area of small frictions. The same result can be observed for
the iteration k = 350.
For example, as shown in Figs. 3(a) and 3(b), at iteration k = 300, p3w00 = (750, 0.5), the MC
estimation of η(π300 |p3w00) is 487.6 and that of η(π301 |p3w00) is 532.0. At iteration k = 301, p3w01 =
(1027.8, 0.5) and the MC estimation of η(π301 |p3w01) is 517.6. As shown in Figs. 3(c) and 3(d),
at iteration k = 350, p3w50 = (861.1, 0.5), the MC estimation of η(π350 |p3w50) is 385.9 and that
of η(π351 |p3w50) is 422.2. At iteration k = 351, p3w51 = (750, 0.5) and the MC estimation of
η(π351 |p3w51) is 394.0. In both cases, the empirical results can support the assumption that we made
in (52), i.e., the expected returns of worst-case environment between two iterations are similar.
A.6 Practical Implementation of MRPO
It can be seen from the expression of (pw kp) that it measures the transition dynamics distance be-
tween the worst-case environment pw and a specific environment p. In simulator, the environment’s
transition dynamics would vary with the environment parameters, such as the friction and mass in
robot simulation. Hence the difference between environment parameters can reflect the distance be-
tween the transition dynamics. In addition, if we use in practice the penalty coefficient of (pw kp)
as recommended by Theorem 1, the subset T would be very small. Therefore, we integrate it with
Lp in (9) as a tunable hyperparameter κ, and propose a practical version of MRPO in Algorithm 2.
15
Under review as a conference paper at ICLR 2021
Figure 3:
750.0 ■
805.6 -
861.1
916.7 -
972.2
1027.8 -
1083.3 -
1138.9 ■
1194.4 ■
1250.0 -
3 一 suαjp
(a) iteration k=300
■"SUSP
(b) iteration k=301
750.0 ■
805.6 -
861.1
916.7 -
972.2
1027.8 -
1083.3 -
1138.9 ■
1194.4
1250.0 -
3 一 suαjp
0.5 0.6 0.6 0.7 0.8 0.8 0.9 1.0 1.0 1.1
friction
(c) iteration k=350
0.5 0.6 0.6 0.7 0.8 0.8 0.9 1.0 1.0 1.1
friction
(d) iteration k=351
of return between
MRPO on Hopper.
at iterations k = 300 and k = 350, using

Algorithm 2 Practical Inplementation of Monotonic Robust Policy Optimization
1:	Initialize policy π0, uniform distribution of environment parameters U, number of environment
parameters sampled per iteration M , maximum number of iterations N and maximum episode
length T .
2:	for k = 0 to N - 1 do
3:	Sample a set of environment parameters {pi}iM=-0 1 according to U.
4:	for i = 0 to M - 1 do
5:	Sample L trajectories {τi,j}jL=-01 in environment pi using πk.
6:	Determine pkw = arg minp ∈{p }M-1 PL-1 G(Ti,j∣Pi)∕L.
7:	Compute E0(pi,∏k) = Pj=0 GLTy|p" - Kkpi - Pw∣∣ for environment pi.
8:	end for
9:	Select trajectory setT = {τi : E 0(pi,πk) ≥ E 0(pkw,πk)}.
10:	Use PPO for policy optimization on T to get the updated policy πk+1.
11:	end for
A.7 B ounded Reward Function Condition in Robot Control Tasks
In Theorem 1, we state the condition that reward function is bounded. Referring to the source code of
OpenAI gym (Brockman et al., 2016), the reward function for the five robot control tasks evaluated
in this paper are listed below.
Hopper and Walker2d:
R = xt+1 - xt + b - 0.001|at|2;
Halfcheetah:
R = xt+1 - xt - 0.001|at | ;
16
Under review as a conference paper at ICLR 2021
I V I
2 11
EnIəj°6巴°>0
O
O 250	500	750 IOOO 1250 1500 1750
iterations
(a) Performance
6 5 4 3 2 1
SQO"posd-°oE4
250	500	750 IOOO 1250 1500 1750
iterations
(b) Time elapsed
Figure 4: (a) Training curves of average return of MRPO on Hopper with different L; (b) Time
elapsed versus number of iterations curves during training.
Cartpole:
R = 1, if the pole does not fall down;
InvertedDoublePendulum:
R = b - cdist - cvel.
In Hopper, Walker2d and Halfcheetah, xt+1 and xt denote the positions of the robot at timestep
t + 1 and t, respectively. For Hopper and Walker2d, b ∈ {0, 1}, and b equals 0 when the robot falls
down or 1 otherwise. The squared norm of action represents the energy cost of the system. Since the
maximum distance that the robot can move in one timestep and the energy cost by taking an action
at each timestep are bounded, these three tasks all have the bounded reward function. In Cartpole,
the reward is always 1. In InvertedDoublePendulum, b equals 0 when the pendulum falls down or
10 otherwise, cdist is the distance between the robot and the centre, and cvel is the weighted sum of
the two pendulum’s angular velocities. Since all the three parameters b, cdist and cvel are physically
bounded, the reward function, as a linear combination of them, is also bounded.
A.8 Analysis of THE MONTE Carlo Estimation of η(∏∣p)
In Theorem 1, the worst-case environment parameter pw needs to be selected according to the ex-
Pected cumulative discounted reward η(∏∣p) of environment p. However, η(∏∣p) is infeasible to
get in the practical implementation. Therefore, as a commonly used alternative approach as in (Ra-
jeswaran et al., 2017), we use the mean of the cumulative discounted reward of L samPled trajec-
tories PL=-o1 G(Tij ∖pi)/L to approximate the expectation η(∏∖pi) = ET [G(τ|p.] of any environ-
ment pi , by using Monte Carlo method. We then determined the worst-case environment pw based
on pL=-01 G(Tij ∖pi)∕L ofa given set of environments PiM=-1∙ In the following, We will analyze the
impact of L on the estimation error.
Theoretical analysis of the impact ofL: Referring to Chebyshev’s inequality, for any environment
Pi and any ε ≥ 0, with probability of at least 1 一 釜,We have
PL-IG(Tij ∖Pi)	PL- Eτi,j [G(Ti,j∖Pi)]
L	L
where σ = V ar(G(T)∖Pi) is the variance of trajectory T’s return. From the above equation, we
find out that the variance of the return does affect the MC estimation of η(π∖P) and a larger L can
guarantee a higher probability for the convergence of PLZc1 GEj ∖Pi)/L to η(∏∖pi).
Empirical evaluation of the impact of L: in practice, we conduct experiment of MRPo on Hop-
per with different choices of L. We find out that the a larger L would not greatly affect the perfor-
mance in terms of average return as shown in Fig. 4(a), but will significantly increase the training
G(Tij ∖Pi)
~L
一 η(π∖Pi) ≤ ε, (53)
17
Under review as a conference paper at ICLR 2021
(a) Average return
(b) 10% worst-case return
Figure 5: (a) Training curves of (a) average return and (b) 10% worst-case return of MRPO on
Hopper with different κ.
time as shown in Fig. 4(b). In other words, for the same number of training iterations, a larger L
would consume significantly longer running time than a smaller L, while the performance is similar.
Therefore, we set L = 1 in our practical implementation of MRPO to strike a trade-off between the
approximation accuracy and time complexity in training.
A.9 Analysis of the Lipschitz Assumption
In robot control tasks, classical optimal control methods commonly utilize the differential equation
to formulate the dynamic model, which then indicates that the transition dynamics model is Lp -
Lipschitz and this formulated dynamic function can be used to estimate the Lipschitz constant Lp .
For example, the inverted double pendulum, one of our test environments, can be viewed as a two-
link pendulum system (Chang et al., 2019). To simplify the analysis, we illustrate here a single
inverted pendulum, which is the basic unit that forms the inverted double pendulum system. The
single inverted pendulum has two state variables θ and θ, and one control input u, where θ and θ
represent the angular position from the inverted position and the angular velocity, respectively, and
u is the torque. The system dynamics can therefore be described as
- mgl Sin θ + u — 0.1θ
ml2
(54)
where m is the mass, g is the Gravitational acceleration, and l is the length of pendulum. In our set-
ting, we may choose m as the variable environment parameter p. Since the above system dynamics
are differentiable w.r.t. m, it can be verified that the maximum value of the first derivative of the
system dynamic model can be chosen as the Lipschitz constant Lp .
A.10 HYPERPARAMETER κ
In Algorithm 2, when we update the sampling distribution P for policy optimization, κ is a hyperpa-
rameter that controls the trade-off between the expected cumulative discounted reward η(∏k |pi) and
distance kpi — pkw k to the worst-case environment. Theoretically, a larger κ means that the policy
cares more about the poorly-performing environments, while a smaller κ would par more attention
to the average performance. As empirical evaluation, we conduct experiment of MRPO on Hopper
with different choices of hyperparameter κ. The training curves of both average return and the 10%
worst-case return are shown in Figs. 5(a) and 5(b), respectively. It can be verified that for the fixed
value choice of κ, the curve of κ = 5 outperforms the curves of κ = 20, 40, 60 in terms of the aver-
age return in Fig. 5(a), while the curve of κ = 60 outperforms the curves of κ = 5, 20, 40 in terms
of the 10% worst-case return in Fig. 5(b). In practical implementation, we gradually increase κ to
18
Under review as a conference paper at ICLR 2021
(a) MRPo
1.0
Z 工 工 4 4
SSeIJU ①-Od
z3∙3∙
SSeIJU φ-
1.00 1.56 2.11 2.67 3.22 3.78 4.33 4.89 5.44
pole length
(a) Cartpole
(b) PWDR
2 3 3 4
SSeIJU ①-Od
rlOOO
∣800
600
400
200
pole length
,0
5.4
6.0
1.56 2.11 2.67 3.22^^^^3 4.89 5.44 6X)0
pole length
(a) MRPO
ωl21⅛1uω1-
1.00 1.01 1.02 1.03 1.04 1.06 1.07 1.08 1.09 1.10
Iengthls
In 12 1⅛1uω 1-
1.00 1.01 1.02 1.03 1.04 1.06 1.07 1.08 1.09 1.10
Iengthls
(b) InvertedDoublePendulum
∣8000
6000
4000
2000
1.08 1.09 1.10
rlOOOO
(c) DR
Iengthls
,0
Figure 6: Heatmap of return in unseen environments on Cartpole and InvertedDoublePendulum with
policies trained by MRPO, PW-DR and DR in the training environments.
a fixed high value. It can therefore strike a tradeoff between the average return and 10% worst-case
return, demonstrating the best performance both in Figs. 5(a) and 5(b).
A.11 Generalization to Unseen Environments of Cartpole and
InvertedDoublePendulum
In Fig. 6, we show the comparison results of MPRO, PR-DR and DR on unseen environments for
the other two benchmarks, Cartpole and InvertedDoublePendulum, to provide empirical support for
the generalization capability of MRPO.
19