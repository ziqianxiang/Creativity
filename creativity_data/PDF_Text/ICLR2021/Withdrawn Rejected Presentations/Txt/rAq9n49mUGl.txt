Under review as a conference paper at ICLR 2021
Using MMD GANs to correct physics models
and improve Bayesian parameter estimation
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian parameter estimation methods are robust techniques for quantifying
properties of physical systems which cannot be observed directly. In estimat-
ing such parameters, one first requires a physics model of the phenomenon to be
studied. Often, such a model follows a series of assumptions to make parame-
ter inference feasible. When simplified models are used for inference, however,
systematic differences between model predictions and observed data may prop-
agate throughout the parameter estimation process, biasing inference results. In
this work, we use generative adversarial networks (GANs) based on the maxi-
mum mean discrepancy (MMD) to learn small stochastic corrections to physics
models in order to minimize inference bias. We further propose a hybrid train-
ing procedure utilizing both the MMD and the standard GAN objective function-
als. We demonstrate the ability to learn stochastic model corrections and elimi-
nate inference bias on a toy problem wherein the true data distribution is known.
Subsequently, we apply these methods to a mildly ill-posed inference problem in
magnetic resonance imaging (MRI), showing improvement over an established in-
ference method. Finally, because 3D MRI images often contain millions of voxels
which would each require parameter inference, we train a conditional variational
autoencoder (CVAE) network on the corrected MRI physics model to perform fast
inference and make this approach practical.
1	Introduction
Bayesian parameter estimation methods are and robust techniques for quantifying properties of
physical systems which cannot be observed directly (von Toussaint, 2011). In order to estimate
such parameters, one first needs to develop a physics model of the phenomenon to be studied. This
process requires deep domain-specific knowledge. For all but the most basic of physical systems,
a series of simplifying assumptions on the physics of the system is required to make parameter
inference computationally feasible. Examples of such methodologies include perturbation theory,
in which higher order terms of Taylor series are dropped; mean-field theory, in which interactions
involving many degrees of freedom are replaced by averaged approximations; and (stochastic) dif-
ferential equations, when used as continuous limits of discrete stochastic processes. The price one
pays for utilizing a given approximation is highly problem dependent. When approximate models
are used for parameter inference, the mismatch between model predictions and data may propagate
throughout the inference process and lead to bias and misestimations of the inferred parameters. In
this work, we addressed the question how can one improve a physics model for the purpose of
parameter inference in a computationally inexpensive way?
In order to make use of the physics contained in approximate but successful models, we propose
a framework inspired by generative adversarial networks (GANs) to augment a model output with
small stochastic corrections learned entirely from unlabeled observations. Here, GAN generators
add stochastic corrections to the model outputs and discriminators critique the generator outputs.
We investigate the use of GANs based on the optimized maximum mean discrepancy (MMD) for
this task, as the MMD is a powerful discriminator between distributions (Bounliphone et al., 2016;
Sutherland et al., 2019). In section 3, we describe the maximum likelihood estimation (MLE) pro-
cedure used for baseline parameter estimation, the MMD, traditional GANs, and MMD GANs. We
further propose a hybrid training procedure which alternates between optimizing MMD GAN and
standard GAN objective functionals. In section 4, we describe two experiments in order to compare
1
Under review as a conference paper at ICLR 2021
the ability of MMD GANs, traditional GANs, and hybrid-trained MMD GANs to improve parameter
inference and to match the data distributions. The first experiment examines a toy problem wherein
the true data distribution is known and the physics model to be corrected is deliberately misspeci-
fied. Subsequently, we consider a mildly ill-posed inference problem in magnetic resonance imaging
(MRI), wherein inference results are compared with an established inference method.
Application to magnetic resonance imaging In MRI, the modelling of MR time signals is always
challenged by impediments such as the complexity of the underlying biological tissue, MRI scanner
hardware limitations, and spatiotemporal resolution limits. However, for (3+1)D MRI images with
1D MR time signals acquired for each voxel in a 3D volume, each time signal is usually well ap-
proximated by a superposition of relatively simple functions that are specific to tissue parameters.
Nevertheless, such superpositions still bias the parameter inference required to produce an image,
particularly for parameters which are sensitive to noise. In order to mitigate these issues, we inves-
tigate learning stochastic model corrections to aid inference. Additionally, parameter estimation is
computationally expensive in MRI. In brain imaging, scan volumes are discretized into millions of
voxels on the order of cubic millimetres, resulting in millions of nonnegative least squares (NNLS)
or maximum likelihood estimation (MLE) inference problems. Therefore, fast approximate data-
driven inference methods are appealing. Recently, conditional variational autoencoders (CVAEs)
were used to accelerate posterior sampling of gravitational wave event source parameters by 6 or-
ders of magnitude (Gabbard et al., 2019). Here, we show that CVAE inference trained on MMD
GAN corrected toy model signals generalizes to signals drawn from the true distribution. Then, we
perform a similar experiment using an MMD GAN as a source of realistic MRI signals, compar-
ing CVAE inference results to an established NNLS-based inference method (Prasloski et al., 2012;
Doucette et al., 2020) as well as a more expensive MLE method.
This work lies between two classes of GAN applications: GANs which are black-box functions
mapping random noise to data samples, providing little understanding of the landscape of the input
space, and GANs with well-understood input spaces, e.g. mapping images to images. By controlling
the size of learned stochastic corrections to physics models, we explicitly trade-off between match-
ing the goal data distribution and retaining model interpretability: smaller corrections give better
understood input spaces, while larger corrections allow for better matching of the data distribution.
2	Related work
Applications of GANs in physics has been of increasing interest recently. For use as black-box sam-
plers, de Oliveira et al. (2017) trained GANs for the generation of physically realistic 2D radiation
patterns of high energy particle collisions from simulated data. Unlike natural images, here the tar-
get distribution is highly sparse, non-smooth, takes values ranging over several orders of magnitude,
and much of the interesting physics occurs in the tail of the distributions. Nevertheless, the GAN
learned to reproduce the data distribution to high accuracy, as well as the distributions of several
low dimensional physics-inspired quantities derived from the images. In follow up work, Paganini
et al. (2018) used GANs to emulate the distribution of simulated 3D particle showers in multi-layer
calorimeters at CERN’s large hadron collider. These studies saw speedups in sampling of up to 5
orders of magnitude compared to traditional simulations. GANs have also been used for inference
problems in imaging which have well-understood input spaces arising from physics. In MRI image
reconstruction, GANs were used to map undersampled image data - acquired in the Fourier domain
-directly to reconstructed images, obtaining higher accuracy than traditional methods (Yang et al.,
2018; Quan et al., 2018). Similarly, Hammernik et al. (2018) proposed using GANs with generators
imbued with physics information for MRI reconstruction. There, the generator is a variational net-
work (Kobler et al., 2017) - a deep network which learns to invert a variational problem - where the
variational objective encodes the MRI physics.
3	Methods
Maximum likelihood estimation with Rician noise As a baseline method for parameter infer-
ence, we implement maximum likelihood estimation (MLE) for inferring parameters θ ∈ RNθ from
noisy observations Y ∈ Rn. As is the case for MRI signal measurements, we take Y to be described
by independent Rician (Rice, 1944) distributions Yi ~ Rice(Xi, ei) conditional on X = F(θ),
2
Under review as a conference paper at ICLR 2021
where F : RNθ → Rn is a mathematical model describing the underlying physics. We are inter-
ested in two cases for : i ≡ 0 unknown and to be determined from the MLE procedure, or i fixed
and possibly X -dependent. In either case, θ and possibly are determined by solving
n
arg min - X log P(Yj | Xj, j) where X = F(θ)	(1)
θ,
,	j=1
P(y | ν, e)=3 exp ( -(妻")) Io (等)is the likelihood of y under Rice(ν, e), where I。is the
modified Bessel function of the first kind with order zero.
Generative adversarial networks and the maximum mean discrepancy GANs (Goodfellow
et al., 2014) consist of two players, a generator and a discriminator, formulating unsupervised learn-
ing tasks as contests in which the discriminator criticizes the generator. We consider GANs for the
task of improving an initial distribution PX : X → R+ toward a goal distribution PY : X → R+
when only samples X 〜 PX and Y 〜 PY are available. Here, X are simulated data, Y are observed
data, and X is the data domain. Let G : X → (X → R+) be a generator mapping X 〜 PX onto
distributions PY : X → R+. In this formulation, Y 〜G(X) are samples from a learned distribu-
tion conditioned on X, with Y representing e.g. realizations of noisy data Y. We then consider two
forms of GANs. First, the traditional form with D : X → (0, 1) where G and D are trained via
alternating gradient descent and ascent steps on the joint objective (Goodfellow et al., 2014)
mGn	EX〜PX,Y〜G(X)	Iog(I - D(Y))
mDax EX 〜PX ,Y 〜PY ,Y 〜G(X) Iog(D(Y )) + log(1 - D(Y )).
(2)
Second, GANs based on the integral probability metric MMD(PX, PY), defined in terms of a re-
producing kernel k : X ×X → R. Given batches of m samples X 〜Pm and Y 〜Pm,
1m
MMDU(X, Y) ：= m(m- 1) X [k(Xi, Xj) + k(Yi, Yj) — k(Xi, Yj) — k(Xj,Yi)]	(3)
is an unbiased estimator of MMD2(PX, PY) with nearly minimal variance among unbiased estima-
tors (Gretton et al., 2012), and is a U statistic; a similar expression for the variance Vm(X, Y):=
Var M\MD2U (X, Y) due to Sutherland (2019) is given in appendix A. The MMD is used in both
generator and discriminator contexts (Dziugaite et al., 2015; Li et al., 2015): the generator is trained
to minimize the MMD, and the discriminator, i.e. the reproducing kernel k, is trained to maximize
the MMD. Or alternatively, Sutherland et al. (2019) show that maximizing the t-statistic estimator
i(x, Y) = MmdU (x, y)/JVm(x, Y)
(4)
asymptotically maximizes the power of rejecting the null hypothesis H0 : PX = PY of a permuta-
tion test on data X and Y using the test statistic m ∙ MMDU. Therefore, t provides a differentiable
proxy for permutation test power which the discriminator can maximize. Letting Y 〜 Πim=1G(Xi),
the joint objective for MMD GANs analogous to equation (2) is (Sutherland et al., 2019):
mGn EX〜Pm,Y〜Pm,Y〜Πm=ιG(Xi) MMDU(Y, Y)
max EX〜Pm,Y-Pm,Y〜Πm IG(Xi) mmdU(Y, y) or t(Y, Y).
k	X , Y , i=1 i
(5)
Model architectures In this work, X = Rn and the reproducing kernel k is the mean of Nbw
Gaussian basis functions with unique bandwidths for each coordinate of the inputs X, Y ∈ Rn ,
1 Nbw
k(X,Y) = N- X exP
Nbw i=1
(6)
where σ ∈ R+Nbw ×n is the matrix of kernel bandwidths. These bandwidths are the free parameters
of the kernel k which are optimized during the maximization over k in equation (5).
3
Under review as a conference paper at ICLR 2021
Algorithm 1: Training procedure for hybrid MMD GANs. G is the generator, D is the discrim-
inator, and k is the reproducing kernel which defines the MMD.
for Nepochs training epochs do
every krate epochs do
Draw m samples of Y and Y and train k according to equation (5)
for Nbatches minibatches do
Draw m samples of Y and Y and train G according to equation (5)
every GANrate epochs do
for Dsteps steps do
L Draw m samples of Y and Y and train D according to equation (2)
Draw m samples of Y and train G according to equation (2)
The discriminator D in equation (2) is taken to be a fully connected neural network with Nd hidden
layers, Nh hidden nodes per layer, ReLU activation on the hidden layers, and sigmoid activation on
the final scalar output. The generator G produces samples Y ~ G(X) such that the components 匕
follow Rician distributions. G is defined implicitly by the sampling scheme
{Yi ~ Rice(Vi, G)
V = |X + gδ (X )|	(7)
log = g(X)
where ν, e ∈ Rn, log and | ∙ | are applied elementwise, and the functions gδ : Rn → (-δ, δ)n and
g : Rn → (log -, log +)n are parameterized neural networks. The architectures of gδ and g are
identical to that of the discriminator D except for their final layers, which have n output nodes and
use tanh activations functions linearly scaled to the ranges (-δ, δ) and (log -, log +), respectively.
This parameterization of G permits only small deterministic corrections gδ (X) for each X which
are bounded a priori by a fixed constant δ. That is, one explicitly places limits on the size of the
non-physical learned corrections. The noise level is additionally allowed to be data-dependent, as
in MRI applications it is common for both signal-to-noise ratio and to vary with signal amplitude
and across scan volumes. The range ofg is also bound by the noise level limits - and +.
Hybrid training procedure We propose a hybrid training procedure which makes use of both
traditional GAN and MMD GAN objective functionals. Let a hybrid GAN consist ofa generator G,
discriminator D, and reproducing kernel k. Then, updates to k and G via equation (5) are interleaved
with updates to D and G via equation (2) as described in algorithm 1. The k update is pulled out of
the minibatch loop and applied only every krate epochs to limit the ability of k to overwhelm G and
cause its gradients to vanish; regularizing k is a difficult problem for which we took the simplest
approach, though many sophisticated techniques exist (Arbel et al., 2018; Li et al., 2015; BinkOWSki
et al., 2018). The traditional GAN updates to G and D are applied every GANrate epochs, with D
updated Dsteps times for every G update.
Conditional variational autoencoders In this work, CVAEs are used to perform fast approximate
sampling of the Bayesian posterior function P(θ | Y) where Y 〜 G(X), G is a pre-trained genera-
tor, and X is output from an uncorrected physics model which depends on θ. The CVAE architecture
we use is exactly that of Gabbard et al. (2019); see appendix B.2 for details.
4	Experiments
Toy physics model We first study a toy physical system with a known data distribution. Consider
Fe(t; θ) = (a0 + aι sinβ(2πft — φ)) exp(-t∕τ)	(8)
where Fβ : R → R is an exponential decay curve modulated by a periodically varying amplitude
parameterized by θ = [f, φ, a0, a1, τ] ∈ R5. The observations are time signals Y ∈ Rn where
4
Under review as a conference paper at ICLR 2021
n = 128, Yi 〜Rice(Vi, 6o), Vi = Fβ=2(ti; θ), ti = i -1, and €0 = 0.01. The behaviour of this
process, however, is modelled by X ∈ Rn where Xi = Fβ=4(t; θ), purposefully introducing a sys-
tematic bias into the recovered parameters θ. We first infer θ and €0 Via equation (1) using both the
true β = 2 and misspecified β = 4 models to establish best and worst case inference performance.
Then, generators defined by equation (7) are trained to learn corrected signals Y via equation (2),
equation (5), and algorithm 1, with the trained generators used to infer θ via equation (1) for com-
parison. Lastly, We show that a CVAE model trained on Y 〜 G(X) generalizes to the true data
Y. In all experiments, the prior space Pθ is defined by the priors f 〜U(614，312 ), φ 〜 U(0, ∏2),
ao 〜U(1, 2), aι 〜 U(⅛, 1), and T 〜 U (16,128); U (a, b) is the uniform distribution on (a, b).
Training data sets for X and Y were created from two draws of 102400 unique θ 〜Pθ samples.
Separate validation and testing data sets were similarly created with 10 240 samples each.
MRI physics model Here, we study an MRI physics model in which multi spin-echo MRI time
signals are modelled as the superposition of two component signals. Let Y ∈ Rn be an MRI time
signal consisting ofn measurements at uniformly spaced sample times ti, and let F : R → R be
2
F(t; θ) = X A' EPG(t,α,T2,')	(9)
'=1
where EPG(t, α, T2,') is a component signal computed using the extended phase graph (EPG) algo-
rithm (Hennig, 1988). EPG(t, α, T2,') is roughly equal to exp(-t∕T2,'), with MRI physics correc-
tions due to the spin flip angle α. We adopt the convention T2,1 ≤ T2,2, denoting T2,short = T2,1 ,
T2,long = T2,2, Ashort = A1 , and Along = A2. These MRI signal parameters are of considerable
academic and clinical interest, for example in myelin water imaging (Mackay et al., 1994; Whittall
& MacKay, 1989) for brain research (Wright et al., 2016; Weber et al., 2020), and in luminal water
imaging for prostate cancer research (Sabouri et al., 2017). The measured signal is modelled as Yi 〜
Rice(Xi, €0) where X ∈ Rn, Xi = F(ti; θ), and θ = [α, T2,short, T2,long, Ashort, Along] ∈ R5,
with θ and ^0 inferred using equation (1). Then, as in the toy model experiments, we train gener-
ators defined by equation (7) via equation (2), equation (5), and algorithm 1, producing corrected
MRI models. MLE is performed to infer θ using these corrected models, and a CVAE model is
trained using signals drawn from the MMD GAN generator. In all experiments, the prior space Pθ
consists of the θ values resulting from the MLE fits of F(t; θ) to the data Y. During fitting, θ was
constrained as: α ∈ [50°, 180°], T2,short ∈ [8ms, 1000ms], T2,i0ng - T2,short ∈ [8ms, 1000ms],
and Ashort, Along ∈ [0, ∞). Half of these θ and corresponding X andY values are used for training
data, with the remaining half split evenly between validation and testing data. The MRI data used
for this experiment is from a brain scan using a CPMG sequence (Whittall et al., 1997) on a 3 T MR
system (Ingenia Elition, Philips Medical Systems, Best, The Netherlands) from a healthy volunteer
giving written and informed consent, approved by our university ethics board. The extracted brain
volume consists of 821 145 signals Y ∈ Rn acquired with n = 48 samples at times t = i ∙ TE,
echo spacing TE = 8 ms, and spatial resolution of 0.96 × 0.96 × 2.5mm3 .
This two-component model is a simplification of an established multicomponent model (Prasloski
et al., 2012) wherein inference is performed using a regularized NNLS-based technique with a fixed
spectrum of many - typically 40 or more - T2,' components. The resulting amplitudes a` together
with T2,' are collectively referred to as the T2 distribution of the MRI signal, and can be thought
ofas a regularized inverse Laplace transform with EPG basis functions substituting for exponential
functions. This method, herein referred to as NNLS, is used for further comparison of inference
results. Note that while this method is well established, it is not optimal; for instance, it implicitly
assumes a uniform Gaussian noise distribution as opposed to a Rician distribution, which is only
justified for high signal-to-noise ratios.
Training All models were trained for two days using the Adam (Kingma & Ba, 2014) optimizer
with default momentum. Hyperparameter sweeps were performed to optimize the following: learn-
ing rate η, k and GAN training rates krate and GANrate, number of D updates per G update Dsteps,
number of σ bandwidths Nbw, kernel loss MMDU or t, number of hidden layers Nh and hidden
nodes Nd, and batch size m. For toy (MRI) models, δ = 0.1 (0.025), log € ∈ [-8, -2] ([-6, -3]),
log σ ∈ [-8, 4] ([-8, 4]). GAN performance was evaluated on maximum log likelihood value on
validation data. In depth training details for these and CVAE models is given in appendix B.2.
5
Under review as a conference paper at ICLR 2021
0.4
0.2
0.0
0.4
0.2
0.0
0.02
0.00
0.010
0.006
0	32	64	96	128 0	64	128
Time [a.u.]
Figure 1: Example toy model signals and corresponding corrections learned via the hybrid training
procedure in algorithm 1. Top-left: noisy true signal Y overlaid with noiseless uncorrected X and
corrected |X + gδ (X) | signals, with X given by Fβ=4 in equation (8); difference between true Fβ=2
and learned |X + gδ (X)| is inset. Bottom-left: noisy true signal Y overlaid with noisy corrected
signal (Hybrid); learned correction gδ(X) and noise level exp(g(X)) is inset. Top-right & bottom-
right: distributions of learned gδ and exp(g) over the validation X data.
Table 1: Comparison of inference results for the toy physics problem. Mean absolute errors for
inferred parameters are shown as percentages relative to respective uniform prior distribution widths.
		Fβ=4	GAN	MMDGAN	Hybrid GAN	CVAE	Fβ=2
Log likelihood		371.7 ± 1.1	409.8 ± 0.4	409.6 ± 0.4	408.4 ± 0.4	一	413.5 ± 0.3
RMSE/0		0.790 ± 0.008	0.239 ± 0.002	0.257 ± 0.002	0.250 ± 0.002	-	0.189 ± 0.002
f	θ1	1.13 ± 0.05	0.79 ± 0.04	0.79 ± 0.03	0.81 ± 0.04	0.77 ± 0.04	0.69 ± 0.03
φ	θ2	2.45 ± 0.07	1.62 ± 0.05	1.53 ± 0.04	1.56 ± 0.05	1.44 ± 0.04	1.32 ± 0.04
a0	θ3	8.72 ± 0.10	1.62 ± 0.04	1.32 ± 0.03	1.48 ± 0.04	1.17 ± 0.03	1.08 ± 0.03
a1	θ4	5.45 ± 0.12	3.20 ± 0.10	3.55 ± 0.10	3.41 ± 0.10	3.16 ± 0.10	3.00 ± 0.09
τ	θ5	0.76 ± 0.03	0.75 ± 0.02	0.65 ± 0.02	0.78 ± 0.03	0.57 ± 0.02	0.54 ± 0.02
5	Results
Toy physics model Figure 1 shows example toy model signals before and after adding learned
corrections, and table 1 compares toy problem inference results using six models: MLE with the
uncorrected Fβ=4 model, estimating ^o; MLE with GAN, MMD GAN, and hybrid GAN generator
models, with learned e; posterior sampling with a CVAE trained on MMD GAN outputs; and MLE
with the true Fβ=2 model, using the true 0 = 0.01. 1000 signals randomly chosen from the
validation data are used for model comparison. Log likelihood values are optimized via equation (1).
Root-mean-square error (RMSE) values relative to 0 are computed between the model fits without
added noise, i.e. |X + gδ (X )|, and the true model Fβ=2 without noise. Mean absolute error is shown
for each inferred parameter relative to the width of the corresponding uniform prior distribution.
Note that CVAE-inferred parameters are nearly as accurate as the perfectly specified model Fβ=2 .
MRI physics model Figure 2 shows learned generator outputs for the MRI physics model, sig-
nal amplitude histograms using bin widths determined by the MRI digitizer and populated with the
48 ∙ 205 287 validation data samples, and the mean over validation signal T2 distributions computed
using the NNLS inference method. Table 2 compares inference results in an identical manner as in
table 1, using F from equation (9) as the uncorrected model, but without a ground truth. In lieu
of a true model, we compute the `1, `2, χ2, and Wasserstein distances between model and Y his-
tograms (Pele & Werman, 2010), as well as the `2 norm of T2 distribution differences. Lastly, as vi-
sualized in figure 3, inferred parameters are compared with NNLS. The spin flip angle α is compared
directly. The geometric means of the T? distribution from NNLS and the T^` values from F are com-
pared, both weighted by the corresponding amplitudes A2,'. Myelin water fraction (MWF) (Mackay
et al., 1994) and T2,i0ng are also visualized in figure 3. For NNLS, MWF is the fraction of a` con-
tained in T2,' ≤ 40 ms, and T2,i0ng is the geometric mean of T2,' ≥ 40 ms; for CVAE, MWF is
Ash0rt/(Ash0rt + Ai0ng) weighted by the logistic function σ((30 ms - T2,sh0rt)/10 ms).
6
Under review as a conference paper at ICLR 2021
80	160	240	320
Time Fmsl
「n.$ uo-onqESTP 目
aunoo
--∙L⅛
1 O
0.8
NNLS-F
NNLS-GAN
NNLS - Hybrid
Figure 2: Left: example noisy MRI signal Y compared with noisy corrected signal (Hybrid), trained
via the hybrid training algorithm 1; distributions of learned gδ and exp(g ) over the validation X
data is inset, as well as the learned correction gδ (X) and signal difference for the data shown. Top-
right: comparison of true Y vs. learned signal amplitude distributions for the uncorrected F (t; θ)
from equation (9), GAN, MMD GAN, and hybrid-trained GAN signal models over the validation
X and Y data, as well as the distribution differences compared to Y . Bottom-right: comparison of
true vs. learned T2 distributions - analagous to regularized inverse Laplace transforms - and T2 dis-
tribution differences compared to Y for the same four signal models. The hybrid training algorithm
produces signal amplitude and T2 distributions which are most similar to the true distributions.
Table 2: Comparison of inference results and distribution similarity for the MRI physics problem.
Maximum likelihood estimation is performed for the uncorrected F (t; θ), GAN, MMD GAN, and
hybrid-trained GAN signal models, with the resulting log-likelihood shown. Spin flip angle α and
geometric mean T2 and compared with NNLS for each signal model, as well as CVAE; note that
NNLS, while commonly used, is not a gold standard and small differences are expected. Five distri-
bution similarity metrics are shown comparing the four signal models with the true data distributions;
the hybrid-trained model performs best in all metrics.
	F	GAN	MMD GAN	Hybrid GAN	CVAE
Log likelihood	163.7 ± 0.6	174.0 ± 0.6	174.0 ± 0.5	175.4 ± 0.5	-
Spin flip angle α	0.86 ± 0.04°	4.87 ± 0.21°	3.08 ± 0.11°	3.59 ± 0.16°	2.99 ± 0.13°
Geo. mean T2	4.30 ± 0.26 ms	6.19 ± 0.35 ms	5.08 ± 0.29 ms	6.00 ± 0.32 ms	10.0 ± 0.5 ms
`1	143 655.0	147 934.0	94 282.0	90 786.0	-
`2	28 298.3	37 807.0	19 491.3	18 573.0	-
χ2	2944.6	5339.2	1279.8	919.6	-
Wasserstein	390.1	342.2	253.5	243.3	-
T2 distribution `2	0.141	0.038	0.024	0.014	-
6	Discussion
In this work, we trained three classes of generative adversarial networks to learn stochastic correc-
tions to physics models and improve Bayesian parameter estimation. As seen in figure 1 and table 1,
all three GAN models as well as the CVAE model trained on MMD GAN outputs result in near
optimal inference performance for the toy problem, which has the advantage of having corrections
that are exactly described by the same parameters θ as the uncorrected model. The MRI physics
model does not have this luxury, and furthermore gδ and exp(g) are on average only 1-2 percent
as large as the signal intensity, yet figure 2 and table 2 show considerable improvement in inference
and distribution similarity. Additionally, figure 3 shows that the robustly estimated parameters α and
mean T2 are consistent between the NNLS method and a CVAE trained on MMD GAN generator
signals. The MWF, a biomarker for brain myelin content, depends strongly on fast decaying sig-
nal components making it sensitive to noise. The CVAE generates a more spatially uniform MWF
map, which is expected from myelin biology, and resolves smaller structures compared to NNLS.
Additionally, T2,long is misestimated by NNLS in the cerebrospinal fluid - the bright regions in the
CVAE image - where it is known that T long 〜1s.
7
Under review as a conference paper at ICLR 2021
180.0o
170.0o
160.0o
150.0o
140.0o
130.0o
120.0o
180.0o
170.0o
160.0o
150.0o
140.0o
130.0o
120.0o
Geo. mean 7⅛
MWF
J-2ilong
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.30
0.25
0.20
0.15
0.10
0.05
0.00
1000.0 ms
800.0 ms
600.0 ms
400.0 ms
200.0 ms
0.0 ms
1000.0 ms
800.0 ms
600.0 ms
400.0 ms
200.0 ms
0.0 ms
Figure 3: Example parameter maps computed using CVAE (top row) and NNLS (bottom row).
Estimation of the spin flip angle α and geometric mean T2 is relatively insensitive to noise, resulting
in little difference between the two methods. MWF is harder to estimate due to its dependence
on fast decaying signal components with inherently lower signal-to-noise ratio; CVAE produces
more spatially uniform MWF compared to NNLS, which is expected from neurobiology. T2,long is
misestimated by NNLS: slowly decaying signal components cannot be distinguished from noise in
NNLS, resulting in underestimation of the long T2 of the cerebrospinal fluid which is known to be
〜1 s. The CVAE, trained using the correct RiCian noise model, does not have this issue.
While training the different models, we observed MMD GANs to have less severe failure modes
than traditional GANs. MMD GANs first learn the mean correction toward the target distribution,
slowly recovering finer details thereafter; in the toy problem, smooth exponentially decaying cor-
rections were learned first, with the oscillatory corrections following. Traditional GANs, however,
would proceed more erratically toward the true distribution. On the other hand, noise amplitude dis-
tributions learned by traditional GANs appeared more physically plausible, exhibiting less spurious
variations in time. In all, the ability of traditional GANs to critique individual signals with high
fidelity and MMD GANs to discriminate between distributions motivated the hybrid training proce-
dure in algorithm 1. For example, the distribution of exp(g) in figure 1 is much more homogeneous
in time compared to MMD GANs; see figures 4 and 5 in appendix B. Note however that traditional
GANs on their own are limited for this application: generators G = G(X(θ)) depend only on θ
and are unable to capture behaviour described by unmodelled parameters. For traditional GANs,
discriminators eventually learn to exploit this mismatch, resulting in undesired behaviour such as
the decreased distribution similarity observed in table 2. In the hybrid training routine, however, the
discriminator acts as an adversarial regularizer to the MMD GAN objective, improving results.
Limitations and future work The experiments described in section 4 place explicit limits on
the range of gδ and g , allowing one to control the size of the stochastic corrections learned by
G. However, we have not shown formally that these corrections cannot substantially change the
physical interpretation of inferred θ. For example, the inversion of the MRI model becomes ill-
posed as T2,short → T2,long in equation (9), leading to inference which is sensitive to small changes
in the MRI signal, such as those introduced by the learned corrections. In this case one could argue
that such θ had limited meaning in the first place, but in general there may be subtle failure modes.
For future work, one may consider allowing G to depend on additional nuisance parameters z . This
would allow G to learn corrections driven by physics present in the goal distribution that is not
described by θ, but could be characterized by z . During inference, inferred θ would be forced to be
independent ofz; for example, a CVAE could be trained to pivot (Louppe et al., 2017) on z.
Lastly, while we have focused on physics motivated signal processing, this framework is fully gen-
eral and may find applications to any system which well approximates a goal distribution but could
benefit from learning controlled stochastic corrections from unlabeled data.
8
Under review as a conference paper at ICLR 2021
References
Michael ArbeL DoUgal Sutherland, Mikolaj Binkowski, and Arthur Gretton. On gradient regular-
izers for MMD GANs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 6700-6710.
Curran Associates, Inc., 2018.
Mikolaj BinkoWski, DoUgal J. Sutherland, Michael ArbeL and Arthur Gretton. Demystifying MMD
GANs. arXiv:1801.01401 [cs, stat], March 2018.
Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, and
Arthur Gretton. A Test of Relative Similarity For Model Selection in Generative Models.
arXiv:1511.04581 [cs, stat], February 2016.
Luke de Oliveira, Michela Paganini, and Benjamin Nachman. Learning Particle Physics by Ex-
ample: Location-Aware Generative Adversarial Networks for Physics Synthesis. Computing
and Software for Big Science, 1(1):4, September 2017. ISSN 2510-2044. doi: 10.1007/
s41781-017-0004-6.
Jonathan Doucette, Christian Kames, and Alexander Rauscher. DECAES - DEcomposition and
Component Analysis of Exponential Signals. Zeitschrift Fur Medizinische Physik, May 2020.
ISSN 1876-4436. doi: 10.1016/j.zemedi.2020.04.001.
Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural
networks via Maximum Mean Discrepancy optimization. arXiv:1505.03906 [cs, stat], May 2015.
Hunter Gabbard, Chris Messenger, Ik Siong Heng, Francesco Tonolini, and Roderick Murray-Smith.
Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave
astronomy. arXiv:1909.06296 [astro-ph, physics:gr-qc], September 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural In-
formation Processing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A Kernel Two-Sample Test. Journal of Machine Learning Research, 13(25):723-773, 2012. ISSN
1533-7928.
Kerstin Hammernik, Erich Kobler, Thomas Pock, Michael P. Recht, Daniel K. Sodickson, and Flo-
rian Knoll. Variational Adversarial Networks for Accelerated MR Image Reconstruction. In
Proceedings of the 26th Annual Meeting of the International Society for Magnetic Resonance in
Medicine (ISMRM), Paris, France, June 2018.
J Hennig. Multiecho imaging sequences with low refocusing flip angles. Journal of Magnetic
Resonance (1969), 78(3):397-407, July 1988. ISSN 0022-2364. doi: 10.1016/0022-2364(88)
90128-X.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980
[cs], December 2014.
Erich Kobler, Teresa Klatzer, Kerstin Hammernik, and Thomas Pock. Variational Networks: Con-
necting Variational Methods and Deep Learning. In Pattern Recognition, Lecture Notes in Com-
puter Science, pp. 281-293. Springer, Cham, September 2017. ISBN 978-3-319-66708-9 978-3-
319-66709-6. doi: 10.1007/978-3-319-66709 6 23.
Yujia Li, Kevin Swersky, and Richard Zemel. Generative Moment Matching Networks.
arXiv:1502.02761 [cs, stat], February 2015.
Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to Pivot with Adversarial Networks.
arXiv:1611.01046 [physics, stat], June 2017.
Alex Mackay, Kenneth Whittall, Julian Adler, David Li, Donald Paty, and Douglas Graeb. In vivo
visualization of myelin water in brain by magnetic resonance. Magnetic Resonance in Medicine,
31(6):673-677, 1994. ISSN 1522-2594. doi: 10.1002/mrm.1910310614.
9
Under review as a conference paper at ICLR 2021
Michela Paganini, Luke de Oliveira, and Benjamin Nachman. Accelerating Science with Gener-
ative Adversarial Networks: An Application to 3D Particle Showers in Multi-Layer Calorime-
ters. Physical Review Letters, 120(4):042003, January 2018. ISSN 0031-9007, 1079-7114. doi:
10.1103/PhysRevLett.120.042003.
Ofir Pele and Michael Werman. The Quadratic-Chi Histogram Distance Family. In Kostas Dani-
ilidis, Petros Maragos, and Nikos Paragios (eds.), Computer Vision - ECCV2010, Lecture Notes
in Computer Science, pp. 749-762, Berlin, Heidelberg, 2010. Springer. ISBN 978-3-642-15552-
9. doi: 10.1007/978-3-642-15552-9-54.
Thomas Prasloski, Burkhard Madler, Qing-San Xiang, Alex MacKay, and Craig Jones. Applications
of stimulated echo correction to multicomponent T2 analysis. Magnetic Resonance in Medicine,
67(6):1803-1814, 2012. ISSN 1522-2594. doi: 10.1002/mrm.23157.
Tran Minh Quan, Thanh Nguyen-Duc, and Won-Ki Jeong. Compressed Sensing MRI Reconstruc-
tion Using a Generative Adversarial Network With a Cyclic Loss. IEEE Transactions on Medical
Imaging, 37(6):1488-1497, June 2018. ISSN 1558-254X. doi: 10.1109/TMI.2018.2820120.
S.	O. Rice. Mathematical Analysis of Random Noise. Bell System Technical Journal, 23(3):282-
332, 1944. ISSN 1538-7305. doi: 10.1002/j.1538-7305.1944.tb00874.x.
Shirin Sabouri, Silvia D. Chang, Richard Savdie, Jing Zhang, Edward C. Jones, S. Larry Gold-
enberg, Peter C. Black, and Piotr Kozlowski. Luminal Water Imaging: A New MR Imaging
T2 Mapping Technique for Prostate Cancer Diagnosis. Radiology, 284(2):451-459, April 2017.
ISSN 0033-8419. doi: 10.1148/radiol.2017161687.
Dougal J. Sutherland. Unbiased estimators for the variance of MMD estimators. arXiv:1906.02104
[cs, stat], June 2019.
Dougal J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex
Smola, and Arthur Gretton. Generative Models and Model Criticism via Optimized Maximum
Mean Discrepancy. arXiv:1611.04488 [cs, stat], June 2019.
Udo von Toussaint. Bayesian inference in physics. Reviews of Modern Physics, 83(3):943-999,
September 2011. doi: 10.1103/RevModPhys.83.943.
Alexander Mark Weber, Yuting Zhang, Christian Kames, and Alexander Rauscher. Myelin water
imaging and R2* mapping in neonates: Investigating R2* dependence on myelin and fibre ori-
entation in whole brain white matter. NMR in biomedicine, 33(3):e4222, March 2020. ISSN
1099-1492. doi: 10.1002/nbm.4222.
Kenneth P Whittall and Alexander L MacKay. Quantitative interpretation of NMR relaxation data.
Journal of Magnetic Resonance (1969), 84(1):134-152, August 1989. ISSN 0022-2364. doi:
10.1016/0022-2364(89)90011-5.
Kenneth P. Whittall, Alex L. Mackay, Douglas A. Graeb, Robert A. Nugent, David K. B. Li, and
Donald W. Paty. In vivo measurement of T2 distributions and water contents in normal human
brain. Magnetic Resonance in Medicine, 37(1):34-43, 1997. ISSN 1522-2594. doi: 10.1002/
mrm.1910370107.
Alexander D. Wright, Michael Jarrett, Irene Vavasour, Elham Shahinfard, Shannon Kolind, Paul
van Donkelaar, Jack Taunton, David Li, and Alexander Rauscher. Myelin Water Fraction Is
Transiently Reduced after a Single Mild Traumatic Brain Injury - A Prospective Cohort Study in
Collegiate Hockey Players. PLOS ONE, 11(2):e0150215, February 2016. ISSN 1932-6203. doi:
10.1371/journal.pone.0150215.
Guang Yang, Simiao Yu, Hao Dong, Greg Slabaugh, Pier Luigi Dragotti, Xujiong Ye, Fangde
Liu, Simon Arridge, Jennifer Keegan, Yike Guo, and David Firmin. DAGAN: Deep De-
Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction.
IEEE Transactions on Medical Imaging, 37(6):1310-1321, June 2018. ISSN 1558-254X. doi:
10.1109/TMI.2017.2785879.
10
Under review as a conference paper at ICLR 2021
A Variance of the MMD
Let k be the reproducing kernel defining the MMD, and let X and Y be collections of m samples
of X, Y ∈ Rn. Following the notation in Sutherland (2019), let KXY ∈ Rm×m be a matrix with
elements (KXY)ij = k(Xi, Yj), with KXX and KYY defined similarly, and let KXX and KYY be
equal to KXX and KYY with their diagonal elements set to zero. Then, equation (4) in Sutherland
(2019) gives the unbiased estimator Vm(X, Y) for Var MMDU(X, Y) as
Vm = τ~γ- h||KKxx 1||2 + WKKYY“同 + ^m-mD MKXY 1||2 + ||KXY 1"2]
(m)4 L	」	(m)2
——T8V [l>KxxKxy 1 + 1>KYYK>XY 1i
m(m)3
+ m2m3 h(1>κXX1 + 1>kYY 1)(1>kxy 1)]	(IO)
2(2m - 3) h >K 2	>K 2]	4(2m - 3)	>K	2
-(m)2(m)4	I(I	KXXI)	+	(I	KYY1)	J -	(m)3	(I	KXY1)
-ɪ h∣∣Kxx∣∣F + ||KYY||F] - 4m(m-2) ∣∣Kxy∣∣f
(m)4 L	」	(m)3
where || ∙ ∣∣2 and || ∙ ||f are the '2 and FrobeniUS norms, 1 ∈ Rm is the vector of all ones, and
(m)k := m(m 一 l)…(m 一 k + 1) is the falling factorial.
Note that while verifying equation (4) in Sutherland (2019) using computer algebra, we found two
typos (highlighted above). The first correction was to the denominator in the second term: m3(m -
1)2 → m3(m - 1)3 = (m)23. The second correction was a flipped sign on the last term: + → -.
See the MATLAB script mmd_variance.m for verification.
B Training
Model hyperparameters were optimized using a brute-force grid search across a broad range of
parameter values. For training, we have access to a compute cluster with compute nodes containing
2.10 GHz Intel Xeon Gold 6130 processors with 16 CPU cores/32 threads each. Because of this
infrastructure, we were able to train many models in parallel for each experiment. In all cases,
models were trained for two days each using the Adam (Kingma & Ba, 2014) optimizer with default
momentum parameters β1 = 0.9 and β2 = 0.999; the optimizer step size η was varied.
B.1	GAN models
Table 3: Hyperparameters tested for traditional GAN, MMD GAN, and hybrid-trained GAN adver-
sarial models for both the toy and MRI experiments. Parameters for which multiple values were
tested are listed within curly braces; the best model corresponds to the bolded value within the list.
	Toy GAN	Toy MMD	Toy Hybrid	MRI GAN	MRI MMD	MRI Hybrid
Nbatches	10	10	10	10	10	10
m	{1024, 2048}	2048	2048	{1024, 2048}	2048	2048
η∕10-5	{10, √1o, 1}	{10√10,10, √10}	{10, √10}	{10, √10, 1}	10	{10, √10}
δ	0.1	0.1	0.1	0.025	{0.025, 0.05}	0.025
[log-,log+]	[-8, -2]	[-8, -2]	[-8, -2]	[-6, -3]	[-6, -3]	[-6, -3]
Nd	{64, 128}	128	128	{64, 128}	{128, 256}	{128, 256}
Nh	{2, 4}	4	4	{2, 4}	{2, 4}	4
Dsteps	{1, 3, 5, 10, 15, 20}	-	{10, 15, 20}	{5, 10, 15, 20}	-	{10, 15, 20}
GANrate	一	-	{5,10, 25}	一	-	{5, 10, 20}
k loss	一	{M∖DU, ^}	{MdDU ,t}	一	{MIMDU, t}	{MdDU, t}
krate	一	{10, 25, 50}	{10, 25, 50}	一	{3, 5, 10, 25}	{10, 25}
Nbw	一	{4, 8}	{4, 8}	一	{2, 4, 6, 8}	{4, 8}
[log σ-, log σ+]	-	[-8, 4]	[-8, 4]	-	[-8, 4]	[-8, 4]
11
Under review as a conference paper at ICLR 2021
Table 3 shows the hyperparameters which were tested for traditional GAN, MMD GAN, and hybrid-
trained GAN models for both the toy and MRI experiments. During training, data sampled from the
validation set was periodically used to perform maximum likelihood estimation (MLE) fits for each
model. Following the training of all six models for all hyperparameter combinations, candidate
top models were chosen for each of the six sweeps according to the highest log likelihood values
(computed from a moving average over 25 epochs) resulting from the MLE fits on the validation
data. Finally, each set of candidate top models were used for MLE fits to 1000 data samples from
the held out testing data set for final model selection.
Figures 4 and 5 show example learned stochastic corrections for traditional, MMD, and hybrid
GANs for the toy problem and MRI problem, respectively. In figure 4, we see an example of the
traditional GAN learning a more uniform noise distribution for the toy problem - centered around
the true value of ∈o = 0.01 - compared to the MMD GAN. This property appears to manifest in the
hybrid GAN, as well. Figure 5 shows that, as with the toy problem, the three GANs for the MRI
problem each learn similar stochastic corrections.
An extensive hyperparameter sweep was performed, but we would like to emphasize that this is not
necessary to apply this method successfully. Rather, given that a sufficiently large computational
infrastructure was available to us, we were interested in investigating the regions of hyperparameter
space which resulted in GANs which either failed to converge, or were particularly successful. In
the end, most parameters were not found to consistently affect the failure or success of the various
GAN models, at least among the fairly conservative ranges of values which were tested. For exam-
ple, all of the candidate top models were able to achieve the same optimized log likelihood values
when accounting for the variance of the sample means. There were, however, some noteworthy
parameters. The number of discriminator training steps, Dsteps, for each generator training step for
the traditional GAN models was 10 or higher in most successful models. We believe this is due to
the baseline uncorrected model being already close to the data distribution, as illustrated in figure
2 of the main text, and therefore the discriminator D needs additional training steps to critique the
subtle imperfections of the generator. The MMD kernel k, on the other hand, is a more powerful
discriminator and had to be limited to being trained only every 10 or more epochs. Interestingly,
training k via maximizing either of MMDU or £ performed similarly well. This is not surprising,
however, as while optimizing t has theoretical advantages for increasing discriminator power, k is
already in a sense “too powerful”, and therefore we believe that making it even more powerful may
not be helpful. Lastly, we experimented with increasing the size of the deterministic correction δ for
the MRI MMD GAN. While the final best model did happen to use δ = 0.05, this larger δ value did
not consistently improve the models, and we preferred to keep the smaller δ = 0.025 value for the
other models. Note that in figure 5, we see that the learned distributions of deterministic corrections
gδ in the MRI experiment are nearly the same for traditional, MMD, and hybrid GANs, despite the
larger δ used for the MMD GAN.
B.2	CVAE models
Table 4: Hyperparameters tested for CVAE models for both the toy and MRI experiments. CVAE
models are trained on (θ, Y) pairs where Y 〜 G(X(θ)) are sampled from pre-trained MMD GAN
generators G. Parameters for which multiple values were tested are listed within curly braces; the
best model corresponds to the bolded value within the list. Both toy and MRI experiments swept
over the same CVAE parameters.
m
η	ηmin	ηrate ηdrop
Nh
Toy CVAE	256	10
MRI CVAE	256	10
10-5	1000	{1, √10}	{32,64,128}	{6,8}	{2,4,6}
10-5	1000	{1, √10}	{32,64,128}	{6,8}	{2,4,6}
The conditional variational autoencoder (CVAE) models use exactly the same architecture as de-
scribed in Gabbard et al. (2019). Briefly, their CVAE architecture consists of three component net-
works: two encoders E1 : Rn → R2Nz and E2 : Rn+Nθ → R2Nz, and a decoder D : RNz → R2Nθ .
Ei maps data samples Y ∈ Rn to an Nz-dimensional latent space, where the 2N outputs of Ei
parameterize Nz normal distributions; E? maps θ ∈ RNθ and Y ∈ Rn pairs to the same latent space;
and D maps latent space samples Z ∈ RNz to 2Nθ outputs which parameterize Nθ normal distribu-
12
Under review as a conference paper at ICLR 2021
tions. The normal distributions are parameterized by pairs (μ, log σ) of means and log bandwidths.
During training, the latent space encoding learned by Ei, which is only given observations Y, is
informed by E?, which has access to both Y and underlying parameters θ. During inference, Ei
samples are decoded by D, producing approximate posterior distribution samples θ.
Table 4 shows the hyperparameters swept over during CVAE training for both the toy and MRI
experiments, each model training on (θ, Y) pairs sampled from pre-trained MMD GAN generators
Y ~ G(X(θ)). As in Gabbard et al. (2019), We use fully-connected networks for all three of
Ei , E2 , and D, using ReLU activation functions on hidden layers and no activation function on
output layers. We use the same notation for number of hidden nodes Nd and hidden layers Nh as
in the GAN experiments, as well as for the initial step size η and batch size m. We additionally
experimented with dropping the learning rate by a multiplicative factor ηdrop every ηrate epochs,
until a minimum value of ηmin . CVAEs were robust to changes in hyperparameters, achieving
nearly the same accuracy for any hyperparameter set. Candidate top CVAE models were first chosen
through evaluating mean absolute error values of inferred θ on the validation data set. The final best
models were chosen through evaluation on a held out test data set.
13
Under review as a conference paper at ICLR 2021
〔.n.gφpn-IduIt0It0u6IS
0.4
0.2
0.0
0.4
0.2
Distribution of eχp(gε)
0.0
0
r lτ h 1J τ⅝ 1

0.02
0.00
0.010
0.006
32	64	96	128 0	64	128
Time [a.u.]
〔.n.gφpn-IduIt0It0u6IS
0.4
0.2
0.0
0.4
0.2
0.0
∕⅞=2-lX+gd(X)l
0.025
0.000
—区+gMX)∣
・ I ・・
—Y
—X
g点。
exp(gβ(A))
0.025
0.000
TJP-I~^1 ~I~1 ~1 ~1 ~Γ
√VVW<{Λr⅝A⅛
........... U.Olb	∏~I-I-I-I-I-I~I-∏ ■
OOQ7L . . . , . . .J-
0	32	64	96	128 0	64	128
Time [a.u.]
—Y
—MMD
0.02
0.00
0.010
0.006
Fβ=2-∖X+gs(X)∖
〔.n.gφpπ4IIduIt0It0u6IS
0.4
0.2
0.0
0.4
0.2
0.0
—Y
—X
0.025
0.000

—区+/(X)I
・	I	,一，
laooo
—Y '
—Hybrid
0
32
gs6)
exp(ge(X))

64	96	128 0
Time [a.u.]
0.02
0.00
0.010
0.006
Figure 4: Example learned stochastic corrections for the toy problem using traditional, MMD, and
hybrid GANs. The top row of plots for each model shows deterministic corrections, and the bottom
row shows the stochastic corrections.
14
Under review as a conference paper at ICLR 2021
〔.n.gφpn-IduIt0It0u6IS
0.01
0.00
-0.01
0.03
0.02
0.00
〔.n.gφpn-IduIt0It0u6IS
0.01
0.00
-0.01
0.03
0.02
0.00
〔.n.gφpπ4IIduIt0It0u6IS
1.5
1.0
0.5
0.0
0.9
0.6
0.3
0.0
			
gs(X)	~ -1 Distrihutinn CfeXT√∙Λ -
8	80	160	240	320	4008	160	320
Time [ms]
0.01
0.00
-0.01
0.03
0.02
0.00
Figure 5: Example learned stochastic corrections for the MRI problem using traditional, MMD, and
hybrid GANs. The top row of plots for each model shows deterministic corrections, and the bottom
row shows the stochastic corrections.
15