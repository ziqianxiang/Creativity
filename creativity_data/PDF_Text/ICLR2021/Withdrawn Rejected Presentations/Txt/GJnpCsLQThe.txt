Under review as a conference paper at ICLR 2021
Gradient Descent Ascent for Min-Max Prob-
lems on Riemannian Manifolds
Anonymous authors
Paper under double-blind review
Ab stract
In the paper, we study a class of useful non-convex minimax optimization prob-
lems on Riemanian manifolds and propose a class of Riemanian gradient descent
ascent algorithms to solve these minimax problems. Specifically, we propose a
new Riemannian gradient descent ascent (RGDA) algorithm for the deterministic
minimax optimization. Moreover, we prove that the RGDA has a sample com-
plexity of O(κ2-2) for finding an -stationary point of the nonconvex strongly-
concave minimax problems, where κ denotes the condition number. At the same
time, we introduce a Riemannian stochastic gradient descent ascent (RSGDA) al-
gorithm for the stochastic minimax optimization. In the theoretical analysis, we
prove that the RSGDA can achieve a sample complexity of O(κ4-4). To further
reduce the sample complexity, we propose a novel momentum variance-reduced
Riemannian stochastic gradient descent ascent (MVR-RSGDA) algorithm based
on a new momentum variance-reduced technique of STORM. We prove that the
MVR-RSGDA algorithm achieves a lower sample complexity of O(κ4c-3) with-
out large batches, which reaches near the best known sample complexity for its
Euclidean counterparts. Extensive experimental results on the robust deep neural
networks training over Stiefel manifold demonstrate the efficiency of our proposed
algorithms.
1	Introduction
In the paper, we study a class of useful non-convex minimax (a.k.a. min-max) problems on the
Riemannian manifold M with the definition as:
min max f (x, y),	(1)
x∈M y∈Y
Where the function f (x, y) is μ-strongly concave in y but possibly nonconvex in x. Here Y ⊆ Rd
is a convex and closed set. f (∙, y) : M → R for all y ∈ Y is a smooth but possibly nonconvex
real-valued function on manifold M, and f (x, ∙) : Y → R for all X ∈ M a smooth and (strongly)-
concave real-valued function. In this paper, we mainly focus on the stochastic minimax optimization
problem f (x, y) := Eξ〜D [f (x, y; ξ)], where ξ is a random variable that follows an unknown distri-
bution D. In fact, the problem (1) is associated to many existing machine learning applications:
1)	. Robust Training DNNs over Riemannian manifold. Deep Neural Networks (DNNs) recently
have been demonstrating exceptional performance on many machine learning applications. How-
ever, they are vulnerable to the adversarial example attacks, which show that a small perturbation
in the data input can significantly change the output of DNNs. Thus, the security properties of
DNNs have been widely studied. One of secured DNN research topics is to enhance the robust-
ness of DNNs under the adversarial example attacks. To be more specific, given training data
D := {ξi = (ai, bi)}in=1, where ai ∈ Rd and bi ∈ R represent the features and label of sam-
ple ξi respectively. Each data sample ai can be corrupted by a universal small perturbation vector
y to generate an adversarial attack sample ai + y, as in (Moosavi-Dezfooli et al., 2017; Chaubey
et al., 2020). To make DNNs robust against adversarial attacks, one popular approach is to solve the
following robust training problem:
1n
minmax —	'(h(ai + y; x), bi),	(2)
x y∈Y n
1
Under review as a conference paper at ICLR 2021
where y ∈ Rd denotes a universal perturbation, and X is the weight of the neural network; h(∙; x)
is the the deep neural network parameterized by x; and '(∙) is the loss function. Here the constraint
Y = {y : kyk ≤ ε} indicates that the poisoned samples should not be too different from the original
ones.
Recently, the orthonormality on weights of DNNs has gained much interest and has been found to
be useful across different tasks such as person re-identification (Sun et al., 2017) and image classifi-
cation (Xie et al., 2017). In fact, the orthonormality constraints improve the performances of DNNs
(Li et al., 2020; Bansal et al., 2018), and reduce overfitting to improve generalization (Cogswell
et al., 2015). At the same time, the orthonormality can stabilize the distribution of activations over
layers within DNNs (Huang et al., 2018). Thus, we consider the following robust training problem
over the Stiefel manifold M:
1n
min max —	'(h(ai + y; x),bi).	(3)
x∈M y∈Y n
i=1
When data are continuously coming, we can rewrite the problem (3) as follows:
min max Eξ[f (x, y; ξ)],	(4)
x∈M y∈Y
where f(x, y; ξ) = `(h(a + y; x), b) with ξ = (a, b).
2)	. Distributionally Robust Optimization over Riemannian manifold. Distributionally robust
optimization (DRO) (Chen et al., 2017; Rahimian & Mehrotra, 2019) is an effective method to
deal with the noisy data, adversarial data, and imbalanced data. At the same time, the DRO in the
Riemannian manifold setting is also widely applied in machine learning problems such as robust
principal component analysis (PCA). To be more specific, given a set of data samples {ξi}in=1, the
DRO over Riemannian manifold M can be written as the following minimax problem:
n1
min max ∖ Vpi'(x; ξi) - kp --1∣1 2 3
x∈M p∈S	n
i=1
(5)
where P = (pi,…，Pn), S = {p ∈ Rn : PZiPi = 1,Pi ≥ 0}. Here '(χ; ξi) denotes the loss
function over Riemannian manifold M, which applies to many machine learning problems such
as PCA (Han & Gao, 2020a), dictionary learning (Sun et al., 2016), DNNs (Huang et al., 2018),
structured low-rank matrix learning (Jawanpuria & Mishra, 2018), among others. For example, the
task of PCA can be cast on a Grassmann manifold.
To the best of our knowledge, the existing explicitly minimax optimization methods such as gradient
descent ascent method only focus on the minimax problems in Euclidean space. To fill this gap, in
the paper, we propose a class of efficient Riemannian gradient descent ascent algorithms to solve the
problem (1) via using general retraction and vector transport. When the problem (1) is deterministic,
we propose a new deterministic Riemannian gradient descent ascent algorithm. When the problem
(1) is stochastic, we propose two efficient stochastic Riemannian gradient descent ascent algorithms.
Our main contributions can be summarized as follows:
1) We propose a novel Riemannian gradient descent ascent (RGDA) algorithm for the de-
terministic minimax optimization problem (1). We prove that the RGDA has a sample
complexity of O(κ2-2) for finding an -stationary point.
2) We also propose a new Riemannian stochastic gradient descent ascent (RSGDA) algorithm
for the stochastic minimax optimization. In the theoretical analysis, we prove that the
SRGDA has a sample complexity of O(κ4-4).
3) To further reduce the sample complexity, we introduce a novel momentum variance-
reduced Riemannian stochastic gradient descent ascent (MVR-RSGDA) algorithm based
on a new momentum variance-reduced technique of STORM (Cutkosky & Orabona, 2019).
We prove the MVR-RSGDA achieves a lower sample complexity of (O(κ4e-3) (please see
Table 1), which reaches near the best known sample complexity for its Euclidean counter-
parts.
4) Extensive experimental results on the robust DNN training over Stiefel manifold demon-
strate the efficiency of our proposed algorithms.
2
Under review as a conference paper at ICLR 2021
Table 1: Convergence properties comparison of our algorithms for obtaining an -stationary point
of the min-max optimization problem (1). K denotes the condition number of function f (x, ∙).
Problem	Algorithm	Learning Rate	Batch Size	Complexity
Deterministic	RGDA 一	Constant	—	O(κ26-2)
Stochastic	RsgDA	Constant	O(κ2e-2	O(κ4e-4)
	MVR-RSGDA	Decrease	。⑴	O(K9/%-3)
	MVR-RSGDAr	Decrease	O(K)	O(κ4C-3)~~
2	Related Works
In this section, we briefly review the minimax optimization and Riemannian manifold optimization
research works.
2.1	Minimax Optimization
Minimax optimization recently has been widely applied in many machine learning problems such as
adversarial training (Goodfellow et al., 2014; Liu et al., 2019), reinforcement learning (Zhang et al.,
2019; 2020), and distribution learning (Razaviyayn et al., 2020). At the same time, many efficient
min-max methods (Rafique et al., 2018; Lin et al., 2019; Nouiehed et al., 2019; Thekumparampil
et al., 2019; Lin et al., 2020; Yang et al., 2020; Ostrovskii et al., 2020; Yan et al., 2020; Xu et al.,
2020a; Luo et al., 2020; XU et al., 2020b; Bot & Bohm, 2020; HUang et al., 2020) have been proposed
for solving these minimax optimization problems. For example, Thekumparampil et al. (2019) have
proposed a class of efficient dual implicit accelerated gradient algorithms to solve smooth min-max
optimization. Lin et al. (2019) have proposed a class of efficient gradient decent ascent methods for
non-convex minimax optimization. Subsequently, accelerated first-order algorithms Lin et al. (2020)
have been proposed for minimax optimization. Xu et al. (2020b) have proposed a unified single-
loop alternating gradient projection algorithm for (non)convex-(non)concave minimax problems.
Ostrovskii et al. (2020) have proposed an efficient algorithm for finding first-order Nash equilibria
in nonconvex concave minimax problems. Xu et al. (2020a); Luo et al. (2020) have proposed a
class of fast stochastic variance-reduced GDA algorithms to solve the stochastic minimax problems.
More recently, Huang et al. (2020) have presented a class of new momentum-based first-order and
zeroth-order descent ascent method for the nonconvex strongly concave minimax problems.
2.2	Riemannian Manifold Optimization
Riemannian manifold optimization methods have been widely applied in machine learning problems
including dictionary learning (Sun et al., 2016), matrix factorization (Vandereycken, 2013), and
DNNs (Huang et al., 2018). Many Riemannian optimization methods were recently proposed. E.g.
Zhang & Sra (2016); Liu et al. (2017) have proposed some efficient first-order gradient methods
for geodesically convex functions. Subsequently, Zhang et al. (2016) have presented fast stochastic
variance-reduced methods to Riemannian manifold optimization. More recently, Sato et al. (2019)
have proposed fast first-order gradient algorithms for Riemannian manifold optimization by using
general retraction and vector transport. Subsequently, based on these retraction and vector transport,
some fast Riemannian gradient-based methods (Zhang et al., 2018; Kasai et al., 2018; Zhou et al.,
2019; Han & Gao, 2020a) have been proposed for non-convex optimization. Riemannian Adam-type
algorithms (Kasai et al., 2019) were introduced for matrix manifold optimization. In addition, some
algorithms (Ferreira et al., 2005; Li et al., 2009; Wang et al., 2010) have been studied for variational
inequalities on Riemannian manifolds, which are the implicit min-max problems on Riemannian
manifolds.
Notations: ∣∣ ∙ ∣∣ denotes the '2 norm for vectors and spectral norm for matrices. hx, y〉denotes the
inner product of two vectors X and y. For function f (x, y), f (x, ∙) denotes function w.r.t. the second
variable with fixing x, and f (∙, y) denotes function w.r.t. the first variable with fixing y. Given a
convex closed set Y, We define a projection operation on the set Y as PY(yo) = arg mi%∈γ 1 ∣∣y 一
yo∣2. We denote a = O(b) if a ≤ Cb for some constant C > 0, and the notation O(∙) hides
logarithmic terms. Id denotes the identity matrix with d dimension. The operation L denotes the
Whitney sum. Given Bt = {ζt}B=ι for any t ≥ 1,let Vfβt (x,y) = -B PB=I Vf (x,y; ξi).
3
Under review as a conference paper at ICLR 2021
(a) Retraction Rx
(b) Vector Transport Txy
Figure 1: Illustration of manifold operations.(a) A vector u in TxM is mapped to Rx(u) in M; (b)
A vector v in TxM is transported to TyM by Txyv (or Tuv), where y = Rx (u) and u ∈ TxM.
3	Preliminaries
In this section, we first re-visit some basic information on the Riemannian manifold M. In general,
the manifold M is endowed with a smooth inner product(•, % : TxM X TxM → R on tangent
space TxM for every X ∈ M. The induced norm ∣∣∙∣∣x of a tangent vector in TxM is associated
with the Riemannian metric. We first define a retraction Rx : TxM → M mapping tangent space
TxM onto M with a local rigidity condition that preserves the gradients at x ∈ M (please see
Fig.1 (a)). The retraction Rx satisfies all of the following: 1) Rx(0) = x, where 0 ∈ TxM; 2)
hVRx(0), Uix = U for U ∈ TxM. In fact, exponential mapping ExPx is a special case of retraction
Rx that locally approximates the exponential mapping Expx to the first order on the manifold.
Next, we define a vector transport T : TM L TM → TM (please see Fig.1 (b)) that satisfies
all of the following 1) T has an associated retraction R, i.e., for x ∈ M and w, U ∈ TxM, Tuw
is a tangent vector at Rx (w); 2) T0v = v; 3) Tu (av + bw) = aTuv + bTuw for all a, b ∈ R a
U, v, w ∈ TM. Vector transport Txyv or equivalently Tuv with y = Rx (U) transports v ∈ TxM
along the retraction curve defined by direction U. Here we focus on the isometric vector transport
Txy, which satisfies hU, vix = hTxyU, Txyviy for all U, v ∈ TxM.
Let Vf (x, y) = (Vxf(x, y), Vyf(x, y)) denote the gradient over the Euclidean space, and let
gradf(x,y) = (gradxf (x, y), gradyf (x, y)) = ProjTxM(Vf(x,y)) denote the Riemannian gradi-
ent over tangent space TxM, where ProjX (z) = arg minx∈X kx-zk is a projection operator. Based
on the above definitions, we provide some standard assumptions about the problem (1). Although
the problem (1) is non-convex, following (Von Neumann & Morgenstern, 2007), there exists a local
solution or stationary point (χ*,y*) satisfies the Nash Equilibrium, i.e.,
f(χ*,y) ≤ f(χ*,y*) ≤ f(χ,y*),
where x* ∈ X and y* ∈ Y. Here X ⊂ M is a neighbourhood around an optimal point x*.
Assumption 1. X is compact. Each component function f(x, y) is twice continuously differentiable
in both x ∈ X and y ∈ Y, and there exist constants L11, L12, L21 and L22, such that for every
x, x1 , x2 ∈ X and y, y1 , y2 ∈ Y, we have
kgradxf (x1, y; ξ) - Txx21gradxf(x2, y; ξ)k ≤ L11kUk,
kgradxf(x,yι; ξ) - gradxf(x,y2; ξ)k ≤ L12ky1 - y k,
kVyf(x1, y; ξ) - Vyf(x2, y; ξ)k ≤ L21kUk,
kVy f(x,yi; ξ) - Vyf(X,y2; ξ)k ≤ L22kyi -y2k,
where U ∈ Tx1 M and x2 = Rx1 (U).
Assumption 1 is commonly used in Riemannian optimization (Sato et al., 2019; Han & Gao,
2020a), and min-max optimization (Lin et al., 2019; Luo et al., 2020; Xu et al., 2020b). Here,
the terms L11, L12 and L21 implicitly contain the curvature information as in (Sato et al.,
2019; Han & Gao, 2020a). Specifically, Assumption 1 implies the partial Riemannian gradi-
ent gradxf (∙,y; ξ) for all y ∈ Y is retraction Ln-Lipschitz continuous as in (Han & Gao,
2020a) and the partial gradient Vy f (x, ∙; ξ) for all X ∈ X is L22-Lipschitz continuous as in
(Linetal., 2019). Since ∣∣gradxf(x,yi；ξ) — gradxf(χ,y2;ξ)k = ∣∣ProjτxM(Vxf(x,yι;ξ))—
4
Under review as a conference paper at ICLR 2021
ProjTxM(Vxf(X,y2;ξ))k ≤ l∣Vχf(x,yι;ξ) - Vχf(x,y2;ξ)k ≤ L12ky1 - y2k, we can ob-
tain ∣∣gradχf(χ,yι;ξ) — gradxf(χ,y2;ξ)k ≤ L12ky1 — y2k by the L12-Lipschitz ContinU-
ous of Vxf (χ, ∙; ξ) for all X ∈ X. Let the partial Riemannian gradient gradyf (∙,y; ξ)
for all y ∈ Y be retraction L21-Lipschitz, i.e., ∣gradyf(xι,y;ξ) — TX2gradyf(x2,y;ξ)∣ ≤
L21ku∣∣. Since ∣∣gradyf(xι,y;ξ) — Tx21 gradyf(x2,y;ξ)∣ = ∣∣ProjτxM(Vyf(xι,y;ξ))—
Tx ProjTx M(Vyf(X2 ,y; ξ))∣ ≤ ∣∣Vy f (xι,y; ξ)-Vy f (x2,y; ξ)∣ ≤ L21∣∣u∣∣, wehave L21 ≥ L21.
For the deterministic problem, let f(X, y) instead of f(X, y; ξ) in AssUmption 1. Since f(X, y) is
strongly concave in y ∈ Y, there exists a UniqUe solUtion to the problem maxy∈Y f(X, y) for any X.
We define the function Φ(x) = maxy∈γ f (x,y) and y* (x) = argmaxy∈γ f (x,y).
Assumption 2. The function Φ(X) is retraction L-smooth. There exists a constant L > 0, for all
X ∈ X, z = Rx(u) with u ∈ TxM, such that
Φ(z) ≤ Φ(x) + hgradΦ(x),u) + 2∣∣u∣∣2.	(6)
Assumption 3. The objective function f (x, y) is μ-strongly concave w.r.t y, i.e.,for any X ∈ M
f(χ,yι)	≤	f(x,y2)	+ hVyf(χ,y2),yι	-y2i	- μl∣yι	-y2∣2,	∀y1,y2	∈	Y.	⑺
Assumption 4. The function Φ(X) is bounded from below in M, i.e., Φ* = inf x∈M Φ(X).
Assumption 5. The variance of stochastic gradient is bounded, i.e., there exists a constant σ1 > 0
such that for all X, it follows Eξ ∣gradx f (X, y; ξ) - gradx f (X, y)∣2 ≤ σ12; There exists a constant
σ2 > 0 such that for all y, it follows Eξ ∣Vy f (X, y; ξ) - Vyf(X, y)∣2 ≤ σ22. We also define
σ = max{σ1, σ2}.
Assumption 2 imposes the retraction smooth of function Φ(X), as in Sato et al. (2019); Han & Gao
(2020b;a). Assumption 3 imposes the strongly concave of f(X, y) on variable y, as in (Lin et al.,
2019; Luo et al., 2020). Assumption 4 guarantees the feasibility of the nonconvex-strongly-concave
problems, as in (Lin et al., 2019; Luo et al., 2020). Assumption 5 imposes the bounded variance of
stochastic (Riemannian) gradients, which is commonly used in the stochastic optimization (Han &
Gao, 2020b; Lin et al., 2019; Luo et al., 2020).
4 Riemanian Gradient Descent Ascent
In the section, we propose a class of Riemannian gradient descent ascent algorithm to solve the
deterministic and stochastic minimax optimization problem (1), respectively.
4.1	RGDA and RSGDA Algorithms
In this subsection, we propose an efficient Riemannian gradient descent ascent (RGDA) algorithm to
solve the deterministic min-max problem (1). At the same time, we propose a standard Riemannian
stochastic gradient descent ascent (RSGDA) algorithm to solve the stochastic min-max problem (1).
Algorithm 1 summarizes the algorithmic framework of our RGDA and RSGDA algorithms.
At the step 5 of Algorithm 1, we apply the retraction operator to ensure the variable Xt for all t ≥ 1
in the manifold M. At the step 6 of Algorithm 1, we use 0 < ηt ≤ 1 to ensure the variable yt for all
t ≥ 1 in the convex constraint Y.
Here we define a reasonable metric to measure the convergence:
_ . - ._ , ~ .. ,, ...
Ht = ∣∣gradφ(χt)k + L∣∣yt - y (χt)∣∣,	(IO)
where L = max(1, L11, L12, L21, L22), and the first term of Ht measures convergence of the iter-
ation solutions {Xt}tT=1, and the last term measures convergence of the iteration solutions {yt}tT=1.
Since the function f(X, y) is strongly concave in y ∈ Y, there exists a unique solution y*(X) to the
problem maxy∈Y f(X, y) for any X ∈ M. Thus, we apply the standard metric ∣yt - y* (Xt)∣
to measure convergence of the parameter y. Given y = y* (Xt), we use the standard metric
∣gradΦ(Xt)∣ = ∣gradxf(Xt, y* (Xt))∣ to measure convergence of the parameter X. Note that we
use the coefficient L to balance the scale of metrics of the variable X and the variable y.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 RGDA and RSGDA Algorithms for Min-Max Optimization
1:	Input: T, parameters {γ, λ, ηt}tT=1, mini-batch size B, and initial input x1 ∈ M, y1 ∈ Y;
2:	for t = 1, 2, . . . , T do
3:	(RGDA) Compute deterministic gradients
Vt = gradχf (Xt, yt), Wt = Vy f(xt, yt);	⑻
4:	(RSGDA) Draw B i.i.d. samples {ξti * * *}iB=1, then compute stochastic gradients
1B	1B
vt = B 工gradχf(Xt,yt;ξt), wt = B 工 Vyf(Xt,yt; ξt);	⑼
5:	Update: xt+1 = Rxt (-γηtvt);
6:	Update: yt+ι = PY(yt + λwt) and yt+ι = yt + ηt(yt+ι 一 yt);
7:	end for
8:	Output: Xζ and yζ chosen uniformly random from {Xt, yt}tT=1.
Algorithm 2 MVR-RSGDA Algorithm for Min-Max Optimization
1:	Input: T, parameters {γ, λ, b, m, c1, c2} and initial input X1 ∈ M and y1 ∈ Y;
2:	Draw B i.i.d. samples B1 = {ξ1i }iB=1, then compute v1 = gradx fB1 (X1, y1) and w1 =
VyfB1(X1,y1);
3:	for t = 1, 2, . . . , T do
4:	COmPUte ηt = (m+t)i∕3 ;
5:	Update: xt+i = Rxt(-γηtvt);
6:	Update: yt+i = PY(yt + λwt) and yt+i = y + ηt(yt+ι — yt);
7:	CompUte αt+1 = c1ηt2 and βt+1 = c2ηt2 ;
8:	Draw B i.i.d. samples Bt+i = {ξti+i}iB=i, then compUte
vt+i = gradxfBt+1 (Xt+i, yt+i) + (1 一 αt+i)Txxtt+1 vt 一 gradxfBt+1 (Xt, yt),	(12)
wt+i = VyfBt+1(Xt+i,yt+i) + (1 一 βt+i) wt 一 VyfBt+1(Xt,yt) ;	(13)
9:	end for
10:	Output: Xζ and yζ chosen Uniformly random from {Xt, yt}tT=i.
4.2 MVR-RSGDA ALGORITHM
In this sUbsection, we propose a novel momentUm variance-redUced stochastic Riemannian gradient
descent ascent (MVR-RSGDA) algorithm to solve the stochastic min-max problem (1), which bUilds
on the momentUm-based variance redUction techniqUe of STORM (CUtkosky & Orabona, 2019).
Algorithm 2 describes the algorithmic framework of MVR-RSGDA method.
In Algorithm 2, we Use the momentUm-based variance-redUced techniqUe of STORM to Update
stochastic Riemannian gradient vt :
vt+i = αt+i gradxfBt+1(Xt+i,yt+i)
X----------{---------}
SGD
十 (1 一 αt+1) (gradxfBt+ι (Xt+1,yt+1) - Txxt+1 (gradxfBt+ι (Xt, yt) 一 vt))
V--------------------------{--------------------------}
SPIDER
=gradxfBt+ι(Xt+1,yt+i) + (I 一 αt+ι)TXxt+1 (vt - gradxfBt+ι(Xt,yt)),	(II)
where αt+i ∈ (0, 1]. When αt+i = 1, vt will degenerate a vanilla stochastic Riemannian gradient;
When αt+i = 0, vt will degenerate a stochastic Riemannian gradient based on variance-redUced
techniqUe of SPIDER (NgUyen et al., 2017; Fang et al., 2018). Similarly, we Use this momentUm-
based variance-redUced techniqUe to estimate the stochastic gradient wt .
6
Under review as a conference paper at ICLR 2021
5 Convergence Analysis
In this section, we study the convergence properties of our RGDA, RSGDA, and MVR-RSGDA al-
gorithms under some mild conditions. For notational simplicity, let L = max(1, L11, L12, L21,L22)
and K = L21∕μ denote the number condition of function f (x, y). We first give a useful lemma.
Lemma 1. Under the assumptions in §3, the gradient of function Φ(x) = maxy∈Y f(x, y) is re-
traction G-Lipschitz, and the mapping or function y*(x) = arg maxy∈γ f (x, y) is retraction K -
Lipschitz. Given any x1, x2 = Rx1 (u) ∈ X ⊂ M and u ∈ Tx1 M, we have:
kgradΦ(xι) - TX21 gradΦ(x2)k ≤ Gkuk,	∣∣y*(xι) — y*3)k ≤ Kkuk,	(14)
where G = κL12 + Lu and K = L21∕μ.
5.1	Convergence Analysis of both the RGDA and RSGDA Algorithms
In the subsection, we study the convergence properties of deterministic RGDA and stochastic RS-
GDA algorithms. The related proofs of RGDA and RSGDA are provided in Appendix A.1.
Theorem 1. Suppose the sequence {xt, yt}tT=1 is generated from Algorithm 1 by using deterministic
gradients. Given η = η for all t ≥ 1, 0 < η ≤ min(1, 21l), 0 < λ ≤ ɪ and 0 < γ ≤ μλ^-, we
2γL	6L	10Lκ
have
1X [kgradφ(Xt)k + Lkyt — y*(χt)k] ≤ 2Pφ(xL φ-.	(15)
T t=1	γηT
Remark 1. Since 0 < η ≤ min(1, 1^τ) and 0 < γ ≤ μλ^-, we have 0 < ηγ ≤ min( μλ^-, 1τ).
2γL	10Lκ	10Lκ 2L
Let ηγ = min(yμ∣κ, 2L), we have ηγ = O(κ2). The RGDA algorithm has convergence rate of
O(τκ∕2). By τκ∕2 ≤ a i.e., EHZ] ≤ G we choose T ≥ κ2e-2. In the deterministic RGDA
Algorithm, we need one sample to estimate the gradients vt and wt at each iteration, and need
T iterations. Thus, the RGDA reaches a sample complexity of T = O(K2-2) for finding an -
stationary point.
Theorem 2. Suppose the sequence {xt , yt}tT=1 is generated from Algorithm 1 by using stochastic
gradients. Given η = η for all t ≥ 1, 0 < η ≤ min(1, 2Yl), 0 < λ ≤ 志 and 0 < γ ≤ ιμλ-, we
have
1XE[kgradφ(Xt)k + Lkyt -n*(Xt)k] ≤ 2Pφ(x1) - C + √√2σ + 5√pσ.	(16)
T =	√Yητ	√B	√Bμ
Remark 2. Since 0 < η ≤ min(1, 2Y1L) and 0 < γ ≤ ιμλ-, we have 0 < ηγ ≤ min( ιμλ-, 2L). Let
ηγ = min(ιμλ-, 2L), we have ηγ = O(-2). Let B = T, the RSGDA algorithm has convergence
rate of O(TK2)∙ By TK2 ≤ G i.e., EHZ] ≤ G we choose T ≥ κ2e-2. In the stochastic RSGDA
Algorithm, we need B samples to estimate the gradients vt and wt at each iteration, and need
T iterations. Thus, the RSGDA reaches a sample complexity of BT = O(K4-4) for finding an
-stationary point.
5.2	Convergence Analysis of the MVR-RSGDA Algorithm
In the subsection, we provide the convergence properties of the MVR-RSGDA algorithm. The
related proofs of MVR-RSGDA are provided in Appendix A.2.
Theorem 3. Suppose the sequence {xt, yt}T=ι is generated from Algorithm 2. Given yι = y*(xι),
c1 ≥ 3b3 + 2λμ, c2 ≥ 323 + 50TT2, b > 0, m ≥ max (2, (2b)3), 0 < γ ≤ 2κL√λ+4μλ and
0 < λ ≤ 6L, we have
T XE[kgradΦ(χt)k + Lkyt-y*(χt)k] ≤ √Mm 1/6 + √M0,	(17)
t=1	T	T
where 5=max(1, c1,c2,2γL) and M0 = 2(φ(xγ)-φD + ^2^ + 乂短+^^产廿 ln(m + T).
7
Under review as a conference paper at ICLR 2021
(a) MNIST
(b) CIFAR-10
(c) CIFAR-100
Figure 2: Training loss of robust training DNNs with orthogonality regularization on weights.
200	400	600	800	1000
Iterations
(a)	MNIST
--RSGDA
--MVR-RSGDA
--SGDA
SSO-I >pea<
200	400	600	800	1000
Iterations
(b)	CIFAR-10
200	400	600	800	1000
Iterations
(c)	CIFAR-100
--RSGDA
--MVR-RSGDA
--SGDA
Figure 3: Attack loss when using uniform attack on DNNs trained by SGDA, RSGDA and MVR-
RSGDA.
T 2
Remark 3. Let ci = 3b3 + 2λμ, c2 = 3b3 + 50μ-, λ = 61T, Y = 2κL√5+4μλ and η = E I
is easy verified that Y = O(K2), λ = O⑴，λμ = O(ɪ), c1 = O⑴，c2 =O(K), m = O(κ3) and
no = O( 1). Without loss of generality, let T ≥ m = O(κ3), we have M 0 = O(κ2 + KB + KB lη(T)).
When B = κ, we have M0 = O κ2 ln(T) . Thus, the MVR-RSGDA algorithm has a convergence
rate of O(TK/3). By TK3 ≤ G i.e., E[Hζ] ≤ e, we choose T ≥ κ3e-3. In Algorithm 2, we require
B samples to estimate the stochastic gradients vt and wt at each iteration, and need T iterations.
Thus, the MVR-RSGDA has a sample ComPIexity of BT =O(κ4e-3) for finding an g-stationary
point of the problem (1). Similarly, when B = 1, the MVR-RSGDA algorithm has a convergence
rate of O(TT/3), and has a sample complexity of BT = O(κ6 * * 9/2 e-3) for finding an e-stationary
point.
Remark 4. In the about theoretical analysis, we only assume the convexity of constraint set Y,
while Lin et al. (2019) not only assume the convexity of set Y, but also assume and use its bounded
(Please see Assumption 4.2 in (Lin et al., 2019)). Clearly, our assumption is milder than (Lin et al.,
2019). When there does not exist a constraint set on parameter y, i.e.,Y = Rd, our algorithms and
theoretical results still work, while Lin et al. (2019) can‘t work.
6	Experiments
In this section, we conduct the deep neural network (DNN) robust training over the Stiefel manifold
St(r, d) = {W ∈ Rd×r : WTW = Ir} to evaluate the performance of our algorithms. In the
experiment, we use MNIST, CIFAR-10, and CIFAR-100 datasets to train the model ( More exper-
imental results on SVHN, STL10, and FashionMNIST datasets are provided in the Appendix B ).
Considering the sample size is large in these datasets, we only compare the proposed stochastic al-
gorithms (RSGDA and MVR-RSGDA) in the experiments. Here, we use the SGDA algorithm (Lin
et al., 2019) as a baseline, which does not apply the orthogonal regularization in the DNN robust
training.
6.1 Experimental Setting
Given a deep neural network h(∙; x) parameterized by X as shown in the above problem (2), the
weights of l-th layer is xi ∈ St(niln, nlout), where St(niln, nlout) is the Stiefel manifold of l-th layer.
8
Under review as a conference paper at ICLR 2021
Table 2: Test accuracy against nature images and uniform attack for MNIST, CIFAR-10, and CIFAR-
100 datasets.
Methods	Eval. Against	MNIST	CIFAR-10	CIFAR-100
RSGDA	Nat. Images	98.92%	62.87%	-29.92%-
	Uniform Attack	98.58%	61.45%	31.14%
MVR-RSGDA	Nat. Images	99.15%	67.45%	32.92%-
	Uniform Attack	99.03%	68.56%	34.69%
SGDA	Nat. Images	98.96%	76.75%	43.41%
	Uniform Attack	98.59%	55.68%	27.81%
For the weights in dense layers, niln , nlout are the number of inputs and outputs neurons. For the
weights in convolution layers, niln is the number of input channels, nlout is the product of the number
of output channels and kernel sizes. Note that the trainable parameters from other components (e.g.
batchnorm) are not in Stiefel manifold.
For both RSGDA and MVR-RSGDA algorithms, we set {γ, λ} to {1.0, 0.1}. We further set {b, m,
c1, c2} to 0.5, 8, 512, 512 for MVR-RSGDA. η in RSGDA is set to 0.01. For both algorithms, the
mini-batch size is set to 512. We set for y as 0.05 and 0.03 for the MNIST dataset and CIFAR-
10/100 datasets. The above settings are the same for all datasets. An 8-layer (5 convolution layers
and 3 dense layers) deep neural network is used in all experiments. All codes are implemented with
McTorch (Meghwanshi et al., 2018) which is based on PyTorch (Paszke et al., 2019).
6.2 Experimental Results
The training loss plots of the robust training problem in the above Eq. (2) are shown in Fig. 2. From
the figure, we can see that MVR-RSGDA enjoys a faster convergence speed compared to the baseline
RSGDA. It’s also clear that when the dataset becomes complicate (from MNIST to CIFAR-10/100),
the advantage of MVR-RSGDA becomes larger.
When it comes to robust training, the training loss is not enough to identify which algorithm is
better. We also use a variant of uniform perturbation to attack the model trained by our algorithms.
We follow the design of uniform attack in previous works (Moosavi-Dezfooli et al., 2017; Chaubey
et al., 2020), and the detail uniform attack objective is shown below:
1n
min — T max 瓜(y + ai) - max hj(y + &i), 0),	s.t. Y = {kyk∞ ≤ ε}
y∈Y n	i	j6=bi
i=1
where hj is the j -th logit of the output from the deep neural network, and y here is a uniform
permutation added for all inputs. In practice, we sample a mini-batch with 512 samples at each
iteration. The optimization of the uniform permutation lasts for 1000 iterations for all settings.
The attack loss is presented in Fig 3. The attack loss for the model trained by MVR-RSGDA is
higher compared to both RSGDA and SGDA, which indicates the model trained by MVR-RSGDA
is harder to attack and thus more robust. The test accuracy with natural image and uniform attack is
shown in Tab. 2, which also suggests the advantage of MVR-RSGDA. More results are provided in
Appendix B.
7 Conclusion
In the paper, we investigated a class of useful min-max optimization problems on the Riemanian
manifold. We proposed a class of novel efficient Riemanian gradient descent ascent algorithms to
solve these minimax problems, and studied the convergence properties of the proposed algorithms.
For example, we proved that our new MVR-RSGDA algorithm achieves a sample complexity of
O(κ4E-3) without large batches, which reaches near the best known sample complexity for its EU-
clidean counterparts.
References
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regular-
izations in training deep networks? In Advances in Neural Information Processing Systems, pp.
4261-4271,2018.
9
Under review as a conference paper at ICLR 2021
RadU Ioan Bot and Axel Bohm. Alternating proximal-gradient steps for (stochastic) nonconvex-
concave minimax problems. arXiv preprint arXiv:2007.13605, 2020.
AshUtosh ChaUbey, Nikhil Agrawal, Kavya Barnwal, Keerat K GUliani, and Pramod Mehta. Uni-
versal adversarial pertUrbations: A sUrvey. arXiv preprint arXiv:2005.08087, 2020.
Robert S Chen, Brendan LUcier, Yaron Singer, and Vasilis Syrgkanis. RobUst optimization for non-
Convex objectives. In Advances in Neural Information Processing Systems, pp. 4705-4714, 2017.
Michael Cogswell, FarUk Ahmed, Ross Girshick, Larry Zitnick, and DhrUv Batra. RedUcing overfit-
ting in deep networks by decorrelating representations. arXiv preprint arXiv:1511.06068, 2015.
Ashok CUtkosky and Francesco Orabona. MomentUm-based variance redUction in non-convex sgd.
In Advances in Neural Information Processing Systems, pp. 15210-15219, 2019.
Cong Fang, Chris JUnchi Li, ZhoUchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018.
Orizon Pereira Ferreira, LR Lucambio Perez, and Sandor Z Nemeth. Singularities of monotone
vector fields and an extragradient-type algorithm. Journal of Global Optimization, 31(1):133-
151, 2005.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Andi Han and Junbin Gao. Riemannian stochastic recursive momentum method for non-convex
optimization. arXiv preprint arXiv:2008.04555, 2020a.
Andi Han and Junbin Gao. Variance reduction for riemannian non-convex optimization with batch
size adaptation. arXiv preprint arXiv:2007.01494, 2020b.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order momentum
methods from mini to minimax optimization. arXiv preprint arXiv:2008.08170, 2020.
Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li. Orthogonal
weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep
neural networks. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pp. 3271-3278.
AAAI Press, 2018.
Pratik Jawanpuria and Bamdev Mishra. A unified framework for structured low-rank matrix learn-
ing. In International Conference on Machine Learning, pp. 2254-2263, 2018.
Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic recursive gradient
algorithm. In International Conference on Machine Learning, pp. 2516-2524, 2018.
Hiroyuki Kasai, Pratik Jawanpuria, and Bamdev Mishra. Riemannian adaptive stochastic gradient
algorithms on matrix manifolds. arXiv preprint arXiv:1902.01144, 2019.
Chong Li, Genaro Lopez, and Victoria Martln-Marquez. Monotone vector fields and the proximal
point algorithm on hadamard manifolds. Journal of the London Mathematical Society, 79(3):
663-683, 2009.
Jun Li, Li Fuxin, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold
via the cayley transform. arXiv preprint arXiv:2002.01113, 2020.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. arXiv preprint arXiv:1906.00331, 2019.
Tianyi Lin, Chi Jin, Michael Jordan, et al. Near-optimal algorithms for minimax optimization. arXiv
preprint arXiv:2002.02417, 2020.
10
Under review as a conference paper at ICLR 2021
Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, and Tianbao
Yang. Towards better understanding of adaptive gradient algorithms in generative adversarial
nets. arXiv preprint arXiv:1912.11940, 2019.
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order
methods for geodesically convex optimization on riemannian manifolds. In Advances in Neural
Information Processing Systems,pp. 4868-4877, 2017.
Luo Luo, Haishan Ye, and Tong Zhang. Stochastic recursive gradient descent ascent for stochastic
nonconvex-strongly-concave minimax problems. arXiv preprint arXiv:2001.03724, 2020.
Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai, and Bamdev
Mishra. Mctorch, a manifold optimization library for deep learning. arXiv preprint
arXiv:1810.01811, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1765-1773, 2017.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning, pp. 2613-2621, 2017.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving
a class of non-convex min-max games using iterative first order methods. In Advances in Neural
Information Processing Systems, pp. 14934-14942, 2019.
Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efficient search of first-order nash
equilibria in nonconvex-concave smooth min-max problems. arXiv preprint arXiv:2002.07919,
2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060,
2018.
Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
preprint arXiv:1908.05659, 2019.
Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi
Hong. Nonconvex min-max optimization: Applications, challenges, and recent theoretical ad-
vances. IEEE Signal Processing Magazine, 37(5):55-66, 2020.
Hiroyuki Sato, Hiroyuki Kasai, and Bamdev Mishra. Riemannian stochastic variance reduced gra-
dient algorithm with retraction and vector transport. SIAM Journal on Optimization, 29(2):1444-
1472, 2019.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere ii: Recovery
by riemannian trust-region method. IEEE Transactions on Information Theory, 63(2):885-914,
2016.
Yifan Sun, Liang Zheng, Weijian Deng, and Shengjin Wang. Svdnet for pedestrian retrieval. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 3800-3808, 2017.
Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algorithms
for smooth minimax optimization. In Advances in Neural Information Processing Systems, pp.
12680-12691, 2019.
11
Under review as a conference paper at ICLR 2021
Bart Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal on
Optimization, 23(2):1214-1236, 2013.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior (commemo-
rative edition). Princeton university press, 2007.
JH Wang, G Lopez, Victoria Martln-Marquez, and Chong Li. Monotone and accretive vector fields
on riemannian manifolds. Journal of optimization theory and applications, 146(3):691-708,
2010.
Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution
for training extremely deep convolutional neural networks with orthonormality and modulation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6176-
6185, 2017.
Tengyu Xu, Zhe Wang, Yingbin Liang, and H Vincent Poor. Enhanced first and zeroth order variance
reduced algorithms for min-max optimization. arXiv preprint arXiv:2006.09361, 2020a.
Zi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A unified single-loop alternating gradient
projection algorithm for nonconvex-concave and convex-nonconcave minimax problems. arXiv
preprint arXiv:2006.02032, 2020b.
Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Sharp analysis of epoch stochastic
gradient descent ascent methods for min-max optimization. arXiv preprint arXiv:2002.05309,
2020.
Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced optimization
for a class of nonconvex-nonconcave minimax problems. arXiv preprint arXiv:2002.09621, 2020.
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Con-
ference on Learning Theory, pp. 1617-1638, 2016.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian svrg: Fast stochastic optimization on
riemannian manifolds. In Advances in Neural Information Processing Systems, pp. 4592-4600,
2016.
Jingzhao Zhang, Hongyi Zhang, and Suvrit Sra. R-spider: A fast riemannian stochastic optimization
algorithm with curvature independent rate. arXiv preprint arXiv:1811.04194, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer BaSar Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
Kaiqing Zhang, Sham M Kakade, Tamer BasSar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020.
Pan Zhou, Xiaotong Yuan, Shuicheng Yan, and Jiashi Feng. Faster first-order methods for stochastic
non-convex optimization on riemannian manifolds. IEEE transactions on pattern analysis and
machine intelligence, 2019.
12
Under review as a conference paper at ICLR 2021
A Appendix
In this section, we provide the detailed convergence analysis of our algorithms. We first review some
useful lemmas.
Lemma 2. (Nesterov, 2018) Assume that f(x) is a differentiable convex function and X is a convex
set. x* ∈ X is the solution ofthe constrained problem minχ∈χ f (x), if
hVf (x*), X — x*i ≥ 0, X ∈ X.	(18)
Lemma 3. (Nesterov, 2018) Assume the function f(x) is L-smooth, i.e., kVf (x) — Vf(y)k ≤
Lkx — yk, and then the following inequality holds
If(y) — f(x) — Vf(x)T(y — x)| ≤ L∙∣∣x — yk2.	(19)
Next, based on the above assumptions and Lemmas, we gives some useful lemmas:
Lemma 4. The gradient of function Φ(x) = maxy∈Y f(x, y) is retraction G-Lipschitz, and the
mapping or function y* (x) = arg maxy∈Y f(x, y) is retraction κ-Lipschitz. Given any x1, x2 =
Rx1 (u) ∈ X ⊂ M and u ∈ Tx1 M, we have
kgradΦ(x1) — Txx21gradΦ(x2)k ≤ Gkuk,
ky*(x1) — y*(x2)k ≤ κkuk,
where G = κL12 + Lu and K = L21∕μ, and vector transport TXχ21 transport the tangent space of
x1 to that ofx2.
Proof. Given any x1, x2 = Rx1 (u) ∈ X and u ∈ Tx1 M, define y* (x1) = arg maxy∈Y f(x1, y)
and y*(x2) = arg maxy∈Y f(x2, y), by the above Lemma 2, we have
(y — y*(x1))T Vyf(x1, y*(x1)) ≤0,	∀y∈Y	(20)
(y — y*(x2))T Vyf(x2, y*(x2)) ≤ 0,	∀y ∈ Y.	(21)
Let y = y*(x2) in the inequality (20) and y = y*(x1) in the inequality (21), then summing these
inequalities, we have
(y*(χ2) — y*(χι))T(vy f (Xι,y*(XI))-Vyf(X2,y*(XZ))) ≤ 0.	(22)
Since the function f (xi, ∙) is μ-strongly concave, We have
f (Xι,y*(XI)) ≤ f (Xι,y*(X2)) + (Vyf(Xι,y*(X2)))T(y*(XI)- y*(X2)) - 2ky*(Xι) - y*(X2)『,
(23)
f(Xi,y*(X2)) ≤ f(Xi,y*(XI)) + (Vyf(xi"(XI)))T(y*(X2)- y*(XI))- 2ky*(XI)-y*(X2)k2.
(24)
Combining the inequalities (23) With (24), We obtain
(y*(χ2) - y*(χI))T(Vyf(Xι,y*(χ2)) - Vyf(Xι,y*(XI))) + μky*(XI)-y*(x2)k2 ≤ 0. (25)
By plugging the inequalities (22) into (25), We have
μky*(XI)-y*(χ2)k2 ≤ (y*(χ2) -y*(χι))T(Vyf(χ2,y*(χ2)) - Vyf(XI,y*(χ2)))
≤ ky* (X2) — y*(X1)kkVyf(X2, y*(X2)) —Vyf(X1,y*(X2))k
≤ L21kukky*(X2) — y*(X1)k,	(26)
Where the last inequality is due to Assumption 1. Thus, We have
ky*(X1) — y*(X2)k ≤ κkuk,	(27)
where K = L21∕μ and χ2 = Rχι (u), U ∈ TXiM.
13
Under review as a conference paper at ICLR 2021
Since Φ(χ) = f (x, y*(χ)), We have gradΦ(χ) = gradχf (x, y*(χ)). Then We have
kgradΦ(x1)-Txx21gradΦ(x2)k
=IIgradxf(Xι,y* (XI))-TX21 gradχf (X2,y*(Xz))Il
≤ ∣∣gradxf(χι,y*(XI))-gradxf(χι,y*(χ2))k + kgradxf(χι,y*(χ2))-TxX21 gradxf(χ2,y*(x2))k
≤ L12∣∣y*(X1) - y*(X2)k + L11∣∣u∣∣
≤ (κL12 + L11)IuI,	(28)
Where u ∈ Tx1 M.
□
Lemma 5. Suppose the sequence {Xt , yt }tT=1 is generated from Algorithm 1 or 2. Given 0 < ηt ≤
2YlL, we have
Φ(Xt+ι) ≤ Φ(Xt) + γL12ηtky*(Xt) - ytk2 + γηtkgradxf (Xt,yt) - Vtk2 - YntkgradΦ(Xt)∣2
-γ4t ∣vtk2.
(29)
Proof. According to Assumption 2, i.e., the function Φ(X) is retraction L-smooth, We have
γ2η2 L
Φ(Xt+ι) ≤ Φ(Xt) - YnthgradΦ(Xt), Vti + ɪ-2— ∣∣vt∣2	(30)
=Φ(Xt) + 斗∣gradΦ(Xt) - vtk2 -乎kgradΦ(Xt)k2 + (乎-乎)∣vtk2
=Φ(Xt) + Ynt∣∣gradΦ(Xt) - gradxf (Xt, yt) + gradxf (Xt,yt) - vt∣2 - Ynt∣∣gradΦ(Xt)k2
22
+ (* -乎)kvtk2
≤ Φ(Xt) + YntkgradΦ(Xt) -gradxf(Xt,yt)∣2 + Yntkgradxf(Xt,yt) - vt∣2 - Ynt∣∣gradΦ(Xt)k2
+ (亨-乎)∣vt∣2
≤ Φ(Xt) + YntkgradΦ(Xt) -gradxf(Xt,yt)∣2 + Yntkgradxf(Xt,yt) - vt∣2 - Ynt|怕"4虫5)『
-子MH
where the last inequality is due to 0 < nt ≤ ^^.
Consider an upper bound of kgradΦ(Xt) - gradxf(Xt, yt)k2, We have
kgradΦ(Xt) - gradxf (Xt,yt)k2 = kgradxf (Xt,y*(Xt)) - gradxf (Xt,yt)k2
≤ L12ky*(Xt)- ytk2.	(31)
Then We have
Φ(Xt+ι) ≤ Φ(Xt) + YntL12ky*(Xt) - ytk2 + Yntkgradxf(Xt,yt) - Vtk2 - YntkgradΦ(Xt)k2
-24t ∣vtk2.
(32)
□
Lemma 6. Suppose the sequence {Xt, yt}tT=1 is generated from Algorithm 1 or 2. Under the above
assumptions, and set 0 < nt ≤ 1 and 0 < λ ≤ 十,we have
kyt+ι - y*(Xt+ι)∣∣2 ≤ (I - nt4μ-)kyt - y*(Xt)k2 - ɪkyt+ι - yt∣∣2
,25ntλ	2	25Y2Mnt	2
十 丁 kVy f (Xt，yt) - wtk + 6〃λ kvtk ,
(33)
where K = L21∕μ.
14
Under review as a conference paper at ICLR 2021
Proof. According to the assumption 3, i.e., the function f (x,y) is μ-strongly concave w.r.t y, we
have
f(χt,y) ≤ f(χt,yt) + Byf (χt,yt),y — IyG — 2Ily — ytk2
=f (Xt) yt) +〈wt, y — yt+ιi + Ny f(χt, yt) — WtJy — yt+ιi
+ "yf (Xt, yt), yt+ι — Iyt- 2 ∣∣y — yt∣∣2∙	(M)
According to the assumption 1, i.e., the function f (x, y) is L22-smooth w.r.t y, and L ≥ L22, We
have
f(χt,yt+1) — f (χt,yt) — "y f(χt,yt),yt+1 — yti ≥--------22 ∣∣yt+1 — yt ||2
f
≥ — 2 llyt+ι — ytl.
Combining the inequalities (34) with (35), we have
f(χt, y) ≤ f (χt, yt+ι) + hwt, y — yt+ιi + Nyf(Xt, yt) — Wt,y — yt+ιi
，，	f.
—2 ∣∣y — yt∣+ 2 llyt+ι — yt∣.
(35)
(36)
According to the step 6 of Algorithm 1 or 2, we have yt+ι = PY (yt + λwt) = arg mi∏y∈γ ɪ ∣∣y —
yt — λwt∣∣2. Since Y is a convex set and the function ɪ∣∣y — yt — λwt∣∣2 is convex, according to
Lemma 2, we have
hyt+1 — yt — λwt, y — yt+ιi ≥ 0, y ∈Y∙
Then we obtain
hwt, y — yt+1i ≤ γhyt+1 — yt, y — yt+1i
λ
=vhyt+1 — yt, yt — yt+1i + ∖(yt+1 — yt, y — yti
人	入
=—ʌ llyt+ι — ytk2 + ʌ hyt+ι — yt, y — yti∙
Combining the inequalities (36) with (38), we have
f(χt,y) ≤ f(χt,yt+ι) + 1 hyt+ι — yt,y — yti + Cyf (χt,yt) — wt,y — yt+ιi
1	,,	T
一ʌ llyt+ι — ytl∣2 — 2 Ily - ytll2 + 2 llyt+ι — ytll'
(37)
(38)
(39)
Let y = y* (Xt) and we obtain
f(χt,y*(Xt)) ≤ f(χt,yt+ι) + 1 hyt+ι — yt,y*(Xt) — yti + hVyf(χt,yt) — wt,y*(Xt) — yt+ιi
入
1	,,	T
一 Mkyt+1 -yt『一 5∣ly*(χt) — yt『+ 5llyt+1 — yt『.	(40)
λ	/	/
Due to the concavity of f (∙, y) and y*(χt) = argmaxy∈γ f (χt,y), we have f (χt,y*(χt)) ≥
f (χt, yt+ι). Thus, we obtain
0 ≤ 1 hyt+ι — yt,y*(Xt) — yti + hvyf(χt,yt) — wt,y*(Xt) — yt+ιi
1 f	,,
一 (^ 一 ^2)lyt+ι 一 yt『一 2Hy*(Xt) — yt『•
(41)
By yt+1 = yt + ηt(yt+ι — yt), We have
l∣yt+ι — y*(χt)∣∣2 = ∣∣yt+ ηt(yt+ι — yt) — y*(χt)∣∣2
=l∣yt	— y*(χt)ll2 + 2ηthyt+ι	—	yt,yt —	y*(χt)i	+ *lyt+ι —	ytll2∙	(42)
15
Under review as a conference paper at ICLR 2021
Then we obtain
hyt+ι - yt,y*(xt) - yti ≤ $一Ilyt - y*(xt)ll2 + kykyt+ι - ytk2 - 5—llyt+ι - y*(xt)ll2∙ (43)
2ηt	2	2ηt
Consider the upper bound of the term Ry f (χt,yt) 一 Wt ,y*(χt) 一 yt+ιi, we have
〈▽yf (χt,yt) - wt,y*(χt) - yt+ιi
=Nyf (Xt,yt) - Wt,y*(χt) - yti + Nyf (Xt,yt) - WtJyt - yt+ιi
≤ yk^yf (Xt)	yt) -	Wtll2	+	4lly*(χt)	- yt∣∣2 十	丁INyfIXtJyt) -	wt∣∣2	+	4Ilyt	-	yt+ι∣∣2
tʃ	，4	tʃ	，4
=Ik Vyf (Xt) yt) - Wtll2 + 4|y*(Xt)- ytIl2 + 4kyt - yt+iH2∙	(44)
μ	4	4
By plugging the inequalities (41), (43) to (44), we have
1	1	S	，，	亍	1
i^^^kyt+ι - y*(Xt)II2 ≤ (^~~ʌ - 7)kyt - y*(χt)12 + Cττ + 7+9- - M)kyt+ι - yt!
2ηtλ	2ηtλ	4	2λ	4	2	λ
2
T---INy f (Xt) yt) - Wtll
从
1	Q T 1	ɔ
≤ (2nɪʌ - 4)Iyt - y*(χt)l∣2 + (^— 2λ)Iyt+i - yt1 + μIVyf(χt,yt) - WtIl2
=(21λ - 4)Iyt - y*(Xt)『-(8λ +8λ - 3L)帧+1 - yt『
2
+ μ INyf (Xt) yt) - Wtll
≤ (5一: - 7)lyt - y*(χt)l∣2 -嬴惊+1 - yt∣∣2 T	IVyf (Xt) yt) - WtIl2)
2ηtλ	4	8λ	μ
(45)
where the second inequality holds by L ≥ L22 ≥ μ and 0 < ηt ≤ 1, and the last inequality is due
to 0 < λ ≤ 志.It implies that
llyt+ι - y*(χt)ll2 ≤ (1 - "2 )llyt - y*(χt)ll2	4tllyt+ι - ytll2 +	l∣Vyf(Xt) yt) - WtIl2∙
(46)
Next, we decompose the term ∣∣yt+ι - y*(χt+ι)∣∣2 as follows:
l∣yt+ι — y*(χt+ι)∣∣2 = ∣∣yt+ι — y*(χt) + y*(χt) — y*(χt+ι)∣∣2
=l∣yt+ι — y*(χt)∣∣2 + 2hyt+ι — y* (Xt))y* (Xt) — y*(χt+ι)i + ∣∣y*(χt) — y*(χt+ι)∣∣2
≤ (1 + ηtμ~)lyt+ι - y*(χt)ll2 + (1T	^)lly*(χt) - y*(χt+ι)∣∣2
4	ηtμλ
≤(1+半)"-y*(Xt)l2+(1+煮ll l2,
(47)
where the first inequality holds by the Cauchy-Schwarz inequality and Young,s inequality, and the
last equality is due to Lemma 4.
By combining the above inequalities (46) and (47), we have
lyt+ι - y*(χt+1)∣∣2 ≤ (1 + 半)(1 -竽)∣∣yt - y*(χt)∣∣2 - (1 + 竽)号|厮+1 - ytl2
+ (1 + ηtτ~)~η~INyf (Xt) yt) - WtIl2 + (1T	-r)η272κ2IlvtIl2.
4 μ	ηtμλ
(48)
16
Under review as a conference paper at ICLR 2021
Since 0 <η ≤ 1, 0 < λ ≤ ^r
6L
we obtain
(1 + *)(1-吟)=
1 7 T 、	1	∖∕1∕1	1	∕T∕1 El
and L ≥ L22 ≥ μ, We have λ ≤ 6L ≤ 6μ and η ≤ 1 ≤ 血.Then
1 - ηtμλ + ηtμλ - η2μ2λ2 ≤ 1 - ηtμλ
2~ 十 ^1	8 — 一 厂
-(1 + * 评
V 3ηt
≤------
一	4
(1 + 处)皿 ≤(1 + LU = 2
I 4 μ — I 24， μ	6μ ,
4	2 2 2	2 2 2 4γ2κ2ηt	γ2κ2ηt 4γ2κ2ηt	25γ2κ2ηt
(I + —^)Y Kn = γ κ ηt + ——ʌ— ≤ -ɪʒ- + ——ʌ- = —τrʒ—
ntμλ	μλ	6μλ μλ	6μλ
Thus We have
kyt+ι - y*(Xt+1)k2 ≤ (I - nt4-)kyt - y*(Xt)k2 —nkyt+ι - ytIl2
+ 25μλkVyf(xt,yt) - wtk2 + 25γκnt∣vtk2.
(49)
(50)
□
A. 1 Convergence Analysis of RGDA and RS GDA Algorithms
In the subsection, We study the convergence properties of deterministic RGDA and stochastic RS-
GDA algorithms, respectively. For notational simplicity, let L = max(1, L11, L12, L21, L22).
Theorem 4.	Suppose the sequence {Xt, yt}tT=1 is generated from Algorithm 1 by using deterministic
gradients. Given n = n for all t ≥ 1, 0 < n ≤ min(1, 21L), 0 < λ ≤ ɪ and 0 < γ ≤ -λ^-, we
2γL	6L	10Lκ
have
TX [Ll∣yt- y*(xt)k + Ilgradφ(χt)k] ≤
t=1
2,Φ(xι) 一 Φ*
ZYnT
(51)
Proof. According to Lemma 6, We have
kyt+ι-y*(Xt+1)k2 ≤	(1-	ntμ-)kyt	-y*(Xt)∣∣2 —ntιιyt+ι-	ytιι2 H	6μ-ι∣vyf(xt,yt)	-wt∣ι2
l 25Y2κ2nt∣∣	∣∣2	O
+	6μλ kvtk .	(52)
We first define a Lyapunov function Λt, for any t ≥ 1
Λt = φ(χt) + 6γL-kyt - y*(χt)∣∣2.	(53)
λμ
According to Lemma 5, We have
At+1 - Λt = φ(χt+ι) - φ(χt)+	I∣yt+1 - y*(χt+ι)∣∣2 -∣∣yt - y*(χt)∣∣2)
≤ γntLi2kyt - y*(xt)∣∣2 + Yntkgradxf(Xt,yt) - vt∣∣2 - Ynt∣∣gradφ(Xt)∣∣2 - γntMil2
2
+ 6yl- ( - 2 kyt - y*(Xt)∣∣2 -迎 kyt+1 - ytk2 + 25λnt kVf(Xt,yt) - wtk2
t	t	t+1	t	y t, t t
l 25Y2Mntu ll2λ
+ Fk kvtk )
≤ -Lγntkyt - y*(xt)k2 - ɪkgradΦ(χt)k2 - ^Lnkyt+1 - ytk2
2	2	2λμ
T 4- 25⅞L^- )γntkvtk2
≤ -L2Yntkyt - y*(xt)k2 -乎kgradΦ(xt)k2,
(54)
17
Under review as a conference paper at ICLR 2021
where the first inequality holds by the inequality (52); the second last inequality is due to L =
max(1, L11,L12,L21,L22) and Vt = gradxf (xt, yt), Wt = Ry f(xt, yt), and the last inequality is
due to 0 < γ ≤ μλ^.. Thus, We obtain
10Lκ
〜2"t ∣∣yt - y*(Xt)k2 + -ɪt kgradφ(Xt)k2 ≤ Λt - At+ι.	(55)
Since the initial solution satisfies yι = y*(xι) = argmaxy∈γ f (xι,y), we have
Λι = Φ(χι) + 6γL- ∣∣yι — y"(χι)k2 = Φ(χι).	(56)
λμ
Taking average over t = 1,2,…，T on both sides of the inequality (55), we have
T XX [Lnkyt —y*(χt)k2 + ηtkgradΦ(χt)k2] ≤ AI-T≡ ≤ ①(XIYT ①*,	(57)
where the last equality is due to the above equality (56) and Assumption 4. Let η = ηι =…=ητ,
we have
T XX [L2kyt-y*(xt)k2 + kgradΦ(xt)k2] ≤ '©:T— "' .	(58)
According to Jensen’s inequality, we have
T XX [L kyt—y*(xt)k + kgradΦ(xt)k]
t=1
yt — y*(xt)k2 + ∣∣gradΦ(χt)k
1/2
4(Φ(xι) — Φ*))1∕2 _ 2pΦ(x1) — Φ*
γηT )	√YηT
□
Theorem 5.	Suppose the sequence {Xt , yt}tτ=1 is generated from Algorithm 1 by using stochastic
gradients. Given η = ηt for all t ≥ 1, 0 < η ≤ min(1, 21l), 0 < λ ≤ ɪ and 0 < γ ≤ μλ^-, we
2γL	6L	10Lκ
have
1 χτΛτπl--ll	*/ . ll ll ,工/ , ll-l	2pΦ(xι) — Φ*	√2σ	5√2Lσ
T J^e[lllyt - y (Xt)Il + Ilgradφ(χt)∣∣] ≤----------√γηT------------+ √b + √Bμ .	(60)
Proof. According to Lemma 6, we have
kyt+ι - y*(xt+ι)ll2 ≤ (1 - 小4 )llyt - y*(xt)ll2	4tllyt+ι - ytll2 +	6μt-l∣Vyf(χt, yt) - wtll2
25γ2κ2ηt	2
+ —Trʒ—Ilvtl .	(61)
6μλ
We first define a Lyapunov function Θt, for any t ≥ 1
Θt = E[Φ(χt) + 6γL- kyt — y*(χt)k2].	(62)
By Assumption 5, we have
1 B	σ-
Ekgradxf(χt,yt) —vtl2 = Ekgradxf(χt,yt) — B Xgradχf (xt,yt;ξt)l2 ≤ b,	(63)
i=1
EkVyf(Xt,yt) — Wtk2 = EkVyf(Xt,yt) — ɪ X Vyf(Xt,yt; ξi)k2 ≤ σ^-	(64)
BB
i=1
18
Under review as a conference paper at ICLR 2021
According to Lemma 5, we have
θt+ι - θt = E[φ(Xt+1)]—E[φ(Xt)] +	λμ~ (Ellyt+ι — y*(Xt+I)Il2—Ellyt- y*(xt)k2)
≤ γηtL12Ekyt — y*(xt)k2 + YntEkgradxf(xt,yt) — Vtk2 — γηtEkgradΦ(xt)k2 — 平∣∣vt∣∣2
2
+ 6γL- (— 学 Ekyt- y*(χt)k2 — 半 E∣yt+ι — ytk2 + 辞 EkVyf (χt,yt) — Wtk
l 25γ2κ2nt ll ll2λ
+ ^μτ-kvtk)
≤ — LYnt Ekyt- y*(χt)k2 一 子 EkgradΦ(χt)k2 — ^Ln Ekyt+1 - ytk2
2	2	2λμ
— (1 — 25μ2λ2γ )γnt∣∣vtIl2 + YntEkgradxf(Xt,yt) — vt∣∣2 + 25：2YntEkVyf(Xt,yt) — wt∣∣2
≤ — 亨Ekyt- y*(Xt)k2 - 苧EkgradΦ(Xt)k2 + 雪 + ^*2,	(65)
2	2	B	Bμ2
where the first inequality holds by the inequality (61); the second last inequality is due to L =
max(1, L11,L12,L21,L22), and the last inequality is due to 0 < γ ≤ yμλκ and Assumption 5.
Thus, we obtain
⅛Ekyt- y*(Xt)k2 + 乎EkgradΦ(Xt)k2 ≤ Θt - Θt+ι + 乍 + ⅛σ2.	(66)
2	2	B	Bμ2
Since the initial solution satisfies yι = y*(∕ι) = argmaxy∈γ f (Xι,y), We have
Θι = Φ(xi) + 6γ^ kyι — y*(X1)k2 = Φ(xi).	(67)
λμ
Taking average over t = 1,2,…，T on both sides of the inequality (66), we have
1 GLL∖nt2nt∣∣..	..*∕~ >∣∣2 ,	nr…吊/~、u2i	∕θt- θt+ι l 1G ntσ2	1 G 25L2ntσ2
TXE[Fk%-y (Xt)k2 +	yl∣gradφ(Xt)Il2]	≤	—γτ— + TtF +	Tk Bμ2
_ Φ(xi) - Φ*	1_ X ntσ2	ɪ X 25L2ntσ2
= YT + T 乙 BeT + T 乙	Bμ2一
t=1	t=1
(68)
where the last equality is due to the above equality (67). Let n = nι =…=nτ, we have
T
-XE[L2kyt — y*(Xt)k2 + kgradΦ(Xt)k2] ≤ M (XI)r-~) + σ- + 25p ： .	(69)
T / / L ∣∣^t y t t/ Ii ，∣∣o V t/ Ii J — YnT ， B ，	Bμ2	\ /
According to Jensen’s inequality, we have
TT
T XE[Lkyt -y*(Xt)k + kgradΦ(Xt)k] ≤ (T XE[L2kyt - y*(Xt)k2 + kgradΦ(Xt)k2)1/2
t=1	t=1
4(Φ(X1) — Φ*) 2σ2	5θL2σ2)1/2
YnT + ɪ + Bμ2 '
2pΦ(X1) - Φ*	√2σ	5√2Lσ
―------------1———+....——,
“^T	√B √Bμ
(70)
where the last inequality is due to (a1 + a2 + a3)1/2 ≤ a11/2 + a12/2 + a13/2 for all a1, a2, a3 > 0.
□
19
Under review as a conference paper at ICLR 2021
A.2 Convergence Analysis of THE MVR-RSGDA Algorithm
In the subsection, we study the convergence properties of the MVR-RSGDA algorithm. For nota-
tional simplicity, let L = max(L11, L12, L21, L22,1).
Lemma 7. Suppose the stochastic gradients Vt and Wt is generated from Algorithm 2, given 0 <
αt+ι ≤ 1 and 0 < βt+ι ≤ 1, we have
EIlgradXf(Xt+1,yt+1) - vt+1∣∣2 ≤ (1 - αt+1)2E∣∣gradXf(Xt,yt) - vt∣∣2 +4(1 - αt+ι)2L2ιγ2*∣vt∣∣2
+ 4(1 -四+I)2L22η2∣∣yt+ι - yt∣∣2 +---t+一∙	(71)
B
E∣vyf(χt+1,yt+1) - wt+1∣∣2 ≤ (1 - βt+1)2E∣Uf (χt,yt) - w∕∣2 +4(1 - βt+ι)2L2172η2∣∣vtll2
+ 4(1 - βt+1)2L22η2kyt+1 - ytk2 + 't+一.	(72)
B
Proof. We first prove the inequality (71). According to the definition of Vt in Algorithm 2, we have
vt+1 - Txt+1 Vt = -αt+1Txt+1 Vt + (1 - αt+1) (gradXfBt十1 (xt+1,yt+I)-Txt+1 gradXfBt十1 (xt,yt力
+ αt+1gradχfBt+1 (xt+1, yt+1).	(73)
Then we have
EkgradXf(Xt+1,yt+1) - vt+1∣∣2	(74)
=EkgradXf(Xt+1,yt+I)-TXt 十1 Vt-(Vt+1 - TXXt+1 Vt)|2
=EkgradXf(Xt+1,yt+1) - TXt+1 Vt + αt+1TXt+1 Vt - at+1gradXfBt+1 (Xt+1,yt+1)
-(1 - 0t+1)(gradXfBt+1 (Xt+1,yt+1) -TXt+1 gradXfBt+1 (Xt,yt))k2
=Ek(I - 0t+1)Txt+1(gradXf(Xt,yt) - Vt) + (1 - 0t+1)(gradXf (Xt+1,yt+1) -TXt+1 gradXf (Xt,yt)
-gradXfBt+1 (Xt+1,yt+1) + Txtt+1 gradXfBt+1 (Xt, yt))
+ αt+1 (gradXf (Xt+1, yt+1) - gradXfBt+1 (±t+1, yt+1)) k2
=(1 - 0t+1)2EkgradXf(Xt, yt) - Vt||2 + 02+1EkgradXf(Xt+1,yt+1) - gradXfBt+1 (Xt+1,yt+1 )∣∣2
+ (1 - 0t+1)2EkgradXf(Xt+1,yt+1) -Txt+1 gradXf (Xt,yt) - gradXfBt+1 (Xt+1,yt+1)
+ TXt+1 gradXfBt+1 (Xt,yt)k2+ 2αt+1(1 - αt+1)(gradXf(Xt+1,yt+1) - Txt+1 gradXf(Xt,yt)
-gradxfBt+1 (Xt+1,yt+1) + Txtt+1 gradxfBt+1 (Xt, yt), gradxf (Xt+1, yt+1) - gradxfBt+1 (Xt+1, yt+1)〉
≤ (1 - αt+1)2E∣∣gradxf(Xt,yt) - Vtk2 + 202+1Ekgradxf (Xt+1, yt+1) - gradxfBt+1 (Xt+1, yt+1)k2
+ 2(1 - 0t+1)2E∣∣gradxf (Xt+1,yt+1) -TXt+1 gradxf (Xt,yt) - gradxfBt+1 (Xt+1 ,yt+1)
+ TXt+1 gradxfBt+1 (Xt,yt)k2
≤ (1 - αt+1)2Ekgradxf (Xt, yt) - Vtk2 + 2αt+1σ
B
+ 2(1 - 0t+1)2 EkgradXfBt+1 (Xt+1,yt+1) -Txt+1 gradxfBt+1 (Xt,yt)k2,
'-------------------------------------V------------------------'
=T1
where the fourth equality follows by E[gradxfBt+1 (Xt+1,yt+1)] = gradxf(Xt+1,yt+1) and
E[gradxfBt+1 (Xt+1,yt+1) - gradxfBt+1 (Xt,yt)] = gradxf (Xt+1,yt+1) - gradxf (Xt,yt); the first
inequality holds by Young,s inequality; the last inequality is due to the equality EkZ - E[Z]k2 =
EkZk2 - ∣∣E[Z]k2 and Assumption 5.
20
Under review as a conference paper at ICLR 2021
Next, we consider an upper bound of the above term T1 as follows:
T1 =EgradxfBt+1(xt+1,yt+1) -Txxtt+1gradxfBt+1(xt,yt)2	(75)
=EIlgradxfBt+ι (Xt+ι,yt+1) - TXXt+1 gradχf(Xt,yt+ι; ξt+ι) + TXtt+1 gradχf (Xt,yt+ι;ξt+1)
- Txxtt+1gradxfBt+1(xt,yt)2
≤ 2E∣∣gradxfBt+ι(Xt+ι,yt+1) -Txt+1 gradxf(Xt,yt+ι;ξt+ι)k2
+ 2Ekgradxf(Xt,yt+1； ξt+ι) - gradxfBt+ι (Xt,yt)I∣2
≤ 2L211γ2ηt2kvtk2 + 2L212kyt+1 -ytk2
=2L2ιγ2η2kvtk2 + 2L22η2kyt+ι -ytk2,	(76)
where the last inequality is due to Assumption 1. Thus, we have
Ekgradxf(Xt+1, yt+1) - vt+1k2 ≤ (1 - αt+1)2 Ekgradx f (Xt, yt) - vtk2 + 4(1 - αt+1)2L121γ2ηt2kvtk2
+ 4(1 - αt+ι)2 L22nt.kyjt+1 - ytk2 +-t+一 .	(77)
B
We apply a similar analysis to prove the above inequality (72). We obtain
EkVy f (Xt+1, yt+1) - wt+ik2 ≤ (I- βt+i)2EkVy f(Xt, yt) - wtk2 +4(I- βt+i)2L2iγ 2n2kvt k2
+ 4(1 - d+1)"22后||%+1 - ytk2 + 't+1一.	(78)
B
□
Theorem 6. Suppose the sequence {Xt, yt}T=1 is generated from Algorithm 2. Given y = y*(X。,
c1 ≥ 323 + 2λμ, c2 ≥ 323 + 50μL2, b > 0, m ≥ max (2, (≡b)3), 0 < Y ≤ 2κL√μ5+4μλ and
0 < λ ≤ 6L, we have
TXE[kgradgt)k + Lkyt — y*(Xt)k] ≤ √Mm - + TM,	(79)
t=1
where ≡ = max(2γL, c1,c2,1) and M0 = 2(φ(xγ)-φ, + 端ObB + 温媲'"ln(m + T).
Proof. Since n is decreasing and m ≥ b3, We have n ≤ no = —b/ɜ ≤ 1. Similarly, due to
m/
m ≥ (2γLb)3, we have n ≤ no = mb/ɜ ≤ %.Due to 0 <nt ≤ 1 and m ≥ max ((c1b)3, (c2b)3)
we have αt+1 = cm2 ≤ c1nt ≤ m⅛ ≤ 1 and βt+1 = c2nt ≤ c2nt ≤ mb ≤ 1. According to
Lemma 7, we have
一Ekgradxf(Xt+1,yt+1) - vt+1k2-------Ekgradxf(Xt,yt) - vtk2	(8O)
nt	nt-1
≤ (—αt+ll---------3-)Ekgradxf(Xt,yt) — vtk2 + 4(1 一 αt+1)2L21Y2ntkvtk2
nt	nt-1	x
+ 4(1 - αt+1^L22ntkyjt+1 - ytk2 +—t+1一
12	ntB
≤ (	t+1------)Ekgradxf(Xt,yt) - vtk2 + 4g172叫|%||2 +4L22ntkyt+1 - ytk2 +	t+1—
nt	nt-1	x	11	12	ntB
=(-------------cmt)Ekgradxf(Xt,yt) - vtk2 + 4L21Y2ntkvtk2 +4L22ntkyt+1 - ytk2 +—t+1一,
nt	nt-1	x	ntB
where the second inequality is due to 0 < αt+1 ≤ 1. By a similar way, we also obtain
—Ek vyf (Xt+1, yt+1) - wt+1k2---------EkVyf (Xt, yt) - Wtk2	(81)
nt	nt-1
1	1	2β2 σ2
≤ (------------c2nt)EkVyf (Xt,	yt)	-	Wtk	+4L21γ	ntkvtk	+4L22ntkyt+1	- ytk	+----ŋ-.
nt	nt-1	21	22	ntB
21
Under review as a conference paper at ICLR 2021
By ηt =(m+bt)i/3, We have
1	1	_
ηt	ηt-ι
≤
≤
1	1
3b(m +1-I)2/3 ≤ 3b(m∕2 + 琢2/
22/3	22/3	b2	22/3 2	2
3b(m +1)2∕3	3b3 (m∕2 + t)2/3	3b3 nt ― 3b3nt，
(82)
Where the first inequality holds by the concavity of function f(x) = x1/3, i.e., (x + y)1/3 ≤
x1/3 + 3χ2∕3 ； the second inequality is due to m ≥ 2, and the last inequality is due to 0 < nt ≤ 1.
Let ci ≥ 奈 + 2λμ, we have
ntEkgradxf(Xt+i，yt+i)-vt+ιk2 - =Ekgradxf(χt,yt) -vtk2	2	「3)
≤ -2λμntEkgradxf (xt,yt) - vtk2 +4L2ιY2ntkvtk2 +4L22ntkyt+i - ytk2 + 2°；B .
〒 2
Let c2 ≥ 亲 + 50μL , we have
—EINyf (Xt+1,yt+1)- wt+1k2---------EkVyf (χt,yt) - wtk2	(84)
nt	nt-1
50 λ L 2	C	C C	C	C	C	2β2-σ2
≤-------ntEkVy f(xt,yt)	- Wtk	+4L21γ	ntkvtk	+4L22ntkyt+1	-	ytk	+-----石一.
μ	nt B
According to Lemma 6, we have
kyt+ι -y*(χt+ι)k2 - kyt - y (Xt)k2 ≤ -ntμ-kyt -y*(xt)k2 - ɪkyt+ι -ytk2
+ 箸kVyf (χt,yt) - wtk2 + 25γμλntkvtk2.
(85)
Next, we define a Lyapunov function Ωt, for any t ≥ 1
ω=e[φ(Xt)]+2λμ( n^^^ Ekgradxf(Xt, yt) - vtk2+n^^^ Ekvy f(χt, yt) - Wtk2)
+—λμ^~ Ekyt - y* (Xt) k2.
(86)
22
Under review as a conference paper at ICLR 2021
Then we have
Ct+1 - Ct = E[φ(Xt+1)] - E[φ(Xt)] +—λ- (Ekyt+ι - y*(xt+ι州2 - Ekyt- y*(χt)k2)
γ1	1
+ ʒʒ-(—Ekgradχf(χt+ι,yt+ι) - vt+ιk-------Ekgradxf(Xt,yt) - Vtk
2λμ ηt	ηt-i
+--EkVy f (χt+1,yt+1) - wt+ik2-----EkVy f(xt, yt) - Wtll 2)
ηt	ηt-1
≤ Li2γηtEkyt- y*(xt)k2 + γηtEkgradxf(χt,yt) - vtk2 - γηtEkgradφ(xt)k2 -乎kvtk2
+ 6YL2( - μληtEkyt - y*(xt)k2 - 3ηtEkyt+ι - ytk2 + 25ληtEkVyf(xt,yt) - Wtk2 +	|同|2)
t	t	t+1 t	y t, t t	t
+ τττ-( - 2λμηtEkgradxf(Xt, yt) - vtk2 + 4L2ιγ2ηtkvtk2 + 4Li2ηtEkyt+1 - ytk2 +
2α2+1σ2
ηtB
—
50λL2	CCC	CC	C	2β2, -,σ2
-μ-ηtEkVyf(Xt,yt) - wtk2 +4L2lY2ηtkvtk2 +4L22ηtEkyt+1 - ytk2 +	~
ʌ,r 2c	…	〜〒2 s	~
≤ - * Ekyt-y*(Xt)k2 -与 EkgradΦ(Xt)k2 -f Ekyt+ι - ytk2 - (Y
2	2	2λμ	4
+ Yα2+ισ2 + Yβ2+ισ2
λμηtB	λμηtB
2	22	22
≤ -γ⅛tEkyt- y*(Xt)k2 -苧EkgradΦ(Xt)k2 + Yp吟 + ⅛⅛,
2	2	λμηtB	λμηtB
—
25Y3K2L2	4γ3L2)	2
-------μλ^)ηtkvtk
(87)
where the first inequality holds by Lemmas 5 and the above inequalities (83), (84) and (85); the
second inequality is due to L = max(1, L11, L12, L21, L22); the last inequality is due to 0 ≤ Y ≤
2κL√5+μλ and K ≥ 1.
According to the above inequality (87), we have
罗(Ekgradgt)k2 + L2Ekyt- y*(Xt)k2) ≤ Q- Mi + γλ⅛B2 + ⅛⅛.	(88)
Taking average over t = 1,2,…，T on both sides of the inequality (88), We have
TT
T XEηt(kgradΦ(Xt)k2 + L2∣∣yt - y*(Xt)k2) ≤ X
t=1	t=1
2(Ct - Ct+i)
YT
+ ɪ	( 2α2+1σ2 + 2β2+1σ2
+ T =, λμηtB + λμηtB
Since the initial solution satisfies yi = y*(∕ι) = argmaxy∈γ f (Xi,y), we have
Ωι = Φ(xi) +
Φ(X1) +
；~ ky1 - y*(X1 )k2 + Yτ~ (— kgradxf (X1,y1) - v1k2 +	kVyf(X1,y1) - w1k2)
λμ	2λμ'ηo	ηo
Y1	1
ɪ(-kgradxf (Xi,yi) - gradxfBι (X1,yi )k2 + 一 kVy f (X1,yi) - Vy fBi (巧用1)||2)
2λμ 'ηo	ηo
2
≤ …λ⅛,
(89)
where the last inequality holds by Assumption 5.
23
Under review as a conference paper at ICLR 2021
Table 3: Benchmark Datasets Used in Experiments
datasets	#SamPles	#dimension	#classes
MNIST	60,000	-28×28-	-10-
CIFAR-10	50,000	32×32 ×3	10
CIFAR-100	50,000	32×32 ×3	100
SVHN	73,257	32×32 ×3	10
Fashion-MNIST	60,000	28×28	10
STL-10	5,000	32×32 ×3	10
Consider ηt is decreasing, i.e., ηT-1 ≥ ηt-1 for any 0 ≤ t ≤ T , we have
1T
T EE(kgradΦ(xt)k2 + L2kyt - y*(xt)k2)
(90)
2(Ct - Ct+1)
TγηT
1T
+而X (
202+1σ
2
2β2+ισ2
λμηtB
λμηtB
≤
t=1
T
≤X
t=1
2Φ(xι)	2σ2	2Φ*	1 X(
Y + λμηoB	Y + Tητ 与(
202+1σ
2(Φ(xι) - Φ*)
TγηT
≤ 2(Φ(xι)- Φ*)
TγηT
≤ 2(Φ(xι)- Φ*)
TγηT
2(Φ(xι) - Φ*)
Tγb
2σ2
+ TλμηoητB +
2σ2
+ Tλμηoητ B +
2σ2
+ Tλμηoητ B
(m+T)1/3+
+
2(c12 + c22 )σ2
Tητ λμB
2(c12 + c22 )σ2
Tητ λμB
λμηtB
T
Xηt3
t=1
2
+
2β2+ισ2
λμηtB
ZT上d
1 m+t
2(c12 + c22 )σ2b3
2σ
Tητ λμB
2
ln(m + T)
TλμηobB
(m+T)1/3+
2(c12 + c22)σ2b2
TλμB
ln(m+T)(m+T)1/3,
where the third inequality holds by PtT=1 ηt3 ≤ R1T ηt3dt. Let M0
2(Φ(xι)-Φ*)
2(c2 +c2)σ2b2
Yb
+温灰+
λμB
ln(m + T), we rewrite the above inequality as follows:
1 T	M0
T EE(kgradΦ(xt)k2 + L2∣∣yt - y*(xt)∣∣2) ≤ — (m + T)1/3.
(91)
t=1
According to Jensen’s inequality, we have
1 T	2 T	1/2
T fE(kgradΦ(xt)k + Lkyt- y*(xt)k) ≤ T fE(kgradΦ(xt )∣∣2 + L 2∣∣yt - y*(xt )∣∣2)
t=1	t=1
≤湾…)1/6二「+书，
(92)
where the last inequality is due to (a1 + a2)1/6 ≤ a11/6 + a12/6 for all a1, a2 > 0.
□
B	Additional Experimental Results
In this section, we provide additional experimental results on SVHN, FashionMNIST and STL-10
datasets, given in Table 3. The training loss and attack loss under uniform attack is shown in Fig 4.
The test accuracy with natural images and uniform attack is shown in Tab. 4. From these results, our
methods are robust to the uniform attack in training DNNs.
24
Under review as a conference paper at ICLR 2021
Table 4: Test accuracy against nature images and uniform attack for FashionMNIST, SVHN and
STL-10 datasets.
Method	Eval. Against	FashionMNIST	SVHN	STL-10
RSGDA	Nat. Images	84.96%	75.72%	52.34%
	Uniform Attack	82.54%	43.19%	47.28%
MVR-RSGDA	Nat. Images	88.49%	76.05%	54.92%
	Uniform Attack	76.75%	45.06%	48.89%
SGDA	Nat. Images	88.57%	91.96%	56.10%
	Uniform Attack	5).90%	45.97%	45.27%
IOQ	150	200
Epochs
(b) SVHN
Epochs
(c) STL-10
50	100	150
Epochs
(a) FashionMNIST
--RSGDA
--MVR-RSGDA
--SGDA
--RSGDA
--MVR-RSGDA
--SGDA
Iterations	Iterations
(e) SVHN	(f) STL-10
Iterations
(d) FashionMNIST
--RSGDA
--MVR-RSGDA
--SGDA
Figure 4: Additional results for robust training (a-c) and uniform attack (d-f) with SGDA, RSGDA
and MVR-RSGDA algorithms.
25