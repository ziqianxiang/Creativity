Under review as a conference paper at ICLR 2021
WeMix: How to Better Utilize Data Augmen-
TATION
Anonymous authors
Paper under double-blind review
Ab stract
Data augmentation is a widely used training trick in deep learning to improve the
network generalization ability. Despite many encouraging results, several recent
studies did point out limitations of the conventional data augmentation scheme in
certain scenarios, calling for a better theoretical understanding of data augmen-
tation. In this work, we develop a comprehensive analysis that reveals pros and
cons of data augmentation. The main limitation of data augmentation arises from
the data bias, i.e. the augmented data distribution can be quite different from the
original one. This data bias leads to a suboptimal performance of existing data
augmentation methods. To this end, we develop two novel algorithms, termed
“AugDrop” and “MixLoss”, to correct the data bias in the data augmentation. Our
theoretical analysis shows that both algorithms are guaranteed to improve the ef-
fect of data augmentation through the bias correction, which is further validated
by our empirical studies. Finally, we propose a generic algorithm “WeMix” by
combining AugDrop and MixLoss, whose effectiveness is observed from exten-
sive empirical evaluations.
1	Introduction
Data augmentation (Baird, 1992; Schmidhuber, 2015) has been a key to the success of deep learning
in image classification (He et al., 2019), and is becoming increasingly common in other tasks such as
natural language processing (Zhang et al., 2015) and object detection (Zoph et al., 2019). The data
augmentation expands training set by generating virtual instances through random augmentation to
the original ones. This alleviates the overfitting (Shorten & Khoshgoftaar, 2019) problem when
training large deep neural networks. Despite many encouraging results, it is not the case that data
augmentation will always improve generalization errors (Min et al., 2020; Raghunathan et al., 2020).
In particular, Raghunathan et al. (2020) showed that training by augmented data will lead to a smaller
robust error but potentially a larger standard error. Therefore, it is critical to answer the following
two questions before applying data augmentation in deep learning:
•	When will the deep models benefit from data augmentation?
•	How to better leverage augmented data during training?
Several previous works (Raghunathan et al., 2020; Wu et al., 2020; Min et al., 2020) tried to address
the questions. Their analysis is limited to specific problems such as linear ridge regression therefore
may not be applicable to deep learning. In this work, we aim to answer the two questions from a
theoretical perspective under a more general non-convex setting. We address the first question in a
more general form covering applications in deep learning. For the second question, we develop new
approaches that are provably more effective than the conventional data augmentation approaches.
Most data augmentation operations alter the data distribution during the training progress. This
imposes a data distribution bias (we simply use “data bias” in the rest of this paper) between the
augmented data and the original data, which may make it difficult to fully leverage the augmented
data. To be more concrete, let us consider label-mixing augmentation (e.g., mixup (Zhang et al.,
2018; Tokozume et al., 2018)). Suppose we have n original data D = {(xi, yi), i = 1, . . . , n},
where the input-label pair (xi, Ni) follows a distribution Pxy = (Pχ, Py(∙∣x)), Px is the marginal
distribution of the inputs and Py (∙∣x) is the conditional distribution of the labels given inputs; We
generate m augmented data D = {(xi, yj, i = 1,..., m}, where (xi, y. 〜 Pee = (Pe, Pe(∙∣e)),
1
Under review as a conference paper at ICLR 2021
and Px = Pe but Py(∙∣x) = Py(∙∣e). Given X 〜 Pχ, the data bias is defined as δy = maxy,y ∣∣y 一
ye ∣. We will show that when the bias between D and D is large, directly training on the augmented
data will not be as effective as training on the original data.
Given the fact that augmented data may hurt the performance, the next question is how to design
better learning algorithms to leash out the power of augmented data. To this end, we develop two
novel algorithms to alleviate the data bias. The first algorithm, termed AugDrop, corrects the data
bias by introducing a constrained optimization problem. The second algorithm, termed MixLoss,
corrects the data bias by introducing a modified loss function. We show that, both theoretically and
empirically, even with a large data bias, the proposed algorithms can still improve the generalization
performance by effectively leveraging the combination of augmented data and original data. We
summarize the main contributions of this work as follows:
•	We prove that in a conventional training scheme, a deep model can benefit from augmented
data when the data bias is small.
•	We design two algorithms termed AugDrop and MixLoss that can better leverage aug-
mented data even when the data bias is large with theoretical guarantees.
•	Based on our theoretical findings, we empirically propose anew efficient algorithm WeMix
by combining AugDrop and MixLoss , which has better performances without extra train-
ing cost.
2	Related Work
A series of empirical works (Cubuk et al., 2019; Ho et al., 2019; Lim et al., 2019; Lin et al., 2019a;
Cubuk et al., 2020; Hataya et al., 2019) on how to learn a good policy of using different data aug-
mentations have been proposed without theoretical guarantees. In this section, we mainly focus on
reviewing theoretical studies on data augmentation. For a survey of data augmentation, we refer
readers to (Shorten & Khoshgoftaar, 2019) and references therein for a comprehensive overview.
Several works have attempted to establish theoretical understandings of data augmentation from
different perspectives (Dao et al., 2019; Chen et al., 2019; Rajput et al., 2019). Min et al. (2020)
shown that, with more training data, weak augmentation can improve performance while strong
augmentation always hurts the performance. Later on, Chen et al. (2020) study the gap between the
generalization error (please see the formal definition in (Chen et al., 2020)) of adversarially-trained
models and standard models. Both of their theoretical analyses were built on special linear binary
classification model or linear regression model for label-preserving augmentation.
Recently, Raghunathan et al. (2020) studied label-preserving transformation in data augmentation,
which is identical to the first case in this paper. Their analysis is restricted to linear least square re-
gression under noiseless setting, which is not applicable to training deep neural networks. Besides,
their analysis requires infinite unlabeled data. By contrast, we do not require the original data to be
unlimited. Wu et al. (2020) considered linear data augmentations. There are several major differ-
ences between their work and ours. First, they focus on the ridge linear regression problem which is
strongly convex, while we consider non-convex optimization problems, which is more applicable in
deep learning. Second, we study more general data augmentations beyond linear transformation.
3	Preliminaries and Notations
We study a learning problem for finding a classifier to map an input X ∈ X onto a label y ∈ Y ⊂
RK, where K is the number of classes. We assume the input-label pair (X, y) is drawn from a
distribution Pxy = (Px, Py (∙∣χ)). Since every augmented example (X, e) is generated by applying
a certain transformation to either one or multiple examples, we will assume that (Xe, ye) is drawn
from a slightly different distribution Pxy = (Px, Py (∙∣e)), where Pe is the marginal distribution on
the inputs X and Pe(∙∣e)) (We can write it as Pe for simplicity) is the conditional distribution of
the labels ye given inputs Xe. We sample n training examples (Xi, yi), i = 1, . . . , n from distribution
Pxy and m training examples (Xei, yei), i = 1, . . . , m from Pexye. We assume that m n due to the
data augmentation. We denote by D = {(Xi, yi), i = 1, . . . , n} and D = (Xei, yei), i = 1, . . . , m}
the dataset sampled from Pxy and Pexye, respectively. We denote by T(X) the set of augmented data
transformed from x. We use the notation E(x,y)〜Pxy [∙] to stand for the expectation that takes over
a random variable (x, y) following a distribution Pxy. We denote by Ywh(w) the gradient of a
function h(w) in terms of variable w. When the variable to be taken a gradient is obvious, we use
2
Under review as a conference paper at ICLR 2021
the notation Vh(w) for simplicity. Let use k ∙ k as the Euclidean norm for a vector or the Spectral
norm for a matrix.
The augmented data D can be different from the original data D in two cases, according to (Raghu-
nathan et al., 2020). In the first case, often referred to as label-preserving, we consider
Py(∙∣x)= Py(∙∣e), ∀e ∈ T(x) butPx = Pχ.	(1)
In the second case, often referred to as label-mixing, we consider
Px = Px but Py(∙∣χ) = Py (∙∣e), ∃e ∈ T (x).	(2)
Examples of label-preserving augmentation include translation, adding noises, small rotation, and
brightness or contrast changes (Krizhevsky et al., 2012; Raghunathan et al., 2020). One important
example of label-mixing augmentation is mixup (Zhang et al., 2018; Tokozume et al., 2018). Due to
the space limitation, we will focus on the label-mixing case, and the related studies and analysis for
the label-preserving case can be found in Appendix A. To further quantify the difference between
original data and augmented data when Px = Pex and Py 6= Pye , we introduce the data bias δy given
X 〜Px as following:
δy := max ky - yek.	(3)
y,ye
The equation in (3) measures the difference between the label from original data and the label from
augmented data given input X. We aim to learn a prediction function f(X; w) : RD × X → RK
that is as close as possible to y, where w ∈ RD is the parameter and RD is a closed convex set. We
respectively define two objective functions for optimization problems over the original data and the
augmented data as
L(w) = E(x,y) [` (y, f (X; w))] ,	Le(w) = E(ex,ye) [` (ye, f (Xe; w))] ,	(4)
where ` is a cross-entropy loss function which is given by
'(y,f(x; W))
K
yipi(x; W), where pi(x; W) = -log
i=1
exp(fi(x; W))
PK=I exP(fj(x；W))
(5)
We denote by w* and W* the optimal solutions to minw L(w) and minw L(w) respectively,
W* ∈ arg min L(W), We * ∈ arg min L(W).
w∈RD	w∈RD
(6)
Taking L(W) as an example, we introduce some function properties used in our analysis.
Definition 1. The stochastic gradients of the objective functions L(W) is unbiased and bounded,
if we have E(x,y) [Vw' (y,f (x; w))] = VL(w), and there exists a constant G > 0, SUCh that
kVw p(x; W)k ≤ G, ∀x ∈ X, ∀W ∈ RD, where p(x; W) = (p1(x; W), . . . , pK (x; W)) is a vector.
Definition 2. L(W) is smooth with an L-Lipchitz continuous gradient, if there exists a constant
L > 0 such that kVL(W) - VL(u)k ≤ LkW - uk, ∀W, u ∈ RD, or equivalently, L(W) - L(u) ≤
(VL(u), w — Ui + L2∣∣w — uk2, ∀w, U ∈ RD.
The above properties are standard and widely used in the literature of non-convex optimiza-
tion (Ghadimi & Lan, 2013; Yan et al., 2018; Yuan et al., 2019; Wang et al., 2019; Li et al., 2020).
We introduce an important property termed Polyak-Eojasiewicz (PL) condition (Polyak, 1963) on
the objective function L(w).
Definition 3. (PL condition) L(w) satisfies the PL condition, if there exists a constant μ > 0 such
that 2μ(L(w) — L(w*)) ≤ ∣∣VL(w)∣2, ∀w ∈ RD, where w* is defined in (6).
The PL condition has been observed in training deep and shallow neural networks (Allen-Zhu et al.,
2019; Xie et al., 2017), and is widely used in many non-convex optimization studies (Karimi et al.,
2016; Li &Li, 2018; Charles & Papailiopoulos, 2018; Yuan et al., 2019; Liet al., 2020). It is also
theoretically verified in (Allen-Zhu et al., 2019) and empirically estimated in (Yuan et al., 2019) for
deep neural networks. It is worth noting that PL condition is weaker than many conditions such as
strong convexity, restricted strong convexity and weak strong convexity (Karimi et al., 2016).
Finally, We will refer to K = L as condition number throughout this study.
3
Under review as a conference paper at ICLR 2021
4	Main Results
In this section, we present the main results for label-mixing augmentation satisfying (2). Due to
the space limitation, we present the results of label-preserving augmentation satisfying (1) in Ap-
pendix A. Since we have access to m n augmented data, it is natural to fully leverage the
augmented data D during training. But on the other hand, due to the data bias δy , the prediction
model learned from augmented data D could be even worse than training the prediction model di-
rectly from the original data D, as revealed by Lemma 1 (its proof can be found in Appendix C) and
its remark. Throughout this section, suppose that a mini-batch SGD is used for optimization, i.e. to
optimize L(w), we have
m0
Wt+1 = Wt — — E Vw' (Vk,t, f(xk,t Wt)),	(7)
m0 k=1
where η is the step size, m0 is the batch size, and (xk,t, yk,t), k = 1, . . . , m0 are sampled from D.
A similar mini-batch SGD algorithm can be developed for the augmented data.
Lemma 1. Assume that L and L satisfy properties in Definition 1, 2 and 3, by setting η = 1/L and
mo ≥ δ8, when t ≥ L log 4(L(w1δ)-L(w*))μ, we have
E[L(wt+ι) - l(w*)] ≤ δG2/μ ≤ O(δy/μ),	(8)
where Wt+1 is output of mini-batch SGD trained on D, δy is defined in (3).
Remark: It is easy to verify (see the details of proof in Appendix D) that if we simply train the
learning model by the original data D, we have
E [L(wn+ι) - L(w*)] ≤ O (Llog(n)∕(nμ2)) .	(9)
Comparing the result in (9) with the result of (8) in Lemma 1, it is easy to show that, when the
data bias is too large, i.e., δ% ≥ Ω(Llog(n)∕(nμ)), We have O (Llog(n)/(n〃2)) ≤ o(δy∕μ)
This implies that training the deep model directly on the original data D is more effective than on
the augmented data D. Hence, in order to better leverage the augmented data in the presence of
large data bias (δj ≥ Ω(κ log(n)∕n), where K = L∕μ), we need to Come UP with approaches that
automatically correct the data bias. BeloW, We develop tWo approaches to correct the data bias. The
first approach, termed “AUgDrop”, corrects the data bias by introdUcing a constrained optimization
approach, and the second approach, termed “MixLoss”, addresses the problem by introdUcing a
modified loss fUnction.
4.1	AugDrop: CORRECTING DATA BIAS BY CONSTRAINED OPTIMIZATION
To address this challenge, we propose a constrained optimization problem, i.e.
., ~, 、 ~ ■一.、
mm L(w) s.t. L(W) — L(W*) ≤ γ,	(10)
w∈RD
where γ > 0 is a positive constant, W* is defined in (6). The key idea is that by utilizing the aug-
mented data to constrain the solUtion in a small region, we will be able to enjoy a smaller condition
number, leading to a better performance in optimizing L(W). To make it concrete, we first define
three important terms:
Yo ：= δ2G2∕(2μ), A(Y) = {w : L(w) —L(W*) ≤ Yo ,	(11)
μ(γ) = max {L(w) — L(w*) ≤ ∣∣VL(w)k2∕(2μ0), W ∈ A(Y)} .	(12)
μ
We then present a proposition about A(Y) and μ(γ), whose proof is included in Appendix E.
Proposition 1. If Y ∈ [yo, 8yo], we have w* ∈ A(Y) and μ(τ) ≥ μ.
According to Proposition 1, by restricting our solutions in A(Y), we have a smaller condition number
(since μ(τ) ≥ μ) and consequentially a smaller optimization error. It is worth mentioning that the
restriction of solutions in A(Y) is reasonable due to the optimal solution w* ∈ A(Y). The idea
of using augmentation transformation to restrict the candidate solution was recognized by several
4
Under review as a conference paper at ICLR 2021
earlier studies, e.g. (Raghunathan et al., 2020). But none of these studies cast it into a constrained
optimization problem, a key contribution of our work.
The next question is how to solve the constrained optimization problem in (10). It is worth noting
that neither L(w) nor L(w) is convex. Although multiple approaches can be used to solve non-
convex constrained optimization problems (Cartis et al., 2011; Lin et al., 2019b; Birgin & Martinez,
2020; Grapiglia & Yuan, 2019; Wright, 2001; O’Neill & Wright, 2020; Boob et al., 2019; Ma et al.,
2019), they are too complicated to be implemented in deep learning. Instead, we present a simple
approach that divides the optimization into two stages, which is referred to as AugDrop (Please see
the details of update steps from Algorithm 2 in Appendix F).
•	Stage I. We minimize L(w) over the augmented data D. It runs a mini-batch SGD against
D at least T1 iterations with the size of mini-batch being m1. We denote by wT1+1 the final
output solution of this stage.
•	Stage II. We minimize L(w) using the original data D. It initializes the solution wT1+1
and runs a mini-batch SGD against D in n/m2 iterations with mini-batch size being m2 .
We notice that AugDrop is closely related to TSLA by (Xu et al., 2020) where the first stage trains
the data with label smoothing and the second stage trains the data without label smoothing. However,
they study the problem how to reduce the variance of stochastic gradient in using label smoothing,
while we study how to correct bias in data augmentation by solving a constrained optimization
problem. The following theorem states that if we run this two stage optimization algorithm, we could
achieve a better performance since μ(8γo) is larger than μ. We include its proof in Appendix F.
Theorem 1. Define μc = μ(8γ0). Assume that L and L satisfy properties in Definition 1, 2 and 3,
set Iearing rate ηι = 1/L in Stage I and learning rate η = ^μ- log (8n*c (L(WIL-L(W*))) in Stage
II for AugDrop. Let w1 be the initial solution in Stage I of AugDrop and wT1+2 , . . . , wT1+n/m2+1
be the intermediate solutions obtained by the mini-batch SGD in Stage II of AugDrop. Choose
T1 = η1μlog 2(L(wι^-L(w*))μ, m1 = (1 + qlog 2T1) δ2 and m2 = (1 + qlog 2n) δ2,
with a probability 1 - δ, we have wt ∈ A(8γ0), ∀t ∈ {T1 + 2, . . . , T1 + n/m2 + 1} and
E[L(W) - L(w*)] ≤ G⅛ ("log (一"(w?「))) ≤ O (LogP) ,	(13)
4nμC ∖ G	G2L	JJ ∖ nμC	)
where wb = wT1+n/m2+1 and δy is defined in in (3).
Remark. Theorem 1 shows that all intermediate solutions wt obtained in Stage II of AugDrop sat-
isfy the constraint L(Wt) - L(W*) ≤ 8γ0, that is to say, Wt ∈ A(8γ0). Based on Propo-
sition 1, We will enjoy a larger μc than μ. Comparing the result of (13) in Theorem 1 with
(9), training by using AugDrop will result in a better performance than directly training on D
due to μc ≥ μ. Besides, when the data bias is large, i.e., δ% ≥ Ω(Llog(n)∕(nμ)), we know
O(L log(n)∕(nμ2)) ≤ O(μδj∕μC) ≤ O(δ,∕μ), where the last inequality holds due to μc ≥ μ. By
comparing (13) with the result of (8) in Lemma 1, we know that training by using AugDrop has a
better performance than directly training on D when the data bias is large. By solving a constrained
problem, the AugDrop algorithm can correct the data bias and thus can enjoy an better performance.
4.2 MixLoss: CORRECTING DATA BIAS BY MODIFIED LOSS FUNCTION
Without loss of generality, we set L(w*) = 0,a common property observed in training deep neural
networks (Zhang et al., 2016; Allen-Zhu et al., 2019; Du et al., 2018; 2019; Arora et al., 2019; Chizat
et al., 2019; Hastie et al., 2019; Yun et al., 2019; Zou et al., 2020). Since ky - yek ≤ δy for any y
and ye and given x, we define a new loss function `a (ye , f (xe; W)) as
'a(y,f(χ;W)) = ,, min '(z,f(X;w)).	(14)
kz-yek≤δy
It has been shown that since the cross-entropy loss '(z, ∙) is convex in terms of Z ∈ Y, then the
minimization problem (14) is a convex optimization problem and has a closed form solution (Boyd
& Vandenberghe, 2004). Using this new loss, we define a new objective function La (W)
La(W) = E(X,y) ['a(y,f (X； w))]= E(e,y)	m!匕 '(z,f(x； W)) .	(15)
kz-yek≤δy
5
Under review as a conference paper at ICLR 2021
Algorithm 1 WeMix
1:	Input: Ti, T2, stochastic algorithms Ai, A2 (e.g., momentum SGD, SGD)
2:	Initialize: w1 ∈ RD, λ ∈ (0, 1), η1, η2 > 0
// First stage: Weighted Mixed Losses
3:	for t = 1, 2, . . . , Ti do
4:	draw examples (xit , yit ) at random from training data construct stochastic gradient of L
5:	generate augmented examples (xejt, yejt)	construct stochastic gradient of La
6:	compute stochastic gradient gt =宓'«维,f (x^; Wt)) + (1 一内0'。@维,f Ixit ； Wt))
7:	wt+i = Ai(wt; gbt, ηi)	update one step ofAi
8:	end for
// Second stage: Augmentation Dropping
9:	for t = Ti + 1, Ti + 2, . . . , Ti + T2 do
10:	draw examples (xit , yit ) at random from training data construct stochastic gradient of L
11:	compute stochastic gradient gt = V'(yit, f (Xit; Wt))
12:	Wt+i = A2(Wt;gbt, η2)	update one step ofA2
13:	end for
14:	Output: wT1+T2+i.
It is easy to verify that La(w*) = 0 and therefore w* also minimizes La(W) (see Appendix G).
In contrast, We*, the minimizer of L(W), can be very different from W*. Hence, we can correct the
data bias arising from the augmented data by replacing L(W) with La (W), leading to the following
optimization problem:
min Lc(W) = λL(W) + (1 - λ)La(W),	(16)
w∈RD
where λ ∈ (0, 1). Since Lc(W) shares the same minimizer with L(W) (see Appendix G), it is
sufficient to optimize Lc(W), instead of optimizing L(W). The main advantage of minimizing
Lc(W) over L(W) is that by introducing a small λ, we will be able to reduce the variance in com-
puting the gradient of Lc(W), and therefore improve the overall convergence. More specifically,
our SGD method is given as follows: at each iteration t, we compute the approximate gradient as
gt = λV'(yt,f (xt; Wt)) + (1 — λ)六 Pm=0ι V'a(xt,i,f(xt,i; wt)), where (xt, yt) is an example
sampled from D at iteration t. We refer to this approach as MixLoss (Please see the details of up-
date steps from Algorithm 3 in Appendix H). We then give the convergence result in the following
theorem, whose proof is included in Appendix H.
Theorem 2. Assume that L, L and La satisfy properties in Definition 1, 2 and 3, by setting m0 ≥
72(λ-λ) and η = μn log rbμλλLW21 ≤ 2l in MixLoss, we have
λLG2	nμ2 L(wi)∖	„ λ λL log(n∕λ2)∖
E[L(Wn+I)-L(W*)] ≤ B (1+5log ⅛⅛2) ≤ O (	n；2/ ) ).	(17)
Remark. According to the results in (17) and (9), We know that O (λLlog(n∕λ2)∕(nμ2)) ≤
O (Llog(n)∕(nμ2)) when an appropriate λ ∈ (0,1) is selected, leading to a better performance
by using MixLoss compared with the performance trained on the original data D. For exam-
ple, one can simply use λ = O(μ∕L). On the other hand, when the data bias is large where
δy satisfying δj ≥ Ω(Llog(n)∕(nμ))), we know O(Llog(n)∕(nμ2)) ≤ O(δ2∕μ). Based on
previous discussion, by choosing an appropriate λ ∈ (0,1) (e.g., λ = O(μ∕L)), we will have
O (λLlog(n∕λ2)∕(nμ2)) ≤ O(δy∕μ). Then by comparing (17) with (8), we know that training
by using MixLoss has a better performance than directly training on D when the data bias is large.
Therefore, by solving the problem with a modified loss function, the MixLoss algorithm can enjoy
a better performance by correcting the data bias.
4.3 WeMix: A GENERIC WEIGHTED MIXED LOSSES WITH AUGMENTATION DROPPING
Algorithm
Inspired by previous theoretical analysis of using augmented data, we propose a generic frame-
work of weighted mixed losses with augmentation dropping that builds upon two algorithms, Aug-
Drop and MixLoss. Algorithm 1 describes our procedure in detail, which is referred to as WeMix.
It consists of two stages, wherein the first stage it runs a stochastic algorithm Ai (e.g., momen-
tum SGD, SGD) for solving weighted mixed losses (16) and the second stage it runs another/same
6
Under review as a conference paper at ICLR 2021
Table 1: Comparison of Testing Top-1 Accuracy (mean ± standard deviation, in %) using Different
Methods on ResNet-18 over CIFAR-10 and CIFAR-100 for mixup
Method	CIFAR-100	CIFAR-10
without mixup	76.97 ± 0.27	94.95 ± 0.17
mixup	78.31 ± 0.18	95.67 ± 0.09
AugDrop (ours)	80.24 ± 0.34	96.03 ± 0.12
MixLoss (ours)	79.70 ± 0.31	95.94 ± 0.11
WeMix (ours)	80.61 ± 0.10	96.11 ± 0.11
MixLoss-s (ours)	79.53 ± 0.13	95.87 ± 0.14
WeMix-s (ours)	80.29 ± 0.22	96.06 ± 0.16
stochastic algorithm A2 (e.g., momentum SGD, SGD) for solving the problem over original data.
The notation A(∙; ∙,η) is one update step of a stochastic algorithm A with learning rate n. For ex-
ample, ifwe select SGD as algorithm A, then SGD(wt; gbt, η) = wt - ηgbt. The proposed WeMix is
a generic strategy where the subroutine algorithm A1 /A2 can be replaced by any stochastic algo-
rithms such as stochastic versions of momentum methods (Polyak, 1964; Nesterov, 1983; Yan et al.,
2018) and adaptive methods (Duchi et al., 2011; Hinton et al., 2012; Zeiler, 2012; Kingma & Ba,
2015; Dozat, 2016; Reddi et al., 2018). We can also replace `a by ` to avoid solving a minimization
problem. The last solution of the first stage will be used as the initial solution of the second stage.
If λ = 0 and `a = `, then WeMix reduces to the AugDrop; while if T2 = 0, WeMix becomes to
MixLoss. For label-preserving case, we only need to simply use `a = ` (i.e, δf = 0) in WeMix.
5	Experiments
To evaluate the performance of the proposed methods, we trained deep neural networks on two
benchmark data sets, CIFAR-10 and CIFAR-1001 (Krizhevsky & Hinton, 2009) for the image clas-
sification task. Both CIFAR-10 and CIFAR-100 have 50,000 training images and 10,000 testing
images of 32×32 resolutions. CIFAR-10 has 10 classes containing 6000 images each, while CIFAR-
100 has 100 classes. We use mixup (Zhang et al., 2018) as an example of lable-mixing augmentation
and Contrast as an example of lable-preserving augmentation and. For the choice of backbone, we
use ResNet-18 model (He et al., 2016) in mixup, and Wide-ResNet-28-10 model (Zagoruyko & Ko-
modakis, 2016) is applied in the Contrast experiment following by (Cubuk et al., 2019; 2020). To
verify our theoretical results, we compare the proposed AugDrop and MixLoss with two baselines,
SGD with mixup/Contrast and SGD without mixup/Contrast (baseline). We also include WeMix in
the comparison. The mini-batch size of training instances for all methods is 256 as suggested by He
et al. (2019) and He et al. (2016). The momentum parameter of 0.9 is used. The weight decay with
the parameter value is set to be 5 × 10-4. The total epochs of training progress is fixed as 200.
Followed by (He et al., 2016; Zagoruyko & Komodakis, 2016), we use 0.1 as the initial learning
rates for all algorithms and divide them by 10 every 60 epochs.
For AugDrop, we drop off the augmentation after s-th epoch, where s ∈ {150, 160, 170, 180, 190} is
tuned. For example, ifs = 160, then it means that we run the first stage of AugDrop 160 epochs and
the second stage 40 epochs. For MixLoss, we tune the parameter δy from {0.5, 0.05, 0.005, 0.0005}
and the best performance is reported. For WeMix, we use the value of δy with the best performance
in MixLoss, and we tune the dropping off epochs s same as AugDrop. We fix the convex combi-
nation parameter λ = 0.1 both for MixLoss and WeMix. We use top-1 accuracy to evaluate the
performance. All top-1 accuracy on the testing data set are averaged over 5 independent random
trails with their standard deviations.
5.1	mixup
Given two examples (xi, yi) and (xj , yj ) that are drawn at random from the training data, mixup
creates a virtual training example as follows x0 = βxi + (1 - β)xj, y0 = βyi + (1 - β)yj, where
β ∈ [0, 1] is sampled from a Beta distribution β(α, α). We use α = 1 in the experiments as suggested
in (Zhang et al., 2018). In this subsection, we want to empirically verify that our theoretical findings
for label-mixing augmentation in Section 4. The experimental results conducted on CIFAR-10 and
CIFAR-100 are listed in Table 1. We can see from the results that both AugDrop and MixLoss are
Ihttps://www.cs.toronto.edu/~kriz/cifar.html
7
Under review as a conference paper at ICLR 2021
Table 2: Comparison of Testing Top-1 Accuracy (mean ± standard deviation, in %) using Different
Methods on ResNet-18 over CIFAR-100 for mixup of three images and ten images
Method	3 images	10 images
Mixup	76.56 ± 0.23	60.36 ± 0.88
AugDrop	80.18 ± 0.19	76.35 ± 0.27
MixLoss	79.61 ± 0.09	75.41 ± 0.19
WeMix	80.41 ± 0.22	78.08 ± 0.11
Table 3: Comparison of Testing Top-1 Accuracy (mean ± standard deviation, in %) using Different
Methods on WideResNet-28-10 over CIFAR-10 and CIFAR-100 for Contrast Transformation
Method	CIFAR-100	CIFAR-10
without Contrast	78.07 ± 0.27	95.51 ± 0.14
Contrast	77.90 ± 0.26	95.66 ± 0.05
AugDrop (ours)	78.40 ± 0.24	95.93 ± 0.21
MixLoss (ours)	78.17 ± 0.20	95.70 ± 0.11
WeMix (ours)	78.79 ± 0.18	95.81 ± 0.11
better than two baselines, with and without mixup, which matches the theory found in Section 4.
The performance of MixLoss is slightly worse than that of AugDrop, but they are comparable.
Besides, the proposed WeMix enjoys both improvements, leading to the best performance among
all algorithms although its convergence theoretical guarantee is unclear.
Next, we implement MixLoss and WeMix with δy = 0 (i.e., use `a = `), which are denoted by
MixLoss-s and WeMix-s, respectively. We summarize the results in Table 1, showing that both
MixLoss-s and WeMix-s drop performance, comparing with MixLoss and WeMix, respectively.
Besides, we use more than two images in mixup such as three and ten images and the results are
shown in Table 2. Although the top-1 accuracy of mixup reduces dramatically, we find that the
proposed WeMix can still improve the performance when it comparing with mixup itself, showing
the robustness of WeMix.
5.2	Contrast
As a simple label-preserving augmentation, Contrast controls the contrast of the image. Its transfor-
mation magnitude is randomly selected from a uniform distribution [0.1, 1.9] following by (Cubuk
et al., 2019). Despite its simplicity, we choose it to demonstrate our theory for the considered case
in Appendix A. The results of highest top-1 accuracy on the testing data sets for different methods
are presented in Table 3. We find that by directly training on data with Contrast, it will drop the
performance a little bit. Even so, the result shows that AugDrop has better performance than two
baselines, which is consistent with the theoretical findings for label-preserving augmentation in Ap-
pendix A that we need use the data augmentation at the early training stage but drop it at the end of
training. Although there is no theoretical guarantee for the label-preserving transformation case, we
implement MixLoss and WeMix by setting δf = 0, i.e., using `a = ` in (14). The results show that
MixLoss and WeMix are better than two baselines but are slightly worse than AugDrop.
6	Conclusions and Future Work
In this paper, we have studied how to better utilize data augmentation in training deep neural net-
works by designing two training schemes with the first one switches augmented data to original
data during the training progress and the second one training on a convex combination of original
loss and augmented loss. We have provided theoretical analyses of these two training schemes in
non-convex smooth optimization setting. With the insights of theoretical results, we have designed a
generic algorithm WeMix that can well leverage data augmentation in practice. We have verified our
theoretical finding throughout extensive experimental evaluations on training ResNet and WideRes-
Net models over benchmark data sets. Despite the effectiveness of WeMix, its theoretical guarantee
is still not fully understand. We would like to leave this open problem as future work.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Henry S Baird. Document image defect models. In Structured Document Image Analysis, pp. 546-
556. Springer, 1992.
EG Birgin and JM Martinez. Complexity and performance of an augmented Iagrangian algorithm.
Optimization Methods and Software, pp. 1-36, 2020.
Digvijay Boob, Qi Deng, and Guanghui Lan. Proximal point methods for optimization with non-
convex functional constraints. arXiv preprint arXiv:1908.02734, 2019.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. On the evaluation complexity of composite
function minimization with applications to nonconvex nonlinear programming. SIAM Journal on
Optimization, 21(4):1721-1739, 2011.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745-754,
2018.
Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization
gap between adversarially robust and standard models. arXiv preprint arXiv:2002.04725, 2020.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. Invariance reduces variance: Understanding data
augmentation in deep learning and beyond. arXiv preprint arXiv:1907.10905, 2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Imre Csiszar and Janos Korner. Information theory: coding theorems for discrete memoryless ^ys-
tems. Cambridge University Press, 2011.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 113-123, 2019.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A kernel
theory of modern data augmentation. In International Conference on Machine Learning, pp.
1528-1537, 2019.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
9
Under review as a conference paper at ICLR 2021
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Geovani N Grapiglia and Ya-xiang Yuan. On the complexity of an augmented lagrangian method
for nonconvex optimization. arXiv preprint arXiv:1906.05622, 2019.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment:
Learning augmentation strategies using backpropagation. arXiv preprint arXiv:1911.06987, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classification with convolutional neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 558-567, 2019.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. 2012.
Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation:
Efficient learning of augmentation policy schedules. In International Conference on Machine
Learning, pp. 2731-2741. PMLR, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Mas-
ter’s thesis, Technical report, University of Tronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification With deep convo-
lutional neural netWorks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. Exponential step sizes for non-convex opti-
mization. arXiv preprint arXiv:2002.05273, 2020.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5564-5574, 2018.
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and SungWoong Kim. Fast autoaugment. In
Advances in Neural Information Processing Systems, pp. 6665-6675, 2019.
Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie Yan, Dahua Lin, and Wanli
Ouyang. Online hyper-parameter learning for auto-augmentation strategy. In Proceedings of the
IEEE International Conference on Computer Vision, pp. 6579-6588, 2019a.
Qihang Lin, Runchao Ma, and Yangyang Xu. Inexact proximal-point penalty methods for non-
convex optimization With non-convex constraints. arXiv preprint arXiv:1908.11518, 2019b.
Runchao Ma, Qihang Lin, and Tianbao Yang. Proximally constrained methods for Weakly convex
optimization With Weakly convex constraints. arXiv preprint arXiv:1908.01871, 2019.
10
Under review as a conference paper at ICLR 2021
Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More
data can help, double descend, or hurt generalization. arXiv preprint arXiv:2002.11080, 2020.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O(1∕k2). Soviet Mathematics Doklady, 27:372-376,1983.
Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
Kluwer Academic Publ., 2004. ISBN 1-4020-7553-7.
Michael O’Neill and Stephen J Wright. A log-barrier newton-cg method for bound constrained
optimization with complexity guarantees. IMA Journal of Numerical Analysis, 2020.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716,
2020.
Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does
data augmentation lead to positive margin? In International Conference on Machine Learning,
pp. 5321-5330, 2019.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85-117,
2015.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):60, 2019.
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classifi-
cation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
5486-5494, 2018.
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, pp.
2403-2413, 2019.
Stephen J Wright. On the convergence of the newton/log-barrier method. Mathematical Program-
ming, 90(1):71-100, 2001.
Sen Wu, Hongyang R Zhang, Gregory Valiant, and Christopher Re. On the generalization effects of
linear transformations in data augmentation. arXiv preprint arXiv:2005.00695, 2020.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial
Intelligence and Statistics, pp. 1216-1224, 2017.
Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, and Rong Jin. Towards understanding label smoothing.
arXiv preprint arXiv:2006.11653, 2020.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momen-
tum methods for deep learning. In International Joint Conference on Artificial Intelligence, pp.
2955-2961, 2018.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence
of testing error over sgd. In Advances in Neural Information Processing Systems, pp. 2604-2614,
2019.
11
Under review as a conference paper at ICLR 2021
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight
analysis of memorization capacity. In Advances in Neural Information Processing Systems, pp.
15558-15569, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. In Advances in neural information processing systems, pp. 649-657, 2015.
Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learn-
ing data augmentation strategies for object detection. arXiv preprint arXiv:1906.11172, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.
12
Under review as a conference paper at ICLR 2021
A Main Results for label-preserving Augmentation
We consider label-preserving augmentation case (1), that is,
Py(∙∣χ) = Py(∙∣e), ∀e ∈ τ(x) butPx = Pχ.
It covers many image data augmentations including translation, adding noises, small rotation, and
brightness or contrast changes (Krizhevsky et al., 2012; Raghunathan et al., 2020). It is worth
mentioning that the compositions of label-preserving augmentation could also be label-preserving.
Similar to the case of label-mixing augmentation, we measure the following difference between Px
and Pex by a KL divergence:
δp ：= DKL(PxkPe) = Ex〜P. [log Px^l .	(18)
Pxe(x)
ɪʌ ,,1F∕∙* C* , 1	T	111	FC	,	1	1	, /ɪʌ	111
Due to the data bias δP, the prediction model learned from augmented data D could be even worse
than training the prediction model directly from the original data D, as revealed by the following
lemma and its remark.
Lemma 2. (label-preserving augmentation) Assume that L and L satisfy properties in Definition 1,
2 and 3, by setting η = 1/L and m0 ≥ η4^, when t ≥ to = L log ("w2)->Gw*))μ，we have
E(Xt,et)[L(Wt+1) - L(W*)] ≤ —P— ≤ O (-P) ,	(19)
μ ∖ μ，
where Wt+1 is output of mini-batch SGD trained on D, δP is defined in (18).
Proof. See Appendix I.1.	□
Remark: Comparing the result in (9) with the result of (19) in Lemma 2, itis easy to show that, when
the data bias is too large, i.e., δP ≥ Ω(L log(n)∕(nμ)), We have O (L log(n)∕(nμ2)) ≤ O(δP/μ).
This implies that training the deep model directly on the original data D is more effective than on
the augmented data D. Hence, in order to better leverage the augmented data in the presence of
large data bias (δP ≥ Ω(κ log(n)∕n), where K = L∕μ), we need to Come UP with an approach that
automatically correct the data bias in D. Below, we use AugDrop to correct the data bias by solving
a constrained optimization problem.
A. 1 AugDrop: CORRECTING DATA BIAS BY CONSTRAINED OPTIMIZATION
To correct data bias, we consider to solve the constrained optimization problem (10). The key idea is
to shrink the solution in a small region by using utilize augmented data to enjoy a smaller condition
number, leading to an improved convergence in optimizing L(W). By introducing a term that
Yl := δPG2∕μ,
we can present a proposition about A(Y) and μ(γ), showing that we have a smaller condition number
and consequentially a smaller optimization error by restricting our solutions to A(γ).
Proposition 2. If Y ∈ [γι, 4γι], we have w* ∈ A(Y) and μ(γ) ≥ μ, where A(Y) and μ(γ) are
defined in (11) and (12), respectively.
Proof. See Appendix I.2.
□
The following theorem shows the convergence result of AugDrop for label-preserving augmentation.
Theorem 3. Define Y = 4)ι,μe = μ(4Yι). Assume that L and L satisfy properties in
Definition 1, 2 and 3, set learning rate η1 = 1∕L in Stage I and learning rate η2 =
2nμ- log (8nμe(L(G2L—L(w* W ) in Stage II for AugDrop. Let wι be the initial solution in Stage I
of AugDrop and WT1+2, . . . , WT1+n/m2+1 be the intermediate solutions obtained by the mini-batch
SGD in Stage II of AugDrop. Choose T = L log 2(L(Wδ)-L(W*))μ, mi =(1 + J3 log 2T1) 看
13
Under review as a conference paper at ICLR 2021
and m2	=(1 +	J3log 2n)言，with a Probability 1 一	δ,	we have	Wt	∈	A(4γι),∀t	∈
{T1 + 2, . . . , T1 + n/m2 + 1} and
G2L	G2L
E[L(W)-L(W方 ≤ 研 + 研 log
8nμ2(L(wι) - L(w*))
G2L
≤ O(L≡F),(20)
where wb = wT1 +n/m2+1 and δP is defined in (18).
Proof. See Appendix I.3.
Remark. Theorem 3 shows that all intermediate solutions wt obtained in Stage II of AugDrop sat-
isfy the constraint L(Wt) - L(W*) ≤ 4γι, that is to say, Wt ∈ A(4γι). Based on Propo-
sition 2, We will enjoy a larger μe than μ. Comparing the result of (20) in Theorem 3 with
(9), training by using AugDrop will result in a better performance than directly training on D
due to μe ≥ μ. Besides, when the data bias is large, i.e., δp ≥ Ω(Llog(n)∕(nμ)), we know
O(Llog(n)∕(nμe)) ≤ O(μδp/μ2) ≤ O(δp/μ), where the last inequality holds due to μe ≥ μ. By
comparing (20) with the result of (19) in Lemma 2, we know that training by using AugDrop has a
better performance than directly training on D when the data bias is large. By solving a constrained
problem, the AugDrop algorithm can correct the data bias and thus enjoy an better performance.
B Technical Results for Cros s -entropy Loss
Lemma 3. Assume that L(w) = E['(y, f (x; w))] satisfies property in Definition 1, where ' is a
cross-entroPy loss, then we have
l∣Vw'(y,f (x; W))-Vw'(y,f(x; W))k ≤ Gky -列,
(21)
and
∣Vw'(y,f(x; W))k ≤ G.
(22)
Proof. The objective function is
L(W) =E(x,y)['(y,f(x;W))],
where the cross-entropy loss function ` is given by
'(y,f (x; w)) = X -yi log (；必(力住 W))
,;	i	K
i=1	j=1 eXp(fj (x; w))
(23)
(24)
Let set
p(x; w) = (pι(x; w), ...,pκ (x; w)), pi(x; w) = - log ( LKXP(fi(x; W))、、
j=1 exp(fj (x; w))
(25)
then the gradient of ` with respective to w is
V'(y,f(x; w)) = hy, Vp(x; w)).
Therefore, ∀x ∈ X and W ∈ RD we have
l∣Vw'(y,f (x; w)) - Vw'(y,f (x; w))k
= khy - ye , Vp(x; W)ik
≤lVp(x; w)l ly - yyl
≤Gly - yyl,
(26)
(27)
and
kVw'(y,f(x; W))k = khy, Vp(x; w))|| ≤ ∣∣Vp(x; w)∣ ∣y∣ ≤ G,
(28)
where uses the facts that ∣Vp(x; w)∣ ≤ G and ∣∣y∣ ≤ ∣∣y∣ι = 1, here ∣ ∙ ∣ is a Euclidean norm ('2
norm) and ∣∙∣∣ 1 is '1 norm.	□
□
14
Under review as a conference paper at ICLR 2021
C Proof of Lemma 1
Proof. Recall that the update of mini-batch SGD is given by
wt+1 = wt - ηget .
Let set the averaged mini-batch stochastic gradients of L(wt) as
1 m0
et := m ∑V' (yt,i,f (Xt,i; Wt)),
0 i=1
then by the Assumption of L satisfying the property in Definition 1, we know that
一	■一 —一	—一 、、、 一 .. 一 、
E(et,i,et,i) v'(yt,i,f(Xt,i；Wt))] = VL(Wt), ∀i ∈ {1,...,m0}	(29)
and thus
E(xet,yet)[Xgt] =VLX(Wt),	(30)
where we write E(xet,yet)[gXt] asE(ext,1,eyt,1)[. .. E(xet,m0,eyt,m0) [gXt]] for simplicity. Then the norm vari-
ance of gXt is given by
E(xet,yet)[kgXt -VLX(Wt)k2]
=E(xet,yet) IA X v' (yt,i,f(xt,i； Wt))- VLe(Wt) I j
m0	2
=)m2XE(Xt,i,et,i) [∣v'(Xt,i,f(xt,i；Wt))-VLe(Wt)∣ J
(b) 4G2
≤-----,
m0
(31)
where (a) is due to (29) and the pairs (xet,1, yet,1), . . . , (xet,m0, yet,m0) are independently sampled
from D; (b) is due to the facts that the Assumption of L satisfying the property in Definition 1
and Lemma 3, and then by Jensen’s inequality, we also have kVwL(W)k ≤ G, implying that
I V'(y, f (X; w)) 一 VL(W)I ≤ 4G2. On the other hand, by the Assumption of L satisfying the
property in Definition 2, we have
E(ext,yet)[L(Wt+1) - L(Wt)]
≤E(ext ,yet )
〈VL(wt), Wt+ι — Wti + IL l∣Wt+ι — Wtk2
=)2E(et,yt) [kVL(wt) - Xtk2 -∣VL(wt)k2 — (1 — ηL) kXtk2]
= 2 (kVL(Wt)-VLr(Wt)k2 +E(χt,yt) hkVLe(wt) - Xtk2i -kVL(wt)k2
— (1 — ηL) E(xet,eyt)[kXgtk2]
≤ η OVL(Wt)- VLe(Wt)∣∣2+—— ∣∣vL(Wt)-I	(32)
2	m0
where the (a) is due to the update of Wt+1 = Wt - ηget; (b) is due to (30); (c) is due to η = 1/L and
(31). By using the Assumption of L and L satisfying the property in Definition 1 and Px = Pxe, we
have
kVL(Wt) -VLe(Wt)k
=kE(x,y)[Vw'(y,f (χ; Wt))] - E(e,e) [Vw'(y,f (X； Wt))]k
(a)
≤E(χ,y,y)[kVw'(y, f(x; wt)) - Vw'(X, f(x; wt))k]
(21)
≤ GE(y,ye)[ky-yek]
(b)
≤ Gδy,
(33)
15
Under review as a conference paper at ICLR 2021
where (a) uses Jensen’s inequality; (b) is due to (3). By using the Assumption of L satisfying the
property in Definition 3 and (33), inequality (32) becomes
E(xet,yet) [L(wt+1) - L(wt)]
≤ ηG2δ2+2mG2 -2—
≤ηG2-δy + 2mG- - ημ (L(Wt) - L(WJ),
which implies
E(et,yt)[L(Wt+1) -L(W*)]
≤ (1 - ημ) (L(wt) - L(w*)) +
ηG2δ2	2ηG2
Fy +
m0
≤(I - ημ)t (L(WI) - L(Wj)+「2 y + n°) X(I - ημ)i.
DUe to (1 - ημ)t ≤ exp(-tημ) and Pt=0(1 - ημ)i ≤ ɪ, When
8
m0 ≥ δ2
y
and
We knoW
L L L “4(L(WI)-L(W*))”
t ≥ μlog	δ2G2	,
E(e*,yt)[L(Wt+ι) -L(W*)] ≤ ——.
μ
□
D Proof of (9)
We first pUt the fUll statement of (9) in the folloWing lemma.
Lemma 4. Assume that L satisfies the properties in Definition 1, 2 and 3, by set-
ting η = 2n1μ log (8nμ2(L(W1-L(W*)) ), we have E(xn,yn) [L(Wn+1) -L(W*)] ≤ GLL +
GL log (8n" (L(W2L-L(W* D ), where Wn+1 is output of SGD trained on D.
Proof. By the AssUmption of L satisfying the property in Definition 2, We have
E(xn,yn)[L(Wn+1) - L(Wn )]
≤E(Xn,yn)[h VL(Wn), wn+1 - Wni] + L'E(Xn,yn) [IM+n - Wn Il2]
--ηE(Xn,yn)KVL(Wn), v' (yn,f(xn; Wn))i] + 2E(Xn打)[kv' (yn,f (xn； Wn))『]
=) - ηkVL(Wn)k2 + η2LE(χn,yn)[∣V' (yn,f(Xn; Wn))『],
where (a) is due to the update of Wn+ι = Wn - ηV' (yn, f (xn； Wn))；(b) is due to the Assumption
of L satisfying the property in Definition 1 that E(χ,y) [Vw' (y, f (x； w))] = VL(w). By using
the Assumption of L satisfying the property in Definition 1 that ∣Vw'(y, f (x; w))∣ ≤ G and the
Assumption of L satisfying the property in Definition 3, we have
E(Xn,yn) [L(Wn+1) - L(Wn)]
η2LG2
≤ -2-
≤ η2LG2
≤	2
-ηkVL(Wn)k2
-2ημ(L(Wn) - L(w*)),
16
Under review as a conference paper at ICLR 2021
which implies
E(χn,yn)[L(Wn+1) -L(W*)]
η2LG2
≤ (I - 2ημ) E(xn-1,yn-l) [L(Wn) - L(w*)] +	2—
≤(I - 2ημ)n (L(wι)- L(WJ) + η~2— X(I - 2ημ)i.
i=0
Due to (1 - 2ημ)n ≤ exp(-2ημn) and Pn-I(I - 2ημ)i ≤ 蠢,then by using the setting of
η
1	8nμ2(L(wι) - L(w*))
诉 log V	G2L
we have
E(xn,yn) [L(Wn+1) - L(W*)]
≤ exp(-2ημn) (L(wι) - L(w*)) + ηG L
4μ
G2L	G2L	(8nμ2(L(wι) -L(w*))
8nμ2 + 8nμ2 og (	G2L
□
E Proof of Proposition 1
Proof. By using the Assumption of L satisfying the property in Definition 3, we have
Le(w*) - Le(we *)
≤M(w*)∣∣2
―	2μ
=)∣∣VL(w*) -VL(w*)k2
2μ
(b) δ2G2
≤ y-—
―	2μ
where (a) is due to the definition of w* in (6) so that VL(w*) = 0; (b) follows the same analysis
δ2G2
of (33) in Lemma 1. Thus We know w* ∈ A(Y) when Y ≥ γo := -^μ^-. On the other hand, by the
definition of μ(γ) in (12) and the Assumption of L satisfying the property in Definition 3, we know
μ(γ) ≥ μ when Y ≤ 8μ0.	□
F	Algorithm AugDrop and Proof of Theorem 1
We present the details of update steps for AugDrop and its convergence analysis in this section.
17
Under review as a conference paper at ICLR 2021
Algorithm 2 AugDrop
1:	Input: Ti
2:	Initialize: w1 ∈ RD, η1, η2 > 0
// Stage I: Train Augmented Data
3:	for t = 1, 2, . . . , T1 do
4:	draw m1 examples (xet,1, yet,1), . . . , (xet,m1 , yet,m1) at random from augmented data
5：	update wt+i = Wt -恶 Pm=* II Vw' (et,i,f (xt,i； Wt))
6:	end for
// Stage II: Train Original Data
7:	for t = Ti + 1, Ti + 2, . . . , Ti + n/m2 do
8:	draw m2 examples (xt,i, yt,i), . . . , (xt,m2, yt,m2) without replacement at random from orig-
inal data
9：	update wt+i = Wt — mm2 * * *2 Pm=I Vw' (yt,i,f (xt,i； Wt))
10:	end for
11:	Output: WT1+n/m2+i.
Proof. In the first stage of the proposed algorithm, we run a mini-batch SGD over the augmented
data D with mi as the size of mini-batch. Let (xet,i , yet,i ), i = 1, . . . , mi be the mi examples
sampled in the tth iteration. Let get be the average gradient for the t iteration, i.e.
1 m1
gt = m X Vw'(yt,i,f (Xt,i； Wt))
i i=i
We then update the solution by mini-batch SGD: Wt+i = Wt -ηigXt. By using Lemma 4 of (Ghadimi
et al., 2016), with a probability 1 - δ0, we have
gXt - VLX(Wt)
3log δ10) j8G^
(34)
ɪʌ , 1	A	, ∙	Γ∙	.∙f∙	,1	,	Trʌ C ∙ . ∙	AFj	1 . C∙	~
By the Assumption of L satisfying the property in Definition 2 and the update of Wt+i = Wt -ηigXt,
we have
、一、
LX(Wt+i) - LX(Wt)
≤ - ηιhVL(Wt),χti + η1— llχtk2
=η21 kVLχ(Wt) - Xtk2 - ηikVLχ(Wt)k2 - ηi(1 -ηiL) kXtk2
(≤)'η (1 + J3log τ0! 8G — ηιμ(Le(Wt) -Le(W*)),
2	δ0 mi
where (a) uses the facts that (34), ηi = 1/L and the Assumption of L satisfying the property in
Definition 3. Thus, with a probability (1 - δ0)t, using the recurrence relation, we have
LX(Wt+i) - LX(WX *)
≤(1 - ηιμ)(L(Wt) - L(W*)) + 罢
≤ (1 一 ηιμ)t (Le(Wi) — Lχ(W*)
1
δ0
3log δ10
t-i
X(1 - ηιμ)i.
i=0
(35)
DUeto (1 - ηιμ)t ≤ exp(-t¾μ) and Pt-0(1 - ¾μ)i ≤ 看,when
〜
〜
t ≥ Ti := ɪ log
ηιμ
δy2G2
2(L(wi) - L(W*))μ
18
Under review as a conference paper at ICLR 2021
we have
~.	.	~. 一.、
L(Wt+ι) — L(W *)
J / l ∖ I %/	∖	7/ ~ ∖
≤ exp(—tηιμ) (L(WI) -L(W*)
≤ 孚 +(1 +,logq
2μ	δ0
Let δ0 = 2δ-, if we choose —i such that
m1
3 log
8
砥,
^~1 2 4G2
3logδo) μmι
(36)
then for any t ≥ Ti, then with a probability 1 — δ∕2 We have
δ2G2
L(Wt+1) 一 L(WJ ≤ —-------.
(37)
μ
In the second stage of the proposed algorithm, we run a mini-batch SGD over the original data D
with m2 as the size of mini-batch. Let (xt,i, yt,i), i = 1, . . . , m2 be the m2 examples sampled in
the tth iteration. Let gbt be the average gradient for the t iteration, i.e.
1	m2
bt = — VW VW '(yt,i ,f(χt,i Wt))
2	i=1
We then update the solution Wt+1 = Wt — η2gbt. By using Lemma 4 of (Ghadimi et al., 2016), with
a probability 1 — δ00, we have
kbt-VL(wt)k ≤
3log δ10) V8Gr
(38)
ɪʌ ,1	,1	c∙ % /	∖	1 ,1	1 , C-	^	1
By the smoothness of L(W) and the update of Wt+1 = Wt — η2gbt, we have
L(Wt+1) — L(Wt)
η2 L
≤ — η2hVL(Wt), bti + η22- kbtk2
=η22 kvL(Wt) - btk2 - η22 kvL(Wt)k2 - η2(1 — n2-) kbtk2
(a)
≤η2kVL(Wt) -VL(Wt)k2 + η2kbt — VL(Wt)k2 — η2μ(L(Wt) -L(W*))
(b)
≤ η2	2δy2G2 +
3log δ10
一η2μ(L(wt) — L(W *)),
(39)
〜
〜
where (a) uses the facts that Young’s inequality, η2 ≤ 1∕- and the Assumption of L satisfying the
property in Definition 3; (b) uses inequality (38) and the same analysis of (33) in Lemma 1. It is
easy to verify that for any t ∈ {Ti + 1,...,Tι + n/m2 },we have, with a probability (1 — J00)n/m2,
we have
L(Wt+ι) — L(W *)
≤ δyG2 + 1 ∣G2δy +
μ μ y
t-1
X(1-η2μ)i
i=0
≤ (1 — η2μ) (Le(Wt)- Le(W*)) + η2 I G2δ2
≤ (1 — η2μ)) (L(wti+i) — Le(W*)) + η2 J G2δ2
19
Under review as a conference paper at ICLR 2021
where the last inequality is due to (1 - η2μ)t ≤ 1, Pt-1(1 - η2μ)i ≤ η1μ, and (37). Let δ00
δ
2n∕m2
then with a probability 1 - δ, for any t ∈ {T1 + 1, . . . , T1 + n/m2} we have
4δy2G2
wt+I)-L(W J ≤ —.
Therefore, Wt ∈ A(8γ0) for any t ∈ {T1 + 2, . . . , T1 + n/m2 + 1}. Following the standard analysis
in Appendix D, we have
E [L(WT1+n/m2 +I)-L(W J] ≤ 4nμ2 + 4nμ2 lθg
4nμ2(L(w1) - L(w*))
G2L
where μc = μ(8γ0).
□
G OPTIMAL SOLUTIONS OF La(w) AND Lc(w)
By the definition of `a in (14) and Px = Pex, we know
`a (ye, f (xe; w))
= min `(z, f (x; w))
kz-yek≤δy
≤'(y,f(x; w))
since ky - ye k ≤ δy. Therefore, by (15), (40) and Px = Pex we have
La (w)
=Ey[La(w)]
=E(X,e,y) ['a(y,f(χ; w))]
≤E(X,e,y) ['(y,f(χ;w))]
=E(X,y) ['(y,f(χ;w))]
=L(w).
Since ` is a non-negative loss function, then we know
0 ≤ La(w*) ≤ L(w*) = 0,
(40)
(41)
which implies that
La(w*)= 0,
and thus
La(w* ) ≤La(w), ∀w.
Therefore, w* also minimizes La (w).
On the other hand, by (16) we know
Lc(w*) = λL(w*) + (1 - λ)La(w*) = 0.
Therefore,
Lc(w*) ≤Lc(w), ∀w,
i.e, w* also minimizes Lc(w), indicating that Lc(W) shares the same minimizer as L(w).
20
Under review as a conference paper at ICLR 2021
H Algorithm MixLoss and Proof of Theorem 2
We present the details of update steps for MixLoss and its convergence analysis in this section.
Algorithm 3 MixLoss
1:	Input: λ
2:	Initialize: w1 ∈ RD, η > 0
3:	for t = 1, 2, . . . , n do
4:	draw an example (xt , yt) without replacement at random from original data
5:	draw m0 examples (xet,1, yet,1), . . . , (xet,m0, yet,m0) at random from augmented data
6：	compute bt = λV'(yt, f (xt; Wt)) + (1- λ) m^ Pm=OL V'a(yt,i,f (xt,i; Wt))
7:	update wt+1 = wt - ηgbt
8:	end for
9:	Output: Wn+1.
Proof. Recall that
Lc(W) = λL(W) + (1 - λ)La (W),	(42)
Where La (W)=E(χ,y) ['a(χ,f(χ; w))] = E(e,y)	@"匕 '(z,f (x； W)) and
kz-yek≤δy
mO
bt = λV'(yt,f (xt； Wt)) + (1- λ)- X V'a(Xt,i,f(Xt,i; wt)).	(43)
By the update of Wt+1 = Wt - ηbgt and by the Assumption of Lc satisfying the property in Defini-
tion 2, We have (Nesterov, 2004)
Et [Lc(Wt+1) - Lc(Wt)]
η* 2 * * * * * *L
≤-ηEt KVLc(Wt), bti] + η2- Et [kbtk2]
(a)
≤ -η(1 - η-)Et [kVLc(Wt)k2] + η2-Et [kgbt-VLc(Wt)k2]
≤) - η(1 - η-)Et [kVLc(Wt)k2] + 9λ2η2-Et [∣∣V'(yt,f(xt; Wt))-VL(Wt)『i
+ 9(1 - λ)2η2-Et	m~ X V'a(yt,i, f (Xt,i, Wt))-VLa(Wt) I ]
≤) - η(i - η-)Et [kVLc(Wt)k2] + 9λ2η2-G2 + 36(1 - λ)2η2-G2
2	m0
(d)
≤ -η(1 - η-)Et [kVLc(Wt)k2] +5λ2η2-G2,	(44)
where EtH is taken over random variables (Xt, yt), (Xt,ι, Xt,ι),∙∙∙, (Xt,m0, Xt,mo)； (a) Uses the
facts that Young’s inequality ka - bk2 ≤ 2kak2 + 2kbk2 and E[gbt] = VLc(Wt); (b) uses the
facts that (42) (43) and Young’s inequality ka + bk2 ≤ (1 + 1/c)kak2 + (1 + c)kbk2 with a = 8;
(c) use the same analysis in (31) from the proof of Lemma 1, the facts that the Assumption of L
satisfying the property in Definition 1 and by Jensen’s inequality, we also have kVL(W)k ≤ G,
implying that ∣∣V'(y, f (x; w)) -VL(W)『≤ 4G2; (d) holds by setting m0 ≥ 72(λ-λ) since
we have sufficiently large number of augmented examples. Thus, since η ≤ 2L and by using the
Assumption of Lc satisfying the property in Definition 3, we have
Et [Lc(wt+ι) - Lc(Wt)] ≤ - ημEt [Lc(Wt)] + 5λ2η2-G2,
21
Under review as a conference paper at ICLR 2021
and therefore
En [Lc(wn+1)]
≤ exp(-ημn) Lc(WI) +
5λ2ηLG2
μ
≤ exp (—ημn) L(wι) +
5λ2ηLG2
μ
(45)
where last inequality is due to the fact that Lc(W) ≤ L(W). In (45), by choosing
1 1 nμ2L(wι)
η = μn log -lgg^~，
we have
En [Lc(wn+1)] ≤
λ2 LG2 5λ2LG2	nμ2L(wι)
nμ2 + nμ2	og X2LG2
(46)
Since L(w) = λLc(W) - 1-λLa(w), then (46) becomes
En [L(wn+1)]
XLG2	5λLG2	nμ2L(wι)
≤ nμ2 + nμ2 og X2LG2
XLG2	5XLG2	nμ2L(wι)
≤ nμ2 + nμ2 og X2LG2
1-X
- F- EL"。]
where the last inequality is due to X ∈ (0,1) and La(Wn+ι) ≥ La(w*) = 0.
(47)
□
I Proofs in Appendix A
We include the proofs for Appendix section “Main Results for label-preserving Augmentation”.
I.1 Proof of Lemma 2
The analysis is similar to that for Lemma 1. For completeness, we include it here.
Proof. Following the same analysis in Lemma 1, we can have the same result as in (32). That is to
say, we have
E(et,yt)[L(Wt+ι) - L(Wt)] ≤ ? (∣Nl(Wt)- ▽《(Wt)k2 +-----------kVL(Wt州2) .	(48)
2	m0
We have
.. ~....
∣∣VL(wt) -VL(wt)k
≤/dχy∣∣Pχ(χ) - Pe(χ)kkV'(y,f(χ; wt))k
(a)
≤ G	dχkPx(χ) - Pxe(χ)k
(bJ
≤ G Vz2DKL(PxkPe)
= G√2δP,
(49)
where (a) is due to the Assumption of L satisfying the property in Definition 1; (b) uses Pinsker’s
inequality (Csiszar & Korner, 2011; Tsybakov, 2008); (C) is due to (18). With inequality (49),
22
Under review as a conference paper at ICLR 2021
by using the facts that η = 1/L and the Assumption of L satisfying the property in Definition 3,
inequality (48) becomes
E(xet,eyt) [L(wt+1) - L(wt)]
4G2	η
≤ηδp G + m------2 ∣∣VL(wt)k
4G2
≤ηδpG2 +-------ημ (L(Wt) — L(w*))
m0
≤2ηδpG2 — ημ(L(wt) — L(w*)),
where the last inequality is due to the selection ofm0 ≥急 . Then we have
E(et,yt)[L(Wt+1) -L(W*)]
≤ (I- nμ) E(et-ι,et-ι)[L(Wt)- L(W*)] + 2ηδPG2
t-1
≤(I- ημ)t (L(WI)- L(Wj) + 2ηδpG2 ^X(I - ημ)i
i=0
DUe to (1 - ημ)t ≤ exp(-tημ) and Pt=0(1 - ημ)i ≤ 击,When
L L L . (L(WI)-L(WJ)μ
t ≥ μlog	2δPG2	,
We knoW
4δP G2
E(et,y,)[L(Wt+ι) -L(W*)] ≤----.
μ
□
I.2	Proof of Proposition 2
Proof. By Using the AssUmption of L satisfying the property in Definition 3, We have
L(w*) - L(W*)
≤kVL(w*)∣∣2
―	2μ
=)∣∣VL(w*) -VL(w*)∣2
2μ
(b)δp G2
≤ ~μ~
where (a) is due to the definition of w* in (6) so that VL(w*) = 0; (b) follows the same analysis
of (49) in Lemma 2. Thus we know w* ∈ A(Y) when Y ≥ γι := δpμG-. On the other hand, by the
definition of μ(γ) in (12) and the Assumption of L satisfying the property in Definition 3, we know
μ(γ) ≥ μ when Y ≤ 4μι.
□
I.3	Proof of Theorem 3
This proof is similar to the proof of Theorem 1. For completeness, we include it here.
Proof. In the first stage of the proposed algorithm, we run a mini-batch SGD over the augmented
data D with m1 as the size of mini-batch. Using the similar analysis in (35) from Theorem 1, we
have, with a probability (1 - δ0)t,
L(Wt+1) -
Le(We *)
≤ (I - ηιμ)t
Le(W1) -
t-1
∑(1 - ηιμ)i.
i=0
23
Under review as a conference paper at ICLR 2021
DUeto (1 - ηιμ)t ≤ exp(-t5μ) and Pt-0(1 - ηιμ)i ≤ 康,when
〜
〜
we have
Let δ0
t ≥ Ti ：= L log
μ
L(wt+ι) - L(W*) ≤
今,if We choose mi such that
2(L(wι) - L(W*))μ
δPG2
______∖ 2
1	8G2
3logδo) 2μm7
(50)
m1
δPG2
k +
3log2T! δp,
〜
〜
then for any t ≥ Ti , we have
δPG2
L(wt+ι)-L(W*) ≤ ^.
μ
(51)
In the second stage of the proposed algorithm, we run a mini-batch SGD over the original data D
with m2 as the size of mini-batch. Using the same analysis in (39) from Theorem 1, we have
Le(Wt+1) - L(Wt) ≤η2INLe(Wt)- VL(wt)k2 + η2kbt - ▽£(Wt)Il2 - η2μ(Le(wt) - Le(W*)).
Then by (38) in the proof of Theorem 1 and (49) in the proof of Lemma 2, we have, with a probability
1 - δ00,
工 、工 、一” Λ	匚―T∖ 8G2	,工、工〜、、
L(wt+ι) - L(Wt) ≤2η2δpG2 + η2 1 + ∖∕3log -O7------η2μ(L(wt) - L(W*)).
δ00	m2
It is easy to verify that for any t ∈ {Ti + 1, . . . , Ti + n/m2}, we have, with a probability (1 -
j//)n/m2, We have
L(wt+ι) - L(W*)
≤ (1 - η2μ) (Le(wt) - Le(W*)) + η2 2δpG2 +
n/m2 -i
X (1-η2μ)i
i=0
≤ (1 - η2μ)n∕m2(L(wTi + i) - L(W*)) + η2 2δpG2 +
≤ δpG2 + 1	2δp G2 +
μ μ
where the last inequality is due to (1 - η2μ)t ≤ 1, Pt-I(I — η2μ)i ≤ 六,and (51). Let δ0
2n∕δm2, if we choose m2 SUch that m2 ≥(1 + q log 悬)δp, for example,
m2
Wl	2n∖ 2 8
3log 手	δP，
then with a probability 1 - δ, for any t ∈ {Ti + 1, . . . , Ti + n/m2} we have
~.	~. 一.、
L(wt+ι) - L(W*) ≤
4δp G2
μ
Therefore, Wt ∈ A(4γi) for any t ∈ {Ti + 2, . . . , Ti + n/m2 + 1}. Following the similar analysis
in Appendix D, we have
E [L(WT1+n/m2 + l) -L(W*)] ≤ 研 + 研 log
4nμ2(L(wι) - L(w*))
G2L
where μe = μ(4γι).
□
24