Under review as a conference paper at ICLR 2021
Physics-aware Spatiotemporal Modules with
Auxiliary Tasks for Meta-Learning
Anonymous authors
Paper under double-blind review
Ab stract
Modeling the dynamics of real-world physical systems is critical for spatiotem-
poral prediction tasks, but challenging when data is limited. The scarcity of real-
world data and the difficulty in reproducing the data distribution hinder directly
applying meta-learning techniques. Although the knowledge of governing partial
differential equations (PDE) of the data can be helpful for the fast adaptation to
few observations, it is mostly infeasible to exactly find the equation for obser-
vations in real-world physical systems. In this work, we propose a framework,
physics-aware meta-learning with auxiliary tasks whose spatial modules incorpo-
rate PDE-independent knowledge and temporal modules utilize the generalized
features from the spatial modules to be adapted to the limited data, respectively.
The framework is inspired by a local conservation law expressed mathematically
as a continuity equation and does not require the exact form of governing equa-
tion to model the spatiotemporal observations. The proposed method mitigates the
need for a large number of real-world tasks for meta-learning by leveraging spatial
information in simulated data to meta-initialize the spatial modules. We apply the
proposed framework to both synthetic and real-world spatiotemporal prediction
tasks and demonstrate its superior performance with limited observations.
1	Introduction
Deep learning has recently shown promise to play a major role in devising new solutions to applica-
tions with natural phenomena, such as climate change (Manepalli et al., 2019; Drgona et al., 2019),
ocean dynamics (Cosne et al., 2019), air quality (Soh et al., 2018; Du et al., 2018; Lin et al., 2018),
and so on. Deep learning techniques inherently require a large amount of data for effective represen-
tation learning, so their performance is significantly degraded when there are only a limited number
of observations. However, in many tasks in physical systems in the real-world we only have access
to a limited amount of data. One example is air quality monitoring (Berman, 2017), in which the
sensors are irregularly distributed over the space - many sensors are located in urban areas whereas
there are much fewer sensors in vast rural areas. Another example is extreme weather modeling and
forecasting, i.e., temporally short events (e.g., tropical cyclones (Racah et al., 2017b)) without suf-
ficient observations over time. Moreover, inevitable missing values from sensors (Cao et al., 2018;
Tang et al., 2019) further reduce the number of operating sensors and shorten the length of fully-
observed sequences. Thus, achieving robust performance from a few spatiotemporal observations in
physical systems remains an essential but challenging problem.
Learning on a limited amount of data from physical systems can be considered as a few shot learn-
ing. While recently many meta-learning techniques (Schmidhuber, 1987; Andrychowicz et al., 2016;
Ravi & Larochelle, 2017; Santoro et al., 2016; Snell et al., 2017; Finn et al., 2017) have been de-
veloped to address this few shot learning setting, there are still some challenges for the existing
meta-learning methods to be applied in modeling natural phenomena. First, it is not easy to find a
set of similar meta-tasks which provide shareable latent representations needed to understand tar-
geted observations. For instance, while image-related tasks (object detection (He et al., 2017) or
visual-question-answering tasks (Andreas et al., 2016; Fukui et al., 2016)) can take advantage of an
image-feature extractor pre-trained by a large set of images (Deng et al., 2009) and well-designed ar-
chitecture (Simonyan & Zisserman, 2014; He et al., 2016; Sandler et al., 2018), there is no such large
data corpus that is widely applicable for understanding natural phenomena. Second, unlike computer
vision or natural language processing tasks where a common object (images or words) is clearly de-
1
Under review as a conference paper at ICLR 2021
fined, it is not straightforward to find analogous objects in the spatiotemporal data. Finally, exact
equations behind natural phenomena are usually unknown, leading to the difficulty in reproducing
the similar dataset via simulation. For example, although there have been some works (de Bezenac
et al., 2018; Lutter et al., 2019; Greydanus et al., 2019) improving data efficiency via explicitly in-
corporating PDEs as neural network layers when modeling spatiotemporal dynamics, it is hard to
generalize for modeling different or unknown dynamics, which is ubiquitous in real-world scenario.
In this work, we propose physics-aware modules designed for meta-learning to tackle the few shot
learning challenges in physical observations. One of fundamental equations in physics describing
the transport of physical quantity over space and time is a continuity equation:
^P + ▽• J =σ,	(I)
∂t
where ρ is the amount of the target quantity (u) per unit volume, J is the flux of the quantity, and
σ is a source or sink, respectively. This fundamental equation can be used to derive more specific
transport equations such as the convection-diffusion equation, Navier-Stokes equations, and Boltz-
mann transport equation. Thus, the continuity equation is the starting point to model spatiotemporal
(conservative) observations which are accessible from sensors. Based on the form of ρ and J with
respect to a particular quantity u, Eq. 1 can be generalized as:
∂u
布=F (Vu, V2u,...),	(2)
∂t
where the function F(∙) describes how the target U is changed over time from its spatial derivatives.
Inspired by the form of Eq. 2, we propose two modules: spatial derivative modules (SDM) and time
derivative modules (TDM). Since the spatial derivatives such as V, V∙, and V2 are commonly used
across different PDEs, the spatial modules are PDE-independent and they can be meta-initialized
from synthetic data. Then, the PDE-specific temporal module is trained to learn the unknown func-
tion F(∙) from few observations in the real-world physical systems.
This approach can effectively leverage a large amount of simulated data to train the spatial modules
as the modules are PDE-independent and thus mitigating the need for a large amount of real-world
tasks to extract shareable features. In addition, since the spatial modules are universally used in
physics equations, the representations from the modules can be conveniently integrated with data-
driven models for modeling natural phenomena. Based on the modularized PDEs, we introduce
a novel approach that marries physics knowledge in spatiotemporal prediction tasks with meta-
learning by providing shareable modules across spatiotemporal observations in the real-world.
Our contributions are summarized below:
•	Modularized PDEs and auxiliary tasks: Inspired by forms of PDEs in physics, we de-
compose PDEs into shareable (spatial) and adaptation (temporal) parts. The shareable one
is PDE-independent and specified by auxiliary tasks: supervision of spatial derivatives.
•	Physics-aware meta-learning: We provide a framework for physcis-aware meta-learning,
which consists of PDE-independent/-specific modules. The framework is flexible to be
applied to the modeling of different or unknown dynamics.
•	Synthetic data for shareable modules: We extract shareable parameters in the spatial
modules from synthetic data, which can be generated from different dynamics easily.
2	Modularized PDEs and Meta-Learning
In this section, we describe how the physics equations for conserved quantities are decomposable
into two parts and how the meta-learning approach tackles the task by utilizing synthetic data when
the data are limited.
2.1	Decomposability of Variants of a Continuity Equation
In physics, a continuity equation (Eq. 1) describes how a locally conserved quantity such as temper-
ature, fluid density, heat, and energy is transported across space and time. This equation underlies
2
Under review as a conference paper at ICLR 2021
n
αiU(xi),	(4)
i=1
many specific equations such as the convection-diffusion equation and Navier-Stokes equations:
U = V ∙ (DVu) - V ∙ (vu) + R,	(Convection-Diffusion eqn.)
U = -(u ∙ V)u + VV2u — Vω + g. (Incompressible Navier-Stokes eqn.)
where the scalar u and vector field u are the variables of interest (e.g., temperature, flow velocity,
etc.). A dot over a variable is time derivative. The common feature in these equations is that the
forms of equations can be digested as (Bar-Sinai et al., 2019; Zhuang et al., 2020):
U = F (Ux, Uy, uxx, Uyy,... ),	(3)
where the right-hand side denotes a function of spatial derivatives. As the time derivative can be
seen as a Euler discretization (Chen et al., 2018), it is notable that the next state is a function of the
current state and spatial derivatives. Thus, knowing spatial derivatives at time t is a key step for
spatiotemporal prediction at time t + 1 for locally conserved quantities. According to Eq. 3, the
spatial derivatives are universally used in variants of Eq. 1 and only the updating function F(∙) is
specifically defined for a particular equation. This property implies that PDEs for physical quantities
can be decomposable into two modules: spatial and temporal derivative modules.
2.2	Spatial Derivative Modules: PDE-independent Modules
Finite difference method (FDM) is widely used to discretize a d-order derivative as a linear combi-
nation of function values on a n-point stencil.
∂du
∂xd
where n > d. According to FDM, it is independent for a form of PDE to compute spatial deriva-
tives, which are input components of F(∙) in Eq. 3. Thus, We can modularize spatial derivatives as
PDE-independent modules. The modules that can be learnable as a data-driven manner to infer the
coefficients (αi) have been proposed recently (Bar-Sinai et al., 2019; Seo et al., 2020). The data-
driven coefficients are particularly useful when the discretization in the n-point stencil is irregular
and low-resolution where the fixed coefficients cause substantial numerical errors.
2.3	Time Derivative Module: PDE-specific Module
Once upto d-order derivatives are modularized by learnable parameters, the approximated spatial
derivatives from the spatial modules are fed into an additional module to learn the function F(∙) in
Eq. 3. This module is PDE-specific as the function F describes how the spatiotemporal observations
change. Since the exact form ofa ground truth PDE is not given, the time derivative module is data-
driven and will be adapted to observations instead.
2.4	Meta-Learning with PDE-independent/-specific Modules
Recently, Raghu et al. (2019) investigate the effectiveness of model agnostic meta-learning
(MAML, Finn et al. (2017)) and it is found that the outer loop of MAML is more likely to learn
parameters for reusable features rather than rapid adaptation. The finding that feature reuse is the
predominant reason for efficient learning of MAML allows us to use additional information which
is beneficial for learning better representations. Previously, the objective in meta-training has been
considered to be matched with one in meta-test as the purpose of meta-learning is to learn good
initial parameters applicable across similar tasks (e.g., image classification to image classification).
We are now able to incorporate auxiliary tasks under a meta-learning setting to reinforce reusable
features for a main task. As described in Sec. 2.1, the spatial modules are reusable across differ-
ent observations, and thus, we can meta-initialize the spatial modules first with spatial derivatives
provided by synthetic datasets. Then, we can integrate the spatial modules with the task-specific
temporal module during meta-test to help adaptation of TDM on few observations. Since the spa-
tial modules are trained by readily available synthetic datasets, a large number of similar tasks for
meta-training is not required.
3
Under review as a COnferenCe PaPer ar ICLR 2021
Lab-Ofobjecfive
7⅛smdependenf modu-es
7⅛sspeQfic modu-e
Forward PaSS
Backward PaSS
FigUre h SChemariC OVerVieW Ofrhe PhySiCS—aware mera—leaming (PiMeraL)∙
3 PHYSlCS—AWARE META—LEARNING WlTH AUXlLIARY TASKS
In this SeCriOFWe develop a PhySiCS—aware mera—leaming framework for the modularized PDES ∙
Fig∙ 1 describes the ProPOSed framework and rs COmPUrariOnaI process-
3 ∙ 1 SPATlAL DERlVATlVE MoDULE
AIgorifhm 1 SPariaI derivative module (SDM)
InpuK GraPh SignaIS UA and edge features = aj ——ad on g Where ad is a COOrdinare Of IlOde i.
OufpuK SPariaI derivatives {F-2 m a and A: m 身 Where 反={弋沁 ZyZ” V5)∙
ReqUirc SPariaI derivative modules !9 一 A; m 反}
I	: for A; m 反 do
2	17⅛'殳⅛ħ(2⅛mHand A m 因} H 9({N 丁 {e} a)
3	- for 2 m W do
e PkW H ɑΛdf +M(j⅛∈HbΛQ⅛s
5 1 end5sr
61 end for
AS We focus On the modeHng and PrediCriOn Of SenSor—based ObSerVariOn∞where the available da〔a
PoinrS are inherentIy On a SParia=y SParSe irregular grip we USe graph IlerWorkS for each module ek
S IeanI the Hnire diKerenCe COefHCienrS (Bar—Sinai er aΓ2019) ∙ Given a graph 9 = (‹ħ) Where
W = {1: •二 N} andH= {(uy∙)".」• m WT a IlOde i denores a physica IIOCarion & = (Hr'9d)
Where a fιmcrion VaIUe £ =ΛRr'£is observed。TheFrhe graph SignaIS Wirh PoSirionalrelariVe
displacement: as edge features are fed Wo the SPariaI modules S approXimare SPariaI derivatives
by Alg∙ L The coeBcienrs (αr'b(d8) on each IlOde i and edge (Uy) are OUrPUr Of φ and they are
Hneally COmbB∙ed Wirh the fιmcrion VaIUeS ua and Uj.反 denores a Ser Of Hnire diKerenCe OPerarorS ∙
For examppif WeSet:反={va;" Vs V* v*}3we have 4 modules WhiCh approXimare first/SeCond
Order Of SPatiaI derivarivesB∙2ldimensioF respectively
3∙2 TlMEDERIVATlVEMoDULE
OnCe SPariaI derivatives are approXi—
mareʤ another Ieamable module is re—
quired S CombB∙e them for a rarge 二 ask∙
The form OfHne 2 in AIg ∙ 2 COmeS from
Eq ∙ 3 and TDM is adapted S Ieam the
UnknoWn fιmcrion F(∙)B∙rhe equaro∙n∙
AS OUrrargerraSk is the regression Of
graph Signal∞We USe a recurre∏r graph
IlerWOrk for TDM ∙
AIgorifhm 2 Time derivative module (TDM)
InpuΓtGraPh SignaIS U and approXimared SPariaI
derivatives F Where k m 反 Ong ∙ Time i∏rerval ∆f
OufpuK PrediCriOn Of SignaIS ar neXrrime SreP O(-)
ReqUire Time derivative module
I: O-H TDM({F F—2 m W and k m 反})
2： Oh) HΛ-——1) + i ∙ ∆e
4
Under review as a conference paper at ICLR 2021
Figure 2: Examples of generated spatial function values and graph signals. Node and edge features
(function value and relative displacement, respectively) are used to approximate spatial derivatives
(arrows). We can adjust the number of nodes (spatial resolution), the number of edges (discretiza-
tion), and the degree of fluctuation (scale of derivatives) to differentiate meta-train tasks.
3.3 Meta-Learning with Auxiliary Objective
As discussed in Sec. 2.1, it is important to know spatial derivatives at time t to predict next signals
at time t + 1 for locally conserved physical quantities, however, it is impractical to access the spa-
tial derivatives in the sensor-based observations as they are highly discretized over space. In this
section, we propose a physics-aware meta-learning framework to meta-initialize a spatial module
by leveraging synthetic dataset with auxiliary tasks to provide reusable features for the main tasks:
prediction spatiotemporal observations in the real-world.
The meta-initialization with the auxiliary tasks from synthetic datasets is particularly important.
First, the spatial modules can be universal feature extractors for modeling observations following un-
known physics-based PDEs. Unlike other domains such as computer vision, it has been considered
that there is no particular shareable architecture for learning spatiotemporal dynamics from physical
systems. We propose that the PDE-independent spatial modules can be applicable as feature extrac-
tors across different dynamics as long as the dynamics follow a local form of conservation laws.
Second, we can utilize synthetic data to meta-train the spatial modules as they are PDE-agnostic.
This property allows us to utilize a large amount of synthetic datasets which are readily generated
by numerical methods regardless of the exact form of PDE for targeted observations. Finally, we
can provide a stronger inductive bias which is beneficial for modeling real-world observations but
not available in the observations explicitly.
Algorithm 3 Meta-initialization with auxiliary tasks: Supervision of spatial derivatives
Input: A set of meta-train task datasets D = {D1, . . . , DB} where Db = (Dbtr, Dbte).
Db = {(ub, ebj, y(a1 ,b),..., y(aK,b)) : i ∈ Vb, (i,j) ∈ Eb} where y(ak,∙) is an k-th auxiliary task
label for the i-th node, given node/edge feature ub and eb, respectively. Learning rate α and β.
Output: Meta-initialized spatial modules Φ.
1:	Initialize auxiliary modules Φ = (φ1, . . . , φK)
2:	while not converged do
3:	for Db in D do
4:	Φb = Φ -αVφ PK=ILaux(Dtr； φk)
5:	end for
6： Φ 一 Φ - βVφ PB=I PK=ι Laux(Dbe; Φb,k)
7: end while
Alg. 3 describes how the spatial modules are meta-initialized by MAML under the supervision of
K different spatial derivatives. First, we generate values and spatial derivatives on a 2D regular grid
from an analytical function. Then, we sample a finite number of points from the regular grid to
represent discretized nodes and build a graph from the sampled nodes. Each graph signal and its
discretization becomes input feature ofa meta-train task and corresponding spatial derivatives are the
auxiliary task labels. Fig. 2 visualizes graph signals and spatial derivatives for meta-initialization.
Once the spatial modules are initialized throughout meta-training, we reuse the modules for meta-
test where the temporal module (the head of the network) are adapted on few observations from
real-world sensors (Alg. 4). Although the standard MAML updates the body of the network (the
spatial modules) as well, we only adapt the head layer (θ) as like almost-no-inner-loop method
5
Under review as a conference paper at ICLR 2021
Algorithm 4 Adaptation on meta-test tasks
Input: A set of meta-test task datasets D = {D1, . . . , DM} where Db = (Dmtr, Dmte).
Meta-initialized SDM (Φ). Learning rate α.
Output: Adapted TDM θm0 on the m-th task.
1:	Initialize temporal modules (θ1, . . . , θM)
2:	for Dm in D do
3:	θm = Om - αVθm L(Dmg,θm)
4:	end for
in Raghu et al. (2019). The task at test time is graph signal prediction and the temporal modules (θ)
are adapted by a regression loss function L = PT=ι ||u(t) — U(t)∣∣2 on length T sequence (Dm)
and evaluated on held-out (t > T) sequence (Dmte) with the adapted parameters.
4 Spatial Derivative Modules: Reusable Modules
We have claimed that the spatial modules provide reusable features associated with spatial deriva-
tives such as Vxu, Vyu, and V2xu across different dynamics or PDEs. While it has been shown
that the data-driven approximation of spatial derivatives is more precise than that of finite difference
method (Seo et al., 2020; Bar-Sinai et al., 2019), it is not guaranteed that the modules effectively
provide transferrable parameters for different spatial resolution, discretization, and fluctuation of
function values. We explore whether the proposed spatial derivative modules based on graph net-
works can be used as a feature provider for different spatial functions and discretization.
We perform two sets of experiments: evaluate
few-shot learning performance (1) when SDM
is trained from scratch; (2) when SDM is meta-
initialized. Fig. 2 shows how the graph signal
and its discretization is changed over the differ-
ent settings. If the number of nodes is large, it
can provide spatially high-resolution and thus,
the spatial derivatives can be more precisely ap-
Table 1: Parameters for synthetic dataset.
	Meta-train	Meta-test
# nodes (N)	{256, 625}	{450, 800}
# edges per a node (E)	{4, 8}	{3, 6, 10}
Initial frequency (F )	{2,5}	{3, 7}
proximated. Table 1 shows the parameters we used to generate synthetic datasets. Note that meta-
test data is designed to evaluate interpolation/extrapolation properties. Initial frequency decides the
degree of fluctuation (In Fig. 2, the middle one has higher F than that of the left one.). For each
parameter combination, we generate 100 different snapshots from the following form in Long et al.
(2017):
Ui = E λk,ι cos(kxi + lyi) + Yk,ι sin(kxi + lyi), λ%ι,γk,ι 〜N (0,0.02),
∣k∣,∣ι∣≤F
(5)
where the index i denotes the i-th node whose coordinate is (xi, yi) in the 2D space ([0, 2π] ×
[0, 2π]) and k, l are randomly sampled integers. From the synthetic data, the first- and second-order
derivatives are analytically given and SDM is trained to approximate them.
The prediction results for spatial derivatives are shown in Table 2. The results show that the pro-
posed module (SDM) is efficiently adaptable to different configuration on few samples from meta-
initialized parameters compared to learning from scratch. The finding implies that the parameters
for spatial derivatives can be generally applicable across different spatial resolution, discretization,
and function fluctuation.
5	Experimental Evaluation
5.1	Preliminary: Which synthetic dynamics need to be generated ?
While Table 2 demonstrates that the PDE-independent representations are reusable across different
configurations, it is still an open question: which topological configuration needs to be used to
construct the synthetic dynamics? According to Table 2, the most important factor affecting error is
6
Under review as a conference paper at ICLR 2021
Table 2: Prediction error (MAE) of the first (top) and second (bottom) order spatial derivatives.
(N,E,F)	(450,3,3)	(450,3,7)	(450,6,3)	(450,6,7)	(450,10,3)	(450,10,7)
SDM (from scratch)	1.337±0.044	7.111±0.148	1.152±0.043	7.206±0.180	1.112±0.036	7.529±0.241
	7.278±0.225	51.544±0.148	5.997±0.083	47.527±0.768	5.353±0.193	47.356±0.560
SDM (pretrained)	1.075±0.005	5.528±0.010	0.836±0.002	5.354±0.001	0.782±0.006	5.550±0.012
	6.482±0.207	46.254±0.262	5.251±0.245	42.243±0.420	4.728±0.244	42.754±0.442
(N,E,F)	(800,3,3)	(800,3,7)	(800,6,3)	(800,6,7)	(800,10,3)	(800,10,7)
SDM (from scratch)	1.022±0.030	5.699±0.242	0.789±0.021	5.179±0.069	0.718±0.010	5.517±0.110
	7.196±0.159	49.602±0.715	5.386±0.136	42.509±1.080	4.536±0.204	39.642±1.173
SDM (pretrained)	0.927±0.006	4.415±0.011	0.656±0.008	3.977±0.025	0.570±0.006	4.107±0.019
	6.553±0.193	44.591±0.002	4.960±0.266	37.629±0.760	4.213±0.275	35.849±0.947
Figure 3: Visualization of the first 5 frames of one extended sequence in the extreme weather dataset.
Dots represent the sampled points. Greenish (purplish) area is higher (lower) surface temperature.
an initial frequency (F), which determines min/max scales and fluctuation of function values, and
it implies that the synthetic dynamics should be similarly scaled to a target dynamics. We use the
same topological configuration in Table 1 to generate synthetic datasets for a task in Section 5.2 and
adapted configuration for a task in Section 5.3. We describe more details in Appendix B.
5.2	Multi-step Graph Signal Generation
Task: We adopt a set of multi-step spatiotemporal sequence generation tasks to evaluate our pro-
posed framework. In each task, the data is a sequence of L frames, where each frame is a set of
observations on N nodes in space. Then, we train an auto-regressive model with the first T frames
(T -shot) and generate the following L - T frames repeatedly from a given initial input (T -th frame)
to evaluate its performance.
Datasets: For all ex- periments, we generate meta-train tasks with the parameters described in Table 1 and the target	Table 3: Multi-step prediction results (MSE) and standard deviations on the two real-world datasets.			
	T -shot	Method	AQI-CO	ExtremeWeather
observations are 2 real-		FDM+RGN (scratch)	0.0291±0.0039	0.9883±0.5567
world datasets: (1) AQI-	5-shot	PA-DGN (scratch)	0.0363±0.0090	0.9653±0.1384
CO: national air quality		PiMetaL (meta-init)	0.0253±0.0055	0.9167±0.0746
index (AQI) observa-		FDM+RGN (scratch)	0.0258±0.0023	0.7626±0.0602
tions (Berman, 2017);	7-shot	PA-DGN (scratch)	0.0225±0.0018	0.7478±0.0199
(2) ExtremeWeather:		PiMetaL (meta-init)	0.0182±0.0019	0.7274±0.0089
the extreme weather		FDM+RGN (scratch)	0.0213±0.0013	0.7090±0.0030
dataset (Racah et al.,	10-shot	PA-DGN (scratch)	0.0146±0.0005	0.4156±0.0145
2017b). For the AQI-CO dataset, we construct 12		PiMetaL (meta-init)	0.0115±0.0004	0.4066±0.0247
				
meta-test tasks with the				
carbon monoxide (CO) ppm records from the first week of each month in 2015 at land-based
stations. For the extreme weather dataset, we select the top-10 extreme weather events with the
longest lasting time from the year 1984 and construct a meta-test task from each event with the
7
Under review as a conference paper at ICLR 2021
Table 4: Graph signal regression results (MSE, 10-3) and standard deviations on the two regions of
weather stations.
T -shot (Region)	GCN	GAT	GraphSAGE	GN	PA-DGN	PiMetaL
5-shot (USA)	2.742±0.120	2.549±0.115	2.128±0.146	2.252±0.131	1.950±0.152	1.794±0.130
10-shot (USA)	2.371±0.095	2.178±0.066	1.848±0.206	1.949±0.115	1.687±0.104	1.567±0.103
5-shot (EU)	1.218±0.218	1.161±0.234	1.165±0.248	1.181±0.210	0.914±0.167	0.781±0.019
10-shot (EU)	1.186±0.076	1.142±0.070	1.044±0.210	1.116±0.147	0.831±0.058	0.773±0.014
observed surface temperatures at randomly sampled locations. Since each event lasts fewer than 20
frames, each task has a very limited amount of available data. In both datasets, graph signals are
univariate. Note that all quantities have fluidic properties such as diffusive and convection. Fig. 3
shows the spatiotemporal dynamics of the extreme weather observations and sampled points. More
details are in the supplementary material.
Baselines: We evaluate the performance of a physics-aware architecture (PA-DGN) (Seo et al.,
2020), which also consists of spatial derivative modules and recurrent graph networks (RGN), to
see how the additional spatial information affects prediction performance for same architecture.
Note that PA-DGN has same modules in PiMetaL and the difference is that PiMetaL utilizes meta-
initialized spatial modules and PA-DGN is randomly initialized for learning from scratch on meta-
test tasks. Additionally, the spatial modules in PA-DGN is replaced by finite difference method
(FDM+RGN) to see if the numerical method provides better PDE-agnostic representations. The
baselines and PiMetaL are trained on the meta-test support set only to demonstrate how the addi-
tional spatial information is beneficial for few-shot learning tasks.
Discussion: Table 3 shows the multi-step prediction performance of our proposed framework against
the baselines on real-world datasets. Overall, PA-DGN and PiMetaL show similar trend such that
the prediction error is decreased as longer series are available for few-shot adaptation. There are
two important findings: first, with the similar expressive power in terms of the number of learn-
able parameters, the meta-initialized spatial modules provide high quality representations which are
easily adaptable across different spatiotemporal dynamics in the real-world. This performance gap
demonstrates that we can get a stronger inductive bias from synthetic datasets without knowing
PDE-specific information. Second, the contribution of the meta-initialization is more significant
when the length of available sequence is shorter (T = 5) and this demonstrates when the meta-
initialization is particularly effective. Finally, the finite difference method provides proxies of exact
spatial derivatives and the representations are useful particularly when T = 5 but its performance is
rapidly saturated and it comes from the gap between the learnable spatial modules and fixed numer-
ical coefficients. The results provide a new point of view on how to utilize synthetic or simulated
datasets to handle challenges caused by limited datasets.
5.3	Graph S ignal Regression
Task, datasets, and baselines: Defferrard et al. (2019) conducted a graph signal regression task:
predict the temperature xt from the temperature on the previous 5 days (xt-5 : xt-1). We split
the GHCN dataset1 spatially into two regions: (1) the USA (1,705 stations) and (2) Europe (EU)
(703 stations) where there are many weather stations full functioning. In this task, the number of
shots is defined as the number of input and output pairs to train a model. As the input length is
fixed, more variants of graph neural networks are considered as baselines. We concatenate the 5-
step signals and feed it into Graph convolutional networks (GCN) (Kipf & Welling, 2017), Graph
attention networks (GAT) (Velickovic et al., 2018), GraphSAGE (Hamilton et al., 2017), and Graph
networks (GN) (Battaglia et al., 2018) to predict next signals across all nodes.
Discussion: Table 4 shows the results of the graph signal regression task across different baselines
and the proposed method. There are two patterns in the results. First, although in general we
observe an improvement in performance for all methods when we move from the 5-shot setting to
the 10-shot setting, PiMetaL’s performance yields the smallest error. Second, for the EU dataset,
while 5-shot seems enough to achieve stable performance, it demonstrates that the PDE-independent
1Global Historical Climatology Network (GHCN) provided by National Oceanic and Atmospheric Admin-
istration (NOAA). https://www.ncdc.noaa.gov/ghcn-daily-description
8
Under review as a conference paper at ICLR 2021
representations make the regression error converge to a lower level. Overall, the experimental results
prove that the learned spatial representations from simulated dynamics are beneficial for learning on
limited data.
6	Related Work
Physics-informed learning Since physics-informed neural networks are introduced in Raissi et al.
(2019), which find that a solution of a PDE can be discovered by neural networks, physical knowl-
edge has been used as an inductive bias for deep neural networks. Advection-diffusion equation is
incorporated with deep neural networks for sea-surface temperature dynamics (de Bezenac et al.,
2018). Lutter et al. (2019); Greydanus et al. (2019) show that Lagrangian/Hamiltonian mechanics
can be imposed to learn the equations of motion of a mechanical system and Seo & Liu (2019)
regularizes a graph neural network with a specific physics equation. Rather than using explicitly
given equations, physics-inspired inductive bias is also used for reasoning dynamics of discrete ob-
jects (Battaglia et al., 2016; Chang et al., 2016) and continuous quantities (Seo et al., 2020). Long
et al. (2017; 2019) propose a numeric-symbolic hybrid deep neural network designed to discover
PDEs from observed dynamic data. While there are many physics-involved works, to the best of our
knowledge, we are the first to provide a framework to use the physics-inspired inductive bias under
the meta-learning settings to tackle the limited data issue which is pretty common for real-world
data such as extreme weather events (Racah et al., 2017b).
Meta-learning The aim of meta-learning is to enable learning parameters which can be used for
new tasks unknown at the time of learning, leading to agile models which adapt to a new task utiliz-
ing only a few samples (Schmidhuber, 1987; Naik & Mammone, 1992; Thrun & Pratt, 1998). Based
on how the knowledge from the related tasks is used, meta-learning methods have been classified as
optimization-based (Andrychowicz et al., 2016; Ravi & Larochelle, 2017; Duan et al., 2017; Finn
et al., 2017; Nichol et al., 2018; Antoniou et al., 2018; Rusu et al., 2018; Grant et al., 2018), model-
based (Santoro et al., 2016; Munkhdalai & Yu, 2017; Duan et al., 2017; Mishra et al., 2018), and
metric-based (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017). Recently, another branch
of meta-learning has been introduced to more focus on finding a set of reusable modules as compo-
nents of a solution to a new task. Alet et al. (2018; 2019) provide a framework, structured modular
meta-learning, where a finite number of modules are introduced as task-independent modules and
an optimal structure combining the modules is found from a limited number of data. Chen et al.
(2019) introduces techniques to automatically discover task-independent/dependent modules based
on Bayesian shrinkage to find more adaptable modules. To our knowledge, none of the above works
provide a solution to use meta-learning for modeling physics-related spatiotemporal dynamics where
it is hard to generate enough tasks for meta-initialization.
7	Conclusion
In this paper, we propose a framework for physics-aware meta-learning with auxiliary tasks. By
incorporating PDE-independent/-invariant knowledge (spatial derivatives) from simulated data, the
framework provide reusable features to meta-test tasks with a limited amount of data. Experiments
show that auxiliary tasks and physics-aware meta-learning help construct reusable modules that
improve the performance of spatiotemporal predictions in real-world tasks where data is limited.
Although introducing auxiliary tasks based on synthetic datasets improves the prediction perfor-
mance, they need to be chosen and constructed manually and intuitively. Designing and identifying
the most useful auxiliary tasks and data will be the focus of our future work.
9
Under review as a conference paper at ICLR 2021
References
Ferran Alet, Tomas Lozano-Perez, and Leslie P Kaelbling. Modular meta-learning. arXiv preprint
arXiv:1806.10166, 2018.
Ferran Alet, Erica Weng, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Neural relational inference with fast
modular meta-learning. In Advances in Neural Information Processing Systems, pp. 11804-11815, 2019.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 39-48, 2016.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances
in neural information processing systems, pp. 3981-3989, 2016.
Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. arXiv preprint
arXiv:1810.09502, 2018.
Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Learning data-driven discretizations
for partial differential equations. Proceedings of the National Academy of Sciences, 116(31):15344-15349,
2019.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning
about objects, relations and physics. In Advances in neural information processing systems, pp. 4502-4510,
2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz
Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive
biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
Lex Berman. National aqi observations (2014-05 to 2016-12). Harvard Dataverse, 2017. doi: 10.7910/DVN/
QDX6L8. URL https://doi.org/10.7910/DVN/QDX6L8.
Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation for
time series. In Advances in Neural Information Processing Systems, pp. 6775-6785, 2018.
Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional object-based
approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equa-
tions. In Advances in neural information processing systems, pp. 6571-6583, 2018.
Yutian Chen, Abram L Friesen, Feryal Behbahani, David Budden, Matthew W Hoffman, Arnaud Doucet, and
Nando de Freitas. Modular meta-learning with shrinkage. arXiv preprint arXiv:1909.05557, 2019.
Gautier Cosne, Guillaume Maze, and Pierre Tandeo. Coupling oceanic observation systems to study mesoscale
ocean dynamics. arXiv preprint arXiv:1910.08573, 2019.
Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes: Incorporating
prior scientific knowledge. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=By4HsfWAZ.
Michael Defferrard, Martino Milani, Frederick Gusset, and Nathanael Perraudin. Deepsphere: a graph-based
spherical cnn. In International Conference on Learning Representations, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Ieee,
2009.
Jan Drgona, Lieve Helsen, and Draguna Vrabie. Stripping off the implementation complexity of physics-based
model predictive control for buildings via deep learning. https://www. nips. cc/, 2019.
Shengdong Du, Tianrui Li, Yan Yang, and Shi-Jinn Horng. Deep air quality forecasting using hybrid deep
learning framework. arXiv preprint arXiv:1812.04783, 2018.
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter
Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in neural information processing
systems, pp. 1087-1098, 2017.
10
Under review as a conference paper at ICLR 2021
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126-
1135. JMLR. org, 2017.
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Mul-
timodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint
arXiv:1606.01847, 2016.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based
meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances in Neural
Information Processing Systems, pp. 15353-15363, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances
in neural information processing systems, pp. 1024-1034, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings Ofthe IEEE
international conference on computer vision, pp. 2961-2969, 2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
International Conference on Learning Representations, 2017.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recog-
nition. In ICML deep learning workshop, volume 2. Lille, 2015.
Yijun Lin, Nikhit Mago, Yu Gao, Yaguang Li, Yao-Yi Chiang, Cyrus Shahabi, and JoSe Luis Ambite. Exploiting
spatiotemporal patterns for accurate air quality forecasting using deep learning. In Proceedings of the 26th
ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 359-368,
2018.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. arXiv preprint
arXiv:1710.09668, 2017.
Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numeric-symbolic hybrid
deep network. Journal of Computational Physics, 399:108925, 2019.
Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model prior for
deep learning. arXiv preprint arXiv:1907.04490, 2019.
A Manepalli, A Albert, A Rhoades, D Feldman, and M Prabhat. Emulating numeric hydroclimate models with
physics-informed conditional generative adversarial networks. Environmetrics, 2019.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner.
In International Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=B1DmUzWAW.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 2554-2563. JMLR. org, 2017.
Devang K Naik and Richard J Mammone. Meta-neural networks that learn by learning. In [Proceedings 1992]
IJCNN International Joint Conference on Neural Networks, volume 1, pp. 437-442. IEEE, 1992.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint
arXiv:1803.02999, 2018.
Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Kahou, Mr. Prabhat, and Chris Pal. Ex-
tremeweather: A large-scale climate dataset for semi-supervised detection, localization, and under-
standing of extreme weather events. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems
30, pp. 3405-3416. Curran Associates, Inc., 2017a. URL http://papers.nips.cc/paper/
6932-extremeweather-a-large-scale-climate-dataset-for-semi-supervised-detection-localizatio
pdf.
Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Mr Prabhat, and Chris Pal. Ex-
tremeweather: A large-scale climate dataset for semi-supervised detection, localization, and understanding
of extreme weather events. In Advances in Neural Information Processing Systems, pp. 3402-3413, 2017b.
11
Under review as a conference paper at ICLR 2021
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards
understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learn-
ing framework for solving forward and inverse problems involving nonlinear partial differential equations.
Journal of Computational Physics, 378:686-707, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference
on Learning Representations, 2017.
Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia
Hadsell. Meta-learning with latent embedding optimization. arXiv preprint arXiv:1807.05960, 2018.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 4510-4520, 2018.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning
with memory-augmented neural networks. In Proceedings of The 33rd International Conference on Machine
Learning, pp. 1842-1850, 2016.
Jurgen SChmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: The
meta-meta-…hook. Diplomarbeit, TeChnisChe Universitat MunChen, MunChen, 1987.
Sungyong Seo and Yan Liu. Differentiable physiCs-informed graph networks. arXiv preprint arXiv:1902.02950,
2019.
Sungyong Seo, Chuizheng Meng, and Yan Liu. PhysiCs-aware differenCe graph networks for sparsely-
observed dynamiCs. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=r1gelyrtwH.
Karen Simonyan and Andrew Zisserman. Very deep Convolutional networks for large-sCale image reCognition.
arXiv preprint arXiv:1409.1556, 2014.
Jake Snell, Kevin Swersky, and RiChard Zemel. PrototypiCal networks for few-shot learning. In Advances in
neural information processing systems, pp. 4077-4087, 2017.
Ping-Wei Soh, Jia-Wei Chang, and Jen-Wei Huang. Adaptive deep learning-based air quality prediCtion model
using the most relevant spatial-temporal relations. Ieee Access, 6:38186-38199, 2018.
Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Charu Aggarwal, Prasenjit Mitra, and Suhang Wang. Joint modeling
of loCal and global temporal dynamiCs for multivariate time series foreCasting with missing values. arXiv
preprint arXiv:1911.10273, 2019.
Sebastian Thrun and Lorien Pratt. Learning to learn: IntroduCtion and overview. In Learning to learn, pp.
3-17. Springer, 1998.
Petar VeliCkoviC, Guillem CuCurulL Arantxa Casanova, Adriana Romero, Pietro LiO, and Yoshua Ben-
gio. Graph attention networks. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=rJXMpikCZ.
Oriol Vinyals, Charles Blundell, Timothy LilliCrap, Daan Wierstra, et al. MatChing networks for one shot
learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Jiawei Zhuang, Dmitrii KoChkov, Yohai Bar-Sinai, MiChael P Brenner, and Stephan Hoyer. Learned disCretiza-
tions for passive sCalar adveCtion in a 2-d turbulent flow. arXiv preprint arXiv:2004.05477, 2020.
12
Under review as a conference paper at ICLR 2021
A	Task 1: Multi-step Graph Signal Generation
A. 1 Meta-train
Data: For all experiments, we generate the data for meta-train tasks from a sum of sinusoidal functions with
different spatial frequencies (Eq. 6).
u(x,y) = £ λk,ι cos(kx + Iy) + γk,ι sin(kx + ly), λk,ι,γk,ι 〜N (0, 0.02),
∣k∣,∣ι∣≤F
(6)
where (x, y) in the 2D space ([0, 2π] × [0, 2π]) and k, l are randomly sampled integers. Once the spatially
continuous function values are generated, we uniformly sample different number of locations from all grid
points as observed nodes to simulate the case where the observations are irregularly distributed in space. We
then construct a k-Nearest Neighbor graph based on the Euclidean distance as the input of graph neural net-
works. The combination of parameters to generate the synthetic dataset is given in Table 5. We construct 100
snapshots per a combination of the parameters (N, E, F) using a unique random seed. 75 snapshots per each
combination are used for Dtr and 25 snapshots are for Dte .
Table 5: Parameters for synthetic dataset
Meta-train Meta-test
#	nodes (N)	{256, 625}
#	edges per a node (E)	{4, 8}
Initial frequency (F)	{2, 5}
{450, 800}
{3, 6, 10}
{3, 7}
Tasks: For each node, we have the first and second order derivatives. We meta-train the spatial derivative
modules (Sec. 3.1) to predict the spatial derivatives by feeding node and edge features (function value at a node
and relative displacement, respectively) as input.
A.2 Meta-test
A.2.1 Synthetic
Data: We generate the synthetic meta-test data from Eq. 6 but set different parameters to simulate the realistic
scenario where meta-train tasks and meta-test tasks do not share the same distribution.
Tasks: We reuse the spatial modules in A.1 to evaluate how the meta-initialized parameters are easily adaptable
to unseen graph signals with different spatial resolution, discretization, and the degree of function fluctuation.
We use 15 snapshots for the adaptation in meta-test and 75 snapshots are used to evaluate the proposed model.
A.2.2	Real-world Dataset
Figure 4: Sensor locations in the AQI-CO dataset. We show sensors as blue nodes and edges of
k-NN graphs as red lines. Borders of provinces are shown in grey.
Data:
13
Under review as a conference paper at ICLR 2021
AQI-CO (Berman, 2017): There are multiple pollutants in the dataset and we choose carbon monoxide (CO)
ppm as a target pollutant in this paper. We select sensors located in between latitude (26, 33) and longitude
(115,125) (East region of China). In this region, we sample multiple multivariate time series whose length
should be larger than 12 steps (12 hours) for multiple meta-tasks. There are around 60 working sensors and the
exact number of the working sensors is varying over different tasks. Fig. 4 shows the locations of selected AQI
sensors.
ExtremeWeather: We select the data in the year 1984 from the extreme weather dataset in (Racah et al.,
2017a). The data is an array of shape (1460, 16, 768, 1152), containing 1460 frames (4 per day, 365 days in
the year). 16 channels in each frame correspond to 16 spatiotemporal variables. Each channel has a size of
768×1152 corresponding to one measurement per 25 square km on earth. For each frame, the dataset provides
fewer than or equal to 15 bounding boxes, each of which labels the region affected by an extreme weather event
and one of the four types of the extreme weather: (1) tropical depression, (2) tropical cyclone, (3) extratropical
cyclone, (4) atmospheric river. In the single feature setting, we only utilize the channel of surface temperature
(TS).
Tasks:
AQI-CO: We select the first sequence of carbon monoxide (CO) ppm records from each month in the year
2015 at land-based stations, and set up the meta-test task on each sequence as the prediction of CO ppm. We
construct a 6-NN graph based on the geodesic distances among stations.
ExtremeWeather: First, we aggregate all bounding boxes into multiple sequences. In each sequence, all
bounding boxes (1) are in consecutive time steps, (2) are affected by the same type of extreme weather, and
(3) have an intersection over union (IoU) ratio above 0.25 with the first bounding box in the sequence. Then
we select the top-10 longest sequences. For each sequence, we consider its first bounding box A as the region
affected by an extreme weather event, and extend it to a new sequence of 20 frames by cropping and appending
the same region A from successive frames. For each region we uniformly sample 10% of available pixels as
observed nodes to simulate irregularly spaced weather stations and build a 4-NN graph based on the Euclidean
distance. Fig. 3 visualizes the first 5 frames of one extended sequence. In the single feature experiment, we set
up a meta-test task on each extended sequence as the prediction of the surface temperature (TS) on all observed
nodes with the initial TS given only.
A.3 Experimental Details
A.3.1 Baselines
PA-DGN (train from scratch) (Seo et al., 2020): For each meta-test task, initialize one PA-DGN model
randomly and train it on the single task. The spatial derivative layer uses a message passing neural network
(MPNN) with 2 GN blocks using 2-layer MLPs as update functions. The forward network part uses a recurrent
graph neural network with 2 recurrent GN blocks using 2-layer GRU cells as update functions. We set its
hidden dimension to 64, in which case PA-DGN has a similar number of parameters with RGN. The PA-DGN
model has 384,653 learnable parameters.
A.3.2	Ours
PiMetaL: Meta-train the spatial derivative modules (SDM) with our proposed Alg. 3 on the meta-train tasks
generated in A.1. Then for each meta-test task, initialize one time derivative module (TDM) randomly and
output of SDM is fed into the TDM to train it on the single task. The architecture for SDM and TDM are
identical for the spatial derivative layer and the recurrent graph network in PA-DGN, respectively.
A.3.3 Training Settings
Training hyperparameters: For all meta-train and meta-test tasks, we use the Adam optimizer with the learn-
ing rate 1e-3. In each training epoch, we sample 1 task from all available tasks.
Environments: All experiments are implemented with Python3.6 and PyTorch 1.3.0, and are conducted with
NVIDIA GTX 1080 Ti GPUs.
Runtime: The baselines RGN (train from scratch) and PA-DGN (train from scratch) will finish in 30 minutes.
All other baselines will finish the meta-train stage in 4 hours and the meta-test stage in 2 hours. The runtime is
measured in environments described above.
14
Under review as a conference paper at ICLR 2021
Figure 5: (left) GHCN weather stations in the USA and (right) GHCN weather stations in Europe.
as
IM
ω
02
1 m
%
-M
-U
■ IiHaltUS*>∣ro*>⅛∙∣
⅛ι∙he* ⅛∙(us*; ITOI>，g
^J ≡ ðɑ 5≡ iSa ≡5 SS 5i⅛
StadonKi
o uβ a» aα> «0	5»	«» τ∞
S⅞Honn
Figure 6: (first and third) Spatial distribution of the GHCN weather stations and synthetic nodes.
(second and fourth) Function values of the GHCN records and synthetic function.
B	Task 2: Graph Signal Regression
B.1	Meta-train
Data: For the graph signal regression task, we generate synthetic dynamics for meta-train tasks and the syn-
thetic data is adapted to a target dataset. Before setting the topological configuration for the synthetic dynamics,
we first examine the target dataset to understand its topological properties. Based on the number of stations and
the scale of records, we tune the topological configuration for the synthetic dataset. We use (N, F) = (1700, 2)
for the USA records and (N, F) = (700, 1.5) for Europe records, respectively, and 100 different initial values
are generated to define different tasks. Fig. 5 visualizes how the regional stations are distributed and Fig. 6
demonstrates how the spatial distribution of synthetic nodes and scales of synthetic values are adapted to the
corresponding target dynamics.
B.2	Meta-test
Data: The GHCN-Daily summaries from land surface stations across the globe provide daily climate records
from numerous sources. As the records from 100,000 stations in 180 countries and territories, the distribution
of the weather stations is spatially non-uniform. We sample sensors from two different regions (1) the USA
and (2) Europe and construct a graph structure from the regional stations based on k-NN algorithm (k = 4)
as described in Defferrard et al. (2019). There are 1,705 and 703 fully functioning sensors in the USA and
Europe, respectively. We use 2010 year records and first few daily records for few-shot training (5 and 10) and
next 100/150 days for validation and test. Note that the number of learnable parameters is significantly reduced
compared to those of the previous task to minimize overfitting as well as be comparable to other variants of
graph neural networks.
Table 6: The number of learnable parameters for baselines and PiMetaL
	GCN	GAT	GraPhSAGE	GN	PA-DGN	PiMetaL
# of parameters	10,801	11,203	21,401	20,385	33,795	33,795
B.3	Experimental Details
B.3.1	Baselines
Since the input length of the regression task is fixed (length=5), we can consider many variants of graph
neural networks for the task. We concatenate the 5-step signals and feed it into Graph convolutional net-
15
Under review as a conference paper at ICLR 2021
Table 7: Regression error (MAE, 10-3) of different topology for synthetic dynamics (Europe)
	(N,F) = (700, 1.5)	(N,F) = (1700, 2)	(N,F) = (128, 7)
5-shot	0.781±0.019	0.981±0.131	1.007±0.096
10-shot	0.773±0.014	0.951±0.151	0.932±0.058
works (GCN)(KiPf & Welling, 2017), Graph attention networks (GAT)(Velickovic et al., 2018), Graph-
SAGE (Hamilton et al., 2017), and Graph networks (GN) (Battaglia et al., 2018) to predict next signals across
all nodes. For the baselines, we commonly consider 3-hop neighbors of i-th node to predict of the i-th node
and the number of learnable parameters is similar to provide similar expressive power.
C Sensitivity Analysis of Synthetic Dynamics
It is important to study how much the model’s performance is dependent on synthetic topology. In this section,
we conduct an ablation study to see whether different choices of the synthetic topology affects the performance
significantly or not. According to Table 4, the regression error is fairly converged from a few samples for the
Europe records. Thus, we apply different synthetic topology for the data to see if the saturated regression error
is significantly changed. For this ablation study, we reuse the synthetic dynamics adapted for the USA records
and generate one more synthetic dynamics for spatially low-resolution cases.
Table 7 shows that the regression performance across different topology is stable regardless of the number
of shots, however, it is significantly degraded when we change the synthetic topology from the adapted one
((N, F) = (700, 1.5)) for meta-training. When we increase the spatial resolution (N = 700 → 1700), the
meta-initialized spatial modules are adapted to learn spatial derivatives defined on spatially higher resolution.
In such case, SDM likely assigns high weights to directly adjacent nodes as well as farther nodes (e.g., 3-hop
nodes) as all neighbor nodes are strongly associated with exact spatial derivatives. On the other hand, if SDM
is meta-initialized from a lower resolution (N = 700 → 128), further nodes are too much underestimated.
Thus, it is important to construct proper topology for transferring the PDE-independent representations from
synthetic dynamics to target dynamics.
16