Under review as a conference paper at ICLR 2021
Joint Perception and Control as Inference
with an Object-based Implementation
Anonymous authors
Paper under double-blind review
Abstract
Existing model-based reinforcement learning methods often study perception mod-
eling and decision making separately. We introduce joint Perception and Control
as Inference (PCI), a general framework to combine perception and control for
partially observable environments through Bayesian inference. Based on the fact
that object-level inductive biases are critical in human perceptual learning and
reasoning, we propose Object-based Perception Control (OPC), an instantiation of
PCI which manages to facilitate control using automatic discovered object-based
representations. We develop an unsupervised end-to-end solution and analyze the
convergence of the perception model update. Experiments in a high-dimensional
pixel environment demonstrate the learning effectiveness of our object-based per-
ception control approach. Specifically, we show that OPC achieves good perceptual
grouping quality and outperforms several strong baselines in accumulated rewards.
1	Introduction
Human-like computing, which aims at endowing machines with human-like perceptual, reasoning
and learning abilities, has recently drawn considerable attention (Lake, 2014; Lake et al., 2015;
Baker et al., 2017). In order to operate within a dynamic environment while preserving homeosta-
sis (Kauffman, 1993), humans maintain an internal model to learn new concepts efficiently from a
few examples (Friston, 2005). The idea has since inspired many model-based reinforcement learning
(MBRL) approaches to learn a concise perception model of the world (Kaelbling et al., 1998). MBRL
agents then use the perceptual model to choose effective actions. However, most existing MBRL
methods separate perception modeling and decision making, leaving the potential connection between
the objectives of these processes unexplored. A notable work by Hafner et al. (2020) provides a
unified framework for perception and control. Built upon a general principle this framework covers a
wide range of objectives in the fields of representation learning and reinforcement learning. However,
they omit the discussion on combining perception and control for partially observable Markov deci-
sion processes (POMDPs), which formalizes many real-world decision-making problems. In this
paper, therefore, we focus on the joint perception and control as inference for POMDPs and provide
a specialized joint objective as well as a practical implementation.
Many prior MBRL methods fail to facilitate common-sense physical reasoning (Battaglia et al., 2013),
which is typically achieved by utilizing object-level inductive biases, e.g., the prior over observed
objects’ properties, such as the type, amount, and locations. In contrast, humans can obtain these
inductive biases through interacting with the environment and receiving feedback throughout their
lifetimes (Spelke et al., 1992), leading to a unified hierarchical and behavioral-correlated perception
model to perceive events and objects from the environment (Lee and Mumford, 2003). Before taking
actions, a human agent can use this model to decompose a complex visual scene into distinct parts,
understand relations between them, reason about their dynamics and predict the consequences of its
actions (Battaglia et al., 2013). Therefore, equipping MBRL with object-level inductive biases is
essential to create agents capable of emulating human perceptual learning and reasoning and thus
complex decision making (Lake et al., 2015). We propose to train an agent in a similar way to gain
inductive biases by learning the structured properties of the environment. This can enable the agent
to plan like a human using its ability to think ahead, see what would happen for a range of possible
choices, and make rapid decisions while learning a policy with the help of the inductive bias (Lake
et al., 2017). Moreover, in order to mimic a human’s spontaneous acquisition of inductive biases
1
Under review as a conference paper at ICLR 2021
AAA
q (S) ~ P (s | X)
Figure 1: The graphical model of joint Perception and Control as Inference (PCI), where S and o
represent the latent state and the binary OPtimality binary variable, respectively. The hierarchical
perception model includes a bottom-up recognition model q(s) and a top-down generative model
p(x,o, S) (decomposed into the likelihood p(x,o∣s) and the prior belief P(S)). Control is performed
by taking an action a to affect the environment state.
throughout its life, we propose to build a model able to acquire new knowledge online, rather than a
one which merely generates static information from offline training (Dehaene et al., 2017).
In this paper, we introduce joint Perception and Control as Inference (PCI) as shown in Fig. (1), a
unified framework for decision making and perception modeling to facilitate understanding of the
environment while providing ajoint objective for both the perception and the action choice. As we
argue that inductive bias gained in object-based perception is beneficial for control tasks, we then
propose Object-based Perception Control (OPC), an instantiation of PCI which facilitates control with
the help of automatically discovered representations of objects from raw pixels. We consider a setting
inspired by real-world scenarios; we consider a partially observable environment in which agents'
observations consist of a visual scene with compositional structure. The perception optimization of
OPC is typically achieved by inference in a spatial mixture model through generalized expectation
maximization (Dempster et al., 1977), while the policy optimization is derived from conventional
temporal-difference (TD) learning (Sutton, 1988). Proof of convergence for the perception model
update is provided in Appendix A. We test OPC on the Pixel Waterworld environment. Our results
show that OPC achieves good quality and consistent perceptual grouping and outperforms several
strong baselines in terms of accumulated rewards.
2	Related Work
Connecting Perception and Control Formulating RL as Bayesian inference over inputs and
actions has been explored by recent works (Todorov, 2008; Kappen et al., 2009; Rawlik et al., 2010;
Ortega and Braun, 2011; Levine, 2018; Tschiatschek et al., 2018; Lee et al., 2019b;a; Ortega et
al., 2019; Xin et al., 2020; O’Donoghue et al., 2020). The generalized free energy principle (Parr
and Friston, 2019) studies a unified objective by heuristically defining entropy terms. A unified
framework for perception and control from a general principle is proposed by Hafner et al. (2020).
Their framework provides a common foundation from which a wide range of objectives can be derived
such as representation learning, information gain, empowerment, and skill discovery. However, one
trade-off for the generality of their framework is the loss in precision. Environments in many
real-world decision-making problems are only partially observable, which signifies the importance
of MBRL methods to solving POMDPs. However, relevant and integrated discussion is omitted
in Hafner et al. (2020). In contrast, we focus on the joint perception and control as inference for
POMDPs and provide a specialized joint-objective as well as a practical implementation.
Model-based Deep Reinforcement Learning MBRL algorithms have been shown to be effective
in various tasks (Gu et al., 2016), including operating in environments with high-dimensional raw
pixel observations (Igl et al., 2018; Shani et al., 2005; Watter et al., 2015; Levine et al., 2016; Finn
and Levine, 2017). Existing methods have considered incorporating reward structure into model-
learning (Farahmand et al., 2017; Oh et al., 2017), while our proposed PCI takes one step forward by
incorporating the perception model into the control-as-inference derivation to yield a single unified
objective for multiple components in a pipeline. One of the methods closely related to OPC is the
World Model (Ha and Schmidhuber, 2018), which consists of offline and separately trained models
for vision, memory, and control. These methods typically produce entangled latent representations
for pixel observations whereas, for real world tasks such as reasoning and physical interaction, it
is often necessary to identify and manipulate multiple entities and their relationships for optimal
performance. Although Zambaldi et al. (2018) has used the relational mechanism to discover and
reason about entities, their model needs additional supervision of location information.
2
Under review as a conference paper at ICLR 2021
Object-based Reinforcement Learning The object-based approach, which recognizes decom-
posed objects from the environment observations, has attracted considerable attention in RL as
well (Schmidhuber, 1992). However, most models often use pre-trained object-based representations
rather than learning them from high-dimensional observations (Diuk et al., 2008; Kansky et al.,
2017). When objects are extracted through learning methods, these models usually require supervised
modeling of the object property, by either comparing the activation spectrum generated from neural
or leveraging the bounding boxes generated
, 2018). MOREL (Goel et al., 2018) applies
network filters with existing types (Garnelo et al., 2016)
by standard object detection algorithms (Keramati et al.
optical flow in video sequences to learn the position and velocity information as input for model-free
RL frameworks.
A distinguishing feature of our work in relation to previous works in MBRL and the object-based RL
is that we provide the decision-making process with object-based abstractions of high-dimensional
observations in an unsupervised manner, which contribute to faster learning.
Unsupervised Object Segmentation Unsupervised object segmentation and representation learn-
ing have seen several recent breakthroughs, such as IODINE (Greff et al., 2019), MONet (Burgess et
al., 2019), and GENESIS (Engelcke et al., 2020). Several recent works have investigated the unsu-
pervised object extraction for reinforcement learning as well (Zhu et al., 2018; Asai and Fukunaga,
2017; Kulkarni et al., 2019; Watters et al., 2019; Veerapaneni et al., 2020). Although OPC is built
upon a previous unsupervised object segmentation back-end (Greff et al., 2017; van Steenkiste et
al., 2018), we explore one step forward by proposing a joint framework for perceptual grouping
and decision-making. This could help an agent to discover structured objects from raw pixels so
that it could better tackle its decision problems. Our framework also adheres to the Bayesian brain
hypothesis by maintaining and updating a compact perception model towards the cause of particular
observations (Friston, 2010).
3	Methods
We start by introducing the environment as a partially observable Markov Decision Process (POMDP)
with an object-based observation distribution in Sect. 3.11. We then introduce PCI, a general
framework for joint perception and control as inference in Sect. 3.2 and arrive at a joint objective for
perception and control models. In the remainder of this section we propose OPC, a practical method
to optimize the joint objective in the context of an object-based environment, which requires the
model to exploit the compositional structure of a visual scene.
3.1	Environment Setting
We define the environment as a POMDP represented by the tuple Γ=hS, P, A, X , U, Ri, where
S , A, X are the state space, the action space, and the observation space, respectively. At time step t,
we consider an agent’s observation xt ∈X≡RD as a visual image (a matrix of pixels) composited of
K objects, where each pixel xi is determined by exactly one object. The agent receives xt following
the conditional observation distribution U(xt|st): S → X, where the hidden state st is defined
by the tuple (zt, θ1t ,...,θt ). Concretely, we denote as zt ∈Z≡[0, 1]D×K the latent variable
which encodes the unknown true pixel assignments, such that zt =1iff pixel zit was generated by
component k. Each pixel xt is then rendered by its corresponding object representations θt ∈ RM
through a pixel-wise distribution Uψt (xt|zt = 1) 2, where ψt = fφ(θt )i is generated by feeding
θt into a differentiable non-linear function fφ . When the environment receives an action at ∈A, it
moves to a new state st+1 following the transition function P (st+1 ∣st,at) : S X A → S .We assume
the transition function could be parameterized and we integrate its parameter into φ. To embed the
control problem into the graphical model, we also introduce an additional binary random variable ot
to represent the optimality at time step t, i.e., ot = 1 denotes that time step t is optimal, and ot =0
denotes that it is not optimal. We choose the distribution over ot to be p(ot = 1∣st, at) α exp(rt),
1Note that the PCI framework is designed for general POMDPs. We extend Sect. 3.1 to object-based
POMDPs for the purpose of introducing the environment setting for OPC.
2We consider U as Gaussian, i.e., Uψt 七(xt∣zt,k = 1)〜N(χi; μ = ψt,k, σ2) for some fixed σ2.
3
Under review as a conference paper at ICLR 2021
where rt ∈ R is the observed reward provided by the environment according to the reward function
R(rt∣st,at) : S X A → R. We denote the distribution over initial state as p(s1) : S → [0,1].
3.2	Joint Perception and Control as Inference
To formalize the belief about the unobserved hidden cause of the history observation x≤t, the agent
maintains a perception model qw(s) to approximate the distribution over latent states as illustrated
in Fig. (1). An agent’s inferred belief about the latent state could serve as a sufficient statistic of
the history and be used as input to its policy π, which guides the agent to act in the environment.
The goal of the agent is to maximize the future optimality o≥t while learning a perception model by
inferring unobserved temporal hidden states given the past observations x≤t and actions a≤t, which
is achieved by maximizing the following objective:
logp(o≥t, x≤t∣a<t) = log X	p(o≥t, s≥1, x≥1,a≥t∣a<t)
s≥1 ,a≥t ,x>t
,L W Qj=ιP(XjIsj)P(S1)Qm=IP(sm+1|sm,am)E
log k q ----------------qw----------------[①]
s≤t
where we denote qw = q(s≤t∣x≤t, a<t) and use ① to represent the term related to control as
①= Σ	∏p(on∣sn,an)∏ p(xk+1∣sk+1)p(sk+1∣sk ,ak )p(ak ∣a<t).
s>t,a≥t,x>t n=t
k=t
The full derivation is presented in Appendix C.1. We denote qw = q(s≤t∣x≤t, a<t) and assume
qw = p(s1) Qt=2 q(sg|x<g,a<g), where we slightly abuse notation for qw by ignoring the fact that
we sample from the model p(s1) for t =1. We then apply Jensen’s inequality to Eq.(1) and get
log P(o≥t, x≤t∣a<t) ≥ Es≤t〜qw
log Qj=I P(XjIsj)p(tsI)Qm=I P(Smrsm,am)]
_	p(s1) Qg=2 q(sg∣x<g,a<g)	_
+ Es≤t 〜qw [log O)]
t	t-1
Es≤jw log Y P(XjIsj) - DKL Y q(sg+1∣x≤g ,a≤g )kp(sg+1Isg ,ag) +E,≤t 〜qw [log 0],
j=1	g=1
|-------------------------------------{z------------------------------------}
Lw(qw,φ)
where DKL represents the Kullback-Leibler divergence (Kullback and Leibler, 1951). As intro-
duced in Sect. 3.1, we parameterize the transition distributionpφ(st+1∣st, at) and the observation
distribution pφ(xt |st) by φ. An instantiation to optimize the above evidence lower bound (ELBO)
Lw (qw ,φ) in an environment with explicit physical properties and high-dimensional pixel-level
observations will be discussed in Sect. 3.3.
We now derive the control objective by extending φ as
log	q(s>t, a≥t, X>t)
s>t,a≥t,x>t
Qn=t P(onIsn,an) Qk=t Pφ(Xk+1Isk+1)Pφ(sk+1Isk,ak)P(akIa<t)
q(s>t, a≥t, X>t)
where we assume q(s>t, a≥t, x>t) = Q%= pφ(xh+1 ∖sh+1)pφ(sh+1 ∖sh, ah)π(ah ∣sh) and denote
qc = q(s>t, a≥t, x>t). We then apply Jensen,s inequality to Es≤t〜qw [log ①]and get
Es≤t 〜qw [log ①]≥Es≤t 〜qw
qc log
s>t,a≥t,x>t
Qn=tp(on∣sn,an) Qk=tpφ(xk+1∣sk+1)pφ(sk+1∣sk, ak)p(ak∣a<t)
Qh=tPφ(xh+1∣sh+1)pφ(sh+1∣sh, ah)π(ah∣sh)
=Es≤t 〜qw EEst+1,at,χt+1 〜ρφ∏ [R(st,a9 - DKL(π(at∣St)∣∣p(at∣a<tD],
t
(2)
where we denote ρφ as pφ(xh+1∣sh+1)pφ(sh+1 ∣sh, ah). Note that as this control objective is an
expectation under ρφ, reward maximization also bias the learning of our perception model. We will
propose an option to optimize the control objective Eq.(2) in Sect. 3.4.
4
Under review as a conference paper at ICLR 2021
3.3	Object-based Perception Model Update
We now introduce OPC, an instantiation of PCI in the context of an object-based environment.
Following the generalized expectation maximization (Dempster et al., 1977), we optimize the ELBO
Lw (qw, φ) by improving the perception model qw about the true posterior Pφ(st+1∣xt+1, st, at) with
respect to a set of object representations θt = [θɪ,..., θtκ] ∈ Ω ⊂ RM×K.
E-step to compute a new estimate qw of the posterior. Assume the ELBO is already maximized
with respect to qw at time step t, i.e., qw = pφ(st+1 |xt+1, st,at)=pφ(zt+1 |xt+1, ψt, at), we can
generate a soft-assignment of each pixel to one of the K objects as
ηi,k =. pφ(zi,+k = 1|xi+ , ψi ,a ).	(3)
M-step to update the model parameter φ. We then find θt+1 to maximize the ELBO as
θt+1 = arg max E	Ilogpφ(xt+1, zt+1, ψt+1∣st,at)] = arg max Aet (θt+1).	(4)
θt+1 zt+ι^ηt	θt+1
See derivation in Appendix C.2. Note that Eq. (4) returns a set of points that maximize Aθt (θt+1),
and we choose θt+1 to be any value within this set. To update the perception model by maximizing
the evidence lower bound with respect to q(st) and θt+1, we compute Eq. (3) and Eq. (4) iteratively.
However, an analytical solution to Eq. (4) is not available because we use a differentiable non-linear
function fφ to map from object representations θt into ψt = fφ(θt )i. Therefore, we get θt+1 by
θkt+1 = θkt + α
∂Aet+ι (θt+2)
∂θk+1 -
D
= θkt + α X ηit,k
θkt+1=θkt	i=1
- xit+1
σ2
∂ψtk
时,
(5)
where α is the learning rate (see details of derivation in Appendix C.3).
We regard the iterative process as a tr°-step rollout of K copies of a recurrent neural net-
work with hidden states θt receiving ηt Θ (ψt - xt+1) as input (see the inner loop of Algo-
rithm 1). Each copy generates a new ψt+1, which is then used to re-estimate the soft-assignments
ηt+1. We parameterize the Jacobian ∂ψt/∂θt and the differentiable non-linear function fφk
using a convolutional encoder-decoder architecture with a recurrent neural network bottleneck,
which linearly combines the out-
put of the encoder with θt from
the previous time step. To fit the
statistical model fφ to capture
the regularities corresponding to
the observation and the transition
distribution for given POMDPs,
we back-propagate the gradient
of ET〜∏ [Aet (θt+1)] through
“time” (also known as the
BPTT Werbos (1988)) into the
weights φ. We demonstrate
the convergence of a sequence
of {Lwt (qw, φ)} generated by
the perception update in Ap-
pendix A. The proof is presented
by showing that the learning
process follows the Global Con-
vergence Theorem (Zangwill,
1969).
Our implementation of the per-
ceptual model is based on the un-
supervised perceptual grouping
method proposed by (Greff et al.,
Perception Model
Food Agent Poison
η
gradient flow
Figure 2: Illustration of OPC and the Pixel Waterworld envi-
ronment. We regard the perception update as K copies of an
RNN with hidden states θt receiving ηt Θ (ψt - xt+1) as input.
Each copy generates a new ψt+1, which is used to re-estimate
the soft-assignments ηt+1 for the calculation of the expected
log-likelihood Aet+1 (θt+2). The hidden states are then used as
the input to decision-making (before being fed into an optional
relational module) to guide the agent’s action.
Decision Making Module
；r
''-¼ x
K {
2017; van Steenkiste et al., 2018). However, using other advanced unsupervised object representations
learning methods as the perceptual model (such as IODINE (Greff et al., 2019) and MONet (Burgess
et al., 2019)) is a straightforward extension.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Learning the Object-based Perception Control
Initialize θ, η, φ, ζ, ζv,Tepi,tro, K
Create Ne environments that will execute in parallel
while training not finished do
Initialize the history Da, Dx, Dη with environment rollouts for tro + 1 time-steps under current policy πζ
for T =1to Tepi - tro do
dφ — 0
Get ηT-1 from Dη
for t = T to T + tro do
Get at , xt from Da , Dx respectively
Feed 瑞-1 Θ (Ψk-1 — Xt) into each of the K RNN CoPy to get θk and forward-output ψk
Compute ηkt by Eq. 3
dφ  dφ + ∂ (—Λθt (θt+1)) /∂φ by Eq. (4)
Perform aτ+tro according to policy ∏ζ(aT+tro ∣θτ+tro)
Receive reward rT +tro and new observation xT +tro+1
Store aτ +tro, xτ+tro+1, ητ in Da, Dx, Dη respectively
Feed ηT+tro Θ (ψT+tro — xτ+tro+1) into each of the K RNN copy to get θT+tro+1
y - rτ+tro + y& (θτ+tro+1)
dZ — RZ log ∏ζ(aτ+tro ∣θτ+tro)(y — R (θτ+tro))
dZv J ∂ (y - Mv(θτ+tro))2∕∂Zv
dφ J dφ + ∂ (y — VZv (θτ+tro))2∕∂φ
Perform synchronous update of φ using dφ, of ζ using dζ, and of ζv using dζv
3.4	Decision-making Module Update
Recall that the control objective in Eq.(2) is the sum of the expected total reward along trajectory ρθπ
and the negative KL divergence between the policy and our prior about the future actions. The prior
can reflect a preference over policies. For example, a uniform prior in SAC (Haarnoja et al., 2018)
corresponds to the preference for a simple policy with minimal assumptions. This could prevent
early convergence to sub-optimal policies. However, as we focus on studying the benefit of combing
perception with control, we do not pre-impose a preference for policies. Therefore we set the prior
equal to our training policy, resulting in a zero KL divergence term in the objective.
To maximize the total reward along trajectory ρθ π, we follow the conventional temporal-difference
(TD) learning approach (Sutton, 1988) by feeding the object abstractions into a small multilayer
perceptron (MLP) (Rumelhart et al., 1986) to produce a (dimR(A) + 1)-dimensional vector, which
is split into a dimR(A)-dimensional vector of ∏'s (the 'actor')logits, and a baseline scalar VZU
(the ‘critic’). The πζ logits are normalized using a softmax function, and used as the multinomial
distribution from which an action is sampled. The Vζ is an estimate of the state-value function at the
current state, which is given by the last hidden state θ of the tro-step RNN rollout. On training the
decision-making module, the Vζ is used to compute the temporal-difference error given by
LτD = (yt+1 — Vζv(θt+1))2,yt+1 = rt+1 + γVζv(θt+2),	(6)
where γ ∈ [0, 1) is a discount factor. LTD is used both to optimize πζ to generate actions with larger
total rewards than Vζ predicts by updating ζ with respect to the policy gradient
NZ log ∏ζ (at+1∣θt+1)(y — Kv(θt+1)),
and to optimize Vζ to more accurately estimate state values by updating ζv . Also, differentiating
LTD with respect to φ enables the gradient-based optimizers to update the perception model. We
provide the pseudo-code for one-step TD-learning of the proposed model in Algorithm 1. By grouping
objects concerning the reward, our model distinguishes objects with visual similarities but different
semantics, thus helping the agent to better understand the environment.
4	Experiments
4.1	Pixel Waterworld
We demonstrate the advantage of unifying decision making and perception modeling by applying
OPC on an environment similar to the one used in COBRA (Watters et al., 2019), a modified
6
Under review as a conference paper at ICLR 2021
Waterworld environment (Karpathy, 2015), where the observations are 84 × 84 grayscale raw pixel
images composited of an agent and two types of randomly moving targets: the poison and the food,
as illustrated in Fig. (2). The agent can control its velocity by choosing from four available actions: to
apply the thruster to the left, right, up and down. A negative cost is given to the agent if it touches any
poison target, while a positive reward for making contact with any food target. The optimal strategy
depends on the number, speed, and size of objects, thus requiring the agent to infer the underlying
dynamics of the environment within a given amount of observations.
The intuition of this environment is to test whether the agent can quickly learn dynamics of a new
environment online without any prior knowledge, i.e., the execution-time optimization throughout the
agent’s life-cycle to mimic humans’ spontaneous process of obtaining inductive biases. We choose
Advantage Actor-Critic (A2C) (Mnih et al., 2016) as the decision-making module of OPC without
loss of generality, although other online learning methods are also applicable. For OPC, we use
K =4and tro = 20 except for Sect. 4.5, where we analyze the effect of the hyper-parameter setting.
4.2	Accumulated Reward Comparisons
# of experienced observations (*1e5)
(a)
0
1
2
3
4
Figure 3: Performance comparisons between methods.
# Rf experienced RbservatiRns (*1e5)
(C)
5
# of experienced observations (*1e5)
(b)
0
1
2
3
4
5
# Of experienced ObServationS (*1e5)
(d)
To verify object-level in-
ductive biases facilitating
decision making, we com-
pare OPC against a set
of baseline algorithms, in-
cluding: 1) the standard
A2C, which uses convo-
lutional layers to trans-
form the raw pixel ob-
servations to low dimen-
sional vectors as input for
the same MLP described
in Sect. 3.4, 2) the World
Model (Ha and Schmid-
huber, 2018) with A2C
for control (WM-A2C),
a state-of-the-art model-
based approach, which
separately learns a vi-
sual and a memorization
model to provide the in-
put for a standard A2C, and 3) the random policy. For both the baseline A2C, WM-A2C and the
decision-making module of OPC, we follow the convention of Mnih et al. (2016) by running the
algorithm in the forward view and using the same mix of n-step returns to update both πζ and
Vζ . Details of the model and of hyperparameter setting can be found in Appendix B. We build the
environment with two poison objects and one food object, and set the size of the agent 1.5 times
smaller than the target. Results are reported with the average value among separate runs of three
random seeds. Note that the training procedure of WM-A2C includes independent off-line training
for each component, thus requiring many more observation samples than OPC. Following its original
early-stopping criteria, we report that WM-A2C requires 300 times more observation samples for
separately training a perception model before learning a policy to give the results presented in Fig. (3).
Fig. (3a) shows the result of accumulated rewards after each agent has experienced the same amount
of observations. Note that OPC uses the plotted number of experienced observations for training
the entire model, while WM-A2C uses the amount of observations only for training the control
module. It is clear that the agent with OPC achieves the greatest performance, outperforming all
other agents regardless of model in terms of accumulated reward. We believe this advantage is
due to the object-level inductive biases obtained by the perception model (compared to entangled
representations extracted by the CNN used in standard A2C), and the unified learning of perception
and control of OPC (as opposed to the separate learning of different modules in WM-A2C).
To illustrate each agent’s learning process through time, we also present the period reward, which
is the accumulated reward during a given period of environment interactions (2e4 of experienced
7
Under review as a conference paper at ICLR 2021
observations). As illustrated in Fig. (3b), OPC significantly improves the sample efficiency of A2C,
which enables the agent acting in the environment to find an optimal strategy more quickly than
agents with baseline models. We also find that the standard version of A2C with four parallel threads
gives roughly the same result as the single-threaded version of A2C (the same as the decision-making
module of OPC), eliminating the potential drawback of single-thread learning.
4.3	Perceptual Grouping Results
To demonstrate the performance of the perception model, we provide an example rollout at the
early stage of training in Fig. (4). We assign a different color to each ψk
and show through the soft-assignment η that
all objects are grouped semantically: the agent
in blue, the food in green, and both poisons
in red. During the joint training of perception
and policy, the soft-assignment gradually consti-
tutes a semantic segmentation as the TD signal
improves the recognition when grouping pixels
into different types based on the interaction with
the environment. Consequently, the learned ob-
ject representations θ becomes a semantic inter-
pretation of raw pixels grouped into perceptual
objects.
for better visualization
Figure 4: A sample rollout of perceptual grouping
by OPC. The observation (top row), the next-step
prediction ψk of each copy of the K RNN copy
(rows 4 to 7), the	ψk (row 2), and the soft-
assignment η of the pixels to each of the copies
(row 3). We assign a different color to each ψk
for better visualization. This sample rollout shows
that all objects are grouped semantically.
4.4	Benefits of Joint Inference
To gain insights into the benefit of the control
objective for joint inference, we further compare
OPC against OP, a perception model with the
same architecture as OPC but no guidance of
the TD signal from RL, i.e., using the same
unsupervised object segmentation back-end as
in Greff et al. (2017). Fig. (5) shows the result of accumulated rewards and the soft-assignment η
produced by both perception models. Results are produced by models running in environments with
the same random seed.
As illustrated in Fig. (5a), OPC outperforms OP in terms of the accumulated rewards through
time with a given amount of observations. This performance difference owes to the fact shown
in Fig. (5b), where the η of OP are overlaid, and the shapes are not delineated in bold colors
but are a mixture, showing that OP has not learned to segment the objects clearly. The pixel
assignment (coloring) does not appear to converge even after an extremely large number of iterations,
suggesting that OP alone cannot properly show signs of any semantic adaptation to the task at hand.
# of experienced observations (*1e5)
⑶
(b)
Figure 5: (a) Performance comparison between OPC and OP. (b) The soft-
assignment η after experiencing 14000 and 14500 observations. Each ψk
is assigned with a different color as in Fig. (4). X is the binary-processed
original observation.
On the other hand, the
quality and consistency of
the soft-assignment gen-
erated by OPC are im-
proved with the TD sig-
nal, suggesting that RL
facilitates the learning
of the perception model.
Furthermore, the inter-
action between objects
shown in Fig. (5b) demon-
strates the agent is mov-
ing away from the poison
(even when the food is
nearby). Because of the
environment setting of the high moving speed, larger target sizes and more poison objects, we believe
that agents are given strong indications to stay away from all stimuli. Thus, the result shows an
understanding of visual reasoning that the agent can separate itself from the rest of the objects.
8
Under review as a conference paper at ICLR 2021
4.5	Ablation Study on Hyper-parameters
We also investigate the effect of hyper-parameters to the learning of OPC, by changing: 1) the number
of recurrent copies K, 2) the rollout steps for recurrent iteration tro, and 3) the use of relational
mechanism across object representations θ (see Sect. 2.2 of van Steenkiste et al. (2018) for more
details). Fig. (3c) and Fig. (3d) show period reward results across different hyper-parameter settings.
As illustrated in Fig. (3c), the number of recurrent copies affects the stability of OPC learning, as
OPC with K =3has experienced larger variance during the agent’s life-cycle. We believe the
difference comes from the environment dynamics as we have visually four objects in the environment.
During the earlier stage of interacting with the environment, OPC tries to group each object into a
distinct class; thus, a different number of K against the number of objects in the environment confuse
the perception model and lead to unstable learning. Although different K settings might affect the
learning stability and slow down the convergence, OPC can still find an optimal strategy within a
given amount of observations.
In Fig. (3d), we compare OPC with different steps of recurrent rollout tro. A smaller tro means fewer
rounds of perception updates and therefore slower convergence in terms of the number of experienced
observations. We believe that the choice of tro depends on the difficulty of the environment, e.g., a
smaller tro can help to find the optimal strategy more quickly for simpler environments in terms of
wall training time.
Meanwhile, results in Fig. (3c) and Fig. (3d) show that the use of a relational mechanism has limited
impact on OPC, possibly because the objects can be well distinguished and perceived by their
object-level inductive biases, i.e. shapes in our experiment. We believe that investigating whether
the relational mechanism will have impact on environments where entities with similar object-level
inductive bias have other different internal properties is an interesting direction for future work.
5	Conclusions
In this paper, we propose joint Perception and Control as Inference (PCI), a general framework to
combine perception and control for POMDPs through Bayesian inference. We then extend PCI to the
context of a typical pixel-level environment with compositional structure and propose Object-based
Perception Control (OPC), an instantiation of PCI which manages to facilitate control with the
help of automatically discovered object-based representations. We provide the convergence proof
of OPC perception model update and demonstrate the execution-time optimization ability of OPC
in a high-dimensional pixel environment. Notably, our experiments show that OPC achieves high
quality and consistent perceptual grouping and outperforms several strong baselines in terms of
accumulated rewards within the agent’s life-cycle. OPC agent can quickly learn the dynamics of a
new environment without any prior knowledge, imitating the inductive bias acquisition process of
humans. For future work, we would like to investigate OPC with more types of inductive biases and
test the model performance in a wider variety of environments.
References
Masataro Asai and Alex Fukunaga. Classical planning in deep latent space: Bridging the subsymbolic-
symbolic boundary, 2017.
Chris L. Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B. Tenenbaum. Rational quantitative
attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour, 1:0064
EP -, 03 2017.
Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical
scene understanding. Proceedings of the National Academy of Sciences, page 201306572, 2013.
Christopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew M
Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation.
ArXiv, abs/1901.11390, 2019.
Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
InfoGAN: Interpretable representation learning by information maximizing generative adversarial
9
Under review as a conference paper at ICLR 2021
nets. In D. D. Lee, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances In Neural
Information Processing Systems 29, pages 2172-2180. Curran Associates, Inc., 2016.
Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is consciousness, and could machines have
it? Science, 358(6362):486-492, 2017.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages
1-38, 1977.
Carlos Diuk, Andre Cohen, and Michael L. Littman. An object-oriented representation for efficient
reinforcement learning. In ICML, pages 240-247, 2008.
Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: Generative
Scene Inference and Sampling of Object-Centric Latent Representations. International Conference
on Learning Representations (ICLR), 2020.
Amir-Massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-Aware Loss Function for
Model-based Reinforcement Learning. volume 54 of Proceedings of Machine Learning Research,
pages 1486-1494, Fort Lauderdale, FL, USA, 20-22 Apr 2017. PMLR.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In Robotics and
Automation (ICRA), 2017 IEEE International Conference on, pages 2786-2793. IEEE, 2017.
Karl Friston. A theory of cortical responses. Philosophical Transactions of the Royal Society of
London B: Biological Sciences, 360(1456):815-836, 2005.
Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11:127
EP -, 01 2010.
Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards deep symbolic reinforcement
learning. arXiv preprint arXiv:1609.05518, 2016.
Vikash Goel, Jameson Weng, and Pascal Poupart. Unsupervised video object segmentation for deep
reinforcement learning. In NIPS, pages 5688-5699. 2018.
Klaus Greff, Sjoerd van Steenkiste, and Jurgen Schmidhuber. Neural expectation maximization. In
NIPS, pages 6691-6701, 2017.
Klaus Greff, Raphael Lopez Kaufmann, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel
Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. In ICML, pages 2424-2433, 2019.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with
model-based acceleration. In ICML, pages 2829-2838, 2016.
David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. In NIPS,
pages 2451-2463. 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. volume 80 of Proceedings
OfMachine Learning Research, pages 1861-1870, Stockholmsmassan, Stockholm Sweden, 10-15
Jul 2018. PMLR.
Danijar Hafner, Pedro A Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action
and perception as divergence minimization. arXiv preprint arXiv:2009.01791, 2020.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for POMDPs. In ICML, pages 2117-2126, 10-15 Jul 2018.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.
10
Under review as a conference paper at ICLR 2021
Ken Kansky, Tom Silver, David A. M6ly, Mohamed Eldawy, Miguel Ldzaro-Gredilla, Xinghua Lou,
Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot
transfer with a generative causal model of intuitive physics. In Doina Precup and Yee Whye Teh,
editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of
Proceedings ofMachine Learning Research, pages 1809-1818, International Convention Centre,
Sydney, Australia, 06-11 Aug 2017. PMLR.
Hilbert J Kappen, Viceng G6mez, and Manfred Opper. Optimal control as a graphical model inference
problem. Machine learning, 87(2):159-182, 2009.
Andrej Karpathy. Reinforcejs: Waterworld. https://cs.stanford.edu/people/
karpathy/reinforcejs/waterworld.html, 2015. Accessed: 2018-11-20.
Stuart A Kauffman. The origins of order: Self-organization and selection in evolution. OUP USA,
1993.
Ramtin Keramati, Jay Whang, Patrick Cho, and Emma Brunskill. Strategic object oriented reinforce-
ment learning. arXiv preprint arXiv:1806.00175, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tejas Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew
Zisserman, and Volodymyr Mnih. Unsupervised learning of object keypoints for perception and
control. arXiv preprint arXiv:1906.11883, 2019.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathemati-
cal statistics, 22(1):79-86, 1951.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.
Brenden M Lake. Towards more human-like concept learning in machines: Compositionality,
causality, and learning-to-learn. PhD thesis, Massachusetts Institute of Technology, 2014.
Tai Sing Lee and David Mumford. Hierarchical bayesian inference in the visual cortex. JOSA A,
20(7):1434-1448, 2003.
Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov.
Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In ICML, pages 1928-1937, 20-22 Jun 2016.
Brendan O’Donoghue, Ian Osband, and Catalin Ionescu. Making sense of reinforcement learning
and probabilistic inference. In International Conference on Learning Representations, 2020.
11
Under review as a conference paper at ICLR 2021
Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 30, pages 6118-6128. Curran Associates, Inc., 2017.
Daniel Alexander Ortega and Pedro Alejandro Braun. Information, utility and bounded rationality. In
International Conference on Artificial General Intelligence, pages 269-274. Springer, 2011.
Pedro A. Ortega, J. X. Wang, M. Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu,
N. Heess, J. Veness, A. Pritzel, P. Sprechmann, Siddhant M. Jayakumar, T. McGrath, K. Miller,
Mohammad Gheshlaghi Azar, Ian Osband, Neil C. Rabinowitz, A. Gyorgy, S. Chiappa, Simon
Osindero, Y. Teh, H. V. Hasselt, N. D. Freitas, M. Botvinick, and S. Legg. Meta-learning of
sequential strategies. ArXiv, abs/1905.03030, 2019.
Thomas Parr and Karl J Friston. Generalised free energy and active inference. Biological cybernetics,
113(5-6):495-513, 2019.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. Approximate inference and stochastic
optimal control. arXiv preprint arXiv:1009.3958, 2010.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Parallel distributed processing: Explorations
in the microstructure of cognition, vol. 1. chapter Learning Internal Representations by Error
Propagation, pages 318-362. MIT Press, Cambridge, MA, USA, 1986.
Jurgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863-879, 1992.
Guy Shani, Ronen I. Brafman, and Solomon E. Shimony. Model-based online learning of pomdps.
In Machine Learning: ECML 2005, pages 353-364, Berlin, Heidelberg, 2005. Springer Berlin
Heidelberg.
Elizabeth S Spelke, Karen Breinlinger, Janet Macomber, and Kristen Jacobson. Origins of knowledge.
Psychological review, 99(4):605, 1992.
Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,
3(1):9-44, Aug 1988.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Emanuel Todorov. General duality between optimal control and estimation. In 2008 47th IEEE
Conference on Decision and Control, pages 4286-4292. IEEE, 2008.
Sebastian Tschiatschek, Kai Arulkumaran, Jan Stuhmer, and Katja Hofmann. Variational inference
for data-efficient model learning in pomdps. arXiv preprint arXiv:1805.09281, 2018.
Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and Jurgen Schmidhuber. Relational neural
expectation maximization: Unsupervised discovery of objects and their interactions. In ICLR,
2018.
Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, Proceedings of the
Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pages
1439-1456. PMLR, 30 Oct-01 Nov 2020.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in neural
information processing systems, pages 2746-2754, 2015.
Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P Burgess, and Alexander Lerchner.
Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven
exploration. arXiv preprint arXiv:1905.09275, 2019.
12
Under review as a conference paper at ICLR 2021
Paul J. Werbos. Generalization of backpropagation with application to a recurrent gas market model.
NeuraINetworks ,1(4):339-356,1988.
CF Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pages
95-103, 1983.
Bo Xin, Haixu Yu, You Qin, Qing Tang, and Zhangqing Zhu. Exploration entropy for reinforcement
learning. Mathematical Problems in Engineering, 2020, 2020.
ViniciUs Flores Zambaldi, David Raposo, Adam Santoro, Victor Bapst, YUjia Li, Igor Babuschkin,
Karl Tuyls, David P. Reichert, Timothy P. Lillicrap, Edward Lockhart, Murray Shanahan, Victoria
Langston, Razvan PascanU, Matthew Botvinick, Oriol Vinyals, and Peter W. Battaglia. Relational
deep reinforcement learning. CoRR, abs/1806.01830, 2018.
Willard I Zangwill. Nonlinear programming: a unified approach, volUme 196. Prentice-Hall
Englewood Cliffs, NJ, 1969.
GUangxiang ZhU, Zhiao HUang, and Chongjie Zhang. Object-oriented dynamics predictor. In NIPS,
pages 9826-9837. 2018.
13
Under review as a conference paper at ICLR 2021
A Convergence of the Object-based Perception Model Update
Under main assumptions and lemmas as introduced below, we demonstrate the convergence of a
sequence of {Lwt (qw,φ)} generated by the perception update. The proof is presented by showing
that the learning process follows the Global Convergence Theorem (Zangwill, 1969).
Assumption 1. Ωθo = {θ ∈ Ω : Lw (qw, φ) ≤ L,。(qw, φ) } is compact for any L,。(qw, φ) < ∞.
Assumption 2. L is continuous in Ω and differentiable in the interior of Ω.
The above assumptions lead to the fact that {Lwt (qw, φ)} is bounded for any θ0 ∈ Ω.
Lemma 1. Let Ωs be the set of stationary points in the interior of Ω, then the mapping
arg maxθt+ι Aet (θt+1) from Eq. 4 is closed over Ω∖Ωs (the complement of Ωs).
Proof. See Wu (1983). A sufficient condition is that Aθt (θt+1) is continuous in both θt+1 and
θt.	口
Proposition 1. Let Ωs be the set of stationary points in the interior of Ω, then (i.)	∀θt ∈
Ωs,Lwt+1 (qw, φ) ≤ Lwt (qw, φ) and (ii.) ∀θt ∈ Ω∖Ωs," (qw, φ) < Lwt (qw, φ).
Proof. Note that (i.) holds true given the condition. To prove (ii.), consider any θt ∈ Ω∖Ωs, we have
∂Lwt(qw, φ)
-∂θt+1 -
θt+1 =θt
∂Aet (θt+1)
∂ θt+1
6=0.
θt+1 =θt
Hence Aθt (θt+1) is not maximized at θt+1 = θt. Given the perception update described by Eq. (5),
we therefore have Aet (θt+1) > Aet (θt), which implies Lwt+ι (qw, φ) < Lwt (qw, φ).	口
Theorem 1. Let {θt} be a sequence generated by the mappingfrom Eq. 4^ Ωs be the set ofstationary
points in the interior of Ω. If Assumptions 1 & 2, Lemma 1, and Proposition 1 are met, then all the
limit points of {θt} are stationary points (local minima) and Lwt (qw,φ) converges monotonically to
J* = Lw* (qw, φ) for some stationary point θ* ∈ Ωs.
Proof. Suppose that θ* is a limit point of the sequence {θt}. Given Assumptions 1 & 2 and
Proposition 1.i), We have that the sequence {θt} are contained in a compact set Ωk ⊂ Ω. Thus, there
is a subsequence {θl}l∈L of {θt} such that θl → θ* as l →∞and l ∈ L.
We first show that Lwt (qw, φ) → Lw* (qw, φ) as t → ∞. Given L is continuous in Ω (Assumption
2), we have Lwl (qw,φ) →Lw* (qw,φ) as l →∞and l ∈ L, which means
Ve > 0,∃l(c) ∈ L s.t. Vl ≥ l(e),l ∈ L, %(qw, φ) - LW*(qw, φ) < J	(7)
Given Proposition 1 and Eq. (4), L is therefore monotonically decreasing on the sequence {θt}∞=0,
which gives
∀t, Lewt(qw, φ) - Lew*(qw, φ) ≥0.	(8)
Given Eq. (7), for any t ≥ l(e), we have
Lwt (qw, Φ) 一 Lw* (qw, Φ) = Lwt (qw, Φ) 一 LI(C (qw, φ + LI(C (qw, Φ) 一 Lw* (qw, Φ) < j (9)
|---------------------------------V----------} |-----------V----------'
≤0	<ε
Given Eq. (8) and Eq. (9), we therefore have Lwt (qw,φ) →Lw* (qw, φ) as t →∞. We then prove
that the limit point θ* is a stationary point. Suppose θ* is not a stationary point, i.e., θ* ∈ Ω∖Ωs, we
consider the sub-sequence {θl+1}ι∈L, which are also contained in the compact set Ωk. Thus, there
is a subsequence {θl0+1}l0∈L0 of {θl+1}l∈L such that θl0+1 → θ*0 as l0 →∞and l0 ∈ L0, yielding
Lw 0+1 (qw,φ) → Lw*0 (qw,φ) as l0 →∞and l0 ∈ L0, which gives
Lew*0(qw,φ)=limLewl0+1(qw,φ)=limLewt(qw,φ)=Lew*(qw,φ).	(10)
l0→∞	t→∞
l0∈L0	t∈N
On the other hand, since the mapping from Eq. (4) is closed over Ω∖Ωs (Lemma 1), and θ* ∈ Ω∖Ωs,
we therefore have θ*0 ∈ arg maxet+1 Ae* (θt+1), yielding Lw*0 (qw,φ) < Lw* (qw,φ) (Proposition
1.ii), which contradicts Eq. (10).	口
14
Under review as a conference paper at ICLR 2021
B Experiment Details
OPC In all experiments we trained the perception model using ADAM Kingma and Ba (2014) with
default parameters and a batch size of 32. Each input consists of a sequence of binary 84 × 84 images
containing two poison objects (two circles) and one food object (a rectangle) that start in random
positions and move within the image for tro steps. These frames were thresholded at 0.0001 to obtain
binary images and added with bit-flip noise (p = 0.2). We used a convolutional encoder-decoder
architecture inspired by recent GANs Chen et al. (2016) with a recurrent neural network as bottleneck,
where the encoder used the same network architecture from Mnih et al. (2013) as
1.	8 × 8 conv. 16 ELU. stride 4. layer norm
2.	4 × 4 conv. 32 ELU. stride 2. layer norm
3.	fully connected. 256 ELU. layer norm
4.	recurrent. 250 Sigmoid. layer norm on the output
5.	fully connected. 256 RELU. layer norm
6.	fully connected. 10 × 10 × 32 RELU. layer norm
7.	4 × 4 reshape 2 nearest-neighbour, conv. 16 RELU. layer norm
8.	8 × 8 reshape 4 nearest-neighbour, conv. 1 Sigmoid
We used the Advantage Actor-Critic (A2C) Mnih et al. (2016) with an MLP policy as the decision
making module of OPC. The MLP policy added a 512-unit fully connected layer with rectifier
nonlinearity after layer 4 of the perception model. The decision making module had two set of
outputs: 1) a softmax output with one entry per action representing the probability of selecting the
action, and 2) a single linear output representing the value function. The decision making module
was trained using RMSProp Tieleman and Hinton (2012) with a learning rate of 7e - 4, a reward
discount factor γ = 0.99, an RMSProp decay factor of 0.99, and performed updates after every 5
actions.
A2C We used the same convolutional architecture as the encoder of the perception model of OPC
(layer 1 to 3), followed by a fully connected layer with 512 hidden units followed by a rectifier
nonlinearity. The A2C was trained using the same setting as the decision making module of OPC.
WM-A2C We used the same setting as Ha and Schmidhuber (2018) to separately train the V model
and the M model. The experience was generated off-line by a random policy operating in the Pixel
Waterworld environment. We concatenated the output of the V model and the M model as the A2C
input, and trained A2C using the same setting as introduced above.
15
Under review as a conference paper at ICLR 2021
C Details of Derivation
C.1 Derivation of Eq.(1)
logp(o≥t, x≤t∖a<t)
= log X	p(o≥t, s≥1, x≥1,α≥t∖α<t)
s≥1 ,a≥t ,x>t
=log	^X	p(o≥t∖s≥1, x≥1, α≥1)p(s≥1, x≥1,α≥1 ∖a<t)
s≥1 ,a≥t ,x>t
= log	X	Y p(on∖sn,αn)p(x≤t∖s≥1, x>t,a≥1)p(s≥1, x>t,a≥1∖a<t)
s≥1 ,a≥t,x>t n = t
t
= log X	Yp(on∖sn,an) ∏p(xj∖sj)p(x>t∖s≥1,a≥1)p(s≥1,a≥1∖a<t)
S≥l ,a≥t,x>t n = t	j = 1
t
= log X ∏p(on∖sn,an) ∏P(XISj) ∏ p(xk∖sk)p(s>t∖s≤t,a≥1)p(s≤t,a≥1∖a<t)
s≥1 ,a≥t,x>t n = t	j = 1	k = t + 1
t	t-1
= log X ∏p(on∖sn,an) ∏P(Xj∖sj) ∏ p(xk∖sk) ∏p(sl+1∖sl,al)p(s1) ∏ p(sm+1∖sm,am) ∏p(ah∖a<t)
s≥1 ,a≥t,χ>t n = t	j=1	k = t + 1	l=t	m = 1	h=t
= log
t	t-1
X∏P(Xj∖sj)p(s1) ∏ p(sm+1 ∖sm,am)
s≤t j = 1	m = 1
E ∏p(on∖sn,an) ∏p(xk+1∖sk+1)p(sk+1∖sk
s>t a≥t χ>t n = t	k = t
'-----------------------------V------------------
,	①
= log
∑>(s≤t∖x≤t
s≤t
<八 ∏j=1 P(xj ∖sj)p(s1) ∏m=1 p(sm+1 ∖sm, am)
a	q(s≤t∖x≤t, a<t)
[①]
C.2 Derivation of Eq. (4)
θt+1 = arg max qw log
θt+1
Qj=Ip(x"sj)p(s1) Qm=Ip(Sm+1|sm,am)
arg max qw log
θt+1
p(s1) Qg=I q(sg+1∣x≤g, α≤g)
p(s1)p(x1 ∣s1) Qt=1 p(xj+1, sj+1∣sj, aj)
arg max ηt log
θt+1
' p(s1) Qg=I q(sg+1∣x≤g, α≤g)
p(x1∣s1) Qt=1 p(xj+1, sj+1∣sj, aj)
QgH ηg+1
drop terms which are constant with respect to θt+1
arg max	E	[logp⅛(xt+1, zt+1, ψt+1 ∣st, αt)]
θt+1 zt+1 〜必
arg maχ Aet (θt+1)
16
Under review as a conference paper at ICLR 2021
C.3 DERIVATION OF EQ. (5)
∂Λθt+ι (θt+2) _ ∂Λθt+ι(θt+2)	∂{ψ+1}τ
=	∙	.
∂θtl+1	∂ ψ+ι	∂{θtl+1 ]T
∂Λθt+ι (θt+2)

∂ Λ3+ι (θt+2)
∣^∂ΨX^∣
∂θt+1
k
WDi
-∂et+1 -
k
∂Λθt+ι (θt+2) ∂ψi+1
--------:----......-
∂ψif	∂θt+1
D
X
i=1
D
X
i=1
D
X
i=1
E Pφ(zi+1 |xt+1, 此 a) logPψt+ι (xt+1, zi+1|st, a
.zt+1
∑>(z"l
zt+1
i
一 K
X Pφ(嗡1 =
,t = 1
kt+1
/+1
加;+
)	∂θt+1
,ψt, a) logPψt+ι (xt+1 ⑶+1)p(zt+11st, *	.
i
1lxi+1, ψi,t ,at) log Pψt+1 (W+1|zt+I)P(Nt+1|st, a
∂ψ+
dθt+1
)1 ,必
此+1
D
X Pφ (
i=1
D
X Pφ (
i=1
D
X Pφ (
i=1
D
X Pφ (
i=1
D
X Pφ (
i=1
D
X Pφ (
zi+ - 1|xt+1, Ψt,at) 口
i,t
[logpψt+13+1归+1加以+1回,/)] ∙ ；；+i
i,k	∂ θjz
zi+1 - 1|xt+1, ψt,at)
喷1-1Wt+1, 此 at)
zi+1 - 1|xt+1, Ψi,at)
z+1 - 1|xt+1, ψt,at) ∂ψt+ι
i=1
D
Eq-(3)X 唳∙
i=1
zi+1 - 1|xt+1, Ψt,at)∙-
*+1-χt+1
σ2
)+ log p(zt+1|st,ɑt)^
)1必
∂ θt+1
—
σ2
—
—
σ2
12(χt+1- Ψt+1)∙(-1) ∂ψ/
2
σ2
∂θtt+1
WT .
dψ+T
∂θt+1
))
]∂ψt+1
i i,t
J ∂θt+1
+
∂θt+1
17