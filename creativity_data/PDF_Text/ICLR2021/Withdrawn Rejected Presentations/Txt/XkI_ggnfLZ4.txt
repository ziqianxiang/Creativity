Under review as a conference paper at ICLR 2021
Uncovering the impact of hyperparameters
FOR GLOBAL MAGNITUDE PRUNING
Anonymous authors
Paper under double-blind review
Ab stract
A common paradigm in model pruning is to train a model, prune, and then either
fine-tune or, in the lottery ticket framework, reinitialize and retrain. Prior work has
implicitly assumed that the best training configuration for model evaluation is also
the best configuration for mask discovery. However, what if a training configura-
tion which yields worse performance actually yields a mask which trains to higher
performance? To test this, we decoupled the hyperparameters for mask discovery
(Hfind) and mask evaluation (Heval). Using unstructured magnitude pruning on vi-
sion classification tasks, we discovered the “decoupled find-eval phenomenon,”
in which certain Hfind values lead to models which have lower performance, but
generate masks with substantially higher eventual performance compared to us-
ing the same hyperparameters for both stages. We show that this phenomenon
holds across a number of models, datasets, configurations, and also for one-shot
structured pruning. Finally, we demonstrate that different Hfind values yield masks
with materially different layerwise pruning ratios and that the decoupled find-eval
phenomenon is causally mediated by these ratios. Our results demonstrate the
practical utility of decoupling hyperparameters and provide clear insights into the
mechanisms underlying this counterintuitive effect.
1	Introduction
There has been significant progress in deep learning in recent years, but many of the best performing
networks are extremely large (Kolesnikov et al., 2019; Brown et al., 2020). This can be problematic
due to the amount of compute and memory needed to train and deploy such models. One popular
approach is model pruning: removing weights (unstructured pruning) or units (structured pruning)
from a trained network in order to generate a smaller network with near-equivalent (and in some
cases, better) performance (Blalock et al., 2020; Lin et al., 2020; Liu et al., 2017; He et al., 2019b;
Molchanov et al., 2019). One of the most commonly used heuristics is magnitude pruning, in which
the lowest magnitude weights/units are removed, as it is simple and competitive with more complex
methods (Han et al., 2015; Gale et al., 2019).
The lottery ticket hypothesis (Frankle & Carbin, 2018), a related concept, posits that a large network
contains a smaller subnetwork at initialization that can be trained to high performance in isolation,
and provides a simple pruning method to find such winning lottery tickets (LTs). In addition to
allowing the training of sparse models from scratch, the existence of LTs also suggests that over-
parameterization is not necessarily required to train a network to high performance; rather, over-
parameterization may simply be necessary to find a good starting point for training.
When pruning a network, one must decide how much to prune each layer. Should all layers be
pruned equally? Or rather, should some layers be pruned more than others? Previous studies have
shown that global pruning results in better compression and performance than layerwise (or uniform)
pruning (Frankle & Carbin, 2018; Morcos et al., 2019). This is because global pruning ranks all
weights/units together independent of layer, granting the network the flexibility to find the ideal
layerwise pruning ratios (LPR), which we define as the percent that each layer is pruned.
One can frame most pruning methods as having two phases of training: one to find the mask, and
one to evaluate the mask by training the pruned model (whether by rewinding and re-training or fine-
tuning). Common practice for methods that rewind weights after pruning, such as lottery ticket prun-
ing, has been to use the same hyperparameters for both finding and evaluating masks (see Blalock
1
Under review as a conference paper at ICLR 2021
et al. (2020) for an extensive review). Methods that fine-tune weights after pruning typically train at
a smaller learning rate than the training phase to find the mask (Han et al., 2015; Renda et al., 2020),
but other hyperparameters are held constant.
Pruning methods also rest on the assumption that models with the best performance will generate
the best masks, such that the optimal hyperparameters for mask generation and mask evaluation
should be identical. However, what if the mechanisms underlying mask generation are not perfectly
correlated with those leading to good performance? Consequentially, what if models which converge
to worse performance can yield better masks? To test this, we explored settings for global magnitude
pruning in which different hyperparameters were used for mask generation and mask evaluation. In
particular, we focused on three hyperparameters that are commonly adjusted in practice: learning
rate, batch size, and weight decay. Using this paradigm, we make the following contributions:
•	Surprisingly, we found that the best hyperparameters to find the mask (Hfind) are often
different from the best hyperparameters to train the regular model or to evaluate the mask
(Heval; Figures 1 and 2), which we term the “decoupled find-eval phenomenon”. Counter-
intuitively, this means that models with worse performance prior to pruning can generate
masks which leads to better performance for the final pruned model than a mask generated
by a higher performance model pre-pruning.
•	We show that this phenomenon is not an artifact of the particular setting we studied and also
occurs for structured pruning (Figure 5), other datasets, and other variants of LTs, including
late rewinding, learning rate warmup, and learning rate rewinding (Figure 4).
•	We found that different hyperparameters for mask generation led to materially different
LPRs (Figures 6a and A18). Notably, we observed that a larger learning rate, smaller batch
size, and larger weight decay (which resulted in worse masks) consistently pruned early
layers far more than in masks found with the opposite.
•	Finally, we show that this phenomenon is causally mediated by the differences in LPR.
When the LPR is fixed to a “good” LPR (i.e., that of a high performance mask), the pre-
viously bad hyperparameters now lead to better performance and better mask generation
(Figure 7). The same is true for the inverse experiment.
2	Methods
2.1	Modified pruning procedures
Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin
(2018) along with follow up work (Renda et al., 2020) for unstructured pruning. We used global
iterative magnitude pruning (IMP) for LTs because it has been shown to perform better than one
shot pruning, where the pruning procedure is only done once rather than iteratively, and local (or
uniform layerwise) pruning, where each layer has the same pruning ratio (Morcos et al., 2019;
Frankle & Carbin, 2018). We also investigated structured one-shot pruning, following Liu et al.
(2017), which also prunes globally. All experiments use magnitude pruning.
We define four different sets of hyperparameters: Hunpruned, which is used for regular training of an
unpruned model; Hfind, used to find masks (i.e. the pre-training part of the pruning procedure); Heval,
used for obtaining the final pruned model to be used for inference and evaluating the final perfor-
mance of a mask; and HLT to refer to the hyperparameters optimized for LTs where Hfind,Heval.
Note that HLT is often different from Hunpruned in practice and in previous literature (Liu et al., 2019;
Frankle & Carbin, 2018). Our modified procedures can incorporate changes in any hyperparame-
ters, but in this paper we focus on experiments with learning rate (LR), batch size (BS), and weight
decay (WD). For the main experiments, we focus on one hyperparameter at a time. When only one
hyperparameter is changed from Hfind and Heval, we will denote it such as LRfind, WDeval, etc.
Unstructured pruning To account for these distinct sets of hyperparameters, we slightly modified
the procedure for unstructured IMP as described in Algorithm A1. We emphasize that our modi-
fication requires no additional compute compared to the original LT procedure if used practically.
However, to generate additional data points for analysis in the present work, we separately evaluated
2
Under review as a conference paper at ICLR 2021
b)
Auembe⅛8J.
Regular lottery tickets, LRf∣nd = LReVal = 0 2
0.5
0.4
0.3
0.2
0.1
0.0
Auembe⅛8J.
Fraction of weights pruned
Fraction of weights pruned
Figure 1: Test accuracy at various sparsity levels, with the regular lottery ticket procedure, i.e. LRfind =
LReval. “Lottery” is the selected mask (winning ticket), “global random” is a mask selected at random globally,
and “preserve LPR” is a random mask that has the same LPR as the winning ticket. (a) LR = 0.5 performed
very poorly, even worse than the random baseline. (b) Curves we would expect from a properly functioning
lottery ticket. The pruned performance is much better, even though training a regular model without pruning is
better with LR as 0.5 than 0.2.
many of the intermediate masks as well. We pruned weights with the smallest magnitudes through-
out the whole network (i.e., global magnitude pruning) by p = 20% at each pruning iteration until
we reached 99.9% pruned, for a total of 30 pruning iterations. For some experiments, we also mod-
ified line 6 for techniques such as late rewinding and learning rate rewinding (Renda et al., 2020).
The original LT algorithm is recovered in Algorithm A1 when Heval = Hfind.
Structured pruning For structured pruning, we consider one-shot pruning using the scale of batch
normalization layers as described in Algorithm A2. This also has no additional compute requirement
compared to the original, and the original is again recovered when Heval = Hfind .
2.2	Models and datasets
For our main results, we used ResNet-50 (He et al., 2016) on Tiny ImageNet1 (Le & Yang, 2015),
and we also ran experiments with ResNet-18 on Tiny Imagenet and ResNet-50 on MiniPlaces (Zhou
et al., 2017). We only pruned convolutional layers, such that the fully connected output layer of
ResNets remained unpruned. Unless otherwise stated, we used SGD with 0.9 momentum and trained
for 110 epochs, decaying the learning rate with a gamma of 0.2 at epochs 70, 85, and 100. See
Section A.1 for details. The defaults for Hfind and Heval are batch size of 512, learning rate of 0.2,
and weight decay of 1e-4; adjustments will be explicitly noted.
2.3	Random baselines
We compare the selected masks to two random baselines. The first, “global random”, selects a
mask at random, uniformly throughout the whole network. This means that all layers have the same
expected pruning ratio. The second, “preserve LPR”, also selects a mask at random, but with the
same LPR as the winning ticket that we compare to; this is equivalent to shuffling the winning ticket
mask within each layer. Both baselines are evaluated with new randomly initialized weights. Shaded
error bars throughout represent standard deviation across 5 runs.
3	Results
3.1	Failing lottery tickets
The failure of large LRs for lottery tickets has been pointed out previously (Liu et al., 2019); win-
ning tickets were only better than random baselines at a small LR. Directionally consistent with this
work, we found a particularly extreme case: not only can random baselines match the performance
of the winning tickets with large LR, they can be significantly better. We found these catastroph-
ically failing lottery tickets by following the LT procedure using a learning rate of 0.5 (for both
LRfind and LReval), as in Hunpruned. This is illustrated in Figure 1a, where the winning tickets (blue)
and the random baseline with the same LPR as the winning ticket (green) are both worse than the
1Use of these datasets is only for noncommercial, research purposes, and not for training networks deployed
in production or for other commercial uses
3
Under review as a conference paper at ICLR 2021
Masks found with WDfind = 2e-5
Figure 2: Test accuracy for different combinations of Hfind and HeVab adjusting learning rate (a, d), batch size
(b, e), or weight decay (c, f). Top row (a-c): given a constant HeVab We compare different values of Hfind.
Bottom row (d-f): given a constant Hfind, We compare different values of HeVab Random baselines have the
same Hfind and Heval as the value in the title. The points at 0% sparsity represent the unpruned model trained
with the same Hfind as the rest of each line. Overall, it is bad for Hfind to have a large LR, small BS, or large
WD, but the opposite is true for HeVab See Figures A2-A6 for additional combinations.
Fraction of weights pruned
global random baseline (orange) by over 25 percentage points at 96-98% sparsity. Surprisingly, this
winning ticket is much worse than the global random baseline, rather than just having no significant
improvement over the random baseline as found in Liu et al. (2019) and Frankle & Carbin (2018).
The learning rate of 0.5 is not unreasonably large - in fact, the unpruned model performs the best at
LR = 0.5. This failure also arises for reasonable values of batch size and weight decay (Figure A1).
Prior works have addressed this problem by training LTs with smaller learning rates (HLT instead
of Hunpruned, where for both, Hfind,Heval) . Similarly in our experiments, dropping the LR from 0.5
to 0.2 resulted in winning tickets that substantially outperform the global random baseline and are
somewhat better than the random baseline with the same LPR as the winning ticket (Figure 1b).
This is why, in the lottery ticket setting, HLT is typically different from Hunpruned (Frankle & Carbin,
2018). However, training the standard unpruned model with a LR of 0.2 instead of 0.5 decreases
performance from 52.5% to 48.9%, as depicted by comparing the leftmost points in Figure 1. The
accuracy is decreased for low sparsity pruned networks as well. Current practice therefore faces a
difficult trade-off between low sparsity and high sparsity model performance.
These findings raise questions in two directions. First, for practical purposes, can we get the best
of both worlds by decoupling the hyperparameters used to find the mask (Hfind) and to evaluate the
mask (Heval)? This is inspired by the fact that large LR/small BS/large WD does well at low sparsity
levels but causes the pruned model to get worse as sparsity increases, suggesting that there may be
potential for better winning tickets. Second, from a scientific standpoint, how do hyperparameters
affect pruning performance, and why does Hunpruned not work well for lottery tickets?
3.2	The decoupled find-eval phenomenon
To test the whether we can adjust hyperparameters to improve the performance of winning tick-
ets, we decoupled Hfind and Heval by setting learning rate (LR), batch size (BS) and weight decay
(WD) independently for mask generation and evaluation. Specifically, we evaluated LRfind, LReval
∈ {0.05, 0.2, 0.5}; BSfind, BSeval ∈ {256, 512, 1024}; and WDfind, WDeval ∈ {2e-5, 1e-4, 5e-4} for
a total of nine find-eval combinations for each hyperparameter.
4
Under review as a conference paper at ICLR 2021
Figure 3: Comparison of the best coupled vs. decoupled run, for all three hyperparameters. For clarity, each
point at a given sparsity level is mean and standard deviation of the best configuration of any decoupled (blue)
or coupled (orange) combination of Hfind, Heval at that sparsity level. This was done because the best Heval
changes with sparsity level, and this was cleaner than plotting several lines.
a) l,5
0.50
0.45
g 0-40
§ 0.35
⅞> 0.30
L 0-25
0-20
0-15
Figure 4: Test accuracy on variants of LT, with two values of LRfind and two values of LReval each. (a) Late
rewinding to 2 epochs instead of initialization. (b) Learning rate warmup. (c) Learning rate rewinding, where
weights continue training from their values during pruning rather than rewinding back to an earlier value. The
better LRfind for pruned models is the smaller one (blue is better than green, orange is better than red), despite
performing worse on the unpruned model, and the best LReval starts high and then decreases.
The results are shown in Figure 2 grouped from two different perspectives. In the top row (a-c),
we compare different values of Hfind for a constant Heval, with one plot for each hyperparameter.
These comparisons show that it is better to find masks with a smaller LRfind (0.2 is slightly better
than 0.05, but both are significantly better than 0.5), larger BSfind, and smaller WDfind. For ease
of discussion, we will denote these as “good Hfind.” In the bottom row (d-f), we compare different
values of Heval for a constant Hfind. Here, we observe the opposite effect: given a constant Hfind, the
best performance is obtained by a larger LReval, smaller BSeval, and larger WDeval; we will denote
these as “good Heval”. More specifically, the best Heval matches Hunpruned for lower sparsity levels
but gradually shift to more intermediate values at higher sparsity levels (around 96-98%).
It is interesting to note that decreasing the batch size has a similar effect as increasing the learning
rate, which supports previous studies about the relationship between the two (Smith et al., 2018;
Goyal et al., 2017; He et al., 2019a) Increasing weight decay also has a similar effect as increasing
the learning rate, corroborating with Zhang et al. (2019). Altogether, these results demonstrate two
key findings: first, the good Hfind values do not achieve the highest model accuracy, which means that
hyperparameter tuning on a regular unpruned models (Hunpruned) may not be relevant when carried
over to pruning. Second, the best Hfind values are not the same as the best Heval values, thus it is
important to decouple the Hfind and Heval in LT pruning. The importance of decoupling can be seen
clearly when we compare the best decoupled configuration to the best coupled (original LT) runs
(Figure 3): decoupled is better by about 3-4 percentage points around sparsity levels 70-90%.
3.3	The decoupled LR phenomenon is maintained across different models,
datasets, configurations, and structured pruning
Other models and datasets To expand beyond ResNet-50 on Tiny ImageNet, we also evaluated
a ResNet-50 on MiniPlaces (Figure A8) and a ResNet-18 on Tiny ImageNet (Figure A9). In both
cases, we observed the same effect: the best Hfind requires smaller LR, larger BS, and smaller WD
compared to Hunpruned and Heval.
5
Under review as a conference paper at ICLR 2021
ʌɔsnuue asəj.
Fraction of weights pruned
4 2 0 s 6 4 2
5 5 5 4 4 4 4
、J XOeJnUUe asəj.
Structured pruning, comparing weight decay
565452504β464442
、—/ XOeJnUUe asəj.
Fraction of weights pruned
Figure 5: Hfind should also be decoupled from Heval in one-shot structured pruning. Test accuracy for two
values of Hfind and two Heval for four combinations each for learning rate (a), batch size (b), and weight decay
(c). The best performing configuration requires decoupled values; though this only applies to low sparsity levels
for batch size, the pattern is clearer for learning rate and weight decay. See Figures A11-A12 for full results.
Other variants of lottery ticket For the same model and dataset as our initial results (ResNet-
50 on Tiny ImageNet), we tried the following: using late rewinding, learning rate rewinding, and
learning rate warmup. We ran late rewinding experiments from Renda et al. (2020), where instead
of rewinding the weights to θ0 in line 6 of Algorithm A1, we set the weights to their values at epoch
2. Following Frankle et al. (2019b), we also found that training from this point on was stable, such
that we could linearly between modes without substantial drops in performance, though notably this
was only true for small learning rates (Figures A13, A14). Epoch 2 was chosen to be roughly similar
to experiments in Morcos et al. (2019), which showed strong winning ticket performance across a
number of datasets. Here, we again see the same pattern, where the best LRfind is smaller than the
best standard LR (Figure 4a). For learning rate warmup, we warmed up the learning rate linearly
over 1000 iterations from 0 to the specified LRfind . The effect of LRfind is the same as well, although
it is more subtle (Figure 4b).
Our learning rate rewinding experiments also encapsulate the fine-tuning experiments (Renda et al.,
2020). That is because both methods prune and then continue training the remaining weights from
that point; the only difference is that learning rate rewinding trains at a large learning rate, and fine-
tuning trains at a small learning rate (due to the decay in learning rate in the training phase before
pruning). Thus, we extended learning rate rewinding to different LRs. We saw the same effect,
with even better overall performance: taking a mask from a small LRfind and using learning rate
rewinding to a high LReval improves accuracy by approximately 3 percentage points compared to
weight rewinding (Figure 4c). This also explains why fine-tuning in Renda et al. (2020) did worse
than learning rate rewinding: a larger LReval is better, and fine-tuning is effectively the same as
learning rate rewinding but with a smaller and worse LReval.
Structured pruning We have demonstrated that our results in Section 3.2 are not merely an arti-
fact of the precise setting we studied, but rather represent a more generic feature of lottery tickets
across models, datasets, and configurations. However, this phenomenon may still be limited to only
lottery tickets. To test its applicability to pruning more generally, we also investigated structured
pruning on ResNet-18 Tiny ImageNet in which entire filters are removed. Once again, we found
that the best performing pruned networks required different Hfind and Heval (Figure 5), though the
improvement is more subtle. Like before, we found that it is generally better to find the mask with a
smaller LRfind and WDfind and larger BSfind, and evaluate with a larger LReval and WDeval and smaller
BSeval. The decoupled configuration can increase the top-1 accuracy by up to 2 percentage points
at moderate sparsity levels. Coupled hyperparameters were often best at high sparsity levels, but at
that point, performance across all configurations were subtantially lower than the unpruned model.
These results demonstrate that the phenomenon of decoupling hyperparameters does not just apply
to lottery tickets, but also applies to additional methods of global magnitude pruning.
3.4	Layerwise pruning ratios causally mediate the decoupled LR phenomenon
In Figure 1a, we observed that using a high LR for HLT resulted in performance below the global
random baseline not only for the lottery ticket, but also for the “preserve LPR” random baseline. This
suggests that LPR may be related to the decoupled find-eval phenomenon. To test this, we examined
the LPR over different pruning iterations for various settings. Consistent with our hypothesis, we
found that LPR varied dramatically across different Hfind (Figure 6). Most strikingly, we observed
6
Under review as a conference paper at ICLR 2021
a)
0.8
LPR at 67% pruned, comparing learning rate
——LAm = 0.05
——LAim = 0.2
Λ A	——LAfM = 0.5
b)
LPR at 67% pruned, comparing batch size
——BSftld = 256
-BSltni = 512
—BSlhd = 1024
6UC-EUJ3-4u3t!9d
6u-u-euJ3.14u3t!9d
5UC-BUJ3-4Ug
c)
Figure 6: Layerwise pruning ratios depend on Hfind. Shown here is the LPR of masks found by the three
different values of LRfind (a), BSfind (b), and WDfind (c). Small LR, large BS, and small WD values tend to
prune more in the earlier layers. See Figure A17 in the appendix for equivalent plots at other sparsity levels.
Good vs. bad LPRfor learning rate
a)
LR σ.5 match LPR 0.2
LR 0.2 match LPR 0.5
LRfind - 0-2, LReraI - 0.5
Good vs. bad LPR for batch size
ZSeJnUUe4->sal-
c)
o.o
Good vs. bad LPR for weight decay
o.o
Fraction of weights pruned	Fraction of weights pruned	Fraction of weights pruned
Figure 7: Test accuracy when LPR is constrained. For each of learning rate (a), batch size (b), and weight
decay (c), the blue line represents pruning using bad Hfind but LPR fixed to a run from good Hfind. The orange
line represents pruning using good Hfind but LPR fixed to a run from bad Hfind, which performs much more
poorly. For comparison, the green lines are regular pruning runs (from Figure 2) that have the same LPR as the
blue line, except with a different Hfind used for finding the masks within each layer. All three lines in each plot
are evaluated with the same Heval.

AlBJnUUe asəj.
三
AlBJnUUe asəj-
that the bad Hfind values (large LRfind, small BSfind, and large WDfind) tend to prune the earlier layers
much more. This imbalance was consistent across different sparsity levels (Figure A17).
We have shown there is a clear difference in LPR across various Hfind, but is the change in LPR
directly responsible for the performance of the pruned models? If this is the case, we should be able
to change the performance of models just by constraining the model’s LPR. To test this, we ran two
types of experiments. Based on the results of Figure 2, we consider the LPR of masks from LRfind
= 0.2, BSfind = 1024, and WDfind = 2e-5 as “good LPR”. Likewise, the LPR of masks from LRfind =
0.5, BSfind = 256, and WDfind = 5e-4 can be considered as “bad LPR”. We ran Algorithm A1 with
bad Hfind but forced the pruning step to prune a fixed amount each layer, specifically, according to
the good LPR from the runs with respective good Hfind (“match-good-LPR”). While the LPR are
fixed, the masks within each layer were still determined by the particular training configuration (the
bad Hfind in this case). We also ran the complementary procedure with good Hfind values but pruning
according to the bad LPR (“match-bad-LPR”).
If LPR causally mediates the decoupled LR phenomenon, we would expect to see match-good-
LPR do well and match-bad-LPR do poorly, despite being trained with Hfind which typically result
in the opposite effect. This is in fact what we observed: match-good-LPR (blue) substantially
outperformed the match-bad-LPR (orange; Figure 7). By constraining LPR, we were able to account
for the entire change in performance present in the decoupled find-eval phenomenon, demonstrating
that this phenomenon is causally mediated by LPR. Furthermore, the match-good-LPR does even
better than the regular pruning runs with good Hfind from Section 3.1 (green), indicating that the
previously “bad” Hfind were only bad because of their LPR. Interestingly, we observe this same
effect when we constrain LPR to be the same for all layers, equivalent to local or uniform layerwise
pruning (Figure A15, Figure A16), confirming the importance of LPR to the decoupled find-eval
phenomenon.
Adjusting weight decay and learning rate together Thus far, we have adjusted each hyperpa-
rameter independently, but these parameters can also be adjusted jointly. One particular result of
interest is that when the weight decay is set to 0, the learning rate barely affects the LPR (Figure 8;
contrast with Figure 6a) - while there is still some visible difference within each block of layers, the
larger sections of the network are all pruned equally. Additionally, higher LRfind now perform very
7
Under review as a conference paper at ICLR 2021
a)
LPR at 67% pruned with no weight decay
50
5U-U-23-4u33」ad
b)
Λ3En8e asaF
Figure 8: LPR for different LRfind values at (a) 67% pruned (b) and 89% pruned. When weight decay is
set to zero, LRfind has a significantly smaller effect on LPR. This is in contrast to Figure 6a, where the early
layers get pruned much more at higher learning rates. As a result, overall performance is no longer harmed by
a large LRfind: when comparing a large range of LRfind, all evaluated with a reasonable LReval = 0.5 and WDeval
= 1e-4, the larger LRfind does better (c).
a)
0.5
Early pruning, LRfind = 0.05
b)
0.5
⅛3b-
Fraction of weights pruned
Fraction of weights pruned

Figure 9: Early pruning: Pruning at epoch 20 (blue) is bad, but pruning at epochs 50 (orange) or 70 (green)
is actually better than pruning at convergence (110 epochs, red) for LRfind = 0.2. Pruning at 70 for LRfind =
0.05 also performs well. All masks are evaluated with LReval = 0.5, except for the points at 0% sparsity, which
represent an unpruned model trained for the corresponding number of epochs.
well, even slightly better than moderate or small LRfind, similar to the match-good-LPR experiments
above. This is in stark contrast to earlier results with a weight decay of 1e-4.
3.5	Early pruning
For a high LRfind , LPR changes dramatically over training, while a low LRfind leads to stable LPR
throughout training, with the exception of the first layer (Figure A18). Furthermore, the distance
between the early mask’s LPR and the actual mask’s LPR decreases quickly and plateaus, likely due
to the learning rate decay (Figure A19). This inspired us to experiment with pruning early, as in Yin
et al. (2020). We found that early pruning can be even better than pruning at convergence: pruning
at either epoch 50 or 70 for LRfind = 0.2 actually improves performance compared to pruning at the
end of training (Figure 9). There are two useful implications here. First, this is evidence of yet
another hyperparameter that follows the same phenomenon: the best settings to train a mask for
evaluation or to train a standard unpruned model (110 epochs) is not necessarily the same as the best
settings to prune the model (50 epochs). Second, for the particular case of epochs spent training,
this allows us to save compute. As mentioned in Section 2.1, pruning with decoupled Hfind and Heval
requires no extra compute compared to the original LT procedure. In this case, Heval decreases the
compute used to find masks by 55% while also gaining a slight increase in test accuracy.
4	Additional related work
Magnitude pruning, in which weights or filters are removed according to their magnitude, was first
introduced in Han et al. (2015), and has had great success, prompting a large field of study, recently
summarized by Blalock et al. (2020). Structured pruning approaches have had great success since
such techniques can accelerate CNN during test-time without requiring specialized hardware and
software support. Li et al. (2016) have demonstrated the potential of channel pruning for improving
the efficiency of CNNs at test time. Later, many methods were proposed to further improve the
performance of structural pruning (Liu et al., 2017; He et al., 2019b; Molchanov et al., 2019; Fan
et al., 2020). Related to our work, Liu et al. (2019) have demonstrated the importance of LPR over
pre-trained weights for a wide range of structural pruning methods. Complementing their results,
8
Under review as a conference paper at ICLR 2021
our findings further suggest that while both weights and LPR are important for pruning techniques,
the value of weights is conditioned on good LPR, which is implicitly determined by hyperparameters
when it comes to global magnitude pruning. They also pointed out the failure of large LRs for lottery
tickets, resulting in lottery tickets being trained with smaller LRs.
The lottery ticket hypothesis argues that over-parameterization is necessary to find a good initializa-
tion rather than for optimization itself (Frankle & Carbin, 2018; Frankle et al., 2019a). Since their
introduction, a number of works have attempted to understand various aspects of this phenomenon,
including their transferability (Morcos et al., 2019), the factors which make particular winning tick-
ets good (Zhou et al., 2019), and linear mode connectivity (Frankle et al., 2019b). The lottery ticket
hypothesis has also been challenged by (Liu et al., 2019), in part because of the requirement for
small learning rates, as we address in this work.
Tanaka et al. (2020) observed that layer collapse, which they defined as entire layers being com-
pletely pruned, is a key issue that causes some pruned models to fail. This is along the same lines
of the bad LPR that we observed, with a few key differences: we saw poor performance even before
layer collapse, and skip connections in ResNets should make layer collapse less of an issue. Further,
they claim that iterative pruning avoids layer collapse, but we see failure cases in iterative pruning.
In Li et al. (2020), the authors also decouple the learning rate between pre-training and fine-tuning,
somewhat analogous to how we decouple the learning rate between finding and evaluating a mask;
they are similar concepts applied to different tasks. As we analyze the impact of LRfind, they analyze
the significance of the learning rate for fine-tuning on a different dataset.
There are several proposed methods to prune at initialization, which does not require training the
model before finding the mask (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020). In contrast
with our experiments, there is no Hfind (beyond the initialization distribution), only Heval . While
pruning at initialization is more efficient resource-wise, Frankle et al. (2020) recently demonstrated
in extensive experiments that performance of these approaches is not on par with pruning after
training. Hence, it is still critical to study methods that do not prune at initialization.
5	Discussion
In this work, we proposed a simple idea: use different hyperparameters for mask discovery and
evaluation for pruning. We uncovered the counterintuitive decoupled find-eval phenomenon (Section
3.2), in which using hyperparameters that lead to worse model performance yield better masks.
This effect is consistent across both lottery tickets and structured pruning (Section 3.3). We also
demonstrated that values of Hfind which correspond to effectively small learning rates also lead to
materially different LPRs in which early layers are pruned less than later layers (Figure 6a), and
that these LPR causally mediate the decoupled find-eval phenomenon (Section 3.4). While LPR is
not quite all you need, it does determine a very significant part of the final pruned performance and
provides insights into the masks found by pruning.
Practical implications Our work also has several key practical implications to improve com-
pressed model performance. Most notably, we found that tuning hyperparameters separately for
the two phases of training can result in substantial performance gains. While this does introduce ad-
ditional hyperparameter dimensions to search over, our experiments suggests several general guide-
lines for tuning these parameters: Hfind requires smaller LR and WD or larger BS; Heval is similar to
Hunpruned at low sparsity levels; and Heval at high sparsity levels should gradually decrease LR and
WD or increase BS. Alternatively, we found that simply turning off weight decay during Hfind pro-
duced good LPRs and, consequentially, good performance. Alternatively, don’t use weight decay
for finding masks and then the LR no longer matters. Finally, we observed that masks generated
earlier in training often yielded better overall performance, suggesting that by pre-training for fewer
epochs, both compute efficiency performance can be improved.
Limitations and future work While we showed that the decoupled find-eval phenomenon is
causally mediated by LPR, we did not find a clear answer to the question of why certain Hfind
leads to better LPR. In addition, a method to derive a “good” LRP would be very useful. We hope
to investigate these question further in future work. Furthermore, it would be interesting to study
additional hyperparameters and how they interact, and experiment with more datasets, architectures,
and tasks beyond vision classification.
9
Under review as a conference paper at ICLR 2021
References
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning?, 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv, pp. arXiv-2005, 2020.
Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with
structured dropout. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SylO2yStDr.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. The lottery ticket
hypothesis at scale. March 2019a. URL http://arxiv.org/abs/1903.01611.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode
connectivity and the lottery ticket hypothesis, 2019b.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Pruning neural
networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576,
2020.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. February
2019. URL http://arxiv.org/abs/1902.09574.
Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, LUkasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training im-
agenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.
02677.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pp. 1135-1143,
2015.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems,
volume 32, pp. 1143-1152. Curran Associates, Inc., 2019a.
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median
for deep convolutional neural networks acceleration. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4340-4349, 2019b.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 2019.
Y. Le and X. Yang. Tiny imagenet visual recognition challenge. 2015.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
10
Under review as a conference paper at ICLR 2021
Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and
Stefano Soatto. Rethinking the hyperparameters for fine-tuning. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
B1g8VkHFPH.
Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning
with feedback. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=SJem8lSFwB.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
International Conference on Computer Vision,pp. 2736-2744, 2017.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rJlnB3C5Ym.
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation
for neural network pruning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 11264-11272, 2019.
Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: gener-
alizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Infor-
mation Processing Systems, 2019.
Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural
network pruning. In International Conference on Learning Representations, 2020.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=B1Yy1BxCZ.
Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow, 2020.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkgsACVKPH.
Shihui Yin, Kyu-Hyoun Kim, Jinwook Oh, Naigang Wang, Mauricio Serrano, Jae-Sun Seo, and
Jungwook Choi. The sooner the better: Investigating structure of early winning lottery tickets,
2020. URL https://openreview.net/forum?id=BJlNs0VYPB.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=B1lz-3Rct7.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 mil-
lion image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2017.
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros,
signs, and the supermask. In Conference on Neural Information Processing Systems, 2019.
11
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
1
2
3
4
A Appendix
A. 1 Additional training details
Algorithm A1: Unstructured iterative magnitude pruning
Input: Model parameters θ, pruning percentage per iteration p, number of total pruning
iterations N, mask-finding hyperparameters Hfind, mask-evaluating hyperparameters
Heval
Output: Performance of trained pruned models
Initialize mask M0 as all ones
Randomly initialize model weights θ0
for i = 1...N do
Train model θ	Mi-1 to convergence using hyperparameters Hfind to obtain θi
Prune p% of parameters θi, the ones with the smallest magnitudes globally and have not
been masked out by Mi-1, to create a binary mask Mi
Rewind weights back to initial θ0
end
Evaluate final mask or other masks of interest (Mi) by training θ	Mi with hyperparameters
Heval and record its performance
Algorithm A2: Structured one-shot magnitude pruning
Input: Model parameters θ, pruning percentage per iteration p, mask-finding hyperparameters
Hfind, mask-evaluating hyperparameters Heval
Output: Performance of trained pruned model
Randomly initialize model weights θ
Train model θ using hyperparameters Hfind to obtain θfinal
Prune p% of parameters θfinal to create a binary mask M by pruning channels with smallest
magnitude in the scale for the batch normalization layers, i.e., γ
Evaluate the mask by training θ M with hyperparameters Heval and record its performance
All unstructured pruning experiments used data augmentation (RandomResizedCrop and Ran-
domHorizontalFlip from torchvision), batch size of 512, and weight decay of 0.0001. We pruned
convolutional layers 20% at each pruning iteration, for a total of 30 iterations. Weights were initial-
ized with Kaiming normal distribution.
ResNet-50 on Tiny ImageNet: we trained 110 epochs, decaying LR by gamma = 0.2 at epochs 70,
85, and 100. Experiments with warmup, late rewinding, and learning rate rewinding (Section 3.3)
use the same hyperparameters, except for modifications explicitly mentioned in the main text.
ResNet-50 on MiniPlaces: we trained 120 epochs, decaying LR by gamma = 0.2 at epochs 80, 110,
and 115.
ResNet-18 on Tiny ImageNet: we trained 110 epochs, decaying LR by gamma = 0.1 at epochs 80
and 100.
Structured pruning: we use the magnitude of γ of batch normalization layers as the ranking metric
to decide which filters to prune. For channels that are connected by residual connections, we sum
their measures as they have to be pruned jointly. We train the pruned network using 200 epochs for
both pre-training and fine-tuning. We use a batch size of 256, weight decay of 5e-5, cosine learning
rate decay, linear learning rate warmup from 0 to the set learning rate within the first 5 epochs, SGD
with 0.9 Nesterov momentum, and 0.1 label smoothing. All layers were initialized with Kaiming
normal distribution. The algorithm is specified in A2. Note that we do one-shot pruning and do not
rewind weights back to their initial values, contrary to the method for unstructured pruning.
Shaded error bars in all figures represent standard deviations over 5 runs with different random
seeds.
12
Under review as a conference paper at ICLR 2021
A.2 Additional figures referenced from Section 3
a)
⅛3J-
256
BSeVal
Regular lottery tickets, BSfind
b)
Regular lottery tickets, WDfind
AOeJnUUe⅛3J.
WDeval = 5
lottery
global random
preserve LPR
BS = 512, WD = le-4, LR = 0.2
Fraction Ofweights pruned	Fraction of weights pruned	Fraction OfWeightS pruned
Figure A1: Test accuracy at various sparsity levels, with the regular lottery ticket procedure, i.e. Hfind= Heval.
Smaller batch size (a) and larger weight decay (b) result in poorly performing lottery tickets, despite being
better for the regular unpruned model, represented by the points at 0% sparsity. (c) is the same as Figure 1b,
copied over for comparison, which uses the default hyperparameters (but not the best ones for training a regular
model).
d)
Fraction of weights pruned	Fraction of weights pruned	Fraction of weights pruned
Fraction Ofweights pruned
Fraction Ofweights pruned
Fraction of weights pruned
Figure A2: All combinations of LRfind and LReval. Top (a-c): given a constant LReval, we compare different
values of LRfind. 0.1 does the best, followed closely by 0.2 then 0.05, and everything is significantly better than
0.5. Bottom (d-f): given a constant LRfind, we compare different values of LReval. 0.5 does the best at lower
sparsity levels and 0.2 at higher sparsity levels. Random baselines have the same LRfind and LReval as the value
in the title.
13
Under review as a conference paper at ICLR 2021
Fraction of weights pruned
	LRflnd	=0.05, LReVal = 0.05
—•—	LRflnd	=0.05, LReva∣ = 0.2
-→-	LRflrxi	—0.05, LReva∣ — 0.5
—•—	LRflnd	=0.2, LReVal = 0.05
	LRflrxi	=0.2, LReva∣ = 0.2
	LRflnd LRflnd	=0.2, LReVal = 0.5 =0.5, LReVal = 0.05
	LRflrxi	=0.5, LReva∣ = 0.2
一	LRflnd	=0.5, LReVal = 0.5
Figure A3: All combinations of learning rate plotted together for comparison
a)
Fraction of weights pruned
Masks found with BSfInd =
d)
Masks evaluated with BSeva∣ = 512
b)
&
A3e-nuuB⅛8J.
v> 0.2
O- O-
Fraction Ofweights pruned
Masks found with BSfInd
&
e
产
Fraction Ofweights pruned
Fraction of weights pruned
Masks found with BSfInd = 1024
Fraction Ofweights pruned
Fraction Ofweights pruned
Figure A4: All combinations of BSfind and BSeval. Top (a-c): given a constant BSeval, we compare different
values of BSfind, showing that larger is better. Bottom (d-f): given a constant BSfind, we compare different
values of BSeval. 256 does the best at low to medium sparsity levels and 512 at higher sparsity levels. Random
baselines have the same BSfind and BSeval as the value in the title.
All batch size combinations
.32.1
Ooo
AUerI UUfSaL
—BSflnd = 256, BSeva∣ = 256
-BSflnd = 256, BSeva∣ = 512
-BSflnd = 256, BSeva∣ = 1024
-BSflnd = 512, BSeval = 256
-BSflnd = 512, BSeval = 512
-BSflnd = 512, BSeva∣ = 1024
-BSflnd = 1024, BSeva∣ = 256
-BSflnd = 1024, BSeva∣ = 512
-BSflnd = 1024, BSeva∣ = 1024
Figure A5: All combinations of batch size plotted together for comparison
14
Under review as a conference paper at ICLR 2021
a)
Aue-Inuue⅛8J.
Masks evaluated with WDeva∣ = 2e-5
ʃ ʃ ʃ ʃ ʃ ʃ ʃ J
Fraction of weights pruned
Masks found with WDflnd = 2e-5
—WDfIM . 2e-5
—WDf1∏<i = le-4
—WDnlXi ≡ 5e-4
—•— global random
—preserve LPR
d)
AUsnUUe⅛3J.
ba
Masks evaluated with WDeva∣ = le-4
Masks evaluated with WDeva∣ = 5e-4
→- WDnM - 2e∙5
--WDnM - le-4
→- WDf1lxi - 5e-4
—*— global random
—preserve LPR
WDflM - ie-4
—WDnW ≡ 5e-4
—global random
—preserve LPR
ʃ ʃ j5 ʃ S 6 Jb S QF 岁 ∕j / S S f ʃ
Fraction of weights pruned	Fraction of weights pruned
AOeJnUUB⅛8J.
Λ3eJnuue⅛8J.
-→- WDβva∣ - 2e-5
τ- WDeM - IeY
-→- WDeVaI - 5e<
—•— global random
—preserve LPR
Fraction Ofweights pruned
Fraction Ofweights pruned
Fraction Ofweights pruned

Figure A6: All combinations of WDfind and WDeval. Top (a-c): given a constant WDeval, we compare different
values of WDfind , showing that smaller is better. Bottom (d-f): given a constant WDfind, we compare different
values of WDeval. 5e-4 does the best at lower sparsity levels and 1e-4 at higher sparsity levels. Random baselines
have the same WDfind and WDeval as the value in the title.
All weight decay combinations
Fraction of weights pruned

Figure A7: All combinations of weight decay plotted together for comparison
15
Under review as a conference paper at ICLR 2021
A.2.1 Other models and datasets
MiniPlaces, LReva∣ = 0.02
b)	MiniPIaCeS, LRevai = 0.1
a)
>UE⊃υυrattυl
Fraction of weights pruned
MiniPIacesJRfind = 0.1
Fraction of weights pruned
MiniPIaces, LReva∣ = 0.5	d)
Fraction of weights pruned	Fraction of weights pruned
Figure A8: ResNet-50 on MiniPlaCes with LRfind and LReVaI from {0.02, 0.1, 0.5}. In (a), (b), and (c), We
use a constant LReval of 0.02, 0.1, and 0.5 respectively, with each plot showing all three values of LRfind. They
show that LReval = 0.1 is the best. Thus, in (d) we compare the different values of LReVab all with LRfind = 0.1,
to demonstrate that the best LReVaI is 0.5, until very high sparsity levels where 0.1 becomes better. In line with
other results, the best LR for standard training (0.5) is not the best LRfind for pruning.
a)
0.5
AUeJnUUe ⅛ΦH
ResNetl8 on Tiny ImageNet, LReva∣ = 0.5
b)
0.5
ResNetl8 on Tiny ImageNet, LReva∣ = 0.8
AUeJnUUe 4SΦF
b a a a a a a a	o∙ a a a a q∙ q∙ a
Fraction of weights pruned	Fraction of weights pruned
Figure A9: ResNet-18 on Tiny ImageNet with LRfind ∈ {0.05, 0.2, 0.5}, compared using a constant LReval of
0.5 in (a) and 0.8 in (b). For both values of LReval, LRfind of 0.05 and 0.2 are very similar and both perform
significantly better than LRfind = 0.8, despite 0.8 performing the best on the unpruned model. LRfind of 0.8 also
does worse than the random baseline (red).
16
Under review as a conference paper at ICLR 2021
a)
Structured pruning, LReva∣ = 0.1
504s46
b)
Structured pruning, LReva∣ = 0.3
504s46
ʌɔsnuuettF
2 0 6 6
5 5 4 4
ʌɔsnuuetiDl-
J 一 一
λ λ λ
Fraction of weights pruned
Structured pruning, LRfInd = 0∙l
Fraction of weights pruned
Structured pruning, LRfIM = 0.4
d)
52
—LRera∣ = 0-1
—LRera∣ = 0.3
—∙— LRera∣ = 0.4
504s4fi
>VE5vra asəl-
e)
Fraction of weights pruned
Fraction Ofwelghts pruned
Structured pruning, LRflnd = 0.1
5048«
XdeJnQUe asəl-
Fraction of weights pruned
Fraction Ofwelghts pruned
Figure A10: Structured pruning: All combinations of LRfind and LReval
compare different values of LRfind. Bottom (d-f): given a constant LRfind,
a)
Structured pruning, BSeVal = 128
b)
Structured pruning, BSeVal
256
≡S⅛d
c)
= 128
. Top (a-c): given a constant LReval,
compare different values of LReVab
Structured pruning, BSeva∣ = 1024
52
ʌɔsnuue asəl-
-BSm = 128
--BSim = 256
BsM = Io24
4s46
ʌɔsnuue n
Bsmd = 256
BSEd = 1024
504s4644
Xoe-Inuue asəj.
Fraction of weights pruned
Structured pruning, BSfInd = 128
2 0 6 6
5 5 4 4
XdeJnQUe lsəl-
d)
e)
Fraction Ofwelghts pruned
Structured pruning, BSfInd = 256
Fraction of weights pruned
Structured pruning, BSfInd = 1024
50484644
X3sn8e jsəf
Fraction of weights pruned	Fraction of weights pruned	Fraction of weights pruned
Figure A11: Structured pruning: All combinations of BSfind and BSeval. Top (a-c): given a constant BSeval,
compare different values of BSfind. Bottom (d-f): given a constant BSfind, compare different values of BSevai.
Structured pruning, WDeva∣
5e-5
Structured pruning, WDeva∣ = 2.5e-4
d)
52
l50
4S
4β
Fraction of weights pruned
Structured pruning, WDflnd = le-5
WD(IE = le-5
WDfln4 = 5e-5
WDfln4 = 2.5e-⅛

Fraction Ofwelghts pruned
Structured pruning, WDfInd = 5e-5
e)
5e-5
2.51
-→- WDen∣ = le-5
-→- WDmaι = 5e-5
1一WDWSI = 2.5e4
—∙— WDmaι
-→- WDwsl
-WDwsl
Fraction of weights pruned
-WD(IE = le-5
-WDg = 5e-5
WDg = 2.5e4
54
52
Fraction of weights pruned
Structured pruning, WDflM = 2.5e-4
42
40
Fraction of weights pruned	Fraction of weights pruned
Figure A12: Structured pruning: All combinations of WDfind and WDeval. Top (a-c): given a constant WDeval,
compare different values of WDfind. Bottom (d-f): given a constant WDfind, compare different values of WDeval.
17
Under review as a conference paper at ICLR 2021
A.2.2 Other variants
Figure A13: Instability analysis from Frankle et al. (2019b) of various networks. We measure the mode
connectivity between two different networks trained with the same initialization but different SGD noise. Plot-
ted is the test error as we linearly interpolate between the two networks, for (a) 48.8% pruned and (b) 73.8%
pruned. Using LR = 0.05 (both LRfind and LReval, as per a regular LT run) starting from 2 epochs, the network
is stable, thus late rewinding of 2 epochs is reasonable. In contrast, LR = 0.05 without late rewinding (orange)
and a larger LR = 0.2 from 2 epochs (green) both are not stable.
Fraction of weights pruned
Figure A14: Regular lottery tickets, with LRfind = LReval = 0.05 and late rewinding to 2 epochs, compared to
random baselines. The winning ticket does significantly better than all random baselines, including “preserve
mask”, which is a random baseline that uses the same mask as the winning ticket but with reinitialized weights.
This indicates that the network is stable at 2 epochs (Frankle et al., 2019b).
a)
0.5
AUaJnUe ls<υL
Local pruning, LReva∣ = 0.2
b)
Fraction Ofweights pruned
o.o
0.5
Local pruning, LReva∣ = 0.5
AUaJnu0}4s<υl-
Fraction of weights pruned
Figure A15: Local pruning: pruning all layers the same amount. We compare LRfind ∈ {0.1, 0.2, 0.5} using
LReval of 0.2 (a) and 0.5 (b). For both values of LReval, we see that the larger the LRfind, the better the pruned
models perform.
18
Under review as a conference paper at ICLR 2021
Figure A16: Local pruning: pruning all layers the same amount. Similar experiment setup as Figure A15,
except comparing different values of BSfind and WDfind. While large BSfind and small WDfind performed well in
global pruning, we see the opposite for local pruning.
A.2.3 Additional LPR figures
a) io
0.9
b)
6U-UTSE3」4u3E9d
LPR at 20% pruned, comparing learning rate
一 LΛm = 0.5
eəj Mcυv-υα
1.0
0.9
LPR at 20% pruned, comparing batch Sl花
6U-UTSE3」4u3E9d
LPR at 20% pruned, comparing weight decay
1.0
0.9
d)
0.6
酸§
I 0.4
⅛ 0.3
⅛0.2
0.1
0.0
g) 30
0.25
?0.20
不
S 0.15
I 0.10
0.05
000
0	10	20	30	40	50
layer
5u-u-eujajau3t!9d
10	20	30	40
layer
LPR at 89% pruned, comparing batch size
0	10	20	30	40	50
layer
LPR at 99% pruned, comparing learning rate
0	10	20	30	40	50
layer
5u-u-eEa4 4U3E&
LPR at 99% pruned, comparing batch sl≡
0	10	20	30	40	50
layer
LPR at 99% pruned, comparing weight decay
Figure A17: Comparison of LPR of masks found by the three different values of LRfind (a), BSfind (b), and
WDfind (c) at 20, 89 and 99% sparsity (1, 10, and 20 pruning iterations respectively). Similar to Figure 6 at
67% sparsity (5 pruning iterations), small LR, large BS, and small WD values tend to prune more in the earlier
layers.
19
Under review as a conference paper at ICLR 2021
a)
LPR throughout training, LRfind = 0.5
E

Figure A18: Layerwise pruning ratios if you were to prune at different epochs, for LRfind = 0.5 (a) and 0.05 (b),
at the first pruning iteration (starting from an unpruned model). In (a), the LPR change significantly throughout
training, whereas for (b), LPR changes very slightly except for the first layer.
0.00
0
Difference in LPR over training
0.20
0.15
0.10
0.05
tp Sqe CSEl±d^l
20	40	60	80	100
epochs
Figure A19: Distance between LPR if pruned at epoch n vs. actual LPR when pruned at the end, for different
LRsfind. Distance is calculated as the mean absolute difference in pruning ratio for each layer. This is done
at the first pruning iteration (starting from an unpruned model). Smaller LRfind values start with much smaller
distances; all LRfind values start to plateau around epoch 70, which is when the learning rate decays.
20