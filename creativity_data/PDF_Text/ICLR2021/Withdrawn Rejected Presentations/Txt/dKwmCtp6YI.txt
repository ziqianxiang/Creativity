Under review as a conference paper at ICLR 2021
Representation and Bias in
Multilingual NLP:
Insights from Controlled Experiments on
Conditional Language Modeling
Anonymous authors
Paper under double-blind review
Ab stract
Inspired by the phenomenon of performance disparity between languages in ma-
chine translation, we investigate whether and to what extent languages are equally
hard to “conditional-language-model”. Our goal is to improve our understanding
and expectation of the relationship between language, data representation, size,
and performance. We study one-to-one, bilingual conditional language modeling
through a series of systematically controlled experiments with the Transformer and
the 6 languages from the United Nations Parallel Corpus. We examine character,
byte, and word models in 30 language directions and 5 data sizes, and observe
indications suggesting a script bias on the character level, a length bias on the byte
level, and a word bias that gives rise to a hierarchy in performance across languages.
We also identify two types of sample-wise non-monotonicity — while word-based
representations are prone to exhibit Double Descent, length can induce unstable
performance across the size range studied in a novel meta phenomenon which we
term erraticity. By eliminating statistically significant performance disparity on
the character and byte levels by normalizing length and vocabulary in the data, we
show that, in the context of computing with the Transformer, there is no complexity
intrinsic to languages other than that related to their statistical attributes and that
performance disparity is not a necessary condition but a byproduct of word segmen-
tation. Our application of statistical comparisons as a fairness measure also serves
as a novel rigorous method for the intrinsic evaluation of languages, resolving a
decades-long debate on language complexity. While all these quantitative biases
leading to disparity are mitigable through a shallower network, we find room for a
human bias to be reflected upon. We hope our work helps open up new directions
in the area of language and computing that would be fairer and more flexible and
foster a new transdisciplinary perspective for DL-inspired scientific progress.
1	Introduction
With a transdisciplinary approach to explore a space at the intersection of Deep Learning (DL) /
Neural Networks (NNs), language sciences, and language engineering, we report our undertaking
in use-inspired basic research — with an application-related phenomenon as inspiration, we seek
fundamental scientific understanding through empirical experimentation. This is not an application
or machine translation (MT) paper, but one that strives to evaluate and seek new insights on language
in the context of DL with a consideration to contribute to our evaluation, segmentation, and model
interpretation practice in multilingual Natural Language Processing (NLP).
Our inspiration: performance disparity in MT The use case that inspired our investigation is
the disparity of MT results reported in Junczys-Dowmunt et al. (2016). Of the 6 official languages of
the United Nations (UN) — Arabic (AR), English (EN), Spanish (ES), French (FR), Russian (RU),
and Chinese (ZH), results with target languages AR, RU, and ZH seem to be worse than those with
EN/ES/FR, regardless of the algorithm, may it be from phrased-based Statistical MT (SMT/Moses
1
Under review as a conference paper at ICLR 2021
(Koehn et al., 2007)) or Neural MT (NMT).1 The languages have the same amount of line-aligned,
high-quality parallel data available for training, evaluation, and testing. This prompts the question:
are some languages indeed harder to translate from or to?
Problem statement: are all languages equally hard to Conditional-Language-Model (CLM)?
A similar question concerning (monolingual) language modeling (LMing) was posed in Cotterell
et al. (2018) and Mielke et al. (2019) along with the introduction of a method to evaluate LMs with
multiway parallel corpora (multitexts) in information-theoretic terms. To explicitly focus on modeling
the complexities that may or may not be intrinsic to the languages, we study the more fundamental
process of CLMing without performing any translation. This allows us to eliminate confounds
associated with generation and other evaluation metrics. One could think of our effort as estimating
conditional probabilities with the Transformer, with a bilingual setup where perplexity of one target
language (ltrg) is estimated given the parallel data in one source language (lsrc), where lsrc 6= ltrg. We
focus on the very basics and examine the first step in our pipeline — input representation, holding
everything else constant. Instead of measuring absolute cross-entropy scores, we evaluate the relative
differences between languages from across 5 magnitudes of data sizes in 3 different representation
types/levels. We consider bias to be present when performance disparity in our Transformer
models is statistically significant.
1.1	Summary of findings and contributions
In investigating performance disparity as a function of size and data with respect to language and
representation on the Transformer in the context of CLMing, we find:
1.	in a bilingual (one-to-one) CLMing setup, there is neutralization of source language instances,
i.e. there are no statistically significant differences between source language pairs. Only pairs of
target languages differ significantly (see Table 1).
2.	We identify 2 types of sample-wise non-monotonicity on each of the primary representation
levels we studied:
(a)	Double Descent (Belkin et al., 2019; Nakkiran et al., 2020): on the word level, for all
languages, performance at 102 lines is typically better than at 103 before it improves again
at 104 and beyond. This phenomenon can also be observed in character models with ZH
as a target language as well as on the word level with non-neural n-gram LMs;
(b)	erraticity: performance is irregular and exhibits great variance across runs. We find
sequence length to be predictive of this phenomenon. We show that this can be rectified by
data transformation or hyperparameter tuning. In our study, erraticity affects AR and RU
on the byte level where the sequences are too long with UTF-8 encoding and ZH when
decomposed into strokes on the character level.
3.	In eliminating performance disparity through lossless data transformation on the character
and byte levels, we resolve language complexity (§ 4 and App. J). We show that, in the
context of computing with the Transformer, unless word-based methods are used, there is no
linguistic/morphological complexity applicable or necessary. There is no complexity that is
intrinsic to a language aside from its statistical properties. Hardness in modeling is relative
to and bounded by its representation level (representation relativity). On the character and
byte levels, hardness is correlated with statistical properties concerning sequence length and
vocabulary of a language, irrespective of its linguistic typological, phylogenetic, historical, or
geographical profile, and can be eliminated. On the word level, hardness is correlated with
vocabulary, and a complexity hierarchy arises through the manual preprocessing step of word
tokenization. This complexity/disparity effected by word segmentation cannot be eliminated
due to the fundamental qualitative differences in the definition of a “word” being one that
neither holds universally nor is suitable/consistent for fair crosslinguistic comparisons. We
find clarification of this expectation of disparity necessary because more diligent error analyses
need to be afforded instead of simply accepting massively disparate results or inappropriately
attributing under-performance to linguistic reasons.
4.	Representational units of finer granularity can help close the gap in performance disparity.
5.	Bigger/overparameterized models can magnify/exacerbate the effects of differences in data
statistics. Quantitative biases that lead to disparity are mitigable through numerical methods.
1We provide a re-visualization of these grouped in 6 facets by target language in Figure 4 in Appendix A.
2
Under review as a conference paper at ICLR 2021
Outline of the paper In § 2, we define our method and experimental setup. We present our results
and analyses on the primary representations in § 3 and those from secondary set of controls in § 4 in
a progressive manner to ease understanding. Meta analyses on fairness evaluation, non-monotonic
behavior, and discussion on biases are in § 5. Additional related work is in § 6. We refer our readers to
the Appendices for more detailed descriptions/discussions and reports on supplementary experiments.
2	Method and definitions
Controlled experiments as basic research for scientific understanding Using the United Na-
tions Parallel Corpus (Ziemski et al., 2016), the data from which the MT results in Junczys-Dowmunt
et al. (2016) stem, we perform a series of controlled experiments on the Transformer, holding the
hyperparameter settings for all 30 one-to-one language directions from the 6 languages constant. We
control for size (from 102 to 106 lines) and language with respect to representational granularity. We
examine 3 primary representation types — character, byte (UTF-8), and word, and upon encountering
some unusual phenomena, we perform a secondary set of controls with 5 alternate representations —
on the character level: Pinyin and Wubi (ASCII representations for ZH phones and character strokes,
respectively), on the byte level: code page 1256 (for AR) and code page 1251 (for RU), and on the
word level: Byte Pair Encoding (BPE) (Sennrich et al., 2016), an adapted compression algorithm
from Gage (1994). These symbolic variants allow us to manipulate the statistical properties of the
representations, while staying as “faithful” to the language as possible. We adopt this symbolic
data-centric approach because we would like to more directly interpret the confounds, if any, that
make language data different from other data types. We operate on a smaller data size range as
this is more common in traditional domain sciences and one of our higher goals is to bridge an
understanding between language sciences and engineering (the latter being the dominant focus in
NLP). We run statistical tests to identify the strongest correlates of performance and to assess whether
the differences between the mean performance of different groups are indeed significant. We are
concerned not with the absolute scores, but with the relations between scores from different
languages and the generalizations derived therefrom.
Information-theoretic, fair evaluation with multitexts Most sequence-to-sequence models are
optimized using a cross-entropy loss (see Appendix B for definition). Cotterell et al. (2018) propose
to use “renormalized” perplexity (PP) to evaluate LMs fairly using the total number of bits divided
by some constant. In our case, we choose instead a simpler method of using an “unnormalized” PP,
directly using the total number of bits needed to encode the development (dev) set, which has a
constant size of 3,077 lines per language.
Disparity/Inequality In the context of our CLMing experiments, we consider there to be “dis-
parity” or “inequality” between languages l1 and l2 if there are significant differences between the
performance distributions of these two languages with respect to each representation. Here, by
performance we mean the number of bits required to encode the held-out data using a trained CLM.
With 30 directions, there are 15 pairs of source languages (lsrc1, lsrc2) and 15 pairs of target languages
(ltrg1, ltrg2) possible. To assess whether the differences are significant, we perform unpaired two-sided
significance tests with the null hypothesis that the score distributions for the two languages are not
different. Upon testing for normality with the Shapiro-Wilk test (Shapiro & Wilk, 1965; Royston,
1995), we use the parametric unpaired two-sample Welch’s t-test (Welch, 1947) (when normal) or the
non-parametric unpaired Wilcoxon test (Wilcoxon, 1945) (when not normal) for the comparisons.
We use the implementation in R (R Core Team, 2014) for these 3 tests. To account for the multiple
comparisons we are performing, we correct all p-values using Bonferroni’s correction (Benjamini
& Heller, 2008; Dror et al., 2017) and follow Holm’s procedure2 (Holm, 1979; Dror et al., 2017) to
identify the pairs of l1 and l2 with significant differences after correction. We report all 3 levels of
significance (α ≤ 0.05, 0.01, 0.001) for a more comprehensive evaluation.
Experimental setup The systematic, identical treatment we give to our data is described as follows
with further preprocessing and hyperparameter details in Appendices B and C, respectively. The
distinctive point of our experiment is that the training regime is the same for all (intuition in App. O.1).
2using implementation from https://github.com/rtmdrr/replicability-analysis-NLP
3
Under review as a conference paper at ICLR 2021
After filtering length to 300 characters maximum per line in parallel for the 6 languages, we made 3
subsets of the data with 1 million lines each — one having lines in the order of the original corpus
(dataset A) and two other randomly sampled (without replacement) from the full corpus (datasets
B & C). Lines in all datasets are extracted in parallel and remain fully aligned for the 6 languages.
For each run and each representation, there are 30 pairwise directions (i.e. one lsrc to one ltrg) that
result from the 6 languages. We trained all 150 (for 5 sizes) 6-layer Transformer models for each run
using the SOCKEYE Toolkit (Hieber et al., 2018). We optimize using PP and use early stopping if no
PP improvement occurs after 3 checkpoints up to 50 epochs maximum, taking the best checkpoint.
Characters and bytes are supposed to mitigate the out-of-vocabulary (OOV) problem on the word
level. In order to assess the effect of modeling with finer granularity more precisely, all vocabulary
items appearing once in the train set are accounted for (i.e. full vocabulary on train, as in Gerz
et al. (2018a;b)). But we allow our system to categorize all unknown items in the dev set to be
unknown (UNK) so to measure OOVs (open vocabulary on dev (Jurafsky & Martin, 2009)). To
identify correlates of performance, we perform Spearman’s correlation (Spearman, 1904) with some
basic statistical properties of the data (e.g. length, vocabulary size (|V |), type-token-ratio, OOV
rate) as metrics — a complete list thereof is provided in Appendix F. For each of the 3 primary
representations — character, byte, and word, we performed 5 runs total in 5 sizes (102-106 lines)
(runs A0, B0, C0, A1, & A2) and 7 more runs in 4 sizes (102-105 lines) (A3-7, B1, & C1), also
controlling for seeds. For the alternate/secondary representations, we ran 3 runs each in 5 sizes
(102-106 lines) (A0, B0, & C0).
3	Experimental results of primary representations
Subfigures 1a, 1b, and 1c present the mean results across 12 runs of the 3 primary representations —
character, byte, and word, respectively. The x-axis represents data size in number of lines and y-axis
the total conditional cross-entropy, measured in bits (Eq. 1 in Appendix B). Each line connects 5 data
points corresponding to the number of bits the CLMs (trained with training data of 102, 103, 104, 105,
and 106 lines) need to encode the target language dev set given the corresponding text in the source
language. These are the same data in the same 30 language directions and 5 sizes with the same
training regime, just preprocessed/segmented differently. This confirms representation relativity —
languages (or any objects being modeled) need to be evaluated relative to their representation. “One
size does not fit all” (Durrani et al., 2019), our conventional way of referring to “language” (as a
socio-cultural product or with traditional word-based approaches, or even for most multilingual tasks
and competitions) is too coarse-grained (see also Fisch et al. (2019) and Ponti et al. (2020)).
Subfigures 1d, 1e, and 1f display the corresponding information sorted into facets by target language,
source languages represented as line types. Through these we see more clearly that results can be
grouped rather neatly by target language (cf. figures sorted by source language in Appendix H) —
as implicit in the Transformer’s architecture, the decoder is unaware of the source language in the
encoder. As shown in Table 1 in § 5 summarizing the number of source and target language pairs
with significant differences, there are no significant differences across any source language pairs.
The Transformer neutralizes source language instances. This could explain why transfer learning or
multilingual/zero-shot translation (Johnson et al., 2017) is possible at all on a conceptual level.
In general, for character and byte models, most language directions do seem to converge at 104
lines to similar values across all target languages, with few notable exceptions. There are some
fluctuations past 104, indicating further tuning of hyperparameters would be beneficial due to our
present setting possibly working most favorably at 104. On the character level, target language ZH
(ZHtrg ) shows a different learning pattern throughout. And on the byte level, ARtrg and RUtrg
display non-monotonic and unstable behavior, which we refer to as erratic. Word models exhibit
Double Descent across the board (note the spike at 103), but overall, difficult/easy languages stay
consistent, with AR and RU being the hardest, followed by ES and FR, then EN and ZH. A practical
takeaway from this set of experiments: in order to obtain more robust training results, use bytes
for ZH (as suggested in Li et al. (2019a)) and characters for AR and RU (e.g. Lee et al. (2017)) —
also if one wanted to avoid any “class” problems in performance disparity with words. Performance
disparity for these representations is reported in Table 1 under “CHAR”, “BYTE”, and “WORD”.
Do note, however, that the intrinsic performance of ZH with word segmentation is not particularly
subpar. But this often does not correlate with its poorer downstream tasks results (recall results
from Junczys-Dowmunt et al. (2016)). Since the notion of word in ZH is highly contested and
4
Under review as a conference paper at ICLR 2021
2500000-
(a) CHAR
2500000-
25000B-
2000000-
(c) WORD
-三三E
(b) BYTE
TRG
—AR
一.EN
—ES
--FR
--RJ
—ZH
SRC
AR
a
æ
FR
RJ
ZH

SRC
— AR
—EN
——ES
——FR
—RU
-ZH
(g)
TRG
——AR
--EN
—ES
--FR
,, ,■ RU
—ZH
(h)
(d) CHAR by target	(e) BYTE by target	(f) WORD by target
Figure 1:	Number of bits (the lower the better) as a function of data size plotted for all 30 directions.
Subfigures 1d, 1e, and 1f depict the corresponding information as in 1a, 1b, and 1c (showing mean across
12 runs), respectively, but sorted in 6 facets by target language and with error bars. Legend in Subfigure 1g
shows the correspondence between colors and source languages, in Subfigure 1h between line types and target
languages. (These figures are also shown enlarged in Appendix G.)
ambiguous — 1) it is often aimed to align with that in other languages so to accommodate manual
feature engineering and academic theories, 2) there is great variation among different conventions, 3)
native ZH speakers identify characters as words — there are reasons to rethink this procedure now
that fairer and language-independent processing in finer granularity is possible (cf. Li et al. (2019b)
as well as Duanmu (2017) for a summary on the contested nature of wordhood in ZH). A more native
analysis of ZH, despite being considered a high-resource language, has not yet been recognized in
NLP.
4 Understanding the phenomena with alternate representations
To understand why some languages show different results than others, we carried out a secondary
set of control experiments with representations targeting the problematic statistical properties of the
corresponding target languages. (An extended version of this section is provided in Appendix P.)
Character level We reduced the high |V | in ZH with representations in ASCII characters — Pinyin
and Wubi. The former is a romanization of ZH characters based on their pronunciations and the latter
an input algorithm that decomposes character-internal information into stroke shape and ordering and
matches these to 5 classes of radicals (Lunde, 2008). We replaced the ZH data in these formats only on
the target side and reran the experiments involving ZHtrg on the character level. Results in Figure 2
and Table 1 show that the elimination of disparity on character level is possible if ZH is represented
5
Under review as a conference paper at ICLR 2021
!
number of lies
TRG
-AR
—∙ EN
-ES
--FR
-RU
-ZH
SRC
AR
EN
ES
FR
RU
ZH
ιe+∣3	ιe+∣5
number of lines
TRG
一AR
一.EN
—ES
--FR
--RJ
—ZH
SRC
AR
a
—æ
FR
—RJ
ZH
2"""一
e+"3	ι.+o5
n umber of lines
(c) Pinyin
TRG
-AR
-EN
-ES
--FR
■— RJ
—ZH
SRC
AR
a
æ
FR
RJ
ZH
(d) Pinyin by target
S "，，，，0，
TRG
-AR
-EN
-ES
--FR
--RU
-ZH
SRC
AR
EN
—ES
FR
—RU
ZH
(a) Wubi	(b) Wubi by target

Figure 2:	Character-level remedies for ZH: Wubi vs. Pinyin.
200””
!
number of lies
TRG
-AR
—■ EN
-ES
--FR
-RU
-ZH
SRC
AR
EN
ES
FR
RU
ZH
20"”
2"""一
(a) Code page 1256 & 1251
200"”一
(b) Code page by target
*+b	1e+θ5	1e+θ03	1e+θ5	1e+θ03	1e+05
number of lines
TRG
一AR
一.EN
—ES
--FR
--RJ
—ZH
SRC
AR
一a
æ
—FR
RJ
ZH
(c) BPE
TRG
-AR
—EN
-ES
--FR
■— RJ
—ZH
SRC
AR
a
æ
FR
RJ
ZH
1e+θ3 1e+θ5	le+θ3 *+θ5	*+b 1e+θ5
number of ines
(d) BPE by target


Figure 3:	Byte-level (Subfigures 3a & 3b) remedies with code page 1256 for target AR and 1251 for
target RU, and word-level (Subfigures 3c & 3d) remedy with BPE for all languages.
through Pinyin (transliteration), as in Subfigure 2c. But models with ZH logographic scripts display
a behaviorial tendency unlike those with other (phonetic) alphabetic scripts (Subfigure 2a). Work
published thus far using Wubi with the Transformer seems to have needed some form of architectural
modification (Gao et al., 2020) or a different architecture altogether (Nikolov et al., 2018; Zhang
et al., 2019), suggesting a possible script bias (to be further discussed in § 5 under “Basis for biases”).
Byte level Length is the most salient statistical attribute that makes AR and RU outliers. To shorten
their sequence lengths, we tested with alternate encodings on ARtrg and RUtrg — code page 1256
and 1251, which provide 1-byte encodings specific to AR and RU, respectively. Results are shown
in Subfigures 3a and 3b. Not only is erraticity resolved, the number of 15 possible target language
pairs with significant differences reduces from 8 with the UTF-8 byte representation to 0 (Table 1
under “ARRUt”), indicating that we eliminated disparity with this optimization heuristic. Since
our heuristic is a lossless and reversible transform, it shows that a complexity that is intrinsic and
necessary in language3 does not exist in computing, however diverse they may be, as our 6 are,
from the conventional linguistic typological, phylogenetic, historical, or geographical perspectives.
Please refer to Appendix J for our discussion on language complexity.
Word level The main difference between word and character/byte models is length not being a top
contributing factor correlating with performance, but instead |V | is. This is understandable as word
segmentation neutralizes sequence lengths. To remedy the OOV problem, we use BPE, which learns
a fixed vocabulary of variable-length character sequences (on word level, as it presupposes word
3aside from its statistical properties related to length and vocabulary. “Language” here refers to language
represented through all representations.
6
Under review as a conference paper at ICLR 2021
Table 1: Number of language pairs out of 15 with significant differences, with respective p-values. ARRUt
refers to AR & RU being optimized only on the target side; whereas ARRUs,t denotes optimization on both
source and target sides (relevant for directions AR-RU and RU-AR).
	p-value	CHAR		Pinyin		Wubi		BYTE		ARRUt		ARRUs,t		WORD		BPE	
		src	trg	src	trg	src	trg	src	trg	src	trg	src	trg	src	trg	src	trg
	0.05	0	7	0	4	0	8	0	9	0	4	0	4	0	11	0	10
	0.01	0	5	0	2	0	6	0	8	0	3	0	4	0	8	0	8
	0.001	0	3	0	0	0	5	0	8	0	0	0	2	0	8	0	7
segmentation) from the training data. It is more fine-grained than word segmentation and is known
for its capability to model subword units for morphologically complex languages (e.g. AR and RU).
We use the same vocabulary of 30,000 as specified in Junczys-Dowmunt et al. (2016). This reduced
our averaged OOV token rate by 89-100% across the 5 sizes. The number of language pairs with
significant differences reduced to 7 from 8 for word models, showing how finer-grained modeling
has a positive effect on closing the disparity gap.
5	Meta-results, analysis, and discussion
Performance disparity Table 1 lists the number of language pairs with significant differences
under the representations studied. Considering how it is possible for our character and byte models
to effect no performance disparity for the same languages on the same data, this indicates that
disparity is not a necessary condition. In fact, the customary expectation that languages ought to
perform differently stems from our word segmentation practice. Furthermore, the order of AR/RU
> ES/FR > EN/ZH (Figure 1c) resembles the idea of morphological complexity. Considering there
are character-internal meaningful units in languages with logographic script such as ZH (cf. Zhang
& Komachi (2018)) that are rarely captured, studied, or referred to as “morphemes”, this goes to
show that linguistic morphology, along with its complexity, as is practiced today4 and that which
has occurred in the NLP discourse thus far, has only been relevant on and is bounded to the “word”
level. The definition of word, however, has been recognized as problematic for a very long time in
the language sciences (see Haspelmath (2011) and references therein from the past century). Since
the conventional notion of word, which has been centered on English and languages with alphabetic
scripts, has a negative impact on languages both morphologically rich (see Minkov et al. (2007),
Seddah et al. (2010), inter alia), AR and RU in our case, as well as morphologically “frugal” (Koehn,
2005), as in ZH, finer-grained modeling with characters and bytes (or n-gram variants/pieces thereof)
is indeed a more sensible option and enables a greater variety of languages to be handled with more
simplicity, fairness, independence, and flexibility.
While the lack of significant differences between pairs of source languages would signify neutraliza-
tion of source language instances, it does not mean that source languages have no effect on target.
For our byte solutions with code pages, we experimented also with source side optimization in the
directions that involve AR/RU as source. This affected the distribution of the disparity results for that
representation — with 2 pairs being significantly different (see Table 1 under “ARRUs,t”). We defer
further investigation on the nature of source language neutralization to future work.
Sample-wise Double Descent (DD) Sample-wise non-monotonicity/DD (Nakkiran et al., 2020)
denotes a degradation followed by an improvement in performance with increasing data size. We
notice word models and character models with ZHtrg, i.e. models with high target |V |, are prone
to exhibit a spike at 103. A common pattern for these is the ratio of target training token count
to number of parameters falls into O(10-4) for 102 lines, O(10-3) at 103, O(10-2) at 104, and
O(10-1) for 105 lines and so on. But for more atomic units such as alphabetic (not logographic)
characters (may it be Latin, Cyrillic, or Abjad) and for bytes, this progression instead begins at
O(10-3) at 102 lines. Instead of thinking this spike of 103 as irregular, we may instead want to
4But there are no reasons why linguistics or linguistic typology cannot encompass a statistical science of
language beyond/without “words”, or with continuous representations of characters and bytes. In fact, that could
complement the needs of language engineering and the NNs/DL/ML communities better.
7
Under review as a conference paper at ICLR 2021
think of this learning curve as shifted by 1 order of magnitude to the right for characters and bytes
and/or the performance at 102 lines for words and ZH-characters due to being overparameterized
and hence abnormal. This would also fit in with the findings by Belkin et al. (2019) and Nakkiran
et al. (2020) attributing DD to overparameterization. If we could use this ratio and logic of higher |V |
to automatically detect “non-atomic” units, ones that can be further decomposed, this observation
could potentially be beneficial for advancing other sciences, e.g. biology. From a cognitive modeling
perspective, the similarity in behavior of ZH characters and words of other languages can affirm the
interpretation of wordhood for those ZH speakers who identify ZH characters as words (see also
last paragraph in § 3 and Appendix J). While almost all work attribute DD to algorithmic reasons,
concurrent work by Chen et al. (2020) corroborates our observation and confirms that DD arises due
to “the interaction between the properties of the data and the inductive biases of learning algorithms”.
Other related work on DD and its more recent development can also be found in their work.
We performed additional experiments testing our setting on the datasets used by the Nakkiran et al.
(2020) and testing our data on a non-neural LM. Results support our findings and are provided in
Appendix K. Number of model parameters can be found in Appendix L.
Erraticity We observe another type of sample-wise non-monotonicity, one that signals irregular
and unstable performance across data sizes and runs. Within one run, erraticity can be observed
directly as changes in direction on the y-axis. Across runs, large variance can be observed, even
with the same dataset (see Figure 18 in Appendix M). Erraticity can also be observed indirectly
through a negative correlation between data size and performance. Many work on length bias in
NMT have focused on solutions related to search, e.g. Murray & Chiang (2018). Our experiments
show that a kind of length bias can surface already with CLMing, without generation taking place. If
the connection between erraticity and length bias can indeed be drawn, it could strengthen the case
for global conditioning (Sountsov & Sarawagi, 2016). (See Appendix M for more discussion and
results.)
Script bias, erraticity, word bias — are these necessary conditions? To assess whether the
observed phenomena are particular to this one setting, we performed one run with dataset A in 4
sizes with the primary representations on 1-layer Transformers (see Appendix N). We observed
no significant disparity across the board. It shows that larger/overparameterized models can
magnify/exacerbate the differences in the data statistics. That hyperparameter tuning — in this
case, through the reduction of the number of layers — can mitigate effects from data statistics
is, to the best of our knowledge, a novel insight, suggesting also that a general expectation of
monotonic development as data size increases can indeed be held. Our other findings remain
consistent (representational relativity, source language neutralization, and DD on word level).
Bases for biases Recall in § 1, we “consider bias to be present when performance disparity in
our Transformer models is statistically significant”. As shown in our data statistics and analysis
(Appendices D and P respectively), script bias, length bias wrt erraticity in CLMing, and word bias
are all evident in the vocabulary and length information in the data statistics. Hence these disparities
in performance are really a result of the Transformer being able to model these differences in data at
such a magnitude that the differences are statistically significant. The meta phenomenon of erraticity,
however, warrants an additional consideration indicative of the empirical limits of our compute (cf.
Xu et al. (2020)), even when the non-monotonicity is not observed during the training of each model.
In eliminating performance disparity in character and byte models by normalizing vocabulary and
length statistics in the data, we demonstrated that performance disparity as expected from the
morphological complexity hierarchy is due to word tokenization, not intrinsic or necessary in language.
This is the word bias. Qualitative issues in the concept of word will persist and make crosslinguistic
comparison involving “words” unfair even if one were to be able to find a quantitative solution to
mitigate the OOV issue, the bottleneck in word-based processing. We humans have a choice in how
we see/process languages. That some might still prefer to continue with a crosslinguistic comparison
with “words” and exert the superiority of “word” tokenization speaks for a view that is centered on
“privileged” languages — in that case, word bias is a human bias.
And, in eliminating performance disparity across the board with our one-layer models, we show that
all quantitative differences in data statistics between languages can also be modeled in a “zoomed-
8
Under review as a conference paper at ICLR 2021
out”/“desensitized” mode, suggesting that while languages can be perceived as being fundamentally
different in different ways in different granularities, they can also be viewed as fundamentally similar.
6	Additional related work
Similar to our work in testing for hardness are Cotterell et al. (2018), Mielke et al. (2019), and
Bugliarello et al. (2020). The first two studied (monolingual) LMs — the former tested on the Europarl
languages (Koehn, 2005) with n-gram and character models and concluded that morphological
complexity was the culprit to hardness, the latter studied 62 languages of the Bible corpus (Mayer
& Cysouw, 2014) in addition and refuted the relevance of linguistic features in hardness based
on character and BPE models on both corpora in word-tokenized form. Bugliarello et al. (2020)
compared translation results of the Europarl languages with BPEs at one data size and concluded
that it is easier to translate out of EN than into it, statistical significance was, however, not assessed.
In contrast, we ablated away the confound of generation and studied CLMing with controls with
a broader range of languages with more diverse statistical profiles in 3 granularities and up to 5
orders of magnitude in data size. That basic data statistics are the driver of success in performance
in multilingual modeling has so far only been explicitly argued for in Mielke et al. (2019). We go
beyond their work in monolingual LMs to study CLMs and evaluate also in relation to data size,
representational granularity, and quantitative and qualitative fairness.
Bender (2009) advocated the relevance of linguistic typology for the design of language-independent
NLP systems based on crosslinguistic differences in word-based structural notions, such as parts
of speech. Ponti et al. (2019) found typological information to be beneficial in the few-shot setting
on the character level for 77 languages with Latin scripts. But no multilingual work has thus far
explicitly examined the relation between linguistic typology and the statistical properties of the data,
involving languages with diverse statistical profiles in different granularities.
As obtaining training data is often the most difficult part ofan NLP or Machine Learning (ML) project,
Johnson et al. (2018) introduced an extrapolation methodology to directly model the relation between
data size and performance. Our work can be viewed as one preliminary step towards this goal. To the
best of our knowledge, there has been no prior work on demonstrating the neutralization of source
language instances through statistical comparisons, a numerical analysis on DD for sequence-to-
sequence models, the meta phenomenon of a sample-wise non-monotonicity (erraticity) being related
to length, or the connection between effects of data statistics and modification in architectural depth.
7	Conclusion
Summary We performed a novel, rigorous relational assessment of performance disparity across
different languages, representations, and data sizes in CLMing with the Transformer. Different dispar-
ity patterns were observed on different representation types (character, byte, and word), which can be
traced back to the data statistics. The disparity pattern reflected on the word level corresponds to the
morphological complexity hierarchy, reminding us that the definition of morphology is predicated on
the notion of word and indicating how morphological complexity can be modeled by the Transformer
simply through word segmentation. As we were able to eliminate disparity on the same data on
the character and byte levels by normalizing length and vocabulary, we showed that morphological
complexity is not a necessary concept but one that results from word segmentation and is bounded to
the word level, orthogonal to the performance of character or byte models. Representational units of
finer granularity were shown to help eliminate performance disparity though at the cost of longer
sequence length, which can have a negative impact on robustness. In addition, we found all word
models and character models with ZHtrg to behave similarly in their being prone to exhibit a peak
(as sample-wise DD) around 103 lines in our setting. While bigger/overparameterized models can
magnify the effect of data statistics, exacerbating the disparity, we found a decrease in model depth
can eliminate these quantitative biases, leaving only the qualitative aspect of “word” and the necessity
of word segmentation in question.
Outlook Machine learning has enabled greater diversity in NLP (Joshi et al., 2020). Fairness, in
the elimination of disparity, does not require big data. This paper made a pioneering attempt to bridge
research in DL/NNs, language sciences, and language engineering through a data-centric perspective.
9
Under review as a conference paper at ICLR 2021
We believe a statistical science for NLP as a data science can well complement algorithmic analyses
with an empirical view contributing to a more generalizable pool of knowledge for NNs/DL/ML. A
more comprehensive study not only can lead us to new scientific frontiers, but also better design and
evaluation, benefitting the development of a more general, diverse and inclusive Artificial Intelligence.
References
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of
Sciences,116(32):15849-15854,2019. ISSN0027-8424. doi: 10.1073∕pnas.1903070116. URL
https://www.pnas.org/content/116/32/15849.
E. M. Bender. Linguistic Fundamentals for Natural Language Processing: 100 Essentials from
Morphology and Syntax. 2013.
Emily M. Bender. Linguistically naive != language independent: Why NLP needs linguistic ty-
pology. In Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics
and Computational Linguistics: Virtuous, Vicious or Vacuous?, pp. 26-32, Athens, Greece,
March 2009. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/W09-0106.
Yoav Benjamini and Ruth Heller. Screening for partial conjunction hypotheses. Biometrics, 64(4):
1215-1222, 2008. ISSN 0006341X, 15410420. URL http://www.jstor.org/stable/
25502204.
Christian Bentz, Tatyana Ruzsics, Alexander Koplenig, and Tanja Samardzic. A comparison between
morphological complexity measures: Typological data vs. language corpora. In Proceedings
of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC), pp. 142-
153, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL https:
//www.aclweb.org/anthology/W16-4117.
Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki
Okazaki. It’s easier to translate out of English than into it: Measuring neural translation difficulty
by cross-mutual information. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 1640-1649, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.149. URL https://www.aclweb.org/
anthology/2020.acl-main.149.
Mauro Cettolo, Christian Girardi, and Marcello Federico. Wit3: Web inventory of transcribed and
translated talks. In Proceedings of the 16th Conference of the European Association for Machine
Translation (EAMT), pp. 261-268, Trento, Italy, May 2012.
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve, 2020.
Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for language
modeling. Computer Speech Language, 13(4):359 - 394, 1999. ISSN 0885-2308. doi: https:
//doi.org/10.1006/csla.1999.0128. URL http://www.sciencedirect.com/science/
article/pii/S0885230899901286.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. Revisiting
character-based neural machine translation with capacity and compression. In Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4295-4305.
Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/
D18-1461.
10
Under review as a conference paper at ICLR 2021
KyUnghyUn Cho, Bart van Merrienboer, Dzmitry Bahdanau, and YoshUa Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103-111, Doha, Qatar,
October 2014. Association for CompUtational LingUistics. doi: 10.3115/v1/W14-4012. URL
https://www.aclweb.org/anthology/W14-4012.
Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. Are all langUages eqUally hard
to langUage-model? In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 2
(Short Papers), pp. 536-541, New Orleans, LoUisiana, JUne 2018. Association for CompUtational
LingUistics. doi: 10.18653/v1/N18- 2085. URL https://www.aclweb.org/anthology/
N18-2085.
Rotem Dror, Gili BaUmer, Marina Bogomolov, and Roi Reichart. Replicability analysis for natUral
langUage processing: Testing significance with mUltiple datasets. Transactions of the Association
for Computational Linguistics, 5:471-486, 2017. URL http://aclweb.org/anthology/
Q17-1033.
San DUanmU. Word and wordhood, modern. In Rint Sybesma (ed.), Encyclopedia of Chinese
Language and Linguistics, pp. 543-549. Brill, 2017.
Nadir DUrrani, Fahim Dalvi, Hassan Sajjad, Yonatan Belinkov, and Preslav Nakov. One size does
not fit all: Comparing NMT representations of different granUlarities. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1504-1516, Minneapolis,
Minnesota, JUne 2019. Association for CompUtational LingUistics. doi: 10.18653/v1/N19-1154.
URL https://www.aclweb.org/anthology/N19- 1154.
Adam Fisch, Jiang GUo, and Regina Barzilay. Working hard or hardly working: Challenges of
integrating typology into neUral dependency parsers. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 5713-5719, Hong Kong, China, November
2019. Association for CompUtational LingUistics. doi: 10.18653/v1/D19-1574. URL https:
//www.aclweb.org/anthology/D19-1574.
Philip Gage. A new algorithm for data compression. C Users J., 12(2):23-38, FebrUary 1994. ISSN
0898-9788. URL http://dl.acm.org/citation.cfm?id=177910.177914.
Yingqiang Gao, Nikola I. Nikolov, YUhUang HU, and Richard H.R. Hahnloser. Character-level
translation with self-attention. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 1591-1604, Online, JUly 2020. Association for CompUta-
tional LingUistics. doi: 10.18653/v1/2020.acl-main.145. URL https://www.aclweb.org/
anthology/2020.acl-main.145.
Martin Gellerstam. Translationese in Swedish novels translated from English. In Lars Wollin and
Hans LindqUist (eds.), Translation Studies in Scandinavia, pp. 88-95. CWK GleerUp, 1986.
Daniela Gerz, Ivan Vulic, Edoardo Ponti, Jason Naradowsky, Roi Reichart, and Anna Korhonen.
LangUage modeling for morphologically rich langUages: Character-aware modeling for word-level
prediction. Transactions of the Association for Computational Linguistics, 6:451-465, 2018a. doi:
10.1162/tacl_a_00032. URL https://www.aclweb.org/anthology/Q18-1032.
Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, Roi Reichart, and Anna Korhonen. On the relation
between linguistic typology and (limitations of) multilingual language modeling. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 316-327,
Brussels, Belgium, October-November 2018b. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/D18-1029.
Martin Haspelmath. The indeterminacy of word segmentation and the nature of morphology and
syntax. Folia Linguistica, 2011.
11
Under review as a conference paper at ICLR 2021
Kenneth Heafield. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,pp.187-197, Edinburgh, Scotland, July 2011. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
W11-2123.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. Scalable modified
Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pp. 690-696, Sofia, Bulgaria,
August 2013. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/P13-2121.
Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, and
Matt Post. The Sockeye neural machine translation toolkit at AMTA 2018. In Proceedings
of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1:
Research Papers), pp. 200-207. Association for Machine Translation in the Americas, 2018. URL
http://aclweb.org/anthology/W18-1820.
Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of
Statistics, 6(2):65-70, 1979. ISSN 03036898, 14679469. URL http://www.jstor.org/
stable/4615733.
Mark Johnson, Peter Anderson, Mark Dras, and Mark Steedman. Predicting accuracy on large
datasets from smaller pilot data. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pp. 450-455. Association for Computational
Linguistics, 2018. URL http://aclweb.org/anthology/P18-2072.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viegas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey
Dean. Google’s multilingual neural machine translation system: Enabling zero-shot transla-
tion. Transactions of the Association for Computational Linguistics, 5:339-351, 2017. doi:
10.1162/tacl_a_00065. URL https://www.aclweb.org/anthology/Q17-1024.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state
and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 6282-6293, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL
https://www.aclweb.org/anthology/2020.acl-main.560.
Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. Is neural machine translation ready
for deployment? A case study on 30 translation directions. In IWSLT 2016, Seattle, October
2016. URL https://www.microsoft.com/en-us/research/publication/
neural-machine-translation-ready-deployment-case-study-30-\
translation-directions/.
Daniel Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition. Pearson Prentice Hall,
second edition, 2009.
Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In 1995
International Conference on Acoustics, Speech, and Signal Processing, volume 1, pp. 181-184.
IEEE, 1995.
Philipp Koehn. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit, pp. 79-86, Phuket, Thailand, 2005. AAMT,
AAMT. URL http://mt-archive.info/MTS- 2005-Koehn.pdf.
Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings
of the First Workshop on Neural Machine Translation, pp. 28-39. Association for Computational
Linguistics, 2017. doi: 10.18653/v1/W17-3204. URL http://aclweb.org/anthology/
W17-3204.
12
Under review as a conference paper at ICLR 2021
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp. 177-180.
Association for Computational Linguistics, 2007. URL http://aclweb.org/anthology/
P07-2045.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation
without explicit segmentation. Transactions of the Association for Computational Linguistics, 5:
365-378, December 2017. doi: 10.1162/tacl_a_00067. URL https://www.aclweb.org/
anthology/Q17-1026.
Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, and William Chan. Bytes are all you need: End-
to-end multilingual speech recognition and synthesis with bytes. In ICASSP 2019-2019 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5621-5625.
IEEE, 2019a.
Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. Is word
segmentation necessary for deep learning of Chinese representations? In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 3242-3252, Florence,
Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1314. URL
https://www.aclweb.org/anthology/P19-1314.
Ken Lunde. CJKV Information Processing. O’Reilly Media, Inc., 2nd edition, 2008. ISBN
0596514476, 9780596514471.
Thomas Mayer and Michael Cysouw. Creating a massively parallel Bible corpus. In Proceedings
of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), pp.
3158-3163, Reykjavik, Iceland, May 2014. European Languages Resources Association (ELRA).
Sabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner. What kind
of language is hard to language-model? In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp. 4975-4989, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-1491. URL https://www.aclweb.
org/anthology/P19-1491.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meeting of the Association of Computational
Linguistics, pp. 128-135, Prague, Czech Republic, June 2007. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/P07-1017.
Kenton Murray and David Chiang. Correcting length bias in neural machine translation. In Proceed-
ings of the Third Conference on Machine Translation: Research Papers, pp. 212-223, Belgium,
Brussels, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6322.
URL https://www.aclweb.org/anthology/W18- 6322.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
Nikola Nikolov, Yuhuang Hu, Mi Xue Tan, and Richard H.R. Hahnloser. Character-level Chinese-
English translation through ASCII encoding. In Proceedings of the Third Conference on Machine
Translation: Research Papers, pp. 10-16, Belgium, Brussels, October 2018. Association for
Computational Linguistics. URL https://www.aclweb.org/anthology/W18- 6302.
Franz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19-51, 2003. doi: 10.1162/089120103321337421. URL
https://www.aclweb.org/anthology/J03-1002.
13
Under review as a conference paper at ICLR 2021
M Opper, W Kinzel, J Kleinz, and R Nehl. On the ability of the optimal perceptron to gen-
eralise. Journal of Physics A: Mathematical and General, 23(11):L581-L586, jun 1990.
doi: 10.1088/0305-4470/23/11/012. URL https://doi.org/10.1088%2F0305-4470%
2F23%2F11%2F012.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019.
Edoardo Maria Ponti, Ivan VuliC, Ryan CotterelL Roi Reichart, and Anna Korhonen. Towards
zero-shot language modeling. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pp. 2900-2910, Hong Kong, China, November 2019. Association
for Computational Linguistics. doi: 10.18653/v1/D19-1288. URL https://www.aclweb.
org/anthology/D19-1288.
Edoardo Maria Ponti, Helen O,Horan, Yevgeni Berzak, Ivan Vulic, Roi Reichart, Thierry Poibeau,
Ekaterina Shutova, and Anna Korhonen. Modeling language variation and universals: A survey on
typological linguistics for natural language processing, 2020.
R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria, 2014. URL http://www.R-project.org/.
Patrick Royston. Remark as r94: A remark on algorithm as 181: The w-test for normality. Journal of
the Royal Statistical Society. Series C (Applied Statistics), 44(4):547-551, 1995. ISSN 00359254,
14679876. URL http://www.jstor.org/stable/2986146.
Djame Seddah, Sandra Koebler, and Reut Tsarfaty (eds.). Proceedings of the NAACL HLT 2010
First Workshop on Statistical Parsing of Morphologically-Rich Languages, Los Angeles, CA, USA,
June 2010. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/W10-1400.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.
org/anthology/P16-1162.
S. S. Shapiro and M. B. Wilk. An analysis of variance test for normality (complete samples)!.
Biometrika, 52(3-4):591-611, 12 1965. ISSN 0006-3444. doi: 10.1093/biomet/52.3-4.591. URL
https://doi.org/10.1093/biomet/52.3-4.591.
Pavel Sountsov and Sunita Sarawagi. Length bias in encoder decoder models and a case for global
conditioning. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 1516-1525, Austin, Texas, November 2016. Association for Computational
Linguistics. doi: 10.18653/v1/D16-1158.
C. Spearman. The proof and measurement of association between two things. The American
Journal of Psychology, 15(1):72-101, 1904. ISSN 00029556. URL http://www.jstor.
org/stable/1412159.
Felix Stahlberg and Bill Byrne. On NMT search errors and model errors: Cat got your tongue? In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.
3356-3362, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1331. URL https://www.aclweb.org/anthology/D19-1331.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra Kuebler, Yannick Versley, Marie Candito,
Jennifer Foster, Ines Rehbein, and Lamia Tounsi. Statistical parsing of morphologically rich
languages (SPMRL) what, how and whither. In Proceedings of the NAACL HLT 2010 First
Workshop on Statistical Parsing of Morphologically-Rich Languages, pp. 1-12, Los Angeles, CA,
USA, June 2010. Association for Computational Linguistics. URL https://www.aclweb.
org/anthology/W10-1401.
14
Under review as a conference paper at ICLR 2021
ReUt Tsarfaty, Djame Seddah, Sandra Kubler, and Joakim Nivre. Parsing morphologically rich
languages: Introduction to the special issue. Computational Linguistics, 39(1):15-22, 2013. doi:
10.1162/COLI_a_00133. URL https://www.aclweb.org/anthology/J13-1003.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
E ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017. URL http:
//papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.
B. L. Welch. The Generalization of ‘Student’s’ Problem when Several Different Population Variances
are Involved. Biometrika, 34(1-2):28-35, 01 1947. ISSN 0006-3444. doi: 10.1093/biomet/34.1-2.
28. URL https://doi.org/10.1093/biomet/34.1-2.28.
Frank Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80-83, 1945.
ISSN 00994987. URL http://www.jstor.org/stable/3001968.
Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable infor-
mation under computational constraints. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=r1eBeyHFDH.
Longtu Zhang and Mamoru Komachi. Neural machine translation of logographic language using
sub-character level information. In Proceedings of the Third Conference on Machine Translation:
Research Papers, pp. 17-25, Belgium, Brussels, October 2018. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/W18-6303.
Wei Zhang, Feifei Lin, Xiaodong Wang, Zhenshuang Liang, and Zhen Huang. Subcharacter Chinese-
English neural machine translation with Wubi encoding, 2019.
MiChaI Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The United Nations Parallel
Corpus v1.0. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Sara
Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis (eds.), Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC 2016), Paris, France, may 2016. European Language Resources
Association (ELRA). ISBN 978-2-9517408-9-1.
15
Under review as a conference paper at ICLR 2021
Appendices
A Re-visualization of Figure 1 in Junczys-Dowmunt et al. (2016) in 6
FACETS BY TARGET LANGUAGE	17
B	Data selection and preprocessing details	17
C	Hyperparameter setting	19
D	Data statistics	20
E	Score tables	24
F	Correlation statistics	25
G Enlarged figures for all 30 language directions (aggregate results
from all runs)	26
H Sample figures from run A0, also sorted by source language for con-
TRAST	42
I	Language pairs with significant differences	43
J	Language complexity	44
K	Sample-wise Double Descent (DD)	46
K.1 Our experimental framework on DD datasets from (Nakkiran
ET AL., 2020) ............................................ 46
K.2 Token-to-parameter ratio for non-neural monolingual LMs . . .	47
L	Number of model parameters	50
M	Erraticity	53
M.1 Erraticity as large variance: evidence from different runs of the
same data ................................................ 53
M.2 Additional experiment with length filtering to 300 bytes . 53
N Experiments with one-layer Transformer	56
O PAQs (Previously asked questions)	57
O.1 One setting for all ...................................... 57
O.2 Translationese / word order .............................. 57
P Understanding the phenomena with alternate representations (ex-
tended version)	59
16
Under review as a conference paper at ICLR 2021
A Re-visualization of Figure 1 in Junczys-Dowmunt et al. (2016) in
6 facets by target language
AR EN ES
hMh
SYSTEM
Figure 4: Results of the Moses baseline systems (right group in each facet) and neural models (left)
with 1.2 million iterations (1 iteration corresponds to 1 mini-batch) for the 30 directions of the 6-way
UN corpus, tokenized (ZH segmented), lowercased, and length filtered to 100 BPE tokens.
B Data selection and preprocessing details
The UN Parallel Corpus v1.0 (Ziemski et al., 2016) consists of manually translated UN documents
from 1990 to 2014 in the 6 official UN languages. Therein is a subcorpus that is fully aligned
by line, comprising the 6-way parallel corpus we use. We tried to have as little preprocessing or
filtering as necessary to eliminate possible confounds. But as the initial runs of our experiment failed
due to insufficient memory on a single GPU with 12 GB VRAM5, we filtered out lines with more
than 300 characters in any language in lockstep with one another for all the 6 languages such that
the subcorpora would remain parallel, thereby keeping the material of each language semantically
equivalent to one another. 8,944,859 lines for each language were retained as our training data which
cover up to the 75th percentile in line length for all 6 languages. In order to monitor the effect of data
size, we made subcorpora of each language in 5 sizes by heading the first 102, 103, 104, 105, 106
lines6. We refer to this as dataset A. In addition, to better understand and verify the consistency of the
phenomena observed, we made 2 supplemental datasets by shuffling the 8,944,859 lines two different
times randomly and heading the number of lines in our 5 sizes for each language, again in lockstep
with one another (datasets B and C).
5GPUs used for experiments in this paper range from a NVIDIA TITAN RTX (24 GB), NVIDIA GeForce
RTX 2080 Ti (11 GB), a GTX Titan X (12 GB), to a GTX 1080 (8 GB). All jobs were run on a single GPU
setting. Some word-level experiments involving ARtrg or RUtrg at 106 had to be run on a CPU as 24 GB
VRAM were not sufficient. Models with higher maximum sequence lengths (e.g. byte models) were trained with
24 GB VRAM. Difference in equipment does not necessarily lead to degradation/improvement in scores.
6The terms “line” and “sentence” have been used interchangeably in the NLP literature. We use “line” to
denote a sequence that ends with a newline character and “sentence” as one with an ending punctuation. Most
parallel corpora, such as ours, are aligned by line, as a line may be part of a sentence or without an ending
punctuation (e.g. a header/title). Using a standardized unit such as “line” would also be a fairer measure to
linguae/scriptiones continuae (languages/scripts with no explicit punctuation).
17
Under review as a conference paper at ICLR 2021
For character modeling, we used a dummy symbol to denote each whitespace. For byte, we turned
each UTF-8-encoded character into a byte string in decimal value, such that each token is a number
between 0 and 255, inclusive. For word, we followed (Junczys-Dowmunt et al., 2016) and used
the Moses tokenizer (Koehn et al., 2007) as is standard in NMT practice when word tokenization is
applied and Jieba7 for segmentation in ZH.
For Pinyin, we used the implementation from https://github.com/lxyu/pinyin in the
numerical format such that each character/syllable is followed by a single digit indicating its lexical
tone in Mandarin. For Wubi, we used the dictionary from the implementation from https://
github.com/arcsecw/wubi.
We have implemented all representations such that they would be reversible even when the sequence
contains code-mixing.
We used the official dev set as provided in (Ziemski et al., 2016), 3,077 lines per language remained
from 4,000 after filtering line length to 300 characters. Data statistics is provided in Appendix D for
reference.
The systematic training regime that we give to our language directions are identical for all. For each
primary representation type (character, byte, and word), we performed:
•	5runsin5 sizes (102 - 106): A0 (seed=13), B0 (13), C0 (9948), A1 (9948), A2 (265), and
•	7morerunsin4 sizes (102 - 105): A3 (777), A4 (42), A5 (340589), A6 (1000), A7 (83146),
B1 (9948), & C1 (13).
For each run and each size, there are 30 pairwise directions (i.e. 1 source language to 1 target language,
e.g. AR-EN for Arabic to English) that result from the 6 languages. We trained all 150 jobs for
each run and representation using the Transformer model (Vaswani et al., 2017) as supported by the
SOCKEYE Toolkit (Hieber et al., 2018) (version 1.18.85), based on MXNet (Chen et al., 2015). A
detailed description of the architecture of the Transformer can be found in (Vaswani et al., 2017). The
same set of hyperparameters applies to all and its values are listed in Appendix C.
Notes on training time Each run of 30 directions in 5 sizes took approximately 8-12 days for
character and byte models. Byte models generally took longer — hence training time is positively
correlated with length (concurring with observations by Cherry et al. (2018) as they compared
character with BPE models). A maximum length of 300 characters entails a maximum length of
at least 300 bytes in UTF-8. Each run of word models (30 directions, 5 sizes) took about 6 days
(excluding the training of some 7-9 directions out of 30 per run involving ARtrg or RUtrg at 106 on
word level which took about 12-18 hours each direction to train on a CPU as these required more
space and would run out of memory (OOM) on our GPUs otherwise). These figures do not include
the additional probing experiments described in § 4.
Evaluation metric Most sequence-to-sequence models are optimized using a cross-entropy loss,
defined as:
N
H(t, s) = - X log2 p(ti | t<i,s)	(1)
i=1
where t is the sequence of tokens to be predicted, ti refers to the ith token in that sequence, s is
the sequence of tokens conditioned on, and N = |t|. It is customary to report scores as PP, which
is 2NH(t,s), i.e. 2 to the power of the cross-entropy averaged by the number of tokens (based on
whichever granularity of unit is used for training) in the data. Cotterell et al. (2018) propose to use
“renormalized” PP to evaluate LMs fairly through the division of an arbitrary constant. In our case, we
choose instead a simpler method of using an “unnormalized” PP, i.e. the total number of bits needed
to encode the development (dev) set, which has a constant size of 3,077 lines per language (after
length filtering of the same dev set used in Junczys-Dowmunt et al. (2016)) for all various training
sizes. As the implementation we used (SOCKEYE (Hieber et al., 2018)) only reports PP, we transform
it back to entropy as defined above by noting that H(t, s) = log2 P P (t|s) × N.
7https://github.com/fxsjy/jieba
18
Under review as a conference paper at ICLR 2021
C Hyperparameter setting
•	encoder transformer;
•	decoder transformer;
•	num-layers 6:6;
•	num-embed 512:512;
•	transformer-model-size 512;
•	transformer-attention-heads 8;
•	transformer-feed-forward-num-hidden 2048;
•	transformer-activation-type relu;
•	transformer-positional-embedding-type fixed;
•	transformer-preprocess d; transformer-postprocess drn;
•	transformer-dropout-attention 0.1;
•	transformer-dropout-act 0.1;
•	transformer-dropout-prepost 0.1;
•	batch-size 15;
•	batch-type sentence;
•	max-num-checkpoint-not-improved 3;
•	max-num-epochs 50;
•	optimizer adam;
•	optimized-metric perplexity;
•	optimizer-params epsilon: 0.000000001, beta1: 0.9, beta2: 0.98;
•	label-smoothing 0.0;
•	learning-rate-reduce-num-not-improved 4;
•	learning-rate-reduce-factor 0.001;
•	loss-normalization-type valid;
•	max-seq-len 300 for character, word, and BPE, 672 for all bytes, 688 for Wubi, 680 for Pinyin;
•	checkpoint-frequency/interval 4000.
(For smaller datasets, the end of 50 epochs is often reached before the first checkpoint. Since SOCKEYE
only outputs scores at checkpoints, we adjusted the checkpoint frequency as follows to get a score outputted
by the end of 50 epochs: 1000 for 100 lines for all character & byte instances, 400 for 100 lines for word
and 500 for 100 lines BPE, 3450 for 1000 lines for word & BPE. For the very few cases that this default
does not suffice due to bucketing of similar length sequences, we manually set the checkpoint frequency to
the last batch.)
19
D Data statistics
2。
•	Number of types, i.e. vocabulary size (∣ V∣). Note that Sockeye adds for its calculation 4 additional types: <pad>, <s>, <∕s>, <unk>.
•	Number of tokens. This excludes the 1 EOS/BOS (eπd-∕begiπniπg-of-seπteπce) marker added by Sockeye to each line.
•	Out-of-vocabulary (OOV) type rate (in %), i.e. the fraction of the types in the dev data that is not covered by the types in the training data.
•	OOV token rate (in %), i.e. the fraction of tokens in the dev data that is treated as UNKnowns.
• Type-token-ratio (in %), i.e. the ratio between the number of types and tokens in the data. This is a rough proxy for lexical diversity in that a value of 1 would indicate that no type is ever seen twice, and a value very close to O would indicate that very few
distinct types account for almost all of the data.
• Line length (excl. EOS/BOS marker): meaπ±standard deviation, and the 0/25/50/75/1 OO-th percentile.
Statistics for dataset A
s,'ss4s∙∙s∙∙,,∙u∙
"u'rarau,r"
∙ss,"∙∙u∙us
*,■常RIJzh--
Msfe-
1•■，电*t*t*t3-u'"^u'"^
ss∙∙sss
i≈,'∙sl∙∙∙s∙
一g≡s≡
腌≈5is
湍湍媵
::::":::
::::":::
湍!!;牌
湍:::濡
*■■BS»RIJzh
潞滥!;:
::!::!:s
::::::湍
:::::::::
:s:s;s
滥滥:1;
*»瑞1»»⑸
us*≈莺*«
酬需,4,≡
s≡s≡!i"
""s黑s4,s∙∙
81
l∙∙l∙∙≈≈
::!::!歌
颂-4∙∙u黑
,,,,,
Bs尴!:=
;：=懒愣
-缴镯
Ss
∙ss,s4ij∙iji
,∙ussis
S's44ai
isi≡u'
:s:s1"
∙'∙∙≈s∙'∙s∙∙∙,
::::s:!:
UnderreVieW as a ConferenCe PaPersICLR 2021
≡sa∙≡
sss≡≡
5sS5ss
,'uisl"i3
s≡≈-
:ii:»:a
≈"≈s≈s
::::s:s
≡s≡s≡
5≈S≡s≡
≈s≡∙
tsais
i≡≈sii≡
sI≡uu
∙i≈is≡
!?:!::!£
2=1

晒黑嚷»≡,
11


≡≡
I
I
≡≡


™
≡,≡,
I
L绪常黑-5,
Ssia
霁-阳


泮
≡的器

•'"•'"懈ξ

≈≈≈
S≡s,

≈≈≈
≈≈≈
≡s≡SE

°一
AR EN ES FR RU ZH	S/49/115/170/^ 温娥镰 湍4潞秘陶 瑞蠲图褊 泰精雕	5郴报黑瀚瑞慨湍需慨廊靛嬴 渭描蹄8…挪搦嬴林藐%斶跪骗 6摆揣陶阖虢搞瀚㈱慨“瑞⅛ 可雕泪（B报混隈峭踹嬴蔚踹 1⅛ √≈‰ M擢假怖擢魏一瑞疏⅛ 战渴昵谭帮郎面蜀踹播防踹	局湍嬴隔滤院战搬龈擀髓腿一魅耦黑 M晶漏湍篇撮混斶踹部赫瑞褊湍渴舞n 7揶端微7湍蠲球。6斶露捣7揭阖龈前部隔					跪罚渴，跪献6搞球微湍潮温制虢篙 瑞甯湍茄粽黑调露瑞溪辘选播球捣 卷跳微储黯，躅就『混揭源播球搞 流程瑞就跣瀚疆温湍那瑞就端黑 1S.54±1O.65 54.75 ±9.53	51.66±1O.S1	51.51 ±11.15	50.45 ± 11.44 1/8/18/27/39 1/19/25/31/58 1/14/22/29/104 1/13/22/30/1" 1/11/20/29/136 17.51±10.6S 53.57±9.55	50.77±10.4S	50.39 ± 10.60	19.36 ±10.ST 2/7/17/25/46 1/18/23/29/76 1/13/20/28/108 1/13/20/28/111 1/11/19/27/167	湍蒜法推漏/踹潞瑞「湍搬工混帚混 凋靛洗施崎雕潞帮搞溜揭猿摇防跪 4瑞薪湍斶茄愣溜帮瑞林瑞乳㈱嘉微 弑瑞"牒调"瑞踹贯强⅛摘牖犒 31.10±17.45 59.11 ±11.51 53.13 ± 11.71 53.0S±15.17	55.07±15.57 4/18/29/45/69 1/21/29/37/68 1/15/23/31/104 1/14/23/32/110 1/12/22/31/166 55.53±15.33 55.46 ± 10.59 51.55 ±10.94 50.S4±10.95	19.S9±11.53 2/13/20/31/58 1/19/25/32/79 1/14/11/19/108 1/13/11/18/111 1/11/20/28/167
			5/雕施陶措温祸猊调蹴髅…/雕菰微				湍湍秘做 547.10±141.14 1/127/249/357/567 94.57±55.S0 1/51/95/134/594		
			517.51 ±134.51 20/77/215/332/489 8559±5L18 7/35/81/127/192	309.5S±114.fe 10/231/318/392/554 116.67±43.S5 6/88/117/1宓/243	湍歌揭嬴 105.00 ±50.61 1/66/103/139/297	559.90 ±137.10 1/155/264/365/566 99.90 ±51.49 1/61/101/138/337			
ZH_pinyin ZH-WUbl AR-CPl256	7瑞跪踹5 燃捣曲	14S.0S± 55.57	127.96 ±63.96	125.31±65.25	11S.47±66.SS 7/109/149/188/312 1/82/129/175/369 1/76/156/174/407 1/63/119/169/645 157.19 ± 4S.00	110.45 ± 54.71	10&86 ± 56.04	105.91 ± 57.41 7/94/129/158/266 1/71/111/150/327 1/67/110/150/364 1/55/103/146/627	描院黑嬴	5搦源踹3		僦瑞哈	微弱除。。		
调蠲跳6。+踹靛最温温孤揄段湍僦）一/胡猊歌。
UnderreVieW as a ConferenCe PaPersICLR 2021
Statistics for dataset B
21
UnderreVieW as a ConferenCe PaPersICLR 2021
Statistics for dataset C
22
UnderreVieW as a ConferenCe PaPersICLR 2021
Statistics for development (dev) set
As a different set of vocabulary is learned from each training dataset and data size, BPE has a distinct dev set for each.
Rep∙e 衿 ntauon Number of lines in train set	CHAR	BYTE	WORD				工，°。。	EPE A		工，。。。，。。。	EPE E 100	1,000	10,000		工。。，°。。	工，。。。，°。。	EPE C 100	1,000	10,000			工。。，。。。	工，。。。，。。。
-NumterofTYPES-			13,≡	70S 656	3,232 2,567	MM	11,473 7,461	12,430 7,556	865	3,991 SOl	3,149	10,735 6,518	1,鸳	13,090 7,578	90S 823	4,119 3,245	10,681 6,551	；普	13, 093 7,573
I			8：551 8,312	680 744	2,832 2,821	6：919 6,925	8,752 8,526	8,871 8,666	833	3,426 827	3,407	7,528 7,489	随	筵	834 JS3	3,424	7,527 7,431	热	8,939 8,708
RU			12,819 7,413	舞4	3,769 4,215	*3器	12,505 7,541	12,7SS 7,654	1,0M	4,438 3,261	4,481	11,ɪ	16,883 7,719	12,954 7,702	1,03«	VOS 4,524	11,ɪ		12,958 7,721
⅛																	
AR,pl256 RU,pl251		130 146															
n<umwr 片RIUKtɪ` S	334,358 391,222	605,516 391,260	61, 371 67, 629	167,574 155,826	115,693 101,782	S3, 001 77,犯9	76,284 71,276	70,527 70,339	149,第9	97,843 140,256	90,871	尊魏	寮鬻	68,270 69,348	149,623 140,377	3T,531	尊嗯		翻：
	443,958 43S,083	452,190 452,556	78, 087 78,745	170,433 166,280	113,犯 3 114,694	88, 634 88,726	方端	导吃	155,第7	103,748 156,256	104,071	84：914 85,115	Si	北钢	154,746 153,745	103,ITS 104,091	8070 85,125	黎随	80, 371 80, 654
	431,538 107,990	793,214 301,085	64,180 60, 013	177,818 96,745	雷馈	79,763 68,129	72,480 63,614	以整	163,319	100,294 93,775	75,636	需播		北,源	163,806 94,127	款黑	T5,O06	70,196 61,916	署卷
⅛	376,979 330,734																
AR,pl256 RU,pl251		334,31															
TTR (%) ⅛			55.54 10.64 10.95 10.56 19.97 12.35	0.45 0.45 0.40 0.45 0.49 3.33		10.84 778 7S1 7S0 15.64 9.37	15.04 10.47 10.64 10.34 1725 11.S5	17.65 10.74 10.91 10.63 17.99 15.19		14.64 8.8β 8.S7 8.80 14.73 10.33	IS.90 10.85 11.01 10.75 IS.35 11.48	19.17 10.93 11.15 10.79 1&51 15.46			1452 &92 &87 &73 14.66 10.41	1&86 IOSl 11.01 10.73 1&35 15.36	19.IS 10.95 11.15 10.SO 1&52 15.49
AR,pl256 RU,pl251		0.04 0.03															
																	
AR	⅛∕⅛ 6湍陶犒揭崎牒			热甯箱Z j涕希强5		湍帮镶	淌耦篇	湍薪洗	温需温渴揣而	战滩场	漏膝选提防踹		J露徜选拣防;诵。		溜陶13策7	湍防箱3提;彘跪	
EN	6/岗揭盟99	6掷报场…瑞彘摘		溜靛温播漪潮：。		潞拨法	,瑞彘洸	礴端总	提瀛搞凋防歌	战陶踹	源龈段雕		篇输就2	湍就踹	湍战踹3	谶潞褐调靛潟	
ES	5幅标嬴	5徜隈槌赤湍溪第3				制瀛遹	混餐洗	湍粽3	溜需"解捣瑞	，凋就跪	湍薪需提牖踹		,潴露斑3	湍箭需5	，阳懿瑞	温调3踹踹	
FR	4糕挪褊4/牌秘温混糕关			秘渴需;一端彘强9		淄萧嬴	溜游/虢	调薪洗	谓需靛温提裳瑞晶	舞储/魏。	混徜3箭热				湍储/拨4	阂靛瑞混薪㈱	
RU	5瑞标沏福牖挪.混孤选			商蓊隈盘球提5溜布捣潞晶温湍挪5					渊靛隈溜糊"混力躲近强耀循泰提				掰揣盘溜力踹拉斯虢5调泰瑞混晶版				
ZH	溜瑞/黑		施方前潴	渴4部温		淌薪选			潞而微泼滋眠		提磕㈱挪防黑		湍龈搞	族竭箍3	翁揣洗	湍简黑4湍;就瑞	
ZHjinyin	湍需牒53																
ZH-WUbl																	
AR,pl256		≡∕⅛∕⅛															
RU,pl251		5常点就嬴															
K
24
E Score tables
Number of bits to encode the dev data for each of the 30 language directions. Shown is meaπ±stdover:
♦ 12 runs for CHAR, BYTE, and WORD from 100 to 100,000 lines,
• 5 runs for 1,000,000 lines, and
• 3 runs for all sizes involving alternate representations (BPE, Pinyin, Wubi, cp 1256 and cp 1251).
ii-
,,∙,s∙'∙4s4s"ss∙,"s444«'4s4a∙,∙,4s""∙∙∙s∙s4''sss∙,ssu∙'a""ss4ss'a≈4s∙,,∙∙ss4∙4∙sssuu"s"∙ssu∙∙ss""∙ss∙∙'s
歌∙-ss≡歌≡≡爆≡≡≡-S≡概S≡£=流is黑»≡∙≡≡
,,,，,瓯曲阳M,，成-,瓯,---
7 ± β± 7 ± 8± L± 8± 8± 8± 8± L± 8± 8± 8± 9± L ± β± 7 ± 8± 8± 1，± β± 7 ± 8± 8± 1，± 8± -H ± β± 7 ± 8±
Ws,∙sa∙sss'∙"«s∙4'∙∙∙s,'∙,∙,,γb≈Wmu«»,,ssπ∙s's∙-4'∙∙∙,s∙4∙uu,∙'∙s',«•,s,≈a∙s*',∙∙∙≈T","s,,∙"ssus'U"
w歌≡≡"s≡∙≡--≡s≡≡s≡s∞≡~™≡黑≡i≡™≡≡"
线
•,4'ia*,sM
≈",'a∙∙,1»

一"aTOW4s'4s∙'s∙4∙slsu∙∙∙s∙∙4ss4ss∙s'4••,"'''s'4',41ssa≈∙4usss4,4∙,'4assnus44,s∙s∙s',∙,∙s4∙,≈4∙u's∙,s4ss∙ssss∙


ii=≡≡∙∙≈™5sis≡iii≈-"--"-翦s5≡i≡i≈≡∙∙s≡-•~"≈s∙-≡5
黑鬻獴
!一
»
--
b6
W1"01010101
,il


∙∙s∙»",""ass∙,ss
-∙≡≡-a
4 >，4 4，3 ” 7，，8，，
一■"s
一s,∙''∙∙"≈'s∙s∙ss4≈

I-
一sss'auss∙'ss"∙ls≈
≡
»
一4su∙,∙,


一∙∙'sss∙∙'sss
一,∙,∙s,M
一s,≈4s,∙us,4su∙s≈
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
Under review as a conference paper at ICLR 2021
F	Correlation statistics
Best correlating metrics, i.e. the union of top 3 metrics for all representations.
For each representation, the top 3 metrics are boldfaced.
All correlations are highly significant (p < 10-30), except for min source length for WORD (p ≈ 0.0001) and
min target length for WORD (p ≈ 0.3861).
Metric	CHAR	Pinyin	Wubi	BYTE	ARRUt	ARRUs,t	WORD	BPE
minimum length (target)	0.84	0.85	0.86	0.60	0.84	0.84	-0.02	0.65
minimum length (source)	0.82	0.84	0.85	0.57	0.84	0.84	0.10	0.64
number of tokens (source)	-0.78	-0.81	-0.82	-0.60	-0.81	-0.81	-0.59	-0.83
TTR (target)	0.83	0.83	0.84	0.48	0.81	0.81	0.61	0.83
|V | (source)	-0.54	-0.51	-0.51	-0.50	-0.67	-0.68	-0.63	-0.86
data size in lines	-0.80	-0.83	-0.83	-0.59	-0.81	-0.81	-0.62	-0.86
OOV token rate (target)	0.69	0.66	0.66	0.47	0.67	0.68	0.66	0.62
OOV type rate (target)	0.70	0.71	0.72	0.47	0.69	0.70	0.65	0.62
TTR (source)	0.67	0.71	0.71	0.60	0.81	0.81	0.56	0.82
The full list of metrics used for the correlation analysis is:
1. minimum length (source),
2. minimum length (target),
3. maximum length (source),
4. maximum length (target),
5. median length (source),
6. median length (target),
7. mean length (source),
8. mean length (target),
9. length std (source),
10. length std (target),
11. data size in lines,
12. number of parameters,
13. number of types (|V |) (source),
14. number of types (|V |) (target),
15. number of tokens (source),
16. number of tokens (target),
17. type-token-ratio (TTR) (source),
18. type-token-ratio (TTR) (target),
19. OOV type rate (source),
20. OOV type rate (target),
21. OOV token rate (source),
22. OOV token rate (target),
23. token ratio,
24. target type-to-parameter ratio,
25. target token-to-parameter ratio,
26. distance between the TTRs of source and target = (1 - TTRsrc/TTRtrg)* 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28,
27. token-to-parameter ratio (i) = (median length source * median length target * num_lines) / num_parameters,
28. token-to-parameter ratio (ii) = (num_source_tokens * num_target_tokens) / num_parameters.
25
Under review as a conference paper at ICLR 2021
G Enlarged figures for all 30 language directions (aggregate results
number of bits
from all runs)
Figure 5: CHAR: character models
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
--- ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
26
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
number of lines
Figure 5: CHAR: character models (target language as facet)
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
27
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
Figure 6: CHAR with Pinyin for ZHtrg
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
28
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
EN
ES
2000000
1500000
1000000
FR	RU	ZH
2000000
1500000
1000000
1e+03	1e+05
1e+03	1e+05
1e+03	1e+05
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
number of lines
Figure 6:	CHAR with Pinyin for ZHtrg (target language as facet)
29
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
Figure 7: CHAR with Wubi for ZHtrg
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
30
Under review as a conference paper at ICLR 2021
AR	EN	ES
2000000
sl_q jo -JeqEnU
1500000
1000000
2000000
1000000
1e+03	1e+05
1e+03	1e+05
1500000
1e+03	1e+05
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
number of lines
Figure 7:	CHAR with Wubi for ZHtrg (target language as facet)
31
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
Figure 8: BYTE models with UTF-8 encoding
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
32
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
2500000
2000000
1500000
1000000
2500000
2000000
1500000
1000000
1e+03	1e+05
1e+03	1e+05
1e+03	1e+05
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
number of lines
Figure 8:	BYTE models with UTF-8 encoding (target language as facet)
33
Under review as a conference paper at ICLR 2021
TRG
一AR
——∙ EN
一 ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
1e+03
1e+05
number of lines
Figure 9: BYTE with ARtrg & RUtrg optimized with code pages 1256 & 1251 (ARRUtrg)
sl_q jo -JeqEnU
34
Under review as a conference paper at ICLR 2021
2000000
1500000
1000000
2000000
1500000
1000000
1e+03	1e+05
1e+03	1e+05
1e+03	1e+05
number of lines
sl_q Jo -JeqEnU
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
Figure 9:	BYTE with ARtrg & RUtrg optimized with code pages 1256 & 1251 (target language as
facet)
35
Under review as a conference paper at ICLR 2021
1800000
1500000
1200000
900000
1e+03
1e+05
number of lines
sl_q Jo -JeqEnU
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
Figure 10:	BYTE with directions AR-RU & RU-AR optimized on both source and target sides
(ARRUsrc,trg)
36
Under review as a conference paper at ICLR 2021
1500000
1000000
1500000
1000000
1e+03	1e+05
1e+03	1e+05
number of lines
1e+03	1e+05
sl_q Jo -JeqEnU
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
Figure 10:	BYTE with directions AR-RU & RU-AR optimized on both source and target sides (target
language as facet)
37
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
Figure 11: WORD models
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
38
Under review as a conference paper at ICLR 2021
AR
EN
ES
1200000
1000000
800000
sl_q jo -JeqEnU
FR
RU
ZH
1200000
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
1000000
800000
1e+03	1e+05
1e+03	1e+05
number of lines
1e+03	1e+05
Figure 11:	WORD models (target language as facet)
39
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
Figure 12: BPE models
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
40
Under review as a conference paper at ICLR 2021
sl_q jo -JeqEnU
1500000
1200000
900000
AR	EN	ES
FR	RU	ZH
1500000
1200000
900000
1e+03	1e+05
1e+03	1e+05
1e+03	1e+05
TRG
—AR
-…EN
——ES
--FR
■■■■ RU
---ZH
SRC
—AR
—EN
—ES
—FR
—RU
一ZH
number of lines
Figure 12:	BPE models (target language as facet)
41
Under review as a conference paper at ICLR 2021
H Sample figures from run A0, also sorted by source language
FOR CONTRAST

SΠH1-
TRG
-AR
-EN
-ES
--FR
■— RU
—ZH
SRC
AR
a
æ
FR
RU
ZH
AR
			
		
1elθ3 1e+l5	1e103	1e+θ5	le+θ3 1e+θ5
number of lines
			
(a) CHAR
SRC
AR
EN
ES
FR
RJ
ZH
210111∣-
(b)	CHAR by target
(c)	CHAR by source
Figure 13:	CHAR: character models from run A0

SΠH1-
SRC
—AR
EN
ES
—FR
RJ
ZH
2500
.......-
		

eg
number of lines
(a) BYTE
，e& ιe+θ5	ιe+θ3 ie+05	，e：B ie+05
number of lines
(b)	BYTE by target
1e+□3 ，e& 1e+□3 ，e& 1e+03 1e+l5
number of lies
(c)	BYTE by source
Figure 14:	BYTE: byte models from run A0

number of lines
TlRG
-AR
-EN
—ES
--FR
■— RU
—ZH
SRC
AR
a
æ
FR
RU
ZH
AR
1e-03	1e+l5	le+03	1e+05	le+03	1e+05
number of lines
(b)	WORD by target
SRC
AR.
EN
ES
FR
AU
ZH
er of ll
(a) WORD
(c)	WORD by source
Figure 15:	WORD: word models from run A0
42
Under review as a conference paper at ICLR 2021
I Language pairs with significant differences
15 (non-directional) language pairs total possible from 30 language directions, p=0.001.
LANG PAIR CHAR Pinyin Wubi BYTE ARRUt ARRUs,t WORD BPE
AR-EN	X	XX
AR-ES
EN-ES	X
AR-FR	X		
EN-FR		X	X
ES-FR
AR-RU	X
EN-RU	X	X	XX
ES-RU	X
FR-RU	X
AR-ZH	X	X	X	X		X
EN-ZH	X	X				
ES-ZH		X			X	X
FR-ZH	X	X			X	X
RU-ZH		X	X	X	X	X
Language pairs with significant differences indicate that the 2 languages are not equally/similarly good or
equally/similarly bad.
•	Character models with ZH behave differently but the disparity can be eliminated with Pinyin.
•	Byte models with AR and RU exhibit unstable performance due to length but this can be rectified with
compression on the target side only (ARRUt).
•	Word-based models, including BPE, however, consistently favor EN and ZH (though it is more of a
“mis-segmentation” for the latter, see § 3 and Appendix J) and disfavor AR and RU (as morphologically
complex languages with higher OOV rates).
43
Under review as a conference paper at ICLR 2021
J Language complexity
In the words of Bentz et al. (2016):
Languages are often compared with regard to their complexity from a compu-
tational, theoretical and learning perspective. In computational linguistics, it is
generally known that methods mainly developed for the English language do not
necessarily transfer well to other languages. The cross-linguistic variation in the
amount of information encoded at the level of a word is, for instance, recognized
as one of the main challenges for multilingual syntactic parsing (formulated as
The Architectural Challenge (Tsarfaty et al., 2013)). Complexity of this kind is
also found to influence machine translation: translating from morphologically rich
languages into English is easier than the other way around (Koehn, 2005).
Morphology is “the study of the formation and internal structure of words”.
Morphemes are “the smallest meaningful units of language”. (Bender, 2013)
AR and RU are traditionally considered morphologically complex (see e.g. Minkov et al. (2007),
Seddah et al. (2010) and proceedings of related workshops in subsequent years), and ZH lacking
morphological richness (Koehn, 2005). But this definition of morphology is predicated on the notion
of word, defined primarily from an alphabetic perspective. As pointed out by Zhang & Komachi
(2018), “the important differences between logographic and alphabetic writing systems have long
been overlooked”. In logographic languages (i.e. languages with logographic scripts), there can be
units within a character that carry semantic and phonetic information that have never been accounted
for in the traditional practice of morphology or in the computation of morphological complexity.
For example, in the comparison of different morphological complexity measures by Bentz et al.
(2016), all measures studied are defined with the notion of word.8 Yet, there is no universally valid
definition of a “word” — the form/idea (as in, the philosophical concept) of a “word” may be there
for most languages/cultures (though that is certainly also debatable), but its instantiations are different
in different languages/cultures, as well as in different genres/settings within one language. The
variability in the definition of word is evident in the variation in language-specific word tokenization
algorithms, along with the “indeterminacy of word segmentation” or a work-in-progress status for the
definition of “word” advocated by Haspelmath (2011), as well as the contested nature of wordhood,
esp. for logographic languages such as ZH (see Duanmu (2017) and Li et al. (2019b) for how some
ZH speakers do indeed consider a ZH character to be a word or how “word”, as conventionally used
in NLP, is not a native term or does not correspond with speakers’ judgement).
Our results with the Transformer indicate that a notion of morphological complexity can be modeled
given our word tokenization scheme, confirming that morphological complexity is only predicated on
the notion of word and bounded within the word level, and orthogonal to the performance of character
or byte models. That is, unless word-based segmentation has been applied, there is no reason to
attribute crosslinguistic performance disparity to differences in morphological complexity. In fact, on
the character and byte level, we were able to achieve performance without disparity. Hence disparity
is not a necessary condition but an expectation that has been in mutual reinforcement with
our practice of word segmentation, while the definitions of “morphological complexity” and
“word” are in a circular dependency with each other.
In this paper, we resolve language complexity, more specifically that of morphological complexity,
in the context of computing through CLMing with the Transformer, in that we explain away the
representation granularities and criteria relevant for such calculation.
TLDR: Up to the point of our taking up the subject of language complexity in this paper, there has
been not a rigorous definition of “language complexity”. Conventionally, “language complexity”
is synonymous to “linguistic complexity” (with the tradition of “linguistics” being primarily word-
8An exception could be that of the type/token ratio (TTR). One could imagine applying TTR on the character
level for ZH, and that would be indicative of its morphological richness on the character level. However, that has
thus far never been practiced or recognized in NLP.
44
Under review as a conference paper at ICLR 2021
based), and people just assume linguistic complexity, e.g. morphological/syntactic complexity, to be
intrinsic and necessary in languages (across representation levels). Our findings show that linguistic
complexity is relative to the representation granularity, i.e. since morphology is based on words, it is
bounded to the word level.
An alternative perspective, with finer prints:
We have also developed a more rigorous interpretation. We take on the definition of “language
complexity” as one that is related to the statistical attributes of languages. We assume and define
solving as the elimination of statistically significant performance disparity.
In larger (6-layer) models, and according to the conventional definition of “language” — i.e. language
as a whole, we solved language complexity with compression of AR and RU in byte representations.
In smaller (1-layer) models, one can think of the situation as: i) no complexity has been modeled by
the Transformer hence there is nothing to solve, or ii) there is no complexity between these languages
to begin with, or iii) the Transformer solved the complexity.
With respect to each representation level/granularity in the larger models:
•	BYTE: one can think of us as having solved complexity with byte representations or with
1-layer models — for these 6 languages empirically. Theoretically, there could be languages
with longer sequence lengths than RU and AR, in those cases, we don’t claim to have solved
the matter empirically but only resolved it conceptually. But this is the most that anyone
could do at the moment, as there is no relevant parallel data available.
•	CHARACTER: one can think of us as having solved it via bytes or 1-layer models. Whether
we can be considered to have solved it via Pinyin for ZH depends on whether the evaluator
accepts decomposition into a phonetic representation only qualifies as a solution for the ZH
language.
•	WORD: one can think of us as having solved it via bytes or 1-layer models. It is not possible
to solve it strictly within the word level without creating word segmentation criteria that
would be unrelatable to native speakers. And since “word” is exclusively a human concept,
we must either claim that a universal solution is undefined or undefinable for computing,
or retreat to a unit that is the greatest common factor crosslinguistically. Since some ZH
speakers consider ZH characters as words, we return to the character-level solution.
It is beyond the scope of our paper to solve the qualitative disparity on the word level. However, we
do advocate a more inclusive evaluation and critical reflection on the possibility of discontinuing the
usage of “word” as such a non-technical term biases against both “morphologically complex” and
“morphologically simple” languages. The world of languages in written form can be divided into
those with logographic scripts and those with (phonetic) alphabetic ones, with the unit of character
being the greatest common factor of them all, from the human perspective. For technical processing,
esp. for fair multilingual sequence-to-sequence modeling with the Transformer, we recommend
measures that are more standardized, such as those based on bytes or characters. There is room
for improvement in the design of character encoding that complements the statistical profiles, e.g.
with relative rank in sequence length, of different languages. We believe there is crosslinguistic
systematicity on the character level to be leveraged.
One’s readiness to accept this as a solution to language complexity can be a subjective matter.
One may insist that language complexity be solved exclusively with monolingual LMing (which
lies outside the scope of the present work), instead of being confounded with the logic of one
language being conditional on another. One may also object to the idea of (re-)solving morphological
complexity being equivalent to or leading to solving language complexity as a whole, for there
could also be e.g. syntactic complexity (although as substantial “information concerning syntactic
units and relations is expressed at word level” in morphologically rich languages (Tsarfaty et al.,
2010), the boundary between morphology and syntax is less distinct for some languages than others
(Haspelmath, 2011)). If, however, our results could be extended, we wonder if syntactic complexity
could be due to our sentence segmentation or a combination of word and sentence segmentation
practice. That we leave for future work for those who are interested in the topic.
45
Under review as a conference paper at ICLR 2021
K Sample-wise Double Descent (DD)
K. 1 Our experimental framework on DD datasets from (Nakkiran et al., 2020)
Text experiments from previous work reporting sample-wise DD involved words (Belkin et al., 2019)
and BPEs (Nakkiran et al., 2020).
We applied our experimental framework — by testing data points with 10n lines — on the datasets
reported in (Nakkiran et al., 2020) to exhibit DD. WMT’149 EN-FR was reported to demonstrate
model-wise DD and IWSLT’14 (Cettolo et al., 2012) DE-EN model-wise and sample-wise DD. We
downloaded and prepared the data with scripts10 from the FAIRSEQ Toolkit (Ott et al., 2019). The
WMT data was preprocessed with 40,000 BPE operations and IWSLT 10,000. Our focus is on
sample-wise DD and hence our goal was to see if the spike at 103 we observed with the UN data
would apply also to these datasets. We used the same training regime11 with the Transformer and
Adam on SOCKEYE as before and tested both language directions on the entirety of both datasets,
with no subsampling. For the IWSLT dataset, we tested data sizes with 102 - 105 lines, then at
160, 239 as that is the total number of lines available. For the WMT dataset, we tested from 102 to
107, then at 35, 762, 532.
TRG
-DE
SRC
DE
e-03	ie+05	ιe+07	ie+03	ie+05	ιe+07	ιe+03 *+05 ιe+07
number of lines	number of lines
(a) WMT’14	(b) WMT’14 by target
1e+03	1e+05
number of ines
(c) IWSLT’14
Ie+03	1e+05	*+B	ie+05
n umber of lines
(d) IWSLT’14 by target


Figure 16: WMT’14 EN-FR and FR-EN and IWSLT’14 DE-EN and EN-DE: sample-wise DD shown at 103
Table 2: Target-Train-Token-to-Parameter ratio (TTT2P ratio) for WMT’14 EN-FR and FR-EN
	100	Number of lines					35,762,532
		1,000	10,000	100,000	1,000,000	10,000,000	
EN: num train tokens	3,248	33,768	313,154	3,123,129	30,852,455	308,640,462	1,174,344,513
FR: num train tokens	3,548	36,507	339,803	3,414,959	33,865,679	343,344,536	1,327,817,765
EN-FR num params	45,609,474	51,039,363	62,871,584	75,630,304	85,210,037	108,226,335	111,417,633
TTT2P ratio	0.000078	0.000715	0.005405	0.045153	0.397438	3.172468	11.917483
FR-EN num params	45,540,219	50,692,575	61,916,891	74,547,874	83,936,258	107,378,859	111,399,165
TTT2P ratio	0.000071	0.000666	0.005058	0.041894	0.367570	2.874313	10.541771
This shows that the effect we reported in § 5 also holds on these datasets: “the ratio of target
training token count to number of parameters falls into O(10-4) for 102 lines, O(10-3) at 103,
O(10-2) at 104, and O(10-1) for 105 lines and so on”.
9http://www.statmt.org/wmt14/translation-task.html
10https://github.com/pytorch/fairseq/blob/master/examples/translation/
prepare-wmt14en2fr.sh and https://github.com/pytorch/fairseq/blob/master/
examples/translation/prepare-iwslt14.sh
11max-seq-len 300; checkpoint-frequency 4000 except for cases where 50 epochs would be reached before
the first checkpoint: 400 for 102 lines and 3450 for 103 lines.
46
Under review as a conference paper at ICLR 2021
Table 3: Target-Train-Token-to-Parameter ratio (TTT2P ratio) for IWSLT’14 DE-EN and EN-DE
	Number of lines				160,239
	100	1,000	10,000	100,000	
DE: num train tokens	2,874	27,675	253,757	2,519,534	4,035,591
EN: num train tokens	2,739	26,416	245,659	2,461,879	3,949,114
DE-EN num params	45,297,348	49,410,683	53,639,825	55,189,376	55,428,584
TTT2P ratio	0.000060	0.000535	0.004580	0.044608	0.071247
EN-DE num params	45,405,078	49,809,797	54,300,056	56,245,643	56,564,366
TTT2P ratio	0.000063	0.000556	0.004673	0.044795	0.071345
K.2 Token-to-parameter ratio for non-neural monolingual LMs
We experimented also on KenLM (Heafield, 2011; Heafield et al., 2013), a non-neural LM with
modified Kneser-Ney smoothing (Kneser & Ney, 1995; Chen & Goodman, 1999), on our dataset
A and found that on the word level, such a spike (or a hump) is common across all languages, see
Figure 17. The target-token-to-parameter ratio is under 1 for most of these smaller data sizes. This
seems related to the analytical findings in Opper et al. (1990) where the pseudo-inverse solution to a
simple learning problem was shown to exhibit non-monotonicity, with the peak exactly as the ratio of
data to parameters (α) approaches 1.
S=qjo∙l∙qlunu
1500000
1250000
1000000
750000
500000
1500000
1250000
1000000
750000
500000
ORDER
--3
--4
--5
--6
REP
--BYTE
--CHAR
— WORD
1e+02
Figure 17: Kneser-Ney (monolingual) n-gram LMs on the same data (A) used for our neural CLMs
The number of parameters of a k-gram model is the number of unique n-grams, 1 ≤ n ≤ k . Table 4
shows the ratios for our trigram model (all n-gram models of higher order exhibit the same effect).
47
Under review as a conference paper at ICLR 2021
On word level, where the function of number of bits to data size is not always monotonic, we observe
less of a monotonic development whenever the token-to-parameter ratio is smaller than 1. This
is more notably shown in the first 4 sizes in AR with a hump-like curve before the performance
improves at 106 . This is different from the sharper descent for ES and FR, where only the first two
data sizes have a non-monotonic relationship and a token-to-parameter ratio less than 1. Taking the
token-to-parameter ratio as a rough proxy for over- (< 1) and under-parameterization (> 1), this
can be seen as an instance of non-monotonicity with respect to data size in the “critical regime”, i.e.
when the model transitions from being (heavily) over- to under-parameterized (Belkin et al., 2019;
Nakkiran, 2019).
A remark on modeling with finer granularity Our KenLM results show the performance of bytes
and characters is not on par with that of words with non-neural algorithms. NNs/DL has enabled
much progress in this regard.
48
Under review as a conference paper at ICLR 2021
Table 4: Token-to-parameter ratios on non-neural monolingual trigram LMs
	lang_numlines	num_tokens	| unigrams |	| bigrams |	| trigrams |	num_params	tokens/params
CHAR	AR_100	9079	85	925	2894	3904	2.325563525
	AR_1000	123832	110	1577	8592	10279	12.04708629
	AR_10000	1083517	152	3216	21479	24847	43.60755826
	AR_100000	10625047	179	5114	44251	49544	214.4567859
	AR_1000000	102064230	242	8517	90353	99112	1029.786807
	EN_100	11730	78	806	2532	3416	3.433840749
	EN_1000	159444	84	1215	5808	7107	22.43478261
	EN_10000	1344001	125	2532	17181	19838	67.7488154
	EN_100000	13132862	170	4231	36104	40505	324.2281694
	EN_1000000	123491871	247	7126	70406	77779	1587.727677
	ES_100	12374	87	781	2398	3266	3.788732394
	ES_1000	171104	93	1210	5045	6348	26.95400126
	ES_10000	1484804	117	2534	15462	18113	81.97449346
	ES_100000	14549703	176	4261	33554	37991	382.9776263
	ES_1000000	138596036	257	7217	67280	74754	1854.02836
	FR_100	12456	89	836	2610	3535	3.523620934
	FR_1000	179048	97	1259	5711	7067	25.33578605
	FR_10000	1490983	133	2607	16282	19022	78.38203133
	FR_100000	14528593	178	4390	35051	39619	366.707716
	FR_1000000	138049189	259	7353	69522	77134	1789.732012
	RU_100	11980	98	952	3051	4101	2.921238722
	RU_1000	168156	111	1415	7106	8632	19.48053753
	RU_10000	1436078	163	3506	20478	24147	59.4723154
	RU_100000	14151728	190	5737	44071	49998	283.0458818
	RU_1000000	134706120	263	10186	94975	105424	1277.755729
	ZH_100	3318	605	2036	2634	5275	0.6290047393
	ZH_1000	42572	1239	13266	24811	39316	1.082816156
	ZH_10000	372003	2270	68178	175730	246178	1.511113909
	ZH_100000	3659617	3403	241215	968852	1213470	3.015828162
	ZH_1000000	34672612	4888	611213	3977112	4593213	7.548661906
BYTE	AR_100	16655	76	320	1163	1559	10.68313021
	AR_1000	227163	98	539	2070	2707	83.91688216
	AR_10000	1985014	133	1616	5974	7723	257.0262851
	AR_100000	19487689	148	2844	14274	17266	1128.674215
	AR_1000000	186171180	165	5219	40507	45891	4056.812447
	EN_100	11731	79	807	2533	3419	3.431120211
	EN_1000	159449	85	1219	5812	7116	22.40711074
	EN_10000	1345771	130	2527	17139	19796	67.98196605
	EN_100000	13158948	154	3971	34985	39110	336.4599335
	EN_1000000	123705128	169	6422	66606	73197	1690.030029
	ES_100	12629	88	766	2414	3268	3.864443084
	ES_1000	175286	94	1146	4901	6141	28.54355968
	ES_10000	1513782	121	2409	14894	17424	86.87913223
	ES_100000	14821495	154	3925	31905	35984	411.8912572
	ES_1000000	141276766	169	6338	62199	68706	2056.250779
	FR_100	12875	90	830	2560	3480	3.699712644
	FR_1000	185227	99	1227	5497	6823	27.14744247
	FR_10000	1542105	133	2492	15615	18240	84.54523026
	FR_100000	15055657	156	4014	33105	37275	403.9076325
	FR_1000000	143495667	175	6423	64044	70642	2031.308103
	RU_100	21751	100	475	1365	1940	11.21185567
	RU_1000	309279	113	694	2732	3539	87.39163606
	RU_10000	2636591	151	1898	8430	10479	251.607119
	RU_100000	25990263	160	3364	18321	21845	1189.757977
	RU_1000000	247098758	169	6224	45935	52328	4722.113553
	ZH_100	8559	140	1524	3532	5196	1.647228637
	ZH_1000	116667	146	2706	12857	15709	7.426761729
	ZH_10000	1019969	156	5596	36176	41928	24.32667907
	ZH_100000	9990046	167	9228	81997	91392	109.3098521
	ZH_1000000	94268840	196	13407	160359	173962	541.893287
WORD	AR_100	1776	869	1534	1669	4072	0.4361493124
	AR_1000	23460	5868	16064	20063	41995	0.5586379331
	AR_10000	206549	26108	116814	164062	306984	0.6728331118
	AR_100000	2035190	97997	776730	1383009	2257736	0.9014295737
	AR_1000000	19410502	304978	4297319	10005650	14607947	1.328763173
	EN_100	2071	682	1567	1869	4118	0.5029140359
	EN_1000	27398	3292	13148	19834	36274	0.7553068313
	EN_10000	236569	12014	83397	155493	250904	0.9428665944
	EN_100000	2339109	37264	428249	1117802	1583315	1.477349106
	EN_1000000	21943139	122457	1818166	6505850	8446473	2.59790554
	ES_100	2232	710	1605	1974	4289	0.5204010259
	ES_1000	29461	3839	13199	20634	37672	0.7820397112
	ES_10000	263024	15116	83900	160078	259094	1.01516824
	ES_100000	2588791	49499	439584	1116177	1605260	1.612692648
	ES_1000000	24654449	142809	1840029	6268684	8251522	2.987866844
	FR_100	2298	745	1737	2072	4554	0.5046113307
	FR_1000	32011	3881	14535	22608	41024	0.780299337
	FR_10000	273195	13998	86815	170729	271542	1.006087456
	FR_100000	2684982	42870	428339	1150965	1622174	1.655175092
	FR_1000000	25595487	118204	1703399	6171437	7993040	3.202221808
	RU_100	1854	886	1589	1734	4209	0.4404846757
	RU_1000	24746	5433	15511	20035	40979	0.603870275
	RU_10000	216638	23403	108516	162401	294320	0.7360627888
	RU_100000	2150746	81342	670857	1306351	2058550	1.044786865
	RU_1000000	20421965	236088	3295028	8617195	12148311	1.681053852
	ZH_100	1751	630	1434	1614	3678	0.4760739532
	ZH_1000	23568	3181	13998	19341	36520	0.6453450164
	ZH_10000	207714	13137	96829	160642	270608	0.7675826287
	ZH_100000	2038639	46941	554739	1278188	1879868	1.08445859
	ZH_1000000	19361101	134492	2527710	8401311	11063513	1.749995774
49

L Number of model parameters
Representation
Number of lines
100	1,000	10,000	100,000	1,000,000
Number of model parameters for dataset A
BYTE
1,000	10,000	100,000	1,000,000
1,000	10,000	100,000	1,000,000
™ES-FR™-ZH-AR™™-ZH™"-ZH
pAAAAAEEEEE
FR-AR
FR-EN
FR-ES
FR-RU
FR-ZH
RU-AR
RU-EN
RU-ES
RU-FR
RU-ZH
ZH-AR
ZH-EN
ZH-ES
ZH-FR
ZH-RU
AR-ZH_pinyin
EN-ZH-Pinyin
ES-ZH_pinyin
FR-ZH_pinyin
RU-ZH_pinyin
AR-ZH.wubi
EN-ZH.wubi
ES-ZH.wubi
FR-ZH.wubi
RU-ZH.wubi
EN-AR.cpl256
ES-AR.cpl256
FR-AR.cpl256
RU-AR.cpl256
ZH-AR.cpl256
AR-RU.cpl251
EN-RU.cpl251
ES-RU.cpl251
FR-RU.cpl251
ZH-RU∞1251
44,226,639	44,245,589	44, 30θ,Π8
44,235,864	44,254,814	44, 300, Θ18
44,237,Θ14	44,258,Θ14	44,317,318
44,247,1≡S	44,273,264	44, 348,068
44,766,814	45, 42θ,464	46, 507,743
44,230,230	44,258,927	44, 322,969
44,232,280	44, 241,502	44, 287, OΘ4
44,234,330	44, 245,602	44, 303,494
44,243,555	44, 25Θ,Θ52	44, 334,244
44,763,230	45, 416,152	46, 4Θ3,Θ1Θ
44,234,838	44, 263,535	44,318,873
44,227,663	44, 236,885	44, 2Θ1,1Θ8
44,238,Θ38	44, 250,210	44, 2ΘΘ,3Θ8
44,248,163	44, 264,560	44, 330,148
44,767,838	45, 420,760	46, 48θ,823
44,235,862	44, 265,583	44, 327,065
44,228,687	44, 238, Θ33	44, 2ΘΘ,3ΘO
44,237,Θ12	44, 248,158	44, 2Θ1,1ΘO
44,24θ,187	44, 266,608	44, 338,340
44,768,862	45, 422,808	46, 4Θ8,O15
44,240,4TO	44,272,751	44, 342,425
44,233,2Θ5	44, 246,101	44,314,750
44,242,520	44, 255,326	44, 306,550
44,244,570	44, 25θ,426	44, 322, Θ5O
44,773,470	45, 429,976	46,513,375
44,500,054	44, 850,287	45,421,20θ
44,4Θ2,87Θ	44, 823,637	45,3Θ3,534
44,502,104	44, 832,862	45,385,334
44,504,154	44, 836, Θ62	45,401,734
44,513,37θ	44, 851,312	45,432,484
44,230,739	44, 254,814	44,2ΘO,668
44,227,155	44, 241,502	44,276,844
44,231,763	44, 246,H0	44,272,748
44,232,787	44, 248,158	44,28O,Θ4O
44,237,395	44, 255,326	44,2Θ6,3OO
44,252,264	44, 271,214	44,310,143
44,248,680	44,257,902	44,2Θ6,31Θ
44,253,288	44, 262,510	44,2Θ2,223
44,254,312	44, 264,558	44,300,415
44,258,Θ2O	44,271,726	44,315,775
77772 49994 61116 05550 49994
61669 80558 53335 85058 29492
02258 66692 75703 75703 96885
σΓMMσΓc-f I⊂ΓC-Γ40CΓ 自MMocT-T MOCΓ4στc-f I4⊂ΓC-focτ
67788 77787 76788 76788 87888
4444t-ʃ 4444t-ʃ 4444M 4444M 4444M
44444 44444 44444 44444 44444
46,028, 980
46,01θ, 755
46,025, ΘO5
46,027, Θ55
46,040, 255
44,355,742
44,351,134
44,354, 206
44,355, 230
44,361,374
44,372,142
44,367, 534
44,370, 606
44,371,630
44,377,774
44,480,248	44,223, 056	44,240,470
44,490,498	44,232, 281	44,24Θ,6Θ5
44,4Θ2,548	44,234, 331	44,254,820
44,4Θ6,648	44,244,581	44,269,170
49,237,273	44,285,581	44,3O2,ΘΘ5
44,477,683	44,221,517	44,247,1≡S
44,4Θ3,O58	44,233,817	44,243,03θ
44,4Θ5,1O8	44,235,867	44,248,164
44,4θθ,208	44,246,117	44,262,514
4θ,23θ,833	44,287,117	44,2Θ6,33Θ
44,482,803	44,226,125	44,251,747
44,487,928	44,22θ,200	44,238,422
44,500,228	44,240,475	44,252,772
44,504,328	44,250,725	44,267,122
4Θ,244,Θ53	44,291,725	44,3OO,Θ47
44,483,827	44,227,149	44,254,307
44,488,Θ52	44,230,224	44,24O,Θ82
44,4θθ,202	44,239,449	44,250,207
44,505,352	44,251,749	44,26θ,682
49,245,977	44,292,749	44,303,507
44,485,875	44,232,26θ	44,261,475
44,4θl,000	44,235,344	44,248,150
44,501,250	44,244,56θ	44,257,375
44,503,300	44,246,61θ	44,262,500
4θ,248,025	44,2Θ7,86Θ	44,310,675
46,853,875	44,252,74θ	44,278,371
46,85θ,000	44,255,824	44,265,046
46,86θ,250	44,265,04θ	44,274,271
46,871,300	44,267,099	44,279,396
46,875,400	44,277,349	44,2Θ3,746
44,808,248		
44,810,808		
44,815,Θ28		
44,816,Θ52		
44,81θ,000		
44,785,698 44,788,258		
44,793,378		
44,794,402		
44,796,450		
	44,230,742	44,25θ,43θ
	44,235,350	44,264,047
	44,236,374	44,266,607
	44,241,4Θ4	44,273,775
	44,261,Θ74	44,2ΘO,671
	44,242,531	44,267,120
	44,244,067	44,260,464
	44,248,675	44,265,072
	44,249,699	44,267,632
	44,275,299	44,291,696
515
2θ0
44, 307,590
44, 326,040
44, 331,165
44, 306,054
44, 2Θ3,754
44, 306,054
44, 324,504
44, 32θ,62θ
44, 301,446
44, 2Θ8,371
44, 301,446
44,31Θ,8Θ6
44, 325,021
44, 307,590
44, 304,515
44, 2Θ5,2ΘO
44, 326,040
44, 331,165
44,316,806
44,313,731
44, 304,506
44,316,806
44, 340,381
44,31θ,366
44,316,2θl
44, 307,066
44,31θ,366
44, 337,816
91511 04606
22684 40941
59028 38131
50768 86280
22233 33335
44444 44444
44444 44444
44,336,795
44,336,795
44,338, 845
44,342, Θ45
44,350,120
44,333, 717
44,33θ, 867
44,341,Θ17
44,346,017
44,353,1Θ2
44,333, 717
44,33θ, 867
44,341,Θ17
44,346,017
44,353,1Θ2
44,334, 741
44,340, 8θl
44,340, 8θl
44,347, 041
44,354, 216
44,336, 78θ
44,342, Θ3Θ
44,342, Θ3Θ
44,344, Θ8Θ
44,356, 264
44,340, 373
44,346, 523
44,346, 523
44,348, 573
44,352, 673
44,360,874
44,360,874
44,367,024
44,360,874
44,388,54θ
44,358,822
44,362,Θ22
44,369,072
44,362,Θ22
44,390,597
44,358,822
44,362,Θ22
44,369,072
44,362,Θ22
44,390,597
44,361,8Θ4
44,365,ΘΘ4
44,365,ΘΘ4
44,365,ΘΘ4
44,3Θ3,66Θ
44,358,822
44,362,Θ22
44,362,Θ22
44,369,072
44,390,597
44,372,646
44,376,746
44,376,746
44,382,8Θ6
44,376,746
45,247,147
45,275, 847
45,311,722
45,456,247
45,1Θ3, 847
45,343, 078
45,180,103
45,215, 978
45,360, 503
45,OΘ8,103
45,357, 414
45,165,739
45,230,314
45,374, 83θ
45,112, 43θ
45,375, 334
45,183, 659
45,212, 359
45,392,759
45,130, 359
45,447, 526
45,255, 851
45,284, 551
45,320, 426
45,202, 551
45,316, 454
45,124,779
45,153, 479
45,18θ, 354
45,333, 879
885560610410110≡≡≡1∙843,-≡941-"565165840-541141816866666
481ss,s,803,-瑞-046s104,--8ggls-≡,s≡,
69,784,815 72, Θ64,365 71,818,415 81,458,540 70, Θ35,8ΘO	132,473,233 145,014,108 138,21θ,383 177,653,183 142,3Θ2,158
77,015,037	163,629,262
65,748,237	113,Θ18,812
64, 602,287	107,124,087
74,242,412	146,557,887
63,719,762	111,2Θ6,862
78, 603,261	16Θ,8Θ3,582
64,156,θll	107,642,257
66,1ΘO,511	113,388,≡
75, 830,636	152,822,207
65, 307,986	117,561,182
78, 030,845	166,499,534
63, 584,4Θ5	104,248,20 θ
66,764,045	116,78θ,084
75,258,220	14θ,428,15θ
64,735,570	114,167,134
82, 846,205	186,197,198
68, 3θθ,855	123,945,873
71,579,405	136,486,748
70, 433,455	129,692,023
69, 550, Θ3O	133,864,798
77, 5ΘO,O13	168,583,886
63,143,663 66, 323,213	106,332,561 118,873,436
65,177,263	112,078,711
74, 817,388	151,512,511
417,226,067
242,477,917
2ΘO,4Θ8,65O
311,35θ,450
286,13θ,325
302,834,525
2⅞S⅛2Θ8
234,122,173
354,953,273
≡≡≡≡≡603≡≡
-411-≡,s,655
,,
8529520"就638≡ls舞5ra霁6461 湍ra8al黑838≡lra3163
gs,ls--480845,--581s,s-5,o44s,55,m,
,,,,,,,,,,,,,,,,,,,,
舞牒湍蜜源≡s≡≡
≡wwww- Cwwxa窗gx-
Ivrvr-Γ4 3一T-ΓM⊂Γ ocΓ⊂Γc-focΓ-T ocΓ⊂Γc-focΓ-T -ΓIVΓVΓ4 ocf⊂Γ-T-TocT
66676 66666 66666 66666 76666 66666
22777 72777 94999 16611 38833 38833
33505 25727 36838 79972 80033 46694
71135 38802 89357 89358 35993 78224
06603 52270 79592 79592 46119 13993
75176 48499 38388 16566 94393 49739
15561 88894 06016 06016 52652 84889
的的的的的的
需镒黑眈既牌 ≡≡≡≡≡潞
g
II 232 III 32 III 32 III 32 22223 21123
89,242,512
89,628,937
89,514,137
89,999,987
89,324,512
6a温s645--湍4ras≡≡≡≡≡581115881731
--86,-s,060s,-0020re--,41s,s,
UnderreVieW as a ConferenCe PaPersICLR 2021
44,365, 492
44,365, 4Θ2
44,366,516
44,368, 564
44,372,148
44,373, 695
44,376,767
44,376,767
44,377,791
44,383, 423
44,437,747
44,437,747
44,440,81θ
44,437,747
44,451,571
44,457,224
44,45θ,272
44,459,272
44,462,344
44,473,096
UnderreVieW as a ConferenCe PaPersICLR 2021
BPE
100	1,000	10,000	100,000	1,000,000
≡s≡4638m6m166lm5m∙m≡l≡1∙5≡4∙58∙5歌re,l≡53∙≡≈

440640540465165
rags⅛
,
期第14584s
ls2"s,
湍1∙6易
s∙s,s,
ro8"≡
--40
412m2112既
ms,s,
,
188≡sl
g⅛mg≡,
ls83°器需薪5"628sl≡黑1018016°1108≈"≡
-W,-B⅛ *岁 B⅛--0gE⅛
≡5s601≡54264o16°1m0湍-020s-428glsls≡s
s,-,1,-61--s,8150s,-噢院歙s,80s,
10085080°湍sl488slra8148848≡"4-ls黑546黑846-8∙6421
≡,-61,∙≡,341-s,s,M■--618∙446,s,-484∙s,s,
Number of model parameters for dataset B
WORD
1,000	10,000	100,000	1,000,000
BYTE
100	1,000	10,000	100,000	1,000,000
CHAR
100	1,000	10,000	100,000	1,000,000
340黑黑5∙∙"4,4∙s43∙≡s≡≡≡ 惠黑240∙11s061∙11
s,≡,214∙s,s,g4,∙341-is-486∙---4g4∙022∙s,-
-=-43s,s44,s,-s2ro54MQ,-43ls,ls
s58430∙584≡i≈60884,i≡-631≡s1m-sml≡s
Il藤 ≡is≡sl!!sslsii
--2∙4
5,51ra
-6re≡"
-s1
前-
-™-
4∙8≈≡l
-s,lg
338≡l-
8∙8sl≡
5664ra6,24
≡≡≡≡≡
5868668560
，，，，，
g"0"∙≡,152
mm，,，，
湍456≡
-墨4g
S?
555280480≡s
s,30s,
限限
38,112re,sl
s,-,,4
限限
黑395湍
s,34-
鬻Mg™
,
51γ常842,lγ
-s,4re
-""’晨
3mm3m
211-,s,
645湍黑
,6s,s,
能™™，
18,≡翁58γ
κs,≡,
≡≡≡≡≡
蠹网-
45∙45∙45∙45∙45∙
能™™，
≡,,4g≡,
3霆33
ro∙384ra4湍
-6s,沿，
，，，，，
26∙m42mm∙5m
侬465654,3
，，，，,
然黑640
圜661黑
44,642,227
44,644,275
44,644,275
44,641,715
44,645,811
44,528,452
44,528,452
44,530,500
44,530,500
44,532,036
18γ舞需
≡,s≡,
35545540s黑
17 9 7 5
18 9 8 5
CCrt-ʃ fʃ fʃ 4
4 4 4 4 5
4 4 4 4 4
44444
4 4 4 4 4
≡31282448°

44∙44∙44∙44∙44∙
41∙3ra常lra虢黑期 W25142685100115∙85∙8M温863563513≡黑585然
33333333333333m333333333333333
⅛⅛H ⅛H⅛ H⅛⅛ H⅛⅛ H⅛⅛ H⅛⅛
0 6 4 8 0
0 7 8 8 6
10 6 3 4
% θʃ θʃ -T 4
4 4 5 6 6
3 3 3 3 3
44444
4 4 4 4 4
≡湍585

⅛⅛H
843∙68268ra3M3

44，44，44，44，44，
8110βl山636∙11
232424262,
68ro68g⅛
，，，，，

616216516m1m1
s,s,31,
，，，，，
si≡m3
-24ls,
-黑必
s,64m10
⅜⅛>⅜
第黑lra
s,41s,
1,6-6ra∙ra
--M■
44，44，44，44，44，
548≡≈
la,24ls
∙4,-s
-s,11
⅛>⅜⅜
823l≡sl
46-s,
黑黑045
2g6ss,
，，，，，
0,2-s
，，，，，
2166m816116641
，，，，，
44,297,866
44,300,426
44,306,570
44,317,322
44,323,466
24lss,
湍 ≡1∙∙
s,ls,2g
H⅜⅜
∙56,31∙81黑
--W
，，，，，
,ra*s胧
-墨11
⅛>⅜⅜
395™黑
s,64s,
⅛>⅜⅜
515l∙"540,4°
s,-84
°2462254™"21∙,2∙45
21973 99651
-s,,侬--
4ra歌30482∙
s,40s,
503≡l≡
s,40-
135585585m0∙60
--M
ml1464γl⑵846嘉常902588-sl244≡≡556ss804s就2∙6≡ 黑646≡l黑
Ss Ss ≡⅛ss,32谶 s⅛ B⅛ 5⅛ s⅛
⅛⅛√' ⅛H⅛ H⅛⅛ H⅛⅛ H⅛⅛
≡188l≡818ssl3re≡l≡l4γ4窗™834ξii∙黑,80s∙88≡iih4084488568128224584
s,a,ra徽s,"2l---"ssras-ms,-2si≡,
⅜⅛√' ⅛H⅛ H⅛⅛ H⅛⅛ H⅛⅛
2244,454∙04∙34∙
23242426g3
l≡,",m
两-ls
lsla,ra
≡4∙8s
ls24s,
,63488ra8-
is-m
58,-s
事，湍，
∙∙6,8,23β3∙∙6,8,23β3
04513 48957
VΓσΓ4自自一ΓvΓ⊂Γc-fc-f
er of PARAMS
AR-EN
AR-ES
AR-FR
AR-RU
AR-ZH
EN-AR
EN-ES
EN-FR
EN-RU
EN-ZH
ES-AR
ES-EN
ES-FR
ES-RU
ES-ZH
FR-EN
FR-ES
FR-RU
FR-ZH
RU-AR
RU-EN
RU-ES
RU-FR
RU-ZH
ZH-AR
ZH-EN
ZH-ES
ZH-FR
ZH-RU
AR-ZH-Pinyin
EN-ZH-Pinyin
ES-ZH.pinyin
FR-ZH-Pinyin
RU-ZH-Pinyin
AR-ZH_wubi
EN-ZH.wubi
ES-ZH.wubi
FR-ZH.wubi
RU-ZH.wubi
EN-AR.cpl256
ES-AR.cpl256
FR-AR.cpl256
RU-AR.cpl256
ZH-AR.cpl256
AR-RU.cpl251
EN-RU.cpl251
ES-RU.cpl251
FR-RU.cpl251
ZH-RU∞1251
5
ι
Representation
Number of lines
™ES-FR-ru-ZH-AR™™-ZH™"-ZH
pAAAAAEEEEE
FR-AR
FR-EN
FR-ES
FR-RU
FR-ZH

RU-AR
RU-EN
RU-ES
RU-FR
RU-ZH
ZH-AR
ZH-EN
ZH-ES
ZH-FR
ZH-RU
AR-ZH_pinyin
EN-ZH-Pinyin
ES-ZH_pinyin
FR-ZH_pinyin
RU-ZH_pinyin
AR-ZH.wubi
EN-ZH.wubi
ES-ZH.wubi
FR-ZH.wubi
RU-ZH.wubi
EN-AR.cpl256
ES-AR.cpl256
FR-AR.cpl256
RU-AR.cpl256
ZH-AR.cpl256
AR-RU.cpl251
EN-RU.cpl251
ES-RU.cpl251
FR-RU.cpl251
ZH-RU∞1251
Number of model parameters for dataset C
CHAR	BYTE	WORD	BPE
100	1,000	10,000	100,000	1,000,000	100	1,000	10,000	100,000	1,000,000	100	1,000	10,000	100,000	1,000,000	100	1,000	10,000	100,000	1,000,000
44,237,394
44,243,544
44,246,61θ
44,257,8Θ4
44,Θ6O,O1Θ
44,247,141
44,233,816
44,236,8θl
44,248,166
44,Θ5O,2Θ1
44,250,213
44,230,738
44,23Θ,Θ63
44,251,238
44,Θ53,363
44,251,74θ
44,232,274
44,238,424
44,252,774
44,Θ54,8ΘΘ
44,257,381
44,237,906
44,244,056
44,247,131
44,Θ6O,531
44,608,101
44,588,626
44,594,776
44,5Θ7,851
44,60S,126
44,233,2Θ4
44,223,566
44,226,638
44,228,174
44,233,806
44,254,81θ
44,245,0θl
44,248,163
44,24θ,6θθ
44,255,331
44,277,862
44,288,112
44,293,237
44, 321,937
45,766,162
44,297,356
44, 268,656
44,273,781
44, 302,481
45,746,706
44, 302,476
44,263,526
44, 278, ΘO1
44, 307,601
45,751,826
44, 305,036
44, 266,086
44,276,336
44,310,161
45,754,386
44,31θ,372
44, 280,422
44,290,672
44,295,797
45,768,722
45, 040,780
45, 001,830
45,012,080
45,017,205
45, 045, ΘO5
44,287,087
44, 267,631
44,272,751
44, 275,311
44, 28θ,647
44, 309,637
44, 2θ0,181
44, 2Θ5,3O1
44, 2Θ7,861
44,312,1Θ7
44,319,874
44, 327,049
44, 333, lθθ
44, 361,8θθ
46, 694,799
44, 338,342
44, 308,617
44,314,767
44, 343,467
46, 676,367
44, 341, Θ26
44, 305,026
44,318,351
44, 347,051
46, 679,951
44, 344, ΘΘ8
44, 308,0θ8
44,315,273
44, 350,123
46, 683,023
44, 35θ,334
44, 322,434
44, 32θ,60θ
44, 335,759
46, 697,359
45,524,646
45,487,746
45,4Θ4,Θ21
45,501,071
45,529,771
44,333,lθθ
44,314,767
44,318,351
44,321,423
44,335,759
44,348,574
44,330,142
44,333,726
44,336,798
44,351,134
44,432, 062
44,432, 062
44,441,287
44,457, 687
47,805, 337
44,470, 537
44,3Θ3, 662
44,402, 887
44,41θ, 287
47,766, 937
44,470, 537
44,3Θ3, 662
44,402, 887
44,41θ, 287
47,766, 937
44,475,145
44,3Θ8, 270
44,3Θ8, 270
44,423, 8Θ5
47,771,545
44,483, 337
44,406, 462
44,406, 462
44,415, 687
47,779,737
46,155, 52θ
46,078, 654
46,078, 654
46,087, 879
46,104, 27θ
44,487, 412
44,44θ,012
44,44θ,012
44,453, 620
44,461,812
44,48θ, 462
44,451,062
44,451,062
44,455, 670
44,463, 862
44,644,154
44,640,054
44,664,654
44,620,579
4θ,141,854
44,704,175
44,580,150
44,604,750
44,560,675
4Θ,O81,Θ5O
44,702,127
44,582,202
44,602,702
44,558,627
49,079,902
44,714,415
44,5Θ4,4ΘO
44,5ΘO,3ΘO
44,57O,Θ15
4Θ,OΘ2,1ΘO
44,6Θ2,3ΘΘ
44,572,474
44,568,374
44,5Θ2,Θ74
49,070,174
46,Θ5O,831
46,83O,ΘO6
46,826,806
46,851,406
46,807,331
44,749,729
44,68θ,825
44,687,777
44,700,065
44,678,04θ
44,744,604
44,684,700
44,682,652
44,6Θ4,Θ4O
44,672,Θ24
44,237, ΘOΘ	44,276,331	44,315,783	44,352, 675	44,380,342	45,632,910	51,878,481	72, 594,782	137,710,265	347,843,439	45,372,198	49,998,240	64, 770,100	86,735, 506	89,389,603
44,242, 00θ	44,285,556	44,317,833	44,353, 700	44,377,267	45,6Θ8,51O	52,453,506	75, 866,582	146,52θ,365	361,666,58θ	45,373,223	50,205,2θ0	66, 481,850	88,137,706	89,650,978
44,248,15θ	44,2θ0,681	44,318,858	44,352, 675	44,381,367	45,720, 035	52,31θ,231	74, 570,982	140,332,215	338,Θ56,68Θ	45,424,473	50,212,465	66,174,350	87,724, 631	8θ,533,103
44,257,384	44,314,256	44, 33θ,358	44,35θ, 850	44,379,317	45,8Θ8, 385	54,253,406	85, 360,132	180,206,765	46O,1Θ8,814	45,577,198	51,4O6,5ΘO	73,204,825	8θ,355, 406	90,003,578
44,286,084	44,326,556	44, 347,558	44,371,125	44,3θl,617	45,608,310	51,865,156	73, 6Θ3,582	141,800,015	346,543,739	45,661,248	50,236,040	65,1Θ4,45O	87,429, 431	8Θ,281,Θ78
44,243,552	44,286,5θl	44, 322,452	44,353,188	44,379,316	45,765,777	53,237,931	80,104,076	165,772,904	434,781,036	45,404,004	50,534,838	68, 840,755	88,226,797	89,768,710
44,236,377	44,275,316	44,311,177	44,353,188	44,378,2θl	45,565, ΘO2	51,096,706	68, 371,926	118,521,42θ	274,8Θ8,461	45,341,47θ	49,669,738	62, 41θ,130	86,64θ, 322	89,272,610
44,242,527	44,280,441	44,312,202	44,352,163	44,382,3θl	45,587, 427	5O,Θ62,431	67, 076,326	112,324,279	252,188,561	45,392,729	49,676,913	62,111,630	86,236,247	8θ,154,735
44,251,752	44,304,016	44, 332,702	44,35θ, 338	44,380,341	45,765,777	52,8Θ6,6O6	77, 865,476	152,1Θ8,82Θ	373,430,686	45,545,454	50,871,038	6θ, 142,105	87,867, 022	8θ,625,210
44,280,452	44,316,316	44, 340, ΘO2	44,370,613	44,3Θ2,641	45,475,702	50,508,356	66,1Θ8,Θ26	113,792,079	259,775,611	45,62θ,504	4θ,700,488	61,131,730	85,Θ41,O47	88,ΘO3,61O
44,245,600	44,2θl,lθθ	44, 323,476	44,353, 700	44,377,780	45,798, 545	53,525,163	81,738,380	170,178,152	441,685,868	45,404,516	50,638,262	69, 695,795	88,Θ27, 213	89,899,270
44,234,325	44,270,699	44,310,151	44,352, 675	44,37θ,830	45,533, 070	5O,8O8,Θ13	66, 734,430	114,107,577	267,980,143	45,34O,Θ66	4θ,566,112	61,562,420	85,947, 538	8Θ,141,7Θ5
44,244,575	44,285,04θ	44,313,226	44,352, 675	44,380,855	45,620,1Θ5	51,24θ,663	68, 710,630	116,729,527	259,093,393	45,3Θ3,241	49,780,337	62, 966,670	86,Θ36, 663	8Θ,285,2Θ5
44,253,800	44,308,624	44, 333,726	44,35θ, 850	44,378,805	45,798, 545	53,183,838	79, 499,780	156,604,077	380,335,518	45,545,Θ66	50,974,462	69, 997,145	88,567, 438	89,755,770
44,282,500	44,320,θ24	44, 341, Θ26	44,371,125	44,3θl,105	45,508, 470	50,795,588	67, 833,230	118,197,327	266,680,443	45,630,016	4Θ,8O3,Θ12	61,986,770	86,641,463	8θ,034,170
44,248,672	44,293,759	44, 323, Θ88	44,353,188	44,37θ,828	45,809,297	53,458,0θl	81,0θl,212	167,082,600	43O,341,ΘΘ6	45,430,116	50,641,846	69, 542,1Θ5	88,720, 877	8Θ,84O,3ΘO
44,237,397	44,273,259	44,310,663	44,352,163	44,381,878	45,543, 822	50,741,841	66, 087,262	111,012,025	256,636,271	45,366,566	4Θ,56Θ,6Θ6	61,408,820	85,741,202	8Θ,O82,Θ15
44,241,4Θ7	44,282,484	44,312,713	44,353,188	44,378,803	45,609, 422	51,316,866	69, 35θ,062	llθ,831,125	270,459,421	45,367,591	49,776,746	63,120,570	87,143, 402	8Θ,344,2ΘO
44,256,872	44,311,184	44, 334,238	44,35θ, 338	44,380,853	45,809,297	53,116,766	78, 852,612	153,508,525	368,θθl,646	45,571,566	50,978,046	69, 843,545	88,361,102	8Θ,6Θ6,8ΘO
44,285,572	44,323,484	44, 342,438	44,370,613	44,3Θ3,153	45,51θ, 222	50,728,516	67,186,062	115,101,775	255,336,571	45,655,616	4θ, 807,496	61,833,170	86,435,127	88,975,290
44,253,280	44,305,535	44, 334,228	44,356,772	44,378,804	45,8Θ8, 385	54,424,235	86, 480,524	187,000,424	4ΘO,ΘO3,Θ16	45,506,404	51,238,326	73, 054,003	8θ,535, 46θ	90,075,398
44,242,005	44,285,035	44, 320, ΘO3	44,355,747	44,380,854	45,632,Θ1O	51,707,985	71,476,574	13O,Θ2Θ,84Θ	317,1Θ8,1Θ1	45,442,854	50,166,176	64, Θ2O,628	86,555,794	89,317,923
44,246,105	44,2θ4,260	44, 322, Θ53	44,356,772	44,377,779	45,6Θ8,51O	52,283,010	74,748,374	139,748,949	331,021,341	45,443,87θ	50, 373,226	66, 632,378	87,957, ΘΘ4	89,579,298
44,252,255	44,2θθ,385	44, 323,978	44,355,747	44,381,87θ	45,720, 035	52,148,735	73, 452,774	133,551,799	308,311,441	45,4Θ5,12Θ	50, 380,401	66, 324,878	87,544,θlθ	8θ,461,423
44,2θ0,180	44,335,260	44, 352,678	44,374,197	44,3Θ2,12Θ	45,608,310	51,6Θ4,66O	72, 575,374	135,01θ,5θθ	315,8Θ8,4Θ1	45,731,ΘO4	50, 403,976	65, 344, Θ78	87,249,719	8Θ,21O,2Θ8
44,267,616	44,311,67θ	44, 338,324	44,362, 404	44,384,Θ48	45,753, 48θ	53,231,275	80, 652, Θ4O	167,815,784	434,131,820	45,548,388	50, 653,622	69, 052,723	88,573, 421	8Θ,714,Θ5O
44,256,341	44,2θl,17θ	44, 324, θθθ	44,361,379	44,386,ΘΘ8	45,488,014	50,515,025	65, 648, ΘΘO	lll,745,20θ	260,426,0θ5	45,484,838	4θ, 581,472	60,θlθ,348	85,593,746	88,957,475
44,260,441	44,300,404	44, 327,049	44,362, 404	44,383,Θ23	45,553,614	51,0θ0,050	68, 920,790	120,564,30θ	274,249,245	45,485,863	4θ, 788,522	62, 631, OΘ8	86,ΘΘ5, Θ46	8θ,218,850
44,266,5θl	44,305,52θ	44, 328,074	44,361,379	44,388,023	45,575,13θ	50,955,775	67, 625,1ΘO	114,367,15θ	251,53θ,345	45,537,113	49,795,697	62, 323,5Θ8	86,582, 871	89,100,975
44,275,816	44,32θ,104	44, 348,574	44,368, 554	44,385,Θ73	45,753, 48θ	52,88Θ,Θ5O	78, 414,340	154,241,70S	372,781,470	45,68θ,838	50, Θ8Θ,822	69, 354,073	88,213, 646	8θ,571,450
44,248,677
44,250,725
44,253,797
44,258,405
44,272,741
44,255,334
44,24θ,702
44,251,750
44,254,822
44,273,766
44,2ΘΘ,Θ16
44,304,524
44,307,084
44,318,860
44,325,004
44,315,281
44,305,041
44,30θ,64θ
44,312,20θ
44,330,12θ
44, 340, ΘO2
44, 341, Θ26
44, 342,438
44, 352,678
44, 356,774
44, 352,683
44, 346,027
44, 347,051
44, 347,563
44, 361,8θθ
44,456, 713
44,457,225
44,456, 713
44,460, 2Θ7
44,465, 929
44,405, 975
44,405, 463
44,405, 975
44,405, 463
44,414, 679
44,636,5θl
44,635,055
44,637,103
44,636,079
44,642,223
44,4Θ2,O67
44,4Θ3,OΘ1
44,4θl,555
44,4Θ3,6O3
44,4Θ8,723
UnderreVieW as a ConferenCe PaPersICLR 2021
Under review as a conference paper at ICLR 2021
M Erraticity
Length has been an issue since the dawn of the encoder-decoder approach for NMT (Cho et al., 2014).
Most work on length bias, except for that by e.g. Sountsov & Sarawagi (2016), seems to have focused
on the evaluation of generated translation output and monitored performance degradation with respect
to sequence length, often arguing that beam size plays a role (Koehn & Knowles, 2017; Murray &
Chiang, 2018). (Related work in Stahlberg & Byrne (2019) provides a good summary on this issue.)
While there could also be confounds in search, our experiments show that a kind of length bias can
surface already with CLMing, without generation taking place. To our knowledge, length bias has
not been expressed as a sample-wise non-monotonicity across a large data size range as ours. While
the connection between erraticity in CLMs and length bias in NMT models remains to be verified on
a case-by-case basis, the knowledge of length also contributing to robustness (not just consistently
poor/poorer performance) could support further experimentation/replication of any study. Failed
attempts to reproduce results may be explainable by erraticity.
One may argue that erraticity may not be relevant when each model is more optimally trained (as
opposed to being treated with our one-setting-for-all regime). But we do want to stress that this very
stark contrast between erratic and non-erratic behavior is possible, prompting a question on fairness:
is there a one-for-all setting under which the languages with non-erratic behavior shown in our study
would demonstrate erraticity and vice versa?
To the best of our knowledge, the meta phenomenon of erraticity, as a sample-wise non-monotonicity
measured intrinsically with cross-entropy and contributing to large variance across runs, is a novel and
original discovery and contribution to research in robustness. We hope our work would inspire further
evaluation on other models/architectures, reflection and theories on our assumption of unbounded
computation (e.g. Xu et al. (2020)), as well as new understanding and solutions that take data statistics
and realistic computational aspects into account. We defer a more comprehensive analysis of erraticity
with further experiments to future work.
M.1 Erraticity as large variance: evidence from different runs of the same
DATA
To confirm that erraticity is not due to data-specific reasons, e.g. when certain data segments might be
“easier” to model than others, we show figures from 2 runs (Figs. 18a and 18b) on the same dataset
of wildly differing performance that only differ in seed. Note that changes in the y-direction can vary
much, indicating large variance across runs.
By establishing that high variance holds across sample sizes, we showcased how it’d be possible to
just test on 2 or 3 data points of smaller sizes to get a gauge on the robustness in higher order. It
serves as a signal of when the system is being “stress-tested” and hyperparameters need re-tuning.
Spot-testing on a couple of smaller data sizes can indeed save much time and energy. Take our run B0
byte models as an example: the training of the 102-line model for EN-RU took 15 minutes, 103 40
minutes, 104 1 hour 50 minutes, and 105 3 hours 36 minutes. One can imagine how these would just
be a fraction of training time for bigger models. (Likewise, for our ratio of target training token count
to number of parameters — knowing when a representation might be prone to DD within a data size
range could help prevent practitioners from prematurely declaring experimental results as negative or
from unnecessarily rerunning an experiment because bigger data did not lead to better results.)
M.2 Additional experiment with length filtering to 300 bytes
Figure 19a and 19b show results of additional experiment with subset of data in byte (UTF-8)
representation length-filtered to 300, including dev data:
Erraticity remains for AR and RU. Scores are lower, though they cannot be compared with the
experiments in the main paper due to difference in dev data size (3,077 lines vs. 1,804 lines here).
Number of total lines for train is 5,533,672 lines for each language, from which we took the initial
102-106. As in our main experiments, we filtered out only whole lines, i.e. not by discarding the
tails of longer lines. 300 bytes aren’t long sequences, but without data transform or hyperparameter
tuning, things can look unfair. The EN translation of the longest RU line in this dataset is: “47. It is
53
Under review as a conference paper at ICLR 2021
s=q=.IaqEnU
SRC
AR
EN
ES
FR
RU
ZH
1e+03	1(
number of lines
(a) run A0
1e+03
1e+05
1e+03
1e+05
number of lines
(b) run A1
ZH
1e+03
1e+05
SRC
AR
EN
ES
FR
RU
ZH
Figure 18:	Same data with differing seeds
noted that there is a lack of information provided by the Government of Trinidad and Tobago with
regard to the legal status of the Convention in the domestic legislation.”
54
Under review as a conference paper at ICLR 2021
1500000-
1000000-
500000 ∙
s=q=.IaqEnU
1(
number of lines
1(
TRG
—AR
——EN
——ES
--FR
----RU
--- ZH
SRC
AR
EN
ES
FR
RU
ZH
1500000-
1000000-
500000 ∙
1500000-
1000000-
500000∙
s=q=.IaqEnU
SRC
AR
EN
ES
FR
RU
ZH
1e+03	1e+05	1e+03	1e+05	1e+03	1e+05
number of lines
(a)	(b)
Figure 19:	Additional experiment with maximum length of 300 bytes (with no hyperparamter tuning,
in our blind one-setting-for-all evaluation). Considering there are languages with much higher
character sequence length than RU, there is food for thought for the design of next-generation
Multilingual Plane.
55
Under review as a conference paper at ICLR 2021
N Experiments with one-layer Transformer
We performed 1 run with dataset A in 4 sizes (102-105 lines, seed=13) with the primary representations
of characters, bytes, and words, on 1-layer Transformers (num-layers 1:1, all other hyperparameters
remain the same as for our main experiments). We compared this against run A0 in 4 sizes with the
same seed. (Based on how our null hypothesis is set up, the higher the number of runs, the more
likely it is for there to be disparity. Important is that we evaluate based on an equal number of runs
and on the same data for all candidates.) Results are shown in Table 5 with no statistically significant
disparity observed on the models trained with 1 layer across the board.
Many are under the impression that big data is the cause to the neutralization of language instances in
DL/NNs. But, as this set of experiments shows, it is possible for there to be no statistically significant
differences between them, with as little as our smallest data size of 100 lines.
Table 5: Number of language pairs out of 15 with significant differences, with respective p-values. BYTE6layers
is the representation with erratic ARtrg and RUtrg .
p-value	CHAR6layers		BYTE6layers		WORD6layers		CHAR1layer		BYTE1layer		WORD1layer	
	src	trg	src	trg	src	trg	src	trg	src	trg	src	trg
0.05	0	0	0	6	0		0	0	0	0	0	0
0.01	0	0	0	6	0	1	0	0	0	0	0	0
0.001	0	0	0	5	0	0	0	0	0	0	0	0
1000000-
500000-
number of lines
TRG
-AR
SRC
-AR
EN
-ES
FR
RU
-ZH
1e+03
number of lines
TRG
一AR
-EN
——ES
--FR
--- RU
---ZH
SRC
一AR
EN
一ES
FR
RU
一ZH
1e+15
2000000-
150BB -
100BB
50BB-
(c) WORD1layer
TRG
一AR
--EN
-ES
--FR
--RU
-ZH
SRC
一AR
EN
一ES
FR
RU
一ZH
(a)	CHARIlayer
AR
2510010-
(b)	BYTEIlayer
AR
2510100-
2000000-
1500000-
1000000-
500000-
25101010-
20101010-
15101010-
1B0B0-
510010-
0-
1e+03
SRC
-AR
EN
-ES
-FR
RU
-ZH
1e+□5	1e+03	1e+□5	1e+03
number of lines
1e+□5
510100-
2000000-
1500000-
IBB00-
0-
FR
2510100-
2BB00-
1510100-
1BB00-
510100-
1e+03
SRC
一AR
EN
一ES
一FR
RU
一ZH
1e+□5	1e+13	1ebs	1e+13
number of lines
1e+□5
250BB-
H0BB -
150BB -
0-
(d)	CHAR1layer by target
(e)	BYTE1layer by target
100BB -
50BB-
2500000-
2000000-
150BB -
100BB -
50BB-
0-
(f) WORD1layer by target
Figure 20:	One-layer Transformer models
56
Under review as a conference paper at ICLR 2021
O	PAQs (Previously asked questions)
O.1 One setting for all
Q: Normally, one trains a model with the objective of optimizing based on the training and evaluation
data with hyperparameter tuning. The experiments here used one setting for all. Some model
configurations might train better and converge close to their optima while other configurations might
not reach their full potential. Can this not create a distortion in the results?
A: For conventional engineering practice, we agree that hyperparameter tuning would be a sine qua
non. However, the evaluation objective is the relational distance between languages, hence we need
to see it in a different light. Here is a loose analogy:
***
Assume 3 objects in 3 different locations in space.
Relative evaluation from one setting allows one to capture the distance between these objects. It does
not matter whether these three objects are in their “best” states.
For example, if one were to use a camera to capture these 3 objects and one does not adjust the setting
(using just one random aperture, shutter speed, and focus), i.e. no tuning to capture any of these 3
specifically, nor does one try to model these 3 to their individual bests separately, what would result
could be a picture that captures one of these 3 objects more favorably than the others, or it could
be that all of these would be blurred. But either way, there is a degree of blurriness to be measured,
giving us an idea of the relative distance between the objects. Such relative measurement is the
evaluation strategy that our paper adopts.
Now, to add to the camera analogy, say one of the objects is running water, which was extra blurry
[erraticity]: we suggest freezing the water, so even from the one arbitrary angle, it could be captured
better. And it worked.
Also, while one might generally like to have a “pretty” photo, one that is e.g. taken with sub-optimal
lighting, say, overexposure, can have a telling effect as it can bring out details in something dark, like
a black box.
***
Alternatively, one can tune hyperparameters for each model individually such that each model would
be a more optimized one and then compare these models. In that case, one would be interpreting
the differences between language in terms of hyperparameters, and the paper would be one that
is algorithm-centric. That is of course also a possibility. Our approach, however, is a data-centric
one. We would, first of all, like to understand the nature of language data, i.e. what it is about
language, if there is anything at all, that makes it a different data type than other data, and what kind
of structural constraints, if any, that we need to take into consideration. Then with findings from this
data perspective, we try to relate back to the algorithm and make connections so to create a more
holistic picture.
O.2 Translationese / word order
Q: Multitexts are parallel texts or translations with the same meaning. There is little to no variation
in word order, hence they are just “Translationese” (Gellerstam, 1986). That is why they turn out to
be the same, with no performance disparity.
A: Our findings do show that when the semantics is properly controlled, such as in multitexts, the
factors influencing performance are statistical properties related to sequence length and vocabulary,
e.g. |V | or TTR, and the languages tested can be different. Semantic equivalence is also not a reason
why we should expect neutralization of source language instances, as that would mean we should
expect equal results across target languages.
We agree that faithfulness is often a priority in producing good translations. Whether the translations
are produced by humans or machines, only a single best translation can surface as the translation
of choice. There may be many other competing hypotheses, but regardless of whether it is done
through an automatic ranking algorithm by a machine or through a human expert, the purpose of
57
Under review as a conference paper at ICLR 2021
translation is the same. However, styles and preferences in translations can vary. While faithfulness
is generally preferred in the translations of legal texts, more freedom with skillful rearrangement
of and play on words (or rather, character or sub-character sequences) or sounds being a criterion
for literary texts could be appreciated by certain readers. We agree that it could be very interesting
and necessary to model these variations, and we understand that languages can surface in many
multimodal forms beyond the confines of texts as well. But with a data-driven perspective, to model
this broader variation in language, we need corresponding datasets — we suggest contrast sets where
the difference in e.g. sequential order is explicit. And for evaluation, we would require an even more
systematic meta evaluation, one that spans different datasets.
But the argument that language or data could be different beyond how it appears in one dataset is
irrelevant in the evaluation of experiments involving said dataset.
58
Under review as a conference paper at ICLR 2021
P Understanding the phenomena with alternate representations
(extended version)
[Appendix P is an extended version of § 4.]
To understand why some languages show different results than others, we carried out a secondary
set of control experiments with representations targeting the problematic statistical properties of the
corresponding target languages.
Character level On the character level, it is well known that ZH differs from the other languages
in its high |V |, in this study it has an averaged mean±std of 2550±144912 across all 5 data sizes
from all 3 datasets compared to 170±87 from all other 5 languages combined, may these be in Latin
or Cyrillic alphabet or the Abjad script. But what is often not known is that the character sequence
length of logographic languages such as ZH is typically short (think and compare the sequence
length of the Ancient Egyptian hieroglyphs or the Demotic script with that of the Greek script on
the Rosetta Stone). Here in our case, the averaged mean sequence length in characters for ZH is
35±19, compared to 129±71 from the other 5 languages. Heuristics to mitigate high |V | often
involve decomposition, which automatically resolve the problem of short sequence length. We tried 2
methods to lower character |V | with representations in ASCII characters — Pinyin and Wubi. The
former is a romanization of ZH characters based on their pronunciations and the latter is an input
algorithm that decomposes character-internal information into stroke shape and ordering and matches
these to 5 classes of radicals (Lunde, 2008). We replaced the ZH data with these formats only on
the target side and reran the experiments involving ZH as a target language (ZHtrg) on the character
level.
Results in Figure 2 and Table 1 show that the elimination of disparity on character level is possible if
ZH is represented through Pinyin (transliteration), as in Subfigure 2c. But Wubi exhibits erraticity
(Subfigure 2a). Wubi in our data has a maximum sequence length of 688 characters. As we shall also
show in our byte-level analysis below, there are reasons to attribute length as cause to erraticity.
Decomposition into strokes may seem like a natural remedy analogous to decomposing an EN word
into character sequences, but one needs to be mindful of not exceeding an optimal length given finite
computation. Considering the ZH in the UN data is represented in simplified characters, decomposing
traditional characters would surely complicate the problem. As there are also sub-character semantic
and phonetic units (Zhang & Komachi, 2018) that can be exploited for information and aligned with
character sequences of other alphabets, qualitative advances in this area can indeed be a new state of
the art.
Byte level On the byte level, we observe irregularity for AR and RU. We find minimum sequence
length of the target language to be one of the highest metrics correlating positively with the total
number of bits (ρ = 0.60).13 Our data is based on 300 characters as maximum length per line.
While we wanted to retain at least 75% of the UN data after length filtering, this length still renders
a maximum sequence length that exceeds 100 words (the default maximum length for the word
alignment model, GIZA++ (Och & Ney, 2003), in the traditional SMT pipeline). Translated into bytes
with UTF-8 encoding, data with 300 characters maximum gives us, e.g. for the 106-line datasets, an
averaged mean±std of 185±106 in length for AR and 246±142 for RU, considerably larger than that
for ZH (94±53) and for EN/ES/FR (≈145.41±77). With UTF-8 encoding, each character in AR, RU,
and ZH contains 2 or more bytes. ZH typically has shorter line length in characters, compensating
for the total byte sequence in length, even when most ZH characters are 3 bytes each. However,
AR and RU generally have long line length in characters, so when converted to bytes, the sequence
length remains long even when most of the characters might be just 2 bytes each. Results from our
pairwise comparisons indicate 8 (non-directional) language pairs to be significantly different (see
Table 1 under “BYTE”): ES-RU, EN-RU, FR-RU, RU-ZH, AR-RU, AR-EN, AR-ZH, and AR-FR
— all involving AR or RU. (Appendix I lists also the language pairs with significant differences for
other representations.)
12Figures are rounded to whole number. Complete tables of data statistics are provided in Appendix D.
13Top-3 correlates for each representation can be found in Appendix F.
59
Under review as a conference paper at ICLR 2021
Leveraging language-specific code pages can be a useful practical trick, a reminder that there are
alternatives to UTF-8 for analyses and back-end processing if data is clean and homogeneous and if
success of larger-scale prediction is not a concern. But one more sustainable alternative is to design a
more adaptive and flexible character encoding scheme in general, taking into account the statistical
profiles such as length (wrt characters and bytes) and sub-character (atomic/elementary/compound)
information of all (or as many as possible) of the world’s languages.
Word level The main difference between word and character/byte models is the absence of length
as a top contributing factor correlating with performance. Instead, what matters more are metrics
concerning word vocabulary, with top correlate being OOV token rate in the target language (ρ =
0.66). This is understandable as word segmentation neutralizes sequence lengths — the longer
lengths in phonetic alphabetic scripts are shortened through multiple-character groupings, while
the shorter lengths in logographic scripts (cf. difference in length for the 3 scripts on the Rosetta
Stone, logographic scripts are typically shorter than phonetic ones) are lengthened by the insertion
of whitespaces. To remedy the OOV problem, we use BPE, which learns a fixed vocabulary of
variable-length character sequences (on word level, as it presupposes word segmentation) from the
training data. It is more fine-grained than word segmentation and is known for its capability to model
subword units for morphologically complex languages (e.g. AR and RU). We use the same vocabulary
of 30,000 as specified in Junczys-Dowmunt et al. (2016). This reduced our averaged OOV token rate
by 89-100% across the 5 sizes. The number of language pairs with significant differences (p ≤ 0.001)
reduced to 7 from 8 for word models, showing how finer-grained modeling has a positive effect on
closing the disparity gap.
60
Under review as a conference paper at ICLR 2021
Version 1.1 (graphs to be updated, score tables added)
61