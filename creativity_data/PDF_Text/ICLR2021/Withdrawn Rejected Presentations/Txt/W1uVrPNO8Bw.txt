Under review as a conference paper at ICLR 2021
Implicit Regularization of SGD via Ther-
MOPHORESIS
Anonymous authors
Paper under double-blind review
Ab stract
A central ingredient in the impressive predictive performance of deep neural net-
works is optimization via stochastic gradient descent (SGD). While some theoret-
ical progress has been made, the effect of SGD in neural networks is still unclear,
especially during the early phase of training. Here we generalize the theory of
thermophoresis from statistical mechanics and show that there exists an effective
force from SGD that pushes to reduce the gradient variance in certain parameter
subspaces. We study this effect in detail in a simple two-layer model, where the
thermophoretic force functions to decreases the weight norm and activation rate
of the units. The strength of this effect is proportional to squared learning rate and
inverse batch size, and is more effective during the early phase of training when
the model’s predictions are poor. Lastly we test our quantitative predictions with
experiments on various models and datasets.
1	Introduction
Deep neural networks have achieved remarkable success in the past decade on tasks that were out
of reach prior to the era of deep learning. Yet fundamental questions remain regarding the strong
performance of over-parameterized models and optimization schemes that typically involve only
first-order information, such as stochastic gradient descent (SGD) and its variants.
In particular, optimization via SGD is known in many cases to result in models that generalize better
than those trained with full-batch optimization. To explain this, much work has focused on how
SGD navigates towards so-called flat minima, which tend to generalize better than sharp minima
(Hochreiter & Schmidhuber, 1997; Keskar et al., 2017). This has been argued by nonvacuous PAC-
Bayes bounds (Dziugaite & Roy, 2017) and Bayesian evidence (Smith & Le, 2018). More recently,
Wei & Schwab (2019) discuss how optimization via SGD pushes models to flatter regions within a
minimal valley by decreasing the trace of the Hessian.
However, these perspectives apply to models towards the end of training, whereas it is known that
proper treatment of hyperparameters during the early phase is vital. In particular, when training a
deep network one typically starts with a large learning rate and small batch size if possible. After
training has progressed, the learning rate is annealed and decreased so that the model can be further
trained to better fit the training set (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al.,
2016b;a; You et al., 2017; Vaswani et al., 2017). Crucially, using a small learning rate during the
first phase of training usually leads to poor generalization and also result in large gradient variance
practically (Jastrzebski et al., 2020; Faghri et al., 2020).
However, limited theoretical work has been done to understand the effect of SGD on the early phase
of training. Jastrzebski et al. (2020) argue for the existence of a “break-even” point on an SGD
trajectory. This point depends strongly on the hyperparameter settings. They argue that the break-
even point with large learning rate and small batch size tends to have a smaller leading eigenvalue
of the Hessian spectrum, and this eigenvalue sets an upper bound for the leading eigenvalue beyond
this point. They also present experiments showing that large learning rate SGD will reduce the
variance of the gradient. However their analysis focuses only on the leading eigenvalue of the
Hessian spectrum and requires the strong assumption that the loss function in the leading eigen-
subspace is quadratic.
1
Under review as a conference paper at ICLR 2021
Meanwhile Li et al. (2020) studied the simple setting of two-layer neural networks. They demon-
strate that in this model, training with large learning rate in the early phase tends to result in better
generalization than training with small learning rate. To explain this, they hypothesize a separation
of features in the data: easy-to-generalize yet hard-to-fit features, and hard-to-generalize, easier-
to-fit features. They argue that a model trained with small learning rate will memorize easy-to-
generalize, hard-to-fit patterns during phase one, and then generalize worse on hard-to-generalize,
easier-to-fit patterns, while the opposite scenario occurs when training with large learning rate. How-
ever, this work relies heavily on the existence of these two distinct types of features in the data and
the specific network architecture. Moreover, their analysis focuses mainly on learning rate instead
of the effect of SGD.
In this paper, we study the dynamics of model parameter motion during SGD training by borrowing
and generalizing the theory of thermophoresis from physics. With this framework, we show that
during SGD optimization, especially during the early phase of training, the activation rate of hidden
nodes is reduced as is the growth of parameter weight norm. This effect is proportional to squared
learning rate and inverse batch size. Thus, thermophoresis in deep learning acts as an implicit
regularization that may improve the model’s ability to generalize.
We first give a brief overview of the theory of thermophoresis in physics in the next section. Then we
generalize this theory to models beyond physics and derive particle mass flow dynamics microscop-
ically, demonstrating the existence of thermophoresis and its relation to relevant hyperparameters.
Then we focus on a simple two-layer model to study the effect of thermophoresis in detail. Notably,
we find the thermophoretic force is strongest during the early phase of training. Finally, we test our
theoretical predictions with a number of experiments, finding strong agreement with the theory.
2	Thermophoresis in Physics
Thermophoresis, also known as the Soret effect, describes particle mass flow in response to both
diffusion and a temperature gradient. The effect was first discovered in electrolyte solutions (Lud-
wig, 1859; Soret, 1897; Chipman, 1926). However it was discovered in other systems such as gases,
colloids, and biological fluids and solid (Janek et al., 2002; Kohler & Morozov, 2016).
Thermophoresis typically refers to particle diffusion in a continuum with a temperature gradient. In
one method of analysis, the non-uniform steady-state density ρ is given by the ”Soret Equilibrium”
(Eastman, 1926; Tyrell & Colledge, 1954; Wurger, 2014),
Vρ + PST VT = 0 ,
(1)
where T is temperature and ST is called the Soret coefficient.
In other work by de Groot & Mazur (1962), mass flow was calculated by non-equilibrium theory.
They considered two types of processes for entropy balance: a reversible process stands for the
entropy transfer and an irreversible process corresponds to the entropy production, or dissipation.
The resulting mass flow induced by diffusion and temperature gradient was found to be
J = -DVρ - ρDTVT ,
(2)
where D is the Einstein diffusion coefficient and DT is defined as thermal diffusion coefficient.
Comparing the steady state in 1 and setting the flow to be zero, the Soret coefficient is simply
The Soret coefficient can be calculated from molecular interaction potentials based on specific
molecular models (Wurger, 2014).
3	Thermophoresis in General
In this section, we first study a kind of generalized random walk that has evolution equations for a
particle state with coordinate q = {qi}i=1,...,n as
qt+1 = qt - ηγf (qt, ξ)
(4)
2
Under review as a conference paper at ICLR 2021
where f is a vector function, γ and ξ are random variables, and η is a small number controlling the
step size. Notice that this is a generalized inhomogeneous random walk for the particle. Before
further analysis, it is noted that the evolution equations 4 is similar to SGD updates in machine
learning and we will show this in the next section.
To isolate the effect of thermophoresis, we assume the random walk is unbiased, in which case
P (γf (q, ξ) = a) = P (γf (q, ξ) = -a),	(5)
for an arbitrary vector a. Thus there is no explicit force exerted on the particle. This simplification
was used to demonstrate a residual thermophoretic force in the absence of a gradient. Including
gradients is straightforward and corresponds to an external field that creates a bias term. We also
denote the probability density, which we also call the mass density, as ρ(q) and
gi(q):=Xγ γ2fi2(q,ξ)dμ(γ,ξ),	(6)
so that ηgi(q) is the standard deviation of the random walk in the ith direction.
From a position q, we consider a subset of coordinate indices, U ⊆ {1, . . . , n}, wherein
sign(fi(q, x)) = sign(fj (q, x)) and ∂igj(q) ≥ 0	(7)
for all i, j ∈ U. We note here that indices will correspond to parameters when we study learning
dynamics. The first property is necessary for our derivation. The second condition will be used at
the end to conclude that each gi decreases.
In order to study the dynamics of the particle and its density function, we focus on the probability
mass flow induced by the inhomogeneous random walk. We will show that there is always a flow
from regions with larger gi (q) to those with smaller gi (q) for i ∈ U, which is a generalization of
thermophoresis in physics.
Since η 1, the movement of the particle will have a mean free path of gi (q) in ith direction.
Therefore the random walk equation 4 becomes
qi = qi - ηgi(q)ζi,	(8)
where i = 1, . . . , n and ζi is a binary random variable with P(ζi = -1) = P (ζi = 1) = 0.5.
Moreover, from Eq. 7, we also have that ζi = ζj for all i and j ∈ U.
Next we will show that the flow projecting on the subspace U is always toward smaller gi (q). Notice
that although U can be multi-dimensional, the degree of freedom of the particle dynamics is 1 within
U due to the sharing of the ζs, and therefore the mass flow projecting on it is also 1-dimensional.
For each i ∈ U, we define the average flow in this dimension to be the mass that enters qi from qi-
minus the mass from the opposite direction qi+. From Eq. 8 and the assumption that η 1, only
mass close to qi will move across qi at each step. We let the farthest mass that will flow across qi
in step i be qi + ∆i+ and qi - ∆i- , where ∆i+ and ∆i+ are positive. ∆i+ and ∆i- are thus defined
implicitly by the equations: ∆i+ = ηgi(q+ ∆+) and ∆i- = ηgi(q- ∆-), respectively. Notice that
if the random walk were homogeneous, we would have ∆i+ = ∆i-. In our inhomogeneous case, we
have ∆+ 〜△-〜ηgi(q) UP to leading order of η, and the next to leading order will be calculated
in order to compute the difference between ∆i+and ∆i- .
Now we are ready to calculate the mass flow through q. The mass flow Projecting onto the subsPace
U is calculated by the mass through q from q + ∆+ minus the mass from q - ∆- where △i+ and
△i- are as above for i ∈ U. It is straightforward to show that1
△i+ - △i- = 2η2 Xgj(q)∂jgi(q) + O(η3).	(9)
j∈U
With this, we can comPute the flow density, J, through q, finding
J「»Xg-η2 j≡SFρ(q)+O(η3),	(IO)
3
Under review as a conference paper at ICLR 2021
Figure 1: Diagram of mass flow in a generalized inhomogeneous random walk used in the derivation
of the Soret coefficient.
where the derivation can be found in Appendix A.3. This can be understood as described in Diagram
1. Notice that this probability mass flow consists of two terms at order η1 2 3. The first represents diffu-
sion and the second corresponds to our goal in this section, namely the flow due to thermophoresis.
By the second property of the gi in Eq. 7, we find that the coefficient of thermophoresis (Soret
coefficient), which is defined as
2 ij∈Ugi(q)gj(q)∂jgi(q)
:= -η --~/	-----
2 Pi∈U gi (q)
≤ 0,
(11)
(12)
is negative. This means that there is an effective force exerted on a particle at position q towards the
smaller variance regime (by analogy, the colder area). The coefficient is proportional to η2 .
4 Mathematical Model
4.1	Two-layer model
To study the physics behind SGD optimization in detail, we consider the simple setting of one-
hidden layer neural networks. The network is a function f : RM → R parameterized as follows:
f(x;V,W,b) = Vσ(Wx + b),
NM
= XViσ(XWijxj +bi).
We also write f (x) for simplicity. The network has a scalar output, which is widely used in regres-
sion and binary classification. x is the network input with dimension M, W and b are the weights
and biases in the first layer with dimension N ×M and N respectively, where N is the number of hid-
den nodes in the hidden layer, and σ is the ReLU activation function defined as σ(a) = max(0, a).
The dataset is drawn i.i.d. from the data distribution, {(x, y)∣(x, y)〜D(x, y)}. In this paper We
consider two cases, where either Xi ≥ 02 or Xi 〜N(0,1)3. Here y ∈ Y and we denote the marginal
distribution of y as DY . Finally, we have the loss function L : R × Y → R+ .
4.2	Training
We consider optimization via SGD, where the gradient of the loss on a batch of size |B| is given by
1A brief derivation can be found in Appendix A.2.
2Usually in convolutional neural networks or intermediate layers.
3Often found when the data are normalized.
4
Under review as a conference paper at ICLR 2021
1	|B|
VLB (V, W, b)=国 NVf L(f(xi),yi)VfX).
(13)
In our two-layer model, we have
M
VVi f (x) = σ(XWijxj + bi),
j=1
M
VWij f (x) = Vixjσ0(	Wikxk + bi),
M
Vbif(x) = Viσ0(X Wikxk + bi).
k=1
For an input vector x, we call the hidden node i activated when σ0(PkM=1 Wikxk + bi) = 1, or
equivalently Wik xk + bi > 0. We thus define the activation rate of the network to be
1 N	M
σ0 = N X Eχσ0(X WikXk + bi) .	(14)
i=1	k=1
This is an important concept to which we will return.
Henceforth, we drop the index i, since the dynamical equations are invariant with respect to node
index, and write V := Vi , Wj := Wij and b := bi by abuse of notation. We also denote
hv(V,W,b) :=Ex[VVf(x)]2,
hw(V, W, b) :=Ex[VWf(x)]2,
hb(V, W, b) :=Ex[Vbf(x)]2,
where Ex denotes average over input x. We have the following property for the functions h:
Property 4.1. Given W, if V12 ≤ V22 and b1 ≤ b2, we have
hv(V1, W, b1) ≤hv(V2,W,b2),
hw(V1, W, b1) ≤hw(V2,W,b2),
hb(V1, W, b1) ≤hb(V2,W,b2),
σ7(v1, w,bι) ≤ σ0(v2, w,b2).
Here we define a ≤ b as min(b - a) ≥ 0.
It is straightforward to see the following:
Property 4.2. When the case of xi ≥ 0 is considered, if V12 ≤ V22, w1 ≤ w2 and b1 ≤ b2, we
have
hv(V1,w1,b1) ≤hv(V2,w2,b2),
hw(V1,w2,b1) ≤hw(V2,w,b2).
hb(V1,w2,b1) ≤hb(V2,w,b2),
σ7(V1, W,bι) ≤ σ7(V2, W,b2).
In our analysis, we focus for simplicity on binary classification tasks, where the loss is typically
binary cross-entropy: L(f, y) = y lnp(f) + (1 - y) ln(1 - p(f)) andp(f) = 1/(1 + exp(f)). We
thus have
VfL(f,y) =p(f)-y.	(15)
Substituting into Eq. 13, the mini-batch gradient becomes
1	|B|
VLB(V, W, b) =同 X(Pi-yi)Vf(xi).	(16)
Our results also hold straightforwardly for squared error.
5
Under review as a conference paper at ICLR 2021
5 Thermophoresis in Deep Learning
In this section, we will show that the parameters in the one hidden layer model and their dynamics
approximately satisfy the criteria of the previous section and that the biases are pushed negative and
V 2 is suppressed during training, the effects of both of which are proportional to squared learning
rate η2 and inverse batch size 1/|B|.
The gradient that dominates model training is defined in 16. Because training samples are i.i.d., the
variance of the gradient is
1	|B|
var [VLb (V, W, b)] =Var 画 X(Pi-期mf (Xi) ,	(17)
= TITvar[(P — y)Vf(x)]	(18)
|B|
The gradient has two components: p - y corresponding to γ in equation 4 and Vf(x) corresponding
to f(q, ξ). We assume that the dataset is unbiased, in which case P(y = 0) = P(y = 1) = 0.5 and
P(p - y = a) = P(p - y = -a), and that p - y and Vf (x) are independent in the first period of
training given that the dataset is complex and can’t be learned by linear model. It is straightforward
to see that it satisfies Eq. 5.
Next we will show that V and b are always in the set of U, i.e. they satisfy the conditions of Eq. 7.
First, if Vi ≥ 0, we have
M
VVi f (x) = σ(XWijxj + bi),	(19)
j=1
≥ 0.	(20)
and
M
Vbif(x) = Viσ0(X Wikxk + bi),	(21)
k=1
≥ 0.	(22)
Since we also have Property 4.1, the conditions in Eq. 7 are satisfied. If Vi < 0, we consider a
coordinate transform that maps Vi to Vi = -Vi. Itis easy to show that Eq. 7 is again satisfied after
this transform.
Next we consider W. The gradient of f with respect to Wij is the product of Vbif and xi. If xi
for i = 1, . . . , M are always ≥ 0, which is usually the case in convolutional neural networks, it is
easy to show that Wij is also in set U and smaller Wij corresponds to smaller variance according to
Property 4.2. If Xi 〜N(0,1), on the other hand, W is excluded from U.
For the following, We only consider the case where Xi ~ N(0,1), and
	1	Γ~f	M gv(Vi, Wi,bi) = pB∣t∕ [(p - y)σ(∑S WijXj+ bi)] 2dμ(x,y) ,	(23) :=$φι(Wi,bi) ,	(24) |B| 1	Γ~f	M gb(Vi, Wi,bi) = PBitJ [(p — y)Viσ0Q2 Wijxj + bi)]2dμ(x,y) ,	(25) V := P^i φ2(Wi,bi) ,	(26)
where g is defined as in Eq. 6. Inserting these into Eq. 10, we find the thermophoresis flow density
tobe	Jt 工 Ψ,	(27)
6
Under review as a conference paper at ICLR 2021
where ψ
Viφlφφ2+Viφlφ2∂bφι+V 3φ2∂b φ2
2 √Φ2+V2Φ2
ρ. This flow biases the model toward smaller bi and smaller
Vi4 with a strength proportional to squared learning rate η2 and inverse batch size. We also note that
ψ can be bounded by a function multiplying with a scalar J(P 一 y)2μ(x, y). It is clear that this scalar
measures the L-2 distance between model predictions and sample labels and decreases on average
during training as prediction getting better. Thus thermophoresis is more effective during the early
phase of training.
Therefore there exists an effective force that pushes to decrease the model’s activation rate, defined
in equation 14, and reduces the weight norm of the second layer. The strength of this force scales as
2
η2
FfX 同.
(28)
In Li & Liang (2018), Theorem 4.1 presents a linear relation between learning rate and training
iterations for a target training error and small learning rate. This implies that if one uses a learning
rate k times larger, the model will require k times fewer optimization steps for the same training
performance. Together with our results, this implies the following: for the same model and initial-
ization, comparing two optimization schemes with η1 ≤ η2 each achieving a given training error,
the activation rate for scheme 1 will be at least as large as that for scheme 2, i.e. σ1 ≥ σ2. Similarly,
denoting the weight norm for scheme 1(2) by v1(v2), we have that v1 ≥ v2.
Model sparsity can mean two different things: sparsity of the weights, and frequency with which
units are activated, called the activation rate. Intuitively, a sparser model has a smaller capacity
Bizopoulos & Koutsouris (2020); Kurtz et al. (2020); Aghasi et al. (2017); Lin et al. (2017). Fur-
thermore, certain forms of model pruning have been shown to improve generalization Frankle &
Carbin (2019); Frankle et al. (2020). Therefore one might surmise that a smaller activation rate in
general correlates with generalization. Moreover, in Appendix A.5, we construct an upper bound of
the Hessian norm and this upper bound monotonously depends on activation rate and weight norm.
This also sheds light on the connection between sparsity, weight norm, and generalization.
Our theory can also be generalized beyond two-layer models. We have shown that there exists
an effective force in deep neural networks from SGD that reduces the gradient variance and have
derived quantitative properties of it.
6	Experiments
The essential result from the previous section is that there exists an effective force from SGD, anal-
ogous to thermophoresis, that pushes to decrease the gradient variance, and in one-hidden-layer
neural networks decreases the model’s activation rate and reduces the weight norm of the second
layer. The strength of the force is proportional to squared learning rate and inverse batch size. In
this section, we present experiments to test these results. Further experiments can be found in the
appendix.
First we consider a one hidden layer model with input dimension 100 and 100 hidden units. The
input data, x, is distributed as N (0, I) where I is identity matrix, and the label is randomly chosen
from {0, 1}. Batch size is set to 1 and the learning rate is varied from 0.025 to 0.1. We calculate the
activation rate and L2 norm of the vector V after each training iteration. The result for activation
rate is shown in the first row of Fig. 2. The leftmost plot shows activation rate as a function of true
iteration on the x-axis, and we see that activation rate decreases during training, and the decreasing is
more rapid with larger learning rate. In the middle plot we rescale the x-axis by a factor proportional
to learning rate η5. This rescaling factor is to offset the movement difference due to learning rate
difference. Itis clear that even after this rescaling, we still observe that larger learning rates decrease
activation rate faster. Finally, on the rightmost plot we rescale the x-axis with a factor proportional
to squared learning rate η2. We see that all trajectories now overlap, which matches our prediction
in the previous section that decreasing rate is proportional to η2 .
4larger Vi if Vi < 0.
5For example, if raw iteration number for η = 0.05 is 1000 and rescaled iteration number is also 1000, the
rescaled iteration number for η = 0.1 is 1000 then its true iteration number is 500.
7
Under review as a conference paper at ICLR 2021
0.45
ω
≡
CL 0.40
O 20∞	40∞	60∞	80∞
9CBa Cof-S
——Ir=O.I
lr=0∙5
——lr=0∙025
20∞	40∞	60∞	80∞
Iterations
0.00∞16
ω
c
® 0.00∞14
O 0.00∞12
0
0.00∞10
O 10	20	30	40	50
Rescaled Iterations (Ir)
10	20	30	40	50
Rescaled Iterations (Ir)
lr=0.01
lr=0∙02
0.00∞16
ω
O
c
® 0.00∞14
>
δ 0.00∞12
∂
0.00∞10
0.0∞7
8 0.0006
c
® 0.0∞5
§
1 0.0004
U
α)
⅛ 0.0003
CB
0 0.0002
0.0∞l
Iterations
0.0∞7 I
8 0.0006
® 0.0∞5
§
1 0.0004
(D
⅛ 0.0003
CB
0 0.0002
0.0∞l
O
0.0007
8 0.0006
C
® 0.0∞5
1 0.0004
U
α>
¾ 0.0003
g
0 0.0002
0.0∞l
10	20	30	40	50
Rescaled Iterations (Ir)
60
Rescaled Iterations flr^2)
Figure 2: All rows include rescaled x-axes as described in the main text. Top Row: Plots of activation
rate as a function of (rescaled) training iterations with different learning rates. The model is a
two-layer fully-connected network with 100 hidden units. Training data is drawn from a normal
distribution. Middle Row: Plots of average gradient variance as a function of (rescaled) training
iterations with different learning rates in 6-layer fully-connected neural networks. Training data is
drawn from normal distribution. Bottom Row: Same as middle row but for 6-layer convolutional
neural networks trained on Fashion-MNIST.
We next test our results for deep neural networks beyond the two-layer model. Instead of activation
rate and weight norm, we plot the gradient variance as predicted by our theory. Networks archi-
tectures are 6-layer fully-connected with hidden layer sizes of 100 and 6-layer convolutional with
10 channels with kernel size of 5*5 and stride 1 except the last fully-connected layer output. The
results are shown in the second row of Fig. 2 and the third row of Fig. 2 respectively.
Further experiments can be found in Appendix A.6, where we show that the evolution of the weight
norm, scaling with batch size, and other results are consistent with our theoretical predictions. We
also study other models and other datasets including CIFAR10.
7	Conclusion
In this paper we generalized the theory of thermophoresis from statistical mechanics and showed that
there exists an effective thermophoretic force from SGD that pushes to reduce the gradient variance.
We studied this effect in detail for a simple two-layer model, where the thermophoretic force serves
to decrease the weight norm and the activation rate of the units. We found that the strength of this
effect is proportional to square of the learning rate, inversely proportional to batch size, and is more
effective during the early phase of training when the model’s predictions are poor. We found good
agreement between our predictions and experiments on various models and datasets.
8
Under review as a conference paper at ICLR 2021
References
Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep
neural networks with performance guarantee. In Advances in Neural Information Processing
Systems (NIPS), 2017.
Paschalis Bizopoulos and Dimitrios Koutsouris. Sparsely activated networks. arXiv preprint
arXiv:1907.06592, 2020.
John Chipman. The Soret effect. Journal of the American Chemical Society, 48(10):2577-2589,
1926.
S.R. de Groot and P. Mazur. Non-equlibrium Thermodynamics. North Holland Publishing, 1962.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In UAI, 2017.
E. D. Eastman. Thermodynamics of non-isothermal systems. Journal of the American Chemical
Society, 48(6):1482-1493, 1926.
Fartash Faghri, David Duvenaud, David J. Fleet, and Jimmy Ba. A study of gradient variance in
deep learning. arXiv preprint arXiv:2007.04532, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations (ICLR), 2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. The lottery
ticket hypothesis at scale. arXiv preprint arXiv:1903.01611, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016b.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
Jurgen Janek, Carsten Korte, and Alan B. Lidiard. Thermodifusion in Ionic Solids —Model Exper-
iments and Theory, pp. 146-183. Springer Berlin Heidelberg, 2002.
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural
networks. In International Conference on Learning Representations (ICLR), 2020.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
ICLR, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012.
Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin,
William Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh. Inducing and exploiting activa-
tion sparsity for fast neural network inference. In International Conference on Machine Learning
(ICML), 2020.
Werner Kohler and Konstantin I. Morozov. The soret effect in liquid mixtures - a review. Journal
of Non-Equilibrium Thermodynamics, 41, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems (NIPS), 2018.
9
Under review as a conference paper at ICLR 2021
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2020.
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Advances in Neural
Information Processing Systems (NIPS), 2017.
C. Ludwig. Sitz. Ber. Akad. Wiss. Wien, 1859.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In ICLR, 2015.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient
descent. In ICLR, 2018.
C. Soret. Arch Sci Phys Nat, 1897.
H. Tyrell and R. Colledge. Thermal diffusion potentials and the Soret effect. Nature, 173:264-265,
1954.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2017.
Mingwei Wei and David J Schwab. How noise affects the hessian spectrum in overparameterized
neural networks. arXiv preprint arXiv:1910.00195, 2019.
Alois Wurger. Is soret equilibrium a non-equilibrium effect? arXiv preprint arXiv:1401.7546, 2014.
Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv
preprint arXiv:1708.03888,, 2017.
10
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Proof of Property 4.1
Proof. By definition, we have
MM
σ(X Wj xj + b1) ≤ σ(X Wj xj + b2) ,	(29)
j=1	j=1
MM
σ0(X Wj xj + b1) ≤ σ0(XWjxj + b2) .	(30)
j=1	j=1
For hv,
hv(V1, W, b1) = Exgv2(x, V1, W, b1) ,
M
= Exσ2(X Wjxj +b1) ,
j=1
M
≤ Exσ2(X Wjxj + b2) ,
j=1
= hv(V2,W,b2) .
Similarly, we have
M
hwi (V1, W, b1) = ExV12xi2σ0(X Wkxk + b1) ,
k=1
M
≤ ExV22xi2σ0(X Wkxk +b1) ,
k=1
M
≤ ExV22xi2σ0(X Wkxk +b2) ,
k=1
=hwi(V2,W,b2) .
Clearly the inequality also holds for hb.	□
A.2 Derivation of Eq. 9
∆i+ - ∆i- = ηgi(q + ∆+) - ηgi(q - ∆-) ,	(31)
n
= η X(∆j+ +∆j-)∂jgi(q) + O(η∆2) ,	(32)
= 2η2 X gj (q)∂jgi (q) + O(η3) .	(33)
j∈U
11
Under review as a conference paper at ICLR 2021
A.3 Derivation of Eq. 10
J = - 2iδ+iP(Q + δP + 2iδ-1P(Q - 4-),
=1 ∆+∣[ρ(q - ∆-) - ρ(q + ∆+)] + 1(∣∆-∣-∣∆+∣)ρ(q - ∆-)
-2 ∣δ+∣(∣δ+ + δ-|)
ρ(q + ∆+) - ρ(q - ∆-)
∣∆+ + ∆-∣
—
1(∣∆+∣2-∣∆-∣2)
2	∣∆+∣ + ∣∆-∣
P(Q - ∆-)
≈-
11∆+∣(∆+ + ∆-)Vρ(q) - 1 PieU(△+ + △，)(△+ - Z-
∆+l + ∣∆-∣
P(Q - ∆-)
-η2Sχ g2(q) X gi(q)∂iP(q) - η2 ^j^jj^ P(q) + O(η3).
i∈U	i∈U	i∈U gi2 (Q)
A.4 Sanity Check of Generalized Theory
If |U| = 1 and gi(Q) = gi(qi), the model will reduce to aforementioned physics model and the Soret
coefficient reduces to
2
C = yg(q)g0(q) ,	(34)
=[(*Y ,	(35)
≈ VT ,	(36)
where T is the effective temperature in the model. This result is consistent with thermophoresis
model in physics.
A.5 Sparsity, Weight Norm and Their Relation to Generalization
In this section, we demonstrate how sparsity is related to the Hessian norm. We first denote the
model’s probabilistic prediction on a C-class classification as
exp zμ
Pk = LC	μ ,
l=1 exp zl
(37)
where k is the probability for label k, μ is the data index, Z is model output and C is the total number
of categories. We consider cross entropy loss of the form
BC
L(W) = - B XX yμ logpμ,	(38)
μ=1k=1
where y is sample labels and p stands for model probability prediction, similar to the previous
definition. We denote the loss for individual sample to be
C
Lμ = - X y log pμ .	(39)
k=1
The gradient with respect to the model output is
(VzLμ)k = -yμ + P .	(40)
And it is easy to show that the Hessian with respect to output is
(VzLμw = δkipμ - pQιμ.	(41)
12
Under review as a conference paper at ICLR 2021
Therefore the Hessian with respect to model parameters is
Hμ = VwL(Z(W)),	(42)
=Vw(VzL * VwZ),	(43)
= (Vwz)(Vz2L)(Vwz) + VzLV2wz ,	(44)
≈ (Vwzμ)ij(VzLμ)jk(Vwzμ)ki .	(45)
To study the spectrum of the Hessian, we calculate the trace and have	
Tr(Hμ) ≈ Tr((Vwζμ)(VzLμ)(Vwζμ)τ),	(46)
=Tr((V2Lμ)(Vwzμ)τ(Vwzμ)),	(47)
=Tr(P*K) ,	(48)
≤ Tr(P) * Tr(K) ,	(49)
where	
P = Vzz Lμ ,	(50)
Kμν = XX(察)(弁). ∂wl	∂wl l ij	wij	wij	(51)
The trace of K therefore can be calculated by chain rule,	
Tr(K ) = XXX( ∂⅛ )2	(52)
μ l ij	ij	
=X(X(δi[μ])2 X(hj-1)2),	(53)
l iu	j	
where δ and h carry backward and forward information respectively. They are defined as	
δi[μ] = δμnL WLLnL-ισ0∙∙∙ Wn+I”,	(54)
hlj-1 = σWjln-l1σ...Wn1n0xn0 .	(55)
It can further be shown that	
X(δi[μ])2 = X δμnL WnLnL-ισ0 ...Wn+12iσ0 * σ0kin5 ...σ0WLnL-inL δnLμ ,	(56)
iu	iu
=Tr(WLσ0...Wl+1σ0σ0Wl+1 …σ0WL) ,	(57)
≤ Tr(σ0WL W Lσ0)Tr(σ0W L-1W L-1σ0) ... Tr(σ0 W l+1W l+1σ0) ,	(58)
as well as
X(hj-1)2 ≤ ∣∣Xk2Tr(W1 σσW 1)Tr(W2σσW2)...Tr(Wl-1σσWl-1) .	(59)
j
Together with the previous calculations and the definition of K , we have
Tr(K) ≤ ∣X∣2 X ∏n=1ITr(WnσσWn)∏L=l+1Tr(σ0WnWnσ0) ,	(60)
l
= ∣X∣2 X Πln-=11∣σW n∣2F ΠnL=l+1∣W nσ0∣2F .	(61)
l
Finally, we derive an upper bound for the trace of the Hessian,
Tr(Hμ) ≤ Tr(P)∣X∣2 X∏n=1ι∣σWn∣F∏L="∣Wnσ0∣F .	(62)
l
Notice that activation rate and weight norm control the magnitude of ∣σWn ∣2F and ∣Wnσ0 ∣2F .
Therefore smaller activation rate and weight norm lead to tiger upper bound of the Hessian trace
and thus indicate smaller matrix norm. This analysis connects sparsity with Hessian norm, Hessian
trace specifically.
13
Under review as a conference paper at ICLR 2021
SEa co⅛>0o<
2000	40∞	6∞0	80∞
Iterations
0.20
o
0.50
0.45
S. o-4°
U
鲁 0.35
>
W
< 0.30
0.25
20∞	4000	60∞	8000
Rescaled Iterations (Ir)
0.50
0.45
急 0.40
CC
I o∙35
,>
S 0.30
<
0.25
0.20
O 20∞	40∞	6000	80∞
Rescaled Iterations (Iiλ2)
一最5 F⅝M
Figure 3:	Plots of activation rate as functions of training iterations with different learning rate.
1.0
ι.o
9 8 7
Ooo
UWN£61M
lr=0.1
lr=0.5
lr=0.025
9 8 7
Ooo
UWN 1⅛ΦM
0.6 ----- Ir-V.UQ	I l∣'
0	2000	4∞0	6000 BOOO
Iterations
lr=0.1
lr=0.5
lr=0.025
0.6 ----- Ir-V.UQ	IT
0	2000	40∞	6000 BOOO
Rescaled Iterations (Ir)
0.6
1.0
9 8 7
- a -
Ooo
UWN 1⅛ΦM
0	2000	40∞	6000	80∞
Rescaled Iterations Λr^2)
Figure 4:	Plots of L2 norm of V as functions of training iterations with different learning rate.
A.6 More Experiments
In this section, we design extensive experiments to test the obtained results. First of all, we consider
binary classification with BCE loss. The model has the form
f(x) =Vσ(Wx+b) .	(63)
Input dimension is 10 and the number of hidden nodes is also 10. x is distributed as N(0, I) where
I is identity matrix and the label is randomly chosen from {0, 1}.
For the first hyperparameter setting, batch size is 1 and learning rate varies from 0.025 to 0.1. We
calculate the activation rate and L2 norm of the vector V after each training iteration. The result
for activation rate and weight norm are shown in Fig. 3 and Fig. 4 respectively. Both figures
contain three plots. The leftmost plots correspond to plot with raw iteration as x-axis. It is shown
that both activation rate and weight norm are decreasing for all cases. Additionally, the decreasing
rate is larger with larger learning rate. The middle plots correspond to plot with rescaled iteration
as x-axis, where the rescaled factor is proportional to learning rate η6. This rescalation factor is
to offset the movement difference due to learning rate difference. It is clear that even after this
rescalation, we still observe activation rate and weight norm difference for different learning rates.
Lastly, the rightmost plots correspond to plot with rescaled iteration as x-axis, where the rescaled
factor is proportional to squared learning rate η2 . The result that all trajectories overlap with each
other matches our prediction in the previous section, that decreasing rate is proportional to η2 .
For the second hyperparameter setting, learning rate is fixed to be 0.05 and batch size varies from
1 to 3. Again we calculate the activation rate and L2 norm of the vector V after each training
6For example, if raw iteration number for η = 0.05 is 1000 and rescaled iteration number is also 1000, the
rescaled iteration number for η = 0.1 is 1000 then its true iteration number is 500.
14
Under review as a conference paper at ICLR 2021
Figure 5: Plots of activation rate as functions of training iterations with different batch size.
Figure 6: Plots of L2 norm of V as functions of training iterations with different batch size.
iteration. The result for activation rate and weight norm are shown in Fig. 5 and Fig. 6 respectively.
We observe similar tendency as we discussed in the previous hyperparameter setting. The result
shows that both activation rate and weight norm are decreasing for all cases. While there exist
decreasing rate difference in the left plots due to batch size discrepancy. This difference can be
offset be rescaling x-axis according to proportional factor 1/|B|, which is the result in the right
plots and it is consistent with our theory prediction.
Subsequently we consider that second case discussed in the previous section. This is also a consider
binary classification with BCE loss. The model, however, has the form
f(x) = Vσ(Wx) .	(64)
Input dimension is 10 and the number of hidden nodes is also 10. x is distributed uniformly between
0 and 1 as U(0, 1) and the label is randomly chosen from {0, 1}.
The first setting is the same as the first setting in the aforementioned experiment. The result for
activation rate and weight norm are shown in Fig. 7 and Fig. 8 respectively. Similar to previous
experiment, both figures contain three plots. The leftmost, middle, right plots correspond to plot with
raw iteration, rescaled iteration with factor η and rescaled iteration with factor η2 respectively as x-
axis. The result that all trajectories overlap with each other matches our prediction in the previous
section, that decreasing rate is proportional to η2.
The second setting is also similar to the second setting in the previous experiment where we fix
learning rate to be 0.05 and vary batch size from 1 to 3. The result for activation rate and weight
norm are shown in Fig. 9 and Fig. 10 respectively. The result shows that both activation rate and
weight norm are decreasing for all cases. While there exist decreasing rate difference in the left
plots due to batch size discrepancy. This difference can be offset be rescaling x-axis according to
15
Under review as a conference paper at ICLR 2021
O 500	10∞	1500	20∞
Iterations
0.15
o
0.50
0.45
0.35
量 0.30
W
< 0.25
0.20
5∞	l∞0	15∞	20∞
Rescaled Iterations (Ir)

lr=0.1
lr=0.5
lr=0.025
0.20
0.15
O 500	10∞	15∞	20∞
Rescaled Iterations (Iiλ2)
Figure 7:	Plots of activation rate as functions of training iterations with different learning rate.
0.80
E-ION 1⅛①M
O 500 IOoO 1500	2000
Iterations
0.80
∈ 0.75
O
N
£ 0.70
15ι
’35
M 0.65
0.60
O 500 IOOo 1500	2000
Rescaled Iterations (Ir)
0.80
O 500 IOoO 1500	2000
Rescaled Iterations (Iiλ2)
Figure 8:	Plots of L2 norm of V as functions of training iterations with different learning rate.
proportional factor 1/|B|, which is the result in the right plots and it is consistent with our theory
prediction.
Furthermore, we use real image dataset CIFAR10 for the next experiment instead of artificial data.
The model in this experiment has one hidden layer with 300 hidden nodes. We first fix batch size to
be 1000 and vary learning rate η. The result of activation rate and weight norm decreasing are shown
in Fig. 11 and Fig. 12 respectively. We then fix learning rate to be 0.02 and vary batch size. The
result of activation rate and weight norm decreasing are shown in Fig. 13 and Fig. 14 respectively.
It is clear that the result matches our theoretical predictions. We skip detailed analysis here as it is
similar to our discussion in previous experiments.
0.20
0.15
O l∞0	20∞	30∞	4000	50∞	6∞0	70∞	8000
Iterations
O 1∞O	2O∞	30∞	4000	50∞	6000	7000	8000
Rescaled Iterations (ITbs)
Figure 9: Plots of activation rate as functions of training iterations with different batch size.
16
Under review as a conference paper at ICLR 2021
0.85
0.80
LuJON£6①M
0.60
Figure 10: Plots of L2 norm of V as functions of training iterations with different batch size.
0	l∞0	20∞	30∞	4000	50∞	6∞0	70∞	8000
Iterations
U 0.75
o
N
£ 0.70
6
,ξ…
S 0.65
0.85
0.80
0.60
0.55
O l∞0	20∞	30∞	4000	50∞	6∞0	70∞	8000
Rescaled Iterations (ITbs)
Iterations
Rescaled Iterations (Ir)
əeeɑ ⊂o⅛≥wo<
0-525
0.500
0.475
0.450
0.425
0.400
0.375
0.350
——lr=0∙2
—lr=O.1
——lr=0∙05
0.0	2.5	5.0	7.5	10.0 12.5
Rescaled Iterations (Iγλ2)
Figure 11:	Plots of activation rate as functions of training iterations with different learning rate.
Dataset is CIFAR10.
17
Under review as a conference paper at ICLR 2021
Iterations
0.∞1125
O.∞ll∞ '
0.∞1075
U 0.001050
0.∞1025
留 0.0010∞
M 0.000975
0.000950
0.000925
0.0
---lr=0.2
——lr=0.1
——lr=0.05
IU-ION 1⅛① M
0.∞110
0.∞105
O.OO1OO
0.00095
0.0	2.5	5.0	7.5	10.0 12.5 15.0
Rescaled Iterations (lr^2)
2.5	5.0	7.5	10.0 12.5 15.0
Rescaled Iterations (Ir)
Figure 12:	Plots of L2 norm of V as functions of training iterations with different learning rate.
Dataset is CIFAR10.
Figure 13: Plots of activation rate as functions of training iterations with different batch size. Dataset
is CIFAR10.
Figure 14: Plots of L2 norm of V as functions of training iterations with different batch size. Dataset
is CIFAR10.
18