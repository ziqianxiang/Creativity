Under review as a conference paper at ICLR 2021
A Maximum Mutual Information Framework
for Multi-Agent Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a maximum mutual information (MMI) framework for
multi-agent reinforcement learning (MARL) to enable multiple agents to learn
coordinated behaviors by regularizing the accumulated return with the mutual
information between actions. By introducing a latent variable to induce nonzero
mutual information between actions and applying a variational bound, we derive
a tractable lower bound on the considered MMI-regularized objective function.
Applying policy iteration to maximize the derived lower bound, we propose a
practical algorithm named variational maximum mutual information multi-agent
actor-critic (VM3-AC), which follows centralized learning with decentralized exe-
cution (CTDE). We evaluated VM3-AC for several games requiring coordination,
and numerical results show that VM3-AC outperforms other MARL algorithms in
multi-agent tasks requiring coordination.
1	Introduction
With the success of RL in the single-agent domain (Mnih et al. (2015); Lillicrap et al. (2015)), MARL
is being actively studied and applied to real-world problems such as traffic control systems and
connected self-driving cars, which can be modeled as multi-agent systems requiring coordinated
control (Li et al. (2019); Andriotis & Papakonstantinou (2019)). The simplest approach to MARL is
independent learning, which trains each agent independently while treating other agents as a part
of the environment. One such example is independent Q-learning (IQL) (Tan (1993)), which is an
extension of Q-learning to multi-agent setting. However, this approach suffers from the problem of
non-stationarity of the environment. A common solution to this problem is to use fully-centralized
critic in the framework of centralized training with decentralized execution (CTDE) (OroojlooyJadid
& Hajinezhad (2019); Rashid et al. (2018)). For example, MADDPG (Lowe et al. (2017)) uses a
centralized critic to train a decentralized policy for each agent, and COMA (Foerster et al. (2018))
uses a common centralized critic to train all decentralized policies. However, these approaches assume
that decentralized policies are independent and hence the joint policy is the product of each agent’s
policy. Such non-correlated factorization of the joint policy limits the agents to learn coordinated
behavior due to negligence of the influence of other agents (Wen et al. (2019); de Witt et al. (2019)).
Thus, learning coordinated behavior is one of the fundamental problems in MARL (Wen et al. (2019);
Liu et al. (2020)).
In this paper, we introduce a new framework for MARL to learn coordinated behavior under CTDE.
Our framework is based on regularizing the expected cumulative reward with mutual information
among agents’ actions induced by injecting a latent variable. The intuition behind the proposed
framework is that agents can coordinate with other agents if they know what other agents will do with
high probability, and the dependence between action policies can be captured by the mutual informa-
tion. High mutual information among actions means low uncertainty of other agents’ actions. Hence,
by regularizing the objective of the expected cumulative reward with mutual information among
agents’ actions, we can coordinate the behaviors of agents implicitly without explicit dependence
enforcement. However, the optimization problem with the proposed objective function has several
difficulties since we consider decentralized policies without explicit dependence or communication in
the execution phase. In addition, optimizing mutual information is difficult because of the intractable
conditional distribution. We circumvent these difficulties by exploiting the property of the latent
variable injected to induce mutual information, and applying variational lower bound on the mutual
1
Under review as a conference paper at ICLR 2021
information. With the proposed framework, we apply policy iteration by redefining value functions
to propose the VM3-AC algorithm for MARL with coordinated behavior under CTDE.
2	Related Work
Learning coordinated behavior in multi-agent systems is studied extensively in the MARL community.
To promote coordination, some previous works used communication among agents (Zhang & Lesser
(2013); Foerster et al. (2016); Pesce & Montana (2019)). For example, Foerster et al. (2016) proposed
the DIAL algorithm to learn a communication protocol that enables the agents to coordinate their
behaviors. Instead of relying on communication, Jaques et al. (2018) proposed social influence
intrinsic reward which is related to the mutual information between actions to achieve coordination.
The purpose of the social influence approach is similar to our approach and the social influence yields
good performance in social dilemma environments. The difference between our algorithm and the
social influence approach will be explained in detail and the effectiveness of our approach over the
social influence approach will be shown in Section 6. Wang et al. (2019) proposed an intrinsic reward
capturing the influence based on the mutual information between an agent’s current actions/states and
other agents’ next states. In addition, they proposed an intrinsic reward based on a decision-theoretic
measure. Although they used the mutual information to enhance exploration, our approach focuses
on the mutual information between simultaneous actions capturing policy correlation not influence.
Besides, they considered independent policies, whereas policies are correlated in our approach.
Some previous works considered correlated policies instead of independent policies. For example,
Liu et al. (2020) proposed explicit modeling of correlated policies for multi-agent imitation learning,
and Wen et al. (2019) proposed a recursive reasoning framework for MARL to maximize the expected
return by decomposing the joint policy into own policy and opponents’ policies. Going beyond
adopting correlated policies, our approach maximizes the mutual information between actions which
is a measure of correlation.
Our framework can be interpreted as enhancing correlated exploration by increasing the entropy
of own policy while decreasing the uncertainty about other agents’ actions. Some previous works
proposed other techniques to enhance correlated exploration (Zheng & Yue (2018); Mahajan et al.
(2019)). For example, MAVEN addressed the poor exploration problem of QMIX by maximizing the
mutual information between the latent variable and the observed trajectories (Mahajan et al. (2019)).
However, MAVEN does not consider the correlation among policies.
3	Background
We consider a Markov Game (Littman (1994)), which is an extention of Markov Decision Process
(MDP) to multi-agent setting. An N -agent Markov game is defined by an environment state space S,
action spaces for N agents Ai,…,AN, a state transition probability T : SX A ×S → [0,1], where
A = QiN=1 Ai is the joint action space, and a reward function R : S × A → R. At each time step t,
Agent i executes action a； ∈ Ai based on state St ∈ S. The actions of all agents at = (a1,…，aN)
yield next state st+1 according to T and yield shared common reward rt according to R under the
assumption of fully-cooperative MARL. The discounted return is defined as Rt = Pτ∞=t γτrτ , where
γ ∈ [0, 1) is the discounting factor.
We assume CTDE incorporating resource asymmetry between training and execution phases, widely
considered in MARL (Lowe et al. (2017); Iqbal & Sha (2018); Foerster et al. (2018)). Under CTDE,
each agent can access all information including the environment state, observations and actions of
other agents in the training phase, whereas the policy of each agent can be conditioned only on its
own action-observation history τti or observation oit in the execution phase. For given joint policy
π = (∏1, ∙∙∙ ,∏n), the goal of fully cooperative MARL is to find the optimal joint policy π* that
maximizes the objective J(π) = Eπ R0 .
Maximum Entropy RL The goal of maximum entropy RL is to find an optimal policy that maximizes
the entropy-regularized objective function, given by
∞
J (∏) = En X Y t}t(st,at) + αH (∏(∙∣st)f)	(1)
t=0
2
Under review as a conference paper at ICLR 2021
It is known that this objective encourages the policy to enhance exploration in the state and action
spaces and helps the policy avoid converging to a local minimum. Soft actor-critic (SAC), which is
based on the maximum entropy RL principle, approximates soft policy iteration to the actor-critic
method. SAC outperforms other deep RL algorithms in many continuous action tasks (Haarnoja et al.
(2018)).
We can simply extend SAC to multi-agent setting in the manner of independent learning. Each
agent trains its decentralized policy using decentralized critic to maximize the weighted sum of the
cumulative return and the entropy of its policy. We refer to this method as Independent SAC (I-SAC).
Adopting the framework of CTDE, we can replace decentralized critic with centralized critic which
incorporates observations and actions of all agents. We refer to this method as multi-agent soft
actor-critic (MA-SAC). Both I-SAC and MA-SAC are considered as baselines in the experiment
section.
4	The Proposed Maximum Mutual Information Framework
We assume that the environment is fully observable, i.e., each agent can observe the environment
state st for theoretical development in this section, and will consider partially observable environment
for practical algorithm construction under CTDE in the next section.
Under the proposed MMI framework, we aims to find the policy that maximizes the mutual informa-
tion between actions in addition to cumulative return. Thus, the MMI-regularized objective function
for joint policy π is given by
∞
J(∏) = En X Yt 卜t(st, at) + α X I(∏i(∙∣st); ∏j(∙∣st)))	⑵
t=0	(i,j)
where a；〜∏i(∙∖st) and α is the temperature parameter that controls the relative importance of the
mutual information against the reward.
As aforementioned, we assume decentralized policies and want the decentralized policies to exhibit
coordinated behavior. By regularization with mutual information in the proposed objective function
(2), the policy of each agent is implicitly encouraged to coordinate with other agents’ policies without
explicit dependency by reducing the uncertainty about other agents’ policies. This can be seen as
follows: Mutual information is expressed in terms of entropy and conditional entropy as
I(∏i(∙∖st);∏j(∙∖st)) = H(∏j(∙∖st)) - H(∏j(∙∖st)∖∏i(∙∖st)).	⑶
If the knowledge of ∏i(∙∖st) does not provide any information about ∏j(∙∖st), the conditional en-
tropy reduces to the unconditional entropy, i.e., H(∏j(∙∖st)∖∏i(∙∖st)) = H(∏j(∙∖st)), and the mutual
information becomes zero. Maximizing mutual information is equivalent to minimizing the un-
certainty about other agents’ policies conditioned on the agent’s own policy, which can lead the
agent to learn coordinated behavior based on the reduced uncertainty about other agents’ policies.
Figure 1: Causal diagram in 2-agent Markov
Game: (a) Standard MARL, (b) Introducing
However, direct optimization of the objective func-
tion (2) is not easy. Fig. 1(a) shows the causal
diagram of the considered system model described
in Section 3 in the case of two agents with de-
centralized policies. Since we consider the case
of no explicit dependency, the two policy distribu-
tions can be expressed as π1(at1∖st) and π2(at2∖st).
Then, for given environment state st observed by
both agents, π1 (at1 ∖st) and π2(at2∖st) are condi-
the latent variable to the standard MARL
tionally independent and the mutual information
I(π1(∙∖st);∏2(∙∖st)) = 0. Thus, theMMI objective
(2) reduces to the standard MARL objective of only the accumulated return. In the following subsec-
tions, we present our approach to circumvent this difficulty and implement the MMI framework and
its operation under CTDE.
3
Under review as a conference paper at ICLR 2021
4.1	Inducing Mutual Information Using Latent Variable
First, in order to induce mutual information among agents’ policies under the considered system
causal diagram shown in Fig. 1(a), we introduce latent variable zt. For illustration, consider the new
diagram with latent variable zt in Fig. 1(b). Suppose that the latent variable zt has a prior distribution
p(zt), and assume that both actions at1 and at2 are generated from the observed random variable st and
the unobserved random variable zt . Then, the policy of Agent i is given by the marginal distribution
∏i(∙∣st) = JZ ∏i(∙∣st, z)p(z)dz marginalized over z. With the unobserved latent random variable z,
the conditional independence does not hold for at1 and at2 and the mutual information can be positive,
i.e., I(∏1(∙∣st); ∏2(∙∣st)) > 0. Hence, We can induce the mutual information between actions without
explicit dependence by introducing the latent variable. In the general case of N agents, we have
π(a1,…，aN|s) = Ez[π1(a1∣s, z) ∙∙∙ πN(aN|s, z)]. Note that in this case we inject a common
latent variable z into all agents’ policies.
4.2	Variational Bound of Mutual Information
Even with non-trivial mutual information I(πi(∙∣st); πj(∙∣st)), it is difficult to directly compute the
mutual information. Note that we need the conditional distribution of atj given (ati, st) to compute
the mutual information as seen in (4), but it is difficult to know the conditional distribution directly.
To circumvent this difficulty, we use a variational distribution q(atj |ait, st) to approximate p(atj |ait, st)
and derive a lower bound on the mutual information I(∏i(∙∣st); ∏j(∙∣st)) =： Iij(St) as
Iij (st)= Ep(at,at∣st) log CqajL + Ep^ ∣ s,)[κL(p(aj | a； ,St) k q(aj | ai,”)]
≥ H (πj (∙lst))+ Ep(at,aj ∣st) hlog C(aj lai,st)i ,	⑷
where the inequality holds because KL divergence is always non-negative. The lower bound becomes
tight when q(atj |ait, St) approximates p(atj|ait, St) well. Using the symmetry of mutual information,
we can rewrite the lower bound as
Iij (St) ≥ 1 hH (πi(∙lst)) + H (nj(|st))+ Ep(at,aj∣st) [log C(at|aj ,st)+log C(aj lat,st)i ]∙⑸
Then, we can maximize this lower bound of mutual information by using the tractable approximation
q(ait|atj, St).
4.3	Modified Policy Iteration
In this subsection, we develop policy iteration for the MMI framework. First, we replace the original
MMI objective function (2) with the following tractable objective function based on the variational
lower bound (5):
∞	NN
J(∏,q) = En XYt (rt(st, at) + αN X H (πi (∙∣st)) + α XX log q(aj |at, st)) ,	(6)
t=0	i=1	i=1 j6=i
where q(atj |ait, St) is the variational distribution to approximate the conditional distribution
p(aj∣at, st). Then, we determine the individual objective function Ji(πi, q) for Agent i as the
sum of the terms in (6) associated with Agent i’s policy πi or action ait, given by Ji(πi, q) =
Eπ
∞β
X Y (rt(st, at)+ β ∙ H(n 3St)),+N X [∣ogq(at|at, St) + logq(at |at, SDD
t=0 S	{z	}	j=i S	{	}
(7)
(a)
(b)
where β = αN is the temperature parameter. Note that maximizing the term (a) in (7) implies
that each agent maximizes the weighted sum of the policy entropy and the return, which can be
interpreted as an extension of maximum entropy RL to multi-agent setting. On the other hand,
maximizing the term (b) with respect to πi means that we update the policy πi so that Agent j well
predicts Agent i’s action by the first term in (b) and Agent i well predicts Agent j’s action by the
4
Under review as a conference paper at ICLR 2021
second term in (b). Thus, the objective function (7) can be interpreted as the maximum entropy
MARL objective combined with predictability enhancement for other agents’ actions. Note that
predictability is reduced when actions are uncorrelated. Since the policy entropy term H(∏i(∙∣Si))
enhances individual exploration due to maximum entropy principle (Haarnoja et al. (2018)) and the
term (b) in (7) enhances predictability or correlation among agents’ actions, the proposed objective
function (7) can be considered as one implementation of the concept of correlated exploration in
MARL (Mahajan et al. (2019)).
Now, in order to learn policy πi to maximize the objective function (7), we modify the policy iteration
in standard RL. For this, we redefine the state and state-action value functions for each agent as
follows:
∞
Vn(S) , En X Yta + βH (∏i(∙∣st)) + NN X log 收)(aii,aj,StASO = S (8)
t=0	j6=i
Qin(S, a) , En r0 + γVin(S1)S0 = S, a0 = a ,	(9)
where q(i,j)(ati, atj, St) , q(ait|atj, St)q(atj|ati, St). Then, the Bellman operator corresponding to Vin
and Qin is given by
TnQi(s, a) , r(s, a) + γEs0〜Pk(S0)],	(10)
where
Vi(S) = Ea〜n Qi(s, a) - βlog∏i(ai∣S) + N X log q(i,j)(ai,aj, s)	(11)
j6=i
In the policy evaluation step, we compute the value functions defined in (14) and (15) by applying
the modified Bellman operator Tn repeatedly to any initial function Qi0 .
Lemma 1. (Variational Policy Evaluation). For fixed π and the variational distribution q, consider
the modified Bellman operator Tn in (16) and an arbitrary initial function Qi0 : S × A → R, and
define Qik+1 = TnQik. Then, Qik converges to Qin defined in (15).
Proof. See Appendix A.
In the policy improvement step, we update the policy and the variational distribution by using the
value function evaluated in the policy evaluation step. Here, each agent updates its policy and
variational distribution while keeping other agents’ policies fixed as follows: (πki +1, qk+1) =
arg max E(ai,α-i)〜(∏i,π-i)
πi,q
Qnk (s, a) — β log ∏i(ai∣s) + NN X log q(i,j)(ai,aj,s)) ,	(12)
j6=i
where a-i，{a1,…，aN}∖{ai}. Then, we have the following lemma regarding the improvement
step.
Lemma 2. (Variational Policy Improvement). Let πni ew and qnew be the updated policy and the
-i	i -i
, old (s, a) ≥ Qi old, old (s, a) for all
πi
variational distribution from (30) in Appendix A. Then, Qi new
(S, a) ∈ (S × A).
Proof. See Appendix A.
The modified policy iteration is defined as applying the variational policy evaluation and variational
improvement steps in an alternating manner. Each agent trains its policy, critic and the variational
distribution to maximize its objective function (7).
5	Algorithm Construction
Summarizing the development above, we now propose the variational maximum mutual information
multi-agent actor-critic (VM3-AC) algorithm, which can be applied to continuous and partially
5
Under review as a conference paper at ICLR 2021
Figure 2: Overall operation of the proposed VM3-AC. We only need the operation in the red box
after training.
observable multi-agent environments under CTDE. The overall operation of VM3-AC is shown in
Fig. 2. Under CTDE, each agent’s policy is conditioned only on local observation, and centralized
critics are conditioned on either the environment state or the observations of all agents, depending on
the situation (Lowe et al. (2017)). Let x denote either the environment state s or the observations of
all agents (o1, ∙ ∙ ∙ ,oN), whichever is used. In order to deal with the large continuous state-action
spaces, we adopt deep neural networks to approximate the required functions. For Agent i, we
parameterize the variational distribution with ξi as qξi (aj |ai, oi, oj), the state-value function with ψi
as Vψi (x), two action-value functions with θi,1 and θi,2 as Qiθi,1 (x, a), Qiθi,2 (x, a), and the policy
with φi as πφi i (a|oi) = Ez [πφi i (a|oi, z)]. We assume normal distribution for the latent variable
which plays a key role in inducing coordination among agents' policies, i.e., Zt 〜N(0, I), and
further assume that the variational distribution is Gaussian distribution with constant variance σ2, i.e.,
qξi (aj∣ai, oi, oj) = N(μξi (ai, oi, oj),σ2), where μξi (ai, oi, oj) is the mean of the distribution.
Centralized Training As aforementioned, the policy is the marginalized distribution over the latent
variable z, where the policies of all agents take the same zt generated from N(0, I) as an input
variable. We perform the required marginalization based on Monte Carlo numerical expectation as
follows:
1L
∏(a∣s) = Ez[∏φι (a1 |s, Z) ∙ ∙ ∙ ∏N (aN|s, Z)] ' LE ∏φ 1 (a1 |s, Z l)…∏N (aN|s, Zl),	(13)
l=1
and we use L = 1 for simplicity. The parameterized value functions, policy, and variational
distributions are trained similarly to the training in SAC. Due to space limitation, training detail and
pseudo code are provided in Appendices B and C.
Decentralized Execution In the centralized training phase, we pick actions (a1, ∙ ∙ ∙ , aN) by using
Monte Carlo expectation based on common latent variable Zl generated from zero-mean Gaussian
distribution, as seen in (13). We consider two methods to achieve the same operation in the decentral-
ized execution phase. First, this can be done by making all agents have the same Gaussian random
sequence generator and distributing the same seed to this random sequence generator only once in
the beginning of the execution phase. This eliminates the necessity of communication for sharing
the latent variable. In fact, this way of sharing Zl can be applied to the centralized training phase
too. Second, we exploit the property of zero-mean Gaussian latent variable Zl . That is, we simply
replace Zl with zero vector with the matching dimensions in the decentralized execution phase. This
substitution method does not deteriorate the performance much as seen in Section 6 since the latent
variable distribution is zero-mean Gaussian and zero has the highest density. Thus, the proposed
algorithm is fully operative under CTDE.
6	Experiment
In this section, we provide numerical results to evaluate VM3-AC. We considered four baselines:
1) MADDPG (Lowe et al. (2017)) - an extension of DDPG with a centralized critic to train a
6
Under review as a conference paper at ICLR 2021
(a) MW (N=3)
(b) MW (N=4)
(d) PP (N=3)
(e) PP (N=4)
(c) PP (N=2)
(f) CN (N=3)
Figure 3: Performance of MADDPG (blue), MA-AC (green), MAVEN (purple), IMOA (black), and
VM3-AC (the proposed method, red) on multi-walker environments (a)-(b), predator-prey (c)-(e), and
cooperative navigation (f). (MW, PP, and CN denote multi-walker, predator-prey, and cooperative
navigation, respectively)
decentralized policy for each agent. 2) Multi-agent actor-critic (MA-AC) - a variant of VM3-AC
(β = 0) without the latent variable. 3) Multi-agent variational exploration (MAVEN) (Mahajan et al.
(2019)). Similarly to VM3-AC, MAVEN introduced latent variable and variational approach for
optimizing the mutual information. However, MAVEN does not consider the mutual information
between actions but consider the mutual information between the latent variable and trajectories of
the agents. 4) Influence MOA (IMOA) also known as social influence (SI) (Jaques et al. (2018)). The
IMOA method models p(atj+1|ait, sit), where agents j and i are influencee and influencer, respectively,
and adds intrinsic reward given by the mutual information I(ait; atj+1 |st) between influencer’s current
action and influencee’s next timestep action not the mutual information I(ait; atj|st) between two
agents’ simultaneous actions, which is considered in our MMI framework. Both MAVEN and IMOA
are implemented on the top of MA-AC since we consider continuous action-space environments.
We evaluated the proposed algorithm and the baselines in three multi-agent environments with the
varying number of agents: multi-walker (Gupta et al. (2017)), predator-prey (Lowe et al. (2017)), and
cooperative navigation (Lowe et al. (2017)). We modified the original environments to require further
coordination among agents. For example, we increased the size of the agent and collision reward in
the cooperative navigation. Hence, the agent should consider other agents more while achieving its
goal. The detailed setting of each environments is provided in Appendix D.
6.1	Result
Fig. 3 shows the learning curves for the considered three environments with the different number
of agents. The y-axis denotes the average of all agents’ rewards averaged over 7 random seeds,
and the x-axis denotes time step. The hyperparameters including the temperature parameter β and
the dimension of the latent variable are provided in Appendix E. As shown in Fig. 3, VM3-AC
outperforms the baselines in the considered environments. Especially, in the case of the multi-walker
environment, VM3-AC has large performance gain. This is because the agents in the multi-walker
environment are required especially to learn coordinated behavior to obtain high rewards. In addition,
the agents in the predator-prey environment, where the number of agents is four, should spread out in
groups of two to get more reward. In this environment, VM3-AC also has large performance gain.
Thus, it is seen that the proposed MMI framework improves performance in complex multi-agent
tasks requiring high-quality coordination. It is observed that both MAVEN and IMOA outperform the
basic algorithm MA-AC but not VM3-AC. Hence, the mutual information between the latent variable
and trajectory (used in MAVEN) and the mutual information between the action of the agent and the
7
Under review as a conference paper at ICLR 2021
Figure 4: The positions of four agents after five time-steps after the episode begins in the early stage
of the training: 1st row - VM3-AC and 2nd row - MA-SAC. The figures in column correspond to a
different seed. The black squares are the preys and each color except black shows the position of
each agent.
next time step action of other agents (used in IMOA) are not as effective for coordinated behavior as
the mutual information between agents’ simultaneous actions used for VM3-AC.
6.2	Ablation Study
In this section, we provide ablation study on the major techniques and hyperparameter of VM3-AC:
1) mutual information versus entropy 2) the latent variable, 3) the temperature parameter β, and 4)
injecting zero vector instead of the latent variable z to policies in the execution phase.
Mutual information versus entropy: The proposed MMI framework maximizes the sum of entropy
and variational conditional probability, which provides a lower bound of mutual information between
actions. As aforementioned, maximizing the entropy and variational conditional probability enhance
exploration and predictability for other agents’ actions, respectively. Hence, the proposed MMI
framework enhances correlated exploration among agents. We compared VM3-AC with MA-SAC,
which is an extension of maximum entropy RL to multi-agent setting. We performed an experiment
in the predator-prey environment with four agents where the number of required agents to catch the
prey is two. In this environment, the agents started at the center of the map. Hence, the agents should
spread out in the group of two to catch preys efficiently. Fig.4 shows the positions of the four agents
at five time-steps after the episode starts. The first row and the second row in Fig.4 show the results
of VM3-AC and MA-SAC in the early stage of the training, respectively. It is seen that the agents of
VM3-AC explore in the group of two while the agents of MA-SAC tend to explore independently.
We provided the performance comparisons of VM3-AC with MA-SAC in Fig.5 (a) and (b).
Latent variable: The role of the latent variable is to induce mutual information among actions and
promote coordinated behavior. We compared VM3-AC and VM3-AC without the latent variable
(implemented by setting dim(z) = 0) in the multi-walker environment. In both cases, VM3-AC
yields better performance that VM3-AC without the latent variable as shown in Fig.5(a) and 5(b).
Injecting zero vector instead of the la-
tent variable: As mentioned in Section
5, we replace the latent variable with zero
vector to execute actions without com-
munication in the execution phase. We
compared the performance of decentral-
ized policies which use zero vector and
decentralized policies which use the la-
tent variable assuming communication.
We used deterministic evaluation based
Table 1: Impact of replacing the latent variable Z 〜
N(0, I) with zero vector z = -→0 in the execution phase
	PP (N=2)	PP (N=3)	PP (N=4)
Z 〜N(0,I) z = -→0	413	734	1123
	409	743	1147
on 20 episodes generated by the corresponding deterministic policy, i.e., each agent selects action
using the mean network of Gaussian policy πφi i. We averaged the return over 7 seeds, and the result
8
Under review as a conference paper at ICLR 2021
(a) PP (N=4)
(b) MW (N=4)	(c) MW (N=3)	(d) MW (N=4)
Figure 5: (a) and (b): VM3-AC (red), VM3-AC without latent variable (orange), and MA-SAC (cyan)
and (c) and (d): performance with respect to the temperature parameter
is shown in Table 3. It is seen that the zero vector replacement method yields almost the same
performance and enables fully decentralized execution without performance loss.
Temperature parameter β : The role of temperature parameter β is to control the relative im-
portance between the reward and the mutual information. We evaluated VM3-AC by varying
β = [0, 0.05, 0.1, 0.15] in the multi-walker environment with N = 3 and N = 4. Fig. 5(c) and 5(d)
show that VM3-AC with the temperature value around [0.05, 0.1] yields good performance.
7	Conclusion
In this paper, we have proposed the MMI framework for MARL to enhance multi-agent coordinated
learning under CTDE by regularizing the cumulative return with mutual information among actions.
The MMI framework is implemented practically by using a latent variable and variational technique
and applying approximate policy iteration. Numerical results show that the derived algorithm named
VM3-AC outperforms other baselines, especially in multi-agent tasks requiring high coordination
among agents. Furthermore, the MMI framework can be combined with other techniques for
cooperative MARL, such as value decomposition (Rashid et al. (2018)) to yield better performance.
References
Praveen Agarwal, Mohamed Jleli, and Bessem Samet. Fixed point theory in metric spaces. Recent
Advances and Applications, 2018.
CP Andriotis and KG Papakonstantinou. Managing engineering systems with large state and action
spaces through deep reinforcement learning. Reliability Engineering & System Safety, 191:106483,
2019.
Christian SChroeder de Witt, Jakob Foerster, Gregory Farquhar, Philip Torr, Wendelin Bohmer, and
Shimon Whiteson. Multi-agent common knowledge reinforcement learning. In Advances in Neural
Information Processing Systems, pp. 9924-9935, 2019.
Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to
CommuniCate with deep multi-agent reinforCement learning. In Advances in neural information
processing systems, pp. 2137-2145, 2016.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
CounterfaCtual multi-agent poliCy gradients. In Thirty-second AAAI conference on artificial
intelligence, 2018.
Gerald B Folland. Real analysis: modern techniques and their applications, volume 40. John Wiley
& Sons, 1999.
SCott Fujimoto, Herke Van Hoof, and David Meger. Addressing funCtion approximation error in
aCtor-CritiC methods. arXiv preprint arXiv:1802.09477, 2018.
Jayesh K Gupta, Maxim Egorov, and Mykel KoChenderfer. Cooperative multi-agent Control using
deep reinforCement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66-83. Springer, 2017.
9
Under review as a conference paper at ICLR 2021
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. arXiv preprint
arXiv:1810.02912, 2018.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A Ortega, DJ Strouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep
reinforcement learning. arXiv preprint arXiv:1810.08647, 2018.
Woojun Kim, Myungsik Cho, and Youngchul Sung. Message-dropout: An efficient training method
for multi-agent deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33,pp. 6079-6086, 2019.
P. Langley. Crafting papers on machine learning. In Pat Langley (ed.), Proceedings of the 17th
International Conference on Machine Learning (ICML 2000), pp. 1207-1216, Stanford, CA, 2000.
Morgan Kaufmann.
Minne Li, Zhiwei Qin, Yan Jiao, Yaodong Yang, Jun Wang, Chenxi Wang, Guobin Wu, and Jieping
Ye. Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning. In
The World Wide Web Conference, pp. 983-994, 2019.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
Minghuan Liu, Ming Zhou, Weinan Zhang, Yuzheng Zhuang, Jun Wang, Wulong Liu, and Yong
Yu. Multi-agent interactions modeling with correlated policies. arXiv preprint arXiv:2001.03415,
2020.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. In Advances in Neural Information Processing Systems, pp. 7611-7622,
2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforce-
ment learning. arXiv preprint arXiv:1908.03963, 2019.
Emanuele Pesce and Giovanni Montana. Improving coordination in small-scale multi-agent deep
reinforcement learning through memory-driven communication. arXiv preprint arXiv:1901.03887,
2019.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330-337, 1993.
Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based multi-agent exploration.
arXiv preprint arXiv:1910.05512, 2019.
10
Under review as a conference paper at ICLR 2021
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. arXiv preprint arXiv:1901.09207, 2019.
Chongjie Zhang and Victor Lesser. Coordinating multi-agent reinforcement learning with limited
communication. In Proceedings of the 2013 international conference on Autonomous agents and
multi-agent systems ,pp.1101-1108, 2013.
Stephan Zheng and Yisong Yue. Structured exploration via hierarchical variational policy networks.
2018.
11
Under review as a conference paper at ICLR 2021
Appendix A: Variational policy evaluation and policy improvement
In the main paper, we defined the state and state-action value functions for each agent as follows:
∞
Vn(S) , En X Ytet + βH(∏i(∙∣st)) + NN X log q(i,j)(at,aj,st)) S0 = S (14)
t=0	j6=i
Qin(S, a) , En r0 + γVin(S1)S0 = S, a0 = a ,	(15)
Lemma 3. (Variational Policy Evaluation). For fixed π and the variational distribution q, consider
the modified Bellman operator Tn in (16) and an arbitrary initial function Qi0 : S × A → R, and
define Qik+1 = TnQik. Then, Qik converges to Qin defined in (15).
TFQi(s, a)，r(s, a) + γEsθ~pM(s0)],	(16)
where
Vi(S) = Ea~n Qi(s, a) - β log πi(ai∣s) + N X log q(i,j)(ai,aj ,s)	(17)
j6=i
Proof. Define the mutual information augmented reward as TπQi (St, at)
i i	β	(ij) i j
r(st, at) + γEst+1~p,at+1~n Qi(St+1, at+ι) - βlog∏ (a∕st) + N £l0g9( j)(at,at, St)
(18)
T(St, at) + YEst+1~p,at+1~n
-β log ∏i (at |St)+ N X logq(i,j)(ait, atj, St)
j6=i
_ - /
{z
rπ(st,at)
(19)
+ YEst+1~p,at+1~n Qi(St+1, at+l)	QO)
=Ir∏(St, at) + γEst+ι~p,at+ι~n Qi(St+ι, at+ι)	(21)
Then, we can apply the standard convergence results for policy evaluation. Define
Tπ (v) = Rπ + YPπv	(22)
for v = [Q(S, a)]s∈S,a∈A. Then, the operator Tπ is a Y-contraction.
kTπ(v) -Tπ(u)k∞= k(Rπ+YPπv)-(Rπ+YPπu)k∞	(23)
= kYPπ(v - u)k∞	(24)
≤ kYPπk∞kv-uk∞	(25)
≤ Y ku - v k∞	(26)
Note that the operator Tπ has an unique fixed point by the contraction mapping theorem, and we
define the fixed point as Qiπ (S, a). Since
kQk(S, a) - Qn(S, a)k∞ ≤ YkQkT(S, a) — Qn(s, a)k∞ ≤ …≤ YkkQ0G a) — Qn(s, a)k∞,
(27)
we have
lim kQik(S,a) - Qin(S, a)k∞ = 0
k→∞
(28)
12
Under review as a conference paper at ICLR 2021
and this implies
lim Qik(s,a) = Qiπ (s, a), ∀(s, a) ∈ (S × A).	(29)
k→∞
□
We proved the variational policy evaluation in the finite state-action sets. We can expand it to the
infinite state-action sets by assuming follows:
•	Assume that Q functions for π are in L infinity
•	From Folland (1999), L infinity is Banach space
• From Agarwal et al. (2018), by Banach fixed point theorem, Q function should be converge
to a unique point in L infinity space and that is the Q function of given π
Lemma 4. (Variational Policy Improvement). Let πni ew and qnew be the updated policy and the
i	-i	i	-i
variational distribution from (30). Then, Qi new, old (s, a) ≥ Qi old, old (s, a) for all (s, a) ∈
(S × A).
(πk +ι,qk +1)= argmax E(ai,a-i)〜(∏,∏-i) Qnk (S, a) - β log πi(ai∖s)	(3O)
πi,q	k
+ N Xlogq(i叫ai,aj,s)) ,	(31)
j6=i
Proof. Let πnew be determined as
(πnew ,qnew ) = argmax E(at,a-i)〜(∏i,∏-i) Qnold (St, at) - β log KaiM )	(32)
πi,q	old
+ N Xlogq(i叫at,aj,st)) .	(33)
j6=i
Then, the following inequality is hold
E(at,a-i)〜(πnew ,π-ld) Qnold (st, at) - β log πiew (atist) + N X log q的W (at, aj , St))	(34)
j6=i
≥ E(ai ,aj)〜(∏old,∏-ld) Qnold(St, at) - β log πold(ait∖st) + N X log qθij)(at,aj ,st))	(35)
j6=i
= Vinold (St).	(36)
13
Under review as a conference paper at ICLR 2021
From the definition of the Bellman operator,
Qnold (st, at) = r(St, at) + YEst+1 〜PKnold (st+1)]	(37)
≤ r(St, at ) + YEst+l~pE(ai+ι,a;+ι)~(∏new ,π-ld) Qnold(St+1, at+1)
-β log ∏Lw (ai+i | st+ι) + β X log qnj (at+1 ,aj+1,st+1)	(38)
j6=i
≤ r(St, at) + γEst+l~PE(at+ι,a-ι)〜(∏new ,π-ld) ri(st+1 , at+1)
-β log nlew (at+1|st+1) + β X log qnj)(at+1,aj+1,st+1) + YVnold (st+2)
j6=i
(39)
.
.
.
i	-i
≤ Qiπnew,πold(St,at).	(40)
□
14
Under review as a conference paper at ICLR 2021
Appendix B: Algorithm Construction
7.1 Centralized Training
The value functions Vψi (x), Qiθ (x, a) are updated based on the modified Bellman operator defined
in (16) and (17). The state-value function Vψi (x) is trained to minimize the following loss function:
LV (ψi) = Est 〜D
(41)
where Vψi(Xt)	=	Ez 〜N (0,I ),{d 〜∏i(∙∣ot,z)}N=1 Qmin(Xt, at) - β log πφ Z (atIoi) +
N Pj=i log q(i,j)(ai, αj,ot, oj) , D is the replay buffer that stores the transitions (xt, at, rt, Xt+ι),
and Qimin(Xt, ait) = min[Qiθi,1 (Xt, ait), Qiθi,2 (Xt, ait)] is the minimum of the two action-value
functions to prevent the overestimation problem Fujimoto et al. (2018). The two action-value
functions are updated by minimizing the loss
LQ(θi)= E(χt,at)〜D 2(Qθi (xt, at) - Q(xt, at))2	(42)
where
ZA /-- 一\	/	_ ʌ .	-∏ Γτ Λ / M
Q(Xt, at) = rt(xt, at) + YExt+1 [vψi (Xt+1)]
(43)
and Vψ is the target value network, which is updated by the exponential moving average method. We
implement the reparameterization trick to estimate the stochastic gradient of policy loss. Then, the
action of agent i is given by ai = fφi (s; d, z), where Ei 〜N(0, I) and Z 〜N(0, I). The policy for
agent i and the variational distribution are trained to minimize the following policy improvement loss,
L∏i,q(Φi,ξ) = E st 〜D, — Qθi,ι (xt, a) + β log ∏φi (ai∣ot) — Ne X log qξ(i,j)(ai,aj,ot,oj)
ei ~ N,	j=i	_
z〜N
(44)
where
qξ(ii,j)(ait, atj ,oit,otj ) = qξi(aitIatj ,oit,otj ) qξi (atj Iait,oit,otj ) .	(45)
X--------{--------'、--------{--------}
(a)	(b)
Since approximation of the variational distribution is not accurate in the early stage of training and
the learning via the term (a) in equation 45 is more susceptible to approximation error, we propagate
the gradient only through the term (b) in equation 45 to make learning stable. Note that minimizing
一 log qξi (aj∣ai, st) is equivalent to minimizing the mean-squared error between aj and μξi (ai, oi, oj)
due to our Gaussian assumption on the variational distribution.
15
Under review as a conference paper at ICLR 2021
Appendix C:	Pseudo Code
Algorithm 1 VM3-AC (L=1)
Centralized training phase
Initialize parameter φi, θi, ψi,ψi,ξi, ∀i ∈ {1, •…,N}
for episode = 1, 2, ∙∙∙ do
Initialize state s0 and each agent observes oi0
for t < T and st 6= terminal do
Generate Zt 〜 N(0, I) and select action at 〜∏i(∙∣ot, Zt) for each agent i
Execute at and each agent i receives rt and oit+1
Store transitions in D
end for
for each gradient step do
Sample a minibatch from D and generate Zl 〜N(0, I) for each transition.
Update θi , ψi by minimizing the loss (42) and (43)
Update φi , ξi by minimizing the loss (44)
end for
____一	_Ti	_	_	_	_ _
Update ψ using the moving average method
end for
Decentralized execution phase
Initialize state s0 and each agent observes oi0
for each environment step do
Select action at 〜πi(∙∣ot, zt) where Zt =
generator with the same seed)
Execute at and each agent i receives oit+1
end for
-→0 (or sample from the Gaussian random sequence
16
Under review as a conference paper at ICLR 2021
Appendix D:	Environment Detail
Multi-walker The multi-walker environment, which was introduced in Gupta et al. (2017), is a
modified version of the BipedalWalker environment in OpenAI gym to multi-agent setting. The
environment consists of N bipedal walkers and a large package. The goal of the environment is
to move forward together while holding the large package on top of the walkers. The observation
of each agent consists of the joint angular speed, the position of joints and so on. Each agent has
4-dimensional continuous actions that control the torque of their legs. Each agent receives shared
reward R1 depending on the distance over which the package has moved and receives negative local
compensation R2 if the agent drops the package or falls to the ground. An episode ends when one of
the agents falls, the package is dropped or T time steps elapse. To obtain higher rewards, the agents
should learn coordinated behavior. For example, if one agent only tries to learn to move forward,
ignoring other agents, then other agents may fall. In addition, the different coordinated behavior is
required as the number of agents changes. We set T = 500, R2 = -10 and R1 = 10d, where d is
the distance over which the package has moved. We simulated this environment in three cases by
changing the number of agents (N = 2, N = 3, and N = 4).
All algorithms used neural networks to approximate the required functions. In the algorithms except
I-SAC, we used the neural network architecture proposed in Kim et al. (2019) to emphasize the
agent’s own observation and action for centralized critics. For agent i, we used the shared neural
network for the variational distribution qξi (aj | a； ,o； ,o j) for j ∈ {1,…，N }∖{i}, and the network
takes the one-hot vector which indicates j as input. Experimental details are given in Appendix E.
Predator-prey The predator-prey environment,
which is a standard task for MARL, consists of
N predators and M preys. We used a variant of
the predator-prey environment into the continuous
domain. The initial positions on the predators are
randomly determined, and those of the preys are
in the shape of a square lattice as shown in figure6
(b). The goal of the environment is to capture as
many preys as possible during a given time T . A
prey is captured when C predators catch the prey
simultaneously. The predators get team reward R1
when they catch a prey. After all of the preys are
captured and removed, we set the preys to respawn
(a)	(b)	(c)
Figure 6: Considered environments: (a) Multi-
walker, (b) Predator-prey, and (c) Cooperative
navigation
in the same position and increase the value of R1. Thus, the different coordinated behavior is needed
as N and C change. The observation of each agent consists of relative positions between agents and
other agents and those between agents and the preys. Thus, each agent can access to all information
of the environment state. The action of each agent is two-dimensional physical action. We set
R1 = 10 and T = 100. We simulated the environment with three cases: (N = 2, M = 16, C = 1),
(N = 3,M = 16,C = 1) and (N = 4,M = 16,C = 2).
Cooperative navigation Cooperative navigation, which was proposed in Lowe et al. (2017), consists
of N agents and L landmarks. The goal of this environment is to occupy all landmarks while avoiding
collision with other agents. The agent receives shared reward R1 which is the sum of the minimum
distance of the landmarks from any agents, and the agents who collide each other receive negative
reward -R2 . In addition, all agents receive R3 if all landmarks are occupied. The observation of
each agent consists of the locations of all other agents and landmarks, and action is two-dimensional
physical action. We set R2 = 10, R3 = 1, and T = 50. We simulated the environment in the cases
of (N = 3, L = 3).
17
Under review as a conference paper at ICLR 2021
Appendix E:	Hyperparameter and Training Detail
The hyperparameters for MA-AC, MA-SAC, MADDPG, and VM3-AC are summarized in Table 2.
Table 2: Hyperparameters of all algorithms
	MA-AC	MA-SAC	MADDPG	VM3 -AC
Replay buffer size	5 × 105	5 × 105	5 × 105	5 × 105
Discount factor	0.99	0.99	0.99	0.99
Mini-batch size	128	128	128	128
Optimizer	Adam	Adam	Adam	Adam
Learning rate	0.0003	0.0003	0.0003	0.0003
Target smoothing coefficient	0.005	0.005	0.005	0.005
Number of hidden layers (all networks)	2	2	2	2
Number of hidden units per layer	128	128	128	128
Activation function for hidden layer	RELU	RELU	RELU	RELU
Activation function for final layer	Tanh	Tanh	Tanh	Tanh
Table 3: The temperature parameter β and the dimension of the latent variable z for VM3-AC on the
considered environments. Note that the temperature parameter β in I-SAC and MA-SAC controls
the relative importance between the reward and the entropy, whereas the temperature parameter β in
VM3-AC controls the relative importance between the reward and the mutual information.
VM3-AC	β	Dim(z)
MW (N=3)	0.05	8
MW (N=4)	0.1	8
PP (N=2)	0.15	8
PP (N=3)	0.1	8
PP (N=4)	0.2	8
CN (N=3)	0.1	8
18