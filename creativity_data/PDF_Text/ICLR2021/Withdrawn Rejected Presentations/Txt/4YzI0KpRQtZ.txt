Under review as a conference paper at ICLR 2021
Streaming Probabilistic Deep Tensor Factor-
IZATION
Anonymous authors
Paper under double-blind review
Ab stract
Despite the success of existing tensor factorization methods, most of them conduct
a multilinear decomposition, and rarely exploit powerful modeling frameworks,
like deep neural networks, to capture a variety of complicated interactions in data.
More important, for highly expressive, deep factorization, we lack an effective
approach to handle streaming data, which are ubiquitous in real-world applications.
To address these issues, We propose SPIDER, a Streaming Probabilistic Deep
tEnsoR factorization method. We first use Bayesian neural networks (NNS) to
construct a deep tensor factorization model. We assign a spike-and-slab prior over
each NN weight to encourage sparsity and to prevent overfitting. We then use mul-
tivariate Delta’s method and moment matching to approximate the posterior of the
NN output and calculate the running model evidence, based on which we develop
an efficient streaming posterior inference algorithm in the assumed-density-filtering
and expectation propagation framework. Our algorithm provides responsive in-
cremental updates for the posterior of the latent factors and NN weights upon
receiving new tensor entries, and meanwhile select and inhibit redundant/useless
weights. We show the advantages of our approach in four real-world applications.
1	Introduction
Tensor factorization is a fundamental tool for multiway data analysis. While many tensor factorization
methods have been developed (Tucker, 1966; Harshman, 1970; Chu & Ghahramani, 2009; Kang
et al., 2012; Choi & Vishwanathan, 2014), most of them conduct a mutilinear decomposition and are
incapable of capturing complex, nonlinear relationships in data. Deep neural networks (NNs) are a
class of very flexible and powerful modeling framework, known to be able to estimate all kinds of
complicated (e.g., highly nonlinear) mappings. The most recent work (Liu et al., 2018; 2019) have
attempted to incorporate NNs into tensor factorization and shown a promotion of the performance, in
spite of the risk of overfitting the tensor data that are typically sparse.
Nonetheless, one critical bottleneck for NN based factorization is the lack of effective approaches
for streaming data. In practice, many applications produce huge volumes of data at a fast pace (Du
et al., 2018). It is extremely costly to run the factorization from scratch every time when we receive
a new set of entries. Some privacy-demanding applications (e.g., SnapChat) even forbid us from
revisiting the previously seen data. Hence, given new data, we need an effective way to update the
model incrementally and promptly.
A general and popular approach is streaming variational Bayes (SVB) (Broderick et al., 2013), which
integrates the current posterior with the new data, and then estimates a variational approximation
as the updated posterior. Although SVB has been successfully used to develop the state-of-the-art
multilinear streaming factorization (Du et al., 2018), it does not perform well for (deep) NN based
factorization. Due to the nested linear and nonlinear coupling of the latent embeddings and NN
weights, the variational model evidence lower bound (ELBO) that SVB maximizes is analytically
intractable and we have to seek for stochastic optimization, which is unstable and hard to diagnose
the convergence. Consequently, the posterior updates are often unreliable and inferior, and in turn
hurt the subsequent updates, leading to poor model estimations finally.
To address these issues, we propose SPIDER, a streaming probabilistic deep tensor factorization
method that not only exploits NN’s expressive power to capture intricate relationships, but also
provides efficient, high-quality posterior updates for streaming data. Specifically, we first use Bayesian
1
Under review as a conference paper at ICLR 2021
neural networks to build a deep tensor factorization model, where the input is the concatenation of the
associated factors in each tensor entry and the NN output predicts the entry value. To reduce the risk of
overfitting, we place a spike-and-slab prior over each NN weight to encourage sparsity. For streaming
inference, we use multivariate Delta’s method (Bickel & Doksum, 2015) that employs a first-order
Taylor expansion of the NN output to analytically compute its moments, and match the moments
to obtain the its current posterior and the running model evidence. We then use back-propagation
to calculate the gradient of the log evidence, with which we match the moments and update the
posterior of the embeddings and NN weights in the assumed-density-filtering (Boyen & Koller, 1998)
framework. Finally, after processing all the newly received entries, we update the spike-and-slab prior
approximation with expectation propagation (Minka, 2001a) to select and inhibit redundant/useless
weights. In this way, the incremental posterior updates are deterministic, reliable and efficient.
For evaluation, we examined SPIDER on four real-world large-scale applications, including both
binary and continuous tensors. We compared with the state-of-the-art streaming tensor factorization
algorithm (Du et al., 2018) based on a multilinear form, and streaming nonlinear factorization methods
implemented with SVB. In both running and final predictive performance, our method consistently
outperforms the competing approaches, mostly by a large margin. The running accuracy of SPIDER
is also much more stable and smooth than the SVB based methods.
2	Background
Tensor Factorization. We denote a K-mode tensor by Y ∈ Rd1 ×...×dK, where mode k includes
dk nodes. We index each entry by a tuple i = (i1, . . . , iK), which stands for the interaction
of the corresponding K nodes. The value of entry i is denoted by yi . To factorize the tensor,
we represent all the nodes by K latent embedding matrices U = {U1 , . . . , UK}, where each
Uk = [u1k, . . . , udk ]> is of size dk × rk, and each ujk is the embedding vector of node j in mode
k. The goal is to use U to recover the observed entries in Y . To this end, the classical Tucker
factorization (Tucker, 1966) assumes Y = W ×1 U1 ×2 . . . ×K UK, where W ∈ Rr1 ×...×rK is a
parametric tenor and ×k the mode-k tensor matrix multiplication (Kolda, 2006), which resembles
the matrix-matrix multiplication. If we set all rk = r and W to be diagonal, Tucker factorization
becomes CANDECOMP/PARAFAC (CP) factorization (Harshman, 1970). The element-wise form
is yi = Pr=IQM=1 Ukk ,j = (u1ι ◦...◦ uiM )>1, where ◦ is the Hadamard (element-wise) product
and 1 the vector filled with ones. We can estimate the embeddings U by minimizing a loss function,
e.g., the mean squared error in recovering the observed elements in Y.
Streaming Model Estimation. A general and popular framework for incremental model estimation
is streaming variational Bayes(SVB) (Broderick et al., 2013), which is grounded on the incremental
version of Bayes’ rule,
p(θ∣Dold ∪ Dnew) H p(θ∣Dold)p(Dnew∣θ)	(1)
where θ are the latent random variables in the probabilistic model we are interested in, Dold all the
data that have been seen so far, and Dnew the incoming data. SVB approximates the current posterior
p(θ∣Doid) with a variational posterior qcm∙(θ). When the new data arrives, SVB integrates qcur(θ)
with the likelihood of the new data to obtain an unnormalized, blending distribution,
p(θ) = qcur(θ)p(Dnew∣θ)	(2)
which can be viewed as approximately proportional to the joint distribution p(θ, Dold ∪ Dnew). To
conduct the incremental update, SVB uses p(θ) to construct a variational ELBO (Wainwright et al.,
2008), L(q(θ)) = Eq[log (p(θ)∕q(θ))], and maximizes the ELBO to obtain the updated posterior,
q* = argmaxq L(q). This is equivalent to minimizing the Kullback-Leibler (KL) divergence between
q and the normalized p(θ). We then set qcm∙ = q* and prepare the update for the next batch of new
data. At the beginning (when we do not receive any data), we set qcur = p(θ), the original prior in
the model. For efficiency and convenience, a factorized variational posterior q(θ) = Qj q(θj ) is
usually adopted to fulfill cyclic, closed-form updates. For example, the state-of-the-art streaming
tensor factorization, POST (Du et al., 2018), uses the CP form to build a Bayesian model, and applies
SVB to update the posterior of the embeddings incrementally when receiving new tensor entries.
3	Bayesian Deep Tensor Factorization
Despite the elegance and convenience of the popular Tucker and CP factorization, their multilinear
form can severely limit the capability of estimating complicated, highly nonlinear/nonstationary
2
Under review as a conference paper at ICLR 2021
relationships hidden in data. While numerous other methods have also been proposed, e.g., (Chu
& Ghahramani, 2009; Kang et al., 2012; Choi & Vishwanathan, 2014), most of them are still
inherently based on the CP or Tucker form. Enlightened by the expressive power of (deep) neural
networks (Goodfellow et al., 2016), we propose a Bayesian deep tensor factorization model to
overcome the limitation of traditional methods and flexibly estimate all kinds of complex relationships.
Specifically, for each tensor entry i, we construct an input xi by concatenating all the embedding
vectors associated with i, namely, Xi = [(uɪɪ )>,..., (UK)>]>. We assume that there is an unknown
mapping between the input embeddings Xi and the value of entry i, f : RPM=I rk → R, which
reflects the complex interactions/relationships between the tensor nodes in entry i. Note that CP
factorization uses a multilinear mapping. We use an M -layer neural network (NN) to model the
mapping f, which are parameterized by M weight matrices W = {W1, . . . , WM}. Each Wm
is Vm × (Vm-1 + 1) where Vm and Vm-1 are the widths for layer m and m - 1, respectively;
V0 = PkK=1 rk is the input dimension and VM = 1. We denote the output in each hidden layer m by
hm (1 ≤ m ≤ M -1) and define h° = x-We compute each hm = σ(Wm [hm-ι; 1]/vzVm-1 + 1)
where σ(∙) is a nonlinear activation function, e.g., ReLU and tanh. Note that We append a constant
feature 1 to introduce the bias terms in the linear transformation, namely the last column in each
Wm. For the last layer, We compute the output by fw(Xi) = WM [hM-ι; 1]/ʤ-ι + 1. Given
the output, we sample the observed entry value yi via a noisy model. For continuous data, we use
a Gaussian noise model, p(yi|U) = N yi|fW(Xi), τ-1 where τ is the inverse noise variance. We
further assign T a Gamma prior, p(τ) = Gamma(T|ao, bo). For binary data, We use the Probit model,
p(yi∣U) = Φ((2yi - 1)fw(Xi)) where Φ(∙) is the cumulative density function (CDF) of the standard
normal distribution.
Despite their great flexibility, NNs take the risk of overfitting. The larger a network, i.e., with more
weight parameters, the easier the network overfits the data. In order to prevent overfitting, we assign
a spike-and-slab prior (Ishwaran et al., 2005; Titsias & Ldzaro-Gredilla, 2011) over each NN weight
to sparsify and condense the network. Specifically, for each weight wmjt = [Wm]jt, we first sample
a binary selection indicator Smjt from P(Smij∣ρo) = Bern(Smjt∣ρo) = ρ0mjt (1 - ρo)1-smjt. The
weight is then sampled from
p(wmjt |Smjt ) = SmjtN(wmjt |0, σ0 ) + (1 - Smjt )δ(wmjt),	(3)
where δ(∙) is the Dirac-delta function. Hence, the selection indicator Smjt determines the type of
prior over wmjt: if Smjt is 1, meaning the weight is useful and active, we assign a flat Gaussian prior
with variance σ02 (slab component); if otherwise Smjt is 0, namely the weight is useless and should
be deactivated, we assign a spike prior concentrating on 0 (spike component).
Finally, we place a standard normal prior over the embeddings U. Given the set of observed tensor
entries D = {yi1 , . . . , yiN}, the joint probability of our model for continuous data is
M Vm Vm-1+1
p(U, W,S,T)=	Bern(Smjt |p0)(smjtN"(wmjt |0, σ0 ) + (1 - Smjt)δ(wmjt))
m=1j =1 t=1
• YK= 1 Yd= I N(Uk |0, I)Gamma(T la0,b0) YN=I N (yin lfW (Xin) ,τ-1 )	(4)
where S = {Smjt}, and for binary data is
M Vm Vm-1+1
p(U,W,S)= ππ π Bern(Smjt lρ0) (SmjtN"(Wmjt l0,σ0 ) + (1 - Smjt)δ(wmjt))
m=1j=1 t=1
・ Y'Yd=1 N(Uk ∣0,I) YN=I φ((2yin - 1)fw (Xin)).	⑸
4 Streaming Posterior Inference
We now present our streaming model estimation algorithm. In general, the observed tensor entries
are assumed to be streamed in a sequence of small batches, {B1, B2, . . .}. Different batches do
not have to include the same number of entries. Upon receiving each batch Bt, we aim to update
the posterior distribution of the embeddings U, the inverse noise variance T (for continuous data),
3
Under review as a conference paper at ICLR 2021
the selection indicators S and the neural network weights W , without re-accessing the previous
batches {Bj }j<t . While we can apply SVB, the variational ELBO that integrates the current
posterior and the new entry batch will be analytically intractable. Take binary tensors as an example.
Given a new entry batch Bt , the EBLO constructed based on the blending distribution (see (2))
is L = -KL(q(U,S, W)1Iqcur(U,S, W)) + Pn∈Bt Eq [logΦ((2yin - 1)fw(Xin))]. Due to the
nested, nonlinear coupling of the embeddings (in each xin) and NN weights W in calculating
fW (xin), the expectation terms in L are intractable, without any closed form. Obviously, the same
conclusion applies to the continuous data. Therefore, to maximize L so as to obtain the updated
posterior, we have to use stochastic gradient descent (SGD), typically with the re-parameterization
trick (Kingma & Welling, 2013). However, without the explicit form of L, it is hard to diagnosis the
convergence of SGD — it may stop at a place far from the (local) optimums. Note that we cannot use
hold-out data or cross-validation to monitor/control the training because we cannot store or revisit
data. The inferior posterior estimation in one batch can in turn influence the posterior updates in the
subsequent batches, and finally result in a very poor model estimation.
4.1	Online Moment Matching for Posterior Update
To address these problems, we exploit the assumed-density-filtering (ADF) framework (Boyen &
Koller, 1998), which can be viewed as an online version of expectation propagation (EP) (Minka,
2001a), a general approximate Bayesian inference algorithm. ADF is also based on the incremental
version of Bayes’ rule (see (1)). It uses a distribution in the exponential family (Wainwright et al.,
2008) to approximate the current posterior. When the new data arrive, instead of maximizing a
variational ELBO, ADF projects the (unnormalized) blending distribution (2) to the exponential
family to obtain the updated posterior. The projection is done by moment matching, which essentially
is to minimize KL(P(θ)∕Z∣∣q(θ)) where Z is the normalization constant. For illustration, suppose We
choose q(θ) to be a fully factorized Gaussian distribution, q(θ) = Qj q(θj) = Qj N(θj∣μj,Vj). To
update each q(θj-), we compute the first and second moments of θj w.r.t p(θ), and match a Gaussian
distribution with the same moments, namely, μj- = Ep(θj) and Vj = Varp(θj) = Ep(θ∕) — Ep(θj)2.
For our model, we use a fully factorized distribution in the exponential family to approximate the
current posterior. When a new batch of data Bt are received, we sequentially process each observed
entry, and perform moment matching to update the posterior of the NN weights W and associated
embeddings. Specifically, let us start with the binary data. We approximate the posterior with
M Vm Vm-1 +1	K dk rk
qcur(W?, U, S) = ɪ ɪ ɪ ɪ ɪ ɪ Bern(Smjt ∖ρmjt)N(Wmjt |μmjt, Vmjt) ] ] ] ] ] ] N(IUj ∖ψkjt, Vkjt) ∙
m=1 j=1 t=1	k=1 j=1 t=1
Given each entry in in the new batch, we construct the blending distribution, P(W, U, S) 8
qcur(W, U, S)Φ (2yin - 1)fW (xin) . To obtain its moments, we consider the normalizer, i.e.,
the model evidence under the blending distribution,
Zn = / qcur(W, U, S)Φ((2%n
- 1)fW (xin ) dWdUdS.
(6)
Under the Gaussian form, according to (Minka, 2001b), we can compute the moments and update the
posterior of each NN weight wmjt and each embedding element associated with in— {uik }k by
*
μ
μ + v
∂ log Zn
∂μ
V — V2 [(
∂ log Zn 2
∂μ
-2
∂ log Zn
∂V
(7)
*
v
where μ and V are the current posterior mean and variance of the corresponding weight or embedding
element. Note that since the likelihood does not include the binary selection indicators S, their
moments are the same as those under qcur and we do not need to update their posterior.
However, a critical issue is that due to the nonlinear coupling of the U and W in computing the NN
output fW (xin), the exact normalizer is analytically intractable. To overcome this issue, we consider
approximating the current posterior of fW (xin ) first. We use multivariate Delta’s method (Oehlert,
1992; Bickel & Doksum, 2015) that expands the NN output at the mean of W and U,
fW(xin) ≈ fE[W](E[xin]) +gn>(ηn -E[ηn])	(8)
where the expectation is under qcur(∙), % = VeC(W ∪ Xin), gn> = VfW(Xin)∣ηn=E[ηn]. Note that
xin is the concatenation of the embeddings associated with in . The rationale of the approximation
4
Under review as a conference paper at ICLR 2021
(8) is that the NN output is highly nonlinear and nonconvex to U and W. Hence, the scale of the
output change rate (i.e., gradient) can be much larger than the scale of the posterior variances of W
and U, which are (much) smaller than prior variance 1 (see (4) and (5)). Therefore, we can ignore
the second-order term and just use the first-order Taylor approximation. We have also tried the
second-order expansion, which, however, is unstable and does not improve the performance.
Based on (8), we can calculate the first and second moments of fW (xin ),
αn = Eqcur [fW (xin)] ≈ fE[W] (E[xin]), βn = Varqcur(fW(xin )) ≈ gn>diag(γn)gn	(9)
where each [γn]j = Varqcur ([ηn]j). Due to the fully factorized posterior form, we have cov(ηn) =
diag(ηn). Note that all the information in the Gaussian posterior (i.e., mean and variance) of W
and U have been integrated to approximate the moments and posterior of the NN output. Now
we use moment matching to approximate the current (marginal) posterior of the NN output by
qcur(fw(Xin)) = N(fw(Xin)∣ɑn, βn). Then We compute the running model evidence (6) by
Zn = EqCUr(W,U ,S)[φ((2yin - 1)fW (Xin ))] = EqCUr(fw (Xin ))[φ((2yin - 1)fW (Xin))]
≈ ZN(fo∣αn, βn)Φ((2yin - 1)fo)dfo = Φ( (2√n-1αn)	(10)
1	+ βn
Where We redefine fo = fW (Xin) for simplicity. With the nice analytical form, We can immediately
apply (7) to update the posterior for W and the associated embeddings in in . In light of the NN
structure, the gradient can be efficiently calculated via back-propagation, Which can be automatically
done by many deep learning libraries.
For continuous data, We introduce a Gamma posterior for the inverse noise variance, qcur(τ) =
Gamma(τ |a, b), in addition to the fully factorized posterior for W, U and S as in the binary case.
After We use (8) and (9) to obtain the posterior of the NN output, We derive the running model
evidence by
Zn = EqCUr(W,U,S,T) [N(yin lfW (Xin ), τ )] = EqCUr(fo)qcur(τ) [N(yin |fO, T )]
≈ Eqcur(τ)[ / Ν(fo∣αn,βn)Ν(yinlfo,T-1)dfo] = EqCUr(T )[N (yin ∣αn,βn + T-1)].	(11)
Next, We Use the first-order Taylor expansion again, at the mean of τ, to approximate the GaUs-
Sian term inside the expectation, N侬/。八，/八 十 T-1) ≈ N(沙马。八，/n + Eq/T]-1) + (τ —
EqCUr [τ]) dN(yin | αn ,βn + τ- 1 ) /dT | τ=EqCUr [τ] ∙ Taking expectation over the Taylor expansion gives
Zn ≈N(yin ∣αn,βn + EqCUr(T)-1) = N+ b/a).	(12)
We noW Can Use (7) to Update the posterior of W and the embeddings assoCiated With the entry. While
We Can also Use more aCCUrate approximations, e.g., the seCond-order Taylor expansion or qUadratUre,
We foUnd empiriCally oUr method aChieves almost the same performanCe.
To Update qCUr (T), We Consider the blending distribUtion only in terms of the NN oUtpUt fo and T so We
have p(fo,T) H qcur(fo)qcur(τ)N(yinlfo,τ-1) = N(fo∣αn,βn)Gamma(τ∣a,b)N(yin∣fo,τ-1).
Then We folloW (Wang & Zhe, 2019) to first derive the Conditional moments and then approximate
the expectation of the conditional moments to obtain the moments. The details are given in the
supplementary material. The updated posterior is given by q*(T) = Gamma(T|a*, b*), where
a* = a + 2 and b* = b + 2 ((yin - an)2 + βn).
4.2	Prior Approximation Refinement
Ideally, at the beginning (i.e., when we have not received any data), we shoUld set qcUr to the prior
of the model (see (4) and (5)). This is feasible for the embeddings U , selection indicators S and the
inverse noise variance T (for continUoUs data only), becaUse their GaUssian, BernoUlli and Gamma
priors are all members of the exponential family. However, the spike-and-slab prior for each NN
weight wmjt (see (3)) is a mixtUre prior and does not belong to the exponential family. Hence, we
introdUce an approximation term,
p(wmjt | smjt) H A(Wmjt,smjt) =Bern(SmjtIC(Pmjt))N"(Wmjt ∖μmjt,vmjt)	(13)
where H means “approximately proportional to" and c(χ) = 1/(1 + exp(-χ)), At the beginning, we
initialize Vmj7 = σ0 and μmjt to be a random number generated from a standard Gaussian distribution
5
Under review as a conference paper at ICLR 2021
truncated in [-σ0 , σ0]; we initialize ρmjt = 0. Obviously, this is a very rough approximation. If
we only execute the standard ADF to continuously integrate new entries to update the posterior
(see (7)), the prior approximation term will remain the same and never be changed. However, the
spike-and-slab prior is critical to sparsify and condense the network, and an inferior approximation
will make it noneffective at all. To address this issue, after we process all the entries in the incoming
batch, we use EP to update/improve the prior approximation term (13), with which to further update
the posterior of the NN weights. In this way, as we process more and more batches of the observed
tensor entries, the prior approximation becomes more and more accurate, and thereby can effectively
inhibit/deactivate the redundant or useless weights on the fly. The details of the updates are provided
in the supplementary material. Finally, we summarize our streaming inference in Algorithm 1 in the
supplementary material.
4.3	Algorithm Complexity
The time complexity of our streaming inference is O(NV + PkK=1 dkrk) where V is the total
number of weights in the NN. Therefore, the computational cost is proportional to N , the size of the
streaming batch. The space complexity is O(V + PkK=1 dkrk), including the storage of the posterior
for the embeddings U , NN weights W and selection indicators S, and the approximation term for the
spike-and-slab prior.
5	Related Work
Classical CP (Harshman, 1970) and Tucker (Tucker, 1966) factorization are multilinear and there-
fore are incapable of estimating complex, nonlinear relationships in data. While numerous other
approaches have been proposed (Shashua & Hazan, 2005; Chu & Ghahramani, 2009; Sutskever
et al., 2009; Hoff, 2011; Kang et al., 2012; Yang & Dunson, 2013; Choi & Vishwanathan, 2014;
Hu et al., 2015; Rai et al., 2015), most of them are still based on the CP or Tucker form. To over-
come these issues, recently, several Bayesian nonparametric tensor factorization models (Xu et al.,
2012; Zhe et al., 2015; 2016) were proposed to estimate the nonlinear relationships with Gaussian
processes (Rasmussen & Williams, 2006). The most recent work, NeurlCP (Liu et al., 2018) and
CoSTCo (Liu et al., 2019) have shown the advantage of NNs in tensor factorization. CoSTCo also
concatenates the embeddings associated with each entry to construct the input and use the NN output
to predict the entry value; but to alleviate overfitting, CoSTCo introduces two convolutional layers to
extract local features and then feed them into dense layers. By contrast, with the spike-and-slab prior
and Bayesian inference, we found that our model can also effectively prevent overfitting, without the
need for extra convolutional layers. NeuralCP uses two NNs, one to predict the entry value, the other
the (log) noise variance. Hence, NeuralCP only applies to continuous data. Our model can be used
for both continuous and binary data. Finally, both CoSTCo and NerualCP are trained with stochastic
optimization, need to pass the data many times (epochs), and hence cannot handle streaming data.
Expectation propagation (Minka, 2001a) is an approximate Bayesian inference algorithm that general-
izes assumed-density-filtering (ADF) (Boyen & Koller, 1998) and (loopy) belief propagation (Murphy
et al., 1999). EP employs an exponential-family term to approximate the prior and likelihood of
each data point, and cyclically updates each approximation term via moment matching. ADF can be
considered as applying EP in a model including only one data point. Because ADF only maintains the
holistic posterior, without the need for keeping individual approximation terms, it is very appropriate
for streaming learning. EP can meet a practical barrier when the moment matching is intractable. To
address this problem, Wang & Zhe (2019) proposed conditional EP that uses conditional moment
matching, quadrature and Taylor approximations to provide a high-quality, analytical solution. Based
on EP and ADF, Hemgndez-Lobato & Adams (2015) proposed probabilistic back-propagation, a
batch inference algorithm for Bayesian neural networks. PBP conducts ADF to pass the dataset
many times and re-update the prior approximation after each pass. A key difference from our work
is that PBP conducts a forward, layer by layer moment matching, to approximate the posterior
of each hidden neuron in the network, until it reaches the output. The computation is limited to
fully connected, feed-forward networks and ReLU activation function. By contrast, our method
computes the moments of the NN output via Delta’s method (i.e., Taylor expansions) and does not
need to approximate the posterior of the hidden neurons. Therefore, our method is free to use any
NN architecture and activation function. Note that multi-variate Delta’s method was also used in
Laplace’s approximation (MacKay, 1992) and non-conjugate variational inference Wang & Blei
(2013). Furthermore, we employ spike-and-slab priors over the NN weights to control the complexity
of the model and to prevent overfitting in the streaming inference.
6
Under review as a conference paper at ICLR 2021
6	Experiment
6.1	Predictive Performance
Datasets. We examined SPIDER on four real-world, large-scale datasets. (1) DBLP (Du et al., 2018),
a binary tensor about bibliography relationships (author conference, keyword), of size 10, 000 ×
200 × 10, 000, including 0.001% nonzero entries. (2) Anime(https://www.kaggle.com/
CooperUnion/anime-recommendations-database), a two-mode tensor depicting binary
(user, anime) preferences. The tensor contains 1, 300, 160 observed entries, of size 25, 838 ×
4, 066. (3) ACC (Du et al., 2018), a continuous tensor representing the three-way interactions (user,
action, file), of size 3, 000 × 150 × 30, 000, including 0.9% nonzero entries. (4) MovieLen1M
(https://grouplens.org/datasets/movielens/), a two-mode continuous tensor of
size 6, 040 × 3, 706, consisting of (user, movie) ratings. We have 1, 000, 209 observed entries.
Competing methods. We compared with the following baselines. (1) POST (Du et al., 2018), the
state-of-the-art probabilistic streaming tensor decomposition algorithm based on the CP model. It uses
streaming variational Bayes (SVB) (Broderick et al., 2013) to perform mean-field posterior updates
upon receiving new entries. (2) SVB-DTF, SVB based deep tensor factorization. (3) SVB-GPTF,
the streaming version of the Gaussian process(GP) based nonlinear tensor factorization (Zhe et al.,
2016), implemented with SVB. Note that similar to NNs, the ELBO in SVB for GP factorization
is intractable and we used stochastic optimization. (4) SS-GPTF, the streaming GP factorization
implemented with the recent streaming sparse GP approximations (Bui et al., 2017). It uses SGD
to optimize another intractable ELBO. (5) CP-WOPT (Acar et al., 2011a), a scalable static CP
factorization algorithm implemented with gradient-based optimization.
Parameter Settings. We implemented our method, SPIDER with Theano, and SVB-DTF, SVB/SS-
GPTF with TensorFlow. For POST, we used the original MATLAB implementation (https:
//github.com/yishuaidu/POST). For SVB/SS-GPTF, we set the number of pseudo inputs
to 128 in their sparse approximations. We used Adam (Kingma & Ba, 2014) for the stochastic
optimization in SVB-DTF and SVB/SS-GPTF, where we set the number of epochs to 100 in process-
ing each streaming batch and tuned the learning rate from {10-5, 5 × 10-5, 10-4, 3 × 10-4, 5 ×
10-4, 10-3,3 × 10-3,5 × 10-3, 10-2}. For SPIDER and SVB-DTF, We used a 3-layer NN, with
50 nodes in each hidden layer. We tested ReLU and tanh activations.
We first evaluated the prediction accuracy after all the (accessible) entries are processed. To do
so, we sequentially fed the training entries into every method, each time a small batch. We then
evaluated the predictive performance on the test entries. We examined the root-mean-squared-error
(RMSE) and area under ROC curves (AUC) for continuous and binary data, respectively. We ran
the static factorization algorithm CP-WOPT on the entire training set. On DBLP and ACC, we used
the same split of the training and test entries in (Du et al., 2018), including 320K and 1M training
entries for DBLP and ACC respectively, and 100K test entries for both. On Anime and MovieLen1M,
we randomly split the observed entries into 90% for training and 10% for test. For both datasets,
the number of training entries is around one million. For each streaming factorization approach,
we randomly shuffled the training entries and then partitioned them into a stream of batches. On
each dataset, we repeated the test for 5 times and calculated the average of RMSEs/AUCs and their
standard deviations. For CP-WOPT, we used different random initializations in each test.
We conducted two groups of evaluations. In the first group, we fixed the batch size to 256 and
examined the predictive performance with different ranks (or dimensions) of the embedding vectors,
{3, 5, 8 10}. In the second group, we fixed the rank to 8, and examined how the prediction accuracy
varies with the size of the streaming batches, {26, 27, 28, 29}. The results are reported in Fig. 1.
As we can see, in both settings, our method SPIDER (with both tanh and ReLU) consistently
outperforms all the competing approaches in all the cases and mostly by a large margin. First,
SPIDER significantly improves upon POST and CP-WOPT — the streaming and static multilinear
factorization, confirming the advantages of the deep tensor factorization. It worth noting that CP-
WOPT performed much worse than POST on ACC and MovieLen1M, which might be due to the poor
local optimums CP-WOPT converged to. Second, SVB/SS-GPTF are generally far worse than our
method, and in many cases even inferior to POST (see Fig. 1b, c, d and f). SVB-DTF are even worse.
Only on Fig. 1c, the performance of SVB-DTF is comparable to or better than CP-WOPT, and in all
the other cases, SVB-DTF is much worse than all the other methods and we did not show its curve in
the figure (similar for CP-WOPT in Fig. 1g). Those results might be due to the inferior/unreliable
stochastic posterior updates. Lastly, although both SPIDER-ReLU and SPIDER-tanh outperform all
the baselines, SPIDER-ReLU is overall better than SPIDER-tanh (especially Fig. 1g).
7
Under review as a conference paper at ICLR 2021
SPIDER-ReLU	SVB-DTF-ReLU	POST	SVB-GPTF
SPIDER-tanh	SVB-DTF-tanh	SS-GPTF	CP-WOPT
O
Z)
<
W 0.7
<
0.6
0.8
LU
ω
CC
3	5	8 10
Rank
(b) Anime
0.55
0.45
0.35
3 5	8 10
Rank
(c) ACC
(d) MovieLenIM
0.9
g 0.85
< 0.8
0.75
3 5	8 10
Rank
(a) DBLP
0.7
<
0.9
0.85
0.8
0.75
2627 28	29
Batch Size
(e) DBLP
0.8
0.6
2627 28	29
Batch Size
(f) Anime
0.35
0.4
0.38
-LU
CL
LU
ω
CC
2627 28	29
Batch Size
(g) ACC
2627 28	29
Batch Size
(h) MovieLenIM
Figure 1: PrediCtive performanCe with different ranks (top row) and streaming batCh sizes (bottom row). In
the top row, the streaming bath size is fixed to 256; in the bottom row, the rank is fixed to 8. The results are
0.9
0.8
0.8
0.7
0.6
0.5
0.95
0.4
O
ɔ
<
O
ɔ
<
1
0.71
0	500 1000
0.5
0	1000 2000
0.34 I-----
0	2000 4000
0.9 t
1500 3000
Number of Batches Number of Batches
Number of Batches
Number of Batches
(C) ACC (r = 3)	(d) MovieLenIM (r
(a) DBLP (r = 3)
(b) Anime (r = 3)
0	1000 2000
54
O O
山sw
=3)
5 9
-9S
O
WSnH
1
Number of Batches Number of Batches Number of Batches Number of Batches
(e) DBLP (r = 8)	(f) Anime (r = 8)	(g) ACC (r = 8)	(h) MovieLenIM (r = 8)
Figure 2: Running prediCtion aCCuraCy along with the number of proCessed streaming batChes. The batCh size
was fixed to 256.
6.2 Prediction On the Fly
Next, we evaluated the dynamiC performanCe. We randomly generated a stream of training batChes
from eaCh dataset, upon whiCh we ran eaCh algorithm and examined the prediCtion aCCuraCy after
proCessing eaCh batCh. We set the batCh size to 256 and tested with rank r = 3 and r = 8. The
running RMSE/AUC of eaCh method are reported in Fig. 2. Note that some Curves are missing
or partly missing beCause the performanCe of the Corresponding methods are muCh worse than all
the other ones. In general, nearly all the methods improved the prediCtion aCCuraCy with more
and more batChes, showing inCreasingly better embedding estimations. However, SPIDER always
obtained the best AUC/RMSE on the fly, exCept at the beginning stage on Anime and MovieLen1M
(r = 8). In addition, the trends of SPIDER and POST are muCh smoother than that of SVB-DTF and
SVB/SS-GPTF, whiCh might again beCause the stoChastiC updates in the latter methods are unstable
and unreliable. Note that in Fig. 2b, SVB-DTF has running AUC steadily around 0.5, implying that
SVB aCtually failed to effeCtively update the posterior. Finally, in the supplementary material, we
provide the running time, and showCase the sparse posteriors of the NN weights learned by SPIDER.
7 Conclusion
We have presented SPIDER, a streaming probabilistiC deep tensor faCtorization approaCh, whiCh Can
effeCtively leverage neural networks to Capture CompliCated relationships for streaming faCtorization.
Experiments on real-world appliCations have demonstrated the advantage of SPIDER.
8
Under review as a conference paper at ICLR 2021
References
E. Acar, D. M. Dunlavy, T. G. Kolda, and M. Morup. Scalable tensor factorizations for incomplete
data. Chemometrics and Intelligent Laboratory Systems, 106(1):41-56, 2011a.
Evrim Acar, Daniel M Dunlavy, Tamara G Kolda, and Morten Morup. Scalable tensor factorizations
for incomplete data. ChemometricS and Intelligent Laboratory Systems, 106(1):41-56, 2011b.
Peter J Bickel and Kjell A Doksum. Mathematical StatiStics: basic ideas and Selected topics, VolUme
I, volume 117. CRC Press, 2015.
Xavier Boyen and Daphne Koller. Tractable inference for complex stochastic processes. In
ProceedingS of the FoUrteenth conference on Uncertainty in artificial intelligence, pp. 33-42,
1998.
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan. Stream-
ing variational bayes. In AdvanceS in neural information ProceSSing systems, pp. 1727-1735,
2013.
Thang D BUi, CUong NgUyen, and Richard E TUrner. Streaming sparse gaUssian process approxima-
tions. In AdVanceS in Neural Information ProceSSing Systems, pp. 3299-3307, 2017.
Joon Hee Choi and S Vishwanathan. Dfacto: Distributed factorization of tensors. In AdVanceS in
NeUral Information ProceSSing Systems, pp. 1296-1304, 2014.
Wei Chu and Zoubin Ghahramani. Probabilistic models for incomplete multi-dimensional arrays.
AISTArS, 2009.
Yishuai Du, Yimin Zheng, Kuang-chih Lee, and Shandian Zhe. Probabilistic streaming tensor
decomposition. In 2018 IEEE International Conference on Data Mining (ICDM), pp. 99-108.
IEEE, 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. DeeP Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
R. A. Harshman. Foundations of the PARAFAC procedure: Model and conditions for
an”explanatory”multi-mode factor analysis. UCLA Working PaPerS in Phonetics, 16:1-84, 1970.
Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning
ofbayesian neural networks. In Intemational Conference on Machine Learning, pp. 1861-1869,
2015.
P.D. Hoff. Hierarchical multilinear models for multiway data. ComPUtational StatiSticS & Data
AnalySis, 55:530-543, 2011. ISSN 0167-9473. URL http://www.stat.washington.
edu/hoff/Code/hoff_2011_csda.
Changwei Hu, Piyush Rai, and Lawrence Carin. Zero-truncated poisson tensor factorization for
massive binary tensors. In UAI, 2015.
Hemant Ishwaran, J Sunil Rao, et al. Spike and slab variable selection: frequentist and bayesian
strategies. The AnnalS of StatiStics, 33(2):730-773, 2005.
U Kang, Evangelos Papalexakis, Abhay Harpale, and Christos Faloutsos. Gigatensor: scaling tensor
analysis up by 100 times-algorithms and discoveries. In ProceedingS of the 18th ACM SIGKDD
international conference on KnoWledge discovery and data mining, pp. 316-324. ACM, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv PrePrint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv PrePrint
arXiv:1312.6114, 2013.
Tamara Gibson Kolda. MUltilinear operators for higher-order decompositions, volume 2. United
States. Department of Energy, 2006.
9
Under review as a conference paper at ICLR 2021
Bin Liu, Lirong He, Yingming Li, Shandian Zhe, and Zenglin Xu. Neuralcp: Bayesian multiway data
analysis with neural tensor decomposition. Cognitive CompUtation, 10(6):1051-1061, 20l8.
Hanpeng Liu, Yaguang Li, Michael Tsang, and Yan Liu. Costco: A neural tensor completion
model for sparse tensors. In Proceedings of the 25th ACM SIGKDD InternationaI Conference on
KnoWIedge DiScovery & Data Mining, pp. 324-334, 2019.
David JC MacKay. The evidence framework applied to classification networks. NeUraI computation,
4(5):720-736, 1992.
Thomas P Minka. Expectation propagation for approximate bayesian inference. In Proceedings of
the SeVenteenth COnferenCe on UnCertainty in artificial intelligence, pp. 362-369, 2001a.
Thomas Peter Minka. A family of algorithms for approximate BayeSian inference. PhD thesis,
Massachusetts Institute of Technology, 2001b.
Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate
inference: An empirical study. In PrOCeedingS of the Fifteenth COnferenCe on UnCertainty in
artificial intelligence, pp. 467—475. Morgan Kaufmann Publishers Inc., 1999.
Gary W Oehlert. A note on the delta method. The AmeriCan Statistician, 46(1):27-29, 1992.
Piyush Rai, Yingjian Wang, Shengbo Guo, Gary Chen, David Dunson, and Lawrence Carin. Scalable
Bayesian low-rank decomposition of incomplete multiway tensors. In PrOCeedingS of the 31th
International COnferenCe on MaChine Learning (ICML), 2014.
Piyush Rai, Changwei Hu, Matthew Harding, and Lawrence Carin. Scalable probabilistic tensor
factorization for binary and count data. In IJCAI, 2015.
Carl Edward Rasmussen and Christopher K. I. Williams. GaUSSian Processes for MaChine Learning.
MIT Press, 2006.
Amnon Shashua and Tamir Hazan. Non-negative tensor factorization with applications to statistics
and computer vision. In Proceedings of the 22th International COnference on MaChine Learning
(ICML), pp. 792-799, 2005.
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan R Salakhutdinov. Modelling relational data using
bayesian clustered tensor factorization. In AdVanCeS in neural information PrOCeSSing systems, pp.
1821-1828, 2009.
Michalis K Titsias and Miguel Lazaro-Gredilla. Spike and slab variational inference for multi-task and
multiple kernel learning. In AdVanCeS in neural information PrOCeSSing systems, pp. 2339-2347,
2011.
Ledyard Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:
279-311, 1966.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. FOUndatiOnS and Trends® in MaChine Learning, 1(1-2):1-305, 2008.
Chong Wang and David M Blei. Variational inference in nonconjugate models. JOUrnaI OfMaChine
Learning ReSearch, 14(Apr):1005-1031, 2013.
Zheng Wang and Shandian Zhe. Conditional expectation propagation. In UAI, pp. 6, 2019.
Zenglin Xu, Feng Yan, and Yuan Qi. Infinite Tucker decomposition: Nonparametric Bayesian models
for multiway data analysis. In Proceedings of the 29th International COnferenCe on MaChine
Learning (ICML), 2012.
Y. Yang and D.B. Dunson. Bayesian conditional tensor factorizations for high-dimensional classifica-
tion. JOUrnaI of the Royal StatiStiCaI SOCiety B, revision SUbmitted, 2013.
Shandian Zhe, Zenglin Xu, Xinqi Chu, Yuan Qi, and Youngja Park. Scalable nonparametric multiway
data analysis. In Proceedings of the Eighteenth International COnference on ArtifiCiaI Intelligence
and StatiStics, pp. 1125-1134, 2015.
10
Under review as a conference paper at ICLR 2021
Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Yuan Qi, and Zoubin
Ghahramani. Distributed flexible nonlinear tensor factorization. In AdvanCes in Neural InfOrmatiOn
PrOCessing Systems, pp. 928-936, 2016.
11