Under review as a conference paper at ICLR 2021
Reducing Class Collapse in Metric Learning
with Easy Positive Sampling
Anonymous authors
Paper under double-blind review
Ab stract
Metric learning seeks perceptual embeddings where visually similar instances
are close and dissimilar instances are apart, but learned representation can be
sub-optimal when the distribution of intra-class samples is diverse and distinct
sub-clusters are present. We theoretically prove and empirically show that under
reasonable noise assumptions, prevalent embedding losses in metric learning, e.g.,
triplet loss, tend to project all samples of a class with various modes onto a single
point in the embedding space, resulting in class collapse that usually renders
the space ill-sorted for classification or retrieval. To address this problem, we
propose a simple modification to the embedding losses such that each sample
selects its nearest same-class counterpart in a batch as the positive element in
the tuple. This allows for the presence of multiple sub-clusters within each class.
The adaptation can be integrated into a wide range of metric learning losses. Our
method demonstrates clear benefits on various fine-grained image retrieval datasets
over a variety of existing losses; qualitative retrieval results show that samples with
similar visual patterns are indeed closer in the embedding space.
1	Introduction
Metric learning aims to learn an embedding function to lower dimensional space, in which semantic
similarity translates to neighborhood relations in the embedding space (Lowe, 1995). Deep metric
learning approaches achieve promising results in a large variety of tasks such as face identification
(Chopra et al., 2005; Taigman et al., 2014; Sun et al., 2014), zero-shot learning (Frome et al., 2013),
image retrieval (Hoffer & Ailon, 2015; Gordo et al., 2016) and fine-grained recognition (Wang et al.,
2014).
In this work we investigate the family of losses which optimize for an embedding representation that
enforces that all modes of intra-class appearance variation project to a single point in embedding
space. Learning such an embedding is very challenging when classes have a diverse appearance.
This happens especially in real-world scenarios where the class consists of multiple modes with
diverse visual appearance. Pushing all these modes to a single point in the embedding space requires
the network to memorize the relations between the different class modes, which could reduce the
generalization capabilities of the network and result in sub-par performance.
Recently researchers observed that this phenomena, where all modes of class appearance “collapse”
to the same center, occurs in case of the classification SoftMax loss (Qian et al.). They proposed
a multi-center approach, where multiple centers for each class are used with the SoftMax loss to
capture the hidden distribution of the data to solve this issue. Instead of using SoftMax, it was shown
that triplet loss may offer some relief from class collapsing (Wang et al., 2014) and this is certainly
true in noise-free environments. However, in this paper, we show that in real-world conditions with
modest noise assumptions, triplet and other metric learning loss yet suffer from class collapse.
Rather than refine the loss, we argue the key lies in an improved strategy for sampling and selecting
the examples. Early work (Malisiewicz & Efros, 2008) proposed per-exemplar distance representation
as a means to overcome class collapsing; inspired by this we introduce a simple sampling method to
select positive pairs of training examples. Our method can be combined naturally with other popular
sampling methods. In each training iteration, given an anchor and a batch of samples in the same
category, our method selects the closest sample to the anchor in the current embedding space as the
1
Under review as a conference paper at ICLR 2021
Figure 1: Given an anchor (circle with dark ring), our approach samples the closest positive example in the
embedding space as the positive element. This results in pushing the anchor only towards the closest element
direction (green arrow), which allows the embedding to have multiple clusters for each class.
positive sample. The metric learning loss is then computed based on the anchor and its positive paired
sample.
We demonstrate the class-collapsing phenomena on a real-world dataset, and show that our method
is able to create more diverse embedding which result in a better generalization performance. We
evaluate our method on three standard zero-shot benchmarks: CARS196 (Krause et al., 2013),
CUB200-2011 (Wah et al., 2011) and Omniglot (Lake et al., 2015). Our method achieves a consistent
performance enhancement with respect to various baseline combinations of sampling methods and
embedding losses.
2	Related Work
Sampling methods. Designing a good sampling strategy is a key element in deep metric learning.
Researchers have been proposed sampling methods when sampling both the negative examples as
well as the positive pairs. For negative samples, studies have focused on sampling hard negatives
to make training more efficient (Simo-Serra et al., 2015; Schroff et al., 2015; Wang & Gupta, 2015;
Oh Song et al., 2016; Parkhi et al., 2015). Recently, it has been shown that increasing the negative
examples in training can significantly help unsupervised representation learning with contrastive
losses (He et al., 2020; Wu et al., 2018; Chen et al., 2020).
Besides negative examples, methods for sampling hard positive examples have been developed in
classification and detection tasks (Loshchilov & Hutter, 2015; Shrivastava et al., 2016; Arandjelovic
et al., 2016; Cubuk et al., 2019; Singh & Lee, 2017; Wang et al., 2017). The central idea is to
perform better augmentation to improve the generalization in testing (Cubuk et al., 2019). Apart
from learning with SoftMax classification, Arandjelovic et al. (2016) propose to perform metric
learning by assigning the near instance from the same class as the positive instance. As the positive
training set is noisy in their setting, this method leads to features invariant to different perspectives.
Different from this approach, we use this method in a clean setting, where the purpose is to get the
opposite result of maintaining the inner-class modalities in the embedding space. Xuan et al. (2020)
also propose to use this positive sampling method with respect to the N-pair loss (Sohn, 2016) in
order to relax the constraints of the loss on the intra-class relations. From a theoretic perspective, we
prove that in a clean setting this relaxation is redundant for other popular metric losses like the triplet
loss (Chechik et al., 2010) and margin loss (Wu et al., 2017). We formulate the noisy-environment
setting and prove that in this case the triplet and margin losses also suffer from class-collapsing and
using our purpose positive sampling method optimizes for solutions without class-collapsing. We
also provide an empirical study that supports the theoretic analysis.
Noisy label problem. Learning with noisy labels is a practical problem when applied to the real
world (Scott et al., 2013; Natarajan et al., 2013; Shen & Sanghavi, 2019; Reed et al., 2014; Jiang et al.,
2017; Khetan et al., 2017; Malach & Shalev-Shwartz, 2017), especially when training with large-scale
data (Sun et al., 2017). One line of work applies a data-driven curriculum learning approach where
the data that are most likely labeled correctly are used for learning in the beginning, and then harder
data is taken into learning during a later phase (Jiang et al., 2017). Researchers have also tried on to
apply the loss only on easiest top k-elements in the batch, determine by lowest current loss (Shen
2
Under review as a conference paper at ICLR 2021
& Sanghavi, 2019). Inspired by these works, our method focuses on selecting only the top easiest
positive relations in the batch.
Beyond memorization. Deep networks are shown to be extremely easy to memorize and over-fit
to the training data (Zhang et al., 2016; Recht et al., 2018; 2019). For example, it is shown the
network can be trained with randomly assigned labels on the ImageNet data, and obtain 100% training
accuracy if augmentations are not adopted. Moreover, even the CIFAR-10 classifier performs well
in the validation set, it is shown that it does not really generalize to new collected data which is
visually similar to the training and validation set (Recht et al., 2018). In this paper, we show that
when allowing the network the freedom not to have to learn inner-class relation between different
class modes, we can achieve much better generalization, and the representation can be applied in a
zero-shot setting.
3	Preliminaries
Let X = {x1, .., xn} be a set of samples with labels yi ∈ {1, .., m}. The objective of metric learning
is to learn an embedding f (∙, θ) → Rk, in which the neighbourhood of each sample in the embedding
space contains samples only from the same class. One of the common approaches for metric learning
is using embedding losses in which at each iteration, samples from the same class and samples from
different classes are chosen according to same sampling heuristic. The objective of the loss is to push
away projections of samples from different classes, and pull closer projections of samples from a
same class. In this section, we introduce a few popular embedding losses.
Notation: Let xi, xj ∈ X, define: Dxfi,xj = k f(xi) -f(xj)k 2. In cases where there is no ambiguity
1 if yi = yj
we omit f and simply write Dxi,xj . We also define the function δxi,xj =	. Lastly,
for every a ∈ R, denote (a)+ := max(a, 0).
The Contrastive loss (Hadsell et al.) takes sample embeddings and pushes the samples from the
different classes apart and pulls samples from the same class together.
Lcon(xi, xj ) = δxi,xj ∙ Dxi,xj + (1 - δxi,Xj) ∙ (α - DXi,Xj) +
Here α is the margin parameter which defines the desired minimal distance between samples from
different classes.
While the Contrastive loss imposes a constraint on a pair of samples, the Triplet loss (Chechik et al.,
2010) functions on a triplet of samples. Given a triplet xa , xp , xn ∈ X, the triplet loss is defined by
Ltrip(xa，XP，Xn)= δXa,Xp ∙ (I- δXa,Xn) ' (DXa,Xp - DXp,xn + α) +
The Margin loss (Wu et al., 2017) aims to exploit the flexibility of Triplet loss while maintaining the
computational efficiency of the Contrastive loss. This is done by adding a variable which determines
the boundary between positive and negative pairs; given an anchor xa ∈ X the loss is defined by
Lmmargin(Xa，X)= δXa,X ∙ (Dfa,X - βXa + α)+ + (1 - δXa,X) YeXa- Dfa,X + &) +
4 Class-collapsing
The contrastive loss objective is to pull all the samples with the same class to a single point in the
embedding space. We call this the Class-collapsing property. Formally, an embedding f : X -→ Rm
has the class-collapsing property, if there exists a label y and a point p ∈ Rm such that {f(Xi)| yi =
y} = {p}.
4.1	Embedding losses optimal solution
It is easy to see that an embedding function f that minimizes:
Ocon(f) = n12 I X Lcon(Xi,xj )
Xi,Xj ∈X
3
Under review as a conference paper at ICLR 2021
has the class-collapsing property with respect to all classes. However, this is not necessarily true for
the Triplet loss and the Margin loss.
For simplification for the rest of this subsection we will assume there are only two classes. Let
A ⊂ X be a subset of elements such that all the elements in A belongs to one class and all the
element in Ac belong to the other class.
Recall some basic set definitions.
Definition 1. For all sets Y, Z ⊂ Rm define:
1.	The diameter of Y is defined by:
diam(Y ) = sup{ky - zk |y, z ∈ Y }
2.	The distance between Y and Z is:
IIY - Zk = inf{ky - z∣∣∣y ∈ Y,z ∈ Z}
Itis easy to see that if f : X → Rm is an embedding, such that diam(f (A)) < 2∙α+kf (A)-f (B)∣,
then:
Otrip(f) = n I X	Ltrip(XMXj ,xk ) I = 0.
xi ,xj ,xk ∈X
Moreover, fixing βxi = α for every Xi ∈ X , then:
Omargin
(f, β)
1
n2
Lf,β
margin
(Xi, Xj)
0.
It can be seen that indeed, the family of embedding which induce the global-minimum with respect
to the Triplet loss and the Margin loss, is rich and diverse. However, as we will prove in the next
subsection, this does not remain true in a noisy environment scenario.
4.2	Noisy environment analysis
For simplicity we will also discuss in this section the binary case of two labels, however this could be
extended easily to the multi-label case.
The noisy environment scenario can be formulated by adding uncertainty to the label class. More
formally, let Y = {Y1, .., Yn} be a set of independent binary random variables. Let A1, .., At ⊂ X,
0.5 < p < 1 such that: |Aj| = and
P(K=k)={po=制 Xi∈Ak
We can also reformulate δ as a binary random variable such that:
δYi,γ∙ := 1Yi=Yj-
For the Triplet loss define:
ELfrip(Xi,xj,xk ) = E eYi,Yj ∙ (I- SYi,Yk )) ∙ (Dfi,xj - Dfi,χk + α)+ .
We are searching for an embedding function which minimize
EOtrip(f) = & X	ELfrip(Xi,Xj ,Xk)
xi,xj,xk ∈X
Theorem 1. Let f : O -→ Rm be an embedding, which minimize EOtrip (f), then f has the class-
collapsing property with respect to all classes.
4
Under review as a conference paper at ICLR 2021
Similarly, we can define:
ELmargin(xi,xj ) = ESYi,匕，(Dfi,Xj - %i + α)+ + E(I- δYi,Yj ) Yexi- Dfi,xj + α) +
Theorem 2. Let f : O -→ Rm be an embedding, which minimize
EOmargin (f,β) =	X ELmargin (Xi,Xj ),
xi,xj ∈X
then f has the class-collapsing property with respect to all classes.
The proof of the last two theorems can be find in Appendix A.
In conclusion, although theoretically in clean environments the Triplet loss and Margin loss should
allow more flexible embedding solutions, this does not remain true when noise is considered. On a
real-world data, where mislabeling and ambiguity can be usually be found, the optimal solution with
respect to both these losses becomes degenerate.
4.3 Easy Positive Sampling (EPS)
Using standard embedding losses for metric learning can result in an embedding space in which
visually diverse samples from the same class are all concentrated in a single location in the embedding
space. Since the standard evaluation and prediction method for image retrieval tasks are typically
based on properties of the K-nearest neighbours in the embedding space, the class-collapsing property
is a side-effect which is not necessarily in order to get optimal results. In the next section, we will
show experimental results, which support the assumption that complete class-collapsing can hurt the
generalization capability of the network.
To address the class-collapsing issue we propose a simple method for sampling, which results in
weakening the objective penalty on the inner-class relations, by applying the loss only on the closest
positive sample. Formally we define the EPS sampling in the following way; given a mini-batch
with N samples, for each sample a, let Ca be the set of elements from the same class as a in the
mini-batch, we choose the positive sample pa to be
argmin(kf(t) - f (a)k)
t∈Ca
For negative samples na we can choose according to various options. In this paper we use the
following methods: (a) Choosing randomly from all the elements which are not in Ca . (b) Using
distance sampling (Wu et al., 2017). (c) semi-hard sampling (Schroff et al., 2015),(d) MS hard-mining
sampling (Wang et al., 2019). We then apply the loss on the triplets (a, pa,na). Using such sampling
changes the loss objective such that instead of pulling all samples in the mini-batch from the same
class to be close to the anchor, it only pulls the closest sample to the anchor (with respect to the
embedding space) in the mini-batch, see Figure 1.
In Appendix B, we formalize this method in the noisy environment framework. We prove (Claim 1,2)
that every embedding which has the class collapsing property is not a minimal solution with respect
to both the margin and the triplet loss with the easy positive sampling. Furthermore, in Claim 3,4
we prove that the objective of the losses with EPS on tuples/triplets is to push away every element
(including positive elements), that is not in the k-closest elements to the anchor, where k is determined
by the noise level p. Therefore, if we apply the EPS method on a mini-batch which has small numbers
of positive elements from each modality, in such case adding the EPS to the losses not only relax
the constraints on the embedding, allowing the embedding to have multiple inner-clusters. It also
optimizes the embedding to have this form.
5	Experiments
We test our EPS method on image retrieval and clustering datasets. We evaluate the image retrieval
quality based on the recall@k metric (JegoU et al., 2011), and the clustering quality by using the
normalized mutual information score (NMI) (Manning et al., 2008). The NMI measures the quality
of clustering alignments between the clusters induced by the ground-truth labels and clusters induced
by applying clustering algorithm on the embedding space. The common practice to choose the NMI
5
Under review as a conference paper at ICLR 2021
Figure 2: Embedding examples from the MNIST validation set, after training using only even/odd labels.
Different colors indicate different digits. Left: Using Triplet-loss, class collapsing pushes all intra-class digits to
overlapping clusters. Right: With EPS, different digits form separate clusters. Retrieval or classification using
the odd-vs-even task/metric is more effectively implemented using the embedding on the right, even though the
embedding on the left is learned with a loss that more strictly optimizes for the task.
Table 1: Recall@k evaluated on MNIST dataset. The train classes are digits 0-5 and the test classes
are digits 6-9
model	MNIST Train Digits			MNIST Test Digits		
	R@1	R@5	R@10	R@1	R@5	R@10
Triplet	42.01	87.51	96.56	35.16	80.86	93.26
EPS + Triplet [ours]	65.78	93.57	97.38	42.31	83.86	93.61
clusters is by using K-means algorithm on the embedding space, with K equal to the number of
classes. However, this prevents from the measurement capturing more diverse solutions in which
homogeneous clusters appear only when using larger amount of clusters. Regular NMI prefers
solutions with class-collapsing. Therefore, we increase the number of clusters in the NMI evaluation
(denote it by NMI+) we also report the regular NMI score.
5.1	MNIST Even/Odd Example
To demonstrate the class-collapsing phenomena, we take the MNIST dataset (Lecun et al., 1998), and
split the digits according to odd and even. From a visual perspective this is an arbitrary separation. We
took the first 6 digits for training and left the remaining 4 digits for testing. We used a simple shallow
architecture which result in an embedding function from the image space to R2 (For implementation
details see Appendix C).
We train the network using the triplet loss. We compare our sampling method to random sampling of
positive examples (the regular loss). As can be seen in Figure 2, the regular training without EPS
suffers from class-collapsing. Training with EPS creates a richer embedding in which there is a clear
separation not only between the two-classes, but also between different digits from the same class.
As expected, the class-collapsing embedding preforms worse on the test data with the unseen digits,
see Table 1.
5.2	Fine-grained Recognition Evaluation
We compare our approach to previous popular sampling methods and losses. The evaluation is
conducted on standard benchmarks for zero-shot learning and image retrieval following the common
splitting and evaluation practice (Wu et al., 2017; Movshovitz-Attias et al., 2017; Brattoli et al.,
2019). We build our implementation on top of the framework of Roth et al., which allow us to have
a fair comparison between all the tested methods with an embedding of fix size (128). For more
implementation details and consistency of the results, see Appendix C.
5.2.1	Datasets
We evaluate our model on the following datasets.
6
Under review as a conference paper at ICLR 2021
Table 2: Recall@k and NMI performance on Cars196 and CUB200- 2011. NMI+ indicate the NMI
measurement when using 10 (number of classes) clusters. Our EPS method improves in all cases. *:
Our re-implemented version with the same embedding dimension.
model	R@1	Cars-196			NMI+	CUB-200				
		R@2	R@4	NMI		R@1	R@2	R@4	NMI	NMI+
Trip. + SH	51.5	63.8	-733-	53.4	-	42.6	55.0	66.4	55.4	-
Trip. + SHt	76.1	84.4	90.0	65.1	68.5	61.5	73.4	82.5	66.2	68.1
ProxyNCA	73.2	82.4	86.4	64.9	-	49.2	61.9	67.9	64.9	-
ProxyNCAt	77.1	85.2	91.2	65.6	68.9	63.1	74.8	83.8	67.2	68.7
Dist-Margin	79.6	86.5	91.9	69.1	70.4	63.6	74.4	83.1	69.0	68.7
MS	77.3	85.3	90.5	-	-	57.4	69.8	80.0	-	-
MSt	81.2	89.1	93.5	60.5	71.1	62.3	73.3	82.1	59.8	68.0
EPS + Trip. + SH	78.3	85.9	^^9∏-	59.8	69.8	61.8	73.6	82.4	62.4	68.0
EPS + Dist-Margin	83.6	89.5	93.6	67.3	72.4	64.7	75.2	84.3	68.2	69.4
EPS + MS	82.9	89.4	93.2	60.0	72.0	63.3	74.2	82.5	61.2	68.2
Table 3: Recall@k and NMI performance on Omniglot dataset. In both cases the training was done
with only language labels. Right: evaluation on language labels. Left: evaluation on letter labels.
NMI+ indicate the NMI measurement when using 30*(number of classes) clusters. Our EPS method
improves in both cases.
model	R@1	Omniglot-letters			NMI	R@1	Omniglot-languages			NMI+
		R@2	R@4	R@8			R@2	R@4	R@8	
Trip. + SH	49.4	60.0	69.2	76.9	66.2	71.0	80.2	87.6	92.4	38.7
ProxyNCA	49.1	60.4	70.9	78.9	69.0	73.0	82.1	88.8	93.5	43.3
Dist-Margin	49.4	61.1	70.1	79.2	68.9	73.2	82.3	89.1	94.0	43.5
MS	57.7	68.5	77.3	83.8	69.2	78.8	86.4	92.0	95.4	46.0
EPS + Trip. + SH	68.4	79.3	86.9	92.1	79.6	85.2	91.1	94.9	97.3	52.6
EPS + Dist-Margin	66.2	76.7	84.8	90.3	77.9	83.0	89.4	93.6	96.4	50.7
EPS + MS	68.7	79.1	86.9	92.2	77.3	86.2	91.7	94.9	97.2	53.8
•	Cars-196 (Krause et al., 2013), which contains 16,185 images of 196 car models. We follow
the split in Wu et al. (2017), using 98 classes for training and 98 classes for testing.
•	CUB200-2011 (Wah et al., 2011), which contains 11,788 images of 200 bird species. We
also follow Wu et al. (2017), using 100 classes for training and 100 for testing.
•	Omniglot (Lake et al., 2015), which contains 1623 handwritten characters from 50 alphabet.
In our experiments we only use the alphabets labels during the training process, i.e, all the
characters from the same alphabet has the same class. We follow the split in Lake et al.
(2015) using 30 alphabets for training and 20 for testing.
5.2.2	Results
We tested our sampling method with 3 different losses: Triplet (Chechik et al., 2010), Margin (Wu
et al., 2017) and Multi-Similarity (MS) (Wang et al., 2019). For the Margin loss experiment, we
combine our sampling method with distance sampling (Wu et al., 2017); this could be done because
the distance sampling only constrains on the negative samples, where our method only constrains
on the positive samples. We set the margin α = 0.2 and initialized β = 1.2 as in (Wu et al., 2017).
For the Triplet we combine our method with semi-hard sampling (Schroff et al., 2015) by fixing the
positive according to EPS and then using semi-hard sampling for choosing the negative examples. For
the MS loss we replace the positive hard-mining method with EPS and use the same hard-negative
method. We use the same hyper-paremeters as in (Wang et al., 2019) α = 2, λ = 1, β = 50.
Results are summarized in Tables 2 and 3. We can see that our method achieves the best performance
on all tested datasets. It is important to note that in the baseline models, when using Semi-hard
sampling, the sampling strategy was done also on the positive part as suggest in the original papers.
We see that replacing the semi-hard positive sampling with easy-positive sampling, improve results
in all the experiments. The improvement gain becomes larger as the dataset classes can be partitioned
7
Under review as a conference paper at ICLR 2021
0.8-
—EPS + Distance-margin
—EPS + Semi-hard negative
—Semi-hard
Distance-margin
,χ沟承、
0.7-
0.6-
0.5-
0.4-
^10	20	30	40	50
Epoch
(a)
0.45-∣-------------1--------------1--------------1--------------ɪ-
0	10	20	30	40
Number of positive samples in batch
(b)
0.3 4
0
Query
Figure 3: Results on Omniglot-letters. (a) Recall@1 performance of each model per epoch. (b)
performance of EPS + distance-margin model on the Omniglot dataset, as a function of the number of
positive samples in batch (where zero is equivalent to only using only distance sampling). Increasing
the number the number of positive samples enhances the model performance.
Figure 4: Retrieval results for randomly chosen query images in Cars196 dataset. Using EPS creates
more homogeneous neighbourhood relationships with respect to the car viewpoint.
more naturally to a small number of sub-clusters which are visually homogeneous. In Cars196 dataset
it is the car viewpoint, where in Omniglot it is the letters in each language. As can be seen in Table
3, using EPS on the Omniglot dataset result in creating an embedding in which in most cases the
nearest neighbor in the embedding consists of element of the same letter, although the network was
trained without these labels. In Figure 4 we can see a qualitatively comparison of CARS16 models
results. EPS seems to create more homogeneous neighbourhood relationships with respect to the the
viewpoint of the car. More results and comparisons can be find in Appendix C.
5.2.3	Positive batch size effect
An important hyperparameter in our sampling method is the number of positive batch samples, from
which we select the closest one in the embedding space to the anchor. If the class is visually diverse
and the number of positive samples in batch is low, than with high probability the set of all the
positive samples will not contain any visually similar image to the anchor. In case of the Omniglot
experiment, the effect of this hyperparameter is clear; It determines the probability that the set of
positive samples will include a sample from the same letter as the anchor letter. As can be seen in
Figure 3(b), the performance of the model increases as the probability of having another sample with
the same letter as the anchor increases.
6	Conclusion
In this work we demonstrate the importance of positive sampling strategies when using embedding
losses for metric learning. We investigate the class collapsing phenomena with respect to popular
embedding losses such as the Triplet loss and the Margin loss. While in clean environments there is a
diverse and rich family of optimal solutions, when noise is present, the optimal solution collapses
8
Under review as a conference paper at ICLR 2021
to a degenerate embedding. We propose a simple solution to this issue based on ’easy’ positive
sampling, and prove that indeed adding this sampling results in non-degenerate embeddings. We
also compare and evaluate our method on standard image retrieval datasets, and demonstrate a
consistent performance boost on all of them. While our method and results have been limited to
metric learning frameworks, we believe that our sampling scheme will also be useful in other related
settings, including supervised contrastive learning, which we leave to future work.
References
Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for
weakly supervised place recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5297-5307, 2016.
Biagio Brattoli, Karsten Roth, and Bjorn Ommer. Mic: Mining interclass characteristics for improved metric
learning. In Proceedings of the Intl. Conf. on Computer Vision (ICCV), 2019.
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image similarity through
ranking. Journal of Machine Learning Research, 11:1109-1135, 2010. doi: 10.1145/1756006.1756042.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. 2020.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to
face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’05), volume 1, pp. 539-546. IEEE, 2005.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning
augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 113-123, 2019.
Istvan Fehervari, Avinash Ravichandran, and Srikar Appalaraju. Unbiased evaluation of deep metric learning
algorithms, 2019.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. Devise: A deep visual-semantic embedding model. In Advances in neural information processing
systems, pp. 2121-2129, 2013.
Albert Gordo, Jon Almazdn, Jerome Revaud, and Diane Larlus. Deep image retrieval: Learning global
representations for image search. In European conference on computer vision, pp. 241-257. Springer, 2016.
R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR06). IEEE. doi:
10.1109/cvpr.2006.100. URL https://doi.org/10.1109/cvpr.2006.100.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.
In Computer Vision - ECCV 2016, pp. 630-645. Springer International Publishing, 2016. doi: 10.1007/
978-3-319-46493-0_38.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2020.
Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. In International Workshop on Similarity-
Based Pattern Recognition, pp. 84-92. Springer, 2015.
H J6gou, M Douze, and C Schmid. Product quantization for nearest neighbor search. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 33(1):117-128, jan 2011. doi: 10.1109/tpami.2010.57.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven
curriculum for very deep neural networks on corrupted labels. arXiv preprint arXiv:1712.05055, 2017.
Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. arXiv
preprint arXiv:1712.04577, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http:
//arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference paper
at the 3rd International Conference for Learning Representations, San Diego, 2015.
9
Under review as a conference paper at ICLR 2021
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained catego-
rization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney,
Australia, 2013.
B.	M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic
program induction. Science, 350(6266):1332-1338, dec 2015. doi: 10.1126∕science.aab3050.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks. arXiv preprint
arXiv:1511.06343, 2015.
David G. Lowe. Similarity metric learning for a variable-kernel classifier. Neural Computation, 7(1):72-85, jan
1995. doi: 10.1162/neco.1995.7.1.72.
Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In Advances in
Neural Information Processing Systems, pp. 960-970, 2017.
Tomasz Malisiewicz and Alexei A. Efros. Recognition by association via learning per-exemplar distances. In
CVPR, June 2008.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. Introduction to Information Retrieval.
Cambridge University Press, 2008. doi: 10.1017/cbo9780511809071.
Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No fuss
distance metric learning using proxies. In 2017 IEEE International Conference on Computer Vision (ICCV).
IEEE, oct 2017. doi: 10.1109/iccv.2017.47.
Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check, 2020.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels.
In Advances in neural information processing systems, pp. 1196-1204, 2013.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured
feature embedding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
4004-4012, 2016.
Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. 2015.
Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric learning without
triplet sampling.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to
cifar-10? arXiv preprint arXiv:1806.00451, 2018.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize
to imagenet? arXiv preprint arXiv:1902.10811, 2019.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich.
Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.
Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting
training strategies and generalization performance in deep metric learning.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face recognition
and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, jun
2015. doi: 10.1109/cvpr.2015.7298682.
Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: Consistency
and maximal denoising. In Conference On Learning Theory, pp. 489-511, 2013.
Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss minimization. In
Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5739-5748, Long
Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/
shen19e.html.
10
Under review as a conference paper at ICLR 2021
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online
hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
761-769, 2016.
Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and Francesc Moreno-Noguer. Dis-
criminative learning of deep convolutional feature point descriptors. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 118-126, 2015.
Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-
supervised object and action localization. In 2017 IEEE international conference on computer vision (ICCV),
pp. 3544-3553. IEEE, 2017.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In D. Lee, M. Sugiyama,
U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29,
pp. 1857-1865. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/
2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness
of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pp.
843-852, 2017.
Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation by joint
identification-verification. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-
berger (eds.), Advances in Neural Information Processing Systems 27, pp. 1988-1996. Curran Associates,
Inc., 2014.
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level
performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1701-1708, 2014.
L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. 2008.
C.	Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying
Wu. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1386-1393, 2014.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV,
2015.
Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive generation via adversary
for object detection. In CVPR, 2017.
Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with general
pair weighting for deep metric learning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5022-5030, 2019.
Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krahenbuhl. Sampling matters in deep
embedding learning. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, oct 2017.
doi: 10.1109/iccv.2017.309.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric
instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3733-3742, 2018.
Hong Xuan, Abby Stylianou, and Robert Pless. Improved embeddings with easy positive triplet mining.
In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO,
USA, March 1-5, 2020, pp. 2463-2471. IEEE, 2020. doi: 10.1109/WACV45572.2020.9093432. URL
https://doi.org/10.1109/WACV45572.2020.9093432.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
11
Under review as a conference paper at ICLR 2021
Appendix
A: Proofs for the Theorems in subsecion 4.2
Theorem 1. Let f : O -→ Rm be an embedding, which minimize EOtrip (f), then f has the class-collapsing
property with respect to all classes.
Proof. Define a new random variables such that for every 1 ≤ r1 , r2 ≤ t:
hr1,r2(Y,Z)=(01 eYls=er1∧Z=r2
e se
observe that
δY1,Y2，(I-	δY1 ,Y3 )	=	1Y1=r1	∙ hr1,r2 (Y2, Y3)	=	1 Y1=r2，hr1 ,r2 (Y3 ,Y2 )
1≤r1 ,r2 ≤t	1≤r1 ,r2 ≤t
r16=r2	r16=r2
Since the variables are independent
E(5yi,Y2 ∙	(1 -	5yi,Y3 )) = 2 ∙ X	E(1Yι=rι )	∙	E(hr1,r2 (Y2,Y3))	+ E(IY1=r2 )	∙	E(hr1,r2 (Y3,Y2)).
1≤r1 ,r2 ≤t
r16=r2
Define： D(x1,x2,x3) := (Dχ1 次一Dχ1,χ3 + α) +
Rearranging the terms we get
n3 ∙ EOtrip(f) =	£	(E(JY1,Y2 ∙ (1 — Jy1 ,Y3)) ∙ D(x1,x2,x3)二
x1,x2,x3∈X
X	(E(1Y1=r1)	∙E(hr1,r2((Y2,Y3))+E(1Y1=r2)	∙E(hr1,r2(Y3,Y2)))	∙DJ(x1,x2,x3)
x1,x2,x3∈X
1≤r1 6=r2 ≤t
X	(hr1,,r2(Y2,Y3)) ∙ (E(1Y1=n)∙ D(x1,x2,x3) + E(lγ1=r2) ∙ D(x1,x3,x2))=
x1,x2,x3∈X
1≤r1 6=r2 ≤t
Therefore, if
K(i,j, k, r1, r2) = ∙ E(1Y1=r1) ∙ DJ (xi, xj, xk) + E(1Y1=r2) ∙ DJ (x1, xk, xj) ,
then EOtrip (f) can be written as
EOtrip(f) = n13	X	E(hr1,r2 (Y' ,Yk) ∙ K (i,j,k,rι ,r)
1≤i,j,k≤n
1≤r1 6=r2 ≤t
For every xi ∈ X, define:
(EOtrip(f))xi = n2 ∙ X	(E(hr1,r2(Yj,Yk)) ∙ K(xi,xj,Xk,r1,r2)
1≤',k≤n
1≤r1 6=r2 ≤t
Let f : X -→ Rm be an embedding, fix 1 ≤ r ≤ t and xi ∈ Ar , xj , xk ∈ X with
k f(xi) - f(xj)k = w, k f(xi) - f(xk)k = h.
By definition:
K(i, j, k, r1 , r2)
p ∙	(h	-	w +	α)+	+ q(w	-	h +	α)+
q ∙	(h	-	w +	α)+	+ p(w	-	h +	α)+
p ∙	(h	-	w +	α)+	+ p(w	-	h +	α)+
q ∙	(h	-	w +	α)+	+ q(w	-	h +	α)+
r1 = r ∧ r2 6= r
r2 = r ∧ r1 6= r
r1 = r ∧ r2 = r
r1 6= r ∧ r2 6= r
12
Under review as a conference paper at ICLR 2021
Since 0 < p < 1, in order to get minimal K(i, j, k, r1, r2) value, h and w must satisfy |h - w| ≤ α. In this
case we have
i (P + q) ∙	α +	(h	—	W)(P —	q)	ri	=	r ∧ r2	=	r
(P + q) ∙	α +	(W	-	h)(P-	q)	Tr=	r ∧ r1=	r
2 ∙ α	ri	=	r ∧ rr	=	r
2 ∙ α	Ti	=	r ∧ rr=	r
Therefore,
£	(E(hr,r2 (Yj,Yk)) ∙ K(xi,xj,xk,r1,r2) + (E(hr2,r (Yj ,Yk)) ∙ K(xi,xj,xk,rι,rr)
r2∈{i,.r-i,r+i,.t}
=(P + q) ∙α(	E	(E(hr,r2(Yj,Yk) + (E(hr2,r(Yj,Yk))) +
r2 ∈{i,.r-i,r+i,.t}
(h—W)(P—q))(	XE(hr,r2(Yj,Yk))—E(hr2,r(Yj,Yk))
r2∈{i,.r-i,r+i,.t}
We split to three cases:
1.	Ifxj,xk ∈ Ar orxj, xk ∈/ Ar then:E(hr,r2(Yj,Yk)) =E(hr2,r(Yj,Yk)). Hence,
(h—W)(P—q))(	X E(hr,r2(Yj,Yk)) —E(hr2,r(Yj,Yk)) =0
r2∈{i,.r-i,r+i,.t}
2.	If xj ∈ Ar and xk ∈/ Ar, then E(hr,r2 (Yj, Yk)) >E(hr2,r(Yj, Yk)), therefore
(h—W)(P—q))(	XE(hr,r2(Yj,Yk))—E(hr2,r(Yj,Yk))
r2∈{i,.r-i,r+i,.t}
Since P > 0.5 and |h — W| ≤ α,the minimal value is achieved whenever h = 0 and W = α.
3.	In the same way if xk ∈ Ar and xj ∈/ Ar, then E(hhr ,r (Yj , Yk)) = E(hr,r2 (Yj , Yk)) and the
minimal value is achieved whenever h = α and W = 0.
In conclusion, if Xi ∈ Ar, an embedding f * satisfies
(EOtrip(f*))xi = min{(EOtrip(f))xi∣f ： X → Rm}
iff f * (Xj) = f * (Xi)	for every Xj	∈	Ar, and ∣∣	f * (Xj)	— f * (xi) k	= α for every Xj	∈	Ar.	□
We will now prove the same theorem with respect to the margin loss.
Theorem 2. Let f : O → Rm be an embedding, which minimize
EOmargin (f,β) = η2	X ELmargin(Xi ,Xj ),
xi,xj ∈X
then f has the class-collapsing property with respect to all classes.
Proof. Observe that if Xi , Xj ∈ Ar, then
ELmargin(Xi，Xj) = P , (Dxi,xj — βxi + α)+ + (1 — P)，(βxi - Dχi,xj + α) +
Since 0 < P < 1, then the maximal value is achieved whenever |Dxi,xj — βxi | ≤ α, in this case:
ELmargin(Xi,Xj) = (2P — I) ∙ (Dxi,xj — βxJ
In the same way in case Xi ∈ Ar and Xj ∈/ Ar then:
ELmargin(Xi, Xj) = (2P — I)，(βxi — Dxi,xj).
Combining both directions we get:
X ELLrgin(Xi,Xj ) = (2p - 1) ∙ (X Dxi,xj - X Dxi,xj )
xj ∈x	∖γj ∈a	Yj/a	)
Since: P > 0.5 and |Dxi,xj — βxi | ≤ α, the minimal value is achieved whenever Dxi,xj = 0, Dxi,xk = 2α
and βxi = α, for every Xi, Xj ∈ Ar, Xk ∈ Ar.	□
13
Under review as a conference paper at ICLR 2021
B: Easy Positive Sampling in noisy environment
In this subsection we analyse the EPS method from the theoretical prospective, using the framework defined in
Section 4. We use the same notions as in sections 3 and 4.
Define: Φ(yi, yj) = 1 yi = yj ∧ Dxi,xj = min{Dxi,xk | yk = yi} . Then, the easy positive sampling loss
0 else
can be defined by:
1 X	φ(yi,yj) ∙ Ltrip(Xi,xj ,xk)
1≤i,j,k≤n
for the triplet loss and
1
n
E (φ(yi,yj) ∙Lmargin(xi, xj)) + 1yi 6=yj L
margin(xi, xj)
1≤i,j ≤n
for the margin loss.
In the noisy environment stochastic case, using section 4 notions, Φ become a random variable:
(
Φ(Yi,匕)
Yi = Yj ∧ ∀t ((Dxi,,xt <Dxi,xj)→Yt 6= Yi)
else
Therefore, the triplet loss with EPS in the noisy environment case, become:
ELEpSt;r'i,p(.xi,xj ,xk) = E (φ(Yi,Yj ) ∙ δYi,Yj ∙ (I- δYi,Yk )) ∙ (Dxi,xj - Dxi,xk +。)十
and for the margin loss with EPS we have:
ELEPSmargin (Xi,Xj ) = E(Φ(Yi,Yj ) ∙ SYi ,j ∙ (Dfi ,叼 — βs +。)+ + E(1 — 和必) •收-。^叼 +。)十
As in section 4.2 we assume that Y = {Y1 , .., Yn} is a set of independent binary random variables. Let
Ai,.., At ⊂ X, 0.5 < p < 1 SuCh that: |Aj| = n and
P(Yi = k) = (p0	1-p Xi ∈ Ak
":=1-1 Xi ∈ Ak
For simplicity We assume that every 1 ≤ i ≤ n satisfies X n∙i , 1, ..,x n∙i . , ∈ Ai
——t	t +1	t +t
We prove first that the minimal embedding With respect to both losses does not satisfy the class collapsing
property. Let f1 be an embedding function such that:
Dxx1i,xj = (0α
(∃r)(Xi, Xj ∈ Ar)
else
and f2 an embedding such that:
Dxx11,x2 = (0α
(∃r)(χi,χj ∈ Ar )∧ 〜((i < 2n ∧ j > 2n) ∨ (i > 2n ∧ j < 2n)
else
f1 represent the case of class collapsing, Where f2 represent the case Where there are tWo modalities for the first
class. In order to shoW that the minimal embedding does not satisfy the class collapsing property it suffice to
prove that
n X	LEPStrip(Xi,xj ,xk) < 1 X	LEPStrip(XW,xj ,xk)
1≤i,j,k≤n	1≤i,j,k≤n
and
n x LEPSmargin(χi,xj) <1 X LEPSmargin(Xi, Xj).
Remark: For both losses the definition requires a strict order betWeen the elements, therefore by distance zero,
We meant infinitesimal close, the order betWeen the elements inside the sub-clusters is random, and element
betWeen set A1 are closer then set Ac1 in both embeddings. For simplification We neglect this infinitesimal
constants in the proofs.
14
Under review as a conference paper at ICLR 2021
Claim 1. There exists M such that if n ≥ M, then:
1
n
X	LEPStrip(Xi,xj,xk) <n X	LEPStrip (Xi,xj,xk)
i≤i,j,k≤n	i≤i,j,k≤n
Proof. Fix X1, WOLOG we may assume in both embeddings that Dxfj1 ,xi < Dxfj1 ,xk for every j ∈ {1, 2} and
1 ≤ i < k ≤ n. It suffice to prove that
1
n
(LfE1P Strip(X1 , Xj , Xk) -LfE2PStrip(X1 , Xj , Xk)) > 0
1≤j,k≤n
Let q = (1 - p), observe that
P(	Yi 6= Yt) = Pm+i • qjT2Tm + PjT2Tmqj+i ≤ 2PjTi
i≤t<j
where m = |{t 11 ≤ j, Yt ∈ Ai }| . Thus if j ≥ 2nt, we have
fE2P Strip(Xi , Xj , Xk) ≤ P( ^
i≤t<j
Yi = Yt) ∙ 2 ∙α ≤ 4 ∙ αpj-1
Therefore,
n(2t-1)
LEP	LEPStrip(χ1,xj,xk) ≤ £ 4∙ɑPj
j> 2⅛ ,1≤k≤n
4∙n∙aP2nt ∙
j=0
For j ≤ 答 and k
≤ 2tof k > nn, we have LfEPStrip
j n n
PJ = 4∙αp2t ∙
1- St n→∞ 0
1-q
(X1 , Xj , Xk ) =LEPStrip (X1 , Xj , Xk ). Hence, the only
case left is j ≤
n
2t
and 2nt < k ≤ In. In this case: LEPStrip(xi,Xj,Xk) = 0, where
1
n
j> 2t
^2f
LEPStrip(xι,Xj,Xk) = (p2 ∙ qjT + q2 ∙ PjT) ∙ α ≥ qj+1α
and we get:
1
n
Choosing M such that
LfE1P Strip(x1 , xj , xk) -LfE2PStrip(x1 , xj , xk) ≥
j≤ 2nt，2nt ≤k≤ n
n
2t
• Eqi = α ∙ q2 ∙
j=0
1 - qn/2t n
1-q
2	1
αq ∙----
1-q
αp
M 1 - qM(2t-1)/2t
ι - q
ɑ ∙
2
q
a∙q2 ∙ I-MI
1 - q
> 4 ∙
will satisfy that for every n > M :
1
n
LfE2PStrip (Xi , Xj , Xk)
1≤i,j,k≤n
1
n
<
LfE1PStrip (xi , xj , xk)
1≤i,j,k≤n
□
Claim 2. There exists M such that if n ≥ M then:
1 X LEPSmargin (Xi, Xj)
1≤i,j≤n
<
1
n
LfE1P Smargin (xi , xj)
1≤i,j≤n
Proof. For every 1 ≤ j ≤ n or n < j ≤ n we have:
LfE1P Smargin (Xi , Xj)
f1P Smargin (xi , xj)
For 2 < j ≤ 2 ：
LEPSmargin(Xi,xj) = 2 ∙ P 7 ∙ Bxi + (PiqjT + qp T) Y2 ∙ α - βxi )
while:
LEPSmargin(Xi,xj) = 2 ∙ P ∙ q Yexi + α) + (P qj-2 + q"'-2) ∙ (α -葭 )
15
Under review as a conference paper at ICLR 2021
Since j > 券 the second therm tend to zero. Therefore, taking M SuCh that
2qp > (p2qMt-2 + q2pMt-2)
will satisfy that for each n ≥ M
1
n
1
n
1≤i,j≤n
LfE2P
Smargin (xi, xj) <
1≤i,j≤n
LEP Smargin
(xi, xj)
□
In the previous two claims we prove that the class collapsing solution is not minimal with respect to both the
EP Smargin and the EP Striplet. In the following claims we prove that not only itis not the minimal solution,
looking locally on the direct effect of the EPS losses on a sample which is not one of the closest elements to to
the anchor. We prove that the optimal solution in this case is an embedding in which the distance between the
sample to the anchor is equal to the margin hyperparameter.
Claim 3. Let f be an embedding. For every i, let i1 , .., in be such that Dxfi,xi < Dxfi,xi < ... < Dxfi,xn,
Then there exists M such that for every j > M the minimal embedding for LfEP Smargin (xi , xj ) is achived
whenever Dxfi ,xj = βxi + α.
Proof. Fix x1 , as in the previous claims we will assume:
Dx1,x1 < Dx1,x2 < ... < Dx1,xn
As was prove in in Claim 1 P(V1≤t<j Yi 6= Yt) ≤ 4pj , thus
E(Φ(Yi,Yj) ∙ δYi,Yj) ≤ P( ^ 匕=Yt) ≤ 4pj-1 j→ 0
1≤t<j
Since the minimal solution for
E(φ(γi,γ∙) ∙ δYi,Yj) ∙ (DXi,χj - βχi + α)+ + E(I-乐,γj Yexi- Dxi,Xj + a) +
satisfies ∣βχi — Dfi,χj | ≤ α,we have:
LEPSmargin(χ1,xj ) = α。(E@(H,Yj )，δY1,Yj ) + E(I- δY1 必))+
(Dfι,Xj - βχι) ∙ (E(Φ(H,Y∙) ∙ SY1,Y∙) - E(1 - J居,Y∙))
Since E(1 — 8巧,匕)≥ 2pq, there exists M such every j > M satisfies
(E(Φ(Y1,Y∙) ∙ jYι,Yj) - E(1 - δYi,Yj)) < 0
Therefore the minimal value is achieved whenever Df ι 叼=α + βχι.	口
The proof in the EPStriplet loss case is similar.
Claim 4. Let f be an embedding. For every i, let i1 , , .., in be such that Dχfi,χi < ... < Dχfi,χn. Then there
exists M such that for every j > M the minimal embedding for:
LE P Strip
(xi , xt , xt+j ) + LfEP Strip(xi, xt+j , xt)
is achieved whenever Dχi,χt+j = Dχi,χt + α.
Proof. Define K(匕,匕,Yk) := E (Φ(K,γ∙) ∙乐必∙(1 -乐,yQ). Fixing xι, assuming。"皿 <
Dχf χ < ... < Dχf χ , We have:
χ1 ,χ2	χ1 ,χn
LEPStrip (x1，xt,xt+j ) + LEPStrip(XI, xt + j , xt) = K (Y1,Yt,Yt+j ) ∙ (DXι,χt - Dfi ,χf + α)十
+ K(Yι,匕+j ,匕)∙ (Dfι,χt+j - Dfi ,t + α) +
As in the previous claim, the minimal value is achieved whenever ∣Dfιχ+j — DΧι,χt | ≤ α in this case:
LEPStrip(XI, xt, xt+j ) + LEPStrip(XI, xt+j,Xt) = α ∙ (K (Y1 ,Yt ,Yt+j ) + K (YI, Yt+j , Yt))Yt)) +
(Dfι,χt - Dfι,χt+j) ∙(Κ^ι,Yt,Yt+j) - K(Yι,K+j,K))
16
Under review as a conference paper at ICLR 2021
Figure 5: t-SNE visualization of Cars196 training classes (each class has a different color). Training
with EPS results in more diverse classes appearance.
Table 4: Results of semi-hard with/without EPS on the Omniglot training dataset. Without EPS the
network feet almost perfectly to the training set. However, using EPS results in batter performances
on the letters fine-grained task.
	Language		Letters	
	Semi- hard	Semi- hard+EPS	Semi- hard	Semi- hard+EPS
NMI	-93.6	673-	-78.4	87.1-
R@1	99.9	94.5	70.3	77.5
R@2	100	96.8	80.4	86.3
R@4	100	98.1	87.9	92.4
R@8	100	99.2	93.3	96.0
(Qi∈{1,2,..,t,t+j} ptiq1-ti)+(Qi∈{1,2,..,t,t+j} p1-tiqti) ≥ qt+1 where
On the one hand: K(Y1, Yt, Yt+j)
1 Y ∈/ A
tk=	0 else fork∈{2,..,t-1,t+j}andtk
k ∈	for k ∈ {1, t}. On the other hand
0 else	or , . n e oer an
K(Y1Yt+j, Yt) ≥ P rob(V1≤k<t+j Y1 6= Yk) ≤ 4pt+j-1. Taking j large enough such that qt+1 ≤ 4pt+j-1,
we have:
(E (φ(Y1,Yt) ∙ δYι,Yt ∙ (1 - Jγι,Yt+jj) - E (φ(Y1,Yt+j) ∙ δYι,Yt+j ∙ (1 - δYι ,Yt))) > 0
therefore in such case the minimum is archived whenever Dxf1 ,xt+j = Dxf1 ,xt + α.
□
C: More experiments and implementation details
Embedding behavior on training sets
The class-collapsing phenomena also occur in the training process of the image retrieval datasets. Figure 5
visualise the t-SNE embedding (van der Maaten & Hinton, 2008) of Cars196 training classes. As can be seen,
when training without EPS each class fits well to a bivariate normal-distribution with small variance and different
means. Training with EPS result in more diverse distributions and in some of the classes fits batter to a mixture
of multiple different distributions.
This can also be measured qualitatively on the Omniglod detest; although training without the EPS results
in batter overfitting to training samples, the results on the letters fine-grained task are significantly inferior
comparing to training with the EPS (Table 4). It is also important to note the low NMI score when using EPS
with the number of clusters equal to the number of languages, and the increment of this score when increasing the
17
Under review as a conference paper at ICLR 2021
Table 5: Std of Recall@1 results. Each model was trained 8 times with different random seeds.
dataset	model	Without EPS Std	With EPS std
cars196	Margin	0.17	0.27
cars196	MS	0.24	0.29
cars196	Trip+SH	0.20	0.47
cub200	Margin	298	0.33
cub200	MS	0.43	0.36
cub200	Trip+SH	0.52	0.35
Omniglot-letters	Margin	073	0.58
Omniglot-letters	MS	0.52	0.71
Omniglot-letters	Trip+SH	0.34	0.61
Table 6: Results of Multi-similarity loss with Embedding size 512 (as in Wang et al. (2019)). Using
EPS improve results in both cases.
	Cars196		CUB200	
	MS	MS+EPS	MS	MS+EPS
R@1	84.1	-85.5	65.7	-66.7
R@2	90.4	90.7	77.0	77.2
R@4	94.0	94.3	86.3	86.4
R@8	96.5	96.7	91.2	90.9
number of clusters to the number of letters. This indicates that training with EPS results in more homogeneous
small clusters, which are more blended in the embedding space comparing to training without EPS.
MNIST architecture details
For the MNIST even/odd experiment we use a model consisting of two consecutive convolutions layer with
(3,3) kernels and 32,64 (respectively) filter sizes. The two layers are followed by Relu activation and batch
normalization layer, then there is a (2,2) max-pooling follows by 2 dense layers with 128 and 2 neurons
respectively.
Recognition datasets architecture details
We use an embedding of size 128, and an input size of 224X224 for the first two datasets, and 80X80 for the
Omniglot dataset. For all the experiments we used the original bounding boxes without cropping around the
object box. As a backbone for the embedding, we use ResNet50 (He et al., 2016) with pretrained weights on
imagenet. The backbone is followed by a global average pooling and a linear layer which reduces the dimension
to the embedding size. Optimization is performed using Adam with a learning rate of 10-5, and the other
parameters set to default values from Kingma & Ba (2014).
Stability analysis
Following Musgrave et al. (2020); Fehervari et al. (2019), it was important to us to have a fair comparison
between all tested models. Therefore, for all the experiments we use the same framework (Roth et al.), with
the same architecture and embedding size (128). We also did not change the default hyper-parameters in all
tested methods. We run each experiment 8 times with different random seeds, the reported results are the mean
of all the experiments. The std of the Recall@1 results of all experiments can be seen in Table 5. In all cases the
differences between the results with and without the EPS are significance.
Multi-similarity comparison
From our experiments, the Multi-similarity loss is highly affected by the batch size. Using Resnet50 backbone,
we restrict the number of batch size to 160 for all tested model, which cause to the inferior results of the
multi-similarity loss comparing to other methods. For the sake of completeness we provide the results also on
inception backbone with embedding size of 512 as in Wang et al. (2019), and batch size of 260. As can be seen
in Table 6, also in these cases the results improve when using EPS instead of semi-hard sampling on the positive
samples.
18
Under review as a conference paper at ICLR 2021
Trimming precentage	Trimming precentage
Figure 6: Recall@1 performance with Trimmed loss across varying trimming percentage. Except for
small improvement in the Distance-margin case on the Omniglot dataset, in all other cases there is no
improvement when applying the Trimmed loss.
Trimmed Loss comparison
The situation where a class consists of multiple modes can also be seen as a noisy data scenario with respect to
the embedding loss, where positive tuples consisting of examples from different modes are considered as ‘bad‘
labelling. One approach to address noisy labels is by back-propagating the loss only on the k-elements in the
batch with the lowest current loss (Shen & Sanghavi, 2019). Although this approach resembles Malisiewicz &
Efros (2008), the difference is that in Malisiewicz & Efros (2008) they apply the trimming only on the positive
tuples. We test the effect of using Trimmed Loss on random sampled triplets with different level of trimming
percentage. As can be seen in Figure 6, there is only a minor improvement when applying the loss on top of
the distance-margin loss on the Omniglot-letters dataset. This emphasizes the importance of constraining the
trimming to the positive sampling only.
19