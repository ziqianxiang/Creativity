Under review as a conference paper at ICLR 2021
Variational Dynamic Mixtures
Anonymous authors
Paper under double-blind review
Ab stract
Deep probabilistic time series forecasting models have become an integral part of
machine learning. While several powerful generative models have been proposed,
we provide evidence that their associated inference models are oftentimes too lim-
ited and cause the generative model to predict mode-averaged dynamics. Mode-
averaging is problematic since many real-world sequences are highly multi-modal,
and their averaged dynamics are unphysical (e.g., predicted taxi trajectories might
run through buildings on the street map). To better capture multi-modality, we
develop variational dynamic mixtures (VDM): a new variational family to infer
sequential latent variables. The VDM approximate posterior at each time step is a
mixture density network, whose parameters come from propagating multiple sam-
ples through a recurrent architecture. This results in an expressive multi-modal
posterior approximation. In an empirical study, we show that VDM outperforms
competing approaches on highly multi-modal datasets from different domains.
1	Introduction
Making sense of time series data is an important challenge in various domains, including ML for
climate change. One important milestone to reach the climate goals is to significantly reduce the
CO2 emissions from mobility (Rogelj et al., 2016). Accurate forecasting models of typical driving
behavior and of typical pollution levels over time can help both lawmakers and automotive engi-
neers to develop solutions for cleaner mobility. In these applications, no accurate physical model
of the entire dynamic system is known or available. Instead, data-driven models, specifically deep
probabilistic time series models, can be used to solve the necessary tasks including forecasting.
The dynamics in such data can be highly multi-modal. At any given part of the observed sequence,
there might be multiple distinct continuations of the data that are plausible, but the average of these
behaviors is unlikely, or even physically impossible. Consider for example a dataset of taxi trajec-
tories1. In each row of Fig. 1a, we have selected 50 routes from the dataset with similar starting
behavior (blue). Even though these routes are quite similar to each other in the first 10 way points,
the continuations of the trajectories (red) can exhibit quite distinct behaviors and lead to points on
any far edge of the map. The trajectories follow a few main traffic arteries, these could be considered
the main modes of the data distribution. We would like to learn a generative model of the data, that
based on some initial way points, can forecast plausible continuations for the trajectories.
Many existing methods make restricting modeling assumptions such as Gaussianity to make learn-
ing tractable and efficient. But trying to capture the dynamics through unimodal distributions can
lead either to “over-generalization”, (i.e. putting probability mass in spurious regions) or on focus-
ing only on the dominant mode and thereby neglecting important structure of the data. Even neural
approaches, with very flexible generative models can fail to fully capture this multi-modality be-
cause their capacity is often limited through the assumptions of their inference model. To address
this, we develop variational dynamic mixtures (VDM). Its generative process is a sequential latent
variable model. The main novelty is a new multi-modal variational family which makes learning
and inference multi-modal yet tractable. In summary, our contributions are
•	A new inference model. We establish a new type of variational family for variational inference
of sequential latent variables. By successively marginalizing over previous latent states, the pro-
cedure can be efficiently carried-out in a single forward pass and induces a multi-modal posterior
1https://www.kaggle.com/crailtap/taxi-trajectory
1
Under review as a conference paper at ICLR 2021
(a) Taxi Data (b) VDM (ours) (c) AESMC (d) CF-VAE (e) VRNN (f) RKN
Figure 1: Forecasting taxi trajectories is challenging due to the highly multi-modal nature of the data
(Fig. 1a). VDM (Fig. 1b) succeeds in generating diverse plausible predictions (red), based the begin-
ning of a trajectory (blue). The other methods, AESMC (Le et al., 2018), CF-VAE (Bhattacharyya
et al., 2019), VRNN Chung et al. (2015), RKN Becker et al. (2019), suffer from mode averaging.
approximation. We can see in Fig. 1b, that VDM trained on a dataset of taxi trajectories produces
forecasts with the desired multi-modality while other methods overgeneralize.
•	An evaluation metric for multi-modal tasks. The negative log-likelihood measures predictive
accuracy but neglects an important aspect of multi-modal forecasts - sample diversity. In Sec-
tion 4, we derive a score based on the Wasserstein distance (Villani, 2008) which evaluates both
sample quality and diversity. This metric complements our evaluation based on log-likelihoods.
•	An extensive empirical study. in Section 4, we use VDM to study various datasets, including
a synthetic data with four modes, a stochastic Lorenz attractor, the taxi trajectories, and a U.S.
pollution dataset with the measurements of various pollutants over time. We illustrate VDM’s
ability in modeling multi-modal dynamics, and provide quantitative comparisons to other methods
showing that VDM compares favorably to previous work.
2	Related Work
Neural recurrent models. Recurrent neural networks (RNNs) such as LSTMs (Hochreiter &
Schmidhuber, 1997) and GRUs (Chung et al., 2014) have proven successful on many time series
modeling tasks. However, as deterministic models they cannot capture uncertainties in their dy-
namic predictions. Stochastic RNNs make these sequence models non-deterministic (Chung et al.,
2015; Fraccaro et al., 2016; Gemici et al., 2017; Li & Mandt, 2018). For example, the variational
recurrent neural network (VRNN) (Chung et al., 2015) enables multiple stochastic forecasts due to
its stochastic transition dynamics. An extension of VRNN (Goyal et al., 2017) uses an auxiliary
cost to alleviate the KL-vanishing problem. It improves on VRNN inference by forcing the latent
variables to also be predictive of future observations. Another line of related methods rely on par-
ticle filtering (Naesseth et al., 2018; Le et al., 2018; Hirt & Dellaportas, 2019) and in particular
sequential Monte Carlo (SMC) to improve the evidence lower bound. In contrast, VDM adopts
an explicitly multi-modal posterior approximation. Another SMC-based work (Saeedi et al., 2017)
employs search-based techniques for multi-modality but is limited to models with finite discrete
states. Recent works (Schmidt & Hofmann, 2018; Schmidt et al., 2019; Ziegler & Rush, 2019) use
normalizing flows in the latent space to model the transition dynamics. A normalizing flow requires
many layers to transform its base distribution into a truly multi-modal distribution in practice. In
contrast, mixture density networks (as used by VDM) achieve multi-modality by mixing only one
layer of neural networks. A task orthogonal to multi-modal inference is learning disentangled repre-
sentations. Here too, mixture models are used (Chen et al., 2016; Li et al., 2017). These papers use
discrete variables and a mutual information based term to disentangle different aspects of the data.
VAE-like models (Bhattacharyya et al., 2018; 2019) and GAN-like models (Sadeghian et al., 2019;
Kosaraju et al., 2019) only have global, time independent latent variables. Yet, they show good
results on various tasks, including forecasting. With a deterministic decoder, these models focus
on average dynamics and don’t capture local details (including multi-modal transitions) very well.
Sequential latent variable models are described next.
2
Under review as a conference paper at ICLR 2021
(a) Generation (Eqs. (1) and (2))	(b) Inference (Eqs. (4), (5) and (7))
Figure 2: Graphical illustrations of VDM. Dashed lines denote deterministic dependencies such
as transformations, marginalization, or computing the mean, as explained in the main text, while
bold lines denote stochastic dependencies. The half-shaded node for st indicates that st is being
marginalized out as opposed to conditioned on.
Deep state-space models. Classical State-space models (SSMs) are popular due to their tractable
inference and interpretable predictions. Similarly, deep SSMs with locally linear transition dynam-
ics enjoy tractable inference (Karl et al., 2017; Fraccaro et al., 2017; Rangapuram et al., 2018;
Becker et al., 2019). However, these models are often not expressive enough to capture complex
(or highly multi-modal) dynamics. Nonlinear deep SSMs (Krishnan et al., 2017; Zheng et al., 2017;
Doerr et al., 2018; De Brouwer et al., 2019; Gedon et al., 2020) are more flexible. Their inference
is often no longer tractable and requires variational approximations. Unfortunately, in order for the
inference model to be tractable, the variational approximations are often simplistic and don’t ap-
proximate multi-modal posteriors well with negative effects on the trained models. Multi-modality
can be incorporated via additional discrete switching latent variables, such as recurrent switching
linear dynamical systems (Linderman et al., 2017; Nassar et al., 2018; Becker-Ehmck et al., 2019).
However, these discrete states make inference more involved.
3	Variational Dynamic Mixtures
We develop VDM, a new sequential latent variable model for multi-modal dynamics. Given sequen-
tial observations x1:T = (x1, . . . , xT), VDM assumes that the underlying dynamics are governed
by latent states z1:T = (z1, . . . , zT). We first present the generative process and the multi-modal
inference model of VDM. We then derive a new variational objective that encourages multi-modal
posterior approximations and we explain how it is regularized via hybrid-training. Finally, we intro-
duce a new sampling method used in the inference procedure.
Generative model. The generative process consists of a transition model and an emission model.
The transition model p(zt | z<t) describes the temporal evolution of the latent states and the emis-
sion model p(xt | z≤t) maps the states to observations. We assume they are parameterized by
two separate neural networks, the transition network φtra and the emission network φdec.To give
the model the capacity to capture longer range temporal correlations we parametrize the transition
model with a recurrent architecture φGRU (AUger-Methe et al., 2016; Zheng et al., 2017) such as a
GRU (Chung et al., 2014). The latent states zt are sampled recursively from
Zt | z<t 〜N(μo,t, σ2,tI),	where [μo,t, σ2,J = φtra(ht-i), ht—i = φGRU(Zt-i, ht-2), (1)
and are then decoded such that the observations can be sampled from the emission model,
xt | z≤t 〜N(μχ,t, σ2,tI), where [μxt, σ2,t] = φdec(zt, ht-i).	(2)
This generative process is similar to (Chung et al., 2015), though we did not incorporate autoregres-
sive feedback due to its negative impact on long-term generation (Ranzato et al., 2016; Lamb et al.,
2016). The competitive advantage of VDM comes from a more expressive inference model.
Inference model. VDM is based on a new procedure for multi-modal inference. The main idea is
that to approximate the posterior at time t, we can use the posterior approximation of the previous
3
Under review as a conference paper at ICLR 2021
time step and exploit the generative model’s transition model φGRU. This leads to a sequential
inference procedure. We first use the forward model to transform the approximate posterior at time
t - 1 into a distribution at time t. In a second step, we use samples from the resulting transformed
distribution and combine each sample with data evidence xt , where every sample parameterizes
a Gaussian mixture component. As a result, we obtain a multi-modal posterior distribution that
depends on data evidence, but also on the previous time step’s posterior.
In more detail, for every zt , we define its corresponding recurrent state as the transformed random
variable st = φGRU(zt, ht-1), using a deterministic hidden state ht-1 = E [st-1]. The variational
family of VDM is defined as follows:
q(z1:T
T
| x1:T) =	q(zt | x≤t)
t=1
YT Z
t=1
q(zt | st-1, xt)q(st-1
| x≤t)dst-1.
(3)
Chung et al. (2015) also use a sequential inference procedure, but without considering the distri-
bution of st . Only a single sample is propagated through the recurrent network and all other in-
formation about the distribution of previous latent states z<t is lost. In contrast, VDM explicitly
maintains st as part of the inference model. Through marginalization, the entire distribution is taken
into account for inferring the next state zt . Beyond the factorization assumption and the marginal
consistency constraint of Eq. (3), the variational family of VDM needs two more choices to be fully
specified; First, one has to choose the parametrizations of q(zt | st-1 , xt) and q(st-1 | x≤t) and
second, one has to choose a sampling method to approximate the marginalization in Eq. (3). These
choices determine the resulting factors q(zt | x≤t) of the variational family.
We assume that the variational distribution of the recurrent state factorizes as q(st-1 | x≤t) =
ω(st-ι, xt)q(st-ι | x<t), i.e. it is the distribution of the recurrent state given the past observation2,
re-weighted by a weighting function ω(st-1, xt) which involves only the current observations. For
VDM, We only need samples from q(st-ι | x<t), which are obtained by sampling from the previous
posterior approximation q(zt-1 | x<t) and transforming the sample with the RNN,
st(-i)1
〜q(st-ι | x<t) equiv. to s(-ι = ΦGRU(z(-ι, ht-2), z(-ι ~ q(zt-i | x<t),	(4)
where i indexes the samples. The RNN φGRU has the same parameters as in the generative model.
Augmenting the variational model with the recurrent state has another advantage; approximating the
marginalization in Eq. (3) with k samples from q(st-1 | x≤t) and choosing a Gaussian parametriza-
tion for q(zt | st-1, xt) results in a q-distribution q(zt | x≤t) that resembles a mixture density
network (Bishop, 2006), which is a convenient choice to model multi-modal distributions.
k
q(Zt | x≤t) = X ""N"?/!?2I),	[μZi,t,σZit2]= φinf (S(-ι, xt).	⑸
i
We assume q(zt | St-1, xt) to be Gaussian and use an inference network φinf to model the effect of
the observation xt and recurrent state St-1 on the mean and variance of the mixture components.
The mixture weights ωt(i) := ω(S(ti-)1, xt)/k come from the variational distribution q(St-1 | x≤t) =
ω(st-ι, xt)q(st-ι | x<t) and importance sampling3. We are free to choose how to parametrize the
weights, as long as all variational distributions are properly normalized. Setting
ω(i) = ω(s(-ι, Xt)Ik := l(i = arg maxp(xt | ht-1 = sj)ι)),	(6)
j
achieves this. In Appendix A, we explain this choice with importance sampling and in Appendix H,
we compare the performance of VDM under alternative variational choices for the weights.
In the next time-step, plugging the variational distribution q(zt | x≤t) into Eq. (4) yields the next
distribution over recurrent states q(st | x≤t). For this, the expected recurrent state ht-ι is required.
2q(st-1 | x<t) is the distribution obtained by transforming the previous zt-ι 〜 q(zt-ι∣x<t) through the
RNN. It can be expressed analytically using the Kronecker δ to compare whether the stochastic variable st-1
equals the output of the RNN: cj(st-ι | x<t) X R δ(st-ι - φGRU(Zt-ι, ht-2))q(zt-1 | xt-1 ,λt-1)dzt-1.
3the ω adjusts for using samples from q(st-ι | x<t) when marginalizing over ω(st-ι, Xt)q(st-ι | x<t)
4
Under review as a conference paper at ICLR 2021
We approximate the update using the same k samples (and therefore the same weights) as in Eq. (5).
ht-1
= E[st-1]
st-1 q(st-1
k
| x≤t)dst-1 ≈ X ωt(i)st(-i)1.
i
(7)
A schematic view of the generative and inference model of VDM is shown in Fig. 2. In summary,
the inference model of VDM alternates between Eqs. (4) to (7). Latent states are sampled from the
posterior approximation of the previous time-step and transformed by Eq. (4) into samples of the
recurrent state of the RNN. These are then combined with the new observation xt to produce the
next variational posterior Eq. (5) and the expected recurrent state is updated (Eq. (7)). These are then
used in Eq. (4) again. Approximating the marginalization in Eq. (3) with a single sample, recovers
the inference model of VRNN (Chung et al., 2015), and fails in modeling multi-modal dynamics as
shown in Fig. 3. In comparison, VDM’s approximate marginalization over the recurrent states with
multiple samples succeeds in modeling multi-modal dynamics.
Variational objective. We develop an objective to optimize the variational parameters of VDM
φ = [φtra, φdec, φGRU, φinf]. The evidence lower bound (ELBO) at each time step is
1k
LELBO (x≤t,φ) := k X ω(S(-i, Xt)Eq(ZtIs 巴,xt) [log P(XtI Zt, ht-1 = s(-j]
i
1k
+ k∑ω(st-1, Xt)Eq(zt∣st-ι ,xt)
i
1 P(Zt | ht-i = s(-1)
.°g q(zt | s(-ι, χt)
1k
-k Xω(St-1, χt) [logω(St-1, χt) + C
i
Claim 1. The ELBO in Eq. (8) is a lower bound on the log evidence log P(Xt | X<t),
log P(Xt | X<t) ≥ LELBO(X≤t, φ),	(see proof in Appendix B) .
(8)
(9)
In addition to the ELBO, the objective of VDM has two regularization terms,
T
LVDM (φ) =	Epdata [-LELBO (X≤t, φ) - ω1 Lpred (X≤t , φ)] + ω2Ladv (X≤t , φ) .	(10)
t=1
In an ablation study in Appendix E, we compare the effect of including and excluding the regular-
ization terms in the objective. VDM is competitive without these terms, but we got the strongest
results by setting ω1,2 = 1 (this is the only nonzero value we tried. This hyperparameter could be
tuned even further.) The first regularization term Lpred, encourages the variational posterior (from
the previous time step) to produce samples that maximize the predictive likelihood,
1k
Lpred(x≤t, φ) = log Eq(St-ι∣χ<t) [p(Xt I St-1, X<t)] ≈ log k^P(Xt I S(-J .	(11)
i
This regularization term is helpful to improve the prediction performance, since it depends on the
predictive likelihood of samples, which isn’t involved in the ELBO. The second optional regular-
ization term Ladv (Eq. (12)) is based on ideas from hybrid adversarial-likelihood training (Grover
et al., 2018; Lucas et al., 2019). These training strategies have been developed for generative mod-
els of images to generate sharper samples while avoiding “mode collapse”. We adapt these ideas to
generative models of dynamics. The adversarial term Ladv uses a forward KL-divergence, which
enables “quality-driven training” to discourage probability mass in spurious areas.
Ladv(X≤t, φ) = DKL(P(Xt I X<t)kPD(Xt I X<t)) = E [log P(Xt I X<t) - log PD (Xt I X<t)] (12)
The expectation is taken w.r.t. P(Xt I X<t). The true predictive distribution PD(Xt I X<t) is
unknown. Instead, we can train the generator of a conditional GAN (Mirza & Osindero, 2014),
while assuming an optimal discriminator. As a result, we optimize Eq. (12) in an adversarial manner,
conditioning on X<t at each time step. Details about the discriminator are in Appendix G.
5
Under review as a conference paper at ICLR 2021
Stochastic cubature approximation (SCA). The variational family of VDM is defined by a
number of modeling choices, including the factorization and marginal consistency assumptions of
Eq. (3), the parametrization of the transition and inference networks Eqs. (4) and (5), and the choice
of weighting function ω(∙). It is also sensitive to the choice of sampling method which We discuss
here. In principle, we could use Monte-Carlo methods. However, for a relatively small number
of samples k, Monte-Carlo methods don’t have a mechanism to control the quality of samples.
We instead develop a semi-stochastic approach based on the cubature approximation (Wan & Van
Der Merwe, 2000; Wu et al., 2006; Arasaratnam & Haykin, 2009), which chooses samples more
carefully. The cubature approximation proceeds by constructing k = 2d + 1 so-called sigma points,
which are optimally spread out on the d-dimensional Gaussian with the same mean and covariance
as the distribution we need samples from. In SCA, the deterministic sigma points are infused with
Gaussian noise to obtain stochastic sigma variables. A detailed derivation of SCA is in Appendix D.
We use SCA for various reasons: First, it typically requires fewer samples than Monte-Carlo meth-
ods because the sigma points are carefully chosen to capture the first two moments of the underlying
distribution. Second, it ensures a persistence of the mixture components; when we resample, we
sample another nearby point from the mixture component and not an entirely new location.
4	Evaluation and Experiments
In this empirical study, we evaluate VDM’s ability to model multi-modal dynamics and show its
competitive forecasting performance in various domains. We first introduce the evaluation metrics
and baselines. Experiments on synthetic data demonstrate that VDM is truly multi-modal thereby
supporting the modeling choices of Section 3, especially for the inference model. Then, experiments
on real-world datasets with challenging multi-modal dynamics show the benefit of VDM over state-
of-the art (deep) probabilistic time-series models.
Evaluation metrics. In the experiments, we always create a training set, a validation set, and a
test set. During validation and test, each trajectory is split into two parts; initial observations (given
to the models for inference) and continuations of the trajectories (to be predicted and not accessible
to the models). The inference models are used to process the initial observations and to infer latent
states. These are then processed by the generative models to produce forecasts.
We use 3 criteria to evaluate these forecasts (i) multi-steps ahead prediction p(xt+±t+τ | xi：t), (ii)
one-step-ahead prediction p(xt+1 | x1:t), and (iii) empirical Wasserstein distance. As in other work
(Lee et al., 2017; Bhattacharyya et al., 2018; 2019), (i) and (ii) are reported in terms of negative
log-likelihood. While the predictive distribution for one-step-ahead prediction is in closed-form,
the long-term forecasts have to be computed using samples. For each ground truth trajectory x we
generate n = 1000 forecasts Xi given initial observations from the beginning of the trajectory
NLL=Tog (n XX √2∏ exp(一(Xixɪ))，	(13)
This evaluates the predictive accuracy but neglects a key aspect of multi-modal forecasts - diversity.
We propose a new evaluation metric, which takes both diversity and accuracy of predictions into
account. It relies on computing the Wasserstein distance between two empirical distributions P , Q
W(P,Q) = inf (1 Xk(Xi- y∏(i)k2) ,	(14)
i
where X and y are the discrete samples of P and Q, and π denotes all permutations (Villani, 2008).
To use this as an evaluation measure for multi-modal forecasts, we do the following. We select n
samples from the test set with similar initial observations. If the dynamics in the data are multi-
modal the continuations of those n trajectories will be diverse and this should be reflected in the
forecasts. For each of the n samples, the model generates 10 forecasts and we get n groups of
samples. With Eq. (14) the empirical W-distance between the n true samples, and each group of
generated samples can be calculated. The averaged empirical W-distance over groups evaluates how
well the generated samples match the ground truth. Repeating this procedure with different initial
trajectories evaluates the distance between the modeled distribution and the data distribution.
6
Under review as a conference paper at ICLR 2021
VDM(k = 9)
DD	p(z2∣D) p(z2∣x≤1)
VDM (k = 1)
DD	p(z2∣D) p(z2∣x≤1)
AESMC(k = 9)
DD	p(z2∣D) p(z2∣x≤1)
Figure 3: Experiments on 2d synthetic data with 4 modes highlight the multi-modality of VDM.
We train VDM(k = 9) (left), VDM(k = 1) (middle), and AESMC(k = 9) (right) on a training set
of trajectories D of length 4, and plot generated trajectories D (2 colors for 2 dimensions). We also
plot the aggregated posterior p(z2∣D), and the predictive prior p(z2∣x≤1) (4 colors for 4 clusters,
and not related to the colors in the trajectories plot) at the second time step. Only VDM learns a
multi-modal predictive prior, which explains its success in modeling multi-modal dynamics.
Table 1:	Prediction error on stochastic Lorenz attractor for three evaluation metrics (details in main
text). VDM(k = 13) achieves the best performance, and AESMC also gives comparable results.
RKN VRNN CF-VAE AESMC
Multi-steps	104.41	65.89±0.21	32.41±0.13	25.01±0.22
One-step	1.88	-1.63	n.a	-1.69
W-distance	16.16	16.14±0.006	8.44±0.005	7.29±0.005
VDM(k = 1)	VDM(k = 13)
25.03±0.28	24.46±0.12
-1.81	-1.81
7.31±0.002	7.28±0.002
Baselines. We choose baselines from three classes of models. Two stochastic recurrent models
are variational recurrent neural network (VRNN) (Chung et al., 2015) and auto-encoding sequential
Monte Carlo (AESMC) (Le et al., 2018). VRNN has a similar but more powerful generative model
than VDM, and AESMC uses SMC to achieve a tighter lower bound. But compared to VDM, both
methods have a less powerful inference model which limits their capacity to capture multi-modal
distributions. The third baseline is a deep SSM. The recurrent Kalman network (RKN) (Becker
et al., 2019) models the latent space with a locally linear SSMs, which makes the prediction step and
update step analytic (as for Kalman filters (Kalman, 1960)). A final baseline is the conditional flow
variational autoencoder (CF-VAE) (Bhattacharyya et al., 2019), which uses conditional normalizing
flows to model a global prior for the future continuations and achieves state-of-the-art performances.
To investigate the necessity of taking multiple samples in the VDM inference model, we also com-
pared to VDM(k = 1) which uses only a single sample in Eq. (5). VDM(k = 1) has a simpler
generative model than VRNN (it considers no autoregressive feedback of the observations x), but
the same inference model. More ablations for the modeling choices of VDM are in Appendix H.
For fair comparison, we fix the dimension of the latent variables zt and ht to be the same for VDM,
AESMC, and VRNN which have the same resulting model size (except for the additional autore-
gressive feedback in VRNN). AESMC and VDM always use the same number of particles/samples.
RKN does not have recurrent states, so we choose a higher latent dimension to make model size
comparable. In contrast, CF-VAE has only one global latent variable which needs more capacity
and we make it higher-dimensional than zt . Details for each experiment are in Appendix G.
Synthetic data with multi-modal dynamics. We generate synthetic data with two dimensions and
four modes and compare the performance of VDM with 9 samples (Fig. 3, left), VDM with a single
sample (Fig. 3, middle), and AESMC using 9 particles (Fig. 3, right). Since variational inference
is known to try to match the aggregated posterior with the predictive prior (Tomczak & Welling,
2018), it is instructive to fit all three models and to look at their predictive prior p(z2∣x≤1) and the
aggregated posterior p(z2|D). Because of the multi-modal nature of the problem, all 3 aggregated
posteriors are multi-modal, but only VDM(k = 9) learns a multi-modal predictive prior (thanks to
its choice of variational family). Although AESMC achieves a good match between the prior and the
aggregated posterior, the predictive prior does not clearly separate into different modes. In contrast,
the inference model of VDM successfully uses the weights (Eq. (6)), which contain information
about the incoming observation, to separate the latent states into separate modes.
Stochastic Lorenz attractor. The Lorenz attractor is a system governed by ordinary differential
equations. We add noise to the transition and emission function to make it stochastic (details in
Appendix F.1). Under certain parameter settings it is chaotic - even small errors can cause consid-
erable differences in the future. This makes forecasting its dynamics very challenging. All models
are trained and then tasked to predict 90 future observations given 10 initial observations. Fig. 4
7
Under review as a conference paper at ICLR 2021
(a) True sample (b) VDM (ours) (C) AESMC (d) CF-VAE (e) VRNN (f) RKN
Figure 4: Generated samples from VDM and baselines for stochastic Lorenz attractor. The models
generate the remaining 990 observations (blue) based on the first 10 observations (red). Due to the
ChaotiC property, the reConstruCtion is impossible even the model learns the right dynamiCs. VDM
and AESMC Capture the dynamiCs very well, while RKN fails in Capturing the stoChastiC dynamiCs.
Trajectory
VDM(k = 13)
VDM(k = 1)
AESMC(k = 13)
Figure 5: An illustration of predictive priors p(zt|x<t) of taxi trajectories from VDM(k = 13),
VDM(k = 1), and AESMC(k = 13) at 3 forks in the road marked on the map (left). VDM(k = 13)
succeeds in capturing the multi-modal distributions, while the other methods approximate them with
uni-modal distributions. For visualization, the distributions have been projected to 2d with KDE.
Table 2:	Prediction error on taxi trajectories for three evaluation metrics (details in main text). CF-
VAE gives the best result in multi-steps prediction, since it uses one global latent variable, while
sequential models rely on a sequence of local latent variables. Meanwhile, VDM(k = 13) outper-
forms all sequential models, and performs better in other metrics than CF-VAE.
RKN VRNN CF-VAE
AESMC
Multi-steps
One-step
W-distance
4.25
-2.90
2.07
5.51±0.002	2.77±0.001	3.31±0.001
-2.77	n.a	-2.87
2.43±0.0002 0.76±0.0003	0.66±0.0004
VDM(k = 1)	VDM(k = 13)
3.26±0.001	2.85±0.002
-2.99	-3.62
0.69±0.0005	0.56±0.0005
illustrates qualitatively that VDM (Fig. 4b) and AESMC (Fig. 4c) succeed in modeling the chaotic
dynamics of the stochastic Lorenz attractor, while CF-VAE (Fig. 4d) and VRNN (Fig. 4e) miss local
details, and RKN (Fig. 4f) which lacks the capacity for stochastic transitions does not work at all.
VDM achieves the best scores on all metrics (Table 1). Since the dynamics of the Lorenz attractor
are governed by ordinary differential equations, the transition dynamics at each time step are not ob-
viously multi-modal, which explains why all models with stochastic transitions do reasonably well.
Next, we will show the advantages of VDM on real-world data with multi-modal dynamics.
Taxi trajectories. The taxi trajectory dataset involves taxi trajectories with variable lengths in
Porto, Portugal. Each trajectory is a sequence of two dimensional locations over time. Here, we cut
the trajectories to a fixed length of 30 to simplify the comparison (details in Appendix F.2). The
task is to predict the next 20 observations given 10 initial observations. Ideally, the forecasts should
follow the street map (though the map is not accessible to the models).
The results in Table 2 show that VDM outperforms the other sequential latent variable models in
all evaluations. However, it turns out that for multi-step forecasting learning global structure is
advantageous, and CF-VAE which is a global latent variable model, achieves the highest results.
However, this value doesn’t match the qualitative results in Fig. 1. Since CF-VAE has to encode the
entire structure of the trajectory forcast into a single latent variable, its predictions seem to average
over plausible continuations but are locally neither plausible nor accurate. In comparison, VDM and
the other models involve a sequence of latent variables. As the forecasting progresses, the methods
update their distribution over latest states, and the impact of the initial observations becomes weaker
and weaker. As a result, local structure is captured more accurately. While the forecasts are plausible
and can be highly diverse, they potentially evolve into other directions than the ground truth. For
this reason, their multi-step prediction results are worse in terms of log-likelihood. That’s why
the empirical W-distance is useful to complement the evaluation of multi-modal tasks. It reflects
that the forecasts of VDM are diverse and plausible. Additionally, we illustrate the predictive prior
p(zt|x<t) at different time steps in Fig. 5. VDM(k = 13) learns a multi-modal predictive prior,
which VDM(k = 1) and AESMC approximate it with an uni-modal Gaussian.
8
Under review as a conference paper at ICLR 2021
(a) NBA data (b) VDM (ours)
(c) AESMC
(d) CF-VAE (e) VRNN
(f) RKN
Figure 6: VDM and CF-VAE generate plausible multi-modal trajectories of basketball plays. Each
model’s forecasts (blue) are based on the first 10 observations (red). Ground truth data is green.
Table 3:	Prediction error on U.S. pollution data for two evaluation metrics (details in main text).
VDM makes the most accurate multi-step and one-step predictions.
RKN VRNN CF-VAE AESMC
Multi-Steps--53.13^^49.32±0.13^^45.86±0.04^^41.14±0.13
One-step 6.98	8.69	n,a	6.93
VDM(k = 1)	VDM(k = 17)
42.33±0.11	36.72±0.08
7.97	6.05
Table 4:	Prediction error on basketball players’ trajectories (details in main text). VDM makes the
most accurate multi-step and one-step predictions.
RKN VRNN CF-VAE AESMC
MUlti-StePS--488^^5.42±0.009^^3.24±0.003^^3.74±0.003
One-step 1.55	-2.78	n.a	-3.91
VDM(k = 1)	VDM(k = 13)
3.56±0.005	3.18±0.005
-4.26	-5.18
U.S. pollution data. In this experiment, we study VDM on the U.S. pollution dataset (details in
Appendix F.3). The data is collected from counties in different states from 2000 to 2016. Each
observation has 12 dimensions (mean, max value, and air quality index of NO2, O3, SO2, and O3).
The goal is to predict monthly pollution values for the coming 18 months, given observations of
the previous six months. We ignore the geographical location and time information to treat the
development tendency of pollution in different counties and different times as i.i.d.. The unknown
context information makes the dynamics multi-modal and challenging to predict accurately. Due
to the small size and high dimensionality of the dataset, there are not enough samples with very
similar initial observations. Thus, we cannot evaluate empirical W-distance in this experiment. In
multi-step predictions and one-step predictions, VDM outperforms the other methods.
NBA SportVu data. This dataset4 of sequences of 2D coordinates describes the movements of
basketball players and the ball. We extract the trajectories and cut them to a fixed length of 30 to
simplify the comparisons (details in Appendix F.4). The task is to predict the next 20 observations
given 10 initial observations. Players can move anywhere on the court and hence their movement
is less structured than the taxi trajectories which are constrained by the underlying street map. Due
to this, the initial movement patters are not similar enough to each other to evaluate empirical W-
distance. In multi-step and one-step predictions, VDM outperforms the other baselines (Table 4).
Fig. 6 illustrates qualitatively that VDM (Fig. 6b) and CF-VAE (Fig. 6d) succeed in capturing the
multi-modal dynamics. The forecasts of AESMC (Fig. 6c) are less plausible (not as smooth as data),
and VRNN (Fig. 6e) and RKN (Fig. 6f) fail in capturing the multi-modality.
5 Conclusion
We have presented variational dynamic mixtures (VDM), a sequential latent variable model for
multi-modal dynamics. The main contribution is a new variational family. It propagates multiple
samples through an RNN to parametrize the posterior approximation with a mixture density network.
Additionally, we have introduced the empirical Wasserstein distance for the evaluation of multi-
modal forecasting tasks, since it accounts for forecast accuracy and diversity. VDM succeeds in
learning challenging multi-modal dynamics and outperforms existing work in various applications.
4A version of the dataset is available at https://www.stats.com/data-science/
9
Under review as a conference paper at ICLR 2021
References
Ienkaran Arasaratnam and Simon Haykin. Cubature kalman filters. IEEE Transactions on automatic
control, 54(6):1254-1269, 2009.
Marie Auger-Methe, Chris Field, Christoffer M Albertsen, Andrew E Derocher, Mark A Lewis,
Ian D Jonsen, and Joanna Mills Flemming. State-space models’ dirty little secrets: even simple
linear gaussian models can have estimation problems. Scientific reports, 6(1):1-10, 2016.
Philipp Becker, Harit Pandya, Gregor Gebhardt, Cheng Zhao, James Taylor, and Gerhard Neumann.
Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces. In
Thirty-sixth International Conference on Machine Learning, 2019.
Philip Becker-Ehmck, Jan Peters, and Patrick Van Der Smagt. Switching linear dynamics for varia-
tional bayes filtering. In International Conference on Machine Learning, pp. 553-562, 2019.
Apratim Bhattacharyya, Bernt Schiele, and Mario Fritz. Accurate and diverse sampling of sequences
based on a “best of many” sample objective. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 8485-8493, 2018.
Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele, and Christoph-Nikolas
Straehle. Conditional flow variational autoencoders for structured sequence prediction. arXiv
preprint arXiv:1908.09008, 2019.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems, pp. 2980-2988, 2015.
Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous
modeling of sporadically-observed time series. In Advances in Neural Information Processing
Systems, pp. 7377-7388, 2019.
Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Tous-
saint, and Sebastian Trimpe. Probabilistic recurrent state-space models. In Thirty-fifth Interna-
tional Conference on Machine Learning, 2018.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in neural information processing systems, pp. 2199-2207,
2016.
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition
and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information
Processing Systems, pp. 3601-3610, 2017.
Daniel Gedon, Niklas Wahlstrom, Thomas B Schon, and Lennart Ljung. Deep state space models
for nonlinear system identification. arXiv preprint arXiv:2003.14162, 2020.
Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir Mohamed, Danilo J
Rezende, David Amos, and Timothy Lillicrap. Generative temporal models with memory. arXiv
preprint arXiv:1702.04649, 2017.
Anirudh Goyal Alias Parth Goyal, Alessandro Sordoni, Marc-Alexandre Cote, Nan Rosemary Ke,
and Yoshua Bengio. Z-forcing: Training stochastic recurrent networks. In Advances in neural
information processing systems, pp. 6713-6723, 2017.
10
Under review as a conference paper at ICLR 2021
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Combining maximum likelihood and
adversarial learning in generative models. In Thirty-Second AAAI Conference on Artificial Intel-
ligence, 2018.
Marcel Hirt and Petros Dellaportas. Scalable bayesian learning for state space models using vari-
ational inference with smc samplers. In The 22nd International Conference on Artificial Intelli-
gence and Statistics, pp. 76-86, 2019.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational
bayes filters: Unsupervised learning of state space models from raw data. In 5th International
Conference on Learning Representations, 2017.
Vineet Kosaraju, Amir Sadeghian, Roberto MartIn-Mart´n,lan Reid, Hamid Rezatofighi, and Silvio
Savarese. Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention
networks. In Advances in Neural Information Processing Systems, pp. 137-146, 2019.
Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state
space models. In Thirty-first aaai conference on artificial intelligence, 2017.
Alex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying Zhang, Saizheng Zhang, Aaron C Courville,
and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In
Advances In Neural Information Processing Systems, pp. 4601-4609, 2016.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood. Auto-encoding sequential
monte carlo. In International Conference on Learning Representations, 2018.
Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan
Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 336-345,
2017.
Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder. In Thirty-fifth International
Conference on Machine Learning, 2018.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual
demonstrations. In Advances in Neural Information Processing Systems, pp. 3812-3822, 2017.
Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski.
Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial
Intelligence and Statistics, pp. 914-922, 2017.
Thomas Lucas, Konstantin Shmelkov, Karteek Alahari, Cordelia Schmid, and Jakob Verbeek. Adap-
tive density estimation for generative models. In Advances in Neural Information Processing
Systems, pp. 11993-12003, 2019.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Christian Naesseth, Scott Linderman, Rajesh Ranganath, and David Blei. Variational sequential
monte carlo. In International Conference on Artificial Intelligence and Statistics, pp. 968-977.
PMLR, 2018.
Josue Nassar, Scott Linderman, Monica Bugallo, and Il Memming Park. Tree-structured recurrent
switching linear dynamical systems for multi-scale modeling. In International Conference on
Learning Representations, 2018.
Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and
Tim Januschowski. Deep state space models for time series forecasting. In Advances in neural
information processing systems, pp. 7785-7794, 2018.
11
Under review as a conference paper at ICLR 2021
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. In 4th International Conference on Learning Representations,
2016.
Joeri Rogelj, MicheI Den Elzen, Niklas HOhne, Taryn Fransen, Hanna Fekete, Harald Winkler,
Roberto Schaeffer, Fu Sha, Keywan Riahi, and Malte Meinshausen. Paris agreement climate
proposals need a boost to keep warming well below 2 c. Nature, 534(7609):631-639, 2016.
Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio
Savarese. Sophie: An attentive gan for predicting paths compliant to social and physical con-
straints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1349-1358, 2019.
Ardavan Saeedi, Tejas D Kulkarni, Vikash K Mansinghka, and Samuel J Gershman. Variational
particle approximations. The Journal of Machine Learning Research, 18(1):2328-2356, 2017.
Florian Schmidt and Thomas Hofmann. Deep state space models for unconditional word generation.
In Advances in Neural Information Processing Systems, pp. 6158-6168, 2018.
Florian Schmidt, Stephan Mandt, and Thomas Hofmann. Autoregressive text generation beyond
feedback loops. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 3391-3397, 2019.
Jakub M Tomczak and Max Welling. Vae with a vampprior. In 21st International Conference on
Artifi-cial Intelligence and Statistics, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Eric A Wan and Rudolph Van Der Merwe. The unscented kalman filter for nonlinear estimation.
In Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and
Control Symposium (Cat. No. 00EX373), pp. 153-158. Ieee, 2000.
Yuanxin Wu, Dewen Hu, Meiping Wu, and Xiaoping Hu. A numerical-integration perspective on
gaussian filters. IEEE Transactions on Signal Processing, 54(8):2910-2921, 2006.
Xun Zheng, Manzil Zaheer, Amr Ahmed, Yuan Wang, Eric P Xing, and Alexander J Smola. State
space lstm models with particle mcmc inference. arXiv preprint arXiv:1711.11179, 2017.
Zachary M Ziegler and Alexander M Rush. Latent normalizing flows for discrete sequences. In
Thirty-sixth International Conference on Machine Learning, 2019.
12
Under review as a conference paper at ICLR 2021
A Supplementary to Weighting function
In this Appendix we give intuition for our choice of weighting function Eq. (6). Since we approx-
imate the integrals in Eqs. (3) and (7) with samples from q(st-ι | x<t) 5 instead of samples from
q(st-1 | x≤t), importance sampling tells us that the weigths should be
(	)=q(st-i | x≤t) = q(xt | St-1, x<t)贸St-1 | x<t)
3 St-1, Xt	q(st-ι | χ<t)	q(χt | χ<t)	q(st-ι | χ<t)
q(xt | St-1, x<t)
= ——Z~~I-----ʌ— H q(xt | St-1, x<t)	(15)
q(xt | x<t)
This is consistent with out earlier definition of q(st-ι | x≤t) = ω(st-ι, xt)q(st-ι | x<t). The
weights are proportional to the likelihood of the variational model q(xt | St-1, x<t). We choose to
parametrize it using the likelihood of the generative model p(xt | ht-1 = St-1) and get
ω(i) = ω(st-ι, xt)∕k ：= l(i = arg max P(Xt | ht-1 = sj)ι)).	(16)
j
With this choice of the weighting function, only the mixture component with the highest likelihood
is selected to be in charge of modeling the current observation Xt. As a result, other mixture compo-
nents have the capacity to focus on different modes. This helps avoid the effect of mode-averaging.
An alternative weight function is given in Appendix H.
B S upplementary to Lower B ound
Claim. The ELBO in Eq. (8) is a lower bound on the log evidence log p(Xt | X<t),
log p(Xt | X<t) ≥ LELBO (X≤t, φ) .
(17)
Proof. We write the data evidence as the double integral over the latent variables zt, and z<t.
log P(Xt | X<t) = log P(Xt | z≤t, X<t)P(zt | z<t,X<t)P(z<t | X<t)dztdz<t (18)
We multiply the posterior at the previous time step P(z<t | X<t) with the ratio of the approximated
posterior q(Z<t∣X<t) and the ratio f(j,b), where f is any suitable function of two variables a and b.
The following equality holds, since the ratios equal to one.
log P(Xt | X<t)
=log Z f (a, b) q(z<t |	x<t)p(z<t	I	X<t) Zp(xt	I	z≤t,	x<t)p(zt	|	z<t, x<t)dztdz<t	(19)
f(a, b) q(z<t | X<t)
We move the integral over z<t with respect to f(a, b)q(z<t I x<t) out of the log operation with
applying the Jensen’s inequality.
log p(Xt | X<t) ≥ Ef(a,b)q(z<t|x<t
) log p(Xt | z≤t, X<t)p(zt | z<t, X<t)dzt
(20)
q(z<t | X<t)
-Ef(a,b)q(z<t|x<t) log f (a, b) + log p(z<t | x<t)
We introduce the variational posterior q(zt | z<t, X≤t), and apply Jensen’s inequality to replace the
intractable integral log p(Xt | z≤t, X<t)p(zt | z<t, X<t)dzt with its lower bound.
logP(XtI x<t) ≥ Ef(a,b)q(z<t∣x<t) Eq(zt∣z<t,x≤t) log
p(Xt | z≤t,X<t)p(zt | z<t, X<t)
q(zt | z<t, X≤t)
- Ef (a,b)q(z<t|x<t)
log f(a, b) + log
q(z<t | X<t)
p(z<t | χ<t)..
5The 〜just helps to visually distinguish the two distributions that appear in the main text.
(21)
13
Under review as a conference paper at ICLR 2021
The expectation with respect to f(a, b)q(z<t | x<t) is approximated with samples. Instead of re-
sampling the entire history, samples from previous time steps are reused (they have been aggregated
by the RNN) and we sample according to Eq. (4). We plugg in the weighting function ω(s(ti-)1, xt)
for f (a, b). The term log P(Z<t∣X<t) is not affected by the incoming observation Xt and can be
treated as a constant.
In this step, we plug in our generative model and inference model as they are described in the main
text for p and q . The conditional independence assumptions can be read of Fig. 2. In the generative
model ht-1 and in the inference model st-1 summarize the dependencies ofzt on the previous latent
variables z<t and observations X<t. In other words, we assume zt is conditionally independent on
z<t and X<t given st(i-)1 in the inference model (or given ht-1 in the generative model).
1k
logP(XtI x<t) ≥ k X ω(St-1, Xt)Eq(Zt∣s(i)],xt) [logP(XtI zt, ht-1 = s(-ι)]
i
1k
+ k∑ω(st-l, Xt)Eq(Zt∣st-ι,xt)
i
1	P(ZtI ht-1 = s(-ι)
log —1L~~—
q(Zt I St-1,Xt)
1k
-k X ω(St-1, Xt) [logω(St-1, Xt) + C
i
(22)
□
C Algorithms of Generative Model and Inference Model
Algorithm 1 Generative model
Inputs: [μz,τ,σ2,τ], hτ-1
Outputs: xτ +1:T
ZT 〜N(μz,τ, σZ,τI)
hτ = φGRU (zτ, hτ-1)
for t = τ + 1 : T do
[μο,t,σ2,t] = φtra(ht-ι)
Zt 〜N(μο,t,σ2,tI)
ht = φGRU(zt , ht-1)
[μχ,t,σX,t] = φdec(zt, ht-ι)
Xt 〜N(μx,t,σX,tI)
end for
Algorithm 2 Inference model
Inputs: xi：t, hο
Outputs: [μz,Lτ,σ2,i:", hτ-1
[μz,ι,σ2,ι] = φinf (hο, xι)
for t = 2 : τ do
Z(21 〜N(μz,t-ι,σ2,t-ιI)
st(i-)1 =φGRU(Z(ti-)1,ht-2)
[μZit,σ(it2 * *] = φinf(s(-ι, Xt)
ω(i) := l(i = arg maXj P(Xt | ht-ι = s(-)1))
[μz,t,σ2,t] = Pk ω(i)N(μZi,t,σ(i)2I)
ht-1 ≈ Pik ωt(i)s(ti-)1
end for
D Supplementary to Stochastic Cubature Approximation
Cubature approximation. The cubature approximation is widely used in the engineering com-
munity as a deterministic method to numerically integrate a nonlinear function f (∙) of Gaussian
random variable Z 〜N(μz, σZ1), with Z ∈ Rd. The method proceeds by constructing 2d + 1 sigma
points z(i) = μz + σzξ(i). The cubature approximation is SimPIy a weighted sum of the sigma points
propagated through the nonlinear function f (∙),
2d+1
f (Z)N (Z I μz, σz2I)dZ ≈ X γ(i)f(Z(i)) .
i=1
Simple analytic formulas determine the computation of weights γ(i) and the locations ofξ(i).
(23)
14
Under review as a conference paper at ICLR 2021
γ(i)
2 2(n+κ)
I
n+κ
,i = 1, ..., 2n
,i=0
,i = 1,..., n
,i = n + 1, ..., 2n
,i=0,
(24)
where κ is a hyperparameter controlling the spread of the sigma points in the n-dimensional sphere.
Further ei represents a basis in the n-dimensional space, which is choosen to be a unit vector in
cartesian space, e.g. e1 = [1, 0, ..., 0].
Stochastic cubature approximation. In SCA, we adopt the computation of ξ(i) in Eq. (24), and
infuse the sigma points with standard Gaussian noise E 〜N(0, I) to obtain stochastic sigma vari-
ables s(i) = μz + σz(ξ(i) + e). We choose K = 0.5 to set the weights Y(i) equally.
E S upplementary to Ablation S tudy of Regularization Terms
We investigate the effect of the regularization terms using the synthetic data from Fig. 3. We can see
in Table 5, VDM(k = 9) can be trained successfully with LELBO only, and both regularization terms
improve the performance (negative log-likelihood of multi-steps ahead prediction), while VDM(k =
1) doesn’t work whatever the regularization terms. Additionally, we tried to train the model only
with the regularization terms (each separate or together) but these options diverged during training.
Table 5: Ablation study of the regularization terms for synthetic data from Fig. 3
LELBO	LELBO&Lpred	LELBO&Ladv LVDM
VDM(k = 9)--2.439±0.005^^2.379±0.008	2.381±0.006^^2.363±0.004
VDM(k = 1)3.756±0.003	3.960±0.008	3.743±0.005	3.878±0.007
F Supplementary to Experiments Setup
F.1 Stochastic lorenz attractor setup
Lorenz attractor is a system of three ordinary differential equations:
dx	dy	dz
dt = σ(y-X),	dt = X(P-Z)-y,	dt = Xy TZ ,	(25)
where σ, ρ, and β are system parameters. We set σ = 10, ρ = 28 and β = 8/3 to make the system
chaotic. We simulate the trajectories by RK4 with a step size of 0.01. To make it stochastic, we add
process noise to the transition, which is a mixture of two Gaussians 0.5N(m0, P) + 0.5N (m2 , P),
where
0.06 0.03 0.01
P = 0.03 0.03 0.03 .	(26)
0.01 0.03 0.05
m0
010
m1
0
-1
0
Besides, we add a Gaussian noise with zero mean and diagonal standard deviation [0.6, 0.4, 0.8]
as the observation noise. Totally, we simulate 5000 sequences as training set, 200 sequences as
validation set, and 800 sequences as test set. For evaluation of Wasserstein distance, we simulate 10
groups of sequences additionally. Each group has 100 sequences with similar initial observations.
F.2 Taxi trajectories setup
The full dataset is very large and the length of trajectories varies. We select the trajectories inside
the Porto city area with length in the range of 30 and 45, and only extract the first 30 coordinates
of each trajectory. Thus we obtain a dataset with a fixed sequence length of 30. We split it into the
training set of size 86386, the validation set of size 200, and the test set of size 10000.
15
Under review as a conference paper at ICLR 2021
F.3 U.S. pollution data setup
The U.S. pollution dataset consists of four pollutants (NO2, O3, SO2 and O3). Each of them has 3
major values (mean, max value, and air quality index). It is collected from counties in different states
for every day from 2000 to 2016. Since the daily measurements are too noisy, we firstly compute
the monthly average values of each measurement, and then extract non-overlapped segments with
the length of 24 from the dataset. Totally we extract 1639 sequences as training set, 25 sequences as
validation set, and 300 sequences as test set.
F.4 NBA SportVu data setup
We use a sliding window of the width 30, and the stride 30 to cut the long sequences to short
sequences of a fixed length 30. We split them into the training set of size 8324, the validation set of
size 489, and the test set of size 980.
G Implementation Details
Here, we provide implementation details of VDM models used across the three datasets in the main
paper. VDM consists of
•	encoder: embed the first observation x0 to the latent space as the initial latent state z0 .
•	transition network: propagate the latent states zt .
•	decoder: map the latent states zt and the recurrent states ht to observations xt .
•	inference network: update the latent states zt given observations xt .
•	latent GRU: summarize the historic latent states z≤t in the recurrent states ht.
•	discriminator: be used for adversarial training.
The optimizer is Adam with the learning rate of 1e - 3. In all experiments, the networks have the
same architectures but different sizes. The model size depends on observation dimension dx , latent
state dimension dz , and recurrent state dimension dh . The number of samples used at each time
step in the training is 2dz + 1. If the model output is variance, we use the exponential ofit to ensure
its non-negative.
•	Encoder: input size is dx ; 3 linear layers of size 32, 32 and 2dz , with 2 ReLUs.
•	Transition network: input size is dh ; 3 linear layers of size 64, 64, and 2dz , with 3 ReLUs.
•	Decoder: input size is dh + dz ; 3 linear layers of size 32, 32 and 2dx, with 2 ReLUs.
•	Inference network: input size is dh + dx ; 3 linear layers of size 64, 64, and 2dz , with 3
ReLUs.
•	Latent GRU: one layer GRU of input size dz and hidden size dh
•	Discriminator: one layer GRU of input size dx and hidden size dh to summarize the pre-
vious observations as the condition, and a stack of 3 linear layers of size 32, 32 and 1, with
2 ReLUs and one sigmoid as the output activation, whose input size is dh + dx .
Stochastic Lorenz attractor. Observation dimension dx is 3, latent state dimension dz is 6, and
recurrent state dimension dh is 32.
Taxi trajectories. Observation dimension dx is 2, latent state dimension dz is 6, and recurrent
state dimension dh is 32.
U.S. pollution data6 Observation dimension dx is 12, latent state dimension dz is 8, and recurrent
state dimension dh is 48.
6https://www.kaggle.com/sogun3/uspollution
16
Under review as a conference paper at ICLR 2021
NBA SportVu data. Observation dimension dx is 2, latent state dimension dz is 6, and recurrent
state dimension dh is 32.
Here, we give the number of parameters for each model in different experiments in Table 6.
Table 6: Number of parameters for each model in three experiments. VDM, AESMC, VRNN, and
RKN have comparable number of parameters. CF-VAE has much more parameters.
	RKN	VRNN	CF-VAE	AESMC	VDM
Lorenz	23170	22506	7497468	22218	22218
Taxi	23118	22248	7491123	22056	22056
Pollution	35774	33192	8162850	31464	31464
SportVu	23118	22248	7491123	22056	22056
H Additional Evaluation Results
We evaluate more variants of VDM in the chosen experiments to investigate the different choices of
sampling methods (Monte Carlo method, and SCA) and weighting functions (Eqs. (27) and (28)).
In addition to Eq. (27) described in the main text, we define one other choice in Eq. (28).
ω(i) = ω(s(-ι, Xt)/k := l(i = arg maxP(Xt | ht-i = Sj)I))	(27)
j
ω(i) = ω(st-ι, Xt)/k := l(i = j 〜Cat(∙ | ω1,..., ωk)), ωj (X P(Xt | ht-i = Sj)i),	(28)
We define the weighting function as an indicator function, in Eq. (27) we set the non-zero component
by selecting the sample that achieves the highest likelihood, and in Eq. (28) the non-zero index
is sampled from a categorical distribution with probabilities proportional to the likelihood. The
first choice (Eq. (27)) is named with δ-function, and the second choice (Eq. (28)) is named with
categorical distribution. Besides, in VDM-Net, we evaluate the performance of replacing the closed-
Table 7: Definition of VDM variants
	VDM(k =	1)	VDM-MC+δ	VDM-SCA+Cat	VDM-SCA+δ
Sampling method Weighting function	Monte-Carlo n.a.		Monte-Carlo δ-function	SCA Categorical distribution	SCA δ-function
form inference of the weighting function with an additional inference network. In Table 7, we show
the choices in different variants. All models are trained with LELBO&Lpred.
H. 1 Stochastic Lorenz attractor
Table 8: Ablation study of VDM’s variants on stochastic Lorenz attractor for three distance metrics
(see main text). The variants are defined in Table 7. All variants give comparable quantitative results.
VDM(k = 1) VDM-Net	VDM-MC+δ VDM-SCA+Cat VDM-SCA+δ
Multi-steps	25.03±0.28	26.65±0.15	24.67±0.16	24.69±0.16	24.49±0.16
One-step	-1.81	-1.71	-1.84	-1.83	-1.81
W-distance	7.31±0.002	7.68±0.002	7.31±0.005	7.30±0.009	7.29±0.003
H.2 Taxi trajectories
H.3 U.S. pollution data
17
Under review as a conference paper at ICLR 2021
Table 9:	Ablation study of VDM’s variants on taxi trajectories for three distance metrics (see main
text). The variants are defined in Table 7. VDM-SCA+δ outperforms other variants and approaches
our default VDM (trained with Ladv additionally).
	VDM(k = 1)	VDM-Net
Multi-StePs	3.26±0.001	3.68±0.002
One-step	-2.99	-2.74
W-distance	0.69±0.0005	0.79±0.0003
VDM-MC+δ	VDM-SCA+Cat	VDM-SCA+δ
3.17±0.001	3.09±0.001	2.88±0.002
-3.21	-3.24	-3.68
0.70±0.0008	0.64±0.0005	0.59±0.0008
Table 10:	Ablation study of VDM’s variants on U.S. pollution data for two distance metrics (see
main text). The variants are defined in Table 7. VDM-SCA+δ outperforms other variants.
VDM(k = 1)^^VDM-Net^^VDM-MC+δ^^VDM-SCA+Cat^^VDM-SCA+δ
MuKi-StePs--42.33±0.11 ~~52.44±0.04^^40.33±0.03	39.58±0.09	37.64±0.07
One-step	7.97	10.70	8.12	7.82	6.91
(a) VDM-SCA+δ (b) VDM-SCA+Cat	(C) VDM-MC+δ	(d) VDM-Net	(e) VDM(k = 1)
Figure 7: Generated trajectories of stochastic Lorenz attractor from VDM variants. The first ten
observations (red) are obtained from models given the first 10 true observations. The rest 990 ob-
servations (blue) are prediCted. We Can see, all variants give very good qualitative results. SinCe
the fundamental dynamiCs is govern by ordinary differential equations, the transition at eaCh time
step is not highly multi-modal. OnCe the model is equipped with a stoChastiC transition, it is able to
model this dynamiCs.
(a) VDM (ours) (b) AESMC	(C) CF-VAE	(d) VRNN	(e) RKN
Figure 8: Generated 50 taxi trajeCtories in 3 different areas from VDM and the baselines. All
models are required to prediCt the future Continuations (red), based the beginning of a trajeCtory
(blue). VDM generates more plausible trajeCtories Compared with the baselines. While the gener-
ated trajeCtories from VDM follow the street map, the generated trajeCtories from all baselines are
physiCally impossible. AESMC and CF-VAE Can Capture the general evolving direCtion, but suffer
from Capturing the multi-modality at eaCh time step.
18
Under review as a conference paper at ICLR 2021
(a) VDM-SCA+δ (b) VDM-SCA+Cat (c) VDM-MC+δ (d) VDM-Net	(e) VDM(k = 1)
Figure 9: Generated 50 taxi trajectories from VDM variants. All models are required to predict the
future continuations (red), based the beginning of a trajectory (blue). VDM-SCA+δ achieves the
best qualitative results among all variants. VDM-SCA+δ can generate plausible trajectories, even
it is trained without the adversarial term Ladv . We can see, for the weighting function, Eq. (27) is
better than Eq. (28), and for the sampling method, SCA is better than Monte-Carlo method.
19