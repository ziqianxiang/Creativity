Under review as a conference paper at ICLR 2021
Faster Federated Learning with Decaying
Number of Local SGD Steps
Anonymous authors
Paper under double-blind review
Ab stract
In Federated Learning (FL), client devices collaboratively train a model without
sharing the private data present on the devices. Federated Stochastic Gradient De-
scent (FedSGD) is a recent generalisation of the popular Federated Averaging al-
gorithm. Recent works show that when client data is distributed heterogeneously,
the loss function minimised by FedSGD differs from the true loss that would be
minimised by centralised training. Previous works propose decaying the client
learning rate, γ, to allow FedSGD to minimise the true loss. We propose instead
decaying the number of local SGD steps, K, that clients perform during training
rounds to allow minimisation of the true loss. Decaying K has the added benefit
of reducing the total computation that clients perform during FedSGD. Real-world
applications of FL use large numbers of low-powered smartphone or Internet-of-
Things clients, so reduction of computation would provide significant savings in
terms of energy and time. In this work, we prove for quadratic objectives that
annealing K allows FedSGD to approach the true minimiser. We then perform
thorough experimentation on three benchmark FL datasets to show that decaying
K can achieve the same generalisation performance as decaying γ , but with up to
3.8× less total steps of SGD performed by clients.
1	Introduction
Federated Learning (FL) is a recent distributed machine learning paradigm that aims to collabo-
ratively train a model using data owned by numerous clients, without these clients sharing their
potentially sensitive data. Practical applications of FL range from ‘cross-device’ scenarios, where
a huge number of typically unreliable clients with small quantities of data per client participate, to
‘cross-silo’ scenarios with smaller numbers of reliable clients, each possessing larger quantities of
data (Kairouz et al., 2019). Cross-device tasks include mobile-keyboard next-word prediction (Hard
et al., 2018), (Ramaswamy et al., 2019), (Wang et al., 2019), digital-assistant keyword-spotting
(Leroy et al., 2019), or content-recommendation (Ammad-ud-din et al., 2019). Cross-silo tasks can
involve medical institutions processing healthcare data (Sheller et al., 2020), (Huang et al., 2019).
The challenges faced in developing practical and useful FL systems (in particular for cross-device
FL) include: the heterogeneous distribution of data across client devices; the communication costs
associated with the training process; and unreliable client devices that can arbitrarily join and leave
the FL process. McMahan et al. (2017) published an early FL algorithm, Federated Averaging (Fe-
dAvg), which has been the subject of much study. FedAvg works in an iterative fashion similar to
distributed SGD, but performs multiple steps of SGD on clients between aggregations. The com-
munication overhead of uploading and downloading the FL model is therefore significantly reduced
over distributed SGD, at the cost of performing more total computation on clients during the training
process. Myriad extensions to FedAvg and iterative FL have been proposed including compression
of the communicated model gradients (Sun et al., 2020), multi-task FL (Smith et al., 2017), FL with
decision trees (Li et al., 2020a), iterative FL where client neurons are aligned between steps (Wang
et al., 2020), and more. Comprehensive surveys ofFL are provided by Kairouz et al. (2019), Li et al.
(2020b), Lo et al. (2020).
Fewer works have been published on analysing the convergence properties of FL and FedAvg, es-
pecially in the heterogeneous setting. Woodworth et al. (2020) proved that, for quadratic objec-
tives, Local-SGD methods (of which FedAvg is a special case) converge at least as fast as dis-
1
Under review as a conference paper at ICLR 2021
tributed SGD, but that Local-SGD does not dominate distributed SGD for convex problems. Li
et al. (2020c) proved a convergence rate of O(十)for FedAvg on strongly convex problems, with
non-Independently, Identically Distributed (non-iid) client data, and without all clients participating
in each round. They also highlighted the need for decaying the client learning rate for FedAvg to
converge. With the aim of improving the convergence speed of FedAvg, Reddi et al. (2020) gen-
eralised FedAvg to update the global model by treating client model updates as a psuedo-gradient.
The global model is then updated with an SGD-like step incorporating a server learning rate, η. This
generalisation is named FedSGD, of which FedAvg is a special case (when η = 1). The authors
then extended FedSGD use adaptive optimization schemes such as Adam (Kingma & Ba, 2014)
when updating the global model (FedAdam). The authors provide convergence rates for FedAdam
in the convex setting with full device participation. Broadly, they showed that a larger number
of local iterations, K, leads to fewer rounds of communication for iid clients. Charles & Konecny
(2020) generalised several distributed learning algorithms (termed Local-Update methods) and anal-
ysed their convergence in the quadratic setting. They showed that, by performing multiple steps of
SGD between aggregations, Local-Update methods (of which FedAvg is a special case) can provide
faster convergence, but converge to a surrogate minimiser that may differ from the true minimizer
that would be attained by training on clients’ pooled data. The authors prove that reducing the client
learning rate during training is necessary to reach the true minimiser.
Previous works show that decaying the client learning rate γ allows FedSGD to optimise the true
objective. In this paper, we propose instead decaying the number of local steps K that clients
perform between aggregation rounds of FedSGD. Decaying K also allows FedSGD to reach the
true minimiser, but with the benefit of decreased total computation on clients. Our contributions are:
(1) convergence analysis of FedSGD with decaying K for quadratic objectives; (2) demonstration
of this convergence on a simple quadratic problem; (3) thorough experimentation that investigate
the impact of decaying γ or K on the generalisation performance of FedSGD using 3 benchmark
FL datasets. The experiments show that decaying K can reach the same generalisation ability as
FedSGD with decaying γ, whilst performing up to 3.8× less total SGD steps.
2	FedSGD with Decaying Number of Local SGD Steps
2.1	The FedSGD Algorithm
Reddi et al. (2020) first generalised FedAvg (McMahan et al., 2017) to FedSGD by treating the
updates sent from workers as a psuedo-gradient. This psuedo-gradient then is used to update the
aggregate model in an SGD-like step. FedSGD allows tuning of the server learning rate η, which
(Reddi et al., 2020) show can improve convergence over standard FedAvg.
Algorithm 1 FedSGD: ServerUpdate		Algorithm 2 FedSGD: ClientUpdate	
1	: Initialise global model, x1	1	function ClientUpdatei(x, γ, K)
2	: while termination criteria not met do	2	
3	:	Select round clients, It	3	x1 J x for k = 1..K do
4	:	for each client i ∈ It in parallel do	4	sample set Sk from Di of size B
5	qi — ClientUpdatei (x, γ, K)	5	gk J (1/B) Pz∈Sk Vf (xk； Z)
6	:	end for	6	xk+1 J xk - γgk
7	qt J Pi∈I1 |Di|) Pi∈It |Di|qi	7	end for
8	t :	xt+1 J xt - ηtqt	8	return x - xk
9	: end while		
Algorithms 1 and 2 outline the operation of FedSGD. The server initialises a global model, x1
(Algorithm 1, line 1), and begins the iterative training phase. Each round, a subset of clients It are
selected for participation (Algorithm 1, line 3). Each client downloads the global model and updates
it locally by performing SGD with client learning-rate γ on their local dataset (Algorithm 2, lines
4-6). The difference between the clients’ local models and the round’s global model are sent to the
server (Algorithm 2, line 8). The sever averages these updates, weighted by the number of samples
on each client (Algorithm 1, line 7), and uses this average update as the psuedo-gradient in an SGD-
2
Under review as a conference paper at ICLR 2021
like step to update the global model with server learning rate η (Algorithm 1, line 8). Another round
then starts with the new global model.
2.2	Convergence Analysis of FedSGD with decaying K
Charles & Konecny (2020) generalised FedSGD further into LocalUpdate, which also encompasses
similar distributed-training algorithms such as MAML (Finn et al., 2017) and distributed SGD. They
showed for quadratic objectives that when K > 1 and γ > 0, the loss function minimised by
LocalUpdate differs from the loss function that would have been minimised by centralised training
or by distributed SGD. The authors showed that by decaying γ during training, the original loss
function can be recovered. We conduct an analysis to show that decaying K also allows the original
loss to be recovered, which gives the extra benefit of less total computation needed during training.
We aim to train a model x ∈ Rd. We consider a set of clients I, with probability distribution P .
Each client i ∈ I possesses a distribution of data Di on an example space Z . We assume each
z ∈ Z can be characterised by symmetric matrix AZ ∈ Rd×d and vector cZ ∈ Rd. The loss for a
given model x and sample z, f(x; z), the expected loss on client i, fi(x), and expected loss over
clients f(x) are given by:
f(x；Z)=2 kA1∕2(X-Cz )k2
fi(x) = E [f(x; z)]
Z〜Di
f(x) = E [fi(x)].
i〜P
(1)
(2)
(3)
In previous works such as McMahan et al. (2017), the authors aim to minimise the objective f (x).
However, heterogeneous client datasets lead the actual objective minimised by FedSGD to differ
from f (x). Charles & Konecny (2020) show that when K > 1 and γ > 0, the objective minimised
by FedSGD, f(x, γ, K), is actually the expectation over client surrogate loss functions fi(x, γ, K):
fi(x,Y,K) = 2k(Qi(γ,K)Ai)1/2(x - Ci)k	(4)
~ , ____ - - ~, ____________
f(x,Y,K) = E [fi(x,Y,K)].	(5)
i〜P
The distortion matrix Qi amplifies the heterogeneity of client data. Charles & Konecny (2020) go
on to demonstrate that when the client learning rate 0 < γ < Li-1 (where Li-1 is the Lipschitz
constant of Ai), the eigenvalues of matrix Qi(γ, K)Ai are given by:
φλ(γ, K) = γ-1(1 - (1 - γλ)K),	(6)
where λ is an eigenvalue of Ai. This allows Charles & Konecny (2020) to bound the distance
between the minimiser of the surrogate loss function x* (γ, K) and the true minimiser x* as:
kx*(γ, K) - x*k ≤ σc(1 + σA) LK -	,	⑺
∖ μ J	φμ(Y,K)
where Var[ci] ≤ σ∣, Var[Ai] ≤ σ^, and with the assumption that μI W Ai W LI, ∀i ∈ I. By
i 〜P	C i〜P
inspecting Equation 7, it can be seen that as Y → 0, then ∣∣x* (γ, K) 一 x*k → 0. Charles & Konecny
(2020) later use this to show that by decaying γ during training, the global loss function minimised
by FedSGD approaches the true loss function. However, inspection of Equation 7 also shows that
as K → 1, this distance also goes to 0. We now use this fact to show that decaying K during
training also allows FedSGD to minimise the true loss, with the added benefit of deceasing client
computation per round during training.
Our goal is to derive a bound on the distance between iterations of FedSGD (where K is decreasing
between iterations), and the true minimiser x*. We first need to bound the distance between surrogate
minimisers with different values of K. For this analysis, we assume a continuous K. In reality K
is an integer, but continuous K can be approximated by rounding Kt . Derivations of the following
Theorems and Corollaries are given in Appendix A.
Theorem 1	Let 0 < γ < L-1 and 1 ≤ K1 ≤ K2, then
∣x*(γ,K1) - x* (γ, K2)∣ ≤ σc
(1 + Φl(y, K2) ) φ φL(Y, KI) - KK2 φL(Y K2)
I	φμ(Y,K2)√ ∖	φμ(Y,K1)
(8)
3
Under review as a conference paper at ICLR 2021
The above theorem bounds the distance between the surrogate minimisers for two values of K,
however, it can be simplified to make the dependence on K1 and K2 more explicit.
Corollary 1 Let 0 < γ < L-1 and 1 ≤ K1 ≤ K2, then
kx*(γ,K1)-x*(γ,K2)k ≤ 2σcL (KK-KI)	⑼
L2
≤ 2σcμ(K2 - Ki).	(IO)
Although Corollary 1 provides a looser bound than Theorem 1, the dependence on the distance
(K2 - K1 ) is clearer. Equation 10 comes from the fact that K2 ≥ 1. Both Equation 9 and the
looser bound in Equation 10 are used within Theorem 2 to show that with appropriately chosen rate
of K-decay, successive surrogate minimisers will be close enough such that the distance does not
increase faster than FedSGD can converge given the choice of η.
To bound the distance between iterations of FedSGD, xt, and the true minimiser, x*, appropriate
schedules of ηt, γ, and Kt must be chosen. Here we decay the ηt with (1/t) as per Charles &
Konecny (2020), but with fixed γ. Kt is decayed with (1/t). The motivation behind the inclusion of
b in the definition ofKt is not immediately obvious, but is required to ensure the surrogate minimiser
does not change too fast for FedSGD to converge. If the condition number K = L∕μ is large enough
to impact the choice of K0, the user could simply increase K0 to sufficiently counteract κ (albeit at
the cost of a lower γ).
Theorem 2	Let
_ at _	3 一 2	∕n(2) - ʃ Ko 11
ηt = b+t,at = ^φ^K),b = 3κ,γ ≤	= maxf b+,1 卜
(11)
then
E[kxt-x*k2] ≤ 2V + ：7丁发，
where
36G2
V = max 1〃2 KOMB，(b +1)kx1 - xιk2J .	(13)
G is a bound on client-update variance given in Charles & Konecny (2020), M is the number of
clients participating in each round and B is the batch size used. This theorem shows that with
FedSGD, the distance between the model at each step and the true minimiser can be bounded when
η and K are decayed. As per Equation 11, when using constant γ, γ must be set sufficiently small
to ensure convergence with the largest value of K, namely K0. The 16σc2κ2K02 term shows that
choosing a larger value of K0 increases the maximum distance to the true minimiser, and this will
most likely dominate the 1/K0 term present in the definition of ν in most cases. However, as
demonstrated later for real-world datasets, choice of larger K0 leads generally to faster convergence
(at least in the initial stages of training).
3	Illustrative Example
As discussed above, the objective function minimised by FedSGD is not necessarily the same as
the function that would be minimised by centralised training on pooled data. In order to intuitively
demonstrate the difference, we perform FedSGD on a simple one-dimensional quadratic problem
using varying K, the decaying Y proposed by Charles & Konecny (2020), and the decaying K
scheme proposed here.
We simulate clients with a quadratic loss function given by f (x; Z) = ɪzx2 - x. Each client is
given an identity i ∈ [1， 3], and possesses one data point zi = i. Therefore, the minimiser for
client i, x* is given by 1/z = 1/i. The (un-normalised) probability of each client being selected
each round is given by: p(i) = 1∕√i. The true loss function (as per Equation 3) is therefore the
expectation over client loss functions, f (x) = E[fi(x)] = R3(2zx2 - X)Z-1 dz. f (x) is minimised
by x* ≈ 0.523. Figure 1 shows the models produced by FedSGD with different γ and K schedules.
For all simulations we use server learning rate η = 0.001, M = 10 clients per round, batch size
4
Under review as a conference paper at ICLR 2021
B = 1, and initial client learning rate γ = 0.1. We start with an arbitrarily-chosen initial model
value of x1 = 0.4 and average each curve over 10 runs. We also vary the γ-decay rate, dγ, where
γt = γ(dγ)t, and K -decay rate, dK, where Kt = dK0(dK)te.
Figure 1: Convergence of FedSGD to true minimiser xψ on a simulated quadratic dataset. (a) Varying K with
constant γ (dγ = 1.0), and decaying γ (dγ = 0.995). (b) Varying dK with constant γ.
Figure 1 (a) shows that with constant Y (dγ = 1), ∣∣x*(γ,K) - x*k increases with K (red, blue,
purple curves), as per Equation 7. Larger values of K converge faster because clients perform more
steps of SGD between rounds. Decaying γ during training (grey curve) when K > 1, however, gives
both fast initial convergence and convergence to x*. As can be seen at round 500, the grey curve
overshoots x* before converging. This illustrates the need for correctly tuning dγ: overly-aggressive
decay will converge slowly due to low learning rates, but weak decay will cause FedSGD to begin
to converge to a surrogate minimiser before x* , also slowing down training.
A similar effect is demonstrated in Figure 1 (b). Here all schemes use constant γ (dγ = 1), and
K0 = 10. We vary dK to illustrate similar training dynamics as occur when varying dγ , as discussed
above. With aggressive decay (dK = 0.99, red curve), K quickly decays to 1, so FedSGD does
not converge much faster than distributed SGD. This is in contrast to the very slow decay case
(dK = 0.9995, grey curve), which shows very rapid initial convergence, but ultimately takes longer
to reach x* because kx*(γ, K) - x* || was not reduced rapidly enough. A decay rate ofdK = 0.995
is a good choice here due to fastest convergence to x* , and perform 84% less total SGD steps
compared to clients with constant K = 10 over 3000 rounds.
This simple 1-dimensional quadratic example serves to demonstrate the trade-off between faster
convergence with larger K, and convergence to x* . The benefit of using large K0 and decaying
K during training is made apparent. We later perform FedSGD with fixed γ, annealing γ and
annealing K on real-world datasets for non-quadratic models, to examine the impact of this theory
in realistic scenarios and the implications for generalisation, which has not been studied before from
this perspective.
4	Experimental Evaluation
4.1	Setup
We experimentally test FedSGD with a constant Y, decaying Y as proposed by Charles & Konecny
(2020), and with decaying K as proposed in this paper, on three benchmark FL datasets: StackOver-
flow (Tensorflow-Federated, 2020), FEMNIST (Caldas et al., 2018), and Shakespeare (Caldas et al.,
2018). The above analysis is for minimisation of quadratic objectives by FedSGD, so does not nec-
essarily apply to non-quadratic objectives and generalisation. We therefore empirically study these
characteristics for the first time. Details of the datasets and models are given below, with further
details given in Appendix B.
5
Under review as a conference paper at ICLR 2021
StackOverlow - contains posts and their associated tags from stackoverflow.com, grouped by the
writer of the post. We limit this dataset to 1000 training clients and 10,000 testing samples, with a
maximum of 500 samples per client. As in Reddi et al. (2020) and Charles & Konecny (2020), We
construct a normalised bag-of-words vector for presence of the 10,000 most-used tokens for each
post, and a binary vector for presence of the 500 most-used tags. We train a logistic regression model
With sigmoid outputs and Binary Cross-Entropy loss. Reddi et al. (2020) shoWed that for this dataset,
replacing the SGD-like step of FedSGD (line 8 of Algorithm 1) With an adaptive optimiser such as
Adam (FedAdam) significantly increases the rate of convergence. We therefore used FedAdam
instead of FedSGD for this dataset, to alloW the model to converge Within 4000 rounds.
FEMNIST - is an adaptation of the EMNIST dataset. Each sample is a 32 × 32 greyscale image
of one of 62 classes of loWercase letters, uppercase letters, and digits. Samples are grouped by the
Writer of the characters. We train a Convolutional Neural NetWork (CNN) on this dataset consisting
of convolutional, max-pooling, and fully-connected layers, using Categorical Cross-Entropy loss.
1000 training clients Were chosen at random, With 20% of each user’s samples held out for testing.
Shakespeare - the complete plays of William Shakespeare. The plays are parsed such that each role
in each play becomes a client, and We take the 1000 clients With most samples, reserving the last 20%
of each client’s samples for testing. This text data is then used for a next-character prediction task.
A Gated Recurrent Unit (GRU) netWork Was trained on the dataset using Categorical Cross-Entropy
loss.
For each dataset, three values of K Were selected such that the model’s test-set accuracy (Recall for
StackOverfloW) could converge Within 4000 rounds. For each value of K, η0, dη and γ Were tested
in a grid search, and the combination of settings that gave the highest test accuracy over the last 500
rounds are presented (With accuracy measured every 40 rounds).
For the three value of K found as above, experiments With decaying-γ and decaying-K Were then
performed. For the decaying-γ experiments, η0, dη and γ0, dγ Were varied in a grid search, using
constant K. For the decaying-K experiments, η0, dη, γ, and dK Were searched. As above, the best
combination of settings to give the highest test-set performance during the 4000 rounds Were plotted
for each K .
Figure 2: Training error and Top-5 Recall of FedAdam on the StackOverflow dataset. (a) ConStant-Y/K
scheme, (b) decaying-γ scheme, (c) decaying-K scheme.
4.2	Results
Figures 2-4 show the training error and test-set performance for the constant-y/K, decaying-γ and
decaying-K schemes, for three datasets. Each curve is an average over 5 trials with different random
seeds. Error curves are smoothed over a window of size 200
Figure 2 (a) shows that for the StackOverflow dataset, FedAdam with constant-y/K can converge
both faster and to a higher test-set Recall with higher K. However, Figures 3 (a) and 4(a) have
6
Under review as a conference paper at ICLR 2021
different dynamics: higher K converges faster, but final test accuracy is harmed. For all three
datasets, the scenarios with higher K were able to reach lower training error within 4000 rounds,
despite using mostly the same γ (hyperparameters for all settings are given in Table 5 of Appendix
B). These results appear to be in contradiction to Equation 7, which states that with increasing K ,
the distance to the true minimiser increases for quadratic objectives. Equation 7 would imply that by
increasing K, the minimum achievable training error would increase. However, when a decaying the
server learning rate, η, is used, higher minimum error does not appear to be a problem (as opposed
to the experiments in Charles & Konecny (2020), where fixed η and Y were used).
Figures 3 (b) and 4 (b) all show that by decaying γ during training, the test-set performance for
FEMNIST and Shakespeare is either the same or is improved compared to constant-γ/K. Table
1 gives the best test-set performance for each scheme and each setting. The proof earlier in the
paper for quadaratic objective concerns minimisation of the loss function, and does not consider
generalisation ability. It appears, however, that by decaying γ during training, this generalisation
performance can be improved. This is likely due to the fact that clients can perform more fine-tuned
adjustments on the model between aggregations steps, and the ‘noise’ included with the global
model by performing multiple steps of SGD between aggregations (represented by distortion-matrix
Qi in the above analysis) is reduced. For StackOverflow, however, decaying γ suring training could
not improve over constant γ . Here, it appears that improvement can only be made by decaying K.
The decaying-K scheme shown in Figures 3 (c) and 4 (c) give very similar improvements to test-
set performance as the decaying-γ scheme. By decaying K during training, the maximum test-
set performance that the global models achieved during training was either the same or improved
compared to constant-γ/K. However, with this scheme, clients performed significantly less total
SGD steps and thus computation over the 4000 training rounds compared to the other two schemes.
Table 1 presents the total amount of SGD steps saved by the decaying-K schemes over the 4000
training rounds for the different scenarios. For the StackOverflow dataset in Figure 2 (c), significant
improvement in terms of test-set Recall were made over constant-y/K or decaying-γ. The unusual
training dynamics of this dataset compared to FEMNIST and Shakespeare may be due to the use of
FedAdam and/or the convex model. Future work may benefit from comparing the training dynamics
of different typos of model and Federated optimiser.
These results imply that the convergence-speed benefit gained by performing multiple steps of SGD
between aggregations comes at the cost of lower test-set performance. Intuitively, by performing
multiple steps of SGD between aggregations in FL, the performance of the global model is slightly
degraded as the heterogeneity in client data distributions is amplified, as demonstrated in the above
analysis. However, towards the end of training, more ‘fine-tuned’ adjustments need to be made to
the model to improve performance. Instead of decaying γ to try to mitigate this performance degra-
dation, these results show that decaying-K to turn FedSGD (and its variants) into simple distributed-
SGD can achieve the same objective.
Figure 3: Training error and Top-1 Accuracy of FedSGD on the FEMNIST dataset. (a) COnStant-Y/K scheme,
(b) decaying-γ scheme, (c) decaying-K scheme.
7
Under review as a conference paper at ICLR 2021
Figure 4: Training error and Top-1 Accuracy of FedSGD on the ShakesPeare dataset. (a) Constant-γ∕K scheme,
(b) decaying-γ scheme, (c) decaying-K scheme.
Table 1: Lowest training error and highest test-set accuracy∕recall achieved during training for the decaying-γ
and decaying-K schemes for different training scenarios, and the total number of SGD steps saved by decaying-
K over 4000 rounds. Best overall test-set performance(s) for each dataset given in bold.
StackOverflow	FEMNIST	Shakespeare
	K	10	33	100	3	10	33	10	33	100
Decaying γ	Min Train Error	0.02	0.02	0.02	0.22	0.11	0.05	1.33	1.12	0.87
	Max Test Acc	0.37	0.40	0.44	0.91	0.92	0.91	0.55	0.57	0.57
Decaying K	Min Test Error	0.02	0.02	0.02	0.23	0.16	0.14	1.44	1.23	0.84
	Max Test Acc	0.41	0.46	0.42	0.91	0.92	0.92	0.54	0.56	0.58
Fraction SGD Steps Used		0.55	0.51	0.50	0.65	0.55	0.26	0.55	0.51	0.50
The results in Table 1 show that the test-set performance achieved by decaying-γ can be matched
or even surpassed by decaying-K , whilst also performing far less computation during training. This
saved computation would represent a significant energy and time saving for a real-word FL deploy-
ment.
5 Conclusion
In this work, we proposed decaying the number of local SGD steps, K, performed in the Clien-
tUpdate phase of the popular FedSGD algorithm during Federated Learning (FL). This leads the
FedSGD algorithm to become distributed SGD as K → 1. We proved that for quadratic objec-
tives, decaying K allows FedSGD to minimise the loss function that would have been minimised
during centralised training on pooled client data, i.e. the true loss. We demonstrated on a simple
quadratic example the importance of tuning the decay rate of K , in order to achieve both the fast
initial convergence provided by FedSGD, but also minimisation of the true loss. Previous works
suggest decaying the client learning rate, γ, during training in order to minimise the true loss. How-
ever, thorough experimentation of three benchmark FL datasets indicates that decaying K provides
very similar benefit in terms of generalisation performance as decaying γ, with the significant added
benefit of decreasing the total number of SGD steps that clients must perform during FedSGD. In
our experiments, out decaying-K scheme was able match or surpass the test-set performance of
the decaying-γ scheme in all scenarios, with up to 3.8× less steps of SGD performed by clients.
In real-world FL deployments, this would represent significant energy and time savings for the FL
process.
8
Under review as a conference paper at ICLR 2021
References
Muhammad Ammad-ud-din, Elena Ivannikova, Suleiman A. Khan, Were Oyomno, Qiang Fu,
Kuan Eeik Tan, and Adrian Flanagan. Federated collaborative filtering for privacy-preserving
personalized recommendation system. arXiv preprint arXiv:1901.09888, 2019.
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konecny, H. Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097,
2018.
Zachary Charles and Jakub Konecny. On the outsized importance of learning rates in local update
methods. arXiv e-prints arXiv:2007.00878, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
volume 70, pp. 1126-1135, 2017.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, FranCoiSe Beaufays, Sean
Augenstein, Hubert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv e-prints arXiv:1811.03604, 2018.
Li Huang, Andrew L. Shea, Huining Qian, Aditya Masurkar, Hao Deng, and Dianbo Liu. Patient
clustering improves efficiency of federated machine learning to predict mortality and hospital stay
time using distributed electronic medical records. Journal of Biomedical Informatics, 99:103291,
2019. ISSN 1532-0464.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, et al. Advances and open problems in feder-
ated learning. arXiv e-prints arXiv:1912.04977, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv e-prints
arXiv:1412.6980, 2014.
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. Federated
learning for keyword spotting. In IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 6341-6345, 2019.
Qinbin Li, Zeyi Wen, and Bingsheng He. Pratical federated gradient boosting decision trees. In 34th
AAAI Conference on Artificial Intelligence, volume 34, pp. 4642-4649, 2020a.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, may 2020b. doi:
10.1109/MSP.2020.2975749.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2020c.
Sin Kit Lo, Qinghua Lu, Chen Wang, Hye-Young Paik, and Liming Zhu. A systematic literature
review of federated machine learning: From a software engineering perspective. Journal of the
Association for Computing Machinery, 37(4), Aug 2020. doi: 10.1145/1122445.1122456.
Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data. 20th International Conference on
Artifical Intelligence and Statistics, 2017.
Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and FranCOiSe Beaufays. Federated learning
for emoji prediction in a mobile keyboard. arXiv e-prints arXiv:1906.04329, 2019.
S. Reddi, Zachary Charles, M. Zaheer, Zachary A. Garrett, Keith Rush, JakUb Konecny, S. Kumar,
and H. McMahan. Adaptive federated optimization. arXiv e-prints arXiv:2003.00295, 2020.
Micah Sheller, Brandon Edwards, G. Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrotsou,
Mikhail Milchenko, Weilin Xu, Daniel Marcus, Rivka Colen, and Spyridon Bakas. Federated
learning in medicine: facilitating multi-institutional collaborations without sharing patient data.
Scientific Reports, 10, 12 2020. doi: 10.1038/s41598-020-69250-1.
9
Under review as a conference paper at ICLR 2021
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task
learning. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, NIPS'17,pp. 4427-4437, 2017. ISBN 9781510860964.
Haifeng Sun, Shiqi Li, F. Richard Yu, Qi Qi, Jingyu Wang, and Jianxin Liao. Towards
communication-efficient federated learning in the internet of things with edge computing. IEEE
Internet of Things Journal, 2020.
Tensorflow-Federated. Tensorflow federated stackoverflow dataset. 2020. URL https:
//www.tensorflow.org/federated/api_docs/python/tff/simulation/
datasets/stackoverflow/load_data.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning Represen-
tations, 2020.
Kangkang Wang, Rajiv Mathews, Chloe Kiddon, HUbert Eichner, Francoise Beaufays, and Daniel
Ramage. Federated evaluation of on-device personalization. arXiv e-prints arXiv:1910.10252,
2019.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? arXiv
e-prints arXiv:2002.07839, 2020.
10
Under review as a conference paper at ICLR 2021
Appendix A	Proof of Theorems
A.1 Theorem 1
Theorem 1 bounds the distance between surrogate minimisers that use different values of K. Our
proof extends the proof of (Charles & Konecny, 2020): the Theorems, Corollaries and Lemmas UP
to Theorem 21 are used in this proof, with modification to/reparametrisations of some formulas.
We encoUrage readers to follow the previoUs proofs to more thoroUghly Understand the proof pre-
sented here. The major reparamterisation is to Lemma 10, which gives the eigenvalUes of matrix
Qi(γ, K)Ai as a fUnction of λ (any eigenvalUe of Ai), γ and K:
φλ,γ(K) = γ-1(1 - (1 - γλ)K).	(14)
This is becaUse we Use constant γ and varying K dUring training, rather than constant K with
varying γ .
A.1.1 Lemma 1
If Y ≤ LT (assuming μI W Ai W LI, ∀i ∈ I), 1 ≤ Ki ≤ K2, then:
C=K1-1Qi(γ,K1)Ai-K2-1Qi(γ,K2)Ai	(15)
is positive semidefinite and satisfies
λmax(C) ≤ K1-1φL,γ(K1) - K2-1φL,γ(K2).	(16)
Proof Since Qi(γ, K1)Ai and Qi(γ, K2)Ai share the same eigenvectors as Ai, and the eigenvalues
of a matrix M multiplied by scalar s are equal to the eigenvalues of sM, using Equation 14, the
eigenvalues of C are of the form
h(λ) =K1-1γ-1(1-(1-γλ)K1)-K2-1γ-1(1-(1-γλ)K2).	(17)
Since Y ≤ L-1, inspection of h(λ) shows that h(λ) ≥ 0 for λ ∈ [μi, Li] and 1 ≤ Ki ≤ K2. This
implies C is positive semidefinite. Computing h0(λ) gives:
h0(λ) = (1 - λY)K1-i - (1 - Yλ)K2-i .	(18)
As 0 < (1 - Yλ) < 1 and Ki ≤ K2 then h0(λ) ≥ 0. The maximum eigenvalue of C therefore
satisfies
λmax(C) ≤ h(λ) =Ki-iφL,γ(Ki)-K2-iφL,γ(K2).	(19)
A.1.2 Proof of Theorem 1
Define the following terms for notational convenience:
Bi,j = Kj-iQi(Y, Kj)Ai
Bj = E[Bi,j]
i
vj = E[Bi ici] - E[Bi 2]E[ci]
i	ii
φL,K = φL,γ (K)
φμ,K = φμ,Y (K)
As per Lemma 30 of (Charles & Konecny, 2020):
∣∣x*(γ,K1) - x*(γ,K2)∣∣ ≤∣∣B-1k∣∣v1-v2∣∣ + ∣∣B-i- B-Ikkv2k.	(20)
'{z'、-----{----} `-----{-----}l{z}
T1	T2	T3	T4
To bound this term, we derive bounds on Ti, T2, T3, T4. Bounding Ti using Theorem 16, Lemma
31, and Lemma 11 of (Charles & Konecny, 2020):
Ti = kBi-ik	(21)
≤ E[kBi-,iik]	(22)
= E[k(Ki-iQi(Y,Ki)Ai)-ik]	(23)
≤ KiΦ-Kι.	(24)
11
Under review as a conference paper at ICLR 2021
Bounding T2 using the definition of Bi,j and Lemma 1:
T2 = kv1 - v2 k	(25)
= ∣∣E[(Bi,ι- Bi,2)(ci-c)]∣∣	(26)
≤ ,E[kBi,ι- Bi,2k2]E[kci-ck2]	(27)
≤ bc，E[kBi,i- Bi,2k2]	(28)
=σcqE[kK-lQi(Y-κlH-K-lQ^(Y^K2)Al	(29)
≤ σc (K-1Φl,k1 - K-1Φl,K2 ) .	(30)
Using the fact that for invertible matrices X, Y ∈ Rn×n :
kX-1-Y-1k=kX-1(X-Y)Y-1k≤kX-1kkY-1kkX-Yk.	(31)
Applying this to T3, bounding kB1-1k and kB2-1 k as per T1, and bounding kB1 -B2 k using Lemma
1:
T3 ≤ kB1-1kkB2-1kkB1-B2k	(32)
≤(KiΦ-,K1) (K2φ-,K2) (K-1ΦL,K1 - K-1Φl,K2) .	(33)
T4 can be bounded using the definition of σc and φL,γ (K):
T4 = kE[Bi,2ci] -E[Bi,2]E[ci]k	(34)
i	ii
≤ σcqE[kBi,2k]2	(35)
=σc,E[kK-1Qi(γ, Θικ)Aik2]	(36)
≤ σcK2-1φL,K2 .	(37)
Combining the terms in Equation 20:
kx*(γ,K1)- x*(γ,K2)k ≤ T1T2 + T3T4
(38)
(39)
(40)
σ φ φL,K1 - K1 φL,K2 + φL,K1 φL,K2 - K2 φL,K2
C 卜	φμ,K1	φμ,K1 φμ,K2
(1+ ΦL,γ (K2) A φ φL,Y (KI) - K2 φL,γ (K2) !
σc ∖ φμ,Y(K2”[	φμ,Y (KI)	J
(41)
(42)
A.2 Corollary 1
The (φL,γ(Ki) - K1 Φl,y(K2)) term from Equation 42 first needs to be bounded. Using the defini-
tion of φL,γ (K):
K	K1	K K2
φL,γ(Ki)-六φL,γ(K2) = E(I-YL)k-1 L - -1 £ (1 - γL)k-1 L (43)
K2	K2
2	k=1	2 k=1
K1	K1
≤ X (1 - γL)k-i L - K X (1 - γL)k-i L (44)
=(1 - K1) φL,Y(K1).	(45)
12
Under review as a conference paper at ICLR 2021
Substituting this and Lemma 35 from (Charles & Konecny, 2020) (φL,γ(K) ≤ L^ into Theorem 1,
and using the fact that L∕μ ≥ 1 and K ≥ 1:
kx*(γ, θ1κ)-χ*(γ, O1：K2 )k≤ σc (1 + ^K) ≤ σc (1+μ)((1- ≤ 2σcI2 (中) L2 ≤ 2σc-2 (K2 - KI). μ2	φ φL,γ (KI)- K" φL,γ (K2)λ (一Φ∑γw—(	(46) K) φL,Y(KI))	(47) K2) φμ,γ(Ki)7	( ) (48) (49)
For the later Theorem 2, both Equation 48 and Equation 49 are used to bound the distance between
minimisers with different values of K.
A.3 Theorem 2
Defining the following for notational convenience:
μt = φμ,γ (Kt)
Lt = φL,γ (Kt)
~ , ~ , _________、
ft(x) = f(χ,γ,Kt)
*
x； = arg min ft(x)
K = L∕μ
The distance from any xt and x* can be bounded as:
E[kxt - x* k2] ≤ 2E[kxt-xt*k2] + 2kxt* -x*k2.	(50)
Lemma 25 of (Charles & Konecny, 2020) bounds the distance between xt+i and x*. The authors
then use this to form a recursion relation for consecutive steps of FedSGD, for any ωt > 0:
E[kχt+1 -χ;k2] ≤ (1 -ηtμt)E[kχt - Wk] + η2KG,	(51)
MB
αt+ι ≤ (I + ω/	((I	- ηtμt) αt +	η2	MB	) + (1+ ω-1)	kx*	-	x*+ιk2.	(52)
Let C = 4σ∣κ2 and let t = b +1. We create the following inductive hypothesis:
ν+C
at ≤ -^—
The term for t = 1 is given as:
Il *∣∣2	(b + I)IIxI- x*k2
αι = kχι - XIk =--------b+----------
ν1
(53)
(54)
≤ E.
Which therefore means we can come to the same conclusions, except using a Kt term instead of K:
/	、	2KtG2	G_2、/ 小 C 1 "KtG2	'
(1-ηtμt) at+ηt -MB ≤ (T)(V+C)- ^2 + ^2 (kμ2MB -V
(55)
'----------------
ξt
}
Lemma 18 of (Charles & Konecny, 2020) allows ξt to be simply bounded. The choice of a constant
γ and decaying K means the tightest constraint on γ must be used for any value of K to ensure
convergence, i.e. Y ≤ ln(2)∕K0μ. Substituting this into the definition of ξt, using the fact that
Kt∕K0 ≤ 1, and using the definition of V:
ξt ≤
9KtG2
36G2	/ 0
μ2K0MB ≤ ,
(56)
13
Under review as a conference paper at ICLR 2021
therefore:
2 KtG2	ν + C	C
(1 + ωt) V1 -ηtμt)αt+ ηtMB-) ≤ 7+Γ - (t-2)(^+i).
(57)
This bounds the first part of Equation 52. Using Corollary 1 (Equation 49), and the fact that κ2 <
, ʌ .
b < t+ 1:
E-W+ιk2 ≤ 4σ2L4 (KK0 - ^+0ι)
_ CK
=^ G+1)2
C
≤ ^2 (^+ι).
Then:	C
(1 + ω- 1) ||x； - xJ+ι∣F ≤ ~~p-、 「-r.
' t t t+1" - (t + 2) (t+1)
Combining Equation 52, Equation 57, Equation 61 proves the inductive hypothesis:
「川	*1 ⑵	V + C	C	C	V + C
E[kxt+1 - xt k ] = αt+1 ≤ 7+Γ - (^- 2) (^+ι) + (^+2) (^+ι) ≤ UT.
(58)
(59)
(60)
(61)
(62)
The second term of Equation 50 can be bounded using Corollary 1 (Equation 48), using K1 K2 = Kt, and the same K <b <t+1 trick:	1 and
L 4 K K0 - -K0 ∖ 2 kx*-x*+ιk2 ≤ 4σCL4(+ ) 4σc2K4 =¥+^ 4σ2κ2 ʌ	. —t+1 Combining equation 50, equation 62 and Equation 65 gives the final Theorem 2: Er∣∣	*∣∣2] V 2(V + C) ,2f 4σC κ2∖ E[kxt - X k ] ≤ t + 1	+ 2 ∖7+Γ) 2ν + 8σ2κ2 (K2 + 1) ≤	b + t 2ν + 16σ2κ2K2 ≤	b + t	.	(63) (64) (65) (66) (67) (68)
14
Under review as a conference paper at ICLR 2021
Appendix B	Experimental Details
Details of the datasets, model architectures and hyperparameters used for Figures 2-4 and Table I
are given here. All models were implemented in Tensorflow.
StackOverflow - posts from the website stackoverflow.com. Each post has an associated list of
tags, which is used as the target for each sample. A post about web development may have the
tags: {‘html’, ‘css’, ‘javascript’}, for example. The data from (Tensorflow-Federated, 2020) is
preprocessed including lowercasing text, separating every token with spaces, removing non-ascii
symbols etc. Similar to (Reddi et al., 2020) and (Charles & Konecny, 2020), We take the 10,000
most common tokens and the 500 most common tags, and construct a (normalised to 1) bag-of-words
vector and a binary tag-vector for each sample. Due to very large variance in number of samples per
user and total number of users available in the dataset, We limit to a maximum of 250 samples per
client, 1000 training clients (for a total of 331027 training samples) and 10000 testing samples. The
model trained is a 1-vs-rest logistic regression model With 10,000 inputs and 500 outputs, trained
With Binary Cross-Entropy. As previously shoWn in (Reddi et al., 2020), convergence for this dataset
using this model is sloW, and did not converge Within 4000 rounds. (Reddi et al., 2020) shoWed that
replacing the update rule of ServerUpdate (Algorithm 1, line 8) With an adaptive optimiser such
as Adam (FedAdam) gave much faster convergence than FedSGD for this dataset. The psuedo-
code for FedAdam as We used in the StackOverfloW experiments is given beloW. The performance
metric used for this dataset is Recall using the Top-5 predictions of the model as implemented in
tensorfloW.keras.metrics.Recall. For all experiments We fix β1 = 0.9, β2 = 0.999 and = 10-8 as
per the original Adam paper (Kingma & Ba, 2014).
Algorithm 3 FedAdam: ServerUpdate
1:	Initialise global model, x1
2:	Initialise 1st and 2nd moments, mi J 0, vι J 0
3:	while termination criteria not met do
4:	Select round clients, It
5:	for each client i ∈ It in parallel do
6:	qti J ClientUpdatei (x, γ, K)
7:	end for
8:	qt J (1/M) Pi∈It qti
9:	mt J β1 mt-1 + (1 - β1)qt
10:	Vt J β2vt-i + (1 — β2)(qt)2
1	1	J	√1-(β2)t
11:	ηt J α 1-(βι)t
12:	xt+1 J Xt - η √m+e
13:	end while
FEMNIST - the EMNIST dataset Where the samples are grouped by the person Who Wrote the
characters. Samples are a 32 × 32 greyscale image of the characters a-zA-Z0-9. 2000 users from the
dataset Were used for training, With 20% of each user’s samples held out for testing. Preprocessing
and grouping Was performed using the utilities provided by (Caldas et al., 2018). The Convolutional
(Conv) Neural NetWork model trained on the dataset had the folloWing architecture. Model Was
trained using Categorical Cross-Entropy loss.
Table 2: Architecture of FEMNIST CNN Model
Layer	Details	Activation
Conv	3 X 3 kernel, stride 1, 32 outputs	ReLU
Max Pooling	2 × 2 pooling, stride 2	-
Conv	3 × 3 kernel, stride 1, 64 outputs	ReLU
Max Pooling	2 × 2 pooling, stride 2	-
Fully Connected	512 outputs	ReLU
Fully Connected	64 outputs	Softmax
Shakespeare - the complete plays of William Shakespeare. Each character in each play becomes
one client. The lines of each client are used as samples for a next-character-prediction task. 1000
15
Under review as a conference paper at ICLR 2021
clients were used for training, with the last 20% of lines for each client used as the testing samples.
Preprocessing and grouping was performed using the utilities provided by (Caldas et al., 2018). The
Gated Recurrent Unit (GRU) model trained on the dataset had the following architecture. Model
was trained using Categorical Cross-Entropy loss.
Table 3:	Architecture of Shakespeare GRU Model
Layer	Details	Activation
Character-embedding	77 → 8 embedding	-
GRU Tanh activation, sigmoid recurrent activation, 256 outputs	-
GRU Tanh activation, sigmoid recurrent activation, 256 outputs	-
Fully-connected	77 outputs	Softmax
Hyperparameters - were tested using a grid-search of the relevant hyperparameters for each of
the Constant-Y/K, decaying-γ, decaying-K schemes. Values tested are given below. Other relevant
hyperparemeters include C = 0.01 fraction of clients sampled per round, batch size B = 32. The
curves presented used the hyperparmeters that gave the highest average test accuracy over the last
500 rounds.
Table 4:	Hyperparameter Grid Search Values
Parameters	no, γo	K°	d^, dγ, dκ
ValUeS {3.0,1.0,0.3,0.1,0.03,0.01}	{100, 33,10, 3}	{0.9996, 0.999, 0.996, 0.99}
The following table gives the actual values chosen for each scenario.
Table 5: Hyperparameters Used for FL Scenarios
Dataset	K	constant-y/K			decaying-γ				no	decaying-K		
		no	dη	γ	no	dη	γo	dγ		dη	γ	dK
	10	0.3	Adam	1.0	0.3	Adam	1.0	0.999	0.3	Adam	1.0	0.9996
StackOverflow	33	0.3	Adam	1.0	0.3	Adam	1.0	0.999	0.3	Adam	1.0	0.9996
	100	0.3	Adam	1.0	0.3	Adam	1.0	0.9996	0.3	Adam	1.0	0.9996
	3	~~Γ	0.9996	0.3	~~Γ~	0.9996	0.3	0.9996	~~Γ~	0.9996	0.3	0.9996
FEMNIST	10	1	0.9996	0.3	1	0.9996	0.3	0.9996	1	0.9996	0.3	0.9996
	100	1	0.9996	0.3	1	0.9996	0.3	0.9996	1	0.9996	0.3	0.999
	10	T.0	0.9996	1.0	"T.0-	0.9996	1.0	0.9996	"T.0-	0.9996	1.0	0.9996
Shakespeare	33	1.0	0.999	1.0	1.0	0.999	1.0	0.999	1.0	0.999	1.0	0.9996
	100	1.0	0.996	1.0	1.0	0.996	1.0	0.996	1.0	0.996	1.0	0.9996
16