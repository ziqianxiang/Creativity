Under review as a conference paper at ICLR 2021
Provably More Efficient Q-Learning in the
One-Sided-Feedback/Full-Feedback Settings
Anonymous authors
Paper under double-blind review
Abstract
Motivated by the episodic version of the classical inventory control problem, we
propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning
(HQL), that enjoys improved efficiency over existing algorithms for a wide variety
of problems in the one-sided-feedback setting. We also provide a simpler variant
of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We estab-
lish that HQL incurs O(H3√T) regret and FQL incurs O(H2√T) regret, where
H is the length of each episode and Tis the total length of the horizon. The regret
bounds are not affected by the possibly huge state and action space. Our numer-
ical experiments demonstrate the superior efficiency of HQL and FQL, and the
potential to combine reinforcement learning with richer feedback models.
1	Introduction
Motivated by the classical operations research (OR) problem-inventory control, We customize Q-
learning to more efficiently solve a wide range of problems with richer feedback than the usual bandit
feedback. Q-learning is a popular reinforcement learning (RL) method that estimates the state-action
value functions without estimating the huge transition matrix in a large MDP (Watkins & Dayan
(1992), Jaakkola et al. (1993)). This paper is concerned with devising Q-learning algorithms that
leverage the natural one-sided-feedback/full-feedback structures in many OR and finance problems.
Motivation The topic of developing efficient RL algorithms catering to special structures is funda-
mental and important, especially for the purpose of adopting RL more widely in real applications. By
contrast, most RL literature considers settings with little feedback, while the study of single-stage
online learning for bandits has a history of considering a plethora of graph-based feedback mod-
els. We are particularly interested in the one-sided-feedback/full-feedback models because of their
prevalence in many famous problems, such as inventory control, online auctions, portfolio man-
agement, etc. In these real applications, RL has typically been outperformed by domain-specific
algorithms or heuristics. We propose algorithms aimed at bridging this divide by incorporating
problem-specific structures into classical reinforcement earning algorithms.
1.1	Prior Work
The most relevant literature to this paper is Jin et al. (2018), who prove the optimality of Q-learning
with Upper-Confidence-Bound bonus and Bernstein-style bonus in tabular MDPs. The recent work
of Dong et al. (2019) improves upon Jin et al. (2018) when an aggregation of the state-action pairs
with known error is given beforehand. Our algorithms substantially improve the regret bounds (see
Table 1) by catering to the full-feedback/one-sided-feedback structures of many problems. Because
our regret bounds are unaffected by the cardinality of the state and action space, our Q-learning
algorithms are able to deal with huge state-action space, and even continuous state space in some
cases (Section 8). Note that both our work and Dong et al. (2019) are designed for a subset of the
general episodic MDP problems. We focus on problems with richer feedback; Dong et al. (2019)
focus on problems with a nice aggregate structure known to the decision-maker.
The one-sided-feedback setting, or some similar notions, have attracted lots of research interests in
many different learning problems outside the scope of episodic MDP settings, for example learning
in auctions with binary feedback, dynamic pricing and binary search (Weed et al. (2016), (Feng
et al. (2018), Cohen et al. (2020), Lobel et al. (2016)). In particular, Zhao & Chen (2019) study the
1
Under review as a conference paper at ICLR 2021
one-sided-feedback setting in the learning problem for bandits, using a similar idea of elimination.
However, the episodic MDP setting for RL presents new challenges. Our results can be applied to
their setting and solve the bandit problem as a special case.
The idea of optimization by elimination has a long history (Even-Dar et al. (2002)). A recent exam-
ple of the idea being used in RL is Lykouris et al. (2019) which solve a very different problem of
robustness to adversarial corruptions. Q-learning has also been studied in settings with continuous
states with adaptive discretization (Sinclair et al. (2019)). In many situations this is more efficient
than the uniform discretization scheme we use, however our algorithms’ regret bounds are unaf-
fected by the action-state space cardinality so the difference is immaterial.
Our special case, the full-feedback setting, shares similarities with the generative model setting in
that both settings allow access to the feedback for any state-action transitions (Sidford et al. (2018)).
However, the generative model is a strong oracle that can query any state-action transitions, while
the full-feedback model can only query for that time step after having chosen an action from the
feasible set based on the current state, while accumulating regret.
Table 1: Regret comparisons for Q-learning algorithms on episodic MDP
Algorithm	Regret	Time	Space
Q-learning+Bernstein bonus Jin et al. (2018)	O(√H3 SAT)	O(T)	O(SAH)
Aggregated Q-learning Dong et al. (2019)	θ(√H4 MT + eT) 1	O(MAT)	O(MT)
Full-Q-learning (FQL)	θ(√H4 T)	O(SAT)	O(SAH)
Elimination-Based Half-Q-learning (HQL)	θ(√H 6 T)	O(SAT)	O(SAH)
2	Preliminaries
We consider an episodic Markov decision process, MDP(S , A,H,P,r), where S is the set of states
with |S| = S, A is the set of actions with |A| = A, H is the constant length of each episode, P is the
unknown transition matrix of distribution over states if some action y is taken at some state x at step
h ∈ [H], and rh : S×A→[0, 1] is the reward function at stage h that depends on the environment
randomness Dh . In each episode, an initial state x1 is picked arbitrarily by an adversary. Then, at
each stage h, the agent observes state xh ∈S, picks an action yh ∈ A, receives a realized reward
rh(xh,yh), and then transitions to the next state xh+1, which is determined by xh, yh, Dh. At the
final stage H, the episode terminates after the agent takes action yH and receives reward rH . Then
next episode begins. Let K denote the number of episodes, and T denote the length of the horizon:
T = H × K, where H is a constant. This is the classic setting of episodic MDP, except that in the
one-sided-feedback setting, we have the environment randomness Dh, that once realized, can help
us determine the reward/transition of any alternative feasible action that “lies on one side” of our
taken action (Section 2.1). The goal is to maximize the total reward accrued in each episode.
A policy π of an agent is a collection of functions {πh : S → A}h∈[H] . We use Vhπ : S→R
to denote the value function at stage h under policy π, so that Vhπ (x) gives the expected sum of
remaining rewards under policy π until the end of the episode, starting from xh = x:
H
Vhr(x) := e[ X rho (xho, ∏%o(xho))∣xh = x].
%，=%
Qhπ : S×A→R denotes the Q-value function at stage h, so that Qhπ (x, y) gives the expected sum
of remaining rewards under policy π until the end of the episode, starting from xh = x, yh = y:
H
Qn(χ,y) := Ebh(χh,y) + X r%，(x%，,Kh(/h0))限=x,y = y]
h0=h+1
1 Here M is the number of aggregate state-action pairs; e is the largest difference between any pair of optimal
state-action values associated with a common aggregate state-action pair.
2
Under review as a conference paper at ICLR 2021
Let ∏* denote an optimal policy in the MDP that gives the optimal value functions Vh+c(χ)
supπ Vhπ(x) for any x ∈Sand h ∈ [H]. Recall the Bellman equations:
Vn(X) = Qn (X, πh(x))	V Vhi(X) = miny Qh(x,y)
Qπ (x,y) := Ex0,rh〜P(∙∣x,y) [rh + Vh+l (x0)] Q Qh(XM := Ex”〜P(∙∣x,y) [rh + Vh+1 (XO)]
Vh+ι(x) = 0, ∀X ∈ S [ Vh+ι(X)=0, ∀X ∈ S
We let RegretMDP (K) denote the expected cumulative regret against π i on the MDP up to the end
of episode k. Let πk denote the policy the agent chooses at the beginning of the kth episode.
K
RegretMDP(K)=X [Vι* ㈤)-Vιπk (Xk)]
k=1
(1)
2.1	One-Sided-Feedback
Whenever we take an action y at stage h, once the environment randomness Dh is realized, we
can learn the rewards/transitions for all the actions that lie on one side of y, i.e., all y0 ≤ y for the
lower one-sided feedback setting (or all y0 ≥ y for the higher side). This setting requires that the
action space can be embedded in a compact subset ofR (Appendix B), and that the reward/transition
only depend on the action, the time step and the environment randomness, even though the feasible
action set depends on the state and is assumed to be an interval A∩[a, ∞) for some a = ah(Xh).
We assume that given Dh, the next state Xh+ι(∙) is increasing in yh, and ah(∙) is increasing in
Xh for the lower-sided-feedback setting. We assume the optimal value functions are concave. These
assumptions seem strong, but are actually widely satisfied in OR/finance problems, such as inventory
control (lost-sales model), portfolio management, airline’s overbook policy, online auctions, etc.
2.2	Full-Feedback
Whenever we take an action at stage h, once Dh is realized, we can learn the rewards/transitions for
all state-action pairs. This special case does not require the assumptions in Section 2.1. Example
problems include inventory control (backlogged model) and portfolio management.
3	Algorithms
Algorithm 1 Elimination-Based HaIf-Q-Iearning (HQL)
Initialization: Qh(y)	- H, ∀(y,	h)	∈ A X	[H];	Ah — A,∀h	∈	[H];	AH+ι 一 A,∀k	∈	[K];
for k = 1, . . . , K do
Initiate the list of realized environment randomness to be empty Dk =[]; Receive X1k;
for h =1,. ..,Hdo
if max{Akh } is not feasible then
Take action yh J closest feasible action to Ahh;
else
Take action yhk J max{Akh };
Observe realized environment randomness Dh, append it to Dh;
Update Xkh+1 J Xh+i (Xh,yk ,d h);
for h = H, . . . , 1 do
for y ∈ Ahh do
Simulate trajectory X0h+1,..., X0τhk(x,y) as ifwe had chosen y at stage h using Dh until we
find τhh(X, y), which is the next time we are able to choose from Aτhhk(x,y);
UPdate Qh(y) J (I- αh )Qh(J) + αh [rh,τk (x,y) + Vh+1 (Xh+1 (Xh,yk ,d h))];
Update yhhi J arg maxy∈Akh Qh (y);
Update Akh+1 J {y ∈ Ah : ∣Qh(yk*) — Qh(y)∣ ≤ Confidence Interval2};
Update Vh (X) J maxfeasible y given x Qh (y);
3
Under review as a conference paper at ICLR 2021
Without loss of generality, we present HQL in the lower-sided-feedback setting. We define constants
αk = (H + 1)/(H + k),∀k ∈ [K]. We use rh,h，to denote the cumulative reward from stage h
to stage h0. We use xh+i (x, y, DDk) to denote the next state given x, y and DDk. By assumptions in
Section 2.1, Qh(x, y) only depends on the y for Algorithm 1, so we simplify the notation to Qh(y).
Main Idea of Algorithm 1 At any episode k, we have a “running set” Akh of all the actions that
are possibly the best action for stage h. Whenever we take an action, we update the Q-values for all
the actions in Akh . To maximize the utility of the lower-sided feedback, we always select the largest
action in Akh, letting us observe the most feedback. We might be in a state where we cannot choose
from Akh . Then we take the closest feasible action to Akh (the smallest feasible action in the lower-
sided-feedback case). By the assumptions in Section 2.1, this is with high probability the optimal
action in this state, and we are always able to observe all the rewards and next states for actions in
the running set. During episode k, we act in real-time and keep track of the realized environment
randomness. At the end of the episode, for each h, we simulate the trajectories as if we had taken
each action in Akh, and update the corresponding value functions, so as to shrink the running sets.
Algorithm 2 Full-Q-Learning (FQL)
Initialization: Qk(x, y) - H, ∀(x, y, h) ∈ S X A X [H].
for k = 1, . . . , K do
Receive x1k ;
for h = 1, . . . , H do
Take action yk — arg maxfeasible、givenXh Qh(xk,y); and observe realized DDk;
for x ∈ Sdo
for y ∈ Ado
UPdate Qh(X, y) — (I- αk)Qh(X, y) + αk 卜h(X, y, Dk)) + Vh+i (*i(X, y, Dk))]；
UPdate Vh(X)《-maxfeasible y given X Qh(X,y);
UPdate Xh+1 - Xh+i(Xh,yk,Dk);
Algorithm 2 is a simPler variant of Algorithm 1, where we effectively set the “Confidence Interval”
to be always infinity and select the estimated best action instead of maximum of the running set. It
can also be viewed as an adaPtion of Jin et al. (2018) to the full-feedback setting.
4	Main Results
Theorem 1.	HQL has O(H3√TI) total expected regret on the episodic MDPproblem in the one-
sided-feedback setting. FQL has O(H2√TI) total expected regret in thefull-feedback setting.
Theorem 2.	For any (randomized or deterministic) algorithm, there exists a full-feedback episodic
MDPproblem that has expected regret Ω(√HT), even ifthe Q-values are independent ofthe state.
5 Overview of Proof
We use Qkh,Vhk to denote the Qh,Vh functions at the beginning of ePisode k .
Recall αk =(H + 1)/(H + k). As in Jin et al. (2018) and Dong et al. (2019), we define weights
α0k := Qjk=i (1 - αj ), and αik := αi Qjk=i+i (1 - αj ), and Provide some useful ProPerties in
Lemma 3. Note that ProPerty 3 is tighter than the corresPonding bound in Lemma 4.1 from Jin et al.
(2018), which we obtain by doing a more careful algebraic analysis.
Lemma 3. The following properties hold for αti :
1.	Pit=i αit =1and αt0 =0for t ≥ 1; Pit=i αti =0and αt0 =1for t =0.
2.	maxi∈[t] αi ≤ 2H and £；=1(a。2 ≤ 2H for every t ≥ L
2For convenience, We use a “Confidence Interval” of √k-(√H5ι), where ι = 9log(AT).
4
Under review as a conference paper at ICLR 2021
3.	P∞=i αi = 1 + H for every i ≥ 1.
4.	表 ≤ Pi=I √⅛ ≤ 1√H-for ever t ≥ 1.
All missing proofs for the lemmas in this section are in Appendix B.
Lemma 4. (shortfall decomposition) For any policy π and any k ∈ [K], the regret in episode k is:
H
(V*- Vnk)(Xk) = E∏[ X( m∈aχ Qh(Xh,y) - Qh(Xh,yh))].	⑵
h=1 y∈
Shortfall decomposition lets us calculate the regret of our policy by summing up the difference
between Q-values of the action taken at each step by our policy and of the action the optimal π *
would have taken if it was in the same state as us. We need to then take expectation of this random
sum, but we get around this by finding high-probability upper-bounds on the random sum as follows:
Recall for any (X, h, k) ∈S×[H] × [K], and for any y ∈ Akh, τhk(X, y) is the next time stage after
k
τhk (x,y)
h in episode k that our policy lands on a simulated next state X
that allows us to take an
action in the running set Aτkhk (x,y). The time steps in between are “skipped” in the sense that we do
not perform Q-value updating or V-value updating during those time steps when we take y at time
(h, k). Over all the h0 ∈ [H], we only update Q-values and V-values while it is feasible to choose
from the running set. E.g. if no skipping happened, then τhk(X, y)=h +1. Therefore, τhk(X, y) is a
stopping time. Using the general property of optional stopping that E[Mτ]=M0 for any stopping
time τ and discrete-time martingale Mτ, our Bellman equation becomes
Qh(U) = Er* k,x0k,琮〜P(∙∣χ,y)[r*,τk + V*k(XTk)]
h,τh	τh
(3)
where We simplify notation τh(χ, y) to Th when there is no confusion, and recall rh,hjo denotes the
cumulative reward from stage h to h0 . On the other hand, by simulating paths, HQL updates the Q
functions backward h = H, . . . , 1 for any X ∈S, y ∈ Akh at any stage h in any episode k as follows:
Qh+1(y) 一(1 - αk)Qh(y) + αk片芹但川⑷ + Vk+1(XB(XTk+%,y))〕	⑷
Then by Equation 4 and the definition of αki ’s, we have
k-1
Qh(y) = α0-1H + X ak-1 [rh,τk(χ,y) + VT+(X,y) (XTk(X,y))]. ⑸
i=1
which naturally gives us Lemma 5. For simpler notation, we use τhi = τhi (X, y).
Lemma 5. For any (X, h, k) ∈S×[H] × [K], and for any y ∈ Akh, we have
k-1
(Qh - Qh)⑹=OLI(H - Qh(y)) + X αk-1 [ (VyI- V*i) (XThi) + ri,τh
i=1	h
-r*,τh +
(x,y) (XTi ) + r*,τhh -
.
Then we can bound the difference between our Q-value estimates and the optimal Q-values:
Lemma 6. For any (X, h, k) ∈S×[H] × [K], and any y ∈ Akh, let ι = 9 log(AT), we have:
I (Qh - Qh) (y)∣≤ O0-1H + X αk-1∣ M+1 - V*i)(X") + rh,Ti- r*,Th∣ + CrHI
i=1	- 1
with probability at least 1 一 1∕(AT)8, and we can choose C = 2√2.
Now we define {δh}hH=+11 to be a list of values that satisfy the recursive relationship
δh = H +(1 + 1/H)δh+ι + c√H3l, for any h ∈ [H],
where C is the same constant as in Lemma 6, and δH+1 =0. Now by Lemma 6, we get:
5
Under review as a conference paper at ICLR 2021
Lemma 7. For any (h, k) ∈ [H] × [K], {δh}hH=1 is a sequence of values that satisfy
max I (Qh 一 Qh)(y) I ≤ δ九∕√k — 1 with probability at least 1 一 1/(AT)5.
y∈Akh
Lemma 7 helps the following three lemmas show the validity of the running sets Akh’s:
Lemma 8. For any h ∈ [H ], k ∈ [K ] ,the optimal action y 力 is in the running set Ah with probability
at least 1 一 1/(AT)5.
Lemma 9. Anytime we can play in Ah, the optimal Q-Value of our action is within 3δh∕√k — 1 of
the optimal Q-value of the optimal policy’s action, with probability at least 1 一 2/(AT)5.
Lemma 10. Anytime we cannot play in Akh, our action that is the feasible action closest to the
running set is the optimal action for the state x with probability at least 1 一 1/(AT)5.
Naturally, we want to partition the stages h =1,...,Hin each episode k into two sets, ΓkA and ΓkB,
where ΓkA contains all the stages h where we are able to choose from the running set, and ΓkB contains
all the stages h where we are unable to choose from the running set. So ΓkB t ΓkA =[H], ∀k ∈ [K].
Now we can prove Theorem 1. By Lemma 4 we have that
maxQh (y)—Qh
(y)-Qh(yk))]
≤ e[ X max (Qh (y) — Qh(yk))] + e[ X m∈ax (Qh ⑻-(QAy))].
h∈ΓkA y	h∈ΓkB y
By Lemma 10, the second term is upper bounded by
0 ∙(I - A5T5) + X H ∙ AT5 ≤ X H ∙ A5T5 .	⑹
h∈ΓkB	h∈ΓkB
By Lemma 7, the first term is upper-bounded by
E[X o(3)卜(mAx (Qh⑺-QU)) ≤ √⅛)
h∈ΓkA	h
+ XH∙p(mAχ(Qh(y)-Qh(yh))>春)≤O(PX √k⅛)+o(PX AHT5).
h∈ΓA	Ph∈ΓkA	Ph∈ΓkA
Then the expected cumulative regret between HQL and the optimal policy is:
KK
RegretMDP (K) = X (V* - V*k) (”1) = (V* - V∏1 )(x1) + X (V* - V∏k)⑶)
k=1	k=2
KH
≤H+X(X
k=2 h∈ΓkB
H X
A5 T5 + 工
h∈ΓkA
δh	X H ) < X O(√H7)
√k-ι + vA A5T5J ≤⅛ √k-1 ~
h∈ΓkA	k=2
≤ O(H3√Tι).
□
5.1	Proofs for FQL
Our proof for HQL can be conveniently adapted to recover the same regret bound for FQL in the
full-feedback setting. We need a variant of Lemma 9: whenever we take the estimated best feasible
action in FQL, the optimal Q-ValUe of our action is within √⅛ of the optimal Q-value of the
optimal action, with probability at least 1 — 2/(AT)5. Then using Lemmas 4,5,6 and 8 where all the
Qkh(y) are replaced by Qkh(x, y), the rest of the proof follows without needing the assumptions for
the one-sided-feedback setting.
For the tighter O(H2√Ti) regret bound for FQL in Theorem 1, We adopt similar notations and
proof in Jin et al. (2018) (but adapted to the full-feedback setting) to facilitate quick comprehension
for readers who are familiar with Jin et al. (2018). The idea is to use V1k — V1πk xkh as a high
probability upper-bound on (V1* — V1πk) x1k , and then upper-bound it using martingale properties
and recursion. Because FQL leverages the full feedback, it shrinks the concentration bounds much
faster than existing algorithms, resulting in a significantly lower regret bound. See Appendix E.
6
Under review as a conference paper at ICLR 2021
6	Example Applications: Inventory Control and More
Inventory Control is one of the most fundamental problems in supply chain optimization. It is
known that base-stock policies (aka. Order-up-to policies) are optimal for the classical models We
are concerned with (Zipkin (2000), Simchi-Levi et al. (2014)). Therefore, we let the actions for the
episodic MDP be the amounts to order inventory up to. At the beginning of each step h, the retailer
sees the inventory xh ∈ R and places an order to raise the inventory level up to yh ≥ xh . Without
loss of generality, we assume the purchasing cost is 0 (Appendix C). Replenishment ofyh -xh units
arrive instantly. Then an independently distributed random demand Dh from unknown distribution
Fh is realized. We use the replenished inventory yh to satisfy demand Dh. At the end of stage h, if
demand Dh is less than the inventory, what remains becomes the starting inventory for the next time
period xh+1 =(yh - Dh)+, and we pay a holding cost oh for each unit of left-over inventory.
Backlogged model: if demand Dh exceeds the inventory, the additional demand is backlogged, so
the starting inventory for the next period is xh+1 = yh -Dh < 0. We pay a backlogging cost bh > 0
for each unit of the extra demand. The reward for period h is the negative cost:
rh(xh,yh)=- ch(yh - xh)+oh(yh - Dh)+ + bh(Dh - yh)+ .
This model has full feedback because once the environment randomness-the demand is realized, we
can deduce what the reward and leftover inventory would be for all possible state-action pairs.
Lost-sales model: is considered more difficult. When the demand exceeds the inventory, the extra
demand is lost and unobserved instead of backlogged. We pay a penalty ofph > 0 for each unit of
lost demand, so the starting inventory for next time period is xh+1 =0. The reward for period h is:
rh (xh,yh)=- ch (yh - xh)+oh(yh - Dh)+ + ph (Dh - yh)+ .
Note that we cannot observe the realized reward because the extra demand (Dh - yh)+ is unob-
served for the lost-sales model. However, we can use a pseudo-reward rh(xh,yh)=- oh(yh -
Dh)+ - ph min(yh,Dh) that will leave the regret of any policy against the optimal policy un-
changed (Agrawal & Jia (2019), Yuan et al. (2019)). This pseudo-reward can be observed because
we can always observe min(yh,Dh). Then this model has (lower) one-sided feedback because once
the environment randomness-the demand is realized, we can deduce what the reward and leftover
inventory would be for all possible state-action pairs where the action (order-up-to level) is lower
than our chosen action, as we can also observe min(yh0 ,Dh) for all yh0 ≤ yh.
Past literature typically studies under the assumption that the demands along the horizon are i.i.d.
(Agrawal & Jia (2019), Zhang et al. (2018)). Unprecedentedly, our algorithms solve optimally the
episodic version of the problem where the demand distributions are arbitrary within each episode.
Our result: it is easy to see that for both backlogged and lost-sales models, the reward only depends
on the action, the time step and the realized demand, not on the state-the starting inventory. How-
ever, the feasibility of an action depends on the state, because we can only order up to a quantity no
lower than the starting inventory. The feasible action set at any time is A∩[xh, ∞). The next state
xh+ι(∙) and ah(∙) are monotonely non-decreasing, and the optimal value functions are concave.
Since inventory control literature typically considers a continuous action space [0, M] for some
M ∈ R+, we discretize [0, M] with step-size M, so A = |A| = T2. Discretization incurs additional
regret Regretgap = O( M ∙ HT) = o(1) by Lipschitzness of the reward function. For the lost-sales
model, HQL gives O(H3√T log T) regret. For the backlogged model, FQL gives O(H2√T log T)
regret, and HQL gives O(H3√T log T) regret. See details in Appendix C.
Comparison with existing Q-learning algorithms: If we discretize the state-action space optimally
for Jin et al. (2018) and for Dong et al. (2019), then applying Jin et al. (2018) to the backlogged
model gives a regret bound of O(T37√logT). Applying Dong et al. (2019) to the backlogged
inventory model with optimized aggregation gives us O(T2/3Mog T). See details in Appendix D.
Online Second-Price Auctions: the auctioneer needs to decide the reserve price for the same item
at each round (Zhao & Chen (2019)). Each bidder draws a value from its unknown distribution and
only submits the bid if the value is no lower than the reserve price. The auctioneer observes the bids,
gives the item to the highest bidder if any, and collects the second highest bid price (including the
reserve price) as profits. In the episodic version, the bidders’ distributions can vary with time in an
7
Under review as a conference paper at ICLR 2021
episode, and the horizon consists of K episodes. This is a (higher) one-sided-feedback problem that
can be solved efficiently by HQL because once the bids are submitted, the auctioneer can deduce
what bids it would have received for any reserve price higher than the announced reserve price.
Airline Overbook Policy: is to decide how many customers the airline allows to overbook a flight
(ChatWin (1998)).^^This problem has lower-sided feedback because once the overbook limit is
reached, extra customers are unobserved, similar to the lost-sales inventory control problem.
Portfolio Management is allocation of a fixed sum of cash on a variety of financial instruments
(MarkOWitz (1952)). In the episodic version, the return distributions are episodic. On each day, the
manager collects the increase in the portfolio value as the reward, and gets penalized for the decrease.
This is a full-feedback problem, because once the returns of all instruments become realized for that
day, the manager can deduce what his reward would have been for all feasible portfolios.
7	Numerical Experiments
We compare FQL and HQL on the backlogged episodic inventory control problem against 3 bench-
marks: the optimal policy (OPT) that knows the demand distributions beforehand and minimizes
the cost in expectation, QL-UCB from Jin et al. (2018), and Aggregated QL from Dong et al. (2019).
For Aggregated QL and QL-UCB, we optimize by taking the Q-values to be only dependent on the
action, thus reducing the state-action pair space. Aggregated QL requires a good aggregation of the
state-action pairs to be known beforehand, which is usually unavailable for online problems. We
aggregate the state and actions to be multiples of 1 for Dong et al. (2019) in Table 2. We do not
fine-tune the confidence interval in HQL, but use a general formula JH Iog(HKA) for all settings.
We do not fine-tune the UCB-bonus in QL-UCB either. Below is a summary list for the experiment
settings. Each experimental point is run 300 times for statistical significance.
Episode length: H =1, 3, 5.	Holding cost: oh =2.
Number of episodes: K = 100, 500, 2000.	Backlogging cost: bh = 10.
Demands: Dh 〜(10 - h)∕2 + U[0,1].	Action space: [0,击,20,..., 10].
Table 2: Comparison of cumulative costs for backlogged episodic inventory control
OPT	FQL	HQL Aggregated QL QL-UCB
H	K	mean	SD	mean	SD	mean	SD	mean	SD	mean	SD
	100	88.2	4.1	103.4	6.6	125.9	19.2	406.6	16.1	3048.7	45.0
1	500	437.2	4.4	453.1	6.6	528.9	44.1	1088.0	62.2	4126.3	43.7
	2000	1688.9	2.8	1709.5	5.8	1929.2	89.1	2789.1	88.3	7289.5	57.4
	100	257.4	3.2	313.1	7.6	435.1	17.6	867.9	29.2	7611.1	46.7
3	500	1274.6	6.1	1336.3	10.5	1660.2	48.7	2309.1	129.8	10984.0	73.0
	2000	4965.6	8.3	5048.2	13.3	5700.6	129.1	7793.5	415.6	22914.7	131.1
	100	421.2	3.3	528.0	10.4	752.6	32.9	1766.8	83.8	11238.4	140.0
5	500	2079.0	8.2	2204.0	13.1	2735.1	114.1	4317.5	95.8	15458.1	231.8
	2000	8285.7	8.3	8444.7	16.4	9514.4	364.2	13373.0	189.2	40347.0	274.6
Table 2 shows that both FQL and HQL perform promisingly, with significant advantage over the
other two algorithms. FQL stays consistently very close to the clairvoyant optimal, while HQL
catches up rather quickly using only one-sided feedback. See more experiments in Appendix F.
8	Conclusion
We propose a new Q-learning based framework for reinforcement learning problems with richer
feedback. Our algorithms have only logarithmic dependence on the state-action space size, and
hence are barely hampered by even infinitely large state-action sets. This gives us not only efficiency,
but also more flexibility in formulating the MDP to solve a problem. Consequently, we obtain the
first O(√T) regret algorithms for episodic inventory control problems. We consider this work to be
a proof-of-concept showing the potential for adapting reinforcement learning techniques to problems
with a broader range of structures.
8
Under review as a conference paper at ICLR 2021
References
Shipra Agrawal and Randy Jia. Learning in structured mdps with convex cost functions: Improved
regret bounds for inventory management. arXiv preprint arXiv:1905.04337, 2019.
Richard E. Chatwin. Multiperiod airline overbooking with a single fare class. Operations Research,
46(6):805-819,1998. doi: 10.1287∕opre.46.6.805. URL https://pubsonline.informs.
org/doi/abs/10.1287/opre.46.6.805.
Maxime C. Cohen, Ilan Lobel, and Renato Paes Leme. Feature-based dynamic pricing. Manage-
ment Science, 0(0):null, 2020. doi: 10.1287/mnsc.2019.3485. URL https://doi.org/10.
1287/mnsc.2019.3485.
Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Provably efficient reinforcement learning with
aggregated states, 2019.
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov
decision processes. In Proceedings of the 15th Annual Conference on Computational Learning
Theory, COLT ’02, pp. 255-270, Berlin, Heidelberg, 2002. Springer-Verlag. ISBN 354043836X.
Zhe Feng, Chara Podimata, and Vasilis Syrgkanis. Learning to bid without knowing your value. In
Proceedings of the 2018 ACM Conference on Economics and Computation, EC ’18, pp. 505-522,
New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450358293. doi:
10.1145/3219166.3219208. URL https://doi.org/10.1145/3219166.3219208.
Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of stochastic iterative
dynamic programming algorithms. In Proceedings of the 6th International Conference on Neural
Information Processing Systems, NIPS’93, pp. 703-710, San Francisco, CA, USA, 1993. Morgan
Kaufmann Publishers Inc.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Ilan Lobel, Renato Paes Leme, and Adrian Vladu. Multidimensional binary search for contextual
decision-making. CoRR, abs/1611.00829, 2016. URL http://arxiv.org/abs/1611.
00829.
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust explo-
ration in episodic reinforcement learning, 11 2019.
Harry Markowitz. Portfolio selection*. The Journal of Finance, 7(1):77-91, 1952. doi: 10.1111/
j.1540-6261.1952.tb01525.x. URL https://onlinelibrary.wiley.com/doi/abs/
10.1111/j.1540-6261.1952.tb01525.x.
Aaron Sidford, Mengdi Wang, Xian Wu, Lin F. Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving markov decision processes with a generative model. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp.
5192-5202, Red Hook, NY, USA, 2018. Curran Associates Inc.
David Simchi-Levi, Xin Chen, and Julien Bramel. The logic of logistics. theory, algorithms, and
applications for logistics and supply chain management. 2nd ed. 2014.
Sean Sinclair, Siddhartha Banerjee, and Christina Yu. Adaptive discretization for episodic rein-
forcement learning in metric spaces. Proceedings of the ACM on Measurement and Analysis of
Computing Systems, 3:1-44, 12 2019. doi: 10.1145/3366703.
Aleksandrs Slivkins. Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272, 2019.
Christopher Watkins and Peter Dayan. Technical note: Q-learning. Machine Learning, 8:279-292,
05 1992. doi: 10.1007/BF00992698.
J. Weed, Vianney Perchet, and P. Rigollet. learning in repeated auctions. 2016.
Hao Yuan, Qi Luo, and Cong Shi. Marrying stochastic gradient descent with bandits: Learning
algorithms for inventory systems with fixed costs. Available at SSRN, 2019.
9
Under review as a conference paper at ICLR 2021
Huanan Zhang, Xiuli Chao, and Cong Shi. Closing the gap: A learning algorithm for the lost-sales
inventory system with lead times. Available at SSRN 2922820, 2018.
Haoyu Zhao and Wei Chen. Stochastic one-sided full-information bandit. CoRR, abs/1906.08656,
2019. URL http://arxiv.org/abs/1906.08656.
P. Zipkin. Foundations of Inventory Management.	McGraw-Hill Companies,Incorporated,
2000. ISBN 9780256113792. URL https://books.google.com/books?id=
rjzbkQEACAAJ.
10