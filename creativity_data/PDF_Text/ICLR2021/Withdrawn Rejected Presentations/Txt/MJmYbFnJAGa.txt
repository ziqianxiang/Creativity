Under review as a conference paper at ICLR 2021
Mime: Mimicking Centralized Stochastic Al-
gorithms in Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) is a challenging setting for optimization due to the het-
erogeneity of the data across different clients. This heterogeneity has been shown
to cause a client drift, which can significantly degrade the performance of algo-
rithms designed for the FL setting. In contrast, centralized learning with centrally
collected data is not affected by such a drift and has seen great empirical and theo-
retical progress with innovations such as momentum and adaptivity. In this work,
we propose a general algorithmic framework, Mime, which mitigates client drift
and adapts arbitrary centralized optimization algorithms such as SGD and Adam
to the federated learning setting. MIME uses a combination of control-variates
and server-level statistics (e.g. momentum) at every client-update step to ensure
that each local update mimics that of the centralized method run on iid data. Our
thorough theoretical and empirical analyses strongly establish Mime’s superiority
over other baselines.
1 Introduction
Federated learning has become an important paradigm in large-scale machine learning where the
training data remains distributed over a large number of clients, which may be mobile phones or
network sensors (Konecny et al., 2016b;a; McMahan et al., 2017; Mohri et al., 2019; Kairouz et al.,
2019). A centralized model, here referred to as a server model, is then trained, without ever transmit-
ting client data over the network, thereby providing some basic levels of data privacy and security.
Two important settings are distinguished in Federated learning (Kairouz et al., 2019, Table 1): the
cross-device and the cross-silo settings. The cross-silo setting corresponds to a relatively small num-
ber of reliable clients, typically organizations, such as medical or financial institutions. In contrast,
in the cross-device federated learning setting, the number of clients may be extremely large and
include, for example, all 3.5 billion active android phones (Holst, 2019). Thus, in that setting, we
may never make even a single pass over the entire clients’ data during training. The cross-device
setting is further characterized by resource-poor clients communicating over a highly unreliable net-
work. Together, the essential features of this setting give rise to unique challenges not present in the
cross-silo setting. Here, we are interested in the cross-device setting, for which we will formalize
and study stochastic optimization algorithms.
The de facto standard algorithm for this setting is FedAvg (McMahan et al., 2017), which performs
multiple SGD updates on the available clients, before communicating to the server. While this
approach can reduce the total amount of communication required, performing multiple steps on the
same client can lead to ‘over-fitting’ to its atypical local data, a phenomenon known as client drift
(Karimireddy et al., 2020). Furthermore, algorithmic innovations such as momentum (Sutskever
et al., 2013; Cutkosky and Orabona, 2019), adaptivity (Kingma and Ba, 2014; Zaheer et al., 2018;
Zhang et al., 2019), and clipping (You et al., 2017; 2019; Zhang et al., 2020) are critical to the success
of deep learning applications and need to be incorporated into the client updates, replacing the
SGD update of FedAvg. Perhaps due to such deficiencies, there exists a large gap in performance
between the centralized setting, where data is centrally collected on the server, and the federated
setting (Zhao et al., 2018; Hsieh et al., 2019; Hsu et al., 2019; Karimireddy et al., 2020).
To overcome such deficiencies, we propose a new framework, Mime, that mitigates client drift and
adapts arbitrary centralized optimization algorithms, e.g. SGD with momentum or Adam, to the
federated setting. In each local client update, Mime uses global statistics, e.g. momentum, and an
1
Under review as a conference paper at ICLR 2021
SVRG-style correction to mimic the updates of the centralized algorithm run on i.i.d. data. These
global statistics are computed only at the server level and kept fixed throughout the local steps,
thereby avoiding a bias due to the atypical local data of any single client.
Contributions. We summarize our main results below.
•	We formalize the cross-device federated learning problem, and propose a new framework MIME
that can adapt arbitrary centralized algorithms to this setting.
•	We prove that incorporating server momentum into each local client update reduces client drift
and leads to optimal statistical rates.
•	Further, we quantify the usefulness of performing multiple local updates on a single client by
carefully tracking the bias (client-drift) introduced. This is the first analysis showing improved
rates by taking additional multiple steps for general smooth functions.
•	Finally, we also propose a simpler variant, MIMELITE, with an empirical performance similar to
Mime. We report the results of thorough experimental analysis demonstrating that both Mime
and MimeLite are faster than FedAvg.
Related work. Analysis of FedAvg: Much of the recent work in federated learning has focused
on analyzing FedAvg. For identical clients, FedAvg coincides with parallel SGD, for which
Zinkevich et al. (2010) derived an analysis with asymptotic convergence. Sharper and more refined
analyses of the same method, sometimes called local SGD, were provided by Stich (2019), and
more recently by Stich and Karimireddy (2019), Patel and Dieuleveut (2019), Khaled et al. (2020),
and Woodworth et al. (2020b), for identical functions. Their analysis was extended to heteroge-
neous clients in (Wang et al., 2019; Yu et al., 2019b; Karimireddy et al., 2020; Khaled et al., 2020;
KoloskoVa et al., 2020). Charles and Konecny (2020) derived a tight characterization of FedAvg
with quadratic functions and demonstrated the sensitivity of the algorithm to both client and server
step sizes. Matching upper and lower bounds were recently given by Karimireddy et al. (2020) and
Woodworth et al. (2020a) for general functions, proving that FedAvg can be slower than even SGD
for heterogeneous data, due to the client-drift.
Comparison to SCAFFOLD: For the cross-silo setting where the number of clients is relatively low,
Karimireddy et al. (2020) proposed the Scaffold algorithm, which uses control-variates (similar
to SVRG) to correct for client drift. However, their algorithm crucially relies on stateful clients
which repeatedly participate in the training process. In contrast, we focus on the cross-device setting
where clients may be visited only once during training and where they are stateless. This is akin to
the difference between the finite-sum and stochastic settings in traditional centralized optimization.
Improvements to FedAvg: Hsu et al. (2019) and Wang et al. (2020c) observed that using server
momentum significantly improves over vanilla FEDAVG. This idea was generalized by Reddi et al.
(2020), who replaced the server update with an arbitrary optimizer, e.g. Adam. However, these
methods only modify the server update while using SGD for the client updates. Mime, on the
other hand, ensures that every local client update resembles the optimizer e.g. MIME would apply
momentum in every client update and not just at the server level. Beyond this, Li et al. (2018)
proposed to add a regularizer to ensure client updates remain close. However, its usefulness is
unclear (cf. Fig. 5, Karimireddy et al., 2020; Wang et al., 2020b). Other orthogonal directions
which can be combined with Mime include tackling computation heterogeneity, where some clients
perform many more updates than others (Wang et al., 2020b), improving fairness by modifying the
objective (Mohri et al., 2019; Li et al., 2019), incorporating differential privacy (Geyer et al., 2017;
Agarwal et al., 2018; Thakkar et al., 2020), Byzantine adversaries (Pillutla et al., 2019; Wang et al.,
2020a; He et al., 2020a), secure aggregation (Bonawitz et al., 2017; He et al., 2020b), etc. We refer
the reader to the extensive survey by Kairouz et al. (2019) for additional discussion.
2 Problem setup
This section formalizes the problem of cross-device federated learning. We first examine some key
challenges of this setting (cf. Kairouz et al., 2019) to ensure our formalism captures the difficulty:
1.	Communication cost between the server and the clients is a major concern and the source of
bottleneck in federated learning; thus, a key metric for optimization in this setting is the number
of communication rounds.
2
Under review as a conference paper at ICLR 2021
2.	Each client is likely to participate at most once, due to the extremely large number of clients;
furthermore, each individual client may have very little data of its own.
3.	There may be a wide heterogeneity or non-i.i.d.-ness due to the difference of data distributions
for the clients.
Thus, our objective will be to minimize the following quantity within the fewest number of client-
server communication rounds:
1 ni
f(x) = Ei〜D [fi(x) := n X fi(x; Zi,ν)].	⑴
Here, fi denotes the loss function of client i and {ζi,1, . . . , ζi,ni} its local data. Since the number of
clients is extremely large, while size of each local data is rather modest, we represent the former as
an expectation and the latter as a finite sum. In each round, the algorithm samples a subset of clients
(of size S) and performs some updates to the server model. There is some inherent tension between
the second and the third challenge outlined above: if there exists a client with arbitrarily different
data whom we may never encounter during training, then there is no hope to actually minimize f .
Thus for (1) to be tractable, it is necessary to assume bounded dissimilarity between different fi .
(A1) G2-BGD or bounded gradient dissimilarity: there exists G ≥ 0 such that
Ei〜D[kVfi(X)-Vf (x)k2] ≤ G2, Vx .
Next, We also characterize the variance in the Hessians. Note that if fi(∙; Z) is L-Smooth, (A2) is
always satisfied with δ ≤ 2L and hence is more ofa definition rather than an assumption. Note that
hoWever, in realistic examples We expect the clients to be similar and hence that δ L.
(A2) δ-BHD or bounded Hessian dissimilarity: Almost surely, f is δ-Weakly convex i.e.
V2fi(x)	-δI and the loss function of any client i satisfies
kV2fi(x; ζ) - V2f(x)k ≤ δ, ∀x .
In addition, We assume that f(x) is bounded from beloW by f? and is L-smooth, as is standard.
3	Using momentum to reduce client drift
In this section We examine the tension betWeen reducing communication by running multiple client
updates each round, and degradation in performance due to client drift (Karimireddy et al., 2020).
To simplify the discussion, We assume a single client is sampled each round and that clients use
full-batch gradients.
Server-only approach. A simple Way to avoid the issue of client drift is to take no local steps. We
sample a client i 〜 D and run SGDm with momentum parameter β and step size η:
xt = xt-1 - η ((1 - β)Vfi(xt-1) +βmt-1) ,	(2)
mt = (1 - β)Vfi(xt-1) + βmt-1 .
Here, the gradient Vfi (xt) is unbiased i.e. E[Vfi (xt)] = Vf(xt) and hence we are guaranteed
convergence. However, this strategy can be communication-intensive and we are likely to spend all
our time waiting for communication with very little time spent on computing the gradients.
FedAvg approach. To reduce the overall communication rounds required, we need to make more
progress in each round of communication. Starting from y0 = xt-1, FEDAVG (McMahan et al.,
2017) runs multiple SGD steps on the sampled client i 〜 D
yk = yk-1 - ηVfi (yk-1) for k ∈ [K] ,	(3)
and then a pseudo-gradient gt = -(yκ — xt) replaces Vfi(xt-ι) in the SGDm algorithm (2). This
is referred to as server-momentum since it is computed and applied only at the server level (Hsu
et al., 2019). However, such updates give rise to client-drift resulting in performance worse than
the naive server-only strategy (2). This is because by using multiple local updates, (3) starts over-
fitting to the local client data, optimizing fi(x) instead of the actual global objective f (x). The net
3
Under review as a conference paper at ICLR 2021
□ x?
Mime updates
Figure 1: Client-drift in FedAvg (left) and Mime (right) is illustrated for 2 clients with 3 local steps
and momentum parameter β = 0.5. The local SGD updates of FEDAVG (shown using arrows for
client 1 and Client2) move towards the average of client optima x1 + x2 which can be quite different
from the true global optimum x? . Server momentum only speeds up the convergence to the wrong
point in this case. In contrast, Mime uses unbiased momentum and applies it locally at every update.
This keeps the updates of MIME closer to the true optimum x? .
effect is that FEDAVG moves towards an incorrect point (see Fig 1, left). If K is sufficiently large,
approximately
yK	xi? , where xi? := arg min fi(x)
x
⇒ Ei〜D [gt] S(Xt - Ei〜D [x?]).
Further, the server momentum is based on gt and hence is also biased. Thus, it cannot correct for
the client drift. We next see how a different way of using momentum can mitigate client drift.
Mime approach. FEDAVG experiences client drift because both the momentum and the client
updates are biased. To fix the former, we compute momentum using only global statistics as in (2):
mt = (1 - BRfi(xt-ι) + βmt-ι .	(4)
To reduce the bias in the local updates, we will apply this unbiased momentum every step:
yk = yk-1 - η((1 - β)Vfi(yk-ι) + βmt-ι) for k ∈ [K] .	(5)
Note that the momentum term is kept fixed during the local updates i.e. there is no local momentum
used, only global momentum is applied locally. Since mt-1 is a moving average of unbiased gra-
dients computed over multiple clients, it intuitively is a good approximation of the general direction
of the updates. By taking a convex combination of the local gradient with mt-1, the update (5)
is potentially also less biased. In this way Mime combines the communication benefits of taking
multiple local steps and prevents client-drift (see Fig 1, right). Sec. C makes this intuition precise.
4	Mime framework
In this section we describe how to adapt arbitrary centralized algorithms (and not just SGDm) to
the federated learning problem (1) while ensuring there is no client-drift. Algorithm 1 describes
two variants Mime and MimeLite, which consists of three components i) a base algorithm we are
trying to mimic, ii) how we compute the global statistics, and iii) the local client updates.
Base algorithm. We assume the centralized base algorithm we are imitating can be decomposed
into two steps: an update step U which updates the parameters x, and a statistics step V(∙) which
keeps track of global statistics s. Each step of the base algorithm B = (U, V) uses a gradient g to
x J x — ηU(g, S),
S J V(g, S).
(BaseAlg)
V may track multiple statistics which we represent collectively as S. While SGDm (2) is clearly of
this form, Appendix ?? shows this for other algorithms like Adam, etc.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Mime and MimeLite
input: initial x and s, learning rate η and base algorithm B = (U, V)
for each round t = 1,…，T do
sample subset S of clients
communicate (x, S) to all clients i ∈ S
communicate C — ∣sS∣ Pj∈s Pfj (x) (only for Mime)
on client i ∈ S in parallel do
initialize local model yi J X
for k = 1,…，K do
sample mini-batch Z from local data
yi J yi - ηU(Pfi(yi； Z) - Pfi(χ; Z) + C S) (Mime)
yi J yi - ηUPfi(yi； Z), S) (MimeLite)
end for
compute full local-batch gradient Pfi (x)
communicate (yi, Pfi (x))
end on client
s J V(看 P∈ Pfi(x), S) (update optimization statistics)
x J 4 P∈ yi (update server parameters)
end for
Compute statistics globally, apply locally. When updating the statistics of the base algorithm,
we use only the gradient computed at the server parameters. Further, they remain fixed throughout
the local updates of the clients. This ensures that these statistics remain unbiased and representative
of the global function f (∙). Atthe end of the round, the server performs
S j v(由 Pi∈S Pfi(x), S) ,	where Pfi(X) = n1 Pn= 1 Pfi(x;Zi,”).	(STATS)
Note that we use full-batch gradients computed at the server parameters X, not client parameters yi.
Local client updates. Each client i ∈ S performs K updates using U of the base algorithm and
a minibatch gradient. There are two variants possible corresponding to MIME and MimeLite
differentiated using colored boxes. Starting from yi J x, repeat the following K times
yi J yi - ηU(g, S) , where
g J Pfi(yi； Z) -Pfi(x; Z) + & Pj∈s Pfj(x) or Vfi(yi; Z).
(CltStep)
MimeLite simply uses the local minibatch gradient whereas Mime uses an SVRG style correc-
tion (Johnson and Zhang, 2013). This is done to reduce the noise from sampling a local mini-batch.
While this correction yields faster rates in theory (and in practice for convex problems), in deep
learning applications we found that MimeLite closely matches the performance of Mime.
Finally, there are two modifications made in practical FL: we weigh all averages across the clients
by the number of datapoints ni, and we perform K epochs instead of K steps (McMahan et al.,
2017). The former modifies the objective (1) with fi being weighted by ni , and the latter has been
empirically observed to perform better, but lacks strong justification (Wang et al., 2020b).
5	Theoretical analysis of Mime
Table 1 summarizes the rates of server-only methods, FedAvg, S caffold and Mime (new results
highlighted in blue). Our theory focuses on Mime with base algorithm of SGD with momentum
based variance reduction1 (MimeMVR) since it obtains optimal rates.
1The momentum based variance reduction (MVR), introduced by Cutkosky and Orabona (2019), is a modi-
fication of the standard SGDm algorithm to make it amenable to analysis. All our theory uses MVR, while our
experiments use SGDm.
5
Under review as a conference paper at ICLR 2021
Table 1: Number of communication rounds required to reach ∣∣Vf (x)k2 ≤ e for L-smooth functions
(log factors are ignored) with S clients sampled each round. G2 bounds the gradient dissimilarity
(A1), and δ bounds the Hessian dissimilarity (A2). FEDAVG is slower than the server-only methods
due to additional ∣drift terms. Convergence of SCAFFOLD depends on the total number of clients
N which is potentially infinite. MIME matches the optimal statistical rates (first term in the rates)
of the server-only methods while improving the optimization (second) term (typically δ《L).
Algorithm	Non-convex		μ-Strongly convex
Server-Only	G2 Q		或+ L μSe	μ
SGD (Ghadimi and Lan, 2013)		+ L	
MVR (Cutkosky and Orabona, 2019)	(√	B) 2+ L	—
Fe dAvg 1			
FedSGD (Karimireddy et al., 2020)	G2 Q	+晶+ L	μSe + % + L
SCAFFOLD2 (Karimireddy et al., 2020)	(N	2 尸⅛	N + L S + μ
Mime 3			
MimeSGD	G2 Q	+ f 3	GL + δ μSe	μ
MimeMVR	(√	K户+ δ	—
Lower bound (Arjevani et al., 2019)	Ω(;	G) 2 /Se)	Ω (G)	
1 Requires K ≥ G number of local updates with within-client variance of σ2.
2 In cross-device FL, the total number of clients (N) can be of the same order as number of rounds
(since we only make few passes over all clients), or even ∞, making the bounds vacuous.
3 Requires K ≥ L∕δ number of local updates.
Theorem I. For L-smooth f with G2 gradient dissimilarity (A1), δ Hessian dissimilarity (A2) and
F := (f (x0 ) - f? ), let us run MimeMVR for T rounds and generate an output xout. This output
satisfies E∣Vf (xout)∣2 ≤ e under the following conditions
PL-Strongly convex without momentum: for η = O (min(
T = O
J + δKpι
+-KiTlo log
1
δK+μK+L，
G)),
0, and
•	Non-Convex without momentum: for η = O (min( δκ+L，(GSFK2
T =O( LGF +(L + δK)F∖
=I Se2 +
0, and
eK
•	Non-convex with momentum: for η = O (min( δκ+L，( g2Tk3 )1/3))，β = 1 一。( (TG2)2∕3),
t = o((—)3/4 +
(L + δK)F∖
-eκ	. `
The expectation in E∣Vf(xout)∣2 ≤ e is taken both over the sampling of the clients during the
running of the algorithm, the sampling of the mini-batches in local updates, and the choice of xout
(which is chosen randomly from the client iterates yi as described in the Appendix).
Table 1 shows that the rate (ignoring constants) of FEDAVG on non-convex functions is (G +
13/2 + L) .This is Slower than simply running SGD which obtains a rate of (悬 + L) . In contrast,
MimeSGD obtains a rate of (S2 + 1) where δ《L thus improving upon both SGD and FedAvg.
While asymptotically these three rates may seem equivalent, in machine learning we care about low
accuracy settings where e is not too small (Bottou, 2010) and so the lower order terms matter, as
6
Under review as a conference paper at ICLR 2021
——SGDm, K= 1
——Mime,K= 2
——Mime, K= 10
0	20	40	60
rounds
Figure 2: SGDm (dashed black), FedSGDm (top), MimeLiteSGDm (middle), and MimeSGDm
(bottom) on simulated data, all with momentum (β = 0.5). FedAvg gets slower as the gradient-
dissimilarity (G) increases (to the right). MimeLite shows a similar pattern, but is consistently
better than FedAvg. Mime is significantly faster than both and is unaffected by heterogeneity (G).
G = IOO
——SGDm1K=I
MImeUts1K= 2
——MImeUts1K=IO
0	20	40	60
also additionally evidenced by our experiments (Sec. 6). We can also compare with SCAFFOLD
2
(Karimireddy et al., 2020) which obtains a rate of (S)3 LL where N is the total number of clients.
While asymptotically this is a faster rate, N in the cross-device setting is potentially infinite or at
least comparable to the total number of training rounds, making these bounds vacuous. This too is
reflected in our experiments (Fig. 5).
Finally, by incorporating momentum based variance reduction, MimeMVR matches the lower bound
_ . 3
of Ω (G∕√Se)2 by Arjevani et al. (2019). The momentum β used in this case is of the order of
(1 - O(T G2)-2/3) i.e. as T increases, our momentum parameter asymptotically approaches 1. In
contrast, previous analyses of distributed momentum (e.g. Yu et al. (2019a)) prove rates of the form
G2	1
sq-β)e2, which are worse than that of standard SGD by a factor of ɪ-e. Thus, ours is the first result
which theoretically showcases the usefulness of using large momentum in distributed and federated
learning. Our theory suggests that the momentum parameter should be increased if G increases i.e.
as the clients become more heterogeneous, there is stronger client-drift and hence we need more
momentum to compensate.
Our analysis is is highly non-trivial and involves three crucial ingredients: i) computing the momen-
tum at the server level to ensure that it remains unbiased and then applying it locally during every
client update to reduce variance, ii) carefully keeping track of the bias introduced via additional lo-
cal steps, and iii) an SVRG correction to allow using mini-batches. Our experiments (Sec. 6) verify
that the first two theoretical insights are indeed applicable in deep learning settings as well, whereas
the latter seems to matter more in convex settings. See App. C where we make this discussion more
concrete and Appendices F-G for detailed proofs and theorem statements.
6 Experimental analysis
We run experiments on simulated and real datasets to confirm our theory. Our main findings are
i) Mime outperforms FedAvg across all settings, ii) its SVRG correction is useful for convex
problems, and iii) momentum significantly improves performance for non-convex problems.
We consider four algorithms: Server-Only, FedAvg, Mime, and MimeLite. Each of these
adapt base optimizers SGD, SGDm, and Adam. The Server-Only method computes a full batch
gradient on each of the sampled clients and uses their aggregate directly in the base optimizer (akin
to (2)). For FedAvg, we follow Reddi et al. (2020) who run multiple epochs of SGD on each client
sampled, and then aggregate the net client updates. This aggregated update is used as a pseudo-
gradient in the base optimizer (called server optimizer). The learning rate for the server optimizer is
fixed to 1 as in (Wang et al., 2020c). This is done to ensure all algorithms have the same number of
hyper-parameters. Finally, Mime and MimeLite follow Algorithm 1 and also run a fixed number
of epochs on the client. Aggregation is weighted by the number of samples on the clients.
7
Under review as a conference paper at ICLR 2021
O 200	400	600	800	1000 O 200	400	600	800 IOOO O 200	400	600	800 IOOO
rounds	rounds	rounds
Figure 3: Server-only, FedAvg, Mime, and MimeLite with SGDm (left) and Adam (middle) run
on (top) EMNIST62 and a 2 hidden layer (300u-100) MLP and (bottom) Resnet20 run on Cifar100.
Mime and MimeLite have very similar performance and are consistently the best. FedAvg is even
worse than the server-only baselines. Also, Mime makes better use of momentum than FedAvg,
with a large increase in performance (right).
6.1	Simulated convex experiments
Our simulated experiments use two clients each with a simple scalar quadratic loss, as in (Karim-
ireddy et al., 2020). We use full-batch gradients with both clients participating every round. The
simulated data has Hessian dissimilarity δ = 1 (A2) and smoothness L = 2. We vary the gradient
dissimilarity (A1) as G ∈ [1, 10, 100]. All the algorithms use momentum with β = 0.5 and their
learning rates were tuned up to a tolerance of 5E-3 to ensure lowest loss after 60 rounds. The results
are collected in Fig. 2. When G is small, we see that FEDAVG can outperform the SERVER-ONLY
(SGDm) baseline, though its loss quickly plateaus. On increasing G, FEDAVG becomes even slower.
MimeLite differs from FedAvg only in how the momentum is used. In all settings, it slightly out-
performs FEDAVG though even it sees a substantial slow down as we increase G. This reflects our
theory which predicts that for convex cases, momentum does not give significant gains. Mime, on
the other hand, is substantially faster than all other methods and is even unaffected by changing G.
Thus, in this simple convex setting, the SVRG correction completely eliminates client drift.
6.2	Real world experiments
We run real world deep learning experiments on EMNIST62 with a 2 layer MLP model and on
Cifar100 with ResNet20, both accessed through Tensorflow Federated (TFF, 2020a). All methods
run 10 local epochs, batch size 20, and the learning rates for all methods were individually tuned.
We refer to Appendix A for additional details and results. Fig 3 shows the results.
Mime > MimeLite > Server-only > FedAvg. Mime and MimeLite have the best performance.
FedAvg is slower than even the naive server-only methods which make no local updates. This
perfectly mirrors our theory that Mime > server-only > FedAvg. The performance of MimeLite is
because SVRG correction may not be necessary in deep learning (Defazio and Bottou, 2019).
With momentum > without momentum. Fig. 3 (right) examines the impact of momentum on
FedAvg and Mime. Momentum slightly improves the performance of FedAvg, whereas it has a
significant impact on the performance of Mime. This is also in line with our theory and confirms
that Mime’s strategy of applying it locally at every client update makes better use of momentum.
Fixed statistics > updated statistics. Finally, we check how the performance of Mime changes
if instead of keeping the momentum fixed throughout a round, we let it change. The momentum is
reset at the end of the round ignoring the changes the clients make to it. Appendix B.1 shows that
this consistently worsens the performance, confirming that it is better to keep the statistics fixed.
8
Under review as a conference paper at ICLR 2021
Together, the above observations validate all aspects of Mime (and MimeLite) design: compute
statistics at the server level, and apply them unchanged at every client update.
7 Conclusion
Our work initiated a formal study of the cross-device federated learning problem. We argued that
the natural heterogeneity among the clients gives rise to client drift and significantly hampers the
performance of approaches such as FedAvg. We then showed how momentum can be an excellent
tool to overcome this client drift if used correctly. Based on this observation, we introduced a
new framework Mime which not only overcomes client drift, but also adapts arbitrary centralized
algorithms such as Adam to the federated setting without any additional hyper-parameters. We
demonstrated the superiority of Mime via strong convergence guarantees and empirical evaluations.
References
Naman Agarwal, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Brendan McMahan.
cpSGD: Communication-efficient and differentially-private distributed SGD. In Proceedings of
NeurIPS, pages 7575-7586, 2018.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems (NeurIPS), 2017.
Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and
optimization. In Advances in neural information processing systems, pages 1756-1764, 2015.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pages 1175-1191. ACM, 2017.
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, H Brendan McMahan, et al. Towards
federated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pages 177-186. Springer, 2010.
Sebastian Caldas, Jakub Konecny, H Brendan McMahan, and Ameet Talwalkar. Expanding the reach
of federated learning by reducing client resource requirements. arXiv preprint arXiv:1812.07210,
2018a.
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konecny, H Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097,
2018b.
Zachary Charles and Jakub Konecny. On the outsized importance of learning rates in local update
methods. arXiv preprint arXiv:2007.00878, 2020.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized SGD. arXiv preprint
arXiv:2002.03305, 2020.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD.
In Advances in Neural Information Processing Systems, pages 15210-15219, 2019.
Aaron Defazio and Leon Bottou. On the ineffectiveness of variance reduced optimization for deep
learning. In Advances in Neural Information Processing Systems, pages 1753-1763, 2019.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. International Conference on Learning Representations (ICLR), 2019.
9
Under review as a conference paper at ICLR 2021
Roy Frostig, Matthew James Johnson, and Chris Leary. Compiling machine learning programs via
high-level tracing. Systems for Machine Learning, 2018.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Haiku. JAX Resnet model in Haiku, 2020. URL https://github.com/deepmind/
dm-haiku/blob/master/haiku/_src/nets/resnet.py.
Jenny Hamer, Mehryar Mohri, and Ananda Theertha Suresh. FedBoost: Communication-efficient
algorithms for federated learning. In 37th International Conference on Machine Learning (ICML),
2020.
Andrew Hard, Kurt Partridge, Cameron Nguyen, Niranjan Subrahmanya, Aishanee Shah, Pai Zhu,
Ignacio Lopez Moreno, and Rajiv Mathews. Training keyword spotting models on non-iid data
with federated learning. arXiv preprint arXiv:2005.10406, 2020.
Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Byzantine-robust learning on heterogeneous
datasets via resampling. arXiv preprint arXiv:2006.09365, 2020a.
Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Secure byzantine-robust machine learning.
arXiv preprint arXiv:2006.04747, 2020b.
Arne Holst. Smartphone users worldwide 2016-2021, 2019.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B Gibbons. The Non-IID data quagmire
of decentralized machine learning. arXiv preprint arXiv:1910.00189, 2019.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pages 315-323, 2013.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pages 795-811. Springer, 2016.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error feedback
fixes SignSGD and other gradient compression schemes. In 36th International Conference on
Machine Learning (ICML), 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated
learning. In 37th International Conference on Machine Learning (ICML), 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local SGD on
indentical and heterogeneous data. In Proceedings of AISTATS, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A unified
theory of decentralized SGD With changing topology and local updates. In 37th International
Conference on Machine Learning (ICML), 2020.
10
Under review as a conference paper at ICLR 2021
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimiza-
tion: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527,
2016a.
Jakub KoneCny, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh,
and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016b.
kuangliu. Train CIFAR10 with PyTorch, 2020 (accessed June 4, 2020). URL https://github.
com/kuangliu/pytorch-cifar.
Tian Li, Anit Kumar Sahu, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.
On the convergence of federated optimization in heterogeneous networks. arXiv preprint
arXiv:1812.06127, 2018.
Tian Li, Maziar Sanjabi, and Virginia Smith. Fair resource allocation in federated learning. arXiv
preprint arXiv:1905.10497, 2019.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgUera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of
AISTATS, pages 1273-1282, 2017.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. arXiv
preprint arXiv:1902.00146, 2019.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Kumar Kshitij Patel and Aymeric Dieuleveut. Communication trade-offs for synchronized dis-
tributed SGD with large step size. In 33rd Conference on Neural Information Processing Systems
(NeurIPS), 2019.
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.
arXiv preprint arXiv:1912.13445, 2019.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Sashank J. Reddi, Jakub Konecny, Peter Richtarik, BarnabaS Poczos, and Alex Smola. Aide: Fast
and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.
Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using
an approximate newton-type method. In International conference on machine learning, pages
1000-1008, 2014.
Sebastian U. Stich. Local SGD converges fast and communicates little. International Conference
on Learning Representations (ICLR), 2019.
Sebastian U. Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
SGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350,
2019.
Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and H. Brendan McMahan. Distributed mean
estimation with limited communication. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 3329-3337. JMLR. org, 2017.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pages
1139-1147, 2013.
TFF. Tensorflow Federated Datasets, 2020a. URL https://www.tensorflow.org/
federated/api_docs/python/tff/simulation/datasets.
11
Under review as a conference paper at ICLR 2021
TFF. Tensorflow Federated Averaging Implementation, 2020b. URL https://www.
tensorflow.org/federated/api_docs/python/tff/learning/build_
federated_averaging_process.
Om Thakkar, SWarooP Ramaswamy, Rajiv Mathews, and Francoise Beaufays. Understanding Unin-
tended memorization in federated learning. arXiv preprint arXiv:2006.07490, 2020.
Quoc Tran-Dinh, Nhan H. Pham, Dzung T. Phan, and Lam M. Nguyen. Hybrid stochastic gradi-
ent descent algorithms for stochastic nonconvex oPtimization. arXiv preprint arXiv:1905.05920,
2019.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-
Parameterized models and an accelerated PercePtron. arXiv preprint arXiv:1810.07288, 2018.
Hongyi Wang, Kartik Sreenivasan, Shashank RajPut, Harit Vishwakarma, Saurabh Agarwal, Jy-
yong Sohn, Kangwook Lee, and Dimitris PaPailioPoulos. Attack of the tails: Yes, you really can
backdoor federated learning. arXiv preprint arXiv:2007.05084, 2020a.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective in-
consistency Problem in heterogeneous federated oPtimization. arXiv preprint arXiv:2007.07481,
2020b.
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. SlowMo: ImProving
communication-efficient distributed sgd with slow momentum. International Conference on
Learning Representations (ICLR), 2020c.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. AdaPtive federated learning in resource constrained edge comPuting systems. IEEE
Journal on Selected Areas in Communications, 37(6):1205-1221, 2019.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local SGD for heteroge-
neous distributed learning. arXiv preprint arXiv:2006.04735, 2020a.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? In 37th
International Conference on Machine Learning (ICML), 2020b.
Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv
preprint arXiv:1708.03888, 2017.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh BhojanaPalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch oPtimization for deeP
learning: Training bert in 76 minutes. In International Conference on Learning Representations,
2019.
Hao Yu, Rong Jin, and Sen Yang. On the linear sPeeduP analysis of communication efficient mo-
mentum sgd for distributed non-convex oPtimization. arXiv preprint arXiv:1905.03817, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deeP learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, Pages 5693-5700, 2019b.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. AdaPtive meth-
ods for nonconvex oPtimization. In Advances in neural information processing systems, Pages
9793-9803, 2018.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, San-
jiv Kumar, and Suvrit Sra. Why ADAM beats SGD for attention models. arXiv preprint
arXiv:1912.03194, 2019.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient cliPPing accelerates
training: A theoretical justification for adaPtivity. In International Conference on Learning Rep-
resentations, 2020.
12
Under review as a conference paper at ICLR 2021
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient
descent. In Advances in neural information processing systems, pages 2595-2603, 2010.
13
Under review as a conference paper at ICLR 2021
Appendix
A Experimental setup
A. 1 Description of datasets and tasks
We perform 4 tasks over 3 datasets: i) On the EMNIST62 (extended MNIST) dataset (Caldas et al.,
2018b) we run a convex logistic regression model, a fully connected MLP with 2 hidden layers
(300u-100), and convolution model with two CNN layers and two dense layers with dropout. ii) we
also run a ResNet20 on CIFAR100.
In all cases we report the top-1 test accuracy in our experiments. EMNIST uses the metadata indicat-
ing the original author of the characters to separate them into multiple clients yielding naturally par-
titioned dataset. Table 2 summarizes the statistics about the different datasets. Note that the average
number of rounds a client participates in (computed as sampled clients×number of rounds/number
of clients) provides an indication of how much of the training data is seen with Shakespeare being
closest to the cross-silo setting and StackOverflow representing the most cross-device in nature.
Table 2: Details about the datasets used and experiment setting.
	EMNIST62	CIFAR100
Clients	3,400	500
Examples	671,585	50,000
Sampled clients	20	20
Batch size	20	20
Number of epochs	10	10
Avg. rounds each client participates	5.9	40
We use Tensorflow federated datasets (TFF, 2020a) to generate the datasets. Our federated learning
simulation code is written in Jax (Frostig et al., 2018). Our Resnet18 model is based off of (Haiku,
2020) (Resnet v2), and following (Hsieh et al., 2019; Reddi et al., 2020) we replace batch norm
with group norm with 2 groups. Black and white was reversed in EMNIST62 (i.e. subtracted from
1) to make them similar to MNIST. CIFAR100 used the usual pre-processing (normalization and
centering), and data augmentation (random crop and horizontal flipping) following (kuangliu, 2020
(accessed June 4, 2020).
A.2 Practicality of experiments
In the experiments we only cared about the number of communication rounds, ignoring that Mime
actually needs twice the number of bits per round and that the Server-Only methods have a much
smaller computational requirement. This is standard in the federated learning setting as introduced
by McMahan et al. (2017) and is justified because most of the time in cross-device FL is spent in
establishing connections with devices rather than performing useful work such as communication or
computation. In other words, latency and not bandwidth or computation are critical in cross device
FL. However, one can certainly envision cases where this is not true. Incorporating communication
compression strategies (Suresh et al., 2017; Alistarh et al., 2017; Karimireddy et al., 2019) or client-
model compression strategies (Caldas et al., 2018a; Frankle and Carbin, 2019; Hamer et al., 2020)
into our Mime framework can potentially address such issues and are important future research
directions.
As we already discussed previously, we believe both the datasets and the tasks being studied here
are quite realistic in nature. We now discuss our choice of other parameters in the experiment setup
(number of training rounds, sampled clients, batch-size, etc.) Each round of federated learning takes
2-3 mins in the real world and is relatively independent of the size of communication (BonaWitz
et al., 2019) implying that training 1000 rounds takes 1.4-2 days. This underscores the importance
of ensuring that the algorithms for federated learning converge in as few rounds as possible, as
well as have very easy to set default hyper-parameters. Thus in our experimental setup we keep
14
Under review as a conference paper at ICLR 2021
all parameters other than the learning rate to their default values. In practice, this learning rate can
be set by set using a small dataset on the server (as in (Hard et al., 2020)). The choice of batch
size being 10 was made both keeping in mind the limited memory available to each client as well
as to match prior work. Finally, while we limit ourselves to sampling 20 workers per round due
to computational constraints, in real world FL thousands of devices are often available for training
simultaneously each round (Bonawitz et al., 2019). They also note that the probability of each of
these devices being available has clear patterns and is far from uniform sampling. Conducting a
large scale experimental study which mimics these alternate forms of heterogeneity is an important
direction for future work.
A.3 Hyperparameter search
We use the EMNIST62 with MLP model as a ‘test-bed‘ for exploring different algorithms given it
being both a representative task of cross-device FL as well as being computationally efficient. All
plots reported are for this setting. A more fine-grained search over hyperparameters to report the
final test accuracies is made over the rest of the tasks/datasets.
For all SGDm methods, we pick momentum β = 0.9. For Adam methods, we fix β1 = 0.9,
β2 = 0.99, and ε = 1 × 10-3 similar to (Reddi et al., 2020). None of the algorithms use weight
decay, clipping etc. The learning rate is then tuned to obtain the best test accuracy.
For all experiments, unless explicitly mentioned otherwise, the learning rate is searched over a grid
of
η∈ [1 × 101,1,1 × 10-1,1 × 10-2,1 × 10-3,1 × 10-4,1 × 10-5].
A.4 Comparison with previous results
As far as we are aware, (Reddi et al., 2020) is the only prior work which conducts a systematic
experimental study of federated learning algorithms over multiple realistic datasets. The algorithms
comparable across the two works (e.g. FedSGD, FedSGDm, and FedAdam) have qualitatively sim-
ilar performance except with one exception: FedAdam consistently underperforms FedSGDm. We
believe this difference is because (Reddi et al., 2020) additionally tune the server learning rate and
the parameter for Adam. As we explain in Section A.2, we chose to keep these parameters to some
default values to compare methods in the ‘low-tuning’ setting. We also point that while FedAdam
struggles to perform in this setup, MimeAdam and MimeLiteAdam are very stable and even often
outperform their SGDm counterparts. This is also the default/recommended behavior in TensorFlow
Federated (TFF, 2020b) and (Wang et al., 2020c).
B Additional experiments
B.1 Effect of changing statistics
Figure 4: Server-only, FedAvg, Mime, and C-Mime with SGDm (left) and Adam (right) run on
EMNIST62 with a 2 hidden layer (300u-100) MLP. C-Mime changes the statistics (momentum for
SGDm, and first two moments for Adam) using the local client updates. These changes are discarded
at the end of the round and the statistics are reset using only the server level gradients as in Mime.
Clearly, C-Mime is always worse than Mime. This shows that adapting statistics during local client
updates makes them too biased, and it best to keep them fixed during each round like Mime does.
rounds
15
Under review as a conference paper at ICLR 2021
Momentum methods on EMNIST62
Figure 5: Comparison with Scaffold and FedProx for cross-device FL: Mime, SCAFFOLD and
FedProx with SGDm run on EMNIST62 with a 2 hidden layer (300u-100) MLP. For FedProx and
SCAFFOLD, in addition to tuning the learning rate, we search for the best server momentum β ∈
[0,0.9,0.99]. FedProx uses an additional regularize1 μ which We search over [0.1,0.5,1] (note that
FedProx with μ = 0 is the same as FedAvg). The best test accuracy (which are plotted here) was
by β = 0 for both and μ = 0.1 for FedProx. Note that FedProx is the slowest method here (in
fact it is even slower than FedAvg). The additional regularizer does not seem to reduce client drift
while still slowing down convergence (Karimireddy et al., 2020; Wang et al., 2020b). SCAFFOLD
is also slower than Mime in this setup. This is because SCAFFOLD was designed for the cross-
silo setting and not corss-device setting. The large number of clients (N = 3.4k) means that each
client is on averaged visited less than 6 times during the entire training (20 clients per round for 1k
rounds). Hence, the client control variate stored is quite stale (from about 200 rounds ago) which
slows down the convergence. This perfectly reflects our theoretical understanding that when the
number of clients N is large relative to training rounds (which is true in the cross-device setting)
SCAFFOLD is outperformed by Mime.
FEDAVG	MIME	MIMELITE
SGD	SGDm	Adam	SGD	SGDm	Adam	SGD	SGDm	Adam
EMNIST62 logistic ∣66.0	66.8	67.5	67.2	67.3	68.0	66.1	67.3	68.0
EMNIST62CNN 84.9	85.4	85.7	86.0	85.6	86.0	85.0	85.4	85.7
Table 3: High tuning setting: final test accuracy (larger is better) with fully tuned hyper-parameters.
For FedAvg we searched over both the client and server learning rates, whereas for Mime and
MimeLite, we search only over client (base) learning rate. This search is performed over a grid
(0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.8). For momentum, we chose best of (0.9, 0.99) and
for Adam, we varied β1 in (0.9, 0.99), β2 in (0.99, 0.999) and in (0.01, 0.001, 0.0001). In this
setting as well Mime and MimeLite outperform FedAvg but with a smaller margin.
Table 4: Additional algorithmic details: Decomposing base algorithms into a parameter update (U)
and statistics tracking (V).
Algorithm	Tracked statistics s	Update step U	Tracking step V
SGD	—	x -ηg	—
SGDm	m	x - η((1 -β)g+βm)	m = (1 - β)g + βm
RMSProp	v	X - τ+√vg	v = (1 - β)g2 +βv
Adam	m, v	X - τ+√v((I - βI)g + βIm)	m = (1 - β1)g + β1m v = (1 - β2 )g 2 + β2 v
C Proof overview
In this section, we give proof sketches of the main components of Theorem I: i) how momentum
reduces the effect of client drift, ii) how local steps can take advantage of Hessian similarity, and iii)
why the SVRG correction improves constants.
16
Under review as a conference paper at ICLR 2021
Improving the statistical term via momentum. Note that the statistical (first) term in Theorem I
without momentum (β = 0) for the convex case is LG. This is (UP to constants) optimal and cannot
LG2
be improved. For the non-convex case however using β = 0 gives the usual rate of LSGr. However,
this can be improved to ((1+S：G F ) / using momentum. This matches a similar improvement in
the centralized setting (Cutkosky and Orabona, 2019; Tran-Dinh et al., 2019) and is in fact optimal
(Arjevani et al., 2019). Let us examine why momentum improves the statistical term. Assume that
we sample a single client it in round t and that we use full-batch gradients. Also let the local client
update at step k round t be of the form
y J y — ηdk.	(6)
The ideal choice of update is of course d? = Vf (y) but however this is unattainable. Instead,
MIME with momentum β = 1 — a uses d∣GDm = τnk J aVfi(y) + (1 — a)mt-ι where mt-ι is
the momentum computed at the server. The variance of this update can then be bounded as
Ekmk — Vf(y)k2 . a2 EkVfit(y) — Vf(y)k2 + (1 — a) Ekmt-I- Vf(y)k2
≈ a2G2 + (1 — a) Ekmt-1 — Vf (xt-2)k2 ≈ aG2 .
The last step follows by unrolling the recursion on the variance of m. We also assumed that η is
small enough that y ≈ xt-2 . This way, momentum can reduce the variance of the update from
G2 to (aG2) by using past gradients computed on different clients. To formalize the above sketch
requires slightly modifying the momentum algorithm similar to (Cutkosky and Orabona, 2019), and
is carried out in Appendix G.
Improving the optimization term via local steps. The optimization (second) term in Theorem I
for the convex case is IKKL and for the non-convex case (with or without momentum) is δKK+L.
In contrast, the optimization term of the server-only methods is L∕μ and L/e respectively. Since in
most cases δ L, the former can be significantly smaller than the latter. This rate also suggests
that the best choice of number of local updates is L∕δ i.e. we should perform more client updates
when they have more similar Hessians. This generalizes results of (Karimireddy et al., 2020) from
quadratics to all functions.
This improvement is due to a careful analysis of the bias in the gradients computed during the local
update steps. Note that for client parameters yk-1, the gradient E[Vfit (yk-1)] 6= E[Vf (yk-1)]
since yk-1 was also computed using the same loss function fit . In fact, only the first gradient
computed at xt-1 is unbiased. Dropping the subscripts k and t, we can bound this bias as:
E[Vfi(y) — Vf(y)] = E[Vfi(y) — Vfi(x) + Vf(x) — Vf(yi)] + Ei[Vfi(x)] — Vf(x)
-----------------------------{z	} `	-	}	`	{--------}
≈V2fi(x)(y-x)--------------------------------------------------------------≈V2f (x)(x-yi)-=0 since unbiased
≈ E[(V2fi(x) — V2f(x))(yi— x)] ≈ δ E[(yi — x)] .
Thus, the Hessian dissimilarity (A2) control the bias, and hence the usefulness of local updates. This
intuition can be made formal using Lemma 3.
Mini-batches via SVRG correction. In our previous discussion about momentum and local steps,
we assumed that the clients compute full batch gradients and that only one client is sampled per
round. However, in practice a large number (S) of clients are sampled and further the clients use
mini-batch gradients. The SVRG correction reduces this within-client variance since
G2	G2
VarWfi(yi.; Z)- Vfi(X; Z) + 啬 pi∈s Vfi(X)) . L2kyi — xk2 + ~s~ ≈ 歹.
Here, We used the smoothness of fi(∙; Z) and assumed that yi ≈ X since we don,t move too far
within a single round. Thus, the SVRG correction allows us to use minibatch gradients in the local
updates while still ensuring that the variance is of the order G2/S.
D Technicalities
We examine some additional definitions and introduce some technical lemmas.
17
Under review as a conference paper at ICLR 2021
D.1 Assumptions and definitions
We make precise a few definitions and explain some of their implications. We first discuss the
two assumptions on the dissimilarity between the gradients (A1) and the Hessians (A2). Loosely,
these two quantities are an extension of the concepts of variance and smoothness which occur in
centralized SGD analysis to the federated learning setting. Just as the variance and smoothness
are completely orthogonal concepts, we can have settings where G2 (gradient dissimilarity) is large
while δ (Hessian dissimilarity) is small, or vice-versa.
Our assumption about the bound on the G gradient dissimilarity can easily be extended to (G, B)
gradient dissimilarity used by (Karimireddy et al., 2019):
EikVfi(x)k2 ≤ G2 + B2kVf(x)k2.	(7)
All the proofs in the paper extend in a straightforward manner to the above weaker notion. Since
this notion does not present any novel technical challenge, we omit it in the rest of the proofs. Note
however that the above weaker notion can potentially capture the fact that by increasing the model
capacity, we can reduce G. In the extreme case, by taking a sufficiently over-parameterized model,
it is possible to make G = 0 in certain settings (Vaswani et al., 2018). However, this comes both at
a cost of increased resource requirements (i.e. higher memory and compute requirements per step)
but can also result in other constants increasing (e.g. B and L).
The second crucial definition we use in this work is that of δ bounded Hessian dissimilarity (A2).
This has been used previously in the analyses of distributed (Shamir et al., 2014; Arjevani and
Shamir, 2015; Reddi et al., 2016) and federated learning (Karimireddy et al., 2020), but has been
restricted to quadratics. Here, we show how to extend both the notion as well as the analysis to
general smooth functions. The main manner we will use this assumption is in Lemma 3 to claim
that for any x and y the following holds:
EkVfi(y;Z) - Vfi(x; Z) + Vf(X)- Vf(y)k2 ≤ δ2∣∣y — x∣∣2.	(8)
Here the expectation is both over ζ as well as the choice of client i. To understand what the above
condition means, it is illuminating to define Ψi(z; ζ) = fi(z; ζ) - f(z). Then, we can rewrite (A2)
and (8) respectively as
l∣V2Ψi(z; Z)k ≤ δ	and	E∣∣VΨi(y; Z) - VΨi(x; Z)k2 ≤ δ2∣∣y — x∣∣2.
Thus (8) and (A2) are both different notions of smoothness of Ψi(x; ζ) (formal definition of smooth-
ness will follow soon). The latter definition closely matches the notion of squared-smoothness used
by Arjevani et al. (2019) and is a promising relaxation of (A2). However, we run into some techni-
cal issues since in our case the variable y can also be a random variable and depend on the choice
of the client i. Extending our results to this weaker notion of Hessian-similarity and proving tight
non-convex lower bounds is an exciting theoretical challenge.
Finally note that if the functions fi(x; ζ) are assumed to be smooth as in (Shamir et al., 2014;
Arjevani and Shamir, 2015; Karimireddy et al., 2020), then Ψi((x; ζ) is 2L-smooth. Thus, we
always have that δ ≤ 2L. But, as Shamir et al. (2014) show, it is possible to have δ L if
the data distribution amongst the clients is similar. Further, the lower bound from Arjevani and
Shamir (2015) proves that Hessian-similarity is the crucial quantity capturing the number of rounds
of communication required for distributed/federated optimization.
We next define the terms smoothness and strong-convexity which we repeatedly use in the paper.
(A3) f is L-smooth and satisfies:
kVf (x) - Vf(y)k ≤ Lkx - yk , for any x,y .	(9)
The assumption (A3) also implies the following quadratic upper bound on f
f (y) ≤ f(x) + hVf(x), y — Xi + 2∣y — x∣2.	(10)
Further, if f is twice-differentiable, (A3) implies that kV2f(x)k ≤ β for any x.
(A4) f is μ-PL strongly convex (Karimi et al., 2016) for μ > 0 if it satisfies:
kVf(x)k2 ≥2μ(f(x)- f?).
Note that PL-strong convexity is much weaker than the standard notion of strong-convexity
(Karimi et al., 2016).
18
Under review as a conference paper at ICLR 2021
D.2 Some technical lemmas
Now we cover some technical lemmas which are useful for computations later on. First, we state a
relaxed triangle inequality true for the squared `2 norm.
Lemma 1 (relaxed triangle inequality). Let {v1, . . . , vτ} be τ vectors in Rd. Then the following are
true:
1.	IIvi + Vj k2 ≤ (1 + C)Ilvik2 + (1 + C)∣Ivj k2 for any c > 0, and
2.	kPiτ=1 vik2 ≤ τ Piτ=1kvik2.
Proof. The proof of the first statement for any c > 0 follows from the identity:
Ilvi + vjk2 = (I + C)kvik2 + (I + C)kvjk2 - k√cvi + √vjk2 .
For the second inequality, we use the convexity of x → IxI2 and Jensen’s inequality
τ 2	τ
TXvi	≤ TXllvill .	□
i=1	i=1
Next we state an elementary lemma about expectations of norms of random vectors.
Lemma 2 (separating mean and variance). Let {Ξ1 , . . . , Ξτ} beT random variables in Rd which are
not necessarily independent. First suppose that their mean is E[Ξi] = ξi and variance is bounded
as E[IΞi - ξi I2] ≤ σ2. Then, the following holds
ττ
E[IXΞiI2] ≤IXξiI2+T2σ2.
Now instead suppose that their conditional mean is E[Ξi∣Ξi-1,... Ξ1] = ξi i.e. the variables {Ξi 一
ξi } form a martingale difference sequence, and the variance is bounded by E[IΞi - ξi I2] ≤ σ2 as
before. Then we can show the tighter bound
ττ
E[IXΞiI2] ≤2IXξiI2+2Tσ2.
i=1	i=1
Proof. For any random variable X, E[X2] = (E[X 一 E[X]])2 + (E[X])2 implying
ττ	τ
E[IX ΞiI2] = IX ξiI2 + E[∣∣X Ξi-ξiI2].
i=1	i=1	i=1
Expanding the above expression using relaxed triangle inequality (Lemma 1) proves the first claim:
ττ
E[∣X Ξi-ξiI2] ≤ T X E[IΞi - ξiI2] ≤ τ2σ2 .
For the second statement, ξi is not deterministic and depends on Ξi-1 , . .
resort to the cruder relaxed triangle inequality to claim
, Ξ1 . Hence we have to
ττ	τ
E[IXΞiI2] ≤2IXξiI2+2E[IXΞi-ξiI2]
i=1	i=1	i=1
and then use the tighter expansion of the second term:
τ
E[IXΞi-ξiI2]=XE(Ξi-ξi)>(Ξj-ξj) = X EIΞi - ξiI2 ≤Tσ2.
i=1	i,j	i
The cross terms in the above expression have zero mean since {Ξi -ξi} form a martingale difference
sequence.	□
19
Under review as a conference paper at ICLR 2021
E PROPERTIES OF FUNCTIONS WITH δ BOUNDED HESSIAN DISSIMILARITY
We now study two lemmas which hold for any functions which satisfy (A2). The first is closely
related to the notion of smoothness (A3).
Lemma 3 (similarity). Thefollowing holds for any two functions fi(∙; Z) and f (∙) satisfying (A2),
and any x, y:
l∣Vfi(y; Z)- Vfi(x; Z) + Vf(X)- Vf(y)k2 ≤ δ2∣∣y - x『.
Proof. Consider the function Ψ(z) := fi (z; ζ) - f(z). By the assumption (A2), we know that
lV2Ψ(z)l ≤ δ for all z i.e. Ψ is δ-smooth. By standard arguments based on taking limits (Nesterov,
2018), this implies that
l∣VΨ(y) - VΨ(x)∣ ≤ δ∣y - x∣.
Plugging back the definition of Ψ into the above inequality proves the lemma.	□
Next, we see how weakly-convex functions satisfy a weaker notion of “averaging does not hurt”.
Lemma 4 (averaging). Suppose f is δ-weakly convex. Then, for any γ ≥ δ, and a sequence of
parameters {yi}i∈S and x:
∣S∣ X f(yi) + γkχ - y』2 ≥ f(y) + γkχ - yk2, Where y:
⅛ X y"
Proof. Since f is δ-weakly convex, Φ(z) := f (Z) + 2∣∣z 一 x∣2 is convex. This proves the claim
Since |S| Pi∈S φWi) ≤ φ⑹.	□
20
Under review as a conference paper at ICLR 2021
F	Analysis of MimeS GD (without momentum)
Let us rewrite the MimeSGD update using notation convenient for analysis. In each round t, we
sample clients St such that |St| = S. The server communicates the server parameters xt-1 as well
as the average gradient across the sampled clients ct-1 defined as
CtT = S X Vfi(XtT).	(11)
i∈St
Note that computing ct-1 itself requires two rounds of communication. But from a theoretical view-
point, this only changes the communication rounds required by a constant factor and hence we ignore
this issue. Practically, we recommend using MimeLite if this additional rounds of communication
are an issue.
Then each client i ∈ St makes a copy yit,0 = xt-1 and perform K local client updates. In each
local client update k ∈ [K], the client samples a dataset ζit,k and
yt,k = yt,k-ι - η(vfi(yt,k-ι; Zlk)- Vfi(Xt-1; Zt,k) + CtT).	(12)
After K such local updates, the server then aggregates the new client parameters as
xt = 1 X yi,κ.	(13)
i∈St
Variance of update. Consider the local update at step k on client i, dropping superscript t
yi,k = yi,k-1 - ηdi,k, where di,k := Vfi(yi,k-1; Zi,k) - Vfi(X; Zi,k) + C.
Lemma 5. Given that assumptions (A1) and (A2) are satisfied, each client update satisfies
3G2
EIIdi,kk ≤	---+3δ kyi,k-1 - xk +3kvf(yi,k-1)k .
S
Proof. Starting from the definition of di,k and the relaxed triangle inequality,
Idi,k I2 = IVfi(yi,k-1; Zi,k) - Vfi(X; Zi,k) + CI2
= IVfi(yi,k-1; Zi,k) - Vfi(X; Zi,k) + Vf (X) - Vf (yi,k-1) + (C - Vf (X)) + Vf (yi,k-1)I2
≤ 3IVfi(yi,k-1; Zi,k) - Vfi(X; Zi,k) + Vf(X) - Vf(yi,k-1)I2 + 3IC - Vf(X)I2 + 3IVf(yi,k-1)I2
≤ 3δ2Iyi,k-1 - XI2 +3IC- Vf (X)I2 + 3IVf (yi,k-1)I2.
We used Lemma 3 to bound the first term. Taking expectations on both sides to bound the second
term via (A1) yields the lemma.	□
Distance moved in each round. We show that the distance moved by a client in each round during
the K updates can be controlled. To further reduce the burden of notation, we will drop he subscript
i, k and refer yi,k-1 simply as y and yi,k as y+.
Lemma 6. For update following (12) for η ≤ ɪ^ Satisfying (A1) and (A2), we have at any SteP k,
G2
E∣y+ — χk2 ≤ (ι + K)ky — χk2 + 6K√2 — + 6Kη2∣Vf(y)k2.
Proof. Starting from the update (12) and the relaxed triangle inequality Lemma 1 with c = K ≥ 1,
EIy+ - XI2 = EIy - ηd - XI2
≤ (1 + C) Eky - χk2 + (1 + c)η2 Ekdk2
G2
≤ (1 + K) Eky - χk2 + 3(1 + K)η2M + 3(1 + K)η2δ2ky - χk2 + 3(1 + K)η2kVf (y)k2
S
M2
≤ (1 + KK + 6Kη2δ2) Eky - xk2 + 6Kη2 — + 6K-∣∣Vf (y)k2 ∙
The second to last step used the variance bound in Lemma 5. The proof now follows from the
restriction on step-size since 16K2η2δ2 ≤ 1.	□
21
Under review as a conference paper at ICLR 2021
Progress in one client update. We now have the tools required to keep track of the progress made
in one round.
Lemma 7. For any ConStant μ ≥ 0 and each SteP of MimeSGD with SteP size η ≤
min (18L, 756lδκ, 4⅛K), and given that (A1)—(A3) hold, we have
；EkVf(yt,k-ι)k2 ≤ At,®- - At,® + (255KLη2)G2 ,
4	2S
where we define
At,k := E[f(yti,k)∖ + δ(1 + K)	Ekyt,k - xtτk2, and
K-®+1
At,k-1 := E[f (yt,k-ι)∖+ δ(1 - μη) (1 + K∙)	Ekyt,k-1 - χT∣∣2.
Proof. The assumption that f is L-smooth implies a quadratic upper bound (10). Using this in our
case, we have
E[f (y+)∖ - E[f (y)∖ ≤ -η E[hVf (y), di∖ + Lη2 Ekdk2
Lη2
-η E[hVf(y), Vfi(y; Z)-Vfi(x;Z)+ c>∖ + / Ekdk
,-----------------------------}
2
}
{z
T1
{z
T2
Let us examine the terms T1 and T2 separately. By our variance bound Lemma 5, we have that
T2 ≤ 3LSG2 + 3LFky - xk2 + 3L2η2kVf(y)k2.
To simplify T1, the biggest obstacle is that E[Vfi (y; ζ)∖ 6= Vf (y) since y itself depends on the
sampling of the client i. Only the server gradient is unbiased and E[c∖ = Vf (x). Instead we will
use the similarity of the functions as in Lemma 3:
T1= -η E[hVf(y), Vfi(y; ζ) - Vfi(x; ζ) + Vf(x)i∖
≤ -2 EkVf(y)k2 + 2kVfi(y; Z)- Vfi(x; Z) + Vf(x) - Vf (y)k2
≤- 2 EkVf (y)k2 + η22 Eky - xk2.
The first inequality above used that for any a, b, the following holds-2ab = (a - b)2 - a2 - b2 ≤
(a - b)2 - a2 . The second used the similarity Lemma 3. Combining the terms T1 and T2 together,
we have
E[f (y+)∖ - E[f (y)∖ ≤ (3Lη2 -η) EkVf(y)k2 + Wδ +产)Eky - x∣∣2 + ^L^ .
2	2	2S
To bound the distance between y and x, We use Lemma 6 multiplied on both sides by δ(l + KK )K k.
Note that δ ≤ δ(l + KK)K k ≤ 21δ. This gives us for any constant μ ≥ 0
δ(ι + KK)K k Eky+ - xk2 ≤ δ(ι + KK)K k(1 + K)ky - xk2 + 6Kδ(1 + KK)K kη2Gr+
6Kδ(l + K )K-k η2kVf (y)k2
≤ δ(1 -μη)(1 + K∙)K-(k-1)ky - xk2 + 6Kδ(1 + ⅜)K∖CG+
K-k	K-k	δ
6Kδ(1 + K∙)	η2kVf(y)k2 +(1 + K)	(μηδ- K)ky - xk2
G2
≤ δ(1 + K)K (k 1)ky - xk2 + 126Kδη2 — + 126Kδη2∣∣Vf(y)k2
+ (21μηδ - K)ky - xk2 .
22
Under review as a conference paper at ICLR 2021
The second inequality from the last used that 1 + 2/K < (1 + 3∕K)(1 - μη) = (1 + 2/K ) + (1∕K -
(1 + 3/K)μη). This is true by our restriction that η < ^μκ, which implies (1 + 3/K)μη < 4μη <
1∕(10K) and so that (1/K - (1 + 3/K)μη) > 0. Adding the two bounds, We get the following
recursion
E[f(y+)] + δ(i + K)K-k Eky+ - χk1 2 ≤ E[f(y)] + δ(i + K)
X-------------------------------------} X---------------------
=:Ai,k
K-(k-1)
(I - μη)ky - xk2
=:Ai,k-1
(252Kδη2 + 3Lη2 - η)
EkVf (y)k2
(ηδ2 + 3Lη2δ2 + 42μηδ)	δ
2
(3Lη2 + 252Kδη2)G2
Eky - xk2
+
+
+
2
K
}
2S
Now, note that our constraint on the step-size η ≤ min(忐,相末)implies that 252Kδη2+3Lη2 ≤
2 and K(ηδ2 + 3Lη2δ2 +42μηδ) ≤ 2δ. Plugging this into the above bound and recalling that δ ≤ L
finishes the proof.	□
Convergence for PL strongly-convex functions. We will unroll the one step progress Lemma 7
to compute a linear rate.
Theorem II.	Suppose that (A1)-(A4) are satisfied for μ > 0. Then the updates of MimeSGD with
step-size η = min(ηmax,
min
(l⅛，756δK,
satisfy
EkVf(Xout)k2 ≤ O(LG2 + ɪ exp --	7	TK))
μi S ηmax	18L + 756δK + 42μK	))
where we define F := f (x0) — f ?, yk is chosen to be yi k for i ∈ St uniformly at random, and the
output xout to be yk with probability proportional to (1 — nμ )kt-kt.
Proof. Note that by PL strong convexity (A4), we have
ηkVf(y)k2 ≤ ηkVf(y)k2 + ημ(f(y)- f?).
48	4
Using this, we can tighten the one step progress Lemma 7 as
8 EkVf(yt,k-1)k2 ≤(1 - μη) E[f(yt,k-1) - f?]+ δ(1 - μη)(1 + K)K-k+1 E∣%-1 - xt-1k2
、
sz
= (1-μn )φt,k-ι
-E[f(yi,k) - f?] + δ(1 + K)κ-k Ekyt,k - xt-1k2 + (255KSη2)G2
X----------------------------------------------}
}
^z'∖∕f^^
:φt,k
Now take a weighted sum over the steps k using weights (1 - nμ)K-k
8 X (1-ημ)K-k EkVf(yt,k-1)k2 ≤ Φi,0 - (1 -等)KΦt,K + X (1-竽)K-k (255KSη2)G2.
k∈[K]	k∈[K]
By the initialization yit,0 = xt-1 and hence Φit,0 = E f(xt-1 ) - f? and further by the averaging
Lemma 4, we have
1 X Φt,K ≥ E f (Xt) - f?.
i∈S
23
Under review as a conference paper at ICLR 2021
Hence, on averaging over the clients we get the one round progress lemma
8S X X(1 - ημ)K-k EkVf(yt,k-1)k2 ≤ Ef(χt-1) - f*-。- μ)k(Ef(χt)-f?)
k∈[K] i∈St
k (255KLη2)G2
2S
k∈[K]
Now further taking a weighted average over the rounds t ∈ [T] with weights proportional to (1 -
ημ)tK gives
8S XX X(1 - ημ)kt-kt EkVf(yt,k-1)k2 ≤ Ef(χ0)-f?
t∈[T] k∈[K] i∈St
+ X X (1 - ημ )KT-kt &55KLn)Gl2
t∈[T] k∈[K]	2S
Finally, choosing the right step size, similar to Lemma 23 of (Karimireddy et al., 2020) yields the
□
desired rate.
Convergence for general functions. We will unroll the one step progress Lemma 7 to compute a
sublinear rate.
Theorem III.	Suppose that (A1)-(A3) are satisfied. Then the updates of MimeSGD with step-size
η = min (ηmax, √255KFlG2T) for ηmax = min( 18L, 756δK) satisfy
1	U / t . ll2	G√LF	(L + δK)F
KTS Σ ∑S EEkVf (yi,k-ι)k ≤ O ( TS + τκ
k∈[K] t∈[T] i∈St	TS
where we define F := f(x0) - f?.
Proof. By summing over the equations from Lemma 7 for all local steps in one round we obtain
2 X EkVf (yt,k-ι)k2 ≤ A* - Ai,κ + 25*.2。2 .
k=1
By the initialization yit,0 = xt-1, hence Ait,0 = Atj,0 = E[f (xt-1)] for all i, j ∈ St. Furthermore,
by Lemma 4
KStI X Ai,K ≥ E[f (xt)] + δkxt-1 - xtk2 ≥ E[f (xt)] = At+1
|S | i∈St
This means that we can keep unrolling over all rounds, obtaining
TK
2S XXX EkVf (yt,k-1)k2 ≤ Al,0-ATκ +
t=1 k=1 i∈St
255TK2Lη2G2
2S
By noting A/ - ATK = (f(x0) - f ?) - (E[f (xT) - f ?) ≤ F and the choice of the stepsize the
theorem follows.	□
24
Under review as a conference paper at ICLR 2021
G Analysis of MimeMVR (with momentum based variance
reduction)
In this section we see how to use momentum based variance reduction (Cutkosky and Orabona,
2019) to reduce the variance of the updates and improve convergence. It should be noted that MVR
does not exactly fit the Mime framework (BaseAlg) since it requires computing gradients at two
points on the same batch. However, it is straightforward to extend the idea of Mime to MVR as we
will now do. We use MVR as a theoretical justification for why the usual momentum works well in
practice. An interesting future direction would be to adapt the algorithm and analysis of (Cutkosky
and Mehta, 2020), which does fit the framework of Mime.
MimeMVR algorithm. Now, we formally describe the MimeMVR algorithm. In each round t,
we sample clients St such that |St| = S. The server communicates the server parameters xt-1, the
momentum mt-1 and the average gradient across the sampled clients ct-1 defined as
CtT = S X Vfi(Xt-2).
i∈St
(14)
Note that both ct-1 and mt-1 use gradients and parameters from previous rounds (different from
the previous section).
Then each client i ∈ St makes a copy yit,0 = xt-1 and perform K local client updates. In each
local client update k ∈ [K], the client samples a dataset ζit,k and
yit,k = yit,k-1 - ηdti,k , where
dt,k = a(vfi(ytt,k-ι; ζt,k) - Vfi(XtT; ζt,k) + CtT) +(I- a)mt-1	(15)
+ (1 - a)(Vfi(yit,k-1; ζit,k) - Vfi(xt-1; ζit,k)) .
After K such local updates, the server then aggregates the new client parameters as
xt = 1 X yj,κ.	(16)
j∈St
The momentum term is updated at the end of the round for a ≥ 0 as
mt = a(⅛ Pj∈st Vfj(XtT)) + (1- a)mt-1 + (1- a)(S Pj∈st Vfj(XtT)- Vfj(xt-2)).
'-------------------------V-------------------}	'--------------------V----------------}
SGDm	correction
(17)
As we can see, the momentum update of MVR can be broken down into the usual SGDm update,
and a correction. Intuitively, this correction term is very small since fi is smooth and Xt-1 ≈ Xt-2 .
Another way of looking at the update (17) is to note that if all functions are identical i.e. fj = fk
for any j, k, then (17) just becomes the usual gradient descent. Thus MimeMVR tries to maintain
an exponential moving average of only the variance terms, reducing its bias. We refer to (Cutkosky
and Orabona, 2019) for more detailed explanation of MVR.
Momentum variance bound. We compute the variance of the server momentum mt-1. Define
the variance term V t = mt - Vf (Xt-1). Then its expected norm can be bounded as follows.
Lemma 8. For the momentum update (17), given (A1) and (A2), the following holds for any a ∈
[0,1]andVt :=mt - Vf(Xt-1)
2a2G2
EkVtk2 ≤ (1 — a) EIlVt-1k2 + 2δ2 EkXtT — xt-2∣∣2 + 2---
S
25
Under review as a conference paper at ICLR 2021
Proof. Starting from the momentum update (17),
Vt = (1-a)Vt-1
+ (1- a) I S X (Vfj(XtT)- Vfj(xt-2)) - Vf(XtT) + Vf(Xt-2)
j∈St
+ a( S X (Vfj (XtT)-Vf(XtT)
j∈St
Now, the term V t-1 does not have any information from round t and hence is statistically indepen-
dent of the rest of the terms. Further, the rest of the terms have mean 0. Hence, we can separate
out the zero mean noise terms from the V t-1 following Lemma 2 and then the relaxed triangle
inequality Lemma 1 to claim
EkVtk2 ≤ (1 - a)2 EkV t-1k2
2
+ 2(1 - a)2 S X (Vfj(XtT)- Vfj(Xt-2)) - Vf(XtT) + Vf(Xt-2)
j∈St
2
+ 2a2 1 X (Vfj(XtT)-Vf(XtT)
j∈St
2a2 G2
≤ (1 - a)2 EkVt-1k2 + 2(1 - a)2δ2kXt-1 - Xt-2k2 + 2aG
S
The inequality used the Hessian similarity Lemma 3 to bound the second term and the heterogeneity
bound (A1) to bound the last term. Finally, note that (1 - a)2 ≤ (1 - a) ≤ 1 for a ∈ [0,1].	□
Update variance bound. Now we examine the variance of our update in each local step dit,k .
Lemma 9. For the client update (15), given (A1) and (A2), the following holds for any a ∈ [0, 1]
3a2G2
Ekdt,k - Vf (yt,k-ι)k2 ≤ 3EkVt-1k2 + 3δ2 Ekyt,k-ι - Xt-2k2 + -S-.
Proof. Starting from the client update (15), we can rewrite it as
dit,k - Vf (yit,k-1) = (1 - a)V t-1
+ (Vfi(yt,k-ι; Zt,k) - Vfi(Xt-2; Zt,k)) - Vf (yt,k-ι + Vf(Xt-2))
+ a(ɪ X Vfj(Xt-2) -Vf(Xt-2∣ .
j∈St
We can use the relaxed triangle inequality Lemma 1 to claim
Ekdit,k - Vf (yit,k-1)k2 = 3(1 - a)2 EkV t-1k2
+ 3(1 - a)2(Vfi(yit,k-1; ζit,k) - Vfi(Xt-2; ζit,k)) - (Vf(yit,k-1) -Vf(Xt-2))2
2
+ 3a2 1 X Vfj(Xt-2) -Vf(Xt-2)
j∈St
3a2G2
≤ 3 EkV t-1k2 + 3δ2kyt,k-ι - Xt-2k2 + 3aS^.
The last inequality used the Hessian similarity Lemma 3 to bound the second term and the hetero-
geneity bound (A1) to bound the last term. Also, (1 - a)2 ≤ 1 since a ∈ [0,1].	□
26
Under review as a conference paper at ICLR 2021
Distance moved in each step. We show that the distance moved by a client in each step during
the client update can be controlled.
Lemma 10. ForMimeMVR updates (15) with η ≤ 6^ and given (A1) and (A2), the following
holds
∆i,k ≤(1 + ɪ)∆t,k-ι + 18η2Kα2CG + ^K EkVt-1k2 + 6η2K∣∣Vf (y：,1)∣∣2,
KS
where we define ∆ti,k := max Ekyit,k - xt-2 k2 , Ekyit,k - xt-1k2, Ekxt-1 - xt-2 k2 .
Proof. Starting from the MimeMVR update (15) and the relaxed triangle inequality with c = 2K,
Ekyit,k-xt-2k2 = Ekyit,k-1 - ηdit,k - xt-2k2
≤ (l + 2K) Ekyt“I - xt-2k2 + (2K + 1)η2 E∣∣dt,kk2
≤ C) Ekyt,k-1 - xt-2k2 + 6Kη2 E∣Mt,k - Vf(yt,1)k2
2K
+ 6Kη2 EkVf(yit,k-1)k2
≤(1 + 2K + 18Kη2δ2) Ekyt,k-1 - xt-2k2
+ 18Kη2 EkV j『+ 18KnSa2G2 + 6Kη2 EkVf (yt,k-ι)k2.
S
The last inequality used the update variance bound Lemma 9. We can simplify the expression
further since η ≤ 6Kδ implies 18Kn2δ2 ≤ 2K. Similar computations for Ekyt k 一 XtTk2 yield
the lemma.	□
Progress in one step. Now we have all the tools required to compute the progress made in each
round.
Lemma 11. For any step OfMimeMVR with step size η ≤ min( L, 40δκ) and momentum parameter
a = 1536η2δ2K2. Then, given that (A1)-(A3) hold, we have
E[f(yi,k)] + 3ηEkVtk2 + 8ηδ2K(1 + K)K k∆t,k
a	aK
≤ E[f (yt,k-ι)] + 3η EkV t-1k2 + 8ηδ2K (1 + K )K-(k-1)∆t,k-ι
-4 EkVf(yt,k-1)k2 + 1≡η3δ⅛
Proof. The assumption that f is L-smooth implies a quadratic upper bound (10).
f (yt,k) - f (yt,k-ι) ≤ -ηhVf (yi,k-ι), dti,ki + Lη2kdt,kk2
=-2kVf(yt,k-1)k2 + Lη22z^kdt,kk2 + 2kdt,k - Vf(yt,k-1)k2.
The second equality used the fact that for any a, b, -2ab = (a - b)2 - a2 - b2 . The second term
can be removed since η ≤ L. Taking expectation on both sides and using the update variance bound
Lemma 9,
Ef(yt,k) - Ef(yt,k-ι) ≤ -η EkVf(yt,k-1)k2 + 3ηa2G
2	2S
+ 3η EkVt-1k2 + 3η2δ2 Ekyt,k-ι - xt-2k2
27
Under review as a conference paper at ICLR 2021
Multiplying the momentum variance bound Lemma 8 by 3η, we have
3η EkV tk2 ≤ 3η EkV t-1k2 + 6ηδ2 EkxtT- xt-2k2 + 6ηaG2 - 3η EkV t-1『.
aa	a	S
We will also multiply the distance bound Lemma 10 by 8nδa K(1 + K)K k. Note that for any
K ≥ 1 and k ∈ [K], We have 1 ≤(1 + K )K k ≤ 8. Then We get
斗(1 + K厂∆i,k ≤ 粤(1 + I;"-*‘一
+ 1152η3δ2K2aG2 + 1152η3δ2K2 EkVt-1『+ 岬"K ∣∣Vf(yt,k-i)∣∣2 ,
Sa	a	,
where recall that we defined ∆it,k := max Ekyit,k - xt-2k2 , Ekyit,k - xt-1 k2, Ekxt-1 - xt-2 k2 .
Combining the three inequalities together, we get
E f(yt,k)+? EkV tk2+8nδ2K(1+K)Kk ∆t,k
a	aK
≤ Ef(yt,k-ι) + 3η EkVt-1k2 + 8ηδ2K [1 + KK)K (k 1)∆t,k-ι
a	aK
+(384η3aδK - 2) EkVf (yt,k-ι)k2
+ (∏52η2δ2Κ2 + 6+3α ) aηSG2
+ (3n + 1^2K2 — 3η) EkVt-1k2
+
3	6	8	2t
2 + a — a)ηδ △1
Note that 1152,**2 = 3n since We defined a = 1536η2δ2K2. Further, a ≤ 1 when defined this
way since we assumed η ≤ 40⅛K. Similarly, the definition of a implies that1仞了*? = 4. Thus,
we can simplify the above expression as
E f(yt,k)+3n EkV tk2+8nδ2K(1+K)K k ∆t,k
a	aK
≤ Ef(yt,k-ι) + 3η EkVt-1k2 + 8ηδ2K (1 + K)K (k 1)∆i,k-1
a	aK
η E t 2 l 11136η3δ2K2G2
-4 EkVf(yi,k-1)k +	S	.
This proves the lemma.	□
Progress in one round. Let us sum over all the steps within a round to compute the progress made
in a full round.
Lemma 12. For any round ofMimeMVR with step size η ≤ min( L, 401K) and momentum param-
eter a = 1536η2δ2K2. Then, given that (A1)-(A3) hold, we have
ɪ X	EkVf(yt,k-1)k2 ≤ Φt-1- Φt +1113*k2G2 ,
4K S	S
k∈[K],j∈St
where we define the sequence
Φt := ɪ E[f (xt)] + 3ηK EkVtk2 + 8ηδ2 Ekxt- xt-1k2.
Ka	a
28
Under review as a conference paper at ICLR 2021
Proof. We start by summing Lemma 11 over the client updates
4 X EkVf(yt,k-ι)k2 ≤ E[f(yt,0)]
k∈[K]
+ 3ηK EkVt-iif + 8ηδ2K
a
a
2K
1 + K) 40
-E[f(yt,K)] - ¥ EkVtk2 + 8ηδ2K$,k
+
11136η3δ2K3G2
S
Recall that we defined ∆it,k := max Ekyit,k - xt-2 k2 , Ekyit,k - xt-1 k2, Ekxt-1 - xt-2k2 . Be-
cause yit,0 = xt-1, we can simplify
E[f (yt,0)] + 8ηδ2K∆t,0 ≤ E[f (xt-1)] + 8ηδ2K EkxtT- xt-2k2
Then by the averaging Lemma 4, we have
S X E[f(yj,K)] + 8ηδ2K j ≥ S X E[f (yj,K)] + EkxT- yj,Kk2
j∈St	j∈S
≥ E[f (xt)] + 8ηδ-K EkxtT- xtk2.
a
So by averaging our recursion over the sampled clients, and diving our summation over the updates
by K, we get
4KS	X	EkVf(yt,k-ι)k
k∈[K],j∈St
2 ≤ K E[f(xτ
)] + — EkVt-1k2 + 8ηδ2 EkxtT- xt-2k2
1
-K
I-
{z
=Φt-1
E[f (xt)] + 3ηK EkVtk2 + 8ηδ2 Ekxt- XtTk2
a
^{^^^^^^^^™
=Φt
a
a
}
a
}
+
11136η3δ2K2G2
S
□
Theorem IV (non-convex convergence of MimeMVR). Let us run MimeMVR with step size η ≤
min
1 FS
15K ∖τδ2G2
1/3 ι
and momentum parameter a = 1536η2δ2K2. Then, given that
(A1)-(A3) hold, we have
KST XX X EkVf(yt,k-ι)k2 ≤O
t∈[T] k∈[K] j∈St
where we define F := f(x0) - f?.
1 + δ)G2F )2/3+ (L + δK )F
KT
Proof. Unroll the one round progress Lemma 12 and average over T rounds to get
KST XX X Ekf(yt,k-ι)k2 ≤
t∈[T] k∈[K] j∈St
4(Φ0 — φT)+ 11136η2δ2K2G2
ηKT
S
≤
ηKT
Our choice of step size now yields the desired rate.
4(f(x0) - f?)+ 11136η2δ2K2G2
S
□
29