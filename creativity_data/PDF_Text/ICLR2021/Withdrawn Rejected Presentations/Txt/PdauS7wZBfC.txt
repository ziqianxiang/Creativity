Under review as a conference paper at ICLR 2021
Predictive Coding Approximates Backprop
along Arbitrary Computation Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Backpropagation of error (backprop) is a powerful algorithm for training machine
learning architectures through end-to-end differentiation. Recently it has been
shown that backprop in multilayer-perceptrons (MLPs) can be approximated using
predictive coding, a biologically-plausible process theory of cortical computation
which relies solely on local and Hebbian updates. The power of backprop, however,
lies not in its instantiation in MLPs, but rather in the concept of automatic differen-
tiation which allows for the optimisation of any differentiable program expressed
as a computation graph. Here, we demonstrate that predictive coding converges
asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary
computation graphs using only local learning rules. We apply this result to develop
a straightforward strategy to translate core machine learning architectures into
their predictive coding equivalents. We construct predictive coding CNNs, RNNs,
and the more complex LSTMs, which include a non-layer-like branching internal
graph structure and multiplicative interactions. Our models perform equivalently
to backprop on challenging machine learning benchmarks, while utilising only
local and (mostly) Hebbian plasticity. Our method raises the potential that standard
machine learning algorithms could in principle be directly implemented in neural
circuitry, and may also contribute to the development of completely distributed
neuromorphic architectures.
1	Introduction
Deep learning has seen stunning successes in the last decade in computer vision (Krizhevsky et al.,
2012; Szegedy et al., 2015), natural language processing and translation (Vaswani et al., 2017;
Radford et al., 2019; Kaplan et al., 2020), and computer game playing (Mnih et al., 2015; Silver
et al., 2017; Schrittwieser et al., 2019; Vinyals et al., 2019). While there is a great variety of
architectures and models, they are all trained by gradient descent using gradients computed by
automatic differentiation (AD). The key insight of AD is that it suffices to define a forward model
which maps inputs to predictions according to some parameters. Then, using the chain rule of calculus,
it is possible, as long as every operation of the forward model is differentiable, to differentiate back
through the computation graph of the model so as to compute the sensitivity of every parameter in
the model to the error at the output, and thus adjust every single parameter to best minimize the
total loss. Early models were typically simple artificial neural networks where the computation
graph is simply a composition of matrix multiplications and elementwise nonlinearities, and for
which the implementation of automatic differentation has become known as ‘backpropagation’ (or
’backprop’). However, automatic differentiation allows for substantially more complicated graphs to
be differentiated through, up to, and including, arbitrary programs (Griewank et al., 1989; Baydin
et al., 2017; Paszke et al., 2017; Revels et al., 2016; Innes et al., 2019; Werbos, 1982; Rumelhart
and Zipser, 1985; Linnainmaa, 1970). In recent years this has enabled the differentiation through
differential equation solvers (Chen et al., 2018; Tzen and Raginsky, 2019; Rackauckas et al., 2019),
physics engines (Degrave et al., 2019; Heiden et al., 2019), raytracers (Pal, 2019), and planning
algorithms (Amos and Yarats, 2019; Okada et al., 2017). These advances allow the straightforward
training of models which intrinsically embody complex processes and which can encode significantly
more prior knowledge and structure about a given problem domain than previously possible.
Modern deep learning has also been closely intertwined with neuroscience (Hassabis et al., 2017;
Hawkins and Blakeslee, 2007; Richards et al., 2019). The backpropagation algorithm itself arose
1
Under review as a conference paper at ICLR 2021
as a technique for training multi-layer perceptrons - simple hierarchical models of neurons inspired
by the brain (Werbos, 1982). Despite this origin, and its empirical successes, a consensus has
emerged that the brain cannot directly implement backprop, since to do so would require biologically
implausible connection rules (Crick, 1989). There are two principal problems. Firstly, backprop in
the brain appears to require non-local information (since the activity of any specific neuron affects all
subsequent neurons down to the final output neuron). It is difficult to see how this information could
be transmitted ’backwards’ throughout the brain with the required fidelity without precise connectivity
constraints. The second problem - the ‘weight transport problem’ is that backprop through MLP style
networks requires identical forward and backwards weights. In recent years, however, a succession
of models have been introduced which claim to implement backprop in MLP-style models using only
biologically plausible connectivity schemes, and Hebbian learning rules (Liao et al., 2016; Guerguiev
et al., 2017; Sacramento et al., 2018; Bengio and Fischer, 2015; Bengio et al., 2017; Ororbia et al.,
2020; Whittington and Bogacz, 2019). Of particular significance is Whittington and Bogacz (2017)
who show that predictive coding networks - a type of biologically plausible network which learn
through a hierarchical process of prediction error minimization - are mathematically equivalent to
backprop in MLP models. In this paper we extend this work, showing that predictive coding can not
only approximate backprop in MLPs, but can approximate automatic differentiation along arbitrary
computation graphs. This means that in theory there exist potentially biologically plausible algorithms
for differentiating through arbitrary programs, utilizing only local connectivity. Moreover, in a class
of models which we call parameter-linear, which includes many current machine learning models,
the required update rules are Hebbian, raising the possibility that a wide range of current machine
learning architectures may be faithfully implemented in the brain, or in neuromorphic hardware.
In this paper we provide two main contributions. (i) We show that predictive coding converges
to automatic differentiation across arbitrary computation graphs. (ii) We showcase this result by
implementing three core machine learning architectures (CNNs, RNNs, and LSTMs) in a predictive
coding framework which utilises only local learning rules and mostly Hebbian plasticity.
2	Predictive Coding on Arb itrary Computation Graphs
Figure 1: Top: Backpropagation on a chain. Backprop proceeds backwards sequentially and explicitly
computes the gradient at each step on the chain. Bottom: Predictive coding on a chain. Predictions,
and prediction errors are updated in parallel using only local information.
Predictive coding is an influential theory of cortical function in theoretical and computational
neuroscience. Central to the theory is the idea that the core function of the brain is to minimize
prediction errors between what is expected to happen and what actually happens. Predictive coding
2
Under review as a conference paper at ICLR 2021
views the brain as composed of multiple hierarchical layers which predict the activities of the layers
below. Unpredicted activity is registered as prediction error which is then transmitted upwards for a
higher layer to process. Over time, synaptic connections are adjusted so that the system improves at
minimizing prediction error. Predictive coding possesses a wealth of empirical support (Friston, 2003;
2005; Bogacz, 2017; Whittington and Bogacz, 2019) and offers a single mechanism that accounts for
diverse perceptual phenomena such as repetition-suppression (Auksztulewicz and Friston, 2016), end-
stopping (Rao and Ballard, 1999), bistable perception (Hohwy et al., 2008; Weilnhammer et al., 2017)
and illusory motions (Lotter et al., 2016; Watanabe et al., 2018), and even attentional modulation
of neural activity (Feldman and Friston, 2010; Kanai et al., 2015). Moreover, the central role of
top-down predictions is consistent with the ubiquity, and importance of, top-down diffuse connections
between cortical areas. Predictive coding is consistent with many known aspects of neurophysiology,
and has been translated into biologically plausible process theories which define candidate cortical
microcircuits which can implement the algorithm. (Spratling, 2008; Bastos et al., 2012; Kanai et al.,
2015; Shipp, 2016).
In previous work, predictive coding has always been conceptualised as operating on hierarchies
of layers (Bogacz, 2017; Whittington and Bogacz, 2017). Here we present a generalized form of
predictive coding applied to arbitrary computation graphs. A computation graph G = {E, V} is a
directed acyclic graph (DAG) which can represent the computational flow of essentially any program
or computable function as a composition of elementary functions. Each edge ei ∈ E of the graph
corresponds to an intermediate step - the application of an elementary function - while each vertex
vi ∈ V is an intermediate variable computed by applying the functions of the edges to the values of
their originating vertices. In this paper, vi denotes the vector of activations within a layer and we
denote the set of all vertices as {vi}. Effectively, computation flows ’forward’ from parent nodes to
all their children through the edge functions until the leaf nodes give the final output of the program
as a whole (see Figure 1 and 2 for an example). Given a target T and a loss function L = g(T, vout),
the graph’s output can be evaluated and, and if every edge function is differentiable, automatic
differentiation can be performed on the computation graph.
Predictive coding can be derived elegantly as a variational inference algorithm under a hierarchical
Gaussian generative model (Friston, 2005; Buckley et al., 2017). We extend this approach to arbitrary
computation graphs in a supervised setting by defining the inference problem to be solved as that of in-
ferring the vertex value vi of each node in the graph given fixed start nodes v0 (the data), and end nodes
vN (the targets). We define a generative model which parametrises the value of each vertex given the
feedforward prediction of its parents, p({vi}) = p(v0 . . . vN) = QiN p(vi|P(vi)) 1, and a factorised,
variational posterior Q({vi}|v0,vN) = Q(v1 . . .vN-1|v0,vN) = QiN Q(vi|P(vi),C(vi)), where
P(vi) denotes the set of parents and C(vi) denotes the set of children of a given node vi. From this,
we can define a suitable objective functional, the variational free-energy F (VFE), which acts as an
upper bound on the divergence between the true and variational posteriors.
F = KL[(Q(v1 . . . vN-1|v0,vN)kp(v0 . . . vN)] ≥ KL[(Q(v1 . . . vN-1)|v0, vN)kp(v1 . . . vN-1 |v0, vN)]
N
≈	iT i
i=0
(1)
Under Gaussian assumptions for the generative model p({vi}) = QN N(vi； Vi, ∑i), and the vari-
ational posterior Q({vi}) = QNN(vi), where the ‘predictions, Vi = f (P(Vi); θi) are defined as
the feedforward value of the vertex produced by running the graph forward, and all the precisions,
or inverse variances, Σi-1 are fixed at the identity, we can write F as simply a sum of prediction
errors (see Appendix D or (Friston, 2003; Bogacz, 2017; Buckley et al., 2017) for full derivations),
with the prediction errors defined as Ei = Vi - Vi. These prediction errors play a core role in the
framework and, in the biological process theories (Friston, 2005; Bastos et al., 2012), are generally
considered to be represented by a distinct population of ‘error units’. Since F is an upper bound on
the divergence between true and approximate posteriors, by minimizing F , we reduce this divergence,
thus improving the quality of the variational posterior and approximating exact Bayesian inference.
Predictive coding minimizes F by employing the Cauchy method of steepest descent to set the
1This includes the prior p(v0), which simply has no parents.
3
Under review as a conference paper at ICLR 2021
dynamics of the vertex variables vi as a gradient descent directly on F (Bogacz, 2017).
dVi	∂F
—=——
dt ∂Vi
∂Vj
-j∈X「菽
(2)
The dynamics of the parameters of the edge functions θ such that Vi = f (P(Vi); θ), can also be
derived as a gradient descent on F . Importantly these dynamics require only information (the current
vertex value, prediction error, and prediction errors of child vertices) locally available at the vertex.
dθi _ ∂F _ ∂Vi
~di = ∂θi = ei ∂θi
(3)
To run generalized predictive coding in practice on a given computation graph G = {E, V}, we
augment the graph with error units ∈ E to obtain an augumented computation graph G = {E, V, E}.
The predictive coding algorithm then operates in two phases - a feedforward sweep and a backwards
iteration phase. In the feedforward sweep, the augmented computation graph is run forward to obtain
the set of predictions {v^i}, and prediction errors {g} = {vi - &} for every vertex. Following
Whittington and Bogacz (2017), to achieve exact equivalence with the backprop gradients computed
on the original computation graph, We initialize Vi = Vi in the initial feedforward sweep so that the
output error computed by the predictive coding network and the original graph are identical.
In the backwards iteration phase, the vertex activities {Vi} and prediction errors {i} are updated
with Equation 2 for all vertices in parallel until the vertex values converge to a minimum of F. After
convergence the parameters are updated according to Equation 3. Note we also assume, following
Whittington and Bogacz (2017), that the predictions at each layer are fixed at the values assigned
during the feedforward pass throughout the optimisation of the V s. We call this the fixed-prediction
assumption. In effect, by removing the coupling between the vertex activities of the parents and the
prediction at the child, this assumption separates the global optimisation problem into a local one for
each vertex. We implement these dynamics with a simple forward Euler integration scheme so that the
update rule for the vertices became v；+1 J Vt - η% where η is the step-size parameter. Importantly,
if the edge function linearly combines the activities and the parameters followed by an elementwise
nonlinearity - a condition which we call 'parameter-linear, 一 then both the update rule for the vertices
(Equation 2) and the parameters (Equation 3) become Hebbian. Specifically, the update rules for the
vertices and weights become 箸=Ei - Pj e7∙ f(θjVjM and 的=eif0(θiVi)ViT, respectively.
2.1 Approximation to Backprop
Here we show that at the equilibrium of the dynamics, the prediction errors 球 converge to the correct
backpropagated gradients ∂L, and consequently the parameter updates (Equation 3) become precisely
∂vi
those of a backprop trained network. Standard backprop works by computing the gradient of a vertex
as the sum of the gradients of the child vertices. Beginning with the gradient of the output vertex
∂∂L, it recursively computes the gradients of vertices deeper in the graph by the chain rule:
∂L _ X ∂L ∂Vj
∂Vi	∂Vj ∂Vi
i j =C (vi) j i
(4)
In comparison, in our predictive coding framework, at the equilibrium point (喑=0) the prediction
errors 球 become,
球
* dVi
j∂V j∂ ∂Vj
j∈C(vi)
(5)
Importantly, this means that the equilibrium value of the prediction error at a given vertex (Equation
5) satisfies the same recursive structure as the chain rule of backprop (Equation 4). Since this
relationship is recursive, all that is needed for the prediction errors throughout the graph to converge
to the backpropagated derivatives is for the prediction errors at the final layer to be equal to the output
gradient: EL = ∂L. To see this explicitly, consider a mean-squared-error loss function 2. at the
2While the mean-squared-error loss function fits most nicely with the Gaussian generative model, other loss
functions can be used in practice. If the loss function can be represented as a log probability distribution, then
the generative model can be amended to simply set the output distribution to that distribution. If not, then there
is no fully consistent generative model (although all nodes except the output remain Gaussian), but the algorithm
will still work in practice. See (Figure 6) in Appendix A for results for CNNs trained with a crossentropy loss.
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Generalized Predictive Coding
■ -¼	, i ʌ .	. <r> r -¾τ- ɪ T t	.	1 z-ι	.	z-ι 1	/7 r ττn Er c、 ∙ r∙	ι
Data: Dataset D = {X, L}, Augmented Computation Graph G = {E, V, E}, inference learning
rate ηv, weight learning rate ηθ
begin
/* For each minibatch in the dataset	*/
for (x, L) ∈ Ddo
/* Fix start of graph to inputs	*/
v^0 J X
/* Forward pass to compute predictions	*/
for Vi ∈ V do
L Vi J f({P(Vi); θ)
/* Compute output error	*/
EL J L — VL
/* Begin backwards iteration phase of the descent on the
free energy	*/
while not converged do
t> /	、一片 1
for (Vi , Ei) ∈ G do
/* Compute prediction errors	*/
Ei J Vi 一 Vi
/* Update the vertex values	*/
Vt+1 J Vt + ηvdFt
_	i
/* Update weights at equilibrium	*/
for θi ∈ E do
L θt+1 J ” ηθ 薪
output layer L = 11 (T 一 VLy with Tasa vector of targets, and defining EL = T 一 Vl. We then
consider the equilibrium value of the prediction error unit at a penultimate vertex EL-1. By Equation
5, we can see that at equilibrium,
EL-I = EL
∂Vl
∂vl-i
(T - vl ) ∂dΓ?
since, (T 一 VL) = ∂L, We can then write,
*	= ∂L ∂Vl = ∂L
ELT	∂Vl ∂vl-1	∂vl-1
(6)
Thus the prediction errors of the penultimate nodes converge to the correct backpropagated gradient.
Furthermore, recursing through the graph from children to parents allows the correct gradients to be
computed3. Thus, by induction, we have shown that the fixed points of the prediction errors of the
global optimization correspond exactly to the backpropagated gradients. Intuitively, if we imagine the
computation-graph as a chain and the error as ’tension’ in the chain, backprop loads all the tension at
the end (the output) and then systematically propagates it backwards. Predictive coding, however,
spreads the tension throughout the entire chain until it reaches an equilibrium where the amount of
tension at each link is precisely the backpropagated gradient. The full algorithm for training the
predictive coding network is explicitly set out in Algorithm 1. Inference is just a forward pass through
the network, and is identical to the corresponding ANN.
3Some subtlety is needed here since vL-1 may have many children which each contribute to the loss.
However, these different paths sum together at the node vL-1, thus propagating the correct gradient backwards.
5
Under review as a conference paper at ICLR 2021
By a similar argument, it is apparent that the dynamics of the parameters θi as a gradient descent on
F also exactly match the backpropagated parameter gradients.
dθi _dF _ * de*
-：-=_ - = e； _ -
dt	dθi	dθi
dL dVi	dL
=---------=-----
dvi dθi	dθi
(7)
Which follows from the fact that e* = dL and that 监=dvi.
i	dvi	dθ	dθi
3 Related Work
A number of recent works have tried to provide biologically plausible approximations to backprop.
The requirement of symmetry between the forwards and backwards weights has been questioned by
Lillicrap et al. (2016) who show that random fixed feedback weights suffice for effective learning.
Recent additional work has shown that learning the backwards weights also helps (Amit, 2019;
Akrout et al., 2019). Several schemes have also been proposed to approximate backprop using only
local learning rules and/or Hebbian connectivity. These include target-prop (Lee et al., 2015) which
approximate the backward gradients with trained inverse functions, but which fails to asymptotically
compute the exact backprop gradients, and contrastive Hebbian (Seung, 2003; Scellier and Bengio,
2017; Scellier et al., 2018) approaches which do exactly approximate backprop, but which require
two separate learning phases and the storing of information across successive phases. There are also
dendritic error theories (Guerguiev et al., 2017; Sacramento et al., 2018) which are computationally
similar to predictive coding (Whittington and Bogacz, 2019; Lillicrap et al., 2020). Whittington
and Bogacz (2017) showed that predictive coding can approximate backprop in MLP models, and
demonstrated comparable performance on MNIST. We advance upon this work by extending the
proof to arbitrary computation graphs, enabling the design of predictive coding variants of a range
of standard machine learning architectures, which we show perform comparably to backprop on
considerably more difficult tasks than MNIST. Our algorithm evinces asymptotic (and in practice
rapid) convergence to the exact backprop gradients, does not require separate learning phases, and
utilises only local information and largely Hebbian plasticity.
4 Results
4.1	Numerical Results
To demonstrate the correctness of our derivation and empirical convergence to the true gradients,
we present a numerical test in the simple scalar case, where we use predictive coding to derive the
gradients of an arbitrary, highly nonlinear test function VL = tan(√θv0) + sin(v2) where θ is an
arbitrary parameter. For our tests, we set v0 to 5 and θ to 2. The computation graph for this function
is presented in Figure 2. Although simple, this is a good test of predictive coding because the function
is highly nonlinear, and its computation graph does not follow a simple layer structure but includes
some branching. An arbitrary target of T = 3 was set at the output and the gradient of the loss
L = (VL - T)2 with respect to the input V0 was computed by predictive coding. We show (Figure 2)
that the predictive coding optimisation rapidly converges to the exact numerical gradients computed
by automatic differentiation, and that moreover this optimization is very robust and can handle even
exceptionally high learning rates (up to 0.5) without divergence.
In summary, we have shown and numerically verified that at the equilibrium point of the global
free-energy F on an arbitrary computation graph, the error units exactly equal the backpropagated
gradients, and that this descent requires only local connectivity, does not require a separate phases or
a sequential backwards sweep, and in the case of parameter-linear functions, requires only Hebbian
plasticity. Our results provide a straightforward recipe for the direct implementation of predictive
coding algorithms to approximate certain computation graphs, such as those found in common
machine learning algorithms, in a potentially biologically plausible manner. Next, we showcase
this capability by developing predictive coding variants of core machine learning architectures -
convolutional neural networks (CNNs) recurrent neural networks (RNNs) and LSTMs (Hochreiter
and Schmidhuber, 1997), and show performance comparable with backprop on tasks substantially
more challenging than MNIST.
6
Under review as a conference paper at ICLR 2021
Learning Rate Comparison	Log Prediction Error
30
25
20
15
10
Figure 2: Top: The computation graph of the nonlinear test function VL = tan(√θv0) + sin(v2).
Bottom: graphs of the log mean divergence from the true gradient and the divergence for different
learning rates. Convergence to the exact gradients is exponential and robust to high learning rates.
O 100	200	300	400	500
Variational Iteration
0	100	200	300	400	500
Variational Iteration
4.2	Predictive Coding CNN, RNN, and LSTM
—∙ Predictive cc
—∙ Backprop tre
—Predictive«
—BaCkPraPtel
30
S
4。0口。。<
15
——∙ Backpnop train accuracy
----- Predictive coding test accuracy
---- Backprop test accuracy
Figure 3: Training and test accuracy plots for the Predictive Coding and Backprop CNN on
SVHN,CiFAR10, and CiFAR10 dataest over 5 seeds. Performance is largely indistinguishable.
Due to the need to iterate the vs until convergence, the predictive coding network had roughly a 100x
greater computational cost than the backprop network.
First, we constructed predictive coding CNN models (see Appendix B for full implementation details).
in the predictive coding CNN, each filter kernel was augmented with ‘error maps’ which measured
the difference between the forward convolutional predictions and the backwards messages. Our
CNN was composed of a convolutional layer, followed by a max-pooling layer, then two further
convolutional layers followed by 3 fully-connected layers. We compared our predictive coding
CNN to a backprop-trained CNN with the exact same architecture and hyperparameters. We tested
our models on three image classification datasets significantly more challenging than MNIST —
SVHN, CiFAR10, and CiFAR100. SVHN is a digit recognition task like MNiST, but has more
naturalistic backgrounds, is in colour with continuously varying inputs and contains distractor digits.
CiFAR10 and CiFAR100 are large image datasets composed of RGB 32x32 images. CiFAR10 has
7
Under review as a conference paper at ICLR 2021
10 classes of image, while CIFAR100 is substantially more challenging with 100 possible classes. In
general (Figure 3), performance was identical between the predictive coding and backprop CNNs
and comparable to the standard performance of basic CNN models on these datasets, Moreover, the
predictive coding gradient remained close to the true numerical gradient throughout training.
Figure 4: Test accuracy plots for the Predictive Coding and Backprop RNN and LSTM on their
respective tasks, averaged over 5 seeds. Performance is again indistinguishable from backprop.
We also constructed predictive coding RNN and LSTM models, thus demonstrating the ability of
predictive coding to scale to non-parameter-linear, branching, computation graphs. The RNN was
trained on a character-level name classification task, while the LSTM was trained on a next-character
prediction task on the full works of Shakespeare. Full implementation details can be found in Appen-
dices B and C. LSTMs and RNNs are recurrent networks which are trained through backpropagation
through time (BPTT). BPTT simply unrolls the network through time and backpropagates through
the unrolled graph. Analogously we trained the predictive coding RNN and LSTM by applying
predictive coding to the unrolled computation graph. The depth of the unrolled graph depends heavily
on the sequence length, and in our tasks using a sequence length of 100 we still found that predictive
coding evinced rapid convergence to the correct numerical gradient, and that the performance was
approximately identical to the equivalent backprop-trained networks (Figure 3), thus showing that the
algorithm is scalable even to very deep computation graphs.
5 Discussion
We have shown that predictive coding provides a local and potentially biologically plausible approxi-
mation to backprop on arbitrary, deep, and branching computation graphs. Moreover, convergence to
the exact backprop gradients is rapid and robust, even in extremely deep graphs such as the unrolled
LSTM. Our algorithm is fully parallelizable, does not require separate phases, and can produce
equivalent performance to backprop in core machine-learning architectures. These results broaden
the horizon of local approximations to backprop by demonstrating that they can be implemented on
arbitrary computation graphs, not only simple MLP architectures. Our work prescribes a straight-
forward recipe for backpropagating through any computation graph with predictive coding using
only local learning rules. In the future, this process could potentially be made fully automatic and
translated onto neuromorphic hardware. Our results also raise the possibility that the brain may
implement machine-learning type architectures much more directly than often considered. Many
lines of work suggest a close correspondence between the representations and activations of CNNs
and activity in higher visual areas (Yamins et al., 2014; Tacchetti et al., 2017; Eickenberg et al., 2017;
Khaligh-Razavi and Kriegeskorte, 2014; Lindsay, 2020), for instance, and this similarity may be
found to extend to other machine learning architectures.
It is important to note that predictive coding, as advanced here, still retains some biologically
implausible features. Although using only local and Hebbian updates, the predictive coding algorithm
still requires identical forward and backwards weights, as well as mandating a very precise one-
to-one connectivity structure between value neurons vi and error neurons i . However, recent
work (Millidge et al., 2020) has begun to show that these implausibilities can be relaxed using
learnable backwards weights instead of requiring weight symmetry, and allowing for learnable
dense connectivity between value and error neurons, without harm to performance in simple MLP
settings. An additional limitation to the biological plausibility of our method is the fixed-prediction
assumption, which requires that the feedforward pass values be somehow stored during the backwards
8
Under review as a conference paper at ICLR 2021
iteration phase. In biological neurons this could potentially be implemented by utilizing synaptic
mechanisms for maintaining information over short periods, such as eligibility traces, or alternatively
through synchronised phase locking (Buzsaki, 2006). Alternatively, it is important to note that
this fixed-prediction assumption is only required for exact convergence to backprop, and predictive
coding networks have been shown to be able to attain strong discriminative classification performance
without it (Whittington and Bogacz, 2017).
Although we have implemented three core machine learning architectures as predictive coding
networks, we have nevertheless focused on relatively small and straightforward networks and thus
both our backprop and predictive coding networks perform below the state of the art on the presented
tasks. This is primarily because our focus was on demonstrating the theoretical convergence between
the two algorithms. Nevertheless, we believe that due to the generality of our theoretical results,
’scaling up’ the existing architectures to implement performance-matched predictive coding versions
of more advanced machine learning architectures such as resnets (He et al., 2016), GANs (Goodfellow
et al., 2014), and transformers (Vaswani et al., 2017) should be relatively straightforward.
In terms of computational cost, one inference iteration in the predictive coding network is about as
costly as a backprop backwards pass. Thus, due to using 100-200 iterations for full convergence, our
algorithm is substantially more expensive than backprop which limits the scalability of our method.
However, this serial cost is misleading when talking about highly parallel neural architectures. In the
brain, neurons cannot wait for a sequential forward and backward sweep. By phrasing our algorithm
as a global descent, our algorithm is fully parallel across layers. There is no waiting and no phases
to be coordinated. Each neuron need only respond to its local driving inputs and downwards error
signals. We believe that this local and parallelizable property of our algorithm may engender the
possibility of substantially more efficient implementations on neuromorphic hardware (Furber et al.,
2014; Merolla et al., 2014; Davies et al., 2018), which may ameliorate much of the computational
overhead compared to backprop. Future work could also examine whether our method is more
capable than backprop of handling the continuously varying inputs the brain is presented with in
practice, rather than the artificial paradigm of being presented with a series of i.i.d. datapoints.
Our work also reveals a close connection between backprop and inference. Namely, the recursive
computation of gradients is effectively a by-product of a variational-inference algorithm which
infers the values of the vertices of the computation graph under a hierarchical Gaussian generative
model. While the deep connections between stochastic gradient descent and inference in terms
of Kalman filtering (Ruck et al., 1992; Ollivier, 2019) or MCMC sampling methods (Chen et al.,
2014; Mandt et al., 2017) is known, the relation between recursive gradient computation itself and
variational inference is underexplored except in the case of a single layer (Amari, 1995). Our method
can provide a principled generalisation of backprop through the inverse-variance Σ-1 parameters
of the Gaussian generative model. These parameters weight the relative contribution of different
factors to the overall gradient by their uncertainty, thus naturally handling the case of backprop with
differentially noisy inputs. Moreover, the Σ-1 parameters can be learnt as a gradient descent on
F: d∑i = - ddF = -Σ-1geTΣ-1 - Σ-1. This specific generalisation is afforded by the Gaussian
form of the generative model, however, and other generative models may yield novel optimisation
algorithms able to quantify and handle uncertainties throughout the entire computational graph.
References
Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, and Douglas B Tweed. Deep
learning without weight transport. In Advances in Neural Information Processing Systems, pages
974-982, 2019.
Shun-Ichi Amari. Information geometry of the em and em algorithms for neural networks. Neural
networks, 8(9):1379-1408, 1995.
Yali Amit. Deep learning with asymmetric connections and hebbian updates. Frontiers in computa-
tional neuroscience, 13:18, 2019.
Brandon Amos and Denis Yarats. The differentiable cross-entropy method. arXiv preprint
arXiv:1909.12830, 2019.
Ryszard Auksztulewicz and Karl Friston. Repetition suppression and its contextual determinants in
predictive coding. cortex, 80:125-140, 2016.
9
Under review as a conference paper at ICLR 2021
Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston.
Canonical microcircuits for predictive coding. Neuron, 76(4):695-711, 2012.
AtIhm GuneS Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. The Journal of Machine Learning
Research,18(1):5595-5637, 2017.
Matthew James Beal et al. Variational algorithms for approximate Bayesian inference. university of
London London, 2003.
Yoshua Bengio and Asja Fischer. Early inference in energy-based models approximates back-
propagation. arXiv preprint arXiv:1510.02777, 2015.
Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and Yuhuai Wu. Stdp-compatible
approximation of backpropagation in an energy-based model. NeUraI computation, 29(3):555-577,
2017.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association, 112(518):859-877, 2017.
Rafal Bogacz. A tutorial on the free-energy framework for modelling perception and learning. Journal
of mathematical psychology, 76:198-211, 2017.
Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The free energy
principle for action and perception: A mathematical review. Journal of Mathematical Psychology,
81:55-79, 2017.
Gyorgy Buzsaki. Rhythms of the Brain. Oxford University Press, 2006.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In Advances in neural information processing systems, pages 6571-6583,
2018.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International conference on machine learning, pages 1683-1691, 2014.
Francis Crick. The recent excitement about neural networks. Nature, 337(6203):129-132, 1989.
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha
Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic
manycore processor with on-chip learning. IEEE Micro, 38(1):82-99, 2018.
Jonas Degrave, Michiel Hermans, Joni Dambre, and Francis Wyffels. A differentiable physics engine
for deep learning in robotics. Frontiers in neurorobotics, 13:6, 2019.
Michael Eickenberg, Alexandre Gramfort, Gael Varoquaux, and Bertrand Thirion. Seeing it all:
Convolutional network layers map the function of the human visual system. NeuroImage, 152:
184-194, 2017.
Harriet Feldman and Karl Friston. Attention, uncertainty, and free-energy. Frontiers in human
neuroscience, 4:215, 2010.
Karl Friston. Learning and inference in the brain. Neural Networks, 16(9):1325-1352, 2003.
Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:
Biological sciences, 360(1456):815-836, 2005.
Karl Friston. Hierarchical models in the brain. PLoS computational biology, 4(11), 2008.
Steve B Furber, Francesco Galluppi, Steve Temple, and Luis A Plana. The spinnaker project.
Proceedings of the IEEE, 102(5):652-665, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pages 2672-2680, 2014.
10
Under review as a conference paper at ICLR 2021
Andreas Griewank et al. On automatic differentiation. Mathematical Programming: recent develop-
ments and applications, 6(6):83-107,1989.
Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated
dendrites. Elife, 6:e22901, 2017.
Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick.
Neuroscience-inspired artificial intelligence. Neuron, 95(2):245-258, 2017.
Jeff Hawkins and Sandra Blakeslee. On intelligence: How a new understanding of the brain will lead
to the creation of truly intelligent machines. Macmillan, 2007.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770-778, 2016.
Eric Heiden, David Millard, and Gaurav Sukhatme. Real2sim transfer using differentiable physics.
In Workshop on Closing the Reality Gap in Sim2real Transfer for Robotic Manipulation, 2019.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Jakob Hohwy, Andreas RoePstorff, and Karl Friston. Predictive coding exPlains binocular rivalry:
An ePistemological review. Cognition, 108(3):687-701, 2008.
Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckus, Elliot Saba, Viral B Shah, and Will
Tebbutt. Zygote: A differentiable Programming system to bridge machine learning and scientific
comPuting. arXiv preprint arXiv:1907.07587, 2019.
Ryota Kanai, Yutaka Komura, Stewart ShiPP, and Karl Friston. Cerebral hierarchies: Predictive Pro-
cessing, Precision and the Pulvinar. Philosophical Transactions of the Royal Society B: Biological
Sciences, 370(1668):20140169, 2015.
Jared KaPlan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte. DeeP suPervised, but not unsuPervised,
models may exPlain it cortical rePresentation. PLoS computational biology, 10(11), 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deeP convolu-
tional neural networks. In Advances in neural information processing systems, Pages 1097-1105,
2012.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target ProPagation.
In Joint european conference on machine learning and knowledge discovery in databases, Pages
498-515. SPringer, 2015.
Qianli Liao, Joel Z Leibo, and Tomaso Poggio. How imPortant is weight symmetry in backProPaga-
tion? In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Timothy P LillicraP and Adam Santoro. BackProPagation through time and the brain. Current opinion
in neurobiology, 55:82-89, 2019.
Timothy P LillicraP, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaPtic
feedback weights suPPort error backProPagation for deeP learning. Nature communications, 7(1):
1-10, 2016.
Timothy P LillicraP, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. BackProP-
agation and the brain. Nature Reviews Neuroscience, Pages 1-12, 2020.
Grace Lindsay. Convolutional neural networks as a model of the visual system: Past, Present, and
future. Journal of Cognitive Neuroscience, Pages 1-15, 2020.
11
Under review as a conference paper at ICLR 2021
Seppo Linnainmaa. The representation of the cumulative rounding error of an algorithm as a taylor
expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, pages 6-7,
1970.
William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video
prediction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-
neuron integrated circuit with a scalable communication network and interface. Science, 345
(6197):668-673, 2014.
Beren Millidge, Alexander Tschantz, Anil Seth, and Christopher L Buckley. Relaxing the constraints
on predictive coding models. arXiv preprint arXiv:2010.01047, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Masashi Okada, Luca Rigazio, and Takenobu Aoshima. Path integral networks: End-to-end differen-
tiable optimal control. arXiv preprint arXiv:1706.09597, 2017.
Yann Ollivier. The extended kalman filter is a natural gradient descent in trajectory space. arXiv
preprint arXiv:1901.00696, 2019.
Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online without
backtracking. arXiv preprint arXiv:1507.07680, 2015.
Alexander Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer. Continual learning of recurrent
neural networks by locally aligning distributed representations. IEEE Transactions on Neural
Networks and Learning Systems, 2020.
Avik Pal. Raytracer. jl: A differentiable renderer that supports parameter optimization for scene
reconstruction. arXiv preprint arXiv:1907.07198, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Chris Rackauckas, Mike Innes, Yingbo Ma, Jesse Bettencourt, Lyndon White, and Vaibhav Dixit.
Diffeqflux. jl-a julia library for neural differential equations. arXiv preprint arXiv:1902.02376,
2019.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.
Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation
of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79-87, 1999.
Jarrett Revels, Miles Lubin, and Theodore Papamarkou. Forward-mode automatic differentiation in
julia. arXiv preprint arXiv:1607.07892, 2016.
Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia
Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep
learning framework for neuroscience. Nature neuroscience, 22(11):1761-1770, 2019.
Dennis W. Ruck, Steven K. Rogers, Matthew Kabrisky, Peter S. Maybeck, and Mark E. Oxley.
Comparative analysis of backpropagation and the extended kalman filter for training multilayer
perceptrons. IEEE Transactions on Pattern Analysis & Machine Intelligence, (6):686-691, 1992.
David E Rumelhart and David Zipser. Feature discovery by competitive learning. Cognitive science,
9(1):75-112, 1985.
12
Under review as a conference paper at ICLR 2021
Joao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits
approximate the backpropagation algorithm. In Advances in Neural Information Processing
Systems, pages 8721-8732, 2018.
Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-
based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard, and Yoshua Bengio. Gener-
alization of equilibrium propagation to vector field dynamics. arXiv preprint arXiv:1808.04873,
2018.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.
H Sebastian Seung. Learning in spiking neural networks by reinforcement of stochastic synaptic
transmission. Neuron, 40(6):1063-1073, 2003.
Stewart Shipp. Neural elements for predictive coding. Frontiers in psychology, 7:1792, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354-359, 2017.
Michael W Spratling. Reconciling predictive coding and biased competition models of cortical
function. Frontiers in computational neuroscience, 2:4, 2008.
Jochen J Steil. Backpropagation-decorrelation: online recurrent learning with o (n) complexity. In
2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541),
volume 2, pages 843-848. IEEE, 2004.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015.
Andrea Tacchetti, Leyla Isik, and Tomaso Poggio. Invariant recognition drives neural representations
of action sequences. PLoS computational biology, 13(12):e1005859, 2017.
Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization. arXiv preprint
arXiv:1702.05043, 2017.
Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian
models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998-6008, 2017.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Eiji Watanabe, Akiyoshi Kitaoka, Kiwako Sakamoto, Masaki Yasugi, and Kenta Tanaka. Illusory
motion reproduced by deep neural networks trained for prediction. Frontiers in psychology, 9:345,
2018.
Veith Weilnhammer, Heiner Stuke, Guido Hesselmann, Philipp Sterzer, and Katharina Schmack. A
predictive coding account of bistable perception-a model-based fmri study. PLoS computational
biology, 13(5):e1005536, 2017.
Paul J Werbos. Applications of advances in nonlinear sensitivity analysis. In System modeling and
optimization, pages 762-770. Springer, 1982.
13
Under review as a conference paper at ICLR 2021
James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm
in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5):
1229-1262, 2017.
James CR Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends in
cognitive sciences, 2019.
Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural computation, 1(2):270-280, 1989.
Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J
DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual
cortex. Proceedings of the National Academy of Sciences, 111(23):8619-8624, 2014.
Appendix A:	Predictive Coding CNN Implementation Details
The key concept in a CNN is that of an image convolution, where a small weight matrix is ’slid’ (or
convolved) across an image to produce an output image. Each patch of the output image only depends
on a relatively small patch of the input image. Moreover, the weights of the filter stay the same during
the convolution, so each pixel of the output image is generated using the same weights. The weight
sharing implicit in the convolution operation enforces translational invariance, since different image
patches are all processed with the same weights.
The forward equations of a convolutional layer for a specific output pixel
k=i+f l=j+f
vi,j =	θk,lxi+k,j+l
k=i-f l=j-f
Where vi,j is the (i, j)th element of the output, xi,j is the element of the input image and θk,l is an
weight element of a feature map. To setup a predictive coding CNN, we augment each intermediate
xi and vi with error units i of the same dimension as the output of the convolutional layer.
Predictions V are projected forward using the forward equations. Prediction errors also need to be
transmitted backwards for the architecture to work. To achieve this we must have that prediction errors
are transmitted upwards by a ’backwards convolution’. We thus define the backwards prediction
errors Wj as follows:
i+f	j+f
^i,j = XX 次,j
k=i-f l=j-f
Where e is an error map zero-padded to ensure the correct convolutional output size. Inference in the
predictive coding network then proceeds by updating the intermediate values of each layer as follows:
dvl
五="1+1
Since the CNN is also parameter-linear, weights can be updated using the simple Hebbian rule of the
multiplication of the pre and post synaptic potentials.
dθι
dt
li,jvl-1iT,j
i,j
There is an additional biological implausibility here due to the weight sharing of the CNN. Since the
same weights are copied for each position on the image, the weight updates have contributions from
all aspects of the image simultaneously which violates the locality condition. A simple fix for this,
which makes the network scheme plausible is to simply give each position on the image a filter with
separate weights, thus removing the weight sharing implicit in the CNN. In effect this gives each
patch of pixels a local receptive field with its own set of weights. The performance and scalability
14
Under review as a conference paper at ICLR 2021
Figure 5: Training loss plots for the Predictive Coding and Backprop CNN on SVHN,CIFAR10, and
CIFAR10 dataset over 5 seeds.
of such a locally connected predictive coding architecture would be an interesting avenue for future
work, as this architecture has substantial homologies with the structure of the visual cortex.
In our experiments we used a relatively simple CNN architecture consisting of one convolutional
layer of kernel size 5, and a filter bank of 6 filters. This was followed by a max-pooling layer with a
(2,2) kernel and a further convolutional layer with a (5,5) kernel and filter bank of 16 filters. This was
then followed by three fully connected layers of 200, 150, and 10 (or 100 for CIFAR100) output units.
Each convolutional and fully connected layer used the relu activation function, except the output
layer which was linear. Although this architecture is far smaller than state of the art for convolutional
networks, the primary point of our paper was to demonstrate the equivalence of predictive coding
and backprop. Further work could investigate scaling up predictive coding to more state-of-the-art
architectures.
Our datasets consisted of 32x32 RGB images. We normalised the values of all pixels of each image
to lie between 0 and 1, but otherwise performed no other image preprocessing. We did not use data
augmentation of any kind. We set the weight learning rate for the predictive coding and backprop
networks 0.0001. A minibatch size of 64 was used. These parameters were chosen without any
detailed hyperparameter search and so are likely suboptimal. The magnitude of the gradient updates
was clamped to lie between -50 and 50 in all of our models. This was done to prevent divergences, as
occasionally occurred in the LSTM networks, likely due to exploding gradients.
The predictive coding scheme converged to the exact backprop gradients very precisely within 100
inference iterations using an inference learning rate of 0.1. This gives the predictive coding CNN
approximately a 100x computational overhead compared to backprop. The divergence between
the true and approximate gradients remained approximately constant throughout training, as shown
by Figure 5, which shows the mean divergence for each layer of the CNN over the course of an
example training run on the CIFAR10 dataset. The training loss of the predictive coding and backprop
networks for SVHN, CIFAR10 and CIFAR100 are presented in Figure 4.
While the experiments in the main paper all used the mean-squared-error loss function, it is also
possible to use alternative loss functions. In Figure 6, we show performance of the CNN on CIFAR
and SVHN datasets is also very close to backprop when trained with a multi-class cross-entropy loss
L = i Ti ln vLi . In this case the output layer used a softmax function as its nonlinearity, to ensure
that the logits passed to the cross-entropy loss were valid probabilities. The cross-entropy loss is
also straightforward to fit into the predictive coding framework since the gradient with respect to
the pre-activations of the output is also just the negative prediction error ∂L = T - vl, although
the softmax function itself may be challenging to implement neurally since it is non-local as its’
normalisation coefficient requires of the exponentiated activities of all neurons in a layer. Nevertheless,
this demonstrates that predictive coding can approximate backprop for any given loss function, not
simply mean-square-error.
Appendix B:	Predictive Coding RNN
The computation graph on RNNs is relatively straightforward. We consider only a single layer RNN
here although the architecture can be straightforwardly extended to hierarchically stacked RNNs. An
RNN is similar to a feedforward network except that it possesses an additional hidden state h which
is maintained and updated over time as a function of both the current input x and the previous hidden
15
Under review as a conference paper at ICLR 2021
(c) FC Layer 1
(d) FC Layer 2
Mean divergence between the true numerical and predictive coding backprops over the course of training. In
general, the divergence appeared to follow a largely random walk pattern, and was generally neglible.
Importantly, the divergence did not grow over time throughout training, implying that errors from slightly
incorrect gradients did not appear to compound.
16
Under review as a conference paper at ICLR 2021
05050505
09988776
LQciQ 0。 00
>U23UU<
crossentropy_svhn test accuracies
5
6
0.
0 5 0
8 7 7
0.0.0.
>us3uu<
0	10	20	30	40
Iterations
(a) SVHN test accuracy
0	10	20	30	40
Iterations
SVHN training accuracy
crossentropy-cifar accuracies
10	20	30	40
Iterations
0.575
0.550
0.525
0.500
0.475
0.450
0.425
0	10	20	30	40
Iterations
(b) CIFAR training accuracy
(c) CIFAR test accuracy
Training and test accuracies of the CNN network on the SVHN and CIFAR datasets using the cross-entropy loss.
As can be seen performance remains very close to backprop, thus demonstrating that our predictive coding
algorithm can be used with different loss functions, not just mean-squared-error.
17
Under review as a conference paper at ICLR 2021
state. The output of the network y is a function of h. By considering the RNN at a single timestep we
obtain the following equations.
ht = f(θhht-1 + θxxt)	(8)
yt = g(θyht)
Where f and g are elementwise nonlinear activation functions. And θh , θx , θy are weight matrices
for each specific input. To predict a sequence the RNN simply rolls forward the above equations to
generate new predictions and hidden states at each timestep.
RNNs are typically trained through an algorithm called backpropagation through time (BPTT) which
essentially just unrolls the RNN into a single feedforward computation graph and then performs
backpropagation through this unrolled graph. To train the RNN using predictive coding we take the
same approach and simply apply predictive coding to the unrolled graph.
It is important to note that this is an additional aspect of biological implausibility that we do not
address in this paper. BPTT requires updates to proceed backwards through time from the end of
the sequence to the beginning. Ignoring any biological implausibility with the rules themselves, this
updating sequence is clearly not biologically plausible as naively it requires maintaining the entire
sequence of predictions and prediction errors perfectly in memory until the end of the sequence, and
waiting until the sequence ends before making any updates. There is a small literature on trying to
produce biologically plausible, or forward-looking approximations to BPTT which does not require
updates to be propagated back through time (Williams and Zipser, 1989; Lillicrap and Santoro, 2019;
Steil, 2004; Ollivier et al., 2015; Tallec and Ollivier, 2017). While this is a fascinating area, we do
not address it in this paper. We are solely concerned with the fact that predictive coding approximates
backpropagation on feedforward computation graphs for which the unrolled RNN graph is a sufficient
substrate.
To learn a predictive coding RNN, we first augment each of the variables ht and yt of the original
graph with additional error units Eht and Eyt. Predictions yt, ht are generated according to the
feedforward rules (16). A sequence of true labels {T1...TT} is then presented to the network, and
then inference proceeds by recursively applying the following rules backwards through time until
convergence.
Eyt = L - yt
Eht = ht - ht
dht
dtr = Eht
- Eyt θyT - Eht+1 θhT
Upon convergence the weights are updated according to the following rules.
dθy 	= dt	T XEyt	∂g(θyht) hT ∂θut	
	t=0	y	
dθx ---= dt	T X Eht t=0	∂f(θhht-1 + θxxt) ∂θX	X	T
dθh ---= dt	T X Eht t=0	∂f(θhht-1 + θχXt) h ∂θh	,	T+1
Since the RNN feedforward updates are parameter-linear, these rules are Hebbian, only requiring
the multiplication of pre and post-synaptic potentials. This means that the predictive coding updates
proposed here are biologically plausible and could in theory be implemented in the brain. The only
biological implausibility remains the BPTT learning scheme.
Our RNN was trained on a simple character-level name-origin dataset which can be found here:
https://download.pytorch.org/tutorial/data.zip. The RNN was presented with sequences of characters
representing names and had to predict the national origin of the name - French, Spanish, Russian, etc.
The characters were presented to the network as one-hot-encoded vectors without any embedding.
The output categories were also presented as a one-hot vector. The RNN has a hidden size of 256
18
Under review as a conference paper at ICLR 2021
units. A tanh nonlinearity was used between hidden states and the output layer was linear. The
network was trained on randomly selected name-category pairs from the dataset. The training loss
for the predictive coding and backprop RNNs, averaged over 5 seeds is presented below (Figure 7).
Figure 8: Training losses for the predictive coding and backprop RNN. As expected, they are
effectively identical.
Appendix C:	Predictive Coding LSTM Implementation Details
dL dv3
dL dv3
dv3 dv2
θ
inp
dL dv2
dv2 dv i
dL dv7
dv7 dv3
dL dv6
dv6 dv4
v7 = v3 + v6
j dL dv7
▼ dv7 dv 6
× × v 6 = v5v4
v4 = σ(θinpv 1) σ v5 = tanh(θcv 1) tαnh
Oc
dL dv4
dv4 dv 1
iL dv 6
v66 dv5
v8 =
Oo
dL dv5
dv5 dv 1
dL dct +1
dct +1 dv7
ct +1
d dL dv9
I dv9 dv7
v9 = tanh (v7) tanh
v 10 = v8v9 1
------►×-
dL dv 10
dv 10 dv9
dL dv io
θydv 10 dv8
dL dv8
dv8 dv i
dL dy
dy dv 10
dL dht +1
dht +1 dv 10
y = σ (θyv 10)
dL
dy
σ
yt- τ
Figure 9: Computation graph and backprop learning rules for a single LSTM cell.
Unlike the other two models, the LSTM possesses a complex and branching internal computation
graph, and is thus a good opportunity to make explicit the predictive coding ’recipe’ for approximating
backprop on arbitrary computation graphs. The computation graph for a single LSTM cell is shown
(with backprop updates) in Figure 8. Prediction for the LSTM occurs by simply rolling forward a
copy of the LSTM cell for each timestep. The LSTM cell receives its hidden state ht and cell state ct
from the previous timestep. During training we compute derivatives on the unrolled computation
graph and receive backwards derivatives (or prediction errors) from the LSTM cell at time t + 1.
19
Under review as a conference paper at ICLR 2021
The equations that specify the computation graph of the LSTM cell are as follows.
vι = ht ㊉ Xt
v2 = σ(θiv1)
v3 = ctv2
v4 = σ(θinpv1 )
v5 = tanh(θcv1)
v6 = v4v5
v7 = v3 + v6
v8 = σ(θov1)
v9 = tanh(v7)
v10 = v8v9
y = σ(θyv10)
The recipe to convert this computation graph into a predictive coding algorithm is straightforward.
We first rewire the connectivity so that the predictions are set to the forward functions of their parents.
We then compute the errors between the vertices and the predictions.
vι = ht ㊉ Xt
^ = σ(θivι)
V3 = CtV
v4 = σ(θinpv1)
V5 = tanh(θcVι)
^ = V4V5
V7 = V3 + V6
V = σ(θ0V1)
V9 = tanh(v7)
V10 = V8V9
Vy = σ(θy VIO)
€1 = vι — Vι
62 = V2 — ^2
€3 = V3 — V3
€4 = V4 — V4
€5 = V5 - V5
€6 = V6 - ^
€7 = V7 — ^7
€8 = V8 - V8
€9 = V9 - V9
€10 = V10 - V10
During inference, the inputs ht ,Xt and the output yt are fixed. The vertices and then the prediction
errors are updated according to Equation 2. This recipe is straightforward and can easily be extended
to other more complex machine learning architectures. The full augmented computation graph,
including the vertex update rules, is presented in Figure 9.
Empirically, we observed rapid convergence to the exact backprop gradients even in the case of very
deep computation graphs (as is an unrolled LSTM with a sequence length of 100). Although conver-
gence was slower than was the case for CNNs or lesser sequence lengths, it was still straightforward
to achieve convergence to the exact numerical gradients with sufficient iterations.
Below we plot the mean divergence between the predictive coding and true numerical gradients as
a function of sequence length (and hence depth of graph) for a fixed computational budget of 200
iterations with an inference learning rate of 0.05. As can be seen, the divergence increases roughly
20
Under review as a conference paper at ICLR 2021
Figure 10: The LSTM cell computation graph augmented with error units, evincing the connectivity
scheme of the predictive coding algorithm.
linearly with sequence length. Importantly, even with long sequences, the divergence is not especially
large, and can be decreased further by increasing the computational budget. As the increase is linear,
we believe that predictive coding approaches should be scalable even for backpropagating through
very deep and complex graphs.
We also plot the number of iterations required to reach a given convergence threshold (here taken
to be 0.005) as a function of sequence length (Figure 11). We see that the number of iterations
required increases sublinearly with the sequence length, and likely asymptotes at about 300 iterations.
Although this is a lot of iterations, the sublinear convergence nevertheless shows that the method can
scale to even extremely deep graphs.
Our architecture consisted of a single LSTM layer (more complex architectures would consist of
multiple stacked LSTM layers). The LSTM was trained on a next-character character-level prediction
task. The dataset was the full works of Shakespeare, downloadable from Tensorflow. The text was
shuffled and split into sequences of 50 characters, which were fed to the LSTM one character at a
time. The LSTM was trained then to predict the next character, so as to ultimately be able to generate
text. The characters were presented as one-hot-encoded vectors. The LSTM had a hidden size and a
cell-size of 1056 units. A minibatch size of 64 was used and a weight learning rate of 0.0001 was
used for both predictive coding and backprop networks. To achieve sufficient numerical convergence
to the correct gradient, we used 200 variational iterations with an inference learning rate of 0.1. This
rendered the predictive LSTM approximately 200x as costly as the backprop LSTM to run. A graph
of the LSTM training loss for both predictive coding and backprop LSTMs, averaged over 5 random
seeds, can be found below (Figure 12).
Appendix D: Derivation of the Free Energy Functional
Here we derive in detail the form of the free-energy functional used in sections 2 and 4. We also
expand upon the assumptions required and the precise form of the generative model and variational
density. Much of this material is presented with considerably more detail in Buckley et al. (2017),
and more approachably in Bogacz (2017).
Given an arbitrary computation graph with vertices {yi}, which we treat as random variables. Here we
treat explicitly an important fact that we glossed over for notational convenience in the introduction.
The vi s which are optimized in the free-energy functional are technically the mean parameters of
21
Under review as a conference paper at ICLR 2021
Figure 11: Divergence between predictive coding and numerical gradients as a function of sequence
length.
日后s,(υhuQU 3JQM-<uq 打|JQ+jfo.lQJ±!
Figure 12: Number of iterations to reach convergence threshold as a function of sequence length.
the variational density Q(yi； vi, σ/ - i.e. they represent the mean (variational) belief of the value of
the vertex. The vertex values in the model, which we here denote as {yi }, are technically separate.
However, due to our Gaussian assumptions, and the expectation under the variational density, in
effect we end up replacing the yi with the vi and optimizing the vis, so in the interests of space and
notational simplicity we began as if the vi s were variables in the generative model, but they are not.
They are parameters of the variational distribution.
22
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
FigUre IW Training IoSSeS for the PrediCtiVe Codmg and backprop LSTMS averaged OVer 5 seed-The
PerfOnnanCe Ofthe two training methods is effectiveIy equivalent，
GiVen an input 公。and a target 亡 N (the multiple input and/or OUtPUt CaSe is a Strghtforward general—
ization)∙ We WiSh to infer the POSteriOr七(大一一 N——虫虫 N)∙ We approXimate this intractable POSteriOr
With VariatiOIIaIilIferelIce，VariatIIai inference proceeds by defining an approximate posterior
Q(E-N——l~ 0) With SOme arbitrary ParameterS e. We then WiShtO minimize the KL divergence
between the true and approximate POSterior
argmin IKu-Q(9LN0) =P(9LNLmP 9N)一
a
AIthOUgh this KL is itself intractabl-since it includes the intractable posteriory we CaIl derive a
tractable bound on this KL Caned the Variatnal free—energy
IKMQ(9LNL;0)I9LNgWNI=M4Q(fNL=p(YNa「NL
PvgFeN)
HIO4qgun~ 0)=f - V 大。+ hl虫虫N)
U IKL(公IZs虫UN，虫。2 N)一 ≤ IKL 一。(虫1一之——1虫一一 之——虫。2 N
/ < \
——F
(9)
Wedefinethe negative free—energy —7 U IKL-Q (公一一N——1)=七(大一一2——17虫虫之)- WhiChiSaIOWerbOUnd
on the divergence between the true and approXimatePOSterrBy thus maximizing the IIegatiVe
free—energy (WhiCh is identical to the ELBo (BeaI et a2003; BIei et al: 2017) L S equivalently
minimizing the free—energy We decrease this divergence and make the Variationdistribution a better
approximation to the true POSterr
Ho ProCeed furtheryit is necessary to define an explicit form Of the generative model 七(公01∙∙Nllj 虫N)
and the approXimate PoSterr QGl-NIrIn PrediCtiVe coding We define a hierarchicGaUSSian
generative model WhiCh mirrors the exact StrUCtUre Ofthe COmPUtatn graph
N
p(9pN) = A⅜。一 学«)α.v(W- j(p(wh 6∈PS)Lw)j
工
Where essentialIy each VerteX 公⑼N GaUSSian Witha mean WhiCh is a function Ofthe PrediCtion Of1
the ParentS Of the vert，and the ParameterS Of their edgelfunctns∙is effectively an dnpuprr∖
WhiCh is SettO O throughout and ignored，The OUtPUt VertiCeS 虫NUTareSettO the target τ.
We also define the VariatIIaI density to be GaUSSiaIl With mean f N——1 and VarianCe QuN——but
Under a mean Hd approXimat尸 Sothat the approXimation at each node is independent Of1 OtherS
23
Under review as a conference paper at ICLR 2021
(note the variational variance is denoted σ while the variance of the generative model is denoted Σ.
The lower-case σ is not used to denote a scalar variable - both variances can be multivariate - but to
distinguish between variational and generative variances)
N-1
Q(yi：N-1； vi:N-i,σi:N-1)= ɪɪ N(yi； Vi,σ∕
i=1
We now can express the free-energy functional concretely. First we decompose it as the sum of an
energy and an entropy
-F = KL[Q(yi:N-1； vi：N-ι,σi:N-1)kp(y0, yi：N-ι,yN)]
=-EQ(yi:N -1；Vl：N-1,bi：N-i) [ln p(y0, y1:N-1, yN )] + EQ(yi：N-1；Vl：N-1,bi：N-i) [ln Q(y1：N-1； v1:N-1, σ1:N-i)]
'------------------------{z----------------------} '---------------------------{z------------------------}
Energy	Entropy
Then, taking the entropy term first, we can express it concretely in terms of normal distributions.
EQ(yi：N-i；vi：N-i,bi：N-i)[ln Q(y1:N-1； v1:N-1, σ1:N-I)] = EQ(yi：N-i；vi：n-iQi：n-i) [〉: lnN(yi; vi, σi)]
i=1
N-1
E EQ (yi ;Vi,bi)[ln Nryi ； vi,σi)]
i=1
N-1	1	(yi - vi)2
^X EQ(yi*i,σi)[-5 lndet(2πσiD + EQ(yi；Vi,σi)[	-	]
2	2σi
i=1
N-1	1	σ
X - 2ln det(2πσi]) + 2-
2	2σi
i=1
N	N-1 1
y + ∑ --ln det(2πσi)
2	i=1	2
The entropy of a multivariate gaussian has a simple analytical form depending only on the variance.
Next we turn to the energy term, which is more complex. To derive a clean analytical result, we must
make a further assumption, the Laplace approximation, which requires the variational density to be
tightly peaked around the mean so the only non-negligible contribution to the expectation is from
regions around the mean. This means that we can successfully approximate the approximate posterior
with a second-order Taylor expansion around the mean. From the first line onwards we ignore the
lnp(y0) and ln p(yN |P (yN)) which lie outside the expectation.
N-1
EQ(yi：N-1；V1：N-1G：N-I)[ln P(yo：N )] = ln p(yo) +ln P(UN IP (UN )) + E EQ^Vi。) [ln P(Ui↑P (yi))]
i=1
=X Eq [lnP(vi∣P(yi))] + EQ[dlnp(∂ilP(yi)) (Vi - yi)]
i=1	∂yi
+ EQ[d2lnPMP(yi)) (Vi - yi)2]
G] / I_Ud2lnp(vi∣P(yi))
=χin P(vi∣P (yi)) +-----∂~2------σi
Where the second term in the Taylor expansion evaluates to 0 since EQ [yi - Vi] = (Vi - Vi) = 0 and
the third term contains the expression for the variance EQ [(yi - Vi)2] = σi.
We can then write out the full Laplace-encoded free-energy as:
ɪ. v^1	/ Ie ∂ I ∂2 lnp(vi∣P(yi))	1 ]」“n ʌ
-F = ‰ln p(vi∣P (yi)) +-------∂y2------σi ——2lndet(2πσi)
24
Under review as a conference paper at ICLR 2021
We wish to minimize F with respect to the variational parameters vi and σi . There is in fact a closed-
form expression for the optimal variational variance which can be obtained simply by differentiating
and setting the derivative to 0.
∂F = ∂2 lnp(v∕P(yi)) _ -i
∂σj	∂y2	Cri
∂F
∂σi
0 ⇒ σi
∂2 ln p(v∕P (yi))
dy2
Because of this analytical result for the variational variance, we do not need to consider it further in
the optimisation problem, and only consider minimizing the variational means vi . This renders all
the terms in the free-energy except the lnp(vi|P(yi))terms constant with respect to the variational
parameters. This allows us to write:
N
-F ≈ lnp(yN|P(yN))+ Xlnp(vi|P(yi))	(10)
i=1
as presented in section 2. The first term lnp(yN|P(yN))is effectively the loss at the output (yN = T)
so becomes an additional prediction error lnp(yN|P(yN)) α (T - VN)t∑-1(T - VN) which can
be absorbed into the sum over other prediction errors. Crucially, although the variational variances
have an analytical form, the variances of the generative model (the precisions Σi) do not and can
be optimised directly to improve the log model-evidence. These precisions allow for a kind of
’uncertainty-aware’ backprop.
Derivation of Variational Update Rules and Fixed points
Here, starting from Equation 10, we show how to obtain the variational update rule for the Vi ’s
(Equation 2), and the fixed point equations (Equation 5) (Friston, 2008; 2005; Bogacz, 2017). We
first reduce the free-energy to a sum of prediction errors.
N
-F ≈ Xlnp(Vi|P(Vi))
i=1
N
≈ X(Vi - f (P (V1))T Σi-1 (Vi - f(P(V1))T + ln 2πΣi-1
i=1
N
= XiTi +ln2πΣi-1
i=1
Where i = Vi - f(P(V1)), and we have utilized the assumption made in section 2 that Σ-1 = I. By
setting all precisions to the identity, we are implicitly assuming that all datapoints and vertices of the
computational graph have equal variance. Next we assume that the dynamics of each vertex Vi follow
a gradient descent on the free-energy.
—
dVi	∂F	∂ N T
dti = ∂Vi = ∂Vi 乜 ej ej ]
j=1
=& % + X U ⅛i
2 ∂Vi 乙 j ∂Vi
i j∈C(vi)	i
—
∂Vj
W J嬴
25
Under review as a conference paper at ICLR 2021
Where we have used the fact that 煞=1 and Ivj
we simply solve for 嚓=0.
-dvj. To obtain the fixed point of the dynamics,
OVi
dvi	∂F	C
——=——=0
dt	∂vi
∂Vj
JXj而
⇒ 0 = G
—
* - L	∂v;
⇒ Ci =	Cj
i 乙 j ∂v*
j∈C(vi)	i
Similarly, since Ci = v* - V* then v* = Ci + V*. So:
*	* I 人*
Vi= Ci + Vi
.* V ∂^
LwiT
26