Under review as a conference paper at ICLR 2021
Understanding and Mitigating Accuracy Dis-
parity in Regression
Anonymous authors
Paper under double-blind review
Ab stract
With the widespread deployment of large-scale prediction systems in high-stakes
domains, e.g., face recognition, criminal justice, etc., disparity on prediction
accuracy between different demographic subgroups has called for fundamental
understanding on the source of such disparity and algorithmic intervention to
mitigate it. In this paper, we study the accuracy disparity problem in regression. To
begin with, we first propose an error decomposition theorem, which decomposes
the accuracy disparity into the distance between label populations and the distance
between conditional representations, to help explain why such accuracy disparity
appears in practice. Motivated by this error decomposition and the general idea
of distribution alignment with statistical distances, we then propose an algorithm
to reduce this disparity, and analyze its game-theoretic optima of the proposed
objective function. We conduct experiments on four real-world datasets. The
experimental results suggest that our proposed algorithms can effectively mitigate
accuracy disparity while maintaining the predictive power of the regression models.
1	Introduction
Recent progress in machine learning has led to its widespread use in many high-stakes domains,
such as criminal justice, healthcare, student loan approval, and hiring. Meanwhile, it has also
been widely observed that accuracy disparity could occur inadvertently under various scenarios in
practice (Barocas & Selbst, 2016). For example, errors are inclined to occur for individuals of certain
underrepresented demographic groups (Kim, 2016). In other cases, Buolamwini & Gebru (2018)
showed that notable accuracy disparity gaps exist across different racial and gender demographic
subgroups on several real-world image classification systems. Moreover, Bagdasaryan et al. (2019)
found out that a differentially private model even enlarges such accuracy disparity gaps. Such accuracy
disparity gaps across demographic subgroups not only raise concerns in high-stake applications but
also can be utilized by malicious parties causing information leakage (Yaghini et al., 2019).
Despite the ample needs of accuracy parity, most prior work limits its scope to studying the problem
in binary classification settings (Hardt et al., 2016; Zafar et al., 2017b; Zhao et al., 2019; Jiang et al.,
2019). In a seminal work, Chen et al. (2018) analyzed the impact of data collection on accuracy
disparity in general learning models. They provided a descriptive analysis of such parity gaps and
advocated for collecting more training examples and introducing more predictive variables. While
such a suggestion is feasible in applications where data collection and labeling is cheap, it is not
applicable in domains where it is time-consuming, expensive, or even infeasible to collect more data,
e.g., in autonomous driving, education, etc.
Our Contributions In this paper, we provide a prescriptive analysis of accuracy disparity and aim
at providing algorithmic interventions to reduce the disparity gap between different demographic
subgroups in the regression setting. To start with, we first formally characterize why accuracy
disparity appears in regression problems by depicting the feasible region of the underlying group-wise
errors. We also provide a lower bound on the joint error and a complementary upper bound on
the error gap across groups. Based on these results, we illustrate why regression models aiming to
minimize the global loss will inevitably lead to accuracy disparity if the input distributions or decision
functions differ across groups (see Figure 1a).
We further propose an error decomposition theorem that decomposes the accuracy disparity into the
distance between the label populations and the distance between conditional representations. To
mitigate such disparities, we propose two algorithms to reduce accuracy disparity via joint distribution
alignment with total variation distance and Wasserstein distance, respectively. Furthermore, we
1
Under review as a conference paper at ICLR 2021
(a) Geometric interpretation of accuracy disparity.
X
Input Data
Alice
Transformed
Representation
Bob
(b) Game-theoretic illustration of our algorithms.
Z
Figure 1: The left figure illustrates how accuracy disparity arises by minimizing the global squared
loss. The right figure gives a schematic illustration of the proposed algorithmic framework.
analyze the game-theoretic optima of the objective function and illustrate the principle of our
algorithms from a game-theoretic perspective (see Figure 1b). To corroborate the effectiveness of
our proposed algorithms in reducing accuracy disparity, we conduct experiments on four real-world
datasets. Experimental results suggest that our proposed algorithms help to mitigate accuracy disparity
while maintaining the predictive power of the regression models. We believe our theoretical results
contribute to the understanding of why accuracy disparity occurs in machine learning models, and the
proposed algorithms provides an alternative for intervention in real-world scenarios where accuracy
parity is desired but collecting more data/features is time-consuming or infeasible.
2	Preliminaries
Notation We use X ⊆ Rd and Y ⊆ R to denote the input and output space. We use X and Y to
denote random variables which take values in X and Y , respectively. Lower case letters x and y
denote the instantiation of X and Y . We use H(X) to denote the Shannon entropy of random variable
X, H(X | Y ) to denote the conditional entropy of X given Y , and I(X; Y ) to denote the mutual
information between X and Y . To simplify the presentation, we use A ∈ {0, 1} as the sensitive
attribute, e.g., gender, race, etc. Let H be the hypothesis class of regression models. In other words,
for h ∈ H, h : X → Y is a predictor. Note that even if the predictor does not explicitly take the
sensitive attribute A as an input variable, the prediction can still be biased due to the correlations with
other input variables. In this work we study the stochastic setting where there is a joint distribution
D over X, Y and A from which the data are sampled. For a ∈ {0, 1} and y ∈ R, we use Da to
denote the conditional distribution of D given A = a and Dy to denote the conditional distribution
of D given Y = y. For an event E, D(E) denotes the probability of E under D. Given a feature
transformation function g : X → Z that maps instances from the input space X to feature space
Z, we define g]D := D ◦ g-1 to be the induced (pushforward) distribution of D under g, i.e., for
any event E0 ⊆ Z, g]D(E0) := D({x ∈ X | g(x) ∈ E0}). We use (•)+ to indicate the value of a
variable remains unchanged if it is positive or otherwise 0, i.e., (Y )+ equals to Y if the value of Y is
positive or otherwise 0.
Given a joint distribution D, the error of a predictor h under D is defined as ErrD (h) := ED [(Y -
h(X))2]. To make the notation more compact, we may drop the subscript D when it is clear from
the context. Furthermore, we also use MSED(Y , Y ) to denote the mean squared loss between the
predicted variable Yb = h(X) and the true label Y over the joint distribution D. Similarly, we also
use CED(A k A) denote the cross-entropy loss between the predicted variable A and the true label A
over the joint distribution D. Throughout the paper, we make the following standard assumption in
regression problems:
Assumption 2.1. There exists M > 0, such that for any hypothesis H 3 h : X → Y, khk∞ ≤ M
and |Y | ≤ M.
Problem Setup We study the fair regression problem: the goal is to learn a regressor that is fair
in the sense that the errors of the regressor are approximately equal across the groups given by the
sensitive attribute A. We assume that the sensitive attribute A is only available to the learner during
2
Under review as a conference paper at ICLR 2021
the training phase and is not visible during the inference phase. We would like to point out that
there are many other different and important definitions of fairness (Narayanan, 2018) even in the
sub-category of group fairness, and our discussion is by no means comprehensive. For example,
two frequently used definitions of fairness in the literature are the so-called statistical parity (Dwork
et al., 2012) and equalized odds (Hardt et al., 2016). Nevertheless, throughout this paper we mainly
focus accuracy parity as our fairness notion, due to the fact that machine learning systems have been
shown to exhibit substantial accuracy disparities between different demographic subgroups (Barocas
& Selbst, 2016; Kim, 2016; Buolamwini & Gebru, 2018). This observation has already brought huge
public attention (e.g., see New York Times, The Verge, and Insurance Journal) and calls for machine
learning systems that (at least approximately) satisfy accuracy parity. Formally, accuracy parity is
defined as follows:
Definition 2.1 (Accuracy Parity). Given a joint distribution D, a predictor h satisfies accuracy parity
if ErrD0 (h) = ErrD1 (h).
The violation of accuracy parity is also known as disparate mistreatment (Zafar et al., 2017a). In
practice the exact equality of on accuracy between two groups is often hard to ensure, so we define
error gap to measure how well the model satisfies accuracy parity:
Definition 2.2 (Error Gap). Given a joint distribution D, the error gap of a hypothesis h is ∆Err(h) :=
|ErrD0 (h) - ErrD1 (h)|.
By definition, if a model satisfies accuracy parity, ∆Err (h) will be zero. Next we introduce two
distance metrics that will be used in our theoretical analysis and algorithm design:
•	Total variation distance: it measures the largest possible difference between the probabilities that
the two probability distributions can assign to the same event E. We use dTV (P, Q) to denote the
total variation:
dTV(P,Q) :=sup|P(E)-Q(E)|.
E
•	Wasserstein distance: the Wasserstein distance between two probability distributions is
W1(P,Q) = sup	fdP-	fdQ,
f∈{f:kf kL≤i} "ω	Jω
where kf ∣∣l is the LiPschitz semi-norm of a real-valued function of f and Ω is the sample space
over which two probability distributions P and Q are defined. By the Kantorovich-Rubinstein
duality theorem (Villani, 2008), we recover the primal form of the Wasserstein distance, defined as
W1(P,Q):
inf
γ∈Γ(P,Q)
d(X,Y)dγ(X,Y),
where Γ(P, Q) denotes the collection of all couplings ofP and Q, and X and Y denote the random
variables with law P and Q respectively. Note that we use Li distance for d(∙, ∙) throughout the
paper, but the extensions to other distance, e.g., L2 distance, is straightforward.
3 Main Results
In this section, we first characterize why accuracy disparity arises in regression models. More
specifically, given a hypothesis h ∈ H, we first describe the feasible region of ErrD0 and ErrD1
by proving a lower bound of joint errors and an upper bound of the error gap. Then, we give a
geometric interpretation to visualize the feasible region of ErrD0 and ErrD1 and illustrate how error
gap arises when learning a hypothesis h that minimizes the global squared error. We further analyze
the accuracy disparity by decomposing it into the distance between label populations and the distance
between conditional representations. Motivated by the decomposition, we propose two algorithms
to reduce accuracy disparity, connect the game-theoretic optima of the objective functions in our
algorithms with our theorems, and describe the practical implementations of the algorithms. Due to
the space limit, we defer all the detailed proofs to the appendix.
3.1	B ounds on Conditional Errors and Accuracy Disparity Gap
When we learn a predictor, the prediction function induces X -h→ Yb , where Yb is the predicted target
variable given by hypothesis h. Hence for any distribution D0 (D1) of X, the predictor also induces a
3
Under review as a conference paper at ICLR 2021
distribution h]D0 (h]D1) of Y . Recall that the Wasserstein distance is metric, hence the following
chain of triangle inequalities holds:
W1(D0(Y),D1(Y)) ≤W1(D0(Y),h]D0)+W1(h]D0,h]D1)+W1(h]D1,D1(Y))
Intuitively, W1(D0(Y ), h]D0) and W1(h]D1, D1(Y )) measure the distance between the true label
distribution and the predicted one on A = 0/1 cases, respectively. This distance is related to the
prediction error of function h conditioned on A = a:
Lemma 3.1. Let Yb = h(X) ∈ R, then for a ∈ {0,1}, Wi(Da(Y), h]D0) ≤ PErrDa (h).
With the above results, we can get the following theorem that characterizes the lower bound of joint
error on different groups:
Theorem 3.1.	Let Yb = h(X) ∈ R, we have ErrD0(h) + ErrD1 (h) ≥ 2 [(Wι(Do(Y), Di(Y))-
W1(h]D0,h]D1)+2.
In Theorem 3.1, we see that if the difference between the label distribution across groups is large,
then statistical disparity could potentially lead to a large joint error. Moreover, Theorem 3.1 could be
extended to give a lower bound on the joint error incurred by h as well:
Corollary 3.1. Let Yb = h(X) ∈ R and α = D(A = 0) ∈ [0,1], We have ErrD(h) ≥ 11 min{α, 1 -
α} ∙ [(Wi(Do(Y), Di(Y)) - W1(h]D0,h]D1))+]2.
Next, we upper bound the error gap to gain more insights on accuracy disparity. For a ∈ {0, 1},
define the conditional variance VarDa [Y |X] = EDa [(Y - EDa [Y |X])2|X] and it shows up as the
irreducible error of predicting Y when we only use the knowledge of X . We also know that the
optimal decision function conditioned on A = a under mean squared error to be EDa [Y |X]. The
following theorem characterizes the upper bound of the error gap between two groups:
Theorem 3.2.	For any hypothesis H 3 h : X → Y, if the Assumption 2.1 holds, then:
∆Err(h) ≤ 8M2 dτv(Do(X), Di(X)) + ∣Edo [VarDo [Y|X]] - EDjVarDjY|X]]|
+4Mmin{ED0[|ED0(Y |X) -ED1(Y |X)|], ED1[|ED0(Y |X) -ED1(Y |X)|]}.
Remark τheorem 3.2 upper bounds the error gap across groups by three terms: the first term
corresponds to the distance of input distribution across groups, the second term is the noise (variance)
difference, and third term is the discrepancy of optimal decision functions across different groups. In
an ideal and fair setting, where both distributions are noiseless, and the optimal decision functions
are insensitive to the group membership, then τheorem 3.2 implies a sufficient condition to guarantee
accuracy parity is to find group-invariant representation that minimize dτv(D0(X), Di(X)).
Geometric Interpretation By τheorem 3.1 and τheorem 3.2, in Figure 1a, we visually illustrate how
accuracy disparity arises given data distribution and the learned hypothesis that aims to minimize the
global squared error. In Figure 1a, given the hypothesis class H, we use the line ErrD0 + ErrD1 = B
to denote the lower bound in τheorem 3.1 and the two lines |ErrD0 - ErrD1 | = A to denote the
upper bound in τheorem 3.2. τhese three lines form a feasible region (the green area) of ErrD0
and ErrD1 under the hypothesis class H. For any optimal hypothesis h which is solely designed to
minimize the overall error, the best the hypothesis h can do is to intersect with one of the two bottom
vertices. For example, the hypotheses (the red dotted line and the blue dotted line) trying to minimize
overall error intersect with the two vertices of the region to achieve the smallest ErrD0 -intercept
(ErrD1 -intercept), due to the imbalance between these two groups. However, since these two vertices
are not on the diagonal of the feasible region, there is no guarantee that the hypothesis can satisfy
accuracy parity (ErrD0 = ErrD1), unless we can shrink the width of green area to zero.
Conditional Distribution Alignment Reduces Accuracy Parity In τheorem 3.2, we illustrate how
accuracy disparity arises in regression models due to noise, distance between representations, and
distance between decision functions. However, it is nearly impossible to collect noiseless data with
group-invariant input distribution. Moreover, there is no guarantee that the upper bound will be
lower if we learn the group-invariant representation that minimizes dτv(D0(X), Di(X)) alone, since
the learned representation could potentially increase the variance. In this regard, we prove a novel
upper bound which is free from the above noise term to motivate aligning conditional distributions to
mitigate the error disparity across groups. τo do so, we relate the error gap to the label distribution
and the predicted distribution condition on Y = y :
4
Under review as a conference paper at ICLR 2021
Theorem 3.3.	If Assumption 2.1 holds, then for ∀h ∈ H, let Yb = h(X), the following inequality
holds:
∆Err(h) ≤ 8M2dTV(D0(Y),D1(Y))
- -. -∙^∙- -∙^∙-.- -. -∙^∙- -∙^∙-.--
+3Mmin{ED0[|ED0y[Yb]-ED1y[Yb]|],ED1[|ED0y[Yb]-ED1y[Yb]|]}.
Remark We see that the error gap is upper bounded by two terms: the distance between label
distributions and the discrepancy between conditional predicted distributions across groups. Note
that this is different from the decomposition we have in Theorem 3.2, where the marginal distribution
is on X instead of Y . Given a dataset, the distance of label distributions is a constant since the label
distribution is fixed. For the second term, if we can minimize the discrepancy of the conditional
predicted distribution across groups, we then have a model that is free of accuracy disparity when the
label distribution is well aligned.
3.2 Algorithm Design
Inspired by Theorem 3.3, we can mitigate the error gap if we align the group distributions via
minimizing the distance of the conditional distributions across groups. However, it is intractable to
do so explicitly in regression problems since Y can take infinite values on R. Next we will present
two algorithms to approximately solve the problem through adversarial representation learning.
Given a Markov chain X -g→ Z -→ Yb , we are interested in learning group-invariant conditional
representations so that the discrepancy between the induced conditional distributions D0Y (Z = g(X))
and D1Y (Z = g(X)) is minimized. In this case, the second term of the upper bound in Theorem 3.3 is
minimized. However, it is in general not feasible since Y is a continuous random variable. Instead, we
propose to learn the representations of Z to minimize the discrepancy between the joint distributions
D0(Z = g(X), Y ) and D1(Z = g(X), Y ). Next, we will show the distances between conditional
predicted distributions D0Y (Z = g(X)) and D1Y (Z = g(X)) are minimized when we minimize the
joint distributions D0(Z = g(X), Y ) and D1(Z = g(X), Y ) in Theorem 3.4 and Theorem 3.5.
To proceed, we first consider using the total variation distance to measure the distance between two
distributions. In particular, we can choose to learn a binary discriminator f : Z × Y -→ Ab that
achieves minimum binary classification error on discriminating between points sampled from two
distributions. In practice, we use the cross-entropy loss as a convex surrogate loss. Formally, we are
going to consider the following minimax game between g and f :
fm∈iFnmgax CED(Akf(g(X),Y))
(1)
Next we show that for the above equation, the optimal feature transformation g corresponds to the
one that induces invariant conditional feature distributions.
Theorem 3.4.	Consider the minimax game in (1). The equilibrium (g*, f *) of the game is attained
when 1). Z = g"X) is independent of A conditioned on Y; 2). f *(Z, Y) = D(A = 1 | Y, Z).
Since in the equilibrium of the game Z is independent of A conditioned on Y, the optimal f * (Z, Y)
could also be equivalently written as f *(Z, Y) = D(A = 1 | Y), i.e., the only useful information
for the discriminator in the equilibrium is through the external information Y . In Theorem 3.4, the
minimum cross-entropy loss that the discriminator (the equilibrium of the game) can achieve is
H(A | Z, Y) (see Proposition A.1 in Appendix A). By the basic property of conditional entropy, we
have:
minCED(Akf(g(X),Y))=H(A|Z,Y)=H(A|Y)-I(A;Z|Y).
f∈F
We know that H(A | Y) is a constant given the data distribution. The maximization of g in (1) is
equivalent to the minimization of minZ=g(X) I(A; Z | Y), and it follows that the optimal strategy
for the transformation g is the one that induces conditionally invariant features, e.g., I(A; Z | Y) = 0.
Formally, we arrive at the following minimax problem:
minmax MSED (h(g(X)), Y)-λ ∙ CEd (A k f(g(X ),Y))
h,g f∈F
(2)
In the above formulation, the first term corresponds to the minimization of prediction loss of the
target task and the second term is the loss incurred by the adversary f . As a whole, the minimax
5
Under review as a conference paper at ICLR 2021
optimization problem expresses a trade-off (controlled by the hyper-parameter λ > 0) between
accuracy and accuracy disparity through the representation learning function g.
Wasserstein Variant Similarly, if we choose to align joint distributions via minimizing Wasstertein
distance, the following theorem holds.
Theorem 3.5.	Let g* := argmi、Wi(Do(g(X),Y),D∖(g(X),Y)), then DY(Z = g*(X))=
DY(Z = g*(X)) almost surely.
One notable advantage of using the Wasserstein distance instead of the TV distance is that, the
Wasserstein distance is a continuous functional of both the feature map g as well as the discriminator
f (Arjovsky et al., 2017). Furthermore, if both g and f are continuous functions of their corresponding
model parameters, (which is the case for models we are going to use in experiments), the objective
function will be continuous in both model parameters. This property of the Wasserstein distance
makes it more favorable from an optimization perspective. Using the dual formulation, equivalently,
we can learn a Lipschitz function f : Z × Y → R as a witness function:
min	max
h,g,Zo〜g]D0,Z1 〜g]Dι f:kf ∣∣L≤1
MSED (h(g(X)), Y)+ λ ∙ f(Z0,Y) - f(Z1,Υ )∣.
(3)
Game-Theoretic Interpretation To make our algorithms easier to follow, we provide a game-
theoretic interpretation of our algorithms in Figure 1b. Consider Alice (encoder) and Bob (discrimi-
nator) participate a two-player game: upon receiving a set of inputs X , Alice applies a transformation
to the inputs to generate the corresponding features Z and then send them to Bob. Besides the
features sent by Alice, Bob also has access to the external information Y, which corresponds to the
corresponding labels for the set of features sent by Alice. Once having both the features Z and the
corresponding labels Y from external resources, Bob’s goal is to guess the group membership A of
each feature sent by Alice, and to maximize his correctness as much as possible. On the other hand,
Alice’s goal is to compete with Bob, i.e., to find a transformation to confuse Bob as much as she can.
Different from the traditional game without external information, here due to the external information
Y Bob has access to, Alice cannot hope to fully fool Bob, since Bob can gain some insights about the
group membership A of features from the external label information. Nevertheless, Theorem 3.4 and
Theorem 3.5 both state that when Bob uses a binary discriminator or a Wasstertein discriminator to
learn A, the best Alice could do is to to learn a transformation g so that the transformed representation
Z is insensitive to the values of A conditioned on any values of Y.
4 Experiments
Inspired by our theoretical results that decompose accuracy disparity into the distance between label
populations and the distance between conditional representations, we propose two algorithms to
mitigate it. In this section, we conduct experiments to evaluate the effectiveness of our proposed
algorithms in reducing the accuracy disparity.
4.1	Experimental Setup
Datasets We conduct experiments on four real-world benchmark datasets: the Adult dataset (Dua &
Graff, 2017), COMPAS dataset (Dieterich et al., 2016), Law School dataset (Wightman & Ramsey,
1998), and Communities and Crime dataset (Dua & Graff, 2017). All datasets contain binary
sensitive attributes (e.g., male/female, white/non-white). We refer readers to Appendix B for detailed
descriptions of the datasets and the data pre-processing pipelines.
Methods We term the proposed algorithms CENET and WASSERSTEINNET for our two proposed
algorithms respectively. For each dataset, we perform controlled experiments by fixing the regression
neural network architecture to be the same. We train the regression nets via mean squared loss. Note
that although the Adult dataset and COMPAS dataset are for binary classification tasks, we can still
take them as regression tasks with two distinctive ordinal values. To the best of our knowledge,
no previous study aims to minimize accuracy disparity in regression using representation learning.
However, there are other similar fairness notions and mitigation techniques proposed for regression
and we add them as our baselines: (1) Bounded group loss (BGL) (Agarwal et al., 2019), which
asks for the prediction errors for any groups to remain below a pre-defined level ; (2) Coefficient
of determination (CoD) (Komiyama et al., 2018), which asks for the coefficient of determination
between the sensitive attributes and the predictions to remain below a pre-defined level .
6
Under review as a conference paper at ICLR 2021
,∙ CENet
I × WassersteinNet
▼ BGL
×
0.44 -
0.42-
0.40-
0.38-
⅛ 0.36-
0.34-
0.32-
0.30-
0.28-
0.055	0.060	0.065	0.070	0.075	0.080
∆∈rr
(a) Adult
0.16-
0.15-
0.14 -
0.13-
• CENet
× WassersteinNet
▼ BGL
■ CoD
0.12-
■
0.11 -
■
0-10-1-I-------1------1-----1------1-----1------1—
0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150
∆∈rr
(b) COMPAS
0.5-
• CENet
× WassersteinNet
▼ BGL
■ CoD
0.35-
0.30-
• CENet
× WassersteinNet
▼ BGL
■ CoD
0.25-
0.20-
0.15-
0.10-
0.2-
0.1 j-η--------1-------1-------1-------1--------1------1-
0.010	0.015	0.020	0.025	0.030	0.035	0.040
∆∈rr
(c) Crime
0.05-
0.00-I_r.
0.00
0.02	0.03	0.04
∆∈rr
(d) Law
Figure 2: Overall results: R2 regression scores and error gaps in different datasets. Our goal is to
achieve high R2 scores with small error gap values (i.e., the points located in the upper-left corner).
Among all methods, we vary the trade-off parameter (i.e., λ in CENET and WASSERSTEINNET
and in BGL and COD) and report and the corresponding R2 scores and the error gap values. For
each experiment, we average the results for ten random seeds. We refer readers to Appendix B for
detailed parameter and hyper-parameter settings in our experiments. We also defer the additional
experimental results and analyses on how the trade-off parameters λ and affects the performance of
different algorithms to Appendix C.
4.2 Results and Analyses
The overall results are visualized in Figure 2.1 The following summarizes our observations and
analyses: (1) Overall, trade-offs exist between the predictive power of the regressors and accuracy
parity: for each method we test, the general trend is that with the decrease of the values of error gaps,
the values of R2 also decrease. The exception is CENET in the Adult dataset and Crime dataset
since training CENET is unstable when λ is large and we will provide more details in Appendix C;
(2) Our proposed methods WassersteinNet and CENet are effective in reducing the error gaps
while keeping the R2 scores relatively high in the Adult, COMPAS and Crime dataset. In the Law
dataset, the error gaps decrease with high utility losses in our proposed methods; (3) Among our
1	CoD cannot be implemented on the Adult dataset since the size of the Adult dataset is large and the QCQP
optimization algorithm to solve CoD needs a quadratic memory usage of the dataset size.
7
Under review as a conference paper at ICLR 2021
proposed methods, WassersteinNet achieves better accuracy and accuracy disparity trade-offs
while CENet suffers significant accuracy loss and may fail to decrease the error gaps in the Adult
and Crime dataset. The reason behind it is that the minimax optimization in the training of CENet
could lead to an unstable training process under the presence of a noisy approximation to the optimal
discriminator (Arjovsky & Bottou, 2017; Arjovsky et al., 2017); (4) Compared to our proposed
methods, BGL and CoD can also decrease error gaps to a certain extent. This is because: (i) BGL
aims to keep errors remaining relatively low in each group, which helps to reduce accuracy disparity;
(ii) CoD aims to reduce the correlation between the sensitive attributes and the predictions (or the
inputs) in the feature space, which might somehow reduce the dependency between the distributions
of these two variables. In comparison, our proposed methods do better in mitigating the error gaps.
5	Related Work
Algorithmic Fairness In the literature, two main notions of fairness, i.e., group fairness and
individual fairness, has been widely studied (Dwork et al., 2012; Zemel et al., 2013; Feldman et al.,
2015; Hardt et al., 2016; Zafar et al., 2017b; Madras et al., 2019; Khani & Liang, 2019). In particular,
Chen et al. (2018) analyzed the impact of data collection on discrimination (e.g., false positive rate,
false negative rate, and zero-one loss) from the perspectives of bias-variance-noise decomposition,
and they suggested collecting more training examples and collect additional variable to reduce
discrimination. In comparison, our work precisely characterizes the disparate predictive accuracy in
terms of the distance between label populations and the distance between conditional representation
and propose algorithms to reduce accuracy disparity across groups in regression.
Fair Regression A series of work focus on fairness under the regression problems (Calders et al.,
2013; Johnson et al., 2016; Berk et al., 2018; Komiyama et al., 2018; Chzhen et al., 2020; Bigot,
2020; Zink & Rose, 2020; Mary et al., 2019; Narasimhan et al., 2020). To the best of our knowledge,
no previous study aimed to minimize accuracy disparity in regression from representation learning.
However, there are other similar fairness notions proposed for regression: Agarwal et al. (2019)
proposed fair regression with bounded group loss (i.e., it asks that the prediction error for any
protected group remain below some pre-defined level) and used exponentiated-gradient approach to
satisfy BGL; Komiyama et al. (2018) aimed to reduce the coefficient of determination between the
sensitive attributes between the predictions to some pre-defined level and used off-the-shelf convex
optimizer to solve the problem. In contrast, we source out the root of accuracy disparity through the
lens of information theory and reducing it via distributional alignment in a minimax game.
Fair Representation A line of work focus on building algorithmic fair decision making systems
using adversarial techniques to learn fair representations (Edwards & Storkey, 2015; Beutel et al.,
2017; Adel et al., 2019; Zhao et al., 2019). The main idea behind is to learn a good representation
of the data so that the data owner can maximize the accuracy while removing the information
related to the sensitive attribute. Madras et al. (2018) proposed a generalized framework to learn
adversarially fair and transferable representations and suggests using the label information in the
adversary to learn equalized odds or equal opportunity representations in the classification setting.
Apart from adversarial representation, recent work also proposed to use distance metrics, e.g., the
maximum mean discrepancy (Louizos et al., 2015) and the Wasserstein distance (Jiang et al., 2019)
to remove group-related information. Compared to their work, we propose to align (conditional)
distributions across groups to reduce accuracy disparity using minimax optimization and analyze the
game-theoretic optima in the minimax game in the regression setting.
6	Conclusion
In this paper, we theoretically and empirically study accuracy disparity in regression problems.
Specifically, we prove an information-theoretic lower bound on the joint error and a complementary
upper bound on the error gap across groups to depict the feasible region of group-wise errors. Our
theoretical results indicate that accuracy disparity occurs inevitably due to the label distributions
differ across groups. To reduce such disparity, we further propose to achieve accuracy parity by
learning conditional group-invariant representations using statistical distances. The game-theoretic
optima of the objective functions in our proposed methods are achieved when the accuracy disparity
is minimized. Our empirical results on four real-world datasets demonstrate that our proposed
algorithms help to reduce accuracy disparity effectively. We believe our results take an important
step towards better understanding accuracy disparity in machine learning models.
8
Under review as a conference paper at ICLR 2021
References
Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller. One-network adversarial
fairness. In Thirty-Third AAAI Conference on Artificial Intelligence, 2019.
Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. Fair regression: Quantitative definitions
and reduction-based algorithms. In International Conference on Machine Learning, pp. 120-129,
2019.
Martin Arjovsky and L6on Bottou. Towards principled methods for training generative adversarial
networks. arxiv e-prints, art. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate
impact on model accuracy. In Advances in Neural Information Processing Systems, pp. 15453-
15462, 2019.
Solon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671, 2016.
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in
criminal justice risk assessments: The state of the art. Sociological Methods & Research, pp.
0049124118782533, 2018.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.
Jeremie Bigot. Statistical data analysis in the wasserstein space. ESAIM: Proceedings and Surveys,
68:1-19, 2020.
Sarah Bird, Miro Dudik, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan,
Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for as-
sessing and improving fairness in AI. Technical Report MSR-TR-2020-32, Microsoft,
May 2020. URL https://www.microsoft.com/en-us/research/publication/
fairlearn- a- toolkit- for- assessing- and- improving- fairness-in-ai/.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classification. In Conference on fairness, accountability and transparency, pp. 77-91, 2018.
Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang. Controlling attribute
effect in linear regression. In 2013 IEEE 13th international conference on data mining, pp. 71-80.
IEEE, 2013.
Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? In
Advances in Neural Information Processing Systems, pp. 3539-3550, 2018.
Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Fair
regression with wasserstein barycenters. arXiv preprint arXiv:2006.07286, 2020.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9236-9246,
2018.
William Dieterich, Christina Mendoza, and Tim Brennan. Compas risk scales: Demonstrating
accuracy equity and predictive parity. Northpointe Inc, 2016.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pp.
214-226, 2012.
9
Under review as a conference paper at ICLR 2021
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897, 2015.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 259-268. ACM, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International
statistical review, 70(3):419-435, 2002.
Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In
Advances in neural information processing systems, pp. 3315-3323, 2016.
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair
classification. arXiv preprint arXiv:1907.12059, 2019.
Kory D Johnson, Dean P Foster, and Robert A Stine. Impartial predictive modeling: Ensuring fairness
in arbitrary models. arXiv preprint arXiv:1608.00528, 2016.
Fereshte Khani and Percy Liang. Noise induces loss discrepancy across groups for linear regression.
arXiv preprint arXiv:1911.09876, 2019.
Pauline T Kim. Data-driven discrimination at work. Wm. & Mary L. Rev., 58:857, 2016.
Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao. Nonconvex optimization
for regression with fairness constraints. In International conference on machine learning, pp.
2737-2746, 2018.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair
autoencoder. arXiv preprint arXiv:1511.00830, 2015.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. arXiv preprint arXiv:1802.06309, 2018.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Fairness through causal awareness:
Learning causal latent-variable models for biased data. In Proceedings of the Conference on
Fairness, Accountability, and Transparency, pp. 349-358, 2019.
J6r6mie Mary, Clement Calauzenes, and Noureddine El Karoui. Fairness-aware learning for continu-
ous attributes and treatments. In International Conference on Machine Learning, pp. 4382-4391,
2019.
Harikrishna Narasimhan, Andrew Cotter, Maya R Gupta, and Serena Wang. Pairwise fairness for
ranking and regression. In AAAI, pp. 5248-5255, 2020.
Arvind Narayanan. Translation tutorial: 21 fairness definitions and their politics. In Proc. Conf.
Fairness Accountability Transp., New York, USA, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Linda F Wightman and Henry Ramsey. LSAC national longitudinal bar passage study. Law School
Admission Council, 1998.
Mohammad Yaghini, Bogdan Kulynych, and Carmela Troncoso. Disparate vulnerability: On the
unfairness of privacy attacks against machine learning. arXiv preprint arXiv:1906.00389, 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness
beyond disparate treatment & disparate impact: Learning classification without disparate mistreat-
ment. In Proceedings of the 26th International Conference on World Wide Web, pp. 1171-1180.
International World Wide Web Conferences Steering Committee, 2017a.
10
Under review as a conference paper at ICLR 2021
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness
constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp. 962-970,
2017b.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. arXiv preprint arXiv:1910.07162, 2019.
Anna Zink and Sherri Rose. Fair regression for health care spending. Biometrics, 76(3):973-982,
2020.
11
Under review as a conference paper at ICLR 2021
Appendix
In the appendix, we give the proofs of the theorems and claims in our paper, the experimental details
and more experimental results.
A Missing Proofs
Lemma 3.1. Let Yb = h(X) ∈ R, then for a ∈ {0,1}, Wι(Da(Y), h]Da) ≤ PErrDa (h).
Proof. The prediction error conditioned on a ∈ {0, 1} is
ErrDa(h)= E[(Y - h(X))2M = a]
≥ E2[|Y - h(X)||A= a]
≥ ( inf	E[|Y - h(X)∣])2
Γ(Da(Y),Da(h(X)))
= W12(Da(Y ),h]Da).
Taking square root at both sides then completes the proof.
Theorem 3.1.	Let Yb = h(X) ∈ R, we have Err。。(h) +Err。](h) ≥ 2 [(Wι(Do(Y), Dι(Y))-
W1(h]D0,h]D1))+2.
Proof. Since Wι(∙, ∙) is a distance metric, the result follows immediately the triangle inequality and
Lemma 3.1:
W1(D0(Y),Dι(Y)) ≤ ,Err。。(h) + W1(h]D0,h]D1) + PErrDW∙
Rearrange the equation above and by AM-GM inequality, we have
W1(D0(Y), Dι(Y)) - W1(h]D0, h]Dι) ≤ PErrD°(h) + PErrDI (h) ≤ P2(Err。。(h) + Err。[ (h)).
Taking square at both sides then completes the proof.
Corollary 3.1. Let Y = h(X) ∈ R and α = D(A = 0) ∈ [0,1], we have ErrD(h) ≥ 1 min{α, 1 -
α} ∙ [(W1(D0(Y), Dι(Y)) - W1(h]D0,h]D1))+]2.
Proof. The joint error is
ErrD(h)
= α ErrD。 (h) + (1 - α) ErrD1 (h)
≥ min{α, 1 - α} ErrD。 (h) + ErrD1 (h)
≥ 1 min{α, 1 - α}[(W1(D0(Y),Dι(Y)) - W1(h]D0, h]Dι))+]2. (Theorem 3.1)
Lemma A.1. If Assumption 2.1 holds, then the following inequality holds: |ED。 [(h(X) -
ED1[Y |X])2]-ED1[(h(X)-ED1[Y |X])2]| ≤ 8M2dTV(D0(X), D1(X)).
Proof. First, we know that ∣∣h(X) - EDa[Y|X]k∞ ≤ 2M, ∀a ∈ {0,1}, since Ilhl∣∞ ≤ M and
|Y | ≤ M . Now it suffices to bound:
∣Edo [(h(X) - Edi [Y|X])2] - Edi [(h(X) - Edi [Y|X])2]|
= |h(h(X) -ED1[Y|X])2,dD0-dD1i|
≤ ∣∣h(X) - Ed1 [Y|X]∣∞∣dDo - dDJ∣ι	(Holder,s inequality)
≤ 4M2 ∣dD0 - dD1∣1	(Assumption 2.1)
=8M2dTV(D0(X),D1(X)).
Note that the last equation follows the definition of total variation distance.
12
Under review as a conference paper at ICLR 2021
Lemma A.2. If Assumption 2.1 holds, then the following inequality holds: ∣Ep0 [(h(X)-
EDCI [Y∣X])]2 - Edo [(h(X) - Ed1 [Y∣X])]21 ≤ 4ME。。[∣Ea, [YX] - Ep1 [Y∣X]∣].
Proof.
∣Edo [(h(X) - Edo [Y∣X])]2 - Edo [(h(X) - Edi [Y∣X])]2∣
=∣Edo [h2 (X) - 2h(X)Edo [Y∣X]+ ED0 [Y∣X] - h2(X) + 2h(X)Edi [Y∣X] - eD1 [Y∣X]]∣
≤ 2MEdo [∣Edo [Y∣X] - Edi [Y∣X]∣]+2ME。。[∣Ed0 [Y∣X] - EdJY∣X]∣]	(Assumption 2.1)
=4MEdo [∣Edo [Y∣X] - Edi [Y∣X]∣].
■
Theorem 3.2.	For any hypothesis H 3 h : X → Y, if the Assumption 2.1 holds, then:
∆Err(h) ≤ 8M2 dTV(D0(X), D1(X)) + ∣Edo [VarDo [Y∣X]] - Edi [VarDi [Y∣X]]∣
+ 4M min{EDo[∣EDo (Y∣X) - Edi (Y∣X)∣], EdJ∣Edo (Y∣X) - Edi (Y∣X)∣]}.
Proof. First, we show that for a ∈ {0,1},
ErrDa (h)
=EDa[(h(X ) - Y )2]
=EDa [(h(X) - EDa MX ]+ EDa MX ] - Y )2]
=EDa [(h(X) - ED。MX ])2]+ EDa [(Y - E。。[Y∣X])2]
-2 EDa [(h(X) - Ed。[YX ])(Y - EDa [Y X ])]
=EDa [(h(X) - EDa MX ])2]+ EDa [(Y - EDa [Y∣X])2]∙
Note that the last equation holds since
EDa [(h(X ) - EDa [Y∣X ])(Y - EDa [Y X ])]
=EDa(X)[EDa(y∣x)[(h(X) - Ed。[YX])(Y - EDa [Y∣X])∣X]]
=EDa(X) [(h(X) - EDa[Y∣X])EDa(y∣x)(Y - Ed。[Y∣X]∣X)]
=EDa(X)[(h(X) - Ed。[Y∣X])(Ed。[Y∣X] - Ed。[Y∣X])]
= 0.
Next we bound the error gap:
∣ErrDo(h) - Erm(h)∣
=∣Edo [(h(X) - Edo [Y∣X])2] - Edi [(h(X) - Edi [Y∣X])2]
+ Edo [(Y - Edo [Y∣X])2] - Edi [(Y - Edi [Y∣X])2]∣
≤ ∣Edo [(h(X) - Edo [Y∣X])2] - Edi [(h(X) - Edi [Y∣X])2]∣ (Triangle inequality)
+ ∣Edo [VarDo [Y∣X]] - Edi [VarDi [Y∣X]]∣.
Now it suffices to bound:
∣Edo [(h(X) - Edo [Y∣X])2] - Edi [(h(X) - Edi [Y∣X])2]∣
=∣Edo [(h(X) - Edo [Y∣X])2] - Edo [(h(X) - Edi [Y∣X])2]
+ Edo [(h(X) - Edi [Y∣X])2] - Edi [(h(X) - Edi [Y∣X])2]∣
≤ ∣Edo [(h(X) - Edo [Y∣X])2] - Edo [(h(X) - Edi [Y∣X])2]∣	(Triangleinequality)
+ ∣Edo [(h(X) - Edi [Y∣X])2] - Edi [(h(X) - Edi [Y∣X])2]∣.
Invoke Lemma A.1 and Lemma A.2 to bound the above two terms:
∣Edo [(h(X) - Edo [Y∣X])2] - Edo [(h(X) - Edi [Y∣X])2]∣
+ ∣Edo [(h(X) - Edi [Y∣X])2] - Edi [(h(X) - Edi [Y∣X])2]∣
≤ 4M Edo [∣Edo [Y∣X] - Edi [Y∣X]∣] + 8M2⅛V(D0(X), D1 (X)). (Lemma A.1 & Lemma A.2)
13
Under review as a conference paper at ICLR 2021
By symmetry, we also have:
|ED0[(h(X)-ED0[Y|X])2]-ED0[(h(X)-ED1[Y|X])2]|
+ |ED0 [(h(X) - ED1 [Y |X])2] - ED1 [(h(X) - ED1 [Y |X])2]|
≤ 4M ED1 [|ED0 [Y |X] - ED1 [Y |X]|] + 8M2dTV(D0(X), D1(X)). (Lemma A.1 & Lemma A.2)
Combining the two inequalities above together, we have:
|ED0(h(X)-ED0[Y|X])2-ED0(h(X)-ED1[Y|X])2|
+|ED0(h(X)-ED1[Y|X])2-ED1(h(X)-ED1[Y|X])2|
≤ 8M2dTV(D0(X),D1(X))
+ 4M min{ED0 [|ED0 [Y |X] -ED1[Y|X]|],ED1[|ED0[Y|X] -ED1[Y|X]|]}.
Incorporating the two variance terms back to the above inequality then completes the proof.
Theorem 3.3.	If Assumption 2.1 holds, then for ∀h ∈ H, let Yb = h(X), the following inequality
holds:
∆Err(h) ≤ 8M2dTV(D0(Y),D1(Y))
- -. -∙^∙- -∙^∙-.- -. -∙^∙- -∙^∙-.--
+3Mmin{ED0[|ED0y[Yb]-ED1y[Yb]|],ED1[|ED0y[Yb]-ED1y[Yb]|]}.
Proof. First, we show that for a ∈ {0, 1}:
ErrDa(h)=EDa[(h(X)-Y)2]=EDa[h2(X)-2Yh(X)+Y2]=EDa[h2(X)-2Yh(X)]+EDa[Y2].
Next, we bound the error gap:
|ErrD0 (h) - ErrD1 (h)|
= |ED0[h2(X)-2Yh(X)]+ED0[Y2]-ED1[h2(X)-2Yh(X)]-ED1[Y2]|
≤ |ED0[h2(X) - 2Y h(X)] -ED1[h2(X) - 2Y h(X)]| + |ED0[Y2] -ED1[Y2]|. (Triangle inequality)
For the second term, we can easily prove that
|ED0[Y2]-ED1[Y2]| = |hY 2, dD0 - dD1i| ≤ kY k2∞kdD0 - dD1k1 ≤ 2M2dTV(D0(Y),D1(Y)),
Where the second equation follows Holder,s inequality and the last equation follow the definition of
total variation distance. Now it suffices to bound the remaining term:
|ED0[h2(X)-2Yh(X)] -ED1[h2(X)-2Yh(X)]|
h(x)(h(x) - 2y) dμo(x, y) - / h(x)(h(x) - 2y) dμι(x, y)
h(x)(h(x) - 2y) dμo(x∣y)dμo(y) - 〃 h(x)(h(x) - 2y) dμo(x∣y)dμι(y)
+ J JJ h(x)(h(x) - 2y) dμι(x∣y)dμι(y) - JJ
h(x)(h(x) — 2y)dμo(x∣y)dμι(y).
We upper bound the first term:
≤
≤
≤
≤
≤
〃 h(x)(h(x) - 2y)dμo(x∣y)dμo(y) - 〃 h(x)(h(x) - 2y) dμ°(x∣y) dμι(y)
J'J' Ih(X)(h(x) - 2y)(dμo(y) - dμι(y)) ∣ dμo(x∣y)
/ ∣ dμo(y) - dμι(y)∣ / ∣ sup h(x)∣∣h(x) - 2y∣dμo(x∣y)
M / EDo[|h(X) - 2Y||Y = y] ∣ dμo(y) - dμι(y)∣
3M2 / ∣ dμo(y) - dμι(y)∣
6M2dTV(D0(Y),D1(Y)).
(Triangle inequality)
(Assumption 2.1)
(Assumption 2.1)
14
Under review as a conference paper at ICLR 2021
Note that the last equation follows the definition of total variation distance. For the second term, we
have:
h(x)(h(x)
-2y)dμι (x|y)
dμι(y)-〃
h(x)(h(x) - 2y) dμo(x∣y) dμι(y)
〃 h2(x)(dμι(x∣y) - dμo(x∣y)) dμι(y) +
-. -∙^∙- -∙^∙- ∙-
≤ 3M ED1 [|ED0y [Yb] - ED1y [Yb]|].
〃 2yh(x)(dμι(x∣y) - dμo(x∣y)) dμι(y)
(Triangle inequality)
(Assumption 2.1)
≤
To prove the last equation, we first see that:
〃 h2(x)(dμι(x∣y) - dμo(x∣y)) dμι(y)
≤ Jj (SUp h(x))h(x)(dμι(x|y) - dμo(x∣y)) dμι(y)
≤ M/ ∣ED°[h(X)|Y = y] - EDjh(X)|Y = y]∣ dμι(y)
-. -∙^∙- -∙^∙-.-
= M ED1 [|ED0y [Yb] - ED1y [Yb]|].
(Assumption 2.1)
Similarly, we also have:
〃 2y h(x)(dμι(x∣y) - dμo(x∣y)) dμι(y)
≤2
〃(supy)h(x)(dμι(x|y) - dμo(x∣y)) dμι(y)
≤ 2M
/ ∣Edo[h(X)|Y
=y] - EDi [h(X)IY = y]∣ dμι(y)
(Assumption 2.1)
-. -∙^∙- -∙^∙--
2M ED1 [IED0y [Yb] - ED1y [Yb]I].
By symmetry, we can also see that:
IED0[h2(X) - 2Yh(X)] -ED1[h2(X) -2Yh(X)]I ≤ 6M2dTV(D0(Y),D1(Y))+3MED1[IED0y[Yb] -ED1y[Yb]I].
Combine the above two equations yielding:
IED0[h2(X) - 2Yh(X)] -ED1[h2(X)-2Yh(X)]I
≤6M2dTV(D0(Y),D1(Y))+3Mmin{ED0[IED0y[Yb] - ED1y [Yb]I], ED1 [IED0y [Yb] - ED1y [Yb]I]}.
Incorporating the terms back to the upper bound of the error gap then completes the proof.
Theorem 3.4.	Consider the minimax game in (1). The equilibrium (g*, f *) of the game is attained
when 1). Z = g*(X) is independent of A conditioned on Y; 2). f *(Z, Y) = D(A = 1 ∣ Y, Z).
Proof. To prove Theorem 3.4, we first give Proposition A.1.
Proposition A.1. For any feature map g : X → Z, assume that F contains all the randomized binary
classifiers and F 3 f : Z × Y → A, then minf ∈F CED(A k f(g(X), Y)) = H(A I Z, Y).
Proof. By the definition of cross-entropy loss, we have:
CED(A k f) = -ED [I(A = 0)log(1 - f(g(X),Y)) + I(A = 1)log(f(g(X),Y))]
=-Eg]D[I(A=0)log(1-f(Z,Y))+I(A=1)log(f(Z,Y))]
= -EZ,Y EA|Z,Y [I(A=0)log(1-f(Z,Y))+I(A=1)log(f(Z,Y))]
= -EZ,Y [D(A=0I Z,Y)log(1-f(Z,Y))+D(A=1 IZ,Y)log(f(Z,Y))]
= EZ,Y [DKL(D(A I Z, Y) kf(Z,Y))]+H(AIZ,Y)
≥H(AIZ,Y),
where DKL(∙∣∣∙) denotes the KL divergence between two distributions. From the above inequality, it
is also clear that the minimum value of the cross-entropy loss is achieved when f(Z, Y) equals the
conditional probability D(A = 1 ∣ Z, Y), i.e., f * (Z, Y) = D(A = 1 ∣ Z = g(X), Y).	■
15
Under review as a conference paper at ICLR 2021
Proposition A.1 states that the minimum cross-entropy loss that the discriminator can achieve is
H(A | Z, Y) when f is the conditional distribution D(A = 1 | Z = g(X), Y). By the basic property
of conditional entropy, we have:
minCED(Akf(g(X),Y))=H(A|Z,Y)=H(A|Y)-I(A;Z|Y).
f∈F
Note that H(A | Y) is a constant given the distribution D, so the maximization of g is equivalent
to the minimization of minZ=g(X) I(A; Z | Y), and it follows that the optimal strategy for the
transformation g is the one that induces conditionally invariant features, e.g., I(A; Z | Y) = 0. On
the other hand, if g* plays optimally, then the optimal response of the discriminator f is given by
f *(Z,Y )= D(A =1 | Z = g* (X ),Y )= D(A =1 | Y).
Theorem 3.5.	Let g* := argmi、Wi(D0(g(X),Y),D∖(g(X),Y)), then DY(Z = g*(X))
DY(Z = g*(X)) almost surely.
Proof. By the definition of Wasstertein distance, we have:
W1(D0(Z,Y),D1(Z,Y))=γ∈Γ(iDnf0,D1)Zd((z0,y0),(z1,y1))dγ((z0,y0),(z1,y1))
inf
γ∈Γ(D0,D1)
inf
γ∈Γ(D0,D1)
Il
ZZ
d((z0,y0), (z1, y1)) dγ(z0, z1 | y0,y1)dγ(y0,y1)
kz0 - z1k1 + |y0 - y1| dγ(z0, z1 | y0,y1)dγ(y0,y1)
≥ inf	|y0 - y1 | dγ(y0, y1) dγ(z0, z1 | y0,y1)
γ∈Γ(D0,D1)
=Y∈Γ(D0iYf,D1(Y J1,0-"11 dγ(y0,yi)
= W1(D0(Y),D1(Y)).
To finish the proof, next We prove the lower bound is achieved when DY(Z = g* (X)) = DY(Z =
g*(X)): it is easy to see W1(D0Y(Z),D0Y(Z)) = R kz0 - z1k1 dγ(z0, z1 | y0,y1) = 0 when the
conditional distributions are equal. In this case, when the Wasserstein distance is minimized, then Z
is conditionally independent of A given Y .
B Experimental Details
Adult The Adult dataset contains 48,842 examples for income prediction. The task is to predict
whether the annual income of an individual is greater or less than 50K/year based on the attributes
of the individual, such as education level, age, occupation, etc. In our experiment, we use gender
(binary) as the sensitive attribute. The target variable (income) is an ordinal binary variable: 0 if <
50K/year otherwise 1. After data pre-processing, the dataset contains 30,162/15,060 training/test
instances where the input dimension of each instance is 113. We show the data distributions for
different demographic subgroups in Table 1.
To preprocess the dataset, we first filter out the data records that contain the missing values. We then
remove the sensitive attribute from the input features and normalize the input features with its means
and standard deviations. Note that we use one-hot encoding for the categorical attributes.
For our proposed methods, we use a three-layer neural network with ReLU as the activation function
of the hidden layers and the sigmoid function as the output function for the prediction task (we
take the first two layers as the feature mapping). The number of neurons in the hidden layers is
60. We train the neural networks with the Adadelta algorithm with the learning rate 0.1 and a
batch size of 512. The models are trained in 50 epochs. For the adversary networks in CENet and
WassersteinNet, we use a two-layer neural network with ReLU as the activation function. The
number of neurons in the hidden layers of the adversary networks is 60. The adversary network in
CENet also use sigmoid function as the output function. The weight clipping norm in the adversary
16
Under review as a conference paper at ICLR 2021
network of WassersteinNet is 0.005. We use the gradient reversal layer (Ganin et al., 2016) to
implement the gradient descent ascent (GDA) algorithm for optimization of the minimax problem
since it makes the training process more stable (Daskalakis & Panageas, 2018). For the rest of the
datasets we used in our experiments, we also use gradient reversal layer to implement our algorithms.
We use the Fairlearn toolkit (Bird et al., 2020) to implement BGL: we use the exponentiated-gradient
algorithm with the default setting as the mitigator and vary the upper bound ∈ {0.07, 0.1, 0.2, 0.5}
of the bounded group loss constraint. For each value of , we run ten random seeds and compute the
means and standard deviations.
COMPAS The COMPAS dataset 6,172 instances to predict whether a criminal defendant will
recidivate within two years or not. It contains attribute such as age, race, etc. In our experiment, we
use race (white or non-white) as the sensitive attribute and recidivism as the target variable. We split
the dataset into training and test set with the ratio 7/3. We show the data distributions for different
demographic subgroups in Table 2.
For all methods, we use a two-layer neural network with ReLU as the activation function of the
hidden layers and the sigmoid function as the output function for the prediction task (we take the
first layer as the feature mapping). The number of neurons in the hidden layers is 60. We train the
neural networks with the Adadelta algorithm with the learning rate 1.0 and a batch size of 512.
The models are trained in 50 epochs. For the adversary networks in CENet and WassersteinNet,
we use a two-layer neural network with ReLU as the activation function. The number of neurons
in the hidden layers of the adversary networks is 10. The adversary network in CENet also use
sigmoid function as the output function. The weight clipping norm in the adversary network of
WassersteinNet is 0.05.
We use the Fairlearn toolkit to implement BGL: we use the exponentiated-gradient algorithm with
the default setting as the mitigator and vary the upper bound ∈ {0.1, 0.2, 0.3, 0.5} of the bounded
group loss constraint. For each value of , we run ten random seeds and compute the means and
standard deviations.
As for CoD, we follow the source implementation.2 We use the same hyper-parameter settings
as (Komiyama et al., 2018): We use the kernelized optimization with the random Fourier features
and the RBF kernel (we vary hyper-parameter of the RBF kernel γ ∈ {0.1, 1.0, 10, 100}) and report
the best results with minimal MSE loss for each time we change the fairness budget . We also vary
∈ {0.01, 0.1, 0.5, 1.0} and run ten random seeds and compute the means and standard deviations.
Table 1: Data distribution of Y and A in Adult dataset.			Table 2: Data distribution of Y and A in COMPAS dataset.	
	Y = 0	Y=1	Y = 0	Y=1
A	=0 20988	9539	A = 0	1849	1148
A	= 1	13026	1669	A = 1	1514	1661
Communities and Crime The Communities and Crime dataset contains 1,994 examples of socio-
economic, law enforcement, and crime data about communities in the United States. The task is
to predict the number of violent crimes per 100K population. All attributes in the dataset have
been curated and normalized to [0, 1]. In our experiment, we use race (binary) as the sensitive
attribute: 1 if the population percentage of the white is greater or equal to 80% otherwise 0. After
data pre-processing, the dataset contains 1,595/399 training/test instances where the input dimension
of each instance is 96. We visualize the data distributions for different demographic subgroups in
Figure 3b.
To preprocess the dataset, we first remove the non-predictive attributes and sensitive attributes from
the input features. Note that all features in the dataset have already been normalized in [0, 1] so that
we do not perform additional normalization to the features. We then replace the missing values with
the mean values of the corresponding attributes.
For all methods, we use a two-layer neural network with ReLU as the activation function of the
hidden layers and the sigmoid function as the output function for the prediction task (we take the first
2https://github.com/jkomiyama/fairregresion
17
Under review as a conference paper at ICLR 2021
layer as the feature mapping). The number of neurons in the hidden layers is 50. We train the neural
networks with the Adadelta algorithm with the learning rate 0.1 and a batch size of 256. The
models are trained in 100 epochs. For the adversary networks in CENet and WassersteinNet,
we use a two-layer neural network with ReLU as the activation function. The number of neurons
in the hidden layers of the adversary networks is 100. The adversary network in CENet also use
sigmoid function as the output function. The weight clipping norm in the adversary network of
WassersteinNet is 0.002.
We use the Fairlearn toolkit to implement BGL: we use the exponentiated-gradient algorithm with the
default setting as the mitigator and vary the upper bound ∈ {0.01, 0.02, 0.03, 0.05} of the bounded
group loss constraint. For each value of , we run ten random seeds and compute the means and
standard deviations.
As for CoD, we follow the same hyper-parameter settings as (Komiyama et al., 2018): We use
the kernelized optimization with the random Fourier features and the RBF kernel (we vary hyper-
parameter of the RBF kernel γ ∈ {0.1, 1.0, 10, 100}) and report the best results with minimal
MSE loss for each time we change the fairness budget . The hyper-parameter settings follow
from (Komiyama et al., 2018). We also vary ∈ {0.01, 0.1, 0.5, 1.0} and run ten random seeds and
compute the means and standard deviations.
350
300
”50
a.
”。。
X
LU
⅛ 150
⅛
100
50
0
.050 .150 .25。.350 .450 .550 .650 .750 .850 .950
Y
I=I A=O
IZZI A=I
(b) Communities and Crime Dataset
(a) Law School Dataset
Figure 3:	Data distributions for different demographic subgroups in two datasets.
Law School The Law School dataset contains 1,823 records for law students who took the bar
passage study for Law School Admission3. The features in the dataset include variables such as
undergraduate GPA, LSAT score, full-time status, family income, gender, etc. In our experiment, we
use gender as the sensitive attribute and undergraduate GPA as the target variable. We split the dataset
into training and test set with the ratio 8/2. We show the data distributions for different demographic
subgroups in Figure 3a.
For all methods, we use a two-layer neural network with ReLU as the activation function of the
hidden layers and the sigmoid function as the output function for the prediction task (we take the first
layer as the feature mapping). The number of neurons in the hidden layers is 10. We train the neural
networks with the Adadelta algorithm with the learning rate 0.1 and a batch size of 256. The
models are trained in 100 epochs. For the adversary networks in CENet and WassersteinNet,
we use a two-layer neural network with ReLU as the activation function. The number of neurons
in the hidden layers of the adversary networks is 10. The adversary network in CENet also use
sigmoid function as the output function. The weight clipping norm in the adversary network of
WassersteinNet is 0.2.
We use the Fairlearn toolkit to implement BGL: we use the exponentiated-gradient algorithm with the
default setting as the mitigator and vary the upper bound ∈ {0.01, 0.02, 0.03, 0.05} of the bounded
group loss constraint. For each value of , we run ten random seeds and compute the means and
standard deviations.
3We use the edited public version of the dataset which can be download here: https://github.com/
algowatchpenn/GerryFair/blob/master/dataset/lawschool.csv
18
Under review as a conference paper at ICLR 2021
As for CoD, we follow the same hyper-parameter settings as (Komiyama et al., 2018): We use
the kernelized optimization with the random Fourier features and the RBF kernel (we vary hyper-
parameter of the RBF kernel γ ∈ {0.1, 1.0, 10, 100}) and report the best results with minimal
MSE loss for each time we change the fairness budget . The hyper-parameter settings follow
from (Komiyama et al., 2018). We also vary ∈ {0.01, 0.1, 0.5, 1.0} and run ten random seeds and
compute the means and standard deviations.
C Additional Experimental Results and Analyses
In this section, we provide additional experimental results and analyses.
C.1 Impact of Fairness Trade-off Parameters
We present additional experimental results and analyses to gain more insights into how the fairness
trade-off parameters (e.g., λ and ) affect the performance of the model predictive performance and
accuracy disparity in each methods.
Table 3:	R2 regression scores and error gaps when λ changes in CENET and WASSERSTEINNET.
Adult
COMPAS
λ	0.0	1.0	10	50	100
~p2~CENet	0.4419±0.0024	0.4179±0.0019	0.3908±0.0136	0.3440±0.0210	0.2813±0.0215
R WASSERSTEINNET	0.4419±0.0024	0.4388±0.0023	0.4136±O.OO32	0.3891±O.OO63	0.3653±O.O12O
ʌ CENET	0.0697±0.0004	0.0647±0.0010	0.0596±0.0027	0.0621±0.0057	0.0678 ±0.0051
∆Err WASSERSTEINNET	0.0697±0.0004	0.0691±0.0006	0.0698±0.0011	0.0631±0.0022	0.0592±0.0033
λ
■0.0	0.1	0.5	1.0	5.0
^p2^^CENET	0.1631±0.0127	0.1610±0.0119	0.1542±0.0129	0.1515±0.0125 0.1418±0.0151
R WassersteinNet	0.1631±0.0127	0.1645±0.0136	0.1564±0.0125	0.1471±0.0151 0.1439±0.0143
ʌ CENET	0.0088±0.0048	0.0075±0.0044	0.0067±0.0046	0.0066±0.0039 0.0063±0.0046
∆Err WASSERSTEINNET	0.0088±0.0048	0.0088±0.0045	0.0079±0.0041	0.0070±0.0036 0.0072±0.0032
λ
0.0
0.1
1.0
5.0
10
R2
CENET	0.5435±0.0077 0.5290±0.0107 0.1632±0.0573 0.1334±0.0720 0.1692±0.1509
Crime
Law
WASSERSTEINNET 0.5435±0.0077 0.5467±0.0063 0.5472±0.0065 0.5446±0.0091 0.5319±0.0143
ʌ CENET	0.0191±0.0003 0.0175±0.0004 0.0230±0.0027 0.0221±0.0079 0.0265±0.0051
∆Err WASSERSTEINNET 0.0191±0.0003 0.0194±0.0004 0.0191±0.0004 0.0180±0.0005 0.0173±0.0010
λ
■0.0	0.1	1.0	50	10
~p2^^CENET	0.1197±0.0314	0.120O±0.O299	0.1O59±O.O277	0.O464±O.O542	0.O235±O.O732
R WassersteinNet	0.1197±0.0314	0.1134±0.0339	0.0902±0.0292	0.0316±0.0476	0.0146±0.0553
ʌ CENET	0.0102±0.0010	0.0101±0.0009	0.O090±0.0O18	O.0070±0.003O	0.O066±0.003O
∆Err WAssERsTEiNNET	0.0102±0.0010	0.0098±0.0016	0.0090±0.0019	0.0072±0.0025	0.0069±0.0027
Table 3 shows R2 regression scores and error gaps when λ changes in CENET and WAssERsTEiN-
NET. We see that the error gap gradually decreases with the increase of the trade-off parameter λ
in most scenarios with small accuracy loss (except for CENet in Adult dataset and Crime dataset
when λ is large), which demonstrates the overall effectiveness of our proposed algorithms. Plus, the
increase of λ generally leads to the instability of training processes with larger variances of both
values of R2 and error gap. in contrast to WAssERsTEiNNET, CENET outperforms in mitigating
the accuracy disparity while achieving similar or better accuracy in COMPAs and Law dataset. in
Adult and Crime dataset, when λ is small, CENET also does better in reducing the error gap than
WassersteinNet with similar accuracy loss. The results follow the fact that minimizing total
variation distance between two continuous distributions ensures the minimization of Wasserstein
distance (Gibbs & su, 2002). However, when λ increases, WAssERsTEiNNET achieves better
accuracy and performance disparity trade-off while CENet suffers significant accuracy loss and may
fail to decrease the error gap. it is not surprising since the estimation of total variation in minimax
optimization could lead to an unstable training process (Arjovsky & Bottou, 2017; Arjovsky et al.,
2017).
Table 4 shows R2 regression scores and error gaps when changes in BGL. We see that with the
decrease of the trade-off parameter , both the values of R2 and error gaps decrease. This is because
when upper bound of BGL is small, the accuracy disparity is also mitigated. When is above/below
a certain threshold, the values of R2 and error gaps then increase/decrease. it is also worth to note
that the exponentiated-gradient approach to solve BGL does not introduce the randomness during
optimization.
19
Under review as a conference paper at ICLR 2021
Table 4:	R2 regression scores and error gaps when changes in BGL.
	€	0.07	0.1	0.2	0.5
Adult	Rr2^-	0.3508±0.0000	0.3508±0.0000	0.3696±0.0000	0.3696±0.0000
	△Err	0.0612±0.0000	0.0612±0.0000	0.0726±0.0000	0.0726±0.0000
	€	0.1	0.2 ɪɪ	0.3	0.5
COMPAS		0.1478±0.0000	0.1478±0.0000	0.1507±0.0000	0.1507±0.0000
	△Err	0.0072±0.0000	0.0072±0.0GGG-	0.0086±0.0000	0.0086±0.0000
	€	0.01	0.02 ɪɪ	0.03	0.05
Crime	~R	0.3922±0.0000	0.3922±0.0000	0.5380±0.0000	0.5380±0.0000
	△Err	0.0189±0.0000	0.0189±0.0000~	0.0238±0.0000	0.0238±0.0000
	€	0.01	0.02 ɪɪ	0.03	0.05
Law		0.1407±0.0000	0.1407±0.0000	0.1407±0.0000	0.1412±0.0000
	△Err	0.0094±0.0000	0.0094±0.0000-	0.0094±0.0000	0.0101±0.0000
Table 5: R2 regression scores and error gaps when changes in COD.					
	€	0.01	0.1	0.5	1.0
COMPAS	~R^-	0.1033±0.0111	0.1144±0.0100	0.1146±0.0099	0.1146±0.0099
	△Err	0.0064±0.0042	0.0083±0.0058-	0.0085±0.0060	0.0085±0.0060
	€	0.01	0.1 ɪɪ	0.5	1.0
Crime		0.1262±0.0000	0.3284±0.0000	0.3603±0.0000	0.3603±0.0000
	△Err	0.0312±0.0000	0.0307±0.0000~	0.0343±0.0000	0.0343±0.0000
	€	0.01	0.1 ɪɪ	0.5	1.0
Law	~R^-	0.1262±0.0000	0.3284±0.0000	0.3606±0.0000	0.3603±0.0000
	△Err	0.0312±0.0000	0.0307±0.0000-	0.0343±0.0000	0.0343±0.0000
Table 5 shows R2 regression scores and error gaps when changes in COD. We see that with the
decrease of the trade-off parameter , both the values of R2 and error gaps decrease. It is worth to
note that the the optimization of QCQP to solve CoD does not introduce the randomness, and the
only randomness introduced in COMPAS dataset is because using the random Fourier features in
prediction achieves the best performance in COMPAS dataset.
C.2 Visualization of Training Processes
We visualize the training processes of our proposed methods CENet and WassersteinNet in the
Adult dataset and COMPAS dataset in Figure 4 and Figure 5, respectively. We also compare their
training dynamics with the model performance that we solely minimize the MSE loss (i.e., λ = 0)
and we term it as No Debias.
(a) MSE Loss
Figure 4: Training visualization of CENET, WASSERSTEINNET (λ = 50) and NO DEBIAS (λ = 0)
in the Adult dataset.
(b) Error Gap
20
Under review as a conference paper at ICLR 2021
msσl WSW
(a) MSE Loss
Figure 5: Training visualization of CENET, WASSERSTEINNET (λ = 5) and NO DEBIAS (λ = 0)in
the COMPAS dataset.
(b) Error Gap
In Figure 4 and Figure 5, we can see that as the training progress, the MSE losses in both datasets
are decreasing and finally converge. However, the training dynamics of error gaps are much more
complex even in the No Debias case. Before convergence, the training dynamics of error gaps
differs among different datasets. Our methods enforce the models to converge to the points where
error gap are smaller while preserving the models’ predictive performance. It is also worth to note
that minimax optimization makes the training processes somehow unstable.
21