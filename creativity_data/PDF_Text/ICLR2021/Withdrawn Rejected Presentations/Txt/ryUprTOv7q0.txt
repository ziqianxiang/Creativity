Under review as a conference paper at ICLR 2021
Quantum Deformed Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We develop a new quantum neural network layer designed to run efficiently on
a quantum computer but that can be simulated on a classical computer when re-
stricted in the way it entangles input states. We first ask how a classical neural
network architecture, both fully connected or convolutional, can be executed on a
quantum computer using quantum phase estimation. We then deform the classical
layer into a quantum design which entangles activations and weights into quantum
superpositions. While the full model would need the exponential speedups deliv-
ered by a quantum computer, a restricted class of designs represent interesting
new classical network layers that still use quantum features. We show that these
quantum deformed neural networks can be trained and executed on normal data
such as images, and even classically deliver modest improvements over standard
architectures.
1 Introduction
Quantum mechanics (QM) is the most accurate description for physical phenomena at very small
scales, such as the behavior of molecules, atoms and subatomic particles. QM has a huge impact on
our every day lives through technologies such as lasers, transistors (and thus microchips), supercon-
ductors and MRI.
A recent view of QM has formulated it as a (Bayesian) statistical methodology that only describes
our subjective view of the (quantum) world, and how we update that view in light of evidence
(i.e. measurements) (;t Hooft, 2016; Fuchs & Schack, 2013). This is in perfect analogy to the
classical Bayesian view, a statistical paradigm extensively used in artificial intelligence where we
maintain probabilities to represent our beliefs for events in the world.
The philosophy of this paper will be to turn this argument on its head. If we can view QM as just
another consistent statistical theory that happens to describe nature at small scales, then we can
also use this theory to describe classical signals by endowing them with a Hilbert space structure.
In some sense, the ’only’ difference with Bayesian statistics is that the positive probabilities are
replaced with complex ’amplitudes’. This however has the dramatic effect that, unlike in classical
statistics, interference between events now becomes a possibility. In this paper we show that this
point of view uncovers new architectures and potential speedups for running neural networks on
quantum computers.
We shall restrict our attention here to binary neural networks. We will introduce a new class of quan-
tum neural networks and interpret them as generalizations of probabilistic binary neural networks,
discussing potential speedups by running the models on a quantum computer. Then we will devise
classically efficient algorithms to train the networks for a restricted set of quantum circuits. We
present results of classical simulations of the quantum neural networks on real world data sizes and
related gains in accuracy due to the quantum deformations. Contrary to almost all other works on
quantum deep learning, our quantum neural networks can be simulated for practical classical prob-
lems, such as images or sound. The quantum nature of our models is there to increase the flexibility
of the model-class and add new operators to the toolbox of the deep learning researcher, some of
which may only reach their full potential when quantum computing becomes ubiquitous.
1
Under review as a conference paper at ICLR 2021
1.1 Related work
In Farhi & Neven (2018) variational quantum circuits that can be learnt via stochastic gradient
descent were introduced. Their performance could be studied only on small input tasks such as
classifying 4 × 4 images, due to the exponential memory requirement to simulate those circuits.
Other works on variational quantum circuits for neural networks are Verdon et al. (2018); Beer et al.
(2019). Their focus is similarly on the implementation on near term quantum devices and these
models cannot be efficiently run on a classical computer. Exceptions are models which use tensor
network simulations (Cong et al., 2019; Huggins et al., 2019) where the model can be scaled to 8 × 8
image data with 2 classes, at the price of constraining the geometry of the quantum circuit (Huggins
et al., 2019). The quantum deformed neural networks introduced in this paper are instead a class of
variational quantum circuits that can be scaled to the size of data that are used in traditional neural
networks as we demonstrate in section 4.2.
Another line of work directly uses tensor networks as full precision machine learning models that
can be scaled to the size of real data (Miles Stoudenmire & Schwab, 2016; Liu et al., 2017; Levine
et al., 2017; Levine et al., 2019). However the constraints on the network geometry to allow for
efficient contractions limit the expressivity and performance of the models. See however Cheng
et al. (2020) for recent promising developments. Further, the tensor networks studied in these works
are not unitary maps and do not directly relate to implementations on quantum computers.
A large body of work in quantum machine learning focuses on using quantum computing to provide
speedups to classical machine learning tasks (Biamonte et al., 2017; Ciliberto et al., 2018; Wiebe
et al., 2014), culminating in the discovery of quantum inspired speedups in classical algorithms
(Tang, 2019). In particular, (Allcock et al., 2018; Cao et al., 2017; Schuld et al., 2015; Kerenidis
et al., 2019) discuss quantum simulations of classical neural networks with the goal of improving the
efficiency of classical models on a quantum computer. Our models differ from these works in two
ways: i) we use quantum wave-functions to model weight uncertainty, in a way that is reminiscent
of Bayesian models; ii) we design our network layers in a way that may only reach its full potential
on a quantum computer due to exponential speedups, but at the same time can, for a restricted
class of layer designs, be simulated on a classical computer and provide inspiration for new neural
architectures. Finally, quantum methods for accelerating Bayesian inference have been discussed in
Zhao et al. (2019b;a) but only for Gaussian processes while in this work we shall discuss relations
to Bayesian neural networks.
2 Generalized probabilistic b inary neural networks
Binary neural networks are neural networks where both weights and activations are binary. Let
B = {0,1}. A fully connected binary neural network layer maps the n` activations h(') at level '
to the N'+ι activations h('+1) at level' + 1 using weights W(') ∈ Bn' n'+1 :
h尸=f(W ('), h(')) = T (N1+1 X W") , T (x) = {0 X < I .	(1)
We divide by n` + 1 since the sum can take the n` + 1 values {0,..., N}. We do not explicitly
consider biases which can be introduced by fixing some activations to 1. In a classification model
h(0) = x is the input and the last activation function is typically replaced by a softmax which pro-
duces output probabilities p(y|x, W), where W denotes the collection of weights of the network.
Given M input/output pairs X = (x1, . . . , xM), Y = (y1, . . . , yM), a frequentist approach would
determine the binary weights so that the likelihood p(Y |X, W) = QiM=1 p(yi|xi, W) is maxi-
mized. Here we consider discrete or quantized weights and take the approach of variational opti-
mization Staines & Barber (2012), which introduces a weight distribution qθ (W) to devise a sur-
rogate differential objective. For an objective O(W), one has the bound maxW ∈BN O(W) ≥
Eqθ(W) [O(W)], and the parameters of qθ(W) are adjusted to maximize the lower bound. In our
case we consider the objective:
M
max log p(Y |X, W) ≥ L := Eqθ (W) [log p(Y |X, W)] = X Eqθ(W) [logp(yi|xi, W)] . (2)
W∈BN
i=1
2
Under review as a conference paper at ICLR 2021
While the optimal solution to equation 2 is a Dirac measure, one can add a regularization term R(θ)
to keep q soft. In appendix A we review the connection with Bayesian deep learning, where qθ(W)
is the approximate posterior, R(θ) is the KL divergence between qθ(W) and the prior over weights,
and the objective is derived by maximizing the evidence lower bound.
In both variational Bayes and variational optimization frameworks for binary networks, we have
a variational distribution q(W) and probabilistic layers where activations are random variables.
We consider an approximate posterior factorized over the layers: q(W) = QL=I q(')(W(')). If
h(E)〜P(E), equation 1 leads to the following recursive definition of distributions:
p('+1)(h('+1))= X X	δ(h('+1) - f(W(E), h(')))p(')(h('))q(')(w(')).	(3)
h∈BN' W∈bn'n'+i
We use the shorthand p(E) (h(E)) for p(E)(h(E) |x) and the x dependence is understood. The average
appearing in equation 2 can be written as an average over the network output distribution:
Eqθ(w)[logp(yi∣Xi, W)] = -Ep(L)(h(L))[gi(yi, h(L))],	(4)
where the function gi is typically MSE for regression and cross-entropy for classification.
In previous works (Shayer et al., 2017; Peters & Welling, 2018), the approximate posterior was taken
to be factorized: q(W (E)) = Qij qi,j(Wi(,Ej)), which results in a factorized activation distribution as
well: p(E)(h(E)) = Qipi(E)(hi(E)). (Shayer et al., 2017; Peters & Welling, 2018) used the local
reparameterization trick Kingma et al. (2015) to sample activations at each layer.
The quantum neural network we introduce below will naturally give a way to sample efficiently from
complex distributions and in view of that we here generalize the setting: we act with a stochastic
matrix Sφ(h0, W0|h, W) which depends on parameters φ and correlates the weights and the input
activations to a layer as follows:
πφ,θ(h0,W0) = X X	Sφ(h0, W0|h, W)p(h)qθ(W).	(5)
h∈BN W∈BNM
To avoid redundancy, we still take qθ (W) to be factorized and let S create correlation among the
weights as well. The choice of S will be related to the choice of a unitary matrix D in the quantum
circuit of the quantum neural network. A layer is now made of the two operations, Sφ and the layer
map f, resulting in the following output distribution:
p(E+1)(h(E+1)) = X X	δ(h(E+1) - f(W(E),h(E)))πφ(E,)θ(h(E), W(E)) ,	(6)
h∈BN W∈Bn'n'+i
which allows one to compute the network output recursively. Both the parameters φ and θ will be
learned to solve the following optimization problem:
min R(θ) + R0 (φ) - L .	(7)
θ,φ
where R(θ), R0(φ) are regularization terms for the parameters θ, φ. We call this model a generalized
probabilistic binary neural network, with φ deformation parameters chosen such that φ = 0 gives
back the standard probabilistic binary neural network.
To study this model on a classical computer we need to choose S which leads to an efficient sampling
algorithm for πφ,θ . In general, one could use Markov Chain Monte Carlo, but there exists situations
for which the mixing time of the chain grows exponentially in the size of the problem (Levin & Peres,
2017). In the next section we will show how quantum mechanics can enlarge the set of probabilistic
binary neural networks that can be efficiently executed and in the subsequent sections we will show
experimental results for a restricted class of correlated distributions inspired by quantum circuits
that can be simulated classically.
3	Quantum implementation
Quantum computers can sample from certain correlated distributions more efficiently than classical
computers (Aaronson & Chen, 2016; Arute et al., 2019). In this section, we devise a quantum circuit
3
Under review as a conference paper at ICLR 2021
that implements the generalized probabilistic binary neural networks introduced above, encoding
πθ,φ in a quantum circuit. This leads to an exponential speedup for running this model on a quantum
computer, opening up the study of more complex probabilistic neural networks.
A quantum implementation of a binary perceptron was introduced in Schuld et al. (2015) as an
application of the quantum phase estimation algorithm (Nielsen & Chuang, 2000). However, no
quantum advantage of the quantum simulation was shown. Here we will extend that result in several
ways: i) we will modify the algorithm to represent the generalized probabilistic layer introduced
above, showing the quantum advantage present in our setting; ii) we will consider the case of multi
layer percetrons as well as convolutional networks.
3.1	Introduction to quantum notation and quantum phase estimation
As a preliminary step, we introduce notations for quantum mechanics. We refer the reader to Ap-
pendix B for a more thorough review of quantum mechanics. A qubit is the vector space of nor-
malized vectors ∣ψi ∈ C2. N qubits form the set of unit vectors in (C2)0N = C2 spanned by
all N-bit strings, |bi,..., bN)≡ ∣bιi 0∙∙∙0 |bN)，b ∈ B. Quantum circuits are unitary matrices
on this space. The probability of a measurement with outcome φi is given by matrix element of
the projector ∣φii hφi∣ in a state ∣ψi, namely Pi = hψ∖φi) hΦi∣Ψi = | hφi∣ψi∣2, a formula known as
Born’s rule.
Next, we describe the quantum phase estimation (QPE), a quantum algorithm to estimate the eigen-
phases of a unitary U. Denote the eigenvalues and eigenvectors of U by exp(2∏ti夕ɑ) and ∖vɑi, and
assume that the φas can be represented with a finite number t of bits:夕a = 2t-1 夕3 +-+2%%.
(This is the case of relevance for a binary network.) Then introduce t ancilla qubits in state ∖0i0t.
Given an input state ∖ψi, QPE is the following unitary operation:
∖0i0t 0 ∖Ψi -PE X hVα∖Ψi ∖ψαi 0 ∖Vɑi .	(8)
α
Appendix B.1 reviews the details of the quantum circuit implementing this map, whose complexity
is linear in t. Now using the notation τ for the threshold non-linearity introduced in equation 1, and
recalling the expansion 2-t夕 =2-1 夕1 + •…+ 2-tφt, We note that if the first bit 夕1 = 0 then
2-t夕 < 1 and T(2-t夕)=0, while if 夕1 = 1, then 2-t夕 ≥ 2 and T(2-t夕)=1. In other words,
δφι,b = δτ(2-tφ),b and the probability p(b) that after the QPE the first ancilla bit is b is given by:
X
hvα∖ψi hψα∖ 0 hvɑ∖)[ ∖bihb∖0 1] (X hvβ ∖ψi ∖ψ β i0∖vβ i) = X ∖hvɑ ∖ψi∖2δτ (2S),b ,⑼
α	βα
where ∖bihb∖ 0 1 is an operator that projects the first bit to the state ∖bi and leaves the other bits
untouched.
3.2	Definition and advantages of quantum deformed neural networks
Armed with this background, we can now apply quantum phase estimation to compute the output of
the probabilistic layer of equation 6. Let N be the number of input neurons and M that of output
neurons. We introduce qubits to represent inputs and weights bits:
N	NM
∖h,Wi ∈Vh0VW, Vh = O(C2)i,	VW = O O(C2)ij.	(10)
i=1	i=1 j=1
Then we introduce a Hamiltonian Hj acting non trivially only on the N input activations and the N
weights at the j -th row:
N
Hj = X BjWi Bih,	(11)
i=1
and Bih (BjWi ) is the matrix B = ∖1i h1∖ acting on the i-th activation (ji-th weight) qubit. Note that
Hj singles out terms from the state ∖h, W i where both hj = 1 and Wij = 1 and then adds them
4
Under review as a conference paper at ICLR 2021
|0i
|0i®t2-1

|0i
|0i®ti-1
-n)ωdo
-j ~l-llgldo 一
|0i —
|0i 馋 tι-1 —
|xi 一
lψW1,: i —
lψW1,: i —
lψw”一
Layer 1 Layer 2
(b)
Figure 1: (a) Quantum circuit implementing a quantum deformed layer. The thin vertical line indi-
cates that the gate acts as identity on the wires crossed by the line. (b) Quantum deformed multilayer
perceptron with 2 hidden quantum neurons and 1 output quantum neuron. |xi is an encoding of the
input signal, y is the prediction. The superscript ' in Uj and Wj,： refers to layer '. We split the
blocks of t` ancilla qubits into a readout qubit that encodes the layer output amplitude and the rest.
(c) Modification of a layer for classical simulations.
up, i.e. the eigenvalues of Hj are the preactivations of equation 1:
N
Hj |h, Wi =以h, Wj,：) |h, Wi ,以h, Wj,：) = X Wjihi.	(12)
i=1
Now define the unitary operators:
Uj = De N2+iι Hj D-1,	(13)
where D is another generic unitary, and as we shall see shortly, its eigenvectors will be related to the
entries of the classical stochastic matrix S in section 2. Since UjUjo = DeN2+1 (Hj +Hj0 )D-1 =
2πi ττ
Ujo Uj, We can diagonalize all the Uj,s simultaneously and since they are conjugate to e N+1 j they
will have the same eigenvalues. Introducing the eigenbasis |h, W iD = D |h, W i, we have:
Uj |h, W〉d = eNΝ+1 奴h，Wj，:) |h, W〉d .	(14)
Note that 夕 ∈ {0,..., N} so we can represent it with exactly t bits, N = 2t - 1. Then we add M
ancilla resources, each of t qubits, and sequentially perform M quantum phase estimations, one for
each Uj , as depicted in figure 1 (a). We choose the following input state
M	N
lψi =	lψih	X O lψiWj,:	,	lψiWj,:	= O	qjqji(wji	= 0)	|0i	+	qjqji(wji	= 1) |1i , (15)
where we have chosen the weight input state according to the factorized variational distribution qij
introduced in section 2. In fact, this state corresponds to the following probability distribution via
Born’s rule:
MN
p(h, W) = | hh, WIΨi |2 = p(h) Y Y qji(Wji),	p(h) = | hh∣ψih |2 .	(16)
j=1 i=1
The state ∣ψhi is discussed below. Now we show that a non-trivial choice of D leads to an ef-
fective correlated distribution. The j -th QPE in figure 1 (a) corresponds to equation 8 where we
identify ∣Vɑi ≡ |h, W)口，|夕仪)≡ 3(h, Wj,：)i and we make use of the j-th block of t ancillas.
After M steps we compute the outcome probability of a measurement of the first qubit in each of
the M registers of the ancillas. We can extend equation 9 to the situation of measuring multiple
qubits, and recalling that the first bit of an integer is the most significant bit, determining whether
2-tφ(h, Wj,：) = (N + 1)-1φ(h, Wj,：) is greater or smaller than 1/2, the probability of outcome
h0 = (h01, . . . , h0M) is
P(h0) = X X δho,f(w,h)∣hψ∣h, W〉d∣2 ,	(17)
h∈BN W∈BNM
5
Under review as a conference paper at ICLR 2021
where f is the layer function introduced in equation 1. We refer to appendix C for a detailed
derivation. Equation 17 is the generalized probabilistic binary layer introduced in equation 6 where
D corresponds to a non-trivial S and a correlated distribution when D entangles the qubits:
n(h, W) = lhΨI D |h, Wi∣2 .	(18)
The variational parameters φ of S are now parameters of the quantum circuit D. Sampling from
π can be done by doing repeated measurements of the first M ancilla qubits of this quantum cir-
2πi ττ
cuit. On quantum hardware e N+1 j can be efficiently implemented since it is a product of diagonal
two-qubits quantum gates. We shall consider unitaries D which have efficient quantum circuit ap-
proximations. Then computing the quantum deformed layer output on a quantum computer is going
to take time O(tM u(N)) where u(N) is the time it takes to compute the action of Uj on an input
state. There exists D such that sampling from equation 18 is exponentially harder classically than
quantum mechanically, a statement forming the basis for quantum supremacy experiments on noisy,
intermediate scale quantum computers (Aaronson & Chen, 2016; Arute et al., 2019). Examples
are random circuits with two-dimensional entanglement patterns, which from a machine learning
point of view can be natural when considering image data. Other examples are D implementing
time evolution operators of physical systems, whose simulation is exponentially hard classically,
resulting in hardness of sampling from the time evolved wave function. Quantum supremacy exper-
iments give foundations to which architectures can benefit from quantum speedups, but we remark
that the proposed quantum architecture, which relies on quantum phase estimation, is designed for
error-corrected quantum computers.
Even better, on quantum hardware we can avoid sampling intermediate activations altogether. At
the first layer, the input can be prepared by encoding the input bits in the state |xi. For the next
layers, we simply use the output state as the input to the next layer. One obtains thus the quantum
network of figure 1 (b) and the algorithm for a layer is summarized in procedure 1. Note that all
the qubits associated to the intermediate activations are entangled. Therefore the input state ∣ψhi
would have to be replaced by a state in Vh plus all the other qubits, where the gates at the next layer
would act only on Vh in the manner described in this section. (An equivalent and more economical
mathematical description is to use the reduced density matrix ρh as input state.) We envision two
other possible procedures for what happens after the first layer: i) we sample from equation 17 and
initialize ∣ψhi to the bit string sampled in analogy to the classical quantization of activations; ii) We
sample many times to reconstruct the classical distribution and encode it in ∣ψhi. In our classical
simulations below we will be able to actually calculate the probabilities and can avoid sampling.
Finally, we remark that at present it is not clear whether the computational speedup exhibited by our
architecture translates to a learning advantage. This is an outstanding question whose full answer
will require an empirical evaluation with a quantum computer. Next, we will try to get as close as
possible to answer this question by studying a quantum model that we can simulate classically.
3.3	Modifications for classical simulations
In this paper we will provide classical simulations of the quantum neural networks introduced above
for a restricted class of designs. We do this for two reasons: first to convince the reader that the
quantum layers hold promise (even though we can not simulate the proposed architecture in its full
glory due to the lack of access to a quantum computer) and second, to show that these ideas can be
interesting as new designs, even “classically” (by which we mean architectures that can be executed
on a classical computer).
To parallelize the computations for different output neurons, we do the modifications to the setup
just explained which are depicted in figure 1 (c). We clone the input activation register M times,
an operation that quantum mechanically is only approximate (Nielsen & Chuang, 2000) but exact
classically. Then we associate the j-th copy to the j-th row of the weight matrix, thus forming pairs
for each j = 1, . . . , M :
N
|h, Wj,：〉∈ Vh 0 Vw,j ,	VWj = O(C2)ji	(19)
i=1
6
Under review as a conference paper at ICLR 2021
Fixing j, We introduce the unitary eN2πi1 Hj diagonal in the basis |h, Wj,：i as in equation 11 and
define the new unitary:
(Jj = Dj e 普ι Hj D-1,	(20)
where w.r.t. equation 13 we now let Dj depend on j. We denote the eigenvectors of Uj by
|h, Wj ：〉d = Dj ∣h, Wj-,：i and the eigenvalue is 夕(h, Wj-,：) introduced in equation 12. Supposing
that we know p(h) = Qi Pi(hi), we apply the quantum phase estimation to (Jj with input:
N
lψji =	lψhi 乳	lψiwj,:	,	lψhi	= O [pPi(hi	= 0)	|0i	+	Ppi(hi	= 1	|li]	,	QI)
i=1
and ∣Ψiw八 is defined in equation 15. Going through similar calculations as those done above
shows that measurements of the first qubit will be governed by the probability distribution of equa-
tion 6 factorized over output channels since the procedure does not couple them: π(h, W ) =
QM=IKψj| Dj |h, Wj,：i|2. SOfar, we have focused on fully connected layers. We can extend
the derivation of this section to the convolution case, by applying the quantum phase estimation on
images patches of the size equal to the kernel size as explained in appendix D.
4	Classical simulations for low entanglement
4.1	Theory
It has been remarked in (Shayer et al., 2017; Peters & Welling, 2018) that when the weight and ac-
tivation distributions at a given layer are factorized, p(h) = Qipi(hi) and q(W ) = Qij qij (Wij),
the output distribution in equation 3 can be efficiently approximated using the central limit theorem
(CLT). The argument goes as follows: for each j the preactivations 夕(h, Wj-,：) = PN=I Wj,ihi are
sums of independent binary random variables Wj,i hi with mean and variance:
μji = Ew〜qji (W)Eh~Pi (h)
,	σji = Ew 〜qji(W2 )Eh~pi (h2 ) - μji = μji(I- μji) ,	(22)
We used b2 = b for a variable b ∈ {0, 1}. The CLT implies that for large N we can approximate
夕(h, Wj,:) with a normal distribution with mean μj- = Pi μ* and variance σj2 = Pi σjj. The
distribution of the activation after the non-linearity of equation 1 can thus be computed as:
p(τ(n+1 φ(h, Wj,:)) = 1)= p(2φ(h, Wj,：) — N > 0) = Φ (-⅜-N) ,	(23)
Φ being the CDF of the standard normal distribution. Below we fixj and omit it for notation clarity.
As reviewed in appendix B, commuting observables in quantum mechanics behave like classical
random variables. The observable of interest for us, DHD-1 of equation 20, is a sum of commut-
ing terms Ki ≡ DBiW BihD-1 and if their joint probability distribution is such that these random
variables are weakly correlated, i.e.
hψ∣ KiK ∣Ψi-hΨ∣ Ki ∣ΨihΨ∣ Ki0 ∣ψi→ 0, if |i - i0∣→∞,	(24)
then the CLT for weakly correlated random variables applies, stating that measurements of
DHDT in state ∣ψi are governed by a Gaussian distribution N(μ, σ2) with
μ = hψ∣ DHD-1 ∣ψi ,	σ2 = hψ∣ DH2D-1 ∣ψ) - μ2 .	(25)
Finally, we can plug these values into equation 23 to get the layer output probability.
We have cast the problem of simulating the quantum neural network to the problem of computing the
expectation values in equation 25. In physical terms, these are related to correlation functions of H
and H2 after evolving a state ∣ψi with the operator D. These can be efficiently computed classically
for one dimensional and lowly entangled quantum circuits D (Vidal, 2003). In view of that here we
consider a 1d arrangement of activation and weight qubits, labeled by i = 0, . . . , 2N - 1, where the
even qubits are associated with activations and the odd are associated with weights. We then choose:
N-1	N-1
D=YQ2i,2i+1 Y P2i+1,2i+2 ,	(26)
i=0	i=0
7
Under review as a conference paper at ICLR 2021
where Q2i,2i+1 acts non-trivially on qubits 2i, 2i + 1, i.e. onto the i-th activation and i-th weight
qubits, while P2i,2i+1 on the i-th weight and i + 1-th activation qubits. We depict this quantum
circuit in figure 2 (a). As explained in detail in appendix E,the computation of μ involves the matrix
element of Ki in the product state ∣ψi while σ2 involves that of KiKi+1. Due to the structure
of D, these operators act locally on 4 and 6 sites respectively as depicted in figure 2 (b)-(c). This
D
2i-1 2i 2i+12i+12
(a)
(b)
012345
Figure 2: (a) The entangling circuit D for N = 3. (b) Ki entering the computation of μ. (c)
KiKi+1 entering the computation of σ2. Indices on P, Q, B are omitted for clarity. Time flows
downwards.
KiKi+1
(c)
implies that the computation of equation 25, and so of the full layer, can be done in O(N) and easily
parallelized. Appendix E contains more details on the complexity, while Procedure 2 describes the
algorithm for the classical simulation discussed here.
Procedure 1 Quantum deformed layer. QPEj(U , I) is quantum phase estimation for a unitary U
acting on the set I of activation qubits and the j-th weights/ancilla qubits. Hj is in equation 11.
Input： {qij}j=0,,...;M-11, ∣ψi, I, D,t
Output: ∣ψi
for j = 0 to M 一 1 do
lψiWj : J Ni=I [√qji |0i + p1 - qji |1i]
IΨi J |0i：③ IΨi ③ MWj,:
U J DeN2+1 HjDT {This requires to approximate the unitary with quantum gates}
IΨiJ QPEj (U, I) IΨi
end for
Procedure 2 Classical simulation of a quantum deformed layer with N (M) inputs (outputs).
Input: {qij}ij==00,, ,,NM--11, {pi}i=0,...,N-1, P = {P2ji-1,2i}ij==10,, ,,NM--11, Q = {Qj2i,2i+1}ij==00,, ,,NM--11
Output: {p0i}i=1,...,M
for j = 0 to M 一 1 do
for i = 0 to N - 1 do
ψ2i J [√pi, √1 - Pi]
ψ2i+1 J [√qij, p1 - qij]
end for
for i = 0 to N - 1 do
μi J computeMu(i, ψ, P, Q) {This implements equation 45 of appendix E}
γi,i+1 J computeGamma(i, ψ, P, Q) {This implements equation 49 of appendix E}
end for
μ J Pi=01 μi
σ2 J 2 Pi=02 (Yi,i+1 - μiμi+1) + Pi=01 (μi - μ2)
PjJ Φ (- W)
end for
8
Under review as a conference paper at ICLR 2021
4.2 Experiments
We present experiments for the model of the
previous section. At each layer, qij and Dj
are learnable. They are optimized to minimize
the loss of equation 7 where following (Peters
& Welling, 2018; Shayer et al., 2017) we take
R = β P',i,j qi? (1 - q(f), and R0 is the L?
regularization loss of the parameters of Dj . L
coincides with equation 2. We implemented
and trained several architectures with different
deformations. Table 1 contains results for two
standard image datasets, MNIST and Fashion
MNIST. Details of the experiments are in ap-
pendix F. The classical baseline is based on
(Peters & Welling, 2018), but we use fewer lay-
ers to make the simulation of the deformation
cheaper and use no batch norm, and no max
pooling.
The general deformation ([PQ]) performs best
in all cases. In the simplest case of a single
dense layer (A), the gain is +3.2% for MNIST
and +2.6% for Fashion MNIST on test accu-
Table 1: Test accuracies for MNIST and Fashion
MNIST. With the notation cKsS - C to indicate
a conv2d layer with C filters of size [K, K] and
stride S, and dN for a dense layer with N output
neurons, the architectures (Arch.) are A: d10; B:
c3s2-8, c3s2-16, d10; C: c3s2-32, c3s2-64, d10.
The deformations are: [/]: Pij,i+1 = Qij,i+1 =
1 (baseline (Peters & Welling, 2018)); [PQ]:
Pij,i+1, Qij,i+1 generic; [Q]: Pij,i+1 = 1, Qij,i+1
generic.
Arch. Deformation	MNIST	Fashion MNIST
A	[/]	91.1	84.2
[PQ]	94.3	86.8
[Q]	91.6	85.1
B	[/, /, /]	96.6	87.5
[PQ, /, /]	97.6	88.1
[Q, /, /]	96.8	87.8
C	[/, /, /]	98.1	89.3
[PQ, /, /]	98.3	89.6
racy. For Convnets, We could only simulate a [Q,/,/]	98.3	89.5
single deformed layer due to computational is-
sues and the gain is around or less than 1%.
We expect that deforming all layers Will give
a greater boost as the improvements diminish With decreasing the ratio of deformation parameters
over classical parameters (qij ). The increase in accuracy comes at the expense of more parameters.
In appendix F We present additional results shoWing that quantum models can still deliver modest
accuracy improvement W.r.t. convolutional netWorks With the same number of parameters.
5 Conclusions
In this Work We made the folloWing main contributions: 1) We introduced quantum deformed neural
netWorks and identified potential speedups by running these models on a quantum computer; 2) We
devised classically efficient algorithms to train the netWorks for loW entanglement designs of the
quantum circuits; 3) for the first time in the literature, We simulated the quantum neural netWorks
on real World data sizes obtaining good accuracy, and shoWed modest gains due to the quantum
deformations. Running these models on a quantum computer Will alloW one to explore efficiently
more general deformations, in particular those that cannot be approximated by the central limit
theorem When the Hamiltonians Will be sums of non-commuting operators. Another interesting
future direction is to incorporate batch normalization and pooling layers in quantum neural netWorks.
An outstanding question in quantum machine learning is to find quantum advantages for classical
machine learning tasks. The class of knoWn problems for Which a quantum learner can have a
provably exponential advantage over a classical learner is small at the moment Liu et al. (2020), and
some problems that are classically hard to compute can be predicted easily With classical machine
learning Huang et al. (2020). The approach presented here is the next step in a series of papers that
tries to benchmark quantum neural netWorks empirically, e.g. Farhi & Neven (2018); Huggins et al.
(2019); Grant et al. (2019; 2018); Bausch (2020). We are the first to shoW that toWards the limit of
entangling circuits the quantum inspired architecture does improve relative to the classical one for
real World data sizes.
9
Under review as a conference paper at ICLR 2021
References
Scott Aaronson and Lijie Chen. Complexity-theoretic foundations of quantum supremacy experi-
ments, 2016.
Jonathan Allcock, Chang-Yu Hsieh, Iordanis Kerenidis, and Shengyu Zhang. Quantum algorithms
for feedforward neural networks. arXiv e-prints, art. arXiv:1812.03089, December 2018.
Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak
Biswas, Sergio Boixo, Fernando GSL Brandao, David A Buell, et al. Quantum supremacy using
a programmable superconducting processor. Nature, 574(7779):505-510, 2019.
Johannes Bausch. Recurrent quantum neural networks, 2020.
Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, and Ra-
mona Wolf. Efficient Learning for Deep Quantum Neural Networks. arXiv e-prints, art.
arXiv:1902.10445, February 2019.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum machine learning. Nature, 549(7671):195-202, 2017.
Yudong Cao, Gian Giacomo Guerreschi, and Alan Aspuru-Guzik. Quantum Neuron: an el-
ementary building block for machine learning on quantum computers. arXiv e-prints, art.
arXiv:1711.11240, November 2017.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Dif-
ferential Equations. arXiv e-prints, art. arXiv:1806.07366, Jun 2018.
Song Cheng, Lei Wang, and Pan Zhang. Supervised learning with projected entangled pair states,
2020.
Carlo Ciliberto, Mark Herbster, Alessand ro Davide Ialongo, Massimiliano Pontil, Andrea Roc-
chetto, Simone Severini, and Leonard Wossnig. Quantum machine learning: a classical perspec-
tive. Proceedings of the Royal Society of London Series A, 474(2209):20170551, January 2018.
doi: 10.1098/rspa.2017.0551.
Iris Cong, Soonwon Choi, and Mikhail D. Lukin. Quantum convolutional neural networks. Nature
Physics, 15(12):1273-1278, August 2019. doi: 10.1038/s41567-019-0648-8.
Edward Farhi and Hartmut Neven. Classification with Quantum Neural Networks on Near Term
Processors. arXiv e-prints, art. arXiv:1802.06002, February 2018.
Christopher A. FUChS and RUdiger Schack. Quantum-Bayesian coherence. Reviews of Modern
Physics, 85(4):1693-1715, Oct 2013. doi: 10.1103/RevModPhys.85.1693.
Edward Grant, Marcello Benedetti, Shuxiang Cao, Andrew Hallam, Joshua Lockhart, Vid Stojevic,
Andrew G Green, and Simone Severini. Hierarchical quantum classifiers. npj Quantum Informa-
tion, 4(1):1-8, 2018.
Edward Grant, Leonard Wossnig, Mateusz Ostaszewski, and Marcello Benedetti. An initialization
strategy for addressing barren plateaus in parametrized quantum circuits. Quantum, 3:214, Dec
2019. ISSN 2521-327X. doi: 10.22331/q-2019-12-09-214. URL http://dx.doi.org/10.
22331/q-2019-12-09-214.
Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut
Neven, and Jarrod R. McClean. Power of data in quantum machine learning, 2020.
William Huggins, Piyush Patil, Bradley Mitchell, K. Birgitta Whaley, and E. Miles Stoudenmire.
Towards quantum machine learning with tensor networks. Quantum Science and Technology, 4
(2):024001, Apr 2019. doi: 10.1088/2058-9565/aaea94.
Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum algorithms for deep convolu-
tional neural networks, 2019.
10
Under review as a conference paper at ICLR 2021
Diederik P. Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparame-
terization trick, 2015.
David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Soc., 2017.
Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep Learning and Quantum
Entanglement: Fundamental Connections with Implications to Network Design. arXiv e-prints,
art. arXiv:1704.01552, April 2017.
Yoav Levine, Or Sharir, Nadav Cohen, and Amnon Shashua. Quantum entanglement in deep learn-
ing architectures. Physical review letters, 122(6):065301, 2019.
Ding Liu, Shi-JU Ran, Peter Wittek, Cheng Peng, RaUl BlazqUez Garcia, Gang Su, and Maciej
Lewenstein. Machine Learning by Unitary Tensor Network of Hierarchical Tree Structure. arXiv
e-prints, art. arXiv:1710.04833, October 2017.
YUnchao LiU, Srinivasan ArUnachalam, and Kristan Temme. A rigoroUs and robUst qUantUm speed-
Up in sUpervised machine learning. arXiv e-prints, art. arXiv:2010.02174, October 2020.
E. Miles StoUdenmire and David J. Schwab. SUpervised Learning with QUantUm-Inspired Tensor
Networks. arXiv e-prints, art. arXiv:1605.05775, May 2016.
M.E. Nielsen and I.L. ChUang. Quantum Computation and Quantum Information. Cambridge
Series on Information and the NatUral Sciences. Cambridge University Press, 2000. ISBN
9780521635035. URL https://books.google.co.uk/books?id=aai-P4V9GJ8C.
Jorn W. T. Peters and Max Welling. Probabilistic binary neUral networks, 2018.
Maria SchUld, Ilya Sinayskiy, and Francesco PetrUccione. SimUlating a perceptron on a qUantUm
computer. Physics Letters A, 379(7):660-663, Mar 2015. ISSN 0375-9601. doi: 10.1016/j.
physleta.2014.11.061. URL http://dx.doi.org/10.1016/j.physleta.2014.11.
061.
Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameteri-
zation trick. arXiv preprint arXiv:1710.07739, 2017.
Joe Staines and David Barber. Variational optimization, 2012.
G. ;t Hooft. The Cellular Automaton Interpretation of Quantum Mechanics. Fundamental Theories
of Physics. Springer International Publishing, 2016. ISBN 9783319412856. URL https://
books.google.co.uk/books?id=ctlCDwAAQBAJ.
Ewin Tang. A quantum-inspired classical algorithm for recommendation systems. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, pp. 217-228,
New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367059. doi:
10.1145/3313276.3316310. URL https://doi.org/10.1145/3313276.3316310.
Guillaume Verdon, Jason Pye, and Michael Broughton. A Universal Training Algorithm for Quan-
tum Deep Learning. arXiv e-prints, art. arXiv:1806.09729, June 2018.
Guifre Vidal. Efficient classical simulation of slightly entangled quantum computations. Physical
review letters, 91(14):147902, 2003.
Nathan Wiebe, Ashish Kapoor, and Krysta M. Svore. Quantum Deep Learning. arXiv e-prints, art.
arXiv:1412.3489, December 2014.
Zhikuan Zhao, Jack K. Fitzsimons, and Joseph F. Fitzsimons. Quantum-assisted gaussian process
regression. Phys. Rev. A, 99:052331, May 2019a. doi: 10.1103/PhysRevA.99.052331. URL
https://link.aps.org/doi/10.1103/PhysRevA.99.052331.
Zhikuan Zhao, Jack K. Fitzsimons, Michael A. Osborne, Stephen J. Roberts, and Joseph F. Fitzsi-
mons. Quantum algorithms for training gaussian processes. Phys. Rev. A, 100:012304, Jul
2019b. doi: 10.1103/PhysRevA.100.012304. URL https://link.aps.org/doi/10.
1103/PhysRevA.100.012304.
11