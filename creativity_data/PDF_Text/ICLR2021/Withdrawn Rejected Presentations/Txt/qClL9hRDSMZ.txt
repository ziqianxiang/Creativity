Under review as a conference paper at ICLR 2021
Implicit bias of gradient descent for mean
SQUARED ERROR REGRESSION WITH WIDE NEURAL
NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We investigate gradient descent training of wide neural networks and the correspond-
ing implicit bias in function space. For 1D regression, we show that the solution
of training a width-n shallow ReLU network is within n-1/2 of the function which
fits the training data and whose difference from initialization has smallest 2-norm
of the weighted second derivative with respect to the input. The curvature penalty
function 1/Z is expressed in terms of the probability distribution that is utilized to
initialize the network parameters, and we compute it explicitly for various common
initialization procedures. For instance, asymmetric initialization with a uniform
distribution yields a constant curvature penalty, and thence the solution function
is the natural cubic spline interpolation of the training data. While similar results
have been obtained in previous works, our analysis clarifies important details and
allows us to obtain significant generalizations. In particular, the result generalizes to
multivariate regression and different activation functions. Moreover, we show that
the training trajectories are captured by trajectories of spatially adaptive smoothing
splines with decreasing regularization strength.
Keywords. Implicit bias, overparametrized neural network, cubic spline
interpolation, spatially adaptive smoothing spline, effective capacity.
1	Introduction
Understanding why neural networks trained in the overparametrized regime and without explicit
regularization generalize well in practice is an important problem (Zhang et al., 2017). Some form of
capacity control different from network size must be at play (Neyshabur et al., 2014) and specifically the
implicit bias of parameter optimization has been identified to play a key role (Neyshabur et al., 2017).
By implicit bias we mean that among the many hypotheses that fit the training data, the algorithm
selects one which satisfies additional properties that may be beneficial for its performance on new data.
Jacot et al. (2018) and Lee et al. (2019) showed that the training dynamics of shallow and deep wide
neural networks is well approximated by that of the linear Taylor approximation of the models at a
suitable initialization. Chizat et al. (2019) observe that a model can converge to zero training loss while
hardly varying its parameters, a phenomenon that can be attributed to scaling of the output weights
and makes the model behave as its linearization around the initialization. Zhang et al. (2019) consider
linearized models for regression problems and show that gradient flow finds the global minimum of
the loss function which is closest to initialization in parameter space. This type of analysis connects
with trajectory based analysis of neural networks (Saxe et al., 2014). Oymak and Soltanolkotabi
(2019) studied the overparametrized neural networks directly and showed that gradient descent finds
a global minimizer of the loss function which is close to the initialization. Towards interpreting
parameters in function space, Savarese et al. (2019) and Ongie et al. (2020) studied infinite-width
neural networks with parameters having bounded norm, in 1D and multi-dimensional input spaces,
respectively. They showed that, under a standard parametrization, the complexity of the functions
represented by the network, as measured by the 1-norm of the second derivative, can be controlled by
the 2-norm of the parameters. Using these results, one can show that gradient descent with `2 weight
penalty leads to simple functions. Sahs et al. (2020) relates function properties, such as breakpoint and
slope distributions, to the distributions of the network parameters.
1
Under review as a conference paper at ICLR 2021
The implicit bias of parameter optimization has been investigated in terms of the properties of the loss
function at the points reached by different optimization methodologies (Keskar et al., 2017; Wu et al.,
2017; Dinh et al., 2017). In terms of the solutions, Maennel et al. (2018) show that gradient flow for
shallow networks with rectified linear units (ReLU) initialized close to zero quantizes features in a
way that depends on the training data but not on the network size. Williams et al. (2019) obtained
results for 1D regression contrasting the kernel and adaptive regimes. Soudry et al. (2018) show that in
classification problems with separable data, gradient descent with linear networks converges to a max-
margin solution. Gunasekar et al. (2018b) present a result on implicit bias for deep linear convolutional
networks, and Ji and Telgarsky (2019) study non-separable data. Chizat and Bach (2020) show that gra-
dient flow for logistic regression with infinitely wide two-layer networks yields a max-margin classifier
in a certain space. Gunasekar et al. (2018a) analyze the implicit bias of different optimization methods
(natural gradient, steepest and mirror descent) for linear regression and separable linear classification
problems, and obtain characterizations in terms of minimum norm or max-margin solutions.
In this work, we study the implicit bias of gradient descent for regression problems. We focus on wide
ReLU networks and describe the bias in function space. In Section 2 we provide settings and notation.
We present our main results in Section 3, and develop the main theory in Sections 4 and 5. In the
interest of a concise presentation, technical proofs and extended discussions are deferred to appendices.
2	Notation and problem setup
Consider a fully connected network with d inputs, one hidden layer of width n, and a single output.
For any given input x ∈Rd, the output of the network is
n
f(x,θ)=XWi(2)φ(hWi(1),xi+bi(1))+b(2),	(1)
i=1
where φ is a point-wise activation function, W(1) ∈Rn×d, W(2) ∈Rn, b(1) ∈Rn and b(2) ∈R are the
weights and biases of layer l = 1,2. We write θ=vec(∪l2=1{W(l),b(l)}) for the vector of all network
parameters. These parameters are initialized by independent samples of pre-specified random variables
W and B in the following way:
Wij) = p1/d W, b(1) = p1/d B
wi(2)=pi/n w, b⑵=pi/nb.
(2)
More generally, we will also allow weight-bias pairs to be sampled from a joint distribution of (W,B)
which we only assume to be sub-Gaussian. In the analysis of Jacot et al. (2018); Lee et al. (2019),
W and B are Gaussian N(0,σ2). In the default initialization of PyTorch, W and B have uniform
distribution U (-σ,σ). The setting (1) is known as the standard parametrization. Some works (Jacot
et al., 2018; Lee et al., 2019) utilize the so-called NTK parametrization, where the factor，1/n is
carried outside of the trainable parameter. If we fix the learning rate for all parameters, gradient
descent leads to different trajectories under these two parametrizations. Our results are presented for
the standard parametrization. Details on this in Appendix C.3.
We consider a regression problem for data {(xj , yj )}jM=1 with inputs X = {xj }jM=1 and out-
puts Y = {yj }jM=1 . For a loss function ` : R × R → R, the empirical risk of our function is
L(θ) = PM=ι'(f (xj,θ),yj). We use full batch gradient descent with a fixed learning rate η to minimize
L(θ). Writing θt for the parameter at time t, and θ0 for the initialization, this defines an iteration
θt+ι=θt-ηVL(θ)=θt-ηVθ f(x ,θt )T Vf(X ,3l,	⑶
where f(X,θt) = [f(x1,θt),...,f(xM ,θt)]T is the vector of network outputs for all training inputs, and
Vf(X,θt)L is the gradient of the loss with respect to the model outputs. We will use subscript i to index
neurons and subscript t to index time. Let ΘΘn be the empirical neural tangent kernel (NTK) of the
standard parametrization attime 0, which is the matrix ΘΘ n = n Vθ f (X ,θo )Vθ f (X ,θo)T.
2
Under review as a conference paper at ICLR 2021
3	Main results and discussion
We obtain a description of the implicit bias in function space when applying gradient descent to
regression problems with wide ReLU neural networks. We prove the following result in Appendix D.
An interpretation of the result and generalizations are given further below.
Theorem 1 (Implicit bias of gradient descent in wide ReLU networks). Consider a feedforward
network with a single input unit, a hidden layer of n rectified linear units, anda single linear output
unit. Assume standard parametrization (1) and that for each hidden unit the input weight and bias
are initialized from a sub-Gaussian (W,B) (2) with joint density pW,B. Then, for any finite data
set {(xj, yj)}jM=1 and sufficiently large n there exist constant u and v so that optimization of the
mean square error on the adjusted training data {(xj , yj - uxj - v)}jM=1 by full-batch gradient
descent with sufficiently small step size converges to a parameter θ* for which the output function
f (x,θ*) (1) attains zero training error. Furthermore, letting Z(x) = Jr|W∣3pw,b(W,-Wx) dW and
S = SuPP(Z) ∩ [miniXj,maXiXj], we have kf (x,θ*) — g*(x)∣∣2 = O(n-2 ),x ∈ S (the 2-norm over S)
with high probability over the random initialization θ0, where g* SOlVeSfollowing variational problem:
min
g∈C2(S)
subject to
u∕sZ(x)(g00(x)-f 00(x,θo))2 dx
(4)
g(xj)=yj—uxj—v, j= 1,...,M.
Interpretation An intuitive interpretation of the theorem is that at those regions of the input space
where Zis smaller, we can expect the difference between the functions after and before training to have a
small curvature. We may callρ= 1/Z a curvature penalty function. The bias induced from initialization
is expressed explicitly. We note that under suitable asymmetric parameter initialization (see Appendix
C.2), it is possible to achieve f (∙,θo) ≡ 0. Then the regularization is on the curvature of the output func-
tion itself. In Theorem 9 we obtain the explicit form of Z for various common parameter initialization
procedures. In particular, when the parameters are initialized independently from a uniform distribution
on a finite interval, Z is constant and the problem is solved by the natural cubic spline interpolation of
the data. The adjustment of the training data simply accounts for the fact that second derivatives define
a function only up to linear terms. In practice we can use the coefficients a and b of linear regression
yj = axj + b + j, j = 1, ... , M, and set the adjusted data as {(xj, j)}jM=1. Although Theorem 1
describes the gradient descent training with the linearly adjusted data, this result can also approximately
describe training with the original training data. Further details are provided in Appendix L.
We illustrate Theorem 1 numerically in Figure 1 and more extensively in Appendix A. In close agree-
ment with the theory, the solution to the variational problem captures the solution of gradient descent
training uniformly with error ofordern-1/2. To illustrate the effect of the curvature penalty function,
Figure 1 also shows the solutions to the variational problem for different values of Z corresponding
to different initialization distributions. We see that at input points where Z is small / peaks strongly, the
solution function tends to have a lower curvature / be able to use a higher curvature in order to fit the data.
With the presented bias description we can formulate heuristics for parameter initialization either
to ease optimization or also to induce specific smoothness priors on the solutions. In particular, by
Proposition 8 any curvature penalty 1/Z can be implemented by an appropriate choice of the parameter
initialization distribution. By our analysis, the effective capacity of the model, understood as the set
of possible output functions after training, is adapted to the size M of the training dataset and is well
captured by a space of cubic splines relative to the initial function. This is a space with dimension
of order M independently of the number of parameters of the network.
Strategy of the proof In Section 4, we observe that for a linearized model gradient descent with
sufficiently small step size finds the minimizer of the training objective which is closest to the initial
parameter (similar to a result by Zhang et al., 2019). Then Theorem 4 shows that the training dynamics
of the linearization of a wide network is well approximated in parameter and function space by that
of a lower dimensional linear model which trains only the output weights. This property is sometimes
taken for granted and we show that it holds for the standard parametrization, although it does not
hold for the NTK parametrization (defined in Appendix C.3), which leads to the adaptive regime. In
Section 5, for networks with a single input and a single layer of ReLUs, we relate the implicit bias
of gradient descent in parameter space to an alternative optimization problem. In Theorem 5 we show
that the solution of this problem has a well defined limit as the width of the network tends to infinity,
3
Under review as a conference paper at ICLR 2021
Figure 1: Illustration of Theorem 1. Left: Uniform error between the solution g* to the variational
problem and the functions f (∙,θ*) obtained by gradient descent training of a neural network (in
this case with uniform initialization W 〜U(-1,1), B 〜U(-2,2)), against the number of neurons.
The inset shows examples of the trained networks (blue) alongside with the training data (dots) and
the solution to the variational problem (orange). Right: Effect of the curvature penalty function on
the shape of the solution function. The bottom shows g* for various different ζ shown at the top.
Again dots are training data. The green curve is for ζ constant on [-2,2], derived from initialization
W~ U(-1,1), B~ U(-2,2);blue isfor Z(x) = 1∕(1+x2 )2, derived from W~ N(0,1), B~ N(0,1);
and orange for Z(x) = 1/(0.1 + x2)2, derived from W ~ N(0,1), B~ N(0,0.1). Theorem 9 shows
how to compute ζ for the above distributions.
Reciprocal curvature penalty Z
Solution g* tothevariational problem
which allows us to obtain a variational formulation. In Theorem 6 we translate the description of the
bias from parameter space to function space. In Theorem 9 we provide explicit descriptions of the
weight function for various common initialization procedures. Finally, we can utilize recent results
bounding the difference in function space of the solutions obtained from training a wide network and
its linearization (Lee et al., 2019, Theorem H.1).
Generalizations Theorem 4 has several generalizations elaborated in Appendix P. For multivariate
regression, we have the following theorem.
Theorem 2 (Multivariate regression). Use the same network setting as in Theorem 1 except that
the number of input units changes to d. Assume that for each hidden unit the input weight and bias
are initialized from a sub-Gaussian (W, B) where W is a d-dimensional random vector and B is
a random variable. Then, for any finite data set {(xj, yj)}iM=1 and sufficiently large n there exist
constant vector u and constant v so that optimization of the mean square error on the adjusted
training data {(xj,yj - hu,xj i -v)}jM=1 by full-batch gradient descent with sufficiently small step size
converges to a parameter θ* for which f (x,θ*) attains zero training error. Furthermore, letU = kWk2,
V = W∕kW∣∣2, C = -B∕kW∣∣2 and Z(V,c)= PV,c(V,c)E(U2∣V = V,C = C) where pv,c is the
joint density of (V ,C). Then we have ∣∣f (x,θ*) — g*(x)k2 = O(n-2 ),x ∈ Rd (the 2-norm over Rd)
with high probability over the random initialization θ0, where g* solves following variational problem:
min
g∈C(Rd)
subject to
/
supp(ζ)
(R{(-△产+1)/2(g-f(∙,θo))}(V,c))2
Z(VC)
dVdc
g(xj) =yj, j = 1,...,M,
R{(-∆)3+1”2(g-f(∙,θo))}(V,c)=0, (V,c) ∈supp(Z).
(5)
Here R is the Radon transform which is defined by R{f}(ω, b) := hω,xi=b f (x)ds(x), and
the power of the negative Laplacian (-△)(d+1)/2 is the operator defined in Fourier domain by
(-∆)(d+1"2f(ξ) = kξkd+1f(ξ).
For different activation functions, we have the following corollary.
4
Under review as a conference paper at ICLR 2021
Corollary 3 (Different activation functions). Use the same setting as in Theorem 1 except that we
use the activation function φ instead of ReLU. Suppose that φ is a Green’s function ofa linear operator
L, i.e. Lφ = δ, where δ denotes the Dirac delta function. Assume that the activation function φ is
homogeneous of degree k, i.e. φ(ax) = ak φ(x) for all a> 0. Then we can find a function p satisfying
Lp ≡ 0 and adjust training data {(xj, yj)}jM=1 to {(xj, yj - p(xj)}jM=1. After that, the statement in
Theorem 1 holds with the variational problem (4) changed to
min、	/ TT^[L(g(x)-f(x,θo))]2 dx St g(xj )=yj-p(xj), j = 1,∙∙∙M,	(6)
g∈C2 (S)	Sζ(x)
where ζ(x) = pC (x)E(W 2k |C =x) and S=supp(ζ) ∩[minixi,maxixi].
Moreover, our method allows us to describe the optimization trajectory in function space (see
Appendix N). Ifwe substitute constraints g(xj) =yj in (4) by a quadratic term 1 Mr PjM=i(g(Xj Anj )2
added to the objective, we obtain the variational problem for a so-called spatially adaptive smoothing
spline (see Abramovich and Steinberg, 1996; Pintore et al., 2006). This problem can be solved
explicitly and can be shown to approximate early stopping. To be more specific, the solution to
following optimization problem approximates the output function of the network after gradient descent
training for t steps with learning rate fj/n:
min
g∈C2(S)
M	11
y?[g(xj)-yj] +与	(g~~∖x)(X)-f (X,θO)) dx.
j=1	ft S ζ(x)
(7)
Related works Zhang et al. (2019) described the implicit bias of gradient descent in the kernel regime
as minimizing a kernel norm from initialization, subject to fitting the training data. Our result can be
regarded as making the kernel norm explicit, thus providing an interpretable description of the bias in
function space and further illuminating the role of the parameter initialization procedure. We prove the
equivalence in Appendix M. Savarese et al. (2019) showed that infinite-width networks with 2-norm
weight regularization represent functions with smallest 1-norm of the second derivative, an example
of which are linear splines. We discuss this in Appendix C.4. A recent preprint further develops this
direction for two-layer networks with certain activation functions that interpolate data while minimizing
a weight norm (Parhi and Nowak, 2019). In contrast, our result characterizes the solutions of training
from a given initialization without explicit regularization, which turn out to minimize a weighted
2-norm of the second derivative and hence correspond to cubic splines. In finishing this work we
became aware of a recent preprint (Heiss et al., 2019) which discusses ridge weight penalty, adaptive
splines, and early stopping for one-input ReLU networks training only the output layer. Williams et al.
(2019) showed a similar result in the kernel regime for shallow ReLU networks where they train only
the second layer and from zero initialization. In contrast, we consider the initialization of the second
layer and show that the difference from the initial output function is implicitly regularized by gradient
descent. We show the result of training both layers and prove that it can be approximated by training
only the second layer in Theorem 4. In addition, we give the explicit form of ζ in Theorem 9, while
the ζ given by Williams et al. (2019) has a minor error because of a typo in their computation. Most
importantly, our statement can be generalized to multivariate regression, different activation functions,
training trajectories.
4	Wide networks and parameter space
4.1	Implicit bias in parameter space for a linearized model
In this section we describe how training a linearized network or a wide network by gradient descent
leads to solutions that are biased, having parameter values close to the values at initialization. First,
we consider the following linearized model:
f lin(x,ω) = f (x,θo) + Vθ f (x,θo )(ω-θo).	(8)
We write ω for the parameter of the linearized model, in order to distinguish it from the pa-
rameter of the nonlinearized model. The empirical loss of the linearized model is defined by
Llin(ω) = PjM=1 `(f lin (Xj,ω),yj). The gradient descent iteration for the linearized model is given by
ω0=θ0, ωt+1=ωt -fVθf(X,θ0)TVflin(X,ωt)Llin.	(9)
5
Under review as a conference paper at ICLR 2021
Next, we consider wide neural networks. According to Lee et al. (2019, Theorem H.1),
Supkf lin(x,ωt)-f (x,θt)k2 = O(n-2)
with arbitrarily high probability. So gradient descent training of a wide network or of the linearized
model give similar trajectories and solutions in function space. Both fit the training data perfectly,
meaning flin (X,ω∞) =f(X,θ∞) =Y, and are also approximately equal outside the training data.
Under the assumption that rank(Vθf (X,θo)) = M, the gradient descent iterations (9) converge to
the unique global minimum that is closest to initialization (Gunasekar et al., 2018a; Zhang et al., 2019),
which is the solution of following constrained optimization problem (further details and remarks are
provided in Appendix E):
minkω-θ0k2 s.t. flin (X,ω) = Y.	(10)
ω
4.2	Training only the output layer approximates training all parameters
From now on we consider networks with a single hidden layer of n ReLUs and a linear output
f(x, θ) = Pin=1 Wi(2) [Wi(1)x + bi(1)]+ + b(2). We show that the functions and parameter vectors
obtained by training the linearized model are close to those obtained by training only the output layer.
Hence, by the arguments of the previous section, training all parameters of a wide network or training
only the output layer gives similar functions.
Let θo = Vec(W⑴,b(1),W(2),b(2)) be the parameter at initialization so that f lin(∙,θ0) = f (∙,θo).
After training the linearized network let the parameter be ω∞ = vec(Wc (1),bb(1), Wc (2),bb(2)). Using
initialization (2), with probability arbitrarily close to 1, W(1) b(1) = O(1) and W(2) ,b(2) = O(n-2 ).1
Therefore, writing H for the Heaviside function, we have
i(1
hW(2)H(W (1)x+b(1))∙x,W (2)H (Wi1)x+bi1))] = O(n-1),
VW(2),b(2) f (x,θ0) = h[W(1)x+ b(1)]+ J] = θ⑴.
(11)
So when n is large, if we use gradient descent with a constant learning rate for all parameters, then
the changes ofW(1), b(1), b(2) are negligible compared with the changes ofW(2). So approximately
(2)
we can train just the output weights, Wi ,i= 1,...,n, and fix all other parameters. This corresponds
11	1.	一	〜	,τ-(1) -(1) ~(2) -(2)	.
to a smaller linear model. Let ωet = vec(W t ,bt ,Wt ,bt ) be the parameter at time t under the
update rule where W (1),b(1), b(2) are kept fixed at their initial values, and
f(2) = W⑵，Wt+11=ft⑵-ηVw⑵ Llin(ωt).	(12)
Let ωe∞ = limt→∞ ωet. By the above discussion, we expect that flin(x,ωe∞) is close to flin(x,ω∞).
In fact, we prove the following for the MSE loss. The proof and further remarks are provided in
Appendix F. We relate Theorem 4 to training a wide network in Appendix G.
Theorem 4 (Training only output weights vs linearized network). Consider a finite data set
{(xi,yi)}M=ι∙ Assume that (1)we use the MSE loss '(y,y) = 2 kb— yk2; (2) infnλmin(ΘΘn) > 0. Let
ωt denote the parameters of the linearized model at time t when we train all parameters using (9),
and let ωet denote the parameters at time t when we only train weights of the output layer using (12).
Ifwe use the same learning rate η in these two training processes and η < —―2(6), thenfor any
x ∈ R, with probability arbitrarily close to 1 over the random initialization (2),
sup∣flin(x,ωt)-flin(x,ωt)∣ = O(n-1), asn→∞.	(13)
t
Moreover, in terms of the parameter trajectories we have SuptkW(1) 一 Ct(I)k2 = O(n-1),
Suptkb(1) —b(1) k2 = O(n-1), Suptkft⑵一Ct⑵ k2 = O(n-3/2), Suptkb(2) —b(2)k = O(n-1).
In view of the arguments in this section, in the next sections we will focus on training only the output
weights and understanding the corresponding solution functions.
1More precisely, for any δ> 0, ∃C, s.t. with prob. 1 — δ, | W i2)∣,∣b ⑵ | ≤ Cn-1/2 and | W i1)∣,∣bi1)∣ ≤ C.
6
Under review as a conference paper at ICLR 2021
5	Gradient descent leads to simple functions
In this section we provide a function space characterization of the implicit bias previously described in
parameter space. According to (10), gradient descent training of the output weights (12) achieves zero
loss, flin (Xj ,ω∞)- f lin(χj ,θo ) = Pn=Ifi⑵-Wi2))[W(I)Xj + bi]+=yj-f(χj ,θo), j = ι,∙∙∙,M,
with minimum kf ⑵-W⑵ ∣∣2. Hence gradient descent is actually solving
n
min∣∣W⑵-W⑵k2 s.t.	X(Wi⑵-W(2))[W(1)Xj + bi↑+ = y∙-f(xj,θo),j = 1,...,M. (14)
i=1
To simplify the presentation, in the following we let flin (x, θ0) ≡ 0 by using the ASI trick (see
Appendix C.2). The analysis still goes through without this.
5.1	Infinite width limit
We reformulate problem (14) in a way that allows us to consider the limit of infinitely wide
networks, with n → ∞, and obtain a deterministic counterpart, analogous to the convergence
of the NTK. Let μn denote the empirical distribution of the samples (W：1), bi)nn=ι, so that
μn(A) = n Pn=I IA ((W(I),bi)). Here IA is the indicator function for measurable subsets A in
R2 . We further consider a function αn : R2 → R whose value encodes the difference of the output
weight from its initialization for a hidden unit with input weight and bias given by the argument,
an(Wi(11 ,bn)= n(Wi(2) - W(2)). Then (14) with ASI can be rewritten as
min
αn∈C(R2)
[ann(W⑴,b)dμn(W⑴,b) s.t./
R2	R2
an (W ⑴,b)[W ⑴ Xj + b]+ dμn(W ⑴,b)= yj∙, (15)
where j ranges from 1 to M. Here we minimize over functions αn in C(R2), but since only the values
on (Wi(1),bi)in=1 are taken into account, we can take any continuous interpolation of αn(Wi(1),bi),
i = 1,…,n. Now we can consider the infinite width limit. Let μ be the probability measure of (W,B).
We obtain a continuous version of problem (15) by substituting μ for μn. Since we know that μn
weakly converges to μ, we prove that in fact the solution of problem (15) converges to the solution
of the continuous problem, which is formulated in the following theorem. Details in Appendix H.
Theorem 5. Let (Wi(1),bi)in=1 be i.i.d. samples from a pair (W,B) of random variables with finite
fourth moment. Suppose μn is the empirical distribution of (Wi(11 ,bn)n=ι and ɑn(W(1),b) is the
solution of (15). Let α(W ⑴,b) be the solution ofthe continuous problem with μ in place of μn. Then
forany bounded [-L,L], suPx∈[-l,l] ∣gn(x,αn) -g(x,α)∣ = O(n-1/2) with high probability, where
gn(x,αn) = Jr2 an (W ⑴，b)[W ⑴ x+b]+ dμn(W ⑴，b) is thefunction represented by a network with
n hidden neurons after training, andg(X,α) = R2α(W⑴,b)[W⑴x+b]+ dμ(W⑴,b) is thefunction
represented by the infinite-width network.
5.2	Function space description of the implicit bias
Next we connect the problem from the previous section to second derivatives by first rewriting it in
terms of breakpoints. Consider the breakpoint c= -b/W (1) ofa ReLU with weight W(1) and bias
b. We define a corresponding random variable C= -B/W and let ν denote the distribution of (W,C).2
Then with γ(W(1),c) = a(W(1),-cW(1)) the continuous version of (15) is equivalently given as
min
γ∈C(R2)
/
R2
γ2 (W(1),c) dν(W(1),c) s.t.
γ(W(1),c)[W(1)(
R2
Xj -c)]+ dν(W(1),c)=yj,
(16)
where j ranges from 1 to M. Let VC denote the distribution of C = - B/W, and νw∣c=c the conditional
distribution of W given C = c. Suppose νC has support supp(νC) and a density function pC(c).
2Here we assume that P(W = 0) =0 so that the random variable C is well defined. Itis not an important restric-
tion, since neurons with weight W(1) =0 give constant functions that can be absorbed in the bias of output layer.
7
Under review as a conference paper at ICLR 2021
Let g(x, γ) = R2 γ(W (1), c)[W(1)(x - c)]+ dν(W (1), c), which again corresponds to the output
function of the network. Then, the second derivative g00 with respect to x (see Appendix I) satisfies
g00 (x,γ )= PC (X)RRY(W (1),x)∣W ⑴ I dνw∣c=χ(W ⑴).Thus Y(W ⑴,c) is closely related to g00 (x,γ)
and we can try to express (16) in terms of g00 (x,γ). Since g00(x,γ) determines g(x,γ) only up to linear
functions, we consider the following problem:
subject to
min
γ∈C(R2),u∈R,v∈R
uxj+v+
Y(W(1),c)[W(1) (
xj-c)]+dν(W(1),c)=yj,
(17)
j= 1,...,M.
Here u,v are not included in the cost. They add a linear function to the output of the neural network.
If u and v in the solution of (17) are small, then the solution is close to the solution of (16). Ongie
et al. (2020) also use this trick to simplify the characterization of neural networks in function space.
Next we study the solution of (17) in function space. This is our main technical result.
Theorem 6 (Implicit bias in function space). Assume W and B are random variables with
P(W = 0) = 0, and let C = -B/W. Let ν denote the probability distribution of (W, C). Suppose
(γ,u,v) is the solution of (17), and consider the corresponding output function
g(x,(Y,U,V)) = Ux+v +
/
R2
Y(W ⑴,c)[W ⑴(X-c)]+ dν (W ⑴,c).
(18)
Let νC denote the marginal distribution ofC and assume it has a densityfunctionpC. Let E(W2|C)
denote the conditional expectation ofW2 given C. Consider the function ζ(X) =pC (X)E(W2 |C = X).
Assume that training data Xi ∈ supp(ζ), i= 1,...,m. Consider the set S = supp(ζ) ∩ [miniXi,maxiXi].
Then g(x,(γ,U,v)) satisfies g00(x,(γ,U,v)) =0 for x∈ S andfor X ∈ S it is the solution ofthefollowing
problem:
min
h∈C2(S) S
(h00(x))2
Z (X)
dX
s.t.
h(Xj)=yj, j = 1,...,m.
(19)
The proof is provided in Appendix I, where we also present the corresponding statement without ASI.
We study the explicit form of this function in the next section.
5.3	Explicit form of the curvature penalty function
Proposition 7. LetpW,B denote the joint density function of (W,B) and letC = -B/W so that pC
is the breakpoint density. Then ζ(X) = E(W2 |C = X)pC (X) = RR|W|3pW,B(W,-WX) dW.
The proof is presented in Appendix J. Ifwe allow the initial weight and biases to be sampled from
a suitable joint distribution, We can make the curvature penalty P =1 /Z arbitrary.
Proposition 8 (Constructing any curvature penalty). Given any function % : R → R>0, satisfying
Z = JR % < ∞, if we set the density of C as PC (x)=十 %(X)and make W independent of C with
non-vanishingsecondmoment, then (E(W2|C = X)PC(X))T = (E(W2)pc(X))T H%(x), X∈R.
Further remarks on sampling and independent variables are provided in Appendix J. To conclude this
section We compute the explicit form of ζ for several common initialization procedures.
Theorem 9 (Explicit form of the curvature penalty for common initializations).
(a)	Gaussian initialization. Assume that W and B are independent, W〜N (0,σW) and B〜N (0,σ2).
Then Z is given by Z(x)=d+2编产.
(b)	Binary-uniform initialization. Assume that W and B are independent, W ∈ {-1, 1} and
B 〜U(—ab,ab) with ab ≥ L. Then Z is constant on [—L,L].
(c)	Uniform initialization. Assume that W and B are independent, W 〜U (—aw, aw) and
B 〜U(—a，b,ab) with ab- ≥ L. Then Z is constanton [—L,L].
aw
The proof is provided in Appendix K. Theorem 9 (b) and (c) shoW that for certain distributions of
(W,B), Z is constant. In this case problem (19) is solved by the cubic spline interpolation of the data
With natural boundary conditions (Ahlberg et al., 1967). The case of general Z is solved by space
adaptive natural cubic splines, Which can be computed numerically by solving a linear system and
theoretically in an RKHS formalism. We provide details in Appendix O.
8
Under review as a conference paper at ICLR 2021
6	Conclusion and Discussion
We obtained a explicit description of the implicit bias of gradient descent for mean squared error
regression with wide shallow ReLU networks. We presented a result for the univariate case and
generalizations to multi-variate ReLU networks and networks with different activation functions. Our
result can also help us characterize the training trajectory of gradient descent in function space.
Our main result shows that the trained network outputs a function that interpolates the training data
and has the minimum possible weighted 2-norm of the second derivative with respect to the input.
This corresponds to an spatially adaptive interpolating spline. The space of interpolating splines is
a linear space which has a dimension that is linear in the number of data points. Hence our result means
that, even if the network has many parameters, the complexity of the trained functions will be adjusted
to the number of data points. Interpolating splines have been studied in great detail in the literature
and our result allows us to directly apply corresponding generalization results to the case of trained
networks. This is related to approximation theory and characterizations for the number of samples
and their spacing needed in order to approximate functions from a given smoothness class to a desired
precision (Rieger and Zwicknagl, 2010; Wendland, 2004).
Zhang et al. (2019) described the implicit bias of gradient descent as minimizing a RKHS norm
from initialization. Our result can be regarded as making the RKHS norm explicit, thus providing an
interpretable description of the bias in function space. Compared with Zhang et al. (2019), our results
give a precise description of the role of the parameter initialization scheme, which determines the
inverse curvature penalty function ζ. This gives us a rather good picture of how the initialization affects
the implicit bias of gradient descent. This could be used in order to select a good initialization scheme.
For instance, one could conduct a pre-assessment of the data to estimate the locations of the input space
where the target function has a high curvature, and choose the parameter initialization accordingly.
This is an interesting possibility to experiment with, based on our theoretical result.
Our result can also be interpreted in combination with early stopping. The training trajectory is
approximated by a smoothing spline, meaning that the network will filter out high frequencies which
are usually associated to noise in the training data. This behaviour is sometimes referred to as a spectral
bias (Rahaman et al., 2019).
References
Felix Abramovich and David M. Steinberg. Improved inference in nonparametric regression
using Lk-smoothing splines. Journal of Statistical Planning and Inference, 49(3):327 - 341,
1996. ISSN 0378-3758. doi: https://doi.org/10.1016/0378-3758(95)00021-6. URL http:
//www.sciencedirect.com/science/article/pii/0378375895000216.
J. H. Ahlberg, Edwin N. Nilson, and J. L. Walsh. The Theory of Splines and Their Ap-
plications. ISSN. Elsevier Science, 1967. ISBN 9780080955452.	URL https:
//books.google.com/books?id=S7d1pjJHsRgC.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pages 2933-2943, 2019.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages
1019-1028, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/dinh17b.html.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms
of optimization geometry. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pages 1832-1841, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018a. PMLR.
URL http://proceedings.mlr.press/v80/gunasekar18a.html.
9
Under review as a conference paper at ICLR 2021
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on lin-
ear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9461-9471.
Curran Associates, Inc., 2018b. URL http://papers.nips.cc/paper/8156-implici
t-bias-of-gradient-descent-on-linear-convolutional-networks.pdf.
Jakob Heiss, Josef Teichmann, and Hanna Wutte. How implicit regularization of neural networks
affects the learned function - part i. arXiv preprint arXiv:1911.02903, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31,
pages 8571-8580. Curran Associates, Inc., 2018.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In Alina
Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning
Theory, volume 99 of Proceedings of Machine Learning Research, pages 1772-1798, Phoenix, USA,
25-28 Jun 2019. PMLR. URL http://proceedings.mlr.press/v99/ji19a.html.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and
sharp minima. In International Conference on Learning Representations, 2017. URL
https://openreview.net/pdf?id=H1oyRlYgg.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein,
and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Garnett,
editors, Advances in Neural Information Processing Systems 32, pages 8572-8583. Curran
Associates, Inc., 2019.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes ReLU network
features. arXiv preprint arXiv:1803.08367, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of
optimization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071, 2017.
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded
norm infinite width ReLU nets: The multivariate case. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=H1lNPxHKDH.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings
of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pages 4951-4960, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/oymak19a.html.
Rahul Parhi and Robert D. Nowak. Minimum "norm" neural networks are splines. arXiv preprint
arXiv:1910.02333, 2019.
Alexandre Pintore, Paul Speckman, and Chris C. Holmes. Spatially adaptive smoothing splines.
Biometrika, 93(1):113-125, 03 2006. ISSN 0006-3444. doi: 10.1093/biomet/93.1.113. URL
https://doi.org/10.1093/biomet/93.1.113.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference
on Machine Learning, pages 5301-5310. PMLR, 2019.
Christian Rieger and Barbara Zwicknagl. Sampling inequalities for infinitely smooth functions, with
applications to interpolation and machine learning. Advances in Computational Mathematics, 32
(1):103, 2010.
10
Under review as a conference paper at ICLR 2021
Justin Sahs, Aneel Damaraju, Ryan Pyle, Onur Tavaslioglu, Josue Ortega Caro, Hao Yang Lu, and
Ankit Patel. A functional characterization of randomly initialized gradient descent in deep ReLU
networks, 2020. URL https://openreview.net/forum?id=BJl9PRVKDS.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? In Alina Beygelzimer and Daniel Hsu, editors, Pro-
ceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of
Machine Learning Research, pages 2667-2690, Phoenix, USA, 25-28 JUn 2019. PMLR. URL
http://proceedings.mlr.press/v99/savarese19a.html.
Andrew M. Saxe, James L. McClelland, and SUrya GangUli. Exact solUtions to the nonlinear dynamics
of learning in deep linear neUral networks. In YoshUa Bengio and Yann LeCUn, editors, 2nd
International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16,
2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6120.
Daniel SoUdry, Elad Hoffer, Mor Shpigel Nacson, SUriya GUnasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Holger Wendland. Scattered data approximation, volUme 17. Cambridge University press, 2004.
Francis Williams, Matthew Trager, Daniele Panozzo, ClaUdio Silva, Denis Zorin, and Joan BrUna.
Gradient dynamics of shallow Univariate relU networks. In Advances in Neural Information
Processing Systems, pages 8378-8387, 2019.
Lei WU, Zhanxing ZhU, and E Weinan. Towards Understanding generalization of deep learning:
Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
ChiyUan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning reqUires rethinking generalization. In International Conference on Learning
Representations, ICLR 2017, 2017. URL https://arxiv.org/abs/1611.03530.
YaoyU Zhang, Zhi-Qin John XU, Tao LUo, and Zheng Ma. A type of generalization error indUced by
initialization in deep neUral networks. arXiv preprint arXiv:1905.07777, 2019.
11