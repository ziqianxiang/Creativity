Under review as a conference paper at ICLR 2021
EpidemiOptim: a toolbox for the optimization
OF CONTROL POLICIES IN EPIDEMIOLOGICAL MODELS
Anonymous authors
Paper under double-blind review
Ab stract
Epidemiologists model the dynamics of epidemics in order to propose control
strategies based on pharmaceutical and non-pharmaceutical interventions (contact
limitation, lock down, vaccination, etc). Hand-designing such strategies is not
trivial because of the number of possible interventions and the difficulty to pre-
dict long-term effects. This task can be cast as an optimization problem where
state-of-the-art machine learning algorithms such as deep reinforcement learn-
ing, might bring significant value. However, the specificity of each domain -
epidemic modelling or solving optimization problem - requires strong Conabo-
rations between researchers from different fields of expertise. This is why we
introduce EpidemiOptim, a Python toolbox that facilitates collaborations between
researchers in epidemiology and optimization. EpidemiOptim turns epidemiolog-
ical models and cost functions into optimization problems via a standard inter-
face commonly used by optimization practitioners (OpenAI Gym). Reinforcement
learning algorithms based on Q-Learning with deep neural networks (DQN) and
evolutionary algorithms (NSGA-II) are already implemented. We illustrate the use
of EpidemiOptim to find optimal policies for dynamical on-off lock-down control
under the optimization of death toll and economic recess using a Susceptible-
Exposed-Infectious-Removed (SEIR) model for COVID-19. Using EpidemiOp-
tim and its interactive visualization platform in Jupyter notebooks, epidemiolo-
gists, optimization practitioners and others (e.g. economists) can easily compare
epidemiological models, costs functions and optimization algorithms to address
important choices to be made by health decision-makers.
1 Introduction
The recent COVID-19 pandemic highlights the destructive potential of infectious diseases in our
societies, especially on our health, but also on our economy. To mitigate their impact, scientific
understanding of their spreading dynamics coupled with methods quantifying the impact of inter-
vention strategies along with their associated uncertainty, are key to support and optimize informed
policy making. For example, in the COVID-19 context, large scale population lock-downs were
enforced based on analyses and predictions from mathematical epidemiological models (Ferguson
et al., 2005; 2006; Cauchemez et al., 2019; Ferguson et al., 2020). In practice, researchers often con-
sider a small number of relatively coarse and pre-defined intervention strategies, and run calibrated
epidemiological models to predict their impact (Ferguson et al., 2020). This is a difficult problem
for several reasons: 1) the space of potential strategies can be large, heterogeneous and multi-scale
(Halloran et al., 2008); 2) their impact on the epidemic is often difficult to predict; 3) the problem
is multi-objective by essence: it often involves public health objectives like the minimization of the
death toll or the saturation of intensive care units, but also societal and economic sustainability. For
these reasons, pre-defined strategies are bound to be suboptimal. Thus, a major challenge consists
in leveraging more sophisticated and adaptive approaches to identify optimal strategies.
Machine learning can be used for the optimization of such control policies, with methods ranging
from deep reinforcement learning to multi-objective evolutionary algorithms. In other domains,
they have proven efficient at finding robust control policies, especially in high-dimensional non-
stationary environments with uncertainty and partial observation of the state of the system (Deb
et al., 2007; Mnih et al., 2015; Silver et al., 2017; Haarnoja et al., 2018; Kalashnikov et al., 2018;
Hafner et al., 2019). Yet, researchers in epidemiology, in public-health, in economics, and in ma-
1
Under review as a conference paper at ICLR 2021
chine learning evolve in communities that rarely cross, and often use different tools, formalizations
and terminologies. We believe that tackling the major societal challenge of epidemic mitigation re-
quires interdisciplinary collaborations organized around operational scientific tools and goals, that
can be used and contributed by researchers of these various disciplines. To this end, we introduce
EpidemiOptim, a Python toolbox that provides a framework to facilitate collaborations between
researchers in epidemiology, economics and machine learning.
EpidemiOptim turns epidemiological models and cost functions into optimization problems via the
standard OpenAI Gym (Brockman et al., 2016) interface that is commonly used by optimization
practitioners. Conversely, it provides epidemiologists and economists with an easy-to-use access to
a variety of deep reinforcement learning and evolutionary algorithms, capable of handling different
forms of multi-objective optimization under constraints. Thus, EpidemiOptim facilitates the inde-
pendent update of models by specialists of each topic, while enabling others to leverage implemented
models to conduct experimental evaluations. We illustrate the use of EpidemiOptim to find optimal
policies for dynamical on-off lock-down control under the optimization of death toll and economic
recess using an extended Susceptible-Exposed-Infectious-Removed (SEIR) model for COVID-19
from Prague et al. (2020).
Related Work. We can distinguish two main lines of contributions concerning the optimization
of intervention strategies for epidemic response. On the one hand, several contributions focus on
providing guidelines and identifying the range of methods available to solve the problem. For exam-
ple, Yanez et al. (2019) framed the problem of finding optimal intervention strategies for a disease
spread as a reinforcement learning problem; Alamo et al. (2020) provided a road-map that goes from
the access to data sources to the final decision-making step; and Shearer et al. (2020) highlighted
that a decision model for epidemic response cannot capture all of the social, political, and ethical
considerations that these decisions impact. These contributions reveal a major challenge for the com-
munity: developing tools that can be easily used, configured and interpreted by decision-makers. On
the other hand, computational contributions proposed actual concrete implementations of such opti-
mization processes. These contributions mostly differ by their definition of epidemiological models
(e.g. SEIR (Yaesoubi et al., 2020) or agent-based models (Chandak et al., 2020)), of optimization
methods (e.g. deterministic rules (Tarrataca et al., 2020), Bayesian optimization (Chandak et al.,
2020), Deep RL (Arango & Pelov, 2020) or evolutionary optimization (Miikkulainen et al., 2020)),
of cost functions (e.g. fixed weighted sum of health and economical costs (Arango & Pelov, 2020),
possibly adding constraints on the school closure budget (Libin et al., 2020), or multi-objective op-
timization (Miikkulainen et al., 2020)), of state and action spaces (e.g. using the entire observed
epidemic history (Yaesoubi et al., 2020) or an image of the disease outbreak to capture the spa-
tial relationships between locations (Probert et al., 2019)), as well as methods for representing the
model decisions in a format suitable to decision-makers (e.g simple summary state representation
(Probert et al., 2019) or real-time surveillance data with decision rules (Yaesoubi & Cohen, 2016)).
See Appendix A.2 for a detailed description of the aforementioned papers.
Given this high diversity of potential methods in the field, our approach aims at providing a standard
toolbox facilitating the comparison of different configurations along the aforementioned dimensions
in order to assist decision-makers in the evaluation of the range of possible intervention strategies.
Contributions. This paper makes three contributions. First, we formalize the coupling of epidemi-
ological models and optimization algorithms with a particular focus on the multi-objective aspect
of such problems (Section 2). Second, based on this formalization, we introduce the EpidemiOptim
library, a toolbox that integrates epidemiological models, cost functions, optimization algorithms
and experimental tools to easily develop, study and compare epidemic control strategies (Section 3).
Third, we demonstrate the utility of the EpidemiOptim library by conducting a case study on the op-
timization of lock-down policies for the COVID-19 epidemic (Section 4). We use a recent epidemi-
ological model grounded on real data, cost functions based on a standard economical model of GDP
loss, as well as state-of-the-art optimization algorithms, all included in the EpidemiOptim library.
This is, to our knowledge, the first contribution that provides a comparison of different optimiza-
tion algorithm performances for the control of intervention strategies on the same epidemiological
model. The user of the EpidemiOptim library can interact with the trained policies via Jupyter note-
book, exploring the space of cost functions (health cost x economic cost) for a variety of algorithms.
The code is made available anonymously at https://tinyurl.com/epidemioptim.
2
Under review as a conference paper at ICLR 2021
2 The epidemic control problem as an optimization problem
Γ≥√Γ7
In an epidemic control problem, the objective is to
find an optimal control strategy to minimize some
cost (e.g. health and/or economical cost) related to
the evolution of an epidemic. We take the approach
of reinforcement learning (RL) (Sutton & Barto,
2018): the control policy is seen as a learning agent,
that interacts with an epidemiological model that is
seen as its learning environment, see Figure 1. Each
run of the epidemic is a learning episode, where
the agent alternatively interacts with the epidemic
through actions and observes the resulting states of
the environment and their associated costs. To define
this optimization problem, we define three elements:
1) the state and action spaces; 2) the epidemiological
model; 3) the cost function.
Policy ∏
∖θ
Learning
algorithm
YrajI^---------
_ Trajectory
collector
' ； ʌ
Epidemiological
model
Cost
functions
Agent	,Ynvironmenty
Figure 1: Epidemic control as an optimiza-
tion problem. s, a, ci refer to environment
states, control actions, and the ith cost, while
traj. is their collection over an episode. Blue
and red arrows match the input and output of
the OpenAI Gym step function.
a
a

7^F
State and action spaces. The state st describes the
information the control policy can use (e.g. epidemiological model states, cumulative costs up to
timestep t, etc.) We also need to define which actions a can be exerted on the epidemic, and how
they affect its parameters. For instance, enforcing a lock-down can reduce the transmission rate of
the epidemic and slow it down.
The epidemiological model. The epidemiological model is akin to a transition function T: it
governs the evolution of the epidemic. From the current state st of the epidemic and the current
action at exerted by the agent, it generates the next state st+1 : T : S, A → S where S, A are the
state and action spaces respectively. Note that this mapping can be stochastic.
Cost functions and constraints. The design of cost functions is central to the creation of opti-
mization problems. Epidemic control problems can often be framed as multi-objective problems,
where algorithms need to optimize for a set of cost functions instead of a unique one. Costs can be
defined at the level of timesteps C(s, a) or at the level ofan episode Ce(trajectory). This cumulative
cost measure can be obtained by summing timestep-based costs over an episode. For mathematical
convenience, RL practitioners often use a discounted sum of costs, where future costs are discounted
exponentially: Ce (trajectory) = Pt γt c(st , at) with discount factor γ (e.g. it avoids infinite sums
when the horizon is infinite). When faced with multiple cost functions Ci∖i∈[i..Nc], a simple ap-
proach is to aggregate them into a unique cost function C, computed as the convex combination of
the individual costs: C(s,a) = Pi βi Ci(s, a). Another approach is to optimize for the Pareto front,
i.e. the set of non-dominated solutions. A solution is said to be non-dominated if no other solution
performs better on all costs and strictly better on at least one. In addition, one might want to define
constraints on the problem (e.g. a maximum death toll over a year).
The epidemic control problem as a Markov Decision Process. Just like traditional RL prob-
lems, our epidemic control problem can be framed as a Markov Decision Process: M :
{S, A, T, ρo, C, Y}, where ρo is the distribution of initial states, see Yanez et al. (2019) for a similar
formulation.
Handling time limits. Epidemics are not episodic phenomena, or at least it is difficult to predict
their end in advance. Optimization algorithms, however, require episodes of finite lengths. This
enables the algorithm to regularly obtain fresh data by trying new policies in the environment. This
can be a problem, as we do not want the control policy to act as if the end of the episode was the end
of the epidemic: if the epidemic is growing rapidly right before the end, we want the control policy
to care and react. Evolution algorithms (EAs) cannot handle such behavior as they use episodic data
(θe , Ce ), where θe are the policy parameters used for episode e. RL algorithms, however, can be
made time unaware. To this end, the agent should not know the current timestep, and should not be
made aware of the last timestep (termination signal). Default RL implementations often ignore this
fact, see a discussion of time limits in Pardo et al. (2018).
3
Under review as a conference paper at ICLR 2021
3	The EpidemiOptim toolb ox
3.1	Toolbox desiderata
The EpidemiOptim toolbox aims at facilitating collaborations between the different fields interested
in the control of epidemics. Such a library should contain state-of-the-art epidemiological models,
optimization algorithms and relevant cost functions. It should enable optimization practitioners to
bring their newly-designed optimization algorithm, to plug it to interesting environments based on
advanced models grounded in epidemiological knowledge and data, and to reliably compare it to
other state-of-the-art optimization algorithms. On the other hand, it should allow epidemiologists
to bring their new epidemiological model, reuse existing cost functions to form a complete epi-
demic control problem and use state-of-the-art optimization algorithms to solve it. This requires a
modular approach where algorithms, epidemiological models and cost functions are implemented
independently with a standardized interface. The toolbox should contain additional tools to facilitate
experiment management (result tracking, configuration saving, logging). Good visualization tools
would also help the interpretation of results, especially in the context of multi-objective problems.
Ideally, users should be able to interact with the trained policies, and to observe the new optimal
strategy after the modification of the relative importance of costs in real-time. Last, the toolbox
should enable reproducibility (seed management) and facilitate statistical comparisons of results.
3.2	Toolbox organization
Overview. The EpidemiOptim toolbox is centered around two modules: the environment and the
optimization algorithm, see Figure 1. To run an experiment, the user can define its own or select the
ones contained in the library. Appendix Figure 4 presents the main interface of the EpidemiOptim
library. The following sections delve into the environment and algorithm modules.
Environment: epidemiological models. The Model module wraps around epidemiological mod-
els, and allows to sample new model parameters from a distribution of models, to sample initial
conditions (ρ0) and to run the model for n steps. Currently, the EpidemiOptim library contains
a Python implementation of an extended Susceptible-Exposed-Infectious-Removed (SEIR) model
fitted to French data, from Prague et al. (2020).
Environment: cost functions and constraints. In the EpidemiOptim toolbox, each cost function
is a separate class that can compute ci(s, a), normalize the cost to [0, 1] and generate constraints on
its cumulative value. The Multi-Cost module integrates several cost functions. It computes the list
of costs ci(s, a)∣i∈[i..Nc] and an aggregated cost c parameterized by the mixing weights βi∣i∈[i..Nc].
The current version of EpidemiOptim contains two costs: 1) a health cost computed as the death
toll of the epidemic; 2) an economic cost computed as the opportunity loss of GDP resulting from
either the epidemic itself (illness and death reduce the workforce), or the control strategies (lock-
down also reduces the workforce). In addition, we define two constraints as maximal values that the
cumulative measures of the two costs described above can take.
Environment: OpenAI Gym interface as a universal interface. The learning environment de-
fines the optimization problem. We use a framework that has become standard in the Reinforcement
Learning community in recent years: the OpenAI Gym environment (Brockman et al., 2016). Gym
environments have the following interface: First, they have a reset function that resets the environ-
ment state, effectively starting a new simulation or episode. Second, they have a step function that
takes as input the next action from the control policy, updates the internal state of the model and
returns the new environment state, the cost associated to the transition and a Boolean signal indicat-
ing whether the episode is terminated, see Figure 1. This standardized interface allows optimization
practitioners (RL researchers especially) to easily plug any algorithm, facilitating the comparison of
algorithms on diverse benchmark environments. In our library, this class wraps around an epidemi-
ological model and a set of cost functions. It also defines the state and action spaces S, A. So far, it
contains the EpidemicDiscrete-v0 environment, a Gym-like environment based on the epidemiolog-
ical model and the bi-objective cost function described above. Agents decide every week whether to
enforce a partial lock-down (binary action) that results in a decreased transmission rate.
4
Under review as a conference paper at ICLR 2021
Optimization algorithms. The optimization algorithm, or algorithm for short, trains learning
agents to minimize cumulative functions of costs. Learning agents are characterized by their control
policy: a function that takes the current state of the environment as input and produces the next
action to perform: Πθ : S → A where Πθ is parameterized by the vector θ. The objective of the
algorithm is, thus, to find the optimal set of parameters θ* that minimizes the cost. These algorithms
collect data by letting the learning agent interact with the environment. Currently, the EpidemiOp-
tim toolbox integrates several RL algorithms based on the famous Deep Q-Network algorithm (Mnih
et al., 2015) as well as the state-of-the-art multi-objective EA NSGA-II Deb et al. (2002).
Utils. We provide additional tools for experimentation: experience management, logging and
plots. Algorithms trained on the same environment can be easily compared, e.g. by plotting their
Pareto front on the same graph. We also enable users to visualize and interact with trained policies
via Jupyter notebooks. Users can select solutions from the Pareto front, modify the balance between
different cost functions and modify the value of constraints set to the agent via sliders. They visual-
ize a run of the resulting policy in real-time. We are also committed to reproducibility and robustness
of the results. For this reason, we integrate to our framework a library for statistical comparisons
designed for RL experiments (Colas et al., 2019).
4	Case study: lock-down on-off policy optimization for the
covid- 1 9 epidemic
The SARS-CoV-2 virus was identified on January, 7th 2020 as the cause of a new viral disease
named COVID-19. As it rapidly spread globally, the World Health Organization (2020) declared
a pandemic on March 11th 2020. Exceptional measures have been implemented across the globe
as an attempt to mitigate the pandemic (Kraemer et al., 2020). In many countries, the government
response was a total lock-down: self-isolation with social distancing, schools and workplaces clo-
sures, cancellation of public events, large and small gatherings prohibition, and travel limitation.
In France, lock-down lasted 55 days from March, 17th to May, 11th. It led to a decrease in gross
domestic product of, at least, 10.1% (Mandel & Veetil, 2020) but allowed to reduce the death toll
by an estimated 690,000 [570,000; 820,000] people (Flaxman et al., 2020). In this case study, we
investigate alternative strategies optimized by RL and EA algorithms over a period of 1 year.
4.1	Environment
The epidemiological model. We use a mathematical structural model to understand the large-scale
dynamics of the COVID-19 epidemic. We focus on the extended Susceptible-Exposed-Infectious-
Removed (SEIR) model from Prague et al. (2020), which was fitted to French data. The estimation
in this model was region-specific. Here, we use their estimated parameters for the epidemic dy-
namics in Ile-de-France (Paris and its surrounding region), presented in Appendix Table 1. The
model explains the dynamics between Susceptibles (S), Exposed (E), Ascertained Infectious (I),
Removed (R), Non-Ascertained Asymptomatic (A) and Hospitalized (H). The underlying system
of differential equations and a schematic view of the mechanistic model can be found in Appendix
Section A.3. To account for uncertainty in the model parameters, we create a distribution of models
resulting from parameter distributions, see details in Appendix Section A.3 and a visualization of 10
sampled models in Appendix Section A.3.2.
State and action spaces. The state of the agent is composed of the epidemiological states
(SEIRAH) normalized by the population size (N = 12, 278, 210), booleans indicating whether
the previous and current state are under lock-down, the two cumulative costs and the level of the
transmission rate (from 1 to 4). The two cumulative costs are normalized by N and 150B respec-
tively, 150B being the approximate maximal cost due to a full year of lockdown. The action occurs
weekly and is binary (1 for lock-down, 0 otherwise). Consecutive weeks of lockdown decrease the
b in four stages (see model description in Appendix Figure 5), while stopping lockdown follows the
opposite path (stair-case increase of b).
Cost functions. In this case-study, an optimal policy should minimize two costs: a health cost
and an economic cost. The health cost is computed as the death toll, it can be evaluated for each
environment transition (weekly) or over a full-episode (e.g. annually): Ch(t) = D(t) = 0.005R(t).
5
Under review as a conference paper at ICLR 2021
The economic cost is measured in euros as the GDP opportunity cost resulting from diseased or dead
workers, as well as people unemployed due to the lock-down policy. At any time t, the GDP can be
expressed as a Cobb-Douglas function F (L(t)) = AK0γk L(t)1-γk, where K0 is the capital stock of
economy before the outbreak that we will suppose constant during the pandemics, L(t) is the active
population employed, γk is the capital elasticity and A is the exogenous technical progress (Douglas,
1976). Let L0 = λN be the initial employed population, where λ is the activity/employment rate.
The lock-down leads to partial unemployment and decreases the size of the population able to work
(illness, isolation, death), thus L(t) = (1 - u(t))λ(N - G(t)), where u(t) is the level of partial
unemployment due to the lock-down and G(t) = I(t) + H(t) + 0.005R(t) is the size of the ill,
isolated or dead population as defined in our SEIRAH model. The economic cost is defined as the
difference between the GDP before (Y0) and after the pandemic, Y0 - F (L(t)) and is given by:
Ceco(t) = Y0 - AK0γk ((1 - u(t))λ(N - G(t))1-γk
Parameters for values of parameters in the economic model are given in Table 1. We also consider
the use of constraints on the maximum value of cumulative costs.
4.2	Optimization Algorithms
We consider three algorithms: 1) a vanilla implementation of Deep Q-Network (DQN) (Mnih et al.,
2015); 2) a goal-conditioned version of DQN inspired from (Schaul et al., 2015); 3) NSGA-II, a
state-of-the-art multi-objective evolutionary algorithm (Deb et al., 2002). The trained policy is the
same for all algorithms: a neural network with one hidden-layer of size 64 and ReLU activations.
Further background about these algorithm is provided in Appendix SectionA.3.
Optimizing convex combinations of costs: DQN and Goal-DQN. DQN only targets one cost
function: c = (1 - β) Ch + β Cec0. Here the health and economic costs are normalized to have
similar ranges (scaled by 1/(65 × 1e3) and 1/1e9 respectively). We train independent policies for
each values of β in [0., 0.05, .., 1]. This consists in training a Q-network to estimate the value of
performing any given action a in any given state s, where the value is defined as the cumulative
negative cost -C expected in the future. Goal-DQN, however, trains one policy Π(s, β) to target any
convex combination parameterized by β. We use the method presented in (Badia et al., 2020): we
train two Q-networks, one for each of the two costs. The optimal action is then selected as the one
maximizing the convex combination of the two Q-values:
Π(s, β) = argmaxa(1 - β) Qs(s, a) + β Qeco(s, a).	(1)
By disentangling the two costs, this method facilitates the representation of values that can have
different scales and enables automatic transfer for any value of β, see justifications in Badia et al.
(2020). During training, agents sample the targeted β uniformly in [0, 1].
Adding constraints with Goal DQN-C. We design and train a variant of Goal DQN to handle
constraints on maximal values for the cumulative costs: Goal-DQN-C. We train a Q-network for
each constraints with a cost of 1 each time the constraint is violated, 0 otherwise (Qhc, Qecco). With a
discount factor γ = 1, this network estimates the number of transitions that are expected to violate
the constraints in the future. The action is selected according to Eq. 1 among the actions that are
not expected to lead to constraints violations (Qc(s, a) < 1). If all actions are expected to lead to
violations, the agent selects the action that minimizes that violation. During training, agents sample
β uniformly, and 50% of the time samples uniformly one of the two constraints in [1000, 62000] for
the maximum death toll and [20, 160] for the maximum economic cost, in billions.
EAs to optimize a Pareto front of solutions. We also use NSGA-II, a state-of-the-art multi-
objective EA algorithm that trains a population of solutions to optimize a Pareto front of non-
dominated solutions (Deb et al., 2002). As others EAs, NSGA-II uses episodic data (θe, Ce). To
obtain reliable measures of Ce in our stochastic environment, we average costs over 30 evaluations.
For this reason, NSGA-II requires more samples than traditional gradient-based algorithms. For
fair comparison, we train two NSGA-II algorithm: one with the same budget of samples than DQN
variants (1e6 environment steps), and the other with 15x as much (after convergence).
6
Under review as a conference paper at ICLR 2021
4.3	Results
Pareto fronts. We aim at providing decision tools to decision-maker and, for this reason, cannot
decide ourselves the right value for the mixing parameter β . Instead, we want to present Pareto
fronts of Pareto-optimal solutions. Only NSGA-II aims at producing an optimal Pareto front as its
result while DQN and Goal DQN optimize a unique policy. However, one can build Pareto fronts
even with DQN variants. We can build population of policies by aggregating independenlty trained
DQN policies, or evaluating a single Goal DQN policy on a set of Npareto goals sampled uniformly
(100 here). The resulting population can then be filtered to produce the Pareto front. Figure 2a
presents the resulting Pareto fronts for all variants. The Pareto fronts made of several DQN policies
(yellow) perform better in the regime of low health cost (< 103 deaths) where it leads to save around
20 B euros over NSGA-II and the Goal DQN without constraint. Over 25 × 103 deaths, NSGA-II
becomes the best algorithm, as it allows to save 10 to 15 B euros in average, the health cost being
kept constant.
Figure 2: Left: Pareto fronts for various optimization algorithms. The closer to the origin point the
better. Right: Evolution of the costs when Goal DQN agents are evaluated on various β . We plot the
mean and standard error of the mean (left) and standard deviations (right) over 10 seeds.
(b)
In the low health cost regime, the DQN policy attempts to break the first wave of the epidemic, then
maintains it low with periodic lock-down (1 or 2 weeks period) until it is sufficiently low. Ifa second
wave appears, it goes back to that strategy, see Figure 3a for an example. Goal DQN with constraints
shows a similar behavior. NSGA-II enters a cyclic 1-week period of lock-down but does not stop it
even when the number of new cases appears null (Appendix Section A.4). In the low economic cost
regime (below 15 B), NSGA-II waits for the first wave to grow and breaks it by a lock-down of a
few weeks (Figure 3b). DQN-based algorithms sometimes find such policies. Policies in the range
of health cost between 103 and 50 × 103 deaths are usually policy that perform low or high health
costs depending on the particular epidemic and its initial state. Appendix Section 4.3 delves into the
different strategies and interesting aspects of the four algorithms.
Optimizing convex combinations. When β is low (respectively high), the cost function is closer
to the health (respectively economic) cost (see Eq 1). Figure 2b presents the evolution of the two
costs when a trained Goal DQN policy (without constraints) is evaluated on different β . As expected,
the higher β the lower the health cost and vice-versa. We see that, outside of the [0.2, 0.8] range, the
policy falls in extreme regimes: either it always locks-down (low β) or it never does (high β).
Interactive decision making. The graphs presented above help make sense of the general per-
formance of the algorithms, in average over multiple runs. However, it is also relevant to look at
particular examples to understand a phenomenon. For this purpose, we include visualization Jupyter
Notebooks in the EpidemiOptim toolbox. There are three notebooks for DQN, Goal DQNs and
NSGA-II respectively. The two first one help making sense of DQN and Goal DQN algorithms
(with or without constraints). After loading a pre-trained model, the user can interactively act on
the cost function parameters (mixing weight β or constraints on the maximum values of the cumu-
lative costs). As the cost function changes, the policy automatically adapts and the evolution of the
epidemic and control strategy are displayed. The third notebook helps visualizing a Pareto front of
the NSGA-II algorithm. After loading a pre-trained model, the user can click on solutions shown
on the Pareto front, which automatically runs the corresponding model and displays the evolution of
the epidemic and control strategy. The user can explore these spaces and visualize the strategies that
correspond to the different regimes of the algorithm.
7
Under review as a conference paper at ICLR 2021
Cost: 37.17
SEeaal"1OJ.
O XOO
SIneaaM∙N
100 200 300
Eco cost: 13.09 B, Death Cost: 41216, Aggregated Cost: 38.25
IOO 200 300
≡}ssol daoM∙N
100 200 300
40000
20∞0
SlHeaa I"1OJ.
≡} SSol dao,loj.
IOO 28 300
days
100 28 300
(a)	(b)
Figure 3: Evolution of cost metrics and strategy over a year of simulated epidemic with aDQN agent
(left) in the low health cost regime and a NSGA-II agent in the low economic cost regime (right).
Discussion. This case study proposed to use a set of state-of-the-art algorithms to target the prob-
lem of lock-down policy optimization in the context of the COVID-19 epidemic (RL and EA algo-
rithms). DQN appears to work better in the regime of low health cost, but requires to train multiple
policies independently. As it trains only one policy, Goal DQN is faster, but does not bring any per-
formance advantage. NSGA-II, which is designed for the purpose of multi-objective optimization,
seems to perform better in the regime of low economic cost.
5 Discussion & Conclusion
On the use of automatic optimization for decision-making. We do not think that optimization
algorithms should ever replace decision-makers, especially in contexts involving the life and death
of individuals such as epidemics. However, we think optimization algorithms can provide decision-
makers with useful insights by taking into account long-term effects of control policies. To this end,
we believe it is important to consider a wide range of models and optimization algorithms to target
a same control problem. This fosters a diversity of methods and reduces the negative impacts of
model- and algorithm-induced biases. The EpidemiOptim toolbox is designed for this purpose: it
provides a one-stop shop centralizing both the modeling and the optimization in one place.
The future of EpidemiOptim. This toolbox is designed as a collaborative toolbox that will be
extended in the future. To this day, it contains one population-based SEIR epidemiological models
and three optimization algorithms. In the future, we plan on extending this library via collaborations
with other research labs in epidemiology and machine learning. We plan on integrating agent-based
epidemiological models that would allow a broader range of action modalities, and to add a variety
of multi-objective algorithms from EAs and RL. We could finally imagine the organization of chal-
lenges, where optimization practitioners would compete on a set of epidemic control environments.
Although this toolbox focuses on the use of optimization algorithm to solve epidemic control poli-
cies, similar tools could be used in other domains. Indeed, just like epidemics, a large diversity of
dynamical systems can be modeled models by ordinary differential equations (e.g. in chemistry,
biology, economics, etc.). One could imagine spin-offs of EpidemiOptim for applications to other
domains. Furthermore, Chen et al. (2018) recently designed a differentiable ODE solver. Such
solvers could be integrated to EpidemiOptim to allow differential learning through the model itself.
Conclusion. This paper introduces EpidemiOptim, a toolbox that facilitates collaboration between
epidemiologists, optimization practitioners and others in the study of epidemic control policies to
help decision-making. Of course, others have studied the use of optimization algorithms for the
control of epidemics. We see this as a strength, as it shows the relevance of the approach and the
importance of centralizing this research in a common framework that allows easy and reproducible
comparisons and visualizations. Articulating our understanding of epidemics evolution with the so-
cietal impacts of potential control strategies on the population (death toll, physical and psychological
traumas, etc) and on the economy will help improve the control of future epidemics.
8
Under review as a conference paper at ICLR 2021
Acknowledgments
References
Teodoro Alamo, Daniel G Reina, and Pablo Millan. Data-driven methods to monitor, model, forecast
and control covid-19 pandemic: Leveraging data science, epidemiology and control theory. arXiv
preprint arXiv:2006.01731, 2020.
Mauricio Arango and Lyudmil Pelov. Covid-19 pandemic cyclic lockdown optimization using rein-
forcement learning. arXiv preprint arXiv:2009.04647, 2020.
Adria Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. arXiv
preprint arXiv:2003.13350, 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Simon Cauchemez, Nathanael Hoze, Anthony Cousien, Birgit Nikolay, et al. How modelling can
enhance the analysis of imperfect epidemic data. Trends in parasitology, 35(5):369-379, 2019.
Amit Chandak, Debojyoti Dey, Bhaskar Mukhoty, and Purushottam Kar. Epidemiologically and
socio-economically optimal policies via bayesian optimization. arXiv preprint arXiv:2005.11257,
2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018.
Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. A hitchhiker,s guide to statistical compar-
isons of reinforcement learning algorithms. arXiv preprint arXiv:1904.06979, 2019.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist mul-
tiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2):
182-197, 2002.
Kalyanmoy Deb, S Karthik, et al. Dynamic multi-objective optimization and decision-making using
modified nsga-ii: a case study on hydro-thermal power scheduling. In International conference
on evolutionary multi-criterion optimization, pp. 803-817. Springer, 2007.
Paul H. Douglas. The cobb-douglas production function once again: Its history, its testing, and some
new empirical values. Journal of Political Economy, 84(5):903-915, 1976. doi: 10.1086/260489.
URL https://doi.org/10.1086/260489.
Neil Ferguson, Daniel Laydon, Gemma Nedjati Gilani, Natsuko Imai, Kylie Ainslie, Marc Baguelin,
Sangeeta Bhatia, Adhiratha Boonyasiri, ZULMA Cucunuba Perez, Gina Cuomo-Dannenburg,
et al. Report 9: Impact of non-pharmaceutical interventions (npis) to reduce covid19 mortality
and healthcare demand. 2020.
Neil M Ferguson, Derek AT Cummings, Simon Cauchemez, Christophe Fraser, Steven Riley, Aron-
rag Meeyai, Sopon Iamsirithaworn, and Donald S Burke. Strategies for containing an emerging
influenza pandemic in southeast asia. Nature, 437(7056):209-214, 2005.
Neil M Ferguson, Derek AT Cummings, Christophe Fraser, James C Cajka, Philip C Cooley, and
Donald S Burke. Strategies for mitigating an influenza pandemic. Nature, 442(7101):448-452,
2006.
Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Thomas A Mellan, Helen Cou-
pland, Charles Whittaker, Harrison Zhu, Tresnia Berah, Jeffrey W Eaton, et al. Estimating the
effects of non-pharmaceutical interventions on covid-19 in europe. Nature, 584(7820):257-261,
2020.
9
Under review as a conference paper at ICLR 2021
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of ICML,
2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In Proceedings of ICLR, 2019.
M Elizabeth Halloran, Neil M Ferguson, Stephen Eubank, Ira M Longini, Derek AT Cummings,
Bryan Lewis, Shufu Xu, Christophe Fraser, Anil Vullikanti, Timothy C Germann, et al. Modeling
targeted layered containment of an influenza pandemic in the united states. Proceedings of the
NationalAcademyofSciences,105(12):4639—4644, 2008.
Karel Havik, Kieran Mc Morrow, Fabrice Orlandi, Christophe Planas, Rafal Raciborski, Werner
Roger, Alessandro Rossi, Anna Thum-Thysen, Valerie Vandermeulen, et al. The production
function methodology for calculating potential growth rates & output gaps. Technical report,
Directorate General Economic and Financial Affairs (DG ECFIN), European . . . , 2014.
Insee. Insee website. https://www.insee.fr/, 2020.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce-
ment learning for vision-based robotic manipulation. In Conference on Robot Learning, pp. 651—
673, 2018.
Moritz U. G. Kraemer, Chia-Hung Yang, Bernardo Gutierrez, Chieh-Hsi Wu, Brennan Klein,
David M. Pigott, Louis du Plessis, Nuno R. Faria, Ruoran Li, William P. Hanage, John S. Brown-
stein, Maylis Layan, Alessandro Vespignani, Huaiyu Tian, Christopher Dye, Oliver G. Pybus,
and Samuel V. Scarpino. The effect of human mobility and control measures on the COVID-19
epidemic in China. Science, 368(6490):493^97, 2020. doi: 10.1126∕science.abb4218.
Pieter Libin, Arno Moonens, Timothy Verstraeten, Fabian Perez-Sanjines, Niel Hens, Philippe
Lemey, and Ann Nowe. Deep reinforcement learning for large-scale epidemic control. arXiv
preprint arXiv:2003.13676, 2020.
Antoine Mandel and Vipin P. Veetil. The economic cost of covid lockdowns: An out-of-equilibrium
analysis. Economics of Disasters and Climate Change, 2020. doi: 10.2139/ssrn.3588421.
Risto Miikkulainen, Olivier Francon, Elliot Meyerson, Xin Qiu, Elisa Canzani, and Babak Hodjat.
From prediction to prescription: Ai-based optimization of non-pharmaceutical interventions for
the covid-19 pandemic. arXiv preprint arXiv:2005.13766, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement
learning. In International Conference on Machine Learning, pp. 4045-4054, 2018.
Melanie Prague, Linda Wittkop, Quentin Clairon, Dan Dutartre, Rodolphe Thiebaut, and
Boris Pierre Hejblum. Population modeling of early covid-19 epidemic dynamics in french re-
gions and estimation of the lockdown impact on infection rate. medRxiv, 2020.
W. J. M. Probert, S. Lakkur, C. J. Fonnesbeck, K. Shea, M. C. Runge, M. J. Tildesley, and M. J. Fer-
rari. Context matters: using reinforcement learning to develop human-readable, state-dependent
outbreak response policies. Philosophical Transactions of the Royal Society B: Biological Sci-
ences, 374(1776):20180277, 2019. doi: 10.1098/rstb.2018.0277.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International conference on machine learning, pp. 1312-1320, 2015.
Freya M Shearer, Robert Moss, Jodie McVernon, Joshua V Ross, and James M McCaw. Infectious
disease pandemic planning and response: Incorporating decision analysis. PLoS medicine, 17(1):
e1003018, 2020.
10
Under review as a conference paper at ICLR 2021
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Luls Tarrataca, CM Dias, Diego Barreto Haddad, and EF Arruda. Flattening the curves: on-off
lock-down strategies for covid-19 with an application to brazi. arXiv preprint arXiv:2004.06916,
2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
World Health Organization. Who director-general’s opening remarks at the media briefing on
COVID-19 - 11 march 2020, 2020. Accessed: 2020-04-02.
Reza Yaesoubi and Ted Cohen. Identifying cost-effective dynamic policies to control epidemics.
Statistics in medicine, 35(28):5189-5209, 2016.
Reza Yaesoubi, Joshua Havumaki, Melanie H Chitwood, Nicolas A Menzies, Gregg Gonsalves,
Joshua Salomon, A David Paltiel, and Ted Cohen. Adaptive policies for use of physical distancing
interventions during the covid-19 pandemic. medRxiv, 2020.
Andrea Yanez, Conor Hayes, and Frank Glavin. Towards the control of epidemic spread: Designing
reinforcement learning environments. In AICS, pp. 188-199, 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 EpidemiOptim interface
Figure 4 presents the main interface of the EpidemiOptim library.
from epidemioptim.environments.models import get_model
from epidemioptinι.environments.cost_functions import get_cost_function
from epidemioptim.environments.gym_envs import get_env
from epidemioptim.optimization import get_algorithm
from epidemioptim.configs.get_params import get_params
config = ,dqn1
#	Get the configuration
params = get_params(config_id=config)
#	Get the epidemiological model
model = get_model(model_id=params[,model_id1], params=params['model.params1])
#	Get cost function
cost_function = get_cost_function(cost_function_id=params[,cost-idl], params=params[1cost_params,])
#	Create the optimization problem as a Gym-Like environment
env = get_env(env_id=params[1env_id'], cost_function=cost_function, model=model, sim_horizon=params['sim_horizon,], seed=params[1 seed'])
#	Get DQN algorithm parameterized by beta
algorithm = get_algorithm(algo_td=params[,algo_id'], env=env, params=params)
#	Run the training loop
algorithm.learn(num_train_steps=params[1num_train_steps1])
Figure 4:	Main running script of the EpidemiOptim library
A.2 Extended related works
Prior to the current COVID-19 pandemics, Yanez et al. (2019) framed the problem of finding optimal
intervention strategies for a disease spread as a reinforcement learning problem, focusing on how
to design environments in terms of disease model, intervention strategy, reward function, and state
representations. In Alamo et al. (2020), the CONCO-Team (CONtrol COvid-19 Team) provides a
detailed SWOT analysis (Strengths, Weaknesses, Opportunities, Threats) and a roadmap that goes
from the access to data sources to the final decision-making step, highlighting the interest of standard
optimization methods such as Optimal Control Theory, Model Predictive Control, Multi-objective
control and Reinforcement Learning. However, as argued in Shearer et al. (2020), a decision model
for pandemic response cannot capture all of the social, political, and ethical considerations that
impact decision-making. It is is therefore a central challenge to propose tools that can be easily
used, configured and interpreted by decision-makers.
The aforementioned contributions provide valuable analyses on the range of methods and challenges
involved in applying optimization methods to assist decision making during pandemics events. They
do not, however, propose concrete implementations of such systems. Several recent contributions
have proposed such implementations. Yaesoubi et al. (2020) applied an approximate policy iteration
algorithm of their own (Yaesoubi & Cohen, 2016) on a SEIR model calibrated on the 1918 Influenza
Pandemic in San Francisco. An interesting contribution of the paper was the development of a
pragmatic decision tool to characterize adaptive policies that combined real-time surveillance data
with clear decision rules to guide when to trigger, continue, or stop physical distancing interventions.
Arango & Pelov (2020) applied a more standard RL algorithm (Double Deep Q-Network) on a SEIR
model of the COVID-19 spread to optimize cyclic lock-down timings. The reward function was a
fixed combination ofa health objective (minimization of overshoots of ICU bed usage above an ICU
bed threshold) and an economic objective (minimization the time spent under lock-downs). They
compared the action policies optimized by the RL algorithm with a baseline on/off feedback control
(fixed-rule), and with different parameters (reproductive number) of the SEIR model. Libin et al.
(2020) used a more complex SEIR model with coupling between different districts and age groups.
They applied the Proximal Policy Optimization algorithm to learn a joint policy that control the
districts using a reward function quantifying the negative loss in susceptibles over one simulated
week, with a constraint on the school closure budget. Probert et al. (2019) used Deep Q-Networks
(DQN) and Monte-Carlo control on a stochastic, individual-based model of the 2001 foot-and-mouth
12
Under review as a conference paper at ICLR 2021
disease outbreak in the UK. An original aspect of their approach was to define the state at time t
using an image of the disease outbreak to capture the spatial relationships between farm locations,
allowing the use of convolutional neural networks in DQN. Other contributions applied non-RL
optimization methods such as deterministic rules (Tarrataca et al., 2020), stochastic approximation
algorithms (Yaesoubi et al., 2020) or Bayesian optimization (Chandak et al., 2020). This latter paper
also proposes a stochastic agent-based model called VIPER (Virus-Individual-Policy-EnviRonment)
allowing to compare the optimization results on variations of the demographics and geographical
distribution of population. Finally, Miikkulainen et al. (2020) proposed an original approach using
Evolutionary Surrogate-assisted Prescription (ESP). In this approach, a recurrent neural network (the
Predictor) was trained with publicly available data on infections and NPIs in a number of countries
and applied to predicting how the pandemic will unfold in them in the future. Using the Predictor, an
evolutionary algorithm generated a large number of candidate strategies (the Prescriptors) in a multi-
objective setting to discover a Pareto front that represents different tradeoffs between minimizing the
number of COVID-19 cases, as well as the number and stringency of NPIs (representing economic
impact).
As we have seen, existing contributions widely differ in their definition of epidemiological models,
optimization methods, cost functions, state and action spaces, as well as methods for representing the
model decisions in a format suitable to decision-makers. Our approach aims at providing a standard
toolbox facilitating the comparison of different configurations along these dimensions in order to
assist decision-makers in the evaluation and the interpretation of the range of possible intervention
strategies.
A.3 Additional methods
A.3.1 Optimization algorithms
The Deep Q-Network algorithm. DQN was introduced in (Mnih et al., 2015) as an extension
of the Q-learning algorithm (Watkins & Dayan, 1992) for policies implemented by deep neural
networks. The objective is to train a Q-function to approximate the value of a given state-action pair
(s, a). This value can be understood as the cumulative measure of reward that the agent can expect
to collect in the future by performing action at now and following an optimal policy afterwards. In
the Q-learning algorithm, the Q-function is trained by:
Q(st, at) J Q(st, at) + α[rt+1 + Y maxa Q(St+1, a) - Q(st, at)].
Here, α is a learning rate, [rt+1 + γ maxa Q(st+1, a) - Q(st, at)] is called the temporal difference
(TD) error with rt+1+γ maxa Q(st+1, a) the target. Indeed, the value of the current state-action pair
Q(st, at) should be equal to the immediate reward rt+1 plus the value of the next state Q(st+1, at+1)
discounted by the discount factor γ. Because we assume we behave optimally after the first action,
action at+1 should be the one that maximizes Q(st+1, a). Once this function is trained, the agent
simply needs to take the action that maximize the Q-ValUe in its current state: a* = max。Q(st, a).
Deep Q-Network only brings a few mechanisms to enable the use of large policies based on deep
neural network see Mnih et al. (2015) for details.
Goal-parameterized DQNs. Schaul et al. (2015) introduced Universal Value Function Approxi-
mators: the idea that Deep RL agents could be trained to optimize a whole space of cost functions
instead of a unique one. In their work, cost functions are parameterized by latent codes called goals.
An agent in a maze can decide to target any goal position within that maze. The cost function is
then a function of that goal. To this end, the agent is provided with its current goal. In the traditional
setting, the goal was concatenated to the state of the agent. In a recent work, Badia et al. (2020) used
a cost function decomposed into two sub-costs. Instead of mixing the two costs at the cost/reward
level and learning Q-functions from this signal, they proposed to learn separate Q-functions, one for
each cost, and to merge them afterwards: Q(s, a) = (1 - β) Q1(s, a) + β Q2(s, a). We use this
same technique for our multi-objective problems.
Using constraints. In RL, constraints are usually implemented by adding a strong cost whenever
the constraint is violated. However, it might be difficult for the Q-function to learn how to model
these sudden jumps in the cost signal. Instead, we take inspiration from the work of Badia et al.
(2020). We train a separate Q-function for each cost, where the associated reward is -1 whenever
13
Under review as a conference paper at ICLR 2021
the constraint is violated. This Q-function evaluates how many times the constraint is expected to
be violated in the future when following an optimal policy. Once this is trained, we can use it to
filtrate the set of admissible actions: if Qconstraint(s, a) > 1, then we expect action a to lead to at least
1 constraint violation. Once these actions are filtered, the action is selected according to the usual
maximization of the traditional Q-function.
NSGA-II. Multi-objective algorithms do not require the computation ofan aggregated cost but use
all costs. The objective of such algorithms is to obtain the best Pareto front. NSGA-II (Deb et al.,
2002) is a state-of-the-art multi-objective algorithm based on EAs (here a genetic algorithm). Start-
ing from a parent population of solutions, parents are selected by a tournament selection involving
the rank of the Pareto front they belong to (fitness) and a crowding measure (novelty). Offspring are
then obtained by cross-over of the parent solutions and mutations of the resulting parameters. The
offspring population is then evaluated and sorted in Pareto fronts. A new parent population is finally
obtained by selecting offspring from the higher-ranked Pareto fronts, prioritizing novel solutions as
a second metric (low crowding measures).
A.3.2 Epidemiological model
Figure 5 presents the schematic view of the mechanistic model behind the SEIRAH model from
Prague et al. (2020) (left), and its underlying system of differential equations (right). See details in
the original paper (Prague et al., 2020).
dS
dt
dE
dt
di
dt
dR
dt
dA
dt
dH
dt
b
W+ ")
N
bS(I + αA) E
N 瓦
τE I I
瓦一瓦一瓦
(/+4) + JL (1)
D1 十 Dh
(1 一 r)E _ A_
De Dj
I H
瓦一瓦
bθ exp(fιyι Ifirst week lock-down
+βw2 1second week lock-down
+βw3 Uthird week lock-down
^^~βw4 1thereafter under lock-down)
Figure 5:	Description of the SEIRAH epidemiological model. Adapted from Prague et al. (2020)
To account for uncertainty in the model parameters, we use a distribution of models. At each episode,
the epidemiological model is sampled from that distribution. The transition function is thus condi-
tioned on a latent code (the model parameters) that is unknown to the agent. This effectively results
in a stochastic transition from the viewpoint of the agent. To build the distribution of models, we
assume a normal distributions for each of the model parameters, using either the standard deviation
from the model inversion for parameters estimated in Prague et al. (2020) or 10% of the mean value
for other parameters. Those values are available in appendix Figure 6 features for reproducibility of
the experiment. We further add a uniformly distributed delay between the epidemic onset and the
start of the learning episode (uniform in [0, 21 days]). This delay models the reaction time of the
decision authorities. This results in the distribution of models shown in Figure 7.
Epidemiological model and economic cost function parameters. Table 1 lists the parameters
of the epidemiological model (from Prague et al. (2020)) and the economic cost function (from the
National Institute of Statistics and Economical Analysis Insee (2020) and Havik et al. (2014)).
Epidemiological model and cost function parameters. Table 1 presents the list of parameters
used in the epidemiological model (top) and the economic cost function (bottom).
14
Under review as a conference paper at ICLR 2021
Param. Interpretation
Value ”Ile-de-France”
eIqh oo ooo
brαDDDDNβ SEIRAHD
Transmission rate of ascertained cases before lock-down	2.23
Ascertainement rate	0.043
Ratio of transmission between A and I	0.55
Latent (incubation) period (days)	5.1
Infectious period (days)	2.3
Duration from I onset to H (days)	0.36
Hospitalization period (days)	30
Population size	12,278,210
Lock-down effects first (βw1), second (βw2),	(-0.11, -0.50,
third (βw3) and thereafter weeks (βw0)	-1.36, -1.46)
Initial number of susceptible	N 一 Eo 一 Io 一 Ro 一 Ao 一 Ho
Initial number of exposed	5004
Initial number of ascertained	16
Initial number of removed	0
Initial number of non ascertained	356
Initial number of hospitalized	4
Initial number of death	0
Ko	Initial capital stock in millions euros	1,388,912
Lo	Number of employed individuals in millions	4.58
λ	Activity/employment rate (%)	37.4
Yo	Initial GDP in million euros / year	424,474
A	Exogenous technical progress	867
u(t)	Level of partial unemployment during lock-down (%)	50
γk	Capital elasticity	0.37
Table 1: Parameter values for the Ile-de-France region for the SEIRAH epidemiological model and
the economy cost function.
Wwww::
UsQ 瓦β2β3β4DDDCrar
Eo	b	βlw	02w	03w	04w	Dq	De	Dh	Di	α	r
52492	-0,024	-0,144	0,024	0,020	0,008	0,009	0	0	0	0	0
-0,024	0,00001	0	0	0	0	0	0	0	0	0	0
-0,144	0	0,004	0	0	0	0	0	0	0	0	0
0,024	0	0	0,003	0	0	0	0	0	0	0	0
0,020	0	0	0	0,003	0	0	0	0	0	0	0
0,008	0	0	0	0	0,0002	0	0	0	0	0	0
0,009	0	0	0	0	0	0,0001	0	0	0	0	0
0	0	0	0	0	0	0	0,26	0	0	0	0
0	0	0	0	0	0	0	0	9	0	0	0
0	0	0	0	0	0	0	0	0	0,05	0	0
0	0	0	0	0	0	0	0	0	0	0,003	0
0	0	0	0	0	0	0	0	0	0	0	0,00002
Figure 6:	Variance-covariance matrix for parameters of the epidemiological model.
15
Under review as a conference paper at ICLR 2021
A.4 Additional results
A.4. 1 A distribution of models
Figure 7 presents the evolution of the SEIRAH model states for a few models sampled from the
distribution of models described in 4.1. Here, no lockdown is enforced.
Eco cost: 1.85 B1 Death Cost:
57649, Aggregated Cost:
Sqlega 5φzs} SSo-Idaosφz
2000
IOOO
0.015
0.010
0.005
0.000
60000
40000
20000
45.27
2 O
P£e6®』664
100 200 300
-32
2 2
CO-S-EWCEF
Figure 7:	10 models sampled from the distribution of epidemiological models used in the case-study.
Now, let us illustrate in details the results of the case study. To this end, we provide several runs of
the different algorithms to illustrate their strategies.
16
Under review as a conference paper at ICLR 2021
A.4.2 DQN
We first present strategies found by DQN.
1200
1180
1160
cost: 155,16 B, Death Cost: 3513, Aggregated Cost
slμl0φα 一£。H
(巴 SSo-IdCIoMφN
(巴 SSo-IdCIo-5of
CO»$»EWCEF
roTX} S
Figure 8:	DQN trained with β = 0.. For one run, this figure shows the evolution of model states
(above) and states relevant for optimization (below). The aggregated cost is shown for various
values of β in [0, 0.25, 0.5, 0.75, 1]. For β = 0, the agent only cares about the health cost, it always
locks-down.
17
Under review as a conference paper at ICLR 2021
1225.0
1222.5
1220.0
roτ×S
cost: 62.81 B, Death Cost: 556, Aggregated Cost: 31.84
Figure 9: DQN trained with β = 0.5. Here the strategy is cyclical with a one or two weeks pe-
riod. For one run, this figure shows the evolution of model states (above) and states relevant for
optimization (below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
18
Under review as a conference paper at ICLR 2021
1200
1180
GOTδS
B, Death Cost: 2804,
Aggregated Cost: 34.86
59.41
Eco cost:
φlel CO»S»EWCEF
nππ
ISoupglBSRSm
3000
2000

slμl0φα 一£。H
j∖______________A
- I
SIlll0φα mφn
300


O
6
O O
2
300

s} SSo-IdCIo-5of
42 Q
Ooo
S SOJ æɑo sφz
Figure 10:	DQN trained with β = 0.55. Here the agent first lock-down to stop the first wave, but
then stops the cyclical lockdown early which induces a second wave later, where the agent also
reacts by a lock-down. For one run, this figure shows the evolution of model states (above) and
states relevant for optimization (below). The aggregated cost is shown for various values of β in
[0, 0.25, 0.5, 0.75, 1].
19
Under review as a conference paper at ICLR 2021
56621, Aggregated Cost:
Death Cost：
31.62
SIlIl0φα mφn
2000
1000
wr⅞ΦQ -5OF
40000
20000
σ>ι
IOO 200 300
O 3
U
P 7
Φ 2
----ɪo
5 0 5 0
l.l.d d
(巴 Sso-I dα□ffll。.L
IOO 200 300
days
100 200
300
■3-2
22
£.! CO-S-EWCEF
300
100 200
ɪ
Figure 11:	DQN trained with β = 0.65. Here the agent mostly cares about the economic cost, which
results in a no-lockdown policy. For one run, this figure shows the evolution of model states (above)
and states relevant for optimization (below). The aggregated cost is shown for various values of β
in [0, 0.25, 0.5, 0.75, 1].
20
Under review as a conference paper at ICLR 2021
A.4.3 Goal-DQN without constraints
Now we present a few strategies found by Goal-DQN without constraints.
roTX> S
1210
1200
cost: 92.74 B, Death Cost: 1350, Aggregated Cost: 20.21
- I
slμeφɑ1≡*joh
Figure 12: Goal-DQN without constraints evaluated in β = 0.2. Here the agent starts with a lasting
lockdown, then pursues with cyclical lock-downs, which ensures the absence of second wave and,
thus, results in low health cost but high economic cost. For one run, this figure shows the evolution
of model states (above) and states relevant for optimization (below). The aggregated cost is shown
for various values of β in [0, 0.25, 0.5, 0.75, 1].
21
Under review as a conference paper at ICLR 2021
1210
1200
roτ×S
cost: 59.84 B, Death Cost: 1301, Aggregated Cost: 28.03
Figure 13: Goal-DQN without constraints evaluated in β = 0.65. Here we find a cyclical strategy
equivalent to the one of shown with a DQN agent trained with β = 0.5. For one run, this fig-
ure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
- I
slμeφɑ1≡*joh
22
Under review as a conference paper at ICLR 2021
Death Cost:
60000
40000
20000
28.88
55507, Aggregated Cost:
100	200
300
-2∙1Q
£.! CO-S-EWCEF
300
100	200
100	200 300
days
--- ⅜
4 2 0
(巴 Sso-I dα0Eo-L
2000
1000
Eco cost: 4.66 B,
wr⅞ΦQ -5OF
SIlIl0φα mφn
100	200	300
IOO 200 300
(巴 SSo-IdCIoMφN
Figure 14: Goal-DQN without constraints evaluated in β = 0.7. Here the agent only use a lock-
down at the very beginning of the epidemic. It is unclear whether this has an impact on the health
cost at all. For one run, this figure shows the evolution of model states (above) and states relevant for
optimization (below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
23
Under review as a conference paper at ICLR 2021
A.4.4 Goal-DQN with constraints
Now we present a few strategies found by Goal-DQN with constraints, and study how it reacts to
health and economic constraints.
12
1222
Sqll0φα mφn
S SSo-IdCIoMφN
cost: 80.75 B, Death Cost: 297, Aggregated Cost: 24.55
Figure 15: Goal-DQN with constraints evaluated in β = 0.3, Meconomic = 160B, Mhealth =
62000 deaths. This boils down to no constraints as they are maximal values. This leads to a cyclical
policy. For one run, this figure shows the evolution of model states (above) and states relevant for
optimization (below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
slμl0φcl -5of
roTX> S
24
Under review as a conference paper at ICLR 2021
Ooo
0 5 0
W 7 5
roτ×1 5
Oooo
5 0 5
7 5 2
0τ×}
O IOO 200	300
Oooo Oooo
5 0 5 8 6 4 2
T (s1×w FoT×v
O IOO 200 300
days
Death Cost：
Eco cost: 42.74 B
1500
- I
SIlll0φα mφn
slIlsdEOJ.
s} SSo-IdCIosφz
O
2 O
S SSo-IdCIo-5of
4000
3000
2000
5 0 5 0
ɪ ɪ
r0τ×} X
O 100	200	300
45278, Aggregated Cost: 61.58
40000
20000
IOO 200	300
--- - Jo
2 10
%。UPφle62664
IOO 200	300
days
^lhuπuo
2 10
£e=EO-S-EWESF


2
2
Figure 16: Goal-DQN with constraints evaluated in β = 0.3, Meconomic = 55B, Mhealth =
62000 deaths. This is the same β as Figure 15 (previous page). Now there is no constraint on
the number of deaths but a strong constraint on the economic cost. The strategy is not cyclical any-
more, as the resulting economic cost would be too high. This strategy stays below the economic
constraint but still tries to minimize the health cost. For one run, this figure shows the evolution of
model states (above) and states relevant for optimization (below). The aggregated cost is shown for
various values of β in [0, 0.25, 0.5, 0.75, 1].
25
Under review as a conference paper at ICLR 2021
4000
3000
2000
IOOO
5 0 5
ɪ ɪ
r0τ×} X
20
O O
8 6
Ooooo
0 5 0 5
2 11
0τ×1 W
Ooo
5 0 5
7 5 2
0τ×}
^752
r0τ×> 5
r0τ×} <
26.41
O IOO 200	300
O IOO 200 300
Death Cost: 54604, Aggregated Cost
wr⅞ΦQ -5OF
L
Figure 17: Goal-DQN with constraints evaluated in β = 0.7, Meconomic = 160B, Mhealth =
62000 deaths. There is no constraint. As the balance favorizes the economic cost, the strategy does
not implement any lock-down. For one run, this figure shows the evolution of model states (above)
and states relevant for optimization (below). The aggregated cost is shown for various values of β
in [0, 0.25, 0.5, 0.75, 1].

1500
IOOO
500
SIlll0φα mφn
S SSo-IdCIoMφN
26
Under review as a conference paper at ICLR 2021
1200
W 8
r0τ×> 5
Oooo
QgO
6 4 2
roτ×}
B, Death
78.37
Eco cost:
Cost: 27214, Aggregated Cost: 67.42
2
30000
20000
O 100	200	300
ISoupglBSRSm
O
Oooo
Ooo
3 2 1
SIllα mn
300
IOO 200
-5OF
2 1
uaSS-ESUe-IJ.
O IOO 200	300
days
5 0 5 0
7 5 2
(巴 SSo-IdCIo-5of
s} SSo-IdCIosφz
300
Figure 18: Goal-DQN with constraints evaluated in β = 0.7, Meconomic = 160B, Mhealth =
30500 deaths. Now we have the same setup as the previous page (Figure 17), except that we have
a strong constraint on the number of deaths. The resulting strategy respect the constraint while
attempting to minimize the economic cost. For one run, this figure shows the evolution of model
states (above) and states relevant for optimization (below). The aggregated cost is shown for various
values ofβin [0, 0.25, 0.5, 0.75, 1].
27
Under review as a conference paper at ICLR 2021
1200
IlOO
Eco cost: 68.56 B, Death
wr⅞ΦQ -5OF
Cost: 15114, Aggregated Cost:
15000
IOOOO
5000
928.43
IOO 200
(巴 SSo-IdCIo1s1oh
IOO 200
£10.! CO»$»EWCEF
GOTX} S
Figure 19: Goal-DQN with constraints evaluated in β = 0.3, Meconomic = 55B, Mhealth =
15000 deaths. Here we have strong constraints on both economic costs. In that case, there is no
good solution. This strategy respects the health constraint but violates the economic constraint. For
one run, this figure shows the evolution of model states (above) and states relevant for optimization
(below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
28
Under review as a conference paper at ICLR 2021
A.4.5 NSGA-II
Now we present a few strategies found by NSGA-II.
1225.0
1222.5
1220.0
O IOO 200	300	O IOO 200	300
days
IOO 200 300
:40.79
Eco cost: 80.76 B, Death Cost: 538, Aggregated Cost
wr⅞ΦQ -5OF
Figure 20: NSGA-II, the closest point to [8000 deaths, 60B] in the Pareto front. In this low health
cost regime, NSGA-II finds a cyclical strategy with a period of 2 weeks. For one run, this fig-
ure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
4 2 0 2 1 0
æ æ æ CO-S-EWCEF
≡≡
O IOO 200	300
29
Under review as a conference paper at ICLR 2021
W 7 5 2
roτ×1 5
cost: 13.53 B, Death Cost: 53414, Aggregated Cost: 47.85
Figure 21: NSGA-II, the closest point to [30000 deaths, 40B] in the Pareto front. Here we find that
NSGA-II use alternative strategies depending on the epidemiological models it faces. The average
of these strategies ends up close to 30000, although the two strategies either find high health costs
(>50000) or low ones (<1000). These plots show the first alternative, where the strategy aims at
breaking the first wave of the epidemic. See the other alternative in Figure 22. For one run, this
figure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
4
2
- I
SIlll0φα mφn
slIlsdEOJ.
O
2 O
s} SSo-IdCIosφz
30
Under review as a conference paper at ICLR 2021
W 7 5 2
roτ×1 5
cost: 13.53 B, Death Cost: 53414, Aggregated Cost: 47.85
Figure 22: NSGA-II, the closest point to [30000 deaths, 40B] in the Pareto front. Here we find that
NSGA-II use alternative strategies depending on the epidemiological models it faces. The average
of these strategies ends up close to 30000, although the two strategies either find high health costs
(>50000) or low ones (<1000). These plots show the second alternative, where the strategy is
cyclical and achieved low health costs. See Figure 21 for the first alternative. For one run, this
figure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
4
2
- I
SIlll0φα mφn
slIlsdEOJ.
O
2 O
s} SSo-IdCIosφz
31
Under review as a conference paper at ICLR 2021
1200
10'8'6,
r0τ×> 5
GOTδU
400
0	100	200	300
0	100	200	300
Eco cost: 13.26 B
1500
- I
SIlll0φα mφn
40000
20000
wr⅞ΦQ -5OF
s} SSo-IdCIosφz
O
2 O
200
150
100
50
0
0	100	200 300
r0τ×} <
r0τ×} X
O IOO 200	300
days
Death Cost：
《巴 SSo-IdCIo-5of
4000
3000
2000
1000
0
0	100	200	300
0	100	200	300
43612, Aggregated Cost: 40.18
IOO 200	300
IOO 200	300
days
---Jo
2 10
%。UPφle62664
二∙ ∙O
2 10
φ⅞<" EO-S-EWESF
300
300
Figure 23: NSGA-II, the closest point to [45000 deaths, 20B] in the Pareto front. NSGA-II seem to
find a robust strategy that consists in a single lock-down of a few weeks to break the first wave. For
one run, this figure shows the evolution of model states (above) and states relevant for optimization
(below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].
32