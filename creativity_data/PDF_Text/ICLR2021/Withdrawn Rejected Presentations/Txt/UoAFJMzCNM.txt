Under review as a conference paper at ICLR 2021
Multi-agent Deep FBSDE Representation For
Large Scale Stochastic Differential Games
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we present a deep learning framework for solving large-scale multi-
agent non-cooperative stochastic games using fictitious play. The Hamilton-
Jacobi-Bellman (HJB) PDE associated with each agent is reformulated into a set
of Forward-Backward Stochastic Differential Equations (FBSDEs) and solved via
forward sampling on a suitably defined neural network architecture. Decision
making in multi-agent systems suffers from curse of dimensionality and strategy
degeneration as the number of agents and time horizon increase. We propose
a novel Deep FBSDE controller framework which is shown to outperform the
current state-of-the-art deep fictitious play algorithm on a high dimensional inter-
bank lending/borrowing problem. More importantly, our approach mitigates the
curse of many agents and reduces computational and memory complexity, allow-
ing us to scale up to 1,000 agents in simulation, a scale which, to the best of our
knowledge, represents a new state of the art. Finally, we showcase the frame-
work’s applicability in robotics on a belief-space autonomous racing problem.
1	Introduction
Stochastic differential games represent a framework for investigating scenarios where multiple play-
ers make decisions while operating in a dynamic and stochastic environment. The theory of differ-
ential games dates back to the seminal work of Isaacs (1965) studying two-player zero-sum dynamic
games, with a first stochastic extension appearing in Kushner & Chamberlain (1969). A key step in
the study of games is obtaining the Nash equilibrium among players (Osborne & Rubinstein, 1994).
A Nash equilibrium represents the solution of non-cooperative game where two or more players
are involved. Each player cannot gain benefit by modifying his/her own strategy given opponents
equilibrium strategy. In the context of adversarial multi-objective games, the Nash equilibrium can
be represented as a system of coupled Hamilton-Jacobi-Bellman (HJB) equations when the system
satisfies the Markovian property. Analytic solutions exist only for few special cases. Therefore, ob-
taining the Nash equilibrium solution is usually done numerically, and this can become challenging
as the number of states/agents increases. Despite extensive theoretical work, the algorithmic part
has received less attention and mainly addresses special cases of differential games (e.g., Duncan
& Pasik-Duncan (2015)), or suffers from the curse of dimensionality (Kushner, 2002). Neverthe-
less, stochastic differential games have a variety of applications including in robotics and autonomy,
economics and management. Relevant examples include Mataramvura & 0ksendal (2008), which
formulate portfolio management as a stochastic differential game in order to obtain a market port-
folio that minimizes the convex risk measure of a terminal wealth index value, as well as Prasad
& Sethi (2004), who investigate optimal advertising spending in duopolistic settings via stochastic
differential games.
Reinforcement Learning (RL) aims in obtaining a policy which can generate optimal sequential de-
cisions while interacting with the environment. Commonly, the policy is trained by collecting histo-
ries of states, actions, and rewards, and updating the policy accordingly. Multi-agent Reinforcement
Learning (MARL) is an extension of RL where several agents compete in a common environment,
which is a more complex task due to the interaction between several agents and the environment,
as well as between the agents. One approach is to assume agents to be part of environment (Tan,
1993), but this may lead to unstable learning during policy updates (Matignon et al., 2012). On the
other hand, a centralized approach considers MARL through an augmented state and action system,
reducing its training to that of single agent RL problem. Because of the combinatorial complexity,
1
Under review as a conference paper at ICLR 2021
the centralized learning method cannot scale to more than 10 agents (Yang et al., 2019). Another
method is centralized training and decentralized execute (CTDE), however the challenge therein lies
on how to decompose value function in the execute phase for value-based MARL. Sunehag et al.
(2018) and Zhou et al. (2019) decompose the joint value function into a summation of individual
value functions. Rashid et al. (2018) keep the monotonic trends between centralized and decentral-
ized value functions by augmenting the summation non-linearly and designing a mixing network
(QMIX). Further modifications on QMIX include Son et al. (2019); Mahajan et al. (2019).
The mathematical formulation of a differential game leads to a nonlinear PDE. This motivates algo-
rithmic development for differential games that combine elements of PDE theory with deep learn-
ing. Recent encouraging results (Han et al., 2018; Raissi, 2018) in solving nonlinear PDEs within
the deep learning community illustrate the scalability and numerical efficiency of neural networks.
The transition from a PDE formulation to a trainable neural network is done via the concept of a sys-
tem of Forward-Backward Stochastic Differential Equations (FBSDEs). Specifically, certain PDE
solutions are linked to solutions of FBSDEs, and the latter can be solved using a suitably defined
neural network architecture. This is known in the literature as the deep FBSDE approach. Han et al.
(2018); Pereira et al. (2019); Wang et al. (2019b) utilize various deep neural network architectures
to solve such stochastic systems. However, these algorithms address single agent dynamical sys-
tems. Two-player zero-sum games using FBSDEs were initially developed in Exarchos et al. (2019)
and transferred to a deep learning setting in Wang et al. (2019a). Recently,Hu (2019) brought deep
learning into fictitious play to solve multi-agent non-zero-sum game, Han & Hu (2019) introduced
the deep FBSDEs to a multi-agent scenario and the concept of fictitious play, furthermore, Han et al.
(2020) gives the convergence proof.
In this work we propose an alternative deep FBSDE approach to multi-agent non-cooperative dif-
ferential games, aiming on reducing complexity and increasing the number of agents the framework
can handle. The main contribution of our work is threefold:
1.	We introduce an efficient Deep FBSDE framework for solving stochastic multi-agent
games via fictitious play that outperforms the current state of the art in Relative Square
Error (RSE) and runtime/memory efficiency on an inter-bank lending/borrowing example.
2.	We demonstrate that our approach scales to a much larger number of agents (up to 1,000
agents, compared to 50 in existing work). To the best of our knowledge, this represents a
new state of the art.
3.	We showcase the applicability of our framework to robotics on a belief-space autonomous
racing problem which has larger individual control and state space. The experiments
demonstrates that the decoupled BSDE provides the possibility of applications for com-
petitive scenario.
The rest of the paper is organized as follows: in Section 2 we present the mathematical preliminaries.
In Section 3 we introduce the Deep Fictitious Play Belief FBSDE, with simulation results following
in Section 4. We conclude the paper and discuss some future directions in Section 5.
2	Multi-agent Fictitious Play FBSDE
Fictitious play is a learning rule first introduced in Brown (1951) where each player presumes other
players’ strategies to be fixed. An N -player game can then be decoupled into N individual decision-
making problems which can be solved iteratively over M stages. When each agent1 converges to a
stationary strategy at stage m, this strategy will become the stationary strategy for other players at
stage m + 1. We consider a N -player non-cooperative stochastic differential game with dynamics
dX (t) = (f (X (t), t) + G(X (t), t)U(t))dt + Σ(X (t), t)dW (t),	X (0) = Xo,	(1)
where X = (x1 , x2, . . . , xN ) is a vector containing the state process of all agents generated by
their controls U = (u1, u2, . . . , uN) with xi ∈ Rnx and ui ∈ Rnu. Here, f : Rnx × [0, T] → Rnx
represents the drift dynamics, G : Rnx × [0, T] → Rnx ×nu represents the actuator dynamics, and
Σ : [0, T] × Rn → Rnx ×nw represents the diffusion term. We assume that each agent is only driven
by its own controls so G is a block diagonal matrix with Gi corresponding to the actuation of agent i.
1Agent and player are used interchangeably in this paper
2
Under review as a conference paper at ICLR 2021
Each agent is also driven by its own nw -dimensional independent Brownian motion Wi , and denote
W = (W1,W2,...,WN).
Let Ui be the set of admissible strategies for agent i ∈ I := {1,2,..., N} and U =③N=IUi as the
Kronecker product space of Ui . Given the other agents’ strategies, the stochastic optimal control
problem for agent i under the fictitious play assumption is defined as minimizing the expectation of
the cumulative cost functional Jti
Jti(X, ui,m; u-i,m-1) = E
g(X(T)) + Z T
Ci(X(τ), ui,m(X(τ), τ), τ; u-i,m-1)dτ
(2)
where g : Rnx → R+ is the terminal cost, and Ci : [0, T] × Rnx × U → R+ is the run-
ning cost for the i-th player. In this paper we assume that the running cost is of the form
C(X, ui,m,t) = q(X) + 1 UrTmRuim + XTQui,m. We use the double subscript ui,m to de-
note the control of agent i at stage m and the negative subscript -i as the strategies excluding player
i, u-i = (u1, . . . , ui-1, ui+1, . . . , uN ). We can define value function of each player as
Vi(t,X(t))= inf Jti(X,ui,m;u-i,m-1) , Vi(T, X(T)) = g(X(T)).	(3)
ui,m ∈Ui
Assume that the value function in eq. (3) is once differentiable w.r.t. t and twice differentiable w.r.t.
x. Then, standard stochastic optimal control theory leads to the HJB PDE
V i + h + KT (f + GUo,-i) + ∣tr(C∑∑T) = 0,	V i(T, X )= g(X (T)),
(4)
where h = Ci + GU*,o. The double subscript of U*,o denotes the augmentation of the optimal
control u*m = -RT(GTVi + QrTx) and zero control u-i,m-ι = 0, and Uo,- denotes the
augmentation of ui,m = 0 and u-i,m-1. Here we drop the functional dependencies in the HJB
equation for simplicity. The detailed proof is in Appendix A. The value function in the HJB PDE
can be related to a set of FBSDEs
dX = (f + GU*,τ)dt + ΣdW,	X (0) = xo
dVi = -(h + ViTGU*,o )dt + VT ∑dW, V (T) = g(X (T)),
(5)
where the backward process corresponds to the value function. The detailed derivation can be found
in Appendix B. Note that the FBSDEs here differ from that of Han & Hu (2019) in the optimal
Control of agent i, GU*,-i, in the forward process and compensation, VxiTGU*,0, in the backward
process. This is known as the importance sampling for FBSDEs and allows for the FBSDEs to be
guided to explore the state space more efficiently.
3 Deep Fictitious Play FBSDE Controller
In this section, we introduce a novel and scalable Deep Fictitious Play FBSDE (SDFP) Controller
to solve the multi-agent stochastic optimal control problem. The framework can be extended to the
partially observable scenario by combining with an Extended Kalman Filter, whose belief propaga-
tion can be described by an SDE for the mean and variance (see derivation in Appendix C). By the
natural of decoupled BSDE, the framework can also been extended to cooperative and competitive
scenario. In this paper, we demonstrate the example of competitive scenario.
3.1	Network Architecture and Algorithm
Inspired by the success of LSTM-based deep FBSDE controllers (Wang et al., 2019b; Pereira et al.,
2019), we propose an approach based on an LSTM architecture similar to Pereira et al. (2019). The
benefits of introducing LSTM are two-fold: 1) LSTM can capture the features of sequential data.
A performance comparison between LSTM and fully connected (FC) layers in the deep FBSDE
framework has been elaborated in Wang et al. (2019b); 2) LSTM significantly reduces the memory
complexity of our model since the memory complexity of LSTM with respect to time is O(1) in the
inference phase compared with O(T) in previous work (Han et al., 2018), where T is the number
of time steps. The overall architecture of SDFP is shown in Fig. 1 and features the same time
discretization scheme as Pereira et al. (2019). Each player’s policy is characterized by its own copy
3
Under review as a conference paper at ICLR 2021
Figure 1: SDFP framework for N Players. Each NN block has the architecture in Fig. 11.
of the network defined in Fig. 11. At stage m, each player can access the stationary strategy of all
other players from stage m - 1. During training within a stage, the initial value of each player is
predicted by a FC layer parameterized by φ. At each timestep, the optimal policy for each player is
computed using the value function gradient prediction Vxi from the recurrent network (consisting of
FC and LSTM layers), parameterized by θ. The FSDE and BSDE are then forward-propagated using
the Euler integration scheme. At terminal time T, the loss function for each player is constructed as
the mean squared error between the propagated terminal value VTT and the true terminal value V^*
computed from the terminal state. The parameters φ and θ of each player can be trained using any
stochastic gradient descent type optimizer such as Adam. The detailed training procedure is shown
in Algorithm 2.
3.2	Mitigating Curse of Dimensionality and Sample Complexity
Scalability and sample efficiency are two crucial criteria of reinforcement learning. In SDFP, as the
number of agents increases, the number of neural network copies would increase correspondingly.
Meanwhile, the size of each neural network should be enlarged to gain enough capacity to cap-
ture the representation of many agents, leading to the infamous curse of dimensionality; this limits
the scalability of prior works. However, one can mitigate the curse of dimensionality in this case
by taking advantage of the symmetric game setup. We summarize merits of symmetric game as
following:
1.	Since all agents have the same dynamics and cost function, only one copy of the network
is needed. The strategy of other agents can be inferred by applying the same network.
2.	Thanks to the symmetric property, we can applied invariant layer to extract invariant fea-
tures to accelerate training and improve the performance with respect to the accumulate
cost and RSE loss.
Sharing one network: It’s important to note that querying other agents should not introduce ad-
ditional gradient paths. This significantly reduces the memory complexity. When querying other
agents’ strategy, one can either iterate through each agent or feed all agents’ states to the network in
a batch. The latter approach reduces the time complexity by adopting the parallel nature of modern
GPU but requires O(N2) memory rather than O(N) for the first approach.
Invariant Layers: The memory complexity
can be further reduced with an invariant layer
embedding (Zaheer et al., 2017). The invari-
ant layer utilizes a sum function along with the
features in the same set to render the network
invariant to permutation of agents. We apply
the invariant layer on X-i and concatenate the
resulting features to the features extracted from
Xi . However, vanilla invariant layer embed-
ding will not reduce the memory complexity.
Thanks to the symmetric problem setup, one
can apply a trick to reduce the invariant layer
memory complexity form O(N2) to O(N). A
Figure 2: Sample efficiency between FBSDE
framework w/ and w/o invariant layer.
detailed introduction to the invariant layer and our implementation can be found in Appendix D and
E. The full algorithm is outlined in Algorithm 1.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Scalable Deep Fictitious Play FBSDE for symmetric simplification
1:	Hyper-Parameters:N: Number of players, T: Number of timesteps, M: Number of stages in
fictitious play, Ngd: Number of gradient descent steps per stage, U0 : the initial strategies for
players in set I, B : Batch size, : training threshold, ∆t: time discretization
2:	Parameters:V (x0； φ): Network weights for initial value prediction, θ: Weights and bias of
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
fully connected layers and LSTM layers.
θ: Initialize trainable papermeters:θ0, φ0
while LOSS is above certain threshold do
for m — 1 to M do
for all i ∈ I in parallel do
Collect opponent agent's policy which is same as ith policy: fLSTMa(')，fFmC-1(∙)
for l — 1 to Ngd do	"	"
for t — 1 to T 一 1 do
if Using Invariant Layer then
Xt = finvariant (Xt )
end if
for j — 1 to B in parallel do
COmPUtenetWorkPredictionfor ith player: Vχji,t = fFmCi(fmsτMi(Xj； θiT))
Compute ith optimal Control :uj,? = -R-1(GTvχji,t + QTxj)
Infer -ith players, network prediction and stop the gradient for them: VX-. t 二
fFci1(fmsτ1Mi(%； θi))	'
Compute -ith optimal Control and stop the gradient for them: uj,?it
-R--i1(GT-ivxj-i,t+QT-ixj-i)
Sample noise ∆Wj 〜N(0, ∆t)
Propagate FSDE: Xj+ι = fFSDE(Xj, uj,?, U*, ∆Wj,t)
Propagate BSDE: Vjt+1 = fBSDE (Xj, *；, ∆Wj,t)
end for
end for
Compute loss: L =击 Pj=I(VT,? - VjT)2
Gradient Update: θl , φl
end for
end for
end for
28: end while
Figure 3: Comparison of SDFP and analytic solution for the inter-bank problem. Both the state (left)
and control (right) trajectories are aligned with the analytic solution (represented by dots).
4 Simulation Results
In this section, we demonstrate the capability of SFDP on two different systems in simulation. We
first apply the framework to an inter-bank lending/borrowing problem, which is a classical multi-
player non-cooperative game with an analytic solution. We compare against both the analytic solu-
tion and prior work (Han & Hu, 2019). Different approaches introduced in Section 3.2 are compared
empirically on this system. We also apply the framework to a variation of the problem for which
no analytic solution exists. Finally, we showcase the general applicability of our framework in an
autonomous racing problem in belief space. All experiment configurations can be found in Ap-
5
Under review as a conference paper at ICLR 2021
Figure 4: Time and memory complexity comparison between batch, iterate and invariant layer+batch
implementations. Time complexity is measured by per-iteration time.
Table 1: Comparison with previous work on the 10-agent inter-bank problem.
(S)UO一⅛-JB±!」ad əEF
Framework	# stages	Learning Rate	RSE	Total Time (hr) 1
Han & Hu (2019)	80	"Te-3	0.0423	2.23
SFDP	80	1e-3	0.0105	1.55
pendix J. we plot the results of 3 repeated runs with different seeds with the line and shaded region
showing the mean and mean±standard deviation respectively. The hyperparameters and dynamics
coefficients used in the inter-bank experiments are the same as Han & Hu (2019) unless otherwise
noted.
4.1 Inter-bank lending/b orrowing problem
We first consider an inter-bank lending and borrowing model (Carmona et al., 2013) where the
dynamics of the log-monetary reserves of N banks is described by the diffusion process
，-一 ，,二.	1 Λ ..
dXt =	[a(X	-	Xi)	+	ut]	dt + σ(ρdWt0	+ √1 -	ρ2dW；),	Xt	= NN EXi,	i ∈	I.	(6)
N i=1
The state Xti ∈ R denotes the log-monetary reserve of bank i at time t > 0. The control uti denotes
the cash flow to/from a central bank, where as a(X - Xi) denotes the lending/borrowing rate of
bank i from all other banks. The system is driven by N independent standard Brownian motion Wti ,
which denotes the idiosyncratic noise, and a common noise Wt0 . The cost function has the form,
Ci,t(X, ui; u-i) = 2U - qUi(X - Xi) +I(X - Xi)2.	⑺
The derivation of the FBSDEs and analytic solution can be found in Appendix F. We compare the
result of implementation corresponding to Algorithm 2 on a 10-agent problem with analytic solu-
tion and previous work from Han & Hu (2019) with the same hyperparameters. Fig. 3 shows the
performance of our method compared with analytic solution. The state and control trajectories out-
putted by the deep FBSDE solution are aligned closely with the analytic solution. Table 1 shows the
numerical performance compared with prior work by Han & Hu (2019). Our method outperforms by
Relative Square Error (RSE) metrics and computation wall time. The RSE is defined as following:
…JP 1≤/≤B(Vi(0, Xj(0)) - Vi(0, Xj(0)))2
-RS E —	ʌ	~	,
P 1≤∈≤B(Vi(0, Xj(0)) - Vi(0, Xj(0)))2
(8)
Where Vi is the analytic solution of value function for ith agents at intial state Xj(0). The initial
state Xj(0) is new batch of data sampled from same distribution as X(0) in the training phase. The
batch size B is 256 for all inter-bank simulations. Vi is the approximated value function for ith
agent by FBSDE controller, and Vi is the average of analytic solution for ith agent over the entire
batch.
Time/Memory Complexity Analysis: We empirically verify the time and memory complexity of
different implementation approaches introduced in 3.2, which is shown in Fig. 4. Note that all
1The experiment is conducted on Nvidia TITAN RTX
6
Under review as a conference paper at ICLR 2021
number of agents	number of agents
Figure 5: RSE and total loss comparison between our FBSDE framework and that of baseline Han
& Hu (2019)
experiments hereon correspond to the symmetric SDFP implementation in Algorithm 1 We also
test sample efficiency and generalization capability of the invariant layer on a 50-agent problem
trained over 100 stages. The number of initial states is limited during the training and the evaluation
criterion is the terminal cost of the test set in which the initial states are different from the initial
states during training. Fig. 2 showcases the improvement in sample efficiency and generalization
performance of invariant layer. We suspect this is due to the network needing to learn with respect
to a specific permutation of the input, whereas permutation invariance is built into the network
architecture with invariant layer.
Importance sampling: An important distinction of SDFP from the baseline in Han & Hu (2019)
is the importance sampling scheme, which helps the LSTM architecture achieve a fast convergence
rate during training. However, the baseline, which uses fully connected layer as backbone, is not
suitable for importance sampling, as it would lead to an extremely deep network with fully connected
layers from gradient topology perspective. Sandler et al. (2018) mentioned that the information loss
is inevitable for this kind of fully connected deep network with nonlinear activation. On the other
hand, LSTM does not suffer from this problem because of the existence of long and short memory.
We illustrate the benefits of importance sampling for LSTM backbone and gradient flow of fully
connected layer backbone in Appendix I.
High dimension experiment: We also analyze the performance of our framework and that of Han
& Hu (2019) both with and without invariant layer on high dimensional problems. We first demon-
strate the mitigation of the invariant layer on the curse of many agents. Fig. 5 demonstrates the
ablation experiment of the two deep FBSDE frameworks (SDFP and Han & Hu (2019)). In order
to illustrate that invariant layer can mitigate the curse of dimensionality, we also integrate invariant
layers on Han & Hu (2019) and shows the performance in the same figure 5. In this experiment, the
weights of FBSDE frameworks with invariant layer are adjusted in order to dismiss the performance
improvement resulting from increased weights from the invariant layers. The total cost and RSE
are computed by averaging the corresponding values over the last twenty stages of each run. It can
be observed from the plot that without invariant layer, the framework suffers from curse of many
agents in the prediction of initial value as the RSE increases with respect to the number of agents.
On the other hand, RSE increases at a slower rate with invariant layer. In terms of total cost, which
is computed from the cost function defined in eq.2, our framework enjoys the benefits of importance
sampling and invariant layer, and achieves better numerical results over the number of agents. We
further analyse the influence of invariant layers in the training phase by demonstrating fig.7. Invari-
ant layer helps mintage over-fitting phenomenon in the training and evaluation phase, meanwhile
accelerating the training process, even though both of frameworks adopting same feature extracting
backbone (LSTM) and importance sampling technique.
We also show that the invariant layer accelerates training empirically on a 500-agent problem. Fig. 6
shows that the FBSDE frameworks converge much faster with invariant layer than without it. We
suspect that the acceleration effect results from increased sample efficiency. Note that the compar-
ison is done on a 500-agent problem because the framework does not scale to 1000 agents without
the invariant layer. A comparison of the two frameworks with invariant layer only on a 1000-agent
problem can be found in Fig 16, which shows similar results to the 500-agent problem.
7
Under review as a conference paper at ICLR 2021
2 XlO1
1
1
ss。，百。J.
6×10β
O 20	40	60
Number of Stages
1
1
山Sy
—FC (Hu&Han Baseline)
FC+lnvariant layer (Ours)
—— L5TM +Invariant Layer (Ours)
0	20	40	60	80
Number of Stages
Figure 6: RSE and total loss trajectory comparison between our FBSDE framework and that of Han
& Hu (2019) w/ and w/o invariant layer for 500 agents.
Figure 7: RSE and training loss trajectory comparison between our FBSDE framework and the
extension of Pereira et al. (2019) w/ and w/o invariant layer for 500 agents.
Superlinear Simulation: We also consider a variant of dynamics in section 4.1,
Z_____	1 ɪ
dXt = [a(X	— Xit)3	+ 斓	dt +	σ(ρdW0	+ √1 - ρ2dW∕), Xt	=	IN	EXi,i ∈	I.	(9)
N i=1
Due to the nonlinearity in the drift term, analytic solution or simple numerical representation of the
Nash equilibrium does not exist (Han & Hu, 2019). The drift rate a is set to 1.0 to compensate for
the vanishing drift term caused by super-linearity. Heuristically, the the distribution of control and
state should be more concentrated than that of the linear dynamics. We compare the state and control
of a fixed agent i at terminal time against analytic solution and deep FBSDE solution of the linear
dynamics with the same coefficients. Fig. 8 is generated by evaluating the trained deep FBSDE
model with a batch size of 50000. It can be observed that the solution from super-linear dynamics is
more concentrated as expected. The terminal control distribution plot verifies that the super-linear
drift term pushes the state back to the average faster than linear dynamics and thus requires less
control effort. Since the numerical solution is not available in the superlinear case, we compare the
total loss and training loss between baseline Han & Hu (2019) and our algorithm in the appendix 12.
Analytic
FBSDE Uear Dynamics
FBSDE Superllear Dynamics
Λ4ωuφα
3
-
2 Q.8.6/ 2 Q
Iiooooo
A4ωuφɑ
2
Os
2
3
Figure 8: Terminal time step state X and control U distribution of ith agent for linear and superlin-
ear dynamics.
8
Under review as a conference paper at ICLR 2021
4.2 Belief Space Autonomous Racing
In this section, we demonstrate the general applicability of our framework on an autonomous racing
example in belief space. We consider a 2-car autonomous racing competition problem with racecar
dynamics
X= [v cos θ,v sin θ, Uacc - Cdragv, Usteerv/L]T
(10)
where x = [x, y, v, θ]T represent the x, y position, forward velocity and heading respectively. Here
we assume x, y, v, Uacc ∈ R, Usteer ∈ [-1, 1]. The goal of each player is to drive faster than the oppo-
nent, stay on the track and avoid collision. An additional competition loss can be added to facilitate
competition between players. During the competition, players have access to the global augmented
states and opponent’s history controller. Additionally, we assume that stochasticity enters the sys-
tem through the control channels and have a continuous-time noisy observation model with full-state
observation. The FBSDE derivation of belief space stochastic dynamics is included in Appendix G.
The framework for the racing problem is trained with
batch size of 64, and 100 time steps over a time horizon
of 10 seconds. Since all the trials will run over 1 lapse of
the circle, here we only show the first 8 second result for
neatness. Fig. 13 demonstrate the capability of our frame-
work. When there is no competition loss, both of cars can
stay in the track. Since there is no competition between
two cars, they demonstrate similar behaviors. When we
add competition loss on both cars, both of them try to
cut the corner in order to occupy the leading position as
shown in the second plot in Fig. 13. If competition loss
is present in only one of the two cars, then the one with
competition loss will dominate the game as shown in the
botton subplots of Figure 13. Notably the simulation is
running in belief space where all states are estimated with
Figure 9: One belief space racing trajec-
tory. The solid line represents the mean
and the circles represent the variance.
observation noise and additive noise in the system. The results emphasizes the generalization ability
of our framework on more complex systems with higher state and control dimensions. Fig. 9 shows
a single trajectory of each car’s posterior distribution.
5 Conclusion
In this paper, we propose a scalable deep learning framework for solving multi-agent stochastic
differential game using fictitious play. The framework relies on the FBSDE formulation with impor-
tance sampling for sufficient exploration. In the symmetric game setup, an invariant layer is incor-
prated to render the framework agnostic to permutatoon of agents and further reduce the memory
complexity. The scalability of this algorithm, along with a detailed sensitivity analysis, is demon-
strated in an inter-bank borrowing/lending example. The framework achieves lower loss and scales
to much higher dimensions than the state of the art. The general applicability of the framework is
showcased on a belief space autonomous racing problem in simulation.
9
Under review as a conference paper at ICLR 2021
References
George W Brown. Iterative solution of games by fictitious play. Activity analysis of production and
allocation ,13(1):374-376,1951.
Rene Carmona, Jean-Pierre Fouque, and Li-Hsien Sun. Mean field games and systemic risk. Avail-
able at SSRN 2307814, 2013.
T. Duncan and B. Pasik-Duncan. Some stochastic differential games with state dependent noise.
54th IEEE Conference on Decision and Control, Osaka, Japan, December 15-18, 2015.
Ioannis Exarchos, Evangelos Theodorou, and Panagiotis Tsiotras. Stochastic differential games: A
sampling approach via FBSDEs. Dynamic Games and Applications, 9(2):486-505, 2019.
Jiequn Han and Ruimeng Hu. Deep fictitious play for finding markovian nash equilibrium in multi-
agent games. arXiv preprint arXiv:1912.01809, 2019.
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510,
2018.
Jiequn Han, Ruimeng Hu, and Jihao Long. Convergence of deep fictitious play for stochastic differ-
ential games. arXiv preprint arXiv:2008.05519, 2020.
Ruimeng Hu. Deep fictitious play for stochastic differential games. arXiv preprint
arXiv:1903.09376, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
R. Isaacs. Differential Games: A Mathematical Theory with Applications to Warfare and Pursuit,
Control and Optimization. New York: Willey, 1965.
AH Jazwinski. Stochastic process and filtering theory, academic press. A subsidiary of Harcourt
Brace Jovanovich Publishers, 1970.
H. Kushner. Numerical approximations for stochastic differential games. SIAM J. Control Optim.,
41:457-486, 2002.
H. Kushner and S. Chamberlain. On stochastic differential games: Sufficient conditions that a given
strategy be a saddle point, and numerical procedures for the solution of the game. Journal of
Mathematical Analysis and Applications, 26:560-575, 1969.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. In Advances in Neural Information Processing Systems, pp. 7613-7624,
2019.
Sure Mataramvura and Bemt 0ksendal. Risk minimizing portfolios and hjbi equations for stochastic
differential games. Stochastics An International Journal of Probability and Stochastic Processes,
80(4):317-337, 2008.
Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learn-
ers in cooperative markov games: a survey regarding coordination problems. 2012.
Martin J Osborne and Ariel Rubinstein. A course in game theory cambridge. MA: MIT Press
[Google Scholar], 1994.
Marcus A Pereira, Ziyi Wang, Ioannis Exarchos, and Evangelos A Theodorou. Learning deep
stochastic optimal control policies using forward-backward sdes. In Robotics: science and sys-
tems, 2019.
Ashutosh Prasad and Suresh P Sethi. Competitive advertising under uncertainty: A stochastic differ-
ential game approach. Journal of Optimization Theory and Applications, 123(1):163-185, 2004.
10
Under review as a conference paper at ICLR 2021
Maziar Raissi. Forward-backward stochastic neural networks: Deep learning of high-dimensional
partial differential equations. arXiv preprint arXiv:1804.07010, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018.
Dan Simon. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley
& Sons, 2006.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learn-
ing to factorize with transformation for cooperative multi-agent reinforcement learning. arXiv
preprint arXiv:1905.05408, 2019.
Peter Sunehag, GUy Lever, AUdrUnas Gruslys, Wojciech Marian Czarnecki, VinIciUs FlOres Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative mUlti-agent learning based on team reward. In AAMAS,
pp. 2085-2087, 2018.
Ming Tan. MUlti-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330-337, 1993.
Ziyi Wang, KeUntaek Lee, MarcUs A Pereira, Ioannis Exarchos, and Evangelos A TheodoroU. Deep
forward-backward SDEs for min-max control. In 2019 IEEE 58th Conference on Decision and
Control (CDC), pp. 6807-6814. IEEE, 2019a.
Ziyi Wang, MarcUs A Pereira, and Evangelos A TheodoroU. Deep 2fbsdes for systems with control
mUltiplicative noise. arXiv preprint arXiv:1906.04762, 2019b.
Yaodong Yang, RasUl TUtUnov, PhU SakUlwongtana, Haitham BoU Ammar, and JUn Wang. αα-rank:
Scalable mUlti-agent evalUation throUgh evolUtion. arXiv preprint arXiv:1909.11628, 2019.
Manzil Zaheer, Satwik KottUr, Siamak Ravanbakhsh, Barnabas Poczos, RUss R SalakhUtdinov, and
Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp. 3391-
3401, 2017.
Ming ZhoU, Yong Chen, Ying Wen, Yaodong Yang, YUfeng SU, Weinan Zhang, Dell Zhang, and
JUn Wang. Factorized q-learning for large-scale mUlti-agent systems. In Proceedings of the First
International Conference on Distributed Artificial Intelligence, pp. 1-7, 2019.
11
Under review as a conference paper at ICLR 2021
Appendix
A Multi-agent HJB Derivation
Applying Bellman’s principle to the value function equation 3 as following
V i(t, X(t)) = inf E
ui ∈Ui
Vi(t+dt,X(t+dt)) +
t+dt
t
Cidτ
= inf E Cidt+Vi(t,X(t)) +Vti(t,X(t))dt
ui ∈Ui	t
+ vχiτ(t, X (t))dX + 2tr(VXx(t, X (t)∑∑T)dt
= inf E Cidt + V i(t, X(t)) +Vti(t,X(t))dt
ui ∈Ui
+ VXT(t, X(t))((f + Gsdt + ∑dW) + Itr(Vix(t, X(t))∑∑T)dt
= inf Cidt + V i(t, X(t)) +Vti(t,X(t))dt
ui ∈Ui	t
+ VXiT(t, X (t))((f + Gsdt) + Itr(Vxix(t, X (t))∑∑τ)dt
⇒ 0 = Vi(t, X(t))+ inf [Ci + VxiT(t, X(t))(f + GU)] + 1tr(Vxix(t, X(t))∑∑T)
ui ∈Ui	2
(11)
Given the cost function assumption, the infimum can be obtained explicitly using optimal control
u*,m = -RT(GTVxi + QTx). With that We can obtain the final form of the HJB PDE as
Vi + h + VxiT (f + GUo,-i) + 1tr(Vxix∑∑T) = 0,	V i(Τ, X )= g(X (T)).	(12)
B FBSDE Derivation
Given the HJB PDE in equation 4, one can apply the nonlinear Feynman-Kac lemma Han & Hu
(2019) to obtain a set of FBSDE as
dX(t) = (f + GU0,-i)dt + ΣdW,	X(0) = x0
dV i = -hdt + VxiTΣdW, V (X(T)) = g(X(T)).
(13)
Note that the forWard process X is driven by the control of all agents other than i. This means that
agent i searches the state space With BroWnian motion only to respond to other agents’ strategies. To
increase the efficiency of the search, one can add any control from agent i to guide its exploration,
as long as the backWard process is compensated for accordingly. In this Work, since We consider
problems With a closed form solution of the optimal control ui,m, We add it to the forWard process
for importance sampling from a neW set of FBSDEs.
dX = (f + GU*,-i)dt + ∑dW, X (0) = xo
dVi = -(h + VxiTGU*,o )dt + VxT∑dW,	V (T) = g(X (T)).
(14)
C Continous Time Extended Kalman Filter
The Partial Observable Markov Decision Process is generally difficult to solve Within infinite di-
mensional space belief. Commonly, the Value function does not have explicit parameterized form.
Kalman filter overcome this challenge by presuming the noise distribution is Gaussian distribution.
In order to deploy proposed ForWard BackWard Stochastic Differential Equation (FBSDE) model
in the Belief space, We need to utilize extended Kalman filter in continuous time JazWinski (1970)
correspondingly. Given the partial observable stochastic system:
dx
--=f (χ,u,w,t),	and Z = h(x,v,t)	(15)
12
Under review as a conference paper at ICLR 2021
Where f is the stochastic state process featured by a Gaussian noise W 〜N(0, Q), h is the obser-
vation function while V 〜N(0, R) is the observation noise. Next, We consider the linearization of
the stochastic dynamics in equation 20 represented as follows:
A
∂f
∂x
,L
^
∂f
∂w
,C
X
dh ,M = dh ,Q = lqLt,R = MRMT
dχ ^	dv X
(16)
one can write the posterior mean state X and prior covariance matrix P- estimation update rule by
Simon (2006):
X(0) = E[x(0)], P-(0) = E[(x(0) - X)(x(0) - X)τ]
K = pc TRT
(17)
X = f(X,u,wo,t) + K [z — h(X, vo,t)]
P- = AP- + P -Aτ + Q — P- C TRTCP-
We follow the notation in (Simon, 2006), where X is the real state, X is the mean of state estimated
by Kalman filter based on the noisy sensor observation, P- represents for the covariance matrix of
the estimated state, nominal noise values are given as w0 = 0 and v0 = 0, where superscript + is the
posterior estimation and - is the prior estimation. Then we can define a Gaussian belief dynamics
as b(Xk, P-) by the mean state X and variance P- of normal distribution N(Xk, P-)
The belief dynamics results in a decoupled FBSDE system as follows:
dbk = g(bk ,uk, 0)dt + Σ(bk,u, 0)dW, dW 〜N (0,I)
?T
dV = -Ci dt+ VXi ΣdW
(18)
where:
g(bk, uk)
Σ(bk, uk)
b(t, X(t), ui,m (t); u-i,m)
Vec(Ak P- + P-AT + Q k - PI-CTR = Ck Pr)
,KkCk P-dt
0
V (T) = g(X(T))
ʌ ,. - ,,,
X^(0) = E[X (0)]
P-(0) = E[(X(0) - X)(X(0) - X)t]
(19)
D Deep Sets
A function f maps its domain from X to Y . Domain X is a vector space Rd and Y is a continuous
space R. Assume the function take a set as input:X = {X1...XN}, then the function f is indifferent
ifit satisfies property (Zaheer et al., 2017).
Property 1. A function f : X → Y defined on sets is permutation invariant to the order of objects
in the set. i.e. For any permutation function n:f ({x>..xn}) = f ({x∏(i)…Xn(N)})
In this paper, we discuss when f is a nerual network strictly.
Theorem 1 X has elements from countable universe. A function f(X) is a valid permutation
invariant function, i.e invariant to the permutation of X, iff it can be decomposed in the from
ρ( X∈X φ(X)), for appropriate function ρ and φ.
In the symmetric multi-agent system, each agents is not distinguishable. This property gives some
hints about how to extract the features of -ith agents by using neural network. The states of -ith
agents can be represented as a set:X = {X1, X2, ..., Xi-1, Xi+1, ..., XN}. We want to design a
neural network f which has the property of permutation invariant. Specifically, φ is represented as
a one layer neural network and ρ is a common nonlinear activation function.
13
Under review as a conference paper at ICLR 2021
Figure 10: Invariant layer architecture.
E Invariant Layer architecture
The architecture of invariant layer is described in Fig. 10. The input of the layer is the states at
time step t. The Invariant Model module in Fig. 10 is described in Appendix D, where φ is a neural
network and ρ is nonlinear activation function. The specific configuration of neural network in
Invariant model can be found in J.
Noticing that all the agents has the access to the global states, we define the state input features of
neural network for ith agent as:
Xt,i = {xi,x1,x2...,xi-1,xi+1,...xN} ,	(20)
with shape of [BS, N]. In the other word, we always put own feature at first place. For each agent i,
there exists such feature tensor, then the shape of input tensor will become [BS, N, N] for invariant
layer. In invariant layer, we first separate the input feature Xt into two parts: Xt,i and Xt,-i. Then
the features of -ith agents Xt,-i will be sent to the invariant model. The shape of Xt,-i will be
[B, N, N - 1] where N is the number of agents. First,we could use neural network to map the
feature into Nf dimension space, where Nf is the feature dimensions. Then the shape of the tensor
will become [BS, N, N - 1, Nf], After summing up the features of all the element in the set, the
dimension of the tensor would reduce to [BS, N, 1, Nf], and we denote this feature tensor as F1.
However, the memory complexity is O(N2 × Nf) which is not tolerable when the number of agent
N increases. Alternatively, we can simply mapping the feature tensor [BS, N] into desired feature
dimension Nf, then the tensor would become [BS, N, Nf], and we denote it to be F2. Now we
create another tensor which is the average of features of element in set with size [BS, 1, Nf] and we
denote it to be F12. Then we denote F0 = (F2 X N - F2)∕(N -1) which has size of [BS, N, Nf ]. We
can find that F20 = F1, and the memory complexity of computing F20 is just O(N). The derivation
is true if the system is symmetric and the agents are not distinguishable. The trick can be extended
to high state dimension for individual agent.
14
Under review as a conference paper at ICLR 2021
SSol 6u⊂-eJJ.
O0
1
Figure 11: FBSDE Network for a Single Agent. Note that the same FC is shared across all timesteps.
20	40	60	80 IOO
Number of Stages
Han&Hu Baseline
LSTW +Inva ria nt Layer (Ours)	2 × 1≡1
Sio1
6×10β
4×UP
Number of Stages
Figure 12: Superlinear dynamics comparison between baseline and our algorithm
F Interbank
By pluging the running cost to the HJB function, one can have,
Vi,t + inf	Xla(X -	Xj ) +	u2]Vxj	+ uui	- qui (X -	Xi)	+	景X	- Xi)I2	+
ui∈Ui	«	2	2	∕C1∖
Lj=I	_|	(21)
2tr(Vxx,i^^T) = 0∙
By computing the infimum explicitly, the optimal control of player i is:ui (X, t) = q(X — Xi)-
Vχ,i(X, t). The final form of HJB can be obtained as
Vi,t + 2tr(Vxx,i ςςT) + a(X - Xi)Vx,i + £[a(X - Xj ) + uj ]Vx,j
j=i	(22)
+ 2(X - Xii))- 2(q(X - Xi)- Vx,i)2 = 0
Applying Feynman-Kac lemma to equation 22, the corresponding FBSDE system is
dX(t) = (f (X(t), t) + G(X(t), t)u(t))dt + Σ(t, X(t))dWt,	X(0) = xo
dVi = -[∣(X - Xi)2 - 1(q(X - Xi) - Vx,i)2 + Ui]dt + VT∑dW,	V(T) = g(X(T)).
(23)
15
Under review as a conference paper at ICLR 2021
-1.0
-1.0 -0.5	0.0	0.5	1.0
-1.0 -0.5	0.0	0.5	1.0
Figure 13: 2 car racing problem with 8 second time horizon.
G B elief Car Racing
The full stochastic model can be written as
dx = (f (x) + G(x)u)dt + Σ(x)dw,	z = h(x) + m
	-V Cos θ 一			0	0		
f(x) =	v sin θ -cdragV 0	,G(X)二	Σ(x) =	0 1 0	0 0 V/L	,h(x)二	x
(24)
Where dw is standard brownian motion. We consider the problem of two cars racing on circle track.
The cost function of each car is designed as
丁—	(\x2 工 y2
Jt = eχp(\形 + 庐
'-----------
{z"^^^^^
track cost
—1 \) + ReLU( — V) + exp ( — d)
} '-------{------} '------{-----}
velocity cost collision cost
Where d is Euclidean distance between two cars. In this showcase, we use continuous time extended
Kalman Filter to propagate belief space dynamics described in equation 19. The detailed algorithm
for Belief space deep fictitious play FBSDE can be found in Appendix.
We introduce the concept of game by using an additional competition cost:
T
Jcompetition = exp(-
cos(θ)
sin(θ)
x1 - x2
y1 - y2
)
Where xi , yi is the x, y position of ith car. When ith car is leading, the competition loss will be
minor, and it will increase exponentially when the car is trailing.
Thanks to decoupled BSDE structure, each car can measure this competition loss separately and
optimize the value function individually.
H Analytic Solution for Inter-Bank B orrowing/Lending
Problem
The analytic solution for linear inter-bank problem was derived in Carmona et al. (2013). We provide
them here for completeness. Assume the ansatz for HJB function is described as:
Vi(t, X)=竽(X - Xi)2 = μ(t)i ∈ I
(25)
16
Under review as a conference paper at ICLR 2021
Figure 14: The gradient path of FBSDE model w/ and w/o importance sampling. The figure on the
left is FBSDE with importance sampling and the figure on the right is FBSDE without importance
sampling. One can identify that the framework with importance sampling would lead to long chain
of gradient.
Figure 15: The training loss and RSE of LSTM backbone FBSDE architecture w/ and w/o impor-
tance sampling
Where η(t), μ(t) are two scalar functions. The optimal control under this ansatz is:
a?(t, X )= q + η(t)(1 - 9) (X - Xi)	(26)
By pluginging the ansatz into HJB function derived in 22, one can have,
η⑴=2(a + q)η(t) + (I -1)η2(t) - (e - q2),η(T) = c,
1	1N 2	(27)
μ(t) = - 2σ2(1-ρ2)(1- -)η(t),μ(T) = 0.
There exists the analytic solution for the Riccati equation described above as,
(t) _ -(e - q2)(e"+-δ-)(TT) - 1) - c(δ+e"+-δ-)(TT) - δ-)	(28)
η	(δ-e(δ+-δ-)(T-t) — δ+) — c(1 — 1/N 2)(e(δ+-δ-)(T-t)) — 1.
Where δ± = —(a + q) ± √R and R = (a + q)2 + (1 一 1/N2)(e 一 q2)
I Importance sampling
Fig. 14 demonstrates how fully connected layers with importance sampling would lead to a extreme
deep fully connected neural network. Fig. 15 demonstrates how importance sampling helps increase
convergence rate in FBSDE with LSTM backbone. The experiment is conducted with 50 agents and
50 stages. All the configuration is identical except the existence of importance sampling.
J	Experiment configurations
This Appendix elaborates the experiment configurations for section 4. For all the simulation in
section 4, the number of SGD iteration is fixed as NSGD = 100. We are using Adam as optimizer
17
Under review as a conference paper at ICLR 2021
1
1
ss。，百。J.
O 20	40	60	80 IOO
Number of Stages
---- FC+lnvarlant layer (Ours)
-L5ΓM +Invariant Layer (Ours)
SSCn 6ucro」J_
Figure 16: The Total loss and RSE of 1000 agents simulation
with 1E-3 learning rate for all simulations.
In section 4.1, For the prediction of initial value function, all of frameworks are using 2 layers
feed forward network with 128 hidden dimension. For the baseline framework, we followed the
suggested configuration motioned in Han et al. (2018). At each time steps, Vx,i is approximated
by three layers of feed forward network with 64 hidden dimensions. We add batch norm Ioffe &
Szegedy (2015) after each affine transformation and before each nonlinear activation function. For
Deep FBSDE with LSTM backbone, we are using two layer LSTM parametrized by 128 hidden
state. If the framework includes the invariant layer, the number of mapping features is chosen to be
256. The hyperparameters of the dynamics is listed as following:
a=0.1, q=0.1, c=0.5,	= 0.5, ρ=0.2, σ = 1,T = 1.	(29)
In the simulation, the time horizon is separated into 40 time-steps by Euler method. Learning rate
is chosen to be 1E-3 which is the default learning rate for Adam optimizer. The initial state for
each agents are sampled from the uniform distribution [δ0, δ1]. Where δ0 is the constant standard
deviation of state X(t) during the process. In the evaluation, we are using 256 new sampled
trajectory which are different from training trajectory to evaluate the performance in RSE error and
total cost error. The number of stage is set to be 100 which is enough for all framework to converge.
In section 4.2, the hyperparameter is listed as following:
cdrag = 0.01, L = 0.1, c= 0.5, T = 10.0	(30)
The observation noise is sampled from Gaussian noise m 〜N(0,0.01I). The time horizon is en-
rolled into 100 time-steps by Euler method. In this experiments, the initial value Vi is approximated
a single trainable scale and Vx,i(t) is approximated by two layers of LSTM parametrized with 32
hidden dimensions. The number of stage is set to be 10.
18
Under review as a conference paper at ICLR 2021
Algorithm 2 Scalable Deep Fictitious Play FBSDE
1: Hyper-Parameters:N: Number of players, T: Number of timesteps, M: Number of stages in
fictitious play, Ngd: Number of gradient descent steps per stage, U0 : the initial strategies for
players in set I, B : Batch size, : training threshold, ∆t: time discretization
2: Parameters:V (x0 ; φ): Network weights for initial value prediction, θ: Weights and bias of
fully connected layers and LSTM layers.
3: Initialize trainable papermeters:θ0, φ0
4: while LOSS is above certain threshold do
5:	for m _ 1 to M do
6:	for all i ∈ I in parallel do
7:	Collect opponent agent,s policy fm-1M-i (∙),fm-1 11i (∙)
8:	for l — 1 to Ngd do
9:	for t - 1 to T - 1 do
10:	for j — 1 to B in parallel do
11:	Compute network prediction for ith player: Vxii,j,t = fFmCi (fLmSTMi (Xtj; θil-1))
12:	Compute ith optimal Control:uij,,t? = -Ri-1 (GiTVxi ,j,t + QiTxij )
13:	Infer -ith players’ network prediction: Vxi ,j,t = fFmC-1 (fLmS-T1M (Xt; θ-i))
14:	Compute -ith optimal Control:uj-,?i,t = -R--i1 (GT-iVxj ,t + QT-ixj-i)
15:	Sample noise ∆Wj 〜N(0, ∆t)
16:	Propagate FSDE: Xj+1 = fFSDE(Xj, uj,?, u-；t，∆Wj,t)
17:	PropagateBSDE:吟^ = fBSDE(Vi,t, Xj,j ∆Wj,t)
18:	end for
19:	end for
20:	Compute loss: L = ~B PB=I(ViT - ViT)2
21:	Gradient Update: θl , φl
22:	end for
23:	end for
24:	end for
25: end while
19