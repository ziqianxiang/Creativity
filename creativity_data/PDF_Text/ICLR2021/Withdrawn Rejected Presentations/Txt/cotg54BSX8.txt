Under review as a conference paper at ICLR 2021
Grey-box Extraction of Natural Language
Models
Anonymous authors
Paper under double-blind review
Ab stract
Model extraction attacks attempt to replicate a target machine learning model from
predictions obtained by querying its inference API. An emerging class of attacks
exploit algebraic properties of DNNs (Carlini et al., 2020; Rolnick & Kording,
2020; Jagielski et al., 2020) to obtain high-fidelity copies using orders of magni-
tude fewer queries than the prior state-of-the-art. So far, such powerful attacks
have been limited to networks with few hidden layers and ReLU activations.
In this paper we present algebraic attacks on large-scale natural language models
in a grey-box setting, targeting models with a pre-trained (public) encoder fol-
lowed by a single (private) classification layer. Our key observation is that a small
set of arbitrary embedding vectors is likely to form a basis of the classification
layer’s input space, which a grey-box adversary can compute. We show how to
use this information to solve an equation system that determines the classification
layer from the corresponding probability outputs.
We evaluate the effectiveness of our attacks on different sizes of transformer mod-
els and downstream tasks. Our key findings are that (i) with frozen base layers,
high-fidelity extraction is possible with a number of queries that is as small as
twice the input dimension of the last layer. This is true even for queries that are
entirely in-distribution, making extraction attacks indistinguishable from legiti-
mate use; (ii) with fine-tuned base layers, the effectiveness of algebraic attacks
decreases with the learning rate, showing that fine-tuning is not only beneficial for
accuracy but also indispensable for model confidentiality.
1	Introduction
Machine learning models are often deployed behind APIs that enable querying the model but that
prevent direct access to the model parameters. This restriction aims to protect intellectual property,
as models are expensive to train and hence valuable (Strubell et al., 2019); security, as access to
model parameters facilitates the creation of adversarial examples (Laskov et al., 2014; Ebrahimi
et al., 2018); and privacy, as model parameters carry potentially sensitive information about the
training data (Leino & Fredrikson, 2020). Model extraction attacks (Tramer et al., 2016) attempt
to replicate machine learning models from sets of query-response pairs obtained via the model’s
inference API, thus effectively circumventing the protection offered by the API.
Several extraction attacks on deep neural networks (see Jagielski et al. (2020) for a recent overview)
follow a learning-based approach (Tramer et al., 2016; Orekondy et al., 2018; Pal et al., 2020;
Krishna et al., 2020), where the target model is queried to label data used for training the replica.
The replicas obtained in this way aim to achieve accuracy on the desired task, or agreement with the
target model on predictions, but recovery of the model weights is out of scope of this approach.
More recently, a novel class of attacks has emerged that uses algebraic techniques to recover the
weights of deep neural networks up to model-specific invariances. Examples are the attacks of Milli
et al. (2018), which leverage observations of gradients to recover model parameters, and Rolnick &
Kording (2020); Jagielski et al. (2020); Carlini et al. (2020), which estimate gradients from finite
differences of logits, and then use this information to recover model parameters. Algebraic attacks
improve on learning-based attacks in that they (i) achieve higher-fidelity replicas and (ii) are orders
of magnitude more query-efficient. So far, however, algebraic attacks have only been applied to
small, fully connected neural networks with ReLU activations. In particular, for modern large-scale
1
Under review as a conference paper at ICLR 2021
natural language models (LLMs) such as BERT or GPT-2, the state-of-the-art model extraction
attack is still learning-based (Krishna et al., 2020).
In this paper, we propose the first algebraic attacks on LLMs. We focus on models consisting of
a pre-trained encoder and a single task-specific classification layer. We assume a grey-box setting
where the encoder is public (and hence known to the adversary), and the classification layer is private
(and hence the main target of the attack).
There are two key observations that enable us to extract LLMs via algebraic attacks. The first is
that it is sufficient for an adversary to know rather than to choose the embeddings that are fed into
the last layer. Existing algebraic attacks can infer the inputs to hidden layers, but can only do so on
piecewise linear networks and would not work on LLMs, which use non-linear activations. In the
grey-box setting, an adversary can compute hidden embeddings of any input by querying the public
encoder model, and can query the target LLM on the same input through the model’s API. We show
in theory, and confirm by experiments, that a random set of n embeddings is likely to form a basis
of the last layer’s input space. The raw outputs (i.e., the logits) on this basis uniquely determine the
parameters of the last linear layer, which can be recovered by a transformation to the standard basis.
Our second observation is that this approach extends to the case where the API returns probabil-
ities rather than raw logits, after normalization by the softmax function. For this, we leverage the
invariance under translation of softmax to establish an invariance result for linear functions. Using
this result, we show that the parameters of the last layer can be recovered (up to invariance) from
embedding vectors spanning its input space and their corresponding probability outputs.
We evaluate our attacks on LLMs of different sizes and fine-tuned to different downstream tasks. We
study the effects of using different types and numbers of extraction queries and different learning
rates for fine-tuning the encoder model. Our key findings are:
•	When the target model’s base layers are frozen during fine-tuning (i.e., the attacker can get
the exact embedding of any input), the attack is extremely effective. With only twice as
many queries as the dimension of the embedding space (e.g., 1536 for BERT-base), we
extract models that achieve 100% fidelity with the target, for all model sizes and tasks.
•	When the model’s base layers are fine-tuned together with the task-specific layer, the em-
beddings of the base model only approximate those of the target model and, as expected,
the fidelity of the extracted models decreases as the learning rate grows. Maybe surpris-
ingly, for some models and downstream tasks, we are still able to extract replicas with up
to 82% fidelity and up to 79% task accuracy, for orders of magnitude fewer queries than
required by state-of-the-art learning-based attacks (Krishna et al., 2020).
•	Extraction is possible using either random or in-distribution queries. Replicas extracted
using in-distribution queries perform well on both in-distribution and random challenge in-
puts. This shows that replicas can be created from small numbers of in-distribution queries,
making attempts to extract the model indistinguishable from legitimate use.
In summary, we propose a novel grey-box extraction attack on natural language models that is
indistinguishable from legitimate use in terms of the content and number of queries required.
2	Attack
We consider classification models h : X → Rn, mapping elements from X to label probabilities in
Rn . We assume that h = log ◦ softmax ◦ f ◦ g consists of three components
h: X → Rn → Rm Iog°softm→ Rm
where
•	g : X → Rn is a contextualized embedding model, such as BERT or GPT-2;
•	f : Rn → Rm is an affine function computing logits from embeddings, i.e., f(x) = Ax + b with
A ∈ Rm×n andb ∈ Rm;
•	softmax : Rm → Rm normalizes logits to probability vectors:
softmax(x)
exp(xi)
Pm=ι exp(4).
(1)
2
Under review as a conference paper at ICLR 2021
We assume the adversary knows the embedding model g and tries to infer A and b (resp. f). We call
this adversary grey-box because it can compute the embeddings from the inputs to h.
2.1	Basic Idea
Milli et al. (2018) show how to reconstruct models from oracle access to gradients. Carlini et al.
(2020) show how to replace gradients by finite differences of logits, enabling reconstructing models
without such an oracle. To explain the basic idea, let ei = (0, . . . 0, 1, 0, . . . 0)T be the ith vector
of the standard basis in Rn , and let x ∈ Rn be arbitrary. The ith column of A can be obtained as
the difference between f (x + ei) and f (x). There are two obstacles that prevent us from directly
applying this idea to classification layers from LLMs, namely (1) the attack requires the embeddings
to be chosen, which would effectively amount to reversing the embedding model; and (2) the attack
uses raw logits, while APIs provide only log probabilities normalized with softmax. We show next
how to overcome these obstacles.
2.2	Extraction from logits
As a first step, we overcome the requirement that the adversary be able to choose the inputs to f .
Specifically, we show that it is sufficient for the adversary to know the inputs, given that they are
sufficiently random. We rely on the following standard result:
Proposition 1. Let x(1), . . . , x(n) ∈ Rn be uniformly distributed in an n-cube. Then x(1), . . . , x(n)
form a basis of Rn with probability 1.
Proof. Let {x(i)}in=1 be a basis of Rn and V the subspace generated by {x(i)}im=1, for m < n. For
x chosen uniformly in an n-cube, the space V ∪ {x} has dimension m+ 1 with probability 1. This is
because for x to fall into V , it would need to have zero as coordinates wrt x(m+1) , . . . , x(n), which
happens with probability zero. Applying this argument inductively proves the proposition. □
Based on this result, we mount the following grey-box attack on f ◦ g:
1.	Choose distinct inputs {x(j) }jN=1 in X with N > n;
2.	Compute their embeddings {y(j) = g(x(j))}jN=1 and logits {z(j) = f(y(j))}jN=1;
3.	Construct a matrix Y ∈ R(n+1)×N where the first component of each column is 1 and the rest
are from the embeddings, i.e., Yi,j = (1, y(j))T, and a matrix Z ∈ Rm×N where the columns are
the logit vectors, i.e., Zi,j = zi(j) ;
4.	Solve for A ∈ Rm×n and b ∈ Rm in (b,A) Y = Z, i.e.,
1 ∖
(b1	α1,1	ɑl,n
∕z(I)…ZIN)
(2)
am,1 am,n
zm(1)	. . .	zm(N)
Proposition 2. Assuming g maps inputs to uniformly distributed embeddings in an n-cube1 , the
attack above uniquely determines the parameters of f. I.e., we have A = A and b = b.
Proof. By construction of (2), we have Ay(j) + b = f (y(j)) for j = 1,..., N. The unique solution
can be obtained multiplying Z by the right-inverse of Y. For uniformly random embeddings, this
right-inverse exists because Y has full rank with probability 1 by Proposition 1.
While in theory N = n + 1 distinct queries are sufficient to mount the attack, computing the inverse
of Y based with finite-precision arithmetic can be numerically unstable. In practice, we gather a
larger set of inputs and construct an over-determined system of equations. We then numerically
compute a least squares solution to Equation (2).
1A language model with vocabulary V and maximum sequence length L can only produce |V |L different
embeddings. This is many more points than representable in the precision used, so not an issue in practice.
3
Under review as a conference paper at ICLR 2021
2.3	Extraction from log probabilities
We have so far assumed that the adversary has access to the raw unnormalized values, or logits.
We next extend the attack to the case where the target model’s inference API exposes only the log
probability of each class, obtained by normalizing logits using softmax and returning the component-
wise log of the result.
Softmax is invariant under translation, i.e., softmax(x) = softmax(x + (c, . . . , c)). We lift this
property to linear functions:
Proposition 3. Let C ∈ Rm×n be a matrix with identical rows, i.e., ci,j = ci0 ,j for all i, i0 , j within
range. Then for all A ∈ Rm×n and y ∈ Rn:
softmax((A + C)y) = softmax(Ay)
Proof. Observe that softmax((A + C)y) = softmax(Ay + (k, . . . , k)T) = softmax(Ay), where
k = Pjn=1 c1,j yj since all rows of C are identical. The last equality follows from the translation
invariance of Softmax.	□
Due to Proposition 3 we cannot hope to recover A and b (as in Proposition 2). However, we can still
obtain weights that yield functionally equivalent results when post-processed with softmax. Based
on the construction of C above, we propose the following grey-box attack on h:
1.	Gather inputs and construct a matrix Y ∈ R(n+1)×N as described in Steps 2 and 3 in Section 2.2.
2.	Define D ∈ R(m-1)×N such that Di,j = pi(j) - p(1j), where p(j) = log(Softmax(f(y(j))))
are the log probabilities of the inputs. That is, we collect differences of log probability vectors as
columns in D, and then subtract the first row from all rows.
一 _	_ T
3.	Define b1 =
from
a«1,j = 0 for j = 1, . . . , n. Solve for the remaining components of b and A
0 and
/ 1
«2,1
a«2,n
1 ∖
(N)
yi
∕p21) - pi1)	…P2N)
(3)
—
P(m1) - P(11)	. . .	P(mN)
am,1
am,n
yn(1)	. . .	yn(N)
—
,7
bm
Proposition 4. The attack determines the parameters of f (x) = Ax + b up to translation, that is:
Softmax(Ay + b) = Softmax(Ay + b)
For a proof, observe that pi(k) = Pjn=1 ai,j yj(k) +bi -log Pim=1 exp zi(k)	where z(k) = Ay(k)+
b. Hence we have, for i = 2, . . . , m:
n
Yi,k = pi(k) - p(1k) = X(ai,j - a1,j)yj(k) + (bi - b1)	(4)
j=1
By construction We have a%,j = a%,j - aι,j and b = b - bi, which implies that the columns of
A - A and b - b are constant vectors (resp. the rows ofA - A are all identical). From Proposition 3,
we finally obtain
Softmax(Ay + b) = Softmax((A - (A - A))y + (b - (b - b))) = Softmax(Ay + b)
As before we can gather more than n + 1 inputs and build an solve the over-determined system (3)
to leverage more queries and improve numerical robustness.
2.4 Extraction using Approximations of Embeddings
The attacks presented in Sections 2.2 and 2.3 rely on the assumption that the adversary has full
access to the public embedding model g. In many settings, the embedding model is fine-tuned
together with the task-specific layer f, and so the target model of the attack is h0 = log ◦ Softmax ◦
4
Under review as a conference paper at ICLR 2021
f ◦ g0, rather than h = log ◦ softmax ◦ f ◦ g. We extend our attack to such settings by using
the public embedding model g as an approximation of the embedding model g0 . That is, we do
not assume that the adversary knows g0 ; instead we attack h0 based on embeddings from g, which
is Why the function f * We extract is only an approximation of f. We then construct the replica
h = log ◦ Softmax ◦ f * ◦ g by wiring the extracted classification layer on top of the public encoder.
Clearly, the performance of this approach depends on the quality of the approximation g ≈ g0 , Which
is likely to decrease With the learning rate η With Which g is transformed into g0 . In the next section
We evaluate the feasibility of this approach for different learning rates η.
Even if the attacker does not knoW which embedding model Was used (i.e., a fully black-box attack),
it is still possible to perform a variant of our attack. The attacker can collect query-response pairs
from the target model (since these do not depend on the choice of embedding model) and run the
attack (offline) using the same pairs but different embedding models. In practice, there are relatively
feW Widely-used embedding models, so the attacker can perform a brute-force exploration.
3	Experiments
We evaluate our attack on models for text classification from the GLUE benchmark: SST-2 (Socher
et al., 2013) and MNLI (Williams et al., 2017). SST-2 is a binary sentiment analysis task Where the
goal is to predict positive or negative sentiment of an input sentence. MNLI is a task With 3 output
classes to predict a relation betWeen a premise and a hypothesis.
We train different target models using tWo base models: 1) BERT-Base With 12 layers and 768-
dimensional embeddings and 2) BERT-Small With 4 layers and 512-dimensional embeddings. We
vary the learning rate of the base model (η ranges from 0 to 2 × 10-5) While the classifier layer is
alWays trained With a fixed learning rate η = 2 × 10-5 . In this section, all references to learning
rate refer to the learning rate of the base layers. All our models are trained for 3 epochs using
Hugging Face Transformers v.3.2.0 2. Our core attack logic is simple and is implemented in only 15
lines of Python code With around 500 lines of boilerplate.
To perform our attack, We vary the type (in-distribution vs. random) and number of queries made to
the target model for extraction of the classification layer. For the real queries, We use the respective
SST-2 and MNLI test sets, since these are in-distribution but unseen during training, and for the
random queries We generate random strings (or pairs of strings for MNLI) of varying length up to the
maximum length of the model (128). For attack evaluation, We use the public BERT model (small or
base depending on the target model) and combine it With the extracted classification layer to form a
complete extracted model (as mentioned in Section 2.4). For each extracted model, We measure the
accuracy and agreement With the target model on the validation set of the task or different random
challenge inputs, respectively.
Research Questions. We experimentally evaluate our attack to ansWer the folloWing questions:
•	Type of extraction queries: HoW does the type of query submitted by the attacker (e.g.,
in-distribution vs. random) affect the utility of the extracted model.
•	Number of extraction queries: HoW does the number of queries used by the attacker
affect the accuracy and agreement of the extracted model as compared to the target model?
•	Effect of base learning rate: HoW does fine-tuning of the base model With different learn-
ing rates impact the success of our attack?
Main Results. Table 1 summarizes the key results across our evaluation for tWo extreme cases
Where (i) the base layers of the target model are frozen (η = 0) or (ii) fine-tuned With the same
learning rate as the classifier layer (η = 2 × 10-5). Our findings are:
•	For frozen base models, the attack produces models With 100% agreement, With numbers of
queries equal (for SST-2) or tWice as large (for MNLI) as the dimension of the embedding
vector.
•	For the fine-tuned models, agreement drops to 10% beloW the learning-based state-of-the-
art (Krishna et al., 2020) for SST-2, but is achieved With an order of magnitude less queries:
1821 versus 67 349.
2https://github.com/huggingface/transformers
5
Under review as a conference paper at ICLR 2021
Table 1: Key results for different model sizes and downstream tasks. Agreement shows on how
many inputs the extracted model exactly matches with the target model. #queries denote the number
of queries required to extract the model. For SST-2, we are limited by the test inputs available in the
dataset i.e., 1821. H is the output dimension of the base model.
Dataset	Base Model	Frozen (η = 0)	Fine-tuned (η = 2e — 5)
		Target Agree- #queries Acc.	ment	Target Agree- #queries Acc.	ment
SST-2	BERT-base BERT-Small	.75	1.0	769 (H + 1) .70	1.0	513 (H + 1)	.91	.83	1821 (2.3 * H) .87	.79	1821 (3.5 * H)
MNLI-3	BERT-base BERT-Small	.43	1.0	1024(2 * H) .45	1.0	1536 (2 * H)	.83	.44	3456 (4.5 * H) .77	.44	3584 (7 * H)
Table 2: Effect of using real vs. random queries for extraction (η = 2 × 10-5, #queries = 2H). L∞A
and L∞b denote the difference between the target and extracted matrices. Agreement is computed
for both in-distribution real inputs and randomly generated inputs.
Dataset ∣ Base Model ∣ Extraction With Real queries ∣ Extraction With Random queries
		L∞A	L∞b	Agreement		L∞A	L∞b	Agreement	
				Real	Rand.			Real	Rand.
SST-2	BERT-base	85	155	0.81	0.89	21	3	0.63	0.90
	BERT-small	881	601	0.74	0.76	12	40	0.67	0.86
MNLI-3	BERT-base	494.05	56.11	.41	.21	30.37	14.01	0.36	0.74
	BERT-small	1704.2	1032.98	0.48	0.39	80.84	99.97	0.33	0.88
•	The attack works irrespective of the size of the base model, and performs better on down-
stream tasks With feWer output classes.
Effect of type of queries: Real vs. Random. The type of queries used to extract the model could
impact both the agreement of the extracted model, as Well as the defender’s ability to detect the
attack. To explore this effect, We perform extraction attacks using both real (i.e., in-distribution)
queries, or randomly-generated queries. In all cases, We use only 2H extraction queries (i.e., 1536
for BERT-Base and 1024 for BERT-Small models). We evaluate the Worst-case scenario (from an
attacker’s perspective) Where the base layers have been fine-tuned With the same learning rate as the
classification layers (η = 2 × 10-5). The results are shoWn in Table 2 and the key observations are:
•	A model extracted using real queries provides a similar level of agreement With the target
model on both real and random inputs. Thus, extraction using real queries Works better in
general and is harder to distinguish from genuine benign queries.
•	A model extracted using random queries provides better agreement With the target model
on random inputs than on real inputs. The gap betWeen agreement on real and random
inputs is more pronounced for this type of model.
•	L∞ distances betWeen the target and extracted Weight matrices and bias vectors are large,
but do not necessarily affect agreement due to the invariance of the softmax operation.
Effect of number of queries. Using real queries, We vary the number of queries used for the
extraction and report both the task accuracy of the extracted model and the agreement betWeen the
target and extracted models in Figure 1. The size of the respective test sets limits the number of
queries We could use for this experiment. Again We use a learning rate of η = 2 × 10-5 to evaluate
the Worst-case scenario from the attacker’s perspective. Note that the baseline task accuracy for a
model performing random guessing on a balanced dataset for SST-2 and MNLI is 50% and 33%
respectively. The key observations are:
• For both tasks and models, our extracted model performs better than a random guess and We
observe clear increase in extracted model task accuracy and agreement as queries increase.
• After a sharp initial increase, there appear to be diminishing returns beyond 2H queries
(i.e., 1536 for BERT-Base and 1024 for BERT-Small models).
6
Under review as a conference paper at ICLR 2021
Figure 1: Effect of number of in-distribution queries on extracted model accuracy and agreement
with the target model. Full results (including BERT-Small and random queries) in Appendix C.
Learning rate (η)	Learning rate (η)
Figure 2: Effect of learning rate on target and extracted model accuracy, and agreement with the
target model. Full results (including BERT-Small and random queries) in Appendix B.
•	As above, we achieve lower absolute task accuracy and agreement for MNLI than for SST-
2, since the number of output classes increases.
Overall, the number of queries we require to achieve reasonable agreement is still orders of magni-
tude lower than prior work.
Effect of learning rate. Finally, we quantify the effect of the learning rate used to fine-tune the base
layers. We attack target models trained with learning rates ranging from 0 (frozen) to 2 × 10-5 (note
that the classification layers all use 2 × 10-5) using real queries. The accuracies and agreement of
target and extracted models are depicted in Figure 2. We highlight the following:
•	Agreement between the target and extracted models always starts at 100% for frozen base
layers but decreases with increase in learning rate.
•	As expected, original accuracy increases with learning rate and, interestingly, extracted
model accuracy also increases slightly initially before decreasing for higher learning rates.
4	Discussion
Defenses against model extraction attacks. Several defences against model stealing attacks focus
on identifying malicious query patterns. Juuti et al. (2019); Atli et al. (2020) collect stateful informa-
tion about queries at the API-level and flag potential attacks as deviations from a benign distribution.
Kariyappa & Qureshi (2019) propose a defense that selectively returns incorrect predictions for out-
of-distribution queries. While such approaches are shown to be effective against learning-based
attacks, our attack can leverage random and in-distribution queries alike, and will hence evade such
defenses. Other defenses rely on limiting the information available to the adversary, for example by
7
Under review as a conference paper at ICLR 2021
quantizing prediction probabilities (Tramer et al., 2016), or adding perturbations (Lee et al., 2018)
to poison the attacker’s training objective (Orekondy et al., 2020), or watermarking the model so
that extraction becomes detectable (Uchida et al., 2017). We expect these kinds of defenses may be
effective against algebraic attacks, but leave an in-depth investigation as future work.
Further improving the extracted model. An attacker could combine our attack with techniques
from existing learning-based extraction attacks. For example, after extracting the classification layer
and adding this to a public embedding embedding model (as described in Section 2.4), the attacker
could fine-tune this new model using the set of query-response pairs originally collected for our
extraction attack, as well as any further query-response pairs from the target model. We investigate
this hybrid attack strategy and its converse in Appendix A but leave a more in depth evaluation to a
later revision.
5	Related Work
There is a growing body of work studying the extraction of machine learning models, see e.g.,Lowd
& Meek (2005); Tramer et al. (2016); Orekondy et al. (2018); RoInick & Kording (2020); Pal et al.
(2020); Krishna et al. (2020); Carlini et al. (2020). These approaches differ in terms of the adver-
sary’s objectives, the model architecture, and the techniques they employ for extraction, see Jagielski
et al. (2020) for a recent taxonomy and survey. For conciseness we focus this discussion on work
that targets natural language model or uses techniques related to ours, as well as on defenses.
Extraction of Natural Language Models. Krishna et al. (2020) are the first to report on model
extraction of large natural language models. They rely on a learning-based approach where they
use the target model to label task-specific queries, which they craft based on random words. They
also observe that transfer learning facilitates model extraction in that the adversary can start with
a known base model for training the replica. Our attack goes one step further in that we leverage
public knowledge about the embeddings for mounting an algebraic attack on the last layer. Krishna
et al. (2020) report on agreement (accuracy) of 0.92 (0.90) for SST for around 60K random queries
on SST2, and of 0.80 (0.76) for 392 702 random queries for MNLI. In contrast, our attack requires
only 2 * H (1024 and 1536 for BERT-Small and BERT-Base) number of queries with H being the
dimension of the base-embedding model.
Algebraic model extraction attacks. Most model extraction attacks rely on training the replica on
labels generated by the target model. Recently, a class of attacks has emerged that uses algebraic
techniques to recover deep neural networks, achieving copies with higher fidelity using smaller
numbers of queries as compared to learning-based approaches. The core idea goes back to Milli et al.
(2018), which leverage observations of gradients to recover model parameters. This is leveraged
by Rolnick & Kording (2020); Jagielski et al. (2020); Carlini et al. (2020) which estimate gradients
from finite differences of logits, and then use this information to recover model parameters.
Our attack differs from these approaches in different aspects. First, we only extract a single layer,
whereas the other attacks have been demonstrated for up to 2 hidden layers. Second, as our attack is
grey-box, we only assume that the attacker knows the inputs, whereas the other approaches require
that the adversary be able to choose. Third, we show how to extract the model despite a softmax
activation layer, which is out of scope of the other approaches.
Relationship to softmax regression. The problem of extracting models where only task-specific
layers are fine-tuned is closely related, but not equivalent, to parameter estimation for softmax re-
gression, see, e.g., Van der Vaart (2000); Yao & Wang (2019). The key difference is that for extrac-
tion the goal is to recover a fixed but unknown set of parameters (i.e. a ground truth) with a minimal
amount of data, whereas for regression the goal is to find the best parameters to fit the data.
6	Conclusion
In conclusion, we propose a novel grey-box extraction attack on natural language models that is in-
distinguishable from legitimate use in terms of the content and number of queries required. Existing
detections and defenses based on the number or type of queries are unlikely to be effective, and thus
other approaches are needed to detect or mitigate grey-box extraction attacks.
8
Under review as a conference paper at ICLR 2021
References
Buse Gul Atli, Sebastian Szyller, Mika Juuti, Samuel Marchal, and N. Asokan. Extraction of Com-
plex DNN Models: Real Threat or Boogeyman?, 2020.
Nicholas Carlini, Matthew Jagielski, and Ilya Mironov. Cryptanalytic extraction of neural network
models, 2020.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples
for text classification, 2018.
Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High
accuracy and high fidelity extraction of neural networks. In 29th USENIX Security Symposium,
2020.
Mika Juuti, Sebastian Szyller, Samuel Marchal, and N. Asokan. Prada: Protecting against dnn model
stealing attacks, 2019.
Sanjay Kariyappa and Moinuddin K Qureshi. Defending against model stealing attacks with adap-
tive misinformation, 2019.
Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, and Mohit Iyyer. Thieves
on Sesame Street! Model Extraction of BERT-based APIs. In ICLR. OpenReview.net, 2020.
Pavel Laskov et al. Practical evasion of a learning-based classifier: A case study. In 2014 IEEE
symposium on security and privacy, pp. 197-211. IEEE, 2014.
Taesung Lee, Benjamin Edwards, Ian Molloy, and Dong Su. Defending against model stealing
attacks using deceptive perturbations. arXiv preprint arXiv:1806.00054, 2018.
Klas Leino and Matt Fredrikson. Stolen memories: Leveraging model memorization for calibrated
white-box membership inference, 2020.
Daniel Lowd and Christopher Meek. Adversarial learning. KDD ’05. ACM, 2005.
Smitha Milli, Ludwig Schmidt, Anca D. Dragan, and Moritz Hardt. Model reconstruction from
model explanations, 2018.
Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of
black-box models, 2018.
Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses
against DNN model stealing attacks. In ICLR. OpenReview.net, 2020.
Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy.
Activethief: Model extraction using active learning and unannotated public data. In AAAI, 2020.
David Rolnick and Konrad Kording. Reverse-engineering deep relu networks. In ICML, 2020.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631-1642, 2013.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep
learning in nlp, 2019.
Florian Tramer, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. Stealing machine
learning models via prediction apis. In USENIX Security. USENIX, 2016.
Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin’ichi Satoh. Embedding watermarks into
deep neural networks. Proceedings of the 2017 ACM on International Conference on Multime-
dia Retrieval, Jun 2017. doi: 10.1145/3078971.3078974. URL http://dx.doi.org/10.
1145/3078971.3078974.
9
Under review as a conference paper at ICLR 2021
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. NAACL-HLT, 2018, 2017.
Yaqiong Yao and HaiYing Wang. Optimal subsampling for softmax regression. Statistical Papers,
60(2):235-249, 2019.
10
Under review as a conference paper at ICLR 2021
Table 3: Results for learning-based (Distill), algebraic attacks (Extract), and hybrid attacks using an
extraction dataset 10% the size of the training dataset of the target model (6734 queries for SST-2
and 39 270 for MNLI). All layers of the target model were fine-tuned with learning-rate 2 × 10-5 .
We use the same hyperparameters as Krishna et al. (2020) for distillation.
Task	Model (Acc.)	Extract		Distill		Extract-then-Distill		Distill-then-Extract	
		Acc.	Agr.	Acc.	Agr.	Acc.	Agr.	Acc.	Agr.
SST-2	BERT-Small (0.875)	0.751	0.803	0.856	0.928	0.806	0.849	0.846	0.930
	BERT-Base (0.912)	0.827	0.858	0.885	0.919	0.907	0.945	0.909	0.936
MNLI	BERT-Small (0.777)	0.479	0.504	0.724	0.854	0.660	0.741	0.731	0.862
	BERT-Base (0.840)	0.496	0.515	0.803	0.888	0.763	0.825	0.806	0.894
A	Combining learning-based and algebraic attacks
We draw a side-by-side comparison between learning-based and algebraic extraction attacks and
explore whether combining both types of attacks can give better overall results. We follow the
learning-based approach from Krishna et al. (2020), which uses (black-box) distillation to extract an
approximation of a target model. We evaluate two different hybrid attacks:
1.	Distill-then-Extract. First distill a model using the public pre-trained encoder as a base.
Then use algebraic extraction reusing the same set of queries to extract and replace the last
layer.
2.	Extract-then-Distill. First extract the last layer using the algebraic attack with the public
pre-trained encoder. Then reuse the same set of queries to distill the resulting model.
The results are shown in Table 3. They show that although learning-based attacks on their own do
generally better than algebraic attacks for large number of queries, their combination using either
strategy achieves consistently better agreement. The Distill-then-Extract strategy appears superior
to the Extract-then-Distill strategy. Given that an algebraic attack reusing queries from distillation is
inexpensive and that it starts from a good baseline, the gains although modest show that combining
both attacks is cheap and effective.
11
Under review as a conference paper at ICLR 2021
B Full experimental results with varying learning rate
Effect of learning rate on task accuracy of extracted models, and agreement with target model on
in-distribution (Fig. 3) and random (Fig. 4) queries.
SST-2 (BERT-Base), #queries = 1536)
MNLI (BERT-Base), #queries = 1536)
Figure 3: Extraction with in-distribution queries
MNLI (BERT-SmaII), #queries = 1024
Learning rate (ŋ)
Figure 4: Extraction with random queries
12
Under review as a conference paper at ICLR 2021
C Full experimental results with varying number of queries
C.1 BERT-BASE AND SST-2
Effect of number of queries on task accuracy of extracted model and agreement with target model,
for in-distribution (Fig. 5) and random (Fig. 6) queries. Baseline accuracy of random guess: 50%.
Number of queries
Number of queries
Number of queries
Number of queries
Figure 5: Extraction with in-distribution queries
Number of queries
Figure 6: Extraction with random queries
Number of queries
13
Under review as a conference paper at ICLR 2021
C.2 BERT-BASE AND MNLI
Effect of number of queries on task accuracy of extracted model and agreement with target model,
for in-distribution (Fig. 7) and random (Fig. 8) queries. Baseline accuracy of random guess: 33%.
Number of queries
Numberofqueries
Numberofqueries
Numberofqueries
Figure 7: Extraction with in-distribution queries
Number of queries	Number of queries
Figure 8: Extraction with random queries
14
Under review as a conference paper at ICLR 2021
C.3 BERT-SMALL AND SST-2
Effect of number of queries on task accuracy of extracted model and agreement with target model,
for in-distribution (Fig. 9) and random (Fig. 10) queries. Baseline accuracy of random guess: 50%.
SST-2 (BERT-SmaII, η = 0), Real queries	SST-2 (BERT-SmaII, η = 10-7), Real queries
Numberofqueries	Numberofqueries
Numberofqueries
Numberofqueries
Numberofqueries
Figure 9: Extraction with in-distribution queries
Numberofqueries	Numberofqueries
Figure 10: Extraction with random queries
15
Under review as a conference paper at ICLR 2021
C.4 BERT-SMALL AND MNLI
Effect of number of queries on task accuracy of extracted model and agreement with target model,
for in-distribution (Fig. 11) and random (Fig. 12) queries. Baseline accuracy of random guess: 33%.
MNLl (BERT-SmaII, η = 0), Real queries
Number of queries
Numberofqueries
Numberofqueries	Numberofqueries
Figure 11: Extraction with in-distribution queries
Number of queries	Number of queries
Figure 12: Extraction with random queries
16