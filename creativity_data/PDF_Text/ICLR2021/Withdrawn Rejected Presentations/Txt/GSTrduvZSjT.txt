Under review as a conference paper at ICLR 2021
Adaptive Gradient Methods Converge Faster
with Over-Parameterization
(and you can do a line-search)
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods are typically used for training over-parameterized
models capable of exactly fitting the data; we thus study their convergence in this
interpolation setting. Under an interpolation assumption, we prove that AMSGrad
with a constant step-size and momentum can converge to the minimizer at the
faster O(1/T) rate for smooth, convex functions. Furthermore, in this setting, we
show that AdaGrad can achieve an O(1) regret in the online convex optimization
framework. When interpolation is only approximately satisfied, we show that
constant step-size AMSGrad converges to a neighbourhood of the solution. On the
other hand, we prove that AdaGrad is robust to the violation of interpolation and
converges to the minimizer at the optimal rate. However, we demonstrate that even
for simple, convex problems satisfying interpolation, the empirical performance of
these methods heavily depends on the step-size and requires tuning. We alleviate
this problem by using stochastic line-search (SLS) and Polyak’s step-sizes (SPS)
to help these methods adapt to the function’s local smoothness. By using these
techniques, we prove that AdaGrad and AMSGrad do not require knowledge
of problem-dependent constants and retain the convergence guarantees of their
constant step-size counterparts. Experimentally, we show that these techniques help
improve the convergence and generalization performance across tasks, from binary
classification with kernel mappings to classification with deep neural networks.
1	Introduction
Adaptive gradient methods such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton,
2012), AdaDelta (Zeiler, 2012), Adam (Kingma & Ba, 2015), and AMSGrad (Reddi et al., 2018)
are popular optimizers for training deep neural networks (Goodfellow et al., 2016). These methods
scale well and exhibit good performance across problems, making them the default choice for many
machine learning applications. Theoretically, these methods are usually studied in the non-smooth,
online convex optimization setting (Duchi et al., 2011; Reddi et al., 2018) with recent extensions to
the strongly-convex (Mukkamala & Hein, 2017; Wang et al., 2020; Xie et al., 2020) and non-convex
settings (Li & Orabona, 2019; Ward et al., 2019; Zhou et al., 2018; Chen et al., 2019; Wu et al.,
2019; Defossez et al., 2020; Staib et al., 2019). An online-batch reduction gives guarantees similar to
stochastic gradient descent (SGD) in the offline setting (Cesa-Bianchi et al., 2004; Hazan & Kale,
2014; Levy et al., 2018).
However, there are several discrepancies between the theory and application of these methods.
Although the theory advocates for using decreasing step-sizes for Adam, AMSGrad and its vari-
ants (Kingma & Ba, 2015; Reddi et al., 2018), a constant step-size is typically used in practice (Paszke
et al., 2019). Similarly, the standard analysis of these methods assumes a decreasing momentum
parameter, however, the momentum is fixed in practice. On the other hand, AdaGrad (Duchi et al.,
2011) has been shown to be “universal” as it attains the best known convergence rates in both the
stochastic smooth and non-smooth settings (Levy et al., 2018), but its empirical performance is rather
disappointing when training deep models (Kingma & Ba, 2015). Improving the empirical performance
was indeed the main motivation behind Adam and other methods (Tieleman & Hinton, 2012; Zeiler,
2012) that followed AdaGrad. Although these methods have better empirical performance, they are
not guaranteed to converge to the solution with a constant step-size and momentum parameter.
1
Under review as a conference paper at ICLR 2021
Another inconsistency is that although the standard theoretical results are for non-smooth functions,
these methods are also extensively used in the easier, smooth setting. More importantly, adaptive gra-
dient methods are generally used to train highly expressive, large over-parameterized models (Zhang
et al., 2017; Liang & Rakhlin, 2018) capable of interpolating the data. However, the standard theoreti-
cal analyses do not take advantage of these additional properties. On the other hand, a line of recent
work (Schmidt & Le Roux, 2013; Jain et al., 2018; Ma et al., 2018; Liu & Belkin, 2020; Cevher &
Vu, 2019; Vaswani et al., 2019a;b; WU et al., 2019; Loizou et al., 2020) focuses on the convergence
of SGD in this interpolation setting. In the standard finite-sum case, interpolation implies that all
the functions in the sum are minimized at the same solution. Under this additional assumption, these
works show SGD with a constant step-size converges to the minimizer at a faster rate for both convex
and non-convex smooth functions.
In this work, we aim to resolve some of the discrepancies in the theory and practice of adaptive
gradient methods. To theoretically analyze these methods, we consider a simplistic setting - smooth,
convex functions under interpolation. Using the intuition gained from theory, we propose better
techniques to adaptively set the step-size for these methods, dramatically improving their empirical
performance when training over-parameterized models.
1.1	Background and contributions
Constant step-size. We focus on the theoretical convergence of two adaptive gradient methods:
AdaGrad and AMSGrad. For smooth, convex functions, Levy et al. (2018) prove that AdaGrad with
a constant step-size adapts to the smoothness and gradient noise, resulting in an O(1∕t + ζ∕√T)
convergence rate, where T is the number of iterations and ζ 2 is a global bound on the variance in the
stochastic gradients. This convergence rate matches that of SGD under the same setting (Moulines
& Bach, 2011). In Section 3, we show that constant step-size AdaGrad also adapts to interpolation
and prove an O(1∕t + σ∕√τ) rate, where σ is the extent to which interpolation is violated. In the
over-parameterized setting, σ2 can be much smaller than ζ2 (Zhang & Zhou, 2019), implying a faster
convergence. When interpolation is exactly satisfied, σ2 = 0, we obtain an O(1∕τ) rate, while ζ2
can still be large. In the online convex optimization framework, for smooth functions, we show that
the regret of AdaGrad improves from O(√T) to O(1) when interpolation is satisfied and retains
its O(√T)-regret guarantee in the general setting (Appendix C.2). Assuming its corresponding
preconditioner remains bounded, we show that AMSGrad with a constant step-size and constant
momentum parameter also converges at the rate O(1∕τ) under interpolation (Section 4). However,
unlike AdaGrad, it requires specific step-sizes that depend on the problem’s smoothness. More
generally, constant step-size AMSGrad converges to a neighbourhood of the solution, attaining an
O(1∕τ + σ2) rate, which matches the rate of constant step-size SGD in the same setting (Schmidt &
Le Roux, 2013; Vaswani et al., 2019a). When training over-parameterized models, this result provides
somejustification for the faster (O(1∕τ) vs. O(1∕√τ)) convergence of the AMSGrad variant typically
used in practice.
Adaptive step-size. Although AdaGrad converges at the same asymptotic rate for any step-size (up to
constants), itis unclear how to choose this step-size without manually trying different values. Similarly,
AMSGrad is sensitive to the step-size, converging only for a specific range in both theory and practice.
In Section 5, we experimentally show that even for simple, convex problems, the step-size has a big
impact on the empirical performance of AdaGrad and AMSGrad. To overcome this limitation, we
use recent methods (Vaswani et al., 2019a; Loizou et al., 2020) that automatically set the step-size
for SGD. These works use stochastic variants of the classical Armijo line-search (Armijo, 1966) or
the Polyak step-size (Polyak, 1963) in the interpolation setting. We combine these techniques with
adaptive gradient methods and show that a variant of stochastic line-search (SLS) enables AdaGrad to
adapt to the smoothness of the underlying function, resulting in faster empirical convergence, while
retaining its favourable convergence properties (Section 3). Similarly, AMSGrad with variants of SLS
and SPS can match the convergence rate of its constant step-size counterpart, but without knowledge
of the underlying smoothness properties (Section 4).
Experimental results. Finally, in Section 5, we benchmark our results against SGD variants with
SLS (Vaswani et al., 2019b), SPS (Loizou et al., 2020), tuned Adam and its recently proposed
variants (Luo et al., 2019; Liu et al., 2020). We demonstrate that the proposed techniques for setting
the step-size improve the empirical performance of adaptive gradient methods. These improvements
are consistent across tasks, ranging from binary classification with a kernel mapping to multi-class
classification using standard deep neural network architectures.
2
Under review as a conference paper at ICLR 2021
2	Problem setup
We consider the unconstrained minimization of an objective f : Rd → R with a finite-sum structure,
f (W) = 1 Pn=1 fi(w).In supervised learning, n represents the number of training examples, and f
is the loss function on training example i. Although we focus on the finite-sum setting, our results
can be easily generalized to the online optimization setting. The objective of our analysis is to better
understand the effect of the step-size and line-searches when interpolation is (almost) satisfied. This is
complicated by the fact that adaptive methods are still poorly understood; state-of-the-art analyses do
not show an improvement over gradient descent in the worst-case. To focus on the effect of step-sizes,
we make the simplifying assumptions described in this section.
We assume f and each f are differentiable, convex, and lower-bounded by f * and 疗,respectively.
Furthermore, we assume that each function fi in the finite-sum is Li -smooth, implying that f is
Lmax-smooth, where Lmax = maxi Li . We also make the standard assumption that the iterates
remain bounded in a ball of radius D around a global minimizer, kwk - w* k ≤ D for all wk (Ahn
et al., 2020). We remark that the bounded iterates assumption simplifies the analysis but is not
essential, and similar to Reddi et al. (2018); Duchi et al. (2011); Levy et al. (2018), our theoretical
results can be extended to include a projection step. We include the formal definitions of these
properties (Nemirovski et al., 2009) in Appendix A.
The interpolation assumption means that the gradient of each fi in the finite-sum converges to zero
at an optimum. If the overall objective f is minimized at w*, Vf (w*) = 0, then for all f we have
Vfi(w*) = 0. The interpolation condition can be exactly satisfied for many over-parameterized
machine learning models such as non-parametric kernel regression without regularization (Belkin
et al., 2019; Liang & Rakhlin, 2018) and over-parameterized deep neural networks (Zhang et al.,
2017). We measure the extent to which interpolation is violated by the disagreement between
the minimum overall function value f* and the minimum value of each individual functions fi*,
σ2 := Ei[f* - fi*] ∈ [0, ∞) (Loizou et al., 2020). The minimizer of f need not be unique for σ2 to
be uniquely defined, as it only depends on the minimum function values. Interpolation is said to be
exactly satisfied if σ2 = 0, and we also study the setting when σ2 > 0.
For a preconditioner matrix Ak and a constant momentum parameter β ∈ [0, 1), the update for a
generic adaptive gradient method at iteration k can be expressed as:
wk+1 = wk - ηk Ak-1mk ; mk = βmk-1 + (1 - β)Vfik (wk)	(1)
Here, Vfik (wk) is the stochastic gradient of a randomly chosen function fik, and ηk is the step-size.
Adaptive gradient methods typically differ in how their preconditioners are constructed and whether
or not they include the momentum term βmk-1 (see Table 1 for a list of common methods). Both
Table 1: Adaptive preconditioners (analyzed methods are bolded), with G0 = 0 and β1, β2 ∈ [0, 1). In practice, a small I is added to ensure Ak 0. *: We use the PyTorch implementation in experiments which includes bias correction.			
Optimizer	Gk	(Vk= Vfik (Wk))	Ak	β
AdaGrad	Gk-1 +diag(VkVk>)	G1k/2	0
RMSProp	β2Gk-1+(1-β2)diag(VkVk>)	G1k/2	0
Adam	(β2Gk-i + (1 - β2)diag(VkVk>))/(1 - βk)	Gk/2	β1
AMSGrad*	(β2Gk-i + (1 - β2 )diag(VkVk>))/(1 - βk)	max{Ak-1, Gk/2 }	β1
RMSProp and Adam maintain an exponential moving average of past stochastic gradients, but as
Reddi et al. (2018) pointed out, unlike AdaGrad, the corresponding preconditioners do not guarantee
that Ak+1 Ak and the resulting per-dimension step-sizes do not go to zero. This can lead to
large fluctuations in the effective step-size and prevent these methods from converging. To mitigate
this problem, they proposed AMSGrad, which ensures Ak+1 Ak and the convergence of iterates.
Consequently, our theoretical results focus on AdaGrad, AMSGrad and other adaptive gradient
methods that ensure this monotonicity. However, we also considered RMSProp and Adam in our
experimental evaluation.
Although our theory holds for both the full matrix and diagonal variants (where Ak is a diagonal
matrix) of these methods, we use only the latter in experiments for scalability. The diagonal variants
3
Under review as a conference paper at ICLR 2021
perform a per-dimension scaling of the gradient and avoid computing the full matrix inverse, so their
per-iteration cost is the same as SGD, although with an additional O(d) memory. For AMSGrad, we
assume that the corresponding preconditioners are well-behaved in the sense that their eigenvalues are
bounded in an interval [amin , amax]. This is a common assumption made in the analysis of adaptive
methods. Moreover, for diagonal preconditioners, such a boundedness property is easy to verify, and
it is also inexpensive to maintain the desired range by projection. Our main theoretical results for
AdaGrad (Section 3) and AMSGrad (Section 4) are summarized in Table 2.
Table 2: ResUlts for smooth, Convex functions.
Method	Adapts		
	Step-size	to smoothness	Rate	Reference
AdaGrad	Constant	X Conservative Lipschitz LS	✓	O(1∕t + σ∕√τ) Theorem 1 θ(1∕τ + σ∕√τ) Theorem 2	
AMSGrad	Constant	X	O(1/T + σ2)	Theorem 3
AMSGrad w/o momentum Armijo SLS	✓		O(1/T + σ2)	Theorem 4
AMSGrad	Conservative Armijo SPS ✓	O(1/T + σ2)	Theorem 5
3	AdaGrad
For smooth, convex objectives, Levy et al. (2018) showed that AdaGrad converges at a rate
O(1∕t + ζ∕√T), where Z2 = SuPw E』||Vf (w) - Vfi(W)I∣2] is a uniform bound on the variance
of the stochastic gradients. In the over-parameterized setting, we show that AdaGrad achieves the
O(1/T) rate when interpolation is exactly satisfied and a slower convergence to the solution if
interpolation is violated.1 The proofs for this section are in Appendix C.
Theorem 1 (Constant step-size AdaGrad). Assuming (i) convexity and (ii) Lmax -smoothness of each
fi, and (iii) bounded iterates, AdaGrad with a constant step-size η and uniform averaging such that
WT = T ∑T=ι Wk, converges at a rate
E[f (WT) - f *] ≤ T + 等，Where α = 1 (D +2η) dLmaχ.
When interpolation is exactly satisfied, a similar proof technique can be used to show that AdaGrad
incurs only O(1) regret in the online convex optimization setting (Theorem 6 in Appendix C.2). The
above theorem shows that AdaGrad is robust to the violation of interpolation and converges to the
minimizer at the desired rate for any reasonable step-size. Although this is a favourable property, the
best constant step-size depends on the problem, and as we demonstrate experimentally in Section 5,
the performance of AdaGrad depends on correctly tuning this step-size.
To overcome this limitation, we use a conservative Lipschitz line-search that sets the step-size on
the fly, improving the empirical performance of AdaGrad while retaining its favourable convergence
guarantees. At each iteration, this line-search selects a step-size ηk that satisfies the property
fik(Wk- ηkVfik(Wk)) ≤ fik(Wk) - cηk l∣vfik(Wk)k2, and η ≤ ηk-ι.
(2)
The resulting step-size is then used in the standard AdaGrad update in Eq. (1). To find an acceptable
step, our results use a backtracking line-search, described in Appendix F. For simplicity, the theoretical
results assume access to the largest step-size that satisfies the above condition.2 Here, c is a hyper-
parameter determined theoretically and typically set to 1/2 in our experiments. The “conservative” part
of the line-search is the non-increasing constraint on the step-sizes, which is essential for convergence
to the minimizer when interpolation is violated. We refer to it as the Lipschitz line-search as it
is only used to estimate the local Lipschitz constant. Unlike the classical Armijo line-search for
1A similar result also appears in the course notes (Orabona, 2019).
2The difference between the exact and backtracking line-search is minimal, and the bounds are only changed
by a constant depending on the backtracking parameter.
4
Under review as a conference paper at ICLR 2021
preconditioned gradient descent, the line-search in Eq. (2) is in the gradient direction, even though
the update is in the preconditioned direction. The resulting step-size found is guaranteed to be in the
range [2(1-c)/Lmax, ηk-1] (Vaswani et al., 2019b) and allows us to prove the following theorem.
Theorem 2. Under the same assumptions as Theorem 1, AdaGrad with a conservative Lipschitz
line-search with c = 1/2, a step-size upper bound ηmax and uniform averaging converges at a rate
* α	λ∕ασ	1
E[f(wτ) 一 f ] ≤ τ + -√t, Where α =r
max
+ 2 ηmax2dLmax.
Intuitively, the Lipschitz line-search enables AdaGrad to take larger steps at iterates where the
underlying function is smoother. It retains the favourable convergence guarantees of constant step-
size AdaGrad, while improving its empirical performance (Section 5). Moreover, if interpolation is
exactly satisfied, we can obtain an O(1/T) convergence without the conservative constraint ηk ≤ ηk-1
on the step-sizes (Appendix C.3).
4	AMSGrad and non-decreasing preconditioners
In this section, we consider AMSGrad and, more generally, methods with non-decreasing precon-
ditioners satisfying Ak Ak-1. As our focus is on the behavior of the algorithm with respect to
the overall step-size, we make the simplifying assumption that the effect of the preconditioning
is bounded, meaning that the eigenvalues of Ak lie in the [amin , amax] range. This is a common
assumption made in the analyses of adaptive methods (Reddi et al., 2018; Alacaoglu et al., 2020) that
prove worst-case convergence rates matching those of SGD. For our theoretical results, we consider
the variant of AMSGrad without bias correction, as its effect is minimal after the first few iterations.
The proofs for this section are in Appendix D and Appendix E.
The original analysis of AMSGrad (Reddi et al., 2018) uses a decreasing step-size and a decreasing
momentum parameter. It shows an O(1∕√t) convergence for AMSGrad in both the smooth and
non-smooth convex settings. Recently, Alacaoglu et al. (2020) showed that this analysis is loose
and that AMSGrad does not require a decreasing momentum parameter to obtain the O(1∕√T) rate.
However, in practice, AMSGrad is typically used with both a constant step-size and momentum
parameter. Next, we present the convergence result for this commonly-used variant of AMSGrad.
Theorem 3.	Under the same assumptions as Theorem 1, and assuming (iv) non-decreasing precon-
ditioners (v) bounded eigenvalues in the [amin, amax] interval, where κ = amax/amin, AMSGrad with
β ∈ [0,1), constant step-size η = I—β ,Lmin and uniform averaging converges at a rate,
1+β 2Lmax
*	1 + β∖ 2LmaχD2dκ	2
Ef(WT) - f ] ≤ I 1-j ) ------T----+ σ .
When σ = 0, we obtain a O(1/T) convergence to the minimizer. However, when interpolation is only
approximately satisfied, we obtain convergence to a neighbourhood with its size depending onσ 2 . We
observe that the noise σ 2 is not amplified because of the non-decreasing momentum (or step-size). A
similar distinction between the convergence of constant step-size Adam (or AMSGrad) vs. AdaGrad
has also been recently discussed in the non-convex setting (Defossez et al., 2020). Unfortunately, the
final bound is minimized by setting β1 = 0 and our theoretical analysis does not show an advantage
of using momentum. Note that this is a common drawback in the analyses of heavy-ball momentum
for non-quadratic functions in both the stochastic and deterministic settings (Ghadimi et al., 2015;
Reddi et al., 2018; Alacaoglu et al., 2020; Sebbouh et al., 2020).
Since AMSGrad is typically used for optimizing over-parameterized models, the violation σ2 is small,
even when interpolation is not exactly satisfied. Another reason that constant step-size AMSGrad
is practically useful is because of the use of large batch-sizes that result in a smaller effective
neighbourhood. To get some intuition about the effect of batch-size, note that if we use a batch-size
of b, the resulting neighbourhood depends on σb2 := EB;|B|=b [fB (w *) - fB(xB*)] where wB* is the
minimizer of a batch B of training examples. By convexity, σb ≤ E[∣∣w* — XB∣∣ kVfB(w*)k]. If
We assume that the distance ∣w* — XB∣∣ is bounded, σb α EkVfB(w*)∣∣. Since the examples in
each batch are sampled with replacement, using the bounds in (Lohr, 2009), σb2 (X n-b ∣vfi(w*)k,
showing that the effective neighbourhood shrinks as the batch-size becomes larger, becoming zero for
5
Under review as a conference paper at ICLR 2021
ax
min
the full-batch variant. With over-parameterization and large batch-sizes, the effective neighbourhood
is small enough for machine learning tasks that do not require exact convergence to the solution.
The constant step-size required for the above result depends on Lmax , which is typically unknown.
Furthermore, using a global bound on Lmax usually results in slower convergence since the local
Lipschitz constant can vary considerably during the optimization. To overcome these issues, we use a
stochastic variant of the Armijo line-search. Unlike the Lipschitz line-search whose sole purpose is
to estimate the Lipschitz constant, the Armijo line-search selects a suitable step-size in the precon-
ditioned gradient direction, and as we show in Section 5, it results in better empirical performance.
Similar to the constant step-size, when interpolation is violated, we only obtain convergence to a
neighbourhood of the solution. The stochastic Armijo line-search returns the largest step-size ηk
satisfying the following conditions at iteration k,
fik (wk ― ηk Ak Nfik (wk )) ≤ fik (wk) ― Cnk ∣∣^ fik (wk ) k A-1 , and ηk ≤ ηmax.	(3)
k
The step-size is artificially upper-bounded by ηmax (typically chosen to be a large value). The line-
search guarantees descent on the current function fik and that ηk lies in the [2amin (1-c)/Lmax, ηmax]
range. In the next theorem, we first consider the variant of AMSGrad without momentum (β = 0)
and show that using the Armijo line-search retains the O(1/T) convergence rate without the need to
know the Lipschitz constant.
Theorem 4.	Under the same assumptions as Theorem 1, AMSGrad with zero momentum, Armijo
line-search with C = 3/4, a step-size upper bound ηmax and uniform averaging converges at a rate,
E[f (wτ) 一 f *] ≤ (3D d2τamax + 3nmaχσ2) max
Comparing this rate with that of using constant step-size (Theorem 3), we observe that the Armijo
line-search results in a worse constant in the convergence rate and a larger neighbourhood. These
dependencies can be improved by considering a conservative version of the Armijo line-search.
However, we experimentally show that the proposed line-search drastically improves the empirical
performance of AMSGrad. We show that a similar bound also holds for AdaGrad (see Theorem 7
in Appendix C). AdaGrad with an Armijo line-search converges to a neighbourhood in the absence
of interpolation (unlike the results in 3). Moreover, the above bound depends on amin which can
be O() in the worst-case, resulting in an unsatisfactory worst-case rate of O(1/T) even in the
interpolation setting. However, like AMSGrad, AdaGrad with Armijo line-search has excellent
empirical performance, implying the need for a different theoretical assumption in the future.
Before considering techniques to set the step-size for AMSGrad including momentum, we present
the details of the stochastic Polyak step-size (SPS) Loizou et al. (2020); Berrada et al. (2019) and
Armijo SPS, our modification to the adaptive setting. These variants set the step-size as:
fik (wk ) - fik	fik (wk ) - fik
nk=mmt C ∣Vfik (wk)k2 ,nmax∫, ArmijOSPS: nk=min C -fik (wk )∣A-1 ,nmaχj.
Here, f； is the minimum value for the function fik .The advantage of SPS over a line-search is that it
dOes nOt require a pOtentially expensive backtracking prOcedure tO set the step-size. MOreOver, it can
be shown that this step-size is always larger than the one returned by line-search, which can lead to
faster convergence. However, SPS requires knowledge of fi for each function in the finite-sum. This
value is difficult to obtain for general functions but is readily available in the interpolation setting for
many machine learning applications. Common loss functions are lower-bounded by zero, and the
interpolation setting ensures that these lower-bounds are tight. Consequently, using SPS with fi =0
has been shown to yield good performance for over-parameterized problems (Loizou et al., 2020;
Berrada et al., 2019). In Appendix D, we show that the Armijo line-search used for the previous
results can be replaced by Armijo SPS and result in similar convergence rates.
For AMSGrad with momentum, we propose to use a conservative variant of Armijo SPS that sets
ηmax = ηk-1 at iteration k ensuring that ηk ≤ ηk-1. This is because using a potentially increasing
step-size sequence along with momentum can make the optimization unstable and result in divergence.
Using this step-size, we prove the following result.
Theorem 5.	Under the same assumptions of Theorem 1 and assuming (iv) non-decreasing precon-
ditioners (v) bounded eigenvalues in the [amin, amax] interval with κ = amax/amin, AMSGrad with
6
Under review as a conference paper at ICLR 2021
β ∈ [0,1), conservative Armijo SPS with C = 1+β∕ι-β and uniform averaging converges at a rate,
E[f (Wτ) — f *] ≤
(1 + β ∖2 2LmaχD2dκ
1-β )	T-
+ σ2.
The above result exactly matches the convergence rate in Theorem 3 but does not require knowledge of
the smoothness constant to set the step-size. Moreover, the conservative step-size enables convergence
without requiring an artificial upper-bound ηmax as in Theorem 8. We note that a similar convergence
rate can be obtained when using a conservative variant of Armijo SLS ( Appendix E.2), although our
theoretical techniques only allow for a restricted range of β .
When Ak = Id, the AMSGrad update is equivalent to the update for SGD with heavy-ball momen-
tum (Sebbouh et al., 2020). By setting Ak = Id in the above result, we recover an O(1/T + σ2)
rate for SGD (using SPS to set the step-size) with heavy-ball momentum. In the smooth, convex
setting, our rate matches that of (Sebbouh et al., 2020); however, unlike their result, we do not require
knowledge of the Lipschitz constant. This result also provides theoretical justification for the heuristic
used for incorporating heavy-ball momentum for SLS in (Vaswani et al., 2019b).
For a general preconditioner, the AMSGrad update in Eq. (1) is not equivalent to heavy-ball mo-
mentum. With a constant momentum parameter γ ∈ [0, 1), the general heavy-ball update (Loizou &
Richtarik, 2017) is given as wk+ι = Wk - αk A-1Vfik (Wk) + Y (Wk - wk-ι) (refer to Appendix E.1
for a relation between the two updates). Unlike this update, AMSGrad also preconditions the momen-
tum direction (Wk - Wk-1). If we consider the zero-momentum variant of adaptive gradient methods
as preconditioned gradient descent, the above update is a more natural way to incorporate momentum.
We explore this alternate method and prove the same O(1/T + σ2) convergence rate for constant
step-size, conservative Armijo SPS and Armijo SLS techniques in Appendix E.3. In the next section,
we use the above techniques for training large over-parameterized deep networks.
5 Experimental evaluation
MarqiriiO.Ol	Margin:0.05
10 12
ɪ0100-0-
1 1
Bo= SSo-U-E
O 50 IOO 150	200
Epoch
50	100	150	200
Epoch
(a) AdaGrad
Figure 1: Synthetic experiments showing the impact of step-size on the performance of AdaGrad,
AMSGrad with varying step-sizes, including the default in PyTorch, and the SLS variants.
Margin :0.01
Marain:0.05
1 O 1
O0-
IlM
6Sso-U-E
50 IoO 150	200
Epoch
100	150	200
Epoch
(b) AMSGrad
Synthetic experiment: We first present an experiment to show that AdaGrad and AMSGrad
with constant step-size are not robust even for simple, convex problems. We use their PyTorch
implementations (Paszke et al., 2019) on a binary classification task with logistic regression. Following
the protocol of Meng et al. (2020), we generate a linearly-separable dataset with n = 103 examples
(ensuring interpolation is satisfied) and d = 20 features with varying margins. For AdaGrad and
AMSGrad with a batch-size of 100, we show the training loss for a grid of step-sizes in the [103, 10-3]
range and also plot their default (in PyTorch) variants. For AdaGrad, we compare against the proposed
Lipschitz line-search and Armijo SLS variants. As is suggested by the theory, for each of these variants,
we set the value of c = 1/2. For AMSGrad, we compare against the variant employing the Armijo
SLS with c= 1/2.3 and use the default (in PyTorch) momentum parameter of β = 0.9. In Fig. 1, we
observe a large variance across step-sizes and poor performance of the default step-size. The best
performing variant of AdaGrad/AMSGrad has a step-size of order 102. The line-search variants have
good performance across margins, often better than the best-performing constant step-size.
3This corresponds to the largest allowable step-size in Theorem 4 without momentum. Unfortunately, the
values of c suggested by the analysis incorporating momentum Theorem 5 are too conservative.
7
Under review as a conference paper at ICLR 2021
CIFAR10 - ResNet34
0	50	100	150	200
Epoch
CIFAR10 - ResNet34
Cifarioo - DenSeNet121
CIFAR100 - ReSNet34	TinylmageNet - ResNetl8
0	50	100	150	200
Epoch
50	100	150	200
Epoch
Cifarioo - DenSeNet121
0.39
0.38-
0.37-
0.36
0.35
0	50	100	150	200
Epoch
Tiny ImageNet - ResNetl8
50	100	150	200	0	50	100	150	200 "	50	100	150	200
Epoch	Epoch	Epoch

-⅛- Amsgrad + SLS -⅛- Amsgrad + SLS + HB —Adabound —Radam —Adam →- SLS Adagrad + SLS
Figure 2: Comparing optimizers for multi-class classification with deep networks. Training loss (top)
and validation accuracy (bottom) for CIFAR-10, CIFAR-100 and Tiny ImageNet.
Real experiments: Following the protocol in (Luo et al., 2019; Vaswani et al., 2019b; Loizou et al.,
2020), we consider training standard neural network architectures for multi-class classification on
CIFAR-10, CIFAR-100 and variants of the ImageNet datasets. For each of these experiments, we use
a batch-size of 128 and compare against Adam with the best constant step-size found by grid-search.
We also include recent improved variants of Adam; RAdam (Liu et al., 2020) and AdaBound (Luo
et al., 2019). To see the effect of preconditioning, we compare against SGD with SLS (Vaswani et al.,
2019a) and SPS (Loizou et al., 2020). We find that SGD with SLS is more stable and has consistently
better test performance than SPS, and hence we only show results for SLS. We also compared against
tuned constant step-size SGD and similar to (Vaswani et al., 2019a), we observe that it is consistently
outperformed by SGD with SLS.
For the proposed methods, we consider the combinations with theoretical guarantees in the convex
setting, specifically AdaGrad and AMSGrad with the Armijo SLS. For AdaGrad, we only show
Armijo SLS since it consistently outperforms the Lipschitz line-search. For all variants with Armijo
SLS, we use c = 0.5 for all convex experiments (suggested by Theorem 4 and Vaswani et al.
(2019a)). Since we do not have a theoretical analysis for non-convex problems, we follow the protocol
in Vaswani et al. (2019a) and set c = 0.1 for all the non-convex experiments. Throughout, we set
β = 0.9 for AMSGrad. We also compare to the AMSGrad variant with heavy-ball (HB) momentum
(with γ = 0.25 found by grid-search). We refer to Appendix F for a detailed discussion about the
practical considerations and pseudocodes for the SLS/SPS variants.
We show a subset of results for CIFAR-10, CIFAR-100 and Tiny ImageNet and defer the rest
to Appendix G. From Fig. 2 we make the following observations, (i) in terms of generalization,
AdaGrad and AMSGrad with Armijo SLS have consistently the best performance, while SGD with
SLS is often competitive. (ii) the AdaGrad and AMSGrad variants not only converge faster than
Adam and Radam but also with considerably better test performance. AdaBound has comparable
convergence in terms of training loss, but does not generalize as well. (iii) AMSGrad momentum
is consistently better than the heavy-ball (HB) variant. Moreover, we observed that HB momentum
was quite sensitive to the setting of γ, whereas AMSGrad is robust to β. In Appendix G, we include
ablation results for AMSGrad with Armijo SLS but without momentum, and conclude that momentum
does indeed improve the performance. In Appendix G, we plot the wall-clock time for the SLS variants
and verify that the performance gains justify the increase in wall-clock time per epoch. In the appendix,
we show the variation of step-size across epochs, observing a warm-up phase where the step-size
increases followed by a constant or decreasing step-size (Goyal et al., 2017).
8
Under review as a conference paper at ICLR 2021
In Appendix G, we also consider binary classification with RBF kernels for datasets from LIB-
SVM (Chang & Lin, 2011) and study the effect of over-parameterization for deep matrix factoriza-
tion (Rolinek & Martius, 2018; Vaswani et al., 2019b). We show that the same trends hold across
different datasets, deep models, deep matrix factorization, and binary classification using kernels.
Our results indicate that simply setting the correct step-size on the fly can lead to substantial empirical
gains, often more than those obtained by designing a different preconditioner. Furthermore, we
see that with an appropriate step-size adaptation, adaptive gradient methods can generalize better
than SGD. By disentangling the effect of the step-size from the preconditioner, our results show
that AdaGrad has good empirical performance, contradicting common knowledge. Moreover, our
techniques are orthogonal to designing better preconditioners and can be used with other adaptive
gradient or even second-order methods.
6 Discussion
When training over-parameterized models in the interpolation setting, we showed that for smooth,
convex functions, constant step-size variants of both AdaGrad and AMSGrad are guaranteed to
converge to the minimizer at O(1/T) rates. We proposed to use stochastic line-search techniques
to help these methods adapt to the function’s local smoothness, alleviating the need to tune their
step-size and resulting in consistent empirical improvements across tasks. Although adaptive gradient
methods outperform SGD in practice, their convergence rates are worse than constant step-size SGD
and we hope to address this discrepancy in the future.
References
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. Sgd with shuffling: optimal rates without component
convexity and large epoch requirements. Advances in Neural Information Processing Systems, 33,
2020.
Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher. A new regret analysis
for adam-type algorithms. arXiv preprint arXiv:2003.09729, 2020.
Larry Armijo. Minimization of functions having lipschitz continuous first partial derivatives. Pacific
Journal Ofmathematics,16(1):1-3,1966.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B. Tsybakov. Does data interpolation contradict
statistical optimality? In The 22nd International Conference on Artificial Intelligence and Statistics,
AISTATS, 2019.
Leonard Berrada, Andrew Zisserman, and M. Pawan Kumar. Training neural networks for and by
interpolation. arXiv preprint:1906.05661, 2019.
Nicolo Cesa-Bianchi, Alex Conconi, and ClaUdio Gentile. On the generalization ability of on-line
learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.
Volkan Cevher and Bang Cong VU. On the linear convergence of the stochastic gradient method with
constant step-size. Optimization Letters, 13(5):1177-1187, 2019.
Chih-ChUng Chang and Chih-Jen Lin. LIBSVM: A library for sUpport vector machines. ACM
Transactions on Intelligent Systems and Technology, 2(3):1-27, 2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin∕libsvm.
Xiangyi Chen, Sijia LiU, RUoyU SUn, and Mingyi Hong. On the convergence of a class of Adam-
type algorithms for non-convex optimization. In 7th International Conference on Learning
Representations, ICLR, 2019.
Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. On the convergence of Adam
and AdaGrad. arXiv preprint:2003.02395, 2020.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.
9
Under review as a conference paper at ICLR 2021
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of
the heavy-ball method for convex optimization. In 2015 European control conference (ECC), pp.
310-315.IEEE, 2015.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. Adaptive computation and
machine learning. MIT press, 2016. URL http://www.deeplearningbook.org/.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training
imagenet in 1 hour. arXiv preprint:1706.02677, 2017.
Elad Hazan. Introduction to online convex oPtimization. Foundations and Trends in Optimization, 2
(3-4):157-325, 2016.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: oPtimal algorithms for
stochastic strongly-convex oPtimization. The Journal of Machine Learning Research, 15(1):
2489-2512, 2014.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth NetraPalli, and Aaron Sidford. Accelerating
stochastic gradient descent for least squares regression. In Conference On Learning Theory, COLT,
2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In 3rd International
Conference on Learning Representations, ICLR, 2015.
Kfir Y. Levy, AlP Yurtsever, and Volkan Cevher. Online adaPtive methods, universality and accelera-
tion. In Advances in Neural Information Processing Systems, NeurIPS, 2018.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaPtive
stePsizes. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS,
2019.
Tengyuan Liang and Alexander Rakhlin. Just interPolate: Kernel “ridgeless” regression can generalize.
arXiv preprint:1808.00387, 2018.
Chaoyue Liu and Mikhail Belkin. Accelerating SGD with momentum for over-Parameterized learning.
In 8th International Conference on Learning Representations, ICLR, 2020.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaPtive learning rate and beyond. In 8th International Conference on
Learning Representations, ICLR, 2020.
Sharon L Lohr. Sampling: design and analysis. Nelson Education, 2009.
Nicolas Loizou and Peter Richtarik. Linearly convergent stochastic heavy ball method for minimizing
generalization error. arXiv preprint:1710.10737, 2017.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic Polyak
steP-size for SGD: An adaPtive learning rate for fast convergence. arXiv preprint:2002.10542,
2020.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In
5th International Conference on Learning Representations, ICLR. OPenReview.net, 2017. URL
https://openreview.net/forum?id=Skq89Scxx.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. AdaPtive gradient methods with dynamic
bound of learning rate. In 7th International Conference on Learning Representations, ICLR, 2019.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The Power of interPolation: Understanding the effec-
tiveness of SGD in modern over-Parametrized learning. In Proceedings of the 35th International
Conference on Machine Learning, ICML, 2018.
Si Yi Meng, Sharan Vaswani, Issam Laradji, Mark Schmidt, and Simon Lacoste-Julien. Fast and furi-
ous convergence: Stochastic second order methods under interPolation. In The 23nd International
Conference on Artificial Intelligence and Statistics, AISTATS, 2020.
10
Under review as a conference paper at ICLR 2021
Eric Moulines and Francis R. Bach. Non-asymptotic analysis of stochastic approximation algorithms
for machine learning. In Advances in Neural Information Processing Systems, NeurIPS, 2011.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of RMSProp and AdaGrad with logarithmic
regret bounds. In Proceedings of the 34th International Conference on Machine Learning, ICML,
2017.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-
1609, 2009.
Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213,
2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, NeurIPS, 2019.
Boris T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i
Matematicheskoi Fiziki, 3(4):643-653, 1963.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In 6th
International Conference on Learning Representations, ICLR, 2018.
Michal Rolinek and Georg Martius. L4: practical loss-based stepsize adaptation for deep learning. In
Advances in Neural Information Processing Systems, NeurIPS, 2018.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition. arXiv preprint:1308.6370, 2013.
Othmane Sebbouh, Robert M Gower, and Aaron Defazio. On the convergence of the stochastic heavy
ball method. arXiv preprint arXiv:2006.07867, 2020.
Matthew Staib, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle points
with adaptive gradient methods. In Proceedings of the 36th International Conference on Machine
Learning, ICML, 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-
parameterized models and an accelerated perceptron. In The 22nd International Conference on
Artificial Intelligence and Statistics, AISTATS, 2019a.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Advances
in Neural Information Processing Systems, NeurIPS, 2019b.
Guanghui Wang, Shiyin Lu, Quan Cheng, Weiwei Tu, and Lijun Zhang. SAdam: A variant of Adam
for strongly convex functions. In 8th International Conference on Learning Representations, ICLR,
2020.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. In Proceedings of the 36th International Conference on
Machine Learning, ICML, 2019.
Xiaoxia Wu, Simon S. Du, and Rachel Ward. Global convergence of adaptive gradient methods for
an over-parameterized neural network. arXiv preprint:1902.07111, 2019.
Yuege Xie, Xiaoxia Wu, and Rachel Ward. Linear convergence of adaptive stochastic gradient descent.
In Silvia Chiappa and Roberto Calandra (eds.), The 23rd International Conference on Artificial
Intelligence and Statistics, AISTATS, volume 108 of Proceedings of Machine Learning Research,
pp. 1475-1485. PMLR, 2020.
11
Under review as a conference paper at ICLR 2021
Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. arXiv preprint:1212.5701, 2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR, 2017.
Lijun Zhang and Zhi-Hua Zhou. Stochastic approximation of smooth and strongly convex functions:
Beyond the O(1/T) convergence rate. In Conference on Learning Theory, COLT, 2019.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive
gradient methods for nonconvex optimization. arXiv preprint:1808.05671, 2018.
12
Under review as a conference paper at ICLR 2021
Supplementary material
Organization of the Appendix
A Setup and assumptions
B Line-search and Polyak step-sizes
C Proofs for AdaGrad
Step-size	Rate	Reference
Constant	O(1∕t + σ∕√τ)	Theorem 1
Conservative Lipschitz LS	θ(1∕τ + σ∕√τ)	Theorem 2
Non-conservative LS (with interpolation)	O(1∕τ)	Theorem 7
D Proofs for AMSGrad and non-decreasing preconditioners without momentum
Constant	O(1∕τ + σ2)	Theorem 8
Armijo LS	O(1∕τ + σ2)	Theorem 4
AMSGrad with momentum		
Constant	O(1∕τ + σ2)	Theorem 3
Conservative Armijo LS	O(1∕τ + σ2)	Theorem 10
Conservative Armijo SPS	θ(1∕τ + σ2)	Theorem 5
Proofs for AMSGrad with heavy ball momentum
Constant	O(1∕τ + σ2)	Theorem 11
Conservative Armijo LS	O(1∕τ + σ2)	Theorem 13
Conservative Armijo SPS	θ(1∕τ + σ2)	Theorem 12
F Experimental details
G Additional experimental results
13
Under review as a conference paper at ICLR 2021
Table 3: Summary of notation
Concept	Symbol	Concept	Symbol
Iteration counter, maximum	k, T	General preconditioner	Ak
Iterates, minimum	* Wk,W*	Preconditioner bounds	[amin, amax]
Step-size	ηk	Maximum smoothness	Lmax
Function value, minimum	f(W),f*	Dimensionality	d
Stoch. function value, minimum	fi(W),fi*	Diameter bound	D
		Variance	σ2 = Ei[fi(W*) —fi*]
A Setup and assumptions
We restate the main notation in Table 3. We now restate the main assumptions required for our
theoretical results
We assume our objective f : Rd → R has a finite-sum structure,
1n
f (w) = - Efi(W),	⑷
ni=1
and analyze the following update, with ik selected uniformly at random,
Wk+1 = Wk — ηk A-1 mk	； mk = βmk-ι + (1 — βNfik (Wk)	(Update rule)
where ηk is either a pre-specified constant or selected on the fly. We consider AdaGrad and AMSGrad
and use the fact that the preconditioners are non-decreasing i.e. Ak Ak-1. For AdaGrad, β = 0.
For AMSGrad, we further assume that the preconditioners remain bounded with eigenvalues in the
range [amin, amax],
aminI	Ak	amaxI .	(Bounded preconditioner)
For all algorithms, we assume that the iterates do not diverge and remain in a ball of radius D, as
is standard in the literature on online learning (Duchi et al., 2011; Levy et al., 2018) and adaptive
gradient methods (Reddi et al., 2018),
IlWk — w*k ≤ D.	(Boundediterates)
Our main assumptions are that each individual function fi is convex, differentiable, has a finite
minimum 疗,and is Li-smooth, meaning that for all V and w,
fi(v) ≥ fi(w) — Nfi(W) w — V)，	(Individual Convexity)
fi(v) ≤ fi(W)+ Nfi(W) v — Wi + ɪ ∣∣v — w『,	(Individual Smoothness)
which also implies that f is convex and Lmax-smooth, where Lmax is the maximum smoothness
constant of the individual functions. A consequence of smoothness is the following bound on the
norm of the gradient stochastic gradients,
IVfi(W)I2 ≤ 2Lmaχ(fi(W) — fi*).
To characterize interpolation, We define the expected difference between the minimum of f, f (w*),
and the minimum of the individual functions 疗,
σ2 = E[fi(W*) — fi*] < ∞.	(Noise)
i
When interpolation is exactly satisfied, every data point can be fit exactly, such that f* = 0 and
f (w*) = 0, we have σ2 = 0.
14
Under review as a conference paper at ICLR 2021
B Line-search and Polyak step-sizes
We now give the main guarantees on the step-sizes returned by the line-search. In practice, we use a
backtracking line-search to find a step-size that satisfies the constraints, described in Algorithm 1
(Appendix F). For simplicity of presentation, here we assume the line-search returns the largest
step-size that satisfies the constraints.
When interpolation is not exactly satisfied, the procedures need to be equipped with an additional
safety mechanism; either by capping the maximum step-size by some ηmax or by ensuring non-
increasing step-sizes, ηk ≤ ηk-1. In this case, ηmax ensures that a bad iteration of the line-search
procedure does not result in divergence. When interpolation is satisfied, those conditions can be
dropped (e.g., setting ηmax → ∞) and the rate does not depend on ηmax. The line-searches depend
on a parameter c ∈ (0, 1) that controls how much decrease is necessary to accept a step (larger c
means more decrease is demanded).
Assuming the Lipschitz and Armijo line-searches select the largest η such that
fi(w - nVfi(w)) ≤ fi(w) - Cn ∣∣Vfi(w)k2, η ≤ nmax,	(Lipschitz line-search)
fi(w - nA-1Vfi(w)) ≤ fi(w) - cn IlVfi(W)∣A-ι, n ≤ nmax, (Armijo line-search)
the following lemma holds.
Lemma 1 (Line-search). If f is Li-smooth, the Lipschitz and Armijo lines-Searches ensure
n kVfi(w)k2 ≤ C(fi(w)-fi*), n ∣Vfi(w)kA-ι ≤ C(fi(w) - /H,	2(1 - cf∣ and	min S nmax, —L— ≤ ≤ n ≤ nmax, 2 λmin( A)(I-C)Ir / and min S nmax, 	L	 ≤ ≤ n ≤ nmax∙
We do not include the backtracking line-search parameters in the analysis for simplicity, as the same
bounds hold, up to some constant. With a backtracking line-search, we start with a large enough
candidate step-size and multiply it by some constant γ < 1 until the Lipschitz or Armijo line-search
condition is satisfied. If n0 was a proposal step-size that did not satisfy the constraint, but γn0 does,
the maximum step-size n that satisfies the constraint must be in the range γn0 ≤ n < n0 .
Proof of Lemma 1. Recall that if fi is Li-smooth, then for an arbitrary direction d,
fi(w - d) ≤ fi(w) - hVfi(w), d + L ∣d∣2 .
For the Lipschitz line-search, d= nVfi (w). The smoothness and the line-search condition are then
Smoothness:	fi(w - nVfi(w)) - fi(w) ≤ (Lη2 - n) IlVfi(W)『,
Line-search:	fi(w - nVfi (w)) - fi(w) ≤ -Cn ∣Vfi(w)∣2 .
As illustrated in Fig. 3, the line-search condition
is looser than smoothness if
(Ln2 - n) kVfi(w)k2 ≤-cn kVfi(w)k2.
The inequality is satisfied for any n ∈ [a, b],
where a, b are values of n that satisfy the equa-
tion with equality, a = 0, b = 2(1-c)/Li, and the
line-search condition holds for n ≤ 2(1-c)/Li.
Figure 3: Sketch of the line-search inequalities.
As the line-search selects the largest feasible step-size, n ≥ 2(1-c)/Li. If the step-size is capped at
nmax, we have n ≥ min{nmax, 2(1-c)/Li}, and the proof for the Lipschitz line-search is complete.
The proof for the Armijo line-search is identical except for the smoothness property, which is modified
15
Under review as a conference paper at ICLR 2021
to use the |卜|匕-norm for the direction d = ηA-`Vfi(w)
fi(w - ηA-1Vfi(w)) ≤ fi(w) - ηhVfi(w),A-1Vfi(w)i + L2iη2 ∣∣A-1Vfi(w)∣∣2,
≤ fi(W) — η kvfi(W)kAT + 2λ Li(A)η2 kVfi(W)kA-ι,
=fi(W)+(xτ-Li	η2 — η) kvfi(W)kA-ι,
2λmin(A)
where the second inequality comes from ∣∣A-1Vfi(W)k2 ≤ 入 ：a) kVfi (W)kA-1 .
□
Similarly, the stochastic Polyak step-sizes (SPS) for fi at W are defined as
SPS: η=min(cfVf-fh,ηmax)，ArmijOSPS: η=min(c∣∣V"-ι'ax)，
where the parameter c > 0 controls the scaling of the step (larger c means smaller steps).
Lemma 2 (SPS guarantees). If f is Li-smooth, SPS and Armijo SPS ensure that
SPS:	η kVfi(W)k2 ≤ C(fi(W)- fi),	min 卜 max, 2⅛i} ≤ η ≤ ηmax
Armijo SPS: η ∣∣Vfi(W)kA-ι ≤ C(fi(W) - f*),	min {ηmaχ, ⅜CLA} ≤ η ≤ ηmax
Proof of Lemma 2. The first guarantee follows directly from the definition of the step-size. For SPS,
η kVfi(W)『=min{ c≡⅛ " kVfi(W)『,
min{ fi (Wc- fi* ,ηmaχ kVfi(W)k2j ≤ C (fi(W) - f?).
The same inequalities hold for Armijo SPS with ∣Vfi(W)∣2A-1 . To lower-bound the step-size, we
use the Li-smoothness of fi, which implies %(w) — fi ≥ 击 ∣∣Vfi(W)k2.For SPS,
fi(W)-fi*、击 kVfi(W)k2 _	1
----------TT ≥ ---------— = ---.
C kVfi(W)k2 — C kVfi(W)k2	2cLi
For Armijo SPS, we additionally use kVfi(W)kA- ≤ λ .1(A) kVfi(W)k2,
fi(W) - fi*	≥	2⅛ kVfi(W)『	=λmin(A)	口
c kVfi(W)kA-ι - cλ-1A7 kVfi(W)k2	2cLi .
16
Under review as a conference paper at ICLR 2021
C Proofs for AdaGrad
We now move to the proof of the convergence of AdaGrad in the smooth setting with a constant
step-size (Theorem 1) and the conservative Lipschitz line-search (Theorem 2). We first give a rate
for an arbitrary step-size ηk in the range [ηmin , ηmax], and derive the rates of Theorems 1 and 2 by
specializing the range to a constant step-size or line-search.
Proposition 1 (AdaGrad with non-increasing step-sizes). Assuming (i) convexity and (ii) LmaX-
smoothness of each f, and (iii) bounded iterates, AdaGrad with non-increasing (ηk ≤ ηk-ι),
bounded step-sizes (ηk ∈ [ηmin, ηmaχ]), and uniform averaging WT = T PT=IWk, converges at a
rate
Ef(wτ)- f *] ≤ α+yασ,
T	√T
where α
)2
+ 2ηmax I
ιaχ.
We first use the above result to prove Theorems 1 and 2. The proof of Theorem 1 is immediate by
plugging η = ηmin = ηmaX in Proposition 1. We recall its statement;
Theorem 1 (Constant step-size AdaGrad). Assuming (i) convexity and (ii) LmaX-
smoothness ofeach f, and (iii) bounded iterates, AdaGrad with a constant step-size η
and uniform averaging such that WT = T PT=I Wk, converges at a rate
E[f(wTT*] ≤ T+√√Tσ, whereα=2 (D+2η) dLmaχ.
For Theorem 2, we use the properties of the conservative Lipschitz line-search. We recall its statement;
Theorem 2. Under the same assumptions as Theorem 1, AdaGrad with a conservative
Lipschitz line-search with C = 1/2, a step-size upper bound ηmaχ and uniform averaging
converges at a rate
Ef(wT)-f *] ≤ α+罢,
T √T
where α
LmaX
+ 2 ηmaχ)2 dLmaχ.
Proof of Theorem 2. Using Lemma 1, there is a step-size ηk that satisfies the Lipschitz line-search
with ηk ≥ 2 (1-c)/Lmax. Setting c = 1/2 and using a maximum step-size ηmaX, we have
mm1,；
LmaX
≤ ηk ≤ ηmaX ,
Before going into the proof of Proposition 1, we recall some standard lemmas from the adaptive
gradient literature (Theorem 7 & Lemma 10 in (Duchi et al., 2011), Lemma 5.15 & 5.16 in (Hazan,
2016)), and a useful quadratic inequality (Levy et al., 2018, Part of Theorem 4.2)). We include proofs
in Appendix C.1 for completeness.
Lemma 3. If the preconditioners are non-decreasing (Ak 占 Ak-ι), the step-sizes are non-
increasing (ηk ≤ ηk-ι), and the iterates Stay within a ball of radius D ofthe minima,
PT=IkWk - w*k2^Ak-—Ak-1 ≤ DTr(AT).
ηk k ηk-ι k 1	T
Lemma 4. For AdaGrad, Ak = [pk=ι ▽% (Wk Rfik (Wk)>] / and satisfies,
PT=IkVfik(Wk)kA-ι ≤ 2Tr(AT),	Tr(AT) ≤，dPT=IkVfik(Wk)『.
17
Under review as a conference paper at ICLR 2021
Lemma 5. If x2 ≤ a(x + b) for a ≥ 0 and b ≥ 0,
X ≤ ɪ (Pa2 + 4ab + a) ≤ a + √0b.
We now prove Proposition 1.
Proof of Proposition 1. We first give an overview of the main steps. Using the definition of the update
rule, along with Lemmas 3 and 4, we will show that
2 Pk=IEfik (wk ), Wk- w*i ≤ (η~~ + 2ηmaχ) Tr(AT )∙	(5)
Using the definition of AT , individual smoothness and convexity, we then show that for a constant a,
PT=1 E[f(wk)- f *] ≤ a( E ]qPT=ιfik (wk)-% (w*)1 + Tσ2),	⑹
Using the quadratic inequality (Lemma 5), averaging and using Jensen’s inequality finishes the proof.
To derive Eq.(5), We start with the Update rule, measuring distances to W in the ∣∣∙∣∣ a^ norm,
kwk+ι - w*kAk = Ilwk - w*kAk - 2ηkhvfik(Wk),wk- w*i + ηk l∣vfik(Wk)kA-ι.
Dividing by ηk , reorganizing the equation and summing across iterations yields
TT
2 XhVfik(Wk), wk- w*i ≤ X kwk-
k=1	k=1
T
≤ X kwk -
w*k7 Ai - A-ι
ηk ηk-ι
w*k7 Ai - A-I
ηk ηk-ι
T
+ Xη kvfik(wk)kA-ι,
k=1
T
+η
maχ	kvfik(wk)k2A-1 .
k
k=1
We use the Lemmas 3, 4 to bound the RHS by the trace of the last preconditioner,
≤ ---Tr(AT) +2ηmaχTr(AT),
ηT
≤ (-----+ 2ηmax ) Tr(AT).
ηmin
(Lemmas 3 and 4)
(ηk ≥ ηmin)
To derive Eq. (6), we bound the trace of AT using Lemma 4 and Individual Smoothness,
Tr(AT) ≤ √d JPT=I kVfik (wk )k2,	(Lemma 4, Trace bound)
≤ √2dLmaχ JPT=I fik (wk) — fik.	(Individual Smoothness)
≤ √≡mxqpT=ι h (wk) - h (w*)+h (w*) - f/	(±fik (w*))
Combining the above inequalities with δik = fik (w*) — fik and a = 1 (Dn + 2ηmaχ)√2dL~0x,
PT=IhVfik (wk ),wk - w*i ≤ a /PT=i fik (wk )-fik (w*) + 限.
Using Individual Convexity and taking expectations,
Pk=I E[f(wk) - f *] ≤ a E [ JPT=Ifik (wk)- fik (w*)+ δik
≤ a^EhPTLi fik(wk) - fik(wi) + δiki.
(Jensen’s inequality)
Letting σ2 := Ei[δi] = Ei[fi(wi) - fii] and taking the square on both sides yields
XT E[f(wk) - fi]! ≤ a2
T
E X fik(wk) - fik (wi)	+ Tσ2 .
k=1
18
Under review as a conference paper at ICLR 2021
The quadratic bound (Lemma 5) x2 ≤ α(x + β) implies X ≤ α + √0β, with
T
X = X E[f(wk) - f *],
k=1
α
2
+ 2ηmax dLmax ,
β=Tσ2,
gives the first bound below. Averaging WT = T PT=Iwk and using Jensen,s inequality give the result;
T
X E[f(wk)- f *] ≤ α + pαβ
k=1
E[f(wT) - f *]≤ α + 与
TT
□
19
Under review as a conference paper at ICLR 2021
C.1 Proofs of adaptive gradient lemmas
For completeness, we give proofs for the lemmas used in the previous section. We restate them here;
Lemma 3. Ifthepreconditioners are non-decreasing (Ak 占 Ak-ι), the step-sizes are
non-increasing (ηk ≤ ηk-ι), and the iterates stay within a ball ofradius D ofthe minima,
PT=I kwk - w*k2^Ak--ɪAk-ι ≤ DTr(AT).
ηk k ηk-ι k 1	T
Proof of Lemma 3. Under the assumptions that Ak is non-decreasing and ηk is non-increasing,
ɪAk------ Ak-ι > 0, so We can use the Bounded iterates assumption to bound
ηη
PT=Ikwk- w*kAk - Ak-I ≤ PT=I λmax(Ak - Ak-I) kwk - w*k2
≤ D2PkT=1 λmax
We then upper-bound λmax by the trace and use the linearity of the trace to telescope the sum,
≤ D2 PT=1 Tr(A - ⅛⅛) = D2 PT=1 Tr(Ak)-T「(M)，
=D2(Tr(AT) - Tr(A0)) ≤ D2ηTTr(AT).	□
Lemma 4. For AdaGrad, Ak = [Pk=ι Pfik (wk) Vfifc (wk)>] / and satisfies,
PT=1 kVfik(wk)kA-ι ≤ 2Tr(AT),	Tr(AT) ≤ √dPT=ι kVfik(wk)k2∙
Proof of Lemma 4. For ease of notation, let Vk := Vfik (wk). By induction, starting With T = 1,
∣∣Vfiι (w1)kA-1 = V>A-1V1 = Tr(V>A-1V1) = Tr(A-1V1V>), (Cyclic property of trace)
=Tr(A-IA2) = Tr(Aι).	(Ai = (VιV>)1/2)
Suppose that it holds for T - 1, PkT=-11 kVkk2A-1 ≤ 2Tr(AT-1). We Will shoW that it also holds for
k
T . Using the definition of the preconditioner and the cyclic property of the trace,
PkT=1 kVfik (wk)k2A-1 ≤ 2Tr(AT-1) + kVT k2A-1	(Induction hypothesis)
kT
=2Tr((AT - VtVT)1/2) + Tr(ATIVTVT)	(AdaGrad update)
We then use the fact that for any X Y	0, We have (Duchi et al., 2011, Lemma 8)
2Tr(X - Y)1/2) + TrX-1/2Y ) ≤ 2TrX1/2).
As X = A2T	Y = VT VT>	0, We can use the above inequality and the induction holds for T .
For the trace bound, recall that AT = G1T/2 Where GT = PiT=1 Vfik (wk)Vfik (wk)>. We use
Jensen’s inequality,
Tr(Aτ) = Tr(GT2) = Pdj=I PjW = d(d Pj=I √λjk),
≤ dqd Pd=i λj(GT) = √dpTr(GT).
To finish the proof, We use the definition of GT and the linearity of the trace to get
PTr(GT) = JTr(PT=ι VkVkT) = √PT=1 Tr(VkVk>) = ,PT=i Mk2.	□
20
Under review as a conference paper at ICLR 2021
Lemma 5. If x2 ≤ a(x + b) for a ≥ 0 and b ≥ 0,
1
X ≤ -
一2
(pa2 +4ab + a
≤ a + √ob.
Proof of Lemma 5. The starting point is the quadratic inequality x2 - ax - ab ≤ 0. Letting r1 ≤ r2
be the roots of the quadratic, the inequality holds if x ∈ [r1, r2]. The upper bound is then given by
using √a + b ≤ √α + Vb
a + ʌ/a2 + 4ab < a + ʌ/o2 + 44ab	+
□
C.2 Regret bound for AdaGrad under interpolation
In the online convex optimization framework, we consider a sequence of functions fk|kT=1, chosen
potentially adversarially by the environment. The aim of the learner is to output a series of strategies
wk|kT=1 before seeing the function fk. After choosing wk, the learner suffers the loss fk(wk) and
observes the corresponding gradient vector Vfk (Wk). They suffer an instantaneous regret rk =
fk (wk ) - fk (w) compared to a fixed strategy w. The aim is to bound the cumulative regret,
T
R(T) = Xfk (Wk)- fk (w*)]
k=1
where w* = arg min PT=I fk(w) is the best strategy if We had access to the entire sequence
of functions in hindsight. Assuming the functions are convex but non-smooth, AdaGrad obtains
an O(1∕√T) regret bound (DUChi et al., 2011). For online convex optimization, the interpolation
assumption implies that the learner model is powerful enough to fit the entire sequence of functions.
For large over-parameterized models like neural networks, where the number of parameters is of the
order of millions, this is a reasonable assumption for large T.
We first recall the update of AdaGrad, at iteration k, the learner decides to play the strategy Wk,
suffers loss fk(Wk) and uses the gradient feedback Vfk(Wk) to update their strategy as
Wk+1 = Wk - ηAk-1Vfk(Wk),
where Ak
hPik=1Vfk(Wk)Vfk(Wk)>i1/2 .
Now we show that for smooth, convex functions under the interpolation assumption, AdaGrad with a
constant step-size can result in constant regret.
Theorem 6. For a sequence of LmaX-smooth, Convexfunctions fk, assuming the iterates remain
boundeds.t.for all k, ∣∣Wk - w* k ≤ D, AdaGradwith a constant step-size η achieves thefolloWing
regret bound,
R(T) ≤ 2
η + 2η) dLmax + j2 (D2 η + 2η) dLmaxσ2 √T
where σ2 is an upper-bound on fk (w*) 一 f*
Observe that σ2 is the degree to which interpolation is violated, and if σ2 = 0, R(T) = O(√T)
matching the regret of (Duchi et al., 2011). However, when interpolation is exactly satisfied, σ2 = 0,
and R(T) = O(1).
Proof of Theorem 6. The proof follows that of Proposition 1 which is inspired from (Levy et al.,
2018). For convenience, we repeat the basic steps. Measuring distances to w* in the ∣∣∙∣∣ a^ norm,
kwk+1 - w*k2Ak = kwk - w*k2Ak - 2ηhVfk(wk),wk - w*i + η2 kVfk(wk)k2A-1 .
21
Under review as a conference paper at ICLR 2021
Dividing by 2η, reorganizing the equation and summing across iterations yields
TT
XhVfk(Wk),Wk - w*i ≤ X IlWk- W*k}Ak Ak-1
k = 1	k = 1	1万-F
T
+η χ kvfk (Wk)kA-i.
2k
k=1
By convexity of fk ,Rfk (Wk), Wk - w*i ≥ fk (Wk) - fk (w*). Using the definition of regret,
TT
R(T) ≤ X kWk - W*k j 皴-Ak-1) + 2 X IVfk (Wk )kA-ι.
k=1	2η	2η	k=1
We use the Lemmas 3, 4 to bound the RHS by the trace of the last preconditioner,
R(T) ≤ (D + η) Tr(AT).
We now bound the trace of AT using Lemma 4 and Individual Smoothness,
Tr(AT) ≤ √d JPT=IkVfk (Wk )『,	(Lemma 4, Trace bound)
≤ √2dLmaχ∖JPT=1 fk(Wk) - fk,	(Individual Smoothness)
≤ √2dLmαxqpl=f⅛T-fk(Wky+fk(Wk)-/*,	(±fk 2)
≤ p2dLmaχPR(T) + σ2T.	(Since fk(w*) - fk ≤ σ2)
Plugging this back into the regret bound,
R(T) ≤ (∣2 + η) P2dLmaχpR(T)+ σ2T ].
Squaring both sides and denoting a = ^Dn + η) √2dLmaχ,
[R(T)]2 ≤ a2 [R(T) +σ2T].
Using the quadratic bound (Lemma 5) x2 ≤ α(x + β) implies X ≤ α + √0β, with
x = R(T),
α = 2 (D2 η + 2η) dLmax,
β=σ2T,
yields the bound,
R(T) ≤ α + Pae = 2 (D2 η + 2η) dLmax + 22 (D2 η + 2η^ dLmaχσ2T.
□
22
Under review as a conference paper at ICLR 2021
C.3 With interpolation, without conservative line-searches
In this section, we show that the conservative constraint ηk+1 ≤ ηk is not necessary if interpolation
is satisfied. We give the proof for the Armijo line-search, that has better empirical performance,
but a worse theoretical dependence on the problem’s constants. For the theorem below, amin is
lower-bounded by in practice. A similar proof also works for the Lipschitz line-search.
Theorem 7 (AdaGrad with Armijo line-search under interpolation). Under the same assumptions
of Proposition 1, but without non-increasing step-sizes, ifinterpolation is satisfied, AdaGrad with
the Armijo line-search and uniform averaging converges at the rate,
E[f(Wτ) — f*] ≤
(D2 + 2ηmax) dLmax
2T
max ʃ ɪ ,Lmax )Y
ηmax amin J
where amin = mink {λmin (Ak)}.
Proof of Theorem 7. Following the proof of Proposition 1,
TT
2Xηk"fik(Wk),wk - w*i = X Ilwk - w*kAk - kwk+ι - w*kAk + η2 l∣vfifc(Wk)IlA-ι.
k=1	k=1	k
On the left-hand side, We use individual convexity and interpolation, which implies fik (w*) =
minw fik (W) and we can bound ηk by ηmin, giving
ηk Wfik (Wk ),Wk - W*i ≥ ηk (fik (Wk ) - fik (w*)) ≥ ηmin(fik (Wk ) - fik (w*)).
X------V-------}
≥0
On the right-hand side, we can apply the AdaGrad lemmas (Lemma 4)
T
X kWk - W*kAk - kWk+i - w*kAk + nlaxl—fik(Wk)kA-1,
k=1
≤ D2Tr(AT) + 2ηm2 axTr(AT),	(By Lemmas 3 and 4)
≤ (D2 + 2η2mχ) √d JPT=IIl Vfik (Wk)k2,	(By the trace bound of Lemma 4)
≤(D2+2ηmaχ)√≡m:χ,PT=Ifik (Wk)- fik (w*).
(By Individual Smoothness and interpolation)
Defining a =哥一 (D2 + 2η2mχ) √2dLmaχ and combining the previous inequalities yields
τ	____________________
X(fik(Wk) - fik(w*)) ≤ a√∑T=1 fik(Wk) - fik(w*).
k=1
Taking expectations and applying Jensen’s inequality yields
PT=1 E[f (Wk) - f(w*)] ≤ a√∑T=ι E[f (wk) - f (w*)].
Squaring both sides, dividing by PT=I E[f (Wk) - f (w*)], followed by dividing by T and applying
Jensen’s inequality,
*	a2	(D2 + 2η2maχ) dLmax
Ef(WT)- f (w)] ≤ T = —2ηminT—.
Using the Armijo line-search guarantee (Lemma 1) with c = 1/2 and a maximum step-size ηmaχ ,
amin
ηmin =min ηmax,	,
Lmaχ
where amin = mink{λmin(Ak)}, giving the rate
(D2 +2ηmaχ)2dLmaχ	1	Lmaχ U 2
Ef(WT)- f(W)] ≤ —^T— I maxη~—,-^-ʃ).	□
23
Under review as a conference paper at ICLR 2021
D Proofs for AM S Grad and non-decreasing preconditioners
WITHOUT MOMENTUM
We now give the proofs for AMSGrad and general bounded, non-decreasing preconditioners in the
smooth setting, using a constant step-size (Theorem 8) and the Armijo line-search (Theorem 4). As
in Appendix C, we prove a general proposition and specialize it for each of the theorems;
Proposition 2. In addition to assumptions of Theorem 1, assume that (iv) the preconditioners are
non-decreasing and have (v) bounded eigenvalues in the [amin, amax] range. Ifthe step-sizes are
constrained to lie in the range [ηmin, ηmaχ] and satisfy
ηk l∣Vfik (wk)kA-ι ≤ M (fik (Wk) - fik),	for some M < 2,	⑺
k
using uniform averaging WT = T PT=I Wk leads to the rate
E[f (Wt) - f *] ≤
1	DNdamax + ( 2	ηmax	l)σ?
T (2 - MMmin 2 - M nmin )
Theorem 8. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing precondition-
ers (v) bounded eigenVaIUeS in the [amin, amax] interval, AMSGrad with no momentum, COnStant
step-size η = 2Lmn- and uniform averaging converges at a rate,
*	2D2damax LmaX	2
E[f (wt) 一 f ] ≤ --------T-----+ σ .
amin T
Proof of Theorem 8. Using Bounded preconditioner and Individual Smoothness, we have that
kVfik(Wk)kA-ι ≤ ɪ kVfik(Wk)k2 ≤ 2Lmax(fik(Wk)-fk).
k	amin	amin
A constant step-size ηmaχ = ηmin = 2Lmin satisfies the step-size assumption (Eq. 7) with M = 1 and
2Lmax
ηmax
M ηmin
ɪ D2damax	2 _2
T (2 - M)ηmin + 2-
2	1 2LmaxD2damax
σ =--------------
T	amin
+ σ2 .
□
We restate Theorem 4;
Theorem 4. Under the same assumptions as Theorem 1, AMSGrad with zero momentum,
Armijo line-search with C = 3∕4, a step-size upper bound ηmaχ and uniform averaging
converges at a rate,
3D2d ∙ amax	q∖
E[f (Wt) - f *] ≤ ( -2T^ax + 3ηmaxσ2)
max
2L
max
amin
Proof of Theorem 4. For the Armijo line-search, Lemma 1 guarantees that
η kvfik (Wk )kA-i	≤	1(fik (Wk ) -	fik ),	and	min η ηmax,	2 λmin (Ak )	(I_C)	∖ ≤ η ≤ ηmax.
k c	Lmax
Selecting C = 3∕4 gives M = 4∕3 and ηmin = min ηmax,
1 D2damax	(	2	ηmax	]) 2
T (2 - M)ηmin + 12 - M ηmin - 厂
≤
)
amin
2Lmax
so
1	D2damax +
T (2 -4AMmin
-1
-1
σ2
1 3D2damax + ( 3ηmax
T 2ηmin	ηmin
3D2damax
2T
max
σ2
+ 3ηmaxσ2 max
2Lmax
amin
□
24
Under review as a conference paper at ICLR 2021
Theorem 9. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing precondition-
ers (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with no momentum, Armijo
SPS with C = 3/4 and uniform averaging converges at a rate,
E[f (WT) - f *] ≤ (-2T max + 3ηmaxσ2)
max「
ηmax
OLmaX
2amin
Proof of Theorem 5. For Armijo SPS, Lemma 2 guarantees that
ηk l∣vfik(wk)kA-ι ≤ 1(fik(Wk) - fik),	and	min [ηmax, amin ] ≤ η ≤ ηmaχ.
k c	2c Lmax
Selecting c = 3/4 gives M = 4/3 and ηmin
min", 32Lmax}
so
1	D2damax +(	2	ηmax	1∖ 2
T (2 - M)ηmin	2 - M ηmin	/
1 U damax +/	2	ηmax
T (2 - 4∕3)ηmin	2 - 4/3 ηmin
-1
σ2
1 3DNdamax + ( 3ηmax
T	2ηmin	I ηmin
-1
σ2
3DNdamax
-2t-
max]，
ηmax
max
2amin
+ 3ηmaxσ2 max
3Lmax
2amin
≤
□
Before diving into the proof of Proposition 2, we prove the following lemma to handle terms of the
form ηk (fik (wk) 一 fik (w*)). If ηk depends on the function sampled at the current iteration,力广,as
in the case of line-search, we cannot take expectations as the terms are not independent. Lemma 6
bounds ηk(fik (wk) — fik (w*)) in terms of the range [ηmin, ηmax];
Lemma 6. If 0 ≤ nm® ≤ η ≤ ηmax and the minimum value of f is fi, then
η(fi(w) — fi(w*)) ≥ ηmin(fi(w) — fi(w*)) - (ηmax — ηmin)(fi (w* ) - f：).
Proofof Lemma 6. By adding and subtracting fi∖ the minimum value of fi, We get a non-negative
and a non-positive term multiplied by η. We can use the bounds η ≥ ηmin and η ≤ ηmax separately;
η[fi(W)- fi(W*)] = η[fi(W)- fi + fi - fi(W*)],
`-------------------------V----} `-----{-----}
≥0	≤0
≥ ηmin[fi(w) - f*]+ ηmax[fi* - fi (w* )].
Adding and subtracting ηminfi(w*) finishes the proof,
= ηmin [fi (w) - fi(w*) + fi(w*) - fi*] +ηmax[fi* - fi(w*)],
=ηmin[fi(w) - fi(w*)] + (ηmax - ηmin)[f* - fi(w*)].	□
Proofof Proposition 2. We start with the Update rule, measuring distances to w* in the ∣∣∙∣Afc norm,
kwk+ι - w*kAk = kwk - w*kAk - 2ηkhvfik(Wk),wk - w*i + η2 l∣vfik(Wk)kA-ι	⑻
To bound the RHS, we use the assumption on the step-sizes (Eq. (7)) and Individual Convexity,
-2ηkEfik(Wk),wk - w*i + η2 l∣vfik(Wk)kA-1,
k
≤ -2ηkEfik(wk),Wk - w*i + Mηk f (Wk) - f：),	(Step-size assumption, Eq. (7))
≤ -2ηk[fik(wk) - fik(w*)] + Mηk f (Wk)- f：),	(Individual Convexity)
≤ -2ηk[fik (Wk) - fik (w*)] + Mηk(fik (Wk)- fik (w*) + fik(w*) - fi*J,	(±fik(w*))
≤ -(2 - M)ηk [fik (wk) - fik (w*)] + Mηmax(fik (w*) - f：).	(ηk ≤ ηmax)
25
Under review as a conference paper at ICLR 2021
Plugging the inequality back into Eq. (8) and reorganizing the terms yields
(2 - M)ηk[fik(Wk)- fik(W*)] ≤ (kwk - w*kAk - kwk+ι - w*k
+ Mnmax(fik (W*)- fik)
2
Ak
(9)
Using Lemma 6, we have that
(2 - M )nk [fik (Wk)- fik (w* )] ≥ (2 - M )nmin(fik (Wk ) - fik (w*))
-(2 - M )(nmax - nmin)(fik (W*) - fiζ) ∙
Using this inequality in Eq. (9), we have that
(2 - M)nmin(fik (Wk)- fik (w* )) - (2 - M)(nmax - "min)(fik (w*) - fi1)
≤ (kWk - W*kAk - kWk+1 - W*kAk) + Mnmax(fik (W*) - fik),
Moving the terms depending on fik (w*) - fik to the RHS,
(2 - M)nmin(fik(Wk) - fik(W*)) ≤ kWk - W*k2Ak - kWk+1 -W*k2Ak
+ (2nmax - (2 - M)nmin)(fik (w*) - fij
Taking expectations and summing across iterations yields
TT
(2 - M)nmin X E[fik (Wk) - fik(W*)] ≤E X kWk - W*k2Ak - kWk+1 - W*k2Ak
k=1	k=1
+(2nmax - (2 - M)nmin)Tσ2.
Using Lemma 3 to telescope the distances and using the Bounded preconditioner,
T
X kWk - W*k2Ak - kWk+1 - W*k2Ak
k=1
T
≤ X kWk - W*k2Ak-Ak-1 ≤ D2 Tr(AT) ≤ D2 damax,
which guarantees that
T
(2 - M)nmin X E[f (Wk) - f(W*)] ≤D2damax + (2nmax - (2 - M)nmin)T σ2.
k=1
Dividing by T(2 - M)nmin and using Jensen’s inequality finishes the proof, giving the rate for the
averaged iterate,
E[f(Wτ) - f (w*)] ≤
1	DNdamax
T (2 - M)nmin
nmax
2	- M nmin
-1
σ2.
+
2
□
26
Under review as a conference paper at ICLR 2021
E AMSGrad with momentum
We first show the relation between the AMSGrad momentum and heavy ball momentum and then
present the proofs with AMSGrad momentum in E.2 and heavy ball momentum in E.3.
E.1 Relation between the AMSGrad update and preconditioned SGD with
heavy-ball momentum
Recall that the AMSGrad update is given as:
Wk+1 = Wk - ηk Ak-1mk ; mk = βmk-1 + (1 - β)Vfik (Wk)
Simplifying,
wk+1
wk+1
Wk - ηk A-1(βmk-i + (1- β)Vfik (Wk))
Wk - ηk(1 - β) A-1 Vfik (Wk) - ηkβA-mk-i
From the update at iteration k - 1,
Wk = Wk-1 - ηk-1 Ak--11mk-1
1
=⇒ -mk-1 = ----------Ak-I(Wk - Wk-I)
ηk-1
From the above relations,
Wk+1 = Wk - ηk(1 - β) A-I Vfik (Wk ) + B nk A-IAk-I (Wk - Wk-I)
k k	ηk-1 k
which is of the same form as
Wk+1 = Wk - ηk Ak-1 + γ(Wk - Wk-1),
the update with heavy ball momentum. The two updates are equivalent up to constants except for the
key difference that for AMSGrad, the momentum vector (Wk - Wk-1) is further preconditioned by
Ak-1Ak-1.
27
Under review as a conference paper at ICLR 2021
E.2 Proofs for AMSGrad with momentum
We now give the proofs for AMSGrad having the update.
Wk+1 = Wk - ηk A-1mk ； mk = βmk-i + (1 - β)Vf⅛ (Wk)
We analyze it in the smooth setting using a constant step-size (Theorem 3), conservative Armijo
SPS (Theorem 5) and conservative Armijo SLS (Theorem 10). As before, we abstract the common
elements to a general proposition and specialize it for each of the theorems.
Proposition 3. In addition to assumptions of Theorem 1, assume that(iv) the preconditioners are
non-decreasing and have (v) bounded eigenvalues in the [amin, amax] range. Ifthe step-sizes are
lower-bounded and non-increasing, ηmin ≤ ηk ≤ ηk-ι and satisfy
nk l∣Vfik (Wk)kA-i ≤ M(fik (Wk)-fik), for some M < 21—β,	(IO)
k	1 + β
using uniform averaging WT = T PT=I Wk leads to the rate
E[f (Wt) - f *] ≤ 占(2 - 1+βM) -1 [Ddamx + Mσ2l.
1 - β ∖	1 - β J L ηminT	J
We first show how the convergence rate of each step-size method can be derived from Proposition 3.
Theorem 3. Under the same assumptions as Theorem 1, and assuming (iv) non-
decreasing preconditioners (v) bounded eigenvalues in the [αmin, amax] interval, where
K = amax/amin, AMSGradWith β ∈ [0,1), constant step-size η = ɪ-β 2Lmn- and uniform
averaging converges at a rate,
*	1+ β V 2LmaχD2dκ	2
Ef(WT) - f ] ≤ 11-j ) —τ-----+ σ .
Proof of Theorem 3. Using Bounded preconditioner and Individual Smoothness, we have that
ηkVfik(Wk)kA-ι ≤ ηɪ kVfik(Wk)k2 ≤ η2Lmax f (Wk)- fŋ.
k	amin	amin
Using a constant step-size η = ɪ-β 2amin satisfies the requirement of Proposition 3 (Eq. (10)) with
constant M = 1+β. The convergence is then,
E[f(WT) - f(W*)] ≤ 占(2 -占M)-1[Ddamx + Mσ2,
1 - β 1 - β	ηminT
=1+ β	D2dɑmaχ + 1 - β 2
1 - β 1-β2Lmn— τ	1 + βσ ,
1 + β 2Lmax
1 + β 2 2LmaxD2dκ	2
=L)τ+σ,
with κ = amax/amin.
□
28
Under review as a conference paper at ICLR 2021
Theorem 5. Under the same assumptions of Theorem 1 and assuming (iv) non-
decreasing preconditioners (v) bounded eigenvalues in the [am®, amax] interval with
K = amax/amin, AMSGradWith β ∈ [0,1), conservative Armijo SPS with C = 1+β∕ι-β
and uniform averaging converges at a rate,
E[f (Wτ) — f *] ≤
(1+ β∖ 2LmaχD2dκ
1-β )	T-
+ σ2.
Proof of Theorem 5. For Armijo SPS, Lemma 2 guarantees that
ηk kVfik(Wk)kA-ι ≤ 1(fik(Wk) - fik),	and	2-am^ ≤ ηk.
k c	2c Lmax
Setting c = 1-β ensures that M = 1/c satisfies the requirement of Proposition 3 and ηmin ≥
1+β 2amin . Plugging in these values into Proposition 3 completes the proof.	□
Theorem 10. Under the assumptions of Theorem 1 and assuming (iv) non-decreasing precon-
ditioners (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with momentum with
parameter β ∈ [0,1∕5), conservative Armijo SLS with C = 21-β and uniform averaging converges
at a rate,
*]『1 1 + β LmaxD2dκ	2
E[f (WT) - f ] ≤ 31 — 5廿——τ——+ 3σ
Proof of Theorem 10. For Armijo SLS, Lemma 1 guarantees that
η kVfik(Wk)kA-1 ≤ 1(fik(Wk) — G	and	2(1 LC) amin ≤ ηk.
k C	Lmax
The line-search parameter C is restricted to [0, 1] and relates to the the requirement parameter M
of Proposition 3 (Eq. (10)) through M = 1/C. The combined requirements on M are then that
1 < M < 21+β, which is only feasible if β < 1. To leave room to satisfy the constraints, let β < 1.
Setting C = M = 2 1+-β satisfies the constraints and requirement for Proposition 3, and
E[f(Wτ) — f(w*)] ≤
1 + β
1-β
D damax
ηminT
+ Mσ2
占(2-3)-
Lmax
D2damax
2(1 — C) amin T
3 1 — β 2,
+ 2E σ
1+ β LmaX D2dκ
1 — β (1 - C) T
+ 3σ2
3 1+ β LmaXD2dK
31 — 5β T
+ 3σ2.
where the last step substituted 1/(1 — C),
2 1 + β 3(1 — β) — 2(1 + β)	1 1 — 5β
1 — c 1 -------------------------------------- -----------
3 1 — β	3(1 — β)	3 1 — β
□
Before diving into the proof of Proposition 3, we prove the following lemma,
Lemma 7. For any set ofvectors a,b,c,d, f a = b + c, then,
∣∣a — d『=∣∣b — d『—∣∣a — b『+ 2(c, a — d)
29
Under review as a conference paper at ICLR 2021
Proof.
∣∣α — d『= Ilb + C — d『= Ilb — d『+ 2〈c, b — d) + IlC『
Since C = a — b,
=Ilb — d『+ 2〈a — b,b — d + Ila — b『
=∣∣b	—	d『+	2(a	— b, b — a + a — d)+ ∣∣a — b『
=∣∣b	—	d『+	2(a	— b, b — a) + 2(a — b, a — d) + ∣∣a	— b『
=Ilb — d『—2 Ila — b『+2ha — b, a — d) + Ila — b『
=Ilb — d『一∣∣a — b『+2(c,a — d)
□
We now move to the proof of the main proposition. Our proof follows the structure of Reddi et al.
(2018); Alacaoglu et al. (2020).
Proofof Proposition 3. To reduce clutter, let Pk = Ak /ηk. Using the update, we have the expansion
Wk+1 — w* = (Wk — PrImk) — w*,
=(Wk — (1 — e)Pk Rfik (Wk)- βPk 1mk-1) - w*,
Measuring distances in the ∣∣∙∣pfc-norm, SUChthatkxkPk = (χ,Pkx),
ι∣wk+ι— w*ιιPk = 1wk— w*nPk— 2(I- e) hwk— w*, v/ik (Wk)),
—2β(Wk — w*,mk-i) + IlmkIIP-ι .
We separate the distance to w* from the momentum in the second inner product using the update and
Lemma 7 with a = C = Pk-I (Wk —w*), b = 0, d = Pk-1(wk-i — w*).
—2(mk-i, Wk — w*) = —2 (Pk-ι(wk-i 一 Wk), Wk — w*),
= IjlWk — Wk-IkPk-I + l∣wk — w*Hpk-1 — 1wk-ι — w*1pk-1 ],
=umk-ihP-ιι + ι∣wk—w*nPk_1 — ι∣wk-ι— w*nPk-1,
≤ Hmk-IHP-II + HWk — w*1pk -HWk-I —"嗑一，
where the last inequality uses the fact that ηk ≤ ηk-ι and Ak 占 Ak-1, which implies Pk 占 Pk-1,
and HWk — w*∣Pk7 ≤ HWk - w*∣Pfc. Plugging this inequality in and grouping terms yields
2(1 - β) (wk - w*, Vfik(wk)) ≤ [∣∣Wk - w* ∣Pk - ∣∣Wk+ι - w*∣∣Pk]
+	β[l∣wk - w*llPk - l∣wk-ι - w*HPk-1]
+	[β I∣mk-1 Il p-11 + HmkH P-1 ]
By convexity, the inner product on the left-hand-side is bounded by (Wk - w*, Vfik(Wk)) ≥
fik (wk) - fik (w*). The first two lines of the right-hand-side will telescope if we sum all iterations,
so we only need to treat the norms of the momentum terms. We introduce a free parameter δ ≥ 0,
that is only used for the analysis, and expand
β Hmk-IHP-1ι + HmkHP-1 = β Hmk-IHP-1ι + (1 + δ) HmkHP-1 - δ HmkHP-1 .
To bound Hmk HP-1, we expand it by its update and use Young,s inequality to get
HmkHP-1 = ι∣βmk-ι +(1 - β)vfik (Wk )HP-1
≤ (1 + e)β2 Hmk-iHP-1 + (1 + ι∕e)(1 - β)2 HVfik(Wk)HP-1 ,
30
Under review as a conference paper at ICLR 2021
where e > 0 is also a free parameter, introduced to control the tradeoff of the bound. Plugging this
bound in the momentum terms, we get
β kmιkP-11 + MkkP-ɪ ≤ β kmιkP-1 + (1 + e)(l + δ)β2 ∣∣mk-ikp-ɪ 一δ kmkkp「ɪ,
+ (1 + 1∕e)(1+ δ)(1- β)2kV九(Wk)kp-ɪ .
k
1	2	2
As P- 1 W Pk_11,wehave that ∣∣mk-ι∣∣p-ɪ ≤ ∣∣mk-ιkp-ɪ which implies
≤ (β + (I + e)(1 + δ)β2) IImk-I iip-ɪɪ- δ ι∣m-kp-ɪ
+ (1 + 1∕e)(1+ δ)(1- β)2 ∣Vfik (w-)kp-ɪ .
k
To get a telescoping sum, we set δ to be equal to β + (1 + e)(1 + δ)β2, which is satisfied if
δ = 1+D；2, and δ > 0 is satisfied if β < 1∕√ι+i. We now plug back the inequality
β iim--ikp-ɪɪ+ im-ip-ɪ ≤δ[imk-ikp-ɪɪ- km-ip-ɪ]
+ (1 + 1∕e)(1 + δ)(1 - β)2∣V九(w-)kp-ɪ ,
k
in the previous expression to get
2(I- β)f (Wk) - fik(W*)) ≤ Ilwk- w*∣∣pk -Ilwk+ι - w*ιιPk
+ β [kwk - w*kPfc - kwk-1 - w*kPk-J
+ δ[kmk-1kP-1] - kmk "P-1]
+ (1 + 1∕e)(1+ δ)(1 - β)2∣Vfi k(wk)kp-ɪ .
fc
All terms now telescope, except the gradient norm which we bound using the step size assumption,
IVfik(wk)∣P-ɪ = ηk ∣V%(wk)kl-ɪ ≤ Mf (Wk) - f-k),
fc	fc
=M(九(wk)-%(w*)) + Mf (w*) - fi*J.
This gives the expression
α (fi k(Wk) - fik(w*)) ≤ Ilwk - w*hPfc - kwk+ι - w*hPfc
+ β [kwk - w*lPk - kwk-1 - w*!pk-J
+ δ[kmk-1kP-ɪɪ - kmkhP-ɪ]
+ (1 + 1∕e)(1 + δ)(1 - β)2Mf (w*) - fi*J,
with Q = 2(1 - β) - (1 + 1∕e)(1 + δ)(1 - β)2M. Summing all iterations, the individual terms are
bounded by the Bounded iterates and Lemma 3;			
T X kwk -w*∣∣pk k = 1	-∣∣wk+1 - w*HPk	≤ D2Tr(Pr)	D2 ≤ "— Tr(Aτ)
T β X Ilwk- w"Pk k = 1	-∣∣wk-ι -w*HPk-ι	≤ β∣ιwτ- w*ιιPτ	≤ β1Tr(Aτ) "min
T δ X nmk-ihp-ʌ -	Tlmk ∣∣p-ι	≤δ nmdiP。	= 0.
k = 1
Using the boundedness of the preconditioners gives Tr(AT) ≤ damax and the total bound
T
α X(Jik (w- )- fik (w*)) ≤
k=1
(1 + β)D2dαmax
"min
T
+ (1 + 1∕e)(1+ δ)(1 - β)2M X(fik (w*) - fŋ.
k = 1
31
Under review as a conference paper at ICLR 2021
Taking expectations,
T
αXE[f (wk) - f (W*)] ≤
k=1
(1+ β)D2damaχ +(1 + 1∕e)(1 + δ)(ι - β)2Mσ2T.
ηmin
It remains to expand α and simplify the constants. We had defined
α = 2(1- β)-(1 + 1∕e)(1 + δ)(1- β)2M> 0, and δ = Ie + (1 + 叱 > 0,
1 - (1 + )β2
where e > 0 is a free parameter. This puts the requirement on β that β < 1/√1 + E. To simplify the
bounds, we set β = 1/(1 + e), E = 1∕β - 1, which gives the substitutions
1	11
1 + E = β	1 + E = T-β
Plugging those into the rate gives
β
2^—
1 - β
1+δ
1 + β
L.
δ
T
α XE[f(wk) - f(w*)] ≤
k=1
(1+ β)D2damaχ + (1 + β)Mσ2τ,
ηmin
while plugging them into α gives
α = 2(1 - β) - (1 + 1∕e)(1 + δ)(1 - β)2M,
=(1 - β) 2 - 1 + ： M , which is positive if M < 21 - ；.
Dividing by αT, using Jensen’s inequality and averaging finishes the proof, with the rate
XE[f (wk) - f (w*)] ≤ 占(2 -占M)-1[D⅛ + Mσ2].	□
32
Under review as a conference paper at ICLR 2021
E.3 Proofs for AMSGrad with heavy ball momentum
We now give the proofs for AMSGrad with heavy ball momentum with the update.
Wk + 1 = Wk - ηk A-IVfik (Wk) + Y (Wk - Wk-1)
We analyze it in the smooth setting using a constant step-size (Theorem 11), a conservative Armijo
SPS (Theorem 12) and conservative Armijo SLS (Theorem 13). As before, we abstract the common
elements to a general proposition and specialize it for each of the theorems.
Proposition 4. In addition to assumptions of Theorem 1, assume that(iv) the preconditioners are
non-decreasing and have (v) bounded eigenvalues in the [amin, amax] range. Ifthe step-sizes are
lower-bounded and non-increasing, ηmin ≤ ηk ≤ ηk-ι and satisfy
nk l∣Vfik (Wk)kA-ι ≤ M(fik (Wk) - KJ, for some M < 2 - 2γ,	(11)
k
AMSGrad with heavy ball momentum with parameter γ < 1 and uniform averaging WT =
T PT=1 Wk leads to the rate
E[f (Wt) - f*] ≤『，M [ 1 (2(1 + Y2)DamaXd + 2γ[f (wo) - f (w*)]) + Mσ2l .
2 - 2γ - MIT ∖	ηmin	)	」
We first show how the convergence rate of each step-size method can be derived from Proposition 4.
Theorem 11.	Under the assumptions of Theorem 1 and assuming (iv) non-decreasing precondi-
tioners (v) bounded eigenvalues in the [amin, amax] range, AMSGrad with heavy ball momentum
with parameter Y ∈ [0,1), constant step-size η = 2amL(I-Y) and uniform averaging converges at
a rate	max
E[f(WT)- f *〕≤ T (∣(j⅛ Lmax D2 κd+(⅛f (wo) - f (w*)])+2σ2.
Proof of Theorem 11. Using Bounded preconditioner and Individual Smoothness, we have that
η kVfik(Wk)kA-ι ≤ ηɪ kVfik(Wk)k2 ≤ η2Lmax f (Wk)-危.
k	amin	amin
A constant step-size η = 2amm (I-Y)/3Lmax means the requirement for Proposition 4 is satisfied with
M = 4 (1 - γ). Plugging (2 - 2γ - M) = 2(1 - γ) in Proposition 4 finishes the proof.	□
Theorem 12.	Under the assumptions of Theorem 1 and assuming (iv) non-decreasing precondi-
tioners (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with heavy ball momentum
with parameter Y ∈ [0,1), COnSerVatiVe Armijo SPS with C = 3∕4(i-γ) and uniform averaging
converges at a rate,
E[f (WT)- f *] ≤ T (2(j-⅛ LmaXD2κd + (⅛[f (W0) - f (W*)])+ 2σ2.
Proof of Theorem 12. For Armijo SPS, Lemma 2 guarantees that
ηk kVfik(Wk)kA-1 ≤ 1(fik (Wk)-鬼)，	and	言』≤ ηk.
k c	2c Lmax
Selecting c = 3/4(1-Y) gives M = 4/3(1 - Y) ≤ 2(1 - Y) and the requirement of Proposition 4 are
satisfied. The minimum step-size is then ηmin = > amin
2cLmax
2amin (1-γ)
3Lmax
, so ηmin and M are the same
as in the constant step-size case (Theorem 11) and the same rate applies.
□
Theorem 13.	Under the assumptions of Theorem 1 and assuming (iv) non-decreasing precondi-
tioners (v) bounded eigenvalues in the [amin, amax] interval, AMSGrad with heavy ball momentum
with parameter Y ∈ [0,1∕4), COnSerVatiVeArmijO SLS with C = 3∕4(i-γ) and uniform averaging
33
Under review as a conference paper at ICLR 2021
converges at a rate,
Ef(WT) - f *] ≤ T (61+γYLmaxD2κd + (⅛f (w0) - f 3]) + 2σ2
Proofof Theorem 13. Selecting C = 3∕4(i-γ) is feasible if γ < 1/4 as c < 1. The Armijo SLS
(Lemma 1) then guarantees that
η kVfik(Wk)kA-ι ≤ 1(fik (Wk) - fik),	and	2(1 Lc) amin ≤ η,
k c	Lmax
which satisfies the requirements of Proposition 4 with M = ∣ (1 - γ). Plugging M in the rate yields
E[f (Wt) - f (w*)] ≤ 1 (61+-γ2-Damaxd +	J[f (wo) - f (w*)]) + 2σ2,
T 1 - γ	ηmin	(1 - γ)
With c = 3/4-, ηmin ≥ 2(1-c)amin = 2amn 1-4γ,. Plugging it into the above bound yields
1-γ
Lmax	Lmax 4(1-γ)
E[f (WT) - f (w*)] ≤ — (6 + ; LmaxD2κd + ~7~,~ɪʒ[f(WO) - f (W*)]) + 2σ2.	□
TI 1 - 4γ	(1 - Y)
We now move to the proof of the main proposition. Our proof follows the structure of Ghadimi et al.
(2015); Sebbouh et al. (2020).
Proof of Proposition 4. Recall the update for AMSGrad with heavy-ball momentum,
Wk+1 = Wk - ηkA-1^fik (Wk) + Y(Wk - Wk-1).	(12)
The proof idea is to analyze the distance from w* to Wk and a momentum term,
I∣δkk2 = ∣∣Wk + mk - w*kAk ,	where mk =亡(Wk - Wk-ι),	(13)
by considering the momentum update (Eq. 12) as a preconditioned step on the joint iterates (Wk+mk),
Wk+1 + mk+1 = Wk + mk - 3η-γA-1Vfik(Wk).
(14)
Let us verify Eq. (14). First, expressing Wk+1 + mk+1 as a weighted difference of Wk+1 and Wk,
Wk+1 + mk+1
wk+1 + 1-γ (wk + 1 - wk ) = 1—Y wk+1 - ↑--γwk ∙
Expanding Wk+1 in terms of the update rule then gives
=1-γ (wk - ηk AkIVfik (wk ) + Y(Wk - wk-1)) - I-Y wk,
=11γ (Wk — ηk A-IVfik (Wk) — YWk-1),
=T-γ wk — T-Y wk-1 — I-Y A-IVfik (wk),
which can then be re-written as Wk + mk 一 I-YA-IVfik (Wk). The analysis of the method then
follows similar steps as the analysis without momentum. Using Eq. (14), we have the recurrence
ι∣δk+ιkAk = kwk+ι+mk+ι-w*kAk = ∣∣wk + mk-售A-IVfik(Wk)-w*∣∣a ,
2η	η2	k (15)
=kδkkAk - 1-----hVfik (wk), wk + mk - w*i + ∩~~^2 llVfik (Wk )kA-1 .
k 1 - Y	(1 - Y)2	k
To bound the inner-product, we use Individual Convexity to relate it to the optimality gap,
*	*Y
hVfik(Wk),Wk + mk - W i = hVfik(Wk),Wk - W i +  -hVfik(wk),Wk - Wk-1),
1-Y
≥ fik (Wk) - fik (w*) +	— [fik (Wk) - fik (Wk-1)],
1-Y
=7^[fik(Wk) - fik(w*)] -	— [fik(wk-1) - fik(W*)].
1-Y	1-Y
34
Under review as a conference paper at ICLR 2021
To bound the gradient norm, we use the step-size assumption that
η kVfik (Wk )kA-ι ≤ Mfik (Wk) - fik ] = M[fik (Wk) - fik (w*)] + M[fik (w*) - fik ].
For simplicity of notation, let us define the shortcuts
hk (W) = fik(W)- fik(W*),	σk = fik (Wio- fik.
Plugging those two inequalities in the recursion of Eq. (15) gives
llδk + 1kAk ≤ kδk IlAk - (1 -kγ)2 (2 - M )hk (Wk ) + (1 -kY)2 hk (WkT) + (] -7)2 σk
We can now divide by ηk∕(i-γ)2 and reorganize the inequality as
(2 - m)hk(Wk) - 2γhk(Wk-I) ≤ -一ηJ (∣∣δkIlAk -∣∣δk+ι∣∣Ak) + Mσk2.
Taking the average over all iterations, the inequality yields
1T	1T
T £(2 - M)MWk) - 2γhk(Wk-I) ≤ TE
k=1
k=1
⅛γh (kδk kAk-kδk+1kAk)+Mσ2.
To bound the right-hand side, under the assumption that the iterates are bounded by kWk - Wi k ≤ D,
we use Young’s inequality to get a bound on kδkk2;
MkL = iiWk + mk - w*i∣2 = Il 1-γ (Wk - w*) - 1-γ (Wk-I - W*)||2
≤	(ɪɪ (kWk - W*k2 + γ2	kwk-1	-	W*k2) ≤	2((ID2	=	∆2.
Given the upper bound kδkk2 ≤ ∆, a reorganization of the sum lets us apply Lemma 3 to get
PT=ι ηk (kδk kA% - kδk+ιkAj = PT=ι kδk kηu Ak- PT=Ikδk+ι∣4 Ak
=PT=1 IMkkLk- PT=21kδk k二Ak-1
ηk k	ηk-1 k-1
≤ PT=1 IMkk = Ak - PT=1 kδkk7Ak_1 + kδlk21 Ao
ηk k	ηk-1 k-1	η0 0
=PT=I Mkk2⅛Ak-/Ak-1 ≤ *,
where the last step uses the convention A0 = 0 and Lemma 3 on δk instead of Wk - Wi . Plugging
this inequality in, we get the simpler bound on the right-hand-side
ɪ XX(2 - M)hk(wk) - 2γhk(wk-i) ≤ 2(1+ ”2amaxd + Mσk.
T k=1	T ηmin
Now that the step-size is bounded deterministically, we can take the expectation on both sides to get
1
T E
- M)h(Wk) - 2γh(Wk-1)
≤ 2(1 + Y2)Dkamaxd + Mσk
,
T ηmin
where h(W) = f(W) - fi and σk = E fik (Wi ) - fii . To simplify the left-hand-side, we change
the weights on the optimality gaps to get a telescoping sum,
PkT=1(2 - M)h(Wk) - 2γh(Wk-1) = PkT=1(2 - 2γ - M)h(Wk) + 2γh(Wk) - 2γh(Wk-1),
= (2 - 2γ - M)hPkT=1 h(Wk)i + 2γ(h(WT) - h(W0)),
≥(2 - 2γ - M)hPkT=1 h(Wk)i - 2γh(W0).
35
Under review as a conference paper at ICLR 2021
The last inequality uses h(wT ) ≥ 0. Moving the initial optimality gap to the right-hand-side, we get
71(2 - 2γ — M) E XX h(wk) ≤ 1 (2(1 + Y"Damaxd +2γh(wo)) + Mσ2.
T	T	ηmin
Assuming 2 - 2γ - M > 0 and dividing, we get
T
h(wk)
k=1
1
≤--------
-2 - 2γ - M
1
T E
1( 2(1 + Y 2)D2 amaχd + 2γh(wo)) + Mσ2
T	ηmin
Using Jensen’s inequality and averaging the iterates finishes the proof.
□
36
Under review as a conference paper at ICLR 2021
F Experimental details
Our proposed adaptive gradient methods with SLS and SPS step-sizes are presented in Algorithms 1
and 3. We now make a few additional remarks on the practical use of these methods.
Algorithm 1 Adaptive methods with SLS(f, precond, β, conservative, mode, w0, ηmax, b,
c∈(0,1),Y<1)
1	for k = 0,...,T — 1 do
2	ik J sample mini-batch of size b
3	Ak J Precond(k)	. Form the preconditioner.
4	:	if mode == Lipschitz then
5	Pk J Vfik (Wk )
6	:	else if mode == Armijo then
7	:	pk J Ak-1Vfik (wk)
8	:	end if
9	:	if conservative then
10	:	if k == 0 then
11	:	ηk J ηmax
12	:	else
13	:	ηk J ηk-1
14	:	end if
15	:	else
16	:	ηk J ηmax
17	:	end if
18	while fik (wk — ηk ∙ Pk) > fik (wk) — Cnk Wfik (wk), Pki do	. Line-search loop
19	:	ηk J Y ηk
20	:	end while
21	:	mk J βmk-1 + (1 — β)Vfik (wk)
22	:	wk+1 J wk — ηkAk-1mk
23	: end for
24	: return wT
Algorithm 2 reset(η, ηmax, k, b, n, γ, opt)
1	if k = 0 then
2	:	return ηmax
3	: else if opt= 0 then
4	:	ηJη
5	: else if opt= 1 then
6	n J n ∙ Yb/n
7	: else if opt= 2 then
8	:	η J ηmax
9	: end if
10	: return η
As suggested by Vaswani et al. (2019b), the standard backtracking search can sometimes result in
step-sizes that are too small while taking bigger steps can yield faster convergence. To this end, we
adopted their strategies to reset the initial step-size at every iteration (Algorithm 2). In particular,
using reset option 0 corresponds to starting every backtracking line search from the step-size used
in the previous iteration. Since the backtracking never increases the step-size, this option enables
the “conservative step-size“ constraint for the Lipschitz line-search to be automatically satisfied.
For the Armijo line-search, we use the heuristic from Vaswani et al. (2019b) corresponding to reset
option 1. This option begins every backtracking with a slightly larger (by a factor of Ybbn Y = 2
throughout our experiments) step-size compared to the step-size at the previous iteration, and works
well consistently across our experiments. Although we do not have theoretical guarantees for Armijo
37
Under review as a conference paper at ICLR 2021
SLS with general preconditioners such as Adam, our experimental results indicate that this is in fact
a promising combination that also performs well in practice.
Algorithm 3 Adaptive methods with SPS(f, [f；]n=[, Precond, β,conservative, mode, wo,
ηmax, b, c)
1	for k = 0,...,T — 1 do
2	ik J sample mini-batch of size b
3	Ak J Precond(k)	. Form the preconditioner
4	:	if mode == Lipschitz then
5	Pk J Pfik (Wk)
6	:	else if mode == Armijo then
7	:	pk J Ak-1Pfik (wk)
8	:	end if
9	:	if conservative then
10	:	if k == 0 then
11	:	ηB J ηmax
12	:	else
13	:	ηB J ηk-1
14	:	end if
15	:	else
16	:	ηB J ηmax
17	:	end if
18	.f f% (Wk)-fik)	i ηk J mint C f (Wk),pki QB)
19	:	mk J βmk-1 + (1 — β)Pfik (wk)
20	:	wk+1 J wk — ηkAk-1mk
21	: end for
22	: return wT
On the other hand, rather than being too conservative, the step-sizes produced by SPS between
successive iterations can vary wildly such that convergence becomes unstable. Loizou et al. (2020)
suggested to use a smoothing procedure that limits the growth of the SPS from the previous iteration
to the current. We use this strategy in our experiments with τ = 2b/n and show that both SPS and
Armijo SPS work well. For the convex experiments, for both SLS and SPS, we set c = 0.5 as is
suggested by the theory. For the non-convex experiments, we observe that all values of c ∈ [0.1, 0.5]
result in reasonably good performance, but use the values suggested in Vaswani et al. (2019b); Loizou
et al. (2020), i.e. c = 0.1 for all adaptive methods using SLS and c = 0.2 for methods using SPS.
38
Under review as a conference paper at ICLR 2021
G Additional experimental results
In this section, we present additional experimental results showing the effect of the step-size for
adaptive gradient methods using a synthetic dataset (Fig. 4). We show the wall-clock times for
the optimization methods (Fig. 5). We show the variation in the step-size for the SLS methods
when training deep networks for both the CIFAR in Fig. 6 and ImageNet (Fig. 7) datasets. We
evaluate these methods on easy non-convex objectives - classification on MNIST (Fig. 8) and
deep matrix factorization (Fig. 10). We use deep matrix factorization to examine the effect of
over-parameterization on the performance of the optimization methods and check the methods’
performance when minimizing convex objectives associated with binary classification using RBF
kernels in Fig. 9. Finally in Fig. 11, we quantify the gains of incorporating momentum in AMSGrad
by comparing against the performance AMSGrad without momentum.
Margin:0.01
Margin:0.05
(6OD",SO-U一euɪ
50	100	150	200	0	50	100	150	200	50	100	150	200
Epoch	Epoch	Epoch
—Adagrad —Default Adagrad -A- Adagrad + Llpschltz LS TL Adagrad + ArmIJo LS
MarginiO.I
Margin:0.5
50	100	150	200
Epoch
(a) AdaGrad
MarqiniO.Ol	Marqin:0.05
MarqiniO.I	Margin:0.5
O 50 IOO 150	200
Epoch
O 50 IOO 150	200 O 50 IOO 150	200
Epoch	Epoch
—Amsgrad -Default Amsgrad -j⅛- Amsgrad + SLS
50 IOO 150	200
Epoch
(b) AMSGrad
(6OD SSO-U'U
Figure 4:	Effect of step-size on the performance of adaptive gradient methods for binary classification
on a linearly separable synthetic dataset with different margins. We observe that the large variance
for the adaptive gradient methods, and the variants with SLS have consistently good performance
across margins and optimizers.
39
Under review as a conference paper at ICLR 2021
LPod49ea 6u-u's∙η ωπsω>< Lpod4?"6u'e'sl ωπsω><
C∣FAR10 - ResNet34
Methods
(a)
Amsgrad + SLS
Amsgrad + SLS + HB
Adagrad + SLS
Adabound
Radam
Adam
SLS
Cifarioo - DenSeNet121
Methods
(b)
Amsgrad + SLS
Amsgrad + SLS + HB
Adagrad + SLS
Adabound
Radam
Adam
SLS
Cifarioo - ReSNet34
103.426
Ooooo
0 8 6 4 2
1
⅞0d⅛3E≈CTC-C-S ωCT≡ω><
Methods
(c)
Tiny ImageNet - ReSNet18
(d)
Amsgrad + SLS
Amsgrad + SLS + HB
Adagrad + SLS
Adabound
Radam
Adam
SLS
Amsgrad + SLS
Amsgrad + SLS + HB
Adabound
Radam
Adam
SLS
Adagrad + SLS
Figure 5:	Runtime (in seconds/epoch) for optimization methods for multi-class classification using
the deep network models in Fig. 2. Although the runtime/epoch is larger for the SLS/SPS variants,
they require fewer epochs to reach the maximum test accuracy (Figure 2). This justifies the moderate
increase in wall-clock time.
40
Under review as a conference paper at ICLR 2021
ClFARlo-ReSNet34
ClFARlo-ReSNet34
Adagrad + SLS →- Adabound
Cl FAR IO-ResNetB 4
→- SLS
50
—Radam —Adam
IO0
10-2
§ 10^4
I ιo^6
& ιo-8
ω
IO-10.
10-i2
200	0
50	100	150	200
Epoch
Amsgrad + SLS Amsgrad + SLS + HB
(a)	CIFAR-10 ResNet
ClFARlO-DenseNetlZl
CIFARlO-DenseNetl?!
ClFARlO-DenseNetlZl
0 12 3
O - - -
Iooo
111
(60一So-U∙JJ.
0	50	100	150	200
Epoch
Adagrad + SLS
-→- Adabound
AUeJnbe uo=eP=e>
4 2 0 8
9 9 9 8
0	50	100	150	200
Epoch
0	50	100	150	200
Epoch
—Radam —Adam →- SLS -⅛- Amsgrad + SLS -⅛- Amsgrad + SLS + HB
(b)	CIFAR-10 DenseNet
CIFAR100-ResNet34
CIFAR100-ResNet34
CIFAR100-ResNet34
10^3
0
(60-) SSO-U's∙JJ.
0.74
>.
e
3 0.72
50	100	150
Epoch
.9 0.70
⅛
卫
⅛ 0.68
0.66
200
IO1
13 5
- - -
Ooo
111
(60D 9zds
Adagrad + SLS →- Adabound
O 50 IOO 150	200 O 50 IOO 150	200
Epoch	Epoch
—Radam —Adam →- SLS -⅛- Amsgrad + SLS -⅛- Amsgrad + SLS + HB
(c) CIFAR-100 ResNet
CIFAR100-DenseNetl21
ClFARlOO-DenseNetlZl
ClFARlOO-DenseNetlZl
0 12 3
O - - -
Iooo
111
(60一) SSO-U∙JJ.
0.66
.9 0.70
⅛
卫
通 0.68
0.74
>.
3 0.72
50	100	150	200	50	100	150	200	50	100	150	200
Epoch	Epoch	Epoch
Adagrad + SLS _Adabound —Radam —Adam →- SLS -⅛- Amsgrad + SLS Amsgrad + SLS + HB
(d) CIFAR-100 DenseNet
Figure 6: Comparing optimization methods on image classification tasks using ResNet and DenseNet
models on the CIFAR-10/100 datasets. For the SLS/SPS variants, refer to the experimental details
in Appendix F. For Adam, we did a grid-search and use the best step-size. We use the default
hyper-parameters for the other baselines. We observe the consistently good performance of AdaGrad
and AMSGrad with Armijo SLS. We also show the variation in the step-size and observe a cyclic
pattern (Loshchilov & Hutter, 2017) - an initial warmup in the learning rate followed by a decrease or
saturation to a small step-size (Goyal et al., 2017).
41
Under review as a conference paper at ICLR 2021
Imagewoof-ResNetia
Oooo
Illl
(60一) SSO-U∙JJ.
10^5
10^6
20	40	60	80	100
Epoch
0.700
0.675
& 0.650-
≡
8 0.625
c 0.600
o
® 0.575-
§ 0.550
ImageWOOf-ReSNet18
Imagewoof-ResNetia
0.525
0.500
113
Orr
loo
1 1
(6) 9zds
20	40	60	80	100
Epoch
20	40	60	80	100
Epoch
—Adabound —Radam —Adam →- SLS Amsgrad + SLS -⅛- Adagrad + SLS Amsgrad + SLS + HB
(a) Imagewoof
Imagenette-ReSNet18
Imagenette-ResNetla
Imagenette-ResNetlS
(60DSSO-U's∙JJ.
Oooo
Illl
(6) 9Zds
10°
20
40
80
00
—Adabound —Radam —Adam	—∣- SLS -A- Amsgrad + SLS -A- Adagrad + SLS -A- Amsgrad + SLS + HB
(b) ImageNette
Tiny ImageNet-ResNetlS
Tiny ImageNet-ResNetlS
Tiny ImageNet-ResNetlS
0 12 3
O - - -
Iooo
111
(60一) SSO-U∙JJ.
sV*∙→**∙*≡**K*
50	75	100 125 150 175 200
Epoch
0	50	100	150	200
Epoch
0	50	100	150	200
Epoch
—Adabound —Radam —Adam →- SLS -≠- Amsgrad + SLS Adagrad + SLS Amsgrad + SLS + HB
(c)	Tiny Imagenet
Figure 7:	Comparing optimization methods on image classification tasks using variants of ImageNet.
We use the same settings as the CIFAR datasets and observe that AdaGrad and AMSGrad with Armijo
SLS is consistently better.
10°
MNIST
12 3
- - -
Ooo
111
(60SSO-Uu
MNIST
IO0
MNIST
0.984
I 0.982
U
-I 0.980
e
5
> 0.978
0	20	40	60	80	100
Epoch
0.976
0
20	40	60	80	100
Epoch
(60-3Z_S daκ
0	20	40	60	80	100
Epoch
—Adam -Adabound Radam	—∣— SLS -j⅛- Adagrad + SLS -⅛- Amsgrad + SLS	-⅛- Amsgrad + SLS + H B
Figure 8:	Comparing optimization methods on MNIST.
42
Under review as a conference paper at ICLR 2021
ιιcnn
-ɪ-2
O O
1 1
BoSSo-Uu
mushrooms
0	20	40	60	80 IOO
Epoch
0	20	40	60	80 IOO
Epoch
—Adabound —Radam —Adam	—∣— SLS -⅛- Adagrad + SLS Amsgrad + SLS -⅛- Amsgrad + SLS + HB
Figure 9: Comparison of optimization methods on convex objectives: binary classification on LIBSVM
datasets using RBF kernel mappings. The kernel bandwidths are chosen by cross-validation following
the protocol in (Vaswani et al., 2019b). All line-search methods use c = 1/2 and the procedure
described in Appendix F. The other methods are use their default parameters. We observe the superior
convergence of the SLS variants and the poor performance of the baselines.
Epoch
True model	Rank 1
1 4 7 0 3
- - ---
10101010-10^
{60=SSO-Usu
Rank 10
Rank 4
0	25	50	75	100	0	25	50	75	100	0	25	50	75	100	0	25	50	75	100
Epoch	Epoch	Epoch	Epoch
-→- Adam →- Adabound -→- Radam	—i— SLS	Adagrad + SLS -^k- Amsgrad + SLS Amsgrad + SLS + HB
Figure 10: Comparison of optimization methods for deep matrix factorization. Methods use the
same hyper-parameter settings as above and we examine the effects of over-parameterization on the
problem: minw^w? Ex〜N(0,1)∣∣ W2W1χ - Axk2 (VasWani et al., 2019b; Rolinek & Martius, 2018).
We choose A ∈ R10×6 with condition number κ(A) = 1010 and control the over-parameterization
via the rank k (equal to 1,4, 10) of W1 ∈ Rk×6 and W2 ∈ R10×k. We also compare against the
true model. In each case, We use a fixed dataset of 1000 samples. We observe that as the over-
parameterization increases, the performance of all methods improves, With the methods equipped
With SLS performing the best.
43
Under review as a conference paper at ICLR 2021
’ ‘ ‘ ’ ‘ J
0 12 3 4
O
Ioooo
Illl
(6) ssuU
50	100	150	200
Epoch	Epoch	Epoch
CIFAR10 - ResNet34 C ,c CIFAR100 - DenseNetl21
4 2 0 8
9 9 9 8
AUeJnbe uo=ep=e>
0	50	100	150	200
Epoch
4 2 0 8 6
7 7 7 6 6
Cifarioo - ReSNet34
Tiny ImageNet - ResNetl8
50	100	150	200	0	50	100	150	200 "	50	100	150	200
Epoch	Epoch	Epoch
Amsgrad + SLS -^k- Adagrad + SLS + mom -^k- Amsgrad + SLS + HB Adam	—∣- SLS	-⅛- Amsgrad + SLS (beta = 0)	-^k- Adagrad + SLS
Figure 11: Ablation study comparing variants of the basic optimizers for multi-class classification
with deep networks. Training loss (top) and validation accuracy (bottom) for CIFAR-10, CIFAR-
100 and Tiny ImageNet. We consider the AdaGrad with AMSGrad-like momentum and do not find
improvements in performance. We also benchmark the performance of AMSGrad without momentum,
and observe that incorporating AMSGrad momentum does improve the performance, whereas heavy-
ball momentum has a minor, sometimes detrimental effect. We use SLS and Adam as benchmarks to
study the effects of incorporating preconditioning vs step-size adaptation.
44