Under review as a conference paper at ICLR 2021
A Near-Optimal Algorithm for Debiasing
Trained Machine Learning Models
Anonymous authors
Paper under double-blind review
Ab stract
We present an efficient and scalable algorithm for debiasing trained models, in-
cluding deep neural networks (DNNs), which we prove to be near-optimal by
bounding its excess Bayes risk. Unlike previous black-box reduction methods to
cost-sensitive classification rules, the proposed algorithm operates on models that
have been trained without having to retrain the model. Furthermore, as the algo-
rithm is based on projected stochastic gradient descent (SGD), it is particularly
attractive for deep learning applications. We empirically validate the proposed
algorithm on standard benchmark datasets across both classical algorithms and
modern DNN architectures and demonstrate that it outperforms previous post-
processing approaches for unbiased classification.
1	Introduction
Machine learning is increasingly applied to critical decisions which can have a lasting impact on
individual lives, such as for credit lending (Bruckner, 2018), medical applications (Deo, 2015), and
criminal justice (Brennan et al., 2009). Consequently, it is imperative to understand and improve the
degree of bias of such automated decision-making.
Unfortunately, despite the fact that bias (or “fairness”) is a central concept in our society today,
it is difficult to define it in precise terms. In fact, as people perceive ethical matters differently
depending on a plethora of factors including geographical location or culture (Awad et al., 2018), no
universally-agreed upon definition for bias exists. Moreover, the definition of bias may depend on
the application and might even be ignored in favor of accuracy when the stakes are high, such as in
medical diagnosis (Kleinberg et al., 2017; Ingold and Soper, 2016). As such, it is not surprising that
several definitions of “unbiased classification” have been introduced. These include statistical parity
(Dwork et al., 2012; Zafar et al., 2017a), equality of opportunity (Hardt et al., 2016), and equalized
odds (Hardt et al., 2016; Kleinberg et al., 2017). Unfortunately, such definitions are not generally
compatible (Chouldechova, 2017) and some might even be in conflict with calibration (Kleinberg
et al., 2017). In addition, because fairness is a societal concept, it does not necessarily translate into
a statistical criteria (Chouldechova, 2017; Dixon et al., 2018).
Statistical parity Let X be an instance space and let Y = {0, 1} be the target set in a standard
binary classification problem. In the fair classification setting, we may further assume the existence
of a (possibly randomized) sensitive attribute s : X → {0, 1, . . . , K}, where s(x) = k if and only
if x ∈ Xk for some total partition X = ∪kXk. For example, X might correspond to the set of
job applicants while s indicates their gender. Here, the sensitive attribute can be randomized if, for
instance, the gender of an applicant is not a deterministic function of the full instance x ∈ X (e.g.
number of publications, years of experience, ...etc). Then, a commonly used criterion for fairness
is to require similar mean outcomes across the sensitive attribute. This property is well-captured
through the notion of statistical parity (a.k.a. demographic parity) (Corbett-Davies et al., 2017;
Dwork et al., 2012; Zafar et al., 2017a; Mehrabi et al., 2019):
Definition 1 (Statistical Parity). Let X be an instance space and X = ∪kXk be a total partition of
X. A classifier f : X → {0, 1} satisfies statistical parity across all groups X1, . . . , XK if:
max	Ex [f (x) |x ∈ Xk] - min	Ex[f(x) |x ∈ Xk] ≤
k∈{1,2,...,K}	k∈{1,2,...,K}
1
Under review as a conference paper at ICLR 2021
k-Nearest neighbours
p(y=l∣x)
0.12
0.02
AO.10
Mo.08
10.06
u- 0.04
Multilayer perceptron
Females
Males
p(y=l∣x)
IO0
IOT
IO-
10^3
p(y=l∣x)
IO0
IO-1
10-2
IO-3
White Black Asian American Others
Indian
0.00
Indian
Figure 1: Top: Histogram of classifiers' predictions on both subpopulations, demonstrating a clear
gender bias in all cases. Bottom: The bias defined as the absolute difference in mean outcome
between genders within different demographic groups, before and after applying the proposed al-
gorithm. Blue bars show the results of the unmodified classifier, orange bars show the results of
optimizing for statistical parity with no regard for demographic information. Finally, green bars are
the results of applying statistical parity on the cross product of gender and ethnicity.
To motivate and further clarify the definition, We showcase the empirical results on the Adult bench-
mark dataset (Blake and Merz, 1998) in Figure 1. When tasked with predicting whether the income
of individuals is above $50K per year, all considered classifiers exhibit gender-related bias. One
way of removing such bias is to enforce statistical parity across genders. Crucially, however, with-
out taking ethnicity into account, different demographic groups may experience different outcomes.
In fact, gender bias can actually increase in some minority groups after enforcing statistical parity.
This can be fixed by redefining the sensitive attribute to be the cross product of both gender and
ethnicity (green bars).
Our main contribution is to present a near-optimal recipe for debiasing models, including deep neural
networks, according to Definition 1. Specifically, we formulate the task of debiasing learned models
as a regularized optimization problem that is solved efficiently using the projected SGD method.
We show how the algorithm produces thresholding rules with randomization near the thresholds,
where the width of randomization is controlled by the regularization parameter. We also show that
randomization near the threshold is necessary for Bayes risk consistency. While we focus on binary
sensitive attributes in our experiments in Section 5, our algorithm and its theoretical guarantees
continue to hold for non-binary sensitive attributes as well.
Statement of Contribution
1.	We derive a near-optimal post-processing algorithm for debiasing learned models (Section 3).
2.	We prove theoretical guarantees for the proposed algorithm, including a proof of correctness
and an explicit bound on the Bayes excess risk (Section 4).
3.	We empirically validate the proposed algorithm on benchmark datasets across both classical
algorithms and modern DNN architectures. Our experiments demonstrate that the proposed
algorithm significantly outperforms previous post-processing methods (Section 5).
In Appendix E, we also show how the proposed algorithm can be modified to handle other criteria
of bias as well.
2	Related Work
Algorithms for fair machine learning can be broadly classified into three groups: (1) pre-processing
methods, (2) in-processing methods, and (3) post-processing methods (Zafar et al., 2019).
2
Under review as a conference paper at ICLR 2021
Preprocessing algorithms transform the data into a different representation such that any classifier
trained on it will not exhibit bias. This includes methods for learning a fair representation (Zemel
et al., 2013; Lum and Johndrow, 2016; Bolukbasi et al., 2016; Calmon et al., 2017; Madras et al.,
2018; Kamiran and Calders, 2012), label manipulation (Kamiran and Calders, 2009), data augmen-
tation (Dixon et al., 2018), or disentanglement (Locatello et al., 2019).
On the other hand, in-processing methods constrain the behavior of learning algorithms in order
to control bias. This includes methods based on adversarial learning (Zhang et al., 2018) and
constraint-based classification, such as by incorporating constrains on the decision margin (Zafar
et al., 2019) or features (Grgic-H山Ca et al., 2018). Agarwal et al. (2018) showed that the task of
learning an unbiased classifier could be reduced to a sequence of cost-sensitive classification prob-
lems, which could be applied to any black-box classifier. One caveat of the latter approach is that
it requires solving a linear program (LP) and retraining classifiers, such as neural networks, many
times before convergence.
The algorithm we propose in this paper is a post-processing method, which can be justified theo-
retically (Corbett-Davies et al., 2017; Hardt et al., 2016; Menon and Williamson, 2018; Celis et al.,
2019). Fish et al. (2016) and Woodworth et al. (2017) fall under this category. However, the former
only provides generalization guarantees without consistency results while the latter proposes a two-
stage approach that requires changes to the original training algorithm. Kamiran et al. (2012) also
proposes a post-processing algorithm, called Reject Option Classifier (ROC), without providing any
theoretical guarantees. In contrast, our algorithm is Bayes consistent and does not alter the original
classification method. In Celis et al. (2019) and Menon and Williamson (2018), instance-dependent
thresholding rules are also learned. However, our algorithm also learns to randomize around the
threshold (Figure 2(a)) and this randomization is key to our algorithm both theoretically as well as
experimentally (Appendix C and Section 5). Hardt et al. (2016) learns a randomized post-processing
rule but our proposed algorithm outperforms it in all of our experiments (Section 5).
Woodworth et al. (2017) showed that the post-processing approach can, sometimes, be highly sub-
optimal. Nevertheless, the latter result does not contradict the statement that our post-processing
rule is near-optimal because we assume that the original classifier outputs a monotone transforma-
tion of some approximation to the posterior probability p(y = 1 | x) (e.g. margin or softmax output)
whereas Woodworth et al. (2017) assumed in their construction that the post-processing rule had
access to the binary predictions only.
We argue that the proposed algorithm has distinct advantages, particularly for deep neural networks
(DNNs). First, stochastic convex optimization methods are well-understood and can scale well to
massive amounts of data (Bottou, 2010), which is often the case in deep learning today. Second, the
guarantees provided by our algorithm hold w.r.t. the binary predictions instead of using a proxy, such
as the margin as in some previous works (Zafar et al., 2017b; 2019). Third, unlike previous reduction
methods that would require retraining a deep neural network several times until convergence (Agar-
wal et al., 2018), which can be prohibitively expensive, our algorithm operates on learned models
that are trained once and does not require retraining.
Besides developing algorithms for fair classification, several recent works focused on other related
aspects, such as proposing new definitions for fairness; e.g. demographic parity (Dwork et al.,
2012; Mehrabi et al., 2019), equalized odds (Hardt et al., 2016), equality of opportunity/disparate
mistreatment (Zafar et al., 2017a; Hardt et al., 2016), and individual fairness (Dwork et al., 2012).
Recent works have also established several impossibility results related to fair classification, such
as Kleinberg et al. (2017); Chouldechova (2017). In our case, we derive a new impossibility result
that holds for any deterministic binary classifier and relate it to the task of controlling the covariance
between the classifier’s predictions and the sensitive attribute (Appendix E).
3	Near-optimal algorithm for Statistical Parity
Notation We reserve boldface letters for random variables (e.g. x), small letters for instances (e.g.
x), capital letters for sets (e.g. X), and calligraphic typeface for universal sets (e.g. the instance
space X). Given a set S, 1S (x) ∈ {0, 1} is the characteristic function indicating whether x ∈ S.
We denote by [n] the set of integers {1, . . . , n} and [x]+ = max{0, x}.
3
Under review as a conference paper at ICLR 2021
Algorithm Given a classifier f : X → [-1, +1] our goal is to post-process the predictions made
by f1 in order to control the bias with respect to a sensitive attribute s : X → [K] as in Definition 1.
To this end, instead of learning a deterministic classifier, we consider randomized prediction rules
of the form
~	____ -	r	-	r
h : X × {1, 2,..., K} × [-1,1] → [0,1],
where h(x) represents the probability of predicting the positive class given (i) instance x ∈ X, (ii)
sensitive attribute s(x), and (iii) classifier’s output f(x).
As discussed in Appendix B, for post-processing rule h(x), and for each group Xk ⊆ X, the fair-
ness constraint in Definition 1 can be written as ∣Eχ[h(x) | X ∈ Xk] - ρ∣ ≤ e, where P ∈ [0,1] is
a hyper-parameter tuned via a validation dataset. On the other hand, minimizing the probability of
altering the predictions of the original classifier can be achieved by maximizing the inner product
Eχ[h(x) ∙ f (x)]. Instead of optimizing this quantity directly, which would lead to a pure thresholding
rule, We minimize the regularized objective: (γ∕2)Eχ[h(x)2] - Eχ[h(x) ∙ f (x)] for some regulariza-
tion parameter γ > 0. This regularization leads to randomization around the threshold, which we
show to be critical, both theoretically (Section 4 and Appendix C) and experimentally (Section 5).
Using Lagrange duality we show that the solution reduces to the update rules in Equation 2 with
optimization variables {λk, μk}k∈[κ] and the corresponding predictor which outputs +1 for group
Xk with probability hγ (x) is given by
(0,	f (x)	≤ λk - μk
hγ(X)	=	( (f (X)-	λk	+ μk ”γ,	λk -	μk ≤ f (X)	≤	λk	-	μk	+ Y	(I)
[1,	f (x)	≥ λk - μk	+	γ
where ξγ is given by Eq. (3).
Update rules To learn these parameters, one can apply the following update rules (Appendix B):
λs(x) - max n0, λs(x) - η (d + P + 豆---ξγ (f (X) - (λs(x) - μs(x)
2	s(x)
μs(χ) J max n0, μs(x) - η(2 - P + ∂μ(/Y(f (X) - (λs(x) - μs(X))
(2)
where, again, P ∈ [0, 1] is a hyperparameter tuned via a validation dataset, s : X → [K] is the
sensitive attribute, and γ > 0 is a regularization parameter that controls the level of randomization.
In addition, the function ξγ : R → R+ is given by:
2
ξγ(W) = 2γ ∙ I{0 ≤ W ≤ γ} + (W - 2) ∙ I{w > γ}	⑶
Note that ξγ is convex and its derivative ξ∖ is (1∕γ)-Lipschitz continuous; it can be interpreted as
differentiable approximation to the ReLU unit (Nair and Hinton, 2010). A full pseudocode of the
proposed algorithm is presented in Appendix A.
4 Theoretical Analysis
Next, we analyze the algorithm. Our first theoretical result is to show that the prediction rule in
Equation 1 learned through the update rules presented in Section 3 satisfies the desired fairness
guarantees on the training sample.
Theorem 1 (Correctness). Let hγ : X → [0, 1] be the randomized predictor in Equation 1 learned
by applying the update rules in Equation 2 starting with μk = 0, λk = 0, ∀k ∈ [K] until conver-
gence. Then, hγ satisfies statistical parity w.r.t. {Xk}k∈[K] in the training sample.
The proof of Theorem 1 is presented in Appendix B. The following guarantee, which holds w.r.t.
the underlying data distribution, shows that the randomized prediction rule converges to the Bayes
1Ideally an estimate of some monotone transformation of 2η(x) - 1, where η(x) = p(y = 1|X = x) is
the Bayes regressor. This is not a strong assumption because many algorithms can be calibrated to provide
probability scores (Platt et al., 1999; Guo et al., 2017).
4
Under review as a conference paper at ICLR 2021
(a) Decision rule	(b) Convergence
τ-,∙	C Z ∖ EI 1	1	.	♦	1	7* /	∖	∙	-ɪ-,	. ∙	-t	i'	i' . t
Figure 2:	(a) The learned post-processing decision rule h(x) in Equation 1 as a function of the
classifier’s score f (x). Randomization is applied when h(x) ∈ (0, 1), which can be controlled
using the regularization parameter γ > 0. (b) The value of λ0 - u0 is plotted against the number
of epochs in the projected SGD method applied to the output of the random forests classifier trained
on the Adult dataset to implement statistical parity with respect to the gender attribute (cf. Section
5 and Figure 1). We observe fast convergence in agreement with Proposition 1.
optimal unbiased classifier if the original classifier f is Bayes consistent. The proof of the following
theorem (Appendix C) is based on the Lipschitz continuity of the decision rule when γ > 0 and the
robustness framework of Xu and Mannor (2010).
Theorem 2. Let h? = arg minh∈H E[h(x) 6= y], where H is the set of binary predictors on X
that satisfy fairness according to Definition 1 for > 0. Let hγ : X → [0, 1] be the randomized
learning rule in Equation 1. IfhY is trained on afreshly sampled data ofsize N, then there exists a
value ofρ ∈ [0, 1] such that the following holds with a probability ofat least 1 - δ:
-, , . ., ..
E[I{hγα) = y}] ≤ E[I{h*(x)= y}]+ E∣2η(x) - 1 - f(x)| +2γ +
+ 4√2k+P
N 3
where η(x) = p(y = 1|x = x) is the Bayes regressor andKis the number of groups Xk.
Consequently, if the original classifier is Bayes consistent and we have: N → ∞, γ → 0+ and
YN-1 → ∞, then E[hY(x) = y] → E[h*(x) = y]. Hence, the updates converge to the optimal
prediction rule subject to the chosen fairness constraint.
Running time As shown in Appendix B, the update rules in Equation 2 perform a projected
stochastic gradient descent on the following optimization problem:
/	、、min 、F = Ex IE (λs(x) + μs(X))+ P (λs(x) — μs(x) ) + ξγ (f(X) — (λs(x) — μs(X)))] (4)
(μι,λι),…，(μκ ,λκ )≥0	L	」
We assume with no loss of generality that f(x) ∈ [—1, 1] since f(x) is assumed to be an estimator to
2η(x) — 1 (see Section 3 and Appendix B) and any thresholding rule over f(x) can be transformed
into an equivalent rule over a monotone increasing function of f (i.e. using the hyperbolic tangent).
Proposition 1. Let μ(0) = λ(0) = 0 and write μ(t), λ(t ∈ RK for the value of the optimization
variables after t updates defined in Equation 2 for some fixed learning rate a = α. Let μ =
(1/T) PT=I μ(t)(x) and X=QlT) PT=I λ㈤(x). Then,
E[F] — F(μ?)) ≤
(1+ρ+)2
I∣μ?ll2 + IIy?II2
2Tα
(5)
2
α
+
where F : RK X RK → R is the objective function in (4) using the averaged solution μ and λ while
F ? is its optimal value. In particular, E[F] — F (μ?))=O(PKTT) when α = O(PK/T).
The proof is in Appendix D. Hence, the post-processing rule can be efficiently computed. In practice,
we observe fast convergence as shown in Figure 2(b).
As shown in Figure 2(a), the hyperparameter γ controls the width of randomization around the
thresholds. A large value of γ may reduce the accuracy of the classifier. On the other hand, γ
5
Under review as a conference paper at ICLR 2021
Bias						
Dataset	Classifier	Original	Proposed	Hardt etal.(2016)	Shift Inference	ROC
	RF	.38 ± .02	.01 ± .003	.01 ± .003	.16± .011	.02 ± .002
ADULT	kNN	.24 ± .02	.02 ± .005	.01 ± .004	.08 ± .006	.08 ± .006
	MLP	.29 ± .02	.01 ± .002	.02 ± .002	.10± .012	.02 ± .003
	LR	.39 ± .02	.01 ± .004	.02 ± .007	.10± .020	.01 ± .002
	RF	.07 ± .03	.01 ± .002	.01 ± .002	.09 ± .010	.02 ± .004
CDDD	kNN	.10± .02	.01 ± .005	.01 ± .001	.18± .005	.02 ± .005
	MLP	.13 ± .02	.01 ± .005	.01 ± .003	.12± .005	.02 ± .003
	LR	.12± .02	.01 ± .003	.01 ± .001	.13 ± .003	.01 ± .003
						
Test error						
Dataset	Classifier	Original	Proposed	Hardt etal. (2016)	Shift Inference	ROC
	RF	34.1±.2%	35.8±.1%	38.0±.1%	34.5±.1%	36.0±.3%
ADULT	kNN	39.9±.2%	39.7±.2%	42.0±.2%	39.2±.2%	39.5±.2%
	MLP	34.8±.1%	36.8±.1%	37.4±.2%	35.7±.2%	36.3±.1%
	LR	35.3±.1%	36.6±.2%	38.8±.1%	35.9±.1%	36.5±.2%
	RF	18.8±.2%	18.2±.1%	19.4±.2%	19.0±.1%	18.5±.2%
CDDD	kNN	20.4±.2%	19.6±.2%	21.3±.1%	22.7±.2%	19.6±.2%
	MLP	19.5±.1%	18.7±.2%	21.2±.2%	19.4±.1%	19.0±.2%
	LR	19.4±.2%	18.3±.1%	21.7±.1%	19.8±.2%	19.5±.2%
Table 1: A comparison of the four post-processing methods on two datasets with four different
original classifiers. Only the proposed algorithm and the algorithm of Hardt et al. (2016) eliminate
bias in all cases whereas ROC can fail in kNN (e.g. ADULT dataset) because debiasing kNN can
require randomization at the thresholds. Bias here is the absolute difference in mean outcomes
across the sensitive attribute (Definition 1).
cannot be zero because randomization around the threshold is, in general, necessary for Bayes risk
consistency as illustrated in the following example:
Example 1 (Randomization is necessary). Suppose that X = {-1, 0, 1} where p(x = -1) = 1/2,
p(x = 0) = 1/3 and p(x = 1) = 1/6. Let η(-1) = 0, η(0) = 1/2 and η(1) = 1. In addition, let
s ∈ {0, 1} be a sensitive attribute, where p(s = 1|x = -1) = 1/2, p(s = 1|x = 0) = 1, and p(s =
1|x = 1) = 0. Then, the Bayes optimal prediction rule f? (x) subject to statistical parity ( = 0)
satisfies: p(f ? (x) = 1|x = -1) = 0, p(f? (x) = 1|x = 0) = 7/10 and p(f? (x) = 1|x = 1) = 1.
Note that the Bayes excess risk bound in Theorem 2 is vacuous when γ = 0. Therefore, γ controls
a trade-off depending on how crucial randomization is around the thresholds (e.g. in k-NN where
the classifier’s scores come from a finite set or in deep neural networks that tend to produce scores
concentrated around {-1, +1}). In our experiments, γ is always chosen using a validation set.
5	Empirical evaluation
Experiment Setup We compare against three post-processing methods: (1) the post-processing
algorithm of Hardt et al. (2016) (2) the shift inference method, first introduced in (Saerens et al.,
2002) and used more recently in (Wang et al., 2020), and (3) the Reject Option Classifier (ROC)
(Kamiran et al., 2012). We use the implementation of the algorithm of Hardt et al. (2016) in the Fair-
Learn software package (Dudik et al., 2020). The training data used for the post-processing meth-
ods is always a fresh sample, i.e. different from the data used to train the original classifiers. The
value of the hyper-parameter θ of the ROC algorithm is chosen in the grid {0.01, 0.02, . . . , 1.0}.
When ROC fails, its solution with the minimum bias is reported. In the proposed algorithm, the
parameter γ is chosen in the grid {0.01, 0.02, 0.05, 0.1, 0.2, . . . , 1.0} while ρ is chosen in the gird
E[y] ± {0, 0.05, 0.1}. All hyper-parameters are selected based on a separate validation dataset.
Tabular Data We empirically evaluate the performance of the proposed algorithm and the base-
lines on two real-world datasets, namely the Adult income dataset and the Default of Credit Card
6
Under review as a conference paper at ICLR 2021
Algorithm
Proposed
Hardt, etal. 2016
ROC
Algorithm
Proposed
Hardt, etal. 2016
ROC
Algorithm
Proposed
Hardt, etal. 2016
ROC
Figure 3:	The tradeoff curves are displayed for each classification problem. The x-axis corresponds
to bias (Definition 1) while the y-axis is the test accuracy. In general, debiasing CDDD improves test
accuracy because bias was introduced to the training data only. In addition, ROC fails at debiasing
four classifiers (see also Tables 1 and 2) due to the absence of randomization.
Clients (DCCC) dataset, both taken from the UCI Machine Learning Repository (Blake and Merz,
1998). The Adult dataset contains 48,842 records with 14 attributes each and the goal is to predict
if the income of an individual exceeds $50K per year. The DCCC dataset contains 30,000 records
with 24 attributes, and the goal is to predict if a client will default on their credit card payment.
Both datasets include sensitive attributes, such as sex and age. In Figure 1 we showcased why, in
some cases, the sensitive attribute can be the cross product of multiple features (e.g. religion, gen-
der, and race). In our experiments in this section, we define the sensitive class to be the class of
females. In the DCCC dataset, we additionally introduce bias in the training set for the purpose of
the experiment: if s(x) = y(x) we keep the instance and otherwise drop it with probability 0.5.
We train four classifiers on each dataset: (1) random forests with maximum depth 10, (2) k-NN
with k = 10, (3) a two-layer fully connected neural network with 128 hidden nodes, and (4) logistic
regression. For the latter, we fine-tune the parameter C in a grid of values chosen in a logarithmic
scale between 10-4 and 104 using 10-fold cross validation. The learning rate in our algorithm is
fixed to 10-1(K/T)1/2, where T is the number of steps, and = 0.
Table 1 shows the bias and accuracy on test data after applying each post-processing method. The
column marked as “original” corresponds to the original classifier without any alteration. As shown
in the table, both our proposed algorithm and the algorithm of Hardt et al. (2016) eliminate bias in all
classifiers. By contrast, the shift-inference method does not succeed at controlling statistical parity
while the ROC method can fail when the output of the original classifier is discrete, such as in kNN,
because it does not learn to randomize. Moreover, the proposed algorithm has a much lower impact
on the test accuracy compared to Hardt et al. (2016) and can even improve it in certain cases. The
fact that fairness can sometimes improve accuracy was recently noted by Blum and Stangl (2020).
The full tradeoff curves between bias and performance are provided in Figure 3.
CelebA Dataset Our second set of experiments builds on the task of predicting the “attractive-
ness” attribute in the CelebA dataset (Liu et al., 2015). CelebA contains 202,599 images of celebri-
ties annotated with 40 binary attributes, including gender. We use two standard deep neural network
architectures: ResNet50 (He et al., 2016) and MobileNet (Howard et al., 2017), trained from scratch
or pretrained on ImageNet. We present the results in Table 2. We observe that the proposed algo-
rithm significantly outperforms the post-processing algorithm of Hardt et al. (2016) and performs,
at least, as well as the ROC algorithm whenever the latter algorithm succeeds. Often, however, ROC
fails at debiasing the deep neural networks because it does not learn to randomize when most scores
produced by neural networks are concentrated around the set {-1, +1}.
7
Under review as a conference paper at ICLR 2021
Bias
Model	Training	Original	Proposed	Hardt et al. (2016)	Shift Inference	ROC
ResNet50	Scratch	.43	.01	.01	.38	.08
	ImageNet	.40	.02	.01	.35	.15
MobileNet	Scratch	.35	.01	.01	.24	.01
	ImageNet	.38	.002	.002	.34	.10
						
Test error						
Model	Training	Original	Proposed	Hardt et al. (2016)	Shift Inference	ROC
ResNet50	Scratch	22.2%	28.7%	34.1%	37.8%	26.9%
	ImageNet	20.3%	28.3%	32.5%	20.7%	22.8%
MobileNet	Scratch	23.1%	28.2%	33.6%	24.1%	28.3%
	ImageNet	20.7%	27.2%	32.5%	21.0%	24.5%
Table 2: A comparison of the four post-processing methods on CelebA (predict attractiveness) ap-
plied to the output of ResNet50 and MobileNet, each trained either from scratch or on ImageNet.
The proposed algorithm performs much better than ROC in terms of bias and much better than Hardt
et al. (2016) in terms of accuracy. Shift inference performs poorly in both objectives.

Q
Females
-0.5	0.0	0.5	1.0
2p(y=l∣x) - 1
.0
1
2 5 -
- -
O O
1 1
AUUanbE
Males
-0.5	0.0	0.5	1.0
2p(y=l∣x) - 1
Figure 4: The distribution of the scores produced by ResNet50 trained from scratch are shown for
both subpopulations. The curves correspond to the randomized post-processing rules, i.e. p(y =
1|x), of Hardt et al. (2016) and the proposed algorithm with γ = 0.1 and ρ = E[y].
We investigated the strong performance compared to that of Hardt et al. (2016) and found that it
is due to the specific form of randomization used by the proposed algorithm. As shown in Figure
4, the post-processing algorithm of Hardt et al. (2016) uses a fixed probability when randomizing
between two thresholds. For CelebA trained from scratch, for example, the post-processing rule of
Hardt et al. (2016) predicts nearly uniformly at random when ResNet50 predicts the negative class
for males. In contrast, our algorithm uses a ramp function that takes the confidence of the scores
into account. In Figure 4, in particular, the male instances with scores close to -1 are flipped with
probability ≈ 0.15, as opposed to ≈ 0.5 in Hardt et al. (2016), and this difference is compensated
for by flipping all examples with scores larger than ≈ -0.9 and all female instances with scores less
than ≈ 0.9. Hence, less randomization is applied when the original classifier is more confident.
Lastly, one important observation We note in Table 2 is the impact of pre-training - pretraining in
our experiments helps in achieving a lower test error rate even after eliminating bias. In other words,
pretraining seems to reduce the cost of debiasing trained models.
6 Concluding Remarks
In this paper, We propose a near-optimal post-processing algorithm for debiasing trained machine
learning models. The proposed algorithm is scalable, does not require retraining the classifiers, and
has a limited impact on the test accuracy. In addition to providing strong theoretical guarantees, We
shoW that it outperforms previous post-processing methods for unbiased classification on standard
benchmarks across classical and modern machine learning models.
8
Under review as a conference paper at ICLR 2021
References
M. A. Bruckner, “The promise and perils of algorithmic lenders’ use of big data,” Chi.-Kent L. Rev.,
2018.
R.	C. Deo, “Machine learning in medicine,” Circulation, 2015.
T. Brennan, W. Dieterich, and B. Ehret, “Evaluating the predictive validity of the COMPAS risk and
needs assessment system,” Criminal Justice and Behavior, 2009.
E. Awad, S. Dsouza, R. Kim, J. Schulz, J. Henrich, A. Shariff, J.-F. Bonnefon, and I. Rahwan, “The
moral machine experiment,” Nature, 2018.
J. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent trade-offs in the fair determination of
risk scores,” 2017.
D. Ingold and S. Soper, “Amazon doesnt consider the race of its customers. should it,” Bloomberg
News, 2016.
C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through awareness,” in Inno-
vations in Theoretical Computer Science, 2012.
M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, “Fairness beyond disparate treat-
ment & disparate impact: Learning classification without disparate mistreatment,” in Interna-
tional Conference on World Wide Web, 2017.
M. Hardt, E. Price, N. Srebro et al., “Equality of opportunity in supervised learning,” in Advances
in Neural Information Processing Systems, 2016.
A. Chouldechova, “Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments,” Big data, 2017.
L. Dixon, J. Li, J. Sorensen, N. Thain, and L. Vasserman, “Measuring and mitigating unintended
bias in text classification,” in Conference on AI, Ethics, and Society, 2018.
S.	Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Algorithmic decision making and the
cost of fairness,” in International Conference on Knowledge Discovery and Data Mining, 2017.
N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on bias and fairness
in machine learning,” arXiv preprint arXiv:1908.09635, 2019.
C.	L. Blake and C. J. Merz, “UCI repository of machine learning databases,” Irvine, CA, 1998.
M. B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. P. Gummadi, “Fairness Constraints: A Flexible
Approach for Fair Classification,” Journal of Machine Learning Research, 2019.
R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, “Learning fair representations,” in Interna-
tional Conference on Machine Learning, 2013.
K. Lum and J. Johndrow, “A statistical framework for fair predictive algorithms,” arXiv preprint
arXiv:1610.08077, 2016.
T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai, “Man is to computer program-
mer as woman is to homemaker? debiasing word embeddings,” in Advances in Neural Informa-
tion Processing Systems, 2016.
F. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R. Varshney, “Optimized pre-
processing for discrimination prevention,” in Advances in Neural Information Processing Sys-
tems, 2017.
D.	Madras, E. Creager, T. Pitassi, and R. Zemel, “Learning adversarially fair and transferable repre-
sentations,” in International Conference on Machine Learning, 2018.
F. Kamiran and T. Calders, “Data preprocessing techniques for classification without discrimina-
tion,” Knowledge and Information Systems, 2012.
9
Under review as a conference paper at ICLR 2021
——, “Classifying without discriminating,” in International Conference on Computer, Control and
Communication, 2009.
F. Locatello, G. Abbati, T. Rainforth, S. Bauer, B. Scholkopf, and O. Bachem, “On the fairness of
disentangled representations,” in Advances in Neural Information Processing Systems, 2019.
B. H. Zhang, B. Lemoine, and M. Mitchell, “Mitigating unwanted biases with adversarial learning,”
in Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 2018.
N. Grgic-Hlaca, M. B. Zafar, K. P. Gummadi, and A. Weller, “Beyond distributive fairness in algo-
rithmic decision making: Feature selection for procedurally fair learning,” in AAAI Conference
on Artificial Intelligence, 2018.
A.	Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach, “A reductions approach to fair
classification,” in International Conference on Machine Learning, 2018.
A. K. Menon and R. C. Williamson, “The cost of fairness in binary classification,” in Conference on
Fairness, Accountability and Transparency, 2018.
L. E. Celis, L. Huang, V. Keswani, and N. K. Vishnoi, “Classification with fairness constraints: A
meta-algorithm with provable guarantees,” in Conference on Fairness, Accountability, and Trans-
parency, 2019.
B.	Fish, J. Kun, and A. D. Lelkes, “A confidence-based approach for balancing fairness and accu-
racy,” in International Conference on Data Mining, 2016.
B.	Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro, “Learning non-discriminatory
predictors,” arXiv preprint arXiv:1702.06081, 2017.
F. Kamiran, A. Karim, and X. Zhang, “Decision theory for discrimination-aware classification,” in
2012 IEEE 12th International Conference on Data Mining. IEEE, 2012, pp. 924-929.
L.	Bottou, “Large-scale machine learning with stochastic gradient descent,” in International Con-
ference on Computational Statistics, 2010.
M.	B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi, “Fairness Constraints: Mechanisms for
Fair Classification,” in Proceedings of the 20th International Conference on Artificial Intelligence
and Statistics, 2017.
J.	Platt et al., “Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods,” Advances in Large Margin Classifiers, 1999.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in
International Conference on Machine Learning, 2017.
V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann machines,” in Inter-
national Conference on Machine Learning, 2010.
H. Xu and S. Mannor, “Robustness and generalization,” 2010.
M. Saerens, P. Latinne, and C. Decaestecker, “Adjusting the outputs of a classifier to new a priori
probabilities: a simple procedure,” Neural Computation, 2002.
Z. Wang, K. Qinami, I. C. Karakozis, K. Genova, P. Nair, K. Hata, and O. Russakovsky, “Towards
fairness in visual recognition: Effective strategies for bias mitigation,” in Conference on Computer
Vision and Pattern Recognition, 2020.
M. Dudik, R. Edgar, B. Horn, and R. Lutz, “fairlearn 0.4.6,” 2020. [Online]. Available:
https://pypi.org/project/fairlearn/
A. Blum and K. Stangl, “Recovering from biased data: Can fairness constraints improve accuracy?”
FROC, 2020.
Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in International
Conference on Computer Vision, 2015.
10
Under review as a conference paper at ICLR 2021
K.	He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Conference
on Computer Vision and Pattern Recognition, 2016.
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and
H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision applications,”
arXiv preprint arXiv:1704.04861, 2017.
S. Boyd and A. Mutapcic, “Stochastic subgradient methods,” 2008. [Online]. Available:
https://see.stanford.edu/materials/lsocoee364b/04-stoch_subgrad_notes.pdf
S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press, 2004.
11
Under review as a conference paper at ICLR 2021
A Full algorithm
Algorithm 1: A Pseudocode of the Proposed Algorithm for Conditional Statistical Parity.
Data: γ > 0; ρ ∈ [0, 1];	≥ 0; f : X → [-1, +1]; s : X → [K]
Result: Optimal values of thresholds: (λι,μι),..., (λκ, μκ).
Training: Initialize (λι, μι),..., (λκ, μκ) to zeros. Then, repeat until convergence:
1.	Sample an instance X 〜p(x)
2.	Perform the updates:
λs(x) J maχ n0, λs(x) - η(2 + P + dʌ—
2	∂λs(x)
∂
μs(x) J max∣0, μs(x) - η(^ - P + d---七
L	-	dμs (X)
ξY (f(X)- (λs(x) - μs(x)
ξY (f(X)- (λs(x) - μs(x)
where ξγ is given by Eq. (3).
Prediction: Given an instance x in the group Xk, predict the label +1 with probability:
I	0,	f(x)	≤ λk — μk
h(x) =	(	(f (x)	— (λk	— μk))∕γ,	λk —	μk ≤ f(x)	≤ 入k — μk + Y
I	1,	f(x)	≥ λk — μk	+ Y
B Proof of Theorem 1
B.1	Constrained Convex Formulation
Suppose we have a binary classifier on the instance space X. We would like to construct an algorithm
for post-processing the predictions made by that classifier such that we control the bias with respect
to a set of pairwise disjoint groups X1, . . . , XK ⊆ X according to Definition 1. We assume that the
output of the classifier f : X → [-1, +1] is an estimate to 2η(x) - 1, where η(x) = p(y = 1|x = x)
is the Bayes regressor. This is not a strong assumption because many algorithms can be calibrated
to provide probability scores (Platt et al., 1999; Guo et al., 2017) so the assumption is valid. We
consider randomized rules of the form:
~	__ -	r	-	r
h : X ×{1, 2,..., K} × [-1,1] → [0,1],
whose arguments are: (1) the instance x ∈ X, (2) the sensitive attribute s(x) ∈ [K], and (3) the
original classifier’s score f (x). Because randomization is sometimes necessary as proved in Section
4, h(x) is the probability of predicting the positive class when the instance is x ∈ X.
C	1	.	1	1'	" r 1 ♦ 1	∙ιι 1	.	1 C τ ,	7/ 、一 l^cτ^l
Suppose we have a training sample of size N, which we will denote by S. Let qi = h(xi) ∈ [0, 1]
for the i-th instance in S. For each group Xk ⊆ S, the fairness constraint in Definition 1 over the
training sample can be written as:
⅛∣ LXXkqi-h 2,
for some hyper-parameter ρ > 0. This holds by the triangle inequality.
To learn f , we propose solving the following regularized optimization problem:
min
0≤qi≤1
N
X(γ∕2) q - f(χi) qi s.t. ∀Xk ∈ G ： I X qi - ρ∖ ≤ a
i=1	i∈Xk
(6)
where γ > 0 is a regularization parameter and k = |Xk| ∕2.
B.2	Reduction to Unconstrained Optimization
Because the groups Xk are pairwise disjoint, the optimization problems in (6) decomposes into K
separate suboptimization problems, one for each group Xk. Each sub-optimization problem can be
12
Under review as a conference paper at ICLR 2021
written in the following general form:
	min1	γq2 γq2 - f (xi)qi 0≤qi ≤1	2 i=1 MM s.t.	(ziqi - b) ≤ ,	-	(ziqi - b) ≤ 0 i=1	i=1
To recall, 0 = M /2. The Lagrangian is:
L(q, α,β,λ, μ) =
X (2q2 - f (Xi)qi) + λ(X(Ziqi - b) -C - μ(X(ziqi - b) + C + X ai(qi - 1) - X民口，
i	i	i	ii
Taking the derivative w.r.t. qi gives us:
	qi = Y (f (Xi) - (λ - μ)zi - αi + β)
Plugging this back, the dual problem becomes:
	q λminαβ	X (2 q2+b(λ - μ)) + (λ+μ)e0+X ai ii S.t.	qi = Y (f (Xi) - (λ - μ)zi - ɑi + βi) λ,μ,αi,βi ≥ 0
Next, we eliminate variables. By eliminating βi , we have:
	q λminαβ	X (2q2 + b(λ - μ)) + (λ + μ)e0 + X ai ii st	qi - Y (f (Xi) - (λ - μ)zi - αi) ≥ 0 λ, μ,αi ≥ 0
Equivalently:	q λminαβ	X (2 q2+b(λ - μ))+(λ+μ)e0+X ai ii S.t.	αi ≥ f (Xi) - Yqi - (λ - μ)zi λ, μ,αi ≥ 0
Next, we eliminate αi to obtain:
min q,λ,μ	X (2q2 + b(λ - M)) + (λ + μ)e0 + X [f (Xi) - γqi - (λ - μ)zi] + ii λ,μ ≥ 0
Finally, let's eliminate the qi variables. For a given optimal μ and λ, it is straightforward to observe
that the minimizer q? to γ∕2q2 + [w - γq]+ must lie in the set {0, w∕γ, 1}. In particular, if w∕γ ≤ 0,
then q? = 0. If w∕γ ≥ 1, then q? = 1. Note here that We make use of the fact that γ > 0.
So, the optimal value of q? to γ∕2q2 + [w - γq]+ is:
	仅	Y ≤ 0 ξγ (w) = ( w2	0 ≤ γ ≤ 1 [w-2 γ ≥ 1
From this, the optimization problem reduces to:
	N 累in SX (b(λ - μ) + e0(λ + μ) + ξγ(f (Xi)- (λ - μ)zi))	(7) ,— i=1
13
Under review as a conference paper at ICLR 2021
This is a differentiable objective function and can be solved quickly using the projected gradient
descent method (Boyd and Mutapcic, 2008). The projection step here is taking the positive parts of
λ and μ. This leads to the update rules in Algorithm 1.
What about the solution? Given λ and μ, the solution of q% is a minimizer to;
2q2 + [f (Xi) - Yqi - (λ - μ)zi] +
As stated earlier, the solution is:
(0,	f(xi) ≤ (λ - μ)zi
qi = ( (1∕γ)(f (Xi) - (λ - μ)zi), γ(λ - μ)zi ≤ f (xi) ≤ (λ - μ)zi + Y	(8)
U,	f(xi) ≥ (λ - μ)zi + Y
So, we have a ramp function. In the proposed algorithm, we have zi = 1 and b = ρ for all examples.
This proves Theorem 1.
C	Proof of Theorem 2
C.1 Optimal Unbiased Predictors
We begin by proving the following result, which can be of independent interest.
Theorem 3. Let f ? = arg min f:χ→{°j} E[I{f (x) = y }] be the Bayes optimal decision rule subject
to group-wise affine constraints of the form E[wk (x) ∙ f (x) | x ∈ Xk] 二 bk for some fixed partition
X = ∪kXk. Ifwk : X → R and bk ∈ R are such that there exists a constant c ∈ (0, 1) in which
p(f (X) = 1) = c will satisfy all the affine constraints, then f? satisfies p(f? (X) = 1) = I{η(X) >
tk} + τk I{η(X) = tk}, where η(X) = p(y = 1|x = X) is the Bayes regressor, tk ∈ [0, 1] is a
threshold specific to the group Xk ⊆ X, and τk ∈ [0, 1].
Proof. Minimizing the expected misclassification error rate of a classifier f is equivalent to maxi-
mizing:
E[f(χ) ∙ y +(1- f(χ)) ∙ (1 - y)] = E∣E[f (χ) ∙ y + (1 - f (χ)) ∙ (1 - y)] ∣ χ]
=E [E[f (χ) ∙ (2η(x) - 1)] IXi + E[1 - η(χ)]
Hence, selecting f that minimizes the misclassification error rate is equivalent to maximizing:
E[f (χ) ∙ (2η(χ) - 1)]	(9)
Instead of maximizing this directly, we consider the regularized form first. Writing g(X) = 2η(X) -
1, the optimization problem is:
m”	(γ∕2)E[f (χ)2] - E[f (χ) ∙ g(χ)]	s.t.	E[w(χ) ∙ f (χ)] = b
0≤f (x)≤1
Here, we focused on one subset Xk because the optimization problem decomposes into K separate
optimization problems, one for each Xk. If there exists a constant c ∈ (0, 1) such that f(X) = c
satisfies all the equality constraints, then Slater’s condition holds so strong duality holds (Boyd and
Vandenberghe, 2004).
The Lagrangian is:
(γ∕2)E[f (χ)2] - E[f (χ) ∙ g(χ)] + μ(E[w(χ) ∙ f (χ)] - b)+ E[α(χ)(f (χ) - 1)] - E[β(χ)f (χ)],
where α(x),β(x) ≥ 0 and μ ∈ R are the dual variables.
Taking the derivative w.r.t. the optimization variable f(X) yields:
γf (x) = g(x) — μw(x) — α(x) + β(x)	(10)
Therefore, the dual problem becomes:
max	-(2γ)-1 E[(g(χ) — μw(χ) — α(χ) + β(χ))2] — bμ — E[α(χ)]
α(x),β(x)≥0
14
Under review as a conference paper at ICLR 2021
We use the substitution in Eq. (10) to rewrite it as:
.mi,nη(Y/2) E[f (x)2] + bμ + E[α(X)]
α(x),β(x)≥0
s.t.∀x ∈ X : Yf(x) = g(x) — μw(x) — α(x) + β(x)
Next, we eliminate the multiplier β (x) by replacing the equality constraint with an inequality:
min (y/2) E[f(x)2] + bμ + E[a(X)]
α(x)≥0
s.t.∀x ∈ X : g(x) — Yf(X) — μw(x) — a(x) ≤ 0
Finally, since a(x) ≥ 0 and ɑ(x) ≥ g(x) — Yf (x) — μw(x), the optimal solution is the minimizer
to:
fm→r (Y/2闽f(X)2]+bμ+E[max{0, g(X)-Yf(X) - μw(X川
Next, let μ? be the optimal solution of the dual variable μ. Then, the optimization problem over f
decomposes into separate problems, one for each x ∈ X. We have:
f(x) = arg min ( (γ∕2)τ2 + [g(x) — YT — μ? w(x)]+ 卜
τ∈R
Using the same argument in Appendix B, we deduce that f(x) is of the form:
{0,	g(x) — μ? w(x) ≤ 0
1	g(x) — μ? w(x) ≥ Y
(1/y) (g(x) — μ? w(x)) otherwise
Finally, the statement of the theorem holds by taking the limit as Y → 0+.	□
C.2 Excess Risk B ound
In this section, we write D to denote the underlying probability distribution and write S to denote
the uniform distribution over the training sample (a.k.a. empirical distribution).
The parameter ρ stated in the theorem is given by:
P =(1/2)(	max	Eχ[h*(x) | X ∈ Xk] + min	Eχ[h*(x) | X ∈ Xk])
k∈{1,2,...,K}	k∈{1,2,...,K}
Note that, by definition, the optimal classifier h? that satisfies statistical parity also satisfies the
constraint in (6) with this choice of ρ. Hence, with this choice of ρ, h? remains optimal among all
possible classifiers.
Observe that the decision rule depends on x only via f(x) ∈ [—1, +1]. Hence, we write z = f (X).
Since the thresholds are learned based on a fresh sample of data, the random variables zi are i.i.d.
In light of Eq. 9, We would like to minimize the expectation of the loss l(hγ, x) = —f (x) ∙ hγ(x)=
—z ∙ q(z) = Z(z) for some function q : [—1, +1] → [0,1] of the form shown in 2(a). Note that Z
is 2(1 + 1/Y)-Lipschitz continuous within the same group and sensitive class. This is because the
thresholds are always in the interval [—1 — Y, 1 + Y]; otherwise moving beyond this interval would
not change the decision rule.
Let hγ be the decision rule learned by the algorithm. Using Corollary 5 in (Xu and Mannor, 2010),
we conclude that with a probability of at least 1 — δ:
IED[l(hγ,X)] — ES[l(hγ,χ)]I ≤ Jnf n(R(1 + 1) +2∖j2(R + K)INg2 + 2log δ o (11)
R≥1	Y
Here, we used the fact that the observations f(X) are bounded in the domain [—1, 1] and that we can
first partition the domain into groups Xk (K subsets) in addition to partitioning the interval [—1, 1]
into R smaller sub-intervals and using the Lipschitz constant. Choosing R = N3 and simplifying
gives us with a probability of at least 1 — δ:
∣Ed[l(hγ, X)] — ES[l(hγ, X)]∣ ≤ 42+支 +2S2K +N2lθg δ-
15
Under review as a conference paper at ICLR 2021
The same bound also applies to the decision rule h； that results from applying optimal threshold
with width γ > 0 (here, “optimal” is with respect to the underlying distribution) because the -cover
(Definition 1 in (Xu and Mannor, 2010)) is independent of the choice of the thresholds. By the union
bound, we have with a probability of at least 1 - δ, both of the following inequalities hold:
IED[l(hγ, x)] - ES[l(hγ, x)]∣ ≤ 4(2+* +2j2κ +Nlog K
IED [l(hγ, x)] - ES [l(hγ, x)]∣ ≤ 42+^ +2《2K +jog V
N 3	N	IN
(12)
(13)
In particular:
〜	〜	4(2 + 1)
ED [l(hγ, x)] ≤ Es [l(hγ, x)]+	mγ
N 3
+2
∕2K + 2log 2
N N
≤ Es [l(hY, x)] + Y + 4⅛i)
N 3
≤ Ed [l(hγ, x)] + γ +8(2+11)
+2
∕2K + 2log 2
N N
+4
∕2K + 2log 2
N N
The first inequality follows from Eq. (12). The second inequality follows from the fact that hγ is an
empirical risk minimizer to the regularized loss, where E[f(x)2] ≤ 1 since f(x) ∈ [0,1]. The last
inequality follows from Eq. (13).
Finally, We know that the thresholding rule h； with width γ > 0 is, by definition, a minimizer to:
(γ∕2)E[h(x)2] - E[h(x) ∙ f (x)]
among all possible bounded functions h : X → [0, 1] subject to the desired fairness constraints.
Therefore, we have:
(γ∕2)E[hγ(x)2] - E[h；(x) ∙ f (x)] ≤ (γ∕2)E[h*(x)2] - E[h*(x) ∙ f (x)]
Hence:
E[l(hγ, x)] = -E[h；(x) ∙ f (x)] ≤ γ + E[l(h?, x)]
This implies the desired bound:
〜	8(2+ 1)
ED[l(hγ,x)] ≤ ED[l(h*,x)] + 2γ +.....—iɪ +4
N 3
2K + 2log 2
V N
Therefore, we have consistency if N → ∞, Y → 0+ and γN1 → ∞. For example, this holds if
γ = O(N-1).
So far, we have assumed that the output of the original classifier coincides with the Bayes regressor.
If the original classifier is Bayes consistent, i.e. E[∣2η(x) - 1 - f (x)∣] → 0 as N → ∞, then we
have Bayes consistency of the post-processing rule by the triangle inequality.
D Proof of Proposition 1
Proof. Since ∣ξγ(w)| ≤ 1, the gradient at a point X during SGD has a square '2-norm bounded by
||(1 + ρ + )2 at all rounds. Following the proof steps of (Boyd and Mutapcic, 2008) and using the
fact that projections are contraction mappings, one obtains:
ɪ X (EN - F (μ?)) ≤ ll”?112+||Y?||2 2+(1+P+巾α2
T	2Tα
t=1
=(1 + P+ f)2α + ∣∣μ?∣∣2 + I∣Y*I∣2
2	+	2Tα
16
Under review as a conference paper at ICLR 2021
By Jensen's inequality, We have T PT=I E[F(t)] ≤ E[F(μ)]. Plugging this into the earlier results
yields:
E[F] - F(μ?)) ≤
(1 + ρ + 心 + ∣∣μ? ||2 + ∣∣γ*∣∣2
2	+	2Tα
□
E Extension to Other Criteria
E.1 Controlling the Covariance
The proposed algorithm can, sometimes, be adjusted to control bias according to other criteria as
Well besides statistical parity. For example, We demonstrate in this section hoW the proposed post-
processing algorithm can be adjusted to control the covariance betWeen the classifier’s prediction
and the sensitive attribute When both are binary random variables.
Let a, b, C ∈ {0,1} be random variables. Let C (a, b) = E [a ∙ b] - E [a] ∙ E[b] be their covariance,
and C(a, b | c) their covariance conditioned on c:
C(a, b | c = C)= E[a ∙ b | C = c] - E[a | C = c] ∙ E[b | C = c].	(14)
Then, one possible criterion for measuring bias is to measure the conditional/unconditional covari-
ance betWeen the classifier’s predictions and the sensitive attribute When both are binary random
variables. Because the random variables are binary, it is straightforWard to shoW that achieving zero
covariance implies independence.
Suppose We have a binary classifier on the instance space X . We Would like to construct
an algorithm for post-processing the predictions made by that classifier such that We guarantee
|C f(x), 1S(x) | x ∈ Xk | ≤ , Where X = ∪kXk is a total partition of the instance space. Infor-
mally, this states that the fairness guarantee With respect to the senstiive attribute 1S : X → {0, 1}
holds Within each subgroup Xk .
We assume, again, that the output of the classifier f : X → [-1, +1] is an estimate to 2η(x) - 1,
Where η(x) = p(y = 1|x = x) is the Bayes regressor and consider randomized rules of the form:
~	.	,	__ -	r	-	r
h: X ×{0,1}×{1, 2,..., K} × [-1,1] → [0,1],
Whose arguments are: (i) the instance x ∈ X, (ii) the sensitive attribute 1S : X → {0, 1} , (iii)
the sub-group membership k : X → [K], and (iv) the original classifier’s score f(x). Because
randomization is sometimes necessary as proved in Section 4, h(x) is the probability of predicting
the positive class When the instance is x ∈ X .
n	ι	.	i i`	τ∖ τ ι ♦	1	∙ ιι ι	. ι /ɔ τ	T /	∖	_ Γr∖ -ɪ 1
Suppose We have a training sample of size N, Which We Will denote by S. Let qi = h(xi) ∈ [0, 1]
for the i-th instance in S. For each group Xk ⊆ S, the desired fairness constraint on the covariance
can be Written as:
∣Xk∣ IX(IS⑶-Pk qil ≤
i∈Xk
,
Where ρk = Ex[1S(x) ∣ x ∈ Xk]. This is because:
击 X(IS⑶-Pk) qi =击 X 1S⑶ f(i) - ∣Xk∣ X f(i)
i∈Xk	i∈Xk	i∈Xk
- , . ~, . . - .. - -~,∙. -
E[1s(x) ∙ f(x) ∣ X	∈ Xk]	- E[1s(x)∣ X	∈ Xk]	∙ Ef(X) ∣	X ∈ Xk]
,~ , . .
C(f(x), 1s(x) ∣ X ∈ Xk),
1	. 1	.	. 1 . • •	1	EI	/'	♦	1	. 1	T	1	. 1
Where the expectation is over the training sample. Therefore, in order to learn h, We solve the
regularized optimization problem:
N
min,	X(γ∕2) q2 - f(xi) qi s.t. ∀Xk ∈ G ： I X (1s (i) - Pk) qi∣ ≤ 6k (15)
0≤qi≤1
i	i=1	i∈Xk
Where γ > 0 is a regularization parameter and 6k = ∣Xk ∣ 6. This is of the same general form
analyzed in Section B.2. Hence, the same algorithm can be applied With b = 0 and zi = 1S (i) - Pk.
17
Under review as a conference paper at ICLR 2021
E.2 Impossibility Result
The previous algorithm for controlling covariance requires that the subgroups Xk be known in ad-
vance. Indeed, our next impossibility result shows that this is, in general, necessary. In other words,
a deterministic classifier f : X → {0, 1} cannot be universally unbiased with respect to a sensitive
class S across all possible known and unknown groups unless the representation x has zero mutual
information with the sensitive attribute or if f is constant almost everywhere. As a corollary, the
groups Xk have to be known in advance.
Proposition 2 (Impossibility result). Let X be the instance space and Y = {0, 1} be a target
set. Let 1S : X → {0, 1} be an arbitrary (possibly randomized) binary-valued function on X
and define γ : X → [0, 1] by γ(x) = p(1S (x) = 1 | x = x), where the probability is evaluated
over the randomness of 1s : X → {0,1}. Write γ = EX[γ(X)]. Then, for any binary predictor
f : X → {0, 1} it holds that
sup	{e∏(x ) ∣C (f(x ),γ(x )| ∏(x ))∣0 ≥ 1 Ex ∣γ(x) - Y∣∙min{Ef, 1- Ef},	(16)
π: X →{0,1}	2
where C f (X), γ (X)| π(X) is defined in Equation 14.
Proof. Fix 0 < β < 1 and consider the subset:
W = {x ∈ X : (γ(χ) — Y) ∙ (f (x) - β) > 0},
and its complement W = X \ W. Since f (x) ∈ {0,1}, the sets W and W are independent of β as
long as it remains in the open interval (0, 1). More precisely:
W = J Y(X)- 7 > 0 ∧	f (X) = I
IY(X)- 7 ≤ 0 ∧	f(x)=0
Now, for any set X ⊆ X, let pX be the projection of the probability measure p(X) on the set X (i.e.
pX (X) = p(X)/p(X)). Then, with a simple algebraic manipulation, one has the identity:
&〜PX [(γ(x) - YNf(X)- β)] = C(Y(x), f (x); X ∈ X) + (Ex〜PX [γ] - 7) •(&〜PX [f] - β)
(17)
By definition of W, we have:
EX〜PW [(y(x) - 7)(f(x) - β)] = EX〜PW [∣y(x) - Y||f (x) - β∣] ≥ min{β, 1 - β}Eχ〜PW |y(x) - 7|
Combining this with Eq. (17), we have:
C(Y(X),f(x); x ∈ W) ≥ min{β, 1 - β}Eχ〜PW IY(X)- 7∣ + (Ex〜PW [y] - Y)(β - EX〜PW [f]) (18)
Since the set W does not change when β is varied in the open interval (0, 1), the lower bound holds
for any value of β ∈ (0, 1). W set:
β = f = 1 (EX 〜PW f (x) + EX 〜PW f(x))	(19)
Substituting the last equation into Eq. (18) gives the lower bound:
C(Y(X),f(x); X ∈ W) ≥ min{f, 1 - f} ∙ EX〜PW IY(X)- 7∣
+	2(Ex〜PW [y] - 7) (Ex〜PWf (x) - EX〜PWf (x))	(20)
Repeating the same analysis for the subset W7 , we arrive at the inequality:
C(Y(X),f(x); X ∈ W) ≤ - min{∕, 1 - f} EX〜PW IY(X)- Yl
+	2(Ex〜PW [y] - 7) (Ex〜PWf (x) - EX〜PWf (x))	(21)
Writing π(X) = 1W (X), we have by the reverse triangle inequality:
E∏(X) ∣C(f (x), γ(x); ∏(x)) ∣ ≥ min{f, 1 - f} ∙ EXIY(X)- γ∣	(22)
Finally:
2/ ≥ p(x ∈ W) ∙ EX〜PW f (x) + p(x ∈ W) ∙ EX〜PW f (x)= E[f]
Similarly, we have 2(1 - f7) ≥ 1 - E[f]. Therefore:
min{∕, 1 - ∕}≥ 2 min{Ef, 1 - Ef}
Combining this with Eq. (22) establishes the statement of the proposition.	□
18