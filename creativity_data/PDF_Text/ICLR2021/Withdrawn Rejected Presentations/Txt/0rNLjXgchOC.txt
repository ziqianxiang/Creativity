Dissecting Hessian: Understanding Common
Structure of Hessian in Neural Networks
Yikai Wu*
Department of Computer Science
Duke University
Durham, NC 27708
yikai.wu@duke.edu
Chenwei Wu
Department of Computer Science
Duke University
Durham, NC 27708
chenwei.wu592@duke.edu
Xingyu Zhu*
Department of Computer Science
Duke University
Durham, NC 27708
xingyu.zhu@duke.edu
Annie Wang
Department of Computer Science
Duke University
Durham, NC 27708
annie.wang029@duke.edu
Rong Ge
Department of Computer Science
Duke University
Durham, NC 27708
rongge@cs.duke.edu
Abstract
Hessian captures important properties of the deep neural network loss landscape.
Previous works have observed low rank structure in the Hessians of neural networks.
We make several new observations about the top eigenspace of layer-wise Hessian
-top eigenspaces for different models have surprisingly high overlap, and top
eigenvectors form low rank matrices when they are reshaped into the same shape
as the corresponding weight matrix. Towards formally explaining such structures
of the Hessian, we show that the new eigenspace structure can be explained by
approximating the Hessian using Kronecker factorization; we also prove the low
rank structure for random data at random initialization for over-parametrized
two-layer neural nets. Our new understanding can explain why some of these
structures become weaker when the network is trained with batch normalization.
The Kronecker factorization also leads to better explicit generalization bounds.
1 Introduction
The loss landscape for neural networks is crucial for understanding training and generalization. In this
paper we focus on the structure of Hessians, which capture important properties of the loss landscape.
For optimization, Hessian information is used explicitly in second order algorithms, and even for
gradient-based algorithms properties of the Hessian are often leveraged in analysis (Sra et al., 2012).
For generalization, the Hessian captures the local structure of the loss function near a local minimum,
which is believed to be related to generalization gaps (Keskar et al., 2017).
* Contributed equally, listed in alphabetical order
Preprint. Under review.
Several previous results including (Sagun et al., 2018; Papyan, 2018) observed interesting structures
in Hessians for neural networks - it often has around C large eigenvalues where C is the number of
classes. In this paper we ask:
Why does the Hessian of neural networks have special structures in its top eigenspace?
A rigorous analysis of the Hessian structure would potentially allow us to understand what the top
eigenspace of the Hessian depends on (e.g., the weight matrices or data distribution), as well as
predicting the behavior of the Hessian when the architecture changes.
Towards this goal, we first focus on the structure for the top eigenspace of layer-wise Hessians. We
observe that the top eigenspace of Hessians are far from random - models trained with different
random initializations still have a large overlap in their top eigenspace, and the top eigenvectors are
close to rank 1 when they are reshaped into the same shape as the corresponding weight matrix.
We formalize a conjecture that allows us to understand all these structures using a Kronecker
decomposition. We also analyze the Hessian in an over-parametrized two-layer neural network for
random data, proving that the output Hessian is approximately rank C - 1 and its top eigenspace can
be easily computed based on weight matrices.
1.1 Our Results
(b) Top 10 singular values of the top 4
eigenvectors of the layer-wise Hessian of
fc1:LeNet5 after reshaped as matrix.
(a) Overlap between dominate eigenspace of layer-wise Hessian at
different minima for fc1:LeNet5 (left) with output dimension 120
and conv11:ResNet18-W64 (right) with output dimension 64.
Figure 1: Some interesting observations on the structure of layer-wise Hessians. The eigenspace
overlap is defined in Definition 4.1 and the reshape operation is defined in Definition 4.2
Structure of Top Eigenspace for Hessians: Consider two neural networks trained with different
random initializations and potentially different hyper-parameters; their weights are usually nearly
orthogonal. One might expect that the top eigenspace of their layer-wise Hessians are also very
different. However, this is surprisingly false: the top eigenspace of the layer-wise Hessians have a
very high overlap, and the overlap peaks at the dimension of the layer’s output (see Fig. 1a). Another
interesting phenomenon is that if we express the top eigenvectors of a layer-wise Hessian as a matrix
with the same dimensions as the weight matrix, then the matrix is approximately rank 1. In Fig. 1b
we show the singular values of several such reshaped eigenvectors.
Understanding Hessian Structure using Kronecker Factorization: We show that both of these
new properties of layer-wise Hessians can be explained by a Kronecker Factorization. Under a
decoupling conjecture, we can approximate the layer-wise Hessian using the Kronecker product of
the output Hessian and input auto-correlation. This Kronecker approximation directly implies that
the eigenvectors of the layer-wise Hessian should be approximately rank 1 when viewed as a matrix.
Moreover, under stronger assumptions, we can generalize the approximation for the top eigenvalues
and eigenvectors of the full Hessian.
Structure of auto-correlation: The auto-correlation of the input is often very close to a rank 1
matrix. We show that when the input auto-correlation component is approximately rank 1, the
layer-wise Hessians indeed have high overlap at the dimension of the layer’s output, and the spectrum
of the layer-wise Hessian is similar to the spectrum of the output Hessian. On the contrary, when the
model is trained with batch normalization, the input auto-correlation matrix is much farther from
rank 1 and the layer-wise Hessian often does not have the same low rank structure.
2
Structure of output Hessian: For the output Hessian, we prove that in an over-parametrized two-
layer neural network on random data, the Hessian is approximately rank c - 1. Further, we can
compute the top c - 1 eigenspace directly from weight matrices. We show that this calculation can
be extended to multiple layers and the result has a high overlap with the actual top eigenspace of
Hessian in most settings.
Applications: As a direct application of our results, we show that the Hessian structure can be used
to improve the PAC-Bayes bound computed in Dziugaite & Roy (2017).
2	Related Works
Hessian-based analysis for neural networks (NNs): Hessian matrices for NNs reflect the second
order information about the loss landscape, which is important in characterizing SGD dynamics
(Jastrzebski et al., 2019) and related to generalization (Li et al., 2020), robustness to adversaries
(Yao et al., 2018) and interpretation of NNs (Singla et al., 2019). People have empirically observed
several interesting phenomena of the Hessian, e.g., the gradient during training converges to the top
eigenspace of Hessian (Gur-Ari et al., 2018; Ghorbani et al., 2019), and the eigenspectrum of Hessian
contains a “spike" which has about c - 1 large eigenvalues and a continuous “bulk" (Sagun et al.,
2016, 2018; Papyan, 2018).
Understanding structures of Hessian: People have developed different frameworks to explain the
low rank structure of the Hessians including hierarchical clustering of logit gradients (Papyan, 2019,
2020), independent Gaussian model for logit gradients (Fort & Ganguli, 2019), and Neural Tangent
Kernel (Jacot et al., 2020). These results work in different settings but are not directly comparable
(among themselves and our paper). A distinguishing feature of this work is that we can characterize
the top eigenspace of the Hessian directly by the weight matrices of the network.
Theoretical analysis for large eigenvalues of Fisher Information Matrices (FIM): The FIM can
be seen as a component of the Hessian matrix for neural networks. Karakida et al. (2019b) showed
that the largest c eigenvalues of the FIM for a randomly initialized neural network are much larger
than the others. Their results rely on the eigenvalue spectrum analysis in Karakida et al. (2019c,a),
which assumes the weights used during forward propagation are drawn independently from the
weights used in back propagation (Schoenholz et al., 2017).
Layer-wise Kronecker factorization for training NNs: The idea of approximating the FIM using
Kronecker factorizations can be dated back to Heskes (2000). More recently Martens & Grosse (2015)
proposed Kronecker-factored approximate curvature (K-FAC) which approximates the inverse of FIM
using layer-wise Kronecker product and perform approximated natural gradient descent (NGD) in
training NNs. Kronecker factored eigenbasis has also been utilized (George et al., 2018). K-FAC has
been generalized to convolutional and recurrent NNs (Grosse & Martens, 2016; Martens et al., 2018),
Bayesian deep learning (Zhang et al., 2018), and structured pruning (Wang et al., 2019). Unlike
these previous works which focus on accelerating computations, in this paper we use Kronecker
factorization to explain the structures of Hessians.
PAC-Bayes generalization bounds: People have established generalization bounds for neural net-
works under PAC-Bayes framework by McAllester (1999). This bound was further tightened by
Langford & Seeger (2001), and Catoni (2007) proposed a faster-rate version. For neural networks, Dz-
iugaite & Roy (2017) proposed the first non-vacuous generalization bound, which used PAC-Bayesian
approach with optimization to bound the generalization error for a stochastic neural network. Their
bound was then extended to ImageNet scale by Zhou et al. (2019) using compression.
3	Preliminaries and Notations
Basic Notations: In this paper, we generally follow the default notation suggested by Goodfellow
et al. (2016). Additionally, for a matrix M, let kM kF denote its Frobenius norm and kM k denote
its spectral norm. For two matrices M ∈ Ra1×b1, N ∈ Ra2 ×b2, let M 0 N ∈ R(a1a2)×(blb2) be
their Kronecker product such that [M 0 Nk1)×a2+i2,(j1-1)×b2+j2 = Mi1,i2 Nj1,j2.
Neural Networks: We consider classification problems with cross-entropy loss. For a c-class
classification problem, we are given a collection of training samples S = {(xi , yi)}iN=1 where
∀i ∈ [N], (xi, yi) ∈ Rd × Rc. We assume S is i.i.d. sampled from the underlying data distribution D.
3
Consider an L-layer fully connected ReLU neural network without skip connection fθ : Rd → Rc .
With σ(x) = x1x≥0 as the Rectified Linear Unit (ReLU) function, the output of this network is a
series of logits z ∈ Rc computed recursively as
z(p) := W (p)x(p) + b(p)
x(p) := σ(z(p)).
(1)
(2)
Here x(p) and z(p) are called the input and output of the p-th layer, and we set x(1) = x, z :=
fθ(x) = z(L). We denote θ := (w(1), b(1), w(2), b⑵，…，W(L), b(L)) ∈ RP the parameters of the
network. In particular, w(i) is the flattened i-th layer weight coefficient matrix W(i) and b(i) is its
bias vector. For convolutional networks, a similar analogue is presented in Appendix A.2.
For a single input x ∈ Rd with label y and logit output z, let n(p) and m(p) be the lengths
of x(p) and z(p). For convolutional layers, we consider the number of output channels as m(p)
and width of unfolded input as n(p). Note that x(1) = x, z(L) = z = fθ(x). We also denote
p := softmax(z) = ez/ Pic=1 ezi the output confidence.
With the loss function `(p, y) = - Pic=1 yi log(pi) ∈ R+ being the cross-entropy loss between the
softmax of logits z = fθ(xi) ∈ Rc and the one-hot label y ∈ Rc, the training process of the neural
network optimizes parameter θ to minimize the empirical training loss:
1N
L⑹=N X 'fθ (Xj …(χ,E∈s ['(Z, y)].
(3)
Hessians: Fixing the parameter θ, We use H'(v, x) to denote the Hessian of some vector V with
respect to scalar loss function ` at input x.
H'(v, X)= VV'(fe(x), y) = VV'(z, y).	(4)
Note v here can be any vector. For example, the parameter Hessian is H'(θ, x), where V = θ. The
layer-wise weight Hessian of the p-th layer is H'(w(p), x).
For simplicity, define E as the empirical expectation over the training sample S unless explicitly
stated otherwise. We focus on the layer-wise weight Hessians Hl(w^p^') = E[H'(w(p), x)] with
respect to loss, which are diagonal blocks in the full Hessian Hl(Θ) = E[H'(θ, x)] corresponding
to the cross terms between the weight coefficients of the same layer. We define Mxp) := H'(z(p), x)
as the Hessian of output z(p) with respect to empirical loss. With the notations defined above, we
have the p-th layer-wise hessian for a single input as
h`(w(P), x) = Vw(P)'(z, y) = Mxp)乳(X(P)X(P)T).	(5)
It follows that
HL(W(P)) = E [Mxp)㊈ X(P)X(P)[ = E [M ㊈ xxτ] .	(6)
The subscription x and the superscription (p) will be omitted when there is no confusion, as our
analysis primarily focuses on the same layer unless otherwise stated.
4	Kronecker Factorization of Hessian
The fact that layer-wise Hessian for a single sample can be decomposed into Kronecker product of
two components naturally leads to the following conjecture:
Conjecture (Decoupling Conjecture). The layer-wise Hessian can be approximated by a Kronecker
product of the expectation of its two components, that is
HL(W(P)) = E [M 0 XXT] ≈ E[M] 0 E[xxτ].	(7)
In particular, the top eigenvalues and eigenspace of HL(W(P)) is close to those of E[M] 0 E[xxτ].
Note that this conjecture is certainly true when M and xxτ are approximately statistically indepen-
dent. In Section 4.1 and Section 4.2 we will show that this conjecture is true in practice. We then
analyze the two components separately in Sections 4.3 and F.3.3.
4
Assuming the decoupling conjecture, we can analyze the layer-wise Hessian by analyzing the two
components separately. Note that E[M] is the Hessian of the layer-wise output with respect to
empirical loss, and E[xxT] is the auto-correlation matrix of the layer-wise inputs. For simplicity we
call E[M] the output Hessian and E[xxT] the input auto-correlation. For convolutional layers, we
define a similar factorization E[M]③ E[xxT] for the layer-wise Hessian, but with a different M
motivated by Grosse & Martens (2016). (See Appendix A.2)
In this paper we also note that the off-diagonal blocks of the full Hessian can also be decomposed
similarly. We can then approximate each block using the Kronecker factorization, and when the
input auto-correlation matrices are close to rank 1, this allows us to approximate the eigenvalues and
eigenvectors of the full parameter Hessian. The details of this approximation is stated in Appendix B.
Experiment Setup: We conduct experiments on the CIFAR-10, CIFAR-100 (MIT) (Krizhevsky,
2009), and MNIST (CC BY-SA 3.0) (LeCun et al., 1998) datasets as well as their random labeled
versions, namely MNIST-R and CIFAR10-R. We use PyTorch (Paszke et al., 2019) framework for all
experiments. We used several different fully connected (fc) networks (a fc network with m hidden
layers and n neurons each hidden layer is denoted as F-nm), several variations of LeNet (LeCun et al.,
1998), VGG11 (Simonyan & Zisserman, 2015), and ResNet18 (He et al., 2016). More representative
results are included in Appendix E. The eigenvalues and eigenvectors of the exact layer-wise Hessians
are approximated using a modified Lanczos algorithm (Golmant et al., 2018) which is described in
detail in Appendix C. We use “layer:network” to denote a layer of a particular network. For example,
conv2:LeNet5 refers to the second convolutional layer in LeNet5.
4.1	Approximation of Layer-wise Hessian and Full Hessian
We compare the top eigenvalues and eigenspaces of the approximated Hessian and the true Hessian.
We use the standard definition of subspace overlap to measure the similarity between top eigenspaces.
Definition 4.1 (Subspace Overlap). For k-dimensional subspaces U , V in Rd (d ≥ k) where the
basis vectors ui ’s and vi ’s are column vectors, with φ as the size k vector of canonical angles
between U and V , we define the subspace overlap of U and V as
OverlaP(U, V):= ∣∣UTV∣∣F/k = ∣∣ cosφ∣∣2∕k.	(8)
Note that when k = 1, the overlap is equivalent to the squared dot product between the two vectors.
2.0-
1.5-
1.0-
0.5-
0.0-
♦**********
Approximated
Exact
10	20
The i-th eigenvalue
2-
1-
0」
(b) Top eigenspace of
layer-wise Hessians
Approximated
Exact
10	20
The i-th eigenvalue
(d) Top eigenspace of the
full Hessian
(a) Top eigenvalues of
layer-wise Hessian of fc1
(c) Top eigenvalues of the
full Hessian
Figure 2: Comparison between the approximated and true layer-wise Hessian of F-2002 .
As shown in Fig. 2, this approximation works reasonably well for the top eigenvalues and eigenspaces
of both layer-wise weight Hessians and the full parameter Hessian.
4.2	Eigenvector Correspondence for Layer-wise Hessians
Suppose the i-th eigenvector for E[xxT] is vi and the j-th eigenvector for E[M] is uj . Then
the Kronecker product E[M] 0 E[xxT] has an eigenvector Uj 0 v%. Therefore if the decoupling
conjecture holds, one would expect that the top eigenvector of the layer-wise Hessian has a clear
correspondence with the top eigenvectors of its two components. Since u 0 v is just the flattened
matrix uvT , we may naturally define the following reshape operation.
Definition 4.2 (Layer-wise Eigenvector Matricization). Consider a layer with input dimension n and
output dimension m. For an eigenvector h ∈ Rmn of its layer-wise Hessian, the matricized form of
h is Mat(h) ∈ Rm×n where Mat(h)i,j = h(i-1)m+j.
5
More concretely, to demonstrate the correspondence between the eigenvectors of the layer-wise
Hessian and the eigenvectors of matrix E[M] and E[xxT], we introduce “eigenvector correspondence
matrices” as shown in Fig. 3.
Definition 4.3 (Eigenvector Correspondence Matrices). For layer-wise Hessian matrix H ∈
RmnXmn with eigenvectors h1, ∙ ∙ ∙ , hmn, and its corresponding auto-correlation matrix E[xxT] ∈
RnXn with eigenvectors v1, ∙ ∙ ∙ , vn. The correspondence between Vi and hj∙ can be defined as
Corr(vi, hj) := k Mat(hj)vi k2.	(9)
For the output Hessian matrix E[M ] ∈ Rmxm With eigenvectors u1,…,Um, We can likewise
define correspondence between vi and hj as
Corr(ui, hj) := k Mat(hj)Tuik2	(10)
We may then define the eigenvector correspondence matrix between H and E[xxT] as a n × mn
matrix whose i,j-th entry is Corr(vi, hj), and the eigenvector correspondence matrix between H
and E[M] as a m × mn matrix whose i, j-th entry is Corr(ui, hj ).
Intuitively, if the i, j -th entry of the correspondence matrix is close to 1, then the eigenvector hj is
likely to be the Kronecker product of vi (or ui) with some vector. If the decoupling conjecture holds,
every eigenvector of the layer-wise Hessian (column of the correspondence matrices) should have a
perfect correlation of 1 with exactly one of vi and one of ui .
(a) H with E[xxT]
0	25	50	75	100	125	150	175	- 0.0
(b) H with E[M]	(c) H with E[xxτ]	(d) H with E[xxτ]
Figure 3: Heatmap of Eigenvector Correspondence Matrices for fc1:LeNet5, which has 120 output
neurons. Here we take the top left corner of the eigenvector correspondence matrices. Similarities
between (a)(c) and (b)(d) respectively verify the decoupling conjecture.
In Fig. 3 we can see that the correspondence matrices for the true layer-wise Hessian H approximately
satisfies this property for top eigenvectors. The similarity between the correspondence patterns for
ʌ
the true and Kroneckor product approximated Hessian H also verifies the validity of the Kronecker
approximation for dominate eigenspace. From Fig. 3a and Fig. 3c, the top m eigenvectors of the
true layer-wise Hessian and the approximated Hessian are all highly correlated with v1 , the first
eigenvector of E[xxT]. From Fig. 3b and Fig. 3d, the correspondence with the E[M] component has
a near diagonal pattern for both the true Hessian and the Kronecker approximation. Thus for small i
we have hi ≈ v1 0 Ui.
4.3	Structure of Input Auto-correlation Matrix E[xxT]
To understand the structure of the auto-correlation matrix, akey observation is that the input x for most
layers are outputs of a ReLU, hence it is nonnegative. We can decompose the auto-correlation matrix
as E[xxT] = E[x]E[x]T + E[(x - E[x])(x - E[x])T. We denote Σx := E[(x - E[x])(x - E[x])T]
the auto-covariance matrix. As every sample x is nonnegative, the expectation E[x]E[x]T has a large
norm and usually dominates the covariance matrix Σx .
We empirically verified this phenomenon on various networks and datasets. The first eigenvector of
E[xxT] has a very high overlap with E[x] (squared dot product mean: 0.997, range: 0.964-1.000).
Meanwhile ∣∣E[x]E[x]T∣∣ is significantly larger than ∣∣Σx∣∣ in our experiments (∣∣E[x]E[x]T∣∣∕k∑χj∣
mean: 12.08, range: 2.28-30.03). This suggests that E[x]E[x]T is approximately equal to E[xxT]
and dominates the covariance Σx . Similar phenomenon also exists for convolution layers. The
complete experiment results are provided in Appendix E.1. We also observe the E[xxT] matrices are
all close to rank 1 throughout the training trajectory as shown in Appendix E.4.
6
4.4	Low Rank Structure of E[M]
Previous works observed the gap in Hessian eigenspectrum around the number of classes c (where
c = 10 in our experiments on CIFAR10 and MNIST). Since E[xxT] is close to rank 1 and the
Kronecker factorization is a good approximation for top eigenspace, the top eigenvalues of layer-wise
Hessian can be approximated as the top eigenvalues of E[M] multiplied by the first eigenvalue
of E[xxT]. Thus, the top eigenvalues of Hessians should have the same relative ratios as the top
eigenvalues of their corresponding E[M]’s. Therefore, the outliers should also appear in E[M].
×10-3	×10-1
0	10 20 30	0	10	20	30
E[M ]	HL
(a) fc1:LeNet5 at initialization
(CIFAR10).
×10-5
0	10	20	30	0	10 20 30
E[M ]	HL
(b) fc1:LeNet5 at minimum
(CIFAR10).
(c) fc1:LeNet5 at minimum
(CIFAR10-R).
Figure 4: Eigenspectrum of the layer-wise output Hessian E[M] and the layer-wise weight Hessian
HL(w(p)). The vertical axes denote the eigenvalues. Similarity between the two eigenspectra is a
direct consequence of a low rank E[xxT] and the decoupling conjecture.
Fig. 4 shows the similarity of eigenvalue spectrum between E[M] and layer-wise Hessians in different
situations, which agrees with our prediction. However, the eigengap only appear at initialization and
at minimum for true labels (Fig. 4a and Fig. 4b), but not at minimum for random labels (Fig. 4c). To
understand the structure of E[M] itself, we consider a simplified setting where we have a random
two-layer neural network with random data.
Theorem 4.1 (informal). For a two-layer neural network with Gaussian input, at initialization, when
the network is large, the output Hessian of the first layer is approximately rank (c - 1) and the
corresponding top eigenspace is R(W⑵)\{ W⑵∙ 1} and R(W⑵)denotes the row space of the
weight matrix W(2) of the second layer.
The formal statement of this theorem and the full proof is in Appendix H. The closed form cal-
culation can be heuristically extended to the case with multiple layers, that the top eigenspace
of the output HeSSian of the k-layer would be approximately R(S(k)) \ {S(k) ∙ 1}, where
S⑹=WS)WST)…W(k+I) and R(S⑹)is the row space of S⑻.
Though our result was only proven for random initialization and random data, we observe that this
subspace also has high overlap with the top eigenspace of output Hessian at the minima of models
trained with real datasets. The corresponding empirical results are shown in Appendix H.4.
5	Understanding Structures of Layer-wise Hessian
5.1	Eigenspace Overlap of Different Models
Several interesting structures of layer-wise Hessians can be explained using the decoupling conjecture
and eigenvalue correspondence. Consider models with the same network structure that are trained
on the same dataset using different random initializations, despite no obvious similarity between
their parameters, we observe surpisingly high overlap between the dominating eigenspace of their
layer-wise Hessians.
Fig. 5 includes 4 different variants of LeNet5 trained on CIFAR10, 3 different variants of ResNet18
trained on CIFAR100, and 3 different variants of VGG11 trained on CIFAR100. For each structural
variant, 5 models are trained from independent random initializations. We plot the average pairwise
overlap between the top eigenspaces of those models’ layer-wise Hessians. In each figure, we vary
the number of output neuron/channels. For the same structure, the top eigenspaces of different models
exhibits a highly non-trivial overlap which peaks near m - the dimension of the layer,s output.
7
0.0 ,
0
4 3 2 1
- - - -
Oooo
dɪ①Ao
50	100
Dimension k
0.0
0
5 4 3 2 1
-----
Ooooo
dɪ①AO
m = 48
m = 64
m = 80
50	100
0.0
0
6 4 2
- - -
Ooo
dɪ①AO
100	200
Dimension k
(c) fc1:LeNet5 (CIFAR10) with
80/100/120/150 output neurons
Dimension k
(a)	conv12:ResNet18 (CIFAR100)
with 48/64/80 output channels
(b)	conv6:VGG11 (CIFAR100) with
48/64/80 output channels
Figure 5: Overlap between the top k dominating eigenspace of different independently trained models.
The overlap peaks at the output dimension m. The eigenspace overlap is defined in Definition 4.1.
As we observed in Section 4.3, the auto-correlation matrix E[xxT] is approximately E[x]E[x]T .
Thus if the i-th eigenvector of E[M] is ui, the i-th eigenvector of the layer-wise Hessian would be
∕∖ ∕∖
close to Ui 0 E[x], where E[x] = E[x]∕∣∣E[x]k, is normalized E[x]. Even though the directions of
ui’s can be very different for different models, at rank m these vectors always span the entire space,
∕∖
as a result the top-m eigenspace for layer-wise Hessian is close to Im 0 E[x].
∕∖ ∕∖
Now suppose we have two different models with E[x]1 and E[x]2 respectively. Their top-m
∕∖ ∕∖
eigenspaces are close to Im 0 E[x]1 and Im 0 E[x]2 respectively. In this case, it is easy to check
that the overlap at m is approximately (E[x]TE[x]2)2. Since E[x]ι and E[x]2 are the same for the
input layer and all non-negative for other layers, the inner-product between them is large and the
overlap is expected to be high at dimension m.
Our explanations of the overlap relies on two properties: the auto-correlation matrix needs to be very
close to rank 1 and the eigenspectrum of output Hessian should have a heavy-tail. While both are
true in many shallow networks and in later layers of deeper networks, they are not satisfied for earlier
layers of deeper networks. In Appendix F.3 we explain how one can still understand the overlap using
correspondence matrices when the above simplified argument does not hold.
5.2 Dominating Eigenvectors of Layer-wise Hessian are Low Rank
A natural corollary for the Kronecker factorization approximation of layer-wise Hessians is that the
eigenvectors of the layer-wise Hessians are low rank. Let hi be the i-th eigenvector of a layer-wise
Hessian. The rank of Mat(hi) can be considered as an indicator of the complexity of the eigenvector.
∕∖
Consider the case that hi is one of the top eigenvectors. From Section 5.1, we have hi ≈ ui 0 E[x].
Thus, Mat(hi) ≈ UiE[x]T, which is approximately rank 1.
Experiments shows that first singular values of Mat(hi) divided by its Frobenius Norm are usually
much larger than 0.5, indicating the top eigenvectors of the layer-wise Hessians are very close to rank
1. Fig. 22 shows first singular values of Mat(hi) divided by its Frobenius Norm for i from 1 to 200.
We can see that the top eigenvectors of the layer-wise Hessians are very close to rank 1.
ConvI	conv2	fc1	fc2	fc3
1.0-
0.8 -
0.6 -
0.4-
0.2-
0.0-
0	20	40	0	20	40	0	20	40	0	20	40	0	20	40
Figure 6: Ratio between top singular value and Frobenius norm of matricized dominating
eigenvectors. (LeNet5 on CIFAR10). The horizontal axes denote the index i
of eigenvector hi, and the vertical axes denote k Mat(hi)k/k Mat(hi)kF.
8
5.3 Batch Normalization and Zero-mean Input
According to our explanation, the good approximation and high overlap of top eigenspace both
depend on the low rank structure of E[xxT]. Also, the low rank structure is caused by the fact that
E[x]E[x]T dominates Σx in most cases. Therefore, it’s natural to conjecture that models trained
using Batch Normalization (BN) (Ioffe & Szegedy, 2015) will change these phenomena as E[x]
will be zero and E[xxT] = Σx for those models. Indeed, as shown in Ghorbani et al. (2019), BN
suppresses the outliers in the Hessian eigenspectrum and Papyan (2020) provided an explanation.
We experiment on our networks with BN. The results are shown in Appendix F.4. We found that
E[xxT] is no longer close to rank 1 for models trained with BN. However, E[xxT] still have a few
large eigenvalues. In this case, all the previous structures (c outliers, high eigenspace overlap, low
rank eigenvectors) become weaker. The decoupling conjecture itself also becomes less accurate.
However, the approximation still gives meaningful information.
6	Tighter PAC-Bayes Bound with Hessian Information
PAC-Bayes bound is commonly used to derive upper bounds on the generalization error.
Theorem 6.1 (PAC-Bayes Bound). (McAllester, 1999; Langford & Seeger, 2001) With the hypothesis
space H parametrized by model parameters. For any prior distribution P in H that is chosen
independently from the training set S, and any posterior distribution Q in H whose choice may
inference S, with probability 1 - δ,
DKL (e(Q)I∣e(Q)) ≤
DKL (QIIP) + log 等
ISI-1
(11)
where e(Q) is the expected classification error for the posterior over the underlying data distribution
and e(Q) is the classification error for the posterior over the training set.
Intuitively, if one can find a posterior distribution Q that has low loss on the training set, and is
close to the prior P, then the generalization error on Q must be small. Dziugaite & Roy (2017)
uses optimization techniques to find an optimal posterior in the family of Gaussians with diagonal
covariance. They showed that the bound is nonvacuous for several neural network models.
We follow Dziugaite & Roy (2017) to set the prior P to be a multi-variant Gaussian. The covariance is
a multiple of identity. Thus, it is invariant with respect to the change of basis. For the posterior, when
the variance in one direction is larger, the distance with the prior decreases; however this also has the
risk of increasing the empirical loss over the posterior. In general, one would expect the variance to
be larger along a flatter direction in the loss landscape. However, since the covariance matrix of Q is
fixed to be diagonal in Dziugaite & Roy (2017), the search of optimal deviation happens in standard
basis vectors which are not aligned with the local loss landscape.
Using the Kronecker factorization as in Eq. (7), we can approximate the layer-wise Hessian’s
eigenspace. We set Q to be a Gaussian whose covariance is diagonal in the eigenbasis of the layer-
wise Hessians. We expect the alignment of sharp and flat directions will result in a better optimized
posterior Q and thus a tighter bound on classification error.
We perform the same optimization process as proposed by Dziugaite & Roy (2017). Our algorithm
is called Approx Hessian (APPR) when we fix the layer-wise Hessian eigenbasis to the one at the
minimum and Iterative Hessian (ITER) when we update the eigenbasis dynamically with the mean of
the Gaussian which is being optimized. To accelerate Iterative Hessian, we used generalization of
Theorem 4.1 to directly approximate the output hessian, which is then used to compute the eigenbasis.
We call this variant algorithm of Iterative Hessian with approximated output Hessian (ITER.M).
The results of this variant are only slightly worse than Iterative Hessian, which also suggests the
approximation of the output hessian is reasonable.
We used identical dataset, network structures and experiment settings as in Dziugaite & Roy (2017),
with a few adjustments in hyperparameters. We also added T-2002 used in Section 4. T-60010 and
T-200120 are trained on standard MNIST while all others are trained on MNIST-2 (see Appendix D.1).
The results are shown in Table 1 with a confidence of 0.965.
9
Table 1: Optimized PAC-Bayes bounds using different methods. T-nm and R-nm represents network
F-nm trained with true/random labels. TESTER. gives the empirical generalization gap. BASE
represents the bound given by the algorithm proposed by Dziugaite & Roy (2017). Appr, Iter, and
Iter.M represents the bound given by our algorithms.
Model	T-600	T-1200	T-3002	T-6002	R-600	T-60010	T-200210
TestEr.	0.015	0.016	0.015	0.015	0.493	0.018	0.021
Base	0.154	0.175	0.169	0.192	0.605	0.287	0.417
Appr (ours)	0.146	0.173	0.142	0.171	0.565	0.242	0.273
Iter (ours)	0.120	0.142	0.125	0.146	0.568	0.213	0.215
Iter.M (ours)	0.126	0.149	0.131	0.150	0.562	0.223	0.273
We also plotted the final posterior variance, s for network T-200120 in Fig. 7. For our algorithms,
Appr, Iter, and Iter.M, we can see that direction associated with larger eigenvalue has a smaller
variance. This agrees with our presumption that top eigenvectors are aligned with sharper directions
and should have smaller variance after optimization. Detailed algorithm description and experiment
results are shown in Appendix G.
10-1
10-2
10-3
10-4
10-5
Base	AppR	ITER	Iter.M
0	50000	100000 150000	0	50000	100000	150000	0	50000	100000	150000	0	50000	100000	150000
Figure 7: Optimized posterior variance s using different algorithms (fc1:T-2002 trained on MNIST).
The horizontal axis denotes the eigenbasis ordered with decreasing eigenvalues. The abbreviation of
algorithms are the same as in Table 1.
7	Conclusions
In this paper we identified two new surprising structures in the dominating eigenspace of layerwise
Hessians for neural networks. Specifically, the eigenspace overlap reveals a novel similarity between
different models. We showed that under a decoupling conjecture, these structures can be explained
by a Kronecker factorization. We analyze each component in the factorization, in particular we prove
that the output Hessian is approximately rank c - 1 for random two-layer neural networks. Our proof
gives a simple heuristic formula to estimate the top-eigenspace of the Hessian. As a proof of concept,
we showed that these structures can be used to find better explicit generalization bounds. Since
the dominating eigenspace of Hessian, which is the sharpest directions on the loss landscape, plays
an important role in both generalization (Keskar et al., 2017; Jiang et al., 2020) and optimization
(Gur-Ari et al., 2018), we believe our new understanding can benefit both fields. We hope this work
would be a starting point towards formally proving the structures of neural network Hessians.
Limitations and Open Problems Most of our work focuses on the layerwise Hessian except
for Section 4.1 and Appendix B. The eigenspace overlap phenomenon depends on properties of
auto-correlation and output Hessian, which are weaker for earlier layers of larger networks. Our
theoretical results need to assume the parameters are random, and only applies to fully-connected
networks. The immediate open problems include why the decoupling conjecture is correct and why
the output Hessian is low rank in more general networks.
10
References
Catoni, O. Pac-bayesian supervised classification: the thermodynamics of statistical learning. arXiv
preprint arXiv:0712.0248, 2007.
Dangel, F., Harmeling, S., and Hennig, P. Modular block-diagonal curvature approximations for
feedforward architectures. In International Conference on Artificial Intelligence and Statistics, pp.
799-808, 2020.
Dziugaite, G. K. and Roy, D. M. Computing nonvacuous generalization bounds for deep (stochastic)
neural networks with many more parameters than training data. In Proceedings of the Thirty-Third
Conference on Uncertainty in Artificial Intelligence, UAI, 2017.
Fort, S. and Ganguli, S. Emergent properties of the local geometry of neural loss landscapes. arXiv
preprint arXiv:1910.05929, 2019.
George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P. Fast approximate natural gradient
descent in a kronecker factored eigenbasis. In Advances in Neural Information Processing Systems,
pp. 9550-9560, 2018.
Ghorbani, B., Krishnan, S., and Xiao, Y. An investigation into neural net optimization via hessian
eigenvalue density. In International Conference on Machine Learning, pp. 2232-2241, 2019.
Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics,
pp. 249-256, 2010.
Golmant, N., Yao, Z., Gholami, A., Mahoney, M., and Gonzalez, J. pytorch-hessian-eigentings:
efficient pytorch hessian eigendecomposition, 2018. URL https://github.com/noahgolmant/
pytorch-hessian-eigenthings.
Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. MIT Press, 2016.
Grosse, R. and Martens, J. A kronecker-factored approximate fisher matrix for convolution layers. In
International Conference on Machine Learning, pp. 573-582, 2016.
Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient descent happens in a tiny subspace. arXiv preprint
arXiv:1812.04754, 2018.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016. doi:
10.1109/CVPR.2016.90.
Heskes, T. On “natural” learning and pruning in multilayered perceptrons. Neural Computation, 12
(4):881-901, 2000.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, pp. 448-456, 2015.
Jacot, A., Gabriel, F., and Hongler, C. The asymptotic spectrum of the hessian of DNN throughout
training. In 8th International Conference on Learning Representations, ICLR, 2020.
Jastrzebski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A. J. On the relation
between the sharpest directions of DNN loss and the SGD step length. In 7th International
Conference on Learning Representations, ICLR, 2019.
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S. Fantastic generalization measures
and where to find them. In International Conference on Learning Representations, 2020.
Karakida, R., Akaho, S., and Amari, S.-i. The normalization method for alleviating pathological
sharpness in wide neural networks. In Advances in Neural Information Processing Systems,
volume 32, pp. 6406-6416, 2019a.
Karakida, R., Akaho, S., and Amari, S.-i. Pathological spectra of the fisher information metric and its
variants in deep neural networks. arXiv preprint arXiv:1910.05992, 2019b.
11
Karakida, R., Akaho, S., and Amari, S.-i. Universal statistics of fisher information in deep neural
networks: Mean field approach. In The 22nd International Conference on Artificial Intelligence
and Statistics ,pp.1032-1041. PMLR, 2019c.
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training
for deep learning: Generalization gap and sharp minima. In 5th International Conference on
Learning Representations, ICLR, 2017.
Kleinman, D. and Athans, M. The design of suboptimal linear time-varying systems. IEEE Transac-
tions on Automatic Control, 13(2):150-159, 1968.
Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009.
Langford, J. and Seeger, M. Bounds for averaging classifiers. Technical report, 2001.
Laurent, B. and Massart, P. Adaptive estimation of a quadratic functional by model selection. Annals
of Statistics, pp. 1302-1338, 2000.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Li, X., Gu, Q., Zhou, Y., Chen, T., and Banerjee, A. Hessian based analysis of sgd for deep nets:
Dynamics and generalization. In Proceedings of the 2020 SIAM International Conference on Data
Mining, pp. 190-198. SIAM, 2020.
Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Martens, J., Ba, J., and Johnson, M. Kronecker-factored curvature approximations for recurrent
neural networks. In International Conference on Learning Representations, 2018.
McAllester, D. A. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999.
Papyan, V. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample
size. arXiv preprint arXiv:1811.07062, 2018.
Papyan, V. Measurements of three-level hierarchical structure in the outliers in the spectrum of
deepnet hessians. In International Conference on Machine Learning, pp. 5012-5021, 2019.
Papyan, V. Traces of class/cross-class structure pervade deep learning spectra. arXiv preprint
arXiv:2008.11865, 2020.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga,
L., and Lerer, A. Automatic differentiation in pytorch. Technical report, 2017.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,
S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035.
Curran Associates, Inc., 2019.
Sagun, L., Bottou, L., and LeCun, Y. Eigenvalues of the hessian in deep learning: Singularity and
beyond. arXiv preprint arXiv:1611.07476, 2016.
Sagun, L., Evci, U., Guney, V. U., Dauphin, Y. N., and Bottou, L. Empirical analysis of the hessian of
over-parametrized neural networks. In 6th International Conference on Learning Representations,
ICLR 2018, Workshop Track Proceedings, 2018.
Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J. Deep information propagation. In
International Conference on Learning Representations, ICLR, 2017.
Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition.
In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations,
ICLR, 2015.
12
Singla, S., Wallace, E., Feng, S., and Feizi, S. Understanding impacts of high-order loss approx-
imations and features in deep learning interpretation. In International Conference on Machine
Learning,pp. 5848-5856, 2019.
Skorski, M. Chain rules for hessian and higher derivatives made easy by tensor calculus. arXiv
preprint arXiv:1911.13292, 2019.
Sra, S., Nowozin, S., and Wright, S. J. Optimization for machine learning. Mit Press, 2012.
Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric
object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30
(11):1958-1970, 2008.
Van der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research,
9(11), 2008.
Wang, C., Grosse, R., Fidler, S., and Zhang, G. Eigendamage: Structured pruning in the kronecker-
factored eigenbasis. In International Conference on Machine Learning, pp. 6566-6575, 2019.
Yao, Z., Gholami, A., Lei, Q., Keutzer, K., and Mahoney, M. W. Hessian-based analysis of large batch
training and robustness to adversaries. In Advances in Neural Information Processing Systems, pp.
4949-4959, 2018.
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M. Pyhessian: Neural networks through the lens of
the hessian. arXiv preprint arXiv:1912.07145, 2019.
Zhang, G., Sun, S., Duvenaud, D., and Grosse, R. Noisy natural gradient as variational inference. In
International Conference on Machine Learning, pp. 5852-5861, 2018.
Zhou, W., Veitch, V., Austern, M., Adams, R. P., and Orbanz, P. Non-vacuous generalization bounds
at the imagenet scale: a pac-bayesian compression approach. In International Conference on
Learning Representations, 2019.
Zhu, S. A short note on the tail bound of wishart distribution. arXiv preprint arXiv:1212.5860, 2012.
13
Appendix Roadmap
1.	In Appendix A, we provide the detailed derivations of Hessian for fully-connected and
convolutional layers.
2.	In Appendix B, we provide detailed description on the approximation of dominating eigen-
vectors of the full hessian.
3.	In Appendix C, we explain how we compute the eigenvalues and eigenvectors of full and
layer-wise Hessian numerically.
4.	In Appendix D, we give the detailed experiment setups, including the datasets, network
structures, and the training settings we use.
5.	In Appendix E, we provide detailed experimental results that are not fully included in the
main text.
6.	In Appendix F, we provide additional and more general explanations of the phenomena we
found.
7.	In Appendix G, we give a detailed description of the PAC-Bayes bound that we optimize
and the algorithm we use to optimize the bound.
8.	In Appendix H, we provided the complete statement and proof for Theorem 4.1.
A Detailed Derivations
A.1 Derivation of Hessian
For an input x with label y, we define the Hessian of single input loss with respect to vector v as
H'(v, x) = VV'(fe(x), y) = VV'(zX, y).	(12)
We define the Hessian of loss with respect to v for the entire training sample as
NN
HL(V) = VVL(θ) = X VV'(fθ(Xi)仍)=X H'(v, Xi) = E [H'(v, x)].	(13)
i=1	i=1
We now derive the Hessian for a fixed input label pair (x, y). Following the definition and notations in
Section 3, we also denote output as z = fθ(x). We fix a layer p for the layer-wise Hessian. Here the
layer-wise weight Hessian is h`(W(P), x). We also have the output for the layer as z(p). Since W(P)
only appear in the layer but not the subsequent layers, we can consider z = fθ(x) = gθ(z(p) (w, x))
where gθ only contains the layers after the p-th layer and does not depend on W(P). Thus, using the
Hessian Chain rule (Skorski, 2019), we have
H'(w(p), x)=(告 J H'(z(P)，x)(信)+ X 亨 VW(p)落(14)
where zi(P) is the ith entry of z(P) and m(P) is the number of neurons in p-th layer (size of z(P)).
Since z(P) = W(P)x(P) + b(P) and W(P) = vec(W(P)) we have
∂ Z(P)
∂w (P)
Im(P) 0 X(P)T .
(15)
Since ∣W(⅛ does not depend on W(P), for all i we have Vw(P) Z(P) = 0. Thus,
H'(W(P), x) = (Im(P) 0 X(P)) H'(Z(P), x) (Im(P) 0 X(P)T) .	(16)
We define MxP) = H'(z(p), x) as in Section 3 so that
H'(w(P), x) =	(Im(P)	0	X(P))	MxP)	(Im(P)	0 X(P)T)	= MxP) 0	X(P)X(P)T	(17)
14
We now look into Mxp) = H'(z(p), x). Again We have Z = gθ(Z(P)) and can use chain rule here,
H'(Z(P),x)= (∂Z)TH'(z,x) (∂Z)+ Xd'∂Zy)Vz(P,Z	(18)
By letting p := softmax(Z) be the output confidence vector, we define the Hessian with respect to
output logit Z as Ax and have
Ax := H'(z, X) = Vzl(z, y) = diag(p) - PpT,	(19)
according to Singla et al. (2019).
We also define the Jacobian of Z with respect to Z (P) (informally logit gradient for layer p) as
Gxp) ：= ∂^Z⅞y∙ For FC layers with ReLUs, we can consider ReLU after the p-th layer as multiplying
Z(P) by an indicator function 1z(p)>0. To use matrix multiplication, we can turn the indicator function
into a diagonal matrix and define it as D (p) where
D(p) := diag (1z(p)>0).	(20)
Thus, we have the input of the next layer as x(p+1) = D(p)Z(p). The FC layers can then be considered
as a sequential matrix multiplication and we have the final output as
z = W(L)D(LT)W(LT)D(L-2)... D(P)Z(P).	(21)
Thus,
G(P) = dZ = W(l)d(l-1)w(L-I)D(L-2)... D(P)
x	∂Z (P)
Since G(xP) is independent of Z(P), we have
V2z(p)zi =0,∀i.
(22)
(23)
Thus,
Mxp) = h`(z(P), x) = Gxp)T AxGxp).	(24)
Moreover, loss Hessian with respect to the bias term b(P) equals to that with respect to the output of
that layer Z(P). We thus have
H'(b(p), x) = MxP) = Gxp)TAxGxp).
(25)
The Hessians of loss for the entire training sample are simply the empirical expectations of the
Hessian for single input. We have the formula as the following:
HL(W(P)) = E hH'(w(P), x)i = E [Mxp)㊈ X(P)X(P)Ti ,	(26)
HL(b(P)) = HL(Z(P)) =E hMx(P)i =E hG(xP)TAxG(xP)i .	(27)
Note that we can further decompose Ax = QTx Qx, where
Qx = diag (√p) (Ic - IcpT),	(28)
with 1c is a all one vector of size c, proved in Papyan (2019).
We can further extend the close form expression to off diagonal blocks and the bias entries to get the
full Gauss-Newton term of Hessian. Let
/Gx(I)T 脸 x(1) ∖
Gx(I)T
Gx⑵T 0 x(2)
FT =	Gx ⑵T	.	(29)
Gx(L)T ㊈ x(n)
Gx(L)T
The full Hessian is given by
HL(θ) = E FxTAxFx +E
X 3) Vθ Z
i=1	zi
(30)
15
A.2 Approximating Weight Hessian of Convolutional Layers
The approximation of weight Hessian of convolutional layer is a trivial extension from the approxi-
mation of Fisher information matrix of convolutional layer by Grosse & Martens (2016).
Consider a two dimensional convolutional layer of neural network with m input channels and n
output channels. Let its input feature map X be of shape (n, X1, X2) and output feature map Z be of
shape (m, P1, P2). Let its convolution kernel be of size K1 × K2. Then the weight W is of shape
(m, n, K1 , K2), and the bias b is of shape (m). Let P be the number of patches slide over by the
convolution kernel, we have P = P1P2.
Follow Dangel et al. (2020), we define Z ∈ Rm×P as the reshaped matrix of Z and W ∈ Rm×nK1 K2
as the reshaped matrix of W. Define B ∈ Rm×P by broadcasting b to P dimensions. Let X ∈
RnK1 K2 ×P be the unfolded X with respect to the convolutional layer. The unfold operation (Paszke
et al., 2019) is commonly used in computation to model convolution as matrix operations.
After the above transformation, we have the linear expression of the p-th convolutional layer similar
to FC layers:
Z(p) = W(p)X(p) + B(p)	(31)
We still omit superscription of (p) for dimensions for simplicity. We also denote z(p) as the vector
form of Z(p) and has size mP. Similar to fully connected layer, we have analogue of Eq. (17) for
convolutional layer as
H'(w(p), X) = (Im % X(P)) Mxp) (Im % X(P)T) ,	(32)
where Mxp) = H'(z(p), X) and is a mP X mP matrix. Also, since convolutional layers can also
be considered as linear operations (matrix multiplication with reshape) together with FC layers and
ReLUs, Eq. (23) still holds. Thus, we still have
H'(z(P), X) = Mxp) = GxTAxGxp),	(33)
where Gxp) = ∂zp) and has dimension C × mP, although is cannot be further decomposed as direct
multiplication of weight matrices as in the FC layers.
However, for convolutional layers, X(p) is a matrix instead of a vector. Thus, we cannot make
Eq. (32) into the form of a Kronecker product as in Eq. (17).
Despite this, it is still possible to have a Kronecker factorization of the weight Hessian in the form
H'(w(P), X) ≈ MXp) % X(P)X(P)T,	(34)
(p)
using further approximation motivated by Grosse & Martens (2016). Note that Mx need to have a
different shape (m × m) from Mxp) (mP × mP), since H'(w(p), X) is mnK 1K2 × mnK 1K2
and X(p)X(p)T is nK1K2 × nK1K2.
Since we can further decompose Ax = Qx TQx, we then have
Mx(p) =G(xp)TAxG(xp) = (QxGx(p))T (QxGx(p)) .	(35)
We define Nx(p) = QxGx(p). Here Qx is c × c and Gx(p) is c × mP so that Nx(p) is c × mP. We
can reshape Nxp) into a CP × m matrix Nxp). We then reduce Mxp) (mP × mP) into a m × m
matrix as
MXp) = P NXp)T Nxp).	(36)
The scalar P is a normalization factor since we squeeze a dimension of size P into size 1.
Thus, we can have similar Kronecker factorization approximation as
Hl(w^p)) = E [h`(w(P), X)i = E h(Im % X(P)) Mxp) (Im % X(P)T)i	(37)
≈ E [MXp) % X(P)X(P)Ti ≈ E [Mxp)i % E [x(P)X(P)Ti .	(38)
16
B Structure of Dominating Eigenvectors of the Full Hessian.
Although it is not possible to apply Kronecker factorization to the full Hessian directly, we can
construct an approximation of the top eigenvectors and eigenspace using similar ideas and our
findings.
In this section, we will always have superscript (p) for all layer-wise matrices and vectors in order to
distinguish them from the full versions.
As shown in Appendix A.1 Eq. (30), we have the full Hessian of fully connected networks as
Hl(Θ) = E [FχT AxFx] + E XX d'(z, y) Vθ& ,	(39)
i=1	zi
where
/Gx(I)T ㊈ x(1)\
Gx(I)T
Gx⑵T㊈X⑵
FxT =	Gx ⑵T	.	(40)
Gx(L)T ㊈ X⑺
Gx(L)T
In order to simplify the formula, we define
χ(p) = (x1p)
(41)
to be the extended input of the p-th layer. Thus, the terms in the Hessian attributed to the bias can be
included in the Kronecker product with the extended input, and FxT can be simplified as
/Gx(I)T ㊈ X(I) \
T Gx(2)T ㊈ X⑵
FxT =	.
(42)
∖Gx(L)T ㊈ XgtI
As discussed in several previous works (Sagun et al., 2016; Papyan, 2018, 2019; Fort & Ganguli,
2019), the full Hessian can be decomposed in to the G-term and the H-term. Specifically, the G-term
is E [FTAxFx], and the H-term is E [p；=i d'(Z'y) VθZii in Eq. (39).
Empirically, the G-term usually dominates the H-term, and the top eigenvalues and eigenspace of
the Hessian are mainly attributed to the G-term. Since we focus on the top eigenspace, we can
approximate our full Hessian using the G-term, as
HL(θ) ≈ E [FxTAxFx] .	(43)
In our approximation of the layer-wise Hessian HL(w(p)) Eq. (6), the two parts of the Kronecker
factorization are the layer-wise output Hessian E[Mx(p)] and the auto-correlation matrix of the input
E[X(p)X(p)T]. Although we cannot apply Kronecker factorization to E FxTAxFx , we can still
approximate its eigenspace using the eigenspace of the full output Hessian.
Note here that the full output Hessian is not a common definition. Let m = PL=I m(P) be the sum
of output dimension of each layer. We define a full output vector Z ∈ Rm by concatenating all the
layerwise outputs together,
/ Z(I) ∖
Z⑵
Z:=	.	.	(44)
.
.
Z(L)
17
We then define the full output Hessian is the Hessian w.r.t. Z. Let the full output Hessian for a single
input X be Mx ∈ Rm×m. Similar to Eq.(26), it can be expressed as
where
Mx ：= H'(Z, X) = GTAxGx,
(45)
G(1)T
x
G(2)T
GT =	Gx
.
.
G(L)T
Gx
similar to Eq. (42). The full output Hessian for the entire training sample is thus
HL(Z) = E[Mx] = E[GT AxGx].
(46)
(47)
We can then approximate the eigenvectors of the full Hessian HL(θ) using the eigenvectors of
E[Mx]. Let the i-th eigenvector of HL(θ) be vi and that of E[Mx] be ui. We may then break up
ui into segments corresponding to different layers as in
u(1)
ui
(2)
Ui
ui
(48)
/
where for all layer p,	ui(p)	∈ Rm(p) .	Motivated by the relation between	Gx	and	Fx,	the	i-th
eigenvector of HL(θ) can be approximated as the following. Let
wi
(U(I) 0 E[χ(1)]∖
u(2) 0 E[x(2)]
.
.
.
∖u(L) 0 E[x<l)]/
(49)
We then have
vi ≈
Wi
kwi k
(50)
We can then use the Gram-Schmidt process to get the basis vectors of the approximated eigenspace.
Another reason for this approximation is that the expectation is the input of each layer E[X(p)]
dominates its covariance as shown in Appendix E.1. Thus, the approximate is accurate for top
eigenvectors and also top eigenspace. For latter eigenvectors, the approximation would not be as
accurate since this approximate loses all information in the covariance of the inputs.
We also approximated the eigenvalues using this approximation. Let the i-th eigenvalue of HL (θ) be
λi and that of E[Mx] be σi. We have
λi ≈ σikwik2.
(51)
Below we show the approximation of the eigenvalues top eigenspace using this method. The
eigenspace overlap is defined as in Definition 4.1. We experimented on several fully connected
networks, the results shown below are for F-2002 (same as Fig. 2d in the main text), F-2004, F-6004,
and F-6008, all with dimension 50.
18
2.5-
2.0-
APProXimated
Exact
1.5-
1.0-
0.5-
0.0-
∣""".∙∙∙************.****..***...*.**.
0	10	20	30	40	50
The i-th eigenvalue
(a)	Eigenvalues for F-2002
(b)	Eigenspace overlap for F-2002
1.5-
1.0-
0.5-
0.0-
0	10	20	30	40	50
Approximated
Exact
The i-th eigenvalue
(c)	Eigenvalues for F-2004
(d)	Eigenspace overlap for F-2004
•	..	A______:... .x.j
1 25 _ *	a Approximated
• Exact
*
1.00-
0.75-	∖
X
0.50-
0.25-
0.00	...................
0	10	20	30	40	50
The i-th eigenvalue
0.10-
0.05-
0.00-Lr.
0
(e)	Eigenvalues for F-6004
(f)	Eigenspace overlap for F-6004
APProXimated
Exact
* ..........
* "*3*二：：：：：：二温：笈K般细般JI般般
10	20	30	40	50
The i-th eigenvalue


(g) Eigenvalues for F-6008	(h) Eigenspace overlap for F-6008
Figure 8: Top 50 Eigenvalues and Eigenspace approximation for full Hessian
19
C Computation of Hessian Eigenvalues and Eigenvectors
For Hessian approximated using Kronecker factorization, we compute E[M] and E[xxT] explicitly.
Let m and v be an eigenvector of E[M] and E[xxT] respectively, with corresponding eigenvalues
λm and λv. Sinceboth matrices are positive semi-definite, m0v is an eigenvector of E[M]0E[xxT]
with eigenvalue λmλv. In this way, since E[M] has m eigenvectors and E[xxT] has n eigenvectors,
we can approximate all mn eigenvectors for the layer-wise Hessian. All these calculation can be
done directly.
However, it is almost prohibitive to calculate the true Hessian explicitly. Thus, we use numerical
methods with automatic differentiation (Paszke et al., 2017) to calculate them. The packages we use
is Golmant et al. (2018) and we use the Lanczos method in most of the calculations. We also use
package in Yao et al. (2019) as a reference.
For layer-wise Hessian, we modified the Golmant et al. (2018) package. In particular, the package
relies on the calculation of Hessian-vector product Hv , where v is a vector with the same size as
parameter θ . To calculate eigenvalues and eigenvectors for layer-wise Hessian at the p-th layer, we
cut the v into different layers. Then, we only leave the part corresponding to weights of the p-th layer
and set all other entries to 0. Note that the dimension does not change. We let the new vector be v (p)
and get the value of u = Hv(p) using auto differentiation. Then, we do the same operation to u and
get u(p).
20
D Detailed Experiment Setup
D.1 Datasets
We conduct experiment on CIFAR-10, CIFAR-100 (MIT) (Krizhevsky, 2009) (https://www.cs.
toronto.edu/~kriz/cifar.html), and MNIST (CC BY-SA 3.0) (LeCun et al., 1998) (http://
yann.lecun.com/exdb/mnist/). The datasets are downloaded through torchvision (Paszke et al.,
2019) (https://pytorch.org/vision/stable/index.html). We used their default splitting of
training and testing set.
To compare our work on PAC-Bayes bound with the work of Dziugaite & Roy (2017), we created a
custom dataset MNIST-2 by setting the label of images 0-4 to 0 and 5-9 to 1. We also created random-
labeled datasets MNIST-R and CIFAR10-R by randomly labeling the images from the training set of
MNIST and CIFAR10. The dataset information is summarized in Table 2
Table 2: Datasets
Dataset	# Data Points		Input Size	# Classes	Label
	Train	Test			
CIFAR10	50000	10000	3 X 32 X 32	10	True
CIFAR10-R	50000	10000	3 × 32 × 32	10	Random
CIFAR100	50000	10000	3 X 32 X 32	100	True
MNIST	60000	10000	28 X 28	10	True
MNIST-2	60000	10000	28 X 28	2	True
MNIST-R	60000	10000	28 X 28	10	Random
All the datasets (MNIST, CIFAR-10, and CIFAR-100) we used are publicly available. According
to their descriptions on the contents and collection methods, they should not contain any personal
information or offensive content. MNIST is a remix of datasets from the National Institute of
Standards and Technology (NIST), which obtained consent for collecting the data. However, we
also note that CIFAR-10 and CIFAR-100 are subsets of the dataset 80 Million Tiny Image (Torralba
et al., 2008) (http://groups.csail.mit.edu/vision/TinyImages/), which used automatic
collection and includes some offensive images.
D.2 Network Structures
Fully Connected Network: We used several different fully connected networks varying in the
number of hidden layers and the number of neurons for each hidden layer. The output of all layers
except the last layer are passed into ReLU before feeding into the subsequent layer. As described in
Section 4.1, we denote a fully connected network with m hidden layers and n neurons each hidden
layer by F-nm. For networks without uniform layer width, we denote them by a sequence of numbers
(e.g. for a network with three hidden layers, where the first two layers has 200 neurons each and the
third has 100 neurons, we denote it as F-2002-100). For example, the structure of F-2002 is shown in
Table 3.
Table 3: Structure of F-2002 on MNIST
#	Name	Module	In Shape	Out Shape
1		Flatten	(28,28)	784
2	fc1	Linear(784, 200)	784	200
3		ReLU	200	200
4	fc2	Linear(200, 200)	200	200
5		ReLU	200	200
6	fc3	Linear(200, 10)	200	10
		output		
21
LeNet5: We adopted the LeNet5 structure proposed by LeCun et al. (1998) for MNIST, and slightly
modified the input convolutional layers to adapt the input of CIFAR-10 dataset. The standard LeNet5
structure we used in the experiments is shown in Table 4. We further modified the dimension of fc1
and conv2 to create several variants for the experiment in Section 5.1. Take the model whose first
fully connected layer is adjusted to have 80 neurons as an example, we denote it as LeNet5-(fc1-80).
Table 4: Structure of LeNet5 on CIFAR-10
#	Name	Module	In Shape	Out Shape
1	conv1	Conv2D(3, 6, 5, 5)	(3, 32, 32)	(6, 28, 28)
2		ReLU	(6, 28, 28)	(6, 28, 28)
3	maxpool1	MaxPooling2D(2,2)	(6, 28, 28)	(6, 14, 14)
4	conv2	Conv2D(6, 16, 5, 5)	(6, 14, 14)	(16, 10, 10)
5		ReLU	(16, 10, 10)	(16, 10, 10)
6	maxpool2	MaxPooling2D(2,2)	(16, 10, 10)	(16, 5, 5)
7		Flatten	(16, 5, 5)	400
8	fc1	Linear(400, 120)	400	120
9		ReLU	120	120
10	fc2	Linear(120, 84)	120	84
11		ReLU	84	84
12	fc3	Linear(84, 10)	84	10
		output		
Networks with Batch Normalization: In Appendix F.4 we conducted several experiments regard-
ing the effect of batch normalization on our results. For those experiments, we use the existing
structures and add batch normalization layer for each intermediate output after it passes the ReLU
module. In order for the Hessian to be well-defined, we fix the running statistics of batch normaliza-
tion and treat it as a linear layer during inference. We also turn off the learnable parameters θ and β
(Ioffe & Szegedy, 2015) for simplicity. For network structure X, we denote the variant with batch
normalization after all hidden layers X-BN. For example, the detailed structure LeNet5-BN is shown
in Table 5.
Table 5: Structure of LeNet5-BN on CIFAR-10
#	Name	Module	In Shape	Out Shape
1	conv1	Conv2D(3, 6, 5, 5)	(3, 32, 32)	(6, 28, 28)
2		ReLU	(6, 28, 28)	(6, 28, 28)
3		BatchNorm2D	(6, 28, 28)	(6, 28, 28)
4	maxpool1	MaxPooling2D(2,2)	(6, 28, 28)	(6, 14, 14)
5	conv2	Conv2D(6, 16, 5, 5)	(6, 14, 14)	(16, 10, 10)
6		ReLU	(16, 10, 10)	(16, 10, 10)
7		BatchNorm2D	(16, 10, 10)	(16, 10, 10)
8	maxpool2	MaxPooling2D(2,2)	(16, 10, 10)	(16, 5, 5)
9		Flatten	(16, 5, 5)	400
10	fc1	Linear(400, 120)	400	120
11		ReLU	120	120
12		BatchNorm1D	120	120
13	fc2	Linear(120, 84)	120	84
14		ReLU	84	84
15		BatchNorm1D	84	84
16	fc3	Linear(84, 10) output	84	10
22
Variants of VGG11: To verify that our results apply to larger networks, we trained a number
of variant of VGG11 (originally named VGG-A in the paper, but commonly refered as VGG11)
proposed by Simonyan & Zisserman (2015). For simplicity, we removed the dropout regularization
in the original network. To adapt the structure, which is originally designed for the 3 × 224 × 224
input of ImageNet, to 3 × 32 × 32 input of CIFAR-10.
Since the original VGG11 network is too large for computing the top eigenspace up to hundreds of
dimensions, we reduce the number of output channels of each convolution layer in the network to 32,
48, 64, 80, and 200. We denote the small size variants as VGG11-W32, VGG11-W48, VGG11-W64,
VGG11-W80, and VGG11-W200 respectively. We use conv1 - conv8 and fc1 to denote the layers of
VGG11 where conv1 is closest to the input feature and fc1 is the classification layer.
Variants of ResNet18: We also trained a number of variant of ResNet18 proposed by He et al.
(2016). As batch normalization will change the low rank structure of the auto correlation matrix
and reduce the overlap, we removed all batch normalization operations. Following the adaptation of
ResNet to CIFAR dataset as in https://github.com/kuangliu/pytorch-cifar, we changed
the input size to 3 × 32 × 32 and added a 1x1 convolutional layer for each shortcut after the first
block.
Similar to VGG11, we reduce the number of output channels of each convolution layer in the
network to 48, 64, 80. We denote the small size variants as ResNet18-W48, ResNet18-W64, and
ResNet18-W80 respectively.
We use conv1 - conv17 and fc1 to denote the layers of the ResNet18 backbone where conv1 is closest
to the input feature and fc1 is the classification layer. For the 1x1 convolutional layers in the shortcut,
we denote them by sc-conv1 - sc-conv3. where sc-conv1 is the convolutional layer on the shortcut of
the second ResNet block and sc-conv3 is the convolutional layer on the shortcut of the fourth ResNet
block.
D.3 Training Process and Hyperparameter Configuration
For all datasets, we used the default splitting of training and testing set. All models (except explicitly
stated otherwise) are trained using batched stochastic gradient descent (SGD) with batch-size 128
and fixed learning rate 0.01 for 1000 epochs. No momentum and weight decay regularization were
used. The loss objective converges by the end of training, so we may assume that the final models are
at local minima.
For generality we also used a training scheme with fixed learning rate at 0.001, and a training scheme
with fixed learning rate at 0.01 with momentum of 0.9 and weight-decay factor of 0.0005. Models
trained with these settings will be explicitly stated. Otherwise we assume they were trained with the
default scheme mentioned above.
Follow the default initialization scheme of PyTorch(Paszke et al., 2019), the weights of linear layers
and convolutional layers are initialized using the Xavier method (Glorot & Bengio, 2010), and bias
of each layer are initialized to be zero.
23
E Additional Experiment Results
E.1 Low Rank Structure of Auto-correlation Matrix E[xxT]
We have briefly discussed about the autocorrelation matrix E[xxT] being approximately rank 1 in
Section 4.3 in the main text. In particular, we claimed that the mean of layer input dominate the
covariance, that E[xxT] ≈ E[x]E[xT]. In this section we provide some additional empirical results
supporting that claim.
We use two metrics to quantify the quality of this approximation: the squared dot product between
normalized E[x] and the first eigenvector of E[xxT] and the ratio between the first and second
eigenvalue of E[xxT]. Intuitively if the first quantity is close to 1 and the second quantity is large,
then the approximation is accurate.
ʌ
Formally, for fully connected layers, define E[x] as the normalized expectation of the layer input x,
ʌ
namely E[x]/kE[x]k. For convolutional layers, following the notations in Appendix A.2, define E[x]
as the first left singular vector of E[X] where E[x] ∈ RnK1K2. Abusing notations for simplicity,
we use E[xxT] to denote the nK1K2 × nK1K2 matrix E[XXT]. In this section we consider the
squared dot product between E[x] and the first eigenvector vι of E[xxT], namely (VTE[x])2.
For the spectral ratio, let λ1 be the first eigenvalue of E[xxT] and λ2 be the second. We have
λι ≥ kE[x]E[x]Tk- k∑χk = kE[x]E[x]Tk _ 1
λ2 ≥	k∑χk	= k∑χk	,	( )
where Σx is the covariance of x. Thus, the spectral norm of E[x]E[x]T divided by that of Σx gives
a lower bound to λ1∕λ2. In our experiments, we usually have λ1∕λ2 ≥ kE[x]E[x]T k∕k∑χk.
Table 6: Squared dot product (VTE[x])2 and spectral ratio λ1∕λ2 for fully connected layers in a
selection of network structures and datasets. We independently trained 5 runs for each instance and
compute the mean, minimum, and maximum of the two quantities over all layers (except the first
layer which takes the input with mean-zero) in all runs.
Dataset	Network	# fc	mean	(VT E[x])2 min	max	mean	λ1 ∕λ2 min	max
MNIST	F-2002	2	1.000	1.000	1.000	12.29	9.65	16.16
	F-6002	2	0.999	0.999	0.999	12.00	11.42	13.00
	F-6004	4	1.000	0.999	1.000	17.81	7.33	28.00
	F-6008	8	0.991	0.965	1.000	6.63	2.28	11.15
CIFAR10	F-6002	2	0.999	0.998	1.000	9.24	4.74	13.74
	F-15003	3	0.999	0.997	1.000	13.27	6.10	18.41
	LeNet5	3	0.998	0.997	0.999	7.21	5.88	9.02
	LeNet5-(fc1-80)	3	0.998	0.996	0.999	7.80	6.77	11.01
	LeNet5-(fc1-100)	3	0.997	0.995	0.999	7.42	6.20	9.10
	LeNet5-(fc1-150)	3	0.998	0.992	0.999	7.35	5.34	9.62
	VGG11-W32	1	0.990	0.988	0.993	6.02	5.57	6.51
	VGG11-W64	1	0.996	0.993	0.999	5.87	5.32	6.26
	VGG11-W64	1	0.995	0.993	0.996	6.24	5.97	6.70
CIFAR100	VGG11-W48	1	0.999	0.999	0.999	17.861	15.456	20.491
	VGG11-W64	1	0.999	0.999	1.000	19.185	18.358	20.410
	VGG11-W80	1	0.999	0.999	1.000	19.455	18.120	21.450
	ResNet18-W48	1	1.000	1.000	1.000	28.23	27.37	29.27
	ResNet18-W64	1	1.000	1.000	1.000	27.07	25.72	29.50
	ResNet18-W80	1	1.000	1.000	1.000	28.23	25.98	30.03
24
Table 7: Squared dot product (VTE[x])2 and spectral ratio λ1∕λ2 for convolutional layers in the
selection of network structures and datasets in Table 6.
Dataset	Network	# conv	mean	(VT E[x])2 min	max	mean	λ1∕λ2 min	max
CIFAR10	LeNet5	1	0.999	0.998	0.999	15.87	11.15	27.20
	LeNet5-(fc1-80)	1	0.998	0.998	0.999	12.36	9.53	13.36
	LeNet5-(fc1-100)	1	0.999	0.999	0.999	19.49	16.69	21.92
	LeNet5-(fc1-150)	1	0.999	0.998	0.999	12.86	7.65	16.34
	VGG11-W32	7	0.995	0.991	0.999	5.31	2.39	9.09
	VGG11-W64	7	0.997	0.993	1.000	5.76	2.50	9.98
	VGG11-W64	7	0.998	0.995	1.000	5.81	2.53	10.62
CIFAR100	VGG11-W48	7	0.996	0.991	0.999	5.72	2.46	9.90
	VGG11-W64	7	0.995	0.991	0.999	5.66	2.50	10.79
	VGG11-W80	7	0.994	0.988	0.998	5.18	2.50	8.45
	ResNet18-W48	19	0.981	0.917	0.998	3.79	1.89	7.56
	ResNet18-W64	19	0.985	0.910	0.998	3.96	1.81	7.53
	ResNet18-W80	19	0.987	0.954	0.997	4.16	2.11	7.04
As we can see from Table 6 and Table 7, in a variety of settings, E[x]E[x]T indeed dominated
the autocorrelation matrix E[xxT] for fully connected layers. Similar phenomenon also holds for
convolutional layers in the modern architectures, but the spectral gap are generally smaller compared
to that of the fully connected layers.
E.2 Eigenspace Overlap Between Different Models
The non trivial overlap between top eigenspaces of layer-wise Hessians is one of our interesting
observations that had been discusses in Section 5.1. Here we provide more related empirical results.
Some will further verify our claim in Section 5.1 and some will appear to be challenge that. Both
results will be explained discussed more extensively in Appendix F.
E.2.1 Overlap preserved when varying hyper-parameters:
We first verify that the overlap also exists for a set of models trained with the different hyper-
parameters. Using the LeNet5 (defined in Table 4) as the network structure. We train 6 models using
the default training scheme (SGD, lr=0.01, momentum=0), 5 models using a smaller learning rate
(SGD, lr=0.001, momentum=0), and 5 models using a combination of optimization tricks (SGD,
lr=0.01, momentum=0.9, weight decay=0.0005). With these 16 models, we compute the pairwise
eigenspace overlap of their layer-wise Hessians (120 pairs in total) and plot their average in Fig. 9.
The shade areas in the figure represents the standard deviation. The pattern of overlap is clearly
preserved, and the position of the peak roughly agrees with the output dimension m, demonstrating
that the phenomenon is caused by a common structure instead of similarities in training process.
Figure 9: Eigenspace overlap of different models of LeNet5 trained with different hyper parameters.
Note that for fc3 (the final output layer), we are not observing a linear growth starting from 0 like
other layers. This can be explained by the lack of neuron permutation. Related details will be
discussed along with the reason for the linear growth pattern for other layers in Appendix F.3.
25
E.2.2 Eigenspace overlap for convolutional layers in large models:
Even though the exact Kroneckor Factorization for layer-wise Hessians is only well-defined for fully
connected layers, we also observe similar nontrivial eigenspace overlap for convolutional layers in
larger and deeper networks including variants of VGG11 and ResNet18 on datasets CIFAR10 and
CIFAR100. Some representative results are shown in Fig. 25 and Fig. 11. For each model on each
dataset, we independently train 5 models and compute the average pairwise eigenspace overlap. The
shade areas represents the standard deviation.
For most of the convolutional layers, the eigenspace overlap peaks around the dimension which is
equal to the number of output channels of that layer, which is similar to the layers in LeNet5 as in
Fig. 9. The eigenspace overlap of the final fully connected-layer also behaves similar to fc3:LeNet5,
which remains around a constant then drops after exceeding the dimension of final output. However,
there are also layers whose overlap does not peak around the output dimensions, (e.g. conv5 of
Fig. 10b) and conv7 of Fig. 11a). We will cluster these “failure cases” in the following paragraph.
(a) VGG11-W32 (CIFAR10)
COnv6 m
(b) VGG11-W200 (CIFAR10)
conv3 m = 200
200
fc1 m
20
Dimension
200
0.4
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.0
100
0.0
100	150
0.5
0.4
0.3
100
0.10
0.08
0.06
0.08
006
0.04
0.02
conv3 m = 80
100
100
0.00-H-
0
0.0
100	150
(c) VGG11-W48 (CIFAR100)
0.04
0.2
0.02
0.1
0.0
0.00
(d) VGG11-W80 (CIFAR100)
Figure 10: Top Eigenspace overlap for varients of VGG11 on CIFAR10 and CIFAR100
26
0.0
0.20
0.15
0.10
0.05
0.00
0.08 -
0.06 -
0.04 -
0.02 -
0.00 -
0.3 -
02
0.L
0.0-
0.5 -
0.4-
0.3 -
02
0.L
0.0-
0	100
Dimension
0	100	0	100
0.10
0.05
0.0
Dimension
Dimension
(a)	ResNet18-W48 (CIFAR100)
0.00
(b)	ResNet18-W64 (CIFAR100)
0	100
Dimension
(c) ResNet18-W80 (CIFAR100)
Figure 11:	Top Eigenspace overlap for variants of ResNet18 on CIFAR100
27
E.2.3 Failure Cases
As seen in Fig. 25 and Fig. 11, there is a small portion of layers, usually closer to the input, whose
eigenspace overlap does peak around the output dimensions. These layers can be clustered into the
following two general cases.
a. Early Peak of Low Overlap
For layers shown in Fig. 12. The overlap of dominating eigenspaces are significantly lower than the
other layers. Also there exists a small peak at very small dimensions.
0.05-
0.04-
0.03-
0.02-
0.01 -
0.00-
0	10	20	30
Dimension (Zoom In)
(a) fc2:F-2002
0.020
0.015
0.010
0.005
0.000
0	10	20	30
Dimension (Zoom In)
(b) conv5:VGG11-W200 (c) conv2:VGG11-W80 (d) conv5:ResNet18-W64
(MNIST)	(CIFAR10)	(CIFAR100)	(CIFAR100)
Figure 12:	Top eigenspace overlap for layers with an early low peak.
Figures in the second row are the zoomed in versions of the figures in the first row.
b. Delayed Peak / Peak Doesn’t Decline
For layers shown in Fig. 13, the top eigenspaces has a nontrivial overlap, but the peak dimension is
larger than predicted output dimension.
(a) conv2:VGG11-W200
(CIFAR10)
(b) conv7:VGG11-W48
(CIFAR100)
Figure 13: Top eigenspace overlap for layers with a delayed peak.
(c) conv7:VGG11-W48
(CIFAR100)
(d) conv7:ResNet18-W64
(CIFAR100)
However, the existence of such failure cases does not undermine the theory of Kronecker factorization
approximation. In fact, both “failure cases” appear because the top hessian eigenspace is not
completely spanned by E[x], and can be predicted by computing the auto correlation matrices and the
output Hessians. The details will also be elaborated in Appendix F.3 with the help of correspondence
matrices.
28
E.3 Eigenvector Correspondence
Here we present the correspondence matrix for fc1, fc2, conv1, and conv2 layer of LeNet5. The top
eigenvectors for all layers shows a strong correlation with the first eigenvector of E[xxT] (which is
ʌ
approximately E[x]). However, the diagonal pattern in the correspondence matrix with E[M] for fc2
is not as clear as the one for fc1.
0---------------
20 -
40 -
60 -
80 -
100 -
ιιιιιιιι
0	25	50	75	100	125	150	175
(a) Correspondence with E[xxT].
0、
20 -
40 -
60 -
80 -
100 -
I	I	I	I	I	-Γ	I	I
0	25	50	75	100	125	150	175
(b) Correspondence with E[M].
0
0
80 -
0
20 -
40 -
60 -
(a) Correspondence with E[xxT].
Figure 15: Eigenvector Correspondence for fc2:LeNet5. (m=84)
25	50	75	100	125	150	175
80 -
0
Figure 14: Eigenvector Correspondence for fc1:LeNet5. (m=120)
20 -
40 -
60 -
(b) Correspondence with E[M].
25	50	75	100	125	150	175
0	10	20	30	40	0	10	20	30	40
(a) Correspondence with E[xxT].	(b) Correspondence with E[M].
Figure 16: Eigenvector Correspondence for conv1:LeNet5. (m=6)
0	10	20	30	40	0	10	20	30	40
(a) Correspondence with E[xxT].	(b) Correspondence with E[M].
Figure 17: Eigenvector Correspondence for conv2:LeNet5. (m=16)
29
E.4 Structure of E[xxT] and E[M] During Training
We observed the pattern of E[xxT] matrix and E[M] matrix along the training trajectory (Fig. 18,
Fig. 19). It shows that E[xxT] is always approximately rank 1, and E[M] always have around c
large eigenvalues. According to our analysis, since the nontrivial eigenspace overlap is likely to be a
consequence of a approximately rank 1 E[xxT], we would conjecture that the overlap phenomenon
is likely to happen on the training trajectory as well.
-----λι
-----λ2
----- 入3
-----λ4
----- 入5
-----入6
-----λ7
----- 18
-----λg
-----λιo
Figure 18: Top eigenvalues of E[xxT] along training trajectory. (fc1:LeNet5)
Figure 19: Top eigenvalues of E[M] along training trajectory. (fc1:LeNet5)
30
F Additional Explanations
F.1 Outliers in Hessian Eigenspectrum
One characteristic of Hessian that has been mentioned by many is the outliers in the spectrum of
eigenvalues. Sagun et al. (2018) suggests that there is a gap in Hessian eigenvalue distribution around
the number of classes c in most cases, where c = 10 in our case. A popular theory to explain the
gap is the class / logit clustering of the logit gradients (Fort & Ganguli, 2019; Papyan, 2019, 2020).
Note that these explanations can be consistent with our heuristic formula for the top eigenspace of
output Hessian at initialization- in the two-layer setting We considered the logit gradients are indeed
clustered.
In the layer-wise setting, the clustering claim can be formalized as follows: For each class k ∈ [c]
and logit entry l ∈ [c], with Q be defined as in Eq. (28), and (x, y) as the input, label pair, let
∆i,j= E Qx⅛)∣y = i .	(53)
∂ wj
Then at the initialization, for each logit entry j, {∆i,j }i∈[c] is clustered around the “logit center”
∆j，Ei∈[c] [∆i,j]; at the minima, for each class i, {∆i,j}j∈[c] is clustered around the “class center”
∆i，Ej∈[c] [∆i,j]. With the decoupling conjectures, we may also consider similar claims for output
Hessians, where
ri,j = E Qx∂zzxjy = i
(54)
A natural extension of the clustering phenomenon on output Hessians is then as follows: At the
initialization, for each logit entry j, {Γij}川@ is clustered around Γj，Ei∈[c] [Γij]; at the minima,
for each class i, {Γij}j∈[c] is clustered around Γi，Ej∈[c] [Γij]. Note that we have the layer-wise
Hessian and layer-wise output Hessian satisfying
HL(w(p))=i,jE∈[c][∆iT,j∆i,j],M(p)=i,jE∈[c][ΓiT,jΓi,j].	(55)
Low-rank Hessian at Random Initialization and Logit Gradient Clustering
We first briefly recapture our explanation on the low-rankness of Hessian at random initialization.
In Appendix H, we have shown that for a two layer ReLU network with Gaussian random initial-
ization and Gaussian random input, the output hessian of the first layer M(1) is approximately
1W ⑵T AW ⑵.We then heuristically extend this approximation to a randomly initialized L-layer
network, that with S(P) = W(L)W(LT)…W(P+1), the output Hessian of the p-th layer H(P)Can
∙-v
be approximated by M(p) where
~	1	,	，一
M(P),许S(P)T as (P).
(56)
Since Ais strictly rank c - 1 with null space of the all-one vector, H(P) is strictly rank c - 1. Thus
H(P) is approximately rank c - 1, and so is the corresponding layerwise Hessian according to the
decoupling conjecture.
Now we discuss the connection between our analysis with the theory of logit gradient clustering. As
previously observed by Papyan (2019), for each logit entry l, {∆i,j}l∈[c] are clustered around the
logit gradients El∈[c] [∆i,j]. Similar clustering effects for {Γi,j}l∈[c] were also empirically observed
by our experiments. Moreover, through the approximation above and the decoupling conjecture, for
each logit entry j, the cluster centers Γj and ∆j can be approximated by
Γj ≈ Γj , (STQ)j
∆j ≈ ∆j , ((E[x]乳 ST)E[Q])j.
(57)
Following Papyan (2019), we used t-SNE (Van der Maaten & Hinton, 2008) to visualize the logit
gradients. As we see in Fig. 20, the “logit centers” of the clustering directly corresponds to the
approximated dominating eigenvectors of the Hessian, which is consistent with our analysis.
31
0.0000 -
0
0.0020
0.0015
0.0010 -
0.0005 -
Sen-AUeW-B
(a) Eigenspectrum of
E[M] at
initialization.
Xhnsj
j=8
j>=0	j=4
j=7
j灵
Xhnsj
-40 -	j =3
-40	-30	-20	-10	0	10	20	30
t-SNE X
(b) Clustering of Γ with logits at
initialization.
-30
-40
-30	-20	-10	0	10	20	30
t-SNEX
(c) Clustering of ∆ with logits at
initialization.
Figure 20:	Logit clustering behavior of ∆ and Γ at initialization (fc1:T-2002)
Gradient Clustering at Minima Currently our theory does not provide an explanation to the low
rank structure of Hessian at the minima. However we have observed that the class clustering of logit
gradients does not universally apply to all models at the minima, even when the models have around
c significant large eigenvalues. As shown in Fig. 21, the class clustering is very weak but there are
still around c significant large eigenvalues. We conjecture that the class clustering of logit gradients
may be a sufficient but not necessary condition for the Hessian to be low rank at minima.
SenFAUeW-B
0.0035
0.0030
0.0025
0.0020
0.0015
0.0010
0.0005
(a) Eigenspectrum of
E[M] at minimum.
0.0000 -
0	10
class cluster centers Ti
• cluster members Γi,j
-20	-10	0	10	20
t-SNE X
class center ∆
cluster members △.
20O
X 3NS
-75	-50	-25	0	25	50	75	100
t-SNE X
(b) Clustering of Γ with class at minimum. (c) Clustering of ∆ with class at minimum.
Figure 21:	Class clustering behavior of ∆ and Γ at minimum. (fc1:T-2002)
F.2 Dominating Eigenvectors of Layer-wise Hessian are Low Rank
A natural corollary for the Kronecker factorization approximation of layer-wise Hessians is that the
eigenvectors of the layer-wise Hessians are low rank. Let hi be the i-th eigenvector of a layer-wise
Hessian. The rank of Mat(hi) can be considered as an indicator of the complexity of the eigenvector.
∕∖
Consider the case that h is one of the top eigenvectors. From Section 5.1, We have hi ≈ Ui 0 E[x].
Thus, Mat(hi) ≈ UiE[x]T, which is approximately rank 1. Experiments shows that first singular
values of Mat(hi) divided by its Frobenius Norm are usually much larger than 0.5, indicating the top
eigenvectors of the layer-wise Hessians are very close to rank 1. Fig. 22 shows first singular values of
Mat(hi) divided by its Frobenius Norm for i from 1 to 200. We can see that the top eigenvectors of
the layer-wise Hessians are very close to rank 1.
32
Figure 22: Ratio between top singular value and Frobenius norm of matricized dominating
eigenvectors. (LeNet5 on CIFAR10). The horizontal axes denote the index i of eigenvector hi, and
the vertical axes denote k Mat(hi)k/k Mat(hi)kF .
fc3
F.3 Eigenspace Overlap of Different Models
From the experiment results in Appendix E together with Fig. 5, we can see that our approximation
and explanation stated in Section 5.1 of the main text is approximately correct but may not be so
accurate for some layers. We now present a more general explanation which addresses why the
overlap before rank-m grows linearly. We will also explain some exceptional cases as shown in
Appendix E.2 and possible discrepancies of our approximation.
Let hi be the i-th eigenvector of the layer-wise Hessian HL(w(p)), under the assumption that the
autocorrelation matrix E[xxT] is approximately rank 1 that E[xxT] ≈ E[x]E[x]T, for all i ≤ m, we
can approximate the h as Ui 0 (E[x]∕∣∣E[x]k) where Ui is the i-th eigenvector of E[M]. Formally,
the trend of top eigenspace can be characterized by the following theorem. For simplicity of notations,
we abuse the superscript within parentheses to refer the two models instead of layer number in this
section.
Theorem F.1. Consider 2 different models with the same network structure trained on the same
dataset. Fix the p-th hidden layer with input dimension n and output dimension m. For the first
model, denote its output Hessian as E[M](1) with eigenvalues T(I) ≥ τ2(^1 ≥ •一≥ TmI ≥ 0 and
eigenvectors r(1),…，rm ∈ Rm ; denote its autocorrelation matrix as E[xxT](1), with eigenvalues
γ(1) ≥ γ21) ≥ ∙∙∙ ≥ Ym) ≥ 0 and eigenvectors tf),…，t(1 ∈ Rn. The variables for the second
matrices are defined identically by changing 1 in the superscript parenthesis to 2.
Assume the Kronecker factorization approximation is accurate that HL(w(p))(1) ≈ E[M](1) 0
E[xxT](1) and HL (w(p))(2) ≈ E[M](2) 0 E[xxT](2). Also assume the autocorrelation ma-
trices of two models are sufficiently close to rank 1 in the sense that Tm(1)γ1(1) > T1(1)γ2(1) and
Tm(2)γ1(2) > T1(2)γ2(2). Then for all k ≤ m, the overlap of top k eigenspace between their layerwise
Hessians HL(W(P))(1)and HL(W(P))(2) will be approximately -m (tf) ∙ tf))2. Consequently, the
top eigenspace overlap will show a linear growth before it reaches dimension m. The peak at m is
approximately (t1 ∙ t2)2.
Proof. Let hi(2) be the i-th eigenvector of the layer-wise Hessian for the first model HL(W(P))(1),
and gi be that of the second model HL(W(P))(2). Consider the first model. By the Kronecker
factorization approximation, since Tm(1)γ1(1) > T1(1)γ2(1), the top m eigenvalues of the layer-wise
Hessian are γ(1)τ(1),…,γ(1)τ7(1). Consequently, for all i ≤ m we have hi ≈ r(1)T 0 tf). Thus,
for any k ≤ m, we have its top k eigenspace as Vk(1) 0 t(11), where Vk(1) ∈ Rm×k has column
vectors r1(1), . . . , rk(1). Similarly, for the second model we have hi(2) ≈ ri(2) 0 t(12) and the top k
eigenspace as Vk(2) 0 t(12), where Vk(2) has column vectors r1(2), . . . , rk(2). The eigenspace overlap of
the 2 models at dimension k is thus
Overlap ") 0 K1),匕⑵ 0 炉)=1 KI)TH(2) 0 W)T娟|)
=(t11) ∙ t12))2 Overlap (H⑴展)∙
(58)
33
Note that for all i ≤ m, ri(1) , ri(2) ∈ Rn, which is the space corresponding to the neurons. Since
for hidden layers, the output neurons (channels for convolutional layers) can be arbitrarily permuted
to give equivalent models while changing eigenvectors. For hi ≈ r 0 tι, permuting neurons will
permute entries in ri . Thus, we can assume that for two models, ri(1) and ri(2) are not correlated and
thus have an expected inner product of，1/m.
It follows from Definition 4.1 that E[Overlap(V.(1), V(2))] = Pk=I E[(r(1) ∙ r(2)) ] = k(1)=]
and thus the eigenspace overlap of at dimension k would be approximately m (tɪ1) ∙ tf' )2. This
explains the peak at dimension m and the linear growth before it.	□
From our results on autocorrelation matrices in Section 4.3 and Appendix E.1, we have E[x]⑴ ≈ tɪ1)
and E[x]⑵ ≈ tf) where E is the normalized expectation. Hence when k = m, the overlap is
approximately (E[x]⑴∙ E[x](2))2. Since E[x]⑴ and E[x](2) are the identical for the input layers,
the overlap is expected to be very high at dimension m for input layers. For other hidden layers in a
ReLU network, X are output of ReLU and thus non-negative. Two non-negative vectors E[x]⑴ and
E[x](2) still have relatively large dot product, which contributes to the high overlap peak.
F.3.1 The Decreasing Overlap After Output Dimension
Consider the (m + 1)-th eigenvector h(m1)+1 of the first model. Following the Kronecker factorization
approximation and assumptions in Theorem F.1, we have h(m1)+1 ≈ r1(1)0t(21). Since top m eigenspace
of the first model is approximately Im 0 t(11) and t(21) is orthogonal to t(11), the h(m1)+1 eigenvector
will be orthogonal to the top m eigenspace of the first model. It will also have low overlap with
Im 0112) since (E[x]⑴∙ E[x]⑵)2 is large.
Moreover, since the remaining eigenvectors of the autocorrelation matrix no longer has the all
positive property as the first eigenvector and structure of the convariance Σx is directly associated
with the ordering of the input neurons which are randomly permuted across different models, the
overlap between other eigenvectors of the autocorrelation matrix across different models will be
close to random, hence the overlap after the top m dimension will decrease until the eigenspaces has
sufficiently many basis vectors to make the random overlap large.
F.3.2 The Output Layer
Note that for the last layer satisfying the assumptions in Theorem F.1, the overlap will stay high
before dimension m and be approximately (tι ∙ t2 )2 since the output neurons directly correspondence
to classes, and hence neurons cannot be permuted. In this case, the overlap will be approximately
(tι ∙ t2)2 for all dimension k ≤ m. This is consistent with our observations.
(a) fc3:F-2002
(MNIST)
(b) fc3:LeNet5
(CIFAR10)
(c) fc1:VGG11-W200
(CIFAR10)
(d) fc1:ResNet18-W64
(CIFAR100)
Figure 23:	Top eigenspace overlap for the final fully connected layer.
34
F.3.3 Explaining “Failure Cases” of Eigenspace Overlap
As shown in Fig. 12 and Fig. 13, the nontrivial top eigenspace overlap does not necessarily peak at
the output dimension for all layers. Some layers has a low peak at very small dimensions and others
has a peak at a larger dimension. With the more complete analysis provided above, we now proceed
to explain these two phenomenons. The major reason for such phenomenons is that the assumption
of autocorrelation matrix being sufficiently close to rank 1 is not always satisfied. In particular,
following the notations in Theorem F.1, for these exceptional layers we have τmγ1 < τ1γ2.
We first consider the first phenomenon (early peak of low overlap) and take fc2:F-2002 (MNIST) in
as an example. Here Fig. 24a is identical to Fig. 12a, which displays the early peak around m = 10.
(a) Eigenspace overlap
(zoomed in)
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
×102
0	10	20
E[xxT ]
(c) True Hessian with
E[xxT]
(d) Approximated Hessian
with E[xxT]
(b) Eigenspectrum of
E[M] and E[xxT]
Figure 24:	Eigenspace overlap, eigenspectrum, and cropped (upper 20 × 20 block)
eigenvector correspondence matrices for fc2:F-2002 (MNIST)
As shown in Fig. 24b, the second eigenvalue of the auto correlation E[xxT] is as large as approx-
imately 1/10 of the first eigenvalue. With the output Hessian have c - 1 = 9 significant large
eigenvalues as described in , it has τ10γ1 < τ1γ2. Thus through the Kronecker factorization ap-
ʌ
ProXimation, the top m dimensional eigenspace is no longer simply Im 0 E[x], but a subset of top
eigenvectors of the output Hessian Kroneckered with a subset of top eigenvectors of E[xxT] as
reflected in Fig. 24d. This “miXture” of Kronecker product is moreover verified in Fig. 24c.
ʌ
As reflected by the first row of Fig. 24c and Fig. 24d, for i ≤ 9 we have hi ≈ ri 0 E[x], which falls
in the regime of Theorem F.1. Hence we are seeing an linearly growing pattern of the overlap for
dimension less than 10 and reaches a mean overlap of around 0.012 by dimension 9. If following
this linear trend, the overlap would be close to 0.25 by the output dimension of 200. However, since
the 10-th eigenvalue of the output Hessian is significantly smaller, little of the 10-19 dimensional
ʌ
eigenspace were contributed by E[x], hence the overlap of dimension larger than 10 falls into the
regime discussed in AppendiX F.3.1, for which we see a sharp decrease of overlap after dimension 9.
Note that this eXample shows that Kronecker factorization can be used to predict when our conditions
in Theorem F.1 fails and also predict the condition can be satisfied up to which dimension. As shown
in Fig. 25, similar eXplanation also applies to convolutional layers in larger networks.
(a) Eigenspace overlap
(zoomed in)
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.0 ■
2.5 -
5.0 -
7.5 -
10.0 -
12.5 -
15.0 -
17.5 -
0	5	10
15
(b)	Eigenspectrum of
E[M] and E[xxT]
(c)	True Hessian with
E[xxT]
0.0 I
2.5 -
5.0-
7.5 -
10.0-
12.5 -
15.0-
17.5 -
0	5	10	15
(d) ApproXimated Hessian
with E[xxT]
Figure 25:	Eigenspace overlap, eigenspectrum, and cropped (upper 20 × 20 block)
eigenvector correspondence matrices for conv5:VGG11-W200 (CIFAR10)
35
We then consider the second phenomenon (delayed peak) and take conv2:VGG11-W200 (CIFAR10)
in as an example. Here Fig. 26a is identical to Fig. 13a, which has the overlap peak later than the
output dimension 200. In this case, the second eigenvalue of the auto correlation matrix is still not
negligible compared to the top eigenvalue. What differentiate this case from the first phenomenon is
that the eigenvalues of the output HeSSian no longer has a significant peak - instead it has a heavy tail
which is necessary for high overlap.
Towards dimension m there gradually exhibits higher correspondence to later eigenvectors of the
ʌ
input autocorrelation matrix and hence less correspondence to E[x]. This eventually results in the
delayed and flattened peak.
10 -
20 -
30 -
40 -
0	10	20	30	40
(a) Eigenspace overlap (b) Eigenspectrum of
(zoomed in)	E[M] and E[xxT]
6
IX"q-性①OuopuOdSOTIOO
0.0
0	50	100	150	200	250	300	350	400
i-th eigenvector (average across 10 eigenvectors)
(e) First Row of Correspondence Matrix of True
Hessian with E[xxT]
1.0
0.8
0.6
(c) True Hessian with (d) Approximated Hessian
E[xxT]	with E[xxT]
IX 豆 q-性①OuopuOdSOTIOO
0.4
0.2
0.0
0	50	100	150	200	250	300	350	400
i-th eigenvector (average across 10 eigenvectors)
(f) First Row of Correspondence Matrix of
Approximated Hessian with E[xxT]
Figure 26:	Eigenspace overlap, eigenspectrum, and cropped (upper 50 × 50 block)
eigenvector correspondence matrices for conv2:VGG11-W200 (CIFAR10)
Since the full correspondence matrices are too large to be visualized, we plotted their first rows up
to 400 dimensions in Fig. 26e and Fig. 26f, in which each dot represents the average of correlation
ʌ
with E[x] for the 10 eigenvector nearby. From these figures it is straightforward to see the gradual
ʌ
decreasing correlation with E[x].
F.4 Batch Normalization and Zero-mean Input
In this section, we show the results on networks with using Batch normalization (BN) (Ioffe &
Szegedy, 2015). For layers after BN, we have E[x] ≈ 0 so that E[x]E[x]T no longer dominates Σx
and the low rank structure of E[xxT] should disappear. Thus, we can further expect that the overlap
between top eigenspace of layer-wise Hessian among different models will not have a peak.
Table 8 shows the same experiments done in Table 6. The values for each network are the average of
3 different models. It is clear that the high inner product and large spectral ratio both do not hold here,
except for the first layer where there is no normalization applied. Note that we had channel-wise
normalization (zero-mean for each channel but not zero-mean for x) for conv1 in LeNet5 so that the
spectral ratio is also small.
Fig. 27a shows that E[xxT] is no longer close to rank 1 when having BN. This is as expected.
However, E[xxT] still has a few large eigenvalues.
Fig. 27b shows the eigenvector correspondance matrix of True Hessian with E[xxT] for fc1:LeNet5.
Because E[xxT] is no longer close to rank 1, only very few eigenvectors of the layer-wise Hessian
will have high correspondance with the top eigenvector of E[xxT], as expected. This directly leads
36
Table 8: Structure of E[xxT] for BN networks
DataSet	Network	# fc	mean	(VT E[χ])2 min	max	mean	λ1∕λ2 min	max
MNIST	F-2002-BN	2	0.062	0.001	0.260	1.16	1.04	1.30
	F-6002-BN	2	0.026	0.000	0.063	1.13	1.02	1.26
	F-6004-BN	4	0.027	0.000	0.146	1.11	1.03	1.19
CIFAR10	LeNet5-BN	3	0.210	0.001	0.803	1.54	1.20	1.89
to the disappearance of peak in top eigenspace overlap of different models, as shown in Fig. 28. The
peak still exists in conv1 because BN is not applied to the input.
×10i fc1	fc2	fc3
2.25
2.00
1.75
1.50
1.25
1.00
0.75
0.50
E[xxT ]	E[xxT ]	E[xxT ]
-,-------------1-------------Γj	Ll--------------1-----------Γj	1 -lI------------1-------------Γj
0	10	20	0	10	20	0	10	20
0.0
2.5
5.0 -
7.5 -
10.0-
12.5-
15.0 -
17.5 -
0.0 -
2.5-
5.0 -
7.5 -
10.0-
12.5 -
15.0 -
17.5 -
(c) Approx HeSSian with
E[xxT] (fc1:LeNet5-BN)
0	5	10	15
0	5	10	15
(a)	Eigenspectrum for E[xxT]
(b)	True HeSSian with
E[xxT] (fc1:LeNet5-BN)
Figure 27:	EigenSpectrum and Eigenvector correSpondence matriceS with E[xxT] for LeNet5-BN.
Figure 28:	EigenSpace overlap of different modelS of LeNet5-BN.
Comparing Fig. 27b and Fig. 27c, we can See that the Kronecker factorization Still giveS a reaSonable
approximation for the eigenvector correSpondence matrix with E[xxT], although worSe than the
caSeS without BN (Fig. 3).
Fig. 29 compare the eigenvalueS and top eigenSpaceS of the approximated HeSSian and the true
HeSSian for LeNet5 with BN. The approximation uSing Kronecker factorization iS alSo worSe than
the caSe without BN (Fig. 2). However, the approximation Still giveS meaningful information aS the
overlap of top eigenSpace iS Still highly nontrivial.
37
1.2-
1.0-
• Approximated
• Exact
0.8-
0.6-
0.4-
0.2-
0	10	20	30	40	50
The i-th eigenvalue
0.5 T
Dimension of Dominating Eigenspace
(b) Top eigenspace overlap between
approximated and true layer-wise Hessian.
(a) Top eigenvalues of approximated
and exact layer-wise Hessian for fc2.
Figure 29:	Comparison between the true and approximated layer-wise Hessians for LeNet5-BN.
G Computing PAC-Bayes Bounds with Hessian Approximation
Given a model parameterized with θ and an input-label pair (x, y) ∈ Rd × Rc, the classification
error of θ over the input sample x is l(θ, x) := 1[arg max fθ (x) = arg max y]. With the underlying
data distribution D and training set S i.i.d. sampled from D, we define
1N
e(θ) := E(χ,y)〜D[l(θ, x)],	e(θ) := N E[l(θ, Xi)]	(59)
i=1
as the expected and empirical classification error of θ, respectively. We define the measurable
hypothesis space of parameters H := RP . For any probabilistic measure P in H, let e(P) =
Eθ〜Pe(θ), e(P) = Eθ〜Pe(θ), and e(P) = Eθ〜PL(θ). Here e(P) serves as a differentiable convex
surrogate of ^(P).
Theorem G.1 (Pac-Bayes Bound). (McAllester, 1999)(Langford & Seeger, 2001) For any prior
distribution P in H that is chosen independently from the training set S, and any posterior distribution
Q in H whose choice may inference S, with probability 1 - δ,
DKL(QllP) +log 苧
DKL (C(Q)ke(Q)) ≤----------∖S∖-1---- .	(60)
Fix some constant b, c ≥ 0 and θ0 ∈ H as a random initialization, Dziugaite & Roy (2017) shows
that when setting Q = N (w, diag(s)), P = N (θ0, λIP), where w, s ∈ H and λ = cexp (-j/b)
for some j ∈ N, and solve the optimization problem
min e(Q) +
w,s,λ
/DKL(QIlP) + log 苧
V	2(∖S∖-1)
(61)
with initialization w = θ, s = θ2, one can achieved a nonvacous PAC-Bayes bound by Eq. (60).
In order to avoid discrete optimization for j ∈ N, Dziugaite & Roy (2017) uses the BRE term to
replace the bound in Eq. (60). The BRE term is defined as
BREM ,J； δ)= DKL(P 必+谓甘 C )+log ?
(62)
where Q = N(w, diag(s)), P = N (θ0, λIP). The optimization goal actually used in the implemen-
tation is thus
w∈rp ,smn,λ∈(0,c)寅Q)+WBRE(W，s,λ;δ).
(63)
38
Algorithm 1 shows the algorithm for Iterative Hessian (ITER) PAC-Bayes Optimization. If we set
η = T, the algorithm will be come Approximate Hessian (APPR) PAC-Bayes Optimization. It is
based on Algorithm 1 in Dziugaite & Roy (2017). The initialization of w is different from Dziugaite
& Roy (2017) because we believe what they wrote, abs(w) is a typo and log[abs(w)] is what they
actually means. It is more reasonable to initialize the variance s as w2 instead of exp[2 abs(w)].
Algorithm 1 PAC-Bayes bound optimization using layer-wise Hessian eigenbasis	
In Ou 1 2 3	put: w0 ∈ RP	. Network parameters (Initialization) w ∈ RP	. Network parameters (SGD solution) S	. Training examples δ ∈ (0, 1)	. Confidence parameter b ∈ N, c ∈ (0, 1)	. Precision and bound for λ τ ∈ (0, 1), T ∈ N	. Learning rate; No. of iterations η ∈ N	. Epoch interval for Hessian calculation tput w	. Optimized network parameters s	. Optimized posterior variances in Hessian eigenbasis λ	. Optimized prior variancce : procedure ITERATIVE-HESSIAN-PAC-BAYES ς J log[abs(w)]	. where s(ς) = exp(2ς) % J	3	. where λ(%) = exp(2%)
4 5 6 7 8 9 10 11 12 13 14 15 16 17	R(w, s,λ) = J2Bre(w, s, λ; δ)	. BRE term : B	(w, s, λ, w0) = L(w0) + R(w, s, λ)	. Optimization goal :	for t = 0 → T - 1 do	. Run SGD for T iterations :	if t mod η == 0 then :	HESSIANCALC(w) :	end if Sample ξ 〜N(0, I)P :	w0(w, ς) = w + TOSTANDARD (ξ exp(ς))	. Generate noisy parameter for SNN W J W — T [VwR(w, s, λ) + VwoL(w0)] ς J ς — τ [VςR(w, s(ς), λ) + TOHESSIAN (VwoL(w0)) Θ ξ Θ exp(ς)] :	% J % — τ V%R(W, s, λ(%))	. Gradient descent :	end for :	return w, s(ς), λ(%) : end procedure
In the algorithm, HESSIANCALC(w) is the process to calculate Hessian information with respect
to the posterior mean w in order to produce the Hessian eigenbasis to perform the change of basis.
For very small networks, we can calculate Hessian explicitly but it is prohibitive for most common
networks. However, efficient approximate change of basis can be performed using our approximated
layer-wise Hessians. In this case, we would just need to calculate the full eigenspace of E[M]
and that of E[xxT] for each layer. For pth layer, we denote them as U(p) and V (p) respectively
with eigenvectors as columns. We can also store the corresponding eigenvalues by doing pairwise
multiplications between eigenvalues of E[M] and E[xxT].
After getting the eigenspaces, we can perform the change of basis. Note that we perform change
of basis on vectors with the same dimensionality as the parameter vector (or the posterior mean).
TOHESSIAN(u) is the process to put a vector u in the standard basis to the Hessian eigenbasis. We
first break u into different layers and let u(p) be the vector for the pth layer. We then define Mat(p)
as the reshape of a vector to the shape of the parameter matrix W (p) of that layer. We have the new
vector v (p) in Hessian basis as
v(p) = vec hU(p)T Mat(p)(u(p))V (p)i .	(64)
The new vector v = TOHESSIAN(u) is thus the concatenation of all the v(p).
39
TOSTANDARD(v) is the process to put a vector v in the Hessian eigenbasis to the standard basis. It
is the reverse process to TOHESSIAN. We also break v into layers and let the vector for the pth layer
be v(p). Then, the new vector u(p) is
u(p) = vec U(p)Mat(p)(v(p))V(p)T ,	(65)
The new vector u = TOSTANDARD(v) is thus the concatenation of all u(p) .
After getting optimized w , s, λ, we compute the final bound using Monte Carlo methods same as
in Dziugaite & Roy (2017). Note that the prior P is invariant with respect to the change of basis,
since its covariance matrix is a multiple of identity λIP. Thus, the KL divergence can be calculate in
the Hessian eigenbasis without changing the value of λ. In the Iterative Hessianwith approximated
output Hessian (ITER.M), we use M to approximate E[M], as in Eq. (56).
We followed the experiment setting proposed by Dziugaite & Roy (2017) in general. In all the results
we present, we first trained the models from Gaussian random initialization w0 to the initial posterior
mean estimate w using SGD (lr=0.01) with batch-size 128 and epoch number 1000.
We then optimize the posterior mean and variance with layer-wise Hessian information using Algo-
rithm 1, where δ = 0.025, b = 100, and c = 0.1. We train for 2000 epochs, with learning rate τ
initialized at 0.001 and decays with ratio 0.1 every 400 epochs. For Approximated Hessian algorithm,
we set η = 1. For Iterative Hessian algorithm, we set η = 10. We also tried η with the same decay
schedule as learning rate (multiply η by 10 every time the learning rate is multiplied by 0.1) and the
results are similar to those without decay. We also used the same Monte Carlo method as in Dziugaite
& Roy (2017) to calculate the final PAC-Bayes bound. Except that we used 50000 iterations instead
of 150000 iterations because extra iterations do not further tighten the bound significantly. We use
sample frequency 100 and δ0 = 0.01 as in that paper.
The complete experiment results are listed in Table 9. We follow the same naming convention as in
Dziugaite & Roy (2017) except adding T-2002 we introduced in Section 4. T-60010, T-600210, and
T-200120 are trained on standard MNIST with 10 classes, and others are trained on MNIST-2 (see
Appendix D.1), in which we combined class 0-4 and class 5-9.
In Table 9, Prev means the previous results in Dziugaite & Roy (2017), APPR means Approximated
Hessian, ITER means Iterative Hessian, ITER (D) means Iterative Hessian with decaying η, ITER.M
means Iterative Hessian with approximated output Hessian. BASE are Base PAC-Bayes optimization
as in the previous paper.
We also plotted the final posterior variance, s. Fig. 30 shown below is for T-200210 . For posterior
variance optimized with our algorithms (Appr, Iter, and Iter.M) we can see that direction
associated with larger eigenvalue has a smaller variance. This agrees with our presumption that top
eigenvectors are aligned with sharper directions and should have smaller variance after optimization.
The effect is more significant and consistent for Iterative Hessian, where the PAC-Bayes bound is
also tighter.
Figure 30: Optimized posterior variance, s. (fc1:T-2002, trained on MNIST), the horizontal axis is
ordered with decreasing eigenvalues.
40
Table 9: Full PAC-Bayes bound optimization results
Network	Method	PAC-Bayes Bound	KL Divergence	SNN loss	λ (prior)	Test Error
T-600	Prev	0.161	5144	0.028	-	0.017
	Base	0.154	4612.6	0.03373	-1.3313	0.0153
	Appr	0.1432	3980.6	0.03417	-1.6063	0.0153
	Iter	0.1198	3766.1	0.02347	-1.2913	0.0153
	Iter(D)	0.1199	3751.1	0.02366	-1.2913	0.0153
	Iter.M	0.1255	3929.9	0.02494	-1.3213	0.0153
T-6002	Prev	0.186	6534	0.028	-	0.016
	Base	0.1921	6966.6	0.03262	-1.4163	0.0148
	Appr	0.1658	5176.1	0.03468	-2.0963	0.0148
	Iter	0.1456	5086.5	0.02473	-1.7963	0.0148
	Iter(D)	0.1443	4956.8	0.02523	-1.7963	0.0148
	Iter.M	0.1502	5024.5	0.02767	-1.8363	0.0148
T-1200	Prev	0.179	5977	0.027	-	0.016
	Base	0.1754	5917.6	0.03295	-1.5463	0.0161
	Appr	0.1725	5318.8	0.03701	-1.8313	0.0161
	Iter	0.1417	5071	0.02292	-1.4763	0.0161
	Iter(D)	0.1413	5021.1	0.02316	-1.4763	0.0161
	Iter.M	0.1493	5185.4	0.02576	-1.5363	0.0161
T-3002	Prev	0.17	5791	0.027	-	0.015
	Base	0.1686	5514.9	0.03329	-1.1513	0.015
	Appr	0.1434	4105.4	0.03296	-1.8063	0.015
	Iter	0.1249	3873.2	0.02514	-1.4763	0.015
	Iter(D)	0.1244	3833.7	0.02526	-1.4763	0.015
	Iter.M	0.1308	3987.2	0.02721	-1.5713	0.015
R-600	Prev	1.352	201131	0.112	-	0.501
	Base	0.6046	1144.8	0.507	-1.8263	0.4925
	Appr	0.5653	390.25	0.5066	-2.4713	0.4925
	Iter(D)	0.5681	431.62	0.5066	-2.4513	0.4925
	Iter.M	0.5616	340.62	0.5065	-2.5263	0.4925
T-200210	Base	0.4165	21896	0.04706	-1.1513	0.0208
	Appr	0.2621	11068	0.0366	-1.4213	0.0208
	Iter	0.2145	9821	0.02229	-1.1513	0.0208
	Iter(D)	0.2311	9758.5	0.03071	-1.1513	0.0208
	Iter.M	0.2728	13406	0.02605	-1.1513	0.0208
T-60010	Base	0.2879	12674	0.03854	-1.1513	0.018
	Appr	0.2424	9095.8	0.04159	-1.6013	0.018
	Iter	0.2132	8697.9	0.02947	-1.3063	0.018
	Iter.M	0.2227	8870.9	0.03294	-1.4613	0.018
T-600210	Base	0.3472	17212	0.03884	-1.1513	0.0186
	Appr	0.2896	11618	0.04723	-2.0563	0.0186
	Iter	0.2431	10568	0.03057	-1.5713	0.0186
41
H Proofs and experiment for the low rank structure of the output Hessian
H.1 Preliminaries
H.1.1 Notations
We use [n] to denote the set {1, .…,n}, and ∣∣M∣∣ to denote the spectral norm of a matrix M. We
use hA, Bi to denote the Frobenius inner product of two matrices A and B, namely hA, Bi ,
Pi,j Ai,j Bi,j. Denote tr(M) the trace of a matrix M and denote 1c the all-one vector of dimension c
(the subscript may be omitted when it’s clear from the context). Furthermore, for notation simplicity,
we will say “with probability 1 over W (1)/W (2), event E is true” to denote
lim lim	Pr	[E] = 1.	(66)
n→∞ d→∞ W ⑴〜N (0,11nd)^W ⑶〜N (0, n Icn)
H.1.2 Problem Setting
Consider a two layer fully connected ReLU neural network with input dimension d, hidden layer
dimension n and output dimension c. The network is trained with cross-entropy objective L. Let σ
denote the element-wise ReLU activation function which acts as σ(x) = X ∙ 1χ≥o. Let W(I) ∈ Rn×d
and W(2) ∈ Rc×n denote the weight matrices of the first and second layer respectively.
Let the neural network have standard normal input X 〜N(0, Id). Denoting the output of the
first and second layer as y and z respectively, we have y = σ(W (1)x) and z = W (2)y. Let
p = softmax(z) denote the softmax output of the network. Let A := diag(p) - ppT. From the
previous analysis of Hessian, we have the output Hessian of the second layer can be written as
M := E[DW(2)TAW(2)D], where D := diag(1y≥0 ) is the diagonal random matrix representing
the activations of ReLU function after the first layer.
In this problem, we look into the state of random Gaussian initialization, in which entries of both
matrices are i.i.d. sampled from a standard normal distribution, and then re-scaled such that each row
of W(1) and W(2) has norm 1. When taking n and d to infinity, with the concentration of norm in
high-dimensional Gaussian random variables, in this problem we assume that entries of W(1) are iid
sampled from a zero-mean distribution with variance 1/d, and entries of W(2) are iid sampled from a
zero-mean distribution with variance 1/n. This initialization is standard in training neural networks.
Since our formula for the top eigenspace is going to depend on W(2), throughout the section when
we take expectation we condition on the value of W(1) and W(2). The expectation is only taken
over the input X 〜N(0, Id) (due to concentration taking expectation on X is similar to having many
samples from the input distribution). In this case, the output Hessian is defined as:
M , E[DW(2)TAW(2)D].	(67)
H.2 Main Theorem and Proof Sketch
In this section, we will provide a formal statement of our main theorem and its proof sketch. First,
we state our main theorem:
Theorem H.1. For all > 0,
lim lim	Pr
n→∞ d→∞ W (I)〜N (0, d Ind),W ⑶〜N (0,1 Icn)
λc(M)
λc-ι(M)
W(1),W(2)	<
1.
(68)
Besides, for all e > 0, if We define Si as the top C 一 1 eigenspace of M, and S? as R(W)\{W ∙ 1}
where R(W) is the row space of W, then
lim lim	Pr	[Overlap (S1, S2) > 1 一 ] = 1.	(69)
n→∞ d→∞ W⑴〜N(0,dInd),W⑶〜N(0, IIcn)
Proof of Theorem H.1.
First of all, let us repeat the expression for the output Hessian M :
M , E[DW(2)TAW(2)D].	(70)
42
In the proof, we will first analyze the properties of D, W(2), A separately:
Firstly, D is a diagonal matrix with 0/1 entries, and the following lemma shows that its entries are
independent when the input dimension tends to infinity.
Lemma 1. When d → ∞, with probability 1 over W (1), the entries of D are independent.
Secondly, since each entry of W (2) is sampled i.i.d. from a spherical Gaussian distribution, this
matrix enjoys some very nice properties when the network width n goes to infinity. We have the
following lemma for W (2).
Lemma 2. (informal) When n is large enough, each row of W(2) has norm very close to 1, and
these rows are nearly orthogonal to each other. Besides, the entries (and the average of the entries)
cannot be too large.
These properties (along with other useful properties) will be formally stated and proved in Ap-
pendix H.3.2.
As for matrix A, it’s very hard to compute its expectation explicitly because the generation of A
involves softmax, but we are able to prove some useful properties of E[A] as shown in the following
lemma.
Lemma 3. A，limn→∞ E[A] exist and is rank-(C 一 1) with probability 1 over W⑵.
This lemma is true because A is positive semi-definite (PSD) and is almost always of rank (c - 1).
Besides, its null space always contains the all-one vector 1c.
Having these properties of these three matrices, we then look at the expression of M again to see
how to compute this expectation. This expectation is not easy to compute as we condition on W(1)
and W(2). With this conditioning, D and A are correlated and hard to decompose. This is when
we need the most important observation in our proof: When the input dimension and the network
width tend to infinity, A and D can be considered independent when computing M . These two
matrices are actually not independent even when we take the limit: For example, if D happens to be
1 whenever the first row of W(2) is positive, then the first logit output is going to be much larger
than the rest which significantly skews the distribution of A. However, since the computation of M
only contains finite-degree polynomials of A and D, we only need a weaker form of independence,
i.e., the distribution of A is approximately invariant condition on finite entries of D, as shown in the
following lemma:
Lemma 4. Let DX denote the distribution of X, andTV (D1, D2) denote the total variation distance
between D1 and D2. Then ∀i, j ∈ [n], ∀ > 0
lim lim
Pr
n→∞ d→∞ W ⑴ ~N (0, 11nd),W ⑶~N (0,1 Icn)
TV (DA, DA|Di,i=Dj,j=1) >	=0.	(71)
The intuition behind this is that A is uniquely determined by the output of the second layer z, where
z = W(2)y is c-dimensional and y is an n-dimensional vector. Since D = diag(1y≥0), fixing finite
number of entries in D is equivalent to fixing the signs of finite entries of y. When n is large enough
compare to c, only constraining finite entries of y shouldn’t change the distribution of z by much.
The formal proof of this theorem is given in Appendix H.3.4.
Having Lemma 4, we can equivalently consider A and D as independent matrices. To formalize this,
we need the following definition:
Definition H.1. Let D0 be an independent copy of D and also independent of A. Define M*，
E[D0W(2)TAW(2)D0].
In other words, M * is the matrix which has the same expression as M except that we assume
D is independent of A in M*. Then we know that M and M* are essentially the same. Since
D is a diagonal matrix with 0/1 entries, multiplying D at both sides of a matrix is equivalent to
independently zero out each row and corresponding column with probability 1. Thus, the probability
of each diagonal entry to be kept is 1 while for off-diagonal ones it,s 1. Formally, we have
M* ≈ 4 (E [w⑵TAW⑵i + diag(E[W⑵TAW⑵]))
(72)
43
We have two terms on the right hand side of this equation: T1 , E W (2)T AW (2) and T2 ,
diag(E[W (2)TAW (2)]). We make two observations of these two terms. On the one hand, they have
the same trace. This is because the diagonal entries of these two matrices are exactly the same. On the
other hand, T1 is a low rank matrix but T2 is approximately full rank. For T1 , since the expectation
is taken over x only, we know that T1 = W(2)TE [A] W(2). Note that E[A] is a rank (c - 1) PSD
matrix, so T1 is also PSD and has rank at most (c - 1). T2 is a diagonal matrix, and each diagonal
entry equals a quadratic form wi(2)T E[A]wi(2) where wi(2) is the i-th column of W(2) (i ∈ [n]). This
term is always positive unless wi(2) lies in the span of 1c, which happens with probability 0. Actually,
due to the random nature of wi(2)’s, the diagonal terms in T2 do not differ too much from one another.
To summarize, T1 and T2 are both PSD matrices with the same trace, while T1 is low-rank but T2 is
approximately full rank. This intuitively indicates that the positive eigenvalues of T1 is significantly
large than those of T2, making the positive eigenvalues of Ti the dominating eigenvalues of M* and
those of T2 the thin but long tail of M* 's eigenvalue spectrum.
Now we have know that T1 is almost the only contributing term to the top eigenvalues and eigenspaces
of M*, so we only need to analyze these for T1 = W(2)TE [A] W(2). Since the rows of W(2) are
close to rank 1 and almost mutually orthogonal, the matrix T1 will roughly keeps all the eigenvalues
of E[A], and the top (c - 1) eigenspace should roughly be the “W(2)-rotated” version of Rc\{1c},
i.e., R(W⑵)\{W⑵∙1J.
Despite the arguments above, we have some technical difficulties, the biggest of which is that the
dimensions of M and M * will become infinite when n goes to infinity. To tackle this problem, we
introduce an indirect and more complicated way to do the proof. We first project these matrices
onto the row span of W(2) and show that this projection roughly keeps all the information of these
matrices. Formally, we have the following lemma:
Lemma 5. With probability 1 over W(2),
lim
n→∞
IlWMW TIlF
-kMF-
1.
After that, we do the analysis in this finite-dimensional span and finish the proof of our main theorem.
H.3 Detailed Proof
H.3.1 Proof of Lemma 1
We first restate Lemma 1 here:
Lemma 1. When d → ∞, with probability 1 over W(1), the entries of D are independent.
Proof of Lemma 1. Remember that D := diag(1y≥0). The off-diagonal entries of D are always
0 and independent of anything. For the diagonal entries, each diagonal entry is decided by a
corresponding entry of y. Therefore, we only need to prove that the entries of y are independent, and
we have the following lemma:
Lemma 6. When d → ∞, with probability 1 over W(1), the entries of y are independent.
Proof of Lemma 6. We will prove this lemma using the multivariate Lindeberg-Feller CLT. For each
i ∈ [n], let wi ∈ Rd denote the i-th column vector of W(1). Let ui = wixi, then we have
dd
u = Xui = Xwixi = W(1)x	(73)
i=1	i=1
Note that xi’s are i.i.d standard Gaussian. It has the moments:
E[xi] = 0,	E[(xi - E[xi])2] = 1,	E[(xi - E[xi])4] = 3.	(74)
It follows that
Var[ui] = Var[wixi] = wiwiT .	(75)
44
Let V =	in=1 V ar [ui],
d
V = X wiwiT = W(1)W(1)T.	(76)
i=1
As d → ∞, from Lemma 9 (we replace n, W with dand W(1)) we have W(1)W(1)T → In in
probability, therefore limd→∞ V = In .
We now verify the Lindeberg condition of independent random vectors {u1, . . . , un}. First observe
that the fourth moments of the ui are sufficiently small.
dl→im∞Xd Ehkuik3i= dl→im∞Xd E
i=1	i=1
≤ lim X E n2 max W (2)	xi2
d→∞ i=1	j∈[n] ji	i
(77)
4n
≤ dl→im∞ n2 i∈[md],ajx∈[n]Wj(i2)	XE (xi - E[xi])4
i=1
Since E[(xi - E[xi])4] = 3 and maxi∈[d],j∈[n] |Wj(i2)| < 2d- 1 with probability 1 from Lemma 11,
it follows from above that
d	4d
lim E	∣^kuik4^∣	≤ n2	lim(2d-3]	3 = 48n2	lim d-4d =	48n2	lim	d- 1	=0.	(78)
d→∞	d→∞	d→∞	d→∞
i=1	i=1
For any > 0, since kui k > in the domain of integration,
dlim Xd Ehkuik21[kuik >]i < dlim Xd E
→∞ i=1	→∞ i=1
ɪ kuik2 1 [kuii
≤ W dl→∞ XX E hkuik4i =0∙
i=1
(79)
As the Lindeberg Condition is satisfied, with limn→∞ V = Ic in probability we have
lim u
d→∞
d
lim X ui -D→ N (0, In) .
d→∞
i=1
(80)
Thus, since u converges to N (0, In) in distribution with probability 1 over W(1), entries of u are
independent. Since y = σ(u) and ReLu is an entry-wise operator, entries of y are independent.
Since the diagonal entries of D are uniquely determined by the corresponding entries of y, we know
that when d → ∞, with probability 1 over W(1), the entries of D are independent. This finishes the
proof of Lemma 1.
H.3.2 Proof of Lemma 2
We first restate Lemma 2 here:
Lemma 2. (informal) When n is large enough, each row of W(2) has norm very close to 1, and
these rows are nearly orthogonal to each other. Besides, the entries (and the average of the entries)
cannot be too large.
This is not a formal lemma and will act as the intuition behind the properties of W(2). In this section,
we will formally state the properties we need and prove them. Besides, for simplicity of notations,
we use W to denote W (2) from now on unless otherwise stated.
Lemma 7. For all i ∈ [c], for all > 0, limn→∞ Pr Pjn=1 Wij ≥	= 0.
45
Proof of Lemma 7. Since each entry of W is initialized independently from N(0, ɪ), by Central
Limit Theorem We have P；=i Wj 〜N(0,1). For any e > 0, fix U By chebyshev,s inequality,
nl→∞ P (X Wj
≥ e I < lim -ɪ = 0.
I	n→∞ ne2
(81)
□
Lemma 8. For all e > 0, limn→∞
Pr (IkWIIF -c| ≥e) =0.
(IkWik2 - 1∣ ≥e) =0.
Besides,for all i ∈ [c], limn→∞ Pr
Proof of Lemma 8. Since each entry of W is initialized independently from N(0,1), we know
that n k W∣∣f = P；=i P；=1 nW∕j∙ follows a χ2n-distribution. Using the tail bound provided by
Lemma 1 in Laurent & Massart (2000), we know that for large enough n,
Pr (∣n k W kF — cn∣ ≥ ne) ≥ Pr (∣n ∣∣ W ∣∣F — cn∣ ≥ 2√cn3/4 + 2n1/2) ≤ 2exp(-n1/2).
(82)
In other words,
lim Pr	(∣k W∣∣F	— c∣ ≥ e)	= lim Pr	(∣n ∣∣ W∣∣F	- cn∣ ≥ ne)	= 0.	(83)
n→∞	∖	n n→∞	∖	)
Similarly, ∀i ∈ [c], n IlWill* follows a ^^-distribution, so for large enough n,
Pr (∣n k Wik* — n∣ ≥ ne) ≤ Pr (∣n ∣∣ W∣∣21 — n∣ ≥ 2n3/4 + 2n1/2) ≤ 2exp(-n1/2),	(84)
which indicates that
lim Pr	(∣∣∣Wik2	- 1∣ ≥ e)	= lim Pr	(∣n ||叱『一n∣ ≥ ne)	=0.	(85)
n→∞	∖	n n→∞	∖	)
□
Lemma 9. For all e > 0, limn→∞ Pr (IlWWT - IJl ≥ e) = 0.
Besides, for all i,j ∈ [c], limn→∞ Pr (∣(WWT)i,j — 6,j ∣ ≥ e) = 0. Here δ is the Kronecker delta
function, i.e., δi,j = 1[i=j].
Proof of Lemma 9. Since each entry of W is initialized independently from N(0,1), we know that
WWT follows Wishart distribution Wc(ɪIc, n). Using the third tail bound in Theorem 1 of Zhu
(2012), for large enough n, we get
Pr (∣∣ WWT - Ij≥ e)
=Pr I 1WWT - 1 Ic
n	n
≤ Pr ( 1WWT - 1 Ic
n	n
≤ 2c exp(- √n).
+ 1)n-1/4 + 2cn
(86)
1
≥ —
n
Therefore,
∀e > 0, lim Pr ( ∣ ∣ WWT - Ic∣∣ ≥ e) =0.	(87)
n→∞	v11	11	,
Moreover, for all i, j ∈ [c], we have
Pr (∣(WWT£ - δi,j∣ ≥ e) ≤ Pr ∣ X ((WWT* - δi,j)2 ≥ e2
∖ij=ι
=Pr (∣∣ WWT - ICH ≥ e2)
=Pr (∣∣ WWt - Ic∣∣≥√c),
(88)
46
which implies that for all i, j ∈ [c],
lim Pr (|(WWT)i,j - % | ≥ e) = 0.	(89)
n→∞
Lemma 10. Let PW be the projection matrix onto the row span of W, then for all > 0,
nl→im∞ Pr hWTW - PW 2F > i = 0.	(90)
Proof of Lemma 10. Without loss of generality, we assume that < 1. Let Wi(i ∈ [c]) be the
i-th row of W, and We will do the Gram-Schmidt process for the rows of W. Specifically, the
Gram-Schmidt process is as following: Assume that { Wi}k=ι are the already normalized basis, We
set Wk+ι，Wk+ι - Pk=ιhWk+ι,Wii and Wk+ι，渭+\ . Finally, from the definition of
projection matrix, we know that PW = WT W.
Let e0，c3,i：2c+i, from Lemma 9 we know that ∀i, j ∈ [c], limn→∞ Pr (| WiWT - δi,j| ≥ e0)=
0. Besides, from Lemma 8 we know that ∀i ∈ [c], limn→∞ Pr | kWi k2 - 1| ≥	= 0. Then
we use induction to bound the difference between W and W. Specifically, we will show that
∀i ∈ [c],∣pWi - WiIl≤ 8i0 . For notation simplicity, in the following proof we will not repeat the
probability argument and assume that ∀i, j ∈ [c], |WiWjT -δi,j | ≤ 0 and ∀i ∈ [c], | kWik2-1| ≤ 0.
We will only use these inequalities finite times so applying a union bound will give the probability
result.
For i = 1, we know that W1 = ∣W1k and ∣k Wj∣- 1| ≤ e0, so ∣∣ Wi - Wi ∣∣ ≤ e0.
If our inductive hypothesis holds for i ≤ k, then for i = k + 1, we have
∀j ≤ k, IhWi,Wji∣ ≤ IhWi, Wji∣ + IhWi,Wj - Wjil
≤ e0 + kWik∙∣∣Wj - WjII
≤ 0+(1+0)8j0
≤ (23j+1 + 1)0.
Therefore,
kWi0 - Wik ≤ X IhWi,Wjil ≤ e0 + X (23j+1 + 1)e0 ≤ (23k+2 - 1)e0,	(92)
j∈[k]	j∈[k]
and
IkWi0k -1I ≤ IkWik -1I+kWi0-Wik ≤ 23k+20.	(93)
Thus,
∣∣Wi - Wi∣∣ ≤ ∣∣Wi - W!∣∣ + kWi - Wik
≤ IkWi0k - 1I+ kWi0 - Wik	(94)
≤ 8k+10,
which finishes the induction and implies that∀e > 0, ∀i ∈ [c], ∣∣ Wi - Wi∣∣ ≤ 8ie0. Thus,
∣∣W- W∣∣F = X ∣∣Wi - Wi∣∣2 ≤ C∙16%0.	(95)
i∈[c]
This means that
∣∣w T W - pw∣∣f = ∣∣w T W - W T W ∣∣F
≤ 2∣∣W - W∣∣F∣∣W∣∣F + ∣∣W - W∣∣F	(96)
≤ 2c ∙ √C ∙ 8c√0 + C ∙ 16%0 ≤ e.

47
Lemma 11. The largest entry of W(2) is reasonably small with high probability as n goes to infinity,
namely,
lim P
n→∞
max	∣W(2)∣ > 2n- 1
i∈[c],j∈[n]	ij
(97)
0
Proof of Lemma 11. For i.i.d. random variables xι ,…，Xn 〜N(0,1), by concentration inequality
on maximum of Gaussian random variables, for any t > 0, we have
P mi=na1x Xi > p2 log(2n) +1] < 2e--t.
(98)
For any i, j, since Wj2) are i.i.d. sampled from N(0, n), with rescaling of 1/√n We may substitute
Xj with Wi(2). It follows that
ij
P
max Wi(j2) >
i∈[c],j∈[n]
，2 log(2cn) + t
t2
< 2e-F.
(99)
Taking t = n6, with C as constant, for large n we have，2 log(2cn) < n 1. Thus for large n,
P
max	Wij2) > 2n-1
i∈[c],j∈[n]
max
i∈[c],j∈[n]
Wi(j2)
n 6 + n 6
√n
max
i∈[c],j∈[n]
Wi(j2)
p2 log(2n) + n 6
√n
n 3
< 2e--
(100)
<
P
P
>
>
With the same argument, we have
P
min
i∈[c],j∈[n]
W(2) < -2n-3
ij
n 3
< 2e-F
(101)
Passing n to infinity completes the proof.

H.3.3 Proof of Lemma 3
We first restate Lemma 3 here:
Lemma 3. A，limn→∞ E[A] exist and is rank-(C 一 1) with probability 1 over W⑵.
Before proving Lemma 3, we need some knowledge about the distribution of A. Since A is
determined by the vector z, it suffice to know the distribution of z:
Lemma 12. limn→∞ Z → N(0, π-∏1 Ic) with probability 1 over W⑵.
Proof of Lemma 12. We will prove this lemma using the multivariate Lindeberg-Feller CLT. For
each i ∈ [n], let wi ∈ Rc denote the i-th column vector of W(2). Let vi = wi(yi - E[yi]), then we
have
n
n
n
z =	wiyi =	vi+	E[yi]	wi .
(102)
i=1
i=1
i=1
From Lemma 6 we know yi’s are i.i.d. rectified half standard normal. It has the moments:
EM] = √2π,
It follows that
E[(yi-E[yi])2]
π 一 1	6π2 一 10π 一 3
M,	EM-EMM = -4∏2—
1.
(103)
V ar[vi]
V ar[wiyi]
π1 WiWT.
2π i i
(104)
<
Let V = Pin=1 V ar [vi],
v=π2π1
n
X wiwiT
i=1
π - 1W⑵W⑵T
2π	.
(105)
48
As n → ∞, from Lemma 9 we have W(2) W(2)T → Ic in probability, therefore 1imn→∞ V =
π-11
2π cc'
We now verify the Lindeberg condition of independent random vectors {v1 , . . . , vn }. First observe
that the fourth moments of the vi’s are sufficiently small.
n
c
2
2
n1→im∞XEhkvik4i
i=1
1→∞ X E(X (W2) (yi- EM]))
i=1
n
1im X E
n→∞
i=1
j=1
c2mj∈a[Cx] Wj(i2)2(yi-E[yi])2!
(106)
≤
n
≤
4n
1im c2	max Wj(i2)	XE (yi -E[yi])4 .
n→∞	i∈[n],j∈[C]
i=1
Since E[(yi - E[y∕)4] < 1 and maxi∈[n],共曰 |吗广)| < 2n- 1 with probability 1 from Lemma 11,
it follows that
n
n1→im∞XE kvik4 ≤ c2 1im
i=1
n→∞
1	1、4 nLy	4	1
(2n-3 ) ∖= 1 = c2 lim 16n-3n = 16c2 lim n-3
n→∞	n→∞
i=1
0. (107)
For any > 0, since kvi k > in the domain of integration,
n
n
nl→im∞XE kvik2 1 [kvik > ] < nl→im∞X
i=1
i=1
⅞k2 kVik2 1 [Mil >e]
≤ g n→∞ X E hkvik4i =0∙
(108)
As the Lindeberg Condition is satisfied, with 1imn→∞
n
i=1
V = π-∏1 IC we have
1im X vi -→d N 0,
n→∞
i=1
(109)
E
U IC).
By Lemma 7, we have 1imn→∞ wi = ~0 with probability 1 over W(2), therefore plugging (Eq. (109))
into (Eq. (102)) we have
lim z → N fθ, π--1 Ic
n→∞	2π
(110)

After that, we can proceed to prove Lemma 3.
Proof of Lemma 3. Note that each entry of A is a quadratic function of p, and p is a continuous
function of z. Therefore, we consider A as a function of z and write A(z) when necessary. From
Lemma 12 we know that 1imn→∞ z follows a standard normal distribution N (0, αIc) with probability
1 over W, where α is some absolute constant. Therefore, A，1imn→∞ E [A] exist and it equals
E[A(1imn→∞ z)] = EZ〜N(o,αic) [A(z)]. For notation simplicity, we will omit the statement “with
probability 1 over W” when there is no confusion.
From the definition ofA we know that A , diag(p) -ppT where p is the vector obtained by applying
softmax to z, so Pic=1 pi = 1 and ∀i ∈ [c], pi ∈ (0, 1). Therefore, for any vector p satisfying the
previous conditions, we have
cc	c
1TA1 = X pi - Xpipj	= X(pi -pi) = 0,
i=1	j=1	i=1
(111)
where 1 is the all-one vector. Therefore, We know that A has an eigenvalue 0 with eigenvector √1c 1.
This means that E[A] also has an eigenvalue 0 with eigenvector √1c 1. Thus, E[A] is at most of rank
(c- 1).
49
Then We analyze the other (C - 1) eigenvalues of A. Since A = QQT where Q = diag(√p)(Ic -
1pT), we know that A is always a positive semi-definite (PSD) matrix, which indicates that E[A]
∙-v
must also be PSD. Assume the C eigenvalues of A are λι ≥ λ2 ≥ ∙∙∙ ≥ λc-ι ≥ λc = 0. Therefore,
by definition, we have
T
λc-i = min V Av = EZ〜N(o,αic)
v∈S,kvk=1
min vTAv ,
v∈S,kvk=1
(112)
where S , Rc\R{1T} is the orthogonal subspace of the span of 1. v ∈ S implies that v ⊥ 1, i.e.,
Pic=1 vi = 0.
Direct computation gives us
cc
vTAv =	vi2pi -	vipi
i=1	i=1
(113)
Define two vectors a, b ∈ Rc as ∀i ∈ [c], ai，vi√pi, b，√pi, then ∣∣bk2 = Pc=I Pi
VTAv = kak2 - ha, bi2 = kak2 ∙ kbk2 - ha, bi2.
1 and
(114)
Therefore,
vTAv ≥ kak2 kbk2 sin2 θ(a, b),
where θ(a, b) is the angle between a and b, i.e., θ(a, b)，arccos1嵩％ . Define po，min
then
(115)
i∈[c] pi,
Since kbk = 1, we have
kak2 =	vi2pi ≥	vi2p0 =p0 kvk2 =p0.
(116)
i=1
i=1
sin2 θ(a, b) =
ka -ha, bi∙ bk2
kak2
(117)
2
c
c
Besides,
c
c
2
ka- ha, bi ∙ bk2 = E Vi√pi - Evjpj) √pi
i=1
j=1
2
cc
pi vi -	vjpj
i=1	j=1
(118)
cc
≥ p0	vi -	vjpj
i=1	j=1
Define s , arg maxi∈[c] vi and t , arg mini∈[c] vi, then
2
i=1
vi -	vjpj	≥
j=1
vs - X vjp) +卜-X vjp) ≥ (v-⅛vtf
(119)
c
c
2
From kvk = 1 we know that maxi∈[c] |vi| ≥ -√c. Besides, since Pc=I vi = 0, we have vs > 0 > vt.
Therefore, vs - vt > maxi∈[c] |vi| ≥ √1c. As a result,
ka - ha, bi ∙ bk2 ≥ P0 ∙ (vs 2vt) > p∣.
(120)
Moreover,
c
c
kak2 =	vi2pi ≤	pi = 1.
(121)
i=1
i=1
50
Thus,
J-0	g
sin2 θ(a, b) ≥ 牛=P0,	(122)
1	2c
which means that
2
VT Av ≥ po ∙1∙ p0 = p0.	(123)
2c	2c
Now we analyze the distribution of p0. Since z follows a spherical Gaussian distribution N (0, αIc),
we know that the entries of z are totally independent. Besides, for each entry zi (i ∈ [c]), we have
|zi| < α with probability β, where β ≈ 0.68 is an absolute constant. Therefore, with probability βc,
forall entries zi(i ∈ [c]), we have |zi| < α. In this case,
=exp(mini∈[c] Zi) ≥ exp(-α)
p0	Pc=I exp(zi) — Cexp(α).
(124)
In other cases, we know that p0 > 0. Thus,
λc-i = Ez~n (o,αic)	min	vT Av
v∈S,kvk=1
≥ βc
e exp(-a)
k cexp(α)
2T^
(125)
The right hand side is independent of n. Therefore, λc-ι > 0, which means that A has exactly
(c - 1) positive eigenvalues and a 0 eigenvalue, and the eigenvalue gap between the smallest positive
eigenvalue and 0 is independent of n.
H.3.4 Proof of Lemma 4
We first restate Lemma 4 here:
Lemma 4. Let DX denote the distribution of X, andTV (D1, D2) denote the total variation distance
between D1 and D2. Then ∀i, j ∈ [n], ∀ > 0
lim lim	Pr
n→∞ d→∞ W⑴~N(0,dInd),W⑶~N(0,1 Icn)
TV(DA,DA|Di,i=Dj,j=1)>	=0.
(71)
Proof of Lemma 4. The proof of this lemma requires knowledge about the distributions of A
condition on two diagonal entries of D. Since A can be uniquely determined by z, it is enough for
us to know the distribution of z condition on the two entries of D. We use the following lemma to
analyze this:
Lemma 13. With probability 1 over W (2), for any i, j ∈ [n], for any (p, q) ∈ {0, 1}2,
lim P (z|Dii = p, Djj = q) -→d z.
n→∞
(126)
Proof of Lemma 13. With {w1, . . . , wn} and {v1, . . . , vn} defined as above, since different
summands contributing to z are independent, and every vi is only affected by its corresponding Dii ,
P (z|Dii =p,Djj = q) = z - vi +P(vi|Dii =p) - vj +P(vj|Djj = q)	(127)
= z - wi(yi - P(yi|Dii =p)) +wj(yj - P(yj|Djj = q)).
For any i ∈ [n], with the condition of Dii = p, when p = 0, P(yi|Dii = p) = 0; when conditioned
with p = 1, P (yi |Dii = p) is of a half standard normal distribution truncated at 0. In both cases
the conditional distribution of P(yi|Dii = p) and hence yi - P(yi|Dii = p) has bounded mean and
variance. For any wi, by Lemma 11 we have
kwi k ≤	c max Wi(j2)2 < p4
cn- 3 with probability 1 over W(2).	(128)
i∈[c],j∈[n]
Since limn→∞ 44cn-2 = 0, as n goes to infinity we have
wi(yi - P(yi|Dii = p)) -→d 0 with probability 1 over W(2).	(129)
51
Therefore
P (z|Dii =p,Djj = q) = z - wi (yi -P(yi|Dii =p)) +wj(yj - P(yj|Djj = q)) -→d z. (130)
From Lemma 13 we conclude that
lim lim	Pr
n→∞ d→∞ W (I)〜N (0, d Ind),W ⑶〜N (0, 1 Icn)
TV(Dz,Dz|Di,i=Dj,j=1)>	=1.
Since A can be uniquely determined by z, we have
TV(Dz, Dz∣Di,i=Dj,j=ι) ≥ TV (Da,。人”,=口,“=1).
Therefore,
lim lim	Pr	TV(DA, DA|D =D =1) >	= 0.
n→∞ d→∞ W⑴〜N(0, dInd),W⑶〜N(0,1 Icn) L	1 i,i	j,j	」
This finishes the proof of Lemma 4.
(131)
(132)
(133)
H.3.5 Proof of Lemma 5
We first restate Lemma 5 here:
Lemma 5. With probability 1 over W (2),
lim
n→∞
IWMW TIlF
-kMF
1.
Proof of Lemma 5. To prove the equivalence between IIWMWT IIF and kM k2F, we need some
other terms, including terms containing M*, as bridges. To prove the equivalence between M and
M*, we need the following lemma which explains the reason why we only need the weaker sense of
independence (Lemma 4) instead of the total independence between A and D.
Lemma 14. Let p(A, D) be a homogeneous polynomial of A and D and is degree 1 in A and degree
2 in D, and let the coefficients in P are upper bounded in 'ι-norm by an absolute constant. Also let
A0 be an independent copy of A. Then
lim lim	Pr
n→∞ d→∞ W ⑴〜N (0, d Ind),W ⑶〜N (0, d Icn )
[|E[p(A, D)] - E[p(A0, D)]| >]
0.
(134)
Proof of Lemma 14. Assume that p(A, D) = Pim=1 ciAs(i),t(i)Du(i),u(i)Dv(i),v(i), then from
linearity of expectation we know
m
E[p(A, D)] =	ciE[As(i),t(i)Du(i),u(i)Dv(i),v(i)].	(135)
i=1
Since the entries of D can only be 0 or 1, we have
E[As(i),t(i)Du(i),u(i)Dv(i),v(i)] = E[As(i),t(i)|Du(i),u(i) = Dv(i),v(i) = 1].	(136)
Assume that the upper bound of the sum of |ci|s is α, i.e., Pm=I α| ≥ α. Set e0 =春 and from
Lemma 4 we know that
lim lim	Pr
n→∞ d→∞ W ⑴〜N (0, d Ind),W(2)〜N (0, n Icn)
TV(DA,DA|Di,i=Dj,j=1)>0
= 0.
(137)
In other words, with probability 1 we have TV(DA, DA|Di,i =Dj,j =1) ≤ 0. Besides, since ∀i, j ∈
[c], i = j,Pi,Pj,Pi + Pj ∈ (0,1), each entry of A (either Pi - p2 or -Pipj) must be in (-ɪ, 4).
Therefore, when n and d goes to infinity, with probability 1 overW(1) andW (2) we have
kE[As(i),t(i)|Du(i),u(i) = Dv(i),v(i) = 1] - E[As(i),t(i)]k
≤TV(Da,DA∣Di,i=Dj,j=ι) ∙
(138)
52
Thus,
m
|E[P(A, D)] - E[p(A0, D)]| ≤ X |ci| ∙ kE[As(i),t(i)|Du(i),u(i) = Dv(i),v(i) = 1] 一 E[As(i),t(i)]k
i=1
≤ (X 同)•三 ≤α ∙ 2a<e.
This finishes the proof of Lemma 14.
After this, using Lemma 4, we have the following lemmas:
Lemma 15. with probability 1 over W,
(139)
lim ≡⅛ = 1.
n→∞ kM*kF
Proof of Lemma 15. Let (D0, A0) be an independent copy of (D, A), then
kMk2F = E[DW T AW D]2F
= E hDWTAWD, D0WTA0WD0i
=E [tr (DWTAWDD0WTA WD0)]
=E [tr (WD0DWTAWDD0WTA0)].
(140)
Expressing the term inside the expectation as a polynomial of entries of A, D, A0 and D0 , we get
tr (W D0DW T AW DD0W T A0〉)
c
:X (WD0DWTAWDD0WTA0)ii
i=1
c
:X (W D0DW T A)ij (W DD0W T Aj
i,j=1	,	,
c
(141)
Σ
i,j=1
cn
Wi,lWk,lD0l,lDl,lAk,j
Wj,tWs,tDt,tD0t,tA0s,i
=	Wi,l Wk,l Wj,tWs,tAk,jA0s,iDl,lD0l,lDt,tD0t,t .
i,j,k,s=1 l,t=1
Now we can bound the `1 norm of the coefficient of this polynomial as follows (note that absolute
value of each entry of A is bounded by 1):
cn
X X Wi,l Wk,l Wj,tWs,t
i,j,k,s=1 l,t=1
cn
≤ E ∑ IWi,ι∣∙∣Wk,ι∣∙∣wj;t∣∙∣Ws,t∣
i,j,k,s=1 l,t=1
cn
cn
EEIWi,ιHWk,ι∣	EEIWj,t∏Ws,t∣
i,k=1 l=1
(P G Wi2I + W洛
≤ E E	i'l 2
i,k=1 l=1
j,s=1 t=1
’X X W2t + Ws2,t
j,s=1 t=1	2
(142)
c
X
i,k=1
kWik2 + kWkk2
c
X
j,s=1
kWj k2 + kWsk2
2
1
2
=(ckWk2F)2=c2kWk4F.
53
When n → ∞, we know that kW k2F = O(c) with probability 1 over W, so the coefficient of this
polynomial is `1 -norm bounded. We know from Lemma 4 that the distribution of A is invariant
condition on two entries of D. Furthermore, since A0 and D0 are independent copies of A and D, we
know that the distribution of (A, A0) is invariant conditioning on two entries of D and two entries of
D0 . Each term in this polynomial is a 4-th order term containing two entries from D and two from
D0. This combined with Lemma 14 gives us
lim
n→∞
kMkF
∣∣M*kF
1.
(143)

Lemma 16. ForaUi,j ∈ [c],limn→∞((WMWT)W一(WM*WT)Μ)=0. Thus,
lim
n→∞
IIWMW TIlF
∣∣WM *W T kF
1.
Proof of Lemma 16. This proof is very similar to that of Lemma 15. First, we focus on a single
entry of the matrix WMWT and express it as a polynomial of entries of D:
(WMWT)i,j = E[(WDWTAWDWT)i,j]
E
E
Σ
k=1
cn
DWTA)i,k(WDWT)k,j
Wi,lWs,lDl,lAs,k	Xt=n1Wk,jWj,tDt,t
(144)
As,kWi,lWs,lWk,tWj,tDl,lDt,t
k,s=1 l,t=1
Then we bound the `1 norm of the coefficients of this polynomial as follows:
cn
X X As,kWi,lWs,lWk,tWj,t
k,s=1 l,t=1
cn
≤ ∑ ∑ ∣Wi,ι∣∙∣Ws,ι∣∙∣Wk,t∣∙∣Wj,t∣
k,s=1 l,t=1
cn
=E Σ∣Wi.ι HWsj
s=1 l=1
c n Wi2l + Ws2
≤ Σ E i,l2
s=1 l=1
∣Wk,tHW∙M
W2,t + W2t
(145)
= c kWik2 + kW k2F	ckWjk2+ kW k2F
≤(2ckWk2F)2=4c2kWk4F.
Similar to Lemma 15, this coefficient is 'ι-normbounded. Therefore, using Lemma 14, We have with
probability 1 over W, for all i,j ∈ [c],limn→∞((WMWT)ij — (WM*WT)ij) = 0, which
indicates that
lim iiWMW T iiF
n→∞ ∣∣WM*WTkF
1.
Lemma 17.
lim
n→∞
llWM *W TIIF
∣M*kF
1.
E
1
2

54
Proof of Lemma 17. The proof of this lemma will be divided into two parts. In the first part, we will
estimate the FrobeniUs norm of M*, and in the second part We do the same thing for WM* WT.
Part 1: We know from the definition of M * that
M* = 1 (E[WTAW] + diag(E[WTAW])) .	(146)
∙-v
Define A := E[A], then
E[W T AW ] = W T E[A]W = W T A W.
(147)
From Lemma 9, ∀0 >0, with probability 1 we have WWT - Ic ≤ 0 . Besides, from Kleinman
& Athans (1968) we know that for positive semi-definite matrices A and B we have λmin(A)tr(B) ≤
tr(AB) ≤ λmax(A)tr(B), so
I ∣∣WTAW∣∣F TlAlIF = ∣tr(WTAWWTAW) - tr(AA)I
=tr(WWTAWWTA) - tr(AA)∣
≤ (∣∣WWT - Ic∣∣ + 1) tr(AWWTA) - tr(AA)I
=(∣∣WWT - Ic∣∣ + 1) tr(WWTAA) - tr(AA)∣
≤ (∣∣WWT - Ic∣∣ + 1)2 tr(AA) - tr(AA)∣
≤∣∣ww T - Ic ∣2∣∣A∣∣F+2 *∣∣WW T - Ic∣∣∣∣A∣∣F.
For any e > 0, set e0 = min{ f,亨} gives us with probability 1,
2
i.e.,
lim
n→∞
W TA WHF TIAH
∙-v
A∣∣F
W T A w∣∣2
lim -----------
n→∞	∣∣A∣∣
2 F = 1.
0,
(149)
(150)
F
2
F
Besides, if we denote the i-th colUmn of W by wi , then
n
∣∣diag(E[W T AW ])∣∣F = X(WT A Wi)2
i=1
n
≤ X(kWik2 ∙∣∣A∣∣)
i=1
2n
= ∣∣A∣∣ XkWik4.
i=1
(151)
Since E[n2 kwik4] = c2 + 2c, by the additive form of Chernoff bound we get
Pr Xi=1 kwik4 ≥ c
Therefore, when n → ∞, with probability 1 we have
+^c =Pr
n
Pn=I n2 kwik4 -(C2 + 2c) ≥ c) ≤ e-2nc2
(152)
n
2n	2	2
∣∣diag(E[WTAW])∣∣f ≤∣∣A∣∣ X kwik4 ≤ ∣∣A∣∣ ∙ -ɪ-
i=1	n
(153)
55
Thus, with probability 1,
lim
n→∞
IIdiag (E[ WT AW])∣∣F
IlW T A w||F
0,
i.e.,
lim
n→∞
1
16
a IIF
kM *kF
1.
Part 2: Plug equation Eq. (146) into WM* W and We get
WM*W = 4 (E[WWTAWWT] + E[Wdiag(WTAW)WT]).
Similar to Part 1, When n → ∞, With probability 1, We have
lim
n→∞
IIE[WWTAWWT]II2F
1.
Besides, When n → ∞, With probability 1 We have
IIw diag(E[W T AW ])W TIIF ≤ k W kF Ia『XX kwik4 ≤ Ja]2 ∙ c2+^c k W kF.
i=1	n
As a result, With probability 1,
lim
n→∞
jW diag(E[W T AW ])W TIIF
∣ww T A ww t∣∣F
0,
i.e.,
lim
n→∞
1
16
AIIF
∣∣WM *W T kF
1.
(154)
(155)
(156)
(157)
(158)
(159)
(160)
Combining the results of Part 1 and Part 2 proves this lemma.
Combining Lemma 15, Lemma 16, and Lemma 17 directly finishes the proof of Lemma 5.
H.3.6 Proof of Theorem Theorem H.1
Proof of Theorem Theorem H.1. NoW We can proceed to the proof of our main theorem. In this
proof, We Will use the bounds for kM kF, Which are formalized into the lemma beloW:
Lemma 18. With probability 1, limn→∞ kMkF is both lower bounded and upper bounded by
constants that are independent of n.
Proof of Lemma 18. From Lemma 15 we know that limn→∞ JMJkF = 1, so we only
need to bound limn→∞ ∣∣M *|京 Since M * = 1 (E[WT AW ]+ diag(E[WT AW ])) and
hE[W T AW], diag(E[W T AW])i ≥ 0, we have
kM*∣F ≥ ∣∣E[WTAW]∣∣f = ∣∣wTAw∣∣f .	(161)
∙-v
From equation Eq. (125), we know that the first (c- 1) eigenvalues of A is bounded by some constant
e exp(-α) ) 2	〜
Y，βc ∙ 3 exp；。" which is independent of n. Then we analyze the eigenvalues of WT A W: From
Lemma 9 and set E = 2, we know that the smallest singular value of W is lower bounded by 2.
Therefore, for any unit vector v in the row span of W, we have
vTWTAWV = (WV)TA(WV) ≥ γ kWvk2 ≥ 4.	(162)
56
∙-v
Thus, Il WTAW∣∣ ≥ 4, which is some constant that is independent of n.
Besides, since D is a dignonal matrix with 0/1 entries, and the absolute value of each entry of A is
bounded by 1, we have
kMkF = IIE[DW T AW D]IIF ≤ IIE[W T AW]IIF ≤ kWk2F kAkF ≤ckWk2F.	(163)
From Lemma 8, we know that with probability 1, kW k2F ≤ 2c, therefore, kM kF is upper bounded
by 2c2, which is independent ofn.
Now we are ready to prove our main theorem.
From Lemma 5 we have
lim
n→∞
IlWMW TIlF
-kMF-
1.
(164)
Then we consider IIWTWMWTWII2F. Note that
IIWTWMWTW II2F = tr(WTWMWTWWTWMWTW)
= tr(WWTWMWTWWTWMWT).
(165)
From Lemma 9 we know that for all 0 > 0,limn→∞ Pr(IW W T - IcI ≥ 0) = 0. For notation
simplicity, in this proof we will omit the limit and probability arguments which can be dealt with
using union bound. Therefore, we will directly state IWWT - Ic I ≤ 0. From Kleinman & Athans
(1968) we know that for positive semi-definite matrices A and B we have λmin(A)tr(B) ≤ tr(AB) ≤
λmax(A)tr(B), so
∣tr(WWT ∙ WMWTWWTWMWT)- tr(WMWTWWTWMWT)∣
≤ max{1 - λmin(WWT), λmax(WWT) - 1}tr(WMWTWWTWMWT)	(166)
≤ IIWWT - IcII tr(WMWTWWTWMWT) ≤ 0tr(WMWTWWTWMWT).
Similarly,
|tr(WMWTWWTWMWT) - tr(WMWTWMWT)|
= ∣tr(WWT ∙ WMWTWMWT) - tr(WMWTWMWT)|	(167)
≤ IIWWT - IcII tr(WMWTWMWT) ≤ 0tr(WMWTWMWT).
Therefore,
| IIWT WM WT W II2F - IIWMWT II2F |
= ∣tr(WWT ∙ WMWTWWTWMWT) - tr(WMWTWMWT)|
≤∣tr(WWT ∙ WMWTWWTWMWT) - tr(WMWTWWTWMWT)|
+ |tr(WMWTWWTWMWT) - tr(WMWTWMWT)|	(168)
≤0tr(WMWTWWTWMWT) +0tr(WMWTWMWT)
≤0(1 + 0)tr(WMWTWMWT) + 0tr(WMWTWMWT)
≤(20 + (0)2)tr(WMWTWMWT) = (20 + (0)2) IIWMWTII2F .
For all e > 0, select e0 < min{等,4 }, We have
In other words,
Hence we get
| IIWTWMWTWII2F - IIWMWTII2F | < IIWMWTII2F .
IIWT WMWT W II2
lim	F-^
n→∞ kWMWT k2F
1.
lim
n→∞
lW T WMW T WllF
kMF
1.
(169)
(170)
(171)
57
Next, consider the orthogonal projection matrix PW that projects vectors in Rn into the subspace
spanned by all rows of W. We will consider the matrix PWMPW. Define δ , WTW - PW,
then from Lemma 10 we get kδ k2F ≤ 0 . Therefore,
| WTWMWTWF - kPWMPWkF | ≤ kPWMδkF + kδMPWkF+kδMδkF
≤ kMkF 2 kPW kF kδkF + kδk2F	(172)
≤ kMkF (2 ∙ 4c2e0 + (C2).
For all e > 0, we choose e0 < min{亨,1^ } and have
| ∣∣WTWMWTW∣∣F -kPwMPWkF |
西	<£，
(173)
which means that
| ∣∣WTWMWTW∣∣F -kPwMPWkF I
n→∞	∣∣W T WMW T W kF
Thus,
lim kPW M PW kF
n→∞ -kM kF-
lim
n→∞
∣∣∣WTWMWTW∣∣F -kPWMPWkF I
=nɪ	kME
(174)
kPWM PWkF	=1	(175)
kW T WMW T W ∣∣F	(	)
0.
Note that kM k2F = kPWMPWk2F+∣∣PWMPW⊥ ∣∣2F+∣∣PW⊥MPW∣∣2F+∣∣PW⊥MPW⊥ ∣∣2F. There-
fore,
∣∣Pw M PW ∣∣F + ∣∣PW M Pw ∣∣F + ∣∣PW M PW ∣∣F	kMkF -kPW M PW kF
n→∞	kM kF	— n→∞ kM kF 一
(176)
In other words,
lim	UPWMPW ∣∣F
n→∞	kM kF
lim	∣∣PW M PW ∣∣F = lim	∣∣PW M PW ∣∣f
n→∞ kM kF	n→∞ kM kF
(177)
From Lemma 18 we know that limn→∞ kMkF (this hasn’t proved to be exist, so we perhaps need
to say “for large enough n”) is lower bounded by some constant that is independent of n, so
lim ∣∣PW M PW⊥ ∣∣F	= lim ∣∣PW⊥ MPW ∣∣F	= lim ∣∣PW⊥ MPW⊥ ∣∣F	= 0.	(178)
n→∞	F n→∞	F n→∞	F
Note that
M = PW MPW + PW MPW⊥ + PW⊥ MPW + PW⊥ MPW⊥ .	(179)
Thus,
lim kM - PWMPW kF = 0.	(180)
n→∞	F
For any e > 0, set δ < min{悬,^IY}, from Lemma 10, we know that with probability 1,
∣∣PW - WT W ∣∣F ≤ δ. Therefore,
∣PWMPW - WTWMWTW ∣F
≤∣∣PW-WTW∣∣2FkMkF+2∣∣PW-WTW∣∣FkMkFkPWkF
≤δ2 ∙ 2c2 + 2δ ∙ 2c2
<.
In other words,
lim ∣∣PW M PW - WTWMWTW ∣∣	= 0.
n→∞	F
Now we conclude that
lim ∣∣M - WT WMWTW ∣∣	= 0.
n→∞	F
(181)
(182)
(183)
58
From Lemma 16WeknoWthat ∀i,j ∈ [c], limn→∞(( WMW T )i,j - (WM * W T )i,j) = 0, i.e.,
lim ∣∣WMWT - WM*WTIL =0.	(184)
n→∞	F
Since
∣∣WTWMWTW - WTWM*WTW∣∣F ≤ kW k2F ∣∣WMWT - WM*WT∣∣F , (185)
from Lemma 8 Which bounds the Frobenius norm of W We knoW that
lim ∣∣WTWMWTW - WTWM*WTW∣∣ = 0.	(186)
n→∞	F
Thus,
lim ∣∣M - WTWM*WTW∣∣ = 0.	(187)
n→∞	F
Note that M* = 4 (E[WTAW] + diag(E[WTAW])), so
4W T WM *W T W = W T WW T A WW T W + W T W diag(E[W T AW ])W T W. (188)
We will first analyze the second term on the RHS of equation Eq. (188). ∀e > 0, set e0 = √,
and from Lemma 9 We knoW that ∣WWT - Ic∣ < 0 With probability 1, Which means that
| ∣∣WWT∣∣F - c| < With probability 1. Set = c, We knoW that ∣∣WWT∣∣F < 2c With probability
1. Note that
∣∣WTWdiag(E[WTAW])WTW ∣∣F ≤ ∣∣WTW ∣∣2F ∣∣diag(E[W T AW])∣∣F
= ∣∣WWT∣∣2F ∣∣diag(E[W T AW])∣∣F (189)
≤ 4c2 ∣∣diag(E[W T AW])∣∣F .
Combine this With equation Eq. (154) and We have
∣∣WTWdiag(E[WTAW])WTW∣∣f _
lim	∏ Σ ∏	= 0.	(190)
n→∞	∣∣w T A w∣∣f
From equation Eq. (162) we know that ∣∣ WTAWn ≥ 4 with probability 1, so
lim ∣∣4WT WM* WT W - WT WWTAWWT W ∣∣ =0.	(191)
n→∞	F
Similarly, define δ , WWT - Ic, then
∣∣wTWWTAWWTW - WTAW∣∣F
≤ ∣∣WTδAδW∣∣F + 2 ∣∣WTAδ∣∣F	(192)
≤kw kF kδkF∣∣A∣∣F + 2 kw kF kδkF∣∣A∣∣F.
Set e0 < min{套,a∕8F}, then from Lemma 9 we know that |怜|恨 < e0 with probability 1, and from
Lemma 8 we have ∣∣ W∣∣f ≤ 2c with probability 1. We also have ∣∣ A∣∣ ≤ C since each entry of A
is bounded by 1 in absolute value. Therefore,
∣∣WTWWTAWWTW - WTAW∣∣F ≤ 4c2(e0)2 ∙ C + 2 ∙ 2ce0 ∙ c< 2 + 2 = e,
which means that
lim ∣∣WTWWTAWWTW - WTAW∣∣ = 0.
n→∞	F
From equations Eq. (187), Eq. (191), and Eq. (194) we get
lim M - 1W T A W	=0.
n→∞ ∣	4	∣
(193)
(194)
(195)
59
Besides, from equation Eq. (95) in Lemma 10 we know that for any 0 > 0,
∣∣W - WllF = XllWi - Wi∣∣2 <e0,	(196)
i∈[c]
where W is the orthogonal version of W, i.e., We run the Gram-Schmidt process for the rows of W.
Define δ，W - W, for any e > 0, set e0 = min{参,p/ɪ}, we have with probability 1,
∣∣wTAW - WTAW∣∣F ≤ 2 kδ∣∣F ∣∣A∣∣F kWkF + ∣∣δ∣∣F ∣∣A∣∣F
4c20 + c(0)2 < .
(197)
Therefore,
lim ∣∣WTAW - WTAW∣∣ =0,	(198)
n→∞	F
which implies
1 VV^T ~ ——
lim M------W AW	=0.	(199)
n→∞ ∣	4	∣
F
From Lemma 3 we know that with probability 1, A is of rank (C - 1). Since A ∙ 1 = 0 is always true,
the top (C - 1) eigenspace of A is Rc∖{1}. Note that the rows in W are of unit norm and orthogonal
to each other, we conclude that WT A W is of rank (C - 1) and the corresponding eigenspace is
CTTyT
R{Wi}C=ι∖{ W ∙ 1}. Moreover, the minimum positive eigenvalue of W AW is lower bounded
by γ .
As for the top C — 1 eigenvectors of M, define δ，M — 4 WT AW, then M = 4 WT AW + δ.
Define Si as the top C - 1 eigenspaces for M, and S? to be the top C - 1 eigenspaces for 41 WT AW.
Then from Davis-Kahan Theorem we know that
ksinΘ(S1, S2)∣∣F ≤ ——k¾T__ .	(200)
λc-1( 11 W AW)
Here Θ(S1, S2) is a (C - 1) × (C - 1) diagonal matrix whose i-th diagonal entry is the i-th canonical
angle between S1 and S?. Since limn→∞ |怜|恨=0, and with probability 1, λc-1(4 WT AW) ≥ Y
which is independent of n, we have with probability 1,
lim ∣∣sinΘ(S1,S2)kF =0,	(201)
n→∞
which indicates that the top C - 1 eigenspaces for M and ɪ WTAW are the same when n → ∞.
TT
Notice that the top C - 1 eigenspaces of W AW are R{Wi}C=1∖{W ∙ 1}, so M will also have the
same top C - 1 eigenspaces. Besides, from equation Eq. (95) we know that limn→∞ ∣∣ W - W11F1 =
0, so R{Wi}c=1∖{W ∙ 1} are the same as R{Wi}c=1∖{W ∙ 1}. This completes the proofofthis
theorem.
H.4 Experiment Results
Table 10: Overlap of R(S(k)) ∖ {S(k) ∙ 1} and the top C - 1 dimension eigenspace of E[M(k)] of
different layers at minima.
Dataset Network	MNIST		MNIST-R		CIFAR10		CIFAR10-R	
	F-15003	LeNet5	F-15003	LeNet5	F-15003	LeNet5	F-15003	LeNet5
fc1	0.602	0.890	0.235	0.518	0.880	0.951	0.903	0.213
fc2	0.967	0.931	0.801	0.912	0.943	0.972	0.931	0.701
fc3	0.982	0.999	0.998	0.999	0.993	0.999	0.996	0.999
Note that the overlap can be low for random-label datasets which do not have a clear eigengap (as in
Fig. 4). Understanding how the data could change the behavior of the Hessian is an interesting open
problem. Other papers have given alternative explanations which are not directly comparable to ours,
however ours is the only one that gives a closed-form formula for top eigenspace. In Appendix F.1
we will discuss the other explanations in more details.
60