Under review as a conference paper at ICLR 2021
Training by Vanilla SGD with
Larger Learning Rates
Anonymous authors
Paper under double-blind review
Abstract
The stochastic gradient descent (SGD) method, first proposed in 1950’s, has been
the foundation for deep-neural-network (DNN) training with numerous enhance-
ments including adding a momentum or adaptively selecting learning rates, or
using both strategies and more. A common view for SGD is that the learning rate
should be eventually made small in order to reach sufficiently good approximate
solutions. Another widely held view is that the vanilla SGD is out of fashion in
comparison to many of its modern variations. In this work, we provide a con-
trarian claim that, when training over-parameterized DNNs, the vanilla SGD can
still compete well with, and oftentimes outperform, its more recent variations by
simply using learning rates significantly larger than commonly used values. We
establish theoretical results to explain this local convergence behavior of SGD
on nonconvex functions, and also present computational evidence, across multi-
ple tasks including image classification, speech recognition and natural language
processing, to support the practice of using larger learning rates.
1 Introduction
We are interested in minimizing a function f : Rd → R with an expectation form:
minf(X) := Eξ [K (x,ξ )]，
(1a)
where the subscript indicates expectation is computed on the random variable ξ. Specially, if the
ξ probability distribution is clear and random variable ξ is uniformly distributed on N terms, the
objective function f can be expressed into a finite-sum form:
1N
minf (X) :=k Efi(X),
x∈Rd	N
i=1
(1b)
where fi(x) : Rd → R is the i-th component function. The optimization of Eqs.(1a) and (1b) is
widely encountered in machine learning tasks (Goodfellow et al., 2016; Simonyan & Zisserman,
2014).
To solve this problem given in Eq.(1b), we can com-
pute the gradient of objective function directly with a
classic GD (Gradient Descent) algorithm. However, this
method suffers from expensive gradient computation for
extremely large N , and hence people apply stochastic
gradient descent method (SGM) to address this issue. In-
cremental gradient descent (IGD) is a primary version of
SGM, where its calculation of gradient proceeds on single
component fi at each iteration, instead of the whole.
As a special case of IGD, SGD (Robbins & Monro, 1951),
a fundamental method to train neural networks, always
updates parameters by the gradient computed on a mini-
batch. There exists an intuitive view that SGD with
constant step size (SGD-CS) potentially leads to faster
Figure 1: The step size vs. iteration
for nonconvex functions: in a neighbor-
hood of the solution, the learning rate
can be a constant value below a thresh-
old.
1
Under review as a conference paper at ICLR 2021
convergence rate. Some studies (Solodov, 1998; Tseng,
1998) show that, under a so-called strong growth condition (SGC), SGD-CS converges to an opti-
mal point faster than SGD with a diminishing step size.
In this work, we study the local convergence (or last-iterate convergence as is called in Jain et al.
(2019)) of SGD-CS on nonconvex functions. Note that SGD-CS does not provide a guarantee of
converge when starting from any initialization point. Therefore, a useful strategy is to use SGD
with a decreasing step size at the beginning, and then switch to SGD-CS in a neighborhood of a
minimizer. Fig.1 illustrates the range of the step size of SGD versus the number of iterations.
Our main theoretical and experimental contributions are as follows.
•	We establish local (or last-iterate) convergence of SGD with a constant step size (SGD-
CS) on nonconvex functions under the interpolation condition. We note that previous re-
sults are mostly for strongly convex functions under strong (or weak) growth condition.
Our result is much closer to common situations in practice.
•	We discover that on linear regression problems with `2 regularization, the size of conver-
gent learning rates can be quite large for incremental gradient descent (IGD). Our numeri-
cal results show that, within a fairly large range, the larger step size is, the smaller spectral
radius is, and the faster the convergence rate is.
•	Based on the above observations, we further propose a strategy called SGDL that uses the
SGD with a large initial learning rate (more than 10 times larger than the learning rate in
SGD with momentum), while still being a vanilla SGD. We conduct extensive experiments
on various popular deep-learning tasks and models in computer vision, audio recognition
and natural language processing. Our results show that the method converges successfully
and has a strong generalization performance, and sometimes outperforms its advanced vari-
ant (SGDM) and other several popular adaptive methods (e.g., Adam, AdaDelta, etc).
2	Related Work
There are many papers on stochastic optimization and we summarize typical ones which are most
relevant with our work. The convergence of SGD for over-parameterized models is analyzed in
(Vaswani et al., 2018; Mai & Johansson, 2020; Allen-Zhu et al., 2019; Li & Liang, 2018). The
power of interpolation is studied in (Ma et al., 2018; Vaswani et al., 2019). The work (Jastrzebski
et al., 2017) investigates a large ratio of learning rate to batch size often leads to a wide endpoint. The
work (Smith & Topin, 2019) shows a phenomenon called super-convergence which is in contrast to
the results in (Bottou et al., 2018). More recently, several new learning rate schedules have proposed
for SGD (Loshchilov & Hutter, 2016; Smith, 2017; Agarwal et al., 2017; Carmon et al., 2018).
Adaptive gradient methods are widely in deep learning application. Popular solutions include
AdaDelta (Zeiler, 2012), RMSProp(Hinton et al., 2012), Adam(Kingma & Ba, 2014) and so on.
Unfortunately, it is believed that the adaptive methods may have a poor empirically performance.
For example, Wilson et al. (2017) have observed that Adam hurts generalization performance in
comparison to SGD with or without momentum.
The work (Schmidt & Roux, 2013) introduces SGD-CS that attains linear convergence rate for
strongly convex functions under the strong growth condition (SGC), and sublinear convergence rate
for convex functions under the SGC. The work (Cevher & Vu, 2018) investigates the weak growth
condition (WGC), the necessary condition for linear convergence of SGD-CS. For a general con-
vex function, the work (Ma et al., 2018) shows that SGD-CS attains linear convergence rate under
the interpolation property. For a finite sum f, the interpolation property means Vfi(x*) = 0 for
i = 1, . . . , N , which is held apparently in over-parameterized DNN. However, the theoretical
analysis of SGD-CS for nonconvex functions is less as mature as in the convex case.
2
Under review as a conference paper at ICLR 2021
3	Preliminaries
3.1	Notations
We denote the minimizer of object function f as x* and use usual partial ordering for symmetric
matrices: A 占 B means A 一 B is positive Semidefinite; similar for the relations W, >, Y. The norm
k ∙ k either denotes the Euclidean norm for vectors or Frobenius norm for matrices. The Hessian
matrix of each component function fi (x) is denoted as Hi (x). With slight abuse of notation, the
Hessian matrix Hi at x* is denoted as Hi*. We also denote the mean and variance of {H1*, . . . , HN* }
as H * ≡ N PN=I Hi and Σ* ≡ N PN=I(H*) 一 (H *)2, and define a ∧ b = min(a, b). X → Y
means random variable X converges to random variable Y in probability, while X -→ Y
means X
converges to random variable Y in L1 norm.
3.2	Assumptions
Our results are under the following mild assumptions:
(A.1) The objective function f has the same structure as given in Eq.(1b).
(A.2) Each component function fi in Eq.(1b) is twice continously differentiable, with the corre-
sponding Hessian matrix Hi being Lipschitz continuous for constant Li at the minimizer
x* . Without loss of generality, we suppose that
kHi (x) 一 Hi (x*)k ≤ Lkx 一 x* k,	for i = 1, . . . , N, where L = max Li .
i=1,...,N
(A.3) As in (Zhang et al., 2000, Remark 5.2), we assume the point of interest x* is a strong
minimizer (i.e. H(x*)	0).
Assumption (A.3) restricts that the objective function is essentially strongly convex in a sufficiently
small neighborhood of a minimizer. The work (Liu et al., 2020) investigates that the loss function is
typically nonconvex in any neighborhood of a minimizer. However, this assumption is not restrictive
in the regularization setting.
Remark 1. Even if the Hessian matrix of the point of interest x* has zero eigenvalues, we can study
the `2 regularized problem instead, and then the Hessian matrix becomes positive definite. This
phenomenon is also validated in our numerical experiment in Section 5, and we add `2 regularizer
in DNNs in Section 6.
Based on the above technical assumptions, we focus on the convergence analysis of SGD-CS by
discussing the convergence behavior precisely:
1m
xk+ι = Xk - α- y^ξ(jfxω (Xk).
m j=1	k
(2)
Next, we introduce the notion of point attraction to help us understand the convergence behavior to
Eq.(2).
Definition 1. We say x* is a point of attraction to Eq.(2) if there is an open ball N(x*, ) of x*,
such that for any x0 ∈ N, {xk} generated by Eq.(2) all lie inN and converges to x* in the `1 norm.
The Ostrowski Theorem says a sufficient condition for X* to be a point of attraction of the deter-
ministic iteration Xk+1 = T(Xk) is that the spectral radius of T0(X) is strictly less than one, which
further implies the condition that there exists an open ball N such that once the initial X0 ∈ N , the
iterates {Xk} will converge to X* with linear rate. We define the strong minimizer X* as a point of
strong attraction following the similar manner.
Definition 2. We say X* is a point of strong attraction to Eq.(2) if there exists a neighborhood N of
X*, such that for any X0 ∈ N, the sequence {Xk} generated by Eq.(2) all lie in N and satisfy
EkXk - x*k2 ≤ Pk ∙ ∣∣x0 — x*k2, for some P ∈ [0,1).
3
Under review as a conference paper at ICLR 2021
The following discussion studies necessary and sufficient conditions for the minimizer x* being a
point of attraction of SGD with the constant step size α. However, starting from any initialization
point, SGD-CS cannot be guaranteed to find such a local neighborhood around x*, except in some
special cases including the loss function studied in Section 5 and Appendix A.2. Therefore, a prac-
tical strategy is to use diminishing step size when starting from a random initial point (Allen-Zhu,
2018), and as long as the iteration points belong to a neighborhood of a point of attraction, one can
use a constant step size.
4 Results for nonconvex Functions
In this section, we rigorously show the necessary condition for the minimizer x* being a point of
attraction, and Theorem 1 provides a sufficient condition for the strong minimizer x* to be a point
of strong attraction with high probability. The detailed proofs are in Appendix.A.1.
4.1	Necessary Condition
Lemma 1. Suppose that the assumptions (A.1) and (A.2) hold, then
∣∣Vfi(x)k <Le2 + ∣∣H*ke for i = 1,...,N,
provided that x ∈ N(x*, ).
Theorem 1. Suppose that the assumptions (A.1) and (A.2) hold. If the minimizer x* is a point of
attraction of Eq.(2), then the interpolation property is satisfied, i.e.,
Vfi (x* ) = 0 for i = 1, . . . , N.
4.2	Sufficient Condition
4.2.1	S ufficient Condition for Points of Strong Attraction
Define the error function ei (x) as
ei(x) = -Hi* ∙ (x - x*) + [Vfi(x) - Vfi(X*)].	(3)
This error function quantifies the residual for the first-order Taylor expansion for Vfi (x) around
the point x* since
Vfi(x) = Vfi(x*) + Hi*(x - x*) + ei(x).
By substituting ei(x) into Eq.(2), we have
1m
xk+1 - X = Xk - α~ ɪ3[Hξj) ^ (xk — X ) + eξj) (Xk )] — X
m j=1 k	k
1m
=m ^X [I — aHξ*(j) )(Xk — x*) — αeξj) (Xk)].
j=1	k
(4)
In addition, our proof for the sufficient condition indicates a bound on the error function.
Lemma 2. Suppose that the assumptions (A.1) and (A.2) hold, and X* is a local minimizer off. If
the interpolation property is satisfied, then the error function defined in Eq.(3) is bounded by:
kei(X)k ≤ LkX — X*k2.
(5)
Under the assumption that {Xk} stay in the local neighborhood, we can show that interpolation
property is a sufficient condition for X* being a point of strong attraction of Eq.(2), provided that the
step-size α is sufficiently small.
Theorem 2.	Suppose that the assumptions (A.1)-(A.3) hold, then {Xk} generated from Eq.(2) all lie
in the neighborhood N (X*, ) and E[kXk — X* k2] ≤ ρE[kXk-1 — X* k2], if the radius ρ and the step
4
Under review as a conference paper at ICLR 2021
size α satisfy the following conditions:
P ≡ λmax ∣I - 2αH* + α2(H*)2 + α2Σ* + α2L2e2I + 2αLe∕) < 1,
0 <α ≤ i=minN { λmaχ(H*) , 2λmaχ(H *) , Lλmin(H)},
0 < α < 2λmin [(H*)2 +Σ* +L2e2I]-1/2(H* -LeI)[(H*)2 +Σ* + L2e2I]-1/2 .
(6a)
(6b)
(6c)
If Vfi (x*) = 0 for i = 1,..., N, then the minimizer x* is a point of strong attraction of the
iteration (2).
4.2.2 Staying in the Local Neighborhood
Now we show that {xk} generated from Eq.(2) all stay in the local neighborhood with high prob-
ability. The proof follows the idea in (Tan & Vershynin, 2019). In the remaining of this section,
we abuse the notation slightly and denote Xk as a random vector in the k-th iteration of SGD-CS
scheme, and xk as the realization of Xk. Considering the setup of Theorem 2 without assuming that
{xk} always lies in the neighborhood N(x*, e), we have the following technical results.
Lemma 3. Let x° ∈ N(x*, e) such that ∣∣xo 一 x*k ≤ √7e for some δ ∈ [0,1). Define the stopping
time τ = min{k : Xk ∈/ N(x*, e)}, then P{τ = ∞} ≥ 1 - δ.
Remark 2. With the setup stated in Lemma 3, the sequence {xk} stay in the local neighborhood
N(x* , e) with probability at least 1 - δ.
Indeed, we can show the result in Theorem 2 by relaxing the assumption into that the sequence
generated by Eq.(2) stays in the local neighborhood within infinite iterates. Under the setup stated
in the Lemma 3, we build the sufficient condition for points of strong attraction with the probabilistic
guarantee.
Theorem 3.	There exists an event E which holds with probability at least P (E) ≥ 1 - δ and the
sequence {xk} generated by Eq (2) satisfies
E[∣Xk -x*∣21E] ≤ρk∣x0 -x*∣2	(7)
Remark 3. It’s clear that x* is a point of attraction if it is a point of strong attraction. Therefore,
once the event that the iterates generated from SGD-CS stay in the local neighborhood happens, the
interpolation property becomes a necessary and sufficient condition for the strong minimizer x* to
be a point of attraction to Eq.(2).
5 Case Studies on Large Learning Rates
This section shows Incremental Gradient Descent (IGD) with a constant step size actually converges
into the global optimum point for some special problems, and then some numerical experiments are
presented to illustrate this point. We present the results through an example as follows, with more
examples appeared in Appendix.A.2.
f(x) = 1 ∣Ax-bk2 + 2 kD1/2xk2.	(8)
Under some mild assumptions (c.f Example 3 in Appendix.A.2), the IGD with constant step size t
converges when
,(	2N
t ∈ 0,ρ(μD + ATA)
where ρ is the spectral radius.
The problem in Eq.(8) can be extended into general nonlinear least squares:
1N	1
f (X) = 2 Eri(X) = 2R(X)TR(X).
i=1
5
Under review as a conference paper at ICLR 2021
Figure 2: Left: the range of spectral radius where IGD converges, it can be seen there are a wide
range t for convergence. Middle: the Log error ∣∣x - X∣∣2. The error line in blue grows slowly after
an initial rapid rise. This implies that in the presence of noise, large t-values can still calculate x
close to x*. Right: calculated solution of a large radius (3.0),the computed values of χ(t) do not
significantly deviate from the noisy data X within the large t-value.
A sufficient condition for the convergence of IGD is
t ∈ S λmaχ(E[Hi(x*)])
where E[Hi(χ*)] ≡ -N Pi Hi(χ*), and Hi(∙) denotes the Hessian matrix for the i-th component
function ri(∙).
We conduct a numerical simulation for the regularized least squares problem in Eq.(8). Let training
data x be generated via X = ι+1ez + N(0, σ2) where Z is uniformly sampled from (0,10). The min-
imizer x* does not necessarily represent the best solution available due to noise, so all approximate
solutions within the same noise level should be equally good in fact.
The experimental results demonstrate that with large step sizes, IGD still converge and have a decent
performance, which meets our expectations. After the first sharp rise, the step size can be updated
insensitive to the error. In our next section, we will implement vanilla SGD with a large initial
learning rate and use this optimizer for large scale problems.
6 Experiments
We show the empirical results of different models to compare our SGDL with other popular opti-
mization methods, including SGD, SGDM, Adam, and AdaDelta. We focus on the CIFAR10 and
CIFAR100 image classification task (Krizhevsky et al., 2009), with the downsampled variant of Im-
ageNet named as ImageNet32 (Chrabaszcz et al., 2017), the Speech Commands Dataset for audio
recognition(Warden, 2018), and the language modeling task on Penn Treebank(Marcus et al., 1993).
6.1	CIFAR 1 0 AND CIFAR100
(a) Train error on CI- (b) Test error on CI- (c) Train error on CI- (d) Test error on CI-
FAR10.	FAR10.	FAR100.	FAR100.
Figure 3: Different learning rate for the vanilla SGD in the ResNet56 model on CIFAR10 and
CIFAR100. Models are trained with learning rate from 0.2 to 1.
The first experiment is carried out on the CIFAR10 and CIFAR100 datasets, which are standard im-
age collection with 10 and 100 classes respectively. After the preprocessing in Appendix.A.3.1,
we train VGG (Simonyan & Zisserman, 2014), ResNet (He et al., 2016) and DenseNet(Huang
et al., 2017) for up to 150 epochs and minibatch size to 256. Additional, for all optimizers without
6
Under review as a conference paper at ICLR 2021
AdaDelta, we use an annealing strategy that the learning rate is lowered by 10 times at epoch 50 and
100. For ResNet experiments, we select a ResNet network with 56 layers and 120 layers respec-
tively. For VGG, we use VGG16. For DenseNet, we use a DenseNet with 100 layers and growth
rate k = 12. We first run a ResNet56 model on CIFAR10 and CIFAR100 with different learning
rates. From the results show in Fig.3 we can see a small learning rate like 0.2, 0.4 has a relatively
poor performance on the test set.
。-。-0-ij-Sel
6
50	100	150	0
Epoch
6
50	100	150	0
Epoch
14
12
10
8
4
50	100	150	0
Epoch
50	100	150
Epoch
W
(a)	Test Error on CIFAR10.
% -0-ij-Sel
% )≡ij-sel
% )≡ij-sel
)
0	50	100	150
Epoch
% )≡ij-sel
26
0	50	100	150	0	50	100	150
Epoch	Epoch
0	50	100	150
Epoch
(b)	Test Error on CIFAR100.
Figure 4: The performance of several popular models on CIFAR10 and CIFAR100. Figures from
left to right are for ResNet56, ResNet110,VGG16 and DenseNet, respectively.
We compare different optimization method including SGD,SGDM,Adam,AdaDelta, and shows the
performance in Fig.4. We can see that our method still works fairly well and has the same or even
better performance in comparison to SGDM and other optimizers.
6.2	Imagenet32
The second experiment is carried out on ImageNet32 dataset (Deng et al., 2009) with more than a
million images in 1000 classes, out of which 50000 images are used as a testing set. Each image has
32 × 32 pixels. The objective is to train an image classifier. We apply a similar train strategy as on
the CIFAR10 and CIFAR100 datasets in Section 6.1, except that the epoch budget is down to 120
and the learning rate is shrunk to one-tenth of its current value every 30 epochs.
(a) Train Error.	(b) Top-1 Error.	(c) Top-5 Error.
Figure 5: Left: Train error for ResNet56 on ImageNet32; Middle: Top-1 error; Right: Top-5 error.
Our experiments is only focus on ResNet56 due to computational resource limitation. To adjust
the difficulty on this dataset, we increase the model capacity for the previously used model by
quadrupling the number of channels on each convolution layer. From the results on Fig.5, we find
our method outperforms SGDM slightly about 0.5% for the best accuracy. More interestingly, SGD
also has a good performance, while two adaptive methods both work just passably.
7
Under review as a conference paper at ICLR 2021
6.3	Audio Recognition
The third experiment is carried out on the Speech Commands Dataset, which consists of recordings
from thousands of different people in uncontrolled recording conditions. Each sample is represented
as a 16000-dimension vector. Each recording is one second in length. We divide the data into a
training set and a testing set.
With the dataset, we train a 2-layered neural
net with 20 channels, a 2D dropout layer and 2
full-connection layers with 1,000 hidden nodes.
More specific hpyerparameter settings is in Ap-
pendix.A.3.2. From the results in Fig.6, we can
see that SGDL has quite competitive effect and
outperforms vanilla SGD exceedingly.
(a) Train Error.
(b) Test Error.
6.4	LSTM Language Model
The fourth experiment is carried out on the Figure 6: LeNet5 on Speech Commands Dataset.
standard language modeling task Penn Tree-
bank (PTB) dataset. The learning rate is se-
lected and optimized for several years and the
current state-of-the-art results supported our
perspective about large learning rate (Merity et al., 2017a). We focus to compare the effect of
SGDL, SGDM and other optimizers on top of a state-of-the-art {2, 3}- LSTM training recipe with
some training tricks (Merity et al., 2017b;a; Inan et al., 2016; Gal & Ghahramani, 2016). The details
on LSTMs are in Appendix.A.3.3.
(a)	Perplexity in 2-layer LSTM.
(b)	Perplexity in 3-layer LSTM.
Figure 7: LSTM on Penn Treebank Dataset. For Fig. (a) and Fig. (b), the left shows Train Perplexity
and the right shows Test Perplexity.
From the result in Fig.7, we evaluate the performance by perplexity metric. We find that Adam is the
fastest on the initial progress, but its final performance is worse than our SGDL. We also investigate
AdaDelta has terrible performance, while SGDM has almost same curves with ADAM.
6.5 Discussion
In Sections 6.1-6.4, we have observed that if vanilla SGD has the same learning rate as SGDM,
it tends to a poor generalization performance in some tasks and this is one of the reasons why
SGD is usually shelved for training models nowadays. Our experiments show that a larger initial
learning rate increases the test accuracy on Fig.3. Further more, it can be seen that SGDL has
an admirable performance in our experiments and outperforms SGDM in some tasks. For various
deep neural networks such as ResNet,DenseNet,VGG, our SGDL works well, which is not affected
by abundance layers or the existence of the residual mechanism. The experimental results have
provided evidence that SGD with large learning rate is a good alternative to SGD with momentum
in some practical tasks.
8
Under review as a conference paper at ICLR 2021
7 Conclusion
We provide a rigorous proof on the local convergence behavior of SGD-CS on smooth and noncon-
vex functions. Motivated by numerical experiments on IGD, we recommend a vanilla SGD with
large learning rate (SGDL) for training neural networks. Extensive evaluations have been carried
out in deep learning tasks (CV,Speech,NLP) with popular neural network architectures. The results
have demonstrated the smooth convergence and effective generalization performance of SGDL. For
our future work, we will analyze the effect from more training strategies such as batch normalization
about convergence. We will also analyze the pros and cons in SGD and its variants (SGDM) in our
further work.
9
Under review as a conference paper at ICLR 2021
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi-
mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, pp.1195-1199, 2017.
Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and non-
convex sgd. In Advances in Neural Information Processing Systems, pp. 1157-1167, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Leon Bottou, Frank E Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal on Optimization, 28(2):1751-1772, 2018.
Volkan Cevher and Bang Cong Vu. On the linear convergence of the stochastic gradient method
with constant step-size, 2018.
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an
alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pp. 1019-1027, 2016.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. Cited on, 14(8), 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A
loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information
theoretically optimal. volume 99 of Proceedings of Machine Learning Research, pp. 1752-1755,
Phoenix, USA, 25-28 Jun 2019. PMLR. URL http://proceedings.mlr.press/v99/
jain19a.html.
StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
10
Under review as a conference paper at ICLR 2021
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-
parameterized systems of non-linear equations: the lessons of deep learning. arXiv preprint
arXiv:2003.00307, 2020.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effec-
tiveness of sgd in modern over-parametrized learning. In International Conference on Machine
Learning, pp. 3325-3334, 2018.
Vien V Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum
for nonsmooth nonconvex optimization. arXiv preprint arXiv:2002.05466, 2020.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-
guage models. arXiv preprint arXiv:1708.02182, 2017a.
Stephen Merity, Bryan McCann, and Richard Socher. Revisiting activation regularization for lan-
guage rnns. arXiv preprint arXiv:1708.01009, 2017b.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition. arXiv preprint arXiv:1308.6370, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Confer-
ence on Applications of Computer Vision (WACV), pp. 464-472. IEEE, 2017.
Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using
large learning rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations
Applications, volume 11006, pp. 1100612. International Society for Optics and Photonics, 2019.
Mikhail V Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Com-
putational Optimization and Applications, 11(1):23-35, 1998.
Yan Shuo Tan and Roman Vershynin. Phase retrieval via randomized kaczmarz: Theoretical guar-
antees. Information and Inference: A Journal of the IMA, 8(1):97-123, 2019.
Paul Tseng. An incremental gradient (-projection) method with momentum term and adaptive step-
size rule. SIAM Journal on Optimization, 8(2):506-531, 1998.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. arXiv preprint arXiv:1810.07288, 2018.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Ad-
vances in Neural Information Processing Systems, pp. 3732-3745, 2019.
11
Under review as a conference paper at ICLR 2021
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv
preprint arXiv:1804.03209, 2018.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Yin Zhang, Richard Tapia, and Leticia Velazquez. On convergence of minimization methods: attrac-
tion, repulsion, and selection. Journal of Optimization Theory and Applications, 107(3):529-546,
2000.
A Appendix
A.1 Proofs for Section 4
ProofofLemma 1. For i = 1,... ,N ,by LiPschitz continuity of Hi at x*, for any y ∈ N (x*, e),
kHi(y) - Hik ≤ Lky — χ*k < Lg
and therefore kHi(y)k ≤ kHi(y) — Hik + IlHik < Le + ∣∣H*k. For fixed X ∈ N(xi, e), define
gi(t) = Vfi(Xi + t(X - Xi)), and therefore gi0(t) = (X - Xi)THi(Xi + t(X - Xi)). By the
fundamental theorem of calculus,
kVfi(x) -Vfi(x*)k = kgi(1) - gi(0)k
1
gi0 (t) dt
≤Z 1 kgi0(t)k dt = Z 1 k(X — Xi)THi(Xi + t(X — Xi))k
≤ Z kX — Xi kkHi(Xi + t(X — Xi))k dt
0
<	Z 1 kX — Xik(Le + kHiik)dt
0
<	Le2 + kHii ke.
□
Proof of Theorem 1. Being a point of attraction, the sequence {Xk} satisfies the Cauchy criteria:
lim sup EkXm — Xn k = 0.
N→∞ m,n≥N
Then limk→∞ EkXk+1 — Xkk = 0. Substituting Xk+1 with the iteration (2),
0 = lim Eα
k→∞
1m
m X vfξj)(χk)
j=1
α lim E E
k→∞
1m
m X vfξj)(χk)
j=1
xk
m
α lim E
k→∞
X ɪ
Nm
'Lm∈{1,…,N }m
1m
• m X vf'j (Xk).
Since k m Pm=I vf` (Xk )k ≥ 0 for G7n ∈ {1,...,N }m
j=1
and all iterates k,
lim E
k→∞
1m
m X vf'j(Xk)
j=1
0, for any 'i：m ∈{1, ...,N }m.
12
Under review as a conference paper at ICLR 2021
In particular, when '1 = •…='m = i for fixed i = 1,..., N,
lim EkVfi(Xk)k =0.	(9)
k→∞
For fixed i = 1, . . . , N, the hyphothesis of the bounded convergence theorem (Durrett, 2019, Theo-
rem 1.5.3) is satisfied:
•	IlVfi (x)k ≤ Le2 + ∣∣H*k∈ for any X ∈ N (x*, e), following the result in Lemma (1).
•	kVfi(Xk)k → ∣∣Vfi(x*)k: Xk -→ x* implies IlVfi(Xk)k → ∣∣V∕i(x*)k. Bycompari-
Sontheoremforconvergence, ∣Vfi(Xk)∣ → ∣∣Vf(x*)∣.
Applying the bounded convergence theorem on (9) over the region N(X*, e) gives
E lim ∣Vfi(Xk)∣ =0 =⇒ E∣Vfi(X*)∣ = ∣Vfi(X*)∣ =0, fori= 1,...,N,
k→∞
which implies the desired result.	□
Proof of Lemma 2. By the interpolation property and definition of ei(X) for i = 1, . . . , N,
ei(X)= -Hi ∙ (x - x*) + Vfi(X)
=-Hi ∙ (x — Xi) + Vfi(Xi + (x — Xi))
=-Hi	∙	(x	— xi) + [Vfi(xi)	+ Hi(Xi	+ t(x	— xi)) ∙	(x	— xi)]	for some t ∈	[0,1]
(10a)
=[Hi(xi + t(x - xi)) - Hi] ∙ (x - xi),	(10b)
where (10a) is by the Taylor expansion on Vfi(X), and (10b) is because Vfi(Xi) = 0. By Cauchy-
schwarz inequality,
∣ei(X)∣ ≤ ∣Hi(Xi + t(X - Xi)) - Hii∣∣X - Xi∣
≤ L∣t(X - Xi)∣∣X - Xi∣	(10c)
≤ LkX - χik2,
where (10c) is by Lipschitz continuity of Hi at xi.	□
Proofof Theorem 2. Since the step size α ≤ mini∈{i,…,n} λ~~1(HT), the term
0 I - αHii	I, for i = 1, . . . , N.
Substituting (4) into the term ∣Xk+1 - Xi ∣2 gives
∣Xk+1 - Xi∣2
1 m
=	~〉： h(I - ɑHξi(j) ) (Xk - Xi) - αeξ(j) (xk )i
m j=1	k	k
≤ max	(Xk - Xi)T(I - αH i(j))2(Xk - Xi)
j∈{1,...,m}	ξk
2
+ α2∣e (j) (Xk)∣2 - 2α(Xk - Xi)T(I - αH i(j))e (j)(Xk)
ξk	ξk	ξk
≤ max	(Xk - Xi)T(I - αH i(j))2(Xk - Xi)
j∈{1,...,m}	ξk
+ α2 ∣eξ(j) (Xk)∣2 + 2α∣Xk - Xi ∣∣I - αH i(j) ∣∣eξ(j) (Xk)∣
ξk	ξk	ξk
≤ max	(Xk - Xi)T(I - αH i(j))2(Xk - Xi)
j∈{1,...,m}	ξk
+ α2L2 ∣Xk - Xi ∣4 + 2αL∣I - αHξi(k) ∣∣Xk - Xi ∣3
= max	(Xk - Xi )T
j∈{1,...,m}
(11a)
(11b)
(11c)
(I - αHξi(j) )2 + α2L2 ∣Xk - Xi ∣2 + 2αL∣Xk - Xi ∣∣I - αHξi(j) ∣ (Xk - Xi )
≤ max (Xk - Xi)T (I - αH i(j) )2 + α2L2e2 + 2αLe (Xk - Xi)	(11d)
j∈{1,...,m}	ξk
13
Under review as a conference paper at ICLR 2021
where (11a) follows from the inequality ∣∣ D Pd=I ®||2 ≤ max^∈{i,…⑷ ∣∣α∕∣2; (11b) follows from
the Cauchy-Schwarz inequality on the last term; (11c) follows from Lemma (2); (11d) is because
that the sequence {xk} ∈ N(x*, e), and that 11 - aH*(fc)∣ ≤ 1.
Therefore, taking conditional expectation both sides given Xk implies
E[∣Xk+ι - x*l∣2 I Xk] ≤ max	(Xk - x*)T
j∈{i,...,m}
^Eξ(j) [(I — aHξ(k) )2] + a2L2c2 + 2aLc)	(12)
(xk - x*)
The expectation term E~){(I - aH^(k))2} can be simplified as follows:
Eξj)[(I - αHN) )2]= Eg [l - 2αH；(,.)+ α2(H；(,.))2
ξk	ξk	ξk \_	& k	ξk
=I - 2αH* + α2Eξ(j) [(H£))2]
„ ι E C
=I - 2αH* + α2- £(再)2	(13)
i=1
=I - 2αH* + α2(H*)2 +α2 (NI X(HA-(H*)2)
=I - 2αH* + α2(H*)2 + α2Σ*.
Combining (12) and (13) gives an upper bound on E[∣xk+1 - x* ∣2 ∣ xk]:
E[∣xk+1 - x*∣2 ∣ xk] ≤ (xk - x* )t I- - 2αH * + α2(H *)2 + α2Σ*+ α2L2e2I+2αLe∕)(xk - x*)
Since α and E are sufficiently small such that
0 ≤ P ≡ λmax(I — 2αH* + α2(H*)2 + α2Σ* + α2L2e2I + 2αLd) < 1,
the conditional expectation is further upper bounded by
E[∣∣xk+ι - x*∣2 I Xk] ≤ PIlXk- x*∣∣2.
Therefore,
E[∣∣Xk+ι - x*∣∣2] = E {E[∣Xk+ι - X*∣∣2 | Xk]} ≤ ρE[∣Xk - x*∣∣2].	(14)
Furtheremore,
E[∣∣xk - x*∣∣2] ≤ ρE[∣∣xk-1 - x*∣∣2] ≤∙∙∙≤ ρk IlXO- x*∣∣.
By (6b),
H * * LeI,	(15)
I - 2αH* 占 0.	(16)
Since H * - LeI * 0 and by (6c),
α((H*)2 + Σ*) + αL2e2I Y 2H* - 2LeI.	(17)
Combining (16) and (17) gives
0 W I - 2αH* + α2(H*)2 + α2Σ* + α2L2e2I + 2αLeI Y I.
□
14
Under review as a conference paper at ICLR 2021
ProofofLemma 3. Let Fk denote the σ-algebra generated by the first k SGD-CS random vectors
ξ(1'm),..., ξf:m). Construct Zk = "pk-x k . Firstly we show that Zk is a supermatingale:
k r k I k i _ k k XT∧(k + l) X Il 1 k _LlRI kXT∧(k+l) X Il 1 H
ElZk+1 IFM= E [	PT∧(k+1)	1τ≤k Fk] + E [	PT∧(k + 1)	1τ>k Fk
IP IlXT ∧k - x*ll2[	下,f Γ kχk+ι - x*ll2[ τ 1
=E [-----L--------1T≤k FkJ + E [------ρk+1----1T>k Fk
=ZkIT≤k + pk + 1 EljlXk+1 - x* ∣∣21T>k | Fk]
≤ Zk 1T≤k + pk+1 PE [||Xk - x* ∣∣21T>k | Fk]
=Zk 1t ≤k + Zk 1T>k = Zk,
Ka LX*k2 r
where (i) is because that U TPk∧k~— 1τ≤k is measurable with respect to Fk;(ii) is by applying (14)
in Theorem (2). As a result,
Zo ≥ E[Zk ∣F0] ≥ E [IXT^k；；x*121k>τ F0l ≥ E [IXT 二x*k21k>τ Fo .
L	pt ^k	—」	[PT	-」
By definition of the stopping time, IlXT - x*∣∣2 ≥ e2; and the term Zo := ∣xo - x*∣2 ≤ δe2 since
∣∣x0 - x* I ≤ √δe. Substituting these two relations into the inequality above gives
δe2 ≥ E pT 1k>τ ∣ Fo =⇒ δ ≥ E ]1k>τ | Fo ≥ E[1k>τ∣Fo] ≥ P{k ≥ T}, ∀k.
Or after a rearrangement, δ ≥ P(τ < ∞), which implies P(τ = ∞) ≥ 1 — δ.
Furthermore,
E[∣Xk - χ*∣21τ=∞] = E[∣Xk - χ*∣2 ∣ τ = ∞]P(τ = ∞) ≥ (1 - δ)E[∣Xk - χ*∣2 ∣ τ = ∞].
(18)
Apply Theorem (2) to bound E[∣Xk - x*∣21τ=∞]:
E[∣Xk- χ*∣21τ=∞] ≤ Pk∣χo - χ*∣2.	(19)
We now estimate E[∣Xk - χ* ∣∣2 ∣ τ = ∞] by utilizing (18) and (19):
k
E[∣Xk - χ*∣2 ∣ τ = ∞] ≤ p-∣χo - χ*∣2.
1-δ
□
ProofofTheorem 3. Define the event E = {τ = ∞}. It suffices to show the relation (7). By direct
calculation,
E[|Xk + 1 - χ*l21τ>k+1 ∣ Xk = χk] ≤ E[||Xk+1 - χ*∣∣21τ>k ∣ Xk = χk]
=E[∣Xk+ι- χ*∣21τ>k ∣ Xk = Xk,Fk]
=E[∣Xk+ι- χ*∣2 ∣ Xk = Xk,Fk]1τ>k
≤ P∣∣χk - χ* k2 1τ>k
where the last inequality follows from (14). As a result,
E[∣Xk+ι - χ*∣21τ>k+ι] = E {E[∣Xk+ι - χ*∣21τ>k+ι ∣ Xk]}
≤ PE[∣Xk - χ*∣21τ>k]
Inductively, E[∣Xk - χ*∣21τ>k] ≤ Pk∣∣χo - χ*∣2. Therefore,
E[∣Xk - χ*∣∣21e] = E[∣Xk - χ*∣21τ=∞] ≤ E[∣Xk - χ*∣21τ>k] ≤ PkIIXO- χ*∣2,
which completes the proof.	□
ProofofRemark 2. Conditioned on the event {τ = ∞}, we have
P(IlXk - χ*∣∣2 > e2 ∣ τ = ∞) = 0.
It follows that
P(IlXk - X*I2 ≤ e2) ≥ P(IlXk - X*∣∣2 ≤ e2 ∣ τ = ∞)P(τ = ∞) ≥ (1 - δ).
□
15
Under review as a conference paper at ICLR 2021
A.2 Examples
Example 1.	(Quadratic Functions) Consider minimizing for the objective function f (x) =
PiN=1 xi2 ≡ xTx, where x ∈ RN×1, and fi = xi2, i ∈ [N]. Then the gradient of each compo-
nent fi is given by
Vfi(x) = 2Ei ∙ x,
where Ei ∈ RN×N is a all-zero matrix except Ei,i = 1. Then the end-to-end N runs of IGD update
with step size t can be expressed as a matrix compact form:
xnew = KN(t)xold,
where
KN (t) = Y I - 2tEi = diag(1 - 2t, 1 - 2t, . . . , 1 - 2t).	(20)
i=1
Applying the basic knowledge in linear algegra, ρ(KN (t)) = |1 - 2t|. As long as t ∈ (0, 1), the
IGD will converge from any initial point.
Example 2.	(Standard Least Squares Problem) Consider the standard un-determined least squares
problem
f(x) = 1 ∣∣Ax - bk2, A ∈ Rm×N,N<n.
Similarly, the end-to-end N runs of IGD update with step size t forms the linear system
xnew = KN(t)xold+tc(t),
with
K0(t) ≡ I,	Kj (t) = Y I - taiaiT , j = 1, 2, . . . , N,
i=1
and c(t) = PjN=1 bjKj-1(t)aj. The sufficient condition for convergence is λmax (KN (t)) < 1, and
the necessary condition is λmax (KN (t)) ≤ 1. In this situation, we can assert that λmax (KN (t)) ≥
1: we can pick x0 ∈ Rn \ {0} so that Ax0 = 0, which implies (KN (t))x0 = x0. This means that 1
is an eigenvalue of KN (t), i.e., λmax(KN (t)) ≥ 1. This example also suggests that regularization
helps with the convergence in optimization.
Example 3.	(Regularized Least Squares Problem) Consider the regularized least squares objective
function
f(x) = 2 ∣Ax-bk2 + 2 kD1/2xk2.
It can be shown that the end-to-end N iteration of IGD with step size t can be expressed as a compact
matrix form:
xnew = KN (t)xold + tc(t),	(21a)
Ko(t) ≡ I,	Kj (t) = YY (l — taiaT - t 上 0),	j = 1,...,N,	(21b)
i=1	m
m
c(t) = XbjKj-1(t)aj.	(21c)
The necessary and sufficient conditions for the convergence of IGD suffice to characterize the con-
dition ρ(t) ≡ ρ(KN (t)) < 1. We need to make use of the famous conjecture about matrix AM-GM
inequality:
For any positive semi-definite matrix P1, P2, . . . , Pn ∈ Rn×n,
⅛ X	Pσι Pσ2 …Pσn W ^^ X ”
σ=(σ1,...,σn)∈Γ	i
16
Under review as a conference paper at ICLR 2021
We make a reasonable assumption that the products within KN (t) are commutative, then applying
this conjecture gives
KN (t)
Hence, a sufficient condition for the convergence would be:
ρ (]I - N [μD + ATA]
N <1.
Or equivalently, we need to pick t such that, for any eigenvalue λ ofthe matrix μD + ATA, we have
tλ N	2N
(1 - N) < O t ∈ (0,ρ(μD + ATA) J
Example 4. (General Nonlinear Least Squares Problem) Consider the general nonlinear least
squares problem:
1N	1
f (X) = 2 Eri(X) = 2 R(X)T R(X).
i=1
By assuming that ri(X) are twice continuously differentiable for all
iteration of IGD with step size t can be safely approximated as
Xnew = KN (t)Xold + tc(t) + o(kX0
Ko(t) ≡ I, Kj(t) = YY(I - tHi(x*)),	j =
i=1
m
c(t) = X Kj-ι(t)Hj (x*)x*.
j=1
i ∈ [N], the end-to-end N
-χ*k2),
1,...,N,
(22a)
(22b)
(22c)
It is reasonable to assume that Hi (x*) isfull rankfor all i, since adding small regularization terms
can resolve the rank deficiency issue. Applying the conjecture again, we imply that
N
KN(t) W I - N X Hi(x*)
i
Therefore, a sufficient condition for the convergence of IGD would be
t ∈ S λmaχ(E[Hi(x*)]) ), where E[Hi B )] ≡ N X Hi(X* ).
A.3 Supplement to Exeperiements
A.3.1 image classification
We normalize data and then augment them by horizontal flips and random crops from the image
padded by 4 pixels, filling missing pixels with reflections of the original image. We adopt Kaiming
initialization(He et al., 2015), BN (Ioffe & Szegedy, 2015) but without dropout. The models are
trained for up to 150 epochs and minibatch size to 256. We select the best learning rate from Table.1
in Appendix.A.3.1 for SGD(M), and for SGDL, the learning rate is more than 10 times larger than the
best SGDM lr, in our paper, we select it from {1.0, 1.1, 1.2, 1.3}, more specific parameter settings
are as follows. Additional, for all optimizers without AdaDelta, we use an annealing strategy to
divide learning rate by 10 on 50 and 100 epochs.
The figures show results for ResNet110,VGG,DenseNet on CIFAR10 and CIFAR100. For each
figure, the specific learning rate is given for SGDL. On CIFAR10, SGDL learning rate is 1.0 for
ResNet 56, 1.1 for DenseNet and 1.2 for ResNet110 and VGG16, while on CIFAR100, learning
rate is 1.0 for VGG16 and 1.3 for ResNet 56, ResNet110 and DenseNet on SGDL. For SGDM, the
learning rate is 0.1 for most experiments, while it is 0.05 for VGG16 on CIFAR10. Finally, 0.001 is
the most suitable learning rate for ADAM. On ImageNet, we use 1.1, 0.1, 0.001 as learning rate for
SGDL, SGDM, and ADAM.
17
Under review as a conference paper at ICLR 2021
Table 1: Popular optimizers in the paper on CIFAR10, CIFAR100, ImageNet32 and Speech Com-
mands Dataset. η denotes momentum, γ is the weight decay coefficient,β is from Kingma & Ba
(2014) and Pis fromZeiler(2012).______________________________________________
Optimizer	Learning rate	Other Parameter Tuning
Adam	{0.0001, 0.001, 0.01}	β = (0.9, 0.999)
AdaDelta	1.0	P = 0.9
SGD	{0.001, 0.01, 0.05, 0.1}	γ = 0.005
SGDM	{0.001, 0.01, 0.05, 0.1}	η=0.9,γ=0.005
SGDL	≥ 10 * Ir(SGDM)	γ = 0.005
A.3.2 Speech Commands Dataset
On Speech Commands Dataset, Similarly, the learning rate is 0.001 for Adam and 1.0 for AdaDelta
as above. We choose 0.01 from {0.001, 0.01, 0.05, 0.1} as the best for SGDM and 0.15 for SGDL
15 times larger than SGDM.
A.3.3 details on LSTM
For LSTMs, there are 1150 units in the hidden layer, an embedding of 400 and a batch size of 20.
We select the best learning rate 1 from {0.001, 0.01, 0.1, 1} for SGD(M) and the learning rate is
same as above sections for Adam and Adadelta. For SGDL, we use 30 as an initial learning rate
from the author’s advise(Merity et al., 2017a) on {2, 3}-Layer LSTM for 150 epochs with the same
annealing mechanism similarly as above. To train all models, we carry out gradient clipping with
maximum norm 0.25. More specifically, for those dropout values, we use (0.4, 0.3, 0.4, 0.1, 0.5)
on word vector, output between LSTM layers, output of final LSTM layer, embedded dropout and
DropConnect respectively.
O
2 8 4 0
求」。
50	100
Epoch
(a) Train Error for Resnet56.
O 8
求」。」由θl
50	100
Epoch
(b) Test Error for Resnet56.
150	0
150
Figure 8: Resnet56 on CIFAR-10.
18
Under review as a conference paper at ICLR 2021
0 6 2 8 4
2 11
求」。击Ul
Adam
AdaDeIta
SGD
SGDM
SGDL
4
2 O
求山 IS-
150
0
0	50	100
Epoch
(a) Train Error for Resnet110.
6
150	0	50	100
Epoch
(b) Test Error for Resnet110.
Figure 9: Resnet110 on CIFAR-10.
8
4 -
0 -
0

50	100
Epoch
150
6 L
0
(a) Train Error for VGG16.
50	100	150
Epoch
(b) TestErrorforVGG16.
Figure 10: VGG16 on CIFAR-10.
%」。」由u2l
4 2 0 8
求」。」由∞θl
---Adam
---AdaDeIta
---SGD
---SGDM
---SGDL
0	50	100	150
Epoch
(a) Train Error for DenseNet.
0	50	100	150
Epoch
(b) Test Error for DenseNet.
Figure 11:	DenseNet on CIFAR10.
υ 8 6 4 2 o
4 3 3 3 3 3
求山 ISəl
---Adam
---AdaDeIta
---SGD
---SGDM
---SGDL
0 6 2 8 4
2 11
求」。击Ul
0	50	100	150
Epoch
(a)	Train Error for Resnet56.
0	50	100	150
Epoch
(b)	Test Error for Resnet56.
Figure 12:	Resnet56 on CIFAR100.
19
Under review as a conference paper at ICLR 2021
2 8 4
求
---Adam
---AdaDeIta
---SGD
---SGDM
---SGDL
0	50	100	150
Epoch
(a)	Train Error for Resnet110.
0	50	100	150
Epoch
(b)	Test Error for Resnet110.
Figure 13:	Resnet110 on CIFAR100.
O
8 4 0
求」。击U1
6 4 2 0
3 3 3 3
。一。山 ISəl
(b) Test Error for VGG16.
Figure 14: VGG16 on CIFAR100.
26
50	100	150	'
Epoch
(a) Train Error for VGG16.
2 8 4
求
6
2
%」。」」「lsθl
---Adam
---AdaDeIta
SGD
---SGDM
SGDL
0	50	100	150
Epoch
(a)	Train Error for DenseNet.
0	50	100	150
Epoch
(b)	Test Error for DenseNet.
Figure 15: DenseNet on CIFAR100.
20