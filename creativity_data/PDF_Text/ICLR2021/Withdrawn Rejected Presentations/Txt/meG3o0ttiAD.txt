Under review as a conference paper at ICLR 2021
Toward Trainability of Quantum Neural Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
Quantum Neural Networks (QNNs) have been recently proposed as generaliza-
tions of classical neural networks to achieve the quantum speed-up. Despite the
potential to outperform classical models, serious bottlenecks exist for training
QNNs; namely, QNNs with random structures have poor trainability due to the
vanishing gradient with rate exponential to the input qubit number. The vanish-
ing gradient could seriously influence the applications of large-size QNNs. In this
work, we provide a first viable solution with theoretical guarantees. Specifically,
we prove that QNNs with tree tensor and step controlled architectures have gra-
dients that vanish at most polynomially with the qubit number. Moreover, our
result holds irrespective of which encoding methods are employed. We numer-
ically demonstrate QNNs with tree tensor and step controlled structures for the
application of binary classification. Simulations show faster convergent rates and
better accuracy compared to QNNs with random structures.
1 Introduction
Neural Networks (Hecht-Nielsen, 1992) using gradient-based optimizations have dramatically ad-
vanced researches in discriminative models, generative models, and reinforcement learning. To effi-
ciently utilize the parameters and practically improve the trainability, neural networks with specific
architectures (LeCun et al., 2015) are introduced for different tasks, including convolutional neural
networks (Krizhevsky et al., 2012) for image tasks, recurrent neural networks (Zaremba et al., 2014)
for the time series analysis, and graph neural networks (Scarselli et al., 2008) for tasks related to
graph-structured data. Recently, the neural architecture search (Elsken et al., 2019) is proposed to
improve the performance of the networks by optimizing the neural structures.
Despite the success in many fields, the development of the neural network algorithms could be
limited by the large computation resources required for the model training. In recent years, quantum
computing has emerged as one solution to this problem, and has evolved into a new interdisciplinary
field known as the quantum machine learning (QML) (Biamonte et al., 2017; HavlIcek et al., 2019).
Specifically, variational quantum circuits (Benedetti et al., 2019) have been explored as efficient
protocols for quantum chemistry (Kandala et al., 2017) and combinatorial optimizations (Zhou et al.,
2018). Compared to the classical circuit models, quantum circuits have shown greater expressive
power (Du et al., 2020a), and demonstrated quantum advantage for the low-depth case (Bravyi et al.,
2018). Due to the robustness against noises, variational quantum circuits have attracted significant
interest for the hope to achieve the quantum supremacy on near-term quantum computers (Arute
et al., 2019).
Quantum Neural Networks (QNNs) (Farhi & Neven, 2018; Schuld et al., 2020; Beer et al., 2020)
are the special kind of quantum-classical hybrid algorithms that run on trainable quantum circuits.
Recently, small-scale QNNs have been implemented on real quantum computers (HavliCek et al.,
2019) for supervised learning tasks. The training of QNNs aims to minimize the objective function
f with respect to parameters θ. Inspired by the classical optimizations of neural networks, a natural
strategy to train QNNs is to exploit the gradient of the loss function (Crooks, 2019). However, the
recent work (McClean et al., 2018) shows that n-qubit quantum circuits with random structures and
large depth L = O(poly(n)) tend to be approximately unitary 2-design (Harrow & Low, 2009),
and the partial derivative vanishes to zero exponentially with respect to n. The vanishing gradient
problem is usually referred to as the Barren Plateaus (McClean et al., 2018), and could affect the
1
Under review as a conference paper at ICLR 2021
trainability of QNNs in two folds. Firstly, simply using the gradient-based method like Stochastic
Gradient Descent (SGD) to train the QNN takes a large number of iterations. Secondly, the esti-
mation of the derivatives needs an extremely large number of samples from the quantum output to
guarantee a relatively accurate update direction (Chen et al., 2018). To avoid the Barren Plateaus
phenomenon, we explore QNNs with special structures to gain fruitful results.
In this work, we introduce QNNs with special architectures, including the tree tensor (TT) structure
(Huggins et al., 2019) referred to as TT-QNNs and the setp controlled structure referred to as SC-
QNNs. We prove that for TT-QNNs and SC-QNNs, the expectation of the gradient norm of the
objective function is bounded.
Theorem 1.1. (Informal) Consider the n-qubit TT-QNN and the n-qubit SC-QNN defined in Fig-
ure 1-2 and corresponding objective functions fTT and fSC defined in (3-4), then we have:
1 + log n
2n
1 + nc
, α(Pin) ≤ Eθ∣∣VθfTTk2 ≤ 2n - 1,
21+nc
• α(ρin) ≤ Eθ∣∣Vθfsc∣2 ≤ 2n — 1,
where nc is the number of CNOT operations that directly link to the first qubit channel in the sC-
QNN, the expectation is taken for all parameters in θ with uniform distributions in [0, 2π], and
α(ρin) ≥ 0 is a constant that only depends on the input state ρin ∈ C2n×2n. Moreover, by preparing
ρin using the L-layer encoding circuit in Figure 4, the expectation of α(ρin) could be further lower
bounded as Eα(ρin) ≥ 2-2L.
Compared to random QNNs with 2-O(poly(n)) derivatives, the gradient norm of TT-QNNs ad SC-
QNNs is greater than Ω(1∕n) or Ω(2-nc) that could lead to better trainability. Our contributions are
summarized as follows:
•	We prove Ω(1∕n) and Ω(2 nc) lower bounds on the expectation of the gradient norm of
TT-QNNs and SC-QNNs, respectively, that guarantees the trainability on related optimiza-
tion problems. Our theorem does not require the unitary 2-design assumption in existing
works and is more realistic to near-term quantum computers.
•	We prove that by employing the encoding circuit in Figure 4 to prepare ρin, the expectation
of term α(ρin) is lower bounded by a constant 2-2L. Thus, we further lower bounded the
expectation of the gradient norm to the term independent from the input state.
•	We simulate the performance of TT-QNNs, SC-QNNs, and random structure QNNs on
the binary classification task. All results verify proposed theorems. Both TT-QNNs and
SC-QNNs show better trainability and accuracy than random QNNs.
Our proof strategy could be adopted for analyzing QNNs with other architectures as future works.
With the proven assurance on the trainability of TT-QNNs and SC-QNNs, we eliminate one bottle-
neck in front of the application of large-size Quantum Neural Networks.
The rest parts of this paper are organized as follows. We address the preliminary including the
definitions, the basic quantum computing knowledge and related works in Section 2. The QNNs
with special structures and the corresponding results are presented in Section 3. We implement
the binary classification using QNNs with the results shown in Section 4. We make conclusions in
Section 5.
2 Preliminary
2.1 Notations and the Basic Quantum Computing
We use [N] to denote the set {1, 2, • • • , N}. The form k • k denotes the k • k2 norm for vectors. We
denote aj as the j-th component of the vector a. The tensor product operation is denoted as “0”.
The conjugate transpose of a matrix A is denoted as AL The trace of a matrix A is denoted as Tr[A].
We denote Vθ f as the gradient of the function f with respect to the vector θ. We employ notations
O and O to describe the standard complexity and the complexity ignoring minor terms, respectively.
2
Under review as a conference paper at ICLR 2021
Now We introduce the quantum computing. The pure state of a qubit could be written as ∣φi =
a|0i +b|1i, where a, b ∈ C satisfies |a|2 + |b|2 = 1, and {|0i = (1, 0)T, |1i = (0, 1)T}, respectively.
The n-qubit space is formed by the tensor product ofn single-qubit spaces. For the vector x ∈ R2n,
the amplitude encoded state |x〉is defined as 亳 Pj2= 1 Xj ji∙ The dense matrix is defined as
P = |x)(x| for the pure state, in whichhx| = (|x))L A single-qubit operation to the state behaves
like the matrix-vector multiplication and can be referred to as the gate -EZH in the quantum circuit
language. Specifically, single-qubit operations are often used including RX (θ) = e-iθX, RY (θ) =
e-iθY , and RZ(θ) = e-iθZ:
X= 01 10,Y= 0i -0i,Z= 01 -01.
Pauli matrices {I, X, Y, Z} will be referred to as {σ0, σ1, σ2, σ3} for the convenience. Moreover,
two-qubit operations, the CNOT gate and the CZ gate, are employed for generating quantum entan-
glement:
CNOT = ɪ
∣0ihO∣0 σo + ∣1ih1∣0 σι, CZ =十=|0〉{0|公 σ0 + |1〉〈1|X σ3.
We could obtain information from the quantum system by performing measurements, for example,
measuring the state ∣φi = a|0)+ b|1〉generates 0 and 1 with probability p(0) = |a|2 andp(1) = |b|2,
respectively. Such a measurement operation could be mathematically referred to as calculating the
average of the observable O = σ3 under the state ∣φ):
hσ3i∣φi ≡ hφ∣σ3∣φi ≡ Tr[∣φihφ∣ ∙ σ3] = |a|2 - |b|2 = p(0) -p(1) = 2p(0) - 1.
The average of a unitary observable under arbitrary states is bounded by [-1, 1].
2.2 Related Works
The barren plateaus phenomenon in QNNs is first noticed by McClean et al. (2018). They prove that
for n-qubit random quantum circuits with depth L = O(poly(n)), the expectation of the derivative
to the objective function is zero, and the variance of the derivative vanishes to zero with rate expo-
nential in the number of qubits n. Later, Cerezo et al. (2020) prove that for L-depth quantum circuits
consisting of 2-design gates, the gradient with local observables vanishes with the rate O(2-O(L)).
The result implies that in the low-depth L = O (log n) case, the vanishing rate could be O( pθyn),
which is better than previous exponential results. Recently, some techniques have been proposed to
address the barren plateaus problem, including the special initialization strategy (Grant et al., 2019)
and the layerwise training method (Skolik et al., 2020). We remark that these techniques rely on
the assumption of low-depth quantum circuits. Specifically, Grant et al. (2019) initialize parameters
such that the initial quantum circuit is equivalent to an identity matrix (L = 0). Skolik et al. (2020)
train parameters in subsets in each layer, so that a low-depth circuit is optimized during the training
of each subset of parameters.
Since random quantum circuits tend to be approximately unitary 2-design1 as the circuit depth in-
creases (Harrow & Low, 2009), and 2-design circuits lead to exponentially vanishing gradients (Mc-
Clean et al., 2018), the natural idea is to consider circuits with special structures. On the other hand,
tensor networks with hierarchical structures have been shown an inherent relationship with classical
neural networks (Liu et al., 2019; Hayashi et al., 2019). Recently, quantum classifiers using hierar-
chical structure QNNs have been explored (Grant et al., 2018), including the tree tensor network and
the multi-scale entanglement renormalization ansatz. Besides, QNNs with dissipative layers have
shown the ability to avoid the barren plateaus (Beer et al., 2020). However, theoretical analysis of
the trainability of QNNs with certain layer structures has been little explored (Sharma et al., 2020).
Also, the 2-design assumption in the existing theoretical analysis (McClean et al., 2018; Cerezo
et al., 2020; Sharma et al., 2020) is hard to implement exactly on near-term quantum devices.
3	Quantum Neural Networks
In this section, we discuss the quantum neural networks in detail. Specifically, the optimizing of
QNNs is presented in Section 3.1. We analyze QNNs with special structures in Section 3.2. We
1We refer the readers to Appendix B for a short discussion about the unitary 2-design.
3
Under review as a conference paper at ICLR 2021
introduce an approximate quantum input model in Section 3.3 which helps for deriving further
theoretical bounds.
3.1	The Optimizing of Quantum Neural Networks
In this subsection, we introduce the gradient-based strategy for optimizing QNNs. Like the weight
matrix in classical neural networks, the QNN involves a parameterized quantum circuit that math-
ematically equals to a parameterized unitary matrix V (θ). The training of QNNs aims to optimize
the function f defined as:
f (θ; Pin) = 2 + 2Tr [O ∙ V(θ) ∙ Pin ∙ V(θ)t] = 2 + 2 hOi V(θ),Pin,	⑴
where O denotes the quantum observable and ρin denotes the density matrix of the input quan-
tum state. Generally, we could deploy the parameters θ in a tunable quantum circuit arbi-
trarily. A practical tactic is to encode parameters {θj } as the phases of the single-qubit gates
{e-iθjσk , k ∈ {1, 2, 3}} while employing two-qubit gates {CNOT, CZ} among them to generate
quantum entanglement. This strategy has been frequently used in existing quantum circuit algo-
rithms (Schuld et al., 2020; Benedetti et al., 2019; Du et al., 2020b) since the model suits the noisy
near-term quantum computers. Under the single-qubit phase encoding case, the partial derivative of
the function f could be calculated using the parameter shifting rule (Crooks, 2019),
∂θ^ = 9hOiV (θ+),Pin - XhOiV (θ-),Pin = f (θ+; ρin) - f (θ-; Pin),	⑵
∂θj	2	2
where θ+ and θ- are different from θ only at the j-th parameter: θj → θj ± ∏. Thus, the gradi-
ent of f could be obtained by estimating quantum observables, which allows employing quantum
computers for fast optimizations using stochastic gradient descents.
3.2	Quantum Neural Networks with Special Structures
In this subsection, we introduce quantum neural networks with tree tensor (TT) (Grant et al., 2018)
and step controlled (SC) architectures. We prove that the expectation of the square of gradient `2 -
norm for the TT-QNN and the SC-QNN are lower bounded by Ω(1∕n) and Ω(2 nc), respectively,
where nc is a parameter in the SC-QNN that is independent from the qubit number n. Moreover, the
corresponding theoretical analysis does not rely on 2-design assumptions for quantum circuits.
Now we discuss proposed quantum neural networks in detail. We consider the n-qubit QNN con-
structed by the single-qubit gate W(k) = e-iθj )σ2 and the CNOT gate σι 0 |1ih1| + σ0 X|0i〈0|.
We define the k-th parameter in the j-th layer as θj(k). We only employ RY rotations for single-
qubit gates, due to the fact that the real world data lie in the real space, while applying RX and RZ
rotations would introduce imaginary term to the quantum state.
We demonstrate the TT-QNN in Figure 1 for the n = 4 case, which employs CNOT gates in the
binary tree form to achieve the quantum entanglement. The circuit of the SC-QNN could be divided
into two parts: in the first part, CNOT operations are performed between adjacent qubit channels; in
the second part, CNOT operations are performed between different qubit channels and the first qubit
channel. An illustration of the SC-QNN is shown in Figure 2 for the n = 4 and nc = 2 case, where
nc denotes the number of CNOT operations that directly link to the first qubit channel. The number
of parameters in both the TT-QNN and the SC-QNN are 2n - 1. We consider correspond objective
functions that are defined as
frr(θ) = 2 + 2Tr[σ3 0 I须nT)VTT(θ)ρinV⅛(θ)*],	(3)
fSC(θ) = 2 + 1 Tr[σ3 010(n-1)VSC(θ)PinVSC(θ)t],	(4)
where VTT(θ) and VSC(θ) denotes the parameterized quantum circuit operations for the TT-QNN
and the SC-QNN, respectively. We employ the observable σ3 0 I须nT) in Eq. (3-4) such that
objective functions could be easily estimated by measuring the first qubit in corresponding quantum
circuits.
4
Under review as a conference paper at ICLR 2021
Main results of this section are stated in Theorem 3.1, in which We prove Ω(1∕n) and Ω(2 nc)
lower bounds on the expectation of the square of the gradient norm for the TT-QNN and the SC-
QNN, respectively. By setting nc = O(log n), we could obtain the linear inverse bound for the
SC-QNN as well. We provide the proof of Theorem 3.1 in Appendix D and Appendix G.
Figure 1: Quantum Neural Network with Figure 2: Quantum Neural Network with the Step
the Tree Tensor structure (n = 4).	Controlled structure (n = 4, nc = 2).
Theorem 3.1. Consider the n-qubit TT-QNN and the n-qubit SC-QNN defined in Figure 1-2 and
corresponding objective functions fTT and fSC defined in Eq. (3-4), then we have:
∖ 旦∙ α(ρin) ≤ Eθ∣∣Vθfττ∣∣2 ≤ 2n — 1,
2n
1+n
l1++nc ∙ α(ρin) ≤ Eθ∣VθfSCk2 ≤ 2n — 1,
21+nc
(5)
(6)
where nc is the number of CNOτ operations that directly link to the first qubit channel in the
SC-QNN, the expectation is taken for all parameters in θ with uniform distributions in [0, 2π],
Pin ∈ C2n×2n denotes the input state, α(ρin) = Tr [σq,o,…，0)∙ ρin∖ + Tr 卜⑶。,…，0)∙ Pin∖ , and
σ(il,i2,…，in ) ≡ σil % σi2 %…脸 σin .
From the geographic view, the value Eθ ∣Vθf ∣2 characterizes the global steepness of the function
surface in the parameter space. Optimizing the objective function f using gradient-based methods
could be hard if the norm of the gradient vanishes to zero. Thus, lower bounds in Eq. (5-6) provide
a theoretical guarantee on optimizing corresponding functions, which then ensures the trainability
of QNNs on related machine learning tasks.
From the technical view, we provide a new theoretical framework during proving Eq. (5-6). Different
from existing works (McClean et al., 2018; Grant et al., 2019; Cerezo et al., 2020) that define the
expectation as the average of the finite unitary 2-design group, we consider the uniform distribution
in which each parameter in θ varies continuously in [0, 2π]. Our assumption suits the quantum
circuits that encode the parameters in the phase of single-qubit rotations. Moreover, the result in
Eq. (6) gives the first proven guarantee on the trainability of QNN with linear depth. Our framework
could be extensively employed for analyzing QNNs with other different structures as future works.
3.3	Prepare the Quantum Input Model: a Variational Circuit Approach
State preparation is an essential part of most quantum algorithms, which encodes the classical in-
2n
formation into quantum states. Specifically, the amplitude encoding |x)= Ei=I Xi∕∣x∣∣ii allows
storing the 2n-dimensional vector in n qubits. Due to the dense encoding nature and the similarity
to the original vector, the amplitude encoding is preferred as the state preparation by many QML
algorithms (Harrow et al., 2009; Rebentrost et al., 2014; Kerenidis et al., 2019). Despite the wide
application in quantum algorithms, efficient amplitude encoding remains little explored. Existing
work (Park et al., 2019) could prepare the amplitude encoding state in time O(2n) using a quantum
circuit with O(2n) depth, which is prohibitive for large-size data on near-term quantum computers.
In fact, arbitrary quantum amplitude encoding with polynomial gate complexity remains an open
problem.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Quantum Input Model
Require: The input vector xin ∈ R2n, the number of alternating layers L, the iteration time T, and
the learning rate {η(t)}tT=-01.
Ensure: A parameter vector β* which tunes the approximate encoding circuit U(β*).
1:	Initialize {βj(k)}jn,,k2=L1+1 randomly in [0, 2π]. Denote the parameter vector as β(0).
2:	for t ∈ {0,1, ∙∙∙ ,T - 1} do
3:	RUn the circuit in Figure 3 classically to calculate the gradient Vβfi∏p∏t∣β=β(t) using the
parameter shifting rule (2), where the function finput is defined in (7).
4:	Update the parameter β(t+1) = β⑴-η(t) ∙ Vβfinput∣β=β").
5:	end for
6:	Output the trained parameter β*.
Alternating Layer
Figure 3: The parameterized alternating layered circuit
W(β) (n = 8, L = 1) for training the corresponding
encoding circuit of the input xin.
Figure 4: The encoding circuit U(β*),
where W(β*) is the trained parameter-
ized circuit in Figure 3.
In this subsection, we introduce a quantum input model for approximately encoding the arbitrary
vector xin ∈ R2n in the amplitude of the quantum state |xini. The main idea is to classically train an
alternating layered circuit as summarized in Algorithm 1 and Figures 3-4. Now we explain the detail
of the input model. Firstly, we randomly initialize the parameter β(0) in the circuit 3. Then, we train
the parameter to minimize the objective function defined in (7) through the gradient descent,
1n	1n 1
finput(β) = - J2hOiiw (β)∣xini = -E kχ-^2 Tr[Oi ∙ W (β) ∙ *泡∙ W (β)t],	⑺
where Oi = σj(i-1) 0 σ3 0 σf(n-i), ∀i ∈ [n], and W(β) denotes the tunable alternating layered
circuit in Figure 3. Note that although the framework is given in the quantum circuit language, we
actually calculate and update the gradient on classical computers by considering each quantum gate
operation as the matrix multiplication. The output of Algorithm 1 is the trained parameter vector β*
which tunes the unitary W(β*). The corresponding encoding circuit could then be implemented as
U(β*) = W(β*)t ∙ X0n, which is low-depth and appropriate for near-term quantum computers.
Structures of circuits W and U are illustrated in Figure 3 and 4, respectively. Suppose we could
minimize the objective function (7) to -1. Then hOii = -1, ∀i ∈ [n], which means the final output
state in Figure 3 equals to W(β*)∣xij = ∣-i0n.2 Thus the state ∣Xini could be prepared exactly
by applying the circuit U(β*) = W(β*)tX0n on the state ∣0i0n. However we could not always
optimize the loss in Eq. (7) to -1, which means the framework could only prepare the amplitude
encoding state approximately.
2We ignore the global phase on the quantum state. Based on the assumptions of this paper, all quantum
states that encode input data lie in the real space, which then limit the global phase as 1 or -1. For the former
case, nothing needs to be done; for the letter case, the global phase could be introduced by adding a single qubit
gate e-iπσ0 = -σ0 = -I on anyone among qubit channels.
6
Under review as a conference paper at ICLR 2021
Algorithm 2 QNNs for the Binary Classification Training
Require: Quantum input states {ρtirain}iS=1 for the dataset {(xtirain, yi)}iS=1, the quantum observable
O, the parameterized quantum circuit V (θ), the iteration time T , the batch size s, and learning
rates {ηθ(t)}tT=-0* 1 and {ηb(t)}tT=-01.
Ensure: The trained parameters θ* and b*.
1:	Initialize each parameter in θ(0) randomly in [0, 2π] and initialize b(0) = 0.
2:	for t ∈ {0,1, ∙∙∙ ,T - 1} do
3:	Randomly sample an index subset It ⊂ [S] with size s. Calculate the gradient
Vθ'it (θ, b)∣θ,b=θ(t),b(t) and ^b'ιt (θ, b)∣θ,b=θ(t),b(t) using the chain rule and the parameter
shifting rule (2), where the function '几(θ, b) is defined in (8).
4：	Update parameters θ(t+1)∕b(t+1) = θ⑴/b⑴一ηθ∕b(t) ∙ Vθ∕b'lt (θ, b)|e,b=e(t),b(t).
5:	end for
6:	Output trained parameters θ* = θ(T) and b* = b(T).
An interesting result is that by employing the encoding circuit in Figure 4 for constructing the input
state in Section 3.2 as Pin = U(β)(∣0)hθ∣)室nU(β)L We could bound the expectation of α(ρi∩)
defined in Theorem 3.1 by a constant that only relies on the layer L (Theorem 3.2).
Theorem 3.2. Suppose the state ρin is prepared by the L-layer encoding circuit in Figure 4, then we
have,
Eβα(ρin) ≥ 2-2L,
where β denotes all variational parameters in the encoding circuit, and the expectation is taken for
all parameters in β with uniform distributions in [0, 2π].
We provide the proof of Theorem 3.2 and more details about the input model in Appendix E. The-
orem 3.2 can be employed with Theorem 3.1 to derive Theorem 1.1, in which the lower bound for
the expectation of the gradient norm is independent from the input state.
4 Application: QNNs for the B inary Classification
4.1 QNNs: training and prediction
In this section, we show how to train QNNs for the binary classification in quantum computers. First
of all, for the training and test data denoted as {(xtirain, yi)}iS=1 and {(xtjest, yj)}jQ=1, where yi ∈
{0, 1} denotes the label, we prepare corresponding quantum input states {ρtirain}iS=1 and {ρtjest}jQ=1
using the encoding circuit presented in Section 3.3. Then, we employ Algorithm 2 to train the
parameter θ and the bia b via the stochastic gradient descent method for the given parameterized
circuit V and the quantum observable O. The parameter updating in each iteration is presented in
the Step 3-4, which aims to minimize the loss defined in (8) for each input batch It, ∀t ∈ [T]:
1	|It|
'it (θ) =囚 Xf (θ; ρirain)-yi + b)2.
(8)
Based on the chain rule and the Eq. (2), derivatives d∂θIt and d∂bt- could be decomposed into products
of objective functions f with different variables, which could be estimated efficiently by counting
quantum outputs. In practice, we calculate the value of the objective function by measuring the
output state of the QNN for several times and averaging quantum outputs. After the training iteration
we obtain the trained parameters θ* and b*. Denote the quantum circuit V * = V (θ*). We do test
for an input state ρtest by calculating the objective function f (ρtest) = ɪ + 2 Tr[O ∙ V * ρtest V *户].
We classify the input ρtest as in the class 0 if f (ρtest) + b* < 1, or the class 1 if f (ρtest) + b* > ɪ.
The time complexity of the QNN training and test could be easily derived by counting resources for
estimating all quantum observables. Denote the number of gates and parameters in the quantum cir-
cuit V(θ) as ngate and npara, respectively. Denote the number of measurements for estimating each
quantum observable as ntrain and ntest for the training and test stages, respectively. Then, the time
complexity to train QNNs is O(ngatenparantrainT), and the time complexity of the test using QNNs
7
Under review as a conference paper at ICLR 2021
I I I
training Iteration
0.15-
--,,,
sso-e
« ɑ βo mo
training Iteration
aw：
ɑæ2
I I I
SsO-l-eh
- "I I I
UUJo-'PeB
——Tn
——sβ⅛w)
——n9κlπn<HM
o a «> ɑ βo ια)
training Iteration
(a)	(b)
(c)	(d)
ioo
40	¢0	«0
training Iteration
(g)
a «0 β> M W
training Iteration
(e)
« SJ M 100
training Iteration
(f)
(h)
I I
卜
i I

Figure 5: Simulations on the MNIST binary classification between (0, 2). The training loss and
the test error during the training iteration are illustrated in Figures 5(a), 5(e) for the n=8 case, Fig-
ures 5(b), 5(f) for the n=10 case, and Figures 5(c), 5(g) for the n=12 case. The gradient norm
of objective functions and the term α(ρin) during the training are shown in Figures 5(d) and 5(h),
respectively for the n=8 case.
is O(ngatentest). We emphasize that directly comparing the time complexity of QNNs with classi-
cal NNs is unfair due to different parameter strategies. However, the complexity of QNNs indeed
shows a polylogarithmic dependence on the dimension of the input data if the number of gates and
parameters are polynomial to the number of qubits. Specifically, both the TT-QNN and the SC-QNN
equipped with the L-layer encoding circuit in this work have O(nL) gates and O(n) parameters,
which lead to the training complexity O(ntrainn2LT) and the test complexity O(ntestnL).
4.2 Numerical simulations
To analyze the practical performance of QNNs for binary classification tasks, we simulate the train-
ing and test of QNNs on the MNIST handwritten dataset. The 28 × 28 size image is sampled into
16 × 16, 32 × 32, and 64 × 64 to fit QNNs for qubit number n ∈ {8, 10, 12}. We set the parameter
in SC-QNNs as nc = 4 for all qubit number settings. Note that the based on the tree structure, the
qubit number of original TT-QNNs is limited to the power of 2. To analysis the behavior of TT-
QNNs for general qubit numbers, we modify TT-QNNs into Deformed Tree Tensor (DTT) QNNs.
The gradient norm for DTT-QNNs is lower bounded by O(1/n), which has a similar form of that
for TT-QNNs. We provide more details of DTT-QNNs in Appendix F and denote DTT-QNNs as
TT-QNNs in simulation parts (Section 4.2 and Appendix A) for the convenience.
We construct the encoding circuits in Section 3.3 with the number of alternating layers L = 1 for
400 training samples and 400 test samples in each class. The TT-QNN and the SC-QNN is compared
to the QNN with the random structure. To make a fair comparison, we set the numbers of RY and
CNOT gates in the random QNN to be the same with the TT-QNN and the SC-QNN. The objective
function of the random QNN is defined as the average of the expectation of the observable σ3 for
all qubits in the circuit. The number of the training iteration is 100, the batch size is 20, and the
decayed learning rate is adopted as {1.00, 0.75, 0.50, 0.25}. We set ntrain = 200 and ntest = 1000 as
numbers of measurements for estimating quantum observables during the training and test stages,
respectively. All experiments are simulated through the PennyLane Python package (Bergholm
et al., 2020).
Firstly, we explain our results about QNNs with different qubit numbers in Figure 5. We train TT-
QNNs, SC-QNNs, and Random-QNNs with the stochastic gradient descent method described in
Algorithm 2 for images in the class (0, 2) and the qubit number n ∈ {8, 10, 12}. The total loss
is defined as the average of the single-input loss. The training loss and the test error during the
training iteration are illustrated in Figures 5(a), 5(e) for the n=8 case, Figures 5(b), 5(f) for the
8
Under review as a conference paper at ICLR 2021
Class pair	TT-QNN test (F1-0, F1-1)	SC-QNN test (F1-0, F1-1)	Random-QNN test (F1-0, F1-1)
0-1	0.959 (0.958, 0.960)	0.979 (0.979, 0.979)	0.920 (0.915, 0.925)
0-2	0.844 (0.852, 0.834)	0.859 (0.859, 0.858)	0.738 (0.751, 0.722)
0-3	0.835 (0.810, 0.854)	0.859 (0.857, 0.860)	0.659 (0.680, 0.635)
0-4	0.938 (0.938, 0.938)	0.925 (0.925, 0.925)	0.779 (0.796, 0.759)
1-2	0.929 (0.930, 0.927)	0.965 (0.965, 0.965)	0.700 (0.728, 0.666)
1-3	0.938 (0.941, 0.934)	0.881 (0.879, 0.884)	0.765 (0.773, 0.756)
1-4	0.821 (0.837, 0.802)	0.871 (0.879, 0.862)	0.705 (0.737, 0.664)
2-3	0.814 (0.791, 0.832)	0.820 (0.810, 0.829)	0.709 (0.743, 0.664)
2-4	0.931 (0.928, 0.934)	0.935 (0.933, 0.937)	0.645 (0.728, 0.487)
3-4	0.931 (0.930, 0.933)	0.944 (0.943, 0.945)	0.819 (0.839, 0.793)
Table 1: The test accuracy and F1-scores for different class pairs (qubit number = 10).
n=10 case, and Figures 5(c), 5(g) for the n=12 case. The test error of the TT-QNN, the SC-QNN
and the Random-QNN converge to around 0.2 for the n=8 case. As the qubit number increases,
the converged test error of both TT-QNNs and SC-QNNs remains lower than 0.2, while that of
Random-QNNs increases to 0.26 and 0.50 for n=10 and n=12 case, respectively. The training loss
of both TT-QNNs and SC-QNNs converges to around 0.15 for all qubit number settings, while
that of Random-QNNs remains higher than 0.22. Both the training loss and the test error results
show that TT-QNNs and SC-QNNs have better trainability and accuracy on the binary classification
compared with Random-QNNs. We record the l2-norm of the gradient during the training for the
n=8 case in Figure 5(d). The gradient norm for the TT-QNN and the SC-QNN is mostly distributed
in [0.4, 1.4], which is significantly larger than the gradient norm for the Random-QNN that is mostly
distributed in [0.1, 0.2]. As shown in Figure 5(d), the gradient norm verifies the lower bounds in the
Theorem 3.1. Moreover, we calculate the term α(ρin) defined in Theorem 3.1 and show the result in
Figure 5(h). The average of α(ρm) is around 0.6, which is lower bounded by the theoretical result 4
in Theorem 3.2 (L = 1).
Secondly, we explain our results about QNNs on the binary classification with different class pairs.
We conduct the binary classification with the same super parameters mentioned before for 10-qubit
QNNs, and the test accuracy and F1-scores for all class pairs {i, j} ⊂ {0, 1, 2, 3, 4} are provided
in Table 1. The F1-0 denotes the F1-score when treats the former class to be positive, and the
F1-1 denotes the F1-score for the other case. As shown in Table 1, TT-QNNs and SC-QNNs have
higher test accuracy and F1-score than Random-QNNs for all class pairs in Table 1. Specifically,
test accuracy of TT-QNNs and SC-QNNs exceed that of Random-QNNs by more than 10% for all
class pairs except the (0, 1) which is relatively easy to classify.
In conclusion, both TT-QNNs and SC-QNNs show better trainability and accuracy on binary classifi-
cation tasks compared with the random structure QNN, and all theorems are verified by experiments.
We provide more experimental details and results about the input model and other classification tasks
in Appendix A.
5 Conclusions
In this work, we analyze the vanishing gradient problem in quantum neural networks. We prove that
the gradient norm of n-qubit quantum neural networks with the tree tensor structure and the step
controlled structure are lower bounded by Ω( 1) and Ω((2)nc), respectively. The bound guaran-
tees the trainability of TT-QNNs and SC-QNNs on related machine learning tasks. Our theoretical
framework requires fewer assumptions than previous works and meets constraints on quantum neu-
ral networks for near-term quantum computers. Compared with the random structure QNN which is
known to be suffered from the barren plateaus problem, both TT-QNNs and SC-QNNs show better
trainability and accuracy on the binary classification task. We hope the paper could inspire future
works on the trainability of QNNs with different architectures and other quantum machine learning
algorithms.
9
Under review as a conference paper at ICLR 2021
References
Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak
Biswas, Sergio Boixo, Fernando GSL Brandao, David A Buell, et al. Quantum supremacy using
a programmable superconducting processor. Nature, 574(7779):505-510, 2019.
Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J Osborne, Robert Salzmann, Daniel
Scheiermann, and Ramona Wolf. Training deep quantum neural networks. Nature communi-
cations, 11(1):1-6, 2020.
Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized quantum cir-
cuits as machine learning models. Quantum Science and Technology, 4(4):043001, 2019.
Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M. Sohaib Alam, Shahnawaz Ahmed,
Juan Miguel Arrazola, Carsten Blank, Alain Delgado, Soran Jahangiri, Keri McKiernan, Jo-
hannes Jakob Meyer, ZeyUe Niu, Antal Szava, and Nathan Killoran. Pennylane: Automatic
differentiation of hybrid quantum-classical computations, 2020.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum machine learning. Nature, 549(7671):195-202, 2017.
Sergey Bravyi, David Gosset, and Robert Konig. Quantum advantage with shallow circuits. Science,
362(6412):308-311, 2018.
M Cerezo, Akira Sone, Tyler Volkoff, Lukasz Cincio, and Patrick J Coles. Cost-function-dependent
barren plateaus in shallow quantum neural networks. arXiv preprint arXiv:2001.00550, 2020.
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In International Conference
on Machine Learning, pp. 794-803. PMLR, 2018.
Gavin E Crooks. Gradients of parameterized quantum gates using the parameter-shift rule and gate
decomposition. arXiv preprint arXiv:1905.13311, 2019.
Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive power of parametrized
quantum circuits. Physical Review Research, 2(3), Jul 2020a.
Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Shan You, and Dacheng Tao. On the learnability of
quantum neural networks. arXiv preprint arXiv:2007.12369, 2020b.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
Journal of Machine Learning Research, 20:1-21, 2019.
Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term pro-
cessors. arXiv preprint arXiv:1802.06002, 2018.
Edward Grant, Marcello Benedetti, Shuxiang Cao, Andrew Hallam, Joshua Lockhart, Vid Stoje-
vic, Andrew G. Green, and Simone Severini. Hierarchical quantum classifiers. npj Quantum
Information, 4(1), Dec 2018.
Edward Grant, Leonard Wossnig, Mateusz Ostaszewski, and Marcello Benedetti. An initialization
strategy for addressing barren plateaus in parametrized quantum circuits. Quantum, 3:214, 2019.
Aram W Harrow and Richard A Low. Random quantum circuits are approximate 2-designs. Com-
munications in Mathematical Physics, 291(1):257-302, 2009.
Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of
equations. Physical review letters, 103(15):150502, 2009.
VejteCh Havllcek, Antonio D Corcoles, Kristan Temme, et al. Supervised learning with quantum-
enhanced feature spaces. Nature, 567(7747):209, 2019.
Kohei Hayashi, Taiki Yamaguchi, Yohei Sugawara, and Shin-ichi Maeda. Exploring unexplored
tensor network decompositions for convolutional neural networks. In Advances in Neural Infor-
mation Processing Systems, pp. 5552-5562, 2019.
10
Under review as a conference paper at ICLR 2021
Robert Hecht-Nielsen. Theory of the backpropagation neural network. In Neural networks for
perception, pp. 65-93. Elsevier, 1992.
William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire.
Towards quantum machine learning with tensor networks. Quantum Science and technology, 4
(2):024001, 2019.
Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus Brink, Jerry M
Chow, and Jay M Gambetta. Hardware-efficient variational quantum eigensolver for small
molecules and quantum magnets. Nature, 549(7671):242-246, 2017.
Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum algorithms for deep convolu-
tional neural networks. arXiv preprint arXiv:1911.01117, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Ding Liu, Shi-JU Ran, Peter Wittek, Cheng Peng, RaUl BlazqUez Garcia, Gang Su, and Maciej
Lewenstein. Machine learning by unitary tensor network of hierarchical tree structure. New
Journal of Physics, 21(7):073059, JUl 2019.
Jarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan BabbUsh, and HartmUt Neven. Bar-
ren plateaUs in qUantUm neUral network training landscapes. Nature communications, 9(1):1-6,
2018.
Daniel K Park, Francesco PetrUccione, and JUne-Koo Kevin Rhee. CircUit-based qUantUm random
access memory for classical data. Scientific reports, 9(1):1-8, 2019.
ZbignieW PUchaIa and JarosIaW Adam Miszczak. Symbolic integration with respect to the haar
measUre on the Unitary groUp. arXiv preprint arXiv:1109.4244, 2011.
Patrick Rebentrost, MasoUd Mohseni, and Seth Lloyd. QUantUm sUpport vector machine for big
data classification. Physical review letters, 113(13):130503, 2014.
Franco Scarselli, Marco Gori, Ah ChUng Tsoi, MarkUs HagenbUchner, and Gabriele Monfardini.
The graph neUral netWork model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Maria SchUld, Alex Bocharov, Krysta M Svore, and Nathan Wiebe. CircUit-centric qUantUm classi-
fiers. Physical Review A, 101(3):032308, 2020.
KUnal Sharma, Marco Cerezo, LUkasz Cincio, and Patrick J Coles. Trainability of dissipative
perceptron-based qUantUm neUral netWorks. arXiv preprint arXiv:2005.12458, 2020.
Andrea Skolik, Jarrod R McClean, MasoUd Mohseni, Patrick van der Smagt, and Martin Leib. Lay-
erWise learning for qUantUm neUral netWorks. arXiv preprint arXiv:2006.14904, 2020.
Wojciech Zaremba, Ilya SUtskever, and Oriol Vinyals. RecUrrent neUral netWork regUlarization.
arXiv preprint arXiv:1409.2329, 2014.
Leo ZhoU, Sheng-Tao Wang, SoonWon Choi, Hannes Pichler, and Mikhail D LUkin. QUantUm
approximate optimization algorithm: performance, mechanism, and implementation on near-term
devices. arXiv preprint arXiv:1812.01041, 2018.
11
Under review as a conference paper at ICLR 2021
The Appendix of this paper is organized as follows. We shortly introduce the notion of unitary
2-designs in Appendix B. Some useful technical lemmas are provided and proved in Appendix C.
We prove the Theorem 3.1 in Appendix D and Appendix G. The Appendix C, Appendix D, and
Appendix G form the theoretical framework for analyzing the gradient norm of QNNs. We provide
the proof of Theorem 3.2 in the main text and more detail about the proposed encoding model in
Appendix E. All experimental details are provided in Appendix A.
A Numerical Simulations
In this section, we provide more experimential details about the input model and other binary clas-
sification tasks.
A.1 Quantum Input model for the MNIST Dataset
(a)
(b)
(c)
(d)
Figure 6: The training loss of the input model for MNIST images in class {0, 1, 2, 3}.
(a)	(b)	(c)	(d)	(e)
Figure 7: The visualization of the encoding circuit for MNIST images in class {0, 1, 2, 3, 4}.
In this section, we discuss the training of the input model (Algorithm 1) in detail. We construct
the encoding circuits in Section 3.3 for each training or test data with the number of alternating
layers L = 1. The number of the training iteration is 100. We adopt the decayed learning rate as
{0.100, 0.075, 0.050, 0.025}. We illustrate the loss function defined in Eq. 7 during the training of
Algorithm 1 for label in {0, 1, 2, 3} for the n=8 case in Figure 6, in which we show the training of
the input model for one image per sub-figure. All shown loss functions converge to around -0.6 after
60 iterations.
For a better understanding to the input model, we provide the visualization of the encoding circuit
in Figure 7. We notice that the encoding circuit could only catch a few features from the input data
(except the image 1 which shows good results). Despite this, we obtain relatively good results on
binary classification tasks which employ the mentioned encoding circuit.
Apart from the binary classification tasks using QNNs equipped with the encoding circuit provided
in Section 3.3, we perform some experiments such that the encoding circuit is replaced by the ex-
act amplitude encoding, which is commonly used in existing quantum machine learning algorithms.
Figure 8 demonstrates the simulation on MNIST classifications between images (0, 2), which shows
the convergence of training loss (Figure 8(a)) and the test error (Figure 8(b)). The norm of the gradi-
ent is counted in Figure 8(c), in which both the TT-QNN and the SC-QNN show larger gradient norm
than the Random-QNN. Thus, the trainability of TT-QNNs and SC-QNNs remains when replace the
encoding circuit with the exact amplitude encoding.
The training and test accuracy for other class pairs are summarized in Table 2. We notice that
compared with QNNs using the encoding circuit, QNNs with the exact encoding tend to have better
12
Under review as a conference paper at ICLR 2021
(a)
(b)	(c)
Figure 8: The training of binary classification task on the MNIST image bettwen classes (0, 2)
using 8-qubit QNNs with the exact amplitude encoding. The training loss and the test error during
the training iteration are illustrated in Figures 8(a), 8(b), respectively. The gradient norm of objective
functions during the training is shown in Figures 8(c).
Class pair	TT-QNN TT-QNN (exact encoding) SC-QNN SC-QNN (exact encoding) training, test	training, test	training, test	training, test
0-1 0-2 1-2	0.959, 0.981	0.980, 0.988	0.958, 0.969	0.986,	0.988 0.850, 0.812	0.902, 0.869	0.840, 0.850	0.881,	0.850 0.896, 0.909	0.924, 0.925	0.932, 0.943	0.919,	0.910
Table 2: The training and test accuracy of TT-QNNs and SC-QNNs for different encoding strategies
and different class pairs.
performance on the accuracy, which is reasonable since the exact amplitude encoding remains all
information of the input data.
A.2 QNNs with different qubit sizes
We summarize the results of training and test accuracy for different qubit numbers in Table 3, which
corresponds to the main results presented in Figure 5 in Section 4.2. As shown in Table 3, both the
training and test accuracy of TT-QNNs and SC-QNNs remain at a high level for all qubit number
settings, while the training and test accuracy of Random-QNNs decrease to around 0.5 for the 12-
qubit case, which means the Random-QNN cannot classify better than a random guessing.
Qubit number	TT-QNN training, test	SC-QNN training, test	Random-QNN training, test
8	0.850, 0.812	0.840, 0.850	0.741, 0.765
10	0.865, 0.844	0.871, 0.859	0.804, 0.738
12	0.884, 0.884	0.844, 0.814	0.497, 0.495
Table 3: The training and test accuracy for different qubit numbers {8, 10, 12} on the classification
between (0, 2) classes.
A.3 QNNs with different label pairs
We summarize the results of training and test accuracy, along with the F1-score, for QNNs on
the classification between different label pairs in Table 4 and Table 5, for qubit number 8 and 10,
respectively. For all label pairs, TT-QNNs and SC-QNNs show higher performance of the training
accuracy, the test accuracy, and the F1-scores than that of Random-QNNs. Moreover, most of test
accuracy of Random-QNNs drop for the same class pair when the qubit number is increased from 8
to 10, which suggest the trainability of Random-QNNs get worse as the qubit number increases.
13
Under review as a conference paper at ICLR 2021
TT-QNN
SC-QNN
Random-QNN
Class pair training, test (F1-0, F1-1) training, test (F1-0, F1-1) training, test (F1-0, F1-1)
0-1
0-2
0-3
0-4
1-2
1-3
1-4
2-3
2-4
3-4
0.959, 0.981 (0.981, 0.981)
0.850, 0.812 (0.820, 0.804)
0.871, 0.856 (0.845, 0.866)
0.919, 0.894 (0.891, 0.896)
0.896, 0.909 (0.914, 0.903)
0.940, 0.940 (0.943, 0.937)
0.891, 0.938 (0.938, 0.937)
0.805, 0.771 (0.750, 0.789)
0.810, 0.871 (0.873, 0.869)
0.920, 0.926 (0.922, 0.930)
0.958, 0.969 (0.969, 0.969)
0.840, 0.850 (0.847, 0.853)
0.870, 0.861 (0.853, 0.869)
0.934, 0.910 (0.907, 0.913)
0.932, 0.943 (0.943, 0.942)
0.922, 0.905 (0.912, 0.896)
0.885, 0.915 (0.917, 0.912)
0.824, 0.790 (0.761, 0.813)
0.845, 0.901 (0.899, 0.904)
0.939, 0.901 (0.898, 0.904)
0.954, 0.927 (0.923, 0.932)
0.741, 0.765 (0.790, 0.734)
0.722, 0.728 (0.734, 0.721)
0.831, 0.766 (0.773, 0.759)
0.627, 0.601 (0.704, 0.390)
0.802, 0.794 (0.826, 0.747)
0.850, 0.871 (0.881, 0.860)
0.684, 0.666 (0.694, 0.633)
0.738, 0.733 (0.752, 0.710)
0.695, 0.701 (0.751, 0.627)
Table 4:	The classification accuracy for different class pairs (n=8).
TT-QNN
SC-QNN
Random-QNN
Class pair training, test (F1-0, F1-1) training, test (F1-0, F1-1) training, test (F1-0, F1-1)
0-1
0-2
0-3
0-4
1-2
1-3
1-4
2-3
2-4
3-4
0.965, 0.959 (0.958, 0.960)
0.865, 0.844 (0.852, 0.834)
0.870, 0.835 (0.810, 0.854)
0.945, 0.938 (0.938, 0.938)
0.915, 0.929 (0.930, 0.927)
0.944, 0.938 (0.941, 0.934)
0.844, 0.821 (0.837, 0.802)
0.829, 0.814 (0.791, 0.832)
0.875, 0.931 (0.928, 0.934)
0.940, 0.931 (0.930, 0.933)
0.959, 0.979 (0.979, 0.979)
0.871, 0.859 (0.859, 0.858)
0.871, 0.859 (0.857, 0.860)
0.938, 0.925 (0.925, 0.925)
0.930, 0.965 (0.965, 0.965)
0.899, 0.881 (0.879, 0.884)
0.854, 0.871 (0.879, 0.862)
0.855, 0.820 (0.810, 0.829)
0.889, 0.935 (0.933, 0.937)
0.944, 0.944 (0.943, 0.945)
0.922, 0.920 (0.915, 0.925)
0.804, 0.738 (0.751, 0.722)
0.654, 0.659 (0.680, 0.635)
0.839, 0.779 (0.796, 0.759)
0.693, 0.700 (0.728, 0.666)
0.759, 0.765 (0.773, 0.756)
0.726, 0.705 (0.737, 0.664)
0.754, 0.709 (0.743, 0.664)
0.649, 0.645 (0.728, 0.487)
0.876, 0.819 (0.839, 0.793)
Table 5:	The classification accuracy for different class pairs (n=10).
A.4 QNNs with different rotation gates
In this section, we simulate variants of TT-QNNs and SC-QNNs such that single-qubit gate op-
erations are extended from {RY} to {RX, RY, RZ}. Results on the binary classification between
MNIST image (0, 2) using 8-qubit QNNs are provided in Figure 9. The training loss converges
to around 0.23 and 0.175 for the TT-QNN and the SC-QNN, respectively, and the test error con-
verges to around 0.3 for both the TT-QNN and the SC-QNN. We remark that based on results in
Figures 5(a) and Figure 5(e), the training loss of original TT-QNN and SC-QNN converge at around
0.15, and the test error converge at around 0.20 and 0.15, respectively. Thus, both the TT-QNN and
the SC-QNN show the worse performance than original QNNs when employing the extended gate
set {RX, RY, RZ}. Another result is provided in Table 6 which shows the difference on the training
and test accuracy.
As a conclusion, employing gate set {RX, RY, RZ} could worse the performance of the QNNs on
real-world problems, which may due to the fact that real-world data lie in the real space, while
operations {RX, RZ} introduce the imaginary term to the state.
Class pair	TT-QNN ({RY}) training, test	TT-QNN ({RX, RY, RZ}) training, test	SC-QNN ({RY}) training, test	SC-QNN ({RX, RY, RZ}) training, test
0-2	0.850, 0.812	0.656, 0.699	0.840, 0.850	0.785, 0.731
Table 6: The training and test accuracy of TT-QNNs and SC-QNNs for different single-qubit gate
settings.
14
Under review as a conference paper at ICLR 2021
SSO-IUma
O 20	40	60	80 IOO
training iteration
(a)
」OJ」%tts
OQII ' ' ' I ' ' ' I ' ' ' I ' ' ' I ' ' ' I
O 20	40	60	80 IOO
training iteration
(b)
Figure 9: The training of binary classification task on the MNIST image bettwen classes (0, 2) using
8-qubit QNNs with the extended gate operation {RX, RY, RZ}. The training loss and the test error
during the training iteration are illustrated in Figures 9(a), 9(b), respectively.
B NOTES ABOUT THE UNITARY 2-DESIGN
In this section, we introduce the notion of the unitary 2-design. Consider the finite gate set S =
{Gi}|iS=|1 in the d-dimensional Hilbert space. We denote U (d) as the unitary gate group with the
dimension d. We denote Pt,t (G) as the polynomial function which has the degree at most t on the
matrix elements of G and at most t on the matrix elements of GL Then, We could say the set S to
be the unitary t-design if and only if for every function Pt,t(∙), Eq. (9) holds:
|S| XS	(Ga
(9)
where dμ(∙) denotes the Haar distribution. The Haar distribution dμ(∙) is defined that for any func-
tion f and any matrix K ∈ U (d),
/	dμ(G)f (G) = /	dμ(G')f(KG') = / dμ(G)f(GK).
The form in the right side of (9) can be viewed as the average or the expectation of the function
Pt,t(G). We remark that only the parameterized gates RY = e-iθσ2 could not form a universal gate
set even in the single-qubit space U (2), thus quantum circuits employing parameterized RY gates
could not form the 2-design. This is only a simple introduction about the unitary 2-design, and we
refer readers to PUchaIa & Miszczak (2011) and Cerezo et al. (2020) for more detail.
C	Technical Lemmas
In this section we provide some technical lemmas.
Lemma C.1. Let CNOT = σ0 0 |0〉〈0| + σι 0 |1〉〈1|. Then
CNOT(σj C σk)CNOT才=(δjo + δji)(δko + δk3)σj C σk + (δj0 + δji)(δki + δk2)σ7∙σι C σk
+ (δj2 + δj3)(δk0 + δk3)σj 0 σkσ3 - (δj2 + δj3)(δk1 + δk2)σj σ1 0 σkσ3.
Further for the case σk = σ0,
CNOT(σj C σo)CNOT* = (δj0 + δji)σj C σ0 + (δj2 + δj3)σj C σ3.
Proof.
CNOT(σj Cσk)CNOT*
= (σ0 C |0ih0| + σ1 C |1ih1|) (σj C σk) (σ0 C |0ih0| + σ1 C |1ih1|)
15
Under review as a conference paper at ICLR 2021
σ∩ + σ3	σ∩ — σ3 λ z	、( σ∩ + σ3	σ∩ — σq
σ0 0 ---+ σ1 0 --一(σj 0 σk)	σ∩ 0 ---+ σ1 0 --一
2	2	2	2
1
4 (σj 0 σk + σ1σ7-σ1 0 σk + σ7- 0 σ3σkσ3 + σ1σ7-σ1 0 σ3σkσ3)
1
4
1
4
1
4
+
(σ7∙σ1 0 σk + σ1σ7- 0 σk - σ7-σ1 0 σ3σkσ3 - σ1σ7- 0 σ3σkσ3)
+
(σ7∙ 0 σkσ3 + σ7- 0 σ3σk - σ1σ7-σ1 0 σkσ3 - σ1σ7-σ1 0 σ3σk)
+
(σ7∙σ1 0 σ3σk - σjσ1 0 σkσ3 + σ1σ7- 0 σkσ3 - σ1σ7- 0 σ3σk)
= (δj0 + δj1)(δk0 + δk3)σj 0 σk + (δj∩ + δj1)(δk1 + δk2)σjσ1 0 σk
+ (δj2 + δj3)(δk∩ + δk3)σj 0 σkσ3 - (δj2 + δj3)(δk1 + δk2)σjσ1 0 σkσ3∙
For the case σk = σ∩, we have,
CNOT(σ7∙ 0 σ∩)CNOTt = (δj∩ + δjl)σj 0 σ∩ + (δj2 + δj3)σj 0 σ3.
□
Lemma C.2. Let CZ = σ∩ 0 |0〉〈0| + σ3 0 |1〉〈1|. Then
CZ(σ7∙ 0 σk)CZt =(δj∩ + δj3)(δk0 + δk3)σ7∙ 0 σk + (δj∩ + δ7∙3)(δki + δk2)σ7∙σ3 0 σk
+ (δj1 + δj2)(δk∩ + δk3)σj 0 σkσ3 - (δj1 + δj2)(δk1 + δk2)σjσ3 0 σkσ3∙
Furtherfor the case σk = σ∩,
CZ(σ7- 0 σ∩)CZt = (δj∩ + δj3)σ7∙ 0 σ∩ + (δj1 + δj2)σ7∙ 0 σ3.
Proof.
CZ(σ7∙ 0 σk)CZt
=(σ∩ 0 |0〉〈0| + σ3 0 |1〉(1|)Sj 0 σk) (σ∩ 0 |0〉〈0| + σ3 0 |1〉(1|)
σ∩ + σ3	σ∩ - σ3	σ∩ + σ3	σ∩ - σ3
= σ∩ 0 ---+ σ3 0 --- 5∙ 0 σk) σ∩ 0 ---+ σ3 0 --一
22	22
1
4 (σ7∙ 0 σk + σ3σ7∙σ3 0 σk + σ7- 0 σ3σkσ3 + σ3σ7∙σ3 0 σ3σkσ3)
1
4
1
4
1
4
+
(σ7∙σ3 0 σk + σ3σ7∙ 0 σk - σ7-σ3 0 σ3σkσ3 - σ3σ7∙ 0 σ3σkσ3)
+
(σ7∙ 0 σkσ3 + σ7- 0 σ3σk - σ3σ7∙σ3 0 σkσ3 - σ3σ7∙σ3 0 σ3σk)
+
(σ7∙σ3 0 σ3σk - σ7∙σ3 0 σkσ3 + σ3σ7∙ 0 σkσ3 - σ3σ7∙ 0 σ3σk)
= (δj0 + δj3)(δk∩ + δk3)σj 0 σk + (δj0 + δj3)(δk1 + δk2)σjσ3 0 σk
+ (δj1 + δj2)(δk∩ + δk3)σj 0 σkσ3 - (δj1 + δj2)(δk1 + δk2)σjσ3 0 σkσ3∙
For the case σk = σ∩, we have,
CZ(σj 0 σ∩)CZt = (δj∩ + δ7∙3)σ7∙ 0 σ∩ + (δj1 + δ7∙2)σ7∙ 0 σ3.
□
Lemma C.3. Let θ be a variable with uniform distribution in [0,2π]. Let A, C : H2 → H2 be
arbitrary linear operators and let B = D = σ7- be arbitrary Pauli matrices, where j ∈ {0,1, 2, 3}.
Then
Eθ Tr [WAWt B] Tr [WCW tD] = — [ Tr [WAWt B] Tr [WCW tD]dθ	(10)
2π 7∩
=j+ δj∩ + δjk Tr [AB] Tr [CD]+ -1 + 乐 + %k Tr [ABσk ] Tr [CDσk ],	(11)
where W = e-iθσk and k ∈ {1, 2, 3}.
16
Under review as a conference paper at ICLR 2021
Proof. First we simply replace the term W = e-iθσk = I cos θ - iσk sin θ.
1	2π
dθ r	dθTr[WAWtB]Tr[WCWtD]
Z~ Z	dθTr[(I cos θ — iσk Sin θ)A(Icos θ + iσk Sin θ)B] ∙ Tr[(Icos θ — iσk Sin θ)C(Icos θ + iσk Sin θ)D]
2π 0
Z- ʃ dθ {cos2 θTr[AB] — i sin θ cos θTr[σkAB] + i sin θ cos θTr[AσkB] + sin2 θTr[σkAσkB]}
・ {cos2 θTr[CD] — i sin θ cos θTr[σkCD] + i sin θ cos θTr[CσkD] + sin2 θTr[σkCσkD]}.
We remark that:
-1 广 dθ cos4 θ = 3
2∏ Jo	8
-1 广 dθ sin4 θ =3
2∏ Jo	8
dθ d	dθ cos2 θ sin2 θ =-
2∏ Jo	8
dθ / dθ cos3 θ sin θ = 0,
Z~ Z	dθ cos θ sin3 θ = 0.
2π o
(12)
(13)
(14)
(15)
(16)
Then
33
The integration term = -Tr[AB]Tr[CD] +——Tr[σkAσkB]Tr[σkCσkD]
88
+—Tr[AB]Tr[σ k Cσk D] + —Tr[σk Aσ k B ]Tr[CD]
88
—GTrIσk AB]Tr[σk CD] — — Tr[Aσk B]Tr[Cσk D]
88
+ ~Tr[σ k AB]Tr[Cσk D] + — Tr[Aσk B]Tr[σkCD]
88
=Tr[AB]Tr[CD] ∣ + δj0 + 'jk
+Tr[ABσk ]Tr[CDσk ] —1+ δj0 + j
The last equation is derived by noticing that for B = σj ,
TrIσkAσkB] = TrIAσk σj σk]
=[2(δj0 + δjk) — 1] ∙ Tr[Aσj]
=[2(δj0 + δjk) — 1] ∙ Tr[AB],
TrIσkAB] — TrIAσkB] = TrIAσj σk] — TrIAσk σj]
=2(1 — δjo — δjk) ∙ Tr[Aσjσk]
= 2(1 — δjo — δjk) ∙ Tr[ABσk],
while similar forms hold for D = σj ,
Tr[σkCσkD] = Tr[Cσkσjσk]
=[2(δj0 + δjk ) — 1] ∙ Tr[Cσj]
=[2(δj0 + δjk) — 1] ∙ Tr[CD],
Tr[σkCD] — Tr[CσkD] = Tr[Cσjσk] — Tr[Cσkσj]
=2(1 — δjo — δjk) ∙ Tr[Cσjσk]
=2(1 — δjo — δjk) ∙ Tr[CDσk].
□
17
Under review as a conference paper at ICLR 2021
Lemma C.4. Let θ be a variable with uniform distribution in [0, 2π]. Let A, C : H2 → H2 be
arbitrary linear operators and let B = D = σj be arbitrary Pauli matrices, where j ∈ {0, 1, 2, 3}.
Then
Eθ Tr [GAW tB] Tr [GCW tD] = — Z	Tr [GAWt B] Tr [GCW tD]dθ
2π 0
2 -	δj0	+	δjk	Tr[AB]Tr[CD]+	-1 -	%0	+	%k	Tr[ABσk]Tr[CDσk],
where W = e-iθσk, G = ^∂W^ and k ∈ {1, 2, 3}.
Proof. First we simply replace the term W = e-iθσk
-I sin θ - iσk cos θ.
I cos θ 一 iσk Sin θ and G =
1	2π
—/	dθTr[GAW tB]Tr[GCW tD]
Z- Z dθTr[(-1 sin θ — iσk Cos θ)A(I cos θ + iσk Sin θ)B]Tr[(-1 sin θ — iσk Cos θ)C(I cos θ + iσk Sin θ)D]
Z- ʃ dθ {- sin θ cos θTr[AB] — i cos2 θTr[σkAB] — i sin2 θTr[AσkB] + cos θ sin θTr[σkAσkB]}
• {- sin θ cos θTr[CD] — i cos2 θTr[σk CD] — i sin2 θTr[Cσk D] + cos θ sin θTr[σk Cσk D]}.
The integration above could be simplified using equations 12-16,
The integration term = UTr[AB]Tr[CD] + UTr[σkAσkB]Tr[σkCσkD]
88
-dTr[AB]τr[σkCσkD] - QTr[σkAσkB]Tr[CD] 88 33 -QTr[σkAB]Tr[σkCD] - -Tr[AσkB]Tr[CσkD] 88 --Tr[σkAB]τr[CσkD] — - Tr[AσkB]TrQkCD] 88				
=Tr[AB]Tr[CD]	1 —— 2	δj0 + δjk -2		
+Tr[ABσk]Tr[CDσk]		1	δjo + δjk -2	2-		.
The last equation is derived by noticing that for B = σj ,
Tr[σkAσkB] = Tr[Aσkσjσk]
= [2(δj0 + δjk) -1] • Tr[Aσj]
= [2(δj0 + δjk ) -1] • Tr[AB],
Tr[σkAB] + Tr[AσkB] = Tr[Aσjσk] + Tr[Aσkσj]
= 2(δj0 + δjk) • Tr[Aσj σk]
= 2(δj0 + δjk) • Tr[ABσk],
while similar forms hold for D = σj ,
Tr[σkCσkD] = Tr[Cσkσjσk]
= [2(δjo + δjk) - 1] ∙ Tr[Cσj]
= [2(δj0+δjk) -1] •Tr[CD],
Tr[σkCD] + Tr[CσkD] =Tr[Cσjσk] - Tr[Cσkσj]
= 2(δj0 + δjk) • Tr[Cσjσk]
= 2(δj0 + δjk) • Tr[CDσk].
□
18
Under review as a conference paper at ICLR 2021
D The Proof of Theorem 3.1: the TT part
Now we begin the proof of Theorem 3.1.
Proof. Firstly we remark that by Lemma D.1, each partial derivative is calculated as
-2TkT = I (Tr[° ∙ Vτ(θ+)ρinVττ(θ+)*] — Tr[O ∙ ½τ(θ-)ρinVTτ(θ-)*]),
∂θj(k)	2
since the expectation of the quantum observable is bounded by [-1, 1], the square of the partial
derivative could be easily bounded as:
∂∂fττ!2 ≤ 1
^∂θjk)) 一
By summing up 2n - 1 parameters, we obtain
kVθfττk2 = X(f! ≤ 2n — L
j,k ∂θj
On the other side, the lower bound could be derived as follows,
1+log n
EθkVθfττk2 ≥ X Eθ
j=1
1+log n
= X 4Eθ
j=1
≥ 1 + log n
≥	2n .
2
∂fττ ʌ
1,0,∙∙∙,0)∙ Pin]2 + Tr [σ(3,0,∙∙∙ ,0) ∙ Pin]2
(17)
(18)
(19)
where Eq. (18) is derived using Lemma D.2, and Eq. (19) is derived using Lemma D.3.
□
Now we provide the detail and the proof of Lemmas D.1, D.2, D.3.
Lemma D.1. Consider the objective function of the QNN defined as
f (θ)=1⅛°i
1 + Tr[O∙ V(θ~)ρinV(θ)t]
2
where θ encodes all parameters which participate the circuit as e-iθj σk, k ∈ 1, 2, 3, Pin denotes the
input state and O is an arbitrary quantum observable. Then, the partial derivative of the function
respect to the parameter θj could be calculated by
f = 1 (Tr[O ∙ V(θ+)ρinV(θ+)t] — Tr[O ∙ V也加V(θ J]),
∂ θj	2
where θ+ ≡ θ + ∏ej and θ- ≡ θ 一 ∏ej.
Proof. First we assume that the circuit V(θ) consists of p parameters, and could be written in the
sequence:
V (θ) = Vp(θp) ∙ VP-1(θp-1)∙∙∙ V1(θ1),
such that each block Vj contains only one parameter.
Consider the observable defined as O0 = Vj+1 ∙ ∙ ∙ Vp^OVp …Vj+ι and the input state defined as
Pin = Vj-I •…VIPin%* •…V-1∙ The parameter shifting rule (Crooks, 2019) provides a gradient
19
Under review as a conference paper at ICLR 2021
calculation method for the single parameter case. For fj (θj) = Tr[O0∙ U (θj) ρ[nU (θj )^],the gradient
could be calculated as
dθj=fj (θj+4)- fj (θj- 4).
Thus, by inserting the form of O0 and ρi0n, we could obtain
f=dfj = fj(θj+∏)-fj(θj-∏) = 1 (Tr[O ∙ V(θ+)PinV(θ+)t] - Tr[O ∙ V(θ-)Pi∏V(θ-)t]).
∂θj	dθj	4	4	2
□
Lemma D.2. For the objective function fTT defined in Eq. (3), the following formula holds for every
j ∈ {1, 2,…,1 + log n}:
Eθ( ∂fT! =4 ∙ Eθ(fTr - 2)2，
(20)
where the expectation is taken for all parameters in θ with uniform distribution in [0, 2π].
Proof. First we rewrite the formulation of fTT in detail:
fττ = 2 + 1 Tr [σ(3,0,…,0) ∙ Vm+ιCXm …CXi匕∙如∙ VtCXt …CXmVm十』，(21)
where m = log n and we denote
σ(iι,i2,…，in) ≡ σiι X σi2 ® …® σin.
The operation V consists of n ∙ 2i-` single qubit rotations W(j) =e 2 ` ) on the (j — 1)∙2'-i + 1-
th qubit, where j = 1, 2,…,n ∙ 21-'. The operation CX' consists of n ∙ 2-` CNOT gates, where
each of them acts on the (j — 1) ∙ 2' + 1-th and (j — 0.5) ∙ 2' + 1-th qubit, for j = 1,2,…，n ∙ 2-'.
Now we focus on the partial derivative of the function f to the parameter θj(i). We have:
f =1 Tr
∂θj1) = 2 Ir
σ(3,0,…,0) ∙ Vm+1CXm …-∩y …CXIVI ∙ Pin ∙ Vt CXt …Vt
θj(i)
+ 2Tr σ(3,0,…,0) ∙ Vm+1 CXm …Vj …CXIVI ∙ Pin ∙
VItCXt …∂⅛
CXmt Vmt+i
(22)
CXmt Vmt+i
(23)
∂V
Tr	σ(3,0,…,0)	∙	Vm+1 CXm	…-(ɪ)…CXIV1	∙ ρin	∙	V1	CXI …Vj	…CXmVm+1	.
θj
(24)
The key idea to derive Eθ (f⅛
The Eq. (24) holds because both terms in (22) and (23) except Pin are real matrices, and Pin = Pitn.
2
=4 ∙ Eθ(fψT 一 1 )2 is that for cases B = D = σj ∈ {σ1,σ3},
the term δj0+δj2 = 0, which means Lemma C.3 and Lemma C.4 collapse to the same formulation:
EθTr[WAWtB]Tr[WCWtD] = EθTr[GAWtB]Tr[GCWtD]
=2 Tr[Aσ1]Tr[Cσ1] + 2 Tr[Aσ3]Tr[Cσ3].
Now we write the analysis in detail.
(25)
20
Under review as a conference paper at ICLR 2021
2
=Eθι∙∙∙EθmEθ⑴ Trσ(3,0,…，0) ∙ Vm+1CXmAmCXm
m+1
-Eθι …EθmEθ(ι^ιTr 卜(3,0,…,0) ∙ Vm+1CXmBmCXmVm +J
1	21	2
二Eθι ∙∙∙ Eθm 2	2Tr	[σ(3,0,…，0,3,0,…，0)	∙	Am]	+	$Tr	]σ(1,0,…，0,0,0,…，0)	∙	Am]
1	21	2
-Eθι …Eθm 2 2Tr [σ(3,0,…,0,3,0,…,0) ∙ Bm] + $Tr [σ(1,0,…,0,0,0,…,0) ∙ Bm]
(26)
(27)
(28)
(29)
where
∂V
Am = VmCXm-I …诘…CXl Vl ∙ Pin ∙ VtCX[…Vt ∙∙∙ CXm-IVm,
θj(1)	1	1 j	m-1 m
Bm = ‰ CXm-I •∙• 吟 …CXl 用∙ Pin ∙ VtCXt …Vt …CXm-I 煦，
and Eq. (28,29) are derived using the collapsed form of Lemma C.1:
CNOT(σι ㊈ σ0)CNOTt = σι ㊈ σ0, CNOT(σ3 ㊈ σ0)CNOTt = σ3 ㊈ σ3,
and θ` denotes the vector consisted with all parameters in the `-th layer. The integration could be
performed for parameters {θm, θm-ι, •…，θj+ι}. It is not hard to find that after the integration of
the parameters θj+ι, the term Tr[σ% ∙ Aj]2 and Tr旧∙ Bj]2 have the opposite coefficients. Besides,
the first index of each Pauli tensor product。(汨，i2,…，in) could only be iι ∈ {1,3} because of the
Lemma C.3. So we could write
-4(fTT - 2)2	(3O)
33
=Eθι∙∙∙ Eθj{ X X …X aiTr ⑸∙ Aj ]2 - aE ⑸∙ Bj ]2 ∖	(31)
(iι∈{1,3} i2 =0	in=0
where
∂V
Aj =前 CXj-1 ∙∙∙ CXiVi ∙ Pin ∙ VtCXf …CXj-IVt
=(Gji)㊈产5-I))A/θj1)(Wj(i)t ㊈ I®(n-i)),
Bj = V∙CXj-i …CX1Vi ∙ ρin ∙ VtCXf …CXj-IVt
=(Wj:1)㊈ 10(n-i))A∕θj1) (Wj(i)t ㊈ 10(n-i)),
and ai is the coefficient of the term Tr [σi ∙ Aj ]2. We denote Gji) = ∂W⅞y and use A，to denote
the rest part ofAj and Bj. By Lemma C.3 and Lemma C.4, we have
Eθ(ι) hTr [σi ∙ Aj]2 - Tr [σi ∙ Bj]2i = 0,
since for the case ii ∈ {1,3}, the term δi10+δi12 = 0, which means Lemma C.3 and Lemma C.4
have the same formulation. Then, we derive the Eq. (20).
□
Lemma D.3. For the loss function f defined in (3), we have:
Ee(fττ - 1)2 ≥ {TrIσ(i,0,…，0) ∙ P加]『+ {Tr ]σ(3,0,…，0) ∙2加]}2	(32)
where we denote
σ(iι,i2,…，in) ≡ σiι X σi2 区.一区 σin ,
and the expectation is taken for all parameters in θ with uniform distributions in [0, 2π].
21
Under review as a conference paper at ICLR 2021
Proof. First we expand the function fTT in detail,
fττ = 2 + 1 Tr [σ(3,0,…,0) ∙ Vm+ιCXm …CXi匕•加∙ VjCX[…CXmVm +J ,	(33)
where m = log n. Now We consider the expectation of (fττ - 11 )2 under the uniform distribution
for θm(1+) 1:
吗件(加-g)2 = 4EΘ(1^1 {Tr 卜(3,0,…,0) ∙ Vmi+iCXm …CXIVI ∙ ρin ∙ vi^cxi …CXmVm+J}
(34)
=8 {Tr [σ(3,0,…,0) ∙ A0]}2 + 8 {Tr [σ(1,0,…,0) ∙ A0]}2	(35)
=8 {Tr [σ(3,0,…,0,3,0,…,0) •A]}1 + 1 {Tr [σ(1,0,…,0,0,…,0) •A]}2	(36)
≥ 8 {n Ei,。,…a。,…0 ∙ A]}2,	(37)
where
A0 = CXmVm …CXiVi ∙ pm ∙ VjCXI …VmCXm,	(38)
A = Vm …CXiVl ∙ Pin ∙ VtCXI …Vm,	(39)
and Eq. (35) is derived using Lemma C.3, Eq. (36) is derived using the collapsed form of
Lemma C.1:
CNOT(σi 0 σ0)CNOTt = σi 0 σ0, CNOT(σ3 0 σ0)CNOTt = σ3 0 σ3,
We remark that during the integration of the parameters {θ'j)} in each layer ' ∈ {1,2,…，m +1},
the coefficient of the term {Tr[σq,0,…，0) ∙ A]}2 only times a factor 1/2 for the case j = 1, and
the coefficient remains for the cases j > 1 (check Lemma C.3 for detail). Since the formulation
{Tr[σ(i,0,…，0) ∙ A]}2 remains the same when merging the operation CX' with σq,0,…，0), for ' ∈
{1,2,…，m}, we could generate the following equation,
% …Eθm+ι (fTT - g)2 ≥ (2)m+2 {Tr hσ(i,0,∙∙∙ ,0) ∙ Vi ∙ Pin ∙ Vitio ,	(4O)
where θ` denotes the vector consisted with all parameters in the `-th layer.
Finally by using Lemma C.3, we could integrate the parameters {θi(j)}jn=i in (40):
Eθ(fTT - 2)2	=Eθ1 …Eθm + 1 (fTT -	1)2 2)	(41)
	≥ {Tr [σ(i,0,…,0) ∙ pin]	}2 + {Tr [σ(3,0,…,0) ∙ pin]}2 2m+3	(42)
	={n[σ(i,0,…，0) ∙ Pin]	}2 + {Tr [σ(3,0,…,0) ∙ pin]}2	(43)
		. 8n	
□
E The Quantum Input Model
For the convenience of the analysis, we consider the encoding model that the number of alternating
layers is L. The model begins with the inital state ∣0i0n, where n is the number of the qubit. Then
we employ the X gate to each qubit which transform the state into ∣1i0n. Next we employ L
alternating layer operations, each of which contains four parts: a single qubit rotation layer denoted
as V2i-i, a CZ gate layer denoted as CZ2, again a single qubit rotation layer denoted as V2i, and
another CZ gate layer with alternating structures denoted as CZi, for i ∈ {1, 2, •…，L}. Each
single qubit gate contains a parameter encoded in the phase: Wj(k) = e-iσ2βj(k), and each single
qubit rotation layer could be written as
Vj- = Vj- (βj) = W(I) 0 w(2) 0∙∙∙0 w(n).
22
Under review as a conference paper at ICLR 2021
0i
0i
0i
0i
0i
0i
0i
0i
Figure 10: An example of the variational encoding model (L = 2 case).
Finally, we could mathematically define the encoding model:
U(Pin) = V2l+iUlUl-i …UiX0n,
where Uj = CZ1 V2j CZ2V2j-1 is the j-th alternating layer.
By employing the encoding model illustrated in Figure 10 for the state preparation, we find that the
expectation of the term α(ρin) defined in Theorem 1.1 has the lower bound independent from the
qubit number.
Theorem E.1. Suppose the input state ρin(θ) is prepared by the L-layer encoding model illustrated
in Figure 10. Then,
Eeα(ρin) = Ee (Tr [σ(1,0,…,0) ∙ Pin] + Tr ]σ(3,0,…,0) ∙ Pin] ) ≥ 2 ",
where β denotes all variational parameters in the encoding circuit, and the expectation is taken for
all parameters in β with uniform distribution in [0, 2π].
Proof. Define Pj = UjUj-i …UiX/0产〈0产nXMUj …uj^-]ULfor j ∈{0,1,…，L}.We
have:
Ee(Tr [σ(1,0,…,0) ∙ Pin] 2 + Tr ]σ(3,0,…,0) ∙ Pin] 2)	(44)
=Eβι …Eβ2L + ι 卜r hσ(1,0,…，0) ∙ V2L+1PL VL+J + Tr hσ(3,0,…，0) ∙ V2L+1PL VL+J ) (45)
=EeI …Eβ2L (Triσ(1,0,…,0) ∙ pl]2 + Tr ]σ(3,0,…,0) ,夕工]2) .	(46)
≥ 2-2L(Tr Ei,。,…,°) ∙ P0] 2 + Tr [。⑶。,…,。)・夕。]2)	(47)
=2-2l • (02 + (-1)2) = 2-2l,	(48)
where Eq. (45) is derived from the definition of PL . Eq. (46) is derived using Lemma C.3. Eq. (47)
is derived by noticing that for each j ∈ {0,1, ∙∙∙ ,L 一 1}, the following equations holds,
EeI …Eβ2j+2 (Tr [σ(i,0,…，0) ∙ Pj+i] 2 + Tr ]σ(3,0,…，0) ∙ Pj+i] 2)
=EeI …Eβ2j+2 (Tr 卜(i,0,…,0) ∙ Uj+ipjUj+i] + Tr 卜(3,0,…,0) ∙ Uj+ipjUj+i]
(49)
(50)
23
Under review as a conference paper at ICLR 2021
=Eβι …Eβ2j+2Tr 卜(1,0,…，0) ∙ cZ1V2j+2cz2V2j + 1ρj v2^j-+icz2v2j+2cZl]
+ Eβι …Eβ2j+2Tr 卜(3,0,…，0) ∙ cZ1V2j+2cZ2V2j + 1ρjVj+iCZZVj+zCZli
=Eβι …Eβ2j+2Tr [σ(i,3,0,…,0) ∙ %∙+2CZ2%∙+1Pj Vj+1CZ2Vj+2] 2
2
2
(51)
(52)
(53)
+ Eβι …Eβ2j+2Tr 卜(3,0,…，0) ∙ V2j+2cZ2V2j+lPjVj+ICZZVj+z]
≥ Eβι …Eβ2j+2Tr 卜(3,0,…,0) ∙ V2j+2cZ2v2j+1ρjVj+ICZZVj+zi
2
2
(54)
(55)
=EeI …Eβ2j+1 (2Tr 卜(1,0,…，0) ∙ CZ2V2j+1ρjVj+iC&i + 2Tr 卜(3,0,…，0) ∙ CZ2V2j + 1ρjVj+1
(56)
=Eβι …Eβ2j+1 (2Tr 卜(1,0,…，0,3) ∙ V2j + 1ρjV2j+l] + 2Tr 卜(3,0,…，0) ∙ V2j+1ρjV2j + l])
(57)
2
≥ Eβι …Eβ2j+1 2Tr 卜(3,0,…,0) ∙ V2j+1ρj V2j+J
=Eβι …Eβ2j 4 (Trl σ(1,0,…，0) ∙ ρj]2 + Tr [σ(3,0,…，0) ∙ ρj]2),
(58)
(59)
where Eq. (50) is derived from the definition of ρj+1. Eq. (51-52) are derived from the definition
of Uj+1. Eq. (53-54) and Eq. (57) are derived using Lemma C.1. Eq. (56) and Eq. (59) are derived
using Lemma C.3.
□
F The Deformed Tree Tensor QNN
Similar to the TT-QNN case, the objective function for the Deformed Tree Tensor (DTT) QNN is
given in the form:
fDTτ(θ) = 2 + 2Tr[σ3 0 I须n-1)VDTτ(θ)ρinVDTτ(θ)*],
(60)
where VDTT denotes the circuit operation of DTT-QNN which is illustrated in Figure 11. The lower
bound result for the gradient norm of DTT-QNNs is provided in Theorem F.1.
Theorem F.1. Consider the n-qubit DTT-QNN defined in Figure 11 and the corresponding objective
function fDTT defined in (60), then we have:
1+nogn ∙ α(ρin) ≤ EθkVθfDTk2 ≤ 2n - 1,
(61)
where the expectation is taken for all parameters in θ with uniform distributions in [0, 2π],
Pin ∈ C2n×2n denotes the input state, α(ρin) = Tr [σq,0,…，0) ∙ ρin∖ + Tr 卜⑶。,…，0) ∙ Pn],and
σ(il,i2,…，in) ≡ σil 0 σi2 0 …0 σin.
Proof. Firstly we remark that by Lemma D.1, each partial derivative is calculated as
"DTT = ɪ (Tr[O ∙ VDTT(θ+)ρinVDiτ(θ+)*] - Tr[O ∙ Vdtt(θ-)PinVdtt(θ-)^]),
∂θj(k)	2
since the expectation of the quantum observable is bounded by [-1, 1], the square of the partial
derivative could be easily bounded as:
dfDTTX Z 1
∂θjk) J -，
24
Under review as a conference paper at ICLR 2021
Figure 11: Quantum Neural Network with the Deformed Tree Tensor structure (qubit number = 12).
By summing up 2n - 1 parameters, we obtain
kVθ fDTTk2 = X( f )2≤ 2n - 1.
j，k	∂θj
On the other side, the lower bound could be derived as follows,
1+dlog ne	2
EθkVθfDττk2 ≥ X Eθ	fDτT'
j=1
1+dlog ne
= X 4Eθ
j=1
≥ 1 + log n
≥	4n
2
(62)
(63)
• (Trlσ(1,0,…，0) ∙ ρin] + Tr ]σ(3,0,…，0) ∙ Pin]
(64)
where Eq. (63) is derived using Lemma F.1, and Eq. (64) is derived using Lemma F.2.
□
Lemma F.1. For the objective function fDTT defined in Eq. (60), the following formula holds for
every j ∈{1, 2,…，1+「log n∖}:
Eθ (:f(T! =4 • Eθ(fDTT - 2)2,
(65)
where the expectation is taken for all parameters in θ with uniform distribution in [0, 2π].
25
Under review as a conference paper at ICLR 2021
Proof. The proof has a very similar formulation compare to the original tree tensor case. First we
rewrite the formulation of fDTT in detail:
fDTT = 2 + 2Tr 卜(3,0,…,0) ∙ Vm+1 CXm …CXIVI ∙ Pin ∙ V'CX …CXmVm +J ,	(66)
where m = dlog ne and we denote
σ(iι,i2,…，in) ≡ σiι X σi2 ® …® σin.
The operation V' consists of [n ∙ 21-`C single qubit rotations W(j) = e-iσ2θ'j) on the (j 一 1) ∙
2'-1 + 1-th qubit, where j = 1, 2, ∙∙∙ , [n ∙ 21-'C. The operation CX' consists of bn ∙ 2-'C CNOT
gates, where each of them acts on the (j 一 1) ∙ 2' + 1-th and (j 一 0.5) ∙ 2' + 1-th qubit, for
j = 1, 2,…，[n ∙ 2-'C.
Now we focus on the partial derivative of the function f to the parameter θj(1). We have:
∂fDTT
∂θj1)
1	Tr σ(3,0,…,0) ∙ Vm+ιCXm …dVj ∙∙∙ CXi Vi ∙ ρi∏ ∙ Vt CXt …Vt
2	θj(1)
+ 2Tr σ(3,0,…,0) ∙ Vm+iCXm …V∙…CXiVI ∙ Pin •
∂Vj'
VItCXl…箭
θj
∂V
Tr σ(3,0,…,0) ∙ Vm+iCXm …^γi)…CXiVI ∙ Pin ∙ Vl CXi …Vj ∙∙∙ CXmVm+i .
θj
(69)
The Eq. (69) holds because both terms in (67) and (68) except Pin are real matrices, and Pin = Pitn.
Similar to the tree tensor case, the key idea to derive Eθ (∂fDTT) = 4 ∙ Eθ(∕dtt - i)2 is that for
cases B = D = σj ∈ {σi, σ3}, the term δj0+δj2 = 0, which means Lemma C.3 and Lemma C.4
collapse to the same formulation:
EθTr[WAWtB]Tr[WCWtD]	= EθTr[GAWtB]Tr[GCWtD]
= j Tr[Aσi]Tr[Cσi] + 2 Tr[Aσ3]Tr[Cσ3].
Now we write the analysis in detail.
Eθ
∂fDTT
2
一 4(fDTT — 2)2
=Eθι …Eθm Eθg]Jr[σ(3,0,…，0) ∙ Vm+iCXmAmCXmVm+i]
-Eθι …EθmEθ落Tr [σ(3,0,…,0) ∙ Vm+iCXmBmCXmVm十』
1	21	2
=Eθι •- Eθm 2 2Tr [σ(3,0,…，0,3,0,…，0) ∙ Am] + 2Tr [σ(i,0,…，0,0,0,…，0) ∙ Am]
1	21	2
-Eθι …Eθm 2 2Tr [σ(3,0,…,0,3,0,…,0) ∙ Bm] + 2Tr [σ(i,0,…,0,0,0,…,0) ∙ Bm]
(70)
(71)
(72)
(73)
(74)
2
2
where
∂V
Am = VmCXm-I …皆…CXiVi ∙ pin ∙ VtCXt …Vt …CXm-IVm,
θj(i)	i i j	m-i m
26
Under review as a conference paper at ICLR 2021
Bm = VmCXm-I …Vj …CXiVl ∙ Pin ∙ VtCx[…Vt …CXm-IVm.
Eq. (73) and Eq. (74) are derived using the collapsed form of Lemma C.1:
CNOT(σ1 乳 σ0)CNOTt = σ1 ㊈ σ0, CNOT(σ3 乳 σ0)CNOTt = σ3 ㊈ σ3,
and θ` denotes the vector consisted with all parameters in the `-th layer. The integration could be
performed for parameters {θm, θmj-ι, •…，θj+ι}. It is not hard to find that after the integration of
the parameters θj+ι, the term Tr[σi ∙ Aj]2 and Tr[σ% ∙ Bj]2 have the opposite coefficients. Besides,
the first index of each Pauli tensor product。(汨缄,…，in)could only be ii ∈ {1,3} because of the
Lemma C.3. So we could write
-4(fDTT - 2)2	(75)
33
=Eθι …Eθj( X X …X aiTr ⑸∙ Aj ]2 - aE ⑸∙ Bj ]2 ∖	(76)
(iι∈{1,3} i2 =0	in=0
where
∂V
Aj = ∂ΘCXCXj-1 ∙∙∙CXiVi ∙Pin ∙ VtCXt'…CXj-IVt
=(Gji)㊈ I^(n-i))A∕θj-1) (Wj(i)t 0 产(n-i)),
Bj = VjCXj-i …CXiVt ∙ ρin ∙ VtCXt …CXj-IVt
=(Wf) 0 10(n-i))A∕θj1) (Wj(i)t 0 10(n-i)),
and ai is the coefficient of the term Tr [σi ∙ Aj ]2. We denote Gji) = ∂Wj1) and use A，to denote
the rest part ofAj and Bj. By Lemma C.3 and Lemma C.4, we have
Eθ(i) [Tr 旧∙ Aj]2 - Tr 旧∙ Bj]2] = 0,
since for the case ii ∈ {1,3}, the term δi10+δi12 = 0, which means Lemma C.3 and Lemma C.4
have the same formulation. Then, we derive the Eq. (65).
□
Lemma F.2. For the loss function fDTT defined in (60), we have:
E (f - 1)2 ≥ {乃[σ(i,。,…，0) ∙ Pn]}2 + {乃[σ(3,。,…，0) ∙ Pin]}2	(77)
θ DTT 2	16n
where we denote
σ(i1,i2,…，in) ≡ σiι 0 σi2 0∙∙∙0 σin ,
and the expectation is taken for all parameters in θ with uniform distributions in [0, 2π].
Proof. First we expand the function fDTT in detail,
fTT = 2 + 1 Tr 卜(3,0,…,0) ∙ Vm+iCXm ∙∙∙ CXiVi ∙ Pin ∙ VtCXf …CXmVm +ii ,	(78)
where m =dlog n]. Now We consider the expectation of (∕dtt - 2)2 under the uniform distribution
for θm(i+) i:
Eθ(% (fDTT - 2)2 = 4Eθ(I)H {Tr [σ(3,0,…,0) ∙ Vm+iCXm …CXiVt ∙ Pin ∙ VtCXf …CXmVm+i]}
(79)
=I {Tr [σ(3,0,…,0) ∙ A0]}2 + I {Tr [σ(i,0,…,0) ∙ A0]}2	(80)
88
27
Under review as a conference paper at ICLR 2021
=8 {Tr [σ(3,0,…,0,3,0,…,0) ∙ A]}2 + 8 {Tr [σ(1,0,…,0,0,…,0) ∙ A]}2	(81)
≥ 8 {n ba,。,…,0,0,…,0)∙ A]}2,	(82)
where
A0 = CXmVm …CXiVi ∙ Pin ∙ VJCXI …VmCXm,	(83)
A = Vm ∙∙∙ CXiVi ∙ Pin ∙ VtCXI …Vm.	(84)
Eq. (80) is derived using Lemma C.3, and Eq. (81) is derived using the collapsed form of
Lemma C.1:
CNOT(σι 0 σ0)CNOTt = σι 0 σ0, CNOT(σ3 0 σ0)CNOTt = σ3 0 σ3,
We remark that during the integration of the parameters {θ'j)} in each layer ' ∈{1,2,…，m +1},
the coefficient of the term {Tr[σ(i,0,…，0)∙ A]}2 only times a factor 1/2 for the case j = 1, and
the coefficient remains for the cases j > 1 (check Lemma C.3 for detail). Since the formulation
{Tr[σ(i,0,…，0)∙ A]}2 remains the same when merging the operation CX' with σq,0,…，0), for ' ∈
{1,2,…，m}, We could generate the following equation,
Eθ2 …Eθm+ι (ZDTT- 2)2 ≥ (2)m+2 nTr hσ(i,0,…,0)∙ Vi ∙ Pin ∙ Vtio ,	(85)
where θ` denotes the vector consisted with all parameters in the `-th layer.
Finally by using Lemma C.3, we could integrate the parameters {θi(j)}jn=i in (85):
Eθ (Zdtt — 2)2	=Eθι …Eθm+1 (ZDTT - 2)2	(86)
	≥ {Tr [σ(i,0,…,0) ∙ Pin]}2 + {Tr [σ(3,0,…,0) ∙ 2in]}2 —	2m+3	(87)
	≥ {Tr [σ(i,0,…,0) ∙ Pin]}2 + {Tr [σ(3,0,…,0) ∙ Pin]}2 -	16n	.	(88)
		□
G The Proof of Theorem 3.1: the SC part
Theorem G.1. Consider the n-qubit SC-QNN defined in Figure 2 and the corresponding objective
function fSC defined in (4), then we have:
1+n
9i+n ∙ α(P in ) ≤ Eθ l∣VθfSC ∣∣	≤ 2n - 1,
2 +nc
(89)
where nc is the number of the control operation CNOT that directly links to the first qubit chan-
nel, and the expectation is taken for all parameters in θ with uniform distributions in [0, 2π],
Pin ∈ C2n×2n	denotes the input state,	α(ρin)	= Tr	[σq,0,…，0)∙	ρin∖	+ Tr [σ(3,0,…，0)∙ Pin∖	, and
σ(il,i2,…，in ) ≡ σil X σi2 %…% σin .
Proof. Firstly we remark that by Lemma D.1, each partial derivative is calculated as
f = 1 (Tr[O ∙ ⅛C(θ+)Pin⅛C(θ+)t ] - Tr[O ∙%C(θ-)Pin%C(θ-)t]),
∂θj( )	2
since the expectation of the quantum observable is bounded by [-1, 1], the square of the partial
derivative could be easily bounded as:
28
Under review as a conference paper at ICLR 2021
By summing up 2n - 1 parameters, we obtain
kvθ fSCk2=X O 2n - 1.
On the other side, the lower bound could be derived as follows,
Eθ kvθfSC k2 ≥ Eθ
(1 + nc) ∙ 4 ∙ Eθ
1 + nc
≥ —__c
21+nc
1,0,∙∙∙,0)∙ Pin]2 + Tr ]σ(3,0,…，0) ∙ Pin]2
where Eq. (91) is derived using Lemma G.1, and Eq. (92) is derived using Lemma G.2.
(90)
(91)
(92)
□
Lemma G.1. For the objective function fSC defined in Eq. (60), the following formula holds for
every j such that θj(1) tunes the single qubit gate on the first qubit channel:
(93)
where the expectation is taken for all parameters in θ with uniform distribution in [0, 2π].
Proof. First we write the formulation of fSC in detail:
fSC = 2 + 2Tr 卜(3,0,…,0) ∙ VnCXn-1 ∙ ∙ ∙ CX1V1 ∙ Pin ∙ V*CX[…CXn-IVnJ ,	(94)
where We denote 仃⑦E…,in)≡ σΜ 0 θi2 0∙∙∙0 σin. The operation CX' is defined as,
CX	CNOT operation on qubits pair (n + 1 - `, n - `)	(1 ≤ ` ≤ n - 1 - nc),
` =	CNOT operation on qubits pair (n + 1 - `, 1)	(n - nc ≤ ` ≤ n - 1).
The operation 腔 is defined as,
(WF) 0 W(2) 0 …0 W(n)	(' = 1),
%=< IMrT' 0 Wf) 0 I须'-1)	(1 ≤ ' ≤ n - 1 - nc),
[W(I) 0 I 0 I 0 …0 I	(n — n ≤ ' ≤ n).
Now we focus on the partial derivative of the function fSC to the parameterθj(1). We have:
∂f	1	∂V
—∩y = WTr σ(3,0,…,0) ∙ VnCXn-1-----------∩γ …CXIVI ∙ Pin ∙ V1CX1 …Vj …CXn-IVn
∂θj()	2	θj()
(95)
1
-Tr
2
+
∂Vjt
σ(3,0,…,0) ∙ VnCXn-1 …Vj …CXIVI ∙ Pin ∙ %*CXI-----------------∩y …CXn-IVn
θj()
(96)
∂V
Tr	σ(3,0,…，0)	∙	VnCXn-1 …-∩y …CXIV1	∙	Pin	∙ % CXI	…Vj	…CX	IVn
θj(1)
(97)
29
Under review as a conference paper at ICLR 2021
The Eq. (97) holds because both terms in (95) and (96) except Pin are real matrices, and Pin = ρin.
The key idea to derive Eθ (∂⅛)) = 4 ∙ Eθ (∕sc - 1 )2 is that for cases B = D = σ7- ∈ {σ1,σ3} in
Lemma C.3 and Lemma C.4, the term δj0+δj2 = 0, which means both lemma collapse to the same
formulation:
EθTr[WAWt B]Tr[WCWt D] = Eθ Tr[GAWt B]Tr[GCWt D]
=2 Tr[Aσ1]Tr[Cσ1] + 2 Tr[Aσ3]Tr[Cσ3].
Now we write the analysis in detail.
=Eθι ∙ ∙ ∙ Eθn-1 EθnTr [σ(3,0,…，0) ∙ VnCXn-1An-1CXn-I吟]
-Eθι …Eθn-1 EθnTr [σ(3,0,∙∙∙,0) ∙ VnCXn-iBn-iCXn-iV^] 2
1	21	2
=EΘ1 •一 Eθn-1 2 2Tr [σ(3,3,0,…,0) ∙ An-1] + 2Tr [σ(1,0,0,…,0) ∙ An-1]
1	21	2
-Eθι •- Eθn-ι 2 2Tr [σ(3,3,0,…，0) ∙Bn-1] + 2Tr [σ(1,0,0,…,O) ∙ Bn-1]
(98)
(99)
(100)
(101)
(102)
where
∂V
An-1 = Vn-ICXn-2 …皆∙∙∙ CX1V1 ∙ Pin ∙ VtCXi …Vt -CXn-VLi,
θj(1)	1	1 j	n-2 n-1
Bn-1 = Vn-ICXn-2 …Vj …CXIVI ∙ Pin ∙ VtCXt …Vt …CXn-2Vt-1 .
Eq. (101) and Eq. (102) are derived using Lemma C.3 and the collapsed form of Lemma C.1:
CNOT(σ1 乳 σ0)CNOTt = σ1 ㊈ σ0, CNOT(σ3 ㊈ σ0)CNOTt = σ3 ㊈ σ3,
and θ` denotes the vector consisted with all parameters in the `-th layer. The integration (99)-
(102) could be performed similarly for parameters {θn-1, θn-2, •…，θj+1}. It is not hard to find
that after the integration of the parameters θj+1, the term Tr[σ% ∙ Aj-]2 and Tr[σ% ∙ Bj-]2 have the
opposite coefficients. Besides, the first index of each Pauli tensor product。(”^?,…，in) could only
be i1 ∈ {1, 3} because of the Lemma C.3. so we could write
)3	3
X X …X aiTM ∙ Aj]2 -。再⑸∙ Bj]2
i1∈{1,3} i2=0	in=0
(103)
(104)
where
∂V
Aj=加⑴ CXj-I …CXIVI ∙ Pin ∙ V1 CXI …CXj-1V∙
=(GjI)㊈ 10(nT))A/θjI)(Wj(I)t ㊈ 10(n-1)),
Bj = V∙CXj-1 …CX1V1 ∙ Pin ∙ VtCXt …CXj-1 Vjt
=(W(1) ㊈ 10(nT))A/θjI)(W(I)t ㊈ I0(n-1)),
30
Under review as a conference paper at ICLR 2021
and ±ai are coefficients of the term Tr [σ% ∙ Aj]2, Tr [σ% ∙ Bj]2, respectively. We denote GjI)=
∂W (1)	/θ(1)
-jŋ- and use A j to denote the rest part of Aj and Bj. By Lemma C.3 and Lemma C.4, We have
∂θj	j
Eθ(i) [Tr [σi ∙ Aj]2 - Tr 旧∙ BjH = 0,
since for the case i1 ∈ {1,3}, the term δi10+δi12 = 0 in Lemma C.3 and Lemma C.4, which means
both lemmas have the same formulation. Then, We derive the Eq. (93).
□
Lemma G.2. For the loss function fSC defined in (4), we have:
Eθ (fSC - 2 J ≥ {"Ew∙,0)∙ρ⅛mσ(3"∙,0)∙T2,	(105)
where we denote
σ(i1,i2,…，in) ≡ σiι X σi2 区.一区 σin ,
and the expectation is taken for all parameters in θ with uniform distributions in [0, 2π].
Proof. We expand the function fSC in detail,
fSC = 2 + 2Tr 卜(3,0,…,0) ∙ VnCXn-I …CXIV1 ∙ Pin ∙ VJCX[…CXn-IVn^
= I + ITrhσ3,0,…，0 ∙ P(n)i ,
where
Pj) = VjCXj-I …CXiVlPin ∙ VtCx[…CXj-1 Vjt, ∀j ∈ H	(106)
Now We focus on the expectation of (fSC 一 ɪ)2:
Eθ (∕sc - 2) = EθιEθ2 …Eθn 4Tr[σ3,o,…,0 ∙ p(n)]	(107)
≥ EθιEθ2 …Eθn-11Tr [σι,0,…,0 ∙。(…)]2	(108)
≥ Eθ1 Eθ2 …Eθn-nc 2^	[。1,0,…,0 ∙。(…r 2	(109)
=Eθ1 Eθ2 …Eθn-nc-1 22^^1,0,-,0 •…1)]2	(110)
=Eθ122‰ Trhσι,0,…,0 ∙ P(I) i2	(111)
=23+nc (Tr [σ1,0,…,0 ∙ ρin] + Tr[σ3,0,…,0 ∙ ρin] ) ,	(112)
where Eq. (112) is derived from Lemma C.3. We derive Eqs. (108-109) by noticing that following
equations hold for n - nc + I ≤ j ≤ n and i ∈ {I, 3},
EθjTr [σi,0,…,0 ∙ ρj[ 2 = Ee,Tr [吗,。,…,0 ∙ VjCXj-Ip(jT)CX3Vjt] 2	(113)
=ITr	[σι,0,…,0 ∙ CXj-iρ(jT)CX3]	2	(114)
+ 2Tr	[σ3,0,…，0 ∙ CXj-iρ(jτ)CX3]2	(115)
≥ ITr	[σι,0,…,0 ∙ CXj-ιρjT)CX3]	2	(116)
=lTr[σι,0,…,0 ∙ ρ(jτ)]2,	(117)
31
Under review as a conference paper at ICLR 2021
where Eq. (113) is derived based on the definition of ρ(j) in Eq. (106), Eqs. (114-115) are derived
based on Lemma C.3, and Eq. (117) is derived based on Lemma C.1.
We derive Eq. (110-111) by noticing that following equations hold for 2 ≤ j ≤ n - nc,
EθjTr [σι,o,…,0 ∙ Pj)[ 2 = EejTr [σι,o,…,0 ∙ VCXj-IPjT)CX3匕[2	(118)
=Tr [σι,o,…,0 ∙ CXj-iρjT)Cx" 2	(119)
=Tr]σι,o,…，0 ∙ PjT)[ 2,	(120)
where Eq. (118) is based on the definition of P(j) in Eq. (106), Eq. (119) is based on Lemma C.3,
and Eq. (120) is based on Lemma C.1.
□
32