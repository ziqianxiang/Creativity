Under review as a conference paper at ICLR 2021
A Theory of Self-Supervised Framework for
Few- S hot Learning
Anonymous authors
Paper under double-blind review
Ab stract
Recently, self-supervised learning (SSL) algorithms have been applied to Few-
shot learning(FSL). FSL aims at distilling transferable knowledge on existing
classes with large-scale labeled data to cope with novel classes for which only
a few labeled data are available. Due to the limited number of novel classes, the
initial embedding network becomes an essential component and can largely affect
the performance in practice. But almost no one analyzes why a pre-trained embed-
ding network with self-supervised training can provide representation for down-
stream FSL tasks in theory. In this paper, we first summarized the supervised FSL
methods and explained why SSL is suitable for FSL. Then we further analyzed the
main difference between supervised training and self-supervised training on FSL
and obtained the bound for the gap between self-supervised loss and supervised
loss. Finally, we proposed potential ways to improve the test accuracy under the
setting of self-supervised FSL.
1	Introduction
Recently, the self-supervised learning (SSL) algorithms have been applied to the FSL. The purpose
of FSL is to extract transferable knowledge from existing classes with large-scale label data to deal
with novel classes with only a few labeled data. The initial embedded network becomes an essential
component and will greatly affect performance because of the limited number of novel classes. In
practice, SSL greatly enhances the generalization of FSL method and increases the potential for
industrial application. Once combining SSL and FSL, we only need to collect a large amount of
related unlabeled data and a few data on the new task to obtain a model with good generalization
performance on the new task. In theory, it is difficult to analyze the performance of self-supervised
pre-trained models on multiple downstream tasks. Because the downstream task itself involves
a large amount of data with different data distribution from the primary training data distribution,
such as multi-view SSL. Besides, downstream tasks and self-supervised tasks may be quite different,
such as classification and segmentation, which further increases the difficulty of theoretical analysis.
However, when back to the purpose of SSL, which is to learn a good pretrain model that can be
transferred to different tasks, we find FSL also focus the same purpose to get an initialization model
that can achieve good results with a few data on a new task by a simple classifier (such as a mean
classifier). Thus, FSL tasks is suitable for evaluation of the effect of SSL.
The main research direction about when and why self-supervised methods improves FSL is to com-
pare the performance of different self-supervised methods through experiments. Almost no one
analyzes why a pre-trained embedded network with self-supervised training can provide a repre-
sentation for downstream FSL tasks in theory. We believe that theoretical analysis is necessary.
For example, MoCo uses momentum update to greatly expand the size of the key-value dictionary,
thereby improving the effect. But we don’t know why the key-value dictionary needs to be large
enough. Is the batch size really the bigger the better? SimCLR proposes a head layer to calculate
the contrastive loss, instead of directly comparing the representations. Why is this method effec-
tive? We find that although self-supervised learning researchers have made great progress, analysis
about why SSL works is halted at experimental and empirical conclusions due to the lack of the-
orical analysis. Therefore, we think it is necessary and useful to analyze self-supervised learning
theoretically.
1
Under review as a conference paper at ICLR 2021
We analyze the self-supervised training process via the specific application scenario of FSL. Under
this settings, we avoid the complexity of downstream tasks, and can directly judge the quality of
self-supervised learning by the performance of new few-shot tasks. Our main intuition is to quan-
tify the gap between self-supervised learning and supervised training on FSL tasks by constructing
supervised metrics corresponding to self-supervised tasks. We find that the self-supervised training
loss is actually an upper bound of the supervised metric loss function (Theorem 1). It means that if
we can reduce the self-supervision loss small enough, we can control the model’s supervision loss
on the training data. And because FSL methods has good generalization on similar downstream
tasks, we conclude that self-supervised training can also have good generalization on similar tasks,
even if the categories of training tasks and test tasks are different.
Unfortunately, it is often difficult to minimize the training loss of self-supervision. Contrastive-based
SSL method samples different augment data as query and positive data, and others as negative data.
Those false negative data have the same class as query. This part of training loss introduced by the
false negative data limits our performance. We separate the negative samples in the self-supervised
training into true negative samples and false negative samples. For true negative samples, we assume
that loss can be small enough by suitable models and optimizers. As for false negative samples, we
bound this loss by the intra-class deviation. This part is also the difference between self-supervised
learning and supervised learning (Theorem 2). We should control the intra-class deviation of these
false negative samples while training according to Theorem 2.
Finally, we discuss potential ways to improve test accuracy under self-supervised FSL settings. First,
we suggest that the larger the batch size is, the better, but within a certain range. Second, increasing
the number of support samples is beneficial to reducing the within-class variance of false negative
samples for good test performance. Technically, we set the different augmented data as the support
samples from the same class. Third, we need to choose unsupervised training data whose number
of categories are large, because large categories will reduce the probability of us sampling false
negative samples. We also introduce the limitations of our theory.
Ideally, one would like to know whether a simple contrastive self-supervised framwork can give
representations that competable with those learned by supervised methods. We show that under
the two assumptions, one can get a test performance close to supervised training. Experiments on
Omniglot also support our theoretical analysis. For instance, the self-supervised framework reaches
the accuracy of 98.23% for 5-way 5-shot classification on Omniglot, which is quite competitive
compared to 98.83% achieved by supervised MAML.
2	Preliminary knowledge and Assumption
2.1	Summary of supervised FSL methods
In the typical few-shot scenario introduced by Vinyals et al. (2016), the model is presented with
episodes composed of a support set and a query set. The support set contains concepts about the
categories into which we want to classify the queries. In fact, models are usually given five categories
(5-way), and one (one-shot) or five (five-shot) images per category. During training, the model is
fed with these episodes and it has to learn to correctly label the query set given the support set.
The category sets seen during training, validation, and testing, are all disjoint. This way we know
for sure that the model is learning to adapt to any data and not just memorizing samples from the
training set. Although most algorithms use episodes, different algorithm families differ in how to
use these episodes to train the model. Recently, transfer learning approaches have become the new
state-of-the-art for few-shot classifications. Methods like Gidaris & Komodakis (2018), pre-train a
feature extractor and linear classifier in a first stage, and remove its last FC layer, then fix the feature
extractor and train a new linear classifier on new samples in the fine-tuning stage. Due to its success
and simplicity, transfer learning approaches have been named “Baseline” on two recent papers Chen
et al. (2019); Dhillon et al. (2019).
We mark the feature extractor in the first stage as fq, and the linear classifier of Chen et al. (2019)
on new samples in the fine-tuning stage as y = fq(x)TW, W = [w1, w2, . . . , wc] ∈ Rd×c. The
classifier in Chen et al. (2020b) is a mean classifier with the weight as the centroid of features
2
Under review as a conference paper at ICLR 2021
from the same class. Let Sc denote the few-shot support samples in class c, then they have wc =
∣S1j Pχ∈sc fq(x). In this paper, we take a generalized mean classifier with Wc =南 Pχ∈s° fk (x).
Because of the arbitrariness and complexity of fk, this generalized mean classifier is nearly an
arbitrary linear classifier.
2.2	Assumptions in self-supervised FSL
Assumption 1 Mean classifier is good enough for evaluation.
We usually analyze the effectiveness of self-supervised with the results on downstream tasks Liu
et al. (2020); Jing & Tian (2020). But another different downstream task usually makes the theoret-
ical analysis more difficult. And a complex classifier usually performs better than mean classifier
on most of tasks Liu et al. (2020); Jing & Tian (2020), i.e., the classification on ImageNet. There-
fore, we consider a simple way to analyze self-supervised learning. We assume that only the mean
classifier is used for classification during the test phase. When the fk in mean classifier is complex
enough, it is equivalent to linear classifier which used in Baseline++. Even if fk is simply the same
as fq, this mean classifier is consistent with the classifier in metric-based FSL methods, such as
ProtoNets Snell et al. (2017). The mean classifier is suitable for self-supervised FSL because of the
the good performance, simple form, convenient measurement and wide usability.
We assume that the mean classifier performs well enough during testing if decreasing supervised
training loss enough. Our hypothesis has been verified on Baseline++ Chen et al. (2019), Pro-
toNet Snell et al. (2017) and some other transfer-based FSL methods Dhillon et al. (2019); Li et al.
(2019a); Hu et al. (2020). Our paper mainly focuses on analyzing the difference between supervised
training loss and self-supervised training loss on the feature extractor fq and the mean classifier
based on fk, regardless of how to generalize from the training set to the test set. We just make an
assumption that generalization remains to work based on existing supervised FSL methods.
Assumption 2 The training data is balanced in different classes.
The main work of this paper is to analyze self-supervised loss. We assume that each training sample
is drawn from a certain data distribution. We regard the sampling process as two steps, first randomly
sample the categories, and then select samples from these categories for training. In order to analyze
the difference between the self-supervised loss and the supervision loss, we mark these samples
with their labels in supervised dataset, and then selecte one sample for each class to construct a
dynamic N -way 1-shot supervised training task. When assumption 2 is satisfied, that is, the number
of samples in each class is the same, our theoretical results are relatively simple and clear.
In practice, the assumption is also acceptable because we can collect data by specifying some key-
words to ensure that the data is roughly balanced. Please note that whether this assumption is nec-
essary is still worth further analysis. There may be different annotations in different self supervised
pretext tasks. Especially in the FSL, the categories in the training set and the test set do not overlap,
so the assumption of balanced data is more likely to be removed. This paper assume this assumption
is true and do not further analysis the necessity.
The above two assumptions are the basis of our subsequent analysis. Our theoretical analysis is
also applicable to other scenarios that satisfied the two assumptions, not just FSL. We chose self-
supervised FSL in our experiment because these two assumptions have been adopted in FSL.
3	Theories for self-supervised loss and supervised loss
3.1	Contrastive self-supervised training framework
Given an unlabeled dataset A, self-supervised training method, like MoCo He et al. (2019) creates
many synthetic query-key matching tasks on-the-fly by randomly sampling NK data at a time and
then augmenting them. A basic consideration is that two synthetic data, x+ = Aug(x) and xq =
Aug0(x), who are augmented from the same ancestor x, hold the same class label. In this case, one
of the NK data is randomly selected to be the positive data, and its two augmented data are taken
as the query and the positive samples, respectively, while the synthetic data augmented from the
remainder NK - 1 data are treated as negative samples. After that, the query encoder fq maps the
3
Under review as a conference paper at ICLR 2021
query into q = fq(xq), and the key encoder fk maps the positive sample into k+ = fk (x+), and
the negative samples into ki- = fk(xi-), i = 1, . . . , NK - 1. These output vectors q, k+, ki- are
normalized by its L2-norm, followed by a metric loss:
NK-1
L = log(1+ X exp(μqτk- - μqTk+)),	(1)
i=1
where μ is a learnable metric scaling scalar Oreshkin et al. (2018) in the hope of facilitating metric
training. As a note, in our experiments, the metric loss L is evaluated based on multiple query data
in a mini-batch manner. Please refer to Appendix C for more details.
In our paper, we assume that the queries from the same class have the same distribution, and the
keys from the same class have the same distribution. The query and keys depend on the specific
self-supervised pretext task. The input xq and xk can be images Hadsell et al. (2006); Wu et al.
(2018); Ye et al. (2019), or context consisting a set of patches Oord et al. (2018) [44]. The networks
fq and fk can be identical Hadsell et al. (2006); Ye et al. (2019); Chen et al. (2020a), partially
shared Oord et al. (2018); Hjelm et al. (2018); Rezaabad & Vishwanath (2019), or different Tian
et al. (2019).
3.2	Supervised and Self-Supervised Training Loss.
Assume the dataset A to be supervised, the same framework can be trained in a supervised few-
shot learning manner using this A. We will show that the self-supervised loss is an upper bound for
supervised evaluation metric, and prove that minimizing unsupervised loss makes sense in Section 4.
Self-Supervised Metric (SSM) for Representations SSM accesses to a flow of unsupervised tasks
Tt which contain augumented data {xq, x+, x1- , . . . ,x-N -1}. We mark their ground-truth labels
by CU={cq, c+, c1-, . . . ,c-N -1}, respectively. Note that xq and x+ are drawn from the same data
distribution Dc+ (since cq=c+) while negative xi- are from Dc- . Let I={1, . . . , NK -1} be the set
of indices of negative data, the unsupervised loss in Eq. (1) can be rewritten as
LU = E log(1 + Xexp(μqTk- - μqτk+)).	(2)
q,k+,ki-	i∈I
Supervised Metric for Representations. The quality of the representation function fq , fk is evalu-
ated by its performance on a multi-class classification task using linear classification. Let C denote
the set of class label with prior distribution ρ. Assume that the augumented data x ∈ X is drawn
from the data distribution Dc, where C 〜ρ. Now considering one N-way task TSup on N different
classes Csup={c1, . . . ,cN}, its multi-class classifier is denoted as the funtion g : X → RN. The
softmax-based cross-entropy loss on data pair (x, y) can be rewritten as
Lsup (g, x, y) = log(1 +	exp(g(x)y0 - g(x)y)),	(3)
y0 6=y
where g(x)y is the y-th element of the vector g(x). It is a general intuition that the data pair (x, y)
with right label y has greater confidence than the data pair (x, y0) with wrong label y0.
We choose the linear classifier as mean classifier g(χ)c=μqτPc, where μ is a scaling scalar and
q=fq(x) is the query representation, and Pc=Ex〜dc [fk (x)] is the mean of representation of inputs
with label c. That is, fq is for the feature extraction, and pc is the weight of linear classifier. Then,
the expected supervised metric loss in terms of fq, fk on N -way tasks is
LSUP =	EC log(i + V eχp(μqτPco - μqTPc)).	(4)
C 〜ρ,x 〜Dc	/ "
c0 6=c
Our goal is to find a unsupervised training method to decrease the value of the evaluation metric
Lsup. Previous supervised FSL methods deal with the evaluation metric in different ways. Pro-
toNets conducts an episode training by using the evaluation metric as supervised loss function but
replacing scaled dot products with Euclidean distance. Chen Chen et al. (2019) shows that few-shot
algorithms, such as MatchNets, ProtoNets, RelationNets, and MAML are based on episode train-
ing on the variants of Lsup. Recent researchs Dhillon et al. (2019); Chen et al. (2019; 2020b) have
4
Under review as a conference paper at ICLR 2021
found the basic episode training on Lsup with proper pretrain outperforms most of methods. The
core of FSL is to generate a flow of supervised N -way M -shot tasks, and training with viriants of
Lsup on each task. For the generality and flexibility of our theories, we will prove that contrastive
self-supervised training methods with loss function Eq. 2 is essentially reducing Lsup on training
data. We analyze the evaluation metric with a class-wise prototype pc rather than a episodic mean
of support samples. It is proved to be a better choice of loss function due to explicitly reducing
intra-class variations Chen et al. (2019).
4 Gaps between supervised and self-supervised training loss
It can be seen that Eq. (2) has a similar form with Eq. (4). As a first step, we show that LU bounds the
Lsup in an ideal situation. This conclusion indicates that it makes sense to minimize the unsupervised
SSM loss LU. We represent the upper bound for the supervised loss Lsup by Theorem 1.
Theorem 1	∀fq , fk ∈ F,
Lsup ≤ γ0 LU + δ,	(5)
where γ0, δ are constants depending on the class distribution ρ. When ρ is uniform and |C | → ∞,
then γ0 → 1, δ → 0.
Proof. The key point for proof is the use of Jensen’s inequality since `(v) = log(1 +
Pi exp(vi)), ∀v ∈RNK -1 is a convex function (vi is the i-th element of v), that is,
LU = E E log(1 + Pi∈ι exp(μqTk- - μqTk+))
q k+ ,ki-
≥ E - log(1 + Pi∈ι exp(μqτPc- - μqTPc+)).
q,c+ ,ci
(6)
However, unsupervised data label CU may contain duplicate classes and even false negative classes.
Clearly, we divide I into two disjoint subsets, true negative index set I-={i ∈ I |ci- 6=c+} and false
negative index set I+={i ∈ I|ci-=c+}. We define Cuni as the label set after de-duplicating class
labels in Cu, Cuni⊆Cu. Since '({vi}i∈i1∪i2)=log(l+Pi∈ι1∪ι2 eχp(vi))≥'({vi}ij)，∀ I1,I2⊆I, We
could decompose Eq. (6) into
E log(1 + Pi∈ι exp(μqτPc- - μqTPc+))
q,c+ ,ci-	i
≥ P(I+ = O) E ['({μqτPc - μqτPc+ }氏九\。+) |I+ = 0]
q,c+
+P(I+ 6=0)E [log(1 + |I+|)|I+ 6= 0].
c+
(7)
The first expectation in Eq. (7) is actually the supervised loss Lsup in Eq. (4) by regarding Csup:=Cuni.
Combining this result with Eq. (6) (7), We obtain the inequality in Theorem 1 with γo = P(/+=。),
δ = - pg：*) E [log(1 + ∣I+∣)∣I+ = 0]. When ρ is uniform and |C| → ∞, then P (I+ = 0) → 0.
Please refer to Appendix B.1 for more details.
The supervised loss is built from unsupervised training. We consider the following tweak in the way:
sample the class set {c+, c1-, . . . , c-N -1} from the class distribution ρ, and random select one class
as positive class, and consider the episode that all negative classes are different from the positive
class (I+ = 0), then sample one data from one negative class independently to get |Csup|-way 1-shot
supervised loss. Notice that this loss is about separating c+ from the total class set, we calculate the
probability by symmetrization under the assumption that training dataset are balanced. We derive
the above encouraging result about the relationship between SSM loss and supervised loss. Thus,
we can decrease the supervised loss by minimizing the unsupervised SSM loss LU . However, LU
can not be small enough if false negative keys present in some tasks and the proportion of false
negative data is considerable, especially for the task scenarios whose label space |C | is small (e.g.,
miniImageNet). In that case, minimizing LU will meet a theoretical bottleneck since the loss on the
false negative data can not be decreased enough.
To explore the effect of some inputs of the same class as positive data sneaking into the negative
data, we further decompose the SSM loss LU into two terms: (1) LU-, the loss on all true negative
data xi- , i ∈ I- . These data has the different labels from the positive data. (2) LU+, the loss on all
5
Under review as a conference paper at ICLR 2021
false negative data xi-, i ∈ I+. These data have the same label as the positive data. Theorem 2 does
reveal the underlying factors behind the explicit gap between Lsup and LU. We define a notation of
intra-Class deviation as s(fk) := ∣μ∣Ec[Eχ∈Dckfk(x) - Pck2]1/2, and ShoW that s(fk) can bound
LU+ . Then, we get a new narrow bound by Theorem 2.
Theorem 2	∀fq , fk ∈ F,
Lsup ≤ γ0LU- + γ1s(fk),	(8)
where γo, γι are constants depending on the class distribution P. When P is uniform and ∣C∣→∞,
then γ0 → 1, γ1 → 0.
Proof. The key point for proof is that '({vi}i∈i1∪i2) ≤ '({vi}i∈iι)+ '({vi}i∈i2 ),∀I1,I2 ⊆ I,
LU ≤	E _ log(1 + Pi∈ι- exp(μqτk- - μqTk+))
q,k+ ,ki-
+ E _ iog(i + Pi∈ι+eχp(μqTk- - μqTk+))	⑼
q,k+ ,ki-
:= LU- +LU+,
Where the first expectation is LU- and the second one is LU+ (imagining that true
negative data and false negative data have been separated during training). With
the following inequalities '({vi}i∈iι)	≤ log(1 + |Ii|) + max{max{Vi}i∈iι, 0}, and
max{vi}i∈iι ≤∣ max{vi}i∈1] ∣≤max{∣v∕}i∈iι ≤Pi∈h ∣Vi∣, We can get the following inequality:
L+ ≤	E	[iog(i + |I+1) + Pi∈ι+(∣μqTk- - μqτk+l)]
q,k+ ,ki-
=P (I + = 0) EJlog(1 + |I+ |)|I + = 0]	(10)
+ E - [Pi∈ι+ |〃qTk- -μqτk+|].
q,k+ ,ki-
By combining Eq. (5) with Eq. (9) and Eq. (10), we get
Lsup ≤γ0(LU-+LU+)+δ
≤ γoL- + γo E	[pi∈ι+ lμqτk- - μqτk+ |]	.11.
q,k+,ki-	(11)
=YoL- + Yo E E	[Pi∈ι+|〃qTk- - μqτk+l].
c+ q,k+,k-〜c+
When the class distribution P is uniform, we have E|I+ | = (NK - 1)/|C| for any class. Considering
that the representations are normalized to satisfy kqk2 = 1, thus the right expectation in Eq. (11)
can be bound by s(fk), that is,
E E [X ∣μqτk- - μqτk+∣] ≤ √2e∣I +∣∙ s(fk).	(12)
c+ q,k+,k-〜c+ i∈ι+
Combining Eq. (11) and Eq. (12), Theorem 2 can be proved, where γι = √2γ0∣NKT), γo =
P(i +=0). When P is uniform and ∣C∣→∞, then P (I+ = 0) → 0, γo → 1, γι → 0. Please refer to
Appendix B.2 for more details.
Compared to Theorem 1, Theorem 2 indicates that the supervised loss Lsup is bounded by two
explicit parts. The first one is LU- , which measures the similarity between the query data with the
positive data and true negative data. Itis somehow like dynamic N -way M -shot supervised training.
The difference is that the value of N and M might change along with the number of true negative
data. The second is s(fk), which acts as the penalty for representation ability by measuring the intra-
class representation deviation. Moreover, the γ1s(fk) is an explicit gap distancing unsupervised
SSM loss from supervised loss. Theoretically, if given an unsupervised set with infinite classes and
data, the performance achieved by SSM can be very close to that by supervised training.
5	Discussion on self-supervised FSL
Impact of the value N, M in dynamic N -way M -shot. In Section 4, we regard LU- as dynamic
N -way M -shot supervised training. It depends on the sampled self-supervised true negative training
6
Under review as a conference paper at ICLR 2021
NK	128	512	2048	8192
Omniglot	90.587±0.006	-^91.475±0.004^^	92.013 ± 0.005	92.001±0.008
miniImageNet	38.964±0.012	41.783 ± 0.007	40.275±0.008	39.654±0.007
Table 1: Accuracy (%) averaged over 1000 random 5-way 1-shot test tasks.
M	1	2	3	4
Omniglot	98.091±0.007^^98.163±0.005^^98.234 ±0.006^^98.258±0.005
mini ImageNet 59.128±0.009 60.016±0.008 60.118±0.006 60.121±0.007
Table 2: Accuracy (%) averaged over 1000 random 5-way 5-shot test tasks.
data. In supervised FSL methods, the performance increases as the N or M increases. So how to
increase N andM in self supervised training, and what is the improvement? We can increase N by
increasing the total negative samples NK. But γ1, the coefficient of the gap, also increases. Thus we
suggest that in the self-supervised FSL, we do not need to use a particularly large NK . Experiments
in Table 1 on Omniglot and miniImageNet show that the best NK is 2048, 512, respectively.
Most of self-supervised methods take one positive key for one query while FSL usually set M
support samples as positive keys. ProtoCLR Medina et al. (2020) proposed that original images x
serve as class prototypes around which their Q augmentations should cluster. And Q = 3 showed
the best performance in their experiments. Supposed that we replace each key representations with
the average of M different key representations from different augmentations of the same input x.
The new intra-class deviation s(fk) := s(∕k)∕√M reduces the gap between self-supervised and
supervised training.
Impact of the class number |C|. The class number affects the coefficient of gap. Assuming that
the class number is infinite, then the probability of sampling false negative data is zero, thus we
have γ0 → 1, γ1 → 0. Concretely, the unsupervised A of miniImageNet only contains 64 classes
while that of Omniglot involves up to 4800 classes, which leads to a smaller γ1 for Omniglot (larger
|C| implies larger P (I+ = 0), larger P (I+ = 0) implies smaller γo, larger |C| + smaller γo imply
smaller γ1). Assume that there is no significant difference about the representation ability of the
backbone on two datasets (even if the intra-class deviation on Omniglot is intuitively smaller than
that on miniImageNet since Omniglot is more easy), the gap γ1s(fk) on Omniglot is smaller than
that on miniImageNet. Because the more categories, the more information introduced, it is difficult
to design a fair comparative experiment. Still, we believe self-supervised methods is suitable for
large number of categories according to the theory.
Impact of the imbalance data. We assume training data is balanced to get simple and clear coeffi-
cient γ0, γ1 in the gap. From Eq. 12, the intra-class deviations of different classes are weighted by
the expected ratio of false negative samples in all negative samples. Moreover, The generalization
performance may also be affected by unbalanced data training. Because the weight of each sample
in the supervision loss varies in different categories.
Limitations of the theories. (1) For simplicity, we only consider the mean classifier, and the mean
classifier in the supervised framework may become a bottleneck. Not all FSL methods take the
mean classifier or linear classifier and training the model in two stages. We do not discuss the
relationship between different FSL methods and the supervision loss in our paper, which may limit
the theory generalizing to other FSL methods. (2) We have not carried out abundant experiments
to verify the theory. There are still many obstacles in guiding experiments with theory. We think
that collecting large categories of unsupervised data, increasing the number of negative samples in
a certain extent, and using multiple augmentations to replace the original keys, will help to reduce
the gap between supervision and self-supervised training. (3) The supervised loss is actually an
evaluation metric, slightly different from the training loss used in FSL methods. Therefore, the
convergence and generalization of the framework need to be further verified.
7
Under review as a conference paper at ICLR 2021
6	Related Work
Few-Shot Learning Compared to the common machine learning paradigm that involves large-scale
labeled training data, the development of FSL is tardy due to its intrinsic difficulty. Early works
for FSL were based on generative models that sought to build a Bayesian probabilistic frame-
work Fei-Fei et al. (2006); Lake et al. (2015). Recently, more works were from the view of
meta-learning, which can be roughly summarized into five sub-categories: learn-to-measure (e.g.,
MatchNets Vinyals et al. (2016), ProtoNets Snell et al. (2017), RelationNets Yang et al. (2018)),
learn-to-finetune (e.g., Meta-Learner LSTM Ravi & Larochelle (2017), MAML Finn et al. (2017)),
learn-to-remember (e.g., MANN Santoro et al. (2016), SNAIL Mishra et al. (2018)), learn-to-adjust
(e.g., MetaNets Munkhdalai & Yu (2017), CSN Munkhdalai et al. (2018)) and learn-to-parameterize
(e.g., DynamicNets Gidaris & Komodakis (2018), Acts2Params Qiao et al. (2018)). These methods
consistently need to create meta-training tasks on a large-scale supervised auxiliary set to obtain an
FSL model that can be transferred across tasks.
Contrastive self-surpervised representation learning Our analysis is based on contrastive repre-
sentation learning. It is a classic machine learning topic Barlow (1989) that aims to acquire a pre-
trained representation space from unsupervised data and works as a pre-bedding for downstream
supervised learning tasks. It can be traced back to Hadsell et al. (2006), these methods learn rep-
resentation by comparing positive and negative pairs. Wu et al. (2018) suggests using a repository
to store instance class representation vectors, which is a method adopted and extended in recent
papers. Other studies have explored the use of intra batch samples instead of memory for negative
sampling Ji et al. (2019); He et al. (2019). Recent literature attempts to link the success of their
methods with the maximization of mutual information between potential representations Oord et al.
(2018); Henaff et al. (2019). However, it is not clear whether the success of methods is determined
by mutual information or by the specific form of sustained loss Tschannen et al. (2019).
Self-Supervised Methods for FSL CACTUs Hsu et al. (2019) first developed a two-stage strategy:
constructing meta-training tasks on an unsupervised set by clustering algorithms and then running
MAML or ProtoNets on the constructed tasks. Comparably, UMTRA Khodadadeh et al. (2019),
AAL Antoniou & Storkey (2019), and ULDA Qin et al. (2020) proposed to construct meta-training
tasks by augmenting the unsupervised data and training by the ready-made ProtoNet or MAML
model. These unsupervised methods for FSL focused on allocating pseudo labels to unsupervised
data. Then the existing supervised FSL models can work without modification with these pseudo
labels. Three latest work, LST Li et al. (2019b), ProtoTransfer Medina et al. (2020) and CC Gidaris
et al. (2019), introduce self-supervised techniques Jing & Tian (2020) to achieve superior perfor-
mance on FSL tasks. LST is a semi-supervised meta-learning method that meta learns how to pick
and label unsupervised data to improve FSL performance further. ProtoTransfer is state-of-the-art
self-supervised FSL methods. CC considers two self-supervised methods in the present FSL work:
CC-Rot, predicting the rotation incurred by an image Gidaris et al. (2018), and CC-Loc, predicting
the relative location of two patches from the same image Doersch et al. (2015). CC can deal with un-
supervised tasks while LTS not. In this work, We reconsidered FSL theoretically and experimentally
from the perspective of self-supervised learning.
7	Conclusion
In this work, we focus on theories of self-supervised FSL. We first give two assumptions and ex-
plain why self-supervised methods are suitable for few-shot learning. Then we decompose the self-
supervised loss into supervised loss and a gap bounded by the intra-class representation deviation.
Experimental results on two FSL benchmarks Omniglot and miniImageNet verify our theories and
we propose potential ways to improve test performance.
Acknowledgments
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper.
8
Under review as a conference paper at ICLR 2021
References
Antreas Antoniou and Amos Storkey. Assume, augment and learn: Unsupervised few-shot meta-
learning via random labels and data augmentation. arXiv preprint arXiv:1902.09884, 2019.
Horace B Barlow. UnsUPervised learning. Neural computation, 1(3):295-311, 1989.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visUal rePresentations. 2020a.
Wei-YU Chen, Yen-Cheng LiU, Zsolt Kira, YU-Chiang Frank Wang, and Jia-Bin HUang. A closer
look at few-shot classification. arXiv preprint arXiv:1904.04232, 2019.
Yinbo Chen, Xiaolong Wang, ZhUang LiU, HUijUan XU, and Trevor Darrell. A new meta-baseline
for few-shot learning. arXiv preprint arXiv:2003.04390, 2020b.
Ekin D CUbUk, Barret ZoPh, Dandelion Mane, Vijay VasUdevan, and QUoc V Le. AUtoaUgment:
Learning aUgmentation Policies from data. arXiv preprint arXiv:1805.09501, 2018.
GUneet S Dhillon, Pratik ChaUdhari, Avinash Ravichandran, and Stefano Soatto. A baseline for
few-shot image classification. arXiv preprint arXiv:1909.02729, 2019.
Carl Doersch, Abhinav GUPta, and Alexei A Efros. UnsUPervised visUal rePresentation learning by
context Prediction. In Proceedings of the IEEE International Conference on Computer Vision, PP.
1422-1430, 2015.
Li Fei-Fei, Rob FergUs, and Pietro Perona. One-shot learning of object categories. TPAMI, 28(4):
594-611, 2006.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaPtation
of deeP networks. In ICML, 2017.
SPyros Gidaris and Nikos Komodakis. Dynamic few-shot visUal learning withoUt forgetting. In
CVPR, 2018.
SPyros Gidaris, Praveer Singh, and Nikos Komodakis. UnsUPervised rePresentation learning by
Predicting image rotations. In ICLR 2018, 2018.
Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Perez, and Matthieu Cord. Boosting few-
shot visUal learning with self-sUPervision. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 8059-8068, 2019.
R. Hadsell, S. Chopra, and Y. Lecun. Dimensionality reduction by learning an invariant map-
ping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’06), 2006.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. 2018.
Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. In ICLR,
2019.
Yuqing Hu, Vincent Gripon, and Stephane Pateux. Leveraging the feature distribution in transfer-
based few-shot learning. arXiv preprint arXiv:2006.03806, 2020.
Olivier J Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,
and Aaron Van Den Oord. Data-efficient image recognition with contrastive predictive coding.
2019.
Zilong Ji, Xiaolong Zou, Tiejun Huang, and Si Wu. Unsupervised few-shot learning via self-
supervised training. arXiv preprint arXiv:1912.12178, 2019.
9
Under review as a conference paper at ICLR 2021
Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks:
A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Siavash Khodadadeh, Ladislau Boloni, and Mubarak Shah. Unsupervised meta-learning for few-
shot image classification. In NeurIPS, 2019.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Aoxue Li, Tiange Luo, Tao Xiang, Weiran Huang, and Liwei Wang. Few-shot learning with global
class representations. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 9715-9724, 2019a.
Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele.
Learning to self-train for semi-supervised few-shot classification. In Advances in Neural Infor-
mation Processing Systems, pp. 10276-10286, 2019b.
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. Self-
supervised learning: Generative or contrastive. arXiv preprint arXiv:2006.08218, 2020.
Carlos Medina, Arnout Devos, and Matthias Grossglauser. Self-supervised prototypical transfer
learning for few-shot classification. arXiv preprint arXiv:2006.11325, 2020.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In ICLR, 2018.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In ICML, 2017.
Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with
conditionally shifted neurons. In ICML, 2018.
Aaron Van Den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. 2018.
Boris Oreshkin, PaU Rodrlguez Lopez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In NeurIPS, 2018.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting
parameters from activations. In CVPR, 2018.
Tiexin Qin, Wenbin Li, Yinghuan Shi, and Yang Gao. Unsupervised few-shot learning via distribu-
tion shift-based augmentation. arXiv preprint arXiv:2004.05805, 2020.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.
Ali Lotfi Rezaabad and Sriram Vishwanath. Learning representations by maximizing mutual infor-
mation in variational autoencoders. 2019.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein. Imagenet large scale visual
recognition challenge. IJCV, 115(3):211-252, 2015.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In ICML, 2016.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
NIPS, 2017.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. 2019.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In NIPS, 2016.
10
Under review as a conference paper at ICLR 2021
Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance-level discrimination. In 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.
Flood Sung Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In CVPR, 2018.
Mang Ye, Xu Zhang, Pong C. Yuen, and Shih Fu Chang. Unsupervised embedding learning via
invariant and spreading instance feature. 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
B Proofs.
Assume that we have access to an episodic unsupervised task Tt with augumented data X =
{xq, x+, x1-, . . . ,xN- -1}. Please refer to Algorithm 1 for details in our paper. We mark their
ground-truth labels by CU = {cq, c+, c1- , . . . ,c-N -1}, respectively. Note that xq, x+ are drawn
from the same data distribution Dc+ (since cq = c+) while negative xi- are drawn from the data
distributions Dc- . Let I = {1, . . . , NK - 1} be the set of indices of negative data, the unsuper-
vised loss function is as shown in Eq. 2. where q, k+, ki- are the representations of data xq, x+, xi-,
respectively.
B.1 Proof of Theorem 1.
We first leverage the convexity of ` to get a lower bound of unsupervised loss LU in Eq. (16). Then
we decompose the lower bound into a supervised loss Lsup plus a degenerate term in Eq. (20).
Step 1. Convexity. `(v) = log(1 + Pi evi ), ∀v ∈ RNK -1 is a convex function. Because, ∀t ∈
R, z, v ∈ RNK-1,
g(t) = `(z +tv) = log(1 +	i ezi +tvi)
g0(t)
Pi Viezi+ tvi
g00(t)
1+Pi ezi + tvi
(Pi v2ezi + tvi )(1+Pi ezi+tvi )-(Pi Viezi+tvi )2
(1+Pi ezi +tvi )2
(13)
where zi , vi are the i-th component of vector z , v . And We have,
(14)
and Cauchy inequality,
vi2ezi+tvi ≥ 0,
i
(15)
2
Thus, g00 (t) are always non-negative. `(v) is a convex function.
Step 2. Jensen’s inequality. The key point in the proof is the use of Jensen’s inequality since
`(v) = log(1 + Pi exp(vi)), ∀v ∈ RNK -1 is a convex function.
LU =	E log(1 + P exp(μqTk- - μqTk+))
q,k+ ,ki-	i∈I
= E	E _ _ log(1 + P exp(μqTk- - μqTk+))
q,c+,c- k +〜c+ ,k-〜C-	i∈I
≥ E _ log(1 + P exp( E_ ∕μqτk- - μqTk+)))
q,c+,c-	i∈I	k + ~c+,k-~c-
= E _ log(1 + P exp(μqrTPc- - μqTPc+)),
q,c+ ,ci-	i∈I	i
(16)
where pc is a class-wise prototype, which is the mean of all representations with the same class
label c. That is, pc- is the prototype of class ci- and pc+ is that of class c+ . There are some subtle
ci
differences between our class-wise prototype and the episodic prototype in ProteNets. Our pc is a
global prototype for each class. But pc in ProteNets is re-calculated by support data in each episode.
Note that in the lower bound quantity of unsupervised loss, the random class labels ci- may be true
negative classes or false negative classes.
Step 3. Decompose the lower bound. We devide all negative classes ci- set into two disjoint subsets,
true negative classes and false negative classes. Clearly, we divide the set of negative data indices
I into two unjoint subsets: true negative indices I- = {i ∈ I|ci- 6= c+} and false negative indices
12
Under review as a conference paper at ICLR 2021
I+ = {i ∈ I∣c- = c+}. We have,
E log(1 + P exp(μqTPC- - μqτPc+))
q,c+,ci	i∈ι	i
= E ,({μqrTPC- - μqτPc+ }i∈l)
q,c+,ci
=P (I+ = 0) E _ h'({μqτPc- - μqτPc+ }i∈i )∣I+ =。]
q,c+,ci L	i	」
+P(I+ = 0) E - p({μqTPC- - μqτPc+ }i∈ι)|I+ = 0] ∙
q,c+,ci L	i	」
(17)
We define CUni as the label set after de-duplicating class labels in CU (class labels including positve
class c+ and negative classes c-). Since we have '({vi}i∈ι1ui2) ：= log(1 + Pi∈jιuj2 exp(vj) ≥
'({vi}i∈∕1), V/1,/2 ⊆ I, we can decompose the above quantity to handle repeated classes.
If I+ = 0, then I = I-, we can choose all de-duplicating negative class indices as 4出.Thus
Iuni ⊆ I- = I, and '({vi}i∈ι) ≥ '(他山口山).Thatis,
E _ p({4qTpC— - μqTPc+ }i∈ι)lʃ+ = 0]
q,c+,c- L	i	」
≥ E _ ∣^'({4qTPC— — μqτPc+ 卜6加)|1+ = 0i
q,c+,ci L	i	」
=EJ'({4qτPc — μqτPc+ }Ο2讳\。+ )|I+ = 0]∙
q,c+
(18)
Observe that the last expectation in the Eq. (18) is actually the supervised loss LSUP by regarding
CSUP ：= Cuni. The supervised loss is based on the positive class and de-duplicating negative classes
of each episodic data in our SSM. It is somehow like dynamic N-way 1-shot supervised loss and N
is the number of unique classes of sampled classes CU.
If I+ = 0, we can choose all false negative class indices I+. All these false negative data have
the same class label c+. Thus, C- = c +,Pc— = Pc+, Vi ∈ I+. Since I+ ⊆ I, and '({vi}i∈ι) ≥
ci
'({vi}i∈ι+). That is,
E _ p({μqTPC- - μqτPc+ }i∈ιXi+ = 0]
q,c+,ci L	i	」
≥ E i p({μqTPC- - μqτPc+ }i∈ι+)|I + = 0]
q,c+,c- L	i	」
=E -['({0}i∈ι+ )∣I+ = 0]
q,c+ ,ci
= E [log(1 + ∣I +∣)∣I+ = 0]
q,c+ ,ci
=E [log(1 + ∣I +∣)∣I+ = 0],
c+
where ∣I +∣ represents the number of elements in the set I+.
From the Eq. (17), Eq. (18) and Eq. (19), we get
E _ log(1 + P exp(μqτPc- - μqτPc+))
q,c+,c-	i∈I	i
≥ P (I+ = 0)Lsup + P (I+ = 0) E [log(1 + ∣I +∣)∣I + = 0]∙
c+
Combining Eq. (20) with Eq. (16), we have,
LU ≥ P (I+ = 0)Lsup + P (I+ = 0) E [log(1 + ∣I +∣)∣I+ = 0] ∙
c+
(19)
(20)
(21)
Thus we have proved the Theorem 1 using the fact that γ0	=	P(I+=0), δ
-1⅛+⅛IE [log(1 + ∣I +∣)∣I+ = 0].
B.2 PROOF OF THEOREM 2.
First, we decompose the unsupervised loss into true negative data loss L- and false negative data
loss LU+ by the property of Eq. (22), and further give an upper bound for the false negative data
13
Under review as a conference paper at ICLR 2021
loss LU+ by the property of Eq. (25). Finally, we get a bound for our SSM in Eq. (29) and prove
Theorem 2.
Step 1. Inequality 1 of `. We note that the function ` satisfies the following constant:
'({vi}i∈iι∪i2) ≤ '({vi}i∈iι)+ '({vi}i∈i2),∀I1,I2 ⊆ I. Because,
'({vi}i∈iι∪i2 ) = log(1+ P evi)
i∈I1∪I2
≤ log(1 + P evi + P evi)
i∈I1	i∈I2
≤ log[(1 + Pevi)(1+ Pevi)]	(22)
i∈I1	i∈I2
= log(1 + P evi) + log(1 + P evi)
i∈I1	i∈I2
='({Vi}i∈Iι )+ '({Vi}i∈I2 ).
Step 2. Decompose the unsupervised loss. We have already divided all negative classes into two
disjoint subsets and gotten their index sets I-, I+. I- is for the true negative data while I+ is for
the false negative data. According to these index sets and the property in Eq. (22), we have,
LU =	E _ log(1 + P exp(μqTk- - μqTk+))
q,k+ ,ki-	i∈I
= E '({μqτk- - μqτk+}i∈ι)
q,k+,ki-
≤ E _ ['({μqTk- - μqτk+}i∈ι-)
q,k+ ,ki-
+'({μqτk- - μqTk+}i∈ι+)]	(23)
=E _ ['({μqτk- - μqτk+}i∈ι-)]
q,k+,ki-
+ E _ ['({μqτk- - μqrTk+}i∈ι+)]
q,k+ ,ki-
:= LU- + LU+,
where the first expectation is L- and the second is L+ ( imagining that the true negative data and
false negative data have been separated during training).
Step 3. Inequality 2 of `. We define vmax ∈ R as the maximum component with indices in I1 , that
is vmax := max{vi}i∈I1. If vmax > 0, we have
'({vi}i∈iι ) = log(1+ P evi)
i∈I1
≤ log(1 + |I1|evmax)	(24)
=IOg(I + lIιl)+log(evmax + ⅛+∣FX)
≤ log(1 + |I1 |) + vmax .
Otherwise, Vi ≤ Vmax ≤ 0,∀i ∈ Ii, we have '({vi}i∈∕J = log(1 + P evi) ≤ log(1 + |Ii|). Thus
i∈I1
we get the inequality,
'({Vi}i∈Iι ) ≤ log(1 + |Ii|) +max{vmax, 0}
≤ log(1 + |I1|) + P |vi|.	(25)
i∈I1
Step 4. The upper bound of LU+. Using the property for the function ` in Eq. (25), we can get an
upper bound for LU+,
LU =	E _ ['({μqτk- - μqTk+}i∈ι+)]
q,k+ ,ki-
≤ E _ log(i + |I +|) + P ∣μqτk- - μqτk+∣
q,k+,ki-	i∈I+
(26)
14
Under review as a conference paper at ICLR 2021
where the second term acts as the penalty for representation ablility by measuring the intra-class
representaion deviation.
E P ∣μqTk- - μqTk+∣
q,k+ ,ki- i∈I+
E |I+|	E	∣μqτk— — μqτk+∣
C+	q,k+ ,k-〜c+ ,i∈I+
≤E
c+
|I+|
E
V q,k+,k-^c+,i∈I+
∣μqTk— — μqTk+∣2
≤^c+ 1 +Vq,k+,k- Ec+,i∈ι+kqk2kk--k+k2
(27)
kqk2 = 11哨1+M+,k-%∈ι+kk--k+k2
∣μ∣Ε
c+
∣μ∣ E
|I+|. /	kk- — Pc+ + Pc+ — k+k2
V k+ ,k-〜c十,i∈I十
|I+| / E	2kpc+- k+k2
V k+〜c+
All data with indices in I+ have the same label c+ , and the expectation in Eq. (27) show the intra-
class representation deviation. Mark the deviation as s(fk) = ∣μ∣ E
c+
/ E	kPc+ — k+k2 . We
V k+ 〜c+
have a uniform class distribution, thus (NK — 1) negative data can be drawn from any class with
equal probability 1/|C|. Then E|I+ | = (NK — 1)/|C| for any positive class c+. Thus, the right
expectation in Eq. (27) can be bound by s(fk), that is,
E V ∣μqTk— - μqTk+∣ ≤ √2 E|I +∣s(fk).	(28)
q,k+ ,ki- i∈I+
From Eq. (28), Eq. (26) and Eq. (23), we have
LU ≤L- + √2 E|I +∣s(fk)+	E	[log (1 + |I +∣)] ,	(29)
q,k+ ,ki-
and we have
E [log(1+|I+|)]
q,k+ ,ki-
=P (I + = 0)	E [log(1 + |I+|)| I + = 0]	(30)
q,k+ ,ki-
= P(I+ 6=0)E [log(1+|I+|)|I+ 6= 0].
c+
Combining Eq. (29), Eq. (30) and Theorem 1, we have proved Theorem 2 by setting γι =
√2γo(Nκ - 1)∕∣C∣.
C Experiments
Self-supervised Algorithm. Note that this is not our contribution. It is a basic self-supervised algo-
rithm from MoCo He et al. (2019). The algorithm is shown in Algorithm 1.
Datasets We evaluate SSM on two FSL benchmark datasets, Omniglot Lake et al. (2015) and
miniImageNet Vinyals et al. (2016). Omniglot is a character image dataset containing 1623 hand-
written characters from 50 alphabets. Each character contains 20 gray images drawn by differ-
ent writers. We resize raw images into size of 28×28 and rotate each character by 0。，90。，180°
and 270° to form 4 different classes. The 1200 characters with rotations (4800 classes) are for
training, 100 characters (400 classes) for validation and 323 characters (1292 classes) for test-
ing. miniImageNet is a downsampled image subset of the large-scale ImageNet Russakovsky et al.
(2015). It consists of 100 classes with 600 RGB images of size 84 × 84 per class, where 64 classes
15
Under review as a conference paper at ICLR 2021
Algorithm 1 Self-Supervised Algorithm
Require: unsupervised auxiliary set A={. . ., xi,. . .}, number of keys per matching task NK, ran-
dom augmentation function Aug(∙), momentum rate β, SGD learning rate lr.
1:	randomly initialize the encoder fq, fk parameters θq ,θk, metric scaling scalar μ
2:	while not done do
3:	sample NK data {x1, . . . , xNK} from A
4:	randomly select xj as postive data, 1 ≤ j ≤ NK
5:	augment xj into Aug(xj) and Aug0(xj)
6:	augment xi into Aug(xi), ∀i ∈ {1, . . . , NK}\j
7:	positive key: x+_Aug(xj) and negative keys: {x-,... ,xNκ-1}-{Aug(xi)}i=j, query:
Xq—Aug0 (Xj),
8:	representation: k+=fk(x+), ki-=fk(xi-), ∀i ∈ {1, . . . , NK -1}, and q=fq(xq),
9:	evaluate task-specific metric loss L by Eq. (1)
10:	back propagation update: (θq, μ) — (θq, μ) - Ir ∙ RE,*)L
11:	momentum update: Θm — β ∙ Θm + (1 — β) ∙ θq
12:	end while
are for training, 16 classes for validation and 20 classes for testing. The above splits all keep the
same with those used by CACTUs Hsu et al. (2019) and UMTRA Khodadadeh et al. (2019) for
comparison fairness. Certainly, the labels of data in training classes have been stripped to form the
unsupervised auxiliary set A.
Setup For fairness, the architecture of encoder fq, fk keeps aligned with that used by CACTUs and
UMTRA, as well as the supervised methods like MAML and ProtoNets. It is comprised of 4 con-
volutional blocks, each of which is a sequential combination of 64-channel 3×3 convolution, batch
normalization, ReLU and 2×2 max-pooling. The last block is followed by a flattening and a nor-
malization to form the feature representation, which leads to 64-/1600-dimensional representations
for the images from Omniglot/miniImageNet, respectively. We set lr = 0.005, β = 0.999 by mon-
itoring validation performance. For augmentation Aug(∙), We keep consistent with UMTRA: for
Omniglot by randomly zeroing pixels and randomly shifting, while for miniImageNet by the ready-
made Auto-Augmentation Cubuk et al. (2018) model. For miniImageNet, we use the augmentation
model trained on CIFAR. Different augmentation methods do matter according to AAL Antoniou
& Storkey (2019), CC-Rot Gidaris et al. (2019), CC-Loc Gidaris et al. (2019). Thus we keep the
same augmentation setting as a basic method though we could do better by changing augmentation
settings. The SGD optimizer is used for BP update for fq.
Compared Methods Compared methods are explicitly divided into unsupervised group, supervised
group and ablation group. Specifically, the unsupervised group includes not only cutting-edge CAC-
TUs and UMTRA, but also some alternate algorithms that can work on unsupervised A (see Hsu
et al. (2019) for more details about them). We also compare to supervised MAML Finn et al. (2017)
and ProtoNets Snell et al. (2017), which are considered as the ceiling limit of the unsupervised
methods. In ablation group, to explore the effectiveness of the negative numbers (NK - 1) and the
number M of positive keys in SSM: (1) baseline, the algorithm in Algorithm 1. (2) NK /4, replacing
the best NK (2048 for Omniglot, 512 for miniImageNet) with NK /4. (3) M, replacing all positive
or negative keys with the mean of three different keys from three augmentations of the same data.
Results on Omniglot Contrast results on Omniglot in Table 3 demonstrate that SSM completely sur-
passes CACTUs-MAML, CACTUs-ProtoNets, UMTRA and other alternate unsupervised methods,
yielding dramatic improvements regardless of (N,K) settings. Another noticeable observation is
the much smaller performance gap between SSM with supervised MAML and ProtoNets. For (5,5)
setting, especially, SSM baseline realizes accuracy 98.09% that is very close to accuracy 98.83% by
supervised MAML, although it needs to use (4800×20+5×5) labeled data whereas our SSM relies
on only 5×5 labeled images for each (5,5) classification task.
Results on miniImageNet Table 4 contrasts SSM to other methods on miniImageNet. Com-
pared to Omniglot, the underlying complexity and ambiguity of the real-world image objects in
miniImageNet cause relatively lower classification accuracy. Although SSM is not finetuned on the
50×5 support data, it still reaches the second-best for (5,50) setting and beats many finetuning un-
16
Under review as a conference paper at ICLR 2021
Algorithm	(5,1)	(5,5)	(20,1)	(20,5)
Training from scratch	52.50	74.78	24.91	47.62
BiGAN knn-nearest neighbors	49.55	68.06	27.37	46.70
BiGAN linear classifier	48.28	68.72	27.80	45.82
BiGAN MLP with dropout	40.54	62.56	19.92	40.71
BiGAN cluster matching	43.96	58.62	21.54	31.06
BiGAN CACTUs-MAML	58.18	78.66	35.56	58.62
BiGAN CACTUs-ProtoNets	54.74	71.69	33.40	50.62
ACAI knn -nearest neighbors	57.46	81.16	39.73	66.38
ACAI linear classifier	61.08	81.82	43.20	66.33
ACAI MLP with dropout	51.95	77.20	30.65	58.62
ACAI cluster matching	54.94	71.09	32.19	45.93
ACAI CACTUs-MAML	68.84	87.78	48.09	76.36
ACAI CACTUs-ProtoNets	68.12	83.58	47.75	66.27
UMTRA	83.80	95.43	?74.25	?92.12
AAL-ProtoNets	84.66	89.14	68.79	74.28
AAL-MAML++	?88.40	?97.96	70.21	88.32
ProtoTransfer	88.00	96.84	72.27	89.08
SSM baseline	92.00	98.09	79.99	94.13
SSM (NK /4)	91.48	97.60	77.90	92.82
SSM (M ×3)	・92.06	•98.23	•80.03	•94.31
Supervised MAML	9446	98.83	84.60	96.29
Supervised ProtoNets	98.35	99.58	95.31	98.81
Table 3: Accuracy (%) on Omniglot (averaged over 1000 N -way K-shot (N,K) test tasks. •: best,
?: previous best). No-SSM results are cited from papers of CACTUs and UMTRA.
Algorithm	(5,1)	(5,5)	(5,20)	(5,50)
Training from scratch	27.59	38.48	51.53	59.63
BiGAN knn -nearest neighbors	25.56	31.10	37.31	43.60
BiGAN linear classifier	27.08	33.91	44.00	50.41
BiGAN MLP with dropout	22.91	29.06	40.06	48.36
BiGAN cluster matching	24.63	29.49	33.89	36.13
BiGAN CACTUs-MAML	36.24	51.28	61.33	66.91
BiGAN CACTUs-ProtoNets	36.62	50.16	59.56	63.27
DeepCluster knn-nearest neighbors	28.90	42.25	56.44	63.90
DeepCluster linear classifier	29.44	39.79	56.19	65.28
DeepCluster MLP with dropout	29.03	39.67	52.71	60.95
DeepCluster cluster matching	22.20	23.50	24.97	26.87
DeepCluster CACTUs-MAML	39.90	53.97	63.84	69.64
DeepCluster CACTUs-ProtoNets	39.18	53.36	61.54	63.55
UMTRA	39.93	50.73	61.11	67.15
AAL-ProtoNets	37.67	40.29	-	-
AAL-MAML++	33.30	49.18	-	-
ULDA-ProtoNets	40.63	55.41	63.16	65.20
ULDA-MetaOptNet	40.71	54.49	63.58	67.65
CC-Rot	41.70	58.64	68.61	71.86
CC-Loc	37.75	53.02	61.38	64.15
ProtoTransfer	?45.67	?62.99	?72.34	?77.22
SSM baseline	41.78	59.13	68.87	71.92
SSM (NK/4)	38.96	54.87	64.72	67.82
SSM (M ×3)	42.58	60.12	69.25	71.98
Supervised MAML	46.81	62.13	71.03	75.54
Supervised ProtoNets	46.56	62.29	70.05	72.04
Table 4: Accuracy (%) on miniImageNet (similar to Table 3).
17
Under review as a conference paper at ICLR 2021
supervised methods, which is a convincing proof for the effectiveness of the representation space
learned by SSM.
18