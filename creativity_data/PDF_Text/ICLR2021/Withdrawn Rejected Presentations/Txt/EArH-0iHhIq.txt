Under review as a conference paper at ICLR 2021
ON NEURAL NETWORK GENERALIZATION VIA
promoting within-layer activation diversity
Anonymous authors
Paper under double-blind review
Ab stract
During the last decade, neural networks have been intensively used to tackle var-
ious problems and they have often led to state-of-the-art results. These networks
are composed of multiple jointly optimized layers arranged in a hierarchical struc-
ture. At each layer, the aim is to learn to extract hidden patterns needed to solve
the problem at hand and forward it to the next layers. In the standard form, a
neural network is trained with gradient-based optimization, where the errors are
back-propagated from the last layer back to the first one. Thus at each optimiza-
tion step, neurons at a given layer receive feedback from neurons belonging to
higher layers of the hierarchy. In this paper, we propose to complement this tra-
ditional ’between-layer’ feedback with additional ’within-layer’ feedback to en-
courage diversity of the activations within the same layer. To this end, we measure
the pairwise similarity between the outputs of the neurons and use it to model the
layer’s overall diversity. By penalizing similarities and promoting diversity, we
encourage each neuron to learn a distinctive representation and, thus, to enrich
the data representation learned within the layer and to increase the total capacity
of the model. We theoretically study how the within-layer activation diversity af-
fects the generalization performance of a neural network in a supervised context
and we prove that increasing the diversity of hidden activations reduces the esti-
mation error. In addition to the theoretical guarantees, we present an empirical
study confirming that the proposed approach enhances the performance of neural
networks.
1	Introduction
Neural networks are a powerful class of non-linear function approximators that have been success-
fully used to tackle a wide range of problems. They have enabled breakthroughs in many tasks,
such as image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012a), and
anomaly detection (Golan & El-Yaniv, 2018). Formally, the output of a neural network consisting
of P layers can be defined as follows:
f(x; W) = ΦP(WP(ΦP-1(…Φ2(W2φ1(W 1x)))),	⑴
where φi(.) is the element-wise activation function, e.g., ReLU and Sigmoid, of the ith layer and
W= {W1,...,WP} are the corresponding weights of the network. The parameters of f(x; W)
are optimized by minimizing the empirical loss:
1N
L(f ) = NN EM"©； w)，y,，	⑵
i=1
where l(∙) is the loss function, and {xi, yi}N=1 are the training samples and their associated ground-
truth labels. The loss is minimized using the gradient decent-based optimization coupled with back-
propagation.
However, neural networks are often over-parameterized, i.e., have more parameters than data. As
a result, they tend to overfit to the training samples and not generalize well on unseen examples
(Goodfellow et al., 2016). While research on Double descent (Belkin et al., 2019; Advani et al.,
2020; Nakkiran et al., 2020) shows that over-parameterization does not necessarily lead to overfit-
ting, avoiding overfitting has been extensively studied (Neyshabur et al., 2018; Nagarajan & Kolter,
1
Under review as a conference paper at ICLR 2021
2019; Poggio et al., 2017) and various approaches and strategies have been proposed, such as data
augmentation (Goodfellow et al., 2016), regularization (KUkacka et al., 2017; Bietti et al., 2019;
Arora et al., 2019), and dropout (Hinton et al., 2012b; Wang et al., 2019; Lee et al., 2019; Li et al.,
2016), to close the gap between the empirical loss and the expected loss.
Diversity of learners is widely known to be important in ensemble learning (Li et al., 2012; Yu et al.,
2011) and, particularly in deep learning context, diversity of information extracted by the network
neurons has been recognized as a viable way to improve generalization (Xie et al., 2017a; 2015b).
In most cases, these efforts have focused on making the set of weights more diverse (Yang et al.;
Malkin & Bilmes, 2009). However, diversity of the activation has not received much attention.
Inspired by the motivation of dropout to co-adapt neuron activation, Cogswell et al. (2016) pro-
posed to regularize the activations of the network. An additional loss using cross-covariance of
hidden activations was proposed, which encourages the neurons to learn diverse or non-redundant
representations. The proposed approach, known as Decov, has empirically been proven to alleviate
overfitting and to improve the generalization ability of neural network, yet a theoretical analysis to
prove this has so far been lacking.
In this work, we propose a novel approach to encourage activation diversity within the same layer.
We propose complementing ’between-layer’ feedback with additional ’within-layer’ feedback to pe-
nalize similarities between neurons on the same layer. Thus, we encourage each neuron to learn a
distinctive representation and to enrich the data representation learned within each layer. Moreover,
inspired by Xie et al. (2015b), we provide a theoretical analysis showing that the within-layer acti-
vation diversity boosts the generalization performance of neural networks and reduces overfitting.
Our contributions in this paper are as follows:
•	Methodologically, we propose a new approach to encourage the ’diversification’ of the
layer-wise feature maps’ outputs in neural networks. The proposed approach has three
variants based on how the global diversity is defined. The main intuition is that by pro-
moting the within-layer activation diversity, neurons within the same layer learn distinct
patterns and, thus, increase the overall capacity of the model.
•	Theoretically, we analyse the effect the within-layer activation diversity on the generaliza-
tion error bound of neural network. The analysis is presented in Section 3. As shown in
Theorems 3.7, 3.8, 3.9, 3.10, 3.11, and 3.12, we express the upper-bound of the estimation
error as a function of the diversity factor. Thus, we provide theoretical evidence that the
within-layer activation diversity can help reduce the generalization error.
•	Empirically, we show that the within-layer activation diversity boosts the performance of
neural networks. Experimental results show that the proposed approach outperforms the
competing methods.
2	Within-layer activation diversity
We propose a diversification strategy, where we encourage neurons within a layer to activate in a
mutually different manner, i.e., to capture different patterns. To this end, we propose an additional
Within-Iayer loss which penalizes the neurons that activate similarly. The loss function L(f) defined
in equation 2 is augmented as follows:
P
Laug (f )= L(f ) + λ X Ji,	(3)
i=1
where Ji expresses the overall pair-wise similarity of the neurons within the ith layer and λ is the
penalty coefficient for the diversity loss. As in (Cogswell et al., 2016), our proposed diversity loss
can be applied to a single layer or multiple layers in a network. For simplicity, let us focus on a
single layer.
Let φin(xj) and φim(xj ) be the outputs of the nth and mth neurons in the ith layer for the same
input sample xj . The similarity snm between the the nth and mth neurons can be obtained as the
average similarity measure of their outputs for N input samples. We use the radial basis function to
2
Under review as a conference paper at ICLR 2021
express the similarity:
1N
Snm = N Σexp( -Y llφn(xj ) - φm(xj 川2),	⑷
where γ is a hyper-parameter. The similarity snm can be computed over the whole dataset or batch-
wise. Intuitively, if two neurons n and m have similar outputs for many samples, their corresponding
similarity Snm will be high. Otherwise, their similarity Smn is small and they are considered “di-
verse”. Based on these pair-wise similarities, we propose three variants for the global diversity loss
Ji of the ith layer:
•	Direct: Ji = Pn6=m Snm . In this variant, we model the global layer similarity directly
as the sum of the pairwise similarities between the neurons. By minimizing their sum, we
encourage the neurons to learn different representations.
•	Det: Ji = -det(S), where S is a similarity matrix defined as Snm = Snm. This variant is
inspired by the Determinantal Point Process (DPP) (Kulesza & Taskar, 2010; 2012), as the
determinant of S measures the global diversity of the set. Geometrically, det(S) is the vol-
ume of the parallelepiped formed by vectors in the feature space associated with S. Vectors
that result in a larger volume are considered to be more “diverse”. Thus, maximizing det(∙)
(minimizing -det(∙)) encourages the diversity of the learned features.
•	Logdet: Ji = -logdet(S)1. This variant has the same motivation as the second one. We
use logdet instead of det as logdet is a convex function over the positive definite matrix
space.
It should be noted here that the first proposed variant, i.e., direct, similar to Decov (Cogswell et al.,
2016), captures only the pairwise diversity between components and is unable to capture the higher-
order “diversity”, whereas the other two variants consider the global similarity and are able to mea-
sure diversity in a more global manner.
Our newly proposed loss function defined in equation 3 has two terms. The first term is the classic
loss function. It computes the loss with respect to the ground-truth. In the back-propagation, this
feedback is back-propagated from the last layer to the first layer of the network. Thus, it can be
considered as a between-layer feedback, whereas the second term is computed within a layer. From
equation 3, we can see that our proposed approach can be interpreted as a regularization scheme.
However, regularization in deep learning is usually applied directly on the parameters, i.e., weights
(Goodfellow et al., 2016; Kukacka et al., 2017), while in our approach, similar to (Cogswell et al.,
2016), an additional term is defined over the output maps of the layers. For a layer with C neurons
and a batch size of N, the additional computational cost is O(C2(N + 1)) for direct variant and
O(C3 + C2N)) for both the determinant and log of the determinant variants.
3	Generalization Error Analysis
In this section, we analyze how the proposed within-layer diversity regularizer affects the general-
ization error ofa neural network. Generalization theory (Zhang et al., 2017; Kawaguchi et al., 2017)
focuses on the relation between the empirical loss, as defined in equation 2, and the expected risk
defined as follows:
L(f ) = E(x,y)〜Q[l(f(x),y)],	(5)
where Q is the underlying distribution of the dataset. Let f * = arg min f L(f) be the expected
risk minimizer and f = arg minf L(f) be the empirical risk minimizer. We are interested in the
estimation error, i.e., L(f*) - L(f), defined as the gap in the loss between both minimizers (Barron,
1994). The estimation error represents how well an algorithm can learn. It usually depends on the
complexity of the hypothesis class and the number of training samples (Barron, 1993; Zhai & Wang,
2018).
1This is defined only if S is positive definite. It can be shown that in our case S is positive semi-definite.
Thus, in practice we use a regularized version (S + I) to ensure the positive definiteness.
3
Under review as a conference paper at ICLR 2021
Several techniques have been used to quantify the estimation error, such as PAC learning (Hanneke,
2016; Arora et al., 2018), VC dimension (Sontag, 1998; Harvey et al., 2017; Bartlett et al., 2019),
and the Rademacher complexity (Xie et al., 2015b; Zhai & Wang, 2018; Tang et al., 2020). The
Rademacher complexity has been widely used as it usually leads to a tighter generalization error
bound (Sokolic et al., 2016; Neyshabur et al., 2018; Golowich et al., 2018). The formal definition of
the empirical Rademacher complexity is given as follows:
Definition 3.1. (Bartlett & Mendelson, 2002) For a given dataset with N samples D = {xi, yi}iN=1
generated by a distribution Q and for a model space F : X → R with a single dimensional output,
the empirical Rademacher complexity RN(F) of the set F is defined as follows:
1N
RN(F)= Eσ sup 而 Xσif(xi) ,	(6)
f∈F N i=1
where the Rademacher variables σ = {σι, •…,qn} are independent uniform random variables in
{-1, 1}.
In this work, we analyse the estimation error bound of a neural network using the Rademacher
complexity and we are interested in the effect of the within-layer diversity on the estimation error.
In order to study this effect, inspired by (Xie et al., 2015b), we assume that with a high probability
τ, the distance between the output of each pair of neurons, (φn(x) - φm(x))2, is lower bounded by
dmin for any input x. Note that this condition can be expressed in terms of the similarity s defined
in equation 4: snm ≤ e(-γdmin) = smin for any two distinct neurons with the probability τ. Our
analysis starts with the following lemma:
Lemma 3.2. (Xie et al., 2015b; Bartlett & Mendelson, 2002) With a probability ofat least 1 - δ
L(f) — L(f*) ≤ 4Rn(A) + Br22°oN/δ	⑺
for B ≥ supx,y,f |l(f (x), y)|, where RN (A) is the Rademacher complexity of the loss set A.
It upper-bounds the estimation error using the Rademacher complexity defined over the loss set
and sup	|l(f (x), y)|. Our analysis continues by seeking a tighter upper bound of this error and
x,y,f
showing how the within-layer diversity, expressed with dmin, affects this upper bound. We start by
deriving such an upper-bound for a simple network with one hidden layer trained for a regression
task and then we extend it for a general multi-layer network and for different losses.
3.1	Single hidden-layer network
Here, we consider a simple neural network with one hidden-layer with M neurons and one-
dimensional output trained for a regression task. The full characterization of the setup can be sum-
marized in the following assumptions:
Assumptions 1.
•	The activation function of the hidden layer, φ(t), is a Lφ-Lipschitz continuous function.
•	The input vector x ∈ RD satisfies ||x||2 ≤ C1.
•	The output scalar y ∈ R satisfies |y | ≤ C2.
•	The weight matrix W = [wι, w2, ∙∙∙ , WM] ∈ RD×M connecting the input to the hidden
layer satisfies ||wm ||2 ≤ C3 .
• The weight vector v ∈ RM connecting the hidden-layer to the output neuron satisfies
||v ||2 ≤ C4.
• The hypothesis class isF = {f |f (x) = PmM=1 vmφm(x) = PmM=1 vmφ(wmT x)}.
• Lossfunction set is A = {l∣l(f (x), y) = 2|f (x) 一 y|2}.
• With a probability T ,for n = m, ∣∣φn(x) - Φm(x)∣∣2 = ∣∣φ(wT x) — Φ(wT, x)∣∣2 ≥ dmin.
4
Under review as a conference paper at ICLR 2021
We recall the following two lemmas related to the estimation error and the Rademacher complexity:
Lemma 3.3. (Bartlett & Mendelson, 2002) For F ∈ RX, assume that g : R -→ R is a Lg -Lipschitz
continuous function and A = {g ◦ f : f ∈ F}. Then we have
RN (A) ≤ LgRN (F).	(8)
Lemma 3.4. (Xie et al., 2015b) Under Assumptions 1, the Rademacher complexity RN(F) of the
hypothesis class F = {f |f (x) = PmM=1 vmφm(x) = PmM=1 vmφ(wmT x)} can be upper-bounded
asfollows:
2LφCi34√M	C4∣φ(0)∣√M
RN(F) ≤ —√Ν~+	√N	,	⑼
where C134 = C1C3C4 and φ(0) is the output of the activation function at the origin.
Lemma 3.4 provides an upper-bound of the Rademacher complexity for the hypothesis class. In
order to find an upper-bound for our estimation error, we start by deriving an upper bound for
supx,f |f (x)|:
Lemma 3.5. Under Assumptions 1, with a probability at least τQ, we have
SUp |f (x)| ≤ √J,	(10)
x,f
where Q is equal to the number of neuron pairs defined by M neurons, i.e., Q = M(MT), and
J = C2(MC52 + M(M - 1)(C52 - dmJ2) and C5 = LφC1C3 + φ(0),
The proof can be found in Appendix 7.1. Note that in Lemma 3.5, we have expressed the upper-
bound of sUpx,f |f (x)| in terms of dmin. Using this bound, we can now find an upper-bound for
sUpx,f,y |l(f (x), y)| in the following lemma:
Lemma 3.6. Under Assumptions 1, with a probability at least τQ, we have
sup |l(f(χ),y)∣ ≤ (√J + C2)2.	(11)
x,y,f
The proof can be found in Appendix 7.2. The main goal is to analyze the estimation error bound of
the neural network and to see how its upper-bound is linked to the diversity, expressed by dmin , of
the different neurons. The main result is presented in Theorem 3.7.
Theorem 3.7. Under Assumptions 1, with probability at least τQ(1 - δ), we have
L(f) - L(f ) ≤ 8(√J + C2)(2Lφc134 + C4|0(O)I) √N- + (√J + C2? r^ N")	(12)
where C134 = C1C3C4, J = C2(MC2 + M(M - 1)(C2 -&讪冈,and CS = LφC1C3 + φ(0).
The proof can be found in Appendix 7.3. Theorem 3.7 provides an upper-bound for the estimation
error. We note that it is a decreasing function of dmin . Thus, we say that a higher dmin, i.e., more
diverse activations, yields a lower estimation error bound. In other words, by promoting the within-
layer diversity, we can reduce the generalization error of neural networks. It should be also noted
that our Theorem 3.7 has a similar form to Theorem 1 in (Xie et al., 2015b). However, the main
difference is that Xie et al. analyse the estimation error with respect to the diversity of the weight
vectors. Here, we consider the diversity between the outputs of the activations of the hidden neurons.
3.2	Binary classification
We now extend our analysis of the effect of the within-layer diversity on the generalization error in
the case of a binary classification task, i.e., y ∈ {-1, 1}. The extensions of Theorem 3.7 in the case
of a hinge loss and a logistic loss are presented in Theorems 3.8 and 3.9, respectively.
Theorem 3.8.	Using the hinge loss, we have with probability at least τQ (1 - δ)
L(f) - Lf *) ≤ 4(2Lφc134 + C4|0(O)I) √N + (I + √J M? bgNN2 / D	(13)
where C134 =C1C3C4,J=C42(MC52+M(M-1)(C52-d2min/2),andC5 = LφC1C3 + φ(0).
5
Under review as a conference paper at ICLR 2021
Theorem 3.9.	Using the logistic loss l(f (x), y) = log(1 + e-yf(x)), we have with probability at
least τQ (1 - δ)
L(f) — L(f *) ≤ 1 + ;(2LφCi34 + C4∣Φ(0)∣) √√N +log(1 + e√J)^20^ (14)
where C134 = C1C3C4, J =C42(MC52+M(M-1)(C52-d2min /2), and C5 = LφC1C3 + φ(0).
The proofs are similar to Lemmas 7 and 8 in (Xie et al., 2015b). As we can see, for the classification
task, the error bounds of the estimation error for the hinge and logistic losses are decreasing with
respect to dmin . Thus, employing a diversity strategy can improve the generalization also for the
binary classification task.
3.3 Multi-layer networks
Here, we extend our result for networks with P (> 1) hidden layers. We assume that the pair-wise
distances between the activations within layer p are lower-bounded by dpmin with a probability τp .
In this case, the hypothesis class can be defined recursively. In addition, we replace the fourth
assumption in Assumptions 1 with: ||Wp∣∣∞ ≤ Cp for every Wp, i.e., the weight matrix of the
p-th layer. In this case, the main theorem is extended as follows:
Theorem 3.10.	With probability of at least QpP=-01(τp)Qp (1 - δ), we have
L(f)- L(f*)	≤
8(√J + C2)
(2L"C0 Y1 √MpCp + 嘿 X1(2Lφ)PT-P Y1 √MjC
p=0	p=0	j=p
+ (√J + C2)2 Vz22°^	(15)
number of neuron pairs in the Pth layer, defined as Qp = MP(MPT),
where Qp is the
and J P is defined recursively using the following identities: J 0 = C30C1 and Jp
M PC p2(M p2(LφCp-1J p-1 + φ(0))2 - M (M - 1) dmin2)) ,for P = 1,...,P.
The proof can be found in Appendix 7.4. In Theorem 3.10, we see that JP is decreasing with respect
to dpmin. Thus, we see that maximizing the within-layer diversity, we can reduce the estimation error
of a multi-layer neural network.
3.4 Multiple Outputs
Finally, we consider the case of a neural network with a multi-dimensional output, i.e., y ∈ RD .
In this case, we can extend Theorem 3.7 by decomposing the problem into D smaller problems and
deriving the global error bound as the sum of the small D bounds. This yields the following two
theorems:
Theorem 3.11.	For a multivariate regression trained with the squared error, we have with proba-
bility at least τQ(1 - δ),
L(f) — L(f*) ≤ 8D(√J + C2)(2LφC134 + C4∣φ(0)∣) √N + D(√J + CN『'*/δ (16)
where C134 =C1C3C4, J=C42(MC52+M(M-1)(C52-d2min /2)) and C5 = LφC1C3 + φ(0).
Theorem 3.12. For a multi-class classification task using the cross-entropy loss, we have with
probability at least τQ(1 - δ),
Lm - L(f *) ≤ D DE-2 J (2LΦC134 + C4取0)1) √N + log (1 + (D - I)e2J ^oN/δ
(17)
where C134 =C1C3C4,J=C42(MC52+M(M-1)(C52-d2min /2)) and C5 = LφC1C3 + φ(0).
The proofs can be found in Appendix 7.5. Theorems 3.11 and 3.12 extend our result for the multi-
dimensional regression and classification tasks, respectively. Both bounds are inversely proportional
to the diversity factor dmin. We note that for the classification task, the upper-bound is exponentially
decreasing with respect to dmin .
6
Under review as a conference paper at ICLR 2021
4	Related work
Diversity promoting strategies have been widely used in ensemble learning (Li et al., 2012; Yu
et al., 2011), sampling (Derezinski et al., 2019; Biyik et al., 2019; Gartrell et al., 2019), ranking
(Yang et al.; Gan et al., 2020), and pruning by reducing redundancy (Kondo & Yamauchi, 2014; He
et al., 2019; Singh et al., 2020; Lee et al., 2020). In the deep learning context, various approaches
have used diversity as a direct regularizer on top of the weight parameters. Here, we present a
brief overview of these regularizers. Based on the way diversity is defined, we can group these
approaches into two categories. The first group considers the regularizers that are based on the
pairwise dissimilarity of components, i.e., the overall set of weights are diverse if every pair of
weights are dissimilar. Given the weight vectors {wm}mM=1, Yu et al. (2011) define the regularizer as
Pmn(1 - θmn), where θmn represents the cosine similarity between wm and wn. Bao et al. (2013)
proposed an incoherence score defined as - log (M(M—i)Pmn β∣θmn∣1), where β is a positive
hyperparameter. Xie et al. (2015a; 2016) used mean(θmn) - var(θmn) to regularize Boltzmann
machines. They theoretically analyzed its effect on the generalization error bounds in (Xie et al.,
2015b) and extended it to kernel space in (Xie et al., 2017a). The second group of regularizers
considers a more globalist view of diversity. For example, in (Malkin & Bilmes, 2009; 2008; Xie
et al., 2017b), a weight regularization based on the determinant of the weights covariance is proposed
and based on determinantal point process in (Kulesza & Taskar, 2012; Kwok & Adams, 2012).
Unlike the aforementioned methods which promote diversity on the weight level and similar to our
method, Cogswell et al. (2016) proposed to enforce dissimilarity on the feature map outputs, i.e., on
the activations. To this end, they proposed an additional loss based on the pairwise covariance of the
activation outputs. Their additional loss, LDecov is defined as the squared sum of the non-diagonal
elements of the global covariance matrix C :
LDecov = $(IICIIF - ||diag(C)ll2),	(18)
where ||.||F is the Frobenius norm. Their approach, Decov, yielded superior empirical performance;
however, it lacks theoretical proof. In this paper, we closed this gap and we showed theoretically
how employing a diversity strategy on the network activations can indeed decrease the estimation
error bound and improve the generalization of the model. Besides, we proposed variants of our
approach which consider a global view of diversity.
5	Experimental results
In this section, we present an empirical study of our approach in a regression context using Boston
Housing price dataset (Dua & Graff, 2017) and in a classification context using CIFAR10 and CI-
FAR100 datasets (Krizhevsky et al., 2009). We denote as Vanilla the model trained with no diversity
protocol and as Decov the approach proposed in (Cogswell et al., 2016).
5.1	Regression
For regression, we use the Boston Housing price dataset (Dua & Graff, 2017). It has 404
training samples and 102 test samples with 13 attributes each. We hold the last 100 sam-
ple of training as a validation set for the hyper-parameter tuning. The loss weight, is
chosen from {0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005} for both our approach and De-
cov (Cogswell et al., 2016). Parameter γ in the radial basis function is chosen from
{0.00001, 0.0001, 0.01, 0.1.1, 10, 100}. As a base model, we use a neural network composed of
two fully connected hidden layers, each with 64 neurons. The additional loss is applied on top of
both hidden layers.
We train for 80 epochs using stochastic gradient descent with a learning rate of 0.01 and mean
square error loss. For hyperparamrter tuning, we keep the model that perform best on the validation
and use it in the test phase. We experiment with three different activation functions for the hidden
layers: Sigmoid, Rectified Linear Units (ReLU) (Nair & Hinton, 2010), and LeakyReLU (Maas
et al., 2013).
7
Under review as a conference paper at ICLR 2021
Table 1: Mean average error of different approaches on Boston Housing price dataset
	ReLU	Sigmoid	LeakyReLU
Vanilla	2.97	-^3.16	2.85
Decov	2.77	2.99	2.80
Ours (direct)	2.72	2.97	2.82
Ours (det)	2.68	2.87	2.83
Ours (logdet)	2.64	2.83	2.77
Table 1 reports the results in terms of the mean average error for the different approaches over the
Boston Housing price dataset. First, we note that employing a diversification strategy (ours and
Decov) boosts the results compared to the Vanilla approach for all types of activations. The three
variants of our approach, i.e., the within-layer approach, consistently outperform the Decov loss
except for the LeakyReLU where the latter outperforms our direct variant. Table 1 shows that the
logdet variant of our approach yields the best performance for all three activation types.
5.2	Classification
For classification, we evaluate the performance of our approach on CIFAR10 and CIFAR100 datasets
(Krizhevsky et al., 2009). They contain 60,000 32 × 32 images grouped into 10 and 100 distinct
categories, respectively. We train on the 50,000 given training examples and test on the 10,000
specified test samples. We hold the last 10000 of the training set for validation. For the neural
network model, we use an architecture composed of 3 convolutional layers. Each convolution layer
is composed of 32 3 × 3 filters followed by 2 × 2 max pooling. The flattened output of the convo-
lutional layers is connected to a fully connected layer with 128 neurons and a softmax layer. The
different additional losses, i.e., ours and Decov, are added only on top of the fully connected layer.
The models are trained for 150 epochs using stochastic gradient decent with a learning rate of 0.01
and categorical cross entropy loss. For hyper-paramters tuning, we keep the model that performs
best on the validation set and use it in the test phase. We experiment with three different activation
functions for the hidden layers: sigmoid, Rectified Linear Units (ReLU) (Nair & Hinton, 2010), and
LeakyReLU (Maas et al., 2013). All reported results are average performance over 4 trials with the
standard deviation indicated alongside.
Tables 2 and 3 report the test error rates of the different approaches for both datasets. Compared
to the Vanilla network, our within-layer diversity strategies consistently improve the performance
of the model. For the CIFAR10, the direct variant yields more than 0.72% improvement for the
ReLU and 2% improvement for the sigmoid activation. For the LeakyReLU case, the determinant
variant achieves the lowest error rate. This is in accordance with the results on CIFAR100. Here,
we note that our proposed approach outperforms both the Vanilla and the Decov models, especially
in the sigmoid case. Compared to the Vanilla approach, we note that the model training time cost on
CIFAR100 increases by 9% for the direct approach, by 36.1% for the determinant variant, and by
36.2%for the log of determinant variant.
Table 2: Test error rates on CIFAR10
	ReLU	Sigmoid	LeakyReLU
Vanilla	32.04 ± 0.57	33.78 ± 0.64	30.99 ± 0.27
Decov	30.98 ± 0.25	32.22 ± 0.51	30.70 ± 0.35
Ours (direct)	31.28 ± 0.49	31.69 ± 0.51	30.86 ± 0.75
Ours (det)	31.28 ± 0.60	32.92 ± 0.49	30.93 ± 0.44
Ours (logdet)	31.26 ± 0.41	32.61 ± 0.46	30.70 ± 0.25
6	Conclusions
In this paper, we proposed a new approach to encourage ‘diversification’ of the layer-wise feature
map outputs in neural networks. The main motivation is that by promoting within-layer activation
diversity, neurons within the same layer learn to capture mutually distinct patterns. We proposed an
additional loss term that can be added on top of any fully-connected layer. This term complements
8
Under review as a conference paper at ICLR 2021
Table 3: Test error rates on CIFAR100
	ReLU	Sigmoid	LeakyReLU
Vanilla	65.81 ± 0.42	78.52 ± 0.40	64.90 ± 0.22
Decov	65.26 ± 0.21	77.08 ± 0.47	64.57 ± 0.23
Ours (direct)	64.95 ± 0.32	76.91 ± 0.91	64.85 ± 0.23
Ours (det)	64.90 ± 0.40	77.79 ± 0.29	64.46 ± 0.40
Ours (logdet)	64.95 ± 0.17	77.70 ± 0.61	64.49 ± 0.19
the traditional ‘between-layer’ feedback with an additional ‘within-layer’ feedback encouraging di-
versity of the activations. We theoretically proved that the proposed approach decreases the esti-
mation error bound, and thus improves the generalization ability of neural networks. This analysis
was further supported by experimental results showing that such a strategy can indeed improve the
performance of neural networks in regression and classification tasks. Our future work includes
extensive experimental analysis on the relationship between the distribution of the neurons output
and generalization.
References
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks,132:428-446, 2020.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. pp. 254-263. Proceedings of Machine Learning Research,
10-15 Jul 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7413-7424, 2019.
Yebo Bao, Hui Jiang, Lirong Dai, and Cong Liu. Incoherent training of deep neural networks to de-
correlate bottleneck features for speech recognition. In International Conference on Acoustics,
Speech and Signal Processing, pp. 6980-6984, 2013.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, pp. 930-945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
Learning, pp. 115-133, 1994.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, pp. 463-482, 2002.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
Research, pp. 63-1, 2019.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
AIberto Bietti, Gregoire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regu-
larizing deep neural networks. In International Conference on Machine Learning, pp. 664-674,
2019.
Erdem Bιyιk, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using determi-
nantal point processes. arXiv preprint arXiv:1906.07975, 2019.
Michael Cogswell, Faruk Ahmed, Ross B. Girshick, Larry Zitnick, and Dhruv Batra. Reducing
overfitting in deep networks by decorrelating representations. In International Conference on
Learning Representations, 2016.
9
Under review as a conference paper at ICLR 2021
Michal Derezinski, Daniele Calandriello, and Michal Valko. Exact sampling of determinantal point
processes with sublinear time preprocessing. In Advances in Neural Information Processing Sys-
tems ,pp.11546-11558,2019.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
LU Gan, Diana NUrbakova, Lea Laporte, and Sylvie Calabretto. Enhancing recommendation diver-
sity using determinantal point processes on knowledge graphs. In Conference on Research and
Development in Information Retrieval, pp. 2001-2004, 2020.
Mike Gartrell, Victor-EmmanUel BrUnel, Elvis Dohmatob, and Syrine Krichene. Learning nonsym-
metric determinantal point processes. In Advances in Neural Information Processing Systems, pp.
6718-6728, 2019.
Izhak Golan and Ran El-Yaniv. Deep anomaly detection Using geometric transformations. In Ad-
vances in Neural Information Processing Systems, pp. 9758-9769, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neUral networks. In Conference On Learning Theory, pp. 297-299, 2018.
Ian Goodfellow, YoshUa Bengio, Aaron CoUrville, and YoshUa Bengio. Deep learning. MIT Press,
2016.
Steve Hanneke. The optimal sample complexity of PAC learning. Journal of Machine Learning
Research, pp. 1319-1333, 2016.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension boUnds for piece-
wise linear neUral networks. In Conference on Learning Theory, pp. 1064-1068, 2017.
Yang He, Ping LiU, Ziwei Wang, Zhilan HU, and Yi Yang. Filter prUning via geometric median
for deep convolUtional neUral networks acceleration. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4340-4349, 2019.
Geoffrey Hinton, Li Deng, Dong YU, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent VanhoUcke, Patrick NgUyen, Tara N Sainath, et al. Deep neUral networks
for acoUstic modeling in speech recognition: The shared views of foUr research groUps. Signal
processing magazine, 29(6):82-97, 2012a.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya SUtskever, and RUslan R SalakhUtdi-
nov. Improving neUral networks by preventing co-adaptation of featUre detectors. arXiv preprint
arXiv:1207.0580, 2012b.
Kenji KawagUchi, Leslie Pack Kaelbling, and YoshUa Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
YUsUke Kondo and Koichiro YamaUchi. A dynamic prUning strategy for incremental learning on a
bUdget. In International Conference on Neural Information Processing, pp. 295-303. Springer,
2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning mUltiple layers of featUres from tiny images.
2009.
Alex Krizhevsky, Ilya SUtskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lUtional neUral networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Jan Kukacka, Vladimir Golkov, and Daniel Cremers. Regularization for deep learning: A taxonomy.
arXiv preprint arXiv:1710.10686, 2017.
Alex Kulesza and Ben Taskar. Structured determinantal point processes. In Advances in Neural
Information Processing Systems, pp. 1171-1179, 2010.
Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. arXiv preprint
arXiv:1207.6083, 2012.
10
Under review as a conference paper at ICLR 2021
James T Kwok and Ryan P Adams. Priors for diversity in generative latent variable models. In
Advances in Neural Information Processing Systems, pp. 2996-3004, 2012.
Hae Beom Lee, Taewook Nam, Eunho Yang, and Sung Ju Hwang. Meta dropout: Learning to perturb
latent features for generalization. In International Conference on Learning Representations, 2019.
Seunghyun Lee, Byeongho Heo, Jung-Woo Ha, and Byung Cheol Song. Filter pruning and re-
initialization via latent space clustering. IEEE Access, 8:189587-189597, 2020.
Nan Li, Yang Yu, and Zhi-Hua Zhou. Diversity regularized ensemble pruning. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 330-345, 2012.
Zhe Li, Boqing Gong, and Tianbao Yang. Improved dropout for shallow and deep learning. In
Advances in Neural Information Processing Systems, pp. 2523-2531, 2016.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In International Conference on Machine Learning, pp. 3, 2013.
Jonathan Malkin and Jeff Bilmes. Ratio semi-definite classifiers. In International Conference on
Acoustics, Speech and Signal Processing, pp. 4113-4116, 2008.
Jonathan Malkin and Jeff Bilmes. Multi-layer ratio semi-definite classifiers. In International Con-
ference on Acoustics, Speech and Signal Processing, pp. 4465-4468, 2009.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, pp. 11615-11626,
2019.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In International Conference on Machine Learning, 2010.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations, 2018.
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix,
Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning III: explaining the non-overfitting
puzzle. arXiv preprint arXiv:1801.00173, 2017.
Pravendra Singh, Vinay Kumar Verma, Piyush Rai, and Vinay Namboodiri. Leveraging filter corre-
lations for deep model compression. In The IEEE Winter Conference on Applications of Computer
Vision, pp. 835-844, 2020.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Lessons from the
rademacher complexity for deep learning. 2016.
Eduardo D Sontag. VC dimension of neural networks. NATO ASI Series F Computer and Systems
Sciences, pp. 69-96, 1998.
Yehui Tang, Yunhe Wang, Yixing Xu, Boxin Shi, Chao Xu, Chunjing Xu, and Chang Xu. Be-
yond dropout: Feature map distortion to regularize deep neural networks. In Association for the
Advancement of Artificial Intelligence, pp. 5964-5971, 2020.
Haotian Wang, Wenjing Yang, Zhenyu Zhao, Tingjin Luo, Ji Wang, and Yuhua Tang. Rademacher
dropout: An adaptive dropout for deep neural network via optimizing generalization gap. Neuro-
computing, pp. 177-187, 2019.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial
Intelligence and Statistics, pp. 1216-1224, 2017a.
11
Under review as a conference paper at ICLR 2021
Pengtao Xie, Yuntian Deng, and Eric Xing. Diversifying restricted boltzmann machine for document
modeling. In International Conference on Knowledge Discovery and Data Mining, pp. 1315-
1324, 2015a.
Pengtao Xie, Yuntian Deng, and Eric Xing. On the generalization error bounds of neural networks
under diversity-inducing mutual angular regularization. arXiv preprint arXiv:1511.07110, 2015b.
Pengtao Xie, Jun Zhu, and Eric Xing. Diversity-promoting bayesian learning of latent variable
models. In International Conference on Machine Learning, pp. 59-68, 2016.
Pengtao Xie, Aarti Singh, and Eric P Xing. Uncorrelation and evenness: a new diversity-promoting
regularizer. In International Conference on Machine Learning, pp. 3811-3820, 2017b.
Ke Yang, Vasilis Gkatzelis, and Julia Stoyanovich. Balanced ranking with diversity constraints. In
International Joint Conference on Artificial Intelligence, pp. 6035-6042.
Yang Yu, Yu-Feng Li, and Zhi-Hua Zhou. Diversity regularized machine. In International Joint
Conference on Artificial Intelligence, 2011.
Ke Zhai and Huan Wang. Adaptive dropout with rademacher complexity regularization. In Interna-
tional Conference on Learning Representations, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
12
Under review as a conference paper at ICLR 2021
7	Appendix
In the following proofs, we use Lipschitz analysis. In particular, a function f : A → R, A ⊂ Rn ,
is said to be L-Lipschitz, if there exist a constant L ≥ 0, such that |f(a) - f (b)| ≤ L||a - b|| for
every pair of points a, b ∈ A. Moreover:
•	supx∈A f ≤ sup(L||x|| + f (0)).
•	if f is continuous and differentiable, L = sup |f0(x)|.
7.1	Proof of Lemma 3.5
Lemma 3.5. Under Assumptions 1, with a probability at least τQ, we have
SUp |f (x)l ≤ √J,	(19)
x,f
where Q is equal to the number of neuron pairs defined by M neurons, i.e. Q = M(MT), and
J = C2(MC52 + M (M - 1)(C52 - dniJ2) and C5 = LφC1C3 + φ(0).
Proof.
vm φm (x)
∣∣v∣∣∞Φm(χ)!	≤ ∣∣v∣∣∞
φm (x)	≤ C42
2
φm(x)
φm (x)φn(x)	= C42	φm(x)2 +	φn (x)φm(x)	(20)
m6=n
m
We have supw,χ φ(x) < sup(Lφ∣wτx| + φ(0)) because φ is Lg-Lipschitz. Thus, ∣∣φ∣∣∞ ‹
LφC1C3 + φ(0) = C5. For the first term in equation 20, we have Pm φm(x)2 <
M(LφC1C3 + φ(0))2 = MC52. The second term, using the identity φm(x)φn(x) =
2 (φm(x)2 + φn(x)2 - (φm(x) - φn(x))2), CanbereWrittenaS
X : φm(X)φn(X) = £ X : φm (X)+ φn (X) - (φm (X) - φn (X)) ∙	(21)
m6=n	m6=n
In addition, we have with a probability T, ∣∣φm(X) - φn(X)∣∣2 ≥ dmin for m = n. Thus, we have
With a probability at least τQ :
X φm(X)φn(X) ≤ 2 X (2C2 - dmin) = M(M - 1)(C2 - dmin/2).	(22)
m6=n	m6=n
Here Q is equal to the number of neuron pairs defined by M neurons, i.e, Q = M(MT). By putting
everything back to equation 20, we have with a probability τQ,
f2(x) ≤ C4 [MC + M(M - 1)(C2 - dmin/2)) = J	(23)
Thus, with a probability τQ,
SUp |f (x)| ≤ r/sup f (x)2 ≤ √J.	(24)
x,f	x,f
□
7.2	Proof of Lemma 3.6
Lemma 3.6. Under Assumptions 1, with a probability at least τQ, we have
sup |l(f(X),y)∣≤ (√J + C2)2	(25)
x,y,f
Proof. We have supχ,y,f |f(x) - y| ≤ 2supχ,y,f (|f(x)| + |y|) = 2(√J + C2). ThUS
Supx,y,f|l(f(x),y)1 ≤ (√J + C2)2.	□
13
Under review as a conference paper at ICLR 2021
7.3	Proof of Theorem 3.7
Theorem 3.7. Under Assumptions 1, with probability at least τQ (1 - δ), we have
L(Z)- L(f ) ≤ 8(√J + C2)(2L0C134 + C4|0(O)I) √N + (√J + C2? r^ N") (26)
where C134 = C1C3C4, J = C4(MC5 + M (M - 1)(C2 -跺加/2》,and C5 = LφC1C3 + φ(0).
Proof. Given that l(∙) is K-Lipschitz with a constant K = Supx,y,f If (x) - y∣ ≤ 2(√J + C2),and
UsingLemma 3.3, we can show that RN(A) ≤ KRN(F) ≤ 2(√J + C2)RN(F). For RN(F),
We use the bound found in Lemma 3.4. Using Lemmas 3.2 and 3.6 completes the proof.	□
7.4	Proof of Theorem 3.10
Theorem 3.10. Under Assumptions 1, with probability ofat least QpP=-01(τp )Qp (1 - δ), we have
L(f - L(f*)	≤	8(√J + C2)
(2L"C0 Y1 √MpCp + 黑 X(2Lφ)-1-p YI √MjCj
p=0	p=0	j=p
(√J +
2log(2∕δ)
N
+
(27)
where Qp is the number of neuron pairs in the pth layer, defined as Qp =
and JP is defined recursively using the following identities: J0 = C30 C1
M pC RM p2(LφCp-1J PT + φ(0))2 - M (M - 1)&需加/2)) ,for p =1,...,P.
M P(M p-1)
2
and Jp
Proof. Lemma 5 in (Xie et al., 2015b) provides an upper-bound for the hypothesis class. We denote
by vp denote the outputs of the pth hidden layer before applying the activation function:
v0 = [w10T x,  , wM0T0 x]	(28)
Mp-1	Mp-1
vp = [ X wjp,1φ(vjp-1),  , X wjp,Mpφ(vjp-1)]	(29)
j=1	j=1
vp = [w1pTφp, ..., wMp pTφp],	(30)
where φp = [φ(vρ-1),…，φ(vp--ι)]. We have
Mp
IIvpII22 = X(wmpTφp)2	(31)
m=1
and wmp Tφp ≤ C3p Pn φpn. Thus,
Mp
IIvpII22 ≤ X(C3pXφpn)2=MpC3p2(Xφpn)2=MpC3p2Xφpmφpn.	(32)
m=1 n	n	mn
We use the same decomposition trick of φpmφpn as in the proof of Lemma 3.5. We need to bound
supx φp :
sup φp < sup(LφIwjp-1T vp-1I + φ(0)) <LφIIWp-1II∞IIvp-1II22+φ(0).	(33)
x
Thus, we have
I∣vp∣l2 ≤ MpCp2(M2(LφCp-1∣∣vp-1 II2 + φ(0))2 - M(M - Mdmin/2» = JP (34)
We found a recursive bound for IIvpII22, we note that for p = 0, we have IIv0II22 ≤ IIW0II∞C1 ≤
C30C1 = J0 . Thus,
sup	If(x)∣ = sup	IvPI ≤ VJP∙
x,fP∈FP	x,fP ∈FP
(35)
□
14
Under review as a conference paper at ICLR 2021
7.5	Proofs of Theorems 3.11 and 3.12
Theorem 3.11.	For a multivariate regression trained with the squared error, we have with proba-
bility at least τQ (1 - δ),
L(f) - L(f *) ≤ 8D(√J + θ(2LφCi34 + C4|0(O)I) √N + D(√J + C2) y21°gN∕(36 (36)
where C134 =C1C3C4,J=C42(MC52+M(M-1)(C52-d2min/2),andC5 = LφC1C3 + φ(0).
Proof. The squared loss ||f (x) - y||2 can be decomposed into D terms (f (x)k - yk)2. Using
Theorem 3.7, We can derive the bound for each term.	口
Theorem 3.12.	For a multiclass classification task using the cross-entropy loss, we have with
probability at least τQ (1 - δ),
L⑶ - L(f *) ≤ D DE-2 J (2LΦC134 + C4lφ⑼ |) √N + log (1 + (D - I)e2J vz2⅛≡
(37)
where C134 =C1C3C4,J=C42(MC52+M(M-1)(C52-d2min/2),andC5 =LφC1C3+φ(0).
Proof. Using Lemma 9 in (Xie et al., 2015b), we have supf,χ,y l = l°g (1 + (D - 1)e2 J) and l
is
D-1
D-1+e-2 j
-Lipschitz. Thus, using the decomposition property of the Rademacher complexity,
we have
R	D(D - 1)	∕2LφCi34√M C4∣φ(0)∣√M
Rn(A) - D - 1 + e-2J l —√N — 十 一√N—
(38)
□
15