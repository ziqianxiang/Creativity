Under review as a conference paper at ICLR 2021
Exploring the Potential of Low-bit Training
of Convolutional Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a low-bit training framework for convolutional neural
networks. Our framework focuses on reducing the energy and time consumption
of convolution kernels, by quantizing all the convolutional operands (activation,
weight, and error) to low bit-width. Specifically, we propose a multi-level scaling
(MLS) tensor format, in which the element-wise bit-width can be largely reduced
to simplify floating-point computations to nearly fixed-point. Then, we describe the
dynamic quantization and the low-bit tensor convolution arithmetic to efficiently
leverage the MLS tensor format. Experiments show that our framework achieves a
superior trade-off between the accuracy and the bit-width than previous methods.
When training ResNet-20 on CIFAR-10, all convolution operands can be quantized
to 1-bit mantissa and 2-bit exponent, while retaining the same accuracy as the
full-precision training. When training ResNet-18 on ImageNet, with 4-bit mantissa
and 2-bit exponent, our framework can achieve an accuracy loss of less than 1%.
Energy consumption analysis shows that our design can achieve over 6.8× higher
energy efficiency than training with floating-point arithmetic.
1	Introduction
Convolutional neural networks (CNNs) have achieved state-of-the-art performance in many computer
vision tasks, such as image classification (Krizhevsky et al., 2012) and object detection (Redmon
et al., 2016; Liu et al., 2016). However, deep CNNs are both computation and storage-intensive.
The training process could consume up to hundreds of ExaFLOPs of computations and tens of
GBytes of storage (Simonyan & Zisserman, 2014), thus posing a tremendous challenge for training
in resource-constrained environments. At present, the most common training method is to use GPUs,
but it consumes much energy. The power of a running GPU is about 250W, and it usually takes more
than 10 GPU-days to train one CNN model on ImageNet (Deng et al., 2009). It makes AI applications
expensive and not environment-friendly.
Table 1: The number of different operations in the training process (batch size = 1). Abbreviations:
“EW-Add”： element-wise addition, ;“F"： forward pass; “B”: backward pass.
Op Name	Op Type	ReSNet18 (ImageNet)	ResNet20 (CIFAR-10)
Conv (F)	Mul&Add	2.72E+10	4.05E+07
Conv (B)	Mul&Add	5.44E+10	8.11E+07
BN (F)	Mul&Add	3.01E+07	1.88E+05
BN (B)	Mul&Add	3.01E+07	1.88E+05
EW-Add (F)	Add	1.49E+07	7.37E+04
EW-Add (B)	Add	1.20E+07	7.37E+04
Weight Update (B)	Add	1.12E+07	2.68E+05
Reducing the precision of NNs has drawn great attention since it can reduce both the storage and
computational complexity. It is pointed out that the power consumption and circuit area of fixed-point
multiplication and addition units are greatly reduced compared with floating-point ones (Horowitz,
2014). Many studies (Jacob et al., 2017a; Dong et al., 2019; Banner et al., 2018b) focus on amending
the training process to acquire a reduced-precision model with higher inference efficiency.
1
Under review as a conference paper at ICLR 2021
Besides the studies on improving inference efficiency, there exist studies that accelerate the training
process. Wang et al. (2018) and Sun et al. (2019) reduce the floating-point bit-width to 8 during the
training process. Wu et al. (2018) implements a full-integer training procedure to reduce the cost but
fails to get acceptable performance.
As shown in Tab. 1, Conv in the training process accounts for the majority of the operations. Therefore,
this work aims at simplifying convolution to low-bit operations, while retaining a similar performance
with the full-precision baseline. The contributions of this paper are:
1.	This paper proposes a low-bit training framework to improve the energy efficiency of
CNN training. We design a low-bit tensor format with multi-level scaling (MLS format),
which can strike a better trade-off between the accuracy and bit-width, while taking the
hardware efficiency into consideration. The multi-level scaling technique extracts the
common exponent of tensor elements as much as possible to reduce the element-wise bit-
width, thus improving the energy efficiency. To leverage the MLS format efficiently, we
develop the corresponding dynamic quantization and the MLS tensor convolution arithmetic.
2.	Extensive experiments demonstrate the effectiveness of our low-bit training framework.
One only needs 1-bit mantissa and 2-bit exponent to train ResNet-20 on CIFAR-10 while
retaining the same accuracy as the full-precision training. On ImageNet, using 4-bit mantissa
and 2-bit exponent is enough for training ResNet-18, with a precision loss within 1%. Our
method achieves higher energy efficiency using fewer bits than previous floating-point
training methods and better accuracy than previous fixed-point training methods.
3.	We estimate the hardware energy that implements the MLS convolution arithmetic. Using
our MLS tensor format, the energy efficiency of convolution can be improved by over 6.8×,
than the full-precision training, and over 1.2× than previous low-bit training methods.
2	Related work
2.1	Post-Training Quantization
Earlier quantization methods like (Han et al., 2015) focused on the post-training quantization, and
quantized the pre-trained full-precision model using the codebook generated by clustering or other
criteria (e.g., SQNR Lin et al. (2015), entropy Park et al. (2017)). Banner et al. (2018b) selected the
quantization bit-width and clipping value for each channel through the analytical investigation. Jacob
et al. (2017b) developed an integer arithmetic convolution for efficient inference, but it’s hard to be
used in training because the scale of the output tensor should be known before calculation. These
quantization methods need pretrained models, and cannot accelerate the training process.
2.2	Quantize-Aware Training
Quantize-aware training considered quantization effects in the training process. Some methods trained
an ultra low-bit network like binary (Rastegari et al., 2016) or ternary (Li et al., 2016) networks, with
a layer-wise scaling factor. Despite that the follow-up studies (Liu et al., 2020; Qin et al., 2019) have
been proposing training techniques to improve the performance of binary networks, the extremely
low bit-width still causes notable performance degradation. Other methods sought to retain the
accuracy with relatively higher precision, such as 8-bit (Jacob et al., 2017a). Gysel et al. (2018)
developed a GPU-based training framework to get dynamic fixed-point models. These methods focus
on accelerating the inference process and the training process is still using floating-point operations.
2.3	Low-Bit Training
To accelerate the training process, studies have been focusing on design a better floating-point data
format. Dillon et al. (2017) proposed a novel 16-bit floating-point format that is more suitable for
CNN training, while Koster et al. (2017) proposed the FlexPoint that contains 16-bit mantissa and
5-bit tensor-shared exponent (scale), which is similar to the dynamic fixed-point format proposed by
Gysel et al. (2018). Recently, 8-bit floating-point (Wang et al., 2018; Sun et al., 2019) was used with
chunk-based accumulation and hybrid format to solve swamping.
2
Under review as a conference paper at ICLR 2021
__________________TensOr Scale__________________
I	EXPt	I	Mant	I
,	(g)	、
. GrOup 0 Scale .	, GrOup 1 ScaIe .	, GrOup N ScaIe .
I	EXPg	IM I I	EXPg	IM I I	EXPg	IM ∣
Θ	0	Θ
S	EXpX	ManX
・…	・…	・…
S	EXpX	ManX
S	EXpX	ManX
・…	・…	・…
S	EXpX	ManX
S	EXpX	ManX
・…	・…	・…
S	EXpX	ManX
Group 0	Group 1	Group N
Figure 1:	Illustration of the multi-level scaling (MLS) low-bit tensor data format.
Some studies used fixed-point in both the forward and backward processes (Zhou et al., 2016).Wu
et al. (2018); Yang et al. (2020) implemented a full-integer training framework for integer-arithmetic
machines. However, their methods caused notable accuracy degradation. Banner et al. (2018a) used
8-bit and 16-bit quantization based on integer arithmetic (Jacob et al., 2017b) to achieve a comparable
accuracy with the full-precision baseline. But it’s not very suitable for training as we discussed earlier.
These methods reduced both the training and inference costs. In this paper, we seek to strike a better
trade-off between accuracy and bit-width.
3	Mulit-level Scaling Low-bit Tensor Format
In this paper, we denote the filters and feature map of the convolution operation as weight (W) and
activation (A), respectively. In the back-propagation, the gradients of feature map and weights are
denoted as error (E) and gradient (G), respectively.
3.1	Mapping Formula of the Quantization S cheme
In quantized CNNs, floating-point values are quantized to use the fixed-point representation. In a
commonly used scheme (Jacob et al., 2017b), the mapping function is f loat = scale × (Fix+Bias),
in which scale and Bias are shared in one tensor. However, since data distribution changes over time
during training, one cannot simplify the Bias calculation as Jacob et al. (2017b) did. Thus, we adopt
an unbiased quantization scheme,and extend the scaling factor to three levels for better representation
ability. The mapping formula of our quantization scheme is
X[i,j,k,l] = Ss[i,j,k,l] X St X Sg[i,j] X X[i,j,k,l]	(1)
where [∙] denotes the indexing operation, Ss is a sign tensor, St is a tensor-wise scaling factor shared
in one tensor, and Sg is a group-wise scaling factor shared in one group, which is a structured subset
of the tensor. Our paper considers three grouping dimensions: 1) grouping by the 1-st dimension of
tensor, 2) the 2-nd dimension of tensor, or 3) the 1-st and the 2-nd dimensions simultaneously. St,
Sg, and X use the same format, which we refer to ashE, M)，a customized floating-point format
with E-bit exponent and M-bit mantissa (no sign bit). A value in the format hE, Mi is
float = 12F(Man, Exp) = Frac X 2-Exp = (l + MMn) X 2-Ex	(2)
where Man and Exp are the M-bit mantissa and E-bit exponent, and Frac ∈ [1, 2) is a fraction.
3.2	Details on the Scaling Factors
The first level tensor-wise scaling factor St is set to be an ordinary floating-point number
(hEt, Mti = h8, 23i), which is the same as unquantized data in training to retain the precision
as much as possible. Considering the actual hardware implementation cost, there are some restric-
tions on the second level group-wise scaling factor Sg . Since calculation results using different
tensor groups need to be aggregated, using Sg in an ordinary floating-point format will make more
expensive conversions and operations necessary in the hardware implementation. We proposed two
3
Under review as a conference paper at ICLR 2021
hardware-friendly group-wise scaling scheme, whose formats can be denoted as hEg, 0i, and hEg, 1i.
The scaling factor in the hEg, 0i format is simply a power of two, which can be implemented easily
as shifting on the hardware. From Eq. 2, a Sg = I2F(Mang, Expg) value in the hEg, 1i format can
be written as
1+
× 2-Expg
2-Expg + 2-Expg -1
2-Expg
Mang = 1
Mang = 0
(3)
which is a sum of two shifting, and can be implemented with small hardware overhead.
The third level scaling factor Sx = I2F (0, Expx) = 2-Expx is the element-wise exponent in
X = Sχ(1 + Manx), and We Can see that the elements of X in Eq. 1 are in ahEx, Mxi format.
The specific values of Ex and Mx determine the type and the cost of the basic multiplication and
accumulation (MAC) operation, Which Will be discussed later in Sec. 5.2.
[Low]
Forward ,	、
T^PHigh]「BN]
[High]
®
ReLU
IQ」Low]
I
Activation!
Weight!
Buffer
Activation!"#
Error!"#
Weight!
backward ∣	土
IQ  ------- BN -一D
Mask J
Error!
Gradient!
Get Gradient
SGD
Θ胃品
Low-bit ConV (Sec. 4.2)
Dynamic Quant. (Sec. 4.1)
High-bit OPs
Low-bit Data
High-bit Data
Figure 2:	Computation flow of the proposed low-bit training.
4 Low-bit Training Framework of CNN
As shown in Fig. 2, the convolution operation (Conv) is followed by batch normalization (BN),
nonlinear operations (e.g. ReLU, pooling). Since Convs account for the majority computational cost,
we apply quantization right before Convs in the training process, including three types: Conv(W,
A), Conv(W, E), and Conv(A, E). Note that the output data of Convs is in floating-point format, and
other operations operate on floating-point numbers. An iteration of this low-bit training process is
summarized in Appendix Alg. 2, in which the major differences from the vanilla training process
are the dynamic quantizaiton procedure DynamicQuantization and the low-bit tensor convolution
arithmetic LowbitC onv.
4.1	Dynamic Quantization
The dynamic quantization converts floating-point tensors to MLS tensors by calculating the scaling
factors Ss,St, Sg and the elements X, as shown in Alg. 1. Exponent(∙) and Fraction。are to
obtain the exponent (an integer) and fraction (a fraction ∈ [0, 1)) of a floating-point number. While
calculating the quantized elements X, we adopt the stochastic rounding (Gupta et al., 2015), and
implement it by using a uniformly distributed random tensor r 〜 U [-1,2].
dxe with probability x - bxc
StochasticRound(x, r) = N earestRound(x + r) =	(4)
bxc with probability dxe - x
Note that Alg. 1 describes how we simulate the dynamic quantization process using floating-point
arithmetic. While in the hardware design, the exponent and mantissa are obtained directly, and the
clip/quantize operations are done by taking out some bits from a machine number.
4
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Algorithm 1: Software-simulated dynamic quantization process
Input: X: float 4-d tensor; Axis: grouping dimension; R: U[-2, 2] distributed random tensor;
hEg, Mgi: format of group-wise scaling factors; hEx, Mxi: format of each element
Output: Ss : sign tensor; St: tensor group-wise scaling factors; Sg : group-wise scaling factors;
X: quantized elements
/* calculating scaling factors */
Ss = Sign(X)
Sr = M ax(Abs(X), axis = Axis)	// Group-wise maximum magnitude
St = M ax(Sr)	// Tensor-wise maximum magnitude
Sgf = Sr ÷ St	// Group-wise scaling factors before quantization < 1
Expg, F racg = Exponent(Sgf), F raction(Sgf)
Expg = C lip(E xpg, 1 - 2Eg , 0)	// Clip Expg to Eg bits
F racg = Ceil(F racg × 2Mg ) ÷ 2Mg	// Quantize Fracg to Mg bits
Sg = Fracg × 2Expg	// Group-wise scaling factors after quantization
/* calculating elements */
Xf = Abs(X) ÷ Sg ÷ St	// Dividing the scaling factors
Expx, Fracx = Exponent(Xf), F raction(Xf)
/* Quantize Fracx to Mx bits with underflow handling* */
Exmin = 1 - 2Ex
Fracxs = Fr acx × 2Mx if not underflow, else F racx × 2Mx -Exmin+Ex
Fracxint = Clip(StochasticRound(Fracxs, R)), 0, 2Mx - 1)
Fr acx = F racxint × 2-Mx if not underflow, else F racxint × 2-Mx +Exmin -Ex
Expx = Clip(Expx , Exmin, -1)
X = Fracx X 2Expx	//Elements after quantization
Return Ss, St, Sg, X
f: The underflow handling follows the IEEE 754 standard (Hough, 2019).
4.2 Low-bit Tensor Convolution Arithmetic
In this section, we describe how to do convolution with two low-bit MLS tensors. Denoting the input
channel number as C and the kernel size as K, the original formula of convolution in training is:
Z[n,co,x,y] =	W[co,ci,i,j] × A[n,ci,x+i,y+j]	(5)
ci=0 i=0 j=0
We take Conv(W, A) as the example to describe the low-bit convolution arithmetic, and the other
two types of convolution can be implemented similarly. Using the MLS data format and denoting
the corresponding values (scaling factors S, exponents Exp, fractions Frac) of W and A by the
superscript (w) and (a), one output element Z[n, co, x, y] of Conv(W, A) can be calculated as:
C-1 K-1 K-1
Z[n, co,x,y] = XXX 卜(W)Sgw)[co, ci]VW[co,ci,i, j]MS(a)Sga)[n, ci]A[n,ci,x + i,y + j])
ci=0 i=0 j=0
C-1	K-1 K-1
=3W)SF)) X[(Sgw)[co,ci]Sga)[n,ci]) X X W[co,ci,i,j]A[n, ci,x + i,y + j]]
C-1
= St(z) X S(p) [n, co, ci]P [n, co, ci]
ci=0
(6)
Eq. 6 shows that the accumulation consist of intra-group MACs that calculates P [n, co, ci] and
inter-group MACs that calculates Z. And the intra-group calculation of P [n, co, ci] is:
K-1
P [n, co, ci] = X F rac(w)[co, ci, i, j]F rac(a)[n, ci, i, j] 2(Exp(w) [co,ci,i,j]+Exp(a) [n,ci,i,j])
i,j=0
(7)
5
Under review as a conference paper at ICLR 2021
Table 2: Comparison of low-bit training methods on CIFAR-10 and ImageNet. Single number in the
bit-width stands for Mx , the corresponding Ex is 0. “f” indicates that FP numbers are used.
Dataset	Method	Bit-Width (W/A/E/Acc)	Model	AccUracy	Baseline
	(Banner et al., 2018a)	112-	ResNet-20	81.5%	90.36%
	(WUetal., 2018)	28832	VGG-like	93.2%	94.1%
	(Rastegari et al., 2016)	1 1 f32 f32	ConvNet	89.83%	91.8%
CIFAR-10		4 44 16	ResNet-20	92.32%	92.45%
		22216	ResNet-20	90.39%	92.45%
	Ours	h1,2ih1,2ih1,2i16	ResNet-20	91.48%	92.45%
		〈2,1〉⑵ 1〉〈2,1〉16	ResNet-20	91.97%	92.45%
	—(ZhoU etal., 2016)^^	8 8 8 32	AlexNet	53.0%	55.9%
	(WUetal., 2018)	28832	AlexNet	48.4%	56.0%
	(Yang et al., 2020)	8 8 8 32	ResNet-18	64.8%	68.7%
	(Banner et al., 2018a)	8 8 16 f32	ResNet-18	66.4%	67.0%
	(SUn etal., 2019)	〈5, 3〉〈5, 3〉〈5, 3〉f32	ResNet-18	69.0%	69.3%
ImageNet		8 8 8 32	ResNet-18	68.5%	69.1%
		66616	ResNet-18	67.6%	69.1%
		44416	ResNet-18	66.5%	69.1%
	Ours	〈1, 6〉 〈1,6〉〈1,6〉16	ResNet-18	68.0%	69.1%
		〈2, 5〉 〈2,5〉〈2,5〉32	ResNet-18	68.3%	69.1%
		〈2, 4〉⑵ 4〉⑵ 4〉32	ResNet-18	68.2%	69.1%
where F rac, Exp are fractions and exponents, whose precision is (Mx + 1)-bits and Ex-bits,
respectively. Thus the intra-group calculation contains (Mx + 1)-bit multiplication, (2Ex+1 - 4)-bit
shifting, and the (2Mx + 2Ex+1 - 2)-bit integer results are accumulated with enough bit-width to
get the partial sum P. And the accumulator has to be floating-point in some previous work (Wang
et al., 2018; Sun et al., 2019), since they use Ex = 5. As for the inter-group calculation, each element
in S(p) is a hE, 2i number obtained by multiplying two hE, 1i numbers. Omitting the n index for
simplicity, the calculation can be written as:
C-1
Z[co,x] = X S(p)[co,ci]P[x,ci] =
ci=0
C-1 (P[x, ci]2-Exp(p)[co,ci]	if Man(P) [co,	ci]	= 00	⑻
X P[x, ci]2-Exp(p)[co,ci]	+P[x,ci]2-Exp(p)[co,ci]-1	if Man(p) [co,	ci]	= 01/10
ci=0 [P[x,ci]21-Exp(P)[co，ci] + P[x,ci]2-Exp(P)[co，ci]-2 if Man(P)[co,ci] = 11
Due to the special format of S(p) , the calculation of the floating-point Z can be implemented
efficiently on hardware, in which no floating-point multiplication is involved.
5	Experiments
We train ResNet (He et al., 2016) on CIFAR-10 (Krizhevsky, 2010) and ImageNet (Deng et al.,
2009) with our low-bit training framework. We experiment with the MLS tensor formats using
different hEx , Mxi configurations. And we adopt the same quantization bit-width for W, A, E,
thus that hardware design is simple. The training results on CIFAR-10 and ImageNet are shown in
Tab. 2. We can see that our method can achieve smaller accuracy degradation using lower bit-width.
Previous study (Zhou et al., 2016) found that quantizing E to a low bit-width hurt the performance.
However, our method can quantize E to Mx = 1 or 2 on CIFAR-10, with a small accuracy drop from
92.45% to 91.97%. On ImageNet, the accuracy degradation of our method is rather minor under
8-bit quantization (0.6% accuracy drop from 69.1% to 68.5%). In the cases with lower bit-width,
our method achieves a higher accuracy (66.5%) with only 4-bit than Banner et al. (2018a) who uses
8-bit (66.4%). With h2, 4i data format, the accuracy loss is less than 1%. In this case, the bit-width
6
Under review as a conference paper at ICLR 2021
Table 3: Ablation study (ResNet-20 on CIFAR-10). “Div.” means that the training failed to converge
#group	Mg	Ex	Mx = 4	Mx = 3	Mx = 2	Mx = 1
1	None	0	90.02	85.68	Div.	Div.
c	0	0	91.54	88.35	82.29	Div.
n	0	0	91.78	89.62	80.71	Div.
nc	0	0	92.14	91.64	88.97	76.98
nc	1	0	92.37	91.73	90.39	82.61
1	None	0	90.02	85.68	Div.	Div.
1	None	1	91.67	90.11	84.72	70.4
1	None	2	92.32	92.34	91.58	90.32
1	None	3	92.66	92.41	92.47	92.04
nc	1	0	92.37	91.73	90.39	82.61
nc	1	1	92.52	92.16	91.48	89.97
nc	1	2	92.37	92.65	92.05	91.97
of the intermediate results is 2Mx + 2Ex+1 - 2 = 14, which means that the accumulation can be
conducted using 16-bit integers, instead of floating-points (Mellempudi et al., 2019).
5.1	Ablation Studies
5.1.1	Grouping Dimension
Group-wise scaling is beneficial because the data ranges vary across different groups. We compare
the average relative quantization error of using the three grouping dimensions (Sec. 3.1) with h8, 1i
group-wise scaling format and h0, 3i element format. The first row of Fig. 3 shows that the AREs are
smaller when each tensor is split to N × C groups.
Furthermore, we compare these grouping dimensions in the training process. The results in the first
section of Tab. 3 show that the reduction of AREs is important to the accuracy of low-bit training.
And when tensors are split to N × C groups, the low-bit training accuracy is higher. And we can see
that Mg = 1 is important for the performance, especially with low Mx (e.g., when Mx = 1).
Figure 3: Average relative quantization errors (AREs) ofW, E, A (left, middle, right) in each layer
when training a ResNet-20 on CIFAR-10. X axis: Layer index. Row 1: Different grouping dimensions
(h0, 3 formatted X,〈8,1)formatted Sg); Row 2: Different Ex ({Eχ, 3〉formatted X, no group-wise
scaling); Row 3: Different Ex (〈Ex, 3) formatted X,(8,1) formatted Sg, N X C groups).
7
Under review as a conference paper at ICLR 2021
5.1.2	Element-wise Exponent
To demonstrate the effectiveness of the element-wise exponent, we compare the AREs of quantization
with different Ex without group-wise scaling, and the results are shown in the second row of Fig. 3.
We can see that using more exponent bits results in larger dynamic ranges and smaller AREs. And
with larger Ex , the AREs of different layers are closer. Besides the ARE evaluation, Tab. 3 also
shows that larger Ex achieves better accuracies, especially when Mx is extremely small.
As shown in Fig. 3 Row 3 and Tab. 3, when jointly using the group-wise scaling and the element-wise
exponent, the ARE and accuracy are further improved. And we can see that the group-wise scaling is
essential for simplifying the floating-point accumulator to fixed-point, since one can use a small Ex
(e.g., 2) by using the group-wise scaling technique.
■
Floating Point Operations
FP MUL
FPAcc
FP MUL
FPAcc
Adder Tree
I I Integer Operations
[Scale η
(b)OUr method
Adder Tree
(a) Original method
⅜
>
Figure 4: The convolution hardware architecture. (a) Previous studies (Mellempudi et al., 2019)
developed low-bit floating-point multiplication (FP MUL) (e.g., 8-bit), but FP32 accumulations are
still needed. (b) We not only makes FP MUL less than 8-bit, but also simplifies the local accumulator.
5.2	Hardware Energy Estimation
Fig. 4 shows a typical convolution hardware architecture, which consists of three main components:
local multiplication, local accumulation, and addition tree. Our algorithm mainly improves the local
MAC. Compared with the full-precision design, we simplify the FP MUL to use a bit-width less than
8 and the local FP Acc to use 16-bit integer. According to the data reported by (Yang et al., 2020),
the energy efficiencies are at least 7× and 20× higher than full-precision design, respectively.
While our method could significantly reduce the cost of the convolution, it also introduces some
overhead: 1) Group-wise maximum statistics (Line 2) and scaling factors division (Line 9) in Alg. 1
accounts for the main overhead of dynamic quantization. The cost of these two operations is
comparable with that of a batch normalization operation, which is relatively small compared with
convolution since the number of operations is fewer by orders of magnitude (Tab. 1). 2) The group-
wise scaling factors introduce additional scaling. Fortunately, when using the hEg, 0i or hEg, 1i
format, we can implement it efficiently with shifting (see Eq. 3).
To summarize, the introduced overhead is small compared with the reduced cost. According to the
numbers of different operations in the training process (Tab. 1) and the energy consumption of each
operation (Appendix Tab. 3) (Horowitz, 2014), we can estimate that our convolution arithmetic is
over 6.8× more energy-efficient than full precision one when training ResNet (see Appendix for the
details). Due to the simplified integer accumulator, our energy efficiency is at least 24% higher than
other low-bit floating-point training algorithms (Mellempudi et al., 2019; Wang et al., 2018).
6	Conclusion
This paper proposes a low-bit training framework to enable training a CNN with lower bit-width
convolution while retaining the accuracy. Specifically, we design a multi-level scaling (MLS) tensor
format, and develop the corresponding quantization procedure and low-bit convolution arithmetic.
Experimental results and the energy analysis demonstrate the effectiveness of our method.
8
Under review as a conference paper at ICLR 2021
References
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of
neural networks. In NeurIPS, 2018a.
Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional
networks for rapid-deployment. In NeurIPS, 2018b.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A. Saurous. TensorFlow Distributions. arXiv
e-prints, art. arXiv:1711.10604, November 2017.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Hawq: Hessian aware
quantization of neural networks with mixed-precision. ArXiv, abs/1905.03696, 2019.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In ICML, 2015.
Philipp Gysel, Jon J. Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for
empirical study of resource-efficient inference in convolutional neural networks. IEEE Transactions
on Neural Networks and Learning Systems, 29:5784-5789, 2018.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
M. Horowitz. 1.1 computing’s energy problem (and what we can do about it). 2014 IEEE International
Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pp. 10-14, 2014.
David G Hough. The ieee standard 754: One for the history books. Computer, 52(12):109-112,
2019.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 2704-2713, 2017a.
Benoit Jacob et al. gemmlowp: a small self-contained low-precision gemm library.(2017), 2017b.
Urs Koster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William Constable, Oguz
Elibol, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, and Naveen
Rao. Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In
NIPS, 2017.
Alex Krizhevsky. Convolutional deep belief networks on cifar-10. 2010.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711,
2016.
Darryl Dexu Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization of deep
convolutional networks. ArXiv, abs/1511.06393, 2015.
9
Under review as a conference paper at ICLR 2021
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and
Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision,
pp. 21-37. Springer, 2016.
Z. Liu, Zhiqiang Shen, M. Savvides, and K. Cheng. Reactnet: Towards precise binary neural network
with generalized activation functions. ArXiv, abs/2003.03488, 2020.
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training
with 8-bit floating point. ArXiv, abs/1905.12334, 2019.
Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo. Weighted-entropy-based quantization for deep
neural networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
7197-7205, 2017.
Haotong Qin, Ruihao Gong, Xianglong Liu, Ziran Wei, Fengwei Yu, and Jingkuan Song. Ir-net:
Forward and backward information retention for highly accurate binary neural networks. ArXiv,
abs/1909.10788, 2019.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In ECCV, 2016.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 779-788, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Xiao Sun, J. Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, V. Srinivasan, Xiaodong
Cui, W. Zhang, and K. Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference
for deep neural networks. In NeurIPS, 2019.
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training
deep neural networks with 8-bit floating point numbers. In NeurIPS, 2018.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep
neural networks. ArXiv, abs/1802.04680, 2018.
Y. Yang, Shuang Wu, Lei Deng, Tianyi Yan, Yuan Xie, and Guoqi Li. Training high-performance and
large-scale deep neural networks with full 8-bit integers. Neural networks : the official journal of
the International Neural Network Society, 125:70-82, 2020.
Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training
low bitwidth convolutional neural networks with low bitwidth gradients. ArXiv, abs/1606.06160,
2016.
10
Under review as a conference paper at ICLR 2021
A Low-bit Training Framework
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Algorithm 2: The t-th iteration of low-bit training with vanilla SGD
Input: L: number of layers; Wt1:L : float weights; A0: inputs; T : label; lr: learning rate
Output: Wt1+:L1: updated float weights
/* forward propagation */
for l in 1 : L do
qWl = DynamicQuantization(Wl )
qAl-1 = DynamicQuantization(Al-1)
Zl = LowbitConv(qWl, qAl-1)
Yl = BatchN orm(Zl)
Al = Activation(Yl)
∂os = Criterion(AL, T)
/* backward propagation */
for l in L : 1 do
d∂γis =甯 × Activation(Yl)
∂lαss — ∂loss X ∂Yl
~∂Z~ = ~∂Y~ × ∂Zl
qEl = DynamicQUantization(讥野)
Gl = LowbitConv (qEl, qAl-1)
Wtl+1 = Wtl - lr × Gl
if l is not 1 then
器号 =LoWbitConv (qEl ,qWl)
∂lαss _ qrp ∂l< ∂lαss ∖
L ∂Al-1 =	( ∂qAl-1 )
Return Wt1:L
B	Experimental Setup
In all the experiments, the first and the last layer are left unquantized following previous studies (Zhou
et al., 2016; Mellempudi et al.,2019; Sun et al., 2019). For both CIFAR-IO and ImageNet, SGD
with momentum 0.9 and weight decay 5e-4 is used, and the initial learning rate is set to 0.1. We
train the models for 90 epochs on ImageNet, and decay the learning rate by 10 every 30 epochs. On
CIFAR-10, We train the models for 160 epochs and decay the learning rate by 10 at epoch 80 and 120.
Qn-e> XeE
3 3 2 2 1
Qn-e> XeUJ
%	5^^10~15^^20~25~30^^35	⅞~^20^~40~60^~80 100 120 140
(a) group number	(b) group number
(C) group number
20
5
0∙n-e> XeUJ
Figure	5: Maximum value of each group of A (left two) and E (right two). (a)(c): Grouped by
channel; (b)(d): Grouped by sample.
11
Under review as a conference paper at ICLR 2021
C Group-wise Scaling
Group-wise scaling is beneficial because the data ranges vary across different groups, as shown in
Fig. 5. The blue line shows the max value in each group when A and E are grouped by channel or
sample. If we use the overall maximum value (green lines in Fig. 5) as the tensor-wise scaling, many
small elements will be swamped. And usually, there are over half of the groups, in which all elements
are smaller than half of the overall maximum (red line). Thus, to fully exploit the bit-width, it is
natural to use group-wise scaling factors.
Figure 6: Performances with different (Eχ, Mxi configurations, no group-wise scaling is used.
D	Element-wise Exponent
Fig. 6	shows the performances of training ReSNet20 on CIFAR-10 with differenthEx, Mx〉configu-
rations. We can see that, when the mantissa bit-width Mx is extremely low (e.g., 1), the element-wise
exponent bit-width Ex is essential for achieving an acceptable performance.
E Energy Efficiency Estimation
Tab. 4 (Horowitz, 2014) reported that the energy consumption of a FP32 multiplication (FP32 MUL)
is about 4 times that of a FP32 addition (FP32 ADD). Denoting the energy consumption of FP32
ADD as C and FP32 MUL as 4C, according to (Yang et al., 2020), we can estimate the cost of FP8
MUL and INT16 ADD as 4/7C and 1/20C, respectively. Then, using the operation statistics in
Tab. 1, we can calculate the energy consumption of one training iteration, and estimate the energy
efficiency improvement ratio:
EnergyRatio
4(#MUL) + 1(#LocalACC) + 1(#TreeADD)
4/7(#MUL) + 1/20(#LocalADD + #Scale) + 1(#TreeACC
≈ 6.8
(9)
Table 4: The cost estimation of primitive operations with 45nm process and 0.9V (Horowitz, 2014).
Params	Energy(pJ)		Area(μm2)	
	Mul	Add	Mul	Add
8-bit Fix	0.2	0.03	282	36
16-bit Float	1.1	0.40	1640	1360
32-bit Float	3.7	0.90	7700	4184
in order to evaluate energy efficiency advantage more accurately, we have implemented the RTL
design of the two MAc units in Fig.4, and used Design compiler to simulate the area and power,
12
Under review as a conference paper at ICLR 2021
the power results are shown in Tab.5. We can see that the simulated results of RTL implemented is
similar with our estimation above, both showing the energy efficiency of our framework.
Table 5: The power evaluation (mW) results of MAC units with different arithmetic with TSMC
65nm process and 100MHz clock, simulated by Design Compiler.
Framework	Full Precision	Other Low-bit Training	Ours
MUL	0.532	0.023	0.0192
LocalACC	0.140	0.140	0.0094
13