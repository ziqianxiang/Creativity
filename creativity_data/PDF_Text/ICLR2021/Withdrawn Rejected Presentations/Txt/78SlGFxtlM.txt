Under review as a conference paper at ICLR 2021
Robust Meta-learning with Noise via
Eigen-Reptile
Anonymous authors
Paper under double-blind review
Ab stract
Recent years have seen a surge of interest in meta-learning techniques for tackling
the few-shot learning (FSL) problem. However, the meta-learner’s initial model is
prone to meta-overfit, as there are only a few available samples with sampling noise.
Besides, when handling the data sampled with label noise for FSL, meta-learner
could be extremely sensitive to label noise. To address these two challenges that
FSL with sampling and label noise. In particular, we first cast the meta-overfitting
problem (overfitting on sampling and label noise) as a gradient noise problem since
few available samples cause meta-learner to overfit on existing examples (clean or
corrupted) of an individual task at every gradient step. We present Eigen-Reptile
(ER) that updates the meta-parameters with the main direction of historical task-
specific parameters to alleviate gradient noise. Specifically, the main direction is
computed by a special mechanism for the parameter’s large size. Furthermore,
to obtain a more accurate main direction for Eigen-Reptile in the presence of
label noise, we propose Introspective Self-paced Learning (ISPL) that constructs
a plurality of prior models to determine which sample should be abandoned. We
have proved the effectiveness of Eigen-Reptile and ISPL, respectively, theoretically
and experimentally. Moreover, our experiments on different tasks demonstrate
that the proposed methods outperform or achieve highly competitive performance
compared with the state-of-the-art methods with or without noisy labels.
1 Introduction
Meta-learning, also known as learning to learn, is the key for few-shot learning (FSL) (Vinyals et al.,
2016; Wang et al., 2019a). One of the meta-learning methods is the gradient-based method, which
usually optimizes meta-parameters as initialization that can fast adapt to new tasks with few samples.
However, fewer samples mean a higher risk of meta-overfitting, as the ubiquitous sampling noise in
mini-batch cannot be ignored. Moreover, existing gradient-based meta-learning methods are fragile
with few samples. For instance, a popular recent method, Reptile (Nichol et al., 2018), updates the
meta-parameters towards the inner loop direction, which is from the current initialization to the last
task-specific parameters. Nevertheless, as shown by the bold line of Reptile in Figure 1, with the
gradient update at the last step, the update direction of meta-parameters has a significant disturbance,
as sampling noise leads the meta-parameters to overfit on the few trained samples at gradient steps.
Many prior works have proposed different solutions for the meta-overfitting problem, such as using
dropout (Bertinetto et al., 2018; Lee et al., 2020), and modifying the loss function (Jamal & Qi, 2019)
etc., which stay at the model level. This paper casts the meta-overfitting problem as a gradient noise
problem that from sampling noise while performing gradient update (Wu et al., 2019). Neelakantan
et al. (2015) .etc have proved that adding additional gradient noise can improve the generalization of
neural networks with large samples. However, it can be seen from the model complexity penalty that
the generalization of the neural network will increase when the number of samples is larger. To a
certain extent adding gradient noise is equivalent to increasing the sample size. As for FSL, there are
only a few samples of each task. In that case, the model will not only remembers the contents that
need to be identified but also overfits on the noise (Zhang et al., 2016).
High-quality manual labeling data is often time-consuming and expensive. Low-cost approaches to
collect low-quality annotated data, such as from search engines, will introduce label noise. Moreover,
training meta-learner requires a large number of tasks, so that it is not easy to guarantee the quality
of data. Conceptually, the initialization learned by existing meta-learning algorithms can severely
1
Under review as a conference paper at ICLR 2021
Gradient noise	Alleviate gradient noise
Reptile	FSL with noisy labels	Eigen-RePtiIe
O Meta-parameters O O Task-specific parameters O Parameters generated by the noisy labels
Figure 1: Inner loop steps of Reptile, Eigen-Reptile. Reptile updates meta-parameters towards the
last task-specific parameters, which is biased. Eigen-Reptile considers all samples more fair. Note
that the main direction is the eigenvector corresponding to the largest eigenvalue.
degrade in the presence of noisy labels. Intuitively, as shown in FSL with noisy labels of Figure
1, noisy labels cause a large random disturbance in the update direction. It means that label noise
(Frenay & Verleysen, 20l3) leads the meta-learner to overfit on wrong samples, which can be seen
as further aggravating the influence of gradient noise. Furthermore, conventional algorithms about
learning with noisy labels require much data for each class (Hendrycks et al., 2018; Patrini et al.,
2017). Therefore, these algorithms cannot be applied to noisy FSL problem, since few available
samples per class. So it is crucial to propose a method to address the problem of noisy FSL.
In this paper, we propose Eigen-Reptile (ER). In particular, as shown in Figure 1, Eigen-Reptile
updates the meta-parameters with the main direction of task-specific parameters that can effectively
alleviate gradient noise. Due to the large scale of neural network parameters, it is unrealistic to
compute historical parameters’ eigenvectors. We introduce the process of fast computing the main
direction into FSL, which computes the eigenvectors of the inner loop step scale matrix instead of the
parameter scale matrix. Furthermore, we propose Introspective Self-paced Learning (ISPL), which
constructs multiple prior models with randomly sampling. Then prior models will discard high loss
samples from the dataset. We combine Eigen-Reptile with ISPL to address the noisy FSL problem,
as ISPL can improve the main direction computed with noisy labels.
Experimental results show that Eigen-Reptile significantly outperforms the baseline model by 5.35%
and 3.66% on corrupted Mini-ImageNet of 5-way 1-shot and clean Mini-ImageNet of 5-way 5-
shot, respectively. Moreover, the proposed algorithms outperform or are highly competitive with
state-of-the-art methods on few-shot classification tasks.
The main contributions of this paper can be summarized as follows:
•	We cast the meta-overfitting issue (overfitting on sampling and label noise) as a gradient
noise issue under the meta-learning framework.
•	We propose Eigen-Reptile that can alleviate gradient noise effectively. Besides, we propose
ISPL, which improves the performance of Eigen-Reptile in the presence of noisy labels.
•	The proposed methods outperform or achieve highly competitive performance compared
with the state-of-the-art methods on few-shot classification tasks.
2	Related Work
There are three main types of meta-learning approaches: metric-based meta-learning approaches
(Ravi & Larochelle, 2016; Hochreiter et al., 2001; Andrychowicz et al., 2016; Liu et al., 2018;
Santoro et al., 2016), model-based meta-learning approaches (Vinyals et al., 2016; Koch et al., 2015;
Mordatch, 2018; Sung et al., 2018; Snell et al., 2017; Oreshkin et al., 2018; Shyam et al., 2017) and
2
Under review as a conference paper at ICLR 2021
gradient-based meta-learning approaches (Finn et al., 2017; Nichol et al., 2018; Jamal & Qi, 2019;
Zintgraf et al., 2018; Li et al., 2017; Rajeswaran et al., 2019; Finn et al., 2018). In this paper, we
focus on gradient-based meta-learning approaches which can be viewed as the bi-level loop. The goal
of the outer loop is to update the meta-parameters on a variety of tasks, while task-specific parameters
are learned through only a small amount of data in the inner loop. In addition, some algorithms
achieve state-of-the-art results by additionally training a 64-way classification task on meta-training
set (Yang et al., 2020; Hu et al., 2020), we do not compare these algorithms for fairness.
Meta-Learning with overfitting. Due to too few samples, meta-learner inevitably tends to overfit
in FSL. Zintgraf et al. (2018) introduces additional context parameters to the model’s parameters,
which can prevent meta-overfitting. Furthermore, Bertinetto et al. (2018) find that regularization
such as dropout can alleviate meta-overfitting in their prior work; Jamal & Qi (2019) propose a
novel paradigm of Task-Agnostic Meta-Learning (TAML), which uses entropy or other approaches to
minimize the inequality of initial losses beyond the classification tasks to improve the generalizability
of meta-learner. All these methods stay at the model level. However, we solve the meta-overfitting
problem from the gradient aspect. We propose Eigen-Reptile, which updates the meta-parameters by
the main direction of task-specific parameters to alleviate meta-learner overfit on noise.
Learning with noisy labels. Learning with noisy labels has been a long-standing problem. There
are many approaches to solve it, such as studying the denoise loss function (Hendrycks et al., 2018;
Patrini et al., 2017; Jindal et al., 2016; Patrini et al., 2017; Wang et al., 2019b; Arazo et al., 2019),
relabeling (Lin et al., 2014), and so on. Nevertheless, most of these methods require much data
for each class. Gao et al. (2019) proposes a model for noisy few-shot relation classification but
without good transferability. For noisy FSL, a gradient-based meta-learner is trained to optimize an
initialization on various tasks with noisy labels. As there are few samples of each class, the traditional
algorithms for noisy labels cannot be applied. When the existing gradient-based meta-learning
algorithms, such as Reptile, update meta-parameters, they focus on the samples that generate the
last gradient step. And these samples may be corrupted, which makes the parameters learned by
meta-learner susceptible to noisy labels. For noisy FSL, we propose ISPL based on the idea of
Self-paced Learning (SPL) (Kumar et al., 2010; Khan et al., 2011; Basu & Christensen, 2013; Tang
et al., 2012) to learn more accurate main direction for Eigen-Reptile. ISPL constructs prior models
to decide which sample should be discarded when train task-specific models, and this process can
be regarded as the introspective process of meta-learner. In contrast, the model with SPL learns the
samples gradually from easy to complex, and the model itself decides the order.
3	Preliminaries
Gradient-based meta-learning aims to learn a set of initialization parameters φ that can be adapted
to new tasks after a few iterations. The dataset D is usually divided into the meta-training set
Dmeta-train and meta-testing set Dmeta-test, which are used to optimize meta-parameters and
evaluate its generalization, respectively. For meta-training, we have tasks {Ti}iB=1 drawn from task
distribution p(T), each task has its own train set Dtrain and test set Dtest, and the tasks in meta-
testing Dmeta-test are defined in the same way. Note that there are only a small number of samples
for each task in FSL. Specifically, the N-way K-shot classification task refers to K examples for each
of the N classes. Generally, the number of shots used in meta-training should match the one used
at meta test-time to obtain the best performance (Cao et al., 2019). In this paper, we will increase
the sample size appropriately to get the main direction of individual tasks during meta-training. To
minimize the test loss L(Dtest, φ) of an individual task, meta-parameters need to be updated n times
to get good task-specific parameters φ. That is minimizing
L(Dtest, B) = - NN X E K X log q (y = y | x, φ, φ , φ = U n(D train, φ) ⑴
(x,y)∈Dt
est
where Un represents n inner loop steps through gradient descent or Adam (Kingma & Ba, 2014) on
batches from Dtrain, q{y = y | x, φ, φ) is the predictive distribution. When considering updating
the meta-parameters in the outer loop, different algorithms have different rules. In the case of Reptile,
after n inner loop steps, the meta-parameters can be updated as: φ ‹— φ + β(φ - φ), where β ISa
scalar stepsize hyperparameter that controls the update rate of meta-parameters.
3
Under review as a conference paper at ICLR 2021
4 Eigen-Reptile for clean and corrupted data
The proposed Eigen-Reptile alleviates gradient noise to alleviate meta-learner overfitting on sampling
and label noise. Furthermore, ISPL improves the performance of Eigen-Reptile in noisy FSL.
4.1 The Eigen-Reptile Algorithm
To alleviate gradient noise to improve the generalizability of meta-learner, we propose Eigen-Reptile,
which updates d meta-parameters with the main direction of task-specific parameters. We train the
task-specific model with n inner loop steps that start from the meta-parameters φ with few examples.
Let i-th column W:,i ∈ Rd×1 of parameter matrix W ∈ Rd×n be the parameters after i-th gradient
update, e.g., W:,i = Ui(φ). And treat W:,i as a d-dimensional parameter point wi in the parameter
space. e ∈ Rd×1 is a unit vector that represents the main direction of n parameter points in W .
Intuitively, projecting all parameter points onto e should retain the most information.
We represent the parameter points by a straight line of the form W = w + le, which shows that the
straight line passes through the mean point W and the signed distance of a point W from W is l. Then
We get the loss function J(l1,l2, ∙∙∙ ,ln, e) = Pn=Ik W + ^e - Wi ∣∣2. And determine the signed
distance l of each point by partially differentiating J with respect to li, we get Ii = e>(W$ - W).
Plugging in this expression for li in J, we get
n	nn
J(e) = - Xe>(Wi - W)(Wi - W)Te + X ∣∣ Wi - W ∣∣2 = -e>Se + X ∣∣ Wi - W ∣∣2 (2)
i=1	i=1	i=1
where S = Pn=ι (Wi - W)(Wi — W)T is a scatter matrix. According to Eq.2, minimizing J is
equivalent to maximizing: eτSe. Note that e needs to be roughly consistent with the gradient update
direction V in the process of learning task-specific parameters. Use Lagrange multiplier method as
max eτSe	s.t. { Ve > 0	, where V =L Xc Wn—i - Wi
eτe = 1	bn/2c i=1
We get the objective function
g(μ, e, λ, η) = eτSe — λ(eτe — 1) + μ(-Ve + η2),	where λ = 0,μ ≥ 0
then partially differentiating g in Eq.4 with respect to μ, e, λ, η,
{-Ve + η2 = 0
2Se - 2λe - μV = 0
eτe - 1 = 0
2μη = 0
(3)
(4)
(5)
According to Eq.5, if η = 0, then V and e are orthogonal, which obviously does not meet our
expectations. So we get η 6= 0, and μ = 0. Then Se = λe.We can see e is the eigenvector of S
corresponding to the largest eigenvalue λ, as we need the main direction. It should be noted that even
if Eq.3 is not directly related to the e, in ER, Eq.3 must be retained because it determines the update
direction of the outer-loop. Otherwise, the algorithm will not converge.
A concerned question about Se = λe is that the scatter matrix S ∈ Rd×d grows quadratically
with the number of parameters d. As the high dimensionality of parameters typically used in neural
networks, computing eigenvalue and eigenvector of S could come at a prohibitive cost (the worst-case
complexity is O (d3) ). Centralize W by subtracting the mean W, and scatter matrix S = WWτ.
ττ
To avoid calculating the eigenvector of S directly, we focus on WτW. As Wτ Web = λeb. Multiply
both sides of the equation with W,
WWτ Web = bλ Web	(6)
|{z}	ι{z}ι{z}
e	λe
It can be found from Eq.6 that WτW ∈ Rn×n and WWτ ∈ Rd×d have the same eigenvalue,
τ
λ = λ. Furthermore, we get the eigenvector of WWτ as e = Web. The main advantage of Eq.6 is
4
Under review as a conference paper at ICLR 2021
that the intermediate matrix W > W now grows quadratically with the inner loop steps. As we are
interested in FSL, n is very small. It will be much easier to compute the eigenvector ebof W>W,
O n3 , which can be ignored (detailed analysis refer to Appendix B). Then we get the eigenvector e
of WW > based on b. Moreover, We project parameter update vectors Wi+ι -Wi, i = 1, 2, ∙∙∙ ,n-1
on e to get the corresponding update stepsize ν, so meta-parameters φ can be updated as
φ《——φ + βνe	(7)
Where β is a scalar stepsize hyperparameter that controls the update rate of meta-parameters. The
Eigen-Reptile algorithm is summarized in Algorithm 1 of Appendix A. To illustrate the validity of
Eigen-Reptile theoretically, We present Theorem 1 as folloW:
Theorem 1 Assume that the gradient noise variable x follows Gaussian distribution (Hu et al., 2017;
Jastrzebski et al., 2017; Mandt et al., 2016), X 〜N (0,σ2). Furthermore, X and neural network
parameter variable are assumed to be uncorrelated. The observed covariance matrix C equals
noiseless covariance matrix Ct plus gradient noise covariance matrix Cx. Then, we get
C=Ct+Cx=Pt(Λt+Λx)Pt>	=Pt(Λt+σ2I)Pt>	= PtΛPt>	= PΛP>	(8)
where Pt and P are the orthonormal eigenvector matrices of Ct and C respectively, Λt and Λ are
the corresponding diagonal eigenvalue matrices, and I is an identity matrix. It can be seen from
Eq.8 that C and Ct has the same eigenvectors. We defer the proof to the Appendix C.
Theorem 1 shoWs that eigenvectors are not affected by gradient noise. So Eigen-Reptile can find
a more generalizable starting point for neW tasks Without overfitting on noise.
4.2 The Introspective Self-paced Learning
Self-paced learning (SPL) learns the samples from loW
losses to high losses, Which is proven beneficial in
achieving a better generalization result (Khan et al.,
2011; Basu & Christensen, 2013; Tang et al., 2012).
Besides, the losses of samples are determined by the
model itself. Nevertheless, in meta-learning setting,
meta-learner is trained on various tasks, and initial
model may have loWer losses for trained classes and
higher losses for unseen classes or noisy samples. For
this reason, We cannot train the task-specific model by
the same Way as SPL to solve noisy FSL problem. In
this paper, We use multiple prior models to vote on sam-
ples and decide Which should be abandoned. As shoWn
in Figure 2, even though the tWo categories of yelloW
and green shoW an excellent distribution that can be
Well separated, some samples are marked Wrong. To
address this noisy label problem, We build three prior
models. Specially, We randomly sample three times,
O True samples
□ Sample for model 1 口 Sample for model 3
口 Sample for model 2
Figure 2: Randomly sample examples to build
different prior models.
and model 1 is trained With a corrupted label. Due to different samples learned by prior models,
building multiple models to vote on the data Will obtain more accurate losses. Such a learning process
is similar to human introspection, and We call it Introspective Self-paced Learning (ISPL). Moreover,
samples With losses above a certain threshold Will be discarded. Furthermore, We imitate SPL to add
the hidden variable v = 0 or 1 that is decided by Q prior models before the loss of each sample to
control Whether the sample should be abandoned. So We get the task-specific loss as
hQ
LISPL (φ, V) = EviL (xi,yi, φ), where Vi =argmin i ELj (xi,yi, Φj) - Yvi	(9)
i=1	v Q j=1
where h is the number of samples from dataset Dtrain , γ is the sample selection parameter, which
gradually decreases, parameter of model j is φj = Un(Dj, φ), Dj ∈ Dtrain. Note that we update
the meta-parameters with the model trained on h samples from Dtrain . The ISPL is summarized in
Algorithm 2 of Appendix A. Intuitively, it is difficult to say whether discarding high-loss samples
containing correct samples and wrong samples will improve the accuracy of eigenvector, so we will
use Theorem 2 to prove the effectiveness of ISPL.
5
Under review as a conference paper at ICLR 2021
(a) ER of iteration 1
(b) ER of iteration 15000
(c) ER of iteration 30000
(d) Reptile of iteration 1
(e)	Reptile of iteration 15000
(f)	Reptile of iteration 30000
Figure 3: Eigen-Reptile and Reptile training process on the regression toy test. (a), (b), (c) and (d),
(e), (f) show that after the gradient update 0, 8, 16, 24, 32 times based on initialization learned by
Eigen-Reptile and Reptile respectively.
Theorem 2 Let Wo be the parameter matrix generated by the corrupted samples. Compute the
eigenvalues and eigenvectors of the expected observed parameter matrix
1 E(Ctr)e = Po(I — W)P>e ≈ Po(I — λ01)P>e > Po(I - λ0-ξI)P>e	(10)
λ	λ	λ	λ-ξ
where Ctr is the covariance matrix generated by true samples, λ is the observed largest eigenvalue,
λo is the largest eigenvalue in the corrupted diagonal eigenvalue matrix Λo. According to Eq.21,
if λo∕λ is SmaUer the observed eigenvector e is more accurate. Assume that the discarded high
loss samples have the same contributions ξ to λ and λo, representing the observed and corrupted
main directional variance, respectively. Note that these two kinds of data have the same effect on the
gradient updating of the model, so this assumption is relatively reasonable. Furthermore, it is easy to
find that (λo 一 ξ)∕(λ — ξ) is smaller than λo∕λ.
Theorem 2 shows that discard the high loss samples can help improve the accuracy of the ob-
served eigenvector of parameter matrix learned with corrupted labels. So ISPL can improve the
performance of Eigen-Reptile, as it discards high loss samples in Dtrain .
5 Experimental Results and Discussion
In our experiments, we aim to (1) evaluate the effectiveness of Eigen-Reptile to alleviate gradient
noise (sampling and label noise), (2) determine whether Eigen-Reptile can alleviate gradient noise in
a realistic problem, (3) evaluate the improvement of ISPL to Eigen-Reptile in the presence of noisy
labels, (4) validate theoretical analysis through numerical simulations. The code and data for the
proposed model are provided for research purposes 1.
5.1	Meta-learning with Noise on Regression
In this experiment, we evaluate Eigen-Reptile by the 1D sine wave K-shot regression problem
(Nichol et al., 2018). Each task is defined by a sine curve y(x) = Asin(x + b), where the amplitude
A 〜 U([0.1, 5.0]) and phase b 〜 U([0,2∏]). The amplitude A and phase b are varied between tasks.
The goal of each task is to fit a sine curve with the data points sampled from the corresponding y(x).
We calculate loss in `2 using 50 equally-spaced points from the whole interval [—5.0, 5.0] for each
task. The loss is
5.0
-5.0
k y(x) — yb(x) k2 dx
(11)
1Code is included in the supplemental material. Code will be released upon the paper acceptance.
6
Under review as a conference paper at ICLR 2021
Table 1: Few Shot Classification on Mini-Imagenet N-way K-shot accuracy. The ± shows 95%
Confidence interval over tasks._____________________________________________________________
Algorithm	5-way 1-shot	5-way 5-shot
MAML (Finn et al., 2017)	48.70 ± 1.84%	63.11 ± 0.92%
Relation Network (Sung et al., 2018)	50.44 ± 0.82%	65.32 ± 0.70%
CAML (512) (Zintgraf et al., 2018)	51.82 ± 0.65%	65.85 ± 0.55%
TAML (VL + Meta-SGD) (Jamal & Qi, 2019)	51.77 ± 1.86%	65.60 ± 0.93%
Meta-dropout(Lee et al., 2019)	51.93 ± 0.67%	67.42 ± 0.52%
Warp-MAML (Flennerhag et al., 2019)	52.30 ± 0.8%	68.4 ± 0.6%
MC (128) (Park & Oliva, 2019)	54.08 ± 0.93%	67.99 ± 0.73%
ARML (Yao et al., 2020)	50.42 ± 1.73%	-
ModGrad (64) (Simon et al., 2020)	53.20 ± 0.86%	69.17 ± 0.69
Reptile (32)(Nichol et al., 2018)	49.97 ± 0.32%	65.99 ± 0.58%
Eign-Reptile (32)	51.80 ± 0.9%	68.10 ± 0.50%
Eign-Reptile (64)	53.25 ± 0.45%	69.85 ± 0.85%
where yb(x) is the predicted function that start from the initialization learned by meta-learner.
The K-shot regression task fits a selected sine curve through K points, here K = 10. For the
regressor, we use a small neural network, which is the same as Nichol et al. (2018), except that
the activation functions are Tanh. Specifically, the small network includes an input layer of size 1,
followed by two hidden layers of size 64, and then an output layer of size 1. In this part, we mainly
compare Reptile and Eigen-Reptile. Both meta-learners use the same regressor and are trained for
30000 iterations with inner loop steps 5, batch size 10, and a fixed inner loop learning rate α = 0.02.
We report the results of Reptile and Eigen-Reptile
in Figure 3. It can be seen that the curve fitted by
Eigen-Reptile is closer to the true green curve, which
shows that Eigen-Reptile performs better. According
to Jamal & Qi (2019), the initial model that has a
larger entropy before adapting to new tasks would
better alleviate meta-overfitting. As shown in Figure
3, from 1 to 30000 iterations, Eigen-Reptile is more
generalizable than Reptile as the initial blue line of
Eigen-Reptile is closer to a straight line, which shows
that the initialization learned by Eigen-Reptile is less
affected by gradient noise. Furthermore, Figure 4
shows that Eigen-Reptile converges faster and gets a
lower loss than Reptile.
Figure 4: Loss of the 10-shot regression.
5.2	Meta-learning in Realistic Problem
We evaluate our method on a popular few-shot classification dataset: Mini-ImageNet (Vinyals et al.,
2016). The Mini-Imagenet dataset contains 100 classes, each with 600 images. We follow Ravi &
Larochelle (2016) to divide the dataset into three disjoint subsets: meta-training set, meta-validation
set, and meta-testing set with 64 classes, 16 classes, and 20 classes, respectively. And we follow
the few-shot learning protocols from prior work (Vinyals et al., 2016), except that the number of
the meta-training shot is 15, which is still much smaller than the number of samples required by
traditional tasks. Moreover, we run our algorithm on the dataset for the different number of shots
and compare our results to the state-of-the-art results. What needs to be reminded is that approaches
that use deeper, residual networks can achieve higher accuracies (Gidaris & Komodakis, 2018), so
for a fair comparison, we only compare algorithms that use convolutional networks as Reptile does.
Specifically, our model follows Nichol et al. (2018), which has 4 modules with a 3 × 3 convolutions
and 64 filters, 2 × 2 max-pooling etc.. The images are downsampled to 84 × 84, and the loss function
is the cross-entropy error. We use the Adam optimizer with β1 = 0 in the inner loop. Our model
7
Under review as a conference paper at ICLR 2021
Table 2: Average test accuracy of 5-way 1-shot on the Mini-Imagenet with symmetric label noise.
Algorithm	p=0.0	p=0.1	p=0.2	p=0.5
Reptile (Nichol et al., 2018)	47.64%	46.08%	43.49%	23.33%
Reptile+ISPL	47.23%	46.50%	43.70%	21.83%
Eign-Reptile	47.87%	47.18%	45.01%	27.23%
Eigen-Reptile+ISPL	47.26%	47.20%	45.49%	28.68%
is trained for 100000 iterations with a fixed inner loop learning rate 0.0005, and 7 inner-loop steps.
Regarding some hyperparameters analysis, defer to Appendix E.
The results of Eigen-Reptile and other meta-learning approaches are summarized in Table 1. The pro-
posed Eigen-Reptile (64 filters) outperforms and achieves highly competitive performance compared
with other algorithms for 5-shot and 1-shot classification problems, respectively. More specifically,
for 1-shot, the result of MC (128 filters) with a higher capacity network is better than that of Eigen-
Reptile. However, as a second-order optimization algorithm, the computational cost of MC will be
much higher than Eigen-Reptile. And obviously, the result of Eigen-Reptile is much better than
Reptile for each task. Compared with Reptile, Eigen-Reptile uses the main direction to update
the meta-parameters to alleviate the meta-overfitting caused by gradient noise. More importantly,
Eigen-Reptile outperforms the state-of-the-art meta-overfitting preventing method Meta-dropout (Lee
et al., 2019), which is based on regularization. This result shows that the effectiveness of addressing
the meta-overfitting problem from the perspective of alleviating gradient noise.
5.3	Meta-learning with Label Noise
We conduct the 5-way 1-shot experiment with noisy labels generated by corrupting the original labels
of Mini-Imagenet. More specifically, in this section, we only focus on symmetric label noise, as
correct labels are flipped to other labels with equal probability, i.e., in the case of symmetric noise
of ratio p, a sample retains the correct label with probability 1 - p. It becomes some other label
with probability p/(N - 1). An example of symmetric noise is shown in Figure 6. Furthermore, the
asymmetric label noise experiment is conducted in Appendix F. Note that we only introduce noise in
the train set during meta-training, where the meta-training shot is 30. Moreover, all meta-learners
with 32 filters are trained for 10000 iterations, with a learning rate of 0.001 in the inner loop. The
sample selection parameter γ = 10 that decreases by 0.6 every 1000 iterations. The other settings of
this experiment are the same as in section 5.2.
As shown in Table 3, with the increase of the ratio p, the performance of Reptile decreases rapidly.
When p = 0.5, the initialization point learned by Reptile can hardly meet the requirements of quickly
adapting to new tasks with few samples. On the other hand, Eigen-Reptile is less affected by noisy
labels than Reptile, especially when the noise ratio is high, i.e., p = 0.5. The experimental results also
verify the effectiveness of ISPL, as Eigen-Reptile+ISPL achieves better results than Eigen-Reptile
when p 6= 0. It also can be seen that ISPL plays a more significant role when p is higher. However,
when p = 0, ISPL harms Eigen-Reptile, as ISPL only discards correct samples. In addition, ISPL
does not significantly improve Reptile, especially when p = 0.5. This is because too many high-loss
samples are removed, causing Reptile to fail to converge quickly with the same number of iterations.
These experimental results show that Eigen-Reptile and ISPL can effectively alleviate the gradient
noise problem caused by noisy labels, thereby alleviating the meta-overfitting on corrupted samples.
6 Conclusion
In this paper, we cast the meta-overfitting problem (overfitting on sampling and label noise) as a
gradient noise problem. Then, we propose a gradient-based meta-learning algorithm Eigen-Reptile.
It updates the meta-parameters through the main direction, which has been proven by theory and
experiments that it can alleviate the gradient noise effectively. Furthermore, to get closer to real-world
situations, we introduce noisy labels into the meta-training dataset, and the proposed ISPL constructs
prior models to select samples for Eigen-Reptile to get more accurate main direction.
8
Under review as a conference paper at ICLR 2021
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016.
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.
Sumit Basu and Janara Christensen. Teaching classification boundaries to humans. In Twenty-Seventh
AAAI Conference on Artificial Intelligence, 2013.
Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.
Tianshi Cao, Marc Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot
learning. arXiv preprint arXiv:1909.11722, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems, pp. 9516-9527, 2018.
Sebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia
Hadsell. Meta-learning with warped gradient descent. arXiv preprint arXiv:1909.00025, 2019.
Benoit Frenay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE
transactions on neural networks and learning systems, 25(5):845-869, 2013.
Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun. Hybrid attention-based prototypical networks
for noisy few-shot relation classification. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 6407-6414, 2019.
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367-4375,
2018.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep networks on labels corrupted by severe noise. In Advances in neural information processing
systems, pp. 10456-10465, 2018.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.
In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.
Shell Xu Hu, Pablo G Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D Lawrence, and
Andreas Damianou. Empirical bayes transductive meta-learning with synthetic gradients. arXiv
preprint arXiv:2004.12696, 2020.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex
stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
Muhammad Abdullah Jamal and Guo-Jun Qi. Task agnostic meta-learning for few-shot learning.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 11719-
11727, 2019.
StaniSlaW JaStrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, ASja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Ishan Jindal, MattheW Nokleby, and XueWen Chen. Learning deep netWorks from noisy labels With
dropout regularization. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pp.
967-972. IEEE, 2016.
9
Under review as a conference paper at ICLR 2021
Faisal Khan, Bilge Mutlu, and Jerry Zhu. How do humans teach: On curriculum learning and teaching
dimension. In Advances in neural information processing Systems, pp. 1449-1457, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in Neural Information Processing Systems, pp. 1189-1197, 2010.
Hae Beom Lee, Taewook Nam, Eunho Yang, and Sung Ju Hwang. Meta dropout: Learning to perturb
latent features for generalization. In International Conference on Learning Representations, 2019.
HB Lee, T Nam, E Yang, and SJ Hwang. Meta dropout: Learning to perturb latent features for
generalization. In International Conference on Learning Representations, 2020.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot
learning. arXiv preprint arXiv:1707.09835, 2017.
Christopher H Lin, Daniel S Weld, et al. To re (label), or not to re (label). In Second AAAI conference
on human computation and crowdsourcing, 2014.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. arXiv
preprint arXiv:1805.10002, 2018.
Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gradient
algorithms. In International conference on machine learning, pp. 354-363, 2016.
Igor Mordatch. Concept learning with energy-based models. arXiv preprint arXiv:1811.02486, 2018.
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint
arXiv:1511.06807, 2015.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Boris Oreshkin, PaU ROdrigUez L6pez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp.
721-731, 2018.
EUnbyUng Park and JUnier B Oliva. Meta-cUrvatUre. In Advances in Neural Information Processing
Systems, pp. 3314-3324, 2019.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen QU. Making
deep neUral networks robUst to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit
gradients. In Advances in Neural Information Processing Systems, pp. 113-124, 2019.
Sachin Ravi and HUgo Larochelle. Optimization as a model for few-shot learning. 2016.
Avinash Ravichandran, RahUl Bhotika, and Stefano Soatto. Few-shot learning with embedded class
models and shot-free meta training. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 331-339, 2019.
Adam Santoro, Sergey BartUnov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-aUgmented neUral networks. In International conference on machine
learning, pp. 1842-1850, 2016.
10
Under review as a conference paper at ICLR 2021
Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In
International Conference on Learning Representations, 2018.
Pranav Shyam, Shubham Gupta, and Ambedkar Dukkipati. Attentive recurrent comparators. In
Proceedings ofthe 34th International Conference on Machine Learning-Volume 70, pp. 3173-3181.
JMLR. org, 2017.
Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. On modulating the gradient
for meta-learning. In European Conference on Computer Vision, pp. 556-572. Springer, 2020.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in neural information processing systems, pp. 4077-4087, 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1199-1208, 2018.
Ye Tang, Yu-Bin Yang, and Yang Gao. Self-paced dictionary learning for image classification. In
Proceedings of the 20th ACM international conference on Multimedia, pp. 833-836, 2012.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Yaqing Wang, Quanming Yao, J Kwok, and LM Ni. Few-shot learning: A survey. arXiv preprint
arXiv:1904.05046, 2019a.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 322-330, 2019b.
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the
noisy gradient descent that generalizes as sgd. 2019.
Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu. Dpgn: Distribution
propagation graph network for few-shot learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 13390-13399, 2020.
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning.
arXiv preprint arXiv:1905.05301, 2019.
Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, and Zhenhui Li. Automated
relational meta-learning. arXiv preprint arXiv:2001.00745, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Caml:
Fast context adaptation via meta-learning. 2018.
11
Under review as a conference paper at ICLR 2021
A PSEUDO-CODE
Algorithm 1 Eigen-Reptile
Require: Distribution over tasks P(T), outer step size β 1:	Initialize meta-parameters φ 2:	while not converged do 3:	W = [ ], ν = 0 4:	Sample batch of tasks {Ti}B=ι 〜P(T) 5:	for each task Ti do 6:	φi = φ 7:	Sample train set Dtrain ofTi 8:	forj = 1, 2, 3, ..., n do 9:	φij = Uj(Dtrain,φi) 10:	W appends wj = f latten(φij ), wj ∈ Rd×1 11:	end for 12:	Mean centering, W = W 一 w, W ∈ Rd×1 > 13:	Compute eigenvalue matrix Λ and eigenvector matrix P of scatter matrix W> W 14:	Eigenvalues λι > λ2 > …> λn in Λ 15:	Compute eigenvector matrix of WW>, P = WPb 16:	Let the eigenvector corresponding to λ1 be a unit vector k ei1 k22= 1 17:	forj = 1, 2, 3, ..., n 一 1 do 18:	ν = ν + (W:,j+1 一 W:,j)ei1 19:	end for 20:	e1 = Pm=I λm × e1	_ 21:	Calculate the approximate direction of task-specific gradient update V: 22:	V = bn∕2c Pb=P W：,n-i+1 一 W:,i 23:	if el ∙ V < 0 then 24:	e1 = -e1 25:	end if 26:	end for 27:	Average the main directions to get e = (1/B) PB=I e； 28:	Update meta-parameters φ4一 φ + β X v/B X e 29:	end while
Algorithm 2 Introspective Self-paced Learning
Require: Dataset Dtrain, initialization φ, batch size b, selection parameter Y, attenuation coefficient
μ, the number of prior models Q
1:	Initialize network parameters φ* = φ fora sampled task
2:	for j = 1, 2,3,…，Q do
3:	Sample examples Dj from Dtrain for training modelj, φj = Um(Dj, φ*)
4:	end for
5:	Train task-specific parameters:
6:	fθr i = 1, 2, 3,…,n do
7:	Compute hidden variable vector v:
8:	V = argminv VqPq=I Lq - YPq=I Vq, where Lq =吉 PQ=I Lj (Xq,y, φj∙)
9:	Update task-specific parameters φ*:
10:	Φ* = argminφ* LISPL (φ*,v)
11:	γ = γ — μ
12:	end for
B	Algorithm Complexity Analysis
As for Eigen-Reptile, the cost of single gradient descent in the inner-loop is O(d), where d is
the number of network parameters. The cost of the covariance matrix computations is O(n2d),
12
Under review as a conference paper at ICLR 2021
where n is the number of inner-loop. Moreover, the worst-case complexity of computing eigenvalue
decomposition is O(n3). Finally, the computational complexity of restoring eigenvector is O(nd).
We set the maximal number of outer-loop to T. Hence the overall time complexity is O(T (n2d +
n3 + nd + nd)). As in FSL, n is usually less than 10 (for this paper n = 7 ), so the overall time
complexity is O(T d). As for Reptile, the computational complexity is also O(T d), which means
that the time complexity of both Reptile and Eigen-Reptile is much lower than the second-order
optimization algorithms.
As for spatial complexity, Eigen-Reptile needs to store a d × n matrix and a n × n matrix. The overall
space complexity is O(d), while the spatial complexity of Reptile is O(d), too.
It can be seen that, compared to Reptile, Eigen-Reptile is the same in spatial complexity and time
complexity. Still, its accuracy is much higher than that of Reptile.
C	Theorem 1
Gradient update always with gradient noise inserted at every iteration, which caused Reptile, MAML,
etc. cannot find accurate directions to update meta-parameters. In this section, we will prove that
Eigen-Reptile can alleviate gradient noise.
Theorem 3 Assume that the gradient noise variable x follows Gaussian distribution (Hu et al., 2017;
Jastrzebski et al., 2017; Mandt et al., 2016), X 〜N(0, σ2). Furthermore, X and neural network
parameter variable are assumed to be uncorrelated. The observed covariance matrix C equals
noiseless covariance matrix Ct plus gradient noise covariance matrix Cx. Then, we get
C=Ct+Cx	=	Pt(Λt	+ Λx)Pt>	=	Pt(Λt +	σ2I)Pt>	=PΛP> = PtΛPt>	(12)
where Pt and P are the orthonormal eigenvector matrices of Ct and C respectively, Λt and Λ are
the corresponding diagonal eigenvalue matrices, and I is an identity matrix. It can be seen from
Eq.12 that C and Ct has the same eigenvectors.
Proof C.1 In the following proof, we assume that the probability density function of gradient noise
variable X follows Gaussian distribution, X 〜N(0, σ2). Treat the parameters in the neural network
as variables, and the parameters obtained by each gradient update as samples. Furthermore, gradient
noise and neural network parameters are assumed to be uncorrelated.
For observed parameter matrix W ∈ Rd×n, there are n samples, let Wi,: ∈ R1×n be the observed
values of the i-th variable Wi, and W = [W[：,…，W>,…，W>：]>. Similarly, we denote the
noiseless parameter matrix by Wt = [(W]J>, ∙∙∙ , (W[)>, ∙∙∙ , (Wt,： )>]>, and
W = Wt + X	(13)
Where X = [X>：,…，X>,…，X>：]> is the dataset of noise variables. Then, centralize each
variable by
_	1 n
Wk = Wk- n NWk,：(i)	(14)
So we get W = [W>, ∙∙∙ , W>]>. Suppose Wt is also centralized by the same way and get
>	>^
Wt = [Wtι ,…，Wtd ]>. Then, we have:
W = Wt + X	(15)
Computing the covariance matrix of W:
C = - WW >
n
=-(Wt + X)(Wt > + X >)	(16)
n
=-(W tWt > + Wt X > + XWt > + XX >)
n
13
Under review as a conference paper at ICLR 2021
Since Wt and X are uncorrelated, WtX> and XWt> are approximately zero matrices. Thus:
C ≈ 1 (WtWt> + XX>) = Ct + Cx	(17)
n
The component Cx(i,j) is the correlation between Xi and Xj which corresponds to the i-th and j-th
rows of X. As the two noise variables are not related to each other, if i 6= j, then Cx(i,j) = 0. So
Cx ∈ Rd×d is a diagonal matrix with diagonal elements σ2. Decompose Ct as:
Ct = PtΛtPt>	(18)
where Pt is the noiseless orthonormal eigenvector matrix and Λt is the noiseless diagonal eigenvalue
matrix, then
Cx = ΛxPtPt> = PtΛxPt> = PtCxPt>	(19)
where Λx = σ2I, and I is the identity matrix. Thus,
C = Ct + Cx
=PtΛtPt>+PtΛxPt>.	(20)
= Pt(Λt + Λx)Pt>.
= PtΛPt>
where Λ = Λt + Λx. It can be seen from Eq.20 that C and Ct has the same eigenvector matrix. In
other words, eigenvector is not affected by gradient noise.
D Theorem 2
In this section, we will prove that discarding high loss samples will result in a more accurate main
direction in noisy FSL.
Theorem 4 Let Wo be the parameter matrix generated by the corrupted samples. Compute the
eigenvalues and eigenvectors of the expected observed parameter matrix
1	E(Ctr)e = Po(I - W)P>e ≈ Po(I - λ01)P>e > Po(I - λO-ξI)P>e	(21)
λ	λ	λ	λ-ξ
where Ctr is the covariance matrix generated by true samples, λ is the observed largest eigenvalue,
λo is the largest eigenvalue in the corrupted diagonal eigenvalue matrix Λo. According to Eq.21,
if λo∕λ is smaller, the observed eigenvector e is more accurate. Assume that the discarded high
loss samples have the same contributions ξ to λ and λo, representing the observed and corrupted
main directional variance, respectively. Note that these two kinds of data have the same effect on the
gradient updating of the model, so this assumption is relatively reasonable. Furthermore, it is easy to
find that (λo 一 ξ)∕(λ — ξ) is smaller than λo∕λ.
Proof D.1 Here, we use w to represent the parameter point obtained after a gradient update. For
convenience, let w be generated by a single sample, w ∈ Rd×1. Then the parameter matrix can be
obtained,
W =	[wtr	w2r	…	wo	…	Wm	…	Wnr ]	(22)
where wo represents the parameters generated by the corrupted sample, and wtr represents the
parameters generated by the true sample. Furthermore, there are n parameter points generated by
n samples. Moreover, there are m corrupted parameter points generated by m corrupted samples.
Mean centering W, and show the observed covariance matrix C as
(W1tr)>
(W2tr)>
.
.
.
(Wntr)>
(23)
C = 1WW>
n
=11Wtr	Wtr	…wnr]
n
=1(wtr (Wtr)> + …+Wo(Wo)> + …+Wm (Wm)> + …+Wnr (Wnr)>)
n	mm	n n
14
ST
(OE)
0< (I)Y ：
0 (°Y-Y)5
/a?" əj,o/ə^
三£
5-oY oY
ld§ (SutpjVDstp i∂ifυ puυ ∂iofoq səniv^uə^iə
Jo Oiim a甲 əmduioj 'jəpom a中 Jo 8uιιυpdn ιu∂jpmS 叫]uo ioəffə əuivs a中 弘叫 mnp Jo SPU碑 oʌu
əsəi[i sυ i°∖ pm χ o吟 suoμnqμιuoo əuivs 叫* əsni[ səjdtuvs ssoj i[Sji[ pəpwosjp 叫* ιυψ 9uιnssy
's∂jduιυs ^sjou pm səjdtuvs ənn uwιuoo 3ιu IPmM is∂ssoj ?s沏q əi[i ψ∕i¼ səjdtuvs əuios pivosiQ
`əimnoov əioui si ə UafIinUS si ∖∕oχ fi 百乙力4。？ Smpioooy `ɑʊ Jo xμιυm 乂o*aaAna8?a WULIOnσψκo
əi[l si Od(ON XK职Ul ㈣修仙海屹ιυuoSmp pəidnɑoo a琳 m ㈣修仙海屹 招沟已] 叫* si o∖ a/叫M
əʃef(lɪ-l)ɑef ≈
(6Z)
≡7cf(ov∣-ι)ocf =
a(ots∣ -I) = a('*,a)a∣
Y &q uo]iυnb∂
əi[l Jb səpjs ψoq əpi^i(j 'ati[DAua讥a Smpuodsdiioo 叫* si ∖ UoiodsudSp p∂ΛΛ∂sqo 叫* si ə a/叫M
(8Z)
(ZZ)
əv = a((3)回 +。U)
ld§ am i3∂n↑υ^ əimifəp sn ∂nιυΛU∂Sp pun λoio∂^1∂^i∂ w∂λjj
o15 + di)回 = (ɔ)a
isυ u∂∏im ∂q uno ɔ fo IiOWmadXa ∂ψ OS 囚o ∂q ɪaiaiɪ ənn ↑↑υ Jo tuns ∂ψ i∂↑ puy
(τ⅛p⅛)aτ^2I
(9Z)
(p⅛z⅛)a^2I
(p⅛τ⅛)a ™Z
Ua甲 i°ι^
(5 Pr>)回
∂q Qmm)眠 PaIdtLLl03 H^fo tuns 8甲 场”
(IMS回-
(SC)
(Pr>5)回
z(⅛)a + ^
(⅛τ≈,)a
(τ≈,⅛)a
z(τ°)a + ⅛-
=(ɪaiaɪ)a


ʊ
iutod Λ∂i∂uιmυd pdidnnoə ∂rqι
Ui s∂iqυuυx IW Jo suonmo∂dx∂ 叫「 əindiuoj « si s∂iqυuυx ιuιodiəiəuimvdpəidnɑoo Jo ∂ouυμυx
∂i[i ιυψ əmnssv iuoιιo∂Λip ^uυ m ∂xom oι ιu∂ipm§ a甲 ∂snυo ^υuι Maqt)I ^sιou pəiməuə^ MulOPUtiI
əi[l pm i^ιιuυ3ifιuSιs ∂Suυψ siəiəiumvd ppoιu 叫* a耳OlU ιυψ səjdtuvs SSol ι∣3叫 IW pivəsip əa^ SW
%foPfo
Wυ~∖
Pυ^υ
PIDIlD
【管4笛

M)
{Pυ^∖


^αιαι
Tp
叫 Od Λ∂i∂uιmυd
dl§uis υ 'S9jduιυs ^sιou 叫* ιuoιf p∂uwιqo siəiəiumvd a琳 pm səjdtuvs ənn 叫* rnoιf p∂uwιqo
SΛ∂i∂uιmυd 叫ι oι p∂iυ↑∂Λ si ∕cw他阚a pəimbəi 剪"邮 ɔ Jo uoiiisoduioəəp 冲 moi/uəəs ∂q uυo ʧ
【COZ mɔi JB jədŋd əɔuəjəjuoɔ B sb mətaəj jəpun
Under review as a conference paper at ICLR 2021
Obviously, λ > λo, and if we don’t discard all samples, then λ > ξ. So Eq.30> 0, which means
discarding high loss samples could reduce λo /λ. Therefore, discarding high loss samples can
improve the accuracy of eigenvector in the presence of noisy labels.
For further analysis, we assume that any two variables are independently and identically distributed,
the expectation of variable a, E(a) = . Thus,
2
2
1 ω = P
λ s’。= λ
δ2 + 2
2
(31)
2
δ2 + 2
where P is the proportion ofnoisy labels, np = m. As can be seen from Eq.31, if pe2∕λ ≈ 0, then
Ωo∕λ is a diagonal matrix. According to proof. C.1, the observed eigenvector e is unaffected by
noisy labels with the corresponding eigenvalue p(δ ；' ).
E	Hyperparameters of Eigen-Reptile
In this section, we follows Lee et al. (2019); Cao et al. (2019) to vary the number of inner-loops
and the number of corresponding training shots to show the robustness of Eigen-Reptile. Besides,
other hyperparameters are the same as section 5.2. As shown in Figure 5, after the number of
inner-loops i reaches 7, the test accuracy tends to be stable, which shows that changing the number of
inner-loops within a certain range has little effect on Eigen-Reptile. That is, Eigen-Reptile is robust
to this hyperparameter. As for train shot, to make the trained task-specific parameters as unbiased as
possible, We specify train shot roughly satisfies d ixbatch-Size ] + 1, where N is the number of classes.
So when i = 7, the number of train shots is 15. It is important to note that in our experiments, Reptile
uses the original implementation hyperparameters, the number of inner-loops is 8, the number of
train shots is 15 and the corresponding accuracy is 65.99%.
Figure 5: The number of inner-loop and accuracy of 5-way 5-shot task on Mini-Imagenet.
F	The Asymmetric Label Noise Experiment
Since asymmetric noise is as common as symmetric noise, we focus on asymmetric noise in this
section. As illustrated in asymmetric noise of Figure 6, we randomly flip the labels of one class to the
labels of another class without duplication in the meta-training dataset (64 classes).
16
Under review as a conference paper at ICLR 2021
(b) asymmetric noise
Figure 6: Examples of noise transition matrix (taking 5 classes and noise ratio 0.2 as an example).
We compare our algorithms with Reptile in table 3. We observe that meta-learning algorithms with
asymmetric noise have closer results compared to symmetric noise. As meta-learner is trained on
tasks with the same noise transition matrix, which allows the meta-learner to learn more useful
information, the results are higher than that with symmetric noise. Similar to the symmetric noise
results, Eigen-Reptile outperforms Reptile in all tasks, and ISPL plays a more significant role when p
is higher. The experimental results also show that when the number of iterations is constant, ISPL
does not significantly improve or even degrades the results of Reptile. On the contrary, ISPL can
provide Eigen-Reptile with a more accurate main direction in difficult tasks, e.g., p = 0.2, 0.5.
Table 3: Average test accuracy of 5-way 1-shot on the Mini-Imagenet with asymmetric label noise.
AlgOrithm	p=0.1	p=0.2	p=0.5
Reptile (NiChOl et al., 2018)	47.30%	45.51%	42.03%
Reptile+ISPL	47.00%	45.42%	41.09%
Eign-Reptile	47.42%	46.50%	42.29%
Eigen-Reptile+ISPL	47.24%	46.83%	43.71%
G Meta-learning on CIFAR-FS
Bertinetto et al. (2018) propose CIFAR-FS (CIFAR100 few-shots), which is randomly sampled
from CIFAR-100 (Krizhevsky et al., 2009), containing images of size 32 × 32. The settings of
Eigen-Reptile in this experiment are the same as in 5.2. Moreover, we do not compare algorithms
with additional tricks, such as higher way (Bertinetto et al., 2018; Snell et al., 2017). It can be seen
from Table 4 that on CIFAR-FS, the performance of Eigen-Reptile is still far better than Reptile
without any parameter adjustment.
Table 4: Few Shot Classification on CIFAR-FS N-way K-shot accuracy. The ± shows 95%
COnfidenCe interval Over tasks.________________________________________________________________
AlgOrithm	5-way 1-shOt	5-way 5-shOt
MAML (Finn et al., 2017)	58.90 ± 1.90%	71.50 ± 1.00%
PROTO NET(Snell et al., 2017)	55.50 ± 0.70%	72.00 ± 0.60%
GNN(SatOrras & EstraCh, 2018)	61.90%	75.30%
Embedded Class MOdels(RaviChandran et al., 2019)	55.14 ± 0.48%	71.66 ± 0.39%
RePtile (NichOl et al., 2018)	58.30 ± 1.20%	75.45 ± 0.55%
Eign-Reptile (64)	61.90 ± 1.40%	78.30 ± 0.50%
Eign-Reptile (128)	62.30 ± 1.40%	78.55 ± 0.45%
17
Under review as a conference paper at ICLR 2021
H	Neural Network Architectures
This section will show the performance of Eigen-Reptile on Mini-Imagenet when using a larger
network as CAML, etc. Note that we only compare our algorithm with meta-learning algorithms based
on the gradient in this section. As shown in Table 5, when Eigen-Reptile uses a larger convolutional
neural network (CNN), higher accuracy can be obtained, which shows that Eigen-Reptile benefits
from increasing model expressiveness.
Table 5: Few Shot Classification on Mini-Imagenet N-way K-shot accuracy. The ± shows 95%
Confidence interval over tasks._______________________________________________________________
Algorithm	Backbone	5-way 1-shot	5-way 5-shot
MAML (Finn et al., 2017)	Conv-4-64	48.7 ± 1.8%	63.1 ± 0.9%
Meta-SGD (Li et al., 2017)	Conv-4-64	50.47 ± 1.87%.	64.03 ± 0.94%
CAML(Zintgraf et al., 2018)	Conv-4-32	47.24 ± 0.65%	59.05 ± 0.54%
CAML(Zintgraf et al., 2018)	Conv-4-512	51.82 ± 0.65%	65.85 ± 0.55%
iMAML (Rajeswaran et al., 2019)	Conv-4-64	49.30 ± 1.88%	-
MC (128) (Park & Oliva, 2019)	Conv-4-128	54.08 ± 0.93%	67.99 ± 0.73%
HSML(Yao et al., 2019)	Conv-4-32	50.38 ± 1.85%	-
Reptile(Nichol et al., 2018)	Conv-4-32	49.97 ± 0.32%	65.99 ± 0.58%
Eign-Reptile	Conv-4-32	51.80 ± 0.90%	68.10 ± 0.50%
Eign-Reptile	Conv-4-64	53.25 ± 0.45%	69.65 ± 0.85%
18