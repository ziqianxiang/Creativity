Under review as a conference paper at ICLR 2021
MQES: Max-Q Entropy Search for Efficient
Exploration in Continuous Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
The principle of optimism in the face of (aleatoric and epistemic) uncertainty has
been utilized to design efficient exploration strategies for Reinforcement Learn-
ing (RL). Different from most prior work targeting at discrete action space, we
propose a generally information-theoretic exploration principle called Max-Q En-
tropy Search (MQES) for continuous RL algorithms. MQES formulates the explo-
ration policy to maximize the information about the globally optimal distribution
of Q function, which could explore optimistically and avoid over-exploration by
recognizing the epistemic and aleatoric uncertainty, respectively. To make MQES
practically tractable, we firstly incorporate distributional and ensemble Q func-
tion approximations to MQES, which could formulate the epistemic and aleatoric
uncertainty accordingly. Then, we introduce a constraint to stabilize the train-
ing, and solve the constrained MQES problem to derive the exploration policy in
closed form. Empirical evaluations show that MQES outperforms state-of-the-art
algorithms on Mujoco environments.
1 Introduction
In Reinforcement Learning (RL), one of the fundamental problems is exploration-exploitation
dilemma, i.e., the agents explore the states with imperfect knowledge to improve future reward
or instead maximize the intermediate reward at the perfectly understood states. The main obstacle
of designing efficient exploration strategies is how the agents decide whether the unexplored states
leading high cumulative reward or not.
Popular exploration strategies, like -greedy (Sutton & Barto, 1998) and sampling from stochastic
policy (Haarnoja et al., 2018), lead to undirected exploration through additional random permuta-
tions. Recently, uncertainty of systems are introduced to guide the exploration (Kirschner & Krause,
2018; Mavrin et al., 2019; Clements et al., 2019; Ciosek et al., 2019). Basically, as Moerland et al.
(2017) points out, two source of uncertainty exists in the RL system, i.e., epistemic and aleatoric
uncertainty. Epistemic uncertainty is also called parametric uncertainty, which is the ambiguity of
models arisen from the imperfect knowledge to the environment, and can be reduced with more
data. Aleatoric uncertainty is an intrinsic variation associated with the environment, which is caused
by the randomness of environment, and is not affected by the model. In the RL system, if the
states are seldom visited, the epistemic uncertainty at these states are relatively large. Hence, the
exploration methods should encourage exploration when epistemic uncertainty is large. Moreover,
heteroscedastic aleatoric uncertainty means that different states may have difference randomness,
which renders different aleatoric uncertainty. If we do not distinguish these two uncertainties and
formulate them separately, we may explore the states visited frequently but with high randomness,
i.e., low epistemic uncertainty and high aleatoric uncertainty, which is undesirable.
By introducing uncertainty, the exploration objectives like Thompson Sampling (TS) (Thompson,
1933; Osband et al., 2016) and Upper Confidence Bound (UCB) (Auer, 2002; Mavrin et al., 2019;
Chen et al., 2017) are utilized to guide the exploration in RL. However, since the aleatoric uncer-
tainty in the RL systems are heteroscedastic, i.e., the aleatoric uncertainty depends on states and
actions and can be different, the above methods are not efficient. Hence, Nikolov et al. (2019)
proposes novel exploration objective called Information-Directed Sampling (IDS) accounting for
1
Under review as a conference paper at ICLR 2021
epistemic uncertainty and heteroscedastic aleatoric uncertainty. However, these methods (Nikolov
et al., 2019; Mavrin et al., 2019; Chen et al., 2017; Osband et al., 2016) can only be applied in the
environment with discrete action space.
In this paper, we propose a generally information-theoretic principle called Max-Q Entropy Search
(MQES) for off-policy continuous RL algorithms. Further, as an application example of MQES, we
combine distributional RL with soft actor-critic method, where the epistemic and aleatoric uncer-
tainty are formulated accordingly. Then, we incorporate MQES to Distributional Soft Actor-Critic
(DSAC) (Ma et al., 2020) method, and show how MQES utilizes both uncertainty to explore. Finally,
our results on Mujoco environments show that our method can substantially outperform alternative
state-of-the-art algorithms.
2 Related Work
Efficient exploration can improve the efficiency and performance of RL algorithms. With the in-
creasing emphasis on exploration efficiency, various exploration methods have been developed. One
kind of methods use intrinsic motivation to stimulate agent to explore from different perspectives,
such as count-based novelty (Martin et al., 2017; Ostrovski et al., 2017; Bellemare et al., 2016; Tang
et al., 2017; Fox et al., 2018), prediction error (Pathak et al., 2017), reachability (Savinov et al.,
2019) and information gain on environment dynamics (Houthooft et al., 2016). Some recently pro-
posed methods in DRL, originating from tracking uncertainty, do efficient exploration under the
principle of OFU (optimism in the face of uncertainty), such as Thompson Sampling (Thompson,
1933; Osband et al., 2016), IDS (Nikolov et al., 2019; Clements et al., 2019) and other customized
methods (Moerland et al., 2017; Pathak et al., 2019).
Methods for tracking uncertainty. Bootstrapped DQN (Osband et al., 2016) combines Thompson
sampling with value-based algorithms in RL. It is similar to PSRL (Strens, 2000; Osband et al.,
2013), and leverages the uncertainty produced by the value estimations for deep exploration. Boot-
strapped DQN has become the common baseline for lots of recent works, and also the well-used
approach for capturing epistemic uncertainty (Kirschner & Krause, 2018; Ciosek et al., 2019). How-
ever, this takes only epistemic uncertainty into account.
Distributional RL approximates the return distribution directly, such as Categorical DQN (C51)
(Bellemare et al., 2017), QR-DQN (Dabney et al., 2018b) and IQN (Dabney et al., 2018a). Re-
turn distribution can be used to approximate aleatoric uncertainty, but those methods do not take
advantage of the return distribution for exploration.
Exploration with two types of uncertainty. Traditional OFU methods either focus only on the
epistemic uncertainty, or consider the two kinds of uncertainty as a whole, which can easily lead the
naive solution to favor actions with higher variances. To address that, Mavrin et al. (2019) studies
how to take advantage of distributions learned by distributional RL methods for efficient exploration
under both kinds of uncertainty, proposing Decaying Left Truncated Variance (DLTV).
Nikolov et al. (2019) and Clements et al. (2019) propose to use Information Direct Sampling
(Kirschner & Krause, 2018) for efficient exploration in RL (IDS for RL), which estimate both kinds
of uncertainty and use IDS to make decision for acting with environment. We refer to the practice
of uncertainty estimation in (Clements et al., 2019) as shown in Sec. 4.2.1. IDS integrates both
uncertainty and has made progress on the issue of exploration, but this is limited on discrete action
space. We do focus on how best to exploit both uncertainty for efficient exploration in a continuous
action space in our paper.
Optimistic Actor Critic. More closely related to our work is the paper of OAC (Ciosek et al., 2019),
which uses epistemic uncertainty to build the upper bound of Q estimation QUB . OAC is based
on Soft Actor-Critic (SAC) (Haarnoja et al., 2018), additionally proposing exploration bonus to
facilitate exploration. Despite the advantages that OAC has achieved over SAC, it does not consider
the potential impact of the aleatoric uncertainty, which may cause misleading for exploration.
2
Under review as a conference paper at ICLR 2021
3 Preliminaries
3.1 Distributional RL
Distributional RL methods study distributions rather than point estimates, which introduce aleatoric
uncertainty from distributional perspective. There are different approaches to represent distribution
in RL. In our paper, we focus on quantile regression used in QR-DQN (Dabney et al., 2018b), where
the randomness of state-action value is represented by the quantile random variable Z with value
z. Z maps the state-action pair to a uniform probability distribution supported on zi, where zi
indicates the value of the corresponding quantile estimates. If τi is defined as the quantile fraction,
the cumulative probabilities of such quantile distribution is denoted by FZ(zi) = Pr(Z < zi) =
τi = 1/N fori ∈ 1,...,N.
Similar to the Bellman operator in the traditional Q-Learning (Watkins & Dayan, 1992), the distri-
butional Bellman operator TDπ under policy π is given as:
TDZ(st, at) = R(st, at) + YZ(st+ι,at+ι), at+1 〜∏(∙∣st+ι)∙	(1)
Notice that this operates on random variables, =D denotes that distributions on both sides have equal
probability laws. Based on the distributional Bellman operator, Dabney et al. (2018b) proposes
QR-DQN to train quantile estimations via the quantile regression loss, which is denoted as:
NN
LQR(θ) = NN XX[PTi (δi,j)]	⑵
i=1 j=1
where δi,j = R(st, at) + Yzj(st+ι,at+ι; θ) - Zi(st, at； θ), Pτ(u) = U * (T - 1u<o), and Ti means
the quantile midpoints, which is defined as Ti = τi+2+τi.
3.2 Distributional Soft Actor-Critic Methods
Following Ma et al. (2020), Distributional RL has been successfully integrated with soft Actor-
Critic (SAC) algorithm. Here, considering the maximum entropy RL, the distributional soft Bellman
operator TDπS is defined as follows:
TDπSZ(st, at) =D R(st, at) + Y[Z(st+1, at+1) - αlogπ(at+1 |st+1)]
(3)
where at+ι 〜π(∙∣st+ι), st+ι 〜P(∙∣st,at). The quantile regression loss in DSAC is different from
original QR-DQN only on the δi,j by considering the maximum entropy RL framework. DSAC
extends the clipped double Q-Learning proposed on TD3 (Fujimoto et al., 2018) to overcome the
overestimation problem. Two Quantile Regression Deep Q Networks have the same structure that
are parameterized by θk, k = 1, 2. Following the clipped double Q-Learning, the TD-error of DSAC
is defined as:
yi = min zi(st+1 , at+1 ; θk)
k=1,2
(4)
δk,j = R(st, at) + γ[yt - alog∏(at+ι∣St+ι; φ)] - zj(st, at； θk)	(5)
where θ and φ represents their target networks respectively. DSAC has the modified version of
critic, while the update of actors is unaffected. It is worth noticing that the state-action value is the
minimum value of the expectation on certain distributions, as
Q(st, at； θ)
min Q(st, at； θk)
k=1,2
1	N-1
N mιn2 E Zi(St,at； θk)
N = , i=0
Thus, in DSAC, the original problem aims to maximize the following objective function:
Jπ(Φ) = Est〜D,gN[log∏(f (St, et; φ)∣St) - Q(st, f(st, et； φ); θ)],
(6)
(7)
where D is the replay buffer, f(st, et； φ) means sampling action with re-parameterized policy.
3
Under review as a conference paper at ICLR 2021
4	Algorithm
This paper proposes a new exploration principle for continuous RL algorithms, i.e., MQES, which
leverages epistemic and aleatoric uncertainties to explore optimistically and avoid over-exploration.
To make MQES practically tractable, distributional and ensemble Q function approximations are in-
troduced to formulate the epistemic and aleatoric uncertainty accordingly. Nevertheless, a constraint
is introduced in the MQES to stabilize the training, and the approximated exploration policy is de-
rived in the closed form. All these mechanisms are detailed in the following sections accordingly.
4.1	Exploration Strategy: Max-Q Entropy Search
To achieve a better exploration, MQES derives an exploration policy πE which aims at reducing the
epistemic uncertainty and obtain more knowledge of the globally optimal Q function.
Firstly, We define exploration action random variable AE 〜∏e(a|s) with value。石 ∈ A, where
A is the action space. Z*(s,a*) is the random variable following the distribution describing the
randomness of return obtained by globally optimal policy ∏*, and the value of Z *(s, a*) is defined
as z*(s, a*). Through maximizing the mutual information between random variables Z*(s, a*) and
AE, we reduce the uncertainty of globally optimal Q function Q* to encourage exploration.
Specifically, at timestamp t, we find the exploration policy πE in the candidate distribution family
Π that can maximize the information about the optimal action random variable A* as follows:
πE = argmaxFπ(st),	(8)
π∈Π
where F(∙) is the mutual information and can be written as follows:
Fn (st)= MI(Z *(s,a* ),A∣s = St)
=H [∏(at∣st)] - H [p(at∣z*(st,a*), St)].	⑼
Here MI(∙) and H(∙) denote the mutual information and entropy of the random variable, respec-
tively. To obtain exploration policy ∏e, we need to measure the posterior probability p(∙) in the
above equation. For simplicity, we omit the timestamp t in the following.
To measure the posterior probabilityp(a|a*, S), we propose the following proposition.
Proposition 1. Generally, the posterior probability is estimated as follows:
p(a∣z*(s, a*), S) H π(a∣s)Φz∏(s,a)(z*(s, a*)),	(10)
where Φx is the cumulative distribution function (CDF) ofx, Z* and ZπE are the random variables,
whose distributions describing the randomness of the returns obtained by optimal policy π* and ex-
ploration policy πE, respectively, and z* is the value of random variable Z*. (see proof in Appendix
A).
To measure the intractable distribution of Z* during training, we use the Z* for approximation (i.e.,
Z* ≈ Z*), which will be defined later. In general, Z* is referred to as the optimistic approximator
(Mavrin et al., 2019; Chen et al., 2017), and can be formulated using the uncertainty, which will be
detailed in Sec. 4.2.1.
Therefore, the Fπ(S) in Eq. 9 can be estimated as follows:
Fn(S) ≈ Fπ(s) = Eπ,z* [logπ(a∣s)(G(s,a) - 1) + G(s,a) log G(s,a)],	(11)
where G(s, a) = C * Φz∏(s,a) (z*(s, a)). Specifically, G(s, a) measures the difference between Zπ
and Z*, i.e., large value of CDF means that z* is much bigger than the mean of Zπ.
By introducing distributional value functions in Eq. 10 to estimate the posterior probability, we can
use the uncertainty of value function to encourage the exploration, which will be discuss in Sec.
4.2.2.
4
Under review as a conference paper at ICLR 2021
4.2	MQES-based Exploration For modern RL Algorithms
In this section, we propose a scheme to incorporate exploration policy derived from MQES to ex-
isting policy-based algorithms, e.g., SAC and TD3, which renders the stable and well-performed
algorithm with more efficient exploration.
First, to obtain exploration policy, we employ a constraint to ensure the difference between the
exploration and target policies within a certain range (i.e., KL(π∣∣∏τ) ≤ α). The target policy ∏t
here, we mean the policy learned by any existing policy-based algorithms. It is worth noting that
MQES introduces distributional and ensemble critics to the existing framework (e.g., introducing
distributional critic to SAC formulates DSAC). Moreover, we utilize the critic of target policy to
formulate Z and ZπE, which will be stated later.
Intuitively, introducing the constraint in MQES ensures that the critic of target policy guides the
exploration properly and stabilizing the training. Otherwise, the exploration could be ineffective,
and the update of target policy can be dramatically bad. Specifically, if the difference between π
and πT are with significant difference, ZπT could not criticize exploration policy properly, and it
may explore with wrong guidance, where the experiences stored in the replay buffer are with poor
quality and the update of target policy fails sequentially.
Second, after introducing the KL constraint, the MQES-based exploration for modern RL algorithms
is given as follows:
π
πE (a|s) = arg max Fπ (s),
π
St KL(π∣∣∏τ) ≤ α,	(12)
where both the exploration ∏e = N(μE, ∑e ) and target policy ∏t = N(μτ, ∑t ) are Gaussian
distributions. By expanding Fπ linearly, we solve the problem in Eq. 12 using the following propo-
sition:
Proposition 2. The MQES exploration policy πE = N(μE, ΣE) derived from Eq. 12 is as follows:
μE = μT +
∂Z*(s,a) |	i∣∣~ "EEZ
~^dr~ |a=H
∂Z* (s,a)
∂a
∣a=μτ
, ΣE = ΣT. (13)
m
In specific, the i-th element of vector m is mi
φZπE (s,μτ )(z*(s,μτ ))
C
log √⅛+1,G(s,μτ)
i ∈ {1, ..., n} and n is the action dimension (see proof in Appendix B).
It is worth noting that the expectation against 旧之* can be estimated by sampling, and the estimation
of Eq. 13 is as follows:
√2α	1	∂Z;(s,a)∣
μE=μT + ɪ Σu	∂Z*(s,α) ~EE m ©	la="T ,	(14)
i=1 ∣∣m ©la=μT LE	L	」
4.2.1	FORMULATION OF Z* AND ZπE
In this section, we formulate the epistemic and aleatoric uncertainty with the critic of target policy,
thereby distributions of Z* and ZπE can be estimated. The remaining parts describe how to achieve
these two estimations, respectively.
Formulation of Z*. In order to formulate the distribution of estimated optimal Q value, i.e., Z*, we
firstly estimate its upper confidential bound, denoted by QUB. Aligned with Clements et al. (2019),
we adopt two independent distribution approximators Z(s, a; θ1) and Z(s, a; θ2) parameterized by
θ1 and θ2, respectively. Then we measure the epistemic uncertainty first as follows:
σepistemic(s,a; θ) = |Ei〜U(i,n)|zi(s,a； θι) - Zi(s,a; θ2)∣,	(15)
where N is the number of quantiles, and zi(s, a; θ) is the value of the i-th quantile drawn from
Z(s, a; θ).
5
Under review as a conference paper at ICLR 2021
Consequently, the upper confidential bound of Q-value is given leveraging the σepistemic as follows:
QU B(S, a; θ) = μZ (S, a; θ) + βσeρistemic (S, a; θ),	(16)
where μz(s, a; θ)= N ςn=i2 ςe=i,2zi(S, a; θk) is the mean estimation over quantile distributions,
β determines the magnitude of uncertainty we use. QUB is commonly considered as a approximation
of the optimal Q value in the existing work (Ciosek et al., 2019; Kirschner & Krause, 2018).
Moreover, as shown in (Dabney et al., 2018b), the aleatoric uncertaintly can be captured by return
distribution, which can be derived in our method by considering those two quantile distributions as
follows:
σaleatoric(s, a; θ) = Vari~U(1,N) [Eθk Zi(S, a; θk )].	(17)
Inspired by (Wang & Jegelka, 2017), we adopt QUB and σa2leatoric as mean and variation and formulate
the Gaussian distribution Z* as follows:
Z*(s,a; θ)〜N(QUB(s,a; 0),。%%/$,q; Θ))1z*≥qub,	(18)
where Z* follows truncated Gaussian distribution ensuring the globally optimal constraint, i.e.,
E[Z*] = Q* ≥ QUB. Nevertheless, since the distributions of Q functions describe the aleatoric
uncertainty, we set the variance of Z* as the aleatoric uncertainty obtained from Eq. 17.
Formulation of ZπE . Since the target for critic in the advanced algorithms, like SAC and TD3,
is usually estimated pessimistically, we take the pessimistic estimation for ZπE to make MQES
compatible with the existing modern RL algorithms. Here we present two modeling approaches:
Gaussian and quantile distributions.
Intuitively, we assume ZπE to be the Gaussian distribution with pessimistic estimation as the mean:
Z πE (s,a; θ)〜N (Qlb(s, a; θ),σ2leatoric(s,a; θ)),	(19)
where QLB (s, a; θ) = μz(s, a; θ) 一 βσepistemic(s, a； θ), estimates its lower confidential bound.
On the other hand, as the quantile distribution is a value distribution which naturally formulates
the underlying aleatoric uncertainty, we can utilize the quantile functions to model the pessimistic
quantile distribution directly, breaking the Gaussian assumption above. Specifically, we take the
smaller estimates at each quantile, and then the distribution ZπE (S, a; θ) is a uniform distribution
over each quantile value ziπE (S, a; θ), shch as
ziπE (S, a; θ)
min zi(S, a; θk).
k=1,2
(20)
Different from the uni-modal Gaussian distribution, the quantile function is able to represent multi-
modal distributions, which is more flexible. As the quantile function represents the inverse function
of CDF, meaning that we can easily get a general idea of the properties of this pessimistic quantile
distribution. It is worth noting that we can utilize other methods to formulate ZπE, like mean
estimation, i.e., E [ZπE] = μz(s, a; θ) and ZnE (s, a; θ) = Ek=1,2 [zi(s, a; θk)]. But, it only affects
the choice of hyper-parameter β and do not affect the final performance.
Algorithm 1 Exploration policy derived from MQES
Initialise: Current state st, current value distribution estimators θk, k = 1, 2, current policy net-
work φ, target policy ∏t(∙∣st; φ)〜N(μτ⑶；φ),στ⑶；φ))
Output: the MQES exploration policy πE
1:	Calculate Z(St, at; θk), k = 1, 2
2:	Calculate epistemic uncertainty σepistemic(St, at) according to Eq. 15
3:	Calculate upper bound QUB(St, at) using σepistemic(St, at) according to Eq. 16
4:	Calculate aleatoric uncertainty σa2leatoric(St, at) according to Eq. 17
5:	Construct Z*(st,at) using QUB(St,at) and。几地比⑶,at) (seeEq. 18)
6:	Construct ZπE (St, at) according to Eq. 19 / 20
7:	Calculate μE using Zπ(St, at) and Z*(St, at) according to Eq. 14
8:	return ∏e 〜N(μE,στ(St; φ))
The above Alg. 1 summarizes the overall procedure of MQES, including the estimation of uncer-
tainty (Line 2, 4) and the upper confidential bound QUB (Line 3), formulation of ZπE and Z* (Line
5-6) and exploration policy generation using KL constraint (Line 7). The generated exploration
policy can be adopted by any modern policy-based RL algorithms for an more effective exploration.
6
Under review as a conference paper at ICLR 2021
4.2.2 Analysis of MQES-based Exploration
This section analytically explains how MQES encourages exploration accounting for the aleatoric
and epistemic uncertainty. For simplicity, we assume that the sample number is K = 1, and Eq. 14
degrades to:
μE = μτ +
∂Z*(s,a) I	Il
la="Tll∑E
ΣE
∂Z*(s, a)
∂a
∣a=μτ
(21)
where mi
log √G(Π⅛
+1.
m
m
Take Gaussian MQES for example, We have ZπE (s, a)〜 N(QLB(s, a),。器8氐3 a)), and the
bias term added to μE is decided by the epistemic and aleatoric uncertainty.
Specifically, since epistemic uncertainty is involved in Z"s, a), the gradient dz d：,a) encourages
the optimistic exploration. Epistemic uncertainty-based exploration, can avoid the pessimistic un-
derexploration. The aleatoric uncertainty is introduced by CDF Φ. IfWe have tWo state-action pairs,
i.e., (sι,aι) and (s2,a2), and ZKE(Si,ai)〜 N(QLB(si,ai),σ2leatoric(si,ai)),i = 1, 2, and we
assume that Qlb(sι,aι) = QLB(S2,a2), σ2 > σ2, and Z*(s1,a1) = Z*(s2,a2). Obviously,
Φz∏(si,aι)(Z*(sι, aι)) < Φz∏(s2,a2)(z*(s2, a2)), which means that larger aleatoric uncertainty
leads to smaller action bias.
Therefore, the MQES encourages the exploration by selecting the action increasing the optimistic
value function, and avoid the over-exploration by setting smaller action bias at the state, where the
aleatoric uncertainty is high.
5	Experiments
MQES is designed for efficient exploration in continuous action space problem in RL, allowing the
agent tobe aware of explore directions that may lead to higher optimistic value function with smaller
aleatoric uncertainty. Comparisons between MQES and state-of-the-art algorithms are conducted to
verify the MQES regarding the effectiveness and efficiency. Empirical evaluations show that MQES
outperforms state-of-the-art algorithms on a series of continue control tasks.
5.1	Implementation details and experiment settings
We compare MQES against SAC (Haarnoja et al., 2018) and its distributional variant DSAC (Ma
et al., 2020). Ma et al. (2020) also shows the performance of TD4, which is the distributional
extension of TD3 (Fujimoto et al., 2018), and can also be used to capture epistemic and aleatoric
uncertainty as is pointed out in Sec. 4.2. However, DSAC outperforms TD4 as shown in Ma et al.
(2020), so we evaluate based only on SAC and DSAC, and further implement MQES based on
DSAC in order to develop the exploration ability.
The training process of MQES is the same as in DSAC, except for the behavior policy used, while we
enrich the experience replay with the data generated by πE. The pseudo code of the whole process
can be found in Appendix C. In order to ensure a fair comparison, the hyper-parameters of DSAC
and MQES are the same (see Appendix D). In addition, we have 3 hyper-parameters associated
with MQES. The parameter y∕2α controls the exploration level, and β determines the magnitude of
uncertainty we use, and C is the normalization factor.
We implement both approaches for building Zπ as illustrated in Sec. 4.2.1, and we use MQES-G and
MQES-Q to indicate respectively to Gaussian distribution and quantile distribution. We test MQES
on several tasks in Mujoco (Todorov et al., 2012), including standard version, as well as modified
sparse version and stochastic version. We limit the maximum length of each episode to 100. We
run 1250 or more epochs for each task where there are 100 training steps per epoch, with evaluating
every epoch where each evaluation reports the average undiscounted return with no exploration
noise.
7
Under review as a conference paper at ICLR 2021
SAC
DSAC
MQES_G
MQES_Q
0	250	500	750 1000 1250
Number of Training Epoch
(a) Walker2d-v2
O
O 500 IOOO 1500 2000 2500
Number of Training Epoch
(b) Ant-v2
1000	2000	3000
Number of Training Epoch
(c) Humanoid-v2
(d) Sparse Walker2d-v2
80
号60
⅛40
< 20
0	500 1000 1500 2000 2500
Number of Training Epoch
0	50	100	150	200	250
Number of Training Epoch
(e) Sparse Ant-v2
(f) Sparse HumanoidStandup-v2
Figure 1: Training curves on continuous control benchmarks in Mujoco. The x-axis indicates num-
ber of training epoch (100 environment steps for each training epoch), while the y-axis is the evalua-
tion result represented by average episode return. The shaded region denotes one standard deviation
of average evaluation over 5 seeds. Curves are smoothed uniformly for visual clarity.
5.2	Standard Mujoco tasks
We evaluate both MQES_G and MQES_Q in 5 tasks of standard Mujoco, and the result in Figure 1
and Appendix F show that our methods outperform SAC for all those tasks, and also reach better
performance than DSAC.
Performance. Our results demonstrate that in complex tasks, such as Humanoid-v2 and Ant-v2, our
MQES-based exploration policy performs better, while DSAC suffers from the inefficiency caused
by deficient exploration. In Ant-v2, DSAC was overtaken in the early stages of training and then
MQES stays ahead. Also in Humanoid-v2, the performance of our algorithm always maintains
better than DSAC. In some relatively easier tasks, it seems that those tasks are not very demanding
for exploration, but MQES performs still at a very advanced level. In Walker2d-v2, MQES and
DSAC alternated lead until 1000 epochs, after which MQES had a significant improvement. The
final results are shown more specifically in the Table 3.
Gaussian and Quantile Zπ . One can find that, expect for Humanoid-v2, there is no absolute
superiority between the two modeling approaches. We hypothesis this is because environment of
Humanoid-v2 is more complex than others, and more flexible quantile Zπ is needed, which could
model the environment more accurately.
5.3	Sparse Mujoco tasks
To further show the strength of our algorithm regarding exploration, we evaluate on the sparse ver-
sion of Mujoco tasks. Specifically, the reward is +1 when the move-distance threshold 1 is reached,
otherwise 0. Obviously, since we set the maximum length of episode to 100, the maximum average
episode return in the sparse tasks is up to 100.
As shown in Figure 1 and Appendix F, SAC performs poorly in those tasks, which is to be expected,
since solving the sparse reward problem requires not only more accurate estimates of critic, but also
more efficient exploration. Even in sparse HumanoidStandup-v2 task, when our algorithm MQES
1We set the threshold according to the statistical analysis of untrained interaction behavior, using 99.9%
quantile value, and the threshold of all environments are shown in Appendix E
8
Under review as a conference paper at ICLR 2021
reaches nearly maximum scores, SAC learns almost nothing, and DSAC performs obviously a bit
inferior. As can be seen in combination with the Figure 1 and Table 3, both MQES algorithms
perform the best, and both do achieve significantly better results faster than DSAC.
MQES performs extremely well in these sparse environments, which on the one hand demonstrates
the importance of exploration when solving sparse reward problems, and on the other hand shows
the advantages and capabilities of MQES for efficient exploration, presenting that incorporating
uncertainty to exploration could render better performance.
5.4	Ablation Study
In this section, we conduct two ablation experiments to show the performance gain by distinguishing
aleatoric and epistemic uncertaity and the sensitivity to the hyper-parameters.
5.4.1	Gain of Aleatoric Uncertainty
The exploration with epistemic uncertainty is proved to be efficient (Ciosek et al., 2019) by avoiding
the under-exploration. Hence, we present the gain brought by the aleatoric uncertainty by conducting
ablation study in this section. Generally, MQES-based actor-critic algorithm degrades to Optimistic
Actor-Critic (OAC) (Ciosek et al., 2019) if mi = 1 in Eq. 13. Consequentially, we could compare
MQES with OAC to show the necessary of introducing the aleatoric uncertainty to exploration.
In order to show that MQES performs robustly in the face of
aleatoric uncertainty, we modified the standard Ant-v2 task by
adding heterogeneous noise to the observation to increase the
aleatoric uncertainty of the environment. We also extend the
OAC to the distributional form (DOAC, Distributional OAC),
i.e., to estimate the values in the same way as MQES, in order
to ensure a fair comparison. As shown in Figure 2, DOAC
does not take into account the aleatoric uncertainty, resulting
in its performance being close to that of DSAC. The experi-
mental results confirm that MQES is effective in avoiding the
interference of aleatoric uncertainty in the environment and
ensuring exploration efficiency.
5.4.2	Ablation study on hyper-parameters
MQES is sensitive to √2α, which controls the distance be-
tween the behavior policy ∏e and the target policy ∏t . If
，2a is quite small, then the MQES degenerates to DSAC and
shows little exploration, and if √2α is larger, then the perfor-
mance becomes worse because of the gap between behavior
policy and target policy. We evaluate the sensitivity to hyper-
parameters on Ant-v2 task using MQES_G, and the sensitivity
to the constraint √2α is shown in Figure 3, and the sensitivity
to β is shown in Appendix F, where the error bar indicates half
standard deviation.
Figure 2: Gain of aleatoric uncer-
tainty, keeping the same plotting
settings as Figure 1.
Figure 3: Ablation study on √2α.
6	Conclusion
In this paper, we propose MQES, a generally exploration principle for continuous RL algorithms,
which formulates the exploration policy to maximize the information about the globally optimal
distribution of Q function. To make MQES practically tractable, we firstly incorporate distribu-
tional and ensemble Q function approximations to MQES, which could formulate the epistemic and
aleatoric uncertainty accordingly. Secondly, we introduce a constraint to stabilize the training, and
solve the constrained MQES problem to derive the exploration policy in closed form. Then, we
analyze and show that it explores optimistically and avoid over-exploration by recognizing the epis-
temic and aleatoric uncertainty, respectively. Empirical evaluations show that MQES works better
at the complex environments, where the exploration is needed.
9
Under review as a conference paper at ICLR 2021
References
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res.,
3:397-422, 2002.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom SchaUL David Saxton, and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Infor-
mation Processing Systems, pp. 1471-1479, 2016.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Doina PrecUp and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70
of Proceedings of Machine Learning Research, pp. 449-458. PMLR, 2017.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-
ensembles. arXiv preprint arXiv:1706.01502, 2017.
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor critic. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc,
Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
December 2019, Vancouver, BC, Canada, pp. 1785-1796, 2019.
William R. Clements, Beno^t-Marie Robaglia, Bastien Van Delft, Reda Bahi Slaoui, and SebaStien
Toth. Estimating risk and uncertainty in deep reinforcement learning. CoRR, abs/1905.09638,
2019.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for dis-
tributional reinforcement learning. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stock-
holm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp.
1104-1113. PMLR, 2018a.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Pro-
ceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th
innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Ed-
ucational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018, pp. 2892-2901. AAAI Press, 2018b.
Lior Fox, Leshem Choshen, and Yonatan Loewenstein. Dora the explorer: Directed outreaching
reinforcement action-selection. In International Conference on Learning Representations, 2018.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582-1591.
PMLR, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 1856-1865. PMLR, 2018.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109-1117, 2016.
Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with het-
eroscedastic noise. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Con-
ference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018, volume 75 of Pro-
ceedings of Machine Learning Research, pp. 358-384. PMLR, 2018.
10
Under review as a conference paper at ICLR 2021
Xiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Distribu-
tional soft actor critic for risk sensitive learning. CoRR, abs/2004.14547, 2020.
Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based explo-
ration in feature space for reinforcement learning. In Proceedings of the Twenty-Sixth Interna-
tional Joint Conference OnArtificial Intelligence, pp. 2471-2478, 2017.
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional
reinforcement learning for efficient exploration. In Kamalika Chaudhuri and Ruslan Salakhutdi-
nov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning
Research, pp. 4424-4434. PMLR, 2019.
Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Efficient exploration with double
uncertain value networks. CoRR, abs/1711.10789, 2017.
Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-
directed exploration for deep reinforcement learning. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via pos-
terior sampling. In Christopher J. C. Burges, Leon Bottou, ZoUbin Ghahramani, and Kilian Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 26: 27th Annual Confer-
ence on Neural Information Processing Systems 2013. Proceedings ofa meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States, pp. 3003-3011, 2013.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via boot-
strapped DQN. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and
Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Confer-
ence on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,
pp. 4026-4034, 2016.
Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. Count-based ex-
ploration with neural density models. In Proceedings of the 34th International Conference on
Machine Learning, pp. 2721-2730, 2017.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the 34th International Conference on Machine
Learning, pp. 2778-2787, 2017.
Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pp. 5062-5071. PMLR, 2019.
Nikolay Savinov, Anton Raichuk, Damien Vincent, Raphael Marinier, Marc Pollefeys, Timothy P.
Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-
Review.net, 2019.
Malcolm J. A. Strens. A bayesian framework for reinforcement learning. In Pat Langley (ed.),
Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000),
Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pp. 943-950. Morgan Kaufmann,
2000.
R. S. Sutton and A. G. Barto. Reinforcement learning - an introduction. Adaptive computation and
machine learning. MIT Press, 1998.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753-2762,
2017.
11
Under review as a conference paper at ICLR 2021
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012,
Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026-5033. IEEE, 2012.
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization. vol-
ume 70 of Proceedings of Machine Learning Research, pp. 3627-3635, International Convention
Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Christopher J. C. H. Watkins and Peter Dayan. Technical note q-learning. Mach. Learn., 8:279-292,
1992.
12
Under review as a conference paper at ICLR 2021
A Proof of Proposition 1
Proof. The proof is similar to Wang & Jegelka (2017). Here, we utilize Zπ to criticize π, i.e.,
p(a∣z*(s, a*), S) = -Ez"“斤(S,a)≤z*(S,a*)],
where C is the normalization factor, and EZn ⑶。)[1z∏ (s,a)≤z*(s,a*)] = Φζ∏(s,a)(z*(s,a*))	□
B Proof of proposition 2
Proof. Inspired by the methods of multipliers, it is equivalent to maximize objective func-
tion Fn(s,a) and minimize constraint KL(π∣∣∏τ) simultaneously. Firstly, We minimize the
KL(π∣∣∏τ) = KL(N(μ, Σ)∣∣N(μτ, ∑t)) against the variance matrix Σ, i.e.,
min KL(N(μ, Σ)∣∣N(μτ, ∑t)).	(22)
The optimal solution for problem (22) is
ΣE = ΣT .	(23)
Then, Fπ (s, a) is expanded linearly around at = μτ:
Fπ(s,a) ≈ aTVaFπ(s,a)∣α=μτ + const	(24)
τ「-	∂Z*(s, a),	^∣
=aτ Tn Θ ——-——-∣a=μτ + const,	(25)
∂a T
where the element of vector Tm = {rmi}rn=ι is
d _ 1 !	_*/ φz φzπ(s,μτ)(Z (S,μT))I 1、	SQ
m i =不 φZπ (s,μτ)(Z (S,μT ))(Iog-----	~=-------+ 1),	(26)
C	C (2π)Σ
n is the dimension of action, and φ(x) is the probability distribution function (pdf). Then, problem
(12) is reformulated as:
f τ Γ ∂Z*(s, a)	]]
max EZ* μ μ m Θ —而-1。=*T (
St 2(μ - μτ)Tς-1 (μ - μτ) ≤ α,	(27)
Then, the Lagrange function of problem (27) is given as:
L(μ) = Ez* μ μτ
八	∂Z*(s, a)
n θ 4a⅛μτ
-λ 2(μ - μτ)Tς-1(m - μτ) - α
(28)
According to the KKT condition, we derive the following equations: First, from the stationary, i.e.,
VμL(μ) = m Θ dZ∂") |。=“7 - λΣ-1(μ - μτ) = 0, we get
μ = μτ + <∑Ez* m Θ
λ
∂Z*(s, a)
一∂^ la=μτ
(29)
where λ > 0. Secondly, since λ > 0, the constraint is active, i.e., (μ - μT)TΣ-1 (μ - μT) = 2α.
Together with Eq. 29, we get the following equation:
快* hm Θ ⅛) ∣a = μτ]∣∣
λ = ---------------=-----------^ς .	(30)
√2α
Finally, (13) is obtained by plugging (30) to (29).	□
C Algorithm 2: MQES for DSAC
In this section, we show the whole algorithm of our implementation of MQES based on DSAC in
Alg. 2.
13
Under review as a conference paper at ICLR 2021
	
Algorithm 2 MQES for DSAC	
Initialise: Value networks θι, θ2, policy network φ and their target networks &, &, φ, quantiles number N, target smoothing coefficient (τ), discount (γ), an empty replay pool D 1:	for each iteration do 2:	for each environmental step do 3:	at 〜∏e(at, St) according to Alg. 1 4:	D -D∪{(st, at, r(st, at), st+ι)} 5:	end for 6:	for each training step do 7:	for i = 1 to N do 8:	for j = 1 to N do 9:	calculate δik,j, k = 1, 2, following Eq. 5 10:	end for 11:	end for 12:	Calculate LQR(θk), k = 1, 2 using δik,j following Eq. 2 13:	Update Ok With VLQR®) 14:	Calculate Jπ (φ), following Eq. 7 15:	Update φ with VJπ (φ) 16:	end for 17:	Update target value network with Gk J τθk + (1 — τ)θk, k = 1,2 18:	Update target policy network with φ J τφ +(1 — T)φ 19:	end for	
Table 1: MQES parameters	
Parameter	Value
Training	discount Target smoothing coefficient τ Learning rate Optimizer Replay buffer size Batch size Quantiles number Action range Environment steps per epoch	0.99 5e-3 3e-4 Adam 10e6 256 20 [-1,1] 100
Exploration	Exploration ratio	√2α Uncertainty ratio	β Normalization factor	C	^08 1.6 0.5
D	Hyper-parameters setting
The hyper-parameters in our experiment are guaranteed to be consistent, as shown in Tab. 1.
E threshold settings for sparse tasks
As illustrated in Sec. 5.3, We set the threshold according to the statistical analysis of untrained
interaction behavior, using 99.9 quantile value, as shown in Tab. 2.
14
Under review as a conference paper at ICLR 2021
Table 2: Threshold settings for sparse reward tasks
Sparse tasks	Threshold
Sparse Ant-V2	0.13
Sparse Hopper-v2	0.605
Sparse HalfCheetah-v2	3.5
Sparse Walker2d-v2	0.525
Sparse Humanoid-v2	0.17
Sparse HumanoidStandup	0.18
Figure 4: Ablation study on β
F	Additional experiment results
Limited by the length of the text, we present the results of the evaluation on some simple tasks in
Fig. 5. All evaluation data can be seen in Tab. 3.
The sensitivity to β is shown in Fig. 4. β controls the uncertainty magnitude, the smaller the value,
the smaller the degree of optimism or pessimism, and vice versa. We evaluate the effect of β in Ant-
v2 task using MQES_Q. What can be seen is that larger β will degrade performance. Although
smaller β are more profitable in the Ant-v2 task, it is not necessarily in other task, so we set a
uniform 1.6 in our experiments.
15
Under review as a conference paper at ICLR 2021
0	500 1000 1500 2000 2500
Number of Training Epoch
(b) Hopper-v2
(d) Sparse Halfcheetah-v2
(a) Pusher-v2
250	500	750 1000 1250
Number of Training Epoch
80
60
40
20
0	1000	2000	3000
Number of Training Epoch
(c) SparseHopper-v2
(e) Sparse Humanoid-v2
Figure 5: Training curves on continuous control benchmarks in Mujoco. The x-axis indicates num-
ber of training epoch (100 environment steps for each training epoch), while the y-axis is the evalua-
tion result represented by average episode return. The shaded region denotes one standard deviation
of average evaluation over 5 seeds. Curves are smoothed uniformly for visual clarity.
Table 3: Average return over 5 seeds with one standard deviation at corresponding training step, i.e.,
1.25 x 105 million training step for Hopper-v2. The maximum value of each row is shown in bold.
Task	1e5	SAC	DSAC	MQES_G	MQES_Q
Walker-v2	1.25	164.4 ± 20.7	204.2 ± 15.3	226.6 ±18.7	212.1 ± 18.7
Ant-v2	2.5	276.0 ± 24.5	456.5 ± 52.2	507.5 ±12.2	456.5 ± 48.4
Humanoid-v2	3.0	487.5 ± 6.0	597.7 ± 4.4	612.3 ±10.5	607.3 ± 13.6
Sparse Walker2d-v2	1.25	35.6 ± 8.8	38.8 ± 4.9	50.6 ±3.1	47.9 ± 3.7
Sparse Ant-v2	2.5	47.2 ± 6.0	60.3 ± 8.4	66.4 ± 7.4	69.3 ±15.3
Sparse HumanoidStandup	0.25	5.3 ± 10.2	82.2 ± 17.9	95.0 ±0.8	94.9 ± 1.4
Pusher-v2	2.5	-55.7 ± 26.9	-21.1 ± 0.6	-21.0 ± 0.6	-20.8 ±0.3
Hopper-v2	1.25	203.3 ± 19.9	248.9 ±4.6	233.6 ± 7.6	234.2 ± 11.6
Sparse Hopper-v2	1.25	25.3 ± 2.5	23.7 ± 2.9	23.3 ± 4.0	26.4 ±2.3
Sparse HalfCheetah-v2	1.25	53.5 ± 12.7	70.2 ± 1.1	70.3 ±0.5	68.3 ± 2.4
Sparse Humanoid-v2	3.0	24.9 ± 3.1	46.6 ± 8.6	60.4 ±6.0	51.0 ± 10.3
16