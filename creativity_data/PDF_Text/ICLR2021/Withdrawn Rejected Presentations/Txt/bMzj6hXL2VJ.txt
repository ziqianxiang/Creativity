Under review as a conference paper at ICLR 2021
Ordering-Based Causal Discovery with
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
It is a long-standing question to discover causal relations among a set of variables
in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved
promising results in causal discovery. However, searching the space of directed
graphs directly and enforcing acyclicity by implicit penalties tend to be inefficient
and restrict the method to small problems. In this work, we alternatively consider
searching an ordering by RL from the variable ordering space that is much smaller
than that of directed graphs, which also helps avoid dealing with acyclicity. Specif-
ically, we formulate the ordering search problem as a Markov decision process,
and then use different reward designs to optimize the ordering generating model.
A generated ordering is then processed using variable selection methods to obtain
the final directed acyclic graph. In contrast to other causal discovery methods,
our method can also utilize a pretrained model to accelerate training. We conduct
experiments on both synthetic and real-world datasets, and show that the proposed
method outperforms other baselines on important metrics even on large graph tasks.
1	Introduction
Identifying causal structure from observational data is an important but also challenging task in many
applications. This problem can be formulated as that of finding a Directed Acyclic Graph (DAG)
that minimizes some score function defined w.r.t. observed data. Though there exist well-studied
score functions like Bayesian Information Criterion (BIC) or Minimum Description Length (MDL)
(Schwarz et al., 1978; Chickering, 2002), searching over the space of DAGs is known to be NP-hard,
even if each node has at most two parents (Chickering, 1996). Consequently, traditional methods
mostly rely on local heuristics to perform the search, including greedy hill climbing and Greedy
Equivalence Search (GES) that explore the Markov equivalence classes (Chickering, 1996).
Along with various search strategies, existing methods have also considered to reduce the search
space while meeting the DAG constraint. A useful practice is to cast the causal structure learning
problem as that of learning an optimal ordering of variables (Koller & Friedman, 2009). Because the
ordering space is significantly smaller than the space of directed graphs and searching over ordering
space can avoid the problem of dealing with acyclic constraints (Teyssier & Koller, 2005). Many
algorithms are used to search for ordering such as genetic algorithm (Larranaga et al., 1996), Markov
chain Monte Carlo (Friedman & Koller, 2003) and greedy local hill-climbing (Teyssier & Koller,
2005). However, these algorithms often cannot find the best ordering effectively.
Recently, with differentiable score functions, several gradient-based methods have been proposed
based on a smooth characterization of acyclicity, including NOTEARS (Zheng et al., 2018) for linear
causal models and several subsequent works, e.g., Yu et al. (2019); Lachapelle et al. (2020); Ng et al.
(2019b;a), which use neural networks to model nonlinear causal relationships. As another attempt,
Zhu et al. (2020) utilize Reinforcement Learning (RL) as a search strategy to find the best DAG
from the graph space and it can be incorporated with a wide range of score functions. Unfortunately,
its good performance is achieved only with around 30 variables, for at least two reasons: 1) the
action space, consisting of directed graphs, is tremendous for large scale problems and hard to be
explored efficiently; and 2) it has to compute scores for many non-DAGs generated during training but
computing scores w.r.t. data is generally be time-consuming. It appears that the RL-based approach
may not be able to achieve a close performance to other gradient-based methods that directly optimize
the same (differentiable) score function for large causal discovery problems, due to its search nature.
1
Under review as a conference paper at ICLR 2021
In this work, we propose a RL-based approach for causal discovery, named Causal discovery with
Ordering-based Reinforcement Learning (CORL), which combines RL with the ordering based
paradigm so that we can exploit the powerful search ability of RL to search the best ordering
efficiently. To achieve this, we formulate the ordering search problem as a Markov Decision Process
(MDP), and then use different reward designs for RL to optimize the ordering generating model. In
addition, we notice that pretrained model can be incorporated into our model to accelerate training.
For a generated ordering, we prune it to the final DAG by variable selection. The proposed approach
is evaluated on both synthetic and real datasets to validate its effectiveness. In particular, the proposed
method can achieve a much improved performance than the previous RL-based method on both linear
and non-linear data models, even outperforms NOTEARS, a gradient-based method, on 150-node
linear data models, and is competitive with Causal Additive Model (CAM) on non-linear data models.
2	Related Works
Existing causal discovery methods roughly fall into three classes. The first class, as described in
the introduction, are the so-called score-based methods. Besides the mentioned BIC/MDL scores,
other score functions include the Bayesian Gaussian equivalent score (Geiger & Heckerman, 1994),
the generalized score based on (conditional) independence relationship (Huang et al., 2018), and a
recently proposed meta-transfer score (Bengio et al., 2020). Another class of methods such as fast
causal inference and PC (Spirtes et al., 2000; Zhang, 2008) which first find causal skeleton and then
decide the orientations of the edges up to the Markov equivalence class, are viewed as constraint-based.
Such methods usually involve multiple independent testing problems; the testing results may have
conflicts and handling them is not easy. The last class of methods relies on properly defined functional
causal models, includeing Linear Non-Gaussian Acyclic Model (LiNGAM), nonlinear Additive Noise
Model (ANM) (Hoyer et al., 2009; Peters et al., 2014), and the post-nonlinear causal model (Zhang
& Hyvarinen, 2009). By placing certain assumptions on the class of causal functions and/or noise
distributions, these methods can distinguish different DAGs in the same Markov equivalence class.
Since we particularly consider ordering-based approaches, here we present a more detailed review
of such methods. Most of the ordering-based methods belong to the class of score-based methods.
Besides the mentioned heuristic search algorithms, Schmidt et al. (2007) proposed L1OBS to conduct
variable selection using `1 -regularization paths based on Teyssier & Koller (2005). Scanagatta et al.
(2015) further proposed an ordering exploration method on the basis of an approximated score
function so as to scale to thousands of variables. The Causal Additive Model (CAM) was developed
by Buhlmann et al. (2014) for nonlinear data models. Some recent ordering-based methods such as
sparsest permutation (Raskutti & Uhler, 2018) and greedy sparest permutation (Solus et al., 2017)
can guarantee consistency of Markov equivalence class, based on some conditional independence
relations and certain assumptions like faithfulness. A variant of greedy sparest permutation was also
proposed in Bernstein et al. (2020) for the setting with latent variables. In the present work, we
mainly work on identifiable cases which may have different assumptions from theirs.
In addition, several exact algorithms such as dynamic programming (Silander & Myllymaki, 2006;
Perrier et al., 2008) and integer or linear programming (Jaakkola et al., 2010; Cussens, 2011; Bartlett
& Cussens, 2017) are used to solve causal discovery problem. However, these exact algorithms
usually work on small graphs efficiently (De Campos & Ji, 2011), and in order to handle larger
problems with hundreds of variables, they need to incorporate heuristics search (Xiang & Kim, 2013)
or limit the maximum number of parents.
Recently, RL has been used to tackle several combinatorial problems such as maximum cut and the
traveling salesman problem, due to their relatively simple reward mechanisms (Khalil et al., 2017).
In combination with the encoder-decoder based pointer networks (Vinyals et al., 2015), Bello et al.
(2016) showed that RL can have a better generalization even when the optimal solutions are used as
labeled data in a supervised way. Kool et al. (2019) further used an attention based encoder-decoder
model for an improved performance. These works aim to learn a policy as a combinatorial solver
based on the same structure of a particular type of combinatorial problems. However, various causal
discovery tasks generally have different causal relationships, data types, graph structures, etc, and
moreover, are typically off-line with focus on a causal graph. As such, we use RL as a search strategy,
similar to Zhu et al. (2020); Zoph & Le (2017); nevertheless, a pretrained model or a policy can offer
a good starting point to speed up training, as shown in our evaluation results (Figure 3).
2
Under review as a conference paper at ICLR 2021
3	Background
3.1	Causal S tructure Learning
Let G = (d, V, E) denotes a DAG, with d the number of nodes, V = {v1, . . . , vd} the set of nodes,
and E = {(vi, vj)|i, j = 1, . . . , d} the set of directed edges from vi to vj. Each node vj is associated
with a random variable Xj. The probability model associated with G factorizes as p(X1, . . . , Xd) =
Qjd=1 p(Xj |Pa(Xj)), where p(Xj |Pa(Xj)) is the conditional probability distribution for Xj given
its parents Pa(Xj) := {Xk|(vk, vj) ∈ E}. We assume that the observed data xj is obtained by the
Structural Equation Model (SEM) with addtive noise: Xj := fθj (Pa(Xj)) + j , j = 1, . . . , d, where
fθj parameterized by θj is used to represent the functional relationship between Xj and its parents,
and j’s denote jointly independent additive noise variables. We assume causal minimality, which is
equivalent to that each fj is not a constant for any Xk ∈ Pa(Xj) in this SEM (Peters et al., 2014).
Given a sample X = [x1, . . . , xd], where X ∈ Rm×d and xj is a vector of m observations for
random variables Xj . Our goal in this paper is to find the DAG G that maximizes the BIC score (or
other well studied scores), defined as
d
SCoreBIC(G) = E
j = 1
XX log P(Xj ∣Pa(χk ),θj )-，log m ,
.k = 1	_
(1)
where ∣θj | is the number of free parameters in p(Xj ∣Pa(Xj), θj). For linear causal relationships,
∣θj | = ∣Pa(Xj )| the number of parents, up to some constant factor.
The problem of finding a directed graph that satisfies the ayclicity constraint
can be cast as that of finding an variable ordering (Teyssier & Koller, 2005;
Schmidt et al., 2007). The score of an ordering is usually defined as the score
of the best DAG that is consistent with the given ordering. Specifically, let
Π denote an ordering of the nodes in V, where the length of the ordering
∣Π∣ = | V| and Π is indexed from 1. If node Vj ∈ V lies in the p-th position,
then Π(p) = Vj. Notation Πγvj denotes the set of nodes in V that precede
node Vj in Π. One can establish a canonical correspondence between an
ordering Π and a fully-connected DAG Gπ; an example with four nodes is
presented in Figure 1. For a given DAG G, it can be consistent with more than
one orderings and the set of these orderings is given by
Φ(Π) := {Π : the fully-connected DAG GniS a super-DAG of G},	(2)
∏ ：= {v2,V4,V1,V3}
Figure 1:	An
example of the
correspondence
between an order-
where a super-DAG of G is a DAG whose edge set is a superset of that of G. ing (down) and a
The score of an ordering is usually defined as the score of the best DAG that fully-connected
is consistent with the given ordering (Teyssier & Koller, 2005; Peters et al., DAG (top).
2014). We provide a formal description in Proposition 1 to show that it is
possible to find the correct ordering with high probability in the large sample
limit. Therefore, the search for the true DAG G* can be decomposed to two phases: finding the
correct ordering and performing variable selection (feature selection); the latter is to find the optimal
DAG that is consistent with the ordering found in the first step.
Proposition 1. Suppose that an identifiable SEM with true causal DAG G* on X = {Xj}jd=1 induces
distribution P (X). Let GΠ be the fully-connected DAG that corresponds to an ordering Π. If there is
an SEM with GΠ inducing the same distribution P(X), then GΠ must be a super-graph of G*, i.e.,
every edge in G* is covered in GΠ.
Proof. The SEM with GΠ may not be causally minimal but can be reduced to an SEM satisfying the
causal minimality condition (Peters et al., 2014). Let Gn denotes the causal graph in the reduced
SEM with the same distribution P(X). Since we have assumed that original SEM is identifiable, i.e.,
the distribution P(X) corresponds to a unique true graph, Gn is then identical to G*. The proof is
complete by noticing that Gn is a super-graph of Gn.	□
3
Under review as a conference paper at ICLR 2021
3.2	Reinforcement Learning
Standard RL is usually formulated as an MDP over the environment state s ∈ S and agent action
a ∈ A, under an (unknown) environmental dynamics defined by a transition probability T (s0|s, a).
We use ∏φ(a∣s) to denote the policy, parameterized by φ, which outputs a discrete (or continuous)
distribution used to select an action from action space A based on state s. For episodic tasks, a
trajectory τ = {st , at}tT=0, where T is the finite time horizon, can be collected by executing the
policy repeatedly. In many cases, an immediate reward r(s, a) can be received when agent executes
an action. The objective of RL is to learn a policy which can maximize the expected cumulative
reward along a trajectory, i.e., J(φ) = Eπφ [R0] with R0 = PtT=0 γtrt(st, at) and γ ∈ (0, 1] being
a discount factor. For some scenarios, the reward is only earned at the terminal time (also called
episodic reward), and J(φ) = Eπφ [R(τ)] with R(τ) = rT (sT , aT).
4	Method
In this section, we first introduce how to model the ordering search problem as an MDP, then we
show how to use RL to find the optimal ordering, and we introduce how to process the searched
ordering to obtain the final DAG, finally we provide a discussion on computational complexity.
4.1	Ordering Search as a Markov Decision Process
We can regard the variable ordering search problem as a muti-stage decision problem with a variable
selected at each decision step. We sort the selected variables according to decision steps to obtain
a variable ordering, which is defined as the searched ordering. The decision-making process is
Markovian, and its elements in the problem can be defined as follows:
State One can directly take the sample data xj of each variable Xj as a state sj . However,
preliminary experiments show that it is difficult for feed-forward neural network models to capture
the underlying causal relationships directly using observed data as states, and the data pre-processed
by a module conventionally named encoder is helpful to finding the better ordering, see Appendix A.1.
The encoder module embeds each xj to sj and all the embedded states constitute the space S :=
{sι,..., sd}. Adding initial state so to the space constitutes the complete state space S:= S ∪ s0.
We use St to denote the state encountered at the t-th decision step.
Action We select an action (variable) from the action space constituted by all the variables at each
decision step, and the space size is equal to the number of variables, |A| = d. Compared to the
previous RL-based method searching from the graph space with size O(2d×d) (Zhu et al., 2020), the
search space is smaller. Note that according to the definition of ordering, the first selected variable is
the source node and the last selected node is the sink node.
State transition The specified state transition is related to the action selected at the current decision
step. Specifically, if the selected variable is vj at the t-th decision step, then the state is transferred to
the state Sj ∈ S which is the j-th output from Transformer encoder, i.e., ^t+ι = Sj.
Reward As we described in Section 3.1, only the variables that have been selected in previous
decision steps can be the potential parents of the currently selected variable. Hence, we can design
the rewards in the following cases: dense reward and episodic reward. For dense reward case, we
can exploit the decomposability of the score function (BIC score) to calculate an immediate reward
for the current decision step based on the potential parent variables (that have been selected), i.e., if
vj is selected at time step t, the immediate reward is calculated by
rt
m
XlogP(Xk|U(Xj),θj)-
k=1
≡ log m,
(3)
where U(Xj ) denotes the potential parent variable set of Xj and the set consists of the variables
associated with the nodes in Πγvj. For episodic reward case, we directly calculate a score as
an episodic reward for a complete variable ordering regardless of whether the scoring function is
decomposable or not, i.e.,
R(T) = rτ(S, a) = SCOreBIC(Gn)	(4)
4
Under review as a conference paper at ICLR 2021
where ScoreBIC has been defined in Equation (1), with the set Pa(Xj) of each variable Xj replaced
by the potential parent variable set U (Xj) here.
4.2	IMPLEMENTATION AND OPTIMIZATION WITH REINFORCEMENT LEARNING
We describe the neural network architectures implemented in our method, which consists of an
encoder and a decoder as shown in Figure 2. Here we briefly describe the model architectures and
leave details regarding model parameters to Appendix A.
Encoder	fφnc : X → S is used to map
the observed data to the embedding space S
{sι,...,sd}. For sample efficiency, we follow Zhu
et al. (2020) to randomly draw n samples from the
dataset X to construct X ∈ Rn×d at each episode
and use X instead of X. We also set the embedding
Sj to be in the same dimension, i.e., Sj ∈ Rn. For
encoder choice, we conduct an empirical compari-
son among several possible structures such as a self-
attention based encoder (Vaswani et al., 2017) and
an LSTM structure. Empirically, we confirm that the
self-attention based encoder in the Transformer struc-
ture performs the best, which is also used in Zhu et al.
(2020). Please find more details in Appendix A.1.
5l	♦ ♦ ♦	Sj	♦ ♦ ♦	5a
a0	♦ ♦ ♦	%	♦ ♦ ♦	aτ
♦
Encoder	Decoder
∖________√	X________/
畲	Ir
X1 ♦ ♦ ♦ Xj ♦…X d	s0 ♦…St ♦…ST
Figure 2: Illustration of the policy model. The
encoder embeds the observed data Xj into the
state Sj. An action at can be selected by the
decoder according to the given state St at each
time step t.
t
Decoder	fφec : S → A maps the state space S to
the action space A. Among several decoder choices
(please see also Appendix A.1 for empirical comparison), we specifically pick an LSTM based
structure that proves effective in our experiments. Although the initial state is usually generated
randomly, we pick it as s° = d Pd=1 Si, considering that the source node is fixed in correct ordering.
We then restrict each node only be selected once by masking the selected nodes so as to generate a
valid variable ordering (Vinyals et al., 2015).
Optimization The optimization objective is to learn a policy which maximizes J(φ) = Eπφ [R],
where πφ denotes the policy model parameterized by the paprameters {φe, φd} of encoder fenc and
decoder fdec. Based on the above definition, policy gradient (Sutton & Barto, 2018) is used to
optimize the ordering generation model parameters. For the dense reward case, policy gradient can
be written as VJ(φ) = E∏φ [PT=。RtVφ log∏φ (at∣^t)], where Rt = PT-t Ylrt+l denotes the
return at time step t. We denote the algorithm using this reward design as CORL-1. For the episodic
reward case, we have the following policy gradient VJ (φ) =Eπφ R(τ) PtT=0Vφ log∏φ (at∣^t)],
the algorithm using this reward design is denoted as CORL-2. Using a parametric baseline to estimate
the expected score typically improves learning (Konda & Tsitsiklis, 2000). Therefore, we introduce a
critic network Vφv (^t) parameterized by φv, which learns the expected return given a state St. It is
trained with stochastic gradient descent using Adam optimizer on a mean squared error objective
between its predicted value and the actual return. The details about the parameters of critic network
are described in Appendix A.2.
Inspired by the benefits of pretrained models in other tasks (Hinton & Salakhutdinov, 2012), we also
consider to incorporate it to our method to accelerate training. Usually, one can obtain some observed
data with known causal structure or correct ordering, e.g., by simulation or real data with labeled
graph. Hence, we pretrain a policy model with such data in a supervised way.
So far we presented CORL in a general manner without specifying explicitly which distribution
family is during the evaluation of rewards. In principle, any distribution family could be employed
as long as its log-likelihood can be computed and no differentiability is required. However, it is not
always clear whether the maximization of the accumulated reward recovers the correct ordering. It
will depend on both the modelling choice of reward and the underlying SEM; in fact, if the causal
relationships fall into the chosen model functions and a right distribution family is assumed, then
given infinite samples the optimal accumulated reward, corresponding to the negative log-likelihood,
must be achieved by a super-DAG of the underlying graph according to Proposition 1. In practice, we
5
Under review as a conference paper at ICLR 2021
Algorithm 1 CORL.
Require: observed data X, initial parameters θe , θd and θv, two empty buffers D and Dscore, initial
value (negative infinite) BestScore and a random ordering BestOrdeing.
1:	while not terminated do
2:	draw a batch of samples from X, encode them to S and calculate the initial state ^o
3:	for t = 0, 1, . . . , T do
4:	collect a batch of data h^t, at, YG with ∏θ : D = D ∪ {h^t, at, r∕}
5:	if hvt, Πγvt ,rti is not in Dscore then
6:	store hvt, Πγvt, Irt in Dscore to avoid repeated computations
7:	end if
8:	end for
9:	update θe, θd, θv according to Section 4.2
10:	if PtT=0 rt > BestScore then
11:	update the BestScore and BestOrdering
12:	end if
13:	end while
14:	get the final DAG by pruning the BestOrdering
can only apply approximate model functions and also need to assume certain distribution family for
caculating the reward.
Our method is summarized in Algorithm 1. In addition, we record the decomposed scores for each
variable Vj with different parental sets Πγvj to avoid repeated computations which are generally
time-consuming (see Section 4.4). Although we cannot guarantee to find the optimal ordering because
policy gradient can at most guarantee local convergence (Sutton et al., 2000) and also we only have
access to the empirical log-likelihood, we remark that the ordering obtained from CORL still enjoy a
good performance in the experiments, compared with consistent methods like GES and PC.
4.3 Variable Selection
If an estimated ordering Π is consistent, then we obtain a fully-connected DAG (SUPer-DAG) Gn of
the underlying DAG G . One can then pursue consistent estimation of intervention distributions based
on Π without any additional need to find the true underlying DAG G* (Buhlmann et al., 2014). For
other purposes, however, we need to recover the true graph from the fully-connected DAG. There
exist several efficient methods such as sparse candidate (Teyssier & Koller, 2005), significance testing
of covariates (Buhlmann et al., 2014), the group Lasso (Ravikumar et al., 2009), or its improved
version with a sparsity-smoothness penalty proposed in Meier et al. (2009).
For linear data models, we apply linear regression to the obtained fully-connected DAG, followed by
thresholding to prune edges with small weights, as similarly used by Zheng et al. (2018); Yu et al.
(2019); Zhu et al. (2020). For the nonlinear model, we follow the pruning process used by Buhlmann
et al. (2014); Lachapelle et al. (2020). Specifically, for each variable Xj , one can fit a generalized
additive model against the current parents of Xj and then apply significance testing of covariates,
declaring significance if the reported p-values are lower or equal to 0.001.
4.4 Computational Complexity
To learn an ordering, CORL relies on the proper training of RL model. Policy gradient and stochastic
gradient are adopted to train the actor and critic respectively, which are the standard choice in RL
(Konda & Tsitsiklis, 2000). Similar to RL-BIC2 (Zhu et al., 2020), CORL requires the evaluation of
the rewards at each episode with O(dm2 + d3) computational cost if linear functions are adopted to
model the causal relations, but does not need to compute the matrix exponential term with O(d3) cost
due to the use of ordering search. In addition, CORL formulates causal discovery as a multi-stage
decision process and we observe that CORL performs fewer episodes than RL-BIC2 before the
episode reward converges (see Appendix C). We suspect that due to a significant reduction in the size
of action space, the model complexity of the RL policy is reduced, thus leading to higher sample
efficiency. The evaluation of Transformer encoder and LSTM decoder in CORL take O(nd2 ) and
6
Under review as a conference paper at ICLR 2021
O(dn2), respectively. However, we find that computing rewards is more dominating in the total
runing time (e.g., around 95% and 87% for 30- and 100-node linear data models, respectively).
Speeding up the calculation of rewards would be helpful in extend our approach to a larger problem,
which is left as a future work.
In contrast with typical RL applications, we treat RL here as a search strategy for causal discovery,
aiming to find an ordering that achieves the best score and then applying a variable selection method to
remove redundant edges. Nevertheless, for the pretraining part with the goal of a good initialization,
we may want sufficient generalization ability and hence consider diverse datasets with different
number of nodes, noise types, causal relationships, etc.
5	Experiments
In this section, we evaluate our methods against a number of methods on synthetic datasets with
linear and non-linear causal relationships and also a real dataset. Specifically, these baselines include
ICA-LiNGAM (Shimizu et al., 2006), three ordering-based approaches L1OBS (Schmidt et al., 2007),
CAM (BUhlmann et al., 2014) and A* Lasso (Xiang & Kim, 2013), some recent gradient-based
approaches NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019) and GraN-DAG (Lachapelle
et al., 2020), and the RL-based approach RL-BIC2 (Zhu et al., 2020). For all the compared algorithms,
we use their original implementations (see Appendix B.1 for details) and pick the recommended
hyper-parameters unless otherwise stated.
Different types of data are generated in synthetic datasets which vary along five dimensions: level of
edge sparsity, graph type, number of nodes, causal functions and sample size. Two types of graph
sampling schemes: Erdos-R6nyi (ER) and Scale-free (SF) are considered in our experiments. We
denote d-node ER or SF graphs with on average hd edges as ERh or SFh. Two common metrics
are considered: True Positive Rate (TPR) and Structural Hamming Distance (SHD). The former
indicates the probability of correctly finding the positive edges among the discoveries (Jain et al.,
2017). Hence, it can be used to measure the quality of an ordering, the higher the better. The latter
counts the total number of missing, falsely detected or reversed edges, the smaller the better.
5.1	Linear Data Models with Gaussian and Non-Gaussian Noise
We evaluate the proposed methods on Linear Gaussian (LG) with equal variance Gaussian noise
and LiNGAM data models, and the true DAGs in both cases are known to be identifiable (Peters
& BUhlmann, 2014; Shimizu et al., 2006). We set h ∈ {2, 5} and d ∈ {30, 50, 100} to generate
observed data following the procedure done in Zheng et al. (2018) (see Appendix B.2 for details).
For variable selection, we set the thresholding as 0.3 and apply it to the estimated coefficients.
Table 1: Empirical results for ER and SF graphs of 30 nodes with LG data.
		RANDOM	NOTEARS	DAG-GNN	RL-BIC2	L1OBS	A* Lasso	CORL-1	CORL-2
ER2	TPR SHD	0.41 (0.04) 140.4 (36.7)	0.95 (0.03) 14.2 (9.4)	0.91 (0.05) 26.5 (12.4)	0.94 (0.05) 17.8 (22.5)	0.78 (0.06) 85.2 (23.8)	0.88 (0.04) 35.3 (14.3)	0.99 (0.02) 5.2 (7.4)	0.99 (0.01) 4.4 (3.5)
ER5	TPR SHD	0.43 (0.03) 210.2 (43.5)	0.93 (0.01) 35.4 (7.3)	0.85 (0.11) 68.0 (39.8)	0.91 (0.03) 45.6 (13.3)	0.74 (0.04) 98.6 (32.7)	0.84 (0.05) 71.2 (21.5)	0.94 (0.03) 37.4 (16.9)	0.95 (0.03) 37.6 (14.5)
SF2	TPR SHD	0.58 (0.02) 118.4 (12.3)	0.98 (0.02) 6.1 (2.3)	0.92 (0.09) 36.8 (33.1)	0.99 (0.02) 3.2 (1.7)	0.83 (0.04) 49.7 (28.1)	0.93 (0.02) 27.3 (18.4)	1.0 (0.01) 0.0 (0.0)	1.0 (0.01) 0.0 (0.0)
SF5	TPR SHD	0.44 (0.03) 165.4 (10.6)	0.94 (0.03) 23.3 (6.9)	0.89 (0.09) 47.8 (35.2)	0.96 (0.03) 11.3 (5.2)	0.79 (0.04) 89.3 (25.7)	0.88 (0.03) 40.5 (19,8)	1.00 (0.00) 0.0 (0.0)	1.00 (0.00) 0.0 (0.0)
Tables 1 &2 present results only for 30- and 100-node LG data models since the conclusions do not
change with graphs of 50 nodes (see Appendix D for 50-node graphs). We report the performance
of the popular ICA-LiNGAM, GraN-DAG and CAM in Appendix D since they are almost never on
par with the best methods presented in this section. CORL-1 and CORL-2 achieve consistently good
results on the LiNGAM datasets which are reported in Appendix E due to the space limit.
We now examine Tables 1 &2 (the values in parentheses represent the standard deviation across
datasets per task). We can see that, across all settings, CORL-1 and CORL-2 are the best performing
methods, both in terms of TPR and SHD, while NOTEARS and DAG-GNN are not too far behind.
7
Under review as a conference paper at ICLR 2021
Table 2: Empirical results for ER and SF graphs of 100 nodes with LG data.
		RANDOM	NOTEARs	DAG-GNN	RL-BiC2	L1OBs	A* Lasso	CORL-1	CORL-2
ER2	TPR sHD	0.33 (0.05) 491.4 (17.6)	0.93 (0.02) 72.6 (23.5)	0.93 (0.03) 66.2 (19.2)	0.02 (0.01) 270.8 (13.5)	0.54 (0.02) 481.2 (49.9)	0.86 (0.04) 128.5 (38.4)	0.98 (0.02) 24.8 (10.1)	0.98 (0.01) 18.6 (5.7)
ER5	TPR sHD	0.34 (0.04) 984.4 (35.7)	0.91 (0.01) 170.3 (34.2)	0.86 (0.16) 236.4 (36.8)	0.08 (0.03) 421.2 (46.2)	0.53 (0.02) 547.9 (63.4)	0.82 (0.05) 244.0 (42.3)	0.93 (0.02) 175.3 (18.9)	0.94 (0.03) 164.8 (17.1)
sF2	TPR sHD	0.48 (0.03) 503.4 (23.8)	0.98 (0.01) 2.3 (1.3)	0.89 (0.14) 156.8 (21.2)	0.04 (0.02) 281.2 (17.4)	0.57 (0.03) 377.3 (53.4)	0.92 (0.03) 54.0 (22.3)	1.00 (0.00) 0.0 (0.0)	1.00 (0.00) 0.0 (0.0)
sF5	TPR sHD	0.47 (0.04) 891.3 (19.4)	0.95 (0.01) 90.2 (34.5)	0.87 (0.15) 165.2 (22.0)	0.05 (0.03) 405.2 (77.4)	0.55 (0.04) 503.7 (56.4)	0.89 (0.03) 114.0 (36.4)	0.97 (0.02) 19.4 (5.2)	0.98 (0.01) 10.8 (6.1)
As we discussed previously, RL-BIC2 only achieves satisfactory results on graphs with 30 nodes.
The TPR of L1OBS is lower than that of A* Lasso, which indicates that L1OBS using greedy
hill-climbing with tabu lists may not find a good ordering. Note that SHD of L1OBS and A* Lasso
reported here are the results of applying our pruning method. We observe that SHD of them is greatly
improved by pruning. For example, specifically, SHD of L1OBS decreases from 171.6 (29.5), 588.0
(66.2) and 1964.5 (136.6) to 85.2 (23.8), 215.4 (26.3) and 481.2 (49.9) in LG ER2 with 30, 50 and
100 nodes respectively, while TPR almost did not degraded. It shows the effectiveness of pruning by
model fitting with a thresholding.
We further evaluate our method on 150-node LG ER2 data models. On the datasets, CORL-1 has
TPR and SHD being 0.95 (0.01) and 63.7 (9.1), while CORL-2 has 0.97 (0.01) and 38.3 (14.3),
respectively. CORL-2 outperforms NOTEARS with 0.94 (0.02) and 50.8 (21.8).
Although both CORL-1 and CORL-2 achieve the de-
sired performance, one can notice that CORL-2 achieves
slightly better SHD than CORL-1. This is a little different
from the usual understanding that RL is usually easier
to learn from dense rewards than from episodic reward
case. We take the 100-node LG ER2 data models as an
example to show the training reward curves of the CORL-1
and CORL-2 in Figure 3. One can notice that CORL-2
converges faster to a better result than CORL-1, which
corresponds to the fact that CORL-2 achieves the slightly
better TPR and SHD than CORL-1. We conjecture that
this is because it is difficult for the critic to learn to predict
the score accurately for each state; in the episodic reward
case, however, it is only required to learn to predict the
score accurately for the initial state.
Figure 3: Learning curves of CORL-1,
CORL-2 and CORL-2-pretrain on 100-
node linear Gaussian dataset.
Pretaining We show the training reward curve of
CORL-2-pretrain in Figure 3, which is CORL-2 based on a pretrained model trained in the su-
pervised way. The testing data task is unseen in the datasets for pretraining which contain 30-node
LiNGAM ER2, 50-node LiNGAM ER2, 30-node LiNGAM SF2 and 30-node GP ER1. Obviously
compared to that of CORL-2 using random initialization (CORL-2), the use of a pretrained model can
accelerate the model learning process. Consistent conclusion can be drawn from the experiment of
CORL-1, see Appendix G. In addition, we consider using the policy model learned only on 30-node
LiNGAM data model as the pretrained model on 100-node LG task. We observe that the performance
is similar to that of using the pretrained model obtained in the supervised way, we do not report the
results repeatedly here.
5.2	Non-Linear Model with Gaussian Process
In this experiment, we consider to use Gaussian Process (GP) to model causal relationships in which
each causal relation f j is a function sampled from a GP with radial basis function kernel of bandwidth
one and normal Gaussian noises, which is known to be identifiable according to Peters et al. (2014).
We set h = 1 and 4 to get ER1 and ER4 graphs, respectively, and generate data by exploiting them
(see Appendix B.2 for details). Our method is evaluated on different sample numbers, the results when
m = 500 are reported here (see Appendix F for additional results). Note that due to the efficiency of
8
Under review as a conference paper at ICLR 2021
(a) TPR on GP10	(b) SHD on GP10
Figure 4: The empirical results on GP data models with 10 and 30 nodes.
(c) TPR on GP30
(d) SHD on GP30
the reward calculation, we only experimented on nonlinear data models of up to 30 nodes scale. The
pruning method for variable selection used here is the CAM pruning from Buhlmann et al. (2014).
The results on 10-node and 30-node datasets with ER1 and ER4 graphs are shown in Figure 3. Here
we only consider some baselines that are competitive in the nonlinear data models, where CAM is a
very strong ordering based baseline. Although GraN-DAG achieves better results than DAG-GNN,
they are worse than CAM overall. We believe this is because 500 samples are so small that GraN-DAG
and DAG-GNN have not learned the good model. RL-BIC2 performs well on 10-node datasets, but
achieves poor results on 30-node datasets, probably due to its lack of scalability. CAM, CORL-1 and
CORL-2 have good results, with CORL-2 achieving the best results on 10-node graphs and slightly
worse than CAM on 30-node graphs. All of these methods have better results on ER1 than on ER4,
especially on 30-node graphs.
5.3	Real Data
The Sachs dataset (Sachs et al., 2005), with 11-node and 17-edge true graph, is widely used for
research on graphical models. The expression levels of protein and phospholipid in the dataset can be
used to discover the implicit protein signal network. The observational dataset has m = 853 samples
and is used to discover the causal structure. GP is used to model the causal relationship in our method.
In this experiment, CORL-1, CORL-2 and RL-BIC2 achieve the best SHD 11. CAM, GraN-DAG, and
ICA-LiNGAM achieve the SHD 12, 13 and 14, respectively. Particularly, DAG-GNN and NOTEARS
result in SHD 16 and 19, respectively, whereas an empty graph has an SHD 17.
6	Conclusion
In this work, we have proposed a RL-based approach for causal discovery named CORL. It searches
the space of variable orderings, instead of the space of directed graphs. We formulate ordering search
as an MDP and have further proposed CORL-1 and CORL-2 for training the ordering generating
model. For a generated ordering, it can be pruned by variable selection to the final DAG. The
empirical results on the synthetic and the real datasets show that our approach is promising.
References
Mark Bartlett and James Cussens. Integer linear programming for the bayesian network structure
learning problem. Artificial Intelligence, 244:258-271, 2017.
Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial
optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk,
Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal
mechanisms. In International Conference on Learning Representations (ICLR), 2020.
Daniel Bernstein, Basil Saeed, Chandler Squires, and Caroline Uhler. Ordering-based causal structure
learning in the presence of latent variables. In International Conference on Artificial Intelligence
and Statistics (AISTATS), pp. 4098-4108. PMLR, 2020.
9
Under review as a conference paper at ICLR 2021
Peter Buhlmann, Jonas Peters, Jan Ernest, et al. CAM: Causal additive models, high-dimensional
order search and penalized regression. The Annals ofStatistics, 42(6):2526-2556, 2014.
David Maxwell Chickering. Learning Bayesian networks is NP-complete. In Learning from Data:
Artificial Intelligence and Statistics V. Springer, 1996.
David Maxwell Chickering. Optimal structure identification with greedy search. Journal of Machine
Learning Research, 3(Nov):507-554, 2002.
James Cussens. Bayesian network learning with cutting planes. In Proceedings of the Twenty-Seventh
Conference on Uncertainty in Artificial Intelligence (UAI), pp. 153-160, 2011.
Cassio P De Campos and Qiang Ji. Efficient structure learning of bayesian networks using constraints.
The Journal of Machine Learning Research, 12:663-689, 2011.
Nir Friedman and Daphne Koller. Being Bayesian about network structure. A Bayesian approach to
structure discovery in Bayesian networks. Machine learning, 50(1-2):95-125, 2003.
Dan Geiger and David Heckerman. Learning Gaussian networks. In Uncertainty Proceedings 1994,
pp. 235-243. Elsevier, 1994.
Geoffrey E Hinton and Russ R Salakhutdinov. A better way to pretrain deep boltzmann machines. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 2447-2455, 2012.
Patrik O. Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear
causal discovery with additive noise models. In Advances in Neural Information Processing
Systems (NeurIPS), 2009.
Biwei Huang, Kun Zhang, Yizhu Lin, Bernhard Scholkopf, and Clark Glymour. Generalized score
functions for causal discovery. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining (KDD), pp. 1551-1560, 2018.
Tommi Jaakkola, David Sontag, Amir Globerson, and Marina Meila. Learning Bayesian network
structure using LP relaxations. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics (AISTATS), pp. 358-365, 2010.
Shantanu Jain, Martha White, and Predrag Radivojac. Recovering true classifier performance in
positive-unlabeled learning. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), 2017.
Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems
(NeurIPS), 2017.
Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT
Press, Cambridge, MA, USA, 2009.
Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems (NeurIPS), 2000.
Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! In
International Conference on Learning Representations (ICLR), 2019.
SebaStien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based
neural DAG learning. In International Conference on Learning Representations (ICLR), 2020.
Pedro Larranaga, Cindy MH Kuijpers, Roberto H Murga, and Yosu Yurramendi. Learning bayesian
network structures by searching for the best ordering with genetic algorithms. IEEE Transactions
on Systems, Man, and Cybernetics-Part A: Systems and Humans, 26(4):487-493, 1996.
Lukas Meier, Sara Van de Geer, Peter Buhlmann, et al. High-dimensional additive modeling. The
Annals of Statistics, 37(6B):3779-3821, 2009.
Ignavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, and Jun Wang. Masked gradient-based
causal structure learning. arXiv preprint arXiv:1910.08527, 2019a.
10
Under review as a conference paper at ICLR 2021
Ignavier Ng, Shengyu Zhu, Zhitang Chen, and Zhuangyan Fang. A graph autoencoder approach to
causal structure learning. arXiv preprint arXiv:1911.07420, 2019b.
Eric Perrier, Seiya Imoto, and Satoru Miyano. Finding optimal bayesian network given a super-
structure. Journal ofMachine Learning Research, 9(Oct):2251-2286, 2008.
Jonas Peters and Peter Buhlmann. Identifiability of gaussian structural equation models with equal
error variances. Biometrika, 101(1):219-228, 2014.
Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Scholkopf. Causal discovery with
continuous additive noise models. Journal of Machine Learning Research, 15(1):2009-2053,
January 2014. ISSN 1532-4435.
Garvesh Raskutti and Caroline Uhler. Learning directed acyclic graph models based on sparsest
permutations. Stat, 7(1):e183, 2018.
Pradeep Ravikumar, John Lafferty, Han Liu, and Larry Wasserman. Sparse additive models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 71(5):1009-1030, 2009.
Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauffenburger, and Garry P Nolan. Causal protein-
signaling networks derived from multiparameter single-cell data. Science, 308(5721):523-529,
2005.
Mauro Scanagatta, Cassio P de Campos, Giorgio Corani, and Marco Zaffalon. Learning bayesian
networks with thousands of variables. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1864-1872, 2015.
Mark Schmidt, Alexandru Niculescu-Mizil, Kevin Murphy, et al. Learning graphical model structure
using L1-regularization paths. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), 2007.
Gideon Schwarz et al. Estimating the dimension of a model. The Annals of Statistics, 6(2):461-464,
1978.
Shohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, and Antti Kerminen. A linear non-Gaussian
acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030,
2006.
Tomi Silander and Petri Myllymaki. A simple approach for finding the globally optimal bayesian
network structure. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 445-452, 2006.
Liam Solus, Yuhao Wang, and Caroline Uhler. Consistency guarantees for greedy permutation-based
causal inference algorithms. arXiv preprint arXiv:1702.03530, 2017.
Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, Prediction,
and Search. MIT press, 2000.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems, 2000.
Marc Teyssier and Daphne Koller. Ordering-based search: A simple and effective algorithm for
learning bayesian networks. In Conference on Uncertainty in Artificial Intelligence (UAI), 2005.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSZ
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems (NeurIPS), 2017.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems (NeurIPS), 2015.
11
Under review as a conference paper at ICLR 2021
Jing Xiang and Seyoung Kim. A* lasso for learning a sparse bayesian network structure for continuous
variables. In Advances in Neural Information Processing Systems (NeurIPS), 2013.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural
networks. In International Conference on Machine Learning (ICML), 2019.
Jiji Zhang. On the completeness of orientation rules for causal discovery in the presence of latent
ConfoUnders and selection bias. Artificial Intelligence, 172(16):1873 - 1896, 2008.
Kun Zhang and AaPo Hyvarinen. On the identifiability of the post-nonlinear causal model. In
Conference on Uncertainty in Artificial Intelligence (UAI), 2009.
Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS:
Continuous optimization for structure learning. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.
Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal discovery with reinforcement learning. In
International Conference on Learning Representations (ICLR), 2020.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International
Conference on Learning Representations (ICLR), 2017.
12
Under review as a conference paper at ICLR 2021
Appendix
A Network Architectures and Hyper-Parameters
A.1 Multiple Network Architecture Designs for Encoder and Decoder
There are a variety of neural network modules for encoder and decoder structures. Here we consider
some representative modules: including , Multi-layer Perceptrons (MLP) module, an LSTM based
recurrent neural network module, and the self-attention based encoder in the Transformer. In addition,
we use the original observation data as the state directly, i.e., no encoder module is used, to show
the necessity of encoder, which is denoted as Null. MLP consists of 3-layer feed-forward neural
networks with 256, 512 and 256 units. The architecture of LSTM and Transformer are that introduced
in Appendix A.2.
The empirical results of CORL-2 on 30-node LG ER2 datasets are reported in Table 3. We observe
that LSTM decoder achieves a better performance than that of MLP decoder. This shows that LSTM
is more effective than MLP in sequential decision tasks. Besides, the overall performance of neural
network encoder is better than that of Null, which shows that the data pre-processed by encoder
module is necessary. Among all these encoders, Transformer encoder achieves the best results.
Similar conclusion was drawned in Zhu et al. (2020). We hypothesize that the performance of
Transformer encoder is benefit from the self-attention scheme.
Table 3: Empirical results of CORL-2 with different choices of encoder and decoder on 30-node LG
ER2 datasets. The smaller SHD the better, the higher TPR the better.
	Encoder			
	Null	LSTM	MLP	Transformer
MLP Decoder	TPR 0.81 (0.07) SHD 54.2 (25.1)	0.86 (0.10) 36.0 (26.7)	0.96 (0.02) 11.0 (5.3)	0.98 (0.02) 5.0 (3.3)
LSTM Decoder	TPR 0.94 (0.04) SHD 20.6 (20.0)	0.88 (0.09) 29.0 (17.8)	0.97 (0.01) 8.6 (4.3)	0.99 (0.01) 4.4 (3.5)
A.2 Network Architecture and Hyper-Parameters of CORL
fc:
input
multi-head
block@3
output
Sl
Sj
Sd
Figure 5: Illustration of the Transformer encoder. The encoder embeds the observed data Xj of each
variable j into the state Sj. Notation block@3 denotes three blocks here.
Both CORL-1 and CORL-2 use the actor-critic algorithm. We use the Adam optimizer with learning
rate 1e-4 and 1e-3 for actor and critic respectively. The discount factor Y is set to 0.98. The actor
13
Under review as a conference paper at ICLR 2021
Figure 6: Illustration of the LSTM decoder. At each time step, it maps the state ^t to a distribution
over action space A := {aι,...,ad}, then an action (variable) can be selected randomly according
to the distribution.
consists of an encoder and a decoder. We illustrate the neural network structure of the Transformer
encoder used in our experiments in Figure 5. It consists of a feed-forward layer with 256 units and
three blocks. Each block is composed of a multi-head attention network with 8 heads and 2-layer
feed-forward neural networks with 1024 and 256 units. Each feed-forward layer is followed by a
normalization layer. Given a batch of observed samples with shape b X d X n, where b denotes the
batch size, d the node number and n the number of observed data for each variable in a batch, the
final output of the encoder is a batch of embedded state with shape b X d X 256.
We illustrate the neural network structure of the decoder in Figure 6, which is mainly a LSTM similar
to the decoder proposed by Vinyals et al. (2015). The LSTM takes a state as input and outputs a
embedding. The embedding is mapped to the action space A by using some feed-forward neural
networks, a soft-max module and the pointer mechanism (Vinyals et al., 2015). The outputs of the
encoder are processed as the initial hidden state h0 of the decoder. The LSTM with 256 hidden units
is used here. All of the feed-forward neural networks used in decoder have 256 units.
The critic uses 3-layer feed-forward neural networks with 512, 256 and 1 units. It takes a state S
as input and outputs a predicted value for the current policy given state S. For CORL-1, the critic
needs to predict the score for each state. For CORL-2, the critic takes the initial state ^o as input and
outputs a predicted value directly for a complete ordering.
B	Baselines and Date Sets
B.1	Details of Baselines
The details of all the baselines considered in our experiments are listed as follows:
•	ICA-LiNGAM assumes linear non-Gaussian additive model for data generating procedure
and applies independent component analysis to recover the weighted adjacency matrix. This
method can usually achieve good performance on LiNGAM datasets. However, it does not
provide guarantee for linear Gaussian datasets. 1
•	NOTEARS recovers the causal graph by estimating the weighted adjacency matrix with the
least squares loss and the smooth characterization for acyclicity constraint. 2
•	DAG-GNN formulates causal discovery in the framework of variational autoencoder and
optimizes a weighted adjacency matrix with the evidence lower bound and a modified
smooth characterization on acyclicity as loss function. 3
1https://sites.google.com/site/sshimizu06/lingam
2https://github.com/xunzheng/notears
3https://github.com/fishmoon1234/DAG-GNN
14
Under review as a conference paper at ICLR 2021
•	GraN-DAG models the conditional distribution of each variable given its parents with feed-
forward neural networks. It uses the smooth directed acyclic constraint from NOTEARS to
find a DAG that maximizes the log-likelihood of the observed samples. 4
•	RL-BIC2 formulates the causal discovery as a one-step decision making process and uses
score function and acyclic constraint from NOTEARS to calculate the reward for the
recovered directed graph. 5
•	CAM conducts a greedy estimation procedure that starts with an empty DAG and adds at
each iteration the edge (vk, vj) between nodes vk and vj that corresponds to the largest
gain in log-likelihood. For a searched ordering, they prune it to the final DAG by applying
significance testing of covariates. They perform the preliminary neighborhood selection to
reduce the ordering space size searched. 6
•	L1OBS performs heuristic search (greedy hill-climbing with tabu lists) through the space
of topological orderings to search a ordering with the best score, then it uses L1 variable
selection to pruning the searched ordering (fully-connected DAG) to the final DAG. 7
•	A* Lasso with a limited queue size incorporates a heuristic scheme into a dynamic pro-
gramming based method. It first prunes the search space by using A* Lasso, then it further
pruning the search space by limiting the size of the priority queue in the OPEN list of A*
Lasso. The queue size usually needs to be adjusted to balance the time cost and the quality
of the solution. 8
B.2	Details on Datasets Generation
We generate synthetic datasets which vary along five dimensions: level of edge sparsity, graph
type, number of nodes, causal functions and sample size. We sampled 5 datasets of the required
number examples for each task as follows: a ground truth DAG G is drawn randomly from either the
ErdGs-Renyi (ER) or Scale-free (SF) graph model; then, the data is generated according to a specific
sampling scheme.
Specifically, for Linear Gaussian (LG) case, we set h ∈ {2, 5} and d ∈ {30, 50, 100} to obtain the
ER graph and SF graph (different types) with different levels of edge sparsity and different number
of nodes, respectively. Then 5 datasets of 3000 examples are generated for each task following
X = WTX + , where W ∈ Rd×d denotes the weight matrix which is obtained by assigning edge
weights independently from Unif([-2, -0.5] ∪ [0.5, 2]). Note that ’s are standard Gaussian noises
with equal variances for each variable so as to LG data model is identifiable (Peters & Buhlmann,
2014).
For LiNGAM data model, the datasets are generated in the similar way with LG but the sampling
for are different. The non-Gaussian noises are obtained by following Shimizu et al. (2006) which
passes the noise samples from Gaussian distribution through a power nonlinearity to make them
non-Gaussian. LiNGAM is identifiable shown in Shimizu et al. (2006).
Another data generating process is GP. We first obtain graph with different density and different
number of nodes. Then the datasets with different sample sizes are generated following Xj =
fj (Pa(Xj)) + j with jointly independent Gaussian noises for all j where the function fj is a function
sampled from a GP with radial basis function kernel of bandwidth one and normal Gaussian noises.
This setting is known to be identifiable according to Peters et al. (2014). Note that due to the efficiency
of the reward calculation, we only experimented on nonlinear data models of up to 30 nodes.
C Total Number of Episodes Before Convergence
Table 4 reports the total number of episodes required for CORL-2 and RL-BIC2 to be converged,
averaged over five datasets. Note that the episodic reward is evaluated once per episode. CORL
4https://github.com/kurowasan/GraN-DAG
5https://github.com/huawei-noah/trustworthyAI/tree/master/Causal_
Structure_Learning/Causal_Discovery_RL
6 https://cran.r- project.org/web/packages/CAM.
7https://www.cs.ubc.ca/~murphyk/Software/DAGlearn/
8http://www.cs.cmu.edu/~jingx/software/AstarLasso.zip
15
Under review as a conference paper at ICLR 2021
formulates causal discovery as a multi-stage decision process and we observe that CORL performs
fewer episodes than RL-BIC2 before the episode reward converges. We suspect that due to a
significant reduction in the size of action space, the model complexity of the RL policy is reduced
thus leading to higher sample efficiency. Some runtimes about them are also provided here (CORL-2
total runtime ≈ 15 minutes against RL-BIC2 ≈ 3 hours for 30-node ER2 graphs, ≈ 4 hours against
≈ 14 hour for 50-node ER2 graphs, and CORL-2 ≈ 7 hours for 100-node ER2 graphs). We set the
maximal runtime up to 24 hours, but RL-BIC2 did not converge within that time on 100-node graphs,
hence we did not report it here. Note that these runtime may be significantly reduced by parallelizing
the evaluation of reward.
Table 4: Total number of iterations (×103) before RL converge on LG data.
	30 nodes		50 nodes		100 nodes	
	ER2	ER5	ER2	ER5	ER2	ER5
CORL-2	1.0 (0.3)	1.1 (0.4)	1.9 (0.3)	2.4 (0.3)	2.3 (0.5)	2.9 (0.4)
RL-BIC2	3.9 (0.5)	4.1 (0.6)	3.4 (0.4)	3.5 (0.5)	×	×
D Additional Results on Linear Gaus s ian Datasets
The results for 50-node LG data models are presented in Table 5. The conclusion is similar to the 30-
and 100-node experiments. The results of ICA-LiNGAM, GraN-DAG and CAM on LG data models
are presented in Table 6. Their performances do not compare favorably to CORL-1 nor CORL-2 in
LG datasets.
It is not surprising that ICA-LiNGAM does not perform well because the algorithm is specifically
designed for non-Gaussian noise and does not provide guarantee for LG data models. We hypothesize
that CAM’s poor performance on LG data models is because it uses nonlinear regression instead
of linear regression. As for GraN-DAG, it uses 2-layer feed-forward neural networks to model the
causal relationships, which may not be able to learn a good linear relationship in this experiment.
Table 5: Empirical results for ER and SF graphs of 50 nodes with LG data. The higher TPR the better,
the smaller SHD the better.
		RANDOM	NOTEARS	DAG-GNN	RL-BIC2	L1OBS	A* Lasso	CORL-1	CORL-2
ER2	TPR SHD	0.31 (0.03) 295.4 (28.5)	0.94 (0.02) 38.6 (10.8)	0.94 (0.04) 30.6 (8.3)	0.79 (0.10) 88.5 (49.3)	0.56 (0.02) 288.0 (66.2)	0.88 (0.03) 154.3 (27.6)	0.97 (0.04) 24.0 (32.3	0.97 (0.02) 17.9 (10.6)
ER5	TPR SHD	0.32 (0.02) 378.4 (24.2)	0.90 (0.01) 67.8 (7.5)	0.87 (0.14) 93.2 (109.4)	0.74 (0.03) 128.9 (40.4)	0.57 (0.03) 299.4 (53.6)	0.82 (0.03) 104.0 (28.3)	0.90 (0.02) 68.3 (10.2)	0.92 (0.02) 64.8 (13.1)
SF2	TPR SHD	0.49 (0.04) 215.6 (14.7)	0.99 (0.01) 3.5 (1.6)	0.90 (0.13) 79.3 (93.2)	0.84 (0.05) 115.2 (57.4)	0.67 (0.02) 182.3 (33.4)	0.89 (0.03) 124.0 (35.2)	1.00 (0.00) 0.0 (0.0)	1.00 (0.00) 0.0 (0.0)
SF5	TPR SHD	0.51 (0.03) 345.6 (24.3)	0.94 (0.12) 20.1 (14.3)	0.88 (0.12) 89.2 (99.2)	0.75 (0.05) 115.2 (57.4)	0.61 (0.03) 217.3 (36.4)	0.81 (0.02) 131.0 (25.3)	0.94 (0.03) 24.3 (11.2)	0.95 (0.02) 20.8 (10.1)
E Results on 30-, 50- and 1 00-Node LiNGAM Datasets
Here, we report the empirical results on 30-, 50- and 100-node LiNGAM datasets in Table 7. The
observed samples are generated according to the same procedure with linear Gaussian datasets (see
Appendix B.2 for details). The non-Gaussian noise is obtained by passing the noise samples from
Gaussian distribution through a power nonlinearity to make them non-Gaussian. For L1OBS, we
increased the authors’ recommended number of evaluations 2500 to 10000. For A* Lasso, we set the
queue size to 10, 500 and 1000, and report the best result for all of these different parameter settings.
The results of L1OBS and A* Lasso reported here are that of pruning using our pruning method. For
other baselines, we pick the recommended hyper-parameters.
Among all these algorithms, ICA-LiNGAM can recover the true graph on most of the LiNGAM data
models. This is because ICA-LiNGAM is specifically designed for non-Gaussian noise. CORL-1
and CORL-2 achieve consistently good results than other baselines.
16
Under review as a conference paper at ICLR 2021
Table 6: Empirical results of ICA-LiNGAM, GraN-DAG and CAM (against CORL-2 for reference)
for ER and SF graphs with LG data. The higher TPR the better, the smaller SHD the better.
		ICA-LiNGAM		GraN-DAG	CAM	CORL-2
30 nodes	ER2	TPR	0.75 (0.03)	0.51 (0.17)	0.47 (0.05)	0.99 (0.01)
		SHD	112.3 (12.8)	96.0 (11.3)	110.8 (10.3)	4.4 (3.5))
	ER5	TPR	0.57 (0.03)	0.52 (0.03)	0.46 (0.02)	0.95 (0.03)
		SHD	161.8 (9.2)	175.2 (27.4)	191.3 (32.5)	37.6 (14.5)
	SF2	TPR	0.58 (0.05)	0.61 (0.04)	0.63 (0.02)	1.0 (0.0)
		SHD	149.0 (19,8)	136.4 (21.2)	115.2 (26.7)	0.0 (0.0)
	SF5	TPR	0.56 (0.04)	0.58 (0.02)	0.60 (0.03)	1.0 (0.0)
		SHD	160.5 (8.9)	142.4 (24.3)	122.2 (17.4)	0.0 (0.0)
50 nodes	ER2	TPR	0.73 (0.03)	0.11 (0.04)	0.55 (0.06)	0.97 (0.02)
		SHD	108.8 (11.3)	173.0 (22.9)	140.8 (35.4)	17.9 (10.6)
	ER5	TPR	0.57 (0.01)	0.64 (0.03)	0.61 (0.02)	0.92 (0.02)
		SHD	199.8 (90.7)	154.2 (36.4)	178.3 (34.8)	64.8 (13.1)
	SF2	TPR	0.59 (0.04)	0.44 (0.05)	0.57 (0.02)	1.00 (0.00)
		SHD	208.5 (83.2)	158.6 (34.5)	131.2 (24.4)	0.0 (0.0)
	SF5	TPR	0.57 (0.01)	0.49 (0.04)	0.53 (0.03)	0.95 (0.02)
		SHD	216.6 (88.4)	243.9 (27.2)	235.2 (34.2)	20.8 (10.1)
100 nodes	ER2	TPR	0.73 (0.02)	0.38 (0.02)	0.43 (0.02)	0.98 (0.01)
		SHD	268.4 (28.5)	191.3 (31.9)	126.4 (27.8)	18.6 (5.7)
	ER5	TPR	0.57 (0.05)	0.42 (0.03)	0.47 (0.02)	0.94 (0.03)
		SHD	311.1 (63.7)	208.2 (54.4)	182.3 (34.9)	164.8 (17.1)
	SF2	TPR	0.69 (0.03)	0.40 (0.03)	0.44 (0.02)	1.00 (0.00)
		SHD	367.6 (67.5)	239.9 (43.2)	35.2 (37.4)	0.0 (0.0)
	SF5	TPR	0.57 (0.05)	0.39 (0.03)	0.48 (0.04)	0.98 (0.01)
		SHD	362.3 (82.8)	219.3 (32.2)	125.2 (24.7)	10.8 (6.1)
Table 7: Empirical results on 30-, 50- and 100-node LiNGAM ER2 datasets. The smaller SHD the
better, the higher TPR the better.
30 nodes ER2	50 nodes ER2	100 nodes ER2
Method	TPR	SHD	TPR	SHD	TPR	SHD
ICA-LiNGAM	1.00 (0.00)	0.0 (0.0)	1.00 (0.00)	0.0 (0.0)	1.00 (0.00)	1.0 (0.9)
NOTEARS	0.94 (0.04)	17.2 (13.2)	0.95 (0.02)	33.2 (16.5)	0.94 (0.03)	69.2 (23.2)
DAG-GNN	0.94 (0.03)	19.6 (10.5)	0.96 (0.01)	24.6 (2.9)	0.93 (0.03)	66.2 (19.2)
GraN-DAG	0.28 (0.09)	100.8 (14.6)	0.20 (0.01)	177.0 (25.9)	0.16 (0.04)	312.8 (25.2)
RL-BIC2	0.94 (0.07)	19.8 (23.0)	0.80 (0.08)	86.0 (51.9)	0.13 (0.12)	291.3 (24.1)
CAM	0.60 (0.11)	310.0 (34.0)	0.33 (0.07)	178.0 (31.9)	0.53 (0.05)	247.2 (32.1)
L1OBS	0.72 (0.04)	85.3 (23.3)	0.47 (0.02)	212.6 (24.6)	0.41 (0.03)	470.5 (48.1)
A* Lasso	0.87 (0.03)	42.3 (16.3)	0.88 (0.03)	82.6 (17.6)	0.85 (0.04)	102.5 (22.6)
CORL-1	0.99 (0.01)	3.8 (6.4)	0.96 (0.06)	24.6 (37.7)	0.98 (0.01)	20.0 (7.9)
CORL-2	0.99 (0.01)	3.9 (5.6)	0.96 (0.08)	20.2 (11.3)	0.99 (0.01)	13.8 (7.2)
F	Results on 20-Node GP Datasets with Different Sample Sizes
We take the 20-node GP data models as an example to show the performance of our method on
different sample numbers. We set h = 4 to get ER4 graphs and generate data by using them. We
illustrate the empirical results in Table 8. Since previous experiments have shown that CORL-2
is slightly better than CORL-1, we only report the results of CORL-2 here. CAM as the most
competitive baseline, we also report its results on these datasets. TPR reported here is calculated
based on the variable ordering. We can see that, as the sample size decreases, CORL-2 ends up
outperforming CAM. We believe this is because CORL-2 benefits from the exploratory ability of RL.
G CORL- 1 with a Pretrained Model
We show the training reward curves of CORL-1 and CORL-1-pretrain which is CORL-1 based on
a pretrained model in Figure 7. To obtain a pretraining model with good generalization ability, we
combine various types data described in Appendix B.2 with different levels of edge sparsity, graph
17
Under review as a conference paper at ICLR 2021
Table 8: Empirical results on 20-node GP ER1 datasets with different sample sizes. The smaller SHD
the better, the higher TPR the better.
Sample size	Name	TPR	SHD
1000	CAM	0.91 (0.03)	30.0 (3.7)
	CORL-2	0.87 (0.03)	36.5 (3.1)
500	CAM	0.86 (0.03)	45.0 (2.5)
	CORL-2	0.85 (0.03)	46.3 (2.3)
400	CAM	0.83 (0.02)	51.0 (2.7)
	CORL-2	0.84 (0.03)	50.5 (3.0)
200	CAM	0.60 (0.03)	66.3 (1.9)
	CORL-2	0.75 (0.02)	63.1 (1.5)
① po-d 山
-4.6-
-4.8-
-5.0-
-5.2-
-5.4-
-5.6-
-5.8-
-6.0-
-6.2-
0	200	400	600	800	1000
Episode
Figure 7: Learning curves of CORL-1 and CORL-1-pretrain on 100-node LG datasets.
type and number of nodes and data generating process to construct observation samples. Next, we
train a policy model by supervised learning on the mixed datasets. Finally, we use the pretrained
model as a start point on the task that has never been seen before during pretraining.
From Figure 7, we can observe that although the pretrained model is trained on other types of datasets,
it can still accelerate the training as the initial model on the LG dataset task. This shows that our
policy model may learn some implicit knowledge across different tasks.
18