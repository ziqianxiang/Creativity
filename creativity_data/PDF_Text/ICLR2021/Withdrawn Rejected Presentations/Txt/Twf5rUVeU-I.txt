Under review as a conference paper at ICLR 2021
Convergence Analysis of Homotopy-SGD for
non-convex optimization
Anonymous authors
Paper under double-blind review
Ab stract
First-order stochastic methods for solving large-scale non-convex optimization
problems are widely used in many big-data applications, e.g. training deep neural
networks as well as other complex and potentially non-convex machine learning
models. Their inexpensive iterations generally come together with slow global
convergence rate (mostly sublinear), leading to the necessity of carrying out a very
high number of iterations before the iterates reach a neighborhood of a minimizer.
In this work, we present a first-order stochastic algorithm based on a combination
of homotopy methods and SGD, called Homotopy-Stochastic Gradient Descent
(H-SGD), which finds interesting connections with some proposed heuristics in
the literature, e.g. optimization by Gaussian continuation, training by diffusion,
mollifying networks. Under some mild assumptions on the problem structure,
we conduct a theoretical analysis of the proposed algorithm. Our analysis shows
that, with a specifically designed scheme for the homotopy parameter, H-SGD
enjoys a global linear rate of convergence to a neighborhood of a minimum while
maintaining fast and inexpensive iterations. Experimental evaluations confirm the
theoretical results and show that H-SGD can outperform standard SGD.
1 Introduction
This paper focuses on the theoretical development and analysis of a stochastic optimization algorithm,
called Homotopy-Stochastic Gradient Descent (H-SGD), based on the combination of homotopy
methods and stochastic gradient descent (SGD). The algorithm we propose is specifically designed to
solve finite-sum problems of the following form
w* ∈ arg min
w∈Rd
1N
f (W) = N X fj(W),
(1)
where f : Rd → R is continuously differentiable, bounded below and not necessarily convex. In
particular, we assume that we only have access to noisy function values and gradients of the objective
function in equation 1 via a stochastic first-order oracle, as in (Nemirovski et al., 2009) and (Ghadimi
& Lan, 2013). Problems of this form typically arise in machine learning and deep learning applications,
where the dimensionality of the datasets makes the full function and gradient evaluations too expensive.
This class of problems is generally approximately solved by stochastic first-order iterative algorithms,
e.g. SGD (Bottou et al., 2018), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2015). At the
iteration t, the algorithms of this class acquire a stochastic estimate of the function value f(Wt, ξt)
and the gradient g(Wt, ξt) by calling the oracle with input Wt, where ξt is a random variable, i.e.
when the noise comes from subsampling as in the mini-batch scenarios, then ξt ∈ {0, 1}N with
kξtkι = M and g(wt, ξt)=吉 PN=I ξt,j ∙ Vfj(wt). In the case of SGD, for a given wo ∈ Rd and
α > 0, the iterates are generated as follows
wt+1 := wt - αg(wt, ξt) .
(2)
Consequently, the iterate wt+1 = wt+1(ξ[t]) is a function of the history ξ[t] := (ξ0, . . . , ξt) (also w0
should be included in case it is a random initial point) of the generated random process and hence is
itself random.
1
Under review as a conference paper at ICLR 2021
In general, stochastic first-order methods enjoy fast convergence when the problem is characterized
by a certain structure. In particular, when the Polyak-匕CjasieWicz (PL) condition (See Karimi et al.,
2016, for more details on the PL condition) holds for the objective function in Problem 1, then,
with a “small enough” value for the step-size, the SGD iterates converge linearly to a minimizer’s
neighborhood (Karimi et al., 2016; Vaswani et al., 2019). Unfortunately, in many machine learning
applications, the PL condition is not a realistic assumption as the landscape is generally characterized
by the presence of multiple local minima and saddle points (Dauphin et al., 2014; Kawaguchi, 2016;
Karimi et al., 2017). At the same time, in the vicinity of the minimizers the problems generally show
stronger structures, i.e. PL or even strong convexity, allowing for a faster local convergence (Karimi
et al., 2017). In such a scenario, a smart initialization hence becomes crucial for the numerical
performance of the method (Sutskever et al., 2013b). Unfortunately, the power of the existing smart
initialization heuristics is often quite limited given the small knowledge of the problem’s landscape
which we generally dispose of. In addition, these heuristics typically can not guarantee that the
SGD iterates start “close enough” to a minimizer, i.e. in the region where the PL condition holds,
such that the method enjoys a linear rate of convergence. Therefore, the ideal scenario would be
to be able to exploit the stronger local structure while the method’s iterates gradually approach a
minimizer and independently from the starting point. In this regard, homotopy methods are a general
strategy for tackling difficult optimization problems by gradually transforming a simplified version of
a target problem, or a version with a known minimizer, back to its original form while following a
solution along the way. Consequently, they preserve in each step the vicinity to a minimizer of the
currently tackled problem, allowing the solver to always work in regions where the problems exhibit
stronger structures. In general terms, homotopy methods (Allgower & Georg, 2003) are a widely
and successfully used mathematical tool to efficiently solve various problems in numerical analysis,
e.g. (Deuflhard, 2011), (Liao, 2012). Such methods are also suitable to solve complex non-convex
optimization problems where no or only little prior knowledge regarding the localization of the
solutions is available, allowing for the exploitation of the stronger local structures of the problems in
order to achieve fast global convergence, e.g. (Xiao & Zhang, 2012; Lin & Xiao, 2014; Suzumura
et al., 2014; Gargiani et al., 2020).
In this work, we propose a stochastic first-order numerical method to solve Problem 1, called
Homotopy-Stochastic Gradient Descent (H-SGD), which is based on the combination of the homotopy
method and SGD. After introducing the method and discussing the related work (Section 2), our
contributions are as follows
1.	In Section 3, we provide a general theoretical analysis of H-SGD under some mild assump-
tions, showing that, if the increments in the homotopy parameter are “small enough”, the
proposed method tracks in expectation an r-optimal solution across homotopy iterations. We
then show that, in the same setting, H-SGD can achieve a global linear rate of convergence
to a minimizer’s neighborhood when used in combination with a specific schedule for the
homotopy parameter, i.e. ∆λi decreases exponentially across homotopy iterations.
2.	In Section 4, we empirically evaluate the performance of H-SGD. Our experiments not only
confirm the theoretical results derived in Section 3 but also show that H-SGD with a smartly
designed homotopy map can outperform SGD.
2 Homotopy- S GD
Homotopy-Stochastic Gradient Descent (H-SGD) is based on the combination of the homotopy
method and SGD, in the hope of combining the best of both worlds. In particular, the goal is
that of maintaining the advantageous properties of SGD, such as its cheap iterations and fast local
convergence under PL condition, while maximally exploiting the stronger local structures via the
homotopy scheme. Therefore, H-SGD relies on the definition of a homotopy map f (w, λ) : Rd ×
[0, 1] → R, such that, when λ = 0 we recover a well-behaved function, e.g. convex, or a function
with a known minimizer’s localization, and by increasing the λ parameter, also called homotopy
parameter, we gradually morph it in order to finally end up with our target objective function
f(w, 1) = f(w) (see Suciu, 2016, for more details on homotopy functions). By using such a
homotopy map, H-SGD finds an approximate solution of Problem 1 by approximately solving a
series of parametric problems that gradually leads to the target one. In particular, in each homotopy
2
Under review as a conference paper at ICLR 2021
iteration i, H-SGD tackles a parametric problem of the form
Wi ∈ arg min f (w, λi),	(3)
w∈Rd
where the homotopy parameter λi is slightly increased at each homotopy iteration. As confirmed by
our theoretical analysis, if the variations of the homotopy parameter, i.e. ∆λi, are “small enough”
across homotopy iterations, the method is able to track in expectation an r-optimal solution from
source to target problem. As shown in Algorithm 1, H-SGD takes as input an approximate solution
for the problem associated with f(w, 0), i.e. w0, and is then characterized by two loops: in the outer
loop (homotopy iterations) the method defines a new objective function by increasing the homotopy
parameter (line 5 in Algorithm 1), and in the inner loop (warm-started SGD iterations) the current
homotopy problem is approximately solved with k iterations of SGD starting from the previously
derived approximate solution, i.e. wi-1 (line 6 in Algorithm 1). Different functions h : N → (0, 1] to
determine the increment ∆λi in the homotopy parameter at each homotopy iteration can be used. As
shown in our analysis, this function greatly impacts on the method’s properties and convergence rate.
In particular, our theoretical analysis confirms that, when a specifically designed scheme for ∆λi is
deployed, i.e. exponentially decreasing schedule, H-SGD is effective in guaranteeing a global linear
rate of convergence to a neighborhood of a minimizer of our target problem, while, given the same
setting, vanilla SGD can only ensure a global sublinear rate of convergence.
Algorithm 1 Homotopy-Stochastic Gradient Descent (H-SGD)
1:	input: w0 ∈ Rd, n ∈ N, k ∈ N, h : N → (0, 1] with Pin=1 h(i) = 1 and α > 0
2:	initialization: i = 0, λ0 = 0
3:	for i = 1, . . . , n do
4:	∆λi — h(i)
5：	λi - λi-i + ∆λi
6：	Wi -SGD(Wi-i,α, k, f (∙, λi))
7: output: wi
2.1 Related Work
Finding a solution of Problem 1 when the objective function is non-convex is often quite challenging.
Different heuristics hence have been proposed to speed up and improve the optimization of such
problems, many of which to be used in combination with stochastic first-order methods such as
SGD. In this regard, the proposed method, despite being new in its general formulation and analysis,
finds many interesting similarities and connections with existing heuristics in the machine learning
literature, e.g. (Bengio et al., 2009; Hinton et al., 2012; Sutskever et al., 2013a). We now briefly
discuss some of the state-of-the-art optimization techniques and initialization strategies for solving
Problem 1 that are most related to H-SGD, drawing connections with existing and ongoing research
works and in the hope that our analysis can also lead to a new interpretation of some widely used
techniques which so far lack a more rigorous theoretical description and analysis.
Graduated Optimization. The graduated optimization approach (Blake & Zisserman, 1987), also
known as coarse-to-grained optimization method, is a general heuristic to solve complex non-convex
problems that relies on the basic principles of the homotopy method. As the name suggests, at first
a coarse-grained and “easy-to-solve” version of the target problem is generated via a smoothing
operation. The method then proceeds by gradually refining the problem versions, using the previous
solution as initial point. Graduated optimization has been utilized explicitly and implicitly as heuristic
in many machine learning and computer vision applications, e.g. object localization (Mobahi et al.,
2012), manifold learning (Gashler et al., 2007), optical flow (Brox & Malik, 2011). Unfortunately,
many of these techniques have practical and/or theoretical gaps, as they generally lack a rigorous
running time and convergence analysis, and/or, as in (Mobahi & Fisher III, 2014) and (Hazan
et al., 2016), they rely on an expensive method, i.e. Gaussian smoothing, to construct coarse-
grained versions of the original target problem. Regarding theoretical contributions on graduated
optimization methods for solving Problem 1, Hazan et al. (2016) are the first and only, to the best
of our knowledge, to provide a theoretical analysis for the running time and convergence rate of
a graduated optimization method based on an approximate, yet still expensive, type of Gaussian
smoothing and SGD. Unfortunately, their analysis shows two major limitations. First, it relies on their
3
Under review as a conference paper at ICLR 2021
Gaussian smoothing approximation as homotopy map, which limits the generality of the conducted
analysis, while our analysis is independent from the specific formulation of the homotopy map used.
Second, the analysis is based on the assumption of local strong convexity, which is a quite strong
requirement and hence might lead to considerably smaller local regions than those considered in our
analysis (Karimi et al., 2017). To conclude this short overview on graduated optimization, many
successful optimization heuristics proposed in the machine learning literature are implicitly related to
graduated optimization and, consequently, to homotopy methods, such as curriculum learning (Bengio
et al., 2009), simulated annealing (Kirkpatrick et al., 1983), noise injection techniques (Hinton et al.,
2012), smart initialization (Sutskever et al., 2013a) and layer-wise pretraining (Bengio et al., 2006).
Transfer Learning. Due to the massive amount of computational resources required by the develop-
ment of modern machine learning applications, the community has started to explore the possibility
of re-using learned parameters across different tasks, leading to the development of many new
transfer-learning algorithms, e.g. (Torrey & Shavlik, 2010; Pan & Yang, 2010; Yosinski et al., 2014;
Gargiani et al., 2020). A simple yet often effective way to transfer knowledge across different tasks
consists in using warm-start initialization. In this perspective, transfer-learning boils down to a sort
of smart initialization heuristic. A first connection between homotopy methods and transfer-learning
was underlined in (Gargiani et al., 2020). The authors propose a transfer-learning algorithm based on
the homotopy method and SGD via the definition of a homotopy map that transforms a source task
into a target task. The method comes together with a general theoretical analysis that is independent
from the specific homotopy map adopted and shows that, under some assumptions, the algorithm can
track in expectation an approximate solution from source to target task, i.e. optimality-tracking. Un-
fortunately, the method’s analysis is limited as it only considers constant increments of the homotopy
parameter, which automatically degrades the linear rate of the local solver to a sublinear one for the
homotopy-based method. In addition, as in (Hazan et al., 2016), the analysis relies on the assumption
of local strong convexity, which might hold in a significantly smaller neighborhood of the minimizers
than the PL condition (Karimi et al., 2017).
3	Theoretical Analysis
In this section, we provide a general theoretical analysis of H-SGD as described in Algorithm 1. In
particular, after discussing the required underlying assumptions (Section 3.1), and the fundamental
theoretical preliminaries (Section 3.2), first we analyze the optimality tracking properties of the
proposed method (Section 3.3), and then we show that, with a specifically designed scheme for the
homotopy parameter, H-SGD enjoys linear convergence to a minimizer’s neighborhood (Section 3.4).
The analysis we conduct is independent from the specific type of homotopy map adopted and it
applies to any scenario where the assumptions listed in Section 3.1 hold.
Recall that the proposed method is based on sequentially and approximately solving a series of n
unconstrained parametric problems of the form
arg min f(w, λi) , ∀i = 1, . . . , n ,	(4)
w∈Rd
where λi < λi+1, λn = 1, λi ∈ (0, 1]. In addition, H-SGD relies on the availability ofan approximate
solution w0 for the source problem with λ0 = 0 as starting point. We use wi to denote the derived
approximate solution for the problem associated with parameter λi that is obtained by applying k > 0
iterations of SGD starting from the previously derived approximate solution for the problem with
parameter λi-1, ∀i = 1, . . . , n.
Notice that wi-1,t = wi-1,t (ξ[i-1,t-1] ) for t = 1, . . . , k with wi-1,k = wi (ξ[i] ) is used
to refer to the random vector generated at the i-th homotopy iteration after t iterations
of SGD, where ξ[i-1,t-1] = (w0,ξ01,...,ξk1-1,...,ξ0i-1,...,ξki--11,ξ0i,...,ξti-1) and ξ[i] =
(w0,ξ01,...,ξk1-1,...,ξ0i-1,...,ξki--11,ξ0i,...,ξki-1) with ξ[0] = w0 are used to refer to the col-
lection of all random sources UP to the current iteration. We use U * (λ) to denote the set of local (and
global) minimizers of the parametric Problem 1.
3.1	Assumptions
We now list and discuss the assumPtions that we consider throughout our analysis. Together with
the standard smoothness and bounded variance assumPtions, we also introduce three regularity
4
Under review as a conference paper at ICLR 2021
assumptions, which describe the localization of the solution map and how the objective function f
changes by varying the homotopy parameter across iterations. In addition to these assumptions, we
also consider a more general and local version of the standard PL condition.Consequently, unlike
the settings considered in (Karimi et al., 2016) and (Vaswani et al., 2019), where the standard PL
condition is unrealistically required to hold globally, ours is often encountered in many different
non-convex scenarios (see Karimi et al., 2017, for more details).
Assumption 3.1 (existence of a regular localization of the solution map). Assume there exists a set
Ω ⊆ Rd X [0,1]z such that W*(λ) := Ω ∩ U*(λ) and Σ := {(y, λ) ∈ Rd X [0,1]z | y ∈ W*(λ)}
are both non-empty and connected. Moreover we assume that for a given λ all the points in W *(λ)
are associated with the same objective function value, which we denote as f *(λ) := f (y, λ) for all
y ∈ W*(λ).
Notice that Assumption 3.1 does not imply vector-valued solutions of the parametric Problem 1.
Assumption 3.2 (regularity 1). Assume there exists δ > 0 such that
|f (w,λ) - f(w,^)∣≤ δkλ - ^k,	∀w ∈ Rd, ∀人 ^ ∈ [0,1]z .	(5)
Assumption 3.3 (regularity 2). Assume there exists γ > 0 such that
. , ~.	. ʌ . . .. ~ ʌ .. ~ ʌ - -一
If*(λ)- f *(λ)∣≤ γkλ - ^k,	∀λ, ^ ∈ [0,1]z .	(6)
Assumption 3.4 (L-smoothness). Assume there exists L > 0 such that
∣∣Vwf(W,λ)-Vwf (W,λ)k≤ L ∣∣W - Wk,	∀W,W ∈ Rd, ∀λ ∈ [0,1]z .	⑺
See Remark C.1 in Section C of the Appendix for more details on Assumptions 3.2 and 3.3.
Assumption 3.5 (bounded “variance”). Consider f(w, λ) with λ ∈ [0, 1]z and let g(w, ξ, λ) be the
stochastic estimate of the true gradient Vwf (w, λ) used in SGD with noise ξ. Assume that
Eξ [g(w, ξ, λ)] =Vwf(w, λ),	∀w∈Rd,∀λ∈ [0,1]z.	(8)
and that there exists σ2 ≥ 0 such that
Eξ	kg(w, ξ, λ) -	Vwf(w, λ)k2 ≤σ2,	∀w∈Rd,∀λ∈ [0,	1]z.	(9)
Assumption 3.6 (“expected” PL condition). Consider f(w, λi) with λi ∈ [0, 1]z and let wi-1,t =
wi-1,t(ξ[i-1,t-1]) denote the iterate that is obtained at the i-th homotopy iteration after t iterations of
2
SGD with t ≤ k. Assume that there exist B > σμ andμ > 0 such that, if Eξ[.-ι t-i] [f (wi-ι,t, λi)] —
f*(λi) ≤ B, then
Eξ[.-I,t-1] [kVwf (Wi-1,t, λi)k2] ≥ 2μ ∙ [Eξ[i-I,t-1] [f(Wi-I,t, λi)] - f*(λi)] .	(10)
See Remark C.2 for additional details on Assumption 3.6.
3.2	Fundamental Theoretical Preliminaries
Before proceeding with the main theoretical contributions, we revise and adjust the existing results in
the literature on global error bounds of SGD, i.e. (Vaswani et al., 2019), to also hold in the considered
setting. The extended results are then used for the derivations in Section 3.3 and 3.4.
Proposition 3.7. Consider f(w, λi) with λi ∈ [0, 1]z and let wi-1,t = wi-1,t(ξ[i-1,t-1] ) denote
the iterate obtained at the i-th homotopy iteration by applying t iterations of SGD with t ≤ k - 1
and α ≤ L. Under Assumptions 3.1 and 3.4-3.6, if Eξ[.-ι 一[[f (wi-ι t, λi)] 一 f *(λi) ≤ B, then
Eξ[.-1,t][f(wi-1,t+1,λi)]-f*(λi) ≤B.	,
Proof. See Section D in the Appendix for a proof.	口
Theorem 3.8. Consider the minimization of f(w, λi) with λi ∈ [0, 1]z via SGD. Let wi-1 =
wi-1(ξ[i-1]) be the random initial point associated with the i-th homotopy iteration with
5
Under review as a conference paper at ICLR 2021
Eξ[i-1] [f(wi-ι,λi)] - f*(λi) ≤ B and Wi-ι,t = Wi-1,t(ξ[i-1,t-i]) denote the t-th SGD iter-
ate with t ≤ k. Under Assumptions 3.1 and 3.4-3.6, SGD with a constant step-size α ≤ L attains the
following convergence rate to a minimizer’s neighborhood
Eξ[i-ι,t-i] [f (Wi-ι,t, Xi)- f *(λi)] ≤ d坟”1][f (Wi-1, Xi)- f *(λi)] + 2μ,
(11)
with P ：= (1 一 αμ). With a = L, we obtain P =(1 一 L).
Proof. See Section E in the Appendix for a proof.
□
3.3	Optimality Tracking
In the following, We define the function φv(λi) := Ev [f (v, λi)] 一 f * (λi) where V is d-dimensional
real random vector. As in (Gargiani et al., 2020) but in a more relaxed setting, i.e. local PL in place
of local strong-convexity, we study the optimality tracking properties of H-SGD. In particular, under
the considered assumptions and by exploiting the previously introduced results on the convergence of
SGD, with Theorem 3.10 we characterize the maximum allowed variation of the homotopy parameter
across homotopy iterations of H-SGD such that, if φwi(Xi) ≤ r, then also φwi+1 (Xi+1) ≤ r. The
upper bound that we derive depends on the number of iterations k performed with SGD as well as
on the convergence characteristics of SGD and the structural properties of the parametric problems.
This result applied recursively across homotopy iterations leads to conclude that, if we adopt a “small
enough” increasing step for the homotopy parameter, H-SGD can track in expectation an r-optimal
solution from source to target problem.
Before proceeding with the actual optimality tracking analysis (Theorem 3.10), we study the con-
ditions on Wi and ∆Xi+1 such that φwi(Xi+1) ≤ B, where Wi is the approximate solution of the
problem associated with parameter Xi that is also used as starting point for the next parametric
problem.
Lemma 3.9. Assume ∣∣λi+ι 一 λik ≤ G 0 ≤ E < δBγ and let Wi denote the i -th iterate of Al-
gorithm 1 with α ≤ L. Under Assumptions 3.1- 3.3 and 3.4- 3.6, if φwi (λi) ≤ B — (δ + Y)e,
then φwi(λi+ι) ≤ B. In addition, let kmax ：= log。(1 — 2""[B"σ ) . If φwi(λi+ι) ≤ B,
0 ≤ e < δ++γ (B 一 σμ ) and k ≥ kmax, then φwi+ι (λi+1) ≤ B 一 (δ + Y)e.
Proof. See Section F in the Appendix for a proof.
2
Theorem 3.10.	Assume there exists σμ < r ≤ B and E := min {ei, e?} with
1 ∕r 、	(1 一Pk)T-σ2∕2μ
e1 ：= (δ+^)(B - r), e2 ：= -Pk (δ + Y)-.
□
(12)
In addition, let kmax ：
logρ (1 - ⅞r)
Consider Algorithm 1 with α ≤ L, k ≥ kmax and
∣Xi+1 一 Xi∣ ≤ E, where 0 ≤ E ≤ EE. Under Assumptions 3.1- 3.3 and 3.4- 3.6, if φwi (Xi) ≤ r, then
φwi+1(Xi+1) ≤ r.
Proof. See Section G in the Appendix for a proof.
□
See Figure 2 in Section B of the Appendix for a graphical representation of the derived results.
3.4 Linear Convergence Rate
We now study the convergence rate of H-SGD and, in particular, if it is possible to recover a global
linear rate of convergence to a minimizer’s neighborhood. The results derived in Theorem 3.11
confirm that, in the considered setting and with a specifically designed schedule for the homotopy
parameter, H-SGD achieves the desired rate of convergence.
6
Under review as a conference paper at ICLR 2021
Theorem 3.11.	Let P ∈(1 — σμ B, 1)and consider Algorithm 1 with α ≤ L, φw0 (λo) ≤ r with
σ∣(1-P) ≤ r ≤ B and k ≥ logρ(p). In addition, let ∈ι := 熊1Y) (B — r) and
(C := ] P-Pk
I Pk (δ+γ)
if k ≥ lθgρ (P)- lθgρ
otherwise,
(13)
γ
with εo := Eξ[0] [f (wo, λo)] — f *(λo). UnderAssumptions 3.1- 3.3 and 3.4- 3.6, ifkλi+ι 一 λik ≤
min {e-ηi, eι} with η ≥ ln (CP P) ,then
σ2 i
Eξ[i+i] [f(wi+ι, λi+ι)] — f*(%+ι) ≤ ρi+1 Rm [/(wo, λo)] — f*(λo)] +- Xρj
μ j=o
(14)
Proof. See Section H in the Appendix for a proof.
□
4	Experimental Evaluation
In this section, we empirically validate the theoretical results derived in Theorem 3.11. First, we
consider a 1-dimensional toy regression problem to illustrate and visualize some of the basic properties
of H-SGD. In this easy scenario, the introduced assumptions can be trivially verified by inspection
(see the Figures 3- 4 in the Appendix). We then move to more complex and high-dimensional
scenarios where the assumptions can not be verified. Inspired by Finn et al. (2017) and Gargiani et al.
(2020), we consider the task of regressing with a neural network from input to output of a sinusoidal
wave. Finally, we also consider a non-convex classification task based on the combination of logistic
regression with a non-linear model for moon-shaped binary data (Chapelle et al., 2006). In all the
considered scenarios, we compare the numerical performance of H-SGD with those of SGD and tune
the step-size based on the performance of the latter.
4.1	Toy-Problem
We start with an easy regression problem motivated by Mobahi (2016): a 1-dimensional neural
network with erf as activation function (see Figure 9 in Section B of the Appendix for a graphical
representation). We generate a synthetic dataset of N = 100 samples, where xj ∈ [—1, 1], yj =
3 ∙ Xj + Ej and Ej ~ N (0, 1). Regarding the choice of a value for the step-size, We use an estimate
L of the smoothness constant L and set α = 1/L. As loss, We use the mean squared error, Which,
composed With the regressing model, leads to the folloWing non-convex optimization problem
1N
w* = argmiR N ∑(yj' — erf(w . Xj))2 .	(15)
By plotting the objective function With respect to w (see Fig. 3 in Section B of the Appendix), it is
easy to observe that the PL condition holds globally, with the value of μ increasing by approaching
the minimizer and μ → 0 for W → 土∞ (see Fig. 4 in the Appendix). Consequently, SGD enjoys a
global linear convergence rate as proved in (Vaswani et al., 2019) but the rate itself, which depends
on the value of μ, will dramatically worsen the further the iterates are from the minimizer, leading to
a great overall sensitivity of the method in terms of convergence rate to the initialization. On the other
side, H-SGD, thanks to the homotopy principle, goes around that issue by preserving the vicinity to
the minimizer of the current homotopy problem at each homotopy iteration (see Figures 7 and 8 in
Section B of the Appendix). In order to achieve that, given wo as initial value, we set yj,o = wo ∙ Xj
for all j = 1, . . . , N and define the following homotopy transformation
yj,λ = λyj + (1 — λ)yj,o .	(16)
As suggested by the theory (see Theorem 3.11), we select an exponentially decreasing scheme for
the increment ∆λi in order to achieve linear convergence. As shown in Figure 5 in the Appendix,
both H-SGD and SGD enjoy a linear rate of convergence, but H-SGD shows a superior numerical
performance. This is due to the fact that the method is designed such that its iterates always lie in the
neighborhood of the minimizers where more favorable values of μ lead to a faster convergence. This
fact allows H-SGD to enjoy a faster global convergence rate than that of SGD.
7
Under review as a conference paper at ICLR 2021
4.2	Regression with Deep Neural Networks
Our second experiment is inspired by Finn et al. (2017) and Gargiani et al. (2020) and focuses on
studying the numerical performance of H-SGD on the task of regressing from input to output of a
sinusoidal function corrupted by Gaussian noise. In particular, the input data are sampled uniformly
from the interval [—1, 1] and yj = sin(10 ∙ Xj) + Ej with Ej 〜N(0,0.1) for all j = 1,..., 500. The
regressor is a feedforward neural network with two hidden layers, each of 10 units, and hyperbolic
tangent as activation function. As for the previous benchmark, we use the mean squared error as
loss. We employ the same values of step-size α and mini-batch M for both H-SGD and SGD, where
the step-size value is tuned based on the numerical performance of SGD for the selected mini-batch
size (M = 5). Regarding H-SGD, We set yj∙,o = xj + Ej with Ej 〜N(0, 0.01) and employ the
same homotopy mapping as in Equation 16. As shown in Figure 6 in the Appendix, also in this
scenario H-SGD shows a superior numerical performance than SGD, i.e. H-SGD reaches a loss
of 10-1 roughly 4 times faster and achieves convergence more than 2 times faster than SGD. The
superior numerical performance of H-SGD can be attributed to its ability of tracking a solution across
homotopy iterations (see Figure 10 in the Appendix) which ensures the method to always work in the
vicinity of a minimizer.
4.3	Non-Linear Binary Classification with Logistic Regression
Finally, we test H-SGD also on a classification benchmark with a logistic regression task. In particular,
we use a 2-dimensional binary moon-shaped dataset (Chapelle et al., 2006) with 1000 samples
corrupted by Gaussian noise. As the dataset is clearly not linearly separable, we opt for a cubic
model, which, used in combination with the logistic regression framework, leads to a non-convex
objective function where the optimization variable w is the collection of the model’s coefficients.
For both H-SGD and SGD we use a mini-batch size of 20 and tune the value of the step-size on
the SGD’s performance for that mini-batch size. Regarding H-SGD, we use as source task the one
obtained considering a linear instead of a cubic model, which results in a convex and hence “easy”
optimization problem. We then gradually increase the non-linearity of the model, i.e. non-convexity
of the problem, until reaching in the final homotopy iteration the target problem with the desired
cubic model. This homotopy map is obtained by multiplying the coefficients of the non-linear terms
in the model by λ as follows
33222	2
λ(c1 xj3,1 + c2 xj3,2 + c3 xj2,1 + c4 xj2,2 + c5 xj2,1 xj,2 + c6 xj,1 xj2,2) + c7 xj,1 + c8 xj,2 + c9 . (17)
For the homotopy parameter we adopt the increasing schedule that is suggested in Theorem 3.11.
H-SGD outperforms SGD by reaching an error of 0.1 more than two times faster than SGD (see
Figure 11 in the Appendix).
5	Conclusions and Future Work
In this paper we propose a new first-order stochastic method for non-convex large-scale problems,
called Homotopy-SGD (H-SGD), based on the combination of homotopy methods and SGD. This
new homotopy-based optimization method allows one to exploit easy-to-solve or already-solved
problems to solve new and complex ones. This is achieved by approximately and sequentially
solving a sequence of optimization problems where the source problem is gradually morphed via
a homotopy map into the target one. We conduct a theoretical analysis of the optimality tracking
properties and convergence rate of H-SGD under some realistic and mild assumptions. The theoretical
results are confirmed by some empirical evaluations, which also show the great potential in terms of
performance of combining SGD with an homotopy strategy. In addition, H-SGD shows interesting
connections with many practical existing heuristics proposed in the machine learning literature to
speed up the convergence of first-order methods, allowing for a new and more rigorous interpretation
of the latter. The current major limitation of the method relies in the design of the homotopy map.
Future work should focus on exploiting the specific problem structure to design optimal homotopy
maps. Moreover, under additional assumptions, more theoretical results concerning the quality of the
tracked solutions could be derived.
8
Under review as a conference paper at ICLR 2021
Acknowledgments
References
E. L. Allgower and Kurt Georg. Introduction to Numerical Continuation Methods. Society for
Industrial and Applied Mathematics, USA, 2003. ISBN 089871544X.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. In Proceedings of the 19th International Conference on Neural Information
Processing Systems, NIPS'06,pp.153-160, Cambridge, MA, USA, 2006. MIT Press.
Yoshua Bengio, J6r6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 41-48,
New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi:
10.1145/1553374.1553380. URL https://doi.org/10.1145/1553374.1553380.
Andrew Blake and Andrew Zisserman. Visual Reconstruction. MIT Press, Cambridge, MA, USA,
1987. ISBN 0262022710.
L6on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018. ISSN 0036-1445. doi: 10.1137/16M1080173.
Thomas Brox and Jitendra Malik. Large displacement optical flow: descriptor matching in variational
motion estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):
500-513, 2011. URL http://lmb.informatik.uni-freiburg.de/Publications/
2011/Bro11a.
O. Chapelle, B. Scholkopf, and A. Zien. Semi-Supervised Learning. Adaptive computation and
machine learning. MIT Press, Cambridge, MA, USA, September 2006.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
and Yoshua Bengio. Identifying and attacking the saddle point problem in high-
dimensional non-convex optimization. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 27, pp. 2933-2941. Curran Associates, Inc., 2014. URL http:
//papers.nips.cc/paper/5486- identifying- and- attacking- the- saddle-
point-problem-in-high-dimensional-non-convex-optimization.pdf.
Peter Deuflhard. Newton Methods for Nonlinear Problems: Affine Invariance and Adaptive Algo-
rithms. Springer Publishing Company, Incorporated, 2011. ISBN 364223898X.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011. URL
http://jmlr.org/papers/v12/duchi11a.html.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning -
Volume 70, ICML’17, pp. 1126-1135. JMLR.org, 2017.
Matilde Gargiani, Andrea Zanelli, Quoc Tran Dinh, Moritz Diehl, and Frank Hutter. Transferring
optimality across data distributions via homotopy methods. In International Conference on
Learning Representations (ICLR), 2020.
Michael Gashler, Dan Ventura, and Tony Martinez. Iterative non-linear dimensionality reduction with
manifold sculpting. In Advances in Neural Information Processing Systems, volume 20, 01 2007.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic
non-convex problems. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA,
June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1833-1841.
JMLR.org, 2016. URL http://proceedings.mlr.press/v48/hazanb16.html.
9
Under review as a conference paper at ICLR 2021
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. arXiv e-prints, 2012.
arXiv:1207.0580.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-undefinedojasiewicz condition. In European Conference on
Machine Learning and Knowledge Discovery in Databases - Volume 9851, ECML PKDD 2016,
pp. 795-811, Berlin, Heidelberg, 2016. Springer-Verlag. URL https://doi.org/10.1007/
978-3-319-46128-1_50.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence under the Polyak-Ecjasiewicz
inequality, February 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 586-594. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6112-
deep-learning-without-poor-local-minima.pdf.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220
(4598):671-680, 1983. doi: 10.1126/science.220.4598.671.
Shijun Liao. Homotopy Analysis Method in Nonlinear Differential Equations. Springer-Verlag Berlin
Heidelberg, 2012. ISBN 978-3-642-25131-3. doi: 10.1007/978-3-642-25132-0_6.
Qihang Lin and Lin Xiao. An adaptive accelerated proximal gradient method and its homo-
topy continuation for sparse optimization. In Eric P. Xing and Tony Jebara (eds.), Proceed-
ings of the 31st International Conference on Machine Learning, volume 32 of Proceedings
of Machine Learning Research, pp. 73-81, Bejing, China, 22-24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/lin14.html.
Hossein Mobahi. Training recurrent neural networks by diffusion. arXiv e-prints, 2016.
arXiv:1601.04114.
Hossein Mobahi and John W. Fisher III. On the link between gaussian homotopy continuation and
convex envelopes. In Xue-Cheng Tai, Egil Bae, Tony F. Chan, and Marius Lysaker (eds.), Energy
Minimization Methods in Computer Vision and Pattern Recognition - 10th International Conference,
EMMCVPR 2015, Hong Kong, China, January 13-16, 2015. Proceedings, volume 8932 of Lecture
Notes in Computer Science, pp. 43-56. Springer, 2014. URL https://doi.org/10.1007/
978-3-319-14612-6_4.
Hossein Mobahi, C.L. Zitnick, and Yi Ma. Seeing through the blur. In Proceedings / CVPR, IEEE
Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, pp. 1736-1743, 06 2012. ISBN
978-1-4673-1226-4. doi: 10.1109/CVPR.2012.6247869.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):
1574-1609, 2009.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. on Knowl. and Data
Eng., 22(10):1345-1359, October 2010. ISSN 1041-4347. doi: 10.1109/TKDE.2009.191. URL
https://doi.org/10.1109/TKDE.2009.191.
Alexandru Suciu. Math 4565 topology homotopy, Spring 2016. URL https://
web.northeastern.edu/suciu/MATH4565/MATH4565-sp16-handout2.pdf.
10
Under review as a conference paper at ICLR 2021
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. In Proceedings of the 30th International Conference on
International Conference on Machine Learning - Volume 28, ICML’13, pp. 111-1139-111-1147.
JMLR.org, 2013a.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of
the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pp. 1139-1147, Atlanta, Georgia, USA, 17-19 Jun 2013b. PMLR. URL
http://proceedings.mlr.press/v28/sutskever13.html.
Shinya Suzumura, Kohei Ogawa, Masashi Sugiyama, and Ichiro Takeuchi. Outlier path: A ho-
motopy algorithm for robust svm. In Eric P. Xing and Tony Jebara (eds.), Proceedings of
the 31st International Conference on Machine Learning, volume 32 of Proceedings of Ma-
chine Learning Research, pp. 1098-1106, Bejing, China, 22-24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/suzumura14.html.
Lisa Torrey and Jude Shavlik. Transfer learning, 2010.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for
over-parameterized models and an accelerated perceptron. In Kamalika Chaudhuri and
Masashi Sugiyama (eds.), Proceedings of Machine Learning Research, volume 89 of Pro-
ceedings of Machine Learning Research, pp. 1195-1204. PMLR, 2019. URL http://
proceedings.mlr.press/v89/vaswani19a.html.
Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares
problem. SIAM Journal on Optimization, 23(2):1062-1091, 2012. doi: 10.1137/120869997.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems 27, pp. 3320-3328. Curran Associates,
Inc., 2014. URL http://papers.nips.cc/paper/5347-how-transferable-are-
features- in- deep- neural- networks.pdf.
11
Under review as a conference paper at ICLR 2021
A Appendix
B Additional Figures
Figure 2: Graphical representation of the re-
sults derived in Theorem 3.10 on the optimal-
ity tracking properties of H-SGD for a general
non-convex 1-dimensional function. In par-
ticular, as shown in the figure, under the con-
sidered assumptions and for “small enough”
variations of the homotpy parameter, H-SGD
tracks in expectation an r-optimal solution
across homotopy iterations.
Oooooo
0 5 0 5 0 5
FT 3JF7 旨【3J
.o W
Figure 1: Graphical representation of a 1-
dimensional function with wi-ι,t ~ N (0, 1)
that satisfies the “expected” PL condition (As-
sumption 3.6) but not the classical PL condi-
tion in the considered region.
Figure 3: Graphical representation of the ob-
jective function in Problem equation 15 vs
w.
10-2
IO0
IoT
(*IAJIlsJ
Z=aJa∙0
10^5
Figure 4: Visualization of the estimated μ
parameter for the objective function in Prob-
lem equation 15 vs w .
12
Under review as a conference paper at ICLR 2021
Figure 5: Expected optimality gap of H-SGD
(blue) and SGD (black) averaged across 100
runs vs epochs for the toy-case described in
Section 4.1
Figure 6: Expected loss of H-SGD (blue)
and SGD (black) averaged across 100 runs
vs epochs for the sine-wave regression case
described in Section 4.2
Figure 7: Visualization of the homotopy ob-
jective functions vs w for different values of
homotopy parameter. The black stars repre-
sent (wi, f (wi, λi)), i.e. the H-SGD iterates
with associated objective function value.
Figure 8: Visualization of the target objec-
tive functions vs w. The black stars represent
(wk, f(wk)), i.e. the SGD iterates with asso-
ciated objective function value.
13
Under review as a conference paper at ICLR 2021
Figure 9: Graphical representation of the 1-dimensional neural network deployed for the toy-case
experiment described in Section 4.1.
(a) λ = 0.63
(b) λ = 0.75
(c) λ = 0.86
Figure 10: Predicted values y∖ (blue) vs true values y∖ (black) for different values of λ generated by
using H-SGD.
(d) λ = 0.98
14
Under review as a conference paper at ICLR 2021
1±
W
O 200	400	600	800 IOOO 1200	1400
epochs
Figure 11: Error averaged across 100 runs of H-SGD (blue) and SGD (black) vs epochs for the
classification task described in Section 4.3.
C	Additional Remarks
Remark C.1. Assumption 3.2 can be read as a local Lipschitz continuity of f with respect to
its second argument. Regarding the second regularity assumption (Assumption 3.3), equation 6
relates the variation of the optimal objective function value for the subset of solutions introduced in
Assumption 3.1 across homotopy iterations with the variations of the homotopy parameter. When
the solution localization of the solution map is vector-valued, i.e. w*(λ) ≡ W *(λ) (this is the case
for instance when the Hessian of the objective function is positive definite at those points), this
assumption can also be derived directly by combining Assumption 3.2 with the following Lipschitz
continuity requirements:
• Let κ1 > 0, we assume that
....~. ^. . . . ^. ^.. .. . .~.	. ^... 〜 ^ 一 ,..
If (w*(λ), λ) - f(w*(λ), ^)∣≤ κιkw*(λ) - w*(^)k, ∀入 ^ ∈ [0,1]z .	(18)
• Let κ2 > 0, we assume that
.. ..~ . . ʌ . ..	.. ~ ʌ .. 〜 ʌ 一 ,..
kw*(λ) - w*(λ)k ≤ K2» - ^k, ∀入 λ ∈ [0,1]z .	(19)
In particular, by combining Inequalities equation 18 and equation 19, we obtain
....〜、ʌ. . . .ʌ. ʌ. . ..~ ʌ.. ~ ʌ 一 ,..
|f (w*(λ), ^) - f(w*(^),λ)∣≤ K2 κιkλ - ^k, ∀λ, ^ ∈ [0,1]z .	(20)
To recover Inequality equation 6 where γ := δ+κ1 κ2, we use Inequalities equation 5 and equation 20
together with the triangle inequality as follows
. . ..〜、 〜、 . ..ʌ . ʌ. . . . . . ~. 〜、 . ..~. ʌ. . . . ~. ʌ. . . . ʌ . ʌ..
|f (w*(λ), X)- f(w*(λ), ^)∣ = If (w*(λ),λ) - f (w*(λ), λ) + f (w*(λ), λ) - f (w*(λ), ^)∣
. . ..〜、 〜、 . ..~.	ʌ . . . . . . ~.	ʌ . . . .ʌ.	ʌ ..
≤ If (w*(λ),λ)	- f (w*(λ),	λ)∣ + If (w*(λ),	λ) -	f (w*(λ),	λ)∣
≤ (δ + κ1κ2)kx - Ak .
(21)
15
Under review as a conference paper at ICLR 2021
Notice that the Assumption in equation 18 is a less restrictive condition of the following general
Lipschitz continuity requirement
|f (v,^) - f(w,^)∣≤ Ckv - wk,	∀v, w ∈ Rd, ∀^ ∈ [0,1]z ,	(22)
where C > 0.
Remark C.2. Assumption 3.6 is a more general version of the classical PL condition (see Section 2
Karimi et al., 2016, for more details on the classical PL condition). In particular, it is straightforward
to observe that the classical PL condition implies Assumption 3.6, but not vice versa. See Figure 1
in Section B of the Appendix for a graphical representation of a one dimensional example with
wi-ι,t 〜N(0,1) where Assumption 3.6 holds, while the classical PL Condition does not The
expected value operates a smoothing of the function landscape resulting in convexity, while the
original function shows many bumps that make it non-convex.
D Proof of Proposition 3.7
Proposition D.1. Consider f(w, λi) with λi ∈ [0, 1]z and let wi-1,t = wi-1,t(ξ[i-1,t-1]) denote
the iterate obtained at the i-th homotopy iteration by applying t iterations of SGD with t ≤ k - 1
and α ≤ L. Under Assumptions 3.4, 3.5 and 3.6, if Eξ1.-ι 一〕[f (wi-ι t, λi)] 一 f *(λi) ≤ B, then
Eξ*i,t][f(wi-ι,t+ι,λi)] - f *(") ≤ B.	,	'
Proof. For ease of notation, we use the shorthands wi-1,t = wi-1,t (ξ[i-1,t-1] ) and gt
g(wi-1,t, ξti, λi).
Considering Assumption 3.4 together with the definition of SGD iterate and using Assumptions 3.5
and 3.6, we obtain the following inequalities
Eξ[i-I,t] [f (wi-1,t+1,	λi )]	≤ Eξ[.-ι,t]	f (wi-1,t,	Xi)- α"f (Wi-1,t, λi),	gti	+	^ IlgtIl2
Law of Iterated Exp.
=	Eξ[.-1,t-1]
Lα2
f(wi-1,t, λi) - αR f(wi-1,t, λi), gti +------------2 Ilgtll ∣ξ[i-1,t-1]
Assumption 3.5
≤
Eξ[.-1,t-1]	f(wi-1,t, λi) +	-α+
∣∣Vf(Wi-l,t, λi)k2 +
Lα2σ2
-2-
If α ≤ L, then
α	Lα2 σ2
Eξ[i-ι,t]	[f (wi-1,t+1,	λi)] ≤	Eξ[i-ι,t-i]	[f ]Wi-1,t,	λi)	+ (-2)	∣∣Vf (wi-l,t	λi)k ] +	2 —.
(23)
We now make use of the “expected” PL condition and derive the following inequalities
22
Eξ[.-ι,t] [f]wi-1,t+1, λi) - f (wi-1,t, λi)] ≤ -2Eξ[i-ι,t-i] [∣Vf]wi-l,t, λi)∣∣2] +	2—
Assumption 3.6	Lα2σ2
≤	-αμ [Eξ[i-I,t-1] [f]wi-1,t, λi)] - f*(λi)] +	2—.
(24)
2
From Inequality equation 24 it follows that, whenever Eξ[.-ι 一耳[f (wi-ι,t, λi)] - f * (λi) ≥ 2σμ,
the objective function decreases in expectation, i.e. Eξ[.-1 t] [f(Wi-1,t+1, λi) - f(Wi-1,t, λi)] ≤ 0.
2
Given that by assumption B > σ-,we can consequently conclude that, If Eξ[.-ι =一][f (Wi-ι,t, λi)]-
f (w*,λi) ≤ B, then Eξ[it][f]Wi-中+],%)] - f*(%) ≤ B.	* '	'口
E Proof of Theorem 3.8
Theorem E.1. Consider the minimization of f(W, λi) with λi ∈ [0, 1]z via SGD. Let Wi-1 =
Wi-1(ξ[i-1]) be the random initial point associated with the i-th homotopy iteration with
16
Under review as a conference paper at ICLR 2021
Eξ[i-1] [f(wi-ι,λi)] - f*(λi) ≤ B and wi-ι,t = wi-1,t(ξ[i-1,t-i]) denote the t-th SGD iter-
ate with t ≤ k. Under Assumptions 3.4, 3.5 and 3.6, SGD with a constant step-size α ≤ L attains
the following convergence rate to a minimizer’s neighborhood
Eξ[i-I,t-1] [f(Wi-1,t, Xi)- f*(λi)] ≤ ρtEξ[i-i] [f (wi-1, Xi)- f*(λi)] + 2μ ,	(25)
with P ：= (1 一 aμ). With a = L, we obtain P =(1 一 L).
Proof. For ease of notation, we use the shorthands wi-1,t = wi-1,t(ξ[i-1,t-1] ) and gt
g(wi-1,t, ξti, Xi).
By combining Assumption 3.4 with the definition of SGD iterate, for all t = 1, . . . , k - 1, we obtain
the following bound on f(wt+1, Xi)
f (wi-1,t+1, λi) ≤ f(wi-1,t, λi) + hVf (Wi-1,t, λi), wi-1,t+1 - wi-1,ti + ɪ Ilwi-1,t+1 - wi-1,tk2
WitTiigt f (Wi-I,t, λi) - αhVf (Wi-i,t, λi),gti + L02IgtI2 .
We now take the expectation with respect to all the sources of randomness involved and then we
apply the law of iterated expectations, together with Assumption 3.5
Eξ[i-ι,t]	[f (wi-1,t+1,	λi)]	≤ Eξ[i-ι,t]	f (wi-1,t,	Xi)- αhvf (Wi-1,t, λi),	gti	+	2 IIgtll2
LawofIt=atedExp. Eξ[i-1,t-1 Je Jf (Wi-ι,t, λiihVf(wi-ι,t, λi),gti + L02IIgtlI2∣ξ[i-ι,t-ι]
Assumption 3.5	Lα2	Lα2σ2
≤	Eξ[i-ι,t-i]	f(wi-ι,t, λi) + (-α +	2J kvf(wi-ι,t, Xi)Il +	2—.
If α ≤ L, then
22
Eξ[i-ι,t] [f(wi-1,t+1, λi)] ≤ Eξ[i-I,t-1] [f(wi-1,t, Xi) + (-2) ∣∣vf(wi-1,t, λi)k2] +	2	.
(26)
We now apply the PL condition to Inequality equation 26, and we obtain
Assumption 3.6
Eξ[i-1,t] [f (Wi-1,t+1, Xi)]	≤	Eξ[i-1,t-1] [f (Wi-1,t, Xi)
(27)
-αμ( f (wi-1,t, Xi) - f *(Xi))] +
L02σ2
2^^
By subtracting f * (Xi) on both sides and setting α = L, we obtain the following inequality
Eξ[i-ι,t] [f (WiT,t+1, Xi)- f *(Xi)] ≤ (1 - μ) Eξ[i-ι,t-i] [f (Wi-1,t, Xi)- f *(Xi)] +
σ2
2L
(28)
By applying Inequality equation 28 recursively, we derive the following bound
Eξ[i-ι,t-i] [f (Wi-1,t,	Xi)- f*(Xi)]	≤	(1	-	L)	Eξ[i-i]	[f (Wi-1,	Xi)- f*(Xi)]
2 k-1
+⅛ χ(ι-μ )j.
j=0
Finally, by using the limit of geometric series, we obtain
Eξ[i-ι,t-i] [f (Wi-1,t,	Xi)- f *(Xi)]	≤ (1 - L)	Eξ[i-1]	[f (Wi-1,	Xi)- f * (Xi)]	+	2μ
(29)
(30)
□
17
Under review as a conference paper at ICLR 2021
F	Proof of Lemma 3.9
LemmaE1. Assume ∣∣λi+ι 一 λik ≤ G 0 ≤ e < δBγ and let Wi denote the i-th iterate ofAlgorithm 1
with α ≤ L. UnderAssumptions 3.4- 3.3 and3.5- 3.6, if φwi(λi) ≤ B — (δ + Y )e, then φwi (λi+ι) ≤
B. In addition, let
2 Γ1 2μ	2μ(δ + Y)E + σ∖]
kmax : - IogP I 1	ʒ~W	I	.	(31)
∖	2μB	J
If φWi (λi+l) ≤ B, 0 ≤ e < δ++γ (B 一 2μ) and k ≥ kmax, then 加计1 (λi+l) ≤ B 一 (δ + Y)e∙
Proof. We start by deriving an upper bound on Eg® [f (wi, λi+ι)] — f * (λi+ι) with φwi (λi) ≤ B
and ∣λi+1 一 λi ∣ ≤ E. For that we use the regularity Assumptions 3.2 and 3.3 together with the
triangle and Jensen inequalities as follows
Eξ[i] [f(wi, λi+l)] — f*(λi+l) = lEξ[i] [f(wi, λi+l)] — f*(λi+J
=	lEξ[i] [f (wi, λi+1) + f (Wi, λi ) — f(wi, λi)] - f*(λi+l)
+ f *(λi) — f *(λi)∣
Triangle and Jensen Ineq.	(32)
≤	lEξ[i]	[f (wi,	λi)]	— f *(λi)1	+ Eξ[i]	[|f (wi,	λi+1)	—	f (Wi	λi )|]
+ If *(λi) — f *(λi+ι)∣
Assumptions 3.2 and 3.3
≤	∣Eξ[i] [f(wi, %)] — f*(λi)∣ + (δ + γ)e.
From Inequality equation 32 it follows that, if φwi(λi) ≤ B — (δ + Y)e with E < 黄)), then
φwi(λi+1) ≤ B.
We now use the results of Theorem 3.8 to derive a lower bound on the number of SGD-steps such that,
if φwi(λi+1) ≤ B, then φwi+1 (λi+1) ≤ B — (δ + Y)E. We start considering the following inequality
2
Eξ[i+i] [f(wi+1, λi+l)] — f*(λi+l) ≤ ρk [Eξ[i] [f(wi, λi+1)] 一 f*(λi+l)] + 2~
2	μ (33)
≤ Pk B + -
2μ
From Inequality equation 33 it follows that, if φwi (λi+1) ≤ B, then φwi+1 (λi+1) ≤ B — (δ + Y)E
with E < δ+γ (B — σ2) whenever
k≥
logρ I」辿「
(34)
□
G Proof of Theorem 3.10
2
Theorem G.1. Assume there exists 丽 < r ≤ B and E := min {ei, E2} with
E1 =(j⅛(B—r),
(1 — Pk) r — σ2 * * * * *∕2μ
Pk (δ + γ)
(35)
In addition, let
kmax :
ConsiderAlgorithm 1 with α ≤ L, k ≥ kmax and ∣∣λi+ι — λi∣ ≤ e, where 0 ≤ E ≤ E.
Under Assumptions 3.4- 3.3 and 3.5- 3.6, ifφwi(λi) ≤ r, then φwi+1 (λi+1) ≤ r.
(36)
18
Under review as a conference paper at ICLR 2021
Proof. We consider φwi(λi) ≤ r. If E ≤ P+γ)(B -r With r ≤ B, then φwi(λi) ≤ B - (δ + Y)E
and, as shown in Lemma 3.9, φwi(λi+1) ≤ B. This allows us to use the results of Theorem 3.8.
We now derive an upper bound on Eξ[i+i] [f (wi+ι, λi+ι)] - f *(λi+ι) by considering the results
of Theorem 3.8 together With the regularity Assumptions 3.2 and 3.3, and the triangle and Jensen
inequalities as follows
2
Eξ[i+1] [f(wi+1, λi+1)] - f*(λi+1) ≤ ρk [Eξ[i] [f (Wi, λi+1)] - f*(λi+1)] + 即
=	ρklEξ[i] [f (wi, λi+1) + f (Wi, λi) - f (Wi, λi)] - f*(λi+1)
2
+f *(%)- f *(%)∣ + 2μ
Triangle and Jensen Ineq.
≤	P [Eξ[i] [f (Wi, λi)] - f * (λi)] + P Eξ[i] [|f (Wi, λi+1) - f (Wi, λi)U
(37)
Assumptions 3.2 and 3.3
≤
2
+ Pk |f *(λi ) - f *(λi+1 )| + 2μ
ρk [Eξ[i] [f (Wi, λi)] - f*(λi)] + ρ"δ + γ)kλi+1 - λik + 2μ .
Using the fact that φwi(λi) ≤ r and that kλi+1 -λik ≤ E, we now solve the following inequality for E
in order to find an upper bound on the variation of the homotopy parameter such that φwi+1 (λi+1) ≤ r
E≤
(1 — ρk) r — σ2∕2μ
ρk (δ + γ)
with r > 2σ2.
(38)
(39)
(40)
□
H Proof of Theorem 3.11
Theorem H.1. Let P ∈(1 — σμ -B, 1)and consider Algorithm 1 with α ≤
φw0 (λ0) ≤ r with
σ2	1
2μ (1-ρ)
≤ r ≤ B and k ≥ logρ(p) .In addition, let E1 :=熊| Y) (B — r) and
1
p-pk εo
Pk	(δ+γ)
if k ≥ IogP(P) - logρ (1 + δε+γ
otherwise,
(41)
1
L,
with ε0 := Eξ[0] [f(W0, λ0)] - f*(λ0).
Under Assumptions 3.4- 3.3 and 3.5- 3.6, if ∣∣λi+1 — λik ≤ min {e-ηi, €1} with η ≥ ln (CP P), then
Eξ[i+i] [f(Wi+1, λi+1)] - f*(λi+1) ≤ Pi+1
σ2	i
[Eξ[0] [f(W0, λo)] - f*(λo)] + 27 XP
μ j=o
Proof. We start assuming that φwi (λi) ≤ r, with 0 ≤ r ≤ B and ∣λi+1 - λi ∣ ≤ E1 with E1 :
(δ++γ) (B — r) such that φwi(λi+1) ≤ B.
(42)
19
Under review as a conference paper at ICLR 2021
In particular, We consider the following upper bound on Eξ[i+i] [f (wi+ι, λi+ι)] - f *(λi+ι),
Eξ[i+1] [f (Wi+1, λi+1)] - f (λi+1) ≤ ρk [Eξ[i] [f (Wi, λi)] - f *(λi)]
2
+ ρk(δ + Y )∆λi+ι + —.
2μ
See the the proof of Theorem 3.10 for a derivation.
We then proceed by induction. Therefore, we assume
2 i-1
Eξ[i] [f (Wi, λi)] - f *(λi) ≤ pi [Eξ[0] [f (W0, λO)] - f *(λ0)] + 2 X Pj ,
aj=0
and derive the conditions on ∆λi+1 such that
σ2 i
Eξ[i+1] [f (Wi+1, λi+1)] - f *(λi+1) ≤ pi+1 [Eξ[0] [f (W0, λO)] - f *(λ0)] + 2~ X pj .
μ j=0
(43)
(44)
(45)
In order to achieve that, we consider the upper bound on Eg0]] [f (Wi+ι, λi+ι)] - f *(λi+ι) given
by Inequality equation 43 and solve the following inequality for ∆λi+ι
2
Pk [Eξ[i] [f (Wi, %)] - f*(λi)] + Pk (δ + Y) δ%+1 + 2μ
σ2	i
≤ Pi+1 [Eξ[0] [f (WO, λo)] - f *(λo)] +-X Pj
μ j=O
(46)
We obtain that Inequality equation 46 is satisfied whenever
ρi+1 [Eξ[0] [f (wo, λo)] - f *(λ°)] - ρk [Eξ[i] [f (Wi, λi)] - f *(λi)] + σ Pii=I Pj
△" ≤	WY
、	、/	，
(47)
✓
{z^^
RHS
We derive a lower bound on the right-had side of equation 47 by considering the induction assumption,
i.e. Inequality equation 44, and we obtain
RHS ≥
Pi (P - ρk) [Eξ[0] [f (wo, λo)] - f *(λo)] - σ2Pk Pj=O Pj + σ2 Pj=I Pj
ρk (δ + γ )
Considering that k ≥ logρ(PP) then
2	i-1	2 i
-2μPk X Pj + 2μ XPj
2	k i-1
彳(1 - P)X P ≥ 0.
2μ ∖	P) M
(49)
Consequently, Inequality equation 47 is satisfied whenever
ʌʌ <(P - Pk)	εO	方i
i+1 ≤	DP,
(48)
(50)
where εo := Egm [f (wo, λo)] - f*(λ°).
To conclude this first part of derivations, we obtain that, whenever Inequality equation 50 is satisfied,
then
σ2 i
Eξ[i+i] [f(Wi+1, λi+l)] - f*(λi+l) ≤ Pi+1 (Eξ[0] [f (wO, λθ)] - f *(λθ)) + 2- X Pj ∙	(51)
μ j=O
In order to ensure that φwi+1 (λi+1) ≤ r we consider Inequality equation 51 and use the fact
that φwo (λo) ≤ r, i.e. Eg^ [f (wo, λo)] - f *(λ0) ≤ r, to upper bound the right-hand side of
Inequality equation 51. We then solve the resulting inequality for r
σ2 i
Pi+1r +声 X Pj ≤ r.	(52)
μ j=O
20
Under review as a conference paper at ICLR 2021
八一 .	， Li 〜，	(l-ρi+1)	, . ,	▼	.一. .CF,
Considering that Ej=O Pj =1(―),We obtain that Inequality equation 52 is satisfied whenever
σ2	1
r ≥ --------
—2μ (1-P)
(53)
By combining Inequality equation 53 with the fact that r ≤ B, we obtain the following upper bound
on ρ
~,1	σ2 1
P ≤1 - 2μB
(54)
To further simplify the bound in equation 50, we define the following constant
otherwise,
if k ≥ logρ(P) - logρ
(55)
and obtain that Inequality equation 50 holds whenever ∆λi+ι ≤ e-η i with η ≥ - ln (CP p).	口
21