Under review as a conference paper at ICLR 2021
Learning Two-Time-Scale Representations For
Large Scale Recommendations
Anonymous authors
Paper under double-blind review
Abstract
We propose a surprisingly simple but effective two-time-scale (2TS) model for
learning user representations for recommendation. In our approach, we will parti-
tion users into two sets, active users with many observed interactions and inactive
or new users with few observed interactions, and we will use two RNNs to model
them separately. Furthermore, we design a two-stage training method for our model,
where, in the first stage, we learn transductive embeddings for users and items,
and then, in the second stage, we learn the two RNNs leveraging the transductive
embeddings trained in the first stage. Through the lens of online learning and
stochastic optimization, we provide theoretical analysis that motivates the design
of our 2TS model. The 2TS model achieves a nice bias-variance trade-off while
being computationally efficient. In large scale datasets, our 2TS model is able to
achieve significantly better recommendations than previous state-of-the-art, yet
being much more computationally efficient.
1 Introduction
A hypothetical user’s interaction with recommenda-
tion systems gives us diminishing returns in terms
of its information value in understanding the user.
For an active user who has lots of historical in-
teractions, she is typically well understood by the
recommender, and each new interaction gives rel-
atively little new information. In contrast, for an in-
active or new user, every additional interaction will
provide interesting information for understanding
this user. Therefore, the representations for active
and inactive users should be updated differently
when a new interaction occurs. Figure 1 illustrates
such information diminishing phenomenon, where
the amount of change in user embedding from φt
to φt+1 due to an additional interaction is decay-
ing. One Can select a particular threshold t* for
the number of interactions, above which the users
can be categorized to active users, and below which
inactive users. Roughly active users’ embeddings
evolve slowly as a function of the number of inter-
actions, while inactive users’ embeddings evolve
fast. Hence a two-time-scale embedding evolution.
Apart from the time-scale difference in temporal
dynamics, the simultaneous presence of active and
inactive users also presents other modeling and
computational challenges. On the one hand, active
Figure 1: Two-time-scale convergence of user em-
beddings motivates us to design a two-stage method,
where the first stage estimates transductive embed-
ding and the second stage learns two different RNNs
for active and inactive users respectively.
7τττ
i F∙÷; H

users lead to long sequences of interactions and high degree nodes in the user-item interaction graph.
Existing sequence models, such as RNN models, have some limitations when dealing with long-range
sequences, due to the difficulty in gradient propagation. Moreover, graph neural network-based
models become computationally inefficient due to the intensive message passing operations through
high-degree nodes introduced by active users. On the other hand, predicting preferences of inactive or




1
Under review as a conference paper at ICLR 2021
new users (also known as the cold-start problem) is a challenging few-shot learning problem, where
a decision needs to be made given only a few number of observations. To address various challenges
imposed by the presence of two types of users, we leverage the different dynamics of these users and
propose (i) a two-time-scale (2TS) model and (ii) a two-stage training algorithm.
2TS model. Based on the number of observed interactions, we partition the users into two sets: active
and inactive users. Our 2TS model (Fig. 1) update the embeddings of active users and inactive users
by two RNNs with independent parameters, in order to respect the two-time-scale nature. Moreover,
the initial embeddings of inactive users are represented by a common embedding ψ, which is shared
across all inactive users. Therefore, the overall model for inactive users is inductive, in the sense that
the learned model can be applied to unseen users. In contrast, the initial embedding of each active
user is a user-specific embedding φu, which is also called transductive embedding. Such embeddings
are very expressive, which can better express users with a long history.
Two-stage training. In stage 1, we first learn transductive user embeddings φu and transductive item
embeddings xi using a classical collaborative filtering method. Then we fix these embeddings, and
in stage 2, we will learn the parameters of the two RNNs and a common initialization ψ for inactive
users. It is notable that the transductive embeddings for inactive users are abandoned in stage 2. Only
those for active users are finally used in the 2TS model. Besides, for active users, we do not use
all interaction data to learn the RNN since their transductive embeddings have already encoded the
information of their history. We only use a small number of last clicked items to learn the adaptation
for active users, which improves the efficiency of the training process.
The proposed 2TS model and the two-stage training algorithm lead to a few advantages:
•	Bias-variance trade-off. The differential use of transductive and inductive embeddings for the
two RNN models allows 2TS to achieve a good overall bias-variance trade-off. We theoretically
analyze such trade-off in Section 2 through the lens of learning-to-learn paradigm for designing
online learning (or adaptation) algorithms. Our theory shows that there exists an optimal threshold
to split users to achieve the best overall excessive risk.
•	Encode long-range sequence. The transductive embeddings φu for active users are user-specific
vectors, so they can memorize the user’s long-range history during the training, without suffering
from the difficulty of gradient propagation. The RNN on top of these transductive embeddings is
only used for adaptation to recently engaged new items.
•	Computational efficiency. The efficiency of our method on large-scale problems mainly comes
from two designs in the algorithm. First, stage 1 learns the transductive embeddings of active
users and items, which contain a large number of parameters. However, it is fast since it does not
involve any deep neural components and the loss is simply a convex function. Second, stage 2 only
learns the RNNs which contain a small number of parameters, and the RNN for active users is
only trained on a few last engaged items, which cuts off the long sequences. Experimentally, our
method reveals to be much more efficient than the baselines on large-scale datasets.
We summarize the contributions of this paper as follows:
•	To explain the intuition and motivation of the 2TS model, we provide theoretical analysis on
a simplified setting, which rigorously argues the need for differential use of transductive and
inductive embeddings for active and inactive users (Section 2).
•	Motivated by the analysis, we design the 2TS model and a two-stage training method, for practical
use (Section 3). The proposed method is applied to two large-scale benchmark datasets and
compared comprehensively to various baseline models, spanning a diverse set of categories, which
shows that our method is advantageous in terms of both accuracy and efficiency (Section 5).
2	Theoretical Motivation: Why Two-time-scale Models ?
We will first present the motivation for designing the 2TS model, through the lens of online learning
and stochastic optimization. Our analysis quantitatively reveals that (i) the embeddings for active
and inactive users evolve in different time scales, and (ii) two different online learning algorithms for
active and inactive users respectively can lead to a better overall estimation of user embeddings.
Our analysis will be carried out in a learning-to-learn setting, where online learning algorithms need
to be designed to tackle a family of tasks for estimating the embedding vector of a user. Though
this idealized setting can not cover all aspects of real-world recommendation problems, it leads to
2
Under review as a conference paper at ICLR 2021
clear insights on the 2TS behavior of active and inactive users and the benefits of using two different
online algorithms for these two respective user groups. These insights also motivate our practical
implementation of the 2TS model using deep learning in Section 3.
2.1	Setting: Learning-to-learn
Our setting consists of three components: the estimation task for an individual user, the distribution
of tasks for a family of users, and the online algorithms which we want to design.
Individual Task. We associate each user U with a ground truth embedding φμ ∈ Rd, which can
be thought of as a vector representation of a user’s preference. This embedding is defined by a
distribution μ over user's clicks over items (x,y), where X ∈ X represents the item embedding
which we assume is bounded, i.e., kxk2 ≤ Bx, and y ∈ {0, 1} indicates whether the item is clicked.
They follow the user-specific distribution (x, y)〜μ. More specifically, the ground truth user
embedding is defined as the minimizer of the expected risk according to a regularized logistic loss
φμ ：= argminφ∈φRμ(Φ), where R"(φ) := E(x,y)〜“'(φ, x,y),
and'(Φ, x,y) ：= -yχ>φ + log(1 + eχp(χ>φ)) + C∣∣φk∣, (1)
where c > 0 is some regularization constant and Φ := {φ ∈ Rd : kφk2 ≤ Bφ}.
Typically, We do not have access to the distribution μ, but a sampled set of T observations z[τ]:=
{(x1,y1), ∙∙∙ , (xτ, yτ)}〜μτ, which can be used as training samples to obtain an initial estimate
φ(z[T] ) of the user embedding. We assume φ(z[T] ) is obtained by applying stochastic gradient
descent (SGD) to the loss ` over z[T] in this section, but this training stage is not limited just to the
SGD algorithm. It is expected that with more samples the estimation will be closer to the ground
truth φμ. The estimation φ(z[τ]) is modeling the offline training stage of a recommendation system.
A Distribution of Tasks. We consider a distribution of users, by assuming that the user-specific
distribution μ is sampled from a meta-distribution μ 〜pu. Furthermore, the number of observed
interactions for a user, denoted by T 〜PT, is a random variable that follows a power law distribution
with density P(T) H (T + 1)-α. The power law distribution models the fact that there will be lots of
users with very few interactions, and very few users with many interactions.1 A key assumption of
our model is that the variance of the ground truth user embeddings is small. That is
Varm = E“〜pu ∣φμ 一 m∣2 ≤ r, where m
=Eμ~PuΦμ.
(2)
The assumption of small variance is critical in the sense that it will allow us to aggregate information
from inactive users to obtain better estimations of their user embeddings.
Online Algorithm Design Problem. Our goal is to design online adaptation algorithms for this
distribution of users, such that the overall excessive risk of the online adaptation across all users
is small. This is to model the online sequential recommendation stage where new user-item click
information is incorporated after system deployment. Note that we are not restricted to design a single
online learning algorithm for all users. In fact, we will show later that 2 online learning algorithms
can actually lead to better risk bound than 1 online learning algorithm.
In this algorithm design problem, each user μ corresponds to an online learning task, where items
arrive sequentially. Starting from an initial embedding φ∖, an online algorithm updates the user
embedding whenever it observes a new user-item interaction (xt, yt)〜μ:
φμ+1 - UPdate(φμ, χt,yt),
(3)
and then it applies φtμ+1 to the next item xt+ι, and suffers the loss '(φμ+1, xt+ι,yt+ι) in Eq. 1. The
excessive risk of this online algorithm after encountering N sequential samples is
N PiN=I '(φμ, xt, yt) - '(φμ, xt, yt).
The design problem involves (1) the initialization for the algorithm and (2) the update step of the
algorithm. One obvious choice is to use the user-specific embedding φ(z[T] ) estimated in the offline
training phase as the initial embedding φ' for the online algorithm. However, is this estimation
the best choice for the initial embedding φ} = φ(z[τ])? Is there a better choice? How should the
initialization depend on T? We answer these questions in the next subsection.
1This assumption is sensible since Fig. 5 shows that T approximately follow a power law in real-world
datasets.
3
Under review as a conference paper at ICLR 2021
2.2	Analysis And Its Implications
We focus on designing online gradient descent (OGD) algorithms for Eq. 3, where the update step
is φμ+1 J Projφ [φμ - Y ∙ ∂'(φμ, xt, yt)]. In our theoretical analysis, We will focus on designing
only the initial embedding for the algorithms and the learning rate γ to gain insights, and deferred the
design of more components of the algorithms to later practical neural implementation.
A key aspect of our analysis is that We will consider a threshold t* ∈ N that divides the users into
two groups: active users with offline training sample T ≥ t* before online adaptation and inactive
users with T < t*. Under such split of the users into two groups, we will design 2 online learning
algorithms, one for the active user group and the other for the inactive user group. Furthermore
•	Different initialization. The initial embeddings φ( for the two online learning algorithms are
designed as: for active users, their learned embeddings φ(z[T])are used as the initialization, and
for inactive users, they use a common embedding as the initialization. Mathematically,
Active User: φ[ = φ(z[τ]) if T ≥ t* and Inactive User: φ1 = m if T < t*,	(4)
where m represents the common initialization. We will later show in Theorem 2.1 that the optimal
choice of the common initialization is the mean m = Eμφμ, and we can also analytically find the
optimal threshold t*. The intuition is that, for inactive user, if we start from the estimation φ(z[τ] )
and make online adaptation for the new items, it will not perform well due the large discrepancy
between φ(z[τ]) and the ground truth φμ. However, since φμ is distributed around the mean m,
we can aggregate the offline observations from inactive users to estimate m. Then it may be better
off to use the estimated m as the shared initial user embedding for the inactive users. In contrast,
for active users, φ(z[τ]) can be a better initialization than m since it is already very close to φμ.
•	Different learning rate. The learning rates γ are designed to be different for the two online
learning algorithms. As later showed in Theorem 2.1, the optimal learning rate for active users is
smaller than that for inactive users, and leads to a better overall excessive risk. The intuition is that,
for an inactive user, each additional observation provides us a lot of information about this user,
and we want to employ a larger learning rate to fully ingest such information. In contrast, for an
active user, the additional observation provide diminishing amount of information about this user,
and we can use a smaller learning rate.
In the theorem below, we show precisely that different initialization and learning rate for the active
and inactive users can lead to a smaller expected excessive risk. Furthermore, there is a theoretical
optimal choice of the threshold t* to achieve this. A more detailed statement of the theorem and the
proof of the theorem are provided in Appendix A.
Theorem 2.1. Assume the setting and notation in Section 2.1. Assume φtu is updated by OGD with
step size γ. With the initialization strategy in Eq. 4, the expected excessive risk is bounded by
EN Pt=ι'(φμ, xt, yt) - '(φμ, xt, yt) ≤ (2γN)-1 (β(t*) + Y2B2N),	(5)
where β(t*)= 式a+：；+1) Q+(1 —式然 +1) )Varm, B = Bx + cBφ, Z is the Hurwitz zeta function,
ζ (α,1)	ζ (α,1)
and Q is a constant larger than Varm. Optimizing the step size Y = B-I ,β(t*) / N, we have
EN Pt=1'(φμ,xt,yt) - '(φ*μ,xt,yt) ≤ Bpβ(t*) /N.	(6)
Moreover, the optimal choice of the threshold t* is
t** = argmint*≥oβ(t*) = [Q / VarmJ - 1.	(7)
Implication 1: A differential design for the initial embeddings is better. If all users are initialized
by their user-specific embeddings φ(z[τ] ), then it corresponds to the case when t* = 0. However, it
is a less optimal choice than t* = t**, and the gap between them can be quantitatively characterized
by β(0) - β(t**) = Pt=1(QQ - Varm)t-α. Since the excessive risk is proportional to pβ(t*),
the gap shows the advantage of the differential initialization for active and inactive users in Eq. 4.
Furthermore, if Varm is smaller, the gap and the advantage of differential initialization are larger.
Implication 2: The optimal step sizes for active and inactive users are different. In the above
theorem, the optimal step size Y for the online algorithm is optimized over all users. However, if we
4
Under review as a conference paper at ICLR 2021
consider using two different step sizes for the group of active users and the group of inactive users,
the optimal step sizes for these two groups will be (see derivations in Appendix A.2)
Yact = ∖ ζZα+∖t++)NQ for active users，and Yin = -1∖Vam for inactive users.
ζ (α,t +)
It is easy to verify that Yact < Yin , which suggests that embeddings of active users should evolve
with a smaller step size than inactive users. This is consistent with our intuition for the 2TS model.
3	Methodology For Practice
In this section, we will describe our concrete methodology, including (i) the parameterization of the
two-time-scale (2TS) model and (ii) a two-stage training algorithm for learning this model efficiently
and effectively. The design is motivated and guided by the analysis in Section 2, though more
advanced techniques are incorporated in order to achieve the state-of-the-art performance.
Instead of following the OGD algorithm in Section 2, we will use more flexible models for the online
update dynamics of user embeddings, as in the meta-learning framework (Ravi & Larochelle, 2016).
In particular, we will use two different RNNs, which we call two-time-scale (2TS) model, to model
the dynamics of active and inactive users respectively, which correspond to the update steps in Eq. 3.
To learn the 2TS model, we first perform a generalized collaborative filtering method to obtain
transductive user and item embeddings, which is similar to the training phase in Section 2, except
that item embeddings also need to be learned from data and a more advanced loss function is adopted.
The learned transductive embeddings will be fixed in stage 2. Transductive embeddings for active
users will be used as the initial embeddings during the training of RNNs, while for inactive users a
common initialization will be learned together with the RNNs.
3.1	Neural Two-time-scale (2TS) Model
Consider a set of users U ∈ U := {1,2,…，U} and items i ∈ I := {1,…，I} in a system. In our
model, users are divided into two sets by the number of observed interactions T in the training set.
With a threshold t* ∈ N, we consider users with more than t* observed interactions as active users
Uact, and those with less than t* interactions as inactive users Uin, which include new users.
Our two-time-scale model consists of two RNNs, one for active users and the other for inactive
users. They can be thought of as two different online adaptation operators for active and inactive
users respectively. Furthermore, these two RNNs will also differ in the way in whether they have
user-specific parameters and how they are initialized. Assume for the moment, we have already
learned an initial set of transductive user embeddings φu ∈ Rd : u ∈ U and item embeddings
xi ∈ Rd : i ∈ I , which will be explained in later two-stage training methods.
The RNN for inactive or new users is purely inductive in the sense that it does not contain user-specific
parameters. We will not use user-specific transductive embeddings φu in this RNN. Instead, the
RNN will start with a learned common initialization ψ ∈ Rd, and then be updated by RNN-based
adaptation operators every time a new interaction is observed. More precisely, for an inactive user
who clicks on items (i1,…，iτ) sequentially, the embedding will be updated sequentially as:
inactive users u ∈ Uin: φ0u = ψ;	φtu+1 = RNNcellΘ1 (φtu, xit).	(8)
For active users, we will view their transductive embeddings as the memories for their long-term
history. Different from inactive users, the initial embeddings in the RNN for active users will be set
to be the user-specific transductive embeddings φu . Then the RNN-based adaptation operator will
update this initial transductive user embeddings if there are more interacted items:
active users u ∈ Uact: φ0u = φu; φtu+1 = RNNcellΘ2 (φtu, xit).	(9)
The inactive and active user models in Eq. 8 and Eq. 9 constitute our two-time-scale model (2TS
model). The model consists of several sets of parameters, including the transductive embeddings for
active users φu ∈ Rd : u ∈ Uact and items xi ∈ Rd : i ∈ I , the two sets of RNN parameters
Θ1 and Θ2 respectively, and the additional common initialization ψ for the inactive user RNN. These
parameters will be learned differently using a two-stage method described in the next section.
5
Under review as a conference paper at ICLR 2021
3.2	Two-stage Training
Let D be the training dataset that records the interactions between users and items as an ordered
sequence where the t-th interaction between some user u and item i is denoted as et := (ut, it). Our
training method for the two-time-scale model consists of two stages. In stage 1, we will learn the
transductive embeddings for all users and items from observed interactions D. In stage 2, we will
learn the parameters of the two RNNs. In summary, the blue-colored transductive embeddings for
active users {φu : u ∈ Uact} and items {xi : i ∈ I} are learned in stage 1 and then fixed in stage 2.
The red-colored components that include the common initialization ψ for inactive users and the RNN
parameters Θ1 and Θ2 are learned in stage 2. More specifically,
Stage 1. We will first ignore the sequential ordering of the interactions, and learn the transductive
user and item embeddings by optimizing the cross-entropy loss (or called softmax loss):
L({φu}, {xi}) = |DD, P(u,i)∈D log [Pj∈O(u,i) eχp(φ>xjU- φ>xi,	(IO)
where the set O(u, i) is the set of items displayed to user u when he/she clicks on item i. Typically
this information is not given, so we could randomly sample p items as the non-clicked items, and use
these p items together with the clicked item i to constitute the offer set O(u, i).
Stage 1 training is similar to collaborative competitive filtering (CCF) in Yang et al. (2011). It is
efficient since the objective is a simple function. Besides, since the objective is convex in either φu
or xi , it is easier to obtain the global optimum and the results are affected less by the initialization.
Since active users have many interactions, their transductive embeddings are learned well and lead
to good predictions in held-out data. However, for inactive users with fewer observed interactions,
the learned transductive embeddings could be overfitted. Thus, in the next stage of training RNNs,
we will re-use the transductive embeddings for active users but will discard those for inactive users.
Furthermore, the learned item embeddings {xi} will also be used in the next stage of training.
Stage 2. We can divide the training set according to active and inactive user as Dact = {(ut, it) :
(ut,it) ∈ D ∧ ut ∈ Uact} and Din = {(ut, it) : (ut, it) ∈ D ∧ ut ∈ Uin}. Then we will train the
parameters of the two RNNs using the following loss functions:
Lin(θ1, ψ) := |Dn P(ut,it)∈Din log [Pj∈O(ut,it) exP(φ>t xj )] - φ>txit,	(II)
Lag) := ∣⅛ P(ut,it)∈砥，log [Pj∈O(ut,it) exP(φ>t xj )] - φ>txit,	(12)
where user embeddings {φut } in Eq. 11 and Eq. 12 are updated sequentially using the corresponding
RNNs in Eq. 8 and Eq. 9, respectively. Furthermore, DaKct ⊂ Dact ⊂ D contains the last K interac-
tions for each active user observed in the training set. That is, for an active user Who has more than t*
interactions, we will only use the last K (K ≤ t*) interaction of the user to train the RNN. First, this
alloWs us to avoid the direct use of RNN for long sequence modeling, Which is inefficient. Second,
the transductive embeddings for active users have already encoded most information from these users
and only a small online adaption is needed to boost the performance further.
Overall, we find that by encoding the history into the transductive embedding and only learn the K
step adaptation can largely reduce the computational cost. This reflects another benefit of treating
active and inactive using the threshold t* .
3.3	Implementation Details
We present two implementation details that are essential for the performance of our model in the
experiments but less relevant to the main ideas of this paper.
2TS-Plus. We propose a variant of our 2TS model called 2TS-Plus. The only difference from 2TS is
that, we replace the user embeddings φut in 2TS by φbut 一 W >[φ>t, xi> ]> where xit-1 is the
item embedding of the user ut’s most recently clicked item it-1 , and W is a learnable weight matrix.
In summary, 2TS-Plus explicitly incorporates the information of the ‘last clicked item’ to compute
the user embeddings. Our experiments show that this can consistently improve performance. In fact,
we incorporate the ‘last clicked item’ because we found that the baseline JODIE (Kumar et al., 2019)
6
Under review as a conference paper at ICLR 2021
performs particularly well on lastfm-1K. Thus we looked into their implementation and found that it
is the ‘last clicked item’ that helps with the performance.
Features. In many datasets, item features {fi}i∈I are provided. In the experiments, we concatenate
the transdUctive item embeddings Xi with the feature-based embedding, gjf), where gφ a simple
network with parameter 夕.The concatenation [x>,gφ(fi)τ]τ will be used as the item embeddings,
and both transductive embeddings Xi and the parameters 夕 are learned in stage 1.
4	Related Work
Our stage 1 training is most related to methods based on matrix factorization (MF) (Koren et al.,
2009). Many methods for MC are collaborative filtering (CF)-based (Koren et al., 2009). Neural
models for recommendation broadly fall into the following three categories.
Graph-based models. Monti et al. (2017) first studied recommender systems with graph neural
network (GNN). Berg et al. (2017) proposed graph convolutional matrix completion (GCMC) which
applies a GNN on the user-item bipartite graph to learn user and item embeddings. It is a transductive
model, as it learns user/item-specific embeddings. PinSage (Ying et al., 2018) and IGMC (Zhang &
Chen, 2020) are recently proposed inductive GNN-based recommender systems. Despite showing
promising results, existing GNN-based recommender systems usually have poor scalability, due to
their expensive neighborhood sampling procedure for performing graph convolution.
Dynamic graph models. Based on the idea that both users and items will evolve over time via
temporal interactions, Dai et al. (2016); Farajtabar et al. (2017); Kumar et al. (2019); Goyal et al.
(2020); Sankar et al. (2020) take the graph temporal evolution into modeling design. However, most
existing dynamic graph approaches cannot scale to large interaction graphs. JODIE (Kumar et al.,
2019) proposed a batching method to make the training process more efficient.
Deep Sequence models. RNN (Hidasi et al., 2015; Jannach & Ludewig, 2017) and LSTM (Devooght
& Bersini, 2017; Chen et al., 2019) based deep models are widely used in sequential recommendation.
Other methods based on attention models (Zhou et al., 2018; 2019) have also been explored. However,
these models still have difficulties in leveraging the information contained in states located far into the
past due to gradient propagation issues (Pascanu et al., 2013). Several recent advances are proposed
to deal with long-range sequences (Tang et al., 2019; Pi et al., 2019)
Cold-start problem. Traditional approaches to address cold-start problems include content filtering,
hybrid models, etc. Vartak et al. (2017) proposed a meta-learning perspective for the item cold-start
problem, where recommending new items to users is formulated as learning a learning algorithm. Lee
et al. (2019) proposed to use meta-learning to estimate new user’s preferences with a few consumed
items. Bose et al. (2019) proposed Meta-Graph to perform few-shot link prediction across multiple
graphs. Wu et al. (2020) proposed to compute embeddings for new users using the embeddings
of active users, via an attention-based model. They share the same idea of splitting the users into
two sets, but their main target is the matrix factorization problem and neither consider the temporal
evolution nor the two-time-scale difference.
5	Experiments
To evaluate the performance of the 2TS model, we conduct
experiments on three public datasets, two of which are the
largest recommendation benchmark datasets that are closer
to industrial scenario, making our results more solid and
Table 2: Dataset Statistics
Dataset	#users	#items	#interactions
lastfm-1K	1,000	1,000	1,293,103
ML-25M	162,538	59,048	24,999,849
Taobao	987,975	4,111,798	96,678,667
convincing. By comparing to a diverse set of state-of-the-art (SOTA) methods, we demonstrate the
scalability and accuracy of our 2TS model. See detailed configurations in Appendix C.
Dataset. We consider 3 public datasets: Taobao dataset (Pi et al., 2019), MovieLens-25M (ML-25M)
(Harper & Konstan, 2015), and lastfm-1K (Celma, 2010). Dataset statistics are summarized in Table 2.
Taobao and ML-25M are large-scale. Especially, Taobao contains about 100 million interactions. The
results on these two datasets are much more convincing. All datasets provide user-item interaction
data where for each interaction, user ID, item ID, timestamp, and item feature are given. For ML-25M,
we ignore the ratings and simply use it as interaction data. For each dataset, we sort the interactions
by timestamp, and use the first 70% as the training set, the following 10% as the validation set, and
the last 20% as the test set. All reported results are estimated on the test set.
7
Under review as a conference paper at ICLR 2021
Table 1: Overall performance for interaction prediction. The best method in each column is colored
blue and second best is colored light blue . The * symbol indicates thatthe original implementations
released by the authors or the proposed methods are not scalable and will not be able to run on the
indicated datasets without our modifications to their methods. The ] symbol indicates that the released
implementations have been modified by us to better adapt to the dataset and evaluation metric, so
that their results in this table should be expected to be better than the original version. The concrete
modifications we made are described in Appendix B.
	method	Taobao		ML-25M		lastfm-1K	
		MRR	Rec@10	MRR	Rec@10	MRR	Rec@10
CF-based	CCF]	0.402	0.621	0.193	0.392	0.051	0.116
GNN-based models	GCMC]	0.303	0.542	0.171	0.378	0.081	0.129
	GCMC-SAGE] GCMC-GAT]	0.149 0.230	0.366 0.494	0.109 0.185	0.264 0.404	0.168 0.064	0.208 0.118
	SumPooling	0.415	0.664	0.302	0.591	0.071	0.180
	GRU4REC]	0.546	0.777	0.364	0.657	0.152	0.344
Deep sequence	DIEN]	0.605	0.834	0.356	0.638	0.100	0.213
models	MIMN]	0.607	0.828	0.363	0.653	0.115	0.261
	SASRec]	0.488	0.702	0.360	0.653	0.147	0.323
	Bert4Rec	0.280*	0.480*	0.397	0.652	0.216	0.369
Dynamic graph	dynAERNN	-oom-	-oom-	0.249	0.509	0.021	0.038
models	JODIE]	0.454*	0.680*	0.354*	0.634*	0.176	0.325
Our method	2TS	0.669	0.844	0.404	0.693	0.151	0.332
	2TS-Plus	0.680	0.844	0.409	0.691	0.203	0.369
Improvement to	2TS	10.2%	1.2%	1.8%	5.5%^^	-	-
best baseline	2TS-Plus	12.0%	1.2%	3.0%	5.2%	-	-
Baselines. We compare 2TS with 12 models spanning
4 categories: (i) CF-based methods: Collaborative
competitive filtering (CCF) (Yang et al., 2011) is an ad-
vanced CF model which takes into account the context
of user’s choice. It is a simple yet very effective method.
(ii) GNN-based models: GCMC (Berg et al., 2017)
is a SOTA graph-based architecture for recommenda-
tion. GCMC-SAGE is its stochastic variant which is
more efficient. GCMC-GAT is another variant based
on graph attention networks (VeIiCkOViC et al., 2017).
(iii) Deep sequence models: SumPooling is a simple
yet effective baseline that is widely used in industry.
GRU4REC (Hidasi et al., 2015) is a representative of
RNN-based models. DIEN (Zhou et al., 2019) is an
advanced attention-based sequence model. MIMN (Pi
et al., 2019) is a memory-enhanced RNN architecture
to better capture long sequential user behaviors. It is a
strong baseline and the authors come from the team that
publicized Taobao dataset. SASRec (Kang & McAuley,
2018) is a 2-layer transformer decoder-like model. Also
built on top of transformer architecture, Bert4Rec (Sun
0.7
0.6
ω 0.5
0 0.4
o 0.3
0.1
Time-to-MRR performance on Taobao
JODIE	---- 2TS-stage1
SumPooling	2TS-stage2
100
0
20
40
60
80
Training Time (hrs)
Time-to-MRR performance on ML-25M
---GRU4Rec
---MIMN
2TS-stage1
2TS-stage2
---GCMC
---GCMC-SAGE
——dynAERNN
---JODIE
一一 SumPooling
0	5	10	15	20	25	30	35	40
Training Time (hrs)
Figure 2: Test MRR versus training time. We
compare the convergence speeds of different
models in wall-clock time. Run-time on lastfm-
1K is not showed because it is small-scale.
et al., 2019) further introduced a BERT type Cloze task with bidirectional attention. (iv) Dynamic
graph models: JODIE (Kumar et al., 2019) and dynAERNN (Goyal et al., 2020) are two dynamic
temporal graph approaches which learn graph node dynamic embeddings via temporal modeling.
Evaluation Metric. We measure the performance of different models in terms of the mean reciprocal
rank (MRR), the average of the reciprocal rank, and recall@10, which is the fraction of interactions
in which the ground truth item is ranked in the top 10. For both metrics, higher values are better. For
every interaction, the ranking of ground truth item is calculated with respect to 500 items where the
other 499 negative items are randomly sampled from the set of all items.
8
Under review as a conference paper at ICLR 2021
Overall Performance (Table 1). We summarize the overall performance in terms of MRR and
Rec@10 in Table 1. On the large-scale datasets Taobao and ML-25M, our models 2TS and 2TS-Plus
have consistent and significant improvements compared to all baselines. In terms of MRR, an
improvement of 12.0% and 3.0% are achieved by 2TS-Plus, respectively. Note that the lastfm-1K
dataset is much smaller than Taobao and ML-25M. We run experiments on lastfm-1K just to show
that the models have very different behaviors on datasets of different scales. It is interesting to see that,
for example, GCMC-SAGE, which performs the worst on both two large-scale datasets, achieves the
3rd best performance on lastfm-1K. The real advantages of our models are for large-scale problems
where candidate items are sparse.
Scalability & Efficiency (Fig. 2). To com-
pare the training efficiency, we evaluate dif-
ferent models’ intermediate checkpoint per-
formances on the test set. Fig. 2 shows
that the performance of 2TS increases fast.
2TS’s first stage transductive training is effi-
cient and provides an effective initialization
for the second stage inductive training. Be-
sides, though not revealed from the figures,
the training time of 2TS for each epoch is
much smaller than the baselines.
ML-25M
0.45
0.40
承 0.35
2 0.30
0 0.25
W
弟 0.20
0 0.15
0.10
0.05
Taobao
#InteraCtionS in Training Set
#InteraCtiOnS in Training Set
Figure 3: Test MRR in different segments of users.
Performance for different users. Fig. 3
shows the MRR performances averaged over users with different numbers of observed interactions
(the more interactions the more active the user is). The dash line indicates the threshold that we use
to split users into inactive and active groups. 2TS and 2TS-Plus lead to consistent improvements over
the entire range of users. On ML-25M, the improvement on inactive users are more obvious, while
on Taobao the improvement on active ones is more obvious.
Two-time-scale behavior (Fig. 4). Recall 2TS
has two RNNs. One updates the embeddings
of inactive users and the other updates those of
active users. Given the learned RNNs, on the test
set, we compute the changes of user embeddings
in 2-norm as the user interacts with more items
(Fig. 4). The behavior aligns with our intuition.
Embeddings of inactive users are changed faster
and those of active users are more static.
s6u一ppφqujφ -Jφsn°φ6u∙,LP
s6u一ppφqujφ -Jφsn°φ6u∙,LP
Figure 4: Two-time-scale behavior of user embeddings.
Ablation Study (Table 3). To show the effectiveness
of the two-stage training, we compare 2TS to 2TS-
SingleStage, which has exactly the same architecture
as 2TS but all parameters are trained together in a sin-
gle stage. 2TS-SingleStage performs worse, which
means the transductive embeddings are learned very
well in Stage 1, benefit from the convexity and easi-
Table 3: Ablation study.
method	ML-25M MRR Rec @10	Taobao MRR Rec@10
2TS	0.404 0.693	0.669 0.894
2TS-SingleStage	0.373 0.636	0.420 0.533
2-GRU	0.354 0.643	0.597 0.791
ness of optimization. Besides, the training of 2TS-SingleStage is a lot slower. Furthermore, to show
the effectiveness of the 2TS model design, we compare it to 2-GRU, which applies two different
RNNs to inactive and active users, but there are no transductive user embeddings for active users.
6 Conclusion
We proposed to learn two-time-scale representation (2TS) for large recommendation systems to
explicitly address the learning efficiency for different user dynamics. We also proposed a two-stage
training schema, to leverage the transductive embedding as the inductive model initialization for
active users. We evaluated 2TS on large scale recommendation ranking tasks, and showed that 2TS is
able to outperform several class of state-of-the-art methods including deep sequence models, graph
neural network, and dynamic graphs. Our results show that, designing different representation to
capture diverse interaction dynamics are desirable. Future work includes separating and updating
user dynamics in an algorithmic way.
9
Under review as a conference paper at ICLR 2021
References
Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion.
arXiv preprint arXiv:1706.02263, 2017.
Avishek Joey Bose, Ankit Jain, Piero Molino, and William L Hamilton. Meta-graph: Few shot link
prediction via meta learning. arXiv preprint arXiv:1912.09867, 2019.
O. Celma. Music Recommendation and Discovery in the Long Tail. Springer, 2010.
Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative adversarial user
model for reinforcement learning based recommendation system. In International Conference on
Machine Learning ,pp.1052-1061. PMLR, 2019.
Hanjun Dai, Yichen Wang, Rakshit Trivedi, and Le Song. Deep coevolutionary network: Embedding
user and item features for recommendation. arXiv preprint arXiv:1609.03675, 2016.
Robin Devooght and Hugues Bersini. Long and short-term recommendations with recurrent neural
networks. In Proceedings of the 25th Conference on User Modeling, Adaptation and Personaliza-
tion, pp. 13-21, 2017.
Mehrdad Farajtabar, Yichen Wang, Manuel Gomez-Rodriguez, Shuang Li, Hongyuan Zha, and
Le Song. Coevolve: A joint point process model for information diffusion and network evolution.
The Journal of Machine Learning Research, 18(1):1305-1353, 2017.
Palash Goyal, Sujit Rokka Chhetri, and Arquimedes Canedo. dyngraph2vec: Capturing network
dynamics using dynamic graph representation learning. Knowledge-Based Systems, 187:104816,
2020.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis), 5(4):1-19, 2015.
Balgzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based
recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.
Dietmar Jannach and Malte Ludewig. When recurrent neural networks meet the neighborhood for
session-based recommendation. In Proceedings of the Eleventh ACM Conference on Recommender
Systems, pp. 306-310, 2017.
Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE
International Conference on Data Mining (ICDM), pp. 197-206. IEEE, 2018.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30-37, 2009.
Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal
interaction networks. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 1269-1278, 2019.
Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-learned user
preference estimator for cold-start recommendation. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 1073-1082, 2019.
Federico Monti, Michael Bronstein, and Xavier Bresson. Geometric matrix completion with recurrent
multi-graph neural networks. In Advances in Neural Information Processing Systems, pp. 3700-
3710, 2017.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-
1609, 2009.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310-1318, 2013.
10
Under review as a conference paper at ICLR 2021
Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. Practice on long sequential user
behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 2671-2679, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. Dysat: Deep neural rep-
resentation learning on dynamic graphs via self-attention networks. In Proceedings of the 13th
International Conference on Web Search and Data Mining, pp. 519-527, 2020.
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential
recommendation with bidirectional encoder representations from transformer. In Proceedings of the
28th ACM International Conference on Information and Knowledge Management, pp. 1441-1450,
2019.
Jiaxi Tang, Francois Belletti, Sagar Jain, Minmin Chen, Alex Beutel, Can Xu, and Ed H. Chi. Towards
neural mixture recommender for long range dependent user sequences. In The World Wide Web
Conference, pp. 1782-1793, 2019.
Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, and Hugo Larochelle.
A meta-learning perspective on cold-start recommendations for items. In Advances in neural
information processing systems, pp. 6904-6914, 2017.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and YoshUa
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Qitian WU, HengrUi Zhang, and HongyUan Zha. IndUctive relational matrix completion. arXiv
preprint arXiv:2007.04833, 2020.
ShUang-Hong Yang, Bo Long, Alexander J Smola, HongyUan Zha, and ZhaohUi Zheng. Collaborative
competitive filtering: learning recommender Using context of User choice. In Proceedings of the
34th international ACM SIGIR conference on Research and development in Information Retrieval,
pp. 295-304, 2011.
Rex Ying, RUining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and JUre Leskovec.
Graph convolUtional neUral networks for web-scale recommender systems. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
974-983. ACM, 2018.
MUhan Zhang and Yixin Chen. IndUctive matrix completion based on graph neUral networks. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=ByxxgCEYDS.
GUorUi ZhoU, Xiaoqiang ZhU, ChenrU Song, Ying Fan, Han ZhU, Xiao Ma, YanghUi Yan, JUnqi Jin,
Han Li, and KUn Gai. Deep interest network for click-throUgh rate prediction. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
1059-1068, 2018.
GUorUi ZhoU, Na MoU, Ying Fan, Qi Pi, Weijie Bian, Chang ZhoU, Xiaoqiang ZhU, and KUn Gai.
Deep interest evolUtion network for click-throUgh rate prediction. In Proceedings of the AAAI
conference on artificial intelligence, volUme 33, pp. 5941-5948, 2019.
11
Under review as a conference paper at ICLR 2021
A	Theoretical Motivation: Why Two-time-scale models ?
Here we will provide more details for Section 2. First, we summarize the setting in Section 2.1
formally and mathematically:
Setting (A).
•	Each item is represented by a vector x in a given bounded space X := {x ∈ Rd : kxk2 ≤ Bx }.
•	Each user is represented by a distribution μ over the space X×{0,1}. Each sample Z := (x,y)〜μ
from this distribution is an item x and a binary label y indicating whether this item is clicked by
the user. Further, We assume that a user μ is drawn from a meta-distribution μ 〜pu.
•	We parameterize the user μ through a logistic regression model so that a groundtruth user embed-
ding is defined as the minimizer to the expected risk:
Φ*μ ：= argminφ∈φRμ(Φ), where R"(φ) := E(x,y)〜“'(φ, x,y),
and'(φ, x,y) := -yx>φ + log(1 + exp(x>φ)) + C∣∣φk2,
where c > 0 is a regularization constant. Assume the parameter space is also bounded Φ := {φ ∈
Rd : ∣φ∣2 ≤ Bφ}.
•	Training samples. Each user is associated with T observed interactions, denoted by z[T] :=
{(xι, yι),…,(XT, yτ)}〜 μτ. Assume T ≥ 0 is a random variable that follows a power law
distribution with density P(T) 8 (T + 1)-α, and denote T 〜PT. T is independent of μ. Given
the observations Z[τ] for a user μ, an estimate of the user embedding φ(z[τ]) can be computed
using learning algorithms such as stochastic gradient descent (SGD). In this paper, we assume
φ(z[τ]) is estimated by SGD with initialization φo = 0 and step size θ = ɪ on the loss '.
•	Test scenario. Each user μ can be viewed as an online learning task, where items arrive sequentially.
Starting from an initial embedding φ∖, an online algorithm updates the user embedding whenever
it observes a new data point (xt, yt)〜μ:
Φμ+1 - UPdate(φμ, xt,yt),
then applies the most updated embedding to φtμ+1 the next item Xt+ι, and suffers from the loss
'(φμ+1, Xt+1,yt+1). The excessive risk of this online algorithm applying to N test samples is
N PN=I '(φμ, xt,yt) - '(φμ, xt,yt).
Before the proof of the main theorem, we first introduce the result of stochastic analysis by (Ne-
mirovski et al., 2009), which can be adapted to derive the following proposition that tells the error of
the estimation φ(z[τ]) from the training samples.
Proposition A.1. Assume Setting (A). Given the T observations Z[τ]〜μτ, ifwe estimate the user
embedding using projected stochastic gradient descent (SGD) with initialization 0, SteP Size C, and
operated on '2 -regularized logistic regression loss T PT=I -yiX> Φ + log(1+exp(x> φ)) + C ∣∣φ∣2,
then the expected squared error of the estimation φ(z[τ] ) can be bounded by
Ez[τJ〜μτkφ(z[τ]) 一 φμk2 ≤ T+1, where Q(μ) = maχ{(Bx +cBφ), kφμk2}.
Proof. Consider the loss '(φ, x, y) = -yx>φ + log(1 + exp(x>φ)) + C ∣∣φ∣2. It is easy to verify
that ` is c-strongly convex, and that the second moment of the gradient is bounded by
E∣∂φ'∣2 = E∣( Fa：、- y)x + cφ∣2 ≤ E(∣x∣2 + c∣φ∣2)2 ≤ (Bx + cBφ).
1 + exp(x φ)
Then applying the result by (Nemirovski et al., 2009) (more specifically, Equation (2.9) in the paper),
we can obtain the above error bound.	□
A.1 Proof of Theorem 2.1.
We first restate Theorem 2.1 as the following theorem, which provides more details. Then we present
the proof.
12
Under review as a conference paper at ICLR 2021
Theorem A.1 (Detailed version of Theorem 2.1). Assume Setting (A). Assume the online algorithm
for updating the embeddings φμ in Eq. 3 is online gradient descent (OGD) with step size γ, i.e.,
φμ+1 = ProjΦ [φμ—γd'(φμ, xt,yt). Let z[T] denote the observed training samples and z[N]
denote the sequence of test samples for the online learning task. With the initialization strategy in
Eq. 4, the expected excessive risk is bounded by
ι	It	、―	、
E(μ,T)MPu×PT )EZ[T]~μτ EZ[N]~μN N ∑'(φμ, Xt,yt) - '(φμ, χt,yt)
t=1
β(t*) + Y 2(Bχ + cBφ)2N
≤	,
2γN
where
8(一 ζ (α + 1,t* + I)O 1 Z S,t* + 1AVar
β (t )=	Z (α,1) Q + C - Z (α,1)心
Q = Eμ〜PuQ(μ) = Eμ〜Pu max{ (Bx ：产)2 , kΦμk2},
∞
and Z(a, b) =	(i + b)-a is the Hurwitz zeta function .
i=0
Choosing the optimal step size γ
1
Bx + cBφ
JeN), the upper bound is
E NX '(φμ, xt, yt)- '(φμ, xt, yt) ≤ Bχ⅛Bφ rβN)
Besides, the optimal choice ofthe threshold t* is
t** = argmin β(t*) =	Q — 1.
t*≥0	_ Varm _
(13)
(14)
(15)
Proof. Denote the N test samples by Z[N]〜μN. Following the analysis and results in (Nemirovski
et al., 2009), one can show that
Ez[N]〜μN kφμ+1 - φμk2 ≤Ez[N]〜μN k φ[ - Φ2 k 2
-2γEz[N]〜μN [(φμ - φμ )>∂'(φtμ, ")] + YIBx + cBφ F
The above inequality corresponds to equation (2.6) in (Nemirovski et al., 2009). Then, since ` is
convex in φ, then
EZ[N]〜μN [(娟-φμ )>d'(φtμ, xt, yt)] ≥ EZ[N]〜μN ['(娟,xt, "t) - '(φ*μ, xt, 9t)] .
Denote et =6名m〜*n ∣∣φμ - φμ∣∣2 and combine the above two inequalities, we have
2γEz[N]〜μN ['(φμ, xt,yt) - '(φμ, xt,yt)] ≤ et-et+1 + Y2(Bx + cBφ)2.
Summing over t, we have
κ	1 G/At	、0(K k	kφ1μ- φμk2 , γ(Bx + cBΦ)2
EZ[N]〜μN N 2√(φμ, xt, yt) - '(φμ, xt, yt) ≤	2YN	∣	2
Taking expectation over the training samples z。，T, and μ, we have
1N
E(μ,T)~(pu ×PT)EZ[τ]~μτEZ[N]~μNNf'(φμ, xt,yt) - '(φμ, xt,yt)
≤ E(μ,T )~(pu×PT )EZ[T ]〜μτ
t=1
kφ(Z[t]) 一 φμk2
2γN
1 [T ≥ t*]
+ E(μ,T )Mpu×pT ) W- 1 [T<t*]
+
Y(Bx + cBφ)
2
13
Under review as a conference paper at ICLR 2021
Regarding the first term on the right hand side, by Proposition A.1,
E(μ,T )~(pu×pT )EZ[τ ]~,T kφ(Z[T]) - φμk2l [T ≥ t*]
≤ E(μ,τ)~(pu×pT)
Q(μ)1 [T ≥ t*]
1 [T ≥ t*]
Q ET ~pT T +1一
Z(α +1,t* + 1)
Q	ζ(α1	.
T+1
∞
=Q X
T=t*
1 [T ≥ t*]
=Eμ-Pu Q (μ)ET-pT -T + J-
1	(T +1)-α
T +1 Z(α, 1)
Regarding the second term,
E(μ,T )~(pu×pT) Ilm- φμ∣2ι [T<t*]
=VarmET〜「竽 1 [T < t*] = Varm (1 - Z.t* +1)].
T	ζ(α, 1)
Combining the above inequalities, We can obtain the first bound in Eq. 13, and Eq. 14 folloWs. NoW
We need to find the optimal choice of the threshold t*. Note that
Q
T + 1
W)= ET ~pT
1 [T ≥ t*] + VarmI [T < t*] ≥ ET~「竽 min T TQI, Varm
Since TQi monotonely decreases as T increases, it is easy to see that
Q
min T+1, Varm
When T< VQm -1
When T, ≥ VQm -1.
Therefore, the lower bound of β(t*) achieves at
t**
Q
Varm
- 1,
and it is the optimizer.
□
A.2 Details for Implication 2 in Section 2
Regarding Implication 2 in Section 2, here We provide more derivation steps for the optimal step
sizes for the tWo group of users.
The expected excessive risk can be Written as the sum of the excessive risks of the tWo groups of
users:
E
1N
N Ε'(φμ, xt, yt) - '(φμ, xt, yt)
t=1
=E [(N X'(φμ, xt,yt) - '(φ*μ, χt,yt))1[T <t*]
+ E ]	X'(φtμ, xt, yt) - '(φμ, xt, yt)) 1[T ≥ t*]
for inactive users
for active users
With similar derivation steps as the proof for Theorem A, We can see that for inactive users, the bound
is
E ](( X'(φμ, xt,yt) - '(φμ, xt,yt)) 1[T <t*]
≤E
Varm
2γN
Y(Bx + cBφ)
+	2
1[T < t*]
Varm
2γN
+ Y(Bx + cBφ)2 ) Pr[T < t*
14
Under review as a conference paper at ICLR 2021
Optimizing the step size γ gives γin
1
Bχ + cBφ
Similarly, for active users, the bound is
E ]([ χ']φμ, Myt)- '⑹，Myt)) 1[T ≥ t*]
≤E
( Q
(T + 1l2YN
Y(Bx + cBφ )2
+	2
1[T ≥ t*]
(QZ (α + 1,t* + 1)	Y(Bx + cBφ)2< (α,t* + 1)
V	2γN	+	2
/ζ(α, t*+1).
1 '*'∖∙f>+; γv⅛ 1 1ι Ti 1 +k 1 4 fa TA 4lrza C	\ Td 4 C — _1_ / Z(a ∣1,t II)^Q
Optimizing the step size Y gives Yact — Bx + cBφ V Z(α,t* + 1)N .
B Baseline specification
Some of the compared baseline methods are not directly scalable to large scale interaction graphs, or
originally designed for ranking task. To make the baselines runnable on large and sparse graphs and
comparable to our proposed method, we have made a few adaptions.
•	CCF: Since item features are provided in the datasets, to allow CCF to make use of the features,
we modify it in the same way as how we incorporate features into our 2TS model (see Section 3.3).
•	JODIE: we made the following adaptions: 1) replaced the static one-hot representation to 64-dim
embeddings because the number of nodes are large; 2) represent the category / tag categorical
feature via a learnable embedding table; 3) used triplet loss with random negative example rather
than original MSE loss, which empirically show improvements.
•	dynAERNN: These two methods are originally designed for discrete graph snapshots, while our
focused tasks are continues interaction graphs. We manually transformed the interactions graph
into 10 snapshots, with equal edge count increments.
For the downstream ranking task, we followed the evaluation method used in dySat (Sankar et al.,
2020): after the node embeddings are trained, we train a logistic regression classifier to predict link
between a pair of user / item node embedding using Hadmard operator. The logits on test set are
used for ranking. For Taobao dataset, we are not able to get reslt of dynAERNN given the memory
constraint.
•	GRU4Rec & DIEN & MIMN: We changed the data batching from per-user based to per-example
based, to better adapt for time ordered interaction data and to better make the full use of the training
data. Besides, the implementation of GRU4Rec follows implementation by the authors of MIMN
paper, which includes some modifications compared to the original version of GRU4Rec released
by their authors, and should be expected to perform better.
•	Bert4Rec: For ML-25M and lastfm-1k dataset, we followed the hyper-parameter setting for ML-
20M dataset, according to the implementation released by the authors, and changed dimension to
128. For Taobao dataset, to fit the model size into 16GB GPU memory, we changed the embedding
dimension to 32 and batch size to 8.
•	GCMC & GCMC-SAGE & GCMC-GAT: We changed the loss function from Softmax over
different ratings to Softmax over [true item, 10 random selected items], to better adapt to the
ranking task.
Besides, we clarify that in the baseline models Bert4Rec, SASRec, and dynAERNN, item features
are not incorporated, since it’s not straightforward to include features into their original model design,
and it’s unclear if adding features will help their setup or not.
C Configuration of 2TS and 2TS-Plus
We will present some important hyperparameters chosen for the 2TS and 2TS-Plus in the experiments.
15
Under review as a conference paper at ICLR 2021
•	Threshold t* ∈ N: Users with more than t* interactions observed in the training set are considered
active users, and those with less than or equal to t* observed interactions are considered inactive
users. For the results that we get in Table 1, the choices of the thresholds are given in Table 4.
Table 4: For the results shown in Table 1, the thresholds t* for splitting the users are specified as
follows.
	Taobao	Movielens-25M	lastfm-1K
2TS	100	200	100
2TS-Plus	100	200	200
•	Last K interactions for active users: In Stage 2 training, for each active user, we only use the last
K interactions to train the RNN. The number K is chosen to be either 10 or 20, and they actually
give similar performances.
•	Embedding dimension: Both user embeddings and item embeddings are set to be of dimension 64.
If the item contains features, we will use another 64 dimension for the feature embedding.
•	Learning rate: the learning rate for training 2TS and 2TS-Plus is searched over 1e - 3 and 1e - 4
only. Most of the time 1e - 4 works better. The optimizer is Adam.
Figure 5: Distribution of the number of observed interactions in the training dataset.
16