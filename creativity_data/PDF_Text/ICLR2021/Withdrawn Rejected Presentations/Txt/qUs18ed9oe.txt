Under review as a conference paper at ICLR 2021
Lyapunov Barrier Policy Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Deploying Reinforcement Learning (RL) agents in the real-world require that the
agents satisfy safety constraints. Current RL agents explore the environment with-
out considering these constraints, which can lead to damage to the hardware or
even other agents in the environment. We propose a new method, LBPO, that uses
a Lyapunov-based barrier function to restrict the policy update to a safe set for
each training iteration. Our method also allows the user to control the conserva-
tiveness of the agent with respect to the constraints in the environment. LBPO
significantly outperforms state-of-the-art baselines in terms of the number of con-
straint violations during training while being competitive in terms of performance.
Further, our analysis reveals that baselines like CPO and SDDPG rely mostly on
backtracking to ensure safety rather than safe projection, which provides insight
into why previous methods might not have effectively limit the number of con-
straint violations.
1	Introduction
Current reinforcement learning methods are trained without any notion of safe behavior. As a result,
these methods might cause damage to themselves, their environment, or even harm other agents
in the scene. Ideally, an agent in a real-world setting should start with a conservative policy and
iteratively refine it while maintaining safety constraints. For example, an agent that is learning to
drive around other agents should start driving slowly and gradually learn to improve its performance
by exploring carefully while avoiding accidents. In contrast, most deep reinforcement learning
methods learn by trial and error without taking into consideration the safety-related consequences
of their actions (Silver et al., 2016; Vinyals et al., 2019; Akkaya et al., 2019). In this work, we
address the problem of learning control policies that optimize a reward function while satisfying
some predefined constraints throughout the learning process.
As in previous work for safe reinforcement learning, we use human-defined constraints to specify
safe behavior. A classical model for RL with constraints is the constrained Markov Decision Process
(CMDP) (Altman, 1999), where an agent tries to maximize the standard RL objective of expected
returns while satisfying constraints on expected costs. A number of previous works on CMDPs
mainly focus on environments that have low dimensional action spaces, and they are difficult to
scale up to more complex environments (Turchetta et al., 2019; Wachi & Sui, 2020). One popu-
lar approach to solve Constrained MDPs in large state and action spaces is to use the Lagrangian
method (Altman, 1998; Ray et al., 2019). This method augments the original RL objective with a
penalty on constraint violations and computes the saddle point of the constrained policy optimization
via primal-dual methods (Altman, 1998). While safety is ensured asymptotically, no guarantees are
made about safety during training. As we show, other methods which claim to maintain safety during
training also lead to many safety violations during training in practice. In other recent work, Chow
et al. (2018; 2019) use Lyapunov functions to explicitly model constraints in the CMDP framework
to guarantee safe policy updates. We build on this idea, where the Lyapunov function allow us to
convert trajectory-based constraints in the CMDP framework to state-based constraints which are
much easier to deal with.
In this work, we present a new method called LBPO (Lyapunov Barrier Policy Optimization) for
safe reinforcement learning in the CMDP framework. We formulate the policy update as an un-
constrained update augmented by a barrier function which ensures that the policy lies in the set of
policies induced by the Lyapunov function, thereby guaranteeing safety. We show that LBPO allows
us to control the amount of risk-aversion of the agent by adjusting the barrier. We also analyze pre-
1
Under review as a conference paper at ICLR 2021
vious baselines that use a backtracking recovery rule and empirically show that their near-constraint
satisfaction can be explained by their recovery rule; this approach leads to many constraint viola-
tions in practice. Finally, we demonstrate that LBPO outperforms state-of-the-art CMDP baselines
in terms of the number of constraint violations while being competitive in performance.
2	Background
We consider the Reinforcement Learning setting where an agent’s interaction with the environment
is modeled as a Markov Decision Process (MDP). An MDP is a tuple (S, A, P, r, s0) with state-
space S, action-space A, dynamics P : S × A → S, reward function r(s, a) : S × A → R and
initial state s0. P (.|s, a) is the transition probability distribution and r(s, a) ∈ [0, Rmax]. We fo-
cus on the special case of constrained Markov decision processes (CMDP) (Altman, 1999), which
is an augmented version of MDP with additional costs and trajectory-based constraints. A CMDP
is represented as a tuple (S, A, P, r, c, s0 , d0). The terms S, A, P, r, s0 are the same as in the un-
constrained MDP; the additional terms c(s) is the immediate cost and d0 ∈ R≥0 is the maximum
allowed value for the expected cumulative cost of the policy. We define a generic version of the
Bellman operator w.r.t policy π and a function h(s, a) : S × A → R as follows:
B∏,h[V ][s] = Eπ(a∣s)[h(s, a) + Y E P(SlS,a)V(s')]	⑴
a	sz∈S
The function h(S, a) can be instantiated to be the reward function or the cost function. When it is the
reward function, this becomes the normal Bellman operator in RL. When h(S, a) is replaced by the
cost function c(S), it becomes the Bellman operator over the cost objective, which will be used later
in designing the Lyapunov function. We further define Jn(so) = E[E∞=0 Ytr(St, at)∣so, ∏] to be
the performance of the policy ∏, Dn (so) = E[E∞=0 Ytc(St,at)∣so,∏] to be the expected cumulative
cost of the policy π, where π belongs to the set of stationary policies P . Given a CMDP, we are
interested in finding a solution to the following constrained optimization problem:
maxn∈P [Jn(So)] s.t Dn(So) ≤ do	(2)
2.1	Safe Reinforcement Learning using Lyapunov Functions
We will build upon the Lyapunov framework introduced by Chow et al. (2018), also known as
SDDPG. It proposes to use Lyapunov functions to derive a policy improvement procedure with
safety guarantees. The basic idea is that, given a safe baseline policy πB, it finds a set of safe
policies based on πB using Lyapunov functions. For each policy improvement step, it will then
choose the policy with the best performance within this set.
The method works by first designing a Lyapunov function for a safe update around the current safe
baseline policy πB. A set of Lyapunov functions is defined as follows:
LnB(So,do) ={L: S→R≥o :BnB,c[L](S) ≤ L(S), ∀S ∈ S; L(So) ≤ do}	(3)
The Lyapunov functions in this set are designed in a way to construct provably safe policy updates.
Given any Lyapunov function within this set LnB ∈ LnB (So, do), we define the set of policies that
are consistent with it to be the LnB -induced policies:
ILπB ={π ∈P :Bn,c[LnB](S) ≤LnB(S),∀S∈S}	(4)
It can be shown that any policy π in the LnB -induced policy set is “safe”, i.e Dn (So) ≤ do (Chow
et al., 2018).
The choice of LnB affects the LnB -induced policy set. We need to construct LnB such that the LnB -
induced policy set contains the optimal policy ∏*. ChoW et al. (2018) show that one such Lyapunov
function is LnB,e(s) = E[∑∞=o Yt(c(st) + E(St))InB, s], where C(St) ≥ 0. The function LnB,e(s)
can be thought of as a cost-value function for policy πB augmented by an additional per-step cost
c(St). Accordingly, we can define the following state-action value function:
QL∏B,e (S,a) = C(S) + e(S) + Y E P(SlS,a)L∏B ,e(S')	(5)
s'
2
Under review as a conference paper at ICLR 2021
It was shown in ChoW et al. (2018) that finding a state dependent function E such that the the optimal
policy is inside the corresponding LnB ,e-induced set is generally not possible and requires knowing
the optimal policy. As an approximation, they suggest to create the Lyapunov function with the
largest auxiliary cost ^, such that LnB,^(s) ≥ BnB,c[L∏b,^](s) and LnB,^(so) ≤ do. A larger
auxiliary cost E per state ensures that we have a larger set of L-induced policies, making it more
likely to include the optimal policy in the set. The authors show that the following E(S) in the form
of a constant function satisfies the conditions described:
E(S) = (1 - Y)(do - DnB (so))
(6)
Plugging this function E(S) and the definition of Qlπb,e (s, a) into the CMDP objective, the policy
update under the set of policies that lie in the LnB ,^-induced policy set, or equivalently the policies
that are safe, is given by:
∏+(.∣s) = maχ Jn (so),
n∈P
S.t
/	(∏(a∣s)
a∈A
-πB (a|S))QLnB,^(s,a) ≤ E(S)
(7)
In the case of a deterministic policy, the policy update becomes:
n+(.|S) = ImPx Jn(SO) s.t QLnBXS,π(Sy)- QLnBXS,πB (S)) ≤ G(S)	⑻
We build upon this objective in our work. We include the proof of the Lyapunov approach for
completeness in Appendix A.1, and we advise the reader to see previous work (Chow et al., 2018)
for a more detailed derivation. Using the Lyapunov function, the trajectory-based constraint of the
CMDP is converted to a per-state constraint (Eq. 7), which is often much easier to deal with.
3	Method
3.1	Barrier function for Lyapunov Constraint
We present Lyapunov Barrier Policy Optimization (LBPO) that aims to update policies inside the
LnB ,^-induced policy set. We work under the standard policy iteration framework which contains
two steps: Q-value Evaluation and Safe Policy Improvement. We initialize LBPO with a safe base-
line policy πB . In practice, we can obtain safe initial policies using a simple (usually poorly per-
forming) hand-designed control policy; in our experiments, we simplify this process and achieve
safe initial policies by training on the safety objective. We assume that we have m different con-
straints, as LBPO naturally generalizes to more than one constraint.
3.1.1	Q Evaluation
We use on-policy samples to evaluate the current policy. We compute a reward Q function QR,
and cost Q functions QCi corresponding to each cost constraint i ∈ [1, 2...m]. Each Q function
is updated using TD(λ) (Sutton, 1988) which helps us more accurately estimate the Q functions.
Furthermore, we use the on-policy samples to get the cumulative discounted cost Dni (So) of the
current policy, which allows us to set up the constraint budget for each constraint given by Ei =
(1 - γ)(dio - Dni (So)) as shown in Eq. 6.
3.1.2	Regularized Safe Policy Update
In this work, we focus on deterministic policies, where we have the following policy update under
the L-induced set for each constraint as given in Eq. 8 :
∏+(.∣s) = maχ Jn(So), s.t QL	<s,π(s)) - QL <s,∏b (s)) ≤ Ei(SNi ∈ [1, 2,…m]	(9)
n∈P	nB，e	nB，e
We can simplify this equation further by replacing QLn ^ with QCB which is the ith cost Q-function
under the policy ∏b , when E is a constant function (see Appendix A.1.1). To ensure that the Lya-
punov constraints are satisfied, we construct an unconstrained objective using an indicator penalty
3
Under review as a conference paper at ICLR 2021
I(QπCBi (s, πθ(s))) for each constraint.
I(QπCBi(s,πθ(s))) = 0∞
QCB(S,πθ(S))- QCB(S,πB(S)) ≤ &(S)
QCB(S,πθ(S))- QCB(S,πB(S)) > Ei(S)
(10)
We will use a differentiable version of the indicator penalty called the logarithmic barrier function:
MQnBi(S,πθ(S))) = -βlog (E(S)-(QCB(S,πθ(S))- QCB(S,πb(S)))	(II)
The function ψ is parameterized by θ and QπCBi (S, πB (S)) is a constant. Our policy update will
use the gradient at πθ = πB , ensuring that the logarithmic barrier function is well defined, since
^(s) > 0 ∀s.
Figure 1 captures the behavior of the function ψ for dif-
ferent β . The parameter β captures the amount of risk-
aversion we desire from the agent. A high β will help
avoid constraint violations occurring due to approxima-
tion errors in our learned Q-functions. We verify this em-
pirically in Section 4.
We use the Deterministic Policy Gradient Theorem (Sil-
ver et al., 2014) for the policy update. For up-
dating a θ-parameterized policy with respect to the
expected return, the objective can be written as:
argmi□θ Es〜ρ∏B [(-QRB(s,∏θ(s)))], where PnB is the
on-policy state distribution.
Since we rely on on-policy samples for Q-function eval-
uation, the Q-function estimation outside the on-policy
state distribution can be arbitrarily bad. Similar to Schul-
man et al. (2015), we constrain our policy update using a
Figure 1: As the difference between Q
values for action a and the baseline ac-
tion reaches e (in this case e = 2), the
loss increases to ∞.
hard KL constraint (i.e. a trust region) between the current policy and the updated policy under the
presence of stochastic exploration noise. The trust region also allows us to make sure that our policy
change is bounded, which allows us to ensure that with a small enough trust region, our first order
approximation of the objective is valid.
Augmenting the on-policy update with the Lyapunov barrier and a KL regularization, we have the
following LBPO policy update:
m
θk+ι = argminθ Es〜p∏b -QnB(S,π(s)) + EΨ(QCB(s,∏θ(s)))	(12)
i=1
subject to Es〜ρ∏B [Dkl(∏θ[.|s] + N(0,δ) ∣∣ ∏b [.|s] + N(0,δ))] < μ (13)
where δ is the exploration noise, PnB is the state distribution induced by the current policy, μ is
the expected KL constraint threshold and we set πB to the safe policy at iteration k, as the update
guarantees safe policies at each iteration. In practice, we expand our objective using a Taylor series
expansion and solve to a leading order approximation around θk. Letting the gradient of the objective
in Eq 12 be denoted by g and the Hessian of the KL divergence by H, our objective becomes:
θk+ι = argminθ gτ(θ — θk), subject to 1(θ - θk)TH(θ - θk) ≤ μ	(14)
We solve this constrained optimization using the Fisher vector product with Conjugate gradient
method similar to Schulman et al. (2015).
4	Experiments
In this section, we evaluate LBPO and compare it to prior work. First, we benchmark our method
against previous baselines to show that LBPO can achieve better constraint satisfaction while be-
ing competitive in performance. Second, we give empirical evidence that previous methods near
constraint satisfaction can be explained by backtracking. Third, we show by a didactic example
4
Under review as a conference paper at ICLR 2021
Figure 2: OPenAI Safety Environments: PointGoal1, PointPush2, CarGoal2, DoggoPuSh2
that LBPO is more robust than CPO and SDDPG to Q-function errors, hence making it a preferable
alternative, especially when function approximation is used. Finally, We show that LBPO allows
flexible tuning of the amount of risk-aversion for the agent.
Comparisons. For our experiments, We com-
pare LBPO against a variety of baselines: PPO,
PPO-lagrangian, SDDPG, CPO and BACK-
TRACK. PPO (Schulman et al., 2017) is an
on-policy RL algorithm that updates in an ap-
proximate trust-region without considering any
constraints. PPO-lagrangian belongs to a class
of Lagrangian methods (Altman, 1998) for safe
Reinforcement Learning, which transforms the
constrained optimization problem to a penalty
form max∏∈pminλ≥oE[E∞=0 Ytr(st, at)+
λ(E∞=0 Ytc(st) - do)∣∏, S0]. ∏ and λ are
jointly optimized to find a saddle point of
the penalized objective. SDDPG (Chow
et al., 2018; 2019) introduces the Lyapunov
framework for safe-RL and proposes an
action-projection method which in theory
guarantees the update of the policy within a
safe set. We evaluate the α-projection version
Constraint Violations
LBPO
PPO-Iagrange
CPO
SDDPG
BACKTRACK
Figure 3: Each point corresponds to a particular
safety method applied to a certain safety environ-
ment. The x-axis shows the fraction of constraint
violations encountered by the behavior policy dur-
ing training and y-axis shows the policy perfor-
mance normalized by the corresponding environ-
ment’s PPO return.
of SDDPG (Chow et al., 2019). Since the original implementation for the method is unavailable,
we re-implemented the method to the best of our abilities. CPO (Achiam et al., 2017) derives a
trust-region update rule which guarantees the monotonic improvement of the policy while satisfying
constraints. CPO also uses a backtracking recovery rule. We elaborate on the BACKTRACK
baseline in Section 4.2.
Tasks. We evaluate these methods using the OpenAI Safety Gym (Ray et al., 2019), which consists
of 12 continuous control MuJoCo tasks (Todorov et al., 2012). These tasks use 3 robots: Point, Car,
and Doggo. Point is the simplest of three which can be commanded to move forward/backward or to
turn. Car has two driven wheels which needs to be controlled together to obtain forward/backward
and turning behavior. Doggo is a quadrupedal robot whose joint angles at hip, knee and torso can
be commanded to obtain similar behavior. Each robot has 2 types of tasks (Goal, Push) with 2
difficulty levels (1, 2). In Goal tasks, the robot has to move to a series of goal locations, and in
Push tasks, the robot has to push a box to a series of goal locations. There are mobile and immobile
obstacles made up of a hazard region, vases and pillars which generate a cumulative penalty for the
agent. Point has an observation space of60 dimensions, Car has 72 dimensions, and Doggo has 104
dimensions, which constitute sensor readings, joint angles, and velocities. The environments are
shown in Figure 2.
4.1	Safe Reinforcement Learning Benchmarks
We summarize the comparison of LBPO to all of the baselines (PPO, PPO-lagrangian, BACK-
TRACK, SDDPG and CPO) on the OpenAI safety benchmarks in Figure 3 and Tables 1 and 2.
Additional training plots for policy return and policy cost can be found in Appendix A.2.1.
Constraint Satisfaction. Table 1 shows that in all the environments, LBPO actively avoids con-
straint violations, staying below the threshold in most cases. In the PointGoal2 environment, no
5
Under review as a conference paper at ICLR 2021
Method	PPO	PPO-Iagrangian	CPO	SDDPG	BACKTRACK	LBPO
PointGoal1	1.00	0.48	0.79	0.78	0.85	0.04
PointGoal2	0.98	0.60	0.75	0.98	0.94	0.34
PointPush1	1.00	0.51	0.14	0.12	0.19	0.00
PointPush2	1.00	0.51	0.66	0.36	0.77	0.00
CarGoal1	1.00	0.56	0.89	0.54	0.79	0.03
CarGoal2	1.00	0.61	0.57	0.68	0.84	0.00
CarPush1	0.99	0.39	0.10	0.26	0.23	0.01
CarPush2	1.00	0.58	0.49	0.23	0.79	0.03
DoggoGoal1	1.00	0.90	0.00	0.00	0.07	0.00
DoggoGoal2	1.00	0.45	0.00	0.00	0.00	0.00
DoggoPush1	0.98	0.56	0.00	0.00	0.00	0.00
DoggoPush2	1.00	0.34	0.00	0.00	0.09	0.00
Table 1: We report the fraction of unsafe behavior policies encountered during training across differ-
ent OpenAI safety environments for the policy updates across 2e7 training timesteps. LBPO obtains
fewer constraint violations consistently across all environments.
Method	PPO	PPO-lagrangian	CPO	SDDPG	BACKTRACK	LBPO
PointGoal1	1.00	0.826	0.450	0.451	0.670	0.480
PointGoal2	1.00	0.200	0.000	0.000	0.045	0.026
PointPush1	1.00	0.659	0.375	0.587	0.527	0.683
PointPush2	1.00	0.483	0.213	0.221	0.413	0.358
CarGoal1	1.00	0.449	0.079	0.097	0.497	0.376
CarGoal2	1.00	0.066	0.172	0.215	0.162	0.210
CarPush1	1.00	0.697	0.000	0.434	0.868	0.485
CarPush2	1.00	0.353	0.403	0.369	0.399	0.430
DoggoGoal1	1.00	0.000	0.003	0.002	0.003	0.007
DoggoGoal2	1.00	0.016	0.002	0.003	0.003	0.003
DoggoPush1	1.00	0.080	0.014	0.001	0.079	0.012
DoggoPush2	1.00	0.071	0.000	0.000	0.063	0.000
Table 2: Cumulative return of the converged policy for each safety algorithm normalized by PPO’s
return. Negative returns are clipped to zero. LBPO tradeoffs return for better constraint satisfaction.
Bold numbers show the best performance obtained by a safety algorithm (thus excluding PPO).
method can achieve good constraint satisfaction which we attribute to the nature of the environment
as it was found that safe policies were not obtained even when training only on the cost objective.
In all the other cases we note that LBPO achieves near-zero constraint violations.
Like our method, SDDPG also builds upon the optimization problem from Equation 8 but solves
this optimization using a projection onto a safe set instead of using a barrier function. We noticed
the following practical issues with this approach: First, in SDDPG, each safe policy is composed
of a projection layer, which itself relies on previous safe policies. This requires us to maintain
all of the previous policies and thus the memory requirement grows linearly with the number of
iterations. SDDG circumvents this issue by using a policy distillation scheme (Chow et al., 2018),
which behavior clones the safe policy into a parameterized policy not requiring a projection layer.
However, behavior cloning introduces errors in the policy leading to frequent constraint violations.
Second, we will show in section 4.3 that SDDPG is more sensitive to Q-function errors. PPO-
lagrangian produces policies that are only safe asymptotically and makes no guarantee of the safety
of the behavior policy during each training iteration. In practice, we observe that it often violates
constraints during training.
Behavior Policy Performance. OpenAI safety gym environment provide a natural tradeoff between
reward and constraint. A better constraint satisfaction often necessitates a lower performance. We
observe in Table 2 that LBPO achieves performance competitive to the baselines.
4.2	Backtracking Baseline
CPO (Achiam et al., 2017) and SDDPG (Chow et al., 2019) both use a recovery rule once the policy
becomes unsafe, which is to train on the safety objective to minimize the cumulative cost until the
6
Under review as a conference paper at ICLR 2021
PointGoaH	CarGoalI
Jsou əmpjəʌv
Environment Steps
0 5 0
JSoɔ əmpjəʌv
0.0001	— 0.001	— 0.005	— 0.01
Figure 5: Increasing β parameter for the barrier increases the risk aversion of the agent as Can be
seen in the plots above.
policy becomes safe again. In this section, We test the hypothesis that CPO and SDDPG are unable
to actively avoid constraint violation but their near constraint satisfaction behavior can be explained
by the recovery rule. To this end, we introduce a simple baseline, BACKTRACK, which uses the
following objective for policy optimization under a trust region (We use the same trust region as in
LBPO):
Jmax∏∈p Es^ρ∏B [Qrb (s, π(s))] if ∏b is safe
Imin∏∈p Es~ρ∏B [Qcb (s, π(s))] if ∏b is unsafe
(15)
Thus, if the most recent policy πB is evaluated to be safe, BACKTRACK exclusively optimizes the
reward; however, if the most recent policy πB is evaluated to be unsafe, BACKTRACK exclusively
optimizes the safety constraint. Effectively, BACKTRACK relies only on the recovery behavior
that is used in CPO and SDDPG, without incorporating their mechanisms for constrained policy
updates. In Tables 1 and 2, we see that BACKTRACK is competitive to both CPO and SDDpG
in terms of both constraint satisfaction and performance (maximizing reward), suggesting that the
recovery behavior is itself sufficient to explain their performance. In Appendix A.2.2, we compare
the number of backtracks performed by CPO, SDDPG and BACKTRACK.
4.3	ROBUSTNESS TO FINITE SAMPLE SIZES
We generally work in the function approxima-
tion setting to accommodate high dimensional
observations and actions, and this makes it nec-
essary to rely on safety methods that are robust
to Q-function errors. To analyze how robust
different methods are to such errors, we de-
fine a simple reinforcement learning problem:
Consider an MDP with two dimensional state
space given by (x, y) ∈ R. The initial state is
(0,0). Actions are two dimensional, given by
a1, a2) : a1, a2 ∈ [-0.2, 0.2]. The horizon is
10 and the transition probability distribution is
3,y/) = (x,y) + (a1,a2) + N(0, 0.1). The
reward function is r(x,y) = Jx2 + y2. The
cost function is equal to the reward function for
all states, and the constraint threshold is set to
2. We plot in Figure 4 the total constraint viola-
tions during 100 epochs of training with vary-
ing number of samples used to estimate the cost
Q-function. We find that LBPO is more robust
to Q-function errors due to limited data com-
On-Policy samples
Figure 4: An analysis of the robustness of safe
RL algorithms CPO, SDDPG, and LBPO to finite
sample Q function errors for a simple didactic en-
vironment. Constraint violations in CPO and SD-
DPG increase quickly as the number of on-policy
samples used to estimate the Q function decreases.
Results are averaged over 5 seeds.
pared to CPO and SDDPG. In this experiment we use β = 0.005, similar to the value used for the
benchmark experiments.
7
Under review as a conference paper at ICLR 2021
4.4	Tuning conservativeness with the barrier
A strength of LBPO is the ability to tune the barrier to adjust the amount of risk-aversion of the
agent. Specifically, β in Equation 11 can be tuned; a larger β leads to more conservative policies.
In Figure 5, we empirically demonstrate the sensitivity of β to the conservativeness of the policy
update. For our benchmark results, we do a hyperparameter search for β in the set (0.005, 0.008,
0.01, 0.02) and found that 0.005 works well across most environments.
5	Related Work
Constrained Markov Decision Process CMDP’s (Altman, 1998) have been a popular framework
for incorporating safety in the form of constraints. In CMDP’s the agent tries to maximize expected
returns by satisfying constraints on expectation of costs. Altman (1999) demontrated that for finite
MDP with known models, CMDP’s can be solved by solving the dual LP program. For large state
dimensions (or continuous), solving the LP becomes intractable. A common way to solve CMDP in
large spaces is to use the Lagrangian Method (Altman, 1999; Geibel & Wysotzki, 2005; Chow et al.,
2017). These methods augment the original RL objective with a penalty on constraint violation and
computes the saddle point of the constrained policy optimization via primal-dual methods. These
methods give no guarantees of safety during training and are only guaranteed asymptotically at
convergence. CPO (Achiam et al., 2017) is another method for solving CMDP’s that derives an
update rule in the trust region which guarantees monotonic policy improvement under constraint
satisfaction, similar to TRPO (Schulman et al., 2015). Chow et al. (2018; 2019) presents another
class of method that formulates safe policy update under a Lyapunov constraint. Perkins & Barto
(2002) explored the relevance of Lyapunov functions in control and (Berkenkamp et al., 2017) used
Lyapunov functions in RL to guarantee exploration such that the agent can return to a ”region of
attraction” in the model-based regime. In our work, we show that previous baselines rely on a
backtracking recovery rule to ensure near constraint satisfaction and are sensitive to Q-function
errors; we present a new method that uses a Lyapunov constraint with a barrier function to ensure a
conservative policy update.
Other notions of safety. Recent works (Pham et al., 2018; Dalal et al., 2018) use a safety layer along
with the policy, which ensures that all the unsafe actions suggested by the policy are projected in the
safe set. Dalal et al. (2018) satisfies state-based costs rather than trajectory-based costs. Thananjeyan
et al. (2020) utilizes demonstrations to ensure safety in the model based framework, and Zhang et al.
(2020) learns the epistemic uncertainty of the environment by training the model in simulation for a
distribution of environments, which is then used to cautiously adapt the policy while deploying on a
new test environment. Another line of work focuses on optimizing policies that minimize an agent’s
conditional value at risk (cVAR). cVAR (Rockafellar et al., 2000) is commonly used in quantitative
finance, which aims to maximize returns in the worst α% of cases. This allows the agent to ensure
that it learns safe policies for deployment that achieve high reward under the aleatoric uncertainty
of the MDP (Tang et al., 2019; Keramati et al., 2019; Tamar et al., 2014; Kalashnikov et al., 2018;
Borkar & Jain, 2010; Chow & Ghavamzadeh, 2014).
6	Conclusion
In this work, we present a new method, LBPO, that formulates a safe policy update as an uncon-
strained policy optimization augmented by a barrier function derived from Lyapunov-based con-
straints. LBPO allows the agent to control the risk aversion of the RL agent and is empirically
observed to be more robust to Q-function errors. We also present a simple baseline BACKTRACK
to provide insight into previous method’s reliance on backtracking recovery behavior to achieve near
constraint satisfaction. LBPO achieves fewer constraint violations, in most cases close to zero, on a
number of challenging continuous control tasks and outperforms state-of-the-art safe RL baselines.
8
Under review as a conference paper at ICLR 2021
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv
preprint arXiv:1705.10528, 2017.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019.
Eitan Altman. Constrained markov decision processes with total cost criteria: Lagrangian approach
and dual linear program. Mathematical methods ofoperations research, 48(3):387-417,1998.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in neural information processing
systems, pp. 908-918, 2017.
Vivek Borkar and Rahul Jain. Risk-constrained markov decision processes. In 49th IEEE Conference
on Decision and Control (CDC), pp. 2664-2669. IEEE, 2010.
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps. In Ad-
vances in neural information processing systems, pp. 3509-3517, 2014.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070-6120, 2017.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-
based approach to safe reinforcement learning. In Advances in neural information processing
systems, pp. 8092-8101, 2018.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under
constraints. Journal of Artificial Intelligence Research, 24:81-108, 2005.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018.
Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be
conservative: Quickly learning a cvar policy. arXiv preprint arXiv:1911.01546, 2019.
Theodore J Perkins and Andrew G Barto. Lyapunov design for safe reinforcement learning. Journal
of Machine Learning Research, 3(Dec):803-832, 2002.
Tu-Hoa Pham, Giovanni De Magistris, and Ryuki Tachibana. Optlayer-practical constrained opti-
mization for deep reinforcement learning in the real world. In 2018 IEEE International Confer-
ence on Robotics and Automation (ICRA), pp. 6236-6243. IEEE, 2018.
Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking Safe Exploration in Deep Reinforce-
ment Learning. 2019.
R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal
of risk, 2:21-42, 2000.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
9
Under review as a conference paper at ICLR 2021
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. arXiv preprint
arXiv:1404.3862, 2014.
Yichuan Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov. Worst cases policy gradients. arXiv
preprint arXiv:1911.03618, 2019.
Brijen Thananjeyan, Ashwin Balakrishna, Ugo Rosolia, Felix Li, Rowan McAllister, Joseph E Gon-
zalez, Sergey Levine, Francesco Borrelli, and Ken Goldberg. Safety augmented value estima-
tion from demonstrations (saved): Safe deep model-based rl for sparse cost robotic tasks. IEEE
Robotics and Automation Letters, 5(2):3612-3619, 2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration for interactive machine
learning. In Advances in Neural Information Processing Systems, pp. 2891-2901, 2019.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Akifumi Wachi and Yanan Sui. Safe reinforcement learning in constrained markov decision pro-
cesses. arXiv preprint arXiv:2008.06626, 2020.
Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman. Cautious adapta-
tion for reinforcement learning in safety-critical settings. arXiv preprint arXiv:2008.06622, 2020.
10