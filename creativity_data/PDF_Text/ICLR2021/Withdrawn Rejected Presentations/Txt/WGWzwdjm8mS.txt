Under review as a conference paper at ICLR 2021
Early Stopping by Gradient Disparity
Anonymous authors
Paper under double-blind review
Ab stract
Validation-based early-stopping methods are one of the most popular techniques
used to avoid over-training deep neural networks. They require to set aside a
reliable unbiased validation set, which can be expensive in applications offering
limited amounts of data. In this paper, we propose to use gradient disparity, which
we define as the `2 norm distance between the gradient vectors of two batches
drawn from the training set. It comes from a probabilistic upper bound on the
difference between the classification errors over a given batch, when the network
is trained on this batch and when the network is trained on another batch of points
sampled from the same dataset. We empirically show that gradient disparity is
a very promising early-stopping criterion when data is limited, because it uses
all the training samples during training. Furthermore, we show in a wide range
of experimental settings that gradient disparity is not only strongly related to the
usual generalization error between the training and test sets, but that it is also
much more informative about the level of label noise.
1 Introduction
Early stopping is a commonly used regularization technique to avoid under/over fitting deep neural
networks trained with iterative methods, such as gradient descent (Prechelt, 1998; Yao et al., 2007;
Gu et al., 2018). To have an unbiased proxy on the generalization error, early stopping requires
a separate accurately labeled validation set. However, labeled data collection is an expensive and
time consuming process that might require domain expertise (Roh et al., 2019). Moreover, deep
learning is becoming popular to use for new and critical applications for which there is simply not
enough available data. Hence, it is advantageous to have a signal of overfitting that does not require
a validation set, then all the available data can be used for training the model.
Let S1 and S2 be two batches of points sam-
pled from the available (training) dataset. Sup-
pose that S1 is selected for an iteration (step) of
stochastic gradient descent (SGD), which then
updates the parameter vector to w1 . The aver-
age loss over S1 is in principle reduced, given
a sufficiently small learning rate. However,
the average loss over the other batch S2 (i.e.,
LS2 (hw1 )) is not as likely to be reduced. It
will remain on average larger than the loss com-
puted over S2, if it was S2 instead ofS1 that had
been selected for this iteration (i.e., LS2 (hw2 )).
The difference is the penalty R2 that we pay for
choosing S1 over S2 (and similarly, R1 is the
penalty that we would pay for choosing S2 over
S1 ). R2 is illustrated in Figure 1 for a hypo-
thetical non-convex loss as a function of a one
dimensional parameter. The expected penalty
measures how much, in an iteration, a model
Figure 1: An illustration of the penalty term R2,
where the y-axis is the loss, and the x-axis indi-
cates the parameters of the model. LS1 and LS2
are the average losses over batches S1 and S2, re-
spectively. w(t) is the parameter at iteration t and
wi(t+1) is the parameter at iteration t + 1 if batch
Si was selected for the update step at iteration t,
with i ∈ {1, 2}.
updated on one batch (S1 ) is able to generalize
on average to another batch (S2) from the dataset. Hence, we call R the generalization penalty.
1
Under review as a conference paper at ICLR 2021
We establish a probabilistic upper-bound on the sum of the expected penalties E [R1] + E [R2] by
adapting the PAC-Bayesian framework (McAllester, 1999a;b; 2003) given a pair of batches S1 and
S2 sampled from the dataset (Theorem 1). Interestingly, under some mild assumptions, this upper
bound is essentially a simple expression driven by kg1 - g2k2, where g1 and g2 are the gradient
vectors over the two batches S1 and S2, respectively. We call this gradient disparity: it measures
how a small gradient step on one batch negatively affects the performance on another one.
Gradient disparity is simple to use and it is computationally tractable during the course of train-
ing. Our experiments on state-of-the-art configurations suggest a very strong link between gradient
disparity and generalization error; we propose gradient disparity as an effective early stopping cri-
terion. Gradient disparity is particularly useful when the available dataset has limited labeled data,
because it does not require splitting the available dataset into training and validation sets so that
all the available data can be used during training, unlike for instance k-fold cross validation. We
observe that using gradient disparity, instead of an unbiased validation set, results in at least 1%
predictive performance improvement for critical applications with limited and very costly available
data, such as the MRNet dataset that is a small size image-classification dataset used for detecting
knee injuries (Table 1).
Task	Method	Test loss	Test AUC score (in percentage)
abnormal	5-fold CV GD	0.284 ± 0.016 (0.307 ± 0.057) 0.274 ± 0.004 (0.275 ± 0.053)	71.016 ± 3.66 (87.44 ± 1.35) 72.67 ± 3.85 (88.12 ± 0.35)
ACL	5-fold CV GD	0.973 ± 0.111 (1.246 ± 0.142) 0.842 ± 0.101 (1.136 ± 0.121)	79.80 ± 1.23 (89.32 ± 1.47) 81.81 ± 1.64 (91.52 ± 0.09)
meniscal	5-fold CV GD	0.758 ± 0.04(1.163 ± 0.127) 0.726 ± 0.019 (1.14 ± 0.323)	73.53 ± 1.30 (72.14 ± 0.74) 74.08 ± 0.79 (73.80 ± 0.24)
Table 1: The loss and area under the receiver operating characteristic curve (AUC score) on the
MRNet test set (Bien et al., 2018), comparing 5-fold cross validation (5-fold CV) and gradient dis-
parity (GD), when both are used as early stopping criteria for detecting the presence of abnormally,
ACL tears, and meniscal tears from the sagittal plane MRI scans. The corresponding curves during
training are shown in Figure 10. The results of early stopping are given, both when the metric has
increased for 5 epochs from the beginning of training and between parenthesis when the metric has
increased for 5 consecutive epochs.
Moreover, when the available dataset contains noisy labels, the validation set is no longer a reliable
predictor of the clean test set (see e.g., Figure 9 (a) (left)), whereas gradient disparity correctly pre-
dicts the performance on the test set and again can be used as a promising early-stopping criterion.
Furthermore, we observe that gradient disparity is a better indicator of label noise level than general-
ization error, especially at early stages of training. Similarly to the generalization error, it decreases
with the training set size, and it increases with the batch size.
Paper Outline. In Section 2, we formally define the generalization penalty. In Section 3, we give
the upper bound on the generalization penalty. In Section 4, we introduce the gradient disparity
metric. In Section 5, we present experiments that support gradient disparity as an early stopping
criterion. In Section 6, we assess gradient disparity as a generalization metric. Finally, in Section 7,
we further discuss the observations and compare gradient disparity to related work. A detailed
comparison to related work is deferred to Appendix H. For our experiments, we consider four image
classification datasets: MNIST, CIFAR-10, CIFAR-100 and MRNet, and we consider a wide range
of neural network architectures: ResNet, VGG, AlexNet and fully connected neural networks.
2	Generalization Penalty
Consider a classification task with input X ∈ X := Rn and ground truth label y ∈ {1,2, ∙∙∙ ,k},
where k is the number of classes. Let hw ∈ H : X → Y := Rk be a predictor (clas-
Sifier) parameterized by the parameter vector W ∈ Rd, and l(∙, ∙) be the 0-1 loss function
l (hw (x), y) = 1 [hw (x)[y] < maxj=y hw (x)[j]] for all hw ∈ H and (x,y) ∈ X X {1, 2,…，k}.
The expected loss and the empirical loss over the training set S of size m are respectively defined as
2
Under review as a conference paper at ICLR 2021
1m
L(hw) = E(χ,y)~D [l (hw(x),y)]	and	LS(hw) = m 工小仅(Xi),yi),	(1)
where D is the probability distribution of the data points and (xi, yi) are i.i.d. samples drawn from
S 〜 Dm. LS(hw) is also called the training classification error. Similar to the notation used in
(Dziugaite & Roy, 2017), distributions on the hypotheses space H are simply distributions on the
underlying parameterization. With some abuse of notation,中LSi refers to the gradient with respect
to the surrogate differentiable loss function, which in our experiments is the cross entropy.
In a mini-batch gradient descent (SGD) setting, consider two batches of points, denoted by S1 and
S2, which have respectively m1 and m2 number of samples, with m1 + m2 ≤ m. The average loss
functions over these two sets of samples are LS1 (hw) and LS2 (hw), respectively. Let w = w(t) be
the parameter vector at the beginning of an iteration t. If S1 is selected for the next iteration, w gets
updated to w1 = w(t+1) with
W1 = W - YVLsι (hw),	(2)
where γ is the learning rate. Conversely, if S2 had been selected instead of S1, the updated param-
eter vector at the end of this iteration would have been w2 = w - γVLS2 (hw ) . Therefore, the
generalization penalty on batch S2 is defined as R2 = LS2 (hw1 ) - LS2 (hw2 ) , which is the gap
between the loss over S2, LS2 (hw1 ), and its target value, LS2 (hw2), at the end of iteration t.
When selecting S1 for the parameter update, Equation (2) makes a step towards learning the input-
output relations of batch S1 . If this negatively affects the performance on batch S2 , R2 will be
large; the model is learning the data structures that are unique to S1 and that do not appear in S2 .
Because S1 and S2 are batches of points sampled from the same distribution D, they have data
structures in common. If, throughout the learning process, we consistently observe that, in each
update step, the model learns structures unique to only one batch, then it is very likely that the
model is memorizing the labels instead of learning the common data-structures. This is captured by
the generalization penalty R.
3	B ound on the Generalization Penalty
We adapt the PAC-Bayesian framework (McAllester, 1999a;b) to account for the trajectory of the
learning algorithm; For each learning iteration we define a prior, and two possible posteriors depend-
ing on the choice of the batch selection. Let W 〜 P bean initial parameter vector that follows a prior
distribution P which is a Ft-measurable function, where Ft denotes the filtration of the available
information at the beginning of iteration t. Let hw1 , hw2 be the two learned single predictors, at the
end of iteration t, from S1 and S2, respectively. In this framework, fori ∈ {1, 2}, each predictor hwi
is randomized and becomes hνi with νi = Wi +ui, where ui is a random variable whose distribution
might depend on Si. Let Qi be the distribution of νi, which is a distribution over the predictor space
H that depends on Si via Wi and possibly ui. Let Gi be a σ-field such that σ(Si) ∪ Ft ⊂ Gi and that
the posterior distribution Qi is Gi-measurable for i ∈ {1, 2}. We further assume that the random
variable νι 〜 Qi is statistically independent from the draw of the batch S2 and, vice versa, that
ν2 〜 Q2 is independent from the batch Si1 , i.e., Gi -LL σ(S2) and G2 ⊥⊥ σ(S1).
Theorem 1. For any δ ∈ (0, 1], with probability at least 1 - δ over the sampling of sets S1 and S2,
the sum of the expected penalties conditional on Si and S2, respectively, satisfies
E	S2皿Q2|1Q1)+2ln 竽 + S2KL(Q1|1Q2)+2ln 竽.⑶
m2 - 2	mi - 2
Theorem 1, whose proof is given in Appendix B, shows why generalization penalties are better
suited to our setting where the two batches Si and S2 are both drawn from the training set S than
the usual generalization errors. After an iteration, the network learns a posterior distribution Qi
on its parameters from Si, yielding to the parameter vector νι 〜 Qi. The expected generalization
1 Batches S1 and S2 are drawn without replacement, and the random selection of indices of batches S1 and
S2 is independent from the dataset S. Hence, similarly to Negrea et al. (2019); Dziugaite et al. (2020), we have
σ(S1) ⊥⊥ σ(S2).
3
Under review as a conference paper at ICLR 2021
error at that time is defined as GEi = Eνι〜Qi [L(hνJ] - Evi〜Qi [Lsi (hνj]. In practice, L(hνJ
is estimated by the test loss over a batch of unseen data, which is independent from νι 〜 Qi.
If S2 is this batch, then2 GEi ≈ Evi〜Qi [Ls2(hvi)] - Evi〜Qi [Lsi (hvi)]. However, this estimate
requires to set S2 aside from S not only during that step but also during all the previous steps,
because otherwise the model hvi would not be independent from S2, making the estimate of GEi
biased. Therefore S2 must be sampled from the validation set, and cannot be used during training.
In contrast, Theorem 1 is valid even if the trained model hvi depends on the samples within the
batch S2. Therefore, the bound on the sum of the (expected) generalization penalties does no longer
require to set S2 aside from S in previous iterations; all data previously reserved for validation can
now be used for training. This is what makes these penalties appealing to measure generalization
especially when the available dataset is limited and/or noisy, as we will see in Section 5.
Theorem 1 remains valid if batch S2 was sampled from the validation set, in which case it can be
compared with known generalization error (GE) bounds, as now hvi does not depend on samples of
S2 . Similarly to GEi , let GE2 be the generalization error when S2 is the training set, while Si is the
test set. By adding GEi and GE2 we obtain E [Ri] + E [R2], hence Theorem 1 also upper bounds
an estimate of GEi + GE2 . We could have obtained another upper bound by directly applying the
bounds from (McAllester, 2003; Neyshabur et al., 2017b), which gives
r 1 r 1	∕2KL(Q2∣∣P) +2ln 2m2	∕2KL(Qι∣∣P) +2ln 2m
E [Ri]+E [R2] ≤ 2V	m2-1--+2V	mj-1--.
(4)
The main difference between Equations (3) and (4) is that the former needs the difference only be-
tween the two posterior distributions, Qi and Q2, whereas the latter requires the difference between
the prior distribution P and the posterior distributions Qi and Q2 . Besides the minor difference
of the multiplicative factor 2, the upper bound in Equation (3) is non-vacuous for a larger class of
distributions than the upper bound in Equation (4): When Qi and Q2 are close to each other but not
to P, the upper bound in Equation (3) is much tighter than the one in Equation (4). Moreover, in
the next section, we show that, under reasonable assumptions, the upper bound in Equation (3) boils
down to a very tractable generalization metric that we call gradient disparity.
4	Gradient Disparity
The randomness modeled by ui, conditioned on the current batch Si, comes from (i) the parameter
vector at the beginning of the iteration w, which itself comes from the random parameter initializa-
tion and the stochasticity of the parameter updates until that iteration, and (ii) the gradient vector
RLSi, which may also be random because of the possible additional randomness in the network
structure due for instance to dropout (Srivastava et al., 2014). A common assumption made in the
literature is that the random perturbation ui follows a normal distribution (Bellido & Fiesler, 1993;
Neyshabur et al., 2017b). The upper bound in Theorem 1 takes a particularly simple form if we
assume that for i ∈ {1, 2} the random perturbations ui are zero mean i.i.d. normal distributions
(Ui 〜N (0, σ2I)), and that Wi is fixed, as in the setting of (DziUgaite & Roy, 2017). KL(Qi ∣∣Q2) is
then the KL-divergence between two multivariate normal distributions .
Let us denote RLSi (hw) by gi ∈ Rd and RLS2 (hw) by g2 ∈ Rd. As wi = w - γgi for i ∈ {1, 2},
the KL-divergence between Qi = N(wi, σ2I) and Q2 = N(w2, σ2I) (Lemma 1 in Appendix A)
is simply
KL(Qi∣∣Q2) = 1 γ22 kgi - g2k2 = KL(Q2∣∣Qi),	(5)
2 σ2
which shows that, keeping a constant step size γ and assuming the same variance for the random
perturbations σ2 in all the steps of the training, the bound in Theorem 1 is driven by kgi - g2 k2 .
2More formally,
ζ
z----------------C-----------------{
GEi = Eνi 〜Qi [Ls2 (hνi)] - Eνi^Qi [Lsi (hνi)] + (Eνi 〜Qi [L(hνJ] - Eνi^Qi [Ls2 (hνi)])
where from HOefding's bound (Theorem 2 in Appendix A) P (|Z| ≥ t) ≤ exp (一2m2t2) , and GEi is approx-
imated with the first term.
4
Under review as a conference paper at ICLR 2021
This indicates that the smaller the `2 distance between gradient vectors is, the lower the upper bound
on the generalization penalty is, and therefore the closer the performance of a model trained on one
batch is to a model trained on another batch.
For two batches of points Si and Sj , with gradient vectors gi and gj , respectively, we define the
gradient disparity (GD) between Si and Sj as
Di,j = kgi - gjk2 .	(6)
Gradient disparity is empirically tractable, and provides a probabilistic guarantee on the sum of the
generalization penalties of Si and Sj , modulo the Gaussianity assumptions made in this section.
Gradient disparity can be computed within batches of the training or the validation set. As discussed
in Section 3, we focus on the first case to have a generalization metric that does not require validation
data and that provides an early stopping criterion with all the available data used for training.
We focus on the vanilla SGD optimizer. In Appendix G, we extend the analysis to other adaptive
optimizers: SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adadelta (Zeiler,
2012), and Adam (Kingma & Ba, 2014). In all these optimizers, we observe that gradient disparity
(Equation (6)) appears in KL(Q1 ||Q2) with other factors that depend on a decaying average of past
gradient vectors. Experimental results support the use of gradient disparity as an early stopping
metric also for these popular optimizers (see Figure 22 in Appendix G).
Computing the gradient disparity averaged over B batches requires all the B gradient vectors at each
iteration, which is computationally expensive if B is large. WeaPProximate it by computing it over
only a much smaller subset of the batches, of size S 《 B, D = PS=ι Pj=ι j= Di,j/s(s — 1).
In the experiments presented in this paper, s = 5; we observed that such a small subset is already
sufficient (see Appendix C.2 for an experimental comparison of different values of s). We also
present an alternative approach that relies on the distribution of gradient disparity instead of its
average in Appendix F to have a finer-grain signal of overfitting.
As training progresses, the gradient magnitude starts to decrease, and therefore the value of gradient
disparity might decrease not necessarily because the distance between two gradient vectors is de-
creasing, but because their magnitudes are decreasing. Hence, in order to compare different stages
of training, We re-scale the loss values within each batch, before computing gradient disparity in D
(refer to Appendix C.1 for more details). Moreover, we consider the mean square error (MSE) for
the choice of the surrogate loss in Appendix C.3 and we observe that gradient disparity is positively
correlated with the MSE test loss as well.
5	Gradient Disparity as an Early Stopping Criterion
Comparison to k-fold Cross Validation. Early stopping is a popular technique used in practice
to avoid overfitting (Prechelt, 1998; Yao et al., 2007; Gu et al., 2018). The optimization is stopped
when the performance of the model on a validation set starts to diverge from its performance on
the training set. Early stopping is of particular interest in the presence of label noise, because the
model first learns the samples with correct labels, and next the corrupted samples (Li et al., 2019). To
emphasize the particular application of gradient disparity, we compare itto k-fold cross validation in
two settings: (i) when the available dataset is limited and (ii) when the available dataset has corrupted
labels. We simulate the limited data scenario by using a small subset of three image classification
benchmark datasets: MNIST, CIFAR-10 and CIFAR-100, and the noisy labeled data scenario is
simulated by using a corrupted version of these datasets. We also evaluate gradient disparity on a
medical dataset (MRNet dataset) with limited available data and in this setting, we use the entire
dataset for training.
(i)	We compare gradient disparity with k-fold cross validation (Stone, 1974) used as an early stop-
ping criterion in Table 2 (top) when there is limited labeled data. We observe that gradient disparity
performs well as an early stopping criterion, especially when the data is complex (CIFAR-100 is
more complex than CIFAR-10). As it uses every available sample, instead of only a 1 - 1/k portion
of the dataset, it results in a better performance on the final unseen (test) data (see also Table 4 and
Figure 8 in Appendix D).
In many real-world applications, collecting (labeled) data is very costly. In some medical applica-
tions, it requires high costs of patient data collection and medical staff expertise. For an example
5
Under review as a conference paper at ICLR 2021
of such an application, we consider the MRNet dataset (Bien et al., 2018), which contains limited
number of MRI scans to study the presence of abnormally, ACL tears and meniscal tears in knee
injuries. We observe that using gradient disparity instead of a validation set, results in over 1% im-
provement (on average over all three tasks) in the test AUC score, and therefore additional correct
detection for more than one patient for each task (see Table 1 and Figure 10 in Appendix D).
(ii)	When the labels of the available data are noisy, the validation set is no longer a reliable estimate
of the unseen set (this can be clearly observed in Figure 9 (left column)). Nevertheless, and although
it is computed over the noisy training set, gradient disparity reflects the performance on the test set
quite well (Figure 9 (middle left column)). As a result (see Table 2 (bottom)), gradient disparity
performs better than k-fold cross validation as an early stopping criterion. The same applies to two
other datasets (see Table 6 and Figure 9 in Appendix D).
Label noise	Method	Test loss I Test accuracy ∣ Top-5 test accuracy		
0%	5-fold CV	4.249 ± 0.028	6.79 ± 0.49	22.19 ± 0.77
	GD	4.057 ± 0.043	9.99 ± 0.92	27.84 ± 1.30
50%	10-fold CV	5.023 ± 0.083	1.59 ± 0.15	6.47 ± 0.52
	GD	4.463 ± 0.038	3.68 ± 0.52	15.22 ± 1.24
Table 2: The final test loss and accuracy when using gradient disparity (GD) and k-fold cross vali-
dation (CV) as early stopping criteria, (top) when the available dataset is limited and (bottom) when
the available data has noisy labeled samples. To simulate a limited-data scenario, we consider as a
training set a subset of 1280 samples of the CIFAR-100 dataset. The configurations in the top row
and the bottom row are ResNet-34 and ResNet-18, respectively. In both methods, the optimization
is stopped when the metric (validation loss or GD) increases for 5 epochs.
6 Validating Gradient Disparity as a Generalization Metric
(a) Test error
(b) Generalization error
Figure 2: The error percentage and D during training with different amounts of randomness in the
training labels for an AlexNet trained on a subset of 12.8 k points of the MNIST training dataset.
Pearson’s correlation coefficient between gradient disparity and test error (TE)/test loss (TL) over
all the iterations and over all levels of randomness are PDTE = 0.861 and PDTL = 0.802. The
generalization error (gap) is the difference between the train and test errors.
In this section, we demonstrate that factors that contribute to improve or degrade the generalization
performance of a model (e.g., label noise level, training set size and batch size), have an often
strikingly similar effect on the value of gradient disparity as well.
Label Noise Level. Deep neural networks, trained with the SGD algorithm, achieve excellent gen-
eralization performance (Hardt et al., 2015), while achieving zero training error on randomly labeled
data in classification tasks (Zhang et al., 2016). Understanding what distinguishes the model when
it is trained on correct labels and when it is trained on randomly labeled data is still an evolving area
of research. We conjecture that as label noise level increases, the gradient vectors diverge more.
Hence, when the network is trained with correctly labeled samples, the gradient disparity is low,
whereas when it is trained with corrupted samples the gradient disparity is high. The experimental
results support this conjecture in a wide range of settings and show that gradient disparity is indeed
very sensitive to label noise level (see also Figures 12, 15, 19 and 20).
6
Under review as a conference paper at ICLR 2021
Figure 2 shows the test error for networks trained with different amounts of label noise. Interestingly,
observe that for this setting the test error for the network trained with 75% label noise remains
relatively small, indicating the good resistance of the model against memorization of corrupted
samples. As suggested both from the test error (Figure 2 (a)) and the average gradient disparity
(Figure 2 (c)), there is no proper early stopping time for these experiments. The generalization error
(Figure 2 (b)) remains close to zero, regardless of the level of label noise, and hence fails to account
for label noise. In contrast, the average gradient disparity is very sensitive to the label noise level in
all stages of training as shown in Figure 2 (c), as desired for a metric measuring generalization.
Training Set Size. The test error decreases
with the size of the training set (Figure 3 (left))
and a reliable generalization metric should
therefore reflect this property. Many of the
previous metrics fail to do so, as shown by
(Neyshabur et al., 2017a; Nagarajan & Kolter,
2019). In contrast, the average gradient dispar-
ity indeed clearly decreases with the size of the
training set, as shown in Figure 3 (right) (see
also Figure 17 in Appendix E).
Batch Size. In practice, the test error increases
with the batch size (Figure 4 (left)). We observe
that gradient disparity also increases with the
batch size (Figure 4 (right)). This observation is
counter-intuitive because one might expect that
gradient vectors get more similar when they are
averaged over a larger batch. This might be the
explanation behind the decrease in gradient dis-
parity from batch size 256 to 512 for the VGG-
19 network. Observe also that gradient dispar-
ity correctly predicts that VGG-19 generalizes
better than ResNet-34 for this dataset. Gradient
disparity matches the ranking of test errors for
different networks, trained with different batch
sizes, as long as the batch sizes are not too large
(see also Figure 18 in Appendix E).
Width. In practice, the test error has been ob-
Figure 3: The test error (TE) and average gradient
disparity (D) for networks that are trained (until
reaching the training loss value of 0.01) over train-
ing sets with different sizes. We observe a very
strong positive correlation: PD TE = 0.984.
Batch size	Batch size
Figure 4: The test error and average gradient dis-
parity for networks that are trained with different
batch sizes. A ResNet-34 and a VGG-19 net-
work that are trained on the CIFAR-10 dataset.
The correlation between D and test error (TE)
for ResNet-34, VGG-19, and both graphs com-
bined are PD TE = 0.985, PD TE = 0.926, and
PDTE = 0.893, respectively.
served to decrease with the network width. We
observe that gradient disparity (normalized with respect to the number of parameters) also decreases
with network width for ResNet, VGG and fully connected neural networks that are trained on the
CIFAR-10 dataset (see Figure 14 in Appendix E.2).
7 Discussion and Related Work
Finding a practical metric that completely captures the generalization properties of deep neural net-
works, and in particular indicates the level of randomness in the labels and decreases with the size of
the training set, is still an active research direction (Dziugaite & Roy, 2017; Neyshabur et al., 2017a;
Nagarajan & Kolter, 2019). A very recent line of work assesses the similarity between the gradient
updates of two batches (samples) in the training set. The coherent gradient hypothesis (Chatterjee,
2020) states that the gradient is stronger in directions where similar examples exist and towards
which the parameter update is biased. He & Su (2020) presents the local elasticity phenomenon,
which measures how prediction over one sample changes, as the network is updated on another
sample. The generalization penalty introduced in our work measures how the prediction over one
sample (batch) changes when the network is updated on the same sample instead of being updated
on another sample, which can signal overfitting implicitly within the training set.
Tracking generalization by measuring the similarity between gradient vectors is particularly ben-
eficial as it is empirically tractable during training and does not require access to unseen data.
Sankararaman et al. (2019) proposes gradient confusion, which is a bound on the inner product
7
Under review as a conference paper at ICLR 2021
of two gradient vectors, and shows that the larger the gradient confusion is, the slower the conver-
gence takes place. Gradient interference (when the inner product of gradient vectors is negative)
has been studied in multi-task learning, reinforcement learning and temporal difference learning
(Riemer et al., 2018; Liu et al., 2019; Bengio et al., 2020). Yin et al. (2017) studies the relation
between gradient diversity, which measures the dissimilarity between gradient vectors, and the con-
vergence performance of distributed SGD algorithms. Fort et al. (2019) proposes a metric called
stiffness, which is the cosine similarity between two gradient vectors, and shows empirically that
it is related to generalization. Fu et al. (2020) studies the cosine similarity between two gradient
vectors for natural language processing tasks. Mehta et al. (2020) measures the alignment between
the gradient vectors within the same class (denoted by Ωc), and studies the relation between Ωc and
generalization as the scale of initialization is increased.
Another interesting line of work is the study of the variance of gradients in deep learning settings.
Negrea et al. (2019) derives mutual information generalization error bounds for stochastic gradient
Langevin dynamics (SGLD) as a function of the sum (over the iterations) of square gradient incoher-
ences, which is closely related to gradients variance. Two-sample gradient incoherences also appear
in Haghifam et al. (2020), there are taken between a training sample and a ”ghost” sample that is
not used during training and therefore taken from a validation set (unlike gradient disparity). The
upper bounds in Negrea et al. (2019); Haghifam et al. (2020) are not intended to be used as early
stopping criteria and are cumulative bounds that increase with the number of iterations. As shown
in Appendix G, gradient disparity can be used as an early stopping criterion not only for SGD with
additive noise (such as SGLD), but also other adaptive optimizers. Jastrzebski et al. (2020) studies
the effect of the learning rate on the variance of gradients and hypothesizes that gradient variance
counter-intuitively increases with the batch size, which is consistent with our observations. How-
ever, Qian & Klabjan (2020) shows that the variance of gradients is a decreasing function of the
batch size. Jastrzebski et al. (2020); Qian & Klabjan (2020) mention the connection between vari-
ance of gradients and generalization as promising future directions. Our study shows that variance
of gradients used as an early stopping criterion outperforms k-fold cross validation (see Table 8).
Mahsereci et al. (2017) proposes an early stopping criterion called evidence-based criterion (EB) that
eliminates the need for a held-out validation set, similarly to gradient disparity. The EB-criterion is
negatively related to the signal-to-noise ratio (SNR) of the gradient vectors. Liu et al. (2020) also
proposes a relation between gradient SNR (called GSNR) and the one-step generalization error, with
the assumption that both the training and the test sets are large, whereas gradient disparity targets
limited datasets. Nevertheless, we have compared gradient disparity to these metrics (namely, EB,
GSNR, gradient inner product, sign of the gradient inner product, variance of gradients, cosine
similarity, and Ωc) in Appendix H. In Table 8, We observe that gradient disparity and variance of
gradients used as early stopping criteria are the only metrics that consistently outperform k-fold
cross validation, and that are more informative of the level of label noise compared to other metrics.
We observe that the correlation between gradient disparity and the test loss is however in general
larger than the correlation between variance of gradients and the test loss (Table 9).
A common drawback of the metrics based on the similarity between two gradient vectors, including
gradient disparity, is that they are not informative when the gradient vectors are very small. In
practice however, we observe (see for instance Figure 13) that the time at which the test and training
losses start to diverge, which is the time when overfitting kicks in, does not only coincide with the
time at which gradient disparity increases, but also occurs much before the training loss becomes
infinitesimal. Hence, this drawback is unlikely to cause a problem for gradient disparity when it is
used as an early stopping criterion. Nevertheless, Theorem 1 trivially holds when the gradient values
are infinitesimal.
Conclusion. In this work, we propose gradient disparity, which is the `2 norm of the difference
between the gradient vectors of pairs of batches in the training set. Our empirical results on state-of-
the-art configurations show indeed a strong link between gradient disparity and generalization error.
Gradient disparity, similar to the test error, increases with the label noise level, decreases with the
size of the training set and increases with the batch size. We therefore suggest, particularly when the
available dataset is limited or noisy, gradient disparity as a promising early stopping criterion that
does not require access to a validation set.
8
Under review as a conference paper at ICLR 2021
References
I Bellido and Emile Fiesler. Do backpropagation trained neural networks have normal weight dis-
tributions? In International Conference on Artificial Neural Networks, pp. 772-775. Springer,
1993.
Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal
difference learning. arXiv preprint arXiv:2003.06350, 2020.
Nicholas Bien, Pranav Rajpurkar, Robyn L Ball, Jeremy Irvin, Allison Park, Erik Jones, Michael
Bereket, Bhavik N Patel, Kristen W Yeom, Katie Shpanskaya, et al. Deep-learning-assisted diag-
nosis for knee magnetic resonance imaging: development and retrospective validation of mrnet.
PLoS medicine, 15(11):e1002699, 2018.
Sat Chatterjee. Coherent gradients: An approach to understanding generalization in gradient
descent-based optimization. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=ryeFY0EFwS.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal Ofmachine learning research, 12(Jul):2121-2159, 2011.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, and Daniel M Roy. On the role of data in
pac-bayes bounds. arXiv preprint arXiv:2006.10929, 2020.
Stanislav Fort, PaWeI Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A
new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019.
Jinlan Fu, Pengfei Liu, Qi Zhang, and Xuanjing Huang. Rethinking generalization of neural models:
A named entity recognition case study. arXiv preprint arXiv:2001.03844, 2020.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu,
Xingxing Wang, Gang Wang, Jianfei Cai, et al. Recent advances in convolutional neural networks.
PatternRecognition, 77:354—377, 2018.
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. arXiv preprint arXiv:2004.12983, 2020.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Hangfeng He and Weijie Su. The local elasticity of neural networks. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
HJxMYANtPH.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-
entropy in classification tasks. arXiv preprint arXiv:2006.07322, 2020.
9
Under review as a conference paper at ICLR 2021
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural
networks. arXiv preprint arXiv:2002.09572, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Douglas M Kline and Victor L Berardi. Revisiting squared-error and cross-entropy functions for
training neural network classifiers. Neural Computing & Applications, 14(4):310-318, 2005.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stop-
ping is provably robust to label noise for overparameterized neural networks. arXiv preprint
arXiv:1903.11680, 2019.
Jinlong Liu, Yunzhi Bai, Guoqing Jiang, Ting Chen, and Huayan Wang. Understanding why neural
networks generalize well through gsnr of parameters. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=HyevIJStwH.
Vincent Liu, Hengshuai Yao, and Martha White. Toward understanding catastrophic interference
in value-based reinforcement learning. Optimization Foundations for Reinforcement Learning
Workshop at NeurIPS, 2019.
Maren Mahsereci, Lukas Balles, Christoph Lassner, and Philipp Hennig. Early stopping without a
validation set. arXiv preprint arXiv:1703.09580, 2017.
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel ma-
chines, pp. 203-215. Springer, 2003.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170, 1999a.
David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999b.
Harsh Mehta, Ashok Cutkosky, and Behnam Neyshabur. Extreme memorization via scale of initial-
ization. arXiv preprint arXiv:2008.13363, 2020.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, pp. 11611-11622,
2019.
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances
in Neural Information Processing Systems, pp. 11013-11023, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017b.
Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pp. 55-69.
Springer, 1998.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12
(1):145-151, 1999.
Xin Qian and Diego Klabjan. The impact of the mini-batch size on the variance of gradients in
stochastic gradient descent. arXiv preprint arXiv:2004.13146, 2020.
10
Under review as a conference paper at ICLR 2021
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. arXiv preprint arXiv:1810.11910, 2018.
Yuji Roh, Geon Heo, and Steven Euijong Whang. A survey on data collection for machine learning:
a big data-ai integration perspective. IEEE Transactions on Knowledge and Data Engineering,
2019.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
Karthik A Sankararaman, Soham De, Zheng Xu, W Ronny Huang, and Tom Goldstein. The impact
of neural network overparameterization on gradient confusion and stochastic gradient descent.
arXiv preprint arXiv:1904.06963, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the
Royal Statistical Society: Series B (Methodological), 36(2):111-133, 1974.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learn-
ing. Constructive Approximation, 26(2):289-315, 2007.
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter
Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. arXiv preprint
arXiv:1706.05699, 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
11
Under review as a conference paper at ICLR 2021
A Additional Theorem
Hoeffding’s bound is used in the proof of Theorem 1, and Lemma 1 is used in Section 4.
Theorem 2 (Hoeffding’s Bound). Let Zι,…，Zn be independent bounded random variables on
[a, b] (i.e., Zi ∈ [a, b] for all 1 ≤ i ≤ n with -∞ < a ≤ b < ∞). Then
P(1 XX (Zi- E[Zi]) ≥ t! ≤ exp(-(b2⅛)
and
P (1 X(Zi- E[zi]) ≤ -t! ≤ exp (-(b2n⅛)
for all t ≥ 0.
Lemma 11f N = N(μι, ∑ι) and N = N(μ2, ∑2) are two multivariate normal distributions in
Rd, where Σ1 and Σ2 are positive definite,
KL(N1||N2)
2 (tr (ς2 1,1)- d +(μ2 - μι)Tς2 1(μ2 - μι) + ln
det ∑2 "
det∑ι))
B Proof of Theorem 1
Proof. We compute the upper bound in Equation (3) using a similar approach as in McAllester
(2003). The main challenge in the proof is the definition of a function XS2 of the variables and
parameters of the problem, which can then be bounded using similar techniques as in McAllester
(2003). S1 is a batch of points (with size m1) that is randomly drawn from the available set S
at the beginning of iteration t, and S2 is a batch of points (with size m2) that is randomly drawn
from the remaining set S \ S1. Hence, S1 and S2 are drawn from the set S without replacement
(Si ∩ S2 = 0). Similar to the setting of Negrea et al. (2019); Dziugaite et al. (2020), as the random
selection of indices ofS1 and S2 is independent from the dataset S, σ(S1) ⊥⊥ σ(S2), and as a result,
G1 ⊥⊥ σ(S2) and G2 ⊥⊥ σ(S1). Recall that νi is the random parameter vector at the end of iteration
t that depends on Si, for i ∈ {1, 2}. For a given sample set Si, denote the conditional probability
distribution of νi by QSi. For ease of notation, we represent QSi by Qi.
Let us denote
and
∆(hν1,hν2), (LS2(hν1)-L(hν1))-(LS2(hν2)-L(hν2)),
XS2 , sup
Q1,Q2
(2 - 1) EVI 〜Qi [EV2 〜Q2 (∆ (hν1, hν2))2	-KL(Q2||Q1).
(7)
(8)
Note that XS2 is a random function of the batch S2 . Expanding the KL-divergence, we find that
(2 - 1) EVI 〜Qi [EV2 〜Q2 (∆ (hνi, hν2))2	-KL(Q2||Q1)
2
ln QI(V2)]
- Q2(ν2)[
ln Eν2 〜Q2
e(等-1)(∆(hνι ,hν2 ))2 QI(V2I
.	Q2(V2)∖∖
2
ln Eν0 〜Qi
where the inequality above follows from Jensen’s inequality as logarithm is a concave function.
Therefore, again by applying Jensen’s inequality
Xs2 ≤ ln Evi 〜Qi Ev0 〜Qi
e(等-1)3(hvi ,hν0))
12
Under review as a conference paper at ICLR 2021
Taking expectations over S2, we have that
Es2 [eXS2] ≤ Es2Eν1 〜QiEν0〜Qi e(等T)3hν1 ,hν1 ))2
=Evi〜QiEν0〜QiEs2 e(詈τ)3hνι,hν1 ))2 ,	(9)
where the change of order in the expectation follows from the independence of the draw of the set
S2 from νι 〜 Qi and ν1 〜 Qi, i.e., Qi is Gi-measurable and Gi ⊥⊥ σ(S2).
Now let
Zi , l(hνi (xi), yi) - l(hνi0 (xi), yi),
for all 1 ≤ i ≤ m2 . Clearly, Zi ∈ [-1, 1] and because of Equations (1) and of the definition of ∆ in
Equation (7),
1 m2
δ (hvi, hv0 ) = --- X (Zi- E[Zi]).
m2 i=i
Hoeffding’s bound (Theorem 2) implies therefore that for any t ≥ 0,
Ps2 (∣∆ (hvi,hv0)| ≥ t) ≤ 2e-詈t2.
(10)
Denoting by p(∆) the probability density function of ∣∆ (h“i ,hv0) |, inequality (10) implies that for
any t ≥ 0,
∞
/ p(∆)d∆ ≤ 2e-等t2
(11)
The density p(∆) that maximizes R∞ e(仔-i)δ2p(∆)d∆ (the term in the first expectation
of the upper bound of Equation (9)), is the density achieving equality in (11), which is
p(∆) = 2m2∆e--22δ . As a result,
iZ∞∞
e( ɪ-iZ 2m2 ∆e-ɪ δ d∆ = /	ZmzAe-^ d∆ = m?
and consequently, inequality (9) becomes
ES2 [eXS2 ] ≤ m2 .
Applying Markov’s inequality on XS2, we have therefore that for any 0 < δ ≤ 1,
2m2	2m2	δ	δ
PS2 [XS2 ≥ ln — J = PS2 [e 2 ≥ — J ≤ 而 ES2 [e 2] ≤ 2.
Replacing XS2 by its expression defined in Equation (8), the previous inequality shows that with
probability at least 1 - δ∕2
(2 - 1) EVI 〜Qi Ev2〜Q2 h(∆(hvi,hv2))2i - KL(QzIIQi) ≤ ln2mm2.
Using Jensen’s inequality and the convexity of (∆(hνi , hν2))z, and assuming that mz > 2, we
therefore have that with probability at least 1 - δ∕2,
(Evi 〜Qi Ev2 〜Q2∆(hvi ,hv2 )])2 ≤ Evi 〜Qi 旧“2 〜Q2
h(∆ (hνi , hν2 )) i ≤
KL(Q2∣∣Qi)+ln 苧
m — 1
2 ɪ
Replacing ∆(hvi , hv2) by its expression Equation (7) in the above inequality, yields that with prob-
ability at least 1 - δ∕2 over the choice of the sample set S2,
EVI 〜Qi [LS2 (hvi) - L(hvi)] ≤ EV2 〜Q2 [LS2 (hv2 ) - L(hv2 )] +
S2kl(Q2 ||Qi) + 2ln 2mm2
m	m2 - 2
(12)
13
Under review as a conference paper at ICLR 2021
Similar computations with S1 and S2 switched, and considering that m1 > 2, yields that with
probability at least 1 - δ∕2 over the choice of the sample set Si,
/2KL(QI ∣∣Q2) + 2ln 2mmi
m	mi - 2
EV2~Q2 [LSι (hV2 ) - L(hν2 )] ≤ Eνι~Qι [LSι (hVI)- L(AVI)] +
(13)
The events in Equations (12) and (13) jointly hold with probability at least 1 - δ over the choice
of the sample sets S1 and S2 (using the union bound and De Morgan’s law), and by adding the two
inequalities we therefore have
Eνι~Qι [LS2 (hνι)]+ EV2~Q2 [LSi (hV2 )] ≤ 限~电2 [LS2(hV2 )]+ Evi~Qi [LSi (hνJ
+
S2KL(Q2 ∣∣QI) + 2ln 2m2^
V	m2 - 2
S2KL(QIIIQ2)+ 2ln 2m1^
m	mi - 2
which concludes the proof.
□
C	Common Experimental Details
The training objective in our experiments is to minimize the cross-entropy loss, and both the cross
entropy and the error percentage are displayed. The training error is computed using Equation (1)
over the training set. The empirical test error also follows Equation (1) but it is computed over the
test set. The generalization loss (respectively, error) is the difference between the test and the training
cross entropy losses (resp., classification errors). The batch size in our experiments is 128 unless
otherwise stated, the SGD learning rate is γ = 0.01 and no momentum is used (unless otherwise
stated). All the experiments took at most few hours on one Nvidia Titan X Maxwell GPU. All the
reported values throughout the paper are an average over at least 5 runs.
To present results throughout the training, in the x-axis of figures, both epoch and iteration are used:
an epoch is the time spent to pass through the entire dataset, and an iteration is the time spent to
pass through one batch of the dataset. Thus, each epoch has B iterations, where B is the number of
batches. The convolutional neural network configurations we use are: AlexNet (Krizhevsky et al.,
2012), VGG (Simonyan & Zisserman, 2014) and ResNet (He et al., 2016). In those experiments
with varying width, we use a scaling factor to change both the number of channels and the number
of hidden units in convolutional and fully connected layers, respectively. The default configuration
is with scaling factor = 1.
In experiments with a random labeled training set, we modify the dataset similar to Chatterjee
(2020). For a fraction of the training samples, which is the amount of noise (0%, 25%, 50%, 75%,
100%), we choose the labels at random. For a classification dataset with a number k of classes, if
the label noise is 25%, then on average 75% + 25% * 1/k ofthe training points still have the correct
label.
C.1 Re-scaling the Loss
Let us track the evolution of gradient disparity (Equation (6)) during training. As training progresses,
the training losses of all the batches start to decrease when they get selected for the parameter up-
date. Therefore, the value of gradient disparity might decrease, not necessarily because the distance
between the two gradient vectors is decreasing, but because the value of each gradient vector is itself
decreasing. To avoid this, a re-scaling or normalization of the loss is needed to compare gradient
disparity at different stages of training. Note that this re-scaling or normalization does not affect the
training algorithm, only the computation of gradient disparity.
14
Under review as a conference paper at ICLR 2021
We perform both re-scaling and normalization.
The re-scaling of the loss values is given by
mj
L = ɪ X	Ii
Sj	mj i=1 Stdi (Ii),
where with some abuse of notation, li is the
cross entropy loss for the data point i in the
batch Sj . The normalization of the loss values
is given by
L =	1	Xj Ii	- Mini (Ii)
Sj	mj	i=1	MaXi	(Ii) - Mini	(li).
Figure 5: Normalizing versus re-scaling loss be-
fore computing average gradient disparity D for a
VGG-11 trained on 12.8 k points of the CIFAR-10
dataset.
We experimentally compare these two ways of
computing gradient disparity in Figure 5. Both
the re-scaled and normalized losses, might get
unbounded if within each batch the loss values are very close to each other. However, in our eXper-
iments, we do not observe gradient disparity becoming unbounded either way. In the presence of
outliers, re-scaling is more reliable than normalizing, because with normalization non-outlier data
might end up in a very small interval between 0 and 1. This might eXplain the mismatch between the
normalized gradient disparity and generalization loss at the end of training in Figure 5. Therefore,
in all eXperiments presented in the paper, we re-scale the loss values before computing the gradient
disparity3.
C.2 THE HYPER-PARAMETER s
In this section, we briefly study the choice of
the size s of the subset of batches to compute
the average gradient disparity
1
S(S - 1)
D =
ss
X X	Di,j
i=1 j=1,j 6=i
Figure 6 shows the average gradient dispar-
ity when averaged over S number of batches4.
When S = 2, gradient disparity is the `2 norm
distance of the gradients of two randomly se-
lected batches and has a quite high variance.
Although with higher values of S the results
have lower variance, choosing a higher S is
more computationally eXpensive (refer to Ap-
pendiX D for more details). Therefore, we find
Figure 6: Average gradient disparity for differ-
ent averaging parameter S for a ResNet-18 that
has been trained on 12.8k points of the CIFAR-10
dataset.
the choice ofS = 5 to be sufficient enough to track down overfitting; in all the presented eXperiments
in this paper, we use S = 5.
3Note that in Figure 5, both the gradient disparity and the generalization loss are increasing from the very
first epoch. If we would use gradient disparity as an early stopping criterion, optimization would stop at epoch
5 and we would have a 0.36 drop in the test loss value, compared to the loss reached when the model achieves
0 training loss.
4In the setting of Figure 6, if using gradient disparity as an early stopping criterion, optimization would stop
at epoch 9 and we would have a 0.28 drop in the test loss value compared to the loss reached when the model
achieves 0 training loss.
15
Under review as a conference paper at ICLR 2021
C.3 The Surrogate Loss Function
It has been shown that cross entropy is better suited for computer-vision classification tasks com-
pared to the mean square error (Kline & Berardi, 2005; Hui & Belkin, 2020). Hence, we choose the
cross entropy criterion for all our experiments to avoid possible pitfalls of the mean square error,
such as not tracking the confidence of the predictor.
Soudry et al. (2018) argues that when using cross entropy, as training proceeds, the magnitude of the
network parameters increases. This can potentially affect the value of gradient disparity. Therefore,
we compute the magnitude of the network parameters over iterations in various settings. We observe
that this increase is very low both at the end of the training and, more importantly, at the time when
gradient disparity signals overfitting (denoted by GD epoch in Table 3). Therefore, it is unlikely that
the increase in the magnitude of the network parameters affects the value of gradient disparity.
Furthermore, we examine gradient disparity for models trained on the mean square error, instead
of the cross entropy criterion. We observe a high correlation between gradient disparity and test
error/loss (Figure 7), which is consistent with the results obtained using the cross entropy criterion.
Therefore, the applicability of gradient disparity as a generalization metric is not limited to settings
with the cross entropy criterion.
Setting	at epoch 0	I at GD epoch	at epoch 200
AlexNet, MNIST	∣	1	I 1.00034	I 1.00123
AlexNet, MNIST, 50% random ∣	1	I 1.00019	I 1.00980
VGG-16, CIFAR-10	∣	1	I 1.00107	I 1.00127
VGG-16, CIFAR-10, 50% random ∣	1	I 1.00222	"∣^^1.00233
Table 3: The ratio of the magnitude of the network parameter vector at epoch t to the magnitude
of the network parameter vector at epoch 0, for t ∈ {0, GD, 200}, where GD stands for the epoch
when gradient disparity signals to stop the training.
Figure 7: Test error (TE), test loss (TL), and gradient disparity (D) for VGG-16 trained with different
training set sizes to minimize the mean square error criterion on the CIFAR-10 dataset. The PearSon
correlation coefficient between TE and D and between TL and D are PDTE = 0.976 and PD TL =
0.943, respectively.
16
Under review as a conference paper at ICLR 2021
D k-FOLD CROSS VALIDATION
k-fold cross validation is done by splitting the available dataset into k sets, training on k - 1 of
them and validating on the remaining one. This is repeated k times so that every set is used once
as the validation set. We can adopt two different early stopping approaches: stop the optimization
either (i) when the validation loss (respectively, gradient disparity) has increased for m = 5 epochs
from the beginning of training (which is marked by the gray vertical bar in Figures 8 and 9), or (ii)
when the validation loss (resp., gradient disparity) has increased for 5 consecutive epochs (which is
indicated by the magenta vertical bar in Figures 8 and 9). When there is low variations and a sharp
increase in the value of the metric, the two coincide (for instance, Figure 8 (b) (middle left)). In
our experiments, we observe that gradient disparity appears to be less sensitive to the choice of the
approaches (i) and (ii) compared to k-fold cross validation. Moreover, in Table 5 we study different
values of m, which is usually referred to as patience among practitioners.
Early stopping should optimally occur when there is a minimum valley throughout the training in the
test loss/error curves, or when the generalization loss/error starts to increase. In those experiments
where such a minimum in the test loss curve exists, we compare gradient disparity to the test loss.
Otherwise, we compare gradient disparity to the generalization loss. For k-fold cross validation,
we compare validation loss to the test loss, because validation loss is expected to predict the test
loss. Note that in the experiments with noisy labeled data, all the available data contains corrupted
samples, hence both validation loss and gradient disparity are computed over sets which may contain
corrupted samples.
Limited Data. We present the results for limited data scenario in Figure 8 and Table 4 for MNIST,
CIFAR-10 and CIFAR-100 datasets, where we simulate the limited data scenario by using a small
subset of the training set. For the CIFAR-100 experiment (Figure 8 (a) and Table 4 (top row), we
observe (from the left figure) that validation loss predicts the test loss pretty well. We observe (from
the middle left figure) that gradient disparity also predicts the test loss quite well. However, the main
difference between the two settings is that when using cross validation, 1/k of the data is set aside
for validation and 1 - 1/k of the data is used for training. Whereas when using gradient disparity,
all the data (1 - 1/k + 1/k) is used for training. Hence, the test loss in the leftmost and middle
left figures differ. The difference between the test accuracy (respectively, test loss) obtained in each
setting is visible in the rightmost figure (resp., middle right figure). We observe that there is over 3%
improvement in the test accuracy when using gradient disparity as an early stopping criterion. This
improvement is consistent for MNIST and CIFAR-10 datasets (Figures 8 (b) and (c) and Table 4).
We conclude that in the absence of label noise, both k-fold cross validation and gradient disparity
predict the optimal early stopping moment, but the final test loss/error is much lower for the model
trained with all the available data (as when gradient disparity is used), than the model trained with a
(1 - 1/k) portion of the data (as in k-fold cross validation). To further test on a dataset that is itself
limited, a medical application with limited labeled data is empirically studied later in this section
(Appendix D.1). The same conclusion is made for this dataset.
Noisy Labeled Data. The results for datasets with noisy labels are presented in Figure 9 and Table 6
for MNIST, CIFAR-10 and CIFAR-100 datasets. We observe (from Figure 9 (a) (left)) that for
the CIFAR-100 experiment, the validation loss does no longer predict the test loss. Nevertheless,
although gradient disparity is computed on a training set that contains corrupted samples, it predicts
the test loss quite well (Figure 9 (a) (middle left)). As a result, there is a 2% improvement in the final
test accuracy (for top-5 accuracy there is a 9% improvement) (Table 6 (top two rows)) when using
gradient disparity instead of a validation set as an early stopping criterion. This is also consistent
for other configurations and datasets (Figure 9 and Table 6). We conclude that, in the presence of
label noise, k-fold cross validation does no longer predict the test loss and fails as an early stopping
criterion, unlike gradient disparity.
Computational Cost. Denote the time, in seconds, to compute one gradient vector, to compute the
`2 norm between two gradient vectors, to take the update step for the network parameters, and to
evaluate one batch (find its validation loss and error) by t1, t2, t3 and t4, respectively. Then, one
epoch of k-fold cross validation takes
CVepoch = k × ( -k-B(t1 + t3) + "k"t4
17
Under review as a conference paper at ICLR 2021
seconds, where B is the number of batches. Performing one epoch of training the network and
computing the gradient disparity takes
GDePoCh = B (t1 + t3) + S (t1 +	2 t2)
seconds. In our experiments, we observe that t1 ≈ 5.1t2 ≈ 100t3 ≈ 3.4t4, hence the approximate
time to Perform one ePoCh for eaCh setting is
CVePoCh ≈ (k - 1)Bt1,	and	GDePoCh ≈ (B + s)t1.
Therefore, as s < B , we have CVePoCh	GDePoCh.
(a) ResNet-34 trained on 1.28 k Points of CIFAR-100
(b) VGG-13 trained on 1.28 k Points of CIFAR-10
(C) AlexNet trained on 256 Points of MNIST
Figure 8: ComParing 5-fold Cross validation (CV) with gradient disParity (GD) as an early stoPPing
Criterion when the available dataset is limited. (left) Validation loss versus test loss in 5-fold Cross
validation. (middle left) Gradient disParity versus test and generalization losses. (middle right
and right) PerformanCe on the unseen (test) data for GD versus 5-fold CV. (a) The Parameters are
initialized by Xavier teChniques with uniform distribution. (b, C) The Parameters are initialized using
He teChnique with normal distribution. (C) The batCh size is 32. The gray and magenta vertiCal bars
indiCate the ePoCh in whiCh the metriC (the validation loss or gradient disParity) has inCreased for 5
ePoChs from the beginning of training and for 5 ConseCutive ePoChs, resPeCtively. In (b) the middle
left figure, these two bars meet eaCh other.
D.1 MRNet Dataset
So far, we have Presented the imProvement of gradient disParity over Cross validation for limited
subsets of MNIST, CIFAR-10 and CIFAR-100 datasets. In this sub-seCtion, we Present the results
for when the available dataset is by itself limited. We Consider the MRNet dataset Bien et al. (2018)
for diagnosis of knee injuries. The dataset Contains 1370 magnetiC resonanCe imaging (MRI) exams
to study the PresenCe of abnormality, anterior CruCiate ligament (ACL) tears and menisCal tears. The
labeled data in MRNet dataset is therefore very limited. EaCh MRI sCan is a set of S sliCes of images
staCked together. EaCh Patient (Case) has three MRI sCans: sagittal, Coronal and axial. The MRNet
dataset is sPlit into training (1130 Cases), validation (120 Cases) and test sets (120 Cases). The test set
is not PubliCly available. We need however to set aside some data to evaluate both gradient disParity
and k-fold Cross validation, henCe, in our exPeriments, the validation set beComes the unseen (test)
18
Under review as a conference paper at ICLR 2021
Setting	Method	Test loss	Test accuracy
CIFAR-100, ResNet-34	5-fold CV	4.249 ± 0.028	6.79 ± 0.49 (top-5: 22.19 ± 0.77)
	GD	4.057 ± 0.043	9.99 ± 0.92 (top-5: 27.84 ± 1.30)
CIFAR-10, VGG-13	5-fold CV GD	1.846 ± 0.016 1.793 ± 0.016	35.982 ± 0.393 36.96 ± 0.861
MNIST, AlexNet	5-fold CV	1.123 ± 0.25	62.62 ± 6.36
	GD	0.656 ± 0.080	79.12 ± 3.04
Table 4: The loss and accuracy on the test set comparing 5-fold cross validation and gradient dispar-
ity as early stopping criterion when the available dataset is limited. The corresponding curves during
training are presented in Figure 8. The above results are obtained by stopping the optimization when
the metric (either validation loss or gradient disparity) has been increased for five epochs from the
beginning of training.
JPatience Method^^^^^	1	5	10	15	20	25
5-fold CV	41.15 ± 5.68	62.62 ± 6.36	81.39 ± 3.64	80.39± 2.88	84.84 ± 2.53	83.55 ± 2.84
GD	30.19 ± 6.21	79.12 ± 3.04	84.82 ± 2.14	85.35± 2.09	87.28 ± 1.24	86.69 ± 1.31
(a) MNIST, AlexNet, limited dataset (Figure 8 (c))
' JPatience Method^^^^^	1	5	10	15	20	25
10-fold CV	96.54 ± 0.15	97.28± 0.20	97.35 ± 0.23	97.22± 0.19	96.60 ± 0.33	94.69 ± 0.87
GD	97.07 ± 0.16	97.32 ± 0.15	97.41 ± 0.15	96.57± 0.64	95.44 ± 0.96	92.58 ± 0.65
(b) MNIST, AlexNet, noisy dataset (Figure 9 (d))
Table 5: The test accuracies achieved by using k-fold cross validation (CV) and by using gradient
disparity (GD) as early stopping criteria for different patience values. For a given patience value of
m, the training is stopped after m increases in the value of the validation loss in k-fold CV (top rows)
and of GD (bottom rows). Throughout the paper, we have chosen m = 5 as the default patience
value for all methods without optimizing it even for GD. However, in this Table, we observe that
even if we tune the patience value for k-fold CV and for GD separately (which is indicated in bold),
GD still outperforms k-fold CV.
set. To perform cross validation, we split the set used for training in Bien et al. (2018) into a first
subset used for training in our experiments, and a second subset used as validation set. Note that, in
this dataset, because slice S changes from one case to another, it is not possible to stack the data into
batches, hence the batch size is 1, which may explain the fluctuations of validation loss and gradient
disparity in this setting. We used the SGD optimizer with the learning rate 10-4 for training the
model. Each task in this dataset is a binary classification with unbalanced set of samples and hence
we report the area under the curve of the receiver operating characteristic (AUC score).
The results for three tasks: detecting ACL tears, meniscal tears and abnormality, are shown in
Figure 10 and Table 7. We can observe that both the validation loss, despite a small bias, and
the gradient disparity predict the generalization loss quite well. Yet, when using gradient disparity,
the final test AUC score is higher (Figure 10 (right)). As mentioned earlier, for this dataset, both
the validation loss and gradient disparity vary a lot. Hence, in Table 7, we present the results of
early stopping, both when the metric has increased for 5 epochs from the beginning of training, and
(in parenthesis) when the metric has increased for 5 consecutive epochs. We conclude that with
both approaches, the use of gradient disparity as an early stopping criterion results in more than 1%
improvement in the test AUC score. Because the test set used in Bien et al. (2018) is not publicly
available, it is not possible to compare our predictive results with Bien et al. (2018). Nevertheless,
a baseline may be the results presented in https://github.com/ahmedbesbes/mrnet,
which report a test AUC score of 88.5% for the task of detecting ACL tears, whereas we observe
in Table 7 that stopping training after 5 consecutive increases in gradient disparity leads to 91.52%
19
Under review as a conference paper at ICLR 2021
Setting	Method	Test loss	Test accuracy
	10-fold CV	5.023 ± 0.083	1.59 ± 0.15 (top-5: 6.47 ± 0.52)
CIFAR-100, ResNet-18	GD	4.463 ± 0.038	3.68 ± 0.52 (top-5: 15.22 ± 1.24)
	Plug-in	4.964 ± 0.057	1.68 ± 0.24 (top-5: 7.05 ± 0.71)
	10-fold CV	4.062 ± 0.091	9.62 ± 1.08 (top-5: 32.06 ± 1.47)
CIFAR-100, ResNet-34	GD	4.592 ± 0.179	10.41 ± 1.40 (top-5: 36.92 ± 1.20)
	Plug-in	4.134± 0.185	10.11 ± 1.60 (top-5: 34.19 ± 2.10)
	10-fold CV	2.126 ± 0.063	34.88 ± 1.66
CIFAR-10, VGG-13	GD	2.519 ± 0.062	36.98 ± 0.77
	Plug-in	2.195 ± 0.142	35.40 ± 3.00
	10-fold CV	0.656 ± 0.034	97.28 ± 0.20
MNIST, AlexNet	GD	0.654 ± 0.031	97.32 ± 0.27
	Plug-in	0.639 ± 0.029	97.31 ± 0.15
Table 6: The loss and accuracy on the test set comparing 10-fold cross validation and gradient dis-
parity as early stopping criterion when the available dataset is noisy. In all the experiments, 50%
of the available data has random labels. The corresponding curves during training are presented in
Figure 9. The above results are obtained by stopping the optimization when the metric (either vali-
dation loss or gradient disparity) has been increased for five epochs from the beginning of training.
The last row in each setting, which we call plug-in, refers to when we plug-in the epoch suggested
by 10-fold CV and then report the test loss and accuracy at that epoch for a network trained on the
entire set. In all these settings, using GD would still result in a higher test accuracy and therefore
the advantage of GD over 10-fold CV is a better characterization of overfitting.
test AUC score for this task. With further tuning, and combining the predictions found on two other
MRI planes of each patient (axial and coronal), our final prediction results could even be improved.
Task	Method	Test loss	Test AUC score
ACL	5-fold CV GD	0.973 ± 0.111 (1.246 ± 0.142) 0.842 ± 0.101 (1.136 ± 0.121)	79.80 ± 1.23 (89.32 ± 1.47) 81.81 ± 1.64 (91.52 ± 0.09)
meniscal	5-fold CV GD	0.758 ± 0.04(1.163 ± 0.127) 0.726 ± 0.019 (1.14 ± 0.323)	73.53 ± 1.30 (72.14 ± 0.74) 74.08 ± 0.79 (73.80 ± 0.24)
abnormal	5-fold CV GD	0.284 ± 0.016 (0.307 ± 0.057) 0.274 ± 0.004 (0.275 ± 0.053)	71.016 ± 3.66 (87.44 ± 1.35) 72.67 ± 3.85 (88.12 ± 0.35)
Table 7: The loss and AUC score on the test set, comparing 5-fold cross validation to gradient
disparity both as early stopping criterion for the MRNet dataset for three different tasks using the
sagittal plane MRI scans. Note that an unassisted general radiologist gives on average 92%, 84%
and 89% accuracy for detecting ACL tears, meniscal tears and abnormality, respectively (Bien et al.,
2018). The corresponding curves during training are presented in Figure 10. We present the results
of early stopping, both when the metric has increased for 5 epochs from the beginning of training,
and in parenthesis when the metric has increased for 5 consecutive epochs.
20
Under review as a conference paper at ICLR 2021
(a) ResNet-18 trained on 1.28 k points of CIFAR-100 dataset with 50% label noise
(b) ResNet-34 trained on the entire CIFAR-100 dataset with 50% label noise
(c) VGG-13 trained on the entire CIFAR-10 dataset with 50% label noise
(d) AlexNet trained on the entire MNIST dataset with 50% label noise
Figure 9:	Comparing 10-fold cross validation with gradient disparity as early stopping criteria when
the available dataset is noisy. (left) Validation loss versus test loss in 10-fold cross validation. (mid-
dle left) Gradient disparity versus test and generalization losses. (middle right and right) Perfor-
mance on the unseen (test) data for GD versus 10-fold CV. (a) The parameters are initialized by
Xavier techniques with uniform distribution. (b, c, and d) The parameters are initialized using He
technique with normal distribution.
21
Under review as a conference paper at ICLR 2021
2.0
1.5
料
0.5
0.0
10°	101	IO2
epochs
generalization loss GD
—gradient disparity
----AUC GD
"-AUC 5-fold
I
10°	101	IO2
epochs
10°	101	IO2
epochs
(a) Task: detecting ACL tears
generalization loss GD
——gradient disparity
5 0 5 0 5 0
7 5 2 0 7 5
2 7 7 7 6 6
Qo.o.o.o.<5
①」。US UD<+JSQL
AZHJedS-P-MU ①一 pe」。
10°	101	102
epochs
ιo0	ιo1	ιo2
epochs
IO0	101	102
epochs
(b) Task: detecting meniscal tears
(c) Task: detecting abnormality
AUCGD
IO0	IO1	IO2
epochs

Figure 10:	Detecting three tasks from the MRNet dataset from the sagittal plane MRI scans. (left)
Validation loss versus test loss in 5-fold cross validation. (middle) Gradient disparity versus gener-
alization loss. (right) Performance comparison on the final unseen data when applying 5-fold CV
versus gradient disparity.
22
Under review as a conference paper at ICLR 2021
E Additional Experiments
Iterations
(a) Cross entropy loss
Figure 11: The cross entropy loss, the error percentage and the average gradient disparity during
training. (a-c): A ResNet-18 trained on a subset of 12.8 k points of the CIFAR-10 training set
(the parameter initialization is Xavier (Glorot & Bengio, 2010)). PearSon's correlation coefficient
P between D and generalization loss/error over all the training iterations are PDgenloss = 0.755
and PDgenerrOr = 0.846. (d-f): An AlexNet trained on a subset of 12.8 k points of the MNIST
training set (the parameters are initialized according to the He (He et al., 2015) method with Normal
distributions). For this experiment, PDgenloss = 0.465 and PDgenerrOr = 0.457. The blue, orange,
green, and red CurveS are the test loss/error, train loss/error, generalization loss/error, and the average
gradient disparity D, respectively.
(f) D vs. error
_O OOoOC
5 4 3 2 1
①6e4u①U」①d」。」」山
3μfDdMP4-lu ①-pe」u
OOMOOo
6 LP 4 3 2 1
----generalization loss
test loss
gradient disparity
(e) D vs. loss
(d) Cross entropy loss
To investigate the relation between the average gradient disparity D and generalization, We compare
two sets of experiments. The first one exhibits clear overfitting, whereas in the second one, the
model generalizes quite well. To test gradient disparity as a generalization metric, not only should
it have very different values for each of these two sets of experiments, but it should also be well
aligned with the generalization error. This is indeed what the experiments show.
In the first set of experiments, the network configuration is ResNet-18 (He et al., 2016) and it is
trained on the CIFAR-10 dataset (Figure 11 (top)). Around iteration 500 (which is indicated by a
thick gray vertical bar in the figures), the training and test losses (and errors) start to diverge, and
the test loss reaches its minimum. This should be the early stopping point as the model is starting to
overfit. Interestingly, around the same time (indicated in Figures 11 (b) and (c)), We observe a sharp
increase in D.
The second set of experiments is on an AlexNet (Krizhevsky et al., 2012) trained on the MNIST
dataset (Figure 11 (bottom)). This model generalizes quite well for this dataset. We observe that,
throughout the training, the test curves are even below the training curves, which is due to the
dropout regularization technique (Srivastava et al., 2014) being applied during training and not dur-
ing testing. The generalization loss/error is almost zero, until around iteration 1100 (indicated in the
figure by the gray vertical bar), which is when overfitting starts and the generalization error becomes
non-zero. At approximately the same time, the average gradient disparity (Figures 11 (e) and (f))
starts to slightly increase, but much more slowly compared to Figures 11 (b) and (c). In both these
experiments, we observe that as overfitting starts, the gradient vectors start to diverge, resulting in
larger gradient disparity. These observations suggest gradient disparity as an effective early stopping
criterion, as it is well aligned with the generalization error.
23
Under review as a conference paper at ICLR 2021
E.1 MNIST Experiments
Figure 12 shows the results for a 4-layer fully connected neural network trained on the entire MNIST
training set5. Figures 12 (e) and (f) show the generalization losses. We observe that at the early
stages of training, generalization losses do not distinguish between different label noise levels,
whereas gradient disparity (Figures 12 (g) and (h)) does so from the beginning. At the middle
stages of training we can observe that, surprisingly in this setting, the network with 0% label noise
has higher generalization loss than the networks trained with 25%, 50% and 75%, and this is also
captured by average gradient disparity. The final gradient disparity values for the networks trained
with higher label noise level are also larger. For the network trained with 0% label noise we present
the results with more detail in Figure 13 and observe again how gradient disparity is well aligned
with the generalization loss/error. In this experiment, the early stopping time suggested by gradient
disparity is epoch 9, which is the exact same time when the training and test losses/errors start to
diverge, which signals therefore the start of overfitting.
(c) Training error	(d) Test error
(e) Generalization loss
(a) Training loss	(b) Test loss
ss0uo⅛ON=BJ^O①。O
Ooooooo
2 0 8 6 4 2
1 1
3μeds-p-l-juə - P① ① 2
IO0	IO1	IO2
Epochs
(g) D
(f) Generalization loss for epoch < 80
Figure 12: The cross entropy loss, error percentage, and average gradient disparity during training
with different amounts of randomness in the training labels for a 4-layer fully connected neural
network with 500 hidden units trained on the entire MNIST dataset. The parameter initialization is
the He initialization with normal distribution.
IO1
Epochs
(h) D for epoch < 80
5http://yann.lecun.com/exdb/mnist/
24
Under review as a conference paper at ICLR 2021
train loss
test loss
generalization loss
gradient disparity
3μ(Ddup+JU ①一 pωJ6 ① ① ><
o∙ 5 - O - 5 O
3 2 2 1 1 5
(a) Loss vs gradient disparity	(b) Error vs gradient disparity
Figure 13: The cross entropy loss, error percentage, and average gradient disparity during training
for a 4-layer fully connected neural network with 500 hidden units trained on the entire MNIST
dataset with 0% label noise. The parameter initialization is the He initialization with normal dis-
tribution. Pearson's correlation coefficient P between D and generalization loss/error over all the
training iterations are PDgenloss = 0.967 and PDgenerrOr = 0.734. The gray vertical bar indicates
when GD increases for 5 epochs from the beginning of training. The magenta vertical bar indicates
when GD increases for 5 consecutive epochs. We observe that the gray bar signals when overfitting
is starting, which is when the training and testing curves are starting to diverge. The magenta bar
would be a good stopping time, because ifwe train beyond this point, although the test error remains
the same, the test loss would increase, which would result in overconfidence on wrong predictions.
E.2 CIFAR-10 Experiments
Width. To compare models with a different number of parameters using gradient disparity, we
need to normalize it. The dimension of a gradient vector is the number d of parameters of the
model. Gradient disparity being the '2-norm of the difference of gradient vectors will thus grow
proportionally to drd, hence to compare different architectures, We propose to use the normalized
gradient disparity D = D/ Vd. We observe in Figure 14 that both the normalized6 gradient disparity
and test error decrease with the network width (the scale is a hyper-parameter used to change both
the number of channels and hidden units in each configuration).
6Note that the normalization with respect to the number of parameters is different than the normalization
mentioned in Section C.1 which was with respect to the loss values. The value of gradient disparity reported
everywhere is the re-scaled gradient disparity; further if comparison between two different architectures is
taking place the normalization with respect to dimensionality will also take place.
25
Under review as a conference paper at ICLR 2021
(a) CNN; (left): PD,TL = 0.970, PD,TE = 0.939, (right): PD,TL = 0.655, PD,TE = 0.958
(b) FCNN; pD,tl = 0.771, pD,te = 0.601
Figure 14: Test error and normalized gradient disparity for networks trained on the CIFAR-10
dataset with different number of channels and hidden units for convolutional neural networks (CNN)
and fully connected neural networks (FCNN). The correlation between normalized gradient dispar-
ity and test loss PD TL and between normalized gradient disparity and test error PD TE are reported
in the captions.
26
Under review as a conference paper at ICLR 2021
Figure 15 shows the results for a 4-layer fully connected neural network, which is trained on the
entire CIFAR-10 training set7. We observe that gradient disparity reflects the test error at the early
stages of training quite well. In the later stages of training we observe that the ranking of gradient
disparity values for different label noise levels matches with the ranking of generalization losses and
errors. In all experiments the average gradient disparity is indeed very informative about the test
error. Figure 16 shows the effect of adding data augmentation on both the test error and gradient dis-
parity. Figure 17 shows test error and gradient disparity for networks that are trained with different
training set sizes. In Figure 18, we observe that, as discussed in Section 6, gradient disparity, similar
to the test error, increases with the batch size for not too large batch sizes, and as expected, when the
batch size is very large (512 for the CIFAR-10 experiment and 256 for the CIFAR-100 experiments)
gradient disparity starts to decrease, because gradient vectors are averaged over a large batch. Note
that even with such large batch sizes, gradient disparity correctly detects the early stopping time, but
the value of gradient disparity can no longer be compared to the one found with other batch sizes.
(a) Training loss	(b) Test loss
(c) Training error
(d) Test error
IO0	IO1	IO2
Epochs
IO0	IO1	IO2
Epochs
(g) D
(h) D for epoch > 20
Figure 15: The cross entropy loss, error percentage, and average gradient disparity during training
with different amounts of randomness in the training labels for a 4-layer fully connected neural
network with 500 hidden units trained on the entire CIFAR-10 dataset. The parameter initialization
is the Xavier initialization with uniform distribution. The training is stopped when the training loss
gets below 0.01.
(e) Generalization loss
(f) Generalization error
7https://www.cs.toronto.edu/~kriz∕cifar.html
27
Under review as a conference paper at ICLR 2021
+ With DA
(a) With DA, ρD,TE = 0.984
(b) Without DA, ρD,TE = 0.983
(c) Both, ρD,TE = 0.619
Figure 16: The test error (bottom row) and gradient disparity (top row) for a ResNet-18 trained on
the CIFAR-10 dataset with different training set sizes. (a) Results with data augmentation (DA) (we
use random crop with padding = 4 and random horizontal flip with probability = 0.5). (b) Results
without using any data augmentation technique. (c) Combined results of (a) and (b). We observe
a strong positive correlation between gradient disparity (D) and test error (TE) regardless of using
data augmentation or not. We also observe that using data augmentation decreases the values of both
gradient disparity and the test error.
(a) VGG-16, CIFAR-10, ρD,TE = 0.972
(b) ALexNet, MNIST, ρD,TE = 0.929
Figure 17: Test error and gradient disparity for networks that are trained with different training set
sizes. The training is stopped when the training loss is below 0.01.
(a)	CIFAR-10, ρD,TE = 0.631
(b)	CIFAR-100, ρD,TE = 0.909
Figure 18: Test error and gradient disparity for networks that are trained with different batch sizes
trained on 12.8 k points of the CIFAR-10 and CIFAR-100 datasets. The training is stopped when
the training loss is below 0.01.
28
Under review as a conference paper at ICLR 2021
E.3 CIFAR-100 Experiments
Figure 19 shows the results for a ResNet-18 that is trained on the CIFAR-100 training set8. Clearly,
the model is not sufficient to learn the complexity of the CIFAR-100 dataset: It has 99% error for the
network with 0% label noise, as if it had not learned anything about the dataset and is just making
a random guess for classification (because there are 100 classes, random guessing would give 99%
error on average). We observe from Figure 19 (f) that as training progresses, the network overfits
more, and the generalization error increases. Although the test error is high (above 90%), very
surprisingly for this example, the networks with higher label noise level, have a lower test loss and
error (Figures 19 (b) and (d)). Quite interestingly gradient disparity (Figure 19 (g)) captures also
this surprising trend as well.
(a) Training loss
(d) Test error
(e) Generalization loss	⑴ Generalization error (g) Average gradient disparity D
Figure 19: The cross entropy loss, error percentage, and average gradient disparity during training
with different amounts of randomness in the training labels for a ResNet-18 trained on the CIFAR-
100 training set. The parameter initialization is the Xavier initialization.
(b) Test loss	(c) Training error
Ooooooo
0 5 0 5 0 5
3 2 2 1 1
A+Jμeds~p4JUS-P(DJ3φ3ee><
F	Number of Batches with Low Gradient Disparity
In this paper, the upper bound in Theorem 1 is tracked by computing the average gradient disparity
D over a subset of batches. In this section, to gain some finer-grain signal of overfitting during the
training process, we track down the distribution of this bound by studying the distribution of the
gradient disparity, i.e., P(Di,j < ζ) for some threshold ζ.
We find the number of pairs of batches in the training set, denoted by Tζ , whose gradient dis-
parity is below the given threshold ζ . For these pairs of batches, the upper bound in Theo-
rem 1, with probability at least 1 - δ and for two batches with the same size m, is below
2∖∕(Y2Z2/σ2 + 2 ln(2m∕δ)) /(m — 2). The lower TZ is, the higher the average upper bound is,
hence the more likely overfitting becomes. As before, for the sake of computational tractability,
instead of going through all the possible pairs, we only compare s(s - 1) pairs of batches (s = 5
for our experiments), so 0 ≤ Tζ ≤ s(s - 1). We empirically estimate P(Di,j < ζ) over s batches
by Tζ /(s(s - 1)). In our experiments, we compute Tζ for ζ ∈ {10, 20}.
We show that as an early stopping criterion, TZ is sometimes (slightly) more accurate than D. For
instance, in Figure 20 (d) we highlighted in gray the minima valley of the test error for the network
trained with 25% noise and we observe that this aligns with the drop in TZ for ζ = 20 (light pink
curve in Figure 20 (g)) better than the increasing time of D (Figure 20 (f)). Also, the slow increase
in the generalization loss for the AlexNet (green curve in Figure 21 (a)) is captured by the drop in
8https://www.cs.toronto.edu/~kriz∕cifar.html
29
Under review as a conference paper at ICLR 2021
(b) Test loss
(d) Test error
(a) Training loss
(c) Training error
⑴ Average gradient disparity D
(g)Tζforζ=20
(e) Generalization error
Figure 20: The cross entropy loss, error percentage, average gradient disparity and Tζ for ζ = 20
during training with different amounts of randomness in the training labels for a ResNet-18 trained
on a subset of 12.8 k points of the CIFAR-10 training set. We can observe that, in this setting,
average gradient disparity distinguishes different label noise levels from the beginning of training,
unlike generalization error.
(a) Loss
(b) D
Figure 21: The cross entropy loss, average gradient disparity D, and T for Z = 10 during training
for an AlexNet trained on a subset of 12.8 k points of the MNIST training set.
80604020
AaμedMP 4UCD一 PB」U
(c) Tζ for ζ = 10
Tζ for ζ = 10 (Figure 21 (c)) slightly better than the increase in the average gradient disparity (Fig-
ure 21 (b)). The drawback of T compared to D is that it introduces an additional hyper-parameter
ζ which requires tuning. In our experiments, as a rough rule of thumb, we observe that setting ζ
around 10% of the initial gradient disparity works well. Further empirical investigation on Tζ is left
as future work.
G Beyond SGD
In the following, we discuss how the analysis of Section 4 can be extended for other optimizers
(refer to Ruder (2016) for an overview on popular optimizers).
G.1 SGD with Momentum
The momentum method (Qian, 1999) is a variation of SGD which adds a fraction of the update
vector of the previous step to the current update vector to accelerate SGD:
υ(t+1) = ηυ(t) + γg(t),
w(t+1) = w(t) - υ(t+1),
30
Under review as a conference paper at ICLR 2021
where g(t) is either g1 or g2 depending on the selection of the batch S1 or S2 for the current update
step. As υ (t) remains the same for either choice, the KL-divergence between Q1 and Q2 for SGD
with momentum, is the same as Equation (5).
G.2 Adagrad
Adagrad (Duchi et al., 2011) performs update steps with a different learning rate for each individual
parameter. By denoting each coordinate of the parameter vector w by d, one update step of the
Adagrad algorithm is
w
(t+1)
d
J gdt)
(14)
—
where the vector g(t) is either g1 or g2 depending on the selection of the batch for the current update
step, and G(dtd) is the accumulative squared norm of the gradients up until iteration t. Hence, for
Adagrad, Equation (5) is replaced by
1 γ2
KL(Q11Q) = 2 W
1	2	1 γ2	1	2	2
GW+! θ (g1-g2)2≤ 2σ2 GW+! 2 kg1 - g2k2,	(15)
where θ denotes the element-wise product of two vectors, where division is also taken element-wise
and where is a small positive constant that avoids a possible division by 0. To compare the upper
bound in Theorem 1 from one iteration to the next one (as needed to determine the early stopping
moment in Section 5), gradient disparity is not the only factor in Equation (15) that evolves over
time. Indeed G(t) is an increasing function of t. However, after a few iterations when the gradients
become small, this value becomes approximately constant (the initial gradient values dominate the
sum in G(t)). Then the right hand side of Equation (15) varies mostly as a function of gradient
disparity, and therefore gradient disparity approximately tracks down the generalization penalty
upper bound.
G.3 Adadelta and RmsProp
Adadelta (Zeiler, 2012) is an extension of Adagrad, which computes a decaying average of
the past gradient vectors instead of the accumulative squared norm of the gradients dur-
ing the previous update steps. G(dtd) in Equation (14) is then replaced by υd(t+1) where
υd(t+1) = ηυd(t) +(1 - η)(gd(t))2. As training proceeds, the gradient magnitude decreases. Also, η is
usually close to 1. Therefore, the dominant term in υd(t+1) becomes ηυd(t). Then, if we approximate
υ1(t+1) = ηυ(t) +(1 - η) (g1)2 ≈ ηυ(t) +(1 - η) (g2)2 = υ2(t+1) (squares are done element-wise),
then for Adadelta we have
2 2
1 γ2	1
KL(Ql||Q2) ≤ 2σ υ(t+i)+ e 2 kg1 - g2k2 ,	(16)
where again the division is done element-wise. The denominator in Equation (16) is smaller than
the denominator in Equation (15). In both equations, the first non-constant factor in the upper bound
of KL(Q1||Q2) decreases as a function of t, and therefore an increase in the value of KL(Q1||Q2)
should be accounted for by an increase in the value of gradient disparity. Moreover, as training
proceeds, gradient magnitudes decrease and the first factor on the upper bound of Equations (15)
and (16) becomes closer to a constant. Therefore, an upper bound on the generalization penalties
can be tracked by gradient disparity.
The update rule of RmsProp9 is very similar to Adadelta, and the same conclusions can be made.
9https://www.cs.toronto.edu/~tijmen/csc321/Slides/lecture_Slides_lec6.
pdf
31
Under review as a conference paper at ICLR 2021
G.4 Adam
Adam (Kingma & Ba, 2014) combines Adadelta and momentum by storing an exponentially decay-
ing average of the previous gradients and squared gradients:
m(t+1) = β1m(t) + (1 - β1)g(t),
υ(t+1) = β2υ(t) + (1 -β2) g(t)2,
m^ (t+1)
m(t+1)
1-(βι)t
U(t+1) = υ(t+1)
1 - (β2)t
w(t+1) = w(t) -
√⅛ m(t+1).
All the operations in the above equations are done element-wise. As β2 is usually very
close to 1 (around 0.999), and as squared gradient vectors at the current update step
are much smaller than the accumulated values during the previous steps, we approximate:
υ1(t+1) = β2υ(t) + (1 - β2) (g1)2 ≈ β2υ(t)+(1 - β2) (g2)2 = υ2(t+1) (squares are done element-
wise). Hence, Equation (5) becomes
KL(QIIQ) ≤ 2 W ⅛⅛
√υt+i)+e 2 kg1 -g2k2.
(17)
The first non-constant factor in equation above decreases with t (because β1 < 1). However it is
not clear how the second factor varies as training proceeds. Therefore, unlike previous optimizers,
it is more hazardous to claim that the factors other than gradient disparity in Equation (17) become
constant as training proceeds. Hence, tracking only gradient disparity for the Adam optimizer may
be insufficient. This is empirically investigated in the next sub-section.
G.5 Experiments
Figure 22 shows gradient disparity and test loss curves during the course of training for adaptive
optimizers. The epoch in which the fifth increase in the value of the test loss and gradient disparity
has happened is presented in the caption of each experiment. We observe that the two suggested
epochs for stopping the optimization (the one suggested by gradient disparity (GD) and the other
one suggested by test loss) are extremely close to each other except in Figure 22 (c) where the fifth
epoch with an increase in the value of gradient disparity is much later than the epoch with the fifth
increase in the value of test loss. However, in this experiment, there is a 23% improvement in the
test accuracy if the optimization is stopped according to GD compared to test loss, due to many
variations of test loss compared to gradient disparity.
As an early stopping criterion, the increase in the value of gradient disparity coincides with the
increase in the test loss in all our experiments presented in Figure 22. In Figure 22 (h), for the
Adam optimizer, we observe that after around 20 epochs, the value of gradient disparity starts to
decrease, whereas the test loss continues to increase. This mismatch between test loss and gradient
disparity might be due to the effect of the other factors that appear in Equation (17). Nevertheless,
even in this experiment, the increase in the test loss and the gradient disparity coincide, and hence
gradient disparity can correctly detect early stopping time. These experiments are a first indication
that gradient disparity can be used as an early stopping criterion for optimizers other than SGD.
32
Under review as a conference paper at ICLR 2021
test loss sgdm
gradient disparity
3μedMP∙l->u ①一 PΠJ.I6
2 ∙ O ∙ ∙
118 6
IO0	IO1
epochs
(a) SGD with Momentum, test loss epoch: 11,
GD epoch: 10
epochs
20	40	60	80
epochs
(b) Adagrad, test loss epoch: 19,
GD epoch: 18
(c) RmsProp, test loss epoch: 15 (loss:1.43, err: 54%),
GD epoch: 36 (loss:1.16, err: 31%)
(d) Adam, test loss epoch: 19,
GD epoch: 20
(f) Adadelta, test loss epoch: 12, GD epoch: 15
(e) Adagrad, test loss epoch: 21, GD epoch: 21
(g) RmsProp, test loss epoch: 20, GD epoch: 18
(h) Adam, test loss epoch: 12, GD epoch: 10
Figure 22: (a-d) VGG-19 configuration trained on 12.8 k training points of CIFAR-10 dataset. (e-h)
VGG-11 configuration trained on 12.8 k points of the CIFAR-10 dataset. The training is stopped
when the training loss gets below 0.01. The presented results are an average over 5 runs. The
captions below each figure give the epoch number where test loss and gradient disparity have re-
spectively been increased for 5 epochs from the beginning of training. The Pearson correlation
coefficient ρ is presented in each figure between gradient disparity and test loss.
33
Under review as a conference paper at ICLR 2021
H Comparison to Related Work
—	Min	GD/Var	EB	GSNR	gi ∙ gj	Sigmgi ∙ gj cos(gi ∙ gj)	Ωc	I k-fold	I No ES
TE	4.84	4.84	4.84	12.82	22.30	12.82	18.31	8.30	4.84	4.96
TL	0.18	~0J8	K18	0.46	0.82	0.46	0.69	0.32	0.18	0.22
(a) MNIST, AlexNet									
—	Min	GD/Var	EB	GSNR	gi ∙ gj	sign(gi ∙ gj cos(gi ∙ gj)	Ωc	k-fold	No ES
TE	13.76	16.66	24.63	35.68	37.92	24.63	35.68	29.40	17.86	25.725
TL	0.75	T08	0.86	1.68	1.82	0.86	1.68	1.46	1.09	0.91
(b) MNIST, AlexNet, 50% random									
	Min	GD/Var	EB	GSNR	gi ∙ gj	Sign(gi ∙ gj) cos(gi ∙ gj)	Ωc	k-fold	No ES
TE	45.54	45.95	61.76	70.46	70.46	55.84	67.09	67.37	51.64	64.19
TL	1.32		1.68	1.92	1.92	1.52	1.83	1.85	1.49	1.98
(c) CIFAR-10, ResNet-18									
—	Min	GD/Var	EB	GSNR	gi∙ gj	Sign(gi ∙ gj) cos(gi ∙ gj)	Ωc I	k-fold I	No ES
TE	59.77	71.97	73.17	77.08	75.91	65.80	75.43	77.71	72.56	75.96
TL	1.75	^200^	2.03	2.12	2.13	T93	2.07	2.13	2.02	2.30
(d) CIFAR-10, ResNet-18, 50% random
Table 8: Test error (TE) and test loss (TL) achieved by using various metrics as early stopping
criteria. On the leftmost column, the minimum values of TE and TL over all the iterations are re-
ported (which is not accessible during training). The results of 5-fold cross validation are reported
on the right-most column, which serve as a baseline. For each experiment, we have underlined those
metrics that result in a better performance than 5-fold cross validation. We observe that gradient
disparity (GD) and variance of gradients (Var) consistently outperform k-fold cross validation, un-
like other metrics. On the rightmost column (No ES) we report the results without performing early
stopping (ES) (training is continued until the training loss is below 0.01).
In Table 8 we compare gradient disparity (GD) to a number of metrics that were proposed either
directly as an early stopping criterion, or as a generalization metric. For those metrics that were not
originally proposed as early stopping criteria, we choose a similar method for early stopping as the
one we use for gradient disparity. We consider two datasets (MNIST and CIFAR-10), and two levels
of label noise (0% and 50%). Here is a list of the metrics that we compute in each setting:
1.	Gradient disparity (GD) (ours): we report the error and loss values at the time when the
value of GD increases for the 5th time (from the beginning of the training).
2.	The EB-criterion (Mahsereci et al., 2017): we report the error and loss values when EB
becomes positive.
3.	Gradient signal to noise ratio (GSNR) (Liu et al., 2020): we report the error and loss values
when the value of GSNR decreases for the 5th time (from the beginning of the training).
4.	Gradient inner product, gi ∙ gj (Fort et al., 2019): We report the error and loss values When
the value of gi ∙ gj decreases for the 5th time (from the beginning of the training).
5.	Sign of the gradient inner product, Sign(gi ∙ gj-) (Fort et al., 2019): we report the error and
loss values when the value of sign(gi ∙ gj-) decreases for the 5th time (from the beginning
of the training).
6.	Cosine similarity between gradient vectors, cos(gi ∙ gj-) (Fort et al., 2019): we report the
error and loss values when the value of cos(gi ∙ gj) decreases for the 5th time (from the
beginning of the training).
7.	Variance of gradients (Var) (Negrea et al., 2019): we report the error and loss values when
the value of Var increases for the 5th time (from the beginning of the training). Variance is
computed over the same number of batches used to compute gradient disparity, in order to
compare metrics given the same computational budget.
34
Under review as a conference paper at ICLR 2021
8.	Average gradient alignment within the class, Ωc (Mehta et al., 2020): We report the error
and loss values when the value of Ωc decreases for the 5th time (from the beginning of the
training).
On the leftmost column of Table 8, we report the minimum values of the test error and the test loss
over all the iterations, which may not necessarily coincide. For instance, in setting (c), the test error
is minimized at iteration 196, whereas the test loss is minimized at iteration 126. On the rightmost
column of Table 8, we report the values of the test error and the test loss when using 5-fold cross
validation, which serves as a baseline.
It is interesting to observe that gradient disparity and variance of gradients produce the exact same
results when used as early stopping criteria (Table 8). Moreover, these two are the only metrics
that consistently outperform k-fold cross validation. However, in Section H.1, we observe that
the correlation between gradient disparity and the test loss is in general larger than the correlation
between variance of gradients and the test loss.
The EB-Criterion, sign(gi ∙ gj), and cos(gi ∙ gj) are metrics that perform quite well as early stopping
criteria, although not as well as GD and Var. In Section H.2, we observe that these metrics are not
informative of the label noise level.
H. 1 Gradient Disparity versus Variance of Gradients
It has been shown that generalization is related to gradient alignment experimentally
in Fort et al. (2019), and to variance of gradients theoretically in Negrea et al.
(2019). Gradient disparity can be viewed as bringing the two together. Indeed, one
can check that E [D2j] = 2σg + 2μjμg - 2E [gτgj], given that μ9 = E[gi] = E[gj] and
σg2 = tr (Cov [gi]) = tr (Cov [gj]). This shows that gradient variance σg2 and gradient alignment
giT gj both appear as components of gradient disparity. We conjecture that the dominant term in
gradient disparity is the variance of gradients, hence as early stopping criteria these two metrics
almost always signal overfitting simultaneously. This is indeed what our experiments show; we
show that variance of gradients is also a very promising early stopping criterion (Table 8). However,
because of the additional term in gradient disparity (the gradients inner product), gradient disparity
emphasizes the alignment or misalignment of the gradient vectors. This could be the reason why
gradient disparity in general outperforms variance of gradients in tracking the value of the general-
ization loss; the positive correlation between gradient disparity and the test loss is often larger than
the positive correlation between variance of gradients and the test loss (Table 9).
Setting	I ρD,TL I	ρVar,TL
AlexNet, MNIST	I 0.433 I	0.169
AlexNet, MNIST, 50% random labels	I 0.535 I	0.161
VGG-16, CIFAR-10	I 0.190 I	0.324
VGG-16, CIFAR-10, 50% random labels	I 0.634 I	0.623
VGG-19, CIFAR-10	I 0.685 I	0.508
VGG-19, CIFAR-10, 50% random labels	I 0.748 I	0.735
ResNet-18, CIFAR-10	I 0.975 I	0.958
ResNet-18, CIFAR-10, 50% random labels	I 0.471 I	0.457
Table 9: Pearson,s correlation coefficient between gradient disparity (D) and test loss (TL) over the
training iterations is compared to the correlation between variance of gradients (Var) and test loss.
35
Under review as a conference paper at ICLR 2021
H.2 Capturing Label Noise Level
In this section, we show in particular three metrics that even though perform relatively well as early
stopping criteria, fail to account for the level of label noise, contrary to gradient disparity.
•	The sign of the gradient inner product, sign(gi ∙ gj∙), should be inversely related to the test
loss; it should decrease when overfitting increases. However, we observe that the value of
sign(gi ∙ gj) is larger for the setting with the higher label noise level; it incorrectly detects
the setting with the higher label noise level as the setting with the better generalization
performance (see Figure 23).
•	The EB-criterion should be larger for settings with higher overfitting. In most stages of
training, the EB-criterion does not distinguish between settings with different label noise
levels, contrary to gradient disparity (see Figure 23). At the end of the training, the EB-
criterion even mistakenly signals the setting with the higher label noise level as the setting
with the better generalization performance.
•	The cosine similarity between gradient vectors, cos(gi ∙ gj∙), should decrease when overfit-
ting increases and therefore with the level of label noise in the training data. But cos(gi ∙ gj∙)
appears not to be sensitive to the label noise level, and in some cases (Figure 24 (a)) it even
increases with the noise level. Gradient disparity is much more informative of the label
noise level compared to cosine similarity and the correlation between gradient disparity
and the test error is larger than the correlation between cosine similarity and the test accu-
racy (see Figure 24).
Figure 23: Test loss, gradient disparity, EB-criterion Mahsereci et al. (2017), and sign(gi ∙ gj∙) for a
ResNet-18 trained on the CIFAR-10 dataset, with 0% and 50% random labels. Gradient disparity,
contrary to EB-criterion and sign(gi ∙ gj∙), clearly distinguishes the setting with real labels from the
setting with random labels.
36
Under review as a conference paper at ICLR 2021
A⅛6」QdS 一 p⅛4①一 pe⅛2φ^2φ><
(a) CIFAR-10, ResNet-18
0% label noise
25% label noise
50% label noise
75% label noise
(b) MNIST, AlexNet
Figure 24: The test error (TE), average gradient disparity (D), and cosine similarity (cos(gi ∙gj∙)) dur-
ing training with different amounts of randomness in the training labels for two sets of experiments.
(a) ResNet-18 trained on 12.8k points of the CIFAR-10 training set. The Pearson correlation coeffi-
cient between test accuracy (TA) and cosine similarity (cos) over all levels of randomness and over
all the iterations is ρcos,TA = -0.0088, whereas the correlation between test error/generalization
error and gradient disparity is PDTE = 0.2029 and PD GE = 0.5268, respectively. (b) AleXNet Con-
figuration trained on 12.8k points of the MNIST dataset. The correlation between the test accuracy
and cosine similarity is Pcos,TA = 0.7521, which is positive and relatively high for this eXperi-
ment. Yet, it is still lower than the correlation between test error and gradient disparity which is
Pdte = 0.8019.
gradient disparity
25% label noise
—50% label noise
— 75% label noise
IO1	IO2	IO3
Iterations
37