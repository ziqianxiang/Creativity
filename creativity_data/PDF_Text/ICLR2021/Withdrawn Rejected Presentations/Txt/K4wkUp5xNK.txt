Under review as a conference paper at ICLR 2021
Invariant Causal Representation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Due to spurious correlations, machine learning systems often fail to generalize to
environments whose distributions differ from the ones used at training time. Prior
work addressing this, either explicitly or implicitly, attempted to find a data repre-
sentation that has an invariant causal relationship with the outcome. This is done
by leveraging a diverse set of training environments to reduce the effect of spuri-
ous features, on top of which an invariant classifier is then built. However, these
methods have generalization guarantees only when both data representation and
classifiers come from a linear model class. As an alternative, we propose Invariant
Causal Representation Learning (ICRL), a learning paradigm that enables out-of-
distribution generalization in the nonlinear setting (i.e., nonlinear representations
and nonlinear classifiers). It builds upon a practical and general assumption: data
representations factorize when conditioning on the outcome and the environment.
Based on this, we show identifiability up to a permutation and pointwise transfor-
mation. We also prove that all direct causes of the outcome can be fully discov-
ered, which further enables us to obtain generalization guarantees in the nonlinear
setting. Extensive experiments on both synthetic and real-world datasets show
that our approach significantly outperforms a variety of baseline methods.
1	Introduction
In recent years, despite various impressive success stories, there is still a significant lack of robust-
ness in machine learning algorithms. Specifically, machine learning systems often fail to generalize
outside of a specific training distribution, because they usually learn easier-to-fit spurious correla-
tions which are prone to change from training to testing environments. We illustrate this point by
considering the widely used example of classifying images of camels and cows (Beery et al., 2018).
The training dataset has a selection bias, i.e., most pictures of cows are taken in green pastures, while
most pictures of camels happen to be in deserts. After training a convnet on this dataset, it is found
that the model fell into the spurious correlation, i.e., it related green pastures with cows and deserts
with camels, and therefore classified green pastures as cows and deserts as camels. The result is that
the model failed to classify images of cows when they are taken on sandy beaches.
To address the aforementioned problem, a natural idea is to identify which features of the training
data present domain-varying spurious correlations with labels and which features describe true cor-
relations of interest that are stable across domains. In the example above, the former are the features
describing the context (e.g., pastures and deserts), whilst the latter are the features describing ani-
mals (e.g., animal shape). Arjovsky et al. (2019) suggest that one can identify the stable features and
build invariant predictors on them by exploiting the varying degrees of spurious correlation naturally
present in training data collected from multiple environments. The authors proposed the invariant
risk minimization (IRM) approach to find data representations for which the optimal classifier is
invariant across all environments. Since this formulation is a challenging bi-leveled optimization
problem, the authors proved the generalization of IRM across all environments by constraining both
data representations and classifiers to be linear (Theorem 9 in Arjovsky et al. (2019)).
Ahuja et al. (2020) studied the problem from the perspective of game theory, with an approach that
we call IRMG for short. They showed that the set of Nash equilibria for a proposed game are equiv-
alent to the set of invariant predictors for any finite number of environments, even with nonlinear
data representations and nonlinear classifiers. However, these theoretical results in the nonlinear
setting only guarantee that one can learn invariant predictors from training environments, but do not
guarantee that the learned invariant predictors can generalize well across all environments including
1
Under review as a conference paper at ICLR 2021
unseen testing environments. In fact, the authors directly borrowed the linear generalization result
from Arjovsky et al. (2019) and presented it as Theorem 2 in Ahuja et al. (2020).
In this work we propose an alternative learning paradigm, called Invariant Causal Representation
Learning (ICRL), which enables out-of-distribution (OOD) generalization in the nonlinear setting
(i.e., nonlinear representations and nonlinear classifier). We first introduce a practical and general
assumption: the data representation factorizes (i.e., its components are independent of each other)
when conditioning on the outcome (e.g., labels) and the environment (represented as an index). This
assumption builds a bridge between supervised learning and unsupervised learning, leading to a
guarantee that the data representation can be identified up to a permutation and pointwise transfor-
mation. We then theoretically show that all the direct causes of the outcome can be fully discovered.
Based on this, the challenging bi-leveled optimization problem in IRM and IRMG can be reduced to
two simpler independent optimization problems, that is, learning the data representation and learning
the optimal classifier can be performed separately. This further enables us to attain generalization
guarantees in the nonlinear setting.
Contributions We propose Invariant Causal Representation Learning (ICRL), a novel learning
paradigm that enables OOD generalization in the nonlinear setting. (i) We introduce a conditional
factorization assumption on data representation for the OOD generalization (Assumption 1). (ii)
Base on this assumption, we show that each component of the representation can be identified up
to a permutation and pointwise transformation (Theorem 1, 2 & 3). (iii) We further prove that
all the direct causes of the outcome can be fully discovered (Proposition 1). (iv) We show that our
approach has generalization guarantees in the nonlinear setting (Proposition 2). (v) Empirical results
demonstrate that our approach significantly outperforms IRM and IRMG in the nonlinear scenarios.
2	Preliminaries
2.1	Identifiable Variational Autoencoders
A general issue with variational autoencoders1 (VAEs) (Kingma & Welling, 2013; Rezende et al.,
2014) is the lack of identifiability guarantees of the deep latent variable model. In other words, it
is generally impossible to approximate the true joint distribution over observed and latent variables,
including the true prior and posterior distributions over latent variables. Consider a simple latent
variable model where O ∈ Rd stands for an observed variable (random vector) and X ∈ Rn for a
latent variable. Khemakhem et al. (2020) showed that any model with unconditional latent distri-
bution pθ(X) is unidentifiable. That is, we can always find transformations of X which change its
value but do not change its distribution. Hence, the primary assumption that they make to obtain
an identifiability result is to include a conditionally factorized prior distribution over the latent vari-
ables pθ (X| U), where U ∈ Rm is an additionally observed variable (Hyvarinen et al.,2019). More
specifically, let θ = (f, T, λ) ∈ Θ be the parameters of the conditional generative model:
pθ (O, X |U) = Pf (o∣x )pt ,λ(x∣u),	(1)
where pf(O|X) = p(O - f(X)) in which is an independent noise variable with probability
density functionp(), and the prior probability density function is especifically given by
k
Pτ,λ(X|U) = ∏i Qi(Xi)∕Zi(U) ∙ exp [Ej=ITij(Xi)M(U)],	⑵
where Qi is the base measure, Zi(U) the normalizing constant, Ti = (Ti,1, . . . , Ti,k) the sufficient
statistics, λi(U) = (λi,1 (U), . . . , λi,k (U)) the corresponding parameters depending on U, and
k the dimension of each sufficient statistic that is fixed in advance. It is worth noting that this
assumption is not very restrictive as exponential families have universal approximation capabilities
(Sriperumbudur et al., 2017). As in VAEs, we maximize the corresponding evidence lower bound:
LiVAE(θ, Φ)=EpD [Eqφ(x∣o,u) [logPθ(O, X|U) - log qφ(X|O, U)]] ,	(3)
where we denote by PD the empirical data distribution given by dataset D = {(O(i), U(i))}[「
This approach is called identifiable VAE (iVAE). Most importantly, it can be proved that iVAE can
identify latent variables X up to a permutation and pointwise transformation under the conditions
stated in Theorem 2 of (Khemakhem et al., 2020).
1A brief description of variational autoencoders is given in Appendix A.
2
Under review as a conference paper at ICLR 2021
2.2	Invariant Risk Minimization
Arjovsky et al. (2019) introduced invariant risk minimization (IRM), whose goal is to construct
an invariant predictor f that performs well across all environments Eall by exploiting the varying
degrees of spurious correlation naturally present in the training data collected from multiple environ-
ments Etr, where Etr ⊆ Eall. Technically, they consider datasets De := {(oie, yie)}in=e1 from multiple
training environments e ∈ Etr , where oie ∈ O ⊆ Rd is the input observation and its corresponding
label2 is yie ∈ Y ⊆ R3 * s . The dataset De, collected from environment e, consists of examples iden-
tically and independently distributed according to some probability distribution P (Oe, Y e). The
goal of IRM is to use these multiple datasets to learn a predictor Y = f(O) that achieves the
minimum risk for all the environments. Here we define the risk reached by f in environment e as
Re(f) = EOe,Y e [`(f (Oe), Y e)]. Then, the invariant predictor can be formally defined as below,
Definition 1 (Invariant Predictor, Arjovsky et al. (2019)). We say that a data representation Φ ∈
HΦ : O → C elicits an invariant predictor w ◦ Φ across environments E if there is a classifier w ∈
Hw : C → Y simultaneously optimal for all environments, that is, W ∈ argminw∈Hw Re(W ◦ Φ)
for all e ∈ E.
Mathematically, IRM can be phrased as the following constrained optimization problem:
min
Φ∈HΦ ,w∈Hw
e∈Etr
Re(w ◦ Φ)
s.t. W ∈ arg min Re(W ◦ Φ), ∀e ∈ Etr.
w∈Hw
(4)
Since this is a generally infeasible bi-leveled optimization problem, Arjovsky et al. (2019) rephrased
it as a tractable penalized optimization problem by transfering the inner optimization routine to a
penalty term. The main generalization result (Theorem 9 in Arjovsky et al. (2019)) states that if both
Φ and W come from the class of linear models (i.e., HΦ = Rn×n and Hw = Rn×1), under certain
conditions on the diversity of training environments (Assumption 8 in Arjovsky et al. (2019)) and
the data generation, the invariant predictor W ◦ Φ across Etr obtained by solving Eq. (4) remains
invariant in Eall. It is worth noting that Ahuja et al. (2020) reconsidered this IRM problem from the
perspective of game theory, called IRMG for short. Although in the new formulation they proved
that there exist such invariant predictors in Etr when both Φ and W are relaxed to the nonlinear
models, their main generalization result in Eall holds only when both Φ and W are linear models
(Theorem 2 in Ahuja et al. (2020)).
3 Problem Setup
3.1 A Motivating Example
In this section, we extend the example which was introduced by Wright (1921) and discussed by
Arjovsky et al. (2019), and provide a further in-depth analysis.
Model 1. Consider the following structural equation model (SEM):
Xl J Gaussian(0,σι(e)), Y J Xi + Gaussian(0,σ2(e)), X? J Y + Gaussian(0, σ3(e)),
where σi(e) ≥ 0, varying in environment e ∈ Eall, and Eall is the set of all environments.
To ease exposition, here we consider the simple scenario in which Eall only contains all modifications
varying the noises of X1, X2 and Y within a finite range, i.e., σi(e) ∈ [0, σm2 ax]. Then, to predict Y
from (Xi, X?) using a least-square predictor Ye = αιXe + a?Xe for environment e, We can
• Case 1: regress from Xe, to obtain αι = 1 and a2 = 0,
• Case 2: regress from Xe, to obtain αι = 0 and α2
σ1(e)+σ2(e)
σ1(e)+σ2(e)+σ3(e),
• Case 3: regress from (Xf, Xe), to obtain ɑι
σ3(e)	and a。—	σ2(e)
σ2(e)+σ3(e) and a2 = σ2(e)+σ3(e).
In general scenarios (i.e., σi(e) 6= 0, σ2(e) 6= 0, and σ3(e) 6= 0), the regression using Xi in Case
1 is an invariant correlation: this is the only regression Whose coefficients do not vary With the
environment e. By contrast, the regressions in both Case 2 and Case 3 have varying coefficients
2This setup for labels applies to both continuous and categorical data, Where the categorical data can be
encoded in the one-hot form.
3
Under review as a conference paper at ICLR 2021
Figure 1: (a) Causal structure of Model 1. (b) A more practical extension of Model 1, where X1 and X2 are
not directly observed and O is their observation. (c) A general version of (b), where we assume there exist
multiple unobserved variables. Each of them could be either a parent, a child of Y , or has no connection with
Y . Grey nodes denote observed variables and white nodes represent unobserved variables.
depending on the environment e. Not surprisingly, only the invariant correlation in Case 1 would
generalize well to new test environments.
From a practical perspective, let us take a closer look at Case 3. Because we do not know in advance
that regressing from X1 alone will lead to an invariant predictor, in practice we may do the regres-
sion from all the accessible data (X1e, X2e). As aforementioned, when σi (e) 6= 0 for i = 1, 2, 3,
the regression does not work. Actually any empirical risk minimization (ERM) algorithm purely
minimizing training error (Vapnik, 1992) would not work in this setting. Invariant Causal Prediction
(ICP) methods (Peters et al., 2015) also do not work, since the noise variance in Y may change
across environments. To this end, Arjovsky et al. (2019) proposed IRM. As aforementioned, how-
ever, IRM and IRMG can generalize well to unseen testing environments only in the linear setting.
This motivates us to develop an approach to enabling the OOD generalization in the nonlinear setting
(i.e., both Φ and w are from the class of nonlinear models).
A more straightforward way to understand the motivating example is in its corresponding graphical
representation3, as shown in Fig. 1a. Following Peters et al. (2015), we treat the environment as a
random variable E , where E could be any information specific to the environment. For simplicity,
we let E be the environment index, i.e., E ∈ {1, . . . , N} and N is the number of training environ-
ments. Note that, here we consider E as a surrogate variable because it itself is not a causal variable
(Zhang et al., 2017; Huang et al., 2020). From Fig. 1a, it is obvious to see that ICP does not work
in this setting, since Y ⊥6⊥ E|X1. In fact, a more practical version appearing in real problems is
present in Fig. 1b, where the true variables {X1, X2} are unobserved and we only can observe their
transformation O, which is a function of {X1, X2}. In this case, even if Y is not affected by E (i.e.,
remove the edge E → Y ), applying ICP to O still does not work, since each variable (i.e., each
dimension) of O is jointly influenced by both X1 and X2. By contrast, both IRM and IRMG work
when the transformation is linear, but not when it is nonlinear. These analyses are also empirically
demonstrated in Section 5.1.
3.2 The General Setting
Inspired by the two dimensional example (i.e., X = (X1, X2) ∈ R2) described above, we naturally
extend it to a more general multi-dimensional setting4 as shown in Fig. 1c. Technically, we have
O ∈ O ⊆ Rd, Y ∈ Y ⊆ Rs, X = (Xp1,...,Xpr,Xc1,...,Xck) ∈ X ⊆ Rn(r+k) (lower-
dimensional, n(r+k) ≤ d), where we assume each Xi ∈ Rn for simplicity. It is worth emphasising
that except Xpr, none of {Xp1 , . . . , Xpr-1 } has an arrow pointed to Y , meaning that all the parents
of Y are absorbed into Xpr. Under this circumstance, more formally, we assume that
Assumption 1. Xi ⊥⊥ Xj |Y , E for any i 6= j.
This assumption is not very restrictive but practically and theoretically reasonable enough to be able
to cover various scenarios, due to the following reasons:
3The relation between SEM and its graphical representation is formally defined in Appendix B.
4As an initial work, we do not explicitly consider unobserved confounders in this paper. Precisely, for
simplicity, we assume that there are no unobserved confounders between Xi , Y , O, and E. In fact, in some
cases, our approach will not be affected even if there exist unobserved confounders. For example, if there were
an unobserved confounder between Xi and O, it would be absorbed into Xi and would not affect our approach.
4
Under review as a conference paper at ICLR 2021
(a)
(e)
(d)
(f)
(b)
E
E
(j)	(k)	(l)
Figure 2: (a) General causal structure over {Xi , Y , O, E}, where the arrow from Xi to O is a must-have
connection and the other four might not be necessarily. (b) 12 possible causal structures from (a).
(h)	(i)
(g)
(m)
Firstly, Assumption 1 allows us to separately deal with each Xi . When taking a closer look at each
Xi in Fig. 1c, it is evident that there exist only five possible connections between Xi , Y , E, and
O, as shown in Fig. 2a. Among them, only the arrow from Xi to O must exist whilst the other
four might not necessarily, which leads to 12 possible types of structures present in Figs. 2b-2m.
These structures cover most scenarios in real-world applications, e.g., Xi could be either a parent or
a child of Y , be either affected or not by E, and even have no connection to Y , etc. It is also worth
noting that Assumption 1 does not rule out the possibility that there might exist certain correlations
between all these latent variables.
Secondly, although it is well known that the key idea behind learning disentangled representations
is that real-world data is generated by a few explanatory factors of variation, Locatello et al. (2019)
show that unsupervised learning of disentangled representations is fundamentally impossible with-
out inductive biases, and that well-disentangled models seemingly cannot be identified without su-
pervision. They further suggest that disentanglement learning should be explicit about the role of
inductive biases and supervision. In this sense, our Assumption 1 reasonably implements this idea.
Thirdly, as described in Section 2.1, identifiability of latent variables in iVAE requires a condition-
ally factorized prior distribution over the latent variables, as we did in Assumption 1, which is a key
condition under which it is guaranteed that latent variables can be identified up to a permutation and
pointwise transformation. This condition further inspires us to develop a generalization theory in
the nonlinear setting that we will formulate in Section 4.3.
Now, under Assumption 1, solving Eq. (4) can be reduced to the key question of how to find the
subset of X that are the direct causes of Y , from the observational data {O, E, Y }, which will be
discussed in the next section.
4 Our Approach
In this section, we formally introduce our algorithm, namely Invariant Causal Representation Learn-
ing (ICRL), which consists of three phases and is summarized in Algorithm 1. The basic idea is
that we first identify true latent variables by leveraging iVAE under Assumption 1 (Phase 1), then
discover direct causes of Y (Phase 2), and finally learn an invariant predictor based on the identified
direct causes (Phase 3).
4.1 Phase 1: Identifying True Latent Variables Using iVAE
Under Assumption 1, it is straightforward how to identify the true hidden factors X from O with
the help of Y and E, by leveraging iVAE. We can directly substitute U with (Y , E) in Eqs. (1),
and obtain its corresponding generative model:
Pθ (O, X |Y, E)= Pf (O|X )Pτ,λ(X |Y, E),	(5)
pf(O|X) = p(O - f(X)).	(6)
5
Under review as a conference paper at ICLR 2021
Algorithm 1: Invariant Causal Representation Learning
Phase 1: We first learn the iVAE model, including the generative model and its corresponding
inference model, by optimizing the evidence lower bound present in Eq. (8) on the data
{O , Y , E}. Then, we use the learned iVAE model to infer the corresponding latent variable
X from {O, Y , E}, which is guaranteed to be identified up to a permutation and pointwise
transformation.
Phase 2: Once obtaining X , according to Theorem 1, we can discover from them which are the
direct causes Pa(Y ) of Y by only performing Rule 1.4, Rule 1.8, Rule 2.1, and Rule 3.1
described in Appendix E.3.
Phase 3: Once obtaining Pa(Y ), we can separately optimize Eq. (9) and Eq. (10) to learn the
invariant data representation Φ and the invariant classifier w.
Likewise, we also obtain its corresponding prior distribution and lower bound:
k
Pτ,λ(X |Y, E )=∏, QilXi) IZid, E) ∙ exp [£ -ITij (Xi)M (Y, E)],	⑺
i	j=1
Lphasei(θ, Φ)=EpD [Eqφ(x∣o,γ,E) [logPθ(O, X|Y,E) - log qφ(X|O, Y,E)]] .	(8)
This bound can be further expanded for computational convenience, which is given in Appendix C.
More importantly, under this setting we can directly borrow the identifiability result from Khe-
makhem et al. (2020) and then restate it below by replacing U with (Y, E).
Theorem 1.	Assume that we observe data sampled from a generative model defined according
to Eqs. (5-7), with parameters θ := (f, T, λ) and k ≥ 2. Assume the following holds: (i)
The set {O ∈ O∣^e(O) = 0} has measure zero, where φe is the characteristic function of
the density p defined in Eq. (6). (ii) The mixing function f in Eq. (6) is injective, and has
all second order cross derivatives. (iii) The sufficient statistics Ti,j in Eq. (7) are twice dif-
ferentiable, and (Ti,j)1≤j ≤k are linearly independent on any subset of X of measure greater
than zero. (iv) There exist nk + 1 distinct points (Y, E)0, . . . , (Y, E)nk such that the matrix
L = (λ((Y,E)1) - λ((Y,E)0),...,λ((Y,E)nk) - λ((Y,E )0)) ofsize nk X nk is invertible.
Then the parameters θ are identifiable up to a permutation and pointwise transformation.
Theorem 1 deals with the general case k ≥ 2, whose proof is given in Khemakhem et al. (2020).5
According to Theorem 1, we can further have the following result of consistency of estimation.
Theorem 2.	Assume the following holds: (i) The family of distributions qφ(X |O, Y, E) contains
pφ(X |O, Y, E). (ii) We maximize Lphase1(θ, φ) with respect to both θ and φ. Then in the limit of
infinite data, iVAE learns the true parameters θ* up to a permutation and pointwise transformation.
An immediate result of Theorem 1 and Theorem 2 is as follows,
Theorem 3.	Assume the hypotheses of Theorem 1 and Theorem 2 hold, then in the limit of infinite
data, iVAE learns the true latent variables X * up to a permutation and pointwise transformation.
The proofs of Theorem 2 and Theorem 3 are given in Appendix E. Theorem 3 says that we can
leverage iVAE to learn the true conditionally factorized latent variables up to a permutation and
pointwise transformation, which achieves our goal stated in Assumption 1.
4.2 Phase 2: Discovering Direct Causes
After identifying all the conditionally factorized latent variables X from O, the question that comes
is how to determine which component of X := (Xp1 , . . . , Xpr, Xc1 , . . . , Xck ) is the direct cause
ofY, denoted by Pa(Y). As discussed in Section 3.2, there are totally 12 possible types of structures
over {Xi, Y, E, O}, as shown in Figs. 2b-2m. Apparently, only in the four of them (i.e., Figs. 2c,
2f, 2i, and 2l) does Xi serve as a parent of Y. Note that, the reason that we also include Figs. 2i
and 2l is that we follow Arjovsky et al. (2019) and consider a more general definition of invariance
which allows for changes in the noise variance of Y. For convenience, we also put the definition
in Appendix B. Surprisingly, given the data {Xi, Y, E, O}, we are able to distinguish all the 12
5We also provide a theorem dealing with the special case k = 1 in Appendix D.
6
Under review as a conference paper at ICLR 2021
structures by performing conditional independence tests (Spirtes et al., 2000; Zhang et al., 2012)
and by leveraging causal discovery algorithms (Janzing et al., 2013; Peters et al., 2017; Zhang et al.,
2017; Huang et al., 2020). This is summarized in Proposition 1 and the proof is given in Appendix
E.
Proposition 1. All the 12 structures shown in Figs. 2b-2m can be independently and completely
distinguished in parallel.
Proposition 1 allows us to efficiently discover all direct causes of Y from X by independently
performing conditional independence tests and causal discovery algorithms for each Xi in parallel.
Besides, for each Xi we only need to check if they are one of the four structures (i.e., Figs. 2c, 2f,
2i, and 2l) and this check can be also performed in parallel.
4.3 Phase 3: Learning an Invariant Predictor
Once obtaining the invariant causal representation Pa(Y ) for Y across the training environments,
learning an invariant predictor w ◦ Φ in Eq. (4) is then reduced to two simpler independent optimiza-
tion problems: (i) learning the invariant data representation Φ from O to Pa(Y ), and (ii) learning
the invariant classifier w from Pa(Y ) to Y . Mathematically, these two optimization problems can
be respectively phrased as
tmin X UR Re(φ) = tmin X UE EOe,Pa(Ye) ['&(Oe),Pa(Ye))] ,	⑼
Φ∈HΦ	e∈Etr	Φ∈HΦ	e∈Etr
WxXe∈Etr Re(W) = wminw Xe∈Etr EPa(YΜ。['(W(Pa(Y力 YW	5 * * * * (IO)
Eq. (9) and Eq. (10) guarantee that ICRL can achieve low error across Etr . Also, in Phase 1&2,
we showed that ICRL can enforce invariance across Etr . Now this brings us to the question: how to
enable the OOD generalization? In other words, how does ICRL achieve low error across Eall ? As
Arjovsky et al. (2019) pointed out, low error across Etr and invariance across Eall leads to low error
across Eall, because the generalization error of W ◦ Φ respects standard error bounds once the data
representation Φ eliciting an invariant predictor W ◦ Φ across Eall is estimated. Thus, enabling the
OOD generalization finally comes to the question: under which conditions does invariance across
Etr imply invariance across Eall? Not surprisingly, Etr must contain sufficient diversity to satisfy an
underlying invariance across Eall. Fortunately, the hypotheses of Theorem 1 automatically provides
such a guarantee, and we therefore have the following result whose proof is in Appendix E.
Proposition 2. Assume the hypotheses of Theorem 1 and Theorem 2 hold, then in the limit of infinite
data, ICRL learns an invariant predictor W ◦ Φ across Eall.
Proposition 2 tells us that under the assumptions given in Theorem 1 and Theorem 2, if ICRL can
learn an invariant predictor W ◦ Φ across Etr in the limit of infinite data, then such a predictor W ◦ Φ
is invariant across Eall .
5	Experiments
We compare our approach with a variety of methods on both synthetic and real-world datasets.
Due to space limit, we put in the supplement a detailed description of the datasets (Appendix F)
and model architectures (Appendix H), as well as some in-depth analysis on experimental results
(Appendix G). In all the comparisons, unless stated otherwise, we averaged the performance of the
different methods over ten runs.
5.1 Synthetic data
As a first experiment, in order to interpret how our approach works, we conduct a series of exper-
iments on synthetic data generated according to an extension of the SEM in Model 1. This more
practical extension is done by increasing the dimensionality of the two true features X := (X1, X2)
to 10 dimensions through a linear or nonlinear transformation, as illustrated in Fig. 1b. Techni-
cally, the goal is to predict Y from O, where O = g(X) and g(∙) is called X Transformer. We
consider three types of transformations: (a) Identity: g(∙) is the identity matrix I ∈ R2×2, i.e.,
7
Under review as a conference paper at ICLR 2021
Table 1: Regression on synthetic data: Comparison of methods in terms of MSE (mean ± std deviation).
X TRANSFORMER	ALGORITHM	TRAIN MSE (σ3 = {0.2, 2})	TEST MSE (σ3 = 5)	TEST MSE (σ3 = 20)	TEST MSE (σ3 = 100)
	ERM	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
Identity	IRM	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
	F-IRM GAME	0.81 ± 0.37	0.84 ± 0.33	1.18 ± 0.32	9.58 ± 17.10
	V-IRM GAME	0.80 ± 0.24	1.32 ± 0.44	9.66 ± 10.86	241.01 ± 301.52
	ICRL	0.01 ± 0.03	0.08 ± 0.02	0.45 ± 0.06	1.00 ± 0.02
	ERM	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
Linear	IRM	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
	F-IRM GAME	0.99 ± 0.01	1.00 ± 0.01	1.01 ± 0.00	1.11 ± 0.16
	V-IRM GAME	0.89 ± 0.22	1.69 ± 1.46	14.72 ± 27.57	380.88 ± 759.94
	ICRL	0.01 ± 0.03	0.03 ± 0.01	0.49 ± 0.02	1.02 ± 0.03
	ERM	0.06 ± 0.01	0.20 ± 0.06	6.56 ± 6.01	220.79 ± 229.97
Nonlinear	IRM	0.08 ± 0.01	0.20 ± 0.04	5.06 ± 3.06	149.60 ± 104.85
	F-IRM GAME	64.48 ± 42.18	954.42 ± 634.18	15606.28 ± 10435.44	360114.72 ± 241206.47
	V-IRM GAME	0.90 ± 0.21	1.74 ± 0.34	24.72 ± 16.34	780.67 ± 729.68
	ICRL	0.31 ± 0.03	0.80 ± 0.02	3.02 ± 0.12	30.38 ± 3.67
O = g(X) = X.(b) Linear: g(∙) is a random matrix S ∈ R2×10, i.e., O = g(X) = X ∙ S. (c)
Nonlinear: g(∙) is implemented by a multilayer perceptron with the 2 dimensional input and the 10
dimensional output, whose parameters are randomly initialized in advance. For simplicity, here we
consider the regression task, in which the mean squared error (MSE) is used as a metric.
We consider an extremely simple scenario in which we fix σ1 = 1 and σ2 = 0 for all environments
and only allow σ3 to vary across environments. In this case, σ3 controls how much the representation
depends on the variable X2 , which is responsible for the spurious correlations. Each experiment
draws 1000 samples from each of the five environments σ3 = {0.2, 2, 5, 20, 100}, where the first
two are for training and the rest for testing. We compare with several closely related baselines6:
ERM, and two variants of IRMG: F-IRM Game (with Φ fixed to the identity) and V-IRM Game
(with a variable Φ).
As shown Table 1, in the cases of Identity and Linear, our approach is better than IRMG but only
comparable with ERM and IRM. This might be because the identifiability result up to a pointwise
nonlinear transformation renders the problem more difficult than itself, that is, converting the orig-
inal identity or linear problem to a nonlinear problem. In the Nonlinear case, it is clear that the
advantage of our approach becomes more obvious as the spurious correlation becomes stronger.
We also perform a series of experiments to further analyze our approach, including the analysis on
the importance of Assumption 1 and on the necessity of iVAE in Phase 1, how accurately the direct
causes can be recovered in Phase 2, and how well the two optimization problems can be addressed
in Phase 3, all of which can be found in Appendix G.
5.2 COLORED MNIST AND COLORED FASHION MNIST
In this section, we conduct experiments on two datasets used in IRM and IRMG: Colored MNIST
and Colored Fashion MNIST. We follow the same setting of Ahuja et al. (2020) to create these two
datasets. The task is to predict a binary label assigned to each image which is originally grayscale
but artificially colored in a way that correlated strongly but spuriously with the class label. We com-
pare with 1) IRM, 2) two variants of IRMG: F-IRM Game (with Φ fixed to the identity) and V-IRM
Game (with a variable Φ), 3) three variants of ERM: ERM (on entire training data), ERM e (on each
environment e), and ERM GRAYSCALE (on data with no spurious correlations), 4) ROBUST MIN
MAX (minimizing the maximum loss across the multiple environments). Table 2 shows that our ap-
proach significantly outperforms all the others on Colored Fashion MNIST. It is worth emphasising
that both train and test accuracies of our method closely approach the ones of ERM GRAYSCALE
and OPTIMAL, implying that it does approximately learn the true invariant causal representation
with nearly no correlation with the color. We can achieve a similar conclusion from the results on
Colored MNIST as shown in Table 3. However, this dataset is seemingly more difficult than the
fashion version, because ERM GRAYSCALE still underperforms even though the spurious correla-
tion with the color is removed, implying that it might involve some other spurious correlations. In
this case, two training environments might be not enough to eliminate all the spurious correlations.
6We also tried ICP, but surprisingly ICP cannot find any parent of Y even in the identity case.
8
Under review as a conference paper at ICLR 2021
Table 2: Colored Fashion MNIST: Comparison of
methods in terms of accuracy (mean ± std deviation).
ALGORITHM	TRAIN ACCURACY	TEST ACCURACY
ERM	83.17 ± 1.01	22.46 ± 0.68
ERM 1	81.33 ± 1.35	33.34 ± 8.85
ERM 2	84.39 ± 1.89	13.16 ± 0.82
ROBUST MIN MAX	82.81 ± 0.11	29.22 ± 8.56
F-IRM GAME	62.31 ± 2.35	69.25 ± 5.82
V-IRM GAME	68.96 ± 0.95	70.19 ± 1.47
IRM	75.01 ± 0.25	55.25 ± 12.42
ICRL	74.32 ± 0.43	73.14 ± 0.56
ERM GRAYSCALE	74.79 ± 0.37	74.67 ± 0.48
OPTIMAL	75	75
Table 3: Colored MNIST: Comparison of methods
in terms of accuracy (mean ± std deviation).
ALGORITHM	TRAIN ACCURACY	TEST ACCURACY
ERM	84.88 ± 0.16	10.45 ± 0.66
ERM 1	84.84 ± 0.21	10.86 ± 0.52
ERM 2	84.95 ± 0.20	10.05 ± 0.23
ROBUST MIN MAX	84.25 ± 0.43	15.24 ± 2.45
F-IRM GAME	63.37 ± 1.14	59.91 ± 2.69
V-IRM GAME	63.97 ± 1.03	49.06 ± 3.43
IRM	59.27 ± 4.39	62.75 ± 9.59
ICRL	70.34 ± 0.29	66.21 ± 1.42
ERM GRAYSCALE	71.81 ± 0.47	71.36 ± 0.65
OPTIMAL	75	75
6	Related Work
Invariant Prediction Peters et al. (2015) originally introduced the theory of Invariant Causal Pre-
diction (ICP), aiming to find the causal feature set (i.e., all direct causes of a target variable of
interest.) by exploiting the invariance property in causality which has been widely discussed under
the term “autonomy”, “modularity”, and “stability” (Haavelmo, 1944; Aldrich, 1989; Hoover, 1990;
Pearl, 2009; DaWid et al., 2010; Scholkopf et al., 2012). This invariance property assumed in ICP
and its nonlinear extension (Heinze-Deml et al., 2018) is limited, because no intervention is allowed
on the target variable Y . Besides, ICP methods implicitly assume that variables of interest X are
given. Magliacane et al. (2018) and SubbasWamy et al. (2019) attempt to find invariant predictors
that maximally predictive using conditional independence tests and other graph-theoretic tools, both
of Which also assume that X are given and further assume that additional information about the
structure over X is knoWn. Arjovsky et al. (2019) reformulate this invariance as an optimization-
based problem, alloWing us to learn the invariant data representation from O that is required to
be linear transformations of X . Ahuja et al. (2020) extend IRM to the nonlinear setting from the
perspective of game theory, but their nonlinear theory holds only in training environments.
Domain Generalization Doman generalization emphasizes the ability to transfer acquired knoWl-
edge to domains unseen during training. A Wide range of methods has been proposed for learning
domain-invariant representations. Khosla et al. (2012) develop a max-margin classifier that explic-
itly exploits the effect of dataset bias and improve generalization ability to unseen domains. Fang
et al. (2013) propose a metric learning approach based on structural SVM such that the neighbors
of each training sample consist of examples from both the same and different domains. Muandet
et al. (2013) propose a kernel-based optimization algorithm called Domain-Invariant Component
Analysis (DICA), Which aims to both minimize the discrepancy among domains and prevent the
loss of relationship betWeen input and output features. Ghifary et al. (2015) train a multi-task au-
toencoder that recognizes invariances among domains by learning to reconstruct analogs of original
inputs from different domains. Motiian et al. (2017) learn an embedding subspace Where samples
from different domains are close if they have the same class labels, and far aWay if they bear differ-
ent class labels. Li et al. (2018b) minimizes the differences in joint distributions to achieve target
domain generalization through the application of a conditional invariant adversarial netWork. Li
et al. (2018a) build on the adversarial autoencoders by considering maximum mean discrepancy
regularization and aligning the domains’ distributions.
7	Conclusion
We developed a novel frameWork to learn invariant predictors from a diverse set of training envi-
ronments. This frameWork is based on a practical and general assumption: the data representation
can be factorized When conditioning on the outcome and the environment. The assumption leads
to a guarantee that the components in the representation can be identified up to a permutation and
pointWise transformation. This alloWs us to further discover all the direct causes of the outcome,
Which enables generalization guarantees in the nonlinear setting. We hope our frameWork Would
inspire neW Ways to address the OOD generalization problem through the causal lens.
9
Under review as a conference paper at ICLR 2021
References
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk min-
imization games. arXiv preprint arXiv:2002.04692, 2020.
John Aldrich. Autonomy. Oxford Economic Papers, 41(1):15-34, 1989.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of
the European Conference on Computer Vision (ECCV), pp. 456-473, 2018.
A Philip Dawid, Vanessa Didelez, et al. Identifying the consequences of dynamic treatment strate-
gies: A decision-theoretic overview. Statistics Surveys, 4:184-231, 2010.
Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased metric learning: On the utilization of
multiple datasets and web images for softening bias. In ICCV, pp. 1657-1664. IEEE Computer
Society, 2013.
Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain gener-
alization for object recognition with multi-task autoencoders. In ICCV, pp. 2551-2559. IEEE
Computer Society, 2015.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.
Trygve Haavelmo. The probability approach in econometrics. Econometrica: Journal of the Econo-
metric Society, pp. iii-115, 1944.
Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal prediction for
nonlinear models. Journal of Causal Inference, 6(2), 2018.
Kevin D Hoover. The logic of causal inference: Econometrics and the conditional analysis of cau-
sation. Economics & Philosophy, 6(2):207-234, 1990.
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlin-
ear causal discovery with additive noise models. In Advances in neural information processing
systems, pp. 689-696, 2009.
Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero, Clark Glymour,
and Bernhard Scholkopf. Causal discovery from heterogeneous/nonstationary data. Journal of
Machine Learning Research, 21(89):1-53, 2020.
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and
generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 859-868, 2019.
Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Scholkopf, et al. Quantifying
causal influences. The Annals of Statistics, 41(5):2324-2358, 2013.
Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoen-
coders and nonlinear ica: A unifying framework. In International Conference on Artificial Intel-
ligence and Statistics, pp. 2207-2217, 2020.
Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba. Undoing
the damage of dataset bias. In ECCV (1), volume 7572 of Lecture Notes in Computer Science, pp.
158-171. Springer, 2012.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adver-
sarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5400-5409, 2018a.
10
Under review as a conference paper at ICLR 2021
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 624-639, 2018b.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In international conference on machine learning, pp. 4114-4124,
2019.
Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M
Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions.
In Advances in Neural Information Processing Systems, pp. 10846-10856, 2018.
Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Unified deep super-
vised domain adaptation and generalization. In ICCV, pp. 5716-5726. IEEE Computer Society,
2017.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In ICML (1), volume 28 of JMLR Workshop and Conference Proceedings,
pp. 10-18. JMLR.org, 2013.
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference using invariant predic-
tion: identification and confidence intervals. arXiv preprint arXiv:1501.01332, 2015.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference. The MIT
Press, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.
On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.
Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction,
and search. MIT press, 2000.
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo Hyvarinen, and Revant Kumar.
Density estimation in infinite dimensional exponential families. The Journal of Machine Learning
Research, 18(1):1830-1888, 2017.
Adarsh Subbaswamy, Bryant Chen, and Suchi Saria. A universal hierarchy of shift-stable distri-
butions and the tradeoff between stability and performance. arXiv preprint arXiv:1905.11374,
2019.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural infor-
mation processing systems, pp. 831-838, 1992.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5018-5027, 2017.
Sewall Wright. Correlation and causation. J. agric. Res., 20:557-580, 1921.
Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Kernel-based conditional
independence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.
Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Scholkopf. Causal discovery
from nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In
IJCAI: Proceedings of the Conference, volume 2017, pp. 1347. NIH Public Access, 2017.
11
Under review as a conference paper at ICLR 2021
A	Variational Autoencoders
We briefly describe the identifiable variational autoencoders (iVAEs) proposed by Khemakhem et al.
(2020). As we know, the framework of variational autoencoders (VAEs) (Kingma & Welling, 2013;
Rezende et al., 2014) allows us to efficiently learn deep latent-variable models and their correspond-
ing inference models. Consider a simple latent variable model where O ∈ Rd stands for an observed
variable (random vector) and X ∈ Rn for a latent variable. The VAE model actually learns a full
generative model pθ(O, X) = pθ(O|X)pθ(X) and an inference model qφ(X|O) that approxi-
mates its posterior pθ(X |O), where θ is a vector of parameters of the generative model, φ a vector
of parameters of the inference model, and pθ(X) is a prior distribution over the latent variables.
Instead of maximizing the data log-likelihood, we maximize its lower bound LVAE(θ, φ):
logPθ(O) ≥ LVAE(θ, φ) := Eqφ(x∣o) [logPθ(O|X)] - KL (qφ(X∣O)l∣Pθ(X)),
where We have used Jensen,s inequality, and KL(∙∣∣∙) denotes the Kullback-Leibler divergence be-
tween two distributions.
B	Definitions
For convenience, we restates some definitions here and please refer to the original papers (Arjovsky
et al., 2019; Peters et al., 2017) for more details.
Definition 2. A structural equation model (SEM) C := (S, N) governing the random vector X =
(X1, . . . , Xd) is a set of structural equations:
Si : Xi — fi(Pa(Xi), Ni),
where Pa(Xi) ⊆ {X1, . . . , Xd} \ {Xi} are called the parents of Xi, and the Ni are independent
noise random variables. We say that “Xi causes Xj ” if Xi ∈ Pa(Xj ). We call causal graph of
X to the graph obtained by drawing i) one node for each Xi, and ii) one edge from Xi to Xj if
Xi ∈ Pa(Xj ). We assume acyclic causal graphs.
Definition 3. Consider a SEM C := (S, N). An intervention e on C consists of replacing one or
several of its structural equations to obtain an intervened SEM Ce := (Se, Ne), with structural
equations:
Se : χe 一 fie(Pae(χe),Nie),
The variable Xe is intervened if Si 6= Sie or Ni 6= Nie.
Definition 4. Consider a structural equation model (SEM) S governing the random vector
(X1, . . . , Xn, Y ), and the learning goal of predicting Y from X. Then, the set of all environ-
ments Eall (S) indexes all the interventional distributions P(Xe, Ye) obtainable by valid interven-
tions e. An intervention e ∈ Eall (S) is valid as long as (i) the causal graph remains acyclic, (ii)
E [Ye|Pa(Y )] = E [Y |Pa(Y )], and (iii) V [Ye|Pa(Y )] remains within a finite range.
C Derivation
In Phase 1, the lower bound is defined by
Lphasei (θ, φ) ：=EpD [Eqφ(x∣o,γ ,E) [log Pθ (O, X∣Y,E) -log qφ (X |O, Y ,E)]]
=EpD [Eqφ(χ∣o,γ,E) [logPf (O|X) + logPT,λ(X| Y, E) — logqφ(X∣O, Y, E)]]
=EPD [Eqφ(X∣O,Y ,E) [log Pf (OIX )]+ Eqφ(X∣O,Y ,E) [log PT ,λ(XIY,E)]
-Eqφ(X∣O,Y,E) [log qφ(X|O, Y, E)]].
The first term is the data log-likelihood and the third term has a closed-form solution,
J	1J
Eqφ(X∣O,Y ,E) [log qφ(X lO, Y,E)] = - 2log(2π) + $ £ (1 +log σj).
j=1
where σj is simply denote the j-th element of the variational s.d. (σ) evaluated at datapoint i that is
simply a function of (O, Y, E) and the variational parameters φ.
12
Under review as a conference paper at ICLR 2021
Now let us look at the second term,
Eqφ(X∣O,Y,E) [log PT ,λ(X IY ,E)]
IRI	1	Q π Qi(Xi)
=Eqφ(X∣O,Y ,E) log I ɪɪ Zi(Y,E4 exp
κ	Q Qi(Xi)
=Eqφ(X∣O,Y ,E) 2^log I Zi(Y,E') exp
k
XTi,j(Xi)λi,j(Y,E)
j=1
k
XTi,j(Xi)λi,j(Y,E)
j=1
k
∞Eqφ(X∣O,Y ,E) IXX Ti,j (Xi)λi,j (Y,E)
1
≈ L XXX Tij (Xi )λi,j (Y,E1),
l i j=1
where we let the base measureQi(Xi) = 1 and L is the sample size.
D	Theorems
Theorem 4. Assume that we observe data sampled from a generative model defined according to
Eqs. (5-7), with parameters θ = (f, T , λ) and k = 1. Assume the following holds: (i) The set
{O ∈ O∣^e(O) = 0} has measure zero, where φe is the characteristic function of the density PW
defined in Eq. (6). (ii) The mixing function f in Eq. (6) is injective, and all partial derivatives of
f are continuous. (iii) The sufficient statistics Ti,j in Eq. (7) are differentiable almost everywhere
and not monotonic, and (Ti,j)1≤j ≤k are linearly independent on any subset of X of measure greater
than zero. (iv) There exist nk + 1 distinct points (Y, E)0, . . . , (Y, E)nk such that the matrix L =
(λ((Y, E)1) - λ((Y, E)0),..., λ((Y, E)nk) - λ((Y, E)0)) ofsize nk X nk is invertible. Then
the parameters θ = (f, T, λ) are identifiable up to a permutation and pointwise transformation.
E	Proofs
E.1 Proof of Theorem 2
This proof is similar to that of Theorem 4 in Khemakhem et al. (2020)
Proof. The loss function in Eq. 3 can be rephrased as follows:
Lphaseι(θ, φ) =logPθ(O∣Y,E) — KL(qφ(X∖O,Y,E)∣∣PΘ(X∣O, Y,E)).
If the family of qφ(X∣O, Y, E) is flexible enough to contain pθ(X∣O, Y, E), then by optimizing
the loss over its parameter φ, we will minimize the KL term which will eventually reach zero, and the
loss will be equal to the log-likelihood. Under this circumstance, the iVAE inherits all the properties
of maximum likelihood estimation. In this particular case, since our identifiability is guaranteed
up to a permutation and pointwise transformation, the consistency of MLE means that we converge
to the true parameter θ* UP to a permutation and pointwise transformation in the limit of infinite
data. Because true identifiability is one of the assumptions for MLE consistency, replacing it by
identifiability up to a permutation and pointwise transformation does not change the proof but only
the conclusion.	□
E.2 Proof of Theorem 3
Proof. Theorem 1 and Theorem 2 guarantee that in the limit of infinite data, iVAE can learn the true
parameters θ* := (f *, T*, λ*) up to a permutation and pointwise transformation. Let (f, T, λ)
be the parameters obtained by iVAE, and We therefore have (f, T, λ)〜P (f *, T*, λ*), where
〜P denotes the equivalence up to a permutation and pointwise transformation. If there were no
13
Under review as a conference paper at ICLR 2021
♦	. ) ∙	1 1	. i . . i 1	1 f* .	i`	j<-* ♦ . ST-	P_1 ∕›∙~x∖ .ι	1 . rτ^ ⅛
noise, this would mean that the learned f transforms O into X = f 1 (O) that are equal to X* =
(f *)-1 (O) UP to a permutation and sighed scaling. If with noise, We obtain the posteriors of the
latents up to an analogous indeterminacy.	□
E.3 Proof of Proposition 1
Proof. The following rules can be independently performed to distinguish all the 12 possible struc-
tures shown in Figs. 2b-2m. For clarity, we divide them into three groups.
Group 1 All the eight structures in this group can be discovered only by performing conditional
independence tests.
•	Rule 1.1	If	Xi	⊥⊥	Y , Xi	⊥⊥ E, and E	⊥⊥	Y ,	then	Fig.	2b is discovered.
•	Rule 1.2	If	Xi	⊥⊥	Y , Xi	⊥⊥ E, and E	⊥6⊥	Y ,	then	Fig.	2h is discovered.
•	Rule 1.3	If	Xi	⊥⊥	Y , Xi	⊥6⊥ E, and E	⊥⊥	Y ,	then	Fig.	2e is discovered.
•	Rule 1.4	If	Xi	⊥6⊥	Y , Xi	⊥⊥ E, and E	⊥6⊥	Y ,	then	Fig.	2i is discovered.
•	Rule 1.5	If	Xi	⊥6⊥	Y , Xi	⊥6⊥ E, and E	⊥⊥	Y ,	then	Fig.	2g is discovered.
•	Rule 1.6	IfXi	⊥6⊥	Y, Xi	⊥6⊥ E,E ⊥6⊥ Y,andXi ⊥⊥ Y |E, then Fig.	2k is discovered.
•	Rule 1.7	IfXi	⊥6⊥	Y, Xi	⊥6⊥ E,E ⊥6⊥ Y,andXi ⊥⊥ E|Y, then Fig.	2j is discovered.
•	Rule 1.8	IfXi	⊥6⊥	Y, Xi	⊥6⊥ E,E ⊥6⊥ Y,andY ⊥⊥ E|Xi, then Fig.	2f is discovered.
Group 2 If Xi	⊥6⊥	Y	, Xi ⊥⊥ E, and E ⊥⊥ Y , then we can discover both Fig. 2c and Fig.
2d. These two structures cannot be further distinguished only by conditional independence tests,
because they come from the same Markov equivalence class. Fortunately, we can further distinguish
them by running binary causal discovery algorithms (Peters et al., 2017), e.g., ANM (Hoyer et al.,
2009) or the bivariate fit model that is based on a best-fit criterion relying on a Gaussian Process
regressor.
•	Rule 2.1 If Xi ⊥6⊥ Y ,	Xi ⊥⊥ E, and	E ⊥⊥ Y , and a chosen binary	causal discovery
algorithm prefers Xi →	Y to Xi J Y,	then Fig. 2c is discovered.
•	Rule 2.2 If Xi ⊥6⊥ Y,	Xi ⊥⊥ E, and	E ⊥⊥ Y, and a chosen binary	causal discovery
algorithm prefers Xi J	Y to Xi → Y,	then Fig. 2d is discovered.
Group 3 IfXi ⊥6⊥ Y, Xi ⊥6⊥ E, E ⊥6⊥ Y, Xi ⊥6⊥ Y|E, Xi ⊥6⊥ E|Y, andY ⊥6⊥ E|Xi, then we
can discover both Fig. 2l and Fig. 2m. These two structures cannot be further distinguished only by
conditional independence tests, because they come from the same Markov equivalence class. They
also cannot be distinguished by any binary causal discovery algorithm, since both Xi and Y are
affected by E. Fortunately, Zhang et al. (2017) provided a heuristic solution to this issue based on
the invariance of causal mechanisms, i.e., P (cause) and P (effect|cause) change independently. The
detailed description of their method is given in Section 4.2 of Zhang et al. (2017). For convenience,
here we directly borrow their final result. Zhang et al. (2017) states that determining the causal
direction between Xi and Y in Fig. 2l and Fig. 2m is finally reduced to calculating the following
term:
△X→y = *iog P(YXi) + ,	(11)
i ∖	hP(Y ∣Xi)i∕
where〈•〉denotes the sample average, P(Y|Xi) is the empirical estimate of P(Y|Xi) on all data
points, and hP(Y|Xi)i denotes the sample average ofP(Y|Xi), which is the estimate ofP(Y|Xi)
in each environment. We take the direction for which △ is smaller to be the causal direction.
14
Under review as a conference paper at ICLR 2021
•	Rule 3.1 IfXi ⊥6⊥ Y, Xi ⊥6⊥ E, E ⊥6⊥ Y, Xi ⊥6⊥ Y |E, Xi ⊥6⊥ E|Y, Y ⊥6⊥ E|Xi, and
∆Xi→Y is smaller than ∆Y→Xi, then Fig. 2l is discovered.
•	Rule 3.2 IfXi ⊥6⊥ Y, Xi ⊥6⊥ E, E ⊥6⊥ Y, Xi ⊥6⊥ Y |E, Xi ⊥6⊥ E|Y, Y ⊥6⊥ E|Xi, and
∆Y →Xi is smaller than ∆Xi→Y , then Fig. 2m is discovered.
□
E.4 Proof of Proposition 2
Proof. Firstly, assumption (iii) and assumption (iv) in Theorem 1 are the requirements of the set of
training environments containing sufficient diversity and satisfying an underlying invariance which
holds across all the environments. Interestingly, assumption (iii) elicits Lemma 4 of Khemakhem
et al. (2020), which is closely similar to the linear general position in Assumption 8 of Arjovsky et al.
(2019). Thus, Lemma 4 can be similarly called the nonlinear general position in our generalization
theory, whose proof can be found in Arjovsky et al. (2019). Secondly, when the set of training
environments lie in this nonlinear general position and the other hypotheses of Theorem 1&2 hold,
it is guaranteed in Theorem 3 that all the latent factors X can be identified up to a permutation and
pointwise transformation. Since this identifiability result holds under the assumptions guaranteeing
that training environments contain sufficient diversity and satisfy an underlying invariance which
holds across all the environments, it also holds across all the environments. Thirdly, Proposition
1 suggests that all the direct causes Pa(Y ) of Y can be fully discovered, which also holds across
all the environments due to the same reason above. Finally, the challenging bi-leveled optimization
problem in both IRM and IRMG now can be reduced to two simpler independent optimization
problems: (i) learning the invariant data representation Φ from O to Pa(Y ), and (ii) learning the
invariant classifier w from Pa(Y ) to Y , as described in Eq. (9) and Eq. (10). For both (i) and
(ii), since there exist no spurious correlations between O and Pa(Y ) and between Pa(Y ) and Y ,
learning theory guarantees that in the limit of infinite data, we will converge to the true invariant
data representation Φ and the true invariant classifier w.	□
It is worth noting that although assumption (iii) and assumption (iv) in Theorem 1 require compli-
cated conditions to satisfy the diversity across training environments for generalization guarantees,
it is not the case in practice. As we will observe in our experiments, it is often the case that two
environments are sufficient to recover invariances.
F Datasets
For convenience and completeness, we provide the descriptions of Colored MNIST Digits, Colored
Fashion MNIST, and Office-Home here. Please refer to the original papers (Arjovsky et al., 2019;
Ahuja et al., 2020; Gulrajani & Lopez-Paz, 2020; Venkateswara et al., 2017) for more details.
F.1 Synthetic Data
For the nonlinear transformation, we use the MLP:
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 10
F.2 Colored MNIST Digits
We use the exact same environment as in Arjovsky et al. (2019). Arjovsky et al. (2019) propose
to create an environment for training to classify digits in MNIST digits data7, where the images
in MNIST are now colored in such a way that the colors spuriously correlate with the labels. The
7https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/
load_data
15
Under review as a conference paper at ICLR 2021
task is to classify whether the digit is less than 5 (not including 5) or more than 5. There are three
environments (two training containing 30,000 points each, one test containing 10,000 points) We
add noise to the preliminary label (y = 0 if digit is between 0-4 and y = 0 if the digit is between
5-9) by flipping it with 25 percent probability to construct the final labels. We sample the color
id z by flipping the final labels with probability pe, where pe is 0.2 in the first environment, 0.1
in the second environment, and 0.9 in the third environment. The third environment is the testing
environment. We color the digit red if z = 1 or green if z = 0.
F.3 Colored Fashion MNIST
We modify the fashion MNIST dataset8 in a manner similar to the MNIST digits dataset. Fashion
MNIST data has images from different categories: “t-shirt”, “trouser”, “pullover”, “dress”, “coat”,
“sandal”, “shirt”, “sneaker”, “bag”, “ankle boots”. We add colors to the images in such a way
that the colors correlate with the labels. The task is to classify whether the image is that of foot
wear or a clothing item. There are three environments (two training, one test) We add noise to
the preliminary label (y = 0: “t-shirt”，“trouser”，“pullover”，“dress”，“coat”，“shirt" and y = 1:
“sandal”, “sneaker”, “ankle boots”) by flipping it with 25 percent probability to construct the final
label. We sample the color id z by flipping the noisy label with probability pe， where pe is 0.2 in the
first environment， 0.1 in the second environment， and 0.9 in the third environment， which is the test
environment. We color the object red if z = 1 or green if z = 0.
Figure 3: Left: Comparison of the raw data X and the mean of X inferred through the learned
inference model in the Nonlinear case. Right: Comparison of the raw data X and the sampled
points X using the reparameterization trick in the Nonlinear case. The comparisons clearly show
that the inferred X is equal to X up to a permutation and pointwise transformation.
G In-depth Analysis on Synthetic Data
G.1 Verifying Phase 1
Theorem 3 tells us that we can leverage iVAE to learn the true conditionally factorized latent vari-
ables up to a permutation and pointwise transformation. We empirically verify this point by com-
paring the raw data X with the corresponding X inferred through the learned inference model in
iVAE. Fig. 3 clearly shows that the inferred X is equal to X up to a permutation and pointwise
transformation.
8https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_
mnist/load_data
16
Under review as a conference paper at ICLR 2021
o-
0
2 1
山 SW~ajh-
20000 40000 60000 80000 100000
Step
4 2
山 SWttωh-
0-
0
20000 40000 60000 80000 100000
Step
(a)	Regression results in the Linear case in terms of MSE, where the inferred X2 is the identified cause.
山 sς~ajh-
1.00-
0.75 -
0.50-
2.5-
ω 2.0-
W
t⅛ 1.5-
ω
I—
1.0-
0	20000 40000 60000 80000 100000	0	20000 40000 60000 80000 100000
Step	Step
(b)	Regression results in the Nonlinear case in terms of MSE, where the inferred X2 is the identified cause.
>U23U⅛ ⊂-2H
Auajnuufsωh-
0	20000 40000 60000 80000 100000	0	20000 40000 60000 80000 100000
Step	Step
(c)	Classification results on Synthetic Data in terms of accuracy, where the inferred Xi is the identified cause.
τr-<∙	A e-∖	∙	,1	∙ i'	F ʌr	F ʌr ∙	C ,1	♦ C 1	i~
Figure 4:	Comparison of the inferred X1 and X2 in terms of their final performance.
To show the importance of iVAE, we conduct an experiment in which we replace iVAE with the
original VAE in Phase 1. As shown in Table 4, the performance of ICRL based on iVAE significantly
outperforms the one based on VAE. It is worth noting that when VAE is instead used in Phase 1,
it usually occurs in Phase 2 that either all the dimensions or no dimension of X are identified as
the parents of Y. This is because all components of X are mixed together and will influence one
another even when conditioning on Y and E .
G.2 Verifying Phase 2
To show how well our method can identify the direct causes of Y in Phase 2, we compare the
final performance when the identified direct cause (i.e., X1) and the identified non-cause (i.e., X2)
are respectively used in Phase 3 to learn the predictor. Note that, there might exist a permutation
between the inferred {X1, X2} and the true {X1, X2}. Fig. 4a and Fig. 4b show the results on the
regression task, from which we can obviously see that the predictor elicited by the identified cause
has a much better generalization performance. The classification result (i.e., Y is binarized) in Fig.
4c further demonstrates this point.
G.3 Verifying Phase 3
In this experiment, we want to verify how well the data representation Φ can be learned by optimiz-
ing the loss in Eq. (9). The main idea is to check how well the learned Φ can purely extract the
17
Under review as a conference paper at ICLR 2021
一.	一 一~一	,	~ 八	ʌ	一、 、一 .一一	,	~ C	Z Z _、 、	_ _
Figure 5:	Left: Energy plot of Xi = fχι (g(X)). Right: Energy plot of X = fχ>(g(X)). Note
that, here Φχi is denoted by fχi .
Table 4: Results on synthetic Data: Comparison of iVAE and VAE used in Phase 1 in terms of MSE (mean ±
std deviation).
X	ALGORITHM	TRAIN MSE	TEST MSE
TRANSFORMER ALGORITHM (σ3 = {0.2, 2})	(σ3 = 5)
TEST MSE
(σ3 = 20)
TEST MSE
(σ3 = 100)
Nonlinear
ICRL-VAE
ICRL-iVAE
0.26 ± 0.09	4.26 ± 4.00 42.03 ± 48.67 1174.96 ± 1385.81
0.31 ± 0.03 0.80 ± 0.02 3.02 ± 0.12	30.38 ± 3.67
cause X1 from O . For this, we first learn Φχi for each Xi . Formally, we have
^ ^ .... .
Xi = φXi(g(X)), where X = (X1,X2).
l-{z-}
O
Then, we observe how Xi will change while tuning Xi and X2 respectively. Fig. 5 shows the energy
plots when Xi is the identified cause of Y and X2 the child of Y . Note that, in the plots, ΦXi is
denoted by fXi . In theory, we are able to learn an invariant data representation ΦX1 for Xi from
O, because there is no spurious correlation between Xi and O. By contrast, we cannot learn an
invariant data representation ΦX2 for X2 from O, because there exist spurious correlations between
X2 and O. The results shown in Fig. 5 clearly verify our theory. Specifically, in the left plot, Xi
remains approximately unchanged when changing X2 but it changes when changing Xi . However,
in the right plot, X】changes whether we change Xi or X].
H Model Architectures
In this section, we describe the architectures of different models used in different experiments.
H.1 Synthetic Data
H.1.1 ERM
Linear ERM
•	Input layer: Input batch (batch size, input dimension)
•	Output layer: Fully connected layer, output size = 1
18
Under review as a conference paper at ICLR 2021
Nonlinear ERM
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 1
H.1.2 IRM
Linear Data Representation Φ
•	Input layer: Input batch (batch size, input dimension)
•	Output layer: Fully connected layer, output size = 1
Nonlinear Data Representation Φ
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 1
H.1.3 F-IRM GAME
Linear Classifier w
•	Input layer: Input batch (batch size, input dimension)
•	Output layer: Fully connected layer, output size = 1
Nonlinear Classifier w
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 1
H.1.4 V-IRM GAME
Linear Data Representation Φ
•	Input layer: Input batch (batch size, input dimension)
•	Output layer: Fully connected layer, output size = 2
Nonlinear Data Representation Φ
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 2
Linear Classifier w
•	Input layer: Input batch (batch size, 2)
•	Output layer: Fully connected layer, output size = 1
Nonlinear Classifier w
•	Input layer: Input batch (batch size, 2)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 1
19
Under review as a conference paper at ICLR 2021
H.1.5 ICRL
iVAE Linear Prior
•	Input layer: Input batch (batch size, input dimension)
•	Mean Output layer: 0, which is a vector full of 0 with the length 2
•	Log Variance Output layer: Fully connected layer, output size = 2
iVAE Nonlinear Prior
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Mean Output layer: 0, which is a vector full of 0 with the length 2
•	Log Variance Output layer: Fully connected layer, output size = 2
iVAE Linear Encoder
•	Input layer: Input batch (batch size, input dimension)
•	Mean Output layer: Fully connected layer, output size = 2
•	Log Variance Output layer: Fully connected layer, output size = 2
iVAE Nonlinear Encoder
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Mean Output layer: Fully connected layer, output size = 2
•	Log Variance Output layer: Fully connected layer, output size = 2
iVAE Linear Decoder
•	Input layer: Input batch (batch size, 2)
•	Mean Output layer: Fully connected layer, output size = output dimension
•	Variance Output layer: 0.01 × 1, where 1 is a vector full of 1 with the length of output
dimension
iVAE Nonlinear Decoder
•	Input layer: Input batch (batch size, 2)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Mean Output layer: Fully connected layer, output size = output dimension
•	Variance Output layer: 0.01 × 1, where 1 is a vector full of 1 with the length of output
dimension
Linear Data Representation Φ
•	Input layer: Input batch (batch size, input dimension)
•	Output layer: Fully connected layer, output size = 1
Nonlinear Data Representation Φ
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 1
20
Under review as a conference paper at ICLR 2021
Linear Classifier w
•	Input layer: Input batch (batch size, 1)
•	Output layer: Fully connected layer, output size = 1
Nonlinear Classifier w
•	Input layer: Input batch (batch size, 1)
•	Layer 1: Fully connected layer, output size = 6, activation = ReLU
•	Output layer: Fully connected layer, output size = 1
H.2 Colored MNIST Digits and Colored Fashion MNIST
iVAE Prior
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 100, activation = ReLU
•	Mean Output layer: 0, which is a vector full of 0 with the length 100
•	Log Variance Output layer: Fully connected layer, output size = 100
iVAE O-Encoder
•	Input layer: Input batch (batch size, 2, 28, 28)
•	Layer 1: Convolutional layer, output channels = 32, kernel size = 3, stride = 2, padding =
1, activation = ReLU
•	Layer 2: Convolutional layer, output channels = 32, kernel size = 3, stride = 2, padding =
1, activation = ReLU
•	Layer 3: Convolutional layer, output channels = 32, kernel size = 3, stride = 2, padding =
1, activation = ReLU
•	Output layer: Flatten
iVAE (Y, E)-Encoder
•	Input layer: Input batch (batch size, input dimension)
•	Output layer: Fully connected layer, output size = 100, activation = ReLU
iVAE (O, Y , E)-Merger/Encoder
•	Input layer: Input batch (batch size, input dimension)
•	Layer 1: Fully connected layer, output size = 100, activation = ReLU
•	Mean Output layer: Fully connected layer, output size = 100
•	Log Variance Output layer: Fully connected layer, output size = 100
iVAE Decoder
•	Input layer: Input batch (batch size, 100)
•	Layer 1: Fully connected layer, output size = 32 × 4 × 4, activation = ReLU
•	Layer 2: Reshape to (batch size, 32, 4, 4)
•	Layer 3: Deconvolutional layer, output channels = 32, kernel size = 3, stride = 2, padding
= 1, outpadding = 0, activation = ReLU
•	Layer 4: Deconvolutional layer, output channels = 32, kernel size = 3, stride = 2, padding
= 1, outpadding = 1, activation = ReLU
21
Under review as a conference paper at ICLR 2021
•	Layer 5: Deconvolutional layer, output channels = 2, kernel size = 3, stride = 2, padding =
1, outpadding = 1
•	Mean Output layer: activation = Sigmoid
•	Variance Output layer: 0.01 × 1, where 1 is a matrix full of 1 with the size of 2 × 28 × 28.
Data Representation Φ
•	Input layer: Input batch (batch size, 2, 28, 28)
•	Layer 1: Convolutional layer, output channels = 32, kernel size = 3, stride = 2, padding =
1, activation = ReLU
•	Layer 2: Convolutional layer, output channels = 32, kernel size = 3, stride = 2, padding =
1, activation = ReLU
•	Layer 3: Convolutional layer, output channels = 32, kernel size = 3, stride = 2, padding =
1, activation = ReLU
•	Layer 4: Flatten
•	Mean Output layer: Fully connected layer, output size = 100
•	Log Variance Output layer: Fully connected layer, output size = 100
Classifier w
•	Input layer: Input batch (batch size, 100)
•	Layer 1: Fully connected layer, output size = 100, activation = ReLU
•	Output layer: Fully connected layer, output size = 1, activation = Sigmoid
22