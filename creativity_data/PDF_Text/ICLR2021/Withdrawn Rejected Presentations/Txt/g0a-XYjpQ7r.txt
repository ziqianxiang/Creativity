Under review as a conference paper at ICLR 2021
Adaptive Personalized Federated Learning
Anonymous authors
Paper under double-blind review
Abstract
Investigation of the degree of personalization in federated learning algorithms has
shown that only maximizing the performance of the global model will confine the
capacity of the local models to personalize. In this paper, we advocate an adaptive
personalized federated learning (APFL) algorithm, where each client will train
their local models while contributing to the global model. We derive the gener-
alization bound of mixture of local and global models, and find the optimal mix-
ing parameter. We also propose a communication-efficient optimization method
to collaboratively learn the personalized models and analyze its convergence in
both smooth strongly convex and nonconvex settings. The extensive experiments
demonstrate the effectiveness of our personalization schema, as well as the cor-
rectness of established generalization theories.
1 Introduction
With the massive amount of data generated by the proliferation of mobile devices and the internet of
things (IoT), coupled with concerns over sharing private information, collaborative machine learning
and the use of federated optimization (FO) is often crucial for the deployment of large-scale machine
learning (McMahan et al., 2017; Kairouz et al., 2019; Li et al., 2020b). In FO, the ultimate goal is to
learn a global model that achieves uniformly good performance over almost all participating clients
without sharing raw data. To achieve this goal, most of the existing methods pursue the following
procedure to learn a global model: (i) a subset of clients participating in the training is chosen at
each round and receive the current copy of the global model; (ii) each chosen client updates the local
version of the global model using its own local data, (iii) the server aggregates over the obtained local
models to update the global model, and this process continues until convergence (McMahan et al.,
2017; Mohri et al., 2019; Karimireddy et al., 2019; Pillutla et al., 2019). Most notably, FedAvg
by McMahan et al. (2017) uses averaging as its aggregation method over local models.
Due to inherent diversity among local data shards and
highly non-IID distribution of the data across clients, Fe-
dAvg is hugely sensitive to its hyperparameters, and as a
result, does not benefit from a favorable convergence guar-
antee (Li et al., 2020c). In Karimireddy et al. (2019), au-
thors argue that if these hyperparameters are not carefully
tuned, it will result in the divergence of FedAvg, as local
models may drift significantly from each other. Therefore,
in the presence of statistical data heterogeneity, the global
model might not generalize well on the local data of each
client individually (Jiang et al., 2019). This is even more
crucial in fairness-critical systems such as medical diagno-
sis (Li & Wang, 2019), where poor performance on local
clients could result in damaging consequences. This prob-
lem is exacerbated even further as the diversity among lo-
cal data of different clients is growing. To better illustrate
this fact, we ran a simple experiment on MNIST dataset
where each client’s local training data is sampled from
a subset of classes to simulate heterogeneity. Obviously,
when each client has samples from less number of classes
⅜	Global	Model	FedAvg (Generalization)	―f—	Global Model SCAFFOLD (Training)
T-	Global	Model	FedAvg (Training)	⅜	Personalized Model (Generalization)
—⅛-	Global	Model	SCAFFOLD (Generalization)	—⅛-	Personalized Model (Training)
Figure 1: Comparing the generaliza-
tion and training losses of our proposed
personalized model with the global
models of FedAvg and SCAFFOLD by
increasing the diversity among the data
of clients on MNIST dataset with a lo-
gistic regression model.
of training data, the heterogeneity among them will be high and if each of them has samples from all
1
Under review as a conference paper at ICLR 2021
classes, the distribution of their local training data becomes almost identical, and thus heterogeneity
will be low. The results of this experiment are depicted in Figure 1, where the generalization and
training losses of the global models of the FedAvg (McMahan et al., 2017) and SCAFFOLD (Karim-
ireddy et al., 2019) on local data diverge when the diversity among different clients’ data increases.
This observation illustrates that solely optimizing for the global model’s accuracy leads to a poor
generalization of local clients. To embrace statistical heterogeneity and mitigate the effect of nega-
tive transfer, it is necessary to integrate the personalization into learning instead of finding a single
consensus predictor. This pluralistic solution for FO has recently resulted in significant research in
personalized learning schemes (Eichner et al., 2019; Smith et al., 2017; Dinh et al., 2020; Mansour
et al., 2020; Fallah et al., 2020; Li et al., 2020a).
To balance the trade-off between the benefit from collaboration with other users and the disadvan-
tage from the statistical heterogeneity among different users’ domains, in this paper, we propose an
adaptive personalized federated learning (APFL) algorithm which aims to learn a personalized
model for each device that is a mixture of optimal local and global models. We theoretically analyze
the generalization ability of the personalized model on local distributions, with dependency on mix-
ing parameter, the divergence between local and global distributions, as well as the number of local
and global training data. To learn the personalized model, we propose a communication efficient
optimization algorithm that adaptively learns the model by leveraging the relatedness between local
and global models as learning proceeds. As it is shown in Figure 1, by progressively increasing
the diversity, the personalized model found by the proposed algorithm demonstrates a better gener-
alization compared to the global models learned by FedAvg and SCAFFOLD. We supplement our
theoretical findings with extensive corroborating experimental results that demonstrate the superior-
ity of the proposed personalization schema over the global and localized models of commonly used
federated learning algorithms.
2 Personalized Federated Learning
In this section, we propose a personalization approach for federated learning and analyze its statis-
tical properties. Following the statistical learning theory, in a federated learning setting each client
has access to its own data distribution Di on domain
Ξ := X × Y, where X ∈ Rd is the input domain
and Y is the label domain. For any hypothesis h ∈ H the loss function is defined as ` : H× Ξ → R+ .
El ,	∙ 1 , 1 1 T , ∙1	∙ F ,FK C / 7 ∖ E	Γ Λ / 7 / ∖ ∖ T -c-r τ	Λ / 7 ∖
The true risk at local distribution is denoted by LDi(h) = E(χ,y)〜Di [' (h(x), y)]. We use LDi(h)
to denote the empirical risk of h on distribution D%. We use D = (1/n) Pn=ι Di to denote the
average distribution over all clients.
2.1	Personalized model
In a standard federated learning scenario, where the goal is to learn a global model for all devices
cooperatively, the learned global model obtained by minimizing the joint empirical distribution D,
i.e., minh∈H LD(h) by proper weighting. However, as alluded to before, a single consensus pre-
dictor may not perfectly generalize on local distributions when the heterogeneity among local data
shards is high (i.e., the global and local optimal models drift significantly). Meanwhile, from the
local user perspective, the key incentive to participate in “federated” learning is the desire to seek a
reduction in the local generalization error with the help of other users’ data. In this case, the ideal
situation would be that the user can utilize the information from the global model to compensate
for the small number of local training data while minimizing the negative transfer induced by het-
erogeneity among distributions. This motivates us to mix the global model and local model with a
controllable weight as a joint prediction model, namely, the personalized model.
Here we formally introduce our proposed adaptive personalized learning schema, where the goal
is to find the optimal combination of the global and the local models, in order to achieve a better
client-specific model. In this setting, global server still tries to train the global model by minimizing
the empirical risk on the aggregated domain D, i.e., h* = argminh∈H LD(h), while each user
trains a local model while partially incorporating the global model, with some mixing weight αi ,
i.e., hl*oc,i = arg minh∈H LDi(αih + (1 - αi)h*). Finally, the personalized model for ith client is
2
Under review as a conference paper at ICLR 2021
a convex combination of h* and h^oc,i:
hai = αihloci + (I- αi)h*,	⑴
It is worth mentioning that, hai is not necessarily the mmιmιzer of empirical risk LDi (•)，because
We optimize h；。。i With partially incorporating the global model.
Example 1. Let us illustrate a simple situation where mixed model does not necessarily coincide
with local ERM model. To this end, consider a setting where the hypothesis class H is the set of
all vectors in R2, lying in `2 unit ball: H = {h ∈ R2 : khk2 ≤ 1}. Assume the local empirical
minimizer is known to be [1,0]>, and h； = [—1,0]>, and a is set to be 0.5. Now, ifwe wish to find
；	；；
a hioc,i, such that hai = α * hioc,i + (1 — α) * h coincides with local empirical minimizer, we
have to solve: 0.5 * h + 0.5 * [—1,0]> = [1,0]>, subject to ∣∣hk2 ≤ 1. This equation has nofeasible
solution, implying that it is not necessarily true that hαi coincides with local empirical minimizer.
In fact, in most cases, as We Will shoW in the convergence of the proposed algorithm, hαi Will incur
a residual risk if evaluated on the training set draWn from Di .
2.2	Generalization guarantees
We noW characterize the generalization of the mixed model. We present the learning bounds for
classification and regression tasks. For classification, We consider a binary classification task, With
squared hinge loss '(h(x), y) = (max{0,1 — yh(x)})2. In the regression task, we consider the
MSE loss '(h(x), y) = (h(x) — y)2. Even though we present learning bounds under these two loss
functions, our analysis can be generalized to any convex smooth loss. Before formally presenting
the generalization bound, we introduce the following quantity to measure the empirical complexity
of a hypothesis class H over a training set S .
Definition 1. Let S be a fixed set of samples and consider a hypothesis class H. The worst case
disagreement between a pair of models measured by absolute loss is quantified by: λH (S) =
suPh,h0∈H |S| P(χ,y)∈S |h(X) — h'(X)h
The empirical discrepancy characterizes the complexity of hypothesis class over some finite set.
The similar concepts are also employed in the related multiple source PAC learning or domain
adaption (Kifer et al., 2004; Mansour et al., 2009; Ben-David et al., 2010; Konstantinov et al., 2020;
Zhang et al., 2020).
We now state the main result on the generalization of the proposed personalization schema. The
proof of the theorem is provided in Appendix D.
Theorem 1. Let hypothesis class H be compact closed set with finite VC dimension d. Assume loss
function ` is Lipschitz continuous with constant G, and bounded in [0, B]. Then with probability at
least 1—δ, there exists a constant C, such that the risk ofthe mixed model hai = αih^ i+(1—ai)h；
on the ith local distribution Di is bounded by:
LDi (hαi ) ≤ 2αi2
LDi (h；)+2C 产m^+GλH(Si)
+ 2(1 — αi)2 (LD(h*) + BkD — Diki + Cr + log(1/6
m
(2)
where mi, i = 1, 2, . . . , n is the number of training data at ith user, m = m1 + . . . +mn is the total
number of all data, Si to be the local training set drawn from Di, kD — Diki
∕ξ lP(χ,y)〜D -
P(χ,y)〜Di ∣dxdy, is the difference between distributions D and Di, and h； = argminh∈H LDi (h).
Remark 1. We note that a very analogous work to ours is Mansour et al. (2020), where a gen-
eralization bound is provided for mixing global and local models. However, their bound does not
depend on αi, and hence we cannot see how it impacts the generalization ability.
In Theorem 1, by omitting constant terms, we observe that the generalization risk of hαi on Di
mainly depends on three keyquantities: i) m: the number of global data drawn from D, ii) diver-
gence between distributions D and Di, and iii) mi: the amount oflocal data drawn from Di. Usually,
3
Under review as a conference paper at ICLR 2021
the first quantity m, the amount of global data is fairly large compared to individual users, so global
model usually has a better generalization. The second quantity characterizes the data heterogeneity
between the average distribution and ith local distribution. If this divergence is too high, then the
global model may hurt the local generalization. For the third quantity, as amount of local data mi is
often small, the generalization performance of local model can be poor.
Optimal mixing parameter. We can also find the optimal mixing parameter a* that minimizes
generalization bound in Theorem 1. Notice that the RHS of (2) is quadratic in αi, so it admits a
minimum value at
(l⅛ W+BkD - Diki+c q d+lom(ι㈤) * 1
i	(LD(h*)+ BkD -Diki + Cqd+甯㈤)+(£d“传)+2Cq^+⅞(≡ + GλH(Si))
The optimal mixture parameter is strictly bounded in [0, 1], which matches our intuition. If the
divergence term is large, then the value becomes close to 1, which implies if local distribution drifts
too much from average distribution, it is preferable to take more local models. If mi is small,
this value will be negligible, indicating that we need to mix more of the global model into the
personalized model. Conversely, if mi is large, then this term will be again roughly 1, which means
taking the majority of local model will give the desired generalization performance.
3	Optimization Method
To optimize the learning problem we cast in the previous section, here we propose a communication
efficient adaptive algorithm to learn the personalized local models and the global model. To do so,
we let every hypothesis h in the hypothesis space H to be parameterized by a vector w ∈ W ⊂ Rd
where W is some convex closed set and denote the empirical risk at ith device by local objective
function fi (w). Adaptive personalized federated learning can be formulated as a two-phase opti-
mization problem: globally update the shared model, and locally update users’ local models. Similar
to FedAvg algorithm, the server will solve the following optimization problem:
1n
Wmin F(w) ：= n £{fi(w)：= Eξi fi(w,ξi)]},	⑶
i=i
where fi(.) is the local objective at ith client, ξi is a minibatch of data in data shard at ith client, and
n is the total number of clients. Motivated by the trade-off between the global model and local model
generalization errors in Theorem 1, we need to learn a personalized model as in (1) to optimize the
local empirical risk. To this end, each client needs to solve this optimization over its local data:
min fi(αiv + (1 - αi)w*),	(4)
v∈W
where w* = arg minw F(W) is the optimal global model. The balance between these two models
is governed by a parameter αi , which is associated with the diversity of the local model and the
global model. We first state the algorithm for a pre-defined proper αi , and then propose an adaptive
schema to learn this parameter as learning proceeds.
Remark 2. As mentioned in Section 2.1, when the hypothesis class is bounded, the mixed model
will not coincide with local ERM model. However, if the class is unbounded, the mixed model will
eventually converge to local ERM model, which means the personalization fails. Hence, to make
sure the correctness of our algorithm, we need to require the parameter comes from some bounded
domain W
Local Descent APFL. To efficiently optimize the problem we cast in (3) and (4), in this subsection
we propose our bilevel optimization algorithm, Local Descent APFL. At each communication round,
server uniformly random selects K clients as a set Ut . Each selected client will maintain three
models at iteration t: local version of the global model wi(t), its own local model vi(t), and the mixed
personalized model v(t) = ɑivit) + (1 - ɑi)w('). Then, selected clients will perform the following
updates locally on their own data for τ iterations:
Wlitt= Y (w(t-1) - ηt^fi (w(t-1);ξt)), Vtt = Y (v(t-1) - η∖vfi (V尸);ξt)) ,⑸
WW
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Local Descent APFL
input: Mixture weights ɑι, ∙∙∙ , αn, Synchronization gap T.
for t = 0,…，T do
parallel for i ∈ Ut do
if t not divides τ then
Wit) = QW (W尸1 - nNfi (w(t-1);ξt)),
Vat = Qw (v(t-1) - InNvfi (v(t-1);ξi))
v(t1 = αiv(t + (1 - αi)w(t, Ut J Ut—i
else
each selected client sends wi(t) to the server
w(t) =击 Pj∈Ut Wjt)
server uniformly samples a subset Ut of K clients.
server broadcast w(t) to all chosen clients
end
end
end
where Vfi (.; ξ) denotes the stochastic gradient of f (.) evaluated at mini-batch ξ. Then, using the
updated version of the global model and the local model, We update the personalized model V(t) as
well. The clients that are not selected in this round will keep their previous step local modelvi(t) =
vi(t-1). After these τ local updates, selected clients will send their local version of the global model
WF) to the server for aggregation by averaging: w(t)= 尚 Pj∈ut Wjt). Then the server will
choose another set of K clients for the next round of training and broadcast this new model to them.
Adaptively updating α. Even though in Section 2.2, we give the information theoretically optimal
mixing parameter, in practice we usually do not know the distance between user’s distribution and
the average distribution. Thus, finding the optimal α is infeasible. However, we can infer it empiri-
cally during optimization. Based on the local objective defined in (4), the empirical optimum value
of ɑ for each client can be found by solving α* = arg minαi∈[o,i] fi (αiv + (1 - 0i)w), where we
can use the gradient descent to optimize it at every communication round, using the following step:
α(t) = α(tT)- ntvɑfi (v(tτ); ξt) = α(t-1) - nt Dvf- Wit-I), Vfi (v(tτ); ξt)E,⑹
which shows that the mixing coefficient α is updated based on the correlation between the differ-
ence of the personalized and the local version of global models, and the gradient at the in-device
personalized model. Meaning, when the global model is drifting from the personalized model, the
value of α changes to adjust the balance between local data and shared knowledge among all devices
captured by the global model.
4 Convergence Analysis
In this section we provide the convergence analysis of Local Descent APFL with fixed αi for
strongly convex and nonconvex functions. To have a tight analysis, as well as putting the opti-
mization results in the context of generalization bounds discussed above, we define the following
parameterization-invariant quantities that only depend on the distributions of local data across clients
and the geometry of loss functions.
Definition 2. We define the following quantity to measure the diversity among local gradients with
respect to the gradient of the ith client: ζi = supw∈Rd kVF (W) - Vfi(W)k22 (Woodworth et al.,
2020a). We also define the sum of gradient diversities of n clients as: ζ = in=1 ζi.
Definition 3. We define ∆% = ∣∣v*-w*k2, where v* = argminv fi(v) ,and w* = argminw F (w)
to measure the gap between optimal local model and optimal global model.
We also need the following standard assumption on the stochastic gradients at local objectives.
5
Under review as a conference paper at ICLR 2021
Assumption 1 (Bounded Variance). The variance of stochastic gradients computed at each local
data shard is bounded, i.e., ∀i ∈ [n]:E[kVfi(x; ξ) 一 Vfi(x)k2] ≤ σ2.
Strongly Convex Loss. We now turn to establishing the convergence of local descent APFL on
smooth strongly convex functions. Specifically, the following theorem characterizes the convergence
of the personalized local model to the optimal local model. The proof is provided in Appendix E.2.3.
Theorem 2. Assume each client's objective function is μ-strongly Convex and L-smooth, and satis-
fies Assumption 1. Also let K = L∕μ, b = min {与,2 }. Using Algorithm 1, by choosing the mixing
weight ai ≥ max{1 - 4√κ，1 一 4√6κ√μ}, learning rate: η =.洲?), where a = max{128κ, T},
and using average SCheme Vi =ST PT=IPtmivit) +(I-αi)Kk Pj∈ut Wjt)), wherePt = (t+a)2,
ST = PT=I Pt, and letting fi to denote the local minimum ofthe ith client, then thefollowing con-
vergence rate holds for all clients i ∈ [n]:
E[fi(Vi)] — f ≤ α2O
+ (1 - αi) O
κ2σ2	κ2τ (τZi + κ2τK)	ζi + K	κL∆i
μbKT +	μbT2	+ μb + b
Ifwe choose T = ,T/K, then:
E[fi(vi)] - fi* ≤ OiO(μT) +(1- αi)2O (KKσ2+μK⅛+κ4贪)+(1- αi)2O (殳拷 + κL∆).
A few remarks about the convergence of personalized local model are in place: (1) If we set αi = 1,
then We recover O (T) convergence rate of single machine SGD. If We only focus on the terms
with (1 - αi)2, which is contributed by the global model’s convergence, and omit the residual error,
we achieve the convergence rate of O(1∕KT) using only KKT communication, which matches
With the convergence rate of vanilla local SGD (Stich, 2018; WoodWorth et al., 2020a), and (2) The
residual error is related to the gradient diversity ζi and local-global optimality gap ∆i . It shows that
taking any proportion of the global model will result in a sub-optimal ERM model. As we discussed
in Section 2.1, hαi will not be the empirical risk minimizer in most cases. Also, we assume that αi
needs to be larger than some value in order to get a tight rate. This condition can be alleviated, but
the residual error will be looser. The analysis of this relaxation is presented in Appendix F.
Nonconvex Loss. The following theorem establish the convergence rate of personalized model
learned by APFL for nonconvex smooth loss functions. The proof is provided in Appendix G.3.
Theorem 3. Let v(t) = αiv(t') + (1 - ai)K^ Pj∈u Wjt)∙ If each client's objective function is
L-smooth and domain W be bounded by DW , that is, ∀W, W0 ∈ W , k
Algorithm 1 with full gradient, by choosing K = n and learning rate η =
W - W0
_	1
k2 ≤ DW. Using
2√5L√T-
, we have
T
T XllVfi(罔	≤ O
t=1
+ (1 - αi)2
+ αi4(1 - αi)2O
+ (I- α2 )2 (ζi + L2DW)
+(I - αi)2o (nT
By choosing T = n-1/4T1/4, it holds that:
T
TX∣∣vfi(v乃|| ≤ O
t=1
+ (I - αi)2θ F +
+ (I- α2 )2 (Zi + L2DW).
Here we show that APFL will converge to stationary point on nonconvex function with sublinear
rate plus some residual error, with n3/4 T 3/4 communication rounds. The rate with factor (1 -
αi)2 is contributed from the global model convergence, and here we have some additive residual
error reflected by Zi and DW. Compared to most related work by HaddadPOUr & MahdaVi (2019)
regarding the convergence of local SGD on nonconvex functions, they obtain O(1/√nT), while we
only have speedup in n on partial terms. This could be solved by using different learning rate for
local and global update. Additionally, we assume K = n to derive the convergence in nonconvex
setting, and leave the analysis for partial participation as a future work.
6
Under review as a conference paper at ICLR 2021
Number of Communication Rounds
Number of Communication Rounds
5 4 3 2 1
60.0.0.0.
SSo-I 6u_u_oll
20	40	60	80
Number of Communication Rounds
Number of Communication Rounds
(a) Non-IID (2 classes per client)
E8o< UoQeP=e>ra8-l
(b) Non-IID (4 classes per client)
(c) IID distribution among clients
—* *— APFL (α = 0.25)	―•— APFL (α = 0.75)	—Localized FedAvg . Localized SCAFFOLD
-→- APFL (a = 0.5)	—∙- APFL (α = 1.0) A Global FedAvg	♦ Global SCAFFOLD
Figure 2: Comparing the the performance of APFL with FedAvg (APFL with α = 0) and SCAF-
FOLD on the MNIST dataset. Top row is the training loss and the bottom row is the generalization
accuracy on training and validation data, respectively. In (a), the accuracy lines of SCAFFOLD and
FedAvg global models are removed since their low values degrade the readability of the plot.
5 Experiments
In this section, we empirically show the effectiveness of the proposed algorithm in personalized
federated learning. Due to lack of space, some experimental results are deferred to Appendix B.
Experimental setup. We run our experiments on Microsoft Azure systems, using Azure ML API.
The code is developed on PyTorch (Paszke et al., 2019) using its “distributed” API with MPI. We de-
ploy this code on Standard F64s family of VMs in Azure. We use four datasets for our experiments,
MNIST, CIFAR10 (Krizhevsky et al., 2009), EMNIST (Cohen et al., 2017), and a synthetic dataset.
For more information on datasets used in the following experiments refer to Appendix B.1. For all
the experiments, we have 100 users (except for EMNIST dataset), each of which has access to its
own data only. The local dataset is randomly divided into 80% for training and 20% for validation,
which is the standard way to examine the local models for personalized use cases. For the learning
rate, we use the linear decay structure with respect to local steps, suggested by Bottou (2012). At
each iteration the learning rate is decreased by 1%, unless otherwise stated. We report the perfor-
mance over training data for optimization error and local validation data (from the same distribution
as training data for each client) for the generalization accuracy. Throughout these experiments we
report the results for the following three models:
• Global Model: Referring to the global model of FedAvg or SCAFFOLD.
• Localized Global Model: Referring to the fine-tuned version of the global model at each round
of communication after τ steps of local SGD. Here, we have either the localized FedAvg or the
localized SCAFFOLD. The reported results are for the average of the performance over all the
local models on each online client. In all the experiments τ = 10, unless otherwise stated.
• Personalized Model: This model is the personalized model produced by our proposed algorithm
APFL. The reported results are the average of the respective performance of personalized models
over all online clients at each round of communication.
Strongly convex loss. First, we run a set of experiments on the MNIST dataset, with different levels
of non-IIDness by assigning certain number of classes to each client. We use logistic regression with
parameter regularization as our strongly convex loss function. In this part, all clients are online for
each round, however, the results when client sampling is involved is discussed in Appendix B.2. We
compare the personalized model of APFL with different rates of personalization as α with global and
localized models of FedAvg and SCAFFOLD, as well as their global models. The initial learning
7
Under review as a conference paper at ICLR 2021
Figure 3: Comparing the APFL with adaptive α and the localized FedAvg. The left figure is the
training performance, and the right one is the accuracy of these models on local validation data.
rate is set to 0.1 and it is decaying as mentioned before. The results of running this experiment on
100 clients and after 100 rounds of communication are depicted in Figure 2, where we move from
highly non-IID data distribution (left) to IID data distribution (right). As it can be seen, global mod-
els learned by FedAvg and SCAFFOLD have high local training losses. On the other hand, taking
more proportion of the local model in the personalized model (namely, increasing α) will result in
the lower training losses. For generalization ability, the best performance is given by personalized
model with α = 0.25 in both (a) and (b) cases, which outperforms the global (FedAvg and SCAF-
FOLD) and their localized versions. However, as we move toward IID distribution, the advantage of
personalization vanishes as expected. Hence, as expected by the theoretical findings, we can benefit
from personalization the most when there is a statistical heterogeneity between the data of different
clients. When the data are distributed IID, local models of FedAvg or SCAFFOLD are preferable.
An interesting observation from the results in Figure 2, which is inline with our theoretical findings
is the relationship of α with both optimization and generalization losses. As it can be seen from
the first row, α has a linear relationship with the optimization loss, that is, with smaller α, training
loss is getting closer to the global model of FedAvg in terms of optimization loss, which matches
with our convergence theory. However, from the second row, it can be inferred that there is no linear
relationship between α and generalization. In fact, according to (2), we know that generalization
bound is quadratic in α, and hence, the generalization performance does not simply increase or
decrease monotonically with α.
Adaptive α update. In this part, we want to show how adaptively learning the value of α across
different clients, based on (6), will affect the training and generalization performance of APFL’s
personalized models. We use the three synthetic datasets as described in Appendix B.1, with logistic
regression as the loss function. We set the initial value of αi(0) = 0.01 for every i ∈ [n]. The
results of this training are depicted in Figure 3, where both optimization and generalization of the
learned models are compared. As it can be inferred, in training, APFL outperforms FedAvg in
the same datasets. More interestingly, in generalization of learned APFL personalized models, all
datasets achieve almost the same performance as a result of adaptively updating α values, while the
FedAvg algorithm has a huge gap with them. This shows that, when we do not know the degree of
diversity among data of different clients, we should adaptively update α values to guarantee the best
generalization performance. We also have results on EMNIST dataset with adaptive tuning of α in
Appendix B.2, wih a 2-layer MLP.
Nonconvex loss. To showcase the results for a nonconvex loss, we use CIFAR10 dataset that is dis-
tributed in a non-IID way with 2 classes per client. We apply it to a CNN model with 2 convolution
layers, followed by 2 fully connected layers, using cross entropy as the loss function. The initial
learning rates of APFL and FedAvg algorithms are set to 0.1 with the mentioned decay structure,
while for SCAFFOLD this value is 0.05 with 5% decay per iteration to avoid divergence. As it can
be inferred from the results in Table 1, the personalized model learned by APFL outperforms the lo-
calized models of FedAvg and SCAFFOLD, as well as their global models, in both optimization and
generalization. In this case adaptively tuning the α achieves the best training loss, while α = 0.25
case reaching the best generalization performance.
Comparison with other personalization methods. We now compare our proposed APFL with two
recent approaches for personalization in federated learning. In addition to FedAvg, we compare
with perFedAvg introduced in Fallah et al. (2020) using a meta-learning approach, and pFedMe
8
Under review as a conference paper at ICLR 2021
	APFL				FedAvg		SCAFFOLD	
	α = 0.25	α = 0.5	α = 0.75	Adaptive a	Global Model	Localized Model	Global Model	Localized Model
Training Loss	0.154±~	0.113±	0.103±	0.101±~	1.789±	0.369±	1.70士	0.593±
	0.003	0.008	0.007	0.013	0.004	0.005	0.001	0.012
Validation Accuracy	89.33%士	88.74%土	89.04%土	88.87%土	~32.51%±~	83.16%土	~37.16%±~	85.25%土
	0.26%	0.14%	0.22%	0.51%	0.47%	0.37%	0.3%	0.2%
Table 1: The results of training a CNN model on CIFAR10 dataset using different algorithms.
introduced in Dinh et al. (2020) using a regularization with Moreau envelope function. We run these
algorithms to train an MLP with 2 hidden layers, each with 200 neurons, on a non-IID MNIST
dataset with 2 classes per client. For perFedAvg, similar to their setting, we use learning rates of
α = 0.01 (different from the α in our APFL) and β = 0.001. To have a fair comparison, we use the
same validation for perFedAvg and we use 10% of training data as the test dataset that updates the
meta-model. For pFedMe, following their setting, we use λ = 15, η = 0.01. We use τ = 20 with
total number of communications to 100 and the batch size is 20. The results of these experiments
are presented in Table 2, where APFL clearly outperforms all other models in both training and
generalization. The APFL model with α = 0.75 has the lowest training loss, and the one with
adaptive α has the best validation accuracy. perFedAvg is slightly better than the localized FedAvg,
however, it is worse than APFL models. pFedMe performs better than the global model of FedAvg,
but it cannot surpass neither the localized model of FedAvg nor APFL models.
	APFL				FedAvg		perFedAvg	pFedMe
	α = 0.25	α = 0.5	α = 0.75	Adaptive α	Global Model	Localized Model	Personalized Model	Personalized Model
Training Loss	0.011±	0.004±	0.002±	-0.004±-	0.240±	0.041±	0.039±	0.182±
	0.0007	0.0004	0.0001	0.0008	0.006	0.002	0.002	0.004
Validation Accuracy	98.07%士	98.04%土	97.86%士	98.10%土	-93.81%±-	97.75%士	97.83%土	95.92%土
	0.10%	0.08%	0.09%	0.10%	0.29%	0.15%	0.12%	0.1%
Table 2: The results of training an MLP on MNIST dataset with different personalization methods.
6 Conclusions
In this paper, we proposed an adaptive federated learning algorithm that learns a mixture of local
and global models as the personalized model. Motivated by learning theory in domain adaptation,
we provided generalization guarantees for our algorithm that demonstrated the dependence on the
diversity between each clients’ data distribution and the representative sample of the overall distri-
bution of data, and the number of per-device samples as key factors in personalization. Moreover,
we proposed a communication-reduced optimization algorithm to learn the personalized models and
analyzed its convergence rate for both smooth strongly convex and nonconvex functions. Finally,
we empirically backed up our theoretical results by conducting experiments in a federated setting.
References
Ines Almeida and Joao Xavier. Djam: distributedjacobi asynchronous method for learning personal
models. IEEE Signal Processing Letters,25(9):1389-1392,2018. 13
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Fed-
erated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019. 14
Aurelien Bellet, Rachid Guerraoui, Mahsa Taziki, and Marc Tommasi. Personalized and private
peer-to-peer machine learning. arXiv preprint arXiv:1705.08435, 2017. 13
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010. 3, 14, 22
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421—
436. Springer, 2012. 7
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konecny, H Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097,
2018. 15, 16
9
Under review as a conference paper at ICLR 2021
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017. 7
Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. arXiv preprint arXiv:2006.08848, 2020. 2, 9, 14
Hubert Eichner, Tomer Koren, Brendan Mcmahan, Nathan Srebro, and Kunal Talwar. Semi-cyclic
stochastic gradient descent. In International Conference on Machine Learning, pp. 1764-1773,
2019. 2
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-
learning approach. arXiv preprint arXiv:2002.07948, 2020. 2, 8, 14
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In ICML, pp. 1126-1135. JMLR. org, 2017. 13, 14, 18
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016. 41
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in feder-
ated learning. arXiv preprint arXiv:1910.14425, 2019. 6
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Lo-
cal sgd with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in
Neural Information Processing Systems, pp. 11080-11092, 2019a. 15
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading
redundancy for communication: Speeding up distributed sgd for non-convex optimization. In
International Conference on Machine Learning, pp. 2545-2554, 2019b. 15
FiliP Hanzely and Peter Richtarik. Federated learning of a mixture of global and local models. arXiv
preprint arXiv:2002.05516, 2020. 14
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Francoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018. 14
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019. 16
Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong
Zhang. Personalized federated learning: An attentive collaboration approach. arXiv preprint
arXiv:2007.03797, 2020. 14
Yihan Jiang, Jakub Konecny, Keith Rush, and Sreeram Kannan. Improving federated learning per-
sonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019. 1, 14
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019. 1, 13, 14, 15
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for on-device federated learn-
ing. arXiv preprint arXiv:1910.06378, 2019. 1, 2
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-
learning methods. In Advances in Neural Information Processing Systems, pp. 5915-5926, 2019.
14
Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. 2004. 3
Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph H Lampert. On the sample com-
plexity of adversarial multi-source pac learning. arXiv preprint arXiv:2002.10384, 2020. 3
10
Under review as a conference paper at ICLR 2021
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009. 7
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryfl:
Personalized and communication-efficient federated learning with lottery ticket hypothesis on
non-iid datasets. arXiv preprint arXiv:2008.03371, 2020a. 2
Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv
preprint arXiv:1910.03581, 2019. 1
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018. 15
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020b. 1
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Feddane: A federated newton-type method. arXiv preprint arXiv:2001.01920, 2020c. 1
Paul Pu Liang, Terrance Liu, Liu Ziyin, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think
locally, act globally: Federated learning with local and global representations. arXiv preprint
arXiv:2001.01523, 2020. 14
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009. 3, 14, 22
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.
2, 3, 14
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In AISTAT, pp.
1273-1282, 2017. 1, 2, 14, 15
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018. 18, 21
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. arXiv
preprint arXiv:1902.00146, 2019. 1, 15, 19
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018. 13, 14
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009. 14
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In NeurIPS, pp. 8024-8035, 2019. 7
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.
arXiv preprint arXiv:1912.13445, 2019. 1
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014. 18, 21
Tao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Fei Wu, and Chao Wu.
Federated mutual learning. arXiv preprint arXiv:2006.16765, 2020. 14
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems, pp. 4424-4434, 2017. 2, 14
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018. 6, 15, 22
11
Under review as a conference paper at ICLR 2021
Paul Vanhaesebrouck, AUrelien BelleL and Marc Tommasi. Decentralized collaborative learning of
personalized models over networks. 2017. 13
Kangkang Wang, Rajiv Mathews, Chloe Kiddon, Hubert Eichner, FranCoise Beaufays, and Daniel
Ramage. Federated evaluation of on-device personalization. arXiv preprint arXiv:1910.10252,
2019. 14
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heteroge-
neous distributed learning. arXiv preprint arXiv:2006.04735, 2020a. 5, 6, 22, 24, 32, 33
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? arXiv
preprint arXiv:2002.07839, 2020b. 15
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adapta-
tion. arXiv preprint arXiv:2002.04758, 2020. 13
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang,
and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv
preprint arXiv:1905.12022, 2019. 16
Valentina Zantedeschi, AUrelien Bellet, and Marc Tommasi. Fully decentralized joint learning of
personalized models and collaboration graphs. 2019. 13
Yuchen Zhang, Mingsheng Long, Jianmin Wang, and Michael I Jordan. On localized discrepancy
for domain adaptation. arXiv preprint arXiv:2008.06242, 2020. 3, 22
12
Under review as a conference paper at ICLR 2021
Supplementary Material:
Adaptive Personalized Federated Learning
Table of Contents
A Additional Related Work	13
B	Additional Experimental Results	15
B.1	Datasets ............................................................... 15
B.2	Additional Results ..................................................... 16
C	Discussions and Extensions	17
D	Proof of Generalization Bound	20
E	Proof of Convergence Rate in Convex Setting	22
E.1 Proof without Sampling .................................................. 23
E.1.1 Proof of Useful Lemmas ........................................... 24
E.1.2 Proof of Theorem 4 ............................................... 27
E.1.3 Proof of Theorem 5 ............................................... 27
E.2 Proof of Convergence of APFL with Sampling .............................. 31
E.2.1 Proof of Useful Lemmas ........................................... 32
E.2.2 Proof of Theorem 6 ............................................... 34
E.2.3 Proof of Theorem 2 ............................................... 35
F Convergence Rate without Assumption on αi	38
G Proof of Convergence Rate in Nonconvex Setting	40
G.1 Proof of Technical Lemmas ............................................... 40
G.2 Proof of Theorem 8 ...................................................... 46
G.3 Proof of Theorem 3 ...................................................... 46
A Additional Related Work
The number of research in federated learning is proliferating during the past few years. In federated
learning, the main objective is to learn a global model that is good enough for yet to be seen data
and has fast convergence to a local optimum. This indicates that there are several uncanny resem-
blances between federated learning and meta-learning approaches (Finn et al., 2017; Nichol et al.,
2018). However, despite this similarity, meta-learning approaches are mainly trying to learn mul-
tiple models, personalized for each new task, whereas in most federated learning approaches, the
main focus is on the single global model. As discussed by Kairouz et al. (2019), the gap between
the performance of global and personalized models shows the crucial importance of personalization
in federated learning. Several different approaches are trying to personalize the global model, pri-
marily focusing on optimization error, while the main challenge with personalization is during the
inference time. Some of these works on the personalization of models in a decentralized setting can
be found in Vanhaesebrouck et al. (2017); Almeida & Xavier (2018), where in addition to the opti-
mization error, they have network constraints or peer-to-peer communication limitation (Bellet et al.,
2017; Zantedeschi et al., 2019). In general, as discussed by Kairouz et al. (2019), there are three
significant categories of personalization methods in federated learning, namely, local fine-tuning,
multi-task learning, and contextualization. Yu et al. (2020) argue that the global model learned by
federated learning, especially with having differential privacy and robust learning objectives, can
hurt the performance of many clients. They indicate that those clients can obtain a better model by
using only their own data. Hence, they empirically show that using these three approaches can boost
the performance of those clients. In addition to these three, there is also another category that fits
the most to our proposed approach, which is mixing the global and local models.
13
Under review as a conference paper at ICLR 2021
Local fine-tuning: The dominant approach for personalization is local fine-tuning, where each
client receives a global model and tune it using its own local data and several gradient descent
steps. This approach is predominantly used in meta-learning methods such as MAML by Finn
et al. (2017) or domain adaptation and transfer learning (Ben-David et al., 2010; Mansour et al.,
2009; Pan & Yang, 2009). Jiang et al. (2019) discuss the similarity between federated learning and
meta-learning approaches, notably the Reptile algorithm by Nichol et al. (2018) and FedAvg, and
combine them to personalize local models. They observed that federated learning with a single
objective of performance of the global model could limit the capacity of the learned model for
personalization. In Khodak et al. (2019), authors using online convex optimization to introduce a
meta-learning approach that can be used in federated learning for better personalization. Fallah et al.
(2020) borrow ideas from MAML to learn personalized models for each client with convergence
guarantees. Similar to fine-tuning, they update the local models with several gradient steps, but they
use second-order information to update the global model, like MAML. Another approach adopted
for deep neural networks is introduced by Arivazhagan et al. (2019), where they freeze the base
layers and only change the last “personalized” layer for each client locally. The main drawback of
local fine-tuning is that it minimizes the optimization error, whereas the more important part is the
generalization performance of the personalized model. In this setting, the personalized model is
pruned to overfit.
Multi-task learning: Another view of the personalization problem is to see it as a multi-task
learning problem similar to Smith et al. (2017). In this setting, optimization on each client can be
considered as a new task; hence, the approaches of multi-task learning can be applied. One other
approach, discussed as an open problem in Kairouz et al. (2019), is to cluster groups of clients based
on some features such as region, as similar tasks, similar to one approach proposed by Mansour
et al. (2020).
Contextualization: An important application of personalization in federated learning is using the
model under different contexts. For instance, in the next character recognition task in Hard et al.
(2018), based on the context of the use case, the results should be different. Hence, we need a
personalized model on one client under different contexts. This requires access to more features
about the context during the training. Evaluation of the personalized model in such a setting has
been investigated by Wang et al. (2019), which is in line with our approach in experimental results
in Section 5. Liang et al. (2020) propose to directly learn the feature representation locally, and
train the discriminator globally, which reduces the effect of data heterogeneity and ensures the fair
learning.
Personalization via model regularization: Another significant trial for personalization is model
regularization. There are several studies to introduce different personalization approaches for feder-
ated learning by regularize the difference between the global and local models. Hanzely & Richtarik
(2020) try to introduce a new formulation for federated learning where they add the regulariza-
tion term on the distance of local and global models. In their effort, they use a mixing parameter,
which controls the degree of optimization for both local models and the global model. The Fe-
dAvg (McMahan et al., 2017) can be considered a special case of this approach. They show that the
learned model is in the convex haul of both local and global models, and at each iteration, depend
on the local models’ optimization parameters, the global model is getting closer to the global model
learned by FedAvg. Similarly, Huang et al. (2020) and Dinh et al. (2020) also propose to use the reg-
ularization between local and global model, to realize the personalized learning. Shen et al. (2020)
propose a knowledge distillation way to achieve personalization, where they apply the regularization
on the predictions between local model and global model.
Personalization via model interpolation: Parallel to our work, there are other studies to introduce
different personalization approaches for federated learning by mixing the global and local models.
The closest approach for personalization to our proposal is introduced by Mansour et al. (2020).
In fact, they propose three different approaches for personalization with generalization guarantees,
namely, client clustering, data interpolation, and model interpolation. Out of these three, the first
two approaches need some meta-features from all clients that makes them not a feasible approach
for federated learning, due to privacy concerns. The third schema, which is the most promising
one in practice as well, has a close formulation to ours in the interpolation of the local and global
models. However, in their theory, the generalization bound does not demonstrate the advantage of
mixing models, but in our analysis, we show how the model mixing can impact the generalization
14
Under review as a conference paper at ICLR 2021
bound, by presenting its dependency on the mixture parameter, data diversity and optimal models
on local and global distributions.
Beyond different techniques for personalization in federated learning, Kairouz et al. (2019) ask an
essential question of “when is a global FL-trained model better?”, or as we can ask, when is per-
sonalization better? The answer to these questions mostly depends on the distribution of data across
clients. As we theoretically prove and empirically verify in this paper, when the data is distributed
IID, we cannot benefit from personalization, and it is similar to the local SGD scenario (Stich, 2018;
Haddadpour et al., 2019a;b; Woodworth et al., 2020b). However, when the data is non-IID across
clients, which is mostly the case in federated learning, personalization can help to balance between
shared and local knowledge. Then, the question becomes, what degree of personalization is best for
each client? While this was an open problem in Mohri et al. (2019) on how to appropriately mix
the global and local model, we answer this question by adaptively tuning the degree of personaliza-
tion for each client, as discussed in Section 3, so it can perfectly become agnostic to the local data
distributions.
B Additional Experimental Results
In this section, we present additional experimental results to demonstrate the efficacy of the proposed
APFL algorithm. First, we describe different datasets we have used in this paper, and then, present
additional results.
B.1	Datasets
For the experiments we use 4 different data sources as follows:
MNIST and CIFAR10 For the MNIST and CIFAR10 datasets tobe similar to the setting in fed-
erated learning, we need to manually distribute them in a non-IID way, hence the data distribution
is pathologically heterogeneous. To this end, we follow the steps used by McMahan et al. (2017),
where they partitioned the dataset based on labels and for each client draw samples from some lim-
ited number of classes. We use the same way to create 3 datasets for the MNIST, that are, MNIST
non-IID with 2 classes per client, MNIST non-IID with 4 classes per client, and MNIST IID, where
the data is distributed uniformly random across different clients. Also, we create a non-IID CIFAR10
dataset, where each client has access to only 2 classes of data.
EMNIST In addition to pathological heterogeneous data distributions, we applied our algorithm
on a real-world heterogeneous dataset, which is an extension to MNIST dataset. The EMNIST
dataset includes images of characters divided by authors, where each author has a different style,
make their distributions different Caldas et al. (2018). We use only digit characters and 1000 authors’
data to train our models on.
Synthetic For generating the synthetic dataset, we follow the procedure used by Li et al. (2018),
where they use two parameters, say synthetic(γ, β), that control how much the local model and
the local dataset of each client differ from that of other clients, respectively. Using these parameters,
we want to control the diversity between data and model of different clients. The procedure is that for
each client we generate a weight matrix Wi ∈ Rm×c and a bias b ∈ Rc, where the output for the ith
client is yi = arg max σ Wi>xi + b , where σ(.) is the softmax. In this setting, the input data
xi ∈ Rm has m features and the output y can have c different values indicating number of classes.
The model is generated based on a Gaussian distribution Wi 〜N (μ%, 1) and b 〜N (μ^ 1),
where μ% 〜 N (0, Y). The input is drown from a Gaussian distribution Xi 〜 N (νi, Σ), where
Vi 〜N (Vi, 1) and Vi 〜N (0, β). Also the variance Σ is a diagonal matrix with value of ∑k,k =
k-1.2. Using this procedure, we generate three different datasets, namely synthetic(0.0, 0.0),
synthetic(0.5, 0.5), and synthetic(1.0, 1.0), where we move from an IID dataset to a highly
non-IID data.
15
Under review as a conference paper at ICLR 2021

—∙- Persoralized APFL (k =0.3)
—►- Persoralized APFL (k =0.5)
—∙- Personalized APFL (k =0.7)
—Localized FedAvg (k = 0.3)
—Localized FedAvg (k = 0.5)
—∙- Localized FedAvg (k = 0.7)
1s
3
0'
SS-3u-u∙-ll
Number of Communication Rounds
三三
0' E 5 0' E 5
∙796∙6κ
e-ln8UoepA
PersonalZed APFL (k =0.3)
PersonslZed APFL (k =0.5)
PersonalZed APFL (k =0.7)
Localized FedAvg (k =0.3)
Localized FedAvg (k =0.5)
Localized FedAvg (k =0.7)
60	80	100
Ommunication Rounds
40
of C
Umber
(a) α = 0.25
0'
=。一 3u-u∙-
三三
0	20	40	60	80	100
Number of Communication Rounds
Personalized APFL (k =0.3)
Personalized APFL (k =0.5)
Personalized APFL (k =0.7)
Localized FedAvg (k = 0.3)
Localized FedAvg (k = 0.5)
Localized FedAvg (k = 0.7)
0.00
o'
=。一 3u-u∙-
(b) α = 0.5
97.0
&
9 96.5
<
§ 96.0
I 95.5
20	40	60	80	100
Number of Communication Rounds
(C) α = 0.75
Figure 4: Evaluating the effeCt of sampling on APFL and FedAvg algorithm using the MNIST
dataset that is non-IID with only 2 Classes per Client with logistiC regression as the loss. The first
row is training performanCe on the loCal model of FedAvg and personalized model of APFL with
different sampling rates from {0.3, 0.5, 0.7}. The seCond row is the generalization performanCe
of models on loCal validation data, aggregated over all Clients. It Can be inferred that despite the
sampling ratio, APFL Can superbly outperform FedAvg.
B.2	Additional Results
In this part, we present more experimental results that Can further illustrate the effeCtiveness of APFL
on other datasets and models.
Effect of sampling. To understand how the sampling of different Clients will affeCt the perfor-
manCe of the APFL algorithm, we run the same experiment with different sampling rates for the
MNIST dataset. The results of this experiment are depiCted in Figure 4, where we run the experi-
ment for different sampling rates of K ∈ {0.3, 0.5, 0.7}. Also, we run it with different values of
α ∈ {0.25, 0.5, 0.75}. The results are reported for the personalized model of APFL and loCalized
FedAvg. As it Can be inferred, deCreasing the sampling ratio has a negative impaCt on both the
training and generalization performanCe of FedAvg. However, we Can see that despite the sampling
ratio, APFL is outperforming loCal model of the FedAvg in both training and generalization. Also,
from the results of Figure 2, we know that for this dataset that is highly non-IID, larger α values are
preferred. InCreasing α Can diminish the negative impaCts of sampling on personalized models both
in training and generalization.
Natural heterogeneous data In addition to the CIFAR10 and MNIST datasets with pathologiCal
heterogeneous data distributions, we apply our algorithm on a natural heterogeneous dataset, EM-
NIST (Caldas et al., 2018). We use the data from 1000 Clients, and for eaCh round of CommuniCation
we randomly seleCt 10% of Clients to partiCipate in the training. We use an MLP model with 2 hid-
den layers, eaCh with 200 neurons and ReLU as the aCtivation funCtion, using Cross entropy as the
loss funCtion. For APFL, we use the adaptive α sCheme with initial value of 0.5 for eaCh Client. We
run both algorithms for 250 rounds of CommuniCation. In eaCh round, eaCh online Client performs
the loCal updates for 1 epoCh on its data. Figure 5 shows the results of this experiment for person-
alized model of APFL and the loCalized model of the FedAvg. APFL with adaptive α Can reaCh to
the same training loss of the loCal FedAvg, while greatly outperforms the loCal FedAvg model in
generalization on loCal validation data.
Data distribution using Dirichlet distribution Another approaCh to distribute data in a non-IID
way is to use the DiriChlet distribution as disCussed in Hsu et al. (2019); YuroChkin et al. (2019). We
use this approaCh and set the parameter of the DiriChlet distribution to 1.0 and repeat the experiments
16
Under review as a conference paper at ICLR 2021
SsO-jSU-U-e」-
50	100	150	200	250
Number of Communication Rounds
A0e-n84 uo-sp--e>
80.0
Figure 5: The results of applying FedAvg and APFL (with adaptive α) on an MLP model using EM-
NIST dataset, which is naturally heterogeneous. APFL achieves the same training loss of localized
FedAVG, while outperforms it in validation accuracy.
for MNIST dataset with MLP model in the main body. Here, again we have 100 clients and run
the experiments for 100 rounds of communication each with 1 epoch of training. The results are
summarized in Table 3. Again, it can be inferred that APFL can generalize well on the local test
dataset of different clients.
	APFL			FedAVG	
	α = 0.25	α = 0.5	α = 0.75	Global Model	Localized Model
Training Loss	0.004±	0.001±	0.001	-0.1946±	0.022±
	0.0009	0.0003		0.003	0.0023
Validation Accuracy	98.43%±	98.52%土	98.34%土	93.71%士	98.04%土
	0.07%	0.09%	0.06%	0.07%	019%
Table 3: The results of training an MLP on MNIST dataset with APFL and FedAvg using Dirichlet
distribution for splitting data across clients. The parameter of the Dirichlet distribution is set to 1.
C Discussions and Extensions
Connection between learning guarantee and convergence. As Theorem 1 suggests, the gener-
alization bound depends on the divergence of the local and global distributions. In the language
of optimization, the counter-part of divergence of distribution is the gradient diversity; hence, the
gradient diversity appears in our empirical loss convergence rate (Theorem 2). The other interesting
discovery is in the generalization bound, We have the term λχ and LdO , which are intrinsic
to the distributions and hypothesis class. Meanwhile, in the convergence result, we have the term
∣∣v* - w*k2, which also only depends on the data distribution and hypothesis class we choose. In
addition, kv* - w*∣2 also reveals the divergence between local and global optimal solutions.
Why APFL is “Adaptive”. Both information-theoretically (Theorem 1) and computationally
(Theorem 2), we prove that when the local distribution drifts far away from the average distribu-
tion, the global model does not contribute too much to improve the local generalization and we have
to tune the mixing parameter α to a larger value. Thus it is necessary to make α updated adaptively
during empirical risk minimization. In Section 3, (6) shows that the update of α depends on the
correlation of local gradient and deviation between local and global models. Experimental results
show that our method can adaptively tune α, and can outperform the training scheme using fixed α.
Comparison with local ERM model A crucial question about personalization is when it is prefer-
able to employ a mixed model?, and how bada local ERM model will be? In the following corollary,
we answer this by showing that the risk of local ERM model can be strictly worse than that of our
personalized model.
Corollary 1. Continuing with Theorem 1, there exist a distribution Di, constant C1 and C2, such
that with probability at least 1 - δ, the following upper bound for the difference between risks of
17
Under review as a conference paper at ICLR 2021
personalized model ho,i and local ERM model h* on Di, holds :
LDi (hɑi ) - LDi (h" ≤ (2a2 - I)LDi (M) + (2α2C1 - C2)1
d +0g≡+2α2GλH(Si)
mi
+ 2(1 - αi)2 (LD(h*) + BkD -DikI + Ci Jd +log(I/δ)
m
By examining the above bound, the personalized model is preferable to local model if this value is
less than 0. In this case, we require (2α2 - 1) and (2αi2C1 - C2) to be negative, which is satisfied by

choosing α ≤ min{ ɪ2,
. Then, the term
d+log(1∕δ)
mi
, should be sufficiently large, and the
divergence term, as well as the global model generalization error has to be small. In this case, from
the local model perspective, it can benefit from incorporate some global model. Using the similar
technique, we can prove the supremacy of mixed model over global model as well.
Proof of Corollary 1. Since in Theorem 1, we already obtained upper bound for LDi (hαi) as fol-
lowing,
LDi (hαi ) ≤ 2αi2
LDi (h：) + 2Ci jd + lm[") + GλH(Si)
+ 2(1 - αi)2 LD((h*) + BkD - Diki + Ci Jd + log(I⑷
m
to find the upper bound of LDi (hαi) - LDi (hi：), we just need the lower bound of LDi (hi： ). The
fundamental theorem of statistical learning (Shalev-Shwartz & Ben-David, 2014; Mohri et al., 2018)
states a lower risk bound for agnostic PAC learning: for a hypothesis class with finite VC dimension
d, then there exists a distribution D, such that for any learning algorithm, which learns a hypothesis
h ∈ H on m i.i.d. samples from D, there exists a constant C, with the probability at least 1 - δ, we
have:
LD(h) - min LD(h0) ≥ C Jd + log(I/JI.
h0∈H	m
Since h： is learnt by ERM algorithm, the agnostic PAC learning lower risk bound also holds for it,
so in worst case it might hold that under distribution Di, if hi： is learnt by ERM algorithm using mi
samples, then there is a C2, such that with probability at least 1 - δ, we have:
LDi(h：) ≥ LDi(h：) + C2∖F + log(I/δ) 1.
i	i	mi
El	1	FC ∕7	∖ C ∕f*∖	f~∙∖	11	a 1 ∙
Thus we can bound LDi (hαi) - LDi (hi：) as Corollary 1 claims.
□
Personalization for new participant nodes. Suppose We already have a trained global model W,
and now a new device k joins in the network, which is desired to personalize the global model
to adapt its own domain. This can be done by performing a few local stochastic gradient descent
updates from the given global model as an initial local model:
vkt+1) = Vkt)- ηNv fk(akVkt) + (1 - a)W; ξkt))	(7)
to quickly learn a personalized model for the newly joined device. One thing worthy of inves-
tigation is the difference between APFL and meta-learning approaches, such as model-agnostic
meta-learning (Finn et al., 2017). Our goal is to share the knowledge among the different users,
in order to reduce the generalization error; while meta-learning cares more about how to build a
meta-learner, to help training models faster and with fewer samples. In this scenario, similar to
FedAvg, when a new node joins the network, it gets the global model and takes a few stochastic
steps based on its own data to update the global model. In Figure 6, we show the results of applying
18
Under review as a conference paper at ICLR 2021
Figure 6: Comparing the effect of fine-tuning with the local model of FedAvg and with the personal-
ized model of APFL on the synthetic datasets. The model is trained for 100 rounds of communication
with 97 clients, and then 3 clients will join in fine-tuning the global model based on their own data.
It can be seen that the model from APFL can better personalize the global model with respect to the
FedAvg method both in training loss and validation accuracy. Increasing diversity makes it harder
to personalize, however, APFL surpasses FedAvg again.
FedAvg and APFL on synthetic data with two different rates of diversity, synthetic(0.0, 0.0)
and synthetic(0.5, 0.5). In this experiment, we keep 3 nodes with their data off in the entire
training for 100 rounds of communication between 97 nodes. In each round, each client updates its
local and personalized models for one epoch. After the training is done, those 3 clients will join
the network and get the latest global model and start training local and personalized models of their
own. Figure 6 shows the training loss and validation accuracy of these 3 nodes during the 5 epochs
of updates. The local model represents the model that will be trained in FedAvg, while the person-
alized model is the one resulting from APFL. Although the goal of APFL is to adaptively learn the
personalized model during the training, it can be inferred that APFL can learn a better personalized
model in a meta-learning scenario as well.
Agnostic global model. As pointed out by Mohri et al. (2019), the global model can be distribu-
tionally robust if we optimize the agnostic loss:
n
min max
w∈Rd q∈∆n
F(w) :=	qifi(w),
i
(8)
where ∆n = {q ∈ Rn+ | qi = 1} is the n-dimensional simplex. We call this scenario “Adaptive
Personalized Agnostic Federated Learning”. In this case, the analysis will be more challenging since
the global empirical risk minimization is performed at a totally different domain, so the risk upper
bound for hαi we derived does not hold anymore. Also, from a computational standpoint, since the
resulted problem is a minimax optimization problem, the convergence analysis of agnostic APFL
will be more involved, which we will leave as an interesting future work.
19
Under review as a conference paper at ICLR 2021
D Proof of Generalization Bound
In this section we present the proof of generalization bound for APFL algorithm. Recall that we
define the following hypotheses on ith local true and empirical distributions:
hi =	a arg min LDi (h		(LOCAL EMPIRICAL RISK MINIMIZER)
hii	arg min LDi (h)		(LOCAL TRUE RISK MINIMIZER)
hi 二	a arg min LD (h) h∈H D		(GLOBAL EMPIRICAL RISK MINIMIZER)
hLc,i 二	arg min LDi (αih + (1	-αi)hi)	(MIXED EMPIRICAL RISK MINIMIZER)
hliOc,i	arg min LDi (αi h + (1	-αi)hi)	(MIXED TRUE RISK MINIMIZER)
where LDi (h) and LDi (h) denote the empirical and true risks on Di, respectively.
From a high-level technical view, since we wish to bound the risk of the mixed model on local
distribution Di, first we need to utilize the convex property of the risk function, and decompose it
into two parts: LDi @"c i) and LDi (h*). To bound LDi (JhIOC i)，a natural idea is to characterize
it by the risk of optimal model LDi (hi) plus Some excess risk. However, due to fact that h；°c i
is not the sole local empirical risk minimizer, rather it partially incorporates the global model, we
need to characterize to what extent it drifts from the local empirical risk minimizer hii . This drift
can be depicted by the hypothesis capacity, so that is our motivation to define λH (S) to quantify
the empirical loss discrepancy over S among pair of hypotheses in H. We have to admit that there
should be a tighter theory to bound this drift, depending how global model is incorporated, which
we leave it as a future work.
The following simple result will be useful in the proof of generalization.
Let LD (h) = E(χ,y)〜D [' (h(x),y)∖ denote the risk of h over D . Ifthe loss function '(∙) is
Lemma 1. Let H be a hypothesis class and D and D0 denote two probability measures over space
bounded by B, then for every h ∈ H:
LD(h) ≤LD0(h)+BkD-D0k1
(9)
where ∣∣D - D0kι = JS ∣P(x,y)〜D - P(x,y)〜D，|dxdy.
Proof.
LD (h) ≤ LD0 (h) + |LD (h) - LD0 (h)|
≤ LD(h) + / |'(y, h(x)川P(χ,y)〜D — P(χ,y)〜D0∣dxdy
LD(h) + B∣D - D0∣1.
□
Proof of Theorem 1 We now turn to proving the generalization bound for the proposed APFL al-
gorithm. Recall that for the classification task we consider squared hinge loss, and for the regression
case we consider MSE loss. We will first prove that in both cases we can decompose the risk as
follows:
LDi(h" ≤ 20LDi @IIC) + 2(1 — 而吃〜(h*(x)) .	(10)
We start with the classification case first. Note that, hinge loss: max{0, 1 - z} is convex in z, so
max{0, 1 - y(αih + (1 - αi)h0)} ≤ αi max{0, 1 - yh} + (1 - αi) max{0, 1 - yh0}, according to
20
Under review as a conference paper at ICLR 2021
Jensen,s inequality. Hence, we have:
LDi (hαi ) = LDi (αihloc,i + (1 - αi)h )
=E(x,y)〜Di (max{0,1 - y(αihjoc,i(x) + (1- 0i)h*(x))?)
=E(x,y)〜Di («i max{0,1 - yhloc,i(x)} + (1 - ai)max{0,1 - yh*(x)}
≤ 2α2E(χ,y)〜Di (maχ{0,1 - yh；。。"(X)})
+ 2(1 - 0i)2E(χ,y)〜Di (max(0,1 - yh*(x)})2
≤ 2αi LDi (hloc,i) + 2(I- 0i)2LDi (h*).
For regression case:
LDi (hαJ = LDi (aihloc,i + (I- αi)h )
=E(x,y)〜DiIIy -Ghioc,i(X) + (1 - 0i)h*(X))Il
=E(x,y)〜Di ∣∣αiy - aihloc,i(X) + (1 - ɑi)y - (1 - 0i)h*(X)II
≤ 2αi E(x,y)〜Di (y - hloc,i(X)I + 2(1 - 0i)2E(x,y)〜Di Uy - h*(X)『
≤ 2α2Ln (h；oc,i) + 2(1 - αi)2Ln (h*)
Thus we can conclude:
LDi(Ki) ≤ 2α2 LDi (h；oc,i)
X-------V--------'
T1
+2(1 — αi)2 LDi (h*).
'----------------V----'
T2
(11)
We proceed to bound the terms Ti and T2 in RHS of above inequality. We first bound Ti as follows.
The first step is to utilize uniform VC dimension error bound over H Mohri et al. (2018); Shalev-
Shwartz & Ben-David (2014):

∀h ∈ H, |LDi(h) - LDi(h)| ≤ C
d + log(1∕δ)
mi
where C is constant factor. So we can bound Ti as:
Ti = LDi (hloc,i) = LDi (hi) + LDi (h loc,i) - LDi (hi)
=LDi (hi )
+ LDi (hloc,i) - LDi(hloc,i) +LDi (hloc,i) - LDi (hi ) + LDi (hi ) - LDi (hi )
------------7-----------}	、-------L	e}'
<C /d+iog(ι∕δ)	≤C./d+l°g(1∕!T
—V mi	— V	mi
≤ LDi (hi) + 2C,∕d +lOg(1/J) + LDi (hloc,i) - LDi (K).
V	mi	,
Note that
1
LDi(hloc,i)-LDi(hi) ≤ G同
|h；oc,i(x) - K(x)| ≤ GλH(Si),
(x,y)∈Si
As a result we can bound Ti by:
Ti ≤LDi(hi)+2C尸尹 + 口丸⑸).
21
Under review as a conference paper at ICLR 2021
We now turn to bounding T2. Plugging Lemma 1 in (11) and using uniform generalization risk
bound will immediately give:
T2 ≤Ld(h*)+ B2kD-Dkι + CJ
d + log(1∕δ)
m
Plugging T1 and T2 back into (11) concludes the proof.
□
Remark 3. One thing worth mentioning is that, we assume the customary boundedness of loss
functions. Actually it can be satisfied if the data and the parameters of hypothesis are bounded.
For example, considering the scenario where we are learning a linear model w with the constraint
kwk ≤ 1, and also the data tuples (x, y) are drawn from some bounded domain, then the loss is
obviously bounded by some finite real value.
Remark 4. As LDi (h；oc) is the risk of the empirical risk minimizer on Di after incorporating a
model learned on a different domain (i.e., global distribution), one might argue that generalization
techniques established in multi-domain learning theory (Ben-David et al., 2010; Mansour et al.,
2009; Zhang et al., 2020) can be utilized to serve our purpose. However, we note that the techniques
developed in Ben-David et al. (2010); Mansour et al. (2009); Zhang et al. (2020) are only applicable
to a settings where we aim at directly learning a model in some combination of source and target
domain, while in our setting, we partially incorporate the model learned from source domain and
then perform ERM on joint model over target domain. Moreover, their results only apply to very
simple loss functions, e.g., absolute loss or MSE loss, while we consider squared hinge loss in
the classification case. Analogous to multiple domain theory, we derive the multi domain learning
bound based on the divergence of source and target domains but measured in absolute distance,
k ∙ ∣∣ι. AsMansour et al. (2009)points out, divergence measured by absolute loss can be large, and
as a result we leave the development of a more general multiple domain learning theory that can
deal with most popular loss functions like hinge loss, cross entropy loss and optimal transport, with
tighter divergence measure on distributions as an open question.
E Proof of Convergence Rate in Convex Setting
In this section, we present the proof of convergence raters. For ease of mathematical derivations, we
first consider the case without sampling clients at each communication step and then generalize the
proof to the setting where K devices are sampled uniformly at random by the server as employed in
the proposed algorithm.
Technical challenges. The analysis of convergence rates in our setting is more involved compared
to analysis of local SGD with periodic averaging by Stich (2018); Woodworth et al. (2020a). The key
difficulty arises from the fact that unlike local SGD where local solutions are evolved by employing
mini-batch SGD, in our setting we also partially incorporate the global model to compute stochastic
gradients over local data. In addition, our goal is to find the convergence rate of the mixed model,
rather than merely the local model or global model. To better illustrate this, let us first clarify the
notations of models that will be used in analysis. Let us consider the simple case for now where
we set K = n (all device participate averaging). We define three virtual sequences: {w(t)}tT=1,
{v(t)}T=ι and {v(t)}T=ι where w(t) = 1 Pn=I Wit),Vit) = αiVtt + (1 — αi)w(t) VF) = α%Vy) +
(1 -αi)w(t). Since the personalized model incorporates 1 - αi percentage of global model, then the
key challenge in the convergence analysis is to find out how much the global model benefits/hurts
the local convergence. To this end, we analyze how much the dynamics of personalized model
V(t) and global model w(t) differ from each other at each iteration. To be more specific, We study
the distance between gradients ∣∣Vfi(V(t)) — VF(w(t))∣2. Surprisingly, we relate this distance
to gradient diversity, personalized model convergence, global model convergence and local-global
optimality gap:
E [∣∣Vfi(v(t)) —VF(w(t))k2i ≤ 6Zi +2L2E [∣∣v(t) — v*『] +6L2E [∣∣w(t) - w*『] +6L2∆/
E [∣∣V(t) — v*∣∣2] and E [∣w(t) — w*『] will converge very fast under smooth strongly convex
objective, and ζi and ∆i will serve as residual error that indicates the heterogeneity among local
functions.
22
Under review as a conference paper at ICLR 2021
Algorithm 2: Local Descent APFL (without sampling)
input: Mixture weights ɑι, ∙∙∙ , α. Synchronization gap T, Local models V(O) for i ∈ [n] and
local version of global model wi(0) for i ∈ [n].
for t = 0,…，T do
if t not divides τ then
Wft = QW (w(tT)-ηtVfi (w(t-1);ξi))
Vp = Qw (-Vvfi]j)；ξt))
V(t) = αiV(t) + (1 — αi )wit)
else
each client sends w(jt) to the server
w(t) = I Pn w(t)
n	j=1 j
server broadcast w(t) to all clients
end
end
for i = 1,…，n do
output： Personalized model: Vi = 1 PT=I pt(αiV,t + (1 — 0i)n Pn=I w(tt);
GIobaI model: W=nsT PT=Ipt Pn=I Wjt).
end
E.1 Proof without Sampling
Before giving the proof of convergence analysis of the Algorithm 1 in the main paper, we first
discuss a warm-up case: local descent APFL without client sampling. As Algorithm 2 shows, all
clients will participate in the averaging stage every τ iterations. The convergence of global and local
models in Algorithm 2 are given in the following theorems. We start by stating the convergence of
global model.
Theorem 4 (Global model convergence of Local Descent APFL without Sampling). If
each client's objective function is μ-strongly convex and L-smooth, and satisfies Assumption 1,
using Algorithm 2, choosing the mixing weight α% ≥ max{1 — 4√6κ，1 — 4√6κ√μ}，learning rate
ηt = μ(1+ °), where a = max{128κ, τ}, and using average SCheme W = n1^ PT=I pt Pn=I Wjt),
where pt = (t + a)2, ST = PtT=1 pt, then the following convergence holds:
E [F (W)] - F (w*) ≤ O ( T )+o( κ2⅛E! + O ( "+3) + O ( nT),
where w* = argminw F(W) is the optimal global solution.
Proof. Proof is deferred to Appendix E.1.2.
□
The following theorem obtains the convergence of personalized model in Algorithm 2.
Theorem 5 (Personalized model convergence of Local Descent APFL without
Sampling). If each client's objective function is μ-strongly convex and L-smooth, and satisfies
Assumption 1, using Algorithm 2, choosing the mixing weight ai ≥ max{1 — 4√κ，1 — 4√6K√μ },
learning rate η =	*(；+ °), where a = max{128κ, τ}, and using average SCheme
Vi =	STPT=Iptmiv(t)	+(I —	0i)nPn=IWjt)),	where	Pt	=(t	+	a)2,	Sτ	=	PT=Ipt,
and fi* is the local minimum of the ith client, then the following convergence holds for all i ∈ [n]:
E[fi(Vi)] - fi ≤ O (T)+α2O
十 (1 — αi)2O (μ + κL∆i)
2 κL ln T	κ2σ2
+ (-W2 O(F)+oGnT)+ o
κ2τ (。2 + T(Zi + Z))
μT 2
+ O (κ4τ⅛^)).
23
Under review as a conference paper at ICLR 2021
Proof. Proof is deferred to Appendix E.1.3.
□
E.1.1 Proof of Useful Lemmas
Before giving the proof of Theorem 4 and 5, we first prove few useful lemmas. Recall that we define
virtual sequences {w(t)}T=ι,{v(t)}T=ι,{V(t)}T=ι where w(t) = ɪ P2ι w(t),v(t) = αiv(t) + (1 -
ai)w't,V^t = αiv(t + (1 - ai)w(t).
We start with the following lemma that bounds the difference between the gradients of local objective
and global objective at local and global models.
Lemma 2. For Algorithm 2, at each iteration, the gap between local gradient and global gradient
is bounded by
E h∣Nfi(V(t)) - VF(w(t))k2i ≤ 2L2E hkv(t) - v*/]+ 6Zi + 6L2E [∣∣w⑷-w*/]+ 6L2∆i.
Proof. From the smoothness assumption and by applying the Jensen’s inequality we have:
E [kVfi(v"-VF(w(t))k2i
≤ 2E [kVfi(V(t)) - Vfi(v*)k2i + 2E [kVfi(v" - VF(W㈤)『]
≤ 2L2E [kV(t) - v*k2] + 6E [kVfi(v*) - Vfi(W*)『]
+ 6E [∣∣Vfi(w*) - VF(w*)k2] + 6E [∣∣VF(w*) - VF(W㈤)∣∣2]
≤ 2L2E [kV(t) - v*『i + 6L2E [kv* - w*『]+ 6Zi + 6L2E [∣∣w⑴-w*『]
≤ 2L2E 1v(t) - v*『i +6L2∆i + 6Zi + 6L2E [∣∣w㈤-w*∣∣2].
□
Lemma 3 (Local model deviation without sampling). For Algorithm 2, at each iteration, the devi-
ation between each local version of the global model Wi(t) and the global model W(t) is bounded
by:
E hkw㈤-w(t)k2i ≤ 3τσ2η2-ι +3(金 + n)τ2η2-ι,
n
—XE hkw(t) - Wi k2i ≤ 3τσ2η2-ι + 6τ2SnLι,
nn
i=1
whereZ = n Pn=IZi -
Proof. According to Lemma 8 in Woodworth et al. (2020a):
n
E hkw(t)-Wf)Il2] ≤ nXE hkwjt) -w(t)k2i
j=1
ζ	t-1	t-1
≤ 3 卜2 + Ziτ+ InT)Enp ∏ (I-μnq)
p=tc	q=p+1
n	nn
n X E h∣W(t) - W(t)k2i≤ : XX E hkWjt)-W(t)k2i
i=1	i=1 j=1
t-1	t-1
≤ 3 σ2 +2τnζ Enp ∏ (1-μnq).
p=tc	q=p+1
24
Under review as a conference paper at ICLR 2021
Plugging in η = μ(a+q) yields:
Similarly,
E lw(t) - wi(t) l2
≤ 3 1 + C" + Zτ)X ηP Y
p=tc q=p+1
≤ 3 1 + ZiT + Zτ)X ηp Y
p=tc q=p+1
≤ 3 (σ2 + ZiT + Zτ)X ηp Y
p=tc q=p+1
a + q — 16
a+q
a + q — 16
a+q
a + q — 2
a+q
≤ 3 (σ2 + ZiT + ZT) S η (a + P - 1)(a + P)
一(	2	n ) J p (a + t — 2)(a + t — I)
p=tc
≤ 3 fσ2 + ZiT + ZT)X ηP 电
n	p=tc p ηp2
≤ 3t (σ2 + ZiT + nΛ η2-1.
W㈤-w(t)k2i ≤ 3τσ2η2-ι + 6τ2ζη2-ι.
n
η XE hk
i=1
□
Lemma 4. (Convergence of global model) Let w(t) = n1 P n=1 w(t). Under the setting of Theorem
5, we have:
E hkw(T+1) - w*k2i≤ G
+(T+16 (++ln(T+a)
E IJlw(I)- w*k2]
1536a2τ (σ2 + 2τZ) L2
(a — 1)2μ4(T + a)3
ι 128σ2T (T + 2a)
+ nμ2(T + a)3 .
Proof. Using the updating rule and non-expensive property of projection, as well as applying strong
convexity and smoothness assumptions yields:
E hkW(t+1) - W*k2i
≤E
1n
W㈤-W⑴-ηt n X Vfj (w(t); ξ) - w*
≤ E hkw(t) - w
2
+ ηt 2σ + ηtE
j=1
k2i - 2ηtE "*n X Vfj(Wjt))
1X Vfj(Wjt))「#
2#
, W(t) - W*
≤ E hkw(t) - w
2
Il2] - 2ηtE KVF(W(t)), W(t) - w*〉] + η2+ η2 E
-2ηt E K1 XX Vfj (Wjt)) - VF (W(t)), W(t) -
w*+#
ηχ Vfj (Wjt))]
"{^^^^^^^^^^^^≡
T1
|	{z	j
T2
σ2
≤ (1 - μηt)E |||W(t) - W*k[ - 2ηt(E[F(W(t))] - F(w*)) + η2~n + Tl + T2,
(12)
|
*
*
|

25
Under review as a conference paper at ICLR 2021
where at the last step we used the strongly convex property.
Now we are going to bound T1. By the Jensen’s inequality and smoothness, we have:
Tl ≤ 2η2E I Il 1X Vfj (Wjt))-VF (w(t))ll I +2η2E IlVF (w(t))∣∣
1n
≤ 2η2L21 XE UWjt)- w(t)k2] +4η2L(E IF(W⑴)]-F(w*))
n j=1
Then, we bound T2 as:
T2 ≤ ηt I 2 E
μ
n XX Vfj(Wjt))-VF(w(t))∣ I + 2E [kw㈤-w*k2i
≤ 2ηtL2 n X E [∣wjt) - w(t) ∣2] + 等 E hkw(t) - w*k2i∙
(13)
(14)
Now, by plugging back T1 and T2 from (13) and (14) in (12), we have:
E [kw(t+1)- W*/]
2
≤ (1-等)E kW(t)	- W*k2	-(2ηt-4η2L)	(E	F(W⑴)-F(w*))	十 褚一
、 {z }	n
≤-ηt
+ (当+2η2L2) n XX E [∣Wjt) - W(t)
≤ (1-*E hkW(t) - W*k2i + η2 2σ+(叶…n XX E
(15)
Now, by using Lemma 3 we have:
E hkW(t+1) - W*k2i
≤ (1 -等)E hkW(t) - W*k2i + (半 + 2η2L2) 3τ (σ2 + 2τZ) η2- + η2三.
Note that (1 - μ2ηt 埼=μ(t+a)16t-8+a) ≤ Ja3 = ηt-1, so We multiply Ptοn both sides and
do the telescoping sum:
PTE h∣∣W(T +1) - W*k2i
ηT
P0 E
η0
P0 E
η0
≤
≤
[kW(1) - W*k2] + X (2L- + 2ηtL2) 3τ (σ2 + 2tZ) Ptnt-I + XPtηtσn
hkW ⑴-W*k2i+ X (2L-+ 2ηtL2)3τ (σ2+ 2τζ) μ256⅛+Xptηt σn2.
(16)
Then, by re-arranging the terms Will conclude the proof:
E hkW(T+1) - W*k2i
≤ (τ⅛pE hkW(I)- W*k2i
1	1536α2τ (σ2 + 2TZ)L2	128σ2T(T + 2a)
+ ( +	(a+1 + n( + a)))	(a - I)2μ4(τ + α)3	+	”2(t + >3 ,
where we use the inequality PT=I t+a ≤ ⅛ + RT t⅛ < ⅛ + In(T + a).	□
26
Under review as a conference paper at ICLR 2021
E.1.2 Proof of Theorem 4
Proof. According to (15) and (16) in the proof of Lemma 4 we have:
T
pTEhkw(T+1) - w*∣∣2] ≤ p0Ehkw(I)- w*k2i - XPt (EhF(W⑴)]-F(w*))
2 2 ζλ	256a2	V σ2
3τ(σ +2τnJ μ2(a — ι)2 + yptηtV,
re-arranging term and dividing both sides by ST = PtT=1 pt > T 3 yields:
S1T XPt (E hF(w(t))i - F(W*)) ≤ STn0E [kw⑴-w*k2i
+ST=(2L-+2ntL2) 3τ 卜2+2τn) μ22α6ai)2+ST=Ptnt n
≤ O (T) + O ( κ⅛2^! + O ( K +F )ln T ! + O (得).
Recall that W = n^ PT=I Pn=I Wjt) and convexity of F, We can conclude that:
E [F(W)] - F(w*) ≤ O (T) + O (κ2⅛^) ! + O (「I- T! + O (得).
□
E.1.3 Proof of Theorem 5
Proof. Recall that we defined virtual sequences {w(t)}Τ=1 where w(t = ɪ Pn=I WT) and Vit)
αivi(t) + (1 - αi)W(t), then by the updating rule and non-expensiveness of projection We have:
Ehkvit+1)- v*k[
1 n
≤ E VF)- Ointkfi(vat) - (1 - αi)nt- Eyfj(Wy))- v*
n j=1
+ E	αint(Vfi(vlħ - Vfi(V(t); ξt)) + (1- αi)ntn = Wfj(Wjt)) -Vfj (w(t); ξj))
n j∈Ut
2
≤ E hkv(t) - v*k2i
- 2E
^α2ntVfi(vit)) + (1 - Oi)ηtn
n
=
j=1
-vi*+#
+ nt2 E
αiVfi(vit)) + (1 - Oi)1 = Vfj(Wjt))1# + α2ηtσ2 + (1- αi)2η2 —
n j=1	n
E hkvit) - v*k2] -2(α2 + 1 - αi)ηtE [(Vfilvit，), Vit)- v*Ei
I
-2ηt (1 - αi )E
{
T1
n
n=Vfj (Wjt))-Vfi(v(t)), vit) -
j = 1
^^^"{z
T2
vi*+#


+ ηt2 E
J
n2
kαiVfi(v( )) + (1 - Oi)n = Vfj(Wj ))k	+αiηtσ +(1 - Oi) ηt —.
n j=1	n
______ - ，
T3
(17)
27
Under review as a conference paper at ICLR 2021
Now, we bound the term T1 as follows:
Ti = -2ηt(αi + 1 - αi)E h<Vfi(V" Vit)-吗
-2ηt(α2 + 1 - ɑi)E [(▽.(Vit)) - Vfi(v(t)), V(t - v，]
≤ -2ηt(α2 + 1 - αi)(E [加邸))]-fi(v*) + .E [赭)-D
+ (α2 + 1 - ɑi)ηt (“(U))EhkVit)- Vit)k2i + μ(1-8(αi-浏E hkVit)-泥k2i)
≤ -2ηt(α2 + 1 - αi)(E [加卯))]-"+ .E [超)-D
+ ηt (μ(8L⅛ -⅞))E hkw(t) - wit)k2i + μ(1-8(αi- a2))E hkVit)- V涧)
≤ -2ηt(αi + 1 - αi)(E [加邸))]-Mvi))-誓E [kVit)-域/]
+ HR))E hkw(t) - w(t)k2i，	(18)
where We use the fact (a + 1 - αQ ≤ 1. Note that, because We set ɑ% ≥ max{1 - ɪ√^, 1 -
4√6κ√μ}，and hence 1 - 8(αi - 02) ≥ 0, so in the second inequality we can use the arithmetic-
geometry inequality.
Next, we turn to bounding the term T2 in (17):
T2 = -2ηt (1 - αi)E
"* n XX Vfj (Wjt)) -V/秒 t)), Vit)-
i
Vi
+#
≤ ηt (1 - αi)
2(1 - αi) E
μ
Vfi(V(t)) - n XX Vfj (Wjt))I #+2(τ⅛)E hkV(t) - Vik2i!
6(1 — ai)2ηt
μ
+E
6(1 — ai)2ηt
μ
+E
E IIVfi(Vit)) -Vfi(V(t))∣∣2 + E IIVfi(Vit)) -VF(w(t))
n	I2
VF(w(t)) - n X Vfj (wjt))	I + 竽EhkVit)- Vik2]
n j=1	I	2
III2
n2
VF(w(t)) - n X Vfj(wjt))	+ 竽EhkVit)- Vik2].
j=1	I
(19)
And finally, we bound the term T3 in (17) as follows:
T3 = E α2Vfi(Vit))+ (1 - αi)?XX Vfj (Wjt)) || |
≤ 2(α2 + 1 - αi)2E[∣∣Vfi(Vit)) k2] +2E	(1 - a。fɪ XX Vfj (w(t)) -Vfi(Vit))
I	j=1
≤ 2(2(a2 + 1 - ai)2E [∣∣Vfi(V(t)) - Vf*k2] + 2(a2 + 1 - a)2E [∣∣Vfi(Vit)) - ▽加解))/])
+ 2(1 - ai)2E	n XX Vfj(wjt)) -Vfi(秒t))∣ #
≤ 8L(a2 + 1 - ai) (E [fi(Vit))] - fi*) + 4(I- ai)2L2EhkW(t) - w(t) k2]
+ 6(1-ai)2 (l2E ||w(t) - w(t)∣∣j + E IVfi(Vit)) -VF(w(t))||j
n
+1X L2E Wt)-Wjt) ||	.	(20)
j=1
2
28
Under review as a conference paper at ICLR 2021
Now, using Lemma 3, (1 -四)2 ≤ 1 and plugging back Ti, T2, and T3 from (18), (19), and (20)
into (17), yields:
—v"l2]
Eblvit) — Vil2] — 2(ηt — 4ηt2L)(α2 + 1 — Ci)(E fi(V(t))]一力⑹))
≤
E [v(t+1)
E
+ ( 8ηtL2(1 - Ci)2
μ(1 - 8(Ci - C)))
+ 6(I- CCiirltL +10(1 - Ci)2
w(t) — w(t)|| ]
+ (6(1 - Ci)2ηtL2
μ
+ 6(1 — Ci
n
1 X E
n
j=i
w(t)-Wjt) ||2]
μ
+
≤
(1 — ai) 2E
NF (w(t)) — Vfi(vit))||2
2
+ α2ηtσ2 + (1 — αi)2ηt -,
Eblvit) — v[∣2] - 2(ηt — 4η2L)(αi + 1 — Ci)(E [fi(V(t))] — fi(v；))
+ ( 8ηtL2(1 - Ci)2
μ(1 - 8(Ci - C)))
+ 6(1 — Cai)2ηL +10(1 — oi)2
3τ 卜2 + (ζi + n )丁) ηt-1
+ (6(1 — Ci)2ηtL2
μ
+ 6(1 — Ci
3τ (σ2 +[7)淄T
μ
(1 — ai) 2E
∣∣VF(w(t)) -Vfi(Vit))||2 +C2ηt2σ2 + (1 — Ci)2η2σ2,	(21)
+
、

{z
T4
where using Lemma 2 we can bound T4 as:
T4 ≤ 6ηt(1 - ai)2 (2L2E [kvi')- v*k2] +6Zi +6L2E |||w。一 w*『] +6L2A,
+ 6η2(1 - a，)2(2L2E Ivf)- v*『]+ 64 + 6L2E [|w(t) - w*『] +6L2A, . (22)
Note that we choose 必 ≥ max{1 - 4⅛，1 - 4√6⅛}, hence 12L'Li)2 ≤ 8 and 12L2(1 -
a，)2 ≤ 8, thereby we have:
T4 ≤ 等IIvT)- v*k2 +36ηt (1+ η) (1 - %)2 (金 + L2E |||w。一 w*『]+ L2Ai).
Now, using Lemma 4 we have:
T4 ≤
臂E [kVit) — v*k2] + 36ηt (μ + η) (1 — Ci)2
ζi + L
a3
(t + a — 1)3
EblW(I) — w*∣∣2i
+ t +16
+ ln(t + a)
1536τ (σ2 + 2τZ) L2 + 128σ2t(t + 2a)
μ4(t + a — 1)3
nμ2(t + a — 1)3
+ L2 ∆i . (23)
29
Under review as a conference paper at ICLR 2021
By plugging back T4 from (23) in (21) and using the fact -(ηt-4*L) ≤ -1ηt, and (a2 + 1-αJ ≥
3, we have:
E 帆t+1)- VM
≤ (1 -等)E [W -煤 k2i -竽(E 卜⑹ t>)] - fi(vi)) + α2η2σ2 + (1 - ^
+ ( 8ηtL2(1 - ai)2
四(I - 8(ai - a2))
+ 6(1 - ai)2ηtL2 + log- ai)
3丁 卜2 + (ζi + Z )丁) ηt-ι
μ
+ (铀2加L2 +6(1-ai
3τ (σ2 +2]τ) ηt-ι
μ
+ 36ηt (μ + ηt) (1 — ai)2 (Zi + L2
+
a3E IjlW⑴-w*『]
(t - 1 + a)3
(t + 16 (a + J + ln(t + a)
1536τ (σ2 + 2τZ) L2
μ4(t + a — 1)3
128σ2t(t + 2a)
+ nμ2(t — 1 + a)3
*)).
Note that (1 -警)^t ≤ nt-1 where Pt = (t + a)2, so, we multiply nt on both sides, and re-arrange
the terms:
W (E [fi(Vit))] - fi(v*))
≤ 公E [∣∣v(t) - vik2] - nE [∣∣vit+1)- vik2] + Ptηt (a≡σ2 + (1 - ai)2三)
+ (μ(8- 8K -I)) + 6(I-『L +10(1 - 吟加L2) 3τ (σ2 + (Zi + I)T) ptη2-1
+ ( 6(1 — ai)2L2
μ
+ 6(1 - ai)2ηtL2) 3τ [2 + 2ZT) Ptη2-ι + 36pt (] + ηt) (1 一 ai)2 (金 + L2∆i)
+ 36pt (μ + ηt) (1 - ai)2
L2 1(t-a+ a)3 +(t + 16θ(lnt))
1536τ (σ2 + 2τZ) L2 + 128σ2t(t + 2a)
μ4(t + a — 1)3
nμ2(t — 1 + a)3
30
Under review as a conference paper at ICLR 2021
By applying the telescoping sum and dividing both sides by ST = PtT=1 pt ≥ T3 we have:
fi(Vi) - fi(vi)
≤
1τ
HZpt(MvIieI) - fi(Vi))
Sτ =1	i
2 σ2
+ (I- αi) -
≤
4poE[W)- vik2i 1 4 τ
3η0ST	+ ST 31⅛ ptηt
+ ST 4 X (μ(8- 8；— —i!2)) + 6(I-7 L + 10(I- αi)2ηtL2) 3τ (σ2 + (Zi + n)τ) PtlL
+st 3X (6(I- 丁"+6(I- αi)2ηt L2)3τ (σ2+2 n) ptη2-1
+ 48(1 - αi)2
LT Xpt (1+ ηt) ((t-a + a)3 +(t + 16θ(Int))
+ 48(1 - αi)2 (Zi + L2∆i) S1- Xpt (μ + ηt)
1536τ (σ2 + 2τZ) L2 + 128σ2t(t + 2a)
μ4(t + a — 1)3
nμ2(t — 1 + a)3
< 4poE hk^(1) - vik2i
3η0ST
+ 4(1 - αi)2
+	3
ι 32T(T + a)
+ -3μSτ一
8L2T
+ (I- αi)2 彳
一 一_________ 6L2T	10L2Θ(lnT)
μ(1 — 8(αi — α2')')Sτ + μSτ +
μSτ
。一 (一2	Z	256a2
3T (。+ (Zi + n)T) μ2(a- 1)2

4 ( 6(1 — ai)2L2T	6(1 —
+ 3(-— + ɪ-
μSτ
αi)2L2Θ(lnT)、q ( 2、<	256a2
ʒST--------Γτlσ + 2nTJ μ2(a- 1)2
+ 48(1 - αi)2L2
a2
(a - 1)2 Sτ
a3®' T) + (T + Θ
1536L2τ (σ2 +2τZ) + 64(2a + 1)σ2T(T + a)
naμ3
a2
(a - 1)2 Sτ
16a3π2 + (Θ(ln T))
6μ
1536L2τ (σ2 + 2τZ) + 2048(2a + 1)σ2
μ5
naμ3
+ 48(1 - αi)2L2
+ (1 - αi)2O
+ 48(1 -
+ (1 - ɑi)2 O(KLTnT
1母
St + 8T (T + 2a)
μ	μ
+O
+O
κτ
+ O — μT
κ4τ(σ2 +2τ Z )
μT 2
!!.
T
where We use the convergence of Pt=I 岩 → O(1), and P∞=ι 表 → ∏-.
E.2 Proof of Convergence of APFL with Sampling
In this section we will provide the formal proof of the Theorem 2. Before proceed to the proof, we
would like to give the convergence of global model here first. The following theorem establishes the
convergence of global model in APFL.
Theorem 6 (Global model convergence of Local Descent APFL). If each client’s objective function
is μ-strongly convex and L-smooth, and satisfies Assumption 1, using Algorithm 1, by choosing
the learning rate ηt = *(1;Ο), where a = max{128κ,τ}, K = L, and using average Scheme
W= KST PT=Ipt PjeUt wjt,where pt = (t + a)2, ST = PT=I pt, and letting F * to denote the
31
Under review as a conference paper at ICLR 2021
minimum of the F, then the following convergence holds:
E [F (W)] - F* ≤ ° ( T ) +° ( κ2τ⅛≠1! + O ( - F 需 On T ) +° ( K),
(24)
where τ is the number of local updates (i.e., synchronization gap) .
Proof. The proof is provided in Appendix E.2.2.	□
Remark 5. It is noticeable that the obtained rate matches the convergence rate of the FedAvg,
and if we choose T = YTK, we recover the rate O(YIKT), which is the convergence rate of
well-known local SGD with periodic averaging (Woodworth et al., 2020a).
Now we switch to the proof of the Theorem 2. The proof pipeline is similar to what we did in
Appendix E.1.3, non-sampling setting. The only difference is that we use sampling method here,
hence, we will introduce the variance depending on sampling size K. Now we first begin with the
proof of some technique lemmas.
E.2.1 Proof of Useful Lemmas
Lemma 5. For Algorithm 1, at each iteration, the gap between local gradient and global gradient
is bounded by
E	Vfi(Vit)) — KK X Vfj(w(t))| #
≤ 2L2E [kvit) — v*k2] + 6(21i + 2* + 6L2E [kw(t) — w*/]+ 6L2∆i.
Proof. From the smoothness assumption and by applying the Jensen’s inequality we have:
E kVfi(vit)) — ɪ X Vfj(w(t))k2
K
j∈Ut
≤ 2E [kVfi(V(t)) — Vfi(v*)k2i + 2E kVfi(v*) — K X Vfj(W⑷)∣∣2
K j∈Ut
≤ 2L2E [kVit) — v*k2i +6E [kVfi(v*) — Vfi(w*)k2]
+ 6E	kVfi(w*)	— ɪ X Vfj(w*)k2	+6E	kVɪ X	Vfj(w*)	-	-1	X	Vfj(w(t))『
K	KK
j∈Ut	j∈Ut	j∈Ut
≤ 2L2E [kV? — v*k2i +6L2E [kv* — w*k2] + 6(2Zi + 2K X Zj) + 6L2E [kw(t) — w*k[
j∈Ut
≤ 2L2E [kVit)- v*k2i +6L2∆i + 6(2Zi + 2K) +6L2E [kw(t) — w*∣∣2].
□
Lemma 6 (Local model deviation with sampling). For Algorithm 1, at each iteration, the deviation
between each local version of the global model wi(t) and the global model w (t) is bounded by:
E hkw(t) - w(t)k2i ≤ 3τσ2η2-i +3(Zi + K)τ2n2-i,
K X E hkw(t) - w(t)k2i ≤ 3τσ2η2-ι + 6τ2KηLι.
i∈Ut
where K =春 P=IZi.
32
Under review as a conference paper at ICLR 2021
Proof. According to Lemma 8 in Woodworth et al. (2020a):
E hkw(t) - w(t)k2i ≤ kK X E hkwjt)- w(t)k2i
j∈Ut
ζ t-1	t-1
≤ 3 (σ2 + Ziτ + KT)Enp ∏ (I-μηq)
p=tc	q=p+1
n	nn
n xE hkw(t) - w(t)k2i≤ n⅛ xxE hkwjt) - W*]
i=1	i=1 j=1
t-1	t-1
≤ 3 σ2 + 2τ-K ∑np ∏ (1-μnq) .
p=tc	q=p+1
Then the rest of the proof follows Lemma 3.
□
Lemma 7. (Convergence of Global Model) Let W((It = K^ Pj∈u Wjt)∙ In Theorem 2's setting,
using Algorithm 1 by choosing learning rate as n(= .(9Ο), we have:
3
E hkw(T+1) - w*k2i ≤ (τ+a)3E [kw⑴-w*k2i
+ (τ + 16 (ɪ+ m(T + a1) 1536a2τ (σ2+2τ K) L + 128σ2T (T + 2a)
+ V + a + ι+ ln(T + a)J J	(a - 1)2μ4(τ + a)3	+ Kμ2(T + a)3
Proof. According to the updating rule and non-expensiveness of projection, and the strong convexity
we have:
E，|w(t+1)- w*k2]
≤ E hkW(t) - W*k2i - 2ηtE ]*KK X Vfj(Wjt)), w(() - W*+]
+η2E ] ⅛ X Vfj (Wt))II η2 ⅛
2
≤ (1 - μηt)E [kW(t) - W*k2] - (2ηt - 2Lη2)E FF(w(t)) - F(WF + η2K
+ n2 K X L2EN
W(jt) - W(t)
j∈Ut
≤ (1 - μnt)E hk
-2ntE ]*K X Vfj(Wjt))- Vfj(w(t)), w(()- W*+]
2
w(() — W*∣∣2] -(2n( — 4Lη2) E [f(w(()) — F(w*)] + n2 —
≤-ηt
+ 2η2L2KK X	E [∣Wjt)	- W(t)∣21	+2ηtL2K X E	[∣Wjt)	- W(t)∣21	+ μηtE	hkW(t)	- W*k2i .
j∈Ut L	」 μ 八 j∈Ut	L	」
(25)
33
Under review as a conference paper at ICLR 2021
Then, merging the term, multiplying both sides with n, and do the telescoping sum yields:
PTE h∣w(T+1)- w*k2] ≤ 竺E [kw⑴-w*k2]
ηT	η0
-E[F(W㈤)-F(w*)]
+ X(2L- + 2ηtL2) PtK X E Mt)- w(t)∣∣2] + XPtnσK.
t=1 ∖ μ	/	j∈Ut L	」 t=l
(26)
Plugging Lemma 6 into (26) yields:
PTE h∣w(T +1) - w*k2] ≤ 竺E [w(1) - w*k2] - E[F(w(t)) - F(w*)]
ηT	η0
+
2L
3Ptηt--1τ σ-
+ 2τKK
T
+X
t=1
σ-
ptnt K.
(27)
μ
Then, by re-arranging the terms will conclude the proof as
E [kw(T +1) - w*k2] ≤ (T+3ψE [kw⑴-w*k2]
ι 1 ι	∖∖ 1536a-L-τ (σ- + 2τ-K
+ (T + 16 (F+1"? + ɑɔ))	(a-IG(T + a)3
128σ-T (T + 2α)
+ Kμ-(T + a)3 .
□
E.2.2 Proof of Theorem 6
Proof. According to (28) we have:
PTE h|w(T +1) - w*k2] ≤ 竺E [w(1) - w*k2] - E[F(w(t)) - F(w*)]
ηT	η0
+
2L
3Ptn2-ιτ (σ- + 2τK) + X
σ-
Ptnt κ
μ
(28)
By re-arranging the terms and dividing both sides by ST = PtT=1 Pt > T3 yields:
1T
S- XPt(E [F(w(t))] — F(w*))
ST t=1
≤ sT0ηoE hkw ⑴"I + : X
T2
3ptη2-1 Tk +2τK)+ ST XptηtK
<O ( μE[∣∣w ⑴一w*k2i) +o
κ2τ (σ2 + 2τ-K)
μT 2
+O
κ2 T (σ2 + 2τ K) ln T
μT 3
+O
Recalling that W = n∣^ PT=I Pt P；=i wjt), from the convexity of F(∙), We can conclude that
E [F (W)] — F W ≤ ° (知+ O ( κ2d⅞≠1! + O ( KKiKK 'ln T ! + ° ( K
□
34
Under review as a conference paper at ICLR 2021
E.2.3 Proof of Theorem 2
Now we provide the formal proof of Theorem 2. The main difference from without-sampling setting
is that only a subset of local models get updated each period due to partial participation of devices,
i.e., K out of all n devices that are sampled uniformly at random. To generalize the proof, we will
use an indicator function to model this stochastic update, and show that while the stochastic gradient
is unbiased, the variance is changed.
Proof. Recall that We defined virtual sequences of {w(t)}T=ι where w(t) = K Pj∈u, w(t) and
v(t) = ɑiVit) + (1 - ɑi)w(t). We also define an indicator variable to denote whether ith client was
selected at iteration t:
1 if i ∈ Ut
0 else
obviously, E 叫 = Kn. Then, according to updating rule and non-expensiveness of projection we
have:
E [k^(t+1)-煤k2i
≤ E	Vit)- α2ItηtVfi(V(t)) -(1- αi)ηtKK X Vfj(Wjt)) - v*| j
+ E	a2liηt	(Vfi(Vit)) - Vfi(V(t);	ξt))	+ (1 -	αi)ηt	( K X	Vfj(Wjt))	- K X Vfj(Wjt);	ξt)
j∈Ut
!|||||2#
j∈Ut
Ehkvit)- V涧-2 (1α2ηtVfi(vit)) + (1- αi)ηt K X Vfj (Wjt)), Vit)- v*∖
n	K j∈Ut
+ η2E	α2liVfi(Vit)) + (1 - αi)K X Vfj(Wjt))I + + α2ηt2 2K^ + (1 - ai)2η22K-.
Ehkvit)- v"l[ -2ηt <(Kα2 + 1 - αj Vfi(Vit)), Vit)- v'i )
-2ηt(1 - ai)E KKI X Vfj(Wjt)) - Vfi(Vit)), Vit)-煤"
玉
+ η2 E α2It Vfi(Vit)) + (1 - ai)ɪ X Vfj (Wjt))	+α2η2 2K； +(1 - αi)2η22σ~.
K	n2	K
j∈Ut
35
Under review as a conference paper at ICLR 2021
Now we switch to bound T1 :
Ti = -2ηt(Kai + 1 - αi)E [〈Vf^vit)), V(t) -%]
— 2ηt(Ka + 1 - ai)E hDvfi(V(t)) - Vfi(Vit)), V(t) -%]
≤ -2ηt(Kai + 1 - ai) (E [fi(V(t))]-力㈤)+ 2E [赭)-叫2])
+ (-ai + 1 - ai)ηt (~k~iK~----2κττE [kv(t) - v(t)k2]
n	4(I - 8(ai - a2K))
+ “a-8-2 Kn)) E [kV(t)-叫 2i
≤ -2ηt(Ka + 1 - ai) (E [fi(V(t))] - fi(vi) + 2E [礴)-叫2])
+ ηt (μ⅛‰ E hkw(t) - w(t)k2i +	E hkV(t)-叫」
≤ -2ηt(Kai + 1 - a.(E [fi(V(t))] - fi(吟)-粤E 1[破) - v"∣2]
+m; e [kw(t)-w(t) k2i,
μ(0(ai	ai ))
(29)
For T2, we use the same approach as we did in (19); To deal with T3, we also employ the similar
technique in (20):
T3 = E 卜2ltVfi(V(t)) + (1 - ai)KK X Vfj(wjt))∣ j
≤ 2(Ka + 1 - ai)2E [kVfi(V(t))k2i + 2E ^|(1 - ai) (K X Vfj(Wjt))- Vfi(V(tj
≤ 2。(KKai + 1 - ai)2E [kVfi(V(t)) - Vf*『]
+2(Ka2 + 1 - ai)2E [∣∣Vfi(vit)) - Vfi(破))『])
+ 2(1-ai)2E ]K X Vfj(Wjt))-Vfi(V(t))∣ j
≤ 8L(Ka + 1 - ai) (e [fi(V(t))i - fi*) + 4(1 - aJ2L2E [忖⑴-w(t)/]
+ 6(1 - ai)2(L2E ∣∣w(t) - w(t)∣∣[ + E IVfi(V(t)) -VF(w(t))|[
+K XLiE [|w(t) - Wjt)II1.	(30)
j∈Ut
36
Under review as a conference paper at ICLR 2021
Then plugging T1 , T2 , T3 back, we obtain the similar formulation as the without sampling case
in (17). Thus:
≤
E [kvit+1) - V
Ik2]
E [l∣vit) - v[∣2] - 2(ηt - 4η2L) (α2KK + 1 -
αi
(Ehfi(V(t))i - fi(vi))
2 2 2K σ2	2 2 2σ2
+ αi ηt------+ (1 - ai) ηt -FΓ
nK
+ 8	8ηtL2(1 — a)	+ 6(1 — αi)2ηtL2
μ(I - 8(ai - a2 KK))
+ 10(1 - αi ) ηt L	E
w(t) - wi(t)III2
+ (6(1 - αi)2ηtL2
μ
+ 6(1 - αi
K X EI
j∈Ut
w(t) - w(jt)III2
μ
+
(1 - αi ) E
K X Vfj(w(t)) -Vfi(Vit))I #
(31)
We then examine the lower bound of a K + 1 - αi. Notice that: a K + 1 - αi = KK ((ai - 2nK )2 +
n	n2
K - 4K2).
Case 1:	2K ≥ 1 The lower bound is attained when ai = 1: 02 K + 1 - a2 ≥ K.
Case 2:	2K < 1 The lower bound is attained when a% = 2K: 02 K + 1 - a% ≥ 1 - 4K > 1.
So * K + 1 - a ≥ b := min{ K, 1} always holds.
Now we plug it and Lemma 6 back to (31):
E hkV(t+1) - v：k2]
≤ (1 — 3μηt) E hkvit) — v"l2] — bηt (E hfi(Vit))] — fi(VI) + α2η2 22n- +(I — ai)2η2等
+	8ηtL2(1 - a)
μ(1 - 8(ai - α2K))
+ 6(1 - a)l2ηtL
+10(1 - ai)2η2L2 卜τη2-ι (σ2 + (Zi + K)τ
μ
+ (6(I- 彳)2。L2 +6(1 - αi)2η2L2) 3τη2-ι (σ2 + 2KT
+
(1 - αi )2 E
K X Vfj(w(t)) -Vfi(Vit))| #
(32)
Plugging Lemma 5 yields:
E [k^(t+1)- V涧
≤ (1 - 等)E 帆')- V涧 -bηt (e [fi(V(t))] - fi(V：)) + a2ηt^K~
2 2 2σ
+ (1 - 0i) ηt ɪ
+
8ηtL2(1 - a)2
μ(1 - 8(ai - a K))
+6(I- αi)2 ηtL2+10(I- ai)2n2L2)3τη2-1
σ + (Zi + K)T
+ (6(I- 丁1 +6(1 - ɑi)2η2L2) 3τη2-ι (σ2 + 2KT)
+ (迎 +6η2) (1 - ai)2 2L2E 1|邸) - v*∣∣2] +6 CZi + 2K) +6L2E ]w(t) - w*k2] +6L2∆i
37
Under review as a conference paper at ICLR 2021
Then following the same procedure in Appendix E.1.3, together with the application of Lemma 7
we can conclude that:
fi(Vi) - fi(vɪ)
1T
≤ ɪ Xpt(fi(v(t)) -fi(v力
ST t=1
≤
p0E hkviI)-vM	1 X ( 2 2 2Kσ2 +∩ F 2 2σ2
bηosτ	+ bsτ ∑ptηt (αi ηt	+(I- αi) ηt -K
+ bST X(I- αi)2L2 (μ(1-8(α8-α2KX + M + 10η) 3τptη2-1 卜2 + (Zi + ^KH)
T
+ bST X(I-W)2L2 (μ + 1°ηt) 3τptηt-1 (σ2 +2 KT
+36(ι-ai )2 b∣2-X Pt(1+小
a3
(t- 1 +a)3
E [∣∣w(I)- w*k2i
+ t+16
+ ln(t + a)∖∖ 1536a2τ(σ2 +2τK) L2 + 128σ2t(t + 2a)
+ ln(t + a)	(a - 1)2μ4(t - 1+ a)3 + Kμ2(t - 1 + a)3
+ α2O ( -‰
μbT
+ 36(1 — ai)2 2Zζi + 2K + L
+ (i)2θ (2ζi-±2i + KL∆i
μb	b
+ (1 - ɑi)2 O(KLnl)+O
O K κ2τ2 (Zi + KK) + κ2τσ2
+	[	μbτ2
+O (κ4⅛^)).
□
F CONVERGENCE RATE WITHOUT ASSUMPTION ON αi
In this section, we provide the convergence results of Algorithm 1 without assumption on αi . The
following Theorem establish the convergence rate:
Theorem 7 (Personalized model convergence of Local Descent APFL without assumption on αi).
If each client's objective function is μ-Strongly-Convex and L-smooth, and its gradient is bounded
by G, using Algorithm 1, learning rate: η = .,+@), where a = max{64κ,τ}, and using average
scheme Vi = . PT=I PtgV, + (1 - ai)-K Pj∈ut Wjt)), Where Pt = (t + a)2, ST = PT=I Pt,
and fi is the local minimum ofthe ith client, then thefollowing ConVergenCe holdsfor all i ∈ [n]:
E[fi(Vi)] - fi ≤ O (bTT3) + α2O (μbτ) + (I- αi)2O
2	KL ln T	κ2σ2	κ2τ 2 (Zi + jK) + κ2τσ2
+ (1 -ɑi)	O(^T^J + O(μκτJ+ O ------μbτ2----
κ4τ (σ2 + 2τK
+OI	μbτ2
(33)
where b = min{ K, 1}
38
Under review as a conference paper at ICLR 2021
Remark 6. Here we remove the assumption ai ≥ max{ 1 一 4√κ，1 - 4√6κ√μ }. The key difference is
that we can only show the residual error with dependency on G, instead of more accurate quantities
Zi and ∆i. Apparently, when the diversity among data shards is small, Zi and ∆i terms become
small which leads to a tighter convergence rate. Also notice that, to realize the bounded gradient
assumption, we need to require the parameters come from a bounded domain W. Thus, we need to
do projection during parameter update, which is inexpensive.
Proof. According to (32):
E Mt+I)-V刑
E hkV(t) - v：k2i - bηt (E [fi(V(t))] - fi(vi))
+ 02η2 2Kσ- +(1 - 0i)2η2 2σ-
nK
8ηtL2(1 - 0i)2	, 6(I- ai)，L2	2 2「2	2
+ [μ(1-8(…2Kn)) + -■— + 10(I- Oi) ηt L I 3τηt-1
σ2 + (Zi + K )τ
μ
+ (6H +6(1-αi)2η2L2) 3τη2-i
σ2+2 A)
2
μ
+
j∈Ut
Here, we directly use the bound E
注 Pj∈Ut Vfj(W㈤)一Vfi(V(t)):
≤ 2G2 . Then we have:
E Mt+I)-V刑
≤ (1 - 午)E IVit)- 叫2i - bηt (E [fi(V(t))] - fi(v")
2 2 2Kσ2	2 2 2σ2
+ αiηt--+(I - αi) η ~ft~
nK
8ηtL2(I- ai产	6(I- Oi)2ηtL2	2 2 r2	2
+	~7~λ~~%-2κτ? +--------+10(1 - Oi) η L	3τηt-ι
μ(1-8(θi- 02Kn))
σ2 + (Zi + K )τ
μ
+ (6(1 - αi)2ηtL2 +6(1-αi)2η2L2) 3心1
+
μ
(1 - αi )2 G2 .
σ2+2 K
+
39
Under review as a conference paper at ICLR 2021
Then following the same procedure in Appendix E.1.3, we can conclude that:
fi(Vi) - fi(vɪ)
1T
≤ ɪ Xpt(fi(Vit)) -fi(v力
ST t=1	i
≤
p0E hkv(1)-v；k2i	1 X ( 2 2 2Kσ2	2 2 2σ2
bηosτ	+ bsτ ∑ptηt (αi ηt -n~+ (1 - αi) ηt ɪ
+ bSτ X(I- ai-2 (μ(1-8(α8-α2 K)) + " + 10ηt) 3τptη2-1 (σ2 + (Zi + KK)
+含 X(I- αi)2L2 (6+ι0ηt) 3τptη2-1 (σ2+2 KT)
+ 12(1- αi)2G2 ɪ
1 + ηt
μ
T
X pt
t=1
+ (1 - αi)2 O
2 (κ ( KL ln T ) ,o( κ2σ2 ) + ° ( KT 2(Zi + K) +κ2τσ2!	o( κ4 τ(σ2+2τ K)!!
+ (I-αi) [O(^T^J+ OIE)+ O[--μbτ2---++ O[-μbτ2—)y
□
G Proof of Convergence Rate in Nonconvex Setting
In this section we will provide the proof of convergence results on nonconvex functions. Let us first
present the convergence rate of the global model of APFL, on nonconvex function:
Theorem 8 (Global model convergence of Local Descent APFL). If each client’s objective function
is L-smooth, using Algorithm 1 with full gradient, by choosing K = n and learning rate η =
2√5L√T，then the following convergence holds：
T
T XwF(w(t))∣∣ ≤ O
t=1
+ O (亨
nT
Proof. The proof is provided in Appendix G.2.	□
As usual, let us introduce several useful lemmas before the formal proof of Theorem 3 and 8.
G. 1 Proof of Technical Lemmas
Lemma 8. Under Theorem 3’s assumptions, the following statement holds true:
fi(v(t+1)) ≤ fi(v(t)) - 8ηkVfi(Vi)k2
+ 3(I-Oi)2η1 X ∣∣w(t) - wjt)∣∣ +3α4(ι - α)2ηL2 ∣∣w(t) - w(t)∣∣
n j=1
+ 6η(1- αi2)2ζi + 12η(1- αi2)2 L2 DW2 + 12η(αi - αi2)2 ∣∣VF (w(t) )∣∣ .
40
Under review as a conference paper at ICLR 2021
Proof. Define the following quantities:
n
g㈤=α2Vfi(V为 + (1 - %) 1 X Vfj(Wjt))
n j=1
Pw(v(t∖g(t'),η) = 1 Vit)- Y 卜) - η ,Vfi(Vf)) + (I-Oi)； XVfj(Wjt))
According to Ghadimi et al. (2016) Lemma 1 :For all W ∈ W ⊂ Rd, g ∈ Rd and η > 0, we have:
hg, PW (W, g, η)i ≥ kPW (W, g, η)k2.
According to the updating rule and smoothness of fi , we have:
fi(v(t+1)) ≤	fi(v(t))	+ DVfi(V(t)), v(t+1)	-	v(t)E + LI∣v(t+1) -	V("2.
≤	fi(v(t))	+ DVfi(Vii()),v*1	-	v(t)E + L I∣v(t+1) -	V(t) ∣∣2
22
≤ fi(Vit)) -ηDVfi(Vit)),Pw(V(t),g(t),η)E + ɪ ∣∣Pw(Vit),g(t),η)∣∣
≤ fi(Vit)) - η(git), Pw(Vit),git'),η)'E - η DVfi(Vit))- git),Pw(Vit),git),η))
+ " ∣∣Pw(Vit),git),η)∣∣2
(34)
Using the identity:
Dgit),Pw(Vit),git),η)E ≥ kgit)k2,
and Cauchy-Schwartz inequality that
DVfi(Vit)) -g⑴,Pw(Vit),git),η)E ≤ 1 kVfi(Vit))-git)k2 +1 kPw(Vit),git),η)k2
we have:
41
Under review as a conference paper at ICLR 2021
fi(vit+1)) ≤ fiMt))-I■叫2 + 2 WWW,g(t),η)∣∣2 + η2L IlPWW,g(t),η):
I	I2
ηI	1 n	I
+ 2 Vfi(V(t)) - α2vfi(v(t)) - (1 - αi)- EVfj(Wjt))
I	n j=1	I
≤ fi W)-Q - η2L )I∣g(t)II2
X----------------------}
{z^^
≤ 4 η
+2
1n
Vfid- α2Vfi(V为一(1 - αi)- EVfj(Wjt))
2
j=1
2
≤ fi(Vit))- 4ηIW)II + (1 - ai)2η
-n
VF(wit)) - - EVfj(Wjt))
n j=1
+ η I,2 (Vfi(Vit))- Vfi(Vit))) - (1 - αi)VF(wit)) + (1- α2)Vfi(Vit)):
n
≤ fi(Vit))-利+叫 + (1- ai)2η- XIIWit)- wjt)II +/12 (Vfi(VP)-Vfi(Vi)))II
4	- j=1
+ 2η II(1 - α2)Vfi(Vit))-(I- α2)VF(Vit)) + (1-尴)VF(Vit))-(I- αi)VF(Wit)):
≤ fi(Vit))- 4η I,") I +(1 - ɑi)2η- X 牍⑴-Wjt) II + 2α4ηL21Vit)- Vit) I
-	j=1
+ 4η II(1 - α2)Vfi(Vit))-(I- α2)VF(V，))1+ 4η ](1 - αf)VF(Vit))-(I- %)VF(Wit))]2
≤ fi(Vit))- 1 η I,") I +(1 - 0i)2η- X 牍⑴-Wjt) I + 2α4(1 - α)2ηL2 i秒⑴-Wit) I
-	j=1
+ 4η(1 - α2)2Zi + 8η I(1 - α2)VF(Vit)) - (1 - α2)VF(Wit)): + 8η IN - α2)VF(Wit))^
n
≤ fi(Vit))- 4η Igit) I +(1 - ai)2η- X 卜⑶-Wjt)I +2α4(1 - α)2ηL2 IWt) - Wit)I
4	- j=1
+ 4η(1 - αi2)2ζi + 8η(1 - αi2)2 L2DW2 + 8η(αi - αi2)2 IIVF (Wit))II
Using the following inequality to replace kgit) k2:
kVfi(Vi)k2 ≤ 2∣∣Vfi(Vi)- g(t)∣∣2 + 2∣∣g(t)k2
hence we can conclude the proof:
42
Under review as a conference paper at ICLR 2021
fi(vit+1)) ≤ fi(V(t)) - 1 η [2kVfi(Vi)k2 -kVfi(Vi) - g㈤k2
+ (1 - ɑi)2η1 X ∣∣w(t) - wjt)∣∣ + 2α4(1 - a')i2ηL2 ∣∣w(t) - w(t)∣∣
n j=1
+ 4η(1 - αi2)2ζi + 8η(1 - αi2)2L2DW2 + 8η(αi - αi2)2 ∣∣VF (w(t))∣∣
≤ fi(V(t)) - 1 ηkVfi(Vi)k2 + 4η Vfi(Vi) - ^α2Vfi(v(t)) + (1- α. 1 X Vfj(Wjt))
n
+ (1- αi)2η— X ∣∣w(t) - Wj) ∣∣ + 2α4(1 - α)2ηL2 ∣∣w⑴一w(t) ∣∣
n j=1
+ 4η(1 - αi2)2ζi + 8η(1 - αi2)2L2DW2 + 8η(αi - αi2)2 ∣∣VF (W(t))∣∣
≤ fi(V(t)) - 1 ηkVfi(Vi)k2
+ 3(I-ai )2η1 X∣∣w(t)- wjt)∣∣ +3α4(1 - α)2ηL2 ∣∣w(t)- w(t)∣∣
n j=1
+ 6η(1 -	αi2)2ζi	+ 12η(1	-	αi2)2L2DW2	+ 12η(αi	-	αi2)2	∣∣VF (W(t))∣∣	.
2
□
Lemma 9. Under Theorem 3’s assumptions, the following statement holds true:
T	Tn
T X∣∣VF (w(t))∣∣ ≤ ηTF (W ⑴)+6L2 T X n X∣∣ Wjt)-W 叫.
t=1	η	t=1	j=1
Proof. Define the following quantities:
n
g(t = n X Vfj (Wjt))
j=1
R(t) = PW (W㈤，g(t),η) = 1 W(t) - Y ( W(t) - η1 X Vfj (Wjt))]
η	W	nj=1
According to the updating rule and smoothness of fi , we have:
F(W(t+1)) ≤ F(W㈤)+ DVF(W(t)), W(t+1) - W(t)E + L ∣∣W(t+1) - W(t)∣∣2
≤ F(W(t)) - η DVF(W(t)), PW(W㈤，g(t),η)> + η2L ∣∣Pw(W㈤,g(t),η)∣∣2
≤ F(W(t)) - η (g。), PW(W(t), g(t),η)) - η DVF(W(t)) - g(t),Pw(W(t), g(t),η))
+ η2L ∣∣pw(W(t),g(t),η)∣∣2
(35)
Using the identity:〈g(t), Pw(W⑶，g(t),η)) ≥ ||g(t)||2, and CaUchy-SchWartz inequality that
〈VF(Wo)) - g(t),Pw (Wo), g(t),η)) ≤ 2 ∣∣VF(wO)) - g(t)k2 + 2 ∣∣Pw (W⑴,g(t),η)k2 we have:
43
Under review as a conference paper at ICLR 2021
F(W(M) ≤ F(W㈤)一η Ug⑴U2 + (2 + η2L) Upw(W(t)，g(t),η)U2 + 2 UVF(W(t)) -g⑴U2
≤ F (W(t))-η Ug(t)U2+(2+¥ )Ug(t)U2+* nX U Wjt)-W(t)U2.
i=1
≤ F(W㈤)-1 -Ug(t):+ ηLL n XU Wjt)- W(t)].
i=1
Using the folloWing inequality to replace kg(t) k2:
IlVF(W(t))k2 ≤ 2kVF(W(t)) -g(t)k2 +2kg(t)k2
hence we obtain:
F(W(t+1)) ≤ F(Wo))- 4η (1 kVF(W㈤)k2 - kVF(Wo))- g㈤k2) + ηL2n XU Wj)- W㈤U2
i=1
≤ F(Wo))- 8ηkvF(WO))k2 + 3ηLn X U Wjt)
- w(t)2
i=1
n
Re-arranging terms and doing the telescoping sum from t = 1 to T :
T	T n
T X UVF(W(t))U ≤ -TF(W(I)) +6L2T X - XU Wjt)- W(t)U
t=1	η	t=1 j=1
□
Lemma 10. Under Theorem 3’s assumptions, the following statement holds true:
Tn
TX-XW)-W(t)U ≤ 10T2η2n,
t=1 i=1
T
T X UW㈤-W(t)∣∣ ≤ 200L2t4η4- + 20τ2η2ζi.
t=1	n
Proof. For the first statement, We define Yt = 1 Pn=IUW(t) - W(t)U , and let tc be the latest
synchronization stage. Then we have:
2
-n
Yt = X
n i=1
tn
Wtc- X η X Vfk(Wj))-
t 2n
T X ⅛ X
j =tc i=1
t 2n
T X ⅛ X
j =tc i=1
j=tc	k=1
1n
n X vfk (Wkj))-Vfi(Wj))
k=1
Wtc - X ηVfi(Wi(j))
j=tc
2
1n
-EVfk(Wkj))- Vfk(Wj)) + Vfk(Wj))- Vfi(Wj)) + Vfi(Wj))- Vfi(Wij))
n k=1
2
≤ T X 5η2(2L2Yj + n).
j=tc
44
Under review as a conference paper at ICLR 2021
Summing over t from tc to tc + τ yields:
tc+τ	tc+τ tc+τ
X Yt ≤ XX 5τη2 2L2γj + Z
t=tc	t=tc j=tc
(r+1)τ
≤ 10L2τ2η2 X Yj +5τ3η2Z.
n
j=rτ
Since η ≤ 2√5τL，We have 10L2τ2η2 ≤ 1, hence by re-arranging the terms We have:
tc+τ
X Yt ≤ 10τ3η2ζ.
n
t=tc
Summing over all synchronization stages tc, and dividing both sides by T can conclude the proof of
the first statement:
1 XYt ≤ 10τ2η2Z.
T t=1	n
(36)
To prove the second statement, let δti
- wi(t)	. Notice that:
δti
Wtc- X η X Vfk(Wkj)) -(Wtc- X nvfi(w(jɔ)ʌ Il
j=tc nk=1	j =tc
t
τXη2
j=tc
t
τXη2
j=tc
1n
-X Vfk(Wj)-Vfi(Wj
nk=1
1n
-EVfk(Wkj))- Vfk(Wj)) + Vfk(Wj))- Vfi(Wj)) + Vfi(Wj))- Vfi(W(j))
nk=1
2
t+τ
≤ T X 5n2 (L2Yj + L2δj + Zi).
j=tc
Summing over t from tc to tc + τ yields:
tc+τ	tc+τ tc+τ
X Yt ≤ X X 5τn2 (L2Yj + L2δj + Zi)
t=tc	t=tc j=tc
tc+τ	tc+τ
≤ 5L2 τ2 η2 X Yj + 5L2τ 2η2 X δji + 5τ 3η2Zi .
j=tc	j=tc
Since n ≤ 2√5τL，We have 5L2τ2n2 ≤ 4, hence by re-arranging the terms we have:
tc+τ	tc+τ
Xδti ≤ 20L2τ2n2 XYj+20τ3n2Zi.
t=tc	j=tc
Summing over all synchronization stages tc, and dividing both sides by T can conclude the proof of
the first statement:
1T	1T
T Xδi ≤ 20L2τ2n2T X Yt + 20τ2n2Zi.
t=1	t=1
Using the result from (36) to bound 21 PT=I Yt we have:
1T	Z
1Xδi ≤ 200L2T4n4Z + 20τ2n2Zi.
T t=1 t	n
□
45
Under review as a conference paper at ICLR 2021
G.2 Proof of Theorem 8
Proof. According to Lemma 9:
T	Tn
τXlIVF(w(t))∣∣ ≤ ητF(W(I))+6L2τXnX Il Wjt)-w(t)∣∣ .
t=1	η	t=1 j=1
Then plugging in Lemma 10 will conclude the proof.
□
G.3 Proof of Theorem 3
Proof. According to Lemma 8:
fi(v(t+1)) ≤ fi(v(t)) - 8ηkVfi(Vi)k2
+ 3(I-Oi)2n1 X l∣w(t) - Wjt)Il +3α4(1 — α产ηL2 ||w(t) - w(t)ι∣
n j=1
+ 6η(1 - αi2)2ζi + 12η(1 - αi2)2 L2 DW2 + 12η(αi - αi2)2 ∣∣VF (W(t) )∣∣ .
Re-arranging the terms, summing from t = 1 to T, and dividing both sides with T yields:
T
T X∣∣vfi(v(t))∣∣
t=1
≤ 8fi(V(1))
一 ηT
T	nT
+ 24α4(1 - a,)*T X Mt)- W⑴ ∣∣ +12(1 -而立21 X T X ∣∣Wjt)- W⑴ ∣∣
T t=1	n j=1 T t=1
1 T∣	∣2
+ 128(1 - ai) T SX ∣∣VF (W ) )∣∣ + 48(I- ai ) ζi + 128(1 - ai ) L DW,
t=1
Then, plug in Lemma 9 and 10 :
T
T X∣∣vfi(v(t))∣∣
t=1
≤ 8fiηT(i)) + 48(1 - a2)2Zi + 128(1 - a2)2L2Dw
T	nT
+ 24a4(1 - ai)2L2T X M)- W⑴ ∣∣ +12(1 -μ红匚 X T X ∣∣Wjt)- W⑴ ∣∣
T t=1	n j=1 T t=1
(C	TTrn	C
⅛F (W ⑴)+6L2 T X n X∣∣ Wjt)-W(t)∣∣
η	t=1	j=1
≤ 8fiηT(i)) + 48(1 - a2)2Zi + 128(1 - a2)2L2Dw
+ 24a4(1 - ai)2L2 ∣200L2T4η4Z + 20τ2η2Zi
in
+ 7800τ2η2(1 - ai)2L2Z + 1024(I.	F(W(I)).
n	ηT
46
Under review as a conference paper at ICLR 2021
Plugging in η
2√5√Tl concludes the proof：
T
T XNfi(V(t))∣∣
t=1
≤O
+ (1 - αi)2
+彳
+ αi4(1 - αi)2O
+ (I- α2 )2 (Zi + L2DW)
+(I- αi)2O( nτζ).
□
47