Under review as a conference paper at ICLR 2021
Optimization Variance: Exploring General-
ization Properties of DNNs
Anonymous authors
Paper under double-blind review
Ab stract
Unlike the conventional wisdom in statistical learning theory, the test error of
a deep neural network (DNN) often demonstrates double descent: as the model
complexity increases, it first follows a classical U-shaped curve and then shows
a second descent. Through bias-variance decomposition, recent studies revealed
that the bell-shaped variance is the major cause of model-wise double descent
(when the DNN is widened gradually). This paper investigates epoch-wise double
descent, i.e., the test error of a DNN also shows double descent as the number of
training epoches increases. Specifically, we extend the bias-variance analysis to
epoch-wise double descent, and reveal that the variance also contributes the most
to the zero-one loss, as in model-wise double descent. Inspired by this result,
we propose a novel metric, optimization variance (OV), to measure the diversity
of model updates caused by the stochastic gradients of random training batches
drawn in the same iteration. OV can be estimated using samples from the training
set only but correlates well with the (unknown) test error. It can be used to predict
the generalization ability of a DNN when the zero-one loss is used in test, and
hence early stopping may be achieved without using a validation set.
1	Introduction
Deep Neural Networks (DNNs) usually have large model capacity, but also generalize well. This
violates the conventional VC dimension (Vapnik, 1999) or Rademacher complexity theory (Shalev-
Shwartz & Ben-David, 2014), inspiring new designs of network architectures (Krizhevsky et al.,
2012; Simonyan & Zisserman, 2015; He et al., 2016; Zagoruyko & Komodakis, 2016) and reconsid-
eration of their optimization and generalization (Zhang et al., 2017; Arpit et al., 2017; Wang et al.,
2018; Kalimeris et al., 2019; Rahaman et al., 2019; Allen-Zhu et al., 2019).
Model-wise double descent, i.e., as a DNN’s model complexity increases, its test error first shows
a classical U-shaped curve and then enters a second descent, has been observed on many machine
learning models (Advani & Saxe, 2017; Belkin et al., 2019a; Geiger et al., 2019; Maddox et al.,
2020; Nakkiran et al., 2020). Multiple studies provided theoretical evidence of this phenomenon
in some tractable settings (Mitra, 2019; Hastie et al., 2019; Belkin et al., 2019b; Yang et al., 2020;
Bartlett et al., 2020; Muthukumar et al., 2020). Specifically, Neal et al. (2018) and Yang et al. (2020)
performed bias-variance decomposition for mean squared error (MSE) and the cross-entropy (CE)
loss, and empirically revealed that the bell-shaped curve of the variance is the major cause of model-
wise double descent. Maddox et al. (2020) proposed to measure the effective dimensionality of the
parameter space, which can be further used to explain model-wise double descent.
Recently, a new double descent phenomenon, epoch-wise double descent, was observed, when in-
creasing the number of training epochs instead of the model complexity1 (Nakkiran et al., 2020).
Compared with model-wise double descent, epoch-wise double descent is relatively less explored.
Heckel & Yilmaz (2020) showed that epoch-wise double descent occurs in the situation where dif-
ferent parts of DNNs are learned at different epochs, which can be eliminated by proper scaling of
step sizes. Zhang & Wu (2020) discovered that the energy ratio of the high-frequency components
of a DNN’s prediction landscape, which can reflect the model capacity, switches from increase to
1In practice, some label noise is often added to the training set to make the epoch-wise double descent more
conspicuous.
1
Under review as a conference paper at ICLR 2021
decrease at a certain training epoch, leading to the second descent of the test error. However, this
metric fails to provide further information on generalization, such as the early stopping point, or
how the size of a DNN influences its performance.
This paper utilizes bias-variance decomposition of the zero-one (ZO) loss (CE loss is still used in
training) to further investigate epoch-wise double descent. By monitoring the behaviors of the bias
and the variance, we find that the variance plays an important role in epoch-wise double descent,
which dominates and highly correlates with the variation of the test error.
Though the variance correlates well with the test error, estimating its value requires training models
on multiple different training sets drawn from the same data distribution, whereas in practice usually
only one training set is available2. Inspired by the fact that the source of variance comes from the
random-sampled training sets, we propose a novel metric, optimization variance (OV), to measure
the diversity of model updates caused by the stochastic gradients of random training batches drawn
in the same iteration. This metric can be estimated from a single model using samples drawn from
the training set only. More importantly, it correlates well with the test error, and thus can be used to
determine the early stopping point in DNN training, without using any validation set.
Some complexity measures have been proposed to illustrate the generalization ability of DNNs, such
as sharpness (Keskar et al., 2017) and norm-based measures (Neyshabur et al., 2015). However, their
values rely heavily on the model parameters, making comparisons across different models very dif-
ficult. Dinh et al. (2017) shows that by re-parameterizing a DNN, one can alter the sharpness of its
searched local minima without affecting the function it represents; Neyshabur et al. (2018) shows
that these measures cannot explain the generalization behaviors when the size of a DNN increases.
Our proposed metric, which only requires the logit outputs of a DNN, is less dependent on model
parameters, and hence can explain many generalization behaviors, e.g., the test error decreases as
the network size increases. Chatterji et al. (2020) proposed a metric called Model Criticality that
can explain the superior generalization performance of some architectures over others, yet it is unex-
plored that whether this metric can be used to indicate generalization in the entire training procedure,
especially for some relatively complex generalization behaviors, such as epoch-wise double descent.
To summarize, our contributions are:
•	We perform bias-variance decomposition on the test error to explore epoch-wise double
descent. We show that the variance dominates the variation of the test classification error.
•	We propose a novel metric, OV, which is calculated from the training set only and correlates
well with the test classification error.
•	Based on the OV, we propose an approach to search for the early stopping point without
using a validation set, when the zero-one loss is used in test. Experiments verified its
effectiveness.
The remainder of this paper is organized as follows: Section 2 introduces the details of tracing bias
and variance over training epochs. Section 3 proposes the OV and demonstrates its ability to indicate
the test behaviors. Section 4 draws conclusions and points out some future research directions.
2	Bias and Variance in Epoch-Wise Double Descent
This section presents the details of tracing the bias and the variance during training. We show that
the variance dominates the epoch-wise double descent of the test error.
2.1	A Unified Bias-Variance Decomposition
Bias-variance decomposition is widely used to analyze the generalization properties of machine
learning algorithms (Geman et al., 1992; Friedman et al., 2001). It was originally proposed for
2Assume the training set has n samples. We can partition it into multiple smaller training sets, each with m
samples (m < n), and then train multiple models. However, the variance estimated from this case would be
different from the one estimated from training sets with n samples. We can also bootstrap the original training
set into multiple ones, each with n samples. However, the data distribution of each bootstrap replica is different
from the original training set, and hence the estimated variance would also be different.
2
Under review as a conference paper at ICLR 2021
the MSE loss and later extended to other loss functions, e.g., CE and ZO losses (Kong & Dietterich,
1995; Tibshirani, 1996; Kohavi et al., 1996; Heskes, 1998). Our study utilizes a unified bias-variance
decomposition that was proposed by Domingos (2000) and applicable to arbitrary loss functions.
Let (x, t) be a sample drawn from the data distribution D, where x ∈ Rd denotes the d-
dimensional input, and t ∈ Rc the one-hot encoding of the label in c classes. The training set
T = {(xi, t)}n=ι 〜Dn is utilized to train the model f : Rd → Rc. Let y = f(x; T) ∈ Rc be
the probability output of the model f trained on T, and L(t, y) the loss function. The expected loss
ET [L(t, y)] should be small to ensure that the model both accurately captures the regularities in its
training data, and also generalizes well to unseen data.
According to Domingos (2000), a unified bias-variance decomposition3 of ET [L(y, t)] is:
ET[L(t, y)] = L(t, y) +β ET[L(y, y)],	(1)
`{}	`----{-----}
Bias Variance
where β takes different values for different loss functions, and y is the expected output:
y = arg min	ET[L(y*, y)].	(2)
y*∈Rc∣Pk=ι yk=1,yk≥0
y minimizes the variance term in (1), which can be regarded as the “center" or “ensemble" of y w.r.t.
different T.
Table 1 shows specific forms of L, y, and β for different loss functions (the detailed derivations
can be found in Appendix A). This paper focuses on the bias-variance decomposition of the ZO
loss, because epoch-wise double descent of the test error is more obvious when the ZO loss is used
(see Appendix C). To capture the overall bias and variance, we analyzed Ex,tET[L(t, y)], i.e., the
expectation of ET [L(t, y)] over the distribution D.
Table 1: Bias-variance decomposition for different loss functions. The CE loss herein is the com-
plete form of the commonly used one, originated from the Kullback-Leibler divergence. Z =
Pk=ι exp{Eτ[log yk]} is a normalization constant independent of k. H(∙) is the hard-max which
sets the maximal element to 1 and others to 0. 1∞n{∙} is an indicator function which equals 1 if its
argument is true, and 0 otherwise. log and exp are element-wise operators.
Loss	L(t, y)	y	β	
MSE	kt - yk2	ETy		1
CE	Pk=ι tk log y	Z exp{Eτ[log y]}		1
ZO	1con{H(t) = H(y)}	H(ET[H(y)])	1if y = -PT (H(y)	t, otherwise =t∣y = H(y))
2.2	Trace the Bias and Variance Terms over Training Epochs
To trace the bias term Eχ,t[L(t, y)] and the variance term Eχ,tEτ[L(y, y)] w.r.t. the training epoch,
we need to sample several training sets and train models on them respectively, so that the bias and
variance terms can be estimated from them.
Concretely, let T* denote the test set, f (x; T, q) the model f trained on T 〜Dn(j=1,2,...,K)
for q epochs. Then, the estimated bias and variance terms at the q-th epoch, denoted as B(q) and
V (q), respectively, can be written as:
B⑷=E(x,t)∈τ* [L (t,∕(x; q))],	⑶
Γ I K	]
V(q) = E(χ,t)∈τ* k £L(/(x; q),f(χ; Tj,q)) ,	(4)
j=1
3In real-world situations, the expected loss consists of three terms: bias, variance, and noise. Similar to
Yang et al. (2020), we view t as the groundtruth and ignore the noise term.
3
Under review as a conference paper at ICLR 2021
where
f(x; q)= H (XX H(f(x; Tj,q)),	(5)
is the voting result of {f (x; Tj, q)}jK=1.
We should emphasize that, in real-world situations, D cannot be obtained, hence Tj in our exper-
iments was randomly sampled from the training set (we sampled 50% training data for each Tj ).
As a result, despite of showing the cause of epoch-wise double descent, the behaviors of bias and
variance may be different when the whole training set is used.
We considered ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) models4 trained
on SVHN (Netzer et al., 2011), CIFAR10 (Krizhevsky, 2009), and CIFAR100 (Krizhevsky, 2009).
SGD and Adam (Kingma & Ba, 2014) optimizers with different learning rates were used. The
batchsize was set to 128, and all models were trained for 250 epochs with data augmentation. Prior
to sampling {Tj }jK=1 (K = 5) from the training set, 20% labels of the training data were randomly
shuffled to introduce epoch-wise double descent.
Figure 1 shows the expected ZO loss and its bias and variance. The bias descends rapidly at first
and then generally converges to a low value, whereas the variance behaves almost exactly the same
as the test error, mimicking even small fluctuations of the test error. To stabilize that, we performed
additional experiments with different optimizers, learning rates, and levels of label noise (see Appen-
dices E and G). All experimental results demonstrated that it is mainly the variance that contributes
to epoch-wise double descent.
VGGll
JeΛ∕ssq/Sso-
0 5 0 5 0 5
3 2 2 1 1 0
■ ■■■■■
HI HI HI HI HI HI
50	100 150 200 250
epoch
ResNetl8
0.40
J °∙35
之 0.30
in
.≡ 0.25
q
S 0.20
O
-0.15
O∙io
50	100 150 200 250
epoch
VGG16
JeΛ∕ssq/Sso-
50	100 150 200 250
epoch
ResNet34
JeΛ∕ssq/Sso-
0 5 0 5 0 5
3 2 2 1 1 0
■ ■■■■■
HI HI HI HI HI HI
50	100 150 200 250
epoch
(a) SVHN
(b) CIFAR10
JeΛ∕ssq/SSo-
50	100 150 200 250
epoch
(c) CIFAR100
Figure 1: The expected test ZO loss and its bias and variance. The models were trained with 20%
label noise. Adam optimizer with learning rate 0.0001 was used.
2.3	Discussion
Contradicting to the traditional view that the variance keeps increasing because of overfitting, our
experimental results show a more complex behavior: the variance starts high and then decreases
rapidly, followed by a bell curve. The difference at the beginning (when the number of epochs is
small) is mainly due to the choice of loss functions (see experimental results of bias-variance de-
composition for MSE and CE losses in Appendix F). CE and MSE losses, analyzed in the traditional
4Adapted from https://github.com/kuangliu/pytorch-cifar
4
Under review as a conference paper at ICLR 2021
learning theory, can reflect the degree of difference of probabilities, whereas the ZO loss only the
labels. At the early stage of training, the output probabilities are close to random guesses, and hence
a small difference in probabilities may lead to completely different labels, resulting in the distinct
variance for different loss functions. However, the reason why the variance begins to diminish at the
late phase of training is still unclear. We will explore this problem in our future research.
3	Optimization Variance (OV)
This section proposes a new metric, OV, to measure the diversity of model updates introduced by
random training batches during optimization. This metric can indicate test behaviors without any
validation set.
3.1	Notation and Definition
Section 2 verified the synchronization between the test error and the variance, but its application is
limited because estimating the variance requires: 1) a test set, and, 2) models trained on different
training sets drawn from the same data distribution. It’d be desirable to capture the test behavior of
a DNN using a single training set only, without a test set.
According to the definition in (1), the variance measures the model diversity caused by different
training samples drawn from the same distribution, i.e., the outputs of DNN change according to the
sampled training set. As the gradients are usually the only information transferred from training sets
to models during the optimization of DNN, we need to measure the variance of a DNN introduced
by the gradients calculated from different training batches. More specifically, we’d like to develop
a metric to reflect the function robustness of DNNs to sampling noises. If the function captured by
a DNN drastically varies w.r.t. different training batches, its generalization error is very likely to
be poor due to a large variance introduced by the optimization procedure. A similar metric is the
sharpness of local minima proposed by Keskar et al. (2017), which measures the robustness of local
minima as an indicator of the generalization error. However, this metric is only meaningful for local
minima and hence cannot be applied in the entire optimization process.
Mathematically, for a sample (x, t)〜D, let f (x; θ) be the logit output of a DNN with parameter
θ. Let Tb 〜Dm be a training batch with m samples, g : TB → Rlθl the optimizer outputting the
update of θ based on TB. Then, we can get the function distribution Fx(TB) over a training batch
Tb, i.e., f (x; θ + g(TB))〜 Fx(TB). The variance of Fx(TB) reflects the model diversity caused
by different training batches. The formal definition of OV is given below.
Definition 1 (Optimization Variance (OV)) Given an input x and model parameters θq at the q-th
training epoch, the OV on x at the q-th epoch is defined as
OVq(x)
ETB [kf (x; θq + g(TB))- ETBf (x; θq + g(TB))k2]
ETB [kf(x; θq + g(TB))k2]
(6)
Note that OVq(x) measures the relative variance, because the denominator in (6) eliminates the
influence of the logit’s norm. In this way, OVq (x) at different training phases can be compared. The
motivation here comes from the definition of coefficient of variation5 (CV) in probability theory and
statistics, which is also known as the relative standard deviation. CV is defined as the ratio between
the standard deviation and the mean, and is independent of the unit in which the measurement is
taken. Therefore, CV enables comparing the relative diversity between two different measurements.
In terms of OV, the variance of logits, i.e., the numerator of OV, is not comparable across epochs
due to the influence of their norm. In fact, even if the variance of logits maintains the same during
the whole optimization process, its influence on the decision boundary is limited when the logits
are large. Consequently, by treating the norm of logits as the measurement unit, following CV we
set OV to Pi σ2 / Pi μ2, where σ% and μ% represent the standard deviation and the mean of the i-th
logit, respectively. Ifwe remove the denominator, the value ofOV will no longer has the indication
ability for generalization error, especially at the early stage of the optimization process.
5https://en.wikipedia.org/wiki/Coefficient_of_variation
5
Under review as a conference paper at ICLR 2021
Intuitively, the OV represents the inconsistency of gradients’ influence on the model. If OVq (x) is
very large, then the models trained with different sampled TB may have distinct outputs for the same
input, leading to high model diversity and hence large variance. Note that here we emphasize the
inconsistency of model updates rather than the gradients themselves. The latter can be measured by
the gradient variance. The gradient variance and the OV are different, because sometimes diverse
gradients may lead to similar changes of the function represented by DNN, and hence small OV.
More on the relationship between the two variances can be found in Appendix B.
3.2	Experimental Results
We calculated the expectation of the OV over x, i.e., Ex [OVq (x)], which was estimated from 1,000
random training samples. The test set was not involved at all.
Figure 2 shows how the test accuracy (solid curves) and Ex [OVq (x)] (dashed curves) change with
the number of training epochs. Though sometimes the OV may not exhibit clear epoch-wise double
descent, e.g., VGG16 in Figure 2(c), the symmetry between the solid and dashed curves generally
exist, suggesting that the OV, which is calculated from the training set only, is capable of predicting
the variation of the test accuracy. Similar results can also be observed using different optimizers and
learning rates (see Appendix I).
1.0
0.9
U 0.8
u
π,0.7
0.6
0.5
VGGll
0.0025
w-” *∙l*
广f*‰kl4	0820
-0.0015
0.0010
0.0005
-----1-----1----1-----F 0.0000
50 IOO 150 200 250
epoch
ResNetlB
epoch
VGG16
UO
U
roo
0.5
0.0025
0.0020
0.0015
0.0010
0.0005
o∙oooo
0	50 100 150 200 250
epoch
(b) CIFAR10
(c) CIFAR100
(a) SVHN
g
ι
o
0
9
8
7
0
6
W词C
'⅛*4W'SIMZZm”
Figure 2: Test accuracy and OV. The models were trained with Adam optimizer (learning rate
0.0001). The number in each legend indicates its percentage of label noise.
Note that epoch-wise double descent is not a necessary condition for applying OV. As shown in
Figure 2 (blue curves), we compared the values of OV and generalization errors of DNNs when
there are 0% label noises, from which we can see the curves of generalization errors have no epoch-
wise double descent, yet the proposed OV still works pretty well.
OVq(x) in Figure 2 was estimated on all training batches; however, this may not be necessary: a
small number of training batches are usually enough. To demonstrate this, we trained ResNet and
VGG on several datasets using Adam optimizer with learning rate 0.0001, and estimated OVq(x)
from different number of training batches. The results in Figure 3 show that we can well estimate
the OV using as few as 10 training batches.
Another intriguing finding is that even unstable variations of the test accuracy can be reflected by
the OV. This correspondence is clearer on simpler datasets, e.g., MNIST (LeCun et al., 1998) and
FashionMNIST (Xiao et al., 2017). Figure 4 shows the test accuracy and OV for LeNet-5 (LeCun
et al., 1998) trained on MNIST and FashionMNIST without label noise. Spikes of the OV and the
test accuracy happen simultaneously at the same epoch.
6
Under review as a conference paper at ICLR 2021
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
0	50	100 150 200 250
epoch
ResNetl8
ResNetl8
ResNet34
(a) SVHN
(b) CIFAR10
(c) CIFAR100
Figure 3: OV estimated from different number of training batches. The models were trained with
20% label noise. Adam optimizer with learning rate 0.0001 was used.
(a) MNIST
(b) FashionMNIST
Figure 4: Test accuracy and OV. The model was LeNet-5 trained on MNIST and FashionMNIST
with Adam optimizer (learning rate 0.0001).
Our experimental results demonstrate that the generalization ability of a DNN can be indicated by
the OV during stochastic training, without using a validation set. This phenomenon can be used to
determine the early stopping point, and beyond.
3.3	Early Stopping without a Validation Set
The common process to train a DNN involves three steps: 1) partition the dataset into a training set
and a validation set; 2) use the training set to optimize the DNN parameters, and the validation set
to determine when to stop training, i.e., early stopping, and record the early stopping point; 3) train
the DNN on the entire dataset (combination of training and validation sets) for the same number of
epochs. However, there is no guarantee that the early stopping point on the training set is the same
as the one on the entire dataset. So, an interesting questions is: is it possible to directly perform
early stopping on the entire dataset, without a validation set?
The OV can be used for this purpose. For more robust performance, instead of using the OV directly,
we may need to smooth it to alleviate random fluctuations.
7
Under review as a conference paper at ICLR 2021
As an example, we smoothed the OV by a moving average filter of 10 epochs, and then performed
early stopping on the smoothed OV with a patience of 10 epochs. As a reference, early stopping with
the same patience was also performed directly on the test accuracy to get the groundtruth. However,
it should be noted that the latter is unknown in real-world applications. It is provided for verification
purpose only.
We trained different DNN models on several datasets (SVHN: VGG11 and ResNet18; CIFAR10:
VGG13 and ResNet18; CIFAR100: VGG16 and ResNet34) with different levels of label noise (10%
and 20%) and optimizers (Adam with learning rate 0.001 and 0.0001, SGD with momentum 0.9 and
learning rate 0.01 and 0.001). Then, we compared the groundtruth early stopping point and the
test accuracy with those found by performing early stopping on the OV6. The results are shown in
Figure 5. The true early stopping points and those found from the OV curve were generally close,
though there were some exceptions, e.g., the point near (40, 100) in Figure 5(a). However, the test
errors, which are what a model designer really cares about, were always close.
(a) Early stopping point
Figure 5: Early stopping based on test error (True) and the corresponding OV (Found). The shapes
represent different datasets, whereas the colors indicate different categories of DNNs (“CF" and
“Res" are short for “CIFAR" and “ResNet", respectively).
0.5-
0.4-
0.3-
0.2-
0.1-
oo-
0.0 0.1 0.2 0.3 0.4 0.5
True
(b) Test error
3.4	Small Size of the Training Set
For large datasets, a validation set can be easily parti-
tioned from the training set without hurting the gener-
alization performance. Therefore, OV is more useful in
the case of small datasets. To this end, we performed ex-
periments with small numbers (2000, 4000, 6000) of the
training samples in CIFAR10 to verify the effectiveness
of OV in this situation. Considering the limited numbers
of training samples, we trained a small Convolution Neu-
ral Network (CNN) using Adam optimizer with learning
rate 0.0001, whose detailed information can be found in
Appendix H.
The experimental results are shown in Figure 6. It can
be observed that: a) When the size of the training set is
small, OV still correlates well with the generalization per-
formance as a function of the training epochs, which ver-
0.3
υ
0.6
0.5-
0.4-
-0.0035
0.0030
-0.0025
-0.0020
[—acc, 2000	OV, 2000
K — acc, 4000	OV, 4000
1— acc, 6000 - OV, 6000
0.2-
0.0040
0.0015
-0.0010
0	50	100 150 200 250
epoch
0.0005
Figure 6: Test accuracy and OV using
small sizes of training sets. The num-
bers represent how many training sam-
ples are used for training.
ifies the validity of our results on small datasets; b) As expected, more training samples usually lead
to better generalization performance, which can also be reflected by comparing the values of OV.
O
6Training VGG11 on SVHN with Adam optimizer and learning rate 0.001 was unstable (see Appendix D),
so we did not include its results in Figure 5.
8
Under review as a conference paper at ICLR 2021
3.5	Network Size
In addition to indicating the early stopping point, the OV can also explain some other generalization
behaviors, such as the influence of the network size. To verify that, we trained ResNet18 with
different network sizes on CIFAR10 for 100 epochs with no label noise, using Adam optimizer with
learning rate 0.0001. For each convolutional layer, we set the number of filters k/4 (k = 1, 2, ..., 8)
times the number of filters in the original model. We then examined the OV of ResNet18 with
different network sizes to validate its correlation with the test accuracy. Note that we used SGD
optimizer with learning rate 0.001 and no momentum to calculate the OV, so that the cumulative
influence during training can be removed to make the comparison more fair.
The results are shown in Figure 7. As k increases, the OV
gradually decreases, i.e., the diversity of model updates
introduced by different training batches decreases when
widening ResNet18, suggesting that increasing the net-
work size can improve the model’s resilience to sampling
noise, which leads to better generalization performance.
The Pearson correlation coefficient between the OV and
the test accuracy reached -0.94 (p = 0.0006).
Lastly, we need to point out that we did not observe a
strong cross-model correlation between the OV and the
test accuracy when comparing the generalization ability
of significantly different model architectures, e.g., VGG
Figure 7: Test accuracy and OV w.r.t.
the network size.
and ResNet. Our future research will look for a more universal cross-model metric to illustrate the
generalization performance.
4	Conclusions
This paper has shown that the variance dominates the epoch-wise double descent, and highly cor-
relates with the test error. Inspired by this finding, we proposed a novel metric called optimization
variance, which is calculated from the training set only but powerful enough to predict how the test
error changes during training. Based on this metric, we further proposed an approach to perform
early stopping without any validation set. Remarkably, we demonstrated that the training set itself
may be enough to predict the generalization ability of a DNN, without a dedicated validation set.
Our future work will: 1) apply the OV to other tasks, such as regression problems, unsupervised
learning, and so on; 2) figure out the cause of the second descent of the OV; and, 3) design regular-
ization approaches to penalize the OV for better generalization performance.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. CoRR, abs/1710.03667, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proc. 36th Int,l Confi on Machine Learning, pp. 242-252, Long Beach, CA,
May 2019.
Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep networks. In Proc. 34th Int’l Conf. on
Machine Learning, volume 70, pp. 233-242, Sydney, Australia, August 2017.
Peter L Bartlett, Philip M Long, Ggbor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 2020. In press.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learn-
ing practice and the classical bias-variance trade-off. Proceedings of the National Academy of
Sciences, 116(32):15849-15854, 2019a.
9
Under review as a conference paper at ICLR 2021
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. CoRR,
abs/1903.07571, 2019b.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Niladri Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module criticality
in the generalization of deep networks. In Proc. Int’l Conf. on Learning Representations, Addis
Ababa, Ethiopia, April 2020.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Proc. 34th Int,l Conf. on Machine Learning, volume 70, pp. 1019-1028, Sydney,
Australia, August 2017.
Pedro Domingos. A unified bias-variance decomposition for zero-one and squared loss. In Proc. of
the 17th National Conf. on Artificial Intelligence, pp. 564-569, Austin, TX, July 2000.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning, vol-
ume 1. Springer series in statistics New York, second edition, 2001.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane d'Ascoli,
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. CoRR, abs/1901.01608, 2019.
Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural networks and the bias/variance dilemma.
Neural Computation, 4(1):1-58, 1992.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. CoRR, abs/1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pp. 770-778, Las
Vegas, NV, June 2016.
Reinhard Heckel and Fatih Furkan Yilmaz. Early stopping in deep networks: Double descent and
how to eliminate it. CoRR, abs/2007.10099, 2020.
Tom Heskes. Bias/variance decompositions for likelihood-based estimators. Neural Computation,
10(6):1425-1433, 1998.
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak,
and Haofeng Zhang. SGD on neural networks learns functions of increasing complexity. In
Proc. Advances in Neural Information Processing Systems, pp. 3491-3501, Vancouver, Canada,
December 2019.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
Proc. Int’l Conf. on Learning Representations, Toulon, France, April 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Int’l
Conf. on Learning Representations, Banff, Canada, April 2014.
Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions.
In Proc. 13th Int’l Conf. on Machine Learning, volume 96, pp. 275-283, Bari, Italy, July 1996.
Eun Bae Kong and Thomas G Dietterich. Error-correcting output coding corrects bias and variance.
In Proc. 12th Int’l Conf. on Machine Learning, pp. 313-321, Tahoe City, CA, July 1995.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL https:
//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep con-
volutional neural networks. In Proc. Advances in Neural Information Processing Systems, pp.
1097-1105, Lake Tahoe, NE, December 2012.
10
Under review as a conference paper at ICLR 2021
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting in
deep models: Effective dimensionality revisited. CoRR, abs/2003.02139, 2020.
Partha P Mitra. Understanding overfitting peaks in generalization error: Analytical risk curves for
l2 and l1 penalized interpolation. CoRR, abs/1906.03667, 2019.
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpo-
lation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1):
67-83, 2020.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In Proc. Int’l Conf. on Learning
Representations, Addis Ababa, Ethiopia, April 2020.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
CoRR, abs/1810.08591, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In Proc. NIPS Workshop on Deep
Learning and Unsupervised Feature Learning 2011, Granada, Spain, December 2011.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proc. of the 28th Conf. on Learning Theory, pp. 1376-1401, Paris, France, July
2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring gener-
alization in deep learning. In Proc. Advances in Neural Information Processing Systems, pp.
5947-5956, Long Beach, CA, January 2018.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In Proc. 36th Int’l Conf.
on Machine Learning, pp. 5301-5310, Long Beach, CA, May 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proc. Int’l Conf. on Learning Representations, San Diego, CA, May 2015.
Robert Tibshirani. Bias, variance and prediction error for classification rules. Technical report,
Department of Preventive Medicine and Biostatistics and Department of Statistics, University of
Toronto, Toronto, Canada, 1996.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE Trans. on Neural Networks,
10(5):988-999, 1999.
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying generalization
properties in neural networks. CoRR, abs/1809.07402, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. CoRR, abs/1708.07747, 2017.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance
trade-off for generalization of neural networks. CoRR, abs/2002.11328, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In Proc. Int’l Conf. on Learning Representa-
tions, Toulon, France, April 2017.
Xiao Zhang and Dongrui Wu. Rethink the connections among generalization, memorization and the
spectral bias of DNNs. CoRR, abs/2004.13954, 2020.
11
Under review as a conference paper at ICLR 2021
A Bias-Variance Decomposition for Different Loss Functions
This section presents detailed deduction of bias-variance decomposition for different loss functions.
A.1 The Mean S quared Error (MSE) Loss
FortheMSEloss, we have L(t, y) = ∣∣t - yk2, and need to calculate y based on (2). We first ignore
the constraints and solve the folloWing problem:
y = argminET[∣∣y* - y∣2],
y*
(7)
whose solution is yy = ETy. It can be easily verified that yy satisfies the constraints in (2), and hence
y = y = ET y.
Then, we can decompose the MSE loss as:
ET [kt - y∣2] = ET [kt - y + y - y∣2]
=ET [kt - yk2 + ky - yk2 + 2(t - y)T (y - y)]
= IIt - y∣∣2 + ET[∣∣y - y∣2]+0,	(8)
where the first term denotes the bias, and the second denotes the variance. We can also get β = 1.
A.2 Cross-Entropy (CE) Loss
For L(t, y)
Pk=ι tk log yk, y can be obtained by applying the Lagrange multiplier
method (Boyd & Vandenberghe, 2004) to (2):
„ .不
l(y*,λ)=ET X碇logyk +λ∙
,*
k=1
1 - X y力.
k=1
(9)
To minimize l(y*,λ), We need to compute its partial derivatives to y* and λ:
∂l
酝
∂l
∂λ
-	c,*	-
ET log yk + 1 - λ,
yk
c
1 - X yk.
k=1
k = 1, 2, ..., c
By setting these derivatives to 0, we get:
yk = 1 exp{Eτ[logyk]},	k = 1, 2,…,c
Z
where Z = Pck=1 exp{ET[logyk]} is a normalization constant independent of k.
Because
(10)
ET X ak log yk = - log Z, ∀ɑfc X
yk	k
k=1
αk = 1,
(11)
k=1


We have:

ET X tk log yk
k=1
ET
X tk (log I+log yk
Xtk log yk+ET Xtk log yk
k=1
k=1


X tk log Tk - log Z
W yk
X tk log tk- + ET
公 yk
X yk log I
(12)
k=1

from which we obtain β = 1.
12
Under review as a conference paper at ICLR 2021
A.3 The Zero-One (ZO) Loss
For the ZO loss, i.e., L(t, y) = 1con{H(t) = H(y)}, y is the voting result, i.e., H(ET[H(y)]), So
that the variance can be minimized. However, the value of β depends on the relationship between y
and t.
When y = t, we have:
ET [1con{H(t) = H(y)}] =0 + ET [1ran{H(y) = H(y)}]
=1con{H(t) = H(y)} + ET [1con{H(y) = H(y)}],	(13)
clearly, β = 1.
When y = t, we have:
ET [1con{H(t) 6= H(y)}] = PT (H(y) 6=t) = 1-PT(H(y)=t)
=1con{H(t) = H(y)}
-PT(H(y) = t∣H(y) = y) PT (H(y) = y)
— PT (H(y)= t∣H(y) = y) PT (H(y) = y).	(14)
Since y = H(t), it follows that
PT (H(y)= t∣H(y) = y) = 0.	(15)
Then, (14) becomes:
ET [1con{H⑻=H(y)}] = 1con{H⑻=HW)}- PT (HW)= t|H(y) = y) PT(HW) = y)
=1con{H(t) = H(y)}
— PT (H(y) = t∣H(y) = y) Et [1con{H(y) = H(y)}],	(16)
hence, β = -PT (H(y) = t∣H(y) = y).
B Connections between the Optimization Variance and the
Gradient Variance
This section shows the connection between the gradient variance and the optimization variance in
Definition 1.
For simplicity, We ignore q in OVq(x) and denote g(TB) - ETBg(TB) by g(TB). Then, the gradient
variance Vg can be written as:
Vg = Etb hkg(TB) - ETBg(TB)k2i = ETB [gC⅞)T3(Tb)] .	(17)
Denote the Jacobian matrix of the logits f(x; θ) w.r.t. θ by Jθ (x), i.e.,
Jθ(X) = [Vθfι(x; θ), Vθf2(x; θ),..., Vθfc(x; θ)],	(18)
where fj (x; θ) is the j-th entry of f(x; θ), and c is the number of classes.
Using first order approximation, we have:
f (x; θ + g(TB )) ≈ f (x; θ) + Jθ (Xyg(TB ),	(19)
and OV (X) can be written as:
OV (X) ≈
ETB [g(TB )T Jθ (x)Jθ (X)Tg(TB )]
f(x; θ)Tf(x; θ)+ ETB [O (kg(TB)k2)]
ETB [g(TB)TJe(x)Jθ(X)TGbB)]
f(x; θ)Tf(x; θ)

The only difference between Ex [OV (x)] and Vg is the middle weight matrix Ex
f(x；e)T f(χ ⑻].
Jθ (x) Jθ (X)T
This suggests that penalizing the gradient variance can also reduce the optimization variance.
13
Under review as a conference paper at ICLR 2021
Figure 8 presents the curves of Vg in the training procedure. It can be observed that Vg also shows
some ability to indicate the generalization performance. However, compared with the results in
Figure 2, we can see that OV demonstrates a stronger power for indicating the generalization error
than Vg . More importantly, Vg loses its comparability when the network size increases, while OV
can be more reliable to architectural changes with the middle weight matrix Ex [ JX(X)f X)eJ to
normalize Vg , which is illustrated in Figure 7.
We also notice that kETB g(TB)k22 is usually far less than ETB kg(TB)k22, hence Vg and the gradient
norm ETB kg(TB)k22 almost present the same curves in the training procedure.
1.0
0.9
υ0∙8
u
l00.7
0.6
0.5
VGGll
i	/N ""4如 M⅛%∣,
~g强加心ZMM
15.0
12.5
10.0
7.5 7
5.0
2.5
0.0
VGG16
0.6
0.5
0.4
0.3
0.2
0.1
30
25
20
15
10
5
0
0	50	100	150 200 250
epoch
0	50	100 150	200 250
epoch
(a) SVHN
(b) CIFAR10
(c) CIFAR100
Figure 8:	Test accuracy and Vg . The models were trained with the Adam optimizer (learning rate
0.0001). The number in each legend indicates its percentage of label noise.
C B ehaviors of Different Loss Functions
Many different loss functions can be used to evaluate the test performance of a model. They may
have very different behaviors w.r.t. the training epochs. As shown in Figure 9, the epoch-wise
double descent can be very conspicuous on test error, i.e., the ZO loss, but barely observable on CE
and MSE losses, which increase after the early stopping point. This is because at the late stage of
training, model outputs approach 0 or 1, resulting in the increase of the CE and MSE losses on the
misclassified test samples, though the decision boundary may be barely changed. When rescaling
the weights of the last layer by a positive real number, the ZO loss remains the same because of
the untouched decision boundary, whereas the CE and MSE losses are changed. Thus, we perform
bias-variance decomposition on the ZO loss to study epoch-wise double descent.
D VGG11 on SVHN by Adam Optimizer with Learning Rate 0.00 1
Training VGG11 on SVHN by Adam optimizer with learning rate 0.001 is unstable, as shown in
Figure 14(a). Figure 10 shows the test error and optimization variance. For 0% and 10% label noise,
the test error stays large (the test accuracy is low) for a long period in the early phase of training.
The optimization variance is also abnormal.
E Loss, Bias and Variance w.r.t. Different Levels of Label Noise
Label noise makes epoch-wise double descent more conspicuous to observe (Nakkiran et al., 2020).
If the variance is the major cause of double descent, it should match the variation of the test error
14
Under review as a conference paper at ICLR 2021
3
S
S ɔ
O 2
1
50	100 150 200 250
epoch
(a) CE loss
ʌ ⅛*⅜Ww∣⅛∙wλ
0.8
0.6
0.4
0.2
50	100 150 200 250
epoch
(b) MSE loss
(c) ZO loss
Figure 9:	Different loss functions w.r.t. the training epoch. ResNet18 was trained on SVHN, CI-
FAR10, and CIFAR100 with 20% label noise to introduce epoch-wise double descent. Adam opti-
mizer with learning rate 0.0001 was used.
(a) 0% label noise
(b) 10% label noise	(c) 20% label noise
Figure 10:	Test accuracy and optimization variance (OV) of VGG11 on SVHN, w.r.t. different levels
of label noise. Adam optimizer with learning rate 0.001 was used.
when adding different levels of label noise. Figure 11 shows an example to compare the loss, vari-
ance, and bias w.r.t. different levels of label noise. Though label noise impacts both the bias and the
variance, the latter appears to be more sensitive and shows better synchronization with the loss. For
instance, when we randomly shuffle a small percentage of labels, say 10%, a valley clearly occurs
between 20 and 50 epoches for the variance, whereas it is less obvious for the bias. In addition, it
seems that the level of label noise does not affect the epoch at which the loss reaches its first min-
imum. This is surprising, because the label noise is considered highly related to the complexity of
the dataset. Our future work will explore the role label noise plays in the generalization of DNNs.
ω-0u -ωqnj-
5 4 3 2 1
■ ■ ■ ■ ■
Ooooo
4 3 2 1 0
■ ■ ■ ■ ■
Ooooo
ω-0u -ωqnj-
4 3 2 1
■ ■ ■ ■
Oooo
4 3 2 1 0
■ ■■■■
Ooooo
ω-0u -ωqnj-
5 4 3 2 1
■ ■ ■ ■ ■
Ooooo
Figure 11:	Loss, variance and bias w.r.t. different levels of label noise. The model was ResNet18
trained on CIFAR10. Adam optimizer with learning rate 0.0001 was used.
15
Under review as a conference paper at ICLR 2021
F Bias and Variance Terms w.r.t. Different Loss Functions
VGGll	VGG13
,le>∕ssq/SSo- -le>∕ssq/SSo-
0.06-
0.04-
0.02-
0.00-
50	100 150 200 250
epoch
ResNetl8
0.05-
0.04-
0.03-
0.02-
0.01-
0.00-
50	100 150 200 250
epoch
,le>∕s'sq/Sso-
50	100 150 200 250
epoch
0.010
上 O oo8
ro
6 0.006
ro
W 0.004
S
in
° 0.002
0.000
VGG16
50	100 150 200 250
epoch
ResNetl8
ResNet34
0.010
1_ 0.008
ro
yj 0.006
ro
3 0.004
■2 0.002
0.000
50	100 150 200 250
epoch
(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 12:	Test MSE loss and the corresponding bias/variance terms. The models were trained with
20% label noise. Adam optimizer with learning rate 0.0001 was used.
VGGll
1.50-
1.25-
1.00-
0.75-
0.50-
0.25-
0.00-
loss
bias
var i***ww*
50	100 150 200 250
epoch
4 3 2 1 0
,IBΛ∕ssq/Sso-
ResNetl8
,le>∕ssq/Sso-
g
-
,le>∕s'sq/SSo-
,le>∕s'sq/Sso-
ResNet34
，50
ep
50
2
(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 13:	Test CE loss and the corresponding bias/variance terms. The models were trained with
20% label noise. Adam optimizer with learning rate 0.0001 was used.
16
Under review as a conference paper at ICLR 2021
G Bias and Variance Terms w.r.t. Different Optimizers and
Learning Rates
VGGll fail
6 4 2
■ ■ ■
Ooo
,IBΛ∕sq/Sso-
VGG13
,leΛ∕s'sq/Sso-
,IBΛ∕ssq/Sso-
VGG16
ResNetl8	ResN et34
ResNetl8

(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 14:	The expected test ZO loss and its bias and variance. The models were trained with 20%
label noise. Adam optimizer with learning rate 0.001 was used.
VGGll
,IBΛ∕ssq/SSo-
VGG13
,leΛ∕s'sq/Sso-
VGG16
ResNetl8	ResNet34
50
50
2
,leΛ∕s'sq/Sso-
50
00
2
(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 15:	Expected test ZO loss and its bias and variance. The models were trained with 20% label
noise. SGD optimizer (momentum = 0.9) with learning rate 0.01 was used.
17
Under review as a conference paper at ICLR 2021
VGGll
4
Llr
,IBΛ∕ssq/SSo-
VGG13
VGG16
ResNetl8
ResNetl8
ResN et34
(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 16:	Expected test ZO loss and its bias and variance. The models were trained with 20% label
noise. SGD optimizer (momentum = 0.9) with learning rate 0.001 was used.
H Detailed Information of the Small CNN model
We present the detailed information of the architecture trained with small numbers of training sam-
ples. It consists of two convolutional layers and two fully-connected layers, as shown in Table 2.
Table 2: Architecture of the small CNN model (“BN" is short for Batch Normalization).
Layers	Parameters	BN	Activation ∣ Max pooling	
Input	input Size=(32, 32) × 3	-	-	-
ConV	filters=(3,3)×32;	X	ReLU	(2, 2)
ConV	filters=(3, 3)×64;	X	ReLU	(2, 2)
Dense	nodes=1024	-	ReLU	-
Dense	nodes=10	-	Softmax	-
18
Under review as a conference paper at ICLR 2021
I Optimization Variance and Test Accuracy w.r.t. Different
Optimizers and Learning Rates
ResNetl8
o
0.0025
o.o-
0.0025
0.0020
0.0015
0.0010
0.0005
.P-OOOO
0	50 100 150 200 250
VGG16
0.6
0.4-
0.2-
一acc, 0%
--0V, 0%
—acc. 20%
0V, 20%
UO
roo
jf⅜∙∙	* '",g-<∙⅝i 'S~f
"∖*^kH∣⅛¼r**∙*****
l	I -
J
0.0020
0.0015
O
0.0010
0.0005
0.5
■I--r——Γ-~~P-~7π~4p.0000
O 50 IOO 150 200 250
epoch
epoch
(a) SVHN
(b) CIFAR10
(c) CIFAR100
UO
U
roo
0.5
1.0
0.9
U 0.8
u
π,0.7
0.6
0.5
ResNetlB
Figure 17: Test accuracy and optimization variance (OV). The models were trained with Adam
optimizer (learning rate 0.001). The number in each legend indicates its percentage of label noise.
	VGG16
0.7-	
0.6	
0.5	fj⅛Γ τ ",*,XΛ≠—**-**^,l -----ɪ-mr
0.4-	1 — acc, 0% acc, 20%
0.3	I — 0V, 0%	0V, 20%
0.2	*h"Z~f H♦.…a
o.ι-	J”	 _
0.0030
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
0	50 100 150 200 250
∕^
%yM4*il*'l<∙九'，"h∙∙f*A¼*J~iι'
0.005
0.004
0.003
0.002
0.001
.p.ooo
0	50 100 150 200 250
epoch
epoch
(a) SVHN
(b) CIFAR10
(c) CIFAR100
e
1
O
9
8
7
O
6
ι
o
0
9
8
7
0
6
31A'"l	ft*'
.--W” ~i∙∙" ,-l""f~
g
Figure 18: Test accuracy and optimization variance (OV). The models were trained with SGD op-
timizer (learning rate 0.01, momentum 0.9). The number in each legend indicates its percentage of
label noise.
19
Under review as a conference paper at ICLR 2021
VGGll
O.OO4
VGG13
0.005
acc	acc
：：rs^
0.003
0.002 g
.p.ooo
0	50 100 150 200 250
epoch
0.001
(a) SVHN
0.9
0.8
to 0.7
0.6
W2 *vv***'∙∙'',4¼ * ■
0.004
0.001
0.003
_O
0.002
0.000
0.5^-------
0	50
100 150 200 250
epoch
(b) CIFAR10
(c) CIFAR100
1
o
O


Figure
19: Test accuracy and optimization variance (OV). The models were trained with the SGD
optimizer (learning rate 0.001, momentum 0.9). The number in each legend indicates its percentage
of label noise.
20