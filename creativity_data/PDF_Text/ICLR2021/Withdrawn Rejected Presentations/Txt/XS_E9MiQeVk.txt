Under review as a conference paper at ICLR 2021
Mirror Sample Based Distribution Alignment
for Unsupervised Domain Adaption
Anonymous authors
Paper under double-blind review
Ab stract
Unsupervised Domain Adaption has great value in both machine learning theory
and applications. The core issue is how to minimize the domain shift. Motivated
by the more and more sophisticated distribution alignment methods in sample
level, we introduce a novel concept named (virtual) mirror, which represents the
counterpart sample in the other domains. The newly-introduced mirror loss us-
ing the virtual mirrors establishes the connection cross domains and pushes the
virtual mirror pairs together in the aligned representation space. Our proposed
method does not align the samples cross domains coarsely or arbitrarily, thus does
not distort the internal distribution of the underline distribution and brings better
asymptotic performances. Experiments on several benchmarks validate the supe-
rior performance of our methods.
1 Introduction
Transductive Unsupervised Domain Adaption (UDA) attracts a lot of attentions in recent years. In
UDA settings, one has the source domain data with labels and is expected to predict for the unlabeled
target domain data under the same task (image classification or segmentation). Those transferable
domains might be the images from different scenes, such as daylight and dark, or artclips and real-
world photos (Saenko et al. (2010)), or the different types of coherent corpus in natural language,
such as news and social media (Ramponi & Plank (2020)).
The main stream solutions are tackling with the core issue: domain shift, especially the covariate
shift (Shimodaira (2000); Shi & Sha (2012)). Specifically, define h(x) as the function to be learned,
we expect to minimize the target domain risk only through the source domain data:
RT (h)
X l l(h(χ),y)pτ(χ,y)dχ = X l l(h(x),y)PT(Xy PS(χ,y)dx	(1)
y∈Y	y∈Y	pS (x, y)
where l : R ×Y → R is the risk function, S and T refer to the source and target domains and Y is the
common label space. Imagine that if the distributions of source and target domains are consistent,
i.e. PS (x, y) a=.s PT (x, y), Eq.(1) can be further simplified as:
RT (h)
X
y∈Y
(h(x), y)PS (x, y)dx = RS (h)
(2)
meaning that minimizing the target risk is equivalently to minimizing the source domain risk. Note
that PS (x, y) = PS (y |x)PS (x), PT (x, y) = PT (y|x)PT (x), where PS(y|x) and PT(y|x) are the
labeling functions (Ben-David et al. (2010)), which cannot be manipulated (we do not consider the
label shift issue). Thus eliminating the covariate shift, i.e. aligning PT (x) and PS (x) in certain
feature space of x, can reduce the gap between the risks.
Following this line, current solutions leverage varieties of networks to extract domain-invariant fea-
tures in different representation spaces under different alignment or discrepancy metrics. Those
networks include one classifiers (Long et al. (2015); Chen et al. (2020)), task-specific classifier (i.e.
2 classifiers)(Long et al. (2016); Saito et al. (2018)), adversarial learning (Ganin et al. (2016); Xu
et al. (2019a); Tzeng et al. (2017); Ganin et al. (2016)) and even multiple stochastic classifiers (Lu
et al. (2020)). The alignment space might be the raw embedding space (Chen et al. (2019b)), re-
producing kernel Hilbert space(Long et al. (2015)), normalized space(Xu et al. (2019b)) and sphere
1
Under review as a conference paper at ICLR 2021
Projected samples in the
▲ •Source samples	Target SamPles	ideally Aligned space	VirtUal mirror
“A Transformation
-→∙ Push-forward operator
Left: aligning Mean/Std
Right: (virtual) mirror pair alignment
Figure 1: (Best viewed in color) Left: traditional class-aware method aligns the statistics (mo-
ment estimations) of the distribution but cannot align the ”shape” of distribution. Middle: mirror
pairs is the same instance in the ideally aligned common space by distribution transformation map
(push-forward operator), where (a, a), (b, b) are mirror pairs since they have the same corresponding
instances a* ,b in the aligned space and the remaining samples do not have mirrors in the opposite
domain sample sets. Right: our method will pull the (virtual) mirror instances together, where the
virtual mirror instances are estimated using manifold properties of the learned feature space. Details
can be found in Section 3.2.
space(Gu et al. (2020)). The discrepancy measures are the crucial part in the model designs, which
range from domain-level discrepancy (Long et al. (2015),Long et al. (2016)), class-wise discrep-
ancy(Kang et al. (2019)) and element-wise discrepancy(Chen et al. (2019b)). Typical measures
include the moment estimations of distribution(Long et al. (2015); Chen et al. (2020); Borgwardt
et al. (2006); Sun & Saenko (2016)), centers or prototype of each class(Pan et al. (2019)), margins
of inter-intra class distances(Chen et al. (2019a); Deng et al. (2019); Kang et al. (2019)) and even the
more sophisticated ones such as d-SNE (Xu et al. (2019c)), Wasserstein Distance (Lee et al. (2019))
and Margin Disparity Discrepancy(Zhang et al. (2019b)), etc. However, the statistics of distribution
or other manipulated measures, more or less suffer from the loss of information for the underline
distribution and distort the internal structure of the original distribution. The left part of Fig. 1 shows
an example, where even the mean and standard deviation (Std) of each class are same for different
domains, there are still many target outliers with respect to the classifier trained on the source data.
In this paper, we propose a novel method by aligning the distribution in the learned representation
space by designing a sophisticated domain constraint in sample level. Specifically, we introduce
new concepts: mirror and virtual mirror for each sample in both domains (see the middle and right
of Fig. 1 and Section 3.2). The mirror of one instance is the one in the other domain having the same
position in the aligned space. By constructing the constraints through the mirror pairs, i.e. the newly-
proposed mirror loss in Section 3.2, the learned representation is expected to be aligned in a more
fine-grained way without breaking the internal structure of the distribution. Our method takes full
advantages of sample-level information and is expected to have superior asymptotic performances
shown in Section 4.
2	Related Works
A large amount of domain adaption methods are committed to reduce the distribution discrepancy
between source and target domains, from coarse to fine. The early work originates from the frame-
work for analyzing and comparing distribution named maximum mean discrepancy (MMD, Gret-
ton et al. (2012)), which refers to the largest difference in expectations over functions in the unit
ball of a reproducing kernel Hilbert space (RKHS). Long et al. (2015) introduced Deep Adaption
Network(DAN) for learning transferable deep embeddings with multiple kernel variant of MMD.
Following this line, Sun & Saenko (2016) aligned the second-order statistics of two distributions;
Zellinger et al. (2017) and Chen et al. (2020) went further to align the high order moments of
domains. In addition to minimizing the domain discrepancy, many methods further work on the
class-level discrepancy. Kang et al. (2019) proposed Contrastive Domain Discrepancy(CDD), which
extended MMD to class-aware discrepancy, i.e. inter- and intra-class discrepancy based on condi-
tional data distribution. Chen et al. (2019a) introduced a discriminative loss by forcibly increasing
the margin between samples of different class and narrowing the distance between samples of the
same class. Pan et al. (2019) introduced the prototype of each class in source domain. By predict-
2
Under review as a conference paper at ICLR 2021
ing the pseudo label for target samples, it aligned the class-aware prototypes for source, target and
source-target mixed domains. Tang et al. (2020) incorporated the soft cluster assignments into the
discriminative clustering to directly uncover the intrinsic target distribution as well as constrained
the clustering solution on source data.
Many other measures of discrepancy are proposed in terms of manifold or optimal transport of dis-
tributions. Wang et al. (2018) embedded Geodesic Flow Kernel (Gong et al. (2012)) to transform
the features to Grassman manifold and used MMD to align marginal and conditional distributions.
Xu et al. (2019c) utilized a relaxed version of Hausdorff distance named d-SNE to minimize intra-
class distance and maximize inter-class distance by nearest neighbors in manifold. Flamary et al.
(2016); Courty et al. (2017); Damodaran et al. (2018) took advantage of simplified discrete version
of optimal transport theory, concluding the domain adaption issue to find a discrete mapping between
source and target data, with regularizations on either the mapping function (Flamary et al. (2016)) or
the final classifiers(Damodaran et al. (2018)) Inspired by GAN(Goodfellow et al. (2014)), adversar-
ial and generative methods are also used to learn the domain-invariant representation. DANN(Ganin
et al. (2016)) introduced the gradient reversal layer to perform gradient-level adversarial updating.
ADDA (Tzeng et al. (2017)) learned two different feature extraction networks with asymmetry ad-
versarial losses on the source and target extractors. DM-ADA (Xu et al. (2019a)) incorporated
domain mix-up to the VAE framework with discriminators using soft label and triplet loss. 3CATN
(Li et al. (2019)) used two feature translators and three domain discriminators to minimize cycle-loss
in the way like Cycle-GAN (Zhu et al. (2017)).
Note that although trying to, the current works do not take full usage of all data information. The data
is a direct sampling of the underline distribution, thus a proper usage of the sample-based alignment
would achieve more fine-grained and better asymptotic domain alignment. There are already some
works (Chen et al. (2019a); Pan et al. (2019); Damodaran et al. (2018)) paying attention to investi-
gate the sample level information in UDA. However, they either use intra-domain pairwise sample
distance to measure the class margin, or arbitrarily construct pairwise data mapping cross domains.
They do not consider the internal structure of the distribution (Flamary et al. (2016); Damodaran
et al. (2018)), and even align the samples that may not be “equivalent” since the samples might
be sampled differently in different domains. Tang et al. (2020) has pointed out the importance of
the internal structure for domain adaption, but they only utilize the class information by structural
clustering. In fact, aligning the distribution by sample data can trace back to the work of Sugiyama
et al. (2008) and Gretton et al. (2009), but there is no related research in the UDA field.
3	Proposed Methods
Similar to the traditional UDA settings, we denote the source samples with ground truth as
{(xis, yis)}in=s1, xis ∈ XS = {xis}in=s1, yis ∈ Y and target samples as {xtj}jn=t 1 = XT , where Y
is the shared labeling set with C classes. pS (x) and pT (x) are the underline sample distributions for
source and target domains, with DS and DT being the supports of those distributions respectively.
XS and XT are the realization sets sampled from DS and DT following pS (x) and pT (x).
3.1	Mirrors cross Domains
We introduce a new concept named mirror that reflect equivalent samples cross domains. Formally,
define the mirror pair as two realizations of random variables in the source and target distributions
respectively that play ”similar roles”. In terms of the transportation theory (COT (2019)), let Ts and
Tt be the two transformation maps (push-forwards operators) on pS and pT such that the resulting
distributions are same, i.e. Ts#pS = Tt#pT , then xs ∈ DS and xt ∈ DT are mirrors ifTs#pS(xs) =
Tt#pT (xt). In general, infinite number of mirrors can be found since ∀xs ∈ DS, we could always
find xt ∈ DT such that Ts#pS(xs) = Tt#pT (xt)(COT (2019)). An ideal distribution alignment can
be achieved by aligning the every mirror pairs.
Rather than investigating Ts and Tt like Flamary et al. (2016) and Damodaran et al. (2018), we
resort to deep neural networks to approximate the transformations. However, it is impractical to
directly find the mirrors in application. The dataset XS and XT are actually a random sampling
from the underline support DS or DT . The real mirror for xis ∈ XS may not exists in XT at all.
3
Under review as a conference paper at ICLR 2021
Figure 2: Illustrations of the virtual mirrors: the mirror
sets Xs and Xt are calculated by Eq.(5) and virtual
mirrors are calculated by Eq.(6). We show the case
when we set k = 3, i.e. using the top-3 nearest samples
to construct the virtual mirrors.
C	∙ 11 ,	1	1	∙ , 1	♦ F Fl Γ∙ -Γ-<∙ t 1	7	C 1 ~ T ~ ~ 1	. 1
Some illustrative examples are shown m the middle of Fig.1, where d, e, f and c, h, i, j do not have
mirrors. To cope with this issue, we summarize the two asymptotic properties of the mirrors. Those
properties are later used to construct the virtual mirror for real world data and further formulate cross
domain distribution constraints for alignment. In the following, we also use xis and xjs, XS and XT ,
pS and pT to denote the samples’ embedding features, sets and distributions after the certain feature
extractor without introducing confusing notations.
Local Similarity of Mirrors. The most obvious mirrors are the centroids of the source and target
domains for each class c, denoted by ccs and ctc. As pointed in Bengio et al. (2013) and Xu et al.
(2019c), we could assume the learned features after certain deep feature extractor lie in a manifold,
then one instance can be represented in terms of local probabilistic neighbors. For any x in DS, DT ,
define the position relative to the class c’s centroid cc as
q{s,t}	=	eχp(-d(χ, c{s,tD)
C	PC=I eχp(-d(χ,c{s,tt}')')
(3)
where d is a distance measure. Then the representation of χ considering the local similarity is
q{s,t} (χ) = [q1{s,t}, q2{s,t}, . . . , qC{s,t}]. If χt ∈ DT and χs ∈ DS are mirrors, their relative distance
representation should be same, i.e. qs (χs ) = qt(χt).
Inter-domain Closeness of Mirrors. If the source domain and target domain are aligned ideally in
the feature space, the centroids of the same class cross and the distribution of the representation χ
are exactly same. In other words, the source and target domains are two different views of the same
distribution. The mirror pairs are exactly the same instance. In practice, strict alignment cannot be
achieved. However, following the idea of d-SNE in (Xu et al. (2019c)), the mirror of an instance
should be closer than any other instances in the counterpart the domains. This can be formulated as
χis = arg min d(χ, χtj )	(4)
i	x∈DS	j
where χis is the mirror of χtj ∈ DT in the source domain DS , vice versa.
The above two inter- and intra- domain properties of the mirrors pave the way to find the mirrors in
real dataset and further achieve the alignment(constraint) cross domains.
3.2	Domain Alignment with Virtual Mirror Samples
The above two properties for mirror pairs are stated in terms of DS and DT rather than real data
sets XS and XT . The mirror of a specific instance χis ∈ XS may not exist in XT . Fortunately, we
could define an approximated mirror, i.e. virtual mirror using the above two properties in terms of
XS and XT . The mirror property in Eq.(4) is rewritten as
X S (xj) = arg >X∈χs d(x,xj)	(5)
where >k is the “top-k" operation that selects the top k of set in ascending order, XS (Xj) is the mir-
ror set in source domain corresponding to the sample xtj . The virtual mirror sample approximating
the real mirror can be constructed following the local linearity assumption in manifold based on the
mirror set (see Fig 2). Specifically, the virtual mirror of xtj is
Xs(Xj)=	X ω(x, xtj )x	(6)
x∈Xs (xtj )
where ω(x,xj) = e-d(x,xj)/ Pχ∈χS(Xt) e-d(x,xj) or simply 1/k, which is experimentally inves-
tigated in Appendix C.2 and the selection of k will be experimentally investigated in Section 5.3.
Note that Xs(Xj) may not exist in XS. Aligning the virtual mirrors is essentially different from the
4
Under review as a conference paper at ICLR 2021
Mirror Selector
Cluster
Features of all
source samples
Calculate Wilh
Label
优
( Features of all
target samples
Calculate By
K-means
CC
C
Figure 3: Overall structure of the model: the mirror selectors are applied after the averaging pooling
of the backbone and the following FC layer; cluster algorithm is also applied on those representations
layers to calculate the centers of both source and target domains.
work like Courty et al. (2017) and Damodaran et al. (2018) since we do not forcibly impose the
pairwise mapping for the sampling data cross domains. When k approaches to ns, the Eq.(5) will
degenerate to MMD (Long et al. (2015)), i.e. the mirror of any target data is the mean of the source
distribution. The alignment only considers the means of both domain distributions.
Obtaining the virtual mirror for each sample in both source and target domains, we propose the
mirror loss, which is based on the local similarity property in Section 3.1 and formulated as
nt	ns
Lmirror = F X KL(q(Xs (M )) k 9闺))+ -s X KL(q(Xt (x； )) k q(x；))	⑺
nt	ns
i=1	i=1
where	xis	∈	XS,	xtj	∈	XT ,	qc(x)	=	e-d(x,cc)/ PcC=1 e-d(x,cc)	and	q(x)	=
[q1 (x), q2(x), ..., qC (x)]. The mirror loss is to minimize the KL divergence of the miror paris’
relative distances to the class centroids, which is motivated by the idea of the derivation of SNE in
Hinton & Roweis (2003). For the source domain, we calculate the class-wise centroids using the
ground truth Cs = / PyS=C x；, nsc denote the number of samples belonging to the class c. With-
out the ground truth, we use K-Means (Dhillon et al. (2004)) to cluster target domain samples to
get ctc. To enhance the alignment of both domains, we use the common centers of both domains
as cc = 1/2ccs + 1/2ctc to calculate the local similarity in Eq.(7). Combining the Eq.(6) and (7)
, the loss can be applied to any learnable representation layer to force a fine-grained alignment of
distribution.
3.3 Model Structure and Algorithm
Incorporating the mirror loss into the model is simple and straightforward. Fig 3 illustrates a typ-
ical model structure. After the backbone network ends with pooling layer, two newly-added full-
connected layers are appended. The first full-connected layer is to generate intermediate repre-
sentation and last full-connected layer plays the role of the classifier based on specific task. We
incorporate the mirror loss to do the alignment in two learnable space: the output of the final pool-
ing layer of backbone and the first FC layer, denoted as f ∈ Rdf and g ∈ Rdg with dimensionality
df and dg respectively. As shown in Fig 3, the mirror selector uses the target feature fjt and the
cached source feature {fis } (source feature fis and cached target feature {fjt}) to find the virtual
sample in f by Eq.(5) and (6). Same operation is carried on for g. The mirror losses for f and g are
and L
Lmirror,f
mirror,g
For the labeled source data {(xis, yis)}in=S1, the generic cross entropy with respect to the source su-
PerviSion is also involved, i.e. Ls = -* Pn=I Pc=ι I(y； = c) logps,c. where ps,c is the Pre-
dicted probability for class c and I is the indicator function. Although there is no ground truth for
target domain, we use the unsuPervised discriminative clustering method introduced in Jabi et al.
(2019). Following Jabi et al. (2019), denote the introduced auxiliary target distribution as zit, then
the discriminative clustering method can be formulated as two stePs: Parameter learning steP, which
regards the zit as the Pseudo label of the target domain labels and uPdates the network by mini-
1 nt
mizing the cross entropy loss; Ldis = - nt ∑2i=ι Zt log pt where Pt is the predicted probability
using current learned network; Auxiliary uPdate steP, which uPdates the auxiliary distribution zit as
5
Under review as a conference paper at ICLR 2021
zt,c (X
Pt,c
(PntI pi,c )1/2.
The detailed derivation can be found in Jabi et al. (2019). The above two steps
will iterate once for each epoch. The overall loss of the model is
Ls + Ltdis + γ(Lm
irror,
f + Lmirror,g )
(8)
L
where γ is the weight of mirror loss. The detailed algorithm can be found in Appendix A.2.
4 Theoretical Analysis
Although the Eq.(1) and (2) can explain the motivation of our proposed method, we will give a
detailed analysis for the mirror sample alignment with respect to the theoretical framework for the
domain adaption in Ben-David et al. (2010). The proofs of the propositions are in Appendix B.
Lemma 1. Ben-David et al. (2010) Given the hypothesis class as H, then we have
∀h ∈ H, RT(h) ≤RS(h) + 2dH∆H(S, T) + λ
where
λ = min{RS(h, hS) + RT(h, hT)}
h∈H
and
dH∆H(S,T) = 2 sup ∣Prχ^DS [h(x) = h0(x)] — Prx〜DT[h(x) = h0(x)]∣
h,h0∈H
(9)
(10)
(11)
hS and hT are the labeling function in each domains.
Lemma 1 lays a generic analysis basis for domain adaption on the risk of target domain RT (h)
under a given function space H. The upper bound consists of three part: the risk of source domain
RS (h), the domain discrepancy in terms of functional differences dH∆H and the combined error of
the ideal joint hypothesis λ.
Proposition 1. Define H as the hypothesis class of function mapping from D to Y, where D is
an intermediate representation space. DS and DT are the supports of distribution for source and
target distribution in D. If S and T are aligned in distribution in space D, then dH∆H (S, T) → 0.
Proposition 1 states that the distribution alignment for certain learnable space will reduce the domain
discrepancy in terms of functional differences dH∆H .
The above distribution alignment can be achieved empirically by minimizing the Lmirror in Eq.(7).
From the definition, we can see that Lmirror is minimized if and only if qs (xis) = qt(xtj) for every
mirror pairs between the domains. It means: 1) the class centers for both source and target domain
are same, 2) the mirror pairs cross domains have the same position relative to the common centers
cc,c = 1,2,…，C. Thus the empirical density function ΦS(x) and ΦT(x) over XS and XT are
same. ΦS (x) = ΦT(x). Under the assumption that XS and XT are UnbiasedIy sampled from DS
and DT according to the density function Φs and Φt, Glivenko-Cantelli theorem (Rachev et al.
(2013)) could assure when nt , ns → ∞, we have
Φs(x) a=. ΦS(x) = ΦT(x) a=. Φt(χ)
(12)
where a.s. means almost surely. Minimizing Lmirror enforces the learned feature space to be the
same, further reducing dH∆H(S, T) to zero empirically when the number of sample is large.
Proposition 2. Define λm + 2 dm∆H as the corresponding λ + 1 "hδh with the Lmirror mini-
mized, λo +1 dH∆H as the same part without minimizing Lmirror. Ifminimizing Lmirror aligns the
distribution in the learned space, we have
λm + 2dm∆H ≤ λo + 2dH∆H
(13)
Proposition 2 states when using the mirror loss would get a lower gap between the target risk and
source risk, which is consistent to our motivation in Eq.(2). The key insight behind proposition 2 is
that if the discrepancy of domains is empirically approaching to 0 by the loss Lmirror according to
proposition 1, we can have a more relaxed feasible hypothesis set of H, leading to a lower value of
λ.
6
Under review as a conference paper at ICLR 2021
5 Experiments and Results
5.1	Datasets and Implementations
We use Office-31 (Saenko et al. (2010)), Office-Home(Venkateswara et al. (2017)), ImageCLEF
and VisDA2017(Peng et al. (2017)) to validate our proposed method. For the first three datasets,
we use all the tasks. For the VisDA2017, we use “train” set as source domain and “validation” set
as target domain on classification task. We implement our experiments in PyTorch. For all tasks,
we use ResNet50 or ResNet101 pre-trained on the ImageNet as backbone. To adapt the task, the
number of units in fully connected layers are changed. Details are in Appendix A.1.
5.2	Results and Comparison with The State-of-the-art
Table 1: Average accuracy(%) on Office-31, Office-Home, ImageCLEF and VisDA2017. All the re-
suits are trained based on ResNet50 except those with mark*, which are trained based on ResNet101.
Red indicates the best result while Blue means the second best.
Method	OffiCe-31	Office-Home	ImageCLEF	VisDA2017
Source Model(He et al. (2016))	761	46.1	80.7	524
DAN (Long et al. (2015))	82.3	56.3	—	—
DANN (Ganin et al. (2016))	82.6	57.6	—	—
ADDA (Tzeng et al. (2017))	83.2	—	—	—
JDDA (Chen et al. (2019a))	80.2	—	—	—
DSR (Cai et al. (2019))	88.6	64.9	—	—
DM-ADA (Xu et al. (2019a))	81.6	—	—	75.6t
rRevGrad+CAT (Deng et al. (2019))	87.6	—	87.3	—
SAFN (Xu et al. (2019b))	87.1	67.3	88.9	—
MDD (Xu et al. (2019b))	88.9	68.1	—	74.6
SymNets (Zhang et al. (2019a))	—	—	89.9	—
CAN (Kang et al. (2019))	90.6	—	—	87.2t
SHOT (Liang et al. (2020))	88.7	—	—	79.6t
MCSD (Zhang et al. (2020))	—	—	90.0	71.3
SRDC (Tang et al. (2020))	90.8	71.3	90.9	—
Ours	91.1	72.6	91.6	87.9t
Table 1 shows the average results of our method on the four datasets (The detailed results for each
task can be found in Appendix C.1). We can find that our method has made a significant improve-
ment over the existing SOTA methods. For the relatively simple datasets Office-31 and ImageCLEF,
our method improves by 0.3% and 0.7%, respectively. For the more challenging dataset Office-
Home, our method improves by 1.3%. The average accuracy of our method can also be significantly
improved on large-scale dataset VisDA2017, i.e. a 0.7% improvement on it.
5.3	Ablation Study And Parameter Sensitivity
Table 2: Ablation studies using Office-Home dataset based on ResNet50.(K = 3)
Baseline	DC	FC Mirror	Backbone Mirror	Avg
X				46.1
X	X			71.6
X	X	X		71.7
X		X	X	65.5
X	X	X	X	72.0
We take Office-Home as an example to investigate the different components of the proposed model.
In Table ??, “Baseline” only uses backbone and the labeled source data to train the model and
test on target domain; “DC” means discriminative clustering described in Section 3.3 and Eq.(8);
“Backbone Mirror” means applying the mirror loss to the last pooling layer of the backbone; “FC
Mirror” means applying the mirror loss to the output of first fully-connected layer after the backbone.
We set K = 3 for all the experiments. From the results, we can observe that both “DC” and
mirror alignment can improve the accuracy on target domain. Without discriminative clustering, the
accuracy is reduced by 6.5%. Without mirror alignment, the accuracy is reduced by 0.4%. Applying
the mirror loss on both the backbone and FC layer is helpful for the final performance. The detailed
results for each task are in in Appendix C.2.
The parameter K in Eq.(5) controls how we construct the virtual mirrors. A larger K means
choosing a larger mirror set and it may lead to more indistinguishable mirrors. A smaller K
may get a wrong/unstable mirror. In Table 3, we investigate the accuracies with different Ks, i.e.
K = 1, 3, 5, 7, 9. In terms of average accuracy, we can see that K = 3 is the best choice. However,
7
Under review as a conference paper at ICLR 2021
Table 3: The inflUence of K in Mirror Selector for Office-Home
K	Ar-Cl	Ar-Pr	Ar-Rw	Cl-Ar	Cl-Pr	Cl-Rw	Pr-Ar	Pr-Rw	Pr-Cl	Rw-Pr	Rw-Cl	Rw-Ar	Avg.
1	52.37	75.3	81.4	71.0	76.9	69.98	^^77.3-	54.57	82.3	85.2	57.22	76.64	71.69
3	52.01	75.7	81.1	71.9	77.0	69.65	78.7	54.89	82.1	85.2	58.55	77.3	72.00
5	51.18	75.3	81.2	70.7	76.2	70.85	76.8	54.89	82.3	84.8	58.59	76.93	71.64
7	51.69	74.7	80.9	70.3	76.2	70.31	77.2	55.71	81.6	84.9	59.37	77.87	71.72
9	51.07	75.8	80.5	70.9	76.3	70.89	77.3	55.34	82.2	84.7	57.93	77.21	71.67
for different tasks, the choice might be different. For example, for the task of Pr-Ar, the optimal K
is 3 while for Rw-Cl, the optimal K will be 7 with a large margin.
Table 4: The sensitivity of γs for the Mirror Loss for Office-Home													
λmirror	Ar-Cl	Ar-Pr	Ar-Rw	Cl-Ar	Cl-Pr	Q-Rw	I	r-Rw	Pr-Ar	Pr-Cl	Rw-Pr	Rw-Cl	Rw-Ar	Avg.
0.0	52.05	74.9	81.7	70.3	76.6	-77.9	82.2	70.56	55.3	84.8	58.09	76.43	71.74
1.0	52.01	75.7	81.1	71.9	77.0	78.7	82.1	69.65	54.89	85.2	58.55	77.3	72.01
2.0	51.98	75.6	81.1	71.3	77.2	77.0	81.4	72	54.68	84.8	58.71	77.46	71.94
3.0	51.43	75.2	80.8	70.5	77.1	77.6	81.9	70.02	54.68	85.1	57.52	76.84	71.56
In Eq.(8), γ controls the weight of mirror loss. In Table 4, we tried different values ofγs (from 0.0
to 3.0) in order to find the best choice suitable for different tasks. Note the γ = 0.0 means we do
not use mirror loss. We can see that different tasks have different optimal γs. In most of the tasks,
the optimal Y is in range 1.0 〜2.0.
5.4 Visual Cases Analysis
Figure 4: Visualization of mirror sets in Office-Home. The source domain is Product(Pr) and target
domain is Art(Ar). In each class, the first row is the “top-3” mirror set using embeddings trained
without mirror loss and the second row is obtained by the proposed method.
To further illustrate what the virtual mirror our method can find, we visualize the mirror set defined
in Eq.(5) in Fig.4. We can see that the top-3 similar samples with mirror loss are more similar
compared with the results without using mirror loss. The results without mirror loss might even
consist of quite dissimilar samples although they belong to the same class label (see the “clock”
class in the upper-right class of Fig.4). This means our proposed method can align the distribution
in more fine-grained way than the methods without using mirror samples. We further visualize the
feature distributions by t-SNE (Hinton & Roweis (2003)) in Appendix D.2 to show our methods
have better alignment for intra-cluster distributions.
6 Conclusion and Future Work
In this paper, we introduce a new concept called (virtual) mirror in unsupervised domain adaption.
Utilizing the asymptotic properties of the mirror samples, we could setup a connection between the
source and target domains by mirror loss. Under the unbiased sampling assumption, the proposed
method is expected to have lower asymptotic empirical risk. We performed extensive experiments
on benchmarks and achieved competitive results. However, the performance of the proposed method
relies on the sampling process of the training data from the unknown underline distribution(although
we generally assumes the sampling is unbiased). Another limitation might be the case when the
dataset is large, the mirror selector might be time-consuming. The optimal approximated scheme
for mirror selection as well as its impacts on the bound of empirical risk can be further investigated
using theory like Ordinal Optimization(Ho et al. (1992)).
8
Under review as a conference paper at ICLR 2021
References
Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355-607,
2019.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Scholkopf,
and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy.
Bioinformatics, 22(14):e49-e57, 2006.
Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao. Learning disentangled
semantic representation for domain adaptation. In IJCAI: proceedings of the conference, volume
2019, pp. 2060. NIH Public Access, 2019.
Chao Chen, Zhihong Chen, Boyuan Jiang, and Xinyu Jin. Joint domain alignment and discriminative
feature learning for unsupervised deep domain adaptation. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 33, pp. 3296-3303, 2019a.
Chao Chen, Zhihang Fu, Zhihong Chen, Sheng Jin, Zhaowei Cheng, Xinyu Jin, and Xian-Sheng
Hua. Homm: Higher-order moment matching for unsupervised domain adaptation. order, 1(10):
20, 2020.
Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu,
and Junzhou Huang. Progressive feature alignment for unsupervised domain adaptation. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 627-636,
2019b.
Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In Advances in Neural Information Processing
Systems, pp. 3730-3739, 2017.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Remi Flamary, Devis Tuia, and Nicolas
Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation.
In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.
Zhijie Deng, Yucen Luo, and Jun Zhu. Cluster alignment with a teacher for unsupervised domain
adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9944-
9953, 2019.
Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral clustering and nor-
malized cuts. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 551-556, 2004.
R Flamary et al. Optimal transport for domain adaptation, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised
domain adaptation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp.
2066-2073. IEEE, 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
9
Under review as a conference paper at ICLR 2021
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard
Scholkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5,
2009.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(25):723-773, 2012. URL
http://jmlr.org/papers/v13/gretton12a.html.
Xiang Gu, Jian Sun, and Zongben Xu. Spherical space domain adaptation with robust pseudo-label
loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 9101-9110, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Geoffrey E Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in neural
information processing systems, pp. 857-864, 2003.
Yu-Chi Ho, R_S Sreenivas, and P Vakili. Ordinal optimization of deds. Discrete ^vent dynamic
systems, 2(1):61-88, 1992.
Mohammed Jabi, Marco Pedersoli, Amar Mitiche, and Ismail Ben Ayed. Deep clustering: On the
link between discriminative models and k-means. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2019.
Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network
for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4893-4902, 2019.
Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein
discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 10285-10295, 2019.
Jingjing Li, Erpeng Chen, Zhengming Ding, Lei Zhu, Ke Lu, and Zi Huang. Cycle-consistent con-
ditional adversarial transfer networks. In Proceedings of the 27th ACM International Conference
on Multimedia, pp. 747-755, 2019.
Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source
hypothesis transfer for unsupervised domain adaptation. In Proceedings of the 37th International
Conference on Machine Learning, 2020.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97-105, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. In Advances in neural information processing systems, pp. 136-
144, 2016.
Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers
for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9111-9120, 2020.
Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, and Tao Mei. Transferrable proto-
typical networks for unsupervised domain adaptation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2239-2247, 2019.
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko.
Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.
10
Under review as a conference paper at ICLR 2021
Svetlozar T. Rachev, Lev B. Klebanov, Stoyan V. Stoyanov, and Frank Fabozzi. Glivenko-Cantelli
theorem and bernstein-kantoroVich invariance principle. 10.1007/978-1-4614-4869-3(ChaPter
12):283-296, 2013.
Alan Ramponi and Barbara Plank. Neural unsupervised domain adaptation in nlp—a survey. arXiv
preprint arXiv:2006.00632, 2020.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. permission. transferring visual category
models to new domains. 2010.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 3723-3732, 2018.
Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised
domain adaptation. arXiv preprint arXiv:1206.6438, 2012.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V Buenau, and Motoaki Kawanabe.
Direct importance estimation with model selection and its application to covariate shift adaptation.
In Advances in neural information processing systems, pp. 1433-1440, 2008.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision, pp. 443-450. Springer, 2016.
Hui Tang, Ke Chen, and Kui Jia. Unsupervised domain adaptation via structurally regularized
deep clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8725-8735, 2020.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 7167-7176, 2017.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. 2017.
Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual domain
adaptation with manifold embedded distribution alignment. In Proceedings of the 26th ACM
international conference on Multimedia, pp. 402-410, 2018.
Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang.
Adversarial domain adaptation with domain mixup. arXiv preprint arXiv:1912.01805, 2019a.
Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive
feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 1426-1435, 2019b.
Xiang Xu, Xiong Zhou, Ragav Venkatesan, Gurumurthy Swaminathan, and Orchid Majumder. d-
sne: Domain adaptation using stochastic neighborhood embedding. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 2497-2506, 2019c.
Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschlager, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.
Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5031-5040, 2019a.
Yabin Zhang, Bin Deng, Hui Tang, Lei Zhang, and Kui Jia. Unsupervised multi-class domain
adaptation: Theory, algorithms, and practice. arXiv preprint arXiv:2002.08681, 2020.
11
Under review as a conference paper at ICLR 2021
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I Jordan. Bridging theory and algorithm
for domain adaptation. arXiv preprint arXiv:1904.05801, 2019b.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2223-2232, 2017.
12
Under review as a conference paper at ICLR 2021
A Algorithm and Implementation Details
A. 1 Datasets
We use Office-31 (Saenko et al. (2010)), Office-Home(Venkateswara et al. (2017)), ImageCLEF
and VisDA2017(Peng et al. (2017)) to validate our proposed method. Office-31 has three domains:
Amazon(A), Webcam(W) and Dslr(D) with 4,110 images belonging to 31 classes. Office-Home is a
more challenging benchmark dataset for unsupervised domain adaption. It contains 15,500 images
of65 classes with four domains: Art(Ar), Clipart(Cl), Product(Pr) and RealWorld(Rw). ImageCLEF
contains 600 images of 12 classes, where the images are divided into three domains: Caltech-256(C),
ILSVRC 2012(I), Pascal VOC 2012(P). For the above three datasets, we use all the adaption tasks.
VisDA2017 is a large-scale dataset which contains ~280K images belonging to 12 classes. These
images are divided into three parts: train, validation and test. We use “train” as source domain
and “validation” as target domain. The source domain is composed of 152,397 images, which are
generated from synthetic renderings of 3D models. The target domain is composed of 55,388 images
cropped from Microsoft COCO dataset Lin et al. (2014).
A.2 Algorithm and Implementations
Following the standard protocol in UDA, we use all the labeled source domain samples and all
unlabeled target domain samples for training. We implement our model in PyTorch. For all tasks,
we use ResNet50 or ResNet101 pre-trained on the ImageNet as backbone. The number of units
in final fully connected layers are changed according to different tasks. All the input images are
resized to 224 and normalized by mean [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224,
0.225]. Data augmentations include “RandomCrop” and “RandomHorizontalFlip” for train images
and only “CenterCrop” for test images. We adopt SGD to optimize the total loss. The learning rate
is adjusted by ηp = η0(1 + αp)-β like Ganin et al. (2016), where p is the epoch which is normalized
in [0, 1], and η0 = 0.001, α = 10, β = 0.75. And the learning rate of fully connected layers is 10
times that of the backbone layers. The detailed algorithm is presented in Algorithm 1. One should
Algorithm 1 Mirror Alignment Algorithm for UDA
Require:
Source Data XS = {xis, yis}in=s 1
Target Data XT = {xit}in=t 1
Total epochs M, batch size B, Iterations for each epoch T, where T = bM/Bc.
The learned parameters in backbone θf, in newly-added FC layer θg and the classifier parame-
ters θc
1:	Create feature caches for source sample FS ∈ Rdf ×ns, GS ∈ Rdg×ns and target samples
FT ∈ Rdf ×nt and GT ∈ Rdg ×nt.
2:	for epoch = 1 to M do
3:	Clear the caches GS, FS, GT, GS.
4:	Calculate the source features fis and gis for each sample and the centers cfs,c, csg,c for each
class using current parameters by cf,c = * Pys=C fS and cg,c = n1s Pys=C g'i∙ Save the
source features to GS and FS accordingly.
5:	Calculate target features and class centers ctf and ctg by K-means(Dhillon et al. (2004)), save
the features to GT and Ft respectively.
6:	for t = 1 to T do
7:	Choose a batch data {xis}bB=1 and {xtj}bB=1 from XS, XT , with features {fis}bB=1, {fit}bB=1,
{gis}bB=1, {git}bB=1.
8:	Find the mirrors based on Eq.(5) and further calculate the loss by Eq.(8).
9:	Update parameters θg , θf and θC by SGD.
10:	end for
11:	end for
note that:
13
Under review as a conference paper at ICLR 2021
•	When calculating the centers ctf,c and ctg,c for the target domain, we use the centers of
source domain for each class cfs,c and cgs,c as the initial centers. The resulting clusters will
share the label of the initial centers.
•	We need to cache all the features for both domains, denoted by FS, GS and FT , GT respec-
tively in order to select mirrors. Those features and centers are updated per epoch rather
than per batch.
•	For large scale dataset such as VisDA2017, the Eq.(5) is memory and time consuming.
Merge sort with top-k minimum heap for each partition is used since we only care about
the top-k features rather than the orders of all samples.
14
Under review as a conference paper at ICLR 2021
B Proofs of Propositions
B.1 Proof of Proposition 1
Proof. Denote ΦS (x), ΦT (x) as the density function for domain S and T in the learned feature
space D, with supports as DS and DT respectively. H is the hypothesis class of functions mapping
from D to Y. Following the definition of dH∆H(S, T) in Lemma 1, we have
dH∆H (S, T) = sup |	ΦS(x)I(h(x) 6= h0(x))dx
h,h0∈H	XzDS
—	ΦT (x)I(h(x) 6= h0(x))dx|
XzDT
(14)
considering the fact that P r[x] = E[I(x)], where I is the indicator function. If S and T are aligned
in space D, then both the density functions ΦS, ΦT and their supports DS, DT are same. We have
|	ΦS (x)I(h(x) 6= h0(x))dx -	ΦT (x)I(h(x) 6= h0(x))dx|
JXzDS	J X 〜DT
≤ (	∣Φs(x) — Φt(x)∣∣I(h(x) = h0(x))|dx → 0
XzD
□
B.2 Proof of Proposition 2
Proof. Based on Ben-David et al. (2010), we have
λo = min{RS(h, hS) + RT (h, hT)}
h∈H
(15)
(16)
Based on the Proposition 1, dH∆H(S, T) will approach 0 empirically if Lmirror is minimized inde-
pent with H. Thus ∀h ∈ H, we have
λm + Wdmδh = λm = min{Rs(h, hs) + RT(h, hτ)}
2	h∈H
When the model is trained without Lmirror, we can define a set H0 ⊂ H that satisfies dH0∆H0
If H0 = 0, the Eq.(13) holds naturally.
If H0 = 0, then ∀h ∈ H, We have
λo + 2dH∆H = min{ mm{Rs(h, hs) + RT(h, hτ)},
,min ,{RS(h, hS) + RT(h, hτ)} + 1 dH∆H}
h∈H-H0	2
(17)
0.
(18)
≥ min{RS (h, hS) +RT(h,hT)}
h∈H
=λm + 2dm∆H
since for any subset H0 ⊆ H, minh∈H0 {h} ≥ minh∈H {h}. hS and hT are the labeling functions
for source and target domains Ben-David et al. (2010).
□
15
Under review as a conference paper at ICLR 2021
C Detailed Results
C.1 Detailed Comparisions with SOTA methods
In this section, we describe the task-level detailed results of our experiment comparing with SOTA
methods.
Table 5 shows the result of 6 tasks on Office-31 dataset. Compared with the SOTA result of
SRDC(Tang et al. (2020)), the average accuracy of our model increases by 0.3%. Specially, we
achieve a 1.5% improvement on task A to W.
Table 6 shows the result of 12 tasks on Office-Home dataset. The average accuracy of our method
is 72.6%, which gains a 1.3% improvement compared with SRDC (Tang et al. (2020)). For some
difficult tasks such as Cl to Ar, Pr to Ar, our method have achieved more than 1% improvement.
Table 7 shows the result of 6 tasks on ImageCLEF dataset. For all tasks, our method gain a new
SOTA result and the average accuracy is 91.6% which has a 0.7% improvement.
For large-scale dataset VisDA2017, we migrated the proposed mirror loss to the existing method
CAN Kang et al. (2019). We can observe from Table 8 that the average accuracy increases by 0.7%
when using mirror on CAN.
16
UnderreVieW as a ConferenCe PaPersICLR 2021
Table 5: Test accuracy(%) on Office-31 dataset for unsupervised domain adaptation based on ResNet50.
Method	A-W	D-W	W-D	A-D	D-A	W-A	Avg
Source Model He et al. (2016)	60	90	993	60	623	607	76.1
DAN Long et al. (2015)	81.3±0.3	97.2±0.0	99.8±0.0	83.1±0.2	66.3±0.0	66.3±0.1	82.3
DANN Ganin et al. (2016)	81.7±0.2	98.0±0.2	99.8±0.0	83.9±0.7	66.4±0.2	66.0±0.3	82.6
ADDA Tzeng et al. (2017)	86.2±0.3	78.8±0.4	96.8±0.2	99.1±0.2	69.5±0.1	68.5±0.1	83.2
JDDA Chen et al. (2019a)	82.6±0.4	95.2±0.2	99.7±0.0	79.8±0.1	57.4±0.0	66.7±0.2	80.2
DSRCai et al. (2019)	93.1	98.7	99.8	92.4	73.5	73.9	88.6
DM-ADA Xu et al. (2019a)	83.9±0.4	99.8±0.1	99.9±0.1	77.5±0.2	64.6±0.4	64.0±0.5	81.6
rRevGrad+CAT Deng et al. (2019)	94.4±0.1	98.0±0.2	100.0±0.0	90.8±1.8	72.2±0.6	70.2±0.1	87.6
SAFN Xu et al. (2019b)	90.1±0.8	98.6±0.2	99.8±0.0	90.7±0.5	73.0±0.2	70.2±0.3	87.1
MDD Xu et al. (2019b)	94.5±0.3	98.4±0.1	100.0±0.0	93.5±0.2	74.6±0.3	72.2±0.1	88.9
CAN Kang etal. (2019)	94.5±0.3	99.1±0.2	99.8±0.2	95.0±0.3	78.0±0.3	77.0±0.3	90.6
SHOT Liang et al. (2020)	90.9	98.8	99.9	93.1	74.5	74.8	88.7
SRDC Tang et al. (2020)	95.7±0.2	99.2±0.1	100.0±0.0	95.8±0.2	76.7±0.3	77.1±0.1	90.8
C)UrS	972±0∙3	99∙2±0∙l	100∙0±0∙0	96∙2±0∙l	75.3±0.1	78∙2±0∙l	91」
1
Table 6: Test accuracy(%) on Office-Home dataset for unsupervised domain adaptation based on ResNet50.
Method	Ar-Cl Ar-Pr Ar-Rw Cl-Ar Cl-Pr Cl-Rw Pr-Ar Pr-Cl Pr-Rw Rw-Ar Rw-Cl Rw-Pr Avg
13.6.9313.6
6.G7.4Z&L2.
45566677
93 用.5.53Q3
9.4.g9.ln5.5.
57778888
2.5用。1214
lll4.7.0.z9.
45555655
91209538
5363636870727677
4.7.5.711.74
CS7.&5.7.&L2.
66677788
2.6.724.6.8.7
l3.3.9.l3.3.5.
34445555
.59/.7.72.7J
8.46G3.L&2.
3Φ456667
24.9.3.9.89.6
6OO9.LL&8.
46667777
.9.5.5.8.9422
LG8.g9.lgz
45566777
4801225
37.45,47,57,64,60,69.72
09143804
8.7.Q7.G7.LL
56777788
00367735
Q79.LL3.G6
55577777
Source Model He et al. (2016)	34.9
DAN Long et al. (2015)	43.6
DANN Ganin et al. (2016)	45.6
DSRCai etal. (2019)	53.4
SAFN Xu et al. (2019b)	52.0
MDD Zhang et al. (2019b)	54.9
SRDC Tang et al. (2020)	52.3
Ours	52.4
UnderreVieW as a ConferenCe PaPersICLR 2021
Table 7: TeSt accuracy (%) On ImageCLEF dataset for unsupervised domain adaptation based On ReSNet50.
Method	I-P	P-I	I-C	C-I	C-P	P-C	Avg
Source Model He et al. (2016)	74.8±0.3	83.9±0.1	91.5±0.3	78.0±0.2	65.5±0.3	91.2±0.3	807
DAN Long et al. (2015)	74.5±0.3	82.2±0.2	92.8±0.2	86.3±0.4	69.2±0.4	89.8±0.4	82.5
DANN Ganin et al. (2016)	75.0±0.6	86.0±0.3	96.2±0.4	87.0±0.5	74.3±0.5	91.5±0.6	85.0
rRevGrad+CAT Deng et al. (2019)	77.2±0.2	91.0±0.3	95.5±0.3	91.3±0.3	75.3±0.6	93.6±0.5	87.3
SAFN Xu et al. (2019b)	79.3±0.1	93.3±0.4	96.3±0.4	91.7±0.0	77.6±0.1	95.3±0.1	88.9
SymNets Zhang et al. (2019a)	80.2±0.3	93.6±0.2	97.0±0.3	93.4±0.3	78.7±0.3	96.4±0.1	89.9
MCSD Zhang et al. (2020)	79.2±0.2	96.2±0.3	96.8±0.1	93.8±0.2	77.8±0.4	96.2±0.0	90.0
SRDC Tang et al. (2020)	80.8±0.3	94.7±0.2	97.8±0.2	94.1±0.2	80.0±0.3	97.7±0.1	90.9
C)WrS	82∙4±0∙l	953±0∙l	97∙9±02	952±0∙2	81∙0±0∙l	98∙0±0∙l	91∙6
∞
Table 8: TeSt accuracy (%) On ViSDA dataset for UnSUPerViSed domain adaptation based On ReSNetlOL
airplane bicycle bus car horse knife motorcycle persion plant skateboard train truck Avg
MethOd
11 11 11 Xɔ Λy
6.L g9.7.z
4 6 7 7 8 8
.9.74.3.92
9 0 4 9 9 1
5 2 2 4 5 6
2.8。3.5,9
L5.9.4.7.5.
4 8 8 8 8 8
9 3 6.93.6
3.G5.4.G 6
5 3 5 8 9 9
4.7.9.5.6J
S9.9.9.6 6
6 4 8 8 9 9
2 16 8 7 4
L3.9.7.0.6
3 5 7 7 8 8
.5.9.8.5.82
&5.L L o9.
3 8 9 8 9 8
2.9。1 2 S
GN9.6 65.
4 4 7 8 9 9
9 3 17 8 2
41,90,94,89,97.97.
4∙0∙6
7 ZS
3 4 7
.53 Q
8.4.6
5 7 7
tɔ 11 11
& G 4。24.
5 7 8 8 8 8
0 0 3 12 2
。3.L L 78.
5 6 6 8 8 8
Source Model He et al. (2016)	34.9
DAN Long et al. (2015)	87.1
SAFN Xu et al. (2019b)	93.6
SHOT Liang et al. (2020)	92.6
CAN Kang et al. (2019)	97.0
CAN+Mirror(Ours)	97.2
Under review as a conference paper at ICLR 2021
C.2 Detailed Results of Ablation Study
The detailed result of ablation study on Office-Home is shown in this section. In Table 9, “Baseline”
only uses backbone and the labeled source data to train the model and test on target domain; “DC”
means Discriminative Clustering which is described in Section 3.3 and Eq.(8); “Backbone Mirror”
means applying the mirror loss to the last pooling layer of the backbone; “FC Mirror” is the results
applying the mirror loss to the output of first fully-connected layer after the last pooling layer.
Besides those above, we also investigate two methods synthesizing the virtual mirror in Eq.(6).
One is ω(x, Xj) = e-d(x,xj)/ Pχ∈χS(Xt) e-d(x,xj), which is denoted as “weighted mirror sample”
the other is ω(x, xit) = 1/k. From Table 9, we can observe that both “DC” and mirror alignment can
improve the accuracy on target domain. Without discriminative clustering, the accuracy is reduced
by 6.5%. Without mirror alignment, the accuracy is reduced by 0.4%. Applying the mirror loss on
both the backbone and FC layer is helpful for the final performance. We could also find that our
method is less sensitivity to way we construct the virtual samples, i.e. either the average sample or
“Weighted Mirror Sample”.
Table 9: Ablation Studies using Office-Home dataset based on ReSNet50.(K = 3)
Baseline	DC	FC Mirror	Backbone Mirror	Weighted Mirror Sample	Ar-Cl	Ar-Pr	Ar-Rw	Cl-Ar	Cl-Pr	Cl-Rw	Pr-Ar
X					^^349-	50.0	58.0	37.4	41.9	46.2	38.5
X	X				51.1	76.1	81.9	70.7	76.18	77.5	71.0
X	X	X			52.0	74.9	81.6	70.2	76.5	77.9	70.5
X		X	X		47.2	69.1	76.1	60.9	66.5	69.7	62.8
X	X	X	X		52.0	75.6	81.0	71.9	77.0	78.6	69.6
X	X	X	X	X	51.7	75.5	81.3	71.1	76.4	77.8	70.2
Baseline	DC	FC Mirror	Backbone Mirror	Weighted Mirror Sample	Pr-Cl	Pr-Rw	Rw-Ar	Rw-Cl	Rw-Pr	Avg	
X					^^31.2-	60.4	53.9	41.2	59.9	46.1	
X	X				54.5	82.2	76.2	56.8	85.1	71.6	
X	X	X			55.3	82.1	76.4	58.0	84.8	71.7	
X		X	X		48.3	77.2	73.1	53.7	81.3	65.5	
X	X	X	X		54.8	82.1	77.3	58.5	85.1	72.0	
X	X	X	X	X	54.8	82.3	76.6	57.5	85.3	71.7	
19
Under review as a conference paper at ICLR 2021
D Visualizations
D. 1 Visual Cases Analysis
Target
With
Top-1	Top-2	Top-3
W/O
MilTor
Mirror
Figure 5: Visualization of mirror sets in Office-31. The source domain is “webcame” and target
domain is “amazon”. In each class, the first row is the top-3 mirror set using embeddings trained
without mirror loss and the second row is obtained by the proposed method.
Besides the case visualization for Office-Home in Section 5.4, we also present the results for Office-
31 in Fig.5. We could have similar observations as Fig.4. Specially, although the “Bottle” in the
upper-left class of the Fig.5 has mirror sets belonging to the same class, but our proposed method
gives results much more similar. For the “Helmet” class in the bottom-right class, the results without
mirror loss consist of even different class samples, such as “mouse”. Those means our proposed
method can align the distribution in more fine-grained way than the methods without using mirror
samples.
D.2 Visualization of Aligned Embeddings
In order to show the effectiveness of our method in distribution alignment more vividly, we visualize
the embedded features of samples using t-SNE(Hinton & Roweis (2003)) for tasks Cl-Rw, Rw-Cl
in Office-Home and tasks A-W and W-A in Office-31 in Fig.6 and Fig.7. Besides the clusters are
more tight for results with mirror loss, the “shape” of each class sample distribution cross domains
are more alike for the results with mirror loss. In some cases, even the source and target samples
are clustered together, the results without mirror loss show more non-overlap samples, meaning the
distributions are not aligned well enough.
20
Under review as a conference paper at ICLR 2021
(a) w/o Mirror, Cl-Rw	(b) Mirror Alignment, Cl-Rw
(c) w/o Mirror, Rw-Cl	(d) Mirror Alignment, Rw-Cl
Figure 6: The t-SNE visualization of feature embeddings for 2 tasks of Office-Home. The solid
points denote source data and circles denote target data. Different classes are distinguished by color.
21
Under review as a conference paper at ICLR 2021
(a) w/o Mirror, A-W	(b) Mirror Alignment, A-W
(c) w/o Mirror, W-A	(d) Mirror Alignment, W-A
Figure 7: The t-SNE visualization of feature embeddings for 2 tasks of Office-31. The solid points
denote source data and circles denote target data. Different classes are distinguished by color.
22