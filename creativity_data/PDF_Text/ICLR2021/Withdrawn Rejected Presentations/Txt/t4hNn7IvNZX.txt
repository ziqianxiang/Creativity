A Distributional Robustness Certificate by
Randomized Smoothing
Anonymous authors
Paper under double-blind review
Ab stract
The robustness of deep neural networks against adversarial example attacks has
received much attention recently. We focus on certified robustness of smoothed
classifiers in this work, and propose to use the worst-case population loss over
noisy inputs as a robustness metric. Under this metric, we provide a tractable upper
bound serving as a robustness certificate by exploiting the duality. To improve the
robustness, we further propose a noisy adversarial learning procedure to minimize
the upper bound following the robust optimization framework. The smoothness
of the loss function ensures the problem easy to optimize even for non-smooth
neural networks. We show how our robustness certificate compares with others
and the improvement over previous works. Experiments on a variety of datasets
and models verify that in terms of empirical accuracies, our approach exceeds the
state-of-the-art certified/heuristic methods in defending adversarial examples.
1	Introduction
Deep neural networks (DNNs) have been known to be vulnerable to adversarial example attacks: by
feeding the DNN with slightly perturbed inputs, the attack alters the prediction output. The attack can
be fatal in performance-critical systems such as autonomous vehicles or automated tumor diagnosis.
A DNN is robust when it can resist such an attack that, as long as the range of the perturbation is not
too large (usually invisible by human), the model produces an expected output despite of the specific
perturbation. Various approaches have been proposed for improving the robustness of DNNs, with
or without a performance guarantee.
Although a number of approaches have been proposed for certified robustness, it is vague how ro-
bustness should be defined. For example, works including Cohen et al. (2019); Pinot et al. (2019); Li
et al. (2019); Lecuyer et al. (2019) propose smoothed classifiers to ensure the inputs with adversarial
perturbation to be classified into the same class as the inputs without. However, since both inputs
are inserted randomized noise, it cannot be guaranteed that the inputs are classified into the correct
class. It is possible that the adversarially perturbed input has the same label as the original one which
is wrongly classified by the DNN. In this case, the robustness guarantee does not make sense any
more. Further, the robustness guarantee is provided at the instance level, i.e., within a certain pertur-
bation range, the modification of an input instance cannot affect the prediction output. But a DNN
is a statistical model to be evaluated on the input distribution, rather than a single instance. Instead
of counting the number of input instances meeting the robustness definition, it is desired to evaluate
the robustness of a DNN over the input distribution.
We introduce the distributional risk as a DNN robustness metric, and propose a noisy adversarial
learning (NAL) procedure based on distributional robust optimization, which provides a provable
guarantee. Assume a base classifier f trying to map instance x0 to corresponding label y. It is found
that when fed with the perturbed instance x (within a l2 ball centered at x0), a smoothed classifier
g(x) = EZ[f (X + z)] with Z 〜Z = N(0, σ2I) can Provably return the same label as g(xo) does.
However, we think such a robustness guarantee cannot ensure g(x0) to be correctly classified as y,
resulting in unsatisfying Performance in Practice. Instead, we evaluate robustness as the worst-case
loss over the distribution of noisy inPuts. For simPlicity, we jointly exPress the inPut instance and
the label as xo 〜Po where Po is the distribution of the original input. By using '(∙) as the loss
function, we evaluate DNNs by the worst-case distributional risk: supS ES [`(θ; s)]. The classifier is
parameterized by θ ∈ Θ, and S = X + Z 〜S where S is a distribution within a certain distance from
P0 . We prove such a loss is upper bounded by a data-dependent certificate, which can be optimized
by the noisy adversarial training procedure:
minimize sup ES [`(θ; s)].	(1)
θ∈Θ	S
Compared to previous robustness certificates via smoothed classifiers, our method provides a prov-
able guarantee w.r.t. the ground truth input distribution. Letting the optimized θ be the parameter of
g(∙) and f (∙) respectively, we further show that the smoothed classifier g(∙) provides an improved
robustness certificate than that of f (∙), due to a tighter bound on the worst-case loss.
The key is that, for mild perturbations, we adopt a Lagrangian relaxation for the usual loss `(θ; x+z)
as the robust surrogate, and the surrogate is strongly concave in x and hence easy to optimize. Our
approach enjoys convergence guarantee similar to the method in Sinha et al. (2018), but different
from Sinha et al. (2018), our approach does not require ` to be smooth, and thus can be applied to
arbitrary neural networks. The advantage of the smoothed classifier also lies in a tighter robustness
certificate than the base classifier. The intuition is that, in the inner maximization step, instead of
seeking one direction which maximizes the loss, our approach performs gradient ascent along the
direction which maximizes the total loss of examples sampled from the neighborhood of the original
input. The noisy adversarial training procedure produces smoothed classifiers robust against the
neighborhood of the worst-case adversarial examples with a certified bound.
Highlights of our contribution are as follows. First, we review the drawbacks in the previous defi-
nition of robustness, and propose to evaluate robustness by the worst-case loss over the input dis-
tribution. Second, we derive a data-dependent upper bound for the worst-case loss, constituting a
robustness certificate. Third, by minimizing the robustness certificate in the training loop, we pro-
pose noisy adversarial learning for enhancing model robustness, in which the smoothness property
entails the computational tractability of the certificate. Through both theoretical analysis and ex-
perimental results, we verify that our certified DNNs enjoy better accuracies compared with the
state-of-the-art defending adversarial example attacks.
2	Related Work
Works proposed to defend against adversarial example attacks can be categorized into the following
categories.
In empirical defences, there is no guarantee how the DNN model would perform against the adver-
sarial examples. Stability training (Zheng et al. (2016); Zantedeschi et al. (2017); Liu et al. (2018))
improves model robustness by adding randomized noise to the input during training but shows lim-
ited performance enhancement. Adversarial training (Goodfellow et al. (2015); Kurakin et al. (2018);
Madry et al. (2017); Kannan et al. (2018); Zhang et al. (2019); He et al. (2019); Wang et al. (2019))
trains over adversarial examples found at each training step but unfortunately does not guarantee the
performance over unseen adversarial inputs. Although without a guarantee, adversarial training has
excellent performance in empirical defences against adversarial attacks.
Certified defences are certifiably robust against any adversarial input within an `p-norm perturba-
tion range from the original input. A line of works construct a computationally tractable relaxation
for computing an upper bound on the worst-case loss over all valid attacks. The relaxations include
linear programming (Wong & Kolter (2018)), mixed integer programming (Tjeng et al. (2018)),
semidefinite programming (Raghunathan et al. (2018)), and convex relaxation (Namkoong & Duchi
(2017); Salman et al. (2019b)). But those deterministic methods are not scalable. Some works such
as Dvijotham et al. (2018) formulate the search for the largest perturbation range as an optimization
problem and solve its dual problem. Sinha et al. (2018) also propose a robustness certificate based
on a Lagrangian relaxation of the loss function, and it is provably robust against adversarial input
distributions within a Wasserstein ball centered around the original input distribution. The certificate
of our work is constructed on a Lagrangian relaxation form of the worst-case loss, but has a broader
applicability than Sinha et al. (2018) with a tighter loss bound due to the smoothness property.
An alternative line of works propose to select appropriate surrogates for each neuron activation
layer by layer (Weng et al. (2018); Zhang et al. (2018)) to facilitate the search for a certified lower
bound. By integrating with interval bound propagation (Gowal et al. (2018)), Zhang et al. (2020)
make the search computationally efficient and scalable. Other works (Mirman et al. (2018); Singh
et al. (2018)) apply the abstract interpretation to train provably robust neural networks. Our work is
orthogonal to these works.
Randomized smoothing introduces randomized noise to the neural network, and tries to provide
a statistically certified robustness guarantee. Pinot et al. (2020) have demonstrated by game theory
that no deterministic classifier can claim to be more robust than all others against any possible ad-
versarial attack. But such a question remains open in the randomized regime, where randomized
smoothing can be considered as a contributing effort. The smoothing method does not depend on
a specific neural network, or a type of relaxation, but can be generally applied to arbitrary neural
networks. The idea of adding randomized noise was first proposed by Lecuyer et al. (2019), given
the inspiration of the differential privacy property, and then Li et al. (2019) improve the certificate
with Renyi divergence. Cohen et al. (2019) obtain a larger certified robustness bound through the
smoothed classifier based on Neyman-Pearson theorem. Phan et al. (2020) extend the noise addi-
tion mechanism to large-scale parallel algorithms. By extending the randomized noise to the general
family of exponential distributions, Pinot et al. (2019) unify previous approaches to preserve ro-
bustness to adversarial attacks. Lee et al. (2019) offer adversarial robustness guarantees for '0-norm
attacks. Both Salman et al. (2019a); Jia et al. (2019) employ adversarial training to improve the per-
formance of randomized smoothing. Following a similar principle, our work trains over adversarial
data with randomized noise. But we provide a more practical robustness certificate and a training
method achieving higher empirical accuracy than theirs.
3	Proposed Approach
We first define the closeness between distributions, based on which we constrain how far the input
distribution is perturbed. Then we introduce our definition of robustness on smoothed classifiers.
Our main theorem gives a tractable robustness certificate which is easy to optimize. Our algorithm
for improving the robustness of the smoothed classifiers is provided. All proofs are collected in the
appendices for conciseness.
3.1	A Distributional Robustness Certificate
Definition 1 (Wasserstein distance). Wasserstein distances define a notion of closeness between
distributions. Let X ⊂ Rd , A, P be a probability space and the transportation cost c : X ×
X → [0, ∞) be nonnegative, lower semi-continuous, and c(x, x) = 0. P and Q are two probability
measures supported on X . Let Π(P, Q) denotes the collection of all measures on X × X with
marginals P and Q on the first and second factors respectively, i.e., it holds that π(A, X) = P (A)
and π(X, A) = Q(A), ∀A ∈ A and π ∈ Π(P, Q). The Wasserstein distance between P and Q is
Wc(P, Q) := inf	Eπ [c (x, y)] .	(2)
π∈Π(P,Q)
For example, the '2-norm c(χ, xo) = ||x - χ0k2 satisfies the aforementioned conditions.
Distributional robustness. Assume the original input x0 is drawn from the distribution P0, and the
perturbed input x is drawn from the distribution P . Each input is added randomized Gaussian noise
Z 〜Z = N(0, σ2I) before being fed to the classifier. Instead of regarding the noise as a part of
the smoothed classifier, we treat s = x + z as a noisy input coming from the distribution S in the
analysis. Since z ∈ Rd, we need to set X = Rd to admit s ∈ X as Lecuyer et al. (2019); Cohen et al.
(2019); Salman et al. (2019a) do. Since the perturbed input should be visually indistinguishable from
the original one, we define the robustness region as P = {P : Wc (P, P0) ≤ ρ, P ∈ P(X)} , where
ρ > 0. Within such a region, we evaluate the robustness as a worst-case population loss over noisy
inputs: supS∈P ES['(θ; s)]. Essentially, we evaluate the robustness ofa smoothed classifer based on
its performance on the worst-case adversarial example distribution. A smaller loss indicates a higher
level of robustness. We will compare the definition against others in the next section. However, such
a robustness metric is impossible to measure in practice as we have no idea about P . Even if P
can be acquired, it can be a non-convex region which renders the constrained optimization objective
intractable. Hence we resort to the Lagrangian relaxation of the problem by assuming a dual variable
γ.
As the main theorem of this work, we provide an upper bound for the worst-case population loss for
any level of robustness ρ. We further show that for small enough ρ, the upper bound is tractable and
easy to optimize.
Theorem 1. Let ` : Θ × X → R and transportation cost function c : X × X → R+ be
continuous. Let x0 be an input drawn from the input distribution P0, x be the adversarial ex-
ample which follows the distribution P and Z 〜Z = N(0,σ2I) be the additive noise of the
same shape as X. The sum of X and Z is denoted as S = X + Z 〜 S and we let φγ (θ; xo) 二
suPx∈x EZ {'(θ; x + Z) — Yc (x + z, xo)} be the robust surrogate. For any γ,ρ > 0 and σ, we
have
sup	ES ['(θ; s)] ≤ YP + Epo [φγ (θ; xo)].	(3)
S=Wc(S,P°)≤ρ
The proof is given in Appendix A.1. It is notable that the right-hand side take the expectation over
Po and Z respectively, and given a particular input xo and a noise sample Z, we seek an adversarial
example which maximizes the surrogate loss. Typically, Po is impossible to obtain and thus we use
an empirical distribution, such as the training data distribution, to approximate Po in practice.
Since Thm. 1 provides an upper bound for the worst-case population loss, it offers a principled
adversarial training approach which minimizes the upper bound instead of the actual loss, i.e.,
minimize EP0 [φγ (θ; xo)].
(4)
In the following we show the above loss function has a form which is tractable for arbitrary neural
networks, due to a smoothed loss function. Hence Thm. 1 provides a tractable robustness certificate
depending on the data.
Properties of the smoothed classifier. We show the optimization objective of Eq. 4 has a form
which is tractable for any neural network, particular for the non-smooth ones with ReLU activation
layers. More importantly, the smoothness of the classifier enables the adversarial training procedure
to converge as we want by using the common optimization techniques such as stochastic gradient
descent. The smoothness of the loss function comes from the smoothed classifier with randomized
noise. Specifically,
Theorem 2. Assume ` : Θ × X → [0, M] is a bounded loss function. The loss function on the
smoothed classifier can be expressed as '(θ; x) := EZ['(θ; X + z)], Z 〜Z = N(0, σ2I). Then we
2M
have ' is -M -smooth w.r.t. '2 -norm, i.e., ' satisfies
▽x%(。；X)- ηVχ%(θ; x0) Il2 ≤ σ-r kx - x0k2.
(5)
The proof is in Appendix A.2. It mainly takes advantage of the randomized noise which has a
smoothing effect on the loss function. For DNNs with non-smooth layers, the smoothed classifier
makes it up and turns the loss function to a smoothed one, which contributes as an important property
to the strong concavity of EZ [`(θ; x + Z) - Yc (x + Z, xo)] and therefore ensures the tractability of
the robustness certificate.
Corollary 1. For any c : X × X → R+ ∪ {∞} 1-strongly convex in its first argument, and ` :
x → EZ ['(θ; x + z)] being 菖-smooth, the function EZ {'(θ; X + z) 一 Yc (x + z, x0)} is strongly
concave in X for any Y ≥ 2M.
The proof is in Appendix A.3. Note that here we specify the requirement on the transportation cost
c to be 1-strongly convex in its first argument. The '- -norm cost satisfies the condition. Before
showing how the strong concavity plays a part in the convergence, we illustrate our algorithm first.
3.2	Noisy Adversarial Learning Alogrithm
Problem 4 provides an explicit way to improve the robustness ofa smoothed classifier parameterized
by θ. We correspondingly design a noisy adversarial learning algorithm to obtain the classifier of
which its robustness can be guaranteed. In the algorithm, we use the empirical distribution to replace
the ideal input distribution Po, and sample z a number of times to substitute the expectation with
the sample average. Assuming we have a total of n training instances xi0 , ∀i ∈ [n], and sample
Zij 〜N(0, σ2I) for the i-th instance for r times, the objective is:
1nr
minimize ——∖ ∖ sup ∣^'(θ; X + Zij) — γc (x + zj,x0)].	(6)
θ∈Θ	nr i=1 j=1 x∈X	0
The detail of the algorithm is illustrated in Alg. 1. In the inner maximization step (line 3-6), we adopt
the projected gradient descent (PGD Madry et al. (2017); Kurakin et al. (2018)) to approximate the
maximizer according to the convention. The hyperparameters include the number of iterations K and
the learning rate η1 . Within each iteration, we sample the Gaussian noise r times, given which we
compute an average perturbation direction for each update. The more noise samples, the closer the
averaging result is to the expectation value, which is definitely at the sacrifice of higher computation
expense. Similarly, a larger number of K indicates stronger adversarial attacks and higher model
robustness, but also incurs higher computation complexity. Hence choosing appropriate values of r
and K is important in practice.
Algorithm 1 Training Phase of NAL
Input: batch size n, number of noise samples r, noise STD σ, learning rate η1, η2, number of
iterations K, penalty parameter γ, training iterations T
Output: the classifier parameter θ
1:
2:
3:
4:
5:
6:
7:
8:
9:
for t ∈ {1, . . . , T} do
for i ∈ {1, . . . , n} do
for k ∈ {0, . . . , K — 1} do
δXk = 1 Pr=I 5xk'(θ; Xik + Zij) - γ Nxxk C(Xk + zij, XO), Where Zij 〜N(0, σ2I)
xik+1 = Xik + η1 ∆Xikk	k
end for
end for
θt+1 = θt — η {n⅛ Pi=ι [5θ Pj=I '(θt; XK + Zij)]}
end for
After training is done, We obtain the classifier parameter θ. In the inference phase, We sample a
number of Z 〜N(0, σ2I) to add to the testing instance. The noisy testing examples are fed to the
classifier to get the prediction outputs.
Convergence. An important property associated With the smoothed classifier is the strong concavity
of the robust surrogate loss, Which is the key to the convergence proof. The detail of the proof can
be found in Appendix A.4. As long as the loss ' is smooth on the parameter space Θ, NAL has a
convergence rate O(1∕√T), similar to Sinha et al. (2018), but NAL does not need to replace the
non-smooth layer ReLU With Sigmoid or ELU to guarantee robustness.
4	A Tighter B ound
We compare our Work With the state-of-the-art robustness definitions and certificates in this section.
4.1	Adversarial Training
Our approach improves the distributional robustness certificate proposed by Sinha et al. (2018). In
Sinha et al. (2018), a classifier f maps input instance xo 〜 Po to corresponding label y. They
perturb X0 to X0 in the same robustness region as ours: P = {P : Wc (P, P0) ≤ ρ, P ∈ P(X )} ,
Where ρ > 0. But their Worst-case population loss is defined on the base classifier Without noise:
suppo：wc(pop )≤ρ Epo ['(θ; X0)]. We show that, given the same classifier parameter θ, our worst-case
loss is smaller than Sinha et al. (2018), suggesting a better robustness certificate.
Theorem 3. Under the same denotations and conditions as Thm. 1, we have
sup ES [`(θ; s)] ≤ inf {γρ + EP0 [φγ (θ; X0)]}
S∈P	γ≥0
(7)
≤ inf γρ + EP0 sup [`(θ; X0) — γc (X0, X0)] = sup	EP0 [`(θ; X0)].
γ≥0 I	x0∈X	J	P0：Wc(P0,Po)≤ρ
The proof is given in Appendix A.5. We demonstrate that not only the worst-case loss is smaller, but
the tractable upper bound is smaller than the certificate of Sinha et al. (2018). If the outer minimiza-
tion problem applies to both sides of the inequality, our approach would obtain a smaller loss when
both classifiers share the same neural architecture.
4.2 Smoothed Classifiers
Works including Lecuyer et al. (2019); Cohen et al. (2019); Pinot et al. (2019); Li et al. (2019)
and others guarantee the robustness of a DNN classifier by inserting randomized noise to the input
at the inference phase. Most of them do not concern about the training phase, but merely provide
a deterministic relationship between the robustness certificate and the additive noise. Specifically,
we have the original input x0 ∈ X and its perturbation x within a given range kx - x0 k2 ≤ ε. The
smoothed classifier g (x) returns class ci with probability pi. For instance x0, robustness is defined by
the largest perturbation radius R which does not alter the instance’s prediction, i.e., g(x) is classified
into the same category as g(x0). Such perturbation radius depends on the largest and second largest
probabilities of pi, denoted by pA, pB respectively. For example, the results in Cohen et al. (2019)
have shown that R = 2 (Φ-1 (PA) - Φ-1 (PB)) where Φ-1 is the inverse of the standard Gaussian
CDF, PA is a lower bound of pa, and PB is an upper bound of PB.
The previous robustness definition only guarantees g(x) to be classi-
fied to the same class as g(x0), but ignores the fact that g(x0) may
be wrongly classified, which is not a precise definition. To make up
for it, Li et al. (2019) propose stability training with noise (STN) and
Cohen et al. (2019) adopt training with noise, both of which learn
smoothed classifiers mapping noisy inputs to correct labels. How-
ever, there is no guarantee to ensure g (x0) to be correctly labeled. Ac-
tually we found the robustness mainly comes from the STN/training
with noise, rather than the noise addition at the inference. In Fig. 1,
we could observe that the model performance indeed improves when
tested with noise. However, the classifier trained without additive
noise (triangle) degrades significantly compared with STN/training
with noise (diamond/circle). The result is an evidence that a classi-
fier almost cannot defend adversarial attacks when trained without
but tested with additive noise. Therefore, we conclude the smoothed
classifier can only improve robustness only if the base classifier is
robust.
We consider robustness refers to the ability of a DNN to classify ad-
versarial examples into the correct classes, and such an ability should
be evaluated on the population of adversarial examples, not a single
instance.
Figure 1: Accuracies of mod-
els trained on MNIST under
different levels of `2 attacks.
Undefend means a naturally
trained model. Solid lines rep-
resent models tested with ad-
ditive noise, and dotted lines
mean that without. σ = 0.1
means adding Gaussian noise
N(0, 0.12I).
5 Experiment
Baselines, datasets and models. Testing accuracies under different levels of adversarial attacks are
chosen as the metric. We compare the empirical performance of NAL with representative baselines
including: WRM (Sinha et al. (2018)), SmoothAdv (Salman et al. (2019a)), STN (Li et al. (2019))
and TRADES (Zhang et al. (2019)). Since WRM requires the loss function to be smooth, we follow
the convention to adapt the ReLU activation layer to the ELU layer. SmoothAdv combines adversar-
ial training with the smoothed classifier and claims to be superior than Cohen et al. (2019). Hence
we omit Cohen et al. (2019) in comparison. TRADES is an adversarial training algorithm which
won 1st place in the NeurIPS 2018 Adversarial Vision Challenge. Experiments are conducted on
datasets MNIST, CIFAR-10, and Tiny ImageNet, and models including a three-layer CNN, ResNet-
18, VGG-16, and their corresponding variants with ReLu replaced by ELU for fair comparison with
WRM. The cross-entropy loss is chosen for ` and c(x, x0) = kx - x0k22 is selected as the cost
function.
Training hyperparameters. Table 1 gives the training hyperparameters in NAL and the batch size
is chosen as 128. The hyperparameters used in baselines are supplied in Appendix B.1. Since NAL
Dataset	η1	η2	epochs	σ	γ	ε
MNIST	0.5∕γ	1 × 10-4	~~25~~	0.05	{0.25, 1.5, 3}	{0.84, 0.34, 0.21}
CIFAR-10(ReSNet-18)	0.5∕γ	1 × 10-4	100	0.1	{0.25, 1.5, 5}	{1.53, 0.92, 0.40}
CIFAR-10(VGG-16)	0.5∕γ	1 × 10-4	100	0.1	{0.25, 1.5, 5}	{1.23, 0.57, 0.28}
Tiny ImageNet	0.5∕γ	2 × 10-5	100	0.1	1.5	0.93
Table 1: Hyperparameters and perturbation ranges on different datasets.
Certificate: f⅛[φy(θjxo)] + YP
—Test worst-ca se: SUPEP[£(6; x)]
(a)
cost without noise
♦ ♦ cost with noise
(b)
NAL-yO.25	- -	WRM-yO.25
«---»	NAL-yl.5	4--o	WRM-yl.5
»~~»	NAL-y3.0	»---	WRM-γ3.0
/2 attack radius
(C)
NAL	»—» SmoothAdv
«~» TRADES	■~■ STN
(d)
Figure 2: (a) gives the distance between the robustness certificate (yellow) and the worst-case performance
on testing data (pink) with an example on MNIST. The gap between the two lines indicates the tightness of
our certificate (Eq. 3). (b) compares the performance of two models trained With different c(∙)s. The classifier
trained with the noise included in the cost has better performance overall. (c) compares the performance of
NAL with WRM on MNIST, CNN (ELU) under different γs. NAL overall has better performance than WRM.
(d) compares NAL with SmoothAdv, TRADES and STN on MNIST, CNN at γ = 0.25 and the corresponding
ε. NAL does not show significant improvement when γ is small.
and WRM bound the adversarial perturbations by the Wasserstein distance ρ which is different from
the `2 -norm perturbation range ε in SmoothAdv and TRADES, we need to establish an equiva-
lence between the perturbation ranges in different methods. Following the convention of Sinha et al.
(2018), we choose different γs and for each γ we generate adversarial examples x by PGD with
15 iterations. We compute ρ as the expected transportation cost between the generated adversarial
examples and the original inputs over the training set:
ε2 = ρ(θ) = EP0 EZ [c(x + z, x0)] .	(8)
And ε can be computed accordingly. The corresponding values of γ and ε used in experiments are
given in Table 1 as well.
Attack parameters. To evaluate the empirical accuracies for different methods, we adopt the PGD
attack Kurakin et al. (2018); Madry et al. (2017) as the adversarial attack following the convention
of Li et al. (2019); Sinha et al. (2018); Zhang et al. (2019), etc. We set the number of iterations in
PGD attack as KattaCk = 20 and the learning rate η = 2εattac⅛/Kattack where εattac⅛ is '2 attack radius.
5.1 Results
Certificate. To better understand how close the upper bound is to the true distributional risk,
we plot our certificate γρ + EPb [φγ (θ; x0)] against any level of robustness ρ, and the out-of-
sample (test) worst-case performance supS∈P ES [`(θ; s)] for NAL (Fig. 2(a)). Since the worst-
case loss is hard to evaluate directly, we solve its Lagrangian relaxation for different values
of γadv . For each γadv, we compute the average distance to adversarial examples in the test
set as ρbtest (θ) := EPb EZ [c (x? + z, x0)] where Ptest is the test data distribution and x? =
arg maXχ EZ {'(θ; X + Z) - Yadvc(x + z, x0)} is the adversarial perturbation of x°. The worst-case
loss is given by (ρbtest (θ) , EPb EZ [` (θ; x? + z)]). As we observe, ρbtest (θ) tends to increase with a
higher noise level. Hence we need to keep the noise at an appropriate level to make our certificate
tractable.
Cost without noise. To find out if NAL works when noise is removed from the cost, we designed
a verification experiment on CIFAR-10 (ResNet-18) by letting c(x, x0) = kx - x0 k22 and inserting
noise only to `. We setγ = 1.5, σ = 0.1, K = 4, r = 4. As Fig. 2(b) has shown, the accuracy perfor-
mance of the model excluding noise from the cost is far inferior, which shows that the randomized
noise is an inherent part in the design.
Sample number and PGD iterations. We also study the impact of the noise sample number s and
PGD iteration K to the model robustness with CIFAR-10 (ResNet-18) as an example. The result in
NAL
TRADES
NAL-yO.25	WRM-yO.25
‹-» NAL-yl.5	—WRM-yl.5
•-▼ NAL-y5.0	、--V WRM-y5.0
(a)
»—» SmoothAdv
■~■ STN
(b)
NAL-yO.25	∙--	WRM-yO.25
J→	NAL-γl.5	4-→	WRM-yl.5
、/	NAL-y5.0	JT	WRM-y5.0
NAL	»—» SmoothAdv
«—» TRADES	■~■ STN
(c)
(d)
Figure 3: NAL outperforms baselines on CIFAR-10, VGG-16 and Tiny ImageNet, ResNet-18. (a),(c) are
trained on ELU models under different γs. For the same γ, NAL exceeds WRM. (b),(d) are trained on ReLU
models with γ = 1.5 and the corresponding ε. NAL yields the highest robustness under different levels of
attack. STN has the highest clean accuracy.
Table 2 shows that while the model performance enhances with K, it does not necessarily increase
with a larger noise samples. We did not test with greater noise samples due to high complexity.
For a combined consideration of computation overhead and accuracy, we choose K = 4, r = 4 by
default in the experiments, which is likely to deliver a sufficiently good performance. Due to space
constraints, complete experimental results are in Appendix B.2.
Penalty and noise level. We vary the value of γ and σ in the experiments to find out their impact. By
the results in Fig. 2 (c),(d) and 3, we observe γ = 0.25 yields the best performance for MNIST, and
γ = 1.5 is best for CIFAR-10 and Tiny ImageNet, considering all levels of adversarial attacks. For
a complete result on γ, one can refer to Appendix B.3. Likewise, the best value of σ also depends
on the dataset, shown by the experimental results in Appendix B.2.
Comparison with baselines. Finally, we compare the empirical accuracies with the baselines and
the results are presented in Fig. 2 (c),(d) and 3. For WRM, the experiments are conducted on the
modified structure of DNNs to ensure smoothness. NAL has superior performance in almost all
cases except that: 1) the clean accuracies (denoted by `2 attack radius = 0) on CIFAR-10 and Tiny
ImageNet of NAL are inferior to STN; 2) on MNIST, the performance of NAL is no worse but does
not exceed baselines by a large margin. For 1), we found STN mostly has far worse performance
than other schemes when the attack radius > 0, which echos the proposition in Salman et al. (2019a)
that adversarial training brings higher robustness than stability training. Hence it can be explained
by the inherent tradeoff between clean accuracy and robustness (Zhang et al. (2019)) that STN has
higher clean accuracies than others. Actually, NAL shows better tradeoff between accuracy and ro-
bustness than baselines, indicated by the relatively flat accuracy lines. For 2), we think MNIST has
a relatively simple decision boundary than the other two datasets and hence allows larger perturba-
tions (smaller γ). Thus the performance boost by NAL is not significant. Actually, when γ is larger,
the performance of NAL exceeds baselines by a large margin (Appendix B.3).
`2 attack radius		0	0.25	0.5	0.75	1	1.25	1.5	1.75
(K, r)	(4, 1)	0.8647	0.7540	0.5950	0.4297	0.2814	0.1713	0.0983	0.0520
(K, r)	(4, 4)	0.8546	0.7643	0.6520	0.5202	0.3922	0.2765	0.1811	0.1120
(K, r)	(4, 8)	0.8537	0.7622	0.6482	0.5171	0.3846	0.2641	0.1702	0.0997
(K, r)	(8, 1)	0.8593	0.7555	0.6091	0.4630	0.3260	0.2205	0.1453	0.0929
(K, r)	(8, 4)	0.8517	0.7663	0.6566	0.5289	0.3970	0.2769	0.1827	0.1129
(K, r)	(8, 8)	0.8493	0.7582	0.6520	0.5302	0.3978	0.2828	0.1895	0.1151
Table 2: Testing accuracies of NAL (CIFAR-10, ResNet-18) on a variety of r and K. Under each setting, the
model with the highest clean accuracy (`2 attack radius = 0) is chosen for testing. Numbers in bold represent
the best performance in defending the attack.
6 conclusion
Our work view the robustness of a smoothed classifier from a different perspective, i.e., the worst-
case population loss over the input distribution. We provide a tractable upper bound (certificate) for
the loss and devise a noisy adversarial learning approach to obtain a tight certificate. Compared with
previous works, our certificate is practically meaningful and offers superior empirical robustness
performance.
References
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. pp. 1310-1320, 2019.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet Kohli.
A dual approach to scalable verification of deep networks. In UAI, volume 1, pp. 2, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. 2015.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Zhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness
to improve deep neural network robustness against adversarial attack. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 588-597, 2019.
Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. Certified robustness for top-k
predictions against adversarial perturbations via randomized smoothing. In International Confer-
ence on Learning Representations, 2019.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2018.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. In Advances in Neural Information Processing
Systems, pp. 4910-4921, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems, pp. 9464-9474, 2019.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 369-385, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. 2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3578-3586,
2018.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
Advances in neural information processing systems, pp. 2971-2980, 2017.
NhatHai Phan, My T Thai, Han Hu, Ruoming Jin, Tong Sun, and Dejing Dou. Scalable differential
privacy with certified robustness in adversarial learning. 2020.
Rafael Pinot, LaUrent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cedric GoUy-
Pailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization.
In Advances in Neural Information Processing Systems, pp. 11838-11848, 2019.
Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, and Jamal Atif. Randomization
matters. how to defend against strong adversarial attacks. 2020.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10877-10887, 2018.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems, pp. 11292-11303, 2019a.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robustness verification of neural networks. In Advances in Neural Information
Processing Systems, pp. 9835-9846, 2019b.
GagandeeP Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10802-10813, 2018.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
PrinciPled adversarial training. 2018.
Vincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer Programming. In International Conference on Learning Representations, 2018.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. ImProving
adversarial robustness requires revisiting misclassified examPles. In International Conference on
Learning Representations, 2019.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, and Inderjit
Dhillon. Towards fast comPutation of certified robustness for relu networks. In International
Conference on Machine Learning (ICML), 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examPles via the convex outer
adversarial PolytoPe. In International Conference on Machine Learning, PP. 5286-5295. PMLR,
2018.
Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adver-
sarial attacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security,
PP. 39-49, 2017.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically PrinciPled trade-off between robustness and accuracy. 2019.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. In Advances in neural information
processing systems, PP. 4939-4948, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks.
2020.
StePhan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. ImProving the robustness of deeP
neural networks via stability training. In Proceedings of the ieee conference on computer vision
and pattern recognition, PP. 4480-4488, 2016.
A Proofs
A.1 Proof of Theorem 1
Proof. We exPress the worst-case loss in its dual form with dual variable γ. By the weak dual
ProPerty, we have
sup ES [`(θ; s)] ≤ inf sup {ES [`(θ; s)] - γWc (S, P0) + γρ} ,	(9)
S∈P	γ≥0 S∈P
the left hand-side of which can be rewritten in integral form:
inf sup {ES [`(θ; x + z)] - γWc(S, P0) + γρ}
γ≥0 S∈P
(10)
inf sup	`(θ; x + z)dZ(z)P (x) - γWc(S, P0) + γρ .
γ≥0 S∈P
Note that for any π ∈ Π(S, P0), we have f (s)dS =	f(s)dπ(s, x0). And by the definition of
Wasserstein distance, we have
inf sup	`(θ; s)dS(s) - γWc(S, P0) + γρ
γ≥0 S∈P
inf sup
γ≥0 S∈P
`(θ; s)dπ(s, x0) - γ	inf	c(s, x0)dπ(s, x0) + γρ
π∈Π(S,P0)
(11)
inf sup sup	[`(θ; s) - γ c(s, x0)]dπ(s, x0) + γρ .
γ≥0 S∈P π∈Π(S,P0)
By the independence between z and x, x0 , one would obtain
ZZ['(θ; S)-YcG x0)]dπ(s, x0)=ZZZ['(θ;X+Z)-Yc(X+z，x0)]dZ(z)dπ(χ, x0)	(12)
By taking the maximum over x,
[`(θ; X + z) - Yc(X + z, X0)]dZ (z)dπ (X, X0)
EZ [`(θ; X + z) - Yc(X + z, X0)]dπ(X, X0)
(13)
≤	sup {EZ [`(θ; X + z) - Yc(X + z, X0)]} dπ(X, X0).
Fixing X to be value that maximizes the expression to be integrated, X in the formula is fixed, so we
only need to integrate dπ(X, X0 ) on X. So we can get:
sup {EZ [`(θ; X + z) - Yc(X + z, X0)]} dπ(X, X0)
=	sup {EZ [`(θ; X + z) - Yc(X + z, X0)]} dP0 (X0)
x0 x
=EP0 sup EZ [`(θ; X + z) - Yc(X + z, X0)].
x
(14)
Because the distribution of z is definite and z is independent of X, and supremum of S is replaced
by the supremum of X. Therefore, Eq. 11 can be written as
inf sup sup	[`(θ; s) - Y c(s, X0)]dπ(s, X0) + Yρ
γ≥0 S∈P π∈Π(S,P0)
≤ inf sup sup	EP0 sup EZ [`(θ; X + z) - Yc(X + z, X0)] + Yρ
γ≥0 S∈P π∈Π(S,P0)	x
inf EP0 sup EZ [`(θ; X + z) - Yc(X + z, X0)] + Yρ .
γ≥0	0 x
By plugging the above into Eq. 9, we could get
sup ES [`(θ; s)] ≤ inf EP0 sup EZ [`(θ; X + z) - Yc(X + z, X0)] + Yρ
S∈P	γ≥0	x
inf {EP0 [φγ (θ; X0)] + Yρ} ≤ EP0 [φγ (θ; X0)] + Yρ.
γ≥0
(15)
(16)
for any given Y ≥ 0, which completes the proof.
□
A.2 Proof of Theorem 2
Proof. The proof of ' being 2M -Smooth is equivalent to ▽% being 2MM -Lipschitz. We apply the
El	∙	X~! Λ	1	, C
Taylor expansion in ▽' at xo and set δ = xo 一 x:
V'(xo) = V'(x) + V2'(x + θδ)δ,	(17)
where 0 < θ < 1. Hence We only need to prove ∣∣V2"χ + θδ)k2 is bounded since ∣∣V%(χ + δ) -
Va(X) k 2 = kV2%(χ + θδ)δ∣∣2. By taking the first and second-order derivatives of %(x), we have
Va(X)= (2πJ2σd+2 / d '(t)(t - x) eχp (一 2⅛ kx - tk2) dt,	(18)
and
v"(x) = (2π∕σd+2 ZJ⑻ eχp (一2⅛kx - tk2) [-I+σ12(t 一 χ)(t - x)τ]dt.	(19)
We divide the right hand-side of Eq. 19 into two halves with the first half:
k (2π)d∕2σd+2 ZLt exp (一2σ2 kx - tk2) (-I)dtk2
1	1M
=σ k'(x)(-i tk2 ≤ 手 k'(χ)k2 ≤ σ ∙
The second half is
k (2π)d∕2σd+4 ɪd '(t) exp (一 2σ2 kx - tk2) ((t 一 x)(t 一 χ)τ)dtk2
≤ (2π)d12σd+4 / d |'(t)| exp (一 2σ2 kx - tk2) k(t — x)(t — x)τk2dt	QI)
≤ (2∏)Mσd+4 ZRdeXP (-2σσ2 kx - tk2) k(t -x)(t - χ)τk2dt∙
Due to the rank of the matrix (t 一 x)(t 一 x)τ is 1, its `2 norm is easy to compute:
k(t 一 x)(t 一 x)τ k2 = (t 一 x)τ (t 一 x).	(22)
Hence
(2π)d∕2σd+4 ZRd exp (一2σ2kx - tk2)相一勾([一 x)τk2dt = σ.	(23)
Finally, combining the two halves we get
kV"(x + θδ)k2 ≤ 2M.	(24)
σ2
□
A.3 Proof of Corollary 1
Proof. Since ' is 2MM -smooth and C is 1-strongly convex in its first argument, we have
2M
VX'(θ; x) W FI, and	(25)
x	σ2
V2xEZc(x + z, zo) = V2x c(x,xo) + dσ2 = V2xc(x, xo)	I.	(26)
Therefore we have
VxEZ {'(θ; X + Z)- Yc (x + z,xo)}W (考 一 Y)I.	(27)
x	σ2
Hence the strong concavity is proved for Y ≥ 2MM .
□
A.4 Convergence Proof
We start with the required assumptions, which roughly quantify the robustness we provide.
Assumption 1. The loss ` : Θ × X → [0, M] satisfies the Lipschitzian smoothness conditions
.. ^ . . ^................. .. .............. ^ . . ^ . .... .. ...
kVθ'(θ;X)- Vθ'(θ0;x)k* ≤ Lθθkθ - θ0k, kVχ'(θ;X)- Vχ'(θ;x0)k* ≤ Lxxkx - x0k,	.28.
___^, . .	_ ^, . .. ..	_ ..	....... ^, . .	_ ^,.. ...	(28)
kVθ'(θ;x) -Vθ'(θ;x0)k* ≤ Lθxkx - x0k, kVx'(θ; χ)-vx'(θ0; x)k* ≤ Lxθkθ - θ0k.
Let k∙k * bethe dual norm to k ∙ k; We abuse notation by using the same norm k ∙ k on Θ and X. Here
We have proved the second condition of Assumption 1 holds true by Theorem 2, with Lxx = 2MM .
El	C	∙ 1' Λ , ∙ r' . 1	.1	.1	T , ∙	IFF,	∙	∙1	C	F	C
Therefore, if ` satisfies the other three conditions, We could adopt a similar proof procedure for
Theorem 2 in Sinha et al. (2018) to prove the convergence of Algorithm 1.
A.5 Proof of Theorem 3
Proof. By Eq. 16 We could get
sup ES['(θ; s)] ≤ inf {γρ + Epo [Φγ(θ; xo)]} ,	(29)
S∈P	γ≥0
Where
EP0 [φγ (θ; X0)] = EP0 sup EZ [`(θ; X + z) - γc(X + z,X0)] .	(30)
x∈X
Since `(θ; X + z) - γc (X + z, X0) is strongly concave for X + z in Sinha et al. (2018), by Jensen
Inequality We have for any fixed X,
EZ {'(θ; x + Z) 一 Yc (x + z, X0)}
≤'(θ; EZ(x + z)) - Yc (EZ(x + z), xo)	(31)
='(θ; x) — Yc (x, xo).
Hence, the folloWing inequality holds
EP0 [φγ (θ; xo)] ≤ EP0 sup [`(θ; x) - Yc (x, xo)] .	(32)
x∈X
By Proposition 1 in Sinha et al. (2018), We could get
inf EP0 sup [`(θ; x) - Yc (x, xo)] + Yρ = sup	EP0 [`(θ; x)].	(33)
γ≥o	x∈X	P0 :Wc (P0 ,P0)≤ρ
Finally, we can get Eq. 7 by concatenating the inequalities which completes the proof.	□
A.6 Connections between Robustness Certificates
Proposition 1. Let pA , pB denote the largest and second largest probabilities returned by the
smoothed classifier g(xo) and R = 2 (Φ-1 (pa) — Φ-1 (pB)). We choose ' as the cross-entropy
loss in the smoothed lossfunction %(x) = EZ ['(θ; X + z)], Z 〜Z = N (0, σ2I) .If
"0; x) ≤ - log (φ Φ-1(pB) + kx -σx0k2 )	(34)
holds, and the ground truth label y = cA, then g(x) is robust against any x such that kx-xok2 ≤ R.
Proof. By Theorem 1 of Cohen et al. (2019), we just need to prove the condition Eq. 34 leads to the
condition kx - xo k2 ≤ R. With ` being the cross-entropy loss,
%(x) = Ez ['(θ; X + z)] = Ez [- log(f (y)(x + z))].	(35)
Then we use Jensen Inequality on - log(x) to obtain
“x) = EZ[- log(f (y)(x + z))] ≥ - log[Ezf (y)(x + z)].	(36)
As y = cA, we have EZf(y) (x + z) = P(f(x + z) = cA). By Eq. 34, we have
-log[P(f(x + Z) = ca)] ≤ EZ [-log(f (y)(x + z))] ≤-log(φ Φ-1(PB)+ kx -σx0k2 ).
(37)
And hence
P(f(x + Z)= CA) ≥ φ [φ-1(PB)+ kx - x0k21 .	(38)
σ
By the proof of Theorem 1 in Cohen et al. (2019),
P(f(x + Z) = ca) = Φ (φ-1 (PA) — kx -σx0k2) ,	(39)
which leads to
Φ-1 (PA) - fc⅛ ≥ Φ-1(PB) + ^X>.	(40)
—	σ	σ
Therefore,
kx - x0k2 ≤ 2 (Φ-1 (PA) - Φ-1 (PB)) = R.	(41)
To sum up, if Eq. 34 holds and x0 is correctly classified, g(x) is robust within a`2 ball with radius R.
One can tell the loss on a single instance is weakly associated with the robustness of the model, and
the condition of g(x) being robust is quite stringent. It is not practical to sum up the single-instance
loss to gauge the model robustness either.	□
B Experiments
B.1	Baseline Settings
We provide the training settings for baselines in Table 3. The learning rate η1 is adjusted according
to different γs and εs. The noise level (σ) is the same for all methods.
Dateset	mechanism	ηι	η2			batch size	epochs
	WRM	0.5∕γ	1	×	10-4	128	F
MNIST	STN	- /2	1	×	10-4	128	25
	SmoothAdv		1	×	10-4	128	25
	TRADES	〃2	1	×	10-4	128	25
	WRM	0.5∕γ	1	×	10-4	128	Igg
CIFAR-10	STN	- /2	1	×	10-4	128	100
	SmoothAdv		1	×	10-4	128	100
	TRADES	〃2	1	×	10-4	128	100
	WRM	0.5∕γ	2	×	10-5	128	1θ0
Tiny ImageNet	STN SmoothAdv	- /2	2 2	× ×	10-5 10-5	128 128	100 100
	TRADES	〃2	2	×	10-5	128	100
Table 3: Baseline hyperparameter settings. γ and ε is chosen from Table 1.
B.2	RESULTS WITH VARYING σ AND (K, r)
We compare NAL with SmoothAdv and STN under the same experimental setting but different
σs. In Table 4, NAL achieves the best performance at σ = 0.1 above all. We believe in different
experimental settings, the best σ value is different. For example, NAL and STN obtain the best
performance at σ = 0.1, whereas SmoothAdv performs best at σ = 0.05. For the same σ, NAL has
superior performance than the other two baselines except that, when σ = 0.05, SmoothAdv is more
robust than NAL for `2 attack radius ≥ 0.75. This is mainly because SmoothAdv achieves the best
performance when σ = 0.05. However, the model accuracies degrade below 0.5 is not our main
consideration.
`2 attack radius			0	0.25	0.5	0.75	1	1.25	1.5	1.75
NAL	σ=	0.05	0.8579	0.7809	0.6761	0.5549	0.4262	0.2916	0.1888	0.1329
NAL	σ=	0.1	0.8522	0.8155	0.7684	0.7140	0.6466	0.5684	0.4829	0.3909
NAL	σ=	0.2	0.8307	0.7781	0.7213	0.6498	0.5644	0.4785	0.3837	0.2959
SmoothAdv	σ=	0.05	0.7643	0.7086	0.6378	0.5644	0.4841	0.4050	0.3297	0.2602
SmoothAdv	σ=	0.1	0.8066	0.7264	0.6281	0.5376	0.4399	0.3467	0.2700	0.2010
SmoothAdv	σ=	0.2	0.7411	0.6758	0.6079	0.5327	0.4689	0.4005	0.3350	0.2736
STN	σ=	0.05	0.8988	0.7347	0.4834	0.2594	0.1167	0.0466	0.0155	0.0063
STN	σ=	0.1	0.8669	0.7609	0.6164	0.4416	0.2847	0.1678	0.0927	0.0443
STN	σ=	0.2	0.8000	0.7060	0.5867	0.4695	0.3523	0.2472	0.1708	0.1125
Table 4: Different methods with different levels of noise on CIFAR-10, ResNet-18, γ = 1.5 and (K, r) =
(4, 4). The best performance at the same noise level is in bold.
In Table 5, we show NAL’s accuracy over a variety of σ, K, r values. We found that the result of
σ = 0.12 is generally better than a larger value. Under the same σ, we choose K ∈ {2, 4, 6, 8}, r ∈
{1, 4},. We found the model cannot converge with (K, r) = (2, 1), and thus did not present the
results. The Table show that a larger K admits better robustness whereas r does not have that impact.
'2 attack radius 0
0.25	0.5	0.75	1
1.25	1.5	1.75
σ = 0.12
KKKKKK
244668
=======
414141
=======
rrrrrr
0.8593	0.8414	0.8152	0.7891	0.7584	0.7177	0.6699	0.6103
0.8480	0.8142	0.7772	0.7306	0.6756	0.6185	0.5521	0.4748
0.8462	0.8129	0.7728	0.7237	0.6692	0.6044	0.5339	0.4646
0.8528	0.8312	0.8105	0.7803	0.7473	0.7081	0.6585	0.5989
0.8424	0.7990	0.7584	0.7022	0.6372	0.5663	0.4853	0.3950
0.8526	0.8418	0.8329	0.8194	0.8049	0.7883	0.7670	0.7365
0.8443	0.8025	0.7494	0.6929	0.6250	0.5469	0.4578	0.3758
σ = 0.25
KKKKKKK
2244668
========
1414141
========
rrrrrrr
0.7522	0.6885	0.6186	0.5403	0.4596	0.3739	0.2971	0.2195
0.7953	0.7421	0.6801	0.6069	0.5226	0.4332	0.3498	0.2691
0.7669	0.7077	0.6399	0.5717	0.4896	0.4103	0.3318	0.2594
0.8121	0.7679	0.7143	0.6540	0.5882	0.5153	0.4381	0.3622
0.7632	0.7082	0.6501	0.5790	0.5095	0.4349	0.3569	0.2805
0.8098	0.7578	0.7059	0.6410	0.5677	0.4875	0.4133	0.3300
0.7808	0.7285	0.6720	0.6073	0.5273	0.4490	0.3694	0.2951
0.8150	0.7694	0.7132	0.6549	0.5896	0.5185	0.4391	0.3574
KKKKKKK
2244668
========
1414141
========
rrrrrrr
0.6434	0.5899	0.5335	0.4739	0.4150	0.3524	0.2958	0.2422
0.7122	0.6681	0.6201	0.5717	0.5212	0.4632	0.4090	0.3496
0.6744	0.6165	0.5631	0.5048	0.4450	0.3829	0.3226	0.2652
0.7186	0.6701	0.6217	0.5699	0.5135	0.4505	0.3966	0.3373
0.6860	0.6335	0.5799	0.5154	0.4566	0.3979	0.3362	0.2811
0.7185	0.6771	0.6243	0.5707	0.5174	0.4608	0.4019	0.3455
0.6943	0.6424	0.5911	0.5380	0.4758	0.4181	0.3548	0.2968
0.7239	0.6804	0.6320	0.5836	0.5345	0.4736	0.4250	0.3716
Table 5: NAL with different σs and (K, r) on CIFAR-10, ResNet-18 when γ = 1.16. The best performance
under the same noise level is in bold.
'2 attack radius	0	0.25	0.5	0.75	1	1.25	1.5	1.75
-ELUModel- ReLU Model	0.8596^^0.8046^^0.7348^^0.647^^0.5465^^0.4387^^0.3315^^0.2351 0.8522	0.8155	0.7684	0.714	0.6466	0.5684	0.4829	0.3909
Table 6: Testing accuracies for the ReLU model and the ELU model on CIFAR-10, ResNet-18.
B.3	RESULTS WITH VARYING γ
We show the impact of γ on MNIST and CIFAR-10. On MNIST, γ takes the value {0.25, 1.5, 3}
and σ is chosen as 0.05. On CIFAR-10, γ ∈ {0.25, 1.5, 5} and σ is set to 0.1. (K, r) = (4, 4) for all
experiments. Fig. 4(a) and 5(a) compare NAL with WRM on models with ELU, whereas the rest of
Fig. 4 and 5 show the comparison with SmoothAdv, TRADES, and STN on regular models. NAL
has superior performance than baselines in almost all cases.
NAL-VO.25	1 -	WRM-yO.25
4-→	NAL-yl.5	4-f	WRM-yl.5
»-»	NAL-y3.0	IT	WRM-y3.0
£2 attack radius
NAL	▼—< SmoothAdv
<_» TRADES	■~■ STN
/2 attack radius
NAL	▼—< SmoothAdv
<_» TRADES	■~■ STN
H attack radius
(C)
NAL	»—» SmoothAdv
<_» TRADES	■~■ STN
I2 attack radius
(d)
(a)	(b)
Figure 4:	NAL versus baselines on MNIST, CNN. (a) NAL versus WRM for different γs. (b) γ = 0.25. (c)
γ = 1.5. (d) γ = 3. Equivalent εs are used in SmoothAdv and TRADES.
NAL
*-→ TRADES
»—» SmoothAdv
■—■ STN
NAL	»—» SmoothAdv
*-→ TRADES	■~~■ STN
NAL-yO.25	- -	VVRM-VO.25
NAL-yl.5	♦- →	WRM-yl.5
NAL-y5.0	1 T	WRM-y5.0
CIFAR10 ResNetlS(ELU)
0 0.0	0.5	1.0	1.5
£2 attack radius
CIFAR10 ResNetlS(ReLU)
8o∣r∖s *
CIFAR10 ResNetlS(ReLU)
60
40
20
NAL	»—» SmoothAdv
*-→ TRADES	■~~■ STN
CIFAR10 ResNetlS(ReLU)
J 6。
0.0	0.5	1.0	1.5
/2 attack radius
(a)
0.0	0.5	1.0	1.5
I2 attack radius
0.0	0.5	1.0	1.5
I2 attack radius
(b)	(c)
(d)
(％)Λ3En33v
Figure 5:	NAL versus baselines on CIFAR-10, ResNet-18. (a) NAL versus WRM for different γs. (b) γ = 0.25.
(c) γ = 1.5. (d) γ = 5. Equivalent εs are used in SmoothAdv and TRADES.
The results on MNIST are similar to those obtained in the experiment section. The gaps between
NAL and baselines in Fig. 4(c)(d) grow larger than that in 4(b), which could be explained different
values of γ. On CIFAR-10, since we switch to ResNet-18, the experimental results are slightly
different from that on VGG-16. The best robustness occurs at γ = 0.25 (Fig. 5(b)). The performance
of STN is consistent with that in the experiment section: it enjoys the highest clean accuracies but
much worse performance over adversarial examples. As we observe on Fig. 5(b),(c),(d), with an
increase ofγ, the clean accuracy increases but at the sacrifice of adversarial accuracies. For example,
NAL degrades to the performance of SmoothAdv and TRADES at γ = 5.
B.4 Comparison between ReLU and ELU
Here we show the difference between ResNet-18 with ReLU and ELU on CIFAR-10 for NAL. From
Fig. 6, throughout the training process, the loss of the ReLU model is smaller than that of the ELU
MOqSnnmMj,
CIFAR10 ResNetl8
IOO
60
~
80
(兴)8V‰SΠJ1U1
Figure 6:	The comparison between the ReLU model (pink) and the ELU model (yellow) on CIFAR-10, ResNet-
18 with γ = 1.5 and σ = 0.1. The ReLU model converges faster than the ELU model.
model, and ReLU model presents faster convergence. The robustness performance of both models
is presented in Table 6. It is clear that in the testing phase, the ReLU model also obtains a better
performance. Hence NAL generally yields better performance on ReLU models than ELU models.