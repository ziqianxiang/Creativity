Under review as a conference paper at ICLR 2021
Factorized linear discriminant analysis for
phenotype-guided representation learning of
NEURONAL GENE EXPRESSION DATA
Anonymous authors
Paper under double-blind review
Ab stract
A central goal in neurobiology is to relate the expression of genes to the structural
and functional properties of neuronal types, collectively called their phenotypes.
Single-cell RNA sequencing can measure the expression of thousands of genes
in thousands of neurons. How to interpret the data in the context of neuronal
phenotypes? We propose a supervised learning approach that factorizes the gene
expression data into components corresponding to individual phenotypic charac-
teristics and their interactions. This new method, which we call factorized linear
discriminant analysis (FLDA), seeks a linear transformation of gene expressions
that varies highly with only one phenotypic factor and minimally with the oth-
ers. We further leverage our approach with a sparsity-based regularization algo-
rithm, which selects a few genes important to a specific phenotypic feature or
feature combination. We applied this approach to a single-cell RNA-Seq dataset
of Drosophila T4/T5 neurons, focusing on their dendritic and axonal phenotypes.
The analysis confirms results obtained by conventional methods but also points
to new genes related to the phenotypes and an intriguing hierarchy in the genetic
organization of these cells.
1	Introduction
The complexity of neural circuits is a result of many different types of neurons that specifically
connect to each other. Each neuronal type has its own phenotypic traits, which together determine
the role of the neuronal type in a neural circuit. Typical phenotypic descriptions of neurons include
features such as dendritic and axonal laminations, electrophysiological properties, and connectiv-
ity (Sanes & Masland, 2015; Zeng & Sanes, 2017; Gouwens et al., 2019). However, the genetic
programs behind these phenotypic characteristics are still poorly understood.
Recent progress in characterizing neuronal cell types and investigating their gene expression, espe-
cially with advances in high-throughput single-cell RNA-Seq (Zeng & Sanes, 2017), provides an
opportunity to address this challenge. With massive data generated from single-cell RNA-Seq, we
now face a computational problem: how to factorize the high-dimensional data into gene expression
modules that are meaningful to neuronal phenotypes? Specifically, given phenotypic descriptions
of neuronal types, such as their dendritic stratification and axonal termination, can one project the
original data into a low-dimensional space corresponding to these phenotypic features and their
interactions, and further extract genes critical to each of these components?
Here we propose a new analysis method named factorized linear discriminant analysis (FLDA). In-
spired by multi-way analysis of variance (ANOVA) (Fisher, 1918), this method factorizes data into
components corresponding to phenotypic features and their interactions, and seeks a linear transfor-
mation that varies highly with one specific factor but not with the others. The linear nature of this
approach makes it easy to interpret, as the weight coefficients directly inform the relative importance
of each gene to each factor. We further introduce a sparse variant of the method, which constrains
the number of genes contributing to each linear projection. We illustrate this approach by applying
FLDA to a single-cell transcriptome dataset of T4/T5 neurons in Drosophila (Kurmangaliyev et al.,
2019), focusing on two phenotypes: dendritic location and axonal lamination.
1
Under review as a conference paper at ICLR 2021
A	Complete table
Phenotypic feature 2
I ə-ln-eəj-d-ouəɪd
	j = O	j = 1	j = 2	j = 3
i = 0	n00	n01	n02	n03
i= 1	n10	n11	n12	n13
i = 2	n20	n21	n22	n23
I ə-ln-eəj-d-ouəɪd
B	Partial table
Phenotypic feature 2
	j = 0	j = 1	j = 2	j = 3
i = 0	n00		n02	n03
i= 1		n11	n12	n13
i = 2	n20	n21		n23
Figure 1: Illustration of our approach. (A,B) In the example, cell types are jointly represented by
two phenotypic features, indexed with labels i and j respectively. If only some combinations of
the two features are observed, one obtains a partial contingency table (B) instead of a complete one
(A). (C) We seek linear projections of the data that separate the cell types in a factorized manner
corresponding to the two features. Here u, v, and W are aligned with Feature 1, Feature 2, and the
interaction of both features, with the projected coordinates y, z, and S respectively.
2	FACTORIZED LINEAR DISCRIMINANT ANALYSIS (FLDA)
Suppose that we are given gene expression data of single neurons which are typically very high-
dimensional. These cells are classified into cell types, as a result of clustering in the high-
dimensional space and annotations based on prior knowledge or verification outcome (Macosko
et al., 2015; Tasic et al., 2016; Shekhar et al., 2016; Tasic et al., 2018; Peng et al., 2019). We know
the phenotypic traits of each neuronal type, therefore each type can also be jointed defined by the
phenotypic features. We want to find an interpretable low-dimensional embedding in which certain
dimensions represent factors of phenotypic features or their interactions. This requires that variation
along one of the axes in the embedding space causes the variation of only one factor. In reality,
this is hard to satisfy due to noise in the data, and we relax the constraint by letting data projected
along one axis vary largely with one factor while minimally with the others. In addition, we ask that
cells classified as the same type are still close to each other in the embedding space, while cells of
different types are far apart.
As a start, let us consider only two phenotypic features of neurons, dendritic stratification, and
axonal termination, both of which can be described with discrete categories, such as differentregions
or layers in the brain (Oh et al., 2014; Euler et al., 2014; Sanes & Masland, 2015; Kurmangaliyev
et al., 2019). Suppose that each cell type can bejointly represented by its dendritic location indexed
as i and axonal lamination indexed as j, with the number of cells within each cell type nj. This
representation can be described using a contingency table (Figure 1A,B). Note here that we allow
the table to be partially filled.
Let Xijk(k ∈ 1, 2,...nij) represent the expression values of g genes in each cell (Xijk ∈ Rg)). How
to find linear projections yij-k = UTXijk and Zijk = VTXijk that are aligned with features i and j
respectively (Figure 1C)? We first asked whether we could factorize, for example, yi7-k, with respect
to components depending on features i and j . Indeed, motivated by the linear factor models used
in multi-way ANOVA and the idea of partitioning variance, we constructed an objective function as
the following, and found u* that maximizes the objective (see detailed analysis in Appendix A):
u* = arg max UTNAU
u∈Rg uTMeu
(1)
When we have a complete table, and there are a levels for the feature i and b levels for the feature j ,
we have
NA= MA - λ1MB -λ2MAB
(2)
where MA, MB , and MAB are the covariance matrices explained by the feature i, the feature j ,
and the interaction of them. λ1 and λ2 are hyper-parameters controlling the relative weights of MB
2
Under review as a conference paper at ICLR 2021
and MAB with respect to MA . Me is the residual covariance matrix representing noise in gene
expressions. Formal definitions of these terms are the following:
MAB
MA	1 ab 二-^ɪ EE(mi. — m..)(mi. — m..)T	(3) a - 1 i=1j =1
MB =	1	ab 二 b-ɪ ££(mj — m..)(m.j - m..)T	(4) b - 1 i=1j =1
ab
(mij -mi.
i=1 j=1
- m.j +m..)(mij - mi. - m.j + m..)T
1
(a - 1)(b -I)
(5)
1 a b 1 nij
Me = N - ab	Σ[n~∙耳(Xjk - mi ) (Xijk - mj )T]	⑹
where
in which
'
ab
i=1j =1
mi =-
b
j=1
1b
1
mj = a
a
i=1
nij
ij
njS
Xij k
(7)
(8)
(9)
(10)
An analogous expression provides the linear projection v* for the feature j, and w* for the inter-
action of both features i and j . Similar arguments can be applied to the scenario of a partial table
to find u* or v * as the linear projection for the feature i or j (see Appendix B for mathematical
details).
Note that NA is symmetric and Me is positive definite. Therefore the optimization problem is a
generalized eigenvalue problem (Ghojogh et al., 2019). When Me is invertible, u* is the eigenvector
associated with the largest eigenvalue of Me-1NA. In general, if we want to embed Xijk into a d-
dimensional subspace aligned with the feature i (d < a), we can take the eigenvectors with the d
largest eigenvalues of Me-1NA, which we call the top d factorized linear discriminant components
(FLDs). Since multi-way ANOVA can handle contingency tables with more than two dimensions,
our analysis can be easily generalized to more than two features.
3	Sparsity-based regularization of FLDA
For this domain-specific application in neurobiology, there is particular interest in finding a small
group of genes that best determine one of the phenotypic features. This leads to finding axes that
have only a few non-zero components. To identify such a sparse solution, we solved the following
optimization problem:
3
Under review as a conference paper at ICLR 2021
一 *
U
arg max
u∈Rg
UT NAu
UT Meu
subject to	||U||0 ≤ l
(11)
from which the number of non-zero elements of u* is less or equal to l.
This is known as a sparse generalized eigenvalue problem, which has three unique challenges, as
listed in Tan et al. (2018): first, when the data are very high-dimensional, Me can be singular
and non-invertible; second, because of the normalization term uTMeu, many solutions for sparse
eigenvalue problems cannot be applied directly; finally, this problem involves maximizing a convex
objective over a nonconvex set, which is NP-hard.
To solve it, we used truncated Rayleigh flow (Rifle), a method specifically developed to solve sparse
generalized eigenvalue problems. The algorithm of Rifle is composed of two steps (Tan et al., 2018):
first, to obtain an initial vector u0 that is close to u*. We used the solution from the non-sparse FLDA
as an initial estimate of u0 ; second, iteratively, to perform a gradient ascent step on the objective
function, and then execute a truncation step that preserves the l entries of u with the largest values
and sets the remaining entries to 0. Pseudo-code for this algorithm can be found in Appendix C.
As proved in Tan et al. (2018), if there is a unique sparse leading generalized eigenvector, Rifle
will converge linearly to it with the optimal statistical rate of convergence. The computational com-
plexity of the second step is O(lg + g) for each iteration, therefore Rifle scales linearly with g, the
dimensionality of the original data. Based on the theoretical proof, to guarantee convergence, the hy-
perparameter η was selected to be sufficiently small such that ηλmax (Me) < 1, where λmax(Me)
is the largest eigenvalue of Me. In our case, the other hyperparameter l, indicating how many genes
to be preserved, was empirically selected based on the design of a follow-up experiment. As men-
tioned later in Results, we chose l to be 20, a reasonable number of candidate genes to be tested in
a biological study.
4	Related work
4.1	Linear Dimensionality Reduction
FLDA is a method for linear dimensionality reduction (Cunningham & Ghahramani, 2015). For-
mally, linear dimensionality reduction is defined as the following: given n data points each of g
dimensions, X = [x1, x2, ..., xn] ∈ Rg×n, and a choice of reduced dimensionality r < g, optimize
an objective function f(.) to produce a linear projection P ∈ Rr×g, and Y = PX ∈ Rr×n is the
low-dimensional transformed data.
State-of-the-art methods for linear dimensionality reduction include principal component analy-
sis (PCA), factor analysis (FA), linear multidimensional scaling (MDS), linear discriminant anal-
ysis (LDA), canonical correlations analysis (CCA), maximum autocorrelation factors (MAF), slow
feature analysis (SFA), sufficient dimensionality reduction (SDR), locality preserving projections
(LPP), and independent component analysis (ICA) (Cunningham & Ghahramani, 2015). Some of
them are obviously unsuitable to the problem we want to solve, for example, MAF and SFA are
developed for data with temporal structures (Larsen, 2002; Wiskott & Sejnowski, 2002), and LPP
focuses on the local structures of the data instead of their global organization (He & Niyogi). The
remaining approaches can be roughly grouped for either unsupervised or supervised linear dimen-
sionality reduction, and we discuss them separately.
4.2	Unsupervised Methods for Linear Dimensionality Reduction
UnsUPervised linear dimensionality reduction, including PCA (Jolliffe, 2002), ICA (Hyvarinen
et al., 2001), FA (Spearman, 1904) and more, project data into a low-dimensional space without
using suPervision labels. They are not suitable to solve our Problem, due to the sPecific charac-
teristics of the gene exPression data. The dimensionality of gene exPression data is usually very
high, with tens of thousands of genes, and exPressions of a fair number of them can be very noisy.
These noisy genes cause large variance among individual cells, but in an unstructured way. Without
suPervision signals from the PhenotyPic features, unsuPervised methods tend to select these genes
4
Under review as a conference paper at ICLR 2021
to construct the low-dimensional space, which offers neither the desired alignment nor a good sep-
aration of cell type clusters. To illustrate this, we performed PCA on the gene expression data and
compared it with FLDA. Briefly, we solved the following objective to find the linear projection:
*	UT XX T U
U = arg max-----T----
(12)
The outcome of this comparison is shown in Results.
4.3	Supervised Methods for Linear Dimensionality Reduction
Supervised linear dimensionality reduction, represented by LDA (Fisher, 1936; McLachlan, 2004)
and CCA (Hotelling, 1936; Wang et al., 2016), can overcome the above issue. By including super-
vised signals of phenotypic features, we can devalue genes whose expressions are non-informative
about the phenotypes.
4.3.1	Linear Discriminant Analysis (LDA)
We name our method FLDA because its objective function has a similar format as that of LDA. LDA
also models the difference among data organized in pre-determined classes. Formally, LDA solves
the following optimization problem:
U
UTΣbU
arg max 一
u∈Rg U ΣeU
(13)
where Σb and Σe are estimates of the between-class and within-class covariance matrices respec-
tively.
Different from FLDA, the representation of these classes is not explicitly formulated as a contin-
gency table composed of multiple features. The consequence is that, when applied to the example
problem in which neuronal types are organized into a two-dimensional contingency table with phe-
notypic features i and j, in general, axes from LDA are not aligned with these two phenotypic
features.
However, in the example above, we can perform two separate LDAs for the two features. This allows
the axes from each LDA to align with its specific feature. We call this approach “2LDAs”. There
are two limitations of this approach: first, it discards information about the component depending
on the interaction of the two features which cannot be explained by a linear combination of them;
second, it explicitly maximizes the segregation of cells with different feature levels which sometimes
is not consistent with a good separation of cell type clusters. Detailed comparisons between LDA,
“2LDAs” and FLDA can be found in Results.
4.3.2	Canonical Correlation Analysis (CCA)
CCA projects two datasets Xa ∈ Rg×n and Xb ∈ Rd×n to Ya ∈ Rr×n and Yb ∈ Rr×n , such that
the correlation between Ya and Yb is maximized. Formally, it tries to maximize this objective:
(u*, v*) = arg max
u∈Rg,v∈Rd
UT(XaXT )-1 XaXT (XbXT)-1 V
(UTU)-2 (VTV)-2
(14)
To apply CCA to our problem, we need to set Xa to be the gene expression matrix, and Xbto
be the matrix of d phenotypic features (d = 2 for two features as illustrated later). In contrast
with FLDA, CCA finds a transformation of gene expressions aligned with a linear combination
of phenotypic features, instead of a factorization of gene expressions corresponding to individual
phenotypic features. This difference is quantified and shown in Results.
5
Under review as a conference paper at ICLR 2021
Figure 2: Quantitative comparison between FLDA and other models. (A) Illustration of data syn-
thesis. See Appendix D for implementation details. Color bar indicates the expression values of
the ten generated genes. (B) Normalized overall SNR metric of each analysis. The SNR values are
normalized with respect to that of LDA. (C) Overall modularity score for each analysis. Error bars
in (B,C) denote standard errors each calculated from 10 repeated simulations.
5	Experiments
5.1	Datasets
In order to quantitatively compare FLDA with other linear dimensionality reduction methods, such
as PCA, CCA, LDA, and the "2LDAs” approach, We created synthetic datasets. Four types of cells,
each containing 25 examples, were generated from a Cartesian product of two features i and j,
organized in a 2x2 complete contingency table. Expressions of 10 genes were generated for these
cells, in which the levels of Genes 1-8 were correlated with either the feature i, the feature j, or the
interactions of them, and the levels of the remaining 2 genes were purely driven by noise (Figure 2A).
Details of generating the data can be found in Appendix D.
To illustrate FLDA in analyzing single-cell RNA-Seq datasets for real problems of neurobiology,
and demonstrate the merit of our approach in selecting a few important genes for each phenotype,
we used a dataset of Drosophila T4/T5 neurons (Kurmangaliyev et al., 2019). T4 and T5 neurons
are very similar in terms of general morphology and physiological properties, but they differ by the
location of their dendrites in the medulla and lobula, two distinct brain regions. T4 and T5 neurons
each contain four subtypes, with each pair of the four laminating their axons in a specific layer in the
lobula plate (Figure 3A). Therefore, we can use two phenotypic features to describe these neurons:
the feature i indicates the dendritic location at the medulla or lobula; the feature j describes the
axonal lamination at one of the four layers (a/b/c/d) (Figure 3B). In this experiment, we focused on
the dataset containing expression data of 17492 genes from 3833 cells collected at a defined time
during brain development.
5.2	Data preprocessing
The T4/T5 neuron dataset was preprocessed as previously reported (Shekhar et al., 2016; Peng
et al., 2019; Tran et al., 2019). Briefly, transcript counts within each column of the count matrix
(genes×cells) were normalized to sum to the median number of transcripts per cell, resulting in
the normalized counts Transcripts-per-median or T PMgc for Gene g in Cell c. We used the log-
transformed expression data Egc = ln (TPMgc + 1) for further analysis. We adopted a common
approach in single-cell RNA-Seq studies that is based on fitting a relationship between mean and
coefficient of variation (Chen et al., 2016; pandey et al., 2018) to select highly variable genes, and
performed FLDA on the expression data with only these genes. We preprocessed the data with pCA
and kept principal components (PCs) explaining 〜99% of the total variance before running FLDA
but not the sparse version of the algorithm. In the experiment below, we set the hyper-parameters λs
in Equation (2) to 1.
5.3	Metrics
We included the following metrics to evaluate our method: A signal-to-noise ratio (SNR) measures
how well each discriminant axis separates cell types compared with noise estimated from the vari-
6
Under review as a conference paper at ICLR 2021
D
medulla	lobula	C
T4 neurons	T5 neurons
B	Axonal termination
Uo-SUo---.lpuəɑ
X	a	b	C	d
medulla	T4a	T4b	T4c	T4d
lobula	T5a	T5b	T5c	T5d
-200
105-r
ITα^H
-150 -100 -50	0	50	100	150
FLDJ
-150
-200
-100	0
FLD J_1
G
-100
100	200	-150 -100 -50	0	50	100	150
FLD J_2
Figure 3: FLDA on the dataset of T4/T5 neurons. (A) T4/T5 neuronal types and their dendritic
and axonal phenotypes. (B) T4/T5 neurons can be organized in a complete contingency table. Here
i indicates the dendritic location and j indicates the axonal termination. (C) SNR metric of each
discriminant axis. (D) Projection of the data into the three-dimensional space consisting of the
discriminant axis for the feature i (FLDi) and the first and second discriminant axes for the feature
j (FLDjI and FLDj2). (E-G) Projection of the data into the two-dimensional space made of FLDi
and FLDjI (E), FLDjI and FLDj2 (F), or FLDj2 and FLDj3 (the third discriminant axis for the
feature j) (G). Different cell types are indicated by different colors as in (A) and (D).
ance within cell type clusters. The explained variance (EV) for each discriminant axis measures
how much variance of the feature i or j is explained among the total variance explained by that axis.
The mutual information (MI) between each discriminant axis and each feature quantifies how ”in-
formative” an axis is to a specific feature. Built on the calculation of ML we included the modularity
score which measures whether each discriminant axis depends on at most one feature (Ridgeway &
Mozer, 2018). The implementation details of these metrics can be found in Appendix E.
6	Results
To quantitatively compare the difference between FLDA and other alternative models including
PCA, CCA, LDA, and “2LDAs”, we measured the proposed metrics from analyses of the synthe-
sized datasets (Figure 2A). Given that the synthesized data were organized in a 2x2 contingency
table, each LDA of the “2LDAs” approach could find only one dimension for the specific feature
i or j . Therefore, as a fair comparison, we only included the corresponding dimensions in FLDA
(FLDi and FLDj ) and the top two components of PCA, CCA, and LDA. The overall SNR values
normalized by that of LDA and the overall modularity scores were plotted for data generated with
different noise levels (Figure 2B,C). The performance of PCA is the worst among all these models
because the unsupervised approach cannot prevent the noise from contaminating the signal. The
supervised approaches in general have good SNRs, but LDA and CCA suffer from low modularity
scores. This is expected because LDA maximizes the separation of cell type clusters but overlooks
the alignment of the axes to the feature i or j , and CCA maximizes the correlation to a linear combi-
nation of phenotypic features instead of individual ones. By contrast, “2LDAs” achieves the highest
modularity scores but has the worst SNR among the supervised approaches, because it tries to max-
imize the separation of cells with different feature levels, which is not necessarily consistent with
maximizing the segregation of cell types. Both the SNR value and the modularity score of FLDA
7
Under review as a conference paper at ICLR 2021
are close to the optimal, as it not only considers the alignment of axes to different features but also
constrains the variance within cell types. A representative plot of the EV and MI metrics of these
models is shown in Figure 5, reporting good alignment of axes to either the feature i or j in FLDA
and ‘2LDAs”, but not in the others.
A question of significance in neurobiology is whether the diverse phenotypes of neuronal cell types
are generated by combinations of modular transcriptional programs, and if so, what is the gene sig-
nature for each of the programs. To illustrate the ability of our approach in addressing this problem,
we applied FLDA to the dataset of Drosophila T4/T5 neurons. The T4/T5 neurons could be orga-
nized in a 2x4 contingency table, therefore, FLDA was able to project the expression data into a
subspace of seven dimensions, with one FLD aligned with dendritic location i (FLDi), three FLDs
aligned with axonal termination j (FLDj 1-3), and the remaining three representing the interaction
of both phenotypes (FLDij 1-3). We ranked these axes based on their SNR metrics and found that
FLDj1, FLDi, and FLDj 2 have much higher SNRs than the rest (Figure 3C). Indeed, data repre-
sentations in the subspace consisting of these three dimensions show a clear separation of the eight
neuronal types (Figure 3D). As expected, FLDi teases apart T4 from T5 neurons, whose dendrites
are located at different brain regions (Figure 3E). Interestingly, FLDj 1 separates T4/T5 neurons into
two groups, a/b vs c/d, corresponding to the upper or lower lobula place, and FLDj 2 divides them
into another two, a/d vs b/c, indicating whether their axons laminate at the middle or lateral part
of the lobula plate (Figure 3E,F). Unexpectedly, among these three dimensions, FLDj 1 has a much
higher SNR than FLDi and FLDj2, whose SNR values are similar. This suggests a hierarchical
structure in the genetic organization of T4/T5 neurons: they are first separated into either a/b or
c/d types, and subsequently divided into each of the eight subtypes. In fact, this exactly matches
the sequence of their cell fate determination during development, as revealed in a previous genetic
study (Pinto-Teixeira et al., 2018). Finally, the last discriminant axis of the axonal feature FLDj3
separates the group a/c from b/d, suggesting its role in fine-tuning the axonal depth within the upper
or lower lobula plate (Figure 3G).
To seek gene signatures for the discriminant components in FLDA, we applied the sparsity-based
regularization to constrain the number of genes with non-zero weight coefficients. Here we set the
number to 20, a reasonable number of candidate genes that might be tested in a follow-up biological
study. We extracted a list of 20 genes each for the axis of FLDi or FLDj 1 . The relative importance
of these genes to each axis is directly informed by their weight values (Figure 4A,C). Side-by-side,
we plotted expression profiles of these genes in the eight neuronal types (Figure 4B,D). For both
axes, the genes critical in separating cells with different feature levels are differentially expressed
in corresponding cell types. We compared our gene lists with those obtained using conventional
methods which were reported in Kurmangaliyev et al. (2019). Consistent with the report, we found
indicator genes for dendritic location such as TfAP -2, dpr2, CG34155, and CG12065, and those
for axonal lamination such as klg, bi, pros. In addition, we found genes that were not reported in
this previous study. For example, our results suggest that the genes Thor and pHCl -1 are impor-
tant to the dendritic phenotype, and Lac and Mip are critical to the axonal phenotype. These are
promising genetic targets to be tested in biological experiments. Lastly, FLDA allowed us to exam-
ine the component that depends on the interaction of both features and identify its gene signature,
which provides clues to transcriptional regulation of gene expressions in the T4/T5 neuronal types
(Figures 6 and 7).
As a supervised approach, FLDA depends on correct phenotype labels to extract meaningful in-
formation. But if the phenotypes are annotated incorrectly, can we use FLDA to raise a flag? We
propose a perturbation analysis of FLDA to address this question built on the assumption that among
possible phenotype annotations, the projection of gene expression data based on correct labels leads
to better metric measurements than incorrect ones. As detailed in Appendix F, we generated three
kinds of incorrect labels for the dataset of T4/T5 neurons, corresponding to three common scenarios
of mislabeling: the phenotypes of a cell type were mislabeled with those of another type; a singular
phenotypic level was incorrectly split into two; two phenotypic levels are incorrectly merged into
one. FLDA was applied to gene expressions of T4/T5 neurons but with these perturbed annotations.
Proposed metrics such as the SNR value and modularity score were plotted in Figure 8. Indeed, the
projection of gene expressions with correct annotation leads to the best SNR value and modularity
score compared with incorrect annotations. This implies that this type of perturbation analysis is a
useful practice in general: it raises the confidence that the original annotation is correct if FLDA on
the perturbed annotations produces lower metric scores.
8
Under review as a conference paper at ICLR 2021
Selected genes for the dendritic phenotype
Selected genes for the axonal phenotype
Figure 4: Critical genes extracted from the sparse algorithm. (A) Weight vector of the 20 genes
selected for the dendritic phenotype (FLDi). The weight value is indicated in the color bar with color
indicating direction (red: positive and green: negative) and saturation indicating magnitude. (B)
Expression patterns of the 20 genes from (A) in eight types of T4/T5 neurons. Dot size indicates the
percentage of cells in which the gene was expressed, and color represents average scaled expression.
(C) Weight vector of the 20 genes selected for the axonal phenotype (FLDj ι). Legend as in (A). (D)
Expression patterns of the 20 genes from (C) in eight types of T4/T5 neurons. Legend as in (B).
7	DISCUSSION
We developed FLDA, a novel supervised linear dimensionality reduction method for understanding
the relationship between high-dimensional gene expression patterns and cellular phenotypes. We
illustrate the power ofFLDA by analyzing a gene expression dataset from Drosophila T4/T5 neurons
that are labeled by two phenotypic features, each with multiple levels. The approach allowed us to
identify new genes for each of the phenotypic features that were not apparent under conventional
methods. Furthermore, we found a hierarchical relationship in the genetic organization of these
cells. These findings point the way for new biological experiments.
The approach is motivated by multi-way ANOVA, and thus it generalizes easily to more than two
features. Future applications in neurobiology include the analysis of phenotypic characteristics such
as electrophysiology and connectivity (Zeng & Sanes, 2017; Gouwens et al., 2019; 2020). More
generally FLDA can be applied to any labeled data set for which the labels form a Cartesian product
of multiple features. For example, this would include face images that can be jointly labeled by the
age, gender, and other features of a person (Moghaddam & Ming-Hsuan Yang, 2002; Zhang et al.,
2017).
FLDA factorizes gene expression data into features and their interactions, and finds a linear pro-
jection of the data that varies with only one factor but not the others. This provides a modular
representation aligned with the factors (Bengio et al., 2012). Ridgeway & Mozer (2018) argued
that modularity together with explicitness could define disentangled representations. Our approach
is linear, which presents an explicit mapping between gene expressions and phenotypic features,
therefore our approach can potentially serve as a supervised approach to disentanglement (Kingma
et al., 2014; Kulkarni et al., 2015; Karaletsos et al., 2016).
Compared with other non-linear embedding methods for cell types (Hinton & Salakhutdinov, 2006;
Feng et al., 2014; Gala et al., 2019), the linear nature of FLDA makes it extremely easy to interpret
the low-dimensional representations, as the weight vector directly informs the relative importance
of each gene. To allow the selection ofa small set of critical genes, we leveraged our approach with
sparse regularization. This makes FLDA especially useful to experimentalists who can take the list
of genes and test them in a subsequent round of experiments.
9
Under review as a conference paper at ICLR 2021
8 Appendix
8.1	A. Objective functions
Here we derive the objective functions used in our analysis. Again if xijk(k ∈ 1, 2, ...nij) represents
the expression values of g genes in each cell (xijk ∈ Rg)), we seek to find a linear projection
yijk = uT xijk that is aligned with the feature i.
8.1.1	Inspiration from ANOVA
We asked what is the best way to factorize yijk. Inspired by multi-way ANOVA (Fisher, 1918), we
identified three components: one depending on the feature i, another depending on the feature j ,
and the last one depending on the interaction of both features. We therefore followed the procedures
of ANOVA to partition sums of squares and factorize yijk into these three components.
Let us first assume that all cell types defined by i and j contain the same number of cells. With cell
types represented by a complete contingency table (Figure 1A), yijk can be linearly factorized using
the model of two crossed factors. Formally, the linear factorization is the following:
yijk = μ + αi + βj + (αβ)ij + Eijk
(15)
where yjk represents the coordinate of the kth cell in the category defined by i and j; μ is the
average level of y; αi is the component that depends on the feature i, and βj is the component that
depends on the feature j; (αβ)ij describes the component that depends on the interaction of both
features i and j; Eijk 〜N(0, σ2) is the residual of this factorization.
Let us say that the features i and j fall into a and b discrete categories respectively. Then without
loss of generality, we can require:
a
X αi = 0
i=1
b
X βj = 0
j=1
ab
X(αβ)ij = X(αβ)ij = 0
i=1	j=1
(16)
(17)
(18)
Corresponding to these, there are three null hypotheses:
H01 : αi = 0
(19)
H02 : βj = 0
(20)
H03 : (αβ)ij = 0
(21)
Here we want to reject H01 while accepting H02 and H03 in order that yijk is aligned with the
feature i.
Next, we partition the total sum of squares. If the number of cells within each cell type category is
n, and the total number of cells is N, then we have
10
Under review as a conference paper at ICLR 2021
abn	a	b
XXX(yijk - y...)2 = bnX(yi..- y...)2 + anX(y.j.- y...)2
i=1 j=1 k=1	i=1	j=1
ab	abn
+n E £(% - y“ - y.j.+y...)2 + ∑∑ EMk- %)2
i=1 j=1	i=1 j=1 k=1
(22)
where y is the average of yj over the indices indicated by the dots. Equation (22) can be written as
SST = SSA + SSB + SSAB + SSe
(23)
with each term having degrees of freedom N - 1, a- 1, b- 1, (a- 1)(b- 1), and N - ab respectively.
Here SSA, SSB, SSAB, and SSe are partitioned sum of squares for the factors αi, βj, (αβ)ij, and
the residual.
ANOVA rejects or accepts a null hypothesis by comparing its mean square (the partitioned sum of
squares normalized by the degree of freedom) to that of the residual. This is done by constructing
F-statistics for each factor as shown below:
FA =	MSA	SSA a— 1
	二 MSe =	二 SSe N—ab
FB =	MSB	SSB b—1
	=MSe =	-SSe N — ab
MSab
^MST
SSAB
(a—1)(b-I)
-~SSe-
N-ab
FAB
(24)
(25)
(26)
Under the null hypotheses, the F-statistics follow the F-distribution. Therefore, a null hypothesis
is rejected when we observe the value of a F-statistic above a certain threshold calculated from the
F-distribution. Here we want FA to be large enough so that we can reject H01, but FB and FAB
to be small enough for us to accept H02 and H03 . In other words, we want to maximize FA while
minimizing FB and FAB . Therefore, we propose maximizing an objective L:
L = FA - λ1FB - λ2FAB
(27)
where λ1 and λ2 are hyper-parameters determining the relative weights of FB and FAB compared
with FA .
8.1.2 Objective functions under a complete contingency table
When the numbers of cells within categories defined by i and j (nij ) are not all the same, the total
sum of squares cannot be partitioned as in Equation (22). However, if we only care about distinctions
between cell types instead of individual cells, we can use the mean value of each cell type cluster
(yij.) to estimate the overall average value (y...), and the average value of each category i (yi..) or j
(y.j.). Therefore, Equation (22) can be modified as the following:
a b	nij	a	b
XX[nL X(yjk - y...)2] = b X(y “- y...)2+a Xj y...)2
i=1 j=1 nij k=1	i=1	j=1
ab
+ XXg.
i=1 j=1
a b 1 nij
-yi.. - y.j. + y...)2 + XX[ - X(yijk - yij.)2]
i=1 j=1 ij k=1
(28)
11
Under review as a conference paper at ICLR 2021
where	=Pn=I yijk	(29) nij 〜	Pj = I yij.	cc∖ yi.. = -ɪ——	(30) b Pa y.j. = Ti=I yij.	(31) Pa Pb y y... = Mi=I j %	(32) ab
If we describe Equation (28) as:
SSt = SSa + SS B + SS AB + SS e	(33)
then following the same arguments, we want to maximize an objective function in the following
format:
L	~ ~ — SSA _ λ, SSB _ λc	SSAB =5 - λ1 L - λ2 (a-1)(b-1)	(34) SSe N-ab
8.1.3 Objective functions under a partial contingency table
When we have a representation of a partial table, we can no longer separate out the component that
depends on the interaction of both features. Therefore, we use another model, a linear model of two
nested factors, to factorize yijk , which has the following format:
	yijk = μ + αi + βj(i) + Gjk	(35)
Note that we now have βj(i) instead of βj + (αβ)ij. In this model, we identify a primary factor, for
instance, the feature denoted by i which falls into a categories, and the other (indexed by j ) becomes
a secondary factor, the number of whose levels bi depends on the level of the primary factor. We
merge the component depending on the interaction of both features into that of the secondary factor
as βj(i) .
Similarly, we have
a bi	nij	a bi
XX[nL X(yijk-y...)2] = XX®-y...)2]
i=1 j=1 ij k=1	i=1 j=1	(36) a	bi	a	bi	nij + XX(yij. - yi..)2 + XX[羡 X(yijk - yij.)2] i=1 j=1	i=1 j=1 ij k=1
which can be written as
	SSt = SSa + SS B + SS e	(37)
with degrees of freedom N - 1, a - 1, M - a, and N - M for each of the terms, where M is:
12
Under review as a conference paper at ICLR 2021
a
M = X bi	(38)
Therefore, we want to maximize the following objective:


~ ~
SSA _ λ SSB
a _ 0-ι -λ M-a
L
SSe
N-M
(39)
8.2 B. FLDA with a partial contingency table
Here we provide the mathematical details of FLDA under the representation of a partial table. When
we have a partial table, if the feature i is the primary feature with a levels, and the feature j is the
secondary feature with bi levels, then NA in Equation (1) is defined as follows:
NA — MA - AMb∣a
(40)
where
MA
1a	bi
1——1∑2∑2(mj. - m..)(mi. - m..)T
a -	i=1 j =1
(41)
1a	bi
Mb|A — 77——∑2∑2(mj - m∙i.)(mj - mi.)T	(42)
M-a
i=1 j=1
and M is defined as in Equation (38). Correspondingly, Me in Equation (1) is defined as:
a	b	nij
Me - N-M E ∑[ nj EM%-mij )(xijk - mij	)T]	(43)
i=1 j=1 ij k=1
and
1a	bi
m.. — M XX mij	(44)
i=1	j=1
1	bi
mi.— E X mij
(45)
The remaining mathematical arguments are the same as those for the complete table. In this scenario,
because we don’t observe all possible combinations of features i and j, we cannot find the linear
projection for the interaction of both features.
8.3	C. Pseudo-code for Rifle
We show pseudo-code for the Rifle algorithm as follows:
13
Under review as a conference paper at ICLR 2021
procedure RIFLE(NA , Me , u0 , l, η)
t=1
. η is the step size
. t indicates the iteration number
while not converge do	. Converge when ut ' ut-1
UT-INA ut-1
pt-1	UT-ιMeUt-1
C J I + (ρt-1)(NA - Pt-IMe)
u JCut-I
ut — l∣Cut-ιl∣2
Truncate ut by keeping the top l entries of u with the largest values and setting the rest
entries to 0
u ut	Ut
Ut jIIutII
tJt+1
2
end while
return ut
end procedure
8.4	D. Implementation details of data synthesis
To quantitatively compare FLDA with alternative approaches, we synthesized data of four cell types,
each of which contained 25 cells. The four cell types were generated from a Cartesian product of
two features i and j, where i ∈ {0, 1} and j ∈ {0, 1}. Expressions of 10 genes were generated for
each cell. The expression value of the hth gene in the kth cell of the cell type ij , gihjk was defined
as the following:
gijk = i + ijk
(46)
gi2jk = j + ijk	(47)
	gi3jk = i ∧ j + ijk	(48)
	gi4jk = i ∨ j + ijk	(49)
	gi5jk = 2i + ijk	(50)
	gi6jk = 2j + ijk	(51)
	gijk = 2i ∧ j + ijk	(52)
	gijk = 2i ∨ j + ijk	(53)
	gi9jk = ijk	(54)
	gijk = 2 + ijk	(55)
where		
	1 if i = 1 j = 1	
	i∧j=	,	, 0, otherwise	(56)
14
Under review as a conference paper at ICLR 2021
and
i∨j
0, if i = 0, j = 0
1, otherwise
(57)
were interactions of the two features. Here ijk was driven by Gaussian noise, namely,
Qjk 〜N(0,σ2)	(58)
We synthesized datasets of 5 different σ values (σ ∈ {0.2, 0.4, 0.6, 0.8, 1.0}). This was repeated 10
times and metrics for each σ value were calculated as the average across the 10 repeats.
8.5 E. Implementation details of the metrics used in the study
We measured the following metrics in our experiments:
8.5.1	Signal-to-Noise Ratio (SNR)
Because we care about the separation of cell types, we define the SNR metric as the ratio of the
variance between cell types over the variance of the noise, which is estimated from within-cluster
variance. For the entire embedding space, given q cell types, if the coordinate of each cell is indicated
by c, then we define the overall SNR metric as the following:
SN Roverall
tr3q=inp(cp. - c..)(cp. - c..)τA
trNp=Rk= I(Cpk- Cp)(Cpk- cp.)T)
(59)
where Cp. is the center of each cell type cluster, and C.. is the center of all data points.
Let c denote the embedded coordinate along a specific dimension. The SNR metric for that axis is
therefore:
SNR
£p=inp(I⅝. - J)2
∑p=ι∑n=ι(Cpk- Cpy2
(60)
8.5.2 Explained Variance (EV)
We want to know whether the variation of a specific dimension is strongly explained by that of a
specific feature. Therefore, we measure, for each axis, how much of the total explained variance is
explained by the variance of the feature i or j. Formally, given the embedded coordinate yijk, we
calculate the EV as the following:
EVi
Pa=I Pb=I nij(yi.. - y...)2
Pa=I Pj=I Pjyejk- y...)2
Pa=I Pj=I nij(y.j. - y...)2
Pia=1 Pjb=1 Pkn=ij1(yijk -yC...)2
(61)
(62)
where yC is the average of yijk over the indices indicated by the dots.
8.5.3 Mutual Information (MI)
The MI between a discriminant axis u and a feature quantifies how much information of the feature
is obtained by observing data projected along that axis. It is calculated as the MI between data
representations along the axis y = uTX and feature labels of the data f, where X is the original
gene expression matrix:
15
Under review as a conference paper at ICLR 2021
I(y,f)=H(y)+H(f)-H(y,f)
= -	p(y) log2 p(y) -	p(f) log2 p(f) -ΣΣp(y, f) log2 p(y, f)
y∈Y	f∈F	y∈Y f∈F
(63)
Here H indicates entropy. To calculate H(y) and H(y, f), we discretize y into 10 bins.
8.5.4 Modularity
Ridgeway & Mozer (2018) argued that in a modular representation, each axis should depend on at
most a single feature. Following the arguments in their paper, the modularity score is computed as
follows: we first calculate the MI between each feature and each axis (mif denotes the MI between
one axis i and one feature f). If an axis is perfectly modular, it will have high mutual information
for only one feature and zeros for the others, we therefore compute a template tif as the following:
t = θi, iff = arg maxg mig
if 0, otherwise
where θi = maxg mig . We then calculate the deviation from the template as:
δ _ Pf (mif - tif )2
δi=	θ2(N - 1)
(64)
(65)
where N is the number of features. The modularity score for the axis i is 1 - δi. The mean of 1 - δi
over i is defined as the overall modularity score.
8.6 F. Implementation details of annotation perturbation
To evaluate the effect of mislabeling phenotypic levels, we made use of the dataset of T4/T5 neurons,
and generated three kinds of perturbation to the original labels:
First, we switched the phenotype labels of T4a neurons with one of the seven other types (T4b,
T4c, T4d, T5a, T5b, T5c, T5d). In this scenario, phenotype labels of two cell types were incorrect,
but the number of cell type clusters was the same. We had two levels of the dendritic phenotypes
(T4/T5), and four levels of the axonal phenotypes (a/b/c/d). Therefore we kept one dimension for
the dendritic feature, and three dimensions for the axonal feature.
Second, we merged the axonal phenotypic level a with another level (b/c/d), as an incorrect new level
(a+b/a+c/a+d). In this scenario, we had three axonal phenotypes, therefore we kept two dimensions
for the axonal feature.
Third, we randomly split each of the four axonal lamination labels (a/b/c/d) into two levels. For
instance, among neurons with the original axonal level a, some of them were labeled with a level
a1, and the others were labeled with a level a2. In this scenario, we had eight axonal phenotypes
(a1/a2/b1/b2/c1/c2/d1/d2), and we kept seven dimensions for the axonal feature.
We performed FLDA on the dataset of T4/T5 neurons but with these perturbed annotations. Metrics
from each of the perturbed annotations were measured and compared with those from the original
annotation.
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New
Perspectives. arXiv:1206.5538 [cs], June 2012.
Hung-I. Harry Chen, Yufang Jin, Yufei Huang, and Yidong Chen. Detection of high variability in
gene expression from single-cell RNA-seq profiling. BMC Genomics, 17 Suppl 7:508, August
2016. ISSN 1471-2164. doi: 10.1186/s12864-016-2897-6.
16
Under review as a conference paper at ICLR 2021
John P. Cunningham and Zoubin Ghahramani. Linear Dimensionality Reduction: Survey, Insights,
and Generalizations. Journal of Machine Learning Research, 16(89):2859-2900, 2015. ISSN
1533-7928.
Thomas Euler, Silke Haverkamp, Timm Schubert, and Tom Baden. Retinal bipolar cells: Elementary
building blocks of vision. Nature Reviews Neuroscience, 15(8):507-519, August 2014. ISSN
1471-0048. doi: 10.1038/nrn3783.
Fangxiang Feng, Xiaojie Wang, and Ruifan Li. Cross-modal Retrieval with Correspondence Au-
toencoder. In Proceedings of the 22nd ACM International Conference on Multimedia, MM ’14,
pp. 7-16, Orlando, Florida, USA, November 2014. Association for Computing Machinery. ISBN
978-1-4503-3063-3. doi: 10.1145/2647868.2654902.
R. A. Fisher. The Use of Multiple Measurements in Taxonomic Problems. Annals of Eugenics, 7
(2):179-188, 1936. ISSN 2050-1439. doi: 10.1111/j.1469-1809.1936.tb02137.x.
Ronald Aylmer Fisher. The Correlation between Relatives on the Supposition of Mendelian Inheri-
tance. Royal Society of Edinburgh], 1918.
Rohan Gala, Nathan Gouwens, Zizhen Yao, Agata Budzillo, Osnat Penn, Bosiljka Tasic, Gabe Mur-
phy, Hongkui Zeng, and Uygar Sumbul. A coupled autoencoder approach for multi-modal analy-
sis of cell types. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle AIche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 9267-
9276. Curran Associates, Inc., 2019.
Benyamin Ghojogh, Fakhri Karray, and Mark Crowley. Eigenvalue and Generalized Eigenvalue
Problems: Tutorial. arXiv:1903.11240 [cs, stat], March 2019. Comment: 8 pages, Tutorial
paper.
Nathan W. Gouwens, Staci A. Sorensen, Jim Berg, Changkyu Lee, Tim Jarsky, Jonathan Ting, Su-
san M. Sunkin, David Feng, Costas A. Anastassiou, Eliza Barkan, Kris Bickley, Nicole Ble-
sie, Thomas Braun, Krissy Brouner, Agata Budzillo, Shiella Caldejon, Tamara Casper, Dan
Castelli, Peter Chong, Kirsten Crichton, Christine Cuhaciyan, Tanya L. Daigle, Rachel Dalley,
Nick Dee, Tsega Desta, Song-Lin Ding, Samuel Dingman, Alyse Doperalski, Nadezhda Dotson,
Tom Egdorf, Michael Fisher, Rebecca A. de Frates, Emma Garren, Marissa Garwood, Amanda
Gary, Nathalie Gaudreault, Keith Godfrey, Melissa Gorham, Hong Gu, Caroline Habel, Kristen
Hadley, James Harrington, Julie A. Harris, Alex Henry, DiJon Hill, Sam Josephsen, Sara Kebede,
Lisa Kim, Matthew Kroll, Brian Lee, Tracy Lemon, Katherine E. Link, Xiaoxiao Liu, Brian
Long, Rusty Mann, Medea McGraw, Stefan Mihalas, Alice Mukora, Gabe J. Murphy, Lind-
say Ng, Kiet Ngo, Thuc Nghi Nguyen, Philip R. Nicovich, Aaron Oldre, Daniel Park, Sheana
Parry, Jed Perkins, Lydia Potekhina, David Reid, Miranda Robertson, David Sandman, Martin
Schroedter, Cliff Slaughterbeck, Gilberto Soler-Llavina, Josef Sulc, Aaron Szafer, Bosiljka Tasic,
Naz Taskin, Corinne Teeter, Nivretta Thatra, Herman Tung, Wayne Wakeman, Grace Williams,
Rob Young, Zhi Zhou, Colin Farrell, Hanchuan Peng, Michael J. Hawrylycz, Ed Lein, Lydia Ng,
Anton Arkhipov, Amy Bernard, John W. Phillips, Hongkui Zeng, and Christof Koch. Classifi-
cation of electrophysiological and morphological neuron types in the mouse visual cortex. Nat.
Neurosci., 22(7):1182-1195, July 2019. ISSN 1546-1726. doi: 10.1038/s41593-019-0417-0.
Nathan W. Gouwens, Staci A. Sorensen, Fahimeh Baftizadeh, Agata Budzillo, Brian R. Lee, Tim
Jarsky, Lauren Alfiler, Katherine Baker, Eliza Barkan, Kyla Berry, Darren Bertagnolli, Kris Bick-
ley, Jasmine Bomben, Thomas Braun, Krissy Brouner, Tamara Casper, Kirsten Crichton, Tanya L.
Daigle, Rachel Dalley, Rebecca A. de Frates, Nick Dee, Tsega Desta, Samuel Dingman Lee,
Nadezhda Dotson, Tom Egdorf, Lauren Ellingwood, Rachel Enstrom, Luke Esposito, Colin Far-
rell, David Feng, Olivia Fong, Rohan Gala, Clare Gamlin, Amanda Gary, Alexandra Glandon,
Jeff Goldy, Melissa Gorham, Lucas Graybuck, Hong Gu, Kristen Hadley, Michael J. Hawrylycz,
Alex M. Henry, DiJon Hill, Madie Hupp, Sara Kebede, Tae Kyung Kim, Lisa Kim, Matthew
Kroll, Changkyu Lee, Katherine E. Link, Matthew Mallory, Rusty Mann, Michelle Maxwell,
Medea McGraw, Delissa McMillen, Alice Mukora, Lindsay Ng, Lydia Ng, Kiet Ngo, Philip R.
Nicovich, Aaron Oldre, Daniel Park, Hanchuan Peng, Osnat Penn, Thanh Pham, Alice Pom, Zo-
ran Popovic, Lydia Potekhina, Ramkumar Rajanbabu, Shea Ransford, David Reid, Christine Ri-
morin, Miranda Robertson, Kara Ronellenfitch, Augustin Ruiz, David Sandman, Kimberly Smith,
17
Under review as a conference paper at ICLR 2021
Josef Sulc, Susan M. Sunkin, Aaron Szafer, Michael Tieu, Amy Torkelson, Jessica Trinh, Her-
man Tung, Wayne Wakeman, Katelyn Ward, Grace Williams, Zhi Zhou, Jonathan T. Ting, Anton
Arkhipov, Uygar Sumbul, Ed S. Lein, Christof Koch, Zizhen Yao, Bosiljka Tasic, Jim Berg,
Gabe J. Murphy, and Hongkui Zeng. Integrated Morphoelectric and Transcriptomic Classifica-
tionof CorticalGABAergicCells. Cell,183(4):935-953.e19,November2020.ISSN0092-8674.
doi: 10.1016/j.cell.2020.09.057.
Xiaofei He and Partha Niyogi. Locality Preserving Projections. pp. 8.
G. E. Hinton and R. R. Salakhutdinov. Reducing the Dimensionality of Data with Neural Networks.
Science, 313(5786):504-507, July 2006. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.
1127647.
Harold Hotelling. RELATIONS BETWEEN TWO SETS OF VARIATES. Biometrika, 28(3-4):
321-377, December 1936. ISSN 0006-3444. doi: 10.1093/biomet/28.3-4.321.
Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. Wiley-
Interscience, New York, 1st edition edition, May 2001. ISBN 978-0-471-40540-5.
I.	T. Jolliffe. Principal Component Analysis. Springer Series in Statistics. Springer-Verlag, New
York, second edition, 2002. ISBN 978-0-387-95442-4.
Theofanis Karaletsos, Serge Belongie, and Gunnar Ratsch. Bayesian representation learning with
oracle constraints. arXiv:1506.05011 [cs, stat], March 2016. Comment: 16 pages, publishes in
ICLR 16.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
Learning with Deep Generative Models. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems
27, pp. 3581-3589. Curran Associates, Inc., 2014.
Tejas D Kulkarni, William F. Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep Convolu-
tional Inverse Graphics Network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2539-2547. Curran
Associates, Inc., 2015.
Yerbol Z Kurmangaliyev, Juyoun Yoo, Samuel A LoCascio, and S Lawrence Zipursky. Modu-
lar transcriptional programs separately define axon and dendrite connectivity. eLife, 8:e50822,
November 2019. ISSN 2050-084X. doi: 10.7554/eLife.50822.
Rasmus Larsen. Decomposition using maximum autocorrelation factors. Journal of Chemometrics,
16(8-10):427-435, 2002. ISSN 1099-128X. doi: 10.1002/cem.743.
Evan Z. Macosko, Anindita Basu, Rahul Satija, James Nemesh, Karthik Shekhar, Melissa Gold-
man, Itay Tirosh, Allison R. Bialas, Nolan Kamitaki, Emily M. Martersteck, John J. Trombetta,
David A. Weitz, Joshua R. Sanes, Alex K. Shalek, Aviv Regev, and Steven A. McCarroll. Highly
parallel genome-wide expression profiling of individual cells using nanoliter droplets. Cell, 161
(5):1202-1214, May 2015. ISSN 0092-8674. doi: 10.1016/j.cell.2015.05.002.
Geoffrey McLachlan. Discriminant Analysis and Statistical Pattern Recognition.	Wiley-
Interscience, Hoboken, N.J, August 2004. ISBN 978-0-471-69115-0.
B.	Moghaddam and Ming-Hsuan Yang. Learning gender with support faces. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 24(5):707-711, May 2002. ISSN 1939-3539. doi:
10.1109/34.1000244.
Seung Wook Oh, Julie A. Harris, Lydia Ng, Brent Winslow, Nicholas Cain, Stefan Mihalas,
Quanxin Wang, Chris Lau, Leonard Kuan, Alex M. Henry, Marty T. Mortrud, Benjamin Ouel-
lette, Thuc Nghi Nguyen, Staci A. Sorensen, Clifford R. Slaughterbeck, Wayne Wakeman, Yang
Li, David Feng, Anh Ho, Eric Nicholas, Karla E. Hirokawa, Phillip Bohn, Kevin M. Joines,
Hanchuan Peng, Michael J. Hawrylycz, John W. Phillips, John G. Hohmann, Paul Wohnoutka,
Charles R. Gerfen, Christof Koch, Amy Bernard, Chinh Dang, Allan R. Jones, and Hongkui
Zeng. A mesoscale connectome of the mouse brain. Nature, 508(7495):207-214, April 2014.
ISSN 1476-4687. doi: 10.1038/nature13186.
18
Under review as a conference paper at ICLR 2021
Shristi Pandey, Karthik Shekhar, Aviv Regev, and Alexander F. Schier. Comprehensive Identification
and Spatial Mapping of Habenular Neuronal Types Using Single-Cell RNA-Seq. Curr. Biol., 28
(7):1052-1065.e7,April2018. ISSN 1879-0445. doi: 10.1016∕j.cub.2018.02.040.
Yi-Rong Peng, Karthik Shekhar, Wenjun Yan, Dustin Herrmann, Anna Sappington, Gregory S.
Bryman, Tave van Zyl, Michael Tri. H. Do, Aviv Regev, and Joshua R. Sanes. Molecular Classi-
fication and Comparative Taxonomics of Foveal and Peripheral Cells in Primate Retina. Cell, 176
(5):1222-1237.e22, February 2019. ISSN 0092-8674. doi: 10.1016∕j.cell.2019.01.004.
Filipe Pinto-Teixeira, Clara Koo, Anthony Michael Rossi, Nathalie Neriec, Claire Bertet, Xin Li, Al-
berto Del-Valle-Rodriguez, and Claude Desplan. Development of Concurrent Retinotopic Maps
in the Fly Motion Detection Circuit. Cell, 173(2):485^98.e11, April 2018. ISSN 0092-8674.
doi: 10.1016/j.cell.2018.02.053.
Karl Ridgeway and Michael C Mozer. Learning Deep Disentangled Embeddings With the F-Statistic
Loss. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 31, pp. 185-194. Curran Associates,
Inc., 2018.
Joshua R. Sanes and Richard H. Masland. The types of retinal ganglion cells: Current status and
implications for neuronal classification. Annu. Rev. Neurosci., 38:221-246, July 2015. ISSN
1545-4126. doi: 10.1146/annurev-neuro-071714-034120.
Karthik Shekhar, Sylvain W. Lapan, Irene E. Whitney, Nicholas M. Tran, Evan Z. Macosko, Monika
Kowalczyk, Xian Adiconis, Joshua Z. Levin, James Nemesh, Melissa Goldman, Steven A. Mc-
Carroll, Constance L. Cepko, Aviv Regev, and Joshua R. Sanes. COMPREHENSIVE CLASSI-
FICATION OF RETINAL BIPOLAR NEURONS BY SINGLE-CELL TRANSCRIPTOMICS.
Cell, 166(5):1308-1323.e30, August 2016. ISSN 0092-8674. doi: 10.1016/j.cell.2016.07.054.
C.	Spearman. ”General Intelligence,” Objectively Determined and Measured. The American Journal
of Psychology, 15(2):201-292, 1904. ISSN 0002-9556. doi: 10.2307/1412107.
Kean Ming Tan, Zhaoran Wang, Han Liu, and Tong Zhang. Sparse generalized eigenvalue problem:
Optimal statistical rates via truncated Rayleigh flow. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 80(5):1057-1086, 2018. ISSN 1467-9868. doi: 10.1111/rssb.
12291.
Bosiljka Tasic, Vilas Menon, Thuc Nghi Nguyen, Tae Kyung Kim, Tim Jarsky, Zizhen Yao, Boaz
Levi, Lucas T. Gray, Staci A. Sorensen, Tim Dolbeare, Darren Bertagnolli, Jeff Goldy, Nadiya
Shapovalova, Sheana Parry, Changkyu Lee, Kimberly Smith, Amy Bernard, Linda Madisen, Su-
san M. Sunkin, Michael Hawrylycz, Christof Koch, and Hongkui Zeng. Adult mouse cortical cell
taxonomy revealed by single cell transcriptomics. Nat. Neurosci., 19(2):335-346, February 2016.
ISSN 1546-1726. doi: 10.1038/nn.4216.
Bosiljka Tasic, Zizhen Yao, Lucas T. Graybuck, Kimberly A. Smith, Thuc Nghi Nguyen, Darren
Bertagnolli, Jeff Goldy, Emma Garren, Michael N. Economo, Sarada Viswanathan, Osnat Penn,
Trygve Bakken, Vilas Menon, Jeremy Miller, Olivia Fong, Karla E. Hirokawa, Kanan Lathia,
Christine Rimorin, Michael Tieu, Rachael Larsen, Tamara Casper, Eliza Barkan, Matthew Kroll,
Sheana Parry, Nadiya V. Shapovalova, Daniel Hirschstein, Julie Pendergraft, Heather A. Sullivan,
Tae Kyung Kim, Aaron Szafer, Nick Dee, Peter Groblewski, Ian Wickersham, Ali Cetin, Julie A.
Harris, Boaz P. Levi, Susan M. Sunkin, Linda Madisen, Tanya L. Daigle, Loren Looger, Amy
Bernard, John Phillips, Ed Lein, Michael Hawrylycz, Karel Svoboda, Allan R. Jones, Christof
Koch, and Hongkui Zeng. Shared and distinct transcriptomic cell types across neocortical areas.
Nature, 563(7729):72-78, November 2018. ISSN 1476-4687. doi: 10.1038/s41586-018-0654-5.
Nicholas M. Tran, Karthik Shekhar, Irene E. Whitney, Anne Jacobi, Inbal Benhar, Guosong Hong,
Wenjun Yan, Xian Adiconis, McKinzie E. Arnold, Jung Min Lee, Joshua Z. Levin, Dingchang
Lin, Chen Wang, Charles M. Lieber, Aviv Regev, Zhigang He, and Joshua R. Sanes. Single-cell
profiles of retinal neurons differing in resilience to injury reveal neuroprotective genes. bioRxiv,
pp. 711762, July 2019. doi: 10.1101/711762.
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On Deep Multi-View Representation
Learning: Objectives and Optimization. arXiv:1602.01024 [cs], February 2016.
19
Under review as a conference paper at ICLR 2021
Laurenz Wiskott and Terrence J. Sejnowski. Slow Feature Analysis: Unsupervised Learning of
Invariances. Neural Computation, 14(4):715-770, APrn 2002. ISSN0899-7667. doi: 10.1162/
089976602317318938.
Hongkui Zeng and Joshua R. Sanes. Neuronal cell-tyPe classification: Challenges, oPPortunities
and the Path forward. Nature Reviews Neuroscience, 18(9):530-546, SePtember 2017. ISSN
1471-0048. doi: 10.1038/nrn.2017.85.
Zhifei Zhang, Yang Song, and Hairong Qi. Age Progression/Regression by Conditional Adversarial
Autoencoder. arXiv:1702.08423 [cs], March 2017. Comment: Accepted by The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR 2017).
9 Additional Information
Figure 5: Representative plots (at σ = 0.6) ofEV and MI metrics for FLDA and other models. (A,B)
EV (A) and MI (B) metrics of FLDA. FLDi and FLDj indicate the factorized linear discriminants
for features i and j. (C,D) EV (C) and MI (D) metrics of 2LDAs. LDi and LDj indicate the linear
discriminant components for features i and j. (E,F) EV (E) and MI (F) metrics of LDA. LDι and
LD2 indicate the first two linear discriminant components. (G,H) EV (G) and MI (H) metrics of
CCA. CCA1 and CCA2 indicate the first two canonical correlation axes. (I,J) EV (I) and MI (J)
metrics of PCA. PCi and PC2 indicate the first two principal components. EVi and EVj are the
explained variance of features i and j along an axis, and MIi and MIj indicate the mutual inform
between an axis and features i and j respectively. Values of EV and MI metrics are also indicated
by the color bars on the right side.
20
Under review as a conference paper at ICLR 2021
B
A
FLDJj-I
FLDJLZ
Figure 6: Additional plots for FLDA on the dataset of T4/T5 neurons. (A,B) Projection of the orig-
inal gene expression data into the two-dimensional space made of the first and second (FLDij 1 and
FLDij2) (A) or the second and third (FLDij2 and FLDij3) (B) discriminant axes for the compo-
nent that depends on the combination of both features i and j. Different cell types are indicated in
different colors as in (B).
Figure 7: Additional plots for critical genes extracted from the sparse algorithm. (A) Weight vector
of the 20 genes selected for the interaction of both dendritic and axonal features (FLDij 1). The
weight value is indicated in the color bar with color indicating direction (red: positive and green:
negative) and saturation indicating magnitude. (B) Expression patterns of the 20 genes from (A)
in eight types of T4/T5 neurons. Dot size indicates the percentage of cells in which the gene was
expressed, and color represents average scaled expression.
21
Under review as a conference paper at ICLR 2021
Switch Type T4a with	Merge Types	Split Types
Figure 8: Evaluation of the effect of incorrect phenotype annotation on the dataset of T4/T5 neu-
rons. (A,B) Normalized overall SNR metric (A) and overall modularity score (B) of FLDA after
switching labels of T4a type with another neuronal type. (C,D) Normalized overall SNR metric (C)
and overall modularity score (D) of FLDA after merging the axonal phenotypic level a with another
phenotypic level (b/c/d). (E,F) Normalized overall SNR metric (E) and overall modularity score (F)
OfFLDA after splitting each axonal phenotypic level into two. Metrics under the original annotation
are colored in green, and the values are indicated by the dashed lines. Here the SNR values are
normalized with respect to that of the original annotation.
22