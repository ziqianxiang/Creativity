Under review as a conference paper at ICLR 2021
Rethinking Convolution: Towards an Optimal
Efficiency
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we present our recent research about the computational efficiency in
convolution. Convolution operation is the most critical component in recent surge
of deep learning research. Conventional 2D convolution takes O(C2K2HW) to
calculate, where C is the channel size, K is the kernel size, while H and W are
the output height and width. Such computation has become really costly consid-
ering that these parameters increased over the past few years to meet the needs
of demanding applications. Among various implementations of the convolution,
separable convolution has been proven to be more efficient in reducing the com-
putational demand. For example, depth separable convolution reduces the com-
Plexity to O(CHW ∙ (C + K2)) while spatial separable convolution reduces the
complexity to O(C2KHW). However, these are considered an ad hoc design
which cannot ensure that they can in general achieve optimal separation. In this
research, we propose a novel operator called optimal separable convolution which
can be calculated at O(C3 KHW) by optimal design for the internal number of
groups and kernel sizes for general separable convolutions. When there is no
restriction in the number of separated convolutions, an even lower complexity at
O(CHW ∙log(CK2)) can be achieved. Experimental results demonstrate that the
proposed optimal separable convolution is able to achieve an improved accuracy-
FLOPs and accuracy-#Params trade-offs over both conventional and depth/spatial
separable convolutions.
1	Introduction
Tremendous progresses have been made in recent years towards more accurate image analysis tasks,
such as image classification, with deep convolutional neural networks (DCNNs) (Krizhevsky et al.,
2012; Srivastava et al., 2015; He et al., 2016; Real et al., 2019; Tan & Le, 2019; Dai et al., 2020).
However, the computational complexity for state-of-the-art DCNN models has also become in-
creasingly high and computationally expensive. This can significantly defer their deployment to
real-world applications, such as mobile platforms and robotics, where the resources are highly con-
strained (Howard et al., 2017; Dai et al., 2020). It is very much desired that a DCNN could achieve
better performance with less computation and fewer model parameters.
The most time-consuming building block of a DCNN is the convolutional layer. There have been
many previous works aiming at reducing the amount of computation in the convolutional layer.
Historically, researchers apply Fast Fourier Transform (FFT) (Nussbaumer, 1981; Quarteroni et al.,
2010) to implement convolution and they gain great speed up for large convolutional kernels. For
small convolutional kernels, a direct application is often still cheaper (Podlozhnyuk, 2007). Re-
searchers also explore low rank approximation (Jaderberg et al., 2014; Ioannou et al., 2015) to
implement convolutions. However, most of the existing methods start from a pre-trained model and
mainly focus on network pruning and compression.
In this research, we study how to design a separable convolution to achieve an optimal implemen-
tation in terms of computational complexity. Enabling convolution separable has been proven to
be an efficient way to reduce the computational complexity (Sifre & Mallat, 2014; Howard et al.,
2017; Szegedy et al., 2016). Comparing to the FFT and low rank approximation approaches, a well-
designed separable convolution shall be efficient for both small and large kernel sizes and shall not
require a pre-trained model to operate on.
1
Under review as a conference paper at ICLR 2021
Table 1: A comparison of computational complexity and the number of parameters of the proposed optimal
separable convolution and existing approaches. The proposed optimal separable convolution is much more
efficient. In this table, C represents the channel size of convolution, K is the kernel size, H and W are
the output height and width, g is the number of groups. “Vol. RF” represents whether the corresponding
convolution satisfies the proposed volumetric receptive field condition.
	Conventional Conv2D	Grouped Conv2D	Depth-wise Conv2D	Point-wise Conv2D	Depth Separable Conv2D	Spatial Separable Conv2D	Optimal Separable Conv2D (N = 2)	Optimal Separable Conv2D (Optimized N)
FLOPS	C2K2HW	C2K2HW∕g	CK2HW	C2 HW	CHW (C + K2)	2C2KHW	2C 2 KHW	eCHW log(CK2)
#ParamS	C2K2	C2K2∕g	CK2	C2	C(C + K2)	2C2K	2C 2 K	eC log(CK2)
Vol. RF	✓	X	X	X	✓	✓	✓	✓
Note	-	-	g = C	K = 1	Depth-wise + Point-wise	K2 → 2K	-	e = 2.71828 …
3x3
Figure 1: Volumetric receptive field and the proposed optimal separable convolution. (a) The volumetric recep-
tive field (RF) of a convolution is the Cartesian product of its (spatial) RF and channel RF. (b) Illustrations of
the channel connections for conventional, depth separable, and the proposed optimal separable convolutions.
O(C2K2)
Conv2D
O(C(C + K2))
Depth Separable
(b)
3x3	1x1
3
O(CEK)
Optimal Separable
In the DCNN research, two most well-known separable convolutions are depth separable (Sifre &
Mallat, 2014) and spatial separable (Szegedy et al., 2016) convolutions. Both are able to reduce
the computational complexity of a convolution. The complexity of a conventional 2D convolution
is quadratic with three hyper-parameters: number of channels (C), kernel size (K), and spatial di-
mensions (H or W ), and its computational complexity is actually O(C2K2HW). Depth separable
convolution is constructed as a depth-wise convolution followed by a point-wise convolution, where
depth-wise convolution is a group convolution with its number of groups g = C and point-wise
convolution is a 1 × 1 convolution. Spatial separable convolution replaces a K × K kernel to K × 1
and 1 × K kernels. Different types of convolutions and their computational costs are summarized
in Table 1. From this table, we can easily verify that depth separable convolution has a complexity
of O(CHW ∙ (C + K2)) and spatial separable convolution has a complexity of O(C2KHW).
Both depth separable and spatial separable convolutions follow an ad hoc design. They are able to
reduce the computational cost to some degree but normally will not achieve an optimal separation.
A separable convolution in general has three sets of parameters: the internal number of groups,
channel size, and kernel size of each separated convolution. Instead of setting these parameters in
an ad hoc fashion, we design a scheme to achieve an optimal separation. The resulting separable
convolution is called optimal separable convolution in this research.
To prevent the proposed optimal separable convolution from being degenerated, we assume that the
internal channel size is in an order of O(C) and propose the following volumetric receptive field
condition. As illustrated in Fig. 1a, similar to the receptive field (RF) of a convolution which is
defined as the region in the input space that a particular CNN’s feature is looking at (or affected
by) (Lindeberg, 2013), we define the volumetric RF of a convolution to be the volume in the input
space that affects CNN’s output. The volumetric RF condition requires that a properly decomposed
separable convolution maintains the same volumetric RF as the original convolution before decom-
position. Hence, the proposed optimal separable convolution will be equivalent to optimizing the
internal number of groups and kernel sizes to achieve the computational objective (measured in
FLOPs1) while satisfying the volumetric RF condition. Formally, the objective function is defined
by Equation (2) under the constraints defined by Equations (3)-(6). The solution to this optimization
problem will be described in detail in Section 2.
1In this research, similar to (He et al., 2016), FLOPs are measured in number of multiply-adds.
2
Under review as a conference paper at ICLR 2021
We shall show that the proposed optimal separable convolution can be calculated at the order of
O(C3 KHW). This is at least a factor of √C more efficient than the depth separable and spatial
separable convolutions. The proposed optimal separable convolution is able to be easily generalized
into an N -separable case, where the number of separated convolutions N can be optimized further.
In such a generalized case, an even lower complexity at O(CHW ∙ log(CK2)) can be achieved.
Extensive experiments are carried out to demonstrate the effectiveness of the proposed optimal sep-
arable convolution. As illustrated in Fig. 3, on the CIFAR10 dataset (Krizhevsky et al., 2009), the
proposed optimal separable convolution achieves a better Pareto-frontier2 than both conventional
and depth separable convolutions using the ResNet (He et al., 2016) architecture. To demonstrate
that the proposed optimal separable convolution generalizes well to other DCNN architectures, we
adopt the DARTS (Liu et al., 2018) architecture by replacing the depth separable convolution with
the proposed optimal separable convolution. The accuracy is improved from 97.24% to 97.67%
with fewer parameters. On the ImageNet dataset (Deng et al., 2009), the proposed optimal separa-
ble convolution also achieves an improved performance. For the DARTS architecture, the proposed
approach achieves 74.2% top1 accuracy with only 4.5 million parameters.
2	The Proposed Approach
2.1	Convolution and its Computational Complexity
A convolutional layer takes an input tensor Bl-1 of shape (Cl-1 , Hl-1, Wl-1) and produces an
output tensor Bl of shape (C1,H1, Wι), where C*, H*, W* are input and output channels, feature
heights and widths. The convolutional layer is parameterized with a convolutional kernel of shape
(Cl, Cl-1, KlH, KlW ), where Kl* are the kernel sizes, and the superscript indicates whether it is
aligned with the features in height or width. In this research, we take C* = O(C), H* = O(H),
W* = O(W), and K*H|W = O(K) for complexity analysis. Formally, we have
Bl(cl, hl, wl) = ΣΣΣBi-i(ci-i, hi-i,wi-i) ∙ Fι(cι, ci-1, kH, kW),	(1)
cl-1 klH klW
where hi = hi-1 + kiH and wi = wi-1 + kiW. Hence, the number of FLOPs (multiply-adds)
for convolution is CHWi ∙ Ci-ιKHKW = O(C2K2HW) and the number of parameters is
CiCi-1KiHKiW = O(C2K2).
For a group convolution, we have g convolutions with kernels of shape (Ci/g, Ci-1/g, KiH, KiW).
Hence, it has O(C2K2HW/g) FLOPs and O(C2K2/g) parameters, where g is the number of
groups. A depth-wise convolution is equivalent to a group convolution with g = C* = C. A point-
wise convolution is a 1 × 1 convolution. A depth separable convolution is composed ofa depth-wise
convolution and a point-wise convolution. A spatial separable convolution replaces a K × K kernel
with K × 1 and 1 × K kernels. Different types of convolutions are summarized in Table 1. From
this table, their FLOPs and number of parameters can be easily verified.
2.2	Rethinking Convolution and the Volumetric Receptive Field Condition
Separable convolution has been proven to be efficient in reducing the computational demand in
convolution. However, existing approaches including both depth separable and spatial separable
convolutions follow an ad hoc design. They are able to reduce the computational cost to some extent
but will not normally achieve an optimal separation. In this research, we shall design an efficient
convolution operator achieving the computational objective by optimal design of its internal hyper-
parameters. The resulting operator is called optimal separable convolution.
One difficulty is that if we do not pose any restriction to a separable convolution, optimizing the
FLOPs target will resulting in a separable convolution being equivalent to a degenerated channel
scaling operator3. Hence, we propose the following volumetric receptive field condition.
2In multi-objective optimization, a Pareto-frontier is the set of parameterizations (allocations) that are all
Pareto-optimal. An allocation is Pareto-optimal if there is no alternative allocation where improvement can be
made to one participant’s well-being without sacrificing any other’s. Here, Pareto-frontier represents the curve
of the accuracies we are able to achieve for different FLOPs/#Params.
3From Table 1, let g = C and K = 1, a convolution will have C parameters and CHW FLOPs. This is
in fact a channel scaling operator. Composition of such operators is not meaningful because the composition
itself is equivalent to a single channel scaling operator.
3
Under review as a conference paper at ICLR 2021
As illustrated in Fig. 1a, the receptive field (RF) of a convolution is defined to be the region in the
input space that a particular CNN’s feature is affected by (Lindeberg, 2013). We define the channel
RF to be the channels that affect CNN’s output and define the volumetric RF to be the Cartesian
product of the RF and channel RF of this convolution. The volumetric RF of a convolution is actually
the volume in the input space that affects CNN’s output. The volumetric RF condition requires that
a properly decomposed separable convolution maintains (at least) the same volumetric RF as the
original convolution before decomposition. Hence, the proposed optimal separable convolution will
be equivalent to optimizing its internal parameters while satisfying the volumetric RF condition.
2.3	Optimal Separable Convolution
In this section, we discuss the case of two-separable convolution. We present the discussion infor-
mally to gain intuition into the proposed approach. In the next section, we shall provide a formal
proof. Suppose that the shape of the original convolutional kernel is (Cout, Cin, KH, KW), where
Cin, Cout are the input and output channels, and (KH, KW) is the kernel size. Let C1 = Cin, and
C3 = Cout . For the proposed optimal separable convolution, we optimize the FLOPs as computa-
tional objective while maintaining the original convolution’s volumetric RF. Formally, the computa-
tional demand of the proposed separable convolution is
f(g1,g2,C2,KHW ) = CKW HW + "HKW HW	⑵
g1	g2
In order to satisfy the volumetric RF condition, the following three conditions need to be satisfied4:
K1H + K2H - 1 = KH	(Receptive F ield C ondition)	(3)
K1W +K2W - 1 = KW	(4)
gi ∙ g2 ≤ C2	(Channel Condition)	(5)
min(Cl, Cl+1) ≥ gl	(Group Convolution C ondition)	(6)
We have three sets of parameters: the number of groups g1, g2, the internal channel size C2, and the
internal kernel sizes KH1W. In this research, We shall assume that the internal channel size C? is in
an order of O(C) and is preset according to a given policy. Otherwise, g1 = g2 = C2 = 1 will be
a trivial solution. This could lead the separable convolution to be over-simplified and not applicable
in practice. Typical policies of presetting C2 include C2 = min(C1, C3) (normal architecture),
C2 = (C1 + C3)/2 (linear architecture), C2 = max(C1, C3)/4 (bottleneck architecture (He et al.,
2016)), or C2 = 4min(C1, C3) (inverted residual architecture (Sandler et al., 2018)).
The proposed problem is a constrained optimization. It is
usually hard to solve it directly . However, we shall show
that the optimal solution shall be g1,g2 〜√C. For large
channel sizes, the optimal solution is usually an interior
point in the solution space rather than on the boundary.
Let KH1W be constants, by substituting g? = C2∕g15 *
and setting f 0(g1) = 0, one can derive that
SC1C2KH KW	不
g1 = V C3KHKW 〜7C,	⑺
and
minf(gι) = 2 ∙《CC2C3KlKWKHKWHW (8)
=O(C 3 KHW)
if we set K1H|W = KH|W and K2H|W = 1.
Figure 2: Given channels C1 = C2 = C3 =
64, and kernel sizes KH = KW = 5
in Equation (2), by setting f0(g1) = 0,
f0(K1) = 0. The solution g1 = 8, K1 = 3
is a saddle point.
One interesting fact is that we can optimize the internal number of groups g1, g2 and internal kernel
sizes KH1W simultaneously. For simplicity, we assume that kernel sizes aligned in height and
4The channel condition (5) gi ∙ g2 ≤ C2 ⇔ C ∙ C ≥ Ci means the product C ∙ Cg2 needs to occupy
each node in the input channel C1 = Cin to maintain the volumetric receptive field. This is further explained
for the channel condition general case (13) in Section 2.4.
5It is trivial to verify that, for any solution (g1,g2) with gi ∙ g2 < C2, (gi,g2 = C2∕g1 > g2) shall be
another feasible solution with a smaller FLOPs target. Hence, the optimal solution must satisfy gi ∙ g2 = C2.
4
Under review as a conference paper at ICLR 2021
width are equal. By setting f0(g1) = 0 and f0(K1) = 0, one can derive that g1 is the same as
Equation (7) and Ki = K = K++1, substituting them into Equation (8), one can get f (gι, Ki)=
O(C3K2HW). This results in a higher complexity than O(C3KHW). In fact, the solution to
f0(gi) = 0 and f0(Ki) = 0 is a saddle point. As illustrated in Fig. 2, given the input channel Ci =
64 and output channel C3 = 64, kernel size (KH, KW) = (5, 5), we take C2 = min(C1,C3) = 64.
By setting f 0(g1) = 0, f(Ki) = 0, the solution gi = √64 = 8, Ki = 5+1 = 3 is a saddle point.
2.4	Optimal Separable Convolution (General Case)
In this section, we shall generalize the proposed optimal separable convolution from N = 2 to an
optimal N and shall provide a formal proof. Suppose that the shape of the original convolutional
kernel is (Cout, Cin, KH, KW). Let Ci = Cin, and CN+i = Cout (CN+i = C3 for N = 2). The
computational demand of an N -separable convolution is
f ({g*}, {KHW})= gKHKWHW + …+ CNMNKHKWHW	(9)
gi	gN
For ease of analysis, We first introduce the notation channels per group n = C, which simply
means: channels per group × number of groups = the number of channels. Then, we have
f({n*},{KHlW}) = C2n1Kl KW HW + …+ CN+inN KN KW HW	(10)
satisfying the volumetric RF condition
KH + KH + …=KH + (N — 1)	(Receptive Field Condition)	(11)
KW + KW + …=KW + (N — 1)	(12)
ni ∙∙∙ Un ≥ Ci ⇔ gi …gN ≤ C2 …CN	(Channel Condition)	(13)
nι ≥ max(1, +i) ⇔ gi ≤ min(Cι, Cι+i)	(Group Convolution Condition)	(14)
Cl
We keep both notations gι and nι . This is because, for the channel condition, it is intuitive that
ni ∙∙∙ un ≥ Ci means that the product of ni ∙∙∙ un needs to occupy each node in the input channel
Ci = Cin. This is equivalent to the less intuitive condition gi …gN ≤ C2 •…CN. Similarly, for
the group convolution condition, gι ≤ min(Cι, Cι+i) means the number of groups can not exceed
the input and output channels of this group convolution, while n ≥ max(1, Cl+1) is less intuitive.
For Equation (10), apply an arithmetic-geometric mean inequality, we can get
f ({n*}, {KH∣W}) ≥ N NFG -CCN +iKH …KNKW …KW HW	1⑸
V	gi ∙∙∙ gN
≥ N qCi ∙∙∙ Cn +iKH ∙∙∙ KH KW ∙∙∙ KN HW	(16)
The equality holds if and only if C2niKHKW =…=CN+inNKNKW. Let nι = βιni, where
C	C2KHkW TeLe	1	C Γc7 N∕πCi∏κHπkw 1
βι =	CKHKW . Let β = ∏βi, we	can solve ni	= NC	= V ckHKW	and
N∏N+iCi∏N=iKH ∏N=iKW
nl =	Cl+iKlH KW	√c'	(17)
Note that the inequality (16) holds for arbitrary KHIW. We need to further optimize KHIW. From
the arithmetic-geometric mean inequality again, we can get KH …KH ≤ (KI +N+KN )N =
(KHNN-i)nand the equality holds if and only if KH = … = KN = KHNN-i. However,
we want the inequality reversed, instead of finding the maximum of this product, we expect to find
its minimum. This still gives us a hint, the maximum is achieved when the internal kernel sizes are
as even as possible, so the minimum should be achieved when the internal kernel sizes are as diverse
as possible. In the extreme case, one of the internal kernel sizes should take KHand all the rest takes
1. A formal proof of this claim can be derived. Hence, we have
f ({nJ, {KH|W}) ≥ N PCi …CN+iKHKWHW = O(NCi+NKNHW).	(18)
5
Under review as a conference paper at ICLR 2021
Figure 3: Experimental results on CIFAR10 for the ResNet architecture (best viewed in color). The proposed
optimal separable convolution (o-ResNet) achieves improved (a) accuracy-FLOPs and (b) accuracy-#Params
Pareto-frontiers than both the conventional (ResNet) and depth separable (d-ResNet) convolutions.
Table 2: Experimental results on CIFAR10 for DARTS. The proposed optimal separable convolution (o-
DARTS) generalizes well to the DARTS architecture, and achieves improved accuracy with approximately the
same FLOPs and fewer parameters. DARTS uses depth separable convolution and an optional d- is prefixed.
Net Arch	FLOPs	#Params	Accuracy	Error Rate
	(billion)	(million)	(%)	(%)
(d-)DARTS (Liu et al., 2018)	0.528	3.35	97.24%	2.76%
o-DARTS	0.572	3.25	97.67%	2.33%
P-DARTS (Chen et al., 2019)	0.532	3.43	97.50%	2.50%
PC-DARTS (Xu et al., 2019)	0.557	3.63	97.43%	2.57%
GOLD-DARTS (Bi et al., 2020)	0.546	3.67	97.47%	2.53%
It can be verified that, for N = 2, Equation (8) and Equation (18) match with the same complexity.
By setting f0 (N) = 0, we can derive that N = log(CK2), and
minf({n*},{KHlW}) = eCHW ∙ log(CK2) = O(CHW ∙ log(CK2))	(19)
where e = 2.71828... is the natural logarithm constant.
The proposed optimal separable convolution can have a spatial separable configuration: a single
kernel takes (KH, KW) or two kernels take (KH, 1) and (1, KW). Besides, the proposed optimal
separable convolution allows using a mask of the internal number of groups and solve for an M-
separable sub-problem (M < N). Details are discussed in Appendix A, where the implementation
of the proposed optimal separable convolution is also presented in Algorithm 1.
3	Experimental Results
In this section, we carry out extensive experiments on benchmark datasets to demonstrate the effec-
tiveness of the proposed optimal separable convolution scheme. In the proposed experiments, we
use a prefix d- or o- to indicate that the conventional or depth separable convolutions in the baseline
networks are replaced with depth separable (dsep) or the proposed optimal separable (osep) convo-
lutions. In this research, we set the number of separated convolutions N = 2. The details of the
training settings for the proposed experiments are described in Appendix B.
3.1	Experimental Results on CIFAR 1 0
CIFAR10 (Krizhevsky et al., 2009) is a dataset consist of 50,000 training images and 10,000 testing
images. These images are with a resolution of 32 × 32 and are categorized into 10 object classes. In
the proposed experiments, we use ResNet (He et al., 2016) as baselines and replace the conventional
convolutions in ResNet with dsep and osep convolutions, resulting in d-ResNet and o-ResNet.
The proposed osep scheme can significantly reduce FLOPs/#Params. In Section 2, We proved that
this reduction factor can be CKK in theory6. As illustrated by the solid lines in Fig. 3 (a) and (b),
the orange solid curve lies in a region With significantly smaller x-values than the blue solid curve.
6For optimal separable, √CK = C；/KKW. For depth separable, 丁.+1/。= CHW(C+W2)< K2 .
6
Under review as a conference paper at ICLR 2021
This indicates that o-ResNet shall have significantly smaller FLOPs and fewer parameters than the
ResNet baseline. For example, o-ResNet110 has even lower FLOPs (0.033 billion vs 0.041 billion)
and fewer parameters (0.177 million vs 0.270 million) than ResNet20, yet with noticeable higher
accuracy (92.12% vs 91.25%). This demonstrates that the proposed osep scheme could significantly
reduce both computational cost and number of parameters for conventional convolutions. For dsep,
this reduction factor is、国2；1/。, which is bounded by K2. For 3 X 3 kernels, this reduction can
be at most 9. Whereas for the proposed osep scheme, no such bounds exist. The advantage of the
proposed osep scheme over dsep is illustrated in Fig. 3 (a) and (b) by the orange and green solid
curves. From which, we can see the proposed osep scheme is more efficient7 8 with smaller x-values.
The proposed o-ResNets can have 8x-16x smaller FLOPs and 10x-18x fewer parameters than the
ResNet baselines in the proposed experiments. For fair comparisons, we introduce the channel mul-
tiplier in order to approximately match the FLOPs. We use the suffix “_m<multiplier>”8 to in-
dicate the channel multiplier. Note that FLOPS/#ParamS is proportional to channel .multiplier3^2
for osep. As illustrated in Fig. 3, from which we can see, the proposed optimal separable convo-
lution scheme is much more efficient than conventional convolutions. The orange curve, including
both solid and dashed parts, achieved a better accuracy-FLOPs Pareto-frontier than the blue curve.
It is worth noting that even under the same FLOPs, the number of o-ResNet parameters is also
smaller than that of ResNet by a large margin. This could result in a more regularized network with
fewer parameters to prevent over-fitting and possibly contribute to the final performance. In Fig.
3, we also present the d-ResNet curves in dashed green by replacing the conventional convolutions
with depth separable convolutions. As can be seen, d-ResNet achieves good accuracy-FLOPs bal-
ances for small networks (e.g. d-ResNet20 and d-ResNet32), but performs comparable or no better
than conventional convolutions for large ones (e.g. d-ResNet56 and d-ResNet110). In summary,
the proposed optimal separable convolution achieves better accuracy-FLOPs and accuracy-#Params
Pareto-frontiers than both conventional and depth separable convolutions.
To demonstrate that the proposed osep scheme generalizes well to other DCNN architectures, we
adopt the DARTS (V2) (Liu et al., 2018) network as the baseline. The DARTS evaluation network
has 20 cells and 36 initial channels, we increase the initial channels to 42 to match the FLOPs. By
replacing the dsep convolutions in DARTS with the proposed osep convolutions, as illustrated in
Table 2, the resulting o-DARTS improved the accuracy from 97.24% to 97.67% with fewer parame-
ters (3.25 million vs 3.35 million). It is worth noting that it is very hard to significantly improve the
DARTS search space. In Table 2, we also include three variants of DARTS, i.e. P-DARTS (Chen
et al., 2019), PC-DARTS (Xu et al., 2019), and GOLD-DARTS (Bi et al., 2020), with more advanced
search strategies for comparison. As can be seen, o-DARTS even achieved higher accuracies than
these advanced network architectures.
Ablation Studies We carry out ablation studies on the effects of the internal BatchNorms and non-
linearities, and the spatial separable configuration. We conclude that internal BatchNorms and non-
linearities have no effects on the results yet introduce extra computation and parameters, while the
spatial separable configuration leads to a slightly worse performance. Hence, they are not adopted
in this research. Details are presented in Table 5 with discussion in Appendix C.
3.2	Experimental Results on ImageNet
We evaluate the proposed optimal separable convolution scheme on the benchmark ImageNet (Deng
et al., 2009) dataset, which contains 1.28 million training images and 50,000 testing images.
3.2.1	ImageNet40
Because carrying out experiments directly on the ImageNet dataset can be resource- and time-
consuming, we resized all the images into 40 × 40 pixels. A 32 × 32 patch is randomly cropped and a
random horizontal flip with a probability of 0.5 is applied before feeding into the network. No extra
data augmentation strategies are used. The baseline ResNet architecture is a modified version of that
used on the CIFAR10 dataset, except that the channel sizes are set to be 4× larger, the features are
7Because there is an overhead and the channels are relatively small (e.g. 16) for CIFAR10 models, the ad-
vantage margin is noticeable but is not remarkable. This margin of advantage can be bigger for larger channels.
8We usually omit this channel multiplier suffix for simplicity when it is clear from the context that we are
comparing different schemes under the same FLOPs/#Params and there are no confusions.
7
Under review as a conference paper at ICLR 2021
Table 3: Experimental results on ImageNet40 for the ResNet architecture. The proposed optimal separable
convolution (o-ResNet) achieves 4-5% performance gain over the ResNet baseline.
Net Arch	Channel Multiplier	FLOPs (billion)	#Params (million)	Accuracy (%)	Error Rate (%)
ResNet20	-	0.162	4.58	40.28	59.72
o-ResNet20	5.375	0.160	5.13	44.94	55.06
ResNet32	-	0.275	7.68	42.98	57.02
o-ResNet32	5.75	0.278	7.78	47.88	52.12
ResNet56	-	0.502	13.88	44.93	55.07
o-ResNet56	6.0	0.497	12.55	49.97	50.03
ResNet110	-	1.012	27.83	46.74	53.26
o-ResNet110	6.25	1.027	23.79	50.72	49.28
Table 4: Experimental results on	full ImageNet for the DARTS architecture. The proposed o-DARTS achieves				
74.2% top1 accuracy with only 4.5 million parameters.					
Net Arch e rc	FLOPs	#Params	Top1	Top1 Error	Top5 Top5 Error
	(billion)	(million)	(%)	(%)	(%)	(%)
(d-)DARTS (Liu et al., 2018)	0.530		4.72	73.3%	26.7%	91.3%	8.7%
o-DARTS	0.554	4.50	74.2%	25.8%	91.9%	8.1%
calculated on scales of [16, 8, 4], and the last fully-connected (FC) layer outputs 1000 categories
for classification. We make this modification because the ImageNet dataset has significantly more
training samples than the CIFAR10 dataset. Experimental results are illustrated in Table 3, as can be
seen, by substituting conventional convolutions with the proposed optimal separable convolutions,
the resulting o-ResNet achieved 4-5% (e.g. 49.97% vs 44.93% for 56-layer and 50.72% vs 46.74%
for 110-layer) performance gains comparing against the ResNet baselines. This demonstrates that
the proposed optimal separable convolution scheme is much more efficient. For o-ResNet56 and o-
ResNet110, they also have fewer parameters that could contribute to a more regularized model. For
o-ResNet20 and o-ResNet32, they have slightly more parameters because the last FC layer accounts
for a great portion of overhead for 1000 classes.
3.2.2	Full ImageNet
Similar to the experiments on CIFAR10, we replace the dsep convolutions in the DARTS (V2)
network with the proposed osep convolutions to demonstrate that the proposed approach is able to
generalize to other network architectures. The experiment is carried out on the full ImageNet dataset.
The DARTS evaluation network has 14 cells and 48 initial channels, we increase the initial channel
size to 56 to match the FLOPs. The resulting network is called o-DARTS. Experimental results are
illustrated in Table 4. It can be seen that, with fewer parameters (4.50 million vs 4.72 million), the
proposed o-DARTS network achieved higher accuracies in both top1 (74.2% vs 73.3%) and top5
(91.9% vs 91.3%) accuracies than the DARTS baseline. This indicates that the proposed osep is
able to achieve better accuracy-FLOPs and accuracy-#Params balances than dsep convolutions.
It is worth noting that in the proposed experiments, we adopt ResNet and DARTS as the base-
lines because these are two most well-known architectures. In practice, one may simply replace
the conventional or depth separable convolutions with the proposed optimal separable convolutions
in a DCNN to reduce the computation and model parameters. By increasing the channel sizes,
better-performing models can be expected. The proposed optimal separable convolution achieved
improved accuracy-FLOPs and accuracy-#Params Pareto-frontiers than both conventional and depth
separable convolutions. Hence, one can either match the accuracy to get a smaller model with
reduced computation and model parameters, or match the FLOPs to get a better-performing model.
4	Conclusions
In this paper, we have presented a novel scheme called optimal separable convolution to im-
prove the computational efficiency in convolution. Conventional convolution took a costly com-
plexity at O(C2K2HW). The proposed optimal separable convolution scheme is able to achieve
8
Under review as a conference paper at ICLR 2021
its complexity at O(C2KHW), which is even lower than that of depth separable convolution at
O(CHW ∙ (C + K2)). Hence, the proposed optimal separable convolution has thefullpotential to
replace the usage of depth separable convolutions in a DCNN. Examples include but are not limited
to the ResNet and DARTS architectures. The proposed optimal separable convolution also has a
spatial separable configuration. A generalized N -separable case can achieve better performance at
O(CHW ∙log(CK2)).
Another potential impact of the proposed optimal separable convolution is for the AutoML com-
munity. The proposed novel operator is able to increase the neural architecture search space. In
a multi-objective optimization formulation, where both accuracy and FLOPs are optimized, we ex-
pect a more efficient network architecture can be discovered in the future using the proposed optimal
separable convolution operator.
References
Kaifeng Bi, Lingxi Xie, Xin Chen, Longhui Wei, and Qi Tian. Gold-nas: Gradual, one-level, differ-
entiable. arXiv preprint arXiv:2007.03331, 2020. 2, 3.1
Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging
the depth gap between search and evaluation. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pp. 1294-1303, 2019. 2, 3.1, B
Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong
Tian, Matthew Yu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe search using neural
acquisition function. arXiv preprint arXiv:2006.02049, 2020. 1
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009. 1, 3.2
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016. 1, 1, 1, 2.3, 3.1, E
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Pro-
ceedings of the IEEE International Conference on Computer Vision, pp. 1314-1324, 2019. E
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1, E
Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training
cnns with low-rank filters for efficient image classification. arXiv preprint arXiv:1511.06744,
2015. 1,E
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. C
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. 1, E
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009. 1, 3.1
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012. 1
Tony Lindeberg. A computational theory of visual receptive fields. Biological cybernetics, 107(6):
589-635, 2013. 1, 2.2
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018. 1,2, 3.1, 4, B,E
9
Under review as a conference paper at ICLR 2021
Henri J Nussbaumer. The fast fourier transform. In Fast Fourier Transform and Convolution Algo-
rithms,pp. 80-111. SPringer,1981. 1, E
Victor Podlozhnyuk. Fft-based 2d convolution. NVIDIA white paper, 32, 2007. 1, E
Alfio Quarteroni, Riccardo Sacco, and Fausto Saleri. Numerical mathematics, volume 37. Springer
Science & Business Media, 2010. 1, E
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
volume 33, pp. 4780-4789, 2019. 1
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018. 2.3, E
LaUrent Sifre and Stephane Mallat. Rigid-motion scattering for image classification. Ph. D. thesis,
2014. 1
RuPesh K Srivastava, KlaUs Greff, and JUrgen Schmidhuber. Training very deep networks. In
Advances in neural information processing systems, pp. 2377-2385, 2015. 1
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
E
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016. 1, E
Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. arXiv preprint arXiv:1905.11946, 2019. 1, E
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong.
Pc-darts: Partial channel connections for memory-efficient architecture search. In International
Conference on Learning Representations, 2019. 2, 3.1
10
Under review as a conference paper at ICLR 2021
Algorithm 1 The Algorithm for Optimal Separable Convolution
Input: Input channel C1 = Cin, output channel CN+1 = Cout, kernel size (KH, KW), number of sepa-
rated convolutions N
Optional Input: internal kernel sizes (optional, preset), internal number of groups (optional, masked values),
spatial separable (True or False)
Output: internal channel sizes C2,…,CN, internal kernel sizes KHIW,…,KHIW, internal number of
groups gι, •…,gN
Calculate internal channel sizes C2, ∙…,Cn as min(Cin, Cout), max(Cin, Cout)/4, or 4 min(Cin, Cout),
etc. according to a preset policy.
if internal kernel sizes KHIW,… ,KHIW are not given then
if spatial separable then
Set KbHN/2c = KH, KbWN/2+1c = KW and all other internal kernel sizes to 1.
else
Set KbHNI/W2c = KHIW and all other internal kernel sizes to 1.
end if
end if
Calculate internal channels per group nl according to nl
NqnN+1Cin=IKH ⅞=ikW
Cl + 1KH KW
Let gl = min(dCl/nle, Cl , Cl+1). If Cl/nl < 1 or Cl/nl > min(Cl, Cl+1) for certain l, re-optimize gl
with a masked number of groups by pre-setting gl = 1 for l ∈ {l : Cl/nl < 1}, gl = min(Cl , Cl+1) for
l ∈{l : CIml > min(C1,C1+1)}.
.Because nι 〜 NC, for large channel sizes, We rarely need to re-optimize.
Return C2,…，Cn; KHIW,…，KHlW; gι,…，gN
A Algorithmic Details of the Proposed Optimal Separable
Convolution
For the proposed optimal separable convolution, one of the internal kernel sizes can take KH|W,
While the rest takes 1. In this research, We simply select the middle kernel size as (KH, KW). In
a spatial separable configuration, We select the middle tWo to have their kernel sizes as (KH, 1)
and (1, KW). It is Worth noting that all these configurations have the same FLOPs. Unlike the
spatial separable convolution Where the spatial separable configuration is able to reduce the com-
plexity from K2 to 2K. This is because the complexity has already been reduced to O(K) for
the proposed optimal separable convolution. Another interesting property of the proposed optimal
separable convolution is that it prefers large kernel sizes over small ones.
For the proposed optimal separable convolution, We are able to preset the internal convolutional
kernel sizes according to a custom policy, and optimize the internal number of groups only. Further-
more, We are able to preset a portion of the internal number of groups to certain values, and optimize
only the remaining internal number of groups. Suppose that the internal channel and kernel sizes are
given. Without loss of generality, we assume that gM+1, ∙∙∙ ,gN are preset. The proposed optimal
separable problem Will be an M -separable convolution sub-problem (M < N):
f ({n*}, {KHlW}) = C2n1KHKWHW + …+ CM+ιnMKMKWHW + COnst	(20)
satisfying the volumetric RF condition
KH + …+ KM = KH + const	(Receptive Field Condition)
(21)
KW + …+ KM = K W + const	(22)
Cl	C2 …CN	…	5	」 . ∖
ni •…nM ≥ ----------⇔ gι •…gM ≤ ------------ (Channel Condition)
M+1	N	gM+1	gN	(23)
nι ≥ max(1, Cl+1) ⇔ gι ≤ min(Cι, C1+1)
Cl
(Group C onvolution Condition)
(24)
This M -separable sub-problem can be solved by the same algorithm. A detailed implementation of
the proposed optimal separable convolution is described by Algorithm 1.
11
Under review as a conference paper at ICLR 2021
Table 5: Experimental results on CIFAR10 for the ResNet architecture with ablation studies of internal BN and
non-linearity and spatial separable configuration.
Net Arch	Channel Multiplier	FLOPs (billion)	#Params (million)	Accuracy (%)	Internal BN and Non-linearity (%)	Spatial Separable (%)
ResNet20	-	0.04055	0.270	91.25	-	-
o-ResNet20	3.875	0.04054	0.206	92.89	92.74	92.32
ResNet32	-	0.06886	0.464	92.49	-	-
o-ResNet32	3.875	0.06760	0.352	93.18	93.22	92.88
ResNet56	-	0.12548	0.853	93.03	-	-
o-ResNet56	3.875	0.12180	0.643	93.32	93.42	92.93
ResNet110	-	0.25289	1.728	93.39	-	-
o-ResNet110	3.875	0.24370	1.298	94.35	94.21	93.96
B	Training Settings
Experiments on CIFAR10 for the ResNet architecture The images are padded with 4 pixels and
randomly cropped into 32 × 32 to feed into the network. A random horizontal flip with a probability
of 0.5 is also applied. All the networks are trained with a standard SGD optimizer for 200 epochs.
The initial learning rate is set to 0.1, with a decay of 0.1 at the 100 and 150 epochs. The batch size
is 128. A weight decay of 0.0001 and a momentum of 0.9 are used.
Experiments on CIFAR10 for the DARTS architecture We follow the same training settings in
(Liu et al., 2018): the network is trained with a standard SGD optimizer for 600 epochs with a batch
size of 96. The initial learning rate is set to 0.025 with a cosine learning rate scheduler. A weight
decay of 0.0003 and a momentum of 0.9 are used. Additional enhancements include cutout, path
dropout of probability 0.2, and auxiliary towers with weight 0.4.
Experiments on ImageNet40 for the ResNet architecture Each network is trained with a stan-
dard SGD optimizer for 20 epochs with the initial learning rate set to 0.1, and a decay of 0.1 at the
10 and 15 epochs. The batch size is 256, the weight decay is 0.0001 and the momentum is 0.9.
Experiments on full ImageNet for the DARTS architecture We follow the training settings in
(Chen et al., 2019) for multi-GPU training: the images are random resized crop into 224 × 224
patches with a random scale in [0.08, 1.0] and a random aspect ratio in [0.75, 1.33]. Random
horizontal flip and color jitter are also applied. The network is trained from scratch for 250 epochs
with batch size 1024 on 8 GPUs. An SGD optimizer with an initial learning rate of 0.5, a momentum
of 0.9, and a weight decay of 3e-5. The learning rate is decayed linearly after each epoch. Additional
enhancements include label smoothing with weight 0.1 and auxiliary towers with weight 0.4.
C Ablation Studies
Internal BatchNorm and Non-linearity For a DCNN, it is generally a good practice to add a
BatchNorm (BN) (Ioffe & Szegedy, 2015) and a non-linearity after each convolution. For the pro-
posed optimal separable convolution, we wonder if it is still necessary to add such a BN and a
non-linearity after each of the internal separated convolutions. Experimental results are illustrated
in Table 5. Comparing the “Internal BN and Non-linearity” column against the “Accuracy” col-
umn, we are able to conclude that with or without internal BN and non-linearity, similar results
with only statistical variances can be generated. This is reasonable because the network has already
been regularized by outer BN and non-linearity layers from the macro architecture. Internal ones
shall offer little to no additional improvements. Because internal BN and non-linearity could intro-
duce extra computation and parameters, in the proposed research, we shall not use internal BN and
non-linearity.
Spatial Separable Another variation of the proposed optimal separable convolution scheme is the
spatial separable configuration. For Equation (16), the optimal solution is achieved when one of
12
Under review as a conference paper at ICLR 2021
Table 6: Experimental results on CIFAR10 for the ResNet with inference time on Windows 10 Intel CPU
i5-8250.
Net Arch	Channel Multiplier	FLOPs (billion)	#Params (million)	Accuracy (%)	Inference Time (s)
ResNet20	-	0.04055	0.270	91.25	0.0310
o-ResNet20	-	0.00567	0.028	87.76	0.0276
o-ResNet20	3.875	0.04054	0.206	92.89	0.0468
d-ResNet20	-	0.00639	0.039	88.90	0.0312
d-ResNet20	2.75	0.0400	0.250	92.66	0.0468
ResNet32	-	0.06886	0.464	92.49	0.0469
o-ResNet32	-	0.00931	0.048	89.37	0.0624
o-ResNet32	3.875	0.06760	0.352	93.18	0.0937
d-ResNet32	-	0.01057	0.066	89.26	0.0625
d-ResNet32	2.75	0.06565	0.429	92.98	0.1154
ResNet56	-	0.12548	0.853	93.03	0.0938
o-ResNet56	-	0.01660	0.088	90.53	0.0948
o-ResNet56	3.875	0.12180	0.643	93.32	0.1562
d-ResNet56	-	0.01893	0.121	90.22	0.1094
d-ResNet56	2.75	0.11890	0.786	92.69	0.1875
ResNet110	-	0.25289	1.728	93.39	0.1563
o-ResNet110	-	0.03300	0.177	92.12	0.1885
o-ResNet110	3.875	0.24370	1.298	94.35	0.3216
d-ResNet110	-	0.03770	0.244	91.90	0.1910
d-ResNet110	2.75	0.23870	1.590	93.40	0.3462
the internal kernel sizes takes KH|Wand all the rest takes 1. It does not matter which one of the
internal kernel sizes takes KH|W. Hence, we have this spatial separable variant: a single kernel
takes (KH, KW) or two kernels take (KH, 1) and (1, KW). The detailed implementation is illus-
trated in Algorithm 1. While spatial separable or not affects neither the FLOPs nor the number of
parameters for the proposed optimal separable convolution, the results could be slightly different.
As illustrated by the column “Spatial Separable” in Table 5, the spatial separable configuration leads
to slightly worse performances. The reason might be that spatial separation fuses horizontal and
vertical features separately, which could be less efficient than fusing them simultaneously.
D Inference Time for the Proposed Optimal Separable
Convolution
FLOPs measures the best possible theoretical speed we are able to achieve. In this Section, we
further report the wall-clock inference time of the proposed optimal separable convolution scheme.
The inference time is measured on a laptop computer with Windows 10 operating system and Intel
i5-8250 CPU, which we use to simulate a mobile platform. The results are illustrated in Table 6. As
can be seen, for ResNet20, o-ResNet20_m3.875, and d-ResNet20_m2.75,they have a similar FLOPs
(≈0.0405 billion), yet o-ResNet20.m3.875 and d-ResNet20.m2.75 take a slightly longer inference
time (0.0468s vs 0.0310s). This is because current implementation of grouped convolution in Py-
Torch is not optimized. From Table 6, we can also conclude that under the same FLOPs, o-ResNet
has slightly better performance than d-ResNet (e.g. o-ResNet32 takes 0.0937s while d-ResNet32
takes 0.1154s). This might because the proposed optimal separable convolution scheme usually has
smaller “groups” hyper-parameters than depth separable convolution. Similar conclusions can also
be drawn for deeper ResNet32, ResNet56, and ResNet110 networks.
13
Under review as a conference paper at ICLR 2021
E	Related Work
There have been many previous works aiming at reducing the amount of computation in convolution.
Historically, researchers apply Fast Fourier Transform (FFT) (Nussbaumer, 1981; Quarteroni et al.,
2010) to implement convolution. For 1D convolution, FFT reduces the number of computations
for H points from O(H2) to O(H log H). For 2D convolution, FFT-2D reduces the computational
complexity from O(HW ∙ K2) to O(HW ∙ (log H + log W)) (Podlozhnyuk, 2007). Hence, it can be
easily concluded that FFT gains great speed up for large convolutional kernels. For small convolu-
tional kernels (K << H or W), a direct application is often still cheaper. Researchers also explore
low rank approximation (Jaderberg et al., 2014; Ioannou et al., 2015) to implement convolutions.
However, most of the existing methods obtain moderate efficiency improvements, and they usu-
ally require a pre-trained model and mainly focus on network pruning and compression. In recent
state-of-the-art deep CNN models, several heuristics are adopted to reduce the heavy computation in
convolution. For example, in (He et al., 2016), the authors use a bottleneck structure. Yet in (Sandler
et al., 2018), the authors adopt an inverted bottleneck structure. Such heuristics may require further
ad hoc design to work in practice, however, they are not solid and shall become less convincing.
Among various implementations of convolution, separable convolution has been proven to be more
efficient in reducing the computational demand. Depth separable convolution is explored exten-
sively in modern DCNNs (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019; Liu
et al., 2018; Tan & Le, 2019). It reduces the computational cost of a conventional convolution
from O(C2K2HW) to O(CHW ∙ (C + K2)). However, the proposed optimal separable ConvolU-
3
tion is even more efficient than depth separable convolution. It can be calculated at O(C2 KHW)
and has the full potential to replace the usage of depth separable convolutions. A second advantage
of the proposed optimal separable convolution is that it can be applied to fully connected layers if
we view them as 1 × 1 convolutional layers, whereas depth separable convolution cannot. Further,
depth separable convolution requires the middle channel size to be equal to the input channel size,
whereas for the proposed optimal separable convolution, the middle channel size can be freely set.
Spatial separable convolution was originally developed to speed up image processing operations.
For example, a Sobel kernel is a 3 X 3 kernel and can be written as (1,2, I)T ∙ (-1,0,1). Spatial
separable will require 6 instead of 9 parameters while doing the same operation. Spatial separable
convolution is also adopted in the design of modern DCNNs. For example, in (Szegedy et al., 2016),
the authors introduce spatial separation to the GoogLeNet (Szegedy et al., 2015) architecture. For
the proposed optimal separable convolution, there is also a spatial separable configuration.
In the body of literature, separable convolution is also referred to factorized convolution or convo-
lution decomposition. In this research, the proposed scheme is called optimal separable convolution
following the naming conventions of depth and spatial separable convolutions.
14