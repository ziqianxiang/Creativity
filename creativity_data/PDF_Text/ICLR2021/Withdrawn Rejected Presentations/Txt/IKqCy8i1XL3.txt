Under review as a conference paper at ICLR 2021
Optimizing Information B ottleneck in Rein-
forcement Learning: A Stein Variational Ap-
PROACH
Anonymous authors
Paper under double-blind review
Ab stract
The information bottleneck (IB) principle is an elegant and useful learning frame-
work for extracting relevant information that an input feature contains about the
target. The principle has been widely used in supervised and unsupervised learning.
In this paper, we investigate the effectiveness of the IB framework in reinforcement
learning (RL). We first derive the objective based on IB in reinforcement learning,
then we analytically derive the optimal conditional distribution of the optimization
problem. Following the variational information bottleneck (VIB), we provide a
variational lower bound using a prior distribution. Unlike VIB, we propose to
utilize the amortized Stein variational gradient method to optimize the lower bound.
We incorporate this framework in two popular RL algorithms: the advantageous
actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO).
Our experimental results show that our framework can improve the sample effi-
ciency of vanilla A2C and PPO. We also show that our method achieves better
performance than VIB and mutual information neural estimation (MINE), two
other popular approaches to optimize the information bottleneck framework in
supervised learning.
1	Introduction
In training a reinforcement learning model, an agent interacts with the environment, explores the
(possibly unknown) state space, and learns a policy from the exploration sample data. Such samples
may be quite expensive to obtain (e.g., requires interactions with the physical environment). Hence,
improving the sample efficiency of the learning algorithm is a key problem in RL and has been
studied extensively. In particular, an effective representation that has less redundant information can
help improve the sample efficiency in RL. This can be seen from the following motivating example:
According to the results of (Sygnowski & Michalewski, 2016), in the classical Atari game, Seaquest,
it may require dozens of millions of samples to converge to an optimal policy when the input states
are raw images (more than 28,000 dimensions), while it requires much less samples when the inputs
are 128-dimension pre-defined RAM data. Clearly, the RAM data contains much less redundant
information than the raw images does.
The information bottleneck (IB) framework (Tishby et al., 2000) is a popular framework for extracting
relevant information that an input feature contains about the target and has been used extensively in
supervised and unsupervised learning. In this paper, we adopt the information bottleneck framework
in the context of RL, with the aim of producing more informative and less redundant representation
and improving the sample efficiency. In the experiments in (Shwartz-Ziv & Tishby, 2017), one can
see that during the training process in a standard supervised learning setting, the neural network
first ”remembers” the inputs by increasing the mutual information (MI) between the inputs and
the representation variables, then compresses the inputs to efficient representation related to the
learning task by discarding redundant information from inputs (decreasing the MI between inputs
and representation variables). We call this phenomena ”information extraction-compression process”
(information E-C process). We verify empirically that such process also exists in some Atari games
(in our experiments, we use A2C). We run simple experiments in three Atari games. Figure 1 shows
the changes of MI between inputs and representations when training an RL agent (we need to use
MINE (Belghazi et al., 2018) for MI estimation). We can see that the MI first increases, which
1
Under review as a conference paper at ICLR 2021
(a) MI in Pong
episode
(b) MI in MsPacman
(c) MI in Assault
Figure 1: Visualizing the mutual information (MI) during the training in 3 Atari games. X-axis
indicates the number of training steps and y-axis the MI.
implies that relevant information is extracted from the inputs, and then MI decreases, suggesting that
the information is compressed (redundant information is discarded). This result is very similar in
spirit to the results shown in (Shwartz-Ziv & Tishby, 2017). This interesting observation motivates
us to adopt the IB framework in RL, in order to accelerate the extraction-compression process. The
IB framework is intended to explicitly enforce RL agent to discard irrelevant information from raw
input data, hence improving the sample efficiency. Our technical contributions can be summarized as
follows:
1.	We incorporate the information bottleneck (IB) principle to the training of reinforcement
learning algorithms. We formulate the optimization problem of the IB framework, and prove
a near-optimality property of our framework under certain conditions. (see Theorem 1).
2.	We derive the optimal conditional distribution for the optimization problem (Theorem 2).
Based on this, we construct a variational lower bound (similar to VIB (Alemi et al., 2016)).
Then, we propose a new training algorithm, that leverages the amortized Stein Variational
(SV) gradient method (Feng et al., 2017) to optimize the variational lower bound.
3.	We conduct the experiments on 20 Atari games: Our experimental results show that actor-
critic algorithms (A2C (Mnih et al., 2016) and PPO (Schulman et al., 2017)) with our
framework are more sample efficient than their original versions.
4.	Moreover, we show that our method outperforms VIB in optimizing IB in RL. In Appendix
C.4, we find that using MINE (Belghazi et al., 2018) for optimizing IB is not suitable in RL.
We discuss the possible reasons for this.
2	Related Work
Various information theoretic methods have been used in the context of RL. Here, we only mention
some prior work which seek to maximize some kind of mutual information. Recall that the IB
framework requires to minimize the MI between the inputs and representations under some constraints.
(Oord et al., 2018) proposed Contrastive Predictive Coding (CPC) to maximize the MI between the
current states and future states, which enables the model to predict the future states. We note that their
method also gives a lower bound to estimate the MI. However, their bound is not tight and would
yield a loose upper bound of IB framework, hence not suitable for optimizing IB framework. (Still &
Precup, 2012; Kim et al., 2018; Kumar, 2018) also studied how to improve exploration of RL agents
by maximizing the MI between states and action embedding representations.
As for applying IB principle in RL, (Peng et al., 2018) applied VIB (Alemi et al., 2016) in imita-
tion learning and inverse reinforcement learning. (Wang et al., 2019) applied VIB in multi-agent
communication. (Igl et al., 2019) applied VIB in Actor-Critic algorithms. However, VIB constrains
the representations in the class of Gaussian distributions, while our optimization method can learn
arbitrary distributions. (Abel et al., 2019) proposed to use IB in apprenticeship learning, and the
method they used to optimize IB is Blahut-Arimoto algorithm (Tishby et al., 2000), which is difficult
to apply in deep neural network. (Genewein et al., 2015; Sims, 2003; Leibfried & Braun, 2015)
proposed to penalize regular RL objectives with an MI regularization term between states and actions,
so that the policy can discard redundant information from the states. However, they only provided
theoretical results. (Abdolmaleki et al., 2018; Grau-Moya et al., 2019; Leibfried & Grau-Moya, 2019)
2
Under review as a conference paper at ICLR 2021
applied the above theoretical results in deep RL and provided a variational lower bound like VIB
(Alemi et al., 2016) to optimize the new objective. Yet because of the particularity of the policy
distribution, they only needed to update Q function to implicitly optimize this lower bound. Their
methods can not be used to solve general IB objectives, e.g., the IB objective in our paper.
3	Preliminaries
A Markov Decision Process(MDP) is a tuple, (X, A, R, P, μ), where X is the set of states, A is
the set of actions, R : X × A × X → R is the reward function, P : X × A × X →[0, 1] is the
transition probability function, where P(X0 |X, a) is the probability of transitioning to state X0 given
that the previous state is X and the agent took action a in X, and μ : X →[0,1] is the starting state
distribution. A policy π : X × A →[0, 1] is a probability function over actions and states, with
π(a∣X) denoting the probability of choosing action a in state X.
In reinforcement learning, we aim to select a policy π which maximizes R(π) =
Eτ~∏[P∞=0YtR(Xt,at,Xt+ι)], where We denote R(Xt,at,Xt+ι) = Irt. Here Y ∈ [0,1) is
a discount factor, τ denotes a trajectory (X0, a0, X1, a1, ...). Define the state value function as
Vπ (X) = ET~∏ [P∞=o γtrt |Xo = X], which is the expected return by policy ∏ in state X. And the
state-action value function Qn (X, a) = ET~∏ [P∞=o γtrt∣Xo = X,ao = a] is the expected return
by policy π after taking action a in state X .
In this paper, we consider actor-critic algorithms, which can take the advantage of both policy gradient
methods and value-function-based methods, such as the well-known A2C algorithm (Mnih et al.,
2016). Specifically, in the case that policy ∏(a∣X; θ) is parameterized by θ, A2C uses the following
as an approximation of the real policy gradient VθR(∏):
∞∞
VθJpg(θ) = X Vθ[logπ(at∣Xt; θ)(Rt - b(Xt)) + α2H(π(∙∣Xt))] = X Vθ Jpg(Xt； θ), (1)
t=0	t=0
where VθJpg (θ) is the approximate policy gradient and VθJpg (Xt； θ) is the approximate policy
gradient on a specific state Xt, Rt = Pi∞=0 Yirt+i is the accumulated return from time step t, H(p)
is the entropy of distribution p and b(Xt) is a baseline function, which is commonly replaced by
Vπ(Xt).
A2C also includes the minimization of the mean square error between Rt and value function Vπ (Xt).
Thus in practice, the total objective function in A2C can be written as:
∞
J(θ) ≈ Xlog∏(at∣Xt; θ)(Rt - Vπ(Xt)) + α2H(π(∙∣Xt))
t=0
∞
— αι kRt- Vπ(Xt) k2 = X J(Xt； θ),	⑵
t=0
where α1, α2 are two coefficients.
In this paper, we use a neural network f to encode a state X to its representation Z. We use stochastic
representation, i.e., we let Z = f (X, ξ; φ), where X is the state, ξ is a random noise and φ is the
parameter of f. This naturally defines a probability distribution Z 〜Pφ(∙∣X). A deterministic value
function Vπ (.) takes either the state or its representation Z as the input. Hence, in the following,
without causing any confusion, we write Vπ(Xt) = Vπ(Zt) 以~尸力(.应)(we can write J(Xt； θ)
and Qπ (Xt, at) similarly).
4	Framework
4.1	Information Bottleneck in Reinforcement Learning
The information bottleneck framework is an information theoretical framework for extracting relevant
information that an input X ∈ X contains about an output Y ∈ Y . An optimal representation of X
would capture the relevant factors and compress X by diminishing the irrelevant parts which do not
3
Under review as a conference paper at ICLR 2021
contribute to the prediction of Y . In a Markovian structure X → Z → Y where X is the input, Z is
representation of X and Y is the label of X, IB seeks an embedding distribution P?(Z|X) such that:
P?(Z|X)
arg max I(Y, Z) - βI (X, Z)
P(Z|X)
= arg max H(Y) - H(Y |Z) - βI(X, Z)
= arg max -H(Y |Z) - βI(X, Z),	(3)
P(Z|X)
which appears as the standard cross-entropy loss1 in supervised learning with a mutual information
regularizer. Here β is a coefficient that controls the magnitude of the regularizer.
Next we derive an information bottleneck framework in reinforcement learning. Just like the label Y
in the context of supervised learning as shown in Equation 3, we assume the supervising signal Y in
RL to be the accurate value Rt of a specific state Xt for a fixed policy π, which can be approximated
by an n-step bootstrapping function Yt = Rt = Pin=-01 γirt+i + γnV π(Zt+n) in practice. Let
P(Y |Z) be the following distribution:
P(Yt∣Zt) Y exp(-α(Yt - Vπ(Zt))2).	(4)
This assumption is heuristic but reasonable: Suppose we have an input Xt , its relative label Yt = Rt ,
and we now have Xt’s representation Zt. Naturally we want to train our decision function Vπ(Zt) to
approximate the true label Yt. If We set our target distribution to be C ∙ exp(一α(Rt 一 Vπ(Zt))2),
the probability decreases as Vπ(Zt) gets far from Yt while increases as Vπ(Zt) gets close to Yt.
For simplicity, We just Write P(R|Z) instead of P(Yt|Zt) in the folloWing context.
With this assumption, Equation equation 3 can be Written as:
P?(Z|X)
arg max EX〜P(X),Z〜Pφ(Z∣X),R〜P(R∣Z)[log P(RIZ)] 一 BI(X, Z)
P(Z|X)
arg max
P(Z|X)
Eχ,R,z[-α(R — Vπ(Z))2] — BI(X, Z).
(5)
The first term is simply the mean squared error. In a netWork With representation parameter φ and
policy-value parameter θ, policy loss Jpg(Z; θ) in Equation 1 and IB loss in Equation 5 can be jointly
Written as:
L(θ, Φ) = Ex,z[Jpg(Z； θ) + Er[-α(R 一 Vπ(Z; θ))2]] 一 BI(X, Z; φ),	(6)
X---------------{------------------}
J (Z/)
where I(X, Z; φ) denotes the MI between X and Z 〜 Pφ(∙∣X). Notice that J(Z; θ) itself is a
standard loss function in RL as shoWn in Equation 2. Finally We get the ultimate formalization of the
IB framework in reinforcement learning:
Pφ*(Z∣X ) = arg max EX 〜P (X ),z 〜Pφ(z∣x )[J (Z ； θ)] 一 BI (X,Z ； Φ)∙	⑺
Pφ(Z∣X)	' '' 八，'
The following theorem shows that if the mutual information I(X, Z) of our framework and an optimal
RL policy are close, then our framework is near optimal.
Theorem 1 (Near-optimality theorem). Policy πr = πθr, parameter φr, an optimal policy π? = πθ?
and its representation parameter φ? are defined as follows:
θr,φr = argminEPφ(x,z) log PPZZX) 一 1J(Z； θ) ；	⑻
θ*,φ* = arg min Epφ(x,z) [— 1 J(Z; θ)l .	(9)
θ,φ	B
Define Jπr as EPφr (X,Z) [J(Z; θr)] and Jπ? as EPφ? (X,Z) [J(Z; θ?)]. Assume that there exists an
E > 0, II(X,Z; φ?) 一 I(X,Z; φr)| < β, we have ∣Jπr — Jπ?| < 匕
The proof can be found in Appendix B.1. In practice, B is very small, e.g., 0.001. Hence, if the gap
between MI is E, then the gap between Jπr and Jπ? is 0.001E.
1Mutual information I(X, Z) is defined as R dXdZP(X, Z) log P(XXPZZ), conditional entropy H(Y|Z)
is defined as - R dYdZP(Y, Z) log P(Y|Z). In a binary-classification problem, - log P(Y|Z) = -(1 -
Y) log(1 — Y(Z)) — Y log(Y(Z)), where Y is the function of discriminator.
4
Under review as a conference paper at ICLR 2021
4.2	Target Distribution Derivation and Variational Lower B ound Construction
In the section we first derive the optimal target distribution in Equation 7, prove the optimality of this
distribution. Then, we optimize the distribution by constructing a variational lower bound.
We would like to solve the optimization problem in Equation 7:
maχ、Ex〜P(X),Z〜Pφ(z∣x)[J(Z； θ) - βlogPφ(Z∣X) + βlogPφ(Z)]∙	(10)
Pφ(ZIX )	×-----------{z---------} ×----{------}
L1 (θ,φ)	L2(φ)
Notice that L1 and L2 are both contained in the expectation. With the derivative of L1 and L2, setting
their summation to 0, we can see the optimal conditional probability as the following form:
Pφ(Z∣X) (X Pφ(Z)exp(1 J(Z; θ))	(11)
β
We provide a rigorous derivation of Equation 11 in Section B.2 of the appendix. We note that though
our derivation is over the representation space instead of the whole network parameter space, the
optimization problem Equation 10 and the resulting distribution Equation 11 are quite similar to
those studied in (Liu et al., 2017) in the context of Bayesian inference. However, we stress that our
formulations follow from the information bottleneck framework, and are mathematically different
from those in (Liu et al., 2017). In particular, the difference lies in the term L2, which depends on the
the distribution Pφ(Z | X) we need to optimize (while in (Liu et al., 2017), the corresponding term
is a fixed prior).
The following theorem shows that the distribution in Equation 11 is an optimal target distribution
(with respect to the IB objective L). The proof can be found in Section B.3 of the appendix.
Theorem 2. (Representation Improvement Theorem) Suppose we are given a fixed policy-value
parameter θ, representation distribution Pφ(Z|X) and state distribution P(X). Consider the objec-
tivefUnction L(θ, φ) = EX〜P(x),z〜p*(z∣x)[J(Z； θ)] 一 βI(X, Z; φ). Define a new representation
distribution: Pφ(Z∣X) X Pφ(Z) exp( 1J(Z; θ)). We have L(θ, φ) ≥ L(θ, φ).
Though we have derived the optimal target distribution, it is still difficult to compute Pφ(Z). In order
to resolve this problem, following VIB (Alemi et al., 2016), we construct a variational lower bound
with a prior distribution U(Z) which is independent of φ. Notice that dZPφ(Z) logPφ(Z) ≥
R dZPφ(Z) log U(Z), which means that EZ[logPφ(Z)] ≥ EZ [log U(Z)]. Now, we can derive a
lower bound of L(θ, φ) in Equation 6 as follows:
L(θ, φ) = Ex,z[J(Z； θ) - β logPφ(Z∣X)] + βEz[logPφ(Z)]
≥ Ex,z[J(Z； θ) - β logPφ(Z∣X)] + βEz[log U(Z)]
=Eχ,z[J (Z; θ) - β log Pφ(Z |X)+ β log U (Z)] = L(θ,φ),	(12)
where L(θ, φ) denotes the new lower bound loss. According to Theorem 2, the target distribution
that maximizes the lower bound is:
Pφ(Z|X) X U(Z)exp( 1J(Z； θ))∙	(13)
4.3	Optimization by Stein Variational Gradient Descent
Next we utilize the method in (Feng et al., 2017; Haarnoja et al., 2017) to optimize the lower bound.
Stein variational gradient descent (SVGD) is a non-parametric variational inference algorithm that
leverages efficient deterministic dynamics to transport a set of particles {Zi}in=1 to approximate given
target distribution Q(Z). We choose SVGD to optimize the lower bound because of its ability to
handle unnormalized target distributions such as Equation 13.
Briefly, SVGD iteratively updates the “particles” {Zi}n=1 via a direction function Φ*(∙) in the unit
ball of a reproducing kernel Hilbert space (RKHS) H:
Zi — Zi + eΦ* (Zi).	(14)
5
Under review as a conference paper at ICLR 2021
In Equation 14, Φ*(∙) is chosen as a direction to maximally decrease2 * the KL divergence between
^(
the particles distribution P(Z) and the target distribution Q(Z) = QCZ (Q is the unnormalized
distribution, C is the normalization constant) in the sense that
Φ?一
arg max
φ∈H
-dɪDKL(P[eφ] UQ) s∙t∙
kΦkH≤1
(15)
where P[Φ] is the distribution of Z + Φ(Z). (Liu & Wang, 2016) provided a closed form of this
direction:
Φ(Z)= EZj~p [K(Zj,Z)VZ logQ(Z) ∣^=zj +VZK(Z,Z) ∣^=zj ],	(16)
where K is a kernel function (typically an RBF kernel function). Notice that C does not appear in the
equation.
In our case, we seek to minimize
DKL (Pφ"X) || CUSeXP(βJ(∙; θ^),
where C = R dZU(Z) exp(βJ(Z; θ)). This is equivalent to maximizing L(θ, φ) (defined in EqUa-
tion 12). The greedy direction yields:
Φ(Z) = EZj [K(Zj, Z)VZ(1J(Z; θ) + log U(Z)) Iz=Zj +VzK(Z, Z) Iz=Zj ].	(17)
In practice we replace log U(Z) with ζ log U(Z) where ζ is a coefficient that controls the magnitude
of VZ log U(Z). Notice that Φ(Z) is the greedy direction that Z moves towards L(θ, φ) (defined
in Equation 12),s target distribution as shown in Equation 13 (distribution that maximizes L(θ, φ)).
This means Φ(Z) is the gradient of L(Z, θ, φ): dL(ZθM 8 Φ(Z).
Since our ultimate purpose is to update φ, by the chain rule, dL(Z,M a Φ(Z) ∂Z. Then for
ʌ , _	.、	—	-2,-	- -r
L(θ,φ) = Epφ(x,z)[L(Z, θ, φ)]:
"∂Φ") (X EX~p(x),z~Pφ(∙∣χ) φ(Z)∂φ] ,	(18)
Φ(Z) is given in Equation 17. In practice we update the policy-value parameter θ by common policy
gradient algorithm since:
∂L(θ,φ)	[∂J(Z； θ)
∂θ = Pφ(x,Z)	∂θ
(19)
and update representation parameter φ by Equation 18.
5	Experiments
In this section, we evaluate our proposed method SVIB (Stein Variational Information Bottleneck) in
the Atari domain. Extensive experiments have shown the following results: 1) our framework can
improve the sample efficiency of vanilla RL algorithms (A2C (Mnih et al., 2016) and PPO (Schulman
et al., 2017)); 2) our method outperforms VIB (Alemi et al., 2016) in 16 out of 20 Atari games; 3) we
run some simple experiments to show why our method works better: We show that our method is
capable of accelerating information E-C process in 3 games as mentioned in Section 1 (Appendix
C.4); 4) as MINE (Belghazi et al., 2018) is a popular method to estimate MI, we investigate the
effectiveness of using MINE to optimize IB in RL (Appendix C.5); 5) we study the impact of different
hyper-parameters β on the performance (Appendix C.6).
2In fact, Φ* is chosen to maximize the directional derivative of F(P) = -DKL(P||Q), which appears to be
the ”gradient” of F .
6
Under review as a conference paper at ICLR 2021
(d) PPO-All
(e) PPO-Pong β = 0.0016
(f) PPO-Qbert β = 0.0016
Figure 2: All reward curves are averaged over 3 random seeds, 10 episodes for each seed. Figure (a)
and (d) show the averaged normalized reward across 20 Atari games for A2C and 18 Atari games for
PPO. The rest four figures show the reward curves for two individual games, in which SVIB performs
better than vanilla A2C and PPO. The details of other games can be found in Appendix C.2.
5.1	Improving Sample Efficiency for A2C and PPO
In A2C with our SVIB framework (the settings of PPO are almost the same as those of A2C), we
sample Z from network φ(X, ξ) where ξ 〜N(∙; 0,0.1) and the number of samples from each state
X is 32. It turns out that the appropriate IB coefficient β varies among different games in our method,
which we will discuss in more detail in Appendix C.6. The β we use for SVIB can be seen in the
figures. We choose two prior distributions U (Z) of our framework. The first one is the uniform
distribution. The second one is a Gaussian distribution generated by the samples of representation Z.
Further details can be found in Appendix C.1. We implement the following four algorithms for A2C:
A2C:Vanilla A2C in Openai-baselines (Dhariwal et al., 2017) with φ(X) as the embedding function.
A2C-uSVIB: Use φ(X, ξ) as the embedding function, optimized by our framework with U(Z) being
uniform distribution. Pseudocode of the algorithm can be found in Appendix A.1.
A2C-GSVIB: Use φ(X, ξ) as the embedding function, optimized by our framework with U(Z) being
Gaussian distribution.
A2C-noise: A2C with the same embedding function φ(X, ξ) as A2C-uSVIB and A2C-GSVIB. We
design this algorithm for ablation study: Since our method needs additional noise as input while
vanilla A2C does not need the noise.
Same as for A2C, we also implemented four algorithms for PPO: PPO, PPO-uSVIB, PPO-GSVIB,
and PPO-noise. In our experiments, we find that in many games, PPO-GSVIB performs worse than
PPO-uSVIB, and PPO-noise worse than PPO. So we just show and compare the performance of
PPO-uSVIB and PPO. Figure 2 shows the reward curves of our method and the baselines. In Figure
2(a) and Figure 2(d), we normalize the episodic rewards to [0, 50] and then average them across 203
Atari games (18 Atari games for PPO). We can see that our method improves sample efficiency of
vanilla A2C and PPO in terms of normalized averaged rewards. The results of individual games can
be seen in Appendix C.1. Specifically, A2C with SVIB is more sample efficient than vanilla A2C in
15 out of 20 games. PPO-uSVIB is more sample efficient than vanilla PPO in 15 out of 18 games.
3 In fact, we have tested 35 games expect tough games like Pitfall and MontezumasRevenge. We find that
A2C in Openai-baselines (Dhariwal et al., 2017) only manages to converge in 20 games. Thus we choose these
20 games for showing the performance of our method and vanilla A2C.
7
Under review as a conference paper at ICLR 2021
Recall that in Figure 1 in the introduction section, we show that
the information extraction-compression phenomena exists in A2C
training process for several games, by visualizing the evolution of
mutual information I(X, Z). We also study the changes of I(X, Z)
for our method. In particular, we visualize I(X, Z) during the
training using A2C-uSVIB in Pong. See Figure 3. Not only the
information E-C process also exists for A2C-uSVIB, we can see
that A2C-uSVIB in fact extracts and compresses information faster
than vanilla A2C. It means that our method is able to accelerate
the information E-C process in Pong, thus improving the sample
efficiency. Further results and details can be found in Appendix C.4.
Figure 3: MI in Pong
5.2	Compare Our Method with VIB
We implement VIB based on A2C in RL. We name this algorithm ”A2C-VIB”. We have spent some
time to tune the parameter β for VIB, but we found that the default β = 0.001 in (Alemi et al., 2016)
works better than others. So we keep β = 0.001 for VIB in all games.
Figure 4 shows the performance of A2C-VIB and A2C-uSVIB. In Figure 4(a), we plot the normalized
averaged reward curves (average across 20 Atari games) for A2C-uSVIB and A2C-VIB, and Figure
4(b) and (c) plot the reward curves for two individual games Pong and Seaquest. More experiment
results for individual games can be found in Appendix C.3. From the figures, we can see that
A2C-uSVIB generally outperforms A2C-VIB (in average, and in 16 out of 20 individual games).
There is another way to solve general IB objectives: Using MINE (originally for estimating MI)
to optimize IB. Yet in RL, our empirical results show that it works even worse than vanilla RL
algorithms. Further discussion and results can be found in Appendix C.5. So among these methods
that can solve general IB objectives in RL, our method achieves the best performance.
Figure 4: All curves are averaged over 3 random seeds, 10 episodes for each seed. Figure (a) shows
the averaged normalized reward across 20 Atari games. The rest two figures show the reward for 2
example games reporting SVIB’s superior performance over VIB. Other individual games can be
found in Appendix C.3. The β in SVIB is as written in the figures, while it is fixed at 0.001 in VIB.
6	Conclusion
We propose an information-bottleneck-based framework for reinforcement learning and derive the
closed form of the optimal target distribution. We construct a lower bound and utilize amortized Stein
Variational gradient method to optimize it. We experimentally show that our framework can improve
the performance of vanilla RL algorithms. To the best of our knowledge, our method achieves the best
performance among information-bottleneck-based algorithms in reinforcement learning. Moreover, in
order to gain a better understanding that why our method works better, we verify that the information
extraction and compression process also exists in the training process of some simple games, and our
framework can accelerate this process.
8
Under review as a conference paper at ICLR 2021
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin
Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.
David Abel, Dilip Arumugam, Kavosh Asadi, Yuu Jinnai, Michael L Littman, and Lawson LS
Wong. State abstraction as compression in apprenticeship learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33,pp. 3134-3142, 2019.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint
arXiv:1801.04062, 2018.
Tessler Chen, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. arXiv
preprint arXiv:1805.11074, 2018.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized stein variational
gradient descent. arXiv preprint arXiv:1707.06626, 2017.
Tim Genewein, Felix Leibfried, Jordi Grau-Moya, and Daniel Alexander Braun. Bounded rationality,
abstraction, and hierarchical decision-making: An information-theoretic optimality principle.
Frontiers in Robotics and AI, 2:27, 2015.
Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft q-learning with mutual-information
regularization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=HyEtjoCqFX.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep
energy-based policies. Proceedings of the 34th International Conference on Machine Learning,
70:1352-1361, 2017.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=Bklr3j0cKX.
Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,
and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and
information bottleneck. In Advances in Neural Information Processing Systems, pp. 13978-13990,
2019.
Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi:
Exploration with mutual information. arXiv preprint arXiv:1810.01176, 2018.
Navneet Madhu Kumar. Empowerment-driven exploration using mutual information estimation.
arXiv preprint arXiv:1810.05533, 2018.
Felix Leibfried and Daniel A Braun. A reward-maximizing spiking neuron as a bounded rational
decision maker. Neural computation, 27(8):1686-1720, 2015.
Felix Leibfried and Jordi Grau-Moya. Mutual-information regularization in markov decision pro-
cesses and actor-critic learning. arXiv preprint arXiv:1909.05950, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in neural information processing systems, pp. 2378-2386, 2016.
Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. arXiv
preprint arXiv:1704.02399, 2017.
9
Under review as a conference paper at ICLR 2021
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, AleX Graves, and Koray Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In International Conference on Machine
Learning ,pp.1928-1937, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational
discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining
information flow. arXiv preprint arXiv:1810.00821, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. ProXimal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black boX of deep neural networks via information.
arXiv preprint arXiv:1703.00810, 2017.
Christopher A Sims. Implications of rational inattention. Journal of monetary Economics, 50(3):
665-690, 2003.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement
learning. Theory in Biosciences, 131(3):139-148, 2012.
Jakub Sygnowski and Henryk Michalewski. Learning from the memory of atari 2600. arXiv preprint
arXiv:1605.01335, 2016.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method.
University of Illinois, 411(29-30):368-377, 2000.
Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning
efficient multi-agent communication: An information bottleneck approach. arXiv preprint
arXiv:1911.06992, 2019.
A Appendix of algorithm
A.1 Algorithm
Algorithm 11nformation-bottleneck-based state abstraction in RL
θ,φ J initialize network parameters
β,Z J initialize hyper-parameters
J learning rate
M J number of samples from Pφ(∙∣X)
repeat
Draw a batch of data {Xt, at, Rt, Xt+1}tn=1 from environment
for each Xt ∈ {Xt}tn=1 do
DraWM samples {Zt}M=1 from Pφ(∙∣Xt)
end for
Get the batch of data D = {Xt, {Zit}iM=1, at, Rt, Xt+1}tn=1
Compute the representation gradients VφL(θ, φ) in D
Compute the RL gradients VL(θ, φ) in D
Update φ: φ J φ + VφL(θ, φ)
Update θ: θ J θ + VθL(θ, φ)
until Convergence
10
Under review as a conference paper at ICLR 2021
B Appendix of theorems and derivations
B.1 Proof of Theorem 1
Theorem. (Theorem 1 restated)Policy πr = πθr, parameter φr, optimal policy π? = πθ? and its
relevant representation parameter φ? are defined as follows:
θr,φr = argmφEPφ(x,z) log PP(ZX - βJ(Z；θ) ；	QO)
θ*,φ* = arg min Epφ(χ,z) - 1 J(Z; θ).
θ,φ	β
(21)
θ,φ
Define Jπr as EPφr (X,Z) [J(Z; θr)] and Jπ? as EPφ? (X,Z) [J(Z; θ?)]. Assume that there exists an
e > 0, ∣I(X, Z; φ?) 一 I(X, Z; φr)∣ < β, we have ∣ Jπr — Jπ? ∣ < 匕 Specifically, in value-based
algorithm, this theorem also holds between expectation of two value functions.
Proof. From Equation 20 we can get:
I(X, Z; φ?) - 1 Jπ? ≥ I(X, Z; φr) - 1Jπ
ββ
r
(22)
From Equation 21 we can get:
-1Jπr ≥ - 1 Jπ?
β ~ β
These two equations give us the following inequality:
β(I(X, Z; φ?) - I(X, Z; φr)) ≥ Jπ? -Jπr ≥0
According to the assumption, naturally we have:
IJπr - Jπ? | < e
(23)
(24)
(25)
Notice that if we use our IB framework in value-based algorithm, then the objective function Jπ can
be defined as:
Jπ = Vπ = (1- γ)-1
(1- γ)-1
dXdπ(X)Rπ(X)
X
/ dXdπ(X升/ dZPφ(Z|X)Rπ(Z)]
(26)
where Rn(Z) = JX∈{Xο .0(Xo)=Z、} dXRπ (X) and dπ is the discounted future state distribution,
readers can find detailed definition of dπ in the appendix of (Chen et al., 2018). We can get:
|Vπr - Vπ? | < e
(27)
B.2 Target Distribution Derivation
We show the rigorous derivation of the target distribution.
Denote P as the distribution of X, PφZ(Z) = Pφ(Z) as the distribution of Z. We use Pφ as the short
hand notation for the conditional distribution Pφ(Z∣X). Moreover, We write L(θ, φ) = L(θ, Pφ) and
hp, qiχ = R dXp(X)q(X). Notice that PZ(Z) = (P(∙),Pφ(Z∣∙)}χ. Take the functional derivative
with respect to Pφ of the first term L1:
/ δL1(θ,pφ) φ∖
∖	δPφ	X XZ
=Z dZdXδL1δPP∣(X X)) Φ(Z,X )= [ 2 Lι(θ,Pφ + 的
δPφ (Z |X )	d	=0
dɪ / dXP(X )Dpφ(Jx ) + eφ(∙, x ),j (∙; θ) - β log(Pφ"X)+ eφ(∙, x A)2
/dXP(X) (Φ(∙,X),J(∙;θ) - βlogPφ(∙∣X)) + <Pφ(∙∣X), -βP^X)) EZ
DP (∙)[J (∙; θ) -β log pφO) - β], φ(∙, ∙))χz
=0
11
Under review as a conference paper at ICLR 2021
Hence, we can see that
δL1(θ, Pφ)
δPφ(Z∣X)
P(X)[J(Z; Θ) - βlogPφ(Z∣x) - β].
(28)
Then we consider the second term. By the chain rule of functional derivative, we have that
δL2(θ, Pφ)
δPφ(Z∣X)
∕δL2(θ,Pφ) δPZ S ∖ = β∕l+log P Z ()
∖ δPφ(∙) , δPφ(Z|X)/Z = β∖ + g φ (),
δPφ (∙)
δPφ(Z∣X)
/ Z
=β/dZ(1+logPZ(Z))δ(Z - Z)P(X) = βP(X)(1 + logPZ(Z))
Combining the derivative of L1 and L2 and setting their summation to 0, we can get that
Pφ(Z|X) X Pφ(Z)exp(1 J(Z; θ))
β
(29)
(30)
B.3 Proof of Theorem 2
Theorem. (Theorem 2 restated)For L(θ, φ) = EX〜P(x),z〜Pφ(z∣X)[J(Z； θ)] 一 βI(X, Z; φ), given
a fixed policy-value parameter θ, representation distribution Pφ(Z |X) and state distribution P (X),
define a new representation distribution:Pφ(Z|X) x Pφ(Z)exp( 1J(Z; θ)), we have L(θ,φ) ≥
L(θ, φ).
Proof. Define I(X) as:
I(X)= [ dZPφ(Z|X)= [ dZPφ(Z)exp(1 J(Z;θ))
Z	Zβ
…八 P E	E	J	Pφ(Z) exp( 1J (Z; θ))
L(θ,φ) = EX{EZ〜Pφ(z∣X)J(Z; θ)] - βEz〜Pφ(z∣X)[log-T(V∖P (7∖---
φ	φ	i (X )Pφ(Z)
=EX{eEZ〜Pφ(Z∣X)[log I(X)] - βEZ〜Pφ(Z∣X)[log ∕φ>J]]}
Pφ(Z)
(31)
]}
βEX [log I(X)] - βEX,Z〜P^(X,Z)[log Sf /]
pφ(Z)
βEχ[logI(X)] - βEz〜Pφ(z) [log
Pφ(Z)
Pφ(Z)
]
=βEχ 〜P (X) [log I (X)] + β Dkl(P^(Z )∣∣Pφ(Z))
L(θ, φ) = EX{βEZ-Pφ(Z∣X)[logexp(万 J(Z; θ))] + βEZ〜Pφ(Z∣X) log p φi∣ ɪʌ
β	Pφ (Z |X )
(32)
}
EX {eEZ 〜Pφ(Z∣X)[log
Pφ(Z)exp( 1J(Z； θ))
Pφ (Z ∣X )I (X)
] +βlogI(X)}
βEX [log I(X)] + βEX〜P(X),Z〜Pφ(Z∣X)[log
Pφ(z∣X)
Pφ (Z |X)
]
=βEX〜P(X)[logI(X)] - βEX〜P(X)[Dkl(Pφ(Z|X)∣∣Pφ(Z∣X))]
ʌ. ... .. - .......................................... ..,
L(θ,φ) - L(θ,φ)= βDκL(Pφ(Z)∣∣Pφ(Z))+ βEX〜P(x)[Dkl(Pφ(Z∣X)∣P^(Z∣X))]
A	1 ∙	.	. 1	■.	. r . 1 TZr 1 ∙	1	r∕c7∖、T∕c∕∖
According to the positivity of the KL-divergence, we have L(θ, φ) ≥ L(θ, φ).
(33)
(34)
C Experimental Results
C.1 Experiment Settings
In A2C With our SVIB framework, We sample Z from network φ(X, ξ) where ξ 〜N(∙; 0,0.1) and
the number of samples from each state X is 32. It turns out that the appropriate IB coefficient β
12
Under review as a conference paper at ICLR 2021
varies among different games in our method. The β we use for SVIB can be seen in the figures.
We choose two prior distributions U(Z) of our framework. The first one is the uniform distribution.
Apparently, when U(Z) is the uniform distribution, NZ log U(Z) |夕=2 can be omitted. The second
one is the Gaussian distribution, which is defined as follows: for a given state Xi , sample a batch of
{Zj}n=ι, then define U(Z) = N(Z; μ = 1 Pn=I Zj, σ2 = 1 Pn=I(Zj — μ)2), in which n = 32.
The setting of this Gaussian distribution is a little bit heuristic: We roughly assume that a Gaussian
distribution generated by the samples of {Zji}jn=1 can approximate the true representation distribution
Pφ(Z). Yet later we find that Gaussian distribution performs nearly the same as uniform distribution
in A2C with our framework, and in PPO with our framework, uniform distribution is even better than
Gaussian distribution.
To calculate Φ(Zi) (defined in Equation 17 in Section 4.3), we replace log U(Z) with ζ log U(Z) to
control the magnitude of NZ log U(Z). We set Z as 0.005步夕ɪ J(Z; θ)∕V;^ log U(Z)k ∣z=z.
Following (Liu et al., 2017), the kernel function we use for Φ(Zi) is Gaussian RBF kernel
K(Zi, Zj) = exp(-kZi - Zj k2/h) where h = med2/2 log(n + 1), med denotes the median
of pairwise distances between the particles {Zji}jn=1. As for the hyper-parameters in RL, we simply
choose the default parameters in A2C of Openai-baselines (Dhariwal et al., 2017). The number of
total time steps is 14 million (56 million frames specifically) except that the number of time steps
for Pong is 7 million, since the agent in Pong converges to the optimal score much faster than other
games.
The settings of PPO are almost the same as those of A2C. Here are some differences: 1)The number
of samples from each state is 26. 2)Since PPO converges faster than A2C in most games, we set the
number of total time steps to be 10 million (40 million frames) except 5 million in Pong. And we find
that in some games, PPO-GSVIB is worse than PPO-uSVIB, and PPO-noise is worse than regular
PPO. So we just compare the performance of PPO-uSVIB and PPO.
C.2 Individual Atari games for A2C and PPO
Figure 5, 6, 7 and 8 show the cumulative reward curves. We use exponential moving average to
smooth each curve (In Pong, we move the y-axis up 21 units so that the lowest score is 0, which is
convenient to make exponential moving average). All curves are averaged over 3 random seeds, 10
episodes for each seed.
Figure 5 and 6 show the cumulative rewards of four A2C-based algorithms. The yellow curve and
red curve show the performance of A2C with our method. We can see that in MsPacman, Carnival,
SpaceInvaders, DemonAttack and YarsRevenge (Figure 5(d), (h), (i) and Figure 6(e), (f)), A2C with
our method performs nearly same as vanilla A2C. While in other 15 games, our method improves
sample efficiency of vanilla A2C. Figure 7 and 8 show the cumulative rewards of two PPO-based
algorithms. The yellow curve shows the performance of PPO with our method. We can see that in
MsPacman, Seaquest, Asterix (Figure 7(c) and Figure 8(d), (f)), PPO with our method performs
nearly same as vanilla PPO. While in other 15 games, our method improves sample efficiency of
vanilla PPO.
C.3 Individual Atari games for VIB
Figure 9 and 10 show the cumulative rewards of A2C-VIB and A2C-uSVIB. The yellow curve shows
the performance of A2C-uSVIB. We can see that in Qbert, Assault, CrazyClimber and YarsRevenge
(Figure 9(b) and Figure 10(a), (l), (n)), A2C-uSVIB performs nearly same as A2C-VIB. While in
other 16 games, A2C-uSVIB is more sample efficient than A2C-VIB.
C.4 SVIB accelerates the information E-C process
This section we verify that the information E-C process exists in several Atari games when we train
agents using A2C (using MINE to estimate the mutual information) and our framework accelerates
this process.
Mutual information neural estimation (MINE) (Belghazi et al., 2018) is an algorithm that can estimate
mutual information (MI) between two high dimensional random variables more accurately and
13
Under review as a conference paper at ICLR 2021
algorithm
----A2C-noise
—A2C-uSVIB
----A2C
----A2C-GSVIB
p-JroMφ.Γ6>e
p-JroMφ.Γ6>e
episode
(b) A2C-Qbert β = 0.001
(c) A2C-BeamRider β = 0.001
(a) A2C-Pong β = 0.001
algorithm
----A2C-noise
一 A2C-uSVIB
----A2C
----A2C-GSVIB
p-JroMφ.Γ6>e
0	25	50	75	100	125	150	175
episode
(f)	A2C-Asteroids β = 0.001
0	25	50	75	100	125	150	175
episode
(e)	A2C-Breakout β = 0.001
0	25	50	75	100	125	150	175
episode
(d)	A2C-MsPacman β = 0.001
algorithm
----A2C-noise
一 A2C-uSVIB
----A2C
----A2C-GSVIB
algorithm
----A2C-noise
一 A2C-uSVIB
----A2C
----A2C-GSVIB
(g) A2C-Assault β = 0.0006
(h) A2C-Carnival β = 0.0006
(i) A2C-SpaceInvaders β = 0.0006
algorithm
----A2C-noise
一 A2C-uSVIB
----A2C
----A2C-GSVIB
A2C-uSVIB
0	25	50	75	100	125	150	175
episode
(l)	A2C-Alien β = 0.0006
algorithm
A2C-GSVIB
0	25	50	75	100	125	150	175
episode
(j)	A2C-AirRaid β = 0.0006
episode
(k)	A2C-BankHeist β = 0.0006
algorithm
A2C-noise
0	25	50	75	100	125	150	175
episode
(m)	A2C-Atlantis β = 0.0006
A2C-uSVIB
0	25	50	75	100	125	150	175
episode
(o) A2C-Seaquest β = 0.0006
0	25	50	75	100	125	150	175
episode
(n) A2C-StarGunner β = 0.0006
algorithm
----A2C-noise
—A2C-uSVIB
Figure 5: Cumulative reward curves on A2C. All reward curves are averaged over 3 random seeds,
10 episodes for each seed.
14
Under review as a conference paper at ICLR 2021
(d) A2C-DemonAttack β = 0.0006
(e) A2C-YarsRevenge β = 0.0006
(a)	A2C-Amidar β = 0.0006
(b)	A2C-Asterix β = 0.0006
(c)	A2C-CrazyClimber β = 0.0006
Figure 6: Cumulative reward curves on A2C. All reward curves are averaged over 3 random seeds,
10 episodes for each seed.
p」PM ①」I6>Π5
20l
P」«3mqj」i6>«5
(a) PPO-Pong β = 0.0016
(b) PPO-Qbert β = 0.0016
p」PM<y」l6>p
O 20	40	60	80 IOO 120	140	160
episode
(e)	PPO-Asteroids β = 0.0016
O 20	40	60	80 IOO 120	140	160
episode
(d)	PPO-Breakout β = 0.001
(c) PPO-MsPacman β = 0.0006
(f) PPO-Assault β = 0.0016
algorithm
----PPO
——PPO-uSVIB
algorithm
20	40	60	80	100	120	140	160
episode
20	40	60	80 IOO 120	140	160
episode
algorithm
40	60	80 IOO 120	140	160
episode
(g)	PPO-Carnival β = 0.0016
(h)	PPO-SpaceInvaders β = 0.0016
(i)	PPO-AirRaid β = 0.8
Figure 7: Cumulative reward curves on PPO. All reward curves are averaged over 3 random seeds, 10
episodes for each seed.
15
Under review as a conference paper at ICLR 2021
P」(oMa>」l6>ro
P」(aMcu」l6>p
(a) PPO-Alien β = 0.002	(b) PPO-Atlantis β = 0.003
(c) PPO-StarGunner β = 0.0016
P」PMCU」l6>B
0	20	40	60	80 IOo 120	140	160
episode
(d) PPO-Seaquest β = 0.0016
O 20	40	60	80 IOO 120	140	160
episode
(e) PPO-Amidar β = 0.0016
algorithm
——PPO
——PPO-uSVIB
O 20	40	60	80 IOO 120	140	160
episode
(f) PPO-Asterix β = 0.0016
episode
(g) PPO-CrazyClimber β = 0.0016
(i) PPO-YarsRevenge β = 0.0016
4035302520115
P」PMa)」l6>ro
algorithm
----A2C-VIB
——A2C-uSVIB
Figure 8: Cumulative reward curves on PPO. All reward curves are averaged over 3 random seeds, 10
episodes for each seed.
algorithm
A2C-VIB
A2C-uSVIB
algorithm
A2C-VIB
A2C-uSVIB
25	50	75 IOO 125	150	175
episode
75	100	125	150	175
episode
(b) A2C-Qbert β = 0.001
(c) A2C-BeαmRider β = 0.001
episode
(a) A2C-Pong β = 0.001
P-IΓ□MΦ,Γ6>P
(d) A2C-MsPacman β = 0.001
(e) A2C-Breakout β = 0.001
(f) A2C-Asteroids β = 0.001
Figure 9: Cumulative reward curves of A2C-VIB and A2C-uSVIB. All reward curves are averaged
over 3 random seeds, 10 episodes for each seed.
16
Under review as a conference paper at ICLR 2021
algorithm
A2C-VIB
A2C-uSVIB
algorithm
A2C-VIB
A2C-uSVIB
algorithm
A2C-VIB
A2C-uSVIB
25	50	75	100	125	150	175
episode
25	50	75	100	125	150	175
episode
25	50	75 LOO 125	150	175
episode
(a) A2C-Assault β = 0.0006	(b) A2C-Carnival β = 0.0006	(c) A2C-SpaceInvaders β = 0.0006
algorithm
A2C-VIB
A2C-uSVIB
algorithm
A2C-VIB
A2C-uSVIB
algorithm
----A2C-VIB
——A2C-uSVIB
75	100	125	150	175
episode
20	40	60	80
episode
75 IOO 125	150	175
episode
(d)	A2C-AirRaid β = 0.0006
(e)	A2C-BankHeist β = 0.0006
(f)	A2C-Alien β = 0.0006
algorithm
A2C-VIB
A2C-uSVIB
p」(aM(u」l6>p
Figure 10: Cumulative reward curves of A2C-VIB and A2C-uSVIB. All reward curves are averaged
over 3 random seeds, 10 episodes for each seed.
17
Under review as a conference paper at ICLR 2021
efficiently. Specifically, for random variables X and Z, assume T to be a function of X and Z,
the calculation of I(X, Z) can be transformed to the following optimization problem according to
(Belghazi et al., 2018):
I(X,Z) = maxEP(X,Z) [T] - log(EP(X)BP(Z) [expT]).	(35)
The optimal function T? (X, Z) can be approximated by updating a neural network T(X, Z; η).
With the aid of this powerful tool, we would like to visualize the mutual information (MI) between
input state X and its relative representation Z: Every a few update steps, we sample a batch of inputs
and their relevant representations {Xi, Zi}in==164 and compute their MI with MINE. The learning
rate of updating η is 0.0007 and the number of training steps is 256. Every time we train MINE
(update η), we just shuffle {Zi}in=1 and roughly assume the shuffled representations {Zishuffled}in=1 to
be independent with {Xi}in=1:
nn
I(X，Z) ≈ mnιx n X[T (Xi，ZZin)] - log( n X[eχpT(X”叫).
(36)
episode
(a) MI in Pong
algorithm
——A2C
—A2C-uSVIB
SSO-OjU- -QnanE
60	80	100	120
episode
algorithm
——A2C
—A2C-uSVIB
20	40	60	80	100	120
episode
(c) MI in Assault
(b) MI in MsPacman
Figure 11: MI estimation in 3 Atari games.
Figure 11 shows the mutual information estimation between X and Z in 3 Atari games. The x-axis is
the number of update steps and the y-axis is the MI estimation. As we can see, in both A2C-uSVIB
and vanilla A2C, the MI first increases to encode more information from inputs (”remember” the
inputs), then decreases to drop irrelevant information from inputs (”forget” the useless information).
And clearly, A2C-uSVIB extracts faster and compresses faster than regular A2C in all 3 games.
C.5 Optimize IB with MINE in RL
This section we show the performance of using MINE as an IB optimizer in RL. And discuss the
possible reasons that may cause the bad performance.
According to Equation 35, the whole optimization problem can be written as follows:
L(θ, φ, η) = EPφ(X,Z)[J(Z; θ)]
' {z '
RL loss term
-βEpφ(x,z)[T(X, Z； η)] + β log(Ep(x)0Pφ(Z)[expτ(XZn)]),	(37)
where η is the parameter of the function T . Then the optimal parameters of this optimization problem
are:
θ?, φ?, η? = arg max arg min L(θ, φ, η).	(38)
θ,φ	η
Thus the key steps to optimize L(θ, φ, η) is to update θ, φ, η iteratively as follows:
ηt+ι — argmnin -β EPφt (χ,z) [T (X,Z ； nt)] + β log(Ep (χ)8Pφt(z)[expT (X,Z ;nt)]),	(39)
θt+1,φt+1 - arg max L(θt, φt,ηt+ι).	(40)
θt,φt
We call this algorithm ”MINE-IB”. Unfortunately, though MINE is a powerful tool as a mutual
information estimator, MINE-IB is quite unstable and may be even hard to converge in reinforcement
18
Under review as a conference paper at ICLR 2021
learning. We implement MINE-IB based on A2C (named ”A2C-MINE-IB”). Figure 12 shows the
cumulative reward of A2C-MINE-IB and vanilla A2C in 9 Atari games. We can see that A2C-MINE-
IB performs much worse than vanilla A2C. In some games, e.g., Pong and BankHeist (Figure 12(a)
and (d)), and the performance of A2C-MINE-IB is nearly same as a random policy.
algorithm
35'——A2C
3o-——A2C-MINE-IB
episode	episode	episode
(a) Pong	(b) BeamRider	(c) Alien
algorithm
——A2C
—A2C-MINE-IB
algorithm
——A2C
—A2C-MINE-IB
algorithm
——A2C
—A2C-MINE-IB
>	50	75	100	125	150	175
episode
(d) BankHeist
5	50	75	100	125	150	175
episode
(e) StarGunner
50	75	100	125	150	175
episode
(f) Breakout
episode	episode	episode
(g) Carnival	(h) SpaceInvaders	(i) Qbert
Figure 12: Cumulative reward curves of A2C-MINE-IB and A2C. All reward curves are averaged
over 3 random seeds, 10 episodes for each seed.
SSoO1JU ∩ QrQnE
algorithm
——A2C
—A2C-MINE-IB
We suspect the following two reasons that may cause such performance:
1.	MINE suffers from exponential variance according to (Song & Ermon, 2019). Considering
training the regular RL objective is already quite difficult and somewhat unstable, if we
combine RL objective with MINE as a mutual information regularizer, the training becomes
even more unstable.
2.	According to Equation 38, optimizing IB with MINE is a min-max optimization problem.
In deep learning, this kind of problems are extremely hard to converge.
We have mentioned that MINE is a useful tool for mutual information maximization according to
(Hjelm et al., 2019). Since mutual information maximization with MINE is a maximization opti-
mization problem. Here is an example about the difference of MI maximization and IB optimization
with MINE. Suppose X is the input, Z = f (X, ; ξ) is the representation, where f is the embedding
function, ξ is the noise and φ is the parameter of f . If we want to maximize the MI between X and
Z using MINE, then the optimization problem can be written as follows:
mφaχ{EPφt(x,z) [T(X, Z; nt)] - log(EP(X)0Pφt (Z) [expT(XZnt)Dh
(41)
19
Under review as a conference paper at ICLR 2021
which is easy to optimize in deep learning as shown in (Hjelm et al., 2019). While if we want
to minimize the MI under some constraints g(Z; φ) (assume that we want to maximize g(φ), like
Equation 38), then the optimization problem can be written as follows:
mφaxmin{-EPφt (X,Z)[T(X,Z； ηt)] + log(EP(X)0Pφt (Z) [exPT(XZnt)]) + g(φ)},	(42)
which is extremely hard to optimize in deep learning.
C.6 THE IMPACT OF β
This section we show the impact of hyper-parameter β.
It turns out that that the proper IB coefficient β varies among different games in our method. Either
large or small β will yield bad performance. Figure 13 shows the cumulative reward curves of A2C-
uSVIB with different β. In Figure 13(a), the proper β in Breakout is 0.001, either the performance
of 0.002 or 0.0006 is worse than 0.001. While in Figure 13(b), the proper β in Breakout is 0.0006,
whose performance is better than 0.002 and 0.0003.
(a) Breakout
Figure 13: Cumulative reward curves of A2C-uSVIB across different hyper-parameter β. All reward
curves are averaged over 3 random seeds, 10 episodes for each seed.
(b) SpaceInvaders
20