Correspondence between neuroevolution and gradient descent
StePhen Whitelam1,* Viktor Selin2, Sang-Won Park1, and Isaac Tamblyn2,3,4*
1Molecular Foundry, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720, USA
2Department of Physics, University of Ottawa, ON, K1N 6N5, Canada
2National Research Council of Canada, Ottawa, ON K1N 5A2, Canada
3 Vector Institute for Artificial Intelligence, Toronto, ON M5G 1M1, Canada
We show analytically that training a neural network by stochastic mutation or “neuroevolution”
of its weights is equivalent, in the limit of small mutations, to gradient descent on the loss function
in the presence of Gaussian white noise. Averaged over independent realizations of the learning
process, neuroevolution is equivalent to gradient descent on the loss function. We use numerical
simulation to show that this correspondence can be observed for finite mutations. Our results provide
a connection between two distinct types of neural-network training, and provide justification for the
empirical success of neuroevolution.
I.	INTRODUCTION
In broad terms there are two tyPes of method used to
train neural networks, divided according to whether or
not they exPlicitly evaluate gradients of the loss function.
Gradient-based methods descend from the backProPaga-
tion algorithm [1-5]. In simple gradient descent, the pa-
rameters (weights and biases) x of a network evolve with
time according to the prescription X = -αVU(x), where
a is a learning rate and VU(x) is the gradient of a loss
function U(x) with respect to the network parameters.
The non-gradient-based methods are stochastic processes
in which changes to a neural network are proposed and
accepted with certain probabilities, and are related to
Monte Carlo [6, 7] and genetic algorithms [8—10]. Here
we focus on perhaps the simplest way of stochastically
training a neural network, mutation or “neuroevolution”
of its parameters [11] [10, 12—15]. Both gradient-based
and non-gradient-based methods have been used to train
neural networks for a variety of applications, and, where
comparison exists, perform similarly well [14, 16-18].
Here we point out that there is a fundamental connec-
tion between gradient descent and neuroevolution. The
connection follows from one identified in the 1990s be-
tween the overdamped Langevin dynamics and Metropo-
lis Monte Carlo dynamics of a particle in an external
potential [19, 20]. In the limit of small Monte Carlo trial
moves, those things are equivalent. Similarly, neuroevo-
lution in the limit of small mutations is equivalent to
noisy gradient descent on the loss function. Specifically,
let us propose a mutation x → x + of all neural-network
parameters, where is a set of independent Gaussian
random numbers of zero mean and variance σ2 , and let
us accept the proposal with the Metropolis probability
min (1, exp[-β ∆U]), a common choice in the physics lit-
erature [6, 7, 21]. Here β is a reciprocal evolutionary tem-
perature, and ∆U is the change of the loss function under
the proposal. When β 6= ∞, the weights of the network
* SWhitelam@lbl.gov
t isaac.tamblyn@nrc.ca
evolve, to leading order in σ, as X 二 一 (βσ2∕2)VU(x)
plus Gaussian white noise. Averaged over independent
realizations of the learning process, simple neuroevolu-
tion is therefore equivalent to simple gradient descent,
with learning rate βσ2 /2. In the limit β = ∞, where
mutations are only accepted if the loss function does not
increase, weights under neuroevolution evolve instead as
X = — (σ∕√2π)∣VU(x)∣-1VU(x) plus Gaussian white
noise, which corresponds to a form of gradient descent
on U (X).
We summarize this neuroevolution-gradient descent
correspondence in Section II, and derive it in Section III.
Our derivation follows the ideas developed in Ref. [20],
with changes made to account for the different set-up [22]
and proposal rates considered in that work and ours, and
our interest in the limit β = ∞. It is natural to associate
the state X of the neural network with the position of a
particle in a high-dimensional space, and the loss function
U(X) with an external potential. The result is a rewriting
of the correspondence between Langevin dynamics and
Monte Carlo dynamics as a correspondence between the
simplest forms of gradient descent and neuroevolution.
Just as the Langevin-Monte Carlo correspondence pro-
vides a basis for understanding why Monte Carlo simula-
tions of particles can approximate real dynamics [23—29],
so the neuroevolution-gradient descent correspondence
shows how we can effectively perform gradient descent
on the loss function without explicit calculation of gradi-
ents. The correspondence holds exactly only in the limit
of vanishing mutation scale, but we use numerics to show
in Section IV that it can be observed for neuroevolution
done with finite mutations and gradient descent enacted
with a finite timestep. We conclude in Section V.
II.	SUMMARY OF MAIN RESULTS
In this section we summarize the main analytic results
of this paper.
Consider a neural network with N parameters (weights
and biases) X = {x1, . . . , xN}, and a loss U(X) that is a
function of the network parameters. The precise archi-
2
tecture of the network is not important. The loss func-
tion may also depend upon other parameters, such as a
set of training data, as in supervised learning, or a set
of decisions and states, as in reinforcement learning; the
correspondence we shall describe applies regardless.
white noise. Averaging over independent stochastic tra-
jectories of the learning process (starting from identical
initial conditions) gives
dhxii	βσ2 ∂U(x)
-----=	----------
dt---2 ∂xi
(5)
A. Gradient descent
Under the simplest form of gradient descent, the pa-
rameters xi of the network evolve according to numerical
integration of
dxi	∂U (x)
---=一α——二-----
dt	∂xi
(1)
which has the same form as the gradient descent equa-
tion (1). Thus, when averaged over many independent
realizations of the learning process, the neuroevolution
procedure 1-4, with non-infinite β, is equivalent in the
limit of small mutation scale to gradient descent on the
loss function.
In the case β = ∞, where mutations are only accepted
if the loss function does not increase, the parameters of
the network evolve according to the Langevin equation
Here time t measures the progress of training, and α is
the learning rate [1-5].
dxi	σ 1	∂U(x)
dT = -√2∏ ∣w (χ)∣ Fr + ηi(t)
(6)
B. Neuroevolution
where η is a Gaussian white noise with zero mean and
variance σ2∕2:
Now consider training the network by neuroevolution,
defined by the following Monte Carlo protocol.
1.	Initialize the neural-network parameters x and cal-
culate the loss function U (x). Set time t = 0.
2.	Propose a change (or “mutation”) of each neural-
network parameter by an independent Gaussian
random number of zero mean and variance σ2 , so
that
x → x + ,	(2)
where e = {6ι,..., EN} and Ei 〜N(0, σ2).
3.	Accept the mutation with the Metropolis probabil-
ity min(1,e-β[U(X+W)-U(X)]), and otherwise reject
it. In the latter case we return to the original neu-
ral network. The parameter β can be considered to
be a reciprocal evolutionary temperature.
4.	Increment time t → t + 1, and return to step 2.
hηi (t)i = 0,
The form (6) is
dient in the first
2
hηi(t)% (t0)i = ^2- δij δ(t — t0)∙	⑺
different to (3), because the gra-
term is normalized by the factor
|VU(x) | = JPN= ι(∂U(x)∕∂xi)2, which serves as an ef-
fective coordinate-dependent rescaling of the timestep,
but (6) nonetheless describes a form of gradient descent
on the loss function U (x). Note that the drift term in
(6) is of lower order in σ than the diffusion term (which
is not the case for non-infinite β ). In the limit of small σ ,
(6) describes an effective process in which uphill moves
in loss cannot be made, consistent with the stochastic
process from which it is derived.
Averaged over independent realizations of the learning
process, (6) reads
dhxii	σ 1	∂U(x)
-----=	.  ------:———：----
dt---√2π VU(x)l ∂xi
(8)
For non-infinite β , and in the limit of small mutation
scale σ, the parameters of the neural network evolve un-
der this procedure according to the Langevin equation
dxi
dt
βσ2 ∂U(x)
2 ∂xi
+ ξi(t),
(3)
The results (3) and (6) show that training a network by
making random mutations of its parameters is, in the
limit of small mutations, equivalent to noisy gradient de-
scent on the loss function.
Writing U(X) = X ∙ VU(x), using (3) and (6), and
averaging over noise shows the evolution of the mean loss
function under neuroevolution to obey, in the limit of
small σ,
where ξ is a Gaussian white noise with zero mean and
2
variance σ 2 :
*
hU(x)i
[一 βσ2 (vu (X))2
I-MVU (X)I
if β 6= ∞
if β = ∞
(9)
hξi(t)i =0,	hξi(t)ξj(t0)i = σ2δijδ(t- t0).	⑷
Eq. (3) describes an evolution of the neural-network pa-
rameters xi that is equivalent to gradient descent with
learning rate ɑ = βσ2∕2 in the presence of Gaussian
equivalent to evolution under the noise-free forms of gra-
dient descent (5) and (8).
In the following section we derive the correspondence
described here; in Section IV we show that it can be ob-
served numerically for non-vanishing mutations and finite
integration steps.
3
III. DERIVATION OF (3) AND (6)
A. Non-infinite β
The master equation [30, 31] for the neuroevolution
procedure defined in Section II B is
∂tP (x, t) = d [P (x - , t)W(x - )
- P (x, t)W(x)] .
(10)
Here P (x, t) is the probability that the network has
the set of parameters x at time t (time being propor-
tional to the number of steps of the procedure); d =
R∞∞ dei …RzO deN ； and
In this section we consider non-infinite β , in which
case we can evaluate (16) and (17) using the results of
Refs. [19, 20] (making small changes in order to account
for differences in proposal rates between those papers and
ours).
Eq. (16) can be evaluated as
W(x) = p() min 1, e-β[U(x+)-U(x)]	(11)
is the rate for going from the set of parameters x to the
set of parameters x + . The first factor in (11) is
N
p() =	p(ei)
i=1
with
p(ei)
(12)
and accounts for the probability of proposing the set of
Gaussian random numbers ； the “min” function in (11)
enforces the Metropolis acceptance rate.
We can pass from the master equation (10) to a Fokker-
Planck equation by assuming a small mutation scale σ,
and expanding the terms in (10) to second order in σ [30,
31]. For example,
P(X - Gt) ≈(1 - e ∙ V + 2(e • ▽)) P(x,t),	(13)
where e ∙ V = PN=I ci∂i (note that ∂i ≡ ∂χ^). Expanding
W(x - ) in the same way and collecting terms gives
∂tP(x, t) ≈ -
/ de(e ∙ V)P(x,t)We(x)
+ 2/ de(e ∙ V)2P (x,t)We(x).
Ai(X) =	de p(e)ei min 1, e-β[U(x+)-U(x)]	(18)
≈ / dep(e)ei min (1, 1 — βe ∙ VU)	(19)
=/ dep(e)eiΘ(-e ∙ VU) + / dep(e)ei(1 - βe∙ VU)Θ(e ∙ VU)	(20)
= de p(e)ei -β / dep(e)Θ(e ∙ VU) X ei ej∂jU j	(21)
=-β / dep(e)Θ(e ∙VU) X eiej∂jU j	(22)
=-2 Jde p(e) χ eiej dj U	(23)
_ βσ2 =-F diU.	(24)
Taking the integrals inside the sums, (14) reads
N∂
∂tP(x,t) ≈-V 厂(Ai(X)P (x,t))
i=1 ∂xi
where
1 N ∂2
+ 2 X ∂xi∂xj(Bj (X)P(X,力)),
i,j =1
Ai(X) ≡ deiW(X),
and
Bij(X) ≡
d eiejW(X).
(14)
(15)
(16)
(17)
What remains is to calculate (16) and (17), which we
do in different ways depending on the value of the evolu-
tionary reciprocal temperature β .
In these expressions Θ(x) = 1 if x ≥ 0 and is zero oth-
erwise. In going from (18) to (19) we have assumed that
βe ∙ VU(x) is small. This condition cannot be met for
β = ∞; that case is treated in Section In B. In going from
(20) to (21) we have used the result Θ(x) + Θ(-x) = 1；
the first integral in (21) then vanishes by symmetry.
The second integral can be evaluated as indicated in
Ref. [20]. Upon a change of sign of the integration vari-
ables, e → -e, the value of the integral is unchanged
and it is brought to a form that is identical except for
a change of sign of the argument of the theta function.
Adding the two forms of the integral removes the theta
functions, giving the form shown in (23), and dividing by
2 restores the value of the original integral. (23) can be
evaluated using standard results of Gaussian integrals.
4
Eq. (17) can be evaluated in a similar way:
Bij(x) =	dp()ijmin 1, e-β[U(x+)-U(x)](25)
We can make progress by introducing the integral form
of the theta function (see e.g. [33]),
Θ(-e∙VU (x))

d p()ij min (1, 1
-βe ∙ VU)
(26)
Z0
-∞
dz δ(z — e ∙ VU(x))
(35)
=/ dep(e)eiej∙Θ(-e ∙ VU)
+ / dep(e)eiej (1 — βe ∙ VU)Θ(e ∙ VU)
≈ dep(e)ij
= σ δij .
10	dz I∞ dω eiω(ZYVU(X)).
-∞ 2π -∞
Then (34) reads
(27)
(28)
(29)
Ai(x)
Z0
-∞
上∞
2∏ J-∞
dω eiωz Gi(1) YG(j0),	(36)
j6=i
where the symbols
The ≈ sign in (28) indicates that we have omitted terms
of order σ3.
Inserting (24) and (29) into (15) gives us, to second
order in σ, the Fokker-Planck equation
∞
djn e-j∕32-iωejdjU (37)
∞
are standard Gaussian integrals. Upon evaluating them
as
∂P(χ,t)〜_X
∂t ≈ ɪ-
i=1
—
手 dUx P(χ,t)
xi
and
G(0) =e-2σ2ω2(∂j U)2
G(I) = -iωσ2(∂iU )e-2 σ2ω2(diU )2,
(38)
(39)
1 N ∂2
+ 2X ∂X2 (σ Pat”.
i=1	i
(30)
(36) reads
Ai(x)
This equation is equivalent [32] to the N Langevin equa-
tions [30, 31]
-[0 dz ZCo dωiσ2ω(∂iU)e-1 ω2σ2lvUl2+iωz
-∞ 2π -∞
dxi	βσ2 ∂U(x)
=
dt	2 ∂xi
+ ξi(t)
∀i,
(31)
∕0 dz √2∏ "U ze-z2∕(2σ2∣VU ∣2)	(40)
L∞ 2∏ v2πσ∣VU∣3ze	(40)
σ ∂iU (x)
√2π |VU(x)].
(41)
where ξ is a Gaussian white noise with zero mean and
2
variance σ2:
hξi(t)i =0,	hξi(t)ξj(t0)i =σ2δijδ(t-t0).	(32)
Eq. (31) describes an evolution of the neural-network pa-
rameters xi that is equivalent to gradient descent with
learning rate ɑ = βσ2∕2, plus a Gaussian white noise.
Averaging over independent stochastic trajectories of the
learning process (starting from identical initial condi-
tions) gives
dhxii	βσ2 ∂U(x)
	=	--
dt-------------------2 ∂xi
(33)
which is equivalent to simple gradient descent on the loss
function.
The form (41) is similar to (24) in that it involves the
derivative of the loss function U(x) with respect to xi,
but contains an additional normalization term, |VU |.
In the limit β → ∞, (17) reads
Bij(X) = J dep(e)eiCjΘ(-e ∙ VU(x))	(42)
=2 / dep(e)qej	(43)
=2 σ2 δj,	(44)
upon applying the symmetry arguments used to evalu-
ate (22). Eq. (44) is half the value of the corresponding
term for the case β 6= ∞, Eq. (29), because one term
corresponds to Brownian motion in unrestricted space,
the other to Brownian motion on a half-space.
Inserting (41) and (44) into (15) gives a Fokker-Planck
equation equivalent to the N Langevin equations
B. The case β = ∞
When β = ∞ we only accept mutations that do not
increase the loss function. To treat this case we return
to (16) and take the limit β → ∞:
dxi	σ 1	∂U(x)
----------.	7---:-----：---
dt	√2π |VU(x)| ∂xi
+ ηi(t)
∀i,
(45)
where η is a Gaussian white noise with zero mean and
variance σ2∕2:
Ai(x) =
dep(e)gΘ(-e ∙ VU(x)).
(34)
hηi(t)i = 0,	hηi(t)ηj (t')i = 2 σ2δj δ(t- t0).	(46)
5
FIG. 1. Numerical illustration of the equivalence of gradient descent and neuroevolution in the case β = ∞. (a) Evolution
With time of 4 of the 90 weights of the neural network (48) under modified gradient descent, Eq. (49) (black:“bp” stands
for“backprOPagation”)，and under neuroevolution with mutation scale λ = 1/10 [see (51)]. Here and in subsequent panels
we show 25 independent neuroevolution trajectories (grey) and the average of 1000 independent trajectories (green). (b) All
weights at time t = 10 under the two methods. (c) Loss as a function of time.
IV. NUMERICAL ILLUSTRATION OF THE
NEUROEVOLUTION-GRADIENT DESCENT
CORRESPONDENCE
In this section we illustrate the neuroevolution-
gradient descent correspondence for the case β = ∞.
In this case the only requirement for correspondence is
that the mutation scale σ is small enough that correction
terms neglected in the expansion leading to (15) and (45)
are small. The required range of σ is difficult to know in
advance, but straightforward to determine empirically.
We consider a simple supervised-learning problem in
which we train a neural network to express the function
f0 (θ) = sin(2πθ) on the interval θ ∈ [0, 1). We calculated
the loss using K = 1000 points on the interval,
N = 3M parameters xi. These parameters are initially
chosen to be Gaussian random numbers with zero mean
and variance σ02 = 10-4 ; we used the same randomly-
initialized network for all simulations (conveniently done
using the same seed for the pseudo-random number gen-
erator used to generate the initial network).
We performed modified gradient descent with learning
rate α = 10-5 , using Euler integration of the noise-free
version of Eq. (6). Explicitly, we updated all weights xi at
each timestep tbp = 1, 2, . . . according to the prescription
XiUbP+1)=XiCbP)—VU(X)I ∂uxχ),	(49)
where
U(x) = ɪ X-1 [fx(j/K) - fo(j∕K)]2 ,
K
j=0
where
M-1
fx (θ) =	x3i+1 tanh(x3i+2θ + x3i+3)	(48)
i=0
(47)
∂U (x) ∂Xi	2 二 K	KX-1[fx(j/K) -f0(j/K)] j=0	∂fx (j/K) ∂Xi	(50)
and				
∂fx(θ) ∂Xi			if i mod 3 =	1
	=	tanh(θXi+1 + Xi+2)	if i mod 3 =	2.
		θXi-1 sech2 (θXi + Xi+1 ) 、Xi-2 sech2(θxi-ι + Xi)	if i mod 3 =	0
Recall that |VU(x)∣ = JPN=ι(∂U(x)∕∂xi)2.
is the output of a single-layer neural network with one
input node, one output node, M = 30 hidden nodes, and
6
FIG. 2. The smaller the neuroevolution mutation scale, the closer the neuroevolution-gradient descent correspondence. (a)
Time evolution of a single neural-network weight, for three different neuroevolution mutation scales [see (51)]. (b) The quantity
∆(t), Eq. (52), a measure of the similarity of networks evolved under gradient descent and neuroevolution.
We did neuroevolution following the Monte Carlo pro-
cedure described in Section IIB, in the limit β = ∞, i.e.
we accepted only moves that did not increase the loss
function. We chose the mutation scale
σ = λα vz2π,
(51)
where λ is a parameter. According to (6) and (49), this
prescription sets the neuroevolution timescale tneuro to
be a factor λ times that of the modified gradient descent
timescale. Thus one neuroevolution step correponds to
λ integration steps of the gradient descent procedure.
In figures, we compare modified gradient
neuroevolution as a function of common
αλtneuro.
t
αtbp
In Fig. 1 (a) we show the evolution
descent with
(scaled) time
of individual
weights under neuroevolution and modified gradient de-
scent (weights are distinguishable because they always
have the same initial values). The correspondence pre-
dicted analytically can be seen numerically: individ-
ual neuroevolution trajectories (gray) fluctuate around
the gradient descent result (black); and when averaged
over individual trajectories, the results of neuroevolu-
tion (green) approximate those of gradient descent. In
Fig. 1(b) we show the individual and averaged values of
the weights of neuro-evolved networks at time t = 10
compared to those of modified gradient descent. In gen-
eral, the weights generated by averaging over neuroevo-
lution trajectories approximate those of gradient descent,
with some discrepancy seen in the values of the largest
weights. In Fig. 1(c) we show loss functions under neu-
roevolution and gradient descent. As predicted by (9),
averaged neuroevolution and gradient descent are equiv-
alent.
In Fig. 2(a) we show the time evolution of a single
weight of the network under modified gradient descent
and neuroevolution, the latter for three sizes of mutation
step σ. As σ increases, the size of fluctuations of individ-
ual trajectories ab out the mean increase, as predicted by
(6). As a result, more trajectories are required to esti-
mate the average, and for fixed number of trajectories (as
used here) the estimated average becomes less precise.
In addition, as σ increases the assumptions underlying
the correspondence derivation eventually break down, in
which case the neuroevolution average will not converge
to the gradient descent result even as more trajectories
are sampled.
In Fig. 2(c) we show the quantity
1三	2
∆(t)三 N X (Xbp(t) -hχneuro(t)i) ,	(52)
i i=1
which is a measure of the extent to which networks
evolved under gradient descent and (averaged) neuroevo-
lution differ. Here xibp (t) is the time evolution of neural-
network parameter i under modified gradient descent,
Eq. (49), and hxineuro(t)i is the mean value of neural-
network parameter i over the ensemble of neuroevolution
trajectories. The smaller is the neuroevolution step size,
the smaller is ∆(t), and the closer the neuroevolution-
gradient descent correspondence.
In Fig. 3 we show the evolution with time of the loss
function for different mutation scales, the trend being
similar to that of the weights shown in Fig. 2.
V. CONCLUSIONS
We have shown analytically that training a neural net-
work by neuroevolution of its weights is equivalent, in
the limit of small mutation scale, to noisy gradient de-
scent on the loss function. Conditioning neuroevolution
on the Metropolis acceptance criterion at finite evolu-
tionary temperature is equivalent to a noisy version of
simple gradient descent, while at infinite reciprocal evo-
lutionary temperature the procedure is equivalent to a
different form of noisy gradient descent on the loss func-
tion. Averaged over noise, the evolutionary procedures
therefore correspond to gradient descent on the loss func-
tion. This correspondence is described by Equations (3),
(5), (6) and (8).
Our results provide a connection between two ap-
parently dissimilar types of neural-network training.
They also provide a foundation for understanding the
7
FIG. 3. Similar to Fig. 2(a), but showing the evolution of the loss function U(x) with time. For small mutations [see (51)] the
neuroevolution-gradient descent correspondence is apparent; for larger mutations the dynamics of neuroevolution and gradient
descent are different, but neurevolution can still learn. For sufficiently large mutations neuroevolution will cease to learn.
empirically-observed ability of stochastic algorithms to
compete with gradient-based methods [14, 16-18]. The
neuroevolution-gradient descent equivalence results from
an unweighted average over (i.e. the typical behavior of)
a population of isolated individuals on which the muta-
tion process acts independently. The mean loss function
under neuroevolution is equal to that generated by noise-
free gradient descent [see Eq. (9)]. By definition, there
exist individuals in a neuroevolution population whose
loss functions are smaller than the mean, and are there-
fore smaller than the loss function corresponding to noise-
free gradient descent (see e.g. Fig. 1(c)). This argument
is also relevant to genetic algorithms that periodically
clone and eliminate members of a neuroevolution popu-
lation. Consider a population of many realizations of the
neuroevolution process, each evolved for a single muta-
tion step. For small mutations, the portion of this popu-
lation for which the loss function does not increase upon
making the step is governed by Eq. (6), and so contains
individuals whose loss functions lie below that produced
by noise-free gradient descent. Choosing those individ-
uals as the starting point for subsequent mutations will
naturally probe the small-loss tail of the neuroevolution
process.
The correspondence we have described is formally ex-
act only in the limit of zero mutation scale, but we have
shown that it can be observed numerically for nonzero
mutations. Indeed, several dynamical regimes are con-
tained within the neurevolution master equation (10),
according to the scale σ of mutations [34]: for vanish-
ing σ , neurevolution is formally noisy gradient descent
on the loss function; for small but nonvanishing σ it ap-
proximates noisy gradient descent enacted by explicit in-
tegration with a finite timestep; for larger σ it enacts
a distinct dynamics, but one that can still learn; and
for sufficiently large σ the steps taken are too large for
learning to occur on accessible timescales. An indication
of these various regimes can be seen in Figs. 2 and 3.
Separate from the question of its precise temporal evo-
lution, the master equation (10) has a well-defined sta-
tionary distribution ρo(x). Requiring the brackets on the
right-hand of (10) to vanish ensures that P(x,t) → ρo(x)
becomes independent of time. Inserting (11) into (10)
and requiring normalization of ρo (x) reveals the sta-
tionary distribution to be the Boltzmann one, ρo (x)=
e-βU(X) / R dχ0e-βU(XO). For non-infinite β the neuroevo-
lution procedure is ergodic, and this distribution will
be sampled given sufficiently long simulation time. For
β → ∞ we have ρ0(x) → δ(U(x) - U0), where U0 is the
global energy minimum; in this case the system is not
ergodic (moves uphill in U (x) are not allowed) and there
is no guarantee of reaching this minimum.
The neuroevolution-gradient descent corresponence we
have identified follows from that between the overdamped
Langevin dynamics and Metropolis Monte Carlo dynam-
ics of a particle in an external potential [19, 20]. Our
work therefore adds to the existing set of connections be-
tween machine learning and statistical mechanics [35, 36],
and continues a trend in machine learning of making
use of old results: the stochastic and deterministic al-
gorithms considered here come from the 1950s [6, 7] and
1980s [1-4], and are connected by ideas developed in the
1990s [19, 20].
Acknowledgments - This work was performed as part
of a user project at the Molecular Foundry, Lawrence
Berkeley National Laboratory, supported by the Office
of Science, Office of Basic Energy Sciences, of the U.S.
Department of Energy under Contract No. DE-AC02-
05CH11231. This work used resources of the National
Energy Research Scientific Computing Center (NERSC),
a U.S. Department of Energy Office of Science User Facil-
ity operated under Contract No. DE-AC02-05CH11231.
I.T. acknowledges funding from the National Science and
Engineering Council of Canada and carried out work at
the National Research Council of Canada under the aus-
pices of the AI4D Program.
[1]	David E Rumelhart, Richard Durbin, Richard Golden,
and Yves Chauvin, “Backpropagation: The basic the-
ory,” Backpropagation: Theory, architectures and appli-
8
cations , 1-34 (1995).
[2]	David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams, “Learning representations by back-propagating
errors,” Nature 323, 533-536 (1986).
[3]	Robert Hecht-Nielsen, “Theory of the backpropagation
neural network,” in Neural networks for perception (El-
sevier, 1992) pp. 65-93.
[4]	Yann LeCun, Bernhard Boser, John S Denker, Donnie
Henderson, Richard E Howard, Wayne Hubbard, and
Lawrence D Jackel, “Backpropagation applied to hand-
written zip code recognition,” Neural computation 1,
541-551 (1989).
[5]	Yves Chauvin and David E Rumelhart, Backpropagation:
theory, architectures, and applications (Psychology press,
1995).
[6]	Nicholas Metropolis, Arianna W Rosenbluth, Marshall N
Rosenbluth, Augusta H Teller, and Edward Teller,
“Equation of state calculations by fast computing ma-
chines,” The Journal of Chemical Physics 21, 1087-1092
(1953).
[7]	W Keith Hastings, “Monte Carlo sampling methods us-
ing markov chains and their applications,” Biometrika 5
7, 97-109 (1970).
[8]	John H Holland, “Genetic algorithms,” Scientific ameri-
can 267, 66-73 (1992).
[9]	David B Fogel and Lauren C Stayton, “On the effective-
ness of crossover in simulated evolutionary optimization,”
BioSystems 32, 171-182 (1994).
[10]	David J Montana and Lawrence Davis, “Training feed-
forward neural networks using genetic algorithms.” in IJ-
CAI, Vol. 89 (1989) pp. 762-767.
[11]	The term neuroevolution is also used to describe muta-
tions of network topology [12]; here we use the term to
describe mutations of the weights and biases of a network.
[12]	Dario Floreano, Peter Diirr, and Claudio Mattiussi,
“Neuroevolution: from architectures to learning,” Evo-
lutionary intelligence 1, 47-62 (2008).
[13]	Felipe Petroski Such, Vashisht Madhavan, Edoardo
Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune,
“Deep neuroevolution: genetic algorithms are a compet-
itive alternative for training deep neural networks for re-
inforcement learning,” arXiv preprint arXiv:1712.06567
(2017).
[14]	Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor,
and Ilya Sutskever, “Evolution strategies as a scalable
alternative to reinforcement learning,” arXiv preprint
arXiv:1703.03864 (2017).
[15]	Stephen Whitelam and Isaac Tamblyn, “Learning to
grow: Control of material self-assembly using evolution-
ary reinforcement learning,” Physical Review E 101,
052604 (2020).
[16]	Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller, “Playing Atari with deep reinforce-
ment learning,” arXiv preprint arXiv:1312.5602 (2013).
[17]	Gregory Morse and Kenneth O Stanley, “Simple evolu-
tionary optimization can rival stochastic gradient descent
in neural networks,” in Proceedings of the Genetic and
Evolutionary Computation Conference 2016 (2016) pp.
477-484.
[18]	Xingwen Zhang, Jeff Clune, and Kenneth O Stan-
ley, “On the relationship between the OpenAI evolution
strategy and stochastic gradient descent,” arXiv preprint
arXiv:1712.06564 (2017).
[19]	K Kikuchi, M Yoshida, T Maekawa, and H Watan-
abe, “Metropolis Monte Carlo method as a numerical
technique to solve the fokker-planck equation,” Chemi-
cal Physics Letters 185, 335-338 (1991).
[20]	K Kikuchi, M Yoshida, T Maekawa, and H Watanabe,
“Metropolis Monte Carlo method for brownian dynamics
simulation generalized to include hydrodynamic interac-
tions,” Chemical Physics letters 196, 57-61 (1992).
[21]	Daan Frenkel and Berend Smit, Understanding molecu-
lar simulation: from algorithms to applications, Vol. 1
(Academic Press, 2001).
[22]	In effect we work with a single particle in a high-
dimensional space, rather than with many particles in
three-dimensional space.
[23]	Stephen Whitelam and Phillip L Geissler, “Avoiding
unphysical kinetic traps in Monte Carlo simulations of
strongly attractive particles,” The Journal of Chemical
Physics 127, 154101 (2007).
[24]	Alex W Wilber, Jonathan PK Doye, Ard A Louis, Eva G
Noya, Mark A Miller, and Pauline Wong, “Reversible
self-assembly of patchy particles into monodisperse icosa-
hedral clusters,” The Journal of Chemical Physics 127,
08B618 (2007).
[25]	Ludovic Berthier, “Revisiting the slow dynamics of a sil-
ica melt using Monte Carlo simulations,” Physical Re-
view E 76, 011507 (2007).
[26]	E Sanz and D Marenduzzo, “Dynamic Monte Carlo ver-
sus Brownian dynamics: A comparison for self-diffusion
and crystallization in colloidal fluids,” The Journal of
Chemical Physics 132, 194102 (2010).
[27]	Stephen Whitelam, “Approximating the dynamical evo-
lution of systems of strongly interacting overdamped par-
ticles,” Molecular Simulation 37, 606-612 (2011).
[28]	Xiao Liu, John C Crocker, and Talid Sinno, “Coarse-
grained Monte Carlo simulations of non-equilibrium sys-
tems,” The Journal of Chemical Physics 138, 244111
(2013).
[29]	Lorenzo Rovigatti, John Russo, and Flavio Romano,
“How to simulate patchy particles,” The European Phys-
ical Journal E 41, 59 (2018).
[30]	Hannes Risken, “Fokker-planck equation,” in The
Fokker-Planck Equation (Springer, 1996) pp. 63-95.
[31]	Nicolaas Godfried Van Kampen, Stochastic processes in
physics and chemistry, Vol. 1 (Elsevier, 1992).
[32]	The diffusion term is independent of x, and so the choice
of stochastic calculus is unimportant.
[33]	Y Bar Sinai, https://yohai.github.io/post/half-gaussian/
(2019).
[34]	Choosing σ to be a parameter in a genetic search scheme
would allow exploration of the dynamical diversity con-
tained within Eq. (10).
[35]	Andreas Engel and Christian Van den Broeck, Statisti-
cal mechanics of learning (Cambridge University Press,
2001).
[36]	Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington,
Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya
Ganguli, “Statistical mechanics of deep learning,” An-
nual Review of Condensed Matter Physics (2020).