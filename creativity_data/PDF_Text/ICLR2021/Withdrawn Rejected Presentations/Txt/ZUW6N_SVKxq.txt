Under review as a conference paper at ICLR 2021
Deep kernel processes
Anonymous authors
Paper under double-blind review
Ab stract
We define deep kernel processes in which positive definite Gram matrices are pro-
gressively transformed by nonlinear kernel functions and by sampling from (in-
verse) Wishart distributions. Remarkably, we find that deep Gaussian processes
(DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite BNNs
with bottlenecks can all be written as deep kernel processes. For DGPs the equiv-
alence arises because the Gram matrix formed by the inner product of features
is Wishart distributed, and as we show, standard isotropic kernels can be written
entirely in terms of this Gram matrix — we do not need knowledge of the under-
lying features. We define a tractable deep kernel process, the deep inverse Wishart
process, and give a doubly-stochastic inducing-point variational inference scheme
that operates on the Gram matrices, not on the features, as in DGPs. We show that
the deep inverse Wishart process gives superior performance to DGPs and infinite
BNNs on standard fully-connected baselines.
1	Introduction
The deep learning revolution has shown us that effective performance on difficult tasks such as
image classification (Krizhevsky et al., 2012) requires deep models with flexible lower-layers that
learn task-dependent representations. Here, we consider whether these insights from the neural
network literature can be applied to purely kernel-based methods. (Note that we do not consider deep
Gaussian processes or DGPs to be “fully kernel-based” as they use a feature-based representation in
intermediate layers).
Importantly, deep kernel methods (e.g. Cho & Saul, 2009) already exist. In these methods, which
are closely related to infinite Bayesian neural networks (Lee et al., 2017; Matthews et al., 2018;
Garriga-Alonso et al., 2018; Novak et al., 2018), we take an initial kernel (usually the dot product
of the input features) and perform a series of deterministic, parameter-free transformations to obtain
an output kernel that we use in e.g. a support vector machine or Gaussian process. However, the
deterministic, parameter-free nature of the transformation from input to output kernel means that
they lack the capability to learn a top-layer representation, which is believed to be crucial for the
effectiveness of deep methods (Aitchison, 2019).
To obtain the flexibility necessary to learn a task-dependent representation, we propose deep ker-
nel processes (DKPs), which combine nonlinear transformations of the kernel, as in Cho & Saul
(2009) with a flexible learned representation by exploiting a Wishart or inverse Wishart process
(Dawid, 1981; Shah et al., 2014). We find that models ranging from DGPs (Damianou & Lawrence,
2013; Salimbeni & Deisenroth, 2017) to Bayesian neural networks (BNNs; Blundell et al., 2015,
App. C.1), infinite BNNs (App. C.2) and infinite BNNs with bottlenecks (App. C.3) can be written
as DKPs (i.e. only with kernel/Gram matrices, without needing features or weights). Practically, we
find that the deep inverse Wishart process (DIWP), admits convenient forms for variational approx-
imate posteriors, and we give a novel scheme for doubly-stochastic variational inference (DSVI)
with inducing points purely in the kernel domain (as opposed to Salimbeni & Deisenroth, 2017,
who described DSVI for standard feature-based DGPs), and demonstrate improved performance
with carefully matched models on fully-connected benchmark datasets.
2	Background
We briefly revise Wishart and inverse Wishart distributions. The Wishart distribution is a general-
ization of the gamma distribution that is defined over positive semidefinite matrices. Suppose that
1
Under review as a conference paper at ICLR 2021
Layer 1
Layer 2	Output Layer
F3 —> Y
F3 —> Y
Figure 1: Generative models for two layer (L = 2) deep GPs. (Top) Generative model for a
deep GP, with a kernel that depends on the Gram matrix, and with Gaussian-distributed features.
(Bottom) Integrating out the features, the Gram matrices become Wishart distributed.
we have a collection of P -dimensional random variables xi with i ∈ {1, . . . , N} such that
Xi iid N (0, V),	then,	PN=IXiXT = S 〜W (V,N)	(1)
has Wishart distribution with scale matrix V and N degrees of freedom. When N > P - 1, the
density is,
W(S； V, N)= 2NPVΓ3 (N) ISI(N-P-1)/2 exP (T Tr (VTS)),	⑵
where ΓP is the multivariate gamma function. Further, the inverse, S-1 has inverse Wishart distribu-
tion, W-1 (V-1, N). The inverse Wishart is defined only for N > P - 1 and also has closed-form
density. Finally, we note that the Wishart distribution has mean NV while the inverse Wishart has
mean V-1/(N - P - 1) (for N > P + 1).
3 Deep kernel processes
We define a kernel process to be a set of distributions over positive definite matrices of different
sizes, that are consistent under marginalisation (Dawid, 1981; Shah et al., 2014). The two most
common kernel processes are the Wishart process and inverse Wishart process, which we write in a
slightly unusual form to ensure their expectation is K. We take G and G0 to be finite dimensional
marginals of the underlying Wishart and inverse Wishart process,
G 〜W (K /N,N),	G0 〜W-1	(δK ,δ	+(P +1)),	(3a)
G* 〜W (K*∕N,N),	G0* 〜W-1	(δK*,δ	+ (P* + 1)),	(3b)
and where we explicitly give the consistent marginal distributions over K*, G* and G0* which are
P* × P* principal submatrices of the P × P matrices K, G and G0 dropping the same rows and
columns. In the inverse-Wishart distribution, δ is a positive parameter that can be understood as
controlling the degree of variability, with larger values for δ implying smaller variability in G0.
We define a deep kernel process by analogy with a DGP, as a composition of kernel processes, and
show in App. A that under sensible assumptions any such composition is itself a kernel process. 1
3.1 DGPs with isotropic kernels are deep Wishart processes
We consider deep GPs of the form (Fig. 1 top) with X ∈ RP ×N0
K'=JN0 XXT	for ' =1,
[K (G'-ι) otherwise,
P(F'∣K') = QN= 1N (f'; O, k`),
G' = N FeFT.
(4a)
(4b)
(4c)
Here, F' ∈ RP×n' are the N' hidden features in layer '; λ indexes hidden features so ff is a single
column of f`, representing the value of the λth feature for all training inputs. Note that K(∙) is a
1 Note that we leave the question of the full Kolmogorov extension theorem (Kolmogorov, 1933) for matrices
to future work: for our purposes, it is sufficient to work with very large but ultimately finite input spaces as in
practice, the input vectors are represented by elements of the finite set of 32-bit or 64-bit floating-point numbers
(Sterbenz, 1974).
2
Under review as a conference paper at ICLR 2021
function that takes a Gram matrix and returns a kernel matrix; whereas k` is a (possibly random)
variable representing a kernel matrix. Note, we have restricted ourselves to kernels, that can be
written as functions of the Gram matrix, g`, and do not require the full set of activations, f`. As
we describe later, this is not too restrictive, as it includes amongst others all isotropic kernels (i.e.
those that can be written as a function of the distance between points Williams & Rasmussen, 2006).
Note that we have a number of choices as to how to initialize the kernel in Eq. (4a). The current
choice just uses a linear dot-product kernel, rather than immediately applying the kernel function K.
This is both to ensure exact equivalence with infinite NNs with bottlenecks (App. C.3) and also to
highlight an interesting interpretation of this layer as Bayesian inference over generalised lengthscale
hyperparameters in the squared-exponential kernel (App. B e.g. Lalchand & Rasmussen, 2020).
For DGP regression, the outputs, Y, are most commonly given by a likelihood that can be written
in terms of the output features, FL+1. For instance, for regression, the distribution of the λth output
feature column could be
P(yλ∣Fl+i)= N (yi; fL+1,σ2ι),	⑸
but our methods can be used with many other forms for the likelihood, including e.g. classification.
The generative process for the Gram matrices, g`, consists of generating samples from a Gaussian
distribution (Eq. 4b), and taking their product with themselves transposed (Eq. 4c). This exactly
matches the generative process fora Wishart distribution (Eq. 1), so we can write the Gram matrices,
g`, directly in terms of the kernel, without needing to sample features (Fig. 1 bottom),
P(GIX) = W (N (NXXT) ,Ni) ,	(6a)
P(G'∣G'-ι) = W (K (G'-ι) /n`,n`) ,	for' ∈{2,...L},	(6b)
P (Fl+i∣Gl) = QN=+1N (fL+1; 0, K (GL)) .	(6c)
Except at the output, the model is phrased entirely in terms of positive-definite kernels and Gram ma-
trices, and is consistent under marginalisation (assuming a valid kernel) and is thus a DKP. At a high
level, the model can be understood as alternatively sampling a Gram matrix (introducing flexibility
in the representation), and nonlinearly transforming the Gram matrix using a kernel (Fig. 2).
This highlights a particularly simple interpretation of the DKP as an autoregressive process. In a
standard autoregressive process, we might propagate the current vector, xt , through a deterministic
function, f(xt), and add zero-mean Gaussian noise, ξ,
xt+1 = f (xt) + σ2ξ	such that	E [xt+1|xt] = f (xt) .	(7)
By analogy, the next Gram matrix has expectation centered on a deterministic transformation of the
previous Gram matrix,
E [G'∣G'-ι]= K (G'-ι),	(8)
so g` can be written as this expectation plus a zero-mean random variable, Ξ', that can be inter-
preted as noise,
g' = K (G'-i) + ξ'.	⑼
Note that Ξ' is not in general positive definite, and may not have an analytically tractable distribu-
tion. This noise decreases as N' increases,
V [G'j] = V [Ξ'j] = N (Kj(G'-i) + Kii(G'-i)Kjj(G'-ι)) .	(10)
Notably, as N' tends to infinity, the Wishart samples converge on their expectation, and the noise
disappears, leaving us with a series of deterministic transformations of the Gram matrix. Therefore,
we can understand a deep kernel process as alternatively adding “noise” to the kernel by sampling
e.g. a Wishart or inverse Wishart distribution (G2 and G3 in Fig. 2) and computing a nonlinear
transformation of the kernel (K(G2) and K(G3) in Fig. 2)
Remember that we are restricted to kernels that can be written as a function of the Gram matrix,
K' = K (G')
= Kfeatures (f`) ,	Kj = k (F',:, F',:) .	(11)
3
Under review as a conference paper at ICLR 2021
:μeq--ΛΛ 1Jeq∙-ΛΛ φw⅛>c-
Figure 2: Visualisations of a single prior sample of the kernels and Gram matrices as they pass
through the network. We use 1D, equally spaced inputs with a squared exponential kernel. As we
transition K(G'-ι) → g`, We add “noise" by sampling from a Wishart (top) or an inverse Wishart
(bottom). As we transition from g` to K(G'), we deterministically transform the Gram matrix
using a squared-exponential kernel.
where KfeatUres(∙) takes a matrix of features, f`, and returns the kernel matrix, k`, and k is the usual
kernel, which takes two feature vectors (rows of f`) and returns an element of the kernel matrix.
This does not include all possible kernels because it is not possible to recover the features from the
Gram matrix. In particular, the Gram matrix is invariant to unitary transformations of the features:
the Gram matrix is the same for f` and F' = UF' where U is a unitary matrix, such that UUT = I,
g` = N F'FT = N F'U'UT FT = N F'F'T.	(12)
Superficially, this might seem very limiting — leaving us only with dot-product kernels (Williams
& Rasmussen, 2006) such as,
k(f, f0)= f ∙ f0 + σ2.	(13)
However, in reality, a far broader range of kernels fit within this class. Importantly, isotropic or radial
basis function kernels including the squared exponential and Matern depend only on the squared
distance between points, R, (Williams & Rasmussen, 2006)
k(f, f0) = k(R),	R= |f-f0|2.	(14)
These kernels can be written as a function of G, because the matrix of squared distances, R, can be
computed from G,
p'	1 X^Ng	`	` 2	1 LNg	' 2	'口'	' 2	'	'	'
Rij = NXλ=ι IFiλ - Fjλ = NTι=ι ((Fiλ	- 2FiλFjλ + (Fjλ) = = Gii - 2Gij + Gjj.
(15)
4	Variational inference in deep kernel processes
A key part of the motivation for developing deep kernel processes was that the posteriors over
weights in a BNN or over features in a deep GP are extremely complex and multimodal, with a
large number of symmetries that are not captured by standard approximate posteriors (MacKay,
1992; Moore, 2016; Pourzanjani et al., 2017). For instance, in the Appendix we show that there are
permutation symmetries in the prior and posteriors over weights in BNNs (App. D.1) and rotational
symmetries in the prior and posterior over features in deep GPs with isotropic kernels (App. D.2).
The inability to capture these symmetries in standard variational posteriors may introduce biases
in the parameters inferred by variational inference, because the variational bound is not uniformly
tight across the state-space (Turner & Sahani, 2011). Intuitively, these symmetries arise in DGPs
with isotropic kernels because the features at the next layer depend only on the kernel matrix at
4
Under review as a conference paper at ICLR 2021
the previous layer, and this kernel is invariant to unitary transformations of the features (Eq. 12).
As such, we can sidestep these complex posterior symmetries by working directly with the Gram
matrices as the random variables for variational inference.
We show that DGPs (Sec. 3.1) and infinite NNs with bottlenecks (App. C.3) are deep Wishart pro-
cesses, so a natural approach would be to define an approximate posterior over the Gram matrices in
the deep Wishart process. However, this turns out to be difficult, predominantly because the approx-
imate posterior we would like to use, the non-central Wishart (App. E), has a probability density
function that is prohibitively costly and complex to evaluate in the inner loop of a deep learning
model (Koev & Edelman, 2006). Instead, we consider an inverse Wishart process prior, for which
the inverse Wishart itself makes a good choice of approximate posterior.
4.1	The deep inverse Wishart processes
By analogy with Eq. (6), our deep inverse Wishart processes (DIWPs) are given by
P(Ω) = WT (δ1I,δ1 + No + 1),	(with Gi = NXΩXτ),	(16a)
P(G'∣G'-ι) = W T (g`; δ'K (G'-ι) ,P +1 + δ'),	for ' ∈{2,...L},	(16b)
P (Fl+1 |Gl) = QNL+1 N fL+1; 0, K (GL)),	(16c)
remember that X ∈ RP×N0, g` ∈ RP×p and f` ∈ RP×NL+1. Note that at the input layer,
K0 = NςXXT may be singular if there are more datapoints than features. Instead of attempting
to use a singular Wishart distributions over G1, which would be complex and difficult to work with
(Bodnar & Okhrin, 2008; Bodnar et al., 2016), we instead define an approximate posterior over the
full-rank No X No matrix, Ω, and use Gi = N■ XΩXτ ∈ RP×p.
Critically, the distributions in Eq. (16b) are consistent under marginalisation as long as δ' is held
constant (Dawid, 1981), with P taken to be the number of input points, or equivalently the size
of K'-ι. Further, the deep inverse Wishart process retains the interpretation as a deterministic
transformation of the kernel plus noise because the expectation is,
E[G'1G'T] = (P +δ'KδG[P + 1) =K (GJ).	(17)
The resulting inverse Wishart process does not have a direct interpretation as e.g. a deep GP, but
does have more appealing properties for variational inference, as it is always full-rank and allows
independent control over the approximate posterior mean and variance. Finally, it is important to
note that Wishart and inverse Wishart distributions do not differ as much as one might expect; the
standard Wishart and standard inverse Wishart distributions have isotropic distributions over the
eigenvectors so they only differ in terms of their distributions over eigenvalues, and these are often
quite similar, especially ifwe consider a Wishart model with ResNet-like structure (App. H).
4.2	An approximate posterior for the deep inverse Wishart process
Choosing an appropriate and effective form for variational approximate posteriors is usually a dif-
ficult research problem. Here, we take inspiration from Ober & Aitchison (2020) by exploiting
the fact that the inverse-Wishart distribution is the conjugate prior for the covariance matrix of a
multivariate Gaussian. In particular, if we consider an inverse-Wishart prior over Σ ∈ RP ×P with
mean δΣo, which forms the covariance of Gaussian-distributed matrix, V ∈ RP×P, consisting of
columns vλ ,
P(∑) = WT (∑; δ∑o,P + 1 + δ),	(18a)
P(V∣∑) = QNVIN(vi； O, ∑),	(18b)
P (Σ∣V) = WT (∑; δ∑o + VVT, P + 1 + δ + NV) .	(18c)
Inspired by this exact posterior that is available in simple models, we choose the approximate pos-
terior in our model to be,
Q (Ω) = WT (Ω; διI + Vi VT, δι + γι + (N + 1)),	(19a)
Q (G'∣G'-ι) = WT (G'； δ'K (G'-ι) + V'VT, δ' + γ' + (P + 1)),	(19b)
Q(Fl+i∣Gl) = QNl+1 N (fL+1; ∑λΛλvλ, Σλ), where Σλ = (KT (GL) + Λλ)-1,
(19c)
5
Under review as a conference paper at ICLR 2021
and where Vi is a learned N X No matrix, {V'}L=2 are P X P learned matrices and {γ'}L=ι are
learned non-negative real numbers. For more details about the input layer, see App. F. At the output
layer, we take inspiration from the global inducing approximate posterior for DGPs from Ober &
Aitchison (2020), with learned parameters being vectors, vλ, and positive definite matrices, Λλ (see
App. G).
In summary, the prior has parameters {δ'}L=ι (which also appears in the approximate posterior),
and the posterior has parameters {V'}L=ι and {γ'}L=ι for the inverse-WiShart hidden layers, and
{vλ}λN=L1+1 and {Λλ}λN=L1+1 at the output. In all our experiments, we optimize all five parameters
({δ', v`, γ'}L=ι) and ({vλ, Λλ}N=+1), and in addition, for inducing-point methods, We also opti-
mize a single set of “global” inducing inputs, Xi ∈ RPi×N0, which are defined only at the input
layer.
4.3	Doubly stochastic inducing-point variational inference in deep inverse
Wishart processes
For efficient inference in high-dimensional problems, we take inspiration from the DGP litera-
ture (Salimbeni & Deisenroth, 2017) by considering doubly-stochastic inducing-point deep inverse
Wishart processes. We begin by decomposing all variables into inducing and training (or test) points
Xt ∈ RPt×N0,
X =(X)	FL+1=(FL+1),	G' =(G'i G':),	(20)
where e.g. GiiiS Pi X Pi and G今 is Pi X Pt where Pi is the number of inducing points, and Pt is the
number of testing/training points. Note that Ω does not decompose as it is No X No. The full ELBO
including latent variables for all the inducing and training points is,
L = E "logP(Y∣FL+ι) + log∣⅛WΦ⅛#，	(21)
Q (ω, {G'}'=2, F L+1lX)
where the expectation is taken over Q (Ω, {G'}L=2, Fl+i∣X). The prior is given by combining all
terms in Eq. (16) for both inducing and test/train inputs,
P (Ω,{G'}L=2, Fl+i∣X) =P(Ω) hQL=2 P(G'|G'-i)i P(Fl+i∣Gl) ,	(22)
where the X-dependence enters on the right because Gi = N^XΩXt. Taking inspiration from
Salimbeni & Deisenroth (2017), the full approximate posterior is the product of an approximate
posterior over inducing points and the conditional prior for train/test points,
Q (Ω, {G'}L=2, Fl+i∣X)=
Q (Ω, {G'i}L=2, FL+1∣Xi) P ({Git}L=2, {Gtt}L=2, FL+1∣Ω, {G'i}L=2, FL+1, x) .(23)
And the prior can be written in the same form,
P (Ω, {G'}L=2, Fl+i∣X)=
P (Ω, {G'i}L=2, FL+1Xi) P ({Git}L=2, {Gtt}L=2, FL+1∣Ω, {G'i}L=2, fL+1, x) .(24)
We discuss the second terms (the conditional prior) in Eq. (28). The first terms (the prior and
approximate posteriors over inducing points), are given by combining terms in Eq. (16) and Eq. (19),
P (Ω, {G'i}L=2, FL+1Xi) = P (Ω) [Qi=? P (G'i∣G'i-1)] P (FL+1IGiL),	(25)
Q (Ω, {G'i}L=2, FL+1∣Xi) =Q(Ω) [QL=2 Q (G'i∣G'i-1)i Q (FL+1GL) ∙	(26)
Substituting Eqs. (23-26) into the ELBO (Eq. 21), the conditional prior cancels and we obtain,
-/ 一八	Q(Ω)[QL=2 Q (G'i∣G'i-1)i Q (fL+1 GL)^
L = E log P (YIFL+1) + log------------------------------------------------ .	(27)
P(Ω) QL=2 P (G'i∣G'i-1) P (FL+1∣GL)
6
Under review as a conference paper at ICLR 2021
Importantly, the first term is a summation across test/train datapoints, and the second term depends
only on the inducing points, so as in Salimbeni & Deisenroth (2017) we can compute unbiased
estimates of the expectation by taking only a minibatch of datapoints, and we never need to compute
the density of the conditional prior in Eq. (28), we only need to be able to sample it.
Finally, to sample the test/training points, conditioned on the inducing points, we need to sample,
P ({G't}L=2, {G't}L=2, FL+1∣Ω, {G'i}L=2, fL+1, X)=
P (FL+1 ∣fL+1, Gl) QL=2 P(G<, G2|G'* Gj) .(28)
The first distribution, P FtL+1 |FiL+1 , GL , is a multivariate Gaussian, and can be evaluated using
methods from the GP literature (Williams & Rasmussen, 2006; Salimbeni & Deisenroth, 2017). The
difficulties arise for the inverse Wishart terms, P (GK G't∣Gii, G'-ι). To sample this distribution,
note that samples from the joint over inducing and train/test locations can be written,
and where Pi is the number of inducing inputs, and Pt is the number of train/test inputs. Defining
the Schur complements,
G't∙i = G't — GZ (Gii)—1 Git,	Ψtt∙i = Ψtt — ΨtiΨ- 1Ψit.	(30)
We know that Gii and (Gii)-1 G't have distribution, (Eaton, 1983)
Gtt∙i∣ Gii,G'-1 〜W —1 (Ψtt∙i,δ' + Pi + Pt +1),	(31a)
(Gii)-1 GjGtU Gii, Ge-1 〜MN (Ψ- 1Ψit, Ψ- 1, Gtt∙i),	(31b)
where MN is the matrix normal. Now, Git and G$, can be recovered by algebraic manipulation.
Finally, because of the doubly stochastic form for the objective, we do not need to sample multiple of
jointly consistent samples for test points; instead, (and as in DGPs Salimbeni & Deisenroth, 2017)
we can independently sample each test point (App. I), which dramatically reduces computational
complexity.
We optimize using standard reparameterised variational inference (Kingma & Welling, 2013;
Rezende et al., 2014) (Ober & Aitchison, 2020, for details on how to reparameterise samples from
the Wishart, see).
5	Computational complexity
As in non-deep GPs, the complexity is O(P 3) for time and O(P2) for space for standard DKPs (the
O(P 3) time dependencies emerge e.g. because of inverses and determinants required for the inverse
Wishart distributions). For DSVI, there is a Pi3 time and Pi2 space term for the inducing points,
because the computations for inducing points are exactly the same as in the non-DSVI case. As we
can treat each test/train point independently (App. I), the complexity for test/training points must
scale linearly with Pt, and this term has Pi2 time scaling, e.g. due to the matrix products in Eq. (30).
Thus, the overall complexity for DSVI is O(Pi3 +Pi2Pt) for time and O(Pi2 +PiPt) for space which
is exactly the same as non-deep inducing GPs. Thus, and exactly as in non-deep inducing-GPs, by
using a small number of inducing points, we are able to convert a cubic dependence on the number
of input points into a linear dependence, which gives considerably better scaling.
Surprisingly, this is substantially better than standard DGPs. In standard DGPs, we allow the ap-
proximate posterior covariance for each feature to differ (Salimbeni & Deisenroth, 2017), in which
case, we are in essence doing standard inducing-GP inference over N hidden features, which gives
complexity of O(N Pi3 + NPi2Pt) for time and O(N Pi2 + NPiPt) for space (Salimbeni & Deisen-
roth, 2017). It is possible to improve this complexity by restricting the approximate posterior to have
the same covariance for each point (but this restriction can be expected to harms performance).
7
Under review as a conference paper at ICLR 2021
Table 1: Performance in terms of ELBO and predictive log-likelihood for a three-layer (two hidden
layer) DGP, NNGP and DIWP on UCI benchmark tasks. Errors are quoted as two standard errors in
the difference between that method and the best performing method, as in a paired t-test. This is to
account for the shared variability that arises due to the use of different test/train splits in the data (20
splits for all but protein, where 5 splits are used Gal & Ghahramani, 2015) some splits are harder
for all models, and some splits are easier. Because we consider these differences, errors for the best
measure are implicitly included in errors for other measures, and we cannot provide a comparable
error for the best method itself._____________________________________________________
metric	dataset	DGP	NNGP	DIWP
	boston	-1.30 ± 0.02	-0.31 ± 0.01	-0.29
	concrete	-0.68 ± 0.01	-0.40 ± 0.00	-0.35
	energy	0.59 ± 0.01	1.47	1.47 ± 0.00
	kin8nm	-0.50 ± 0.01	-0.40 ± 0.00	-0.33
ELBO	naval	-1.42 ± 0.16	1.38 ± 0.22	1.44
	power	-0.04 ± 0.00	0.00 ± 0.00	0.01
	protein	-1.07	-1.11 ± 0.00	-1.09 ± 0.01
	wine	-1.39 ± 0.01	-1.17 ± 0.00	-1.16
	yacht	-0.19 ± 0.38	1.62 ± 0.02	1.66
	boston	-3.44 ± 0.14	-2.46 ± 0.02	-2.43
	concrete	-3.20 ± 0.03	-3.13 ± 0.02	-3.09
	energy	-0.90 ± 0.05	-0.71	-0.71 ± 0.01
	kin8nm	1.05 ± 0.01	1.10 ± 0.00	1.12
test LL	naval	2.80 ± 0.12	5.74	5.73 ± 0.21
	power	-2.85 ± 0.00	-2.83 ± 0.00	-2.82
	protein	-2.80	-2.88 ± 0.01	-2.87 ± 0.01
	wine	-1.18 ± 0.03	-0.96 ± 0.01	-0.95
	yacht	-2.45 ± 0.49	-0.77 ± 0.07	-0.67
6	Results
We began by comparing the performance of our deep inverse Wishart process (DIWP) against infi-
nite Bayesian neural networks (known as the neural network Gaussian process or NNGP) and DGPs.
To ensure sensible comparisons against the NNGP, we used a ReLU kernel in all models (Cho &
Saul, 2009). For all models, we used three layers (two hidden layers and one output layer), with three
applications of the kernel. In each case, we used a learned bias and scale for each input feature, and
trained for 8000 gradient steps with the Adam optimizer with 100 inducing points, a learning rate of
10-2 for the first 4000 steps and 10-3 for the final 4000 steps. For evaluation, we used 100 samples
from the final iteration of gradient descent, and for each training step we used 10 samples in the
smaller datasets (boston, concrete, energy, wine, yacht), and 1 sample in the larger datasets.
We found that DIWP usually gives better predictive performance and ELBOs. We expected DIWP
to be better than (or the same as) the NNGP as the NNGP was a special case of our DIWP (sending
δ' → ∞ sends the variance of the inverse Wishart to zero, so the model becomes equivalent to the
NNGP). We found that the DGP performs poorly in comparison to DIWP and NNGPs, and even to
past baselines on all datasets except protein (which is by far the largest). This is because we use a
ReLU, rather than a squared exponential kernel, as in (Salimbeni & Deisenroth, 2017), and because
we used a plain feedforward architecture for all models. In contrast, Salimbeni & Deisenroth (2017)
found that good performance with DGPs on even UCI datasets required a complex architecture
involving skip connections. Here, we used simple feedforward architectures, both to ensure a fair
comparison to the other models, and to avoid the need for an architecture search. In addition, the
inverse Wishart process is implicitly able to learn the network “width"，δ', whereas in the DGPs,
the width is fixed to be equal to the number of input features, following standard practice in the
literature (e.g. Salimbeni & Deisenroth, 2017).
Next, we considered fully-connected networks for small image classification datasets (MNIST and
CIFAR-10). We used the same models as in the previous section, with the omission of learned bias
and scaling of the inputs. Note that we do not expect these methods to perform well relative to
8
Under review as a conference paper at ICLR 2021
Table 2: Performance in terms of ELBO test log-likelihood and test accuracy for fully-connected three-layer (two hidden layer) DGPs, NNGP and DIWP on MNIST and CIFAR-10.				
metric	dataset	DGP	NNGP	DIWP
ELBO	MNIST	-0.301 ± 0.001	-0.268 ± 0.001	-0.214 ± 0.001
	CIFAR-10	-1.735 ± 0.002	-1.719 ± 0.001	-1.659 ± 0.001
test LL	MNIST	-0.130 ± 0.001	-0.134 ± 0.002	-0.122 ± 0.001
	CIFAR-10	-1.516 ± 0.002	-1.539 ± 0.002	-1.525 ± 0.003
test acc.	MNIST	96.5 ± 0.1%	96.5 ± 0.0%	96.9 ± 0.0%
	CIFAR-10	46.8 ± 0.1%	47.4 ± 0.1%	47.7 ± 0.2%
standard methods (e.g. CNNs) for these datasets, as we are using fully-connected networks with
only 100 inducing points (whereas e.g. work in the NNGP literature uses the full 60, 000 × 60, 000
covariance matrix). Nonetheless, as the architectures are carefully matched, it provides another
opportunity to compare the performance of DIWPs, NNGPs and DGPs. Again, we found that DIWP
usually gave statistically significant but perhaps underwhelming gains in predictive performance
(except for CIFAR-10 test-log-likelihood, where DIWP lagged by only 0.01). Importantly, DIWP
gives very large improvements in the ELBO, with gains of 0.09 against DGPs for MNIST and 0.08
for CIFAR-10 (App. K). For MNIST, remember that the ELBO must be negative (because both
the log-likelihood for classification and the KL-divergence term give negative contributions), so the
improvement from -0.301 to -0.214 represents a dramatic change.
7	Related work
Our first contribution was the observation that DGPs with isotropic kernels can be written as deep
Wishart processes as the kernel depends only on the Gram matrix. We then gave similar obser-
vations for neural networks (App. C.1), infinite neural networks (App. C.2) and infinite network
with bottlenecks (App. C.3, also see Aitchison, 2019). These observations motivated us to consider
the deep inverse Wishart process prior, which is a novel combination of two pre-existing elements:
nonlinear transformations of the kernel (e.g. Cho & Saul, 2009) and inverse Wishart priors over
kernels (e.g. Shah et al., 2014). Deep nonlinear transformations of the kernel have been used in
the infinite neural network literature (Lee et al., 2017; Matthews et al., 2018) where they form de-
terministic, parameter-free kernels that do not have any flexibility in the lower-layers (Aitchison,
2019). Likewise, inverse-Wishart distributions have been suggested as priors over covariance matri-
ces (Shah et al., 2014), but they considered a model without nonlinear transformations of the kernel.
Surprisingly, without these nonlinear transformations, the inverse Wishart prior becomes equivalent
to simply scaling the covariance with a scalar random variable (App. L; Shah et al., 2014). Further
linear (inverse) Wishart processes have been used in the financial domain to model how the volatility
of asset prices changes over time (Philipov & Glickman, 2006b;a; Asai & McAleer, 2009; Gourier-
oux & Sufana, 2010; Wilson & Ghahramani, 2010; Heaukulani & van der Wilk, 2019). Importantly,
inference in these dynamical (inverse) Wishart processes is often performed by assuming fixed, inte-
ger degrees of freedom, and working with underlying Gaussian distributed features. This approach
allows one to leverage standard GP techniques (e.g. Kandemir & Hamprecht, 2015; Heaukulani &
van der Wilk, 2019), but it is not possible to optimize the degrees of freedom and the posterior over
these features usually has rotational symmetries (App. D.2) that are not captured by standard varia-
tional posteriors. In contrast, we give a novel doubly-stochastic variational inducing point inference
method that operates purely on Gram matrices and thus avoids needing to capture these symmetries.
8	Conclusions
We proposed deep kernel processes which combine nonlinear transformations of the Gram matrix
with sampling from matrix-variate distributions such as the inverse Wishart. We showed that DGPs,
BNNs (App. C.1), infinite BNNs (App. C.2) and infinite BNNs with bottlenecks (App. C.3) are all
instances of DKPs. We defined a new family of deep inverse Wishart processes, and give a novel
doubly-stochastic inducing point variational inference scheme that works purely in the space of
Gram matrices. DIWP performed better than NNGPs and DGPs on UCI, MNIST and CIFAR-10
benchmarks.
9
Under review as a conference paper at ICLR 2021
References
Laurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. arXiv
preprint arXiv:1910.08013, 2019.
Manabu Asai and Michael McAleer. The structure of dynamic correlations in multivariate stochastic
volatility models. Journal OfEconometrics,150(2):182-192, 2009.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Taras Bodnar and Yarema Okhrin. Properties of the singular, inverse and generalized inverse parti-
tioned wishart distributions. Journal of Multivariate Analysis, 99(10):2389-2405, 2008.
Taras Bodnar, StePan Mazur, and Krzysztof Podgdrski. Singular inverse wishart distribution and its
application to portfolio theory. Journal of Multivariate Analysis, 143:314-326, 2016.
Youngmin Cho and Lawrence K Saul. Kernel methods for deeP learning. In Advances in neural
information processing systems, PP. 342-350, 2009.
Andreas Damianou and Neil Lawrence. DeeP gaussian Processes. In Artificial Intelligence and
Statistics, PP. 207-215, 2013.
A PhiliP Dawid. Some matrix-variate distribution theory: notational considerations and a Bayesian
aPPlication. Biometrika, 68(1):265-274, 1981.
Morris L.. Eaton. Multivariate Statistics. A Vector Space Approach.-A Volume in the Wiley Series in
Probability and Mathematical Statistics. Wiley-Interscience, 1983.
Yarin Gal and Zoubin Ghahramani. DroPout as a Bayesian aPProximation: RePresenting model
uncertainty in deeP learning. arXiv:1506.02142, 2015.
Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow gaussian Processes. arXiv preprint arXiv:1808.05587, 2018.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory
meets bayesian inference. In Advances in Neural Information Processing Systems, pp. 1884-
1892, 2016.
Christian Gourieroux and Razvan Sufana. Derivative pricing with wishart multivariate stochastic
volatility. Journal of Business & Economic Statistics, 28(3):438-451, 2010.
Creighton Heaukulani and Mark van der Wilk. Scalable bayesian dynamic covariance modeling with
variational wishart and inverse wishart processes. In Advances in Neural Information Processing
Systems, pp. 4582-4592, 2019.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J Smola. Kernel methods in machine learn-
ing. The annals of statistics, pp. 1171-1220, 2008.
Melih Kandemir and Fred A Hamprecht. The deep feed-forward gaussian process: An effective
generalization to covariance priors. In Feature Extraction: Modern Questions and Challenges,
pp. 145-159, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Plamen Koev and Alan Edelman. The efficient evaluation of the hypergeometric function ofa matrix
argument. Mathematics of Computation, 75(254):833-846, 2006.
AN Kolmogorov. Grundbegriffe der wahrscheinlichkeitreichnung. Ergebnisse der Mathematik,
1933.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
10
Under review as a conference paper at ICLR 2021
Vidhi Lalchand and Carl Edward Rasmussen. Approximate inference for fully bayesian gaussian
process regression. In Symposium on Advances in Approximate Bayesian Inference, pp. 1-12.
PMLR, 2020.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-
tation, 4(3):448-472, 1992.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
David A Moore. Symmetrized variational inference. In NIPS Workshop on Advances in Approximate
Bayesian Inferece, volume 4, pp. 31, 2016.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Sebastian W Ober and Laurence Aitchison. Global inducing point variational posteriors for bayesian
neural networks and deep gaussian processes. arXiv preprint arXiv:2005.08140, 2020.
Alexander Philipov and Mark E Glickman. Factor multivariate stochastic volatility via wishart
processes. Econometric Reviews, 25(2-3):311-334, 2006a.
Alexander Philipov and Mark E Glickman. Multivariate stochastic volatility via wishart processes.
Journal of Business & Economic Statistics, 24(3):313-328, 2006b.
Arya A Pourzanjani, Richard M Jiang, and Linda R Petzold. Improving the identifiability of neural
networks for bayesian inference. In NIPS Workshop on Bayesian Deep Learning, volume 4, pp.
31, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Omar Rivasplata, Vikram M Tankasali, and Csaba Szepesvari. Pac-bayes with backprop. arXiv
preprint arXiv:1908.07380, 2019.
Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian
processes. In Advances in Neural Information Processing Systems, pp. 4588-4599, 2017.
Amar Shah, Andrew Wilson, and Zoubin Ghahramani. Student-t processes as alternatives to Gaus-
sian processes. In Artificial intelligence and statistics, pp. 877-885, 2014.
Muni S Srivastava et al. Singular wishart and multivariate beta distributions. The Annals of Statistics,
31(5):1537-1560, 2003.
Pat H Sterbenz. Floating-point computation. Prentice-Hall, 1974.
Richard E Turner and Maneesh Sahani. Two problems with variational expectation maximisation
for time-series models. In D Barber, T Cemgil, and S Chiappa (eds.), Inference and Learning in
Dynamic Models. Cambridge University Press, 2011.
Harald Uhlig. On singular wishart and singular multivariate beta distributions. The Annals of Statis-
tics, pp. 395-405, 1994.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning.
MIT press Cambridge, MA, 2006.
Andrew Gordon Wilson and Zoubin Ghahramani. Generalised Wishart processes. arXiv preprint
arXiv:1101.0240, 2010.
11
Under review as a conference paper at ICLR 2021
A	DKPs are kernel processes
We define a generic DKP to be K(K), for a random matrix K ∈ RP×P. For instance, we could
take,
K(K) =W(K,N)	or	K(K) =W-1(K,δ+(P +1)),	(32)
where N is a positive integer and δ is a positive real number. A deep kernel process, D, is the
composition of two (or more) underlying kernel processes, K1 and K2,
Gi 〜Kι(K),	G2 〜K2(G1),	(33a)
G2 〜D(K).	(33b)
We define K*, G； and G2 as principle submatrices of K, Gi and G2 respectively, dropping the
same rows and columns. To establish that D is consistent under marginalisation, we use the consis-
tency under marginalisation of Ki and K2
G；〜Ki(K)	G；〜K2(GI),	(34a)
and the definition of the D as the composition of Ki and K2 (Eq. 33)
G；2 〜 D(K；).	(34b)
D is thus consistent under marginalisation, and hence is a kernel process.
Further, note that we can consider K to be a deterministic distribution that gives mass to only a single
G. In that case, K can be thought of as a deterministic function which must satisfy a corresponding
consistency property,
G = K(K),	G； = K(K；),	(35)
and this is indeed satisfied by all deterministic transformations of kernels considered here. In prac-
tical terms, as long as G is always a valid kernel, it is sufficient for the elements of Gi6=j to depend
only on Kij , Kii and Kjj and for Gii to depend only on Kjj , which is satisfied by e.g. the squared
exponential kernel (Eq. 15) and by the ReLU kernel (Cho & Saul, 2009).
B The first layer of our deep GP as Bayesian inference over a
GENERALISED LENGTHS CALE
In our deep GP architecture, we first sample Fi ∈ RP ×N1 from a Gaussian with covariance K0 =
N^- XXT (Eq. 4a). This might seem odd, as the usual deep GP involves passing the input, X ∈
RP×N0, directly to the kernel function. However, in the standard deep GP framework, the kernel
(e.g. a squared exponential kernel) has lengthscale hyperparameters which can be inferred using
Bayesian inference. In particular,
kparam( √N0xi, √N0Xj ) = exp (-2N0 (Xi - Xj ) ω (Xi - Xj ) ) ∙	(36)
where kparam is a new squared exponential kernel that explicitly includes hyperparmeters Ω ∈
RN0 × N0, and where Xi is the ith row of X. Typically, in deep GPs, the parameter, Ω, is diagonal,
and the diagonal elements correspond to the inverse square of the lengthscale, li, (i.e. Ω^ = 1/122).
However, in many cases it may be useful to have a non-diagonal scaling. For instance, we could use,
Ω 〜W (N11, Ni) ,	(37)
which corresponds to,
Ω = WWT,	where	Wiλ 〜N(0, Nk), W ∈ RN0 ×N1.	(38)
Under our approach, we sample F = Fi from Eq. (4b), so F can be written as,
F=XW,
fi = XiW,
(39)
12
Under review as a conference paper at ICLR 2021
Layer 1	Layer 2	Layer 3 Output Layer
X
X->
X-、
W1
F1
Ki - Fi
K1
W2
F2
K2 - F2
K2
K3
F3
W3
K3 一 F3
	W4 I	
—	→F4	—› Y
K4	T F4	—> Y
K4	→F4	—> Y
Figure 3:	A series of generative models for a standard, finite BNN. Top. The standard model, with
features, f`, and weights W` (Eq. 41). Middle. Integrating out the weights, the distribution over
features becomes Gaussian (Eq. 44), and We explicitly introduce the kernel, k`, as a latent variable.
Bottom. Integrating out the activations, f`, gives a deep kernel process, albeit one where the distri-
butions P (K'∣K'-ι) cannot be written down analytically, but where the expectation, E [K'∣K'-ι]
is known (Eq. 45).
where fi is the ith row of F. Putting this into a squared exponential kernel without a lengthscale
parameter,
k(√NN0fi, √1N0fj) = exp (-2NN0 (fi - fjMfi - fj)T)，
=exP (-2N0 (XiW - XjW)(XiW - XjW)T),
= exp (-2N0 (Xi -Xj)WWT(Xi -Xj)T ,
=exP (-2⅛0 (Xi - xj) ω (Xi - xj )T),
=kporam(√NN0Xi √1N0Xj).	(4O)
We find that a parameter-free squared exponential kernel applied to F is equivalent to a squared-
exponential kernel with generalised lengthscale hyperparameters applied to the input.
C BNNs as deep kernel processes
Here we show that standard, finite BNNs, infinite BNNs and infinite BNNs with bottlenecks can be
understood as deep kernel processes.
C.1 Stanard finite BNNs (and general DGPs)
Standard, finite BNNs are deep kernel processes, albeit ones which do not admit an analytic ex-
pression for the probability density. In particular, the prior for a standard Bayesian neural network
(Fig. 3 top) is,
P (W') = QNN= iN (wλ； 0,I∕N'-1),	W' ∈ Rn'-1×n',	(41a)
F' = [XW∖w for' =∙1,	F' ∈ RP×n'，	(41b)
φ (F'-i) W' otherwise,
where wλ' is the λth column ofW'. In the neural-network case, φ is a pointwise nonlinearity such as
a ReLU. Integrating out the weights, the features, F' , become Gaussian distributed, as they depend
linearly on the Gaussian distributed weights, W',
P (F'∣F'-1 ) = QN= iN fλ； 0, K') = P (F'∣K'),	(42)
where
K' = Nh Φ(F'-ι)Φτ (F'-i).	(43)
13
Under review as a conference paper at ICLR 2021
Crucially, f` depends on the previous layer activities, F'-ι only through the kernel, k`. As such,
we could write a generative model as (Fig. 3 middle),
1X XXT	for ' =1,
K =	N0	,
一∖-1Φ Φ(F'-ι)Φτ(F'-ι) otherwise,
P(F'∣K') = QN= ιN fλ; 0, Kg),
(44a)
(44b)
where We have explicitly included the kernel, k`, as a latent variable. This form highlights that
BNNS are deep GPs, in the sense that Fa are Gaussian, with a kernel that depends on the activations
from the previous layer. Indeed note that any deep GP (i.e. including those with kernels that cannot
be written as a function of the Gram matrix) as a kernel, k`, is by definition a matrix that can be
written as the outer product of a potentially infinite number of features, φ(Fg) where we allow φ to
be a much richer class of functions than the usual pointwise nonlinearities (Hofmann et al., 2008).
We might now try to follow the approach we took above for deep GPs, and consider a Wishart-
distributed Gram matrix, Gg = NFgFTT. However, for BNNs we encounter an issue: we are not
able to compute the kernel, Kg just using the Gram matrix, g`: we need the full set of features, Fg.
Instead, we need an alternative approach to show that a neural network is a deep kernel process.
In particular, after integrating out the weights, the resulting distribution is chain-structured (Fig. 3
middle), so in principle we can integrate out Fg to obtain a distribution over Kg conditioned on
Kg-1, giving the DKP model in Fig. 3 (bottom),
P (Kg∣Kg-ι) = / dFg-1 δD (Kg- Nφ(Fg-ι)φτ(Fg-1)) P(Fg-ι∣Kg-ι),	(45)
where P (Fg-1|Kg-1) is given by Eq. (44b) and δD is the Dirac-delta function, representing the
deterministic distribution, P (Kg|Fg-1) (Eq. 44a). Using this integral to write out the generative
process only in terms of Kg gives the deep kernel process in Fig. 3 (bottom). While this distribution
exists in principle, it cannot be evaluated analytically. But we can explicitly evaluate the expected
value of Kg given Kg-1 using results from Cho & Saul (2009). In particular, we take Eq. 44a, write
out the matrix-multiplication explicitly as a series of vector outer products, and note that as fλg is IID
across `, the empirical average is equal to the expectation of a single term, which is computed by
Cho & Saul (2009),
E [Kg+ι∣Kg] = NPN= ι E [φ(fλ)φτ(fλ)∣Kg] = E [φ(fg)φτ(fλ)|Kg],
=/ dfλ N fλ； O, Kg) Φ(f )ΦT f) ≡ K(Kg).	(46)
Finally, we define this expectation to be K(Kg) in the case of NNs.
C.2 Infinite NNs
We have found that for standard finite neural networks, we were not able to compute the distribution
over Kg conditioned on Kg-1 (Eq. (45)). To resolve this issue, one approach is to consider the limit
ofan infinitely wide neural network. In this limit, the Kg becomes a deterministic function ofKg-1,
as Kg can be written as the average of Ng IID outer products, and as Ng grows to infinity, the law of
large numbers tells us that the average becomes equal to its expectation,
Nli→∞Kg+i = Nl→∞]⅛PNιΦ(f )ΦT f ) = E [Φ(fλ)ΦT f )∣Kg] = K(Kg).	(47)
C.3 Infinite NNs with bottlenecks
In infinite NNs, the kernel is deterministic, meaning that there is no flexibility/variability, and hence
no capability for representation learning (Aitchison, 2019). Here, we consider infinite networks with
bottlenecks that combine the tractability of infinite networks with the flexibility of finite networks
(Aitchison, 2019). The trick is to separate flexible, finite linear “bottlenecks” from infinite-width
nonlinearities. We keep the nonlinearity infinite in order to ensure that the output kernel is determin-
istic and can be computed using results from Cho & Saul (2009). In particular, we use finite-width
14
Under review as a conference paper at ICLR 2021
Layer 1
W1
M1
X
X
X
X
K1
÷F1
÷F1
G1
G1
K2
Layer 2	Output Layer
+ F2
W2
一 K2 一 F2 一
M2		W3
		4,
	>F2			>F3
G2 -> f2	——	K3 一 F3
G2			K3 一 F3
GC 	 G2			X fʌe
		> F 3
Y
Y
Y
Y
KI -> f 1 —> GI —> F1
Figure 4:	A series of generative models for an infinite network with bottlenecks. First row. The
standard model. Second row. Integrating out the weights. Third row. Integrating out the features,
the Gram matrices are Wishart-distributed, and the kernels are deterministic. Last row. Eliminating
all deterministic random variables, we get a model equivalent to that for DGPs (Fig. 1 bottom).
f` ∈ RP ×N' and infinite width F' ∈ RP ×M', (We send m` to infinity while leaving n` finite),
P(W') = QN= 1N (wλ； 0,I∕M'-1)	Mo = M,	(48a)
F = (XW'	if' =1,
∣φ(F'-1)W' otherwise,
P (M') = QM=IN (mλ; 0, I∕N'),	(48c)
F' = f`m`.	(48d)
This generative process is given graphically in Fig. 4 (top).
Integrating over the expansion weights, M' ∈ rn'×m' , and the bottleneck weights, W` ∈
rm'-i×n' , the generative model (Fig. 4 second row) can be rewritten,
_ ∫N^XXT	for' = 1,
κ' = [ M-T φ (F'-1) φT (F'_1) OtherWise,	(49a)
P(F'∣K') = QN= iN (fλ； 0, K'),	(49b)
G' = N F'FT,	(49c)
P (F'∣G') = QM=IN (fλ'; 0, G') .	(49d)
Remembering that K'+1 is the empirical mean of M' IID terms, as M' → ∞ it converges on its
expectation
Mli→∞ K'+1= Mli→∞ MPN= 1Φ (fλ') φτ (fλ') = E[φ(fλ')φT(fλ')∣G'] = K(G').	(50)
and we define the limit to be K(G'). Note ifwe use standard (e.g. ReLU) nonlinearities, we can use
results from Cho & Saul (2009) to compute K(G'). Thus, we get the following generative process,
恳 XXT	for ' =1,
K(G'-1) otherwise,
(51a)
P (G') = W (G'； NK',N' .	(51b)
Finally, eliminating the deterministic kernels, K' , from the model, we obtain exactly the deep GP
generative model in Eq. 6 (Fig. C.3 fourth row).
D Standard approximate posteriors over features and weights
FAIL TO CAPTURE SYMMETRIES
We have shown that it is possible to represent DGPs and a variety of NNs as deep kernel processes.
Here, we argue that standard deep GP approximate posteriors are seriously flawed, and that working
with deep kernel processes may alleviate these flaws.
15
Under review as a conference paper at ICLR 2021
In particular, we show that the true DGP posterior has rotational symmetries and that the true BNN
posterior has permutation symmetries that are not captured by standard variational posteriors.
D. 1 Permutation symmetries in DNNs posteriors over weights
Permutation symmetries in neural network posteriors were known in classical work on Bayesian
neural networks (e.g. MacKay, 1992). Here, we spell out the argument in full. Taking P to be a
permutation matrix (i.e. a unitary matrix with PPT = I with one 1 in every row and column), we
have,
φ(F)P = φ(FP).	(52)
i.e. permuting the input to a nonlinearity is equivalent to permuting its output. Expanding two steps
of the recursion defined by Eq. (41b),
F' = φ(φ(F'-2)W'-1)W',	(53)
multiplying by the identity,
F' = φ(φ(F'-2)W'-1)PPτ W',	(54)
where P ∈ RN'-1 ×N'-1, applying Eq. (52)
F' = φ(φ(F'-2)W'-1P)PTW',	(55)
defining permuted weights,
W'0-1 = W'-1P,	W0'=PTW',	(56)
the output is the same under the original or permuted weights,
F' =φ(φ(F'-2)W'0-1)W'0 =φ(φ(F'-2)W'-1)W'.	(57)
Introducing a different perturbation between every pair of layers we get a more general symmetry,
W10 = W1P1,	(58a)
W' = PT-1W'P' for ' ∈ {2,..., L},	(58b)
W0L+1 = PLWL+1,	(58c)
where P' ∈ RN'-1×N'-1. As the output of the neural network is the same under any of these
permutations the likelihoods for original and permuted weights are equal,
P(Y∣X, Wι,..., Wl+i) = P(Y|X, W1,..., WL+1),	(59)
and as the prior over elements within a weight matrix is IID Gaussian (Eq. 41a), the prior probability
density is equal under original and permuted weights,
P (Wι,..., Wl+i) = P (W1,..., WL +1) .	(60)
Thus, the joint probability is invariant to permutations,
P(Y∣X, Wι,..., Wl+i) P(Wi,..., Wl+i) = P(Y|X, Wj,..., WL+J P (Wj,..., WL 十；
(61)
and applying Bayes theorem, the posterior is invariant to permutations,
P(Wι,..., Wl+i∣Y, X) = P (W1,..., WL +11 Y, X) .	(62)
Due in part to these permutation symmetries, the posterior distribution over weights is extremely
complex and multimodal. Importantly, it is not possible to capture these symmetries using standard
variational posteriors over weights, such as factorised posteriors, but it is not necessary to capture
these symmetries if we work with Gram matrices and kernels, which are invariant to permutations
(and other unitary transformations; Eq. 12).
16
Under review as a conference paper at ICLR 2021
D.2 Rotational symmetries in DEEP GP posteriors
To show that deep GP posteriors are invariant to unitary transformations, u` ∈ Rn'×n' , where
U'UT = I, we define transformed features, F',
F' = f` u`.	(63)
To evaluate P(F'|F'_J, we begin by substituting for F'_1,
P (FZIFZ1) = ∏N= 1N(	琛;0, Kl	'	1	τp∕ N-1F '-	1 F'-1)),	(64)
=∏N= 1N (	fλ'; 0, Kl	(N- F'-	1 UjUZ1FZ1)),	(65)
=∏N= 1N (	fλ'; 0, Kl	(N— F'-	. 1FZ1)),	(66)
= P(F0IF'_	1).			(67)
To evaluate P (F'∣F'_1), we substitute for F' in the explicit form for the multivariate Gaussian
probability density,
P(F'∣F'-ι) = -2 Tr (FTK-11F') + const,	(68)
=-1 Tr (K-LIF'F't) + const,	(69)
=-1 Tr (K∕1F'U'UTFT) + const,	(70)
=-2 Tr (K"'FT) + const,	(71)
=P(F'∣F'-ι).	(72)
where K'_1 = K (N^F'_1Fj_1^, and the constant depends only on F'_1. Combining these
derivations, each of these conditionals is invariant to rotations of f` and F'_1,
P (F'∣F'τ) = P (F'|F'_1) = P (F'|F'_1).	(73)
The same argument can straightforwardly be extended to the inputs, P (F1∣X),
P(F1∣X)=P(F1∣X),	(74)
and to the final probability density, for output activations, Fl+1 which is not invariant to permuta-
tions,
P(Fl+1 ∣FL)=P(Fl+1∣FL) ,	(75)
Therefore, we have,
P (FL,…，FL, Fl+1, Y∣X) = P(YIFL+1) P (Fl+1∣FL) (n P (F'∣F'τ)) P(FjX),
=	(76)
= P(YIFL+i)P(Fl+i∣Fl) (U P(FIFj)) P(Fι∣X), (77)
=P(Fι,..., Fl, Fl+i, Y∣X).	(78)
Therefore, applying Bayes theorem the posterior is invariant to rotations,
P(F1,..., FL, Fl+i∣X, Y)=P(Fi,..., Fl, Fl+i∣X, Y).	(79)
importantly, these posterior symmetries are not captured by standard variational posteriors with
non-zero means (e.g. Salimbeni & Deisenroth, 2017).
D.3 THE true posterior over features in a DGP HAS ZERO MEAN
We can use symmetry to show that the posterior of F' has zero mean. We begin by writing the
expectation as an integral,
旧此事―1, f'+1] = / dFF P(F'=FIF'_1, F'+1) .	(80)
17
Under review as a conference paper at ICLR 2021
Changing variables in the integral to F0 = -F, and noting that the absolute value of the Jacobian is
1, we have
=/ dF0 (-F0) P (F'= (-F0) ∣F'-ι, F'+ι),	(81)
using the symmetry of the posterior,
=/ dF0 (-F0)P(F'=F0∣F'-ι, F'+ι),	(82)
=-E [F'∣F'-ι, F'+ι],	(83)
the expectation is equal to minus itself, so it must be zero
E [F'|F'-1, F'+1] = 0.	(84)
E Difficulties with VI in deep Wishart processes
The deep Wishart generative process is well-defined as long as we admit nonsingular Wishart dis-
tributions (Uhlig, 1994; Srivastava et al., 2003). The issue comes when we try to form a variational
approximate posterior over low-rank positive definite matrices. This is typically the case because
the number of datapoints, P is usually far larger than the number of features. In particular, the only
convenient distribution over low-rank positive semidefinite matrices is the Wishart itself,
Q (G') = W (g`; NΨ,N) .	(85)
However, a key feature of most variational approximate posteriors is the ability to increase and
decrease the variance, independent of other properties such as the mean, and in our case the rank of
the matrix. For a Wishart, the mean and variance are given by,
E [G'] = Ψ,	(86)
Q(G')
OVJGj = N (ψ2j + ψiiψjj).
Q(Ge)
(87)
Initially, this may look fine: we can increase or decrease the variance by changing N' . However,
remember that N' is the degrees of freedom, which controls the rank of the matrix, G'. As such,
N' is fixed by the prior: the prior and approximate posterior must define distributions over matrices
of the same rank. And once N' is fixed, we no longer have independent control over the variance.
To go about resolving this issue, we need to find a distribution over low-rank matrices with indepen-
dent control of the mean and variance. The natural approach is to use a non-central Wishart, defined
as the outer product of Gaussian-distributed vectors with non-zero means. While this distribution
is easy to sample from and does give independent control over the rank, mean and variance, its
probability density is prohibitively costly and complex to evaluate (Koev & Edelman, 2006).
F Singular (inverse) Wishart processes at the input layer
In almost all cases of interest, our the kernel functions K(G) return full-rank matrices, so we can use
standard (inverse) Wishart distributions, which assume that the input matrix is full-rank. However,
this is not true at the input layer as Ko = N^ XXT will often be low-rank. This requires Us to
use singular (inverse) Wishart distributions which in general are difficult to work with (Uhlig, 1994;
Srivastava et al., 2003; Bodnar & Okhrin, 2008; Bodnar et al., 2016). As such, instead we exploit
knowledge of the input features to work with a smaller, full-rank matrix, Ω ∈ RN0 × N0, where,
remember, N0 is the number of input features in X. For a deep Wishart process,
NXΩXt = Gi 〜W (NIKo,Nι) ,	where	Ω	〜W (NII,Nι) ,	(88)
and for a deep inverse Wishart process,
NXΩXτ = Gi 〜WT (διKo,δι + P + 1), where Ω 〜WT (διl,δι + N + 1). (89)
Now, we are able to use the full-rank matrix, Ω rather than the low-rank matrix, Gi as the random
variable for variational inference. For the approximate posterior over Ω, in a deep inverse Wishart
process, we use
Q (Ω) = WT (διI + ViVT, δι + γι + (No + 1)) .	(90)
Note in the usual case where there are fewer inducing points than input features, then the matrix K0
will be full-rank, and we can work with Gi as the random variable as usual.
18
Under review as a conference paper at ICLR 2021
G Approximate posteriors over output features
To define approximate posteriors over inducing outputs, we are inspired by global inducing point
methods (Ober & Aitchison, 2020). In particular, we take the approximate posterior to be the prior,
multiplied by a “pseudo-likelihood”,
Q (Fl+i∣Gl) « P (Fl+i∣Gl) QNl+1 N (vι; fL+1, Λ-1) .	(91)
This is valid both for global inducing inputs and (for small datasets) training inputs, and the key
thing to remember is that in either case, for any given input (e.g. an MNIST handwritten 2), there
is a desired output (e.g. the class-label “2”), and the top-layer global inducing outputs, vλ, express
these desired outcomes. Substituting for the prior,
Q (Fl+i∣Gl) « QNl+1 N fL+1; 0, K(GL)) N (vι; fL+1, Λ-1),	(92)
and computing this value gives the approximate posterior in the main text (Eq. 19).
H	Using eigenvalues to compare deep Wishart, deep residual
Wishart and inverse Wishart priors
One might be concerned that the deep inverse Wishart processes in which we can easily perform
inference are different to the deep Wishart processes corresponding to BNNs (Sec. C.1) and infinite
NNs with bottlenecks (App. C.3). To address these concerns, we begin by noting that the (inverse)
Wishart priors can be written in terms of samples from the standard (inverse) Wishart
G = LΩLT,	G0 = LΩ0LT,	(93)
where K = LLT such that,
Ω 〜W (NLN),	Ω	〜WT (NI,λN),	(94)
G 〜W (NK,N),	G0 〜WT (NK,λN).	(95)
Note that as the standard Wishart and inverse Wishart have uniform distributions over the eigenvec-
tors (Shah et al., 2014), they differ only in the distribution over eigenvalues of Ω and Ω0. We plotted
the eigenvalue histogram for samples from a Wishart distribution with N = P = 2000 (Fig. 5
top left). This corresponds to an IID Gaussian prior over weights, with 2000 features in the input
and output layers. Notably, there are many very small eigenvalues, which are undesirable as they
eliminate information present in the input. To eliminate these very small eigenvalues, a common
approach is to use a ResNet-inspired architecture (which is done even in the deep GP literature,
e.g. Salimbeni & Deisenroth, 2017). To understand the eigenvalues in a residual layer, we define a
ResW distribution by taking the outer product ofa weight matrix with itself,
WWT = ω0 〜ResW (N, α),	(96)
where the weight matrix is IID Gaussian, plus the identity matrix, with the identity matrix weighted
as α,
W = √⅛2 (A ξ + al),	ξi,λ~N (0,1).	(97)
With α = 1, there are still many very small eigenvalues, but these disappear as α increases. We
compared these distributions to inverse Wishart distributions (Fig. 5 bottom) with varying degrees
of freedom. For all degrees of freedom, we found that inverse Wishart distributions do not produce
very small eigenvalues, which would eliminate information. As such, these eigenvalue distributions
resemble those for ResW with α larger than 1.
I Doubly stochastic variational inference in deep inverse
Wishart processes
Due to the doubly stochastic results in Sec. 4.3, we only need to compute the conditional distribution
over a single test/train point (we do not need the joint distribution over a number of test points). As
such, we can decompose G and Psi as,
(98)
19
Under review as a conference paper at ICLR 2021
Aou①nb①」-
Aou①nb①」-
W (Ni, N)	ResW(N, 1)	ResW(N,2)	ResW(N, 3)
300
200
100
0
W-1( N I, N)	W-1( N 1,3 N)	W-1 (N 1,5 N)	W-1 (N I,10 N)
300 -
200 -
100 -
0 -
0	1	2	3 0	1	2	3 0	1	2	3 0	1	2	3
eigenvalue	eigenvalue	eigenvalue	eigenvalue
Figure 5: Eigenvalue histograms for a single sample from the labelled distribution, with N = 2000.
where Gii,田社 ∈ RPi×Pi, g( ∈ RPitimesI and ψ^ ∈ RPi×1 are column-vectors, and gtt and ψtt are
scalars. Taking the results in Eq. (31) to the univariate case,
gtt∙i = gtt - gT' (Gi)TgL	ψtt∙i = ψtt - ψTψ-1ψit.	(99)
As g'∙i is univariate, its distribution becomes Inverse Gamma,
gtt∙i∣G∩, G'-ι 〜InverseGamma (α = 2 (δ' + Pt + Pi + 1), β = ɪψtt∙i) .	(100)
As gi`t is a vector rather than a matrix, its distribution becomes Gaussian,
(Gfi)TgiMi, G'“ g`-i 〜N (Ψ-1ψit, gtt∙iΨ1-1) .	(101)
J	S amples from the 1D prior and approximate posterior
First, we drew samples from a one-layer (top) and two-layer (bottom) deep inverse Wishart process,
with a squared-exponential kernel (Fig. 6). We found considerable differences in the function family
corresponding to different prior samples of the top-layer Gram matrix, GL (panels). While differ-
ences across function classes in a one-layer IW process can be understood as equivalent to doing
inference over a prior on the lengthscale, this is not true of the two-layer process, and to emphasise
this, the panels for two-layer samples all have the same first layer sample (equivalent to choosing a
lengthscale), but different samples from the Gram matrix at the second layer. The two-layer deep
IW process panels use the same, fixed input layer, so variability in the function class arises only
from sampling G2.
Next, we exploited kernel flexibilities in IW processes by training a one-layer deep IW model with
a fixed kernel bandwidth on data generated from various bandwidths. The first row in Figure 7
shows posterior samples from one-layer deep IW processes trained on different datasets. For each
panel, we first sampled five full G1 matrices using Eq.(31a) and (31b). Then for each G1, we
use Gaussian conditioning to get a posterior distribution on testing locations and drew one sample
from the posterior plotted as a single line. Remarkably, these posterior samples exhibited wiggling
behaviours that were consistent with training data even outside the training range, which highlighted
the additional kernel flexibility in IW processes. On the other hand, when model bandwidth was
fixed, samples from vanilla GPs with fixed bandwidth in the second row displayed almost identical
shapes outside the training range across different sets of training data.
20
Under review as a conference paper at ICLR 2021
-4 -2 0 2 4	-4 -2 0 2 4	-4 -2 0 2 4	-4 -2 0 2 4	-4 -2 0 2 4
X	XXXX
Figure 6: Samples from a one-layer (top) and a two-layer (bottom) deep IW process prior (Eq. 16).
On the far left, we have included a set of samples from a GP with the same kernel, for comparison.
This GP is equivalent to sending δ0 → ∞ in the one-layer deep IW process and additionally sending
δ1 → ∞ in the two-layer deep IW process. All of the deep IW process panels use the same squared-
exponential kernel with bandwidth 1. and δ0 = δ1 = 0. For each panel, we draw a single sample
of the top-layer Gram matrix, GL, then draw multiple GP-distributed functions, conditioned on that
Gram matrix.
Figure 7: The additional flexibility in a one-layer deep IW process can be used to capture mismatch
in the kernel. We plot five posterior function samples from trained IW processes in the first row,
and samples from trained GPs below. We generate different sets of training data from a GP with
different kernel bandwidths (0.5, 1, 2, 5, 10) across columns, while we keep the kernel bandwidth in
all models being 1.
21
Under review as a conference paper at ICLR 2021
K	Why we care about the ELBO
While we have shown that DIWP offers some benefits in predictive performance, it gives much
more dramatic improvements in the ELBO. While we might think that predictive performance is
the only goal, there are two reasons to believe that the ELBO itself is also an important metric.
First, the ELBO is very closely related to PAC-Bayesian generalisation bounds (e.g. Germain et al.,
2016). In particular, the bounds are generally written as the average training log-likelihood, plus the
KL-divergence between the approximate posterior over parameters and the prior. This mirrors the
standard form for the ELBO,
L = E [log P (x|z)] - DKL (Q (z) || P (z)) ,	(102)
Q(z)
where x is all the data (here, the inputs, X and outputs, Y), and z are all the latent variables.
Remarkably, Germain et al. (e.g. 2016) present a bound on the test-log-likelihood that is exactly
the ELBO per data point, up to additive constants. As such, in certain circumstances, optimizing the
ELBO is equivalent to optimizing a PAC-Bayes bound on the test-log-likelihood. Similar results are
available in Rivasplata et al. (2019). Second, we can write down an alternative form for the ELBO
as the model evidence, minus the KL-divergence between the approximate and true posterior,
L = logP (x) - DKL (Q (z) || P (z|x)) ≤ logP (x) .	(103)
As such, for a fixed generative model, and hence a fixed value of the model evidence, log P (x),
the ELBO measures the closeness of the variational approximate posterior, Q (z) and the true pos-
terior, P (z|x). As we are trying to perform Bayesian inference, our goal should be to make the
approximate posterior as close as possible to the true posterior. If, for instance, we can set Q (z) to
give better predictive performance, but be further from the true posterior, then that is fine in certain
settings, but not when the goal is inference. Obviously, it is desirable for the true and approxi-
mate posterior to be as close as possible, which corresponds to larger values of L (indeed, when the
approximate posterior equals the true posterior, the KL-divergence is zero, and L = log P (x) ).
L	Differences with Shah et al. (2014)
For a one-layer deep inverse Wishart process, using our definition in Eq. (16)
Ko = N10 XXT,	(104a)
P (Gι∣Ko) = W-1 (δ1Ko, δι + (P + 1)),	(104b)
P(yλ∣Kι)= N(yλ； 0, K (Gι)).	(104C)
Importantly, we do the nonlinear kernel transformation after sampling the inverse Wishart, so the
inverse-Wishart sample aCts as a generalised lengthsCale hyperparameter (App. B), and henCe dra-
matiCally Changes the funCtion family.
In Contrast, for Shah et al. (2014), the nonlinear kernel is Computed before, the inverse Wishart is
sampled, and the inverse Wishart sample is used direCtly as the CovarianCe for the Gaussian,
Ko = K (n10 XXT) ,	(105a)
P (Gι∣Ko) = W-1 (δ1K0, δι + (P + 1)),	(105b)
P(yλ∣Kι)= N(yλ; 0,G1).	(i05c)
This differenCe in ordering, and in partiCular, the laCk of a nonlinear kernel transformation between
the inverse-Wishart and the output is why Shah et al. (2014) were able to find trivial results in their
model (that it is equivalent to multiplying the CovarianCe by a random sCale).
22