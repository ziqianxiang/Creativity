Under review as a conference paper at ICLR 2021
Local SGD Meets Asynchrony
Ab stract
Distributed variants of stochastic gradient descent (SGD) are central to train-
ing deep neural networks on massive datasets. Several scalable versions of
data-parallel SGD have been developed, leveraging asynchrony, communication-
compression, and local gradient steps. Current research seeks a balance be-
tween distributed scalability-seeking to minimize the amount of synchronization
needed-and generalization performance-seeking to achieve the same or better ac-
curacy relative to the sequential baseline. However, a key issue in this regime
is largely unaddressed: if “local” data-parallelism is aggressively applied to bet-
ter utilize the computing resources available with workers, generalization perfor-
mance of the trained model degrades.
In this paper, we present a method to improve the ”local scalability” of decen-
tralized SGD. In particular, we propose two key techniques: (a) shared-memory
based asynchronous gradient updates at decentralized workers keeping the local
minibatch size small, and (b) an asynchronous non-blocking in-place averaging
overlapping the local updates, thus essentially utilizing all compute resources at
all times without the need for large minibatches. Empirically, the additional noise
introduced in the procedure proves to be a boon for better generalization. On
the theoretical side, we show that this method guarantees ergodic convergence for
non-convex objectives, and achieves the classic sublinear rate under standard as-
sumptions. On the practical side, we show that it improves upon the performance
of local SGD and related schemes, without compromising accuracy.
1	Introduction
In this paper, we consider the classic problem of minimizing an empirical risk, defined simply as
min X fi(x),	(1)
x∈Rd
i∈[I]
where d is the dimension, x ∈ Rd denotes the set of model parameters, [I] is the training set, and
fi (x) : Rd → R is the loss on the training sample i ∈ [I]. Stochastic gradient descent (SGD) (Rob-
bins & Monro, 1951) is an extremely popular iterative approach to solving this problem:
xk+ι = Xk - αkVfBk(Xk),	(2)
where VfBk (Xk) = ρ⅛j Pi∈^Bk Vfi(Xk) is the sum of gradients computed over samples, typically
selected uniformly and randomly as a minibatch Bk ⊆ [I], and αk is the learning rate at iteration k.
1.1	Background on Decentralized Data-parallel SGD
For better or worse, Sgd and its variants currently represent the computational backbone for many
large-scale optimization tasks, most notably the training of deep neural networks (DNNs). Arguably
the most popular SGD variant is minibatch SGD (Mb-sgd) (Bottou (2012)). In a distributed setting
with decentralized workers q ∈ [Q], it follows the iteration
1Q
xk+1 = Xk — αk Q EVfBq ,	(3)
Q q=1
where Bkq ⊆ [I] is a local minibatch selected by worker q ∈ [Q] at iteration k. This strategy is
straightforward to scale in a data-parallel way, as each worker can process a subset of the samples
in parallel, and the model is then updated by the average of the workers’ gradient computations. For
convenience, we assume the same batch size per worker. This approach has achieved tremendous
popularity recently, and there has been significant interest in running training with increasingly large
batch sizes aggregated over a large number of GPUs, e.g. Goyal et al. (2017).
An alternative approach is parallel or local SGD (L-sgd) (Zinkevich et al. (2010); Zhang et al.
(2016c); Lin et al. (2020)):	1
吟,t+1 =吟t-αj,NfBj,t, 0 ≤ t < Kj; Xq+1,0 = Q Σ>q,Kj,	⑷
q
1
Under review as a conference paper at ICLR 2021
where xjq,t denotes the local model at worker q ∈ [Q] after j synchronization rounds followed by t
local gradient updates and Bjq,t is the local minibatch sampled at the same iteration. Kj denotes the
number of local gradient update steps before the jth synchronization. Essentially, workers run SGD
without any communication for several local steps, after which they globally average the resulting
local models. This method is intuitively easy to scale, since it reduces the frequency of the commu-
nication. Recently, a variant called post local SGD (Pl-sgd) (Lin et al. (2020)), was introduced to
address the issue of loss in generalization performance of L-sgd, wherein the averaging frequency
during the initial phase of training is high and is reduced later when optimization stabilizes.
Method	BlOC	Train Loss	Train Acc.	Test Loss	Test Acc	Time (Sec)	Quality/ Perf.
Mb-sgd	128	0.016	99.75	0.234	92.95	1754	Baseline
Mb-sgd	1024	0.023	99.51	0.293	91.38	1201	OK
Pl-sgd	128	0.018	99.69	0.245	92.98	1603	Good
Pl-sgd	1024	0.154	94.69	0.381	87.81	1159	Poor
Table 1 : resnet-20/cifar-10 training for 300 epochs on 2 gpus.
Throughout the training the local BS is kept constant across workers i.e.
|Bkq| = Bloc, ∀q ∈ [Q], k ≥ 0 for MB-SGD and |Bjq,t | =
Bloc, ∀q ∈ [Q], j, t ≥ 0 for PL-SGD. The LR is warmed up for the first
5 epochs to scale from a0 to α0 × BBc ×Q , where Bloc = 128, Q is
the number of workers, 2 here, and α° = 0.1. In Pl-SGD, We average the
model after each gradient update for first 150 epochs and thereafter averag-
ing frequency K is set to 16 as in Lin et al. (2020); other HPs are identical
to theirs. The listed results are average of 3 runs with different seeds.
Although very popular in practice, these two
approaches suffer from the same limitation:
their generalization accuracy decreases for
larger local batch sizes, as would be ap-
propriate to fully utilize the computation
power offered by the workers. We illus-
trate this in Table 1: examine the results of
training ResNet-20 (He et al. (2016)) over
Cifar- 1 0 (Krizhevsky (2009)) with Mb-
sgd and Pl-sgd over a workstation packing
two Nvidia GeForce RTX 2080 Ti GPUs (a
current standard). We observe that as the lo-
cal batch size Bloc grows, the throughput im-
proves significantly, however, the optimiza-
tion results degrade sharply, more glaringly
with PL-SGD. Clearly, these methods can not tolerate a larger Bloc, though the GPUs can support
them. This shortcoming of the existing methods in harnessing the growing data-parallelism is also
identified via empirical studies (Golmant et al. (2018); Shallue et al. (2019)) existing in literature.
To our knowledge no effective remedy (yet) exists to address this challenge.
Notice that, here our core target is maximally harnessing the local data-parallelism and therefore the
larger local batch size, as against the existing trend in the literature wherein large number of GPUs
are deployed to have a large aggregated global batch size with a relatively small Bloc . For example,
refer to the performance of Mb-sgd and Pl-sgd as listed in Table 1 of Lin et al. (2020). Notice
that with 16 GPUs, each with Bloc = 128, thus totaling the minibatch size as 2048, identical to the
one with 2 GPUs each with Bloc = 1024 as above, with exactly the same LR scaling and warmup
strategy, both Mb-sgd and Pl-sgd do not face generalization degradation. However, unfortunately,
such an implementation setting would incur excessive wastage of available data-parallel compute
resources on each of the GPUs. Indeed, the existing specific techniques such as LARS (You et al.
(2017)) to address the issue of poor generalization for global large batch training are insufficient for
the larger local minibatch size; we empirically describe it in Section 3 (Table 11).
1.2 Locally-asynchronous Parallel SGD
Now, consider an implementation scheme as the following:
1.	In a decentralized setting of L-SGD, i.e. wherein each worker q ∈ [Q] has a local model xq
undergoing local SGD updates as described earlier, multiple local concurrent processes u ∈ Uq
share the model xq. Processes u ∈ Uq perform asynchronous concurrent gradient updates locally.
2.	The workers average their models whenever any one of them would have had at least Kj local
shared updates, where Kj is as that in Equation 4. The averaging is performed asynchronously
and in a non-blocking way by the (averaging-) processes aq on behalf of each worker q ∈ [Q].
Essentially, the decentralized workers run shared-memory-based asynchronous Sgd locally and pe-
riodically synchronize in a totally non-blocking fashion.
More formally, consider Algorithm 1. The model xq on a GPU q ∈ [Q] is shared by the processes
p ∈ Pq = {{aq} ∪Uq} locally. The processes p ∈ Pq also maintain a shared counter Sq, initialized
to 0. The operation read-and-inc implements an atomic (with lock) read and increment of
Sq, whereas, read provides an atomic read. Sq essentially enables ordering the shared gradient
updates. In turn, this order streamlines the synchronization among workers, thereby determines the
averaging rounds j. The (updater) processes u ∈ Uq asynchronously and lock-freely update xq with
2
Under review as a conference paper at ICLR 2021
gradients computed over a non-blocking, potentially inconsistent, snapshot va,q of xq , essentially
going Hogwild! (Recht et al. (2011)), see Algorithm 1a.
1	Initialize s = 0;
2	while s ≤ T do
3	vu,q [i] := xq [i], ∀ 1 ≤ i ≤ d;
4	s := read-and-inc(S);
5	Compute VfBq (vu,q);
6	xq[i] -= αsVfBq (vu,q)[i],
∀ 1 ≤ i ≤ d;
(a) Local asynchronous gradient update by
process u ∈ Uq.
1	Initialize scur = spre = |Uq|, j = 0;
2	while scur ≤ T do
3	scur := read(S); Compute j corresponding to scur;
4	if scur - spre ≥ Kj then
5	vja,q [i] := xq [i], ∀ 1 ≤ i ≤ d;
6	Synchronize across ar, r ∈ [Q] \	{q} to compute
vj := Q1 Pq∈[Q] va,q;
7	Compute ∆vj = Vj - Vaq ； Spre	：= Scur ；
8	xq[i] += ∆vj [i], ∀ 1 ≤ i ≤ d;	j = j + 1;
(b) Asynchronous non-blocking in-place averaging.
Algorithm 1: Locally-asynchronous Parallel SGD (Lap-sgd)
The process aq, which performs averaging for the worker q ∈ [Q], concurrently keeps on atomically
reading Sq, see Algorithm 1b. As soon as it notices an increment Kj in Sq, i.e. xq got concurrently
updated with Kj number of gradients , it takes a non-blocking snapshot vja,q of xq and synchro-
nizes with ar of peers r ∈ [Q]/q to compute the average Vj of the snapshots. Thereafter, aq adds
the difference of the average with the snapshot vja,q to the model xq without blocking the concur-
rent asynchronous local gradient updates. We call this method locally-asynchronous parallel SGD
(Lap-sgd). This method closely resembles Hogwild++ (Zhang et al. (2016a)), which targets the
heterogeneous NUMA based multi-core machines, though there are key differences which we de-
scribe in Section 4. Results of the same training task as before by Lap-sgd is given in Table 2. The
distinction of this implementation is that it harnesses the compute power of the GPUs not by increas-
ing the size of Bloc but by concurrently computing many minibatch gradients. Evidently, LAP-SGD
provides speed-up without losing the quality of optimization in comparison to the baseline.
Recently, Kungurtsev et al. (2019) presented
a shared-memory based method wherein they
showed that partitioned gradient updates for some
iterations during the course of training over a
shared model can reasonably save on total compu-
tation cost by means of restricted backpropagation
without necessarily losing on optimization quality.
|Uq|	Bloc	Train Loss	Train Acc.	Test Loss	Test Acc	Time (Sec)	Quality/ Perf.
4	128	0.012	99.83	0.270	92.92	1304	Excellent
6	128	0.023	99.51	0.266	92.91	1281	Excellent
4	256	0.010	99.90	0.280	92.93	1219	Excellent
Table 2: ResNet-20/Cifar- 10 training for 300 epochs on 2
GPUs by Lap-sgd. The LR is warmed up similar to Pl-sgd.We
follow the same averaging schedule and other HPs as those of Pl-
sgd. The listed results are average of 3 runs.
Their method is limited to a centralized shared-
memory setting. Moreover, aiming to establish
convergence under non-smoothness assumption, they ensure write consistency under a model-wide
lock. Having designed our asynchronous parallel SGD, it inspires us to adapt the partitioned gradient
update strategy to our lock-free decentralized setting.
More specifically, building on Lap-sgd, we
consider locally partitioned gradient computation
along with asynchronous lock-free updates. Es-
sentially, we partition the model xq to {xiq(u) } for
U ∈ Uq, i(u) ∩ i(w) = 0, ∀u,w ∈ Uq (i.e.,
non-overlapping block components of the vector
x). With that, a partitioned gradient computation
|Uq|	Bioc	Train Loss	Train Acc.	Test Loss	Test Acc	Time (Sec)	Quality/ Perf.
4	128	0.019	99.62	0.270	92.98	1153	Excellent
6	128	0.021	99.58	0.262	92.84	1085	Excellent
4	256	0.019	99.65	0.267	92.95	1047	Excellent
Table 3: Lpp-sgd performance. Other details are identical to
those in Table 2.
will amount to computing Vi(U)fps(vq,u), the minibatch gradient with respect to the partition
xiq(u) at line 5 in Figure 1a. Accordingly, the update step at line 6 in Algorithm 1a transforms
to xq [i] -= αsVfBsq (vq,u)[i], ∀ i ∈ i(u). It is to be noted that we do not use write lock for
iterations at any stage. Having devised a partitioned update scheme, we propose locally-partitioned-
asynchronous parallel SGD (LPP-SGD) as described below.
1.	Processes U ∈ Uq maintain a process-local variable last_iter which can take two values
PARTITIONED and FULL. EaCh U ∈ Uq initializes last_iter as FULL.
2.	While s ≤ Tst, each process U ∈ Uq performs LAP-SGD updates as lines 3 to 6 of Algorithm 1a.
3.	If Tst < s ≤ T , each process U ∈ Uq performs
3
Under review as a conference paper at ICLR 2021
(a)	a partitioned gradient computation and update: xq,u[i] -= a§NfBq (vu,q)[i], ∀i ∈ i(u) if
last-iter = FULL, and sets last_iter = PARTITIONED.
(b)	an LAP-SGD update if last_iter = PARTITIONED, and sets last_iter = FULL.
Essentially, after some initial stabilizing epochs each process u ∈ Uq alternates between a full and a
partitioned lock-free asynchronous gradient updates to the model xq . Our experiments showed that
Tst = IT was almost always sufficient to obtain a competitive optimization result. The results of a
sample implementation of Lpp-sgd are available in Table 3. It is clear that Lpp-sgd handsomely
speeds up the computation and provides equally competitive optimization results.
2 Convergence Theory
At a naive first glance, studying the convergence properties of locally asynchronous SGD would be
an incremental to existing analyses for local SGD, e.g. Stich (2018); Zhou & Cong (2017), in partic-
ular the local stochastic gradient evaluations at delayed lock-free Hogwild!-like parameter vectors.
However, there is one significant difference that presents a theoretical challenge: sometimes the
vectors used for gradient computation or components thereof, have been read from the local shared
memory before the last averaging across GPUs had taken place. Especially in the nonconvex case,
a priori it is impossible to place a reasonable bound on the accuracy of these gradient evaluations
relative to what they “should be” in order to achieve descent.
1
2
3
4
5
6
Initialize Xo = x0 for all q;
for j = 1, ..., J do
for all q do
Set x0,j = Xj;
for t = 1, ..., Kj do
Let Xqj = Xq-I —
qj
L αj,t,q" (i(j,t,q)f(vt );
Ycj
Let Xj+ 1 = Q P XKj ;
q=ι
Algorithm 2: Iterations of the view Xj.
In order to present a convergence rate result, We need to
7
define an anchor point on which we can consider conver-
gence to some approximate stationary point in expectation.
This is not trivial as both local iterations and averaging is
performed asynchronously across different GPUs at distinct
moments in time, with the time at which each iteration oc-
curs potentially varying with system-dependent conditions,
while the averaged iterates are what is important to consider
for convergence.
We seek to define the major iteration Xj as consistent with
the analysis presented for the convergence of local SGD in
the nonconvex setting in Zhou & Cong (2017). In this case,
with asynchrony, Xj is a theoretical construct, i.e., it may
not exist at any particular GPU’s memory at any moment in time. Let sjq := scur - |Uq| be the
current state of the shared counter before the jth synchronization round at the GPU q, then Xj is
defined as Xqq + ∆vj where Xqq is the state of the model for GPU q when va,q was saved and made
sj	* sj	j
available for averaging for “major iteration” j . Thus although de facto averaging could have taken
place after a set of local updates in time, these local updates correspond to updates after iteration j
conceptually. This makes Xj properly correspond to the equivalent iterate in Zhou & Cong (2017).
With that, we consider a sequence of major iteration views {Xj} with associated inter-averaging
local iteration quantities Kj and local views {vtq,j} at which an unbiased estimate of the (possibly
partitioned) gradient is computed, with 0 ≤ t < Kj as well as the local model in memory {Xtq,j }.
The partition, which could be the entire vector in general, is denoted by i(j, t, q). As each GPU has
its own corresponding annealed stepsize, we denote it in general as αj,t,q as well.
We state the formal mathematical algorithm as Algorithm 2. Note that this is the same procedure as
the practical “what the processor actually does” Algorithm 1, however, with the redefined terms in
order to obtain a precise mathematical sequence well-defined for analysis.
We make the following standard assumptions on unbiased bounded variance for the stochastic gra-
dient, a bounded second moment, gradient Lipschitz continuity, and lower boundedness of f.
qj	qj
Assumption 2.1. 1. It holds that Vif (vt,) satisfies, independent of i, E Vif (vt,)	=
Vif(Vq,j); E ∣∣Vif(vq,j)-Vif(vq,j"∣2 ≤ σ; E ∣∣Vif(vq,j)∣∣2 ≤ g.
2. f is Lipschitz continuously differentiable with constant L and is bounded below by fm.
We must also define some assumptions on the probabilistic model governing the asynchronous com-
putation. As these are fairly technical we defer them to the appendix.
4
Under review as a conference paper at ICLR 2021
Theorem 2.1. Given assumption 2.1, it holds that,
Q PPj=1 PPq=1 PPt=0 1[ɑj,t,qC1 - aj,t,qC2]E∣∣Vi(j,t,q)f(vq,j)∣∣2
+ Q PJ=1 PQ=1 PKj-1[αj,t,qC3 - α2,t,qC4]E ∣∣Vi(j,t,q)f (Xj )『
≤ f (Xθ) - fm
(5)
where C1, C2, C3, and C4 depend on L, B and probabilistic quantities defining the asynchronous
computation (see Appendix. Thus there exists a set of such constants such that if aj,t,q = Θ (√1j)
then Algorithm 2 ergodically converges with the standard O(1∕√J) ratefor nonconvex objectives.
Proof Summary: The proof follows the structure of the ergodic convergence proof of K-step local
SGD given in Zhou & Cong (2017), wherein at each round of averaging there are QKj total updates
to the model vector associated with the Q GPUs and Kj minor iterations.
Insofar as these updates are close (stochastically as an unbiased estimate, and based on the local
models not having changed too much) to the globally synchronized model vector at the last averaging
step, there is an expected amount of descent achieved due to the sum of these steps. This is balanced
with the amount of possible error in this estimate based on how far the model vector had moved.
In cases wherein vtq,,ij = xsq,,ij for s < 0 (i.e., the stochastic gradients are taken, due to local asyn-
chrony, at model vectors with components which existed in memory before the last averaging step),
we simply bound the worst-case increase in the objective.
To balance these two cases, the analysis takes an approach, inspired partially by the analysis given
in Cartis & Scheinberg (2018) of separating these as “good” and ”bad” iterates, with ”good” iterates
corresponding to views read after the last model was stored for averaging, with some associated
guaranteed descent in expectation, and “bad” iterates those read beforehand.
By considering the stochastic process governing the amount of asynchrony as being governed by
probabilistic laws, we can characterize the probability of a “good” and “bad” iterate and ultimately
seek to balance the total expected descent from one, and worst possible ascent in the other, as a
function of these probabilities.
Remark 2.1. [Speedup due to concurrent updates] Consider the case of classical vanilla local
SGD, in which there is complete symmetry in the number of local gradient computations between
averaging steps and block sizes across the processors. In this case, for every major iteration there
are Q gradient norms on the left hand side, and at the same time it is divided by Q. Thus local SGD
as a parallel method does not exhibit classical speedup, rather it can be considered as an approach
of using parallelism to have a more robust and stable method of performing gradient updates with
multiple batches computed in parallel. However, due to the idle time that exists between the slowest
and fastest processors, it will exhibit negative speedup, relative to the fastest processor. With the
approach given in this paper, this negative speedup is corrected for in that this potential idleness is
filled with additional stochastic gradient computations by the fastest process. Alternatively, one can
also consider this as exhibiting positive speedup relative to the slowest process, whereas standard
local SGD has zero speedup relative to the slowest process. Above and beyond this, we can consider
that as more processes introduces additional latency and delay, which has a mixed effect: on the one
hand, we expect that gradient norms at delayed iterates to be larger as the process is converging, thus
by having more delayed gradients on the left hand side, convergence is faster, and on the other hand,
such error in the degree to which the gradient decreases the objective, would specifically increase
the constants C2 and C4 .
3	Implementation Details and Numerical Results
3.1	Experimental Set-up
Decentralized training. We evaluate the proposed methods LAP-SGD and LPP-SGD compar-
ing them against existing Mb-sgd and Pl-sgd schemes, using CNN models ResNet-20 (He
et al. (2016)), SqueezeNet (Iandola et al. (2017)), and WideResNet-16x8 (Zagoruyko & Ko-
modakis (2016)) for the 10-/100-class image classification tasks on datasets Cifar- 1 0/Cifar- 1 00
(Krizhevsky (2009)). We also train ResNet-18 for a 1000-class classification problem on Ima-
genet (Russakovsky et al. (2015)) dataset. We keep the sample processing budget identical across
the methods. We use the typical approach of partitioning the sample indices among the workers that
can access the entire training set; the partition indices are reshuffled every epoch following a seeded
5
Under review as a conference paper at ICLR 2021
random permutation based on epoch-order. To this effect we use a shared-counter among concurrent
process u ∈ Uq in asynchronous methods. Thus, our data sampling is i.i.d.
Platform specification. Our experiments are based on a set of Nvidia GeForce RTX 2080 Ti
GPUs (Nvidia (2020)) (referred to as GPUs henceforth) with 11 GB on-device memory. We use the
following specific settings: (a) S1: a workstation with two GPUs and an Intel(R) Xeon(R) E5-1650
v4 CPU running @ 3.60 GHz with 12 logical cores, (b) S2: a workstation with four GPUs and two
Intel(R) Xeon(R) E5-2640 v4 CPUs running @ 2.40 GHz totaling 40 logical cores, and (c) S3: two
S2 workstations connected with a 100 GB/s infiniband link. The key enabler of our implementation
methodology are multiple independent client connection between a CPU and a GPU. Starting from
early 2018 with release of Volta architecture, Nvidia’s technology Multi-process Service (MPS)
efficiently support this. For more technical specifications please refer to their doc-pages MPS (2020).
Implementation framework. We used open-source Pytorch 1.5 (Paszke et al. (2017)) library for
our implementations. For cross-GPU/cross-machine communication we use NCCL (NCCL (2020))
primitives provided by Pytorch. Mb-sgd is based on DistributedDataParallel Pytorch
module. Pl-sgd implementation is derived from author’s code (LocalSGD (2020)) and adapted to
our setting. Having generated the computation graph of the loss function of a CNN, the autograd
package of Pytorch allows a user to specify the leaf tensors with respect to which gradients are
needed. We used this functionality in implementing partitioned gradient computation in Lpp-sgd.
Locally-asynchronous Implementation. One key requirement of the proposed methods is to sup-
port a non-blocking synchronization among GPUs. This is both a challenge and an opportunity.
To specify, we use a process on each GPU, working as a parent, to initialize the CNN model
and share it among spawned child-processes. The child-processes work as u ∈ Uq, ∀q ∈ [Q]
to compute the gradients and update the model. Concurrently, the parent-process, instead of re-
maining idle as it happens commonly with such concurrency models, acts as the averaging process
aq ∈ Pq, ∀q ∈ [Q], thereby productively utilizing the entire address space occupied over the GPUs.
The parent- and child-processes share the iteration and epoch counters. Notice that, here we are us-
ing the process-level concurrency which is out of the purview of the thread-level global interpreter
lock (GIL) (Python (2020)) of python multi-threading framework.
Hyperparameters (HPs). Each of the methods use identical momentum (Sutskever et al. (2013))
and weight-decay (Krogh & Hertz (1991)) for a given CNN/dataset case; we rely on their previously
used values (Lin et al. (2020)). The learning rate (LR) schedule for Mb-sgd and Pl-sgd are
identical to Lin et al. (2020). For the proposed methods we used cosine annealing schedule without
any intermediate restart (Loshchilov & Hutter (2017)). Following the well-accepted practice, we
warm up the LR for the first 5 epochs starting from the baseline value used over a single worker
training. In some cases, a grid search (Pontes et al. (2016)) suggested that for Lpp-sgd warming up
the LR up to 1.25× of the warmed-up LR of Lap-sgd for the given case, improves the results.
3.2	Experimental Results
In the following discussion we use these abbreviated notations: U : |Uq|,
B : Bloc, Tr.L.: Training Loss, Tr.A.: Training Accuracy, Te.L.: Test
Loss, Te.A.: Test Accuracy, and T: Time in Seconds. The asynchronous
methods have inherent randomization due to process scheduling by the
operation system. Therefore, each micro-benchmark presents the mean
of 3 runs unless otherwise mentioned.
U	B	TrA	Te.A.	T
4	128	99.62	92.81	1304
6	128	99.85	92.92	1266
8	128	99.62	92.64	1259
4	256	99.72	92.83	1219
6	256	99.90	92.24	1166
Table 4:	Lap-sgd perfor-
mance.
Concurrency on GPUs. We allocate the processes on a GPU up to
the availability of the on-device memory. However, once the data-parallel compute resources get
saturated, allocating more processes degrades the performance. For example, see Table 4 which lists
the average performance of 5 runs for different combinations of U and B for training ResNet-
20/CIFAR- 1 0 by LAP-SGD on the setting S1.
Asynchronous Averaging Frequency. Following PL-SGD, as a general rule, for the first half of
the training, call it up until P epochs, we set the averaging frequency K = 1. However, notice
that, unlike Pl-sgd, setting a K < Q in Lap-sgd and Lpp-sgd may not necessarily increase the
averaging rounds j in aggregation. Intuitively, in a locally-asynchronous setting, along with the non-
blocking (barrier-free) synchronization among GPUs, the increment events on the shared-counter S
would be “grouped” on the real-time scale if the processes u ∈ Uq do not face delays in scheduling,
which we control by allocating an optimal number of processes to maximally utilize the compute
6
Under review as a conference paper at ICLR 2021
resources. For instance, Table 5 lists the results of 5 random runs of ResNet-20/Cifar- 1 0 training
with B = 128 and U = 6 with different combinations of K and P over S1. This micro-benchmark
indicates that the overall latency and the final optimization result of our method may remain robust
under small changes in K, which it may not be the case with Pl-sgd.
Scalability. Table 6 presents the results of WIDERESNET-
16x8/Cifar- 1 0 training in the three system settings that we consider.
We observe that in each setting the relative speed-up of different
methods are approximately consistent. In particular, we note the
following: (a) reduced communication cost helps Pl-sgd marginally
outperform MB-SGD, (b) interestingly, increasing Bloc from 128 to
512 does not improve the latency by more than 〜4%; this non-linear
KlP I Tr.A. I Te.A. ∣ T
1	150	99.76	93.01	1266
4	150	99.85	92.78	1263
8	150	99.62	93.08	1262
8	0	99.78	92.83	1263
16	150	99.90	93.16	1261
Table 5:	Lap-sgd perfor-
mance.
scalability of data-parallelism was also observed by Lin et al. (2020) , (c) each of the methods scale
by more than 2x as the implementation is moved to S2, which has 40 CPU cores, from S1 which
has 12 CPU cores, furthermore, this scalability is approximately 2x with respect to performance
on S3 in comparison to S2, this shows that for each of the methods we utilize available compute
resources maximally, (d) in each setting LAP-SGD achieves 〜30% better throughput compared to
MB-SGD standard batch, (e) in each setting LPP-SGD outperforms LAP-SGD by 〜12% making it
the fastest method, (f) the training and test accuracy of local large minibatch is poor, and (g) the
methods LAP-SGD and LPP-SGD consistently improve on the baseline generalization accuracy.
8265	7920	8293	7829 7500	6368	5551 0	⑴	(2)	(3)	(4)	(5)	(6)	4045	3913	4022	3784	Lap-sgd	■ Lpp-sgd	■ Mb-sgd(B=128)	■	■ Mb-sgd (B=512) ■ Pl-sgd (B=128) ■ Pl-sgd (B=512)
	III (7)	(8)	(9)	(10)	(11)	(12)	2116	2076 1562	1377 ■	■	■	■ (13)	(14)	(15)	(16)	2105 1977 ■ ■ (17)	(18)
Il Method ∣∣ IQl Tr.A. ∣ Te.A. ∣ T ∣∣ IQl Tr.A. ∣ Te.A.∣ T ∣∣ IQl Tr.A. ∣ Te.A.∣ T ∣∣
Lap-sgd	(1)	2	100.0	94.92	6368	(7)	4	100.0	94.91	3092	(13)	8	100.0	94.69	1562
Lpp-sgd	(2)	2	99.97	94.89	5551	(8)	4	99.96	94.87	2724	(14)	8	99.99	93.78	1377
Mb-sgd-128	(3)	2	100.0	93.98	8265	(9)	4	100.0	94.22	4045	(15)	8	100.0	93.83	2116
Mb-sgd-512	(4)	2	95.68	88.42	7920	(10)	4	99.87	89.46	3913	(16)	8	87.54	73.97	2076
Pl-sgd-128	(5)	2	99.99	94.21	8293	(11)	4	99.98	94.02	4022	(17)	8	99.98	94.18	2105
Pl-sgd-512	(6)	2	95.34	88.33	7829	(12)	4	98.52	84.45	3784	(18)	8	89.19	81.80	1977
Table 6: WIDERESNET-16x8/CIFAR-10 training in the settings S1: Q = 2, S2: Q = 4, and S3: Q = 8. MB-
SGD-128 indicates MB-SGD method with Bloc = 128 and similarly for others. For LAP-SGD, we spawned 4
concurrent processes on each GPU: U = 4, whereas for Lpp-sgd U = 3 concurrent processes were spawned.
I Method	B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T Il Method		B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T I
Lap-sgd	128	6	0.295	1.191	92.23	69.43	1285	Mb-sgd	1024	-	0.464	1.212	86.46	67.04	1244
Lap-sgd	128	4	0.299	1.198	92.15	69.78	1325	Mb-sgd	128	-	0.360	1.111	89.93	69.73	1754
Lpp-sgd	128	6	0.404	1.173	88.69	69.47	1085	Pl-sgd	1024	-	0.373	1.152	89.48	67.97	1198
Lpp-sgd	128	4	0.378	1.161	89.50	69.75	1154	Pl-sgd	128	-	0.379	1.099	89.29	69.67	1613
Table 7: Performance of RESNET-20 on CIFAR- 1 00 over the setting S1.															
I Method	B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T Il Method		B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T I
Lap-sgd	64	4	0.007	1.341	99.98	75.97	1564	Mb-sgd	512	-	0.034	1.918	99.64	63.74	2076
Lpp-sgd	64	3	0.008	1.554	99.97	75.42	1376	Pl-sgd	128	-	0.020	1.354	99.81	72.38	2108
Mb-sgd	128	-	0.007	1.348	99.97	73.29	2115	Pl-sgd	512	-	0.393	1.758	88.91	60.47	1976
Table 8: Performance of WIDERESNET-16x8 on CIFAR- 1 00 over the setting S3.
I Method	B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T Il Method		B	U	Tr.L.	Te.L.	Tr.A.	Te.A.	T I
Lap-sgd	128	4	0.004	0.385	99.95	92.48	1251	Mb-sgd	128	-	0.005	0.294	99.93	92.41	2243
Lpp-sgd	256	3	0.007	0.355	99.89	92.55	1027	Pl-sgd	1024	-	0.005	0.329	99.93	91.99	1202
Mb-sgd	1024	-	0.003	0.325	99.97	91.50	1580	Pl-sgd	128	-	0.006	0.283	99.91	92.51	1843
Table 9: Performance of S QUEEZENET on CIFAR- 1 0 over the setting S1.
Other Cifar-10/Cifar-100 Results. Performance of the proposed methods in comparison to
the baselines for other training tasks on Cifar- 1 0/Cifar- 1 00 datasets are available in Tables 7,
8, and 9. In each case we use K = 16 in Lap-sgd, Lpp-sgd, and Pl-sgd after 50% of total
sample processing budget. As an overview, the relative latency of the methods are as seen in Table
6, whereas in each case Lap-sgd and Lpp-sgd recovers or improves the baseline training results.
Imagenet Training Results. Having comprehensively evaluated the proposed methods on
Cifar- 1 0/Cifar- 1 00, here we present their performance on 1000-classes Imagenet dataset.
7
Under review as a conference paper at ICLR 2021
—	Method	B	U	Tr.L.	Tr.A.	Te.L.	Te.A.	T I
⑴	Lap-sgd	32	4	1.319	68.97	1.215	70.58	26237
⑵	Lpp-sgd	32	4	1.397	68.31	1.257	69.91	24066
⑶	Mb-sgd	128	-	1.237	70.63	1.220	70.07	26772
(4)	Mb-sgd	32	-	1.339	68.53	1.231	70.13	30706
⑸	Pl-sgd	128	-	1.256	70.22	1.223	70.02	25850
⑹	Pl-sgd	32	-	1.350	68.32	1.236	70.02	31468
Table 10: RES NET-18/IMAGENET Training results.
Figure 2: Top-1 Test Accurcy.
Notice that for this training task, with 8 commodity GPUs at our disposal we are very much in the
small minibatch setting. Plethora of existing work in the literature efficiently train a ResNet on Im-
agenet with BS up to multiple thousands. Other system-dependent constraint that our considered
setting faces is that there is hardly any leftover compute resources for us to exploit in the local setting
of a worker. Yet, we see for ResNet-18, see Table 10, that Lap-sgd improves on generalization
accuracy of the baseline with speed-up.
LR Tuning strategy. It is pertinent to mention that the ex-
isting techniques, such as LARS (You et al. (2017)), which
provide an adaptive LR tuning strategy for large minibatch set-
tings over a large number of GPUs, wherein each worker lo-
cally processes a small minibatch, are insufficient in the case
Bloc is increased. For example, see Table 11 which lists the
Method	U	B	Tr.A.	Te.A.	T
MB-SGD	1	1024	95.84	85.67	1324
Pl-sgd	1	1024	93.67	84.69	1304
LAP-SGD	4	256	99.87	92.90	1232
Lpp-sgd	4	256	99.59	92.83	1071
Table 11: LARS performance.
average performance of 5 runs on the setting S1 for Q = 2 for training RESNET-20/CIFAR- 1 0
using the compared methods combined with LARS with η = 0.001 (You et al. (2017)). We scale
the LR proportionately: α° X BB×Q, where Bloc = 128, Q is the number of workers, 2 here, and
— Bloc	---
ao = 0.1. Evidently, LARS did not help MB-SGD and PL-SGD in checking the poor generalization
due to larger B.
3.3	On The Generalization of the Proposed Methods
Let us comment on a theoretical basis of the remarkable performance in terms of generalization error
of our methods. In Lin et al. (2020), the model of SGD algorithms as a form of a Euler-Maruyama
discretization ofa Stochastic Differential Equation (SDE) presents the perspective that the batch-size
can correspond to the inverse of the injected noise. Whereas distributed SGD combined stochastic
gradients as such effectively results in an SGD step with a larger batch-size, local SGD, by averaging
models rather than gradients, maintains the noise associated with the local small-batch gradients.
Given the well-established benefits of greater noise in improving generalization accuracy (e.g. Smith
& Le (2018) and others), this presents a heuristic argument as to why local SGD tends to generalize
better than distributed SGD. In the Appendix we present an additional argument for why local SGD
generalizes well.
However, we see that our particular variation with asynchronous local updates and asynchronous
averaging seems to provide additional generalization accuracy above and beyond local SGD. We
provide the following explanation as to why this could be the case, again from the perspective of
noise as it would appear in a SDE. Let us recall three facts,
1.	The noise appearing as a discretization of the Brownian motion term in the diffusion SDE, and,
correspondingly the injected noise studied as a driver for increased generalization in previous
works on neural network training is i.i.d.,
2.	Clearly, the covariances of a mini-batch gradients as statistical estimates of the gradient at x and
x0 are going to be more similar when x and x0 are closer together,
3.	(see Section 2) A challenging property from the perspective of convergence analysis with locally
asynchronous updates is gradients taken at models taken at snapshots before a previous all-to-all
averaging step, and thus far from the current model in memory.
Thus, effectively, the presence of these “highly asynchronous” stochastic gradients, while being
potentially problematic from the convergence perspective, effectively brings the analogy of greater
injected noise for local SGD over distributed data-parallel closer to accuracy by inducing greater
probabilistic independence, i.e., the injected noise, for these updates, is far close to the i.i.d. noise
that appears in a discretized SDE.
4	Related Work
In the previous sections we cited the existing literature wherever applicable, in this section we
present a brief overview of closely related works and highlight our novelty. In the shared-memory
8
Under review as a conference paper at ICLR 2021
setting, Hogwild ! (Recht et al. (2011)) is now the classic approach to implement SGD. How-
ever, it remains applicable to a centralized setting of a single worker and therefore is not known
to have been practically utilized for large scale DNN training. Its success led to designs of vari-
ants which targeted specific system aspects of a multi-core machine. For example, Buckwild! (Sa
et al. (2015)) proposed using restricted precision training on a CPU. Another variant, called Hog-
wild!++ (Zhang et al. (2016b)), harnesses the non-uniform-memory-access (NUMA) architecture
based multi-core computers. In this method, threads pinned to individual CPUs on a multi-socket
mainboard with access to a common main memory, form clusters.
In principle, the proposed Lap-sgd can be seen as deriving from Hogwild ! ++. However, there are
important differences: (a) at the structural level, the averaging in Hogwild ! ++ is binary on a ring
graph of thread-clusters, further, it is a token based procedure where in each round only two neigh-
bours synchronize, whereas in Lap-sgd it is all-to-all, (b) in Hogwild ! ++ each cluster maintains
two copies of the model: a locally updating copy and a buffer copy to store the last synchronized
view of the model, whereby each cluster essentially passes the “update” in the local model since
the last synchronization to its neighbour, however, this approach has a drawback as identified by the
authors: the update that is passed on a ring of workers eventually “comes back” to itself thereby
leading to divergence, to overcome this problem they decay the sent out update; as against this,
Lap-sgd uses no buffer and does not track updates as such, averaging the model with each peer,
similar to L-sgd, helps each of the peers to adjust their optimization dynamics, (c) it is not known
if the token based model averaging of Hogwild ! ++ is sufficient for training DNNs where gener-
alization is the core point of concern, in place of that we observed that our asynchronous averaging
provides an effective protocol of synchronization and often results in improving the generalization,
(d) comparing the Hogwild ! ++ thread-clusters to concurrent processes on GPUs in Lap-sgd, the
latter uses a dedicated process that performs averaging without disturbing the local gradient up-
dates thereby maximally reducing the communication overhead, (e) finally, the convergence theory
of Lap-sgd guarantees its efficacy for DNN training, which we demonstrated experimentally, by
contrast, Hogwild ! ++ does not have any convergence guarantee.
Recently, Wang et al. (2020) proposed Overlap-local-SGD, wherein they suggested to keep a model
copy at each worker, very similar to Hogwild ! ++, which is simultaneously averaged when se-
quential computation for multiple iterations happen locally. They showed by limited experiments
that it reduced the communication overhead in a non-iid training case based on Cifar- 1 0, however,
not much is known about its performance in general cases. The asynchronous partitioned gradient
update of Lpp-sgd derives from Kungurtsev et al. (2019), however, unlike them we do not use locks
and our implementation setting is decentralized, thus scalable.
5	Conclusion
Picking from where Golmant et al. (2018) concluded referring to their findings: “These results
suggest that we should not assume that increasing the batch size for larger datasets will keep training
times manageable for all problems. Even though it is a natural form of data parallelism for large-
scale optimization, alternative forms of parallelism should be explored to utilize all of our data more
efficiently”, our work introduces a fresh approach in this direction to addressing the challenge.
In our experiments, we observed that the natural system-generated noise in some cases effectively
improved the generalization accuracy, which we could not obtain using the existing methods ir-
respective of any choice of seed for random sampling. The empirical findings suggest that the
proposed variant of distributed Sgd has a perfectly appropriate place to fit in the horizon of efficient
optimization methods for training deep neural networks. As a general guideline for the applicability
of our approach, we would suggest the following: monitor the resource consumption of a GPU that
trains a CNN, if there is any sign that the consumption was less than 100%, try out Lap-sgd and
Lpp-sgd instead of arduously, and at times unsuccessfully, tuning the hyperparameters in order to
harness the data-parallelism. The asynchronous averaging protocol makes Lap-sgd and Lpp-sgd
specially attractive to settings with large number of workers.
There are a plethora of small scale model and dataset combinations, where the critical batch size-
after which the returns in terms of convergence per wall-clock time diminish-is small relative to
existing system capabilities (Golmant et al. (2018)). To such cases Lap-sgd and Lpp-sgd be-
come readily useful. Yet, exploring the efficiency of Lap-sgd and Lpp-sgd to train at massive
scales, where hundreds of GPUs enable training Imagenet in minutes (Ying et al. (2018)), is an
ideal future goal. We also plan to extend the proposed methods to combine with communication
optimization approaches such as QSGD (Alistarh et al. (2017)).
9
Under review as a conference paper at ICLR 2021
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and M. Vojnovic. Qsgd: Communication-
efficient sgd via gradient quantization and encoding. In NIPS, 2017.
Nils Berglund. Kramers’ law: Validity, derivations and generalisations. arXiv preprint
arXiv:1106.5799, 2011.
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421-
436. Springer, 2012.
Coralia Cartis and Katya Scheinberg. Global convergence rate analysis of unconstrained optimiza-
tion methods based on probabilistic models. Mathematical Programming, 169(2):337-375, 2018.
Xiaowu Dai and Yuhua Zhu. Towards theoretical understanding of large batch training in stochastic
gradient descent. arXiv preprint arXiv:1812.00542, 2018.
Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, A. Gholami, Kai Rothauge, M. Ma-
honey, and Joseph E. Gonzalez. On the computational inefficiency of large batch sizes for stochas-
tic gradient descent. ArXiv, abs/1811.12941, 2018.
Priya Goyal, P. Dollar, Ross B. Girshick, P. Noordhuis, L. Wesolowski, Aapo Kyrola, Andrew Tul-
loch, Y. Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. ArXiv,
abs/1706.02677, 2017.
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Forrest N. Iandola, Matthew W. Moskewicz, K. Ashraf, Song Han, W. Dally, and K. Keutzer.
SqUeezenet: AIexnet-level accuracy with 50x fewer parameters and j1mb model size. ArXiv,
abs/1602.07360, 2017.
Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail
Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima.
In 5th International Conference on Learning Representations, ICLR 2017, 2019.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
A. Krogh and J. Hertz. A simple weight decay can improve generalization. In NIPS, 1991.
V. Kungurtsev, Malcolm Egan, Bapi Chatterjee, and Dan Alistarh. Asynchronous stochastic subgra-
dient methods for general nonsmooth nonconvex optimization. arXiv: Optimization and Control,
2019.
Tao Lin, S. Stich, and M. Jaggi. Don’t use large mini-batches, use local sgd. ArXiv, abs/1808.07217,
2020.
LocalSGD. https://github.com/epfml/LocalSGD-Code. 2020.
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017.
Nvidia MPS. https://docs.nvidia.com/deploy/mps/index.html. 2020.
Giorgi Nadiradze, Ilia Markov, Bapi Chatterjee, Vyacheslav Kungurtsev, and Dan Alistarh. Elas-
tic consistency: A general consistency model for distributed stochastic gradient descent. arXiv
preprint arXiv:2001.05918, 2020.
NCCL. https://docs.nvidia.com/deeplearning/nccl/index.html. 2020.
Nvidia.	https://www.nvidia.com/de-at/geforce/graphics- cards/
rtx-2080-ti/. 2020.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
10
Under review as a conference paper at ICLR 2021
F. J. Pontes, Gabriela da F. de Amorim, P. Balestrassi, A. P. Paiva, and J. R. Ferreira. Design of
experiments and focused grid search for neural network parameter optimization. Neurocomputing,
186:22-34, 2016.
Python.	https://docs.python.org/3/glossary.html#
term- global- interpreter- lock. 2020.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in neural information processing systems,
pp. 693-701, 2011.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Zhiheng Huang, A. Karpathy,
A. Khosla, M. Bernstein, A. Berg, and Li Fei-Fei. Imagenet large scale visual recognition chal-
lenge. International Journal of Computer Vision, 115:211-252, 2015.
C. D. Sa, Ce Zhang, K. Olukotun, and C. Re. Taming the wild: A unified analysis of hogwild-style
algorithms. Advances in neural information processing systems, 28:2656-2664, 2015.
Christopher J. Shallue, Jaehoon Lee, J. Antognini, Jascha Sohl-Dickstein, Roy Frostig, and G. Dahl.
Measuring the effects of data parallelism on neural network training. J. Mach. Learn. Res., 20:
112:1-112:49, 2019.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. In International Conference on Learning Representations, 2018.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018.
Ilya Sutskever, J. Martens, G. Dahl, and Geoffrey E. Hinton. On the importance of initialization and
momentum in deep learning. In ICML, 2013.
Jianyu Wang, Hao Liang, and G. Joshi. Overlap local-sgd: An algorithmic approach to hide com-
munication delays in distributed sgd. ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 8871-8875, 2020.
Chris Ying, S. Kumar, Dehao Chen, T. Wang, and Youlong Cheng. Image classification at super-
computer scale. ArXiv, abs/1811.06992, 2018.
Yang You, Igor Gitman, and B. Ginsburg. Large batch training of convolutional networks. arXiv:
Computer Vision and Pattern Recognition, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. ArXiv, abs/1605.07146, 2016.
Huan Zhang, Cho-Jui Hsieh, and V. Akella. Hogwild++: A new mechanism for decentralized
asynchronous stochastic gradient descent. 2016 IEEE 16th International Conference on Data
Mining (ICDM), pp. 629-638, 2016a.
Huan Zhang, Cho-Jui Hsieh, and Venkatesh Akella. Hogwild++: A new mechanism for decentral-
ized asynchronous stochastic gradient descent. In 2016 IEEE 16th International Conference on
Data Mining (ICDM), pp. 629-638. IEEE, 2016b.
Jian Zhang, C. D. Sa, Ioannis Mitliagkas, and C. Re. Parallel sgd: When does averaging help?
ArXiv, abs/1606.07365, 2016c.
Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradi-
ent descent algorithm for nonconvex optimization. arXiv preprint arXiv:1708.01012, 2017.
Martin Zinkevich, M. Weimer, Alex Smola, and L. Li. Parallelized stochastic gradient descent. In
NIPS, 2010.
11
Under review as a conference paper at ICLR 2021
A Appendix A: Convergence Theory
A. 1 Probabilistic Assumptions Governing the Asynchronous Computation
Now let us discuss the formalities of the asynchronous computation. We consider that the pres-
ence of local HogWild-like asynchronous computation introduces stochastic delays, i.e., at each
stochastic gradient computation, the set of parameters at which the stochastic gradient is evaluated
is random, it follows some distribution. Thus, considering that, in the general case,
vtq,,ij ∈ ∪k∈{0,...,j} {xqs,,ik}s∈{0,...,t}
we can now define a probability that this parameter view block is equal to each of these potential
historical parameter values. To this end we define Its,,ik,q,j as the event that block i in the view for
GPU q at major iteration j and minor iteration t is equal to the actual parameter xqs,,ik, and pts,,ik,q,j is
its probability. Now, vtq,,ij could be, e.g., xqs,,ij-1 for some s ∈ {0, ..., Kj-1}, i.e., it could have been
evaluated at a parameter before the last averaging took place.
In the nonconvex setting, it would be entirely hopeless to bound kxqs,,ij-1 - vtq,,ij k in general, in this
case we can simply hope that the objective function decrease achieved by iterations with a gradient
computed after an averaging step outweighs the worst-case increase that takes place before it. In
order to perform such an analysis, we will need bound the probability that this worst case occurs. In
order to facilitate the notation for this scenario, let us define xq-,1j,i = xqK,jj--11-1,i, xq-,j2,i = xqK,jj--11-2,i,
etc., and then pt-,i1,,qj,j correspondingly. We can extend this analogously to delays from before two
averaging steps, etc. Note that with this notation, plt,,ji,q,j is well defined for any l ≤ t, l ∈ Z, of
j-1
course as long as |l| ≤	Kk.
k=0
In order to derive an expected decrease in the objective, we need to bound the probability of an
increase, which means bounding the probability that the view is older than the previous averaging,
which can be bounded by a bound on the probability that a particular read is more than some number
τ iterations old. We thus make the following assumptions,
Assumption A.1. It holds that,
1.	plt,,ji,q,j = 0 for l ≤ t - D (Maximum Delay)
2.	There exists {pτ}τ ∈{1,...,D} such that for all (q, j, t), it holds that P ∪iIlt,,ji,q,j ≤ pt-τ for
l = t - τ (Uniform Bound for Components’ Delays)
With these, we can make a statement on the error in the view. In particular it holds that,
E ［卜q,j-xq,j∣∣ l∩i ∪1≥0lt,,j叫 ≤ αj,t,qB	(6)
for some B > 0. This bound comes from Section A.B.6 in Nadiradze et al. (2020). Thus, if the
view is taken such that all components were read after the last averaging step then we can bound the
error as given.
A.2 Proof of Main Convergence Theorem
Recall the major iterations Xj is defined as the value of the parameters after an averaging step has
taken place, which is of course well-defined as every GPU will have the same set of values for the
parameters. The update to Xj can be written as,
-	KjT	一
Xj-Eαj,t,q3(q,j,t)f 3门)	⑺
t=0
qj
Where We define, Pif (vq,j) as the vector of Size n whose i ith components are the calculated
stochastic gradient vector defined at vtq,j , with the rest of the components padded with zeros. We
indicate the component chosen i(q, j, t) depends on the GPU and minor and major iteration, allowing
for flexibility in the choice of block update (including the entire vector).
12
Under review as a conference paper at ICLR 2021
We are now ready to prove the convergence Theorem. The structure of the proof will follow Zhou &
Cong (2017), who derives the standard sublinear convergence rate for local SGD in a synchronous
environment for nonconvex objectives.
We begin with the standard application of the Descent Lemma,
f(Xj+ι) - f (Xj) ≤ Z(Xj), Q PQ=1 PK0-1 αj,t,q▽认 j,t,q)f (vt，)i
+H Q PQ=I PKo-1 αj V i(j,t,q)f (vq,j )∣2
Now, since EVi(j,t,q)f (嘴,)=Vi(j,t,q)f(熄',),
-EhVf(Xj ), V i(j,t,q)f (VF )i
=-2 (kvi(j-,t,q)f (Xj )k2 + EkVi(j,t,q)f (Vt,j )k2 - EkVi(j,t,q)f(vq,j ) - Vi(j,t,q)f (Xj )k2)
(9)
We now split the last term by the two cases and use equation 6,
EkVi(j,t,q)f (vq,j) - Vi(j,tq)f (Xj )k2
=E [kVi(j,t,q)f (vq,j ) - Vi(j,t,q)f (Xj )k2∣∩i ∪l≥0lt,j,叫 P [∩i ∪l≥0 Itj,q,j]
+E [kVi(j,t,q)f(vq,j) - Vi(j,t,q)f(Xj)k2| (∩i ∪1≥0It,j,q,j)ci P [(∩i ∪l≥0 It,产)ci
≤ αj2,t,qBP h∩i ∪l≥0 Ilt,,ji,q,ji
+2 (kVi(j,t,q)f (Xj)k2 + EkVi(j,t,q)f (vq,j)k2) P [(∩i ∪1≥0 It,j,q,j)ci
and thus combining with equation 22 we get the overall bound,
-EhVf(Xj), Vi(j,t,q)f (vq,j )i
≤ -2 (kVi(j,t,q)f(Xj)k2 + EkVi(j,t,q)f (vq,j)k2 - α2,t,qB)P [∩i ∪l≥0 It,j,q,j]
+ 2 (kVi(j,t,q)f(Xj )k2 + EkVi(j,t,q)f (vq,j )k2) P [(∩i ∪l≥0 It,j,q,j )c]	(10)
≤ - (kVi(j,t,q)f(Xj)k2 + EkVi(j,t,q)f(vq,j)k2) P h∩i ∪l≥0 It,j叫
+ 2 (kVi(j,t,q)f(Xj )k2 + EkVi(j,t,q)f (vk,j )k2) + α⅛B
It can be seen from this expression that we must have,
P h∩i ∪1≥0 It,j,q,j] ≥ 2+ δ	(11)
for some δ > 0 to achieve descent in a expectation for iteration j for a sufficient small stepsizes.
Since we are taking the sum of such iterations, we must have, ultimately,
1	PQ	PKj-1 ' l^JW l^c II、	πt,i,q,j^l 1	〜	(C"	I ɑj,t,q BM IEΓ∣∣V7	√Vc.q,j∖ l∣2
Q	Tq=I Tt=0 αj,t,q IP [∩i ∪l≥0 Il,j ] - 2 - αj,t,q	(QKj	+ 2E EkVi(j,t,q) f (Vt )k
+ ɪ PQ PKj-1 ar	∣P h∩∙∪r ∩ It,i,q,j ]	- 1 -	α2,t,qB i	EkVv- + f (X∙)k2 ≥ δ-
+ Q 2 q=1 2 t=0 αj,t,q	[P [∩i ∪l≥0 Il,j -	2	2 IEkVi(j,t,q)f (Xj )k ≥ δj
(12)
∞
with E δj ≥ f (X0) 一 fm, where recall that fm is a lower bound on f, in order to achieve asymp-
j=0
totic convergence. The standard sublinear SGD convergence rate is recovered with any choice with
aj,t,q = Θ (√j) and thus ultimately Sj = Ω (√j).
Let us now consider the quantity P ∩i ∪l≥0 Ilt,,ji,q,j in more detail and study how the nature of the
concurrency affects the possibility and rate of convergence.
In particular notice that,
P h∩iIlt,,ji,q,ji ≤ 1 - pt-τ
for l ≥ t - τ . In general of course we expect this quantity to increase as l is closer to t.
Consider two extreme cases: if there is always only one SG iteration for all processes for all major
iterations j, i.e., Kj ≡ 1, any delay means reading a vector in memory before the last major iteration,
13
Under review as a conference paper at ICLR 2021
and thus the probability of delay greater than zero must be very small in order to offset the worst
possible ascent.
On the other hand, if in general Kj >> τ, then while the first τ minor could be as problematic
at a level depending on the probability of the small delay times, for t > τ clearly the vector vtq,j
satisfies equation 6.
Thus we can sum up our conclusions in the following statements:
1.	Overall, the higher the mean, variance, and thickness of the tails of the delay, the more
problematic convergence would be,
2.	The larger the quantity of local iterations each GPU performs in between averaging, the
more likely a favorable convergence would occur.
The first is of course standard and obvious. The second presents the interesting finding that if you are
running HogWild-type SG iterations on local shared memory, performing local SGD with a larger
gap in time between averaging results in more robust performance for local SGD.
This suggests a certain fundamental harmony between asynchronous concurrency and local SGD,
more “aggressive” locality, in the sense of doing more local updates between averaging, coincides
with expected performance gains and robustness of more “aggressive” asynchrony and concurrency,
in the sense of delays in the computations associated with local processes.
In addition, to contrast the convergence in regards to the block size, clearly the larger the block the
faster the overall convergence, since the norms of the gradient vectors appear. An interesting possi-
bility to consider is if a process can roughly estimate or predict when averaging could be triggered,
robustness could be gained by attempting to do block updates right after an expected averaging step,
and full parameter vector updates later on in the major iteration.
A.3 Convergence - Simpler Case
In reference to the classic local SGD theory in particular Stich (2018) for the strongly convex
case and Zhou & Cong (2017) for the nonconvex case, we consider the simpler scenario wherein
i(q, j, t) = [n] and vtq,,ij = xsq,,ij with s ≥ 0 for all vtq,,ij , i.e., at no point are local updates computed
at gradients evaluated at model components existing in memory prior to the last averaging step. We
shall see the local asynchrony introduces a mild adjustment in the constants in the strongly convex
case, relative to the classic result, and results in no change whatsoever in the nonconvex case.
A.3.1 Strongly Convex Case
The proof of convergence will be based on Stich (2018), the synchronous case.
The formalism of the argument changes to Algorithm 4. Note that this is functionally the same, and
simply the concept of major iterations is dispensed with, except to define Kj .
1	Initialize xq0 for all q for t = 1, ..., T do
2	for all q do
3	Let Xt = X(LI- at,qVf (Vt)
4	if (t MOD PjJ=1 Kj = 0) for some J then
1Qq
5	Let xt+1 = Q ɪ2 xt
_	q=ι
The only change in the procedure is that the stochastic gradients are computed as evaluated at a
vector vtq , so we shall see how the convergence results contrasts with Stich (2018) for synchronous
averaging when computations are performed in this manner.
14
Under review as a conference paper at ICLR 2021
e
L
I-Ql-Ql-Ql-Ql-Q
=====
ttttt
g O〈9。g
qt
X
=
∑Q=1 v 〃婢),
P乙V 〃婢),
PAV 〃姆),
P 乙 Vf(W)
We have, as in the proof of Lemma 3.1 Stich (2018)
IE+1 - x*∣∣2 = Ilxt - atgt - x*∣∣2 = IE[ x* - αtgtk2 + α2kbt — gt∣∣2
+2叫〈珊-χ* - αtgt,gt - bti
(13)
Continuing,
∣∣Xt - x* - atgt∣∣2 = Ilxt - x*k2 + α2∣∣ gt∣∣2 - 2αt{xt 一 x*,gti
=IIXt-X*∣2 + % pq=ι IIVf(Xq )∣2 -蓍 P 和 Xt-Vq + 鸣-χ*, Vf 3))
=IIXt-X*i2 + Q PqL IVf (χq)- Vf (χ*)I2
-等 Pqyvq -x*, Vf(Vt))一等 PqyXt-Vq, Vf(Vt))
Using Young,s inequality and L-smoothness,
-2hXt - vq, Vf(Vt)) ≤ 2L∣∣χt - vqI2 + 芸IIVf(Vq)∣∣2
=2LkXt-Vq I2 + 笠 IIVf(Vq )-Vf(χ*)∣∣2
≤ 2L∣∣Xt-Vq I2 + (f (Vq) - f *)
Applying this to the above estimate of IXt - χ* - at gtI2, We get,
IXt-X* - atgtll2 ≤ ∣∣xt - X*I2 + ɪ Pq=i Ixt - VqIl2
+ 等 PqL ((atL - 1) (f (V?) - f *) - 2IVq - x*I2)
Let at ≤ 4⅛ so (atL - ɪ) ≤ -1. By the convexity of i(f (x) - f (x*)) + 2∣∣x - x*I2,
-蓍 PqL G (f(Vq) - f *) + 2 Iv? - x*I2)
≤-等 P乙(4(f(Xq) - f*) + 2Ixq - χ*I2)
+ 20t PA (Ivq -XqI +2μIvq -XqI2)
Putting this in equation 13 and taking expectations, we get,
EIXt+1 - x*I2 ≤ (1 - μαt)E∣∣Xt - x*I2 + α2EI 加-btI2
-号 E(f (Xt) - f *) + 好 P 乙 IXt-Vq i2
+患 P 乙(∣vq -Xq I + 2μ∣vq -Xq I2)
By Assumption 2.1, we have,
(14)
We have that,
and similarly,
1 Qf	、02
E∣ gt -bt∣2 = E∣-X (Vf(vq) - Vf(Vq)) ∣2 ≤ Q
W q=i
P 乙 IVq -Xq I≤ P 乙 |隧-Xq Ii
≤ PqL PS:Lax t-τ,to αsIV f (Vq )I1
≤ αtQτ√nG
P乙 IVq -XqI2 ≤ P乙Ps二aχ —,to α2IV/侬)i2
≤ Qα2τG2
(15)
(16)
(17)
15
Under review as a conference paper at ICLR 2021
Letting index to be such that t - to ≤ H := max{ Kj } when averaging takes place, i.e. Xt° = χ;°
for all q , we have,
Q PqLEkxt-Vq k2
=Q PQ=I Ekvq - xq + xq - xto- (Xt-Xt° )k2
≤ QQ PQ=I E [kvq -XqIl2 + Ilxq - xt0 - (Xt-Xt°)k2]
≤ Q PQ=IEkxq - Xt° k2 + 2α2τG2	(18)
≤ 看 PQ=1 Hαt0 PS=t0 E忖f (Xq)k2 + 2α2τG2
≤ ⅝ PQ=ι H2α2°G2 ≤ H2α20G2 + 2α2τG2
≤ 8H2αt2G2 + 2αt2τG2
where we use EkX - EXk2 = EkXk2 - kEXk2 and equation 17 to go from the third to the fourth
line.
Finally, putting equation 15, equation 18, equation 16 and equation 17 into equation 14 we get that,
Ekxt+ι - x* k2 ≤ (I - μα∕Ekxt- x* k2 + αtσQ
-等E(f (Xt) - f *) + 16α3LH2G2
+ α2τ√nG +2(μ + 2L)τ03G2
Finally, using Lemma 3.4 Stich (2018), we obtain, with a > maχ{16κ, H} for K = L∕μ, and
wt = (a + t)2 ,
Ef ( qSq PQ=I PTo1 wtxq)- f *
≤ 2s3 kxo-x*k2+4Tμsτ2a) 传+τ√nG)	(19)
+ 患ST (16LH2G2 + 2(μ + 2L)τG2)
which simplifies to, using Eμ∣∣xo - x*k ≤ 2G,
Ef (qSq PQ=I PTo1 wtxq)-f*
=O (μQT + μ+T2) σ2 + O (τ√n) G	(20)
+o (T √μT+H)) G + O (κH2+Tμ+2L) + ⅛⅛H3) G2
A.3.2 Nonconvex Case
This proof will again follow Zhou & Cong (2017). In this case the step-sizes {α(j, t, q)} are inde-
pendent of t and q, i.e., they are simple {αj }. Thus,
Kj-1
Xj- X αjVf(vq,j)
t=o
And thus,
f(xj+ι) - f(xj) ≤ -hVf(Xj),吉PQ=IPKj-IajVf(vq,j)i
+2∣∣ 吉 PQ=1 PKj-Iaj V f(vq,j)∣∣2	QI)
Now, since EVf (vq,α) = Vf(vq,α),
-EhVf(Xj), Vf(vq,j )i
=-2 (kVf(Xj)k2 + EkVf (vq,j)k2 - EkVf(vq,j) - Vf(Xj)k2)	(22)
≤ -2 (kVf(xj)k2 + EkVf(vq,j)k2 - L2Ekvq,j -xjk2)
We now continue the proof along the same lines as in Zhou & Cong (2017). In particular, we get
t-1
Ekvq,j - Xjk2 ≤ t2a2σ2 + ta2EX ∣∣Vf (vq,j)k2
s=o
Let us define K = maxj{Kj} and K = minj{Kj}.
16
Under review as a conference paper at ICLR 2021
We now have,
_α - PKj-I E/Vf (X -) ∖7 f(vq,j )i ≤ — (K+1)αj (1 _ L2ɑjK(K-I) A kʊf (χ.)∣∣2
αj 乙 t=0 EhVf(Xj), Vf(Vt )i ≤	2 I1	2(K+1) JkVf(Xj )k
-等(1 - L2α2(K+I)(K-2)) PK-IEkVf(Vqj)IF + LWσ2(2K-I)K(KT)
Similarly, it also holds that,
2
L	1	Kj-1	LK2α2σ2 LKjα2	Kj-1
■2	Q	X ajVf(Vqj)	≤	2QQ	+	X EkVf(Vqj)k2
2 Q t=0	2Q	2 t=0
And so, finally,
Ef(Xj+1) - f (xj) ≤ -(K+1αj (1 - L2α2K+KT) - Kα+K) kVf(Xj)k2
-α (1 - L2α2(K+I)(K-2) - LajK) PQ=1 PKj-I EkVf(Vqj)k2
L2α3 σ2(2K -I)K(K-1)	LK2α2σ2
+ j 12	~~- + -^Q-
Now, if/once αj is small enough such that,
1〉Laj (K + i)(K - 2)
≥	2
+ LajK
then the second term above disappears, and the result is exactly the same as in Zhou & Cong (2017).
Specifically, if 1 - δ ≥ L2aj2
E PJ	αj kVf (Xj )k2 ≤	2(f (XI)-F D
E 乙j=1	PJ=1 αj	— (K-1+δ) PJ=1 αj
l PJ	LK αjM	( K ι L(2K-1)(K-1)αj∙
+ 乙Q = 1 PL αι (K-1+δ) (Q +	6
B Appendix B: An Argument for Increased Generalization
Accuracy for Local SGD
B.1 Wide and Narrow Wells
In general it has been observed that whether a local minimizer is shallow or deep, or how “flat” it
is, seems to affect its generalization properties Keskar et al. (2019). Motivated by investigating the
impact of batch size on generalization Dai & Zhu (2018) analyzed the generalization properties of
SGD by considering the escape time from a “well”, i.e., a local minimizer in the objective landscape,
for a constant stepsize variant of SGD by modeling it as an overdamped Langevin-type diffusion
process,
dXt = -Vf(Xt)dt + √2^dWt
In general “flatter” minima have longer escape times than shallow ones, where the escape time is
the expectation in the number of iterations (defined as a continuous parameter in this sense) until the
iterates leave the well to explore the rest of the objective landscape. Any procedure that increases
the escape time for flatter minima as compared to shallower ones should, in theory, result in better
generalization properties, as it is more likely then that the procedure will return an iterate that is in
a shallow minimizer upon termination.
Denote with indexes w for a “wide” valley local minimizer and n for a “narrow” value, which also
corresponds to smaller and larger minimal Hessian eigenvalues, respectively.
The work Berglund (2011) discusses the ultimately classical result that as → 0, the escape time
from a local minimizer valley satisfies,
E[τe] = HeC/
and letting the constant H depend on the type of minimizer, it holds that that Hw > Hn, i.e., this
time is longer for a wider valley.
We also have from the same reference,
P[τe > sE[τe]] = e-s
17
Under review as a conference paper at ICLR 2021
B.2 Averaging
We now contrast two procedures and compare the difference in their escape times for shallow and
wider local minimizers. One is the standard SGD, and in one we perform averaging every τa time. In
each case there are Q processors, in the first case running independent instances of SGD, and in the
other averaging their iterates. We model averaging probabilistically as it resulting in a randomized
initialization within the well, and thus the escape time is a sequence of independent trials of length
Ta with an initial point in the well, i.e., escaping at time Te means that there are Q ∣"τe"∣ trials wherein
none of the Q sequences escaped within τa, and then one of them escaped in the next set of Q trials.
For ease of calculation, let Us assume that Ta = 1 E[τfWw] = 2E[τ}], where Tew and Tn are the
calculated single process escape time from a wide and shallow well, respectively.
If any one of the local runs escapes, then there is nothing that can be said about the averaged point,
so a lack of escape is indicated by the case for which all trajectories, while periodically averaged,
stay within the local minimizer value.
Now consider that if no averaging takes place, we sum up the probabilities for the wide valley that
they all escape after time (i - 1)T time and, given that they do so, not all of them escape after iTa.
E[Tew] ≤ Pi∞=1 P (Tew > (i - 1)Ta)Q(1 - P (Tew > Tai|Tew > (i - 1)Ta)Q)iTa
≤ P∞=1 (e-Qi-1)(1-e-ɪ) Tai)
For the narrow well this is,
E[Ten] ≥ Pi∞=1 P (Ten > (i - 1)Ta)Q(1 - P (Ten > Tai|Ten > (i - 1)Ta)Q)(i - 1)Ta
≥ P∞=1 (e-2Q(iT) (l-e-2Q)Ta(i- 1))
The difference in the expected escape times satisfies,
E[tW - Ten] ≤ P∞=ι hh(e-Qi-I)(1 - e-Q2)) - (e-2QCi-D(1 - e-2Q))i (i - 1)
+ (e-Qi-)(1 - e-Q))i Ta
Recall that in the case of averaging, if escaping takes place between (i - 1)Ta and iTa there were no
escapes with less that Ta for M processors multiplied by i - 1 times trials, and at least one escape
between (i - 1)Ta and iTa, i.e., not all did not escape between these two times.
The expected first escape time for any trajectory among Q from a wide valley, thus, is,
E[Tea,w] ≤ Pi∞=1 P[Tew > Ta](i-1)Q (1 - P[Tew > Ta]Mi)Tai
L∞	- (LI)Q	- iQ
≤ Ei=I e	2 (I - e 2 )Tai
And now with averaging, the escape time from a narrow valley satisfies,
E[T：，n] ≥ Pi=I (P[tη > Ta](iT)Q(1 - P[t} > Ta]Qi)Ta(i - 1))
≥ Pi∞=1 e-2(i-1)Q(1 - e-2iQ)Ta(i - 1)
With now the difference in the expected escape times satisfies,
E[t：，w - T：，n] ≤ P∞=J[e-(i-1)Q (1-e-iQ)
-e-2(iτ)Q(1 - e-2iQ)] (i - 1) + e-(i-1)Q (1 - e-iQ)] Ta
It is clear from the expressions then that the upper bound for the difference is larger in the case
of averaging. This implies that averaging results in a greater difference between the escape times
between wider and shallow local minimizers, suggesting that, on average if one were to stop a pro-
cess of training and use the resulting iterate as the estimate for the parameters, this iterate would
more likely come from a flatter local minimizer if it was generated with a periodic averaging pro-
cedure, relative to standard SGD. Thus it should be expected, at least by this argument that better
generalization is more likely with periodic averaging.
Note that since they are both upper bounds, this isn’t a formal proof that in all cases the escape times
are more favorable for generalization in the case of averaging, but a guide as to the mathematical
intuition as to how this could be the case.
18