Under review as a conference paper at ICLR 2021
Implicit Regularization Effects of Unbiased
Random Label Noises with SGD
Anonymous authors
Paper under double-blind review
Ab stract
Random label noises (or observational noises) widely exist in practical machine
learning settings. we analyze the learning dynamics of stochastic gradient descent
(SGD) over the quadratic loss with unbiased label noises, and investigate a new
noise term of dynamics, which is dynamized and influenced by mini-batch sam-
pling and random label noises, as an implicit regularizer. Our theoretical analysis
finds such implicit regularizer would favor some convergence points that could
stabilize model outputs against perturbation of parameters. To validate our anal-
ysis, we use our theorems to analyze the implicit regularizer of SGD with unbi-
ased random label noises for linear regression via Ordinary Least-Square (OLS),
where the numerical simulation backups our theoretical findings. We further ex-
tend our proposals to interpret the newly-fashioned noisy self-distillation tricks for
deep learning, where the implicit regularizer demonstrates a unique capacity of se-
lecting models with improved output stability through learning from well-trained
teachers with additive unbiased random label noises.
1	Introduction
Stochastic Gradient Descent (SGD) has been widely used as an effective way to train deep neural
networks with large datasets (Bottou, 1991). While the mini-batch sampling strategy was firstly pro-
posed to lower the cost of computation per iteration, it has been consider to incorporate an implicit
regularizer preventing the learning process from converging to the local minima with poor gener-
alization performance (Zhang et al., 2017; Zhu et al., 2019; Jastrzebski et al., 2017; Hoffer et al.,
2017; Keskar et al., 2017). To interpret such implicit regularization, one can model SGD as gradient
descent (GD) with gradient noises caused by mini-batch sampling (Bottou et al., 2018). Studies have
demonstrated the potentials of such implicit regularization or gradient noises to improve the gener-
alization performance of learning from both theoretical (Mandt et al., 2017; Chaudhari & Soatto,
2018; Hu et al., 2019b; Simsekli et al., 2019) and empirical aspects (Zhu et al., 2019; Hoffer et al.,
2017; Keskar et al., 2017). In summary, gradient noises keep SGD away from converging to the
sharp local minima that generalizes poorly (Zhu et al., 2019; Hu et al., 2019b; Simsekli et al., 2019)
and would select a flat minima (Hochreiter & Schmidhuber, 1997) as the outcome of learning.
In this work, we aim at investigating the influence of random label noises to the implicit regulariza-
tion under mini-batch sampling of SGD. To simplify our research, we assume the training dataset as
a set of vectors D = {χι, χ2,χ3, ...,xn }.The label yi for every vector Xi ∈ D is the noisy response
of the true neural network f * (x) such that
y = yi + εi, yi = f * (Xi), and E[εi] = 0, var[εi] = σ2 ,	(1)
where the label noise εi is assumed to be an independent zero-mean random variable. In our work,
the random label noises can be either (1) drawn from probability distributions before training steps
(but dynamized by mini-batch sampling of SGD) or (2) realized by the random variables per training
iteration (Han et al., 2018). Thus learning is to approximate f(X, θ) that beats f* (X), such that
1N	1N
argmdn INXX '⑻ := NXX(f(xi,θ)-yi) ∫∙
(2)
Inspired by (Hochreiter & Schmidhuber, 1997; Zhu et al., 2019), our work studies how unbiased
label noises εi (1 ≤ i ≤ N) would affect the “selection” of θ from possible solutions, in the
viewpoint of learning dynamics (Saxe et al., 2014) of SGD under mini-batch sampling (Li et al.,
2017; Wu et al., 2020; Hu et al., 2019b).
1
Under review as a conference paper at ICLR 2021
Contributions. Our analysis shows that under mild conditions, with gradients of label-noisy
losses, SGD might incorporate an additional data-dependent noise term, complementing with the
stochastic gradient noises (Li et al., 2017; Wu et al., 2020) of label-noiseless losses, through re-
sampling the samples with label noises (Li et al., 2018) or dynamically adding noises to labels over
iterations (Han et al., 2018). We consider such noises as an implicit regularization caused by unbi-
ased label noises, and interpret the effects of such noises as a solution selector of learning procedure.
More specifically, this work has made unique contributions as follow.
(1)	Implicit Regularizer. We reviewed the preliminaries (Li et al., 2017; Ali et al., 2019; Hu et al.,
2019b; Wu et al., 2020) and extent the analytical framework in (Li et al., 2017) to interpret the
effects of unbiased label noises as an additional implicit regularizer on top of the continuous-time
dynamics of SGD. Through discretizing the continuous-time dynamics of label-noisy SGD, we write
discrete-time approximation to the learning dynamics, denoted as θkULN for k = 1, 2, . . . , as
N
θULN 一 θULN - N X vl*(θUln) + ξk(θULN) + ξULN(θULN),	⑶
N i=1
where L*(θ) = (f (xi, θ) - f *(χi))2 refers to the label-noiseless loss function with sample Xi and
the true (noiseless) label yi, the noise term ξ%(θ) refers to the stochastic gradient noise (Li et al.,
2017) of label-noiseless loss function L* (θ), then We can obtain the new implicit regularizer caused
by the unbiased label noises (ULN) for ∀θ ∈ Rd , which can be approximated as follow
1
ξULN(θ) ≈ rη (% X Vθf (xi,θ)Vθf(xi,θ)>) Zk, and Zk 〜N(0d,Id),	(4)
where zk refers to a random noise vector drawn from the standard Gaussian distribution, θk refers
to the parameters of network in the kth iteration, (∙)1/2 refers to the Chelosky decomposition of the
matrix, V f (xi, θ) = ∂f (xi, θ)∕∂θ refers to the gradient of the neural network output for sample
xi over the parameter θk, and B and ηare defined as the learning rate and the batch size of SGD
respectively. Obviously, the strength of such implicit regularizer is controlled by σ2, B and η.
(2)	Effects to Linear Regression. To understand behaviors of the implicit regularizer ξtULN (θt)
to the learning dynamics, we studied SGD over Ordinary Least Square (OLS).With the proposed
model, we could easily obtain the implicit regularizer such that
ξULN (β) ≈ Pn/B (σ2 ΣN)1/2 Zk and Zk 〜N(0d,L)	(5)
where ΣN = NN PN=I XiX> referring to the sample covariance matrix of the training dataset. Our
theoretical elaboration suggests that SGD with unbiased random label noises would converge to a
distribution of Gaussian-alike centered at the optimal solution of OLS, where the span and shape
of the distribution would controlled by σ2 and ΣN when η and B are constant. We conducted the
extensive experiments using SGD with various σ2 and ΣN, and successfully obtain the results that
coincide our theories and directly visualize the effects of the implicit regularizer over the path of
learning and converging distribution of SGD for noisy linear regression.
(3)	Inference Stabilizer. The regularization effects of unbiased random label noises should be
2 N	2 N	2
.. .一--	..o	T)ZT” .—.	C T)ZT” .—.	rt
EzjξULNg)∣∣2 ≈ BNNxkVθ“Xi©)k2 = BBNNχ dθf3,θk)	,	(6)
BN i=1	BN i=1 ∂θ 2
where Vθf(X, θ) refers to the gradient of f over θ and the effects is controlled by the batch size
B and the variance of label noises σ2 . We extend above results to understand the newly-fashioned
noisy self-distillation (Zhang et al., 2019a; Kim et al., 2020) paradigms, where a well-trained model
is supposed to be further improved through learning from its noisy outputs. Our analysis showed
that, when the new convergence achieved, the noisy self-distillation strategy would prefer to re-select
a model with a lower neural network gradient norm NN PN=1 ∣∣Vθf (xi, θ)k2, where the gradient
norm characterizes the variation/instability of neural network inference results (over perturbations)
around parameters of the model. We carry out extensive experiments while results backup our
theories. Note that while the earlier work (Bishop, 1995) found training with input noises can also
bring regularization effects, our work focuses on the observational noises on labels.
2
Under review as a conference paper at ICLR 2021
2	Preliminaries and Related Work
SGD Dynamics and Implicit Regularization We follow settings in (Li et al., 2017) and consider
SGD as an algorithm that, in the kth iteration with the estimate θk , it randomly picks up a B-length
subset of samples from the training dataset i.e., Bk ⊂ D, and estimates the mini-batch stochastic
gradient b P∀χ,∈Bk VLi(θk), then updates the estimate for θk+ι based on θk, as follow
θk+1 - ( θk - B X vLi(θk) j = ( θk - N X vLi(θk) + √ηVk(θk) j ,	⑺
∀xi∈Bk	∀xi ∈D
where η refers to the step-size of SGD, and Vk(θk) refers to a stochastic gradient noise term caused
by mini-batch sampling. The noise would converge to zero with increasing batch size, as follow
Vk (θk) = √η ( N X vLi(θk) - B X vLi(θk) ) → 0d, as B → N ∙	⑻
∀xi∈D	∀xi∈Bk
Let define ΣSNGD(θk) as the sample covariance matrix of loss gradients vLi(θk) for 1 ≤ i ≤ N,
where we follow (Li et al., 2017) and do not make low-rank assumptions on ΣSNGD(θk). Under mild
conditions (Li et al., 2017; Chaudhari & Soatto, 2018), one can approximate SGD as Gk such that
Θk+1 一 θk - N X VLi(θk) + ξk(θk), ξk(θk) = jɪ (∑Ngd(θk))2 Zk, Zk 〜N(0,Id) . (9)
∀xi∈D
The implicit regularizer of SGD could be considered as ξk(Gk) = pn/B (∑1NGD(Gk))2 Zk which is
data-dependent and controlled by the learning rate η and batch size B (Smith et al., 2018). (Mandt
et al., 2017; Chaudhari & Soatto, 2018; Hu et al., 2019b) discussed SGD for varational inference
and enabled novel applications to samplers (Zhang et al., 2019b; Xiong et al., 2019). To understand
the effect to generalization performance, (Zhu et al., 2019; Smith et al., 2018) studied the escap-
ing behavior from the sharp local minima (Keskar et al., 2017) and convergence to the flat ones.
Finally, (Gidel et al., 2019) studied regularization effects to linear DNNs and (Wu et al., 2020)
proposed new multiplicative noises to interpret SGD and obtain stronger theoretical properties.
SGD Implicit Regularization for Ordinary Least Square (OLS) The most recent and relevant
work in this area is (Ali et al., 2019; 2020), where the same group of authors studied the implicit
regularization of gradient descent and stochastic gradient descent for OLS. They investigated an
implicit regularizer of '2-norm alike on the parameter, which regularizes OLS as a Ridge estimator
with decaying penalty. Prior to these efforts, F. Bach and his group have studied the convergence of
gradient-based solutions for linear regression with OLS and regularized estimators under both noisy
and noiseless settings in (Dieuleveut et al., 2017; Marteau-Ferey et al., 2019; Berthier et al., 2020).
Self-Distillation and Noisy Students Self-distillation (Zhang et al., 2019a; Xie et al., 2020; Xu
et al., 2020; Kim et al., 2020) has been examined as an effective way to further improve the general-
ization performance of well-trained models. Such strategies enable knowledge distillation using the
well-trained ones as teacher models and optionally adding noises (e.g., dropout, stochastic depth,
and label smoothing or potentially the label noises) onto training procedure of student models.
Discussion on the Relevant Work Though tremendous pioneering studies have been done in this
area, we still make contributions in above three categories. First of all, this work characterizes the
implicit regularization effects of label noises to SGD dynamics. Compared to (Ali et al., 2019;
2020) working on linear regression, our model interpreted general learning tasks. Even from lin-
ear regression perspectives (Ali et al., 2019; 2020; Berthier et al., 2020), we precisely measured
the gaps between SGD dynamics with and without label noises using the continuous-time diffu-
sion.Compared to (Lopez-Paz et al., 2016; Kim et al., 2020), our analysis emphasized role of the
implicit regularizer caused by label noises for model selection, where models with high inferential
stability would be selected. (Li et al., 2020) is the most relevant work to us, where authors studied the
early stopping of gradient descent under label noises via neural tangent kernel (NTK) (Jacot et al.,
2018) approximation. Our work made the analyze for SGD without assumptions for approximation
such as NTK. To best of our knowledge, this work is the first to understand the effects of unbiased la-
bel noises to SGD dynamics, by addressing technical issues including implicit regularization, OLS,
self-distillation, model selection, and the stability inference results.
3
Under review as a conference paper at ICLR 2021
3	Learning Dynamics and Implicit Regularization of SGD with
Unbiased Random Label Noises
From initialization θ0ULN, SGD with Unbiased Random Label Noises uses an iterative algorithm
that updates the estimate incrementally. Specifically, in the kth iteration, SGD randomly picks up a
batch of sample Bk ⊆ D to estimate the stochastic gradient, as follow
1	1N
gk(θULN) = TBn X vLi(θULN) = N XVL*(ΘULN) + SBUln) + gULN(0ULN), (10)
|Bk| xi∈Bk	N i=1
where VL* (θ) for ∀θ ∈ Rd refers to the loss gradient based on the label-noiseless sample (xi, yi)
and yi = f *(xi), ξ*(θ) refers to stochastic gradient noises (Li et al., 2017) through mini-batch
sampling over the gradients of label-noiseless samples, and ξkULN (θ) is an additional noise term
caused by the mini-batch sampling and the Unbiased Random Label Noises, such that
VL*(θ) = d-(f (Xi,θ) - f*(xi))2 = (f (Xi,θ) - f*(xi)) ∙ Vf (Xi,θ),
∂θ
ξk(θ) = TB^ X 卜L;(θ)- N X Li(θ) , and EB%[ξ*(θ)] = 0d,
(11)
ξkULN(θ)
1
TBkI
E εj∙Vf(xj,θ), and EBk,"ξULN(θ)] = 0d.
xj ∈Bk
Note that, for every iteration ∀θ ∈ Rd, the random vectors ξk*(θ) and ξkULN(θ) are with zero-mean
as E(εj ) = 0. To characterize the variances of the two random vectors, we define two matrix-value
functions ΣSNGD(θ) and ΣUNLN(θ) over θ ∈ Rd based on the label-noiseless losses, such that
1N	1N	1N
ςgd(θ) = N X (VL;(θ) — N X L*(θ)) (VL;(θ) — N X L*(θ)
2N
∑nln(θ) = N X Vθf(xj,θ)Vθ f(xj,θ)> as var[εj] = σ2 .
j=1
(12)
Under mild conditions, we have var[ξk(θ)] = 1/B ∙ ∑NGD(θ) and var[ξULN(θ)] = 1/B ∙ ∑Nln(θ).
SGD Learning Dynamics with Unbiased Random Label Noises We consider the SGD al-
gorithm with unbiased random label noises in the form of gradient descent with additive data-
dependent noise. When η → 0, we assume the noise terms ξk* (θk) and ξkULN(θk) are independent,
then we can follow the analysis in (Hu et al., 2019a) to derive the diffusion process of SGD with
unbiased random label noises, denoted as θULN (t) over continuous-time t ≥ 0, such that
1 ɪ	E	1
dθu6 = — N X VLi(θULN)dt +《B (∑N3D(θULN))2 dw1(t)
N i=1	B
+rB (∑nln(θULN)) 2 dw2(t),
(13)
where Wi (t) and W2 (t) refer to two independent Brownie motions over time and dt = √η. Again,
we can obtain the discrete-time approximation (Li et al., 2017; Chaudhari & Soatto, 2018) to the
SGD dynamics, denoted as HULN for k = 1, 2,..., which in the kth iteration behaves as
NL	K 1	1	1 、
嘿N 一 θUln-NXVl*(HULN)十卷((∑NGd(HUln))2 Zk + (∑Nln(HULN))2 Zk), (i4)
N i=i	B
where Zk and Zk0 are two independent d-dimensional random vectors drawn from a standard d-
dimensional Gaussian distributionN(0d, Id) per iteration independently, and θ0ULN = θULN(t = 0).
Note that the errors from the SGD algorithm to its continuous-time diffusion process and from the
continuous-time dynamics to its discretization are bounded under weak convergence (Hu et al.,
2019a). In this way, we can use the trajectory of the discrete-time dynamics θHkULN to analyze the
behaviors of SGD algorithm θkULN over iterations.
4
Under review as a conference paper at ICLR 2021
Implicit Regularizer Influenced by Unbiased Random Label Noises Compared the stochastic
gradient with unbiased random label noises gk (θ) and the stochastic gradient based on the label-
noiseless losses, we find an additional noise term ξkULN (θ) as the implicit regularizer.
To interpret ξULN(θ), We first define the diffusion process of SGD based on Label-NoiseLess losses
i.e., L*(θ) for 1 ≤ i ≤ N as dθLNL = -N P=1 VL*(θLNL)dt + PB (∑NGD(θLNL))I W(t).
Through comparing θULN (t) with θLNL(t), the effects of ξkULN(θ) over continuous-time form should
be，n/B3NLN(0))1/2dW(t). Then, in discrete-time, we could get results as follow.
Proposition 1 (The implicit regularizer ξkULN(θ)) The implicit regularizer of SGD with unbiased
random label noises could be approximated (with O(√η) approximation error due to discretiza-
tion (Li et al., 2017)) as follow,
1
ξULN(θ) ≈ rη (N X Vθf (Xi⑼Vθ f (xi,θ)>) Zk, and Zk 〜N(0d, Id) .	(15)
In this way, we can estimate the expected strength of the implicit regularizer ξkULN (θ) as follow,
2N
EzkkξULN(θ)k2 = BN X kVθf(xi,θ)k2 .	(16)
i=1
In this way, we can conclude that the effects of implicit regularization caused by unbiased random
label noises for SGD is proportional to N PN=1 ∣∣Vθ f (xi, θ) ∣∣2 一 the average gradient norm Ofthe
neural network f(x, θ) over samples. Please refer to appendix for the proof.
Inference Stabilizer Here we extend the existing results on SGD (Zhu et al., 2019; Wu et al.,
2018) to understand Proposition 1 as follows.
⑴ InferenCe Stability - The gradient norm N P=1 ∣∣Vθf(xi,θ)∣2 = N pN=1 |隅f (xi,θ)k2
characterizes the variation of neural network output f (x, θ) based on samples Xi (for 1 ≤ i ≤ N)
over the parameter interpolation around the point θ. Lower NN PN=I ∣∣Vθf (xi, θ)∣2 comes higher
stability of neural network f(X, θ) outputs against the (random) perturbations over parameters.
(2)	Escape and Converge - When the noise ξULN(θ) is θ-dependent (section 4 would present a spe-
cial case that ξkULN (θ) is θ-independent with OLS), we follow (Zhu et al., 2019) and suggest that
∙-v
the implicit regularizer help SGD escape from the point θ with high neural network gradient norm
N PN=I ∣∣Vθ f(xi, θ)∣2, as the scale of noise ξULN(石)is large. Reciprocally, we follow (WU et al.,
2018) and suggest that when the SGD with unbiased random label noises converges, the converging
point θ* should be with small N PN=I ∣∣Vθ f(χi,θ*)∣∣2.
(3)	Performance Tuning - Considering ησ2/B as the coefficient balancing the implicit regularizer
and vanilla SGD, one can regularize/penalize the SGD learning procedure with the fixed η and B
more fiercely using a larger σ2 . More specifically, we could expect to obtain solutions with lower
-N PN=ι ∣∣Vθf (xi, θ)∣2 or higher inference stability of neural networks, as regularization effects
become stronger when σ2 increases.
4	Implicit Regularization Effects to Linear Regression
Here, we consider a special example of SGD with unbiased random label noises using linear regres-
sion, where a simple quadratic loss function is considered for OLS, such that
1N	1N	2
bOLS — argmin < —X Li(Ie) =卡 X (Xirβ — yi『卜
β∈Rd	N i=1	N i=1
(17)
where samples are generated through y% = x>β* + εi, E[ε∕ = 0 and var[εi] = σ2. Note that in this
section, we replace the notation of θ with β to present the parameters of linear regression models.
5
Under review as a conference paper at ICLR 2021
Learning Dynamics and Implicit Regularization Effects The continuous-time diffusion pro-
cesses for SGD algorithms with and without unbiased label noises are as follow
N _N	/—
dβULN(t) = -N X Xi(x>eULN(t)-x>e*)dt + J∑L∑NGD(βULN⑴)1/2dWi(t)
N i=1	∑
+Bη∑∑nLN(βULN(t))1/2dW2(t)	(18)
____	ι NL	_ ____	_	K _ __ ____	_
dβLNL(t) = - N X Xi(χ> βLNL (t) -χ[β*)dt + Bl∑. ∑NGD(βLNL ⑴)1∕2dw (t)
i=1
where βULN (t) and βLNL (t) refer to the SGD dynamics for OLS under Unbiased Label Noises and
Label NoiseLess settings. We then denote the sample covariance matrix of N samples as ΣN =
N PN=I χ∙jχ>. Matrices ΣSGD(β(t)) and ΣULN(β(t)) in this case are defined as
1N
∑NGD(β) = N X (xix>β - ∑Nβ) (xix>β - ∑Nβ)> and ΣNLN(β) = σ2ΣN ,
N i=1
(19)
which are both time-homogeneous. Compared to βLNL(t), the dynamics βULN (t) incorporates an
additional noise term Pn∑NLN(βULN(t))1/2dW2(t) which affects the dynamics.
Proposition 2 (Implicit Regularization on OLS) In this way, we could approximate the implicit
regularizer of SGD with the random label noises for OLS through discretization such as,
∑∑NlnMULN(t))"dW2(t) ⇒ ξULN(β) ≈ Jη∑2 (∑N)1/2 zk, and Zk 〜N(0d,Id), (20)
which is independent with β and k (the time). According to (Berthier et al., 2020), SGD for noiseless
linear regression would asymptotically converge to the optimal solution β*. With an additional noise
term ξUULN (β) and the Single optima β* (for both noisy and noiseless losses), we could conclude that
when k → ∞, SGD with unbiased random label noises would converge to a distribution centered at
β*. The distribution would tend to be a Gaussian distribution when σ2 is significant (we could not
ignore the effects of stochastic gradient noise terms of noiseless loss to the overall distribution), as
the term ,σ2η∕∑ΣN/2 dW(t) corresponds to a Gaussian distribution. The span and shape of the
distribution are controlled by σ2 and ΣN when η and ∑ are COnStant.
Numerical Validation To validate Proposition 2, we carry out numerical evaluation using syn-
thesize data to simply visualize the dynamics over iteration of SGD algorithms with label-noisy
OLS and label-noiseless OLS. In our experiments, we use 100 random samples realized from a
2-dimension Gaussian distribution Xi 〜N(0, ∑1,2) for 1 ≤ i ≤ 100, where ∑1,2 is an symmet-
ric covariance matrix controlling the random sample generation. To add the noises to the labels,
we first drawn 100 copies of random noises from the normal distribution with the given variance
εi 〜N(0, σ2), then We setup the OLS problem with (Xi,匕)pairs using 匕=X> β* + εi and
β* = [1, 1]> and various settings of σ2 and ∑1,2. We setup the SGD algorithms with the fixed
learning rate η = 0.01, and bath size ∑ = 5, with the total number of iterations K = 1, 000, 000 to
visualize the complete paths.
Figure 1 presents the results of numerical validations. In Figure 1(a)-(d), We gradually increases the
variances of label noises σ2 from 0.25 to 2.0, where we can observe (1) SGD over label-noiseless
OLS converges to the optimal solution β* = [1.0, 1.0]> in a fast manner, (2) SGD over OLS with
unbiased random label noises would asymptotically converge to a distribution centered at the op-
timal point, and (3) when σ2 increases, the span of the converging distribution becomes larger. In
Figure 1(e)-(h), we use four settings of ∑1,2, where we can see (4) no matter how ∑1,2 is set for
OLS problems, the SGD with unbiased random label noises would asymptotically converge to a dis-
tribution centered at the optimal point. Compared the results in (e) with(f), we can find that, when
the trace of ∑1,2 increases, the span of converging distributions would increases. Furthermore, (5)
the shapes of converging distributions depend on ∑1,2. In Figure 1(g), when we place the principal
component of ∑1,2 onto the vertical axis (i.e., ∑Ver = [[10, 0]>, [0, 100]>]), the distribution lays on
6
Under review as a conference paper at ICLR 2021
----- ⅜Λ∕ith I nhɑl NCiUaU
(e) Σ1,2 = 10 Td	⑴ Σ1,2 = 100 ∙ Id	(g) Σ1,2 = ΣVer	(h) Σ1,2 = ∑Hor
Figure 1: Trajectories of SGD over OLS with and without Unbiased Random Label Noises using
various σ2 and Σ1,2 settings for (noisy) random data generation. For Figures (a)-(d), the experiments
are setup with a fixed Σ1,2 = [[20, 0]>, [0, 20]>] and varying σ2. For Figures (e)-(h), the experi-
ments are setup with a fixed σ2 = 0.5 and varying Σ1,2, where We set ΣVer = [[10, 0]>, [0,100]>]
and ΣHor = [[100, 0]>, [0, 10]>] to shape the converging distributions.
the vertical axis principally. Figure 1(h) demonstrates the opposite layout of the distribution, when
we set ΣHor = [[100, 0]> , [0, 10]>] as Σ1,2. The scale and shape of the converging distribution
backups our theoretical investigation in Eq 20.
Note that the unbiased random label noises are added to the labels prior to the learning procedure.
In this setting, it is the mini-batch sampler of SGD that “dynamizes” the noises and influences the
dynamics of SGD through forming the implicit regularizer.
5 Implicit Regularization Effects to Deep Neural Networks
500000
300000
200000
(e.g., porσ2)
Scale of label
400000
600000
(a) G.N. (SVHN)
(b) G.N. (CIFAR10) (c) G.N. (CIFAR100)
(d) Val. (SVHN) (e) Val. (CIFAR10) (f) Val. (CIFAR100)
(g) Test. (SVHN) (h) Test. (CIFAR10) (i) Test. (CIFAR100)
Figure 2: Gradient Norms, Validation Accuracy, and Testing
Accuracy in Noisy Self-Distillation using ResNet-56 with
varying scale of label noises (e.g., p and σ2).
Given a well-trained model, Noisy
Self-Distillation algorithms (Zhang
et al., 2019a; Xu et al., 2020; Kim
et al., 2020; Xie et al., 2020) intend
to further improve the performance
of a model through learning from the
“soft label” outputs (i.e., logits) of
the model (as the teacher). Further-
more, some practices found that the
self-distillation could be further im-
proved through incorporating certain
randomness and stochasticity in the
training procedure so as to obtain bet-
ter generalization performance (Xie
et al., 2020; Kim et al., 2020). In this
work, we study the way that directly
adds random label noises to the logit
outputs of the pre-trained model so as
to improve the self-distillation (Han
et al., 2018). More specifically, we
study two well-known strategies for
additive noises as follow.
(1) Gaussian Noises. Given a pre-
trained model with L-dimensional
logit output, for every iteration of self-distillation, this simple method that draw random vectors
7
Under review as a conference paper at ICLR 2021
from a L-dimensional Gaussian distribution N (0L, σ2IL), adds the vectors to the logit outputs of
the model, and makes the student model learn from the noisy outputs. Note that in our analysis,
we assume the output of the model is single dimension while, in self-distillation, the logit labels
are with multiple dimensions. Thus, the diagonal matrix σ2IL now refers to the complete form the
variances and σ2 controls the scale.
(2) Symmetric Noises.. Basically, this strategy is derived from (Han et al., 2018) that generates
noises through randomly swapping the values of logit output among the L dimensions. Specifically,
in every iteration of self-distillation, given a swap-probability p, every logit output (denoted as y
here) from the pre-trained model, and every dimension of logit output denoted as yl , the strategy in
probability p swaps the logit value in the dimension that corresponds to yl with any other dimension
ym6=l in equal prior (i.e., in (L - 1)-1 probability). In the rest 1 -p probability, the strategy remains
the original logit output there. In this way, the new noisy label y is with expectation E[y] as follow,
E[yι] = (1 - P) ∙ yι + P. p∀m=l ym	(21)
L-1
This strategy introduces explicit bias to the original logit outputs. However, when we consider the
expectation E[y] as the innovative soft label, then the random noise around the new soft label is still
unbiased as E[y - E[y]] = 0 for all dimensions. Note that this noise is not the symmetric noises
studied for robust learning (Wang et al., 2019).
Figure 2 presents the results of above two methods with increasing scales of noises, i.e., increasing
σ2 for Gaussian noises and increasing P for Symmetric noises. In Figure 2(a)-(c), we demonstrate
that the gradient norms of neural networks N∣∣Vθf (xi, θ)k2 decrease with growing σ2 and P for
two strategies. The results backup our theoretical investigation , which means the model would be
awarded high inferential stability, as the variation of neural network outputs against the potential ran-
dom perturbation in parameters has been reduced by the regularization. In Figure 2(d)-(f) and (g)-
(i), we plot the validation and testing accuracy of the models obtained under noisy self-distillation.
The results show that (1) some of models have been improved through noisy self-distillation com-
pared to the teacher model, (2) noisy self-distillation could obtain better performance than noiseless
self-distillation, and (3) it is possible to select noisily self-distilled models using validation accuracy
for better overall generalization performance. All results here are based on 200 epochs of noisy
self-distillation.
6	Discussion and Conclusion
While previous studies primarily focus on the performance degradation caused by label noises or
corrupted labels (Jiang et al., 2018; Li et al., 2020), we investigate the implicit regularization effects
of random label noises, under mini-batch sampling settings of stochastic gradient descent (SGD).
Specifically, we adopt the dynamical systems interpretation of SGD to analyze the learning proce-
dure based on the quadratic loss with unbiased random label noises. We decompose the mini-batch
stochastic gradient based on label-noisy losses into three parts in Eq. (11): (i) vL* (θ) - the true gra-
dient of label-noiseless losses, (ii) ξk(θ) - the stochastic gradient noise caused through mini-batch
sampling over the label-noiseless losses, and (iii) ξULN(θ) — the noise term influenced by the both
random label noises and mini-batch sampling. Our research considers ξkULN(θ) as an implicit regu-
larizer, and finds that effects of such implicit regularizer is to lower the gradient norm of the neural
networks NN PN=1 ∣Vθf (xi, θ)∣2 over the learning procedure, where the gradient norm of neural
networks here characterizes the variation/stability of the neural network outputs against the random
perturbation around the parameters. In summary, the new implicit regularizer ξkULN (θ) helps SGD
select a point with higher inference stability for convergence.
We carry out extensive experiments to validate our theoretical investigations. The numerical study
with linear regression clearly illustrates the trajectories of SGD with and without unbiased random
label noises, the observation coincides the SGD dynamics derived from our theories. Evaluation
based on deep neural network shows that, in self-distillation settings, one can lower the gradient
norm of neural networks, improve the inference stability of networks, and obtain better solutions,
through iteratively adding noises to the outputs of teacher models. Note that we do not want to
claim that the implicit regularization caused by the label noises would improve the generalization
performance in this work. The experiments results well backup our theories.
8
Under review as a conference paper at ICLR 2021
References
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. AISTATS, 2019.
Alnur Ali, Edgar Dobriban, and Ryan J Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, 2020.
Raphael Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for
stochastic gradient descent under the noiseless linear model. arXiv preprint arXiv:2006.08212,
2020.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation,
7(1):108-116,1995.
Leon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nimes, 91(8),
1991.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. In International Conference on Learning Represen-
tations, 2018.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger con-
vergence rates for least-squares regression. The Journal of Machine Learning Research, 18(1):
3520-3570, 2017.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. In Advances in Neural Information Processing Systems, pp.
3202-3211, 2019.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In
NeurIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.
org/abs/1503.02531.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 1731-1741. Curran Associates, Inc., 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of noncon-
vex stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4(1):3-32,
2019a.
Wenqing Hu, Zhanxing Zhu, Haoyi Xiong, and Jun Huan. Quasi-potential as an implicit regularizer
for the loss function in the stochastic gradient descent. arXiv preprint arXiv:1901.06054, 2019b.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
9
Under review as a conference paper at ICLR 2021
StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural netWorks on corrupted labels. In International Conference
on Machine Learning, pp. 2304-2313, 2018.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training
for deep learning: Generalization gap and sharp minima. In In International Conference on
Learning Representations (ICLR), 2017.
Kyungyul Kim, ByeongMoon Ji, Doyoung Yoon, and Sangheum HWang. Self-knoWledge distilla-
tion: A simple Way for better generalization. arXiv preprint arXiv:2006.12000, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, 2009.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent With early stopping is
provably robust to label noise for overparameterized neural netWorks. In International Conference
on Artificial Intelligence and Statistics, pp. 4313-4324. PMLR, 2020.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101-2110, 2017.
Tianyang Li, Liu Liu, Anastasios Kyrillidis, and Constantine Caramanis. Statistical inference using
sgd. In AAAI, 2018.
David LoPez-Paz, Leon Bottou, Bernhard Scholkopf, and Vladimir Vapnik. Unifying distillation
and privileged information. In In International Conference on Learning Representations (ICLR),
2016.
Stephan Mandt, MattheW D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, and Alessandro Rudi. Beyond least-
squares: Fast rates for regularized empirical risk minimization through self-concordance. In
Conference on Learning Theory, pp. 2294-2340, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and AndreW Y Ng. Read-
ing digits in natural images With unsupervised feature learning. In NeurIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.
AndreW M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural netWorks. In International Conference on Learning
Reporesentations (ICLR), 2014.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient
noise in deep neural netWorks. arXiv preprint arXiv:1901.06053, 2019.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. In International Conference on Learning Representations, 2018.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross en-
tropy for robust learning With noisy labels. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 322-330, 2019.
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the
noisy gradient descent that generalizes as sgd. In International Conference on Machine Learning
(ICML), 2020.
Lei Wu, Chao Ma, andE Weinan. HoW sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
10
Under review as a conference paper at ICLR 2021
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10687-10698, 2020.
Haoyi Xiong, Kafeng Wang, Jiang Bian, Zhanxing Zhu, Cheng-Zhong Xu, Zhishan Guo, and Jun
Huan. Sphmc: Spectral hamiltonian monte carlo. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pp. 5516-5524, 2019.
Guodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge distillation meets self-
supervision. In European Conference on Computer Vision, 2020.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. In International Conference on Learning Representations, 2017.
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 3713-3722, 2019a.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical
stochastic gradient mcmc for bayesian deep learning. In International Conference on Learning
Representations, 2019b.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. In ICML,
pp. 7654-7663, 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Sketched Proofs in Proposition 1
To obtain Eq 16, we can use the simple vector-matrix-vector products transform that, for the random
vector v and symmetric matrix A there has Ev [v>Av] = trace(AE[vv>]), such that
EzkkξkULN(θ)k22 =Ezk ξkULN(θ)>ξkULN(θ)
ησ2	1 N
≈ ^BEzk Zk (N X Nθf (Xi,θWθf (xi, θ) Z zk
=ηB"trace ((N X口f(xi,θRθf(xi,θ)>) EzJzkz>]
as Ezk [zkzk ] = Id f or zk 〜N(0d, Id)
σ 2	1 N
% trace N X Ne f(xi,θRe f(xi,θ)>
B	N i=1
2N
BN X IB f (Xi,θ)k2
i=1
2
BN ∂θf(xi,θ)
ησ
2
2
(22)
A.2 Training Details for Noisy Self-Distillation with Deep Neural Networks
We choose the ResNet-56 (He et al., 2016), one of the most practical deep models, for conducting the
experiments on three datasets: SVHN (Netzer et al., 2011), CIFAR10 and CIFAR100 (Krizhevsky
et al., 2009). We follow the standard training procedure (He et al., 2016) for training a teacher model
(original model). Specifically we train the model from scratch for 200 epochs and adopt the SGD
optimizer with batch size 64 and momentum 0.9. The learning rate is set to 0.1 at the beginning of
training and divided by 10 at 100th epoch and 150th epoch. A standard weight decay with a small
regularization parameter (10-4) is applied. As for noiseless self-distillation, we follow the standard
procedure (Hinton et al., 2015) for distilling knowledge from the teacher to a student of the same
network structure. The training setting is the same as training the teacher model.
For noisy self-distillation, we continue to use the training setting except the labels are noised by
the two types of noises as introduced in the main text. We choose the best scale of label noises
using a validation set, where we divide the original training set into a new training set (80%) and a
validation set (20%). A set of {0.1, 0.2, ..., 0.9, 1.0} is tried for the scale of symmetric noises. A
set of {0.1, 0.5, 1.0, 2.0, 3.0, ..., 9.0, 10.0} is tried for the scale of Gaussian noises. For clarity, we
also present the results using all the choices of scales of label noises on test set, where the original
training set is used for training.
A.3 Training Process of Deep Neural Networks
We show the evolution of training and test losses during the entire training procedure, and compare
the settings of adding no label noises, symmetric and Gaussian noises for self-distillations. Figure 3
presents the results on the three datasets, i.e., SVHN, CIFAR10 and CIFAR100.
12
Under review as a conference paper at ICLR 2021
SSOaM∙aUIeα
0	50	100	150	200
Training Epoch
(a)	Training Loss (SVHN)
SSOaM∙aUIe白
0	50	100	150	200
Training Epoch
(b)	Training Loss (CIFAR10)
4 3 2 1
SSoaMUIUItα
0
0	50	100	150	200
Training Epoch
(c)	Training Loss (CIFAR100)
8 6 4
Ooo
SSoa≡CL
----w/o noise
----Symmetric
----GaUssian
0.2
0	50	100	150	200
Training Epoch
(d)Test Loss (SVHN)
O
.
1
SSoas-φI
3 2
SSoas尚
1
0	50	100	150	200
Training Epoch
(f) Test Loss (CIFAR100)
0	50	100	150	200
Training Epoch
(e) Test Loss (CIFAR10)
Figure 3
13