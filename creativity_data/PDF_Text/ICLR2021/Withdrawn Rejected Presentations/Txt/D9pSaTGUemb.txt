Under review as a conference paper at ICLR 2021
Implicit Acceleration of Gradient Flow in
Overparameterized Linear Models
Anonymous authors
Paper under double-blind review
Abstract
We study the implicit acceleration of gradient flow in over-parameterized
two-layer linear models. We show that implicit acceleration emerges from a
conservation law that constrains the dynamics to follow certain tra jectories.
More precisely, gradient flow preserves the difference of the Gramian ma-
trices of the input and output weights and we show that the amount of
acceleration depends on both the magnitude of that difference (which is fixed
at initialization) and the spectrum of the data. In addition, and generalizing
prior work, we prove our results without assuming small, balanced or spectral
initialization for the weights, and establish interesting connections between
the matrix factorization problem and Riccati type differential equations.
1 Introduction
Understanding over-parameterization in deep learning is a puzzling question. Contrary to
the common belief that over-parameterization may hurt generalization and optimization,
recent work suggests that over-parameterization may actually bias the optimization algorithm
towards solutions that generalize well, a phenomenon known as implicit regularization or
implicit bias, and even accelerate convergence, a phenomenon known as implicit acceleration.
Recent work on the implicit bias in the over-parameterized regime (e.g. Gunasekar et al.
(2018a;b); Chizat & Bach (2020); Ji & Telgarsky (2019b)) shows that gradient descent on
unregularized problems finds minimum norm solutions. For instance, Soudry et al. (2018);
Ji & Telgarsky (2019a) analyze linear networks trained for binary classification on linearly
separable data and show that the predictor converges to a max-margin solution. Similar
ideas have been developed for matrix factorization, yielding solutions with minimum nuclear
norm (Gunasekar et al., 2017; Li et al., 2018) or low-rank (Arora et al., 2019a). It has also
been shown that optimization methods which introduce multiplicative stochastic noise, such
as dropout and dropblock, induce nuclear norm regularization (Cavazza et al., 2018) and
spectral k-support norm regularization (Pal et al., 2020), respectively.
Recent work on the implicit acceleration of gradient descent for matrix factorization and
deep linear networks (Arora et al., 2018) shows that when the initialization is sufficiently
small and balanced (see Definition 2), over-parameterization acts as a pre-conditioning of the
gradient that can be interpreted as a combination of momentum and an adaptive learning
rate. They claim that acceleration for `p-regression is possible only if p > 2, though there is
no theory supporting such claim. SaXe et al. (2014) focused on '2-regressi0n with balanced
spectral initializations (see Definition 3) and similarly concluded that depth may actually
slow down the convergence. For two-layer linear networks, SaXe et al. (2019); Gidel et al.
(2019) analyzed the dynamics of gradient flow and obtained eXplicit solutions under the
assumption of vanishing spectral initialization, highlighting the sequential learning of the
hierarchical components as a phenomenon that could improve generalization. Several recent
papers (e.g Arora et al. (2019b); Du & Hu (2019); Du et al. (2018b)) have also analyzed the
convergence behaviour of gradient descent in the over-parameterized setting, particularly
for very wide networks and have concluded linear convergence when the initialization is
gaussian or balanced. While a precise study of the connections between gradient descent and
gradient flow dynamics for non-conveX problems remains elusive, recent work (Franca et al.,
2020) shows that discrete-time convergence rates can be derived from their continuous-time
1
Under review as a conference paper at ICLR 2021
Table 1: Relationships between our work and the state of the art.
	Small and Balanced	Imbalanced	Spectral	Non spectral
Gradient flow	Our work Saxe et al. (2014) Saxe et al. (2019)	Our work	Our work Saxe et al. (2014) Saxe et al. (2019)	Our work
Gradient descent	Arora et al. (2018) Gidel et al. (2019)	None	Gidel et al. (2019)	Arora et al. (2018)
counterparts via symplectic integrators. Therefore, our work focuses on the analysis of
gradient flow as a stepping stone for future analysis of gradient descent.
In this paper, we present a new analysis of the implicit acceleration of gradient flow for over-
parametrized two-layer neural networks that applies not only in the case of small, balanced,
or spectral initialization but also extends to imbalanced and non-spectral initializations. We
show that the key reason for the implicit acceleration of gradient flow is the existence of a
conservation law that constrains the dynamics to follow a particular path.1 More precisely,
the quantity that is preserved by gradient flow is the difference of the Gramians of the input
and output weight matrices, which in turn implies that the difference of the square of the
norm of the weight matrices is preserved. The particular case where this difference is zero
corresponds to the case of balanced weights, but the more general case of imbalanced weights
also emerges as a conserved quantity and plays an important role. In particular, we show that
acceleration can occur even in the case of '2-regressi0n as a result of imbalanced initialization.
The reason this phenomenon was not previously observed in (Saxe et al., 2014; 2019; Gidel
et al., 2019) is precisely due to the assumption of balanced initialization, which follows as a
particular case of our analysis. Our work also establishes interesting connections with Riccati
type differential equations. Indeed, some of our results have a similar flavor to those in
(Fukumizu, 1998), while others are more general and provide an explicit characterization of
the continuous-time convergence rate. In short, our work makes the following contributions.
1.	In Section 2, we analyze the implicit acceleration properties of gradient flow for symmetric
matrix factorization, providing a closed form solution and a convergence rate that depends
on the eigenvalues of the data without the assumptions of spectral and small initialization.
2.	In Section 3, we analyze the implicit acceleration properties of gradient flow for asymmetric
matrix factorization with spectral initialization. We show that implicit acceleration
emerges as a consequence of conservation laws that only appear in over-parameterized
settings due to an underlying rotational symmetry.
3.	In Section 4, we analyze the implicit acceleration properties of gradient flow for asymmetric
matrix factorization with an arbitrary initialization. We make connections with Riccati
differential equations, obtaining a more general characterization of the convergence rate
and establish an interesting link with explicit regularization.
2 Gradient Flow Dynamics for Symmetric Matrix Factorization
In this section, we analyze and compare the dynamics of gradient flow,2
,,.
X(t) = -VX '(X (t)),	(1)
when applied to two problems. The first one is learning a symmetric one-layer linear model
min {'(X) ≡ 2||Y-χ||F},	⑵
X ∈Rm×m	2
where Y ∈ Rm×m is a given data matrix that one wishes to approximate by X ∈ Rm×m. The
second one is learning its over-parameterized symmetric matrix factorization counterpart
U mm×k {'(U) ≡1 ||Y - UU T ||F}.	⑶
1A quantity Q(x(t)) is said to be conserved by the flow X(t) = f (x(t)) if it remains constant
through dynamical evolution, i.e., £Q(x(t)) = 0. For example, in mechanics the sum of potential
and kinetic energies remains constant for a conservative system. A conservation law is usually a
consequence of an underlying symmetry (Noether’s theorem). In optimization, this can be seen as a
constraint Q(x) = Q0 that is automatically satisfied without having to explicitly enforce it.
2Gradient descent, Xn+ι = Xn — ηV'(xn), is simply an explicit Euler discretization of (1).
2
Under review as a conference paper at ICLR 2021
We show that the dynamics of the linear model converge at a rate O(e-t), while the over-
parameterized model has a rate of O(e-4tlσil), where σi is the ith eigenvalue of the data
matrix Y . Therefore, different spectral components are learned at different rates, which can
be faster or slower than the non-overparameterized model depending on the eigenvalues of Y .
Linear model. Let us start with the trivial problem of learning a non-overparameterized
linear model (2).3 Applying the gradient flow (1) to problem (2) yields X(t) + X(t) = Y
with X (0) = X0 . This is a linear differential equation whose unique solution is given by
X(t) =Y+(X0 - Y)e-t.	(4)
Thus, kX (t) - YkF = e-tkX0 - YkF and limt→∞ X(t) = Y at an exponential rate of O(e-t).
For completeness, it will be interesting to consider the particular case in which X is
constrained to be positive semidefinite (PSD), i.e. X 0. In this case, notice that if X0 0
and Y 0, then X(t)	0 for all t > 0, hence the same dynamics and convergence rate still
apply without having to enforce the PSD constraint. Otherwise, if Y is not PSD, gradient
flow is not directly applicable.
Symmetric matrix factorization model. Consider the more interesting case of learning
a two-layer linear model with tied weights U ∈ Rm×k formulated as the symmetric matrix
factorization problem in (3). In classical low-rank matrix factorization, one assumes k < m.
Here, we consider an over-parameterized formulation where k > m plays the role of the
number of hidden units (width). The gradient flow (1) on problem (3), for U, now yields
U7 = 2(Y — UUT)U with U(0) ≡ U0. Letting X(t) ≡ U(t)U(I)T 占 0 and X(0) = UoU0 占 0
one can easily verify that
X = U U T + UU T = 2(Y — X )X + 2X (Y — X) = 2YX + 2XY — 4X 2.	(5)
Equation (5) is known to be rank preserving, i.e. if rank(Xo) = r ≤ m and X* = limt→∞ X(t)
exists then rank(X*) ≤ r. It is thus impossible to recover a solution of rank higher than the
rank of the initialization.
We also note that (5) is a (matrix) differential equation of the Riccati type. Such equations
often characterize dynamical systems behind least squares problems and have been extensively
studied in the context of optimal control. Using results from this literature, we obtain (see
Appendix A for the proof ):
Proposition 1. For any X0 ∈ Rm×m, the solution to (5) exists and is given by
X(t) = e2tγXo (I + Y-1(e4tγ - I)Xo)T e2tY,	(6)
provided that Y and the matrix inside the parenthesis above are invertible.
This solution is derived for any X0 , while the over-parameterized model requires X0 =
U0 U0T 0. Thus in using (6) as an analysis tool, it is important to keep in mind the set of
allowable initializations. In what follows, we consider the spectral initialization (Saxe et al.,
2019; Gidel et al., 2019), and show that the eigenspace of the data is preserved throughout
the entire evolution of the learning dynamics.
Definition 1 (Symmetric Spectral initialization). Let Y = ΦΣΦT be the eigendecompo-
sition of the data. A spectral initialization is defined as Uo ≡ΦΣ1/2 and Xo ≡ UOUT =Φ∑oΦT
where Σ0 0 is a diagonal matrix.
From the explicit solution (6), we can readily obtain a convergence rate for a spectral
initialization (see Appendix B for a short proof).
Corollary 1. IfY = ΦΣΦT = Pim=1 σiφiφiT is invertible and Xo = ΦΣoΦT = Pim=1 σo,i φi φiT
is a spectral initialization, the solution to (5) is X(t) = ΦΣ(t)ΦT =	im=1 σi (t)φiφiT, where
(t) =	σo,iσie4tσi	=	4	σi(σ0,i — σ/
σi	σi + σo,i(e4tσi — 1)	°i	σi + σo,i(e4tσi — 1),
(7)
3 In a linear neural network, Y plays the role of the input-output data correlation matrix and X
plays the role of the model’s input-output map. In this trivial model, the input correlation matrix is
assumed to be the identity as is the case when the data is whitened.
3
Under review as a conference paper at ICLR 2021
m	TT
provided the denominator is nonzero. Moreover, if Y = i=1 max(σi, 0)φiφiT = ΦΣΦT is the
projection of Y onto the PSD cone, then for all initializations X0 0 such that rank(Σ0Σ) =
rank(Σ), X(t) converges to Y, at a rate O(e-4tσmin(γ)), where σmin(Y) = mini ∣σ∕ is the
smallest eigenvalue of Y in magnitude.
Note from (7) that the ith eigencomponent of X converges at a rate of O(e-4tlσil), so
that components with 4∣σ∕ > 1 are accelerated compared to the non-overparameterized
case, components with 4∣σ∕ < 1 are slowed down, and if 4σmin(Y) > 1 all components are
accelerated. This result about different components of the network being learned at different
rates by gradient flow is related in spirit to the result in (Saxe et al., 2019; Gidel et al., 2019)
about sequential learning in the asymmetric case with spectral balanced initialization. Here
the balancedness is enforced by construction.
Next, we derive the same convergence rate with a more general (non-spectral) initialization.
The proof is in Appendix C and makes use of several interesting relations for Riccati
differential equations.
Proposition 2 (Convergence rate). Consider the eigenvalue decomposition Y =
Em=I σiφiφT. Let Y = Em=I max(σi, 0)φiφT be the projection of Y onto the PSD cone
and Y = Pmm=ι ∣σi∣φiφT. For any initialization X。占 0, assume that I + Y-1(X0 — Y) and
Y are nonsingular. Then the solution X(t) of (5) converges exponentially to Y at a rate
∣∣X(t) — YIlF ≤ Ce-4tσmin(Y),	(8)
where σmin (Y) is the smallest eigenvalue of Y in absolute value and C > 0 is a constant.
It follows from Proposition 2 that the implicit acceleration for symmetric matrix factorization
with spectral initialization can be extended to any positive semidefinite initialization X0
provided I + YT(X。— Y) is invertible, which is an extension of the previous assumption
on Xo, namely that rank(∑o∑) = rank(Σ). The primary difference is that in the spectral
initialization case we can derive the convergence rate for each eigenvalue of X (t), while in
general we can only obtain a global convergence rate of the solution.
3 Gradient Flow Dynamics for Asymmetric Matrix
Factorization with Spectral Initialization
In this section, we analyze the dynamics of gradient flow for the more general asymmetric
matrix factorization problem. We show that the implicit acceleration phenomenon is still
present and provide an explanation for it based on a conservation law for the difference of
the Gramians of the factors. We transform the dynamics to a canonical form and show that
the solutions under the spectral initialization are diagonal and can be computed in closed
form. The closed form solution reveals a convergence rate of O(e-t 4σi2+λ20,i ), where σi is
the ith singular value of Y and λ0,i defines the level of imbalance in the initialization for the
ith component. As in the symmetric case, data matrices with large singular values induce
implicit acceleration. However, in the asymmetric formulation, additional acceleration can
be gained by choosing an imbalanced initialization.
Asymmetric matrix factorization model. Consider the asymmetric factorization prob-
lem:
min{'(u,v) ≡ 2IlY — uvTllF}
U,V
(9)
where U ∈ Rm×k, V ∈ Rn×k and k ≥ n ≥ m. The gradient flow for this problem takes the
form
U = —Vu ' = (Y — UV T )V,	V = —Vv ' = (Y — UV T )T U.
(10)
In what follows, We will make use of a conservation law for the difference of the Gramian
matrices UTU and VTV which has been previously identified in (Du et al., 2018a; Arora
et al., 2018). Previous works have used this conservation law to ensure balancedness under
vanishingly small initialization. In contrast, our analysis is the first to highlight the role of
imbalance and the resulting acceleration of gradient flow.
4
Under review as a conference paper at ICLR 2021
Conservation law. A straightforward calculation shows that (10) admits an invariant:
Q ≡ UTU	— VTV,	dQ	=	UTU +	UTU —	VTV —	VTV = 0	=⇒	Q(t) = Q(0). (11)
dt
The origin behind this conserved quantity Q is a global rotational symmetry of the system in
(10), i.e., the system is invariant under the orthogonal group O(k). To see this, consider the
singular value decomposition Y = ΦΣΨT and, following (Saxe et al., 2019), define matrices
U and V through
U = ΦU GT,	V = ΨV GT,	(12)
where G is an arbitrary element of O(k). These transformed variables obey
U = (Σ — U V T )V,	V = (Σ — U V T )T U.	(13)
Note that these equations have exactly the same form as (10), up to a gauge freedom on the
choice of G. Since Q is real and symmetric, it is diagonalizable by an orthogonal matrix.
Therefore, we can choose G to be the matrix that diagonalizes Q—this is a gauge choice.
Hence, from (11) we have
UTU - VTV = GTQ(t)G = Λq = Λqo = UTU - VTV0,	(14)
where ΛQ0 is the (constant) diagonal matrix containing the k eigenvalues of Q(0) ≡ Q0
(or Q) which is completely specified by the initial conditions U0 and V0 alone. Note that
the number of conserved quantities in ΛQ0 depends on k, which is equal to the degree of
over-parameterization. Though we do not assume balanced initialization in this paper, for
further reference and comparison with prior work (Arora et al., 2018; Saxe et al., 2014; 2019)
let us state its precise meaning since it relates to the conservation law.
Definition 2 (Balanced initialization). (U0, V0) is said to be balanced if kQkF =
kQ0kF ≤ for sufficiently small > 0, i.e. the conserved quantity in (11), or equiva-
lently (14), almost vanishes.
Under the above transformation, the matrix problem with spectral initialization can be
reduced to solving k one-dimensional systems (one for each component). Proposition 3
provides a closed form solution and explicitly characterizes the evolution of each component
along with its implicit acceleration (See Appendix D for the proof).
Definition 3 (Asymmetric Spectral initialization). Let Y = ΦΣΨT be the SVD of the
data. The spectral initialization is defined as Uo = ΦU0G, V0 = ΨV0G, and X0 = UoV0T,
where Uo and V0 are rectangular diagonal matrices and G is any orthogonal matrix.
Proposition 3 (Implicit acceleration of asymmetric factorization with spectral
initialization). Let Y = ΦΣΨT = Pim=1 σi φi ψiT be the SVD of the data. The solution to
(10) with spectral initialization X0 = ΦΣ0ΨT = Pim=1 σ0,iφiψiT yields X(t) = U (t)V (t)T =
ΦΣ(t)ΨT = Pim=1 σi(t)φiψiT, where
σi(t)
σie2t√4σ2+λ2,i - 2Ciλ2,iet√4σ2+λ2,i — 4σiλ2,iC
e2t√4σ2+λ0,i + 8σiGet√4σ2+λ2,i - 4λ2,iC2
(15)
λo = diag(UTUo - VTVo) and Ci = CgDo,i , σ0,i) is a constant. Moreover, the ith
eigencomponent of X(t) converges to the ith eigencomponent of Y at a rate O(e-tΛ∕4σ2+λ0,i).
Implicit acceleration. Recall from (4) that the
convergence rate for the non-overparameterized
problem in (2) is O(e-t), which does not depend on
the data or the initialization. It follows from (15)
that the asymptotic behavior of the singular values
of the over-parameterized solution is:
∣σi(t) - σi∣ ` 2Ci(4σ2 + λo,i)e-t√4σ2+λ0,i, (16)
which depends on both σi (singular values of the
Table 2: Regimes where implicit acceler-
ation is achieved (✓) or not achieved (X)
depending on the data singular values σ
and the level of imbalance λ0 of the initial
conditions (see (16)).
	Balanced λo ≈ 0	Imbalanced λo》0
small σ	X	✓
large σ	J	J
data) and λo,i (level of initialization imbalance). It is immediate to see that over-
parameterization leads to implicit acceleration when 4σi2 + λo2,i > 1. As illustrated in
5
Under review as a conference paper at ICLR 2021
Table 2 and previously observed in the literature (Saxe et al., 2014), acceleration is possible
under a balanced initialization i.e., λ0,i ≈ 0 when the singular values are sufficiently large.
This is similar to the symmetric case, which is not surprising since a symmetric factorization
is by construction balanced. A new phenomenon that emerges from our analysis is that
acceleration can also be achieved by increasing the level of imbalance, and that acceleration
can always be achieved regardless of the data when ∣λo,i∣ > 1. Moreover, a faster convergence
can be achieved by using a more imbalanced initialization.
4 Dynamics of Gradient Flow for Asymmetric Matrix
Factorization without Spectral Initialization
We now relax the assumption of spectral initialization (Definition 3). Defining the quantities
R(t) ≡
U(t)
V(t)
S≡
0Σ
ΣT 0
Im	0
0	-In
(17)
one can immediately obtain from (13) and (14) the Riccati-like differential equation
RR= SR - 1RRT R + SRΛq0
(18)
where from (14) we conclude that 2RTSRo = Λq° with R(0) ≡ R0. However, in general,
one cannot go back from (18) to (13) unless the conservation law (14) is explicitly imposed
for all times t. The natural question is then, when are they equivalent? Our next result
provides the answer and additionally reveals an interesting relation between (18) and a
matrix factorization problem with explicit regularization (proof in Appendix F).
Proposition 4 (Explicit regularization). The differential equation (18) is equivalent to
U = (Σ - UVT)V - 1 U(UtU - VTV - AQ0),
• 一 一 一 一
V = (Σ - UVT)tU + 1 V(UTU - VTV - AQ0).
(19)
This system corresponds to the dynamics of gradient flow applied to the regularized problem
min { 21∣∑ - U V TIIF +1 ||U t U - V t V - ΛQ. ∣∣F 卜
(20)
Moreover, if Q(t) ≡ Uτ(t)U(t) - Vτ(t)V(t) obeys Q(to) = AQ0 at some t = t0, then
Q(t) = ΛQ0 for all t ≥ to. In particular, if We initialize (19)—or equivalently (18)—such
that Q(0) = 2RτSRo = ΛQ0, then the conservation law (14) holds true for all t and both the
dynamics of (19) and (13) are the same.
Some remarks are appropriate:
•	A solution to (13) implies a solution to (19) because when (11) holds the 2nd terms on
the RHS of (19) vanish, while the 1st terms are exactly (13). However, the converse is
not necessarily true, unless (19) is initialized in the same way as (13). The above result
relates implicit acceleration (as will be shown in Proposition 5) to an explicit regularization;
namely one can either select a particular initialization and solve an unregularized problem,
or start at an arbirary initialization and explicitly regularize.
•	Note the specific weight of 1/8 in (20) is special: If one replaces 1/8 by some constant
α > 0, the gradient flow dynamics, i.e. the analog of (19), will not be equivalent to (18).
Note also problem (20) also appeared in (Du et al., 2018a), but without such connections.
Equation (18) is reminiscent of a Riccati differential equation due to the cubic term in R
(similar to the gradient flow in the symmetric case) but we believe that, in general, it cannot
be solved exactly due to the last term. However, it can be solved exactly in a particular case
(proof in Appendix E).
Proposition 5 (Exact solution and convergence rate). If ΛQ0 = λoIk for some constant
λo, then the differential equation (18) reduces to the following equation with a closed form
solution for RRT
R = SR - 2RRτR,	R(t)Rτ(t) = etSRoR(I + 1 S-1(e2tS - I)RoRT) 1 etS,	(21)
6
Under review as a conference paper at ICLR 2021
where S ≡ S +	λ°S=	ΦΣΦt,	R0	≡	R(0).	Moreover, if S and I +	2^^-l(R0RT	- R?RT)
are invertible then R(t)RT(t) converges exponentially to R?R?T, defined as the projection
of the matrix 2S on the PSD cone, S = Φ∣Σ∣Φτ. More precisely, if Y is a square matrix
the convergence rate is O(e-tV4σ2in+λ0), where σmin is the smallest eigenvalue of Y, and
otherwise the rate is O(e-∣λ0∣t).
Note that (21) is the gradient flow for the symmetric factorization problem minR {8||2S —
RRT ||2F}. The particular case ΛQ0 = λ0Ik is mathematically interesting because it is
amenable to an analytical treatment. However, it may not be realizable in practice because
the conserved quantity Q0 (or ΛQ0) must have low rank, i.e.,
rank(Q0) = rank(UTU - V0TV0) ≤
Since rank(λ0Ik) =_k,ʃhoosing U0 and V0
such that UTU0 — VTV0 = λ0Ik is not gen-
erally possible in an over-parameterized set-
ting with k > m+n. On the other hand, the
choice Λqo = λ0Ik does not present a prob-
lem if We consider the system (19) where We
have the freedom to choose any initialization.
The experiments in Section 5 illustrate that
(19), or equivalently (21), is actually enough
to capture the general behaviour of (13).
5 Numerical Experiments
Imbalanced initialization. We now pro-
vide numerical evidence supporting our the-
oretical results. First, we generate a ran-
dom matrix Y with Yij 〜N(0,1), and set
m = 5, n = 10 and k = 50. We approximate
the dynamics of gradient flow for one-layer
and two-layer linear models by using gra-
dient descent with a fixed small step size
η = 10-3 (smaller step sizes did not lead to
a discernible change). We evaluate the recon-
struction error ∣∣Y — X(t)∣∣F/∣∣Y∣∣f, where
X(t) = U(t)Vt(t), and compare the evolu-
tion of the singular values of X(t). We con-
sider Gaussian initializations, i.e., U0 and V0
have entries 〜N(0, σ2), where σ is varied to
obtain different degrees of imbalance. In or-
der for the two models to start from the same
state, we choose X(0) = U0 VT for the one-
layer case. The results are shown in Fig. 1.
From our theoretical analysis, we expect
a different behaviour for the convergence
rate depending on whether the initializa-
tion is balanced or imbalanced, i.e. whether
IIQIlF = kQ0kF ≡ IlUTU0-VTV0∣F is small
or large, respectively. When it is very small
(Fig. 1a), the strength of the singular value
dominates and we expect the components
to be learned sequentially from the largest
to the smallest, which agrees with the find-
ings in (Saxe et al., 2019; Gidel et al., 2019).
As we make the weights more imbalanced
(Fig. 1b) the singular values are learned
faster, even the smaller ones. Finally, as
rank(UTU0) + rank(V0TV0) ≤ m + n. (22)
Two layer model	One layer model
(a) ∣∣Q0∣∣ =0.02	(b) ∣∣Q0∣∣ = 1.83	(C) ∣∣Q0∣∣
1.0 r	1.0	8
2000	4000
Iterations
2000	4000
Iterations
Figure 1: Top row: reconstruction error for one- vs.
two-layer linear models. Bottom row: evolution
of singular values of the solution. From left to
right we use σ = 10-2, σ = 10-1 , and σ = 1
respectively.
」o=0 UO-sntsu80H
0
0
(a) k = 50,λo = 111.32
196.55
0
0	2000	4000
Iterations
(c) k = 200,λo = 460.99
-----λolk
Qo
100,λo = 203.66
0	------------------ 0	、------------------
2000	4000	0	2000	4000	0	2000	4000
Iterations	Iterations	Iterations
Figure 2: Top row: reconstruction error for the
asymmetric factorization dynamics without reg-
ularization in (13) and (14) and general Q0 (red
dashed line), versus the regularized dynamics in
(19) with a diagonal ΛQ0 = λ0Ik (black solid line).
Bottom row: evolution of the corresponding singu-
lar values of the solutions. From left to right we
set k = 50, k = 100, and k = 200, respectively.
7
Under review as a conference paper at ICLR 2021
kQkF becomes very large, the implicit acceleration becomes more prominent and the solution
of the factorized problem converges significantly faster (Fig. 1c). In other words, these
numerical results are consistent with Proposition 2 and Proposition 5.
ΛQ0 = λ0I is general enough. Since Proposition 5 contains the case where an exact
solution is available, we want to investigate whether this is general enough to capture the
qualitative behaviour of the general problem, i.e. the general dynamics of (13). To avoid
confusion, let Us refer to UjI and VI as the variables of (19), as well as ΛQo ≡ λ°Ik; here I
stands for “identity.” The variables U and V refer to the original dynamical system (13),
with its conserved quantity ΛQ completely fixed by the initial conditions; see (14). We want
to show that it is possible to find an “optimal” λ0 ∈ R such that both cases have very close
dynamics. Hence, we initialize Uo and Vo (and thus equivalently Uo and %) with entries
sampled from N(0, σ2), and we fix σ = 1. The same initial condition is also used for (19), i.e.
UI = Uo and VI = Vo. We set η = 10-5, Y ~ N(0,1), m = 5, n = 10 and vary k. Thus, we
look for a λo that minimizes the error kXI(t) - X (t)kF . In Fig. 2 we illustrate that, indeed,
this can be done. Note that in these three cases the evolution of both dynamical systems are
nearly indistinguishable.
Extension to nonlinear networks. Our analysis so far has
shown that the acceleration phenomenon is a result of imbal-
ance and is induced through a conservation law for which the
definition is expected to change when introducing nonlinearities.
In fact, both the architecture of the network and the objective
function should affect the invariances. As such, conducting a
full analysis of the symmetries and conservation laws for more
complex nonlinear networks is necessary to characterize the
implicit bias in such cases, which we leave for future work.
Nonetheless, we provide numerical evidence in a more realistic
setting than the one of linear networks, where we only add a
nonlinearity (sigmoid) to the final layer in order to preserve
some of the structure of the linear model. We train the two
networks (one layer vs two layers) on synthetic data, i.e. we
compare the dynamics of gradient descent for the ob jectives:
'ι(W) = 1 ||Y-φ(XW)||F and'2(U, V) = ɪ||Y-φ(XUVT)∣∣F
where φ is the sigmoid function, X ∈ Rn×d and Y ∈ Rn×d
represent the training samples and labels respectively (n =
103 , d = 10), W ∈ Rd×d , U, V ∈ Rn×k are the weight matrices
and the width is k = 100. We generated the matrices W * and
X with entries drawn from N(0,1) and Y = φ(XW*) + e where
~ 10-3N(0, I). Our results shown in figure 3 interestingly
0	100	200	300	400	500
Figure 3: Evolution of the
training loss for nonlinear one-
layer and two-layer models.
Top row: ||Q0 ||2 = 0. Bot-
tom row: ||Q0 ||2 = 4.6. Initial
weights are drawn from a nor-
mal distribution N(0, 10-1).
suggest that our conclusions about the role of imbalance hold in this case as well.
6 Conclusion
We considered the gradient flow dynamics for two-layer linear neural networks, providing an
analytical treatment to a great level of detail. Our results establish a detailed characterization
of the “implicit acceleration” phenomenon in this case, without assuming balanced or
vanishingly small initializations, which so far have been present in all prior work in this
vein. More specifically, our analysis shows that the implicit acceleration of gradient flow
is a consequence of an emerging rotational symmetry induced by over-parameterization
and giving rise to several conservation laws that constrain the dynamics to follow specific
tra jectories. Moreover, conserved quantities are completely fixed by the initialization, which
has profound effects on the convergence rate. Although our analysis focuses on the simple
case of linear networks, it reveals a potential key to understanding implicit bias which lies
in the conservation laws that arise from the symmetries of the problem. These symmetries
depend on the network architecture, objective, optimization algorithm, and they constrain
the dynamics to an invariant manifold that encapsulates the implicit regularization and
acceleration effects. Understanding this in more complex models may thus be reduced to
finding dynamical invariants, for which our results provide a foundational starting point.
8
Under review as a conference paper at ICLR 2021
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks:
Implicit acceleration by overparameterization. In International Conference on Machine
Learning (ICML), 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep
matrix factorization. In Advances in Neural Information Processing Systems (NeurIPS).
2019a.
Sanjeev Arora, Noah Golowich, Nadav Cohen, and Wei Hu. A convergence analysis of
gradient descent for deep linear neural networks. In 7th International Conference on
Learning Representations, ICLR 2019, 2019b.
F. M. Callier, J. Winkin, and J. L. Willems. On the exponential convergence of the time-
invariant matrix Riccati differential equation. In Proceedings of the 31st IEEE Conference
on Decision and Control, pp. 1536-1537 vol.2, 1992.
F.	M. Callier, J. Winkin, and J. L. Willems. Convergence of the time-invariant Riccati
differential equation and lq-problem: mechanisms of attraction. International Journal of
Control, 59(4):983-1000, 1994.
Jacopo Cavazza, Benjamin D Haeffele, Connor Lane, Pietro Morerio, Vittorio Murino, and
Rene Vidal. Dropout as a low-rank regularizer for matrix factorization. In Artificial
Intelligence and Statistics (AISTATS), volume 84, pp. 435-444, 2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural
networks trained with the logistic loss. In arXiv preprint arXiv:2002.04486, 2020.
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural
networks. In International Conference on Machine Learning, pp. 1655-1664, 2019.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homo-
geneous models: Layers are automatically balanced. In Advances in Neural Information
Processing Systems (NeurIPS), 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. In International Conference on Learning
Representations, 2018b.
G.	Franca, M. Jordan, and R. Vidal. On dissipative symplectic integration with applications
to gradient-based optimization. arXiv:2004.06840 [math.OC], 2020.
Kenji Fukumizu. Effect of batch learning in multilayer neural networks, 1998.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete
gradient dynamics in linear neural networks. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan
Srebro. Implicit regularization in matrix factorization. In Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems (NeurIPS), 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit
bias in terms of optimization geometry. In Proceedings of the 35th International Conference
on Machine Learning (ICML), pp. 1832-1841, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient
descent on linear convolutional networks. In Advances in Neural Information Processing
Systems 31 (NeurIPS), pp. 9461-9471, 2018b.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations (ICLR), 2019a.
9
Under review as a conference paper at ICLR 2021
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Proceedings of the Thirty-Second Conference on Learning Theory (COLT), pp. 1772-1798,
2019b.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-
parameterized matrix sensing and neural networks with quadratic activations. In Proceed-
ings of the 31st Conference On Learning Theory (COLT), 2018.
B.	P. Molinari. The time-invariant linear-quadratic optimal control problem. Automatica, 13
(4):347-357, July 1977.
Ambar Pal, Connor Lane, Rene Vidal, and Benjamin D Haeffele. On the regularization
properties of structured dropout. In IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7671-7679, 2020.
Takao Sasagawa. On the finite escape phenomena for matrix Riccati equations. IEEE
Transactions on Automatic Control, 1982.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. In International Conference on
Learning Representations (ICLR), 2014.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. A mathematical theory of
semantic development in deep neural networks. Proceedings of the National Academy of
Sciences, 116(23):11537-11546, 2019. ISSN 0027-8424. doi: 10.1073/pnas.1820226116.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on
separable data. In International Conference on Learning Representations (ICLR), 2018.
10
Under review as a conference paper at ICLR 2021
A Solution to the Matrix Riccati Differential Equation
Here we prove Proposition 1.
First, we note that a general matrix Riccati differential equation takes the form
P(t)= AP(t) + P(t)AT - P(t)RP(t) + Q, P(0) = Po,	(23)
where P (t) ∈ Rn×n, and A, R, Q, P0 ∈ Rn×n are constant matrices. Associated to (23) one
has the linear system
Xι(t)] = Γ-AT R∖ ∖Xι(t)'
・忘⑴]=[Q a] [X2(t)
X1(0)
X2(0)
In
P0
(24)
The closed form solution of (23) follows as a consequence of lemma 6 below.
Lemma 6 ((Sasagawa, 1982)). Consider the two initial value problems (23) and (24). We
have:
•	The initial value problem (23) has a solution in the interval [0, t1] if and only if the
matrix X1 (t) in the solution of the linear differential equation (24) is invertible for
all t ∈ [0, t1). Moreover, the solution to (23) is unique and given by
P(t) = X2(t)X1(t)-1.
(25)
•	Let P be a solution to the algebraic Riccati equation (ARE)
AP + PAT - PRP + Q = 0.	(26)
Then the solution of (24) is given by (27) below, where A = A — PR and A =
AT — RP:
X1(t)
X2(t)
e-tA + (/； ds e-(t-S)AReSA) (P0 — P)
PeTA + P (Rt dse-J)AReSA)(PO — P)+ etA(P0 — P)
(27)
Now, we apply Lemma 6 to the Riccati equation induced by a symmetric factorization,
namely,
X(t) = 2X (t)Y + 2YX (t) — 4X (t)2.
(28)
The associated linear system is
-	∙	,	, -I	I-
X1(t) _ —2Y
X2(t)] = [ 0
4I	X1(t)
2Y	X2 (t)
X1(0)
X2(0)
In
X0
(29)
and the algebraic Riccati equation is given by
XY + YX — 2X2 = 0.
(30)
This equation admits the trivial solution X = 0. Thus, one can verify that as long as the
given matrix Y is invertible we have
X1(t)	e-2tY +e-2tYY-1(e4Yt — I)X0
X2(t) =	e2tY X0
(31)
Therefore, the unique solution to (5) is explicitly given by
X(t) = X2(t)X1(t)-1 = e2tYXo (I + YT(e4tY — I)X0)T e2tY,	(32)
as long as the matrix between parenthesis is invertible. Note that Y-1 (e4tY — I) is always
positive semidefinite for t ≥ 0. Thus, for all Xo 0, the matrix I + Y-1 (e4tY — I)Xo is
invertible and the solution of the Riccati equation is well defined for all t ≥ 0.
11
Under review as a conference paper at ICLR 2021
B Proof of Corollary 1
The first part follows trivially by substituting in (6) and verifying the invertibility of the
matrix in parenthesis. For the second part, note from (7) that if σi > 0, then σi(t) → σi
as t → ∞ at a rate O (e-4tσi), and if σi < 0, then σi(t) → 0 at a rate O(e4tσi ). Therefore,
Σ(t) → max(Σ, 0) and X(t) → Φ max(Σ, 0)ΦT at a rate O(e-4tσmin(Y)), as claimed.
C Rate of Convergence in the Symmetric Case
In this section, we show that the solution of the Riccati equation X(t) = 2Y X(t) + 2X (t)Y -
4X(t)2 converges exponentially to Y at a rate O(e-4tσmin(γ)), where σmin(Y) = mini ∣σ∕ is
the smallest eigenvalue of Y in magnitude. We already know that if X(t) converges to X*,
then
•	X* is positive semidefinite if X。占 0 and rank(X*) ≤ rank(Xo),
•	X* is a solution to the algebraic Riccati equation 2YX + 2XY 一 4X2 = 0.
Our proof of proposition 2 is inspired by the proofs in (Molinari, 1977; Callier et al., 1992;
1994) which studied the solution of the Riccati equation under a more general setting. The
strategy will be to show that the algebraic Riccati equation has a unique PSD solution X+
such that the eigenvalues of A = A = 2Y 一 4X+ have negative real parts. Such solution is
usually referred to as the strong solution or stabilizing solution in optimal control literature
because it is the only solution of the algebraic Riccati equation such that the matrix A is
exponentially stable, i.e. exp(tA) converges to 0. The stability of the matrix A is important
because it appears in the solution of the Riccati equation as can be observed in (27).
We start by proving that Y is a solution to the algebraic Riccati equation, i.e. it is a critical
point of the problem.
Due to the symmetric and positive semidefinite nature of the matrices X and Y, in our case
the algebraic equation (30) can be reduced to
X(X 一 Y) = 0.	(33)
For X = Y, we thus have
~ . ~ .
Y(Y 一 Y)
max{σi, 0
(max{σi , 0} 一
max{σi, 0
min{σi , 0
(34)
The first sum contains only the positive eigenvalues while the second sum contains only the
negative ones. Therefore, no eigenvalue will appear in both sums and using the orthogonality
of the vectors φi we conclude that Y(Y 一 Y) = 0.
Next, We prove that P = Y is the unique symmetric positive semidefinite solution of the
algebraic Riccati equation such that the eigenvalues of A = A - RP have negative real parts.
Note that A = 2Y - 4Y = -2Y where Y = Pm=1 ∣σ∕φiφT > 0.
We proceed by contradiction. Let X2 be a PSD solution of the (ARE) such that the
eigenvalues of A2 = 2Y - 4X2 have negative real parts and X2 6= Y .
Now consider ∆ = Y - X2 . By a straightforward calculation, we can show that ∆ is a
solution of
A∆ + ∆A + 4∆2 = 0.
(35)
Since ∆ is not necessarily invertible, we consider a basis such that
∆ = z 0 D z T = z ∆ z T,
(36)
12
Under review as a conference paper at ICLR 2021
where D is invertible. Note that our proof holds and is more trivial if ∆ is invertible. We
write
A — Z	I-W1	W2〕ZT — ZAZT
A = Z	W3	W4	Z = ZAZ
(37)
After the change of basis, equation (35) becomes
A∆ + ∆ A + 4∆2 = 0.	(38)
From (35) , we can deduce the following for the block matrices:
W2 = 0,
W3 = 0,
DW4+W4D + 4D2 = 0.
Note that the last equation is similar to equation (35) for the invertible block D. Moreover,
in the new basis, A is a block diagonal matrix. Using the change of variable T = D-1, We
obtain the Lyapunov equation
TW4 + W4T + 4I = 0.	(39)
Since A = -2Y Y 0 is invertible, the block W4 is also invertible and its eigenvalues are a
subset of the eigenvalues of _2Y. As a result, the Lyapunov equation has the unique trivial
solution T = -2W-1 * 0, therefore D = -ɪW4 * 0.
Using the solution of (35), We can obtain a new derivation for A2 as follows
~ ~
A2 = 2Y - 4X2 = 2Y - 4Y + 4∆
A + 4∆ = Z W1
0
ZT
with W1 Y 0 and -W4 * 0. This contradicts with the initial assumption on the eigenvalues
of A2 having negative real parts.
e-2tY [I + YT(I - e-4Yt)(X0 - Y)]
YLIttY [I + YT(I - e-4Yt)(X0 - Y)] + e2tY(X0 - Y).
(40)
Now we can use Lemma 6 with JP = Y to obtain a new expression of the solution. We have
A = A= -2Y and
X1 (t)
X2(t)
Therefore, the solution of the Riccati equation is also given by
X(t) = X2(t)X1(t)-1 = Y + e-2tY(Xo - Y) [I + YT(I - e-4Yt)(Xo - Y)]-1e-2tY (41)
Note that the inverse exists for t ≥ 0 when X0 0 because we have proven the existence
of the solution using the previous expression (32) (see lemma 2 in (Sasagawa, 1982) for a
detailed proof).
We introduce the function H(t) = (Xo - Y) [I + YT(I - e-4Yt)(Xo - Y)] -1. Thus,
X(t) - Y = e-2tYH(t)e-2tY	(42)
The function H has the following properties for t ≥ 0:
•	It is decreasing:
dH^) = -4H(t)e-4YtH(t) ≤ 0	(43)
•	If I + Y (Xo - Y) is invertible then
lim H(t) = (Xo - Y)[I + Y(Xo - Y)]-1 = H.	(44)
t→∞
• H is bounded on R+;
~ , , , . ~ ,
H ≤ H(t) ≤ H(0) = X0 - Y.	(45)
Therefore, we can conclude that there exists a constant C > 0 such that
∣∣X(t) - YkF ≤ Ce-4σmint,	(46)
where σmin = min{∣σ∕} is the smallest eigenvalue of Y in absolute value.
13
Under review as a conference paper at ICLR 2021
D Closed form solution for the asymmetric case under
spectral initialization
Under the spectral initialization, Uo and V⅞ are diagonal, hence so are U(0) and V(0). As a
result, U(t) and V(t) remain diagonal for all t ≥ 0, because the components of (13) can be
decoupled and the evolution of (13) will induce no change in the off-diagonal elements. To
see this, observe that
Uii =	(σi -	UiiVii)Vii,	Vii	=	(σi	-	UiiVii)Uii	(1 ≤ i ≤ m),	(47)
whereas the off-diagonal terms obey Uij = 0 and Vij = 0 (i = j). Thus (47) describes the
evolution of the singular values of the solution. This decouples the problem into a set of
independent one-dimensional equations, therefore it suffices to consider the scalar system
U = (σ — UvlV,	v = (σ — UV)U,	(48)
where we drop the index i = 1, . . . , m for simplicity. In (Saxe et al., 2019) there is a strong
assumption, i.e. Uii(0) = vii(0) for all i which is a balanced initialization (Definition 2). Here
we solve (47) without such an assumption. From (48), it is immediate that the conservation
law (11) becomes £ (u2 — v2) = 0. Trajectories (u(t),v(t)) are thus constrained to lie on
hyperbolas defined by
U2(t) — v2(t) = U2 — V0 = λo = const.	(49)
Since we are mostly interested in the behavior of the product x(t) = U(t)v(t) by making
explicit use of the conservation law (49), specifically λ0 = U4 — 2U2V2 + V4 = U4 + V4 — 2x2, and
(U2 + V2)2 = λ2 +4x2, we obtain the following first-order differential equation for x ≡ x(t):
x
(σ - Uv)v2 + (σ — Uv)u2
(σ — Uv)P (V2 + U2)2 = 2(σ — x) Jx2 + λ0/4.
(50)
Even though this is a nonlinear differential equation, it is separable, thus integrating both
sides yields precisely (15) (we restore i and x → σi represents the corresponding component
associated with singular value σi and conserved quantity λ0,i), where C > 0 is a constant:
C = VZ 4σ2λ2 + 16σ2x0 + λ0 + 4x2λ2 — 4σx0 — λ2
4λ0(σ — xo)
(51)
Above, only m out of k ≥ m conserved quantities are used. Hence, there is degeneracy in
the solution and only m effective degrees of freedom regardless how large k is. Note that if
k < m (under-parameterized case) then (47) becomes under-determined.
E Rate of Convergence in the Asymmetric Case
Here we prove Proposition 5. First, note that equation RR = SR — 1RRR follows directly
from R = SR — 22RRTR + SRΛq° when Λq0 = λ0I. This leads to a Riccati differential
equation for P(t) = R(t)RT(t):
dP(t) = RRT + RRτ = (SR — 1RRTR) RT + R (ɪSR — 1RRTR)T
dt	2	2	(52)
=SP (t)+ PsS — (P (t))2
with initial conditions P (0) = RoRoT. By the arguments used in Appendix A, the exact
solution of the above equation is given by;
(RRT)(t)= etSRoRT(I + 2S-1(e2tS - I)RoRT)-2 etS.	(53)
One can show that RRT converges exponentially to the matrix R?R?T; defined as the
pro jection of 2S on the positive semidefinte cone; this is derived by the same arguments
leading to Proposition 2 (see Appendix C). The convergence rate depends on the eigenvalues
14
Under review as a conference paper at ICLR 2021
of 2S which can be determined using Schur’s complement formula. For a block matrix M
one has
AB
M = CA DB	=⇒	det(M) = det(D) det(A - BD-1C).	(54)
Thus, using the above formula for S - λI
Γ (λ0∕2-λ)im	ς	]
ΣT	-(λ0 /2+λ+)In
we have
m
det(S - λI) = (-1)m+n(λo∕2 + λ)n-m Y(σ2 + λ2∕4 - λ2).	(55)
i=1
Therefore, the eigenvalues of S are:
1.	2m eigenvalues ±si where Si = 2 ,4σ2 + λ0 (i = 1,...,m).
2.	The eigenvalue -λ0∕2 of multiplicity at least n - m.
In general the smallest magnitude eigenvalue will be ∣λo∣∕2. However, if Y is a square matrix
then only the eigenvalues ±sSi above will be present.
F From Implicit Acceleration to Explicit Regularization
We now prove Proposition 4. Replacing (17) into (18) immediately yields (19). It is also
straightforward to verify that applying the gradient flow, dU= -VU' and dV= -VV'
with ` being the ob jective function in (20), yields (19). Next, consider the formal Taylor
series
-..	-,	τ.	.c>.
Q(t) = Q(to) + (t — t0)Q(t0) + 2(t — t。)Q(to) + ….	(56)
Define P(t) ≡ UT(I)U(t) + VT(t)V(t). From (19) one obtains
d Q(t) = - 2 (Q(t) - Λqo )p (t) -1P (t)(Q(t) - Λqo ).	(57)
Since this is "linear" in (Q — Λq0), higher order derivatives take the form
募 Q(t) = X Zi(t)(Q(t)- Λqo )Wi(t)	(58)
i
where the functions Zi ’s and Wi ’s contain a sum of powers and time derivatives of P (t). For
instance, the second order derivative yields
Q = dd2 Q = -1 Q(t)P (t) - 1(Q - Λqo )P(t) - 2 P (t)Q(t) -1 P(t)(Q - Λq0 )
=-4(Q- λQo)(p2 + 2P) - 4(P2 + 2P)(Q - λQo) - 1 P(Q- λQo)p.
Therefore, if Q(t。) = Λq0 at t = t。 then all derivatives (58) vanish identically. As a
consequence, the expansion (56) implies Q(t) = Q(t。) = Λq° for any other t ≥ t。as well.
G On matching rates between Gradient Flow and Gradient
Descent
We provide an explicit example to illustrate why studying the continuous-time dynamics of
the gradient flow is expected to reproduce the behaviour of its discretization, i.e. gradient
descent. For the sake of simplicity let us limit the discussion to the case considered in Section
2 and in the scalar case, Y = σ ∈ R. What we would really like to do is to compare two
different algorithms, namely gradient descent applied to problem (2) versus GD applied to
the factorized problem (3). We thus have
Xk+1 = Xk + η(σ - Xk)	(59)
versus
Uk+1 = Uk + 2η(σ - Uk2 )Uk .	(60)
15
Under review as a conference paper at ICLR 2021
The respective continuous-time limits of these algorithms are
X = (σ - X)	(61)
versus
U = 2(σ - U2)U,	(62)
for X = X(t) and U = U (t). The solution of (61) is given by X(t) - σ = (X0 - Y)e-t,
yielding a rate O(e-t). Let Us introduce the perturbed variable Xk ≡ Xk 一 σ. Hence (59)
readily gives Xk+ι = (1 — η)Xk, thus a matching rate of O(e-ηk). (This example is trivial
because both systems are linear.) Now let us move on to the more interesting nonlinear case.
Consider (61) and let X(t) ≡ U2(t). Thus X = 4σX — 4X2, whose solution is
X(t) = r-⅛σt ≈ σ — ce-4σt,	(63)
which implies a continuous-time rate of O(e-4σt). (Compare this with the last phrase in
Corrolary 1.) Now let us see what happens for (60). Note that this is a complicated nonlinear
recurrence relation. Fortunately, we can solve it approximately. By introducing Xk ≡ Uk2
this recurrence becomes
Xk+1 = Xk + 4η(σ — Xk)Xk + 4η2(σ — Xk)2Xk.	(64)
Now we do the following trick. Let Xk ≡ Xk — σ be a perturbation from equilibrium. For
a small enough η we can neglect terms of O(η2), hence Xk+1 ≈ Xk — 4ηXk (σ + Xk) ≤
(1 — 4ησ)Xk . This implies that Xk → 0, or equivalently Xk → σ, at a discrete-time rate of
e-4σηk. This matches the continuous-time rate.
It is not hard to see how such nonlinear recurrence relations quickly become intractable
for more complicated problems. On the other hand, even though the continuous-time limit
provided by the gradient flow consists of a nonlinear ODE, the analysis is much more
feasible, besides introducing connections with interesting areas of mathematics such as
Riccati differential equations.
16