Under review as a conference paper at ICLR 2021
Revisiting Explicit Regularization in Neural
Networks for Reliable Predictive Probability
Anonymous authors
Paper under double-blind review
Ab stract
From the statistical learning perspective, complexity control via explicit regular-
ization is a necessity for improving the generalization of overparameterized mod-
els, which deters the memorization of intricate patterns existing only in the train-
ing data. However, the impressive generalization performance of overparameter-
ized neural networks with only implicit regularization challenges the importance
of explicit regularization. Furthermore, explicit regularization does not prevent
neural networks from memorizing unnatural patterns, such as random labels. In
this work, we revisit the role and importance of explicit regularization methods
for generalization of the predictive probability, not just the generalization of the
0-1 loss. Specifically, we analyze the possible cause of the poor predictive prob-
ability and identify that regularization of predictive confidence is required during
training. We then empirically show that explicit regularization significantly im-
proves the reliability of the predictive probability, which enables better predictive
uncertainty representation and prevents the overconfidence problem. Our findings
present anew direction to improve the predictive probability quality of determinis-
tic neural networks, which can be an efficient and scalable alternative to Bayesian
neural networks and ensemble methods.
1 Introduction
As deep learning models have become pervasive in real-world decision-systems, the importance of
producing a reliable predictive probability is increasing. In this paper, we call predictive probabil-
ity reliable if it is well-calibrated and precisely represents uncertainty about its predictions. The
calibrated behavior refers to the ability to match its predictive probability of an event to the long-
term frequency of the event occurrence (Dawid, 1982). The reliable predictive probability benefits
many downstream tasks such as anomaly detection (Malinin & Gales, 2019), classification with re-
jection (Lakshminarayanan et al., 2017), and exploration in reinforcement learning (Gal & Ghahra-
mani, 2016). More importantly, deep learning systems with more reliable predictive probability can
provide better feedback for explaining what is going on, situations when its prediction becomes
uncertain, and unexpected anomalies to users. Unfortunately, neural networks are prone to be over-
confident and lack uncertainty representation ability, and this problem has become a fundamental
concern in the deep learning community.
Bayesian methods have innate abilities to produce reliable predictive probability. Specifically, they
express the probability distribution over parameters, in which uncertainty in the parameter space is
automatically determined by data (MacKay, 1992; Neal, 1993). Then, uncertainty in prediction can
be represented by means of providing rich information about aggregated predictions from different
parameter configurations such as entropy and mutual information. From this perspective, deter-
ministic neural networks selecting a single parameter configuration that cannot provide such rich
information naturally lack the uncertainty representation ability. However, the automatic determi-
nation of parameter uncertainty in the light of data, i.e., posterior inference, comes with prohibitive
computational costs. Therefore, the mainstream approach for improving the predictive probabil-
ity quality has been an efficient adoption of the Bayesian principle into neural networks (Gal &
Ghahramani, 2016; Ritter et al., 2018; Teye et al., 2018; Joo et al., 2020a).
Recent works (Lakshminarayanan et al., 2017; Muller et al., 2019; Thulasidasan et al., 2019) has dis-
covered the hidden gems of label smoothing (Szegedy et al., 2016), mixup (Zhang et al., 2018), and
1
Under review as a conference paper at ICLR 2021
adversarial training (Goodfellow et al., 2015), which improve the calibration performance and the
uncertainty representation ability. These findings present a new possibility of improving the reliabil-
ity of the predictive probability without changing the deterministic nature of neural networks. This
direction is appealing because it can be applied in a plug-and-play fashion to the existing building
blocks. This means that they can inherit the scalability, computational efficiency, and surprising gen-
eralization performance of the deterministic neural networks, for which Bayesian neural networks
often struggle (Wu et al., 2019; Osawa et al., 2019; Joo et al., 2020a).
Motivated by these observations, we investigate a general direction from the regularization perspec-
tive to mitigate the unreliable predictive probability problem, rather than proposing new constructive
heuristics or discovering hidden properties of specific methods. Our main contribution is twofold.
First, we present a new direction for alleviating the unreliable predictive behavior, which is readily
applicable, computationally efficient, and scalable to large-scale models compared to Bayesian neu-
ral networks or ensemble methods. Second, our findings provide a novel view of the role of explicit
regularization in deep learning, which improves the reliability of the predictive probability.
2 Analyzing the cause of unreliable predictive probability
2.1	Background
We consider a classification problem with i.i.d. training samples D = {(x(i),y(i))}[ι drawn from
unknown distributions Px,y whose corresponding tuple of random variables is (x, y). We denote
X as an input space and Y as a set of categories {1, 2,…，K}. Let f W : X → Z be a neural
network with parameters W where Z = RK is a logit space. On top of the logit space, the softmax
σ:RK → 4K-1 normalizes the exponential of logits:
φkW (x)
exP(fW (X))
Pi exP(fW(X))
(1)
where we let φkW (X) = σk (fW (X)) for brevity. σk (f W (X)) is often interpreted as the predictive
probability that the label of X belongs to class k (Bridle, 1990)
The probabilistic interpretation of neural network outputs gives the natural minimization objective
for ClaSSifiCation-the cross-entropy between the predictive probability and the one-hot encoded la-
bel: ICE(y, φW(x)) = - Pk ly(k)log φW(x), where l∕(ω) is an indicator function taking one
if ω ∈ A and zero otherwise. By minimizing the cross-entropy (or equivalently maximizing the
log-likelihood) with stochastic gradient descent (SGD) (Robbins & Monro, 1951) or its variants,
modern neural networks achieve the surprising generalization performance.
As the demand for neural networks in real-world decision-making is emerging, reliable predictive
probability has been of interest in the machine learning community. One important quality of pre-
dictive probability is calibrated behavior. Specifically, based on the notion of calibration in classical
forecasting problem (Dawid, 1982), the perfectly calibrated model can be defined as follows:
p(y = k∣ΦW(X)= P)= Pk, ∀p ∈ 4K-1,k ∈ {1, 2,…，K}	⑵
Here note that the calibrated model does not necessarily be ones producing φkW (X) = p(y =k |X).
In practice, expected calibration error (ECE) (Naeini et al., 2015) is widely used for calibration
performance measure. ECE on dataset DT can be computed by binning predictions into M -groups
based on their confidences1 and then averaging their calibration scores by:
X iDT∣ IaCC(Gi)- COnf(Gi)I
(3)
where Gi = X : i/M < maxk φkW (X) ≤ (1 + i)/M, X ∈ DT ; acc(Gi) and conf(Gi) are average
accuracy and confidence of predictions in group Gi , respectively.
1Throughout this paper, the confidence (or predictive confidence) at x refers to maxk φkW (x), which is
different from the confidence in statistics literature.
2
Under review as a conference paper at ICLR 2021
Another metric for evaluating the reliability of predictive probability is based on predictive entropy,
which evaluates how well is the model aware of its ignorance. To this end, predictive entropy is mea-
sured on samples that the model is ignorant of, such as misclassified or out-of-distribution (OOD)
samples. For the classifier having reliable predictive probability, we expect the high uncertainty of
φW (x) on such samples, i.e., the answer “I don’t know.”
However, several recent findings show that the resulting neural net-
work produces unreliable predictive probability, which interprets the
softmax output as the “predictive probability” implausible (Gal &
Ghahramani, 2016). For example, Figure 1 illustrates the unreliable
predictive behavior of ResNet (He et al., 2016): the network produces
outputs with high confidence in misclassified examples (Figure 1 (up-
per)) and provides low predictive entropy on out-of-distribution sam-
ples, albeit the samples belong to none of the classes it learned (Fig-
ure 1 (lower)).
One simple yet effective solution for improving the predictive proba-
bility’s quality is temperature scaling (Guo et al., 2017), which adjusts
the smoothness of the softmax so that the resulting predictive proba-
bility maximizes the log-likelihood on unseen dataset D0 :
exp(fW(X)/t )
哗x(x。og Pj eχp(fW(x)∕τ)
(4)
Figure 1:	Reliability
curve (upper) and pre-
dictive uncertainty on
out-of-distribution samples
(lower) of ResNet.
where W is a fixed pretrained weight and τ is a temperature con-
trolling the smoothness of the softmax output. This simple method
makes the softmax output more reliable predictive probability. For
instance, the predictive confidence matches its actual accuracy well,
and the predictive entropy on out-of-distribution samples significantly
increases (Figure 1).
2.2	A closer look at the log-likelihood on unseen samples.
Motivated by the success of the temperature scaling, we aim to find the relationship between the log-
likelihood and the calibration performance. To this end, let s be a binary random variable indicating
whether a model correctly classifies a sample. Then, we can derive the following upper bound by
using the law of total expectation:
Ex,y[logφyW(x)] = Ex ps|x(s = 1)Ey|s=1,x log φyW (x) + ps|x(s = 0)Ey|s=0,x log φyW (x)
≤ Ex ps|x(s = 1) log φmWx (x) + ps|x(s = 0) log(1 - φmWx (x))	(5)
where mx is the predictive class such that mx = arg maxk fkW (x), ps|x(s = 1) = p(y =
arg maxk φmW (x)|x), and the inequality comes from the fact that Ey|s=0,x log φyW (x) ≤
maxk6=mx log φkW (x) ≤ log(1 - φmWx (x)). The upper bound can be thought of as the probabilistic
measure of the calibration performance. Specifically, suppose a neural network produces an answer
with probability φmW (x) and refuses to answer otherwise. Then, the upper bound becomes the di-
vergence between the model’s ability to correctly predict the sample and the model’s willingness to
answer. When we consider the inequality in equation 5 is applied to the empirical mean on the test
dataset, this inequality clearly explains why the temperature scaling is helpful; it increases a lower
bound of the calibration error with modified confidence by τ closer to its accuracy.
More importantly, the inequality in equation 5 explains the impacts of the cross-entropy minimiza-
tion on the behavior of the neural network. Specifically, suppose E(x,y)〜D [log φW (x)] → 0
for some dataset D. Then, it can be shown that ps|x(s = 1) → 1 and φmWx (x) → 1 for all
(x, y) ∈ D. This holds because we have mink φkW (x) ≥ 1/K for any x and the minimum of
plog q + (1 - p) log(1 - q) are (p, q) → (1, 1) or (p, q) → (0, 0). Therefore, for high-capacity
models that can make the log-likelihood on the training dataset Dtr close to zero, e.g., deep neural
networks, its behavior on the training set will converge to a configuration that corrects all samples
with perfect confidence. Figure 2 illustrates this phenomenon in ResNet trained on CIFAR-100:
Ex〜Dtr [log ΦW] → 0 (a) and E(χ,y)〜Dtr [Ps∣x(s = 1)] → 1 (b) as the training continues.
3
Under review as a conference paper at ICLR 2021
Figure 2: Monitoring changes in the behavior of ResNet during training on CIFAR-100. In (a),
LL indicates the log-likelihood Ex,y[log φyW (x)] and max log-prob indicates Ex [log φmWx (x)]. In
(c), the L2 norm is approximated with respect to empirical distributions of training samples and
validation samples.
Why are these convergences problematic? We observe that function’s properties, which depends
only on x, evaluated on two different datasets are very close to each other if they are drawn from the
same distribution, unlike values that depend on the external randomness y |x; that is, ∣Eχ〜Dtr [g(x)] 一
Ex0〜Dvai[g(x0)]∣ ≪ ∣E(χ,y)〜Dtr[h(x,y)] 一 E(χ0,y0)〜Dvai[h(x0, y0)]∣ for some functions g(∙) and
h(∙). For example, the empirical mean of the maximum log-probability log φW(x) (Figure 2 (a))
and L2 norm of fW (Figure 2 (c))2 on training samples are significantly similar to those values
on unseen samples compared to the log-likelihood (Figure 2 (a)) and the accuracy (Figure 2 (b)).
We conjecture that the log-likelihood maximization with a high-capacity neural network naturally
results in a high calibration error on unseen samples due to this discrepancy; that is, it produces
perfect confidence on unseen samples as it does on the training samples where it cannot produce the
perfect accuracy.
Fortunately, this result also indicates that restricting the predictive confidence φmWx (x) on training
samples will directly impact the predictive confidence on unseen samples and therefore can reduce
the calibration error. For this reason, we explore various ways to restrict confidence and show their
efficacy on a wide range of tasks for the rest of the paper.
3 Confidence control for reliable predictive probability
The goal of this section is to examine the effectiveness of restricting predictive confidence on im-
proving the reliability of predictive probability. Therefore, we apply several existing regularizers to
the (pre-activation) ResNet (He et al., 2016) trained on CIFAR (Krizhevsky et al., 2009), which is
one of the most prevalent basis models in many state-of-the-art architectures (Huang et al., 2017;
Xie et al., 2017). We also present the VGG (Simonyan & Zisserman, 2015) as a representative of
models without residual connection in appendix B, in which we observe results similar to ResNet.
We follow the standard training strategy presented in He et al. (2016) except for the initial learning
rate warm-up, clipping the gradient when its norm exceeds one, and making an extra validation set
of 10,000 samples split from the training set. We describe a detailed setup in appendix A.
We mainly use the ECE for measuring the reliability of the predictive probability. In addition,
we used negative log-likelihood (NLL), following the common practice (Lakshminarayanan et al.,
2017; Ovadia et al., 2019; Guo et al., 2017). Here, NLL evaluates how well the predictive probability
explains the test data DT, whose optimal score is achieved if and only if φW(x) perfectly matches
p(y|x).
3.1	Confidence control by weight decay
The simplest way to constrain the confidence may conceivably be using stronger weight decay
(Krogh & Hertz, 1992), which can encourage the production of less extreme outputs by shrink-
ing weights. Therefore, we first explore the confidence control by varying the weight decay ratio
λ, conjecturing that the weight decay ratio used in the base model, e.g., λ = 0.0001 in ResNet,
2This norm is evaluated on the function space by ∣∣ fW ∣∣2= (R |fW(x)∣2dPX(x))1/2, which We will
discuss with more detail in Section 3.2.
4
Under review as a conference paper at ICLR 2021
is too small to prevent overconfident predictions. Figure 3 (upper) illustrates the impact of λ on
generalization performance and calibration performance. We can see that ECE decreases to some
extent as the decay ratio increases. However, the ECE improvement becomes inversely proportional
to a generalization performance improvement when the decay ratio is larger than 0.001. This means
that improving the reliability with strong weight decay is against the primary goal of supervised
learning.
We further investigate this undesirability by monitoring the L2
function norm of fW (cf. equation 7) under different weight de-
cay ratios. This analysis is motivated by the fact that temperature
scaling modifies only the Lp norm of fW as it divides logits with
a scalar. Under the single-label classification model, this measure
is a useful proxy for the predominance of the maximum predictive
probability. To set the desired value of the L2 norm, we “leak” the
test set DT and then apply temperature scaling to the neural network
trained with λ = 0.0001.
Figure 3 shows that the SGD with various decay ratios finds only a
trivial solution or an infeasible solution to the following optimiza-
tion problem:
minED [ice(y, Φw(χ))]	s.t. k fW k2≤k fW0/τ* k2	(6)
where E D [∙] is the empirical mean on D,
the trained weights under λ
arg maxτ
EDT [log {σ(fW0(x)∕τ)}].
0.0001, and
This means
W0 is
τ*	=
that re-
weight decay ratio on ECE
and accuracy (upper) and the
L2 norm (lower).
ducing the predictive confidence to the desired level without
sacrificing generalization performance is a significantly challeng-
ing optimization problem. Specifically, Figure 3 (lower) shows that
the L2 norm goes to zero under the decay ratio λ ≥ 0.003, which means that all weights collapse
to zero, i.e., the trivial solution. This happens when the magnitude of the decay ratio overwhelms
the magnitude of the weight update by the gradient of the cross-entropy, e.g., at approximately the
epoch 50 under λ = 0.003 (Figure 3 (lower)). However, ratios of 0.001 and 0.0001 do not suffer
from the weight collapse, but the scale of the L2 norm under such ratios is significantly higher than
k fW0∕τ* ∣∣2, which corresponds to infeasible solutions (Figure 3 (lower)). These results may
seem natural because the weight decay does not explicitly consider the constraint about predictive
confidence. Therefore, we explore an alternative method that adds a regularization loss explicitly
concerning predictive confidence, e.g., the constraint in equation 6.
3.2	Direct confidence control by explicit regularization
In this subsection, we examine two types of regularizers that directly constrain the predictive confi-
dence on training samples.
Regularization in the function space. The first approach regards fW as an element of Lp(X )
space. Lp space is the space of measurable functions with the norm:
k 严 kp= (JxfW(x)∣pdPχ(x)) 1/p < ∞	⑺
Here, we note that the norm is computed with respect to the input generating distribution Px , which
allows concerning how the function fW actually behaves on the data manifold. Since Px is un-
known, so it is approximated by the Monte-Carlo approximation with minibatch samples. Then, the
approximate function norm can be computed by ∣∣ f W kp≈ mm PijIfW (x(i))∣p. By penalizing the
complexity with the Lp norm, the continuous increase in the leading entry of logit towards infinity,
or continuous decreases in the nonleading entry to negative infinity, can be prevented. In this paper,
we examine k fW k1 and k fW k22 regularization losses.
Regularization in the probability distribution space. The second approach regards fW(x) as
a random variable and then minimize its distance to the bell-shaped target distribution. Since the
5
Under review as a conference paper at ICLR 2021
Table 1: Experimental results under various regularization methods on ResNet. Arrows on the
metrics represent the desirable direction. We searched four hyperparameters for each method and
chose the best hyperparameter based on validation accuracy (cf. appendix A). Values represent mean
obtained from five repetitions, and all values are rounded to two decimal places.
method	CIFAR-10				CIFAR-100			
	Acc ↑	NLL ；	ECE ；	k fW k2	Acc ↑	NLL ；	ECE ；	k fW k2
Vanilla	94.17	0.29	4.06	20.12	74.64	1.31	13.95	43.81
k fW k1	94.32	0.25	2.89	6.81	76.28	1.27	7.77	9.07
k fW k22	94.38	0.23	3.27	8.19	75.84	1.07	5.52	10.55
SWι(μW ,ν)	94.46	0.23	2.12	5.4	76.27	1.1	7.02	9.67
PER	94.30	0.23	2.89	6.94	76.23	1.15	4.67	7.56
bell-shaped distribution has a peak mass at zero and the same density on points with the same
distance to zero, this regularization can make logits have small values and make each entry of logits
have a similar scale, which can prevent neural nets from producing overconfident answers. Among
various bell-shaped distributions, we choose the standard Gaussian for the target distribution due
to its simplicity and popularity. As a metric in the probability distribution space, we use the sliced
Wasserstein distance of order one because of its computational efficiency and ability to measure
the distance between probability distributions with different supports, which is useful when dealing
with the empirical distribution. We refer to Peyre et al. (2019) for more detailed explanations about
this metric. Specifically, given minibatch samples D0 = {x(i)}im=1, let an empirical measure of
logits be μW(A) = ^^ Pi 1a(∕W(x(i))) and the standard Gaussian measure on Z be V(A)=
2∏K/2 JA exp (-1 k Z ∣∣2) dz. Then, the sliced Wasserstein distance can be computed by:
SWI (μW，v )=L L
m
Fμθ (X)- mm X l(∞,x)(hz(i),θi) dxdλ(θ)
i=1
(8)
where z(i) = fW(x(i)), λ is a uniform measure on the unit sphere SK-1, and μθ is a measure
obtained by projecting μW at angle θ. Projected error function regularization (PER) (Joo et al.,
2020b) simplifies the computation of the SWι(μW, V) by applying the Minkowski inequality to the
above equation. As a result, the gradient of PER resembles the gradient of Huber loss (Huber, 1964)
in the projected space, which allows combining advantages of both the L1 norm and the L2 norm
as well as capturing the dependency between logits of each location by a projection operation (Joo
et al., 2020b).
Image classification with ResNet. Table 1 lists the experimental results, in which both regulariza-
tion in the function space and the Wasserstein probability space successfully controlled the confi-
dence, e.g., reduced the L2 norm of ResNet by at least 34% on CIFAR-10 and 68% on CIFAR-100.
We note that regularization methods can constrain the confidence without compromising the gener-
alization performance; actually, all regularization methods achieves small but consistent improve-
ments of test error rates. We also note that the sum of the Frobenius norm of weights often increases
compared to the vanilla method and changes only at most 2% when it decreases, which again shows
the undesirability of adjusting the weight decay ratio for confidence control.
More importantly, the predictive probability’s reliability significantly improves under all considered
measures compared to the vanilla method. For instance, the regularization methods reduce the NLL
of ResNet by at least 13% CIFAR-10 and 6% on CIFAR-100 and reduce the ECE of ResNet by at
least 19% on CIFAR-10 and 41% on CIFAR-100. These improvements are comparable to or better
than those of temperature scaling. For instance, ResNet with temperature scaling gives an NLL
of 1.15 and an ECE of 8.41 on CIFAR-100. Here, We split the test set into two equal-size sets-a
performance measurement set and a temperature calibration set-and measure the performance after
temperature scaling with the calibration set, and repeat the same procedure by reversing their roles.
We note that a more realistic evaluation requires drawing the temperature calibration set from the
training set. In this case, its performance decreases as it cannot fully exploit the entire dataset during
training.
Predictive uncertainty. We also investigate the usefulness of explicit regularization on the uncer-
tainty representation ability on misclassified samples and OOD samples. Since the model is ignorant
6
Under review as a conference paper at ICLR 2021
Ll Iogit regularization
, Correct (ln-dist.)
ɪ --- Wrong (ln-dist.)
I——∞□
0.00
0	2	4
Entropy
Vanilla
o.oo-
----Correct (ln-dist.)
----Wrong (ln-dist.)
——OOD
N Iogit regularization
0 5 0 5
Q 7∙∙
一u9a
0	2	4
Entropy
Deep ensemble
----Correct(ln-dist.)
----WronganYistJ
——OOD
Sliced Wasserstem
ι.5--------------------
铲。
0 0.5
0.0
0	2	4
Entropy
MC-dropout (P = O.2)
ir^u9α
PER
0.0-	.	,	,
0	2	4
Entropy
MC-dropout (P = O.3)
----Correct(ln-dist.)
----Wrong(ln-dist.)
——OOD
2 1
i^u9a
3 2 1
i^u9a
ir^u9a
a-75-50
loo
u9a
----Correct (ln-dist.)
----Wrong (ln-dist.)
—OOD
Entropy
° O	2
Entropy
----Correct(ln-dist.)
----Wrong(ln-dist.)
——OOD
Entropy
4
0.00l-l^--------A W-]-
0	2	4
Entropy
Figure 4:	Density of predictive uncertainty on CIFAR-100 (in-distribution) and SVHN (OOD). The
upper figures illustrate explicit regularization methods, and the lower figures illustrate the vanilla
method, ensemble methods, and Bayesian neural networks.
Table 2: Misclassification detection and OOD detection task performances based on NBAUCC0.5
(higher is better). MC-D (p) represents MC-dropout with probability p, and Ens represents deep
ensemble.
Task	Vanilla	MC-D (0.2)	MC-D (0.3)	Ens.	k fW k1	k fW k22	SWι(μW ,V)	PER
Miscls	1.55	3.33	5.94	0.59	9.53	6.85	5.10	10.24
OOD	15.98	22.14	31.28	28.46	55.51	31.47	31.63	49.77
of misclassified samples and OOD samples as they do not belong to any category, the neural network
should produce the answer of“I don’t know” for these samples. Figure 4 illustrates the predictive un-
certainty of ResNet-50 with respect to CIFAR-100 (in-distribution) and SVHN (Netzer et al., 2011)
(OOD). The vanilla method’s predictive uncertainty on SVHN and misclassified samples remains in
the somewhat confident region, albeit less confident compared to those on CIFAR-100. In contrast,
explicit regularization successfully gathers a mass of predictive uncertainty for both SVHN samples
and misclassified samples around the maximum-entropy region.
We compare the uncertainty representation abilities of regularization methods to those of Bayesian
neural networks and ensemble methods. Specifically, we use the scalable Bayesian neural network,
called MC-dropout (Gal & Ghahramani, 2016), because other methods based on variational infer-
ence (Graves, 2011; Blundell et al., 2015; Wu et al., 2019) or MCMC (Welling & Teh, 2011; Zhang
et al., 2020) require modifications to the baseline including the optimization procedure and the ar-
chitecture, which deters a fair comparison. We searched a dropout rate over {0.1, 0.2, 0.3, 0.4, 0.5 }
and used 100 number of Monte-Carlo samples at test time, i.e., 100x more inference time. We also
use the deep ensemble (Lakshminarayanan et al., 2017) with 5 number of ensembles, i.e., 5x more
training and inference time. Figure 4 shows that the regularization-based methods produce signifi-
cantly better uncertainty representation than the MC-dropout and deep ensemble; even though both
deep ensemble and MC-dropout can move mass on less certain regions, the positions are still far
from the highest uncertainty region, unlike the regularization-based methods. Finally, we note sev-
eral approaches (Malinin & Gales, 2019; Papadopoulos et al., 2019) explicitly maximize uncertainty
on OOD training samples for OOD detection tasks. However, we do not consider such approaches,
assuming we have no access to OOD samples during training.
Misclassification/OOD detection. We also evaluate the uncertainty representation ability for
misclassified and out-of-distribution samples. To this end, we use recently proposed normalized
bounded area under the calibration curve (NBAUCC) (Kong et al., 2020), which handles the disad-
vantage of classical measures such as AUROC and AUPRC that cannot take the calibration property
into account. Specifically, NBAUCC with the upper bound of confidence threshold for misclassified
7
Under review as a conference paper at ICLR 2021
Table 3: Experimental results under various regularization methods on BERT. Arrows on the metrics
represent the desirable direction. Values represent mean obtained from five repetitions, which are
rounded to two decimal places.
method	20-Newsgroup				Web of science			
	Acc ↑	NLL ；	ECE ；	k fW k2	Acc ↑	NLL ；	ECE ；	k fW k2
Vanilla	84.51	0.70	6.40	9.71	81.28	0.94	8.26	20.13
k fW k1	84.71	0.61	4.67	6.02	81.68	0.90	7.02	8.89
k fW k22	85.04	0.64	3.24	5.11	81.83	0.94	5.79	7.70
SWι(μW ,ν)	84.76	0.67	3.78	4.98	81.44	0.89	7.22	11.33
PER	85.02	0.65	4.50	6.21	81.36	0.87	8.19	13.21
1.6
20
15
80ι一			
			
75-∙1—			
小			
65 20 10	0	200 300		
1.2-
NLL
1.4-
	Eariy Stoplng
rtɪj


20 100	200 300
Training Epochs (baseline： 200)
(a) Early stopping
ECE
10
5-
VXF		PER
		
		
20 100	200 300		
LOL-T-「―1I
8 32 128 512 2048
NLL
Batch Size (baseline： 128)
(b) Varying noise of stochastic gradient
Figure 5:	Effects of implicit regularization methods on accuracy, negative log-likelihood, and ECE.
or OOD samples, denoted by τ , is computed by:
1M τ
NBAUCCT = M X FI (Mr)	⑼
i=1
where F1 (t) computes F1 score by regarding samples with predictive confidence higher than τ
as correct (resp. in-distribution) samples and incorrect (resp. OOD) samples otherwise; M is a
predetermined step size. We present the misclassification/OOD detection performance in Table 2,
where all regularization methods significantly improve detection performances based on their better
predictive uncertainty representation abilities.
Document classification with BERT. Finally, we perform additional experiments on the natural
language processing domain in order to more thoroughly evaluate the effectiveness of explicit reg-
ularization. Inspired by the recent finding (Kong et al., 2020), we perform document classification
on 20 newsgroup dataset (Socher et al., 2012) and web of science dataset (Kowsari et al., 2017). We
obtain a classifier by adding one linear layer on top of the BERT (Devlin et al., 2018). We provide
training details in Appendix A. As consistent with the findings in the image classification task, all
explicit regularization methods improve the reliability of predictive probability with consistent gains
in test accuracy (Table 3).
3.3 Can the improved reliability be attributed to improved generalization ?
Considering that the log-likelihood is the training objective, the improved reliability may be caused
by the improved generalization due to regularization effects. To clarify this point, we investigate
the impact of improved generalization via implicit regularization mechanisms (early stopping and
exploitation of noise from SGD) to ResNet-50 on CIFAR-100. Figure 5 (a) shows that the early
stopping does not result in dramatic improvements in all measures compared to PER. Instead, NLL
and ECE slightly improve under longer training due to increased classification accuracy. Consider-
ing that the L2 norm of logits achieves a significantly high value at the early stage of training (cf.
Figure 2 (b)), this undesirable result may seem natural. Next, we examine the effects of varying
batch sizes under the baseline learning rate or linear scaling rule (Goyal et al., 2017), in which a
smaller batch size increases the noise of the stochastic gradient. In Figure 5 (b), we observe that in-
creasing the noise level by using smaller batch sizes benefits generalization performance. However,
it does not benefit and even worsens the predictive distribution quality (NLL and ECE). In addi-
tion, we observed that these implicit regularizers fail to produce high predictive entropy on OOD
samples, unlike explicit regularization. These observations corroborate the efficacy of confidence
control in improving the reliability of predictive probability, which highlights the importance of
explicit regularization.
8
Under review as a conference paper at ICLR 2021
4	Related work
Recent works show that joint modeling of a generative model p(x) along with a classifier p(y|x),
or p(x, y) directly, helps to produce reliable predictive probability (Alemi et al., 2018; Nalisnick
et al., 2019; Grathwohl et al., 2020). Specifically, Alemi et al. (2018) argue that modeling stochastic
hidden representation through the variational information bottleneck principle (Alemi et al., 2017)
allows representing better predictive uncertainty. This can be related to the effectiveness of en-
semble methods, which aggregate representations of several models, on enhancing predictive un-
certainty representation and confidence calibration (Lakshminarayanan et al., 2017; Ovadia et al.,
2019; Ashukha et al., 2020). In this regard, hybrid modeling and ensemble methods share a similar
principle to the Bayesian methods, concerning the stochasticity of the function. However, this paper
concentrates on explicit regularization for controlling the predictive confidence, which is fundamen-
tally different from previous works focusing on building the stochastic representation.
Other works concentrate on the structural characteristics of neural networks. Specifically, Hein
et al. (2019) identify the cause of the overconfidence problem based on an analysis of the affine
compositional function, e.g., ReLU. The basic intuition behind this analysis is that one can always
find a multiplier λ to an input x, which makes a neural network produce one dominant entry on
λx. Verma & Swami (2019) point out that the region of the highest predictive uncertainty under the
softmax forms a subspace in the logit space, so the volume of an area representing high predictive
uncertainty would be negligible. However, our approach suggests that these structural characteris-
tics’ inherent flaws can be easily cured by adding an explicit regularization term without changing
the existing components of neural networks.
From the perspective of the statistical learning theory (Vapnik, 1995), a regularization method min-
imizing some form of complexity measures, e.g., Rademacher complexity (Bartlett et al., 2005)
or VC-dimension (Vapnik & Chervonenkis, 2015), is a “must” to achieve better generalization of
overparameterized models by preventing memorization of intricate patterns existing only in training
samples. However, the role of capacity control with explicit regularization is challenged by many
observations in deep learning. Specifically, overparameterized neural networks achieve impressive
generalization performance with only implicit regularizations contained in the optimization proce-
dures (Hardt et al., 2016; Li et al., 2019) or the structural characteristics (Gunasekar et al., 2018;
Hanin & Rolnick, 2019; Luo et al., 2019). Moreover, Zhang et al. (2017) show that explicit regular-
ization cannot prevent neural networks from easily fitting random labels that cannot be generalized
to unseen examples. Therefore, the importance of capacity control with explicit regularization seems
to be questionable in deep learning. In this work, we reemphasize its importance, presenting a dif-
ferent view on the role of regularization in terms of generalization of the predictive probability, not
solely on better accuracy.
5	Conclusion
In this works, we show the usefulness of the explicit regularization on improving the reliability of
predictive probability, which presents a novel view on the role and importance of explicit regular-
ization. Specifically, our extensive experimental results show that several existing regularization
methods improves calibration, predictive uncertainty representation, misclassification/OOD detec-
tion performances, and even the test accuracy. Compared to the temperature scaling method, the
regularization-based approach can be more widely applicable in more general settings such as con-
tinual learning and online learning as it does not requires an additional hold-out dataset. In addition,
the regularization-based approaches provided a small but consistent gain in generalization perfor-
mances, while the temperature scaling does not impact the generalization performances. Compared
to marginalization-based approach such as Bayesian neural networks and ensemble methods, the
regularization-based approach is appealing in terms of computational efficiency and scalability. De-
spite these advantages, the regularization methods are limited in that they cannot utilize more so-
phisticated uncertainty quantification methods based on stochastic representation on the predictive
probability space, such as mutual information measuring epistemic uncertainty (Smith & Gal, 2018),
due to its deterministic nature. We leave this limitation as an important future direction of research,
which may be solved by more expressive parameterization, e.g., (Wilson et al., 2016; Skafte et al.,
2019; Malinin & Gales, 2019; Joo et al., 2020a).
9
Under review as a conference paper at ICLR 2021
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. In International Conference on Learning Representations, 2017.
Alexander A Alemi, Ian Fischer, and Joshua V Dillon. Uncertainty in the variational information
bottleneck. arXiv preprint arXiv:1807.00906, 2018.
Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain
uncertainty estimation and ensembling in deep learning. In International Conference on Learning
Representations, 2020.
Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local Rademacher complexities. The
AnnalsofStatistics, 33(4):1497-1537, 2005.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. In International Conference on Machine Learning, 2015.
John S Bridle. Probabilistic interpretation of feedforward classification network outputs, with rela-
tionships to statistical pattern recognition. In Neurocomputing, pp. 227-236. Springer, 1990.
A Philip Dawid. The well-calibrated Bayesian. Journal of the American Statistical Association, 77
(379):605-610, 1982.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Will Grathwohl, KUan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. In International Conference on Learning Representations, 2020.
Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems, 2011.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. ImPlicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, 2018.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, 2017.
Boris Hanin and David Rolnick. DeeP relu networks have surPrisingly few activation Patterns. In
Advances in Neural Information Processing Systems, 2019.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In International Conference on Machine Learning, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deeP into rectifiers: SurPass-
ing human-level Performance on imagenet classification. In IEEE International Conference on
Computer Vision, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity maPPings in deeP residual
networks. In European Conference on Computer Vision, PP. 630-645, 2016.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence Predictions far away from the training data and how to mitigate the Problem. In IEEE
Conference on Computer Vision and Pattern Recognition, 2019.
10
Under review as a conference paper at ICLR 2021
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
Peter J Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35
(1):73-101,1964.
Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being Bayesian about categorical probability. In
International Conference on Machine Learning, 2020a.
Taejong Joo, Donggu Kang, and Byunghoon Kim. Regularizing activations in neural networks
via distribution matching with the Wasserstein metric. In International Conference on Learning
Representations, 2020b.
Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, and Chao Zhang. Calibrated lan-
guage model fine-tuning for in-and out-of-distribution data. In Conference on Empirical Methods
in Natural Language Processing, 2020.
Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, Matthew S Ger-
ber, and Laura E Barnes. Hdltex: Hierarchical deep learning for text classification. In 2017 16th
IEEE international conference on machine learning and applications (ICMLA), pp. 364-371.
IEEE, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances
in Neural Information Processing Systems, 1992.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, 2017.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. In Advances in Neural Information Processing Systems,
2019.
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Towards understanding regularization
in batch normalization. In International Conference on Learning Representations, 2019.
David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural Compu-
tation, 4(3):448-472, 1992.
Andrey Malinin and Mark Gales. Reverse KL-divergence training of prior networks: Improved
uncertainty and adversarial robustness. In Advances in Neural Information Processing Systems,
2019.
Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems, 2019.
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated proba-
bilities using Bayesian binning. In AAAI Conference on Artificial Intelligence, 2015.
Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
Hybrid models with deep and invertible features. In International Conference on Machine Learn-
ing, 2019.
Radford M Neal. Bayesian learning via stochastic dynamics. In Advances in Neural Information
Processing Systems, 1993.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
11
Under review as a conference paper at ICLR 2021
Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh Jain, Runa Eschenhagen,
Richard E Turner, and Rio Yokota. Practical deep learning with Bayesian principles. In Advances
in Neural Information Processing Systems, 2019.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems,
2019.
Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, and Jiamian Wang.
Outlier exposure with confidence control for out-of-distribution detection. arXiv preprint
arXiv:1906.03509, 2019.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning,11(5-6):355-607, 2019.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable Laplace approximation for neural
networks. In International Conference on Learning Representations, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 400-407, 1951.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations, 2015.
Nicki Skafte, Martin J0rgensen, and S0ren Hauberg. Reliable training and estimation of variance
networks. In Advances in Neural Information Processing Systems, 2019.
Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detec-
tion. arXiv preprint arXiv:1803.08533, 2018.
Richard Socher, Yoshua Bengio, and Christopher D Manning. Deep learning for nlp (without
magic). In Tutorial Abstracts of ACL 2012, pp. 5-5. 2012.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In IEEE Conference on Computer Vision and
Pattern Recognition, 2016.
Mattias Teye, Hossein Azizpour, and Kevin Smith. Bayesian uncertainty estimation for batch nor-
malized deep networks. In International Conference on Machine Learning, 2018.
Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Micha-
lak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
In Advances in Neural Information Processing Systems, 2019.
Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 1995.
Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. In Measures of Complexity, pp. 11-30. Springer, 2015.
Gunjan Verma and Ananthram Swami. Error correcting output codes improve probability estimation
and adversarial robustness of deep neural networks. In Advances in Neural Information Process-
ing Systems, 2019.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
International Conference on Machine Learning, 2011.
Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Deep kernel learning. In
International Conference on Artificial Intelligence and Statistics, 2016.
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose Miguel Hernandez-Lobato,
and Alexander L Gaunt. Deterministic variational inference for robust Bayesian neural networks.
In International Conference on Learning Representations, 2019.
12
Under review as a conference paper at ICLR 2021
Saining Xie, Ross Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond empiri-
cal risk minimization. In International Conference on Learning Representations, 2018.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and AndreW Gordon Wilson. Cyclical
stochastic gradient MCMC for Bayesian deep learning. In International Conference on Learning
Representations, 2020.
13
Under review as a conference paper at ICLR 2021
A Detailed experimental setup
ResNet base setup. We trained ResNet for 200 epochs by SGD with momentum coefficient 0.9,
minibatch size of 128, and a weight decay ratio 0.0001; weights were initialized by He initialization
(He et al., 2015); an initial learning rate was 0.1, and decreased by a factor of 10 at 100 and 150
epochs. We also used the standard augmentation presented in He et al. (2015).
VGG setup. We trained VGG by re-using the ResNet setup for convenience, except increasing the
weight decay ratio to 0.0005 as in Simonyan & Zisserman (2015).
BERT setup. Our configuration is based on Kong et al. (2020). Specifically, the model is trained
for 5 epochs by Adam optimizer with learning rate 0.00005 and minibatch size of 32.
Hyperparameters. We searched four regularization loss coefficients for each method, and chose
best one based on validation set accuracy (Table 4). The search spaces were: {0.1, 0.03, 0.01, 0.003}
for L1 norm; {0.03, 0.01, 0.003, 0.001} for L2 norm; {0.1, 0.03, 0.01, 0.003} for sliced Wasserstein
regularization; {1.0, 0.3, 0.1, 0.03} for PER (10x lower coefficient for CIFAR-10).
Sliced Wasserstein regularization and PER involve the integral over the unit sphere, which is eval-
uated by Monte-Carlo approximation. In this paper, we used 256 number of evaluations, following
(Joo et al., 2020b).
Table 4: Best hyperparameters for each configuration
Regularizer	VGG-16 & CIFAR-10	VGG-16 & CIFAR-100	ResNet-50 & CIFAR-10	ResNet-50 & CIFAR-100
k fW k1	0.01	0.03	0.01	0.01
k fW k22	0.003	0.01	0.003	0.01
SWi(μW ,ν)	0.001	0.03	0.001	0.01
PER	0.003	1.0	0.03	1.0
B VGG results
As consistent with the results of ResNet, all regularization losses improves NLL, ECE, and accuracy
(Table 5), except L1 regularization on CIFAR-100. However, the improvements are less significant
compared to ResNet because the small capacity of VGG makes the vanilla method produces less
confident answers and then less vulnerable to the confidence penalty. This can be inferred from that
values of k f W k2 of VGG are reduced by almost 50% compared to those of ResNet.
Table 5: Experimental results under various regularization methods. Arrows on the metrics repre-
sent the desirable direction. Values represent μ ± σ obtained from five repetitions, and all values are
rounded to two decimal places.
Model & Data	Regularizer	Acc ↑	NLL J	ECE J	k fW k2
VGG-16 &	Vanilla	92.97 ±0.20	0.35 ±0.01	4.96 ±0.16	9.62 ±0.05
CIFAR-10	k fW k1	93.07 ±0.08	0.33 ±0.01	4.1 ±0.11	6.62 ±0.01
	k fW k22	93.06 ±0.03	0.31 ±0.01	4.58 ±0.07	7.44 ±0.03
	SWl(μW ,ν)	93.13 ±0.06	0.29 ±0.0	1.9 ±0.07	5.4 ±0.01
	PER	93.1 ±0.17	0.31 ±0.01	4.79 ±0.13	8.49 ±0.03
VGG-16 &	Vanilla	71.96 ±0.12	1.4 ±0.01	16.9 ±0.09	22.98 ±0.31
CIFAR-100	k fW k1	72.71 ±0.17	1.44 ±0.01	11.37 ±0.18	9.27 ±0.1
	k fW k22	72.68 ±0.16	1.31 ±0.01	10.79 ±0.28	9.28 ±0.11
	SWι(μW,ν)	72.26 ±0.23	1.35 ±0.01	12.59 ±0.34	9.72 ±0.02
	PER	72.89 ±0.24	1.34 ±0.01	6.47 ±0.12	7.37 ±0.05
14