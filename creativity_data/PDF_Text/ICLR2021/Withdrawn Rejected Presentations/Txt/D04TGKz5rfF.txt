Under review as a conference paper at ICLR 2021
A frequency domain analysis of gradient-
BASED ADVERSARIAL EXAMPLES
Anonymous authors
Paper under double-blind review
Ab stract
It is well known that deep neural networks are vulnerable to adversarial examples.
We attempt to understand adversarial examples from the perspective of frequency
analysis. Several works have empirically shown that the gradient-based adver-
sarial attacks perform differently in the low-frequency and high-frequency part of
the input data. But there is still a lack of theoretical justification of these phenom-
ena. In this work, we both theoretically and empirically show that the adversarial
perturbations gradually increase the concentration in the low-frequency domain
of the spectrum during the training process of the model parameters. And the
log-spectrum difference of the adversarial examples and clean image is more con-
centrated in the high-frequency part than the low-frequency part. We also find out
that the ratio of the high-frequency and the low-frequency part in the adversarial
perturbation is much larger than that in the corresponding natural image. Inspired
by these important theoretical findings, we apply low-pass filter to potential ad-
versarial examples before feeding them to the model. The results show that this
preprocessing can significantly improve the robustness of the model.
1	Introduction
Recently, deep neural networks (DNN) have achieved great success in the field of image process-
ing, but it was found that DNNs are vulnerable to some synthetic data called adversarial examples
((Szegedy et al., 2013), (Kurakin et al., 2016)). Adversarial examples are natural samples plus ad-
versarial perturbations, and the perturbations between natural samples and adversarial examples are
imperceptible to human but able to fool the model. Typically, generating an adversarial example can
be considered as finding an example in an -ball around a natural image that could be misclassified
by the classifier. Recent studies designed Fast Gradient Sign Method (FGSM, (Goodfellow et al.,
2014)), Fast Gradient Method (FGM, (Miyato et al., 2016)), Projected Gradient Descent (PGD,
(Madry et al., 2017)) and other algorithms (Carlini & Wagner (2017), (Su et al., 2019), (Xiao et al.,
2018), (Kurakin et al., 2016), (Chen et al., 2017)) to attack the model.
Since the phenomenon of adversarial examples was discovered, many related works have made
progress to study why they exist. Several works studied this phenomenon from the perspective of
feature representation. Ilyas et al. (2019) divided the features into non-robust ones that are respon-
sible for the model’s vulnerability to adversarial examples, and robust ones that are close to human
perception. Further, they showed that adversarial vulnerability arises from non-robust features that
are useful for correct classification.
Another way to characterize adversarial examples is to investigate them in the frequency domain via
Fourier transform. Wang et al. (2020a) divided an image into low-frequency component (LFC) and
high-frequency component (HFC) and they empirically showed that human can only perceive LFC,
but convolutional neural networks can obtain useful information from both LFC and HFC. Yin et al.
(2019) filters out the input data with low-pass or high-pass filters to study the sensitivity of the ad-
ditive noise with different frequencies. Wang et al. (2020b) claimed that existing adversarial attacks
mainly concentrate in the high-frequency part. Sharma et al. (2019) found that when perturbations
are constrained to the low-frequency subspace, they are generated faster and are more transferable,
and will be effective to fool the defended models, but not for clean models.
All of these works showed that spectrum in frequency domain is an reasonable way to study the
adversarial examples.
1
Under review as a conference paper at ICLR 2021
However, there is a lack of theoretical understanding about the dynamics of the adversarial pertur-
bations in frequency domain along the training process of the model parameters. In this work, we
focus on the frequency domain of the adversarial perturbations to explore the spectral properties of
the adversarial examples generated by FGM (Miyato et al., 2016) and PGD (Madry et al., 2017).
In this work, we give a theoretical analysis in frequency domain of natural images for the adversarial
examples.
•	For a two-layer neural network with non-linear activation function, we prove that adversar-
ial perturbations by FGM and l2 -PGD attacks gradually increase the concentration in the
low-frequency part of the spectrum during the training process over model parameters.
•	Meanwhile, the log-spectrum difference of the adversarial examples (the definition will
be clarified in Section 2.2) will be more concentrated in the high-frequency part than the
low-frequency part.
•	Furthermore, we show that the ratio of the high-frequency and the low-frequency part in
the adversarial perturbation is much larger than that in the corresponding clean image.
Empirically,
•	we design several experiments on the two-layer model and Resnet-32 with CIFAR-10 to
verify the above findings.
•	Based on these phenomena, we filter out the high-frequency part of the adversarial ex-
amples before feeding them to the model to improve the robustness. Compared with the
adversarially trained model with the same architecture, our method achieves comparable
robustness with the similar computational cost of normal training and almost no loss of
accuracy.
The rest of the paper is organized as follows: In Section 2, we present preliminaries and some
theoretical analysis on the calculation of the spectrum and log-spectrum. Then we provide our
main results about the frequency domain analysis of gradient-based adversarial examples in Section
3. Furthermore, some experiments for supporting our theoretical findings are shown in Section 4.
Finally we conclude our work and conduct a discussion about the future work in Section 5. All the
details about the proof and experiments are shown in the Appendix.
2	Background
2.1	Preliminaries
Notations We use {0,1,…，d} to denote the set of all integers between 0 and d and use ∣∣ ∙ ∣∣p to
denote lp norm. Specifically, We denote by ∣∣ ∙ ∣∣ the l2 norm. For a d-dimensional vector x, We use
Xμ to denote its μ-th component with index starting at 0. For a scalar function f (x): Rd → R, Vxf
and ∂μf denote the gradient vector and its μ-th component. We let sgn(x) = 1 for x > 0 and -1
for x < 0. Normal training refers to training on the original training data for learning the optimal
weights for the neural networks. X = F(x) denotes the Discrete Fourier Transform (DFT) of x.
Discrete Fourier Transform The k-th frequency component g[k] of one-dimensional DFT of a
vector g is defined by
d-1
g[k]= F (g)[k] = X gμeik 2π μ,
μ=0
where k ∈ {-d/2, -d/2 + 1, ..., d/2} if d is even and k ∈ {-(d - 1)/2, ..., (d - 1)/2} if d
is odd. For convenience, we always consider an odd d and one can easily generalize the case to
even dimensions. For an integer cut-off (cutoff frequency) kr ∈ (0, (d - 1)/2), the Low Frequency
Components (LFC) and High Frequency Components (HFC) of g are denoted by
gι[k] = {”]
if - kc ≤ k ≤ kc,
otherwise
[k]	0 if - kc ≤ k ≤ kc,
gh	[讥 k] otherwise.
2
Under review as a conference paper at ICLR 2021
Then the frequency space Rd can be decomposed as Sl L Sh where gl ∈ Sl and gh ∈ Sh. Let Ng =
JPd=-d∕2 ∣g[k]∣1 2, then the ratio of a frequency component with respect to the whole frequency
spectrum is denoted by
T (讥 k]) = «
and the ratio of LFC is denoted by
Lu C _ X I讥k]∣2
T (g ∈ Sl) =工 N2 .
k = -kc	g
Setup In this paper, we consider a two-layer neural network f : Rd 7→ R
m
f(x,θ) =X	arσ(wrTx)	(1)
r=1
where σ is an activation function, a = (a1 , a2 ,..., am)T is an m-dimensional vector and wr’s are
d-dimensional weight vectors.
Van der Schaaf & van Hateren (1996) showed that natural images obey a power law in the frequency
domain. Therefore, to make our settings closer to widely studied image related tasks, we also assume
our input x obey the power law
∣X[k]∣ H k-α
to imitate LFC-ConCentrated images, where the constant ɑ ≥ 1 and ∣X[0]∣ = α° is another constant.
Besides, we choose the cut-off kc in such a way that
kc	(d-1)∕2
X k2α + α2 > X k2α，	⑵
k=1	k=kc +1
which indicates that T(X ∈ Sl) > T(X ∈ Sh) and can be easily satisfied for a small k「
Gradient-based adversarial attack For a loss function `, we find lp -norm -bounded adversarial
perturbations by solving the optimization problem
Ilmax '(f(x + δ),y),
kδkp≤
where (X, y ) ∈ Rd × R is d-dimensional input and output following the joint distribution D.
To solve the above optimization problem, Goodfellow et al. (2014) proposed FGSM, an attack for an
l∞-bounded adversary, and computed an adversarial example as XadV = X + ESgn (Vχ'(θ, x, y)).
Miyato et al. (2016) proposed FGM with an l2-bounded adversary, XadV = X + E [[£'((Xy))∣∣ . Madry
et al. (2017) introduced PGD which training adversarial examples in an iterative way that Xt+1 =
Πχ+S (Xt + α Sgn(Vx'(θ, X,y))) or Xt+1 = Πχ+s(Xt + α 八黑寝工 )
To simplify the analysis, we make the following assumptions on the normal training process:
Assumption 1 There exist β0, β > 0 such that β0 ≤ ∣∂'∕∂f ∣ ≤ β.
Assumption 2 If' = ɪ (y 一 f (x, θ))2, ∣∂'∕∂f ∣ should be a small quantity such that ∣∂'∕∂f ∣ =
E1+v if the network is well trained in the end stage of normal training, where 0 < v < 1.
Assumption 3 There exist λ, γ > 0 such that 0 ≤ σ0 ≤ λ and 0 ≤ σ00 ≤ γ1.
Assumption 4 ar ’s are i.i.d drawn from a distribution2 and will not be updated during the training.
1 e.g. λ = 1 and Y = 4 if σ = ln(1 + ex).
2e.g. Bernoulli distribution.
3
Under review as a conference paper at ICLR 2021
2.2 Exploring the log-spectrum and spectrum
Wang et al. (2020a) considered the log-spectrum of an image: 20 * log (|F(x)|), where X is a 2d-
dimensional matrix representing an image. One way to calculate the ”log-spectrum difference of
the adversarial examples” is to consider the difference between the log-spectrum of the adversarial
and natural image and as follows,
20 *∣log(∣F (X + δ)∣) - log(∣F (x)|)| ≈ 20 *log(1 + JF≡) ≈ 20 * JF≡.	(3)
|F(x)|	|F(x)|
We can observe that this quantity is approximately proportional to the ratio of the perturbation’s over
the natural image’s frequency. Moreover, the average Relative Change in discrete cosine Transforms
(RCT) in (Wang et al., 2020b)) approximately equals to it. Fig. 5 empirically shows that this ratio
is large for high frequency component but small for low frequency one. However, Van der Schaaf
& van Hateren (1996) claimed that the LFC of F(x) is much bigger than its HFC, and the right
side of Eq. 3 is inversely proportional to F (x). The denominator of the right side of Eq.(3) in
low-frequency part is much larger than it in high frequency part, so it may be imprudent to consider
the frequency distribution of the adversarial perturbations only based on the ratio in the right side of
Eq.(3).
Yin et al. (2019) provided another spectral measurement for adversarial examples. They considered
the “spectrum of the adversarial perturbation” |F(δ)∣, which directly reflects the distribution
of adversarial perturbations in the frequency domain. Unfortunately, they claimed that ‘The ad-
versarial perturbations for the normally trained model are uniformly distributed across frequency
components’ without sufficient empirical and theoretical justification. In this work, we provide a
totally different theoretical finding that the adversarial perturbations for the normally trained model
are gradually concentrated in the low-frequency domain, further verified by extensive experiments.
Fig. 1 visually shows the calculation methods of the spectrum of perturbations F(δ) and the log-
spectrum difference Ofadversarial examples | log(∣F(x + δ)∣) — log(∣F(x)| ≈
IF(δ)∣
IFxH.
3 Main Results
In Section 3.1, we show that, for a well
trained two-layer neural network described
in Eq.(1) with LFC-concentrated inputs, l2-
norm adversarial perturbations generated by
FGM (Theorem 1) prefer to concentrate in the
low frequency domain. Section 3.2 theoreti-
cally proves the HFC-concentration of the log-
spectrum difference of adversarial examples.
Section 3.3 attempts to demonstrate the effec-
tiveness of masking the HFC of the adversar-
ial examples to improve model’s robustness de-
spite that perturbations can concentrate in the
low frequency domain. All technical proofs are
deferred to Appendix A. Results on l2 PGD per-
turbations are presented in Appendix B.
In this work, we consider the two-layer neu-
ral networks and the loss function ` =
1 (y — f (x, θ)) . For any X in this setting, We
Image:
output(x) = 'bird’
X
+
J
IFWI
Spectrum:
Log-spectrum:
J
log(∣F(x)∣)
δ
J
∣F(δ∣
∣log(∣FG+δ)∣)-Iog(IF(X)I)I
→
一
output(x + δ) = 'frog'
x + δ
J
∣F(x + δ)∣
J
log(∣F(x+δ)∣)
have
∂f=σ(wTX)，	∂Wr=arχ,
Nxf = Ear Wr,
Figure 1: Visualization of spectrum and log-
spectrum for adversarial attacks . Section 3.1 fo-
cuses on the center picture F(δ) to prove that the
LFC ratio of F(δ) is increasing. Section 3.2 fo-
cuses on the third-line center picture | log(|F (x +
δ)∣) — log(∣F(x|)|, and shows that its LFC is
smaller than its HFC.
r
where a0r = arσ0(wrT X) and a0 = (a01, a02, ..., a0m). At the (t + 1)-th step of gradient descent in the
normal training process of the classifier, weight w is updated as
w,+1) = Wrt) — ηf	a；(t)x.
(4)
4
Under review as a conference paper at ICLR 2021
We use Lx and Hx to denote the LFC and HFC of the clean input
(d-1)/2
Lx = £|X[k]]2 andHx = E ∣X[k]∣2.
(5)
k=0
k=kc +1
3.1 SPECTRAL TRAJECTORIES OF l2-NORM FGM PERTURBATIONS
Consider the l2-norm FGM perturbation δ and its DFT δ[k]
d' ▽ f	]
δ = e ∂f , δ[k] =e sgn	f [k],
k ∂f Vxf k ,	[] g	gxfk f[]，
(6)
一	rZ~^ …,	__________、……	_	_ _ . 一 _	rZ~^ 一,............. _	__	_
where Vf [k] = F (Vxf) [k]. Since the coefficient before Vf [k] in (6) is same for different frequen-
cies k, we only need to analyze the spectrum of Vf[k] to investigate the ratio of certain frequency
component over the whole frequency spectrum, i.e.
T (δ[k]) = T (Vf [k]) .	(7)
In this subsection, we explore the evolving of T(δ [k]) along the normal training process. For a
randomly initialized network, the FGM-attack perturbation will not have bias towards either HFC or
LFC. We attack this model with FGM at each step of the normal training to study the trajectory of
these perturbations in the frequency domain. According to Eq.(4), Vxf is updated to the order ofη
at the (t + 1)-th step as
m
Vxf (M = X(I-Mt))ar㈤Wrt)- η⑴X + O(η2),
r=1
where ηrt = ηarkx∣∣2 f (t)σ00(t)(wrt)Tx) and η(t) = η∂f (t)ka0⑴k2 are composite learning rates
for easing our notation. For ReLU activation function σ(x) = max(0, x), we have that ηr(t) =
Ei∈{1,2,...,m} [ηi(t)], therefore, considering that Softplus has similar shape as ReLU, we derive that
η(t) = η(t) + O(η2), where η(t) = η∣∣xk2 f (t)Er∈{i,2,...,m} Iarσ00㈤(Wrt)Tx)]. In this way, the
1d-DFT of Vx f (t+1) has the following form,
Vf(t+1)[k] = (1 - η⑴)Vf (t)[k] - η㈤X[k].	(8)
(t)
We are now ready to study the trajectory of T (δ[k]) along the training step t. Let 4久)denote the
(t)
difference of phases between Vf [k] and X[k] for frequency k at the t-th step. And for ease of
notation, denote
kc
4τ(t)，-2 X |Vf ( )[k]∣∣X[k]∣ cos (△/t)),
k=0
(d-1)/2
△$，-2 X |Vf(t)[k]∣∣X[k]∣ cos (△/t)),
k=kc +1
where η(t)4τz(t) and η(t)4τht)are approximately changed amounts for LFC and HFC of |Vff (t) |2
at the t-th step of the normal training of the network, respectively. Since the network is randomly
initialized and the clean inputs x are highly concentrated in the low frequency domain, we always
study the case where T(δ(0) ∈ Sl) < T(X ∈ Si). We now, taking Eq.(7) into consideration , present
our main theorem about l2 norm FGM perturbations as follows.
Theorem 1 (The spectral trajectory of l2 FGM perturbation) During the training process of the
two-layer neural network f(x) in (1), the l2 norm FGM adversarial perturbation will change its
ratio of LFC, T(δ ∈ Si), at the (t + 1)-th step as follows,
t(δ(t+1) ∈ Si) = t(δ⑴ ∈ Si) + η㈤
4Tι(t)T (δ⑴ ∈ Sh) - 4τht)τ (δ⑴ ∈ Si)
Pk=o1"2∣Vf (t+1)[k]∣2
;
there will be a t1 s.t. ∀t ≥ t1, we have
τ(δ(t+1) ∈ Si) > τ(δ⑴ ∈ Si).
(9)
(10)
5
Under review as a conference paper at ICLR 2021
Remark According to theorem 1, during the normal training process for a randomly initialized
network with LFC-concentrated input, there will be a t0 s.t. ∀t ≥ t0, we have 4τl(t) > 4τh(t),
and T (δ(t+1)	∈ Sl) > τ (δ⑴ ∈	Sl)	if T	(δ㈤ ∈	Sl)	< τ (δ⑴	∈ Shy For ReLU activa-
tion function,	starting With T(δ(0) ∈	Sl)	= 1/2	- Z	and 4τl(0) = 4τh0) + bZ, We have
t0 = max {θ, — nηβo(Lx-Hx) }, where n = mint∈[o,t0] ∣∣a0⑴k2. Besides, given SUCh a net-
Work trained With at least t0 steps, there Will be a tι > to such that 4τl(t1)τ (δ(t1) ∈ Sh) ≥
4τh(t1 τ(S(tl) ∈ Sl) and FGM perturbation will increase its ratio of LFC for all t > t1 no matter
whether it is smaller than 1/2. Finally, if T(S(t2)∈ Sl) > T(S(t2)∈ Sh) for some t2 > to, this
relation will hold for any t ≥ t2 .
3.2	High-frequency concentration for log-spectrum difference of
ADVERSARIAL EXAMPLES
In this subsection, we explore spectral trajectories of log-spectrum difference ofl2 FGM adversarial
examples
∣f⅛]∣ X 也]∣
k =	∣x[k]∣	∣x[k]∣
along normal training step t as in Section 3.1 to study their HFC-concentration phenomenon. Since
(o)
|Vf [k]| for a randomly initialized network (1) has no bias towards high frequency or low fre-
quency while |X| χ k-α, we adopt the following setting
Rk(o) X kα
without loss of generality and present below our result on the log-spectrum difference of the adver-
sarial examples.
Theorem 2 (Concentration of the log spectrum difference) For l2 FGM perturbations of the net-
work in (1), for any k > k0 > 0 which satisfies that R(ko) > Rk(o0), there exists a t0 such that
R(kt) > Rk(t0) for all t < t0 steps of the normal training. For ReLU activation function, we have
t0 = RkO) cos(4PkO)) - RkO) eos(4/O))
=	ηβkak2
(11)
while if the initialization satisfies that R(kO) cos(4P(kO)) ≤ R(kO0) cos(4P(kO0)) then t0 = ∞.
The theorem shows that although δ itself can concentrate in the low frequency domain, it does
not concentrate as dense in the low frequency domain as the inputs X which obey the power law.
Therefore, the log-spectrum difference (i.e. equivalent to spectrum of the ratio of the perturbation
to the clean data) of adversarial examples will express a HFC-concentrated phenomenon instead
(Fig.1).
3.3	Masking HFC of the adversarial examples to improve robustness
In this subsection, we compare the ratio of LFC of the adversarial perturbations T(δ ∈ Sl ) and the
ratio of LFC of original images T(X ∈ Sl).
Theorem 3 (Comparison on the LFC ratio of clean data and perturbations) For l2 FGM per-
turbation of the two-layer neural network in (1), at the t-th step of normal training, let Z = T (X ∈
Sl) — t(δ(0) ∈ Sl) > 0, then we will have T(δ(t) ∈ Sl) ‹ T(X ∈ Sl) forall t > 0 ifthe initilization
of the netowrk satisfies that
4τl(0) < τ(X ∈ Sl)(4Tl(O) + 4Tm.	(12)
6
Under review as a conference paper at ICLR 2021
Remark When both 4τl(0) and 4τh(0) are positive, condition (12) states that the ratio of low fre-
quency component of 4τl(0) + 4τh(0) is smaller than that of clean data, which can be easily satisfied
for a randomly initialized network. The ratio of LFC of the perturbations δ will be less than that of
clean data x if the condition Eq.(12) is satisfied for l2 FGM perturbations during the normal train-
ing process. Masking the HFC of an adversarial example x + δ will then erase more amount of
the perturbation than that of the clean data which leads to a new ”not-perturbed-much” adversarial
example. As a result, one can expect the improvement for robustness of the model, as demonstrated
in Section 4.3 later.
4 Experiment
In this section, we conduct several experiments to validate our theoretical conclusions.
Setup. We use the Resnet-32 (He et al., 2016), a fully connected network as the last layer and cross
entropy loss, and train all the parameters on the CIFAR-10 dataset with Adam optimizer (Kingma
& Ba, 2014). We use PGD-attack with = 8/255, 40 iterations and step size ξ = 4/255.
We supplement more experiments in Appendix C to fill the gap between our theory and more com-
plicated settings: we use the MSE loss and keep other conditions unchanged to resolve the discrep-
ancy of loss function; experiments to support our theorems on two-layer neural network with fixed
outer-layer parameters are also provided.
4.1	The increase of LFC concentration
In this part, we empirically verify the theorem in Section 3.1, to show that adversarial perturbations
gradually concentrate more in the low-frequency part of the spectrum during the training process.
Consider a neural network with random initialization, each time before updating all the parameters
of neural network, we randomly sample 100 images and apply the l2 FGM /PGD attack to them. For
each iteration in PGD attack, we calculate their ratios of LFC and plot them in Fig. 2. It shows that
the LFC ratios of the perturbations τ (δ ∈ Sl ) are gradually increasing along the normal training.
(a)
(b)
Figure 2: The LFC ratio of the adversarial perturbations in the training process. (a) The attack
method is l2 FGM. (b) The attack method is l2 PGD
above images.
7
Under review as a conference paper at ICLR 2021
For a sufficiently trained neural network, we choose an im-
age, use PGD-attack to obtain its adversarial examples and
perturbations, and calculate its spectrum. We randomly se-
lect 10 images in the test set and show the original images
and the spectrum of perturbations in Fig. 3. We also ran-
domly select 1000 images in the test set and average their
spectrum to show the expectation of the spectrum in Fig.
4(a). They show that the spectrum of adversarial perturba-
tions are more concentrated in the low-frequency domain
after a sufficient training. Ortiz-Jimenez et al. (2020) show
that the distance to the boundary (margin) in different fre-
quency bands is heavily dependent of the distribution used
for training, which also supports our claims.
(a)	(b)
Figure 4: The expectation of: (a) the
spectrum of adversarial perturbations;
(b) the log-spectrum difference of ad-
versarial examples.
4.2 HFC concentration of log-spectrum
DIFFERENCE
We first use PGD-attack to get the adversarial example of a image for a sufficiently trained neural
network. We then make the log-spectrum of the original image and its adversarial example and
calculate the difference of their log-spectrum (Fig.5). Fig.4(b) displays the expectation of the spec-
trum for 1000 random images in the test set. They both show that the log-spectrum difference of
adversarial examples is generally concentrated around.
is the original example, the adversarial example attacked by PGD in a normally trained Resnet-32
and the perturbation. The second line is the log-spectrum of the original example, the log-spectrum
of the adversarial example and the difference of the two log-spectrum.
4.3	Low-pass filter improves robustness
In this part, we empirically show how a low-pass filter (which filters out HFC of the inputs) can
improve the robustness of the model with a cheat sheet in the sense that the attacker does not know
the existence of the filter while the model is aware of the frequency distribution of the gradient-based
adversarial examples.
The procedure of our low-pass filter basically has three steps: performing DFT on the inputs, then
masking their high frequency components (set as 0) in the frequency domain and finally performing
inverse DFT on them to give the desired inputs with only low frequency components preserved.
We evaluate two types of accuracy. One is filtering the clean images out of high frequencies and
feed them into the model to test accuracy, named as ”no-attack accuracy”. The other is firstly
PGD-attacking the input images in the normally trained model, and then check the test accuracy of
filtered adversarial images, called “PGD-attack accuracy”.
Here, we use data augmentation to train the model to the best. We show the results in Fig. 6. As
the cut-off increases, the no-attack accuracy gradually increases, and the PGD-attack accuracy first
increases and then decreases. The highest robustness achieves 51.7% when the cut-off k = 12 and
its robustness accuracy is 81.7%. And the normal accuracy (no filter, no attack) is 93.03%. We
also adversarially train the model with PGD-attacks. Its normal accuracy is 79% and PGD-attack
8
Under review as a conference paper at ICLR 2021
accuracy is 48% without the filtering. Our low-pass filter model not only performs better on both
no-attack accuracy and PGD-attack accuracy, but also does not need computationally expensive
adversarial training.
Ooooo
0 8 6 4 2
1
ssunqoAue,Jr□ue
—no attack
→- PGD attack
no attack accuracy = 81.81%
k=12
PGD attack accuracy = 51.7%
O	5	IO	15
20
25
cut-off
Figure 6: Normally trained model’s performance and robustness after a low-pass filter with cut-off
from 1 to 22.
Theorem 3 provides a reasonable explanation for the above results. When the cut-off is low, the
filter masks most of the original image and the adversarial perturbation, so both of the no-attack and
PGD-attack models perform not well. When the cut-off is proper, there remains enough information
of original images but little of perturbations, so the PGD attacker cannot fool the model thoroughly.
Therefore, with a suitable cut-off, the low-pass filter can indeed improve the model’s robustness
without computationally expensive adversarial training.
5 Conclusion
We investigate the adversarial examples through the lens of frequency analysis. Our work both
theoretically and empirically clarifies the definition of log-spectrum difference of the adversarial
examples in existing literature. Besides, we devote to understanding the spectral trajectories of ad-
versarial examples: l2 FGM perturbations gradually increase the concentration in the low-frequency
part of the spectrum but their ratios of LFC will never exceed that of clean data during the training
process over model parameters. Inspired by these findings, we find that a low-pass filter can improve
the robustness of the model and give a reasonable explanation for this phenomenon.
Future work can focus on analysis of other perturbations in the frequency domain such as FGSM
and l∞ PGD perturbations or providing theoretical understanding of the phenomenon that the ratio
of the LFC of the perturbation for adversarially trained model is much higher than that for normally
trained model (Appendix D).
References
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
9
Under review as a conference paper at ICLR 2021
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp.125-136, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-
supervised text classification. arXiv preprint arXiv:1605.07725, 2016.
Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
Frossard. Hold me tight! influence of discriminative features on deep network boundaries. arXiv
preprint arXiv:2002.06349, 2020.
Yash Sharma, Gavin Weiguang Ding, and Marcus A Brubaker. On the effectiveness of low frequency
perturbations. In Proceedings of the 28th International Joint Conference on Artificial Intelligence,
pp. 3389-3396. AAAI Press, 2019.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep
neural networks. IEEE Transactions on Evolutionary Computation, 23(5):828-841, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
van A Van der Schaaf and JH van van Hateren. Modelling the power spectra of natural images:
statistics and information. Vision research, 36(17):2759-2770, 1996.
Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain
the generalization of convolutional neural networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 8684-8694, 2020a.
Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, and Zihao Ding. Towards frequency-based
explanation for robust cnn. arXiv preprint arXiv:2005.03141, 2020b.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adver-
sarial examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.
Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier per-
spective on model robustness in computer vision. In Advances in Neural Information Processing
Systems, pp. 13276-13286, 2019.
10
Under review as a conference paper at ICLR 2021
A Proofs
A.1 Proof of theorem 1
According to (8), the update of τ (δ[k]) for k-th frequency component of l2 norm FGM adversarial
perturbation at the (t + 1)-th step is proportional to
(1 - 2η(t))∣ff㈤[k]∣2 - 2η(e)f ㈤[矶x[k]∣ cos(4, .	(13)
We adopt the following representations to see trends of cos
（4痔）
through the training process:
(t)
Vf [k] : = -→ = (|c(t)| cos θ, |c(t)| Sm θ)
x[k] ： = -→ = (∖x(k)∖ cos ζ, ∣X(k)∣ Sin Z),
then ∖Vf('+0[k]∖∖X[k]∖cos G於，)
will have the form of
<--→,-→> =(i - η⑴)<-→,-→> - η㈤ma2 + o(η2)
at the (t + 1)-th step. At the (t + 1)-th step, we have
4τι(t+1) - 4τht+1) = (1 - ηW(4τ(t) - 4τht)) + n(t)(Lx - Hχ),	(14)
which means that if 4τl(t) > 4τh(t) then we must have 4τl(t0 ) > 4τh(t0 ) for all t0 ≥ t. Besides,
there must be a step which leads to this condition since Lx > Hx . Let L(t) = Pkkc=0 ∖Vff [k]∖2 and
H(t) = Pkd=k1+2 ∖Vf [k]∖2. At the (t + 1)-th step, the changed amount of T(δ ∈ Sl) is
(1 — 2η(t))L(t) + η(t)4τl(t)
(1 - 2η⑴)(L⑴ + H⑴)+ η㈤(4τ(t) + 4Tht))
L㈤
L⑴+ H⑴
=η㈤
4τ(tτ (δ⑴ ∈ Sh) - 4τhtτ (δ⑴ ∈ Sl)
Pkd-O1)/2 ∖ff(t+1)[k]∖2
(15)
If t > t0 and T δ (t) ∈
> T δ (t) ∈ Sl then the above changed amount will be pos-
itive and gradient descent will increase T (δ ∈ Sl) at this step. If 4τl(t)τ(S(t) ∈
<
4τhtτ(S(t) ∈ Sl), then gradient descent will increase T(S(t) ∈ Sh) Untill the step tι at which
4τl(t1)τ(S(t1) ∈ Sh) ≥ 4τht1)τ(S(t1) ∈ Sl), then for any t > tι, We have T(S(t+1) ∈ Sl) >
T δ(t) ∈
A.2 Proof of theorem 2
We provide the proof for ReLU activation function for positive eta, the case for negative η is similar.
The update rules of Rk at the (t + 1)-th step are
Rkt+1)2 = RkR- 2η(t)Rkt cos(4Pk)
Rkt) cos(4点)) = RktT) cos (△PktT))- η(t-1).
For any k > k0 > 0 with R(k0) > R(k00), we have
Rkt+1)2 - Rkto+1)2 = Rkt)2 - Rkto)2 - 2η(t) [Rf) cos(4p黑)-Rk cos(4@般)]
t
=Rk0)2 - RkO)2 - 2 X η(t0) [Rk0) cos(4pk。)) - RkO) cos (4pk0))i
t0 =0
11
Under review as a conference paper at ICLR 2021
at the (t + 1)-th step. If R(0 cos(4夕k0)) - R(0 cos(4夕k0)) > 0, considering the case η(t0) =
ηmax for any t0 and the condition that the left L.H.S of the above equation is larger than 0 gives Us
Eq.(11). On the other hand, if R，) cos(4夕：0)) - Rk0) cos(4夕k0)) < 0, then Rkt) > R：? for all
t ≥ 0.
A.3 Proof of theorem 3
We consider the case for l2 FGM perturbations with η > 0, the case for negative η is similar. Let
the initialization of the network always satisfy the condition
L(0)
L(O) + H ⑼
=T(δ(0) ∈ SI) < T(X ∈ Sl) = L LxH
Lx + Hx
(16)
such that τ(X ∈ Sl) - T(δ(0) ∈ Sl) = Z > 0. Then at the (t)-th step, We can express T(δ(0) ∈ Sl)
as follows
L⑴	=____________L(O) + Pt-10 η(t0)4τl(t0)_________
L㈤+ H ⑴—L⑼+ H ⑼ + Pt-IO η(t0)(4τht0) + 4Ts 0)
=______________L(O) + Pt-IO η(t0)(P"o η(t00Lχ + 4Tl(0))________________
L(。) + H ⑼ + Pt-IO FQ= H(Lx + Hx) + 4Tl(O) + 4丁仔)
Let
t-1	t	t-1	t	t-1	t-1
a = X η(t0)X η(t00)Lχ, b = X n(t0)X η(t00)Hx, C = X n(t0)4Tl(O) ,d = X η(t0 )△记,
t0=O	t00=O	t0=O	t00 =O	t0=O	t0=O
then
a	c	4Tl(O)
a+b =T (X ∈ Sl),c+d = 4TWΓ4TW
and
L(t)	L(O) + a + c
L(t) + H(t) = L(O) + H(O) + a + b + C + d
T(X ∈ Sl)
1+
C . L(O)
a + 丁~
1 I L(O)+H(O)	c+d .
1 +	a+b	+ a+b
If
C
L(O) + H(O)
C	L(O)
a + a
+ t(S(O)∈ Sl)
4Tl(O) — τ(X ∈ Sl )(4Tl(O) + 4τh0))
L(O) + H(O)	C+d
< -----------1------
a+b	a+b
<	T(X ∈ Sl) (1 + L(O) + H(O))
<	ζ L(O) + H(O)
then we must have
T(δ(t) ∈ Sl) < T(X ∈ Sl)
(17)
at the t-th step. A stronger condition is that
4Tl(O) < T(X ∈ Sl )(4Tl(O)+ 4ThO))
then we will have T(δ(t) ∈ Sl) < T(X ∈ Sl) for all t. This condition approximately states that the
”ratio” of low frequency component of 4Tl(O) + 4Th(O) is smaller than that of clean data.
12
Under review as a conference paper at ICLR 2021
B l2 PGD PERTURBATIONS
The PGD update rule for finding perturbations of x with learning rate ξ at step j + 1 is:
δ(j+1) = PB(0,)
∂' (j)
δ⑶ + ξd' Vxf (j)
(18)
where B(0, ) is a ball centered at 0 with radius in Euclidean space and P is the projection operator
defined as
PB(0,) [δ] = argmin kδ0 - δk2.
δ0∈B(0,)
Note that in this part quantities removing the step script j (e.g. f and Vxf) refers to not involving
perturbations. For convenience, we use the same learning rate ξ for all steps j and instead explore
the update of
κ(j)=叱
ξ
(19)
to the order of δ to see trends of PGD perturbations in frequency domain alongside PGD iterations
since T(δ[k]) = T(κ[k]). For ease of notation, we adopt the following representations: Let 4φj)
denote the difference of phases between Vf [k] and K (j)[k] ; let βj = ∂'∕∂f + δ(j) ∙ Vxf where
β(0 > 0 and one can drive similar results for ∕3(0) < 0; We denote
kc	(d-1)/2
4τj，2 ^X |K(j)[k]||Vf [k]| cos (4φj)) and 4τj，2 ^X |K(j)[k]||Vf [k]| cos (4φj)),
k=0	k=kc +1
where B(j)4τj and B(j)4τj are changed amounts of LFC and HFC of ∣κ∣2 at the (j + 1)-
th step of PGD iteration. We provide below our results on frequency spectrum of l2-norm PGD
perturbations.
Theorem 4 (The spectral trajectory of l2 PGD perturbation) At the (j + 1)-th step of PGD, it-
eration of PGD will change the ratio of LFC of l2 norm PGD adversarial perturbation for a neural
network(1) which satisfies ∣∂'∕∂f | = e1+ν with 0 <ν< 1 asfollows,
τ(δ ∈ Sl) 一 τ(δ ∈ Si) + β⑶
△铲T Gj) ∈ Sh) - △铲T (δ⑺ ∈ Sl)
(20)
Remark If the two-layer neural network in (1) is trained with at least t ≥ t1 steps (t1 determined
by theorem 1) such that T(Vf ∈ Sl) > T(Vf ∈ Sh ), then, according to theorem 4, there exists
j0
_________△铲-△铲
B (pk=kc∣ff [k]∣2 - Pk：kd-1)/2 ∣Vf [k]∣2)
(21)
where B = maXi∈[o,jo]Bs SUCh that T(δ(j+1) ∈ Sl) > T(δ(j) ∈ Sl) for all j > jo if T(δ(j) ∈
Sl) <t(δ⑶ ∈ sr：
B.1 Proof of Theorem 4
At the (j + 1)-th step of PGD update for κ(j+1), if
1. PB(j()0,) = I:
K(j+1) = Kj) + ∂∂fVxf + ∂∂f X arσ00δ⑶∙ W：,rW：,r + δ⑶∙ VxfVxf + O(δ2);
13
Under review as a conference paper at ICLR 2021
2 P (j)	6= I :
2. PB(0,) 6= I:
κ(j+1) =__________________
= IlKj) + Vx '() k
κ(j) + f O)
In either case, the quantity T(κ[k]) is proportional to
F(K(j) + ∂f(j)Vχf⑶!	= F (Kj) + If Vxf + δ⑺∙ VxfVxf) P
since the term δ∂'∕∂f < δ2 and can be dropped. Therefore, We now consider
K (j+1) [k ] = K(j) [k ]+ f + δ(j) ∙ VX f) Vf [k ]	(22)
at the (j + 1)-th step of PGD in the frequency domain to explore ratio of frequency k to the whole
frequency spectrum of perturbations
T (δj+1)[k]) Z	|K(j)[k]∣2 + 2	(β0	+ δ(j)	∙	VXf)	|K(j)[k]||Vf [k]| cos(4φj)).	(23)
Lemma 1 (Dynamics of l2 Norm PGD in the Frequency Domain) If the initialization of l2 norm
PGD adversarial perturbation satisfies f + δ(0) ∙ VX f > 0, then it will be positive at every iteration
of PGD3.
Similar to Eq.(15), one can derive the changed amount of T(δ ∈ Sl) in theorem 4 at the (j + 1)-th
step and find the necessary condition.
C Supplementary experiment
In this part, we present experiments on models with MSE loss and two-layer neural networks with
fixed outer-layer parameters in Section 4.
C.1 MSE LOSS
We show the supplementary experiment of Section 4.1 in Fig. 7 and Fig. 8. They have similar results
to show that the spectrum of adversarial perturbations are more concentrated in the low-frequency
domain after a sufficient training.
Figure 7: The difference of the spectrum between original and adversarial examples. The model are
trained with MSE loss.
Then we show the supplementary experiment of Section 4.2 in Fig. 9 and Fig. 10. They have
similar results to show the log-spectrum difference of adversarial examples is generally concentrated
around.
In general, there is little empirical gap between MSE loss and CrossEntropy loss.
3A similar conclusion exists when ∂∂f + δ(0) ∙ Nx f < 0.
14
Under review as a conference paper at ICLR 2021
Figure 8:	The expectation of the spectrum of adversarial perturbations. The model are trained with
MSE loss.
Figure 9: The difference of the log-spectrum between original and adversarial examples. The model
are trained with MSE loss.
Figure 10:	The expectation of the log-spectrum of adversarial perturbations. The model are trained
with MSE loss.
C.2 Two-layer neural network
We use two-layer neural networks with fixed outer-layer parameters. The input layer is of size
3 * 32 * 32 followed by a ReLU activation function, and the hidden layer is of size 500 followed by
a sigmoid activation function. The dataset is still CIFAR10. We use random initialization and SGD
optimizer without any training tricks. This setting can be used in practice and get higher than 40%
test accuracy. The attack method is the same as the setup in Section 4.
We show the LFC ratio of the adversarial perturbations during the training process in Fig.11. After
sufficient training, we show the expectation of the spectrum in Fig.12(a) and the expectation of the
log-spectrum difference in Fig.12(b).
(-a⅛k
O 250	500	750 IOOO 1250 1500 1750 2000	O 250	500	750 IOOO 1250 1500 1750 2000
iteration	iteration
(a)	(b)
Figure 11:	The LFC ratio of the adversarial perturbations for two-layer neural networks in the
training process. (a) The attack method is l2 FGM; (b) The attack method is l2 PGD.
15
Under review as a conference paper at ICLR 2021
(a)	(b)
Figure 12: For two-layer neural network, the expectation of: (a) the spectrum of adversarial pertur-
bations; (Due to the large value gap between LFC and HFC, it is hard to see the trend in original
result, so we show the logarithm of the result which will not change the size relationship.) (b) the
log-spectrum difference of adversarial examples.
D perturbations of PGD-attack adversarial trained model are
more concentrated in LFC
We do use the same setup in Section 4, and the PGD-attack adversarial training is also PGD-attack
with = 8/255, 40 iterations and step size ξ = 4/255 in each training step. Then we test the ratio
of LFC of original images, perturbations in a normally trained model and perturbations in a PGD-
attack adversarially trained model. The results are shown in Fig. 13. It shows that the LFC ratio
of perturbations in PGD-attack adversarially trained model is generally higher than that in normally
trained model.
5359)r JQs31^°
----original images
—— perturbations in normally trained model
---- perturbations in PGOattack adversarially trained model
Figure 13: the ratio of LFC of original images, perturbations in a normally trained model and per-
turbations in a PGD-attack adversarially trained model
16