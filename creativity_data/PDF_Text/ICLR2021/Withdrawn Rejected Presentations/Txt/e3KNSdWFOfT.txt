Under review as a conference paper at ICLR 2021
Solving Min-Max Optimization with Hidden
Structure via Gradient Descent Ascent
Anonymous authors
Paper under double-blind review
Ab stract
Many recent AI architectures are inspired by zero-sum games, however, the behav-
ior of their dynamics is still not well understood. Inspired by this, we study stan-
dard gradient descent ascent (GDA) dynamics in a specific class of non-convex
non-concave zero-sum games, that we call hidden zero-sum games. In this class,
players control the inputs of smooth but possibly non-linear functions whose out-
puts are being applied as inputs to a convex-concave game. Unlike general zero-
sum games, these games have a well-defined notion of solution; outcomes that
implement the von-Neumann equilibrium of the “hidden” convex-concave game.
We prove that if the hidden game is strictly convex-concave then vanilla GDA
converges not merely to local Nash, but typically to the von-Neumann solution. If
the game lacks strict convexity properties, GDA may fail to converge to any equi-
librium, however, by applying standard regularization techniques we can prove
convergence to a von-Neumann solution of a slightly perturbed zero-sum game.
Our convergence guarantees are non-local, which as far as we know is a first-
of-its-kind type of result in non-convex non-concave games. Finally, we discuss
connections of our framework with generative adversarial networks.
1 Introduction
Traditionally, our understanding of convex-concave games revolves around von Neumann’s cele-
brated minimax theorem, which implies the existence of saddle point solutions with a uniquely
defined value. Although many learning algorithms are known to be able to compute such saddle
points (Cesa-Bianchi & Lugoisi, 2006), recently there has there has been a fervor of activity in
proving stronger results such as faster regret minimization rates or analysis of the day-to-day behav-
ior (Mertikopoulos et al., 2018; Daskalakis et al., 2018; Bailey & Piliouras, 2018; Abernethy et al.,
2018; Wang & Abernethy, 2018; Daskalakis & Panageas, 2019; Abernethy et al., 2019; Mertikopou-
los et al., 2019; Bailey & Piliouras, 2019; Gidel et al., 2019; Zhang &Yu, 2019; Hsieh et al., 2019;
Bailey et al., 2020; Mokhtari et al., 2020; Hsieh et al., 2020; Perolat et al., 2020).
This interest has been largely triggered by the impressive successes of AI architectures inspired
by min-max games such as Generative Adversarial Networks (GANS) (Goodfellow et al., 2014a),
adversarial training (Madry et al., 2018) and reinforcement learning self-play in games (Silver et al.,
2017). Critically, however, all these applications are based upon non-convex non-concave games,
our understanding of which is still nascent. Nevertheless, some important early work in the area has
focused on identifying new solution concepts that are widely applicable in general min-max games,
such as (local/differential) Nash equilibrium (Adolphs et al., 2019; Mazumdar & Ratliff, 2019),
local minmax (Daskalakis & Panageas, 2018), local minimax (Jin et al., 2019), (local/differential)
Stackleberg equilibrium (Fiez et al., 2020), local robust point (Zhang et al., 2020). The plethora
of solutions concepts is perhaps suggestive that “solving” general min-max games unequivocally
may be too ambitious a task. Attraction to spurious fixed points (Daskalakis & Panageas, 2018),
cycles (Vlatakis-Gkaragkounis et al., 2019), robustly chaotic behavior (Cheung & Piliouras, 2019;
Cheung & Piliouras, 2020) and computational hardness issues (Daskalakis et al., 2020) all suggest
that general min-max games might inherently involve messy, unpredictable and complex behavior.
Are there rich classes of non-convex non-concave games with an effectively unique game theo-
retic solution that is selected by standard optimization dynamics (e.g. gradient descent)?
1
Under review as a conference paper at ICLR 2021
Our class of games. We will define a general class of min-max optimization problems, where
each agent selects its own vectors of parameters which are then processed separately by smooth
functions. Each agent receives their respective payoff after entering the outputs of the processed
decision vectors as inputs to a standard convex-concave game. Formally, there exist functions F :
RN → X ⊂ Rn and G : RM → Y ⊂ Rm and a continuous convex-concave function L : X × Y →
R, such that the min-max game is
min max L(F(θ), G(φ)).	(Hidden Convex-Concave (HCC))
We call this class of min-max problems Hidden Convex-Concave Games. It generalizes the recently
defined hidden bilinear games of Vlatakis-Gkaragkounis et al. (2019).
Our solution concept. Out of all the local Nash equilibria of HCC games, there exists a special
subclass, the vectors (θ*,φ*) that implement the Von Neumann solution of the convex-concave
game. This solution has a strong and intuitive game theoretic justification. Indeed, it is stable even
if the agents could perform arbitrary deviations directly on the output spaces X, Y . These parameter
combinations (θ*,φ*) “solve” the “hidden" convex-concave L and thus We call them von Neumann
solutions. Naturally, HCCs will typically have numerous local saddle/Nash equilibria/fixed points
that do not satisfy this property. Instead, they correspond to stationary points of the F, G Where
their output is stuck, e.g., due to an unfortunate initialization. At these points the agents may be
receiving payoffs Which can be arbitrarily smaller/larger than the game theoretic value of game L.
Fortunately, We shoW that Gradient Descent Ascent (GDA) strongly favors von Neumann solutions
over generic fixed points.
Our results. In this Work, We study the behavior of continuous GDA dynamics for the class of
HCC games Where each coordinate of F, G is controlled by disjoint sets of variables. In a nutshell,
We shoW that GDA trajectories stabilize around or converge to the corresponding von Neumann
solutions of the hidden game. Despite restricting our attention to a subset of HCC games, our
analysis has to overcome unique hurdles not shared by standard convex concave games.
Challenges of HCC games. In convex-concave games, deriving the stability of the von Neumann so-
lutions relies on the Euclidean distance from the equilibrium being a Lyapunov function. In contrast,
in HCC games Where optimization happens in the parameter space of θ, φ, the non-linear nature of
F, G distorts the convex-concave landscape in the output space. Thus, the Euclidean distance Will
not be in general a Lyapunov function. Moreover, the existence of any Lyapunov function for the
trajectories in the output space ofF, G does not translate to a Well-defined function in the parameter
space (unless F, G are trivial, invertible maps). Worse yet, even if L has a unique solution in the
output space, this solution could be implemented by multiple equilibria in the parameter space and
thus each of them can not be individually globally attracting. Clearly any transfer of stability or
convergence properties from the output to the parameter space needs to be initialization dependent.
Lyapunov Stability. Our first step is to construct an initialization-dependent Lyapunov function that
accounts for the curvature induced by the operators F and G (Lemma 2). Leveraging a potentially
infinite number of initialization-dependent Lyapunov functions in Theorem 4 We prove that under
mild assumptions the outputs of F, G stabilize around the von Neumann solution of L.
Convergence. Mirroring convex concave games, We require strict convexity or concavity ofL to pro-
vide convergence guarantees to von Neumann solutions (Theorem 5). Barring initializations Where
von Neumann solutions are not reachable due to the limitations imposed by F and G, the set of von
Neumann solutions are globally asymptotically stable (Corollary 1). Even in non-strict HCC games,
We can add regularization terms to make L strictly convex concave. Small amounts of regulariza-
tion alloWs for convergence Without significantly perturbing the von Neumann solution (Theorem 6)
While increasing regularization enables exponentially faster convergence rates (Theorem 7).
Organization. In Section 2 We provide some preliminary notation, the definition of our model and
some useful technical lemmas. Section 3 is devoted to the presentation of our the main results. Sec-
tion 4 discusses applications of our frameWork to specific GAN formulations. Section 5 concludes
our Work With a discussion of future directions and challenges. We defer the full proofs of our results
as Well as further discussion on applications to the Appendix.
2
Under review as a conference paper at ICLR 2021
2 Preliminaries
2.1	Notation
Vectors are denoted in boldface x, y unless otherwise indicated are considered as column vectors.
We use ∣∣∙k corresponds to denote the '2-norm. For a function f : Rd → R We use Vf to denote
its gradient. For functions of two vector arguments, f (x, y) : Rd1 X Rd2 → R , we use Vχf, Vyf
to denote its partial gradient. For the time derivative We Will use the dot accent abbreviation, i.e.,
X = ddt [x(t)]. A function f will belong to Cr if it is r times continuously differentiable. The term
“sigmoid” function refers to σ : R → R such that σ(x) = (1 + e-x)-1.
2.2	Hidden Convex Concave Games
We will begin our discussion by defining the notion of convex concave functions as well as strictly
convex concave functions. Note that our definition of strictly convex concave functions is a superset
of strictly convex strictly concave functions that are usually studied in the literature.
Definition 1. L : Rn × Rm → R is convex concave if for every y ∈ Rn L(∙, y) is convex and
for every X ∈ Rm L(x, ∙) is concave. Function L will be called strictly convex concave if it is
convex concave andfor every X × y ∈ Rn × Rm either L(∙, y) is StriCtly convex or L(x, ∙) is StriCtIy
concave.
At the center of our definition of HCC games is a convex concave utility function L. Additionally,
each player of the game is equipped with a set of operator functions. The minimization player
is equipped with n functions fi : Rni → R while the maximization player is equipped with m
functions gj : Rmj → R. We will assume in the rest of our discussion that fi , gj , L are all C2
functions. The inputs θi ∈ Rni and φj ∈ Rmj are grouped in two vectors
θ =[θl	θ	…θn]>	F(θ)	=	[fl(θl)	f2(θ2)…fN (θn)]>
φ = [φ1	φ2	…	φm]>	G(O)	=	[gl(φ1)	g2(φ2)	…gM (φm)]>
We are ready to define the hidden convex concave game
(θ*,φ*) = arg min arg max L(F(θ), G(φ)).
θ∈RN	φ∈RM
where N = Pin=1 ni and M = Pjm=1 mj . Given a convex concave function L, all stationary points
of L are (global) Nash equilibria of the min-max game. We will call the set of all equilibria of L,
von Neumann solutions of L and denote them by Solution(L). Unfortunately, Solution(L) can be
empty for games defined over the entire Rn × Rm . For games defined over convex compact sets,
the existence of at least one solution is guaranteed by von Neumann’s minimax theorem. Our defi-
nition of HCC games can capture games on restricted domains by choosing appropriately bounded
functions fi and gj . In the following sections, we will just assume that Solution(L) is not empty.
We note that our results hold for both bounded and unbounded fi and gj . We are now ready to write
down the equations of the GDA dynamics for a HCC game:
3
Under review as a conference paper at ICLR 2021
2.3	Reparametrization
The following lemma is useful in studying the dynamics of hidden games.
Lemma 1. Let k : Rd → R be a C2 function. Let h : R → R be a C1 function and x(t) denote the
unique solution of the dynamical system Σ1. Then the unique solution for dynamical system Σ2 is
z(t) = x(R0t h(s)ds)
Figure 1: Neither Gradient Descent nor Ascent can traverse stationary points. An immediate conse-
quence of Lemma 1 is that ifwe initialize in the above example θi(0) at (a), fi(θi(t)) can not escape
the purple section. This extends to cases where θi is vector of variables.
(2)
By choosing h(t) = -∂L(F(t), G(t))∕∂fi and h(t) = ∂L(F(t), G(t))∕∂gj respectively, We can
connect the dynamics of each θi and φj under Equation (1) to gradient ascent on fi and gj. Apply-
ing Lemma 1, We get that trajectories of θi and φj under Equation (1) are restricted to be subsets
of the corresponding gradient ascent trajectories With the same initializations. For example, in Fig-
ure 1 θi(t) can not escape the purple section if it is initialized at (a) neither the orange section if
it is initialiazed at (f). This limits the attainable values that fi (t) and gj (t) can take for a specific
initialization. Let us thus define the folloWing:
Definition 2. For each initialization x(0) of Σ1, Imk (x(0)) is the image of k ◦ x : R → R.
Applying Definition 2 in the above example, Imfi (θi(0)) = (fi(-2), fi(-1)) if θi is initialized at
(c). Additionally, observe that in each colored section fi(θi(t)) uniquely identifies θi(t). Generally,
even in the case that θi are vectors, Lemma 1 implies that for a given θi(0), fi(θi(t)) uniquely
identifies θi(t). As a result We get that a neW dynamical system involving only fi and gj
Theorem 1. For each initialization (θ(0), φ(0)) of Equation (1), there are C1 functions
Xθi (0) , Xφj (0) such that θi(t) = Xθi(0)(fi(t)) and φj (t) = Xφj(0)(gj(t)). If (θ(t), φ(t)) sat-
isfy Equation (1) then fi(t) = fi(θi(t)) and gj (t) = gj (φj (t)) satisfy
∂L
fi = -kVθifi(Xθi(0)(fi))k2 而(F,G)
∂fi
∂L
gj = kvφjgj(Xφj(0)(gj))k ∂-(F, G)
∂gj
(3)
By determining the ranges of fi and gj , an initialization clearly dictates if a von Neumann solution
is attainable. In Figure 1 for example, any point of the pink, orange or blue colored section like
(e), (f) or (g) can not converge to a Von Neumann solution with fi(θi) = f*. The notion of safety
captures Which initializations can converge to a given element of Solution(L).
Definition 3. . We will call the initialization (θ(0), φ(0)) safe for a (p, q) ∈ Solution(L) if
φi (0) and θj(0) are not stationary points of fi and gj respectively and pi ∈ Imfi (θi (0)) and
qj ∈ Imgj(φj(0)).
Finally, in the following sections we use some fundamental notions of stability. We call an equilib-
rium x* of an autonomous dynamical system X = D(x(t)) Stable if for every neighborhood U of
x* there is a neighborhood V of x* such that if x(0) ∈ V then x(t) ∈ U for all t ≥ 0. We call a set
S asymptotically stable if there exists a neighborhood R such that for any initialization x(0) ∈ R,
x(t) approaches S as t → +∞. If R is the whole space the set globally asymptotically stable.
4
Under review as a conference paper at ICLR 2021
3 Learning in Hidden Convex Concave Games
3.1 General Case
Our main results are based on designing a Lyapunov function for the dynamics of Equation (3):
Lemma 2. If L is convex concave and (φ(0), θ(0)) is a safe for (p, q) ∈ Solution(L), then the
following quantity is non-increasing under the dynamics of Equation (3):
N	fi	z - pi	M gj
H(F, G)=X=XL kvfi(Xθi(0)(z))k2dz+XL
Observe that our Lyapunov function here is not
the distance to (p, q) as in a classical convex con-
cave game. The gradient terms account for the
non constant multiplicative terms in Equation (3).
Indeed if the game was not hidden and fi and gj
were the identity functions then H would coin-
cide with the Euclidean distance to (p, q). Our
first theorem employs the above Lyapunov func-
tion to show that (p, q) is stable for Equation (3).
Theorem 2.	If L is convex concave and
(φ(0), θ(0)) is a safe for (p, q) ∈ Solution(L),
then (p, q) is stable for Equation (3).
Clearly, for the special case of globally invertible
functions F, G we could come up with an equiv-
alent Lyapunov function in the θ, φ-space. In this
case it is straightforward to transfer the stabil-
ity results from the induced dynamical system of
F, G (Equation (3)) to the initial dynamical sys-
tem of θ, φ (Equation (1)). For example we can
prove the following result:
(4)
______z-j_______dz
kVgj (Xφj (0)(z))k2
F(θ)
Figure 2: Level sets of Lyapunov function of
Equation (4) for both F and G being one dimen-
sional sigmoid functions.
Theorem 3.	If fi and gj are sigmoid functions and L is convex concave and there is a (φ(0), θ(0))
that is safe for (p, q) ∈ Solution(L), then (F-X(p), G-X(q)) is stable for Equation (1).
In the general case though, stability may not be guaranteed in the parameter space of Equation (1).
We will instead prove a weaker notion of stability, which we call hidden stability. Hidden stability
captures that if (F(θ(0)), G(φ(0))) is close to a von Neumann solution, then (F(θ(t)), G(φ(t)))
will remain close to that solution. Even though hidden stability is weaker, it is essentially what we
are interested in, as the output space determines the utility that each player gets. Here we provide
sufficient conditions for hidden stability.
Theorem 4	(Hidden Stability). Let (p, q) ∈ Solution(L). Let Rfi and Rgj be the set of regular
values1 of fi and gj respectively. Assume that there is a ξ > 0 such that [pi-ξ, pi + ξ] ⊆ Rfi and
[qj -ξ, qj + ξ] ⊆ Rgj . Define
r(t) = kF(θ(t)) - pk2 + kG(φ(t)) - qk2.
If fi and gj are proper functions2, then for every > 0, there is an δ > 0 such that
r(0) < δ =⇒ ∀t ≥ 0 : r(t) < .
Unfortunately hidden stability still does not imply convergence to von Neumann solutions. Vlatakis-
.
Gkaragkounis et al. (2019) studied hidden bilinear games and proved that H = 0 for this special
class of HCC games. Hence, a trajectory is restricted to be a subset of a level set of H which is
bounded away from the equilibrium as shown in Figure 2. To sidestep this, we will require in the
next subsection the hidden game to be strictly convex concave.
1A value a ∈ Im f is called a regular value of f if ∀q ∈ dom f : f (q) = a, it holds Vf (q) = 0.
2A function is proper if inverse images of compact subsets are compact.
5
Under review as a conference paper at ICLR 2021
3.2 Hidden strictly convex concave games
In this subsection we focus on the case where L is a strictly convex concave function. Based on
Definition 1, a strictly convex concave game is not necessarily strictly convex strictly concave and
thus it may have a continum of von Neumann solutions. Despite this, LaSalle’s invariance principle,
combined with the strict convexity concavity, allows us to prove that if (θ(0), φ(0)) is safe for
Z ⊆ Solution(L) then Z is locally asymptotically stable for Equation (3).
Lemma 3. Let L be strictly convex concave and Z ⊂ Solution(L) is the non empty set of equilbria
of L for which (θ(0), φ(0)) is safe. Then Z is locally asymptotically stable for Equation (3).
The above lemma however does not suffice to prove that for an arbitrary initialization (θ(0), φ(0)),
(F(t), G(t)) approaches Z as t → +∞. In other words, a-priori it is unclear if (F(θ(0)), G(φ(0)))
is necessarily inside the region of attraction (ROC) of Z. To get a refined estimate of the ROC of Z,
we analyze the behavior of H as fi and gj approach the boundaries of Imfi (θi (0)) and Imgj (φj (0))
and more precisely we show that the level sets of H are bounded. Once again the corresponding
analysis is trivial for convex concave games, since the level sets are spheres around the equilibria.
Theorem 5.	Let L be strictly convex concave and Z ⊂ Solution(L) is the non empty set of equi-
lbria of L for which (θ(0), φ(0)) is safe. Under the dynamics of Equation (1) (F(θ(t)), G(θ(t)))
converges to a point in Z as t → ∞.
The theorem above guarantees convergence to a von Neumann solution for all initializations that
are safe for at least one element of Solution(L). However, this is not the same as global asymptotic
stability. To get even stronger guarantees, we can assume that all initializations are safe. In this case
it is straightforward to get a global asymptotic stability result:
Corollary 1. Let L be strictly convex concave and assume that all intitializations are safe for at
least one element of Solution(L). The following set is globally asymptotically stable for continuous
GDA dynamics.
{(θ*,φ*) ∈ Rn X Rm : (F(θ*),G(φ*)) ∈ Solution(L)}
Notice that the above approach on global asymptotic convergence using Lyapunov arguments can
be extended to other popular alternative gradient-based heuristics like variations of Hamiltonian
Gradient descent. For concision, we defer the exact statements, proofs in Section 8.2.2
3.3 Convergence via regularization
Regularization is a key technique that works both in the practice of GANs Mescheder et al. (2018);
KUrach et al. (2019) and in the theory of convex concave games PeroIat et al. (2020); Roth et al.
(2017); Sanjabi et al. (2018). Our settings of hidden convex concave games allows for provable
gUarantees for regUlarization in a wide class of settings, bringing closer practical and theoretical
gUarantees. Let Us have a Utility L(x, y) that is convex concave bUt not strictly. Here we will
propose a modified Utility L0 that is strictly convex strictly concave. Specifically we will choose
L0(χ, y) = L(χ,y) + 2kxk2- 2kyk2
The choice of the parameter λ captUres the trade-off between convergence to the original eqUilibriUm
of L and convergence speed. On the one hand, invoking the implicit fUnction theorem, we get that
for small λ the eqUilibria of L are not significantly pertUrbed.
Theorem 6.	If L is a convex concave function with invertible Hessians at all its equilibria, then for
each > 0 there is a λ > 0 such that L0 has equilibria that are -close to the ones of L.
Note that invertibility of the Hessian means that L mUst have a UniqUe eqUilibriUm. On the other hand
increasing λ increases the rate of convergence of safe initializations to the pertUrbed eqUilibriUm
Theorem 7. Let (θ(0), φ(0)) be a safe initialization for the unique equilibrium of L0 (p, q). If
r(t) = kF(θ(t)) - pk2 + kG(φ(t)) - qk2
then there are initialization dependent constants c0, c1 > 0 such that r(t) ≤ c0 exp(-c1 t).
6
Under review as a conference paper at ICLR 2021
4 Applications
In this section we show how our theorems provide connections between the framework of hidden
games and practical applications of min-max optimization like training GANs.
Hidden strictly convex-concave games. We will start our discussion with the fundamental gen-
erative architecture of Goodfellow et al. (2014a)’s GAN. In the vanilla GAN architecture, as it is
commonly referred, our goal is to find a generator distribution pG that is close to an input data dis-
tribution pdata. To find such a generator function, we can use a discriminator D that “criticizes” the
deviations of the generator from the input data distribution. For the case of a discrete pdata over a set
N, the minimax problem of Goodfellow et al. (2014a) is the following:
min max
Pg(x)≥0, D∈(0,1)lNl
Px∈N pG(x)=1
V(G,D) =	pdata(x) log(D(x)) +	pG(x) log(1 - D(x))
x∈N
x∈N
The problem above can be formulated as a constrained strictly convex-concave hidden game. On
the one hand, for a fixed discriminator D*, the V(G, D*) is linear over the PG(x). On the other
hand, for a fixed generator G*, V(G*, D) is Strongly-Concave. We can implement the inequality
constraints on both the generator probabilities and discriminator using sigmoid activations. For the
equality constraint Px∈N pG(x) = 1 we can introduce a Langrange multiplier. Having effectively
removed the constraints, we can see in Figure 3, the dynamics of Equation (1) converge to the unique
equilibrium of the game, an outcome consistent with our results in Corollary 1. It is worth noting
that while the Euclidean distance to the equilibrium is not monotonically decreasing, H(t) is.
Figure 3: Comparison of`2 distance from the equilibrium and the Lyapunov function. Both of them
converge to zero as we state but `2 distance not monotonically to zero. For pdata we choose a fully
mixed distribution of dimension d = 4. Given the sigmoid activations all the initializations are safe.
We defer the detailed proof of convergence in Section 9.2.
Hidden convex-concave games & Regularizaiton. An even more interesting case is Wassertein
GANs-WGANs (ArjoVsky et al. (2017)). One of the contributions of Lei et al. (2019) is to show
that WGANs trained with Stochastic GDA can learn the parameters of Gaussian distributions whose
samples are transformed by non-linear activation functions. It is worth mentioning that the original
WGAN formulation has a Lipschitz constraint in the discriminator function. For simplicity, Lei et al.
(2019) replaced this constraint with a quadratic regularizer. The min-max problem for the case of
one-dimensional Gaussian N(0, a2) and linear discriminator Dv(x) = v>x with x2 activation is:
min max VWGAN(Gα, Dv) = EX〜Pdata D(X)] - EX〜PG [D(X)] -VI2
α∈R v∈R
=Ex〜N(0,a2)2 [vx] - Ex〜N(0,a2)2 [vx] - ν2/2
=(Q2 - α2)v - v2/2
Observe that VWGAN is not convex-concave but it can posed as a hidden strictly convex-concave
game with G(α) = (α2 - α2) and F(V) = v. When computing expectations analytically without
sampling, Theorem 5 guarantees convergence. In contrast, without the regularizer VWGAN can be
modeled as a hidden linear-linear game and thus GDA dynamics cycle. Empirically, these results
are robust to discrete and stochastic updates using sampling as shown in Figure 4. Therefore regu-
larization in the work of Lei et al. (2019) was a vital ingredient in their proof strategy and not just an
implementation detail. In Section 9.3, we also discuss applications of regularization to normal form
zero sum games.
7
Under review as a conference paper at ICLR 2021
a
40000
20000
10000
t 30000
——GDA
---Lei et. al
Figure 4: On the left, We show the trajectories of regularized GDA for α2 = 1 as well as the level
sets of Equation (4). All trajectories converge to one of the two equilibria (0, 1) and (0, -1) whereas
without regularization, GDA would cycle on the level sets. In the right figure, we replace the exact
expectations in VWGAN with approximations via sampling and continuous time updates on α and v
with discrete ones. For small learning rates and large sample sizes, unregularized GDA continues to
cycle. In contrast, the regularization approach of Lei et al. (2019) converges to the (0, 1) equilibrium.
The two applications of HCC games in GANs are not isolated findings but instances of a broader
pattern that connects HCC games and standard GAN formulations. As noted by Goodfellow (2017),
if updates in GAN applications were directly performed in the “functional space”, i.e. the generator
and discriminator outputs, then standard arguments from convex concave optimization would imply
convergence to global Nash equilibria. Indeed, standard GAN formulations like the vanilla GAN
Goodfellow et al. (2014a), f-GAN (Nowozin et al. (2016)) and WGAN Arjovsky et al. (2017) can all
be thought of as convex concave games in the space of generator and discriminator outputs. Given
that the connections between convex concave games and standard GAN objectives in the output
space is missing from recent literature, in Section 9.1 we show how one can apply Von Neumann’s
minimax theorem to derive the optimal generators and discriminators even in the non-realizable case.
In practice, the updates happen in the parameter space and thus convexity arguments no longer apply.
Our study of HCC games is a stepping stone towards bridging the gap in convergence guarantees
between the case of direct updates in the output space and the parameter space.
5 Discussion
In this work, we introduce a class of non-convex non-concave games that we call hidden convex
concave (HCC) games. In this class of games, the competition on the output/operator space has a
convex concave structure but training happens in the input/parameter space, where the mappings
between input and output space are smooth but non-convex non-concave functions. The main in-
spiration for this class is the indirect competition of the parameters of generator and discriminator
on GANs’ architectures. Our analysis combines ideas from game theory, dynamical systems and
control theory such Lyapunov functions and LaSalle’s theorem. Our convergence results favor not
arbitrary local Nash equilibria, but only von Neumann solutions. To the best of our knowledge, such
last iterate convergence results are the first result of their kind. Given the modular structure of our
model and proofs, HCC games show particular promise as a theoretical testbed for studying which
dynamics are more well suited to which GANs. We believe that further positive results of this kind
for different combinations of GAN formulations and learning algorithms are possible by properly
adapting our current techniques.
8
Under review as a conference paper at ICLR 2021
References
Jacob Abernethy, Kevin A Lai, Kfir Y Levy, and Jun-Kun Wang. Faster rates for convex-concave
games. In COLT, 2018.
Jacob Abernethy, Kevin A Lai, and Andre Wibisono. Last-iterate convergence rates for min-max
optimization. arXiv preprint arXiv:1906.02027, 2019.
Leonard Adolphs, Hadi Daneshmand, AUrelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In The 22nd International Conference on Ar-
tificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pp.
486-495, 2019. URL http://Proceedings.mlr.press∕v89∕adolphs19a.html.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. CoRR, abs/1701.07875,
2017. URL http://arxiv.org/abs/1701.07875.
James Bailey and Georgios Piliouras. Fast and furious learning in zero-sum games: vanishing
regret with non-vanishing step sizes. In Advances in Neural Information Processing Systems, pp.
12977-12987, 2019.
James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In
Proceedings of the 2018 ACM Conference on Economics and Computation, Ithaca, NY, USA,
June 18-22, 2018, pp. 321-338, 2018. doi: 10.1145/3219166.3219235. URL https://doi.
org/10.1145/3219166.3219235.
James P Bailey, Gauthier Gidel, and Georgios Piliouras. Finite regret and cycles with fixed step-size
via alternating gradient descent-ascent. In COLT, 2020.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob N. Foerster, Karl Tuyls, and Thore
Graepel. The mechanics of n-player differentiable games. In Jennifer G. Dy and Andreas Krause
(eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings ofMachine
Learning Research, pp. 363-372. PMLR, 2018. URL http://proceedings.mlr.press/
v80/balduzzi18a.html.
Sanjay P. Bhat and Dennis S. Bernstein. Nontangency-based lyapunov tests for convergence and
stability in systems having a continuum of equilibria. SIAM J. Control and Optimization, 42(5):
1745-1775, 2003. doi: 10.1137/S0363012902407119. URL https://doi.org/10.1137/
S0363012902407119.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, USA,
2004. ISBN 0521833787.
Nikolo Cesa-Bianchi and Gabor Lugoisi. Prediction, Learning, and Games. Cambridge University
Press, 2006.
Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:
Chaos and butterfly effects of online learning in zero-sum games. In COLT, 2019.
Yun Kuen Cheung and Georgios Piliouras. Chaos, Extremism and Optimism: Volume Analysis of
Learning in Games. arXiv e-prints, art. arXiv:2005.13996, May 2020.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradi-
ent descent in min-max optimization. In Advances in Neural Information Process-
ing Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, 3-8 December 2018, Montreal, Canada., pp. 9256-9266, 2018. URL
http://papers.nips.cc/paper/8136- the- limit- points- of- optimistic-
gradient- descent- in- min- max- optimization.
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. In 10th Innovations in Theoretical Computer Science Con-
ference, ITCS 2019, January 10-12, 2019, San Diego, California, USA, pp. 27:1-27:18, 2019.
doi: 10.4230/LIPIcs.ITCS.2019.27. URL https://doi.org/10.4230/LIPIcs.ITCS.
2019.27.
9
Under review as a conference paper at ICLR 2021
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with
optimism. In ICLR, 2018.
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained
min-max optimization, 2020.
Lawrence C Evans. Partial differential equations and monge-kantorovich mass transfer. Current
developments in mathematics,1997(1):65-126, 1997.
Ky Fan. Minimax theorems. Proceedings of the National Academy of Sciences of the United States
of America, 39(1):42, 1953.
Farzan Farnia and Asuman E. Ozdaglar. Gans may have no nash equilibria. CoRR, abs/2002.09124,
2020. URL https://arxiv.org/abs/2002.09124.
Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in stackelberg
games: Equilibria characterization, convergence analysis, and empirical study. In ICML, 2020.
GaUthier GideL Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-JUlien. A
variational inequality perspective on generative adversarial networks. In ICLR, 2019. URL
https://openreview.net/forum?id=r1laEnA5Ym.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014a.
Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160,
2017. URL http://arxiv.org/abs/1701.00160.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neu-
ral Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672-2680, 2014b. URL
http://papers.nips.cc/paper/5423- generative- adversarial- nets.
Yu-Guan Hsieh, Franck Iutzeler, JerOme Malick, and Panayotis Mertikopoulos. On the convergence
of single-call stochastic extra-gradient methods. In Advances in Neural Information Processing
Systems, pp. 6938-6948, 2019.
Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggres-
sively, update conservatively: Stochastic extragradient methods with variable stepsize scaling.
arXiv preprint arXiv:2003.10162, 2020.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Minmax optimization: Stable limit points
of gradient descent ascent are locally optimal. CoRR, abs/1902.00618, 2019. URL http://
arxiv.org/abs/1902.00618.
Hassan K Khalil. Nonlinear systems; 3rd ed. Prentice-Hall, Upper Saddle River, NJ, 2002. URL
https://cds.cern.ch/record/1173048.
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study
on regularization and normalization in gans. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-
15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning
Research, pp. 3581-3590. PMLR, 2019. URL http://proceedings.mlr.press/v97/
kurach19a.html.
Qi Lei, Jason D Lee, Alexandros G Dimakis, and Constantinos Daskalakis. Sgd learns one-layer
networks in wgans. arXiv preprint arXiv:1910.07030, 2019.
Chris J. Maddison, Daniel Paulin, Yee Whye Teh, Brendan O’Donoghue, and Arnaud Doucet.
Hamiltonian descent methods. CoRR, abs/1809.05042, 2018. URL http://arxiv.org/
abs/1809.05042.
10
Under review as a conference paper at ICLR 2021
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Eric Mazumdar and Lillian J Ratliff. Local nash equilibria are isolated, strict local nash equilibria in
‘almost all’ zero-sum continuous games. In 2019 IEEE 58th Conference on Decision and Control
(CDC),pp. 6899-6904. IEEE, 2019.
Panayotis Mertikopoulos, Christos H. Papadimitriou, and Georgios Piliouras. Cycles in adversar-
ial regularized learning. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pp. 2703-
2717, 2018. doi: 10.1137/1.9781611975031.172. URL https://doi.org/10.1137/1.
9781611975031.172.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra(-gradient) mile. In ICLR, 2019. URL https://openreview.net/forum?id=
Bkg8jjC9KQ.
Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3478-3487.
PMLR, 2018. URL http://proceedings.mlr.press/v80/mescheder18a.html.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artificial Intelligence and Statistics, pp. 1497-1507. PMLR, 2020.
Yurii E. Nesterov. Introductory Lectures on Convex Optimization - A Basic Course, volume 87 of
Applied Optimization. Springer, 2004. ISBN 978-1-4613-4691-3. doi: 10.1007/978-1-4419-
8853-9. URL https://doi.org/10.1007/978-1-4419-8853-9.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neu-
ral samplers using variational divergence minimization. In Daniel D. Lee, Masashi
Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in
Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 271-279, 2016.
URL http://papers.nips.cc/paper/6066-f-gan-training-generative-
neural-samplers-using-variational-divergence-minimization.
Brendan O’Donoghue and Chris J. Maddison. Hamiltonian descent for composite objectives. In
Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d,Alche-Buc, Emily B. Fox,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December
2019, Vancouver, BC, Canada, pp. 14443-14453, 2019. URL http://papers.nips.cc/
paper/9590- hamiltonian- descent- for- composite- objectives.
Lawrence Perko. Differential Equations and Dynamical Systems. Springer, 3nd. edition, 1991.
Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pe-
dro A. Ortega, Neil Burch, Thomas W. Anthony, David Balduzzi, Bart De Vylder, Georgios
Piliouras, Marc Lanctot, and Karl Tuyls. From Poincare recurrence to convergence in imperfect
information games: Finding equilibrium via regularization. CoRR, abs/2002.08456, 2020. URL
https://arxiv.org/abs/2002.08456.
Kevin Roth, AUreIien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Infor-
mation Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 2018-2028,
2017. URL http://papers.nips.cc/paper/6797-stabilizing-training-
of- generative- adversarial- networks- through- regularization.
11
Under review as a conference paper at ICLR 2021
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D. Lee. On the convergence and robust-
ness of training gans with regularized optimal transport. In Samy Bengio, Hanna M. Wallach,
Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada, pp. 7091-7101,
2018. URL http://papers.nips.cc/paper/7940-on-the-convergence-and-
robustness- of- training- gans- with- regularized- optimal- transport.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. PoinCare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In Advances in Neural Information Processing Systems, pp. 10450-10461, 2019.
Jun-Kun Wang and Jacob D Abernethy. Acceleration through optimistic no-regret dynamics. In
Advances in Neural Information Processing Systems, pp. 3824-3834, 2018.
Guojun Zhang and Yaoliang Yu. Convergence of gradient methods on bilinear zero-sum games. In
International Conference on Learning Representations, 2019.
Guojun Zhang, Pascal Poupart, and Yaoliang Yu. Optimality and Stability in Non-Convex-Non-
Concave Min-Max Optimization. arXiv e-prints, art. arXiv:2002.11875, February 2020.
12
Under review as a conference paper at ICLR 2021
Contents
1	Introduction	1
2	Preliminaries	3
2.1	Notation .................................................................... 3
2.2	Hidden Convex Concave Games ................................................. 3
2.3	Reparametrization ........................................................... 4
3	Learning in Hidden Convex Concave Games	5
3.1	General Case ................................................................ 5
3.2	Hidden strictly convex concave games ........................................ 6
3.3	Convergence via regularization .............................................. 6
4	Applications	7
5	Discussion	8
6	Background	14
6.1	Background in dynamical systems ............................................ 14
6.2	Background in convex optimization .......................................... 16
6.3	Background in Game Theory .................................................. 16
7	Preliminaries	17
8	Hidden Convex Concave Games	18
8.1	General case ............................................................... 19
8.2	Hidden strictly convex concave games ....................................... 22
8.2.1	Gradient Descent-Ascent Dynamics .................................... 22
8.2.2	Connections to Hamiltonian Descent .................................. 25
8.3	Regularization and convergence ............................................. 26
9	Applications	28
9.1	Connecting GANs and Hidden Convex-Concave Games ............................ 28
9.1.1	GAN ................................................................. 29
9.1.2	f-GAN ............................................................... 31
9.1.3	WGAN ................................................................ 34
9.2	GANs and Hidden Constrained Optimization ................................... 35
9.3	Zero-Sum Games ............................................................. 37
13
Under review as a conference paper at ICLR 2021
6 Background
6.1	Background in dynamical systems
Our analysis combines tools from dynamical systems, stability analysis and invariance prin-
ciples theory. We start with the definitions of the different stability notions. We remind the
well known Lyapunov,s Lyapunov stability criterion (Theorem 8) Stability analysis in con-
vex concave games is further complicated due to the possibility of non-isolated fixed points.
To tackle this issue, We recall KraSOVSkii-LaSalle,s Invariance Principle (Theorem 9), a
powerful result that has several implications for the asymptotic stability of a set in an au-
tonomous (possibly nonlinear) dynamical system. In the special case where the goal set con-
tains only stable fixed points a pointwise convergence theorem can be derived (Theorem 10).
Finally, we remind the notions of diffeomorphism and topological conjugacy of two dynam-
ical systems, which are useful to transfer behavioral claims between equivalent dynamics.
Let f : D → Rn be a locally Lipschitz map from a domain D ⊂ Rn to Rn. We consider dynamical
systems of the form
X = f (χ)	(?)
A point X for which f (X) = 0 is called a fixed point. We will be interested in the following notions
of stability for the fixed point points of Equation (?).
Definition 4 (Stability properties, (Khalil, 2002, Definition 4.1)). The fixed point X = 0 of Equa-
tion (?) is
•	stable if, for each > 0, there is a δ = δ() > 0 such that
kX(0)k <δ =⇒ kX(t)k <	∀t≥0
•	unstable if it is not stable
•	asymptotically stable if it is stable and δ can be chosen such that
kX(0)k < δ =⇒ lim X(t) = 0
t→∞
The Lyapunov Theorem will be a useful tool to prove (asymptotic) stability of a fixed point.
Theorem 8 (Lyapunov Theorem, (Khalil, 2002, Theorem 4.1)). Let X = 0 be a fixed point point
for Equation (?) and D ⊂ Rn be a domain containing X = 0. Let V : D → R be a continuously
differentiable function such that
V (0) = 0 and V (X) > 0 in D - {0}
V(x) ≤ 0 in D
then X = 0 is stable. Moreover if
V(X) < 0 in D 一 {0}
then X = 0 is asymptotically stable.
Unfortunately, the Lyapunov theorem is not very helpful when it comes to proving convergence in
dynamical systems with non isolated fixed points. By definition, non-isolated fixed points cannot
be asymptotically stable. Non isolated fixed points may give rise to more complex behaviour than
point-wise convergence.
Definition 5. We say that a trajectory X(t) approaches a set M as t → ∞ if for each > 0 there is
a T > 0 such that
dist(X(t), M) < , ∀t > T
where the operator “dist” is the minimum distance from a point to a set M
dist(p, M) = inf kp 一 Xk
14
Under review as a conference paper at ICLR 2021
Definition 6. We say that a set M is invariant for Equation (?) if
x(0) ∈ M =⇒ x(t) ∈ M, ∀t ∈ R
We will say M is positively invariant if the above holds for t ≥ 0.
We are ready to state LaSalle’s Invariance Principle, a general theorem that can help us study the
stability of non isolated fixed points.
Theorem 9 (LaSalle's Invariance Principle, (Khalil, 2002, Theorem 4.4)). Let Ω ⊂ D bea compact
set that is positively invariant with respect to Equation (?). Let V : D → R be a continuously
♦ ♦
differentiable function such that V (x) ≤ 0 in Ω. Let E be the set of all points where V (x) = 0. Let
M be the largest invariant set in E. Then every solution starting in Ω approaches M as t → ∞.
LaSalle’s theorem does not give us pointwise convergence directly. But in the special case that M
contains only stable fixed points we can apply the following theorem
Theorem 10 (Pointwise Convergence Theorem, (Bhat & Bernstein, 2003, Proposition 5.4)). Let
x(t) be a trajectory of Equation (?). If the positive limit sets of x(t) contain a stable fixed point then
x(t) converges to it as t → ∞.
Definition 7 (Differomorphism, Perko (1991)). Let U, V be manifolds. A map f : U → V is called
a diffeomorphism iff carries U onto V and also both f and f-1 are smooth.
Definition 8 (Topological conjugacy, Perko (1991)). Two flows Φt : A → A and Ψt : B → B are
conjugate if there exists a homeomorphism g : A → B such that
∀x ∈ A, t ∈ R : g(Φt(x)) = Ψt(g(x))
Furthermore, two flows Φt : A → A and Ψt : B → B are diffeomorphic if there exists a diffeomor-
phism g : A → B such that
∀x ∈ A, t ∈ R : g(Φt(x)) = Ψt(g(x)).
If two flows are diffeomorphic, then their vector fields are related by the derivative of the conjugacy.
That is, we get precisely the same result that we would have obtained if we simply transformed the
coordinates in their differential equations.
15
Under review as a conference paper at ICLR 2021
6.2 Background in convex optimization
For the sake of completeness, We recall here the definition of (strict) Convex/concave func-
tion and its first order necessary and sufficient criterion. We will also discuss strong convex-
ity and its second order characterizations.
We will be interested in notions from convex optimization throughout this work
Definition 9 ((Boyd & Vandenberghe, 2004, p. 67)). Let f : Rn → R be a function then
•	f is convex if
∀x,y ∈ Rn,t ∈ [0, 1] : f(tx+ (1 - t)y) ≤ tf (x) + (1 - t)f (y)
•	f is strictly convex if
∀x,y ∈ Rn,t ∈ (0, 1) : f(tx+ (1 - t)y) < tf (x) + (1 - t)f (y)
•	f is (strictly) concave if -f is (strictly) convex.
We will also use the first order characterizations of convex and concave functions
Theorem 11 ((Boyd & Vandenberghe, 2004, p. 69-70)). Let f : Rn → R be a differentiable
function.
•	f is convex ifand only if ∀χ, y ∈ Rn : f (y) ≥ f (x) + Vf (x)T(y 一 x)
•	f is concave ifand only if∀x, y ∈ Rn : f(y) ≤ f(x) + Vf(x)T(y — x)
To establish convergence rates, we will use the notion of strong convexity
Definition 10 ((Nesterov, 2004, p. 63)). A continuously differentiable function f of Rn will be
called μ strongly Convexfor a positive constant μ iffor all x, y ∈ Rn we have
f(y) ≥ f(x) + hvf(xb y — xi + 2kx — yk2
We will also use second order characterizations of strong convexity
Theorem 12 ((Nesterov, 2004, p. 65)). A twice continuously differentiable function f is μ StrOngly
COnvexfOr a positive constant μ ifand only iffor all X ∈ Rn we have
V2 f (x) ≥ μI
Symmetrically, a function will be called μ strongly concave if —f is μ strongly convex.
6.3 Background in Game Theory
In this short section, we remind to the reader a generalization of Von-Neumann,s Minimax
theorem, which we will exploit to analyze the equilibrium solution of the different GANs'
architectures. A special case of Fan's minimax theorem is the following
Corollary 2 (Fan’s minimax theorem, Fan (1953)). Let X ⊂ Rn and Y ⊂ Rm be convex non-
empty sets. Suppose that X is compact and f : X X Y → R is a function such that f (∙,y) is lower
semicontinuous on X for each y ∈ Y and that f is convex concave. Then we have that
min sup f(x, y) = sup min f(x, y).
x∈X y∈Y	y∈Y x∈X
16
Under review as a conference paper at ICLR 2021
7 Preliminaries
The below time-reparametrization lemma shows that the solution for a non-autonomous sys-
tem, multiplicative to a gradient flow can be derived by just time-rescaling of the solution
of the simplified gradient ascent dynamics. Indeed, since the multiplicative term is common
across all terms of the vector field then over the time it dictates only the magnitude of the
vector field (the speed of the motion), but does not affect the directionality other than moving
backwards or forwards along the same trajectory.
Lemma 1. Let k : Rd → R be a C2 function. Let h : R → R be a C1 function and x(t) denote the
unique solution of the dynamical system Σ1. Then the unique solution for dynamical system Σ2 is
z(t) = x(R0t h(s)ds)
ʃ X =	vk(χ)l : ς z z =	h⑴Vk(Z)I ：
(0) = xinit 1 z(0) = xinit
(5)
Proof. Firstly, notice that it holds x(0) = Xinit and X = Vk(x), since X is the unique solution of ∑ι
It is easy to check that:

(0)=X(Z0
0
h(s)ds) = X(0)
Xinit
d[R0th(S)ds]
dt
Vk
h(t) = Vk(z)h(t)
□
In order to leverage the convex-concave properties of the operators in our hidden structure
under the Gradient Descent Ascent dynamics we need to recover the equivalent system (T)
G、
in the operator space (G) = T
— VL(F(θ), G(φ))∖ ≡ ∫θi	= - Vθi fi(θi)hfi,L(t)]
VL(F⑹,G(φA∕ — lφ j	= vφj gj (φj )hgj,L⑴/
From this point, applying the aforementioned lemma, under GDA each f and gj follows
a time dependent rescaling of the corresponding gradient ascent solution. Exploiting the
monotonicity of fi(t) and gj (t) under gradient ascent, we can construct an invertible map
between the parameter space {(θi,φj)} and the operator space {(fi,gj)} which allows us
to construct the equivalent system T in the operator space. Notice that the properties of
gradient ascent are crucial since the operator space can be arbitrarily smaller in dimension.
In this case a smooth invertible map that is common for all initializations cannot exist.
Theorem 1.	For each initialization (θ(0), φ(0)) of Equation (1), there are C1 functions
Xθi(0) ,	Xφj (0) such that	θi(t) =	Xθi(0)(fi(t)) and	φj (t)	=	Xφj(0)(gj(t)).	If(θ(t),φ(t))	sat-
isfy Equation (1) then fi(t) = fi(θi(t)) and gj (t) = gj (φj (t)) satisfy
∂L
fi = -IUifi(XJi(0)(fi))k 和(F, G)
∂fi
∂L
gj = kV6jgj(XΦj(0)(gj))k ∂g-(F, G)
(3)
Proof. Let us first study a simpler dynamical system (Σ*) with unique solution of Yθi(o)(t).
(∑*) ≡
.
Z
(0)
Vfi(Z)
θi(0)
17
Under review as a conference paper at ICLR 2021
It is easy to observe that:
fi = Vf (z)Z = kVf(z)k2
If θi (0) is a stationary point of fi then the trajectory of z is a single point. But the trajectory of θi
under the dynamics of Equation (1) is also a single point so we can pick the following function
Xθi(0) (fi) = θi (0).
On the other hand if θi(0) is not a stationary point of fi, fi continuously increases along the trajec-
tory of (Σ*). Therefore Aθi(o) (t) = fi(γθi(o)(t)) is an increasing function and therefore invertible.
Let us call Aθ-1(0)(fi) the inverse.
Let’s recall now the θ i part of the dynamical system of interest Equation (1)
∂L
θi = -Vθi fi(θi) f (F(θ),G(φ))
initialized at θi(0). Applying Lemma 1 for the first equation with
∂L
h(t) = — — (F(θ(t)),G(φ(t)))
we have that under the dynamics of Equation (1)
θi(t) = γθi(0) Z h(s)ds
(P)
Thus it holds
fi (θi (t)) = f γθi (0)	Z h(s)ds	= Aθi(0) Z h(s)ds
or equivalently
Z t h(s)ds = Aθ-i1(0) (fi(θi(t)))
Plugging in back to Equation (P)
θi(t) = γθi(0)(Aθ-i1(0)(fi(θi(t))))
Therefore we can pick
Xθi (0) (fi) = γθi(0) (Aθ-i1(0) (fi))
which is C1 as composition of C1 functions. We can perform an equivalent analysis for φj (0) and
gj to pick C1 function Xφj(0). Let us now track the time derivative of fi(θi) and gj(φj)
∂L
fi = Vθi fi(θi)θi = kVθi fi(θi)k2 — (F,G)
∂L
gj = vΦjgj(φjM = kvΦjgj(φj)k Tj-(F'G)
∂gj
We can now replace θi = Xθi(o)(fi) and φj = Xφj(o)(gj-) to get the equations required.	□
8 Hidden Convex Concave Games
In this section, we analyze the derived stability properties of the hidden convex concave
games. It is worth mentioning that without strict/strong convexity/concavity from at least
one of the operators, the quality of the results are limited to “Lyapunov Stability”. Firstly,
we present a construction of a Lyapunov function for the operators, dynamics Theorem 2.
Then, in Theorem 3, Theorem 4 we explore the stability of the initial conditions in the
parameter space.
18
Under review as a conference paper at ICLR 2021
8.1 General case
The following theorem presents the construction of a Lyapunov potential function for the in-
duced operator dynamics. To motivate its construction, We can study a fundamental convex-
concave function L(χ, y) = (X -p)2 - (y - q)2 with saddle point (p, q). Under the gradient-
descent-ascent dynamics
-VχL(x,y)
Vy L(x,y)
(minimization of convex part)
(maximization of concave part)
it is easy to check that H(x, y) = (x - p)2 + (y - q)2 meets all the criteria of a Lyapunov
function. The construction below extends this argument to any convex-concave function
L(F, G) and bypasses the more complex multiplicative terms for the gradient induced dy-
namics of Theorem 1. Notice that
Nrfi	~	~	Mrgj
H (F, G) = X	%、、“2 dz + X
i=1 ∙∕pi k Vfi(Xθi(0) (Z)) k2	j=1 Jqj
_______Z-j___________dz
kVgj (Xφj (0)(z))k2
coincides with the '2 distancefrom (p, q) in the case of gradient norms equal to one, i.e.
kVfik2 = kVgj k2 = 1
Lemma 2. If L is convex concave and (φ(0), θ(0)) is a safe for (p, q) ∈ Solution(L), then the
following quantity is non-increasing under the dynamics of Equation (3):
N	fi	z - pi	M	gj
H(F, G)=XLi kVfi(Xθi(0)(z))∣∣2dz+XL
_______z-j___________dz
kVgj (Xφj (0)(z))k2
(4)
Proof. Simple substitution gets us the following
H= - X(fi - Pi) ∂f (F，G) + X(gj - qj ) ∂g- (F, G)
=-hF-p,VFL(F,G)i+hG-q,VGL(F,G)i
By Theorem 11 for the convex L(∙, G) and concave L(F, ∙).
-hF - p, VFL(F, G)i ≤L(p,G)-L(F,G)
hG-q,VGL(F,G)≤L(F,G)-L(F,q)
Thus we can end up writing
.
H ≤ L(p, G) - L(F, G) + L(F, G) - L(F, q)
≤ L(p, G) - L(p, q) + L(p, q) - L(F, q) ≤ 0
The last inequality holds since (p, q) ∈ Solution(L). Indeed, if (p, q) is a saddle point of L then
L(p, G) ≤ L(p, q) ≤ L(F, q).
Theorem 2.	IfL is convex concave and (φ(0), θ(0)) is a safe for (p, q) ∈ Solution(L), then (p, q)
is stable for Equation (3).
Proof. Leveraging Lemma 2, there is a function H which is well defined in D =
{Imfi(θi(0))}i=1 X {Imgj(φj(0))}M=I and in this domain H ≤ 0. Given the safety conditions
we know that (p, q) ∈ D. Observe that for the proposed function, it holds that H(p, q) = 0. Also
for each fi and gj term in H we know that it has its minimum of value 0 at the corresponding pi
and qj. We can deduce this by taking the derivative of each term to study its monotonicity. For
example, the fi terms are strictly increasing in fi > pi and strictly decreasing in fi < pi . Thus for
all D - {(p, q)}, H > 0. Applying Theorem 8 for the continuously differentiable H we have that
(p, q) is stable for Equation (3).	□
19
Under review as a conference paper at ICLR 2021
In the following example, We examine how it is possible to transfer the stability properties
between two (topological conjugate) dynamical systems.
Theorem 3.	If fi and gj are sigmoid functions and L is convex concave and there is a (φ(0), θ(0))
that is safe for (p, q) ∈ Solution(L), then (F-1(p), G-1(q)) is stable for Equation (1).
Proof. Firstly, we recall the property of sigmoid’s gradient:
dσ(x) = σ(x)(1 - σ(x)).
dx
Thus the transformed dynamical system in the operator space can be written as:
(T ).= (f, = - fi2(1 - fi)2 ∂Li (F, G))
()：=3=	gj(1 - gj)2⅜(F, G)∫
Notice that
1.	The dynamical system (T) in the operator space is independent of the initial conditions.
In fact, the dynamical system of (T) and the one of Equation (1), called (Σ) for short, are
diffeomorphic for all initializations, not just a specific trajectory.
2.	Since (θ(0), φ(0)) is safe, using Theorem 2 we get that (p, q) is stable for (T).
We would like to prove that for every open neighborhood V of (F-1(p), G-1 (q)) there exists an
open neighborhood U of (F-1(p), G-1(q)) such that
(θinit, φinit) ∈ U =⇒ ∀t ≥ 0 : (θ(t), φ(t)) ∈ V.
Using the diffeomorphism γ = γΣ→T between GDA dynamics of (Σ) and (T) , γ(V ) is an open
neighborhood of (p, q) since V is open and γ((F-1(p), G-1(q))) ≡ (p, q) ∈ γ(V ). By Item 2,
since (p, q) is stable for (T) there is an open neighborhood U of (p, q) such that:
(Finit, Ginit) ∈ UPVt ≥ 0: (F(t), G(t)) ∈ Y(V)
or equivalently
γ(θinit,φinit) ∈ U =⇒ Vt ≥ 0 : γ(θ(t),φ(t)) ∈ Y(V)
Indeed, using the inverse diffeomorphism Y-1, we can establish that for U = YT (U) it holds that
(θinit, φinit) ∈ U =⇒ Vt ≥0 : (θ(t), φ(t)) ∈ V
□
Until now, we have established the stability of a pair (p, q) for the induced dynamics (T).
By the construction of the induced dynamics, (T) is coupled only with a very specific initial
condition (θinit, φinit). In order to tackle the challenge of a stability result for a whole region
of initial conditions, in the following lemma we prove that r(θ, φ) = ∣∣F(θ)-pk2 + k G(φ) 一
qk2 can work like an intrinsic measure of closeness for the {θ, φ}-parameter space around a
hidden fixed point of the {F, G}-operator space. Under this “hidden” neighborhood notion,
stability property can be taken by assuming the properness of the hidden operators.
Theorem 4.	Let (p, q) ∈ Solution(L). Let Rfi and Rgj be the set of regular values3 of fi and gj
respectively. Assume that there is a ξ > 0 such that [pi -ξ,pi +ξ] ⊆ Rfi and [qj -ξ, qj +ξ] ⊆ Rgj.
Define
r(t) = ∣F(θ(t)) - p∣2 + ∣G(φ(t)) - q∣2.
If fi and gj	are properfunctions4, then for every > 0, there is an δ > 0 such that
r(0) < δ =⇒ Vt ≥ 0 : r(t) < .
3A value a ∈ Im f is called a regular value of f if ∀q ∈ dom f : f (q) = a, it holds Vf (q) = 0.
4A function is proper if inverse images of compact subsets are compact.
20
Under review as a conference paper at ICLR 2021
Proof. Let us define the following sets
∀i ∈ [n]	:	Ai	=	{	θi	∈Rni	|	fi(θi)	∈	[pi -ξ,pi+ξ]}
∀j ∈ [m]	:	Bj	=	{	φj	∈	Rmj	|	gj (φj )	∈	[qj - ξ, qj + ξ]}
Since f and gj are proper Ai and Bj are compact sets. Thus, the continuous functions ∣∣Vfi(θi) ∣∣2
and ∣∣Vgj(φj)∣2 have a minimum and maximum value on Ai and Bj respectively. Let us call Kfi
and Kgj the maxima and κfi and κgj the minima. Observe that the minima and maxima must be all
greater than zero since [pi - ξ,pi + ξ] and [qj - ξ, qj + ξ] are regular values. Let us define
κ = min{ min κfi, min κgj }
1≤i≤n	1≤j≤m
K = max{ max Kfi , max Kgj }
1≤i≤n	1≤j≤m	j
where K ≥ κ > 0 as we discussed. Let us create the following set
S = {(θ, φ) ∈ RN × RM | ∀i ∈ [n] : θi ∈ Ai, ∀j ∈ [m] : φj ∈ Bj }
We can prove that every (θ, φ) ∈ S is a safe initialization for (p, q). Of course, every θi and φj
are not stationary points of fi and gj respectively. We also need to prove that the equilibrium (p, q)
is feasible. We will prove this by contradiction. Let there be a (θ, φ) ∈ S such that (p, q) is not
feasible. Without loss of generality we can assume that there is an i ∈ [n] such that pi ∈/ Imfi (θi).
The case for the gj is symmetrical. Along the gradient ascent trajectory of fi with initialization at
θi, observe that fi(t) cannot attain an infimum or a supremum in [pi - ξ,pi + ξ] because there are
no stationary points of fi in Ai. Observe also that at initialization fi (θi) ∈ [pi - ξ, pi + ξ]. Thus
[pi - ξ,pi + ξ] ⊆ Imfi (θi), a contradiction.
Let us pick an initialization (θ(0), φ(0)) such that r(0) ≤ ξ2. It is clear that (θ(0), φ(0)) ∈ S and so
it is safe for (p, q). We can do the same steps as in Theorem 2 to prove that the function H(F, G)
below does not increase under the dynamics of Equation (1):
N	fi	z - pi	M	gj	z - qj
( , ) = i=1 Zi kVfi(Xθi(0)(z))k2dz + j=1 L ∣Vgj(Xφj(0)(z))k2dz
Observe that since (θ(0), φ(0)) ∈ S we have that the interval between pi and fi (θi (0)) belongs in
[pi — ξ,pi + ξ] and ∣∣Vfi(∙)k2 ≥ K in this interval. Thus We can write
(fi(θi(0))-Pi)2 ≥ [fi(θi(O))	Z — Pi	d
2κ ≥ Jpi	∣Vfi(Xθi(0)(z))k2 z
Repeating the same argument for all fi and gj we have that
r(r ≥ H (F(θ(0)), G(φ(0))) ≥ H (F(θ(t)), G(φ(t)))
Let us pick r(0) < min{ξ2,ξ2KK} = ξ2K. We already know that trajectories start in S. We will
prove that they also remain in S. We will do this by contradiction. If a trajectory escaped S, then
without loss of generality this means that there is at least an i ∈ [n] such that at some t > 0,
fi(θi(t)) ∈/ [pi — ξ,pi + ξ]. The case ofgj is similar. Clearly we have that
fi(θi(t))
pi
m,：— P；、“2dz ≥ min
kVfi(Xei(o)(z))k2
______z-Pi_______dz
kVfi(Xθi(0)(z))k2 ,
pi+ξ
pi
_______z-Pi_________dz
∣Vfi(Xθi (0)(z))k2
1
As above, we have that the gradients in the integrals of the right hand side are less or equal than K
so
fi(θi(t))
pi
______z-Pi_______dz ≥ 殳
kVfi(Xθi(0)(z))k2	≥ 2K'
The terms of H are all non-negative so we have that
r(0)	fi(θi(t))
宗 ≥ H(F(θ(t)), G(φ(t))) ≥ J,
______z-Pi_______dz ≥ S
kVfi(Xθi(0)(z))k2	≥ 2K∙
21
Under review as a conference paper at ICLR 2021
But r (0) < ξ2 KK, a contradiction. So the trajectories will stay in S. We can then write
Z - Pi d > (fi(θi(t)) - Pi)2
kVfi(Xθi(0)(z))k2 z ≥	2K
Repeating the same argument for all fi and gj we have that
r≡ ≥ H(F(θ(t)),G(Φ(t))) ≥ 祟.
2κ	2K
For every e > 0, there is a positive δ = mm{K ,'}κ such that
r(0) < δ =⇒ r(t) < .
□
A special case of the above result is the standard convex-concave games:
Corollary 3. Let L(x, y) be strictly convex concave and Solution(L) is the non empty set of equil-
bria of L. Then Solution(L) is locally asymptotically stable for continuous GDA dynamics.
Proof. The proof of the above classical result can be derived by the straightforward application of
Lemma 3 for the case of F(x) = x and G(y) = y. Notice that i) if F, G are the identity maps all
the initial configurations are safe and ii) if kVFk2 = kVGk2 = 1, then the initialization-dependent
Lyapunov functions coincide to a single Lyapunov function, which is actually the squared Euclidean
distance r(θ, φ) = ∣∣F(θ) - p∣∣2 + ∣∣G(φ) - qk2 = kθ - p∣∣2 + kφ - q∣∣2.
8.2 Hidden strictly convex concave games
8.2.1	Gradient Descent-Ascent Dynamics
In the following preliminary result, We show that strict convexity or concavity in L(∙, ∙), for
at least one of its arguments, suffices to yield locally asymptotic stability starting from a safe
initial condition. Our argumentation leverages the power of Theorem 9 and combines the
previous section stability results. Here, We will firstly outline the basic steps below:
1.	We start by showing that there exists a compact set Ω ⊂ D.
.
2.	Therefore, since H ≤ 0 (Lyapunov property), any configuration (F(0), G(0))
starting from a bounded sub-level set Ω of H, will remain inside Ω over all time.
3.	The second crucial observation is that thanks to the strictness on convexity or con-
.
cavity of L, the largest invariant set of H = 0 contains only points belonging to
Von Neumann,s Solution(L).
Then Theorem 9 implies the local asymptotic stability of set Z for Equation (3).
Lemma 3. Let L be strictly convex concave and Z ⊂ Solution(L) is the non empty set of equilbria
of L for which (θ(0), φ(0)) is safe. Then Z is locally asymptotically stable for Equation (3).
Proof. Pick a point (p, q) ∈ Z. Since our initialization is safe for this saddle point, we can construct
the H function as in Theorem 2 and prove that it has the following property
H ≤ 0 in D = {Imf"θi(0))}N=ι X {皿。(φj (0)j
If (F(θ(0)), G(φ(0))) = (p, q) then the theorem holds trivially. Otherwise, take a ball B centered
at the equilibirum with a small enough radius such that it is contained in the interior of D .
H0 =	min H(F, G)
(F,G)∈∂B
Ω = {(F, G) ∈ B∣H(F, G) ≤ H0∕2}
We know that in both of the cases H0 > 0 from Theorem 2.
22
Under review as a conference paper at ICLR 2021
Since H ≤ 0, starting in Ω, it implies that H(F(t), G(t)) ≤ H for t ≥ 0, so Ω is forward invariant.
Since Ω ⊂ D We know that it is bounded. Ω is closed since it is a sublevel set of a continuous
function. Notice that the restriction of Ω on B does not affect the above properties since Ω is in the
interior of B. Thus Ω is a compact forward invariant set, satisfying the requirement of Theorem 9
.
Let E = {(F, G) ∈ B|H(F, G) = 0}. Without loss of generality we can assume that L(∙, q) is
strictly convex as the case of L(p, ∙) being strictly concave is similar. In the following inequality
.
H ≤ L(p, G) - L(p, q) + L(p, q) - L(F, q) ≤ 0
we know that L(p, G) - L(p, q) ≤ 0 and L(p, q) - L(F, q) ≤ 0.
So H = 0 implies L(p, G) = L(p, q) = L(F, q). By the strict convexity of L(∙, q) we know that
this means that F = p. Let M be the largest invariant set inside E. By the properties of M being
invariant subset of E we have
(F(0), G(0)) ∈ M =⇒ ∀t : F(t) = p and L(p, G(t)) = L(p, q)
Taking the time derivatives on each of the constant quantities, they should be zero.
∂L
fi = 0 ⇒	∀i ∈ [N] ：	kVθifi(Xθi(0)(Pi))k2乔(p,G) = 0
∂fi
L(P,G(t)) =0 ⇒	XkvΦjgj(χΦj(o)(gj))k2 ]∂g^(p, G)	=0
We know that kVθi fi(Xθi(0)(pi))k 6= 0 by the safety conditions and that kVφj gj (Xφj (0) (gj))k2 6=
0 inside D again by safety conditions. This implies
∂L
∀i ∈ N]: f (Py)
0
∂L
∀j ∈ M]：国(DG)=0
Thus M contains only stationary points of L so M ⊆ Solution(L). In addition M ⊆ D so
only stationary points of L for which the initialization is safe are allowed so M ⊆ Z. Applying
Theorem 9 we have that for any initialization of Equation (3) inside Ω, as t → ∞ (F(t), G(t))
approaches M and thus Z is locally asymptotically stable for Equation (3).	□
A special case of the above result is the standard convex-concave games:
Corollary 4. Let L(x, y) be strictly convex concave and Solution(L) is the non empty set of equil-
bria of L. Then Solution(L) is locally asymptotically stable for continuous GDA dynamics.
In the following main result of our work, we show that strict convexity or concavity in L(∙, ∙),
for at least one of its arguments, suffices to yield a convergence result to a Von Neumann's
Solution(L) starting from a safe initial condition. In order to get convergence results for
any safe initialization, we need to study the region of attraction of the set Z ⊂ Solution(L).
We refine the estimation of the region of attraction as proposed in Lemma 3 by analyzing
the behavior of the level sets of H. More precisely, we show that the proposed Lyapunov
function
Nff	Z - P	M Fgj
H (F, G) = X 2“ Pi2 dz + 1X
i=1 Jpi k Vfi(Xθi(0) (Z)) k2	j=1 Jqj
_______Z-j___________dz
kVgj (Xφj (0)(z))k2
is radially unbounded. In other words, while the operators converges to their limit values
(SUPremUm/infimum of their domain) H → +∞. In order to show that we analyze the
asymptotic behavior of RcF 帖；1产,while F → SuP fi. Hence,
A)	Theorem 9 implies that the trajectory will approach the set of stationary points of
H or equivalently a set of Von Neumann,s Solution(L).
B)	The stability of Solution(L) and Theorem 10, leads to the conclusion that the tra-
jectory will converges to a specific point of Solution(L).
23
Under review as a conference paper at ICLR 2021
Theorem 5. Let L be strictly convex concave and Z ⊂ Solution(L) is the non empty set of equi-
lbria of L for which (θ(0), φ(0)) is safe. Under the dynamics of Equation (1) (F(θ(t)), G(θ(t)))
converges to a point in Z as t → ∞.
Proof. Again let’s pick a point (p, q) ∈ Z. Since our initialization is safe for this saddle point, we
can construct the H function as in Theorem 2 and prove that it has the following property
H ≤ 0 in D = {Imfi(θi(0))}N=ι × {Img, (φj(0))}2
If (F(θ(0)), G(φ(0))) = (p, q) then the theorem holds trivially. Otherwise define
H0=H (F(θ(0)), G(φ(0)))
Ω = {(F,G) ∈ D∣H(F,G) ≤ Ho}
where We know that Ho > 0 from Theorem 2. Let Us assume that indeed Ω is in the interior of D.
Then, applying the same argumentation as in Lemma 3 combined with Theorem 2, all fixed points in
Z are stable. So applying Theorem 10 we get that the trajectory initialized at (F(θ(0)), G(φ(0))) ∈
Ω converges to a point in Z. It remains to prove our assertion about the set Ω:
Claim 1. Ω is in the interior of D.
Proof. We will argue that as (F, G) approaches the boundary of D, the value of H should become
unbounded. If this is true then for the finite upper bound of Ho, Ω should have no points close to the
boundary of H and thus it should be in the interior.
As (F, G) approach the boundary of D, at least one of the variables fi or gj approaches the end-
points points of Imfi (θi (0)) or Imgj (φj(0)) respectively. We will study the case of fi since the case
of gj is symmetrical. The endpoint fis can be either the supremum or the infimum of the gradient
ascent trajectory on fi or ±∞ if they do not exist. Let fis be the supremum or ∞ depending on if
the former exists. We can take the gradient ascent dynamics and apply Lemma 1 to get
fi = kVθifi(xθi(0) (Zi))k2
We know that fi(θi(t)) goes to fis when initialized at fi(θi(0)). Let us define the following function
fi
a(fi) =
pi
------------------dz
kVfi(Xθi(o)(z))k2
Observe that a = 1, thus limt→∞ a(fi(t)) = ∞. In other words
lim
t→∞
fi(t)
pi
-----------------dz
kVfi(Xθi(o)(z))k2
fis
pi
------------------d Z = ∞
kVfi(Xθi(o) (z))k2
Symmetrically if fis is the infimum or -∞, then the limit above would be -∞. In either case
fi → fis =⇒ Zpi kVfi(Xθi(pi(z))k2 dz →∞
For the last step it is important to note that pi is not at the boundary of D based on the safety
conditions. Therefore as (F, G) approach the boundary of D in the dynamics of Equation (3), at
least one of the terms of H goes to infinity. Also note that all the terms of H are individually non-
negative so no matter what the other variables in (F, G) are doing they cannot stop H → ∞.	□
□
Again, a special case of the above result is the standard convex-concave games:
Corollary 5. Let L(x, y) be strictly convex concave and Solution(L) is the non empty set of equil-
bria ofL. Under the continuous GDA dynamics (x(t), y(t)) converges to a point in Solution(L) as
t → ∞.
24
Under review as a conference paper at ICLR 2021
8.2.2 Connections to Hamiltonian Descent
In GANs numerous learning heuristics are being tested and explored. One technique that has par-
ticular interesting theoretical justification as well as practical performance is Hamiltonian Gradient
Descent (HGD). Understanding the convergence guarantees for HGD is an open research question
Maddison et al. (2018); Balduzzi et al. (2018); O’Donoghue & Maddison (2019). We provide some
new justification about its success in GANs by provably establishing convergence of a modified
version of HGD in a relatively simple but illustrative subclass of hidden convex concave games,
namely 2x2 hidden bi-linear games. This class of games is fairly expressive. Despite the restriction
of planar bi-linear competition in the output space, the hidden game can have an arbitrary number
of variables in the parameter space. It’s important to note that given the bi-linear nature of com-
petition, the classical GDA dynamics cycles instead of converging to the equilibrium as shown in
Vlatakis-Gkaragkounis et al. (2019)
More precisely, in the hidden 2x2 bi-linear game presented in Vlatakis-Gkaragkounis et al. (2019),
we have two functions f : RN → [0, 1] and g : RM → [0, 1] and two constants (p, q) ∈ (0, 1)2
where (p, q) is the fully mixed equilibrium of the bi-linear game. Without loss of generality, we are
interested in solving the following problem
min max
θ∈RM φ∈RN
(f (θ) - p)(g(φ) - q)
Defining L(θ, φ) = (f(θ) - p)(g(φ) - q), the dynamics of HGD are:
θ = - 1 VθkVφL(θ,φ)k2 - 1 VθkVθL(θ,φ)k2
21	21	(6)
Φ = -2VφkVθL(θ,φ)k2 - 2VφkVφL(θ,φ)k2
Observe that the second term of each right hand side would be zero in a classical bi-linear game but
involves second order derivatives of f and g in the case of hidden bi-linear games. To circumvent the
complexities of the second order derivatives and mimic the classical bi-linear game we will study a
modified version of Equation (6), namely:
θ= - 2 Vθ kVφL(θ,φ)k2	φ = - 2 VφkVθ L(θ,φ)k2	(7)
Employing an analysis similar to the one in Section 3.2, we get the following convergence result:
Theorem 13. Let (θ(0), φ(0)) be safe for (p, q). Then (f (θ(t)), g(φ(t))) converges to (p, q) under
the dynamics of Equation (7).
Proof. Simple substitution gives us
θ= -Vθf(θ)kVφg(φ)k2(f(θ) - P)
φ= -Vφg(φ)kVθf(θ)k2(g(φ) - q)
Applying Lemma 1 and following the same steps as before
f = -kVθf(Xθ(0)(f))k2kVφg(Xφ(0)(g))k2(f -P)
g = -kV0g(Xφ(o)S))k2kVef(Xφ(o)(f ))k2(g - q)
Once again we consider the function
H(f,g)=Zf
p
Z - P d
kVf(Xθ(0)(z))k2 Z +
Zg
q
_____口_________dz
kVg(Xφ(0)(z))k2
Simple substitution gives
H = -(f - P) (kVφg(Xφ(0)(g))k2(f - P)) - (g - q) (kVθ f(Xφ(0)(f ))k2(g - q))
A little bit of reorganization gives
HH = -(f - P)2kV6g(Xφ(0)S))k2 - (g - q)2kVef (Xθ(o)f))k2 ≤ 0
25
Under review as a conference paper at ICLR 2021
Thus, we get
Hl ≤ 0 in D = Imf (θ(0)) X Img(φ(0))
Similarly with the strict convex analysis of the previous section, if (f (θ(0)), g(φ(0))) = (p, q) then
the theorem holds trivially. Otherwise define
H0=H (f (θ(0)), g(φ(0)))
Ω = {(f,g) ∈ D∣H(f,g) ≤ Ho}
where we know that H0 > 0 from Theorem 2. Additionally, we can apply Claim 1 even in the new
.
dynamics, so Ω is in the interior of D. Since H ≤ 0, starting in Ω, it implies that H(f (t), g(t)) ≤ H
for t ≥ 0, so f (t), g(t) stays in Ω. Additionally, Ω is closed since it is a sublevel set of a continuous
function. Notice that the restriction of Ω on D does not affect the above properties since Ω is in the
interior of D. Thus Ω is a compact forward invariant set.
For a safe initialization (θ(0), φ(0),both ∣∣Vφg(Xφ(o)(g(t)))k, kVθ f (Xθ(o)(f (t)))k cannot go to 0
.
as this happens only at the boundaries of D which are outside Ω. So H = 0 only at (p, q) in Ω.
Therefore, applying Theorem 9, we get that (f (θ(t)), g(φ(t))) converges to (p, q)
□
8.3 Regularization and convergence
In this section, We show that even in the absence of strict convexity/concavity for both of the
operators, it is possible to achieve a positive convergence result by sacrificing the exactness
of a targeted equilibrium. In other words, we prove that by adding a small regularization
term, the new utility function becomes strictly convex strictly concave. Beside the guaran-
teed convergence of the “perturbed” L0, we can always choose sufficiently small magnitude
of regularization such that the new equilibria are arbitrarily close to the initial ones.
Theorem 6.	If L is a convex concave function with invertible Hessians at all its equilibria, then for
each > 0 there is a λ > 0 such that L0 has equilibria that are -close to the ones of L.
Proof. For any choice of λ > 0 we have that L0 is strictly convex strictly concave so the KKT
conditions are sufficient to determine its equilibria.
∂L(χ, y)
∂Xi
+ λxi = 0
∂L(χ, y)
dy
- λyj = 0
We can view the above set of constraints as a single vector constraint r(λ, χ, y) = 0. Note that by
assumption of the Hessians being invertible at all equilibria, L has a unique equilibrium (x*, y*).
Clearly we have that r(0, x*, y*) = 0. Observe that for the Jacobian of r at (0, x*, y*) with respect
to (x, y) we have that
D(x,y)r(0, x*, y*) = V2L(x*, y*)
and thus it is invertible. Invoking the Implicit function Theorem, there is a differentiable function g ,
defined in a small enough neighborhood of 0, that takes a λ and returns g(λ) = (x(λ), y(λ)) such
that r(λ, g(λ)) = 0. Thus for a small enough λ, we have that g returns the corresponding equilibria
of L0. By continuity of g, for all there is a δ > 0
∀0<λ<δ: ∣x(λ) - x(0)∣2 + ∣y(λ) - y(0)∣2 ≤2
But (x(0), y(0)) = (x*, y*) so the equilbrium of L0 has an -close equilibrium of L for λ < δ. By
strict convexity strict concavity of L0, it has a unique equilibrium as well. So the equilibria of L0
and L are e-close to each other.	□
26
Under review as a conference paper at ICLR 2021
The previous theorem highlights that small values of λ induce only small changes to the
equilibria of the hidden game. As is the case for classical convex concave games, larger
values of λ lead to (exponentially) faster convergence. To prove this for HCC games, We
provide a detailed upper and lower bound analysis of the gradients of f and gj.
Theorem 7.	Let (θ(0), φ(0)) be a safe initialization for the unique equilibrium of L0 (p, q). If
r(t) = kF(θ(t)) - pk2 + kG(φ(t)) - qk2
then there are initialization dependent constants c0, c1 > 0 such that r(t) ≤ c0 exp(-λc1 t).
Proof. Following the same analysis with the strict convex concave analysis of the previous section,
if (F(θ(0)), G(φ(0))) = (p, q) then the theorem holds trivially. Otherwise, since our initialization
is safe for (p, q), we can construct the H function as in Theorem 2 and prove that it has the following
property in D = {Imfi (θi(0))}iN=1 × {Imgj (φj (0))}jM=1
*
H ≤ L0(p, G)- L0(p, q) + L0(p, q) - L0(F, q)
≤-2 (kF(θ(t))-pk2 + kG(Φ(t)) - qk2)
≤ - 2 r(t)
Where the second step follows from L0(p, ∙) being λ strongly concave and L0(∙, q) being λ strongly
convex and q, p being the corresponding optima of these functions since (p, q) is an equilibrium.
Let us define
H0=H (F(θ(0)), G(φ(0)))
Ω = {(F,G) ∈ D∣H(F,G) ≤ Ho}
where we know that H0 > 0 from Theorem 2. Additionally, we can apply Claim 1 even in
.
the new dynamics, so Ω is in the interior of D. Since H ≤ 0, starting in Ω, it implies that
H(F(θ(t)), G(φ(t))) ≤ Ho for t ≥ 0, so (F(t), G(t)) stays in Ω. Additionally, Ω is closed since it
is a sublevel set of a continuous function. Notice that the restriction of Ω on D does not affect the
above properties since Ω is in the interior of D. Thus Ω is a compact forward invariant set.
For a safe initialization (θ(0), φ(0)), the following continuous functions must have a minimum and
maximum value on Ω respectively.
Kfi ≥ kVfi(Xθi(0)(∙))k2 ≥ Kfi
Kgj ≥ kVgj(Xφj(0)(∙))k2 ≥ Kgj
Observe that the minima and maxima must be all greater than zero , since both
kVφjgj(Xφj(0)(g(t)))k, kVθi fi(Xθi(0)(f (t)))k cannot go to 0 as this happens only at the bound-
aries of D which are outside Ω.
Let us define
K = min{ min Kfi, min Kgj }
1≤i≤n	1≤j≤m j
K = max{ max Kfi , max Kgj }
1≤i≤n	1≤j≤m
Observe that K ≥ ∣∣Vfi(Xθi(o)(∙))k2 ≥ K in this interval. Thus we can write
(fi(θi(t))- Pi) ≥ [ fi(θi(t))	Z- Pi d ≥ (fi(θi(t))- Pi)
2κ	≥ Jpi	kVfi(Xθi(o)(z))k2 z ≥	2K
Repeating the same argument for all fi and gj we have that
M ≥ H(F(θ(t)), G(Φ(t))) ≥ 2K
Thus we can extend our analysis
H ≤ -λr(t) ≤ -2κλH(t) ⇒ H(t) ≤ Hoe-λκt ⇒ r(t) ≤ 2 X K X H°e-λκt
□
27
Under review as a conference paper at ICLR 2021
9 Applications
9.1	Connecting GANs and Hidden Convex-Concave Games
At the heart of many GAN formulations like the standard GAN Goodfellow et al. (2014b), f-GAN
Nowozin et al. (2016) and Wassertein GAN (WGAN) Arjovsky et al. (2017) lies a classical convex
concave game in the operator output space. Indeed for the realizeable case Goodfellow et al. (2014b)
used the underlying convexity properties to find the Nash equilibria of standard GAN and Farnia &
Ozdaglar (2020) did the same thing for the f-GAN and WGAN. Perhaps surprisingly, neither work
references explicitly the convex concave nature of the operator output space game or von Neumann’s
minimax theorem. To highlight the significance of von Neumann equilibria as a solution concept
for GANs, We show how the optimal G* and D* can be derived separately from each other by
solving the corresponding min-max (max-min) problems. This allows one to independently verify
the validity of von Neumann’s minimax theorem and its generalizations for GANs. We also extend
our analysis to a wide class of non-realizeable cases as well.
In practice however, as noted explicitly by Goodfellow (2017), the updates in GAN training happen
in the parameter space giving rise to a HCC game. This has exactly motivated studying the learning
dynamics of HCC games in Section 3.
Thus, in this section, we present these connections between Hidden Convex-Concave games and the
different architectures of Generative Adversarial Networks. More specifically, we start by exploring
the structure of GANs and we verify their hidden convex-concave intrinsic form.
1.	Under this scope of hidden games, the strong (or even strict) convexity/concavity ofat least
one of the players (Discriminator/Generator) in combination with the convergence results
of the following sections provide some theoretical explanation about the convergence prop-
erties of those architectures even under the vanilla Gradient Descent-Ascent Dynamics.
2.	To indicate the relation of Von-Neumann solution with this hidden model, we leverage this
hidden convex-concave structure in order to compute the well-known both min max and
max min optima of GANs under the realizability or not assumption. The results of this
section are summarized in the following table:
Type of GAN	G*	D*	Hidden Structure
GAN GAN	pdata arg minG∈G JSD(pdata||pG)	1 2 Pdata Pdata +PG *	Linear VS Strongly-Concave Linear VS Strongly-Concave
f-GAN	pdata	f0(1)	Linear VS Concave
f-GAN	arg minG∈G Df(pdata||pG)	f 0 P Pdata Pg* )	Linear VS Concave
WGAN	pdata	c	Linear VS Linear
WGAN	arg minG∈G EMD(pdata||pG)	-	Linear VS Linear
Table 1: Pdata represents the target data distribution. G* is the min-max generator and D* is the
max-min discriminator. JSD denotes the Jensen-Shannon divergence, D f the f -divergence for the
convex function f and EMD the earth mover distance and c the constant discriminator. xGAN,
xGAN correspond to the realizable and the non-realizable case accordingly. - indicates the lack of
a closed form solution for D* of WGAN.
28
Under review as a conference paper at ICLR 2021
In the following three subsections, We analyze both the derivation of arg min max and
arg max min for the “vanilla-GANs”, f-GANs, W-GANs using min-max optimization ar-
guments based on the Minimax Theorem for convex-concave functions. More precisely,
1.	In the LemmaS 4, 9 and 14, we present the optimal discriminators which consist
the best-response for the case of a fixed generator. In all these maximization prob-
lems, typically each D(X) is decoupled and DG(x) is derived by the hidden con-
cavity of the discriminator architecture.
2.	In the LemmaS 5, 10 and 15, we present the optimal generators which consist the
best-response for the case of a fixed discriminator. In all these minimization prob-
lems, typically the generator can cheat the fixed discriminator by producing greed-
ily a distribution only over the restricted subset of the points for which the discrim-
inator has the highest confidence about their originality.
3.	In the LemmaS 6,11 and 16, we leverage lemmas of (Item 1) to understand the
form GAN's utility function which corresponds typically to JSD, f-divergence and
Wasserstein distance which donate their name to their GAN architecture as well.
Thus, it is then trivial to show that Pdata is the optimal choice in the realizable case.
4.	In the LemmaS 7, 12 and 17, on the other side of the coin, we emphasize to derive
the minmax solutions too. Our proof strategy invokes the partition to two basic sets,
Sg^ and SG^ ,the “preferable” or not data points by the generator. Leveraging the
concavity part of the objective, we show that the best strategy for the discriminator
is to label all the points uniformly with the same confidence in order to incentivize
the generator to expands its support to the maximum possible.
5.	In the LemmaS 8 and 13, we analyze the non-realizable case. One the one hand
using Item 3 we are able to compute the arg max min generator G*. To conclude
about the arg min max discriminators we apply the Von Neumann,s Minimax the-
orem to prove D* = Best-Response(G)
9.1.1 GAN
The utility of the zero-sum game V (G, D) for the distribution pdata over the discrete setN is
V (G, D) = X pdata(x) log(D(x)) + X pG(x) log(1 - D(x))
x∈N
x∈N
On the one hand, it is easy to check that for a fixed discriminator D, the utility function is linear
over the pG operator. On the other hand, for a fixed generator G, the utility function is of the form
a log(D) + b log(1 - D) which is strongly-concave.
We start our work with the following lemmas
Lemma 4 (Goodfellow et al. (2014b)). For a fixed generator G the optimal discriminator is
DG (X)=	Pdata⑺ ,、
pdata(x) + pG(x)
Proof. Observe that the optimization problem for each D(X) is decoupled. Thus
DG (x) = arg maxPdata(X) log(D)+ PG(x)log(1 - D)
D∈[0,1]
By concavity the unique maximum of the above is given by
爪(X)=	Pdata(X)—
G( )	Pdata(X) + PG(X)
□
Lemma 5. For a fixed discriminator D, any distribution supported only on
Sg*d = {x ∈N : ∀X0∈N D(x) ≥ D(x0)}
is an optimal generator when it is allowed to choose any distribution over N.
29
Under review as a conference paper at ICLR 2021
Proof. Observe that for a fixed discriminator, the optimal generator optimizes
X pG(x) log(1 - D(x))
x∈N
since the other term is independent of the generator. Let us define the following
Dmax = max D(x)
x∈N
Then we have that
pG(x) log(1 - D(x)) ≥ log(1 - Dmax)
x∈N
with the equality being true only for distributions supported only on Sg^ .	□
Lemma 6 (Goodfellow et al. (2014b)). The min-max generator is the following distribution
G* = argminJSD(P data ||p G).
G∈G
Proof. We can substitute in V (G, D) the optimal discriminator from Lemma 4. Thus we get
V (GDG)=XN Pdata(X)Iog (Pdd⅛b)+XN PG(X)Iog (pdα⅛G‰)
We can now prove that
V(G, DG) = - log⑷ + KL (Pdata||PG ∖pdata)
+ KL (pg||PGFa)
- log(4) + 2JSD(Pdata||PG)
By minimizing V(G, DG), the result follows trivially.
□
Lemma 7. The max-min discriminator is
∀x ∈N : D*(x)
1
2
when the generator is allowed choose any distribution over N,
Proof. We can substitute in V(G, D) the optimal generator from Lemma 5
V(GD,D)=log(Dmaχ) E Pdata(X) +log(1 - Dmax) E PG(X)
x∈SGW	x∈SGW
+ E Pdata(X) log(D(x)) + E PG(x)log(1- D(X))
x/SgW)
x/SGD 0
Observe that for X ∈/ SGW , if D takes more than two values then setting D equal to the highest of
the them for all X ∈/ SGW improves utility. So for an optimal discriminator we would have a single
value Dmax > Dmin . In the end we have that
x ∈ SGD =⇒ D*(X)= Dmin
X ∈ SGD =⇒ D*(x) = Dmax
Observe that for any combination of Dmax and Dmin with Dmax > Dmin, the constant discrimi-
nator Dmax has higher utility. Therefore we can focus our attention on the constant discriminator
Dconst(X) = D
V(GDconSt，Dconst) = log(D) + log(1 - D)
The optimal value for D is 2 and as a result
D*(x) = 2
□
30
Under review as a conference paper at ICLR 2021
Lemma 8 (Non-realizable case). Ifwe assume that choice of generator G is restricted in G, a convex
compact subset of the |N | dimensional simplex, such that pdata ∈/ G. Then
(G*,D*) = (argminJSD(P data ||p G),	P Paaa	)5
∖ G∈G	PPata + PG* )
Proof. We cannot readily apply von Neumann’s minimax theorem since the V (G, D) may be infinite
at the boundary points of D = (0, 1)|N | for the discriminator. We can still apply Fan’s Minimax
Theorem
min sup V (G, D) = sup min V (G, D).
G∈G D∈D	D∈D G∈G
It is easy to check that Lemma 6 holds even in the non-realizable case. As a result, the generator is
minimizing JSD(Pdata||PG) whose value is finite. Clearly the quantities above are finite. Thus there
exists a real number v , the value of the game, such that:
∫∀D ∈ D : V(G*,D) ≤ V = V(G*,D*) (A)I
∀GG ∈ G : V(g,D*) ≥ v = V(G*,D*) (B)J
for G* the minimizer of JSD(Pdata||pg) and a D* ∈ [0,1] |N|. Now applying Lemma 4, We have that
D = Best-ResPonse(G*) = -Pdata——.
Pdata + PG*
Additionally, by the optimality of the response and the consequence (A) of Minimax Theorem it
holds that V(G*,D) = v. Finally, since V(G*, ∙) is strongly concave, all other discriminators
receive value less than v and are not optimal. Thus
D* = D=	Pdata
Pdata+PG*
□
9.1.2 F-GAN
The utility of the zero-sum game V(G, D) for the distribution Pdata over the discrete setN is
V(G, D) = X Pdata(x)D(x) - X PG(x)f* (D(x))
x∈N	x∈N
We will assume that f is a strictly convex function with f(1) = 0. On the one hand, it is easy
to check that for a fixed discriminator D, the utility function is linear over the PG operator. On
the other hand, for a fixed generator G, the utility function is of the form aD - bf* (D) which is
strictly-concave.
We start our work with the following lemmas
Lemma 9 (Nowozin et al. (2016)). For a fixeP generator G the optimal Piscriminator is
DG (X) = f0( * )
PG (x)
Proof. Observe that the optimization problem for each D(x) is decoupled. Thus
DG* (x) = arg maxPdata(x)D - PG(x)f* (D)
D
By concavity the unique maximum of the above is given by Fermat criterion
DG* (x) = ((f*)0)-1
P Pdata(X)
k PG(X)
P Pdata(X))
k PG(X)J
□
5We note that D*, may take the value 1 for some X ∈ N if the generator G* does not have full support.
Assigning D(x) = 1 for some x may lead to infinite utilities in general. We prove however for that the pair
(G*, D*) this is not the case. We thus consider that pair an equilibrium.
31
Under review as a conference paper at ICLR 2021
Lemma 10. For a fixed discriminator D, any distribution supported only on
SGD = {x ∈N ： ∀x0 ∈ N f*(D(x)) ≥ f*(D(x0))}
is an optimal generator when it is allowed to choose any distribution over N.
Proof. Observe that for a fixed discriminator, the optimal generator optimizes
-X PG(x)f*(D(x))
x∈N
since the other term is independent of the generator. Let us define the following
Fmax = max f *(D(x))
x∈N
Then we have that
- X pG(x)f* (D(x)) ≥ -Fmax
x∈N
with the equality being true only for distributions supported only on Sg^ .	□
Lemma 11 (Nowozin et al. (2016)). The min-max generator is the following distribution
G* = argminDf(pdata||pG).
G∈G
Proof. We can substitute in V (G, D) the optimal discriminator from Lemma 9. Thus we get
V(G,DG) = X Pdata(x)f0 (Pd^) - X PG(x)f * (f
G	x∈N	pG (x)	x∈N
P Pdata(X)
k PG(X)
We will first prove that:
V (G, DG*) = Df (Pdata||PG)
Let’s recall firstly the definition of f-divergence:
Df (Pdata ||PG) =	PG(X)f
x∈N
P Pdata(X)
k PG(X)
Since f is convex and lower semi-continuous, Frenchel convex duality guarantees that we can write
f in terms of its conjugate dual as f(u) = supv∈R uv - f* (v) . Equivalently we get:
Df(Pdata||PG) =	PG(X) sup
x∈N	v∈R
KPdata(X)
PG(X)
v-f*(v)
sup {Pdata(X)v - f *(v)PG(X)}
x∈N v∈R
XPdata(X)U*) - £PG(X)f* (MT))
The last line follows arguments similar to Lemma 9 applied for each term. By minimizing
V(G, DG), the result follows trivially.	□
Lemma 12. The max-min discriminator is
∀X ∈ N : D*(X) = f0(1)
when the generator is allowed choose any distribution over N.
32
Under review as a conference paper at ICLR 2021
Proof. We want to substitute in V (G, D) the optimal generator from Lemma 5. Observe that for
all X ∈ Sg^, We may not have all D(X) to be equal. Only the values of f * are guaranteed to be
equal, f *(D(X)) = Fmax. However, if there are two distinct D values then we can always pick the
higher one and improve utility. Thus we can focus on discriminators that are constant over Sg' . Let
DFmax be the corresponding value
V (G*D, D) = DFmax Xpdata(X)-f*(DFmax) X pG(X)
x∈Si	x∈Si
+ X Pdata(X)D(X)- X PG(x) f *(D(X))
x∈SG'D	x∈SGD ∖~{0"*z
Observe that for X ∈ Sg' , if D takes more than two values then setting D equal to the highest of
the them for all x ∈ SGx improves utility. So for an optimal discriminator we would have a single
value DFmin withf*(DFmin) < f*(DFmax). As a result
X ∈/ SGxD =⇒ D* (X) = DFmin
X ∈ SGxD =⇒ D* (X) = DFmax
We now have two cases. For any combination with DFmin > DFmax , the constant discriminator
D(X) = DFmin has higher utility. Symmetrically, for any combination with DFmax > DFmin, the
constant discriminator D(X) = DFmax has higher utility. Thus the optimal discriminator is constant.
Plugging in the constant discriminator Dconst(X) = D we get
V (G*Dconst, Dconst) = D + f* (D)
The optimal value for D follwoing the approach of Lemma 9 is f0(1) and as a result
D*(X)=f0(1)
□
Lemma 13 (Non-realizable case). Assume that f ∈ C1 is strictly convex and limχ→0+ Xf (ɪ) exists
and is finite6. If the choice of generator G is restricted in G, a convex compact subset of the |N|
dimensional simplex, such that Pdata ∈/ G then
(G*,D*) = (argminDf(Pdαtα||pG),f0 (PaaX
G∈G	PGx
Proof. We cannot readily apply von Neumann’s minimax theorem since the V (G, D) since D =
R|N | is not compact for the discriminator. We can still apply Fan’s Minimax Theorem
min sup V (G, D) = sup min V (G, D).
G∈G D∈D	D∈D G∈G
It is easy to check that Lemma 12 holds even in the non-realizable case. As a result, the generator
is minimizing Df(Pdata||PG) whose value is finite under the assumptions we made on f. Clearly the
quantities above are finite. Thus there exists a real number v, the value of the game, such that:
∀D ∈ D : V (G*, D) ≤ v = V (G*, D*) (A)
∀G ∈ G : V (G, D*) ≥ v = V (G*, D*) (B)
for G* the minimizer of Df(Pdata∣∣Pg) and a D* ∈ R|N|. Now applying Lemma 9 we have that
D = Best-Response(G*) = f 0 (Pdata(X)
PGx (X)
Additionally, by the optimality of the response and the consequence (A) of Minimax Theorem it
holds that V(G*, D) = v. Finally, assuming that f is strictly convex we get that V(G, ∙) is strictly
concave, Best-Response(G*) is unique and thus
D* = D= f0 (Pdata(X)
P ∖PGX (x)
□
6This assumption guarantees that the Df is always finite even if the distribution chosen by the generator is
not fully supported on N. This in turn guarantees that D* is also finite resulting in a meaningful equilibrium.
Unbounded divergences like KL are known to be problematic for GANs even in practice Arjovsky et al. (2017).
33
Under review as a conference paper at ICLR 2021
9.1.3 WGAN
The utility of the zero-sum game V (G, D) for the distribution pdata over the discrete metric space
(N, dist)
V(G D)= EX〜pdata[D(X)] - EX〜PG D(X)]
=	(pdata(x) - pG(x))D(x) where kDkLip ≤ 1
x∈N
On the one hand, it is easy to check that for a fixed discriminator D, the utility function is linear
over the pG operator. On the other hand, for a fixed generator G, the utility function is linear over
D.
We start our work with the following lemmas
Lemma 14 (Arjovsky et al. (2017)). For a fixed generator G the optimal discriminator is a solution
of the following linear program
maximize over D(∙)	X(P data (x) - P G (X))D(X)
x∈N
subject to	|D(X) - D(X0)| ≤ dist(X, X0), ∀X, X0 ∈ N
where the optimal value of the LP is the Earth mover’s distance between Pdata and PG.
Proof. Indeed, by definition any solution of the above LP is an optimal discriminator over a fixed
generator G. To complete the proof of the statement, we recall that Earth Mover’s distance of
(Pdata , PG ) is equal to
min	E(x,χo)〜∆[dist(X,X 0)].
∆∈Coupling(pdata,pG)
Now if we consider the dual formulation of the Wasserstein distance, then the Kantorovich duality
Evans (1997); Villani (2008) implies that the above linear program consists exactly the dual linear
program which computes the Earth Mover's distance.	□
Lemma 15. For a fixed discriminator D, any distribution supported only on
Sg*° = {x ∈N : ∀x0∈N D(x) ≥ D(x0)}
is an optimal generator when it is allowed to choose any distribution over N.
Proof. Observe that for a fixed discriminator, the optimal generator optimizes
-	PG(X)D(X)
x∈N
since the other term is independent of the generator. Let us define the following
Dmax = max D(X)
x∈N
Then we have that
-	PG(X)D(X) ≥ -Dmax
x∈N
with the equality being true only for distributions supported only on Sg^ .	□
Lemma 16 (Arjovsky et al. (2017)). The min-max generator is the following distribution
G* = argminEMD(P data ,P G).
G∈G
Proof. We can substitute in V(G, D) the optimal discriminator from Lemma 14. Thus we get
V (G,DG ) = EMD(pdata,PG)
By minimizing V(G, DG), the result follows trivially.	□
34
Under review as a conference paper at ICLR 2021
Lemma 17. The max-min discriminator is
∀x ∈ N : D*(x) = c,Constant function
when the generator is allowed choose any distribution over N,
Proof. We can substitute in V (G, D) the optimal generator from Lemma 5
V(GD ,D) =Dmax X pdata(x) - Dmax X pG(x)
x∈Sg*3	x∈Sg*3
+ E Pdata(X)D(X)- E PG(x) D(X)
x/SGD	x∈SGD ^ɔɔ―
Observe that for X ∈ Sg' , if D takes more than two values then setting D equal to the highest of
the them for all x ∈ Sg' improves utility. So for an optimal discriminator We would have a single
value Dmax > Dmin . In the end we have that
x ∈ SGD =⇒ D*(X)= Dmin
x ∈ SGD =⇒ D*(x) = DmaX
Observe that for any combination of Dmax and Dmin with Dmax > Dmin, the constant discrimi-
nator Dmax has higher utility. Therefore we can focus our attention on the constant discriminator
Dconst(X) = D, where the optimal value is exactly zero.
V (GDcOnSt，DConSt) = 0
Finally, it is easy to check that the choice of constant discriminator satisfies trivially the Lipschitz
constraints, i.e ∣Dconst(∕) - Dconst(X0)I = 0 ≤ dist(x, x0) for any metric function dist.	口
9.2 GANs and Hidden Constrained Optimization
In the following section, we will generalize the results of Section 3.2 and Section 3.3 for the
case of vanilla GAN of Goodfellow et al. (2014a) whose objective is linear-strong-concave
where the maximization part is constrained in the distributional simplex. More precisely,
min max
Pg(x)≥0,	D∈(0,1)lNl
px∈N PG(X)=I
V(G, D) = E Pdata(X) log(D(X)) + E Pg(x) log(1 - D(x))
χ∈N	χ∈N
At a first glance, by rewriting the equivalent Langrangian formulation of the aforementioned
constrained min-max problem we can see that strong-concavity property does not hold any
more. However our following theorem shows that by exploiting further the structure of
Goodfellow et al. (2014a)'s architecture a convergence result is possible.
Theorem 14. Let V(Genθ, Discφ) be Goodfellow GAN as described in Section 4, where G, D use
sigmoid activations. Then for a fully mixed distribution Pdata, (F(t) = Genθ(t), G(t) = Discφ(t))
converges to (pdata, 2 1∣n∣ ) as t → ∞ under the dynamics ofEquation (1).
Proof. Let us write down our original objective
min max
Pg(x)≥0,	D∈(0,1)lNl
Px∈N pG(x)=1
V(G, D) =	Pdata(X) log(D(X)) +	PG(X) log(1 - D(X))
x∈N	x∈N
In order to remove the constraints from the objective above, we plan to make use of a Lagrange
multiplier. We remind the reader that since both the discriminator and the generator use the sig-
moid activations, we only have to capture the Px∈N PG(X) = 1 constraint. Thus, our equivalent
Langragian is:
min max	L(F, G,λ) = Pdata> log(G(φ)) + F(θ)> log(1 - G(φ)) + λ(F>1∣N∣ - 1)
θ∈RlNl φ∈RlNl,λ∈R
35
Under review as a conference paper at ICLR 2021
where
F(θ)
G(φ)
fl(θl) f2(θ2)…f∣N∣(θ∣N∣)]
[gι(φι) g2(φ2)	…g∣N∣(φ∣N∣)]
and fi and gj are sigmoid functions and θi and φj are their one dimensional inputs. Let’s write again
the equivalent dynamics of Equation (3) for the sigmoid activations and the Langrage multiplier.
Applying the same steps with Theorem 3 for sigmoids:
	
fi2(i-fi)2 f PG) ∀i ∈ [∣N∣]
gj
gj(1-gj)2∂j(F, G) ∀j ∈ [∣N∣] >
.
λ
P|iN=0| fi-1
Since all initializations are safe in this game, our “generalized” Lyapunov function:
H (F G λ) = X [fi	Z - Pdata(Xi)dz + X ∕gj Z - 1/2 dz + (λ - λ*)1
(,,)= i=0 Alata(Xi) Z2(1-Z)2	+ j=0Λ∕2 Z2(1-Z)2	+^^
where λ* is the Langrange multiplier at the equilibrium of the non-hidden game and Xi is the i-th
element of N. Applying the same steps as in Lemma 3 we get that GDA approaches the largest
invariant set E of points (F, G, λ) that have the following properties
L(Pdata, G, λ) = L (Pdata, g 1∣N∣, λ*)
L fF, 21∣N∣,λ") = L (Pdata, q 1∣N∣, λ*)
For the first equality, we have that the value of λ does not affect L when the generator respects the
sum to one constraint. Thus
L(Pdata,G,λ) = L(Pdata ,G,λ*)
Then We can observe that L(Pdata, G, λ*) is strictly concave in G and given that 11 1∣n∣ is its unique
minimum we have that
L(Pdata, G,λ" ) = L (Pdata, 2 1∣N∣ ,λ* )-------⇒ G = 2 1∣N∣
.
Given that E is an invariant set and G is constant in E, we have that G = 0. In other words,
0=21 (1- 2) ∂j (F，21"λ)	∀j ∈[1N1]
As a consequence we have that
∂gL 卜,2 1∣N∣,λ) = 0 =⇒ fj = Pdata(xj) ∀j ∈ [|N|]
.
Once again, given that E is an invariant set and F is constant in E, we have that F = 0
0 = Pdata(Xi)2 (1 - Pdata(Xi)) ∂ff (Pdata, 21∣N∣, λ)	∀i ∈ [|N|]
This leads to
∂f (pdata, 21∣N∣,λ) =0 =⇒ λ = log(2) ∀i ∈ [|N|]
Observe that by the optimality conditions of the non-hidden game, λ* needs to satisfy the same
equation and thus λ = λ*. Clearly we have that
(F,G,λ) ∈ E =⇒ (F,G,λ)= (Pdata, 21∣N∣,λ*
Thus the dynamics converge to the unique equilibrium of the hidden game.
□
36
Under review as a conference paper at ICLR 2021
9.3 Zero-Sum Games
We close this section with an application of our regularization machinery in hidden bilinear games.
Hidden bilinear zero-sum games were introduced by Vlatakis-Gkaragkounis et al. (2019) and they
are formally defined as:
Definition 11 (Hidden Bilinear Zero-Sum Game). In a hidden bilinear zero-sum game there are
two players, each one equipped with a smooth function F : Rn → RN and G : Rm → RM and a
payoff matrix UN ×M such that each player inputs its own decision vector θ ∈ Rn and φ ∈ Rm and
is trying to maximize or minimize r(θ, φ) = F (θ)>UG(φ) respectively.
For the special case of hidden bilinear games, Vlatakis-Gkaragkounis et al. (2019) proved that if
the dimension of the game is greater or equal than two like (e.g. akin to Rock-Paper-Scissors)
then GDA dynamics tend to “cycle” through their parameter space with an even more complex
behavior than a typical periodic trajectory. Specifically, the system is formally analogous to PoinCare
recurrent systems (e.g. many body problem in physics). In contrast, leveraging Theorem 6, we
know that by adding a small regularization term we can “break” the cycling behavior and converge
to an approximate Nash Equilibrium. We close this section by presenting a comparison between
the optimization portraits of GDA dynamics with the absence or not of a regularization for the
archetypical game of Rock-Paper-Scissors:
Figure 5: Trajectories of a single player using gradient-descent-ascent dynamics for a hidden bilin-
ear game L(F(θ), G(φ)) = F> (θ)AG(φ) where A is the classical Rock-Paper-Scissors table and
F, G have the sigmoid activations. The two left figures present the Poincare recurrence for different
initializations of the dynamics, a behavior consistent with the Lyapunov stability of Theorem 2. On
the other hand, the two figures on the right illustrate convergent to the mixed Nash equilibrium exe-
cutions which exploit the regularization tools as described in Section 3.3. The regularization terms
added are centered at the mixed equilbrium of the game, leading to convergence to the unmodified
equilibrium of the Rock-Paper-Scissors game.
37