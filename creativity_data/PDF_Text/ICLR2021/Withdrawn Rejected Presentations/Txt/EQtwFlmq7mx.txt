Under review as a conference paper at ICLR 2021
Stochastic Proximal Point Algorithm for
Large-scale Nonconvex Optimization:
Convergence, Implementation, and Applica-
tion to Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We revisit the stochastic proximal point algorithm (SPPA) for large-scale nonconvex
optimization problems. SPPA has been shown to converge faster and more stable
than the celebrated stochastic gradient descent (SGD) algorithm, and its many
variations, for convex problems. However, the per-iteration update of SPPA is
defined abstractly and has long been considered expensive. In this paper, we show
that efficient implementation of SPPA can be achieved. If the problem is a nonlinear
least squares, each iteration of SPPA can be efficiently implemented by Gauss-
Newton; with some linear algebra trick the resulting complexity is in the same
order of SGD. For more generic problems, SPPA can still be implemented with
L-BFGS or accelerated gradient with high efficiency. Another contribution of this
work is the convergence of SPPA to a stationary point in expectation for nonconvex
problems. The result is encouraging that it admits more flexible choices of the step
sizes under similar assumptions. The proposed algorithm is elaborated for both
regression and classification problems using different neural network structures.
Real data experiments showcase its effectiveness in terms of convergence and
accuracy compared to SGD and its variants.
1	Introduction
Algorithm design for large-scale machine learning problems have been dominated by the stochastic
(sub)gradient descent (SGD) and its variants (Bottou et al., 2018). The main reasons are two-fold:
on the one hand, the size of the data set may be so large that obtaining the full gradient information
is too costly; on the other hand, solving the formulated problem to very high accuracy is typically
unnecessary in machine learning, since the ultimate goal of most tasks is not to fit the training data
but to generalize well on unseen data. As a result, stochastic algorithms such as SGD has gained
tremendous popularity recently.
There has been many variations and extensions of the plain vanilla SGD algorithm to accelerate its
convergence rate. One line of research focuses on reducing the variance of the stochastic gradient,
resulting in famous algorithms such as SVRG (Johnson and Zhang, 2013) and SAGA (Defazio
et al., 2014), which results in extra time/memory complexities of the algorithm (significantly). More
recently, adaptive learning schemes such as AdaGrad (Duchi et al., 2011) and Adam (Diederik
P. Kingma, 2014) have shown to be more effective in keeping the algorithm fully stochastic and
light-weight. In terms of theory, there has also been surging amount of work quantifying the best
possible rate using first-order information (Lei et al., 2017; Allen-Zhu, 2017; 2018a;b), as well as its
ability to obtain not only stationary points but also local optima (Ge et al., 2015; Jin et al., 2017; Xu
et al., 2018; Allen-Zhu, 2018).
1.1	Stochastic proximal point algorithm (SPPA)
In this work, we consider a different type of stochastic algorithm called the stochastic proximal
point algorithm (SPPA), also known as incremental proximal point method (Bertsekas, 2011a;b) or
stochastic proximal iterations (Ryu and Boyd, 2014). Consider the following optimization problem
1
Under review as a conference paper at ICLR 2021
with the objective function in the form of a finite sum of component functions
minimize
θ ∈Rd
1 n
-∑h (O) = L(θ).
n幺
(1)
SPPA takes the following Simple form:________________________________________________________
1:	repeat
2:	randomly draw i from {1,...,孔}
3:	d+1 - argmine λt I (O) + (1/2)k O - O t k2 = Proxzf ιi (仇)
4:	until convergence
The update rule in line 3 is called the proximal operator of the function λt I evaluated at Ot. This is
the stochastic version of the proximal point algorithm, which dates back to Rockafellar (1976).
Admittedly, SPPA is not as universally applicable as SGD, due to the abstraction of the per-iteration
update rule. It is also asking for more information from the problem than merely the first-order
derivatives. However, with the help of more information inquired, there is also hope that it provides
faster and more robust convergence guarantees.
As we will see in numerical experiments, SPPA is able to achieve good optimization
performance by taking fewer number of passes through the data set, although it
takes a little more computations for each batch. We believe in many cases it is
worth trading off more computations for fewer memory accesses.
To the best of our knowledge, convergence analyses of SPPA has only been studied for convex
problems (Bertsekas, 2011a; Ryu and Boyd, 2014; Bianchi, 2016). Their study shows that SPPA
converges somewhat similar to SGD for convex problems, but the updates are much more robust
to instabilities in the problem. Most authors also accept the premise that the proximal operator is
sometimes difficult to evaluate, and thus proposed variations to the plain vanilla version to handle
more complicated problem structures (Wang and Bertsekas, 2013; Duchi and Ruan, 2018; Asi and
Duchi, 2019b; Davis and Drusvyatskiy, 2019).
In terms of nonconvex optimization problems, there is very little work until very recently (Davis
and Drusvyatskiy, 2019; Asi and Duchi, 2019a). However, their convergence analysis is somewhat
unconventional. Typically for a nonconvex problem, we would expect a theoretical claim that the
iterates generated by SPPA converges (in expectation) to a stationary point. This is not easy, and the
result given by (Davis and Drusvyatskiy, 2019) and (Asi and Duchi, 2019a) defined an imaginary
sequence (that is not computed in practice) {eOt } as eOt = arg mine λt L(O) + (1/2)kO - Ot k2, i.e., the
proximal operator of the full loss function from the algorithm sequence {Ot }. Their results show that
this imaginary sequence {Ot } converges to a stationary point in expectation.
1.2 Contributions
There are two main contributions we present in this paper; efficient implementations of proximal
operator update for SPPA with application on regression and classification problems, and convergence
to a stationary point for SPPA algorithm for general non-convex problems. The cost of this abstract
per-iteration update rule has been the burden for SPPA algorithm, even though it has been shown to
converge faster and more stable than the celebrated SGD algorithm. In this paper, we show that it is
actually not a burden when the per-iteration update is efficiently implemented.
In the implementation section we will discuss the implementation of abstract per-iteration update
rule for non-linear least squares (NLS) problem and other non-linear problems. We present two
different implementations for these two different categories of problems. SPPA-Gauss Newton
(SPPA-GN) and SPPA-Accelerated, respectively for regression with nonlinear least squares and
classification problems. We apply SPPA to a large family of nonconvex optimization problems and
show that the seemingly complicated proximal operator update can still be efficiently obtained. Both
implementations give results that are comparable with the state of the art stochastic algorithms.
On the other hand, there is large group of nonlinear problems that are not necessarily expressed as
a NLS problem (non-NLS). Many of the classification problems in deep learning applications as
of today, use different type of loss functions than mean least squared error. Hence, we suggest an
2
Under review as a conference paper at ICLR 2021
Figure 1: CIFAR10: cross entropy loss
Figure 2: CIFAR10: prediction accuracy
alternative to SPPA-GN where we implement the per-iteration update rule based on well-known
stochastic optimization algorithms, preferably with the ones that take less number of iterations to
converge like L-BFGS (Liu and Nocedal, 1989).
As a second contribution, we show that SPPA, when applied to a nonconvex problem, converges to a
stationary point in expectation, as informally stated as follows
Theorem 1. (informal.) For SPPA with fixed λt 二λ, the expected gradient converges to a region
where the norm is bounded with radius proportional to 尢 For diminishing step sizes such that
1. T
lim ʒ-------r,
T →∞ ∑t=ι 4-1
the expected gradient converges to zero.
Finally, we apply SPPA with the novel efficient updates to some classical regression and classification
using 4 different data sets for neural network training. Detailed algorithmic descriptions are provided,
and we show the outstanding performance of SPPA when effectively executed.
As a sneak peek of the numerical performance of the proposed SPPA implementation, Figures 1
and 2 shows the cross entropy loss and prediction error on the test set over the progression of SPPA
verses various baseline algorithms. SPPA indeed converges much faster than all SGD-based methods,
achieving peak prediction accuracy after only going through the data set approximately 10 times. We
should stress that the per-iteration complexity of SPPA is higher than other methods, since it tries to
solve a small optimization problem rather than a simple gradient update. However, as we have argued
before, in many cases it is worth trading off more computations for fewer memory accesses.
2 Efficient Implementation
In this section we introduce several efficient methods to calculate the proximal operator update given
in pseudo-code of SPPA section 1.1 line 3. The two methods emerge from the question of ‘How can
we implement the proximal operator update efficiently?’. The answer to this question depends on
the type of the objective function. We considered the problems in two different categories, nonlinear
least squares (NLS) and other generic nonlinear problems.
2.1 SPPA-GN for nonlinear least squares
A nonlinear least squares (NLS) problem takes the following form
1 A 1
minimize - Y -(φi(θ))2,
θ∈Rd	〃4，2
i=1
(2)
where each 亚 is a general nonlinear function with respect to 仇 Itisa classical nonlinear programming
problem (Bertsekas, 1999; Boyd and Vandenberghe, 2018) with many useful applications, including
3
Under review as a conference paper at ICLR 2021
least squares neural networks (Van Der Smagt, 1994), where each φi corresponds to the residual
of fitting for the 力th data sample for regression. We will show that problems of this form can be
efficiently executed by SPPA-GN, despite its seemingly complication.
To apply SPPA to NLS, the main challenge is to efficiently evaluate the proximal operator
d+1 J arg min g(φi (J ))2 + k-∖∖- -d k2.	⑶
σ 2	2
Notice that this function itself is a nonlinear least squares objective, although with only one component
function together with the proximal term.
The traditional wisdom to solve a NLS problem is to apply the Gauss-Newton (GN) algorithm: at each
iteration, we first take a first-order approximation of the vector-valued function inside the Euclidean
norm, and set the update as the solution of the approximated linear least squares problem. It is a
well-known algorithm that can be found in many standard textbooks, e.g., (Bertsekas, 1999; Nocedal
and Wright, 2006; Boyd and Vandenberghe, 2018).
To apply GN to (3), we first take linear approximation of φi at the current update J as
φi (θ)≈ φi (θ) + Vφ,∙ (θ)τ(0 - θ),
and set the solution of the following problem J+ as the next update
minimize 告(φi(θ) + Vφ,∙(θ)τ(θ - θ))2 + 1 ∖θ - θt∖2.	(4)
θ 2	2
Obviously, (4) has a closed form solution
θ+ = θ t-( VI + gig>)	gi (φi (θ )-g>0 - J t)),	(5)
where we denote / = Vφr∙ (J) to simplify notation. Notice that the matrix to be inverted in (5) has a
simple “identity plus rank-one” structure, implying that it can be efficiently computed in linear time.
Using the “kernel trick” (Boyd and Vandenberghe, 2018, pp.332)
(ATA + α I)-1 AT = Aτ( AAT + a I)-1,	(6)
update (5) simplifies to
J+=J t -吗-+XXJ t)").
(7)
As we can see, each GN update only takes
O(d) flops, which is as cheap as that of
a SGD step. To fully obtain the proximal
operator (3), one has to run GN for several
iterations. However, thanks to the superlin-
ear convergence rate of GN near its optimal
(Nocedal and Wright, 2006), which is in-
deed the case if we initiate at Jt because
of the proximal term, it typically takes no
more than 5-10 GN updates.
The detailed description of the proposed
algorithm, which we term SPPA-GN, for
solving general NLS problems is shown in
Algorithm 1.
Algorithm 1 SPPA-GN
1
2
3
4
5
6
7
8
9
10
initialize J0, t J 0
repeat
randomly draw i from {1,...,η}
J+ J Jt
repeat
J+ J Jt -
until convergence
Jt+1 J J+, t J t + 1
四 ⑼-Vwi ⑸>向-%)
Λ-1 + ∖V φi ⑸k2
until convergence
Vφi (J)
J J J+
2.2 SPPA-LBFGS and SPPA-AGD for generic losses
Even for generic nonlinear programming problems, it is still possible to efficiently evaluate the
proximal update beyond merely a simple gradient step. The idea is to apply the limited-memory
BFGS algorithm (Nocedal and Wright, 2006), or L-BFGS for short. L-BFGS is a memory-efficient
4
Under review as a conference paper at ICLR 2021
implementation of the famous quasi-Newton algorithm BFGS. In a nut shell, L-BFGS is an iterative
algorithm that makes use of the second-order information from the optimization loss function, but
does not require solving matrix inverses and only requires explicitly evaluating first-order gradients.
For a prescribed number of iterations, it requires one matrix-vector multiplication and multiple vector
multiplications. As a result, the overall complexity is again O(d) if the initial guess of the Hessian
matrix is diagonal.
While there are certain limitations for applying L-BFGS to general nonlinear programming problems,
we reckon that it fits perfectly in the context of SPPA implementations.
•	L-BFGS has to specify a good initial guess of the Hessian approximation matrix. While
in many cases people simply use the identity matrix to start, it may result in very poor
approximation. Fortunately, thanks to the proximal term, the identity matrix is in fact a very
good initial guess for the Hessian matrix for SPPA updates.
•	In order to save memory consumption, L-BFGS has to prescribe the number of iterations
before running the algorithm. Obviouly, if the prescribed number of iteration is too large,
we incur unnecessary computations, while if it is too small we need to invoke another
round with a new estimated Hessian matrix. However, again in the context of SPPA, the
proximal term M - θt k2 naturally provides a good initialization θt.OUr experience show
that prescribing 10 iterations of L-BFGS updates is more than enough to obtain extremely
accurate solutions.
On the other hand, it perhaps makes more sense to simply use some more advanced first-order
methods such as Nesterov’s accelerated gradient descent (1983; 2013) to calculate the proximal
update. Both L-BFGS and accelerated gradient descent evaluates the gradient of the loss function
with O(d) complexity, and the question is how to leverage convergence rate versus sophistication.
3 Convergence Analysis
In this section we provide convergence analysis of SPPA for general nonconvex loss functions (1) to
a stationary point, as informally stated in Theorem 1. To the best of our knowledge, convergence of
SPPA for nonconvex problems is still missing as of this writing.
There is a well-known resemblance between proximal methods and gradient descent: while a full
gradient descent step takes the form θt+1 = Gt -儿 VL(d),the definition of a full proximal step
guarantees that λtVL(Gt+1) = Gt+1 - Gt, meaning that Gt+1 = Gt - 2tVL(Gt+ι)∙ Therefore, one might
expect that a well-established convergence analysis of SGD for nonconvex problems, for example
(Bottou et al., 2018, §4.3), can be seamlessly applied to SPPA. This is unfortunately not the case.
Consider E[kVL(Gt)k2], where the expectation is taken over the sampling procedure conditioned on
Gt, for SGD we typically require Vli (Gt) to be an unbiased estimator of VL(Gt), which is easy to
satisfy. For SPPA then one needs to consider E[kVL(Gt+1)k2], again over the sampling procedure
conditioned on Gt . This is in fact difficult to quantify because the update Gt+1 depends on the sample
that is drawn from the data set. It requires a somewhat different approach to establish its convergence.
The convergence analysis is developed under the following three assumptions, and we explain their
implications when applied to SPPA. The detailed proofs are presented in supplementary document.
Assumption 1. The loss function (1) has bounded deviation: | E[L(G) - l(G)] | ≤ σ.
Assumption 2. Each component function li (G) is differentiable and Lipschitz continuous with
parameter 〃： ∣∣Vli(G) - Vli(G)k ≤ 〃||G - G∣∣,	∀ G, G ∈ domli.
Assumption 3. The SPPA updates lie in a compact set, i.e., there exists a constant c that ∣∣G - Gk ≤ c
. .. ~
for all G and G.
Under these rather standard assumptions, we have the following proposition, which serves as the
stepping stone for our main convergence results. The proof of Proposition 1 is relegated to the
appendix.
Proposition 1. Under Assumptions 1, 2 and 3, at iteration t we have
∣∣VL( G t )k2 ≤ 4μ2λt (L (Gt)-E[ L (Gt+ι)]+ b)+ λ(,	(8)
where C = c2(n + 1) and the expectation is taken over the sampling of i Conditioned on Gt.
5
Under review as a conference paper at ICLR 2021
With the help of Proposition 1, the rest of the results follows similarly to those presented in (Bottou
et al., 2018, §4.3) for constant step sizes and diminishing step sizes. However, we point out two
significant advantages compared to the standard SGD results:
•	For constant step sizes, there is no limit on how small λ should be. Obviously a larger λ
rapidly converges to a bigger vicinity around a stationary point, while a smaller λ slowly
converges to a smaller vicinity. However, unlike SGD, the constant step size λ can be
arbitrarily large, and the expected gradient can still be bounded.
•	For diminishing step sizes, we only need λt → 0 and 2 λt → ∞, but not the square
summable assumption 2 λ2 < ∞, ThiS gives US the option of picking a slower diminishing
step size such as 1∕√ for faster convergence rate.
3.1 Convergence with constant step sizes
Theorem 2.	Under Assumptions 1, 2 and 3, suppose SPPA is run with a constant step size λt = λ for
all t, then the expected averaged squared gradients satisfy
E ɪ 2 kVL(仇)k2 ≤ 4^2λ(L(,^ɪ- Linf + #+ λC τ-→∞ λ(4〃2b + C)	(9)
t=1
Proof. Taking the total expectation of (8) and summing over t = 1,...,T, we get
τ
E 2 WL(θt)k2 ≤ 4∕λ (L(θ0)- E[L(θτ)] + Tσ) + TλC.
t=1
Replace E [L(θτ)] with the infimum Linf ≤ E [L(θτ)] and divide both sides by T shows (9).	□
Theorem 2 shows that as T increases, SPPA spends increasingly more time in regions where the
updates have relatively small gradients. The size of the region is proportional to λ. The interesting
observation is that λ can be arbitrarily large, unlike the SGD case, albeit the bound on the expected
gradient would also be thereupon large as well.
3.2 Convergence with diminishing step sizes
To further drive the expected gradients closer to zero, we can use diminishing step sizes as shown
below. Notice that we only require that the sum of the step size reciprocals to grow faster than T,
without the more stringent condition that the diminishing step sizes are square summable. This allows
us to use step sizes that decrease a lot slower, such as λt = 1 /√t, for faster convergence rate, while
ensuring that the expected gradient goes to zero asymptotically. An analogous result has not been
achieved for SGD when applied to nonconvex problems.
Theorem 3.	Under Assumptions 1, 2 and 3, suppose SPPA is run with a diminishing step size, then
the expected averaged squared gradients satisfy
1 τ
击二团尸L（仇）k2
E
4〃2 (L(θ0) - Linf + Tb) + TC τ→∞ 0
AT
where 以=1∕λt and AT = 23 必,ifthe step sizes satisfy that
T
lim -- = 0.
τ →∞ Aτ
(10)
(11)
Proof. We rearrange (8) into
λ-1 kvL(θt)k2 ≤ 4〃2 (L(θt)-E[L(θt+1)] + b) + C.
Then apply similar strategy as in the proof of Theorem 2 to obtain the first line of (10). Furthermore,
if (11) holds, (10) goes to zero as T → ∞.	□
6
Under review as a conference paper at ICLR 2021
Figure 3: MNIST: cross entropy loss	Figure 4: MNIST: prediction accuracy
4	Experiments
We show some real data experiments to demonstrate effectiveness of the proposed SPPA implementa-
tions in classification and regression problems respectively. We compare our proposed algorithms
with three variants of SGD: the original version with various step strategies, SGD with momentum
(Sutskever et al., 2013), Adagrad (Duchi et al., 2011), and Adam (Diederik P. Kingma, 2014), with
the default settings suggested in their original papers. We used the PyTorch (Paszke et al., 2019)
implementation of the algorithms and used the same platform for implementing our customized
optimization algorithm.
4.1	Classification with SPPA-AGD and SPPA-LBFGS
We present the results of the classification problem with two data sets (MNIST (LeCun et al., 2010),
CIFAR10 (Krizhevsky et al., 2010)) using SPPA-AGD algorithm. In all the results presented, the
settings are the default settings provided by PyTorch library. We experimented with different settings
with all the algorithms and the best results are obtained when the settings were the default settings.
MNIST MNIST Handwritten Digits Dataset (LeCun et al., 2010) is a common dataset used for
showing the effectiveness of many state of art the algorithms. It consists of 60,000 training, 10,000
test images of size 28 × 28 in 10 classes. The network we used for this experiment is based on
convolutional deep neural network. It consists of 4 layers, the first 2 of which being convolutional
and last 2 being fully connected. The loss function is cross entropy loss, and the activation function
is RELU. Batch size is 64. To show the behaviour of the algorithms more in detail we recorded the
loss values at every 25 stochastic update, the experiment is run for 1 epoch and in total 38 updates
are presented (25 × 38 × 64 giving the number of training samples). The accuracy is calculated on
the test data at every 500 stochastic update, using the accuracy calculation in PyTorch tutorial1. As
observed in Figures 3 and 4, SPPA-based methods outperforms all SGD-based algorithms in terms of
both the cross entropy loss and prediction accuracy on the test set.
CIFAR10 CIFAR10 (Krizhevsky et al., 2010) is relatively extensive dataset compared to MNIST.
It consists of 50,000 training, 10,000 test images of size 32 × 32 in 10 classes. The network we used
for this experiment is based on convolutional deep neural network. It consists of 5 layers, the first 2
of which being convolutional and last 3 being fully connected. The loss function is cross entropy
loss, and the activation function is RELU. Batch size is 4. To show the behaviour of the algorithms
more in detail we recorded the loss values at every 500 stochastic update, the experiment is run for 5
epochs and in total 125 updates are presented (25 × 500 × 4 giving the number of training samples).
The accuracy is calculated on the test data at every 500 stochastic update overall for 5 epochs, using
the accuracy calculation in PyTorch tutorial. Convergence plots are shown in Figures 1 and 2 at the
beginning of the paper.
1https://github.com/pytorch/tutorials/blob/master/beginner_source/
blitz/cifar10_tutorial.py
7
Under review as a conference paper at ICLR 2021
Boston House Prices Dataset
RegreSSion________
θɪθ 05 LO L5 ZO ∑5 ɜɪθΓ5
Epoch (Batch Size 1)
----SPPA-GN (lr=0.01)
----SPPA-GN (l∕sqrt(t))
----Adam
----SGD with Momentum
----SGD
Figure 5: Boston housing prices
Figure 6: Ames housing prices
4.2	Regression with SPPA-GN
In this subsection, we compare the performance of SPPA in two regression problems. Using the
least squares loss, the problem becomes an instance of nonlinear least squares, thus our proposed
SPPA-GN algorithm could be applied. The inner loop in SPPA-GN algorithm is run at most 10 times
for the results presented below.
Boston House Prices Boston House Prices data set (Harrison Jr and Rubinfeld, 1978) is a relatively
small data set but one of the most commonly used one for regression. It consists of 506 samples (404
training, 102 test ) with 13 features each to predict housing prices. The neural network used for this
experiment is a multi-layer feed-forward network. It consists of 1 input layer, 2 hidden layers, and 1
output layer (13-(10-10)-1). All layers are fully connected. The batch size is 1. We present the loss
value that we observed in every epoch. Figure 5 shows the test error obtained after each pass of the
entire training set. Similar to the previous experiments, SPPA converges faster than all of the baseline
SGD-based algorithms, although the improvement is not as great as the previous experiments due to
the small-size of this data set.
Ames Housing Prices Ames Housing Dataset (De Cock, 2011) is an extended version of Boston
House Prices data set (Harrison Jr and Rubinfeld, 1978). It consists of 1460 samples (1168 training,
292 test) with 80 features. The neural network used for this experiment is again a multi-layer feed-
forward network. It consists of 1 input layer, 2 hidden layers, and 1 output layer (288-(72-18)-1).
All layers are fully connected. The batch size is 1. Figure 6 shows the test error obtained after each
pass of the entire training set. Again SPPA performs better than all other methods on this slightly
bigger data set for regression. A somewhat remarkable observation is that the testing loss manages to
achieve the minimum in only one pass of the entire data set (of course with a little more computations
when handling each data point). If the computing node prefers more computations than data loading,
SPPA is much more preferable in this case.
5	Conclusion
In this paper we presented efficient implementations of the stochastic proximal point algorithm
(SPPA) for large-scale nonconvex learning problems. The specific approach to efficiently evaluate
the proximal operators is based on the somewhat forgotten quasi-Newton methods with superlinear
convergence rate such as Gauss-Newton and L-BFGS. We also proved that SPPA converges, in
expectation, to a stationary point of a nonconvex problem with more flexible choices of the step
sizes. Detailed proof can be found in the appendix. The resulting algorithm has cheap per-iteration
complexity, while enjoying faster and more robust convergence under milder conditions. We demon-
strate the performance of SPPA with our proposed efficient implementation on some well-known
classification and regression data sets for neural network training, and showed that they outperform
many existing methods.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In
STOC, 2017.
Zeyuan Allen-Zhu. Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Op-
timization. In Proceedings of the 35th International Conference on Machine Learning, ICML ’18,
2018a.
Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically. In Proceedings of the 32nd
Conference on Neural Information Processing Systems, NeurIPS ’18, 2018b.
Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. In Advances in Neural
Information Processing Systems, pages 2675-2686, 2018.
Hilal Asi and John C Duchi. The importance of better models in stochastic optimization. Proceedings
of the National Academy of Sciences, 116(46):22924-22930, 2019a.
Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence,
optimality, and adaptivity. SIAM Journal on Optimization, 29(3):2257-2290, 2019b.
Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
Dimitri P Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical
programming, 129(2):163, 2011a.
Dimitri P Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimiza-
tion: A survey. Optimization for Machine Learning, 2010(1-38):3, 2011b.
Pascal Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal on
Optimization, 26(4):2235-2260, 2016.
Leon Bottou, Frank E Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Stephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices,
and Least Squares. Cambridge University Press, 2018.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207-239, 2019.
Dean De Cock. Ames, iowa: Alternative to the boston housing data as an end of semester regression
project. Journal of Statistics Education, 19(3), 2011.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pages 1646-1654, 2014.
Jimmy Ba Diederik P. Kingma. Adam: A method for stochastic optimization. In Proceedings of the
3rd International Conference on Learning Representations, 2014.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
John C Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization
problems. SIAM Journal on Optimization, 28(4):3229-3259, 2018.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.
David Harrison Jr and Daniel L Rubinfeld. Hedonic housing prices and the demand for clean air.
1978.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 1724-1732. JMLR. org, 2017.
9
Under review as a conference paper at ICLR 2021
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pages 315-323, 2013.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). URL http://www. cs. toronto. edu/kriz/cifar. html, 5, 2010.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. 2010. URL
http://yann. lecun. com/exdb/mnist, 7:23, 2010.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
scsg methods. In Advances in Neural Information Processing Systems, pages 2348-2358, 2017.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
o(1/k2). In Dokl. akad. nauk Sssr, volume 269, pages 543-547, 1983.
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in neural information processing systems,
pages 8026-8037, 2019.
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on
control and optimization, 14(5):877-898, 1976.
Ernest K Ryu and Stephen Boyd. Stochastic proximal iteration: a non-asymptotic improvement upon
stochastic gradient descent. Preprint, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pages 1139-
1147, 2013.
P Patrick Van Der Smagt. Minimisation methods for training feedforward neural networks. Neural
networks, 7(1):1-11, 1994.
Mengdi Wang and Dimitri P Bertsekas. Incremental constraint projection-proximal methods for
nonsmooth convex optimization. SIAM J. Optim.(to appear), 2013.
Yi Xu, Rong Jin, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points
in almost linear time. In Advances in Neural Information Processing Systems, pages 5530-5540,
2018.
10
Under review as a conference paper at ICLR 2021
A Appendix: proof of Proposition 1
First, we introduce a new notation
θE)I= argmin小li(θ) + 1 呢-仇k2	(12)
I+1	θ	2
to denote the update at iteration t had We sampled the 力th data point. Since li is nonconvex, there is
typically no guarantee that the exact minimum (12) is obtained. However, as we will see soon, the
convergence analysis stays valid as long as the folloWing tWo conditions hold:
•	decrease:
ɪ kθ*-仇 k2≤ Iig- Ii(θ*);	(13)
2At
•	stationarity:
ΛtVli(0(+)ι)=仇-θt+∖.	(14)
Lemma 1. Under Assumption 1, we have at iteration t
E[k仇+1 - θt k2] ≤ 2, (L(仇)-E[L(θt+ι)] + b),	(15)
where expectation is taken over the sampling of i conditioned on θt.
E[kθt+1 - θtk2] = - E Mt+)ι - θtk2 ≤ 22t
i=1
Proof. Taking expectation of (13) over i conditioned on θt gets
(L (仇)-ɪ E li(θ t(+)ι)!.
Invoking Assumption 1 shoWs (15).
Lemma 2. Under Assumption 2 and 3, we have at iteration t
kVL(θt)k2 ≤ 2〃2E[|Wt+1 -仇k2] + 2C2,	(16)
where expectation is taken over the sampling of i conditioned on 0t .
Proof. Apply Assumption 2 to 0t(+i)1 and 0t, and take expectation over i conditioned on 0t, We get
[九	n
- E kVli(0t+)-)-Vli(仇)k ≤ - E kθt+1-仇k.	(17)
n i=1	n i=1
The left-hand-side of (17) can be loWerbounded by the triangle inequality
1 n	1 n
-E Vli(0*)-VL(Ot) ≤ - E kVli(0*)-Vli(仇)k	(18)
i=1	i=1
Where
1 n
VL (01) = - gvii (01)
by definition. Then We apply triangle inequality again to obtain
n	n
kVL(01)k- - E kVli(0t+1)k ≤ - E Vli(0(+)1) - VL(01)
i=1	i=1
(19)
Invoking Assumption 3, stationary condition (14), and combining them With (17), (18), and (19)
gives us
n
kVL(0t)k≤ - E k0(+)1 - 01k+ 西C	(20)
- i=1
11
Under review as a conference paper at ICLR 2021
The right-hand-side of (20) can be UPPerboUnded by the norm inequality k ∙ kι ≤ √k ∙ ∣∣2 as
£ k。小仇k + ?
i=1	N
≤、(” 1)(管 + 与 k。(+)1 -仇k2!
=SW(W + I) E[k仇+1 -仇 k2] + 'C "〃2" + I)
Combining (20) and (21) and sqUaring both sides shows (16).
(21)
SimPly PUtting Lemma 1 and Lemma 2 together, we obtain (8) in ProPosition 1.
12