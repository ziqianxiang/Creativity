Under review as a conference paper at ICLR 2021
Stabilizing DARTS with Amended Gradient Es-
timation on Architectural Parameters
Anonymous authors
Paper under double-blind review
Ab stract
Despite the great advantage in search efficiency, DARTS often suffers weak sta-
bility, which reflects in the large variation among individual trials as well as the
sensitivity to the hyper-parameters of the search process. This paper owes such
instability to an optimization gap between the super-network and its sub-networks,
namely, improving the validation accuracy of the super-network does not necessar-
ily lead to a higher expectation on the performance of the sampled sub-networks.
Then, we point out that the gap is due to the inaccurate estimation of the archi-
tectural gradients, based on which we propose an amended estimation method.
Mathematically, our method guarantees a bounded error from the true gradients
while the original estimation does not. Our approach bridges the gap from two as-
pects, namely, amending the estimation on the architectural gradients, and unifying
the hyper-parameter settings in the search and re-training stages. Experiments on
CIFAR10, ImageNet, and Penn Treebank demonstrate that our approach largely
improves search stability and, more importantly, enables DARTS-based approaches
to explore much larger search spaces that have not been investigated before.
1 Introduction
With the development of deep learning (LeCun et al., 2015), deep neural networks (Krizhevsky et al.,
2012) have become the standard tool for learning representations in a complicated feature space.
Recent years have witnessed the trend of using deeper (He et al., 2016) and denser (Huang et al.,
2017) networks, while there is no justification that whether these manually designed architectures are
optimal. Recently, researchers started considering the possibility of learning network architectures
automatically from data, which led to the appearance of neural architecture search (NAS) (Zoph &
Le, 2017), a sub research field in automated machine learning (AutoML).
The idea of NAS is to replace the manual network design with an automatic algorithm. The early
age of NAS mainly involves using heuristic search methods (e.g., reinforcement learning (Zoph &
Le, 2017; Zoph et al., 2018; Liu et al., 2018) and evolutionary algorithms (Real et al., 2017; Xie &
Yuille, 2017; Real et al., 2019)) to sample architectures from a large search space and evaluating them
individually. Despite the notable success, these methods often requires a vast amount of computation.
To alleviate the burden, researchers proposed to reuse computation from the previously optimized
networks (Cai et al., 2018) or build a super-network (Pham et al., 2018) to share computation among all
possible architectures. These efforts combined into the so-called ‘one-shot’ search methods (Howard
et al., 2019; Guo et al., 2019) which, as the super-network is trained only once, are typically 2-3
orders of magnitude faster than the individual search methods.
This paper focuses on DARTS (Liu et al., 2019), a specific class of one-shot search in which the
search space is slacked so that the choices among operators are formulated using some continuous
‘architectural’ parameters. This simplifies the search procedure into an end-to-end optimization,
but also raises the risk of instability (Li & Talwalkar, 2019; Sciuto et al., 2019; Zela et al., 2020).
Researchers reported that DARTS-based algorithms can sometimes generate weird architectures
that produce considerably worse accuracy than those generated in other individual runs (Zela et al.,
2020). Although some practical methods (Chen et al., 2019; Nayman et al., 2019; Xu et al., 2020)
have been developed to reduce search variance, the following property of DARTS persists and has
not been studied thoroughly: when DARTS gets trained for sufficiently long, e.g., extending the
default number of 50 epochs to 200 epochs, almost all DARTS-based approaches converge to a
1
Under review as a conference paper at ICLR 2021
dummy architecture in which all edges are occupied by skip-connect. These architectures, with few
trainable parameters, are often far from producing high accuracy, in particular, on large datasets like
ImageNet, although the validation accuracy in the search stage continues growing with more epochs.
Existing solutions include searching for several times and choosing the best one in validation (Liu
et al., 2019), using other kinds of techniques such as decoupling modules (Cai et al., 2019; Guo et al.,
2019), adjusting search space during optimization (Chen et al., 2019; Nayman et al., 2019; Noy et al.,
2020), regularization (Xu et al., 2020; Chen & Hsieh, 2020), early termination (Liang et al., 2019),
etc. However, these approaches seem to develop heuristic remedies rather than analyze it from the
essence, i.e., how instability happens in mathematics and how to maximally avoid it.
In this paper, we offer a new perspective to this problem. The key observation is that the instability
issue is caused by the optimization gap between the search and re-training stages. That is to say,
a high validation accuracy of the super-network does not guarantee the high quality of the final
architecture. To solve this issue, we investigate it from a perspective which is less studied before. We
reveal that DARTS has been using an inaccurate approximation to calculate the gradients with respect
to the architectural parameters (α as in the literature), and we amend the error by slightly modifying
the second-order term in gradient computation. Mathematically, we prove that the amended term
has a bounded error, i.e., the angle between the true and estimated gradients is smaller than 90°,
while the original DARTS did not guarantee so. With this modification, the search performance is
stabilized and the dummy all-skip-connect architecture does not appear even after a very long search
process. Consequently, one can freely allow the search process to arrive at convergence, and hence
the sensitivity to hyper-parameters is alleviated.
Practically, our approach involves using an amended second-order gradient, so that the computational
overhead is comparable to the second-order version of DARTS. Experiments are performed on
two image classification datasets, CIFAR10 and ImageNet, and a language modeling dataset, Penn
Treebank. In all experiments, the search process, after arriving at convergence, produces competitive
architectures and classification accuracy comparable to the state-of-the-arts.
2 An Amended Architectural Gradient Estimation for DARTS
2.1	Preliminaries: Differentiable Architecture Search
Differentiable NAS approaches start with defining a super-network, which is constrained in a search
space with a pre-defined number of layers and a limited set of neural operators. The core idea is to
introduce a ‘soft’ way operator selection (i.e., using a weighted sum over the outputs ofafew operators
instead of taking the output of only one), so that optimization can be done in an end-to-end manner.
Mathematically, the super-network is a function f(x; ω, α), with x being input, and parameterized
by network parameters ω (e.g., convolutional kernels) and architectural parameters α (e.g., indicating
the importance of each operator between each pair of layers). f (x; ω , α) is differentiable to both ω
and α, so that gradient-based approaches (e.g., SGD) can be applied for optimization.
In the example of DARTS, f(x; ω, α) is composed of a few cells, each of which contains N nodes,
and there is a pre-defined set, E , denoting which pairs of nodes are connected. For each connected
node pair (i,j), i < j, node j takes xi as input and propagates it through a pre-defined operator set,
O, and sums up all outputs: y(i,j) (xi)
.	exp(αθi,j))
o∈O Po0∈o exp(αθi,j))
• O(Xi).
Here, normalization is performed by computing softmax on the architectural weights. Within each
unit of the search process, ω and α get optimized alternately. After that, the operator o with the
maximal value of α(oi,j) is preserved for each edge (i, j). All network parameters ω are discarded
and the obtained architecture is re-trained from scratch.
2.2	The Optimization Gap of DARTS
Our research is motivated by an observation that DARTS, at the end of a regular training process with,
say, 50 epochs (Liu et al., 2019), has not yet arrived at convergence. To verify this, we increase the
length of each training stage from 50 to 200 epochs, and observe two weird facts shown in Figure 1.
First, the weight of the none operator monotonically goes up - at 200 epochs, the weight has achieved
0.95 on most edges of the normal cells, however, the none operator is not considered in the final
2
Under review as a conference paper at ICLR 2021
Figure 1: Left: a typical search process of the first-order DARTS, in which 200 search epochs are
used. Red, green and blue lines indicate the average weight of none, the ratio of preserved skip-
connect operators, and the re-training accuracy, respectively, with respect to the number of search
epochs. Right: the normal cell after 200 search epochs, in which all operators are skip-connect.
Such failure consistently happens for both first-order and second-order DARTS.
I c_{k-1}
I c_{k-2}
skip_connect
Skip connect
2
skip connect
3
c_{k}
skip connect
0
skip connect
skip connect
skip_connect
skip connect
architecture. Second, almost all preserved operators are the skip-connect (a.k.a., identity) operator,
a parameter-free operator that contributes little to feature learning - and surprisingly, it occupies
30% to 70% of the weight remained by the none operator. Such a network has very few trainable
parameters, and thus usually reports unsatisfying performance at the re-training stage, in particular,
lower than a randomly sampled network (please refer to the experiments in Section 3.1.1).
Despite dramatically bad sub-networks are produced, the validation accuracy of the super-network
keeps growing as the search process continues. In a typical run of the first-order DARTS on CIFAR10,
from 50 to 200 search epochs, the validation accuracy of the super-network is boosted from 88.82% to
91.06%, while the re-training accuracy of the final architecture reduced from 97.00% to 93.82%. This
implies an optimization gap between the super-network and its sub-networks. Specifically, the search
process aims to improve the validation accuracy of the super-network, but this does not necessarily
result in high accuracy of the optimal sub-network determined by the architectural parameters1, α.
A practical solution is to terminate the search process early (Liang et al., 2019), however, despite
its effectiveness, early termination makes the search result sensitive to the initialization (α and ω),
the hyper-parameters of search (e.g., learning rate), and the time of termination. Consequently, the
stability of DARTS is inevitably weakened.
2.3	Amending the Architectural Gradients
We point out that the failure owes to inaccurate estimation of VaLvai(ω*(α), α) b=α,, the gradient
with respect to α. Following the chain rule of gradients, this quantity equals to
VaLval(ω, αXω=ω*(αt),α=αt + Vaω*9Xα=αt，^ωLval3 a) L=ω*(αt),α=αt .	⑴
For the simplicity of notations, We denote gι = VaLvai(ω, α)∣ω=ω*(αt),α=ɑt and g2 =
Vɑω?(α)∣α=αt ∙ Vω Lvai(ω, α)∣ω=ω*(αt ),α=αt. gι is easy to compute as is done in the first-
order DARTS, while g2, in particular Vaω*(α)∣α=α,, is not (see Appendix A.2). To estimate
Vaω?(α)∣α=α,, We note that ω?(α) has arrived at the optimality on the training set, hence
VωLtrain(ω, α) L=ω*(at ),α=αt ≡ 0 holds for any al Differentiating with respect to any at
on both sides, we have Vat (v。Ltrain(ω, α)∣ω=ω*(αt),α=ɑt) ≡ 0. When at = αt, it becomes:
Vat ( vωLtrain(ω, a) L=ω*(at ),a=at )［十=仪=0.	⑵
Again, applying the chain rule to the left-hand side gives:
va,ω Ltrain(ω, α"ω=ω*(at),a=at + Va3?(a)|a=at . VMLtrain3 a) lω=ω*(aja=at = 0,
(3)
1This opinion is different from that of (Zela et al., 2020), which believed a super-network with higher
validation accuracy must be better, and owed unsatisfying sub-network performance to the final discretization
step of DARTS. Please refer to Appendix A.1 for the detailed elaboration.
3
Under review as a conference paper at ICLR 2021
where We use the notation of YQa,3(∙) = Va(Vω(∙)) throughout the remaining part of this pa-
per. We denote H = V3Ltrain(ω, α)L=ω*(α=)a=aj the Hesse matrix corresponding to the
optimum, ω? (αt). H is symmetric and positive-definite, and thus invertible, which gives that
Va3*(a)|a=at = — V.,3 LtrainU, a) l3=3*(at),a=at ∙ H-1. SUbStitUtingitinto g2 gives
g2 = - v.,3Ltrain(ω, a) L=ω*(at),a=at，HjVωLval3 a) lω=ω*(at),a=at ∙⑷
Note that no approximation has been made till now. To compute g2, the main difficulty lies in
H-1 which, due to the high dimensionality of H (over one million in DARTS), is computationally
intractable. We directly replace H-1 with H, which leads to an approximated term:
g2 = -η∙ v.,3 Ltrain(ω, a) I3=3*(at),a=at ∙ H，V3Lval3 a) lω=3*(aja=at, ⑸
where η > 0 is named the amending coefficient, the only hyper-parameter of our approach, and its
effect will be discussed in the experimental section. A nice property of g20 is that the angle between
g2 and g2 does not exceed 90^ (i.e., hg, g2i > 0, as proved in Appendix A.3).
As the final step, we compute g1 and g20 following the second-order DARTS (see Appendix A.4).
Overall, the computation of Eqn equation 5 requires similar computational costs of the second-order
DARTS. On an NVIDIA Tesla-V100 GPU, each search epoch requires around 0.02 GPU-days on the
standard 8-cell search space on CIFAR10.
2.4	What Makes Our Approach Better Than DARTS?
DARTS fails mostly due to inaccurate estimation of g2 . The first-order DARTS directly discarded
this term, i.e., setting g21st = 0, and the second-order DARTS used I (the identity matrix) to replace
H-1 in Eqn equation 4. Let us denote the approximated term as g22nd, then the property that
g22nd , g2 > 0 does not hold. Consequently, there can be a significant gap between the true and
estimated values of VaLval(ω*(α), α)∣.=.t. SUCh inaccuracy accumulates with every update on
α, and eventually causes α to converge to the degenerated solutions. Although all (up to 100) our
trials of the original DARTS search converge to architectures with all skip-connect operators. We do
not find a theoretical explanation why this is the only ending point and leave it as an open problem.
On the contrary, with an amended approximation, g20 , our approach can survive after a sufficiently
long search process. The longest search process in our experiments has 500 epochs, after which the
architecture remains mostly the same as that after 100 epochs. In addition, the final architecture
seems converged (not changing with more search epochs) as (i) the none operator does not dominate
any edge; and (ii) the weight of the dominating operator in each edge is still gradually increasing.
The rationality of our approach is also verified by the validation accuracy of the search stage. In a
typical search process on CIFAR10, the first-order DARTS, by directly discarding g2, reports an
average validation accuracy of 90.5%. The second-order DARTS adds g22nd but reports a reduced
validation accuracy. Our approach, by adding g20 , achieves an improved validation accuracy of 91.5%,
higher than both versions of DARTS. This indicates that our approach indeed provides more accurate
approximation so that the super-network is better optimized.
Hyper-Parameter Consistency Our goal is to bridge the optimization gap between the search and
re-training phases. Besides amending the architectural gradients to avoid ‘over-fitting’ the super-
network, another important factor is to make the hyper-parameters used in search and re-training
consistent. In the contexts of DARTS, examples include using different depths (e.g., DARTS used 8
cells in search and 20 cells in re-training) and widths (e.g., DARTS used a basic channel number of 16
in search and 36 in re-training), as well as using different training strategies (e.g., during re-training,
a few regularization techniques including Cutout (DeVries & Taylor, 2017), Dropout (Srivastava
et al., 2014) and auxiliary loss (Szegedy et al., 2015) were used, but none of them appeared in search).
More importantly, the final step of search (removing 6 out of 14 edges from the structure) can cause
another significant gap. In Section 3.1.1, we will discuss some practical ways to bridge these gaps
towards higher stability. For more analysis on this point, please refer to Appendix B.1.
2.5	Discussions and Relationship to Prior Work
A few prior DARTS-based approaches noticed the issue of instability and alleviated it in different
ways. For example, P-DARTS (Chen et al., 2019) fixed the number of preserved skip-connect
4
Under review as a conference paper at ICLR 2021
operators, PC-DARTS (Xu et al., 2020) used edge normalization to eliminate the none operator, while
XNAS (Nayman et al., 2019) and DARTS+ (Liang et al., 2019) introduced a few human expertise
to stabilize search. However, we point out that (i) either P-DARTS or PC-DARTS, with carefully
designed methods or tricks, can also fail in a long enough search process (more than 200 epochs);
and that (ii) XNAS and DARTS+, by adding human expertise, somewhat violated the design principle
of AutoML, in which one is expected to avoid introducing too many hand-designed rules.
A recent work (Zela et al., 2020) also tried to robustify DARTS in a simplified search space, and
using various optimization strategies, including '2-regularization, adding Dropout and using early
termination. We point out that these methods are still sensitive to initialization and hyper-parameters,
while our approach provides a mathematical explanation and enjoys a better convergence property
(see Section 3.1.1 and Appendix C.2).
We also draw the connection between our approach and heuristic search methods including using
reinforcement learning or an evolutionary algorithm as a controller. From the viewpoint of optimiza-
tion, these methods are more stable because the optimization of α is decoupled from that of ω, so
that it does not require ω to arrive at ω*, but only need a reasonable approximation of ω* to predict
model performance. Our approach sheds light on introducing a similar property, i.e., robustness to
approximated ω? , which helps in stabilizing differentiable search approaches.
3	Experiments
3.1	RESULTS ON CIFAR 1 0
The CIFAR10 dataset (Krizhevsky & Hinton, 2009) has 50,000 training and 10,000 testing images,
equally distributed over 10 classes. We mainly use this dataset to evaluate the stability of our approach,
as well as analyze the impacts of different search options and parameters.
We search and re-train similarly as DARTS. During the search, all operators are assigned equal
weights on each edge. The batch size is set to be 96. An Adam optimizer is used to update
architectural parameters, with a learning rate of 0.0003, a weight decay of 0.001 and a momentum
of (0.5, 0.999). The number of epochs is to be discussed later. During re-training, the base channel
number is increased to 36. An SGD optimizer is used with an initial learning rate starting from
0.025, decaying with cosine annealing, and arriving at 0 after 600 epochs. The weight decay is set to
be 0.0003, and the momentum is 0.9. The amending coefficient, η, is set to be 0.1 throughout the
experiments. For a detailed analysis on η, please refer to Appendix C.1.
3.1.1	S tab ilized Search Results
First, we investigate the stability of our approach
by comparing it (with η = 0.1) to DARTS (Liu
et al., 2019), P-DARTS (Chen et al., 2019), and PC-
DARTS (Xu et al., 2020). We run all the competitors
for 200 search epochs2 to guarantee convergence in
the final architecture. Results are summarized in Ta-
ble 1. One can see that all others, except PC-DARTS,
produce lower accuracy than that of random search
(some of them, DARTS and P-DARTS, even degener-
ate to all-skip-connect architectures), but our approach
survives, indicating the amended approximation effec-
tively boosts search robustness. Moreover, we verify
the search stability by claiming a 0.58% advantage over
Architecture	Test Err.	Params	#P
	(%)	(M)	
RandOm SearCht	3.29	3.2	-
DARTS (first-order)	-6:18-	1.4	0
DARTS (second-order)	5.15	1.5	0
P-DARTS	5.38	1.5	0
PC-DARTS	3.15	2.4	3
Our Approach	-2：71	3.3	7
w/o amending term	3.15	3.9	6
w/o consistency	3.08	3.3	5
Table 1: The performance of DARTS, P-
DARTS, PC-DARTS and our approach af-
ter 200 search epochs on CIFAR10. ‘#P’
means the number of parametric operators
(any convolution) in the final normal cell.
t: We borrow the results from DARTS (LiU
et al., 2019).
random search.
Table 1 also provides an ablation study by switching
off the amending term or the consistency of training
hyper-parameters (i.e., adding Cutout, Dropout, and the auxiliary loss tower to the search stage with
the same parameters, e.g., the Dropout ratio, as they are used in re-training). Without any one of
2For P-DARTS, we run each of its three search stages for 100 epochs, and do not use the heuristic rule that
preserves exactly two skip-connect operators.
5
Under review as a conference paper at ICLR 2021
them, the error rate significantly increases to more than 3%, still better than random search but the
advantage becomes much weaker. These results verify our motivation, i.e., shrinking the optimization
gap from any aspects can lead to better search performance. The architectures with and without
unified hyper-parameters are shown in Figure 2 (top) and Figure 5 (middle), respectively.
We also perform experiments in the simplified search space defined by (Zela et al., 2020). Without
bells and whistles, we obtain an error rate of 2.55% which is significantly better than random search
(3.05%) reported in the paper. Please refer to Appendix C.2 for more results under this setting.
3.1.2	Exploring More Complex Search Spaces
Driven by the benefit of shrinking the optimization gap, we further apply two modifications. First, to
avoid edge removal, we partition the search stage into two sub-stages: the former chooses 8 active
edges from the 14 candidates, and the latter, restarting from scratch, determines the operators on
each preserved edge. Technical details are provided in Appendix B.2. Another option that can save
computational costs is to fix the edges in each cell, e.g., each node i is connected to node i - 1 and the
least indexed node (denoted by ck-2 in most conventions). Note that our approach also works well
with all 14 edges preserved, but we have used 8 edges to be computationally fair to DARTS. Second,
we use the same width (i.e., the number of basic channels, 36) and depth (i.e., the number of cells, 20)
in both search and re-training. The modification on depth reminds us of the depth gap (Chen et al.,
2019) between search and re-training (the network has 8 cells in search, but 20 cells in re-training).
Instead of using a progressive search method, we directly search in an augmented space (see the next
paragraph), thanks to the improved stability of our approach.
We denote the original search space used in DARTS as S1, which has six normal cells and two
reduction cells, and all cells of the same type share the architectural parameters. This standard space
contains 1.1 × 1018 distinct architectures. We also explore a more complex search space, denoted by
S2, in which we relax the constraint of sharing architectural parameters, meanwhile the number of
cells increases from 8 to 20, i.e., the same as in the re-training stage. Here, since the GPU memory is
limited, we cannot search with all seven operators, so we only choose two, namely skip-connect
and sep-conv-3x3, which have very different properties. This setting allows a total of 1.9 × 1093
architectures to appear, significantly surpassing the capacity of most existing cell-based search spaces.
The searched results in S1 and S2 with fixed or searched edges are shown in Figure 2, and their
performance summarized in Table 2. By directly searching in the target space, S2, the error is
further reduced from 2.71% to 2.60% and 2.63% with fixed and searched edges, respectively. The
improvement seems small on CIFAR10, but when we transfer these architectures to ImageNet, the
corresponding advantages become more significant (0.4%, see Table 3).
c_{k-1}
sep_conv_5x5
Sep conv 3x
sep_conv_5x5
sep_conv_5x5
avg pool 3x3
sep_conv_5x5
c_{k-2}
dil conv 3x3
skip connect
Figure 2: Top: the normal and reduction cells found in S1 with edge pruning, i.e., following the
original DARTS to search on full (14) edges. Middle & Bottom: the architecture found in S2
with fixed and searched edges, in which red thin, blue bold, and black dashed arrows indicate
skip-connect, sep-conv-3x3, and concatenation, respectively. This figure is best viewed in color.
6
Under review as a conference paper at ICLR 2021
Architecture	Test Err.	Params	Search Cost	Search Method
	~(%)―	(M)	(GPU-days)	
DenseNet-BC (Huang et al., 2017)	346	25.6	-	manual
ENAS (Pham et al., 2018) w/ Cutout	289	4.6	0.5	RL
NAONet-WS (Luo et al., 2018)	3.53	3.1	0.4	NAO
AmoebaNet-B (Real et al., 2019) w/ Cutout	2.55±0.05	2.8	3150	evolution
SNAS (moderate) (Xie et al., 2018) w/ Cutout	2.85±0.02	2.8	1.5	gradient-based
DARTS (first-order) (Liu et al., 2019) w/ Cutout	3.00±0.14	3.3	0.4	gradient-based
DARTS (second-order) (Liu et al., 2019) w/ Cutout	2.76±0.09	3.3	1.0	gradient-based
P-DARTS (Chen et al., 2019) w/ Cutout	2.50	3.4	0.3	gradient-based
PC-DARTS (Xu et al., 2020) w/ Cutout	2.57±0.07	3.6	0.1	gradient-based
Amended-DARTS, S1, pruned edges, w/ Cutout	2.71±0.09	3.3	1.7	gradient-based
Amended-DARTS, S2, fixed edges, w/ Cutout	2.60±0.15	3.6	1.1	gradient-based
Amended-DARTS, S2, searched edges, w/ Cutout	2.63±0.09	3.0	3.1*	gradient-based
Table 2: Comparison with the state-of-the-arts on CIFAR10. *: Edge search, the (optional) first step
of our approach, takes 2.1 out of 3.1 GPU-days (please refer to Appendix B.3 for details).
We also execute DARTS (with early termination, otherwise it fails dramatically) and random search
on S2 with the fixed-edge setting, and they report 0.25% and 0.29% deficits compared to our approach
(DARTS is slightly better than random search). When we transfer these architectures to ImageNet,
the deficits become more significant (1.7% and 0.8%, respectively, and DARTS performs even
worse). This provides a side evidence to randomly-wired search (Xie et al., 2019), advocating for the
importance of designing stabilized approach on large search spaces.
3.1.3	Comparison to the State-of-the-Arts
Finally, we compare our approach with recent approaches, in particular, differentiable ones. Result
are shown in Table 2. Our approach produces competitive results among state-of-the-arts, although it
does not seem to beat others. We note that existing approaches often used additional tricks, e.g., P-
DARTS assumed a fixed number of skip-connect operators, which shrinks the search space (so as to
guarantee stability). More importantly, all these differentiable search approaches must be terminated
in an early stage, which makes them less convincing as search has not arrived at convergence. These
tricks somewhat violate the ideology of neural architecture search; in comparison, our approach,
though not producing the best performance, promises a more theoretically convinced direction.
3.2	Results on ImageNet
We use ILSVRC2012 (Russakovsky et al., 2015), a subset of ImageNet (Deng et al., 2009), for
experiments. It has 1,000 classes, 1.3M training images, and 50K testing images. We directly use
the architectures searched on CIFAR10 and compute a proper number of basic channels, so that the
FLOPs of each architecture does not exceed 600M, i.e., the mobile setting. During re-training, there
are a total of 250 epochs. We use an SGD optimizer with an initial learning rate of 0.5 (decaying
linearly after each epoch), a momentum of 0.9 and a weight decay of 3 × 10-5.
Architecture	Test Err. (%)		Params (M)	×+ (M)	Search Cost (GPU-days)	Search Method
	top-1	top-5				
ShuffleNet 2× (v2) (Ma et al., 2018)	25.1	-	-F-	591	-	manual
MnasNet-92 (Tan et al., 2019)	25.2	~8.0-	4.4	388	-	RL
PNAS (Liu et al., 2018)	25.8	8.1	5.1	588	225	SMBO
AmoebaNet-C (Real et al., 2019)	24.3	7.6	6.4	570	3150	evolution
SNAS (mild) (Xie et al., 2018)	27.3	9.2	4.3	522	1.5	gradient-based
DARTS (second-order) (Liu et al., 2019)	26.7	8.7	4.7	574	4.0	gradient-based
P-DARTS (CIFAR10) (Chen et al., 2019)	24.4	7.4	4.9	557	0.3	gradient-based
PC-DARTS (Xu et al., 2020)本	24.2	7.3	5.3	597	3.8	gradient-based
Amended-DARTS, S1, pruned edges	24.7	7.6	5.2	586	1.7	gradient-based
Amended-DARTS, S2, fixed edges	24.3	7.4	5.5	590	1.1	gradient-based
Amended-DARTS, S2, searched edges	24.3	7.3	5.2	596	3.1	gradient-based
Table 3: Comparison with the state-of-the-arts on ILSVRC2012, under the mobile setting. t these
architectures are searched on ImageNet.
7
Under review as a conference paper at ICLR 2021
The comparison of our approach to existing work is shown in Table 3. In the augmented search space,
S2, our approach reports a top-1 error rate of 24.3% without either AutoAugment (Cubuk et al.,
2019) or Squeeze-and-Excitation (Hu et al., 2018). This result, obtained after search convergence, is
competitive among state-of-the-arts. In comparison, without the amending term, DARTS converges
to a weird architecture in which some cells are mostly occupied by skip-connect and some others by
sep-conv-3x3. This architecture reports a top-1 error of 26.0%, which is even inferior to random
search (25.1%). Last but not least, the deficit of S1, compared to S2, becomes more significant on
ImageNet. This verifies the usefulness of shrinking the optimization gap in challenging datasets.
Architecture	Perplexity		(GPU-days)
	val	test	
NAS (ZoPh & Le, 2017)	-	64.0	-〜10000-
ENAS (Pham et al., 2018)*	60.8	58.6	0.5
Random search baseline*	61.8	59.4	2.0
DARTS (first order) (Liu et al., 2019)	60.2	57.6	0.5
DARTS (second order) (Liu et al., 2019)	58.1	55.7	1.0
GDAS (Dong & Yang, 2019)	59.8	57.5	0.4
NASP (Yao et al., 2020)	59.9	57.3	0.1
R-DARTS (Zela et al., 2020)	-	57.6	-
Amended-DARTS	57.1	54.8	1.0
Table 4: Comparison among NAS approaches on Penn
Treebank. * Results borrowed from (LiU et al., 2019).
Figure 3: The recurrent cell searched by our
approach on the Penn Treebank dataset. The
random seed is 3.
3.3	Results on Penn Treebank
We also evaluate our approach on the Penn Treebank dataset, a popular language modeling task on
which the recurrent cell connecting LSTM units are being searched (Zoph & Le, 2017; Pham et al.,
2018; Liu et al., 2019). We follow the implementation of DARTS to build the search pipeline and
make two modifications, namely, (i) amending the second-order term according to Eqn equation 5,
(ii) unify the weight decay and variational Dropout ratio between search and evaluation. The search
process continues till the architecture does not change for sufficiently long, and the evaluation stage
is executed for 8,000 epochs (i.e., until convergence, following the released code of DARTS), on
which the best snapshot on the validation set is transferred to the test set.
Results are shown in Table 4, and the searched recurrent cell shown in Figure 3. Note that the DARTS
paper reported 58.1/55.7 validation/test perplexity (ppl), but our reproduction obtains 58.5/56.3,
slightly lower than the original implementation. Our approach with amended gradients reports
57.1/54.8, showing a significant gain over the baseline. As far as we know, this is the best results
ever reported in the DARTS space. Prior DARTS-based approaches mostly reported worse results
than the original DARTS (see Table 4), or restricted to image-level operations (e.g., PC-DARTS (Xu
et al., 2020)), but our approach is a fundamental improvement over DARTS that boosts both computer
vision and language modeling tasks. More importantly, we emphasize that after gradient fixation, the
search process becomes more robust: we have achieved similar performance, in terms of ppl, using a
few different random seeds, while the original DARTS seems quite sensitive to the random seed.
4	Conclusions
In this paper, we present an effective approach for stabilizing DARTS, the state-of-the-art differen-
tiable search method. Our motivation comes from that DARTS-based approaches mostly converge
to all-skip-connect architectures when they are executed for a sufficient number of epochs. We
analyze this weird phenomenon mathematically and find the reason to be in the dramatic inaccuracy
in gradient computation of the architectural parameters. With an alternative approximation based on
the optimality of the network parameters, for the first time we prove a bounded estimation error. In
both image classification and language modeling, our approach shows improved stability, with which
we are able to explore much larger search spaces and obtain better performance.
Our research sheds light on NAS research in several aspects. First, we reveal the importance of
proper approximation in differentiable architecture search. Second, we put forward the usefulness of
hyper-parameter consistency in improving search results. Third, thanks to improved stability, our
algorithm can explore larger search spaces, which we believe is the future trend of NAS.
8
Under review as a conference paper at ICLR 2021
References
H. Cai, T. Chen, W. Zhang, Y. Yu, and J. Wang. Efficient architecture search by network transforma-
tion. In AAAI Conference on Artificial Intelligence, 2018.
H. Cai, L. Zhu, and S. Han. Proxylessnas: Direct neural architecture search on target task and
hardware. 2019.
X. Chen, L. Xie, J. Wu, and Q. Tian. Progressive differentiable architecture search: Bridging the
depth gap between search and evaluation. In International Conference on Computer Vision, 2019.
Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-
based regularization. In International Conference on Machine Learning, 2020.
E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation
policies from data. In Computer Vision and Pattern Recognition, 2019.
J.	Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Computer Vision and Pattern Recognition, 2009.
T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.
arXiv preprint arXiv:1708.04552, 2017.
Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In Computer
Vision and Pattern Recognition, 2019.
Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun. Single path one-shot neural
architecture search with uniform sampling. arXiv preprint arXiv:1904.00420, 2019.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition, 2016.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In
International Conference on Computer Vision, 2019.
J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Computer Vision and Pattern
Recognition, 2018.
G.	Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Computer Vision and Pattern Recognition, 2017.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems, 2012.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, 2009.
Yann LeCun, YoshUa Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search.
arXiv preprint arXiv:1902.07638, 2019.
H.	Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, and Z. Li. Darts+: Improved differentiable
architecture search with early stopping. arXiv preprint arXiv:1909.06035, 2019.
C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. J. Li, L. Fei-Fei, A. L. Yuille, J. Huang, and
K. Murphy. Progressive neural architecture search. In European Conference on Computer Vision,
2018.
H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. In International
Conference on Learning Representations, 2019.
R. Luo, F. Tian, T. Qin, E. Chen, and T. Y. Liu. Neural architecture optimization. In Advances in
Neural Information Processing Systems, 2018.
9
Under review as a conference paper at ICLR 2021
N. Ma, X. Zhang, H. T. Zheng, and J. Sun. Shufflenet v2: Practical guidelines for efficient cnn
architecture design. In European Conference on Computer Vision, 2018.
N. Nayman, A. Noy, T. Ridnik, I. Friedman, R. Jin, and L. Zelnik-Manor. Xnas: Neural architecture
search with expert advice. arXiv preprint arXiv:1906.08031, 2019.
Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, and
Lihi Zelnik. Asap: Architecture search, anneal and prune. In International Conference on Artificial
Intelligence and Statistics, 2020.
H. Pham, M. Guan, B. Zoph, Q. V. Le, and J. Dean. Efficient neural architecture search via parameter
sharing. In International Conference on Machine Learning, 2018.
E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Large-scale
evolution of image classifiers. In International Conference on Machine Learning, 2017.
E. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regularized evolution for image classifier architecture
search. In AAAI Conference on Artificial Intelligence, 2019.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211-252, 2015.
Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the
search phase of neural architecture search. arXiv preprint arXiv:1902.08142, 2019.
N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper with convolutions. In Computer Vision and Pattern Recognition,
2015.
M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le. Mnasnet: Platform-
aware neural architecture search for mobile. In Computer Vision and Pattern Recognition, 2019.
L. Xie and A. L. Yuille. Genetic CNN. In International Conference on Computer Vision, 2017.
S. Xie, H. Zheng, C. Liu, and L. Lin. SNAS: Stochastic neural architecture search. arXiv preprint
arXiv:1812.09926, 2018.
Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. Exploring randomly wired neural
networks for image recognition. In International Conference on Computer Vision, 2019.
Y. Xu, L. Xie, X. Zhang, X. Chen, G. J. Qi, Q. Tian, and H. Xiong. Pc-darts: Partial channel
connections for memory-efficient differentiable architecture search. In International Conference
on Learning Representations, 2020.
Q. Yao, J. Xu, W. W. Tu, and Z. Zhu. Efficient neural architecture search via proximal iterations. In
AAAI Conference on Artificial Intelligence, 2020.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.
Understanding and robustifying differentiable architecture search. In International Conference on
Learning Representations, 2020.
B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In International
Conference on Learning Representations, 2017.
B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable
image recognition. In Computer Vision and Pattern Recognition, 2018.
10
Under review as a conference paper at ICLR 2021
A Mathematical Proofs and Analyses
In this section, we provide some details to complement the theoretical part of the main article.
Recall that the main goal of this paper is to compute VaLvai(ω? (α), α)∣α=αt, the gradient with
respect to the architectural parameters, α. It is composed into the sum of g1 + g2 , and we use g20 to
approximate g2. The form of g1, g2 and g20 is:
g1 = vαLval(ω, αXω=ω*(αt),α=αt.	⑹
g2 = Va3?(a)|a=at。V3 Lval 3 a) lω=ω*(αjα=αt
=-vα,ωLtrain3 αVω=ω*(αt),α=αt ∙ H 1.V3Lval 3 a) lω=ω*(αjα=αt.	⑺
g2 = -η ∙ vα,ω Ltrain(ω, α"ω=ω*(αt),α=αt ∙ H ∙ V3 Lval 3 a) lω=ω*(αjα=αt .⑻
A. 1 Our Opinions on the Optimization Gap
This part corresponds to Section 2.2 in the main article.
In (Zela et al., 2020), the authors believed that a super-network with a higher validation accuracy
always has a higher probability of generating strong sub-networks, and they owed the weird behavior
of DARTS to the discretization step (preserving the operator with the highest weight on each edge
and discarding others) after the differentiable search phase. Here, we provide a different opinion,
detailed as follows:
1.	A high validation accuracy does not necessarily indicate a better super-network. In
training a weight-sharing super-network, e.g., in DARTS, the improvement of validation
accuracy can be brought by two factors, namely, the super-network configuration (corre-
sponding to α) becomes better or the network weights (corresponding to ω) are better
optimized. We perform an intuitive experiment in which we fix the initialized α and only
optimize ω. After 50 epochs, the validation accuracy of the super-network is boosted from
10% (random guess) to 88.28%. While the performance seems competitive among a regular
training process, the sampled sub-network is totally random.
2.	The advantage of our approach persists even without discretization-and-pruning.
From another perspective, we try to skip the discretization-and-pruning step at the end
of the search stage and re-train the super-network (with α fixed) directly. To reduce compu-
tational costs, we use a shallower super-network, and perform both search and re-training
for 600 epochs on CIFAR10 to guarantee convergence. Without the amending term, the
validation error in the search stage and the testing error in the re-training stage are 12.8%
and 7.4%, respectively, and these numbers become 10.5% and 5.4% after the amending
term is added. This indicates that amending the gradient estimation indeed improves the
super-network, and the advantage persists even when discretization-and-pruning is not used.
3.	Inaccuracy of gradient estimation also contributes to poor performance. As detailed
in Section A.2, we construct a toy example that applies bi-level optimization to the loss
function of L (ω, α; x) = (ωx - α)2 . We show that even when ω? is achieved at every
iteration, the gradient of α, VaLval(ω*(α), α)∣α=αt, Can be totally incorrect. When this
happens in architecture search, the super-network can be pushed towards sub-optimal or
even random architectures, while the validation accuracy can continue growing.
Therefore, our opinion is that the ‘optimization gap’ is brought by the inconsistency between search
and re-training - edge pruning after search is one aspect, and the inaccuracy of gradient estimation is
another one which seems more important. We alleviate the pruning issue by first selecting (or fixing)
a few edges and then determining the operator on each preserved edge. However, such a two-stage
search process can be very unstable if the gradient estimation remains inaccurate. In other words,
amending errors in gradient computation lays the foundation of hyper-parameter consistency.
A.2 IMPORTANCE OF THE SECOND-ORDER GRADIENT TERM, g2
This part corresponds to Section 2.3 in the main article.
11
Under review as a conference paper at ICLR 2021
20
5
1
ro6 0 LUo/roud5
O
O
IOO 200	300	400	500
Searched Epochs
SSoI -ra>
Qoo
3 2 1
10
5 O
6BE0∕qd5
o
O 100	200	300	400	500
Searched Epochs
SSoI -ra>
(a) A search process without the amending term. (b) A search process with the amending term.
Figure 4: TWo search processes with a simple loss function L (ω, α; x) = (ω ∙ X - α)2, with all of
ω, α and x being 1-dimensional scalars. Red, green and blue curves indicate the value of α, ω and
the validation loss, respectively.
DARTS (LiU et al., 2019) owed the inaccuracy in computing g2 to that ω*(α) is difficult to arrive at
(e.g., requiring a lot of computation), and believed that g2 goes to 0 when the optimum is achieved.
We point out that this is not correct even in a very simple example of convex optimization, detailed as
follows.
Let the loss function be L (ω, α; x) = (ω ∙ X - α)2. Then, the only difference between
Ltrain (ω, α) = L (ω, α; Xtrain) and Lval (ω, α) = L (ω, α; Xval)IieS in the input, x. Assume
that the training dataset contains a sample, xtrain = 1 and validation dataset contains another
sample, Xval = 2. It is easy to derive that the local optimum of Ltrain (ω, α) is ω? (α) = α.
Substituting Xval = 2 into the loss function yields Lval (ω, α) = (ω ∙ 2 - α)2. When α = at
(αt denotes the current status of α), ω arrives at ω? (αt), g1 = 2 (αt - 2αt) = -2αt and
g2 = 4αt 6= 0. In other words, when ω arrives at ω? (α), Lval (ω? (α) , α) = α2, we have
VaLval(ω*(a) , a)|a=at = 2at = gl.
In summary, g2 will not be zero even when ω? is achieved, which implies that ignoring g2 may lead
to a wrong direction of optimization - in the toy example above, the loss will increase monotonically
towards infinity if g2 is ignored. We believe that this inaccuracy also contributed to the weird
phenomena in real-world data (i.e., when DARTS was run on CIFAR10, the optimization process
collapses to a dummy network architecture which produces poor classification results).
To provide better understanding of the search process, we plot the statistics from two search processes,
without and with the amending term, in Figure 4. Without the amending term, i.e., the computation of
g2 suffers considerable inaccuracy, α (shown as the red curve) is always going towards an incorrect
direction. The validation loss is first reduced (because the initialized ω is poor and thus in the first half
of the search process, the accuracy improvement brought by ω surpasses the accuracy drop brought
by α), but, as the search process continues, the first-order gradient is reduced and the inaccuracy of
g2 dominates VaLval(ω*(a), a)b=a/ Consequently, both parameters are quickly pushed away
from the optimum and the search process ‘goes wild’ (i.e., the loss ‘converges’ to infinity). This
problem is perfectly solved after the amending term is applied.
A.3 PROOF OF hg02,g2i > 0
This part corresponds to Section 2.3 in the main article.
Substituting g2 and g20 into hg20 , g2i gives:
hg2 , g2 i = η ∙ Vω Lval (ω, a) ∣>=ω*(at),a=at ∙ H-1 ∙ A。H。VωLval (ω, a) ∣ω=ω*(at),a=at, (9)
where A = νω,aLtrain(ω, a) lI=ω*(at),a=at ' v2,ω Ltrain(ω, a) ∣ω=ω*(aja=at, is a semi-
positive-definite matrix. According to the definition of Hesse matrix, H is a real symmetric positive-
definite matrix.
Let {ψk} be an orthogonal set of eigenvectors with respect to A (i.e., ψ> ∙ ψk = 1), and {λk} be the
corresponding eigenvalues satisfying λk > 0. Let φι be an eigenvector of H ∙ A ∙ HT + H-1 ∙ A ∙ H,
12
Under review as a conference paper at ICLR 2021
Left-multiplying Eqn equation 12 by
K
(λk - μi) ∙ λk ∙ ak ∙ bk
k=1
[PK=1 bk ∙ (λk - μl) ∙ (H-1 ∙ ψk)i
K
-X bk ∙ (λk - μi) ∙ (H 1 ∙ ψk)
k=1
and expanding H ∙ φι and H-1 ∙ φι based on {ψk} derives the following equation for any l:
KK
φι = X ak ∙ (H ∙ ψk) = X bk ∙ (H-1 ∙ ψk),	(IO)
k=1	k=1
where {ak} and {bk} are the corresponding expansion coefficients. According to the definition of
eigenvalues, we have:
(H A ∙ HT + H-1∙A ∙ H) ∙ φι = μι∙ φι,	(11)
where μι is the corresponding eigenvalue of φι. Substituting Eqn equation 10 into Eqn equation 11,
we obtain:
KK
X ak ∙ λk	(H ∙	ψk ) + X bk ∙	(λk	- μi)	∙ (H 1 ∙	ψk)	=	0∙	(12)
k=1	k=1
obtains:
2
6 0.	(13)
2
A is a real symmetric matrix sized M1 × M1 , and it is obtained by multiplying a M1 × M2
matrix to a M2 × M1 . Here, M1 and M2 are the dimensionality of ω and α, respectively, and
M1 is often much larger than M2 (e.g., millions vs. hundreds in DARTS). Hence, A has much
fewer different eigenvalues compared to its dimensionality, and so we can choose many sets of
{αk} which are orthogonal to each other, and generate many sets of {ak} and {bk} satisfying
Pk=I a®bk = β> ∙ β > 0. That being said, in most of time, we can choose one set of eigenvectors
from the subspace, {αk}, and the elements within are orthogonal to each other, which makes
akbk > 0 in most cases, and hence μι > 0 for any l.
Since all of the eigenvalues with respect to the real symmetric matrix H ∙ A ∙ HT + H-1 ∙ A ∙ H is not
smaller than zero, so it is semi-positive-definite. This derives that H-1 ∙ A ∙H is semi-positive-definite,
and thus hg20 ,g2i > 0.
A.4 COMPUTING g02 USING FINITE DIFFERENCE APPROXIMATION
This part corresponds to Section 2.3 in the main article.
Although we avoid the computation of H-1, the inverse matrix of H, computing H ∙
VωLvai(ω, α)∣ω=ω*(α=),α=ɑ= in g2 is still a problem since the size of H is often very large, e.g.,
over one million. We use finite difference approximation just as used in the second-order DARTS (Liu
et al., 2019).
Let be a small scalar, which is set to be 0.01/ kVωLval(ω, α)k2 as in the original DARTS. Using
finite difference approximation to compute the gradient around ω1 = ω + E ∙ VωLvai(ω, α) and
ω2 = ω — J VωLval(ω, α) gives:
H ∙VωLvai(ω, α) ≈ ^Ltrain^1, α) - vωLtrain(ω2, α).	(14)
Let	ω3	= ω + 1 [Vω Ltrain(ω1, α) — Vω Ltrain(ω2, α)]	and ω4	=
ω — 2 [VωLtrain(ω1, α) 一 VLtrain(ω2, α)], We have:
g0 ≈ η X VaLtrain(ω3, α) -VaLtrain(ω4,α).	(15)
B Technical Details and Search Costs
B.1	More about Hyper-Parameter Consistency
An important contribution of this paper is to reveal the need of hyper-parameter consistency, i.e.,
using the same set of hyper-parameters, including the depth and width of the super-network, the
13
Under review as a conference paper at ICLR 2021
Dropout ratio, whether to use the auxiliary loss term, not to prune edges at the end of search, etc.
This is a natural idea in both theory and practice, but most existing work ignored it, possibly because
its importance was hidden behind the large optimization gap brought by the inaccurate gradient
estimation in the bi-level optimization problem.
We point out that even when bi-level optimization works accurately, we may miss the optimal
architecture(s) without hyper-parameter consistency. This is because each hyper-parameter will more
or less change the value of the loss function and therefore impact the optimal architecture. We provide
a few examples here.
•	The most sensitive change may lie in the edge pruning process, i.e., preserving 8 out of 14
edges in each cell and eliminating others. This may incur significant accuracy drop even
when only one edge gets pruned. For example, in the normal cell searched on the S1-A
space (see Figure 9), removing the skip-connect operator will cause the re-training error to
increase from 2.71% ± 0.09 to 3.36% ± 0.08. The dramatic performance drop is possibly
due to the important role played by the pruned edge, e.g., the pruned skip-connect may
contribute to rapid information propagation in network training.
•	Some hyper-parameters will potentially change the optimal architecture. The basic channel
width and network depth are two typical examples. When the channel number is small,
the network may expect large convolutional kernels to guarantee a reasonable amount of
trainable parameters; also, a deep architecture may lean towards small convolutional kernels
since the receptive field is not a major bottleneck. However, the original DARTS did not
unify these quantities in search and re-training, which often results in sub-optimality as
noticed in (Chen et al., 2019).
•	Other hyper-parameters, in particular those related to the training configuration, are also
important. For example, if we expect Dropout to be used during the re-training stage, the
optimal architecture may contain a larger number of trainable parameters; if the auxiliary loss
tower is used, the optimal architecture may be deeper. Without hyper-parameter consistency,
these factors cannot be taken into consideration.
B.2	Details of the Two-Stage Search Process
This part corresponds to Section 3.1 in the main article.
To avoid the optimization gap brought by edge pruning, we adopt a two-stage search process in which
we perform edge search followed by operator search. In the DARTS setting, the first stage involves
preserving 8 out of 14 edges to be preserved. For each node, indexed j, we use Ej to denote the set of
all combinations of edges (in the DARTS setting, each node preserves two input edges, so Ej contains
all (i1, i2) pairs with 0 6 i1 < i2 < j). The output of node j is therefore computed as:
yj =
(i1 ,i2)∈Ej
exp(β(i1,i2))
P(i1,i2)∈Ej exp(β(i1,i2))
• (yiι + y，2).
(16)
where β(i1,i2) denotes the edge-selection parameter of the node combination of (i1, i2). This mecha-
nism is similar to the edge normalization introduced in (Xu et al., 2020) but we take the number of
preserved inputs into consideration.
After edge selection is finished, a regular operator search process, starting from scratch, follows on
the preserved edges to determine the final architecture.
B.3	Search Cost Analysis
This part corresponds to Section 3.1 in the main article.
Note that the first (edge selection) stage requires around 2× computational costs compared to the
second (operator selection) stage. This is because edge selection works on 14 edges while operator
selection on 8 edges. Fortunately, the former stage can be skipped (at no costs) if we choose to search
operators on a fixed edge configuration, which also reports competitive search performance.
14
Under review as a conference paper at ICLR 2021
Search Method	TestErr. (%)					Params (M)		
	Seed #1	Seed #2	Seed #3	average	best	Seed #1	Seed2	Seed #3
Random SearCht	-	-	-	-	2.95	-	-	-
DARTS (early termination)，	-	-	-	3.71±1.14	3.07	-	-	-
R-DARTS (Dropout)t Drop Prob. = 0.0	5.30	3.73	3.34	4.12±1.04	3.34	2.21	2.43	2.85
Drop Prob. = 0.2	3.22	3.16	3.26	3.21±0.05	3.16	3.62	4.04	2.99
Drop Prob. = 0.4	2.93	3.15	3.52	3.20±0.30	2.93	4.10	3.74	3.38
Drop Prob. = 0.6	3.09	5.44	3.56	4.03±1.24	3.09	4.46	2.30	2.66
Our Approach, pruned edges	2.76±0.10	2.64±0.07	2.68±0.09	2.69±0.06	2.64±0.07	3.09	3.21	3.40
Our Approach, searched edges	2.58±0.07	2.55±0.13	2.76±0.04	2.63±0.11	2.55±0.13	3.84	3.21	2.85
Table 5: Comparison between DARTS (early termination), R-DARTS (Dropout) and our approach in
the reduced search space of S3 with different SeedsJ: We borrow the experimental results from (Zela
et al., 2020).
Skip-ConneCt
c_{k-1}
skip connect
skip connect
skip connect
c_{k-2}
skip connect
skip connect
SkiP-ConneCt
SkiP-ConneCt
(a) η 6 0.01, CIFAR10 Test Error: 6.18%
(b) η = 0.1, CIFAR10 Test Error: 3.08%
(c) η > 1, CIFAR10 Test Error: 7.16%
Figure 5: The normal cells (in the standard DARTS space) obtained by different amending coefficients.
C Additional Experimental Results
C.1 Impact of the Amending Coefficient
This part corresponds to Section 3.1 in the main article.
We first investigate how the amending coefficient, η, defined in Eqn equation 5, impacts architecture
search. To arrive at convergence, we run the search stage for 500 epochs. We evaluate different
η values from 0 to 1, and the architectures corresponding to small, medium and large amending
coefficients are summarized in Figure 5.
We can see that after 500 epochs, η = 0.1 produces a reasonable architecture that achieves an error
rate of 3.08% on CIFAR10. Actually, even with more search epochs, this architecture is not likely to
change, as the preserved operator on each edge has a weight not smaller than 0.5, and most of these
weights are still growing gradually.
When η is very small, e.g., η = 0.001 or η = 0.01, the change brought by this amending term to
architecture search is negligible, and our approach shows almost the same behavior as the first-order
version of DARTS, i.e., η = 0. In addition, in such scenarios, although the search process eventually
runs into an architecture with all skip-connect operators, the number of epochs needed increases
15
Under review as a conference paper at ICLR 2021
∣j{k-2}匚而_Con v_3x3
sep_conv_3x3
Sep Conv 3x3
skip connect
Skip connect
Sep Conv 3x3
Rk-2} Sep=Con⅛
l__L___lski p_connect
N-*,
'<.^sk ip_connect
Figure 6: Top: the normal and reduction cells found in S3 with edge pruning. Bottom: the normal
and reduction cells found in S3 with searched edges.
sep_conv_3x3
Skip connect
Sep COnv 3x3
Sep conv 3x3
Sep conv 3x3
sep conv 3x3
c=)k-1∣
significantly, which verifies that the amending term indeed pulls architecture search away from
degeneration.
On the other hand, if we use a sufficiently large η value, e.g., η = 1, the amending term, g02, can
dominate optimization, so that the first term, i.e., the gradient of architectural parameters, has limited
effects in updating α. Note that the amending term is closely related to network regularization,
therefore, in the scenarios of a large η, the network significantly prefers avg-pool-3x3 to other
operators, as average pooling can smooth feature maps and avoid over-fitting. However, pooling is
also a parameter-free operator, so the performance of such architectures is also below satisfaction.
Following these analyses, we simply use η = 0.1 for all later experiments. We do not tune η very
carefully, though it is possible to determine η automatically using a held-out validation set. Besides,
we find that the best architecture barely changes after 100 search epochs, so we fix the search length
to be 100 epochs to reduce computational costs.
C.2 Comparison to (Zela et al., 2020)
(Zela et al., 2020) described various search spaces and demonstrated that the standard DARTS fails
on them. They proposed various optimization strategies to robustify DARTS, including adding
'2-regularization, adding Dropout, and using early termination. We search in a reduced space which
we denote as S3, in which the candidate operators only include sep-conv-3x3, skip-connect, and
none. Note that this is the ‘safest’ search space identified in R-DARTS (Zela et al., 2020), yet as
shown in Table 5, R-DARTS often produced unsatisfying architectures, and choosing the best one
over a few search trials can somewhat guarantee a reasonable architecture. Meanwhile, R-DARTS is
sensitive to the search hyper-parameters such as the Dropout ratio.
Our approach works smoothly in this space, without the need of tuning hyper-parameters, and in either
pruned or searched edges (see Figure 6). Thanks to the reduced search space, the best architecture
often surpasses the numbers we have reported in Table 2.
C.3	200 SEARCH EPOCHS FOR DARTS, P-DARTS, PC-DARTS, AND OUR APPROACH
We execute all algorithms for 200 search epochs on CIFAR10. The searched architectures by DARTS,
P-DARTS, and PC-DARTS are shown in Figure 7, and that of our approach shown in Figure 2.
We find that both DARTS and P-DARTS failed completely into dummy architectures with all
preserved operators to be skip-connect. PC-DARTS managed to survive after 200 epochs mainly due
to two reasons: (i) the edge normalization technique is useful in avoiding skip-connect to dominate;
and (ii) PC-DARTS sampled 1/4 channels of each operator, so that the algorithm converged slower -
200 epochs may not be enough for a complete collapse.
We also visualize our approach throughout 200 epochs in S1 (under the edge-pruning setting) to
investigate its behavior. In Figure 8, the visualized factors include the average weight of none, the
ratio of preserved skip-connect operators, the super-network validation accuracy, and the re-training
accuracy of some checkpoints during the search process.
16
Under review as a conference paper at ICLR 2021
2
Skip-Connect
R√k}
0
skip-connect
/ skip_connect
∣j{k-2}匚Iiki汇connect
skip_connect
Ic {k-ι> ∣^^^^^Skip-∞nnect
∖ skip_connect
skip_connect	I_
ɪ---—3
I-------1 max_pool_3x3
I C_{k 2}匕 ____mgx_pooT_3x3
max_pool_3x3
avg_pool_3x3
"{k-i}
avg pool 3x3
c_{k}
avg pool 3x3
dil_conv_5x5
skip_connect
skip_connect
max_pool_3x3
avg pool 3x3
3
max pool 3x3
1
0
dil_conv_5x5
sep_conv_3x3
avg pool 3x3
c_{k-2}
avg pool 3x3
J{k-1}
avg pool 3x3
Figure 7: Top: the normal and reduction cells found by DARTS (test error: 6.18%). Middle: the
normal and reduction cells found by P-DARTS (test error: 5.38%). Bottom: the normal and reduction
cells found by PC-DARTS (test error: 3.15%).
avg_pool_3x3
c_{k-2汇^^0vg-Pθθl-3x3
avg_pool_3x3
dil conv 5x5
dil conv 5x5
dil conv 5x5
c_{k-1}
SkiP connect
dil_conv_3x3
Searched Epochs	Searched Epochs
Figure 8:	Details of a search process of our approach in Si with 200 search epochs. Left: Red, green
and blue curves indicate the average weight of none, the ratio of preserved skip-connect operators,
and the re-training accuracy, respectively (corresponding to Figure 1). Right: Red and yellow curves
indicate the validation accuracy of the super-network (in the search stage) searched by DARTS and
our approach, and green and blue curves indicate the test accuracy (in the re-training stage) produced
by the architectures found by DARTS and our approach, respectively.
17
Under review as a conference paper at ICLR 2021
C.4 Searching with Different Seeds
On CIFAR10, we perform edge search (the first search stage) in S2 with different seeds (i.e., random
initialization), yielding three sub-architectures named S2-A, S2-B, and S2-C. Then, we execute the
operator search process with different seeds for 3 times in S1 (with edge pruning), S2-A, S2 -B,
S2-C, and S2-F (fixed edges), and re-train each discovered architecture for 3 times. The results
are summarized in Table 6. We also transfer some of the found architectures to ImageNet, and the
corresponding results are listed in Table 7. The edge-searched architectures in S2 report inferior
performance compared to the edge-fixed ones, arguably due to the inconsistency of hyper-parameters.
All the searched architectures are shown in Figures 9-12.
Architecture	Test Err. (%)				Params (M)	Search Cost (GPU-days)
	Run #1	Run #2	Run #3	average		
Amended-DARTS, S1-A, pruned edges	2.81	2.65	2.67	2.71±0.09	3.3	1.7
Amended-DARTS, S1-B, pruned edges	2.82	2.73	2.90	2.82±0.09	2.9	1.7
Amended-DARTS, S1-C, pruned edges	2.82	2.68	2.73	2.74±0.07	3.8	1.7
Amended-DARTS, S1-F, fixed edges	2.69	2.68	3.05	2.81±0.21	3.5	1.0
Amended-DARTS, S2-A-A, searched edges	2.67	2.79	2.66	2.71±0.07	2.8	3.1
Amended-DARTS, S2-A-B, searched edges	2.62	2.79	2.56	2.66±0.12	3.0	3.1
Amended-DARTS, S2-A-C, searched edges	2.67	2.69	2.69	2.68±0.01	2.9	3.1
Amended-DARTS, S2-B-A, searched edges	2.59	2.73	2.57	2.63±0.09	3.0	3.1
Amended-DARTS, S2-B-B, searched edges	2.64	2.57	2.48	2.56±0.08	3.4	3.1
Amended-DARTS, S2-B-C, searched edges	2.64	2.66	2.80	2.70±0.09	3.0	3.1
Amended-DARTS, S2-C-A, searched edges	2.56	2.63	2.69	2.66±0.09	3.3	3.1
Amended-DARTS, S2-C-B, searched edges	2.61	2.70	2.77	2.69±0.08	3.2	3.1
Amended-DARTS, S2-C-C, searched edges	2.76	2.62	2.57	2.65±0.10	3.2	3.1
Amended-DARTS, S2-F-A, fixed edges	2.51	2.51	2.77	2.60±0.15	3.6	1.1
Amended-DARTS, S2-F-B, fixed edges	2.59	2.64	2.58	2.60±0.03	3.5	1.1
Amended-DARTS, S2-F-C, fixed edges	2.64	2.60	2.72	2.65±0.06	2.9	1.1
Amended-DARTS, S3-A, pruned edges	2.65	2.81	2.83	2.76±0.10	3.1	0.8
Amended-DARTS, S3-B, pruned edges	2.57	2.71	2.64	2.64±0.07	3.2	0.8
Amended-DARTS, S3-C, pruned edges	2.59	2.67	2.77	2.68±0.09	3.4	0.8
Amended-DARTS, S3-D-A, searched edges	2.52	2.43	2.69	2.55±0.13	3.2	3.0
Amended-DARTS, S3-D-B, searched edges	2.65	2.58	2.52	2.58±0.07	3.8	3.0
Amended-DARTS, S3-D-C, searched edges	2.80	2.74	2.73	2.76±0.04	2.9	3.0
Table 6: Results of architecture search with different seeds in S1, S2, and S3 on CIFAR10.
Architecture	Test Err. (%)		Params	×+	Search Cost
	top-1	top-5	(M)	(M)	(GPU-days)
Amended-DARTS, S1-A, pruned edges	24.7	7.6	5.2	586	1.7
Amended-DARTS, S1-F, fixed edges	24.6	7.4	5.2	587	1.0
Amended-DARTS, S2-A-A, searched edges	24.7	7.5	4.9	576	3.1
Amended-DARTS, S2-B-A, searched edges	24.3	7.3	5.2	596	3.1
Amended-DARTS, S2-B-B, searched edges	24.6	7.5	5.3	585	3.1
Amended-DARTS, S2-B-C, searched edges	24.3	7.3	5.3	592	3.1
Amended-DARTS, S2-C-A, searched edges	24.3	7.5	5.2	592	3.1
Amended-DARTS, S2-F-A, fixed edges	24.3	7.4	5.5	590	1.1
Table 7: Transferring some of the searched architectures on CIFAR10 to ILSVRC2012. The mobile
setting is used to determine the basic channel width of each transferred architecture.
18
Under review as a conference paper at ICLR 2021
I~~-~~—|	sep CCnv 5x5
Rk-⅞j⅛53
Sep Conv 3X3
skip connect
avg_poo 3x3
SePqnv_5x5
(a)	the normal cell of S1 -A
(b)	the reduction cell of S1 -A
skip_8nnect
c_{k-2}
max pool 3x3
skip .connect
J{k-1}
dil conv 5x5
skip_connect
sep_conv_3x
sep_conv_3x3
sep_conv_5x5
(c)	the normal cell of S1 -B
sep conv_3X3________________
-----------Sep_Conv_3X3
Sep conv 5x5
dl conv 5X5
skp_8mect
3x3---^d l_conv_5x5
SeP conv 5X5
(e) the normal cell of S1 -C
(g) the normal cell of S1 -F
max_pool_3x3
SkiP-connect
Sep conv 3x3
dl_conv_3x3
SeP_Oonv_3x3
sep conv 3x3
SeP conv 3x3
(d) the reduction cell of S1 -B
(f) the reduction cell of S1 -C
(h) the reduction cell of S1 -F
Figure 9:	Architectures searched in S1 with pruned or fixed edges.
19
Under review as a conference paper at ICLR 2021
(a) the architecture of S2-A-A
(b) the architecture of S2-A-B
(c) the architecture of S2-A-C
(d) the architecture of S2-B-A
(e) the architecture of S2-B-B
(f) the architecture of S2-B-C
Figure 10: (Part I) Architectures searched in S? with searched or fixed edges, in which red thin,
blue bold, and black dashed arrows indicate skip-connect, sep-conv-3x3, and channel-wise
concatenation, respectively. Thisfigure is best viewed in color..
20
Under review as a conference paper at ICLR 2021
(a) the architecture of S2-C-A
(b) the architecture of S2-C-B
(c) the architecture of S2-C-C
(d) the architecture of S2-F-A
(e) the architecture of S2-F-B
(f) the architecture of S2-F-C
Figure 11: (Part II) Architectures searched in S2 with searched or fixed edges, in which red thin,
blue bold, and black dashed arrows indicate skip-connect, sep-conv-3x3, and channel-wise
concatenation, respectively. Thisfigure is best viewed in color..
21
Under review as a conference paper at ICLR 2021
J{k-1}
SkiP-ConneCt ______________
IskpTonneC	Sep-Con13χ3
c_{k-2}
skip connect
SeP-ConV_3x3
SkiP-ConneCt
SeP-ConV_3x3
sep_conv_3x3
SkiP-ConneCt
c-{k-2} Γ SkiP-Connect	skip_connect
J{k-1}
SkiP-ConneCt
sep Conv 3x3
seP-ConV_3x
SkiP-ConneCt
(a)	the normal cell of S3-A
(b)	the reduction cell of S3 -A
Sep-ConV_3x3
sep_conv_3x3
Sep ConV 3x3
skip connect
SkP connect
sep conv 3x3
skip connect
J{k-1}
skip connect
sep_conv_3x3
sep_conv_3x3
1
Skip-Connect__________________
-	skip-Connect
Sep-Conv_3x3
Sep-Conv_3x3	Sep-Conv_3x3
sep_conv_3x3
(c)	the normal cell of S3-B
(d)	the reduction cell of S3-B
Skip-CCrlnect
SkiP-CCnnect
sep conv 3x3
Sep Conv 3x3
SkP connect
sep conv 3x3
sep conv 3x3
sep_conv_3x3
Sep-Conv_3x3
-Jpconnet	Skip-ConneCt
I C-{k-1}
Sep-Conv_3x3
skip connect
skip connect
Sep-Conv_3x3
skp_connect
(e)	the normal cell of S3-C
(f)	the reduction cell of S3 -C
—-_--ι sep conv 3x3
j≡⅛nV^
∣j{k-2}	skip-conned:
sep conv 3x3
sep conv 3x3
Skip-CCnlneCt
Skip-Comect
sep conv 3x3
skip-connect
sep conv 3x3
2
SkiP connect
sep ConV 3x3
sep conv 3x3
c_{k-1}
sep ConV 3x3
sep ConV 3x3
sep_conv_3x3
(g)	the normal cell of S3-D-A
(h)	the reduction cell of S3 -D-A
ρ-{k-2} Sep_ConV=3x3
I - j I gep_conv_3x3
sep_conv_3x3
[7_{k—2} SepqnJ3x3
skip connect
sep_conv_3x3
sep_conv_3x3
sep conv 3x3
skip connect
sep_conv_3x3
sep Conv 3x3
I c_{k-1}
c_{k}
conv 3x3
SeP ConV 3x3
SkiP connect
skip connect
skip connect
(i)	the normal cell of S3-D-B
(j)	the reduction cell of S3-D-B
sep_conv_3x3
c_{k-2}卜
_____Lskip_connect
J c-{k-2} Skip-Cornect
skip connect
Skip-CCnnect
skip connect
SepI-CIonv_3x3
Sep conv 3x3
Skip-Connect
SeP ConV 3x3
SeP conv 3x3
sep conv 3x3
c_{k-1}
sep conv 3x3
sep conv 3x3
skip_connect
(k)	the normal cell of S3-D-C
(l)	the reduction cell of S3-D-C
Figure 12: Architectures searched in S3 .
22