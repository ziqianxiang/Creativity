Under review as a conference paper at ICLR 2021
Perturbation Type Categorization for Multi-
PLE `p B OUNDED ADVERSARIAL ROBUSTNESS
Anonymous authors
Paper under double-blind review
Ab stract
Despite the recent advances in adversarial training based defenses, deep neural
networks are still vulnerable to adversarial attacks outside the perturbation type
they are trained to be robust against. Recent works have proposed defenses to
improve the robustness of a single model against the union of multiple perturbation
types. However, when evaluating the model against each individual attack, these
methods still suffer significant trade-offs compared to the ones specifically trained
to be robust against that perturbation type. In this work, we introduce the problem
of categorizing adversarial examples based on their `p perturbation types. Based
on our analysis, we propose Protector, a two-stage pipeline to improve the
robustness against multiple perturbation types. Instead of training a single predictor,
Protector first categorizes the perturbation type of the input, and then utilizes
a predictor specifically trained against the predicted perturbation type to make
the final prediction. We first theoretically show that adversarial examples created
by different perturbation types constitute different distributions, which makes it
possible to distinguish them. Further, we show that at test time the adversary faces a
natural trade-off between fooling the perturbation type classifier and the succeeding
predictor optimized with perturbation specific adversarial training. This makes
it challenging for an adversary to plant strong attacks against the whole pipeline.
In addition, we demonstrate the realization of this trade-off in deep networks by
adding random noise to the model input at test time, enabling enhanced robustness
against strong adaptive attacks. Extensive experiments on MNIST and CIFAR-10
show that Protector outperforms prior adversarial training based defenses by
over 5%, when tested against the union of '1,'2,'∞ attacks.1
1	Introduction
There has been a long line of work studying the vulnerabilities of machine learning models to small
changes in the input data. In particular, most existing works focus on `p bounded perturbations
(Szegedy et al., 2013; Goodfellow et al., 2015). While majority of the prior work aims at achieving
robustness against a single perturbation type (Madry et al., 2018; Kurakin et al., 2017; Tramer et al.,
2018; Dong et al., 2018; Zhang et al., 2019; Carmon et al., 2019), real-world deployment of machine
learning models requires them to be robust against various imperceptible changes in the input,
irrespective of the attack type. Prior work has shown that when models are trained to be robust against
one perturbation type, such robustness typically does not transfer to attacks of a different type (Schott
et al., 2018; Kang et al., 2019). As a result, recent works have proposed to develop models that are
robust against the union of multiple perturbation types (Tramer & Boneh, 2019; Maini et al., 2020).
Specifically, these works consider adversaries limited by their `p distance from the original input for
p ∈ {1, 2, ∞}. While these methods improve the overall robustness against multiple perturbation
types, when evaluating the robustness against each individual perturbation type, the robustness of
models trained by these methods is still considerably worse than those trained on a single perturbation
type. Further, these methods are found sensitive to small changes in hyperparameters.
In this work, we propose an alternative view that does not require a single predictor to be robust against
a union of perturbation types. Instead, we propose to utilize a union of predictors to improve the
1We will open-source the code, pre-trained models, and perturbation type datasets upon publication.
1
Under review as a conference paper at ICLR 2021
overall robustness, where each predictor is specialized to defend against certain perturbation types. In
particular, we introduce the problem of categorizing adversarial examples based on their perturbation
types. Based on this idea, we propose PROTECTOR, a two-stage pipeline that performs Perturbation
Type Categorization for Robustness against multiple perturbations. Specifically, first a perturbation
type classifier predicts the type of the attack. Then, among the second-level predictors, Protector
selects the one that is the most robust to the predicted perturbation type to make final prediction.
We validate our approach from both theoretical and empirical aspects. First, we present theoretical
analysis to show that for benign samples with the same ground truth label, their distributions become
highly distinct when added with different types of perturbations, and thus can be separated. Further,
we show that there exists a natural tension between attacking the top-level perturbation classifier and
the second-level predictors - strong attacks against the second-level predictors make it easier for
the perturbation classifier to predict the adversarial perturbation type, and fooling the perturbation
classifier requires planting weaker (or less representative) attacks against the second-level predictors.
As a result, even an imperfect perturbation classifier is sufficient to significantly improve the overall
robustness of the model to multiple perturbation types.
Empirically, we show that the perturbation type classifier generalizes well on classifying adversarial
examples against different adversarially trained models. Then we further compare Protector to
the state-of-the-art defenses against multiple perturbations on MNIST and CIFAR-10. Protector
outperforms prior approaches by over 5% against the union of the 'ι, '2 and '∞ attacks. While past
work has focused on the worst case metric against all attacks, on average they suffer significant trade-
offs against individual attacks. From the suite of 25 different attacks tested, the average improvement
for Protector over all the attacks w.r.t. the state-of-art baseline defense is 〜15% on both MNIST
and CIFAR10. In particular, by adding random noise to the model input at test time, we further
increase the tension between attacking top-level and second-level components, and bring in additional
improvement of robustness against adaptive attackers. Additionally, Protector provides a modular
way to integrate and update defenses against a single perturbation type.
2	Related Work
Adversarial examples. The realization of the existence of adversarial examples in deep neural
networks has spun active research on attack algorithms and defense proposals (Szegedy et al., 2013).
Among different types of attacks (Madry et al., 2018; Hendrycks et al., 2019; Hendrycks & Dietterich,
2019; Bhattad et al., 2020), the most commonly studied ones constrain the adversarial perturbation
within an `p region of radius p around the original input. To improve the model robustness in the
presence of such adversaries, the majority of existing defenses utilize adversarial training (Goodfellow
et al., 2015), which augments the training dataset with adversarial images. Till date, different variants
of the original adversarial training algorithm remain the most successful defenses against adversarial
attacks (Carmon et al., 2019; Zhang et al., 2019; Wong et al., 2020; Rice et al., 2020). other types
of defenses include input transformation (Guo et al., 2018; Buckman et al., 2018) and network
distillation (Papernot et al., 2016), but were rendered ineffective under stronger adversaries (He
et al., 2017; Carlini & Wagner, 2017a; Athalye et al., 2018; Tramer et al., 2020). other works have
explored the relation between randomizing the inputs and adversarial examples. Tabacof & Valle
(2016) analyzed the change in adversarial robustness with varying levels of noise. Hu et al. (2019)
evaluated the robustness of a data point to random noise to detect adversarial examples, whereas
Cohen et al. (2019) utilized randomized smoothing for certified robustness to adversarial attacks.
Defenses against multiple perturbation types. Recent research has been drawn towards the goal of
universal adversarial robustness. Since `p-norm bounded attacks are amongst the strongest attacks in
adversarial examples literature, defending against a union of such attacks is an important step towards
this end goal. Schott et al. (2018); Kang et al. (2019) showed that models that were trained for a given
'p-norm bounded attacks are not robust against attacks in a different 'q region. Succeeding work has
aimed at developing one single model that is robust against the union of multiple perturbation types.
Schott et al. (2018) proposed the use of multiple variational autoencoders to achieve robustness to
multiple 'p attacks on the MNIST dataset. Tramer & Boneh (2019) used simple aggregations of
multiple adversaries to achieve non-trivial robust accuracy against the union of the '1,'2, '∞ regions.
Maini et al. (2020) proposed the MSD algorithm that takes gradient steps in the union of multiple `p
regions to improve multiple perturbation robustness. In a related line of work, Croce & Hein (2020a)
2
Under review as a conference paper at ICLR 2021
proposed a method for provable robustness against all `p regions for p ≥ 1. Instead of presenting
empirical results, they study the upper and lower bounds of certified robust test error on much smaller
perturbation radii. Therefore, their work has a different focus, and is not directly comparable to
empirical defenses studied in our work.
Detection of adversarial examples. Multiple prior works have focused on detecting adversarial
examples (Feinman et al., 2017; Lee et al., 2018; Ma et al., 2018; Cennamo et al., 2020; Fidel et al.,
2019; Yin et al., 2019a;b). However, most of these defenses have been shown to be vulnerable in the
presence of adaptive adversaries (Carlini & Wagner, 2017a; Tramer et al., 2020). In comparison, our
work focuses on a more challenging problem of categorizing different perturbations types. However,
we show that by establishing a trade-off between fooling the perturbation classifier and the individual
`p-robust models, even an imperfect perturbation classifier is sufficient to make our pipeline robust.
3	Protector: Perturbation Type Categorization for Robustness
In this section, we discuss our proposed Protector approach, which performs perturbation type
categorization to improve the model robustness against multiple perturbation types. We first illustrate
the Protector pipeline in Figure 1, then discuss the details of each component.
Perturbation	Specialized
Classifier	Robust
Predictors
Perturbation	Specialized
Classifier	Robust
Predictors
(a)
(b)
Figure 1: An overview of our PROTECTOR pipeline. (a) The perturbation classifier Cadv correctly categorizes
representative attacks of different types. (b) An illustration of the trade-off in Theorem 2, where an adversarial
example fooling Cadv (the '∞ sample marked in red) becomes weaker to attack the second-level Mp models.
At a high level, Protector performs the classification
task as a two-stage process. Given an input x, PROTEC-
TOR first utilizes a perturbation classifier Cadv to predict
its adversarial perturbation type. Then, based on the `p
attack type predicted by Cadv , PROTECTOR uses the cor-
responding second-level predictor Mp to provide the final
prediction, where Mp is specially trained to be robust
against the `p attack. Formally, let fθ be the PROTECTOR
model, then the final prediction is:
fθ(x) = Mp(x);	s.t. p = argmaxCadv(x)	(1)
Note that when the input is a benign image, it could be clas-
sified as any perturbation type by Cadv , since all second-
level predictors should achieve a high test accuracy on
benign images.
Figure 2: PCA for different types of adver-
sarial examples on MNIST.
AS shown in Figure 1, although We consider the robustness against three attack types, i.e., '1,'2,'∞
perturbations, unless otherwise specified, our perturbation classifier performs binary classification
between p = {{1, 2}, ∞}. As will be discussed in Section 6, using two second-level predictors
3
Under review as a conference paper at ICLR 2021
achieves better overall robustness than using three second-level predictors. We hypothesize that
compared to the '∞ adversarial examples, '1 and '2 attacks are harder to separate, especially when
facing an adaptive adversary which aims to attack the entire pipeline. To provide an intuitive
illustration, we randomly sample 10K adversarial examples generated with PGD attacks on MNIST,
and visualize the results of the Principal Component Analysis (PCA) in Figure 2. We observe that
the first two principal components for '1 and '2 adversarial examples are largely overlapping, while
those for '∞ are clearly from a different distribution. Note that this simple visualization by no means
suggest that '1 and '2 adversarial examples are not separable, it merely serves as a motivation.
4	Theoretical Analysis
In this section, we provide a theoretical justification of our Protector framework design. First, we
formally illustrate the setup of robust classification against multiple 'p perturbation types, and we
consider models trained for a binary classification task. Based on this problem setting, in Theorem 1,
we show the existence of a classifier that can separate adversarial examples belonging to different
perturbation types. Moreover, in Theorem 2, we show that our Protector framework naturally
offers a trade-off between fooling the perturbation classifier Cadv and the individual robust models
Mp , thus it is extremely difficult for adversaries to stage attacks against the entire pipeline. Note that
we focus on the simplified binary classification task for the convenience of theoretical analysis, but
our Protector framework could improve the robustness of models trained on real-world image
classification benchmarks as well, and we will discuss the empirical examination in Section 6.
4.1	Problem Setting
Data distribution. We consider a dataset of inputs sampled from the union of two multi-variate
Gaussian distributions D, such that the input-label pairs (x,y) can be described as:
y UM {-1, +1};	X0〜N(yα,σ2), xι, .. . ,xd %dN(yη,σ2)	⑵
where X = [xo, xi,..., xd] ∈ Rd+1 and η = √, such that the absolute value of the mean for any
dimension is equal for inputs sampled from both the positive and the negative labels. This setting
demonstrates the distinction between a feature x0 that is strongly correlated with the input label, and
d weakly correlated features that are (independently) normally distributed with mean yη and variance
σ2. For the purposes of this work, we assume that a > 10 (xo is strongly correlated) and d > 100
(remaining d features are weakly correlated, but together represent a strongly correlated feature). We
adapt this problem setting from Ilyas et al. (2019), where they used a stochastic feature x0 = y with
probability p, as opposed to a normally distributed input feature as in our case. Our results hold in
their setting as well. However, our setting better represents the true data distribution, where input
features are seldom stochastically flipped. More discussion could be found in Appendix A.
Perturbation types. We focus our discussion on adversaries constrained within a fixed 'p region of
radius p around the original input, forp ∈ S = {1, 2, ∞}. Such adversaries are frequently studied
in existing work, primarily for finding the optimal first-order adversaries for different perturbation
types. We define ∆p, as the 'p threat model of radius and ∆S = Sp∈S ∆p,. For a model fθ
parametrized over θ, the objective of the adversary is to find the optimal perturbation δ*, such that:
δ* = arg max '(fθ(X + δ),y)	(3)
δ∈∆S
where '(∙, ∙) is the cross-entropy loss. Based on the model design in Section 3, we focus on discussing
the separation of '1 and '∞ in the following theorems, but our proofs could also naturally be adapted
to analyze the separability of other perturbation types.
4.2	Separability of Adversarial Perturbations
Consider a standard classifier M trained with the objective of correctly classifying the label of inputs
X ∈ D. Since the original distribution of the input data for each label is known to us, we first aim to
examine how adversaries confined within different perturbation regions modify the input. The goal
of the adversary is to fool the label predictor M, by finding the optimal perturbation δp ∀p ∈ S. The
theorem below shows that the distributions of adversarial inputs within different 'p regions can be
separated with a high accuracy, and we present the formal proof in Appendix B.
4
Under review as a conference paper at ICLR 2021
Theorem 1 (Separability of perturbation types). Given a binary Gaussian classifier M trained on D,
consider Dpy to be the distribution of optimal adversarial inputs (for a class y) against M, within `p
regions of radius Cp, where ∈ι = a, e∞ = α∕√d. Distributions Dy (P ∈ {1, ∞}) can be accurately
separated by a binary Gaussian classifier Cadv with a misclassification probability Pe ≤ 10-24.
The proof sketch is as follows. We first calculate the optimal weights of a binary Gaussian classifier
M trained on D. Accordingly, for any input x ∈ D, we find the optimal adversarial perturbation
δp∀P ∈ {1, ∞} against M. We discuss how these perturbed inputs x + δp also follow a normal
distribution, with shifted means. Finally, for data points belonging to a given classification label, we
show that Cadv is able to predict the correct perturbation type with a very low classification error. We
present the formal proof in Appendix B.
4.3	Adversarial Trade-off
In Section 4.2, we showed that the optimal perturbations corresponding to different perturbation types
belong to distinct data distributions, and it is fairly easy to separate them using a simple classifier.
However, in the white-box setting, the adversary has knowledge of both the perturbation classifier
Cadv and specialized robust models Mp at test time. Therefore, the adversary can adapt the attack to
fool the entire pipeline, instead of individual models Mp alone.
Note that there are some overlapping regions among different `p perturbation regions. For example,
every adversary could set δp = 0 as a valid perturbation, and thus it is clearly not possible for the
attack classifier Cadv to correctly classify the attack (∀P ∈ {1, 2, ∞}) in such a scenario. However,
such perturbation is not useful, because all the base models can correctly classify unperturbed inputs
with a high probability. In the following theorem, we examine the robustness of our Protector
pipeline in the presence of such strong dynamic adversaries.
Theorem 2 (Adversarial trade-off). Given a data distribution D, adversarially trained models
Mp,p, and an attack classifier Cadv that distinguishes perturbations of different `p attack types
for P ∈ {1, ∞}, The probability of successful attack for the worst-case adversary over the entire
Protector pipeline is Pe < 0.01 for q = α + 2σ and e∞ = α√2σ.
Here, the worst-case adversary refers to an adaptive adversary that has full knowledge of the
defense strategy, and makes the strongest adversarial decision given the perturbation constraints. In
Appendix C.2, We discuss how q, e∞ are set so that the '1 and '∞ adversaries can fool M∞,e∞ and
M1,1 models respectively with a high success rate. our proof sketch is as follows. We first show that
when trained on D, an adversarially robust model Mp can achieve robust accuracy of greater than
99% against the attack type it was trained for. on the contrary, when subjected to attacks outside the
trained perturbation region, such robust accuracy reduces to under 2%. Then, we analyze the modified
distributions of the perturbed inputs by different `p attacks. Based on this analysis, we construct
a simple decision rule for the perturbation classifier Cadv . Finally, we compute the perturbation
induced by the worst-case adversary. We show that there exists a trade-off between fooling the
perturbation classifier Cadv (to allow the alternate My,∈p model to make the final prediction for an 'q
attack ∀P, q ∈ {1, ∞}; P 6= q), and fooling the alternate Mp,p model itself. Here, by “alternate” we
mean that for an 'q attack, the prediction is made by the My2 model, where p, q ∈ {1, ∞};P = q.
We provide an illustration of the trade-off in Figure 1b, and present the formal proof in Appendix C.
5	Training and Inference
Having motivated Protector through a toy-task in Section 4, we now scale the approach to deep
neural networks for common image classification benchmarks. Specifically, following prior work
on defending against multiple perturbation types, we evaluate on MNIST (LeCun et al., 2010) and
CIFAR-10 (Krizhevsky, 2012) datasets. Now, we discuss the training details, a strong adaptive
white-box attack against Protector, and our inference procedure against such attacks.
5.1	Training
To train our perturbation classifier Cadv , we create a dataset that includes adversarial examples of
different perturbation types. Specifically, we perform '1, '2, '∞ PGD attacks (Madry et al., 2018)
5
Under review as a conference paper at ICLR 2021
against each of the two individual Mp models used in PROTECTOR. Thus the size of our dataset is 6
times that of the original MNIST and CIFAR10 datasets respectively. For the MNIST dataset, we
use the M2, M∞ models in PROTECTOR, and we use M1, M∞ models for CIFAR10. The choice is
made based on the robustness of {M2, Mι} models against the {'1, '2} attacks respectively, as will
be depicted in Table 2. As discussed in Section 3, to assign the ground truth label for training the
perturbation classifier (Cadv), we find that it is sufficient to assign the same label to '1 and '2 attacks.
In other words, Cadv performs a binary classification between '1/'2 attacks and '∞ attacks.
In contrast with prior defenses against multiple perturbation types (Tramer & Boneh, 2019; Maini
et al., 2020), which require adversarial training, we find that it is sufficient to train our Protector
pipeline over a static dataset (constructed as mentioned above) to achieve high robustness. Therefore,
the training of our perturbation classifier is fast and stable. Specifically, using a single P100 GPU, our
perturbation classifier can be trained within 5 minutes on MNIST, and within an hour on CIFAR-10.
On the other hand, training state-of-the-art models robust to a single perturbation type require up to 2
days to train on the same amount of GPU power, and existing defenses against multiple perturbation
types take thrice as long as the training time for a model robust to a single perturbation type.
A key advantage of Protector’s design is that it can build upon existing defenses against individual
perturbation types. Specifically, we leverage the adversarially trained models developed in prior
work (Zhang et al., 2019; Carmon et al., 2019) as Mp models in our pipeline, and the CNN architecture
of Cadv is also similar to a single Mp model. More details are deferred to Appendix D.
5.2	Adaptive Attacks against the Protector Pipeline
To generate adversarial examples against Protector, the most straightforward approach is to
generate the adversarial perturbation to optimize Equation 3 using existing attack algorithms. Since
the final prediction of the pipeline only depends on a single Mp model, the pipeline does not allow
gradient flow across the two levels, and thus makes it difficult for gradient-based adversaries to attack
Protector. Therefore, besides this standard adaptive attack, in our evaluation, we also consider a
stronger adaptive adversary, which utilizes a combination of the predictions from each individual
second-level Mp models, rather than only utilizing the predictions from a single Mp model with
p = argmax Cadv (x) alone. Specifically, we modify fθ(x) in Equation 3 as follows:
C = Softmax(Cadv(x));	fθ(x) = Ecp ∙ Mp(X)
p∈S
(4)
where cp denotes the probability of the input x being classified as the perturbation type p by Cadv .
We also experiment with other strategies of aggregating the predictions of different components,
e.g., tuning hyper-parameters to balance among attacking Cadv and each Mp model, but these
alternative methods do not perform better. Note that Equation 4 is only used for the purpose of
generating adversarial examples and performing gradient-based attack optimization. For consistency
throughout the paper, we still use Equation 1 to compute the model prediction at inference (final
forward-propagation). We do not see any significant performance advantages of either choice at
inference time, and briefly report a comparison on two attacks in Appendix H.4.
5.3	Inference Procedure against Adaptive Adversaries
Though training the perturbation classifier on a static dataset is sufficient to achieve robustness using
existing attack approaches, we observe that the accuracy drops when Protector is presented with
the stronger adaptive attacks discussed in Section 5.2. To improve the model robustness against such
adversaries, we add random noise to the input before feeding it into Protector at the test time.
While Hu et al. (2019) suggest that adding random noise does not help defend against adversarial
inputs, it is the unique exhibition of the trade-off described in Theorem 2 that adversarial attacks
against Protector, on the contrary, are highly likely to fail when added with random noise.
Intuitively, the trade-off between fooling the two stages of Protector confines the adversary in a
very small region for generating successful adversarial attacks.
Consider the illustrative example in Figure 3, where the input x with the true label y =
0 is subjected to an '∞ attack. We assume that the M∞,∞ model is a perfect classi-
fier for inputs within a fixed ∞ ball. The dotted line shows the decision boundary for
6
Under review as a conference paper at ICLR 2021
the perturbation classifier Cadv, which correctly classifies inputs subjected to '∞ pertur-
bations δ0 as '∞ attacks (green), but can misclassify samples with smaller perturbations.
When the adversary adds a large perturbation δ00,
the prediction of Mi for the resulted input x00
becomes wrong, but the perturbation classifier
also categorizes it as an M∞ attack, thus the
final prediction of Protector is still correct
since it will be produced by M∞,e∞ model in-
stead. On the other hand, when the adversary
adds a small perturbation δ0 to fool the perturba-
tion classifier, adding a small amount of random
noise can recover the correct prediction with
a high probability. Note that every point on
the boundary of the noise region (yellow circle)
is correctly classified by the pipeline. In this
way, adding random noise exploits an adversar-
ial trade-off for Protector to achieve a high
accuracy against adversarial examples, in the ab-
sence of adversarial training. In our implemen-
tation, we sample random noise Z 〜 N(0,I),
and add Z = e2 ∙ z∣∖z∖2 to the model input.
Figure 3: Illustration of the effect of random noise for
generating adversarial examples. Note that the notion of
small and large perturbations is only used to illustrate
the scenario in Figure 3, and in general none of the
perturbation regions subsumes the other.
6	Experiments
In this section, we present our experiments on MNIST and CIFAR-10 datasets. We will discuss the
results for both the perturbation classifier Cadv alone, and the entire Protector pipeline.
6.1	Experimental Setup
Baselines. We compare Protector with the state-of-the-art defenses against multiple perturbation
types, which consider the union of '1,'2,'∞ adversaries (Tramer & Boneh, 2019; Maini et al., 2020).
For Tramer & Boneh (2019), we compare two variants of adversarial training: (1) the MAX approach,
where for each image, among different perturbation types, the adversarial sample that leads to the
maximum increase of the model loss is augmented into the training set; (2) the AVG approach, where
adversarial examples for all perturbation types are included for training. We also evaluate the MSD
algorithm proposed by Maini et al. (2020), which modifies the standard PGD attack to incorporate the
union of multiple perturbation types within the steepest decent itself. In addition, we also evaluate
Mi, M2, M∞ models trained with '1,'2,'∞ perturbations separately, as described in Appendix D.
Attack evaluation. We evaluate our methods with the strongest attacks in the adversarial learning
literature, and with adaptive attacks specifically designed for Protector (Section 5.2). First, we
utilize a comprehensive suite of both gradient-based and gradient-free attacks from the Foolbox
library (Rauber et al., 2017). Further, we also evaluate our method against the AutoAttack library from
Croce & Hein (2020c), which achieves the state-of-art adversarial error rates against multiple recently
published models. In line with prior work (Tramer & Boneh, 2019; Maini et al., 2020), the radius of
the {'1,'2,'∞} perturbation regions is {10, 2,0.3} for the MNIST dataset and {12,0.5,0.03} for
the cIFAR10 dataset. We present the full details of attack algorithms in Appendix F.
Following prior work (Tramer & Boneh, 2019; Maini et al., 2020), for both MNIST and CIFAR-10,
we evaluate the models on adversarial examples generated from the first 1000 images of the test set.
our main evaluation metric is the accuracy on all attacks, which means that for an input image, if any
of the attack algorithm in our suite could successfully fool the model, then the input is a failure case.
6.2	EMPIRICAL PERTURBATIoN oVERLAP AND CHoICE oF ep
While we justify the choice of perturbation sizes in our theoretical proofs in Appendix B.4 and C.2,
in this section we demonstrate the empirical agreement of the choices of perturbation sizes we make
7
Under review as a conference paper at ICLR 2021
Table 1: Studying the empirical overlap of 'p attack perturbations in different 'q % regions for
(a) MNIST (1,2,∞) = (10, 2.0, 0.3); (b) CIFAR-10 (1,2,∞) = (12, 0.5, 0.03). Each column
represents the range (min - max) of 'q norm for perturbations generated using 'p PGD attack.
Attack		MNIST			CIFAR10		
	'∞ < 0∙3	'2 < 2∙0	'ι < 10	'∞ < 0.03	'2 < 0.5	'1 < 12
PGD '∞	≤ 0.3	(3.67 - 6.05)	(54.8 - 140.9)	≤ 0.03	(1.33 - 1.59)	(62.7 - 85.5)
PGD '2	(0.40-0.86)	≤ 2.0	(11.2 - 24.1)	(0.037-0.10)	≤ 0.05	(15.4 - 20.9)
Sparse '1	(0.70-1.0)	(2.08 - 2.92)	≤ 10.0	(0.27 - 0.77)	(1.32 - 1.88)	≤ 12.0
Table 2: Worst-case accuracies against different `p attacks: (a) MNIST; (b) CIFAR-10. Ours represents
PROTECTOR against the adaptive attack strategy (Section 5.2), and Ours* is the standard setting.
MNIST	M∞	M2	M1	MAX	AVG	MSD	Ours	Ours*
Clean Accuracy	99.1%	99.2%	99.0%	98.6%	98.1%	98.3%	98.7%	98.7%
'∞ attacks ( = 0.3)	90.0%	2.7%	0.0%	38.8%	57.8%	63.5%	80.1%	87.3%
'2 attacks ( = 2.0)	7.6%	72.1%	47.7%	58.2%	56.6%	65.4%	67.2%	77.1%
'1 attacks ( = 10)	10.8%	69.6%	77.3%	41.1%	36.8%	62.7%	65.7%	72.4%
All Attacks	6.0%	2.7%	0.0%	28.6%	32.4%	56.9%	63.8%	68.5%
(a)
CIFAR-10	M∞	M2	Mi	MAX	AVG	MSD	Ours	Ours*
Clean accuracy	93.3%	91.2%	89.3%	81.0%	84.6%	81.1%	90.8%	90.8%
'∞ attacks ( = 0.03)	59.3%	34.8%	35.1%	35.0%	39.5%	43.7%	57.2%	62.1%
'2 attacks ( = 0.5)	64.4%	77.2%	71.5%	61.8%	65.0%	64.5%	66.8%	67.1%
'1 attacks ( = 12)	18.8%	34.7%	55.0%	34.5%	54.1%	51.5%	55.9%	56.0%
All attacks	18.8%	29.2%	35.0%	27.6%	38.6%	43.2%	52.1%	54.7%
(b)
for our results on MNIST and CIFAR10 datasets. To measure how often adversarial perturbations
of different attacks overlap, we empirically quantify the overlapping regions by attacking a benign
model with PGD attacks. In Table 1 we report the range of the norm of perturbations in the alternate
perturbation region for any given attack type. The observed overlap is exactly 0% in all cases and
the observation is consistent across MNIST and CIFAR10 datasets. A similar analysis on attacking
Protector can be found in Appendix G.
6.3	PERTURBATION TYPE CLASSIFICATION RESULTS OF Cadv
To examine the performance of the perturbation type classification, we evaluate Cadv on a dataset of
adversarial examples, which are generated against the six models we use as the baseline defenses in
our experiments. Note that Cadv is only trained on adversarial examples against the two Mp models
that are part of PROTECTOR. We observe that Cadv transfers well across the board. First, Cadv
generalizes to adversarial examples against new models; i.e., it preserves a high accuracy, even if the
adversarial examples are generated against models that are unseen for Cadv during training. Further,
Cadv also generalizes to new attack algorithms. As discussed in Section 5.1, we only include PGD
adversarial examples in our training set for Cadv . However, on adversarial examples generated by the
AutoAttack library, the classification accuracy of Cadv still holds up. In particular, the accuracy is
> 95% across all the individual test sets created. These results suggest two important findings that
validate our results in Theorem 1. That is, independent of (a) the model to be attacked; and (b) the
algorithm for generating the optimal adversarial perturbation, the optimal adversarial images for a
given 'p region follow similar distributions. We present the full results in Appendix H.1.
6.4	Results of the Protector Pipeline
Overall results. In Table 2, we summarize the worst-case performance against all attacks within
a given perturbation type for MNIST and CIFAR-10 datasets. In particular, ‘Ours’ denotes the
robustness of Protector against the adaptive attacks described in Section 5.2, and ‘Ours*’ denotes
the robustness of Protector against standard attacks based on Equation 1. The adaptive strategy
8
Under review as a conference paper at ICLR 2021
Table 3: Effect of different design choices on the CIFAR-10 dataset against PGD-based attacks.
PROTECTOR(n) means the number of specialized robust predictors Mp is n in the pipeline.
	Without Noise		With Noise	
	PROTECTOR(2)	PROTECTOR(3)	PROTECTOR(2)	Protector(3)
Clean accuracy	90.8%	92.1%	90.8%	92.2%
APGD '∞(6 = 0.03)-	54.7%	50.7%	64.8%	56.3%
APGD `2 ( = 0.5)	65.0%	64.3%	68.8%	69.2%
Sparse-PGD `1 (6 = 12)		48.7%	42.5%		55.9%	52.3%
effectively reduces the overall accuracy of Protector by 2-5%, showing that incorporating the
gradient and prediction information of all second-level predictors results in a stronger attack.
Despite that we evaluate PROTECTOR against a stronger adaptive adversary, in terms of the all attacks
accuracy, PROTECTOR still outperforms all baselines by 6.9% on MNIST, and 8.9% on CIFAR-10.
Compared to the previous state-of-the-art defense against multiple perturbation types (MSD), the
accuracy gain on '∞ attacks is especially notable, i.e., greater than 15%. In particular, if We compare
the performance gain on each individual attack algorithm, as shown in Appendix H.2 and H.3 for
MNIST and CIFAR-10 respectively, the improvement is also significant, With an average accuracy
increase of 15.5% on MNIST, and 14.2% on CIFAR-10. These results demonstrate that PROTECTOR
considerably mitigates the trade-off in accuracy against individual attack types.
PROTECTOR retains a high accuracy on benign images, as opposed to past defenses that have to
sacrifice the benign accuracy for the robustness on multiple perturbation types. In particular, the
clean accuracy of PROTECTOR is over 6% higher than such baselines on CIFAR-10, and the accuracy
is similar to that of Mp models trained for a single perturbation type.
The effect of noise. As discussed in Section 5.3, though adding random noise is not required to
defend against standard attacks, it is helpful in defending against the stronger adaptive adversary
against our pipeline. Specifically, in Table 3, We present the results on adversarial examples generated
by PGD-based algorithms, Which are amongst the strongest gradient-based attacks in the literature.
We observe a consistent improvement among all attacks, increasing the accuracy by up to 10%.
Different number of second-level Mp predictors. We also evaluate our PROTECTOR approach
With three second-level predictors, i.e., M1, M2 and M∞, and We present the results in Table 3, this
alternative design considerably reduces the overall accuracy of the pipeline model. We hypothesize
that this happens because the M1 model is already reasonably robust against the `2 attacks, as shoWn
in Table 2b. HoWever, having both M1 and M2 models alloWs adaptive adversaries to find larger
regions for fooling both Cadv and Mp , thus hurts the overall performance against adaptive adversaries.
7 Conclusion
In this Work, We propose Protector, Which performs perturbation type categorization toWards
achieving robustness against the union of multiple perturbation types. Based on a simplified problem
setup, theoretically, We demonstrate that adversarial inputs of different attack types naturally have
different distributions and can be separated. We further elaborate the existence of a natural tension for
any adversary trying to fool our model - between fooling the attack classifier and the specialized robust
predictors. Our empirical results on MNIST and CIFAR-10 datasets complement our theoretical
analysis. In particular, by posing another adversarial trade-off through the effect of random noise, our
PROTECTOR pipeline outperforms existing defenses against multiple `p attacks by over 5%.
Our work serves as a stepping stone towards the goal of universal adversarial robustness, by dissecting
various adversarial objectives into individually solvable pieces and combing them via Protector.
Our study opens up various exciting future directions, including the new problem of perturbation
categorization, extending our approach to defend attacks beyond `p adversarial examples, and defining
sub-classes of perturbation types to further improve the overall adversarial robustness.
9
Under review as a conference paper at ICLR 2021
References
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efficient black-box adversarial attack via random search. In European Conference on
Computer Vision, 2020.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018.
Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and D. A. Forsyth. Unrestricted adversarial
examples via semantic manipulation. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=Sye_OgHFwH.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. In International Conference on Learning
Representations, 2018.
J. Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot way
to resist adversarial examples. In ICLR, 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security,pp. 3-14, 2017a.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39-57. IEEE, 2017b.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp.
11190-11201, 2019.
Alessandro Cennamo, Ido Freeman, and Anton Kummert. A statistical defense approach for detecting
adversarial examples. In Proceedings of the 2020 International Conference on Pattern Recognition
and Intelligent Systems, pp. 1-7, 2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. volume 97 of Proceedings of Machine Learning Research, pp. 1310-1320, Long
Beach, California, USA, 09-15 Jun 2019. PMLR.
Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-perturbations
for p ≥ 1. In International Conference on Learning Representations, 2020a. URL https:
//openreview.net/forum?id=rklk_ySYPB.
Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
boundary attack. In International Conference on Machine Learning, 2020b.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020c.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Gil Fidel, Ron Bitton, and Asaf Shabtai. When explainability meets adversarial learning: Detecting
adversarial examples using shap signatures. arXiv preprint arXiv:1909.03418, 2019.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
10
Under review as a conference paper at ICLR 2021
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018.
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defense: Ensembles of weak defenses are not strong. In 11th {USENIX} Workshop on Offensive
Technologies ({WOOT} 17), 2017.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations, 2019.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. arXiv preprint arXiv:1907.07174, 2019.
Shengyuan Hu, Tao Yu, Chuan Guo, Wei-Lun Chao, and Kilian Q Weinberger. A new defense
against adversarial images: Turning a weakness into a strength. In Advances in Neural Information
Processing Systems,pp. 1635-1646, 2019.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125-136, 2019.
Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt. Testing robustness against
unforeseen adversaries. arXiv preprint arXiv:1908.08016, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
ICLR Workshop, 2017. URL https://arxiv.org/abs/1607.02533.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing
Systems, pp. 7167-7177, 2018.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Pratyush Maini, Eric Wong, and J. Zico Kolter. Adversarial robustness against the union of multiple
perturbation models. In International Conference on Machine Learning, 2020.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2574-2582, 2016.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark
the robustness of machine learning models. In Reliable Machine Learning in the Wild Workshop,
34th International Conference on Machine Learning, 2017. URL http://arxiv.org/abs/
1707.04131.
11
Under review as a conference paper at ICLR 2021
Leslie Rice, Eric Wong, and J Zico Kolter. Overfitting in adversarially robust deep learning. arXiv
preprint arXiv:2002.11569, 2020.
Jerome Rony, Luiz G Hafemann, Luiz S Oliveira, Ismail Ben Ayed, Robert SaboUrin, and Eric
Granger. Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and
defenses. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4322-4330, 2019.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robust neural network model on mnist. In International Conference on Learning Representations,
2018.
Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1-learning rate,
batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
P. Tabacof and E. Valle. Exploring the space of adversarial images. In 2016 International Joint
Conference on Neural Networks (IJCNN), pp. 426-433, 2016.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
Advances in Neural Information Processing Systems, pp. 5866-5876, 2019.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In International Conference on
Learning Representations, 2018.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Ro-
bustness may be at odds with accuracy. In International Conference on Learning Representations,
2018.
Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020.
Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier
perspective on model robustness in computer vision. In Advances in Neural Information Processing
Systems, pp. 13276-13286, 2019a.
Xuwang Yin, Soheil Kolouri, and Gustavo K Rohde. Adversarial example detection and classification
with asymmetrical adversarial training. arXiv preprint arXiv:1905.11475, 2019b.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482, 2019.
12
Under review as a conference paper at ICLR 2021
A	Problem Setting: Theoretical Analysis
In this section, we formally define the problem setting and motivate the distinctions made with respect
to the problem studied by Ilyas et al. (2019). The classification problem consists of two tasks: (1)
Predicting the correct class label of an adversarially perturbed (or benign) image using adversarially
robust classifier Mp ; and (2) Predicting the type of adversarial perturbation that the input image was
subjected to, using attack classifier Cadv .
Setup We consider the data to consist of inputs to be sampled from two multi-variate Gaussian
distributions such that the input-label pairs (x,y) can be described as:
y B" {-i,+i},
iid	(5)
X0〜N(yα,σ2), xι,...,xd"± N(yη,σ2)
where the input X 〜 N(yμ, Σ) ∈ R(d+1); η = α∕√d for some positive constant α; μ =
[α, η, . . . , η] ∈ R+(d+1) and Σ = σ2I ∈ R+(d+1)×(d+1). We can assume without loss of gen-
erality, that the mean for the two distributions has the same absolute value, since for any two
distributions with mean μι, μ2, we can translate the origin to μ1+ μ2. This setting demonstrates the
distinction between an input feature x0 that is strongly correlated with the input label and d weakly
correlated features that are normally distributed (independently) with mean yη and variance σ 2 each.
We adapt this setting from Ilyas et al. (2019) who used a stochastic feature x0 = y with probability p,
as opposed to a normally distributed input feature as in our case. All our findings hold in the other
setting as well, however, the chosen setting better represents true data distribution, with some features
that are strongly correlated to the input label, while others that have only a weak correlation.
B Separability of perturbation types (Theorem 1)
In this section, our goal is to evaluate whether the optimal perturbation confined within different `p
balls have different distributions and whether they are separable. We do so by developing an error
bound on the maximum error in classification of the perturbation types. The goal of the adversary is
to fool a standard (non-robust) classifier M. Cadv aims to predict the perturbation type based on only
viewing the adversarial image, and not the delta perturbation.
First, in Appendix B.1 we define a binary Gaussian classifier that is trained on the given task. Given
the weights of the binary classifier, we then identify the optimal adversarial perturbation for each of
the '1,'2,'∞ attack types in Appendix B.2. In Appendix B.3 we define the difference between the
adversarial input distribution for different `p balls. Finally, we calculate the error in classification of
these adversarial input types in Appendix B.4 to conclude the proof of Theorem 1.
B.1 B inary Gaussian Classifier
We assume for the purposes of this work that we have enough input data to be able to empirically
estimate the parameters μ, σ of the input distribution via sustained sampling. The multivariate
Gaussian representing the input data is given by:
p(x|y = yi) =
p21p∣∑exp (- 1(x - yi.μ)TςT(X- yiM
∀yi ∈ {-1, 1}	(6)
We want to find p(y = yi|X) ∀yi ∈ {-1, +1}. From Bayesian Decision Theory, the optimal decision
rule for separating the two distributions is given by:
y=1
p(y = 1)p(X|y = 1) > p(y = -1)p(X|y = -1)
y=-1
p(y = 1)p(X|y = 1) < p(y = -1)p(X|y = -1)
(7)
Therefore, for two Gaussian Distributions N(μι, ∑ι), N(μ2, ∑2),wehave:
13
Under review as a conference paper at ICLR 2021
y=1
0 < x>Ax - 2b>x + c
A = Σ1-1 - Σ2-1
b = ςi 1μι - ς2 lμ2
C = μ>∑-1μι - μ>∑-lμ2 + log k^j- 2log p(y =1)、
μ1 1 μ1	μ2 2 μ2 + g ll∑2∣l	gp(y = -1)
(8)
Substituting (6) and (7) in (8), we find that the optimal Bayesian decision rule for our problem is
given by:
x>μ y=10
(9)
which means that the label for the input can be predicted with the information of the sign of χ>μ
alone. We can define the parameters W ∈ Rd+1 of the optimal binary Gaussian classifier MW ,
such that kWk2 = 1 as:
αα
WO = -f≡,	Wi = ~~π=i	∀i ∈ {1, . . . , d}
2	2d
MW (x) = x>W
(10)
B.2 OPTIMAL ADVERSARIAL PERTURBATION AGAINST MW
Now, we calculate the optimal perturbation δ that is added to an input by an adversary in order to
fool our model. For the purpose of this analysis, we only aim to fool a model trained on the standard
classification metric as discussed in Section 4 (and not an adversarially robust model). The parameters
of our model are defined in (10).
The objective of any adversary δ ∈ ∆ is to maximize the loss of the label classifier MW . We assume
that the classification loss is given by —y X MW(x + δ). The object of the adversary is to find δ*
such that:
'(x + δ,y; MW) = -y × MW (x + δ) = —yx>W
δ* = arg max '(x + δ, y; MW)
= arg max -y(x + δ)>W = arg max -yδ>W
(11)
We will now calculate the optimal perturbation in the `p balls ∀p ∈ {1, 2, ∞}. For the following
analyses, we restrict the perturbation region ∆ to the corresponding `p ball of radius {1, 2, ∞}
respectively. We also note that the optimal perturbation exists at the boundary of the respective `p
balls. Therefore, the constraint can be re-written as :
δ* = arg max -yδ> W
kδkp =p
(12)
We use the following properties in the individual treatment of `p balls:
kδkp
∂j Mkp = P
P T
(13)
• P|MpT Sgn(Sj)
sgn(δj)
14
Under review as a conference paper at ICLR 2021
p = 2 Making use of langrange multipliers to solve (12), we have:
Vδ(-δ>Σ-1μ) = λVδ(kδkP - ep)
-W = λ0kδkpVδ (kδkp)
Combining the results from (13) and replacing δ with δ2 we obtain :
-W = λ0kδ2k2 (号)sgn(δ2)
δ2 = F (赢)=…
(14)
(15)
p = ∞ Recall that the optimal perturbation is given by :
δ*
arg max
kδk∞=∞
-yδ>W
arg max
kδk∞=∞
d
-yXδiWi
i=0
(16)
Since ∣∣δ∣∣∞ = e∞, We know that maxi ∣δ∕ = e∞. Therefore (16) is maximized when each
δi = -y∞ sgn Wi ∀i ∈ {0, . . . , d}. Further, since the weight matrix only contains non-negative
elements (α is a positive constant), we can conclude that the optimal perturbation is given by:
δ∞ = -y∞ 1
(17)
p = 1 We attempt an analytical solution for the optimal perturbation δ1 . Recall that the optimal
perturbation is given by :
d
δ* = arg max -y X δiWi
kδk1=1	i=1
d
= arg max -yδ0W0 - y	δiWi
kδk1=1	i=1
d
αα
=arg max -yδo^= - y〉δi~^=
%δ"ι 0 0√2	g± √2d
Since kδk1 = 1, (18) is maximized when:
δ0 = -y1 sgn(α) = -y1 ,	δi = 0 ∀i ∈ {1 . . .d}
(18)
(19)
Combining the results From the preceding discussion, it may be noted that the new distribution
of inputs within a given label changes by a different amount δ depending on the perturbation type.
Moreover, if the mean and variance of the distribution of a given label are known (which implies that
the corresponding true data label is also known), the optimal perturbation is independent of the input
itself, and only dependent on the respective class statistics (Note that the input is still important in
order to understand the true class).
B.3 PERTURBATION CLASSIFICATION BY Cadv
In this section, we aim to verify if it is possible to accurately separate the optimal adversarial inputs
crafted within different `p balls. For the purposes of this discussion, we only consider the problem
of classifying perturbation types into '1 and '∞, but the same analysis may also be extended more
generally to any number of perturbation types.
15
Under review as a conference paper at ICLR 2021
We will consider the problem of classifying the correct attack label for inputs from true class y = 1
for this discussion. Note that the original distribution:
Xt
rue 〜N(y.〃,
Since the perturbation value δp is fixed for all inputs corresponding to a particular label, the new
distribution of perturbed inputs Xi and X∞ in case of '1 and '∞ attacks respectively (for y = 1)is
given by:
Xi 〜N(μ + δι, Σ)
X∞ 〜N(μ + δ∞, Σ)
(20)
We now try to evaluate the conditions under which we can separate the two Gaussian distributions
with an acceptable worst-case error.
B.4 Calculating a bound on the error
Classification Error A classification error occurs if a data vector x belongs to one class but falls in
the decision region of the other class. That is in (7) the decision rule indicates the incorrect class.
(This can be understood through the existence of outliers)
Pe
P(error|x)p(x)dx
min [p(y = 'ι∣χ)p(χ),p(y = '∞∣χ)p(χ)] dx
(21)
Perturbation Size We set the radius of the '∞ ball, e∞ = η and the radius of the 'i ball, ei = α.
We further extend the discussion about suitable perturbation sizes in Appendix C.2. These values
ensure that the '∞ adversary can make all the weakly correlated labels meaningless by changing
the expected value of the adversarial input to less than 0 (E[xi + δ∞ (i)] ∀i > 0), while the `i
adversary can make the strongly correlated feature x0 meaningless by changing its expected value to
less than 0 (E[x0 + δi(0)]). However, neither of the two adversaries can flip all the features together.
Translating the axes We can translate the axis of reference by (一μ -(δ1+δ∞)) and define
μadv = (δ1-2δ∞), such that:
Xi 〜N(μadv, ∑)
X∞ 〜N(-μadv, ∑)
(22)
We can once again combine this with the simplified Bayesian model in (9) to obtain the classification
rule given by:
>	p=i
X μadv > 0
(23)
Combining the optimal perturbation definitions in (17) and (19) that μodv = (δ1^2δ∞) =
1 [-ei + e∞, e∞,..., e∞]. We can further substitute ei = α and e∞ = η = √. No-
tice that μadv(i) > 0 ∀i > 0. Without loss of generality, to simplify further discussion We
can flip the coordinates of x0 , since all dimensions are independent of each other. Therefore,
μadv
√j2 [√d - 1,1,..., i]. Consider a new variable Xz such that:
Xz = X0 ∙
(ι-√d)+√d X Xi=2 (χ>μadv)
(24)
since each Xi∀i ≥ 0 is independently distributed, the new feature Xz 〜N(μz,σz), where
16
Under review as a conference paper at ICLR 2021
σ
2
z
-2√d+ XXXd
(25)
σ2
Therefore, the problem simplifies to calculating the probability that the meta-variable xz > 0.
For σ > 10 and d> 1, we have in the z-table, Z > 10:
Pe ≤ 10-24	(26)
which suggests that the distributions are significantly distinct and can be easily separated. This
concludes the proof for Theorem 1.
Note: We can extend the same analysis to other `p balls as well, but we consider the case of `1 and
'∞ for simplicity.
C Robustness of the Protector Pipeline (Theorem 2)
In the previous section, we show that it is indeed possible to distinguish between the distribution of
inputs of a given class that were subjected to 'ι and '∞ perturbations over a standard classifier. Now,
we aim to develop further understanding of the robustness of our two-stage pipeline in a dynamic
attack setting with multiple labels to distinguish among. The first stage is a preliminary classifier
Cadv that classifies the perturbation type and the second stage consists of multiple models Mp that
were specifically trained to be robust to perturbations to the input within the corresponding `p norm.
First, in Appendix C.1, we calculate the optimal weights for a binary Gaussian classifier Mp, trained
on dataset D to be robust to adversaries within the `p ball ∀p ∈ {1, ∞}. Based on the weights
of the individual model, we fix the perturbation size p to be only as large, as is required to fool
the alternate model with high probability. Here, by ‘alternate, we mean that for an 'q attack, the
prediction should be made by the Mp,ep model,where p,q ∈ {1, ∞};P = q. In Appendix C.3 we
calculate the robustness of individual Mp models to `p adversaries, given the perturbation size p as
defined in Appendix C.2. In Appendix C.4, we analyze the modified distributions of the perturbed
inputs after different `p attacks. Based on this analysis, we construct a simple decision rule for the
perturbation classifier Cadv. Finally, in Appendix C.5 we determine the perturbation induced by the
worst-case adversary that has complete knowledge of both Cadv and Mp,p ∀p ∈ {1, ∞}. We show
how there exists a trade-off between fooling the perturbation classifier (to allow the alternate Mp,p
model to make the final prediction), and fooling the alternate Mp,p model itself.
Perturbation Size We set the radius of the '∞ ball, e∞ = η + Z∞ and the radius of the 'ι ball,
1 = α + ζ1 , where ζp are some small positive constants that we calculate in Appendix C.2. These
values ensure that the '∞ adversary can make all the weakly correlated labels meaningless by
changing the expected value of the adversarial input to less than 0 (E[xi + δ∞(i)] ∀i > 0), while
the `1 adversary can make the strongly correlated feature x0 meaningless by changing its expected
value to less than 0 (E[x0 + δ1(0)]). However, neither of the two adversaries can flip all the features
together. The exact values of ζp determine the exact success probability of the attacks. We defer
this calculation to later when we have calculated the weights of the models Mp . For the following
discussion, it may be assumed that ζp → 0 ∀p ∈ {1, ∞}.
17
Under review as a conference paper at ICLR 2021
C.1 B INARY GAUSSIAN CLASSIFIER Mp
Extending the discussion in Appendix B.1, we now examine the learned weights of a binary Gaussian
classifier Mp that is trained to be robust against perturbations within the corresponding `p ball of
radius p . The optimization equation for the classifier can be formulated as follows:
mWn E[-yx> W]+1 λ∣∣W∣∣2	(27)
where λ is tuned in order to make the '2 norm of the optimal weight distribution, ∣∣W*∣∣2, = 1.
Following the symmetry argument in Lemma D.1 Tsipras et al. (2018) we extend for the binary
Gaussian classifier that :
W； = W = WM ∀i,j ∈{i,∙∙∙,d}
(28)
We deal with the cases pertaining to p ∈ {∞, 1} in this section. For both the cases, we consider
existential solutions for the classifier Mp to simplify the discussion. This gives us lower bounds on
the performance of the optimal robust classifier. The robust objective under adversarial training can
be defined as:
d
Wo ∙(xo + δo)+ WM ∙ X(Xi + δi)
+ 1 λkWk2
min max E
W kδkp≤p
min
W
α + d × WM
+ 1 λkWk2 + llmaχ	E
2	kδkp≤p
-y	W0δ0+WMXd δi
)))
(29)
Further, since the λ constraint only ensures that ||W； ||2
equation by substituting W0 = √ 1 - d ∙ WM as follows,
1, we can simplify the optimization
min--1 (α ʌ/l - d ∙ Wm2 + d × WM
WM
+ max E
kδkp≤p
-y 卜 Jl - d ∙ Wm2 + WM X δ) }
(3i=0)1
p = ∞ As discussed in (17) the optimal perturbation δ∞ is given by -y∞ 1. The optimization
equation is simplified to:
min
WM
(∞-α)	1
-d ∙ Wm2 + d × Wm
(31)
Recall that e∞ =蠢 + Z∞. To simplify the following discussion we use the weights of a classifier
trained to be robust against perturbations within the '∞ ball of radius e∞ = √. The optimal solution
is then given by:
lim WM = 0
ζ∞ →0
(32)
Therefore, the classifier weights are given by W = [W0, W1, ∙ ∙ ∙ , Wd] = [1, 0, ∙ ∙ ∙ , 0]. We also
show later in Appendix C.3 that the model achieves greater than 99% accuracy against '∞ adversaries
for the chosen values of ζ∞ .
p = 1 We consider an analytical solution to yield optimal weights for this case. Recall from (19)
that the optimal perturbation δ1 depends on the weight distribution of the classifier. Therefore, if
W0 > WM the optimization equation can be simplified to
min ∣Wo(eι - α) - d × WM黄 + 1 λ∣∣W∣∣2
(33)
18
Under review as a conference paper at ICLR 2021
and if WM > W0
min - -W0α - WM (√dα - e1) + 1 λ∣∣W∣∣2
(34)
Recall that 1 = α + ζ1. Once again to simplify the discussion that follows we will lower bound
the robust accuracy of the classifier M1 by considering the optimal solution when zeta1 = 0. The
optimal solution is then given by:
lim W
ζ1→0
M
1
(35)
For the robust classifier Mi, the weights W = [Wo, Wι,..., Wd] = [0,册,册,...,册].While
this may not be the optimal solution for all values of ζ1, we are only interested in a lower bound
on the final accuracy and the classifier described by weights W simplifies the discussion hereon.
We also show later in Appendix C.3 that the model achieves greater than 99% accuracy against `1
adversaries for the chosen values of ζ1.
C.2 PERTURBATION SIZES FOR FOOLING Mp MODELS
Now that we exactly know the weights of the learned robust classifiers M1 and M∞, we can move
towards calculating values ζ1 and ζ∞ for the exact radius of the perturbation regions for the `1 and
'∞ metrics. We set the radii of these regions in such a way that an 'i adversary can fool the model
M∞ with probability 〜98% (corresponding to Z = 2 in the z-table for normal distributions), and
similarly, the success of '∞ attacks against the Mi model is 〜98%.
Let Pp1,p2 represent the probability that model Mp1 correctly classifies an adversarial input in the `p2
region. For pi = ∞ and p2 = 1,
P∞,1 = Px〜N(yμ,∑)[y ∙ M∞(X + δI) > 0]
=Px〜N(yμ,Σ) [y ∙ (x + δI)TW > 0]
≥ Px〜N(μ,Σ) [x0 > Ei]
i - α α + ζi - α ζi
z =----=---------= — = 2
σ	σσ
ζi = 2σ
Ei = α + 2σ
(36)
To simplify the discussion for the Mi model, we define a meta-feature xM as:
1d
XM = ~j= x x xi,
d
i=i
which is distributed as :
XM 〜N(yη√d, σ2)〜N(yα, σ2)
19
Under review as a conference paper at ICLR 2021
For pi =	1 and p2 = ∞, pi,∞ = Px〜N(yμ,Σ)[y ∙ MI(X + δ∞) > 0] =Px〜N(yμ,Σ) [V ∙ (x + δ∞)τW > 0] =Px〜N(yμ,∑)[y ∙ √d X(Xi +…)>0] V i=1 =Px〜N(yμ,∑)[y ∙ (xm - VZd ∙ e∞) > 0] ≥ Px〜N(μ,Σ) [xM > √d ∙ E∞]	(37) √d ∙ e∞ - α α + √d ∙ ζ∞ - a	√d ∙ ζ∞	C Z =	=	=	= 2 σ	σ	σ ζ∞ = √d α + 2σ & = ~7Γ
C.3 Robustness of individual MP models
Additional assumptions We add the following assumptions: (1) the dimensionality parameter d
of input data is larger than 100; and (2) the ratio of the mean and variance for feature xo is greater
than 10.
	d ≥ 100,	α ≥ 10	(38) σ
We define PP as the probability that for any given input X 〜N(yμ, Σ), the classifier MP outputs the
correct label y for the input X + δp.
p = ∞	p∞,∞ = Px〜N(yμ,Σ)[y ∙ m∞ (x + δ∞) > 0] =Px〜N(yμ,∑)[v ∙ (x + δ∞)TW > 0] =Px~N(yμ,Σ)[y ∙ (XO + δ∞(O)) > 0]	(39) ≥ Px〜N(μ,∑) [x0 > E∞] Z = e∞ - α = α (ɪ - 1、+ɪ σ	σ √d-	1) + √d
using the assumptions in (38),
P∞,∞ ≥ 0.999	(40)
P = I	pi,1 = Px〜N(yμ,∑)[y ∙ MI(X + δ1) > 0] =Px〜N(yμ,Σ) [y ∙ (x + δ1)TW > 0] 1	”、 =Px~N(yμ,Σ)[y ∙ √ E(Xi+δ1⑴)>0] √d i=1 =Px〜N(yμ,Σ)[y ∙ (XM + δM) > 0]	(41) ≥ Px〜N(μ,∑)卜M > √^ z = √⅛^ = σ (√d-1) + √d
using the assumptions in (38),
P1,1 ≥ 0.999	(42)
20
Under review as a conference paper at ICLR 2021
C.4 DECISION RULE FOR Cadv
We aim to provide a lower bound on the worst-case accuracy of the entire pipeline, through the
existence of a simple decision tree Cadv . For given perturbation budgets 1 and ∞, we aim to
understand the range of values that can be taken by the adversarial input. Consider the scenarios
described in Table 4 below:
Table 4: The table shows the range of the values that the mean can take depending on the decision
taken by the adversary. μadv and μMv represent the new mean of the distribution of features xo and
xM after the adversarial perturbation.
Attack Type	adv μ0		μMv	
	y=1	y = -1	y = 1	y = -1
None	α	-α	η√d	-η√d
'∞	{α - ∞ , α + ∞}	{-α - ∞ , -α + ∞}	{ηvzd + e∞d, η∕d - e∞d}	{-η√d + e∞d, -η√d - e∞d}
`1	{α - 1, α + 1}	{-α - 1, -α + 1}	{η^√d + eι,η√d - eι}	{-η√d + 61, -ηvzd - eι}
Note that any adversary that moves the perturbation away from the y-axis is uninteresting for our
comparison, since irrespective of a correct perturbation type prediction by Cadv, either of the two
second level models naturally obtain a high accuracy on such inputs. Hence, we define the following
decision rule with all the remaining cases mapped to `1 perturbation type.
Cadv(X) = [1, f l|X01- α<∞ + 2	(43)
0, otherwise
where the output 1 corresponds to the classifier predicting the presence of '∞ perturbation in the
input, while an output of 0 suggests that the classifier predicts the input to contain perturbations of
the `1 type.
If we consider a black-box setting where the adversary has no knowledge of the classifier Cadv , and
can only attack Mp it is easy to see that the proposed pipeline obtains a high adversarial accuracy
against the union of 'ι and '∞ perturbations:
Note: (1) There exists a single model that can also achieve robustness against the union of 'ι and '∞
perturbations, however, learning this model may be more challenging in real data settings. (2) The
classifier need not be perfect.
C.5 TRADE-OFF BETWEEN ATTACKING Mp AND Cadv
To obtain true robustness it is important that the entire pipeline is robust against adversarial attacks.
More specifically, in this section we demonstrate the natural tension that exists between fooling the top
level attack classifier (by making an adversarial attack less representative of its natural distribution)
and fooling the bottom level adversarially robust models (requiring stronger attacks leading to a
return to the attack’s natural distribution).
The accuracy of the pipelined model f against any input-label pair (x, y) sampled through some
distribution N(yμ0dv, Σ) (where μ°dv incorporates the change in the input distribution owing to the
adversarial perturbation) is given by:
P[f (X)= y] = Px〜N(yμadv ,Σ) [Cadv (X)] Px〜N(yμ.dv ,Σ)似∙ M∞ (X) > 0|Cadv (X)]
+	(I- Px〜N(yμadv,Σ) [Cadv (X)DPx〜N(yμ0dv,Σ) [y ∙ MI(X) > 0|-Cadv (X)]
=Px~N(μadv,Σ) [Cadv (x)] Px~N(μadv,Σ) [M∞(X) > 0|Cadv (x)]
+	(1 - Px〜N(μadv ,∑) [Cadv (X)DPx〜N(μadv ,∑) [M1 (X) > 0|-CadV (X)]
(44)
'∞ adversary: To simplify the analysis, we consider loose lower bounds on the accuracy of the
model f against the '∞ adversary. Recall that the decision of the attack classifier is only dependent
of the input X0 . Irrespective of the input features Xi∀i > 0, it is always beneficial for the adversary to
perturb the input by μ% = -e∞. However, the same does not apply for the input x°. Analyzing for the
21
Under review as a conference paper at ICLR 2021
scenario when the true label y = 1,ifthe input xo lies between 2 - e∞ of the mean a, irrespective of
the perturbation, the output of the attack classifier Cadv = 1. The M∞ model then always correctly
classifies these inputs. The overall robustness of the pipeline requires analysis for the case when input
lies outside 2 - e∞ of the mean as well. However, We consider that the adversary always succeeds in
such a case in order to only obtain a loose lower bound on the robust accuracy of the pipeline model
f against '∞ attacks.
P[f (X) = y] = Px~N(μadv,Σ) [Cadv (X)] Px~N(μadv,∑) [M∞ (X) > 0lCadv (X)]
+ (I- Pχ~N(μadv ,∑) [Cadv (X)DPx~N(μadv ,∑) [M1(χ) > 0|-Cadv (X)]
≥ Px~N(μadv,Σ) [Cadv (x)] Px~N(μ,
≥ Px~N(μ,Σ) [|X0 - a| ≤ 2 - e∞
≥ 2Px~N(μ,Σ) [x0 ≤ α -
(α — 2 + e∞) — α
Z =-----------------
σ
α
2 + e∞
α
adv,Σ) [M∞ (X) > 0|Cadv (X)]
i
i
3σ
2σ	2σ√d
(45)
using the assumptions in (38),
P[f (X) = y] ~ 0.99
(46)
`1 adversary: It may be noted that a trivial way for the `1 adversary to fool the attack clas-
sifier is to return a perturbation δ1 = 0. In such a scenario, the classifier predicts that the ad-
versarial image was subjected to an '∞ attack. The label prediction is hence made by the M∞
model. But we know from (40) that the M∞ model predicts benign inputs correctly with a prob-
ability P∞,∞ > 0.99, hence defeating the adversarial objective of misclassification. To achieve
misclassification over the entire pipeline the optimal perturbation decision for the `1 adversary
when Xo ∈ [-α — 2 — 6ι, —α + 2 + eι] the adversary can fool the pipeline by ensuring that the
Cadv(X) = 1. However, in all the other cases irrespective of the perturbation, either Cadv = 0 or
the input features Xo has the same sign as the label y. Since, P1,1 > 0.99 for the M1 model, for
all the remaining inputs Xo the model correctly predicts the label with probability greater than 0.99
(approximate lower bound). We formulate this trade-off to elaborate upon the robustness of the
proposed pipeline.
P[f (X)= y] = Px~N(“adv ,Σ) [Cadv (x)] Px~N(“adv ,Σ) [M∞ (X) > 0|Cadv (x)]
+ (1 — Px~N(“adv ,∑) [Cadv (X)DPx~N(“adv ,Σ) [M1(X) > 0l-Cadv (x)]
≥ Px~N(μ,Σ) [ —α - 2 — e1 ≤ X0 ≤ -α +2+ e1
+ 0.999(Px~N (μ,∑)
Ro < 一α —
2 — 6ι or Xo > —a + 2 + eι
≥ 0.999(Px~N(μ,∑)
[x0 < -a —
2 — ∈ι or Xo > —a + 2+ eι
(47)
)
)
using the assumptions in (38),
P [f(X) =y] ~ 0.99
(48)
This concludes the proof for Theorem 2, showing that an adversary can hardly stage successful
attacks on the entire pipeline and faces a natural tension between attacking the label predictor and
the attack classifier. Finally, we emphasize that the shown accuracies are lower bounds on the actual
robust accuracy, and the objective of this analysis is not to find the optimal solution to the problem
of multiple perturbation adversarial training, but to expose the existence of the trade-off between
attacking the two stages of the pipeline.
22
Under review as a conference paper at ICLR 2021
D	Model Architecture
Second-level Mp models. A key advantage of our PROTECTOR design is that we can build upon
existing defenses against individual perturbation type. Specifically, for MNIST, we use the same
CNN architecture as Zhang et al. (2019) for our Mp models, and we train these models using their
proposed TRADES loss. For CIFAR-10, we use the same training setup and model architecture as
Carmon et al. (2019), which is based on a robust self-training algorithm that utilizes unlabeled data to
improve the model robustness.
Perturbation classifier Cadv. For both MNIST and CIFAR-10 datasets, the architecture of the
perturbation classifier Cadv is similar to the individual Mp models. Specifically, for MNIST, we
use the CNN architecture in Zhang et al. (2019) with four convolutional layers, followed by two
fully-connected layers. For CIFAR-10, Cadv is a WideResNet (Zagoruyko & Komodakis, 2016)
model with depth 28 and widening factor of 10 (WRN-28-10).
E Training Details
E.1	SPECIALIZED ROBUST PREDICTORS Mp
MNIST. We use the Adam optimizer (Kingma & Ba, 2015) to train our models along with a
piece-wise linearly varying learning rate schedule (Smith, 2018) to train our models with maximum
learning rate of 10-3. The base models M1, M2, M∞ are trained using the TRADES algorithm for
20 iterations, and step sizes αι = 2.0, α2 = 0.3, and α∞ = 0.05 for the '1, '2, '∞ attack types within
perturbation radii 1 = 10.0, 2 = 2.0, and ∞ = 0.3 respectively.2
CIFAR10. The individual Mp models are trained to be robust against {'∞, '1, '2} perturbations of
{∞, 1, 2} = {0.003, 12.0, 0.05} respectively. For CIFAR10, the attack step sizes {α∞, α1, α2} =
{0.005, 2.0, 0.1} respectively. The training of the individual Mp models is directly based on the work
of Carmon et al. (2019).
E.2 PERTURBATION CLASSIFIER Cadv
MNIST. We use a learning rate of 0.01 and Adam optimizer for 10 epochs, with linear rate decay to
0.001 between the fourth epoch and the tenth epoch. The batch size is set to 100 for all experiments.
CIFAR10. We use a learning rate of 0.01 and SGD optimizer for 5 epochs, with linear rate decay to
0.001 between the fourth epoch and the tenth epoch. The batch size is set to 100 for all experiments.
Creating the Adversarial Perturbation Dataset. We create a static dataset of adversarially
perturbed images and their corresponding attack label for training the perturbation classifier
Cadv . For generating adversarial images, we perform weak adversarial attacks that are faster
to compute. In particular, we perform 10 iterations of the PGD attack. For MNIST, the at-
tack step sizes {α∞, α1, α2} = {0.05, 2.0, 0.3} respectively. For CIFAR10, the attack step sizes
{α∞, α1, α2} = {0.005, 2.0, 0.1} respectively. Note that we perform the Sparse-'1 or the top-k PGD
attack for the '1 perturbation ball, as introduced by Tramer & Boneh (2019). We set the value of k to
10, that is we move by a step size 贽 in each of the top 10 directions with respect to the magnitude of
the gradient.
F	Attacks Used for Evaluation
A description of all the attacks used for evaluation of the models is presented here. From the Foolbox
library(Rauber et al., 2017), apart from '1, '2 and '∞ PGD adversaries, we also evaluate the following
attacks for different perturbation types.
(1)	For '1 perturbations, we include the Salt & Pepper Attack (SAPA) (Rauber et al., 2017) and
Pointwise Attack (PA) (Schott et al., 2018).
2We use the Sparse '1 descent Tramer & Boneh (2019) for the PGD attack in the '1 constraint.
23
Under review as a conference paper at ICLR 2021
Table 5: Vanilla Model: Empirical overlap of 'p attack perturbations in different 'q % regions for
(a) MNIST (1,2,∞) = (10, 2.0, 0.3); (b) CIFAR-10 (1,2,∞) = (12, 0.5, 0.03). Each column
represents the range (min - max) of 'q norm for perturbations generated using 'p PGD attack.
Attack		MNIST			CIFAR10		
	'∞ < 0∙3	'2 < 2∙0	'ι < 10	'∞ < 0.03	'2 < 0.5	'1 < 12
PGD '∞	≤ 0.3	(3.67 - 6.05)	(54.8 - 140.9)	≤ 0.03	(1.33 - 1.59)	(62.7 - 85.5)
PGD '2	(0.40-0.86)	≤ 2.0	(11.2 - 24.1)	(0.037-0.10)	≤ 0.05	(15.4 - 20.9)
Sparse '1	(0.70-1.0)	(2.08 - 2.92)	≤ 10.0	(0.27 - 0.77)	(1.32 - 1.88)	≤ 12.0
Table 6: Protector： Empirical overlap of 'p attack perturbations in different 'q Wq regions for
(a) MNIST (1,2,∞) = (10, 2.0, 0.3); (b) CIFAR-10 (1,2,∞) = (12, 0.5, 0.03). Each column
represents the range (min - max) of 'q norm for perturbations generated using 'p PGD attack.
Attack		MNIST			CIFAR10		
	'∞ < 0.3	'2 < 2.0	'ι < 10	'∞ < 0.03	'2 < 0.5	'1 < 12
PGD '∞	≤ 0.3	(5.03-6.12)	(100.40-138.52)	≤ 0.03	(1.46-1.69)	(73.15-93.26)
PGD '2	(0.35—0.95)	≤2.0	(17.06-27.88)	(0.036-0.29)	≤0.05	(5.83-21.21)
Sparse '1	(0.81-1.0)	(2.13-2.98)	≤10.0	(0.42-1.0)	(1.50-2.91)	≤12.0
(2)	For '2 perturbations, we include the Gaussian noise attack (Rauber et al., 2017), Boundary Attack
(Brendel et al., 2018), DeepFool (Moosavi-Dezfooli et al., 2016), DDN attack (Rony et al., 2019),
and C&W attack (Carlini & Wagner, 2017b).
(3)	For '∞ perturbations, we include FGSM attack (Goodfellow et al., 2015) and the Momentum
Iterative Method (Dong et al., 2018).
From the AutoAttack library from Croce & Hein (2020c), we make use of all the three variants of
the Adaptive PGD attack (APGD-CE, APGD-DLR, APGD-T) (Croce & Hein, 2020c) along with
the targeted and standard version of Fast Adaptive Boundary Attack (FAB, FAB-T) (Croce & Hein,
2020b) and the Square Attack (Andriushchenko et al., 2020). We utilize the AA+ version for strong
attacks.
Attack Hyperparameters For the attacks in the Foolbox and AutoAtack libraries we use the
default parameter setting in the strongest available mode (such as AA+). For the custom PGD attacks,
we evaluate the models with 10 restarts and 200 iterations of the PGD attack. The step size of
the {'∞, '1, '2} PGD attacks are set as follows: For MNIST, the attack step sizes {α∞, α1, α2} =
{0.01, 1.0, 0.1} respectively. For CIFAR10, the attack step sizes {α∞, α1, α2} = {0.003, 1.0, 0.02}
respectively.
Further, similar to Tramer & Boneh (2019); Maini et al. (2020) We evaluate our models on the
first 1000 images of the test set of MNIST and CIFAR-10, since many of the attacks employed are
extremely computationally expensive and sloW to run. Specifically, on a single GPU, the entire
evaluation for a single model against all the attacks discussed With multiple restarts Will take nearly 1
month, and is not feasible.
G	Empirical Perturbation Overlap
FolloWing Section 6.2, We also present results on the perturbation overlap When We attack Protector
With PGD attacks. To contrast the results With that of attacking a vanilla model, We also present the
table in the main paper for convenience. It is noteWorthy that the presence of a perturbation classifier
forces the adversaries to generate such attacks that increase the norm of the perturbations in alternate
'q region. Secondly, We also observe that in the case of CIFAR10, the '2 PGD attack has a large
overlap With the '1 norm of radius 10. HoWever, recall that in case of '2 attacks for CIFAR10, both
the base models M1 and M∞ Were satisfactorily robust. Hence, the attacker has no incentive to
reduce the perturbation radius for an 'q norm since the perturbation classifier only performs a binary
classification betWeen '1 and '∞ attacks. The results can be observed in Tables 5 and 6.
24
Under review as a conference paper at ICLR 2021
Table 7: Perturbation type classification accuracy for different perturbation types. Note that the pertur-
bation classifier Cadv is only trained on adversarial examples against two Mp models. Each column
represent the model used to create transfer-based attack via the attack type in the corresponding row.
The represented accuracy is an aggregate over 1000 randomly sampled attacks of the '∞,'2, '1 types
for the corresponding algorithms (and datasets).
	M∞	M2	M1	MAX	AVG	MSD
MNIST-PGD	100.0%	100.0%	99.3%	99.0%	99.6%	99.1%
MNIST-AutoAttack	100.0%	100.0%	99.0%	99.5%	100.0%	100.0%
cifario-pgd	99.90%	99.50%	100.0%	100.0%	98.70%	95.70%
CIFAR10-AUtoAttaCk	99.90%	99.90%	100.0%	100.0%	99.70%	99.70%
H B reakdown of Complete Evaluation
In this section, we present the results of the perturbation type classifier Cadv against transfer adver-
saries. We also present the breakdown results of the adversarial robustness of baseline approaches
and our Protector pipeline against all the attacks that we tried, and also report the worst case
performance against the union of all attacks.
H.1 ROBUSTNESS OF Cadv
The results for the robustness of the perturbation classifier Cadv in the presence of adaptive adversaries
is presented in Table 7. Note that Cadv transfers well across the board, even if the adversarial examples
are generated against new models that are unseen for Cadv during training, achieving extremely high
test accuracy. Further, even if the adversarial attack was generated by a different algorithm such as
from the AutoAttack library, the transfer success of Cadv still holds up. In particular, the obtained
accuracy is > 95% across all the individual test sets created. The attack classification accuracy is in
general highest against those generated by attacking M1 or M∞ for CIFAR10, and M2 or M∞ for
MNIST. This is an expected consequence of the nature of generation of the static dataset for training
the perturbation classifier Cadv as described in Section 5.1.
H.2 MNIST
In Table 8, we provide a breakdown of the adversarial accuracy of all the baselines, individual
Mp models and the PROTECTOR method, with both the adaptive and standard attack variants on
the MNIST dataset. PROTECTOR outperforms prior baselines by 6.9% on the MNIST dataset. It
is important to note that Protector shows significant improvements against most attacks in the
suite. Compared to the previous state-of-the-art defense against multiple perturbation types (MSD),
if we compare the performance gain on each individual attack algorithm, the improvement is also
significant, with an average accuracy increase of 15.5% on MNIST dataset. These results demonstrate
that Protector considerably mitigates the trade-off in accuracy against individual attack types.
H.3 CIFAR- 1 0
In Table 9, we provide a breakdown of the adversarial accuracy of all the baselines, individual
Mp models and the PROTECTOR method, with both the adaptive and standard attack variants on
the CIFAR10 dataset. PROTECTOR outperforms prior baselines by 8.9%. Once again, note that
Protector shows significant improvements against most attacks in the suite. Compared to the
previous state-of-the-art defense against multiple perturbation types (MSD), if we compare the
performance gain on each individual attack algorithm, the improvement is significant, with an average
accuracy increase of 14.2% on. These results demonstrate that PROTECTOR considerably mitigates
the trade-off in accuracy against individual attack types.
Further, Protector also retains a higher accuracy on benign images, as opposed to past defenses
that have to sacrifice the benign accuracy for the robustness on multiple perturbation types. In
particular, the clean accuracy of PROTECTOR is over 6% higher than such existing defenses on
CIFAR-10, and the accuracy is close to Mp models trained for a single perturbation type.
25
Under review as a conference paper at ICLR 2021
	M∞	M2	Mi	MAX	AVG	MSD	Ours	Ours*
Benign Accuracy	99.2%	98.7%	98.8%	98.6%	99.1%	98.3%	98.7%	98.7%
'∞ attacks (E = 0.3)								
FGSM	96.3%	87.7%	12.2%	81.2%	85.4%	83.0%	94.8%	94.8%
PGD-Foolbox	95.5%	27.5%	0.4%	67.8%	76.0%	76.2%	86.7%	95.1%
MIM	94.1%	52.5%	1.1%	70.6%	76.7%	74.2%	86.8%	96.7%
APGD-CE	91.6%	3.8%	0.0%	41.0%	59.0%	65.3%	86.9%	91.6%
APGD-DLR	91.9%	7.7%	0.0%	43.8%	61.9%	65.8%	91.8%	91.5%
APGD-T	91.8%	2.8%	0.0%	39.6%	58.9%	64.5%	90.5%	92.5%
FAB	91.8%	5.5%	0.0%	51.0%	65.7%	66.8%	95.6%	97.7%
FAB-T	92.6%	4.9%	0.0%	48.7%	64.2%	65.5%	95.2%	98.0%
SQUARE-T	90.1%	7.3%	0.0%	46.0%	64.9%	68.0%	97.2%	97.3%
`2 attacks (E = 2.0)								
PGD-'2	78.2%	73.6%	50.6%	62.6%	65.9%	71.0%	73.5%	74.3%
PGD-Foolbox	95.4%	83.2%	60.2%	74.3%	79.2%	79.0%	83.7%	83.3%
Gaussian Noise	99.4%	98.5%	98.6%	98.6%	99.1%	98.1%	98.5%	98.5%
DeepFool	94.4%	88.9%	71.0%	81.0%	86.7%	81.7%	87.7%	88.0%
DDN	41.7%	75.9%	53.2%	62.1%	64.7%	70.3%	92.4%	94.7%
CWL2	51.7%	76.3%	54.7%	63.6%	65.2%	71.5%	86.8%	98.2%
APGD-CE	78.5%	74.2%	50.8%	61.9%	64.9%	69.7%	75.7%	76.4%
APGD-DLR	79.6%	75.4%	53.9%	63.2%	65.1%	70.8%	76.5%	78.2%
APGD-T	80.4%	73.5%	48.0%	61.2%	63.8%	69.4%	74.5%	76.8%
FAB	11.6%	77.5%	54.9%	63.4%	64.3%	69.5%	98.0%	98.0%
FAB-T	13.2%	74.9%	49.4%	62.3%	63.6%	69.0%	86.3%	96.3%
SQUARE-T	25.9%	81.9%	67.1%	71.9%	71.4%	73.7%	96.5%	96.6%
`1 attacks (E = 10.0)								
PGD-'ι	65.6%	73.6%	78.2%	52.0%	57.1%	65.5%	69.3%	79.4%
Salt & Pepper	67.1%	95.4%	96.2%	93.9%	88.2%	91.2%	95.2%	95.4%
Pointwise Attack	15.6%	84.1%	88.4%	66.0%	46.2%	78.3%	79.5%	89.5%
FAB-T	19.1%	71.5%	77.9%	43.9%	41.1%	67.6%	82.1%	95.0%
Table 8: Attack-wise breakdown of adversarial robustness on the MNIST dataset. Ours represents
the PROTECTOR method against the adaptive attack strategy described in Section 5.2, and Ours*
represents the standard attack setting.
26
Under review as a conference paper at ICLR 2021
	M∞	M2	Mi	MAX	AVG	MSD	Ours	Ours*
Benign Accuracy	89.4%	93.9%	89.3%	81.0%	84.6%	81.1%	90.8%	90.8%
'∞ attacks (E = 0.03)								
FGSM	70.2%	62.7%	51.3%	53.3%	50.4%	53.0%	80.4%	72.1%
PGD-Foolbox	69.3%	53.9%	47.0%	52.3%	49.0%	52.0%	68.8%	68.9%
MIM	66.1%	49.8%	43.0%	49.0%	44.5%	49.6%	67.4%	72.1%
APGD-CE	61.9%	35.6%	35.7%	38.7%	41.0%	46.2%	64.8%	63.8%
APGD-DLR	61.1%	38.0%	37.7%	39.6%	43.0%	46.4%	62.5%	63.2%
APGD-T	59.4%	34.8%	35.1%	36.8%	39.7%	43.8%	62.0%	62.1%
FAB	60.5%	38.2%	36.5%	40.9%	40.4%	44.4%	87.1%	88.1%
FAB-T	60.0%	35.8%	35.5%	40.8%	40.2%	44.0%	78.2%	83.1%
SQUARE-T		67.2%	57.4%	50.5%	51.7%	50.9%	51.5%	86.1%	84.1%
'2 attacks (E = 0.5)								
PGD-'2	66.4%	77.5%	72.4%	64.5%	67.8%	66.2%	67.0%	68.3%
PGD-Foolbox	68.7%	78.1%	72.7%	64.9%	67.9%	66.3%	69.8%	70.4%
Gaussian Noise	89.4%	93.9%	89.1%	81.7%	84.7%	82.3%	89.3%	89.3%
DeePFool	72.9%	79.4%	73.0%	64.3%	67.3%	65.6%	72.8%	73.1%
DDN	66.8%	77.5%	72.6%	64.6%	67.7%	66.2%	83.6%	85.2%
CWL2	67.2%	77.5%	72.0%	63.2%	71.5%	65.1%	88.4%	68.3%
APGD-CE	66.2%	77.4%	72.3%	64.3%	67.2%	66.1%	68.8%	70.2%
APGD-DLR	65.6%	77.6%	71.9%	62.9%	66.0%	65.3%	68.8%	69.3%
APGD-T	64.8%	77.3%	71.5%	62.1%	65.5%	64.5%	67.8%	68.6%
FAB	65.7%	77.8%	71.8%	62.5%	65.6%	64.6%	89.9%	88.2%
FAB-T	65.0%	77.4%	71.7%	62.7%	65.6%	64.5%	85.2%	85.6%
SQUARE-T		80.8%	86.2%	82.0%	72.0%	77.3%	72.1%	89.4%	88.6%
'1 attacks (E = 12.0)								
PGD-'1	20.6%	38.2%	55.9%	41.2%	54.7%	53.4%	55.9%	56.3%
Salt & Pepper	66.9%	81.1%	80.2%	73.8%	80.2%	74.2%	74.4%	72.0%
Pointwise Attack	50.4%	70.6%	74.6%	54.9%	75.1%	69.6%	78.9%	70.1%
FAB-T		28.3%	36.8%	56.4%	41.3%	60.2%	52.2%	70.5%	70.1%
Table 9: Attack-wise breakdown of adversarial robustness on the CIFAR-10 dataset. Ours represents
the PROTECTOR method against the adaptive attack strategy described in Section 5.2, and Ours*
represents the standard attack setting.
27
Under review as a conference paper at ICLR 2021
Table 10: Comparison between using a ‘softmax’ based aggregation of predictions from different
specialized models versus using the prediction from the model corresponding to the most likely attack
(only at inference time). Results are presented for APGD '2, '∞ attacks on the CIFAR10 dataset.
Attack	Max-approach (Eq. 1)	Softmax-approach (Eq. 4)
APGD-CE `2 (2 = 0.5)	75.7%	75.6%
APGD-DLR `2 (2 = 0.5)	76.5%	76.7%
APGD-CE '∞ (e∞ = 0.03)	86.9%	86.9%
APGD-DLR '∞ (e∞ = 0.03)		91.8%	91.2%
H.4 AGGREGATING PREDICTIONS FROM DIFFERENT Mp AT INFERENCE
In all our experiments in this work the adversary constructs adversarial examples using the softmax
based adaptive strategy for aggregating predictions from different Mp models, as described in
Equation 4 for the column ‘Ours’ and using the ‘max’ strategy (Equation 1) for results described in
the column ‘Ours*’
However, for consistency of our defense strategy irrespective of the attacker’s strategy, the defender
only utilizes predictions from the specialized model Mp corresponding to the most-likely attack
(Equation 1) to provide the final prediction (only forward propagation) for generated adversarial
examples. In our evaluation, we found a negligible impact of changing this aggregation to the
‘softmax’ strategy for aggregating the predictions. For example, we show representative results in
case of the APGD ('∞, '2) attacks on the CIFAR10 dataset in Table 10.
28