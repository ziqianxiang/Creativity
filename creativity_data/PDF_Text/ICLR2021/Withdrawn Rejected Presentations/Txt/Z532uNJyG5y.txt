Under review as a conference paper at ICLR 2021
Iterative Graph Self-Distillation
Anonymous authors
Paper under double-blind review
Ab stract
How to discriminatively vectorize graphs is a fundamental challenge that attracts
increasing attentions in recent years. Inspired by the recent success of unsupervised
contrastive learning, we aim to learn graph-level representation in an unsupervised
manner. Specifically, we propose a novel unsupervised graph learning paradigm
called Iterative Graph Self-Distillation (IGSD) which iteratively performs the
teacher-student distillation with graph augmentations. Different from conventional
knowledge distillation, IGSD constructs the teacher with an exponential moving
average of the student model and distills the knowledge of itself. The intuition
behind IGSD is to predict the teacher network representation of the graph pairs
under different augmented views. As a natural extension, we also apply IGSD to
semi-supervised scenarios by jointly regularizing the network with both supervised
and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-
trained models with self-training can further improve the graph representation
power. Empirically, we achieve significant and consistent performance gain on
various graph datasets in both unsupervised and semi-supervised settings, which
well validates the superiority of IGSD.
1 Introduction
Graphs are ubiquitous representations encoding relational structures across various domains. Learning
low-dimensional vector representations of graphs is critical in various domains ranging from social
science (Newman & Girvan, 2004) to bioinformatics (Duvenaud et al., 2015; Zhou et al., 2020). Many
graph neural networks (GNNs) (Gilmer et al., 2017; Kipf & Welling, 2016; Xu et al., 2018) have
been proposed to learn node and graph representations by aggregating information from every node’s
neighbors via non-linear transformation and aggregation functions. However, the key limitation of
existing GNN architectures is that they often require a huge amount of labeled data to be competitive
but annotating graphs like drug-target interaction networks is challenging since it needs domain-
specific expertise. Therefore, unsupervised learning on graphs has been long studied, such as graph
kernels (Shervashidze et al., 2011) and matrix-factorization approaches (Belkin & Niyogi, 2002).
Inspired by the recent success of unsupervised representation learning in various domains like images
(Chen et al., 2020b; He et al., 2020) and texts (Radford et al., 2018), most related works in the
graph domain either follow the pipeline of unsupervised pretraining (followed by fine-tuning) or
InfoMax principle (Hjelm et al., 2018). The former often needs meticulous designs of pretext tasks
(Hu et al., 2019; You et al., 2020) while the latter is dominant in unsupervised graph representation
learning, which trains encoders to maximize the mutual information (MI) between the representations
of the global graph and local patches (such as subgraphs) (Velickovic et al., 2018; SUn et al., 2019;
Hassani & Khasahmadi, 2020). However, MI-based approaches usually need to sample subgraphs as
local views to contrast with global graphs. And they usually require an additional discriminator for
scoring local-global pairs and negative samples, which is computationally prohibitive (Tschannen
et al., 2019). Besides, the performance is also very sensitive to the choice of encoders and MI
estimators (Tschannen et al., 2019). Moreover, MI-based approaches cannot be handily extended
to the semi-supervised setting since local subgraphs lack labels that can be utilized for training.
Therefore, we are seeking an approach that learns the entire graph representation by contrasting the
whole graph directly without the need of MI estimation, discriminator and subgraph sampling.
Motivated by recent progress on contrastive learning, we propose the Iterative Graph Self-Distillation
(IGSD), a teacher-student framework to learn graph representations by contrasting graph instances
directly. The high-level idea of IGSD is based on graph contrastive learning where we pull sim-
1
Under review as a conference paper at ICLR 2021
ilar graphs together and push dissimilar graph away. However, the performance of conventional
contrastive learning largely depends on how negative samples are selected. To learn discriminative
representations and avoid collapsing to trivial solutions, a large set of negative samples (He et al.,
2020; Chen et al., 2020b) or a special mining strategy (Schroff et al., 2015; He et al., 2020) are
necessary. In order to alleviate the dependency on negative samples mining and still be able to learn
discriminative graph representations, we propose to use self-distillation as a strong regularization to
guide the graph representation learning.
In the IGSD framework, graph instances are augmented as several views to be encoded and projected
into a latent space where we define a similarity metric for consistency-based training. The parameters
of the teacher network are iteratively updated as an exponential moving average of the student network
parameters, allowing the knowledge transfer between them. As merely small amount of labeled data
is often available in many real-world applications, we further extend IGSD to the semi-supervised
setting such that it can effectively utilize graph-level labels while considering arbitrary amounts
of positive pairs belonging to the same class. Moreover, in order to leverage the information from
pseudo-labels with high confidence, we develop a self-training algorithm based on the supervised
contrastive loss for fine-tuning.
We experiment with real-world datasets in various scales and compare the performance of IGSD
with state-of-the-art graph representation learning methods. Experimental results show that IGSD
achieves competitive performance in both unsupervised and semi-supervised settings with different
encoders and data augmentation choices. With the help of self-training, our performance can exceed
state-of-the-art baselines by a large margin.
To summarize, we make the following contributions in this paper:
•	We propose a self-distillation framework called IGSD for unsupervised graph-level representation
learning where the teacher-student distillation is performed for contrasting graph pairs under
different augmented views.
•	We further extend IGSD to the semi-supervised scenario, where the labeled data are utilized
effectively with the supervised contrastive loss and self-training.
•	We empirically show that IGSD surpasses state-of-the-art methods in semi-supervised graph
classification and molecular property prediction tasks and achieves performance competitive with
state-of-the-art approaches in unsupervised graph classification tasks.
2	Related Work
Contrastive Learning Modern unsupervised learning in the form of contrastive learning can be
categorized into two types: context-instance contrast and context-context contrast (Liu et al., 2020).
The context-instance contrast, or so-called global-local contrast focuses on modeling the belonging
relationship between the local feature of a sample and its global context representation. Most
unsupervised learning models on graphs like DGI (Velickovic et al., 2018), InfoGraPh (SUn et al.,
2019), CMC-Graph (Hassani & Khasahmadi, 2020) fall into this category, following the InfoMax
principle to maximize the the mutual information (MI) between the input and its representation.
However, estimating MI is notoriously hard in MI-based contrastive learning and in practice tractable
lower bound on this quantity is maximized instead. And maximizing tighter bounds on MI can result
in worse representations without stronger inductive biases in sampling strategies, encoder architecture
and parametrization of MI estimators (Tschannen et al., 2019). Besides, the intricacies of negative
sampling in MI-based approaches impose key research challenges like improper amount of negative
samples or biased negative sampling (Tschannen et al., 2019; Chuang et al., 2020). Another line
of contrastive learning approaches called context-context contrast directly study the relationships
between the global representations of different samples as what metric learning does. For instance,
a recently proposed model BYOL (Grill et al., 2020) bootstraps the representations of the whole
images directly. Focusing on global representations between samples and corresponding augmented
views also allows instance-level supervision to be incorporated naturally like introducing supervised
contrastive loss (Khosla et al., 2020) into the framework for learning powerful representations.
Graph Contrastive Coding (GCC) (Qiu et al., 2020) is a pioneer to leverage instance discrimination
as the pretext task for structural information pre-training. However, our work is fundamentally
different from theirs. GCC focuses on structural similarity to find common and transferable structural
2
Under review as a conference paper at ICLR 2021
patterns across different graph datasets and the contrastive scheme is done through subgraph instance
discrimination. On the contrary, our model aims at learning graph-level representation by directly
contrasting graph instances such that data augmentation strategies and graph labels can be utilized
naturally and effectively.
Knowledge Distillation Knowledge distillation (Hinton et al., 2015) is a method for transferring
knowledge from one architecture to another, allowing model compression and inductive biases transfer.
Self-distillation (Furlanello et al., 2018) is a special case when two architectures are identical, which
can iteratively modify regularization and reduce over-fitting if perform suitable rounds (Mobahi et al.,
2020). However, they often focus on closing the gap between the predictive results of student and
teacher rather than defining similarity loss in latent space for contrastive learning.
Semi-supervised Learning Modern semi-supervised learning can be categorized into two kinds:
multi-task learning and consistency training between two separate networks. Most widely used semi-
supervised learning methods take the form of multi-task learning: arg minθ Ll(Dl, θ) + wLu (Du, θ)
on labeled data Dl and unlabeled data Du . By regularizing the learning process with unlabeled
data, the decision boundary becomes more plausible. Another mainstream of semi-supervised
learning lies in introducing student network and teacher network and enforcing consistency between
them (Tarvainen & Valpola, 2017; Miyato et al., 2019; Lee, 2013). It has been shown that semi-
supervised learning performance can be greatly improved via unsupervised pre-training of a (big)
model, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples
for refining and transferring the task-specific knowledge (Chen et al., 2020c). However, whether
task-agnostic self-distillation would benefit semi-supervised learning is still underexplored.
3	Preliminaries
3.1	Formulation
Unsupervised Graph Representation Learning Given a set of unlabeled graphs G = {Gi}iN=1,
we aim at learning the low-dimensional representation of every graph Gi ∈ G favorable for down-
stream tasks like graph classification.
Semi-supervised Graph Representation Learning Consider a whole dataset G = GL ∪ GU
composed by labeled data GL = {(Gi , yi)}li=1 and unlabeled data GU = {Gi }li+=ul+1 (usually u l),
our goal is to learn a model that can make predictions on graph labels for unseen graphs. And with K
augmentations, we get GL0 = {(G0k, yk0 )}kK=l 1 and GU0 = {G0k}kK=(ll++u1) as our training data.
3.2	Graph Representation Learning
We represent a graph instance as G(V, E) with the node set V and the edge set E. The dominant ways
of graph representation learning are graph neural networks with neural message passing mechanisms
(Hamilton et al., 2017): for every node v ∈ V, node representation hvk is iteratively computed from
the features of their neighbor nodes N (v) using a differentiable aggregation function. Specifically, at
the iteration k we get the node embedding as:
hV = σ (Wk ∙ CONCAT (hV-1, AGGREGATEk ({hU-1,∀u ∈ N(v)})))	(1)
Then the graph-level representations can be attained by aggregating all node representations using a
readout function like summation or set2set pooling (Vinyals et al., 2015).
3.3	Graph Data Augmentation
It has been shown that the learning performance of GNNs can be improved via graph diffusion, which
serves as a homophily-based denoising filter on both features and edges in real graphs (Klicpera et al.,
2019). The transformed graphs can also serve as effective augmented views in contrastive learning
(Hassani & Khasahmadi, 2020). Inspired by that, we transform a graph G with transition matrix
T via graph diffusion and sparsification S = Pk∞=0 θkT k into a new graph with adjacency matrix
S as an augmented view in our framework. While there are many design choices in coefficients
θk like heat kernel, we employ Personalized PageRank (PPR) with θkPPR = α(1 - α)k due to its
3
Under review as a conference paper at ICLR 2021
superior empirical performance (Hassani & Khasahmadi, 2020). As another augmentation choice,
we randomly remove edges of graphs to attain corrupted graphs as augmented views to validate the
robustness of models to different augmentation choices.
4	Iterative Graph Self-distillation
Intuitively, the goal of contrastive learning on graphs is to learn graph representations that are close
in the metric space for positive pairs (graphs with the same labels) and far between negative pairs
(graphs with different labels). To achieve this goal, IGSD employs the teacher-student distillation to
iteratively refine representations by contrasting latent representations embedded by two networks and
using additional predictor and EMA update to avoid collapsing to trivial solutions. Overall, IGSD
encourages the closeness of augmented views from the same graph instances while pushing apart the
representations from different ones.
4.1	Iterative Graph SELF-DISTILLATION Framework
In IGSD, We introduce a teacher-student architec-
ture comprises two networks in similar structure
composed by encoder fθ, projector gθ and predic-
tor hθ. We denote the components of the teacher
network and the student network as f8，, ge，and
fθ, gθ, he respectively.
The overview of IGSD is illustrated in Figure
1. In IGSD, the procedure of contrastive learn-
ing on negative pairs is described as follows: we
first augment the original input graphs Gj to get
augmented view(s) Gj. Then Gj and different
graph instance Gi are fed respectively into two
encoders fe ,fe，for extracting graph represen-
tations h, h0 = fe(Gi), fe， (G0j) with iterative
message passing in Eqn. (1) and readout func-
Figure 1: Overview ofIGSD. Illustration of our
framework in the case where we augment input
graphs G once to get G0 for only one forward
pass. Blue and red arrows denote contrast on
positive and negative pairs respectively.
tions. The following projectors ge , ge， transform
graph representations to projections z, z0 via z = ge(h) = W(2)σ(W(1)h) and z0 = ge， (h0) =
W 0(2)σ(W 0(1)h0), where σ denotes a ReLU nonlinearity1. To prevent collapsing into a trivial solution
(Grill et al., 2020), a specialized predictor is used in the student network for attaining the prediction
he(z) = Wh(2)σ(Wh(1)z) of the projection z. For positive pairs, we follow the same procedure except
feeding the original and augmented view of the same graph into two networks respectively.
To contrast latents he (z) and z0, we use L2 norm in the latent space to approximate the semantic
distance in the input space and the consistency loss can be defined as the mean square error between
the normalized prediction he (z) and projection z0. By passing two graph instances Gi and Gj
symmetrically, we can obtain the overall consistency loss:
Lcon(Gi,Gj)=	he(zi)-zj022+khe(zi0)-zjk22	(2)
With the consistency loss, the teacher network provides a regression target to train the student network,
and its parameters θ0 are updated as an exponential moving average (EMA) of the student parameters
θ after weights of the student network have been updated using gradient descent:
θt J τθ0-ι + (I - T)θt
(3)
With the above iterative self-distillation procedure, we can aggregate information for averaging
model weights over each training step instead of using the final weights directly (Athiwaratkun et al.,
2018). It should be noted that maintaining a slow-moving average network is also employed in some
models like MoCo (He et al., 2020) with different motivations: MoCo uses an EMA of encoder
1Although IGSD could directly predict the representations without projections, previous contrastive learning
work (Chen et al., 2020b) in the image domain has shown that using projections improves performance
empirically. We include the experimental results to validate the effects of projectors in Appendix A.3
4
Under review as a conference paper at ICLR 2021
and momentum encoder to update the encoder, ensuring the consistency of dictionary keys in the
memory bank. On the other hand, IGSD uses a moving average network to produce prediction targets,
enforcing the consistency of teacher and student for training the student network.
4.2	Unsupervised Learning with IGSD
In IGSD, to contrast the anchor Gi with other graph instances Gj (i.e. negative samples), we employ
the following unsupervised InfoNCE objective (Oord et al., 2018):
LUnsUp = -EGi 〜G
lo _______________exp(-Lcon(Gi,Gi))________________
Jg exp (-Lcon (Gi, Gi)) + Pj二 11i=∙∙ exp(-Lcon @, Gj))
(4)
At the inference time, as semantic interpolations on samples, labels and latents resUlt in better
representations and can improve learning performance greatly (Zhang et al., 2017; Verma et al., 2019;
Berthelot et al., 2019), we obtain the graph representation h by interpolating the latent representations
h = fθ (G) and h0 = f8, (G) with MiXUP function Mixλ(a, b) = λ ∙ a +(1 — λ) ∙ b:
~
h = Mixλ(h, h0)
(5)
4.3	Semi-supervised Learning with IGSD
To bridge the gap between UnsUpervised pretraining and downstream tasks, we eXtend oUr model to
the semi-sUpervised setting. In this scenario, it is straightforward to plUg in the UnsUpervised loss as a
regUlarizer for representation learning. However, the instance-wise sUpervision limited to standard
sUpervised learning may lead to biased negative sampling problems (ChUang et al., 2020). To tackle
this challenge, we can Use a small amoUnt of labeled data fUrther to generalize the similarity loss to
handle arbitrary nUmber of positive samples belonging to the same class:
Kl	Kl
LsUpCOn = X KNN7 XIi=j ∙ Iy0=yj ∙ LCOn(G，，Gj)	⑹
i=1 KNyi j=1
where Ny0 denotes the total nUmber of samples in the training set that have the same label yi0 as
anchor i. Thanks to the graph-level contrastive natUre of IGSD, we are able to alleviate the biased
negative sampling problems (Khosla et al., 2020) with sUpervised contrastive loss, which is crUcial
(ChUang et al., 2020) bUt Unachievable in most MI-based contrastive learning models since sUbgraphs
are generally hard to assign labels to. Besides, with this loss we are able to fine-tUne oUr model
effectively Using self-training where pseUdo-labels are assigned iteratively to Unlabeled data.
With the standard sUpervised loss like cross entropy or mean sqUare error L(GL, θ), the overall
objective can be sUmmarized as:
Lsemi = L(GL, θ) + wLUnsUp(GL ∪ GU, θ) + w0LsUpcon(GL, θ)	(7)
Common semi-sUpervised learning methods Use consistency regUlarization to measUre discrepancy
between predictions made on pertUrbed Unlabeled data points for better prediction stability and
generalization (Oliver et al., 2018). By contrast, oUr methods enforce consistency constraints between
latents from different views, which acts as a regUlarizer for learning directly from labels.
Labeled data provides eXtra sUpervision aboUt graph classes and alleviates biased negative sampling.
However, they are costly to attain in many areas. Therefore, we develop a contrastive self-training
algorithm to leverage label information more effectively than cross entropy in the semi-sUpervised
scenario. In the algorithm, we train the model Using a small amoUnt of labeled data and then fine-tUne
it by iterating between assigning pseUdo-labels to Unlabeled eXamples and training models Using the
aUgmented dataset. In this way, we harvest massive pseUdo-labels for Unlabeled eXamples.
With increasing size of the aUgmented labeled dataset, the discriminative power of IGSD can be
improved iteratively by contrasting more positive pairs belonging to the same class. In this way, we
accUmUlate high-qUality psUedo-labels after each iteration to compUte the sUpervised contrastive loss
in Eqn. (6) and make distinction from conventional self-training algorithms (Rosenberg et al., 2005).
On the other hand, traditional self-training can Use psUedo-labels for compUting cross entropy only.
5
Under review as a conference paper at ICLR 2021
5	Experiments
5.1	Experimental Setup
Evaluation Tasks. We conduct experiments by comparing with state-of-the-art models on three
tasks. In graph classification tasks, we experiment in both the unsupervised setting where we only
have access to all unlabeled samples in the dataset and the semi-supervised setting where we use a
small fraction of labeled examples and treat the rest as unlabeled ones by ignoring their labels. In
molecular property prediction tasks where labels are expensive to obtain, we only consider the
semi-supervised setting.
Datasets. For graph classification tasks, we employ several widely-used graph kernel datasets
(Kersting et al., 2016) for learning and evaluation: 3 bioinformatics datasets (MUTAG, PTC, NCI1)
and 3 social network datasets (COLLAB, IMDB-B, IMDB-M) with statistics summarized in Table
1. In the semi-supervised graph regression tasks, we use the QM9 dataset containing 134,000
drug-like organic molecules (Ramakrishnan et al., 2014) with 9 heavy atoms and select the first ten
physicochemical properties as regression targets for training and evaluation. For detailed description
of the properties in the QM9 dataset, see the Appendix C of (Sun et al., 2019).
Baselines. In the unsupervised graph classification, we compare with the following representative
baselines: CMC-Graph (Hassani & Khasahmadi, 2020), InfoGraph (Sun et al., 2019), GCC (Qiu
et al., 2020), Graph2Vec (Narayanan et al., 2017) and Graph Kernels including Random Walk Kernel
(Gartner et al., 2003), Shortest Path Kernel (Kashima et al., 2003), GraPhlet Kernel (Shervashidze
et al., 2009), Weisfeiler-Lehman Sub-tree Kernel (WL SubTree) (Shervashidze et al., 2011), Deep
GraPh Kernels (Yanardag & Vishwanathan, 2015), Multi-Scale LaPlacian Kernel (MLG) (Kondor &
Pan, 2016) and GraPh Convolutional Kernel Network (GCKN) (Chen et al., 2020a).
For the semi-suPervised graPh classification, we comPare our method with comPetitive baselines
like InfoGraPh, InfoGraPh* and Mean Teachers. And the GIN baseline doesn’t have access to
the unlabeled data. In the semi-suPervised molecular ProPerty Prediction tasks, baselines include
InfoGraPh, InfoGraPh* and Mean Teachers (Tarvainen & ValPola, 2017).
Model Configuration. In our framework, We use GCNs (KiPf & Welling, 2016) and GINs (Xu
et al., 2018) as encoders to attain node rePresentations for unsuPervised and semi-suPervised graPh
classification resPectively. For semi-suPervised molecular ProPerty Prediction, we emPloy message
Passing neural networks (MPNNs) (Gilmer et al., 2017) as backbone encoders to encode molecular
graPhs with rich edge attributes. All Projectors and Predictors are imPlemented as two-layer MLPs.
For more details on hyPer-Parameters selection, refer to aPPendix A.2
In semi-suPervised molecular ProPerty Prediction tasks, we generate multiPle views based on edge
attributes (bond tyPes) of rich-annotated molecular graPhs for imProving Performance. SPecifically,
we Perform label-Preserving augmentation to attain multiPle diffusion matrixes of every graPh on
different edge attributes while ignoring others resPectively. The diffusion matrix gives a denser graPh
based on each tyPe of edges to leverage edge features better. We train our models using different
numbers of augmented training data and select the amount using cross validation.
For unsuPervised graPh classification, we adoPt LIB-SVM (Chang & Lin, 2011) with C Parameter
selected in {1e-3, 1e-2, . . . , 1e2, 1e3} as our downstream classifier. Then we use 10-fold cross
validation accuracy as the classification Performance and rePeat the exPeriments 5 times to rePort the
mean and standard deviation. For semi-suPervised graPh classification, we randomly select 5% of
training data as labeled data while treat the rest as unlabeled one and rePort the best test set accuracy
in 300 ePochs. Following the exPerimental setuP in (Sun et al., 2019), we randomly choose 5000,
10000, 10000 samPles for training, validation and testing resPectively and the rest are treated as
unlabeled training data for the molecular ProPerty Prediction tasks.
5.2	Numerical Results
Results on unsupervised graph classification. We first Present the results of the unsuPervised
setting in Table 1. All graPh kernels give inferior Performance excePt in the PTC dataset. The
Random Walk kernel runs out of memory and the Multi-Scale LaPlacian Kernel suffers from a long
running time (exceeds 24 hours) in two larger datasets. IGSD outPerforms state-of-the-art baselines
6
Under review as a conference paper at ICLR 2021
slenreK hparG desivrepusn
Datasets # graphs # classes Avg # nodes	MUTAG 188 2 17.9	IMDB -B 1000 2 19.8	IMDB-M 1500 3 13.0	NCI1 4110 2 29.8	COLLAB 5000 3 74.5	PTC 344 2 25.5
Random Walk	83.7 ± 1.5	50.7 ± 0.3	34.7 ± 0.2	OMR	OMR	57.9 ± 1.3
Shortest Path	85.2 ± 2.4	55.6 ± 0.2	38.0 ± 0.3	51.3 ± 0.6	49.8 ± 1.2	58.2 ± 2.4
Graphlet Kernel	81.7 ± 2.1	65.9 ± 1.0	43.9 ± 0.4	53.9 ± 0.4	56.3 ± 0.6	57.3 ± 1.4
WL subtree	80.7 ± 3.0	72.3 ± 3.4	47.0 ± 0.5	55.1 ± 1.6	50.2 ± 0.9	58.0 ± 0.5
Deep Graph	87.4 ± 2.7	67.0 ± 0.6	44.6 ± 0.5	54.5 ± 1.2	52.1 ± 1.0	60.1 ± 2.6
MLG	87.9 ± 1.6	66.6 ± 0.3	41.2 ± 0.0	>1 Day	>1 Day	63.3 ± 1.5
GCKN	87.2 ± 6.8	70.5 ± 3.1	50.8 ± 0.8	70.6 ± 2.0	54.3±1.0	58.4 ± 7.6
Graph2Vec	83.2 ± 9.6	71.1 ± 0.5	50.4 ± 0.9	73.2 ± 1.8	47.9 ± 0.3	60.2 ± 6.9
InfoGraph	89.0 ± 1.1	74.2 ± 0.7	49.7 ± 0.5	73.8 ± 0.7	67.6 ± 1.2	61.7 ±1.7
CMC-GRAPH	89.7 ± 1.1	74.2 ± 0.7	51.2 ± 0.5	75.0 ± 0.7	68.9 ± 1.9	62.5 ± 1.7
GCC	86.4 ± 0.5	71.9 ± 0.5	48.9 ± 0.8	66.9 ± 0.2	75.2 ± 0.3	58.4 ± 1.2
Ours (Random)	85.7 ± 2.1	71.6 ± 1.2	49.2 ± 0.6	75.1 ± 0.4	65.8 ± 1.0	57.6 ± 1.5
Ours	90.2 ± 0.7	74.7 ± 0.6	51.5 ± 0.3	75.4 ± 0.3	70.4 ± 1.1	61.4 ± 1.7
Table 1: Graph classification accuracies (%) for kernels and unsupervised methods on 6
datasets. We report the mean and standard deviation of final results with five runs. ‘>1 day’
represents that the computation exceeds 24 hours. ‘OMR’ means out of memory error.
Datasets	IMDB-B	IMDB-M	COLLAB	NCI1
Mean Teachers	69.0	49.3	72.5	71.1
InfoGraph*	71.0	49.3	67.6	71.1
GIN (Supervised Only)	67.0	50.0	71.4	67.9
Ours (Unsup)	72.0	50.0	72.6	70.6
Ours (SupCon)	75.0	52.0	73.4	67.9
GIN (Supervised Only)+self-training	72.0	51.3	70.4	74.0
Ours (Unsup)+self-training	73.0	54,0	71	72.5
Ours (SupCon)+self-training	77.0	55.3	73.6	77.1
Table 2: Graph classification accuracies (%) of semi-supervised experiments on 4 datasets. We
report the best results on test set in 300 epochs.
like InfoGraph, CMC-Graph and GCC in most datasets, showing that IGSD can learn expressive
graph-level representations for downstream classifiers. Besides, our model still achieve competitive
results in datasets like IMDB-M and NCI1 with random dropping augmentation, which demonstrates
the robustness of IGSD with different choices of data augmentation strategies.
Results on semi-supervised graph classification. We further apply our model to semi-supervised
graph classification tasks with results demonstrated in Table 4, where we set w and w0 in Eqn. (7)
to be 1 and 0 as Ours (Unsup) while 0 and 1 as Ours (SupCon). In this setting, our model performs
better than Mean Teachers and InfoGraph*. Both the unsupervised loss and supervised contrastive
loss provide extra performance gain compared with GIN using supervised data only. Besides, both
of their performance can be improved significantly combined using self-training especially with
supervised contrastive loss. It makes empirical sense since self-training iteratively assigns psuedo-
labels with high confidence to unlabeled data, which provides extra supervision on their categories
under contrastive learning framework.
Results on semi-supervised molecular property prediction. We present the regression perfor-
mance of our model measured in the QM9 dataset in Figure 2. We display the performance of our
model and baselines as mean square error ratio with respect to supervised results and our model
outperforms all baselines in 9 out of 10 tasks compared with strong baselines InfoGraph, InfoGraph*
and Mean Teachers. And in some tasks like R2 (5), U0 (7) and U (8), IGSD achieves significant
performance gains against its counterparts, which demonstrates the ability to transfer knowledge
learned from unsupervised data for supervised tasks.
5.3	Ablation Studies and Analysis
Effects of self-training. We first investigate the effects of self-training for our model performance
in table 4. Results show that self-training can improve the GIN baseline and our models with
unsupervised loss (Unsup) or supervised contrastive loss (SupCon). The improvement is even
more significant combined with supervised contrastive loss since high-quality pseudo-labels provide
7
Under review as a conference paper at ICLR 2021
■ InfoGraph ■ InfoGraph* Mean Teachers ■ Ours (IGSD)
5 0 5
LL0.
O_1」0±l山
Mu (0) Alpha (1) HOMO (2) LUMO (3) Gap (4) R2 ⑸ ZPVE (6) UO ⑺ U (8) H (9)
Molecular Properties
72.5
25
膏 73∙5
⅛
ω
氏 73.0
74.5
9	54
Figure 2: Semi-supervised molecular property prediction results in terms of mean absolute
error (MAE) ratio. The hiStogram ShowS error ratio with reSpect to SuperViSed reSultS (1.0) of eVery
Semi-SuperViSed modelS. Lower ScoreS are better and a model outperformS the SuperViSed baSeline
when the Score iS leSS than 1.0.
48
10	20
30	40	50	60
Fraction of labeled data used (%)
2 O
5 5
(％) AωEJnωω4
50	75	100	125
Batch size (Negative Samples)
(a)	(b)
Figure 3: Ablation studies. (a) UnSuPerViSed performance with different batch size (number of
negative pairs); (b) Semi-supervised graph classification accuracy with different proportion of labeled
data.
additional information of graph categorieS. MoreoVer, our Self-training algorithm conSiStently
outperformS the traditional Self-training baSeline, which further ValidateS the Superiority of our model.
Effects of different amount of negative pairs. We then conduct ablation experimentS on the amount
of negatiVe pairS by Varying batch Size oVer {16, 32, 64, 128} with reSultS on IMDB-B dataSet Shown
in Figure 3a. Both methodS contraSt negatiVe pairS batch-wiSe and increaSing batch Size improVeS
the performance of IGSD while degradeS CMC-Graph. When batch Size iS greater than 32, IGSD
outperformS CMC-Graph and the performance gap becomeS larger aS the batch Size increaSeS,
which meanS IGSD iS better at leVeraging negatiVe pairS for learning effectiVe repreSentationS than
CMC-Graph.
Effects of different proportion of labeled data. We alSo inVeStigate the performance of different
modelS with different proportion of labeled data with IMDB-B dataSet. AS illuStrated in Figure 3b,
IGSD outperformS Strong InfoGraph* baSeline giVen different amount of labeled data conSiStently.
And the performance gain iS moSt Significant when the fraction of labeled data iS 10% Since our
modelS can leVerage labelS more effectiVely by regularizing original unSuperViSed learning objectiVe
when labelS are Scarce.
6	Conclusions
In thiS paper, we propoSe IGSD, a noVel unSuperViSed graph-leVel repreSentation learning framework
Via Self-diStillation. Our framework iteratiVely performS teach-Student diStillation by contraSting
augmented ViewS of graph inStanceS. Experimental reSultS in both unSuperViSed and Semi-SuperViSed
SettingS Show that IGSD iS not only able to learn effectiVe graph repreSentationS competitiVe with
State-of-the-art modelS but alSo robuSt with choiceS of encoderS and augmentation StrategieS. In the
future, we plan to apply our framework to other graph learning taSkS and inVeStigate the deSign of
View generatorS to generatiVe effectiVe ViewS automatically.
8
Under review as a conference paper at ICLR 2021
References
Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. There are many consis-
tent explanations of unlabeled data: Why you should average. arXiv preprint arXiv:1806.05594,
2018.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering. In Advances in neural information processing systems, pp. 585-591, 2002.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems, pp. 5049-5059, 2019.
Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Fifth IEEE
international conference on data mining (ICDM’05), pp. 8-pp. IEEE, 2005.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):1-27, 2011.
Dexiong Chen, Laurent Jacob, and Julien Mairal. Convolutional kernel networks for graph-structured
data. arXiv preprint arXiv:2003.05189, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations, 2020b.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029,
2020c.
Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. Debi-
ased contrastive learning, 2020.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael BombarelL Timothy HirzeL Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.
Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks, 2018.
Thomas Gartner, Peter Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient
alternatives. In Learning theory and kernel machines, pp. 129-143. Springer, 2003.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent:
A new approach to self-supervised learning, 2020.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in neural information processing systems, pp. 1024-1034, 2017.
Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In Proceedings of International Conference on Machine Learning, pp. 3451-3461. 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), Jun 2020. doi: 10.1109/cvpr42600.2020.00975. URL http:
//dx.doi.org/10.1109/cvpr42600.2020.00975.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
9
Under review as a conference paper at ICLR 2021
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks, 2019.
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs.
In Proceedings ofthe 20th international conference on machine learning (ICML-03), pp. 321-328,
2003.
Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund.
de.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning, 2020.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Johannes Klicpera, Stefan WeiBenberger, and StePhan Gunnemann. Diffusion improves graph
learning, 2019.
Risi Kondor and Horace Pan. The multiscale laplacian graph kernel. In Advances in Neural
Information Processing Systems, pp. 2990-2998, 2016.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep
neural networks. In Workshop on challenges in representation learning, ICML, volume 3, 2013.
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. Self-
supervised learning: Generative or contrastive, 2020.
Takeru Miyato, Shin-Ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A
regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 41(8):1979-1993, Aug 2019. ISSN 1939-3539. doi: 10.1109/
tpami.2018.2858821. URL http://dx.doi.org/10.1109/TPAMI.2018.2858821.
Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation amplifies regularization
in hilbert space. arXiv preprint arXiv:2002.05715, 2020.
Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu,
and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint
arXiv:1707.05005, 2017.
Mark EJ Newman and Michelle Girvan. Finding and evaluating community structure in networks.
Physical review E, 69(2):026113, 2004.
Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic
evaluation of deep semi-supervised learning algorithms, 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang,
and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
1150-1160, 2020.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training, 2018.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1-7, 2014.
10
Under review as a conference paper at ICLR 2021
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object
detection models. 2005.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 815-823, 2015.
Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.
Efficient graphlet kernels for large graph comparison. In Artificial Intelligence and Statistics, pp.
488-495, 2009.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.
Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
supervised graph-level representation learning via mutual information maximization. arXiv preprint
arXiv:1908.01000, 2019.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results, 2017.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning? arXiv preprint arXiv:2005.10243, 2020.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Petar VeliCkovic, William Fedus, William L Hamilton, Pietro Lio, YoshUa Bengio, and R Devon
Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz,
and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In
International Conference on Machine Learning, pp. 6438-6447. PMLR, 2019.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets,
2015.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365-1374,
2015.
Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help
graph convolutional networks? arXiv preprint arXiv:2006.09136, 2020.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization, 2017.
Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data
augmentation for graph neural networks, 2020.
Yadi Zhou, Fei Wang, Jian Tang, Ruth Nussinov, and Feixiong Cheng. Artificial intelligence in
covid-19 drug repurposing. The Lancet Digital Health, 2020.
11
Under review as a conference paper at ICLR 2021
A	Appendix
A. 1 Related Work
Graph Representation Learning Traditionally, graph kernels are widely used for learning node
and graph representations. This common process includes meticulous designs like decomposing
graphs into substructures and using kernel functions like Weisfeiler-Leman graph kernel (Shervashidze
et al., 2011) to measure graph similarity between them. However, they usually require non-trivial
hand-crafted substructures and domain-specific kernel functions to measure the similarity while yields
inferior performance on downstream tasks like node classification and graph classification. Moreover,
they often suffer from poor scalability (Borgwardt & Kriegel, 2005) and great memory consumption
(Kondor & Pan, 2016) due to some procedures like path extraction and recursive subgraph construction.
Recently, there has been increasing interest in Graph Neural Network (GNN) approaches for graph
representation learning and many GNN variants have been proposed (Ramakrishnan et al., 2014; Kipf
& Welling, 2016; Xu et al., 2018). However, they mainly focus on supervised settings.
Data augmentation Data augmentation strategies on graphs are limited since defining views of
graphs is a non-trivial task. There are two common choices of augmentations on graphs (1) feature-
space augmentation and (2) structure-space augmentation. A straightforward way is to corrupt
the adjacency matrix which preserves the features but adds or removes edges from the adjacency
matrix with some probability distribution (VelickoVic et al., 2018). Zhao et al. (2020) improves
performance in GNN-based semi-supervised node classification via edge prediction. Empirical results
show that diffusion matrix can serve as a denoising filter to augment graph data for improving graph
representation learning significantly both in supervised (Klicpera et al., 2019) and unsupervised
settings (Hassani & Khasahmadi, 2020). Hassani & Khasahmadi (2020) shows the benefits of treating
diffusion matrix as an augmented view of mutual information-based contrastive graph representation
learning. Attaining effective views is non-trivial since we need to consider factors like mutual
information to preserve label information w.r.t the downstream task (Tian et al., 2020).
A.2 Hyper-Parameters
For hyper-parameter tuning, we select number of GCN layers over {2, 8, 12}, batch size over {16,
32, 64, 128, 256, 512}, number of epochs over {20, 40, 100} and learning rate over {1e-4, 1e-3} in
unsupervised graph classification.
The hyper-parameters we tune for semi-supervised graph classification and molecular property
prediction are the same in (Xu et al., 2018) and (Sun et al., 2019), respectively.
In all experiments, we fix the fixed α = 0.2 for PPR graph diffusion and set the weighting coefficient
of Mixup function to be 0.5 and tune our projection hidden size over {1024, 2048} and projection
size over {256, 512}. We start self-training after 30 epochs and tune the number of iterations over
{20, 50}, pseudo-labeling threshold over {0.9, 0.95}.
A.3 Effect of Projectors
While we could directly predict the representation y and not a projection z, previous contrastive
learning works in the image domain like (Chen et al., 2020b) have empirically shown that using this
projection improves performance. We also further investigate the performance with and without the
projector on 4 datasets:
Datasets	MUTAG	IMDB-B	IMDB-M	PTC
IGSD	90.2 ± 0.7	74.7 ± 0.6	51.5 ± 0.3	61.4 ± 1.7
IGSD w/o projector	87.7 ± 0.9	74.2 ± 0.6	51.1 ± 0.6	56.7 ± 0.9
Table 3: Effect of projectors on Graph classification accuracies (%).
Results above show that dropping the projector degrades the performance, which indicates the
necessity of a projector.
12
Under review as a conference paper at ICLR 2021
Meanwhile, to investigate the effect of projectors on model performance, we fix the output size of
layers in encoders so that their output size is always 512. Then we conducted ablation experiments
on different size of the projection head on IMDB-B with the following results:
Size	218	512	1024	2048
IGSD 74.5 ± 0.6	74.8 ± 0.4	74.7 ± 0.6	74.9 ± 0.8
Table 4: Effect of projector hidden size on Graph classification accuracies (%).
In general, the performance is insensitive to the projection size while a larger projection size could
slightly improve the unsupervised learning performance.
13