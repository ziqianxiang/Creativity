Under review as a conference paper at ICLR 2021
Predicting What You Already Know Helps:
Provab le Self-Supervised Learning
Anonymous authors
Paper under double-blind review
Ab stract
Self-supervised representation learning solves auxiliary prediction tasks (known as
pretext tasks), that do not require labeled data, to learn semantic representations.
These pretext tasks are created solely using the input features, such as predicting
a missing image patch, recovering the color channels of an image from context,
or predicting missing words in text, yet predicting this known information helps
in learning representations effective for downstream prediction tasks. This paper
posits a mechanism based on approximate conditional independence to formalize
how solving certain pretext tasks can learn representations that provably decrease
the sample complexity of downstream supervised tasks. Formally, we quantify
how the approximate independence between the components of the pretext task
(conditional on the label and latent variables) allows us to learn representations
that can solve the downstream task with drastically reduced sample complexity by
just training a linear layer on top of the learned representation.
1	Introduction
Self-supervised learning revitalizes machine learning models in computer vision, language modeling,
and control problems (see reference therein (Jing & Tian, 2020; Kolesnikov et al., 2019; Devlin
et al., 2018; Wang & Gupta, 2015; Jang et al., 2018)). Training a model with auxiliary tasks based
only on input features reduces the extensive costs of data collection and semantic annotations for
downstream tasks. It is also known to improve the adversarial robustness of models (Hendrycks et al.,
2019; Carmon et al., 2019; Chen et al., 2020a). Self-supervised learning creates pseudo labels solely
based on input features, and solves auxiliary prediction tasks in a supervised manner (pretext tasks).
However, the underlying principles of self-supervised learning are mysterious since it is a-priori
unclear why predicting what we already know should help. We thus raise the following question:
What conceptual connection between pretext and downstream tasks ensures good representations?
What is a good way to quantify this?
As a thought experiment, consider a simple downstream task of classifying desert, forest, and sea
images. A meaningful pretext task is to predict the background color of images (known as image
colorization (Zhang et al., 2016)). Denote X1, X2, Y to be the input image, color channel, and
the downstream label respectively. Given knowledge of the label Y , one can possibly predict the
background X2 without knowing much about X1 . In other words, X2 is approximately independent
of X1 conditional on the label Y . Consider another task of inpainting (Pathak et al., 2016) the front
of a building (X2) from the rest (X1). While knowing the label “building” (Y) is not sufficient
for successful inpainting, adding additional latent variables Z such as architectural style, location,
window positions, etc. will ensure that variation in X2 given Y, Z is small. We can mathematically
interpret this as X1 being approximate conditionally independent of X2 given Y, Z.
In the above settings with conditional independence, the only way to solve the pretext task for X1 is
to first implicitly predict Y and then predict X2 from Y . Even without labeled data, the information
of Y is hidden in the prediction for X2 .
Contributions. We propose a mechanism based on approximate conditional independence (ACI)
to explain why solving pretext tasks created from known information can learn representations
that provably reduce downstream sample complexity. For instance, learned representation will
1
Under review as a conference paper at ICLR 2021
only require O(k) samples to solve a k-way supervised task under conditional independence (CI).
Under ACI (quantified by the norm of a certain partial covariance matrix), we show similar sample
complexity improvements. We verify our main Theorem (4.2) using simulations. We check that
pretext task helps when CI is approximately satisfied in text domain, and demonstrate on a real-world
image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.
1.1	Related work
Self-supervised learning (SSL) methods in practice: There has been a flurry of self-supervised
methods lately. One class of methods reconstruct images from corrupted or incomplete versions of
it, like denoising auto-encoders (Vincent et al., 2008), image inpainting (Pathak et al., 2016), and
split-brain autoencoder (Zhang et al., 2017). Pretext tasks are also created using visual common
sense, including predicting rotation angle (Gidaris et al., 2018), relative patch position (Doersch
et al., 2015), recovering color channels (Zhang et al., 2016), solving jigsaw puzzle games (Noroozi
& Favaro, 2016), and discriminating images created from distortion (Dosovitskiy et al., 2015). We
refer to the above procedures as reconstruction-based SSL. Another popular paradigm is contrastive
learning (Chen et al., 2020b;c). The idea is to learn representations that bring similar data points
closer while pushing randomly selected points further away (Wang & Gupta, 2015; Logeswaran &
Lee, 2018; Arora et al., 2019) or to maximize a contrastive-based mutual information lower bound
between different views (Hjelm et al., 2018; Oord et al., 2018; Tian et al., 2019). A popular approach
for text domain is based on language modeling where models like BERT and GPT create auxiliary
tasks for next word predictions (Devlin et al., 2018; Radford et al., 2018). The natural ordering or
topology of data is also exploited in video-based (Wei et al., 2018; Misra et al., 2016; Fernando
et al., 2017), graph-based (Yang et al., 2020; Hu et al., 2019) or map-based (Zhang et al., 2019)
self-supervised learning. For instance, the pretext task is to determine the correct temporal order for
video frames as in (Misra et al., 2016).
Theory for self-supervised learning: Our work initiates some theoretical understanding on the
reconstruction-based SSL. Related to our work is the recent theoretical analysis of contrastive
learning. Arora et al. (2019) shows guarantees for representations from contrastive learning on
linear classification tasks using a class conditional independence assumption, but do not handle
approximate conditional independence. Recently, Tosh et al. (2020a) show that contrastive learning
representations can linearly recover any continuous functions of the underlying topic posterior under
a topic modeling assumption for text. While their assumption bears some similarity to ours, the
assumption of independent sampling of words that they exploit is strong and not generalizable to other
domains like images. More recently, concurrent work by Tosh et al. (2020b) shows guarantees for
contrastive learning, but not reconstruction-based SSL, with a multi-view redundancy assumptions that
is very similar to our CI assumption. (Wang & Isola, 2020) theoretically studies contrastive learning
on the hypersphere through intuitive properties like alignment and uniformity of representations;
however there is no theoretical connection made to downstream tasks. There is a mutual information
maximization view of contrastive learning, but (Tschannen et al., 2019) points out issues with it.
Previous attempts to explain negative sampling (Mikolov et al., 2013) based methods use the theory
of noise contrastive estimation (Gutmann & Hyvarinen, 2010; Ma & Collins, 2018). However,
guarantees are only asymptotic and not for downstream tasks. CI is also used in sufficient dimension
reduction Fukumizu et al. (2009; 2004). CI and redundancy assumptions on multiple views (Kakade
& Foster, 2007; Ando & Zhang, 2007) are used to analyze a canonical-correlation based dimension
reduction algorithm. Finally, (Alain & Bengio, 2014; Vincent, 2011) provide a theoretical analysis
for denoising auto-encoder.
1.2	Overview of results:
Section 2 introduces notation, setup, and the self-supervised learning procedure considered in this
work. In Section 3, we analyze downstream sample complexity under CI. Section 4 presents our main
result with relaxed conditions: under ACI with latent variables, and assuming finite samples in both
pretext and downstream tasks, for various function classes, and both regression and classification
tasks. Experiments verifying our theoretical findings are in Section 5.
2
Under review as a conference paper at ICLR 2021
2	Preliminary
2.1	Notation
We use lower case symbols (x) to denote scalar quantities, bold lower case symbols (x) for vector
values, capital letters (X) for random variables, and capital and bold letters X for matrices. PX
denotes the probability law of random variable X , and the space of square-integrable functions with
probability P is denoted by L2 (P). We use standard O notation to hide universal factors and O to
hide log factors. k ∙ k stands for '2-norm for vectors or Frobenius norm for matrices.
Linear conditional expectation. EL[Y |X] denotes the prediction of Y with linear regression:
EL[YX = x] := W*x + b*, where W*,b* := argminE[∣∣Y - WX - b∣∣2].
W,b
In other words, EL[Y|X] denotes the best linear predictor ofY given X. We also note that E[Y|X] ≡
minf E[kY - f(X)k2] is the best predictor ofY given X.
(Partial) covariance matrix. For random variables X, Y, we denote ΣXY to be covariance matrix
of X and Y. For simplicity in most cases, we assume E[X] = 0 and E[Y] = 0; thus we do not
distinguish E[XY] and ΣXY . The partial covariance matrix between X and Y given Z is:
∑χγ∣z =cov{X - EL[X|Z], Y - EL[Y∣Z]} ≡ ΣXY - ΣXZ ΣZ-Z1 ΣZY	(1)
Sub-gaussian random vectors. A random vector X ∈ Rd is ρ2-sub-gaussian if for every fixed unit
vector V ∈ Rd, the variable v>X is ρ2-sub-gaussian, i.e., E[es∙v>(x-E[x])] ≤ es2ρ2/2 (∀s ∈ R).
2.2	Setup and methodology
We denote by X1 the input variable, X2 the target random variable for the pretext task, and Y the
label for the downstream task, with X1 ∈ X1 ⊂ Rd1,X2 ∈ X2 ⊂ Rd2 andY ∈ Y ⊂ Rk. IfYis
finite with |Y| = k, we assume Y ⊂ Rk is the one-hot encoding of the labels. PX1X2Y denotes the
joint distribution over X1 × X2 × Y. PX1Y, PX1 denote the corresponding marginal distributions.
Our proposed self-supervised learning procedure is as follows:
Step 1 (pretext task): Learn representation ψ(x1) through ψ := arg ming∈H E kX2 - g(X1)k2F,
where H can be different for different settings that we will specify and discuss later.
Step 2 (downstream task): Perform linear regression on Y with ψ(X1),i.e. f (xι) := (W* )>ψ(xι),
where W * — argminw Eχι,γ [∣∣Y - W >ψ(X1)k2]. Namely we learn f(∙) = EL [Y ∣ψ(∙)].
Performance of the learned representation on the downstream task depends on the following quantities.
Approximation error. We measure this for a learned representation ψ by learning a linear function
on top of it for the downstream task. Denote eapx(ψ) = minW E[kf*(X1) - Wψ(X1)k2], where
f*(x1) = E[Y |X1 = x1] is the optimal predictor for the task. This gives a measure of how well ψ
can do with when given infinite samples for the task.
Estimation error. We measure sample complexity of ψ on the downstream task and assume
access to n i.i.d. samples (xf), y(1)), ∙∙∙ ,(x，2), y(n2)) drawn from PχιY. We express
the n2 samples collectively as X1down ∈ Rn2×d1 , Y ∈ Rn2×k and overload notation to say
ψ(χdown) = [ψ(χ11))∣ψ(χ12)) ∙ ∙ ∙ ∣ψ(χ1n2))]> ∈ Rn2×d2. We perform linear regression on the
learned representation ψ and are interested in the excess risk that measures generalization.
W 一 argminɪ∣∣Y - ψ(Xι)WkF; ERψ(W) := 1 Ekf*(Xι) - W>Ψ(Xι)k2
W	2n2	2
3	Guaranteed rec overy with conditional independence
In this section, we focus on the case when input X1 and pretext target X2 are conditionally indepen-
dent (CI) given the downstream label Y. While this is a strong assumption that is rarely satisfied in
practice, it helps us understand the role of CI with clean results and builds up to our main results with
3
Under review as a conference paper at ICLR 2021
ACI with latent variables in Section 4. As a warm-up, we show how CI helps when (X1, X2, Y ) are
jointly Gaussian to give us a flavor for the results to follow. We then analyze it for general random
variables under two settings: (a) when the function class used for ψ is universal, (b) when ψ is
restricted to be a linear function of given features. For now we assume access to a large amount of
unlabeled data so as to learn the optimal ψ* perfectly and this will be relaxed later in Section 4. The
general recipe for the results is as follows:
1.	Find a closed-form expression for the optimal solution ψ* for the pretext task.
2.	Use conditional independence to argue that eapx(ψ*) is small.
3.	Exploit the low rank structure of ψ* to show small estimation error on downstream tasks.
Data assumption. Suppose Y = f*(Xι) + N, where f = E[Y∣Xι] and hence E[N] = 0. We
assume N is σ2-subgaussian. For simplicity, we assume non-degeneracy: ΣXiXi, ΣYY are full rank.
3.1	Warm-up: jointly Gaussian variables
We assume X1 , X2 , Y are jointly Gaussian, and so the optimal regression functions are all linear, i.e.,
E[Y|X1] = EL[Y|X1]. We also assume data is centered: E[Xi] = 0 and E[Y] = 0. Non-centered
data can easily be handled by learning an intercept. All relationships between random variables can
then be captured by the (partial) covariance matrix. Therefore it is easy to quantify the CI property
and establish the necessary and sufficient conditions that make X2 a reasonable pretext task.
Assumption 3.1. (Jointly Gaussian) X1, X2, Y are jointly Gaussian.
Assumption 3.2. (Conditional independence) X1⊥X21Y.
Claim 3.1 (Closed-form solution). Under Assumption 3.1, the representation function and optimal
prediction that minimize the population risk can be expressed as follows:
ψ*(xι) := El[X2∣Xi = xι]= ΣX2 X1 Σ-X11 X1 x1	(2)
Our target f *(xι) := EL[Y∣Xι = xι] = ∑γχι ∑χ[χι xι.	(3)
Our prediction for downstream task with representation ψ* will be: g(∙) := EL[Y∣ψ*(Xι)]. Recall
from Equation 1 that the partial covariance matrix between Xi and X2 given Y is ∑X1X2∣γ ≡
ΣX1X2 - ΣX1Y ΣY-1Y ΣY X2 . This partial covariance matrix captures the correlation between X1 and
X2 given Y. For jointly Gaussian random variables, CI is equivalent to ΣχιX2∣γ = 0. We first
analyze the approximation error based on the property of this partial covariance matrix.
Lemma 3.2 (Approximation error). Under Assumption 3.1, 3.2, if Σχ2γ has rank k, e apx (ψ*) = 0.
Remark 3.1. ΣX2Y being full column rank implies that E[X2 |Y] has rank k, i.e., X2 depends on all
directions of Y and thus captures all directions of information of Y. This is a necessary assumption
for X2 to be a reasonable pretext task for predicting Y. e apx(ψ*) = 0 means f * is linear in ψ*.
Therefore ψ* SeleCtS d2 out of di features that are SuffiCient to predict Y.
Next we consider the estimation error that characterizes the number of samples needed to learn a
prediction function f (xi) = Wψ*(xι) that generalizes.
Theorem 3.3 (Estimation error). Fix a failure probability δ ∈ (0, 1). Under Assumption 3.1,3.2, if
n2》k + log(1∕δ), excess risk of the learned predictor xi → W ψ*(xι) on the target task satisfies
ERψ.(W) ≤O (Tr(CYYXjf + log(S))),
with probability at least 1 - δ.
Here ∑γγ∣Xι ≡ ∑γγ 一 ∑γχι ∑χiχ1 ∑Xι γ captures the noise level and is the covariance matrix
of tespeche residual term Y - f*(Xi) = Y - ΣYX1 Σ-XiX Xi. Compared to directly using Xi
to predict Y, self-supervised learning reduces the sample complexity from O(di) to O(k). We
generalize these results even when only a weaker form of CI holds.
Assumption 3.3 (Conditional independence given latent variables). There exists some latent variable
Z ∈ Rm such that X1⊥X21 Y, and ∑χ2γ is ofrank k + m, where Y = [Y, Z].
4
Under review as a conference paper at ICLR 2021
This assumption lets introduce some reasonable latent variables that capture the information between
Xi and X2 apart from Y. ∑X2Y being full rank Says that all directions of Y are needed to predict
X2 , and therefore Z is not redundant. For instance, when Z = X1 , the assumption is trivially true
but Z is not the minimal latent information we want to add. Note it implicitly requires d2 ≥ k + m.
Corollary 3.4. Under Assumption 3.1, 3.3, the approximation error eαpχ(ψ*) is 0.
Under CI with latent variable, we can generalize Theorem 3.3 by replacing k by k + m.
3.2	General random variables
Next we move on to general setting where the variables need not be Gaussian.
Assumption 3.4. Let X1 ∈ Rd1 , X2 ∈ Rd2 be random variables from some unknown distribution.
Let label Y ∈ Y be a discrete random variable with k = |Y | < d2. We assume conditional
independence: X1⊥X2∣Y.
Here Y can be interpreted as the multi-class labels where k is the number of classes. For regression
problems, one can think about Y as the discretized values of continuous labels. We do not specify
the dimension for Y since Y could be arbitrarily encoded but the results only depend on k and the
variance of Y (conditional on the input X1 ).
Universal function class. Suppose we learn the optimal ψ* among all measurable functions The
optimal function ψ* in this case is naturally given by conditional expectation: ψ*(xι) = E[X2 |Xi =
xi]. We now show that CI implies that ψ* is good for downstream tasks, which is not apriori clear.
Lemma 3.5 (Approximation error). Suppose random variables X1, X2, Y satisfy Assumption 3.4,
and matrix A ∈ RY×d2 with Ay,： := E[X2∣Y = y] is Ofrank k = |Y|. Then eapx(ψ*) = 0.
This tells us that although f * could be nonlinear in xi, it is guaranteed to be linear in ψ*(xι). Note
that Y does not have to be linear in X2. We provide this simple example for better understanding:
Example 3.1. Let Y ∈ {-1, 1} be binary labels, and Xi, X2 be 2-mixture Gaussian random
variables with Xi ~N (Y μ1,I),X2 ~N (Y μ2,I). In this example, X1⊥X2 |Y. Although f * =
E[Y∣X2] is not linear, E[Y∣ψ] is linear: ψ(xι) = P(Y = 1|Xi = x1)μ2 一 P(Y = -1|Xi =
x1)μ2 and f *(xi) = P(Y = 1|Xi = xi) - P(Y = -IXi = xi) ≡ μTΨ(x1)∕kμ2k2.
Given that ψ* is good for downstream, we now care about the sample complexity. We will need to
assume that the representation has some nice concentration properties. We make an assumption about
the whitened data ψ*(Xi) to ignore scaling factors.
Assumption 3.5. We assume the WhitenedfeatUre variable U := Σ-i∕2ψ(Xi) is a ρ2-subgaussian
random variable, where Σψ = E[ψ(Xi)ψ(Xi)>].
We note that all bounded random variables satisfy sub-gaussian property.
Theorem 3.6 (General conditional independence). Fix a failure probability δ ∈ (0, 1), under the
same assumption as Lemma 3.5 andAssumption 3.5for ψ*, ifadditionally n>ρ4(k + log(1∕δ)),
then the excess risk of the learned predictor xi → W>ψ*(xi) on the downstream task satsifies:
ER IWl < O ( k + log(k/6 m∖
ERψ* [W ] ≤ O ----------------σ .
n2
Function class induced by feature maps. Given feature map φi : Xi → RD1 , we consider the
function class Hi = {ψ : Xi → Rd2 ∣∃B ∈ Rd2×D1 ,ψ(xi) = Bφi(xi)}.
Claim 3.7 (Closed form solution). The optimal function in H is ψ*(xi) = ΣX2φ1 Σφ-iφ xi, where
ΣX2φ1 := ΣX2φ1 (X1) and Σφ1 φ1 := Σφ1 (X1 )φ1 (X1).
We again show the benefit of CI, this time only comparing the performance of ψ * to the original
features φi. Since ψ* is linear in φi, it cannot have smaller approximation error than φi. However CI
will ensure that ψ* has the same approximation error as φi and enjoys much better sample complexity.
5
Under review as a conference paper at ICLR 2021
Lemma 3.8 (Approximation error). If Assumption 3.4 is satisfied, and if the matrix A ∈ RY×d2 with
Ay,： := E[X2∣Y = y] isofrank k = |Y|. Then e apx (ψ*) = e apx(Φ1).
We additionally need an assumption on the residual a(xι) := E[Y∣Xι = xι] 一 EL[Y∣φι(xι)].
Assumption 3.6. (Bounded approx. error; Condition 3 in Hsu et al. (2012))) We have almost surely
k∑-ι1φ2Φι(Xι)a(Xι)>kF ≤ bo√k
Theorem 3.9. (CI with approximation error) Fix a failure probability δ ∈ (0, 1), under the same
assumption as Lemma 3.8, Assumption 3.5 for ψ* and Assumption 3.6, if n>ρ4(k + log(1∕δ)),
then the excess risk of the learned predictor xι → W >ψ*(xι) on the downstream task satisfies:
ERψ* [W] ≤ eapx(φι) + O (k + /k") σ2}
Theorem 3.9 is also true with Assumption 3.3 instead of exact CI, if we replace k by km. Therefore
with SSL, the requirement of labels is reduced from complexity for H to O(k) ( or O(km)).
Remark 3.2. We note that since X1⊥X2∣Y ensures X1⊥h(X2)∣Y for any deterministicfunction h,
we could replace X2 by h(X2) and all results hold. Therefore we could replace X2 with h(X2) in
our algorithm especially when d2 < km.
4 Beyond conditional independence
In the previous section, we focused on the case where exact CI is satisfied. A weaker but more
practical assumption is that Y captures some portion of the dependence between X1 and X2 but
not all. A warm-up result with jointly-Gaussian variables is defered in Appendix C.1, where ACI is
quantified by the partial covariance matrix. In the section below, we generalize the result from linear
function space to arbitrary function space, and introduce the appropriate quantities to measure ACI.
4.1	Learnability with general function space
We state the main result with finite samples for both pretext task and downstream task to achieve good
generalization. Let Xpre = [x11,pre),…，χ1n1,pre)]> ∈ Rn1×d1 and X? = [χ21), ∙ ∙ ∙ , x2n1)]> ∈
Rn1 ×d2 be the training data from pretext task, where (x(1i,pre), x(2i)) is sampled from PX1X2 . We
consider two types of function spaces: H ∈ {Hi, Hu}. Recall Hi = {ψ : Xi → Rd2 ∣∃B ∈
Rd2×D1 , ψ(x1) = Bφ1(x1)} is induced by feature map φ1 : X1 → RD1 . Hu is a function space
with universal approximation power (e.g. deep networks) that ensures ψ* = E[X? |Xi] ∈ Hu. We
learn a representation from H by using ni samples: ψ =argmi□f∈^2 n^∣∣X2 一 f (XPre)kF. For
downstream tasks we similarly define Xidown ∈ Rn2×d1 , Y ∈ Rn2×d31, and learn a linear classifier
trained on ψ(Xdown):
W 一 argminɪ∣Y 一 ψ(Xdown)W∣F, ERψ(W) := EXIkfH(Xi) 一 W>ψ(Xι)∣2.
W 2n2
Here fH = EL[Y∣Φι(Xι)] when H = Hi and fH = f * for H = H“.
Assumption 4.1 (Correlation between X2 and Y, Z). Suppose there exists latent variable Z ∈
Z, |Z| = m that ensures ∑φyX2 isfull column rank and k∑γφy ∑X2φ-12 = 1∕β, where At is
pseudo-inverse, and φy is the one-hot embeddingfor Y = [Y, Z].
Definition 4.1 (Approximate conditional independence with function space H).
1.	For H = Hi, define eCI= |0-比£θ但包 ||尸.
2.	For H = Hu ,define eC := E^ [∣ E[X? |Xi] 一 EY [E[X? ∣Y]Xi]k2].
Exact CI for both cases ensures eCI = 0. We present a unified analysis in the appendix that shows the
eCI for the second case is same as the first case, with covariance operators instead of matrices.
1 d3 = k and Y ≡ φy (Y ) (one-hot encoding) refers multi-class classification task, d3 = 1 refers to regression.
6
Under review as a conference paper at ICLR 2021
Figure 1: Left two: how MSE scales with k (the dimension of Y ) and CI (ACI 4.1) with the linear
function class. Right two: how MSE scales with k and e with ψ* and non-linear function class.
Mean of 30 trials are shown in solid line and one standard error is shown by shadow.
When H = Hu, the residual term N := Y - E[Y |X1] is mean zero and assumed to be σ2-
subgaussian. When we use non-universal features φ1, E[Y - fH* (X1)|X1] may not be mean zero.
We thus introduce the standard assumption on a := f * — f**ι = E[Y|X1 ] — EL[Y∣φ1(X1)]:
Assumption 4.2. (Bounded approximation error (Hsu et al., 2012)) There exists a universal constant
60, such that ∣∣Σ-ι1φ/2 φ1(X1 )a(X1 )>kF ≤ 60 √k almost surely.
Theorem 4.2. For a fixed δ ∈ (0, 1), under Assumptions 4.1, 4.2 for ψ and ψ* and 3.5 for non-
universal feature maps, if n1,n2》ρ4 (d2 + log1∕δ), and we learn the pretext tasks such that:
E ∣ψ(X1) — ψ* (X1 )∣F ≤ eP re. Then the generalization error for downstream task with probability
1 — δ is:
ERψ(W) ≤ O (σ2d2 + log(d2∕0 + 餐 + 餐!	(4)
n2	β2	β2
We defer the proof to the appendix. The proof technique is similar to that of Section 3. The difference
is now our ψ(X(down)) ∈ Rn2 ×d2 Will be an approximately low rank matrix (low rank + small norm),
where the low rank part is the high-signal features that implicitly comes from Y, Z that will be
useful for downstream. The remaining part comes from eCI and epre. Again by selecting the top km
(dimension of φy) features we could further improve the sample complexity:
Remark 4.1. By applying PCA on ψj(Xdowrι) and keeping the top km principal components only,
we can improve the bound in Theorem 4.2 to
ERψ (W) ≤O - km + l「) + J + βe! .	(5)
We take a closer look at the different sources of errors in (5): 1) the noise term Y — f* (X1) with
noise level σ2; 2) eCI that measures the approximate CI; and 3) epre the error from not learning the
pretext task exactly. The first term is optimal setting ignoring log factors as we do linear regression on
mk-dimensional features. The second and third term are non-reducible due to the fact that f * is not
exactly linear in ψ while we use it as a fixed feature and learn a linear function on it. Therefore it is
important to fine-tune when we have sufficient downtream labeled data. We leave this as future work.
Compared to traditional supervised learning, learning fH* requires sample complexity scaling with
the (Rademacher/Gaussian) complexity of H (see e.g. Bartlett & Mendelson (2002); Shalev-Shwartz
& Ben-David (2014)), which is very large for complicated models such as deep networks.
In Section D, we consider a similar result for cross-entropy loss.
5	Experiments
In this section, we empirically verify our claim that SSL performs well when ACI is satisfied.
7
Under review as a conference paper at ICLR 2021
Figure 2: Performance on SST of baseline φ1(x1), i.e. bag-of-words, and learned ψ(x1) for the two
settings. Left: Classification accuracy, Right: Regression MSE.
Simulations. With synthetic data, we verify how excess risk (ER) scales with the cardinality/feature
dimension of Y (k), and ACI (CI in Definition 4.1). We consider a mixture of Gaussian data and
conduct experiments with both linear function space (H1 with φ1 as identity map) and universal
function space Hu. We sample the label Y uniformly from {1, ..., k}. For i-th class, the centers
μ1i ∈ Rd1 and μ2i ∈ Rd2 are uniformly sampled from [0,10). Given Y = i, α ∈ [0,1], let
X1 〜 N(μii,I), X 〜 N(μ2i, I), and X2 = (1 - α)X2 + αX1. Therefore α is a correlation
coefficient: α = 0 ensures X2 being CI with X1 given Y and when α = 1, X2 fully depends on X1.
(if d1 6= d2, we append zeros or truncate to fit accordingly).
We first conduct experiments with linear function class. We learn a linear representation ψ with
n1 samples and the linear prediction of Y from ψ with n2 samples. We set d1 = 50, d2 = 40,
n1 = 4000, n2 = 1000 and ER is measured with Mean Squared Error (MSE). As shown in Figure
1(a)(b), the MSE of learning with ψ(X1) scales linearly with k as indicated in Theorem 3.9, and
scales linearly with CI associated with linear function class as indicated in Theorem 4.2. Next we
move on to general function class, i.e., ψ* = E[Y∣X1 ] with a closed form solution (see example 3.1).
We use the same parameter settings as above. For baseline method, we use kernel linear regression to
predict Y using X1 (we use RBF kernel which also has universal approximation power). As shown
in Figure 1(c)(d), the phenomenon is the same as what we observe in the linear function class setting,
and hence they respectively verify Theorem 3.6 and Theorem 4.2 with Hu .
NLP task. We look at the setting where both X1 and X2 are the set of sentences and perform
experiments by enforcing CI with and without latent variables. The downstream task is sentiment
analysis with the Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013), where inputs are
movie reviews and the label set Y is {±1}. We use the representation class H1, with features φ1 being
the bag-of-words representation (D1 = 13848). For X2 we use a d2 = 300 dimensional embedding
of the sentence, that is the mean of word vectors (random gaussians) for the words in the sentence.
For SSL data we consider 2 settings, (a) enforce CI with the labels Y, (b) enforce CI with extra latent
variables, for which we use fine-grained version of SST with label set Y = {1, 2, 3,4, 5}2. We test
the learned ψ on SST binary task with linear regression and linear classification; results are presented
in Figure 2. We observe that in both settings ψ outperforms φ1, especially in the small-sample-size
regime. Also exact CI is better than CI with extra latent variables, as suggested by theory.
6	Conclusion
In this work we theoretically quantify how an approximate conditional independence assumption
that connects pretext and downstream task data distributions can give sample complexity benefits
of self-supervised learning on downstream tasks. Our theoretical findings are also supported by
experiments on simulated data and also on real CV and NLP tasks. We would like to note that
approximate CI is only a sufficient condition for a useful pretext task. We leave it for future work to
investigate other mechanisms by which pretext tasks help with downstream tasks.
2Ratings {1, 2} correspond to y = -1 and {4, 5} correspond to y = 1
8
Under review as a conference paper at ICLR 2021
References
Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating
distribution. The Journal ofMachine Learning Research,15(1):3563-3593, 2014.
Rie Kubota Ando and Tong Zhang. Two-view feature generation model for semi-supervised learning.
In Proceedings of the 24th international conference on Machine learning, pp. 25-32, 2007.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A
theoretical analysis of contrastive unsupervised representation learning. In Proceedings of the 36th
International Conference on Machine Learning, 2019.
Charles R Baker. Joint measures and cross-covariance operators. Transactions of the American
Mathematical Society, 186:273-289, 1973.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945, 1993.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp.
11190-11201, 2019.
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to fine-tuning. arXiv preprint arXiv:2003.12862,
2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020b.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029,
2020c.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.
1422-1430, 2015.
Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox.
Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE
transactions on pattern analysis and machine intelligence, 38(9):1734-1747, 2015.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video
representation learning with odd-one-out networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 3636-3645, 2017.
Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised
learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):
73-99, 2004.
Kenji Fukumizu, Francis R Bach, Michael I Jordan, et al. Kernel dimension reduction in regression.
The Annals of Statistics, 37(4):1871-1905, 2009.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.
9
Under review as a conference paper at ICLR 2021
Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, and Alexei A Efros. A century of portraits:
A visual historical record of american high school yearbooks. In Proceedings of the IEEE
International Conference on Computer Vision Workshops, pp. 1-7, 2015.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical
dependence with hilbert-schmidt norms. In International conference on algorithmic learning
theory, pp. 63-77. Springer, 2005.
David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions
on Information Theory, 57(3):1548-1566, 2011.
Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning
can improve model robustness and uncertainty. In Advances in Neural Information Processing
Systems, pp. 15637-15648, 2019.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9-1, 2012.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
Tzee-Ming Huang. Testing conditional independence using maximal nonlinear conditional correlation.
The Annals of Statistics, 38(4):2047-2091, 2010.
Eric Jang, Coline Devin, Vincent Vanhoucke, and Sergey Levine. Grasp2vec: Learning object
representations from self-supervised grasping. arXiv preprint arXiv:1811.06964, 2018.
Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Sham M Kakade and Dean P Foster. Multi-view regression via canonical correlation analysis. In
International Conference on Computational Learning Theory, pp. 82-96. Springer, 2007.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-
tion learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pp. 1920-1929, 2019.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. In Proceedings of the International Conference on Learning Representations, 2018.
Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical efficiency. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, 2018.
Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine
Learning Research, 7(Dec):2651-2667, 2006.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in neural information processing
systems, 2013.
10
Under review as a conference paper at ICLR 2021
Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning using
temporal order verification. In European Conference on Computer Vision, pp. 527-544. Springer,
2016.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69-84. Springer, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2536-2544, 2016.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.
Michael Reed. Methods of modern mathematical physics: Functional analysis. Elsevier, 2012.
Mark Rudelson, Roman Vershynin, et al. Hanson-wright inequality and sub-gaussian concentration.
Electronic Communications in Probability, 18, 2013.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empirical methods in natural language processing, 2013.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic
posterior information to linear models. arXiv preprint arXiv:2003.02234, 2020a.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020b.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-
tion, 23(7):1661-1674, 2011.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103, 2008.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In Proceedings of the IEEE International Conference on Computer Vision, 2015.
Donglai Wei, Joseph J Lim, Andrew Zisserman, and William T Freeman. Learning and using the
arrow of time. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 8052-8060, 2018.
Han Yang, Xiao Yan, Xinyan Dai, and James Cheng. Self-enhanced gnn: Improving graph neural
networks using model outputs. arXiv preprint arXiv:2002.07518, 2020.
Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European
conference on computer vision, pp. 649-666. Springer, 2016.
11
Under review as a conference paper at ICLR 2021
Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning
by cross-channel prediction. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp.1058-1067, 2017.
Zaiwei Zhang, Zhenxiao Liang, Lemeng Wu, Xiaowei Zhou, and Qixing Huang. Path-invariant map
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 11084-11094, 2019.
12
Under review as a conference paper at ICLR 2021
A Some Useful Facts
A.1 Relation of Inverse Covariance Matrix and Partial Correlation
en For a covariance matrix of joint distribution for variables X, Y , the covariance matrix is
		ΣXX ΣY X	ΣXY ΣYY		Σ Σ Σ	1 X1 2X1 YX1	ΣX1X2 ΣX2X2 ΣX2Y		ΣX1Y ΣX2Y ΣYY	.	
Its inverse matrix Σ-	1 satisfies										
			Σ-1		=	A ρ>	ρ B	.			
Here A-1 = ΣXX	- ΣXY Σ		Y-1Y ΣY X	≡ cov(X -			EL[X|Y]		,X - EL[X|Y]) :		=ςXX∙Y, the
partial covariance matrix of X given Y .
A.2 Relation to Conditional Independence
Proof of Lemma C.5.
FactA.1. When X1⊥X2∣Y ,the partial covariance between X1,X2 given Y is 0:
∑X1X2∙Y =cov(X1 - EL[Xl∣Y],X2 - El[X2∣Y])
≡ΣX1X2 - ΣX1Y ΣY-1Y ΣY X2 = 0.
The derivation comes from the following:
Lemma A.1 (Conditional independence (Adapted from Huang (2010))). For random variables
X1,X2 and a random variable Y with finite values, conditional independence X1⊥X2∣Y is equiva-
lent to:
sup	E[f(X1)g(X2)|Y] =0.	(6)
f∈N1,g∈N2
Here Ni = {f : Rdi → R : E[f(Xi)|Y] = 0}, i = 1,2.
Notice for arbitrary function f, E[f (X)|Y] = EL[f (X)∣φy (Y)] with one-hot encoding of discrete
variable Y. Therefore for any feature map we can also get that conditional independence ensures:
ςΦi(Xi)Φ2(X2)∣Y =COv(OI(XI)- ELM(XI)lφy (Y )],φ2(χ2) - EL [φ2(X2)lφy (Y)])
Here φ1(X1) = φ1(X1) 一 E[φ1(X1)∣φy(Y)] is mean zero given Y, and vice versa for φ2(X2). This
thus finishes the proof for Lemma C.5.	□
A.3 Technical Facts for Matrix Concentration
We include this covariance concentration result that is adapted from Claim A.2 in Du et al. (2020):
Claim A.2 (covariance concentration for gaussian variables). Let X = [χ1,χ2, •…Xn]> ∈ Rn×d
where each Xi 〜N(0, Σχ). Suppose n》k + log(1∕δ) for δ ∈ (0,1). Thenfor any given matrix
B ∈ Rd×m that is of rank k and is independent of X, with probability at least 1 —a over X we
0.9B>ΣxB W -B>X>XB W 1.1B>ΣxB.	(7)
n
And we will also use Claim A.2 from Du et al. (2020) for concentrating subgaussian random variable.
Claim A.3 (covariance concentration for subgaussian variables). Let X = [χ1,χ2, •…Xn]> ∈ Rn×d
where each Xi 〜N(0, Σχ). Suppose n》ρ4(k + log(1∕δ)) for δ ∈ (0,1). Thenfor any given
matrix B ∈ Rd×m that is ofrank k and is independent of X, with probability at least 1 —磊 over X
wehave	0.9B>ΣχB W 1 B>X>XB W 1.1B>∑χB.	(8)
n
13
Under review as a conference paper at ICLR 2021
Claim A.4. Let Z ∈ Rn×k be a matrix with row vectors sampled from i.i.d Gaussian distribution
N(0, ΣZ). Let P ∈ Rn×n be a fixed projection onto a space of dimension d. Then with a fixed
δ ∈ (0, 1), we have:
kPZkF . Tr(∑z)(d + log(k∕δ)),
with probability at least 1 - δ.
Proof of Claim A.4. Each t-th column of Z is an n-dim vector that is i.i.d sampled from Gaussian
distribution N(0, Σtt).
k
kPZk2F=XkPztk2
t=1
k
= X zt> Pzt.
t=1
Each term satisfy Σ-1 ∣∣Pzt k2 〜X (d), and therefore with probability at least 1 - δ0 over zt,
Σk-k1kPztk2 . d + log(1∕δ0).
Using union bound, take δ0 = δ∕k and summing over t ∈ [k] we get:
kPZk2F . Tr(ΣZ)(d + log(k∕δ)).
□
Theorem A.5 (Hanson-Wright Inequality (Theorem 1.1 from Rudelson et al. (2013))). Let X =
(X1, X2, •…Xn) ∈ Rn be a random vector with independent components Xi which satisfy E [Xi] = 0
and kXi kψ2 ≤ K. Let A be an n × n matrix. Then, for every t ≥ 0,
p[∣χ>Aχ - E[χ>Aχ ]| >t] ≤ 2exp 卜 C min (KkAI, KlAi )}.
Theorem A.6 (Vector Bernstein Inequality (Theorem 12 in Gross (2011))). Let Xi,…,Xm be
independent zero-mean vector-valued random variables. Let
m
N= kXXik2.
i=i
Then
P[N ≥ √V + t] ≤ exp (-V^),
where V = Pi E kXi k22 and t ≤ V ∕(max kXi k2).
Lemma A.7. Let Z ∈ Rn×k be a matrix whose row vectors are n independent mean-zero (condi-
tional on P) σ-sub-Gaussian random vectors. With probability 1 - δ:
kPZk2.σ2(d+log(d∕δ)).
ProofofLemma A.7. Write P = UU> = [ui,…,Ud] where U is orthogonal matrix in Rn×d
where U> U = I.
kPZk2F =kU>Zk2F
d
=Xkuj>Zk2
j=i
dn
=XkXujizik2,
j=i i=i
14
Under review as a conference paper at ICLR 2021
where each zi ∈ Rk being the i-th row of Z is a centered independent σ sub-Gaussian random
vectors. To use vector Bernstein inequality, we let X := Pin=1 Xi with Xi := ujizi. We have Xi is
zero mean: E[Xi] = E[uji E[z∕uji[] = E[uji ∙ 0] = 0.
V :=XE kXik22
i
= X E[uj2izi>zi]
i
= X Euji [uji E[kzi k2 |uj i]]
i
≤σ2XEuji[uj2i]
i
Therefore by vector Bernstein Inequality, with probability at least 1 - δ∕d, ∣∣X∣∣ ≤ σ(1 +/log(d∕δ)).
Then by taking union bound, We get that ∣∣PZ∣∣2 = P；=i ∣∣u>Z∣∣2 . σ2(d + log(d∕δ)) with
probability 1 - δ.
□
Corollary A.8. Let Z ∈ Rn×k be a matrix whose row vectors are n independent samples from
centered (conditioned on P) multinomial probabilities (p1,p2, ∙…Pk) (where Pt could be different
across each row). Let P ∈ Rn×n be a projection onto a space of dimension d (that might be
dependent with Z). Then we have
∣PZ∣2 . d + log(d∕δ).
with probability 1 - δ.
B Omitted Proofs with Conditional Independence
Proof of Lemma 3.2.
cov(X1 |Y, X2 |Y ) = ΣX1X2 - ΣX1Y ΣY-1Y ΣYX2 = 0.
By plugging it into the expression of EL[X2 |X1], we get that
ψ(x1) :=EL[X2|X1 =x1] =ΣX2X1Σ-X11X1x1
= ΣX2Y ΣY-1Y ΣYX1Σ-X11X1x1
=∑X2Y ∑-Y EL [Y ∣Xι].
Therefore, as long as ΣX2Y of rank k, it has left inverse matrix and we get: EL[Y|X1 = x1] =
∑X γΣγγψ(χι). Therefore there,s no approximation error in using ψ to predict Y.
□
ProofofCorollary 3.4. Let selector operator Sy be the mapping such that Sy Y = Y, we overload it
as the matrix that ensure Sy ΣγX = Σγχ for any random variable X as well.
From Lemma 3.2 we get that there exists W such that EL[Y∣Xι] = W EL[X2∣X1 ],just plugging in
Sy we get that EL[Y |X1] = (SyW)EL[X2|X1].
□
ProofofTheorem 3.3 . Since N is mean zero, f*(X1) = E[Y∣Xι] = (A*)τXι.
EL [Y |X1 = Xi] = ∑X2Y ∑YY ψ(xι). Let W * = Σγγ ∑Yχ2.
15
Under review as a conference paper at ICLR 2021
First we have the basic inequality,
1- kY - Ψ(X1)W kF ≤ 1- kY - XdkF
2n2	2n2
=xl kY - Ψ(Xι)w *kF.
2n2
Therefore
∣∣ψ(X1)W* — ψ(X1)Wk2 ≤2{N, ψ(X1)W* — ψ(X1)Wi
=2hPψ(Xι)N,ψ(Xι)W* - ψ(Xι)Wi
≤2∣Pψ(χι)N∣Fkψ(Xι)W* - ψ(Xι)W∣F
.. , ʌ ..
⇒ kΨ(Xι)W* - Ψ(Xι)Wk ≤2∣Pψ(χι)N∣F
.JTr(∑γγ χι)(k + log k∕δ).	(from Claim A.4)
The last inequality is derived from Claim A.7 and the fact that each row of N follows gaussian
distribution N(0, Σγγ∣χ) Therefore
-1 kΨ(Xι)W* - Ψ(Xι)WkF . ”('YNx'k + logkk”.
n2	n2
Next we need to concentrate 1∕nX1>X1 to ΣX. Suppose EL[X2|X1] = B>X1, i.e., φ(x1) =
B>x1, and φ(X1) = X1B. With Claim A.2 we have1∕nφ(X1)>φ(X1) =1∕nB>X1>X1B
satisfies:
0.9B>ΣXB 1∕n2φ(X1)>φ(X1) 1.1B>ΣXB
Therefore we also have:
E[(W* - W)>ψ(xι)]
=k∑X/2B(W* - W)kF
1
≤------
0.9n2k
kΨ(Xι)w* - ψ(Xι)WkF . n(:SYY|Xi)(k + logk/e
n2
□
B.1 Omitted Proof for General Random Variables
Proof of Lemma 3.5. Let the representation function ψ be defined as:
ψ(∙) := E[X2X1] = E[E[X2X1,Y]X1]
= E[E[X2 |Y]|X1]	(uses CI)
= XP(Y = y|X1) E[X2|Y = y]
y
=:f(X1)>A,
where f : Rd1 → ∆Y satisfies f(x1)y = P(Y = y|X1 = x1), and A ∈ RY×d2 satisfies Ay,: =
E[X2|Y = y]. Here ∆d denotes simplex of dimension d, which represents the discrete probability
density over support of size d.
Let B = At ∈ RY×d2 be the pseudoinverse of matrix A, and We get BA = I from our assumption
that A is of rank |Y|. Therefore f(x1) = Bψ(x1), ∀x1. Next we have:
E[Y |X1 = x1] =XP(Y = y|X1 = x1) × y
y
=Yf(x1)
=(YB) ∙ Ψ(X1).
Here We denote by Y ∈ Rk×Y, Y:,y = y that spans the Whole support Y. Therefore let W* = YB
Will finish the proof.
□
16
Under review as a conference paper at ICLR 2021
ProofofTheorem 3.6. With Lemma 3.5 We know eapχ = 0, and therefore W*ψ(Xι) ≡ f*(X∖).
Next from basic inequality and the same proof as in Theorem 3.3 we have:
∣∣ψ(Xι)W*- ψ(Xι)W∣∣ ≤2∣∣Pψ(χι)NIIF
Notice N is a random noise matrix whose row vectors are independent samples from some centered
distribution. Also we assumed E[INI2|X1] ≤ σ2, i.e. E[INI2|N] ≤ σ2. Also, Pψ(X1) is a
projection to dimension c. From Lemma A.7 we have:
Ilf*(Xι) - ψ(Xι)Wk ≤σPc + logc∕δ.
Next, with Claim A.3 We have when n》ρ4(c + log(1∕δ)), since W * 一 W ∈ Rd2×k,
0.9(W* 一 W)>Σψ(W* 一 W)
W-1(W* 一 W)T Xψ(x1i))ψ(xIi))T(W* 一 W) W 1.1(W* 一 W)>Σψ(W* 一 W)
2i
And therefore we could easily conclude that:
EkW tΨ(Xi )一 f *(X1)k2 .σ2 C + log(c∕δ).
n2
□
B.2 Omitted proof of linear model with approximation error
Proof of Theorem 3.9. First we note that Y = f*(X1) + N, where E[N |X1] = 0 but Y 一 (A*)TX1
is not necessarily mean zero, and this is where additional difficulty lies. Write approximation error
term a(X1) := f*(X1) 一 (A*)TX1, namely Y = a(X1) + (A*)TX1 + N. Also, (A*)TX1 ≡
(W *)Tψ(X1) with conditional independence.
Second, with KKT condition on the training data, we know that E[a(X1)X1T] = 0.
Recall W = argmi□w ∣∣ Y 一 ψ(Xι)W∣∣F. We have the basic inequality,
ɪ∣Y - Ψ(Xι)W∣F ≤ɪ∣Y -XιA*kF
2n2	2n2
=xl ∣y 一 Ψ(Xι)W *kF.
2n2
i.e., ɪ∣ψ(Xι)W* + a(Xι) + N 一 ψ(Xι)W∣F ≤ɪ∣a(Xι) + N∣F.
2n2	2n2
Therefore
ɪkΨ(Xι)W* - ψ(Xι)W∣2
2n2
1
≤ ——ha(Xι) + N,ψ(Xι)W* 一 ψ(Xι)W)
n2
1
=——ha(Xι),ψ(Xι)W* 一 ψ(Xι)Wi 一 hN,ψ(Xι)W* 一 ψ(Xι)W)	(9)
n2
With Assumption 3.6 and by concentration 0.9^^XIXT W ∑Xι W 1.1 ^^XιX>, we have
√⅛ka(XI)XTςxRf ≤ 1.1b0√k
(10)
17
Under review as a conference paper at ICLR 2021
Denote ψ(X1) = X1B, where B = Σ-X1ΣX1X2 is rank k under exact CI since ΣX1X2 =
ΣX1YΣY-1ΣYX2. We have
1
一ha(Xι), ψ(Xι)W* - ψ(Xι) Wi
n2
1
二一ha(Xι), XiBW* - XiBW)
n2
=n1 h∑χY2X>a(Xι), ∑X12(BW* - BW)i
≤akkk∑X12(BW* - bW)∣∣f	(from Ineq. (10))
n2	1
Back to Eqn. (9), we get
1
厂∣∣ψ(Xi)W* - ψ(Xi)WIlF
2n2
.W k∑X12(BW * - bW)∣∣f + n1 ∣∣Pχι N kF IXi (BW * - bW)∣∣f
.	—+ -1 ∣∣PχιNIlFhlXI(BW* - BW)IIF
n2 n2	1
=⇒√LkΨ(Xi)W* - Ψ(Xι)W∣F . Sk + logkk/.
√n2	V n
Finally, by concentration we transfer the result from empirical loss to excess risk and get:
E[∣∣ψ(Xι)W* - ψ(Xι)W∣2] . k + log(k/6 .
n2
□
B.3 Argument on Denoising Auto-encoder or Context Encoder
Remark B.1. We note that since Xi⊥X2∣Y ensures X1⊥h(X2)∣Y forany determiniSticfunction h,
we could replace X2 by h(X2) and all results hold. Therefore in practice, we could use h(ψ(Xi))
instead of ψ(Xi) for downstream task. Specifically with denoising auto-encoder or context encoder,
one could think about h as the inverse of decoder D (h = D-i) and use D-iψ ≡ E the encoder
function as the representation for downstream tasks, which is more commonly used in practice.
This section explains what we claim in Remark B.1. For context encoder, the reconstruction loss
targets to find the encoder E * and decoder D* that achieve
min min E IX2 - D(E(Xi))I2F,	(11)
ED
where X2 is the masked part we want to recover and Xi is the remainder.
If We naively apply our theorem We should use D* (E*(∙)) as the representation, while in practice We
instead use only the encoder part E*(∙) as the learned representation. We argue that our theory also
support this practical usage if We vieW the problem differently. Consider the pretext task to predict
(D*)-i(X2) instead ofX2 directly, namely,
E 一 argminE∣(D*)-i(X2) - E(Xi)I∣2,	(12)
E
and then We should indeed use E(Xi) as the representation. On one hand, When Xi⊥X2 |Y, it
also satisfies Xi⊥(D*)-i(X2)∣Y since (D*)-i is a deterministic function of X2 and all our theory
applies. On the other hand, the optimization on (11) or (12) give us similar result. Let
E* = arg min E[IX2 - D*(E(Xi))I2],
E
18
Under review as a conference paper at ICLR 2021
and E ∣∣X2 一 D*(E*(X1))k2 ≤ 3 then with pretext task as in (12) We have that:
E k(D*)-1(X2) 一 E*(Xi)∣∣2 = E k(D*)-1(X2)-(D*)-% D*(E*(X1))∣∣2
≤k(D*)-1kLipEkX2 - D*(E*(X1))k2
≤L2,
where L := k(D*)-1∣ LiPiSthe Lipschitz constant for function (D *)-1. This is to say, in practice,
We optimize over (11), and achieves a good representation E*(X1) such that EPre ≤ L√e and thus
performs well for downstream tasks. (Recall pre is defined in Theorem 4.2 that measures how well
we have learned the pretext task.)
C Omitted Proofs B eyond Conditional Independence
C.1 Warm-up: Jointly Gaussian Variables
As before, for simplicity we assume all data is centered in this case.
Assumption C.1 (Approximate Conditional Independent Given Latent Variables). Assume there
exists some latent variable Z ∈ Rm such that
Iςx1∕2ςXi,X2Y kF ≤ E CI,
σk+m(∑Yγ Σγ χ2) = β > 0 3 and ∑χ2,γ is of rank k + m, where Y = [Y, Z ].
When X1 is not exactly CI of X2 given Y and Z, the approximation error depends on the norm of
k∑x1∕2∑xι,x2∣γk2. Let W be the solution from Equation 2.
Theorem C.1. Under Assumption C.1 with constant ECI and β, then the excess risk satisfies
ERψ* [W] := E[kW>ψ*(X1) - f*(X1)kF] . β⅛ +Tr(Σγγ∣χι)d2 +1；(d2/6 .
Proof of Theorem C.1. Let V := f*(X1) ≡ X1Σ-X1X Σ1γ be our target direction. Denote the
optimal representation matrix by Ψ := ψ(X1) ≡ X1A (where A := Σ-X1X ΣX1X2).
Next we will make use of the conditional covariance matrix:
ςXi X2∣Y := ςXiX2 一 ςXiY ςy1ςy X2 ,
and plug it in into the definition of Ψ:
ψ =xiςx1xi 以M%%X2 + X1 ςx1xi ςXiX2∣Y
=:L + E,
where L := xiςx1xi ςXiY ςy1ςy X2 and E := xiςx1xi ςXiX2 |Y. We analyze these two
terms respectively.
For L, we note that span (V) ⊆span(L): L∑X2γ7 Σγ = Xι∑χ1χ1 ∑XιY. By right multiplying
the selector matrix SY we have: L∑X2γλΣγY = Xι∑χ1χ1 ∑XιY, i.e., LW = V, where W :=
∑X2γ Σγγ. From our assumption that σr(∑YγΣγX2) = β, we have kWk2 ≤ k∑X2γΣγk2 ≤
1∕β. (Or we could directly define β as σk (∑Yγ Σγ χ?) ≡ IlW ∣∣2.)
By concentration, we have E = Xι∑χ1χ1 ∑X1X2∣γ converges to Σχ1X2ι Σχ1 χ2∣γ. Specifically,
when n》k + log1∕δ, ∣E∣f ≤ 1.1kΣχ1X1Σχlχ2∣γkF ≤ 1.1ECI (by using Lemma A.2 ).
Together we have kEWkF . ΕCI∕β.
Let W = argminw kY 一 ΨWk2. We note that Y = N + V = N + ΨW — EW where V is
our target direction and N is random noise (each row of N has covariance matrix Σγγ∣χι).
3σk (A) denotes k-th singular value of A, and At is the pseudo-inverse of A.
19
Under review as a conference paper at ICLR 2021
From basic inequality, we have:
kψw^ - Y kF ≤∣∣ψw - Y kF = kN - EW kF.
=⇒ ∣∣ΨVW - V - EWk2 ≤2<ψW - V - EW, N - EW)
=⇒ ∣∣ΨVW - V - EWk ≤∣P[ψ,e,v]Nk + kEWk
=⇒ kΨW - Vk .kE∣fkWk + (pd2 + Plog1∕δ)qTr(Σγγ∣χι).
(from Lemma A.7)
≤√n 予 + (pd2+Piθgi∕δ)qτT(∑YY IXI).
(from Assumption C.1)
Next, by the same procedure that concentrates n^X>X1 to ΣχιXi with Claim A.2, We could easily
get
ER[W] := E[k W>ψ(Xι) - f*(X1)k2].皋 + Tr(∑γγ∣χι) d2 +lθg1∕δ.
β 2	1 n2
□
C.2 Technical Facts
Lemma C.2 (Approximation Error of PCA). Let matrix A = L + E where L is rank r <size of A
and kEk2 ≤ and Σr (A) = β . Then we have
k sin Θ(A, L)k2 ≤ ∕β.
Proof. We use Davis Kahan for this proof. First note that kA - Lk = kE k ≤ . From Davis-Kahan
we get:
k sinθ(A，L)k2 ≤ ∑r(A)Rr+ι(L)
=kE k2
∑r (A)
.∕β.
□
C.3 Measuring conditional dependence with cross-covariance operator
L2(PX) denotes the Hilbert space of square integrable function with respect to the measure PX, the
marginal distribution of X. We are interested in some function class Hx ⊂ L2(PX) that is induced
from some feature maps:
Definition C.3 (General and Universal feature Map). We denote feature map φ : X → F that maps
from a compact input space X to the feature space F. F is a Hilbert space associated with inner
product: hφ(x), φ(x0)iF. The associated function class is: Hx = {h : X → R∣∃w ∈ F, h(x)=
hw, φ(x)iF, ∀x ∈ X}. We call φ universal if the induced Hx is dense in L2 (PX).
Linear model is a special case when feature map φ = Id is identity mapping and the inner product is
over Euclidean space. A feature map with higher order polynomials correspondingly incorporate high
order moments (Fukumizu et al., 2004; Gretton et al., 2005). For discrete variable Y we overload φ
as the one-hot embedding.
Remark C.1. For continuous data, any universal kernel like Gaussian kernel or RBF kernel induce
the universal feature map that we require (Micchelli et al., 2006). Two-layer neural network with
infinite width also satisfy it, i.e., ∀x ∈ X ⊂ Rd, φNN (x) : Sd-1 × R → R, φNN (x)[w, b] =
σ(w> x + b) (Barron, 1993).
20
Under review as a conference paper at ICLR 2021
When there’s no ambiguity, we overload φ1 as the random variable φ1(X1) over domain F1, and H1
as the function class over X1 . Next we characterize CI using the cross-covariance operator.
Definition C.4 (Cross-covariance operator). For random variables X ∈ X , Y ∈ Y with joint
distribution P : X × Y → R, and associated feature maps φx and φy, we denote by Cφxφy =
E[φx(X) 0 φy(Y)] = Jχ×γ φχ(x) 0 φy(y)dP(x, y), the (Un-Centered) Cross-CoVariance operator.
Similarly we denote by Cχφy = E[X 0 φy (Y)] : Fy → X.
To understand what Cφxφy is, we note it is of the same shape as φx(x) 0 φy(y) for each in-
dividual x ∈ X, y ∈ Y. It can be viewed as a self-adjoint operator: Cφxφy : Fy → Fx,
Cφxφyf = RX×Yhφy(y),fiφx(x)dP(x,y),∀f ∈ Fy. For any f ∈ Hx and g ∈ Hy, it satisfies:
hf, Cφχφy giHx = Eχγ[f (X )g(Y )](Baker, 1973; FUkUmiZUetaL,2004). CI ensures Cφ1X2∣φy =0
for arbitrary φ1, φ2 :
Lemma C.5. With one-hot enCoding map φy and arbitrary φ1, X1 ⊥X2 |Y ensures:
cφlX2∣Φy ：= CφlX2 -Cφlφy C-y∖φy CΦyX2 = 0.	(I3)
A more complete discUssion of cross-covariance operator and CI can be foUnd in (FUkUmiZU et al.,
2004). Also, recall that an operator C : Fy → Fx is Hilbert-Schmidt (HS) (Reed, 2012) if for
complete orthonormal systems (CONSs) {ζi} ofFx and {ηi} ofFy, kCk2HS := Pi,j hζj, Cηii2F < ∞.
The Hilbert-Schmidt norm generaliZes the FrobeniUs norm from matrices to operators, and we will
later use ∣∣Cφ1X2∣φy k to quantify approximate CL
We note that covariance operators (FUkUmiZU et al., 2009; 2004; Baker, 1973) are commonly Used to
capture conditional dependence of random variables. In this work, we utiliZe the covariance operator
to quantify the performance of the algorithm even when the algorithm is not a kernel method.
C.4 Omitted Proof in General Setting
Claim C.6. For feature maps φ1 with universal property, we have:
ψ*(X1) := E[X2∣X1] = EL [X2∣φι]
=CX2φ1Cφ-11φ1φ1(X1).
Our target f *(Xι) := E[Y∣Xι] = EL[Y∣Φι]
=CYφ1 Cφ-11φ1 φ1(X1).
For general feature maps, we instead have:
Ψ*(X1) ：= argminEχ1X2 ∣∣X2 — f(Xι)k2
f∈H1d2
=CX2φ1Cφ-11φ1φ1(X1).
Our target f *(Xι) := arg min EχιY ∣∣Y — f (Xι)∣2
f∈H1k
=CYφ1 Cφ-11φ1 φ1 (X1).
To prove Claim C.6, we show the following lemma:
Lemma C.7. Let φ : X → Fx be a universal feature map, then for random variable Y ∈ Y we
have:
E[Y |X ] = EL [Y ∣φ(X)].
Proof of Lemma C.7. Denote by E[Y |X = x] =: f (x). Since φ is dense in X, there exists a linear
operator a : X → R such that JxEX a(χ)φ(χ)[∙]dχ = f (∙) a.e. Therefore the result comes directly
from the universal property of φ.	□
Proof of Claim C.6. We want to show that for random variables Y, X, where X is associated with a
universal feature map φx, wehaveE[Y|X] = CYφx(X)Cφ-x1(X)φx(X)φx(X).
21
Under review as a conference paper at ICLR 2021
First, from Lemma C.7, We have that E[Y|X] = EL[Y∣φχ(X)]. Next, write A* : Fx → Y as the
linear operator that satisfies
E[Y|X] =A*φx(X)
s.t. A* = arg min E[kY - Aφx (X)k2].
A
Therefore from the stationary condition We have A* EX [φx(X) 0 φx(X)] = EXY [Y 0 φx(X)]. Or
namely we get A* = CγφxC-1φx SimPly from the definition of the cross-covariance operator C. □
ClaimC.8. kC-ιΦ2CφιX2∣Φy 隘=EχJ∣∣ E[X2∣X1] - EY [E[X2∣Y]∣X1]k2 ]= %
Proof.
kCΦιΦι2CΦιX2∣Φy IIhs
/	/ p PX1X2(Xι, χ2)	pX1⊥X2∣Y(X1, χ2)∖ v ,	2 ,
=	-------z—ʌ--------------5—ʌ------- X2dpχ2	dpxi
X1	X2	PX1 (X1)	PX1 (X1)	2	1
=ExJk E[X2∣X1] - EY[E[X2∣Y]∣Xι]k2]∙
□
C.5 Omitted Proof for Main Results
We first prove a simpler version without approximation error.
Theorem C.9. For a fixed δ ∈ (0, 1), under Assumption 4.1, 3.5, if there is no approximation error,
i.e., there exists a linear operator A such that f *(Xι) ≡ Aφ1(X1), if nι,n2》ρ4(d2 + log1∕δ),
and we learn the pretext tasks such that:
E∣∣ψ(Xι)- ψ*(X1)kF ≤ ePre∙
Then we are able to achieve generalization for downstream task with probability 1 - δ:
E[kfH 1 (Xi) — W >Ψ(Xι)k2] ≤ O{σ2 d2 +lθg d2” + 餐 + ⅜e }∙	(14)
n2	β β
Proof of Theorem C.9. We follow the similar procedure as Theorem C.1. For the setting of no
approximation error, we have f* = fH* , and the residual term N := Y - f*(X1) is a mean-
zero random variable with E[kN k2 |X1] . σ2 according to our data assumption in Section 3.
N = Y - f* (X1down) is the collected n2 samples of noise terms. We write Y ∈ Rd3 . For
classification task, we have Y ∈ {ei, i ∈ [k]} ⊂ Rk (i.e, d3 = k) is one-hot encoded random variable.
For regression problem, Y might be otherwise encoded. For instance, in the yearbook dataset, Y
ranges from 1905 to 2013 and represents the years that the photos are taken. We want to note that our
result is general for both cases: the bound doesn’t depend on d3, but only depends on the variance of
N.
Let Ψ*, L, E, V be defined as follows:
Let V = f*(X1down) ≡ fH* (X1down) ≡ φ(X1down)Cφ-1Cφ1Y be our target direction. Denote the
optimal representation matrix by
ψ* =ψ* (Xdown)
=φ(X1down)Cφ-11φ1Cφ1X2
down -1	-1	down -1
=φ(X1	)CφιφιCφιφyCφy AφyX2 + φ(X1	)CφιφιcΦ1X2∣Φy
=:L + E,
where L =。^^驻—'CφιφsC-e1CφyX2 and E = 0(Xdown)C-£Cφ1X2∣γ.
In this proof, we denote SY as the matrix such that SYφy = Y. Specifically, if Y is of dimension ʤ,
SY is of size d3 × |Y ||Z |. Therefore SYΣφyA = ΣYA for any random variable A.
22
Under review as a conference paper at ICLR 2021
Therefore, similarly we have:
L^Wy 当次 SY = L^Wy %了=LW = V
where W := ∑X2φ, ∑φyY satisfies ∣∣WW ∣∣2 = 1∕β. Therefore span (V) ⊆span(L) since We have
assumed that ∑X2φ. ∑φyγ to be full rank.
On the other hand, E = XdOWnC-LφιCφ1χ2∣γ concentrates to C-L^2CφιX2∣φy. Specifically, when
n》C + log1∕δ, ∣∣E∣∣f ≤ l.lkC-1φ12Cφ1X2∣φy ∣∣f ≤ 1.1e0 (by using Lemma A.3 ). Together we
have IlEWIIF . eo∕β.
We also introduce the error from not learning ψ* exactly: Epre = Ψ 一 Ψ* := ψ(Xdown) — ψ*(Xfown).
With proper concentration and our assumption, we have that E ∣∣ψ(X1) — ψ*(Xι)∣2 ≤ EPre and
√n2Ψ(Xdown) — Ψ*(Xdown)k2 ≤ 1∙lEpre.
Also, the noise term after projection satisfies ∣P[ψ E V]N∣∣ . dd2 + log d2∕δσ as using Lemma
A.7. Therefore Ψ = Ψ* — Epre = L + E — Epre. , ,
Recall that W = argminw ∣ψ(Xdown) W — Y∣∣F. And with exactly the same procedure as Theorem
C.1 we also get that:
∣ΨW - VIl ≤2∣∣EWIl + 2∣∣EPreW∣ + ∣P[ψ,E,v,^p^N∣∣
.√n2 0 R Pre + σpd2 + log(d2∕δ)
β
With the proper concentration we also get:
E[∣ W>ψ(X1) — fH 1 (Xι)∣2].心+圣 + σ2 d2 +log(d2∕δ).
1	β2	n2
□
Next we move on to the proof of our main result Theorem 4.2 where approximation error occurs.
Proof of Theorem 4.2. The proof is a combination of Theorem 3.9 and Theorem C.9. We follow the
same notation as in Theorem C.9. Now the only difference is that an additional term a(X1down) is
included in Y :
Y =N + f * (Xdown)
=N + Ψ*W + a(Xdown)
=N +(Ψ + EPre)W + a(Xdown)
=ΨW + (N + EPreW + a(Xdown)).
From re-arranging + ∣∣Y — ΨW ∣∣F ≤	∣∣Y — ΨW ∣∣F,
ɪ∣∣Ψ(W — W) + (N + Epre + a(Xdown))∣∣F ≤ ɪ∣∣N + EPreW + a(Xdown)∣∣F	(15)
2n2	1 F	2n2	1 F
⇒ ɪ∣Ψ(W — W)∣F ≤ ɪhΨ(W — W), N + EPreW + a(χdown)i.	(16)
2n2	F	n2	1
23
Under review as a conference paper at ICLR 2021
Then with similar procedure as in the proof of Theorem 3.9, and write Ψ as φ(X1down)B, we have:
1
—hΨ(W - W),a(Xdown)i
n2
=—(B(W - W),φ(Xdown)>a(Xdown)i
n2
=-1 hcφ∕B(W - W),C-1∕2φ(Xdown)>a(Xdown)i
≤rd2 kcii2B(w - w)kF
≤1.1 二尸kφ(Xdown)B(W - W)kF
√n2 V n
=1.1 迎 ∣∣ψ(vw - W)kF.
n2
Therefore plugging back to (16) we get:
1— kΨ(W - W)kF ≤ 1- hΨ(W - W), N + EpreW + a(Xdown)i
2n2	F	n2	1
⇒ 1-kΨ(W - W)∣f ≤ 1-∣EpreWIf + ɪkPψNkF + 1.1 运.
2n2	2n2	2n2	n2
⇒ ɪkΨW - fH 1 (Xdown)kF -kEWkF ≤ -^(1.1pd2 + kEpreWk + Pd + log(d2∕δ))
2 √n2	1	√n2
.1 IIxlnQ #(KdOwnw V d +log d2∕δ ECI + EPrf
⇒ 2√nf∣ψw - fH 1 (Xd )kF .y —n— +
Finally by concentrating Ψ>Ψ to E[ψ(X1)ψ(X1)>] we get:
E[kW>ψ(X1) - fH 1 (Xι)k2] . d2 + lθg(Id2/ +E⅛pre,
n2	β
with probability 1 - δ.	□
D Theoretical analysis for classification tasks
D. 1 Classification tasks
We now consider the benefit of learning ψ from a class H1 on linear classification task for label set
Y = [k]. The performance of a classifier is measured using the standard logistic loss
Definition D.1. For a task with Y = [k], classification loss for a predictor f : X1 → Rk is
'cif(f) = E['log(f(X1), Y)], where 'Iog(y,y) = - log
The lossfor representation ψ : Xi → Rd1 and linear classifier W ∈ Rk×d1 is denoted by ' Clf (W ψ).
We note that the function `log is 1-Lipschitz in the first argument. The result will also hold for the
hinge loss 'hinge(y, y) = (1 - yy + maxyo=y Oy，)+ which is also I-LiPschitz, instead of '∣og.
We assume that the optimal regressor fHι for one-hot encoding also does well on linear classification.
Assumption D.1. The best regressor for 1-hot encodings in Hi does well on classification, i.e.
'Clf(YfH 1) ≤ Eone-hot is SmaUfOr some scalar γ.
Remark D.1. Note that if Hi is universal, then fH ɪ (xi) = E[Y |Xi = xi] and we know that fH
is the Bayes-optimal predictor for binary classification. In general one can potentially predict the
label by looking at arg maxi∈[k] fHɪ (xi)i. The scalar Y captures the margin in the predictor fHɪ.
24
Under review as a conference paper at ICLR 2021
We now show that using the classifier W obtained from linear regression on one-hot encoding with
learned representations ψ will also be good on linear classification. The proof is in Section D
Theorem D.2. For a fixed δ ∈ (0, 1), under the same setting as Theorem 4.2 and Assumption D.1,
we have:
'CIf (YWψ) ≤ O (Y jσ2 d2 + n也δ + β +E j + eone-hot,
with probability 1 - δ.
Proof of Theorem D.2. We simply follow the following sequence of steps
'c∣f (YWΨ) = E['∣og (γVWψ(Xl), Y)]
≤⑺ E 卜log (YfH 1 (Xι),Y) + YkWψ(Xι) - fH 1 (Xι)k]
≤⑹ eone-hot + Y JE [kW Ψ(Xl)- fH I (Xl)k2i
=¢one-hot + Y ,ER/ [W]
where (a) follows because `log is 1-Lipschitz and (b) follows from Assumption D.1 and Jensen’s
inequality. Plugging in Theorem 4.2 completes the proof.	□
E Four Different Ways to Use CI
In this section we propose four different ways to use conditional independence to prove zero approxi-
mation error, i.e.,
Claim E.1 (informal). When conditional independence is satisfied: X1⊥X2 |Y, and some non-
degeneracy is satisfied, there exists some matrix W such that E[Y|X1] = W E[X2 |X1].
We note that for simplicity, most of the results are presented for the jointly Gaussian case, where
everything could be captured by linear conditional expectation EL[Y|X1] or the covariance matri-
ces. When generalizing the results for other random variables, we note just replace X1, X2, Y by
φ1(X1), φ2(X2), φy (Y) will suffice the same arguments.
E.1	Inverse Covariance Matrix
Write Σ as the covariance matrix for the joint distribution PX1X2Y .
Σ
ΣXX ΣXY	Σ-1 A ρ
ΣY>Y ΣYY , Σ = ρ> B
where A ∈ R(d1+d2)×(d1+d2), ρ ∈ R(d1+d2)×k, B ∈ Rk×k. Furthermore
ρ1 ; A = A11 A12
ρ2	A21 A22
for ρi ∈ Rdi×k, i = 1, 2 and Aij ∈ Rdi×dj for i, j ∈ {1, 2}.
Claim E.2. When conditional independence is satisfied, A is block diagonal matrix, i.e., A12 and
A21 are zero matrices.
Lemma E.3. We have the following
E[X1|X2] =	二(Aιι- ριρ>)-1(Pιρ2>	- A12 )X2	(17)
E[X2|X1] =	二(A22 - ρ2p>)T(P2p1>	- A21)X1	(18)
E[Y|X] =	二-B-2 (ρ>Xi + ρ>X2)		(19)
25
Under review as a conference paper at ICLR 2021
where Pi = ρiB~ 1 for i ∈ {1, 2}. Also,
(A11 — ριρ1) 1ριρ>
(A22 — P2P> 厂 1P2P;
1
1 — ρ> Aιι1ρ1
A111 PIP>
1- P>A21P2 再"
Proof. We know that E[X1∣X2] = ∑12∑22lX2 and E[X2∣X1 ] = ∑21∑[Jχ1, where
∑XX
∑11	∑12
∑21	∑22
First using ΣΣ-1 = I, we get the following identities
ςXX A + ςXY ρτ = i
∑χγ A + Σγγ ρτ =0
Σχχ P + Σχγ B = 0
∑Xγ P + Σγγ B = I
(20)
(21)
(22)
(23)
From Equation (22) we get that Σχγ = -ΣχχρB-1 and plugging this into Equation (20) we get
Σχχ A-Σχχ ρB-1ρτ = I
=⇒ ∑χχ = (A - ρB-1ρτ)-1 = (A - PPT)T
∑11	∑12
ς21 ς22
A11 - PIP> a12 - PIP>
A21 — P2P> A22 — p2p>
-1
We now make use of the following expression for inverse of a matrix that uses Schur complement:
M/α = δ — YaTe is the Schur complement of α for M defined below
If M
,then, M-1
a-1 + a-1β(M/a)-1γa-1
For M = (A - P0>), we have that Σχχ
∑12∑221 = -α
—(M/a)-1 Ya-I
= M-1 and thus
-a-1β(M/a)T
(M /a)-1
—a-1
β(M/α)-1((M/α)-1)-1
β
a β
Y δ
=(A11 - PIPi) I(PIP2^ - A12)
This proves Equation (17) and similarly Equation (18) can be proved.
For Equation (19), we know that E[Y∣X = (X1,X2)] = Σγχ∑χXX = ∑Xγ∑χXX. By using
Equation (22) we get Σχγ = -ΣχχρB-1 and thus
E[Y ∣X = (X1,X2)] = -B-1ρτΣχχ ΣχX X
= -B-1ρ>X = B-1(ρ1 X1 + ρ2 X2)
=-B-2 (Piχ1 + ρ2^X2)
For the second part, we will use the fact that (I - abτ)-1 = I + 一；Tbabτ. Thus
(A11 - PIPT) 1p1p2 = (I - A111 ρ1p;)AIIIP1ρ2^
1
=(I +
1 - p> AIIIPI
A11 PIPI)A111PIP>
A-11 (I +
1 - p1 A111 ρ1
P1P1A-I)P1P>
A111 (PIP> +
A111 ρ1ρ>(1 +
PIA-IPI
1 - PTAIIIPI
PIA-IIPI
P1P>)
1 - ρ: AIIIPI
1 - ρ: AIIIPI
A-1ρ1ρ>
1
)
1
26
Under review as a conference paper at ICLR 2021
The other statement can be proved similarly.	□
Claim E.4.
E[X2∣X1] = (A22 - P2P>)-1P2P>X1. E[Y∣Xι] = -BT/2p>Xi - BT/2p> E[X2∣X1]
Therefore E[Y|X1] is in the same direction as E[X2|X1].
E.2 Closed form of Linear Conditional Expectation
Refer to Claim 3.1 and proof of Lemma 3.2. As this is the simplest proof we used in our paper.
E.3 From Law of Iterated Expectation
EL[X2|X1] =EL[EL[X2|X1,Y]|X1]
-1
=E [ΣX2X1, ΣX2Y] ΣΣXY1XX1 ΣΣXY1YY	XY1 |X1
=AX1 + BEL[Y|X1].
Using block matrix inverse,
A = (ΣX2X1 - ΣX2Y ΣY-1Y ΣYX1)(ΣX1X1 - ΣX1Y ΣY-1Y ΣYX1 )-1 ∈ Rd2×d1
=ςXiX2∣Y (£xiXi|Y ) 1
B = ςX2Y∣Xi (£YY|Xi)-1 ∈ Rd2×Y.
Therefore in general (without conditional independence assumption) our learned representation will
be ψ(χ1) = Aχ1 + Bf *(x1), where f *(•):= EL[Y∣X1].
It's easy to see that to learn f * from representation ψ, We need A to have some good property, such
as light tail in eigenspace, and B needs to be full rank in its column space.
Notice in the case of conditional independence, ∑X1X2∣γ = 0, and A = 0. Therefore we could
easily learn f * from ψ if X2 has enough information of Y such that ∑X2Y∣X1 is of the same rank as
dimension of Y.
E.4 FROME[X2|X1,Y] = E[X2 |Y]
Proof. Let the representation function ψ be defined as follows, and let we use law of iterated
expectation:
ψ(∙) ：= E[X2X1] = E[E[X2X1,Y]∣X1]
= E[E[X2 |Y]|X1]	(uses CI)
= XP(Y = y|X1) E[X2|Y = y]
y
=:f(X1)>A,
where f : Rd1 → ∆Y satisfies f(x1)y = P(Y = y|X1 = x1), and A ∈ RY×d2 satisfies Ay,: =
E[X2|Y = y]. Here ∆d denotes simplex of dimension d, which represents the discrete probability
density over support of size d.
Let B = At ∈ RY×d2 be the pseudoinverse of matrix A, and we get BA = I from our assumption
that A is of rank |Y|. Therefore f(x1) = Bψ(x1), ∀x1. Next we have:
E[Y |X1 =x1] =XP(Y=y|X1 =x1) × y
y
ʌ
=Y f(χι)
,ʌ
=(YB) ∙ ψ(X1).
27
Under review as a conference paper at ICLR 2021
Here We denote by Y ∈ Rk×Y, Y,y = y that spans the whole support Y. Therefore let W * = Y B
will finish the proof.
□
28
Under review as a conference paper at ICLR 2021
Figure 3: Left: MSE of using ψ to predict Y versus using X1 directly to predict Y . Using ψ
consistently outperforms using X1. Right: MSE of ψ learned with different n1. The MSE scale with
1/n2 as indicated by our analysis. Simulations are repeated 100 times, with the mean shown in solid
line and one standard error shown in shadow.
-----Xn (linear)
山 P3」ajnbs urŋ① W
Figure 4: Left: Example of the X2 (in the red box of the 1st row), the X1 (out of the red box of the
1st row), the input to the inpainting task (the second row), ψ(X1) (the 3 row in the red box), and in
this example Y = 1967. Middle: Mean Squared Error comparison of yearbook regression predicting
dates. Right: Mean Absolute Error comparison of yearbook regression predicting dates. Experiments
are repeated 10 times, with the mean shown in solid line and one standard error shown in shadow.
F	More on the experiments
In this section, we describe more experiment results.
Simulations. Following Theorem 4.2, we know that the Excessive Risk (ER) is also controlled by
(1) the number of samples for the pretext task (n1), and (2) the number of samples for the downstream
task (n2), besides k and CI as discussed in the main text. In this simulation, we enforce strict
conditional independence, and explore how ER varies with n1 and n2 . We generate the data the same
way as in the main text, and keep α = 0, k = 2, d1 = 50 and d2 = 40 We restrict the function class
to linear model. Hence ψ is the linear model to predict X2 from X1 given the pretext dataset. We use
Mean Squared Error (MSE) as the metric, since it is the empirical version of the ER. As shown in
Figure 3, ψ consistently outperforms X1 in predicting Y using a linear model learnt from the given
downstream dataset, and ER does scale linearly with 1/n2, as indicated by our analysis.
Computer Vision Task. We testify if learning from ψ is more effective than learning directly
from X1 , in a realistic setting (without enforcing conditional independence). Specifically, we test on
the Yearbook dataset (Ginosar et al., 2015), and try to predict the date when the portraits are taken
(denoted as YD), which ranges from 1905 to 2013. We resize all the portraits to be 128 by 128. We
crop out the center 64 by 64 pixels (the face), and treat it as X2 , and treat the outer rim as X1 as
shown in Figure 4. Our task is to predict YD, which is the year when the portraits are taken, and
the year ranges from 1905 to 2013. For ψ, we learn X2 from X1 with standard image inpainting
29
Under review as a conference paper at ICLR 2021
2
L
」0」」山 pə-enbs Ue①W
1,0
L L
9 8 7 6
O 200	400	600	800 IOOO 1200	0	2	4	6	8
Number of labeled data	Top 10 eigen-values
Figure 5: Left: Mean Squared Error comparison of predicting gender and predicting date. Right: the
spectrum comparison of covariance condition on gender and condition on date.
techniques (Pathak et al., 2016), and full set of training data (without labels). After that we fix the
learned ψ and learn a linear model to predict YD from ψ using a smaller set of data (with labels).
Besides linear model on X1, another strong baseline that we compare with is using ResNet18 (He
et al., 2016) to predict YD from X1. With the full set of training data, this model is able to achieve a
Mean Absolute Difference of 6.89, close to what state-of-the-art can achieve (Ginosar et al., 2015).
ResNet18 has similar amount of parameters as our generator, and hence roughly in the same function
class. We show the MSE result as in Figure 4. Learning from ψ is more effective than learning from
X1 or X2 directly, with linear model as well as with ResNet18. Practitioner usually fine-tune ψ with
the downstream task, which usually leads to more competitive performance (Pathak et al., 2016).
Following the same procedure, we try to predict the gender YG. We normalize the label (YG, YD) to
unit variance, and confine ourself to linear function class. That is, instead of using a context encoder to
impaint X2 from X1 , we confine ψ to be a linear function. As shown on the left of Figure 5, the MSE
of predicting gender is higher than predicting dates. We find that k∑X∕X1 ∑X1X2∣YG ∣∣f = 9.32,
while ∣∣∑χ1Xι ∑XιX2∣Yd ∣∣f = 8.15. Moreover, as shown on the right of Figure 5, conditioning on
YD cancels out more spectrum than conditioning on YG. In this case, we conjecture that, unlike YD,
YG does not capture much dependence between X1 and X2. And as a result, CI is larger, and the
downstream performance is worse, as we expected.
30