Under review as a conference paper at ICLR 2021
Density estimation on low-dimensional mani-
folds: an inflation-deflation approach
Anonymous authors
Paper under double-blind review
Ab stract
Normalizing Flows (NFs) are universal density estimators based on Neuronal Net-
works. However, this universality is limited: the density’s support needs to be
diffeomorphic to a Euclidean space. In this paper, we propose a novel method
to overcome this limitation without sacrificing the universality. The proposed
method inflates the data manifold by adding noise in the normal space, trains an
NF on this inflated manifold and, finally, deflates the learned density. Our main re-
sult provides sufficient conditions on the manifold and the specific choice of noise
under which the corresponding estimator is exact. Our method has the same com-
putational complexity as NFs, and does not require to compute an inverse flow.
We also show that, if the embedding dimension is much larger than the manifold
dimension, noise in the normal space can be well approximated by some Gaus-
sian noise. This allows using our method for approximating arbitrary densities on
non-flat manifolds provided that the manifold dimension is known.
1	Introduction
Many modern problems involving high dimensional data are formulated probabilistically. Key con-
cepts, such as Bayesian Classification, Denoising or Anomaly Detection, rely on the data generating
density p* (x). Therefore, a main research area and of crucial importance is learning this data gen-
erating density p*(x) from samples.
For the case where the corresponding random variable X ∈ RD takes values on a manifold diffeo-
morphic to RD, a Normalizing Flow (NF) can be used to learn p*(x) exactly (Huang et al., 2018).
Recently, a few attempts have been made to overcome this topological constraint. However, to do
so, all of these methods either need to know the manifold beforehand (Gemici et al. (2016), Rezende
et al. (2020)) or they sacrifice the exactness of the estimate (Cornish et al. (2019), Dupont et al.
(2019)).
Our goal in this paper is to overcome both the aforementioned limitations of using NFs for density
estimation on Riemannian manifolds. Given data points from a d-dimensional Riemannian mani-
fold embedded in RD , d < D, we first inflate the manifold by adding a specific noise in the normal
space direction of the manifold, then train an NF on this inflated manifold, and, finally, deflate the
trained density by exploiting the choice of noise and the geometry of the manifold. See Figure 1 for
a schematic overview of these points.
Our main theorem states sufficient conditions on the manifold and the type of noise we use for the
inflation step such that the deflation becomes exact. To guarantee the exactness, we do need to know
the manifold as in e.g. Rezende et al. (2020) because we need to be able to sample in the manifold’s
normal space. However, as we will show, for the special case where D d, the usual Gaussian
noise is an excellent approximation for a noise in the normal space component. This allows using our
method for approximating arbitrary densities on Riemannian manifolds provided that the manifold
dimension is known. In addition, our method is based on a single NF without the necessity to invert
it. Hence, we don’t add any additional complexity to the usual training procedure of NFs.
Notations: We denote the determinant of the Gram matrix of f as gf (x) := | det Jf (x)T Jf (x) |
where Jf (x) is the Jacobian of f. We denote the Lebesque measure in Rn as λn. Random variables
will be denoted with a capital letter, say X, and its corresponding state space with the calligraphical
1
Under review as a conference paper at ICLR 2021
Figure 1: Schematic overview of our method. 1. A density p* (x) with support on a d-dimensional
manifold X (top left) is inflated by adding noise σ2 in the normal space (top right). 2. We have
an NF F-1(χ) learn this inflated density q(X) using a well-known reference measure PU(u). 3.
We deflate the learned density to obtain an estimate p(χ) for p*(x). 4. Our main result provides
sufficient conditions for the manifold X and the choice of noise such that P(X) = p*(x).
version, i.e. X . Small letters correspond to vectors with dimensionality given by context. The letters
d, D, n, and N are always natural numbers.
2	Background and problem statement
An NF transforms a known auxiliary random variable by using bijective mappings parametrized
by Neuronal Networks such that the given data points are samples from this transformed random
variable, see Papamakarios et al. (2019). Formally, an NF is a diffeomorphism Fθ : U → X and
induces a density on X throughpθ(x) = (gF(U)-2PU(U) wherePU(U) is known and U = F-1(x).
The parameters θ are updated such that the KL-divergence between p* (x) andpθ(x),
DKL(P*(x)∣∣Pθ (x)) = Ex 〜p*(χ)[log Pθ (x)]+ const.	(1)
is minimized. If Fθ is expressive enough, it was proven that in the limit of infinitely many samples,
updating θ to minimize this objective function converges to a θ* such that it holds PX -almost surely
P*(x) = Pθ* (x), see (Huang et al., 2018).
More generally, let X ∈ X ⊂ RD be generated by an unobserved random variable Z ∈ Z ⊂ Rd
with density π(z), that is X = f(Z) for some function f : Z → X where typically d < D. In
Gemici et al. (2016), f is an embedding1, and it was shown that one can calculate probabilities
such as PX (A) for measurable A ⊂ X using a density P*(x) with respect to the volume form dVf
induced by f, that is
P(X ∈ A)
/	π(z )dz =	P* (x)dVf (x)
(2)
withp*(x) = π(z)gf (Z)-2 and dVf (x) =，gf (Z)dz where Z = f-1(x). Hence, given an explicit
mapping f and samples from P* (x), we can learn the unknown density π(z) using a usual NF in
Rd. However, in general, the generating function f is either unknown or not an embedding creating
numerical instabilities for training inputs close to singularity points.
In Brehmer & Cranmer (2020), f and the unknown density π are learned simultaneously. The main
idea is to define f as a level set of a usual flow in RD and train it together with the flow in Rd used
to learn π(Z). To evaluate the density, one needs to invert f and thus this approach may be very slow
for high-dimensional data. Besides, to guarantee that f learns the manifold they proposed several
ad hoc training strategies. We tie in with the idea to use an NF for learning P*(x) with unknown f
and study the following problem.
1Thus, a regular continuously differentiable mapping (called immersion) which is, restricted to its image, a
homeomorphism.
2
Under review as a conference paper at ICLR 2021
Problem 1 Let X be a d-dimensional manifold embedded in RD. Let X = f (Z) be a random
variable generated by an embedding f : Rd → RD and a random variable Z 〜π (z ) in Rd . Given
N Samplesfrom p*(x) as described above, find an estimator P ofp* such that in the IimitofinfiniteIy
many samples we have thatp(x) = p*(x), PX — almost surely.
3 Methods
To solve Problem 1, we want to exploit the universality of NFs. We want to inflate X such that the
inflated manifold X becomes diffeomorphic to a set U on which a simple density exists. Doing so,
allows us to learn the inflated density p(X) exactly using a single NF, see Section 2. Then, given such
an estimator for the modified density, We approximate p* (x) and give sufficient conditions when this
approximation is exact.
3.1	The Inflation step
Given a sample x of X , if we add some noise ε ∈ RD to it, the resulting new random variable
X = X + ε has the following density
q(x)
q q(X∣x)dPχ (x).
X
(3)
Denote the tangent space in x as Tx and the normal space as Nx . By definition, Nx is the orthogonal
complement of Tx. Therefore, we can decompose the noise ε into its tangent and normal component,
ε = εt + εn. In the following, we consider noise in the normal space only, i.e. εt = 0, and denote
the density of the resulting random variable as qn(X). The corresponding noise density qn(X∣χ)
has mean X and domain Nχ. We denote the support of qn(∙∣χ) by Nqn(.㈤.The random variable
X = X + εn is now defined on X = Ux∈x Nqn(∙∣ x). We want X to be diffeomorphic to a set U on
which a known density can be defined.
Example 1 (a) Let X = S1 = {x ∈ R2 | ||x|| = 1} be the unit circle. For each x ∈ S1 there
exists Z ∈ [0, 2π) such that X = e『(x) = (cos(z), Sin(Z))T. To sample a point X in Nχ,
which is Spanned by e『(x), we Sample a scalar value Y and set X = X + Yer (x). With Y 〜
Uniform[—1, 1), we have that
Xe = [ {X + Yer(X)|Y ∈ [—1, 1)} = {X ∈ R2 | ||X||2 < 2}	(4)
x∈X
which is the open disk with radius 2. The open disk is diffeomorphic to (0, 1) × (0, 1). Thus,
qn(X) can be learned by a single NF F-1 andPU(u) = Uniform ((0,1) X (0,1)) as reference.
(b) As in (a), we consider the unit circle. Now we set Y to be a χ2 — distribution with support
[—1, ∞). Then
Xe = [ {X + Yer(X)|Y ∈ [—1, ∞)} = R2.	(5)
x∈X
Thus, qn(X) can be learned by a single NF F-1 andPU (u) = N(u; 0, ID) as reference.
Both cases can be analogously extended to higher dimensions.
Remark 1 To be precise, the random variable εn is generated by a random variable in RD-d, say
Γ, with measure Pr. Then, qn(X∣X) is the density of the pushforward of the noise measure Pr with
regard to the mapping h : RD-d → Nx. Hence, formally, the density q(X∣X) is with respect to the
induced volume form dVh, see Section 2. However, ifwe choose an orthonormal basis for Nx, say
n(b) * * * * * * * * * (1),..., n(D-d), then we have that X = h(γ) = AY + X where the columns of A ∈ RD×(D-d)
are given by these basis vectors, i.e. A = [n(1), . . . , n(D-d)]. Thus, the Gram determinant of h
is gh = det(ATA) = 1 and we have that dVh(X) = dγ where dγ denotes the volume form with
respect to λD-d. In this case, we can think of qn(X∣X) as a density with respect to λD-d.
If qn(X∣X) depends only on ∣∣X — x∣∣, as it is for the Gaussian distribution, we have that
qn(X∣X) = qn(∣∣X — X∣∣) = qn(∣∣γ∣∣) because h is an isomorphism. Thus, for this case it holds that
qn(X∣X)dVh (X) = qn(∣∣Y∣∣)dγ. Then, for convenience, we may abuse notation by writing Y 〜q(X∣X)
or εn 〜r(τ) where rg) is the density of Pr with respect to λD-d.
3
Under review as a conference paper at ICLR 2021
3.2	The Deflation step
Our main idea is to find conditions such that
qn(x) = qn(x∣χ)p*(χ)	(6)
for almost surely all X ∈ X and for an almost surely unique X ∈ X. Because then, given an exact
estimator of qn(X), say Gn(X), We have for X = X thatp*(x) = ^n(χ)/qn(χ∖χ).
For equation (6) to be true, we need to guarantee that almost every X corresponds to only one
X ∈ X. This is certainly the case Whenever all the normal spaces have no intersections at all (think
of a simple line in R2). We can relax this assumption by allowing null-set intersections. Moreover,
only those subsets of the normal spaces are of interest which are generated by the specific choice of
noise qn(X∖∕). Thus, only the support of qn(X∣X), Nqn(∙∣χ), matters. The key concept for our main
result is expressed in the following definition:
Definition 1 Let X be a d—dimensional manifold and Nx the normal space in X ∈ X. Let qn(∙∣X)
be a density defined on Nx and denote by Nqn(∙∣x) the domain of qn(∙∣X). Denote the collection of
all such densities as Q := {qn(∙∖X)}x∈x. For X ∈ X, we define the set of all possible generators of
X as A(X) = {x0 ∈ X∖Nqn(∙∣x0) 3 X}. We say X is Q— normally separated if for all X ∈ X holds
that PX ∣χ=x [X ∈ Nx |# A(X) > 1] = 0 where # A(X) is the cardinality of the set A(X). In words,
every X ∈ Nx is PX ∣χ=x-almost surely determined by X.
To familiarize with this concept, consider Figure 2 and the following example:
Example 2 For the circle in example 1, we choose εn to be uniformly distributed on the half-open
interval [—1,1). The point (0, 0)t is contained in Nqn(∙∣x) for all X ∈ X and thus Nqn(∙∖xo) ∩
Nqn(∙∣x) ={(0, 0)t} for all X = x0, see Figure 2 (middle). Hence, for any given X ∈ Nx we have
X Af八	XX	ifX	= (0,0)t, Jlq	∞ Af八 ∞∞	ifX =	(0,0)t,
that A(x) =	<	l '	，	and therefore	#A(x) = ti ɪ (	'
Thus, PX∣χ=x [x ∈ X|#A(X) > l] = PX∣χ=x [X = (0, 0)t] = 0 for all X ∈ X. What follows is
that X is Q—normally seperated.
Ifwe were to choose εn to be uniformly distributed on [—1.5, 1), see Figure 2 (right), the normal
spaces would overlap and we would have that PX∣X=x X ∈ X |#A(X) > l] > 0. In this case, X
would not be Q—normally seperated.
Figure 2: Q-normal separability for different noise distributions qn(X∖χ) used to inflate X = S1
(black line). Left: X is Q-normally separable since every point in the inflated space X (red shaded
area) has a unique generator. Middle: X is Q-normally separable since PX —almost every point
in X has a unique generator. Right: X is not Q-normally separable since every point in the dark
shaded area has two generators.
Theorem 1 Let X be a d—dimensional manifold. For each X ∈ X, let qn(∙∖x) denote a distribution
on the normal space of X. Let X be Q—normally separated where Q := {qn(∙∖x)}x∈x. Assume
ε∏ ~ Uniform[-1.5,1)
-2	-1	0	1	2
4
Under review as a conference paper at ICLR 2021
that we can learn the density qn(X) as defined in equation (3), by using a single NF F-1, thus
qn(x) = (gF(FT(X)))-2PU(FT(X)) for some known density PU. Then, Jor PX — almost all
X ∈ X holds that qn(x) = p*(x)qn(x∣x), therefore this equation, when evaluated at x = x, yields
p*(x)
qn(x)
qn(χ∣χ)
(7)
The proof can be found in Appendix A.1.
3.3 GAUSSIAN NOISE AS NORMAL NOISE AND THE CHOICE OF σ2
Our proposed method depends on three critical points. First, we need to be able to sample in the
normal space of X. Second, we need to determine the magnitude and type of noise. Third, we need
to make sure that the conditions of Theorem 1 are fulfilled. We address (partially) those three points.
1.	For the special case where D d, we show that a full Gaussian noise is an excellent approxi-
mation for a Gaussian noise restricted to the normal space. Consider ε = εt + εn, ε 〜N(0, σ2I0).
Then, the expected absolute squared error when approximating normal noise with full Gaussian
noise is E [∣ε — £口|2] = E [信|2] = dσ2. The expected relative squared error is therefore
d
D — d — 2
∣ε — εn∣2
ɪF -
dσ2E [⅛
E
(8)
because εt and εn are independent and D-d follows a scaled inverse χ2—distribution with D — d
degrees of freedom and scale parameter 1∕σ2. Thus, if D》d, Gaussian noise is an excellent
approximation for a Gaussian in the normal space. We denote the inflated density with Gaussian
noise by qσ2 (X) in the following.
2.	The inflation must not garble the manifold too much. For instance, adding Gaussian noise with
magnitude σ ≥ r to S1 will blur the circle. Since the curvature of the circle is 1/r, intuitively, we
want σ to scale with the second derivative of the generating function f. Additionally, we do not
want to lose the information of p* (x) by inflating the manifold. If the generating distribution π(z)
makes a sharp transition at zo, π(zo — ∆z0)《π(zo + ∆z0) for ∣∆z0∣《1, adding to much noise
in X0 = f(z0) will smooth out that transition. Hence, we want σ to inversely scale with π00 (z).
We formalize these intuitions in Proposition 1 and prove them in Appendix A.2. In accordance
with Theorem 1, We say pσ(X) approximates wellp*(χ) if limσ2→oPσ(χ)∕qn(χ∣χ) = p*(χ) for all
X ∈ X where qn(X|X) is the normalization constant ofa (D—d)—dimensional Gaussian distribution.
Proposition 1 Let X ∈ RD be generated by Z 〜π(z) through an embedding f : Rd → RD,
i.e. f (Z) = X. Let π ∈ C2 (Rd). For qσ2 (X) to approximate well p*(χ), in the sense that
limσ2→o qσ2 (χ)∕qn(χ∣χ) = p*(χ) for X ∈ X, a necessary condition is that:
2
2π(z^ II∏00(Z0) Θ (Jf Jf )-1ll+ W 1	(9)
where ||A||+ = Pid,j=1 Aij for A ∈ Rd × Rd and denotes the elementwise product, and
(π00(zo))ij = ∂z∏∂z) ∣z=zo is the Hessian oftheprior evaluated at zo = f-1(χ).
Intuitively, a second necessary condition is that the noise magnitude should be much smaller than
the radius of the curvature of the manifold which directly depends on the second-order derivatives
of f . This can be illustrated in the following example:
Example 3 For the circle2 in R2 generated by f(z) = (cos(z), sin(z))T and a von Mises distri-
bution π(z) H exp(κ Cos(Z)), we get that σ2 W min Q κ(κ SidjZj-Cos(Z)J /2) where the first
condition comes from Proposition 1 and the second one comes from the curvature argument.
Even though this bound may not be usefull as such in practice when f and π are unknown, it can
still be used if f and π are estimated locally with nearest neighbor statistics.
2Technically, the circle does not fulfill the conditions of Proposition 1 since the domain of f is not R.
5
Under review as a conference paper at ICLR 2021
From a numerical perspective, inflating a manifold using Gaussian noise circumvents degeneracy
problems when training an vanilla NF for low-dimensional manifolds. In particular, the flows Ja-
cobian determinant becomes numerically unstable, see equation (1). This determinant is essentially
a volume changing factor for balls. From a sampling perspective, these volumes can be estimated
with the number of samples falling into the ball divided by the total number of points. Therefore, we
suggest to lower bound σ with the average nearest neighbor obtained from the training set to make
sure that these volumes are not empty and thus avoid numerical instabilities.
3.	Intuitively, if the curvature of the manifold is not too high and if the manifold is not too entangled,
Q-normal separability is satisfied for a sufficiently small magnitude of noise. Also in the manifold
learning literature, the entangling must not be too strong. Informally, the reach number provides a
necessary condition on the manifold such that it is learnable through samples, see Chapter 2.3 in
Berenfeld & Hoffmann (2019). Formally, the reach number is the maximum distance τX such that
for all X in a TX -neighbourhood of X the projection onto X is unique. In Appendix A.3 We prove
Theorem 2 which states that any closed manifold X with τX > 0 is Q-normally separable.
Theorem 2 Let X ⊂ RD be a closed d-dimensional manifold. If X has a positive reach number
TX, then X is Q-normally separable where Q := {qn (∙∣x)}χ∈χ is the collection of uniform dis-
tributions on a ball with radius TX, i.e. qn(X∣x) = Uniform(X; B(x, TX) ∩ Nx) where B(x, TX)
denotes a D-dimensional ball with radius TX and center x.
4	Results
We have three goals in this section: First, We numerically confirm the scaling factor in equation (7).
Second, We verify that Gaussian noise can be used to approximate a Gaussian noise restricted to the
normal space. Third, We numerically test our bound for σ2 3 derived in Section 3.3. Finally, We shoW
that We can learn complicated distributions on S2 Without using explicit charts. For training details,
We refer to Appendix B.1 and B.2, respectively.
4.1	Von Mises on a circle
Let X be a circle with radius 3 and let π(z) 8 exp(8 Cos(Z)) be a 1D von Mises distribution. Given
Z 〜∏(z), we generate a point in X according to the mapping f (Z) = 3(cos(z), sin(z)). We want to
learn the induced density p*(x). Note that 3p*(χ) = ∏(z) since 1/3 is the volume form induced by
f . To benchmark our performance, we use the idea in Gemici et al. (2016) to first embed the circle
into R, using e.g. f-1, learn the density there with an NF, and transform this learned density back to
S1. In Brehmer & Cranmer (2020), this method is named Flow on manifolds (FOM) and we stick to
this notation in the following. Note that f is not injective and to illustrate the benefit of our method
we choose the singularity point to be (1, 0)T. By moving points close to (1, 0)T slightly away from
(1, 0)T, we numerically ensure that f is an embedding.
1.	The Inflation step: We inflate X using 3 types of noise: Gaussian in the normal space (NG),
Gaussian in the full ambient space (FG), and χ2-noise as described in examples 1(b) with scale
parameter 3. Technically, NG violates the Q-normal separability assumption. However, if σ2
is small and the scale parameter for the von Mises distribution is large enough, this is practically
fulfilled.
2. Learning qn(X): For the NFs we use a Block Neural Autoregressive Flow (BNAF) (De Cao et al.,
2019). We use the same NF architecture and training procedures across all models.
3. Deflation: Given an estimator for qn(X), we use equation (7) to calculatep*(x). For FG and NG,
we have that qn(x∣x) = 1/√2πσ2 and for the normal χ2-noise is qn(x∣x) = √3e-3S∕(√8Γ(3)).
In Figure 3, we show the results for σ2 = 0.01 andσ2 = 1. In the respective plot, the first row shows
training samples from the inflated distributions q02 (X) (left), and qn(X) (middle), respectively. We
color code a sample X = X + ε according to p*(x) to illustrate the impact of noise on the inflated
density. Note that the FOM model (top right) does not need any inflation and therefore is trained
on samples from p*(x) only. In the respective plot, the second row shows the learned density for
the different models and compares it to the ground truth von Mises distribution π(Z) depicted in
black. As we can see, for σ2 = 0.01 all models perform very well, although the FOM model
6
Under review as a conference paper at ICLR 2021
slightly fails to capture p(z) for z close to 0 which corresponds to the chosen singularity point. For
σ2 = 1, we see a significant drop in the performance of the Gaussian model. Although the manifold
is significantly disturbed, the normal noise model still learns the density almost perfectly3, so does
the normal χ2-noise model, as predicted by Theorem 1.
normal χ2 - noise
full Gaussian noise
σ2 = 1.0
normal Gaussian noise

彳,
0 s,
Figure 3: Learned densities for σ2
0.01 (above) and σ 2
1 (below), respectively. First row:
Samples used for training the respective model: FG (left), NG (middle), FOM/ χ2 (right). The black
line depicts the manifold X (a circle with radius 3) and the color codes the value of p* (x). Second
row: Colored line: Learned density p(χ) according to equation (7) multiplied by 3. Blackline:
ground truth von Mises distribution.
To measure the dependence of our method on the magnitude of noise, we iterate this experiment
for various values of σ2 and estimate the Kolmogorov-Smirnov (KS) statistics. The KS statistic is
3 Note that our method still depends on how well an NF can learn the inflated distribution.
7
Under review as a conference paper at ICLR 2021
defined as KS = supx∈X |F (x) - G(x)|, where F and G are the cumulative distribution functions
associated with the probability densities p(x) and q(x), respectively. By definition, KS ∈ [0, 1] and
KS = 0 if and only if p(x) = q(x) for almost every x ∈ X . However, equation (6) is only valid
if the conditions of Theorem 1 are fulfilled. There is no reason why using equation (7) for the full
Gaussian noise would lead to a density on the manifold X. The KS statistics is ill-posed in this case.
Nevertheless, we are interested in measuring the sensitivity to the noise, and thus consider the KS
statistics as a relative performance measure.
In Figure 4, we display the KS values depending on different levels of noise, for the NG (blue)
and FG (orange) noise compared with the ground truth von Mises distribution. Also, we embed the
circle into higher dimensions D = 5, 10, 15, 20 and repeat this experiment. The result for D = 2
and D = 20 are shown in the first row (left and right).4 For D = 2, we add the FOM model (which is
independent of σ2) horizontally. Besides, we depict the lower and upper bound for σ2 from Chapter
3.3 with dashed vertical lines. In the lower-left image, we show the optimal KS values obtained
for both models depending on D. The lower-right image shows the corresponding σ2 for those
optimal KS. In bright, the optimal average σ2 is shown whereas the dark regions are the minimum
respectively maximum values for σ2 such that we outperformed the FOM benchmark. We note that
for both cases, the averaged optimal σ2 is within the predicted bounds for σ2 .
Figure 4: KS values for the NG- (blue) and FG-noise method (orange) depending on σ2 ∈
[10-9, 10] and the embedding dimension D = 5, 10, 15, 20 in log-scale. For D = 2 and D = 20
(top right), the two vertical lines represent the lower and upper bound for σ2 estimated according to
Chapter 3.3 with 10K samples. We plot horizontally the KS value obtained from FOM. Bottom left:
Optimal KS values depending on D. Bottom right: Optimal averaged σ2 such that optimal KS is
obtained (bright). The maximum and minimum σ2 such that the FOM benchmark is outperformed
(dark). The dashed horizontal lines are again the theoretical bounds. We used 10 seeds for the error
bars and plot in log-scale.
Several aspects are remarkable. The flat course of the KS vs. σ2 plot is an indicator that the method
is not very sensitive to noise and this does not change with the dimensionality of the embedding
space. Also, the optimal KS values do not change much depending on D and the NS and FG model
approach each other, as predicted.
Interestingly, the onset for the increase in the KS value for the NS-noise is roughly 3 which is
the radius of the circle. For increasing σ2, X resembles more and more a double cone which is not
4Note that the scaling factor depends on D, qn(x∣x) = 1∕(2πσ2) D-
8
Under review as a conference paper at ICLR 2021
diffeomorphic to R2 and thus the NF used to train the inflated distribution may not be able to capture
the density close to the circle’s center correctly.
4.2	MIXTURES ON S2
We show that we can learn a complicated distribution, a mixture of 2-dimensional von Mises dis-
tributions on a sphere with radius 1, without using any knowledge about the manifold except for its
intrinsic dimension. For certain magnitudes of σ2, we obtain similar estimates as the FOM bench-
mark as we can see in the direct comparison of the learned densities, see Figure 5 (top right), and
the KS statistics (bottom right). As for the circle, the Gaussian restricted in the normal space allows
for a greater range of noise magnitude without sacrificing the quality of the estimate.
Target density
FoM
full GaUSSian σ2 = 0.01
3.0
[3.0
2.5
2.0
1.5
1.0
0.5
1.0
0.5
2
2
4	6
4	6

O
2
3
Zl
4	5
O
O
O
O
O
2
2
5
0
Figure 5: Left: Target density. Upper right: Learned densities using FOM and our method with
Gaussian noise and σ2 = 0.01. Lower right: KS vs. σ2 plot of the Gaussian noise model (full and
in normal space) compared to the FOM with the theoretical bounds from Chapter 3.3 for σ2 depicted
in vertical dashed lines (with 10K samples used to approximate these bounds).
5	Discussion
To overcome the limitations of NFs to learn a density p* (x) defined on a low-dimensional manifold,
we proposed to embed the manifold into the ambient space such that it becomes diffeomorphic
to RD , learn this inflated density using an NF, and, finally, deflate the inflated density according
to Theorem 1. There, we provided sufficient conditions on the choice of inflation such that we
can compute p*(x) exactly. Notably, We don,t need to assume that p*(x) is supported on a flat
manifold. Our method depends on some critical points which we addressed in Section 3.3. So far,
the magnitude of noise σ2 when using NFs on real-world data is somewhat chosen arbitrarily. As
a first step to overcome this arbitrariness, we derived an upper bound for σ2 in Proposition 1 and
established an interesting connection to the manifold learning literature in Theorem 2.
We hope that our theoretical results motivate some new research directions. Using full Gaussian
noise to learn the inflated distribution smears information on p* (x), in particular if p* (x) has many
local extrema. This loss of information may be especially impactful in out of distribution (OOD)
detection or when it comes to adversarial robustness. Therefore, developing methods which allow to
generate noise in the manifold’s normal space could improve the performance of NFs on such tasks.
Another interesting direction is to exploit the product form of equation (6) and learn low-dimensional
representations by forcing the NF tobe noise insensitive in the first d-components and noise sensitive
in the remaining ones. Inverting the corresponding flow allows to sample directly from the manifold
which has the potential to improve the generative ability of NFs.
9
Under review as a conference paper at ICLR 2021
References
Clement Berenfeld and Marc Hoffmann. Density estimation on an unknown submanifold. arXiv
preprint arXiv:1910.08477, 2019.
Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estima-
tion. arXiv preprint arXiv:2003.13913, 2020.
Rob Cornish, Anthony L Caterini, George Deligiannidis, and Arnaud Doucet. Relaxing bijectivity
constraints with continuously indexed normalising flows. arXiv preprint arXiv:1909.13833, 2019.
Nicola De Cao, Ivan Titov, and Wilker Aziz. Block neural autoregressive flow. arXiv preprint
arXiv:1904.04676, 2019.
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32, pp. 3140-3150. Curran Associates, Inc., 2019. URL http:
//papers.nips.cc/paper/8577- augmented- neural- odes.pdf.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In
Advances in Neural Information Processing Systems, pp. 7511-7522, 2019.
Mevlana C Gemici, Danilo Rezende, and Shakir Mohamed. Normalizing flows on riemannian man-
ifolds. arXiv preprint arXiv:1611.02304, 2016.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregres-
sive flows. volume 80 of Proceedings of Machine Learning Research, pp. 2078-2087, Stock-
holmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://Proceedings.
mlr.press/v80/huang18d.html.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019.
Danilo Jimenez Rezende, George Papamakarios, SebaStien RaCaniere, Michael S Albergo, Gurtej
Kanwar, Phiala E Shanahan, and Kyle Cranmer. Normalizing flows on tori and spheres. arXiv
preprint arXiv:2002.02428, 2020.
A Appendix
A.1 Proof of Theorem 1
We denote the probability measure of the random variable X as PX and it is defined on (X , B(X))
where B(X) is the set of borel measure in RD intersected with X. For a realisation ofX, say x, we
denote the probability measure of the shifted random variable X + εn as PX∣χ=χ and it is defined on
(Nx, B(Nx)). We extend both measures to (RD, B(RD)) by setting the probabilities to 0 whenever
a set A ∈ B(RD ) has no intersection with X or Nx, respectively. For instance, that means for
X ∈ Nx that
P[x + εn ∈ (X, X + dX)] =P[x + εn ∈ (X, X + dX) ∩ Nx] = PX∣χ=x[(X, X + dX) ∩ Nx]	(10)
where (X, X + dX) denotes an infinitesimal volume element around X.
The mapping (X, ) 7→ X + is B(RD) × B(RD )-measurable because RD is a topological vector
space and B(RD) the Borel σ-algebra. What follows is that X = X + εn is a random variable on
(RD, B(RD)) and has the pushforward of P(X,εn) with regard to the mapping (X, εn) → X + εn as
probability measure where P(X,εn) is the joint measure of X and εn. Thus, for A ∈ B(X), we have
that
PX(A)= P(χ,εn) ({(x, C) ∈ RD X RD|x + C ∈ A}) .	(11)
10
Under review as a conference paper at ICLR 2021
Now let X ∈ Nx for an X ∈ X. Since X is Q-normally separated, PX-almost all X are uniquely
determined by (x, E) such that X = X + e. Therefore, we have for PX-almost all X = X + E that
PX((X,X + dX) ∩X) = P(x,εn) ({(x,e) ∈ RD × RD|x + E ∈ (X,X + dX) ∩ X})
=P (X + εn ∈ (X,X + dX) ∩ X)
=P (X ∈ (x, X + dX) ∩ X) ∙ P (x + εn ∈ (X, X + dX) ∩ Nx)
=PX ((x, X + dx) ∩X) ∙ PX ∣χ=x ((X, X + dX) ∩ Nx)	(12)
where for the first equality we used equation (11) and for the third the fact that X and E are almost
surely uniquely determined by X.
Both probability measures on the right-hand side have a density. For PX with respect to dVf, see
Section 2, this density is p*(x). Similarly, since Nx is a linear subspace of RD, qn(X∣x) is the
density of PX∣x=x with respect to the volume form dVh where h is the mapping from RD-d to Nx,
see Remark 1.
Then, the corresponding density of Pχ is with respect to the product measure dVh ∙ dVf. However,
this product measure is equivalent to λD when restricted to subsets of X and thus qn is the density
of PX with respect to λD .5 Therefore, we can write equation (12) in terms of densities as follows:
qn(X) = p*(X)qn(X∣X)
and it holds that
In qn(X)dX = I / p* (X)qn(X∣X)dVh(X)dVf (x)
Xe	X Nx
=	p* (X)dVf (X)
X
=1,
(13)
(14)
as needed for a density on X. By setting X to X in equation (13), we obtain equation (7). This ends
the proof.
Remark 2 Note that in Theorem 1, we need that X is diffeomorPhic to RD. This requires that
the noise distribution qn(∙∣χ) is continuous for all X.
A.2 Proof of Proposition 1
The generating function f is an embedding for X and X = f(Z) has the density p*(X) for X ∈ X.
We may extend the domain of p* (X) to include all points X ∈ RD using the Dirac-delta function as
follows
p* (X) =	δ(X - f (z))π(z)dz
Z
After inflating X , we have that
内(X)=L
N(X; f (z), Σ)π(z)dz
(15)
(16)
with covariance matrix Σ ∈ RD×D. Assume X = X for some X ∈ X. We Taylor expand f (x)
around z0 = f-1 (X) up to first order,
f(z) ≈ f(z0) + Jf (z0)(z - z0),	(17)
and π(z ) up to second order,
∏(z) ≈ n(zo) + ∏(zo)0(z - Zo) + 1(z - zo)τ∏00(zo)(z - zo).	(18)
5This is because the column vectors of Jf and Jh form a basis of RD.
11
Under review as a conference paper at ICLR 2021
where π(z0)0 denotes the gradient and π00(z0) the Hessian of π evaluated at z0, thus π(z0)0 ∈ Rd
and π00(z0) ∈ Rd×d. Then, we can approximate pΣ(x) as follows:
1
pΣ (x)≈
√(2π)D det(Σ)
/ exp(-2(Z — zO)T Jf ς-1 Jf(Z 一 zo))∙
• (∏(z0) + ∏(zo)0T (Z — Zo) + 1(z — zo)T ∏00(zo)(z — zo))dz.
(19)
Now define Σ-1
pΣ (x)≈
JfTΣ-1Jf. Then,
Jdet(∑)
√(2π)D-d det(Σ)
J(2π)d det(Σ)
exp(-2(Z — ZO)Tς-I(Z - zo))∙
Z
1
• (π(zo) + π(zo)0T(Z — zo) + 2(z - Zo)Tπ00(zo)(z 一 zo))dz.
Thus, we can exploit this Gaussian in Z -space and get
Tdet^
p∑(x) ≈ ,	=
PdetW
「Jdet(∑)
=Pdet(W
(π(zo) + 2E [(z - ZO)Tn00(zo)(z — Zo)])
(∏(Zo) + 1 IIn00(Zo) © ∑Il + ),
(20)
(21)
where © stands for the elementwise multiplication and IIAII+ = Pidj=1 Aij for a Rd × Rd matrix
A.	,
For the special case where Σ = σ2ID, we can simplify this expression by exploiting that
Jdet(Σ)	_	1	σ-D
P(2π)D-ddet(Σ) = (2π)D- σ-dPg
_	1
(2πσ2)D^ pgf
Thus, in total, we get for this special choice of Σ
(22)
Pσ (X) ≈ ——八 L . J (n(zθ) + V ||n00(ZO) © (Jf Jf )-1k)
(2πσ2) 2 Mgf	2
= L L∏(zo)(1 + IIn00(Zo) © (JTJf)-1∣∣+)	(23)
(2πσ2)ɪ Vzgf	2n(ZO)	j
We assume now	2
2∏σ^) IIn00(Zo) © (JTJf)-1II+ W 1.	(24)
Note that 1∕(2nσ2)D- from equation (23) is exactly the normalization constant obtained when
inflating the manifold with Gaussian noise in the normal space, qn(x1x) = 1∕(2nσ2)--^. What
follows is that limσ2→oPσ(x)/qn(x1x) = p*(x) as We wanted to show.
A.3 Proof of Theorem 2
The theorem follows directly from the definition of the reach number τX of X . It is defined as
the supremum of all r ≥ 0 such that the orthogonal projection prX on X is well-defined on the
r -neighbourhood Xr of X,
Xr := {X ∈ RD I dist(X, X) ≤ r}	(25)
12
Under review as a conference paper at ICLR 2021
where dist(X, X) denotes the distance of X to X. Thus,
TX = sup {r ≥ 0 | ∀X ∈ RD, dist(X, X) ≤ r =⇒ ∃!x ∈ X s.t. dist(X, X) = ||X 一 x||} , (26)
see Definition 2.1. in Berenfeld & Hoffmann (2019). By assumption TX > 0. Thus for all X ∈ XTX
we have that X := PrX(X) is unique. Since X is a manifold, it must hold that X ∈ Nx where Nx
denotes the normal space in X. Let the noise generating distributions be a uniform distribution on
the ball with radius TX, thus
qn(X∣X) = Uniform(X; B(x, τχ) ∩ Nx),	(27)
where B(x, TX) denotes a D-dimensional ball with radius TX and center x.. Then, We have for
X = Ux∈X NqnSx) that
Xe = XτX .	(28)
Thus, X is Q-normally separable where Q := {qn(∙∣X)}x∈X.
B	Experiments
B.1	Technical Details for circle experiments
For the Normalizing Flow T-1(X) we use a BNAF (Block Neural Autoregressive Flow) for the circle
experiment. The number of hidden dimensions was adapted to the dimensionality of the data and
the difficulty of the target density. These details are reported in the corresponding tabular. For the
optimization scheme, we used Adam optimizer with an initial learning rate 0.1, a learning rate decay
of 0.5 after 2000 optimization steps without improvement (learning rate patience). The batch size
was set to 200. The total number of iterations (one iteration corresponds to updating the parameters
using one batch sample) used is also reported in the tabular. No hyperparameter fine-tuning was
done.
For the FOM and χ2-noise models, we use the same architecture as for the D = 2 case.
Data dimension	hidden layers	hidden dimension	total parameters	iterations
2	3	100	31,204	70000
5	3	250	192,010	^^70000
10	3	500	764,000	^^70000
15	3	750	1,716,030	100000
20	3	1000	3,048,040	100000
Table 1: BNAF details for circle experiments.
B.2	TECHNICAL DETAILS FOR MIXTURE OF VON MISE DISTRIBUTIONS ON S2
For the Normalizing Flow T-1 (X) we use rational-quadratic neural spline flows, alternating cou-
pling layers and random feature permutations, see Durkan et al. (2019). For the optimization
scheme, we used AdamW optimizer with an initial learning rate 0.0003, a learning rate cosine decay
to 0 after every 2000 optimization steps, see Loshchilov & Hutter (2016), a weight decay of 0.0001
and a dropout probability of 0. The batch size was set to 200. No hyperparameter fine-tuning was
done. See table 2 for more details. We use the same architecture for the FOM model and 3 seeds for
the error bars.
I Coupling residual blocks hidden features bins SPIine range total ParameterS	iterations]
I 10	3	50	8	3	171,845	50000 ∣
Table 2:	BNAF details for a mixture of von Mises on S2 .
13
Under review as a conference paper at ICLR 2021
The target distribution is amixture of four VonMises distributions, pl (φι, Θ1),P2(Φ2,Θ2),P3(Φ3, θ3)
and p4(φ4, θ4). Each of those distributions has the same product form
*(人 A、	exp(κcos(θi - μi)) exp(κcos(2(φi - mi)))
pi(φi,θi) = 2∏I0(K)----------------------W-------------.	(29)
We set K = 6.	However, they differ in their mean values	μi	and m% see table 3.	We used	3 different
seeds in total to obtain the confidence intervals.
i	μi	mi
∏	π	π 1	2	莅 4	4π	3π 2	^3^	ɪ π	π	3π 3	2	„4 4	4	4π
Table 3:	Mean values for the mixture of von Mises distributions.
14