Under review as a conference paper at ICLR 2021
Decentralized SGD with Asynchronous,
Local, and Quantized Updates
Ab stract
The ability to scale distributed optimization to large node counts has been one of
the main enablers of recent progress in machine learning. To this end, several
techniques have been explored, such as asynchronous, decentralized, or quantized
CommUnication-Which significantly reduce the cost of synchronization, and the
ability for nodes to perform several local model updates before CommUniCating-
Which reduces the frequency of synchronization.
In this paper, We shoW that these techniques, Which have so far been considered
independently, can be jointly leveraged to minimize distribution cost for train-
ing neural netWork models via stochastic gradient descent (SGD). We consider a
setting With minimal coordination: We have a large number of nodes on a com-
munication graph, each With a local subset of data, performing independent SGD
updates onto their local models. After some number of local updates, each node
chooses an interaction partner uniformly at random from its neighbors, and aver-
ages a possibly quantized version of its local model With the neighbor’s model.
Our first contribution is in proving that, even under such a relaxed setting, SGD
can still be guaranteed to converge under standard assumptions. The proof is based
on a neW connection With parallel load-balancing processes, and improves exist-
ing techniques by jointly handling decentralization, asynchrony, quantization, and
local updates, and by bounding their impact. On the practical side, We imple-
ment variants of our algorithm and deploy them onto distributed environments,
and shoW that they can successfully converge and scale for large-scale image clas-
sification and translation tasks, matching or even slightly improving the accuracy
of previous methods.
1	Introduction
Several techniques have been recently explored for scaling the distributed training of machine learn-
ing models, such as communication-reduction, asynchronous updates, or decentralized execution.
For background, consider the classical data-parallel distribution strategy for SGD (Bottou, 2010),
With the goal of solving a standard empirical risk minimization problem. Specifically, We have a set
of samples S, and Wish to minimize the d-dimensional function f : Rd → R, Which is the average of
losses over samples from S, by finding x? = argmin X Ps∈s's (x)/ |S|. We have n compute nodes
Which can process samples in parallel. In data-parallel SGD, each node computes the gradient for
one sample, folloWed by a gradient exchange. Globally, this leads to the iteration:
n
χt+ι = Xt -nt£gi(Xt),
i=1
where Xt is the value of the global parameter, initially 0d, ηt is the learning rate, and gi (Xt) is the
stochastic gradient With respect to the parameter xt, computed by node i at time t.
When executing this procedure at large scale, two major bottlenecks are communication, that is, the
number of bits transmitted by each node, and synchronization, i.e., the fact that nodes need to wait for
each other in order to progress to the next iteration. Specifically, to maintain a consistent view of the
parameter Xt above, the nodes need to broadcast and receive all gradients, and need to synchronize
globally at the end of every iteration. Significant work has been dedicated to removing these two
barriers. In particular, there has been progress on communication-reduced variants of SGD, which
propose various gradient compression schemes (Seide et al., 2014; Strom, 2015; Alistarh et al.,
2017; Wen et al., 2017; Aji and Heafield, 2017; Dryden et al., 2016; Grubic et al., 2018; Davies
et al., 2020), asynchronous variants, which relax the strict iteration-by-iteration synchronization
(Recht et al., 2011; Sa et al., 2015; Duchi et al., 2015), as well as large-batch or periodic model
averaging methods, which aim to reduce the frequency of communication (Goyal et al., 2017; You
1
Under review as a conference paper at ICLR 2021
et al., 2017) and (Chen and Huo, 2016; Stich, 2018), or decentralized variants, which allow each
node to maintain its own, possibly inconsistent, model variant (Lian et al., 2017; Tang et al., 2018;
Koloskova et al., 2019). (We refer the reader to the recent surveys of (Ben-Nun and Hoefler, 2019;
Liu and Zhang, 2020) for a detailed discussion.) Using such techniques, it is possible to scale SGD,
even for complex objectives such as the training of deep neural networks. However, for modern
large-scale models, the communication and synchronization requirements of these parallel variants
of SGD can still be burdensome.
Contribution. In this paper, we take a further step towards removing these scalability barriers,
showing that all the previous scaling techniques—decentralization, quantization, asynchrony, and
local steps—can in fact be used in conjunction. We consider a highly decoupled setting with n
compute agents, located at vertices of a connected communication graph, each of which can ex-
ecute sequential SGD on its own local model, based on a fraction of the data. Periodically, after
some number of local optimization steps, a node can initiate a pairwise interaction with a uniform
random neighbor. Our main finding is that this procedure can converge even though the nodes can
take several local steps between interactions, may perform asynchronous communication, reading
stale versions of each others’ models, and may compress data transmission through quantization.
However, both in theory and practice, we observe trade-offs between convergence rate and degree
of synchronization, in that the algorithm may need to perform additional gradient steps in order to
attain a good solution, relative to the sequential baseline.
Our algorithm, called SwarmSGD, is decentralized in sense that each node maintains local version
of the model, and two interacting nodes only see each others’ models. We further allow that the data
distribution at the nodes may not be i.i.d. Specifically, each node i is assigned a set of samples Si,
and maintains its own parameter estimate xi . Each node i performs local SGD steps on its model
xi based on its local data, and then picks a neighbor uniformly at random to share information with,
by averaging of the two models. (To streamline the exposition, we ignore quantization and model
staleness unless otherwise specified.) Effectively, if node i interacts with node j, node i’s updated
model becomes
i __ -L 2
i	xt,Hi + xt,Hj
xt+ι 弋	2
(1)
where t is the total number of interactions performed by all nodes up to this point, j is the interaction
partner of i at step t + 1, and the input models xit,H and xtj,H have been obtained by iterating the
SGD step Hi and Hj times, respectively, locally from the previous interaction of either node. We
assume that Hi and Hj are random variables with mean H, that is, each node performs H local
steps in expectation between two communication steps. The update for node j is symmetric, so
that the two models match after the averaging step. In this paper, we analyze variants of the above
SwarmSGD protocol.
The main intuition behind the algorithm is that the independent SGD steps will allow nodes to
explore local improvements to the objective function on their subset of the data, while the averaging
steps provide a decentralized way for the models to converge jointly, albeit in a loosely coupled
way. We show that, as long as the maximum number of local steps is bounded, this procedure still
converges, in the sense that gradients calculated at the average over all models are vanishing as we
increase the number of interactions.
Specifically, assuming that the n nodes each take a constant number of local SGD steps on average
before communicating, We show that SwarmSGD has Θ(√n) speedup to convergence in the non-
convex case. This matches results from previous work which considered decentralized dynamics but
which synchronized upon every SGD step, e.g. (Lian et al., 2017; 2018). Our analysis also extends
to arbitrary regular graph topologies, non-blocking (delayed) averaging of iterates, and quantization.
Generally, we show that the impact of decentralization, asynchrony, quantization, and local updates
can be asymptotically negligible in reasonable parameter regimes.
On the practical side, we show that this algorithm can be mapped to a distributed system setting,
where agents correspond to compute nodes, connected by a dense communication topology. Specif-
ically, we apply SwarmSGD to train deep neural networks on image classification and machine
translation (NMT) tasks, deployed on the Piz Daint supercomputer (Piz, 2019). Experiments con-
firm the intuition that the average synchronization cost of SwarmSGD per iteration is low: it stays
around 10% or less of the batch computation time, and remains constant as we increase the number
2
Under review as a conference paper at ICLR 2021
of nodes. For example, using SwarmSGD deployed on 16 nodes, we are able to train a Transformer-
XL (Vaswani et al., 2017) model on WMT17 (En-Ge) 1.5× faster than a highly-optimized large-
batch SGD baseline, and to slightly higher accuracy, without additional hyper-parameter tuning.
At the same time, our method appears to be faster and more accurate than the previous practical
decentralized methods, e.g. (Lian et al., 2017; 2018; Assran et al., 2018), in the same setting.
Importantly, we also note a negative result: in less overparametrized settings such as training residual
CNNs (He et al., 2016) on ImageNet (Russakovsky et al., 2015), nodes do need to perform more
iterations over the dataset relative to the baseline in order to recover full accuracy. This is predicted
by the analysis, and confirms similar findings in previous work (Assran et al., 2018). Overall,
however, our family of methods should be well-suited to training very large modern models in
large-scale settings, where global synchronization among all nodes is prohibitively expensive.
Related Work. The study of decentralized optimization algorithms dates back to Tsitsiklis (1984),
and is related to the study of gossip algorithms for information dissemination (Kempe et al., 2003;
Xiao and Boyd, 2004; Boyd et al., 2006). Gossip is usually studied in one of two models (Boyd et al.,
2006): synchronous, structured in global rounds, where each node interacts with a randomly chosen
neighbor, and asynchronous, where each node wakes up at times given by a local Poisson clock,
and picks a random neighbor to interact with. The model we consider can be seen as equivalent to
the asynchronous gossip model. The key differences between our work and averaging in the gossip
model, e.g. Boyd et al. (2006), are that that 1) we consider local SGD steps, which would not make
sense in the case of averaging fixed initial values; and 2) the gossip input model is static (node inputs
are fixed, and node estimates must converge to the true mean), whereas we study a dynamic setting,
where models are continually updated via SGD. Several optimization algorithms have been analyzed
in this setting (Nedic and Ozdaglar, 2009; Johansson et al., 2009; Shamir and Srebro, 2014), while
Tang et al. (2018); Koloskova et al. (2019) analyze quantization in the synchronous gossip model.
Lian et al. (2017; 2018) and Assran et al. (2018) considered SGD-type algorithms in gossip-like
models. Specifically, they analyze the SGD averaging dynamic in the non-convex setting but do not
allow nodes to perform local updates or quantize. In particular, nodes perform pairwise averaging
upon every SGD step. Table 2 in the Appendix provides a thorough comparison of assumptions,
results, and rates. Their results are phrased in the synchronous gossip model, in which nodes interact
in a sequence of perfect matchings, for which they provide O(1∕√Tn) convergence rates under
analytical assumptions. Lian et al. (2018) extends these results to a variant of the gossip model
where updates can be performed based on stale information, similarly to our non-blocking extension.
Upon careful examination, one can find that their results can be extended to the asynchronous gossip
setting we consider, as long as nodes are not allowed to perform local SGD updates to their models
(corresponding to H = 1) orto quantize communication. Extending the analysis of distributed SGD
to allow for local steps is challenging even in centralized models, see for instance Stich (2018). If
we assume H = 1, our technique yields similar or better bounds relative to previous work in the
decentralized model, as our potential analysis is specifically-tailored to this dynamic interaction
model. For instance, for Assran et al. (2018), the speedup with respect to the number of nodes
depends on a parameter C, which in turn, depends on 1) the dimension d of the objective function,
2) the number of iterations for the graph given by edge sets of all matrices used in averaging to be
connected, and the 3) diameter of the aforementioned connected graph. In the dynamic interaction
model we consider, the parameter C will be at least linear in the number of nodes n, which will
eliminate any speedup. We present a systematic comparison in Appendix B.
In sum, relative to prior work on decentralized algorithms, our contributions are as follows. We are
the first to consider the impact of local updates, asynchrony, and quantization in conjunction with
decentralized SGD. We show that the cost for the linear reduction in communication in H given by
local steps is at worst a squared variance increase in the parameter H . Our analysis technique relies
on a fine-grained analysis of individual interactions, which is different than that of previous work,
and can yield improved bounds even in the case where H = 1. By leveraging the lattice-based
quantization scheme of Davies et al. (2020), we also allow for communication-compression. From
the implementation perspective, the performance of our algorithm is superior to that of previous
methods, notably D-PSGD (Lian et al., 2017), AD-PSGD (Lian et al., 2018) and SGP (Assran et al.,
2018), mainly due to the ability to take local steps.
Wang and Joshi (2018) and Koloskova et al. (2020) provide analysis frameworks for the syn-
chronous version of decentralized SGD with local updates, and possibly changing topologies. This
is a different setting from ours, since it requires each agent to take an equal number of gradient
3
Under review as a conference paper at ICLR 2021
steps before every interaction round, and therefore does not allow for agents to progress at different
speeds (asynchrony). Further, we support quantization, and validate our analysis at scale.
2	Preliminaries
The Distributed System Model. We consider a model which consists of n ≥ 2 anonymous agents,
or nodes, each of which is able to perform local computation. We assume that communication
network of nodes is a r-regular graph G with spectral gap λ2 , which denotes the second smallest
eigenvalue of the Laplacian of G. This choice of communication topology models supercomputing
and cloud networks, which tend to be regular, densely connected and low-diameter, mimicking
regular expanders (Kim et al., 2008; Besta and Hoefler, 2014).
The execution proceeds in discrete steps, where in each step we sample an edge of the graph G uni-
formly at random and we allow the agents corresponding to the edge endpoints interact. Each of the
two chosen agents updates its state according to a state update function, specified by the algorithm.
The basic unit of time is a single pairwise interaction between two nodes. Notice however that in a
real system Θ(n) of these interactions could occur in parallel. Thus, a standard global measure is
parallel time, defined as the total number of interactions divided by n, the number of nodes. Parallel
time intuitively corresponds to the average number of interactions per node to convergence. We
note that our model is virtually identical to the population model of distributed computing (Angluin
et al., 2006), or to asynchronous gossip models (Xiao and Boyd, 2004).
Stochastic Optimization. We assume that the agents wish to minimize a d-dimensional, differen-
tiable function f : Rd → R. Specifically, we will assume the empirical risk minimization setting,
in which agents are given access to a set of m data samples S = {s1, . . . , sm} coming from some
underlying distribution D, and to functions `i : Rd → R which encode the loss of the argument at
the sample s” The goal of the agents is to converge on a model x* which minimizes the empirical
loss over the m samples, that is
m
x* = argminχf(x) = argminχ(1/m) £'i(x).	(2)
i=1
In this paper, we assume that the agents employ these samples to run a decentralized variant of
SGD, described in detail in the next section. For this, we will assume that each agent i has access to
stochastic gradients gei of the function f, which are functions such that
E[ei(X)] = Vf (χ).	(3)
Stochastic gradients can be computed by each agent by sampling i.i.d. the distribution D, and
computing the gradient of f at θ with respect to that sample. (Our analysis can be extended to the
case where each agent is sampling from its own partition of data, see Section H in the Appendix.) We
will assume a the following conditions about the objective function (One of the extensions removes
the second moment bound):
1.	Smooth Gradients: The gradient Vf (x) is L-Lipschitz continuous for some L > 0, i.e. for all
x, y ∈ Rd :
kVf(x)-Vf(y)k ≤Lkx-yk.	(4)
2.	Bounded Second Moment: The second moment of the stochastic gradients is bounded by some
M 2 > 0, i.e. for all x ∈ Rd and agent i:
Egei (x) 2 ≤M2.	(5)
Note that throughout this paper for any random variable X, by EkX k2 we mean E[kX k2].
3	The SwarmSGD Algorithm
Algorithm Description. We now describe a decentralized variant of SGD, designed to be executed
by a population of n nodes, interacting over the edges of r-regular graph G. We assume that each
node i has access to local stochastic gradients gei, and maintains a model estimate Xi. For simplicity,
we will assume that this initial model is 0d at each agent, although its value may be arbitrary. Each
4
Under review as a conference paper at ICLR 2021
agent performs SGD steps on its local estimate Xi . At random times given by a clock of Poisson
rate, we pick two neighboring agents i and j uniformly at random from G, and have them average
their estimates. The interaction is precisely described in Algorithm 1.
For simplicity, the pseudocode is sequential, although in practice nodes perform their local SGD
steps in parallel. Also, we have assumed a constant learning rate; we will detail the update procedure
in the next section, as well as more complex variants of this basic update.
Algorithm 1 Sequential SwarmSGD pseudocode for each interaction between nodes i and j.
% Let G be r-regular graph.
% Sample an edge (i, j) of G uniformly at random.
Require: agents i and j chosen for interaction
% choose Hi and Hj
% agent i performs Hi local SGD steps
for q = 1 to Hi do
Xi — Xi - η7i(Xi)
end for
% agent j performs Hj local SGD steps
for q = 1 to Hj do
Xj — Xj - ηgj (Xj)
end for
% agents average their estimates coordinate-wise
avg J (Xi + Xj )/2
Xi J Xj J avg
4 The Convergence of SwarmSGD
We begin by analyzing the convergence of the baseline SwarmSGD algorithm. Fix an integer H ≥ 1.
First, we will consider a variant where Hi and Hj are independent, geometrically-distributed random
variables, with mean H . This corresponds to interaction times being chosen by a Poisson clock of
constant rate. To handle the fact that the number of local steps upon an interaction is a random
variable, in this first case we will require stochastic gradients to satisfy the bounded second moment
assumption, specified above. Intuitively, this is required since otherwise the “distance travelled” by
a node could be virtually unbounded. In this setting, we prove the following:
Theorem 4.1.	Let f be an non-convex, L-smooth function, whose stochastic gradients satisfy the
bounded second moment assumption above. Let the number of local stochastic gradient steps per-
formed by each agent upon interaction be a geometric random variable with mean H. Let the
learning rate we USe be η = n∕√T. Define μt = Pn=ι Xi/n, where Xi is a value of model i after
t interactions, be the average of the local parameters.
number of interactions T ≥ n4 :
Then, for learning rate η = n∕√T and any
1_ X ElNf(U )k2 ≤ 4(f (μo)- f(x*)) + 2304H2max(1,L2)M2
T =	"t 川-	√TH	√T
+1
Discussion. First, we note that this notion of convergence is standard in the non-convex case,
e.g. (Lian et al., 2015; 2017; 2018), and that each of the upper bound terms has an intuitive in-
terpretation: the first represents the reduction in loss relative to the initialization, and gets divided by
the number of local steps H, since progress is made in this term in every local step; the second rep-
resents the influence of the variance of each individual local step multiplied by a term which bounds
the impact of the graph topology on the convergence. In particular, this term negatively impacts
convergence for large values ofH, L, and M, but gets dampened if the graph is well-connected (i.e.
large λ2). For example, in the case of the complete graph, we have λ2 = n.
Second, let us consider the algorithm’s communication complexity, which we measure in terms of
the total number of communication steps. We notice an interesting trade-off between the linear
reduction in H in the first term of the bound, showing that the algorithm takes advantage of the local
gradient steps, and the quadratic increase in the second variance term also due to H, in the second
term. Hence, the end-to-end speedup of our algorithm versus the variant with H = 1 will depend
on the relationship between these two terms, which depends on the parameter values.
5
Under review as a conference paper at ICLR 2021
Third, importantly, the time T in this bound counts the total number of interactions. However, in
practice Θ(n) pairwise interactions will occur in parallel, as they are independent. Therefore, we can
replace T by nT in the above formula, to estimate the speedup in terms of wall-clock time, obtaining
a speedup of Θ(√n). At the same time, notice that this speedup is dampened in the second term by
the non-trivial additional variance due to noisy local gradient steps, a fact which we will revisit in
the experimental section.
Fourth, although the requirement T ≥ n4 appears restrictive, some non-trivial dependency between
n and T is necessary, as gradient information has to “mix” well in the graph before global optimiza-
tion can occur. Previous work requires stronger variants of this restriction: specifically, Lian et al.
(2018) require T ≥ n6, while Assran et al. (2018) requires T = Ω(nd2).
Proof Overview. At a high level, the argument rests on two technical ideas. The first idea is to show
that, due to the pairwise averaging process, and in spite of the local steps, the nodes’ parameters
will have to remain concentrated around their mean μt. The second is to show that, even though
stochastic gradients are taken at perturbed, noisy estimates of this mean, the impact of this noise on
convergence can be bounded.
In particular, the main technical difficulty in the proof is to correctly “encode” the fact that parame-
ters are well concentrated around the mean. For this, we define the potential Γt, which denotes the
variance of models after t interactions. Formally,
n
Γt = EkXi -μtk2,
(6)
i=1
where μt = £乙 Xi/n. We bound the expected evolution of Γt in terms of r, the degree of nodes
in the interaction graph G, and λ2, the second smallest eigenvalue of the Laplacian of G. For both
algorithm variants we consider, our bound depends on the learning rate, number of local steps, and
the bound provided by the assumption on the stochastic gradients (the bound M2). The critical point
is that the upper bound on the expectation ofΓt does not depend on the number of interactions t. Our
approach leverages techniques from the analysis of static load balancing schemes, e.g. Berenbrink
et al. (2009). Two key elements of novelty in our case are that (1) for us the load balancing process
is dynamic, in the sense that loads (gradients) get continually added; (2) the load-balancing process
we consider is multi-dimensional, whereas usually the literature considers simple scalar weights.
The complete argument is presented in the Appendix.
This technique is quite powerful, as it allows for a number of non-trivial extensions:
Extension 1: Removing the second-moment bound and allowing for non-i.i.d. local data. In the
first extension of the algorithm, we assume that the number of local steps performed by each agent is
fixed and is equal to H. In this case, we are able to remove the bounded second moment assumption,
and are able to prove convergence under standard assumptions for non-i.i.d data. Specifically, in the
non-i.i.d. setting, we consider that each fi (x) is the local function of agent i (computed over the
samples available to i). We will require that 1) the function fi is L-smooth, and that 2) for each
agent i, gi is unbiased estimate of fi and that 3) for any x, E[kai(x) - fi(x)k2] ≤ σ2. We define
f (x) = Pn=1 fi(x)∕n and the bound PtIkVfi(X)-Vf (x)k2∕n ≤ ρ2.
Theorem 4.2.	Let f be an non-convex, L-smooth function whose minimum x? we are trying to find
via the SwarmSGD procedure given in Algorithm 1. Assume the local functions of agents satisfy
the conditions discussed above. Let H be the number of local stochastic gradient steps performed
by each agent before interacting. Define μt = En=I Xi/n, where Xi is a value of model i Ofter t
interactions. For learning rate η = √n^ and T = Ω (n4H2max(1,L2) (λ2 +1)) we have that:
PT-I Ekft )k2
T
≤ 击Eff(〃0)- f(x?)] + 376H2 max(1√L2)(σ2 +4的(1+ 1).
TH	T	λ2
Please see Appendix H for the details of the proof; we note that we did not optimize for constants.
Relative to Theorem 4.1, we have the same quadratic dependency on the number of local steps H
and on L, but now the second moment bound is replaced by the variance terms. We emphasize that
for non-i.i.d data under second-moment bounds, the exact same bounds as in Theorem 4.1 will hold.
Extension 2: Non-blocking averaging. Algorithm 1 is blocking, in that it requires both nodes
to complete their local iterations at the same time before they can interact. In practice, nodes can
6
Under review as a conference paper at ICLR 2021
average their local updates without synchronizing, as follows. Each node i keeps two copies of the
model: the live copy Xi (on which local SGD iterations are applied) and the communication copy
Yi , which can be accessed asynchronously by communicating nodes. When completing its local
steps, a node i first checks if some other node averaged against its communication copy Yi since
its last communication step. If the answer is yes, it simply applies its locally-generated update to
its communication model Yi, updates the live copy so that Xi = Yi, and proceeds with the next
iteration of local computation. If no other node has averaged against its communication copy, then
the node actively seeks a random communication partner j , and averages its live copy against its
model Yj, updating both to (Xi + Yj)/2. The node then proceeds with the next iteration of local
computation. Please see Appendix F for the precise definition of the algorithm, and for the formal
convergence guarantee for this variant.
Extension 3: Quantization. For large models, the cost of the averaging step can become significant,
due to bandwidth constraints. To remove the bandwidth bottleneck, we allow the averaging step to be
performed with respect to quantized versions of the two models. While communication-compression
has been considered in a decentralized context before, e.g. (Tang et al., 2018; Lu and Sa, 2020), our
approach is different. Instead of modifying the algorithm or maintaining neighbor information at
every node, we make use of a quantization scheme with some useful properties by Davies et al.
(2020), which we slightly adapt to our context.
The key issue when quantizing decentralized models is that for most known quantization schemes,
e.g. (Alistarh et al., 2017), the quantization error depends on the norm of the inputs: here, the inputs
are models, which are not necessarily close to the origin. Thus, the quantization error at each step
would depend on the norm of the models, which would break our bound on Γt . Instead, we observe
that the quantization scheme of Davies et al. (2020) has error which is bounded by the distance
between inputs, rather than input norms. Crucially, we show that Γt can in fact be used to bound the
distance between models, so we can bound the quantization error in terms of Γt at each step. This
allows us, with some care, to generalize the analysis to the case where the models are quantized. We
provide a full description and proof of convergence in Appendix G.
Specifically, quantization ensures the same convergence bounds as in Theorem 4.1, but with an
expected communication cost of O(d + logT) bits per step.1 By contrast, non-quantized decen-
tralized algorithms assume that nodes can exchange infinite-precision real numbers, while the only
other memory-less compression scheme (Lu and Sa, 2020) induces a linear dependence in d in the
rate. In our applications, d log T , and therefore our cost is essentially constant per dimension;
specifically, we show that we can quantize to 8 bits per coordinate without loss of accuracy.
5 Experimental Results
In this section, we validate our analysis, by applying the algorithm to training deep neural networks
for image classification and machine translation. We map the algorithm onto a multi-node supercom-
puting setting, in which we have a large number of compute nodes, connected by fast communication
links. The key overhead in this setting is synchronization: at large node counts, the cost of synchro-
nizing all nodes so they execute in lock-step can be very high, see e.g. Li et al. (2019) for numerical
results on different workloads. SwarmSGD mitigates this overhead, since nodes synchronize only
sporadically and in pairs. Harnessing the computational power of this large-scale distributed setting
is still an underexplored area (Ben-Nun and Hoefler, 2019).
Target System and Implementation. We run SwarmSGD on the CSCS Piz Daint supercomputer,
which is composed of Cray XC50 nodes, each with a Xeon E5-2690v3 CPU and an NVIDIA Tesla
P100 GPU, using a state-of-the-art Aries interconnect over a Dragonfly network topology, which
is regular. Please see (Piz, 2019) for more details. We implemented SwarmSGD in Pytorch and
TensorFlow using NCCL and MPI-based primitives. Both variants implement the version with non-
blocking averaging. The Pytorch implementation is on top of SGP framework (Assran et al., 2018),
and uses SwarmSGD to train ResNets on the CIFAR-10/100 (Krizhevsky et al., 2014) and Ima-
geNet (Russakovsky et al., 2015) datasets, while we use the TensorFlow implementation to train a
much larger Transformer-XL model (Vaswani et al., 2017) on the WMT17 (En-Ge) dataset. We note
that all algorithms used the same topology overlay (fully-connected with random pairings), and that
SGP was run with overlap factor 1, as suggested by Assran et al. (2018).
1The unusual log T factor arises because the quantization scheme of Davies et al. (2020) can fail with some
probability, which we handle as part of the analysis.
7
Under review as a conference paper at ICLR 2021
Model / Dataset	∣	SGDToP-1	∣	LBSGDToP-1	∣	SWarmSGD	∣	Parameters
ResNet20/CIFAR-10	∣	91.7%(200 叩ochs)	|	91.5% (200 epochs)	∣	91.79%(280	叩ochs)	|	4 local	steps
ResNet18 / ImageNet	∣	69.76 % (90 epochs)	∣	69.17% (90 epochs)	∣	69.79%	(240	epochs)	∣	3 local	steps
ResNet50 / ImageNet	∣	76.14% (90 epochs)	∣	75.43% (90 epochs)	∣	75.68%	(240	epochs)	∣	2 local	steps
Table 1: Parameters for full Top-1 validation accuracy on CIFAR-10 and ImageNet running on 32
nodes. SWarm step count represents local SGD steps per model betWeen tWo averaging steps, and
epochs are counted in terms of total passes over the data.
Training time (minutes)
(a) Convergence versus Time.
Figure 1: Convergence and Scalability on the Transformer/WMT Task With multiplier = 1.
16 nodes 32 nodes 64 nodes
(b) Throughput vs previous Work. Higher is better.
Training Process. Our training methodology folloWs data-parallel training, With some differences
due to decentralization, and is identical to previous Work on decentralized and local SGD, e.g. (Lian
et al., 2017; Assran et al., 2018; Lin et al., 2018). Training proceeds in epochs, each of Which
corresponds to processes collectively performing a full pass over the dataset. At the beginning of
each epoch, We re-shuffle the dataset and partition it among processes (Lin et al., 2018).
As noted in previous Work (Lian et al., 2017; 2018; Assran et al., 2018) variants of decentralized
SGD are not alWays able to recover sequential SGD accuracy Within the same number of epochs as
this baseline. This is justified by Theorems 4.1 and 4.2, Which predict that the sloWer mixing (and
higher local model variance) can affect convergence. Thus, in some experiments, We Will alloW the
decentralized schemes to execute for more epochs, by a constant multiplier factor betWeen 1 and 3.
Once We have fixed the number of epochs, we do not alter the other training hyperparameters: in
particular, the learning rate schedule, momentum and Weight decay terms are identical to sequential
SGD, for each individual model.
Accuracy and Speed. We first examined Whether SWarmSGD can in fact recover full accuracy
versus the sequential or large-batch SGD baselines. In Table 1 We provide an overvieW of param-
eter values to recover or exceed large-batch SGD accuracy (folloWing (Goyal et al., 2017)) using
SWarmSGD, on the ResNet/ImageNet/CIFAR tasks. We execute for 32 nodes on ImageNet, and 8
nodes on CIFAR-10. (Local batch sizes are 256 for ResNet20 and ResNet18, and 128 for ResNet50.
Quantization is not applied.) The results shoW that SWarm can recover or slightly exceed the ac-
curacy of the large-batch baselines, and that it has loWer practical communication cost relative to
existing methods (see Figure 2(b), Where We separate the average computation cost per batch). HoW-
ever, SWarm requires significant additional passes over the data (up to 2.7×) to achieve full accuracy,
Which negates its performance benefits in this specific setting, relative to large-batch SGD. (Please
see Appendix Figure 5 for an end-to-end time comparison. We do not take the cost of fine-tuning
the hyperparameters for large-batch SGD into account in this example.) This finding is in line With
previous Work on decentralized methods (Assran et al., 2018).
Next, We examine accuracy for the WMT17 task. The results are provided in Figure 1(a), in
accuracy-vs-time format, for 16 and 32 nodes, executing for 10 global epochs. Here, the large-batch
SGD (LB-SGD) baseline (BLEU score 26.1 at 16 nodes) is a poor alternative at high node counts:
its throughput is very loW, due to the size of the model (see Figure 1(b)). At 16 nodes, SWarm
8
Under review as a conference paper at ICLR 2021
(a) Convergence in time versus number of local
steps for ResNet18 on ImageNet. All variants re-
cover the target accuracy, but we note the lower
convergence of variants with more local steps.
The experiment is run on 32 nodes.
(b) Average time per batch for previous methods,
compared to SwarmSGD, on ResNet18/ImageNet.
The base value on the y axis (0.4) is the average
computation time per batch, so values above rep-
resent average communication time per batch.
Figure 2: Convergence results and performance breakdown for ResNet18/ImageNet.
slightly exceeds the baseline accuracy at 26.17 BLEU, for an end-to-end speedup of 〜 1.5×. In
the same setting, Swarm outperforms all other decentralized methods (the fastest previous method,
AD-PSGD, is 30% slower, and less accurate), both in terms of BLEU score, and in terms of end-
to-end time. (The objective loss graph is similar, and is given in Appendix Figure 7.) At 32 nodes,
all decentralized methods reach lower scores (〜23.5) after 10 epochs. However, we observed ex-
perimentally that running Swarm for an additional 5 epochs at 32 nodes recovered a BLEU score of
〜25.9, 30% faster than the 16-node version in terms of end-to-end time (omitted for visibility).
In addition, we investigated 1) the accuracy of the real average of all models throughout training: it
is usually more accurate than an arbitrary model, but not significantly, corroborating the claim that
individual models tend to stay close to the mean; 2) the influence of the number of local steps on
accuracy: perhaps surprisingly, we were able to recover baseline accuracy on ResNet18/ImageNet
for up to 4 local steps (see Figure 2(a)); 3) the impact of quantization on convergence, where we
were able to recover accuracy when applying 8-bit model quantization to Swarm. We encourage
the reader to examine the full experimental report in the Appendix, which contains data on these
experiments, as well as additional ablation studies.
Discussion. Generally, the performance and accuracy of SwarmSGD are superior to previous de-
centralized methods (see Figure 1 for an illustration, and Figure 2(b) for a performance breakdown).
In particular, a closer examination of the average batch times in Figure 2(b) shows that time per node
per batch (including communication and computation) is largely constant as we increase the number
of nodes, which gives our method close-to-ideal scaling behaviour. This advantage relative to pre-
vious schemes, notably AD-PSGD, comes mainly from the reduction in communication frequency:
Swarm communicates less often, and therefore incurs lower average communication cost.
The main disadvantage of Swarm is that, similar to previous decentralized methods, it may need
additional data passes in order to fully recover accuracy at high node counts. However, we also note
that our method did not benefit from the high level of hyperparameter tuning applied to large-batch
SGD, e.g. (Goyal et al., 2017). We find it interesting that this accuracy issue is less prevalent in
the context of large, over-parameterized models, such as the Transformer, where Swarm could be a
practically-viable alternative to large-batch SGD within the same number of epochs.
6 Conclusions and Future Work
We analyzed the convergence of SGD in a decoupled model of distributed computing, in which
nodes mostly perform independent SGD updates, interspersed with intermittent pairwise averaging
steps, which may be performed in an inconsistent and noisy manner. We showed that SGD still con-
verges in this restrictive setting, and under considerable consistency relaxations, and moreover can
still achieve speedup in terms of iteration time. Empirical results in a supercomputing environment
complement and validate our analysis, showing that this method can outperform previous proposals.
A natural extension would be to generalize the bounds to arbitrary communication graphs, or in
terms of the assumptions on the objective, or to experiment on large-scale decentralized testbeds.
9
Under review as a conference paper at ICLR 2021
References
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT,2010, pages 177-186. Springer, 2010.
F. Seide, H. Fu, L. G. Jasha, and D. Yu. 1-bit stochastic gradient descent and application to data-
parallel distributed training of speech dnns. Interspeech, 2014.
Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth
Annual Conference of the International Speech Communication Association, 2015.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Randomized
quantization for communication-efficient stochastic gradient descent. In Proceedings of NIPS
2017, 2017.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, pages 1508-1518, 2017.
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent.
arXiv preprint arXiv:1704.05021, 2017.
Nikoli Dryden, Sam Ade Jacobs, Tim Moon, and Brian Van Essen. Communication quantization
for data-parallel training of deep neural networks. In Proceedings of the Workshop on Machine
Learning in High Performance Computing Environments, pages 1-8. IEEE Press, 2016.
Demjan Grubic, Leo Tam, Dan Alistarh, and Ce Zhang. Synchronous multi-gpu deep learning with
low-precision communication: An experimental study. In EDBT, 2018.
Peter Davies, Vijaykrishna Gurunathan, Niusha Moshrefi, Saleh Ashkboos, and Dan Alistarh. Dis-
tributed variance reduction with optimal communication, 2020.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In NIPS, pages 693-701, 2011.
C. M. De Sa, C. Zhang, K. Olukotun, and C. Re. Taming the wild: A unified analysis of hogwild-
style algorihms. In Advances in Neural Information Processing Systems, 2015.
John C Duchi, Sorathan Chaturapruek, and Christopher Re. Asynchronous stochastic convex opti-
mization. arXiv preprint arXiv:1508.00882, 2015.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 2017.
Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block training
with intra-block parallel optimization and blockwise model-update filtering. In 2016 ieee interna-
tional conference on acoustics, speech and signal processing (icassp), pages 5880-5884. IEEE,
2016.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jio Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. arXiv preprint arXiv:1705.09056, 2017.
Hanlin Tang, Ce Zhang, Shaoduo Gan, Tong Zhang, and Ji Liu. Decentralization meets quantization.
arXiv preprint arXiv:1803.06443, 2018.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. arXiv preprint arXiv:1902.00340, 2019.
10
Under review as a conference paper at ICLR 2021
Tal Ben-Nun and Torsten Hoefler. Demystifying parallel and distributed deep learning: An in-depth
concurrency analysis. ACM Computing Surveys (CSUR), 52(4):1-43, 2019.
Ji Liu and Ce Zhang. Distributed learning systems with first-order methods. Foundations and
TrendsR in Databases, 9(1):1-100, 2020. ISSN 1931-7883. doi:10.1561/1900000062. URL
http://dx.doi.org/10.1561/1900000062.
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. In International Conference on Machine Learning, pages 3043-3052. PMLR,
2018.
The CSCS Piz Daint supercomputer. http://www.cscs.ch/computers/piz_daint,
2019. Accessed: 2020-1-25.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998-6008, 2017.
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat. Stochastic gradient push
for distributed deep learning. arXiv preprint arXiv:1811.10792, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770-778, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical
report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.
David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-based computation of aggregate informa-
tion. In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.,
pages 482-491. IEEE, 2003.
Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control
Letters, 53(1):65-78, 2004.
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip al-
gorithms. IEEE/ACM Trans. Netw., 14(SI):2508-2530, June 2006. ISSN 1063-6692.
doi:10.1109/TIT.2006.874516. URL https://doi.org/10.1109/TIT.2006.874516.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Transactions on Automatic Control, 54(1):48, 2009.
Bjorn Johansson, Maben Rabi, and Mikael Johansson. A randomized incremental subgradient
method for distributed optimization in networked systems. SIAM Journal on Optimization, 20
(3):1157-1170, 2009.
Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd
Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850-
857. IEEE, 2014.
Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A
unified theory of decentralized sgd with changing topology and local updates. arXiv preprint
arXiv:2003.10422, 2020.
John Kim, Wiliam J Dally, Steve Scott, and Dennis Abts. Technology-driven, highly-scalable drag-
onfly topology. In 2008 International Symposium on Computer Architecture, pages 77-88. IEEE,
2008.
11
Under review as a conference paper at ICLR 2021
Maciej Besta and Torsten Hoefler. Slim fly: A cost effective low-diameter network topology. In
SC’14: Proceedings of the International Conference for High Performance Computing, Network-
ing, Storage and Analysis, pages 348-359.lEEE, 2014.
Dana Angluin, James Aspnes, Zoe Diamadi, Michael J Fischer, and Rene Peralta. Computation in
networks of passively mobile finite-state sensors. Distributed computing, 18(4):235-253, 2006.
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for
nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737-
2745, 2015.
Petra Berenbrink, Tom Friedetzky, and Zengjian Hu. Anew analytical method for parallel, diffusion-
type load balancing. J. Parallel Distrib. Comput., 69(1):54-61, January 2009. ISSN 0743-7315.
doi:10.1016/j.jpdc.2008.05.005. URL https://doi.org/10.1016/j.jpdc.2008.05.
005.
Yucheng Lu and Christopher De Sa. Moniqua: Modulo quantized communication in decentralized
SGD. In Proc. International Conference on Machine Learning (ICML), 2020.
Shigang Li, Tal Ben-Nun, Salvatore Di Girolamo, Dan Alistarh, and Torsten Hoefler. Taming un-
balanced training workloads in deep learning with partial collective operations. arXiv preprint
arXiv:1908.04207, 2019.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online:
http://www.cs.toronto. edu/kriz/cifar.html, 55, 2014.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.
Bhaskar Ghosh and S. Muthukrishnan. Dynamic load balancing by random matchings. J. Comput.
Syst. Sci., 53(3):357-370, December 1996. ISSN 0022-0000. doi:10.1006/jcss.1996.0075. URL
https://doi.org/10.1006/jcss.1996.0075.
12
Under review as a conference paper at ICLR 2021
A Summary of the Appendix sections
Appendix contains the following sections:
•	In Section B we compare SwarmSGD with some of the existing algorithms. We list con-
vergence bounds and the assumptions needed to achieve them.
•	In Section C we provide crucial properties for the load balancing on the graph.
•	In Section D we provide definitions for the local steps we use in the later sections.
•	In Section E we provide the sketch of proof of Theorem 4.1, which shows the conver-
gence of SwarmSGD assuming the second moment bound on the gradients. Recall that the
number of local steps in this case is a geometric random variable with mean H .
•	In Section F we provide the proof for the non-blocking version of the swarm SGD algo-
rithm. We again assume the second moment bound on the gradients and that the number of
local steps is a geometric random variable with mean H .
•	In Section G we provide the proof for the quantized version of the swarm SGD algorithm.
We again assume the second moment bound on the gradients and that the number of local
steps is a geometric random variable with mean H .
•	In Section H we prove Theorem 4.2. In this case we do not assume the second moment
bound, data is not distributed identically and the number of the local steps performed by
each agent is a fixed number H.
•	In Section I we provide additional experimental results for SwarmSGD.
B Comparison of results
In this section we compare convergence rates of existing algorithms, while specifying the bounds
they require for convergence. In the tables T -corresponds to the parallel time and n is a number of
processes. We use the following notations for needed bounds (or assumptions):
1.	σ2 - bound on the variance of gradient .
2.	M2 - bound on the second moment of gradient.
3.	d - bounded dimension.
4.	λ2 - bounded spectral gap of the averaging matrix (interaction graph in case of
SwarmSGD).
5.	τ - bounded message delay.
6.	r - interaction graph is r-regular.
7.	∆ - bounded diameter of interaction graph.
Algorithm	Assumptions	Convergence Rate
SwarmSGD SwarmSGD AD-PSGD Lian et al. (2018) SGP Assran et al. (2018)	σ2,λ2,r	O(1∕√Tn) M 2,λ2,r	o(i∕√τn) σ2,λ2,τ	O(1∕√Tn) σ2,d, ∆,τ	O(1∕√Tn)
Table 2: Comparison of theoretical results in the non-convex case.
Discussion. We compare in more detail against Lian et al. (2018) and Assran et al. (2018), since
these are the only other papers which do not require explicit global synchronization in the form
of rounds. (By contrast, e.g. Wang and Joshi (2018); Koloskova et al. (2020) require that nodes
synchronize in rounds, so that at every point in time each node has taken the same number of steps.)
In Assran et al. (2018), all nodes perform gradient steps at each iteration, but averaging steps can be
delayed by τ iterations. Unfortunately, in this case the mixing time depends on the dimension - d
13
Under review as a conference paper at ICLR 2021
(more precisely, it contains a √d factor!), on the delay bound T, and on ∆, defined as the number
of iterations over which the interaction graph is well connected. Additionally, the analysis is not
suitable for random interactions. On the other hand, Lian et al. (2018) consider random interaction
matrices and do not require the agents to perform the same number of gradient steps. Unlike our
model, in their case more than two nodes can interact during the averaging step.
To circumvent the global synchronization issue, Lian et al. (2018) allow agents to have outdated
views during the averaging step. Yet, we would like to emphasize that they require blocking during
the averaging steps, while we allow some amount of non-blocking property. By some amount we
means that algorithm needs blocking only in the case when some node takes more than two consec-
utive iterations to complete it’s local gradient steps. This means that for each node i to complete
Hi local steps should not more take more then O(n) global steps (since each node interacts with
probability 2/n at each step), this assumption also holds for Lian et al. (2018).
In summary, our algorithm reduces the synchronization required by averaging steps, by considering
pairwise interactions and by introducing local steps and providing non-blocking version of the al-
gorithm as well (in SGP and AD-PSGD, agents perform one local step and one averaging step per
iteration). We would like to point out that we also allow a random number of local steps between in-
teractions in the case when we have second moment bound on the stochastic gradient, which reduces
synchronization costs even further. Finally, our algorithm requires T ≥ O(n4) number of iterations
to achieve the convergence rate of O(1 /√Tn) in the case of blocking algorithm and T = Ω(n6) in
general. (By contrast, Lian et al. (2018) requires T = Ω(n6).)
C	Properties of the Load Balancing
In this section provide the useful lemmas which will help as in the later sections.
We are given a simple undirected graph G, with n nodes (for convenience we number them from 1
to n) and edge set E. Each node is adjacent to exactly r nodes.
Each node i of graph G keeps a local vector model Xti ∈ Rd (t is the number of interactions or
steps); let Xt = (Xt1, Xt2, ..., Xtn) be the vector of local models at step t.
An interaction (step) is defined as follows: we pick an edge e = (u, v) of G uniformly at random
and update the vector models correspondingly.
Let μt = Pn=ι Xi/n be the average of models at step t and let Γt = Pn=ι IlXi - μt∣∣2 be a
potential at time step t.
Let L be the Laplacian matrix ofG and let let λ2 be a second smallest eigenvalue ofL. For example,
if G is a complete graph λ2 = n.
First we state the following lemma from Ghosh and Muthukrishnan (1996):
Lemma C.1.
λ2=	min n 学 |X Vi = θ}.	⑺
v=(v1,v2,...,vn)	vT v
i=1
Now, we show that Lemma C.1 can be used to lower bound P(i,j)∈E IXti - XtjI2:
Lemma C.2.
n
X kXi- Xjk2 ≥λ2 X kXi -μtk2 = λ2Γt.	(8)
(i,j)∈E	i=1
Proof. Observe that
X	kXi	- Xjk2= X	k(Xi	- μt) -	(Xj	-	μt)k2.	(9)
(i,j)∈E	(i,j)∈E
Also, notice that Lemma C.1 means that for every vector v = (v1, v2, ..., vn ) such that Pin=1 vi = 0,
we have:
n
X (vi-vj)2 ≥λ2Xvi2.	(10)
(i,j)∈E	i=1
14
Under review as a conference paper at ICLR 2021
Since Pn=I(Xi - μt) is a 0 vector, We can apply the above inequality to the each of d components
of the vectors X1 - μt,X2 - μt,…，Xn - μt separately, and by elementary properties of 2-norm
we prove the lemma.	□
D Definitions for the Local Steps
In this section we provide the formal definition of the local steps performed by our algorithms.
Recall that Xti is a local model of node i at step t. Let Hti be the number of local steps node i
performs in the case when it is chosen for interaction at step t + 1. A natural case is for Hti to be
fixed throughout the whole algorithm, that is: for each time step t and node i, Hti = H . However,
optimal choice Hti depends on whether a second moment bound on gradients (5) is assumed. Let:
ehi0(Xti) =0.
and for 1 ≤ q ≤ Hti let:
q-1
ehiq(Xti) = gei(Xti -	ηehis(Xti)),
s=0
Note that stochastic gradient is recomputed at each step, but we omit the superscript for simplicity,
that is: ehiq(Xti) = geiq(Xti - Pqs=0 ηehis(Xti)). Further, for 1 ≤ q ≤ Hti, let
q-1	q-1
h(Xi) = E[ei(Xi - Xηes(xi))] = Vf(Xi- Xηes(xi))
s=0	s=0
be the expected value ofehiq(Xti) taken over the randomness of the stochastic gradient gei. Letehi(Xti)
be the sum of Hti local stochastic gradients we computed:
Hti
ehi (Xti) =	ehiq(Xti).
q=1
Similarly, for simplicity we avoid using index t in the left side of the above definition, since it is clear
that if the local steps are applied to model Xti we compute them in the case when node i interacts at
step t + 1. The update step in Swarm SGD (Algorithm 1) is (before averaging):
Hti	Hti	q-1
Xti+1 = Xti	- ηehi(Xti)	=	Xti	- η	ehiq(Xti) =	Xti	- η	gei(Xti	-	ηehis(Xti)).
q=1	q=1	s=0
Notice that
q-1	Assumption 5
Ekehiq(Xti)k2 = Ekgei(Xti -	ηehis(Xti))k2	≤	M2.	(11)
s=0
E Analysis under second moment bound and random number of
LOCAL STEPS
In this section we consider Algorithm 1, where for each node i, Hi is a geometric random variable
with mean H . We also assume a gradient second moment bound (5). We provide only a sketch of
the proof since the proof for the non-blocking version of algorithm in Section F is more general. If
nodes i and j interact at step t + 1 and their local models have values of Xti and Xtj after step t.
Their new model values become:
Xi+1 = Xj+1 = (Xi + Xj - ηei(Xi) - ηej(Xj))/2.
Recall that μt is average of the values of models after time step t and Γt = Pn=IkXi - μtk2 First
of all we can prove that (see Lemma F.1, the result can be achieved even though algorithms differ):
E[Γt+1] ≤ E[Γt](1
(12)
15
Under review as a conference paper at ICLR 2021
We can further show that Lemma F.2, and therefore Lemmas F.3 and F.4, also hold, yielding:
E[Γt] ≤ (40r + 80r2)nη2H2M2,	(13)
λ2	λ2
and
n	3Hn
EEhVf(μt), -hi(Xi)i ≤ 2HL2E[Γt] - -ɪ-EkVf(μt)∣∣2 + 12H3nL2M2η2.	(14)
i=1
Next in the similar fashion as in the proof of Theorem F.8 we can show that :
E[f(μt+ι)] ≤ E[f (μt)] + fη∙ XXEDVf (μt), -hi(Xi)) + 20Ln2H2M2.	(15)
n i=1	n
This allows to show that:
Theorem 4.1. Let f be an non-convex, L-smooth function, whose stochastic gradients satisfy the
bounded second moment assumption above. Let the number of local stochastic gradient steps per-
formed by each agent upon interaction be a geometric random variable with mean H. Let the
learning rate we use be η = n∕√T. Define μt = PZi Xt /n, where Xi is a value of model i after
t interactions, be the average of the local parameters.
number of interactions T ≥ n4 :
Then, for learning rate η = n∕√T and any
1 T-1
T EEkVf(μt)k2 ≤
T t=0
4(f(μo)- f(x*))
√TH
2-04H2 max(1, L2)M2 r2
√T	(λ2
+ 1).
+
Proof. We again skip the calculations and follow steps from the proof of Theorem F (note that
constants can be improved, but for simplicity we keep them the same). After applying (13) and (14),
this results in:
160r	-20r2 η3H3M2L2n	Hn	2
E[f(μt+ι)] - E[f(μt)] ≤ (~λ~ +	λ2~)---------------4~EkVf(μt)k
76H 3L2M 2η3	20Lη2 H 2M 2
n	n2
once we sum up the above inequality t = 0 to t = T - 1 and massage terms we get (additionally
recall that E[f(μτ)] ≥ f (x*)):
ɪ ∑EkVf(μt)k2 ≤ 4n(f*J(x*)) + (640r + 等)η2H?M
T t=0	THη	λ2	λ22
+ 80TLnHM 2 +-04TH 2L2M 2η2.
n
Finally since η = n∕√T ≤ * (because T ≥ n4) We get the proof of the lemma. Note that the
difference between this theorem and Theorem F.8 is that we have lower bound of n4 for T here,
instead of n4(n + 1)2. The reason is that We are not required to use Lemmas F.7 and F.5 since our
algorithm alloWs blocking and this means that interacting agents do not have incomplete values for
the models.	□
F Analysis of the Nonblocking Variant, with Second Moment
Bound and Random Number of Local Steps
First We define hoW the non-blocking property changes our interactions. Let i andj be nodes Which
interact at step t + 1, We set
X i	—Xk Xj0
Xt+1∕2 =可 + 亏,
Xtj+1/2
Xj + Xi0
2 + 2
16
Under review as a conference paper at ICLR 2021
and
Xti+1 = Xti+1/2 - ηehi(Xti),
Xt+1 = Xt+1/2 - ηehj(Xt ),
where for each node k, if ptk + 1 is the last time interacting before and including step t:
Xtk0 = Xpitk+1/2 = Xptk+1 + ηehk(Xpktk) = Xtk + ηehk(Xpktk).	(16)
Intuitively the last definition means that node k has computed Xpik+1/2 but has not finished com-
puting Xpk+1, hence when some other node tries to read Xpk+1, it reads the value which is missing
local gradient update step, but it does not have to wait for node k to finish computing. Since ptk + 1
is the last step node k interacted we have that Xtk = Xpkk+1. More formally:
Algorithm 2 Sequential non-blocking SwarmSGD pseudocode for each interaction between nodes
i and j .
% Let G be r-regular graph.
% Sample an edge (i, j) of G uniformly at random.
Require: agents i and j chosen for interaction, i is initiator
% choose Hi and Hj
% agent i performs Hi local SGD steps
Si J Xi
for q = 1 to Hi do
Xi J Xi - ηgei(Xi)
end for
% agent j performs Hj local SGD steps
Sj J Xj
for q = 1 to Hj do
Xj J Xj - ηgej (Xj)
end for
% agents update their estimates
Xi J (Si + Xj0)/2 + (Xi -Si)
Xj J (Sj + Xi0)/2 + (Xj - Sj)
Notice the differences between the main algorithm and non-blocking one: first, local gradient steps
are applied only after the averaging steps (this corresponds to term Xi - Si for node i), and second,
nodes get access to the model of their interacting partner, which might not be complete for the
reasons described above (for example, node i is forced to use Xj 0 instead of Xj in its averaging
step). If node i is the initiator of the interaction and its chosen interaction partner j is still computing
the local gradients from its previous interaction, this algorithm allows node i not to wait for j to finish
computation. In this case, i simply leaves its value Xi0 in j ’s memory. Notice that since i is finished
computation it does not need to pass its outdated model to j , but we assume the worst case.
We proceed by proving the following lemma which upper bounds the expected change in potential:
Lemma F.1. For any time step t we have:
E[Γt+1] ≤ E[Γt](1
i=1
kehi(Xti)k2 + Ekehi Xpiit k2
Proof. First we bound change in potential ∆t = Γt+1 - Γt for some fixed time step t > 0.
For this, let ∆it,j be the change in potential when we choose agents (i, j) ∈ E for interaction
ij	ηehj Xpjj	j
(While calculating ∆J We assume that Xt is fixed). Let Ri = -ηhi(Xi) +------------------∖ M and Rj =
17
Under review as a conference paper at ICLR 2021
ηehi Xpii
-ηhj(Xj) +------\ %, . We have that:
Xti+1
Xtj+1
Xi + Xt
2
Xi + Xt
2
+ Rti .
+ Rtj .
This gives us that:
1 - μt+1
Xi+1 - μt+1
Rti +Rtj
μt+ι = μt H------
n
Xi+X…-1 Rt	i Rj	“
—2 — + 丁 Rt- nRt-μt∙
Xi + Xj , n - 1 Rj	1 Rt	U
—2— + 丁 Rt- nRt-μt∙
For k 6= i, j we get that
Xk+ι-μt+1=Xk- n(Ri+Rj) - μt.
Hence:
∆t,j = ∣∣ X+Xtj
+ n-1 Rt- 1 Rj-
n
n - 1
+----
n
Rj- nRt-μt∣∣ - ∣∣Xj -μt∣∣
+ II-
n
Observe that:
+ X (IlXk - n(Rt + Rt)-μtk2 - IIXk - μt∣∣)
k6=i,j
Xi - μt + Xij -
—2 一 +
+ DXi - μt + Xj- μt, TRi + n-2RjE
nn
+∣∣≡1 Rt - n Rj∣∣2+∣∣? Rj - n Rt ∣∣2
+ X 2DXk - μt, - n(Ri+Rj)E
k6=i,j
+ X (1)2kRi + Rjk2.
n
k6=i,j
n
X (Xk - μt, - n(Rt + Rj)) = 0.
k=1	n
After combining the above two equations, we get that:
2
2
∆it,j
kχi - Xjk2
2
+一 %
■+DXi- μt+Xt- μt, Rt+RjE
Rt+Rjf+∣∣ L Rt- 1 RjII2+∣∣ L
nn
Rj-1 Rt∣∣2
—
n
Cauchy-Schwarz
≤
kXti - Xtjk2
—
2
n - 2	1
+ 2( T + n +
+DXi- μt+Xj- μt, Rt+RjE
⅛i^ )(kRtk2 + kRjk2).
(17)
18
Under review as a conference paper at ICLR 2021
ηehj Xjj	ηehi Xpii
Recall that Ri = -ηhi(Xt)+------∖ pt and Rj = -ηhj(Xt)+---------∖ Pt, Using Cauchy-Schwarz
inequality we get that
2
kRtk2 ≤ 2η2kei(xi)k2 + η2kej(xpj)k
2
kRjk2 ≤ 2η2kej(Xj)k2 + + kei(Xpt)k
2
2
Denote 2η2kei(Xi)k2 + η2kei (Xpi) k2 by Si and2η2kei(Xj)k2 + %向(xpj k2 by Sj. Hence
(17) can be rewritten as:
∆it,j ≤
—
—
≤
kXi -2Xjk2 + DXi - μt + Xj- μt, Ri + RjE
+2(—+n+—)Gt+Sj)
kXi -2Xjk2 + DXi- μt + Xj- μt, Ri + RjE
+ 2(Sti + Stj).
Further:
DXi - μt+Xj- μt, Ri+RjE
Young λ2∣∣Xi - μt + Xtj - μtk2
≤	8r
2r∣∣Ri + Rj『
λ2
+
Cauchy-Schwarz
≤
+λ2IIXj
4r
4r∣∣Ri∣∣2 +4r∣∣Rj∣∣2
λ2
+ λ2∣∣Xj
4r
+ 4r(Si + Sj)
λ2
≤
+
This gives us:
X	∆it,j ≤ X -
(i,j)∈E	(i,j)∈E
kXi - Xjk2 + λ2∣∣Xi - μt∣∣ + λ2∣∣Xj- μt∣∣	+	4r(St + Sj)
2	+	4r	+ λ2
+ 2(Sti +Stj)
Lemma C.2
≤
λ2Γt
- 丁 +
n
XX (2r +4r22 )Si + ∑
i=1	2	i=1
-泮 + XX(2r +4r22 间.
i=1	2
Next, we use the above inequality to upper bound ∆t in expectation:
1
E[∆t∣X0,Xι,…,Xt] = rn/2 E E∆i,jX0,X1,…,Xt]
rn (i,j)∈E
≤ r⅛ (- λ2τ + X(2r+4⅛ )EhSi|X0,X1,…,Xti)
i=1
=—独十 XX + 8r EhSi∣X0,X1,…,Xt]
2rn	i=1	λ2
n
19
Under review as a conference paper at ICLR 2021
Finally, we remove the conditioning:
E[∆t] = E[E[∆t∣Xo, Xi,…,Xt]] ≤ -λ2Er1 + (4+ 8r) XX ES]∙
2rn	λ2	n
i=1
By considering the definition of ∆t and Sti, we get the proof of the lemma.
□
Next, we upper bound the second moment of local updates , for any step t and node i:
Lemma F.2.
n
X Ekηehi(Xti)k2≤2η2nH2M2.
i=1
Proof.
n	∞	nu
X Ekηehi(Xti)k2 = η2 X Pr[Hti=u]X EkX ehiq(Xti)k2
i=1	u=1	i=1 q=1
∞	nu
≤η2X Pr[Hti=u]X uX Ekehiq(Xti)k2
u=1	i=1 q=1
(5)	∞	n
≤ η2X P r[Hti = u]u2 X M2 ≤ 2nη2H2M2.
u=1	i=1
Where in the last step we used
∞
X P r[Hti = u]u2 = E[(Hti)2] = 2H2- H ≤ 2H2.
u=1
□
This allows us to upper bound the potential in expectation for any step t.
Lemma F.3.
E[rt] ≤(40： +
nη2H2M 2.
(18)
Proof. We prove by using induction. Base case t = 0 trivially holds. For an induction step step we
assume that E[Γt] ≤ (* + 学；)nη2H 2M 2r2. Wegetthat:
E[Γt+i] ≤ E[Γt](1 - ^n) + (2+ λr)η2 XX(4Ekei(Xi)∣∣2 + Ekei 国)k2)
2rn	λ2	i=1
Lemma(R2) (1 — λ2-)E[Γt] + (20 + 40r)H2M2η2
2rn	λ2
≤(I- 2rn )(40r+ 8λr2 )nη2H 2M2 + (20+λ02)H 2M 2η2
40r	80r2	2 2 2
=(丁 + P )nη2H 2M2.
λ2	λ2
□
n
The next lemma allows Us to upper bound Ei=I Eh▽/(μt), -hi(Xt)) which Will be used later
once we apply L-SmoothneSS to upper bound f(μt+ι). The intuition is as follows: if for each i,
i
hi(Xti) was just a sum of single stochastic gradient(Hi = 1) by the unbiasedness property we would
have to upper bound Pn=IEhVf (μt), -Vf (Xi))= Pn=I (EhVf (μt), Vf(μt) - Vf(Xi))-
EkVf (μt)k2), which can be done by using L-smoothness and then definition of Γt.
20
Under review as a conference paper at ICLR 2021
Lemma F.4. For any time step t.
n	3Hn
EEhVf(μt), -hi(Xi)i ≤ 2HL2E[Γt] - -4-EkVf(〃t)『+ 12H3nL2M2η2.
i=1
(19)
Proof.
n	n∞	u
XEhVf (μt), -ei(Xi)i = XXPr[Ht = u]EhVf(μt), - Xeq(Xi)i
n∞	u
=XX
Pr [Hi = u] X (EhVf (μt), Vf (μt) - % (X；)〉- E∣∣Vf (μt)∣∣2)
i=1 u=1	q=1
n ∞	u	q-1
=XXPr[Ht = u] X (EhVf(μt), Vf(μt) - Vf(Xi- X淌IXitY))- E∣∣Vf(μt)k2)
i=1 u=1	q=1	s=0
Using Young S inequality We can upper bound EhVf (μt), Vf (μt) - Vf (Xi — PS=0 ηhi(Xt)) by
Ekvf4μt)k + e∣∣ Vf (μt) 一 Vf(Xi - Pq-1 ηhs(Xi))∣∣ . Plugging this in the above inequality We
get:
n
XEhVf(μt),-ehi(Xti)) ≤
i=1
≤ XXPr[Ht = u]X (EkVf(μt) - Vf(Xi - XηeS(Xi))k2 - 3EkVTt)k2)
i=1 u=1	q=1	s=0
≤) X X Pr [Hi = u] X (L2Ekμt - Xi + X ηhS(Xi))k2 - ≡fμ<).
i=1 u=1	q=1	s=0
q-1
Next We use Cauchy-SchWarz inequality on Ekμt - Xti +	sq=-01 ηhis(Xti))k2
n
XEhVf(μt),-ehi(Xti)) ≤
i=1
≤ X X Pr[Hi = u] X (2L2Ekμt - Xik2 + 2L2Ek XηeS(Xi))k2 - 3EkVTt)k2 )
i=1 u=1	q=1	s=0
q-1
Term Ek s=0 ηhis(Xti))k2 can be upper bounded by q2M2 using Cauchy-SchWarz and assumption
(5). Hence:
n
XEhVf(μt),-ehi(Xti)) ≤
i=1
≤ X X Pr[Hi = u] X(2L2Ekμt - Xik2 + 2L2η2q2M2 - 3EkVTt)k2 )
i=1 u=1	q=1
=XXPr[Hi = u]u(2L2Ekμt - Xik2 - 3EkVf(μ"k2)
i=1 u=1
n∞
+XXPr [Hi = u]u(u + 1)(2u + 1)L2M 2η2∕3	(20)
i=1 u=1
Note that:
X XPr[Hi = u]u(2L2Ekμt --3EkVf(μ"k2) = 2HL2E[Γt] - 3HnE∣∣Vf(μt)k2.
i=1 u=1
(21)
21
Under review as a conference paper at ICLR 2021
Also:
n∞
XX
Pr [Ht = u]u(u + 1)(2u + 1)L2M 2η2∕3
i=1 u=1
n∞
≤XXP r[Hti = u]2u3L2M2η2
i=1 u=1
≤ 12H3nL2M2η2.	(22)
Where in the last step we used (Recall that Hti is a geometric random variable with mean H):
∞
X P r[Hti = u]u3 = E[(Hti)3] = 6H3 - 6H2 + H ≤ 6H3 .
u=1
By plugging inequalities (22) and (21) into inequality (20) We get the proof of the lemma. □
n
Our next goal is to upper bound Ei=I Eh▽/(μt), -ht(Xpi)).
Lemma F.5.
n	n	5Hn
EEhVf (μt), ht(XPi )i ≤ 2HL2 £ Ekμt - Xp i k2 + -ɪ- EkVf (μt)k2 + 12H 3nL2M 2η2.
i=1	i=1
(23)
Proof. The proof is very similar to the proof of lemma F.4, except that When We subtract and add
term E|Vf (μt)k2 in the proof it will eventually end up with a positive sign (After using Young's
inequality it will have factor of ɪ + 1 instead of factor of 4 一 1) and we have Pn=ι E∣∣μt - XpiIl2
instead of Γt = Pn=ι E∣∣μt — Xt∣∣2∙ Thus, we omit the proof in this case.	□
Next step is to upper bound P= E∣μt — Xpi k2,for this we will need the following lemma:
Lemma F.6. For any node i and time step t,
Ekμt- μpi k2 ≤ 10η2H2M2.
Proof. Notice that
t	t-1	2 t t-1	2
Ekμt-μpt k2 = X Pr [pi = t0]E∣∣ X μs+ι —Ms|| ≤ XX Pript = t0](t-t0)E∣∣μs+ι —Ms|| .
t0=0	s=t0	t0=0 s=t0
(24)
where we used Cauchy-Schwarz inequality in the lastt step. Fix step s. Let u and v be nodes which
interact at step s + 1. We have that
Ekμs+1 — μsk2 = E||- T +	- ―
≤ 4η2 Ekeu(XU)k2+4η2 Ekev(Xv)k2+
-2	s	-2	s
22
+ — Ekeu (Xuu)k2 + — Ekev (XPv )k2∙
+
We again used the Cauchy-Schwarz inequality since the expectation is taken only over the random-
ness of sampling and number of local steps. We can use the approach from lemma F.2 to upper
boundEkehu(Xsu)k2,Ekehv(Xsv)k2,Ekehu(Xpusu)k2andEkehv(Xpvsv)k2.
(In the lemma we upper bound the sum of- similar terms, but with η2.) Hence:
t t-1	2
Ekμt — μpt 112 ≤XXPr [pt = t0](t- t0)E^s+ι — 〃s||
t0=0 s=t0
≤ X X Pr[ρt = t0](t — 怦 20η2HM2 = ^HME[(pt - t)2]∙
t0=0 s=t0
22
Under review as a conference paper at ICLR 2021
t - pit is a geometric random variable with mean n/2 (because the probability that node i interacts
is 2/n at every step). Thus, E[(pt -1)2] = 2(E[t - pi])2 - E[t - pt] ≤ n .Thus, E∣∣μt - μpi ∣∣2 ≤
10η2H 2M2.	t □
Finally we can show that:
Lemma F.7. For any node i and time step t,
XXE∣μt - Xpiik2 ≤ 20nH2M2r2 + (80r + 华2)n2η2H2M2.
t	λ2	λ2
i=1	2
Proof. Using Cauchy-Schwarz inequality we get:
nn
χEkμt- Xpik2 ≤ X(2Ekμt- μpt k2 + 2E[μpt- Xptk2)
≤ XX(2E∣μt - μpt k2 + 2E[Γpt]) ≤ 20nH2M2η2 + (8λr + T)n2η2H2M2.
i=1	2	2
Where the last inequality comes from Lemmas F.3 and F.6.	□
Now we are ready to prove the main theorem.
Theorem F.8. Let f be an non-convex, L-smooth, function satisfying assumption 5, whose minimum
x? we are trying to find via the non-blocking version of SwarmSGD procedure (See, algorithm 2).
Let the number of local stochastic gradient steps performed by each agent upon interaction be a
geometric random variable with mean H. Let the learning rate we use be η = n∕√T. Define
μt = En=I Xlln, where Xi is the value of model i after t interactions. Then, for learning rate
η = n∕√T and any T ≥ n4(n + 1)2:
T ∑ EkVf (μt)∣2 ≤ 4⅛f≡ + 2304H2 m"L2)M2
T t=0	TH	T
Proof. Let Et denote expectation conditioned on {X1t, X2t, ..., Xnt }. By L-smoothness we have that
Et[f (μt+ι)] ≤ f (Mt) + EthVf(Mt),μt+ι - μti + -2-Et∣∣μt+1 - μt∣∣2.
After removing conditioning:
E[f (μt+l)] = E[Et[f (μt+l)]] ≤ E[f (μt)] + E(Vf (μt), μt+1 - μti + LE∣"l - μtk2∙ (25)
First We look at E[μt+ι — μt]. If agents i and j interact, (which happens with probability m1/2). We
have that μt+ι - μt = - η hi (Xi) - nhj (Xj) + 券 hi (Xpi) + ^nej (Xj Hence we get that
Et[μt+1- μt] = rn/2 X at—nei(Xi) - nej(Xj)]+2n几(XP)+2nhj(Xpt))
(i,j)∈E
nn
=- n X Et[ei(Xi)]+n Xei(Xpt).
i=1	i=1
and
nn
E[μt+ι - μt] = E[Et[μt+ι - μt]] =-* X E[hi(Xi)] + + X E[hi(Xpi)].
n i=1	n i=1
23
Under review as a conference paper at ICLR 2021
Next We look at E∣∣μt+ι - μt∣∣2. If agents i and j interact, (which happens with probability m1/^).
一	一	S-T ，一:、	S-T ,_J、	S-T ,一: .	S-T ,一d .	一
Wehavethat μt+ι - μt = - VXIji)- % hj (Xt) + 券hi (Xpi) + 券hj (Xpj). Hence we get that
Etkμt+1-μtk2=焉 x Etii- n hi(Xj)-n e(Xj )+2n hi(^p Pl )+2^ ej (Xpj)Il
(i,j)∈E
Cauchy-Schwarz 1	2
≤	n(4Etkhi(Xj)k2+4Et k η hj- (Xj )k2
(j,j)∈E
+ kehj(Xpjit)k2+kehj(Xpjj)k2
2n 42	2n 2
=2 X 4⅞khi(Xj)k2 + 2 X 与khj(Xpt)k2
n n2	n n2	t
j=1	j=1
Lemr216η2H2M2+2 X η⅛(Xpt)k2.
n2 n n2 pt
and
Ekμt+1 - μtk2 = E[[Etkμt+1 - μtk2]]
16η2H2M2	2 X η2 e Yi .心 LemmaF.2 20η2H2M2
≤ —n — + D / Ekhi(Xpt) k	≤	-n—
i=1
Hence, we can rewrite (25) as:
nn
E[f (μt+ι)] ≤ E[f (μt)]+n X E(Nf (μt),-ei(Xt))+n X E(▽/(也), hj (Xp t))
i=1
i=1
20Lη2H 2M 2
n2	.
Next, we use Lemmas F.4 and F.5:
E[f(μt+1)] ≤ E[f(μt)] + 22(2HLE[Γt] - 3HnEkVf (μt)k2 + 12H3nL2M2η2)
n2	4
+ -η2(2HL2 XEkμt - Xpik2 + 5HnEkVf (μt)k2 + 12H3nL2M2η2)
n	i=1	4
4Lη2H 2M 2
n2
=E[f(μt)] + 4hl⅛≡ + 2HLs= Ek“t-Xpik2) - HEkVf(μt)k2
n2	n2	4n
36H 3L2M 2η3	20Lη2H 2M 2
+------------ + ——-------.
n	n2
We Use Lemmas F.3 and F.7 to upper bound E[Γt] and Pn=I Ekμt 一 Xpi k2 respectively :
160r	320r2 η3H3M2L2 (n2 + n)	Hn
E[f(μt+=)] - E[f(μt)] ≤ (X + F)η-----n2( + ) - 丁EkVf(μt)k2
76H3 L2 M 2η3	20Lη2 H 2 M 2
+------------ +-----------.
n	n2
by summing the above inequality for t = 0 to t = T - 1, we get that
T-1	160r	320r2 η3H 3M 2L2(n + 1) ηH	2
E[f(μτ)] - f(μo) ≤ E (J-ʌ i	λ2-)	n	JnEkVf(μt)k
t=0	λ2	λ2	n	4n
20Lη2 H 2 M2	76H3 L2 M 2η3 \
+ —n — +	n b
24
Under review as a conference paper at ICLR 2021
From this we get that :
T-1ηH	160r	320r2 η3H3M2L2T (n + 1)
EnnEkVf (μt)k ≤ f (μo) - E[f(μτ)] + (千 + K)η-n ( 十 )
20TLn2 H 2 M2	76TH3 L2 M 2η3
++-.
n2	n
Note that E[f (μτ)] ≥ f (x*), hence after multiplying the above inequality by ^^ We get that
1 T-1E∣L” , ll9 4n(f(μo) — f (x*))	, 640r 1280r2 , 9 ττ9 ll rr9 τ9 .	.
不 EEkVf(μt)k2 ≤ f(μ04 f )) + (-τ- + F-)η2H2M2L2(n +1)
T t=0	THη	λ2	λ22
+ 80LnHM2 + 304H 2L2M 2η2.
n
Observe that η = n/√T ≤ i+^, since T ≥ n4(n +1)2. This allows Us to finish the proof:
1 T-IFlE" ∖∣∣2 / 4n(f (μο) - f(x*)) l ∕-40r , 1280r2∖ L2ηM2H 2
τ2EkVf( )k THη	+(F + Fr) —n—
80LηHM2	304H 2L2M 2η
nn
__ (4f(μ0) — f(x*))十(640r + 1280r2)L2M2H2
80LHM2	304H 2L2M2
+ ―√τ — +	√τ
“ 4(f (μο) — f (x*)) ɪ 2304H2 max(1, L2)M2 / r2
≤ —√TH - +	√τ	(舄 + 1).
□
G Analysis of Quantized Averaging, Assuming Second Moment
B ound and Random Number of Local Steps
First we define how quantization of models changes our interactions. Both the algorithm and the
analysis in this case are similar to those of Section F. Let i and j be nodes which interact at step
t + 1, we set
Xti+1/2
Xi , Xj0
互+〒
Xtj+1/2
Xj + χi0
2	2
and
Xti+1 = Xti+1/2 — ηehi(Xti),
Xt+1 = Xt+1/2 — ηehj(Xt ),
where for each node k, Xtk0 is a quantized version of the model Xtk . We use the quantization
provided in Davies et al. (2020). The key property of this quantization scheme is summarized
below:
Lemma G.1. Letq be a parameter we will fix later. If the inputs xu, xv at nodes u and v, respectively
satisfy ∣∣xu — Xv∣∣ ≤ qq G then with probability at least 1 — lοglοg(ɪ∣∣xu — Xv∣∣) ∙ O(q-d), the
quantization algorithm of Davies et al. (2020) provides node v with an unbiased estimate xu0 ofxu,
with ∣∣xu0 — Xuk ≤ (q2 + 7)g and uses O (dlog(q ∣∣xu — Xv ∣∣)) bits to do so.
25
Under review as a conference paper at ICLR 2021
For our purposes, xu and xv are the local models of the nodes u and v and d is their dimension (we
omit the time step here). In the following , we refer to the above lemma as the quantization lemma.
Recall that in section E, for each node k, if ptk + 1 is the last time interacting before and including
step t:
Xk 0 = Xpk+1∕2 = Xk + ηek (Xpk).	(26)
Analysis Outline. The crucial property we used in the analysis is
that E[ehk (Xpkk )] ≤ 2H2M2η2
k
(see Lemma F.2). We also used that for hk (Xkk ) , we can use the smoothness property (4) in
pt
Lemmas F.5 and F.7. In our case we plan to use the quantization lemma above. For this, first notice
that the estimate is unbiased: this means that E[Xtk 0] = Xtk , eliminating the need to use Lemma
F.5 and subsequently F.7, since Pn=ι Eh▽/(μt),Xt - XfOi = 0 in our case. Secondly if We set
(q2 * + 7) = HηM we also satisfy Lemma F.2. This means that entire analysis can be replicated,
and even further as in the case of section E We Will only need T ≥ n4 (In one case We do not use
Lemma F.5 at all, and in the second case upper bound can be replaced by 0, Which is the same as
not using it). NoW We concentrate on calculating the probability that kxu - xv k ≤ qqd (Which We
call the distance criterion) required by the quantization lemma to hold over T steps and each pair of
nodes. We also need to calculate the probability With Which We fail to decode.
Assume that Lemma F.3 holds for step t, as in the proof of this lemma We Will use induction and
Lemma F.1 (We omit the proof since it Will be exactly the same given that the conditions We discuss
above hold). That is: E[Γt] ≤ (40r- + 8^r2)nη2H2M2.
Notice that for a pair of nodes u and v, kXtu - Xtvk2 ≤ 2Γt (Using Cauchy-SchWarz). Hence We
need to calculate the probability that Γt ≥ (qqd )2/2. Using Markov’s inequality, the probability of
this happening is at most:
2E[Γt]	(80r	160r2 ` n(q2 + 7)2e2	(80r	160r2 ` n(q2 + 7)2
Wd^ ≤ (X + ^λΓj	(qqde)2	= (X +	(qqd)2 '
We set q = 2 + T3∕d, this means that (qqd)2 ≥ 2T3. So given that T ≥ n4, We have that Pr[Γt ≥
(qqd )2 /2] ≤ O(1/T 4) (note that r ≤ n 一 1 and λ2 = Ω(1∕n2) since our graph is connected).
Hence the distance criterion is satisfied With probability 1 - O(1/T 2). Given that it is satisfied, We
also have failure probability loglog(ɪ∣∣XU - Xvk) ∙ O(q-d) = O( logTogT).
So, the total probability of failure, either due to contravening the distance criterion or by probabilistic
failure, is at most O(T -2). Hence With probability 1 - O(1/T 2) We can use Lemma F.1 and prove
that
E[Γt+1 ] ≤
40r + *) nη2H2M2.
What is left is to union bound over T steps2, and We get that With probability 1 - O(1/T) =
1 - O(1/n4) the quantization algorithm never fails and the distance criterion is alWays satisfied.
The total number of bits used per step is O(dlog q) = O(d + logT).
With this We can state the main theorem:
Theorem G.2. Let f be an non-convex, L-smooth function, whose stochastic gradients satisfy the
bounded second moment assumption above. Consider the quantized version of the algorithm 1.
Let the number of local stochastic gradient steps performed by each agent upon interaction be a
geometric random variable with mean H. Let the learning rate we use be η = n∕√T. Define
μt = En=I Xt/n, where Xi is a value of model i after t interactions, be the average of the local
parameters. Then, for learning rate η = n∕√T and any number of interactions T ≥ n4, with
probability at least 1 - O(1/n4) we have that:
1 T-1
T EEkVf(μt)k2 ≤
T t=0
4(f (μ0) - f (x*))	2304H2 max(1, L2)M2
√TH +	√T
and additionally we use O(d + log T) communication bits per step.
2Note that We do not need to union bound over all pairs of u and v, since can assume that u and v are the
ones Which interact at step t + 1
26
Under review as a conference paper at ICLR 2021
H Fixed number of local steps with Variance Bound and
non-identically distributed data
We again deal with a non-convex L-smooth function, but we no longer require a second moment
bound, and no longer assume that data is distributed identically.
We use a constant learning rate η and fixed local steps sizes Hti = H, for each node i and step t.
Each agent i has access to local function fi such that:
1.	For each agent i, the gradient Vfi(x) is L-LiPschitz continuous for some L > 0, i.e. for all
x, y ∈ Rd :
kVfi(x) - Vfi(y)k ≤Lkx-yk.	(27)
2.	for every x ∈ Rd :
n
X fi (x)/n = f (x).	(28)
i=1
3.	For each agent i and x ∈ Rd :
E[gei(x)] = Vfi(x).	(29)
4.	For each agent i and x ∈ Rd there exist σ2 such that:
Ekgei(x)-Vfi(x)k2 ≤σ2.	(30)
5.	For each x ∈ Rd there exist σ2 such that:
n
X kVfi(x) -Vf (x)k2∕n ≤ ρp.	(31)
i=1
Notice that since data is not distributed identically, for 1 ≤ q ≤ H, we no longer have that
q-1	q-1
hq (Xi) = E[hqχ)]=旧扇羽-Enhs(Xi))] = Vf (Xi - Enhs(Xi)).
s=0	s=0
Instead,
q-1	q-1
hq(Xi) = E[hq(Xi)]=旧质(骂-Enhs(Xi))] = Vfi(Xi- Enhs(Xi)).
s=0	s=0
We Proceed by Proving the following lemma which uPPer bounds the exPected change in Potential:
Lemma H.1. For any time step t , we have:
E[Γt+ι]≤ (1 -六)E[Γt] + (2+8r)n2 X E≡≡
2rn	λ2	n
i=1
Proof. First we bound change in Potential ∆t = Γt+1 - Γt for some fixed time steP t > 0.
For this, let ∆it,j be the change in Potential when we choose agents (i, j) ∈ E for interaction.
We have that:
Xti+1 = (Xti + Xtj + nehi(Xti) + nehj(Xtj))/2.
Xtj+1 = (Xti + Xtj + nehi(Xti) + nehj(Xtj))/2.
ij
μt+ι = μt + nhi(XJ∕n + nh∙ (Xt)∕n.
This gives us that:
Xi+ι - μt+ι = Xi+Xtj + :nei(Xi) + :nej(Xj)-必
2	2n	2n
Xt+ι - μt+ι = X⅛Xt- + n2-2nei(Xi) + n2-2nej(Xj)- 〃加
2	2n	2n
27
Under review as a conference paper at ICLR 2021
For k 6= i, j we get that
Xk+1 - μt+1 = Xk - n (ηhi(Xi) + ηhj(Xj))- μt.
Hence:
∆t,j = I (Xi + Xj)/2 + n2n2(ηei(Xi) + ηej(Xj))- 〃t『TIXi - 〃t『
+ I (Xi + Xj)/2 + n2n2(ηei(Xi) + ηej(Xj))- μt∣∣2 - IIXj - μt∣∣2
+ X (∣∣Xk- 1(ηei(Xi) + ηej(Xj))-μtk2-∣∣Xk-μt∣∣2)
k6=i,j
2
Xi - μt + Xj -
—2 一 +
2
+ 2DXi - μt + Xj - μt, 2n ηhi(Xi) +	2n~ηej(Xj)E
+ 2( n-2 )2kηei(Xi)+ ηej (Xj )k2
+ X 2DXk- μt, - n (ηhi(Xi)+ηhj (Xj))E
k6=i,j
+ X (1)2kηei(Xi) + ηej(Xj)k2.
k6=i,j
Observe that:
n
X (Xtk - μt, --(ηei(Xi) + ηej(Xj)))= 0.
k=1	n
After combining the above two equations, we get that:
△了 = -kXi -2Xjk2 + DXi - μt + Xj - μt, ηei(Xi) + ηej(Xj)E
+(2( n2-2 )2 + (n - 2)(1)2) kηei(Xi) + ηej (Xj )k2
2n	n
Cauchy-Schwarz	kXi - Xj k2
≤	-k t 2 t k +(Xi -μt + Xj -μt,ηhi(Xi) + ηhj(Xj))
+ 2(2( n2n2 )2 + (n - 2)( 1)2)(kηei(Xi)k2 + kηej (Xj)k2)
≤ - kXi -2Xjk2 + DXi - μt + Xj- μt, ηei(Xi) + ηej(Xj)E
+ kηehi(Xti)k2+kηehj(Xtj)k2).
This gives us:
28
Under review as a conference paper at ICLR 2021
∆it,j ≤
(i,j)∈E	(i,j)∈E
—
kXi _2X" + DXi - μt + Xj- μt,ηei(xi) + ηej(Xj)E
+ kηehi(Xti)k2 + kηehj(Xtj)k2
Lemma C.2
≤
-λΓt + X rkηei(Xi)k2
i=1
Young
≤
+ X DXi- μt+Xj- μt,ηhi(Xi)+ηhj(Xj)E
(i,j)∈E
λ22Γt + X rkηhi(Xi)k2
i=1
+
(i,j)∈E
λ2∣∣Xi -μt + Xj-μtk2	2r∣∣ηhi(Xi)+ηhj(Xj)Il
8r
λ2
+
—
Cauchy-Schwarz λ Γ n
≤	- λ2Γ + X rkηhi(Xi)k2
i=1
i=1
λ2
i=1
4r2kηehi(Xti)k2
n
+X
i=1
Next, we use definition of Γt :
X	∆i,j≤-浮 + X(r + 4r2)kηei(Xi)k2.
4	λ2
(i,j)∈E	i=1
(32)
Next, we use the above inequality to upper bound ∆t in expectation:
E[∆t∣X0,X1,...,Xt] = ɪ X E[∆t,j∣X0,X1,...,Xt]
rn/2
(i,j)∈E
1 λ Γ n	4r2
≤ rn/2 (—4~ + X(r+X)E[kηhi(Xt)k χ0,χ1,…,χJ)
-r + X (2 + 8r )η2
2rn	λ2
i=1
E kehi(Xti)k2|X0,X1,...,Xt
Finally, we remove the conditioning:
Ed=EiEdXo, X1,…,Xt]] ≤ - λ2Enti +(2+ir- )η2 X
i=1
Ekehi(Xti)k2
By considering the definition of ∆t , we get the proof of the lemma.
—
+
n
n
□
29
Under review as a conference paper at ICLR 2021
Lemma H.2. For any 1 ≤ q ≤ H and step t, we have that
n	n	q-1
XEkVfi(μt) - hq(Xi)k2 ≤ 2L2E[Γt] + X2L2η2Ek Xes(Xi)k2.
i=1	i=1	s=0
Proof.
n	n	q-1
XEkVfi(μt) - hq(Xi)k2 = XEkVfi(μt) - Vfi(Xi- Xηes(xi))k2
i=1	i=1	s=0
(27) n
≤ ∑L2Ekμt - Xi + ηhq-1(xi))k2
i=1
Cauchy-Schwarz
≤
n	n	q-1
X2L2EkXi -μtk2 + X2L2η2Ek Xes(Xi)k2
i=1	i=1	s=0
n	q-1
2L2E[Γt]+X2L2η2EkXehis(Xti)k2.
i=1	s=0
□
Lemma H.3. For any 1 ≤ q ≤ H and step T, we have that
n
XEkehiq(Xti)k2 ≤nσ2+4nρ2
i=1
n	q-1	n
+ 16L2E[Γt] + X 16L2η2Ek Xes(Xi)k2 + 4nE∣∣ X hq(Xi)∕nk2.
i=1	s=0	i=1
Proof.
(28)	2 n
≤ nσ2 + E
i=1
n	n	n	q-1
XEkehiq(Xti)k2 ≤ X(σ2 + Ekhiq(Xti)k2) = nσ2 + X EkVfi(Xti - X ηehis(Xti))k2
i=1	i=1	i=1	s=0
n
Vfi(Xi - ηhq-1(Xi)) - Vfi(μt) + Vfi(μt) - Vf (μt) + X Vfj(μt)∕n
j=1
n	q-1	n	q-1	2
-XVfj(Xtj - X ηehjs(Xtj))∕n + X Vfj(Xtj - X ηehjs(Xtj))∕n
j=1	s=0	j=1	s=0
Cauchy-Schwarz
≤ nσ2
nn
+ X 4EkVfi(μt) - hq (Xi)k2 + 4nEk X(Vfg-hi (Xi))∕n∣∣2
i=1	i=1
nn
+ 4nEk X hq (Xi)∕nk2 + 4 X kVfi(μt) - Vf (μt)∣∣2
i=1	i=1
Cauchy-Schwarz,(31)
≤	nσ2
nn
+ 4nρ2 + X 8E∣∣Vfi(μt) - hq(X"∣2 +4nE∣∣ X hi(Xi)∕n∣∣2
i=1	i=1
Lemma H.2
≤	nσ2
n	q-1	n
+4nρ2+16L2E[Γt]+X16L2η2EkXehis(Xti)k2+4nEkXhiq(Xti)∕nk2.
i=1	s=0	i=1
□
Next we use the above lemma to show the upper bound for PqH=1 Pin=1 Ekehiq(Xti)k2:
30
Under review as a conference paper at ICLR 2021
Lemma H.4. For η ≤ 6Lh, we have that:
H n	H n
X X Ekeq(Xi)k2 ≤ 2Hn(σ2 +4ρ2) + 32HL2E[Γt]+8n X Ek X hq(Xi)∕nk2
Proof. Notice that if η ≤ 6L1H the Lemma H.3 gives us that :
n
XEkehiq(Xti)k2 ≤ n(σ2 + 4ρ2) + 16L2E[Γt]
i=1
(33)
n	q-1	n
+ X 2HH2Ek Xes(Xi)k2 +4nEk X hq(Xi)∕nk2
i=1	s=0	i=1
n	q-1	n
≤ n(σ2 + 4ρ2) + 16L2E[Γt]+ X 急 XEkes(Xi)k2 + 4nEk X hi(Xi)∕n∣∣2
i=1 2H s=0	i=1
n	q-1	n
≤ n(σ2 + 4ρ2) + 16L2E[Γt] + X击XEkehis(Xti)k2+4nEkXhiq(Xti)∕nk2.
(34)
i=1 s=0
i=1
For 0 ≤ q ≤ H, let
nq
Rq = XXEkehis(Xti)k2.
i=1 s=0
Observe that the inequality 33 can be rewritten as:
1n
Rq - Rq-I ≤ Z-77Rq-1 + n(σ + 4p) + 16L2 E[rt] + 4nEk ɪ2 hq (Xt )/nk2 .
2H
i=1
which is the same as
1n
Rq ≤ (1 + 2H)Rq-1 + n(σN + 4p2 ) + 16L2E[rt] + 4nEk ɪ2 hq (Xt )/nk2 .
By unrolling the recursion we get that
H-1	n
RH ≤ X (1 + 2H)q (n(σ2 + 4ρ2) + 16L2E[Γt] + 4nE∣∣ X h∏-q(Xi)∕n∣∣2)
q=0	i=1
Since,
(1 +焉)H ≤ (e2H)H = e1/2 ≤ 2
2H
we have that
Hn	H	n
RH = XX Ekeq (Xi) k2 ≤ 2 X (n(σ2 +4ρ2) + 16L2E[Γt]+4nEk X hi (Xi)∕n∣∣2)
q=1 i=1	q=1	i=1
Hn
= 2Hn(σ2 +4ρ2) + 32HL2E[Γt] + 8nXEk Xhiq(Xti)∕nk2.
q=1	i=1
□
Next we derive the upper bound for PtT=0 E[Γt]:
Lemma H.5. For η ≤--------/ 1
/ 一 10HLλ∕2r∕λ2+8r2∕λ2
we have that :
T
XE[Γt]≤
t=0
8nrη2(σ2 + 4ρ2)H 2T	8r
λ2	λ2
F(2+ λr) XXEk X hq(Xi)∕nk2.
2	2 t=1 q =1	i=1
31
Under review as a conference paper at ICLR 2021
Proof. By Lemma H.1 we get that:
λ	2H 8r H n
En+1] ≤(1 - 2rn )E[rt ] + ηn- (2+%)XX Ekhq (Xi)k2
Lemma H.4	λ
≤ (I- 2rn网t]
η2 -	8r	n
+ 彳(2 + ^) (2-n(σ2 + 4ρ2) + 32-L2E[Γt] + &n£ Ek E hq(Xi)∕n∣∣2)
n	2	q=1	i=1
(1 - :λL )E[Γt ] + 2η2(σ2 + 4ρ2) + -2 (2 + 8r)+ 32--ILIn (2 + Ir )E[Γt]
2rn	λ2	n	λ2
8r H	n
+ 8η2-(2+『X Ek X hq(Χi)∕nk2
2 q=1	i=1
Notice that for η ≤ 12HL√2r∕λ2 +82% We can rewrite the above inequality as
λ	8r	8r H	n
E[Γt+ι] ≤ (1-4n;)E[Γt]+2η2(σ2 +4ρ2)-2(2+	)+8η2-(2+ 了)£EIl Ehq(Xi)∕n∣∣2.
nr	2	2 q=1	i=1
since P∞=ο(1 - 4nr )i ≤ jJ W = 4ηr Wegetthat:
1	(1	4nr)
T	Hn
X E[Γt] ≤ τ~ (2(η2 + 4ρ2)σ2-2(2 + λ-) + 8η2-(2 +『XEk X h(Xt)∕n∣2)
t=0	2	2	2 q=1	i=1
=8nrη2(σ2+4ρ2)-2T(2 + λr) + 32nrη2-(2 + λr) £XEk X hq(Χi)∕nk2.
2	2	2	2 t=1 q =1	i=1
□
NoW, We are ready to prove the folloWing theorem:
Theorem 4.2. Let f be an non-convex, L-smooth function whose minimum x? we are trying to find
via the SwarmSGD procedure given in Algorithm 1. Let local functions of agents satisfy conditions
(27), (28), (29), (30) and (31). Let - be the number of local stochastic gradient steps performed
by each agent upon interaction. Define μt = En=I Xt ∕n, where Xi is a value of model i after t
interactions. For learning rate η =为 and T ≥ 57600n4-2 max(1, L2)(£ + 1)2 we have that: * 1
PΤο1 EkVf(μt)k2
T
V 1 fγ,/ 、	"*∖]∣ 376-2 max(1,L2)(σ2 + 4P2) ∕r2∣”
≤ √τ-E[f(μ0) - f(x )] +----------√τ-----------(λ2 + 1).
Proof. Let Et denote expectation conditioned on {X1t, X2t, ..., Xnt }. By L-smoothness We have that
Et[f(μt+l)] ≤ f(μt) + EthVf (μt),μt+1 - μti + 2Etkμt+1 - μtk2 .	(35)
First we look at Et [μt+ι 一 μt]. If agents i and j interact (which happens with probability r1/2), we
η	ηj
have that μt+ι — μt = — η hi (Xt) — ⅛ hj (Xj). Hence we get:
1	η	η	2n η
Et[μt+1 - μt] = 一72 X Et[--------hi (Xt )--hj (Xt )] = - X Et[---hi(Xt )]
rn∕2	n	n	n	n
(t,j)∈E	t=1
n	nH
=-n X Et[hi(Xi)] (=) - n XX Et[hq (Xi)].
t=1	t=1 q=1
32
Under review as a conference paper at ICLR 2021
Using the above inequality we have:
2η n H
EthNf(μt),μt+ι - μti = hvf(μt),Et[μt+ι - μt]i = hvf(μt), — n XXEt[hi(Xt)]i
i=1 q=1
Hn	n
η X (EtkVf(μt) - X hq(Xi)∕nk2 -kvf(μt)k2 - EtkX hq(Xi)/nk2)
q=1	i=1	i=1
Hn	n
C= η X (EtkX(Vfi(μt) - hq(Xi))∕nk2 -kVf(μt)k2 - Etk X hq(Xi)∕nk2)
q=1	i=1	i=1
Hn	n	n
≤ η X (n XEtk X Vfi(μt) - hq(Xi)k2 -kVf(μt)k2 - Etk X hq(Xi)∕nk2).
q=1 i=1	i=1	i=1
Here WeUsedCaUchy-SchWarz inequality at the last step. Next we look at Etkμt+ι - μtk2. If agents
η
i and j interact, (which happens with probability rn/2). We have that μt+ι - μt = -ηhi(Xii)-
ηj
£ hj (Xj). Hence we get that
Etkμt+1 -μtk2 =力 X 叫-nhi(Xi)-ηhj(Xj"
(i,j)∈E
Cauchy-Schwarz
≤
焉 X	n2 (2Etkei(Xi)k2+2Etk η ej (Xj)k2)
(i,j)∈E
2
n
n 2 2	Cauchy-Schwarz 4 2H n H
X 2⅛ khi(Xi)k	≤	4⅛h XX Etkhq (Xi)k2.
i=1	i=1 q=1
So, we can rewrite (35) as:
Hn
Et [f (μt+ι)] ≤ f (μt) + η X ∈ Etk XVf(μt)-hq (Xi)k2
q=1	i=1
n
-kVf(μt)k2 - Etk X hq(Xi)∕nk2)
i=1
+ 2⅞H X X Etkeq(Xi)k2.
i=1 q=1
33
Under review as a conference paper at ICLR 2021
Next, we remove conditioning:
E[f(μt+l)]= E[Et[f (μt+ι)]]
Hn
≤ E[f (μt)] + ηX (1xEkVfi(μt)- h(Xi)k2
q=1	i=1
n
-EkVf (μt)k2- Ek X hi(Xt)/nk2)
i=1
+ 2Ln3H X X Ekeq (Xi )k2
i=1 q=1
Lemma H.2
≤
H	n	q-1
E[f (μt)] + η X (-(2L2E[Γt]+ X 2L2η2Ek XeS(Xi)k2)
q=1	i=1	s=0
n
- EkVf (μt)k2 - Ek X hi(Xi)∕nk2)
i=1
+2LT X X Ekeq(Xi )k2
i=1 q=1
Lemma H.4	2ηL2H	Hη
≤	E[f(μt)] + -η-^E[Γt] - HEkVf(μt)k2
n2	n
2-2η3	H n
+ 寸-(2Hn(σ2 + 4ρ2) + 32H-2E[Γt] + 8n £ Ek E hi(Xi)∕nk2)
q=1	i=1
2-Hη2
n3
+
Hn
2Hn(σ2 +4ρ2) +32H-2E[Γt] + 8n X Ek X hiq (Xti)/nk2
q=1	i=1
Hn
-η X EkX hi(Xi)∕nk2.
n q=1	i=1
Next We choose η ≤ əɪ and η ≤ &0LH, so that 16Lnn ≤ 言 and 16LH η ≤ ɪ. This together with
the above inequalities allows Us to derive the following upper bound for E[f (μt+ι)] (We eliminate
terms with positive multiplicative factor Ek Pin=1 hiq(Xti)∕nk2):
E[f(μt+1)] ≤ E[f(μt)] + 2η⅞HE[Γt] - HnEkVf(μt)k2
n2	n
+ 2-n2η3 (2Hn(σ2 + 4ρ2) + 32H-2E[Γt])
+ 2-Hη2 0Hn(σ2 + 4ρ2) + 32H-2E[Γt])
Hn
-5n X Ek X hi(Xi)∕nk2.
q=1	i=1
We proceed by summing up the above inequality for 0 ≤ t ≤ T - :
∑ E[f(μt+1)] ≤ £ E[f(μt)] + 4-2Hn3(；2 +4P" + 4-H" +4P2)T
t=0	t=0
+
2η-2H
n2
T-1
X E[Γt] +
t=0
64-4Hη3
n2
T-1
X E[Γt] +
t=0
处尹 X1 E[Γt]
n3
t=0
-∑ 5n XEk X hi(Xi)∕nk2 - X1 ηHEkVf (μt)k2.	(36)
t=0	i=1 i=1	i=0
34
Under review as a conference paper at ICLR 2021
Next we use Lemma H.5:
2ηL2H
n2
T-1
X E[Γt] +
t=0
64L4Hη
n2
3 T-1
E[Γt]+
t=0
64L3Hn2 X1 E[Γt]
n3
t=0
V 2nL2H 88nrη2(σ2 + 4ρ2)H2T	8r
- n2	∖	λ2	λ2，
+ 32nrη2H(2+ λr) X1XEk X h(Xi)∕nk2
2	2 t=1 q=1	i=1
64L4Hη3 8nrη2 (σ2 + 4ρ2) + H2T	8r
+ —n2 —1	λ2	( + λ2)
+ 32nrη2H(2+ λr) X1XEk X h(Xi)∕nk2
2	2 t=1 q=1	i=1
64L3H 2η2 8nrη2 (σ2 + 4ρ2 ) + H2T	8r
+ —n —1	λ2	( + λ2)
+ 32等H(2+ λr) X1XEk X h(Xi)∕nk2
2	2 t=1 q=1	i=1
By choosing η ≤	18HL√2r∕λ2+8r2∕λ2，	η W	11H"L(2r∕*8r2/2and
1/3
η ≤ MrHO ' ,Zl 2八2、i/3 We can eliminate terms with the multiplicative factor
22LH (2r∕λ2 +4r ∕λ )
PqH=1 Ek Pin=1 hiq(Xti)∕nk2 in the inequality (36):
T-1	T-1	T-1
XE[f(μt+ι)] ≤ XE[f(μt)] - X ηn-EkVf(μt)k2
t=0	t=0	i=0
4L2Hη3(σ2 +4ρ2)T	4LH 2η2 (σ2 +4ρ2)T
n	n2
+ 16η3L2H3T(σ2 +4的(2r∕λ2 + 8r2∕λ2)
n
+ 512L4H3η5T(σ2 +4ρ2) a”？ + 8r2∕λ2)
n2
+ 512L3H 4η4(σ2 +4ρ2)T	+M2 2).
n2
After rearranging terms and dividing by ηTH we get that
PM1 "t)k2 ≤ ɪE[f(μο) - f(μt)] + 4L2η2(σ2 +4ρ?) +
T	ηTH	n
+ 16η2L2H2 (σ2 + 4ρ2)(2r∕λ2 + 8r2∕λ22)
+ 512L4H2η4 (σ2 + 4ρ2)(2r∕λ2 + 8r2 ∕λ22)
+ 5123H3η3(σ2 +4ρ2) (2r∕λ2 +8r2∕λ2).
n
35
Under review as a conference paper at ICLR 2021
Next We use η ≤ 1/n and η ≤ 6HL:
pT=01 EkVf(μt)k2	n	4L2η(σ2 +4ρ2)	4LHη(σ2 +4ρ2)
--------τ--------≤ ητH E[f (μ0)- f (μt)] +----n-----+---------n------
十 竺ηLHQU (2r∕λ2 +4r2∕λ2)
n
+ 384L4H2η%σ2 +4ρ2) (2r∕λ2 + 8r2∕λ2)
n
384L3H3η3σ2
+---------L— (2r∕λ2 + 8r2∕λ2)
n
V n	、	4L ʌi , 4L2η(σ2 + 4ρ2)	4LHη(σ2 +4ρ2)
≤ ητΗ E[f (μ0)- f (μt)] +-----n----+--------n-------
+ 16nL2H 2(σ2 +4ρ2)	+8r2∕λ2)
n
+ 15L2旅2 +4ρ2)	+8r2∕λ2)
n
15LHησ2	2 2
+---------(2r∕λ2 + 8r2 ∕λ2)
n
Recall that n =为 to get:
PT01 EkVfMk2 ≤ 焉E[f(μo) - f(μt)] + 4L+ 4LH展4ρ2)
T	TH	T	T
16L2H2 (σ2 + 4ρ2)
+------√T + " ) (2r∕λ2 + 8r2∕λ2)
15L2 (σ2 + 4ρ2 )	2 2	15LH σ2	2 2
+ ——疗 (2"尢 + 8r2∕λ2) +	√t	(2”尢 + 8r2∕λ2)
V	1	、	f( X1 376H2 max(1,L2)。2 +4ρ2)	r2
≤ √THE[f (μo) - f (μt)] +------√T-----------(Λ∣ +1)
V	1	、	"*∖1∣ 376H2 max(1,L5σ2 + 4P2)	∕r2∣ι∖
≤ √THE[f(μo) - f(x )] +--------√T-----------(λ2 + 1).
where in the last step we used f (μt) ≥ f (x*). Notice that all assumptions and upper bounds on n
are satisfied if
V	1
2 — 240nH max(1, L)(鼻 + 1)，
(37)
which is true
2
T ≥ 57600n4H2 max(1, L2)(—2 + 1)2.	(38)
λ2
□
I Additional Experimental Results
We validated our analysis, by applying the algorithm to training deep neural networks for image
classification and machine translation.
Target System and Implementation. We run SwarmSGD on the CSCS Piz Daint supercomputer,
which is composed of Cray XC50 nodes, each with a Xeon E5-2690v3 CPU and an NVIDIA Tesla
P100 GPU, using a state-of-the-art Aries interconnect. Please see (Piz, 2019) for hardware details.
36
Under review as a conference paper at ICLR 2021
(a) Convergence of ResNet50/ImageNet versus
number of gradient steps. SwarmSGD is able to
recover the baseline top accuracy.
ResNet18 on ImageNet. All variants recover the target
accuracy, but we note the lower convergence of variants
with more local steps.
Figure 3: Additional convergence results for ImageNet dataset.
We implemented SwarmSGD in Pytorch and TensorFlow using NCCL/MPI respectively. Basically,
each node implements a computation thread, and a communication thread, each of which stores
a copy of the model. The “live” copy, which is being updated with gradients, is stored by the
computation thread. Periodically, the threads synchronize their two models. When interacting,
the two nodes exchange model information via their communication threads. Our implementation
closely follows the non-blocking Swarm algorithm description.
We used SwarmSGD to train ResNets on the classic CIFAR-10/ImageNet datasets, and a Trans-
former Vaswani et al. (2017) on the WMT17 dataset (English-Germa). The code will be made
available upon publication.
Hyperparameters. The only additional hyperparameter is the total number of epochs we execute
for. Once we have fixed the number of epochs, we do not alter the other training hyperparameters:
in particular, the learning rate schedule, momentum and weight decay terms are identical to sequen-
tial SGD, for each individual model. Practically, if sequential SGD trains ResNet18 in 90 epochs,
decreasing the learning rate at 30 and 60 epochs, then SwarmSGD with 32 nodes and multiplier 2
would 90 * 2/32 ` 5.6 epochs Per node, decreasing the learning rate at 2 and 4 epochs.
Specifically, for the ImageNet experiments, we used the following hyper-parameters. For ResNet18
and ResNet50, we ran for 240 total parallel epochs using 32 parallel nodes. The first communicated
every 3 local steps, whereas the second communicated every 2 local steps. We used the same hyper-
parameters (initial learning rate 0.1, annealed at 1/3 and 2/3 through training, and standard weight-
decay and momentum parameters).
For the WMT17 experiments, we ran a standard Transformer-large model, and executed for 10
global epochs at 16, 32, and 64 nodes. We ran a version with multiplier 1 (i.e. 10/NUM_NODES
epochs per model) and one with multiplier 1.5 (i.e. 15/num_NODES epochs per model) and regis-
tered the BLEU score for each.
Baselines. We consider the following baselines:
•	Data-parallel SGD: Here, we consider both the small-batch (strong scaling) version,
which executes a global batch size of 256 on ImageNet/CIFAR experiments, and the large-
batch (weak-scaling) baseline, which maximizes the batch per GPU. For the latter version,
the learning rate is tuned following Goyal et al. (2017).
•	Local SGD: Stich (2018); Lin et al. (2018) We follow the implementation of Lin et al.
(2018), communicating globally every 5 SGD steps (which was the highest setting which
provided good accuracy on the WMT task).
•	Previous decentralized proposals: We experimented also with D-PSGD Lian et al.
(2017), AD-PSGD Lian et al. (2018), and SGP Assran et al. (2018). Due to computational
constraints, we did not always measure their end-to-end accuracy. Our method matches the
sequential / large-batch accuracy for the models we consider within 1%. We note that the
37
Under review as a conference paper at ICLR 2021
best performing alternative (AD-PSGD) is known to drop accuracy relative to the baselines,
e.g. (Assran et al., 2018).
Results. The accuracy results for ImageNet experiments are given in Table 1 and Figures 3(a)
and 3(b). As is standard, we follow Top-1 validation accuracy versus number of steps.
Figure 4: Average time per batch for previous methods, compared to SwarmSGD, on
ResNet18/ImageNet, across 1000 repetitions with warm-up. Notice that 1) the time per batch of
SwarmSGD stays constant relative to the number of nodes; 2) it is lower than any other method.
This is due to the reduced communication frequency. Importantly, the base value on the y axis of
this graph (0.4) is the average computation time per batch. Thus, everything above 0.4 represents
the average communication time for this model.
Figure 5: Convergence versus time for ResNet18/Imagenet for the SGD baseline vs Swarm, execut-
ing at 32 nodes. We note that Swarm iterates for 2.7× more epochs for convergence, which explains
the similar runtime despite the better scalability of Swarm.
Communication cost. We now look deeper into SwarmSGD’s performance. For this, we examine
in Figure 4 the average time per batch of different methods when executed on our testbed. The
base value on the y axis (0.4s) is exactly the average time per batch, which is the same across all
methods. Thus, the extra values on the y axis equate roughly to the communication cost of each
algorithm. The results suggest that the communication cost can be up to twice the batch cost (SGP
and D-PSGD). Moreover, this cost is increasing when considered relative to the number of workers
(X axis), for all methods except SwarmSGD.
This reduced cost is justified simply because our method reduces communication frequency: it com-
municates less often, and therefore the average cost of communication at a step is lower. We can
therefore conclude that our method is scalable, in the sense that its communication cost remains
constant relative to the total size of the system. Figure 3(b) shows the convergence versus time for
ResNet18 on the ImageNet dataset, at 32 nodes, with 3 local steps per node, and 〜7 epochs per
model.
Convergence versus Steps and Epochs. Figure 8 shows and discusses the results of additional
ablation studies with respect to the number of nodes/processes and number of local steps / total
epochs on the CIFAR-10 dataset / ResNet20 model. In brief, the results show that the method still
38
Under review as a conference paper at ICLR 2021
model) for CIFAR-10/ResNet20, at node counts
between 8 and 256. We note that the algorithm
converges and recovers SGD accuracy (91.35%
Top-1) for all node counts, although there are
oscillations at high node counts.
CIFAR-10/ResNet20. The original schedule for this
model has 300 epochs, and this experiment is executed
on 8 nodes. If the convergence scaling were perfect,
300/8 = 37.5 epochs would have been sufficient to
converge. However, in this case we need an epoch mul-
tiplier of 2, leading to 75 epochs to recover full accu-
racy (which in this case is 91.35%).
Figure 6: Additional convergence results for CIFAR-10 dataset, versus number of nodes (left), and
local steps (right).
Figure 7: Objective loss versus time for the Transformer-XL/WMT experiment, for various methods,
executing at 16 nodes.
preserves convergence even at very high node counts (256), and suggest a strong correlation between
accuracy and the number of epochs executed per model. The number of local steps executed also
impacts accuracy, but to a much lesser degree.
Quantization. Finally, we show convergence and speedup for a WideResNet-28 model with width
factor 2, trained on the CIFAR-10 dataset. We note that the epoch multiplier factor in this setup is
1, i.e. Swarm (and its quantized variant) execute exactly the same number of epochs as the baseline.
Notice that the quantized variant provides approximately 10% speedup in this case, for a < 0.3%
drop in Top-1 accuracy.
39
Under review as a conference paper at ICLR 2021
(a) Convergence versus number of steps for the quan-
tized variant.
Figure 8: Convergence results for quantized 2xResNet28 trained on the CIFAR-10 dataset, versus
iterations (left), and time (right).
40