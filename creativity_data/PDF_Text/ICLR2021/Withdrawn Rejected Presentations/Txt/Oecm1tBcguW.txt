Under review as a conference paper at ICLR 2021
Meta-Learning Bayesian Neural Network
Priors Based on PAC-Bayesian Theory
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian deep learning is a promising approach towards improved uncertainty
quantification and sample efficiency. Due to their complex parameter space,
choosing informative priors for Bayesian Neural Networks (BNNs) is challenging.
Thus, often a naive, zero-centered Gaussian is used, resulting both in bad
generalization and poor uncertainty estimates when training data is scarce. In
contrast, meta-learning aims to extract such prior knowledge from a set of related
learning tasks. We propose a principled and scalable algorithm for meta-learning
BNN priors based on PAC-Bayesian bounds. Whereas previous approaches require
optimizing the prior and multiple variational posteriors in an interdependent
manner, our method does not rely on difficult nested optimization problems, and
moreover, it is agnostic to the variational inference method in use. Our experiments
show that the proposed method is not only computationally more efficient but also
yields better predictions and uncertainty estimates when compared to previous
meta-learning methods and BNNs with standard priors.
1	Introduction
Bayesian Neural Networks (BNNs) offer a probabilistic interpretation of deep learning by inferring
distributions over the model’s weights (Neal, 1996). With the potential of combining the scalability
and performance of neural networks (NNs) with a framework for uncertainty quantification, BNNs
have lately received increased attention (Blundell et al., 2015; Gal & Ghahramani, 2016). In particular,
their ability to express epistemic uncertainty makes them highly relevant for applications such as active
learning (Hemandez-Lobato & Adams, 2015) and reinforcement learning (Riquelme et al., 2018).
However, BNNs face two major issues: 1) the intractability of posterior inference and 2) the difficulty
of choosing good Bayesian priors. While the former has been addressed in an extensive body
of literature on variational inference (e.g. Blundell et al., 2015; Blei et al., 2016; Mishkin et al.,
2018; Liu & Wang, 2016), the latter has only received limited attention (Vladimirova et al., 2019;
Ghosh & Doshi-Velez, 2017). Choosing an informative prior for BNNs is particularly difficult
due to the high-dimensional and hardly interpretable parameter space of NNs. Due to the lack of
good alternatives, often a zero-centered, isotropic Gaussian is used, reflecting (almost) no a priori
knowledge about the problem at hand. This does not only lead to poor generalization when data is
scarce, but also renders the Bayesian uncertainty estimates poorly calibrated (Kuleshov et al., 2018).
Meta-learning (Schmidhuber, 1987; Thrun & Pratt, 1998) acquires inductive bias in a data-driven way,
thus, constituting an alternative route for addressing this issue. In particular, meta-learners attempt
to extract shared (prior) knowledge from a set of related learning tasks (i.e., datasets), aiming to learn
in the face of a new, related task. Our work develops a principled and scalable algorithm for meta-
learning BNN priors. We build on the PAC-Bayesian framework (McAllester, 1999), a methodology
from statistical learning theory for deriving generalization bounds. Previous PAC-Bayesian bounds for
meta-learners (Pentina & Lampert, 2014; Amit & Meir, 2018) require solving a difficult optimization
problem, involving the optimization of the prior as well as multiple variational posteriors in a nested
manner. Aiming to overcome this issue, we present a PAC-Bayesian bound that does not rely on nested
optimization and, unlike (Rothfuss et al., 2020), can be tractably optimized for BNNs. This makes the
resulting meta-learner, referred to as PACOH-NN, not only much more computationally efficient and
scalable than previous approaches for meta-learning BNN priors (Amit & Meir, 2018), but also agnos-
tic to the choice of approximate posterior inference method which allows us to combine it freely with
recent advances in MCMC (e.g. Chen et al., 2014) or variational inference (e.g. Wang et al., 2019).
1
Under review as a conference paper at ICLR 2021
Our experiments demonstrate that the computational advantages of PACOH-NN do not result in
degraded predictive performance. In fact, across several regression and classification environments,
PACOH-NN achieves a comparable or better predictive accuracy than several popular meta-learning
approaches, while improving the quality of the uncertainty estimates. Finally, we showcase how meta-
learned PACOH-NN priors can be used in a real-world bandit task concerning the development of vac-
cines, suggesting that many other challenging real-world problems may benefit from our approach.
2	Related work
Bayesian Neural Networks. The majority of research on BNNs focuses on approximating the
intractable posterior distribution (Graves, 2011; Blundell et al., 2015; Liu & Wang, 2016; Wang
et al., 2019). In particular, we employ the approximate inference method of Liu & Wang (2016).
Another crucial question is how to select a good BNN prior (Vladimirova et al., 2019). While the
majority of work (e.g. Louizos & Welling, 2016; Huang et al., 2020) employs a simple zero-centered,
isotropic Gaussian, Ghosh & Doshi-Velez (2017) and Pearce et al. (2020) have proposed other prior
distributions for BNNs. In contrast, we go the alternative route of choosing priors in a data-driven way.
Meta-learning. A range of popular methods in meta-learning attempt to learn the “learning program”
in form of a recurrent model (Hochreiter et al., 2001; Andrychowicz et al., 2016; Chen et al., 2017),
learn an embedding space shared across tasks (Snell et al., 2017; Vinyals et al., 2016) or the initializa-
tion of a NN such that it can be quickly adapted to new tasks (Finn et al., 2017; Nichol et al., 2018;
Rothfuss et al., 2019b). A group of recent methods also uses probabilistic modeling to also allow
for uncertainty quantification (Kim et al., 2018; Finn et al., 2018; Garnelo et al., 2018). Although the
mentioned approaches are able to learn complex inference patterns, they rely on settings where meta-
training tasks are abundant and fall short of performance guarantees. In contrast, we provide a formal
assessment of the generalization properties of our algorithm. Moreover, PACOH-NN allows for prin-
cipled uncertainty quantification, including separate treatment of epistemic and aleatoric uncertainty.
This makes it particularly useful for sequential decision algorithms (Lattimore & Szepesvari, 2020).
PAC-Bayesian theory. Previous work presents generalization bounds for randomized predictors, as-
suming a prior to be given exogenously (McAllester, 1999; Catoni, 2007; Germain et al., 2016; Alquier
et al., 2016). More recent work explores data-dependent priors (Parrado-Hernandez et al., 2012; Dziu-
gaite & Roy, 2016) or extends previous bounds to the scenario where priors are meta-learned (Pentina
& Lampert, 2014; Amit & Meir, 2018). However, these meta-generalization bounds are hard to mini-
mize as they leave both the hyper-posterior and posterior unspecified, which leads to nested optimiza-
tion problems. Our work builds on the results of Rothfuss et al. (2020) who introduce the methodology
to derive the closed-form solution of the PAC-Bayesian meta-learning problem. However, unlike ours,
their approach suffers from (asymptotically) non-vanishing terms in the bounds and relies on a closed-
form solution of the marginal log-likelihood. By contributing a numerically stable score estimator
for the generalized marginal log-likelihood, we are able to overcome such limitations, making PAC-
Bayesian meta-learning both tractable and scalable for a much larger array of models, including BNNs.
3	Background: The PAC-Bayesian Framework
Bayesian Neural Networks. Consider a supervised learning task with data S = {(xj, yj)}jm=1
drawn from unknown distribution D. In that, X = {xj}jm=1 ∈ Xm denotes training in-
puts and Y = {yj}jm=1 ∈ Ym the targets. For brevity, we also write zj := (xj , yj) ∈ Z.
Let hθ : X → Y be a function parametrized by a NN with weights θ ∈ Θ. Using
the NN mapping, we define a conditional distribution p(y|x, θ). For regression, we set
p(y∣x, θ) = N(y∖hθ(x),σ2), where σ2 is the observation noise variance. For classification, We
choose p(y|x, θ) = Categorical(softmax(hθ (x))). For Bayesian inference, one presumes a prior
distribution p(θ) over the model parameters θ which is combined with the training data S into a
posterior distribution p(θ∖X, Y) 8 p(θ)p(Y∖X, θ). For unseen test data points x*, we form the
predictive distribution as p(y*∖x*, X, Y) = Rp(y*∖x*,θ)p(θ∖X, Y)dθ.
The Bayesian framework presumes partial knowledge of the data-generating process in form of a prior
distribution. However, due to the practical difficulties in choosing an appropriate BNN prior, the prior
is typically strongly misspecified (Syring & Martin, 2018). As a result, modulating the influence of the
prior relative to the likelihood during inference typically improves the empirical performance of BNNs
and is thus a common practice (Wenzel et al., 2O20). Using such a “tempered" posteriorp「(θ∖X, Y) H
p(θ)p(Y∖X, θ)τ with τ > 0 is also referred to as generalized Bayesian learning (Guedj, 2019).
2
Under review as a conference paper at ICLR 2021
The PAC-Bayesian Framework. In the following, we introduce the most relevant concepts of
PAC-Bayesian learning theory. For more details, we refer to Guedj (2019). Given a loss function
l : θ × Z → R, we typically want to minimize the generalization error L(θ, D) = Ez*〜D l(θ, z*).
Since D is unknown, the empirical error L(θ, S) = ml Pm=I l(θ, Zi) is usually employed, instead.
In the PAC-Bayesian framework, we are concerned with randomized predictors, i.e., probability
measures on the parameter space Θ, allowing us to reason about epistemic uncertainty. In particular,
we consider two such probability measures, a prior P ∈ M(Θ) and a posterior Q ∈ M(Θ). In
here, by M(Θ), we denote the set of all probability measures on Θ. While in Bayesian inference, the
prior and posterior are tightly coupled through Bayes’ theorem, the PAC-Bayesian framework only
requires the prior to be independent of the data S. Using the definitions above, the so-called Gibbs
error for a randomized predictor Q is defined as L(Q, D) = Eh〜Q L(h, D). Similarly, We define its
ʌ ʌ
empirical counterpart as L(Q, S) = Eh〜Q L(h, S). The PAC-Bayesian framework provides upper
bounds for the unknown Gibbs error in the following form:
Theorem 1. (Alquier et al., 2016) Given a data distribution D, a prior P ∈ M(Θ), a confidence
level δ ∈ (0,1], with probability at least 1 一 δ over samples S 〜Dm, we have:
1	1
∀Q ∈ M(Θ):	L(Q,D) ≤ L(Q, S) + √m [Dkl(Q∣∣P) +ln $+ Ψ(沟)]	(1)
where Ψ(√m) = lnEθ〜PEs∈Dm exp [√m ]L(θ, D) — L(θ, S))].
In that, Ψ(√m) is a log moment generating function that quantifies how strong the empirical error
deviates from the Gibbs error. By making additional assumptions about the loss function l, we can
bound Ψ(√m) and thereby obtain tractable bounds. For instance, if l(θ, z) is bounded in [a, b], We
obtain Ψ(√m) ≤ ((b — a)2)∕8 by HOeffding's lemma. For unbounded loss functions, it is common
to assume bounded moments. For instance, a loss is considered sub-gamma with variance factor
s2 and scale parameter c, under a prior P and data distribution D, if its deviations from the mean
can be characterized by random variable V := L(h, D) — l(h, z) whose moment generating function
is upper bounded by that of a Gamma distribution Γ(s, c) (Boucheron et al., 2013). In such case,
we obtain Ψ(√m) ≤ s2∕(2 — √2=)).
Connecting the PAC-Bayesian framework and generalized Bayesian learning. In PAC-
Bayesian learning we aim to find the posterior that minimizes the bound in (1) which is in general a
challenging optimization problem over the space of measures M(Θ). However, to our benefit, it can
be shown that the Gibbs posterior is the probability measure that minimizes (1). For details we refer
to Lemma 2 in the Appendix or Catoni (2007) and (Germain et al., 2016). In particular, this gives us
Q*(θ):= argmin √mL(Q,S) + Dkl(Q∣∣P )= P (θ)e-√mL(θ^/Z (S,P) ,	(2)
Q∈M(Θ)
where Z (S, P ) is a normalization constant. In a probabilistic setting, our loss function is the
negative log-likelihood, i.e. l(θ,Zi) := — logp(zi∣θ). In this case, the optimal Gibbs posterior
coincides with the generalized Bayesian posterior Q*(θ; P, S) H P (θ)p(S ∣θ)1/√m∕Z (S,P) where
Z(S, P) = ʃθ P(θ)p(S∣θ)1/Vm dθ is the generalized marginal likelihood of the data sample S.
4	PAC-Bayesian bounds for Meta-Learning
This section describes the PAC-Bayesian meta-learning setup and discusses how we can obtain
generalization bounds that can be transformed into practically useful meta-learning objectives. In
that, we draw on the methodology of Rothfuss et al. (2020) which allows us to derive a closed form
solution of the PAC-Bayesian meta-learning problem.
In the standard supervised learning setup (see Sec. 3), the learner has prior knowledge in the form
of a prior distribution P, given exogenously, over the hypothesis space H. When the learner faces a
new task, it uses the evidence, provided as a dataset S, to update the prior into a posterior distribution
Q. We formalize such a base learner Q(S, P) as a mapping Q : Zm × M(H) → M(H) that takes
in a dataset and prior and outputs a posterior.
In contrast, in meta-learning we aim to acquire such a prior P in a data-driven manner, that is, by con-
sulting a set of n statistically related learning tasks {τ1, ..., τn}. We follow the setup of Baxter (2000)
in which all tasks τi := (Di, Si) share the same data domain Z := X × Y, parameter space Θ and
loss function l(θ, z), but may differ in their (unknown) data distributions Di and the number of points
3
Under review as a conference paper at ICLR 2021
mi in the corresponding dataset Si 〜Dmi. For our theoretical expositions, We assume w.l.o.g that
m = mi ∀i. Further, each task Ti 〜T is drawn i.i.d. from an environment T, a probability distribu-
tion over data distributions and datasets. The goal is to extract knoWledge from the observed datasets
which can then be used as a prior for learning on new target tasks T 〜T (Amit & Meir, 2018). To
extend the PAC-Bayesian analysis to the meta-learning setting, we again consider the notion of proba-
bility distributions over hypotheses/parameters. However, while the object of learning has previously
been the NN parameters θ, it is now the prior distribution P ∈ M(Θ). Accordingly, the meta-learner
presumes a hyper-prior P ∈ M(M(H)), i.e., a distribution over priors P. Combining the hyper-prior
P with the datasets S1, ..., Sn from multiple tasks, the meta-learner then outputs a hyper-posterior
Q over priors. The hyper-posterior’s performance/quality is measured in form of the expected Gibbs
error when sampling priors P from Q and applying the base learner, the so-called transfer-error:
L(Q,T) := EP〜Q [E(D,m)” [Es〜Dm [L(Q(S,P), D)]]]
While the transfer error is unknown in practice, we can estimate it using the empirical multi-task error
n
L(Q,Sι,...,Sn):=EP〜Q
1
n X L(Q(Si，P )，Si)
i=1
Similar to the PAC-Bayesian guarantees for single task learning, we can bound the transfer error
ʌ
by its empirical counterpart L(Q, S1, ..., Sn) plus several tractable complexity terms:
Theorem 2. Let Q : Zm × M(H) → M(H) be a base learner and P ∈ M(M(H)) some fixed
hyper-prior. For all hyper-posteriors Q ∈ M(M(H)) and δ ∈ (0, 1],
L(Q, T) ≤ L(Q, S1,...,Sn) + n√m+ + √n) DKL(Q∖∖P)
1n 1
+ nΣ √mEP〜Q [DKL(Q(Si)P)∖∖P)] + C(δ, n, m)
∙? = 1 V
(3)
i=1
holds with probability 1 - δ. If the loss function is bounded, that is l : H × Z → [a, b], the above
inequality holds with C(δ, n, m) = (b8√mk) + M-η)——√* ln δ. Ifthe loss function is sub-gamma
with variance factor sI2 and scale parameter cI for data distributions Di and sI2I, cII for the task
22
distribution T, the inequality holds with C(δ, n, m) = 2("_学)+ 2(√nI-C加 一 √1n ln δ .
Under bounded loss assumption, Theorem 2 provides a structurally similar, but tighter bound than
Pentina & Lampert (2014) and Rothfuss et al. (2020). In particular, by using a improved proof
methodology, we are able to forgo a union bound argument, allowing us to reduce the negative
influence of confidence parameter δ. Compared to Rothfuss et al. (2020), the DKL(Q) P) term has
an improved decay rate, that is, 1 /(n√m) + 1/√n as opposed to 1/√m +1/√n. Importantly, the
bound in (3) is consistent, i.e. C(δ, n, m) → 0 as n, m → ∞. Unlike Pentina & Lampert (2014)
and Amit & Meir (2018), the theorem also provides guarantees for unbounded loss functions under
moment constraints (see Appendix A.1 for details). This makes Theorem 2 particularly relevant
for probabilistic models such as BNNs in which the loss function coincides with the inherently
unbounded negative log-likelihood.
Amit & Meir (2018) propose to meta-learn NN priors by directly minimizing a bound similar to (3).
However, the posterior inference for BNNs, i.e. obtaining Qi = Q(Si, P), is a stochastic optimization
problem in itself whose solution in turn depends on P. Hence, minimizing such meta-level bound
w.r.t. P constitutes a computationally infeasible two-level optimization problem. To circumvent
this issue, they jointly optimize P and n approximate posteriors Qi that correspond to the different
datasets Si , leading to an unstable and poorly scalable meta-learning algorithm.
To overcome these issues, we employ the methodology of Rothfuss et al. (2020) and assume the
Gibbs posterior Q*(Si, P) as a base learner. As discussed in Section 3, Q*(Si, P) does not only
constitute a generalized Bayesian posterior but also minimizes the PAC-Bayesian bound. Thus,
the resulting bound in Corollary 1 is tighter than (3). More importantly, the bound can be stated
in terms of the partition function Z(Si, P) which allows us to forgo the explicit reliance on the
task posteriors Qi . This makes the bound much easier to optimize as a meta-learning objective than
previous bounds (e.g. Pentina & Lampert, 2014; Amit & Meir, 2018), since it no longer constitutes
a two-level optimization problem. Moreover, it renders the corresponding meta-learner agnostic
4
Under review as a conference paper at ICLR 2021
to the choice of approximate inference method used to approach the Gibbs/Bayes posterior Q*(S, P)
when performing inference on a new task.
Corollary 1. When choosing the Gibbsposterior Q*(Si,P):= P(θ)exp(-√mL(Si,θ))∕Z(Si, P)
as a base learner, under the same assumptions as in Theorem 2, it holds with probability 1 - δ that
L(Q, T) ≤---X -=e^p>〜Q [ln Z(Si,P)]+ (-J= +-----^) DKL(QIIP) + C(δ, n, m) (4)
ni m	n n m
wherein Z(Si, P) = Eθ〜P
卜xp(-√mL(Si,θ))]
is the generalized marginal log-likelihood.
A natural way to obtain a PAC-Bayesian meta-learning algorithm could be to minimize (4) w.r.t. Q.
Though, in general, this is a hard problem since it would require a minimization over M(M(H)), the
space of all probability measures over priors. Following Rothfuss et al. (2020), we exploit once more
the insight that the minimizer of (4) can be written as Gibbs distribution (c.f. Lemma 2), allowing us
to to derive such minimizing hyper-posterior Q*, i.e. the PACOH, in closed form:
Proposition 1. (PAC-Optimal Hyper-Posterior) Given a hyper-prior P and datasets S1, ..., Sn, the
hyper-posterior Q that minimizes the PAC-Bayesian meta-learning bound in (4) is given by
P(P)eχp (√n1+ι Pn=ιlnZ(Si,p))
Z ii (Sι,...,Sn, P)
Q* (P)
(5)
with the partition function defined as Z ii = EP 〜P 卜XP ( √n1+ι Pn=Iln Z (Si, P))].
We refer to Q* (P) as PAC-optimal since, among all meta-learners, it gives us the best possible
PAC-Bayesian guarantees induced by Theorem 2.
5	PACOH-NN: A s calab le algorithm for learning BNN priors
We discuss our main contribution, that is, how to translate the PACOH (Prop. 1) into a practical
algorithm for meta-learning BNN priors. To this end, we first specify various components of
the generic meta-learning setup presented in Sec. 4 and then discuss how to obtain a tractable
approximation of Q*.
The setup. First, we define a family of priors {Pφ : φ ∈ Φ} over the NN parameters θ. For
computational convenience, we employ Gaussian priors with diagonal covariance matrix, i.e.
Pφ = N(μp,diag(σp)) with φ := (μp,logσp). Note that We represent σp in the log-space
to avoid additional positivity constraints. In fact, any parametric distribution such as normalizing
flows (Rezende & Mohamed, 2015) that allows for re-parametrized sampling and has a tractable
log-density could be used. As typical in the Bayesian framework, our loss function is the negative
log-likelihood, i.e. l(θ, z) = - lnp(yIx, θ) for which we assume an additive Gaussian noise model
p(yIx, θ) = N(y; hθ (x), σy2) in regression and a categorical softmax distribution in case of classi-
fication. Moreover, we use a zero-centered, spherical Gaussian P := N(0, σP2 I) as a hyper-prior
over the parameters φ that specify the prior. In our setup, the hyper-prior acts a form of meta-level
regularization that penalizes complex priors.
Approximating the hyper-posterior. Given the hyper-prior and (level-I) log-partition function
ln Z(Si, P), we can compute the PACOH Q* up to the normalization constant ZII. Such a setup
lends itself to approximate inference methods (Blei et al., 2016). In particular, we employ Stein
Variational Gradient Descent (SVGD) (Liu & Wang, 2016) which approximates Q* as a set of
ʌ
particles Q = {φι,…，φκ}. Initially, the particles φκ 〜P (i.e. the priors, parameters) are sampled
randomly. Subsequently, the method iteratively transports the set of particles to match Q*, by
ʌ
applying a form of functional gradient descent that minimizes DKL(QIQ*) in the reproducing kernel
Hilbert space induced by a kernel function k(∙, ∙). In each iteration, the particles are updated by
φk - φk + ηψ*(φk) with the step size η and
1K
ψ (φ) = Kf [k(φk0 ,φ)Vφk0ln Q (φk0) + Vφk0 k(φk0, φ)] .	(6)
k0 =1
In that, vΦkln Q*(φk) = vΦkln P (φk)+ √nm+1 Pn=I vΦkln Z (Si, Pφk)is the SCOre of q*.
Approximating the generalized marginal log-likelihood. The last remaining issue towards a
viable meta-learning algorithm is the intractable generalized marginal likelihood ln Z(Si, Pφ) =
5
Under review as a conference paper at ICLR 2021
Training with 1 samples Training with 5 samples Training with 10 samples Training with 1 samples Training with 5 samples Training with 10 samples
XXX	XXX
(a) BNN With isotropic, zero-centered Gaussian prior (b) BNN With meta-learned PACOH-NN prior
Figure 1: BNN posterior predictions with (a) standard Gaussian prior vs. (b) meta-learned prior.
Meta-learning with PACOH-NN-SVGD was conducted on the Sinusoids environment.
ln Eθ〜Pφe-√miLN,Si). Estimating and optimizing ln Z(Si, Pφ) is not only challenging due to the
high-dimensional expectation over Θ but also due to numerical instabilities inherent in computing
e-√miL(θ,Si) when m⅛ is large. Aiming to overcome these issues, we compute numerically stable
Monte Carlo estimates of Vφ lnZ(Si,Pφk) by combining the LogSumExp (LSE) with the re-
parametrization trick (Kingma & Welling, 2014). In particular, we draw L samples θl := f(φ, l) =
μp + σp Θ eι, e?〜N(0, I) and compute the forward pass as follows:
1 L
ln Z(Si,Pφ) := ln L X e-√miLC,Si)= LSEL=I (-"£(仇,刈-ln L, θl 〜Pφ	(7)
l=1
The corresponding gradients follow as softmax-weighted average of score gradients:
L
Vφ ln Z(Si,Pφ) = -√mi X
l=1
ʌ .. .
e-√miL(θι,Si)
PL ∖e-√miL(θι,Si)
X---=-----------------
,	,~Γ	^ .	.
Vφf(φ, eι)> VθιL^(θι,Si)
{z'∖∕f^^
softmax
)X--------{----------------{--------}
}	re-param.	score
Jacobian
(8)
Note that ln Z(Si, Pφ) is a consistent but not an unbiased estimator of ln Z(Si, Pφ). The following
proposition ensures us that we still minimize a valid bound (see Appx. B.3 for details).
Proposition 2. In expectation, replacing ln Z(Si, Pφ) in (4) by the estimate ln Z(Si, P) still yields
a valid upper bound of the transfer error L(Q, T) for any L ∈ N.
Moreover, by the law of large numbers, we have that ln Z(Si,P) —→ ln Z(Si,P) as L → ∞, that
is, for large sample sizes L, we recover the original PAC-Bayesian bound in (4). In the opposite
edge case, i.e. L = 1, the boundaries between tasks vanish meaning that the meta-training data
{S1, ..., Sn} is treated as if it were one large dataset Si Si (see Appx. B.3 for further discussion).
The algorithm. Algorithm 1 in Appendix B summarizes the proposed meta-learning method which
we henceforth refer to as PACOH-Nn. To estimate the score Vφk0 ln Q* (φko) in (6), we can even use
mini-batching on the task level. This mini-batched version, outlined in Algorithm 2, maintains K
particles to approximate the hyper-posterior, and in each forward step samples L NN-parameters
(of dimensionaly ∣Θ∣) per prior particle that are deployed on a mini-batch of nbs tasks to estimate
the score of Q*. As a result, the total space complexity is in the order of O(IΘ∣K + L) and the
computational complexity of the algorithm for a single update (c.f. (6)) is O(K2 + KLnbs ).
A key advantage of PACOH-NN over previous methods for meta-learning BNN priors (e.g. Pentina
& Lampert, 2014; Amit & Meir, 2018) is that it turns the previously nested optimization problem
into a much simpler stochastic optimization problem. This makes meta-learning not only much more
stable but also more scalable. In particular, we do not need to explicitly compute / maintain the task
posteriors Qi and can do mini-batching over tasks. As a result, the space and compute complexity
do not depend on the number of tasks n. In contrast, MLAP (Amit & Meir, 2018) has a memory
footprint of O(∣θ∣n) making meta-learning prohibitive for more than 50 tasks.
A central feature of PACOH-NN is that is comes with principled meta-level regularization in form
of the hyper-prior P which combats overfitting to the meta-training tasks (Qin et al., 2018). As we
show in our experiments, this allows us to successfully perform meta-learning with as little as 5
tasks. This is unlike the the majority of the popular meta-learners (Finn et al., 2017; Kim et al., 2018;
Garnelo et al., 2018, e.g.) which rely on a large number of tasks to generalize well on the meta-level
(Qin et al., 2018; Rothfuss et al., 2020).
6	Experiments
We empirically evaluate the method introduced in Section 5, in particular, two variants of the
algorithm: PACOH-NN-SVGD with K = 5 priors as particles and the edge case K = 1 which
6
Under review as a conference paper at ICLR 2021
	Cauchy	SwissFel	Physionet-GCS	Physionet-HCT	Berkeley-Sensor
BNN (LiU & Wang, 2016)	0.327 ± 0.008	0.529 ± 0.022	2.664 ± 0.274	3.938 ± 0.869	0.109 ± 0.004
PACOH-NN-SVGD (OurS) PACOH-NN-MAP (ours)	0.195 ± 0.001 0.202 ± 0.003	0.372 ± 0.002 0.375 ± 0.004	1.561 ± 0.061 1.564 ± 0.200	2.405 ± 0.017 2.480 ± 0.042	0.043 ± 0.001 0.047 ± 0.001
PACOH-GP (ROthfuSS et al., 2020)	0.209 ± 0.008	0.376 ± 0.024	1.498 ± 0.081	2.361 ± 0.047	0.065 ± 0.005
-MLAP-M (Amit & Meir, 2018)- MLAP-S (Amit & Meir, 2018)	0.219 ± 0.002 0.219 ± 0.004	0.492 ± 0.009 0.486 ± 0.026	2.232 ± 0.261 2.009 ± 0.248	2.541 ± 0.140 2.470 ± 0.039	0.052 ± 0.003 0.050 ± 0.005
-FOMAML (Nichol et al., 2018)- MAML (Finn et al., 2017)	0.260 ± 0.007 0.219 ± 0.004	0.897 ± 0.071 0.730 ± 0.057	2.545 ± 0.615 1.895 ± 0.141	2.408 ± 0.064 2.413 ± 0.113	0.059 ± 0.005 0.045 ± 0.003
BMAML (Kim et al., 2018)	0.225 ± 0.004	0.577 ± 0.044	1.894 ± 0.062	2.500 ± 0.002	0.073 ± 0.014
NPS (Garnelo etal.,2018)	0.224 ± 0.008~~	0.471 ± 0.053~~	2.056 ± 0.209-	2.594 ± 0.10T-	0.079 ± 0.00F-
Table 1: Comparison of standard and meta-learning algorithms in terms of test RMSE in 5 meta-
learning environments for regression. Reported are mean and standard deviation across 5 seeds.
	Cauchy	SwissFel	Physionet-GCS	Physionet-HCT	Berkeley-Sensor
BNN (Liu&Wang, 2016)	0.055 ± 0.006	0.085 ± 0.008	0.277 ± 0.013	0.307 ± 0.009	0.179 ± 0.002
PACOH-NN-SVGD (OurS)	0.046 ± 0.001	0.027 ± 0.003	0.267 ± 0.005	0.302 ± 0.003	0.067 ± 0.005
PACOH-NN-MAP (ours)	0.051 ± 0.002	0.031 ± 0.003	0.268 ± 0.015	0.306 ± 0.003	0.063 ± 0.016
PACOH-GP (RothfuSS et al., 2020)	0.056 ± 0.004	0.038 ± 0.006	0.262 ± 0.004	0.296 ± 0.003	0.098 ± 0.005
-MLAP-M (Amit & Meir, 2018)-	0.088 ± 0.004	0.104 ± 0.015	0.339 ± 0.012	0.297 ± 0.007	0.077 ± 0.010
MLAP-S (Amit & Meir, 2018)	0.086 ± 0.015	0.090 ± 0.021	0.343 ± 0.017	0.344 ± 0.016	0.108 ± 0.024
BMAML (Kim et al.,2018)	0.061 ± 0.007	0.115 ± 0.036	0.279 ± 0.010	0.423 ± 0.106	0.161 ± 0.013
NPs (Garnelo et al., 2018)	0.057 ± 0.009-	0.131 ± 0.056-	0.299 ± 0.012-	0.319 ± 0.00T-	0.210 ± 0.00^~
Table 2: Comparison of standard and meta-learning algorithms in terms of test calibration error in 5
meta-learning environments for regression. Reported are mean and standard deviation across 5 seeds.
coincides with a maximum-a-posteriori (MAP) approximation of Q*, thus referred to as PACOH-
NN-MAP. In order to evaluate the quality of the meta-learned prior, i.e. meta-testing, we need to do
approximate the BNN posterior Q*(S, P) for which We use SVGD with 5 particles, too.
Comparing it to various NN-based meta-learning approaches on various regression and classification
environments, we demonstrate that PACOH-NN (i) outperforms previous meta-learning algorithms in
terms of predictive accuracy, (ii) improves the quality of uncertainty estimates and (iii) is much more
scalable than previous PAC-Bayesian meta-learners. Finally, we showcase how meta-learned PACOH-
NN priors can be harnessed in a real-world bandit task concerning peptide-based vaccine development.
6.1	Meta-Learning BNN priors for regression and classification
Figure 1 illustrates BNN predictions on a sinusoidal regression task with a standard Gaussian prior as
well as a PACOH-NN prior meta-learned on sinusoidal functions of varying amplitude, phase and
slope (details can be found in Appendix C.1). In Figure 1a we can see that the standard Gaussian
prior provides poor inductive bias, not only leading to bad mean predictions away from the testing
points but also to poor 95% confidence intervals (blue shaded areas). In contrast, the meta-learned
PACOH-NN prior encodes useful inductive bias towards sinusoidal function shapes, leading to good
predictions and uncertainty estimates, even in face of minimal training data (i.e. 1 training point).
Meta-learning benchmark. In the following, we present a comprehensive benchmark study. First,
we use a BNN with a zero-centered, spherical Gaussian prior and SVGD posterior inference (Liu
& Wang, 2016) as a baseline. Second, we compare our proposed approach against various popular
meta-learning algorithms, including model-agnostic meta-learning (MAML) (Finn et al., 2017), its
first-order version (FOMAML) (Nichol et al., 2018), Bayesian MAML (BMAML) (Kim et al., 2018)
and two variants of the PAC-Bayesian approach by Amit & Meir (2018) (MLAP). For experiments
with regression tasks, we also include into our comparison neural processes (NPs) (Garnelo et al.,
2018) and the GP based meta-learner of Rothfuss et al. (2020) (PACOH-GP). The latter, is similar to
our method as it also approximates a form of the PAC-optimal Hyper-Posterior with SVGD. However,
unlike PACOH-NN it uses Gaussian Processes (GPs) as base learners and relies on a closed-form
marginal log-likelihood. Among all, MLAP is the most similar to our approach as it is neural network
based and minimizes PAC-Bayesian bounds of the transfer error. Though, unlike PACOH-NN, it
relies on nested optimization of the task posteriors Qi and the hyper-posterior Q.
Regression environments. In our experiments, we consider one synthetic and four real-world meta-
learning environments for regression. As a synthetic environment we follow Rothfuss et al. (2020),
employing a 2-dimensional mixture of Cauchy distributions plus random GP functions. As real-world
environments, we use datasets corresponding to different calibration sessions of the Swiss Free
Electron Laser (SwissFEL) (Milne et al., 2017; Kirschner et al., 2019b), as well as data from the
PhysioNet 2012 challenge, which consists of time series of electronic health measurements from
7
Under review as a conference paper at ICLR 2021
	Accuracy		Calibration error	
	Omniglot 2-shot	Omniglot 5-shot	Omniglot 2-shot	Omniglot 5-shot
BNN (Liu&Wang,2016)	0.6709 ± 0.006	0.795 ± 0.006	0.173 ± 0.009	0.135 ± 0.009
PACOH-NN-SVGD (ours) PACOH-NN-MAP (ours)	0.733 ± 0.009 0.735 ± 0.010	0.885 ± 0.090 0.866 ± 0.005	0.094 ± 0.004 0.099 ± 0.009	0.091 ± 0.010 0.075 ± 0.006
MLAP-M (Amit & Meir, 2018) MLAP-S (Amit & Meir, 2018)	0.635 ± 0.015 0.615 ± 0.037	0.804 ± 0.0168 0.700 ± 0.0135	0.108 ± 0.008 0.129 ± 0.018	0.119 ± 0.0193 0.108 ± 0.010
FO-MAML (Nichol et al., 2018) MAML (Finn et al.,2017)	0.429 ± 0.047 0.571 ± 0.018	0.590 ± 0.010 0.693 ± 0.013	N/A N/A	N/A N/A
BMAML (Kim et al., 2018)	0.651 ± 0.008~~	0.764 士。025~	0.132 ± 0.00F-	0.191 ± 0.0Γ8~~
Table 3: Comparison of meta-learning algorithms in terms of test accuracy and calibration error on
the Omniglot environment with 2-shot and 5-shot 5-way-classification tasks.
intensive care patients (Silva et al., 2012), in particular the Glasgow Coma Scale (GCS) and the
hematocrit value (HCT). Here, the different tasks correspond to different patients. Moreover, we
employ the Intel Berkeley Research lab temperature sensor dataset (Berkeley-Sensor) (Madden, 2004)
where the tasks require auto-regressive prediction of temperatures measurements corresponding to
sensors installed in different locations of the building. Further details can be found in Appendix C.1.
Table 1 reports the results of our benchmark study in terms of the root mean squared error (RMSE) on
unseen test tasks. Among the approaches, PACOH-NN consistently performs best or is among the best
two methods, demonstrating that the introduced meta-learning framework is not only sound, but also
endows us with an algorithm that works well in practice. Only for low-dimensional and small-scale
regression environments like Physionet, we find that the GP-based meta-learner PACOH-GP which
is build on a similar theoretical foundation as our method works better than PACOH-NN.
Further, we hypothesize that by acquiring the prior in a principled data-driven manner (e.g., with
PACOH-NN), we can improve the quality of the BNN’s uncertainty estimates. To investigate the effect
of meta-learned priors on the uncertainty estimates of the BNN, we compute the probabilistic predic-
tors’ calibration error, reported in Table 2. The calibration error measures the discrepancy between
predicted confidence regions and actual frequencies of test data in the respective areas (Kuleshov
et al., 2018). Note that, since MAML only produces point predictions, the concept of calibration does
not apply to it. First, we observe that meta-learning priors with PACOH-NN consistently improves the
standard BNN’s uncertainty estimates. For meta-learning environments where the task similarity is
high, i.e. SwissFel and Berkeley-Sensor, the improvement is substantial. Surprisingly, while improving
upon the standard BNN in terms of the RMSE, we find that NPs consistently yields worse-calibrated
predictive distributions than the BNN without meta-learning. This may be due to meta-level overfitting
as NPs lack any form of meta-level regularization (cf. Qin et al., 2018; Rothfuss et al., 2020).
Classification environments. We conduct experiments with the multi-task classification environment
Omniglot (Lake et al., 2015), consisting of handwritten letters across 50 alphabets. Unlike previous
work (e.g. Finn et al., 2017) we do not perform data-augmentation and do not re-combine letters of dif-
ferent alphabets, preserving the data’s original structure. In particular, one task corresponds to 5-way
classification of letters within an alphabet. This leaves us with much fewer tasks (i.e. 30 train and 20
test tasks), making the environment more challenging and more interesting for uncertainty quantifica-
tion. This also allows us to include MLAP in the experiment which hardly scales to more than 50 tasks.
In Table 3, we report both the accuracy
and calibration error for 2-shot and 5-shot
classification on test tasks. Again, PACOH-
NN yields the most accurate classification
results and the lowest calibration error.
Note that MAML fails to improve upon the
standard BNN, i.e. demonstrating negative
transfer. This is consistent with previous
work (Qin et al., 2018; Rothfuss et al.,
2020) raising concerns about overfitting
to the meta-training tasks and observing
that MAML requires a large number of
tasks to generalize well. In contrast, by its
very construction on meta-generalization
Figure 2: Scalability comparison of PACOH-NN and
MLAP-S in memory footprint and compute time, as the
number of meta-training task grows.
bounds, PACOH-NN is able to achieve positive transfer even when the meta-training tasks are diverse
and small in number.
8
Under review as a conference paper at ICLR 2021
Scalability. Unlike the MLAP (Amit & Meir, 2018), PACOH-NN does not need to maintain
posteriors Qi for the meta-training tasks and can use mini-batching on the task level. As a result, it
is computationally much faster and more scalable than previous PAC-Bayesian meta-learners. This is
reflected in its computation and memory complexity, discussed in Section 5. Figure 2 showcases this
computational advantage during meta-training with PACOH-NN-MAP and MLAP-S in the Sinusoids
environment with varying number of tasks, reporting the maximum memory requirements, as well
as the training time. While MLAP’s memory consumption and compute time grow linearly and
becomes prohibitively large even for less than 100 tasks, PACOH-NN maintains a constant memory
and compute load as the number of tasks grow.
6.2	Meta-Learning for Bandits - Vaccine Development
We showcase how a relevant real-world application such as vaccine design can benefit from our
proposed method. In particular, the goal is to discover peptide sequences which bind to major
histocompatibility complex class-I molecules (MHC-I). MHC-I molecules present fragments of
proteins from within a cell to T-cells, allowing the immune system to distinguish between healthy
and infected cells. Following the bandit setup of Krause & Ong (2011), each task corresponds to
searching for maximally binding peptides, a vital step in the design of peptide-based vaccines. The
tasks differ in their targeted MHC-I allele, i.e., correspond to different genetic variants of the MHC-I
protein. We use data from Widmer et al. (2010) which contains the binding affinities (IC50values) of
many peptide candidates to the MHC-I alleles. The peptide candidates are encoded as 45-dimensional
feature vector and the binding affinities were standardized.
We use 5 alleles (tasks) to meta-learn a BNN
prior with PACOH-NN and leave the most
genetically dissimilar allele (A-6901) for our
bandit task. In each iteration, the experimenter
(i.e. bandit algorithm) chooses to test one
peptide among the pool of more than 800
candidates and receives its binding affinity as a
reward feedback. In particular, we employ UCB
(Lattimore & Szepesvari, 2020) and Thompson-
Sampling (TS) (Thompson, 1933) as bandit
algorithms, comparing the BNN-based bandits
with meta-learned prior (PACOH-UCB/TS)
against a zero-centered Gaussian BNN prior
Figure 3: MHC-I peptide design task: Regret for
different priors and bandit algorithms. A meta-
learned PACOH-NN prior substantially improves
the regret, compared to a standard BNN/GP prior.
(BNN-UCB/TS) and a Gaussian process (GP-UCB) (Srinivas et al., 2009).
Figure 3 reports the respective average regret and simple regret over 50 iterations. Unlike the bandit
algorithms with standard BNN/GP prior, PACOH-UCB/TS reaches near optimal regret within less
than 10 iterations and after 50 iterations still maintains a significant performance advantage. This
highlights the importance of transfer (learning) for solving real-world problems and demonstrates
the effectiveness of PACOH-NN to this end. While the majority of meta-learning methods rely on
a large number of meta-training tasks (Qin et al., 2018), PACOH-NN allows us to achieve promising
positive transfer, even in complex real-world scenarios with only a handful (in this case 5) tasks.
7	Conclusion
Based on PAC-Bayesian theory, we present a novel, scalable algorithm for meta-learning BNN priors,
that overcomes previous issues of nested optimization, by employing the closed-form solution of
the PAC-Bayesian meta-learning problem. Experiments show that our method, PACOH-NN, does not
only come with computational advantages, but also achieves comparable or better predictive accuracy
than several popular meta-learning approaches, while improving the quality of the uncertainty
estimates - a key aspect of our approach. The benefits of our principled treatment of uncertainty
一 showcased in the real-world vaccine development bandit task - are particularly amenable to
interactive machine learning systems. This makes the integration of PACOH-NN with Bayesian
optimization and reinforcement learning a potentially promising avenue to pursue. While our
experiments are limited to diagonal Gaussian priors and SVGD as approximate inference method,
we hope that future work will build on the added flexibility of our framework to possibly explore
more recent approaches in variational inference (e.g. Wang et al., 2019) or consider more expressive
priors such as normalizing flows (Rezende & Mohamed, 2015).
9
Under review as a conference paper at ICLR 2021
References
Pierre Alquier, James Ridgway, Nicolas Chopin, and Yee Whye Teh. On the properties of variational
approximations of Gibbs posteriors. Journal of Machine Learning Research, 2016.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In
International Conference on Machine Learning, 2018.
Marcin Andrychowicz, Misha Denil, Sergio Gomez Colmenarejo, Matthew W Hoffman, David Pfau,
Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent
by gradient descent. arXiv, 2016.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research,
2000.
James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. In International Conference on
Machine Learning, 2013.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
arXiv, 2016.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv, 2015.
Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities : a nonasymptotic
theory of independence. chapter 2.4, pp. 27-30. Oxford University Press, 2013.
Olivier Catoni. PAC-Bayesian supervised classification: the thermodynamics of statistical learning.
arXiv, 2007.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International Conference on Machine Learning, 2014.
Yutian Chen, Matthew W Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P Lillicrap,
Matt Botvinick, and Nando De Freitas. Learning to Learn without Gradient Descent by Gradient
Descent. In International Conference on Machine Learning, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Uncertainty in
Artificial Intelligence, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems, 2018.
Vincent Fortuin, Gunnar Ratsch, and Stephan Mandt. Multivariate time series imputation with
variational autoencoders. arXiv preprint arXiv:1907.04155, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, 2016.
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and
Yee Whye Teh. Neural processes. arXiv, 2018.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory
meets bayesian inference. In Advances in Neural Information Processing Systems, 2016.
Soumya Ghosh and Finale Doshi-Velez. Model selection in bayesian neural networks via horseshoe
priors. arXiv, 2017.
Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems, 2011.
10
Under review as a conference paper at ICLR 2021
Benjamin Guedj. A primer on PAC-Bayesian learning. arXiv, 2019.
Jose MigUel Hemandez-Lobato and Ryan P Adams. Probabilistic backpropagation for scalable
learning of bayesian neural networks. In International Conference on Machine Learning, 2015.
Sepp Hochreiter, A. Steven YoUnger, and Peter R. Conwell. Learning To Learn Using Gradient
Descent. In International Conference on Artificial Neural Networks, 2001.
Chin-Wei HUang, Ahmed ToUati, Pascal Vincent, Gintare Karolina DziUgaite, Alexandre Lacoste,
and Aaron CoUrville. Stochastic neUral network with kronecker flow. In International Conference
on Artificial Intelligence and Statistics, 2020.
TaesUp Kim, Jaesik Yoon, OUsmane Dia, SUngwoong Kim, YoshUa Bengio, and SUngjin Ahn.
Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems,
2018.
Diederik P Kingma and Max Welling. AUto-encoding variational bayes. In International Conference
on Learning Representations, 2014.
Johannes Kirschner, Mojmir Mutny, Nicole Hiller, Rasmus Ischebeck, and Andreas Krause. Adaptive
and safe bayesian optimization in high dimensions via one-dimensional sUbspaces. In International
Conference on Machine Learning, 2019a.
Johannes Kirschner, Manuel Nonnenmacher, Mojmir Mutny, Nicole Hiller, Andreas Adelmann,
Rasmus Ischebeck, and Andreas Krause. Bayesian optimization for fast and safe parameter tuning
of swissfel. In International Free-Electron Laser Conference (FEL2019), 2019b.
Andreas Krause and Cheng S Ong. Contextual gaussian process bandit optimization. In Advances in
Neural Information Processing Systems, 2011.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In International Conference on Machine Learning, 2018.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 2015.
Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2020.
Qiang Liu and Dilin Wang. Stein Variational Gradient Descent: A General Purpose Bayesian
Inference Algorithm. arXiv, 2016.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, 2016.
Samuel Madden. Intel lab data. http://db.csail.mit.edu/labdata/labdata.html,
2004. Accessed: Sep 8, 2020.
David A McAllester. Some PAC-Bayesian theorems. Machine Learning, 1999.
Christopher J Milne, Thomas Schietinger, Masamitsu Aiba, Arturo Alarcon, Jurgen Alex, Alexander
Anghel, Vladimir Arsov, Carl Beard, Paul Beaud, Simona Bettoni, et al. Swissfel: the swiss x-ray
free electron laser. Applied Sciences, 2017.
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and Mohammad Emtiyaz Khan.
Slang: Fast structured covariance approximations for bayesian deep learning with natural gradient.
In Advances in Neural Information Processing Systems, 2018.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, 1996.
Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv,
2018.
Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes
Bounds with Data Dependent Priors. Journal of Machine Learning Research, 2012.
11
Under review as a conference paper at ICLR 2021
Tim Pearce, Russell Tsuchida, Mohamed Zaki, Alexandra Brintrup, and Andy Neely. Expressive
priors in bayesian neural networks: Kernel combinations and periodic functions. In Uncertainty in
Artificial Intelligence, 2020.
Anastasia Pentina and Christoph Lampert. A PAC-Bayesian bound for lifelong learning. In Interna-
tional Conference on Machine Learning, 2014.
Yunxiao Qin, Weiguo Zhang, Chenxu Zhao, Zezheng Wang, Hailin Shi, Guojun Qi, Jingping Shi,
and Zhen Lei. Rethink and redesign meta learning. arXiv, 2018.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows.
International Conference on Machine Learning, 2015.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. In International Conference on
Learning Representations, 2018.
Jonas Rothfuss, Fabio Ferreira, Simon Boehm, Simon Walther, Maxim Ulrich, Tamim Asfour,
and Andreas Krause. Noise Regularization for Conditional Density Estimation. arXiv preprint
arXiv:1907.08982, 2019a.
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. In International Conference on Learning Representations, 2019b.
Jonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes-Optimal Meta-Learning with
PAC-Guarantees. arXiv, 2020.
Juergen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn:
The meta-meta-... hook. PhD thesis, Technische Universitaet Munchen, 1987.
Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting in-hospital
mortality of icu patients: The physionet/computing in cardiology challenge 2012. In Computing in
Cardiology, 2012.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, 2017.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process
optimization in the bandit setting: No regret and experimental design. In International Conference
on Machine Learning, 2009.
Nicholas Syring and Ryan Martin. Calibrating general posterior credible regions. Biometrika, 2018.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 1933.
Sebastian Thrun and Lorien Pratt (eds.). Learning to Learn. Kluwer Academic Publishers, 1998.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems, 2016.
Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel. Understanding priors in
Bayesian neural networks at the unit level. In International Conference on Machine Learning,
2019.
Ziyu Wang, Tongzheng Ren, Jun Zhu, and Bo Zhang. Function space particle optimization for
bayesian neural networks. arXiv, 2019.
Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Swikatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes
posterior in deep neural networks really? arXiv, 2020.
Christian Widmer, Nora C Toussaint, Yasemin Altun, and Gunnar Ratsch. Inferring latent task
structure for multitask learning by multiple kernel learning. BMC bioinformatics, 2010.
12
Under review as a conference paper at ICLR 2021
Appendix
A Proofs and Derivations
A.1 Proof of Theorem 2
Lemma 1. (Change of measure inequality) Let f be a random variable taking values in a set A and
let Xi ,…，Xi be independent random variables, with Xk ∈ A with distribution μk. For functions
gk : A X A → R,k = 1,...,l, let ξk(f) = EXk〜*忆[gk(f, Xk)] denote the expectation of gk under
μk for any fixed f ∈ A. Thenfor any fixed distributions π, ρ ∈ M(A) and any λ > 0, we have that
Ef〜P Xξk(f) — gk(f,Xk) ≤ 1 (DKL(P∣∣∏) + lnEf〜∏ 卜(Pk=Iξk(f)-gk(f,Xk))]).
k=1	λ
(9)
Proof of Theorem 2 To prove the Theorem, we need to bound the difference between transfer
error L(Q, T) and the empirical multi-task error L(Q, S1, ..., Sn). To this end, we introduce an
intermediate quantity, the expected multi-task error:
1n
L(Q, Di,..., Dn)= EP 〜Q — EES 〜Dmi [L(Q(S,P), Di)]	(10)
ni
i=i
In the following we invoke Lemma 1 twice. First, in step 1, we bound the difference between
L(Q, Di, ..., Dn) and L(Q, Si, ..., Sn), then, in step 2, the difference between L(Q, T) and
∙-v
L(Q, Di, ..., Dn). Finally, in step 3, we use a union bound argument to combine both results.
Step 1 (Task specific generalization) First, we bound the generalization error of the observed
tasks τi = (Di, mi), i = 1, ..., n, when using a learning algorithm Q : M × Zmi → M, which
outputs a posterior distribution Qi = Q(Si, P) over hypotheses θ, given a prior distribution P and
a dataset Si 〜Dmi of size m》 In that, We define m := (Pn=I m-1)-i as the harmonic mean of
sample sizes.
In particular, We apply Lemma 1 to the union of all training sets S0 = Sin=i Si With l = Pin=i mi.
Hence, each Xk corresponds to one data point, i.e. Xk = Zij and μk = D%. Further, we set
f = (P, hi, ..., hn) to be a tuple of one prior and n base hypotheses. This can be understood as
a two-level hypothesis, wherein P constitutes a hypothesis of the meta-learning problem and hi a
hypothesis for solving the supervised task τi. Correspondingly, we take ∏ = (Q, Qn) = P ∙ Qn=i P
and ρ =(Q,Qn) = Q∙Qi=ιQi as joint two-level distributions and gk (f, Xk) = nm- l(hi, Zij) as
summand of the empirical multi-task error. We can now invoke Lemma 1 to obtain that (11) and (14)
1n	1n	1
-EEP〜Q [L(Qi, Di)] ≤- E EP〜Q [L(Qi, Si)] + λ (DKL [(Q, Qn)ll(P, Pn)]
n i=i	n i=i	(11)
+ lnEP〜PEh〜P heλ Pn=I(L(h，Di)-L(hS)i )
Using the above definitions, the KL-divergence term can be re-written in the following way:
DKL [(Q,Qn)∣∣(P,Pn)]= EP〜Q ∣Eh〜Qi [ln QPPP⅛ 邛铲口	(12)
P(P) i=i P(h)
n
+ X EP〜Q
i=i
n
DKL (Q∣∣P) + X EP 〜Q [DκL(Qi∣∣P)]	(14)
i=i
13
Under review as a conference paper at ICLR 2021
Using (11) and (14) we can bound the expected multi-task error as follows:
1	1n
L(Q, Di,..., Dn) ≤ L(Q, Si, ...,Sn) + λ Dkl(Q∣∣P ) + λ ∑Ep 〜Q [DκL(Qi∣∣P)]
i=1
+ 1lnEP〜PEh〜P hen Pn=I(L(hd-LSS))]
'------------------------------------}
(15)
{z∖^^^
ΥI(λ)
Step 2 (Task environment generalization) Now, we apply Lemma 1 on the meta-level. For that,
We treat each task as random variable and instantiate the components as Xk = Ti, l = n and μk = T.
Furthermore, we set P = Q, ∏ = P, f = P and gk(f, Xk) = ɪL(Qi, Di). This allows US to bound
the transfer error as
L(Q,T) ≤ L(Q,Di,...,Dn)+ 1 DκL(ρ∣∣∏) + ΥII(β)	(16)
β
wherein ΥII(β) = 1 ln EP〜P heβ Pi=ι E(D，S)-T[L(Q(P,S),D)]-L(Q(P,Si),Di)].
Combining (15) with (16), we obtain
L(Q,T) ≤L(Q,Si,...,Sn)+ (1 + 1) Dkl(Q∖∖P)
1 n	(17)
+ λ ^X EP 〜Q [DKL(Qi∖∖p)]+ YI(X) + Yn(e)
Step 3 (Bounding the moment generating functions)
e(ΥI(λ)+ΥII(β)) =EP〜P hen Pn=I E(d,s)〜τ[L(Q(P,S),D)]-L(Q(P,Si),Di)i 1"
EP〜PEh〜P heλ Pn=I(L(h,Di)-L(h,Si)i 1∕λ
"n	"I 1/e
=EP 〜P Y e( n E(D,S)~T[L(Q(P,S),D))I-L(Q(P,Si),Di)
(18)
-n mi	- 1”
EP〜PEh〜P Y Y enmi(L(h,Di)T(hi,zij))
ii
Case I:	bounded loss
If the loss function l(hi, zij) is bounded in [ak, bk], we can apply Hoeffding’s lemma to each factor
in (18), obtaining:
eγI(λ)+γII(β) ≤ EP〜P eβ2(bk-ak)2	∙ EP〜PEh〜P he蔡(bk-ak)2]"入	(19)
=e( 8n+8nm )(bk-ak )2	(20)
Next, we factor out √n from λ and β, obtaining
1
eΥI(λ)+ΥII(β) = (eΥI(λ√n)+ΥII(β√n)) √n
(21)
Using
ETEd1 …EDnheYI(λ√n)+γπ(β√n)i ≤ e(8√n + ^nm)(bk-ak)2	(22)
we can apply Markov’s inequality w.r.t. the expectations over the task distribution T and data
distributions Di to obtain that
YI(λ) + γII(β) ≤ 7Γ~(bk - ak )2 + 4—— (bk - ak )2-『ln δ
8n	8nm	√n
'--------V---------} '------------
ΨI(β )	ΨII(λ)
(23)
14
Under review as a conference paper at ICLR 2021
with probability at least 1 - δ.
Case II:	sub-gamma loss
First, we assume that, ∀i = 1, ..., n the random variables ViI := L(h, Di) - l(hi, zi,j) are sub-gamma
with variance factor sI2 and scale parameter cI under the two-level prior (P, P) and the respective
data distribution Di . That is, their moment generating function can be bounded by that of a Gamma
distribution Γ(sI2 , cI):
Ez 〜Di EP 〜P Eh 〜P [eγ(L(h,Di)-l(h,z))i ≤ exp Qj-sj Y))	∀Y ∈ (0,1/ci)	(24)
Second, We assume that, the random variable VII := E(d,s)〜t [L(Q(P, S), D)] - L(Q(P) Si), Di)
is sub-gamma with variance factor sI2I and scale parameter cII under the hyper-prior P and the
task distribution T. That is, its moment generating function can be bounded by that of a Gamma
distribution Γ(sI2I,cII):
E(D S)〜TEP〜P [eγ E(D,S)^τ[L(Q(P,S),D)]-L(Q(P,S),D)i ≤ exp ( Y sII ) ∀γ ∈ (0,1/cII)
2(1 -cII γ)
(25)
These tWo assumptions alloW us to bound the expectation of (18) as folloWs:
E heγI(λ)+γII(β)i ≤ exp (—, λs2∖“、) ∙ exp (	，βs2I。，、)	(26)
L	」	∖2nrm(1 — cIλ∕(nm) J ∖2n(1 — CIIe/n))
Next, We factor out √n from λ and β, obtaining
1
eτI(λ)+ΥII(β) = (eΥI(λ√n)+Υπ(β√n)) √n
(27)
Finally, by using Markov’s inequality We obtain that
ΥI(λ) +ΥII(β) ≤
_______λS2________+ —βsI	Lln δ
2nm(1 — cIλ∕(nm)__2n(1 — cIIβ∕n)	√n
×--------V--------} X-------V------}
(28)
ΨI(β)
ΨII(λ)
With probability at least 1 — δ.
To conclude the proof, we choose λ = n√rnm and β = √n.
A.2 Proof of Corollary 1
Lemma 2. (Catoni, 2007) Let A be a set, g : A → R a function, and ρ ∈ M(A) and π ∈ M(A)
probability densities over A. Then for any β > 0 and ∀a ∈ A,
ρ*(a) :
(29)
Z
is the minimizing probability density
arg min βEa 〜P [g(a)] + Dkl(p∣∣∏) ∙
ρ∈M(A)
(30)
15
Under review as a conference paper at ICLR 2021
Proof of Corollary 1 When we choose the posterior Q as the optimal Gibbs posterior Q* :
Q* (Si, P), it follows that
1	1
L(Q, si,…,Sn) + ɪ2 ^T^EP~Q [DκL(Qi ∣∣p力
n =而
n X (EP ~qe%~qJls,Si)i+ ―^ (EP ZQ[DKL(Q*∣∣P)D
i=1 '	V
(31)
(32)
Q*(h)
P(h) 一
(33)
P(h)e-√1m Pm=I l(h,zi)
-P (h)Z (Si,P)-
(34)
1n
1 X
n
i=1
—j=^ (-EP〜Q [ln Z(Si)P用.
√m
(35)
n
n
m
This allows us to write the inequality in (3) as
L(Q,τ) ≤
1 工 1	.
-n Σ m EP ~Q [ln Z(Si，P)]+
i=1 1 i
Dkl(Q∣∣P ) + C(δ,n㈤.
(36)
According to Lemma 2, the Gibbs posterior Q*(Si, P) is the minimizer of (33), in particular ∀P ∈
M(H)Zi = 1,...,n :
P(h)e-√mL(h,Si)	ι ι ι 1
Q*(Si,P )= ( )q 0、— = argmin Ehy L(h,Si) + K Dkl(Q∖∖P ) ∙	(37)
Z(Si,p )	Q∈M(H)	L	」√m
Hence, we can write
L(Q, T)
11
≤--E √r≡EP〜Q [ln Z(Si, P)] +
ny√m
i=1
Dkl(Q∣∣P ) + C(δ,n,m
(38)
1n	1
—EEP~q CminiJ、L(Q,Si) + √≡dkl(q\\p)
n 匕	∣Q∈M(H)	√m
+	+	7≡ + C(δ)n，m)
1n	1
≤— 53 EP〜Q L(Q, Si) +-kDKL(QIIP)
n弋 L 而
+
Dkl(Q∣∣P ) + C(δ,n,m)
ʌ
L(Q,S1,…，Sn) +
G + n⅛) DKL(QIIP)
EP〜Q [DκL(Qi∣∣P)] + C(δ, n, m)
(39)
(40)
(41)
(42)
(43)
(44)
which proves that the bound for Gibbs-optimal base learners in (36) and (4) is tighter than the bound
in Theorem 2 which holds uniformly for all Q ∈ M(H).
n
16
Under review as a conference paper at ICLR 2021
meta-training
target training
target testing
P B→
Meta-
learner
i = 1,...,n
Si —Dj)4~
7 = ι,...,m
Zl ♦
*
♦xk
―(qMp>-
Base
learner
―►(Q~►(h)- -k(iik)
价=1,...,mi
Zj ♦
T
1,..., m*
Figure S1: Overview of our meta-learning framework with environment T, meta-task distributions
Di, target task distribution D, hyper-prior P, hyper-posterior Q, target prior P, target posterior Q,
datasets S, and data points z = (x, y).
A.3 Proof of Proposition 1: PAC-Optimal Hyper-Posterior
An objective function corresponding to (4) reads as
1n
J (Q) = -EQ [√nm+ι Xln Z SiP)
+DKL(Q||P).
(45)
To obtain J (Q), we omit all additive terms from (4) that do not depend on Q and multiply by the
SCalingfaCtOr √mn+ι
SinCe the desCribed transfOrmatiOns are mOnOtOne, the minimizing distributiOn
Of J (Q), that is,
Q* = arg min J(Q),
Q∈M(M(H))
(46)
is alsO the minimizer Of (4). MOre impOrtantly, J(Q) is struCturally similar tO the generiC minimizatiOn
prOblem in (30). HenCe, we Can invOke Lemma 2 with A = M(H), g(a) = - Pin=1 ln Z(Si , P),
β = rX , to show that the optimal hyper-posterior is
q*(P)= P(P)exp (√⅛ Pn=Iln Z(SiiP))
Q ( )=	ZII(S1,...,Sn, P)
(47)
wherein
Z II(S1,...,Sn, P ) = EP 〜P
exp (√n1+ι Xln Z(SiiP))]

TeChniCally, this ConCludes the proof of Proposition 1. However, we want to remark the following
result:
If we Choose Q = Q*, the PAC-Bayes bound in (4) Can be expressed in terms of the meta-level
partition funCtion ZII, that is,
L(QiT)≤-
+	lnZπ(Sii...,Sn, P) + C(δ,n,m) .
(48)
We omit a detailed derivation of (48) sinCe it is similar to the one for Corollary 1.
B PACOH-NN: A scalable algorithm for learning BNN priors
In this seCtion, we summarize and further disCuss our proposed meta-learning algorithm PACOH-NN.
An overview of our proposed framework is illustrated in Figure S1. Overall, it Consists of two stages
meta-training and meta-testing whiCh we explain in more details in the following.
17
Under review as a conference paper at ICLR 2021
B.1 Meta-training
The hyper-posterior distribution Q that minimizes the upper bound on the transfer error is given by
Q*(P)
P(P) exp (√⅛Ξ Pn=Iln Z(Si,PD
(49)
ZII(S1,...,Sn,P)
Provided with a set of datasets S1 , ..., S2, the meta-learner minimizes the respective meta-objective,
in the case of PACOH-SVGD, by performing SVGD on the Q*. Algorithm 1 outlines the required
steps in more detail.
Algorithm 1 PACOH-NN-SVGD: meta-training
Input: hyper-prior P, datasets Si,..., Sn, kernel k(∙, ∙), step size η, number of particles K
{Φ1,…，Φκ} ~ P	// Initialize prior particles
while not converged do
for k = 1, ..., K do
{θι,…，Θl}〜Pφk	// sample NN-parameters from prior
for i = 1, ..., n do
ln Z(Si,Pφk) - LSEL=I (-√m^L(θι,Si)) - ln L // estimate generalized MLL
VφkQ*(φk) - Vφk lnP(φk) + Vnh Pi=I Vφk lnZ(Si,pφk)	〃COmPUteSCOre
∀k ∈	[K]	: φk	— φk	+ K	PK0=ι	hk(φk0,	φk)vφk0 ln Q* (OkO)	+	vφk0 k(φk0, φkR	〃 SVGD
Output: set of priors {Pφ1 , ..., PφK}
■"W
Alternatively, to estimate the score of Vφk Q*(φk) we can use mini-batching at both the task and
the dataset level. Specifically, for a given meta-batch size of nbs and a batch size ofmbs, we get
Algorithm 2.
Algorithm 2 PACOH-NN-SVGD: mini-batched meta-training
Input: hyper-prior P, datasets Si, ..., Sn
Input: kernel function k(∙, ∙), SVGD step size η, number of particles K
{φi,…,φκ} ~ P	// Initialize prior particles
while not converged do
{Ti, ..., Tnbs } ⊆ [n]	// sample nbs tasks uniformly at random
for i = 1, ..., nbs do
∙-v
Si — {zi,…，Zmbs} ⊆ Sτi	// sample mbs datapoints from STi uniformly at random
for k = 1, ..., K do
{θι,…，Θl} ~ Pφk	// sample NN-parameters from prior
for i = 1, ..., nbs do
ln Z(Si,Pφk) — LSEL=I (-√m^L(θι,Si)) - ln L // estimate generalized MLL
∙-v	∙-v
vΦk Q*(φk)— vΦk lnP(φk)+√nm+1 nns Pn=sι vΦk ln Z(SMPΦk) 〃 COmPUteScOre
KK PK=1 [k(OkO, φk )vΦk0 ln Q*(OkO) + vφk0 k(φk0, φk R
φk — φk +
part.
∀k ∈ [K] // update
Output: set of priors {Pφ1 , ..., PφK}
B.2 Meta-testing
The meta-learned prior knowledge is now deployed by a base learner. The base learner is given
∙-v
a training dataset S ~ D pertaining to an unseen task T = (D, m)〜T. With the purpose of
approximating the generalized Bayesian posterior Q*(S, P), the base learner performs (normal)
posterior inference. Algorithm 3 details the steps of the approximating procedure - referred to as
target training - when performed via SVGD. For a data point x*, the respective predictor outputs a
18
Under review as a conference paper at ICLR 2021
probability distribution given asp)(y*∣x*, S) J KL PK=I P* L=IP(y*∣hθk(x*))∙ We evaluate the
quality of the predictions on a held-out test dataset S* 〜D from the same task, in a target testing
phase (see Appendix C.2).
Algorithm 3 PACOH-NN-SVGD: meta-testing
∙-v
Input: set of priors {Pφ1 , ..., PφK}, target training dataset S, evaluation point x*
Input: kernel function k(∙, ∙), SVGD step size ν, number of particles L
for k = 1, ..., K do
{θk,…,θL}〜 Pφk	// initialize NN posterior particles from k-th prior
while not converged do
for l = 1, ..., L do
VθkQ*(θk)) J Vθk lnPφk (θf)) + √m VθkL(l, S)	// compute score
峙 J θk + V PL=Ihk(θιk0,θk )Vθko ln Q*(θk)+ Vθ0 k(θk ,θk )i ∀l ∈ [L]	//update
particles
Output: a set ofNN parameters SkK=1{θ1k ..., θLk }
B.3 Properties of the score estimator
Since the marginal log-likelihood of BNNs is intractable, we have replaced it by a numerically stable
∙-v
Monte Carlo estimator ln Z(Si, Pφ) in (7), in particular
1 L
ln Z(Si,Pφ) :=ln L X e-√mi%θl,Si)= LSEL=I (-√m- L(θl ,S)) - ln L,仇〜Pφ . (50)
l=1
Since the Monte Carlo estimator involves approximating an expectation of an exponential, it is not
unbiased. However, we can show that replacing ln Z(Si, Pφ) by the estimator ln Z(Si, Pφ), we still
minimize a valid upper bound on the transfer error (see Proposition 3).
Proposition 3. In expectation, replacing ln Z(Si , Pφ ) in (4) by the Monte Carlo estimate
ln Z(Si, P ):= ln L PL=I e-√miL(θl,si), θι 〜P still yields an valid upper bound of the transfer
error. In particular, it holds that
1n 1
L(Q)T) ≤----ɪ2 -7≡EP〜Q [ln Z(Si,P)] +
n √m
i=1
Dkl(Q∖∖P )+ C(δ,n,m) (51)
回i,...,Θl〜P [lnZ(Si,P)]]
Dkl(Q∖∖P ) + C(δ,n,m).
(52)
Proof. Firsts, we show that:
「1 L	一
Eθι,...,θL〜P [lnZ(Si,P)] = Eθι,...,θl~p ln L Xe Vz_i"( l, i)
l=1
L
≤ ln L XEθl〜P 卜-匹L(θl'Si)]
l=1
=ln Eθ〜P 卜-际L®Si)i
=lnZ(Si,P)	(53)
which follows directly from Jensen’s inequality and the concavity of the logarithm. Now, Proposition
3 follows directly from (53).	□
19
Under review as a conference paper at ICLR 2021
In fact, by the law of large numbers, it is straightforward to show that as L → ∞,the ln Z(Si, P) -→
ln Z (Si, P), that is, the estimator becomes asymptotically unbiased and we recover the original PAC-
Bayesian bound (i.e. (52) -a-.→s. (51)). Also it is noteworthy that the bound in (52) we get by our
estimator is, in expectation, tighter than the upper bound when using the naive estimator
1L
> 1	1	__ 1 X-Λ 八.
ln Z(S”P):= -√m L fL(θι,Si)仇〜Pφ
l=1
which can be obtained by applying Jensen's inequality to ln Eθ〜p6
[e-√miαθ,Si)]. In the edge CaSe
∙-v
L = 1 our LSE estimator ln Z(Si, P) falls back to this naive estimator and coincides in expectation
with E[ln Z(Si, P)] = -ʌ/mi Eθ〜PL(θι,Si). As a result, we effectively minimize the looser upper
bound
n
L(Q,T)≤ - XEθ〜P 伍(θ,Si)]
n i=1
+
Dkl(Q∣∣p ) + C(δ,n,m).
(54)
Eθ〜P
n mi
熹斗 - ln p(yij |xij, θ)
+
Dkl(Q∣∣P )+ C(δ,n, m)
(55)
As we can see from (55), the boundaries between the tasks vanish in the edge case ofL = -, that is, all
data-points are treated as if they would belong to one dataset. This suggests that L should be chosen
greater than one. In our experiments, we used L = 5 and found the corresponding approximation to
be sufficient.
C Experiments
C.1 Meta-Learning Environments
In this section, we provide further details on the meta-learning environments used in Section 6.
Information about the numbers of tasks and samples in the respective environments can be found in
Table S1.
	Sinusoid	Cauchy	SwissFEL	Physionet	Berkeley
n	20	20	5	100	36
mi	5	20	200	4-24	288
Table S1: Number of tasks n and samples per task mi for the different meta-learning environments.
C.1.1 Sinusoids
Each task of the sinusoid environment corresponds to a parametric function
fa,b,c,β(x) = β * X + a * sin(1.5 * (X - b)) + c ,	(56)
which, in essence, consists of an affine as well as a sinusoid function. Tasks differ in the function
parameters (a, b, c, β) that are sampled from the task environment T as follows:
a 〜U(0.7,1.3), b 〜N(0,0.12),	C 〜N(5.0,0.12), β 〜N(0.5, 0.22) .	(57)
Figure S2a depicts functions fa,b,c,β with parameters sampled according to (57). To draw training
samples from each task, we draw X uniformly fromU(-5, 5) and add Gaussian noise with standard
deviation 0.1 to the function values f (X):
X 〜U(-5, 5) , y 〜N(fa,b,c,β(x), 0.12) .	(58)
20
Under review as a conference paper at ICLR 2021
(a) Sinusoid tasks	(b) Cauchy tasks
Figure S2: Depiction of tasks (i.e., functions) sampled from the Sinusoid and Cauchy task environ-
ment, respectively. Note that the Cauchy task environment is two-dimensional (dim(X) = 2), while
(b) displays a one-dimensional projection.
C.1.2 Cauchy
Each task of the Cauchy environment can be interpreted as a two dimensional mixture of Cauchy
distributions plus a function sampled from a Gaussian process prior with zero mean and SE kernel
function k(x, x0) = exp (IIx-Xɪ) With l = 0.2. The (unnormalized) mixture of Cauchy densities
is defined as:
m(x)
6	3
π ∙(I +||x - μιll22) + π ∙ (1 + ||x - 〃2|船
(59)
with μι = (-1, -1)> and μ2 = (2, 2)>.
Functions from the task environments are sampled as folloWs:
f (x) = m(x) + g(x) , g 〜GP(0, k(x, x0)).
(60)
Figure S2b depicts a one-dimensional projection of functions sampled according to (60). To draw
training samples from each task, we draw x from a truncated normal distribution and add Gaussian
noise with standard deviation 0.05 to the function values f (x):
X := min{max{X, 2}, -3} , X 〜N(0, 2.52) ,	y 〜N(f(x), 0.052) .	(61)
C.1.3 SWISSFEL
Free-electron lasers (FELs) accelerate electrons to very high speed in order to generate shortly
pulsed laser beams with wavelengths in the X-ray spectrum. These X-ray pulses can be used to map
nanometer scale structures, thus facilitating experiments in molecular biology and material science.
The accelerator and the electron beam line of a FEL consist of multiple magnets and other adjustable
components, each of which has several parameters that experts adjust to maximize the pulse energy
(Kirschner et al., 2019a). Due do different operational modes, parameter drift, and changing (latent)
conditions, the laser’s pulse energy function, in response to its parameters, changes across time. As a
result, optimizing the laser’s parameters is a recurrent task.
Overall, our meta-learning environment consists of different parameter optimization runs (i.e., tasks)
on the SwissFEL, an 800 meter long laser located in Switzerland (Milne et al., 2017). A picture
of the SwissFEL is shown in Figure S3. The input space, corresponding to the laser’s parameters,
has 12 dimensions whereas the regression target is the pulse energy (1-dimensional). For details on
the individual parameters, we refer to Kirschner et al. (2019b). For each run, we have around 2000
data points. Since these data-points are generated with online optimization methods, the data are
non-i.i.d. and get successively less diverse throughout the optimization. As we are concerned with
meta-learning with limited data and want to avoid issues with highly dependent data points, we only
take the first 400 data points per run and split them into training and test subsets of size 200. Overall,
21
Under review as a conference paper at ICLR 2021
Figure S3: Accelerator of the Swiss Free-Electron Laser (SwissFEL).
we have 9 runs (tasks) available. 5 of those runs are used for meta-training and the remaining 4 runs
are used for meta-testing.
C.1.4 PhysioNet
The 2012 Physionet competition (Silva et al., 2012) published an open-access dataset of patient stays
on the intensive care unit (ICU). Each patient stay consists of a time series over 48 hours, where up
to 37 clinical variables are measured. The original task in the competition was binary classification of
patient mortality, but due to the large number of missing values (around 80 % across all features), the
dataset is also popular as a test bed for time series prediction methods, especially using Gaussian
processes (Fortuin et al., 2019).
In this work, we treat each patient as a separate task and the different clinical variables as different
environments. We use the Glasgow coma scale (GCS) and hematocrit value (HCT) as environments
for our study, since they are among the most frequently measured variables in this dataset. From the
dataset, we remove all patients where less than four measurements of CGS (and HCT respectively)
are available. From the remaining patients we use 100 patients for meta-training and 500 patients
each for meta-validation and meta-testing. Here, each patient corresponds to a task. Since the number
of available measurements differs across patients, the number of training points mi ranges between 4
and 24.
C.1.5 Berkeley-Sensor
We use data from 46 sensors deployed in different locations at the Intel Research lab in Berkeley
(Madden, 2004). The dataset contains 4 days of data, sampled at 10 minute intervals. Each task
corresponds to one of the 46 sensors and requires auto-regressive prediction, in particular, predicting
the next temperature measurement given the last 10 measurement values. In that, 36 sensors (tasks)
with data for the first two days are use for meta-training and whereas the remaining 10 sensors with
data for the last two days are employed for meta-testing. Note, that we separate meta-training and
-testing data both temporally and spatially since the data is non-i.i.d. For the meta-testing, we use the
3rd day as context data, i.e. for target training and the remaining data for target testing.
C.2 Experimental Methodology
In the following, we describe our experimental methodology and provide details on how the empirical
results reported in Section 6 were generated. Overall, evaluating a meta-learner consists of two phases,
meta-training and meta-testing, outlined in Appendix B. The latter can be further sub-divided into
target training and target testing. Figure S1 illustrates these different stages for our PAC-Bayesian
meta-learning framework.
The outcome of the training procedure is an approximation for the generalized Bayesian posterior
Q* (S, P) (See Appendix ), pertaining to an unseen task T = (D, m) ~ T from which We observe a
dataset S ~ D. In target-testing, we evaluate its predictions on a held-out test dataset S * ~ D from
the same task. For PACOH-NN, NPs and MLAP the respective predictor outputs a probability distri-
bution p(y* |x*, S) for the x* in S*. The respective mean prediction corresponds to the expectation
22
Under review as a conference paper at ICLR 2021
ʌ ~
of p^, that is y = E(y*∣χ* ,S). In the case of MAML, only a mean prediction is available. Based on
the mean predictions, we compute the root mean-squared error (RMSE):
RMSE =
t
Sl	X	(y - y)2 .
I I (x* ,y*)∈S*
(62)
and the calibration error (see Appendix C.2.1). Note that unlike e.g. Rothfuss et al. (2019a) who
report the test log-likelihood, we aim to measure the quality of mean predictions and the quality of
uncertainty estimate separately, thus reporting both RMSE and calibration error.
The described meta-training and meta-testing procedure is repeated for five random seeds that
influence both the initialization and gradient-estimates of the concerned algorithms. The reported
averages and standard deviations are based on the results obtained for different seeds.
C.2. 1 Calibration Error
The concept of calibration applies to probabilistic predictors that, given a new target input xi, produce
a probability distribution p(yi∣Xi) over predicted target values yi. Corresponding to the predictive
density, We denote a predictor,s cumulative density function (CDF) as 户(y7∙ |xj) = J-∞ p(y∖χi)dy.
For confidence levels 0 ≤ qh < ... < qH ≤ 1, we can compute the corresponding empirical frequency
∖{yj ∖ F (yj ∖xj) ≤ qh,j = 1, ..., m}∖
qh =------------------------------------------
(63)
m
based on dataset S = {(xi, yi)}im=1 of m samples. If We have calibrated predictions We Would expect
that ^h → qh as m → ∞. Similar to (Kuleshov et al., 2018), We can define the calibration error as a
function of residuals ^h 一 qh in particular,
1H
calib-err = H £ ∖^h - qh∖ .
h=1
(64)
Note that we while (Kuleshov et al., 2018) reports the average of squared residuals ∖qh, 一 qhι ∖2, we
report the average of absolute residuals ∖^h 一 qh, ∖ in order to preserve the units and keep the calibration
error easier to interpret. In our experiments, we compute (64) with K = 20 equally spaced confidence
levels between 0 and 1.
C.3 Hyper-Parameter Selection
For each of the meta-environments and algorithms, we ran a separate hyper-parameter search to select
the hyper-parameters. In particular, we use the hyperopt1 2 package (Bergstra et al., 2013) which
performs Bayesian optimization based on regression trees. As optimization metric, we employ the
average log-likelihood, evaluated on a separate validation set of tasks.
The scripts for reproducing the hyper-parameter search are included in our code repository2 For
the reported results, we provide the selected hyper-parameters and detailed evaluation results under
[Link will be made added upon acceptance]
C.4 Meta-Learning for Bandits - Vaccine Development
In this section, we provide additional details on the experiment in Section 6.2.
We use data from Widmer et al. (2010) which contains the binding affinities (IC50 values) of
many peptide candidates to seven different MHC-I alleles. Peptides with IC50 > 500nM are
considered non-binders, all others binders. Following Krause & Ong (2011), we convert the IC50
values into negative log-scale and normalize them such that 500nM corresponds to zero, i.e. r :=
一 log10 (IC50) + log10(500) with is used as reward signal of our bandit.
1http://hyperopt.github.io/hyperopt/
2[Link will be made added upon acceptance]
23
Under review as a conference paper at ICLR 2021
Allele	A-0202	A-0203	A-0201	A-2301	A-2402
mi	1446	1442	3088	103	196
Table S2: MHC-I alleles used for meta-training and their corresponding number of meta-training
samples mi .
We use 5 alleles to meta-learn a BNN prior. The alleles and the corresponding number of data points,
available for meta-training, are listed in Table S2. The most genetically dissimilar allele (A-6901) is
used for our bandit task. In each iteration, the experimenter (i.e. bandit algorithm) chooses to test
one peptide among the pool of 813 candidates and receives r as a reward feedback. Hence, we are
concerned with a 813-arm bandit wherein the action at ∈ {1, ..., 813} = A in iteration t corresponds
to testing at-th peptide candidate. In response, the algorithm receives the respective negative log-IC50
as reward r(at).
As metrics, we report the average regret
1T
RTvg= max r(a) — T	r(at)
a∈A	t=1
and the simple regret
simple
RT := ma∈aAx r(a) - t maxT r(at)
To ensure a fair comparison, the prior parameters of the GP for GP-UCB and GP-TS are meta-learned
by minimizing the GP’s marginal log-likelihood on the five meta-training tasks. For the prior, we
use a constant mean function and tried various kernel functions (linear, SE, Matern). Due to the
45-dimensional feature space, we found the linear kernel to work the best. So overall, the constant
mean and the variance parameter of the linear kernel are meta-learned.
24