Under review as a conference paper at ICLR 2021
MixCon: Adjusting the Separability of Data
Representations for Harder Data Recovery
Anonymous authors
Paper under double-blind review
Ab stract
To address the issue that deep neural networks (DNNs) are vulnerable to model
inversion attacks, we design an objective function, which adjusts the separability
of the hidden data representations, as a way to control the trade-off between data
utility and vulnerability to inversion attacks. Our method is motivated by the
theoretical insights of data separability in neural networking training and results
on the hardness of model inversion. Empirically, by adjusting the separability of
data representation, we show that there exist sweet-spots for data separability such
that it is difficult to recover data during inference while maintaining data utility.
1	Introduction
Over the past decade, deep neural networks have shown superior performances in various domains,
such as visual recognition, natural language processing, robotics, and healthcare. However, recent
studies have demonstrated that machine learning models are vulnerable in terms of leaking private
data He et al. (2019); Zhu et al. (2019); Zhang et al. (2020b). Hence, preventing private data from
being recovered by malicious attackers has become an important research direction in deep learning
research.
Distributed machine learning Shokri & Shmatikov (2015); Kairouz et al. (2019) has emerged as an
attractive setting to mitigate privacy leakage without requiring clients to share raw data. In the case
of an edge-cloud distributed learning scenario, most layers are commonly offloaded to the cloud,
while the edge device computes only a small number of convolutional layers for feature extraction,
due to power and resource constraints Kang et al. (2017). For example, service provider trains and
splits a neural network at a “cut layer,” then deploys the rest of the layers to clients Vepakomma
et al. (2018). Clients encode their dataset using those layers, then send the data representations back
to cloud server using the rest of layers for inference Teerapittayanon et al. (2017); Ko et al. (2018);
Vepakomma et al. (2018). This gives an untrusted cloud provider or a malicious participant a chance
to steal sensitive inference data from the output of “cut layer” on the edge device side, i.e. inverting
data from their outputs Fredrikson et al. (2015); Zhang et al. (2020b); He et al. (2019).
In the above distributed learning setup, we investigate how to design a hard-to-invert data represen-
tation function (or hidden data representation function), which is defined as the output of the neural
network’s intermediate layer. We focus on defending data recovery during inference. The goal is
to hide sensitive information and to protect data representations from being used to reconstruct the
original data while ensuring that the resulted data representations are still informative enough for de-
cision making. We use the model inversion attack that reconstructs individual data He et al. (2019);
Zhang et al. (2020b) to evaluate defense performance and model accuracy to evaluate data utility.
The core question here is how to achieve the goal, especially protecting individual data from being
recovered.
We propose data separability, also known as the minimum (relative) distance between (the represen-
tation of) two data points, as a new criterion to investigate and understand the trade-off between data
utility and hardness of data recovery. Recent theoretical studies show that if data points are separable
in the hidden embedding space of a DNN model, it is helpful for the model to achieve good clas-
sification accuracy Allen-Zhu et al. (2019b). However, larger separability is also easier to recover
inputs. Conversely, if the embeddings are non-separable or sometimes overlap with one another, it
is challenging to recover inputs. Nevertheless, the model may not be able to learn to achieve good
performance. Two main questions arise. First, is there an effective way to adjust the separability of
data representations? Second, are there “sweet spots” that make the data representations difficult for
inversion attacks while achieving good accuracy.
1
Under review as a conference paper at ICLR 2021
This paper aims to answer these two questions by learning a feature extractor that can adjust the
separability of data representations embedded by a few neural network layers. Specifically, we pro-
pose to add a self-supervised learning-based novel regularization term to the standard loss function
during training. We conduct experiments on both synthetic and benchmark datasets to demonstrate
that with specific parameters, such a learned neural network is indeed difficult to recover input data
while maintaining data utility.
Our contributions can be summarized as:
•	To the best of our knowledge, this is the first proposal to investigate the trade-off between
data utility and data recoverability from the angle of data representation separability;
•	We propose a simple yet effective loss term, Consistency Loss - MiXCon for adjusting data
separability;
•	We provide the theoretical-guided insights of our method, including a new exponential
lower bound on approximately solving the network inversion problem, based on the Expo-
nential Time Hypothesis (ETH); and
•	We report experimental results comparing accuracy and data inversion results with/without
incorporating MiXCon. We show MiXCon with suitable parameters makes data recovery
difficult while preserving high data utility.
The rest of the paper is organized as follow. We formalize our problem in Section 2. In Section 3, we
present our theoretical insight and introduce the consistency loss. We demonstrate the experiment
results in Section 4. We defer the technical proof and experiment details to Appendix.
2	Preliminary
Distributed learning framework. We consider a distributed learning framework, in which users
and servers collaboratively perform inferences Teerapittayanon et al. (2017); Ko et al. (2018); Kang
et al. (2017). We have the following assumptions: 1) Datasets are stored at the user sides. During
inference, no raw data are ever shared among users and servers; 2) Users and servers use a split
model Vepakomma et al. (2018) where users encode their data using our proposed mechanism to ex-
tract data representations at a cut layer of a trained DNN. Servers take encoded data representations
as inputs and compute outputs using the layers after the cut layer in the distributed learning setting;
3) DNN used in the distributed learning setting can be regularized by our loss function (defined
later).
Threat model. We consider the attack model with access to the shared hidden data representations
during the client-cloud communication process. The attacker aims to recover user data (i.e., pixel-
wise recovery for images in vision task). To quantify the upper bound of privacy leakage under
this threat model, we allow the attacker to have more power in our evaluation. In addition to having
access to extracted features, we allow the attacker to see all network parameters of the trained model.
Problem formulation. Formally, let h : Rd → Rm denote the local feature extractor function,
which maps an input data x ∈ Rd to its feature representation h(x) ∈ Rm. The local feature
extractor is a shallow neural network in our setting. The deep neural network on the server side is
denoted as g : Rm 7→ RC, which performs classification tasks and maps the feature representation
to one of C target classes. The overall neural network f : Rd 7→ RC, and it can be written as
f = g ◦ h.
Our overall objectives are:
•	Learn the feature representation mechanism (i.e. h function) that safeguards information
from unsolicited disclosure.
•	Jointly learn the classification function g, and the feature extraction function h to ensure
the information extracted is useful for high-performance downstream tasks.
3	Consistency Loss for Adjusting Data Separability
To address the issue of data recovery from hidden layer output, we propose a novel consistency loss
in neural network training, as shown in Figure 1. Consistency loss is applied to the feature extractor
2
Under review as a conference paper at ICLR 2021
h to encourage encoding closed but separable representations for the data of different classes. Thus,
the feature extractor h can help protect original data from being inverted by an attacker during
inference while achieving desirable accuracy.
Figure 1: Schematic diagram of our data representation encoding scheme in deep learning pipeline. We show
a simple toy example of classifying data points of triangles, squares, and circles. In embedding space (the
middle block), data representations from different classes are constrained to a small ball with diameter ∆H,
while they are separate from each other at least with distance δh.
Classifier
High-level
features
3.1	Data separation as a guiding tool
Our intuition is to adjust the information in the data representations to a minimum such that down-
stream classification tasks can achieve good accuracy but not enough for data recovery through
model inversion attacks He et al. (2019). The question is, what is the right measure on the amount
of information for successful classification and data security? We propose to use data separability
as the measure. This intuition is motivated by the theoretical results of deep learning. In particular,
•	Over-parameterized deep learning theory — the well separated data requires a narrower
network to train,
•	In-approximation theory — the worse separability of the data, the harder of the inversion
problem.
Definition 3.1 (Data separability). Let δh denote the separability of hidden layer over all pairwise
inputs x1,x2, ∙∙∙ ,xn ∈ Rd, i.e.,
δh := min kh(xi) - h(xj)k2. (controlling accuracy)
i6=j∈[n]
Let S denote a set of pairs that supposed to be close in hidden layer. Let ∆H denote the maximum
distance with respect to that set S
∆H := max kh(xi) - h(xj)k2. (controlling invertibility)
(i,j)∈S
Intuitively, we expect the upper bound on data separability (∆H) relates to the invertibility and the
lower bound on data separability (δh) relates to the accuracy.
Lower bound on data separability implies better accuracy Recent line of deep learning theory
Allen-Zhu et al. (2019b) indicates that data separability is perhaps the only matter fact for learnabil-
ity (at least for overparameterized neural network), leading into the following results.
Theorem 3.2. Allen-Zhu et al. (2019b) Suppose the training data points are separable, i.e., δh >
0. If the width of a L-layer neural network with ReLU gates satisfies m ≥ poly(n, d, L, 1∕δh),
initializing from a random weight matrix W, (stochastic) gradient descent algorithm can find the
global minimum of neural network function f.
Essentially, the above theorem indicates that we can (provably) find global minimum of a neural
network given well separated data, and better separable data points requires narrower neural network
and less running time.
Upper bound on data separability implies hardness of inversion. When all data representation
is close to each other, i.e. ∆H is sufficiently small, we expect the inversion problem is hard. We
3
Under review as a conference paper at ICLR 2021
support this intuition by proving that the neural network inversion problem is hard to approximate
within some constant factor when assuming NP6=RP.1
Existing work Lei et al. (2019) indicates that the decision version of the neural network inversion
problem is NP-hard. However, this is insufficient since it is usually easy to find an approximate
solution, which could leak much information on the original data. It is an open question whether
the approximation version is also challenging. We strengthen the hardness result and show that by
assuming NP6=RP, it is hard to recovery an input that approximates the hidden layer representation.
Our hardness result implies that given hidden representations are close to each other, no polyno-
mial time can distinguish their input. Therefore, it is impossible to recover the real input data in
polynomial time.
Theorem 3.3 (Informal). Assume NP6=RP, there is no polynomial time algorithm that is able to
give a constant approximation to thee neural network inversion problem.
The above result only rules out the polynomial running time recovery algorithm but leaves out the
possibility of a subexponential time algorithm. To further strengthen the result, we assume the
well-known Exponential Time Hypothesis (ETH), which is widely accepted in the computation
complexity community.
Hypothesis 3.4 (Exponential Time Hypothesis (ETH) Impagliazzo et al. (1998)). There is a δ > 0
such that the 3SAT problem cannot be solved in O(2δn) time.
Assuming ETH, we derive an exponential lower bound on approximately recovering the input.
Corollary 3.5 (Informal). Assume ETH, there is no 2o(n1-o(1)) time algorithm that is able to give a
constant approximation to neural network inversion problem.
3.2	CONSISTENCY LOSS — MixCon
Follow the above intuitions, we propose a novel loss term MixCon loss — Lmixcon — to incorporate
in training. MixCon adjusts data separability by forcing the consistency of hidden data represen-
tations from different classes. This additional loss term balances the data separability, punishing
feature representations that are too far or too close to each other. Noting that we choose to mix
data from different classes instead of the data within a class, in order to bring more confusion in
embedding space and potentially hiding data label information2.
MixCon loss Lmixcon: We add consistency penalties to force the data representation of i-th data in
different classes to be similar, while without any overlapping for any two data points.
11 p
LmixcOn := P |C| ∙ (∣C∣ - 1) E E (dist(i, Cl, C2) + β∕dist(i, Ci, C2)).	(1)
A practical choice for the pairwise distance is dist(i, C1, C2) = kh(xi,c1 ) - h(xi,c2)k22 3, where xi,c
is the i-th input data point in class C, p := minc∈C |C|, and β > 0 balances the data separability. Note
that the order i is not fixed for a data point due to random shuffling in the regular training process.
Thus Eq. 1 can nearly perform as all-pair comparisons in training with data shuffling. The first term
punishes large distance while the second term enforces sufficient data separability. In general, we
could replace (dist + β∕dist) by convex functions with asymptote shape on non-negative domain,
that is, function with value reaches infinity on both ends of [0, ∞).
We consider the classification loss
NC
Lclass := -ΣΣyi,c ∙ log(yi,c) (cross entropy)	(2)
i=1 c=1
1The class RP consists of all languages L that have a polynomial-time randomized algorithm A with the
following behavior: If x ∈/ L, then A always rejects x (with probability 1). If x ∈ L, then A accepts x in L
with probability at least 1/2.
2We show the comparison in Appendix D.1.
3In practice, we normalize kh(x)k2 to 1. To avoid division by zero, we can use a positive small (	1)
and threshold distance to the range of [, 1/].
4
Under review as a conference paper at ICLR 2021
where yi ∈ RC is the one-hot representation of true label and ybi = f(xi) ∈ RC is the prediction
score of data i ∈ {1,..., N}. The final objective function is L := Lclass + λ ∙ Lmixcon. We simulta-
neously train h and g, where λ and β are tunable hyper-parameters associated with consistency loss
regularization to adjust separability. We discuss the effect of λ and β in experiments (Section 4).
4	Experimental Results
Our hypothesis in this work is MixCon loss can adjust the separability of data hidden representa-
tions. A moderate reduction of representation separability can still keep data utility while making it
harder to recover from its representation. To show this hypothesis hold, we conduct two experiments.
Synthetic experiments provide a straightforward illustration for the relationship among data separa-
bility, data utility and reversibility. Benchmark experiments are used for image data classification
and recovery evaluation.
4.1	Data recovery model
To empirically evaluate the quality of inversion, we formally define the white-box data recovery
(inversion) model He et al. (2019) used in our experiments. The model aims to solve an optimization
problem in the input space. Given a representation z = h(x) of a testing data point, and a public
function h (the trained network that generates data representations), the inversion model tries to find
the original input x:
x* = arg min L(h(s), z) + α ∙ R(S)	(3)
s
where L is the loss function that measures the similarity between h(s) and z, and R is the regu-
larization term. We specify L and R used in each experiment later. We solve Eq. (3) by iterative
gradient descent.
4.2	Experiments with synthetic data
To allow precise manipulation and straightforward visualization for data separability, our experi-
ments use generated synthetic data with a 4-layer fully-connected network, such that we can control
the dimensionality. In this section, we want to answer the following questions:
Q1 What is the impact of having β in Eq.(1) to bound the smallest data pairwise distance?
Q2 Is feature encoded with MixCon mechanism harder to invert?
Network, data generation and training. We defined the network as
y = q(softmax(f (x))), f (x) = W4 ∙ σ(W3 ∙ σ(W2 ∙ (σ(W1x + bi)) + b?) + b3) + b4
x ∈ R10, W1 ∈ R500×10, W2 ∈ R2×500, W3 ∈ R100×2, W4 ∈ R2×100, b1 ∈ R500, b2 ∈
R2, b3 ∈ R100, b4 ∈ R2. For a vector z, we use q(z) to denote the index i such that |zi| > |zj |,
∀j = i. We initialize each entry of Wk and bk from N(Uk, 1), where Uk 〜 N(0, α) and k ∈
{1, 2, 3, 4}.
We generate synthetic samples (x, y) from two multivariate normal distribution. Positive data are
sampled from N(0, I), and negative data are sampled from N (-1, I), ending up with 800 training
samples and 200 testing samples, where the covariance matrix I is an identity diagonal matrix.
Lmixcon is applied to the 2nd fully-connected layer.
We train the network for 20 epochs with cross-entropy loss and SGD optimizer with 0.1 learning
rate. We apply noise to the labels by randomly flipping 5% of labels to increase training difficulty.
Testing setup. We compare the results under the following settings:
•	Vanilla: training using only Lclass .
•	MixCon: training with MixCon loss with parameters (λ, β)4.
4 λ is the coefficient of penalty and β is balancing term for data separability.
5
Under review as a conference paper at ICLR 2021
(a) Vanilla E = 0
(b) Vanilla
E=20
(e) MixCon
β=0E=0
(f) MixCon
β=0E=20
(c) MixCon (d) MixCon
β = 0.01 E = 0β = 0.01 E = 20
Figure 2: Data hidden representation h(x) ∈ R2 from the 2nd fully-connected layer of synthetic data at
different epoch (E). Two settings of MixCon are given default λ = 0.1 but have different β. Compare to
Vanilla, MixCon squeezes data representations to a smaller space over training. When β = 0, MixCon map all
data to h(x) = (0, 0), which is not learnable.
	Vanilla	default	MixCon β = deeper net	0.01 wider net	default	MiXCon β = deeper net	0 wider net
Train Accuracy (%)	91.5-	88.9	895-	91.5	50.0	50.0	50.0
Test Accuracy (%)	91.5	88.0	88.5	90.5	50.0	50.0	50.0
Table 1: Data utility (accuracy). Vanilla is equivalent to (λ = 0, β = 0). Two MixCon “default” settings both
use λ = 0.1 but vary in β = 0.01 and β = 0. “Deeper”/“Wider” indicate increasing the depth / width of layers
in the network on server side g(x).
We perform model inversion using Eq. (3) without any regularization term R(X) and L is the 'ι-loss
function. Detailed optimization process is listed in Appendix C.1.
Results. To answer Q1 that how β in Eq.(1) affect the smallest data pairwise distance, we visualize
the change of data representations at initial and ending epochs in Figure 2. First, in vanilla training
(Figure 2 a-b), data are dispersively distributed and enlarge their distance after training. The obvious
difference for MixCon training (Figure 2 c-f) is that data representations become more and more
gathering through training. Second, we direct the data utility results of Vanilla and two “default”
MiXCon settings - (λ = 0.1, β = 0.01) and (λ = 0.1, β = 0) to Table 1. When β = 0, MiXCon
achieves chance accuracy only as it encodes all the h(x) to hidden space (0,0) (Figure 2 f). While
having β > 0 balancing the separability, MiXCon achieves similar accuracy as Vanilla.
Based on Theorem 3.2, we further present two strategies to ensure reasonable accuracy while com-
prise of reducing data separability by increasing the depth or the width of the layers g(z), the network
after the layer that is applied Lmixcon. In practice, we add two more fully-connected layers with 100
neurons after the 3nd layer for “deeper” g(x), and change the number of neurons on the 3nd layer to
2048 for “wider” g(x). We show the utility results in Table 1. Using deeper or wider g(z), MiXCon
(λ = 0.1, β = 0.01) improves accuracy. Whereas MiXCon (λ = 0.1, β = 0) fails, because zero data
separability is not learnable no matter how g(z) changes. This gives conformable answer that β is
an important factor to guarantee neural network to be trainable.
To answer Q2, we evaluate the quality of data
recovery using the inversion model. We use
both square error (SE) and cosine similarity
(CS) of x and x* to evaluate the data recov-
ery accuracy. We show the quantitative inver-
sion results in Table 2 with the mean and worst
case values. Higher SE or lower CS indicates
a worse inversion. Apparently, data representa-
tion from MiXCon trained network is more dif-
ficult to recover compared to Vanilla strategy.
	Vanilla	MixCon (0.1,0.01)	MixCon (0.1, 0)
SE	1.92(0.31)	-2.08 (0.37)	2.35 (0.44)
CS	0.169 (0.960)	0.118 (0.939)	0.161 (0.921)
Table 2: Inversion results on synthetic dataset re-
ported in mean(worst) format for the 200 testing sam-
ples. Higher MSE or lower MCS indicates a worse in-
version. (λ, β) denoted in header.
4.3	Experiments with benchmark
DATASETS
In this section, we would like to answer the following questions:
6
Under review as a conference paper at ICLR 2021
(a) MNIST
(b) FashionMNIST
(c) SVHN
Figure 3: Trade-off between data separability and data utility. We show testing accuracy and mean pairwise
distance (data separability) on three datasets with different λ and β . λ and β show complementary effort on
adjusting data separability. A sweet-spot can be found at the (λ, β) resulting in small data separability and high
data utility.
Original Image	VaniUa
ACC= 99.15%
MixCon (1,1e-2) MixCon (1,1e-5) MixCon (1,1e-7) MixCon (0.1,1e-2) MiXCon (10, 1e-2) MixCon (100,1e-2)
ACC=99.00%	ACC=98.94%
ACC=97.97%
ACC=89.80%
ACC=88.78%
ACC=88.05%
ACC=87.80%
ACC=87.57%
ACC=99.04%
ACC=89.76%
ACC=98.33%
ACC=98.04%
ACC=87.51%
ACC=85.96%
ACC=88.08%
ACC=88.32%
ACC=87.37%
ACC=85.72%
ACC=81.96%
ACC=64.52%
(C1)

(C2)
(C3)
(C4)
(C5)
(C6)
(C7)
(C8)
7 7


Figure 4: Qualitative evaluation for image inversion results. (λ, β) settings of MixCon denoted on the header.
The corresponding testing accuracy of each dataset is denoted on the top of each row. Compared to vanilla
training, inversions from the MixCon model are less realistic and distinguishable from the original images
without significant accuracy dropping.
Q3 How does MixCon loss affect data separability and accuracy on image datasets?
Q4 Are there parameters (λ, β) in MixCon (Eq. (1)) to reach a “sweet-spot” for data utility and
the quality of defending data recovery?
Network, datasets and training setup. The neural network architecture used in the experiments
is LeNet5 LeCun (2015)5. Lmixcon is applied to the outputs of the 2nd convolutional layer blocks
of LeNet5. The experiments use three datasets: MNIST LeCun et al. (1998), Fashion-MNIST Xiao
et al. (2017), and SVHN Netzer et al. (2011).
Neural network is optimized using cross-entropy loss and SGD optimizer with learning rate 0.01
for 20 epochs. We do not use any data augmentation or manual learning rate decay. MixCon loss is
applied to the output of 2nd convolutional layer blocks in LeNet5. We use mini-batch training and
each batch contains 40 data points from each class. 6 We train the model with different pairs of (λ, β)
5We change input channel to 3 for SVHN dataset.
6In the regular mini-batch training using data shuffling, when calculating the MixCon loss, we truncate the
size of batch to p|C| and each mini-batch contains an equal number of samples of each class, Here p is the
number of training points of the smallest class and |C| is the number of classes.
7
Under review as a conference paper at ICLR 2021
	MNIST		FashionMNIST		SVHN	
	Vanilla -	MixCon (λ = 1.0, β = 10-4)	Vanilla -	MixCon (λ=1.0,β=10-4)	Vanilla -	MixCon (λ = 0.5, β = 10-4)
Acc (%)	991	98.6	898	88.9	884	88.2
SSIM	0.64 ± 0.11(0.83)	0.14 ± 0.11(0.48)	0.43 ± 0.17(0.78)	0.17 ± 0.09(0.52)	0.76 ± 0.19(0.92)	0.61 ± 0.15(0.84)
PSIM	0.78 ± 0.05(0.88)	0.44 ± 0.07(0.69)	0.71 ± 0.i3(0.92)	0.42 ± 0.08(0.66)	0.69 ± 0.07(0.81)	0.59 ± 0.07(0.72)
Table 3: Quantitative evaluations for image recovery results. For fair evaluation, we match the data utility
(accuracy) for Vanilla and MixCon.Structural similarity index metric (SSIM) and perceptual similarity (PSIM)
are measured on 100 testing samples. Those scores are presented in mean ± std and worst-case (in parentheses)
format. Lower scores indicate harder to invert.
in Eq. (1) for the following testing. Specifically, we vary λ from: {0.01, 0.1, 0.5, 1, 2, 5, 10, 100}
and β from: {10-2,10-3, 10-4, 10-5, 10-6, 10-7, 10-8}.
Testing setup. We record the testing accuracy and pairwise distance of data representation under
each pair of (λ, β) for each dataset. Following a recent model inversion method He et al. (2019), we
define L in Eq. (3) as '2-loss function, R as the regularization term capturing the total variation of
a 2D signal defined as R(a) = Pi,j ((ai+1,j - ai,j)2 + (ai,j+1 - ai,j)2)1/2 . The inversion attack
is applied to the output of 2nd convolutional layer blocks in LeNet5 and find the optimal of Eq. (3)
though SGD optimizer. Detailed optimization process is listed in Appendix C.2.
We use metrics normalized structural similarity index metric (SSIM) Wang et al. (2004) and percep-
tual similarity (PSIM) Johnson et al. (2016) to measure the similarity between the recovered image
and the original image. The concrete definitions of SSIM and PSIM are listed in Appendix C.3.
Results To answer Q3, we plot the complementary effects of λ and β in Figure 3. Note that β
bounds the minimal pairwise of data representations, and λ indicate the penalty power on data sep-
arability given by MixCon. Namely, a larger λ brings stronger penalty of MixCon, which enhances
the regularization of data separability and results in lower accuracy. Meanwhile, with a small β, λ
is not necessary to be very large, as smaller β leads to a smaller bound of data separability, thus
resulting in lower accuracy. Hence, λ and β work together to adjust the separability of hidden data
representations, which can affect on data utility.
To answer Q4, we evaluate the quality of inversion qualitatively and quantitatively through a model
inversion attack defined in “Test setup” paragraph. Specifically, for each private input x, we execute
the inversion attack on hmixcon(x) and hvanilla(x) of testing images. As it is qualitatively shown in
Figure 4, first, the recovered images using model inversion from MixCon training (such as given
(λ, β) ∈ {(1, 1 × 10-7), (10, 1 × 10-2), (100, 1 × 10-2)}) are visually different from the original
inputs, while the recovered images from Vanilla training still look similar to the originals. Second,
with the same λ (Figure 4 column c3-c5), the smaller the β it is, the less similar of the recovered
images to original images. Last, with the same β (Figure 4 column c3 and c6-c8), the larger the λ it
is, the less similar of the recovered images to original images.
Further, we quantitatively measure the inversion performance by reporting the averaged similarity
between 100 pairs of recovered images by the inversion model and their original samples. We se-
lect (λ, β) to match the accuracy 7 of MixCon with Vanilla training (see Accuracy in Table 3), and
investigate if MixCon makes the inversion attack harder. The inverted results (see SSIM and PSIM
in Table 3) are reported in the format of mean ± std and the worst case (the best-recovered data)
similarity in parentheses for each metric. Both qualitative and quantitative results agree with our
hypothesis that 1) adding Lmixcon in network training can reduce the mean pairwise distance (sep-
arability) of data hidden representations; and 2) smaller separability make it more difficult to invert
original inputs. By sweet spot, we can define as the set of (β, λ) that suffers with negligible accu-
racy loss (say within 1%) and the model inversion becomes significantly harder w.r.t computational
complexity or breaks the attack (less similarity to the original input data per se). Thus by visiting
through possible (λ, β), we are able to find a spot, where data utility is reasonable but harder for
data recovery, such as (λ = 100, β = 1e - 2) for MNIST (Figure 4). Thus our proposed method is
helpful if the user is willing to give up on some accuracy in the hope of getting a more robust model.
7Accuracy reduction is within a small tolerance, i.e., 1%.
8
Under review as a conference paper at ICLR 2021
5 Discussion and Conclusion
In this paper, we have proposed and studied the trade-off between data utility and data recovery from
the angle of the separability of hidden data representations in deep neural network. We propose using
MixCon, a consistency loss term, as an effective way to adjust the data separability. Our proposal is
inspired by theoretical data separability results and a new exponential lower bound on approximately
solving the network inversion problem, based on the Exponential Time Hypothesis (ETH).
We conduct two sets of experiments, using synthetic and benchmark datasets, to show the effect of
adjusting data separability on accuracy and data recovery. Our theoretical insights help explain our
key experimental findings: MixCon can effectively adjust the separability of hidden data representa-
tions, and one can find “sweet-spot” parameters for MixCon to make it difficult to recover data while
maintaining data utility. Our experiments are limited to small benchmark datasets in the domain of
image classifications. It will be helpful to conduct experiments using large datasets in multiple do-
mains to further the study of the potential of adjusting data separability of data representations to
trade-off between data utility and data recovery.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems (NeurIPS), pp. 6158-6169, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS, 2019c.
Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Uni-
versity Press, 2009.
Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. Computing a nonnegative matrix
factorization—provably. In STOC, 2012.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning (ICML), pp. 322-332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 8141-8150, 2019b.
Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. In Conference on Learning Theory (COLT), pp. 195-268. PMLR, 2019.
Frank Ban, Vijay Bhattiprolu, Karl Bringmann, Pavel Kolev, Euiwoong Lee, and David P Woodruff.
A ptas for lp-low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Sym-
posium on Discrete Algorithms (SODA), pp. 747-766. SIAM, 2019.
Arnab Bhattacharyya, Suprovat Ghoshal, Karthik C. S., and Pasin Manurangsi. Parameterized in-
tractability of even set and shortest vector problem from gap-eth. In ICALP, pp. 17:1-17:15,
2018.
Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. Neural
Networks, 5(1):117-127, 1992.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networks in near-linear time. arXiv preprint arXiv:2006.11648, 2020.
9
Under review as a conference paper at ICLR 2021
Parinya Chalermsook, Marek Cygan, Guy Kortsarz, Bundit Laekhanukit, Pasin Manurangsi,
Danupon Nanongkai, and Luca Trevisan. From gap-eth to fpt-inapproximability: Clique, domi-
nating set, and more. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science
(FOCS),pp. 743-754. IEEE, 2017.
Sitan Chen, Adam R. Klivans, and Raghu Meka. Learning deep relu networks is fixed-parameter
tractable. arXiv preprint arXiv:2009.13512, 2020.
Rajesh Chitnis, Andreas Emil Feldmann, and Pasin Manurangsi. Parameterized approximation al-
gorithms for bidirected steiner network problems. In ESA, pp. 20:1-20:16, 2018.
Amit Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings of the forty-
eighth annual ACM symposium on Theory of Computing (STOC), pp. 105-117, 2016.
Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf’s. In
Conference on Learning Theory (COLT), pp. 815-830, 2016.
Amit Daniely and Gal Vardi. Hardness of learning neural networks with natural weights. arXiv
preprint arXiv:2006.03177, 2020.
Irit Dinur. Mildly exponential reduction from gap 3sat to polynomial-gap label-cover. In Electronic
Colloquium on Computational Complexity (ECCC), volume 23, 2016.
Irit Dinur. Personal communication. 2017.
Irit Dinur and Pasin Manurangsi. Eth-hardness of approximating 2-csps and directed steiner net-
work. In ITCS, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR, 2019.
Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and
applications of models of computation, pp. 1-19. Springer, 2008.
Amir Erfan Eshratifar, Mohammad Saeed Abrishami, and Massoud Pedram. Jointdnn: an efficient
training and inference engine for intelligent mobile cloud computing services. IEEE Transactions
on Mobile Computing, 2019.
Uriel Feige. Relations between average case complexity and approximation complexity. In Proceed-
ings of the thiry-fourth annual ACM symposium on Theory of computing (STOC), pp. 534-543.
ACM, 2002.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security (CCS), pp. 1322-1333, 2015.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. In ICLR, 2018.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-
mial time. In Conference on Learning Theory (COLT), pp. 1004-1042. PMLR, 2017.
Johan Hastad. On bounded occurrence constraint satisfaction. Information Processing Letters, 74
(1-2):1-6, 2000.
Johan Hastad. Some optimal inapproximability results. Journal ofthe ACM (JACM), 48(4):798-859,
2001.
Johann Hauswald, Thomas Manville, Qi Zheng, Ronald Dreslinski, Chaitali Chakrabarti, and Trevor
Mudge. A hybrid approach to offloading mobile image classification. In 2014 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 8375-8379. IEEE, 2014.
Zecheng He, Tianwei Zhang, and Ruby B Lee. Model inversion attacks against collaborative in-
ference. In Proceedings of the 35th Annual Computer Security Applications Conference, pp.
148-162, 2019.
10
Under review as a conference paper at ICLR 2021
Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. Texthide: Tackling data
privacy in language understanding tasks. In The Conference on Empirical Methods in Natural
Language Processing (Findings of EMNLP), 2020a.
Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes for
private distributed learning. In Internation Conference on Machine Learning (ICML), 2020b.
Russell Impagliazzo, Ramamohan Paturi, and Francis Zane. Which problems have strongly ex-
ponential complexity? In Proceedings. 39th Annual Symposium on Foundations of Computer
Science (FOCS),pp. 653-662. IEEE, 1998.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571-8580, 2018.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European Conference on Computer Vision (ECCV), pp. 694-711. Springer,
2016.
Peter Kairouz, H. Brendan McMahan, et al. Advances and open problems in federated learning,
2019.
Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason Mars, and Lingjia
Tang. Neurosurgeon: Collaborative intelligence between the cloud and mobile edge. ACM
SIGARCH Computer Architecture News, 45(1):615-629, 2017.
B Laekhanukit KCS and P Manurangsi. On the parameterized complexity of approximating domi-
nating set. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing
(STOC), pp. 1283-1296, 2018.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of
halfspaces. Journal of Computer and System Sciences, 75(1):2-12, 2009.
Jong Hwan Ko, Taesik Na, Mohammad Faisal Amir, and Saibal Mukhopadhyay. Edge-host parti-
tioning of deep neural networks with feature space encoding for resource-constrained internet-
of-things platforms. In 2018 15th IEEE International Conference on Advanced Video and Signal
Based Surveillance (AVSS), pp. 1-6. IEEE, 2018.
Jakub Konecny, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed opti-
mization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015.
Yann LeCun. Lenet-5, convolutional neural networks. http://yann.lecun.com/exdb/
lenet, 20(5):14, 2015.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score
sampling for neural networks. In NeurIPS, 2020.
Qi Lei, Ajil Jalal, Inderjit S Dhillon, and Alexandros G Dimakis. Inverting deep generative models,
one layer at a time. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13910-
13919, 2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 8157-8166, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-
tion. In Advances in neural information processing systems (NIPS), pp. 597-607, 2017.
11
Under review as a conference paper at ICLR 2021
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in neural information processing Systems (NeurIPS), pp. 855-863,
2014.
Alice Lucas, Santiago Lopez-Tapia, Rafael Molina, and Aggelos K Katsaggelos. Generative adver-
sarial networks and perceptual losses for video super-resolution. IEEE Transactions on Image
Processing, 28(7):3312-3327, 2019.
Pasin Manurangsi. Almost-polynomial ratio eth-hardness of approximating densest k-subgraph. In
STOC, pp. 954-961. ACM, 2017.
Pasin Manurangsi and Prasad Raghavendra. A birthday repetition theorem and complexity of ap-
proximating dense csps. In ICALP, pp. 78:1-78:15, 2017.
Pasin Manurangsi and Daniel Reichman. The computational complexity of training relu (s). arXiv
preprint arXiv:1810.04207, 2018.
Dana Moshkovitz and Ran Raz. Two-query pcp with subconstant error. In Journal of the ACM
(JACM), volume 57(5), pp. 29. A preliminary version appeared in the Proceedings of The 49th
Annual IEEE Symposium on Foundations of Computer Science (FOCS 2008), 2010.
Karthik Nandakumar, Nalini Ratha, Sharath Pankanti, and Shai Halevi. Towards deep neural net-
work training on encrypted data. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pp. 0-0, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. In IEEE Journal on Selected Areas in
Information Theory. IEEE, 2020.
Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank approximations with
provable guarantees. In Proceedings of the 48th Annual Symposium on the Theory of Computing
(STOC), 2016.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC Conference on Computer and Communications Security (CCS), pp. 1310-1321,
2015.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.
arXiv preprint arXiv:1906.03593, 2019.
Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation with entrywise l1-norm
error. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing
(STOC), pp. 688-701, 2017.
Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In
SODA, 2019.
Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Distributed deep neural net-
works over the cloud, the edge and end devices. In 2017 IEEE 37th International Conference on
Distributed Computing Systems (ICDCS), pp. 328-339. IEEE, 2017.
Luca Trevisan. Non-approximability results for optimization problems on bounded degree instances.
In Proceedings of the thirty-third annual ACM symposium on Theory of computing (STOC), pp.
453-461, 2001.
Paul Vanhaesebrouck, AUrelien BelleL and Marc Tommasi. Decentralized collaborative learning of
personalized models over networks. arXiv preprint arXiv:1610.05202, 2016.
Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health:
Distributed deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564,
2018.
12
Under review as a conference paper at ICLR 2021
Chaoyue Wang, Chang Xu, Chaohui Wang, and Dacheng Tao. Perceptual adversarial networks
for image-to-image transformation. IEEE Transactions on Image Processing, 27(8):4066-4079,
2018.
Tianhao Wang, Yuheng Zhang, and Ruoxi Jia. Improving robustness to model inversion attacks via
mutual information regularization. arXiv preprint arXiv:2009.05241, 2020.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:
from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-
612, 2004.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks. In
ICML, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Ziqi Yang, Bin Shao, Bohan Xuan, Ee-Chien Chang, and Fan Zhang. Defending model inversion
and membership inference attacks via prediction purification. arXiv preprint arXiv:2005.03915,
2020.
Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-
parameterized adversarial training: An analysis overcoming the curse of dimensionality. In
NeurIPS, 2020a.
Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer:
generative model-inversion attacks against deep neural networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 253-261, 2020b.
Kai Zhong, Zhao Song, and Inderjit S. Dhillon. Learning non-overlapping convolutional neural
networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In ICML, 2017b.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, 2019.
13
Under review as a conference paper at ICLR 2021
Roadmap of Appendix The Appendix is organized as follows. We discuss related work in Sec-
tion A. We provide theoretical analysis in Section B. The details of data recovery experiment are in
Section C and additional experiment details are in Section D.
A Related Work
A.1 Hardness and Neural Networks
When there are no further assumptions, neural networks have been shown hard in several different
perspectives. Blum & Rivest (1992) first proved that learning the neural network is NP-complete.
Different variant hardness results have been developed over past decades Klivans & Sherstov (2009);
Daniely (2016); Daniely & Shalev-Shwartz (2016); Goel et al. (2017); Livni et al. (2014); Weng et al.
(2018); Manurangsi & Reichman (2018); Lei et al. (2019); Daniely & Vardi (2020); Huang et al.
(2020b;a). The work of Lei et al. (2019) is most relevant to us. They consider the neural network
inversion problem in generative models and prove the exact inversion problem is NP-complete.
A.2 Data Separability and Neural Network Training
One popular distributional assumption, in theory, is to assume the input data points to be the Gaus-
sian distributions Zhong et al. (2017b); Li & Yuan (2017); Zhong et al. (2017a); Ge et al. (2018);
Bakshi et al. (2019); Chen et al. (2020) to show the convergence of training deep neural networks.
Later, convergence analysis using weaker assumptions are proposed, i.e., input data points are sep-
arable Li & Liang (2018). Following Li & Liang (2018); Allen-Zhu et al. (2019b;c;a); Zhang et al.
(2020a), data separability plays a crucial role in deep learning theory, especially in showing the
convergence result of over-parameterized neural network training. Denote δ is the minimum gap
between all pairs data points. Data separability theory says as long as the width (m) of neural net-
work is at least polynomial factor of all the parameters (m ≥ poly(n, d, 1∕δ)), i.e., n is the number
of data points, d is the dimension of data, and delta is data separability. Another line of work Du
et al. (2019); Arora et al. (2019a;b); Song & Yang (2019); Brand et al. (2020); Lee et al. (2020)
builds on neural tangent kernel Jacot et al. (2018). It requires the minimum eigenvalue (λ) of neural
tangent kernel is lower bounded. Recent work Oymak & Soltanolkotabi (2020) finds the connection
between data-separabiblity δ and minimum eigenvalue λ, i.e. δ ≥ λ∕n2 .
A.3 Distributed Deep Learning System
Collaboration between the edge device and cloud server achieves higher inference speed and lowers
power consumption than running the task solely on the local or remote platform. Typically there
are two collaborative modes. The first is collaborative training, for which training task is distributed
to multiple participants Konecny et al. (2015); Vanhaesebrouck et al. (2016); Kairouz et al. (2019).
The other model is collaborative inference. In such a distributed system setting, the neural network
can be divided into two parts. The first few layers of the network are stored in the local edge device,
while the rest are offloaded to a remote cloud server. Given an input, the edge device calculates the
output of the first few layers and sends it to the cloud. Then cloud perform the rest of computation
and sends the final results to each edge device Eshratifar et al. (2019); Hauswald et al. (2014); Kang
et al. (2017); Teerapittayanon et al. (2017). In our work, we focus on tackling data recovery problem
under collaborative inference mode.
A.4 Model Inversion Attack and Defense
The neural network inversion problem has been extensively investigated in recent years Fredrikson
et al. (2015); He et al. (2019); Lei et al. (2019); Zhang et al. (2020b). As used in this paper, the
general approach is to cast the network inversion as an optimization problem and uses a problem
specified objective. In particular, Fredrikson et al. (2015) proposes to use confidence in prediction
as to the optimized objective. He et al. (2019) uses a regularized maximum likelihood estimation.
Recent work Zhang et al. (2020b) also proposes to use GAN to do the model inversion.
There are very few studies about defenses against model inversion attack. Existing data privacy pro-
tection mechanisms mainly rely on noise injection Fredrikson et al. (2015); Dwork (2008); Abadi
14
Under review as a conference paper at ICLR 2021
et al. (2016) or Homomorphic Encryption Nandakumar et al. (2019). While being able to mitigate at-
tacks, existing methods significantly hinder model performance. Recently MID Wang et al. (2020)
was proposed to limit the information about the model input contained in the prediction, thereby
limiting the ability of an adversary to infer data information from the model prediction. Yang et al.
(2020) proposed to add a purification block following by prediction output, so that the confidence
score vectors predicted by the target classifier are less sensitivity of the prediction to the change of
input data. However, the above two methods target the logit output layer (i.e., performing argmax).
They either require auxiliary information (i.e., knowing attack model) or modifying network struc-
ture (i.e., building variational autoencoder structure for mutual information calculation). In contrast,
our proposed method MixConcan easily and efficiently serve as a plug-in loss to the middle layers
of arbitrarily network structures to defend inversion attack during inference.
B Hardnes s of neural network inversion
B.1	Preliminaries
We first provide the definitions for 3SAT, ETH, MAX3SAT, MAXE3SAT and then state some fun-
damental results related to those definitions. For more details, we refer the reader to the textbook
Arora & Barak (2009).
Definition B.1 (3SAT problem). Given n variables and m clauses in a conjunctive normal form
CNF formula with the size of each clause at most 3, the goal is to decide whether there exists an
assignment to the n Boolean variables to make the CNF formula be satisfied.
Hypothesis B.2 (Exponential Time Hypothesis (ETH) Impagliazzo et al. (1998)). There is a δ > 0
such that the 3SAT problem defined in Definition B.1 cannot be solved in O(2δn) time.
ETH is a stronger notion than NP6= P, and is well acceptable the computational complexity com-
munity. Over the few years, there has been work proving hardness result under ETH for theoretical
computer science problems Chalermsook et al. (2017); Manurangsi (2017); Chitnis et al. (2018);
Bhattacharyya et al. (2018); Dinur & Manurangsi (2018); KCS & Manurangsi (2018) and machine
learning problems, e.g. matrix factorizations Arora et al. (2012); Razenshteyn et al. (2016); Song
et al. (2017); Ban et al. (2019), tensor decomposition Song et al. (2019). There are also variations
of ETH, e.g. Gap-ETH Dinur (2016; 2017); Manurangsi & Raghavendra (2017) and random-ETH
Feige (2002); Razenshteyn et al. (2016), which are also believable in the computational complexity
community.
Definition B.3 (MAX3SAT). Given n variables and m clauses, a conjunctive normal form CNF
formula with the size of each clause at most 3, the goal is to find an assignment that satisfies the
largest number of clauses.
We use MAXE3SAT to denote the version of MAX3SAT where each clause contains exactly 3
literals.
Theorem B.4 (Hastad (2001)). For every δ > 0, it is NP-hard to distinguish a satisfiable instance of
MAXE3SAT from an instance where at most a 7/8 + δ fraction ofthe clauses can be simultaneously
satisfied.
Theorem B.5 (Hastad (2001); Moshkovitz & Raz (2010)). Assume ETH holds. For ^very δ > 0,
there is no 2o(n1-o(1)) time algorithm to distinguish a satisfiable instance of MAXE3SAT from an
instance where at most a fraction 7/8 + δ of the clauses can be simultaneously satisfied.
We use MAXE3SAT(B) to denote the restricted special case of MAX3SAT where every variable
occurs in at most B clauses. Hastad Hastad (2000) proved that the problem is approximable to
within a factor 7/8 + 1/(64B) in polynomial time, and that it is hard to approximate within a factor
7/8+ 1∕(log B)ω(D. In 2001, Trevisan improved the hardness result,
Theorem B.6 (Trevisan (2001)). Unless RP=NP, there is no polynomial time (7/8 + 5∕√B)-
approximate algorithm for MAXE3SAT(B).
Theorem B.7 (HaStad (2001); Trevisan (2001); Moshkovitz & Raz (2010)). Unless ETH fails, there
is no 2o(n1 °⑴) time (7/8 + 5∕√B)-approximate algorithmfor MAXE3SAT(B).
15
Under review as a conference paper at ICLR 2021
B.2	Our results
We provide a hardness of approximation result for the neural network inversion problem. In partic-
ular, we prove unless RP=NP, there is no polynomial time that can approximately recover the input
of a two-layer neural network with ReLU activation function8. Formally, consider the inversion
problem
h(x) = z,	x ∈ [-1, 1]d,	(4)
where z ∈ Rm2 is the hidden layer representation, h is a two neural network with ReLU gates,
specified as
h(x) = W2σ(W1x+b),	W2 ∈Rm2×m1,W1 ∈ Rm1×d,b∈ Rm1
We want to recover the input data x ∈ [-1, 1]d given hidden layer representation z and all parameters
of the neural network (i.e., W(1) , W(2) , b). It is known the decision version of neural network
inversion problem is NP-hard Lei et al. (2019). Itis an open question whether approximation version
is also hard. We show a stronger result which is, it is hard to give to constant approximation factor.
Two notions of approximation could be consider here, one we called solution approximation
Definition B.8 (Solution approximation). Given a neural network h and hidden layer representation
z, we say x0 ∈ [-1, 1]d is an approximation solution for Eq. (4), if there exists x ∈ [-1, 1] ∈ Rd,
such that
∣∣x — x[∣2 ≤ e√d and h(x) = z.
Roughly speaking, solution approximation means We recovery an approximate solution. The √d
factor in the above definition is a normalization factor and it is not essential.
One can also consider a Weaker notion, Which We called function value approximation
Definition B.9 (Function value approximation). Given a neural network h and hidden layer repre-
sentation z, we say x0 ∈ [—1, 1]d is e-approximate of value to Eq. (4), if
I∣h(x0) 一 y∣2 ≤ e√m2.
Again, the √m2 factor is only for normalization. Suppose the neural network is G-LiPSchitz con-
tinuous for constant G (Which is the case in our proof), then an e-approximate solution implies
Ge-approximation of value. For the purpose of this paper, we focus on the second notion (i.e.,
function value approximation). Given our neural network is (constant)-Lipschitz continuous, this
immediately implies hardness result for the first one.
Our theorem is formally stated below. In the proof, we reduce from MAX3SAT(B) and utilize
Theorem B.6
Theorem B.10. There exists a constant B > 1, unless RP = NP, it is hard to 60B-approximate
Eq. (4) . Furthermore, the neural network is O(B)-Lipschitz continuous, and therefore, it is hard to
find an Ω(1∕B2) approximate solution to the neural network.
Using the above theorem, we can see that by taking a suitable constant B > 1, the neural network
inversion problem is hard to approximate within some constant factor under both definitions. In
particular, we conclude
Theorem B.11 (Formal statement of Theorem 3.3). Assume NP 6= RP, there exists a constant
e > 0, such that there is no polynomial time algorithm that is able to give an e-approximation to
neural network inversion problem.
Proof of Theorem B.10. Given an 3SAT instance φ with n variables and m clause, where each vari-
able appears in at most B clauses, we construct a two layer neural network hφ and output represen-
tation z satisfy the following:
•	Completeness. If φ is satisfiable, then there exists x ∈ [0, 1]d such that hφ(x) = z.
8We remark there is a polynomial time algorithm for one layer ReLU neural network recovery
16
Under review as a conference paper at ICLR 2021
•	Soundness. For any X such that ∣∣hφ(χ) - z∣∣2 ≤ 601B√m2, We can recover an assignment
to φ that satisfies at least (7 + √B) m clauses
•	Lipschitz continuous. The neural netWork is O(B)-Lipschitz.
We set d = n, m1 = m + 200B2n and m2 = m + 100B 2 n. For any j ∈ [m], We use φj to
denote the j-th clause and use h1,j(x) to denote the output of the j-th neuron in the first layer, i.e.,
h1,j (x) = σ(Wj(1)x + bi), Where Wj(1) is the j-th roW of W(1). For any i ∈ [n], We use Xi to denote
the i-th variable.
Intuitively, We use the input vector x ∈ [-1, 1]n to denote the variable, and the first m neurons in
the first layer to denote the m clauses. By taking
1,
-1,
0,
Xi ∈ Φj;
Xi ∈ φj; and bj = -2
otherWise.
for any i ∈ [n], j ∈ [m], and vieWing xi = 1 as Xi to be false and xi = -1 as Xi to be true. One
can verify that h1,j (x) = 0 if the clause is satisfied, and h1,j (x) = 1 if the clause is unsatisfied. We
simply copy the value in the second layer hj (x) = h1,j (x) forj ∈ [m].
For other neurons, intuitively, We make 100B2 copies for each |xi| (i ∈ n) in the output layer. This
can be achieved by taking
hm+(i-1)∙100B2 + k (X) = hm+(i-1)∙ 100B2+k (X) + h1,m+100B2n+(i-1)∙ 100B2 + k (X)
and set
h1,m+(i-1)∙ 100B2 + k (X) = maχ{χi, 0} h1,m+100B2n+(i-1)∙ 100B2 + k (x) = max{-Xi, 0}
for any i ∈ [n], k ∈ [100B2]. Finally, We set the target output as
Z =(0,…，0,1,…，1)
m	100B2n
We are left to prove the three claims We made about the neural netWork h and the target output z . For
the first claim, suppose φ is satisfiable and X = (Xi, ∙∙∙ , Xn) is the assignment. Then as argued
before, We can simply take Xi = 1 if Xi is false and Xi = -1 is Xi is true. One can check that
h(X) = z.
For second claim, suppose We are given X ∈ [-1, 1]d such that
llh(X) - zk2 ≤ 61B√m2
We start from the simple case When X is binary, i.e., X ∈ {-1, 1}n. Again, by taking Xi to be true
if Xi = -1 and Xi to be false When Xi = 0. One can check that the number of unsatisfied clause is
at most
lh(X) - zl22
1
≤ ----m2
一 3600B2 2
1
3600B2
1
(m + 100B2 n)
1
≤ 12 m + 3600B2
(5)
≤ 1 m - ɪ
^ 8	√B
The third step folloWs from n ≤ 3m, and the last step folloWs from B ≥ 15000.
Next, We move to the general case that X ∈ [-1, 1]d. We Would round Xi to -1 or +1 based on the
sign. DefineX ∈ {-1,1}n as
Xi = arg min |t - Xi |
t∈{-1,1}
17
Under review as a conference paper at ICLR 2021
We prove that X induces an assignment that satisfies (7 + √√b)m clauses. It suffices to prove
3
kh(X) - zk2- kh(X) - zk2 ≤ loom
since this implies the number of unsatisfied clause is bounded by
kh(x) - zk2 ≤ kh(x) - zk2 + (kh(x) - zk2 -kh(x) - zk2)
≤ (——m +------m) +------m
一'12	36B2 ’	100
≤ —m-------m,
-8	5√B
where the second step follow from Eq. (5)(6), and the last step follows from B ≥ 107.
We define △% := |Xi 一 x/ = 1 一 ∣x∕ ∈ [0,1] and T := m + 128B2n. Then We have
(6)
T
kh(X)-Zk2 -kh(X)-Zk2 = X(hj (X)-Zj)- (hj (X)-Zj )2
j=1
m
=X(hj(X)- Zj)2- (hj(X)- Zj)2
j=1
T
+ X (hj(X)- Zj)2- (hj(X)- Zj)2
j=m+1
mn
=X hj(X)2 - hj(X)2- 100B2 X ∆
j=1	i=1
mn
≤ 2 X ∣hι,j(X)-hι,j(x)|- 100B2 X ∆2
j=1	i=1
mn
≤2XX∆i - 100B2 X ∆i2
j=1 i∈φj	i=1
nn
≤2BX∆i - 100B2 X ∆i2
i=1	i=1
n
≤ 一
一 100
3m
≤ 100.
The third step follow from Zj = 0 for j ∈ [m] and for j ∈ {m + 1, ∙∙∙ ,m + 100B2n}, Zj = 1,
∣∣hj(X) — Zj k = 0 and ∣∣hj(x) — Zjk2 = △% given j ∈ [m + (i — 1) ∙ 100B2 + 1,i ∙ 100B2]. The
fourth step follows from that hj (X) = h1,j (X) ∈ [0, 1] for j ∈ [m]. The fifth step follows from the
1-Lipschitz continuity of the ReLU. The sixth step follows from each variable appears in at most B
clause. This concludes the second claim.
For the last claim, by the Lipschitz continuity of ReLU, we have for any X1, X2
h(X1) - h(X2) = W(2)σ(W(1)X1 + b) - W (2)σ(W (1)X2 + b)
≤∣W⑵k∙kW⑴k∣Xι- X2k2
It is easy to see that
kW(2)k ≤ 2
and
∣∣W(2)k ≤ P200B2 + 3B ≤ √203B2 ≤ 15B,
18
Under review as a conference paper at ICLR 2021
where the second step follows from B ≥ 1.
Thus concluding the proof.
□
By assuming ETH and using Theorem B.7, we can conclude
Corollary B.12 (Formal statement of Corollary 3.5). Unless ETH fails, there exists a constant > 0,
such that there is no 2o(n1-o(1)) time algorithm that is able to give an -approximation to neural
network inversion problem.
The proof is similar to Theorem B.10, we omit it here.
C Details of Data Recovery Experiments
C.1 Inversion Model Details for Synthetic Dataset
In synthetic experiment, a malicious attacker recover original input data x ∈ Rd by solving the the
following optimization:
x* = arg min ∣∣h(s) — z∣∣ι
s∈Rd
To estimate the optimal, we run an SGD optimizer with a learning rate of 0.01 and decayed weight
10-4 for 500 iterations. We test data recovery results on all the 200 testing samples. Namely, we
solve the above optimization problems 200 times. Each time for a testing data point.
C.2 Inversion Model Details for Benchmark Dataset
In benchmark experiment, a malicious attacker recover original input data x ∈ Rd by solving the
the following optimization:
X* = arg min l∣h(s) — z∣2 + Z f((si+ι,j — Si,j )2 + (Sij+1 — Si,j )2)1/2,
s∈Rd
i,j
where i, j are the indexes of pixels in an image.
To estimate the optimal, we run an SGD optimizer with a learning rate of 10 and decayed weight
10-4 for 500 iterations. We used a grid searching on the space of ζ. We find that the best data
recovery comes from ζ = 0.01 for SVHN dataset and ζ = 10-5 for MNIST and FashionMNIST by
grid search.
C.3 Quantitative Metrics for Image Similarity Measurement
We adopt the following two known metrics to measure the similarity between x* and x:
• Normalized structural similarity index metric (SSIM), a perception-based metric that con-
siders the similarity between images in structural information, luminance and contrast. It
is widely used in image and video compression research to quantify the difference between
the original and compressed images. The detailed calculation can be found in Wang et al.
(2004). We normalize SSIM to take value range [0, 1] (original SSIM takes value range
[—1, 1]).
• Perceptual similarity (PSIM). Perceptual loss Johnson et al. (2016) has been widely used
for training image generation and style transferring models Johnson et al. (2016); Lucas
et al. (2019); Wang et al. (2018). It emerges as a novel measurement for evaluating the
discrepancy between high-level perceptual features that extracted by deep learning model
of the reconstructed image and ground-truth image. We define PSIM as 1— perceptual loss.
19
Under review as a conference paper at ICLR 2021
D Additional Experimental Results
D. 1 Compare Penalty Strategies
A natural approach arise to reduce data separability could be adding a penalty on the pair-wise
distance for the data representations within a class. We name this approach as UniCon. Its loss
function denoted as Lunicon can be written as:
Lunicon
1	1
C ∣Cc∣∙(∣Cc∣-1)
kh(xi)-h(xj)k22,
c∈C i∈Cc j∈Cc
The final objective function L := Lclass + λ ∙ Lunicon. This approach is similar to contrastive learn-
ing Khosla et al. (2020). However, we observed that the approach is not as ideal as our proposed
MixCon, in the sense of defending inversion attack. The intuition is that MixCon can induce con-
fusing patterns to fool the neural network learning typical patterns from a class. Here we show the
visualization for the three benchmark datasets in Figure 5. We select λ = 1 for MNIST and Fash-
ionMNIST and λ = 0.5 for SVHN in both UniCon and MixCon. Then we choose the β = 1e - 4
for MixCon to match the accuracy to Vanilla and UniCon. We use the same training and testing of
MixCon for UniCon experiment. From the representative samples (while typical to the rest of the
data samples), we observe worse data recovery quality of MixCon. Notably, the recovered results
from UniCon keep the pattern of their class. While MixCon results in more blurred and indistin-
guishable patterns across classes. We compare the quantitative evaluation results between MixCon
and UniCon in Table 4. 9 We use metric SSIM and PSIM to evaluate the similarity between the
recovered image and the original image. Lower scores indicate worse data recovery results. The
data recovery experiment is performed on 100 testing samples, and we report the mean ± std and
worst case (the best-recovered data) results. Except for the PSIM scores evaluated on MNIST, we
get conformable evidence showing MixCon training is apt to defend inversion.
Figure 5: Qualitative evaluation for image inversion results.
Acc (%)
SSIM
PSIM
MNIST
UniCon	MixCon
λ = 1.0	(λ = 1.0, β = 10-4)
99.2	98.6
0.31 ± 0.11(0.59)	0.14 ± 0.11(0.48)
0.41 ± 0.07(0.60)	0.44 ± 0.07(0.69)
FashionMNIST
UniCon	MixCon
λ =1.0	(λ = 1.0,β = 10-4)
896	889
0.19 ± 0.09(0.53)	0.17 ± 0.09(0.52)
0.45 ± 0.07(0.64)	0.42 ± 0.08(0.66)
SVHN
UniCon	MixCon
λ= 0.5	(λ= 0.5,β= 10-4)
88.3	88.2
0.67 ± 0.11(0.91)	0.61 ± 0.15(0.84)
0.62 ± 0.05(0.75)	0.59 ± 0.07(0.72)
Table 4: Quantitative evaluations for image recovery results. For fair evaluation, we match the data utility
(accuracy) for Vanilla and MixCon. SSIM and PSIM are measured on 100 testing samples. Those scores
are presented in mean ± std and worst-case (in parentheses) format. The smaller scores indicate harder data
recovery.
9We have presented the comparison between MixCon and vanilla training in Table 3.
20
Under review as a conference paper at ICLR 2021
D.2 Effects on the Selection of Middle Layers
The trade-off between data separability and data utility can be different for adding MixConon the
different layers. In our benchmark experiment, we use a LeNet5 LeCun (2015) — a five-layer CNN.
Thus there are four split methods, namely four intermediate outputs. In our collaborative inference
setting 10, we visit the all four possible middle layers to apply MixCon loss. We plot the accuracy
and data separability plots over the different combinations of (β, λ), together with SSIM and PSIM
scores (mean and the worst-case results), for each layer on our three benchmark datasets. The results
are shown as Figure 6 to Figure 17. There is a clear trend that the shallower the h(x) it is, the easier
to recover original x on average with respect to the mean SSIM and PSIM scores. The worst-case
measurement may suffer from some outliers and imperfectness of the evaluation metrics. In most
cases, distance and recovery similarity score for the first three layers shows a positive relationship,
i.e. in Figure 6 - Figure 8. Usually, inversion from the deeper layers is not stable. Also, splitting a
network at a deeper layer in the collaborative inference setting is not common or realistic because
clients, such as edge-end devices, do not have powerful computational resources. Notably, the
relationship between accuracy and similarity is highly non-linear. The sweet spot for a trade-off
between accuracy and difficulty of recovery is in the space where the accuracy degradation curve is
slow, while recovery similarity is low. Users can search the best parameters and ”cut layer” to meet
certain accuracy and data recovery defending requirements in practice.
10There is no necessity to add MixCon loss for the layers before ”cut layer”, because the attacker is not able
to get access to the original data hidden representations form those layers
21
Under review as a conference paper at ICLR 2021
Accuracy
Distance
gue〕s_p
05
(a) Accuracy vs
. Distance
-0.85
-0.80
-0.75
-0.70
(b) Quantitative evaluations on data recovery
Figure 6: Adding MixCon to the 1st layer of CNN on MNIST dataset. (a) The trade-off between data separa-
bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with different
λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of data
recovery results. We show SSIM and PSIM scores with different λ and β.
22
Under review as a conference paper at ICLR 2021
Accuracy
Distance
0.075
0.050
0.025
(a) Accuracy vs
. Distance
-0.35
-0.30
-0.25
-0.20
(b) Quantitative evaluations on data recovery
Figure 7: Adding MixCon to the 2nd layer of CNN on MNIST dataset. (a) The trade-off between data
separability and data utility . We show testing accuracy and mean pairwise distance (data separability) with
different λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation
of data recovery results. We show SSIM and PSIM scores with different λ and β.
23
Under review as a conference paper at ICLR 2021
Distance
λ Ie-O 6	∩
P	le-08	0
1.0
0.5
0.0
(a) Accuracy vs
. Distance
(b) Quantitative evaluations on data recovery
Figure 8: Adding MixCon to the 3rd layer of CNN on MNIST dataset. (a) The trade-off between data separa-
bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with different
λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of data
recovery results. We show SSIM and PSIM scores with different λ and β.
0.525
-0.500
-0.475
-0.450
-0.425
-0.400
0.375
0.350
24
Under review as a conference paper at ICLR 2021
(b) Quantitative evaluations on data recovery
Figure 9: Adding MixCon to the 4th layer of CNN on MNIST dataset. (a) The trade-off between data separa-
bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with different
λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of data
recovery results. We show SSIM and PSIM scores with different λ and β.
-0.085
-0.080
0.075
-0.070
-0.065
-0.060
-0.055
-0.500
-0.475
-0.450
-0.425
■0.400
25
Under review as a conference paper at ICLR 2021
Accuracy
UB-JnUB
8 6 4
CieiCi
Distance
Suewp
05
(a)	Accuracy vs
0.975
-0.950
-0.925
-0.900
-0.875
j-0.850
.0.825
0.925
-0.900
-0.875
-0.850
-0.825
0.800
0.775
0.750
0.01
0.0001
R le-06
P	le-08
1.0
(b)	Quantitative evaluations on data recovery
Figure 10: Adding MixCon to the 1st layer of CNN on FashionMNIST dataset. (a) The trade-off between
data separability and data utility . We show testing accuracy and mean pairwise distance (data separability)
with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative
evaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
26
Under review as a conference paper at ICLR 2021
Distance
-0.15
-0.10
-0.05
gu 区 s_p
. Distance
(a) Accuracy vs
(b) Quantitative evaluations on data recovery
Figure 11: Adding MixCon to the 2nd layer of CNN on FashionMNIST dataset. (a) The trade-off between
data separability and data utility . We show testing accuracy and mean pairwise distance (data separability)
with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative
evaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
27
Under review as a conference paper at ICLR 2021
(b) Quantitative evaluations on data recovery
Figure 12: Adding MixCon to the 3rd layer of CNN on FashionMNIST dataset. (a) The trade-off between
data separability and data utility . We show testing accuracy and mean pairwise distance (data separability)
with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative
evaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
0.675
0.650
-0.625
-0.600
-0.575
-0.550
-0.525
0.500
28
Under review as a conference paper at ICLR 2021
(b) Quantitative evaluations on data recovery
Figure 13: Adding MixCon to the 4th layer of CNN on FashionMNIST dataset. (a) The trade-off between
data separability and data utility . We show testing accuracy and mean pairwise distance (data separability)
with different λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative
evaluation of data recovery results. We show SSIM and PSIM scores with different λ and β.
0.410
-0.405
-0.400
-0.395
0.390
-0.385
0.380
-0.65
-0.60
0.55
0.50
0.775
-0.750
-0.725
-0.700
-0.675
-0.650
0.625
0.600
29
Under review as a conference paper at ICLR 2021
(a) Accuracy vs. Distance
(b) Quantitative evaluations on data recovery
-0.65
-0.60
Figure 14: Adding MixCon to the 1st layer of CNN on SVHN dataset. (a) The trade-off between data separa-
bility and data utility . We show testing accuracy and mean pairwise distance (data separability) with different
λ and β. λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation of data
recovery results. We show SSIM and PSIM scores with different λ and β.
30
Under review as a conference paper at ICLR 2021
(b) Quantitative evaluations on data recovery
-0.60
-0.58
Figure 15: Adding MixCon to the 2nd layer of CNN on SVHN dataset. (a) The trade-off between data
separability and data utility . We show testing accuracy and mean pairwise distance (data separability) with
different λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation
of data recovery results. We show SSIM and PSIM scores with different λ and β.
0.800
0.775
0.750
-0.725
-0.700
-0.675
-0.650
0.625
31
Under review as a conference paper at ICLR 2021
Accuracy
Distance
1.0
0.5
. Distance
(a) Accuracy vs
(b) Quantitative evaluations on data recovery
Figure 16: Adding MixCon to the 3rd layer of CNN on SVHN dataset. (a) The trade-off between data
separability and data utility . We show testing accuracy and mean pairwise distance (data separability) with
different λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation
of data recovery results. We show SSIM and PSIM scores with different λ and β.
32
Under review as a conference paper at ICLR 2021
(a) Accuracy vs. Distance
(b) Quantitative evaluations on data recovery
Figure 17: Adding MixCon to the 4th layer of CNN on SVHN dataset. (a) The trade-off between data
separability and data utility . We show testing accuracy and mean pairwise distance (data separability) with
different λ and β . λ and β show complementary effort on adjusting data separability. (b) Quantitative evaluation
of data recovery results. We show SSIM and PSIM scores with different λ and β.
0.700
0.675
-0.650
-0.625
-0.600
-0.575
-0.550
0.525
33