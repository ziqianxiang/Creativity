Under review as a conference paper at ICLR 2021
Linear Convergence and Implicit Regular-
ization of Generalized Mirror Descent with
Time-Dependent Mirrors
Anonymous authors
Paper under double-blind review
Ab stract
The following questions are fundamental to understanding the properties of over-
parameterization in modern machine learning: (1) Under what conditions and at
what rate does training converge to a global minimum? (2) What form of im-
plicit regularization occurs through training? While significant progress has been
made in answering both of these questions for gradient descent, they have yet to
be answered more completely for general optimization methods. In this work,
we establish sufficient conditions for linear convergence and obtain approximate
implicit regularization results for generalized mirror descent (GMD), a general-
ization of mirror descent with a possibly time-dependent mirror. GMD subsumes
popular first order optimization methods including gradient descent, mirror de-
scent, and preconditioned gradient descent methods such as Adagrad. By using
the Polyak-Lojasiewicz inequality, we first present a simple analysis under which
non-stochastic GMD converges linearly to a global minimum. We then present
a novel, Taylor-series based analysis to establish sufficient conditions for linear
convergence of stochastic GMD. As a corollary, our result establishes sufficient
conditions and provides learning rates for linear convergence of stochastic mirror
descent and Adagrad. Lastly, we obtain approximate implicit regularization re-
sults for GMD by proving that GMD converges to an interpolating solution that is
approximately the closest interpolating solution to the initialization in `2 -norm in
the dual space.
1	Introduction
Recent work has established the optimization and generalization benefits of over-parameterization
in machine learning (Belkin et al., 2019; Liu et al., 2020; Zhang et al., 2017). In particular, several
works including Vaswani et al. (2019); Du et al. (2018); Liu et al. (2020); Li & Liang (2018) have
demonstrated that over-parameterized models converge to a global minimum when trained using
stochastic gradient descent and that such convergence can occur at a linear rate. Independently,
other work, such as Gunasekar et al. (2018), have characterized implicit regularization of over-
parameterized models, i.e., the properties of the solution selected by a given optimization method,
without proving convergence.
Recently, Azizan & Hassibi (2019); Azizan et al. (2019) simultaneously proved convergence and an-
alyzed approximate implicit regularization for mirror descent (Beck & Teboulle, 2003; Nemirovsky
& Yudin, 1983). In particular, by using the fundamental identity of stochastic mirror descent (SMD),
they proved that SMD converges to an interpolating solution that is approximately the closest one to
the initialization in Bregman divergence. However, these works do not provide a rate of convergence
for SMD and assume that there exists an interpolating solution within in Bregman divergence from
the initialization. In this work, we provide sufficient conditions for linear convergence and ob-
tain approximate implicit regularization results for generalized mirror descent (GMD), an extension
of mirror descent that introduces (1) a potential-free update rule and (2) a time-dependent mirror;
namely, GMD with invertible φ : Rd → Rd and learning rate η is used to minimize a real valued
loss function, f, according to the update rule:
Φ(t)(w(t+1)) = φ(t)(w(t)) — ηkf(w(t)).
(1)
1
Under review as a conference paper at ICLR 2021
We discuss the stochastic version of GMD (SGMD) in Section 3. GMD generalizes both mirror
descent and preconditioning methods. Namely, if for all t, φ(t) = Vψ for some strictly convex
function ψ, then GMD corresponds to mirror descent with potential ψ ; if φ(t) = G(t) for some
invertible matrix G(t) ∈ Rd×d, then the update rule in equation (1) reduces to
w(t+1) = w(t) - ηG(t)-1Vf(w(t))
and hence represents applying a pre-conditioner to gradient updates. The following is a summary of
our results:
1.	We provide a simple proof for linear convergence of GMD under the Polyak-Lojasiewicz
inequality (Theorem 1).
2.	We provide sufficient conditions under which SGMD converges linearly under an adaptive
learning rate (Theorems 2 and 3)1.
3.	As corollaries to Theorems 1 and 3, in Section 5 we provide sufficient conditions for linear
convergence of stochastic mirror descent as well as stochastic preconditioner methods such
as Adagrad (Duchi et al., 2011).
4.	We prove the existence of an interpolating solution and linear convergence of GMD to
this solution for non-negative loss functions that locally satisfy the PL* inequality (Liu
et al., 2020). This result (Theorem 4) provides approximate implicit regularization results
for GMD: GMD converges linearly to an interpolating solution that is approximately the
closest interpolating solution to the initialization in `2 norm in the dual space induced by
φ(t).
2	Related Work
Recent work (Azizan et al., 2019) established convergence of stochastic mirror descent (SMD) for
nonlinear optimization problems. It characterized the implicit bias of mirror descent by demon-
strating that SMD converges to a global minimum that is within epsilon of the closest interpolating
solution in Bregman divergence. The analysis in Azizan et al. (2019) relies on the fundamental
identity of SMD and does not provide explicit learning rates or establish a rate of convergence for
SMD in the nonlinear setting. The work in Azizan & Hassibi (2019) provided explicit learning rates
for the convergence of SMD in the linear setting under strongly convex potential, again without a
rate of convergence. While these works established convergence of SMD, prior work by Gunasekar
et al. (2018) analyzed the implicit bias of SMD without proving convergence.
A potential-based version of generalized mirror descent with time-varying regularizes was presented
for online problems in Orabona et al. (2015). That work is primarily concerned with establishing
regret bounds for the online learning setting, which differs from our setting of minimizing a loss
function given a set of known data points. A potential-free formulation of GMD for the flow was
presented in Gunasekar et al. (2020).
The Polyak-Lojasiewicz (PL) inequality (Lojasiewicz, 1963; Polyak, 1963) serves as a simple condi-
tion for linear convergence in non-convex optimization problems and is satisfied in a number of set-
tings including over-parameterized neural networks (Liu et al., 2020). Work by Karimi et al. (2016)
demonstrated linear convergence of a number of descent methods (including gradient descent) under
the PL inequality. Similarly, Vaswani et al. (2019) proved linear convergence of stochastic gradient
descent (SGD) under the PL inequality and the strong growth condition (SGC), and Bassily et al.
(2018) established the same rate for SGD under just the PL inequality. Soltanolkotabi et al. (2019)
also used the PL inequality to establish a local linear convergence result for gradient descent on 1
hiddden layer over-parameterized neural networks.
Recently, Xie et al. (2020) established linear convergence for a norm version of Adagrad (Adagrad-
Norm) using the PL inequality, while Wu et al. (2019) established linear convergence for Adagrad-
Norm in the particular setting of over-parameterized neural networks with one hidden layer. An
alternate analysis for Adagrad-Norm for smooth, non-convex functions was presented in Ward et al.
(2019), resulting in a sub-linear convergence rate.
1We also provide a fixed learning rate for monotonically decreasing gradients Vf (w(t)).
2
Under review as a conference paper at ICLR 2021
Instead of focusing on a specific method, the goal of this work is to establish sufficient conditions
for linear convergence by applying the PL inequality to a more general setting (SGMD). We arrive at
linear convergence for specific methods such as mirror descent and preconditioned gradient descent
methods as corollaries. Moreover, our local convergence results provide an intuitive formulation
of approximate implicit regularization for GMD and thus mirror descent. Namely, instead of re-
sorting to Bregman divergence, we prove that GMD converges to an interpolating solution that is
approximately the closest interpolating solution to the initialization in `2 norm in the dual space
induced by φ(t) .
3	Algorithm Description and Preliminaries
We begin with a formal description of SGMD. Let fi : Rd → R denote real-valued, differentiable
loss functions and let f (x) = 1 pn=1 f (x). In addition, let φ(t) : Rd → Rd be an invertible
function for all non-negative integers t. We solve the optimization problem
arg min f(x)
x∈Rd
using stochastic generalized mirror descent with learning rate η2:
φ㈤(W(t+1)) = φ⑴(W㈤)-nN fit (W⑴),	(2)
where it ∈ [n] is chosen uniformly at random. As described in the introduction, the above algorithm
generalizes both gradient descent (where φ(x) = x) and mirror descent (where φ(t) (x) = Nψ(x)
for some strictly convex potential function ψ). In the case where φ(t)(x) = G(t)x for an invertible
matrix G(t) ∈ Rd×d, the update rule in equation (2) reduces to:
W(t+1) = W(t) - ηG(t)-1Nfit (W(t))
Hence, when φ(t) is an invertible linear transformation, Equation (2) is equivalent to pre-conditioned
gradient descent. We now present the Polyak-Lojasiewicz inequality and lemmas from optimization
theory that will be used in our proofs3.
Polyak-LojasieWicz (PL) Inequality. Afunction f : Rd → R is μ -PL if for some μ > 0:
2 kVf(x)k2≥ μ(f(x) - f(x*)) ∀X ∈ Rd,	(3)
where x* ∈ Rd is a global minimizer for f.
A useful variation of the PL inequality is the PL* inequality introduced in Liu et al. (2020) which
does not require knowledge of f(x*).
Definition. Afunction f : Rd → R is μ-PL* iffor some μ > 0:
2 kVf (x)k2≥ μf (x) ∀x ∈ Rd,	(4)
A function that is μ-PL* is also μ-PL when f is non-negative. Additionally, We will typically assume
that f is L-smooth (with L-Lipschitz continuous derivative).
Definition. A function f : Rd → R is L-smooth for L > 0 iffor all x, y ∈ Rd:
kVf (x) - Vf(y)k≤ Lkx - yk.
If φ(t)(x) = x for any t and x ∈ Rd then SGMD reduces to SGD. If f is L-smooth and satisfies
the PL-Inequality, then SGD converges linearly to a global minimum (Bassily et al., 2018; Karimi
et al., 2016; Vaswani et al., 2019). Moreover, the following lemma (proven in Appendix A) shows
that the PL* condition implies the existence of a global minimum x* for non-negative, L-smooth f .
Lemma 1. If f : Rd → R is μ-PL*, L-smooth and f (x) ≥ 0 for all X ∈ Rd, then gradient descent
with learning rate n < L converges linearly to x* satisfying f (x*) = 0.
2The framework also allows for adaptive learning rates by using η(t) to denote a time-dependent step size.
3We assume all norms are the 2-norm unless stated otherwise.
3
Under review as a conference paper at ICLR 2021
Hence, in cases where the loss function is nonnegative (for example the squared loss), we can re-
move the usual assumption about the existence of a global minimum, x*, and instead assume that
f satisfies the PL* inequality. We now reference standard properties of L-smooth functions (Zhou,
2018), which will be used in our proofs.
Lemma 2. If f : Rd → R is L-smooth, then for all x, y ∈ Rd :
(a)	f (y) ≤ f (χ) + Ef(X), y - Xi + Lky - χk2,
(b)	kVf(x)k2≤ 2L(f(x)- f(x*)).
The following lemma relates μ and L (the proof is in Appendix B).
Lemma 3. If f : Rd → R is μ-PL and L-smooth, then μ ≤ L.
Using Lemma 2b in place of the strong growth condition (i.e. Ei[kVfi(X)k2] ≤ ρkVf (X)k2) yields
slightly different learning rates when establishing convergence of stochastic descent methods (as is
apparent from the different learning rates between Bassily et al. (2018) and Vaswani et al. (2019)).
The following simple lemma will be used in the proof of Theorem 3.
Lemma 4. If f(x) = 1 En=I fi(x) where f : Rd → R are Li-smooth, then f is SuPi Li-smooth.
Note that there could exist some other constant L0 < supi Li for which f is L0-smooth, but this
upper bound suffices for our proof of Theorem 3. Lastly, we define and reference standard properties
of strongly convex functions (Zhou, 2018), which will be useful in demonstrating how our GMD
results generalize those for mirror descent.
Definition. For α > 0, a differentiable function, ψ : Rd → R, is α-strongly convex iffor all X, y,
ψ(y) ≥ ψ(x) + hVψ(χ), y — Xi + 2ky — χ∣∣2.
Lemma 5. If ψ : Rd → R is α-strongly convex, then for all X, y:
ψ(y) ≤ Ψ(x) + hVψ(x),y - Xi + 1-∣∣Vψ(y) — Vψ(x)k2.
2α
With these preliminaries in hand, we now present our proofs for linear convergence of SGMD using
the PL-Inequality.
4 Sufficient Conditions for Linear Convergence of SGMD
In this section, we provide sufficient conditions to establish (expected) linear convergence for
(stochastic) GMD. We first provide simple conditions under which GMD converges linearly by
extending the proof strategy from Karimi et al. (2016). We then present alternate conditions for
linear convergence of GMD, which can be naturally extended to the stochastic setting.
4.1	Simple Conditions for Linear Convergence of GMD
We begin with a simple set of conditions under which (non-stochastic) GMD converges linearly (the
full proof is presented in Appendix C). The main benefit of this analysis is that it is a straightforward
extension of the proof of linear convergence for gradient descent under the PL-Inequality presented
in Karimi et al. (2016).
Theorem 1. Suppose f : Rd → R is L-smooth and μ-PL and φ⑶：Rd → Rd is an invertible,
α(ut) -Lipschitz function where lim α(ut) < ∞. Iffor all X, y ∈ Rd and for all timesteps t there exist
t→∞
αl(t) > 0 such that
hφ(t)(X) - φ(t)(y),X - yi ≥ αl(t)kX - yk2,
and lim αl(t) > 0, then generalized mirror descent converges linearly to a global minimum for any
t→∞ l
η⑴ < 卒.
4
Under review as a conference paper at ICLR 2021
Remark. Theorem 1 yields a fixed learning rate provided that αl(t) is uniformly bounded. In ad-
dition, note that Theorem 1 applies also under weaker assumptions, namely when φ(t) is locally
Lipschitz. Finally, the provided learning rate can be computed exactly for settings such as linear
regression, since it only requires knowledge of L and α(t) (See Section 7). When η = αj- and given
w* a minimizer of f, the proof of Theorem 1 implies that:
f(w(t+1)) - f(w*) ≤	1-
(t)2
即)(…-f(w*)).
Lα(t)2
Letting κ(t) = E thus generalizes the condition number introduced in Definition 4.1 of Liu
μαl
et al. (2020) for gradient descent. Provided that κ = lim κ(t) > 0, then Theorem 1 guarantees
t→∞
linear convergence to a global minimum. When α⅛ is decreasing in t, the rate is given by:
f (w(t+1)) — f (w*) ≤ (l - K) + (f(w(O))- f (w*)).
4.2	Taylor Series Analysis for Linear Convergence in GMD
Although the proof of Theorem 1 is succinct, it is nontrivial to extend to the stochastic setting4.
In order to develop a convergence result for the stochastic setting, we turn to an alternate set of
conditions for linear convergence by using the Taylor expansion of φ-1. We use Jφ to denote the
Jacobian of φ. For ease of notation, we consider non-time-dependent αl, αu, but our results are
trivially extendable to the setting when these quantities are time-dependent.
Theorem 2.	Suppose f : Rd → R is L-smooth and μ-PL and φ : Rd → Rd is an infinitely
differentiable, analytic function with analytic inverse, φ-1. If there exist αl, αu > 0 such that
(a)	αlI 4 Jφ 4 αuI,
k!
(b)	∣∂iι,...ik φj 1(x)∣≤ 20^d ∀x ∈ Rd ,iι, ...ik ∈ [d],j ∈ [d],k ≥ 2,
then generalized mirror descent converges linearly for any η(t) <
min
_____1_____
2√dgf(W ⑴)k
The full proof is provided in Appendix D. Importantly, the adaptive component of the learning rate
is only used to ensure that the sum of the higher order terms for the Taylor expansion converges. In
particular, if φ(t) is a linear function, then our learning rate no longer needs to be adaptive. Note that
alternatively, we can establish linear convergence for a fixed learning rate given that the gradients
monotonically decrease or if f is non-negative and μ-PL*. We analyze this case in Appendix E and
provide an explicit condition on μ and L under which this holds.
4.3 Taylor Series Analysis for Linear Convergence in Stochastic GMD
The main benefit of the above Taylor series analysis is that it naturally extends to the stochastic
setting as demonstrated in the following result (with proof presented in Appendix F).
Theorem 3.	Suppose f (x) = -1 En=I fi(x) where fi : Rd → R are non-negative, Li-smooth
functions with L = supi∈[n] Li and f is μ-PL*. Let φ : Rd → Rd be an infinitely differentiable,
analytic function with analytic inverse, φ-1 . SGMD is used to minimize f according to the updates:
φ(w(t+1)) = φ(w(t)) - η(e)^fit(w(t)),
where it ∈ [n] is chosen uniformly at random and η(t) is an adaptive step size. If there exist
αl, αu > 0 such that:
(a)	αlI 4 Jφ 4 αuI,
(b)	∣∂iι,...ik φ-1(x)∣≤	2k!	;L	∀x	∈	Rd,iι,...ik ∈	[d],j	∈	[d],k ≥	2,
then SGMD with n(t)< min (5L⅛，2√dmaXikVfi(w(t))k)
converges linearly to a global minimum.
4The main difficulty is relating w(t+1) - w(t) to the gradient at timestep t.
5
Under review as a conference paper at ICLR 2021
Remark. Note that there is a slight difference between the learning rate in Theorem 2 and Theorem 3
due to a multiplicative factor of μ. Consistent with the difference in learning rates between Bassily
et al. (2018) and Vaswani et al. (2019), we can make the learning rate between the two theorems
match if We assume the strong growth condition (i.e. Ei[kVfi(x)k2] ≤ Pk▽/(x)k2) With P = μ
instead of using Lemma 2b. Moreover, as maxikVfi(w(t))k≤ ∖Jc2nLf (W(O)), we establish linear
convergence for a fixed step size η < min (LOl , 2√2d 1/(=w)) as well.
5	Corollaries of Linear Convergence in SGMD
We now present how the linear convergence results established by Theorems 1, 2, and 3 apply to
commonly used optimization algorithms including mirror descent and Adagrad. In this section, we
primarily extend the analysis from Theorem 1 for the non-stochastic case. However, our results
can be extended analogously to give expected linear convergence in the stochastic case by using the
extension provided in Theorem 3.
Gradient Descent. For the case of gradient descent, φ(x) = x and so αl = αu = 1. Hence, we
see that gradient descent converges linearly under the conditions of Theorem 1 with η < L, which
is consistent with the analysis in Karimi et al. (2016).
Mirror Descent. Let ψ : Rd → R be a strictly convex potential. Thus, φ(x) = Vψ(x) is an
invertible function. If ψ is αl-strongly convex and (locally) αu-Lipschitz and f is L-smooth and
μ-PL, then the conditions of Theorem 1 are satisfied. Moreover, since the a〃-Lipschitz condition
holds locally for most potentials considered in practice, our result implies linear convergence for
mirror descent with αl-strongly convex potential ψ.
Adagrad. Let φ(t) = G(t)2 where G(t) is a diagonal matrix such that
t
Gi(,ti) = X Vfi(w(k))2.
k=0
Then GMD corresponds to Adagrad. In this case, we can apply Theorem 1 to establish linear conver-
gence of Adagrad under the PL-Inequality provided that φ(t) satisfies the condition of Theorem 1.
The following corollary proves that this condition holds and hence that Adagrad converges linearly.
Corollary 1. Let f : Rd → R be an L-smooth function that is μ-PL. Let α((t = mini∈[d] G(t)
and Out 二 maxi∈[d] G(t). If lim α(t) = 0, then Adagrad converges Iinearlyfor adaptive step size
,	t→∞ αu
(t)
√t)=止
η = L .
The proof is presented in Appendix H. While Corollary 1 can be extended to the stochastic setting
via Theorem 3, it requires knowledge of μ to setup the learning rate, and the resulting learning
rate provided is typically smaller than what we can use in practice. We analyze this case further
in Section 7. Additionally, since the condition limt→∞ (Ot = 0 is difficult to verify in practice,
we provide Corollary 2 in Appendix H, which presents a veurifiable condition under which Adagrad
converges linearly.
6	Local Convergence and Implicit Regularization in GMD
In the previous sections, we established linear convergence for GMD for real-valued loss, f : Rd →
R, that is μ-PL for all X ∈ Rd. In this section, we show that f need only satisfy the PL inequality
locally (i.e. within a ball of fixed radius around the initialization) in order to establish linear conver-
gence. The following theorem (proof in Appendix G) extends Theorem 4.2 from Liu et al. (2020)
to GMD and uses the PL* condition to establish both the existence of a global minimum and linear
convergence to this global minimum under GMD5. We use B(x, R) = {z ; z ∈ Rd, kx - zk2≤ R}
to denote the ball of radius R centered at x.
5We require additional assumptions on φ(t) for the case of time-dependent mirrors (see Appendix G.)
6
Under review as a conference paper at ICLR 2021
Theorem 4. Suppose φ : Rd → Rd is an invertible, αu -Lipschitz function and that f :
Rd → R is non-negative, L-smooth, and μ-PL* on B = {x ; φ(x) ∈ B(φ(w(0)),R)} with
R = 2√z2Lvf(w())αu ∙ Iffor all x,y ∈ Rd there exists αι > 0 such that
hφ(x) - φ(y),x-yi ≥ αlkx - yk2,
then,
(1)	There exists a global minimum w(∞) ∈ B.
(2)	GMD converges linearly to w(∞) for η = αl.
L
(3)	If w* = arg min ∣∣φ(w) 一 φ(w(0))k, then, kφ(w*) 一 φ(w(∞))k≤ 2R.
w∈B ; f(w) = 0
Approximate Implicit Regularization in GMD. When R is small, we can view the result of Theo-
rem 4 as a characterization of the solution selected by GMD, thereby obtaining approximate implicit
regularization results for GMD. Namely, for δ = RR, we have ∣∣φ(w*) 一 φ(w∞)∣≤ δ. Hence Pro-
vided that R is small (which holds for small f(w(0))), GMD selects an interpolating solution that is
Close to w* in '2-norm in the dual space induced by φ. This view is consistent with the Characteri-
zation of aPProximate imPlicit regularization in Azizan et al. (2019), as is shown by Corollary 3 in
Appendix I). In particular, Corollary 3 implies the assumptions used in Azizan et al. (2019) for the
full batch case by proving (1) the existence of such a w(∞), (2) linear convergence of w(0) to w(∞),
and (3) providing explicit forms for (where = R2 above). Importantly, the approximate implicit
regularization result for mirror descent does not need to be stated in terms of Bregman divergence,
but can be viewed more naturally as ∣∣Vψ(w(∞)) 一 Vψ(w*)∣2 being small.
7	Experimental Verification of our Theoretical Results
We now present a simple set of experiments under which we can explicitly compute the learning
rates in our theorems. We will show that in accordance with our theory, both fixed and adaptive
versions of these learning rates yield linear convergence. We focus on computing learning rates
for Adagrad in the noiseless regression setting used in Xie et al. (2020). Namely, we are given
(X, y ) ∈ Rn×d × Rn such that there exists a w* ∈ Rd such that Xw* = y . If n < d, then
the system is over-parameterized, and if n ≥ d, the system is sufficiently parameterized and has a
unique solution.
In this setting, the squared loss (MSE) is L-smooth with L = λmaχ(XXT), and it is μ-PL with
μ = λmin(XXT) where λmaχ and λm%n refer to the largest and smallest non-zero eigenvalues,
respectively6 . Moreover, for Adagrad, we can compute αl(t) = mini∈[d](Ptk=0 Vfi(w(k)))2 and
α(ut) = maxi∈[d] (Ptk=0 Vfi(w(k)))2 at each timestep. Hence for Adagrad in the noiseless linear re-
gression setting, we can explicitly compute the learning rate provided in Theorem 3 for the stochastic
setting and in Corollary 1 for the full batch setting.
Figure 1 demonstrates that in both, the over-parameterized and sufficiently parameterized settings,
our provided learning rates yield linear convergence. In the stochastic setting, the theory for fixed
learning rates suggests a very small rate (≈ 10-9 for Figure 1d) and hence we chose to only present
the more reasonable adaptive step size as a comparison. In the full batch setting, the learning rate
obtained from our theorems out-performs using the standard fixed learning rate of 0.1, while per-
formance is comparable for the stochastic setting. Interestingly, our theory suggests an adaptive
learning rate that is increasing (in contrast to the usual decreasing learning rate schedules). In par-
ticular, while the suggested learning rate for Figure 1a starts at 0.99, it increases to 1.56 at the end
of training.
In Appendix J, we present experiments on over-parameterized neural networks. While the PL-
condition holds in this setting (Liu et al., 2020), it can be difficult to compute the smoothness pa-
rameter L (which was the motivation for developing Adagrad-Norm). Interestingly, our experiments
6We take μ as the smallest non-zero eigenvalue since Adagrad updates keep parameters in the span of the
data.
7
Under review as a conference paper at ICLR 2021
NoiSeIeSS RegreSSion
Convergence of Adagrad
Dddddddd
1 1 2 3 4 5 6
------
(60)」0」」山 pə-enbs ueφw
500	1000	1500	2000
Number of Epochs
(a)
Convergence of Adagrad
(60-j)」0」」山 pə-enbs ueφw
Convergence of Stochastic Adagrad
-15.0
2500	3000	0	2500 5000 7500 10000 12500 15000 17500 20000
Number of Epochs
(b)
NoiSeIeSS RegreSSion (Over-ParameteriZed)
Convergence of Stochastic Adagrad
---- Our Step Size (Adaptive)
---- Our Step Size (Fixed)
----Step Size of 0.1
500	1000	1500	2000	2500	3000
Number of Epochs
(60-j)」0」」山 pə-enbs ueφw
20000	40000	60000	80000	100000
Number of Epochs
(d)
Figure 1: Using the rates provided by Corollary 1 leads to linear convergence for (Stochastic) Ada-
grad in the noiseless linear regression setting also considered in Xie et al. (2020). (a, b) Noiseless
linear regression on 2000 examples in 20 dimensions. (c, d) Noiseless linear regression on 200
examples in 1000 dimensions.
demonstrate that our increasing adaptive learning rate from Theorem 1, using an approximation for
L, provides convergence for Adagrad in over-parameterized networks. The link to the code is pro-
vided in Appendix J.
8 Conclusion
In this work, we presented stochastic generalized mirror descent, which generalizes both mirror
descent and pre-conditioner methods. By using the PL-condition and a Taylor-series based analysis,
we provided sufficient conditions for linear convergence of SGMD in the non-convex setting. As a
corollary, we obtained sufficient conditions for linear convergence of both mirror descent and pre-
conditioner methods such as Adagrad. Lastly, we prove the existence of an interpolating solution
and linear convergence of GMD to this solution for non-negative loss functions that are locally PL*.
Importantly, our local convergence results allow us to obtain approximate implicit regularization
results for GMD. Namely, we prove that GMD linearly converges to an interpolating solution that is
approximately the closest interpolating solution to the initialization in `2 norm in the dual space. For
the full batch setting, this result provides a more natural characterization of implicit regularization
in terms of `2 norm in the dual space, as opposed to Bregman divergence.
Looking ahead, we envision that the generality of our analysis (and the PL-condition) could pro-
vide useful in the analysis of other commonly used adaptive methods such as Adam (Kingma & Ba,
2015). Moreover, since the PL-condition holds in varied settings including over-parameterized neu-
ral networks (Liu et al., 2020), it would be interesting to analyze whether the learning rates obtained
here provide an improvement for convergence in these modern settings.
8
Under review as a conference paper at ICLR 2021
References
Navid R. Azizan and Babak Hassibi. Stochastic Gradient/Mirror Descent: Minimax Optimality and
Implicit Regularization. In International Conference on Learning Representations (ICLR), 2019.
Navid R. Azizan, Sahin Lale, and Babak Hassibi. Stochastic Mirror Descent on Overparameterized
Nonlinear Models. In International Conference on Machine Learning (ICML) Generalization
Workshop, 2019.
Raef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of sgd in non-convex
over-parametrized learning. arXiv preprint arXiv:1811.02564, 2018.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31:167-175, 2θ03.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks, 2018. arXiv:1810.02054.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods forOnline Learning and
Stochastic Optimization. Journal of Machine Learning Research (JMLR), 12:2121-2159, 2011.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning (ICML), 2018.
Suriya Gunasekar, Blake Woodworth, and Nathan Srebro. Mirrorless Mirror Descent: A More
Natural Discretization of Riemannian Gradient Flow. arXiv preprint arXiv:2004.01025, 2020.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear Convergence of Gradient and Proximal-
Gradient Methods Under the Polyak-Lojasiewicz Condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 8157-8166, 2018.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-
parameterized systems of non-linear equations: the lessons of deep learning. arXiv preprint
arXiv:2003.00307, 2020.
Stanislaw Lcjasiewicz. A topological property of real analytic subsets (in French). Les equations
aux deriveespartielles.,117:87-89, 1963.
Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-
mization. SIAM, 27(2):264-265, 1983.
Francesco Orabona, Koby Crammer, and Nicold Cesa-Bianchi. Mirror descent and nonlinear pro-
jected subgradient methodsfor convex optimization. Machine Learning, 99:411-435, 2015.
Boris Polyak. Gradient methods for minimizing functionals (in Russian). Zh. Vychisl. Mat. Mat.
Fiz., 3(4):643-653, 1963.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimiza-
tion landscape of over-parameterized shallow neural networks. IEEE transaction on Information
Theory (2018), 65(2):742 - 769, 2019.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and Faster Convergence of SGD for Over-
Parameterized Models and an Accelerated Perceptron. In International Conference on Artificial
Intelligence and Statistics (AISTATS), 2019.
9
Under review as a conference paper at ICLR 2021
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad Stepsizes: Sharp Convergence Over Non-
convex Landscapes. In International Conference on Machine Learning (ICML), 2019.
Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for
an over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.
Yuege Xie, Xiaoxia Wu, and Rachel Ward. Linear Convergence of Adaptive Stochastic Gradient
Descent. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolution network, 2015. arXiv:1505.00853.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations (ICLR), 2017.
Xingyu Zhou. On the Fenchel Duality between Strong Convexity and Lipschitz Continuous Gradi-
ent. arXiv preprint arXiv:1803.06573, 2018.
10
Under review as a conference paper at ICLR 2021
Appendix
A Proof of Lemma 1
We restate the lemma below.
Lemma. If f : Rd → R is μ-PL*, L-smooth and f (x) ≥ 0 for all X ∈ Rd, then gradient descent
with learning rate η < 2 converges linearly to x* satisfying f (x*) = 0.
Proof. The proof follows exactly from Theorem 1 of Karimi et al. (2016). Since f is L-smooth, by
Lemma 2a it holds that:
f(w(t+1)) — f(w(t)) ≤ hVf(w(t)),w(t+1) — Wltt) + 刍∣w(t+1) — w(t)k2.
=⇒ f(w(t+D - f(w⑴)≤ —ηkVf(w(t))k2+2η2kVf(w(t))k2
=⇒ f (w(t+1t - f (w(t)) ≤ (-η + η2L) 2μf (w(t))
=⇒ f (w(t+1t ≤(1 — 2μη + μη2L) f (w(t))
Hence, if η < L, then C =(1 — 2μη + μη2L) < 1. Thus, We have f (w(t+1t) ≤ Cf (w(t)) for
C < 1. Thus, as f is bounded below by 0 and the sequence {f (w(t))}t∈N monotonically decreases
with infimum 0, the monotone convergence theorem implies lim f (w(t)) = 0.	□
t→∞
B Proof of Lemma 3
Proof. From Lemma 2 and from the PL condition, we have:
2μ(f(x) — f(x*)) ≤ kVf(x)k2≤ 2L(f(x) — f(x*)) =⇒ μ ≤ L □
C Proof of Theorem 1
Proof. Since f is L-smooth, by Lemma 2a it holds that:
f(w(t+1t) — f(w(t)) ≤ hVf(w(t)),w(t+1) — Wltt) + 刍∣w(t+1t - w(t)k2.	(5)
Now by the condition on φ(t) in Theorem 1, we bound the first term on the right as follows:
hφlt)(Wlt+1)) — φlt)(Wlt)), Wlt+1) — Wlt)) ≥ αllt)kWlt+1) — Wlt)k2
=⇒ h-ηVf (w(t)),w(t+1) — w(t)i ≥ α(t)kw(t+1) — w(t)∣∣2 using Equation (2)
lt)
=⇒ hVf(w(t)),w(t+1) - w(t)i ≤ - αl-kw(t+1) - w(t)k2.
η
Substituting this bound back into the inequality in (5), we obtain
f(w(t+% - f(w(t)) ≤ (-斗 + L) kw(t+1) - w(t)k2.
11
Under review as a conference paper at ICLR 2021
Since the learning rate is selected so that the coefficient of kw(t+1) - w(t) k2 on the right is negative,
we obtain
f(w(t+1)) -
1
(t)
αu
1
(t)
αu
∣w(t+1) - w(t)∣2
2 kφ(t)(w(t+1)) - φ(t)(w(t))k2
2∣∣-ηVf (w(t))k2 using Equation ⑴
2
2μjη-2 (f (w(t)) — f(w*)) as f is μ-PL
α(u)
ηα(t)
=⇒ f (W(t+I))- f (w*) ≤	1 - 2μ与;2 + μ	(f (w(t)) - f(W)),
αu
where the second inequality follows since φ(t) is α(ut)-Lipschitz. For linear convergence, we need.
(t)	2
0 < 1 - 2μj + 〃& < 1.
aut	∖ut
(6)
α(t)2L
From Lemma 3, μ < u(t)L always holds and implies that the left inequality in (6) is satisfied for all
α
2α(t)
η(t). The right inequality holds by our assumption that η(t) < -⅞-, which completes the proof. □
D Proof of Theorem 2
We repeat the theorem below for convenience.
Theorem. Suppose f : Rd → R is L-smooth and μ-PL and φ : Rd → Rd is an infinitely differen-
tiable, analytic function with analytic inverse, φ-1. If there exist αl, αu > 0 such that:
(a)	αlI 4 Jφ 4 αuI,
k!
(b)	∣∂iι,...ik φj 1(x)∣≤ 20^d ∀x ∈ Rd ,iι, ...ik ∈ [d],j ∈ [d],k ≥ 2,
then generalized mirror descent converges linearly for η(t)
< min
______1_____
,2√dKf(w(t) )k
Proof. Since f is L-smooth, it holds by Lemma that 2:
f(w(t+1)) — f(w(t)) ≤ hVf(w(t)),w(t+1) — W(t)i + 刍∣w(t+1) — w(t)∣2.
Next, we want to bound the two quantities on the right hand side by a multiple of ∣Vf(W(t))∣2. We
do so by expanding W(t+1) — W(t) using the Taylor series for φ-1 as follows:
W(t+1) — W(t) = φ-1(φ(W(t)) — ηVf(W(t))) — W(t)
=—nJ©— (Φ(w(t)))Vf (w(t))
∞1
+ Xk! [Pdι,i2...ik=ι(-n)k∂iι,...ikΦ-1(Φ(w(t)))(Vf(w(t))iι...Vf(w(t))ik)].
k=2
The quantity in brackets is a column vector where we only wrote out the jth coordinate for j ∈ [d].
Now we bound the term hVf (W(t)), W(t+1) — W(t)i:
hVf (w(t)),w(t+1) — w(t)i = -nVf(w(t )Tj-1(w ⑴)Vf(w ⑴)
∞
+Vf (w(t))T X k!
k=2
d
P	(-η)k∂i1,...ikφj-1(φ(W(t)))(Vf(W(t))i1 ...Vf(W(t))ik) .
i1 ,i2...ik=1
12
Under review as a conference paper at ICLR 2021
We have separated the first order term from the other orders because we will bound them separately
using conditions (a) and (b) respectively. Namely, we first have:
-ηVf(w(t))TJ-1(w㈤)Vf(w㈤)≤ -ɪkVf(W⑴)k2.
αu
Next, we use the Cauchy-Schwarz inequality on inner products to bound the inner product of
Vf (w(t)) and the higher order terms. In the following, We use α to denote 5-ɪʒ.
2αud
∞1
Vf(WWT X k! [pdι,i2...ik=ι(-η)k∂iι,..ik φ-1(φ(w(t)))(Vf (w⑴)iι ... Vf(W㈤)ik)]
k=2
∞ 1
≤Nf(w㈤)kXk! J[Pdι,i2...ik=ι(-η)k∂ii,...ikφ-1 (φ(w⑴))(Vf(W㈤)ii ... Vf(w⑴)ik)
∞ αk!
skVf(w(t))kX R(η)k∣∣[Pd1,i2...ik=1(∣Vf (W⑴)ii∣...∣Vf(w㈤)ik∣)]∣∣
k=2
∞
=kVf (W㈤)kα X √d(η)k (|Vf (W⑴)ι∣+ ... |Vf(W㈤))d∣)k
k=2
∞	∣Vf(W ㈤)1|
= kVf(W㈤)kαX(η)k√d∣h	.	, Iilk
k=2	|Vf(W(t))d|
∞
≤kVf(W㈤)kα X(η)k √dkVf (W⑴)kk(√d)k
k=2
∞
=α X(√d)k+1(η)kkVf(W㈤)kk+1
k=2
=α(√d)3 (η)2 kVf(W㈤)k3X (√d)k (η)k ∣∣v∕(w㈤)kk= 丁次震(：；)“3
k=0	1 - dηkVf(W(t))k
Hence weCan SeleCtη < 2√dK(w(t))k
suCh that:
α( √d)3(η)2kVf (W(t))k3
1 -√dη∣∣Vf(W㈤)k
α( √d)3(η)2kVf (W(t))k3
-√dηkVf (W⑴)k-
dαηkVf(W(t))k2.
Thus, we have established the following bound:
hVf (W(t)), W(t+1) - W(t)i ≤
OU + dαη)kVf(WWk2=(-氐)kvf("肝.
ProCeeding analogously as above, we establish a bound onkW(t+1) - W(t)k2:
kW(t+I)-W⑴k2≤ (O + α2d2η2) kVf(W㈤)k2= (O + 工)kVf(W㈤)k2.
Putting the bounds together we obtain:
f(W(t+1))-f(W(t)) ≤ (-太 + £ + g) kVf(W㈤肝.
We seleCt our learning rate to make the CoeffiCient ofkVf(W(t)k2 negative, and thus by the PL-
inequality (4), we have:
f (W(t+1))- f (w㈤)≤ (-2η- + 碧 + Lη2) 2μ(f (w⑴)-f (w*))
2αu	2αl	8αu
=⇒ f (w(t+1))- 〃w*) ≤ (i -竺+μLη2+μLη22) (f(w⑴)-f W)).
αu	αl	4αu
13
Under review as a conference paper at ICLR 2021
Hence, w (t) converges linearly when:
0<1-
μη + μLη2 + μLη2 < 1
αu	α2	4αU
To show that the left hand side is true, we analyze when the discriminant is negative. Namely, we
have that the left side holds if:
<0
μ2	4μL	μL
α2	α2	α2
αu	αl	αu
μ	4L	L
αU	< -2 + αl2	αU
4Lα2
-^u + L
=⇒ μ <
Since μ < L by Lemma 3, this is always true. The right hand side holds when η < 5Lα⅛h，which
holds by the assumption of the theorem, thereby completing the proof.	口
Note that if f is non-negative and μ-PL*, then we have:
t1	1	1
η(t) ≤ —,____ ,	= ≤ 一,______ ,	= ≤   --------------
_ 2√2Ld√f(w(0)) — 2√2Ld√f(w(t)) ^ 2√d∣Nf(w(t))k
Hence, We can use a fixed learning rate of η = min (54Oh, 2√∑Ld√7Γ^))由 由‘‘ setting.
E Conditions for Monotonically Decreasing Gradients
As discussed in the remarks after Theorem 2, we can provide a fixed learning rate for linear conver-
gence provided that the gradients are monotonically decreasing. As we show below, this requires
special conditions on the PL constant, μ, and the smoothness constant, L, for f.
Proposition 1. Suppose f : Rd → R is L-smooth and μ-PL and φ : Rd → Rd is an infinitely
differentiable, analytic function with analytic inverse, φ-1. If there exist αl, αu > 0 such that:
(a)	αlI 4 Jφ 4 αuI,
k!
(b)	∣∂iι,...ik φj 1(x)∣≤ 20^d ∀x ∈ Rd ,iι,...ik ∈ [d],j ∈ [d], k ≥ 2,
μ > 4αU + α2
L 4aU + 2α2，
then generalized mirror descent converges linearly for any η < min
_______1______
,2√dkVf(w(0))k
Proof. Let C = 1 -詈 + μLr~ + μμLτ-. We follow exactly the proof of Theorem 2 except that
at each timestep We need C < L (which is less than 1 by Lemma 3) in order for the gradients to
converge monotonically since:
∣∣Vf (w(t+1))k2 ≤ 2L(f (w(t+1)) - f(w*)) See Lemma 2
≤ 2LC(f(w㈤)-f(w*))
LC
≤ — ∣Vf(w⑴)∣2 As f is μ-PL.
μ
Hence in order for ∣Vf (w(t+1))∣2< ∣Vf(w(t))∣2,we need C < L. Thus, we select our learning
rate such that:
0<1-
μη + μLn + μLη2 < μ
αu αl2	4α2u L .
14
Under review as a conference paper at ICLR 2021
Now, in order to have a solution to this system, we must ensure that the discriminant of the quadratic
equation in η when considering the right hand side inequality is larger than zero. In particular we
require:
μ > 4αU + α2
L	4αU + 2α2 ,
>0
which completes the proof.
□
F Proof of Theorem 3
We repeat the theorem below for convenience.
Theorem. Suppose f (x) = n1 En=I fi(x) where f : Rd → R are non-negative, Li-smooth func-
tions with L = supi∈[n] Li and f is μ-PL*. Let φ : Rd → Rd be an infinitely differentiable, analytic
function with analytic inverse, φ-1. SGMD is used to minimize f according to the updates:
Φ(w(t+1)) = φ(w(t)) — η(t) Vfit (w(t)),
where it ∈ [n] is chosen uniformly at random and η(t) is an adaptive step size. If there exist
αl, αu > 0 such that:
(a)	αlI 4 Jφ 4 αuI,
(b)	∣∂iι,...ik φ-1(x)∣≤ 2α! ;L ∀x ∈ Rd,iι,...ik ∈ [d],j ∈ [d],k ≥ 2,
then
min
SGMD converges linearly
(Aμα2____________1_________ʌ
∖5L2au , 2√d maxi kVfi(w(t)) k J '
to a global minimum for any	η(t)
<
Proof. We follow the proof of Theorem 2. Namely, Lemma 4 implies that f is L-smooth and hence
f(w(t+1)) — f(w(t)) ≤ hVf(w(t)),w(t+1) — Wltt) + 刍|w(t+1) — w(t)k2.
As before, we want to bound the two quantities on the right by kVf (w(t))k2. Following the bounds
from the proof of Theorem 2, provided η(t) < 2√dkVfi(w(t))k , We have
∞1
vf(w(t))TXk！ [pdι,i2...ik=ι(-η)k∂iι,...ikφ-1(φ(w(t)))(Vfit(w(t))iι...Vfit(W⑶)lk)]
k=2
≤ 学kVf(w(t))kkVfit(w(t))k.
2αuL
To remove the dependence of η(t) on it, We take η(t) <
2√dmaxi|3i(w(t))k ∙ Since f	”-^
fi is non-negative for all i ∈ [n], kVfi(W(ttk≤ j2Lfi(w⑴)Thus, we can take
η(t)
<
11
—，	,_= ≤ -F=-
2 √2dLn，f (w(t))	2 √d maxi ∣∣V fi (w(t)) k
This implies the following bounds:
hVf(w(t)),w(t+1) — W⑴)≤ —产Vf(W⑴)TJ-1(w(t))Vfit(w(t))+ (富、kVf(w(t))kkVfit(w(t))k,
φ	2αuL
kW(t+1) - W(ttk2≤ (F + 占! kVfit(W(tt)k2.
15
Under review as a conference paper at ICLR 2021
Putting the bounds together we obtain:
f(w(t+1)) - f(w⑴)≤ -η⑶Nf(W⑴)TJ-1(w⑴)Vfit(W㈤)+ (门、∣∣Vf(W⑴)k∣∣Vfit(W⑴)k
2αuL
+ (η(22 + 占! kVfit(W⑴肝
αl2	4αu2	t
≤ -产Vf(W⑴)TJ-1(W⑴)Vfit(W⑴)+ (置)2L/f(W㈤)fit(W⑴)
+ (B2 + 厂! kVfit(W⑴肝
αl2 4αu2	t
Now taking expectation over it, we obtain
E[f (W(t+1))] - f(W ⑴)≤ (-r)kVf(W㈤)k2+ ( R ) ,f(W㈤)E qfit (W㈤)
Lη(t)2
+ ∖^0r +
E[2L(fit(W⑴)-fit (w*))]
≤
LOL+⅞?!….
αl 4αu
where the second inequality follows from Jensen’s inequality and the third inequality follows from
Lemma 2. Hence, we have:
E[f(Wi]≤ (1-叱 + H + 的! (/(WB).
αu αl2 4α2u
NOWletC =(-吟 + Loy + L W ). Then taking expectation with respect to it,it-ι, ...iι,
yields
Eit,...,i1[f(W(t+1))] ≤(1+C)(Eit,...,i1[f(W(t))]
= (1 + C)(Eit-1,...,i1[Eit|it-1,...i1[f(W(t))]])
= (1 + C)(Eit-1,...,i1f(W(t))]).
Hence, we can proceed inductively to conclude that
Eit,...,i1[f(W(t+1))] ≤(1+C)t+1(f(W(0)))).
Thus if 0 < 1 + C < 1, we establish linear convergence. The left hand side is satisfied since μ < L,
and the right hand side is satisfied for η(t) < 4μι0l , which holds by the theorem,s assumption,
5 L αu
thereby completing the proof.	口
G Proof of Theorem 4
We restate the theorem below.
16
Under review as a conference paper at ICLR 2021
Theorem. Suppose φ : Rd → Rd is an invertible, αu-Lipschitz function and that f : Rd →
R is non-negative, L-smooth, and μ-PL* on B = {x ; φ(x) ∈ B(φ(w(0)), R)} with R =
ɑιμ
α2	d
u . If for all x, y ∈ Rd there exists αl > 0 such that
hφ(x) - φ(y),x-yi ≥ αlkx - yk2,
then,
(1)	There exists a global minimum w(∞) ∈ B.
(2)	GMD converges linearly to w(∞) for η = αl.
L
(3)	If w* = arg min ∣∣φ(w) 一 φ(w(0))k then, kφ(w*) 一 φ(w(∞))k≤ 2R.
w∈B ; f (w) = 0
Proof. The proof follows from the proofs of Lemma 1, Theorem 1, and Theorem 4.2 from Liu et al.
Lα 2
(2020). Namely, We Will proceed by strong induction. Let K = Lou2. At timestep 0, We trivially
have that w(0) ∈B and f(w(0)) ≤ f(w(0)). At timestep t, we assume that w(0),w(1),.. .w(t) ∈B
and that f (w(i)) ≤ (1 一 κ-1)f (w(i-1)) for i ∈ [t]. Then at timestep t + 1, from the proofs of
Lemma 1 and Theorem 1, We have:
f (w(t+1)) ≤ (1 一 κ-1)f(w(t))
Next, we need to show that w(t+1) ∈ B. We have that:
t
kφ(w(t+1)) - φ(w(0))k = X -Nf(W*
i=0
t
≤ ηX∣Vf (w(i))k By the Triangle Inequality
i=0
(7)
________ t
η ,2Lf(w ⑼)—X(1 -KT)i
αl i=0
≤ ηq2Lf (w(0))αu --√^=
αι 1 -	1 - K-1
≤ η√2Lf (w(0))αu -ɪ
αι K-1
=αl q∕2Lf(w(0)) αu 2 αuL
LV	αι	αιμ
=2√2L√f(WW)αU = R
ɑιμ
17
Under review as a conference paper at ICLR 2021
The identity in (7) follows from the proof of f (w(t+1)) ≤ (1 - κ-1)f (w(t)). Namely,
f(w(t+1)) - f(w㈤)≤ -2L2k-ηVf(w㈤)k2
kVf (w(t))k≤
f (w(t)) - f(w(t+1))
Vf(w(t))≤η
f(w(t)) - f(w(t+1))
Hence We conclude that w(t+1) ∈ B and so induction is complete.
□
In the case that φ(t) is time-dependent, we establish a similar convergence result by assuming that
P φ(i) (w(i)) - φ(i-1)(w(i)) = δ < ∞. Additionally if α(ut) has a uniform upper bound and αl(t)
i=1
has a uniform loWer bound, then:
φ(t)(w(t+1)) - φ(0)(w(0))k = kφ(t)(w(t+1)) - φ(t)(w(t)) + φ(t)(w(t)) - φ(t-1)(w(t))
+ φ(t-1)(w(t)) - φ(t-1)(w(t-1)) +... φ(0)(w(1)) - φ(0)(w(0))
t
t
≤ X φ(i)(w(i+1)) - φ(i)(w(i))
i=0
≤R+δ
+	φ(i)(w(i)) - φ(i-1)(w(i))
i=1
Hence We Would conclude that φ(t) (w(t+1)) ∈ B(φ(0) (w(0)), R + δ).
H Proof of Corollary 1 and Corollary 2
We repeat Corollary 1 beloW.
Corollary. Let f : Rd → R be an L-smooth function that is μ-PL. Let α(t) = mi□i∈[d] G(t) and
α(ut)2
η(t)
二 maxi∈[d] Giti. If lim —⅛ = 0, then Adagrad converges linearly for adaptive step Size
i,i t→∞ αu
(t)
αl
L .
Proof. By definition of G(t), We have that:
(1)	αl(t) = min
l	i∈[d]
(2)	α(t)2 = max
u	i∈[d]
From the proof of Theorem 1, using learning rate η(t) = αL- at timestep t gives:
f(w(t+1)) — f(w*) ≤
1 - μα(M
Laut)2
(f (w(t)) — f(w*))
α(t) 2
Let κ(t) = "%)2. Although we have that (1 一 κ(t)) < 1 for all t, we need to ensure that
Lα(ut)
∞
Q (1 - κ(i)) = 0 (otherwise we would not get convergence to a global minimum). Using the
i=0
α(t)
assumption that lim ¾∙ = 0, let lim (1 一 κ(t)) = 1 一 c < 1. Then using the definition of
t→∞ αu	t→∞
the limit, for 0 <	< c, there exists N such that for t > N , κ(t) 一 c < . Hence, letting
18
Under review as a conference paper at ICLR 2021
c* = min (C — e, ʃmin )κ(t) J, implies that (1 — κ(t)) < 1 — c* for all timesteps t. Thus, We
have that:
YY(I-K⑺)< YY(I- c*)= 0
i=0	i=0
Thus, Adagrad converges linearly to a global minimum.
□
We present Corollary 2 beloW.
Corollary 2. Let f : Rd → R be an L-smooth function that is μ-PL Let ai = mi□i∈[d] Gii) ∙
α(t)	α(0)
Then Adagrad converges linearly for adaptive step size η(t) = ^jL- or fixed step size η = ^L- if
俨
2L(f(w (O))-f(w*))
> μ
Proof. By definition of G(t), We have that:
(1) αl(t) = min
l	i∈[d]
(2) α(t)2 = max
u	i∈[d]
In particular, We can choose αl = αl(0) uniformly. We need to noW ensure that α(ut) does not diverge.
(t)2
We prove this by using strong induction to shoW that αu	≤ S uniformly for some S > 0. The
base case holds by Lemma 2 since We have:
40)2 ≤kVf(W(O))k2= S
NoW assume that α(ui) < S for i ∈ {0, 1, . . . t — 1}. Then We have:
t
α(ut)2 ≤ XkVf(w(i))k2
i=0
t
≤ X 2L(f(w(i)) — f (w*)) by Lemma 2
i=0
t-1 i
≤ 2L(f(w(0)) — f(w*)) X Y 1—
i=0 j=0
t-1 i
≤ 2L(f(w(0)) — f(w*)) X Y 1—
μα(j)
LoF
(O)2 * 4
μ0l
LS
i=0 j=0
≤ 2L(f(w(O))- f (w*))---* l^op
1 _ 1 + μ%
-ɪ + LS
LS
= 2L(f (W(O)) — f (w ))——2 < S by assumption
Hence, by induction, α(ut) is bounded uniformly for all timesteps t.
□
19
Under review as a conference paper at ICLR 2021
I Proof of Corollary 3
We present the corollary below.
Corollary 3. Suppose ψ is an ɑι -strongly convex function and that Vψ is au -Lipschitz. Let
Dψ (x,y) = ψ(x) — ψ(y) — Vψ(y)T(X — y) denote the Bregman divergence for x,y ∈ Rd. If
f : Rd → R is non-negative, L-smooth, and μ-PL* on B = {x ; Vψ(x) ∈ B(Vψ(w(0)), R)} with
ɑιμ
, then:
R
R2
⑴ There exists a global minimum w( ) ∈ B such that Dψ(w( ),w(0)) ≤ -—.
(2)	Mirror descent with potential ψ converges linearly to w(∞) for η = Ol.
(3)	If w* = arg min Dψ (w,w(0)), then D(w*,w(∞)) ≤ Ou3——+ —.
{w ; f (w)=0}	αl	αl
Proof. The proof of existence and linear convergence follow immediately from Theorem 4. All that
remains is to show that Dψ(w(∞),w(0)) ≤ R. As ψ is αι-strongly convex, We have:
ψ(w(∞)) ≤ ψ(w⑼)+ hVψ(w(0)),w(∞) — W(O)i + ɪ∣∣Vψ(w(∞)) — Vψ(w⑼)k2 ByLemma5
2αl
1	R2
=⇒ Dψ(w(∞),w(0)) ≤ 而∣Vψ(w(∞)) — Vψ(w(0))∣∣2≤ 布
Now let w* = argmin{w ； f(w)=o} Dψ(w,w(0)). Hence Dψ(w*,w(0)) < R2 by definition. Then
we have:
Dψ(w*
w(∞)) ≤
≤
≤
≤
≤
ɪ kV@(w*)—V@(w(M)k2
2αl
L(2∣Vψ(w*) — Vψ(w(0))∣2 +2∣Vψ(W(O)) — Vψ(w(∞))∣2)
2αl
0u ∣w* — W(O)k2 + —
αl	αl
0u -Dψ (w*,w(0)) + — By Definition 3
αl αl	αl
Ou R2	R2
31
Ol3	Ol
□
J Experiments on Over-parameterized Neural Networks
Below, we present experiments in which we apply the learning rate given by Corollary 1 to over-
parameterized neural networks. Since the main difficulty is estimating the parameter L in neural
networks, we instead provide a crude approximation for L by setting L(t) = .99 叱黑⑴？ . The
intuition for this approximation comes from Lemma 2. While there are no guarantees that this
approximation yields linear convergence according to our theory, Figure 2 suggests empirically that
this approximation provides convergence. Moreover, this approximation allows us to compute our
adaptive learning rate in practice.
Code for all experiments is available at:
https://anonymous.4open.science/r/cef30260-473d-4116-bda1-1debdcc4e00a/
20
Under review as a conference paper at ICLR 2021
ConvergenCe of Adagrad In Over-Parameterlzed NeuraI NetWorkS
1 Hidden Layer, Leaky ReLUActivation
——Step Size 0.1
---Our Step Size (Adaptive)
ιo.o
2000	4000	6000	8000	10000
Number of Epochs
(a)
1 Hidden Layer, X + sin(x) Activation
2000	4000	6000	8000	10000
Number of Epochs
(b)
Figure 2: Using the adaptive rate provided by Corollary 1 with L approximated by L(t) =
.99 kf W(t))l leads to convergence for Adagrad in the noisy linear regression setting (60 exam-
ples in 50 dimensions with uniform noise applied to the labels). (a) 1 hidden layer network with
Leaky ReLU activation Xu et al. (2015) and 100 hidden units. (b) 1 hidden layer network with
x + sin(x) activation with 100 hidden units. All networks were trained using a single Titan Xp, but
can be trained on a laptop as well.
21