Under review as a conference paper at ICLR 2021
enhancing visual representations for effi-
cient object recognition during online distil-
LATION
Anonymous authors
Paper under double-blind review
Ab stract
We propose ENVISE, an online distillation framework that ENhances VISual rep-
resentations for Efficient object recognition. We are motivated by the observation
that in many real-world scenarios, the probability of occurrence of all classes is
not the same and only a subset of classes occur frequently. Exploiting this fact, we
aim to reduce the computations of our framework by employing a binary student
network (BSN) to learn the frequently occurring classes using the pseudo-labels
generated by the teacher network (TN) on an unlabeled image stream. To main-
tain overall accuracy, the BSN must also accurately determine when a rare (or
unknown) class is present in the image stream so that the TN can be used in such
cases. To achieve this, we propose an attention triplet loss which ensures that the
BSN emphasizes the same semantically meaningful regions of the image as the
TN. When the prior class probabilities in the image stream vary, we demonstrate
that the BSN adapts to the TN faster than the real-valued student network. We
also introduce Gain in Efficiency (GiE), a new metric which estimates the relative
reduction in FLOPS based on the number of times the BSN and TN are used to
process the image stream. We benchmark CIFAR-100 and tiny-imagenet datasets
by creating meaningful inlier (frequent) and outlier (rare) class pairs that mimic
real-world scenarios. We show that ENVISE outperforms state-of-the-art (SOTA)
outlier detection methods in terms of GiE, and also achieves greater separation
between inlier and outlier classes in the feature space.
1	Introduction
Deep CNNs that are widely used for image classification (Huang et al. (2017)) often require large
computing resources and process each image with high computational complexity (Livni et al. (2014)).
In real-world scenarios, the prior probability of occurrence of individual classes in an image stream is
often unknown and varies with the deployed environment. For example, in a zoo, the image stream
input to the deep CNN will mostly consist of animals, while that of vehicles would be rare. Other
object classes such as furniture and aircraft would be absent. Therefore, only a subset of the many
classes known to a deep CNN may be presented to it for classification during its deployment.
To adapt to the varying prior class probability in the deployed scenario with high efficiency, we
propose an online distillation framework - ENVISE. Here, we employ a high capacity general purpose
image classifier as the teacher network (TN), while the student network (SN) is a low capacity
network. For greater efficiency and faster convergence, we require the coefficients of the SN to be
binary and refer to it as the binary student network (BSN). When the BSN is first deployed, it is
trained on the unlabeled image stream using the predicted labels of the TN as pseudo-labels. Once the
BSN converges to the performance of the TN, it is used as the primary classifier to rapidly classify the
frequent classes faster than the TN. However, if a rare class appears (i.e. a class absent during online
training) in the image stream , the BSN must accurately detect it as a class it has not yet encountered,
which is then processed by the TN. Since the BSN is trained only on the frequent classes, we refer to
these classes as inlier (IL) and the rare classes as outlier (OL). It is important to note, that the OL
classes are outliers with respect to the BSN only, but are known to the TN. Detecting extremely rare
classes which are unknown to both the BSN and TN (global unknowns) is beyond the scope of this
paper. Thus, assuming that the TN knows all possible classes from the deployed environment, we
1
Under review as a conference paper at ICLR 2021
aim to increase the overall efficiency of the system (without sacrificing performance) by exploiting
the higher probability of occurrence of frequent classes in a given scenario.
Our approach for detecting OL classes is motivated from the observation by Ren et al. (2019), where
networks incorrectly learn to emphasize the background rather than the semantically important
regions of the image leading to poor understanding of the IL classes. We know that attention maps
highlight the regions of the image responsible for the classifier’s prediction (Selvaraju et al. (2017)).
We empirically observe that, the attention map of the BSN may focus on the background even
when the attention map of the TN emphasizes the semantically meaningful regions of the image. In
doing so, the BSN memorizes the labels of the TN, making it difficult to differentiate between the
representations of IL and OL classes. To mitigate these issues, we propose an attention triplet loss
that achieves two key objectives - a) guide the attention map of the correct prediction of the BSN
to focus on the semantically meaningful regions, and b) simultaneously ensure that the attention
map from the correct and incorrect predictions of the BSN are dissimilar. We show that by focusing
on the semantically relevant regions of the image, the BSN will learn to distinguish between the
representations of IL and OL classes, thereby improving its ability to detect OL classes.
To assess the overall gain in efficiency of ENVISE, we propose a new evaluation metric - GiE, based
on the number of times the BSN and TN are used to process the image stream. Since the deployed
scene is comprised mostly of IL classes with few OL classes, we expect the BSN to be employed
most of the time for classifying IL classes. The TN is used rarely i.e. only when the BSN detects an
OL class. We refer to efficiency as the overall reduction in FLOPs in the online distillation framework
to process the varying prior probability of classes in the image stream. This term differs from
conventional model compression techniques (Frankle & Carbin (2019); Chen et al. (2020)) which
process the image stream comprising of classes with equal probability using a single compressed
model. To the best of our knowledge, we are the first to propose supervision on attention maps for
OL detection, and a new evaluation metric that measures the gain in computational efficiency of an
online distillation framework. A summary of our main contributions is:
•	Faster convergence of BSN: We theoretically justify and empirically illustrate that the
BSN adapts to the performance of the TN faster than the real-valued SN (RvSN). We also
demonstrate the faster convergence of the BSN for different BSN architectures over its
corresponding RvSN.
•	Attention triplet loss (Lat), which guides the BSN to focus on the semantically meaningful
regions of the image, thereby improving OL detection.
•	New evaluation metric - GiE to measure the overall gain in computational efficiency of
the online distillation framework, and
•	We benchmark CIFAR-100 and tiny-imagenet datasets with SOTA OL detection meth-
ods by creating meaningful IL and OL class pairs. ENVISE outperforms these baseline
methods for OL detection, improves separation of IL and OL classes in the feature space,
and yields the highest gain in computational efficiency.
2	Related Work
Online distillation : Distilling knowledge to train a low-capacity student network from a high-
capacity teacher network has been proposed as part of model compression (Hinton et al. (2015)).
Wang & Yoon (2020) provide a detailed review of different knowledge distillation methods. Mullapudi
et al. (2019) propose an online distillation framework for semantic segmentation in videos, while
Abolghasemi et al. (2019) use knowledge distillation to augment a visuomotor policy from visual
attention. Lin et al. (2019) propose an ensemble student network that recursively learns from the
teacher network in closed loop manner while Kim et al. (2019) use a feature fusion module to distill
knowledge. Gao et al. (2019) propose online mutual learning and Cioppa et al. (2019) propose to
periodically update weights to train an ensemble of student networks. These ensemble based methods
require large computational resources and are expensive to train. However, ENVISE involves training
a compact model that mimics the performance of the TN with less computation.
Outlier detection : Outlier detection or out-of-distribution detection (OOD) refers to detecting a
sample from an unknown class (Hendrycks & Gimpel (2016)). Existing SOTA OOD methods use
outlier samples during training or validation. Yu & Aizawa (2019) increase the distance between
IL and OL samples during training while Vyas et al. (2018); Lee et al. (2018) add perturbations
2
Under review as a conference paper at ICLR 2021
onto test images and train their network to differentiate between IL and OL samples. However in
ENVISE, the BSN is adaptively trained only on IL class images without any knowledge of OL class
images, which is a more realistic scenario. Without using outlier classes during training, Hendrycks
& Gimpel (2016); Papadopoulos et al. (2019); Bendale & Boult (2016); Hendrycks et al. (2019)
employ class probabilities or measure the similarity between IL and OL features. Yoshihashi et al.
(2019) generate discriminative feature space using reconstruction based approaches. However, the
likelihood of correct classification by these networks is based on background regions as described in
Ren et al. (2019). Hence, these methods fail when applied to real-world IL and OL class pairs since
the confidence of prediction is based on background information. We overcome this drawback by
proposing supervision on attention maps where we guide the BSN to focus on the important semantic
regions using the attention map of the teacher network as a reference. This enables ENVISE to
classify IL images with high confidence while also improving OL detection.
Teacher network
Figure 1: (a) The ENVISE architecture adaptively trains the BSN from predictions of the TN, (b)
The process updates real valued weights that minimize the error produced by their binarized version.
attention map
瓦 J = sign(wiβ
(b)
3	Online distillation for efficient object recognition
The framework of ENVISE is shown in Figure 1 (a). Initially, the TN (indicated by “1” in the figure)
classifies the images from the input stream with high accuracy, albeit at a slower rate. The BSN
(red dotted box) comprises of an adaptation module (“2”) and a binary CNN (“3”). Given an image
stream, the adaptation module learns the optimum binary weights to mimic the performance of the
TN. Once its accuracy converges to that of the TN, the adaption of the real-valued weights stops,
but the inference with binary weights continues. The OL detector (“4”) uses the softmax output of
the BSN as confidence to either generate the final prediction (if the confidence is high), or treats the
image as an OL class and redirects it to the TN for classification.
Binarization algorithm for student network: During adaptive training, the BSN does not have
access to the labeled data and mimics the behavior of the TN on the IL classes as shown in Figure 1(b).
For an input image x, let f(g(wi,j), x) represent the output of the BSN where wi,j are real-valued
weights of ith kernel in the jth layer, and g(.) is a binarizing function for these weights. During
adaptive training, the error between the output of the BSN and TN is minimized by optimizing
w↑j = argming(wi,)∣∣Lk, where L is the overall loss function used for training the BSN. While
the intermediate weight wi,j is computed during adaptive learning, the binary versions of these
weights Wij = g(wi,j) are used for fast computation. After convergence, the real valued weights are
discarded, and only the binary weights are used. The question becomes what is a good choice for
the binarization function g(.)? Following Rastegari et al. (2016), we find a binary vector bi,j (whose
elements are either +1 or -1), and a non-negative scalar αi,j such that Wii,j = αi,j.bi,j is the best
approximation of Wij in a minimum squared error sense, where αi,j = N IWij |1, bi,j = Sign(Wij).
However, unlike Rastegari et al. (2016) where a standalone network is binarized, our BSN learns the
Wii,j during adaptive training in an online distillation framework. In doing so, the BSN compensates
for the effects of binarization on the overall loss function.
3
Under review as a conference paper at ICLR 2021
Our motivation for using the BSN is that it classifies the image stream with high accuracy and
utilizes fewer FLOPs as compared to the RvSN. This results in an increase in efficiency of the overall
framework. Furthermore, the BSN converges to the performance of the TN faster than the RvSN,
which we theoretically emphasize in the Lemma below and experimentally validate in Section 4. We
provide the sketch proof here and the complete proof in Appendix A.1.
Lemma 3.1. Let RvSN and BSN represent the real-valued student network and binary student
network respectively with the same network architecture. Let R(.) denote the rate of convergence in
terms of accuracy when the student network is adaptively trained using the pseudo-labels from the
teacher network. Then, R(BSN) > R(RvSN) for the same image stream and number of iterations.
Proof : We assume our image stream as x(n) where n = 1, 2, 3...N, comprises of N samples from
the deployed scenario. Since the weights of the BSN are derived from the RvSN, we prove this lemma
first for RvSN and then extend it to the BSN. Let w* be the optimal weight value which represents
network convergence i.e. misclassification error = 0.
The weight update rule using back propagation is given by:
w(n + 1) = w(n) + ηx(n)
[ Since η = 1] w(n + 1) = w(n) + x(n)
Computing the bounds of the weights of the RvSN, we have
n2α2	2
—772 ≤ kw(n + I)k ≤ nβ
kw*k
(1)
(2)
where α = argminx(n)∈C w?x(n) and β = argmaxx(n)∈C kx(k)k2. From eq. 2, we can say that
for this inequality to be satisfied, there exists nr? which denotes the optimal number of samples for
which the RvSN converges i.e. we obtain w? at n = nr?. This is given as follows:
n?2 ɑ2
n?e
nr?
β kw*k2
α2
(3)
Thus from eq. 3, we can say that the RvSN achieves convergence after being trained on e吧 k
number of samples. Since, the weights of the BSN are derived from the RvSN, we substitute the
value of binary weight W* from Rastegari et al. (2016) to obtain
n? = n?	(4)
Here, nb? is the optimal number of samples for which the BSN converges. Comparing eq. 3 and eq. 4,
we observe that number of nb* < nr? i.e. the BSN takes fewer samples to converge to the performance
of the TN than RvSN given the same network architecture.
Knowledge transfer from teacher to student network: Given an unlabeled image stream, the
BSN is trained from the predictions of the TN as hard pseudo-labels using cross-entropy loss, as
Ld = --N Pi yilog(si). Here, y and S are the pseudo-labels generated by the TN and the softmax
output of the BSN respectively, N is the total images in the image stream. Once the BSN converges
to the performance of the TN, we employ the BSN as the primary classifier during inference. When
the deployed scenario does not change for a long duration, the BSN classifies the IL classes faster
than the TN without relying on the latter. This improves the overall efficiency of the framework as
the TN is now used to classify only the OL classes (number of OL images << number of IL images).
However, to maintain overall accuracy, the BSN must also accurately determine when the image from
the input stream belongs to an OL class so that the TN can be used in such cases.
We know that attention maps highlight the regions of the image responsible for the classifier’s
prediction (Li et al. (2018)). Some examples of the attention maps from the correct predictions of
the BSN and the TN are shown in Figure 2 (a). The first three columns in Figure 2 (a) show that
although the BSN (adaptively trained using Ld) and the TN both correctly classify the images, their
attention maps are significantly different. The attention map from the correct prediction of the BSN
4
Under review as a conference paper at ICLR 2021
Attention map Attention map
BSN from BSN from Attention map
Input Attention map highest second highest BSN after using
Image	TN	prediction prediction	Lat
C = 0.93 C = 0.68 C = 0.21 C = 0.96
c = 0.77 c = 0.42 c = 0.33 c = 0.92
(a)
Figure 2: (a) The attention map visualization illustrating that after the BSN is trained using the
proposed attention triplet loss, it learns to focus on the semantically meaningful regions in the image.
The confidence of prediction (c) also increases. (b) Separation between IL and OL classes in feature
space improves with the proposed attention triplet loss. More visualizations in Appendix A.4
Ld
+
Lat
(b)
focuses on the background while that of the TN lies on the semantically meaningful regions of the
image. Furthermore, the attention maps of the BSN’s correct and incorrect predictions are visually
similar which causes the BSN to learn incorrect representations of the IL images while correctly
classifying them. To better differentiate between IL and OL classes, the BSN should not memorize
the predictions of the TN, but learn proper representations of the IL images.
To achieve this, we propose the attention triplet loss given in Equation 5 that causes the BSN’s
attention map to be similar to that of the TN while forcing the attention map from the BSN’s correct
and incorrect prediction to be different. We use Grad-CAM (Selvaraju et al. (2017)) to compute the
attention map A from the one-hot vector of the predicted class. To generate the attention map triplets,
we use the attention map from the TN’s prediction At as the anchor. The hard positive attention
map Asp is obtained from correct prediction of the BSN. When the BSN misclassifies an image
(prediction of BSN and TN are different), we use the label from the TN to compute Asp . Motivated
by the findings in Wang et al. (2019), we observe that the attention maps generated from the correct
and incorrect predictions of the BSN are visually similar. Hence, we use the attention map from
an incorrect prediction as the hard negative attention map Asn to enforce its separability from Asp .
To avoid bad local minima and a collapsed model early on during training, mining suitable hard
negatives are important (Schroff et al. (2015)). Hence, we use the attention map from the second most
probable class (incorrect prediction) as our hard negative due to its similarity with the hard positive
(third and fourth column in Figure 2 (a)). Thus, we formulate the attention triplet loss Lat as:
NK	K
Lat = NK X(X IlAtk- AspklI2 - X IIAtk
nk	k
Asnk2 +δ)	(5)
—
Initially, the hard negative lies within the margin since its squared distance with the hard positive is
small. Hence, Lat enforces separation between hard positive and hard negative by a distance greater
than the value of margin δ, which we empirically set as 1.5. N is the total number of samples and K
is the total number of pixels in A. Using Lat and Ld, we formulate our final objective function as:
Lf = λ1Ld + λ2Lat
(6)
where λ1 and λ2 are the weights of Ld and Lat which we empirically set as 1 and 0.2 respectively. The
effect of Lat is shown in the last column of Figure 2 (a) where the BSN learns proper representations
of the IL image due to its attention map being very similar to that of the TN (second column).
Furthermore, the improvement in feature space separation between the IL and OL classes in Figure 2
(b) shows that the BSN not only forms clusters of the different IL classes (in red), but is also well
separated from the OL classes (in purple).
4	Experimental Details
Datasets and implementation: We evaluate ENVISE on CIFAR-100 (Krizhevsky & Hinton (2009))
and tiny-imagenet (TI) (Yao & Miller (2015)) datasets by creating meaningful IL and OL super-class
pairs that mimic real-world scenarios. On the test set of CIFAR-100 and validation set of TI dataset,
we create 12 and 10 super-classes respectively. We summarize our experimental settings in Table 1.
5
Under review as a conference paper at ICLR 2021
We use DenseNet-201 (Huang et al. (2017)) as the TN, pre-trained on the training set of all classes of
CIFAR-100 or the TI dataset individually. The BSN is a VGG16 (Simonyan & Zisserman (2014))
network whose weights, except for the first and last convolution layer, are binarized (Rastegari et al.
(2016)). We also compare this with other choices for the BSN including AlexNet, ResNet-18 and
ResNet-50. In the Appendix A.3, we illustrate that ENVISE is insensitive to the specific TN and BSN
architecture used and achieves high performance gains even with different network architectures.
Table 1: Different pairs of IL and OL super-classes on the CIFAR-100 and TI datasets.
CIFAR-100 (Krizhevsky & Hinton (2009))				tiny-imagenet (Yao & Miller (2015))							
IL \ OL super-classes	# IL train	# OL train	# IL test	#OL test	# total I classes	IL \ outlier super-classes	# IL train	#OL train	# IL test	#OL test	# total classes
aquatic animals \ food container (C1 )	840	0	1200	500	17	animals \ garments (T1)	875	0	1250	900	43
flora \ electrical items (C2 )	630	0	900	500	14	reptiles \ edible items (T2)	630	0	900	800	34
fruits \ furniture (C3 )	350	0	500	500	10	aquatic animals \ birds (T3)	420	0	600	250	17
insects \ manmade things (C4)	560	0	800	500	13	household items \ nature (T4)	1432	0	2050	700	55
animals \ people (C5 )	1750	0	2500	500	30	vehicles \ miscellaneous (T5)	525	0	750	1800	51
outdoor places \ vehicles (C6)	420	0	600	1000	16	-	-	-	-	-	
Training and evaluation: Throughout our experiments, we fix the TN and do not train it. We train
the BSN using the pseudo-labels and attention map from the TN using the cost function in eq. 6 only
on the IL classes of the super-class from Table 1. This is done using a learning rate of 1e-4 for 10
epochs using the Adam optimizer (Kingma & Ba (2014)) with a batch size of 10. Once the BSN
converges to the performance of the TN, we employ it as our primary classifier during inference. To
create a more realistic scenario during inference, we random center crop and randomly rotate the
image stream between [-15°, 15°]. We assume that the distribution of classes Win not change rapidly
in the deployed scenario, and that the BSN can be used for inference for long durations (e.g days,
weeks or months). Thus, the epochs for online distillation are expected to require a small fraction of
that time. For each image from the input stream during inference (comprising of IL and OL classes),
following Hendrycks & Gimpel (2016), we compute the confidence of prediction from the softmax
probability of the predicted class. If the confidence is low, we treat the image as an OL class and
transfer it to the TN for classification.
I—TN —Retrained BSN —Retrained RVSN —QUantiZed BSN -StandalOne BSM
(a) BSN : Binary Alexnet
(b) BSN : Binary VGG-16
(c) BSN : Binary ResNet-18
Figure 3: (a) : (d) The BSN converges to the performance of the TN faster than the RvSN and the
variants of BSN. The red period illustrates adaptive training and white period illustrates inference.
We observe a similar convergence pattern with different binary network architectures.
(d) BSN : Binary ResNet-50
Gradually changing the deployed scenario: When the prior probabilities of the classes change in
the deployed scenario, the BSN quickly re-trains to learn the new IL classes and regains efficiency
on the new image stream. Figure 3 illustrates the learning behavior of the BSN when the IL and
OL classes are changed. Initially, we assume that the image stream is comprised of only IL classes
from C1 . Once the BSN (in green) converges to the performance of the TN (in red), we stop training
the BSN and observe that it achieves an accuracy comparable to the TN’s accuracy. The learning
behaviour of the RvSN (in blue) is slower than the BSN and has poorer accuracy after 10 epochs.
6
Under review as a conference paper at ICLR 2021
Directly binarizing the RvSN (purple line) is also worse than the BSN, and substantially differs from
the binarization algorithm in Section 3 which compensates for the quantization effect. The yellow
line shows the performance of the standalone BSN which is not trained during the adaptive training.
In Figure 3 we keep the IL/OL superclasses the same for 40 epochs, and then switch the input stream
to a different scenario. The 10 epochs shaded in red indicate the adaptive training phase, while the
unshaded (white) intervals indicate the 30 epochs for inference. We randomly choose our inference
phase as 30 epochs. We show in Appendix A.3 that during inference, the combined accuracy of
ENVISE is identical to that of the stand-alone TN, which illustrates that ENVISE maintains the
overall accuracy of the system. When the scenario changes after 40 epochs (e.g from C1 to C2),
we observe similar and consistent learning behaviour, and the BSN retrains quickly from the TN
to regain efficiency. We also observe a similar convergence pattern for different BSN architectures
when adaptively trained from the same TN in Figure 3. Here, we observe that the rate of convergence
for Binary AlexNet is the fastest and that of Binary Resnet-50 is the slowest. This illustrates that
a smaller binary network would make an ideal BSN in real-world scenarios, since it would utilize
fewer epochs during adaptive training.
ED	CE	CS	ODIN	OECC	MCD	CAL	ENVISE
P = 0.68	P = 0.55	P = 0.42	P = 0.59	P = 0.064 P = 0.077	P = 0.39	P = 0.041
P = 0.51	p = 0.46	p = 0.40	p = 0.28	P = 0.21	P = 0.37	P = 0.58	P = 0.016
Figure 4: Feature space separation of the IL and OL classes for different baseline methods. p-value
denotes the overlap between the IL and OL features where, lower value indicates better separation.
Comparison with SOTA outlier detection methods: The ability to accurately distinguish between
IL and OL classes is key for improving the efficiency of ENVISE. We benchmark ENVISE on the
CIFAR-100 and TI datasets with SOTA OL detection methods like error detection (ED) (Hendrycks
& Gimpel (2016)), confidence estimation (CE) (DeVries & Taylor (2018)), confidence scaling (CS)
(DeVries & Taylor (2018)), ODIN (Liang et al. (2018)), outlier exposure with confidence control
(OECC) (Papadopoulos et al. (2019)), maximum classifier discrepancy (MCD) (Yu & Aizawa (2019))
and confidence aware learning (CAL) (Moon et al. (2020)). We use the official code of these methods
and adaptively train them using their proposed loss functions on our experimental settings. For fair
comparisons in terms of network architecture, we use the TN as DenseNet-201 and the SN as our
binary VGG-16. Following Hendrycks & Gimpel (2016), we use FPR at 95% True positive rate
(TPR), detection error (DE), area under ROC curve (AuROC), and area under precision-recall curve
(AuPR) as our evaluation metric. From Table 2, we observe that ENVISE outperforms the best
performing baseline method by achieving the lowest FPR and detection error with high AuROC and
AuPR. SOTA model compression techniques (Frankle & Carbin (2019)) do not focus on processing
images from the input stream with varying prior class probabilities. Hence, direct comparison with
these methods is not meaningful since their objectives are different from those of ENVISE.
We visualize the separation of IL and OL classes in the feature space using UMAP (McInnes
et al. (2018)) across the last fully connected layer of the BSN. Figure 4 shows that ENVISE
has the best separation between IL and OL classes which is consistent with the quantitative
analysis shown in Table 2. To quantify the feature separation, we compute the p-value using
Wilcoxon’s rank sum test (Wilcoxon (1992)) for the null hypothesis that the IL and OL feature
distribution are the same i.e they overlap. Ideally for high separation, the p-value should be
less than 0.05 (rejecting the hypothesis with 95% confidence). We observe from Figure 4 that
the p-value of ENVISE has the smallest value as compared to the SOTA OL detection methods.
This indicates that ENVISE achieves the least overlap between IL and OL classes, thereby
outperforming the SOTA OL detection methods in its ability to better detect OL classes. A
detailed comparison of ENVISE with each baseline method on different pairs of IL and OL
classes and the corresponding feature space visualization is presented in the Appendix A.4.
Furthermore, in Appendix A.2, we ablate the effect of the proposed Lat, margin (δ) with the BSN
7
Under review as a conference paper at ICLR 2021
and also performance of OL detection with RvSN. We present additional discussions in Appendix A.3.
Table 2: Performance comparison of ENVISE with the best performing baseline method on different
IL and OL class pairs on CIFAR-100 and TI dataset. J and ↑ indicate smaller and greater value is
better respectively. The detailed comparison with each baseline is in the Appendix A.4
Dataset	Group	Method \ Metric	FPR (95% TPR) J	Detection error J	AuROC ↑	AuPR (IL) ↑	AuPR (OL) ↑
		Best Baseline.	0.90	0.42	0.59	0.77	0.37
	C1	ENVISE	0.88	0.39	0.62	0.80	0.39
		Best Baseline	0.69	0.28	0.79	0.86	0.71
	C2	ENVISE	0.41	0.16	0.89	0.88	0.92
		Best Baseline	0.69	0.21	0.86	0.87	0.87
CIFAR-100	C3	ENVISE	0.57	0.18	0.89	0.91	0.87
		Best Baseline	0.65	0.27	0.79	0.85	0.72
	C4	ENVISE	0.61	0.26	0.75	0.82	0.77
		Best Baseline	0.82	0.39	0.62	0.90	0.23
	C5	ENVISE	0.76	0.29	0.79	0.95	0.42
		Best Baseline	0.63	0.22	0.85	0.79	0.90
	C6	ENVISE	0.49	0.16	0.91	0.87	0.97
		Best Baseline.	0.72	0.28	0.84	0.88	0.65
	T1	ENVISE	0.69	0.23	0.89	0.91	0.71
		Best Baseline	0.80	0.29	0.75	0.79	0.68
	T2	ENVISE	0.78	0.26	0.81	0.86	0.73
		Best Baseline	0.78	0.32	0.68	0.76	0.61
Tiny-Imagenet	T3	ENVISE	0.74	0.20	0.81	0.93	0.63
		Best Baseline	0.83	0.20	0.69	0.78	0.51
	T4	ENVISE	0.78	0.18	0.75	0.91	0.44
		Best Baseline	0.80	0.30	0.90	0.85	0.79
	T5	ENVISE	0.78	0.26	0.91	0.85.	0.84
(a)
(b)
Figure 5: (a) Comparison of ENVISE with SOTA OL detection methods, and (b) Comparison of
different BSN architectures in terms of GiE on different super-classes of the CIFAR-100 dataset.
Gain in Efficiency (GiE): The main focus of our work is to develop an efficient system to classify
the image stream with low computational cost and high accuracy. We propose a new evaluation metric
- GiE to measure the overall gain in efficiency from the number of times the BSN and TN are used
individually to classify the image stream. During inference, we require the BSN to accurately classify
the IL classes such that it does not rely on the TN thereby reducing the overall computation cost.
Furthermore, the BSN should detect an image as OL so that the TN can be used in such cases. To
process a single image on the CIFAR-100 dataset, the TN uses 9048 MFLOPs and 28.5msec, while
the RvSN uses 310 MFLOPs and 3.04 msec. However, the BSN achieves 6 MFLOPs and 0.57 msec;
a 〜50x reduction in computation and a 〜6× speed improvement over the RvSN. Furthermore, the
BSN occupies 〜30x and 〜100x less memory than the RvSN and TN respectively.
During adaptive training, since each image is processed by both the BSN and TN, the total FLOPs
used is FLtr = (X + Y ), where X and Y are the FLOPs used by the TN and BSN respectively.
Once the the BSN converges to the performance of the TN, only the BSN is used to process the input
image. Hence, for IL images the total FLOPs is FLin = Y + (fp x X), where (fp x X) indicates
the number of times the TN is used when the BSN misclassifies an IL class as an OL. Similarly for
OL images, the FLOPs is FLo = Y + (od x X), where (od x X) denotes the OL classes correctly
detected by the BSN which is then processed by the TN. Since the adaptive training and inference
phase for a given super-class occur for Ntr and Nin epochs respectively, the relative FLOPS of
8
Under review as a conference paper at ICLR 2021
ENVISE compared to the TN is given as
RFL
Ntr	FLtr	Nin	(P X FLin + q X FLo)
Nt X X + NT X	X
(7)
Here, NT = Ntr + Nin . For high computational efficiency, RFL should be small. If the BSN works
with perfect accuracy (i.e. fP = 0 and od = 1.0), then the minimum value of RFL during inference
is RFLmin = Y +χ×x. We define GiE as RFL normalized with respect to its minimum value, i.e.
GiE
RFLmin
RFL
Y + q X X
hNr X FLtr + NNin X (P X FLin + q X FLo)i
(8)
To numerically compute GiE, we use X = 9048 (TN), Y = 6 (BSN), Ntr = 10 and Nin = 30,
while P and q are calculated from Table 1. However, as discussed previously, Nin can be a very large
value since the inference phase can occur for a very long duration. Figure 5 illustrates that ENVISE
outperforms SOTA OL detection methods in terms of GiE for different scenarios on CIFAR-100
dataset. Thus, the Lat ensures that the BSN can accurately distinguish between representations of IL
and OL classes which enables ENVISE to operate in an efficient manner.
5	Conclusion
We propose ENVISE, an online distillation framework which uses a BSN to adaptively learn relevant
knowledge from a TN to quickly classify the frequent classes in the deployed scenario. In doing
so, it automatically allocates processing resources (i.e. either the BSN or the TN) to reduce overall
computation. To learn proper representations of the IL classes, we propose an attention triplet loss
that enables the BSN to learn the semantically relevant regions of the image that are emphasized
by the TN. This enables the BSN to accurately distinguish between representations of IL and OL
classes which is key for maintaining overall accuracy. Our experiments show that the BSN i) quickly
converges to the performance of the TN, ii) classifies IL classes more accurately than a RvSN and
other variants of the BSN, and iii) distinguishes between IL and OL classes with lower FPR and
detection error than other SOTA OL detection methods on CIFAR-100 and tiny-imagenet datasets.
We introduce a new metric GiE to assess the overall gain in efficiency, and experimentally show that
the attention triplet loss enables ENVISE to achieve higher GiE than SOTA OL detection methods.
We show that ENVISE is agnostic to the specific TN and BSN and achieves high gains with different
BSN and TN architectures.
References
Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah, and Ladislau Boloni. Pay attention!-robustifying
a deep visuomotor policy through task-focused visual attention. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 4254-4262, 2019.
Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1563-1572, 2016.
Yun-Chun Chen, Chen Gao, Esther Robb, and Jia-Bin Huang. Nas-dip: Learning deep image prior
with neural architecture search. In European Conference on Computer Vision (ECCV), 2020.
Anthony Cioppa, Adrien Deliege, Maxime Istasse, Christophe De Vleeschouwer, and Marc
Van Droogenbroeck. Arthus: Adaptive real-time human segmentation in sports through online
distillation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp. 1-9, 2019.
Terrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection in
neural networks. arXiv preprint arXiv:1802.04865, 2018.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. International Conference on Learning Representations, 2019.
Liang Gao, Xu Lan, Haibo Mi, Dawei Feng, Kele Xu, and Yuxing Peng. Multistructure-based
collaborative online distillation. Entropy, 21(4):357, 2019.
9
Under review as a conference paper at ICLR 2021
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. Proceedings of International Conference on Learning Representations, 2019.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Jangho Kim, Minsung Hyun, Inseop Chung, and Nojun Kwak. Feature fusion for online mutual
knowledge distillation. arXiv preprint arXiv:1904.09058, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffreyx Hinton. Learning multiple layers of features from tiny images. 2009.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing
Systems, pp. 7167-7177, 2018.
Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look: Guided
attention inference network. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 9215-9223, 2018.
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution
image detection in neural networks. International Conference on Learning Representations, 2018.
Rongcheng Lin, Jing Xiao, and Jianping Fan. Mod: A deep mixture model with online knowledge
distillation for large scale video temporal concept localization. arXiv preprint arXiv:1910.12295,
2019.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in neural information processing systems, pp. 855-863, 2014.
Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold
approximation and projection. The Journal of Open Source Software, 3(29):861, 2018.
Jooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang. Confidence-aware learning for
deep neural networks. In International Conference on Machine Learning, 2020.
Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, and Kayvon Fatahalian. On-
line model distillation for efficient video inference. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 3573-3582, 2019.
Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, and Jiamian Wang.
Outlier exposure with confidence control for out-of-distribution detection. arXiv preprint
arXiv:1906.03509, 2019.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European conference on computer
vision, pp. 525-542. Springer, 2016.
Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and
Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In Advances in
Neural Information Processing Systems, pp. 14680-14691, 2019.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 815-823, 2015.
10
Under review as a conference paper at ICLR 2021
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, and Theodore L
Willke. Out-of-distribution detection using an ensemble of self supervised leave-out classifiers. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 550-564, 2018.
Lezi Wang, Ziyan Wu, Srikrishna Karanam, Kuan-Chuan Peng, Rajat Vikram Singh, Bo Liu, and
Dimitris N Metaxas. Sharpen focus: Learning with attention separability and consistency. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 512-521, 2019.
Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelli-
gence: A review and new outlooks. arXiv preprint arXiv:2004.05937, 2020.
Frank Wilcoxon. Individual comparisons by ranking methods. In Breakthroughs in statistics, pp.
196-202. Springer, 1992.
Leon Yao and John Miller. Tiny imagenet classification with convolutional neural networks. CS
231N, 2(5):8, 2015.
Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, and Takeshi Naemura.
Classification-reconstruction learning for open-set recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 4016-4025, 2019.
Qing Yu and Kiyoharu Aizawa. Unsupervised out-of-distribution detection by maximum classifier
discrepancy. In Proceedings of the IEEE International Conference on Computer Vision, pp.
9518-9526, 2019.
A Appendix
A.1 Proof of convergence
In Section 3 of the main paper, we describe that, one of the motivations for using the BSN instead of
the RvSN is the ability of the BSN to adapt to the performance of the TN faster than the RvSN.
Lemma A.1. Let RvSN and BSN represent the real-valued student network and binary student
network respectively with the same architecture. Let R denote the rate of convergence in terms of
accuracy when the student network is adaptively trained using the teacher network’s predictions.
Then, R(BSN) > R(RvSN) for the same input image stream and number of iterations.
Proof For the ease of understanding, we assume to have a binary classification problem with 2 classes
i.e. C = 2, the data points are linearly separable, the weights are initialized to 0 i.e. w(0) = 0 and
learning rate η = 1. The initialization value of weights and learning rate do not affect the proof.
Let us further assume that we have a stream of input images x(n) where n = 1, 2, 3...N, where N
is the total number of samples. Since the weights of the BSN are derived from the RvSN (From
Section 3.1 of the main paper), we prove this lemma for RvSN first and then extend it to the BSN.
Since, the pretrained performance of the SN is worse than the TN (comparing starting point of blue
line with the red line from Figure 3(a) of the main paper), we assume that the SN misclassifies the
images from the input stream i.e. w(n)x(n) < 0 where w(n) is the weight of the network. The
misclassification generates an error through which the network’s weights are updated and there exists
an optimal weight value w* such that the network converges i.e. misclassification error = 0. The
weight update rule using back propagation is given by:
w(n + 1) = w(n) + ηx(n)
[ Since η = 1] w(n + 1) = w(n) + x(n)
(9)
11
Under review as a conference paper at ICLR 2021
Expanding eq (9), we have:
w(1) = w(0) + x(0)
w(2) = w(1) + x(1)
⇒ w(2) = w(0) + x(0) + x(1)
[ Since w(0) = 0] w(2) = x(0) + x(1)
(10)
Similarly we have w(3) = x(2) + x(1) + x(0) and this recursively continues untill w(n + 1). Thus,
w(n + 1) = x(1) + x(2) + x(3) +  x(n)
(11)
Ideally, when the network converges, we obtain an optimal weight vector, which is represented as
w?. We define the margin α as the shortest distance of w? to the datapoint x ∈ x(n). The concept of
margin is similar to that of support vectors. Hence, the margin α is denoted as
α = argmin w?x(n)
(12)
x(n)∈C
We ignore x(0) for ease of calculation. Pre-multiplying both the sides of eq. (11) by w?, we have
w?w = w?x(1) + w?x(2) +  w?x(n)
[From eq. (12)]	w?w ≥ nα
From Cauchy - Schwarz inequality, we have
kw? k2 kw(n + 1)k2 ≥ (w?w(n + 1))2
kw?k2 kw(n + 1)k2 ≥ (nα)2
(13)
	22
kw(n +1)k2 ≥	nα ∣∣2
	kw?k2
(14)
We highlight eq. (14) as we will be using this in the later stages of the proof. We re-write eq. (9) in a
different way as w(k + 1) = x(k) + w(k) ∀ k ∈ [0, 1, 2, 3..n]. Squaring both sides of this equation
and expanding it, we have
kw(k + 1)k2 = kx(k) + w(k)k2
⇒ kw(k + 1)k2 = kx(k)k2 + kw(k)k2 + 2x(k)w(k)
[Since in misclassification w(n)x(n) ≤ 0]	kw(k + 1)k2 ≤ kx(k)k2 + kw(k)k2
⇒ kw(k + 1)k2 - kw(k)k2 ≤ kx(k)k2
Iterating eq (15) for all values of k ∈ [0, 1, 2, 3..n], we have
(15)
[Since when k = 0, w(k) = 0] kw(k + 1)k2 ≤ X kx(k)k2
(16)
k=0
Since kx(k)k2 > 0, we define β = argmaxx(n)∈C kx(k)k2 , thus eq. (9) can be re-written as
kw(n + 1)k2 ≤ nβ
(17)
From eq. (14) and (17), we have
n2α2	2
—772 ≤ kw(n + I)k ≤ nβ
llw*k
(18)
From eq. (18), we can say that for this inequality to be satisfied, there exists n? which denotes the
optimal number of samples for which the network converges i.e. we obtain w? at n = n? . This is
given as follows:
n?2 α2
n-ʌ = n*β
kw?k2
n? = βkw*k2
α2
(19)
n
12
Under review as a conference paper at ICLR 2021
Thus from eq. (19), We Can say that the network achieves convergence after being trained on β% k
number of samples. In the case of the BSN, the binary weights w^* = △*D*, where ∆* = N ∣w^*∣1
and D* = sign(w *), Let nb? be the number of samples required for the BSN to converge. Thus, eq.
(19) for the BSN would be given as
nb?
nb? =
nb?
_e kw ? k2
=
β ∣∣∆*D* k2
α2
β kw*D* k2
N 2α2
?	β∣∣w?k2 D*tD*
nb = -No—
n?N
[From eq. (19) and Rastegari et al. (2016)] n? = ^2
? n?
nb= N
(20)
From eq. (19) and (20), we see that nb* < n* i.e. the BSN takes fewer samples than the RvSN to
converge to the TN’s performance given the same network architecture. We also experimentally
observe the higher rate of convergence of the BSN over the RvSN in Figure 3 of the main paper.
A.2 Ablation study
All the ablation studies are performed on the CIFAR-100 dataset. We illustrate the effectiveness of
the attention triplet loss (Lat), the value of margin in the (Lat , significance of the hard negative term
in Lat and the performance of the BSN over the RvSN for OL detection. The quantitative analysis is
summarized in Table 3.
Effect of attention triplet loss (Lat) : To test the effectiveness of the attention triplet loss (Lat), we
train the BSN without it. Specifically, we use the TN as DenseNet-201, the BSN as Binary VGG-16
and train the BSN using only the distillation loss Ld . Comparing column ID 1 and 2 in Table 3, we
observe that training the BSN using Ld + Lat improves the mean FPR by 35.4%, mean detection
error by 41.6%, mean AuROC by 12.5%, mean AuPR of IL class by 7.4% and mean AuPR of OL
class by 30.3%. From Figure 6, we observe that the IL (red) and OL features (purple) are barely
separable when evaluated on a pre-trained BSN. When the BSN is adaptively trained using only Ld,
the features begin to separate but still have some overlap. However training the BSN using Ld + Lat
significantly enhances the feature separation thereby improving OL detection. Furthermore, the
p-value (noted in Figure 6 below each UMAP visualizations) of BSN when trained using Ld + Lat
has the smallest value of 0.041. This validates our assertion that Lat learns the proper representations
of the IL classes thereby improving its ability to differentiate between IL and OL classes.
(a)	(b)
p-value = 0.56	p-value = 0.18
(c)
p-value = 0.041
Figure 6: (a):(c) Illustration of the effect of applying Ld and its combination with Lat on the
separation between the IL (red) and OL (purple) classes in the feature space.
Effect of margin δ : We mention in Section 3 that, in order to avoid a collapsed model early on
during training, we select the hard negative attention map such that its squared distance is closest
13
Under review as a conference paper at ICLR 2021
to the hard positive attention map. Without using any OL images during training or validation, we
empirically observe that L2 norm of the hard negative attention map with respect to the hard positive
attention map ranges between [0.33, 2.8]. Hence, we choose δ as 1.5 to ensure separability with
respect to the hard positive attention map.
To show that ENVISE is insensitive to the specific value of margin, we train the BSN using Lat
with value of δ as 0.5, 1.0, 1.5, 2.0 and 2.5. From Table 3, comparing column 3 with 4,5,6,7
and 8, we observe that ENVISE is insensitive to the specific value of margin and outperforms the
best performing baseline method for each super-class, even with different values of the margin.
Furthermore, from Fig. 7, we observe that ENVISE has the best performance when the margin is set
to 1.5
P = 0.14	P = 0.046	P = 0.021	P = 0.038	P = 0.041
Figure 7: Effect of different values of margin on the separation between the IL (red) and OL (purple)
classes in the feature space.
Table 3: Ablation study to illustrate the effect of proposed Lat loss and the effect of different values
of margin δ on the performance of ENVISE for outlier detection on CIFAR-100 dataset.
Metric	super-class	ENVISE Ld only	ENVISE Ld+Lat	Best Baseline	ENVISE δ=0.5	ENVISE δ=1.0	ENVISE δ=1.5	ENVISE δ=2.0	ENVISE δ=2.5
column ID		1	2	I 3	4	5	6	7	8
	C1	0.93	0.88	0.90	0.95	0.89	0.88	0.88	0.93
	C2	0.81	0.41	0.69	0.49	0.61	0.41	0.41	0.41
FPR	C3	0.9	0.57	0.69	0.58	0.66	0.57	0.65	0.83
(95% TPR)	C4	0.88	0.61	0.65	0.84	0.61	0.61	0.79	0.78
J	C5	0.87	0.76	0.82	0.78	0.81	0.76	0.83	0.77
	C6	0.64	0.49	0.63	0.54	0.53	0.49	0.67	0.57
	mean	0.84	0.62	I 0.73	0.70	0.69	0.62	0.69	0.72
	C1	0.39	0.40	0.42	0.46	0.33	0.40	0.38	0.41
	C2	0.33	0.16	0.28	0.17	0.21	0.16	0.16	0.16
Detection	C3	0.35	0.18	0.21	0.22	0.25	0.18	0.2	0.24
error	C4	0.35	0.26	0.27	0.26	0.26	0.3	0.26	0.3
J	C5	0.39	0.29	0.39	0.29	0.33	0.29	0.31	0.3
	C6	0.21	0.16	0.22	0.16	0.17	0.16	0.25	0.16
	mean	0.34	0.24	I 0.30	0.27	0.26	0.24	0.26	0.26
	C1	0.64	0.62	0.59	0.54	0.71	0.62	0.66	0.6
	C2	0.83	0.89	0.79	0.9	0.87	0.89	0.95	0.91
AuROC	C3	0.69	0.89	0.86	0.84	0.83	0.89	0.87	0.82
↑	C4	0.67	0.75	0.79	0.76	0.78	0.75	0.80	0.76
	C5	0.64	0.79	0.62	0.76	0.72	0.79	0.74	0.76
	C6	0.87	0.91	0.85	0.89	0.89	0.91	0.83	0.91
	mean	0.72	0.81	I 0.75	0.78	0.80	0.81	0.81	0.79
	C1	0.81	0.80	0.77	0.75	0.85	0.80	0.82	0.79
	C2	0.86	0.88	0.86	0.94	0.92	0.88	0.87	0.95
AuPR (inlier)	C3	0.73	0.91	0.87	0.87	0.84	0.91	0.89	0.85
↑	C4	0.76	0.82	0.85	0.85	0.88	0.82	0.82	0.84
	C5	0.88	0.95	0.90	0.94	0.91	0.95	0.93	0.94
	C6	0.81	0.87	0.79	0.85	0.84	0.87	0.76	0.86
	mean	0.81	0.87	I 0.84	0.87	0.87	0.87	0.85	0.87
	C1	0.41	0.39	0.37	0.32	0.45	0.39	0.43	0.35
	C2	0.65	0.92	0.71	0.82	0.76	0.92	0.88	0.84
AuPR (outlier)	C3	0.64	0.93	0.87	0.78	0.81	0.93	0.85	0.77
↑	C4	0.52	0.77	0.72	0.61	0.62	0.77	0.68	0.63
	C5	0.26	0.42	0.23	0.38	0.33	0.42	0.34	0.38
	C6	0.88	0.97	0.90	0.92	0.92	0.97	0.88	0.94
	mean	0.56	0.73	I 0.59	0.63	0.65	0.73	0.68	0.65
14
Under review as a conference paper at ICLR 2021
Table 4: Ablation study to illustrate the effect of employing the BSN over RvSN on the performance
of ENVISE for outlier detection on CIFAR-100 dataset.
Method	mean FPR⑷	mean Detection error (1)	mean AuROC (↑)	mean AuPR (IL) (↑)	mean AuPR (OL) (↑)
ENVISE (w/ RvSN)	0.65	0.30	0.73	0.73	0.63
ENVISE (w/ BSN)	0.62	0.24	0.81	0.87	0.73
Significance of using hard negative term in Lat : As mentioned in Section 3 of the main paper,
we observe that the attention maps from the correct and incorrect predictions are visually similar.
This causes the BSN to learn improper representations of the IL classes during online distillation.
Hence, we choose the attention map from the incorrect prediction (second most probable class) as a
hard negative to ensure separability with the attention map from the correct prediction. To illustrate
the significance of the hard negative attention map, we train ENVISE without the second term in
equation 5 along with Ld . The attention L2 loss is formulated as:
nk
(21)
From Figure 8, we observe that the confidence of classifying an image decreases when ENVISE is
trained using La . Furthermore, the attention map of ENVISE after training using Ld + La does not
focus on the semantically meaningful regions of the image (fourth column) unlike training it with
Lat (last column). Furthermore, we also observe training ENVISE with Ld + La results in a poor OL
detection with 0.98 FPR and 0.47 detection error respectively on C1 as compared to 0.88 and 0.40
FPR and detection error respectively with Ld + Lat. This validates that the hard negative attention
map ensures separability with the hard positive attention map and also improves the confidence of
classification and OL detection.
Attention map Attention map Attention map
Input	Attention map BSN from BSN after using BSN after using
Image	TN	highest
prediction	La	Lat
c = 0.93 c = 0.68 c = 0.41 c = 0.96
c = 0.77 c = 0.42 c = 0.83 c = 0.92
Figure 8: The attention map visualization representing the regions of the image that ENVISE focuses
on when trained using La and Lat. The confidence of classification is reported below each image.
Effect of using RvSN instead of BSN To illustrate the effectiveness of using the BSN to improve
the overall gain in efficiency of the system, we employ a RvSN instead of the BSN as our SN. We use
the TN as DenseNet-201 and real-valued VGG-16 as our SN and adaptively train it using Ld + Lat.
We observe from Table 4 that using BSN instead of RvSN as our SN improves the mean FPR by
4.8%, mean detection error by 25%, mean AuROC by 11%, mean AuPR ofIL class by 19.2% and
mean AuPR of OL class by 15.9%. Furthermore, we also observe that mean GiE for the BSN is 0.53
while that of the RvSN is 0.78 which illustrates that ENVISE has 47.2% gain in efficiency with the
BSN as compared to the RvSN.
15
Under review as a conference paper at ICLR 2021
Figure 9: Shannon Entropy of IL and OL of the BSN during online distillation. The entropy of the IL
decreases, the entropy of OL is high. This shows that BSN does not learn OL class information.
A.3 Discussions
The TN does not leak OL class information to the BSN during online distillation We mention
in Section 3 that the BSN is trained using only the hard pseudo-labels from the TN on the IL classes.
We show that during adaptive training, the TN does not transfer OL class information to the BSN.
Hence, the ability of the BSN to detect OL classes is due to its ability to accurately differentiate
between IL and OL class representations. To illustrate this, we compute the Shannon entropy H(.)
of OL and IL classes of the BSN during the online distillation process (when BSN is trained using
Ld + Lat). Figure 9 illustrates that while the H(IL|X) classes decreases, the H(OL|X) class is
already high, where X is the image from the input stream. This illustrates that during the online
distillation, the BSN learns the information of the IL classes, while information about the OL class is
completely absent. Thus, the ability of the BSN to detect OL classes better than the SOTA outlier
detection methods is not due to an OL information leak, but due to the BSN’s ability to accurately
differentiate between the representations of the IL and OL classes.
Gain with smaller teacher network: We investigate the importance of the architecture and size of
the TN network on the performance of the BSN for efficient OL detection. Here, we use ResNet-18
(11M parameters) for the originally used DenseNet-201 (18M parameters) as the TN, which is
a much smaller network as compared to the latter. We train ResNet-18 using the same training
procedure as discussed in Section 4. Once the BSN converges to the performance of the TN, we
evaluate the BSN on the different CIFAR-100 super-classes in Table 1 for OL detection. From Table
5, we observe that regardless of the TN used, ENVISE outperforms the best performing baseline
method for OL detection by achieving lower FPR and detection error with higher AuROC and AuPR.
Furthermore, we also observe that the BSN achieves similar gain in performance with ResNet-18 as
with DenseNet-201. From Figure 3 and Table 5, we show that ENVISE is agnostic to the specific TN
and BSN network architectures and achieves similar performance gains with different architectures.
Table 5: Performance of ENVISE in terms ofOL detection, with a smaller TN network (ResNet-18) as
compared larger TN (DenseNet-201) using the same BSN (Binary VGG-16) on different super-classes
of CIFAR-100 dataset. Notations R : ResNet-18 as TN, D : DenseNet-201 as TN.
Metric	Best Baseline (R)	ENVISE (R)	Best Baseline (D)	ENVISE (D)
mean FPR (95% TPR)]	0.83	0.69	0.76	0.62
mean Detection error ]	0.29	0.24	0.30	0.24
mean AuROC ↑	0.74	0.80	0.74	0.81
mean AuPR (IL) ↑	0.83	0.86	0.83	0.87
mean AuPR (OL) ↑	0.61	0.81	0.63	0.73
GiE when probability of IL class >> probability of OL class: We evaluate GiE of ENVISE for
the case when the IL class occurs with a much higher probability than the OL class. Hence for C5,
16
Under review as a conference paper at ICLR 2021
we consider all 2500 images from IL class and 300 images from OL class, making the probability of
occurrence of IL class P = 0.9 (2505+3。0 = 0.89 〜0.9). In such a setting, We obtain an FPR (f) of
0.26 and detection error (od) of 0.15. From the formulation of Flr mentioned in Section 4 of the
main paper, We use X = 9048 × 1e6 and Y = 6 × 1e6 to obtain GiE = 0.79. Thus, in the case When
the images from IL class occur more frequently than OL class (i.e p = 0.9), ENVISE achieves a gain
in efficiency of 36.2%.
Trade-off between accuracy and GiE in ENVISE: Table 6 shoWs the accuracy of the BSN, the TN
and their combined performance (i.e. ENVISE) for all six IL super-class pairs on the CIFAR-100
dataset. Comparing roW 2 and roW 3 in Table 6, We observe a very minimal loss in accuracy in
ENVISE With respect to the TN on different super-classes of the CIFAR-100 dataset. In ENVISE,
during inference, the BSN classifies the IL classes from the image stream. When the BSN misclassifies
an IL class as an OL class, the image is then given to the TN for classification. This ensures that the
overall accuracy of the system is bounded by the performance of the TN. Furthermore, from Figure 5,
We observe that ENVISE has the highest GiE Which indicates that, along With high computational
efficiency, ENVISE also has minimal loss in overall accuracy as compared to a standalone TN.
Table 6: Comparison of accuracy betWeen BSN, TN and ENVISE on the IL classes of CIFAR-100
dataset. The IL images that the BSN misclassifies as OD, are correctly reclassified by the TN.
Model	C1	C2			C3	C4	C5	C6		
BSN	0.72	0.80	0.86	0.83	0.79	0.83
TN	0.76	0.81	0.89	0.86	0.83	0.84
ENVISE	0.75	0.81	0.88	0.84	0.81	0.84
A.4 Additional quantitative and qualitative analysis
Complete comparison with baseline methods: We present a detailed comparison of ENVISE
With the SOTA OL methods on CIFAR-100 and TI datasets in Table 9. We observe that ENVISE
achieves loWer FPR and detection error as compared to the baseline methods. Specifically, ENVISE
outperforms the best performing baseline method (ODIN) by 23% and 25% in terms of mean FPR
and mean detection error respectively. Furthermore, ENVISE also achieves higher AuROC and AuPR
(outlier class) by outperforming ODIN by 9.5% and 15.9% in terms of mean AuROC and mean AuPR
(outlier class) respectively on CIFAR-100 dataset. Table 9 also shoWs that ENVISE outperforms the
baseline methods achieving loWer FPR and detection error and higher AuROC and AuPR (IL and
OL class) on TI dataset. Specifically, ENVISE achieve 9.3% and 30.4% loWer mean FPR and mean
detection error respectively as compared to the best performing baseline method (MCD). ENVISE
also outperforms MCD by 5.5%, 14.1% and 3.1% in terms of mean AuROC, mean AuPR (inlier
class) and mean AuPR (outlier class) respectively.
Performance of ENVISE is insensitive to specific IL \ OL class pairs: As mentioned in Table 1,
We evaluate ENVISE on meaningful IL \ OL class pairs that mimic real-life scenarios. HoWever, We
shoW that ENVISE is insensitive to the specific IL \ OL class pairs created in Table 1. From Table
10, Table 11 and Table 12, We shoW that ENVISE outperforms all baseline methods When each IL
class is compared to every other OL class by achieving loWer FPR and detection error and higher
AuROC and AuPR on the CIFAR-100 dataset.
Feature space separation: Figure 10 illustrates the comparison of ENVISE With all baselines in
terms of the separation betWeen IL and OL images in the feature space for all IL \ OL class pairs on
CIFAR-100 dataset. To quantify the feature separation betWeen the IL and OL samples, We compute
the p-value using Wilcoxon’s rank sum test Wilcoxon (1992) for the null hypothesis that the IL and
OL feature distribution are the same. Hence, loWer p-value indicates better separation. We observe
that ENVISE achieves better separation than baseline methods With the loWest p-value. Furthermore,
We also observe clusters of the IL images (in red) in the feature space Which illustrates that ENVISE
is capable of learning representations of IL images rather than memorizing labels from the TN.
Sub-classes within each super-class: Table 7 and Table 8 represents the sub-classes Within each
super-class of the CIFAR-100 and tiny-imagenet (TI) datasets. These sub-classes are the original
classes of these datasets respectively. Each super-class is denoted by Cxi and Cxo Where x =
[1, 2, 3, 4, 5, 6] representing the inlier and outlier super-class on CIFAR-100 dataset. Furthermore, the
17
Under review as a conference paper at ICLR 2021
Cl	ED	CE	CS	ODIN	OECC	MCD	CAL	ENVISE
	p = 0.68	p = 0.55	p = 0.42	p = 0.59	p = 0.20	p = 0.28	p = 0.39	p = 0.041
	...	z	z			∙∙ - ■ • ■, - - '..	.二	• '' .
C2	二 .	. ■..	::		■ ■	`	1，		
	.			■'		..		,上...
	p = 0.48	p = 0.52	p = 0.55	p = 0.36	p = 0.18	p = 0.053	p = 0.47	p = 0.021 ⅛	U
C3								士
	p = 0.43	p = 0.57	p = 0.55	p = 0.31	p = 0.38	p = 0.048	p = 0.53	p = 0.033
				：		”	S'		S *	'
C4					匚*'			-.W ,
					;二:			
	p = 0.16	p = 0.58	p = 0.46	p = 0.55	p = 0.028	p = 0.01	p = 0.42	p = 0.028
						• 二	-	i	-
C5				:	.∙.'∙√ •..			承,Z - L
					、.二；/			
	P = 0.51	p = 0.46	p = 0.40	p = 0.28	p = 0.21	p = 0.37	p = 0.58	* ，.. p = 0.016
				二				
C6								
P = 0.25 P = 0.62 P = 0.85
P = 0.58 p = 0.044 p = 0.16
p = 0.49
p = 0.043
Figure 10:	Comparison of ENVISE with baseline methods in terms of separation between IL (in
red) and OL (in purple) samples in the feature space on CIFAR-100 dataset. The p-value obtained
from Wilcoxon’s rank sum test is denoted below each visualization where lower value denotes better
separation.
super-classes of the tiny-imagenet dataset are denoted as Tyi and Tyo for inlier and outlier super-class,
where y = [1, 2, 3, 4, 5]. We use these notations for comparing ENVISE with the SOTA OL detection
methods in Table 9, Table 10, Table 11 and Table 12 respectively.
18
Under review as a conference paper at ICLR 2021
Table 7: Different sub-classes within each inlier and outlier super-classes on the CIFAR-100 dataset.
These sub-classes represent the original classes in the CIFAR-100 dataset.
Super-classes	Sub-classes
water animals (C1i)	beaver, dolphin, otter, seal, whale, aquarium fish, flatfish, ray shark, trout, crab, lobster
food containers (C1o)	bottles, bowls, caps, cups, plates
flora (C2i)	orchids, poppies, roses, sunflower tulips, maple, oak, palm, pine, willow
household electric devices (C2o)	clock, keyboard, lamp, telephone, television
fruit and vegetables (C3i)	apple, mushrooms, orange, pears, sweet peppers
household furniture (C3o)	bed, chair, couch, table, wardrobe
insects (C4i)	bee, beetle, butterfly, caterpillar, cockroach, snail, spider, worm
manmade things (C4o)	bridge, castle, house, road, skyscrapper
animals (C5i)	camel, cattle, chimpanzee, elephant, kangaroo, fox, porcupine, possum racoon,skunk, hamster, mouse, rabbit, shrew, squirel, crocodile dinosaur, lizard, turtle, snake, bear, leopard, tiger, lion, wolf
people (C5o)	baby, boy, girl, man, women
outdoor scene (C6i)	cloud, forest, mountain, plain, sea
vehicles (C6o)	bicycle, bus, motorcycle, pickup truck train, lawn-mover rocket, streetcar tank, tractor
Table 8: Different sub-classes within each inlier and outlier super-classes on the tiny-imagenet
dataset. These sub-classes represent the original classes in the tiny-imagenet dataset.
Super-classes	Sub-classes
animals (T1i)	Labrador retriever, hog, Chihuahua, orangutan, chimpanzee, koala, lesser panda golden retriever, lion, baboon, African elephant, Yorkshire terrier, bison, standard poodle, cougar, gazelle, ox, Egyptian cat Persian cat, German shepherd, guinea pig, bighorn
birds (T1o)	black stork, goose, king penguin, albatross, crane
reptiles / insects (T2i)	European fire salamander, bee, scorpion, fly, black widow, ladybug, slug, cockroach, grasshopper, centipede, mantis, sulphur butterfly dragonfly, snail, boa constrictor, spider web, trilobite, tarantula
aquatic animals (T2o)	brain coral, American lobster, American alligator, tailed frog, spiny lobster sea slug, dugong, coral reef, bullfrog, sea cucumber, jellyfish, goldfish
edible items (T3i)	orange, pizza, mushroom, banana, ice cream, pomegranate, pretzel, mashed potato potpie, cauliflower, meat loaf, bell pepper, guacamole, lemon, ice lolly, confectionery
garments (T3o)	swimming trunks, apron, poncho, academic gown, military uniform, neck brace vestment, kimono, sombrero, fur coat, cardigan, bikini, miniskirt, bow tie sunglasses, sandal, sock, Christmas stocking
Household items (T4i)	picket fence, candle, chain, rocking chair, torch, iPod frying pan, dumbbell, water jug, teddy, plate, walking stick, computer keyboard bucket, comic book, sewing machine, remote control, teapot, barn, volleyball lawn mower, hourglass, desk, lampshade, bathtub, wok, CD player, rugby ball stopwatch, magnetic compass, space heater, plunger, backpack, wooden spoon broom, dining table, basketball, punching bag, refrigerator, umbrella
places / manmade things (T4o)	seashore, water tower, triumphal arch, cliff dwelling, suspension bridge dam, steel arch bridge, cliff, monarch, obelisk, lakeside, fountain, altar, alp
Vehicles (T5i)	school bus, lifeboat, jinrikisha, beach wagon, tractor, moving van, trolleybus police van, go kart, freight car, bullet train, sports car, gondola, limousine, convertible
Miscellaneous (T5o)	beaker, parking meter, drumstick, reel, potter’s wheel, beacon, acorn, gasmask, projectile cannon, cash machine, maypole, pay phone, flagpole, bannister, thatch, pill, bottle, pop bottle barbershop, birdhouse, binoculars, organ, abacus, nail turnstile, beer bottle, oboe viaduct, scoreboard, barrel, pole, syringe, chest, butcher shop, espresso, snorkel, brass
19

inlier ∖ outlier ED CE CS ODIN OECC MCD CAL ENVISE
class
τli∖τlo 0 T2i ∖	τ2o	0 τ3i ∖	τ3o	0 τ4i ∖	τ4o	0 τ5i ∖	τ5o	0	72 87 78 94 88	0 0 0 1 0	99 98 89 ,0 88	0 1 0 1 0	98 ,0 82 ,0 87	0 0 0 0 0	98 87 82 83 82	0 0 0 0 0	81 84 82 87 90	0 0 0 0 0	87 80 81 84 80	0 0 0 0 0	93 87 85 92 83	0.69 0.78 0.74 0.78 0.78
-	0	84	0	95	0	93	0	86	0	85	0	82	0	88	0.75
Tli∖τlo 0 T2i ∖ τ2o	0 τ3i ∖ τ3o	0 τ4i ∖ τ4o	0 τ5i ∖ τ5o	0	39	0	48	0	44	0	43	0	28	0	38	0	39	0.23
	40 45 29 34	0 0 0 0	43 39 31 37	0 0 0 0	42 41 33 34	0 0 0 0	31 32 38 34	0 0 0 0	29 37 51 36	0 0 0 0	31 32 20 30	0 0 0 0	34 43 32 36	0.26 0.20 0.18 0.26
-	0	37	0	40	0	39	0	36	0	36	0	30	0	37	0.23
τli∖τlo 0 T2i ∖	τ2o	0 τ3i ∖	τ3o	0 T4i ∖	T40	0 τ5i ∖	τ5o	0	72 75 65 69 90	0 0 0 0 0	56 55 51 54 55	0 0 0 0 0	55 55 50 53 54	0 0 0 0 0	58 65 58 64 69	0 0 0 0 0	55 66 64 64 61	0 0 0 0 0	84 71 68 69 73	0 0 0 0 0	79 72 65 69 90	0.89 0.81 0.81 0.75 0.91
-	0	74	0	54	0	53	0	63	0	62	0	73	0	75	0.77
6 3 15
8 9 9 8
6 8 6 7 5
8 7 7 7∙
8 6 6 8 4
8 6 7 7 8
τlτ2τ3τ4τ5
τlτ2τ3τ4τ5
9
8
0.80
78
0
8
6
72
0
9
5
9
5
乃
0
13 3 4 4
7 7 6 4 8
S
5 7 9 18
6 6 5 5 7
5 8 118
6 6 6 5 7-
6559444357-
8 4 7 5
4-4 4
7 19 8
4 5 2 2
8 2 9 9
4 5 2 2
8 0 7 9
3 5 4 2
0.67
0.64
5
6
0.54
9
7 -
0.74-
3
7 -
9
7 -
0.57
46
0
46
0-
49
0
Metric	inlier ∖ outlier class	ED	CE	CS	ODIN	OECC	MCD	CAL	ENVISE
	OIa ∖ Cio	0.93	0.90	0.90	0.90	0.90	0.95	0.95	0.88
	025 ∖ C2o	0.91	0.89	0.72	0.72	0.86	0.85	0.69	0.41
FPR	Qa ∖ C3o	0.83	0.70	0.70	0.72	0.90	0.89	0.69	0.57
(95% TPR)	∖	0.91	0.65	0.66	0.66	0.97	0.94	0.87	0.61
Ψ	05∙i ∖ 05o	0.94	0.92	0.93	0.92	0.90	0.95	0.82	0.76
	06∙i ∖ 06o	0.83	0.69	0.68	0.66	0.89	0.95	0.63	0.49
	mean	0.89	0.79	0.76	0.76	0.90	0.92	0.77	0.62
	OIa ∖ °i。	0.42	0.46	0.46	0.45	0.43	0.45	0.45	0.39
	025 ∖ C2o	0.33	0.28	0.29	0.28	0.39	0.35	0.41	0.16
Detection	Qa ∖ C3o	0.32	0.22	0.21	0.22	0.50	0.50	0.39	0.18
error	∖	0.42	0.27	0.28	0.27	0.45	0.42	0.42	0.26
Ψ	05∙i ∖ 05o	0.41	0.39	0.39	0.39	0.46	0.44	0.44	0.29
	06∙i ∖ 06o	0.29	0.22	0.22	0.22	0.43	0.40	0.42	0.16
	mean	0.37	0.31	0.31	0.30	0.44	0.43	0.42	0.24
	OIa ∖ Cio	0.56	0.54	0.54	0.54	0.59	0.54	0.55	0.62
	025 ∖ C2o	0.71	0.78	0.77	0.79	0.62	0.68	0.59	0.89
AuROC	Qa ∖ C3o	0.73	0.85	0.86	0.85	0.45	0.43	0.60	0.89
↑	C<4∙i ∖	0.59	0.78	0.79	0.79	0.53	0.57	0.59	0.75
	05∙i ∖ 05o	0.59	0.62	0.62	0.62	0.50	0.56	0.54	0.79
	06∙i ∖ 06o	0.78	0.85	0.84	0.85	0.57	0.60	0.60	0.91
	mean	0.66	0.74	0.74	0.74	0.54	0.56	0.58	0.81
	OIa ∖ C10	0.73	0.74	0.74	0.74	0.77	0.74	0.75	0.80
	025 ∖ C20	0.83	0.86	0.85	0.86	0.73	0.79	0.71	0.88
AuPR (inlier)	Qa ∖ C3o	0.75	0.85	0.87	0.85	0.47	0.45	0.63	0.91
↑	C4w ∖	0.71	0.84	0.85	0.84	0.66	0.65	0.67	0.82
	05∙i ∖ 05o	0.87	0.90	0.90	0.90	0.85	0.87	0.85	0.95
	06∙i ∖ 06o	0.66	0.79	0.79	0.79	0.39	0.43	0.44	0.87
	mean	0.76	0.83	0.83	0.83	0.65	0.66	0.67	0.87
	OIa ∖ C10	0.35	0.35	0.35	0.36	0.37	0.31	0.32	0.39
	025 ∖ C20	0.55	0.70	0.70	0.71	0.48	0.52	0.44	0.92
AuPR (outlier)	Qa ∖ C3o	0.73	0.86	0.87	0.86	0.46	0.45	0.56	0.93
↑	C*4∙i ∖ C40	0.46	0.72	0.70	0.72	0.40	0.46	0.49	0.77
	05∙i ∖ 05o	0.23	0.21	0.21	0.21	0.16	0.19	0.19	0.42
	06∙i ∖ 06o	0.86	0.90	0.89	0.90	0.70	0.74	0.74	0.97
	mean	0.53	0.62	0.62	0.63	0.43	0.44	0.46	0.73
Jolsq SIonMA JoaoJ°βsoa∙2PUI - PUB
J-sq∙2ən-EA J-IEulS SQa∙-p∙s→ ωql ∙z-qEI∙spə1uəsəjd SE Sləs-ep P PUE ooxv工 lɔ Uo səss-ɔ
Uo PUEI iuəjəjp uo SPOqJəui OU=OSEq ətp IPIMSI>NJo UOSIiEduloɔ əɔuEUnoJJωd 3 9qEI
IZOZ xOIJOdEd əɔuəjəjuoɔ E SE maəj Jωpu∩
Metric inlier ∖ outlier ED CE CS ODIN OECC MCD CAL ENVISE
class
8 8 8 6 2 9
8 6 6 7 4 7
S
Oooooo
1 2 3 4 5 6
。。。。。。
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
Illlll
。。。。。。
FPR
(95% TPR)
mean 0.94	0.94	0.92	0.81	0.91	0.91	0.83	0.70
.J-sq Sl ə-BAJəleəj00S-B∙-
AjəAə q∙-Λ∖ mo PUE I°SJRd
9 12 2 8 5
3 3 3 3 13
S
5 8 6 5 9 8
4 3 3 3 2 3
5 0 8 7 4 7
4 4 4 3 3 3
3 0 4 5 7 1
4 4 4 4 2 4
5 5 8 16 5
4 3 3 4 3 3
6 9 0 2 0 0
4 4 5 4 5 4
6 9 0 0 0 3
4 4 5 5 5 4
2 0 9 6 9 9
4 5 4 4 4 4
1 2 3 4 5 6
Illlll
Detection
error
37
9
3
2 6 4 9
6 7 6 7
5 17 0
5 7 6 7
4 4 8 3
5 6∙6
9 2 6 5
5-5 5
4 4 3 1
5---
4 9 9 5
5 4 3 4
4 7 10
5 4 3 5
6 6 12
5 4 4 5
9 3
8 7
3 9
8 6
9 9
7 6
7 8
-5
-71-63
4 7
6 6
3 7
2-6
7 9
4 4
«•
70
0
0.67
0.61
6
6
3
5
45
0-
48
0
0 4 3 1
8 8 7 7
5 5 8 4
7 7 6 7
74
0
77
0
74
0
74
0
74
0
73
0
9469

«-'
9 4 8 5 8 2
3 6 6 6 7 7
S
64
0'0'0∙
3
8
3
6
73
0
2 8 8 8 5 8
3 5 5 5 7 6
59
0 7 5
7 5 6
2
8
0.61
9
6
13 2 0 3 5
3 3 4 5 7 5
48
7 13
--∙
-81-71-78
0.60
6 O
--
9 3 8 8 9
6 6 6 5 6
8 0 8 8 6
6 6 6 5 6
7 4 4 8 4
--∙-5
Oooooo Oooooo
1 2 3 4 5 6 1 2 3 4 5 6
。。。。。。an。。。。。。
∖∖∖∖∖∖∖∖∖∖∖∖
.2 .2 ,2 .2 .2 .2 m ,2 .2 .2 .2 .2 .2
Illlll Illlll
。。。。。。 。。。。。。
AuROC
↑
AuPR (inlier)
↑
74
0
73
0
0.67
6
6
6
6
an
7 0 5 3 2 3
3 4 3 4 5 5
6 3 4 1
3---
5 9 2 9
3 2 4 3
5 8 16
3 2 2 2
5 8 7 1
3 2 2 3
43
0
3 6
--
1 2
4 5
1 7
2 5
9 6
2 4
0.60
9
3
-31
0
2
3
Oooooo
1 2 3 4 5 6
WWW
.2 .2 ,2 .2 .2 .2 m
Illlll
。。。。。。
AuPR (outlier)
p∙s- PUE J-sq∙2ən-EA J-IEUlS s-E∙-p∙s→ 9ql°o'xv工 lɔ Uo ss-ɔ Uo
SSEIojI uo SpOqlOUI QU=QSEq ωql q∙Λ∖ωSI>NωJo UO-JBdUIOOOIqEI
1707 xu□I IE JodEdoouoJ°JU0。E SE Λ∖-AoJ Jopu∩
inlier ∖ outlier ED CE CS ODIN OECC MCD CAL ENVISE
class
9 10 8 5 9
6 4 7 6 4 6
S
5
8
9
6
81
6 5 9
8 8 8
8 6 8
7 7 8
6 6 7
8 8 8
9 6 6 7 3 3
7 8 8 8 7 8
7 2 9 3 6 3
8 7 8 7 9 8
9 2 8
8 7 8
3 8 6
9 8 8
2 9 9
8 8 8
2 9 6
9 8 8
2 13 5 2 2
9 9 9 9 9 9
Oooooo
1 2 3 4 5 6
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
2 2 2 2 2 2
60
3
8
2
8
3
8
6
8
5
8
2
9
16 6 4 15
3 13 3 2 3
S
1x 1x oð oð
4 4 3 3 2 3
8 5 8 3 10
3 3 3 4 3 4
3 9 0 9 4 6
3 3 4 3 2 3
4
19 5 5 5 6
4 2 4 4 4 4
18 5 5 3 7
4 2 4 4 4 4
13 4 7 9 3
4 3 4 4 3 4
Oooooo
1 2 3 4 5 6
Cccccc
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
2 2 2 2 2 2
				O	42	O	42	O	43	O	35	O	35	O	38	O	37	0.	23
	P P P P P P	5 。2 。4 。5	ɔ	O O O O O O	61 71 54 54 61 58	O O O O O O	53 78 60 53 64 51	O O O O O O	57 77 59 60 64 50	O O O O O O	68 79 61 59 77 62	O O O O O O	72 62 62 64 83 68	O O O O O O	69 68 60 57 77 64	O O O O O O	68 59 71 61 76 63	0. 0. 0. O 0. 0.	74 89 74 62 85 70
ZZ				O	58	O	59	O	61	O	68	O	69	O	66	O	68	0.	71
	P P P P P P	5 。2 。4 。5	ɔ	O O O O O O	64 83 61 62 75 56	O O O O O O	63 86 51 57 54 66	O O O O O O	63 85 60 57 59 66	O O O O O O	61 86 71 63 77 61	O O O O O O	61 73 71 61 80 62	O O O O O O	63 79 67 66 74 63	O O O O O O	69 71 69 58 75 63	O 0. 0. 0. 0. 0.	77 88 73 67 89 68
3
9
6
9
6
9
6
9
6
2
6
6
6
2 2 5 9 0 1
6 9 6 6 8 7
S
3 4 3 8 0 8
5 4 5 5∙6
5 2 13 19
5 5 5 5 7 6
9 8 8 9 0 0
5 4 4 4-'
O - _ - _ - _ j^D
-----∙
10 9 9 7 4
4 7 2 4 4 5
10 6 0 8 4
4-2 5 4 5
4 5 18 3 9
4 5 4 3 4 5
Oooooo
1 2 3 4 5 6
Cccccc
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
2 2 2 2 2 2
60
5
6
47
0
47
0-
45
0
inlier ∖ outlier ED
class
ENVISE
CAL
ODIN OECC MCD
CS
CE
Metric
7 7 7 3 9 5
4 5 5 7 2 6
8 9 9
6 7 6
8 2 9
7 8 8
4 0 0
7 8 9
0.55
22
«•
O
8 2 4
13 1
3 7 9 4
8 7 8 8
3
9
∙75
84
«•
8 17 18
7 9 7 9 6
S
7 8 9
7 4 6
0 0 7
9 7 8
2 8 6
9 6 8
Oooooo
1 2 3 4 5 6
。。。。。。
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
3 3 3 3 3 3
。。。。。。
FPR
(95% TPR)
0.72
8
2
19 9 9 7
3 3 2 13
0.80	0.81	0.83
0.34
6
2
0-
0.86
0.87
mean 0.92
.J-sq Sl ə-BAJəleəj00S-B∙-
AjəAə q∙-Λ∖ ɑ PUE m°SJRd
48
0
47
44
0
3 0 8 9 6
3 5 3 2 3
3 0 10 5
3 5 4 2 3
40
0
2 6 2 3
2 3 3 3
弘
2
3
5
3
0.79 0.72 0.45 0.60	0. 0. 0. 0.	,81 ,73 ,43 ,64	0. 0. 0. 0.	,79 ,69 ,60 ,75
0.86	0.	,87	0.	,86
0.68	0.	,67	0.	,69
0.72	0.	,74	0.	,73
0.70	0.	,69	0.	,66
0.69 0.47 0.57 0.87 0.49	0. 0. 0. 0. 0.	,70 ,45 ,70 ,89 ,67	0. 0. 0. 0. 0.	,73 ,63 ,71 ,88 ,66
43
44
0
0.21
2
2
2 2
4 3
8 9 0
4 4 4
8 9 1
4 4 4
8 0 8
3 5 3
1 2 3 4 5 6
3 3 3 3 3 3
Detection
error
0.57 0.59 0.73 0.59	0. 0. 0. 0.	,60 ,46 ,85 ,41	0. 0. 0. 0.	,59 ,47 ,86 ,42	0. 0. 0. 0.	,74 ,74 ,85 ,79
0.49 0.64	0. 0.	,39 ,59	0. 0.	,38 ,59	0. 0.	,72 ,73
0.54	0.	,54	0.	,54	0.	,76
0.55 0.57 0.75 0.57 0.50 0.45	0. 0. 0. 0. 0. 0.	,42 ,41 ,85 ,43 ,42 ,37	0. 0. 0. 0. 0. 0.	,40 ,41 ,87 ,43 ,41 ,37	0. 0. 0. 0. 0. 0.	,73 ,75 ,85 ,76 ,69 ,64
Oooooo Oooooo
1 2 3 4 5 6 1 2 3 4 5 6
。。。。。。an。。。。。。
∖∖∖∖∖∖∖∖∖∖∖∖
.2 .2 ,2 .2 .2 .2 m ,2 .2 .2 .2 .2 .2
3 3 3 3 3 3 3 3 3 3 3 3
。。。。。。 。。。。。。
AuROC
↑
AuPR (inlier)
↑
80
S
5
5
an
4 6 3 4 2 2 3
8 7 9 7 9 8 8
S
9 8 6 9 6 2
6 6 5 6 8 7
66
7 0 5 9 2 7 3
7 7 4 6 8 7 7
7 9 6 6
-'4 5
3
8
0-
4 3 6 2
---∙
2 7
--
乃
8 3 7 8
4 4 8 4
4 7
4 7
S S
9 3 6 9 5 7
4 4'4 4-7
4 7 3 6 0 5
5 5'5 5-
57
0.57
0.60
Oooooo
1 2 3 4 5 6
WWW
.2 .2 ,2 .2 .2 .2 m
3 3 3 3 3 3
。。。。。。
AuPR (outlier)
p∙s- PUE J-sq∙2ən-EA J-IEUlS s-E∙-p∙s→ 9ql°o'xv工 lɔ Uo ss-ɔ Uo
ss-ɔjI uo SPoqləuɪ OU=QSEq ωql q∙Λ∖ωSI>NωJo Uo-JBdUIoɔ =1-qEI
1707 xu□I IE JodEdoouoJ°JU0。E SE Λ∖-AoJ Jopu∩
inlier ∖ outlier ED CE CS ODIN OECC MCD CAL ENVISE
class
045 ∖ Cio 045 ∖ C2o 4a ∖ c3o C4W ∖ 。45 ∖ 。45 ∖ Cθo	0 0 0 0 0 0	94 93 95 91 93 92	0 0 0 0 0	,0 96 90 65 89 87	1 0 0 0 0 0	,0 96 90 66 90 87	0 0 0 0 0 0	71 73 80 66 80 74	0 0 0 0 0 0	86 89 91 97 79 92	0 0 0 0 0 0	85 87 92 94 82 90	0 0 0 0 0 0	77 67 69 87 62 70	0.51 0.59 0.62 0.61 0.33 0.61
-	0	94	0	93	0	93	0	78	0	88	0	88	0	73	0.57
4a \ 5。 045 ∖ C2o 4a ∖ C3o 。45 \。4。 。45 \。5。 04∙i ∖ 06o	0	42 45 49 42 47 42	C	,5	0	,5	0	41	0	32	0	31	0	29	0.26
	0 0 0 0 0		C 0 0 0 0	,5 46 27 47 41	0 0 0 0 0	,5 48 28 47 41	0 0 0 0 0	44 37 27 41 40	0 0 0 0 0	36 39 45 22 37	0 0 0 0 0	34 38 42 21 34	0 0 0 0 0	39 41 42 27 33	0.30 0.31 0.26 0.16 0.31
-	0	44	0	44	0	45	0	39	0	34	0	33	0	35	0.27
5 ∖ Clo C4i ∖ C2o C4i ∖ C3o 04∙i ∖ 04o 。45 \。5。 。45 \。6。	0 0 0 0	58 55 50 71 52 59	0 0 0 0	29 27 39 84	0 0 0 0	26 38 40 85	0 0 0 0	70 60 56 84	0 0 0 0	72 65 61 66	0 0 0 0	74 70 65 65	0 0 0 0	73 73 66 67	0.75 0.78 0.69 0.82
	0 0		0 0	41 53	0 0	40 53	0 0	62 78	0 0	82 66	0 0	84 69	0 0	88 75	0.90 0.78
-	0	57	0	44	0	45	0	67	0	68	0	70	0	73	0.78
4a \ 5。 045 ∖ C2o 4a ∖ C3o 。45 \。4。 。45 \。5。 04∙i ∖ 06o	0 0 0 0 0 0	68 65 62 71 62 66	0 0 0 0 0 0	43 44 45 84 53 79	0 0 0 0 0 0	44 45 44 85 53 79	0 0 0 0 0 0	71 64 62 84 74 79	0 0 0 0 0 0	69 63 69 66 88 39	0 0 0 0 0 0	69 67 63 65 81 43	0 0 0 0 0 0	68 67 66 67 83 44	0.73 0.68 0.69 0.82 0.92 0.87
-	0	64	0	52	0	52	0	71	0	59	0	69	0	72	0.78
5 ∖ Clo C4i ∖ C2o C4i ∖ C3o 04∙i ∖ 04o 。45 \。5。 。45 \。6。	0 0 0 0	45 42 39 46	0 0 0 0	24 25 25 72	0 0 0 0	26 24 25 70	0 0 0 0	67 60 56 72	0 0 0 0	56 50 47 40	0 0 0 0	59 66 59 46	0 0 0 0	66 67 62 49	0.71 0.67 0.68 0.77
	0 0	41 61	0 0	38 44	0 0	37 44	0 0	69 73	0 0	66 66	0 0	69 69	0 0	77 71	0.86 0.78
-	0	47	0	37	0	37	0	65	0	57	0	62	0	65	0.72

Metric inlier ∖ outlier ED CE CS ODIN OECC MCD CAL ENVISE
class
2 2 9 2 6 9
6 7 6 9 7 5
S
3 12 12 8
7 8 7 8 8 8
2 7 14 5 3
9 8 9 9 9 9
9 4 0 4 0 2
8 8 9 9 9 9
Oooooo
7 6 4 8 2 4
---∙9∙
88。-94。-92。
lll
5 4 3 5 4 4
9 9 9 9 9 9
Oooooo
1 2 3 4 5 6
。。。。。。
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
5 5 5 5 5 5
。。。。。。
FPR
(95% TPR)
mean 0.94	0.96	0.96	0.79	0.90	0.92	0.82	0.72
.J-sq Sl ə-BAJəleəj00S-B∙-
AjəAə q∙-Λ∖ 9°PUE°SJRd
0 3 6 3 9 6
3 3 3 4 2 2
S
3 119 4 4
3 4 4 3 4 3
4 4 7 4 4 8
3 3 3 4 4 3
4 5 8 3 6 8
3 3 3 4 4 3
2 9 8 9 9 0
4 3 3 3 3 4
4 0 4 0 9 0
4 5 4 5 3 5
3 0 3 0 9 0
4 5 4 5 3 5
2 5 0 3 10
4 4 4 4 4 4
Oooooo
1 2 3 4 5 6
Cccccc
∖∖∖∖∖∖
•2 ∙2 ∙2 ∙2 ∙2 ∙2
5 5 5 5 5 5
Detection
error
9
3
9
3
9
3
40
0
47
0
47
0-
43
0
an
9 6 19 9 6
6 7 7 6 7 7
S
8 2 2 3 4 9
5 7 6 6 5 6
8 9 4 6 6 4
6 6 6 5 5 6
3 7 0 3
6 5 5 6
∙73
0.64
3
6
3
6
4 9
--
4341
8 7
-5
2
6
刀
6
6
6 5
3 3
2
6
47
0
44
0
2 0 6 6
4 4 3 3
9 6 18
5 5-5
2
6
47
0-
9 9
5 5
1 2 3 4 5 6
5 5 5 5 5 5
AuROC
↑
43
0-
6
5
an
8 7 5 0 5 5
8 8 8 8 9 8

S0-0'0-0-
0.87
3 2 6 9 2 0
6 6 5 4 4 5
0∙0-0∙



4
5
79
0
70
0
70
0
乃
0
0'
0-
0.87
6 0 15 0
6 8 7 8 8
77
0
2 8 9
6 6 4
49
«•
9
0
44
0
48
0
7079乃8777
9
6
9
6
0'
76
0
6 0 5
5 6 4
49
«•
9
0
48
0
0.37
8 5 5 9
6 7 8 6
8 2 0 3
--9∙
7 9 0 9
7 6 9 5
0 6 9 0 9
-7-7'69 5
6 8 7 7 8
8 8 8 8 7
1 2 3 4 5 6
5 5 5 5 5 5
AuPR (inlier)
↑
73
0
74
0
73
0
72
0-
5
8
an
6 0 3
5 4 4
0.31
6
0
47
0
40
0
0.50
0.21
0.20
9
0
7 7 3
∙3 4
0.21
0-
45
0
0 3 0
2 2 3
9 2 0
J 2 3
9 2 9
12 1
0.21
9
2
0.24
-21
0
9
2
3 4
2 3
1 2 3 4 5 6
5 5 5 5 5 5
AuPR (outlier)
0.24
2
2
an
p∙s- PUE J-sq∙2ən-EA J-IEUlS s-E∙-p∙s→ 9ql°o'xv工 lɔ Uo ss-ɔ Uo
SSEIOjI uo SpOqlOUI QU=QSEq ωql q∙Λ∖ωSI>NωJo UO-JBdUIOO iZI OIqEI
1707 xu□I IE JodEdoouoJ°JU0。E SE Λ∖-AoJ Jopu∩
inlier ∖ outlier class	ED	CE	CS	ODIN	OECC	MCD	CAL	ENVISE
4a ∖ 5。	0.93	0.72	0.73	0.73	0.82	0.79	0.71	0.63
065 ∖ C2o	0.93	0.79	0.79	0.72	0.85	0.80	0.66	0.55
06i ∖ C3o	0.95	0.81	0.82	0.81	0.87	0.86	0.64	0.58
06∙i ∖	0.95	0.87	0.87	0.77	0.87	0.86	0.69	0.51
06∙i ∖ 05o	0.95	0.73	0.74	0.74	0.76	0.73	0.53	0.37
06∙i ∖ 06o	0.83	0.69	0.68	0.66	0.89	0.95	0.63	0.49
-	0.92	0.76	0.76	0.75	0.84	0.83	0.69	0.52
C6W ∖ 0lo	0.43	0.36	0.36	0.41	0.37	0.39	0.32	0.32
065 ∖ C2o	0.44	0.42	0.42	0.40	0.30	0.32	0.30	0.29
06i ∖ C3o	0.46	0.41	0.40	0.37	0.36	0.36	0.32	0.30
06∙i ∖	0.49	0.45	0.44	0.39	0.30	0.32	0.28	0.26
06∙i ∖ 05o	0.43	0.37	0.37	0.39	0.19	0.21	0.28	0.18
06∙i ∖ 06o	0.29	0.22	0.22	0.22	0.43	0.40	0.42	0.16
	0.43	0.37	0.37	0.37	0.31	0.35	0.32	0.25
4a ∖ 5。	0.58	0.65	0.65	0.65	0.77	0.76	0.79	0.83
065 ∖ C20	0.56	0.53	0.54	0.65	0.64	0.63	0.66	0.71
06i ∖ C3o	0.55	0.60	0.60	0.67	0.68	0.67	0.70	0.75
06∙i ∖ 04o	0.50	0.53	0.53	0.62	0.75	0.72	0.69	0.76
06∙i ∖ 05o	0.59	0.66	0.66	0.66	0.86	0.86	0.88	0.88
06∙i ∖ 06o	0.78	0.85	0.84	0.85	0.57	0.60	0.60	0.91
-	0.58	0.64	0.64	0.68	0.73	0.67	0.73	0.81
4a ∖ 5。	0.58	0.57	0.57	0.65	0.76	0.73	0.71	0.77
065 ∖ C2o	0.56	0.49	0.50	0.66	0.72	0.70	0.73	0.79
06i ∖ C3o	0.55	0.54	0.55	0.67	0.64	0.62	0.68	0.74
。6石 ∖ 04o	0.49	0.50	0.50	0.61	0.65	0.60	0.66	0.66
06∙i ∖ 05o	0.58	0.60	0.60	0.66	0.79	0.82	0.79	0.84
06∙i ∖ 06o	0.56	0.78	0.79	0.78	0.58	0.26	0.46	0.87
	0.55	0.58	0.59	0.67	0.69	0.57	0.56	0.77
06匕 \ 5。	0.55	0.70	0.71	0.64	0.72	0.73	0.69	0.74
065 ∖ C2o	0.54	0.62	0.62	0.64	0.71	0.71	0.69	0.78
06i ∖ C3o	0.52	0.62	0.64	0.66	0.66	0.65	0.71	0.75
06∙i ∖ 04o	0.49	0.57	0.57	0.66	0.70	0.69	0.73	0.81
06∙i ∖ 05o	0.53	0.69	0.69	0.65	0.81	0.81	0.80	0.90
。6石 ∖ Cqo	0.86	0.90	0.89	0.90	0.70	0.74	0.74	0.97
-	0.57	0.68	0.69	0.69	0.73	0.70	0.73	0.82
Under review as a conference paper at ICLR 2021
Input
Image
Attention map Attention map Attention map
Attention map BSN from BSN from BSN after using
TN	highest second highest
prediction prediction	Lat
Input
Image
Attention map Attention map Attention map
Attention map BSN from BSN from BSN after using
TN	highest second highest
prediction prediction	Lat
c = 0.93
c = 0.77
c = 0.68
c = 0.21
c = 0.96
c = 0.42
c = 0.33
c = 0.92
c = 0.63
c = 0.48
c = 0.29
c = 0.91
c = 0.51
c = 0.71
c = 0.31
c = 0.83
C = 0.83
C = 0.55
c = 0.28
c = 0.89
c = 0.97
c = 0.81
c = 0.49
c = 0.28
C = 0.99
c = 0.68
c = 0.74
c = 0.21
c = 0.69
c = 0.52
c = 0.39

c = 0.81
c = 0.61
c = 0.23
c = 0.85
c = 0.88
c = 0.44
c = 0.39
c = 0.96
c = 0.89
Figure 11:	The attention map visualization representing the regions of the image that the network
focuses on for classification. The confidence of correct classification is reported below each image
Attention map Attention map Attention map	Attention map Attention map Attention map
Input	Attention map	BSN from BSN from BSN after using	Input	Attention map	BSN from	BSN from BSN after using
Image	TN	highest second highest	Image	TN	highest	second highest
prediction prediction Lat	prediction prediction Lat
■雀 LTIEEierLG
c = 0.59
c = 0.51
c = 0.99
c = 0.93
c = 0.26
c = 0.81
c = 0.18
c = 0.93
c = 0.54
c = 0.81
c = 0.82
c = 0.13
c = 0.88
c = 0.43
c = 0.41
c = 0.89
IgTSN ∙κe 日
c = 0.91
c = 0.68
c = 0.26
c = 0.97
c = 0.98
c = 0.53
c = 0.19
c = 0.99
c = 0.95
c = 0.66
c = 0.22
c = 0.99
c = 0.92
c = 0.51
c = 0.38
c = 0.97
电盘修畦窿雪您N盟零
c = 0.80
c = 0.39
c = 0.28
c = 0.92
c = 0.88
c = 0.43
c = 0.31
c = 0.91
配金？皂3的
c = 0.76
c = 0.46
c = 0.27
c = 0.83
c = 0.83
c = 0.47
c = 0.23
c = 0.86
Figure 12:	The attention map visualization representing the regions of the image that the network
focuses on for classification. The confidence of correct classification is reported below each image
24