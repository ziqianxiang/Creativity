Under review as a conference paper at ICLR 2021
Learning Safe Policies with Cost-sensitive
Advantage Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement Learning (RL) with safety guarantee is critical for agents perform-
ing tasks in risky environments. Recent safe RL algorithms, developed based on
Constrained Markov Decision Process (CMDP), mostly take the safety require-
ment as additional constraints when learning to maximize the return. However,
they usually make unnecessary compromises in return for safety and only learn
sub-optimal policies, due to the inability of differentiating safe and unsafe state-
actions with high rewards. To address this, we propose Cost-sensitive Advantage
Estimation (CSAE), which is simple to deploy for policy optimization and ef-
fective for guiding the agents to avoid unsafe state-actions by penalizing their
advantage value properly. Moreover, for stronger safety guarantees, we develop a
Worst-case Constrained Markov Decision Process (WCMDP) method to augment
CMDP by constraining the worst-case safety cost instead of the average one. With
CSAE and WCMDP, we develop new safe RL algorithms with theoretical justi-
fications on their benefits for safety and performance of the obtained policies.
Extensive experiments clearly demonstrate the superiority of our algorithms in
learning safer and better agents under multiple settings.
1	Introduction
In recent years, Reinforcement Learning (RL) has achieved remarkable success in learning skillful
AI agents in various applications ranging from robot locomotion (Schulman et al., 2015a; Duan
et al., 2016; Schulman et al., 2015c), video games (Mnih et al., 2015) and the game of Go (Silver
et al., 2016; 2017). These agents are either trained in simulation or in risk-free environments, and
the deployed RL algorithms can focus on maximizing the cumulative return by exploring the envi-
ronment arbitrarily. However, this is barely workable for real-world RL problems where the safety
of the agent is important. For example, a navigating robot cannot take the action of crashing into a
front obstacle even if the potential return on reaching the target faster is higher. Actually, in reality,
some states or actions might be unsafe and harmful to the system, and the agent should learn to avoid
them in deployment when performing certain tasks. Conventional RL algorithms do not particularly
consider such safety-constrained environments, which limits their practical application.
Recently, Safe Reinforcement Learning (Garcia & Fernandez, 2015; Mihatsch & NeUneier, 2002;
Altman, 1999) has been proposed and drawn increasing attention. Existing safe RL algorithms
generally fall into two categories based on whether or not the agents are reqUired to always stay safe
dUring learning and exploration. The algorithms with exploration safety (Dalal et al., 2018; Pecka
& Svoboda, 2014) insist that safety constraints never be violated even dUring learning, and thUs
they UsUally reqUire certain prior knowledge of the environment to be available, e.g., in the form of
hUman demonstrations. Comparatively, deployment safety (Achiam et al., 2017; Chow et al., 2018)
RL algorithms train the agents from interaction with the environment and allow safety constraints
violation dUring learning to some extent. This is reasonable since whether a state is safe will not be
clear Until the agent visits that state. Since hUman demonstrations are too difficUlt or expensive to
collect in some cases and may not cover the whole state space, we focUs on deployment safety in
this work.
RL problems with deployment safety are typically formUlated as Constrained Markov Decision Pro-
cess (CMDP) (Altman, 1999) that extends MDP by reqUiring the agent to satisfy cUmUlative cost
constraints in expectation in the meanwhile of maximizing the expected retUrn. Leveraging the
1
Under review as a conference paper at ICLR 2021
success of recent deep learning powered policy optimization methods (Schulman et al., 2015b),
Constrained Policy Optimization (CPO) (Achiam et al., 2017) makes the first attempt on high-
dimensional control tasks in continuous CMDPs. However, CPO only considers the total cost of
a trajectory of a sequence of state-action pairs during policy optimization. It does not differenti-
ate the safe state-action pairs from the unsafe ones in the trajectories. Due to such incapability of
exploiting the intrinsic structure of environments and trajectories, CPO sacrifices too much on the
expected return for learning the safety policy.
In this work, we propose Cost-sensitive Advantage Estimation (CSAE) which generalizes the con-
ventional advantage estimation for safe RL problems by differentiating safe and unsafe states, based
on the cost information returned by the environment during training. CSAE depresses the advan-
tage value of unsafe state-action pairs but controls effects upon their adjacent safe state-actions in
the trajectories. Thus, the learned policy can maximally gain rewards from the safe states. Based
on CSAE, we develop a new safe RL algorithm with proved monotonic policy performance im-
provement in terms of both safety and return from safe states, showing superiority over other safe
RL algorithms. Moreover, to further enhance the agent’s ability of enforcing safety constraints, we
propose Worst-case Constrained Markov Decision Process (WCMDP), an extension of CMDP by
constraining the cumulative cost in worst cases through the Conditional Value-at-Risk (Tamar et al.,
2015), instead of that in expectation. This augmentation makes the learned policy not only safer but
also better, both experimentally and theoretically.
With CSAE and WCMDP, we develop a new safe RL algorithm by relating them to trust region
methods. We conduct extensive experiments to evaluate our algorithm on several constrained robot
locomotion tasks based on Mujoco (Todorov et al., 2012), and compare it with well-established
baselines. The results demonstrate that the agent trained by our algorithm can collect a higher
reward, while satisfying the safety constraints with less cost.
2	Related Work
Safe Reinforcement Learning has drawn growing attention. There are various definitions of ‘safety’
in RL (Garcia & Fernandez, 2015; Pecka & Svoboda, 2014), e.g., the variance of return (Heger,
1994; Gaskett, 2003), fatal transitions (Hans et al., 2008) and unknown states (Garcia et al., 2013).
In this paper, we focus on the RL problems with trajectory-based safety cost, under the constrained
MDP (CMDP) framework. Through Lagrangian method, Geibel & Wysotzki (2005) propose to
convert CMDP into an unconstrained problem to maximize the expected return with a cost penalty.
Though such a problem can be easily solved with well-designed RL algorithms, e.g. (Schulman et al.,
2015b; 2017), the trade-off between return and cost is manually balanced with a fixed Lagrange
multiplier, which cannot guarantee safety through learning. To address this, inspired by trust region
methods (Schulman et al., 2015b), Constrained Policy Optimization (Achiam et al., 2017) (CPO)
establishes linear approximation to the safety constraint and solves the corresponding optimization
problem in the dual form. Compared with previous CMDP algorithms, CPO scales well to high-
dimensional continuous state-action spaces. However, CPO does not distinguish the safe states from
the unsafe ones in the training process, limiting its performance in the return.
Besides developing various optimization algorithms, some recent works also explore other ap-
proaches to enhance the safety constraints, e.g., adopting the Conditional Value-at-Risk (CVaR)
of the cumulative cost as the safety constraint (Tamar et al., 2015). Along this direction, Tamar
et al. (2015) develop a gradient estimator through sampling to optimize CVaR with gradient de-
scent. Prashanth (2014) further applies this estimator to CVaR-Constrained MDP to solve the
stochastic shortest path (SSP) problem.
Our work considers a similar framework to CPO (Achiam et al., 2017), but it treats states differently
by extending Generalized Advantage Estimation (Schulman et al., 2015c) tobe safety-sensitive. Our
proposed CSAE can boost the policy performance in terms of the return while ensuring the safety
property. Moreover, our algorithm with WCMDP is safer than CPO in terms of constraint violation
ratio during learning.
There are also some non-CMDP based algorithms for safe RL that are not in the scope of this work.
In (Dalal et al., 2018), a linear safety-signal model is built to estimate per-step cost from state-action
pairs and rectify the action into a safe one. However, this method requires a pre-collected dataset
2
Under review as a conference paper at ICLR 2021
to fit the linear cost estimation model, which limits its application. Similarly, Cheng et al. (2019)
augment the model-free controller to enforce safety per step by designing a modle-based controller
with control barrier functions (CBFs). Some works introduce Lyapunov functions to build safe RL
algorithms. For example, Berkenkamp et al. (2017) apply Lyapunov functions for safely recovering
from exploratory actions, while Chow et al. (2018) construct Lyapunov functions that explicitly
model constraints.
3	Preliminaries
A standard Markov Decision Process (MDP) (Sutton et al., 1998) is defined with a tuple
(S, A, P, R, γ, μ), where S and A denote the set of states and actions respectively, P : S×A×S →
[0, 1] is the transition dynamics modeling the probability of transferring from state s to s0 after taking
action a, R(s, a, s0) represents the reward function during this transition, γ ∈ [0, 1] is the discount
factor and μ : S → [0,1] denotes the starting state distribution.
An MDP agent is usually equipped with a policy π(a∣s), which denotes the probability distribu-
tion over actions a given a state s. The performance of a policy π is measured with the expected
discounted total reward J (π) = ET 〜∏,s0 〜μ[P∞=o γt R(st,at,st+ι)], where τ = (s0 ,a0 ,sι,...)
is a trajectory generated by following policy π. RL algorithms for MDPs try to find the pol-
icy π* that achieves the highest reward, i.e., π* = argmax∏ J(π). They commonly use
the value function V∏(s) = ET〜∏[P∞=0YtR(St,at,st+1)∣s0 = s], the action value function
Qn(s, a) = ET〜∏[P∞=o γtR(st,at,st+1)∣s0 = s,a0 = a] and the advantage function An(s, a)=
Qπ (s, a) - Vπ (s). The discounted future state distribution will also be useful, which is defined as
dn(s) = (1-γ) Pt=o YtP(St = s∣π).
Constrained Markov Decision Process (CMDP) (Altman, 1999) extends MDP to environments with
safety cost that could harm the agent when undesired actions are taken. As various safety costs
may exist in a single CMDP, we relate them with m cost functions {C1(s, a, s0), . . . , Cm(s, a, s0)},
each of which denotes the cost an agent receives for each transition (s, a, s0) (similar to reward
functions). Let Ci(τ) = Pt∞=0 YtCi(st, at, st+1) denote the cumulative cost along a trajectory τ
generated from policy π. We consider a trajectory-based cost constraint in CMDP, which limits the
cumulative cost in expectation JCi = ET〜∏,s0〜μ[Ci(τ)] with value d” Then safe RL aims to learn
the policy π under CMDP by solving the following problem,
∏* = arg max	J (∏),	s.t.	JCi	=	ET ^∏,so^μ[Ci(τ)]	≤ di ,i	=	1,...,m.	(1)
Safe RL algorithms search for the policy ∏* that achieves the maximal cumulative reward and mean-
while does not violate the imposed safety constraints on the costs. In the following, analogous to
the definition of value functions (i.e., Vn , Qn and An), we use VnCi , QnCi and AnCi to denote the
cost-value functions w.r.t. cost function Ci .
4 Method
In this section, we develop a policy gradient based algorithm for solving the safe Reinforcement
Learning problem in Equation 1. We will first derive a novel cost-sensitive advantage estimation
method and present theoretical guarantees on the performance of its learned policy in terms of
rewards from safe states. Then, we further develop a worst-case constrained MDP to augment the
safety guarantee for learning policies. Finally, we present our safe RL algorithm in details.
4.1	Cost-sensitive Advantage Estimation
Conventional policy optimization methods (either for RL or for Safe RL) usually model the policy
with a parametric function approximator (e.g., neural networks), and directly optimize the expected
return J(πθ), where πθ denotes the policy parameterized with θ. The gradient estimator g for policy
gradient methods (Schulman et al., 2015b;c) generally takes the following form:
∞
g = E J^Φ(st, at)Vθ∏θ(at∣st) ,	(2)
t=0
3
Under review as a conference paper at ICLR 2021
where Φ(st, at) is responsible for guiding the policy updating direction and one popular choice for
Φ(st, at) is Generalized Advantage Estimator (GAE) (Schulman et al., 2015c) which substantially
reduces the variance of policy gradient estimate. The formulation for GAE1 is given by
∞
AGAE(M) := X(γλ)lδt+1,	⑶
l=0
where λ ∈ [0, 1] is a hyper-parameter. When λ = 0, it reduces to one-step TD error estimator; when
λ = 1, it reduces to the empirical return estimator.
Cost-sensitive Advantage Estimation Existing safe RL algorithms directly deploy these estima-
tors without adaptation to the specific feature of safe RL problems and fail to consider the safety
requirement within the gradient estimation. For example, CPO (Achiam et al., 2017) uses environ-
ment reward to estimate the advantage function for policy optimization, without considering that
some high-reward states may also be unsafe.
In safe RL, an unsafe state with high reward would bias policy update towards favoring such a state
and wrongly encourage the agent to violate cost constraints, if directly applying the GAE estimator.
A natural solution is to penalize the reward for unsafe states. However, it is difficult to adjust the
penalty appropriately. Specifically, over-penalization would suppress visiting the nearby safe states
with high reward as their Φ(st, at) will be negatively affected during bootstraping. On the other
hand, the unsafe state cannot be avoided when the penalty is too small.
Since δt can be considered as an estimate of the advantage value of taking action at at step t, the
policy gradient estimator g points to the direction of increasing π(ajst) only if the advantage of at
is greater than zero. Therefore, to guarantee that agents can gain rewards mainly from safe states,
we propose to generalize GAE for safe RL by zeroing the TD error δ of unsafe states to avoid the
agents from further exploring these regions. This is given by
∞
ACSAE(M) ：= X(γλ)lɑt+1 δt+ι,	(4)
l=0
where αt is a binary variable denoting whether a transition (st, at, st+1) is safe (αt = 1) or not
(αt = 0). Following standard assumption in safe RL (Achiam et al., 2017), given the returned
cost from the environment in the training phase, αt can be obtained by binarizing the cost value
C(st, at, st+1), i.e., αt = 1[C(st, at, st+1) > 0]. With this new advantage estimation, the policy
gradient estimator for safe RL is given by
∞
gCSAE = E XACSAE(Y,λPθ∏θ(at∣st),
t=0
which is compatible with any policy gradient based methods.
CSAE and Reward Reshaping The above CSAE is equivalent to a moderate reward reshaping to
penalize the reward for unsafe states. More specifically, it replaces the reward value for an unsafe
state with the expected one-step reward an agent can receive at this state:
ŋ/	、— R R(st, at,st+1),	if αt = 1,
R(st,at,st+I) = (Ea-[R(StMs0)] ,if αt=0.	⑸
Using this reshaped reward function induces the above CSAE advantage estimator. To see this, we
use rt and 尸 t to substitute R(st, at, st+ι) and R(st, at, st+ι ),respectively, in the following and drop
subscript π from the value function for notation simplicity2 . Following standard definition, at time
step t, a k-step advantage estimation A(tk) using the value function V and our revised reward signal
r can be expressed as
Atk) = -V (st) + rt + γrt+ι +----+ YkT 尸 t+k-1 + YkV (st+k).	(6)
1We use AGAE(Y,λ) to denote AGAE(Y,λ)(st,at).
2Note that the reward revision mechanism in Equation 5 is only used for advantage estimation. For fitting
the value function during learning, we still use the original reward function R(s, a, s0).
4
Under review as a conference paper at ICLR 2021
By substituting one-step TD error δt and reward function (Equation 5) into Equation 6, the above
advantage can be rewritten as
k-1
At(k) = X γlαt+lδt+l .	(7)
l=0
See the appendix for the complete proof. Analogous to GAE, CSAE can be obtained by
taking the exponentially-weighted average of above k-step advantage: ACSAE(γ,λ) ：= (1 -
λ) Pk∞=1 λk-1At(k) = Pl∞=0(γλ)lαt+lδt+l. This provides another perspective, from reward reshap-
ing, to interpret the proposed CSAE. As policy optimization methods will automatically force agents
to find high-reward regions in the state space, using the averaged reward can prevent unsafe yet high-
reward states from attracting the agent during learning.
From the reward reshaping perspective, another possible approach to deal with the cost is to include
the cost ct in the reward by reshaping rt to Rt = rt +λ × ct. But it is difficult to properly choose the
trade-off parameter λ due to: 1) if λ has fixed value, it is not easy to balance rt and ct as their best
trade-off varies across environments, as verified by Tessler et al. (2018). In contrast, our proposed
method is free of hyperparameter tuning and easy to deploy. 2) if λ is treated as the dual variable
for safety hard constraints and updated in a similar way as PDO, the performance is worse than our
method, due to the optimization difficulties, as justified in our experiments.
Worst-Case Constraints As discussed in Sec. 3, in a CMDP, the trajectory-based safety
cost for cost function Ci is computed and constrained in expectation, i.e., JCi (π) =
ET〜∏[P∞=o YtCi(st, at, st+ι)] ≤ di. However, this will certainly lead the agent to violate the
constraints frequently during learning. To further enhance safety, we instead consider the worst
cases and constrain the cost from the trajectories incurring largest cost.
We propose the Worst-case Constrained MDP (WCMDP), an MDP with a constraint on the CVaR of
cost values (Tamar et al., 2015; Prashanth, 2014) in safe RL. It tries to find a policy that maximizes
the cumulative return, while ensuring the conditional expectation of other cost functions given some
confidence level β, to be bounded. Formally, for a cost function Ci and a given β ∈ (0, 1), the worst
case constraint is given by
∞
JCi (π) = Eτ~∆∏,β )1γtCi(St, at, st+1),	⑻
where ∆π,β is the set of top β worst trajectories with the largest costs. We found the performance is
robust to the value of β and we empirically set β = 0.1. Accordingly, the safety constraint related
to cost function Ci is expressed as JCβ (π) ≤ di.
4.2 Safe RL Algorithm with CSAE
Different from general RL problems, for safe RL, it is critical to ensure that the agent mostly gains
reward from safe states and transitions. Thus, we are concerned with the following cost-sensitive
return developed from the reshaped rewards in Equation 5 in safe RL:
∞
Jsafe (π ) := ET 〜π,so 〜μ Y ER(St,at, st+ι)
t=0
(9)
where R(St, at, st+ι) = αtR(st, at, s0) + (1 - αt)Ea,st+ι [R(st, a, s0)]. Different from the con-
ventional return that accumulates the rewards from both safe and unsafe states, the above reshaped
return characterizes how much the agent can gain reward from safe state-actions. In this section,
we demonstrate adopting the proposed CSAE in policy optimization would naturally optimize Jsafe.
To this end, we establish the following theoretical result that gives performance guarantees for the
policies in terms of the cost-sensitive return Jsafe(π).
Theorem 1. For any policies π0, π with eπ' = max§ | Ea 〜∏o [ACSAE(Y'(s, a)]| ,thefolloWing bound
holds:
0
JsafeW- Jsafe(n) ≥ 1 E/ ACSAE(Y' (s, a)-{ DTV ①团同
(10)
5
Under review as a conference paper at ICLR 2021
Here DTV denotes the total variance divergence, which is defined as DTV (p∖∖q) = 2 Pi ∣Pi - qi | for
discrete probability distributions p and q. Due to space limit, we defer all the proofs to the appendix.
The above result bounds the difference of two policies in terms of the cost-sensitive return via the
CSAE. Leveraging such a result, our safe RL algorithm updates the policy by
∏k+ι = arg max Es 〜d∏k,a 〜∏ [ACSAE(Y,λ*s,a)] — Vk DTV (∏∖∖∏k )[s]
β π	k	(11)
s.t. JCi = ET〜∆π,β [Ci(τ)] ≤ di, i = 1,...,m.
In particular, from Equation 10, for appropriate coefficients νk, the above update ensures mono-
tonically non-decreasing return from safe states. Details of the practical implementation of this
algorithm are provided in the appendix.
5	Experiments
As this work targets at obtaining safer and better policies, through experiments we aim to investigate:
1) whether our designed CSAE is effective for guiding the policy optimization algorithm to achieve
higher cumulative reward while satisfying safety constraints; 2) whether the new policy search al-
gorithm induced from WCMDP can guarantee stronger safety without sacrificing the performance;
and 3) whether our method is able to adjust the advantage value of each transition properly to better
guide policy optimization. Therefore, we evaluate our methods on multiple high-dimensional con-
trol problems that mainly include two different tasks. 1) Circle (Schulman et al., 2015b) where the
agent is required to walk in a circle to achieve the highest cumulative reward, but the safe region
is restricted to lie in the middle of two vertical lines. 2) Gather where several apples are randomly
placed in both safe and unsafe regions, and an agent should collect as many apples as possible from
the safe regions and avoid entering the unsafe regions. In our experiments, the reward for collecting
one apple is 10, and the cost is 1 for each time the agent walks into an unsafe region. See Fig. 3 for an
example of the gather environment. For the circle environment, we use three different robot agents
in Mujoco (Todorov et al., 2012), i.e., point mass, ant and humanoid. For the gather environment,
we conduct experiments with point mass and ant.
We use CSAE (Sec. 4.2) to denote the safe policy search algorithm equipped with our proposed
cost-sensitive advantage estimation, and CSAE-WC to denote the algorithm that further includes
worst-case constraints. We compare these two methods with three well-established baselines.
TRPO (Schulman et al., 2015b): the most widely used policy optimization method; CPO (Achiam
et al., 2017): the state-of-the-art safe RL algorithm for large-scale CMDP; PDO: a primal-dual
optimization based safe RL algorithm (Achiam et al., 2017). For all the experiments, we use a
multi-layer perceptron with two hidden layers of (64, 32) units as the policy network. Our imple-
mentation is based on rllab (Duan et al., 2016) and the Github repository3. The hyper-parameters
for the environments and algorithms are given in the supplementary material.
Results The learning curves for all the methods and environments are plotted and compared in
Fig. 1. The first row is the cumulative reward. As we are dealing with environments with safety cost,
we only accumulate the rewards collected through safe transitions as an optimal safe RL algorithm
should be able to acquire rewards from safe states and avoid high-reward unsafe states. We also
visualize the full returns in Fig. 1 (second row) for completeness. From the results, one can observe
that our CSAE surpasses CPO throughout all the environments. This demonstrates the effectiveness
of CSAE for learning safe agents with higher rewards. Furthermore, with the help of worst-case
constraints, CSAE-WC performs the best in terms of rewards from safe states for PointCircle and
PointGather or comparably well for AntCircle, HumanCircle and AntGather, outperforming CPO.
The second and third rows in Fig. 1 plot the cumulative cost and ratio of the safe trajectories4 in all
the trajectories at each sampling. Specifically, a safe ratio of 1 means all the collected trajectories
are safe. From the results, the cost value of TRPO agents explodes as the training proceeds, while
all the other three methods converges. Among them, CSAE achieves comparable cost value as CPO
and higher safe ratio. CSAE-WC surpasses the other methods—it not only satisfies the constraint
with less cost but also achieves highest safe ratio (nearly 1). These results clearly show that our
method is effective at both enforcing safety and collecting more rewards, or it is safer and better.
3https://github.com/jachiam/cpo/
4One trajectory is counted as safe if its cumulative cost is smaller or equal to the constraint value d.
6
Under review as a conference paper at ICLR 2021
AntCircIe
500	l∞0
O	l∞0	2∞0	30∞
O	59	IOO UO
500	l∞0
10s
101
IO1
10"
io-ɪ
10*1
PO
POO
PO
AE
CSAE-WC
Figure 1:	Learning curve comparison between our methods (CSAE and CSAE-WC) and the state-of-
the-arts (TRPO, PDO, CPO)for five safe RL problems. First row: safe cumulative reward. Second
row: total cumulative reward. Third row: cumulative cost. Fourth row: ratio of safe trajectories. X
axes denote the training iteration. (Best viewed in color). Each curve is obtained by averaging over
five random runs. The standard deviation of different runs is visualized with the shade.
TRPO
Figure 2:	Agents trained in PointCircle. The grey circle denotes the path with highest reward. The
two red dotted lines are the boundaries and the agent is constrained to run between them. Lines with
different colors starting from the center are agent trajectories learned with different random seeds.
Visualization To intuitively justify our method indeed learns agents that take safer and better ac-
tions, we visualize agent trajectories for the circle task (Fig. 2) and the gather task (Fig. 3). Fig. 2
shows TRPO agent follows the circle specified by the reward function without considering con-
straints. The other safe RL agents can learn to obey the constraints to some extent. However, they
do not perform well as they usually get stuck in a corner (e.g., for PDO and CPO). Our CSAE-WC
agents, however, can walk along the arcs and safe boundaries. Similar observations can be made
in AntGather, where TRPO agent inevitably violates the constraint and rushes into unsafe regions
(i.e., the red squares). The other agents learn to avoid such cost but sacrifice the rewards. However,
CSAE and CSAE-WC can work better to collect more rewards than others. In summary, both visu-
alizations in Fig. 2 and Fig. 3 demonstrate the effectiveness of our method for learning better agents
that generate more reasonable and safer trajectories.
Analysis We here investigate how our proposed CSAE helps the training process and the resulted
agents. We use PointCircle as the environment to conduct the following analysis. First, we justify
7
Under review as a conference paper at ICLR 2021
Figure 3: Agents trained in AntGather. The green circles denote the randomly placed apples to
collect and red-colored squares are the unsafe regions. The blue lines are trajectories of an agent
trying to explore the environment to collect apples.
Figure 4: (a) Comparison of average return on PointCircle for different reward modifications.
“Mean” is our method in Eqn. equation 5 and “Zero” reshapes reward into rt = at X rt. (b)
Advantage value visualization. Each colored dot represents a transition in the trajectories, whose
intensity denotes relative value of the corresponding advantage. (Best viewed in color).
the method of replacing the reward with the expected one-step reward (Equation 5) for unsafe states.
We compare it with a simple reward reshaping method that zeros the reward of unsafe transitions
and plot their learning curves (of average return) in Fig. 4a. The results show that our method
(denoted by “Mean” in Fig. 4a) performs much better. This indicates that our method can overcome
the shortcomings of penalizing the reward of unsafe transitions not properly.
Second, it is important for safe RL algorithms to help the agent distinguish high-reward but unsafe
states from the safe ones. To investigate the differences of safe RL algorithms (PDO, CPO and our
CSAE-WC) in this ability, we sample 300 trajectories (100 from each method). For different algo-
rithms, we use their deployed reward and value functions to estimate the advantage value for each
transition in these trajectories. The advantage values are visualized in Fig. 4b, where more reddish
means higher relative advantage value and bluish means lower value. From such visualization, one
can observe that these three methods can recognize high-reward and safe state-actions by assign-
ing higher advantage values, as shown in the left-bottom and right-top in Fig. 4b. However, our
algorithm CSAE-WC prefers these safe and high-reward regions more with higher advantage val-
ues. Importantly, as shown in the right-bottom (unsafe but high-reward regions), our method gives
state-actions within such regions much lower advantage. In contrast, PDO and CPO even assign
above-the-average advantages to them. This result clearly demonstrates the superior and desired
ability of our method to distinguish unsafe states from the safe ones for policy learning.
6	Conclusion
In this paper we consider Safe Reinforcement Learning and propose a novel CSAE method to appro-
priately estimate the advantage value for policy optimization under risky environments. Compared
to conventional advantage estimation, CSAE eliminates the negative effect of high-reward but un-
safe state-actions by depressing their advantages. To further enforce safety constraints, we augment
the CMDP with the worst-case cost constraint and proposed WCMDP. We theoretically analyze their
performance and safety benefits. We then develop a new safe RL algorithm which is shown effective
for learning safer and better agents in multiple large-scale continuous control environments.
8
Under review as a conference paper at ICLR 2021
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 22-31.
JMLR. org, 2017.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in neural information processing
systems, pp. 908-918, 2017.
Richard Cheng, Gabor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforce-
ment learning through barrier functions for safety-critical continuous control tasks. arXiv preprint
arXiv:1903.08792, 2019.
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps. In Ad-
vances in neural information processing systems, pp. 3509-3517, 2014.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-
based approach to safe reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 8092-8101, 2018.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In Maria Florina Balcan and Kilian Q. Weinberger
(eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of
Proceedings of Machine Learning Research, pp. 1329-1338, New York, New York, USA, 20-22
Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/duan16.html.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
Javier Garcia, Daniel Acera, and Fernando Fernandez. Safe reinforcement learning through proba-
bilistic policy reuse. RLDM 2013, pp. 14, 2013.
Chris Gaskett. Reinforcement learning under circumstances beyond its control. 2003.
Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under
constraints. Journal of Artificial Intelligence Research, 24:81-108, 2005.
Alexander Hans, Daniel SChneegaβ, Anton Maximilian Schafer, and Steffen Udluft. Safe explo-
ration for reinforcement learning. In ESANN, pp. 143-148, 2008.
Matthias Heger. Consideration of risk in reinforcement learning. In Machine Learning Proceedings
1994, pp. 105-111. Elsevier, 1994.
Oliver Mihatsch and Ralph Neuneier. Risk-sensitive reinforcement learning. Machine learning, 49
(2-3):267-290, 2002.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.
Martin Pecka and Tomas Svoboda. Safe exploration techniques for reinforcement learning-an
overview. In International Workshop on Modelling and Simulation for Autonomous Systems,
pp. 357-375. Springer, 2014.
LA Prashanth. Policy gradients for cvar-constrained mdps. In International Conference on Algo-
rithmic Learning Theory, pp. 155-169. Springer, 2014.
9
Under review as a conference paper at ICLR 2021
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1889-1897, Lille, France, 07-09 JUl 2015a. PMLR. URL http://PrOceedings.mlr.press/
v37/schulman15.html.
John SchUlman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. TrUst region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015b.
John SchUlman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continUoUs control Using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015c.
John SchUlman, Filip Wolski, PrafUlla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja HUang, Chris J. Maddison, ArthUr GUez, LaUrent Sifre, George van den Driess-
che, JUlian Schrittwieser, Ioannis AntonogloU, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya SUtskever, Timothy P. Lilli-
crap, Madeleine Leach, Koray KavUkcUoglU, Thore Graepel, and Demis Hassabis. Mastering
the game of go with deep neUral networks and tree search. Nature, 529(7587):484-489, 2016.
doi: 10.1038/natUre16961. URL https://dOi.Org/10.1038/nature16961.
David Silver, JUlian Schrittwieser, Karen Simonyan, Ioannis AntonogloU, Aja HUang, ArthUr GUez,
Thomas HUbert, LUcas Baker, Matthew Lai, Adrian Bolton, YUtian Chen, Timothy Lillicrap, Fan
HUi, LaUrent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of go withoUt hUman knowledge. Nature, 550:354-, October 2017. URL http://dx.
dOi.Org/10.1038/nature24270.
Richard S SUtton, Andrew G Barto, et al. Introduction to reinforcement learning, volUme 135. MIT
press Cambridge, 1998.
Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. In Twenty-Ninth
AAAI Conference on Artificial Intelligence, 2015.
Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. arXiv
preprint arXiv:1805.11074, 2018.
EmanUel Todorov, Tom Erez, and YUval Tassa. MUjoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
10
Under review as a conference paper at ICLR 2021
7	Appendix
7.1	Policy optimization with worst-case constraints
Since solving the exactly optimal policy is intractable for large-scale problems, policy gradient based
methods represent the policy with a θ-parameterized model, and try to search for the best policy
within the parameter space ∏θ, i.e., π* = arg max∏∈∏θ J(∏θ). Similarly, in the optimization Prob-
lem induced from the worst case constrained policy optimization, we additionally require the policy
to satisfy a set of safety constraints ΠβC . In other words, we are optimizing the policy to achieve
highest cumulative reward over the intersection of Πθ and ΠβC, which is formulated as
max	J(π), s.t. JCβ (π) ≤ di,	i = 1, . . . , m.
Compared to CMDP objective in Equation 1, our proposed method requires the worst β-quantile
trajectories (instead of the average cost) to still satisfy the safety constraints. This will yield a safer
policy as proved later. Before presenting our algorithm in full details, the following result is given
that is useful for connecting the worst case safety cost of two different policies and their difference.
Theorem 2. Let Pβ denote the state transition probability P of β-worst case trajectories. For any
policies π and π0, define 史Ii = max§ |Ea 〜∏,,s'〜93[Ci (s, a, s0)+ YVCi (s0) 一 VCi (s)]|. Let 吗 denote
the discounted future state distribution for the β-worst trajectories. Then the following bound holds:
0
1	2γπ
E------Es〜dπ	Ani (S, α) + -----JDTV(π IIn)[s]
1 一 γ β π	1 一 γ
a〜π	L
(12)
The above gives an upper bound on the worst case cost for policy π0 . Explicitly constraining such
an upper bound during the policy learning would enforce less cost constraint violation. Compared
with the risk-sensitive CVaR models (Chow & Ghavamzadeh, 2014), this work is among the first to
introduce such worst-case constraints into safe RL problems. Besides, it is also the first to present
theoretical analysis on the expected worst-case cost of two policies, which is of independent interest.
We now show how to develop a practical algorithm for safe RL based on WCMDP and CSAE.
Inspired by trust region methods (Schulman et al., 2015b), by replacing JCi with JCβ and applying
Theorem 2, we reformulate the update in Equation 11 into
∏k+ι = arg max Es 〜d∏k ,。〜∏ [A∏SAE(S,a)]
s.t. JCi (πk ) + lɪ-Es〜dξπk [A∏k (S,a)] ≤ di,	(13)
1 一 Y	a 〜π
DKL(πIIπk) ≤ δ, i = 1, . . . , m,
which is guaranteed to produce policies with monotonically non-decreasing returns from safe state-
actions. Meanwhile, the policies will satisfy the original safety cost constraints.
7.2 Algorithm details
To efficiently solve Equation 13, we linearize the objective and cost constraint around πk and expand
the trust region constraint to second order, similar to (Schulman et al., 2015b; Achiam et al., 2017).
We use θk to denote parameters of policy πk . Denote the gradient of objective and constraint for
JCβ as g and bi , respectively, and the Hessian of the KL-divergence as H . The approximation
to Equation 13 is given by
θk+1 = arg max g>(θ 一 θk)
θ
s.t. bi> (θ 一 θk) + JCβi (πk) 一 di ≤ 0, i = 1, . . . , m	(14)
2(θ 一 θk )丁 H (θ 一 θk) ≤ δ∙
11
Under review as a conference paper at ICLR 2021
Algorithm 1 Worst-case Constrained Policy Optimization
Input: Initial policy π0 ∈ Πθ , tolerance α and confidence level β
for i = 0, 1, 2, dots do
Sample trajectories Di = {τ}, T 〜∏θi.
Form sample estimates g, b, H, C With Di
if If the primal problem in Equation 14 is feasible then
Solve dual problem in Equation 15 to get λ*, ν*
Compute updated policy parameters θ* with Equation 16.
else
Compute recovery policy parameters θ* with
θ*=θk- r ^H-IbH T b.
end if
Obtain θk+1 by backtracking line search to enforce satisfaction of sample estimates of con-
straints in Equation 13.
end for
As H is always positive semi-definite, the above optimization problem can be efficiently solved in
its dual form when the gradient g and bi are appropriately estimated. Here, g can be easily obtained
by taking derivative of the objective after replacing GAE with our proposed CSAE. For estimating
the gradient bi of the CVaR constraint JCβ , we adopt the likelihood estimate proposed by Tamar
et al. (2015):	i
bi = VθJ= ET^∆∏,β h(JCi(so) - VaRe(JCi(s0))) Vθ logπθ(a|s)i .
Here VaRβ (JCβ (s0)) is empirically estimated from the batch of sampled trajectories used for each
update. Then we use the same algorithm as CPO (Achiam et al., 2017) to learn the policy.
We now derive the full algorithm to solve the optimization problem in Equation 14, which can
also be found in CPO Achiam et al. (2017). Let Ci denotes JCβ (πk) - di, B =. [b1, . . . , bm] and
C =. [C1, . . . , Cm]T, we can express the dual to Equation 14 as follows:
max —ɪ (gτHTB _ 2rτV + VtSV) + VtC — —,	(15)
λ≥o,νho 2λ 'g	+	7 +	2 ,	' '
where r = gτ H-1B, S = Bτ H-1B. Solving this problem is much easier than the primal problem
especially when the number of constraints is low. Let λ*, V* denote a solution to the dual problem,
the solution to the primal is given by
θ* = θk + ] H-1(g — Bv *).	(16)
λ*
We now ready to present the full algorithm in Algorithm 1.
7.3	Experimental Parameters
For circle tasks, the cost function is given by
C(s, a, s0) = 1[|x| > xlim],	(17)
where x is the horizontal position of the agent after this transition, xlim is a hyper-parameter speci-
fying the location of the two vertical lines defining the safe regions.
For all the experiments, we set the discount factor λ to be 0.995, and the KL step size for trust region
to be 0.01. The other parameters for environment and algorithms in our experiments are listed in the
following table.
12
Under review as a conference paper at ICLR 2021
	PointCircle	AntCircle	HumannoidCircle	PointGather	AntGather
	 Circle Radius	10	3	5	-	-
xlim	4	1.5	2	-	-
Batch Size	50k	100k	100	50k	100k
Trajetory Length	50	1000	1000	30	500
Constraint Value	5	10	10	3	1
β	0.5	0.1	0.1	0.1	0.1
Table 1: Hyper-parameter settings.
7.4	PROOF OF k-STEP ADVANTAGE
We have V	(st)	=	Ea,st+1	[rt	+ γV	(st+1)].	Rearranging it gives	Eat,st+1	[rt]	=	V (st)	-
YEst+ι [V(st+ι)], which actually provides an unbiased estimator ^ for the expected one-step re-
wardEat,st+1 [rt], as given by
^ = V (st) — YV (st+1).	(18)
Rewrite rt as rt = αtrt + (1 - αt)rt, then We have
Atk) = -V (St) + rt + γrt+ι +----+ Y k-1 九+k-i + Yk V (st+k)
=-V (St) + αtr + γαt+1rt+1 +-----+ Yk-1αt+k-irt+k-i + Yk V (st+k)
+ (1 — at)rt + Y(1 — αt+ι)rt+ι +----+ Yk-1(1 - αt+k-i)rt+k-i
=-V (St) + αtr + Yαt+1rt+1 +-----+ Yk-1αt+k-irt+k-i + Yk V (st+k)
+ (1 - αt)[V(St) - YV(St+1)]
+ Y(1 - αt+1)[V(St+1) - YV(St+2)]
+…
+ Yk-1(1 - αt+k-1)[V (St+k-1) - YV(St+k)]
= -V(St) + αt[rt + YV(St+1) - V(St)] + [V(St) - YV(St+1)] + YkV(St+k)
+ Yαt+1[rt+1 +YV(St+2) - V(St+1)] + Y[V(St+1) - YV(St+2)]
+…
+ Yk-1αt+k-1 [rt+k-1 + YV(St+k) - V(St+k-1)] + Yk-1 [V (St+k-1) - YV(St+k)]
k-1
=	Yl αt+lδt+l,
l=0
where δt+l =. rt+l + YV(St+l+1) - V(St+l).
7.5 Proof of Theorem 1
Lemma 1. Achiam et al. (2017) For any function f : S → R and any policy π,
(1 - Y)Es〜μ[f (s)]+ Es〜d： [Yf (s0)] - Es〜d∏ [f (s)] = 0.	(19)
a〜π0
s0〜P
Combining this with Equation 9, we obtain the following, for any function f and any policy π :
1
Jnafe = Es 〜μ[f (s)] + -- Es 〜d： [f(s, a, Sr) + Yf (s')- f (s)]	(20)
1 - Y a〜π0
s0〜P
In particular, we choose the function f(S) to be value function Vπ(S). Thus, we have
1
Jnafe = Es〜μ[Vπ(s)] + -- Es〜d： [f(s, a, s0) + YVπ (s0) - Vπ (s)]
1 - Y a〜n0
s0〜P
13
Under review as a conference paper at ICLR 2021
Lemma 2. For any function: f : S → R and any policies π and π0, define
Ln⑺=Esa^ [(⅛⅛)-1 卜(S，a，SO) + γf(s0)- f(s))
S0〜P
and ef' = maxs |E。〜∏0,s0 〜P [f(s, a, s0) + Yf (s0) — f (s)]|. Then the following bounds hold:
Jsafe(∏0) — Jsafe(∏) ≥ 1-- (L∏,f(π0) — 2ef0DTV(dπ∣∣dπ)
Jsafe(∏0) — Jsafe(∏) ≤ 占(L∏,f(∏0) + 2ef0DTV(dπ∣∣dπ)
where DTV is the total variational divergence.
(21)
(22)
Proof. The proof can be established by following the one for Lemma 2 in Achiam et al. (2017)
where We substitute Jafe in Equation 20.	□
Lemma 3. Achiam et al. (2017) The divergence between discounted furture state visitation distri-
butions, kdπ0 — dπ k1, is bounded by an average divergence of the policies π0 and π:
Ildn — dπ k 1 ≤ ~~ɔ-Es〜dπ [DTV (π0∣ In) [s]],	(23)
1 一 Y
where DTV(π0IIπ)[S] = (1/2) PaIπ0(aIS) — π(aIS)I.
Now, with Lemma 2 and Lemma 3, we are ready to prove Theorem 1 as follows.
Proof. By choosing f(St) = Vπ(St) to be the safety value function in Lemmas 2 and , we have
L∏,f (∏0) = E st〜dπ Ft + YVπ(st+ι) — Vπ(st)] — E st〜dπ 阮 + YVπ(st+ι) — Vπ(st)]
a〜π0	at〜π
st+1 〜P	st+1 〜P
=E st〜dπ 鼻 + γrt+ι + γ2Vπ(st+2) — Vπ(st)] — Est〜dπ 鼻 + γrt+ι + γ2Vπ⑶+？)— Vπ⑶)]
a 〜π0	αt^π
st+1 〜P	st+1 〜P
E st〜d： [rt + …+ γk-1rt+k-i + Yk Vπ⑶+®) — Vπ⑶)]
a〜π0
st+1 〜P
—E st〜d∏ 阮 + …+ Yk-1rt+k-i + YkVπ(st+k) — Vπ(st)]
at〜π
st+1 〜P
E st〜d∏
α〜π0
st+1 〜P
[Ak] — E st〜d∏ [Ak]
a〜π
st+1 〜P
Thus, computing the exponentially average of Lπ,f π0 with λ as the weighting cofficient gives:
L∏,f (∏0) = E *〜d∏ [ACsae] - E st〜d∏ [ACSAE] ≥ E st〜d∏ [AC吗
a〜π0	a〜π	a〜π0
st+1 〜P	st+1 〜P	st+1 〜P
(24)
The last inequality comes from the fact that E ”4： [Acsae] ≤ 0. Then applying Lemma 3 gives
a〜π
st+1 〜P
the result.
□
7.6 Proof of Theorem 2
Define ξ to be the β-worst-case distribution over the trajectories, i.e., ξ(τ) = 1∕β if C(τ) is among
the top β most costly trajectories; and ξ(τ) = 0 otherwise. Denote Pβ = ξ ◦ P to be the weighted
probability distribution and dβπ to be the discounted future state distribution for the β-worst cases.
14
Under review as a conference paper at ICLR 2021
Then the expected cost over the β-worst-case trajectories can be expressed compactly as:
JC(π) =	Es〜dβ [C(SMSO)].	(25)
1 - Y a 〜π
s0 〜Pe
We also have the following identity:
(I- γPβ)d∏ = (1- Y)μ.	(26)
With the above relation, we can obtain the following lemma.
Lemma 4. For any function f : S → R and any policy π,
(1 - Y)Es〜μ[f(s)] + Es〜dπ [γf(s0)] - Es〜dπ [f(s)]=0.	(27)
a〜π
s0 〜Pe
Combining with Equation 25, we have
JC(∏) = Es〜μ[f (s)] + ɪEf [C(s, a, s0) + γf (s0) - f (s)].	(28)
1 Y a 〜π
s0 〜Pe
Choosing the cost value function VCπ as f gives:
JC(∏) = - (s)] + ɪEs〜dπ [C(s, a, S) + YVC(s0) - VC(s)]∙	(29)
1 - Y	a 〜π
s0 〜Pe
Following the proof for Theorem 1, we obtain Theorem 2.
7.7 Experiments on WCMDP
To further study how our two contributions (CSAE and WCMDP) contribute to the final algorithm,
we perform ablation study where the safe algorithm does not dampen the advantage function but
respects the worst-case constraints, which is referred as WC in the following. We compare WC
with CSAE, CSAE-WC and the other baseline methods in Fig. 5. Compared to CSAE, though
WC is able to give better safety guarantee, it actually produces inferior performance, especially
on PointCircle and AntGather. Besides, CSAE also demonstrates faster convergence speed than
WC. By involving them in the same algorithm, CSAE-WC is able to combine their strengths and
overcome their weaknesses, thus results in superior return performance and safety guarantee.
15
Under review as a conference paper at ICLR 2021
AntCircIe
O	59	IOO UO
AntGather
O	l∞0	2000
O	200	400
2∞0
O	l∞0	2∞0	30∞
O	59	IOO UO
l∞0
2000
150
O	200	400
500	l∞0
IOO
2000
O	200	400
500	l∞0
59

Figure 5: Learning curve comparison between our methods (CSAE, WC and CSAE-WC) and the
state-of-the-arts (TRPO, PDO, CPO)for five safe RL problems. First row: safe cumulative reward.
Second row: total cumulative reward. Third row: cumulative cost. Fourth row: ratio of safe trajec-
tories.
16