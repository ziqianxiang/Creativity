Under review as a conference paper at ICLR 2021
Run Away From your Teacher: a New Self-
Supervised Approach Solving the Puzzle of
BYOL
Anonymous authors
Paper under double-blind review
Ab stract
Recently, a newly proposed self-supervised framework Bootstrap Your Own La-
tent (BYOL) seriously challenges the necessity of negative samples in contrastive
learning frameworks. BYOL works like a charm despite the fact that it discards
the negative samples completely and there is no measure to prevent collapse in its
training objective. In this paper, we suggest understanding BYOL from the view of
our newly proposed interpretable self-supervised learning framework, Run Away
From your Teacher (RAFT). RAFT optimizes two objectives at the same time:
(i) aligning two views of the same data to similar representations and (ii) run-
ning away from the model’s Mean Teacher (MT, the exponential moving average
of the history models) instead of BYOL’s running towards it. The second term
of RAFT explicitly prevents the representation collapse and thus makes RAFT a
more conceptually reliable framework. We provide basic benchmarks of RAFT on
CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that
BYOL is equivalent to RAFT under certain conditions, providing solid reasoning
for BYOL’s counter-intuitive success.
1	Introduction
Recently the performance gap between self-supervised learning and supervised learning has been
narrowed thanks to the development of contrastive learning (Chen et al., 2020b;a; Tian et al., 2019;
Chen et al., 2020b; Sohn, 2016; Zhuang et al., 2019; He et al., 2020; Oord et al., 2018; Hadsell
et al., 2006). Contrastive learning distinguishes positive pairs of data from the negative. It has
been shown that when the representation space is l2 -normalized, i.e. a hypersphere, optimizing
the contrastive loss is approximately equivalent to optimizing the alignment of positive pairs and the
uniformity of the representation distribution at the same time (Wang & Isola, 2020). This equivalence
conforms to our intuitive understanding. One can easily imagine a failed method when we only
optimize either of the properties: aligning the positive pairs without uniformity constraint causes
representation collapse, mapping different data all to the same point; scattering the data uniformly
in the representation space without aligning similar ones yields no more meaningful representation
than random.
The proposal of Bootstrap Your Own Latent (BYOL) fiercely challenges our consensus that negative
samples are necessary to contrastive methods (Grill et al., 2020). BYOL trains the model (online
network) to predict its Mean Teacher (moving average of the online, refer to Appendix B.2) on two
augmented views of the same data (Tarvainen & Valpola, 2017). There is no explicit constraint
on uniformity in BYOL, while the expected collapse never happens, what’s more, it reaches the
SOTA performance on the downstream tasks. Although BYOL has been empirically proven to be
an effective self-supervised learning approach, the mechanism that keeps it from collapse remains
unrevealed. Without disclosing this mystery, it would be disturbing for us to adapt BYOL to other
problems, let alone further improve it. Therefore solving the puzzle of BYOL is an urgent task.
In this paper, we explain how BYOL works through another interpretable learning framework which
leverages the MT in the exact opposite way. Based on a series of theoretical derivation and em-
pirical approximation, we build a new self-supervised learning framework, Run Away From your
Teacher (RAFT), which optimizes two objectives at the same time: (i) minimize the representation
1
Under review as a conference paper at ICLR 2021
distance between two samples from a positive pair and (ii) maximize the representation distance
between the online and its MT. The second objective of RAFT incorporates the MT in a way exactly
opposite to BYOL, and it explicitly prevents the representation collapse by encouraging the online to
be different from its history (Figure 2a). Moreover, we empirically show that the second objective of
RAFT is a more effective and consistent regularizer for the first objective, which makes RAFT more
favorable than BYOL. Finally, we solve the puzzle of BYOL by theoretically proving that BYOL
is a special form of RAFT when certain conditions and approximation hold. This proof explains
why collapse does not happen in BYOL, and also makes the performance of BYOL0 an approximate
guarantee of the effectiveness of RAFT.
The main body of the paper is organized in the same order of how we explore the properties of
BYOL and establish RAFT based on them (refer to Appendix A for more details). In section 3, we
investigate the phenomenon that BYOL fails to work when the predictor is removed. In section 4,
we establish two meaningful objectives out of BYOL by upper bounding. Based on that, we propose
RAFT due to its stronger regularization effect and its accordance with our knowledge. In section 5,
we prove that, as a representation learning framework, BYOL is a special form of RAFT under
certain achievable conditions.
In summary, our contributions are listed as follows:
•	We present a new self-supervised learning framework RAFT that minimizes the alignment
and maximizes the distance between the online network and its MT. The motivation of
RAFT conforms to our understanding of balancing alignment and uniformity of the repre-
sentation space, and thus could be easily extended and adapted to future problems.
•	We equate two seemingly opposite ways of incorporating MT in contrastive methods under
certain conditions. By doing so, we unravel the puzzle of how BYOL avoids representation
collapse.
2	Background and related work
2.1	Two metrics optimized in contrastive learning
Optimizing contrastive learning objective has been empirically proven to have positive correlations
with the downstream task performance (Chen et al., 2020b;a; Tian et al., 2019; Chen et al., 2020b;
Sohn, 2016; Zhuang et al., 2019; He et al., 2020; Oord et al., 2018). Wang & Isola (2020) puts the
contrastive learning under the context of hypersphere and formally showcases that optimizing the
contrastive loss (for preliminary of contrastive learning, refer to Appendix B.1) is equivalent to opti-
mizing two metrics of the encoder network when the size of negative samples K is sufficiently large:
the alignment of the two augmented views of the same data and the uniformity of the representation
population. We introduce the alignment objective and uniformity objective as follows.
Definition 2.1 (Alignment loss) The alignment loss Lalign(f, Ppos) of the function f over positive-
pair distribution Ppos is defined as:
Lalign(f； Ppos)，E(x1,x2)〜PUkf(X1)— f (X2)k2]，	⑴
where the positive pair (χ1,χ2) are two augmented views of the same input data X 〜 X, i.e.
(xi, X2) = (tι(χ), t2(χ)) and tι 〜Tl,t2 〜石 are two augmentations. For the sake of simplicity,
we omit Ppos and use Lalign(f) in the following content.
Definition 2.2 (Uniformity loss) The loss of uniformity Luniform (f; X) of the encoder function f
over data distribution X is defined as
LUniform(f； X)，log E(x,y)〜X2 [e-tkf(X)-“力耳，	⑵
where t > 0 is a fixed parameter and is empirically set to t = 2. To note here, the vectors in the
representation space are automatically l2-normalized, i.e. f(x) , f (x)/kf (x)k2, as we limit the
representation space to a hypersphere following Wang & Isola (2020) and Grill et al. (2020) and the
representation vectors in the following context are also automatically l2-normalized, unless specified
otherwise. Wang & Isola (2020) has empirically demonstrated that the balance of the alignment loss
2
Under review as a conference paper at ICLR 2021
and the uniformity loss is necessary when learning representations through contrastive method. The
rationale behind it is straightforward: Lalign provides the motive power that concentrates the similar
data, and Luniform prevents it from mapping all the data to the same meaningless point.
2.2	BYOL: bizarre alternative of contrastive
A recently proposed self-supervised representation learning algorithm BYOL hugely challenges the
common understanding, that the alignment should be balanced by negative samples during the con-
trastive learning. It establishes two networks, online and target, approaching to each other during
training. The online is trained to predict the target’s representations and the target is the Exponential
Moving Average (EMA) of the parameters of the online. The loss of BYOL at every iteration could
be written as
LBYOL , E(x,tι,t2)〜(X,T1,T2) [∣∣qw (fθ(tl(X)))- fξ(t2 (X))俏卜
(3)
where two vectors in representation space are automatically l2-normalized. fθ is the online encoder
network parameterized by θ and qw is the predictor network parameterized by w. X 〜X is the
input sampled from the data distribution X, and t1(X), t2(X) are two augmented views of X where
tι 〜T1,t2 〜T are two data augmentations. The target network fξ is of the same architecture as
fθ and is updated by EMA with τ controlling to what degree the target network preserves its history
ξ J τξ +(1 - T)θ.
(4)
From the scheme of BYOL training, it seems like there is no constraint on the uniformity, and
thus most frequently asked question about BYOL is how it prevents the representation collapse.
Theoretically, we would expect that when the final convergence of the online and target is reached,
LBYOL degenerates to Lalign and therefore causes representation collapse, while this speculation
never happens in reality. Despite the perfect SOTA performance of BYOL, there is one inconsistency
not to be neglected: it fails with representation collapse when the predictor is removed, which means
qw(X) = X for any given X. This inconsistent behavior of BYOL weakens its reliability and further
poses questions on future adaptation of the algorithm. The motivation of understanding and even
solving this inconsistency is the start point of this paper.
3	On-and-off BYOL: failure without predictor
We start by presenting a dissatisfactory property of BYOL: its success heavily relies on the existence
of the predictor qw. The experimental setup of this paper is listed in Appendix C. The performance of
BYOL original model, whose predictor qw is a two-layer MLP with batch normalization, evaluated
on the linear evaluation protocol (Kolesnikov et al., 2019; Kornblith et al., 2019; Chen et al., 2020a;
He et al., 2020; Grill et al., 2020) reaches 68.08 ± 0.84%. When the predictor is removed, the
performance degenerates to 20.92 ± 1.29%, which is even lower than the random baseline’s 42.74 ±
0.41%. We examine the speculation that the performance drop is caused by the representation
collapse both visually (refer to Appendix F.1) and numerically. Inspired by Wang & Isola (2020),
we use Luniform(fθ; X) to evaluate to what degree the representations are spread on the hypersphere
and Lalign(qw ◦ fθ) to evaluate how the similar samples are aligned in the representation space. The
results in Table 1 show that with the predictor, BYOL optimizes the uniformity of the representation
distribution. On the contrary, when taken away the predictor, the alignment of two augmented views
is overly optimized and the uniformity of the representation deteriorates (Figure 4), therefore we
conclude the predictor is essential to the collapse prevention in BYOL.
One reasonable follow-up explanation on the efficacy of the predictor may consider its specially
designed architecture or some good properties brought by the weight initialization, which makes
it hard to understand the mechanism behind it. Fortunately, after replacing the current predictor,
two-layer MLP with batch normalization (Ioffe & Szegedy, 2015), with different network architec-
tures and weight initializations, we find that there is no significant change either on linear evaluation
protocol or on the model behavior during training (Table 1, for detailed training trajectory, refer to
Figure 4). We first replace the complex structure with linear mapping qw(∙) = W(∙). This replace-
ment provides a naive solution to representation collapse: W = I , while it never converges to this
apparent collapse. Surprisingly enough when we go harsher on this linear predictor by initializing
3
Under review as a conference paper at ICLR 2021
Table 1: Evaluation results of BYOL variants on CIFAR10 after 300 epochs of training, used as
evidence supporting the proposal of RAFT. (α, β) = (1, 1) are set in BYOL0 and RAFT. Lalign(qw ◦
fθ) and Luniform(fθ) are evaluated by averaging the last 10 epochs of training. We highlight the
overly optimized Lalign and failed Luniform in this table. For the accuracy on the linear evaluation
protocol, we also only highlight the ones that underperform the random baseline.
Model	qw	Lalign(qw ◦ fθ)	LUniform(fθ)	LinearEvaluationProtocol(%)
Rand-Baseline	W	78.09 X 10-^	-0.51	42.74 ± 0.41
BYOL	MLP	25.03 X 10-4	-222	64.32 ± 0.89
BYOL0	MLP	22.09 X 10-4	-2.07	69.21 ± 1.01
RAFT	MLP	18.90 X 10-4	-2.04	71.31 ± 0.75
BYOL-LP	W	7.71 X 10-4^^	^^-2?l6	67.64 ± 0.90
BYOL0-LP	W	7.32 X 10-4	-2.19	68.61 ± 0.73
RAFT-LP	W	7.42 X 10-4	-2.23	67.55 ± 0.55
BYOL-NP	I	1.94 X 10-10	^^^-0.14	20.92 ± 1.29
BYOL0-NP	I	1.35 X 10-10	--0.10	16.92 ± 1.05
RAFT-NP	I	16.07 X 10-4	-0.006	11.72 ± 0.05
TanBYOL-LP	W	7.83 X 10-4^^	-192	67.63 ± 1.27
TanBYOL0-LP	W	7.52 X 10-4	-2.19	67.66 ± 0.76
TanRAFT-LP	W	7.61 X 10-4	-2.13	69.31 ± 0.87
W with the apparent collapse solution I , the model itself seems to have a self-recovering mecha-
nism even though it starts off at a poor position: the loss quickly approaches to 0 and the uniformity
deteriorates for 10-20 epochs and suddenly it deflects from the collapse and keeps on the right track.
We conduct a theoretical proof that a randomly initialized linear predictor prevents the (more strict
form of) representation collapse by creating infinite non-trivial solutions when the convergence is
achieved (refer to Appendix I), while we fail to correlate the consistently optimized uniformity with
the presence of the predictor, which indicates that a deeper rationale needs to be found.
4	Run away from your teacher: more effective regularizer
4.1	Disentangle the BYOL loss by upper bounding
Analyzing LBYOL is hard, since it only has one single mean squared error term and there are many
factors entangled within it, e.g., two augmented views of the same data, predictor, and the EMA
updating rule. Inspired by the Bias-Variance decomposition on squared loss (Geman et al., 1992),
we extract the alignment loss by subtracting and adding the same term qw (fθ (t2 (x))) and further
yield the upper bound of LBYOL. For details, please refer to Appendix G.
Definition 4.1 (Cross-model loss) The cross-model loss Lcross-model(f, g; X) of the function f and
g over the data distribution X is defined as
LCross-model(f, g; X) , Ex〜X
f (x) - g(x)22 .
(5)
Definition 4.2 (BYOL0 loss) The BYOL0 loss LBYOL0 is defined as
LBYOL0 , αLalign(qw ◦ fθ ; Ppos ) + βLCross-model (qw ◦ fθ , fξ ; X2 )	(6)
where α, β > 0 are Constants, Ppos is defined in Eq. 1 and X2 = T2 (X) is the distribution of the
augmented data. For the sake of simpliCity, we use Lalign(qw ◦ fθ) to denote Lalign(qw ◦ fθ; Ppos)
in the following Content. For the sake of symmetry, we use LCross-model (qw ◦ fθ) to denote
(1/2)[LCross-model(qw ◦ fθ,fξ,X1) + LCross-model(qw ◦ fθ,fξ,X2)] to Compute the Cross-model loss.
4
Under review as a conference paper at ICLR 2021
Figure 1: Framework diagram of RAFT and BYOL. The online network is composed of an encoder
fθ and an extra predictor qw. The Mean Teacher fξ is the EMA of the encoder fθ. In BYOL, the
loss is computed by minimizing the distance between the prediction of one view xi and another
view x2 ’s representation generated by the MT. In RAFT, we optimize two objectives together: (i)
minimize the representation distance between two samples from a positive pair and (ii) maximize
the representation distance between the online network and its MT.
Theorem 4.1 (LBYOL0 is an upper bound of LBYOL) LBYOL0 is an upper bound of LBYOL if we ig-
nore the scalar multiplication. Concretely speaking, for any given constants α, β > 0, we have
LBYOL ≤ ( α + β ) LBYOL0.
(7)
Proof Please refer to Appendix G.
Ideally, minimizing LBYOL0 would yield similar performance as minimizing LBYOL. We exemplify
the legitimacy of LBYOL0 by setting (α, β) = (1, 1). In Table 1, the performance of BYOL and
BYOL0 are close to each other with respect to three metrics: alignment, uniformity, and downstream
linear evaluation protocol, regardless of the form of predictors. When the predictor is linear map-
ping, the performance differences between them are subtle. Besides, when the predictor is removed,
the representation collapse also happens to BYOL0 . So we conclude that optimizing LBYOL0 is al-
most equivalent to LBYOL. In spite of the performance similarity, LBYOL0 is of a more disentangled
form than LBYOL and therefore we focus on studying the former instead of the latter.
The new objective consists of two terms: the first term Lalign minimizes the representation distance
between samples from a positive pair and has already been shown crucial to the successful con-
trastive methods (Wang & Isola, 2020). Intuitively, it provides the motive power to concentrate sim-
ilar data in the representation space. Based on the form of BYOL0 , we conclude that MT is used to
regularize the alignment loss. This perspective of two terms regularizing each other is crucial to our
analysis and improvement of the original BYOL framework. Understanding why BYOL works with-
out collapse is approximately equivalent to understanding how minimizing Lcross-model(qw ◦ fθ, fξ)
effectively regularizes the alignment loss, or even actively optimizes the uniformity.
4.2 RAFT: run away from your teacher
The major difficulty of correlating Lcross-model with Luniform is that their optimization intentions are
not only irrelevant, but somewhat opposite. Minimizing the cross-model loss asks the network
to produce close representations for certain inputs, while optimizing the uniformity loss requires
it to produce varying representations. The disparity residing in the form pushes us to question
the original motivation of BYOL: do we really want the online network to approach to the Mean
Teacher? To test our suspicion, we minimize [Lalign(qw ◦ fθ) - Lcross-model(qw ◦ fθ , fξ)] instead
of [Lalign(qw ◦ fθ) + Lcross-model(qw ◦ fθ, fξ)], and we find it works as well. This bizarre phe-
nomenon will be explained in Section 5. Removing the predictor, we observe that although mini-
mizing [Lalign(fθ) - Lcross-model(fθ, fξ)] fails to yield better representation than the random baseline,
5
Under review as a conference paper at ICLR 2021
Figure 2: Analysis on the legitimacy of RAFT and Why it's more favorable. (a) Diagram demon-
strating how RAFT conceptually works: if two samples, updating directions are opposite, MT helps
pushing them away at the next several iterations. Here Zi = qw (fθ(xi)), Z = fξ(xi), i = 1, 2.
(b) Objective categories diagram with respect to the effect constraining the alignment loss. In con-
trastive methods, the most favorable objective actively optimizes uniformity, including both BYOL0
and RAFT when there exists predictor. The secondly favorable objective is the effective regularizer
of alignment. RAFT remains to restrain alignment loss without predictor, while BYOL fails to do
so, which implies that RAFT is a more unified objective.
it prevents the overly-optimized alignment loss, i.e. it works as an effective regularizer for the align-
ment loss, while minimizing Lcross-model(fθ, fξ) does not.
Based on the conclusion above and law of Occam’s Razor, we propose a new self-supervised learn-
ing framework, Run Away From your Teacher (RAFT), which optimizes two learning objectives
simultaneously: (i) minimize the alignment loss of two samples from a positive pair and (ii) maxi-
mize the distance between the online network and its MT (refer to Figure 1 and Algorithm 1).
Definition 4.3 (RAFT loss) The RAFT loss LRAFT is defined as
LRAFT , αLalign (qw ◦ fθ ; Ppos ) - βLcross-model (qw ◦ fθ , fξ ; X2 ),	(8)
where α, β > 0 are constants and other components follows the Definition 4.2.
Compared to BYOL and BYOL0, RAFT better conforms to our knowledge and is a conceptually
non-collapsing algorithm. There has been a lot of work demonstrating that weight averaging is
roughly equal to sample averaging (Tarvainen & Valpola, 2017), thus if two samples’ representa-
tions are close to each other at the beginning and their initial updating directions are opposite, then
RAFT consistently separates them in the representation space. All the forms of loss terms could
be classified into three categories: uniformity optimizer, effective regularizer for alignment loss,
and others (refer to Figure 2b). According to our experiments, when the predictor is removed, run-
ning away from MT remains an effective regularizer for the alignment loss while BYOL’s running
towards MT fails to do so, thus RAFT is of more unified and consistent form. In summary, our pro-
posed learning framework RAFT is completely based on the intention of solving the inconsistency
of the predictor in BYOL, and it’s better than BYOL in threefold:
•	Consistency. Compared to BYOL, our newly proposed method has an effective regularizer
for the alignment loss regardless of the presence of predictor.
•	Interpretability. Mean teacher uses the technique of weight averaging and thus could be
considered as an approximate ensemble of the previous versions of the model. Running
away from the mean teacher intuitively encourages the diversity of the representation,
which is positively correlated to the uniformity.
•	Disentanglement. The learning objective is decoupled into aligning two augmented views
and running away from the mean teacher, and hence could be independently studied.
6
Under review as a conference paper at ICLR 2021
We will discuss the relationship between RAFT and BYOL0 in the next section, and we find BYOL0
is a special form of RAFT under certain conditions, which makes the performance of BYOL0 a
guarantee of the effectiveness of RAFT. We provide benchmarks of alignment, uniformity, and
downstream linear evaluation performance on CIFAR10 (Table 3). We discover that balancing the
alignment loss and the cross-model loss is not an easy job with the predictor taken away. The im-
balance between the alignment loss and the cross-model loss would lead to representation collapse
or over-regularized alignment where every data is randomly projected. One interesting research di-
rection is to study the efficacy of the predictor. The reason why it helps the two terms to achieve an
equilibrium is left to be answered.
5	Understanding BYOL via RAFT
In Section 4.1 we derive an upper bound LBYOL0 of LBYOL and explicitly extract two terms Lalign and
Lcross-model. In BYOL0, two terms are simultaneously minimized, while in RAFT, we minimize Lalign
but maximize Lcross-model instead. To clearly distinguish the difference between the two objectives,
we rewrite them as following:
LBYOL0 = αLalign (qw ◦ fθ) + βLcross-model (qw ◦ fθ , fξ),	(9)
LRAFT = αLalign(qw ◦ fθ) - βLcross-model(qw ◦ fθ, fξ),	(10)
where α, β > 0 are constants.
In form, LRAFT and LBYOL0 seem to evolve in opposite optimizing direction on the second term, but
the empirical study has shown that both of them work. How can two opposite optimization goals
produce similar effect? Since RAFT is a conceptually working method, we analyze the mechanism
of BYOL0 by establishing the equivalence between the parameters of BYOL0 and RAFT under mild
conditions.
Theorem 5.1 (One-to-one correspondence between BYOL0 and RAFT) There is a one-to-one
correspondence between parameter trajectories of BYOL0 and RAFT when the following three con-
ditions hold:
i.	the representation space is a hypersphere;
ii.	the predictor is a linear transformation, i.e. qw (∙) = W(∙);
iii.	only the tangential component of the gradient on the hypersphere is preserved.
Proof We prove the theorem by construction. For the detail, please refer to Appendix H.
Remark The third condition conforms to the property of the hypersphere representation space and
is easy to achieve. One can preserve only the tangential gradient by slightly modifying the loss.
For example, suppose the representation of the MT is Z and the representation of the input is Z
which are both normalized, the cross-model loss IB - z^2 can be revised as ∣∣Z - λz∣b∕λ, where
λ = sg( hz, Zi) stands for stopping gradient of the inner producthz, Zi. Our experiments in Table 1
demonstrates that the condition of the tangential component of the gradient doesn’t turn any of the
algorithms including BYOL, BYOL0 and RAFT into a collapsed one.
In Theorem 1, we show that optimizing LBYOL0 with initial parameters (θ(0) , W(0) ) is equivalent to
optimizing LRAFT with initial parameters (θ(0), -W(0)) when the aforementioned three conditions
are satisfied. This equivalence demonstrates that the final encoder network fθ and fθ0 equal to each
other. Therefore we conclude that, as representation learning framework, BYOL0 is equivalent to
our newly proposed RAFT.
From a geometric point of view, the optimization process is the data points moving in the represen-
tation space under the guidance of the training loss. The loss function measures the potential energy
of the parameters, and the gradient with regard to the data points is the motive force. If the represen-
tation space is a hypersphere as in BYOL, then the tangential force, i.e. the tangential component
of the gradient, is the only key to scattering or concentrating the data points in the representation
space. By the central symmetry of the hypersphere, clockwise and counterclockwise moving direc-
tions are equivalent to some extent, for example, pushing a point by n/2 and pulling it by n/2 on
the 2-dimensional sphere causes the same effect.
7
Under review as a conference paper at ICLR 2021
The equivalence between BYOL0 and RAFT offers us a direct way to understand some strange
phenomena we observe which are also reported in the original BYOL paper. Firstly, the non-collapse
of BYOL is explained, since the RAFT is an intuitively and practically working algorithm. The
equivalence of BYOL0 and RAFT when predictor is linear helps us understand why BYOL is an
effective self-supervised learning algorithm. It also explains our initial question why BYOL fails to
avoid representation collapse without the predictor: removing the predictor means fixing W = I,
which breaks the RAFT’s designing principle of running away from the MT. Secondly, though the
BYOL’s optimization procedure is of the form that two models approaching to each other, there has
been no report of convergence in the original paper. The established equivalence perfectly explains
it. RAFT incorporates the MT in an extremely dynamic way since it continuously varies from the
history models, thus there would be no convergence of the data points. So does the parameters.
6	Conclusion and future work
In this paper, we address the problem of why the newly proposed self-supervised learning framework
Bootstrap Your Own Latent (BYOL) works without negative samples. By decomposing, upper
bounding and approximating the original loss of BYOL, we establish another interpretable self-
supervised learning method, Run Away From your Teacher. We show that RAFT contains an explicit
term that prevents the representation collapse and we also empirically validate the effectiveness of
RAFT. By constructing a one-to-one correspondence from RAFT to BYOL0 (variant of BYOL), we
successfully explain the mechanism behind BYOL that makes it work and therefore implies the huge
potential of our proposed RAFT. Based on the observation and the conclusion, here we have several
suggestions for future work:
Theoretical guarantees of RAFT. Though we have intuitively explained why running away from
the MT is an effective regularizer, we don’t provide theoretical guarantees why optimizing RAFT
would be favorable with respect to the representation learning. In future, one can try to relate RAFT
to the theory of Mutual Information (MI) maximization (Belghazi et al., 2018; Hjelm et al., 2018;
Tschannen et al., 2019), as the training objective of contrastive learning InfoNCE has been proven
to be a lower bound of the MI (Poole et al., 2019). One detail should be noticed when attempting
to correlate RAFT with MI maximization. Even though RAFT is an effective regularizer, it fails to
yield good-quality representations when the predictor is removed, thus any theoretical proof on the
effectiveness of RAFT should well explain the mechanism behind this extra predictor.
On the efficacy of the predictor. It has become a popular and almost standardized method to add
an extra MLP on top of the network in contrastive learning methods (Chen et al., 2020a;b; Grill
et al., 2020), while most of the work adopts this method as a special trick without considering the
effect this MLP brings to the algorithm. In this paper, however, we find that this extra MLP may
bring some unexpected properties to the original training objective: although the representations are
optimized by disparate motivations (in our paper, BYOL0 running towards MT and RAFT running
away from MT), the encoder network is trained to be exactly the same. This observation indicates
that the mechanism of the extra MLP to the network needs to be further studied.
8
Under review as a conference paper at ICLR 2021
References
Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. There are many con-
sistent explanations of unlabeled data: Why you should average. ICLR, 2019.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning,pp. 531-540, 2018.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):
124018, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural networks and the bias/variance dilemma.
Neural computation, 4(1):1-58, 1992.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recogni-
tion (CVPR’06), volume 2, pp. 1735-1742. IEEE, 2006.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual represen-
tation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recog-
nition, pp. 1920-1929, 2019.
Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661-2671,
2019.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
9
Under review as a conference paper at ICLR 2021
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia-
tional bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Advances in
neural information processing systems, pp. 1857-1865, 2016.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning results. In Advances in neural information
processing systems, pp. 1195-1204, 2017.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and
intelligent laboratory systems, 2(1-3):37-52, 1987.
Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning
of visual embeddings. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 6002-6012, 2019.
10
Under review as a conference paper at ICLR 2021
A Main thread of paper
The proposal of RAFT is based on a series of theoretical derivation and empirical approximation.
Therefore the logic chain of our paper is fundamental to the legitimacy of our explanation on BYOL
and the superiority of our newly proposed RAFT. Here we organize our main thread in the same
order as the sections, to provide a clear view with readers.
In Section 3,
•	As a learning framework, BYOL does not consistently work. It heavily relies on the exis-
tence of the predictor. We want to understand why this inconsistency exists.
•	The architecture of the predictor doesn’t affect the collapse of BYOL, the fact that the linear
predictor qw (∙) = W(∙) prevents collapse will be used as a crucial condition in Section 5.
In Section 4.1,
•	A new disentangled objective LBYOL0 = αLalign(qw ◦ fθ) + βLcross-model(qw ◦ fθ, fξ) is
established by upper bounding.
•	We showcase that minimizing LBYOL0 is close to minimizing LBYOL in terms of alignment,
uniformity, and linear evaluation protocol, which indicates that understanding the behavior
of optimizing BYOL’s upper bound is approximately equivalent to understanding BYOL.
In Section 4.2,
•	We find minimizing [Lalign(qw ◦ fθ) - Lcross-model(qw ◦ fθ , fξ)] works as well, which has
the exact opposite way of incorporating the cross-model loss to BYOL0 .
•	Based on the observation above, we propose a new self-supervised learning approach Run
AWay From your Teacher, which regularizes LaIign(qw ◦ fθ) by maximizing LCrOSS-model(qw ◦
fθ, fξ). Compared with BYOL, RAFT accords more with our common understanding.
•	Additional experiments show that without predictor, BYOL0 fails to regularize Lalign(fθ),
let alone optimizing uniformity. On the contrary, although not able to actively optimize
uniformity either, RAFT’s maximizing Lcross-model(fθ, fξ) continues to be an effective reg-
ularizer for Lalign(fθ), which makes it more favorable (Figure 2b).
In Section 5,
•	We prove that when the predictor is linear (qw = W) and the representation space is a
hypersphere where only the tangential component of gradient is preserved during training,
minimizing Lcross-model(W ◦ fθ, fξ) and maximizing it obtain the same encoder fθ.
•	Based on the equivalence above, we conclude that BYOL0 is a special case of RAFT un-
der conditions above. The equivalence established helps understanding several counter-
intuitive behaviors of BYOL.
11
Under review as a conference paper at ICLR 2021
B	Background and related work
B.1 Contrastive learning
Contrastive methods relies on the assumption that two views of the same data point share the infor-
mation, and thus creates a positive pair. By separating the positives and the negatives, the neural
network trained by the algorithm learns to extract the most useful information from the data and
performs better on the downstream tasks. Typically, the algorithm uses the InfoNCE objective:
h(x,x+)
Lc0ntrast(h, K ) = E(X ,χ+卜Ppo	- lθg (χ +)+ PK eh(χ,χ-) ,	(II)
{X-}Κ=1 〜Xk L e	+ i=1=l e	J
where (x, x+ ) are sampled from the positive pair distribution Ppos, which is built by a series of
data augmentation functions [ref]. The negative samples {x1-}K are i.i.d sampled for K times from
the data distribution X ; function h(x, y) measures the similarity between two input data (x, y).
Empirically for the sake of symmetry, the measurement function h(x, y) = d(f (x), f (y)) has an
encoder f (∙) and a similarity metric d(∙, ∙) evaluating how close the two representations are.
B.2 Mean teacher
There is one type of semi-supervised learning method that BYOL constantly reminds people of,
Mean Teacher (MT) (Tarvainen & Valpola, 2017; Laine & Aila, 2016). Like BYOL, MT is also of
Teacher-Student (T-S) framework, where the teacher network is also the EMA of the student net-
work. The additional consistency loss between the teacher and student is applied to the supervised
signals. There has been a lot of work demonstrating the efficacy of MT(Athiwaratkun et al., 2019;
Novak et al., 2018; Chaudhari et al., 2019), among which the major conclusion states that the consis-
tency loss between the student and its MT acts as a regularizer for better generalization. The proven
properties of MT might lead us to focus on how the online network’s learning from MT effectively
regularizes Lalign in BYOL. In this paper, however, we propose the opposite way of leveraging MT
in contrastive methods.
C Experimental setup
Dataset Our main goal is to unravel the mystery why BYOL doesn’t collapse during training and
to solve the predictor-inconsistency. The most important metric is whether the algorithm collapses
or not, and we don’t target on developing a more powerful self-supervised learning algorithm that
surpasses SOTA on large dataset. In this repsect, we limit our experiments to the scope of the
CIFAR10 dataset. Each image is resized from 32 × 32 to 96 × 96. This change is the consequence
of the tradeoff between the effect of the data augmentation and batch size: larger size of the image
would allow more subtle and informative data augmentation scheme while it will reduce the training
batch size, which has already been empirically shown is harmful to the model performance.
Model architecture In our experiments, the model is composed of three stages: an encoder fθ
that adopts the ResNet18 architecture (without the classifier on top); a projector gθ that is comprised
of a linear layer with output size 512, batch normalization, rectified linear units (ReLU), and a final
linear layer with output size 128; a predictor qw that is comprised of the same architecture as the
projector but without the batch normalization.
Training We adopt the same data augmentation scheme that is used in Chen et al. (2020a) and
Grill et al. (2020) and train the BYOL on the training set for 300 epochs with batch size 128 on
3 random seeds. The objective of training is specified accordingly and the model is trained on the
Adam optimizer with learning rate 3 × 10-4 (Kingma & Ba, 2014). Unless stated otherwise, we
update the target network with the EMA rate 4 × 10-3 without the cosine smoothing trick.
Evaluation After training, we evaluate the encoder’s performance on the widely adopted linear
evaluation protocol: we fix the parameter of the encoder and we train another linear classifier on
top of it using all the training labels for 100 epochs with learning rate 5 × 10-4. The final classi-
fication accuracy indicates to what degree the representations of the same class concentrate and the
representations of the different class separate, and thus tells the quality of the representation.
12
Under review as a conference paper at ICLR 2021
D Tables
Table 2: Look-up table for the models that appear in the paper.
Model Name	Description
BYOL-MLPP (BYOL) BYOL-NP BYOL-LP BYOL-LPI TanBYOL-LP	BYOL with MLP-Predictor BYOL with No Predictor BYOL with Linear Predictor BYOL with Linear Predictor initialized with I BYOL with Linear Predictor, preserving only the Tangential gradient
BYOL0-MLPP (BYOL0) BYOL0-LP BYOL0-NP TanBYOL0-LP	trained with LBYOL0 = Lalign + Lcross-model, MLP Predictor trained with LBYOL0 = Lalign + Lcross-model, linear predictor trained with LBYOL0 = Lalign + Lcross-model, No Predictor BYOL0 with Linear Predictor, preserving only the Tangential gradient
RAFT-MLPP (RAFT) RAFT-NP RAFT-LP TanRAFT-LP	RAFT with MLP-Predictor RAFT with No Predictor RAFT with Linear Predictor RAFT with Linear Predictor, preserving only the Tangential gradient
Table 3: Evaluation results of RAFT on CIFAR10. α and β represents the weight of Lalign(qw ◦ fθ),
and Lcross-model (qw ◦ fθ , fξ), i.e. L = αLalign(qw ◦ fθ ) + βLcross-model (qw ◦ fθ , fξ). All the quantifiable
metrics are evaluated after 300 epochs of training of the training set of CIFAR10. Compared to
BYOL0 , our proposed RAFT is better in terms of the effectiveness of regularizing the Lalign.
Model	qw	α β	Lalign(qw ◦ fθ)	LUniform(fθ)	Linear Evaluation Protocol(%)
Rand-Baseline	W--			7.81 × 10-3	-0.51	42.74 ± 0.41
RAFT	MLP	1	-0.1	8.37 × 10-5^^^	-2.00	65.53 ± 0.99
	MLP	1	-1	1.89 X 10-3	-2.04	71.31 ± 0.75
	MLP	1	-10	1.00 × 10-2	-0.29	25.88 ± 0.42
RAFT-LP	W	1	-0.1	8.22 × 10-6^^	-1.61	52.57 ± 2.72
	W	1	-1	7.42 × 10-4	-2.25	67.55 ± 0.55
	W	1	-10	3.70 × 10-4	-2.15	66.10 ± 0.82
RAFT-NP	I	1	-1	1.61 × 10-3^^	-0.01	11.72 ± 0.05
	I	1	-10	1.54 × 10-2	-0.99	32.13 ± 0.52
	I	1	-100	1.56 × 10-2	-1.29	29.36 ± 0.53
BYOL0-NP	I	1	1	1.35 × 10-10^^	-0.12	16.92 ± 1.05
	I	1	10	2.38 × 10-10	-0.88	24.42 ± 1.14
	I	1	100	4.55 × 10-8	-1.14	37.48 ± 2.06
13
Under review as a conference paper at ICLR 2021
E Algorithms
Algorithm 1: RAFT: Run Away From your Teacher
Inputs :
X,T1,andT2
θ and fθ
w and qw
ξ and fξ
optimizer
K and N
{τk}kK=1 and {ηk}kK=1
for k = 1 to K do
set of images and distributions of transformations
model parameters and encoder
predictor parameters and predictor
MT parameters and MT
optimizer, updates online parameters using the loss gradient
total number of optimization steps and batch size
target network update schedule and learning rate schedule
1
2
3
4
5
6
7
8
9
10
11
12
13
B J {xi}N=ι 〜XN	// sample a batch of N images
for xi ∈ B do
tι 〜T and t2 〜T2	// sample image transformations
z1 J qw(fθ(t1(xi))) and z2 J qw(fθ(t2(xi)))	// reps for model
z10 J fξ(t1(xi)) and z20 J fξ(t2(xi))	// reps for MT
li = Ilk⅛ - kz2⅛ll2	// loSsfOralignment
li = -1 (llι⅛ - ι⅛ll2 + ∣lι⅛ - ι⅛ll2) // loss for CroSS-model
end
δθJ Nn (P a® li + dθ li
θ J optimizer(θ, δθ, ηk)
ξJτkξ+(1-τk)θ
end
// compute the loss gradient w.r.t. θ
// update trainable parameters
// update target parameters
Output: encoder fθ
14
Under review as a conference paper at ICLR 2021
F Visualization of training evolution
F.1 Representation distribution evolution of different learning algorithms
epoch-100
epoch-200
-1.0	-05	0.0	05	1.0
epoch-300
-1.0	-0.5	0.0	0.5	1.0
(a) Supervised learning representation distribution evolution
epoch-1
epoch-50
epoch-200
epoch-300
-1.0	-0.5 OJO 0.5	1.0
-1.0	-0.5	0.0	0.5	1.0
-1.0	-0.5	0.0	0.5	1.0
-1.0	-05	0.0	05	1.0
-1.0	-05	0.0 OS 1.0
(b) BYOL representation distribution evolution
epoch-1
epoch-50
epoch-100
epoch-200
epoch-300
-1.0	-0.5 OJO 0.5	1.0
-1.0	-0.5	0.0	0.5	1.0
-1.0	-0.5	0.0	0.5	1.0
-1.0	-05	0.0	05	1.0
-1.0	-05	0.0 OS 1.0
(c) BYOL representation distribution evolution Wlo predictor
-1.0	-0.5 OJO 0.5	1.0
-1.0	-0.5	0.0	0.5	1.0
-1.0	-0.5	0.0	0.5	1.0
-1.0	-05	0.0	05	1.0
-1.0	-05	0.0 OS 1.0
epoch-1
-1.0	-0.5 OX) 0.5	1.0
epoch-50
-1.0	-0.5	0.0	0.5	1.0
(d) BYOL0(Lalign + Lcross-model) representation distribution evolution
(e) RAFT(LaHgn — Lcross-model) representation distribution evolution
Figure 3: Visualization of the representation distribution evolution on CIFAR10 training set. We
project the representation fθ(x) to 2-D dimension using PCA (Wold et al., 1987) and then normalize
to a unit sphere. The width of the circle shows the density of the data points that are projected to
that particular position. Two dots residing on each side of the blue line across the circle represents
two augmented views of the same data. (a) Supervised learning has no restriction on the uniformity
of representation space. (b) BYOL with predictor evenly projects the data to different positions. (c)
BYOL w/o predictor tends to project the huge portion of the data to the same position. (d) BYOL’s
upper bound BYOL0 also effectively disperses representations on the sphere. (e) Our RAFT shows
that minimizing/maximizing Lcross-model has similar effect on the final representation distribution.
15
Under review as a conference paper at ICLR 2021
F.2
Evolution of numerical metric in different learning algorithms
76543210
bl) SSOl CTC-C-EF
0	50	100	150	200	250	300
Training epochs
(a)
⅛≡E⅛⅞⊃
0	50	100	150	200	250	300
Training epochs
(b)
8 6 4 2
这 XOeJnUUe uoenΛ9 Jeuun
0	50	100	150	200	250	300
Training epochs
(c)
Figure 4: Training behaviors of BYOL with varying structures of the predictor. For detailed expla-
nation on the model architecture, refer to Appendix C. (a) Evolution of training loss LBYOL. When
taken out the predictor, the training loss quickly converges to 0 (red curve, BYOL-NP). Replacing
the MLP-predictor (blue curve, BYOL-MLPP) with the linear predictor (green curve, BYOL-LP)
will not cause the collapse even though I is an apparent solution for collapse. Furthermore, ini-
tializing the linear predictor with I forces the loss quickly approaching to 0 at beginning, while it
recovers from the seemingly collapse after 10-20 epochs of training (orange curve, BYOL-LPI). (b)
Evolution of the representation uniformity. BYOL with predictor consistently optimizes the unifor-
mity of the representation distribution even though the uniformity is not explicitly included in the
loss term. One interesting fact to note here is that the uniformity loss is optimized with a constant
rate with linear predictor (green curve, BYOL-LP; orange curve, BYOL-LPI) after certain phase of
training. (c) linear evaluation protocol on CIFAR10. Different structures of the predictor provide
close performance on the downstream classification task.
至EjOsun
0	50	100	150	200	250	300
Training epochs
——6 = 0.1
—6 = 1
——0 = 10
A = IOO
0	50	100	150	200	250	300
Training epochs
至EjOsun
O 50 IOO 150	200	250	300
Training epochs
β=10
fl=100
0	50	100	150	200	250	300
Training epochs
Q 5 Q
TT
至EjOun
0	50	100	150	200	250	300
Training epochs

(a)	(b)	(c)
Figure 5: The evolution traces of Lalign(qw ◦ fθ, fξ) and Luniform(fθ) in BYOL0 and our proposed
RAFT. (a) Evolution trace of BYOL0 -NP. Increasing β (weight of Lcross-model, regularizer for Lalign)
does not prevent the failed regularization: Lalign converges to 0 quickly. (b) Evolution trace of
RAFT-NP. Small value of β doesn’t effectively regularize Lalign, but increasing the weight helps.
In this respect, RAFT is a more effective regularizer, while the uniformity optimization holds no
huge difference from BYOL0-NP. (c) Evolution trace of RAFT-LP. With the linear predictor, on the
contrary to RAFT-NP, the uniformity is optimized consistently during training, which implies deeper
rationale of the existence of predictor.
16
Under review as a conference paper at ICLR 2021
G Proof of BYOL upper b ounding
In this section, we provide how we derive the upper bound of LBYOL. For the sake of simplicity,
without loss of rigor, we use t1 = t1(x) to represent the transformed input x.
LBYOL = Ex 〜X ,tι 〜Tι,t2 〜T2 [kqw (fθ (tI(X)))- fξ (t2(X))k2]
= E hkqw(fθ(x1)) -qw(fθ(x2))+qw(fθ(x2))-fξ(x2)k22i .
By applying the Cauchy-Schwarz’s inequality to Eq. 12, we yield:
LBYOL ≤ (1 + 1) (E h∣∣qw (fθ (X1))-qw (fθ (X2))∣∣2i + λE h∣∣qw (fθ (x2)) - fξ (X2 升 2i)
=(1 + ∖)(Lalign(qw ◦ fθ; Ppos) + λLcross-model(qw ◦ fθ,fξ, X))
λ
(12)
(13)
which stands for any λ > 0, where the positive-pair distribution Ppos is modeled by the chain rule
of the conditional probability:
Ppos(X1,X2)= X(x) ∙ T1(tl∣x) ∙ T2(t2∣x).
For any given pair α,β > 0, we let λ = β∕ɑ and substitute it back to Eq. 13, yielding
β
αβ
LBYOL ≤ (1 + β ) Lalign (qw ◦ fθ; Ppos) + — LCroSS-model(9w ◦ fθ ,fξ, X)
α
=(α + 耳)[αLalign(qw ◦ fθ; PpoS) + βLcross-model(qw ◦ fθ,fξ, X)]
=(α + J ) LBYOL0,
and as an optimization objective, we have
min( — + β)LBYOL0 ⇔ min LBYOLO
(14)
(15)
Therefore we have proven that LBYOL 0 as optimization objective is the upper bound of LBYOL.
To note here, one can subtract and add a different term fξ(X1) to form the alignment loss on the side
ofMT fξ,
LBYOL = E h∣∣qw(fθ(X1)) - fξ(X1) + fξ(X1) - fξ(X2)∣∣22i ,	(16)
while it doesn’t help to solve the problem since the alignment constraint on the side of MT doesn’t
generate gradients.
17
Under review as a conference paper at ICLR 2021
H Proof of one-to-one correspondence between BYOL0 and RAFT
Theorem (One-to-one correspondence between BYOL0 and RAFT) There is a one-to-one cor-
respondence between parameter trajectories of BYOL0 and RAFT when the following three condi-
tions hold:
i.	the representation space is a hypersphere;
ii.	the predictor is a linear transformation, i.e. qw (∙) = W(∙);
iii.	only the tangential component of the gradient on the hypersphere is preserved.
Without losing generality, suppose that x1 = t1 (x), x2 = t2(x) where x is an arbitrary in-
put and batch size is 1, and (α, β) = (1, 1). We set BYOL0 and RAFT with initial parameters
(θ0, W0) = (θ(0), W(0)) and (θ, W) = (θ(0), -W(0)) respectively. For convenience, we assumes
the dot product “." ignores the row layout or column layout in the chain rule of derivatives and We
define the following symbols:
z2 = fξ (x2), z10 = W 0fθ0 (x1), z20 = W 0fθ0 (x2), z1 = Wfθ(x1), z2 = Wfθ (x2 ). Based on the notations defined, we rewrite the loss terms of BYOL0 and RAFT as follows: LaBlYigOnL0 = z10 -z2022, Lcross-model = || z2 - z2 ^ 2， LaRlAigFnT = z1 -z222, LRAFT	=_Uz _钉||2 Lcross-model = -|z2 - z2 |2 . The two objectives are following:	(17) (18) (19) (20) (21) (22) (23) (24) (25)
LBYOL0 = LaBlYigOnL0 + LcBrYosOs-Lm0 odel, L	LRAFT + LRAFT LRAFT = Lalign + Lcross-model . We claim that under the third condition, the following equations hold: h ∂LBYOL0 i _ h ∂Lraft i h ∂LBYOL0 i __h ∂ LRAFT i ∂~~∂θr~Jk = [ ∂θ k k, [ ∂W0 Jk = - [ ∂W J k, where subscript k denotes the tangential component of the gradient.	(26) (27) (28)
Firstly we show the equivalence with respect to θ. Differentiate LaBlYigOnL0 , LaRlAigFnT with respect to θi0j,
θij respectively, we obtain
∂L∂θj-=2 h(ZI-Z2 )k+(ZI-Z2 )⊥i ∙ (∂zj - ∂⅜!, ∂Lj- = 2 h(ZI-ZGk + (ZI-Z2)」∙ (∂⅛ - ∂⅞), h¥ik=2(ZI-Z2)k ∙ (ft - ¾!, hd⅛Tik =2(Z1 -Z2)k ∙ (∂⅛-∂⅛),	(29) (30) (31) (32)
18
Under review as a conference paper at ICLR 2021
where (z10 - z20 ) , (z1 - z2) are vectors at the points z20 and z2 on the hypersphere and we decompose
the vector into the tangential (denoted by k) and normal component (denoted by ⊥):
(z10 - z20 ) = h(z10 - z20)k + (z10 - z20)⊥i , (z1 - z2) = h(z1 - z2)k + (z1 - z2)⊥i ,	(33)
Generally, suppose z is a unit vector starting at the origin point, which is perpendicular to the unit
hypersphere at the point z, for any vector v starting at the point z, we have
v⊥ = hv, z)∙ z, Vk = V — v⊥ = V — hv, Zi ∙ z.	(34)
Then we can compute the tangential component of the gradient:
(ZI - z2)k = (ZI - z2)- hz1 - z2, z2i ∙ z2
=z1 - hz2, z1 i ∙ z2,
(zi 一 Z2)k =(Z1 一 Z2) 一 hzi 一 Z2,Z2) ∙ Z
=Zi ― hz2,z1i ∙ Z2.	(35)
Because of the initialization, zi = -zi, z2 = -z2, therefore We have
(36)
(37)
So we show that
(zi0 一 z20 )k = 一 (zi 一 z2 )k ,
BYOL0	RAFT
h dLalign i _ h dLalign i
[∂θ0j Li = [~jΓL∣.
(38)
We differentiate LcBrYosOs-Lm0 odel, LcRrAosFsT-model With respect to θi0j , θij respectively, We obtain that
h dLj ik = -2㈢-z2 )∣∙W 0 ∙ df∂j,	(39)
hdLRAFTmodeli =2(司―z2)∣ ∙ W ∙ dfθ(xɪ.	(40)
dθij	k	dθij
Similar to Eq. 35, We derive that
(Z2 一 z2)k = (Z2 一 z2) k	(41)
Since θ0 = θ, dfe，(x2)/dθ% = dfθ(χ2)∕dθj and W0 = —W, we have that
h dLBY%del i = h dLRAFTmodeI i	(42)
|∂θ0j	∂θj	k	()
Therefore by Eq. 38 and Eq. 42, RAFT’s updating of the parameter θ is equal to BYOL0:
h ∂LBYOL0 i _ h ∂Lraft i
[∂θ0 J k = [ ∂θ ]|「
(43)
Also we differentiate LaBlYigOnL0 , LaRlAigFnT with respect to Wi0j , Wij respectively, we obtain that
BYOL0
h -LW- ik=2」)k ∙
RAFT
h 1⅛ ik=2(zLZ2)k∙
(44)
(45)
19
Under review as a conference paper at ICLR 2021
Note that z1 = -z10 , z2 = -z20 and similar to Eq. 35, easy to show that
	(z10 - z20 )k =	-(z1 - z2)k.	(46)
Also,	∂z1 ∂Wτ	∂z1 =∂Wj,	(47)
	∂z2 ∂Wj~	=∂Z2 =∂Wj .	(48)
So we have	h y i	_h ∂ LRAnT i	(49)
	〔∂Wj」k	—-[∂Wj	
Differentiate LcBrYosOs-Lm0 odel , LcRrAosFsT-model with respect to Wi0j , Wij respectively, we obtain that
	∂LBYOL0	∂z0 h-∂Wmdelik = -2(z2 -z2)k ∙ ∂Wj,	(50) RAFT h Frdel ik=2(z2-z2)k∙ ∂W⅛.	(5i)
Then we have that	h ∂LBrYOLmdel i =_h ∂LRAFTmOdel i	(52) 〔∂Wj Ik	〔 ∂Wj	()
By Eq. 49 and Eq. 52, we prove that the cross-model loss of BYOL0 generates the opposite gradient
to RAFT, namely,
h ∂Lbyol0 i _ h ∂LRAFTi
[∂W0 ]|| = -[ ∂W ]|「
(53)
Therefore by the two main conclusions Eq. 43 and Eq. 53, for BYOL0 with parameters (θ0, W0) =
(θ(0), W(0)) and RAFT with parameters (θ, W) = (θ(0), -W(0)) respectively, we have
BYOL:	(θ0(1)= θ0(0)-ηhdLBγOL0i	,W0(1) = W0(0)- ηh¾°L0i	),
∂θ k θ0 =θ0(0)	∂W k W0=W0(0)
(54)
RAFT:	(θ⑴=θ(O)- ηh⅛Ti	,W⑴=W(O)- ηh⅛Ti	). (55)
∂θ kθ=θ(0)	∂W kW=W(0)
We derive that θ(1) = θ0(1), W(1) = -W0(1), and furthermore, θ(k) = θ0(k), W(k) = -W 0(k)
at any iteration k. In this way, we establish an one-to-one correspondence between the parameter
trajectories of BYOL0 and RAFT in training, referred to as H:
H : RAFT(θ,W) 7→BYOL0(θ,-W)	(56)
20
Under review as a conference paper at ICLR 2021
I Non-trivial solutions created by predictor
Suppose inputs x1 = t1 (x) and x2 = t2(x) is n-dimensional. And in linear model, fθ, fξ and qw is
parameterized by matrices (θij)n×n, (ξij)n×n and (Wij)m×m respectively.
The objective is
LBYOL = E(x,tι,t2)〜(X ,Ti ,T2) [∣∣qw(fθ (tI(X)))- fξ (t2(X))II2]
=E(x,ti,t2)〜(X ,T ,T2) [kwθx1 - ξx2k2]
(57)
Differentiate kW θX1 - ξX2 k22 with respect to θij and Wij , we have
∂ [kWθχι- ξχ2k2] =XX ∂ [Wk,"θχι)- ξk,ɑ2]2
dθij	= k=1	dθij
XXinW (A d d 1 d [Wk,"θxl) - ξk,ɑ2]
=2^2[WkXθχι) - ξk,ɑ2]-------θ~.------
k=1	θij
m
= X 2TkWk,:(X1)j
k=1
=2 [(W>T) x>]ij ,	(58)
∂ [kWθxi- ξx2k2] =XX ∂ [Wk,"θχι)- ξk,"2]2
∂Wij	= k=ι	∂Wj
=X 2%(θx1)-ξfd⅛⅛^
m
= X 2Tk(θX1)j1{k=i}
k=1
=2Tiθj,∙,x∖
= 2 [T(θX1)>]ij ,	(59)
where Tk = [Wk,= (θxι) - ξk,Q2], T = (Tι,T2,.. .,Tm')τ = W (θχι) - ξχ2, and Wk,：&,： are the
k-th row of W and ξ respectively.
So
∂ kW θX1 - ξX2k22
∂ kW θX1 - ξX2k22
∂θ
∂W
(60)
Let
dLBYOL _ 0 dLBYOL _ 0
∂θ = , ∂W =,
(61)
we have that
dE(x,t1,t2)〜(X,T1,T2) [kwθx1 - ξx2k2]
∂θ
dE(x,t1,t2)〜(X,T1,T2) [kwθx1 - ξx2k2]
∂W
E(X,tl,t2)~(X ,T1 ,T2 )[
∂kWθX1 - ξX2k22
E(x,t1,t2)~(X,T1 ,T2 )[
∂θ
∂kWθX1 - ξX2k22
∂W
]=0
]=0
(62)
(63)
When the weight of target ξ converge, we have ξ(k) = ξ(k+1) in the updating rule,
ξ(k+1) = τkξk + (1 - τk)θ(k)
θ(k) = ξ(k+1) = ξ(k)
(64)
21
Under review as a conference paper at ICLR 2021
Substituting ξ by θ, we obtain
E(x,tι,t2)〜(X,Tι,T2) [W>(Wθxι - θx2)x>] = 0	(65)
E(x,tι,t2)〜(X,Tι,T2) [(Wθx1 - θx2) x>θ>] =0	(66)
Let E(χ,t1,t2)〜(X,τ1,T2)(χ1χ>) = A, E(χ,t1,t2)〜(X,τ1,T2)(χ2χ>) = B, we have that
eq : sylW> (WθA) = W>θB
W θAθ> = θBθ>
⇒ Wθ - θBA-1 = 0
(67)
To solve Eq. ?? (which is called Sylvester’s equation), we using the Kronecker product notation and
the vectorization operator vec, we can rewrite the equation in the form
(Im 0 W — (BAT)T 0 In) Vecθ = Vec。	(68)
So it has a non-trivial solution θ if and only if (Im 0 W 一(BA-I)T 0 In) has a non-trivial null
space. An equivalent condition to having a non-trivial null space is having zero as an eigenvalue.
Let W has eigenvalues in common with BA-1, then we have a non-trivial solution of θ, which is
exactly the prevention for collapse.
22