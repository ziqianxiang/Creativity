Under review as a conference paper at ICLR 2021
Ensembles of Generative Adversarial
Networks for Disconnected Data
Anonymous authors
Paper under double-blind review
Ab stract
Most computer vision datasets are composed of disconnected sets, such as images
of different objects. We prove that distributions of this type of data cannot be
represented with a continuous generative network without error, independent of the
learning algorithm used. Disconnected datasets can be represented in two ways:
with an ensemble of networks or with a single network using a truncated latent
space. We show that ensembles are more desirable than truncated distributions
for several theoretical and computational reasons. We construct a regularized
optimization problem that rigorously establishes the relationships between a single
continuous GAN, an ensemble of GANs, conditional GANs, and Gaussian Mixture
GANs. The regularization can be computed efficiently, and we show empirically
that our framework has a performance sweet spot that can be found via hyperpa-
rameter tuning. The ensemble framework provides better performance than a single
continuous GAN or cGAN while maintaining fewer total parameters.
1	Introduction
Generative networks, such as generative adversarial networks (GANs) (Goodfellow et al., 2014)
and variational autoencoders (Kingma & Welling, 2013), have shown impressive performance in
generating highly realistic images that were not observed in the training set (Karras et al., 2017;
2019a;b). However, even state of the art generative networks such as BigGAN (Brock et al., 2018)
generate poor quality imagery if conditioned on certain classes of ILSVRC2012 (Russakovsky et al.,
2015). We argue that this is due to the inherent disconnected structure of the data.
In this paper, we theoretically analyze the effects of disconnected data on GAN performance. By
disconnected, we mean that the data points are drawn from an underlying topological space that is
disconnected (the rigorous definition is provided below in Section 3.1). As an intuitive example,
consider the collection of all images of badgers and all images of zebras. These two sets are
disconnected, because images of badgers do not resemble images of zebras, and modeling the space
connecting these sets does not represent real images of animals.
We rigorously prove that one cannot use a single continuous generative network to learn a data
distribution perfectly under the disconnected data model. Because generative networks are continuous,
they cannot map a connected latent space (R') into the disconnected image space, resulting in the
generation of data outside of the true data space. In related work, (Khayatkhoei et al., 2018) has
empirically studied disconnected data but does not formally prove the results in this paper. In addition,
the authors use a completely unsupervised approach to attempt to find the disconnected components
as a part of learning. In contrast, we use class labels and hence work in the supervised learning
regime.
Our suggested approach to best deal with disconnected data is to use ensembles of GANs. We
study GANs in particular for concreteness and because of their widespread application; however,
our methods can be extended to other generative networks with some modification. Ensembles of
GANs are not new, e.g., see (Nguyen et al., 2017; Ghosh et al., 2018; Tolstikhin et al., 2017; Arora
et al., 2017), but there has been limited theoretical study of their properties. We prove that ensembles
can learn the data distribution under the disconnected data assumption and study their relationship
to single GANs. Specifically, we develop a first-of-its-kind theoretic framework that relates single
GANs, ensembles of GANs, conditional GANs, and Gaussian mixture GANs. The framework makes
1
Under review as a conference paper at ICLR 2021
it easy to, e.g., develop regularized GAN ensembles that encourage parameter sharing, which we
show outperform cGANs and single GANs.
While our primary focus here is on theoretical insight, we also conduct a range of experiments to
demonstrate empirically that the performance (measured in terms of FID (Heusel et al., 2017), MSE
to the training set (Metz et al., 2016), Precision, and Recall (Sajjadi et al., 2018)) increases when we
use an ensemble of WGANs over a single WGAN on the CIFAR-10 dataset (Krizhevsky & Hinton,
2009). The performance increase can be explained in terms of three contributing factors: 1) the
ensemble has more parameters and hence has higher capacity to learn complex distributions, 2) the
ensemble better captures the disconnected structure of the data, and 3) parameter sharing among
ensemble networks enables successful joint learning, which we observe can increase performance.
We summarize our contributions as follows:
•	We prove that generative networks, which are continuous functions, cannot learn the data dis-
tribution if the data is disconnected (Section 3.2). The disconnected data model is defined in
Section 3.1, where we argue that it is satisfied in many common datasets, such as MNIST, CIFAR-
10, and ILSVRC2012. Restricting the generator to a disconnected subset of the domain is one
solution (Section 3.3), but we study a better solution: using ensembles.
•	We demonstrate how single GANs and ensembles are related (Section 4.1). We then prove
that ensembles are able to learn the true data distribution under our disconnected data model
(Section 4.2). Finally, we demonstrate that there is an equivalence between ensembles of GANs
and common architectures such as cGANs and GM-GANs due to parameter sharing between
ensemble components (Section 4.3).
•	We empirically show that, in general, an ensemble of GANs outperforms a single GAN (Sec-
tion 5.1). This is true even if we reduce the number of parameters used in an ensemble so that it
has fewer total parameters than a single GAN (Section 5.2). Finally, we empirically show that
parameter sharing among ensemble networks leads to better performance than a single GAN
(Section 5.3) or even a cGAN (Section 5.4).
2	Background and Related Work
2.1	Generative adversarial networks (GANs)
GANs are generative neural networks that use an adversarial loss, typically from another neural
network (Goodfellow et al., 2014). In other words, a GAN consists of two neural networks that
compete against each other. The generator G : R' → Rp is a neural network that generates p-
dimensional images from an '-dimensional latent space. The discriminator D : Rp → p0, 1q is
a neural network which is trained to classify between the training set and generated images. As
compositions of continuous functions (Goodfellow et al., 2016), both G and D are continuous.
G has parameters Θg P RlθGl, where ∣Θg∣ is the possibly infinite cardinality of Θg. Similarly,
D has parameters Θd P RlθD |. The latent, generated, and data distributions are Pz, Pg, and PX,
respectively. We train this network by solving the following optimization problem:
min max V pθG, θDq “ min max Ex„PX rlog Dpxqs ` Ez„Pz rlogp1 ´ DpGpzqqqs.	(1)
θG θD	θG θD
Here we write min and max instead of minimize and maximize for notational compactness, but we
are referring to an optimization problem. The objective of this optimization is to learn the true data
distribution, i.e., PG “ PX. Alternatively, we can use the Wasserstein distance instead of the typical
cross-entropy loss: V pθG, θDq “ Ex„PX Dpxq ´ Ez„Pz DpGpzqq restricted to those θG, θD which
force D to be 1-Lipschitz as done in the WGAN paper (Arjovsky et al., 2017). Thus, we will use V
to denote either of these two objective functions.
2.2	GANs that treat subsets of data differently
Ensembles of GANs. Datasets with many different classes, such as ILSVRC2012 (Russakovsky
et al., 2015), are harder to learn in part because the relationship between classes is difficult to quantify.
Some models, such as AC-GANs (Odena et al., 2017), tackle this complexity by training different
2
Under review as a conference paper at ICLR 2021
models on different classes of data in a supervised fashion. In the AC-GAN paper, the authors train
100 GANs on the 1000 classes of ILSVRC2012. The need for these ensembles is not theoretically
studied or justified beyond their intuitive usefulness.
Several ensembles of GANs have been studied in the unsupervised setting, where the modes or
disconnected subsets of the latent space are typically learned (Pandeva & Schubert, 2019; Hoang
et al., 2018; Khayatkhoei et al., 2018) with some information theoretic regularization as done in (Chen
et al., 2016). These are unsupervised approaches which we do not study in this paper. Models such
as SGAN (Chavdarova & Fleuret, 2018) and standard GAN ensembles (Wang et al., 2016) use
several GANs in part to increase the capacity or expressibility of GANs. Other ensembles, such as
Dropout-GAN (Mordido et al., 2018), help increase robustness of the generative network.
Conditional GANs (cGANs). Conditional GANs (Mirza & Osindero, 2014) attempt to solve the
optimization problem in (1) by conditioning on the class y, a one-hot vector. The generator and
discriminator both take y as an additional input. This conditioning can be implemented by having the
latent variable be part of the input, e.g., the input to the generator will be rzT yTsT instead of just z.
Typically, conventional cGANs have the following architecture modification. The first layer has an
additive bias that depends on the class vector y and the rest is the same. For example, consider a
multilayer perceptron, with matrix W in the first layer. Converting this network to be conditional
would result in the following modification to the matrix in the first layer:
xx
Wconditional 彳=[W Bs 彳 = Wx + By = Wx + B∙,k.
Hence, We can think of B as a matrix with columns B∙,k,k p{1,...,K } being bias vectors and W
being the same as before. We pick a bias vector B.,k based on what class we are conditioning on but
the other parameters of the network are held the same, independent of k. This is done to both the
generator and the discriminator. Some cGANs condition on multiple layers, such as BigGAN (Brock
et al., 2018), or on different types of layers, such as convolutional layers, but our formulation here
extends clearly to those other architectures.
Gaussian Mixture GANs (GM-GANs). The latent distribution Pz is typically chosen to be either
uniform, isotropic Gaussian, or truncated isotropic Gaussian (Goodfellow et al., 2014; Radford
et al., 2015; Brock et al., 2018). We are not restricted to these distributions; research has been
conducted in extending and studying the affect of using different distributions, such as a mixture of
Gaussians (Ben-Yosef & Weinshall, 2018; Gurumurthy et al., 2017).
3	Continuous generative networks cannot model distributions
DRAWN FROM DISCONNECTED DATA
3.1	Disconnected data model
We begin by introducing a new data model that accounts for disconnected data. Typical datasets with
class labels satisfy this model; we provide additional examples below.
Definition 1 (Disconnected data model). We assume that the data lies on K disjoint, compact
sets Xk U Rp,k p {1,...,K} so that the whole data lies on the disjoint union of each component:
L)K=I Xk = X. Moreover, we assume that each component Xk is connected (Rudin, 1964). We then
draw data points from these sets in order to construct our finite datasets.
In Definition 1, we let each Xk be compact in order to remove the degenerate case of having two
components Xk and Xj that are arbitrarily close to one another, which is possible if we only assume
that X is closed and disjoint. If that is the case, there are trivial counter-examples (see the appendix)
to the theorems proved below.
Lemma 1. X is a disconnected set, and Xj is disconnected from Xk for j ‰ k.
Disconnected datasets are ubiquitous in machine learning (Khayatkhoei et al., 2018; Hoang et al.,
2018; Pandeva & Schubert, 2019). For example, datasets with discrete labels (typical in classification
problems) will often be disconnected. We study this disconnected data property, because generative
networks are unable to learn the distribution supported on such a dataset, as we show below.
3
Under review as a conference paper at ICLR 2021
3.2	Continuous generative networks cannot represent a disconnected data
DISTRIBUTION EXACTLY
In this section, we prove that, under Definition 1, continuous generative networks cannot learn the
true data distribution exactly due to model misspecification.
Suppose that (Ω, F, PZ) is a probability space with Pz being the distribution of the random vector
Z : Ω → R`. We assume that Pz is equivalent to the LebeSgUe measure λ. This just means that
λpzpA)) “ 0 if and only if PzpA) “ 0 for any set A P F. This is true for a Gaussian distribution,
for example, which is commonly used as a latent distribution in GANs (Arjovsky et al., 2017). The
transformed (via the generative network G) random vector X “ G o Z : Ω → Rp is determined by
the original probability measure Pz but is defined on the induced probability space (Ω1, F 1,Pg).
Theorem 1. If G can generate from multiple components of X (say X1 and X2), then the probability
of generating samples outside of X is positive: PG(X P Rp∖X) > 0. Otherwise if we can only
generate from one component (say X1), then PGpXi) “ 0 for x P t2, . . . , Ku.
The continuity of G is the fundamental reason why Theorem 1 is true. A continuous function cannot
map a connected space to a disconnected space. This means that all generative networks must
generate samples outside of the dataset if the data satisfies Definition 1.
Suppose that our data is generated from the true random vector Xdata : Ω1 → Rp using the probability
distribution PX. Also, suppose that we learn PG by training a generative network.
Corollary 1. Under Definition 1, we have that d(Pg, PX) > 0 for any distance metric d and any
learned distribution PG.
From Corollary 1, we see that learning the data distribution will incur irreducible error under
Definition 1 because our data model and the model that we are trying to train do not match. Hence,
we need to change which models we consider when we train in order to best reflect the structure of
our data. At first thought a discontinuous G might be considered, but that would require training G
without backpropagation. Instead, we focus on restricting G to a discontinuous domain (Section 3.3)
and training an ensemble of GANs (Section 4.1) as two possible solutions.
3.3	Restricting the generator to a disconnected subset of the latent
DISTRIBUTION
In this section, we study how we can remove the irreducible error in Theorem 1 from our models
after training. Suppose that we train a generator G on some data so that G(R') ɔ X. Therefore, we
can actually generate points from the true data distribution. We know that the distributions cannot be
equal because of Theorem 1, implying that if we restrict the domain of G to the set Z “ GT(X)
then G(Z) “ X. The next theorem shows how the latent distribution is related to restricting the
domain of G.
Theorem 2 (Truncating the latent space reduces error). Suppose that Pz (Z) > 0 and let the
generator G learn a proportionally correct distribution over X. In other words, there exists a real
number c P R so that
PG(A) “ CPX(A)	A P F 1,A U X.
Then, we use the truncated latent distribution defined by PzT (B) “ 0 for all B P F that satisfy
B X Z “ H. This allows us to learn the data distribution exactly, i.e.
PG|Z(A) “ PX (A)	APF1.
We write PG|Z because, by truncating the latent distribution, we effectively restrict G to the domain
Z. Theorem 2 shows that if we learn the data distribution approximately by learning a proportional
distribution, then we can learn the true data distribution by truncating our latent distribution. By 4.22
in (Rudin, 1964), Z must be disconnected, which implies that a disconnected latent distribution is a
solution to remove the irreducible error in Theorem 1.
Although Theorem 2 suggests that we truncate the latent distribution, there are several limitations
with this approach. First, the latent distribution cannot be truncated without knowing a closed form
expression for PG. Second, we may learn the disconnected set Z by training a mixture distribution
for Pz as is done in (Ben-Yosef & Weinshall, 2018; Gurumurthy et al., 2017). The problem with
4
Under review as a conference paper at ICLR 2021
this is that the geometric shape of Z is restricted to be spherical or hyperellipsoidal. Third, before
truncating the latent space, we need to train a generative network to proportionally learn the data
distribution, which is impossible to confirm.
Given these limitations, we introduce the use of ensembles of generative networks in Section 4.1.
This class of models addresses the issues above as follows. First, we will not need to have access
to PG in any way before or after training. Second, knowing the geometric shape of Z is no longer
an issue because each network in the ensemble is trained on the connected set Xk instead of the
disconnected whole X. Finally, since the k-th network will only need to learn the distribution of Xk,
we reduce the complexity of the learned distribution and do not have to confirm that the distribution
learned is proportionally correct.
4 Ensembles of GANs and parameter sharing
We demonstrate how to train ensembles of GANs practically and relate ensembles to single GANs,
cGANs, and GM-GANs. We focus on feedforward (Goodfellow et al., 2016) GANs in this section
for concreteness; therefore, we study an ensemble of discriminators as well as generators.
4.1 Ensemble of GANs vs. a single GAN
Given an ensemble of GANs, We will write Gk : R' → Rp as the k-th generator with parameters
θGk P RlθGl for k P {1,..., K}, where K is the number of ensemble networks. We assume that each
of the generators has the same architecture, hence ∣Θg∕ “ |。3；for all i,j; thus we drop the subscript
and write ∣Θg∣. Likewise, we write Dk : Rp → [0,1] for the k-th discriminator with parameters
θDk P RlθDl since the discriminators all have the same architecture. The latent distribution is the
same for each ensemble network: Pz. The generated distributions will be denoted PGk .
For concreteness, we assume that K is the number of classes in the data; for MNIST, CIFAR-10, and
ILSVRC2012, K would be 10, 10, and 1000, respectively. If K is unknown, then an unsupervised
approach (Hoang et al., 2018; Khayatkhoei et al., 2018) can be used. Define the parameter π P RK
such that XK“i ∏k = 1. We then draw a one-hot vector y 〜Cat(π) randomly and generate a sample
using the k-th generator if the k-th component of y is 1. Hence, we have that a generated sample is
given by x “ Gkpzq. This ensemble of GANs is trained by solving
mθinmθaxVpθGk,θDkq	(2)
for k P t1, . . . , Ku. Note that with an ensemble like this, the overall generated distribution PGpxq “
XkK“1 πkPGk pxq is a mixture of the ensemble distributions. This makes comparing a single GAN to
an ensemble challenging; for example, consider comparing a Gaussian to a mixture of Gaussians.
In order to compare a single GAN to an ensemble of GANs, we define a new hybrid optimization
min
θG1 ,...,θGK
KK
θ	max	X VpθGk , θDkq s.t. X }θDj ´ θDk}0 W t
θD1 ,...,θDK k“1	k“1
j“k
K
s.t. X }θGj ´ θGk}0 W t,
k“1
j“k
(3)
where } ∙ }o “ 1 denotes the 'o "norm," which counts the number of non-zero values in a vector.
Thus, t20 serves as a value indicating how many parameters are the same across different networks,
which is more general than having tied weights between networks (Ghosh et al., 2018). We penalize
the parameters because it is convenient, although it is not equivalent to, penalizing the functions
themselves. This is true because Θg% — θGj “ 0 =^ Gk — Gj “ 0 but the converse is not true.
We analyze the behavior of (3) as we vary t in the next theorem.
Theorem 3. Let G and D be the generator and discriminator network in a GAN. Suppose that for
k P t1, . . . , Ku we have that Gk and Dk have the same architectures as G and D, respectively.
Moreover, assume that PXpXjq “ PXpXkq for all j, k. Then,
K K´1 ∣Θg∣}
i) Suppose that temax
Then for all k P t1, . . . , Ku we have
that (θG fc, θDj is a solution to(3) if and only if (θG fc, θDj is a solution to (2).
5
Under review as a conference paper at ICLR 2021
ii) Suppose that t = 0. Then we have that (θG, θD) is a solution to (3)foreach k p {1,..., K}
ifand only if (θG, θD) is a solution to (1).
Informally, Theorem 3 shows that, when t “ 0, we essentially have a single GAN, because all of
the networks in the ensemble have the same parameters. If t is large then we have an unconstrained
problem such that the ensemble resembles the one in Equation (2). Therefore, this hybrid optimization
problem trades off the parameter sharing between ensemble components in a way that allows us to
compare performance of single GANs with ensembles.
Unfortunately, Equation (3) is a combinatorial optimization problem and is computationally in-
tractable. Experimentally, we relax Equation (3) to the following
(K	k	ʌ K
maχ £ VPθGk, θDk)´ λ £ }θDj ´ θDk}ι + λ £ }θGj ´ θGk }ι	(4)
θDι,…,θDκ k“1	k“1	k“1
j“k	j“k
in order to promote parameter sharing and have an almost everywhere differentiable regularization
term that we can backpropagate through while training. Although Equation (4) is a relaxation
of Equation (3), we still have the same asymptotic behavior when we vary λ as when we vary t as
shown in Appendix A.
4.2 Optimality of ensembles fo GANs
This next theorem shows that if we are able to learn each component’s distribution, PXk, then an
ensemble can learn the whole data distribution PX .
Theorem 4.	Suppose that G* is the network that generates Xk for each k P {1,...,K}, i.e.
Pg* = PXk. Under Definition 1, we can learn each GZ by solving (2) with V being the objective
k
function in Equation (1).
We know from (Goodfellow et al., 2014) that a globally optimal solution is achieved when the
distribution of the generated images equals PX . Hence, this theorem has an important consequence:
Training an ensemble of networks is optimal under our current data model.
It is important to note that the condition “Gk is the network that generates Xk” is necessary but not
too strong because we may have a distribution that cannot be learned by a generative network or
that our network does not have enough capacity to learn. We do not care about such cases however,
because we are studying the behavior of generative networks under Definition 1.
4.3 Relation of ensembles of GANs to other GAN architectures
Relation to cGANs. We compare a cGAN to an ensemble of GANs. Recall from Section 2.2 that
a cGAN has parameters θG and θD that do not change with different labels but there are matrices BG
and BD that do depend on the labels. Specifically, they solve the optimization problem Theorem 3
with the additional constraint that the only parameters that can be different are the biases in the first
layer. For other variants of cGANs a similar result applies.
Theorem 5.	A cGAN is equivalent to an ensemble of GANs with parameter sharing among all
parameters except for the biases in the first layer. Moreover, the optimization in (3) can be modified
so that it is equivalent to the cGAN optimization problem.
Relation to GM-GANs. Another generative network that is related to ensembles is the GM-GAN.
The first layer in GM-GANs transforms the latent distribution from isotropic Gaussian into a mixture
of Gaussians. This new layer plays a similar role as the B.,k in the CGAN comparison above,
meaning that GM-GANs solve the optimization problem (3) with the additional constraint that the
only parameters that can be different are the parameters in the first layer.
Theorem 6.	A GM-GAN is equivalent to an ensemble of GANs with parameter sharing among all
parameters except for the first layer. Moreover, the optimization in (3) can be modified so that it is
equivalent to the GM-GAN optimization problem.
6
Under review as a conference paper at ICLR 2021
FID	Average MSE Precision	Recall
Figure 1: Ensembles of WGANs with fewer total parameters than a single WGAN perform better on CIFAR-10.
We do not have to sacrifice computation to achieve better performance, we just need models that capture the
underlying structure of the data. The dotted red line is the baseline WGAN, the solid blue line is the equivalent
ensemble, and the dashed black line is the full ensemble.
5 Experimental Results
In this section, we study how ensembles of WGAN (Arjovsky et al., 2017) compare with a single
WGAN and a conditional WGAN. We use code from the authors’ official repository (Arjovsky et al.,
2018) to train the baseline model. We modified this code to implement our ensembles of GANs and
cGAN. For evaluating performance, we use the FID score (Heusel et al., 2017; 2020), average MSE
to the training data (Metz et al., 2016; Lipton & Tripathi, 2017), and precision/recall (Sajjadi et al.,
2018; 2019). More details about the experimental setup are discussed in Appendix C.
5.1	Ensembles perform better than single networks
We consider a basic ensemble of WGANs where we simply copy over the WGAN architecture
10 times and train each network on the corresponding class of CIFAR-10; we call this the “full
ensemble”. We compare this ensemble to the baseline WGAN trained on CIFAR-10.
Figure 1 shows that the full ensemble of WGANs performs better than the single WGAN. It is not
immediately clear, however, whether this boost in performance is due to the functional difference of
having an ensemble or if it is happening because the ensemble has more parameters. The ensemble
has 10 times more parameters than the single WGAN, so the comparison is hard to make. Thus, we
consider constraining the ensemble so that it has fewer parameters than the single WGAN.
5.2	Ensembles with fewer total parameters still outperform a single network
The “equivalent ensemble” (3,120,040 total generator parameters) in Figure 1 still outperforms the
single WGAN (3,476,704 generator parameters) showing that the performance increase comes from
using the ensemble rather than just having larger capacity. In other words, considering ensembles
of GANs allows for improved performance even though the ensemble is simpler than the original
network in terms of number of parameters.
We see a performance boost as a result of increasing the number of parameters, in Figure 1. Therefore,
we perform better because of having a better model (an ensemble) as well as by having more
parameters. Now, we investigate a way that we can further improve performance.
5.3	Parameter sharing among ensemble components leads to better
PERFORMANCE
We study how the regularization penalty λ affects performance. As discussed in Section 4.1, we can
learn a model that is somewhere between an ensemble and a single network by using `1 regularization.
In Figure 2, the performance increases when we increase λ in the equivalent ensemble from 0 to
0.001, implying that there is some benefit to regularization. Recall that by having λ > 0, we force
parameter sharing between generator and discriminator networks. This performance increase is likely
data dependent and has to do with the structure of the underlying data X. For example, we can have
pictures of badgers pX1q and zebras pX2q in our dataset and they are disconnected. However, the
7
Under review as a conference paper at ICLR 2021
Average MSE
FID
epoch
epoch	epoch	epoch
Figure 2: Ensembles of WGANs have a performance sweet spot when we regularize the optimization problem
in expression (4) with different values of λ. Each curve is calculated using the equivalent ensemble of WGANs
discussed in Section 5.2. We see that as we increase λ to 0.001, the performance increases but then decreases
when we continue to increase λ to 0.01. This implies that there is an optimal value for λ that can be found via
hyperparameter tuning. The solid blue line is the equivalent ensemble with λ “ 0.01, the dotted red line is the
equivalent ensemble WGAN, and the dashed black line is the equivalent ensemble with λ “ 0.001.
FID	Average MSE Precision
Recall
Figure 3: Regularized ensembles of WGANs using the optimization in (4) outperform cWGANs, even though
cGANs are a type of ensemble. Here, cWGAN actually performs similarly to the baseline WGAN even though it
takes into consideration class information. The solid blue line is the baseline, the dotted red line is the cWGAN,
and the dashed black line is the equivalent ensemble with λ “ 0.001.
backgrounds of these images are likely similar so that there is some benefit in G1 and G2 treating
these images similarly, if only to remove the background.
As we increase λ from 0.001 to 0.01 we notice that performance decreases. This means that there is
a sweet spot and We may be able to find an optimal 0 < λ* < 0.01 via hyperparameter tuning. We
know that the performance is not monotonic with respect to λ because it decreases and then increases
again; in other words, the performance has a minima that is not at λ = 0 or λ → 8. The optimization
problem in expression (4) therefore can be used to find a better ensemble than the equivalent ensemble
used in Section 5.2 which still has fewer parameters than the baseline WGAN.
5.4	Ensembles outperform cGANs
We modify a WGAN to be conditional and call it cWGAN. This cWGAN is trained on CIFAR-10,
and we compare cWGAN to ensembles of WGANs. We do this because we showed in Section 4.3
that cGANs are an ensemble of GANs with a specific type of parameter sharing.
As can be seen from Figure 3, ensembles perform better than the cWGAN. The baseline WGAN
model actually performs similarly to the cWGAN, which implies that the conditioning is not helping
in this specific case. We hypothesize that our model (λ “ 0.001) performs better because there are
more parameters that are free in the optimization, instead of just the bias in the first layer. Thus,
although cGANs are widely used, an ensemble with the regularization described in Section 4.1 can
outperform them because the ensemble captures the disconnected structure of the data better.
References
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
8
Under review as a conference paper at ICLR 2021
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. https://github.
com/martinarjovsky/WassersteinGAN, 2018. Accessed: June 3, 2020.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In International Conference on Machine Learning (ICML),
pp. 224-232. JMLR. org, 2017.
Matan Ben-Yosef and Daphna Weinshall. Gaussian mixture generative adversarial networks for
diverse datasets, and the unsupervised clustering of images. arXiv preprint arXiv:1808.10356,
2018.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Tatjana Chavdarova and Frangois Fleuret. SGAN: An alternative training of generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 9407-9415, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems (NIPS), pp. 2172-2180, 2016.
Arnab Ghosh, Viveka Kulharia, Vinay P Namboodiri, Philip HS Torr, and Puneet K Dokania. Multi-
Agent diverse generative adversarial networks. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 8513-8521, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NIPS), pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu. DeLiGAN: Genera-
tive adversarial networks for diverse and limited data. In Proceedings of the IEEE conference on
computer vision and pattern recognition (CVPR), pp. 166-174, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems (NIPS), pp. 6626-6637, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Two
time-scale update rule for training GANs. https://github.com/bioinf-jku/TTUR,
2020. Accessed: June 3, 2020.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. MGAN: Training generative adversarial
nets with multiple generators. In International Conference on Learning Representations (ICLR).
OpenReview, 2018.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of StyleGAN. arXiv preprint arXiv:1912.04958, 2019b.
Mahyar Khayatkhoei, Maneesh K Singh, and Ahmed Elgammal. Disconnected manifold learning for
generative adversarial networks. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 7343-7353, 2018.
9
Under review as a conference paper at ICLR 2021
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Master’s thesis, University of Toronto, 2009.
Yixing Lao. Pytorch-reversegan. https://github.com/yxlao/reverse-gan.pytorch,
2017. Accessed: June 3, 2020.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Zachary C Lipton and Subarna Tripathi. Precise recovery of latent vectors from generative adversarial
networks. arXiv preprint arXiv:1702.04782, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Gongalo Mordido, Haojin Yang, and Christoph MeineL DroPoUt-GAN: Learning from a dynamic
ensemble of discriminators. arXiv preprint arXiv:1807.11346, 2018.
Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial nets. In
Advances in Neural Information Processing Systems (NIPS), pp. 2670-2680, 2017.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary
classifier GANs. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2642-2651. JMLR. org, 2017.
Teodora Pandeva and Matthias Schubert. MMGAN: Generative adversarial networks for multi-modal
distributions. arXiv preprint arXiv:1911.06663, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Walter Rudin. Principles of mathematical analysis. McGraw-hill New York, third edition, 1964.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing
generative models via precision and recall. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 5228-5237, 2018.
Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. As-
sessing generative models via precision and recall. https://github.com/msmsajjadi/
precision-recall-distributions, 2019. Accessed: June 3, 2020.
Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. AdaGAN: Boosting generative models. In Advances in Neural Information Processing
Systems (NIPS), pp. 5424-5433, 2017.
Yaxing Wang, Lichao Zhang, and Joost Van De Weijer. Ensembles of generative adversarial networks.
arXiv preprint arXiv:1612.00991, 2016.
10
Under review as a conference paper at ICLR 2021
A Proofs
We first show that under Definition 1, our data is disconnected.
Proof of Lemma 1. Since Xj X Xk “ Xj- X Xk = 0, We see that X is disconnected.	口
Remark 1. Note that if our data is disconnected, it doesn’t necessarily follow Definition 1. This
means that Definition 1 is a stronger condition than just having disconnected data. This can be
seen by the following counter-example. We denote a truncated Gaussian as N(μ, σ2)∣s, where the
distribution is non-zero on S . Let
PX = 2PXI ' 2PX2 = 2N(0, 1)|( — 8,0) + 2N(0, 1)|(0,8)
be the true distributions. Note that p´8, 0q and p0, 8q are disconnected but do not follow Definition 1
because they are not compact. Moreover, we can learn this distribution easily by letting G be the
identity function and having Pz = N(0, 1q; it is trivial to show that this results in PG = PX. Hence,
disconnected data is too weak of an assumption—we need there to be a non-zero distance between
our disconnected sets and that is what Definition 1 captures.
Proof of Theorem 1. Without loss of generality, we can assume that G can generate at least from
the two components Xi and X2. We define B = GT(X) and note that G(B) must be disconnected
because G can generate from at least X1 and X2 and they are disconnected. Since, X is disconnected
and closed in Rp, we have that B is disconnected and closed in R' because G is continuous (Theorem
4.8 and 4.22 in (Rudin, 1964)). Since B is closed in R', this means that R'∖B is an open set.
Moreover, R`∖B is not empty because we know that R' is connected and B is not. We also know
that the Lebesgue measure λ of a nonempty, open set is positive, hence we have that
λ(R'∖B) > 0.
Since λ is equivalent to Pz, we have that Pz(Z P R'∖B) > 0. Thus,
PG(X P Rp∖X) = Pz(z P GT(RpzX)) “ Pz(z P R'∖B)〉0,
as desired.	口
Proof of Corollary 1. Since the data lies only on X, we know that PX (xdata P RzX) = 0 for any
valid probability measure. However, we have that PG(x P RpzX) > 0. Hence, d(Pg, Pχ) > 0 for
any metric d.	口
Proof of Theorem 2. The truncated latent distribution is denoted PzT and is defined as
PzT (B) =
Pz(B X Z)
Pz(Z)
for any set B P F. Hence, we have that
PGIZ(A)= PzT (GT(A))
=Pz(GT(A)X Z)
=~PZ(Z)
=万片Pz(GT(A)X GT(X))
Pz(Z)
=ɪʌ Pz (G´1(A X X))
Pz(Z)
=启PG(A X X)
c
=E PX (A X X)
c
“F PX(A)
11
Under review as a conference paper at ICLR 2021
for any A P F1. The last equality is true because PXpXq “ 1, so that any points outside of X have
zero probability. For the result above, set A “ Rp to see that c “ Pz pZq, implying that
PG|ZpAq “ PX pAq	APF1,
as desired.	口
Proof of Theorem 3. First We prove i). If temax
on (3) are unnecessary so that the problem reduces to
K K´1 ∣Θg∣}
then the constraints
KK
min max VpθGk,θDkq “ min	maxVpθGk,θDkq
θG1,...,θGK θD1,...,θDK k“1	θG1,...,θGK k“1 θDk
K
“ minmaxVpθGk,θDkq,
k“1 θGk θDk
Which is equivalent to solving the optimization problem
minmaxVpθGk,θDkq,	k P t1, . . . ,Ku.
Thus, i) is shoWn.
Next, We prove ii). Suppose that t “ 0. Note that given a distribution PX, We can restrict it to each
component Xk and normalize to get the restricted distributions
PXk pAq “
PX (A)
PX (Xk q
for each A P F1 and each k. Since We assume that PX (Xjq “ PX(Xkq for all j, k and that
Pχ(X) “ XK=I Pχ(Xkq = 1, we see that Pχ(Xk) “ K for each k. This implies that for any
measurable function f : Rp → R, we have that
K
Ex„PXk
k“1
rf (χ)s “ q f
k“1 xPXk
f(xqPXk (dxq
“K
∑ L
k“1 xPXk
f (xqPX (dxq
“ K	f (xqPX (dxq
卜ince X “ U Xk)
“ KEx„PX rf(xqs .
Suppose that V is the standard cross entropy objective function. We will use the notation
V (θG, θD; Pq to show that we are evaluating V (θG, θDq with the data distribution P. Then, we see
that
KK
∑ V (Θg, θd ； PXkq “ ∑ Eχ~pχk riog D(χ)s ` Ez„Pz riog(i ´ D(G(zqqqs
k“1	k“1
“KEx„PX rlog D(xqs ` KEz„Pz rlog(1 ´ D(G(zqqqs
“ KV(θG, θD; PXq.
Similarly, if V is the Wasserstein objective function, then
KK
∑ v (Θg, θd ； PXk q “ ∑ Eχ~pχk rD(χqs ´ Ez„Pz rD(G(zqqs
k“1	k“1
“ KEx„PXrD(xqs ´ KEz„Pz rD(G(zqqs
“ KV (Θg, Θd ； PX q.
12
Under review as a conference paper at ICLR 2021
This means that
KK
Amin I	max	£ V pθGk , θDk ； PXkq s.t. £ }θDj ´ θDk}0 “ 0
θG1,...,θGK	θD1,...,θDK k“1	k“1
j“k
K
s.t. £
k“1
j“k
}θGj ´ θGk}0
“0
K
“ min max £ V(Θg, Θd; PXk)
θG θD
k“1
“ min max KVpθG, θD; PXq,
θG θD
which is equivalent to the optimization problem in (1), as desired.
□
Theorem 7. Let G and D be the generator and discriminator network in a GAN. Suppose that for
k P t1, . . . , K u we have that Gk and Dk have the same architectures as G and D, respectively.
Moreover, assume that PXpXjq “ PXpXkq for all j, k. Then,
i)	Suppose that λ = 0. Thenfor all k P {1,...,K} we have that (θGk, θDj is a solution
to (4) if and only if (θGfc, ®Dj is a solution to (2).
ii)	Suppose that λ → 8. Then we have that (θG, θD) is a solution to (4) for each k P
{1,..., KU ifand only if (θG, θD) is a solution to (1).
Proof of Theorem 7. First we prove i). If λ “ 0 then we have that
KK
λ £ }θDj ´ θDk}1 “ λ £ }θGj ´ θGk}1 “ 0
k“1	k“1
j“k	j“k
on (4). Hence, the problem reduces to the unconstrained problem of (2).
Next, we prove ii). Since λ → 8, any solution where θDk ‰ θDj or θGk ‰ θGj for all j, k P
t1, . . . , Ku is suboptimal. Consequently, it means that the optimization problem in (4) reduces to
min
θG1 ,...,θGK
KK
θ maχ £ V PθGk, θDk ； PXk)´ λ £ }θDj ´ ΘdJ∣i
θD1,...,θDK k“1	k“1
j“k
K
+λ £ }θGj
k“1
j“k
´ θGk}1
“ min max KVpθG, θD; PX),
θG θD
which is equivalent to the optimization problem in (1). We mainly just outline the proof here because
it is so similar to the proof of Theorem 3.	口
Proof of Theorem 4. Note that PX is the total data distribution and that PXk is the distribution of
each disconnected set. This means that
K
PX “ £ πkPXk
k“1
for some mixture coefficients α* > 0 so that XK“1 αk = 1.
Fix an arbitrary k p {1,..., K}. Since Pg* “ PXk, we have that
k
min max V (θg=k, ΘdJ “ min max Eχ~PχJlog Dk (x)] + Ez~Pz[log(1 — Dk (Gk(Z)))S
has a solution of PG* “ PXk from Theorem 1 of (Goodfellow et al., 2014). Since this is true for
k
every k and since PX “ XK“ ∏k PXk, We learn the complete data distribution.	口
13
Under review as a conference paper at ICLR 2021
Proof of Theorem 5. First we show that ensembles are equivalent to cGANs under the right condi-
tions.
Fix the architecture of the networks considered and only focus on the generator. We want to show
that cGANs and ensembles are equivalent, so we first focus on the generators in cGANs. We define
the set of functions which represent conditional versions of the fixed architecture as
GpKq “	Gθ,B
:R' x{1,...,K u→ Rp : B, θ are network parameters
,
where B is a matrix with K columns and whose rows depend on the width of the first hidden layer
as discussed in Section 2.1. The rest of the parameters are represented as the vector θ above. It is
clear that for a function in GpKq, there could be many corresponding networks; some of these are
due to activation and weight symmetries (Bishop, 2006). This implicitly create an equivalence class
of networks; in particular, two networks are equivalent if they are the same mapping, regardless of
weights. These symmetries do not affect our argument but it a subtlety to keep in mind.
Obviously not every ensemble is the same as the generator in a cGAN; however, we focus on a very
specific type of ensemble. We define the set of all ensembles that have a variable bias in the first layer
as
GE (Kq = {(Gθ,bk
:R' → Rp)K“1 ：
bk, θ are network parameters for each k
)
which is a collection of K-tuples of functions that represent networks with our fixed architecture.
In the definition above, for j ‰ k, we see that Gθ,bj and Gθ,bk share the same parameter θ, but
the biases bj and bk may be different. Ensembles that are used in the construction of GE (Kq are
essentially ensembles that have parameter sharing everywhere except in the bias term of the first
layer; these biases are not constrained to be similar at all.
Since a network that induces a function in G (Kq is the conditional version of the networks in the
ensembles that are used to construct GE(Kq, as described in Section 2.1, then parameter equality
implies functional equality. In other words, for Gθ,rb1,...,bK s P G(Kq and (Gθ,bk qkK“1 P GE(Kq we
have that Ge,5,…,bκS (z, kq = Gθ,bk (Z) for all Z P R', k p {1,...,K} and all parameters θ, bk.
We will now show that there exists a one-to-one correspondence between these two sets. Suppose
that T : GE (Kq → G(Kq is defined by
(T '(Gθ,bj )K“1)) (z,k) = Gθ,bk (z) = Gθ,[bι,...,bκ s(z,k)
for each Z P R' and k p {1,..., K}. Informally, we map an ensemble to a single network by just
picking the k-th network in the ensemble. For a fixed θ and b1, . . . , bK we see that T (Gθ,bj qjK“1 is
indeed a function from R' x{1,...,K} to Rp. Moreover, T((Ge,bj)K“1) is equal to (as a function)
to Gθ,rb1,...,bK s P G(Kq so that T is well defined.
Let Gθ,B P G(Kq be given. Then we just let b1, . . . , bK be the columns of B and we see that
T (Gθ,bj qjK“1 = Gθ,bk implies thatT is surjective. Next suppose that Gθα,Bα = Gθβ,Bβ are
functions in G(Kq with Bα = rb1α , . . . , bαK s and Bβ = rb1β , . . . , bβK s. Then clearly we have that for
(Gea,ba)K“1 and &◎ &)K_1 in GE(K) that
Gθα,bkα (Zq = Gθα,Bα (Z, kq = Gθβ,Bβ (Z, kq = Gθβ,bβ (Zq,
implying that T is injective.
Thus, T is a one-to-one correspondence between GE(Kq and G(Kq. Hence, for every ensemble of
networks defined above, we can find a cGAN which is equivalent to the ensemble. This equivalence
is defined as the equivalence of the functions induced by these networks. The above result holds for
any architecture and all K P Z'. Thus, it also holds for the discriminator networks.
Next, we want to show that a modified version of the optimization problem from (3) yields the cGAN
optimization problem.
14
Under review as a conference paper at ICLR 2021
We begin with the generic optimization problem from (3) and see that it can be rewritten as
KK
min I max Σ VPθGk , θDk q s.t. Σ }θDj ´ θDk }o & t
θG1,...,θGK	θD1,...,θDK k“1	k“1
j“k
min
θG1,...,θGK
min
θG1,...,θGK
max	V pθGk , θDkq s.t. CD s.t. CG
θD1,...,θDK k“1	k k
QmaxDK Σ V (I(BGkk,J, „(BDqk,J) s.t. CD)
K
s.t. Σ }θGj
k“1
j“k
´ θGk }0 ≤ t
s.t. CG,
where We simply use the name CD for the constraint XK“1 }θDj∙ 一。口卜}o ≤ t and similarly for CG.
j“k
This is purely for notational convenience. Likewise, We simply denote θGk as [(θjk)τ (BG)TksT
and similarly for θDk , for each k. Keep in mind that BG and BD are matrices such that the k-th
column is the the bias of the first layer of the k-th network in the ensemble. So far, we have only
introduced notational changes.
Consider what happens if we change the constraints to
K
CD1 “ Σ }θD1 j ´ θD1 k }0 “ 0
k“1
j“k
K
CG “ ∑ }θGj ´ θGk }o = 0.
k“1
j“k
We have that BG and BD are unconstrained and that θG1 is forced to be equal to θG1 for all k and j .
Similarly θD1 “ θD1 for all k and j . Hence, we can say that the optimization problem above with
the new constraint is
min
θG1 ,...,θGK
QmaxDK S V (I(BG)k,ll(BDk∙,J)
k“1
s.t. CD1
s.t. CG1
K	θG	θD
min max V	,	,
,…月GK θDι ,...,θDκ /-jl ∖ (BG)∙,k	(BD)∙,k )
∕√ - -- -j
which is equivalent to the cGAN optimization problem. Here, we just define θG to be shorthand for
any one of the θGk vectors, since they are all the same.
Hence, a cGAN is equivalent to solving the ensemble optimization problem in (3) with a modified
constraint.
□
Proof of Theorem 6. The proof for this is very similar to the proof for Theorem 5.	□
B Estimation of ensemble parameters
In Section 4.1, we assume that k „ pk is a multinomial distribution of degree K parameters: πi for
i “ 1, . . . , K. Using the maximum likelihood estimator (Bishop, 2006) we obtain
1N
∏MLE = N ∑ i(yj “ i)
N j“1
for i = t1, . . . , Ku. For datasets like MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky &
Hinton, 2009), k is a uniformly distributed random variable. For others one may have to calculate pk
based on class imbalances.
15
Under review as a conference paper at ICLR 2021
C Experimental details
In this section we describe the details of our experiments.
C.1 Performance measures
We use FID (Heusel et al., 2017), average MSE (Metz et al., 2016), precision, and recall (Sajjadi
et al., 2018) to evaluate our models.
For FID, precision, and recall we use the official repositories (Heusel et al., 2020; Sajjadi et al., 2019).
For each of these methods, we compare a set of generated images to a set of images from the training
set. For the FID calculation, we use the precalculated statistics for CIFAR-10 and compare to 10,000
generated images from our trained networks. For precision and recall, we compare 10,000 generated
images to 10,000 images in the training set. All other parameters are left the same.
For the average MSE calculation, we use the algorithm introduced in (Lipton & Tripathi, 2017),
which was empirically shown to work 100% of the time on DCGAN architecture, such as WGAN.
We modified the code in (Lao, 2017) so that it can be run with multiple restarts if desired. We ran our
experiments with 1000 iterations and 5 restarts. We ran the code on 100 training images.
C.2 Baseline model
For the baseline model, we ran the default WGAN code for 1000 epochs on CIFAR-10. All other
parameters are left at their default values.
C.3 Full ensemble
To create the full ensemble, we just copied over the baseline model 10 times and trained each network
pair pGk, Dkq in the ensemble on a single class of CIFAR-10. The training also lasted for 1000
epochs. This is equivalent to solving the optimization problem in (2).
C.4 Equivalent ensemble
Normally WGAN is trained with the following two architecture parameters: ngf = 64 and ndf = 64.
However, to get 10% of the parameters we trained each ensemble component with ngf = 15 and ndf =
20. The depth of the generator and discriminator in the equivalent ensemble are the same as in the
single WGAN, however, we modify the width of each corresponding layer so that the total parameters
are fewer in the ensemble than in the single WGAN. Specifically, the generator of the WGAN has
3, 576, 704 parameters and each generator of the equivalent ensemble has 312, 004 parameters. The
discriminator of the WGAN has 2, 765, 568 parameters and each discriminator of the equivalent
ensemble has 272, 880 parameters. Reducing the width of each layer is not necessarily the optimal
way to reducing parameters in a network. We do this because it is easy and effective, not because we
are trying to reduce parameters in an optimal way, which is out of the scope of this paper. This is
equivalent to solving the optimization problem in (2).
C.5 Regularized ensembles
For all the ensembles with λ > 0, we use the equivalent ensemble architecture, while solving (4).
C.6 The cGAN model
For this architecture, we modify the baseline architecture and concatenate the class label, represented
as a one-hot vector, to the input of the generator and discriminator networks.
16