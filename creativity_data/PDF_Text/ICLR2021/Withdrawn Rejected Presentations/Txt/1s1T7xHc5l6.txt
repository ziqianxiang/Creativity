Under review as a conference paper at ICLR 2021
FILTRA: Rethinking Steerable CNN by Filter
Transform
Anonymous authors
Paper under double-blind review
Ab stract
Steerable CNN imposes the prior knowledge of transformation invariance or
equivariance in the network architecture to enhance the the network robustness
on geometry transformation of data and reduce overfitting. Filter transform has
been an intuitive and widely used technique to construct steerable CNN in the past
decades. Recently, group representation theory is used to analyze steerable CNN
and reveals the function space structure of a steerable kernel function. However, it
is not yet clear on how this theory is related to the filter transform technique. In this
paper, we show that kernel constructed by filter transform can also be interpreted
in the group representation theory. Meanwhile, we show that filter transformed
kernels can be used to convolve input/output features in different group represen-
tation. This interpretation help complete the puzzle of steerable CNN theory and
provides a novel and simple approach to implement steerable convolution opera-
tors. Experiments are executed on multiple datasets to verify the feasibilty of the
proposed approach.
1	Introduction
Beyond the well-known property of equivariance under translation, there has been substantial recent
interest in CNN architectures that are equivariant with respect to other transformation groups, e.g.
reflection and rotation. Applications of such architectures range over scenarios where object orien-
tation might variate, including OCR, aerial image processing, 3D point cloud processing, medical
image processing, texture analysis and etc.
Previous works on constructing equivariant CNN can be coarsely categorized as two aspects. The
first aspect designs special steerable filters so that the convolutional output is hard-baked to trans-
form accordingly when the input reflects or rotates. A plenty of works develop this idea by filter
rotation, including hand-crafted filters (Oyallon & Mallat, 2015) and learned filters (Laptev et al.,
2016; Zhou et al., 2017; Cheng et al., 2018; Marcos et al., 2017). TI-Pooling (Laptev et al., 2016)
produce invariant output as input rotates. ORN (Zhou et al., 2017) and RotDCF (Cheng et al., 2018)
produces output which circularly shifted as input rotates. Since each dimension of such permutable
output corresponds to a uniformly discrete rotation angle, RotEqNet (Marcos et al., 2017) propose
to extract rotation angle from the permutable features. Another approach to construct steerable fil-
ters is to linearly combine a set of steerable bases. These basis can be solved in discrete function
space (Cohen & Welling, 2014; 2016) or continuous function space (Worrall et al., 2017; Weiler
& Cesa, 2019). Weiler & Cesa (2019) comprehensively summarize works on steerable bases using
polar Fourier basis.
The second aspect exploits specific transforms to act on input. Spatial Transformer Network (STN)
is a well-known representative, which predicts an affine matrix to transform its input to the canonical
form. Tai et al. (2019) inherits this idea to design equivariant network. Another choice of transform
is to the polar coordinate system (Henriques & Vedaldi, 2017; Esteves et al., 2018). Since 2D
rotation in Cartesian coordinate system corresponds to 2D translation in polar coordinate system,
rotation equivariance can be achieved by conventional translation equivariant CNN.
The approach proposed in this paper falls into the first category. Weiler & Cesa (2019) proves that
all steerable convolutional operator could be denoted as the combination of a specific set of polar
Fourier bases. However, it is not clear yet how this interpretation is related with the widely used
filter transform scheme. In this paper, we aim to establish the missing connection between the
1
Under review as a conference paper at ICLR 2021
group representation based analysis for steerable filters and filter transform scheme. To this end, we
propose a new approach (FILTRA) to use filter transform to establish steerability between features in
different group representation in cyclic group CN and dihedral group DN . We verify the feasibility
of FILTRA for the classification and regression tasks on different datasets.
2	Preliminaries
We make use of several NumPy or SciPy functions in equations including roll1, flipud2 and
Circulant3. We omit the variable in bracket sometimes by writing κ* = κ*(g) and K* = K*(φ).
2.1	Steerable CNN
We recapitulate the basic concepts of steerable
CNN which will be frequently used in this pa-
per. For detailed introduction, readers can refer
to Weiler & Cesa (2019) for a comprehensive
information. We mainly consider the 2D im-
age case and denote x ∈ R2 as a pixel coordi-
nate. We use vector field f (x) ∈ RC to denote
a general multi-channel image, where C is the
number of channels. Typical examples of f(x)
include RGB image f (x) ∈ R3 and gradient
image f(x) ∈ R2. Consider a group G of trans-
formations and an element g ∈ G. Examples
f (X)	n(g) ∙ f = ρ(g)f(g-1x)
ρ≡1
g
ψ1
g
of G include rotation, translation and flip. A
vector field f(x) follows the below rules when
undergoing the act π(g) of a group element g:
∏(g) ∙ f = ρ(g)f (g-1x),	(1)
where ρ(g) is a group representation related to
vector field f . Fig. 1 shows an example of dif-
ferent types of ρ for RGB images and gradient
images under a rotation transform element g.
Figure 1: Examples of images (feature maps) with
different group representation ρ. Both images un-
dergo 90deg rotation. The upper row is an RGB
image whose 3-channel colors remain the same
when the image is rotated. The lower row is a gra-
dient image whose two channel value should be
rotated in the same way when the gradient image
The group representation of RGB is ρ(g) ≡ 1
is rotated.
while for gradient image ρ(g) is a 2D rotation matrix which also rotates vector f(x) by g.
ρ
In the scenario of convolutional neural network, a convolution operator f → K ∙ f is considered as
steerable if it satisfies
K ∙ [π1(g)f] = π2(g)[κ ∙ f],	⑵
i	.e. the output vector field transforms equivariantly under g when the input is transformed by g.
2.2	Reflection Group, Cyclic Group and Dihedral Group
We consider steerable filters on reflection group ({±1}, *), cyclic group CN and dihedral group
DN = ({±1}, *) n CN. To unify the notations in derivation, We interpret CN = ({1}, *) n CN and
({±1}, *) = ({±1}, *) n Ci = Di so that a element in these three groups can always be denoted
as a pair g = (i0, i1), whose range is Z2 × Z1 for reflection group, Z1 × ZN for cyclic group and
Z X ZN for dihedral group. Each element in CN corresponds to rotation angle θ^ = 2Nπ.
2.3	Group Representation
A linear representation ρ of a group G on a vector space Rn is a group homomorphism from G to
the general linear group GL(n), denoted as
ρ : G 7→ GL(n) s.t. ρ(gg0) = ρ(g)ρ(g0), ∀g, g0 ∈ G.	(3)
1https://numpy.org/doc/stable/reference/generated/numpy.roll.html
2https://numpy.org/doc/stable/reference/generated/numpy.flipud.html
3https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.circulant.html
2
Under review as a conference paper at ICLR 2021
We consider three types of linear representation in this paper, i.e. trivial representation, regular
representation and irreducible representation (irrep). Readers can refer to Serre (1977) for further
background for these concepts.
The trivial representation of a group element is always ρtri(g) ≡ 1. The regular representation of a
finite group G acts on a vector space R|G| by permuting its axis. Therefore, for a rotation element
g = (0,i1) ∈ CN orDN,weget
ρrCegN (g) = P(i1),	ρrDegN (g) = P(0i1) P(0i ) , where P(i1) = roll(IN,i1,0).	(4)
For a reflected element g = (1, i1) ∈ DN, we get
ρrDegN(g) = B(0i ) B(0i1) , where B(i1) = flipud(P (-i1 - 1)).	(5)
By selecting suitable change of basis of the vector space, a representation can be converted to a
equivalent representation, which is the direct sum of several independent representations on the
orthogonal subspaces. A representation is called irreducible representation if no non-trivial decom-
position exists. This conversion is denoted as
ρ(g) = Q L(i0,i1)∈I ψi(g) Q-1,	(6)
where I is an index set specifying the irreducible repsentations ψi and Q is the change of basis.
2.4	Decomposing Regular Representation
We decompose the regular representation into a set of irreps. Define the following base irrep
ψj,k(i0, i1)
f((-1)j )i0
J (-1)i1 ∙((-1)j)i0
I cos(kθiι)	— sin(kθiι)"∣ Γl
I sin(kθiι)	cos(kθiι) _| |_0
0
(—1)i0
•((—1)j)i0
k=0
k = N ,N is even
otherwise
(7)
where j, k are referred as the reflection and rotation frequency of the irrep. Concretely, if the action
g reflects/rotates an object once, ψj,k (g) will reflects/rotates in vector space j/k times. We also
define the following discrete cosine transform basis
V = [β0 βι	…βbNNc],
I 1n
where βk = J [cos(kθ0) cos(kθ1) …cos(kθN-1)]
I cos(kθo) cos(kθι)…CoS(kθ N-ι)
[sin(kθo) sin(kθι)…sin(kθN-ι)
The following decomposition for ρrCegN (0, i1) holds
ρrCegN (g) = VDCN V>,	DCN = M ψ0,k(0, i1).	(9)
0≤k≤b NN C
k=0
k = NN, N is even ⑻
.
otherwise
The decomposition for ρrDegN (i0, i1) holds in a bit more complicated form, i.e.
ρrDegN (i0,i1) =WDDNW>,
where W = VV —VV , DDN = M	ψj,k(i0,i1),
L	」	0≤j≤1,0≤k≤b N c
(10)
and each column of W is refered by βj,k = [β>	( —1)j β>]>. See Fig. 2 for a visualization of this
decomposition.
We also mention a property of βk that is easy to verify and will be useful in our derivation.
ψ0,k(0,i1)βk> = βk>P(i1), ψ1,k(0,i1)βk> =βk>P(i1),	(11a)
ψo,k (1,iι)β> = β>B(iι), Ψι,k(1,iι)β> = —β>B(iι),	(11b)
where ψ0,k(i0, i1) rotates column vectors of βk> as if they are circularly shifted.
3
Under review as a conference paper at ICLR 2021
ρrDegN (g)	W	DDN (g)	W>	ρrDegN (g)	W	DDN (g) W>
Figure 2: Illustrations for (10) for g = (0, 1) at left and g = (1, 1) at right. Red, light yellow and
green denotes negative, 0 and positive values, respectively.
2.5	Harmonic Filters
Weiler et al. (2018) proposes the condition of a filter kernel κ to be equivariant under the action
g ∈ G.
Lemma 1. The map f → K ∙ f is equivariant under G ifand only iffor all g ∈ G,
κ(gx) = ρout(g)κ(x)ρin(g)-1.	(12)
Weiler & Cesa (2019) proves that such filters can be denoted by a series of harmonic bases b(φ), i.e.
κ(r, φ) =	ωb(r)b(φ),	(13)
b∈K
where ωb(r) is the per radial weights and K is a set of harmonic bases as dervied in the appendix of
Weiler & Cesa (2019). For example, consider ρin = ψi,m and ρout = ψj,n in DN,
Kψj,m-ψi,n = {bμ,γ,s(φ) = ψ(μφ)ξ(s)∣μ = m - sn, s ∈ {±1} .	(14)
3	Main Results
(12) and (13) provide a general approach to verify and construct steerable CNN with different rep-
resentations. In this section, we relate these theories with filter transform and show how to use filter
transform to construct steerable filters with input/output of different representations.
For readers who are not interested in group theory and mathematical derivation of the theory con-
nection, we highlight the key equations to construct steerable filters in boxes. It should not be
difficult to implement steerable filters directly from these equations using any modern deep learning
framework. Fig. 3 shows illustration for these equations.
In our derivation, we mainly consider the angular coordinate of polar coordinate functions κ(r, φ)
and write them κ(φ). We will also frequently make use of the following property:
κ(φ - θ0) = κ(φ+ θ0),	κ(φ - θi) = κ(φ+ θN-i).	(15)
3.1 From Trivial Representation to Regular Representation
Rotation Group CN Consider the the rotating filter K and its reflected version K which are Com-
monly used in previous works, e.g. TI-Pooling, ORN, RotEqNet and RotDCF:
K(φ) =	[κ0	K1	…	KN-1]> ,	Kn(φ)	= κ(φ - θn),
K(φ) =	[κ0	K1	…	KN-1]>,	Kn(φ)	= κ(θn - φ).
(16)
The output of convolution with the above kernels naturally permutes as the input rotates in CN .
This intuitively corresponds to property of a steerable filter transforming from trivial representation
to regular representation. In this paper, We use K and K as the basic filters to construct different types
of steerable filters in CN and DN. We verify the observation of the above steerability by substituting
K into the lhs of Lemma 1 with g = (0, 1) and write:
K(φ + θι)	=	[κ(φ	+ θι)	κ0	…	KN-2]> =	[kn-1	k0	…	KN-2]>	(17a)
= ρrCegN (0, 1)Kρtri(0, 1)-1.	(17b)
4
Under review as a conference paper at ICLR 2021
K0D→Nreg
KjD,kN→reg
reg = K
KkC→N reg	diag(K)βk0
diag(K)βk1
-diag(K)β0
— diag(K)β1
K
Figure 3: Visualization of FILTRA filter examples. Based on a same weight kernel K, we generate
filters K0C→Nreg, K0D→Nreg, KkC→N reg and KjD,kN→reg. In this example we set j = 1, k = 1, N = 8. The
two-columns of matrix βk is splitted as βk0 and βk1 for visualization. Red, light yellow and green
denotes negative, 0 and positive values, respectively. Please view this figure in color.
The above equation can be similarly verified for other g = (0, i1) and also on K. Thus WLOG We
select the steerable filter which transforms trivial representation to regular representation on CN as
(18)
Dihedral Group DN The steerable filter that transforms trivial representation to regular represen-
tation on DN can be constructed as
KDNreg (Φ)= [K>	K],
(19)
Which corresponds to enumerating each DN element and act on the kernel κ. Forg = (0, i1), K0D→Nreg
can be verified to folloW (12) in the same Way as (17a), i.e. K0D→Nreg(φ+θ) = ρrDegN (g)K0D→Nregρtri(g)-1.
For reflected action, When g = (1, 1), We Write:
K(-φ + θι)	=	[κ(-φ + θι)	κ(-φ -	θ0)	κ(-φ -	θι)	…	κ(-φ -	Θn-2)]>
=[κ1 K0 KN-1	…κ2]> = B(1)K.	()
Similarly, We can shoW for g = (1, i1),
K(-φ + θiι ) = B(iι)K,	K(-φ + θiι ) = B (iι)K.	(21)
Thus We verify (12) for the reflected actions g = (1, i1) by summarizing the above as K0D→Nreg(-φ +
θi1 ) = ρrDegN (g)K0D→Nregρtri(g)-1.
3.2	From Irrep to Regular Representation
Rotation Group CN Consider a CN irrep ψ0,k (g) With frequency (0, k). We shoW that the fol-
loWing kernel
KC→ reg = diag(K)βk,	(22)
transforms from ψ0,k (g) to regular representation for the action g = (0, i1). The derivation of
correctness can be found in the appendix.
5
Under review as a conference paper at ICLR 2021
Dihedral Group DN Consider a DN irrep ψj,k(i0, i1) with frequency (j, k). We show that the
following kernel:
KDN→reg= hKC→reg>	(-1尸示出reg1	(23)
transforms from ψj,k(i0, i1) to regular representation for the action g = (i0, i1) ∈ DN.
3.3	From Regular Representation to Regular Representation
Regular representation possesses a nice property that it can be averaged, pooled or activated channel-
wise without violating steerability (Weiler & Cesa, 2019). Thus it is convenient to used regular
representation for the intermediate features of a steerable CNN. We show in this subsection that the
following kernels can be use to construct a steerable kernel whose input and output features are both
in regular representation.
Rotation Group CN
(24)
Dihedral Group DN
KDg→reg = [KD,N→reg …KDNNC→reg	KDN→reg …KDNNC→reg] W - 1.	(25)
The above two kernels can be verified to transform regular representation to regular representation
in similar way and we show the derivation for the CN case (25) as an example in the appendix.
3.4	Reversed Transform of Representations
It is obvious to find that for (12), if ρin, ρout are orthogonal matrices, i.e. ρi-n 1 = ρi>n , ρo-u1t = ρo>ut,
the transpose of (12) naturally proves the equivariance of κ> under a reversed representation trans-
form direction, i.e. from ρout to ρin. Thus we can easily obtain equivariance kernel from regular
representation to trivial/irreducible representation by simply transposing (18), (19), (22) and (23).
3.5	Conventional Rotating Filters
We comprehensively study the approach to use filter rotation to form steerable convolutional kernels
with regular representation features as input or output. Conventional filter rotation based networks
exploit some basic forms introduced in this section. TI-Pooling (Laptev et al., 2016) exploits kernel
KCN to transform trivial to regular representation, executes orientation pooling to convert regular to
trivial representation and loses orientation information. RotDCF and ORN exploits a kernel of form
KOCRNN = circulant(K).	(26)
It is easy to verify that KOCRNN also follows Lemma 1 to be a steerable filter. However, compared to
KCg→reg, KCRN consumes Same filter storage but has less weight capacity (N v.s. N bNC). RotEqNet
constructs 2D vector field which could rotate as its input rotates but regards the 2D vector field
as independent trivial representation in convolution. As shown in this paper, it preserves better
steerability to regards the vector field as irrep representation with frequency 1.
3.6	Numerical Accuracy for Discrete Kernels
Note that when implementing discrete convolution, the equality of (17a) does not perfectly hold.
For example, consider κn(φ) = κ(φ - θn), κn (θn) = κ(0) holds for a continuous κ. However,
for discrete κ, κn(φ) is a rotated interpolation of κ(φ) and this equality does not precisely hold
in general. There exist some exceptions where the equality can be achieved for discrete κ. One
example is when κn(φ) is a 90° rotation of K and it can be precisely constructed from κ. Another
example is when Kn is a 45° rotation interpolated by nearest pixel from a K of size 3 X 3.
6
Under review as a conference paper at ICLR 2021
layer	k	s	output	δt (FIL)	δt (R2)
conv+relu	5	1	128 (reg)	0.12	0.17
conv+relu	5	1	192 (reg)	0.13	0.13
pool	3	2	256 (reg)	-	-
conv+relu	5	1	256 (reg)	0.13	0.13
conv+relu	5	1	384 (reg)	0.23	0.23
pool	3	2	384 (reg)	-	-
conv+relu	5	1	512 (reg)	0.32	0.48
conv+relu	5	1	768 (reg)	0.62	0.91
pool	3	2	768 (reg)	-	-
(a) The backbone network structure used in our experiments
is composed by convolution, ReLU and pooling layers. The
convolution layers are realized by FILTRA, R2Conv and con-
ventional convolution respectively while the rest layers re-
main the same. Three realizations have the same number of
output channels in each layer but organize the channels to be
follow regular representation for FILTRA and R2Conv. k:
kernel size. s: stride. δt: filter generation time in ms.
layer	k	s	output
GroupPool	-	1	24 (reg)
fc+relu	-	-	16 (reg)
fc	-	-	10 (tri)
(b)	The classification head network struc-
ture used in our experiments uses a Group-
ing Pooling (Weiler & Cesa, 2019) to gener-
ate transform invariant features.
layer	k	s	output
PAMaxPool	-	-	24 (reg)
conv+relu	1	1	16 (reg)
conv	1	1	2 (irrep)
(c)	The regression head network struc-
ture used in our experiments uses a
PointwiseAdaptiveMaxPool (PAMax-
Pool) (Weiler & Cesa, 2019) to summarize
feature in regular representation.
Table 1: Network structure in experiments
3.7 Steerable CNN with Multiple Layers
A conventional CNN is usually composed convolution, pooling, nonlinearity and fully-connected
layers. To achieve equivariance for the overall network, it is desired that all the component layers
are steerable. As analyzed in the appendix of Weiler & Cesa (2019), channel-wise nonlinearity and
channel-wise pooling preserves the steerability on feature maps with regular representation. fully-
connected layers is a special case of convolution with 1 × 1 kernels and thus can be easily realized
by steerable convolution.
4	Experiments
The proposed equivariant convolution, refered as FILTRA, can be interpreted as an alternative for-
mulation for the harmonic based (Weiler & Cesa, 2019) implementation of steerable convolution.
In this section we show the pros and cons of each implementation by experiments. We make use
of the framework E2CNN (Weiler & Cesa, 2019) for our experiments as it provides the general
interface and operations for steerable CNN network. Experiments are executed on the MNIST, KM-
NIST (Clanuwat et al., 2018), FashionMNIST (Xiao et al., 2017), EMNIST (Cohen et al., 2017) and
CIFAR10 datasets.
We compare FILTRA against two convolution operations, i.e. the representative harmonic based
convolution R2Conv (Weiler & Cesa, 2019) from E2CNN and the conventional vanilla convolution.
All MNIST-like datasets are experimented on a same feature extraction backbone as described in
Table 1a, with convolution operator realized by the three experimented approaches. CIFAR10 is
experimented with WideResNet (Zagoruyko & Komodakis, 2016) in the setting similar to Weiler &
Cesa (2019). We found that on CIFAR10, C4 steerable network performs better than C8 for both
approaches. For all experiments, we randomly rotate or reflect according to the experiment settings.
The settings and evaluation results are listed in Table 2. Different from Weiler et al. (2018), we force
the three convolution kernels to output same number of channels. For example, compared to vanilla
convolution, the number of free weights for a C8 FILTRA is reduced to 1/8 and for a D8 is reduced
to 1/16. The filters for all the approaches will thus have exactly same shape at the deploy stage.
Experiments are executed on GTX 2070. The training procedure of FILTRA and R2Conv can both
be implemented as a vanilla convolution plus a filter generation step. For C8 case the runtime of
both generator is similar and for D8 case FILTRA is slightly faster. We show runtime of D8 case in
Table 1a at training stage. R2Conv additionally requires a initialization of about 2 min. Both of the
approaches consume the same inference time as of vanilla convolution.
7
Under review as a conference paper at ICLR 2021
Tasks		Classification (acc)				Regression (angle err deg)			
	-mnist-	kmnist	fmnist	emnist	Cifar10	mnist	kmnist	fmnist	emnist
Aug	-S^^O	-S^^O	-S^^O	-S^^O	Wrn Wrn	SO	S O	SO	-S~~O
Net eqiv	C8	D8	C8	D8	C8	D8	C8	D8	C4	D4	C8	D8	c8d8	C8 D8	C8 D8
FILTRA	98.9 98.1	97.1 97.0	90.5 90.8	77.1 80.5	93.4 92.8	3.3 5.4	3.2 3.6	2.6 2.8	29.8 24.9
R2Conv	98.8 98.1	97.3 96.8	90.5 90.8	76.7 80.1	93.6 92.7	4.8 8.9	3.4 4.5	2.9 3.7	34.5 29.2
Conv	98.5 98.0	96.4 95.2	89.3 88.3	72.6 80.1	93.2	-	6.6 10.6	4.8 6.4	3.1 3.6	37.4 25.5
Table 2: Performance on MNIST and CIFAR10. S: randomly augmented over SO(2). O: randomly
augmented over O(2). wrn: WideResNet. Zagoruyko & Komodakis (2016).
4.1	Classification Task
The most typical experiment used in previous works on conventional steerable CNN is the clas-
sification task. We follow this convention and compare the classification performance of the ex-
perimented three approaches in Table 2. FILTRA show comparable performance to R2Conv and
slightly improves accuracy for OCR-like (*MNIST) tasks where high frequency texture is limited.
On CIFAR10, the performance of FILTRA is minorly disadvantageous. The explanation comes in
the interpolation artifacts mentioned in Subsect. 3.6. As the interpolation of high frequency compo-
nents deviates more, this harms the performance on CIFAR10 with high frequency texture.
4.2	Regression Task
Besides the typical classification task, we find that the property of steerability is naturally advan-
tageous for many regression tasks whose input might rotate or reflect. In this paper, we evaluate
the regression performance with an example task to predict the character direction. Similar tasks
are commonly used in OCR techniques. When the character rotates, the predicted direction should
rotate with the same rotating frequency. This means the predicted 2D direction vector is following a
irrep ψ0,1 for CN. We reuse the backbone in Table 1a to extract features and use a regression head
in Table 1c to predict a unit 2D vector denoting the direction. The network is trained with MSE loss.
Note that the images should be masked by a disk to avoid the network to overfit the direction from
rotated black boundary. Different approaches are evaluated by the mean included angle between the
predicted and groundtruth directions as shown in Table 2. FILTRA with C8 steerability performs
best when trained on data augmented over SO(2). We owe this to the fact that FILTRA weight is
naturally organized by the discrete grid layout. Each element of discrete weight matrix contribute to
one more DoF of the filters. In contrast, R2Conv uses filters parameterized with a polar coordinate.
The DoF of the filters is slightly reduced due to the discretization.
5	Conclusions
In this paper, we establish the connection between the recent steerable CNN structure based on group
representation theory and the conventional transformed filters. To this end, we propose an approach
to construct steerable convolution filters, which transform between features in trival, irreducible and
regular representations. We verify the feasibility of FILTRA for the classification and regression
tasks on several datasets.
References
Xiuyuan Cheng, Qiang Qiu, Robert Calderbank, and Guillermo Sapiro. Rotdcf: Decomposition of
convolutional filters for rotation-equivariant deep networks. arXiv preprint arXiv:1805.06846,
2018.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
8
Under review as a conference paper at ICLR 2021
Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups.
In International Conference on Machine Learning, pp. 1755-1763, 2014.
Taco S Cohen and Max Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016.
Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar transformer
networks. In International Conference on Learning Representations, 2018.
Joao F Henriques and Andrea Vedaldi. Warped convolutions: Efficient invariance to spatial trans-
formations. In International Conference on Machine Learning, pp. 1461-1469. PMLR, 2017.
Dmitry Laptev, Nikolay Savinov, Joachim M Buhmann, and Marc Pollefeys. Ti-pooling:
transformation-invariant pooling for feature learning in convolutional neural networks. In Pro-
ceedings of the IEEE conference on computer vision and pattern recognition, pp. 289-297, 2016.
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field
networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5048-
5057, 2017.
EdoUard Oyallon and StePhane Mallat. Deep roto-translation scattering for object classification.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2865-
2873, 2015.
Jean-Pierre Serre. Linear representations of finite groups, volUme 42. Springer, 1977.
Kai Sheng Tai, Peter Bailis, and Gregory Valiant. EqUivariant transformer networks. In International
Conference on Machine Learning (ICML), 2019.
MaUrice Weiler and Gabriele Cesa. General e (2)-eqUivariant steerable cnns. In Advances in Neural
Information Processing Systems, pp. 14334-14345, 2019.
MaUrice Weiler, Mario Geiger, Max Welling, WoUter Boomsma, and Taco S Cohen. 3d steerable
cnns: Learning rotationally eqUivariant featUres in volUmetric data. In Advances in Neural Infor-
mation Processing Systems, pp. 10381-10392, 2018.
Daniel E Worrall, Stephan J Garbin, Daniyar TUrmUkhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation eqUivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Han Xiao, Kashif RasUl, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Sergey ZagorUyko and Nikos Komodakis. Wide residUal networks. arXiv preprint
arXiv:1605.07146, 2016.
Yanzhao ZhoU, Qixiang Ye, Qiang QiU, and Jianbin Jiao. Oriented response networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 519-528, 2017.
9