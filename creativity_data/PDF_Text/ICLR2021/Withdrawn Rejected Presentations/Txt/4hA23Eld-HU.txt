Under review as a conference paper at ICLR 2021
Learning to Control on the Fly
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes an algorithm which learns to control on the fly. The pro-
posed algorithm has no access to the transition law of the environment, which is
actually linear with bounded random noise, and learns to make decisions directly
online without training phases or sub-optimal policies as the initial input. Neither
estimating the system parameters nor the value functions online, the proposed al-
gorithm adapts the ellipsoid method into the online decision making setting. By
adding linear constraints when the feasibility of the decision variable is violated,
the volume of the decision variable domain can be collapsed and we upper bound
the number of online linear constraints needed for the convergence of the state to
be around the desired state under the bounded random state noise. The algorithm
is also proved to be of constant bounded online regret given certain range of the
bound of the random noise.
1 Introduction
To regret deeply is to live afresh. - Henry David Thoreau
Differently from the optimal control, which knows the system dynamics, hence has the privilege
to stand at the beginning of the time and optimize the cumulative cost up to the terminal time as a
function of the control action sequence (Stengel, 1994), (Camacho & Alba, 2013), differently from
reinforcement learning (Montague, 1999), which has the training phase to learn the state-action
value function, also differently from other online control work which requires a stable policy as
the initial input to the algorithm (Dean et al., 2018), (Abbasi-Yadkori et al., 2019), the proposed
algorithm in this paper does not know the transition law of the environment, which is actually linear
with bounded random noise, and do not have any initial stabilizing policy as input, and can only
learn to make decisions directly online. The algorithms neither estimate the system parameters, nor
the value functions.
Only knowing the alignment relationship between the agent’s sensors and actuators and an upper
bound of the noise, with the perfect observer of the state, we would like to design the pure learning
to control on the fly algorithms which can learn to steer the state of the agent to converge around the
desired state asymptotically with theoretical guarantees.
Related Work:
Online learning (Cesa-Bianchi & Lugosi, 2006) portrays optimization as a process. Such view has
been applied in many practical applications, where it is necessary and beneficial to learn and opti-
mize from experience as more aspects of the problem are observed (Hazan, 2019). Recent studies
have also considered the theory of online optimization with stochastic or cumulative constraints (Yu
et al., 2017), (Yuan & Lamperski, 2018). The ellipsoid method (Bland et al., 1981), which is usually
used for black-box represented convex optimizations, has also been adapted for online learning in
Yang et al. (2009). However, the above works can all be considered as a special case of our learning
to control on the fly problem in stateless systems (A = 0 in equation 1) (Agarwal et al., 2019a).
The potential link between online learning to reinforcement learning and control has been studied
recently in Cheng et al. (2019) and Wagener et al. (2019). In both works, online learning is managed
to be fit into the framework of reinforcement learning or model predictive control, to help design
new reinforcement learning, or control algorithms. In contrast, in our work, we would like to adopt
the online learning perspective to solve the same decision making problem: to purely learn to make

Under review as a conference paper at ICLR 2021
decisions from online interacting experience as more aspects of the system can be inferred from the
online noisy data without any training phase or parameter tuning.
Recently, there has been a renewed interest in learning linear dynamical systems online in the ma-
chine learning literature (Arora et al., 2018), (Hazan et al., 2018), (Fazel et al., 2018), (Hazan
et al., 2017). Beside, some works also study the online control problem with the guaranteed re-
gret bound, by either assuming known dynamics but adversarially changing loss functions (Agarwal
et al., 2019b), or assuming unknown dynamics, but initial sub-optimal policies. The stochastic noise
with normal distribution is usually considered in these works.
More specifically, Abbasi-Yadkori & Szepesvari (2011) constructs a high-probability confidence set
around the system parameters based on online least-squares estimation, and derives the regret bound
around O(√T) for the first time for the linear quadratic control problem. However its implemen-
tation requires solving a non-convex optimization problem to precision O(T-1/2), which can be
computationally intractable. Dean et al. (2018) proposes the first polynomial-time algorithm for the
adaptive LQR problem that provides high probability guarantees of sub-linear regret. However, in
proving the regret upper bound, a stable initial policy is assumed to be given as input.
Instead of the interplay between regret minimization and parameter estimation online, model-free
approaches for reinforcement learning (RL) is applied online in Abbasi-Yadkori et al. (2019) to solve
2
the linear quadratic control problem with regret upper boundO(T3) proved. Least-squares temporal
difference learning Tu & Recht (2017) is used to approximate the state-action value functions online.
However, these algorithms also require a stable policy as input.
Major Contributions:
1)	To the best of our knowledge, the proposed algorithm is the first learning to control on the fly
algorithm with a theoretical proof in terms of convergence and online regret that neither uses any
information about the transition law of the environment, nor owns a sub-optimal policy as input, and
requires neither parameter tuning nor training phase before testing the algorithm.
2)	The algorithm is analyzed for the case where the environment is subject to bounded random
noise. We propose the algorithm which adapts the idea from the ellipsoid method. By adding linear
constraints when the feasibility of the decision variable is violated, we can collapse the volume of
the decision variable domain and prove the upper bound of the number of times of activating the
separation oracle before convergence of the state to be around the desired state. We also show that
the algorithm can be of constant regret given a certain range of the noise bound.
2 Preliminaries
The control theory is built on a strong assumption of the system dynamics model
xt+1 = Axt + But + wt+1,	(1)
where the state xt ∈ Rn, the agent’s action ut ∈ Rm , the state transition matrix A ∈ Rn×n, the
state-action alignment matrix B ∈ Rn×m . wt+1 corresponds to the bounded random state noise, in
other words, ||wt+1 ||2 ≤ . The action ut is parameterized as
ut = Ktxt = Φtkt ,	(2)
where the control gain Kt ∈ Rm×n, Φt ，XT 0 Im, 0 denotes the kronecker product, kt，
VeC(Kt), where vec(∙) is the vector operator.
Now if A is unknown to us, B is known to us, and we do not have a training chance to collect any
data offline and can only observe the noisy data of xt online directly, and if we do not know any
policy beforehand, can we still find a non-anticipative policy that will still drive the state sequence
{xt } to converge around the desired state value? What is the convergence rate? Without loss of
generality, we consider the desired state as the equilibrium 0. Hence we would like to trap the state
{xt} around the ball G = {x : ||x||2 ≤ } asymptotically given bounded state noise.
Furthermore, we define the online loss ft (kt) that the agent suffers at each step t after taking an
action ut = Φtkt and observing xt+1 as
ft(kt)=2 χT+1xt+1+2 UTut,
(3)

Under review as a conference paper at ICLR 2021
where t = 0, 1, ..., T, T is the terminal time step, η ∈ R+ is the state weighting parameter, and
β ∈ R+ is the control weighting parameter.
The cumulative loss (the regret) up to the terminal time step T is defined as
TT
RegT = X ft(kt)- X ft(k)	(4)
where k* = Vec(K *), and K * = limτ →∞ argminK T1 PT=I E[ft(K)], which is the fixed control
gain given by the infinite horizon Linear Quadratic Regulator (LQR) knowing the transition matrix
A beforehead.
In the online setting, we run the designed algorithm to learn to control on the fly in real time. At
each time step t, the agent suffers real loss by taking an action and being at certain state. Hence we
are also interested in making as close to the optimal as possible our cumulative loss (regret).
3	The Proposed Algorithm: Learning to Control On the Fly
To learn to control when the environment is subject to bounded noise with theoretical guarantee,
we introduce the ellipsoid method to the learning to control on the fly problem and propose the
algorithm with analysis in this section.
The following assumptions are made first.
Assumption 1: It is assumed that the state xt, ∀t = 0, 1, 2, ..., can be observed exactly.
Assumption 2: There exists K, such that ||A+BK||2 can be placed arbitrarily (See Abbasi-Yadkori
et al. (2014) and Ibrahimi et al. (2012) for the similar assumptions.)
3.1	The Ellipsoid Method Preliminaries
Vectorized kt: To use the ellipsoid method to keep collapsing the space of the control gain Kt, we
need to first rewrite the matrix Kt ∈ Rn×m into its vector form kt = VeC(Kt) ∈ Rm∙n×1. Similar
as equation 2, We can rewrite the fantasy action u^i-1(Kt) as the following
Ui-ι(Kt) = Ui-ι(kt) = Ktxi-ι = Φi-ikt,	(5)
where Φi-ι，xτ-1 0 Im.
The ball with Unknown Radius r: Our goal is to achieVe the exponential contraction of the
state, which means that we would like to trap the online solutions of kt into a ball which satisfies
||A + BK||2 ≤ δ < 1, albeit the radius of the ball r is unknown to us. We define the unknown
set as Dδ = {k : ||A + BK||2 ≤ δ < 1}. According to Assumption 2,δ ∈ (0, 1) can be chosen
arbitrarily by us.
The feasible set Q: We define the feasible set Q of the Vectorized control gain Variable kt : any
kt which makes ∣∣xt+1∣∣2 一 e < δ∣∣xt∣∣2 hold true.
The ball with Radius R: We assume that the feasible set Q is contained in the ball E0 = {k :
||k||2 ≤ R} of a giVen radius R, which means Q ⊂ E0 . The ball E0 will be the initial ellipsoid,
which yields that
k0 =0,H0 =RI,E0 = {k0 + H0z|zTz ≤ 1}.	(6)
The Collapsing Sequence of the Ellipsoids: We define Et be the ellipsoid at time t, such that
kt ∈ Et . We also define ρt be the radii of the Euclidean ball of the same Volume as Et ’s. We set
ρ0 = R.
Now we introduce the following geometric facts. Let Et-ι be an m ∙ n-dimensional ellipsoid and
Et = {k ∈ Et-i∖aTk ≤ aTkt-i}, a = 0
iii
Under review as a conference paper at ICLR 2021
be a half of Et-1. If m ∙ n > 1, then Et is contained in the ellipsoid Et of the smallest volume
Ben-Tal & Nemirovski (2001):
Et = {k = kt + Htz|zTz ≤ 1},
Ht-Iat
q aT Ht-IHt-Iat
kt-1 - m ： + ] Ht-IPt,
m ∙ n	m ∙ n
H	.9==Ht-1 + (----—j-
√(m ∙ n)2 - 1	m ∙ n + 1
m ∙ n
(m(m ∙ n)2 — 1
)Ht-1pt)ptT ,
ρt
|DetHt|1/n =(
m ∙ n
(m(m ∙ n)2 — 1
)(m∙n-1)∕m∙n(
)Hρt-1.
(7)
(8)
(9)
(10)
(11)
—
m ∙ n
m ∙ n + 1
Hence, the m ∙ n-dimenSionaI volume VoI(Et) of the ellipsoid Et is less than the volume of
VolEt-1) in the following relation:
mn	mn
Vol(Et)=(卜 3	)m∙n-1---- Vol(Et-ι) ≤ exp{-1∕2m ∙ n}Vol(E1).	(12)
√(m ∙ n)2 - 1 m ∙ n + 1
It also follows from equation 11 that
∀m ∙ n ≥ 2, pτ ≤ exp{-τ∕2(m ∙ n)2}R, τ = 1, 2, ..., t.	(13)
In the next part, we will build the algorithm which generates such valid at online to collapse the
volume of Et as described above.
3.2	Learning to Control On the Fly
At the initial time t = 0, the agent observes the state x0 . the action u0 = 0 is taken (k0 = 0), and
the environment equation 1 transits the state to x1 at time t = 1. The initial ellipsoid E0 is defined
in equation 6.
At time step t ≥ 1, we do the following to update the control gain kt and the ellipsoid Et.
The Separation Oracle: We test if ∣∣xt∣∣2 - C ≥ δ∣∣xt-1∣∣2, where δ ∈ (0,1), and C is the bound
of the noise wi, i = 1, ..., t. If it holds true, it means kt-1 ∈∕ Q. If etT BΦet-1 6= 0, we can add a
constraint
etT B Φet-1 (kt-1 - k) ≥ 0	(14)
into the set Et-1, where
_	Xt
et = i⅛,
φet-ι = eT-1 0 Im.
(15)
(16)
This linear inequality cuts off the ellipsoid Et-1 centered at kt-1 into a half-ellipsoid which we
further cover by the ellipsoid of the smallest volume by applying the standard formulas equation 7,
equation 8, equation 9, equation 10. We substitute at = etT BΦet-1 into equation 7, equation 8,
equation 9, equation 10 to update kt and the ellipsoid Et . We call the separation oracle is activated
if both ∣∣Xt∣∣2 - C ≥ δ∣∣Xt-1∣∣2 and at = 0 hold true.
The agent updates its action
ut = Φtkt,
(17)
where Φt = xtT 0 Im . After taking the action ut, the environment equation 1 transits the state to
xt+1.
The algorithm is summarized as the following.
iv
Under review as a conference paper at ICLR 2021
Alg: Learning to Control on the Fly (B, T, R, δ, )
For t = 0 to T :
Alg observes xt
If t = 0:
k0 = 0
Else:
If ∣∣Xt∣∣2 -C ≥ δ∣∣Xt-i∣∣2:
at = etTBΦet-1
If at = 0:
kt = kt-1
Else:
Et = {k ∈ Et-i∣aTk ≤ aTkt-i}
Update kt and Et = {k = kt + Htz|zTz ≤ 1} according to equation 7, equation 8,
equation 9, equation 10
End If
Else:
kt = kt-1
End If
End If
Alg takes action ut according to equation 17
End For
3.3	The Analysis of the Algorithm: Learning to Control on the Fly
Theorem 3.1.	By adding the constraint equation 14 when the separation oracle is active, we cut off
half volume of the ellipsoid Et-1 whose k ∈/ Dδ.
Proof. It follows from the environment dynamics equation 1 and the parameterized action update
equation 2 that
xt - (A + B Kt-1 )xt-1 = wt ,
multiplying both sides of which by the unit directional vector et，Jyxxt^ yields
etT[xt - (A + BKt-1)xt-1] = ||xt||2 - etT[A + BKt-1]||xt-1||2et-1 = etTwt.
It follows from the fact ||wt||2 ≤ C that
||xt||2 - etT[A + BKt-1]||xt-1||2et-1 = etTwt ≤ C.
Thus it is also true that
||xt ||2 - C ≤ etT[A + BKt-1]||xt-1 ||2 et-1.	(18)
When the constraint equation 14 is active, the IF test in the separation oracle holds true, which means
δ∣∣xt-ι∣∣2 ≤ ∣∣xt∣∣2 - c, substituting which into equation 18 yields
etT [A + B Kt-1]et-1 ≥ δ.	(19)
It follows from Assumption 2 that for any δ ∈ (0, 1), there exists K such that kA + BKk ≤ δ < 1.
Such K belongs our desired subset Dδ of K with the unknown radius r, and we denote as Kδ .
It follows that
etT[A + BKδ]et-1 ≤ ketk2kA+BKδk2ket-1k2 ≤ δ	(20)
equation 19 minus equation 20 yields
etTB(Kt-1 - Kδ)et-1 ≥ 0.	(21)
v
Under review as a conference paper at ICLR 2021
It follows from equation 2 that Kxt-ι = (XT-I 0 Im)k. Normalizing both sides by dividing
∣∣Xt-ι I∣2 yields that Ke— = (eTT-ι 0 Im)k，Φe-k. We thus plug Kt-Iet-ι = Φe1kt-ι,
and Kδet-1 = Φet-1 kδ into equation 21, which yields that
etTBΦet-1(kt-1 -kδ) ≥ 0.	(22)
Thus by adding the above constraints we cut off the other half space of the ellipsoid Et-1 which
violates equation 22, and those k does not belong to the desired subset Dδ = {k : ||A + BK||2 ≤
δ < 1}.	□
Theorem 3.2.	Let m ∙ n ≥ 2, and under Assumption 1 and 2 such that there exists a desired subset
Dδ = {k : ||A + BK||2 ≤ δ < 1} with unknown radius r. We assume there exists an initial
ball E0 = {k : ||k||2 ≤ R} of a given radius R which contains the feasible set Q. No more than
N = 2(m ∙ n)2 ln RR times of activation of the separation oracle is needed before the Control gain
being trapped into the desired subset Dδ .
Proof. First it can be easily seen that the feasible set Q contains the desired subset Dδ , since if
||A + BKt ||2 ≤ δ, then ||xt+1 ||2 = ||(A + BKt)xt + wt+1 || ≤ ||A + BKt||2 ||xt ||2 +	≤
δ∣∣Xt∣∣2 + e.
From Theorem 3.1, we know that by adding the linear constraint equation 14, we cut off the half
volume of the previous ellipsoid Et-1 whose k ∈/ Dδ . Then by applying the formulas equation 7,
equation 8, equation 9, equation 10, we update the current ellipsoid Et to be of the smallest volume
but covers the remaining half space of the previous ellipsoid Et-1.
Thus every time the separation oracle is activated, and after we do the above procedures to update
Et, equation 13 holds true for ∀m ∙ n ≥ 2, where the time T denotes the index of the number of
times when the separation oracle is activated, τ = 1, 2, ..., N.
We prove the theorem by contradiction, and we assume that the control gain has not been trapped
into Dδ after N = 2(m ∙ n)2 ln R times of activation of the separation oracle, which means the radii
ρt of the Euclidean ball of the same volume as Et’s, compared with the radii r of the Euclidean ball
of the same volume as Dδ, follows that
P > 1.	(23)
r
It follows from equation 13 that
pt ≤ exp{-N∕2(m ∙ n)2}R.
Substituting equation 24 into equation 23 yields that
N < 2(m ∙ n)2 ln —,	(24)
r
which is the desired contradiction.	□
3.4 No Pain, No Gain and No Gain, No Pain
No Pain, No Gain: In Theorem 3.2, it is stated that at most N = 2(m ∙ n)2 ln R times activation
,r
of the separation oracle is needed, before the agent can learn the control gain kt ∈ Dδ , which is
equivalent of saying that the state xt can be converged asymptotically.
Each time when the separation oracle is activated, the current state xt 6= 0, and ||xt||2 ≥
δ∣∣xt-ι∣∣2 + e. Compared to that when the separation oracle is not activated, either Xt = 0, or
||xt ∣∣2 ≤ δ∣∣xt-ι ∣∣2 + e, We suffer a bigger online loss according to equation 3, and a bigger regret
according to equation 4, which can be interpreted as the ”pain” that we suffer online.
However, it can also be seen from the learning to control algorithm that: only through activating
the separation oracle, we get a chance to update the control gain kt, and collapse the volume of the
domain Et of the control gain kt , by which we learn to control. If our desired goal and gain is to
learn a good policy online from scratch such that Xt can be converged asymptotically, we have to
take the ”pain” to activate the separation oracle no more than N times.
vi
Under review as a conference paper at ICLR 2021
No Gain, No Pain: When the terminal time T is finite, the goal of trapping the control gain kt into
Dδ cannot be guaranteed to be achieved. One counter example is the following: it follows from the
learning to control on the fly algorithm design that k0 = 0. Suppose if ||Ax0||2 ≤ , and the random
bounded noise, which is adversary, is chosen to be w1 = -Ax0. Then it follows from equation 1
that x1 = 0. If wt = 0, for t = 2, 3, ., T - 1, then from t = 1, .., T - 1, xt = 0, so we won’t
have any chance to update kt and learn to control. From this perspective, if we define our desired
gain to be to learn a good policy, for such cases where the separation oracle is not activated, we have
no ”gain”.
In such situations where the separation oracle is not activated, either xt = 0, or ||xt ||2 ≤
δ∖∖xt-ι∣∣2 + e, although We have no "gain" in the knowledge of how to control, our online loss
and regret are also small compared with when activated, hence there is also ”no pain” along with
”no gain”.
If we would like to re-define our goal to be to suffer as little regret as possible in the finite running
time, we can evaluate the proposed learning to control on the fly algorithm in the following theorem.
Theorem 3.3. Assume finite running time T N. We also assume that there exists an initial ball
E0 = {k : ∖∖k∖∖2 ≤ R} of a given radius R which contains the feasible set Q. Under Assumption
1 and 2, with bounded noise ∖∖wt ∖∖ ≤ , ∀t, the regret of the proposed learning to control on the
fly algorithm is upper bounded by RegT ≤ O(T). If E ≤ 焉,where C ≥ 0 is any constant, the
proposed algorithm is a no-regret algorithm.
Proof. Using the proposed learning to control on the fly algorithm, the case in which the agent
suffers the maximal regret is when the agent keeps activating the separation oracle for the first
N = 2(m ∙ n)2 ln R steps. Because N is not dependent on T, the cumulative loss for the first N
steps PN=I 2xT+ιXt+ι + 2uTut is bounded by a constant.
It follows from Theorem 3.2 that after the first N steps of activating the oracle, ∖∖A+ BKt∖∖2 ≤ δ <
1, and xt will start to converge.
Next we compute the exact ball that xt will converge to at the worst case scenario, where t > N.
Since kt ∈ Dδ , the separation oracle will not be activated anymore. Hence, Kt will stay the same.
The Kt corresponding to the slowest convergence rate is Kδ such that ∖∖A+BKδ∖∖2 = δ. We denote
the most adversarial random noise as wδ, the converged invariant state under wδ as xδ, which
satisfies (A + BKδ)xδ + wδ = xδ. Hence the upper bound for xδ yields that
∖∖xδ∖∖2 ≤ ∖∖[I - (A + BKδ)]tWδ∖∖2 ≤∖∖[I — (A + BKδ)]t∖∖2∖∖wδ∖∖2∙	(25)
We denote ∖∖[I 一 (A + BKδ)]t∖∖2，α, which is constant. Hence equation 25 yields that ∖∖xδ∖∖2 ≤
α2E2.
We define the exact ball as G0 = {x : ∖∖x∖∖2 ≤ αE}. The first time when xt converges around G0
is denoted by tc . The converging speed of xt when t > N is exponential fast before xt converges
around G0. Hence the number of steps tc - (N+ 1) is also upper bounded by a constant which does
not depend on T. Thus the cumulative loss Pt=N+1 2xT+ιXt+ι + 2UTUt can also bounded by a
constant.
The last part of the cumulative loss is bounded as the following:
T	βT	β
X 2χT+ιχt+ι + 2UrTUt ≤ X 2∖∖xδ∖∖2 + 2∖∖Kδ∖∖2∖∖xδ∖∖2
t=tc +1	t=tc +1
T
≤ X 2α2e2 + 2r2α2e2 ≤ (j + βr2)α2E2T = O(T).
t=tc +1
If E ≤ 焉,where C is a constant, then PTT=tc+1 2xT+ιXt+ι + 2UTUt ≤ (2 + 2r2)α2c, which is
a constant which does not depend on T.
Since the cumulative loss from time 1 to N and from N + 1 to tc are both bounded by constants,
and from tc + 1 to T is bounded by O(T), the total cumulative loss PT=I 2xT+ιXt+ι + βUTUt is
bounded by O(T),and specially is bounded by a constant if E ≤ √^.
vii
Under review as a conference paper at ICLR 2021
The optimal cumulative loss PT=I ft(k*) in equation 4 can be lower bounded by a constant, which
follows from the property of LQR (Kwakernaak & Sivan, 1972).
Hence we can see that the regret of the proposed learning to control on the fly algorithm is upper
bounded by RegT ≤ O(T). If E ≤ √cp, the proposed algorithm has constant regret, which is also
called to be of no regret.	□
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 1-
26, 2011.
Yasin Abbasi-Yadkori, Peter Bartlett, and Varun Kanade. Tracking adversarial targets. In Interna-
tional Conference on Machine Learning, pp. 369-377, 2014.
Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesvari. Model-free linear quadratic control
via reduction to expert prediction. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 3108-3117, 2019.
Naman Agarwal, Brian Bullins, Elad Hazan, Sham M Kakade, and Karan Singh. Online control
with adversarial disturbances. arXiv preprint arXiv:1902.08721, 2019a.
Naman Agarwal, Elad Hazan, and Karan Singh. Logarithmic regret for online control. In Advances
in Neural Information Processing Systems, pp. 10175-10184, 2019b.
Sanjeev Arora, Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Towards provable
control for unknown linear dynamical systems. 2018.
A Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: Analysis. Algorithms,
and Engineering Applications, SIAM, Philadelphia, 2001.
Robert G Bland, Donald Goldfarb, and Michael J Todd. The ellipsoid method: A survey. Operations
research, 29(6):1039-1091, 1981.
Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer Science &
Business Media, 2013.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Ching-An Cheng, Remi Tachet des Combes, Byron Boots, and Geoff Gordon. A reduction from
reinforcement learning to no-regret online learning. arXiv preprint arXiv:1911.05873, 2019.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust
adaptive control of the linear quadratic regulator. In Advances in Neural Information Processing
Systems, pp. 4188-4197, 2018.
Maryam Fazel, Rong Ge, Sham M Kakade, and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. arXiv preprint arXiv:1801.05039, 2018.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Elad Hazan, Karan Singh, and Cyril Zhang. Learning linear dynamical systems via spectral filtering.
In Advances in Neural Information Processing Systems, pp. 6702-6712, 2017.
Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Spectral filtering for general
linear dynamical systems. In Advances in Neural Information Processing Systems, pp. 4634-
4643, 2018.
Morteza Ibrahimi, Adel Javanmard, and Benjamin V Roy. Efficient reinforcement learning for high
dimensional linear quadratic systems. In Advances in Neural Information Processing Systems,
pp. 2636-2644, 2012.
viii
Under review as a conference paper at ICLR 2021
Huibert Kwakernaak and Raphael Sivan. Linear optimal control systems, volume 1. Wiley-
interscience New York, 1972.
P Read Montague. Reinforcement learning: An introduction, by sutton, rs and barto, ag. Trends in
cognitive sciences, 3(9):360, 1999.
Douglas C Montgomery, Elizabeth A Peck, and G Geoffrey Vining. Introduction to linear regression
analysis, volume 821. John Wiley & Sons, 2012.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
Trends® in Machine Learning, 4(2):107-194, 2012.
Robert F Stengel. Optimal control and estimation. Courier Corporation, 1994.
Stephen Tu and Benjamin Recht. Least-squares temporal difference learning for the linear quadratic
regulator. arXiv preprint arXiv:1712.08642, 2017.
Nolan Wagener, Ching-An Cheng, Jacob Sacks, and Byron Boots. An online learning approach to
model predictive control. arXiv preprint arXiv:1902.08967, 2019.
Liu Yang, Rong Jin, and Jieping Ye. Online learning by ellipsoid method. In Proceedings of the
26th Annual International Conference on Machine Learning, pp. 1153-1160, 2009.
Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic constraints.
In Advances in Neural Information Processing Systems, pp. 1428-1438, 2017.
Jianjun Yuan and Andrew Lamperski. Online convex optimization for cumulative constraints. In
Advances in Neural Information Processing Systems, pp. 6137-6146, 2018.
ix