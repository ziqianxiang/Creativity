Under review as a conference paper at ICLR 2021
Lie Algebra Convolutional Networks with
Automatic Symmetry Extraction
Anonymous authors
Paper under double-blind review
Ab stract
Existing methods for incorporating symmetries into neural network architectures
require prior knowledge of the symmetry group. We propose to learn the symme-
tries during the training of the group equivariant architectures. Our model, the
Lie algebra convolutional network (L-conv), is based on infinitesimal generators
of continuous groups and does not require discretization or integration over the
group. We show that L-conv can approximate any group convolutional layer by
composition of layers. We demonstrate how CNNs, Graph Convolutional Networks
and fully-connected networks can all be expressed as an L-conv with appropriate
groups. By allowing the infinitesimal generators to be learnable, L-conv can learn
potential symmetries. We also show how the symmetries are related to the statistics
of the dataset in linear settings. We find an analytical relationship between the
symmetry group and a subgroup of an orthogonal group preserving the covariance
of the input. Our experiments show that L-conv with trainable generators performs
well on problems with hidden symmetries. Due to parameter sharing, L-conv also
uses far fewer parameters than fully-connected layers.
Many machine learning (ML) tasks involve data from unfamiliar domains, which may or may not have
hidden symmetries. While much of the work on equivariant neural networks focuses on equivariant
architectures, the ability of the architecture to discover symmetries in a given dataset is less studied.
Convolutional Neural Networks (CNN) (LeCun et al., 1989; 1998) incorporate translation symmetry
into the architecture. Recently, more general ways to construct equivariant architectures have been
introduced (Cohen & Welling, 2016a;b; Cohen et al., 2018; Kondor & Trivedi, 2018). Encoding
equivariance into an ML architecture can reduce data requirements and improve generalization,
while significantly reducing the number of model parameters via parameter sharing (Cohen et al.,
2019; Cohen & Welling, 2016b; Ravanbakhsh et al., 2017; Ravanbakhsh, 2020). As a result, many
other symmetries such as discrete rotations in 2D (Veeling et al., 2018; Marcos et al., 2017) and 3D
(Cohen et al., 2018; Cohen & Welling, 2016a) as well as permutations (Zaheer et al., 2017) have been
incorporated into the architecture of neural networks.
Many existing works on equivariant architectures use finite groups such as permutations in Hartford
et al. (2018) and Ravanbakhsh et al. (2017) or discrete subgroups of continuous groups, such as 90
degree rotations in (Cohen et al., 2018) or dihedral groups DN in Weiler & Cesa (2019). Ravanbakhsh
(2020) also proved a universal approximation theorem for single hidden layer equivariant neural
networks for Abelian and finite groups. General principles for constructing group convolutional
layers were introduced in Cohen & Welling (2016b), Kondor & Trivedi (2018), and Cohen et al.
(2019), including for continuous groups. A challenge for implementation is having to integrate over
the group manifold. This has been remedied either by generalizing Fast Fourier Transforms (Cohen
et al., 2018), or using irreducible representations (irreps) (Weiler et al., 2018a) either directly as
spherical harmonics as in Worrall et al. (2017) or using more general Clebsch-Gordon coefficients
(Kondor et al., 2018). Other approaches include discretizing the group as in Weiler et al. (2018a;b);
Cohen & Welling (2016a), or solving constraints for equivariant irreps as in Weiler & Cesa (2019), or
approximating the integral by sampling (Finzi et al., 2020).
The limitations in all of the approaches above are that: 1) they rely on knowing the symmetry group
a priori, and 2) require encoding the whole group into the architecture. For a continuous group, it is
not possible to encode all elements and we have to resort to discretization or a truncated sum over
irreps. Our work attempts to resolve the issues with continuous groups by using the Lie algebra (the
linearization of the group near its identity) instead of the group itself. Unlike the Lie group which is
1
Under review as a conference paper at ICLR 2021
infinite, the Lie algebra usually has a finite basis (notable exception being Kac-Moody Lie algebras
for 2D Conformal Field Theories (Belavin et al., 1984) in physics). Additionally, we show that the
Lie algebra basis can be learned during training, or through a separate optimization process. Hence,
our architecture, which generalizes a group convolutional layer, is potentially capable of learning
symmetries in data without imposing inductive biases.
Learning symmetries in data was tackled in restricted settings of mostly commutative Lie groups
as in Cohen & Welling (2014) and 2D rotations and translations in Rao & Ruderman (1999) and
Sohl-Dickstein et al. (2010) or permutations (Anselmi et al., 2019). However, the symmetries learned
by the architecture are not necessarily familiar spatial symmetries. As we show in the case of linear
regression, the symmetries may correspond to transformations preserving the statistics of the data.
Specifically, we show a general relation between the symmetries of linear regression and a deformed
orthogonal group preserving the covariance matrix. Such symmetries of the probability distribution
and ways to incorporate them into the architecture were also discussed in Bloem-Reddy & Teh (2019).
The work that is closest in spirit and setup to ours is Zhou et al. (2020) which uses meta-learning to
automatically learn symmetries. Although the weight-sharing scheme of Zhou et al. (2020) and their
encoding of the symmetry generators is different, their construction does bear some resemblance to
ours and we will discuss this after introducing our architecture.
Contributions Our main contributions can be summarized as follows
•	We propose a group equivariant architecture using the Lie algebra, introducing the Lie
algebra convolutional layer (L-conv).
•	In L-conv the Lie algebra generators can be trained to discover symmetries, and it outper-
forms CNN on domains with hidden symmetries, such rotated and scrambled images.
•	Group convolutional layers on connected Lie groups can be approximated by multi-layer
L-conv, and Fully-connected, CNN and graph convolutional networks are special cases of
L-conv.
•	In linear regression, we show analytical relations between symmetries in and orthogonal
groups preserving covariance of data.
1	Equivariance in Supervised Learning
Consider the functional mapping yi = f(xi) of inputs X = (x1, . . . , xn) to outputs Y =
(y1, . . . , yn). We assume each input x ∈ Rd×m where Rd are the “space” dimensions and Rm
the “channels”, and y ∈ Rc (or Zc2 for categorical variables). We assume a group G acts only
on the space factor (Rd, shared among channels) of x through a d-dimensional representation
Td : G → GLd(R) mapping each g to an invertible d × d matrix. The map Td must be continuous
and satisfy Td(u)Td(v) = Td(uv) for all u, v ∈ G (Knapp, 2013, IV.1). Similarly, let G act on y via
a c-dimensional representation Tc . To simplify notation, we will denote the representations simply as
uα ≡ Tα(u). A function f solving yi = f(xi) is said to be equivariant under the action of a group
G by representations Tc , Td if
ucy = ucf(x) = f (udx)
⇔ f(x) = ucf(ud x).
∀u ∈ G
(1)
Lie Groups and Lie Algebras The full group of invertible d × d matrices over R is the general
linear group, denoted as GLd(R). It follows that every real d- dimensional group representation
Td(G) ⊂ GLd(R). If T is a “faithful representation” (i.e. T(u) 6= T(v)ifu 6= v), then G ⊂ GLd(R).
In our problem, we only know the group G through its linear action on X or on output hl of layer l
in a neural network. Therefore we may assume the representation Tdl acting on hl with largest dl is
faithful. GLd (R) is a Lie group and all of its continuous subgroups G are also Lie groups. Next, we
will first briefly review the Lie algebra of a Lie group and then use it to introduce our equivariant
architecture.
Notation Unless stated or obvious, a in Aa is an index, not an exponent. We write matrix products
as A ∙ B ≡ Pa AaBa Recall that X ∈ Rd×m. For a linear transformation A : Rd1 → Rd2
2
Under review as a conference paper at ICLR 2021
acting on the spatial index or the channel index, we will use one upper and one lower index as in
Aμhμ = [A ∙ h]ν. We will use (a, b, C) for channels, and (μ, ν, ρ) for spatial, and (i,j, k) for Lie
algebra basis indices. We will occasionally keep explicit summation Pi for clarity.
For any Lie group G ⊂ GLd(R), group elements infinitesimally close to the identity element can
be written as u = I + iLi. We can find a basis Li which span the “Lie algebra” g of the group G,
meaning they are closed under commutations
[Li, Lj] = LiLj - LjLi = XfijkLk	(2)
k
with fij k called the structure constants of the Lie algebra. The Li are called the infinitesimal
generators of the Lie group. They define vector fields spanning the tangent space of the manifold of
G near its identity element I . For elements u ∈ GI in the connected component GI ⊂ G containing
the identity, an exponential map exp : g → GI can be defined such that U = exp[t ∙ L]. For matrix
groups, if G is connected and compact, the matrix exponential defined through a Taylor expansion is
such a map and it is surjective. For most other groups (except GLd(C) and nilpotent groups) it is not
surjective. Nevertheless, for any connected group ( including GI) every element u can be written
as a product U = Qa exp[tɑ ∙ L] using the matrix exponential (Hall, 2015). We will use this fact to
modify existing results about group equivariant architectures to introduce L-conv.
2	Group Equivariant Architecture
Consider the case where the solution f to y = f(x) is implemented as a feedforward neural network.
Denote the linear output of layer l by hl = Fl(hl-1) ∈ Rdl ×ml, with h0 = xi. Kondor & Trivedi
(2018); Cohen & Welling (2016a) showed that a feedforward neural network is equivariant under the
action of a group G if and only if each of its layers implement a group convolution (G-conv) given by
hl
(fl * gι)(hi-i) = / dμ(u)fι(u--1ιhi-i)gι(u)
G
(3)
where dμ(u) is the Haar measure on the group manifold, Ul is an appropriate di-dimensional
representation of G, and gl : G → Rml ×ml+1 is a set of convolution kernels, where ml and ml+1
are the number of input and output channels (filters). Here fl are point-wise activation functions. In
the formalism of Kondor & Trivedi (2018), we may consider hl to be a map from the homogeneous
space Xl to Rml where Xl is a space such that the space of maps LR[Xl] is parameterized by Rdl . To
use equation 3 in practice, we need to make the integral over the group manifold tractable. Although,
Finzi et al. (2020), which approximates the integral over G by sampling, approach the idea of working
in the Lie algebra, they use a logarithm map to define the Lie algebra. We take a different approach
and formulate the architecture in terms of the Lie algebra basis. We also allow the architecture to
learn basis Li for the symmetries.
For continuous symmetry groups we will now show that using the Lie algebra can approximate
G-conv without having to integrate over the full group manifold, thus alleviating the need to discretize
or sample the group. The key point is the discreteness and finiteness of the Lie algebra basis for many
Lie groups.
Proposition 1. Let G be a Lie group and Td : G → Rdl a dl -dimensional representation of G. If
a convolution kernel gl : G → Rml-1×ml has support only on an infinitesimal η neighborhood of
identity, a G-conv layer of equation 3 can be written in terms of the Lie algebra.
Proof: Linearization over η yields Ul ≈ I + e ∙ L in equation 3, with Li ∈ Rdl ×dl. The inverse is then
u-1 ≈ I 一 e ∙ L + O(e2). Since gl has support only in an η neighborhood of identity, fixing a basis
Li, we can reparametrize gl(I + e ∙ L)= gl(e) as a function over the Lie algebra gl : g → Rml ×ml+1.
Dropping the layer index l for brevity, we have
(f * g)(h) ≈ g g(e)f ((I - e ∙ L)h) dnLe ≈ (W0I - W ∙ L) h ∙Vhf(h)
G *
W0 ≡ g g(e)dnL e,	Wi ≡ e eig(e)dnL e.
GG
(4)
(5)
3
Under review as a conference paper at ICLR 2021
where the number of Wi is nL, the number of Li. When f(h) is a homogeneous function like ReLU
or linear activation, h ∙ Vhf (h) = af (h). In general, since We are designing the layers, We can
choose h ∙ Vhf (h) = σ(h) to be a desired activation function.	□
Note that in G-conv, the convolutional kernels g(u) are the trainable Weights. The difficulty With
learning them is that one has to parametrize and integrate the convolution over the group G. The
major advantage of Working in the Lie algebra instead of G is that in equation 4 the integrals of
g() separate from f (h), leaving us With a set of integrated Weights W. This means that instead of
the architecture needing to implement the integral Jg g(u)f (uh)dμ, we only need to learn a finite
number of convolutional Weights W, like the filter Weights in CNN. Additionally, We can learn Li.
Indeed, having learnable Li to learn hidden symmetries is one of main features We are proposing, as
We shoW in our experiments.
L-conv Layer The construction in equation 4 is at the core of our results. h ∈ Rdl-1 ×ml-1 has
components hμ, where μ ∈ {1,…，dι-ι} is the spatial index and a ∈ {1,…mι-ι} the channel
index. We define the Lie algebra convolutional layer (L-conv) as
L-conv : FL (h) = σ (XX Li ∙ h ∙ Wi + b)
FL(h)μ= σ (X[LC hV 尸1： + ba I	(6)
ν,c,i
where σ is the activation function, L0 = I and
Wi ∈ Rml ×ml-1 are the convolutional weights,
and the second line shows the components (all
symbols are indices, not exponents). Note that,
similar to discussion in Weiler et al. (2018a), an
arbitrary nonlinear activation σ may not keep
the architecture equivariant under G, but linear
activation remains equivariant. Also, note that
in equation 3, fι is the activation σι-1 of the pre-
vious layer. In defining L-conv we are including
the activation σ after the convolution, which is
why σ appears outside in equation 6 instead of
as σ(h).
Since in connected Lie groups larger group ele-
ments can be constructed as u = ∏a exP[tɑ ∙ L]
(Hall, 2015) from elements near identity, it fol-
lows that any G-conv layer can be constructed
from multiple L-conv layers on these groups.
Proposition 2. Any G-conv layer on a con-
nected Lie group can be approximated by a
multi-layer L-conv to arbitrary accuracy.
L-conv layer
channels
Figure 1: L-conv layer architecture. Li only act
on the d spatial dimensions, and Wi only act on
the mι feature. For each i, this action is analogous
to a Graph Convolutional Network with d nodes
and mι features per node.
Proof: To show this we need to take two steps. First, we discretize the support of g(u) in equation 3
to infinitesimally small sets G = Sk Gk, each having support around an element uk ∈ Gk. This
allows us to write (f * g)(h) ≈ Pk ∆μkg(uk)f (U-Ih), where ∆μk = JGk dμ. Next, we write
each uk as a product. On connected groups for any uk ∈ G, we can write uk = Qια=1 vα where
Va = exp[tɑ ∙ L] ≈ (I + ta ∙ L) are elements close to identity. Then, f (Uh) can be written as
a composition l G-conv layers, each with weights gα(u) = δ(u - vα) (defined using a faithful
representation of G)
f (u-1h) = f (Y Vah) = f (Fl。…。Fι(h)),	Fa(h) = ∣gdμ(u)fa(uah)ga(u)	(7)
Since Ua are near identity, each Fa can be converted to an L-conv layer with linear activation and
weights W = ta and W0 = 1. Thus, the G-conv layer can be approximated as sum and composition
4
Under review as a conference paper at ICLR 2021
of L-conv layers. Each Gk can be considered one output channel for the multi-layer L-conv and
g(uk) are weights of a final aggregation layer.
So far we have shown that L-conv layers could replace G-conv for connected groups. The advantage
of using L-conv layers is that for many potential symmetry groups, such as orthogonal and unitary
groups, the number of generators Li is finite and often a small number of them may be sufficient. For
instance, consider SO(2) rotations on p × p images. The flattened image has d = p2 dimensions and
the number of GLd (R) generators is p4. However the induced representation of SO(2) on Rd has a
single generator. In fact, in many domains we expect symmetries hidden in the data to be much lower
in dimensionality than the data itself. Additionally, in the other extreme, where there is no restriction
on the symmetry, meaning G = GLd(R), we observe that L-conv becomes a fully-connected layer,
as shown next.
Proposition 3. A fully-connected neural network layer can be written as an L-conv layer using
GLd (R) generators, followed by a sum pooling and nonlinear activation.
Proof: The generators of GLd(R) are one-hot E ∈ Rd×d matrices Li = E(α,β) which are non-zero
only at index i = (α, β) 1 with elements written using Kronecker deltas
GLd(R) generators : Lν,μ = [E(α,β)]μ = δμαδV	⑻
Now, consider the weight matrix w ∈ Rm×d and bias b ∈ Rm of a fully connected layer acting on
h ∈ Rd as F(h) = σ(w ∙ h + b). The matrix element can be written as
Wb = XX wα1β [E(α,β)]μ = XX W(bα1β)[E(a,β)]μ = X W b,1 ∙ [L]μ	(9)
μ α,β	μ α,β	μ
meaning an L-conv layer with weights W(bα,1,β) = wbα1β (1 input channel, and 1 being a vector of
ones) followed by pooling over μ is the same as a fully connected layer with weights w.	□
Thus, interestingly, more restricted groups, rather than large, meaning groups with fewer Li, lead to
more parameter sharing. Indeed, as we show below, the most restricted L-conv are graph convolutional
networks. It is also easy to see that familiar CNN can be encoded as L-conv. Note that, generally shift
operators used in CNN are thought of as group elements for a discrete permutation group. However,
the same shift operators Li can be used to create continuous fractional shifts as (I + αLi)∕(α + 1).
Since shifts commute with each other, they form a basis for the Lie algebra of an Abelian subgroup
of GLd(R).
Proposition 4. CNN is a special case of L-conv where the generators are shift operators.
Proof: In 1D CNN, for a kernel size k We have k shift operators Li given by L%μ = δμ,ν-i. Plugging
this into equation 6 and doing the sum over ν, we recover the convolution formula
F(h)μ = σ (X hμ-iWC,a + b)	(10)
In higher dimensional CNN, i covers the relevant flattened indices for the required shifts. For instance,
a p × q image, when flattened becomes a vector of length pq. To cover a (2, 2) convolutional kernel,
we need shifts to be by 0, 1, q, q + 1 pixels. Thus, we need four Li given by
Libμ = δμ,ν-Si ,	Si = [0, 1,q,q + 1]i	(11)
This means that the number of generators Li is the size of the convolutional kernel in CNN. General-
ization to higher dimensions is straightforward.	□
In CNN, covering larger patches is equivalent to g(u) covering a larger part of the group. But large
convolutional kernels are not generally used. Instead, we achieve larger receptive fields by adding
more CNN layers, similar to Proposition 2.
1We may also label them by single index like i = α + βd, but two indices is more convenient.
5
Under review as a conference paper at ICLR 2021
Connection with Graph Convolutional Networks Finally, we note that the L-conv layer equa-
tion 6 has the structure of a Graph Convolutional Networks (GCN) Kipf & Welling (2016) with
multiple propagation rules Li derived from graph adjacency matrices and μ, V indexing graph vertices.
Thus, an L-conv with a single L is a GCN. In fact, there is a deeper connection here, as we show now.
Proposition 5. A GCN with propagation rule A ∈ Rd×d is equivalent to an L-conv, with the group
being 1D flows generated by A - I.
Proof: We can define a linear flow similar to a diffusion equation
h(t + δt) = AhV (t)	⇒ —ɑ = λ(A — I )h(t)	(12)
dt
where λ = δt∕dt sets the time scale. Thus L = λ(A — I) is the generator a 1D group of flows with
elements u = exp[tL], a subgroup of the group of diffeomorphisms on Rd, with a single generator
L. Thus, a GCN with propagation rule A is an L-conv using L0 = I and L1 = L with the same
convolutional weights for L0, L1.
Hence, the most restricted L-conv based on 1D flow groups with a single generator are GCN. These
flow groups include Hamiltonian flows and other linear dynamical systems. CNN can also be
interpreted as a GCN with multiple propagation rules, where each shift operator is a subgraph of the
grid network.
3	Learning Potential Symmetries
When dealing with unknown continuous symmetry groups, it can be virtually impossible to design
a G-conv, as it relies on the structure of the group. The Lie algebra, however, has a much simpler,
linear structure and universal for all Lie groups. Because of this, L-conv affords us with a powerful
tool to probe systems with unknown symmetries. L-conv is a generic weight-sharing ansatz and the
number of Li can be expected to be small in many systems. This means that even if we do not know
Li , it may be possible to learn the Li from the data. In fact, as we show in our experiments, learning
low-rank Li via SGD simultaneously with other weights yield impressive performance on data with
hidden symmetries (Fig. 2), without needing any input about the symmetry.
Learning the Li We learn the Li using SGD, simultaneously with Wi and all other weights. Our
current implementation is similar to a GCN F(h) = σ(Li ∙ h ∙ Wi + b) where both the weights Wi
and the propagation rule Li are learnable. When the spatial dimensions d of the input x ∈ Rd×c is
large, e.g. a flattened image, the Li with d2 parameters can become expensive to learn. However,
generators of groups are generally very sparse matrices. Therefore, we represent Li using low-rank
decomposition Li = Ui Vi . An encoder V of shape nL × dh × d encodes nL matrices Vi , and a
decoder U of shape nL × d × dh. Here d is the input dimensions and dh the latent dimension with
dh d for sparsity. In order for the Li to form a basis for a Lie algebra, they should be closed
under the commutation relations equation 2, as well as orthogonal under the Killing Form (Hall,
2015, Chapter 6). These conditions can be added to the model as regularizers (see Appendix F.2),
but regularization also introduces an additional time complexity of O(n2Ld2hd), which can be quite
expensive compared to the O(nldhd) of learning Li via SGD. Therefore, in the experiments reported
here we did not use any regularizers for Li .
Comparison with Meta-learning Symmetries by Reparameterization (MSR) Recently Zhou
et al. (2020) also introduced an architecture which can learn equivariances from data. We would like
to highlight the differences between their approach and ours, specifically Proposition 1 in Zhou et al.
(2020). Assuming a discrete group G = {g1, . . . , gn}, they decompose the weights W ∈ Rs×s of a
fully-connected layer, acting on x ∈ Rs as vec(W) = UGv where UG ∈ Rs×s are the “symmetry
matrices” and v ∈ Rs are the “filter weights”. Then they use meta-learning to learn UG and during
the main training keep UG fixed and only learn v. We may compare MSR to our approach by setting
d = s. First, note that although the dimensionality of U ∈ Rnd×d seems similar to our L ∈ Rn×d×d,
the Li are n matrices of shape d × d, whereas U has shape (nd) × d with many more parameters
than L. Also, the weights of L-conv W ∈ Rn×ml ×ml-1, with ml being the number of channels, are
generally much fewer than MSR filters v ∈ Rd . Finally, the way in which Uv acts on data is different
from L-conv, as the dimensions reveal. The prohibitively high dimensionality of U requires MSR
to adopt a sparse-coding scheme, mainly Kronecker decomposition. Though not necessary, we too
6
Under review as a conference paper at ICLR 2021
choose to use a sparse format for Li , finding that very low-rank Li often perform best. A Kronecker
decomposition may bias the structure of U G as it introduces a block structure into it.
Contrast with Augerino In a concurrent work, (Benton et al., 2020) propose Augerino, a method
to learn invariances with neural networks. Augerino uses data augmentation to transform the input
data, which means it is restricting the group to be a subgroup of the augmentation transformations.
The data augmentation is written as g = exp (Pi iθiLi) (equation (9) in Benton et al. (2020)),
with randomly sampled i ∈ [-1, 1]. θi are trainable weights which determine which Li helped
with the learning task. However, in Augerino, Li are fixed a priori to be the six generators of affine
transformations in 2D (translations, rotations, scaling and shearing). In contrast, our approach is more
general. We learn the generators Li directly without restricting them to be a known set of generators.
Additionally, we do not use the exponential map, hence, implementing L-conv is very straightforward.
Lastly, Augerino uses sampling to effectively cover the space of group transformations. Since the
sum over Lie algebra generators is tractable, we do not need to use sampling.
Next, we show that in certain cases, such as linear regression, the symmetry group can be derived
analytically. While the results may not apply to more non-linear cases, they give us a good idea of
the nature of the symmetries we should expect L-conv to learn.
3.1	Symmetries of Linear Regression
In the case of linear regression we can derive part of the symmetries explicitly, as shown next. We
absorb biases into the regression weights A as the last row and append a row of 1 to the input.
Theorem 1. Consider a linear regression problem on inputs X ∈ Rd×n and labels Y ∈ Rc×n like
above. We are looking for the linear function Y = AX. For this problem is equivariant under a
group G, through two representations u ∈ Td(G) and uc ∈ Tc(G) acting on X and Y , respectively,
it is sufficient for u and uc to satisfy
uHuT = H	ucYXTuT = YXT.	(13)
where H ≡ -1XXT is the covariance matrix ofthe input.
Proof: The equivariance condition equation 1 becomes
ucY =ucAX = AuX	⇒ A = ucAu-1	(14)
Assuming that the number of samples is much greater than features, n d, and unbiased data, XTX
will be full rank d × d and that its inverse exists. The solution to the linear regression Y = AX
is given by A = Y XT(XXT)-1. Using the definition of covariance H above, the condition
equation 14 becomes A = ucY XTuT(uXXTuT)-1. Thus, a sufficient condition for equation 14
to hold is equation 13.
The first condition in equation 13 is unsupervised, stating that u preserves the covariance H, while
the second condition is supervised, stating that uc and u preserve the cross-correlation of input and
labels Y XT. Next, we show that the group satisfying equation 13 is related to an orthogonal group.
Corollary 1. The subgroup of symmetries of linear regression satisfying uHuT = H in equation 13
form an orthogonal group isomorphic to S O(d), with a Lie algebra basis given by
Li = H1/2L0iH-1/2	L0i ∈ so(d)	(15)
Proof: multiplying uHuT = H by H -1/2 from both sides we see that u0 ≡ H -1/2 uH 1/2
satisfies u0u0T = I meaning U ∈ SO(d). Expanding elements near identity we get u ≈ I + e ∙ L =
H 1∕2(I + e0 ∙ L0)H-1∕2. Setting e = e0 proves equation 15. (see SUPP D.2 for more details) 口
4	Experiments
To understand precisely how L-conv performs in comparison with CNN and other baselines, we
conduct a set of carefully designed experiments. Defining pooling for L-conv merits more research.
Without pooling, we cannot use L-conv in state-of-the-art models for familiar problems such as image
7
Under review as a conference paper at ICLR 2021
classification. Therefore, we use the simplest possible models in our experiments: one or two L-conv,
or CNN, or FC layers, followed by a classification layer.
Test Datasets We use four datasets: MNIST, CIFAR10, CIFAR100, and FashionMNIST. To test
the efficiency of L-conv in dealing with hidden or unfamiliar symmetries, we conducted our tests
on two modified versions of each dataset: 1) Rotated: each image rotated by a random angle
(no augmentation); 2) Rotated and Scrambled: random rotations are followed by a fixed random
permutation (same for all images) of pixels. We used a 80-20 training test split on 60,000 MNIST
and FashionMNIST, and on 50,000 CIFAR10 and CIFAR100 images. Scrambling destroys the
correlations existing between values of neighboring pixels, removing the locality of features in
images. As a result, CNN need to encode more patterns, as each image patch has a different
correlation pattern.
Test Model Architectures We conduct controlled experiments, with one (Fig. 2) or two (Fig. 3)
hidden layers being either L-conv or a baseline, followed by a classification layer. For CNN, L-conv
and L-conv with random Li , we used nf = ml = 32 for number of output filters (i.e. output
dimension of Wi). For CNN we used 3 × 3 kernels and equivalently used nL = 9 for the number of
Li in L-conv and random L-conv. As described in sec. 3, we encode Li as sparse matrices Li = UiVi
with hidden dimension dh = 16 in Fig. 2 and dh = 8 in Fig. 3, showing that very sparse Li can
perform well. The weights W i are each ml × ml+1 dimensional. The output of the L-conv layer is
d X mι+ι. As mentioned above, We use two FC baselines. The FC in Fig. 2 and FC(〜L-ConV) in Fig.
3 mimic L-conv, but lacks weight-sharing. The FC weights are W = ZV with V being (nLdh) × d
and Z being (ml+1 × d) × dh . For “FC (shallow)” in Fig. 3, we have one wide hidden layer with
u = nL-conv/(mdc), where nL-conv is the total number of parameters in the L-conv model, m and
c the input and output channels, and d is the input dimension. We experimented with encoding Li as
multi-layer perceptrons, but found that a single hidden layer with linear activation works best. We
also conduct tests with two layers of L-conv, CNN and FC (Fig. 3), with each L-conv, CNN and FC
layer as descried above, except that we reduced the hidden dimension in Li to dh = 8.
Baselines We compare L-conv against three baselines: CNN, random Li , and fully connected (FC).
Using CNN on scrambled images amounts to using poor inductive bias in designing the architecture.
Similarly, random, untrained Li is like using bad inductive biases. Testing on random Li serves to
verify that L-conv’s performance is not due to the structure of the architecture, and that the Li in
L-conv really learn patterns in the data. Finally, to verify that the higher parameter count in L-conv
is not responsible for the high performance, we construct two kinds of FC models. The first type
(“Fully Conn." in Fig. 2 and “FC (〜L-COnv)"in Fig. 3) is a multilayer FC network with the same
input (d × m0), hidden (k × nL for low-rank Li) and output (d × m1) dimensions as L-conv, but
laCking the weight-sharing, leading to muCh larger number parameters than L-Conv. The seCond type
(“FC (shallow)” in Fig. 3) Consists of a single hidden layer with a width suCh that the total number of
model parameters matCh L-Conv.
Results Fig. 2 shows the results for single layer experiments. On all four datasets both in the
rotated and the rotated and sCrambled Case L-Conv performed Considerably better than CNN and the
baselines. Compared to CNN, L-Conv naturally requires extra parameters to enCode Li , but low-rank
enCoding with rank dh d only requires O(dhd) parameters, whiCh Can be negligible Compared
to FC layers. We observe that FC layers Consistently perform worse than L-Conv, despite having
muCh more parameters than L-Conv. We also find that not training the Li (“Rand L-Conv”) leads to
signifiCant performanCe drop. We ran tests on the unmodified images as well (Supp. Fig 4), where
CNN performed best, but L-Conv trails Closely behind CNN.
Additional experiments testing the effeCt of number of layers, number of parameters and pooling are
shown in Fig. 3. On CIFAR100, we find that both FC configurations, FC(〜L-COnV) and FC(shallow)
Consistently perform worse than L-Conv, evidenCe that L-Conv’s performanCe is not due to its extra
parameters. L-ConV outperforms all other tested models on rotated and sCrambled CIFAR100. Without
pooling, we obserVe that both L-ConV and CNN do not benefit from adding a seCnd layer. This Can be
explained by Proposition 2, whiCh states that multi-layer L-ConV is still enCoding the same symmetry
group G, only CoVering a larger portion of G. Our hypothesis is that laCk of pooling is the reason
behind this, as we disCuss next.
Pooling on L-conv On Default CIFAR100, CNN with Maxpooling signifiCantly outperforms regular
CNN and L-ConV, indiCating the importanCe of proper pooling. Interestingly, on rotated and sCrambled
8
Under review as a conference paper at ICLR 2021
CIFAR10
MNlST, 1 Conv. or 2 FC Hidden Layers
>UE3WW< ⅛UF
SJs,υUJeJed #
CIFARIO, 1 Conv. or 2 FC Hidden Layers
Rotated	Rot. & Scrambled
Fashion mnist, i conv. or 2 f.c. Hidden Layers
0.83 -	―
X 0.82
I 0.81
§ 0.80
⅛jθ∙7g
g 0.78
0.77
w'
SJUJeJed #
FASHION MNIST
ClFARl00, 1 Conv. or2 FC Hidden Layers
SJ,υsUJeJed #
Rand L<onv
Fully Conn.
L<onv
CNN
Rot. & Scrambled
Be
SJ,υsUJeJed #
Rotated


Figure 2: Results on four datasets with two variant: “Rotated” and “Rotated and scrambled”. In all
cases L-ConV performs best. On MNIST, FC and CNN come close, but using 5x more parameters.
Figure 3: Comparison of one and two layer performance of L-conv (blue), CNN without pooling
(orange), CNN with Maxpooling after each layer (green), fully connected (FC) with structure similar
to L-conv (red) and shallow FC, which has a single hidden layer with width such that the total number
of parameters matches L-conv (purple). The labels indicate number of layers and layer architecture
(e.g. “2 L-conv” means two layers of L-conv followed by one classification layer). Left and middle
plots show test accuracies on CIFAR100 with rotated and scrambled images, and on the original
CIFAR100 dataset, respectively. The plot on the right show the number of parameters of each model,
which is the same for the two datasets.
CIFAR100, we find that max-pooling does not yield any improvement. We believe the role of pooling
is much more fundamental than simple dimensionality reduction. On images, pooling with strides
blurs our low-level features, allowing the next layer to encode symmetries at a larger scale. Cohen
& Welling (2016a) showed a relation between pooling and coset of subgroups and that strides are
subsampling the group to a subgroup H ⊂ G, resulting in outputs which are equivariant only under
H and not the full G. These subgroups appearing at different scales in the data may be quite different.
However, a naive implementation of pooling on L-conv may involve three Li and be quite expensive
(see Appendix C). Devising an efficient and mathematically sound pooling algorithm for L-conv is a
future step we are working on.
5 Discussions
We introduced the L-conv layer, a group equivariant layer based on Lie algebras of continuous groups.
We showed that many familiar architectures such as CNN, GCN and FC layers can be understood as
L-conv. L-conv is easy to setup and can learn symmetries during training. On domains with hidden
symmetries we find that an L-conv layer outperforms other comparable baseline layer architectures.
L-conv can in principle be inserted anywhere in an architecture. L-conv can also be composed in
multiple layers, though Proposition 2 suggest it would be approximating the same symmetry group,
and we did not observe significant improvements in performance in two layer tests. However, we
believe this is due to lack of pooling for L-conv. For CNN on images, coarse-graining (maxpooling
with strides) allow the system to find features at a different scale, whose symmetry may be a subset
of the lower scale (Cohen & Welling, 2016a) or perhaps new symmetries. Thus, one future work
9
Under review as a conference paper at ICLR 2021
direction is defining proper pooling (see Supp C). Lastly, to ensure Li behave like a Lie algebra
basis, we need to include regularizers enforcing orthogonality among the Li , which is another future
direction.
References
Fabio Anselmi, Georgios Evangelopoulos, Lorenzo Rosasco, and Tomaso Poggio. Symmetry-adapted
representation learning. Pattern Recognition, 86:201-208, 2019.
Alexander A Belavin, Alexander M Polyakov, and Alexander B Zamolodchikov. Infinite conformal
symmetry in two-dimensional quantum field theory. Nuclear Physics B, 241(2):333-380, 1984.
Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in
neural networks. arXiv preprint arXiv:2010.11882, 2020.
Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetry and invariant neural networks.
arXiv preprint arXiv:1901.06082, 2019.
Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups.
In International Conference on Machine Learning, pp. 1755-1763, 2014.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference
on machine learning, pp. 2990-2999, 2016a.
Taco S Cohen and Max Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016b.
Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on ho-
mogeneous spaces. In Advances in Neural Information Processing Systems, pp. 9142-9153,
2019.
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional
neural networks for equivariance to lie groups on arbitrary continuous data. arXiv preprint
arXiv:2002.12880, 2020.
Brian Hall. Lie groups, Lie algebras, and representations: an elementary introduction, volume 222.
Springer, 2015.
Jason Hartford, Devon R Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of
interactions across sets. arXiv preprint arXiv:1803.02879, 2018.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Anthony W Knapp. Lie groups beyond an introduction, volume 140. Springer Science & Business
Media, 2013.
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. arXiv preprint arXiv:1802.03690, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. In Advances in Neural Information Processing Systems, pp. 10117-
10126, 2018.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541-551, 1989.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
10
Under review as a conference paper at ICLR 2021
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector
field networks. In Proceedings of the IEEE International Conference on Computer Vision, pp.
5048-5057, 2017.
Rajesh PN Rao and Daniel L Ruderman. Learning lie groups for invariant visual perception. In
Advances in neural information processing systems, pp. 810-816, 1999.
Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. arXiv preprint arXiv:2002.02912,
2020.
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-sharing.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2892-
2901. JMLR. org, 2017.
Jascha Sohl-Dickstein, Ching Ming Wang, and Bruno A Olshausen. An unsupervised algorithm for
learning lie group transformations. arXiv preprint arXiv:1001.1027, 2010.
Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. In International Conference on Medical image computing and
computer-assisted intervention, pp. 210-218. Springer, 2018.
Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. In Advances in Neural
Information Processing Systems, pp. 14334-14345, 2019.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. In Advances in Neural Information
Processing Systems, pp. 10381-10392, 2018a.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation
equivariant cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 849-858, 2018b.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization.
arXiv preprint arXiv:2007.02933, 2020.
11