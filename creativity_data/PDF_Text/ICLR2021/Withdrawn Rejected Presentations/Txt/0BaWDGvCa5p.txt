Under review as a conference paper at ICLR 2021
A Provably Convergent and Practical Algo-
rithm for Min-Max Optimization with Applica-
tions to GANs
Anonymous authors
Paper under double-blind review
Ab stract
We present a first-order algorithm for nonconvex-nonconcave min-max optimiza-
tion problems such as those that arise in training GANs. Our algorithm provably
converges in poly(d, L, b) steps for any loss function f : Rd × Rd → R which
is b-bounded with L-Lipschitz gradient. To achieve convergence, we 1) give a
novel approximation to the global strategy of the max-player based on first-order
algorithms such as gradient ascent, and 2) empower the min-player to look ahead
and simulate the max-player’s response for arbitrarily many steps, but restrict the
min-player to move according to updates sampled from a stochastic gradient oracle.
Our algorithm, when used to train GANs on synthetic and real-world datasets, does
not cycle, results in GANs that seem to avoid mode collapse, and achieves a training
time per iteration and memory requirement similar to gradient descent-ascent.
1	Introduction
We consider the problem of min-max optimization minx∈Rd maxy∈Rd f(x, y), where the loss func-
tion f may be nonconvex in x and nonconcave in y. Min-max optimization of such loss functions has
many applications to machine learning, including to GANs (Goodfellow et al., 2014) and adversarial
training (Madry et al., 2018). In particular, following Goodfellow et al. (2014), GAN training can
be formulated as a min-max optimization problem where x encodes the parameters of a “generator”
network, and y encodes the parameters of a “discriminator” network. Unlike standard minimization
problems, the min-max nature of GANs makes them particularly difficult to train (Goodfellow,
2017), and has received wide attention. A common algorithm to solve these min-max optimization
problems, gradient descent ascent (GDA), alternates between stochastic gradient descent steps for x
and ascent steps for y.1 The advantage of GDA is that it just requires first-order access to f and each
iteration is efficient in terms of memory and time, making it quite practical. However, as many works
have observed, GDA can suffer from issues such as cycling (Arjovsky & Bottou, 2017) and “mode
collapse” (Dumoulin et al., 2017; Che et al., 2017; Santurkar et al., 2018).
Several recent works have focused on finding convergent first-order algorithms for min-max opti-
mization (Rafique et al., 2018; Daskalakis et al., 2018; Liang & Stokes, 2019; Gidel et al., 2019b;
Mertikopoulos et al., 2019; Nouiehed et al., 2019; Lu et al., 2020; Lin et al., 2020; Mokhtari et al.,
2019; Thekumparampil et al., 2019; Mokhtari et al., 2020). However, these algorithms are also not
guaranteed to converge for general nonconvex-nonconcave min-max problems. The challenge is
that min-max optimization generalizes nonconvex minimization, which, in general, is intractable.
Algorithms for nonconvex minimization resort to finding “local” optima or assume a starting point
“close” to a global optimum. However, unlike minimization problems where local notions of optima
exist (Nesterov & Polyak, 2006), it has been challenging to define a notion of convergent points for
min-max optimization, and most notions of local optima considered in previous works (Daskalakis &
Panageas, 2018; Jin et al., 2020; Fiez et al., 2019) require significant restrictions for existence.
Our contributions. Our main result is a new first-order algorithm for min-max optimization (Al-
gorithm 1) that for any ε > 0, any nonconvex-nonconcave loss function, and any starting point,
converges in poly(d, L, b, 1∕ε) steps, if f is b-bounded with L-LiPschitz gradient (Theorem 2.3).
1In practice, gradients steps are often replaced by ADAM steps; we ignore this distinction for this discussion.
1
Under review as a conference paper at ICLR 2021
A key ingredient in our result is an approximation to the global max function maxz∈Rd f (x, z).
Unlike GDA and related algorithms that alternate between updating the discriminator and generator
in an incremental fashion, our algorithm lets the discriminator run a convergent algorithm (such as
gradient ascent) until it reaches a first-order stationary point. We then empower the generator to
simulate the discriminator’s response for arbitrarily many gradient ascent updates. Roughly, at each
iteration of our algorithm, the min-player proposes a stochastic (batch) gradient update for x and
simulates the response of the max-player with gradient ascent steps for y until it reaches a first-order
stationary point. If the resulting loss has decreased, the updates for x and y are accepted; otherwise
they are only accepted with a small probability (a la simulated annealing).
The point (x? , y?) returned by our algorithm satisfies the following guarantee: if the min-player
proposes a stochastic gradient descent update to x? , and the max-player is allowed to respond by
updating y? using any “path” that increases the loss at a rate of at least ε — with high probability, the
final loss cannot decrease by more than ε. See Section 2 for our convergence guarantees, Section 4
for the key ideas in our proof, and Appendix C for a comparison to previous notions of convergence.
Empirically, we apply our algorithm for training GANs (with the cross-entropy loss) on both synthetic
(mixture of Gaussians) and real-world (MNIST and CIFAR-10) datasets (Section 3). We compare our
algorithm’s performance against two related algorithms: gradient/ADAM descent ascent (with one or
multiple discriminator steps), and Unrolled GANs (Metz et al., 2017). Our simulations with MNIST
(Figure 1) and mixture of Gaussians (Figure 2) indicate that training GANs using our algorithm can
avoid mode collapse and cycling. For instance, on the Gaussian mixture dataset, we found that by
around the 1500’th iteration GDA learned only one mode in 100% of the runs, and cycled between
multiple modes. In contrast, our algorithm learned all four modes in 68% of the runs, and three modes
in 26% of the runs. On 0-1 MNIST, we found that GDA tends to briefly generate shapes that look
like a combination of 0’s and 1’s, then switches between generating only 1’s and only 0’s. In contrast,
our algorithm seems to learn to generate both 0’s and 1’s early on and does not stop generating either
digit. GANs trained using our algorithm generated both digits by the 1000’th iteration in 86% of
the runs, while those trained using GDA only did so in 23% of the runs. Our CIFAR-10 simulations
(Figure 3) indicate that our algorithm trains more stably, resulting in a lower mean and standard
deviation for FID scores compared to GDA. Furthermore, the per-step computational and memory
cost of our algorithm is similar to GDA indicating that our algorithm can scale to larger datasets.
Related work
Guaranteed convergence for min-max optimization. Several works have studied GDA dynamics in
GANs (Nagarajan & Kolter, 2017; Mescheder et al., 2017; Li et al., 2018; Balduzzi et al., 2018;
Daskalakis & Panageas, 2018; Jin et al., 2020) and established that GDA suffers from severe
limitations: GDA can exhibit rotation around some points, or otherwise fail to converge. Thus, we
cannot expect global convergence guarantees for GDA. To address these convergence issues for GDA,
multiple works have proposed algorithms based on Optimistic Mirror Descent (OMD), Extra-gradient
method, or similar approaches (Gidel et al., 2019b; Daskalakis et al., 2018; Liang & Stokes, 2019;
Daskalakis & Panageas, 2019; Mokhtari et al., 2019; 2020). These algorithms avoid some of the
pathological behaviors of GDA and achieve guaranteed convergence in Poly(K, log(1∕ε)) iterations
where κ is the condition number of f. However, all these results either require convexity/concavity
assumptions on f, which usually do not hold for GANs, or require that the starting point lies in a small
region around an equilibrium point, and hence provide no guarantees for an arbitrary initialization.
Some works also provide convergence guarantees for min-max optimization (Nemirovski & Yudin,
1978; Kinderlehrer & Stampacchia, 1980; Nemirovski, 2004; Rafique et al., 2018; Lu et al., 2020;
Lin et al., 2020; Nouiehed et al., 2019; Thekumparampil et al., 2019). However, they require f to be
concave in y, again limiting their applicability.
As for nonconvex-nonconcave min-max optimization, Heusel et al. (2017) prove convergence of
finite-step GDA, under the assumption that the underlying continuous dynamics converge to a local
min-max optimum (this assumption may not even hold for f that is bi-linear). Jin et al. (2020)
present a version of GDA for min-max optimization (generalized by Fiez et al. (2019)) such that if the
algorithm converges, the convergence point is a local min-max optimum. Both these results require
that the min-player use a vanishingly small step size relative to the max-player, resulting in slow
convergence. Wang et al. (2020) present an algorithm that can converge for nonconvex-nonconcave
functions, but requires the initial point to lie in a region close a local min-max optimum (such
optima are not guaranteed to exist). In contrast to the above works, our algorithm is guaranteed to
2
Under review as a conference paper at ICLR 2021
converge for any nonconvex-nonconcave loss, from any starting point, in poly(d, L, b, 1∕ε) steps, if f
is b-bounded with L-Lipschitz gradient.
Greedy paths. The paths along which the max-player is allowed to make updates in our equilibrium
definition are inspired from the work of Mangoubi & Vishnoi (2020), which gives a second-order
algorithm for min-max optimization. The “greedy paths” considered in their work are defined such
that at every point along these paths, f is non-decreasing, and the first derivative of f is at least ε or
the 2nd derivative is at least √ε. In contrast, We just require a condition on the first derivative of f
along the path. This distinction gives rise to a different notion of equilibrium than the one presented
in their Work. The first-order condition on the paths crucially also results in our algorithm being
applicable to machine learning settings Where only first-order oracles are available, because unlike
Mangoubi & Vishnoi (2020), traversing such a path only requires first-order access to f.
Training GANs. Starting With GoodfelloW et al. (2014), there has been considerable Work to develop
algorithms to train GANs. One line of Work focuses on modifying the loss to improve conver-
gence (Arjovsky et al., 2017; Bellemare et al., 2017; Lim & Ye, 2017; Mao et al., 2017; Salimans
et al., 2018; Metz et al., 2017). Another line of Work regularizes the discriminator using gradient
penalties or spectral normalization (Gulrajani et al., 2017; Kodali et al., 2017; Miyato et al., 2018).
Metz et al. (2017) introduced Unrolled GANs, Where the generator optimizes an “unrolled” loss
function that alloWs the generator to simulate a fixed number of discriminator updates. While this
has some similarity to our algorithm there are tWo important distinctions: 1) the discriminator in
Unrolled GANs may not reach a first-order stationary point, and hence their algorithm does not come
With any convergence guarantees, and 2) unlike our algorithm, the implementation of the generator
in Unrolled GANs requires memory that groWs With the number of discriminator steps, limiting its
scalability. We observe that our algorithm, applied to training GANs, trains stably and avoids mode
collapse, While achieving a training time per iteration and memory requirements that are similar to
GDA, and much loWer than Unrolled GANs (Metz et al., 2017) (see also the discussion in Section 5).
Remark 1.1 (Loss functions in GANs). Loss functions f : Rd × Rd → R which take bounded values
on Rd × Rd arise in many GAN applications. For instance, GANs with mean-squared error loss
Mao et al. (2017) have uniformly bounded f. GANs with cross entropy loss Goodfellow et al. (2014)
have f uniformly bounded above, and Wasserstein GANs Arjovsky et al. (2017) have a loss function
f(x, y) which is bounded above as a function of y and is uniformly bounded below.
2	Theoretical results
We consider the problem minx maxy f(x, y), Where x, y ∈ Rd, and f is a function Rd ×Rd → R. We
consider f that is an empirical risk loss over m training examples. Thus, We have f = :^ Pi∈g] fi,
and are given access to f via a randomized oracle F such that E[F] = f. We call such an oracle a
stochastic zeroth-order oracle for f. We are also given randomized oracles Gχ, Gy for Vχf, Ny f,
such that E[Gχ] = Vχf, and E[Gy] = Vyf. We call such oracles stochastic gradient oracles for
f. In practice, these oracles are computed by randomly sampling a “batch” B ⊆ [m] and returning
F = 1/1B1 Pi∈B fi,Gχ = 1/1B1 Pi∈B Vxfi, and Gy = 1∕∣b∣ P° Vy f
For our convergence guarantees, We require bounds on standard smoothness parameters for
functions fi : b such that for all i and all x, y, We have |fi (x, y)| ≤ b, and L such that
kVfi(x, y) - Vfi(x0, y0)k2 ≤ Lkx - x0k2 + Lky - y0k2. Such smoothness/Lipschitz bounds are
standard in convergence guarantees for optimization algorithms (Bubeck, 2017; Nesterov & Polyak,
2006; Ge et al., 2015), and imply that f is also continuous, b-bounded, and L-gradient-Lipschitz.
Our algorithm is described informally in Algorithm 1 and formally as Algorithm 2 in the Appendix.
Intuition for the algorithm. To solve the min-max problem, the max-player Would ideally find
the global maximum maxz f(x, z). HoWever, since f may be nonconcave in y, finding the global
maximum may be computationally intractable. To get around this problem, roughly speaking, in our
algorithm the max-player computes its update by running gradient ascent until it reaches a first-order
ε-stationary point y0, that is, a point Where kVyf(x, y0)k ≤ ε. This alloWs our algorithm to compute
an approximation Lε(x, y) = f(x, y0) for the global maximum. (Note that even though maxz f(x, z)
is only a function of x, Lε may depend on both x and the initial point y.)
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Algorithm for min-max optimization
input: A stochastic zeroth-order oracle F for loss function f : Rd × Rd → R, and stochastic gradient
oracles Gx for Nxf, and Gy for Vyf. An initial point (x, y), and an error parameter ε.
output: A point (x? , y? )
hyperparameters: rmax (maximum number of rejections); τ1 (hyperparameters for annealing);
ADAM hyperparameters
Set r J 0,i J 0
while r ≤ rmax do
fold J F (x, y), i J i + 1
Set Gx J Gx (x, y) {Compute a stochastic gradient}
Use stochastic gradient Gx to compute a one-step ADAM update ∆ for x
Set x0 J x + ∆ {Compute the proposed update for the min-player}
Starting at point y, use stochastic gradients Gy (x0, ∙) to run multiple ADAM steps in the
y-variable, until a point y0 is reached such that kGy (x0, y0)k1 ≤ ε {Simulate max-player’s update}
Set fnew J F (x0, y0) {Compute the new loss value}
Set Accept J True.
if fnew > fold — ε∕2, Set Accept J False with probability max(0,1 — e-"τ1) {accept or reject}
if Accept = True then Set x J x0, y J y0, r J 0 {Accept the updates}
else Set r J r + 1 {Reject the updates, and track how many successive steps were rejected.}
return (x, y )
We would like the min-player to minimize Lε(x, y). Ideally, the min-player would make updates in
the direction -VxLε. However, Lε(χ, y) may not be differentiable and may even be discontinuous
in x (see Section 2.2 for an example), making it challenging to optimize. Moreover, even at points
where Lε is differentiable, computing VxLε may require memory proportional to the number of
max-player steps used to compute Lε (for instance, this is the case for Unrolled GANs (Metz et al.,
2017)). For this reason, we only provide our min-player with access to the value of Lε.
One approach to minimize Lε would be to use a zeroth-order optimization procedure where the
min-player proposes a random update to x, and then only accept this update if it results in a decrease
in Lε . At each iteration of our algorithm, the min-player proposes an update roughly in the direction
-Vx f (x, y ). To motivate this choice, note that once the min-player proposes an update ∆ to x, the
max-player’s updates will only increase f, i.e., Lε(x + ∆, y) ≥ f(x + ∆, y). Moreover, since y is a
first-order stationary point of f (x, ∙) (because y was computed using gradient ascent in the previous
iteration), we also have Lε(x, y) = f(x, y). Therefore, we want an update ∆ such that
f(x+∆,y) ≤ Lε(x+∆,y) ≤ Lε(x,y) = f(x, y),	(1)
which implies that any proposed step which decreases Lε must also decrease f (although the converse
is not true). This motivates proposing steps in the direction of the gradient -Vx f (x, y).
Unfortunately, updates in the direction -Vx f (x, y) do not necessarily decrease Lε. Our algorithm
instead has the min-player perform a random search by proposing a stochastic update in the direction
of a batch gradient with mean -Vxf(x, y) (or, more precisely, the ADAM batch gradients), and
accepts this update only if Lε decreases by some fixed amount. We show empirically that these
directions allow the algorithm to rapidly decrease the simulated loss. The fact that Lε decreases
whenever the min-player takes a step allows us to guarantee that our algorithm eventually converges.
A final issue is that converging to a local minimum point does not guarantee that the point is desirable
from an applications standpoint. To allow our algorithm to escape undesirable local minima of
Lε(∙, y), we use a randomized accept-reject rule inspired by simulated annealing - if the resulting
loss has decreased the updates for x and y are accepted; otherwise they are only accepted with a
small probability e-"τ1, where τι is a “temperature” parameter.
2.1 Convergence guarantees
We first formally define “simulated loss” and what it means for f to increase rapidly.
Definition 2.1. For any x, y, and ε > 0, define E(ε, x, y) ⊆ Rd to be points w s.t. there is a
continuous and (except at finitely many points) differentiable path γ(t) starting at y, ending at w,
4
Under review as a conference paper at ICLR 2021
and moving with “speed” at most 1 in the '∞-norm2 ∣∣ 奈 γ(t) ∣∣∞ ≤ 1 Such that at any point on γ,3
ddtf (x,γ(t)) > ε	⑵
We define Lε(x, y) := supw∈E(ε,x,y) f(x, w), and refer to it as the simulated loss.
AfeW remarks are in order. Observe that L-∞(x, y) = maxy f (x, y). Further, if ∣∣Vyf (x, y)kι ≤ ε,
then E(ε, x, y) = {y} and Lε(x, y) = f (x, y) (this follows from Holder,s inequality, since γ(t) has
speed at most 1 in the '∞-norm, the dual of the 'ι-norm). Note that the path Y need not be in the
direction of the gradient, and there can potentially be infinitely many such paths starting at y.
Unfortunately, the simulated loss may not even be continuous4 in x, and thus, gradient-based
notions of approximate local minima do not apply. To bypass this discontinuity (and hence non-
differentiability), we use the idea to sample updates to x, and test whether Lε has decreased (equa-
tion 35). This leads to the following definition of local min-max equilibrium (see also Section
2.3):
Definition 2.2. Given a distribution Dx,y for each x, y ∈ Rd and ε? > 0, we say that (x?, y?) is an
ε?-local min-max equilibrium with respect to the distribution D if
∣Vyf(x?,y?)ki ≤ ε?,	and ,	(3)
Pr△〜Dx*,y? [Lε? (x? + ∆, y?) < Lε? (x?, y?) - ε?] < ε?,	(4)
In our main result, we set D to roughly be the distribution of ADAM stochastic gradients for -Vxf.
Also note that from the above discussion, equation 34 implies that Lε? (x?, y?) = f(x?, y?). To
allow convergence of ADAM (and avoid non-convergence issues such as those encountered in Reddi
et al. (2019)), in our main result we constrain the values of the ADAM hyperparameters 5. Now, we
can state the formal guarantees of our algorithm.
Theorem 2.3. Algorithm 2, with appropriate hyperparameters for ADAM and some constant τ1 > 0,
given access to stochastic zeroth-order and gradient oracles for a function f = i∈[m] fi where
each fi is b-bounded with L-Lipschitz gradient for some b, L > 0, and ε > 0, with probability at
least 9∕i0 returns a point (x?, y?) ∈ Rd X Rd such that, for some ε? ∈ [2ε, ε], the point (x?, y?)
is an ε?-local min-max equilibrium point with respect to the distribution Dxy, where Dxy is the
distribution of GX(x, y)/,GX(x, y), where the division “/” is applied element-wise. The number of
stochastic gradient and function evaluations required by the algorithm is poly(1∕ε, d, b, L).
We present key ideas in the proof in Section 4, and a proof overview in Section B. The full proof
appears in Appendix D. Note that Dx,y is the distribution of the stochastic gradient updates with
element-wise normalizations. Roughly, this corresponds to the distribution of ADAM steps taken by
the algorithm for updating x if one uses a small step-size parameter for ADAM.
We conclude this section with a few technical remarks about the theorem. Our algorithm could
provide guarantees with respect to other distributions Dx,y in equation 35 by sampling update steps
for x from Dx,y instead of ADAM. The norm in the guarantees of the stationary point y? for our
algorithm is `1 since we use ADAM for updating y. A simpler version of this algorithm using SGD
would result in an '2-norm guarantee.
Comparison to notions of local optimality. Definition 2.2 provides a new notion of equilibrium
for min-max optimization. Consider the problem minx maxy f(x, y), but with constraints on the
player’s updates to x, y — the max player is restricted to updating y via a path which increases
f(x, y) at a rate of at least ε? at every point on the path, and the min player proposes an update for
x, ∆ sampled from D. Then (x?, y?) satisfies Definition 2.2 if and only if (i) there is no update to
y? that the max player can make which will increase the loss at a rate of at least ε? (equation 34),
and, (ii) with probability at least 1 一 ε? for a random step ∆ 〜D proposed by the min player, the
above-constrained max player can update y? s.t. the overall decrease in the loss is at most ε? from its
original value f(x?, y?).
2We use the '∞ -norm in place of the Euclidean '2-norm, as it is a more natural norm for ADAM gradients.
3In this equation the derivative 奈 is taken from the right.
4Consider the example f(x, y) = min(x2y2, 1). The simulated loss function for ε > 0 is Lε(x, y) =
f(x, y) if 2x2y < ε, and 1 otherwise. Thus L1/2 is discontinuous at (1/2, 1).
5In particular, we set the ADAM hyperparameters β1 , β2 to be β1 = β2 = 0.
5
Under review as a conference paper at ICLR 2021
As one comparison to a previous notion of local optimality, any point which is a local optimum under
the definition used previously e.g. in Daskalakis & Panageas (2018), also satisfies our Definition 2.2
for small enough ε and distribution D corresponding to small-enough step size. On the other hand,
previous notions of local optima including the one in Daskalakis & Panageas (2018) are not guaranteed
to exist in a general setting, unlike our definition. (See Appendix C for a detailed comparison of how
Definition 2.2 relates to previously proposed notions of local optimality).
3	Empirical Results
We seek to apply our min-max optimization algorithm for training GANs on both real-world and
synthetic datasets. Following Goodfellow et al. (2014), we formulate GAN training as a min-max
optimization problem using the cross entropy loss, f(x, y) = log(Dy(ζ)) + log(1 - Dy(Gx(ξ)),
where x, y are the weights of the generator and discriminator networks G and D respectively, ζ is
sampled from the data, and ξ 〜N(0, Id). For this loss, the smoothness parameters b, L may not be
finite. To adapt Alg. 1 to training GANs, we make the following simplifications in our simulations:
(1) Temperature schedule: We use a fixed temperature τ, constant with iteration i, making it simpler
to choose a good temperature value rather than a temperature schedule. (2) Accept/reject rule: We
replace the randomized acceptance rule with a deterministic rule: If fnew ≤ fold we accept the
proposed step, and if few > fold We only accept if i is a multiple of e/, corresponding to an
average acceptance rate of e-1/T. (3) Discriminator steps: We take a fixed number of discriminator
steps at each iteration, instead of taking as many steps needed to achieve a small gradient.
These simplifications do not seem to significantly affect our algorithm’s performance (see Appendix
F.5 for simulations shoWing it effectively trains GANs Without most of these simplifications). More-
over, our simulations shoW a smaller number of discriminator steps k is usually sufficient in practice.
Datasets and Metrics. We perform simulations on MNIST (LeCun et al., 2010) and CIFAR-10
(Krizhevsky et al.) datasets to evaluate Whether GANs trained using our algorithm converge, and
Whether they are able to learn the target distribution. Convergence is evaluated by visual inspection
(for MNIST and CIFAR), and by tracking the loss (for MNIST) and FID scores (for CIFAR).
As noted by previous Works (Borji, 2019; Metz et al., 2017; Srivastava et al., 2017), it is challenging
to detect mode collapse on CIFAR and MNIST, visually or using standard quantitative metrics such
as FID scores, because CIFAR (and to some extent MNIST) do not have Well-separated modes. Thus,
We consider tWo datasets, one real and one synthetic, With Well-separated modes, Whence mode
collapse can be clearly detected by visual inspection.
For the real dataset We consider the 0-1 MNIST dataset (MNIST restricted to digits labeled 0 or
1). The synthetic dataset consists of 512 points sampled from a mixture of four Gaussians in tWo
dimensions With standard deviation 0.01 and means at (0, 1), (1, 0), (-1, 0) and (0, -1).
Hyperparameters and hardware. The details of the netWorks and hyperparameter choices are
given in Appendix E. Simulations on MNIST and Gaussian datasets used four 3.0 GHz Intel Scalable
CPUs, provided by AWS. On CIFAR-10, We used one High freq. Intel Xeon E5-2686 v4 GPU.
3.1	Evaluating the performance of our algorithm
We compare our algorithm’s performance to both GDA and unrolled GANs. All algorithms are
implemented using ADAM (Kingma & Ba, 2015).
MNIST. We trained a GAN on the full MNIST dataset using our algorithm for 39,000 iterations
(with k = 1 discriminator steps and acceptance rate e-1/T = 1∕5). We ran this simulation five times;
each time the GAN learned to generate all ten digits (see Appendix F.1 for generated images).
0-1 MNIST. We trained a GAN using our algorithm on the 0-1 MNIST dataset for 30,000 iterations
and ploted a moving average of the loss values. We repeated this simulation five times; in each of the
five runs our algorithm learned to generate digits which look like both the “0” and “1” digits, and the
loss seems to decrease and stabilize once our algorithm learns how to generate the two digits. (See
Appendix F.3 for the generated images and loss value plots.)
6
Under review as a conference paper at ICLR 2021
GDA	Our algorithm
DDElD ΠDDΠ QODD DΠOQ D^QΠ DΠD□ DQQD □ΠQQ HDΞΠ BSHΠ ΠDDQ ΠΠ□EI ΞΠOΠ QΠΠD nπnπ πππn π□od dħπξ desqξ 250	500	1,000	2,000	3,000	□BOO QQ□□ DΞDH BΠΠΞ ΠKIQΞ HΞDΠ ΠΠBK ΞDDD ΠD□Ξ DΠΠΠ ΞΞDD QQDD DOBD DQQQ QΠΞΠ ΞDOO ΞΞΞO E1HQΞ ΞQBH DΠKIΞ 250	500	1,000	2,000	3,000
Figure 1: We trained a GAN using our algorithm on 0-1 MNIST for 30,000 iterations (with k = 1 discriminator
steps and acceptance rate e-1/T = 1∕5). We repeated this experiment 22 times for our algorithm and 13 times for
GDA. Shown here are the images generated from one of these runs at various iterations for our algorithm (right)
and GDA (left) (see also Appendix F.3 for images from other runs).
Figure 2: Our algorithm (bottom right), unrolled GANs with k = 6 unrolling steps (top right), and GDA with
k = 1 (top left) and k = 6 discriminator steps (bottom left). Each algorithm was trained on a 4-Gaussian
mixture for 1500 iterations. Our algorithm used k = 6 discriminator steps and acceptance rate e-1/T = 1∕4.
Plots show the points generated by each of these algorithms after the specified number of iterations.
Unrolled GANs with 6 unrolling steps
. .#.	战.	■			—;— 1•♦
0	200	400	600	1000	1500
Our algorithm
0	200	400	600	1000	1500
CIFAR-10. We ran our algorithm (with k = 1 discriminator steps and acceptance rate e-1/T = 1∕2)
on CIFAR-10 for 50,000 iterations. We compare with GDA with k = 1 discriminator steps. We
plotted the FID scores for both algorithms. We found that both algorithms have similar FID scores
which decrease over time, and produce images of similar quality after 50,000 iterations (Figure 3).
Clock time per iteration. When training on the 0-1 MNIST dataset (with k = 1 discriminator steps
each iteration), our algorithm took 1.4 seconds per iteration on the AWS CPU server. On the same
machine, GDA took 0.85 seconds per iteration. When training on CIFAR-10, our algorithm and GDA
both took the same amount of time per iteration, 0.08 seconds, on the AWS GPU server.
Mitigating mode-collapse on 0-1 MNIST. We trained GANs using both GDA and our algorithm on
the 0-1 MNIST dataset, and ran each algorithm for 3000 iterations (Figure 1). GDA seems to briefly
generate shapes that look like a combination of 0’s and 1’s, then switches to generating only 1’s, and
then re-learns how to generate 0’s. In contrast, our algorithm seems to learn how to generate both 0’s
and 1’s early on and does not mode collapse to either digit.
We repeated this simulation 22 times for our algorithm and 13 times for GDA, and visually inspected
the images at iteration 1000. GANs trained using our algorithm generated both digits by the 1000’th
iteration in 86% of the runs, while those trained using GDA only did so in 23% of the runs at the
1000’th iteration (see Appendix F.4 for images from all runs).
Mitigating mode-collapse on synthetic data. We trained GANs on the 4-Gaussian mixture dataset
for 1500 iterations (Figure 2) using our algorithm, unrolled GANs with k = 6 unrolling steps, and
GDA with k = 1 and k = 6 discriminator steps. We repeated each simulation 10-20 times.
By the 1500’th iteration GDA with k = 1 discriminator steps seems to have learned only one mode
in 100% of the runs. GDA with k = 6 discriminator steps learned two modes 65% of the runs, one
mode 20% of runs, and four modes 15% of runs. Unrolled GANs learned one mode 75% of the runs,
two modes 15% of the runs, and three modes 10% of the runs. In contrast, our algorithm learned all
four modes 68% of the runs, three modes 26% of the runs, and two modes 5% of the runs.
7
Under review as a conference paper at ICLR 2021
GDA
Figure 3: GAN trained using our algorithm (with k = 1 discriminator steps and acceptance rate e-1/T = 1∕2)
and GDA on CIFAR-10 for 50,000 iterations. The images generated from the resulting generator for both our
algorithm (middle) and GDA (left). Over 9 runs, our algorithm achieves a very similar minimum FID score
(33.8) compared to GDA (33.0), and a better average FID score over 9 runs (mean μ = 35.6, std. dev. σ = 1.1)
compared to GDA (μ = 53.8, σ = 53.9). Images are shown from one run each; see Appendix F.2 for full results.
Our algorithm
4	Key Ideas in the Proof
For simplicity, assume b = L = τ1 = 1. There are two key pieces to proving Theorem 2.3. The first
is to show that our algorithm converges to some point (x?, y?) in poly(d, 1∕ε) gradient and function
evaluations (Lemma D.7). Secondly, we show that, y? is a first-order ε-stationary point for f (x?, ∙)
and x? is, roughly, a ε-local minimum for the simulated loss function Lε(∙, y?) (LemmaD.9).
Step1: Bounding the number of gradient evaluations: After Θ(log(ɪ)) steps, the decaying accep-
tance rate of the simulated annealing step ensures that our algorithm stops whenever『m&x = O(1∕ε)
proposed steps are rejected in a row. Thus, for every O(rmaχ∕ε2) iterations where the algorithm
does not terminate, with probability at least 1 - ε the value of the loss decreases by more than ε.
Since f is 1-bounded, this implies our algorithm terminates after roughly O(rmaχ∕ε3) iterations of the
minimization routine (Proposition D.6).
Next, since f is 1-bounded with 1-Lipschitz gradient, in each iteration, we require at most poly(d∕ε)
gradient ascent steps to reach an ε-stationary point. Since each step of the maximization subroutine
requires one gradient evaluation, and each iteration of the minimization routine calls the maximization
routine exactly once, the total number of gradient evaluations is poly(d, 1∕ε).
Step 2: Show x? is an ε-local minimum for Lε(∙,y?) and y? is an ε-stationary point. First, we
note that since our algorithm runs the gradient ascent maximization subroutine until it reaches an
ε-stationary point, we have, kVyf (x*,y*)∣∣ι ≤ ε.
Our stopping condition for the algorithm implies the last rmax updates ∆ proposed by the max-player
were all rejected, and hence were sampled from the distribution Dx?,y? of the ADAM gradient at
(x?, y?). Roughly, this implies
Pr△〜Dχ*,y* [f (x? + ∆, y0) ≥ f(x*, y?) - ε] ≥ 1 - ε,	(5)
where the maximization subroutine computes y0 by gradient ascent on f (x? + △, ∙) initialized at y?.
In other words, equation 5 says that at the point (x?, y?) where our algorithm stops, if the min-player
samples an update △ from the distribution Dx*,y*, followed by the max-player updating y? using
gradient ascent, with high probability the final loss value cannot decrease by more than ε.
To show equation 35 holds, we need to replace f in the above equation with the simulated loss Lε .
We first show that the gradient ascent steps form an "E-increasing" path, starting at y? with endpoint
y0, along which f increases at rate at least ε (Prop. D.8). This crucially exploits that our algorithm
restricts the max player to only use such "ε-increasing" paths. Since Lε is the supremum of f at the
endpoints of all such ε-increasing paths starting at y? , we get
f(x? + △, y0) ≤ Lε(x? + △, y?).	(6)
Finally, recall from Section 2 that kVyf(x?, y?)k1 ≤ ε? implies Lε(x?, y?) = f(x?, y?). Combining
the above observations implies the ε-local minimum condition equation 35.
Note that we could pick any distribution D for the updates and the proof still holds - the distribution
of ADAM gradients works well in practice. Also, we could replace simulated annealing with a
deterministic rule, but such an algorithm often gets stuck at poor local equilibria in GAN training.
8
Under review as a conference paper at ICLR 2021
5	Conclusion
In this paper, we develop a convergent first-order algorithm for min-max optimization and show
how it can lead to a stable and scalable algorithm for training GANs. We prove that our algorithm
converges in time polynomial in the dimension and the smoothness parameters of the loss function.
Our simulations show that a version of our algorithm can lead to more stable training of GANs on
synthetic and real-world datasets. And yet the amount of memory and time required by each iteration
of our algorithm is competitive with GDA. Our algorithm synthesizes a first-order approximation
to the global strategy of the max-player, a look ahead strategy based on batch gradients for the min-
player, and simulated annealing. We believe that these ideas of imposing computational restrictions
on the min- and max-players should be useful in obtaining convergent and practical algorithms for
other applications of min-max optimization, such as adversarial learning.
References
Jacob Abernethy, Kevin A Lai, and Andre Wibisono. Last-iterate convergence rates for min-max
optimization. arXiv preprint arXiv:1906.02027, 2019.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223, 2017.
Rowel Atienza.	GAN by example using keras on tensorflow backend.
“https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-
1a6d515a60d0”, 2017.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel.
The mechanics of n-player differentiable games. In Jennifer Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
ofMachine Learning Research, pp. 354-363, Stockholmsmassan, Stockholm Sweden, 10-15 Jul
2018. PMLR.
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein
gradients. CoRR, abs/1705.10743, 2017. URL http://arxiv.org/abs/1705.10743.
Jerome Bolte, Lilian Glaudin, Edouard Pauwels, and Mathieu Serrurier. Ah\” olderian backtracking
method for min-max and min-min problems. arXiv preprint arXiv:2007.08810, 2020.
Ali Borji. Pros and cons of gan evaluation measures. Computer Vision and Image Understanding,
179:41-65, 2019.
Jason Brownlee. How to develop a GAN to generate CIFAR10 small color pho-
tographs. “https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-
for-a-cifar-10-small-object-photographs-from-scratch/”, 2019.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning, 8, 2017.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. In International Conference on Learning Representations, ICLR, 2017.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9236-9246.
Curran Associates, Inc., 2018.
9
Under review as a conference paper at ICLR 2021
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. In 10th Innovations in Theoretical Computer Science Confer-
ence, ITCS 2019, January 10-12, 2019, San Diego, California, USA, pp. 27:1-27:18, 2019. doi:
10.4230/LIPIcs.ITCS.2019.27.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with
optimism. In International Conference on Learning Representations, 2018.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro,
and Aaron C. Courville. Adversarially learned inference. In 5th International Conference on
Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings, 2017.
Tanner Fiez, Benjamin Chasnov, and Lillian J. Ratliff. Convergence of learning dynamics in
stackelberg games. CoRR, abs/1906.01217, 2019. URL http://arxiv.org/abs/1906.
01217.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A
variational inequality perspective on generative adversarial networks. In International Conference
on Learning Representations (ICLR 2019), 2019a.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le Priol, Gabriel Huang,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of Machine Learning Research,
volume 89 of Proceedings of Machine Learning Research, pp. 1802-1811. PMLR, 16-18 Apr
2019b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160,
2017.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in neural information processing systems,
pp. 5767-5777, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In ICML 2020. International Machine Learning Society (IMLS), 2020.
Renu Khandelwal.	Generative adversarial network (GAN) using keras.
“https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-
ce1c05cfdfd3”, 2019.
David Kinderlehrer and Guido Stampacchia. An introduction to variational inequalities and their
applications, volume 31. Siam, 1980.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference for Learning Representations, pp. 1595-1625, 2015.
Naveen Kodali, Jacob D. Abernethy, James Hays, and Zsolt Kira. How to train your DRAGAN.
CoRR, abs/1705.07215, 2017. URL http://arxiv.org/abs/1705.07215.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
10
Under review as a conference paper at ICLR 2021
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of first-order
approximation in GAN dynamics. In International Conference on Machine Learning, pp. 3011-
3019, 2018.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 907-915, 2019.
Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.
Qihang Lin, Mingrui Liu, Hassan Rafique, and Tianbao Yang. Solving weakly-convex-weakly-
concave saddle-point problems as weakly-monotone variational inequality. arXiv preprint
arXiv:1810.10207, 2018.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave minimax
problems. In 37th International Conference on Machine Learning, ICML 2020. International
Machine Learning Society (IMLS), 2020.
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive ap-
proximation for one-sided non-convex min-max problems: algorithms and applications. IEEE
Transactions on Signal Processing, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ICLR, 2018.
Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided
evolutionary strategies: augmenting random search with surrogate gradients. In International
Conference on Machine Learning, pp. 4264-4273, 2019.
Oren Mangoubi and Nisheeth K Vishnoi. A second-order equilibrium in nonconvex-nonconcave
min-max optimization: Existence and algorithm. arXiv preprint arXiv:2006.12363, 2020.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 2794-2802, 2017.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
(gradient) mile. In International Conference on Learning Representations (ICLR 2019), 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=B1QRgziT-.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. Proximal point approximations achieving a
convergence rate of o (1/k) for smooth convex-concave saddle point problems: Optimistic gradient
and extra-gradient methods. arXiv preprint arXiv:1906.01115, 2019.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artificial Intelligence and Statistics, pp. 1497-1507. PMLR, 2020.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
11
Under review as a conference paper at ICLR 2021
Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with
Lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAMJournaIon Optimization,15(1):229-251, 2004.
Arkadi S Nemirovski and David Berkovich Yudin. Cesari convergence of the gradient method of
approximating saddle points of convex-concave functions. In Doklady Akademii Nauk, volume
239, pp. 1056-1059. Russian Academy of Sciences, 1978.
Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.
Mathematical Programming, 108(1):177-205, 2006.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solv-
ing a class of non-convex min-max games using iterative first order methods. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d' Alche-Buc, E. Fox, and R. Garnett (eds.), Advances inNeural
Information Processing Systems 32, pp. 14905-14916. Curran Associates, Inc., 2019.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal
transport. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=rkQkBnJAb.
Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry. A classification-based study of covariate
shift in GAN distributions. In International Conference on Machine Learning, pp. 4480-4489.
PMLR, 2018.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan:
Reducing mode collapse in gans using implicit variational learning. In Advances in Neural
Information Processing Systems, pp. 3308-3318, 2017.
Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient
algorithms for smooth minimax optimization. NeurIPS, 2019.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In International Conference on Learning Representations, 2020.
12
Under review as a conference paper at ICLR 2021
A The full algorithm
Algorithm 2 Algorithm for min-max optimization (formal version)
input: Stochastic zeroth-order oracle F for loss function f : Rd × Rd → R, and stochastic gradient
oracle Gx for Nxf, and Gy for Vyf
input: Initial point (xo, yo), Error parameter ε
output: A point (x? , y? )
hyperparameters: rmax (max number of rejections); τ1 > 0 (hyperparameter for simulated anneal-
ing); hyperparameters α, η, δ > 0 and β1, β2 ∈ [0, 1] for ADAM
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
Set i4—0, r4—0, a4—0, mo4—0, vo4—0, m4—0, V4—0, S4—0, ε 1 = 2
Set fold - ∞ {Set fold to be ∞ (or the largest possible value allowed by the computer) to
ensure that the first step is accepted}
while r ≤ rmax do
Seti-i+1
Set gx,i - Gx (xi, yi) {Compute proposed stochastic gradient}
Set Mi+1 - β1mi + (1 - β1)gx,i, and Vi+1 - β2vi + (1 - β2)gx2,i {Compute proposed ADAM
update for first- and second- moment estimates}
Set Xi+ι J Xi — α --1a+ι Mi+ι∕(. 1-1+ι %+ι + δ) { Compute proposed ADAM update of
1-β1	1-β2
x-variable}
Run Algorithm 3 with inputs x J Xi+1, yo J yi, (m, v, s), and ε0 J εi(1 + η).	Use ADAM optimizer to
Set Yi+1 J ystationary and (M, V, S) J (mout,vout,sout) to be the outputs of Alg. 3.	simulate the
Set fnew J F(Xi+1,Yi+1)	max player’s response.
if fnew > fold — 4 , then
___________________________________________________ i
Set Accepti J False with probability max(0,1 — e τι ) {Decide to accept or reject}
if Accepti = True then
Set xi+1 J Xi+1, yi+1 J Yi+1 {accept the proposed x and y updates}
Set mi+1 J Mi+1, vi+1 J Vi+1, and (m, v, s) J (M, V, S) {accept the proposal’s ADAM
moment estimates}
Set fold J fnew
Set a J a + 1 {keep track of how many steps we accepted}
Set r J 0
Set εi+1 J εi(1 + η)2
else
Set xi+1 J xi, yi+1 J yi {go back to the previous x and y values}
Set mi+1 J mi, vi+1 J vi {go back to the previous ADAM moment estimates}
r J r + 1 {Keep track of how many steps were rejected since the last acceptance. If too
many steps were rejected in a row, stop the algorithm and output the current weights.}
Set εi+1 J εi
return (x?, y?) J (xi, yi)
13
Under review as a conference paper at ICLR 2021
Algorithm 3 ADAM (for the max-player updates)
input: Stochastic gradient oracle Gy for Vy f
input: x, y0, m, v, ε0, number of steps s taken at previous iterations
output: A point ystationary Which is a first-order ε0-stationαry point for f (x, ∙), and Sout, m°ut, VoUt
hyperparameters: η >	0; ADAM hyperparameters α, δ >	0 and β1,β2	∈
[0, 1]
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
Set j - 0
Set Stop = False
while Stop = False do
Set j J j + 1
Set gy,j J Gy(X, yj)
if kgy,j k1 > ε0 then
Set j J j + 1
Set mj J β1mj + (1 - β1)gy,j, and vj J β2vj + (1 - β2)gy2,j {Compute proposed ADAM
update for first- and second- moment estimates}
Set yj+1 J Yj + α 1-β1+j+ι mj/(JI-&j+i Vj
y-variable}
else
+ δ) {Compute proposed ADAM update of
Set Stop = True
Set ystationary J yj and sout J s + j , mout J mj , Vout J Vj
B	Proof overview
To prove Theorem 2.3 We Would like to shoW tWo things: First, that our algorithm converges to some
point (x?, y?) in a number of gradient and function evaluations which is polynomial in * 1∕ε, d, b, L
(Lemma D.7, in the appendix). Secondly, roughly, We Would like to shoW y? is a first-order ε-
stationary point for f (x?, ∙) and x? is a ε-local minimum for the simulated loss function Lε(∙,y?)
(Lemma D.9).
Bounding the number of gradient and function evaluations (Lemma D.7) First, we bound the
number of gradient evaluations for the maximization subroutine for y (Line 6 of Algorithm 1), and
then bound the number of iterations in the minimization routine for x (the “While” loop in Alg. 1).
Step 1: Bounding the number of gradient ascent steps for the maximization subroutine: Consider the
sequence of ADAM gradient ascent steps y = y1,y2 ...,yg = y0 the max-player uses to compute
her update y0 in Line 6 of Alg. 1. For our choice of hyperparameters, the ADAM update is yj+1 =
yj + αGy(x, yj)/(Gy2 (x, yj))1/2, where α is the ADAM learning rate. Since the magnitude of our
ADAM gradient satisfies kGy(x, yj)/(G2y (x, yj))1/2k2 = kGy(xi, yj)k1, our stopping condition for
gradient ascent (Line 6 of Alg. 1), which says that gradient ascent stops whenever kGy(x, yj)k1 ≤ ε,
implies that at each step of gradient ascent our ADAM update has magnitude at least ɑε in '2-norm.
Using this, we then show that at each step yj of gradient ascent, we have with high probability that
f (yj+ι) — f (yj) ≥ Ω(αε),	⑺
if the ADAM learning rate satisfies α ≈ Θ(ε∕(Ld)) (Proposition D.5). Since f is b-bounded, this
implies that the ADAM gradient ascent subroutine must terminate after O(b∕(αε)) = O(bLd∕ε) steps.
Step 2: Bounding the number of iterations for the minimization routine: We first show a concentration
bound for our stochastic zeroth-order oracle (Proposition D.3) and use it to show that, for our
temperature schedule of Ti = τι∕i, after I := τι log(2rmaχb∕ε2) iterations, our algorithm rejects any
proposed step x0 = x + ∆ for which f(x0, y0) > f(x, y) - ε with probability at most roughly
1 一 e-1/Ti ≤ 1 一 ε2∕(2rmaχb). Therefore, roughly, with probability at least 1 一 ε, for the first ?TmaXb∕
iterations after I, we have that only proposed steps x0 = x + ∆ for which f(x0, y0) ≤ f(x, y) 一 ε
are accepted. Moreover, our stopping condition (Line 2 in Alg. 1) stops the algorithm whenever rmax
proposed steps are rejected in a row. Therefore, with probability at least 1 一 ε we have that the value
2
of the loss decreases by more than 2b∕ε between iterations I and I + rmaxb∕ε2, unless our algorithm
terminates during these iterations. Since f is b-bounded, this implies our algorithm must terminate
after roughly O(rm2 axb∕ε2) iterations of the minimization routine (Proposition D.6).
14
Under review as a conference paper at ICLR 2021
Now, each of the O(bLd∕ε) steps of the maximization subroutine requires one gradient evaluation, and
each of the O(rr2axb∕ε2) iterations of the minimization routine calls the maximization routine exactly
once (and makes one call to stochastic oracles for the function f and the x-gradient). Therefore, the
total number of gradient and function evaluations is roughly bLd∕ε X r2aχb∕ε2, which, for our choice
of hyperparameter rmaχ of roughly rmaχ = 1∕ε, is polynomial in 1∕ε, d, b, L.
Showing (x?, y?) satisfies Inequalities equation 34 and equation 35 (Lemma D.9)
Step 1: Show y? is a first-order ε-stationary pointfor f (x?, ∙) (equation 34) Our stopping condition
for the maximization subroutine says kGy(x?, y?)k1 ≤ ε at the point y? where the subroutine
terminates. To prove equation 34, we show a concentration bound for our stochastic gradient Gy
(the second part of Proposition D.3) and use this to show that the bound kGy(x?, y?)k1 ≤ ε on the
stochastic gradient obtained from the stopping condition implies the desired bound on the exact
gradient, ∣Nyf (x?,y?)ki ≤ ε.
Step 2: Show x? is an ε-local minimum for Lε(∙, y?) (equation 35) First, we show our stopping
condition for the minimization routine implies the last rmax steps ∆ proposed by the algorithm were
all rejected. This implies the last rmax proposed steps were sampled from the distribution Dx?,y?
of the ADAM gradient Gx(x?, y?)/(Gx2 (x?, y?))1/2 and, since they were rejected, our stopping
condition implies, roughly, f(x? + ∆, y?) > f(x?, y?) - ε for all these proposed steps ∆. Roughly,
we use this, together with our Poly(1∕ε, d, b, L) bound on the number of iterations (Proposition D.6),
to show
Pr△〜Dx?,y? [f (x? + ∆, y0) ≥ f (x*, y?) - ε] ≥ 1 - ε,	(8)
where the maximization subroutine computes y0 by gradient ascent on f (x* + △, ∙) initialized at y?.
To show that equation 35 holds, roughly, we would like to replace “f” in the bound in Ineq. equation 8
with the simulated loss function Lε . Towards this end, we first show that the steps traced by the
gradient ascent maximization subroutine form an "E-increasing" path, with endpoint y0, along which
f increases at rate at least ε (Prop. D.8). Although we would ideally like to use this fact to show that
f(x? + △, y0) = Lε(x? + △, y?), this equality does not hold in general since Lε is defined using a
large set of such ε-increasing paths, only one of which is simulated by the maximization subroutine.
To get around this problem, we instead use the fact that Lε is the supremum of the values of f at the
endpoints of all ε-increasing paths starting at y? which seek to maximize f (x? + △, ∙), to show that
f(x? + ∆,y0) ≤Lε(x? + ∆,y?).	(9)
Finally, we already showed that ∣∣Vyf(x*,y?)ki ≤ ε?; recall from Section 2 that this implies
Lε(x?, y?) = f(x?, y?). Combing this with equation 8, equation 9 implies the ε-local minimum
condition equation 35.
C Comparison of notions of local optimality
In previous works Daskalakis & Panageas (2018); Heusel et al. (2017), a local saddle point (equiva-
lently a local Nash point) has been considered as a possible notion of local min-max optimum. A point
(x? , y? ) is said to be a local saddle point if there exists a small neighborhood around x and y such
that y? is a first-order stationary point for the function f (x*, ∙), and x? is a local minimum for the
function f (∙,y?). A key difference between a local saddle point and the local min-max equilibrium
we introduce in our paper (Definition 2.2) is that a local saddle point does not take into account the
order in which the min-player and max-player choose x and y .
While a local saddle point is not guaranteed to exist in general nonconvex-nonconcave min-max
problems, if there exists a local saddle point, this saddle point is also a local min-max equilibrium as
given by Definition 2.2. More specifically, any local saddle point for a smooth function f is an ε-local
min-max equilibrium for every ε > 0, provided that we pick the step size for the min-player to be
small enough. This is because, if there if a point (x?, y?) is a local saddle, then y? is a first-order
stationary point for the function f (x*, ∙). Thus, the gradient at (x*, y?) must be zero and the first
condition (Inequality equation 34) is satisfied.
If we pick the step size small enough, the step △ will be such that x? + △ lies within this neighborhood
of x? and hence
Lε(x*,y?) = f (x?, y?) ≤ f (x? + △, y?) ≤ Lε(x* + △,y?)
15
Under review as a conference paper at ICLR 2021
for all ε > 0 6. Thus the second condition (Inequality equation 35) is satisfied for any ε > 0.
Another notion of a local minimax point was proposed in Jin et al. (2020). In their definition, the max
player is able to choose her move after the min-player reveals her move. The max-player is restricted
to move in a small ball around y? , but is always able to find the global maximum inside this ball.
In contrast, in our definition, the max-player is empowered to move much farther, as long as she is
following a path along which the loss function increases rapidly. Hence, in general these two notions
are incomparable. However, even under mild smoothness conditions, a local minimax point is not
guaranteed to exist Jin et al. (2020), whereas a local min-max equilibrium (Definition 2.2) is.
Finally, in a parallel line of work, we prove the existence of a stronger, second-order notion of min-
max equilibrium for nonconvex-nonconcave functions motivated by the notion of approximate local
minimum introduced in Nesterov & Polyak (2006). The advantage of the notion in our current paper
(Definition 2.2) is that it yields a concise proof of convergence and the accompanying algorithm, as
our empirical results show, is effective for training GANs.
D Main Result
Recall that we have defined Dx,y to be the distribution of the point-wise normalized stochastic
gradient Gχ(χ, y)/,GX(χ,y). We also recall that We have made the following assumptions about
our stochastic gradient, which we restate here for convenience:
Assumption D.1 (smoothness). Suppose that ft(x, y)〜Dχ,y is SamPledfrom the data distribution
for any x, y ∈ Rd. Then, with probability 1, ft is b-bounded, L1-Lipschitz, and has L-Lipschitz
gradients Vft.
Assumption D.2 (batch gradients).
1 b0
F (χ,y) = b^ X ft(χ,y)
1 bx
Gx(X,y) = b~〉: Vxft(X,y),
1 by
Gy (x,y) = b- ^Vy ft(x,y),
where, f1 , . . . ft are sampled iid (with replacement) from the distribution D(x,y) and b0 , bx , by > 0
are batch sizes. Every time Gx, Gy, or F is evaluated, a new, independent, batch is used.
Setting parameters: For the theoretical analysis, we set the following parameters, and we define
Z :
Z
1 if z = 0. We also assume 0 < ε ≤ 1.
1. α = 1
2.
3.
4.
5.
6.
7.
8.
β1 = 0, β2 = 0, δ = 0
ω = 236bLd+16 [(TI + εb2 ) 簧 log(τ1¾6 )log4(100 (τ1 + 1)( 8b + 1))]-2
rmax = 1ε8 log2 (1ε0(τι + 1)(8b + 1) +log(ω1))
Define I := τι log( rmax) + 8rmaχ b + 1
η = 2 2i — 1
5I ≤ η ≤ I)
(in particular, note that the fact that η = 2 2ι 一 1 and I ≥ 1 implies that
α = ε⅛⅛)
Define J ：=袈
6Recall from Sec. 2 that if Eq. equation 34 is satisfied then Lε(x?, y?) = f(x?, y?).
16
Under review as a conference paper at ICLR 2021
9.	ε1 = √⅛(I - ι+η)
10.	bx= 1
11.	b0 = ε-21402b2 log(1∕ω)
12.	by = ε-21402L2 log(1∕ω)
In particular, We note that ω ≤ ⑴J+⑹1 and rmaχ ≥ 4 log( 10OI). At every iteration i, where We
set ε = ɛi, We also have ε0 ≤ ε0(1 + η)2I ≤ ε and hence that (^0 + LyfdayJ^d ≤ (1 一 ι+η)εo ≤
(I - ι++η)ε0.
Proposition D.3. For any ε1, ω > 0, if we use batch sizes by = ε-21402L1 log(1∕ω) and b0
ε-21402b2 log(1∕ω), we have that
P HIGy(x,y) - Vyf(X,y)∣2 ≥ ɪɪʌ <ω,
and
P (|F(χ,y) - f(χ,y)l ≥ ε0 ʌ < ω.
(10)
(11)
Proof. From Assumption D.2 we have that
1 by
Gy(X,y) - Vyf (X,y) = b- E[Vyft(X,y) -Vyf (X,y)],
by t=1
Where f1 , . . . ft are sampled iid (With replacement) from the data distribution D.
But by Assumption D.1, We have (With probability 1) that
l∣Vyft(x,y) - Vyf(x,y)k2 ≤ ∣∣Vyft(x,y)k2 + ∣∣Vyf(χ,y)k2 ≤ 2Lι∙
NoW,
E[Vyft (X, y) - Vyf(X, y)] = E[Vyft(X, y) - E[Vyft(X, y)]] = 0.
Therefore, by the Azuma-Hoefding inequality for mean-zero bounded vectors, We have
P (Ibl^ X[Vy ft(X,y) 一 Vyf (X,y)]
≥ S 吗 +1 2Lι 1 <2e1-1 s2
by
2
∀s > 0.
Hence, if we set S = 6log1/2 (ω), We have that 7 log 1/2(2)pby + 1 ≥ Spby + 1 and hence that
P (Ij X[Vyft(X,y) -Vyf(X,y)]	≥ 7log ；j)py2Lι
< ω.
Therefore,
P (| j X[Vyft(X, y) - Vyf (x, y)]	≥ 10 j < ω
Which completes the proof of Inequality equation 10.
Inequality equation 11 folloWs from the exact same steps as the proof of Inequality equation 10, ifWe
replace the bound L1 for ∣Vyft(X, y)∣2 With the bound b on |ft(X, y)|.
□
17
Under review as a conference paper at ICLR 2021
Proposition D.4. For every j we have
kyj+ι - Yjl∣2 ≤ α√d.	(12)
Moreover, for every i we have,
∣∣Xi+ι 一 Xi∣∣2 ≤ α√d.	(13)
Proof.
kyj+ι -yjk2 ≤ αkmj/√vjk2
d	2
αt X (gy,j[k]/Vzgjki)
k=1
α√d
This proves Inequality equation 12. The proof of Inequality equation 13 follows from the same steps
as above.	□
Proposition D.5. Algorithm 3 terminates in at most J := 32α^ε iterations of its “While” loop, with
probability at least 1 - ω × J.
Proof. Recalling that / and √∙ denote element-wise operations, We have
d	____
h(mj/√vj), gy,ji = X (gy,j[k]/VzgyjW) × gy,j[k]	(14)
k=1
d
= X |gy,j [k]|
k=1
ε
=kgy,jkι ≥ ε0 ≥ 2
since, whenever Algorithm 3 is called by Algorithm 2, Algorithm 2 sets Algorithm 3’s input ε0 to
SomevaIUe ε0 ≥ 2.
Therefore, we have that
Prop. D. 3	ɛi
hyj+1 —	yj,	Vy f (x, yj )i	≥	hyj+ι 一	yj,	gy,ji-kyj+1 —	yj	k2 X 10
.	..	.. ɛl
=hα(mj/√vj), gy,ji -kyj+ι - yjk2 X 10
Eq.14 ʌ ε	ει
≥ α2 — kyj+1 - yj k2 X 10
Prop. D. 4 ε	γ-	ε-∣
≥	62 一 α √d X 11
、4鹿
≥ — αε,
-10	,
(15)
with probability at least 1 一 ω, since ει ≤ √.
Since f has L-Lipschitz gradient, there exits a vector u, with lul2 ≤ Llyj+1 一 yj l2, sUch that
f(yj+1) 一 f(yj) = hyj+1 一 yj, Vyf(x,yj) +ui
= hyj+1 一 yj, Vyf(x, yj)i + hyj+1 一 yj, ui
Eq.15 4
≥ 10αε 一 Lkyj+1 - yj k2
(16)
Prop.D.4
≥
42
一αε — Lc^2d
10
18
Under review as a conference paper at ICLR 2021
≥ -3ɑε,
-10	,
with probability at least 1 - ω, since α ≤ ^0Ld.
Since f takes values in [-b, b], Inequality equation 16 implies that Algorithm 3 terminates in at most
J = 32α^bε iterations, with probability at least 1 - ω XJ.
□
Proposition D.6. Algorithm 2 terminates in at most I := τι log( rmωax) + 8rmaχb + 1 iterations of
its “While” loop, with probability at least 1 — 2ω × (rmax等 + 1).
4 £
Proof. For any i > 0, let Ei be the “bad” event that both f (xi+1,yi+1) - f(χi,yi) > -4 and
Accepti = True.
Then by Proposition D.3, since ^0 ≤ 8, we have that
i
P(Ei) ≤ e-τ1 + ω.	(17)
Define I := τι log(rmax).
Then for i ≥ I, from Line 12 of Algorithm 2 We have by Inequality equation 17 that
P(Ei ) ≤ 2ω.
Define h := rmax等 + 1. Then
max 4 ε
∕ι+h ʌ
P [ Ei	≤ 2ω × h.	(18)
.^.LΛ,
Since f takes values in [-b, b], if U,=^ Ei does not occur, the number of accepted steps over the
iterations I ≤ i ≤ I + h (that is, the size of the set {i : I ≤ i ≤ I + h, Accepti = True}) is at most
2b
节.
Therefore, since h = rmax 瑞 +1, we must have that there exists a number i, with I ≤ i ≤ i+rmax ≤
I + h, such that Accepti = False for all i ∈ [i, i + rmax].
Therefore the condition in the While loop (Line 9) of Algorithm 2 implies that Algorithm 2 terminates
.^-LΛ,
after at most i + rmax ≤ I + h iterations of its While loop, as long as Ui=^ Ei does not occur.
Therefore, Inequality 18 implies that, with probability at least 1 - 2ω × (rmaxτ∣ + 1), Algorithm 2
terminates after at most
I+ h = T1 log( rmax) + 8rmax - + 1
ωε
iterations of its “While” loop.
□
Lemma D.7. Algorithm 2 terminates after at most (τι log( rmax) + 4rmax∣ + 1) × (J× by + b0 + bχ)
gradient and function evaluations.
Proof. Each iteration of the While loop in Algorithm 2 computes one batch gradient with batch size
bx, and one stochastic function evaluation of batch size b0, and calls Algorithm 3 exactly once.
Each iteration of the While loop in Algorithm 3 computes one batch gradient with batch size by.
The result then follows directly from Propositions D.6 and D.5.	□
19
Under review as a conference paper at ICLR 2021
Recall the paths γ(t) from Definition 2.1. From now on We will refer to such paths as "ε-increasing
paths”. That is, for any ε > 0, we say that a path γ(t) is an "ε-increasing path” if at every point along
this path we have that ∣∣ 奈Y(t) ∣∣∞ and that 奈f (x, γ(t)) > ε (Inequality equation 33).
Proposition D.8. Every time Algorithm 3 is called we have that, with probability at least 1 - 2ωJ ,
the path consisting of the line segments [yj, yj+1] formed by the points yj computed by Algorithm 3
has a parametrization γ(t) which is an 1+不ε0-increasing path.
Proof. We consider the following continuous parametrized path γ(t):
Y(t) = yj + tvj t ∈ [α(j — 1), αj],	j ∈ [jmaχ],
where Vj= gyj/，(gyj)2 and jmaχ is the number of iterations of the While loop of Algorithm 3.
First, we show that this path has at most at most unit-velocity in the infinity-norm at all but a finite
number of time-points. For any point in one of the open intervals t ∈ (α(j - 1), αj), we have that7
k d∣Y(t)k∞ = kvjk∞ = ∣∣gy,j /q(gy,j )2 k∞ ≤ ll(1,..., 1)k∞ = L
This implies that the path γ(t) has unit velocity in the infinity norm at all but a finite number of
time-points.
Next, we show that 奈f (x, γ(t)) ≥ ε0.
Now,
kvj k2 = √d	(19)
and hence,
l∣yj+ι - Nj 112 = l∣αvjk2 = α√d	QO)
at every step j .
By Line 12 of Algorithm 3 we have that kgy,j/∖∕∖gy,j Ik2 > ε0 for every j ∈ [jmaχ], where we define
jmaχ ∈ N ∪ {∞} to be the number of iterations of the While loop in Algorithm 3. Therefore, for
every j ∈ [jmaχ], by Proposition D.3 we have with probability at least 1 - ω that
ddtf(χ,γ(t)) ≥ [Vyf(χ,yj-)- Lkyj+ι- yj-Il2u]>Vj
喳0 [Vyf(x, yj-) - L√dɑu]>vj
=[gy,j — ɪ1 W — L√d^u]>Vj
=g>jVj - [ε1 W + L√dαu]>Vj
10
=g>j (gy,j/ J(gy,j)2) - [10W + L√dαu]>vj
=l∣gy,j/∕∖gy,j∖k2 -岛W + L√dαu]>vj
≥ kgy,j/q∕∖gy,j|k2 - (10 + L√dα)kvj∣∣2
(21)
Eq.19
≥ ε0
kgy,j/ J∖gy,j|k2 - (11 + L√dα)√d
-(fl + L√dα)√d
≥ τ⅛ ε0	∀t ∈ [α(j- 1),训
7Recall that we use the convention 0/0 = 0 when computing the Adam update in our algorithm. This implies
that ∣gy,j/P(gj21≤ (1,...,1).
20
Under review as a conference paper at ICLR 2021
for some unit vectors u, w ∈ Rd, since (εo + L√d(y-')√d ≤(I - j++η )ε0 ≤(I — ι++η )ε0.
But by Proposition D.5 we have that jmax ≤ J with probability at least 1 - ω × J . Therefore
inequality equation 21 implies that
df f (x, γ(t)) ≥ vɪε0	∀t ∈ [a(j — 1), αj]	∀j ∈ [jmax],
dt	1 + η
with probability at least 1 - 2ωJ .
□
Lemma D.9. Let i? be such that i? - 1 is the last iteration i of the “While” loop in Algorithm 2 for
which Accepti = True∙ Then Withprobability at least 1 — 2ω JI — 2ω X (rmaχ-2b + 1) we have that
…	'	'	'4ε	’
kVyf(x?,y?)ki ≤ -7=^εi?.	(22)
1+η
Moreover, with probability at least 1 —品—2ω × (rmax 蛋 + 1) we have that
and that
P△〜Dχ*,y*
(l%* (x? + ∆, y?) ≤ Lεi* (x?, y?) — 1 ε
x?,y?) ≤ 2ε∙
ε
-≤ εi* ≤ ε.
2i
(23)
(24)
Proof. First, we note that (x?, y?) = (xi, yi) for all i ∈ {i?, . . . , i? + rmax}, and that Algorithm 2
stops after exactly i? + rmax iterations of the “While” loop in Algorithm 2.
Define ∆i := gχ,i/√(gχ,i)2 for every i. Then △，〜Dχi,yi.
Let Hi be the “bad” event that, when Algorithm 3 is called during the ith iteration of the “While” loop
in Algorithm 2, the path traced by Algorithm 3 is not an εi-increasing path. Then, by Proposition D.8
we have that
P(Hi) ≤ 2ωJ.
(25)
Let Ki be the “bad” event that ∣∣Gy(Xi, yi) — Vy f(xi, yi)k2 ≥ ^0. Then by Propositions D.3 and
D.5 we have that
P(Ki) ≤ 2ωJ.	(26)
Whenever Kic occurs we have that
∣Vy f (xi , yi )∣1 ≤ ∣Gy (xi , yi)∣1 + ∣Gy (xi , yi) — Vyf(xi , yi )∣1
≤ εi + ʌ^dk Gy (xi, yi ) - Vy f (xi, yi ) k 2
1	ει
≤ Gεi +10
1
(27)
≤ ,	εi,
-√1+η
where the second Inequality holds by Line 12 of Algorithm 3, and the last inequality holds since
ει ≤ 1 _	]
10 ≤ 1	√ι+η.
Therefore, Inequalities equation 26 and equation 27 together with Proposition D.6 imply that
∣Vyf(x*,y*)kι ≤√=1+= εi*
with probability at least 1 — 2ωJI — 2ω × (rmaχτb + 1). This proves Inequality equation 22.
'	4ε	/	ɪ	,	，
21
Under review as a conference paper at ICLR 2021
Inequality equation 27 also implies that, whenever Kic occurs, the set Sεi,xi,yi of εi-increasing paths
with initial point yi (and x-value xi) consists only of the single point yi . Therefore, we have that
Lεi(xi,yi) = f(xi,yi)	(28)
whenever Kic occurs.
Moreover, whenver Hic occurs we have that Yi+1 is the endpoint of an εi-increasing path with starting
point (xi + ∆i, yi). Now, Lεi (xi + ∆i, yi) is the supremum of the value of f at the endpoints of all
εi-increasing paths with starting point (xi + ∆i, yi). Therefore, we must have that
Lεi(xi+∆i,yi) ≥ f(xi+∆i,Yi+1)	(29)
whenever Hic occurs.
Therefore,
P∆〜Dχi,yi (Lεi(xi+△,%) > Lεi (xi,yi)- 2 εlxi,yi)	(30)
Eq.28,29	1
≥	P△〜Dχi,yi	f(Xi	+	∆i, Yi+ι)	>f (xi,yi)	-	2εxi,yΛ	-	P(Hi)- P(Ki)
Prop.D.3	1
≥	P△〜Dχi,yi (F(Xi +∆,Yi+ι) > F(xi,yi) — 4ε xi,ye) - 2ω - P(Hi)- P(Ki)
≥ P (Accepti = FalselXi, y, — 2ω — P(Hi) — P(Ki)
Eq.25,26
≥ P Accepti = FalselXi, yi - 2ω - 2ωJ - 2ωJ ,	∀i ≤ I,
where the second inequality holds by Proposition D.3, since ^0 ≤ ε.
Define Pi := P△〜。。稔频(£三科(Xi + ∆,yi) > Lεi(xi,yi) 一 2ε Xi,y) for every i ∈ N. Then In-
equality equation 30 implies that
P (Accepti = False∣Xi, y, ≤ Pi + ω(4J + 2) ≤ Pi + 1 ε	∀i ≤ I,
since ω ≤ 32JJ+16 .
(31)
We now consider What happens for indices i for which Pi ≤ 1 一 2ε. Since (xi+s,yi+s) = (xi, yi)
whenever Accepti+k = False for all 0 ≤ k ≤ s, we have by Inequality equation 31 that
P 卜r=0x{ACCePti+s = False} Pi ≤ 1 - 1 ε) ≤ (1 - 4ε)rmax ≤ 100I	∀i ≤I- rmax
since rmax ≥ ε 1°g( 100I )∙
Therefore, with probability at least 1 一 i0ει ×I = 1-高,we have that the event ∩r=ax {Accepti+§ =
False} does not occur for any i ≤ I - rmax for which Pi ≤ 1 - 2ε.
Recall from Proposition D.6 that Algorithm 2 terminates in at most I iterations of its “While” loop,
with probability at least 1 一 2ω X (rmaχτb + 1).
4ε
Therefore,
P (Pi? > 1 一 2ε) ≥ 1 一 100 一 2ω X (rmax" + 1).	(32)
In other words, by the definition of Pi? , Inequality equation 32 implies that with probability at least
1 一 ι0o 一 2ω X (rmaχτbε + 1),the point (x*, y?) is such that
P△〜Dx*,y*(L，i* (x* + ∆, y?) ≤ Lεi? (x*, y?) - 1 ε
x*,y*) ≤ 2ε.
22
Under review as a conference paper at ICLR 2021
This completes the proof of inequality equation 23.
Finally we note that when Algorithm 2 terminates in at most I iterations of its “While” loop , we have
号i? = εo(1 + η)2i ≤ εo(1 + η)2I ≤ ε. This completes the proof on Inequality equation 24.	□
We can now complete the proof of the main theorem:
Proof of Theorem 2.3. First, by Lemma D.7 our algorithm converges to some point (x?, y?) after at
most (τι log(rmωax) + 4rmaχb + 1) X (JX by + bo + bχ) gradient and function evaluations, which
is polynomial in 1∕ε, d, b, Lι,L. In particular, for L,b ≥ 1, the number of gradient and function
evaluations is O(d2L2b6ε-11).
By Lemma D.9, if We set ε? = εi?, We have that Inequalities equation 34 and equation 35 hold for
ε? ∈ [ 1 ε, ε] with probability at least 1 一 2ω JI — 2ω X (rmaxjr1 + 1) ≥ 养.	□
E S imulation Setup
In this section we discuss the neural network architectures, choice of hyperparameters, and hardware
used for both the real and synthetic data simulations.
Datasets. We evaluate the performance of our algorithm on both real-world and synthetic data. The
real-world datasets used are MNIST and CIFAR-10. In one of our MNIST simulations we train our
algorithm on the entire MNIST dataset, and on the remaining simulations we train on the subset of
the MNIST dataset which includes only the digits labeled 0 or 1 (note however that the algorithms do
not see the labels). For simplicity, we refer to this dataset as the 0-1 MNIST dataset.
The synthetic dataset we use consists of 512 points points sampled at the start of each simulation
from a mixture of four equally weighted Gaussians in two dimensions with standard deviation 0.01;
and means positioned at (0, 1), (1, 0), (一1, 0) and (0, 一1).
For all of our simulations on both real and synthetic datasets, following Goodfellow et al. (2014) and
Metz et al. (2017), we use the cross entropy loss function for training, where f (x, y) = log(Dy (ζ)) +
log(1 一 Dy(Gx(ξ)), where x are the weights of the generator’s neural network and y are the weights
of the discriminator’s neural network, where ζ is sampled from the data distribution, and ξ is a
Gaussian with identity covariance matrix. The neural network architectures and hyperparameters for
both the real and synthetic data simulations are specified in the following paragraphs.
Hyperparameters for MNIST simulations. For the MNIST simulations, we use a batch size
of 128, with Adam learning rate of 0.0002 and hyperparameter β1 = 0.5 for both the generator
and discriminator gradients. Our code for the MNIST simulations is based on the code of Renu
Khandelwal Khandelwal (2019) and Rowel Atienza Atienza (2017), which originally used gradient
descent ascent and ADAM gradients for training.
For the generator we use a neural network with input of size 256 and 3 hidden layers, with leaky
RELUS each with “alpha” parameter 0.2 and dropout regularization of 0.2 at each layer. The first
layer has size 256, the second layer has size 512, and the third layer has size 1024, followed by an
output layer with hyperbolic tangent (“tanh”) acvtivation.
For the discriminator we use a neural network with 3 hidden layers, and leaky RELUS each with
“alpha” parameter 0.2, and dropout regularization of 0.3 (for the first two layers) and 0.2 (for the last
layer). The first layer has size 1024, the second layer has size 512, the third layer has size 256, and
the hidden layers are followed by a projection to 1 dimension with sigmoid activation (which is fed
as input to the cross entropy loss function).
Hyperparameters for Gaussian mixture simulations. For the simulations on Gaussian mixture
data, we have used the code provided by the authors of Metz et al. (2017), which uses a batch size 512,
Adam learning rates of 10-3 for the generator and 10-4 for the discriminator, and Adam parameter
β1 = 0.5 for both the generator and discriminator.8
8 Note that the authors also mention using slightly different ADAM parameters and neural network architecture
in their paper than in their code; we have used the Adam parameters and neural network architecture provided in
their code.
23
Under review as a conference paper at ICLR 2021
We use the same neural networks that were used in the code from Metz et al. (2017): The generator
uses a fully connected neural network with 2 hidden layers of size 128 and RELU activation, followed
by a linear projection to two dimensions. The discriminator uses a fully connected neural network
with 2 hidden layers of size 128 and RELU activation, followed by a linear projection to 1 dimension
(which is fed as input to the cross entropy loss function). As in the paper Metz et al. (2017), we
initialize all the neural network weights to be orthogonal with scaling 0.8.
Hyperparameters for CIFAR-10 simulations. For the CIFAR-10 simulations, we use a batch size
of 128, with Adam learning rate of 0.02 and hyperparameter β1 = 0.5 for both the generator and
discriminator gradients. Our code for the CIFAR-10 simulations is based on the code of Jason
Brownlee Brownlee (2019), which originally used gradient descent ascent and ADAM gradients for
training.
For the generator we use a neural network with input of size 100 and 4 hidden layers. The first
hidden layer consists of a dense layer with 4, 096 parameters, followed by a leaky RELU layer,
whose activations are reshaped into 246 4 × 4 feature maps. The feature maps are then upscaled to
an output shape of 32 x 32 via three hidden layers of size 128 each consisting of a convolutional
Conv2DTranspose layer followed by a leaky RELU layer, until the output layer where three filter
maps (channels) are created. Each leaky RELU layer has “alpha” parameter 0.2.
For the discriminator, we use a neural network with input of size 32 × 32 × 3 followed by 5 hidden
layers. The first four hidden layers each consist of a convolutional Conv2DTranspose layer followed
by a leaky RELU layer with “alpha” parameter 0.2. The first layer has size 64, the next two layers
each have size 128, and the fourth layer has size 256. The output layer consists of a projection to 1
dimension with dropout regularization of 0.4 and sigmoid activation function.
FID scores were computed every 2500 iterations. To compute each FID score we used 10,000
randomly selected images from the CIFAR-10 dataset, and 10,000 generated images.
Setting hyperparameters. In our simulations, our goal was to be able to use the smallest number
of discriminator or unrolled steps while still learning the distribution in a short amount of time, and
we therefore decided to compare all algorithms using the same hyperparameter k. To choose this
single value of k, we started by running each algorithm with k = 1 and increased the number of
discriminator steps until one of the algorithms was able to learn the distribution consistently in the
first 1500 iterations. This resulted in a choice of k = 1 for the MNIST datasets and a choice of k = 6
for the Gaussian mixture model data. For the CIFAR-10 dataset we simply used k = 1 for both
algorithms (since for CIFAR-10 it is difficult to visually determine if all modes were learned).
Our temperature hyper-parameter was set by running our algorithm with temperatures in the set
{1, 2, 3, 4, 5, 10}, and choosing the temperature which gave the best performance.
Hardware. Our simulations on the MNIST, 0-1 MNIST, and Gaussian datasets were performed on
four 3.0 GHz Intel Scalable CPU Processors, provided by AWS.
Our simulations on the CIFAR-10 dataset were performed on one GPU with High frequency Intel
Xeon E5-2686 v4 (Broadwell) processors, provided by AWS.
F	Additional simulation results
In this section we show additional results from simulations which we did not have space to include in
the main body of the paper.
F.1 Our algorithm on the full MNIST dataset
Here we show the results of the simulation of our algorithm on the full MNIST dataset (Fig. 4).
F.2 Our algorithm and GDA trained on the CIFAR- 1 0 dataset.
In this section we show the results of all the runs of the simulations of our algorithm and GDA on the
CIFAR-10 dataset, which were mentioned in Section 3.1 (Figures 5 and 6).
24
Under review as a conference paper at ICLR 2021
ΞQBIΞBQΠΞΞO	ΞΞΞE1DSHE1QΠ	□ħħξħξbq□d
QΞΞBΠBE3B□Q	πξqħbqqeieid	bξs□hξdξħħ
ΞEIΞHΞΞDDDS!	HΞΞΠEIHDaHn	DSΞOQΞΞΞQH
ΞSDEIQE3ΞBQD	ξħξqħξξξπei	QQBBΞEIBΞDS
B□SBBBHΞΞΞ	BQΞΞDBΠΞQQ	HSΞΠHQΞQDB
BHQBBQSBQΞ	□BBΞQ□ΞΞBH	qbqbħqξqsπ
QΞHSQElQn□B	Ξ□QΞΞ□QBΞΞ	ξqqħq□ħξξb
oeiħ□ħξπhξξ	beiξξξbħξħs	□Π□BΞBΞΞBΞ
HΠ□QΞΠΞŋBΞ	El目ElH目WR 口目	QΞQSQ□ΠΞ□E3
ΞΠE1SQΞΞBΠD	ΞΞΞE!□□ΞHΠH	目 E!E]ElIiEl∙0EIEa
ΞΞHBQ□E1E1HQ	oħ□qξξξqoħ	
ħħππħħq□ħd	ξ□ξhdξqbħo	
dqξξqξħb□q	ΞΞQQΞQQQΞS	
ħbξ□□ξdsξξ	qheieiqħbdhħ	
□B□BBΠQQHQ	BΠQQHQBQE9Π	
bhħqπci□ξξq	SBΠBBnSBQB	
bħξb□bħbqq	QΞQBΞ□BBΞ□	
sdqħξħqħbξ	QΞΞΞΞΞQΞBΠ	
ΞΞDΞSHΞQΞQ	DgJmΞΞB□BBQ	
ΞΞBQΞQΞΞE1Π	QΞΞ□QΞOΞΠQ	
Figure 4: We ran our algorithm (with k = 1 discriminator steps and acceptance rate e-T = 1) on the full
MNIST dataset for 39,000 iterations, and then plotted images generated from the resulting generator. We
repeated this simulation five times; the generated images from each of the five runs are shown here.
F.3 Our algorithm trained on the 0-1 MNIST dataset.
Here we show the results of the simulations of our algorithm on 0-1 MNIST which were mentioned
in Section 3.1 (Figures 7 and 8).
F.4 Comparison with GDA on MNIST
In this section we show the results from the different runs of the simulations of our algorithm and
GDA on the 0-1 MNIST dataset, which were mentioned in Section 3.2 (Figures 9 and 10).
F.5 Randomized acceptance rule with decreasing temperature
In this section we give the simulations mentioned in the paragraph towards the beginning of Section
3, which discusses simplifications to our algorithm. We included these simulations to check whether
our algorithm also works well when it is implemented using a randomized acceptance rule with a
decreasing temperature schedule.
F.6 Comparison of algorithms on mixture of 4 Gaussians
In this section we show the results of all the runs of the simulation mentioned in Figure 2, where all
the algorithms were trained on a 4-Gaussian mixture dataset for 1500 iterations. For each run, we plot
points from the generated distribution at iteration 1,500. Figure 13 gives the results for GDA with
k = 1 discriminator step. Figure 14 gives the results for GDA with k = 6 discriminator steps. Figure
15 gives the results for the Unrolled GANs algorithm. Figure 16 gives the results for our algorithm.
25
Under review as a conference paper at ICLR 2021
GDA
!1■■■■■
聋髓竭啰■里
再。E2第I |>
wb⅜oħh
能坐Ei警已翘
Our Algorithm
■a ■胭哮诬哨
Figure 5:	GAN trained using our algorithm (with k = 1 discriminator steps and acceptance rate
e-1∕τ = 1∕2) and GDA. We repeated this simulation nine times; We display here images generated from
the resulting generator for each of the nine runs of GDA (top) and our algorithm (bottom). The final FID
scores at 50,000 iterations for each of the nine runs (corresponding to the images above from left to right
and then top to bottom) Were {35.6, 36.3, 33.8, 35.2, 34.5, 36.7, 34.9, 36.9, 36.6} for our algorithm and
{33.0, 197.1, 34.3, 34.3, 33.8, 37.0, 45.3, 34.7, 34.7} for GDA.
26
Under review as a conference paper at ICLR 2021
0OK
20K	40K
iteration
Figure 6:	Plots of FID scores for GANs trained using our algorithm (with k = 1 discriminator steps and
acceptance rate e-1/T = 1∕2) and GDA on CIFAR-10 for 50,000 iterations. We repeated this simulation nine
times, and plotted the FID scores for our algorithm (dashed blue) and GDA (solid red).
OUr algorithm
αα≡BB
HDDQD
eiξξbħ
QDDDa
DDDDD
100
ΞQΞDH
QQΠDΠ
DDDQQ
ΠΠ^ΞΞ
ΠDΠΞD
2,000
DB□ΠQ
ΠΞUΠD
ΠΠΠΠQ
E1ΞΞΞΠ
ElQQQD
20,000
ΠDΠΞΞ
SDDΠΠ
QBQΞΞ
DODOD
DDDΞΞ
500
ΞQΞQΠ
ΠΠΞQΞ
Π□ΞΞΠ
ΠΞΠBΠ
ΠΠΠΠQ
5,000
ΠDΠΞΠ
ΞQΞΞQ
DSΠΠQ
DΞΞDΠ
ΠΞQΠ□
30,000
E3QΠΠΠ ΞΞΠDΠ
ΠQSΠ^ DQΞDD
ΠΠDUΞ ΞQQΠD
QΞ□DΠ ΞDD@D
B∏HΞQ ΞΞΞΞΠ
1,000	1,500
DΞQ∏n ΞQDΞQ
IIDnMn ΞΠDΠQ
QΠΠDΠ QDΞ□Ξ
ΠΞΞDΠ ΠΠΞΞD
QΠI1ΞΞ ΞDΞDΠ
10,000	15,000
Figure 7: We trained our algorithm on the 0-1 MNIST dataset for 30,000 iterations (with k = 1 discriminator
steps and acceptance rate e-T = ∣). We repeated this experiment five times. For one of the runs, We plotted
25 generated images produced by the generator at various iterations. We also plotted a moving average of the
computed loss function values, averaged over a window size of 50.
QΠΞΞΞ H □ΞΞΠΠ： ι ΞUΠΞD：K 囱□	口	0	5000 10000 15000 20000 25000 30000	Ξnπ∏Q ∏	I Q□ΞΞD Π□DΠD：k DQΞΠD \	J ∏Bj]HB¾Π 13 0一荷。工血。荷。。茄。。益。。蔡。:	ΠΠΠΞΠ H Ξ□ΞΞD k DQΠΠQ：¾ ΠΞΞΠΠ 口 回以 口	0 5000 10000 15000 20000 25000 30000
DΞΠΞQ -07f∣	QΠΠΞΞ ∏	
QΞDEI□ :K	ΞΞQΠQ ：： I	
ΞQS0Ξ ：： ⅛	ΞΞΠDΞ -ɪ0 m	
ΠQDQΞ PvK__	ΞΞQ□D	
股 @ 口	M	0	5000	10000	15000	20000 25000	30000	田 口 P3 “ M	0	5000	10000 15000 20000 25000 30000	
Figure 8: We show the images generated at the 30,000’th iteration for all 5 runs of the simulation in Figure 7,
together with a plot of their computed loss function values. Recall from the caption of Figure 7 that we trained
our algorithm on the 0-1 MNIST dataset for 30,000 iterations (with k = 1 discriminator steps and acceptance
rate e-T = ∣). We repeated this experiment five times. For one of the runs, We plotted 25 generated images
produced by the generator at various iterations. We also plotted a moving average of the computed loss function
values, averaged over a window size of 50.
27
Under review as a conference paper at ICLR 2021
OQDQD	DΠDDD	DΠQQS	QQDDD	QDDDD	^qπħd	QDSΠD
QQ^DD	DQΞDD	QD□ΠD	DDEaQD	Π□DΠK1	DΞDHE3	ΠDDD□
DDDQB	ODΠΠEi	ODDDS	SDSSB	DQDnD	SDDDΠ	ODDDD
MEIlII1”	DQΞI1Q	DDΠQII	QDDQΠ	DDSDB	∏ EIDHQ	DDSDB
^DΠD^		^ΠΠΠD^	IInnKaEl	nnnm_	^ΞI0ΠIQ	πun∏H
DΠ^BD	ΠDEIDD	SHODD	DQDDD	QDDHB	DΠQDB	
HΠDDD	Π□ΠDEI	DQDDD	QDDDH	BDQDD	DDDOD	
DODEiO	DQQBD	EiQSQII	KlEmn 以	HDQΠE3	B^ΠQΠ	
ΠBΞ□D	HHDQD	IIEmEm	HQDOD	QDQΠD	ΠS□QD	
ΠI¾∙ΠΠ	πnπw	πι¾gaπιι		mannπ	IInnEa”	
Figure 9: Images generated at the 1000’th iteration of the 13 runs of the GDA simulation mentioned in Figure 1.
In 77% of the runs the generator seems to be generating only 1’s at the 1000’th iteration.
QEIHΞD	OΠΠΞD	ΞΞΞDD	QΞn∏D	ΞQ□QD	ΠΠΠE2D
"U翻"回	■围国麴口	HEIE1DD	^qhħd	O^DDD	DHHDH
BDDDD	HD∏nH	DDDHD	DDHDQ	DQQΠD	QΠi^ΠD
DDHDH	OΞQΞD	DQ^QD	!!整Ea国国	DQHDD	DDSDE
ElQEIEIEB	必π^i∕ιn	卤 n¾a也ιι		D"ΠB¾	■nMn
国EaEI	QDΠΠΠ	DΞOΞE2	DBQQEi	nαnoD	DΞΞDΞ
DΠOHD	SDDQD	UU邀阿n	DDDΞE	的 UKmla	QQDD^
HDHBΠ	nnn∏D	DΞS3D0	ΞQDΞD	DDDQD	∏ΞQΞΞ
ħdπħħ	n国n圆Ei	DDQDB	ΞS^DB	DHDDD	ΞQQD0
Qnmm	ιι^nnιι	n 目imu	ιιιιπ^n	“■Duel	超电性n囱
DE3EilΞD	DDQHD	ΞDDDD	DDQElE]	□DΠΞQ	ΞΞDDE1
EDDΞΞ		DΠDΞD	股■国口目	HD^SQ	DDDDΞ
DDEDD	DDQQEl	DDDΞD	ElQDDD	QΞQQD	DΞDQD
ΞHQHΞ	ODHQD	HDDSΞ	HKlHHEa	n缠U典u	SDDDΞ
ngaπ⅞⅞u	BinEiaKi	^IDΞHD	nnEMn	ma窗❸Ea	KinnD 囱
“国KI国”	QDD^D	DDHDD	ΞQΞDD		
□DΞ□D	DΠ□DD	E9Ξ□ΠD	ΞΠQΞΠ		
		QDΞDEi	BSΞEID		
HDODD	“国U四U	DDSBD	QΞHΠΠ		
^ΠΠΠD^		Eln底的Ka			
Figure 10: Images generated at the 1000'th iteration of each of the 22 runs of our algorithm for the simulation
mentioned in in Figure 1.
DΞD□O	ΞΞBΞQ	口国画切嬲	DQΞDD
E∣QDQB	DΞQΞQ	QΞΠΞD	ΠΞΠΠQ
DOQ@n	HΞOQΞ	DDΞBD	DDDEIQ
DΞI≡D!≡	DDDQD	SDDDB	HDDHD
HQQDB	QQ^∣ΞΞ	HHDHQ	Q^DDD
500	1,000	1,500	2,000
DQΞΞQ	ΠΠΞQΞ	DΞQQn	ΞΞΞ∏n
ΞΠ□□Q	QQQΞΠ	ΞΞDQD	ΠΞΠΠQ
□ΞOΞΠ	ΞDQD□	DQ□DΞ	QDΞOΠ
ΞQΞ□Ξ	ΞΞΞQQ	ΞΠΠΠΞ	ΞΞDΠΞ
DΠ□ΠH	DBΞQΠ	ΠQΠD□	ΠBΠQD
5,000	10,000	20,000	39,000
Figure 11: In this simulation We used a randomized accept/reject rule, with a decreasing temperature schedule.
——1	1
The algorithm was run for 39,000 iterations, with a temperature schedule of e Ti = 6(i/20000)2 . Proposed
steps which decreased the computed value of the loss function were accepted with probability 1, and proposed
steps which increased the computed value of the loss function were rejected with probability max(0,1 - e-五)
at each iteration i. We ran the simulation 5 times, and obtained similar results each time, with the generator
learning both modes. In this figure, we plotted the generated images from one of the runs at various iterations,
with the iteration number specified at the bottom of each figure (see also Figure 12 for results from the other four
runs)
28
Under review as a conference paper at ICLR 2021
ΞQΞQΞ	⅞DΞDΠ	ΞΞΞΠΠ	ΠΞDΞB	ΞΠ□OD
QΞDDD	ΞΠDΞ^	ΠΞDΠD	DDΞ□D	ΞΞDΞΞ
DDΞ□Π	OG3DΞΞ	SDΞDD	□Ξ□DIΞ	DDΞDD
ΞΠDΠD	SQDUS	ΞΞΠΠI≡	E1E∣ΞE2E]	DDQDΞ
BOQΠΠ	Π□ΠΠΞ	ΠBΠDD	BDDΞD	ΞΠQSΠ
Figure 12: Images generated at the 39,000’th iteration of each of the 5 runs of our algorithm for the simulation
___1
mentioned in Figure 11 with a randomized acceptance rule with a temperature schedule of e Ti
_______1
4+e(i∕20000)2
GDA
Figure 13: The generated points at the 1500’th iteration for all 9 runs of the GDA algorithm with k = 1
discriminator step, for the simulation mentioned in Figure 2. At the 1500’th iteration, GDA had learned exactly
one mode for each of the 9 runs.
discriminator steps, for the simulation mentioned in Figure 2. At the 1500’th iteration, GDA had learned two
modes 65% of the runs, one mode 20% of the runs, and four modes 15 % of the runs.
75% of the runs, two modes 15% of the runs, and three modes 10% of the runs.
Figure 16: The generated points at the 1500’th iteration for all 19 runs of our algorithm, for the simulation
mentioned in Figure 2. Our algorithm used k = 6 discriminator steps and an acceptance rate hyperparameter of
T = 1. By the 1500'th iteration, our algorithm seems to have learned all four modes 68% of the runs, three
modes 26% of the runs, and two modes 5% of the runs.
29
Under review as a conference paper at ICLR 2021
G Extension to functions with compact convex support
In this section we introduce a version of our algorithm (Section G.1) for loss functions with compact
convex support, and run it on a simple bilinear loss function (Section G.2). We also introduce
a version of our ε-local min-max equilibrium (Section G.3) which applies to loss functions with
compact convex support. We then show that, if f is also convex-concave, then, for ε = 0, this ε-local
min-max equilibrium is equivalent to a global min-max point (Section G.4).
G. 1 Projected gradient min-max algorithm for compactly supported loss
Algorithm 4 Algorithm for min-max optimization on compact support
input: A stochastic zeroth-order oracle F for loss function f : X × Y → R where X , Y ⊆ Rd
are compact convex sets. Stochastic gradient oracles Gx for Nxf, and Gy for Vyf. Projection
oracles PX : Rd → X for X and PY : Rd → Y for Y. An initial point (x, y) ∈ X × Y, and an error
parameter ε.
output: A point (x? , y? )
hyperparameters:	rmax	(maximum number of rejections), η >
0.
Set r J 0,i J 0
while r ≤ rmax do
fold J F (x, y),	i J i + 1
Set Gx J Gx (x, y) {Compute a stochastic gradient}
Set x0 J PX (x - ηGx) {Compute the proposed update for the min-player}
Starting at point y, use stochastic gradients Gy (χ0, ∙) to run multiple projected gradient ascent
steps in the y-variable, until a point y0 is reached such that kGy(x0, y0)k2 ≤ ε {Simulate max-
player’s update}
SetjJ0
Set y J y and Stop = False
while Stop = False do
SetjJj+1
Set gy,j J Gy (x, yj )
if 1 ky - PY(y - ηgy,j)k2 > ε0 then
Set j J j + 1
Set y J PY(y - ηgy,j) {Simulate max-player’s update via projected gradient ascent}
else
Set Stop = True
Set fnew J F (x0, y0) {Compute the new loss value}
Set Accept J True.
if fnew > fold — ε∕2, set Accept J False {accept or reject}
if Accept = True then Set x J x0, y J y0, r J 0 {Accept the updates}
else Set r J r + 1 {Reject the updates, and track how many successive steps were rejected.}
return (x, y )
G.2 Numerical simulation on compactly supported bilinear function
In this appendix, we discuss numerical simulations on the bilinear loss function f(x, y) = xy with
compact support (x, y) ∈ [—1, 1] × [—1, 1]. For this function, the gradient descent-ascent algorithm
is known to diverge away from the global min-max point (see for instance Jin et al. (2020)).
This function has global min-max point at every point in the set {(x, y) : x = 0, y ∈ [—1, 1]}. We
ran Algorithm 4 on this function with hyperparamters η = 0.2, ε = 0.06, and rmax = 5, value oracle
F(x, y) = f(x, y), gradient oracle Gy (x, y) = Vyf(x, y), stochastic gradient oracle Vxf(x, y) + ξ
where ξ 〜N(0,1), and initial point (x, y) = (0.4,0.4). After 341 iterations of the outer loop, our
algorithm reached the point (0.0279, —0.9944), which is very close to one of its true global min-max
points, (0, —1).
30
Under review as a conference paper at ICLR 2021
G.3 Local min-max equilibrium for compactly supported loss functions
Global min-max point. First, we recall the definition of global min-max point:
Definition G.1. We say that (x?, y?) ∈ X ×Y is a global min-max point for a function f : X ×Y → R
if
f(χ*,y*)=maγχ f(x*,y)
and
f (x?，y?)=mixmaxf (χ,y).
Local min-max equilibrium for projected subgradients. In this section we introduce a version
of the local min-max equilibrium which applies to compactly supported loss functions (Definition
G.3). The main difference with our previous definition (Definition 2.2) is the need for a projected
gradient to deal with the compact support of the objective function.
In the following we assume that f : X × Y → R, where X, Y ⊂ Rd are two compact convex sets,
and that f is continuously differentiable on XXY. We denote by ▽； the projected gradient in the y
variable for the set Y.
We first formally define “simulated loss” and what it means for f to increase rapidly.
Definition G.2. For any x, y, and ε > 0, define E(ε, x, y) ⊆ Y to be points w s.t. there is a
continuous and (except at finitely many points) differentiable path γ(t) contained in Y 9, starting at
y, ending at w, and moving with “speed" at most 1 in the '∞-norm, ∣∣ 奈Y(t) ∣∣∞ ≤ 1 Such that at
any point on γ, 10
余f(x,γ(t)) >ε.	(33)
We define Lε(x, y) := supw∈E(ε,x,y) f(x, w), and refer to it as the simulated loss.
Definition G.3. Given a distribution Dχ,y, with Pr△〜Dx,y(x + ∆ ∈ X) = 1 for each x,y e Rd,
and ε? ≥ 0, we say that (x?, y?) is an ε?-local min-max equilibrium with respect to the distribution
D if
INYf(x*,y*)kι ≤ ε?, and ,	(34)
Pr△〜Dx*,y* [Lε? (x? + ∆, y?) < Lε? (x?, y?) - ε?] ≤ ε?,	(35)
Remark G.4. As a simple application of Algorithm 4, consider the bilinear loss function f(x, y) =
Xy where X and y are constrained to the set [— 2, ɪ ]. It is easy to see that the set ofglobal min-max
points consists of the points (x, y) where X = 0 and y is any point in [— 11, 2 ]. The local min-max
equilibria according to our definition are the set of points (x, y) where x is any point in [-ε, ε] and y
is any point in [— 1, 1 ].
This is because, when running Algorithm 4 on this example, if x is outside the set [-ε, ε], the max-
player will follow an increasing trajectory to always return a point y = - 1 or y = 1, which means
that, roughly speaking, the min-player is attempting to minimize the function 1 X. This means that the
algorithm will accept all updates X + ∆ for which 1 |x + ∆∣ < 11 |x| — ∣, implying that the algorithm
converges towards a point with |X| ≤ ε.
Thus, as ε goes to zero, the set of local min-max equilibrium points coincides with the set of global
min-max optima for the function f(X, y) = Xy.
This latter fact holds more generally for any convex-concave function with compact convex domain
(see Theorem G.5).
G.4 Comparison of local min-max and global min-max in the compactly
supported convex-concave setting.
The following theorem shows that, in the compactly supported convex-concave setting, a point
(X?, y?) is a local min-max equilibrium for ε = 0 (in the sense of Definition G.3) if and only if it is a
global min-max point:
9In other words there is some τ ≥ 0 such that γ : [0, τ] → Y.
10In this equation the derivative 奈 is taken from the right.
31
Under review as a conference paper at ICLR 2021
Theorem G.5. Let f : X × Y → R be convex-concave, where X, Y ⊆ Rd are compact convex sets.
And let Dx,y be a continuous distribution with support on Y such that, for every (x, y) ∈ X × Y,
Dx,y there is some open ball B ⊂ Rd containing x such that Dx,y has non-zero probability density
at every point in B ∩ Y. Then (x?, y?) is a ε-local min-max equilibrium for ε = 0 if and only if it is
a global min-max point.
Proof. Define the “global max” function ψ(x) := maxy∈Y f(x, y) for all x ∈ X. We start by
showing that the function ψ(x) is convex on the convex set X. Indeed, for any x1, x2 ∈ X and any
λ ∈ [0, 1] we have
λψ(λx1 + (1 - λ)x2) = max f (λx1 + (1 - λ)x2, y)
y∈Y
≤ max[λf (x1, y) + (1 - λ)f (x2, y)]
y∈Y
≤ λ[max f (x1, y)] + (1 - λ)[maxf(x2,y)]
y∈Y	y∈Y
= λψ(x1) + (1 - λ)ψ(x2),
where the second inequality holds by convexity of f (∙, y).
Moreover, we note that, since, for all X ∈ X, f (x, ∙) is continuously differentiable on a compact
convex set, every allowable path (with parameter ε = 0) can be extended to an allowable path whose
endpoint y has projected gradient ▽；f (x?, y) = 0.
Therefore, for every (x, y) ∈ X × Y, there exists an allowable path with initial point y whose endpoint
y satisfies
VY f(χ,y) = 0.	(36)
Since f (x, ∙) is concave, equation 36 implies that
f(x,y) =maχ f (x,y),	(37)
y∈Y
and hence that
Lo(χ,y) = f (X,y).	(38)
Thus, equation 37 and equation 38 imply that
L0(x, y) = ψ(x)	∀(x, y) ∈ X × Y	(39)
since ψ(x) = maxy∈Y f(x, y).
1.	First we prove the “only if” direction:
Suppose that (x?, y?) is a ε-local min-max equilibrium of f for ε = 0. Let y be a global
maximizer of the function f (x?, ∙) (the function achieves its global maximum since it is
continuous and Y is compact). Then the projected gradient at this point is
VY f (x? ,yt) = 0.	(40)
Since f (x, ∙) is concave for all x, and VYf (x?, y?) = 0, at every point y along the line
[yt, y?] connecting the points yt and y?, equation 40 implies that
VyY f (x?, y) =0,	∀y∈ [yt, y?].	(41)
Therefore, equation 41 implies that
f(x?,yt)=f(x?,y?),
and hence that
f(x?, y?) = max f(x?, y),	(42)
y∈Y
since maxy∈Y f(x?, y) = f(x?, yt).
32
Under review as a conference paper at ICLR 2021
Now, since (x?, y?) is a ε-local min-max equilibrium for ε = 0,
Pr	[L0(x? + ∆, y?) < L0(x?, y?)] = 0.
∆〜Dx?,y?
Thus, equation 39 and equation 49 together imply that
Pr	[ψ(x? + ∆) < ψ(x?)] = 0.
△〜Dx?,y?
(43)
(44)
Since ψ is convex, and since there is an open ball B for which Dx? ,y? has non-zero
probability density at every point in B ∩ Y , equation 44 implies that x? is a global minimizer
for ψ :
ψ(x?) = minψ(x).
(45)
Therefore, equation 42 and equation 45 imply that (x?, y?) is a global min-max point for
f : X × Y → R whenever (x?, y?) is a ε-local min-max equilibrium of f for ε = 0.
2.	Next, we prove the “if” direction:
Conversely, suppose that (x? , y?) is a global min-max point for f : X × Y → R. Then
f(x?, y?) = maxy∈Y f(x?, y). Since f is differentiable on X × Y, this implies that
VYf(x*,y*)=0.	(46)
Moreover, since f(x?, y?) is a global min-max point, we also have that
f(x?, y?) = min maxf(x,y) = min ψ(x),
x∈X y∈Y	x∈X
and hence that
ψ(x?) = min ψ(x).	(47)
x∈X
Since we have already shown that ψ is convex, equation 47 implies that
Pr	[ψ(x? +∆) < ψ(x?)] = 0.	(48)
δ~dx*,U?
Since we have also shown in equation 39 that ψ(x) = L0(x, y) for all (x, y) ∈ X × Y,
equation 48 implies that
Pr	[L0(x? + ∆, y?) < L0(x?, y?)] = 0.
∆〜Dx?,y?
(49)
Therefore, equation 46 and equation 49 imply that, for ε = 0, (x?, y?) is a ε-local min-max
equilibrium of f : X × Y → R whenever (x? , y? ) is a global min-max point for f .
□
H Comparison to other min-max algorithms with maximization
SUBROUTINES
Many algorithms for min-max games can be viewed as using an inner maximization subroutine
(e.g., unrolled GANs Metz et al. (2017), and even versions of the GDA algorithm where the max-
player’s update is computed using multiple gradient ascent steps Goodfellow et al. (2014), as well as
Maheswaranathan et al. (2019); Bolte et al. (2020)). However, in contrast to these algorithms, our
algorithm has polynomial-time guarantees on the number of gradient and function evaluations when
f is bounded with Lipschitz Hessian.
In particular, while Bolte et al. (2020) provides theoretical guarantees, there are two key differences
between their work and ours: (i) Their min-max algorithm (Algorithm 3 in their paper) requires
33
Under review as a conference paper at ICLR 2021
access to an oracle which returns the global maximum argmaxyf (x, y) for any input x. However,
since f can be nonconcave in y, computing the global maximum may be intractable and one therefore
may not have access to an oracle for the global maximum value in practice. In contrast, our algorithm
only requires access to a (stochastic) oracle for the gradient and function value of f. (ii) Bolte et al.
(2020) only prove that their algorithm converges asymptotically, and do not provide any bounds on
the time to convergence. In contrast, our algorithm has polynomial-time guarantees on the number of
gradient and function evaluations.
I Comparison to min-max algorithms with converegence
guarantees in nonconvex-nonconcave settings
Multiple works provide min-max optimization algorithms with convergence guarantees in various
settings where f may be nonconvex-nonconcave. However, the convergence results in these works
still require strong assumptions on the loss function. For instance, Mertikopoulos et al. (2019); Lin
et al. (2018); Gidel et al. (2019a) provide convergence guarantees under the assumption that f satisfies
a variational inequality, such as the “coherence” condition of Mertikopoulos et al. (2019). Specifically,
one of the assumptions of this coherence condition is that there exists a global min-max solution
point (x?, y?) for minx maxy f (x, y) which satisfies the variational inequality Exf (x, y),x - x?i -
hVyf (x, y), y - y?i ≥ 0 for all (x, y) in Rd X Rd. This is a relatively strong assumption since it
says that at every point (x, y) ∈ Rd × Rd, the vector field (-Vχf (x, y), Vyf (x, y)) must not point
in a direction “away” from the global min-max point (x?, y?).
Another setting where convergence guarantees have been show for min-max algorithms in the
nonconvex-nonconcave setting is when f satisfies a “sufficient bilinearity” condition Abernethy et al.
(2019). Roughly, this condition says that there is a number γ > 2 such that, at every x, y ∈ Rd,
all the singular values of the cross derivative V2xy f (x, y) are greater than γ. If, in addition, f is
1-Lipschitz then Abernethy et al. (2019) show that their algorithm reaches a first-order ε-stationary
point-that is a point where the gradient for the min- and max- players has magnitude at most ε-in
roughly O(』log(1)) evaluations of a Hessian-vector product of f.
γε
In contrast to these works, our main result only assumes that the loss function is bounded with
Lipschitz Hessian.
34