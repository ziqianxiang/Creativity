Under review as a conference paper at ICLR 2021
BASGD: Buffered Asynchronous SGD
for B yzantine Learning
Anonymous authors
Paper under double-blind review
Ab stract
Distributed learning has become a hot research topic due to its wide application in
cluster-based large-scale learning, federated learning, edge computing and so on.
Most traditional distributed learning methods typically assume no failure or attack
on workers. However, many unexpected cases, such as communication failure
and even malicious attack, may happen in real applications. Hence, Byzantine
learning (BL), which refers to distributed learning with failure or attack, has recently
attracted much attention. Most existing BL methods are synchronous, which are
impractical in some applications due to heterogeneous or offline workers. In these
cases, asynchronous BL (ABL) is usually preferred. In this paper, we propose a
novel method, called buffered asynchronous stochastic gradient descent (BASGD),
for ABL. To the best of our knowledge, BASGD is thefirst ABL method that can
resist malicious attack without storing any instances on server. Compared with
those methods which need to store instances on server, BASGD takes less risk of
privacy leakage. BASGD is proved to be convergent, and be able to resist failure
or attack. Empirical results show that BASGD significantly outperforms vanilla
ASGD and other ABL baselines when there exists failure or attack on workers.
1	Introduction
Due to the wide application in cluster-based large-scale learning, federated learning (Konevcny et al.,
2016; Kairouz et al., 2019), edge computing (Shi et al., 2016) and so on, distributed learning has
recently become a hot research topic (Zinkevich et al., 2010; Yang, 2013; Jaggi et al., 2014; Shamir
et al., 2014; Zhang & Kwok, 2014; Ma et al., 2015; Lee et al., 2017; Lian et al., 2017; Zhao et al.,
2017; Sun et al., 2018; Wangni et al., 2018; Zhao et al., 2018; Zhou et al., 2018; Yu et al., 2019a;b;
Haddadpour et al., 2019). Most traditional distributed learning methods are based on stochastic
gradient descent (SGD) and its variants (Bottou, 2010; Xiao, 2010; Duchi et al., 2011; Johnson &
Zhang, 2013; Shalev-Shwartz & Zhang, 2013; Zhang et al., 2013; Lin et al., 2014; Schmidt et al.,
2017; Zheng et al., 2017; Zhao et al., 2018), and typically assume no failure or attack on workers.
However, in real distributed learning applications with multiple networked machines (nodes), different
kinds of hardware or software failure may happen. Representative failure include bit-flipping in
the communication media and the memory of some workers (Xie et al., 2019). In this case, a
small failure on some machines (workers) might cause a distributed learning method to fail. In
addition, malicious attack should not be neglected in an open network where the manager (or server)
generally has not much control on the workers, such as the cases of edge computing and federated
learning. Some malicious workers may behave arbitrarily or even adversarially. Hence, Byzantine
learning (BL), which refers to distributed learning with failure or attack, has recently attracted much
attention (Diakonikolas et al., 2017; Chen et al., 2017; Blanchard et al., 2017; Alistarh et al., 2018;
Damaskinos et al., 2018; Xie et al., 2019; Baruch et al., 2019; Diakonikolas & Kane, 2019).
Existing BL methods can be divided into two main categories: synchronous BL (SBL) methods and
asynchronous BL (ABL) methods. In SBL methods, the learning information, such as the gradient in
SGD, of all workers will be aggregated in a synchronous way. On the contrary, in ABL methods the
learning information of workers will be aggregated in an asynchronous way. Existing SBL methods
mainly take two different ways to achieve resilience against Byzantine workers which refer to those
workers with failure or attack. One way is to replace the simple averaging aggregation operation
with some more robust aggregation operations, such as median and trimmed-mean (Yin et al., 2018).
1
Under review as a conference paper at ICLR 2021
Krum (Blanchard et al., 2017) and ByzantinePGD (Yin et al., 2019) take this way. The other way is
to filter the suspicious learning information (gradients) before averaging. Representative examples
include ByzantineSGD (Alistarh et al., 2018) and Zeno (Xie et al., 2019). The advantage of SBL
methods is that they are relatively simple and easy to be implemented. But SBL methods will result
in slow convergence when there exist heterogeneous workers. Furthermore, in some applications like
federated learning and edge computing, synchronization cannot even be performed most of the time
due to the offline workers (clients or edge servers). Hence, ABL is preferred in these cases.
To the best of our knowledge, there exist only two ABL methods: Kardam (Damaskinos et al.,
2018) and Zeno++ (Xie et al., 2020). Kardam introduces two filters to drop out suspicious learning
information (gradients), which can still achieve good performance when the communication delay
is heavy. However, when in face of malicious attack, some work finds that Kardam also drops out
most correct gradients in order to filter all faulty (failure) gradients. Hence, Kardam cannot resist
malicious attack (Xie et al., 2020). Zeno++ scores each received gradient, and determines whether
to accept it according to the score. But Zeno++ needs to store some training instances on server for
scoring. In practical applications, storing data on server will increase the risk of privacy leakage or
even face legal risk. Therefore, under the general setting where server has no access to any training
instances, there have not existed ABL methods to resist malicious attack.
In this paper, We propose a novel method, called buffered asynchronous stochastic gradient
descent (BASGD), for ABL. The main contributions of BASGD are listed as follows:	一
•	To the best of our knoWledge, BASGD is the first ABL method that can resist malicious
attack without storing any instances on server. Compared with those methods which need to
store instances on server, BASGD takes less risk of privacy leakage.
•	BASGD is theoretically proved to be convergent, and be able to resist failure or attack.
•	Empirical results show that BASGD significantly outperforms vanilla ASGD and other ABL
baselines when there exist failure or malicious attack on workers. In particular, BASGD can
still converge under malicious attack, when ASGD and other ABL methods fail.
2	Preliminary
This section presents the preliminary of this paper, including the distributed learning framework used
in this paper and the definition of Byzantine worker.
2.1	Distributed Learning Framework
Many machine learning models, such as logistic regression and deep neural networks, can be
formulated as the following finite sum optimization problem:
min F (w)
w∈Rd
1n
n Xf (W; Zi)，
i=1
(1)
where W is the parameter to learn, d is the dimension of parameter, n is the number of training
instances, f(W; zi) is the empirical loss on the training instance zi. The goal of distributed learning is
to solve the problem in (1) by designing learning algorithms based on multiple networked machines.
Although there have appeared many distributed learning frameworks, in this paper we focus on the
widely used Parameter Server (PS) framework (Li et al., 2014). In a PS framework, there are several
workers and one or more servers. Each worker can only communicate with server(s). There may exist
more than one server in a PS framework, but for the problem of this paper servers can be logically
conceived as a unity. Without loss of generality, we will assume there is only one server in this paper.
Training instances are disjointedly distributed across m workers. Let Dk denote the index set of
training instances on worker_k, we have ∪m=ιDk = {1, 2,...,n} and Dk ∩ Dk，= 0 if k = k0. In
this paper, we assume that server has no access to any training instances. If two instances have the
same value, they are still deemed as two distinct instances. Namely, zi may equal zi0 (i 6= i0).
One popular asynchronous method to solve the problem in (1) under the PS framework is ASGD (Dean
et al., 2012) (see Algorithm 1 in Appendix A). In this paper, we assume each worker samples one
instance for gradient computation each time, and do not separately discuss the mini-batch case.
2
Under review as a conference paper at ICLR 2021
In PS based ASGD, server is responsible for updating and maintaining the latest parameter. The
number of iterations that server has already executed is used as the global logical clock of server.
At the beginning, iteration number t = 0. Each time a SGD step is executed, t will increase by 1
immediately. The parameter after t iterations is denoted as wt. If server sends parameters to worker,k
at iteration t0, some SGD steps may have been excuted before server receives gradient from worker,k
next time at iteration t. Thus, We define the delay of worker_k at iteration t as Tk = t -10. Worker,k
is heavily delayed at iteration t if τkt > τmax , where τmax is a pre-defined non-negative constant.
2.2	Byzantine Worker
For workers that have sent gradients (one or more) to server at iteration t, we call worker_k loyal
worker if it has finished all the tasks without any fault and each sent gradient is correctly received by
the server. Otherwise, worker_k is called Byzantine worker. If worker_k is a Byzantine worker, it
means the received gradient from worker_k is not credible, which can be an arbitrary value. In ASGD,
there is one received gradient at a time. Formally, we denote the gradient received from worker_k at
iteration t as gkt . Then, we have:
gt = f Vf (wt0; zi),	if worker_k is loyal at iteration t;
1 arbitrary value,	if worker_k is Byzantine at iteration t,
where 0 ≤ t0 ≤ t, and i is randomly sampled from Dk.
Our definition of Byzantine worker is consistent with most previous works (Blanchard et al., 2017;
Xie et al., 2019; 2020). Either accidental failure or malicious attack will result in Byzantine workers.
3	Buffered Asynchronous SGD
In synchronous BL, gradients from all workers are received at each iteration. During this process,
we can compare the gradients with each other, and then filter suspicious ones, or use more robust
aggregation rules such as median and trimmed-mean for updating. However, in asynchronous BL,
only one gradient is received by the server at a time. Without any training instances stored on server,
it is difficult for server to identify whether a received gradient is credible or not.
In order to deal with this problem in asynchronous BL, we propose a novel method called buffered
asynchronous SGD (BASGD). BASGD introduces B buffers (0 < B ≤ m) on server, and the
gradient used for updating parameters will be aggregated from these buffers. The detail of the
learning procedure of BASGD is presented in Algorithm 2 in Appendix A. In this section, we will
introduce the details of the two key components of BASGD: buffer and aggregation function.
3.1	Buffer
In BASGD, the m workers do the same job as that in ASGD, while the updating rule on server is
modified. More specifically, there are B buffers (0 < B ≤ m) on server. When a gradient g from
worker_s is received, it will be temporarily stored in buffer b, where b = S mod B, as illustrated in
Figure 1. Only when each buffer has stored at least one gradient, a new SGD step will be executed.
Please note that no matter whether a SGD step is executed or not, the server will immediately send
the latest parameters back to the worker after receiving a gradient. Hence, BASGD introduces no
barrier, and is an asynchronous algorithm.
For each buffer b, more than one gradient may have been received at iteration t. We will store
the average of these gradients (denoted by hb) in buffer b. Assume that there are already (N - 1)
gradients gι, g2,..., gN-ι which should be stored in buffer b, and %(。⑷ = N-ɪ PN-I gi. When
the N-th gradient gN is received, the new average value in buffer b should be:
hb(new) = N PN=I gi = NN ∙ hb(old) + N ∙ gN.
This is the updating rule for each buffer b when a gradient is received. We use Nbt to denote the
total number of gradients stored in buffer b at the t-th iteration. After the parameter w is updated,
all buffers will be zeroed out at once. With the benefit of buffers, server has access to B candidate
gradients when updating parameter. Thus, a more reliable (robust) gradient can be aggregated from
the B gradients of buffers, if a proper aggregation function Aggr(∙) is chosen.
3
Under review as a conference paper at ICLR 2021
Figure 1: An example of buffers. Circle represents worker, and the number is worker ID. There are
15 workers and 5 buffers. The gradient received from worker_s is stored in buffer_{s mod 5}.
3.2	Aggregation Function
When a SGD step is ready to be executed, there are B buffers providing candidate gradients. An
aggregation function is needed to get the final gradient for updating. A naive way is to take the mean
of all candidate gradients. However, mean value is sensitive to outliers which are common in BL. For
designing proper aggregation functions, we first define the q-Byzantine Robust (q-BR) condition to
quantitatively describe the Byzantine resilience ability of an aggregation function.
Definition 1 (q-Byzantine Robust). For an aggregation function Aggr (∙): Aggr ([h1,..., hB ]) = G,
where G = [G1,..., Gd]t and hb = [hb1,..., hbd]t, ∀b ∈ [B], we call Aggr(∙) q-Byzantine
Robust (q ∈ Z, 0 < q < B/2), if it satisfies the following two properties:
(a)	. Aggr([h1 + h0, . . . , hB + h0]) = Aggr([h1, . . . , hB]) + h0, ∀h1, . . . , hB ∈ Rd,∀h0 ∈ Rd;
(b)	. mins∈S {hsj} ≤ Gj ≤ maxs∈S {hsj}, ∀j ∈ [d], ∀S ⊂ [B] with |S| = B - q,
Intuitively, property (a) in Definition 1 says that if all candidate gradients hi are added by a same
vector h0 , the aggregated gradient will also be added by h0 . Property (b) says that for each coordinate
j, the aggregated value Gj will be between the (q + 1)-th smallest value and the (q + 1)-th largest
value among the j -th coordinates of all candidate gradients. Thus, the gradient aggregated by a q-BR
function is insensitive to at least q outliers. We can find that q-BR condition gets stronger when q
increases. In other words, if Aggr(∙) is q-BR, then for any 0 < q0 < q, Aggr(∙) is also q0-BR.
Remark 1. It is not hard to find that when B > 1, mean function is not q-Byzantine Robust for any
q > 0. We illustrate this by a one-dimension example: h1, . . . , hB-1 ∈ [0, 1], and hB = 10 × B.
Then -B PB=I hb ≥ hB = 10 ∈ [0,1]. Namely, the mean is larger than any of the first B - 1 values.
We find that the following two aggregation functions satisfy Byzantine Robust condition.
Definition 2 (Coordinate-wise median (Yin et al., 2018)). For candidate gradients h1, h2, . . . , hB ∈
Rd, hb = [hb1, hb2, . . . , hbd]T, ∀b = 1, 2, . . . , B. Coordinate-wise median is defined as:
Med([hι,..., hB ]) = [Med(h∙ι),..., Med(h∙d )]t ,
where Med(hj∙) is the scalar median of the j -th coordinates, ∀j = 1, 2,..., d.
Definition 3 (Coordinate-wise q-trimmed-mean (Yin et al., 2018)). For any positive interger q <
B/2 and candidate gradients h1, h2, . . . , hB ∈ Rd, hb = [hb1, hb2, . . . , hbd]T, ∀b = 1, 2, . . . , B.
Coordinate-wise q-trimmed-mean is defined as:
Trm([hι,..., hB]) = [Trm(h∙ι),..., Trm(h∙d)]t,
where Trm(hj) is the scalar q-trimmed-mean: Trm(hj) = B-2q Eb∈M, hbj∙. Mj is the subset
of{hbj}bB=1 obtained by removing the q largest elements and q smallest elements.
In the following content, coordinate-wise median and coordinate-wise q-trimmed-mean are also called
median and trmean, respectively. Proposition 1 shows the q-BR property of these two functions.
Proposition 1. Coordinate-wise q-trmean is q-BR, and coordinate-wise median is [ B-IC -BR.
Here, bxc is the maximum integer not larger than x. According to Proposition 1, both median and
trmean are proper choices for aggregation function in BASGD. The proof can be found in Appendix B.
4
Under review as a conference paper at ICLR 2021
Now we define another class of aggregation functions, which is also important in analysis in Section 4.
Definition 4 (Stable aggregation function). Aggregation function Aggrg is said to be stable provided
that ∀hι,..., hB, h 1,..., hB ∈ Rd, letting δ = (PB=I ∣∣hb 一 hbk2)2, we have:
11 A	/1	1	∖	A	(Λ	1'	∖ 11 JC
∣Aggr(h1, . . . ,hB) 一 Aggr(h1, . . . ,hB)∣ ≤ δ.
If Aggr(∙) is a stable aggregation function, it means that when there is a disturbance with L2-norm δ
on buffers, the disturbance of aggregated result will not be larger than δ.
Definition 5 (Effective aggregation function). A stable aggregation function Aggrg is called an
(A1, A2)-effective aggregation function, provided that when there are at most r Byzantine workers
and Tk = 0 for each loyal workerk (∀t = 0,1,...,T - 1), it satisfies thefolloWing two properties:
(i)	. E[VF(wt)TGSyn | wt] ≥ ∣∣VF(Wt)II2 - Ai, ∀wt ∈ Rd;
(ii)	. E[∣Gtsyn∣2 | wt] ≤ (A2)2, ∀wt ∈ Rd;
where A1,A2 ∈ R+ are two non-negative constants, Gsyn is the gradient aggregated by Aggr(∙) at
the t-th iteration in cases without delay (τmax = 0).
For different aggregation functions, constants A1 and A2 may differ. A1 and A2 are also related to
loss function F(∙), distribution of instances, buffer number B, maximum Byzantine worker number r
and so on. Inequalities (i) and (ii) in Definition 5 are two important properties in convergence proof of
synchronous Byzantine learning methods. As revealed in (Yang et al., 2020), there are many existing
asynchronous Byzantine learning methods. Krum, median, and trimmed-mean are proved to satisfy
these two properties (Blanchard et al., 2017; Yin et al., 2018). SignSGD (Bernstein et al., 2019) can
be seen as a combination of 1-bit quantization and median aggregation, while median satisfies the
properties. Bulyan (Guerraoui et al., 2018) uses an existing aggregation rule to obtain a new one, and
the property of Bulyan is difficult to be analyzed alone. Zeno (Xie et al., 2019) has an asynchronous
version called Zeno++ (Xie et al., 2020), and it is meaningless to check the properties for Zeno.
Please note that too large B will slow down the updating frequency and damage the performance,
which is supported by both theoretical (in Appendix B) and empirical (in Section 5) results. In
practical application, we could estimate Byzantine worker number r in advance, and set B to make
Aggr(∙) be r-BR. Specially, B is suggested to be (2r + 1) for median, since median is [B-1C -BR.
4 Convergence
In this section, we theoretically prove the convergence and resilience of BASGD against failure
or attack. There are two main theorems. The first theorem presents a relatively loose but general
bound for all q-BR aggregation functions. The other one presents a relatively tight bound for
each distinct (A1, A2)-effective aggregation function. Since the definition of (A1, A2)-effective
aggregation function is usually more difficult to verify than q-BR property, the general bound is also
useful. Here we only present the results. Proof details are in Appendix B. We first make the following
assumptions, which also have been widely used in stochastic optimization.
Assumption 1. Global loss function F (w) is bounded below: ∃F * ∈ R,F(w) ≥ F *, ∀w ∈ Rd.
Assumption 2 (Bounded bias). For any loyal worker, it can use locally stored training instances to
estimate global gradient with bounded bias κ: ∣E[Vf (w; zi)] - VF (w)∣ ≤ κ, ∀w ∈ Rd .
Assumption 3 (Bounded gradient). VF (w) is bounded: ∃D ∈ R+, ∣VF (w)∣ ≤ D, ∀w ∈ Rd.
Assumption 4 (Bounded variance). E[||Vf (w; zi) - E[Vf (w; zi) | w]||2 | w] ≤ σ2, ∀w ∈ Rd.
Assumption 5 (L-smoothness). Global loss function F(w) is differentiable and L-smooth:
||VF (w) - VF (w0)|| ≤ L||w - w0||, ∀w, w0 ∈ Rd.
Remark 2. Please note that we do not give any assumption about convexity. The analysis in this
section is suitable for both convex and non-convex models in machine learning, such as logistic
regression and deep neural networks. Also, we do not give any assumption about the behavior of
Byzantine workers, which may behave arbitrarily.
Let N(t) be the (q + 1)-th smallest value in {Nbt}b∈[B], Nbt is the total number of gradients stored in
buffer b at the t-th iteration. We define the constant 八6总/= √(B-r)；B-r+：i), which will appear
in Lemma 1 and Lemma 2.
5
Under review as a conference paper at ICLR 2021
Lemma 1. If Aggr(∙) is q-BR, and there are at most r Byzantine workers (r ≤ q), we have:
E[∣∣Gt∣∣2 | wt] ≤ ΛB,q,rd ∙ (D2 + σ2∕N㈤).
Lemma 2. If Aggrg is q-BR, and the total number of heavily delayed workers and Byzantine
workers is not larger than r (r ≤ q), we have:
||E[Gt - VF (wt) | wt]|| ≤ AB,q,r d ∙ (TmaxL ∙ [AB,q,rd(D2 + σ2 /N(t))] 2 + σ + K).
Theorem 1.	Let D= T PU(Dl2 + σ2∕N(t)) 1. If Aggr(∙) is q-BR, B = O(r), and the total
number of heavily delayed workers and Byzantine workers is not larger than r (r ≤ q), set learning
rate η = O( γ√r), we have:
PT01 E[||VF(Wt)|12] ≤O (L[F(R-F*]) + O ( .r rdD 1 . ! + O (-^Ddσ-τ)
T	T T 2 J T2 2 (q - r + 1)2)	∖(q — r + 1)2 J
rDdκ ) + o ( r 3 LDDd3 Tmax )
(q - r + 1)1 )	∖ (q - r + 1)3 )
+O
Please note that the convergence rate of vanilla ASGD is O(T-2). Hence, Theorem 1 indicates that
BASGD has a theoretical convergence rate as fast as vanilla ASGD, with an extra constant variance.
The term O(rDdσ(q - r + 1)-2) is caused by the aggregation function, which can be deemed as a
sacrifice for Byzantine resilience. The term O(rDdκ(q - r + 1)- 1) is caused by the differences of
training instances among different workers. In independent and identically distributed (i.i.d.) cases,
κ = 0 and the term vanishes. The term O(r3 LDDd2τm,ax(q - r + 1)-3) is caused by the delay,
and related to parameter Tmax. The term is also related to the buffer size. When Nbt increases, N(t)
may increase, and thus D will decrease. Namely, larger buffer size will result in smaller D. Besides,
the factor (q - r + 1)- 1 or (q - r + 1)-3 decreases as q increases, and increases as r increases.
Although general, the bound presented in Theorem 1 is relatively loose in high-dimensional cases,
since d appears in all the three extra terms. To obtain a tighter bound, we introduce Theorem 2 for
BASGD with (A1, A2)-effective aggregation function (Definition 5).
Theorem 2.	If the total number of heavily delayed workers and Byzantine workers is not larger than
r, B = O(r), and Aggr(∙) is an (Ai, A∣)-effective aggregation function in this case. Set learning
rate η = O( √L^), and in general asynchronous cases, we have:
PT01 E[∣∣VF (Wt)II2 ]
≤O
L2 [F(w0) - F*]] +O LL1 TmaxDA2r1
T 2	I T	T 2
T
Theorem 2 indicates that if Aggr(∙) makes a synchronous BL method converge (i.e., satisfies
Definition 5), BASGD converges when using Aggr(∙) as aggregation function. Hence, BASGD can
also be seen as a technique of asynchronization. That is to say, new asynchronous methods can
be obtained from synchronous ones when using BASGD. The extra constant term A1 is caused by
gradient bias. When there is no Byzantine workers (r = 0), and instances are i.i.d. across workers,
letting B = 1 and Aggr(h1, . . . , hB) = Aggr(h1) = h1, BASGD degenerates to vanilla ASGD.
Under this circumstance, there is no gradient bias (A1 = 0), and the extra constant term vanishes.
In general cases, Theorem 2 guarantees BASGD to find a point such that the squared L2-norm of its
gradient is not larger than A1 (but not necessarily around a stationary point), in expectation. Please
note that Assumption 3 already guarantees that gradient’s squared L2-norm is not larger than D2. We
introduce Proposition 2 to show that A1 is guaranteed to be smaller than D2 under a mild condition.
Proposition 2. Aggr(∙) is an (Aι,A∣)-effective aggregation function, and Gsyn is aggregated by
Aggr(∙) in synchronous setting. If E[∣Gsyn — VF(Wt)k | wt] ≤ D, ∀wt ∈ Rd, then Ai ≤ D2.
Gsyn is the aggregated result of Aggr(∙), and is a robust estimator of VF(wt) used for updating.
Since IVF (wt)I ≤ D, VF (wt) locates in a ball with radius D. E[IGtsyn - VF (wt)I | wt] ≤ D
means that the bias of Gsyn is not larger than the radius D, which is a mild condition for Aggr(∙).
6
Under review as a conference paper at ICLR 2021
95
90
85
65
50
45
20
40
0
40	60	80	100	120	140	160
Epoch
75
<
70
-≡- ASGD
+ BASGD with median (B=5)
BASGD with median (B=10)
BASGD with median (B=15)
-θ-BASGD with median (B=30)
-⅛- Kardam (7=2)
∙*∙ Kardam ()=10)________
90
85
80
75
<
70
65
(υ
60
ra
55
<
50
45
40
-≡-- ASGD
T- BASGD with trmean (B=5, q=1)
BASGD with trmean (B=10, q=3)
BASGD with trmean (B=15, q=5)
-θ-BASGD with trmean (B=30, q=10)
∙∙*∙∙ Kardam (τ=2)
••茨•• Kardam (τ=10)_____________
0	20	40	60	80	100	120	140	160
(a) no attack
(c) 3 Byzantine workers (RD-attack)
Epoch
(b) no attack
(d) 3 Byzantine workers (NG-attack)
(e) 6 Byzantine workers (RD-attack)
Figure 2: Average top-1 test accuracy w.r.t. epochs when there are no Byzantine workers (the first
row), 3 Byzantine workers (the second row) and 6 Byzantine workers (the last row), respectively.
Subfigures (c) and (e) are for RD-attack, while Subfigures (d) and (f) for NG-attack.
90
80
70
40
30
20
10
-B-ASGD
+ BASGD with median (B=15)
BASGD with 6-trimmed mean (B=15)
—Kardam (γ=6)
-θ- Kardam (∙γ=10)
-*-Kardam (γ=14)_________________
0-------ι-----ι------ι-----1-----1------1-----1-----1
0	20	40	60	80	100	120	140	160
Epoch
(f) 6 Byzantine workers (NG-attack)
As many existing works have indicated (Assran et al., 2020; Nokleby et al., 2020), speed-up is also
an important aspect of distributed learning methods. In BASGD, different workers can compute
gradients concurrently, make each buffer be filled more quickly, and thus speed up the model updating.
However, we mainly focus on Byzantine-resilience in this work. Speed-up will be thoroughly studied
in future work. Besides, heavily delayed workers are considered as Byzantine in the current analysis.
We will analyze heavily delayed worker’s behavior more finely to obtain better results in future work.
5	Experiment
In this section, we empirically evaluate the performance of BASGD and baselines in both image
classification (IC) and natural language processing (NLP) applications. Our experiments are con-
ducted on a distributed platform with dockers. Each docker is bound to an NVIDIA Tesla V100 (32G)
GPU (in IC) or an NVIDIA Tesla K80 GPU (in NLP). Please note that different GPU cards do not
affect the reported metrics in the experiment. We choose 30 dockers as workers in IC, and 8 dockers
in NLP. An extra docker is chosen as server. All algorithms are implemented with PyTorch 1.3.
7
Under review as a conference paper at ICLR 2021
Table 1: Filtered ratio of received gradients in Kardam under NG-attack (3 Byzantine workers)
Term		I By Frequency Filter		By Lipschitz Filter	In total
LOYAL GRADS (γ		3)	10.15% (31202/307530)	40.97% (126000/307530)	51.12%
B yzantine Grads	(γ	= 3)	10.77% (3681/34170)	40.31% (13773/34170)	51.08%
LOYAL GRADS (γ		8)	28.28% (86957/307530)	28.26% (86893/307530)	56.53%
B yzantine Grads	(γ	= 8)	28.38% (9699/34170)	28.06% (9588/34170)	56.44%
L OYAL GRADS (γ	=	14)	85.13% (261789/307530)	3.94% (12117/307530)	89.07%
B yzantine Grads	(γ	= 14)	84.83% (28985/34170)	4.26% (1455/34170)	89.08%
5.1	Experimental Setting
We compare the performance of different methods under two types of attack: negative gradient
attack (NG-attack) and random disturbance attack (RD-attack). Byzantine workers with NG-attack
send gNG = -katk ∙ g to server, where g is the true gradient and katk ∈ R+ is a parameter. Byzantine
workers with RD-attack send gRD = g + g”d to server, where g”d is a random vector sampled
from normal distribution N(0, ∣∣σatkg∣∣2 ∙ I). Here, σ0tk is a parameter and I is an identity matrix.
NG-attack is a typical kind of malicious attack, while RD-attack can be seen as an accidental failure
with expectation 0. Besides, each worker is manually set to have a delay, which is kdel times the
computing time. Training set is randomly and equally distributed to different workers. We use the
average top-1 test accuracy (in IC) or average perplexity (in NLP) on all workers w.r.t. epochs as
final metrics. For BASGD, we use median and trimmed-mean as aggregation function.
Because BASGD is an ABL method, SBL methods cannot be directly compared with BASGD. The
ABL method Zeno++ either cannot be directly compared with BASGD, because Zeno++ needs to
store some instances on server. The number of instances stored on server will affect the performance of
Zeno++ (Xie et al., 2020). Hence, we compare BASGD with ASGD and Kardam in our experiments.
We set dampening function Λ(τ) = 1+^ for Kardam as suggested in (Damaskinos et al., 2018).
5.2	Image Classification Experiment
In IC experiment, algorithms are evaluated on CIFAR-10 (Krizhevsky et al., 2009) with deep learning
model ResNet-20 (He et al., 2016). Cross-entropy is used as the loss function. We set katk = 10 for
NG-attack, and σatk = 0.2 for RD-attack. kdel is randomly sampled from truncated standard normal
distribution within [0, +∞). As suggested in (He et al., 2016), learning rate η is set to 0.1 initially
for each algorithm, and multiplied by 0.1 at the 80-th epoch and the 120-th epoch respectively. The
weight decay is set to 10-4. We run each algorithm for 160 epochs. Batch size is set to 25.
Firstly, we compare the performance of different methods when there are no Byzantine workers.
Experimental results with median and trmean aggregation functions are illustrated in Figure 2(a) and
Figure 2(b), respectively. ASGD achieves the best performance. BASGD (B > 1) and Kardam have
similar convergence rate to ASGD, but both sacrifice a little accuracy. Besides, the performance of
BASGD gets worse when the buffer number B increases, which is consistent with the theoretical
results. Please note that ASGD is a degenerated case of BASGD when B = 1 and Aggr(h1) = h1.
Hence, BASGD can achieve the same performance as ASGD when there is no failure or attack.
Then, for each type of attack, we conduct two experiments in which there are 3 and 6 Byzantine
workers, respectively. We respectively set 10 and 15 buffers for BASGD in these two experiments.
For space saving, we only present average top-1 test accuracy in Figure 2(c) and Figure 2(d) (3
Byzantine workers), and Figure 2(e) and Figure 2(f) (6 Byzantine workers). Results about training
loss are in Appendix C. We can find that BASGD significantly outperforms ASGD and Kardam
under both RD-attack (accidental failure) and NG-attack (malicious attack). Under the less harmful
RD-attack, although ASGD and Kardam still converge, they both suffer a significant loss on accuracy.
Under NG-attack, both ASGD and Kardam cannot converge, even if we have tried different values
of assumed Byzantine worker number for Kardam, which is denoted by a hyper-parameter γ in this
paper. Hence, both ASGD and Kardam cannot resist malicious attack. On the contrary, BASGD still
has a relatively good performance under both types of attack.
8
Under review as a conference paper at ICLR 2021
(a) RD-attack
10广104
(b) RD-attack (magnified)
ytixelpreP fo mhtiragoL
aS ASGD (no attack)
τ- BASGD With median (B=3)
T-Kardam (7=1)
-⅛- Kardam (∙γ=3)
*ASGD
0	5	10	15	20	25	30	35	40
Epoch
800
1000
900
0---------1-------1------1-------1-------1-------1-------1-------1
0	5	10	15	20	25	30	35	40
Epoch
(d) NG-attack (magnified)
(c) NG-attack
Figure 3: Average perplexity w.r.t. epochs with 1 Byzantine worker. Subfigures (a) and (b) are
for RD-attack, while Subfigures (c) and (d) for NG-attack. Due to the differences in magnitude
of perplexity, y-axes of Subfigures (a) and (c) are in log-scale. In addition, Subfigures (b) and (d)
illustrates that BASGD converges with only a little loss in perplexity compared to the gold standard.
Moreover, we count the ratio of filtered gradients in Kardam, which is shown in Table 1. We can
find that in order to filter Byzantine gradients, Kardam also filters approximately equal ratio of loyal
gradients. It explains why Kardam performs poorly under malicious attack.
5.3	Natural Language Processing Experiment
In NLP experiment, the algorithms are evaluated on the WikiText-2 dataset with LSTM networks.
We only use the training set and test set, while the validation set is not used in our experiment. For
LSTM, we adopt 2 layers with 100 units in each. Word embedding size is set to 100, and sequence
length is set to 35. Gradient clipping size is set to 0.25. Cross-entropy is used as the loss function.
For each algorithm, we run each algorithm for 40 epochs. Initial learning rate η is chosen from
{1, 2, 5, 10, 20}, and is divided by 4 every 10 epochs. The best test result is adopted as the final one.
The performance of ASGD under no attack is used as gold standard. We set katk = 10 and σatk = 0.1.
One of the eight workers is Byzantine. kdel is randomly sampled from exponential distribution with
parameter λ = 1. Each experiment is carried out for 3 times, and the average perplexity is reported
in Figure 3. We can find that BASGD converges under each kind of attack, with only a little loss in
perplexity compared to the gold standard (ASGD without attack). On the other hand, ASGD and
Kardam both fail, even if we have set the largest γ (γ = 3) for Kardam.
6	Conclusion
In this paper, we propose a novel method called BASGD for asynchronous Byzantine learning. To
the best of our knowledge, BASGD is the first ABL method that can resist malicious attack without
storing any instances on server. Compared with those methods which need to store instances on
server, BASGD takes less risk of privacy leakage. BASGD is proved to be convergent, and be able to
resist failure or attack. Empirical results show that BASGD significantly outperforms vanilla ASGD
and other ABL baselines, when there exists failure or attack on workers.
9
Under review as a conference paper at ICLR 2021
References
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in
Neural Information Processing Systems,pp. 4613-4623, 2018.
B. M. Assran, A. Aytekin, H. R. Feyzmahdavian, M. Johansson, and M. G. Rabbat. Advances in
asynchronous parallel and distributed optimization. Proceedings of the IEEE, 108(11):2013-2031,
2020. doi: 10.1109/JPROC.2020.3026619.
Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for
distributed learning. In Advances in Neural Information Processing Systems, pp. 8635-8645, 2019.
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD with
majority vote is communication efficient and fault tolerant. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=BJxhijAcY7.
Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries: Byzantine
tolerant gradient descent. In Advances in Neural Information Processing Systems, pp. 119-129,
2017.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings ofthe
International Conference on Computational Statistics, pp. 177-186. Springer, 2010.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing
Systems, 1(2):1-25, 2017.
Georgios Damaskinos, Rachid Guerraoui, Rhicheek Patra, Mahsa Taziki, et al. Asynchronous
Byzantine machine learning (the case of SGD). In Proceedings of the International Conference on
Machine Learning, pp. 1145-1154, 2018.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In
Advances in Neural Information Processing Systems, pp. 1223-1231, 2012.
Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust
statistics. arXiv preprint arXiv:1911.05911, 2019.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Being robust (in high dimensions) can be practical. In Proceedings of the International Conference
on Machine Learning, pp. 999-1008, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Rachid Guerraoui, Sebastien Rouault, et al. The hidden vulnerability of distributed learning in
byzantium. In International Conference on Machine Learning, pp. 3521-3530, 2018.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading
redundancy for communication: Speeding up distributed SGD for non-convex optimization. In
Proceedings of the International Conference on Machine Learning, pp. 2545-2554, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Martin Jaggi, Virginia Smith, Martin Takac, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann,
and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in
Neural Information Processing Systems, pp. 3068-3076, 2014.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
10
Under review as a conference paper at ICLR 2021
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv:1912.04977, 2019.
Jakub Konevcny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh,
and Dave Bacon. Federated learning: Strategies for improving communication efficiency.
arXiv:1610.05492, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, 2009.
Jason D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance reduced
gradient methods by sampling extra data with replacement. The Journal of Machine Learning
Research,18(1):4404-4446, 2017.
Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed
machine learning with the parameter server. In Advances in Neural Information Processing Systems,
pp. 19-27, 2014.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method. In
Advances in Neural Information Processing Systems, pp. 3059-3067, 2014.
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richtarik, and Martin Takac.
Adding vs. averaging in distributed primal-dual optimization. In Proceedings of the International
Conference on Machine Learning, pp. 1973-1982, 2015.
Matthew Nokleby, Haroon Raja, and Waheed U Bajwa. Scaling-up distributed processing of data
streams for machine learning. arXiv preprint arXiv:2005.08854, 2020.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss
minimization. Journal of Machine Learning Research, 14(Feb):567-599, 2013.
Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using
an approximate newton-type method. In Proceedings of the International Conference on Machine
Learning, pp. 1000-1008, 2014.
Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. Edge computing: Vision and
challenges. IEEE Internet of Things Journal, 3(5):637-646, 2016.
Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, and Tie-Yan Liu. Slim-dp: a multi-agent system
for communication-efficient distributed deep learning. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 721-729, 2018.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Advances in Neural Information Processing Systems, pp.
1299-1309, 2018.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In Proceedings of the International Conference on Machine
Learning, pp. 6893-6901, 2019.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous SGD. In
Proceedings of the International Conference on Machine Learning, 2020.
11
Under review as a conference paper at ICLR 2021
Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate
ascent. In Advances in Neural Information Processing Systems, pp. 629-637, 2013.
Zhixiong Yang, Arpita Gang, and Waheed U Bajwa. Adversary-resilient distributed and decentralized
statistical inference and machine learning: An overview of recent advances under the byzantine
threat model. IEEE Signal Processing Magazine, 37(3):146-159, 2020.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In Proceedings of the International Conference on
Machine Learning, pp. 5650-5659, 2018.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Defending against saddle point
attack in byzantine-robust distributed learning. In Proceedings of the International Conference on
Machine Learning, pp. 7074-7084, 2019.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient
momentum SGD for distributed non-convex optimization. In Proceedings of the International
Conference on Machine Learning, pp. 7184-7193, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019b.
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number indepen-
dent access of full gradients. In Advances in Neural Information Processing Systems, pp. 980-988,
2013.
Ruiliang Zhang and James Kwok. Asynchronous distributed admm for consensus optimization. In
Proceedings of the International Conference on Machine Learning, pp. 1701-1709, 2014.
Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, and Wu-Jun Li. SCOPE: scalable composite
optimization for learning on spark. In Proceedings of the Thirty-First AAAI Conference on Artificial
Intelligence, pp. 2928-2934. AAAI Press, 2017.
Shen-Yi Zhao, Gong-Duo Zhang, Ming-Wei Li, and Wu-Jun Li. Proximal SCOPE for distributed
sparse learning. In Advances in Neural Information Processing Systems, pp. 6551-6560, 2018.
Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu.
Asynchronous stochastic gradient descent with delay compensation. In Proceedings of the Interna-
tional Conference on Machine Learning, pp. 4120-4129, 2017.
Yi Zhou, Yingbin Liang, Yaoliang Yu, Wei Dai, and Eric P Xing. Distributed proximal gradient
algorithm for partially asynchronous computer clusters. The Journal of Machine Learning Research,
19(1):733-764, 2018.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient
descent. In Advances in Neural Information Processing Systems, pp. 2595-2603, 2010.
12
Under review as a conference paper at ICLR 2021
Algorithm 1 Asynchronous SGD (ASGD)
Server:
Initialization: initial parameter w0, learning rate η;
Send initial w0 to all workers;
for t = 0 to tmax - 1 do
Wait until a new gradient gk is received from arbitrary worker,k;
Execute SGD step: wt+1 J Wt - η ∙ gk;
Send wt+1 back to worker_k;
end for
Notify all workers to stop;
Worker_k: (k = 0,1,…，m — 1)
repeat
Wait until receiving the latest parameter w from server;
Randomly sample an index i from Dk;
Compute Vf (w; Zi);
Send Vf (w; Zi) to server;
until receive server’s notification to stop
A Algorithm Details
A.1 Asynchronous SGD (ASGD)
One popular asynchronous method to solve the problem in (1) under the PS framework is ASGD (Dean
et al., 2012), which is presented in Algorithm 1.
A.2 Buffered Asynchronous SGD (BASGD)
The details of learning procedure in BASGD is presented in Algorithm 2.
B Proof Details
B.1 Proof of Proposition 1
Proof. Firstly, we prove coordinate-wise q-trimmed-mean is q-BR. It is not hard to check that trmean
satisfies the property (a) in the definition of q-BR, then we prove that it also satisfies property (b).
Without loss of generality, we assume h1j, . . . , hBj are already in descending order. By definition,
Trm(h∙j) is the average value of Mj, which is obtained by removing q largest values and q smallest
values of {hij}iB=1. Therefore,
h(q+1)j
max {x} ≥ Trm(hj) ≥ min {x} = h(n-qj
For any S ⊂ [B] with |S| = B - q, by Pigeonhole Principle, S includes at least one of
h1j, . . . , h(q+1)j, and includes at least one of h(n-q)j, . . . , hBj. Therefore,
max{hsj } ≥ h(q+1)j ;	min{hsj } ≤ h(n-q)j .
Combining these two inequalities, we have:
max{hsj} ≥ Trm(h∙j) ≥ min{hsj}.
s∈S	s∈S
Thus, coordinate-wise q-trimmed-mean is q-BR.
By definition, coordinate-wise median can be seen as [B-1 C-trimmed-mean, and thus is [B-1 C-BR.
□
13
Under review as a conference paper at ICLR 2021
Algorithm 2 Buffered Asynchronous SGD (BASGD)
Server:
Input: learning rate η, buffer number B,
aggregation function: Aggr(∙);
Initialization: initial parameter w0, learning rate η;
Send initial w0 to all workers;
Set t — 0;
Set buffer: hb J 0, Nbt J 0;
repeat
Wait until receiving g from some Workejs;
Choose buffer: b J s mod B ;
Nbt J Nbt + 1;
hb J (Na+;
b
if Nbt > 0 for each b ∈ [B] then
Aggregate: Gt = Aggr([h1, . . . ,hB]);
Execute SGD step: wt+1 J wt — η ∙ Gt;
for b = 1 to B do
Zero out buffer: hb J 0, Nbt J 0;
end for
t J t + 1;
end if
Send back the latest parameters back to worker_s, no matter whether a SGD step is executed or
not.
until stop criterion is satisfied
Notify all workers to stop;
Worker_k: (k = 0,1,…，m — 1)
repeat
Wait until receiving the latest parameter w from server;
Randomly sample an index i from Dk ;
Compute Vf (w; Zi);
Send Vf (w; Zi) to server;
until receive server’s notification to stop
B.2 Proof of Lemma 1
To begin with, we will introduce a lemma to estimate the ordered statistics.
Lemma 3. X1 , . . . , XM are non-negative, independent and identically distributed (i.i.d.) random
variables sampled from distribution D, and have limited expectation E[X]. Denote the K-th largest
value in {Xι,..., XM} as X(K), then E[X(K)] ≤ Cm,k ∙ E[X], where
M,
CM,K = ∖ M!(K-1)k-1(M-K)M-K
(K (K-1)!(M-K)!(M-1)M-1，
K=1;
1 < K < M.
Proof. Denote the Probability Density Function (PDF) and Cumulative Density Function (CDF) of
D as p(x) and P (x), respectively. Then the PDF ofX(K) is:
P(K)(X)= (K — 1)Mf — K)![1 - P(X)]K-1P(X)M-KP(X).
Thus,
E[X(K)] = Z	X
0
• p(K)(x)dx
+∞	M !
/	[(K — 1)!(M — K)! • [1 - P(X)]	P(X)	]
XP(X)dX
14
Under review as a conference paper at ICLR 2021
(a) f+∞
≤ J0
M!
[(K - 1)!(M - K)!
(K - 1)KT(M - K)m-k
(M - I)MT
]∙ xp(x)dx
M!(K -1)KT(M - K)M-K	]
(K - 1)!(M - K)!(M - 1)mT []
Inequality (a) is derived based on [1 - P(x)]KTP(x)M-k ≤ (KT)(M-(MM-K)-, which is
obtained by the following process:
Let θ(x) = (1 — X)K-IXM-k, X ∈ [0,1].
Then θ0(x) = (1 — X)K-2XM-K-1[(M — K)(1 — x) — (K — 1)x].
Let θ0(x) = 0. Solving the equation, we obtain X = MM-K, 0 or 1.
Also, we have θ(0) = θ(1) = 0, and θ(MK) = (KT)(M-(IM-K产-K
Then we have maxxe[Q,1] θ(x) = θ(MM-K) = (KT)(M-(IM-K严 K ∙
Thus, [1 - P(x)]KTP(X)M-k = θ(P(x)) ≤ (KT)(K--(M-K严-K ∙
□
Proposition 3. ∀B, q, r ∈ Z+, 0 ≤ r ≤ q < B,
C	(B - r)√B - r + 1
*…≤ P(B-q-1)(q-F∙
Proof. By Stirling,s approximation, we have:
√2πn ∙ nne-n ≤ n! ≤ e√n ∙ nne-n, ∀n ∈ Z+∙
Therefore,
√2πn ∙ e-n ≤ — ≤ e√n ∙ e-n, ∀n ∈ Z+∙	(2)
—nn -
By definition of CM,k,
_ M!(K - 1)k-1(M - K)m-k
m,k =(K - 1)!(M - K)!(M - I)MT
(M - 1)!	(K - 1)k-1 (M - K)m-k
IVl ∙ ------— • ---------- --------------
(M - 1)mT	(K - 1)!	(M - K)!
,______ ,	、	eK-1	eM-K
≤M ∙ [e√M - 1 ∙ e-(MT)] • /	• /	=
一	√2π(K - 1)	√2π(M - K)
_ e	M√M-1
=--------.	.
2π √(M - K)(K - 1)
where the inequality uses Inequality (2).
Case (i).	When r < q,
e (B — r)√B — r — 1
B-r'q-r+1 ≤ 2π ∙，(B -q- 1)(q 等
V	(B - r)√B - r +1
P(B - q -1)(q - r + 1)
Case (ii).	When r = q, by definition of Cm,k , we have:
CB-r,q-r+1 = CB-q,1 = B - q
(B — r) ʌ/B — r +1
P(B - q - I)(q - r + 1)
In conclusion, when r ≤ q, we have:
CB-r,q-r+1 ≤
(B - r) √B - r + 1
VZ(B - q - I)(q - r + 1)
□
15
Under review as a conference paper at ICLR 2021
When B and q are fixed, the upper bound of CB-r,q-r+1 will increase when r (number of Byzantine
workers) increases. Namely, the upper bound will be larger if there are more Byzantine workers.
When B and r are fixed, q measures the Byzantine Robust degree of aggregation function Aggr(∙).
The factor [(B - q - 1)(q - r)]-2 is monotonically decreasing with respect to q, when q < B-1+r.
Since r ≤ q < B, the upper bound will decrease when q increases. Also, B - q decreases when q
increases. Namely, the upper bound will be smaller if Aggr(∙) has a stronger q-BR property.
In the worst case (q = r), the upper bound of CB-r,q-r+1 is linear to B. Even in the best case (r =
0, q = bB2-1 C), the denominator is about BB and the upper bound of CB-r,q-r+1 is linear to √B.
Thus, larger B might result in larger error. Hence, buffer number is not supposed to be set too large.
Now we prove Lemma 1.
Proof.
E[||Gt||2 | wt]
=E[||Aggr([h1,...,hB])||2| wt]
d
= X E[Aggr([h1, . . . , hB])j2| wt],
j=1
where Aggr([h1, . . . , hB])j represents the j-th coordinate of the aggregated gradient.
We use Ht to denote the credible buffer index set, which is composed by the index of buffers, where
the stored gradients are all from loyal workers.
For each b ∈ Ht, hb has stored Nbt gradients at iteration t: g1, . . . , gNt, and we have:
1 Nbt
hb = Nt X g
Then,
E[khbk2 | wt] =E[khb - E[hb | wt]k2 | wt] + kE[hb | wt]k2
1	Nbt	1 Nbt
=E[kN ∑(gi - E[gi I wt])k2 I wt] + kE[N Xgi I wt]k2
(a)	σ 2	1 Nbt
≤ Nt + kE[Nt Xgi | wt]k2
σ2	1 Nbt
=N + 可 k X E[gi I wt]k2
(b)	σ2	1	Nbt
≤ Nt + (N)2 ∙ Nb ∙ X kE[gi | w ]k2
(c)	σ2	2
≤ Nbt + D.
Inequality (a) is derived based on Assumption 4 and the fact that gi is mutually uncorrelated.
Inequality (b) is derived by the following process:
Nbt	Nbt
kXE[gi Iwt]k2=XkE[gi I
i=1	i=1
wt]k2 +	E 2 ∙ E[gi I WlTE[g0 ∣ Wl
1≤i<i0≤Nbt
Nbt
≤ X kE[gi I Wt]k2 +
i=1
(kE[gi I Wt]k2 + kE[gi0 I Wt]k2
1≤i<i0≤Nbt
16
Under review as a conference paper at ICLR 2021
Nbt	Nbt
=X kE[gi | wt]k2 + (Nbt - 1) ∙ X kE[gi | wt]k2
i=1	i=1
Nbt
=Nt ∙ X kE[gi I wt]k2.
i=1
Inequality (c) is derived based on Assumption 3.
Because there are no more than r Byzantine workers at iteration t, no more than r buffers contain
Byzantine gradient. Thus, the credible buffer index set Ht has at least (B - r) elements. In case
that Ht has more than (B - r) elements, we take the indices of the smallest (B - q) elements in
{hbj}b∈Ht to compose Htj, and we have |Htj | = B - q.
Note that Aggr(∙) is q-BR, and by definition We have:
min {hbj} ≤ Aggr([h1, . . . , hB])j ≤ max{hbj}.
b∈Htj	b∈Htj
Therefore,
dd
X E[Aggr([h1, . . . ,hB])j2|wt] ≤ X E[maxt{hb2j}|wt].
j=1	j=1 b∈Hj
There are (B - r) credible buffers, and We choose the smallest (B - q) buffers to compose Hjt .
Therefore, for all b ∈ Htj, hbj is not larger than the (q - r + 1)-th largest one in {hbj}b∈Ht. Let N(t)
be the (q + 1)-th smallest value in {Nbt}b∈[B]. Using Lemma 3, We have:
E[maxt{hb2j}|wt] ≤E[maxt{khb k2}|wt]
≤e⅛ {D2+N }|wt]
2
=CB-r,q-r+1 ∙ (D2 + Nt) )
Thus,
d	σ2
E|||G"2 1 wt] ≤ X Elmax {h2j }|wt] ≤ CB-r,q-r + 1d ∙ (D2 + Nt) ).
By Proposition 3, We have:
E[∣∣Gt∣∣21 wt] ≤ d∙ /：B -r)vB-r∙ (d2+σ2))).
(B - q - 1)(q -r+ 1)	N(t)
□
B.3 Proof of Lemma 2
Proof.
E[Gt - VF(wt) ∣ wt]
=E[Aggr([h1, ...,hB]) - VF(wt) | wt]
=E[Aggr([h1 - VF (wt),..., hB - VF (wt)]) | wt],	(3)
Where the second equation is derived based on the Property (b) in the definition of q-BR.
For each b ∈ Ht, hb has stored Nbt gradients at iteration t: g1, . . . , gNt, and We have:
1 Nbt	1 Nbt
ht - VF (Wt) = N 与gi - VF(Wt) = Nt 与[Vf (Wtk ； Zik )- VF(Wt)],
17
Under review as a conference paper at ICLR 2021
where 0 ≤ t - tk ≤ τmax, Vk = 1, 2,..., Nt
Taking expectation on both sides, we have:
E[∣∣hb - VF(wt)∣∣ ∣wt]
I	Nb
=E[∣∣N X(Vf(wtk; Zik) - VF(wt))∣∣ ∣wt]
b k = 1
ι Nb
≤N XE[∣∣Vf (wtk; Zik) - VF(wt)∣∣ ∣wt]
b k = 1
(a)	I 池
≤ Nt X{E[∣∣VF(wtk) -VF(wt)∣∣ ∣wt]
b k = 1
+ E[∣∣Vf(Wtk;Zik) - E[Vf(Wtk;Zik)]∣∣ ∣w1
+ E[∣∣E[Vf (wtk; Zik)] -VF(wtk)∣∣ ∣wt]},
where (a) is derived based on Triangle Inequality.
The first part:
E[∣∣VF(wtk) -VF(wt)∣∣ ∣wt]
(b)
≤L ∙ E[∣∣wtk — wt∣∣ ∣wt]
t-1
=L ∙ E[∣∣ X Gt'∣∣ ∣wt]
t0=tk
t-1
≤ X L ∙ E[∣∣Gt01∣ ∣wt]
t0=tk
t-1	_______________
=X L ∙ √E[∣∣Gt°∣∣∣wt]2
t0=tk
t-1	_______________
≤ X L ∙ √E[∣∣Gt°∣∣2 ∣wt]
t0=tk
(C) t—1	______________________________
≤ X L ∙ d<JB-r,q-r+1d ∙ (D2 + σ2∕N㈤)
t0=tk
(d)	I------------------------------
≤ TmaxL ∙ ∖CB-r,q-r + 1d ∙ (D2 + σ2/N(t)),
where (b) is derived based on Assumption 5, (c) is derived based on Lemma 1 and (d) is derived
based on t - tk ≤ Tmax.
The second part:
E川Vf(Wtk; ZiM) — E[V∕(Wtk; Zik)]∣∣ ∣wt]
= √E[∣∣V/(wtk; Zik) - E[V∕(wtk; Zik)]∣∣ ∣wt]2
≤pE[∣∣Vf(wtk; Zik) - E[Vf(wtk; Zik)]∣∣2 ∣wt]
(e)
≤ σ,
where (e) is derived based on Assumption 4.
By Assumption 2, we have the following estimation for the third part:
E[∣∣E[Vf (wtk; Zik)] - VF(wtk)∣∣ ∣wt] ≤ κ.
18
Under review as a conference paper at ICLR 2021
Therefore,
E[||hb - VF(wt)∣∣∣wt]
≤ Nt SXkmaXLqCB-r,q-r + 1d ∙ (D2 + Q2/N (t)) + Q + K)
=TmaXLqCB-r,q-r + 1d ∙ (D2 + Q2∕N(t)) + Q + K.
(4)
Similar to the proof of Lemma 1, ∀j ∈ [d], we have:
理{hj -VF(Wt)j}
≤Aggr([hι - VF (wt),..., hB - VF (wt)]j
≤ max{hbj — VF(WDj},
where Hj is composed by the indices of the smallest (B - q) elements in {hbj - VF(wt)j }b∈χt.
Therefore,
||E[Aggr([hi - VF(wt),..., hB - VF(wt)]) | wt]||
d
≤ X ||E[Aggr([hi -VF(wt),..., hB -VF(Wt)])j | wt]||
j=ι
d
≤XE[||Aggr([hi -VF(wt),...,hB -VF(Wt)])j|| | wt]
j=ι
(f)A
≤ £E[max||hbj -VF(wt)jH | wt]
j=ι be%
(g)	Vd^	+	+
≤ ɪ2 CB-r,q-r+1E[||hbj -VF(w j || |w ]
j=i
d
≤ X CB-r,q-r+1E[||hb -VF(wt)|| |wt]
j=i
(h)	ʌ	/---------------------------------
≤ ∕J CB-r,q-r+1 ∙ (TmaXLq CB-r,q-r + 1d , (DI2 + q2∕N (t)) + Q + K)
j=i
=CB-r,q-r+1d , (τmaxL↑J CB-r,q-r+1d ∙ (D2 + σ2∕N(t) ) + σ + κ),
(5)
where (f) is derived based on definition of q-BR, (g) is derived based on Lemma 3, and (h) is derived
based on Inequality (4).
Combining Equation (3) and Inequality (5), we obtain:
||E[Gt - VF(Wt) | wt]|| ≤ CB-r,q-r+1d , (TmaXLqCB-r,q-r+1d YD2 + QillN(t)) + σ + K).
By Proposition (3), we have:
||E[Gt - VF(wt) | wt]|| ≤ d(B - r)√B - T +1
'(B - q - i)(q - r + 1)
∙(τmɑχL[∕d-Z (B - r)√B- T +1	. (D2 + σ2∕N(t)) + Q + κ).
(max V √(B - q - 1)(q - r + 1) (I)	)
□
19
Under review as a conference paper at ICLR 2021
B.4 Proof OF Theorem 1
Proof.
E[F(wt+1) | wt] =E[F(Wt - η ∙ Gt) | wt]
(a)	L
≤E[F(wt) - η ∙ VF(wt)τGt + ɪη2∣∣Gt∣∣2 | wt]
η ,	η2 L
=F(wt) - η ∙ E[VF(Wt)TGt ∣ wt] + %E[∣∣Gt∣∣2 ∣ wt]
η2 L
=F(wt) - η ∙ VF(wt)τE[Gt ∣ wt] + %E[∣∣Gt∣∣2 ∣ wt]
η2 L
=F(wt) - η ∙ VF(wt)τVF(wt) + 〜E[∣∣Gt∣∣2 ∣ wt]
-η ∙ VF(wt)τE[Gt - VF(wt) ∣ wt]
η2 L
≤F(wt) - η ∙ ∣∣VF(wt)∣∣2 + η2-E[∣∣Gt∣∣2 ∣ wt]
+ η ∙ ∣∣VF(wt)∣∣∙∣∣E[Gt - VF(wt) ∣ wt]∣∣,
where (a) is derived based on Assumption 5.
Using Lemma 1 and Lemma 2, we have:
E [F (wt+1) ∣ wt]
η2 L
≤F(wt) - η ∙ ∣∣VF(wt)∣∣2 + 号附-2一+M ∙ (D2 + σ2/N(t))
+ η ∙ CB-r,q-r+1d ∙ (Tmax-J CB-r,q-r+1d ∙ (D2 + σ2∕N(t) ) + σ + K) ∙ ∣∣VF (Wt)∣∣∙
Also, by Assumption 3, ∣∣VF(wt)∣∣ ≤ D.
Taking total expectation and combining ∣∣VF(wt)∣∣ ≤ D, we have:
η2 L
E[F(wt+1)] ≤E[F(wt)] - η ∙ E[∣∣VF(wt)∣∣2] + 勺金-小-叶1" ∙ (D2 + σ2∕N(t))
+ η ∙ CB-r,q-r+1Dd(Tmɑx-JCBfq-r+M ∙ (D2 + σ2∕N(t)) + σ + κ).
Let D = T PT-1 pD2 + σ2∕N(t). By telescoping, we have:
T-1
η ∙ X E[∣∣VF(wt)∣∣2] ≤{F(w0) - E[F(wT)]}
t=0
-	1 T-1
+	η2T ∙ 2cB-r,q-r + 1d ∙ T X (D+ 拉/N(t))
t=0
+	ηT ∙ CB-r,q-r+1 Dd(Tmax-D，CB-r,q-r+1d + σ + κ)∙
Note that E[F(wT)] ≥ F*, and let η = O (厂+):
PNI E[∣∣VF(wt)∣∣2] ≤o (-[F(w√)- F*]) + O (C⅛-r,"+1Dd!
+	O(CB-r,q-r+1Dd ∙ (Tmax-D PCB-r,q-r+1d + σ + K))
When q = r and B = O(r), we have Cb-γ q-r+1 ≤ /(Bf) VZB-r+1
y	L	B r,q r+1 — √(B-q-1)(q-r+1)
O —. Thus,
∖(q-r+1)2 )
PNIE亭F (Wt)∣∣2ι ≤o (-[F (R- F *])+o (JDN)+o()
T	∖ T 2 J ∖T 2 (q - r + 1) 2 J ∖(q - r + 1)2 J
20
Under review as a conference paper at ICLR 2021
+O
rDdκ
∖(q - r + 1)2
+ O(r 2 LDD) d 2 Tmax
∖ (q - r + 1)4
□
B.5 Proof of Theorem 2
Proof. Let h0b be the value of the b-th buffer, if all received loyal gradients were computed based on
wt. Note Gt = Aggr(h1, . . . , hB).
E[F (wt+1) | wt]
=E[F(Wt - η ∙ Gt) | wt]
(a)	L
≤E[F(wt) - η ∙ VF(Wt)TGt + En2IIGtII2 I wt]
η2 L
=F(wt) - n ∙ E[VF(Wt)TGt I wt] + ɪ-E[IIGtII2 I wt],	(6)
where (a) is derived based on Assumption (5).
Firstly, we estimate the value of E[VF (Wt)T Gt I Wt].
Since there are at most r Byzantine workers, at most r buffers may contain Byzantine gradients.
Without loss of generality, suppose only the first r buffers may contain Byzantine gradients.
Let Gtsyn = Aggr(h1, . . . , hr, h0r+1, . . . , h0B), where h1, . . . , hr may contain Byzantine gradients
and be arbitrary value, and h0r+1 , . . . , h0B each stores loyal gradients computed based on Wt . Thus,
E[VF(Wt)TGtsyn I Wt] ≥ kVF (Wt)k2 - A1,	(7)
E[kGtsynk2 IWt] ≤ (A2)2.	(8)
Let α = 2η2L2τm2 ax(B - r) < 1.
E[kGt - Gsynk2 I wt] ≤ (1 αt+1 + ʌ) ∙ (A2)2,
2	1-α
and
2
E[kGtk2 I wt] ≤ (αt+1 + κ) ∙ (A2)2.
Now we prove it by induction on t.
Step 1. When t = 0, all gradients are computed according to W0, and we have G0 = Gs0yn. Thus,
E[kG0 - G0ynk2 I w0]=0 ≤ (1 ɑ1 + ʌ) ∙ (A2)2,
2	1-α
2
E[kG0k2 I w0] = E[kG0ynk2 I W0] ≤ (A2)2 ≤ (α1 + E) ∙出产
Step 2. If
E[kGt0 - Gsynk2 I wt0] ≤ (1 ɑt0+1 + ʌ) ∙ (A2)2,
syn	2	1 - α
E[kGt0k2 I wt0] ≤ (αt0+1 + ɪ) ∙(A2)2,
1-α
holds for all t0 = 0, 1, . . . , t - 1 (induction hypothesis), then:
E[kGt - Gtsynk2 IWt]
=E[kAggr(h1, . . . , hr, hr+1, . . . ,hB) - Aggr(h1, . . . , hr, h0r+1, . . . ,h0B)k2 I Wt]
(b)	B
≤E[	khb - h0bk2 IWt]
b=r+1
21
Under review as a conference paper at ICLR 2021
B	1 Nbt
=X E[k Nt X(Vf(Wtk ； Zik )-W(wt; Zik ))k2 I Wl
b=r+1	b i=1
(c)	B 1 Nbt
≤ E E[ Nt EkVf(Wtk; Zik )-Vf(wt; Zik )k2 I wt]
b=r+1	b i=1
(d)	B 1 Nbt
≤ X E[Nt XL2kwtk - wtk2 I wt]
b=r+1	b i=1
=L (N- r XE[kwtk - wtk2 I Wt]
Nb	i=1
=1L2N-r XE[k X η∙Gt0k2I wt]
b i=1	t0=tk
≤) η2L2Nt -r X E[(t- tk) X kGt0k2I wt]
b	i=1	t0 =tk
≤)"W" -r X[(t - tk) X (αt0+1 + 占)∙ (A2)2]
b	i=1	t0 =tk	- α
≤η2L2Nt — r) X[(t - tk)X (αt + 1-α) ∙ (A2)2]
b	i=1	t0 =tk	- α
(g)	2
≤ (η2L2(B — r)τmaχ) ∙(αt + 1-α) ∙ (A2)2
(h)1	t 2	2
≤ 2α ∙ (α +ι-α) ∙(A2)
=(1 αt+1 + ʌ) ∙(A2)2,
2	1- α
(9)
where (b) is derived based on the definition of stable aggregation function, (c) is derived based on
Cauchy’s Inequality, (d) is derived based on Assumption 5, (e) is also derived based on Cauchy’s
Inequality, (f) is derived based on induction hypothesis, (g) is derived based on that t - tk ≤ τmax,
and (h) is derived based on that α = 2η2L2τm2 ax (B - r).
Therefore,
E[kGtk2 I Wt] =E[IIGtsyn + (Gt - Gtsyn)II2 IWt]
(i)
≤2 ∙ E[kGSynk2 I wt]+2 ∙ E[∣∣Gt - GSynII2 I wt]
(j)
≤2 ∙(A2)2 +2 ∙ E[IIGt - GSynII2 I wt]
(k)	1	α
≤2 ∙ (A2)2 +2 ∙ (≡-αt+1 + -——)∙ (A2)2
2	1- α
2
=(α + E) ∙ (A2)2，
(10)
where (i) is derived based on that kx + yk2 ≤ 2kxk2 + 2kyk2 , ∀x, y ∈ Rd, (j) is derived by the
definition of (A1 , A2 )-effective aggregation function, and (k) is derived based on Inequality (9).
By Inequality (9) and (10), the claimed property also holds for t0 = t.
In conclusion, for all t = 0, 1, . . . , T - 1, we have:
E[kGt-GSynk2iw1≤ (1ɑt+1 + ι-α) ∙(A2)2,
(11)
22
Under review as a conference paper at ICLR 2021
and
2
E[kGtk2 I wt] ≤ (αt+1 + 1-^)∙ (A?)?.
Also, E[kGtk | wt]2 + V ar[kGtk | wt] = E[kGtk? | wt]. Therefore,
E[kGtk |
wt] = pE[∣∣Gtk∣ wt]2 ≤ α αt+1 + ―2——A2.
1- α
We have:
η ∙ E[VF(Wt)TGtIwt]
=η ∙ E[VF(wt)TGtsyn I wt] + η ∙ E[VF (wt)T (Gt - Gtsyn) I wt]
(l)
≥η∙(kVF(wt)k2-A1)+η∙E[VF(wt)T(Gt-Gtsyn) Iwt]
≥η∙kVF(wt)k2-η∙A1-η∙kVF(wt)k∙kE[(Gt-Gtsyn) I wt]k
(m)
≥ η ∙ IIvf(w)k2 - η ∙ Ai - η ∙ D ∙ kE[(G- Gsyn) | WIk
(n)	1	α
≥η ∙ IlVF(wt)k2 - η ∙ Ai - η ∙ D ∙ N2αt+1 + ɪ-ɑ ∙ A2,
(12)
(13)
(14)
where (l) is derived based on the definition of (Ai, A2)-effective aggregation function, (m) is derived
by Assumption 3, and (n) is derived based on Inequality (11).
Combining Inequalities (6), (12), (14) and taking total expectation, we have:
E[F(wt+i)] ≤E[F(wt)] -η∙E[IVF(wt)I2]
+ η ∙ AI + η ∙ Da/αt+1+ + —— ∙ A2 + -η2L(αt+1 + --------) ∙(A2)2.
2	1- α 2	1- α
By telescoping, we have:
T-i
-	12
η ∙ N E[kVF(wt)k2] ≤{F(w0) - E[F(WT)]} + -η2TL(α +	)∙ (A?)2
+ ηTA1 + ηTD ∙ \ —α + --∙ A2.
2	-α
Divide both sides of the equation by ηT, and let η = O(√=):
PT=01 E[∣∣VF (Wt)Il2]
T
{F(w0) - E[F(wτ)]}	1	2	2	/1 ɑ
≤	ητ	+2 ηL(α+--α) ∙ (A2) + AI+D ∙ V2α+κ ∙ A2
≤ √L[F(w0) - F*]
≤	√t
√L(1 α + ι1α) ∙ (A2)2
+	√T
.	1 r 3 — α ιi
+ A1 + α2[2(1-0)]2 ∙ DA2.
Note that α = 2η2L2τm°χ(B - r) = O (LTma铲-r)), finally We have:
PT=01 E[∣VF (wt)k2] ≤O √LL ∙ [F (w0) - F * ]) + O (√L(Aa1 + α)
+ O (α1 DA2) + Ai
=O L L 2 [F (w0)- F *]! + O L L1 Tmax(B -r)2 DA2
(	T 2	)	(	T 2
23
Under review as a conference paper at ICLR 2021
以Afl + Of L 5(A2)2τmax (B -r)
+ A1.
Specailly, when B = O(r), we have:
PT-o1 E[∣∣VF (wt)k2] ≤O L L 1 [F (w0)- F *]) + Of L 2 TmaxDA2「1
T
L 2(A2)2) + O L L 5 (A2 产Tm αxr
+ A1 .
B.6 Proof of Proposition 2
Proof. Under the condition that ∀wt ∈ Rd, E[kGtsyn - VF(wt)k ≤ D | wt], we have:
E[VF (wt)T Gtsyn | wt]
= E[VF(wt)T [VF(wt) + (Gtsyn -VF(wt)) | wt]
=	kVF (wt)k2 + E[VF (wt)T (Gtsyn - VF (wt)) |wt]
≥	kVF(wt)k2 -kVF(wt)k∙ E[kGSyn - VF(wt)k | wt]
≥	kVF(wt)k2 -D × D
=	kVF(wt)k2 -D2.
Combining with the property (i) of (A1, A2)-effective aggregation function, we have A1 ≤ D2.
24
Under review as a conference paper at ICLR 2021
C	More Experimental Results
Figure 4, Figure 5 and Figure 6 illustrate the average training loss w.r.t. epochs when there are no
Byzantine workers, 3 Byzantine workers and 6 Byzantine workers. Please note that in Figure 5 and
Figure 6, some curves do not appear, because the value of loss function is extremely large or even
exceeds the range of floating-point numbers, due to the Byzantine attack. γ is the hyper-parameter
about the assumed number of Byzantine workers in Kardam. The experimental results about training
loss give further support to the experimental summary in Section 5.
ssoL gniniarT
10
ssoL gniniarT



Figure 4:	Average training loss w.r.t. epochs when there are no Byzantine workers. The aggregation
function in BASGD is set to be median (left) and trimmed-mean (right), respectively.
ssoL gniniarT
-B-ASGD
—x—BASGD with median (B=10)
BASGD with 3-trimmed mean (B=10)
-A-Kardam (γ=3)
-⅛-Kardam (7=6)
ssoL gniniarT


Figure 5:	Average training loss w.r.t. epochs in face of random disturbance attack (left) and negative
gradient attack (right), when the number of Byzantine workers r = 3. Some curves do not appear
in the figure, because the value of loss function is extremely large or even exceeds the range of
floating-point numbers.
ssoL gniniarT
-B- ASGD
—x—BASGD with median (B=15)
BASGD with 6-trimmed mean (B=15)
-A-Kardam ⅛=6)
-⅛-Kardam (7=1。)______________



Figure 6:	Average training loss w.r.t. epochs in face of random disturbance attack (left) and negative
gradient attack (right), when the number of Byzantine workers r = 6. Some curves do not appear
in the figure, because the value of loss function is extremely large or even exceeds the range of
floating-point numbers.
25