Under review as a conference paper at ICLR 2021
Mixup Training as the Complexity Reduction
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning models often suffer from the problem of over-fitting. Many
data augmentation methods have been proposed to tackle such a problem, and one
of them is called mixup. Mixup is a recently proposed regularization procedure,
which linearly interpolates a random pair of training examples. This regulariza-
tion method works very well experimentally, but its theoretical guarantee is not
adequately discussed. In this study, we aim to discover why mixup works well
from the aspect of the statistical learning theory. In addition, we reveal how the
effect of mixup changes in each situation. Furthermore, we also investigated the
effects of changes in the parameter of mixup. Our work contributes to searching
for the optimal parameters and estimating the effects of the parameters currently
used. The results of this study provide a theoretical clarification of when and how
effective regularization by mixup is.
1	Introduction
Machine learning has achieved remarkable results in recent years due to the increase in the number of
data and the development of computational resources (Michie et al., 1994; Bishop, 2006; Goldberg,
2017; Deng et al., 2009; Everingham et al., 2010). However, despite such excellent performance,
machine learning models often suffer from the problem of over-fitting (Hawkins, 2004; Lawrence
& Giles, 2000; Dietterich, 1995). In recent years, a concept called mixup (Zhang et al., 2018) or
BC-Learning (Tokozume et al., 2018b) has attracted attention as one of the powerful regularization
methods for machine learning models. The main idea of these regularization methods is to prepare
(Xij,yij) = (λxi + (1 - λ)xj,λyi + (1 - λ)yj) mixed with random pairs (xi, xj) of input vectors
and their corresponding labels (yi,yj) and use them as training data. This regularization method is
very powerful and has been applied in various fields such as image recognition (Tokozume et al.,
2018a; Inoue, 2018) or speech recognition (Medennikov et al., 2018; Xu et al., 2018). Despite these
strong experimental results, there is not enough discussion about why this method works well.
In this paper, we give theoretical guarantees for regularization by mixup and reveal how regulariza-
tion changes in each setting. Our main idea is that there must be some different quantities before and
after the regularization. We focus on the Rademacher complexity and the smoothness of the convex
function characterizing the Bregman divergence, which are measures of model richness, as such a
quantity. In other words, the model’s complexity should change with the mixup regularization, and
by observing how these changes, we can theoretically clarify the effects of mixup. Furthermore, we
also investigated the effects of changes in mixup’s parameter λ . This contributes to searching for
the optimal parameters and estimating the effects of the parameters currently used.
To summarize our results, mixup regularization leads to the following effects:
•	For linear classifiers, the effect of regularization is higher when the sample size is small,
and the sample standard deviation is large.
•	For neural networks, the effect of regularization is higher when the number of samples is
small, and the training dataset contains outliers.
•	When the parameter λ is close to 0 or 1, mixup can reduce the variance of the estimator,
but this will be affected by bias.
•	When the parameter λ has near the optimal value, mixup can reduce both the bias and
variance of the estimator.
1
Under review as a conference paper at ICLR 2021
•	Geometrically, mixup reduces the second-order derivative of the convex function that char-
acterizes the Bregman divergence.
2	Related Works
2.1	Mixup Variants
Mixup is originaly proposed by (Xu et al., 2018). The main idea of these regularization methods is
to prepare (X,y订)=(λxi +(1 - λ)xj, λyi + (1 - λ)yj) mixed with random pairs (Xi, xj) of input
vectors and their corresponding labels (yi,yj) and use them as training data, where λ 〜Beta(α, α),
for α ∈ (0, ∞).
Because of its power and ease of implementation, several variants have been studied. Verma et al.
(2019) proposed the Manifold mixup, which is a method to mix up the output of an intermediate
layer of neural networks (including the input layer) instead of the input data. Berthelot et al. (2019)
proposed MixMatch, a heuristic method that combines the ideas of mixup and semi-supervised
learning. Puzzle Mix (Kim et al., 2020) leverages the saliency information while respecting the
underlying local statistics of the data, and Nonlinear Mixup Guo (2020) relaxes the constraint of
convex combination in mixup. There are several other variants, but many are heuristic methods and
have insufficient theoretical explanations (Yun et al., 2019; Lim et al., 2019; Sohn et al., 2020).
On the other hand, there are several theoretical analyses of the effects of mixup. Archambault et al.
(2019) suggested that mixup training is connected to adversarial training.
Carratino et al. (2020) have further shown that mixup amounts to empirical risk minimization on
modified points plus multiple regularization terms through a Taylor approximation.
3	Notations and Preliminaries
We consider a binary classification problem in this paper. However, our analysis can easily be
applied to a multi-class case.
Let X be the input space, Y = {-1, +1} be the output space, and C be a set of concepts we may
wish to learn, called concept class. We assume that each input vector x ∈ Rd is of dimension d. We
also assume that examples are independently and identically distributed (i.i.d) according to some
fixed but unknown distribution D.
Then, the learning problem is formulated as follows: we consider a fixed set of possible concepts
H, called hypothesis set. We receive a sample B = (x1, . . . , xn) drawn i.i.d. according to D as well
as the labels (c(x1), . . . ,c(xn)), which are based on a specific target concept c ∈ C : X 7→ Y . Our
task is to use the labeled sample B to find a hypothesis hB ∈ H that has a small generalization error
with respect to the concept c. The generalization error R(h) is defined as follows.
Definition 1. (Generalization error) Given a hypothesis h ∈ H, a target concept c ∈ C, and unknown
distribution D, the generalization error of h is defined by
R(h) = E%〜D [ɪh(X)=C(x)] ,	(I)
where lω is the indicator function of the event ω.
The generalization error of a hypothesis h is not directly accessible since both the underlying dis-
tribution D and the target concept c are unknown Then, we have to measure the empirical error of
hypothesis h on the observable labeled sample B. The empirical error R(h) is defined as follows.
Definition 2. (Empirical error) Given a hypothesis h ∈ H, a target concept c ∈ C , and a sample
B = (x1, . . . , xn), the empirical error ofh is defined by
1n
R(h) = ∑ Σ ɪh(xi)= C(xi).	⑵
n i=1
In learning problems, we are interested in how much difference there is between empirical and
generalization errors. Therefore, in general, We consider the relative generalization error R(h)-
2
Under review as a conference paper at ICLR 2021
R(h). The Rademacher complexity and the learning bound using it can be used to provide useful
information about the relative generalization error.
Definition 3. (Empirical Rademacher complexity) Given a hypothesis set H and a sample B =
(x1, . . . ,xn), the empirical Rademacher complexity ofH is defined as:
RB ( H )= Eσ[
1
sup一
h∈H n
∑ σih(xi)i,
(3)
where σ = (σ1, . . . , σn)T with Rademacher variables σi ∈ {-1, +1} which are independent uniform
random variables.
Definition 4. (Rademacher complexity) Let D denote the distribution according to which samples
are drawn. For any sample size n ≥ 1, the Rademacher complexity of H is the expectation of the
empirical Rademacher complexity over all samples of size n drawn according to D:
Rn(H) = EB〜Dn [rB(H)i .
(4)
Intuitively, this discribes the richeness of hypothesis class H.
The Rademacher complexity is a very useful tool for investigating hypothesis class H. By the
following theorem, we can quantify the relative generalization error.
Theorem 1. Given a hypothesis h ∈ H and the distribution D over the input space X , we assume
that RB (H) is the empirical Rademacher complexity of the hypothesis class H. Then, for any δ > 0,
with probability at least 1 - δ over a sample B of size n drawn according to D, each of the following
holds over H uniformly:
R( h) - R (h)
R(h) - R(h)
≤	R n ( H) +
≤	5R B (H) + 3
(5)
(6)
For a proof of this theorem, see Appendix A. This theorem provides a generalization bound based
on the Rademacher complexity. We can see that this bound is data-dependent due to the fact that
empirical Rademacher complexity RB (H) is a function of the specific sample B.
From the above discussion, we can see that if we can quantify the change of empirical Rademacher
complexity before and after mixup, we can evaluate the relative generalization error of the hypothesis
class H. Our main idea is to clarify the effects of the mixup regularization by examining how these
Rademacher complexity changes before and after regularization. Note that we are not interested in
the tightness of the bound, but only in the difference in the bound.
4 Complexity Reduction of Linear Classifiers with Mixup
In this section, we assume that h` is a class of linear functions:
h(x) ∈ h` = {x → WTx I W ∈ Rd, kw∣∣2 ≤ λ},
(7)
where w is the weight vector and Λ is a constant that regularizes the L2 norm of the weight vector.
The following theorem provides a relaxation of the Rademacher complexity of the linear classifier
by mixup.
Theorem 2. Given a hypothesis set h` and a sample B = (xι,…，Xn), We assume that RB(h`)
is the empirical Rademacher complexity of the hypothesis class h` and RB(H`) is the empirical
Rademacher complexity of h` when mixup is applied. The difference between the two Rademacher
complexity RB(h`) - RB(h`) is less than or equal to a constant multiple of the sample variance of
the norm of the input vectors:
Cλ l------
5RB(h`)-RB(h`) ≤√nΨ2kχk2,
(8)
3
Under review as a conference paper at ICLR 2021
0.00
10
30
50
70
130
150
170
190
90	110
n
Figure 1: The relationship between RB(H`) - RB(h`) and the number of samples n and variance
σ2 when mixup is applied. Each data point was sampled from the normal distribution N (0, σ2)
and the constant part was set to 1. It can be seen that as the number of samples n increases, the
effect of complexity mitigation by mixup decreases. We also find that the greater the variance in the
distribution of the data, the higher the effect of mixup.
where CλΛ is a constant that depends on the parameter λ of mixup and s2 is the sample variance
computed from the sample set.
As can be seen from the equation 8, the complexity relaxation by mixup decreases as the number of
samples n increases (this can be seen by taking the right-hand side of the theorem to the limit for n,
see Figre 1).
Proof. Let Xi = Exy [λxi + (1 - λ )xj] be the expectation of the linear combination of input vectors
by mixup, where λ is a parameter in mixup and is responsible for adjusting the weights of the two
vectors. Then, we have
RB(He)- fRB(h`)	≤
λ (∑ 忸 k2)- λ (i∑E巧 hλ χi+(I- λ)Xji
Λ|√-λ1 q s 2(kχk2)≥ 0,
n
(9)
(10)
Here, let CΛ = Λ∣1 - λ | and We can obtain equation 8.
□
For a complete proof, see Appendix B.
The above results are in line with our intuition and illustrate well how mixup depends on the shape
of the data distribution. In the next section, we discuss neural networks as a more general application
destination for the mixup.
5 Complexity Reduction of neural networks with mixup
Let HL,WL be the function class of a neural network:
h(X) ∈ HL,WL = nh : kvk2 = 1,∏kWikF ≤ WLo,
i=1
(11)
where L is the number of layers, Wi is the weight matrix, v ∈ RML represents the normalized lin-
ear classifier operating on the output of the neural networks with input vector X and kAkF is the
4
Under review as a conference paper at ICLR 2021
Figure 2: The relationship between RB (HL,wl ) - RB (HL,wl ) and the number of samples n and the
noise of the outliers . When there are extreme outliers in the sample, we can see that mixup allows
the neural network to make robust estimation. In addition, we can see that the effect of regularization
decreases as the sample size n increases.
Frobenius norm of the matrix A = (aij).
kAkF
F
The following theorem provides relaxation of the Rademacher complexity of the neural network by
mixup.
Theorem 3.	Given a hypothesis set HL,WL and a sample B = (x1, . . . , xn), we assume that
RB (HLWl ) is the empirical Rademacher complexity of the hypothesis class HL,wl and RB (HL,Wl )
is the empirical Rademacher complexity of HL,WL when mixup is applied. In addition, we assume
that each sample Xi occurs with the population mean μχ plus the some noise G. In other words, We
assume that Xi = μχ + G. The difference between the maximum of two Rademacher complexity
RB (HLWL) — RB (HL,Wl ) is less than or equal to a constant multiple of the maximum value of noise
in a sample of training data when the number of samples n is sufficiently large:
CL
max RB (HL,wL) — max RB (HL ,wL) ≤ √n max ke i k,	(12)
where CλL is a constant that depends on the parameter λ of mixup and the number of layers L of
neural networks.
This theorem shows that mixup regularization for neural networks is more effective when there are
outliers in the sample.
Proof. The upper bound of the Rademacher complexity of the neural network with ReLU as the
activation function and regularization by the constant WL for the norm of each weight is bounded
as follows (Neyshabur et al., 2015):
RB(HL,wl) ≤ √n2l + 2 Wlmax∣∣xi∣∣.
Rademacher complexity of HL,WL with mixup is
(13)
^RB( Hl ,wL)	≤
≤
* 2 l + 2 Wl max ∣∣E j [λ X i + (1 — λ )x 川 ∣
√n 2 L+ 2 Wl m ax{λ |同 + (1 - λ )∣∣Ej[xj]k}
(14)
5
Under review as a conference paper at ICLR 2021
Figure 3: Beta distribution Bet a(α, α) for
each α. Here, the probability density func-
tion of the Beta distribution is f(x; α,β) =
B，R、xα-1(1 - x)β-1, where B(α, β) is the
B(α,β )
beta function. From this figure, it can be seen
that when α = 1, it is equivalent to a uniform
distribution, and when α > 1, it becomes bell-
shaped. we can also see that when α < 1, sam-
pled λ is close to 0 or 1.
Then,
max RB (HL, WL) - max RB (HL, WL)	≤
≤
2L+1L + 1 Wl max
ni
nkxik-(λkxik+(1-λ)kEj[xj]ko
1√-λ2l +1 Wlmax {∣∣μχ∣∣ + 怕IlTlxI∣}	(15)
1λ 1
+ 2 Wl max 怕 ∣.	(16)
ni
The inequality in equation 15 is guaranteed by the subadditivity nature of the norm, and the equality
in equation 16 is guaranteed by the law of large numbers. Here, let CL = (1 — λ)2L + 2 Wl and We
can obtain equation 12.	□
For a complete proof, see Appendix B.
While neural networks have wealthy representational power due to their ability to approximate com-
plicated functions, they are prone to over-fitting into training samples. In other words, it approxi-
mates a function that fits well for unusual examples that occur accidentally in the training sample b.
According to the above theorem, mixup allows the neural networks robust learning for outliers with
accidentally large noise in the training sample B.
6 The Optimal Parameters of Mixup
In this section, we consider the optimal parameter of mixup. Here, we let the parameter λ ∈ (0, 1).
From equation 10 and equation 16, we can see that a large 1 - λ has a good regularization effect.
In other words, if the weight of one input vector is more extreme than the other, the mixup effect is
more significant. By swapping i and j, we can see that λ should be close to 0 or 1.
In the original mixup paper (Zhang et al., 2018), the parameter λ is sampled from the Beta distribu-
tionBeta(α, α), where α is another parameter. Figure 3 shows some shapes of the Beta distribution
changing α. From this figure, we can see that when α < 1, λ is sampled such that one of the in-
put vectors has a high weight (in other words, λ is close to 0 or 1). We treated λ as a constant
in the above discussion, but if we treat it as a random variable λ 〜Beta(α, α), we can obtain the
following:
E[λ]
Var(λ)
α1
----=--
α+α 2
α2	α2	_	1
(α + α )2(α + α +1)	4α 2(2α +1)	4(2α +1)
where α > 0. Since the E[λ] is a constant, we can see that when the weight parameter λ is close to
0 or 1, α is expected to be close to 0.
Figure 4 shows the experimental results for CIFAR-10 (Krizhevsky et al., 2009). We use ResNet-
18 (He et al., 2016) as a classifier and apply mixup with each parameter α for λ 〜Beta(α, α). This
shows that the generalization performance is higher when the parameter α is a small value. The right
side of Figure 4 shows a plot of the training loss and test loss of the classifier and their differences
for each α. We can see that when the value of parameter α is small, the difference between train loss
6
Under review as a conference paper at ICLR 2021
Sso- UO4BP=e>
Figure 4: Experimental results for CIFAR-10 dataset.We use ReSNet-18 as a classifier and apply
mixup With each parameter a for λ 〜Beta(α, α). Left: Learning curve of ResNet-18 With mixup.
The generalization performance is higher when the parameter a is small value. Right: Plot of train
loss, test loss, and their differences for each α.
and test loss is small. Appendix D shows the details of the experiments and additional experimental
results.
7 Geometric Perspective of Mixup Training： Parameter Space
Smoothing
In this section, we consider the effect of mixup geometrically on the space of the parameters to be
searched. The following theorem suggests that mixup,s regularization contributes to the smoothing
of convex functions corresponding to the parameter space.
Theorem 4.	Let p(x; θ) be the exponential distribution family that depends on the unknown pa-
rameter vector θ. When mixup is applied, the second-order derivative VVψλ (θ) of ψλ (θ) that char-
acterizes the Bregman divergence between the parameter θ and θ + dθ, which is a slight change of
the parameter, satisfies the following:
VVψλ (θ) = λ 2(VVψ(θ)),	(17)
where ψ(θ) is a convex function of the original data distribution and λ ∈ (0, 1) is a parameter of the
mixup.
in optimization, the smaller the change in the gradient of the convex function, the more likely it is
to avoid falling into a local solution. This means that mixup reduces the complexity in the context
of the parameter search.
Proof. An exponential family of probability distributions is written as
p(x; θ) = exp
∑θixi+k(x)- ψ(θ) ,
(18)
where p(x; θ) is the probability density function of random variable vector x specified by parameter
vector θ and k(x) is a function of x. Also, ψ(θ) can be written as
ψ(θ)=logZexp ∑θixi+k(x) dx.
(19)
By differentiating equation 19, we can confirm that the Hessian becomes a positive definite matrix,
which means that ψ (θ ) is a convex function. Here, the Bregman divergence from ξ to ξ0 is defined
by using the convex function φ(ξ):
D夕［ξ ： ξI =暇ξ) — 暇ξ0) - Vφ(ξ0) ∙ (ξ - ξ0)
(20)
7
Under review as a conference paper at ICLR 2021
Figure 5: Bregman divergence from θ0 to θ. This divergence derived from the convex function ψ(θ)
and its supporting hyperplane with normal vector Vψ(θ0).
Let ψ(∙) = φ(∙) and θ = ξ, then We can naturally define a Bregman divergence for ψ(∙) and θ.
Differentiating equation 18, we can obtain
∂θ / ∑θ θixi + k(X)- Ψ⑹
E[x].
(21)
Differentiating again,
0
.∙. VVψ(θ)
-∂θ∂θ.Ψ(θ) + /(Xi- E[xi])(xj - E[xj])P(x; θ)dX
Var(X).
(22)
Here, if we adopt the linear combination X = λX + (1 - λ)xj to find the parameter θ, we can obtain
V ψλ (θ) = E[X ] = E[λ X + (1 — λ )E[x]] = E[x],
VVψλ (θ) = Var(λX+(1-λ)E[X]) =λ2ψ(θ),
(23)
(24)
where ψλ (∙) is defined by
p (X; θ) = exp
{∑ θixi + k(X) - ψλ (θ)}.
(25)
0
」. Vψ(x)
卜X
From Bayes theorem, we would be computing the probability of a parameter given the likelihood
of some data: P(X; θ) = P(X; θ)P(θ)/∑θP(X; θ0)P(θ0), and applying mixup means P(x; θ) →
p(X; θ). And then, we can obtain equation 17.	□
For a complete proof, see Appendix C.
Bregman divergence is a generalization of KL-divergence, which is frequently used in probability
distribution spaces, such as loss functions for parameter search. The above theorem means that the
magnitude of the gradient of the convex function characterizing the Bregman divergence can be
smoothed by using the mixup.
8 Conclusion and Discussion
In this paper, we provided a theoretical analysis of mixup regularization for linear classifiers and
neural networks with ReLU activation functions. Our results show that a theoretical clarification of
when and how effective regularization by mixup is.
Our future work includes considering whether similar arguments can be made for some variants of
mixup (Verma et al., 2019; Berthelot et al., 2019; Yun et al., 2019; Lim et al., 2019; Sohn et al.,
8
Under review as a conference paper at ICLR 2021
2020). Because of the simplicity of the idea and ease of implementation, there are many variants of
mixup, but most of them are heuristic approaches.
Also, Tokozume et al. (Tokozume et al., 2018b) suggest that BC-Leaning, a concept roughly equiv-
alent to the mixup, behaves in a way that increases the Fisher’s criterion (Fisher, 1936). This claim
is impressive, and they provide experimental support for this hypothesis, but the theoretical argu-
ments are insufficient. It is worth considering to show theoretically that data augmentation by mixup
contributes to the increase of Fisher’s criterion, and to clarify how much this changes the value.
Another possible future study is a theoretical consideration of mixing data on manifolds (Verma
et al., 2019). Taking data as a point in the manifold, it would lead to more advanced research to
investigate how mixup training behaves on the manifold.
We believe it would be useful to divert the discussion we have had in this paper to clarify whether
such modifications improve mixup and, if so, to what extent.
References
Guillaume P Archambault, Yongyi Mao, Hongyu Guo, and Richong Zhang. Mixup as directional
adversarial training. arXiv preprint arXiv:1906.06875, 2019.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems,pp. 5050-5060, 2019.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
LUigi Carratino, Moustapha Cisse, RodolPhe Jenatton, and Jean-PhiliPPe Vert. On mixup regular-
ization. arXiv preprint arXiv:2006.06049, 2020.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Tom Dietterich. Overfitting and undercomputing in machine learning. ACM computing surveys
(CSUR), 27(3):326-327, 1995.
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):
303-338, 2010.
Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7
(2):179-188, 1936.
Yoav Goldberg. Neural network methods for natural language processing. Synthesis Lectures on
Human Language Technologies, 10(1):1-309, 2017.
Hongyu Guo. Nonlinear mixup: Out-of-manifold data augmentation for text classification. In The
Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-
tive Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020, pp. 4044-4051. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/
AAAI/article/view/5822.
Douglas M Hawkins. The problem of overfitting. Journal of chemical information and computer
sciences, 44(1):1-12, 2004.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
9
Under review as a conference paper at ICLR 2021
Hiroshi Inoue. Data augmentation by pairing samples for images classification. arXiv preprint
arXiv:1801.02929, 2018.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning (ICML), 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Steve Lawrence and C Lee Giles. Overfitting and neural networks: conjugate gradient and back-
propagation. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural
Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Mil-
Iennium, volume 1, pp. 114-119. IEEE, 2000.
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In
Advances in Neural Information Processing Systems, pp. 6662-6672, 2019.
Ivan Medennikov, Yuri Y Khokhlov, Aleksei Romanenko, Dmitry Popov, Natalia A Tomashenko,
Ivan Sorokin, and Alexander Zatvornitskiy. An investigation of mixup training strategies for
acoustic models in asr. In Interspeech, pp. 2903-2907, 2018.
Donald Michie, David J Spiegelhalter, CC Taylor, et al. Machine learning. Neural and Statistical
Classification, 13(1994):1-298, 1994.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classifi-
cation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
5486-5494, 2018a.
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Learning from between-class examples for
deep sound recognition. In International Conference on Learning Representations, 2018b. URL
https://openreview.net/forum?id=B1Gi6LeRZ.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden
states. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th In-
ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learn-
ing Research, pp. 6438-6447, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/verma19a.html.
Kele Xu, Dawei Feng, Haibo Mi, Boqing Zhu, Dezhi Wang, Lilun Zhang, Hengxing Cai, and
Shuwen Liu. Mixup-based acoustic scene classification using multi-channel convolutional neural
network. In Pacific Rim Conference on Multimedia, pp. 14-23. Springer, 2018.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-
ings of the IEEE International Conference on Computer Vision, pp. 6023-6032, 2019.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-
pirical risk minimization. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1Ddp1-Rb.
10
Under review as a conference paper at ICLR 2021
A Rademacher Complexities Bounds
A.1 Theorem 1
Lemma 1. Let G : Z = X × Y 7→ [0, 1] be a family of functions. Then, for any δ > 0, with
probability at least 1 - δ , the following holds for all g ∈ G :
E g(z) ≤
E g(z) ≤
n ∑g (zɔ+2R” (G)+lom#]
n i=1	m
1n
∑ g (Zi)+ 2RB ( G) + 3
n i=1
2m
(26)
(27)
Proof. (Lemma 1) For any sample B = (zι,...,Zn) and for any g ∈ G, we denote by EB[g] the
empirical average of g over B : EB[g] = 1 ∑n=1 g(Zi). We define the function Φ(∙) for any sample B
as follows:
Φ(B) = SUP E[g] - EB[g].	(28)
g∈G
Let B and B0 be two samples differing by exactly one point, which mean Zn ∈ B ∧ Zn ∈/ B0 and
Zn0 ∈ B0 ∧ Zn0 ∈/ B. Then, we have
Φ(B0)-Φ(B)	≤	sup EB [g] - EB0 [g]二 g∈G	= sup 业-心 ≤ 1 g∈G	n	n	(29)
Φ(B)-Φ(B0)	≤	sup EB0 [g] - EB [g]二	= sup g(zn)-g(Zn) ≤ 1 g∈G	n	n	(30)
Φ(B) -Φ(B0)	≤	g∈G 1 . n		(31)
Then, by McDiarmid,s inequality, for any δ > 0, with probability at least 1 - 2, the following holds:
Φ(B) ≤	EB[Φ(B)]+
1n
EB [φ(B)] ≤ Eσ,B,Bo SUP- ∑ σi(g(zi) - g(Zi))
g∈G n i=1
1n
2Eσ, B sup- ∑ σig(Zi) = 2R n (G).
g∈G ni=1
Then, using MacDiarmid,s inequality, with probability 1 - 2 the following holds:
Rn (G) ≤ RB (G) +
(32)
(33)
(34)
(35)
Finally, we use the union bound and we can have the result of this lemma.
□
Lemma 2. Let H be a family of functions taking values in {-1, +1} and let G be the fam-
ily of loss functions associated to H: G = {(ɪ,y) → Ih(X)= γ : h ∈ H}. For any samples B =
((x1,y1), . . . , (xn,yn)), let SX denote the its projection over X : SX = (x1, . . . , xn). Then, the
following relation holds between the empirical Rademacher complexities of G and H :
R B (G ) = 1R SX (H).
(36)
11
Under review as a conference paper at ICLR 2021
Proof. (Lemma 2) For any sample B = ((x1,y1), . . . , (x2,y2)) of elements in X × Y , the empirical
Rademacher complexity of G can be written as:
RB (G )
1
2
1
2
1n
sup- ∑σ⅛(Xi)=yi
h∈H n
h∈H n i=1
1n
(37)
(38)
(39)
Proof. (Theorem 1) From Lemma 1 and Lemma 2, we can have the result of Theorem 1 immedi-
ately.
For more details, see textbooks on statistical learning theory (e.g., Shalev-Shwartz & Ben-David
(2014); Mohri et al. (2018)).
B Proof of the Complexity Reduction
In this section, we show the proofs of the theorems of the Rademacher complexity reduction. First
we prove the theorem on linear discriminators, then we prove the theorem on neural networks.
B.1	Theorem 2
Proof. By the Definition 3, empirical Rademacher complexity of h(x) = wTx is as follows:
1n
-	sup ∑ σiWT x i
n kwk2≤Λ i=1
1n
-	sup wT ∑ σiXi
n kwk2≤Λ	i=1
1
n
n
1
n
Λ
n
sup
kwk2≤Λ
i=1
Λ∑ σixi
Cauchy-Schwarz's inequality)
i=1
∑ σi xi
i=1
)	(√ Jensen,s inequality)
n
n
2
2
(40)
Let Xi = Exj [λxi + (1 - λ)xj] be the expectation of the linear combination of input vectors by
mixup, where λ is a parameter in mixup and is responsible for adjusting the weights of the two
12
Under review as a conference paper at ICLR 2021
vectors. Then, we have
R B (H) = λ ( ∑∑ 1国，2
=Λ (∑ E JλXi + (1 - λ)xji
Λ
n
)一 (;Linearity of expectation)
≤ Λ恪(Mxik2+∣∣(1-λ)Eχj[xj]∣∣2);
=Λ 02 ∑ kxik2 + (1 — λ)2 ∑∣Eχj[xj]
From eqUation 40 and eqUation 41, we can have
1
2
(∙.∙ SUbadditivity of norm)
2! 2
RB (H) - RB (H) ≤
Λ n	n	n
-(∑ kx i k2-λ 2 ∑ kx i k2-(ι—λ )2 ∑∣∣Eχ j [x j ]
n	i=1	i=1	i=1
2! 2
(1-λ)2∑kxik22-(1-λ)2∑	E
i=1
i=1
xj
Wl ∑kxik2 — ∑L∣Eχj[xj]
n	i=1	i=1
2! 2
1
λ11-^1 ∑ kχik2-
n i=1
Λ∣1 — λ |
√n
Λ∣1 — λ |
√n
S 2(kxk2) + kx k2-kx I∣2
1 ∑ kxl∣2 J (； i∙i∙d∙)
n i=1
1
∖ 2
≠2(kxk2) ≥ O.
(41)
(42)
(43)
(44)
(45)
□
≤
Λ
n
n
n
2! 2
B.2 Theorem 3
Proof. By the Upper boUnd of (NeyshabUr et al., 2015), empirical Rademacher complexity of h(x) ∈
HL,WL is as follows:
RRB(HL,WL) ≤ √√n2L+1 Wlmaxkxik.
(46)
Let xi = Exj [λxi + (1 一 λ)xj] be the expectation of the linear combination of input vectors by
mixUp, where λ is a parameter in mixUp and is responsible for adjUsting the weights of the two
vectors. Then, we have
R B (HL WL)	≤
≤
√n 2l +1 Wl max kEj[λ X i + (1 — λ )xj ]k
√n 2l + 2 Wl max kλ X i + (1 — λ )Ej[Xj ]k
√^ 2l + 2 Wl max {λ kx i k + (1 一 λ )kEj[Xj]k}. (; Subadditivity of norm)
(47)
13
Under review as a conference paper at ICLR 2021
Now we consider to bound the difference between the maximum values of each quantity,
max
max
{R b (HL ,wL)0 = √n 2l+1 Wl max Ilx i k,
{RB (HL ,WL)0 = √1n 2l+1 Wl m ax {λ Ilx ik + (1- λ )|叫[叼]∣∣},
and then, from equation 46 and equation 47, we can have
max Rb (HL ,wL)-max 5RB(HL ,Wl ) = √-n 2L + 2 WL { max kx i k - max {λ ∣∣x i k + (1- λ )∣Ej[xj ]k}}
≤√n 2 L+ 2 WL m ax∣kxik2 - (λ kxik2 + (1- λ )kEj[x>∣
=√n2L + 2 Wlmax∣(1 -λ)kxi∣∣2 -(1-λ)kxk2∣
=1-12L + 1 Wlmax∣kxik2 - kxl∣2∣
ni
=1√-λ 2 l +1 Wl max∣kμχ + G∣∣2 -kx k2∣
≤ 1-^ 2L +1 Wl max∣kμχ k2 + k€ i k2 - kx k2∣ (v SUbadditivity of norm)
ni
1-λ 1
=—^2l + 2 Wlmax ∣∣Gk2 (VLaW of large numbers)	(48)
ni
≥ 0 (∙∙T- λ ≥ 0,∣∣q∣∣2 ≥ 0),
here equation 48 is supported by the law of large numbers.
lim P fl X - μ k > ε} = 0 (∀ε > 0).
n→∞
(49)
□
C	Effect of Mixup on the Convex Function Characterizing the
B regman Divergence
In this section, We shoW the proof of the theorem of the Effect of mixup on the convex function
characterinzing the Bregman divergence.
C.1 Definitions
Definition 5. (Bregman divergence) For some convex function φ(∙) and d-dimensional parameter
vector ξ ∈ Rd , the Bregman divergence from ξ to ξ0 is defined as folloWs:
D φ [ξ: ξ0] = φ (ξ) - φ (ξ0) - Vφ(ξ0)∙ (ξ-ξ0).	(50)
C.2 Theorem 4
Proof. An exponential family of probability distributions is Written as
p(x; θ) = exp ∑θixi+k(x) -ψ(θ) ,
(51)
Where p(x; θ) is the probability density function of random variable vector x specified by parameter
vector θ and k(x) is a function ofx. Since R p(x; θ) = 1, the normalization term ψ(θ) can be Written
as:
ψ(θ) = logZ exp ∑θi xi + k(x) dx
(52)
14
Under review as a conference paper at ICLR 2021
which is known as the cumulant generating function in statistics. By differentiating equation 52, we
can confirm that the Hessian becomes a positive definite matrix, which means that ψ(θ) is a convex
function. Here, the Bregman divergence from ξ to ξ0 is defined by using the convex function φ(ξ):
D φ [ξ : ξ 0] = φ (ξ) - φ(ξ0) - Vφ(ξ0 )∙ (ξ - ξ0)	(53)
Let ψ(∙) = φ(∙) and θ = ξ, then We can naturally define the Bregman divergence for ψ(∙) and θ.
Differentiating equation 51, we can obtain
∙∙∙版⑹
Vψ (x)
Differentiating it again,
.∙. VVψ (θ)
∂θ Zexp{∑ θiXi + k(x) - ψ(θ)卜X
Z xi^i - ∂θ Ψ(θ)∣P(x； θ)dX
ZXiP(x;θ)dχ - ∂θψ(θ)
xi p(X； θ)dX = E[xi]
E[X].
Z ∂θj n XL 焉ψ (θ)o P(X； θ)+{ XL 焉ψ (θ)o ∂⅜ P(X； θ) dX
Z - 薪 ψ(θ) dX+zn xi-京i ψ(θ)}{ Xj- ∂θ 叭@} P(x； θ) dX
∂2
∂θi∂θj
ψ(θ) + 卜Xi- E[Xi])(Xj - E[Xj])P(x；θ)dX
-褊徜ψ(θ)+ E[(Xi -E[Xi])(Xj -E[Xj])]
∂ θi∂ θj
Var(X).
(54)
(55)
(56)
Here, if we adopt the linear combination X = λX + (1 - λ)xj to find the parameter θ, we can obtain
Vψλ (θ) = E[X ]= E[λ X + (1 - λ )E[x]] = E[x],	(57)
VVψλ (θ) = Var(λ X + (1 - λ)E[X])
= λ 2Var(X) + (1 - λ)2Var(E[X])
= λ 2Var(X) =λ2ψ(θ)	(58)
where ψλ (∙) is defined by
P (X; θ) = exp
{∑ θiXi + k(X) - ψλ(θ)}.
(59)
0
0
From Bayes theorem, we would be computing the probability of a parameter given the likelihood of
some data:
P(X. θ)=	P(X; θ) P (θ)
P(X.θ) = ∑θ P (X; θ0) P (θ0),
and applying mixup means P(x; θ) → P(X; θ).
(60)
And then, we can obtain equation 17.
□
15
Under review as a conference paper at ICLR 2021
D	Experimental Results for Generalization Error
In this section, we introduce additional experimental results on the relationship between the mixup’s
parameter α and the generalization error. In these experiments, we used ResNet-18 as the network
with lr = 0.1, epochs = 200. In addition, we performed 10 trials with different random seeds and
reported the mean values of the trials.
Table 1 shows the effect of the parameter α on the generalization gap between train and test loss for
each dataset. We can see that the smaller the value of α, the smaller the gap between training loss
and test loss.
Table 1: Effect of the parameter α on the generalization gap between train and test loss for each
dataset.
dataset	α = 0.1	α = 0.2	α = 0.4	α = 0.8	α= 1.0	α = 2.0	α = 4.0
CIFAR10 (Krizhevsky et al., 2009)	0.0061	0.0126	0.0106	0.0610	0.0935	0.0982	0.1303
CIFAR100 (Krizhevsky et al., 2009)	0.1825	0.2592	0.2778	0.2923	0.3485	0.5965	0.6951
STL10 (Coates et al., 2011)	0.0137	0.0215	0.0296	0.0901	0.1210	0.1206	0.1691
SVHN (Netzer et al., 2011)	0.0499	0.0508	0.0571	0.0623	0.0875	0.1330	0.1828
16