Under review as a conference paper at ICLR 2021

TOWARDS  UNDERSTANDING  LINEAR  VALUE

DECOMPOSITION  IN  COOPERATIVE  MULTI-AGENT

Q-LEARNING

Anonymous authors

Paper under double-blind review

Ab stract

Value decomposition is a popular and promising approach to scaling up multi-
agent reinforcement learning in cooperative settings.  However, the theoretical
understanding of such methods is limited. In this paper, we introduce a variant of
the        fitted Q-iteration framework for analyzing multi-agent Q-learning with value
decomposition.  Based on this framework, we derive a closed-form solution to
the    empirical Bellman error minimization with linear value decomposition. With
this  novel  solution,  we  further  reveal  two  interesting  insights:  i)  linear  value
decomposition implicitly implements a classical multi-agent credit assignment
called counterfactual difference rewards; and ii) On-policy data distribution or
richer  Q  function  classes  can  improve  the  training  stability  of  multi-agent  Q-
learning. In the empirical study, our experiments demonstrate the realizability of
our theoretical closed-form formulation and implications in the didactic examples
and a broad set of StarCraft II unit micromanagement tasks, respectively.

1    INTRODUCTION

Cooperative multi-agent reinforcement learning (MARL) has great promise for addressing coor-
dination problems in a variety of applications, such as robotic systems (Hüttenrauch et al., 2017),
autonomous cars (Cao et al., 2012), and sensor networks (Zhang & Lesser, 2011). Such complex tasks
often require MARL to learn decentralized policies for agents to jointly optimize a global 
cumulative
reward signal, and post a number of challenges, including multi-agent credit assignment (Wolpert &
Tumer, 2002; Nguyen et al., 2018), non-stationarity (Zhang & Lesser, 2010; Song et al., 2019), and
scalability (Zhang & Lesser, 2011; Panait & Luke, 2005). Recently, by leveraging the strength of 
deep
learning techniques, cooperative MARL has made a series of great progress (Sunehag et al., 2018;
Baker et al., 2020; Wang et al., 2020b;a), particularly in value-based methods that demonstrate 
state-
of-the-art performance on challenging tasks such as StarCraft unit micromanagement (Samvelyan
et al., 2019). Sunehag et al. (2018) proposed a popular approach called value-decomposition network
(VDN) based on the paradigm of centralized training with decentralized execution (CTDE; Foerster
et al., 2016). VDN learns a centralized but factorizable joint value function Qtₒt, represented as 
the
summation of individual value functions Qi.  During the execution, decentralized policies can be
easily derived for each agent i by greedily selecting actions with respect to its local value 
function
Qi. By utilizing this decomposition structure, an implicit multi-agent credit assignment is realized
because Qi is learned by neural network backpropagation from the total temporal-difference error
on the single global reward signal, rather than on a local reward signal specific to agent i.  This
decomposition technique significantly improves the scalability of multi-agent Q-learning algorithms
and fosters a series of subsequent works, including QMIX (Rashid et al., 2018), QTRAN (Son et al.,
2019), and QPLEX (Wang et al., 2020a).

In spite of the empirical success in a broad class of tasks, multi-agent Q-learning with linear 
value
decomposition has not been theoretically well-understood.  Because of its limited representation
complexity, the standard Bellman update is not a closed operator in the joint action-value function
class with linear value decomposition. The approximation error induced by this incompleteness is
known as inherent Bellman error (Munos & Szepesvári, 2008), which usually deviates Q-learning to
an unexpected behavior. To develop a deeper understanding of learning with value decomposition,
this paper introduces a multi-agent variant of the popular Fitted Q-Iteration (FQI; Ernst et al., 
2005;

1


Under review as a conference paper at ICLR 2021

Levine et al., 2020) framework and derives a closed-form solution to its empirical Bellman error
minimization. To the best of our knowledge, it is the first theoretical analysis that characterizes 
the
underlying mechanism of linear value decomposition in cooperative multi-agent Q-learning, which
can serve as a powerful toolkit to establish follow-up profound theories and explore potential 
insights
from different perspectives in this popular value decomposition structure.

By utilizing this novel closed-form solution, this paper formally reveals two interesting insights: 
1)
Learning linear value decomposition implicitly implements a classical multi-agent credit assignment
method called counterfactual difference rewards (Wolpert & Tumer, 2002), which draws a connection
with COMA (Foerster et al., 2018), a multi-agent policy-gradient method. 2) Multi-agent Q-learning
with linear value decomposition potentially suffers from the risk of unbounded divergence from
arbitrary initialization. On-policy data distribution or richer Q function classes can provide 
local or
global convergence guarantees for multi-agent Q-learning, respectively.

Finally, we set up an extensive set of experiments to demonstrate the realizability of our 
theoretical
implications. Besides the FQI framework, we also consider deep-learning-based implementations of
different multi-agent value decomposition structures. Through didactic examples and the StarCraft II
benchmark, we design several experiments to illustrate the consistency of our closed-form formula-
tion   with the empirical results, and that online data distribution and richer Q function classes 
can
significantly alleviate the limitations of VDN on the offline training process (Levine et al., 
2020).

2    RELATED  WORK

Deep Q-learning algorithms that use neural networks as function approximators have shown great
promise in solving complicated decision-making problems (Mnih et al., 2015).  One of the core
component of such methods is iterative Bellman error minimization, which can be modelled by a
classical framework called Fitted Q-Iteration (FQI; Ernst et al., 2005).  FQI utilizes a specific Q
function class to iteratively optimize empirical Bellman error on a dataset D. Great efforts have 
been
made towards theoretically characterizing the behavior of FQI with finite samples and imperfect
function classes (Munos & Szepesvári, 2008; Farahmand et al., 2010; Chen & Jiang, 2019). From
an empirical perspective, there is also a growing trend to adopt FQI for empirical analysis of deep
offline Q-learning algorithms (Fu et al., 2019; Levine et al., 2020). In MARL, the joint Q function
class grows exponentially with the number of agents, leading many algorithms (Sunehag et al., 2018;
Rashid et al., 2018) to utilize different value decomposition structures with limited 
expressiveness to
improve scalability. In this paper, we extend FQI to a multi-agent variant as our grounding 
theoretical
framework for analyzing cooperative multi-agent Q-learning with linear value decomposition.

To achieve superior effectiveness and scalability in multi-agent settings, centralized training with
decentralized executing (CTDE) has become a popular MARL paradigm (Oliehoek et al., 2008;
Kraemer & Banerjee, 2016). Individual-Global-Max (IGM) principle (Son et al., 2019) is a critical
concept for value-based CTDE (Mahajan et al., 2019), that ensures the consistency between joint and
local greedy action selections and enables effective performance in both training and execution 
phases.
VDN (Sunehag et al., 2018) utilizes linear value decomposition to satisfy a sufficient condition of
IGM. The simple additivity structure of VDN has achieved excellent scalability and inspired many
follow-up methods.  QMIX (Rashid et al., 2018) proposes a monotonic Q network structure to
improve the expressiveness of the factorized function class. QTRAN (Son et al., 2019) tries to 
realize
the entire IGM function class, but its method is computationally intractable and requires two extra
soft regularizations to approximate IGM (which actually loses the IGM guarantee). QPLEX (Wang
et al., 2020a) encodes the IGM principle into the Q network architecture and realizes a complete IGM
function class, but it may also have potential limitations in scalability. Based on the advantages 
of
VDN’s simplicity and scalability, linear value decomposition becomes very popular in MARL (Son
et al., 2019; Wang et al., 2020a;d). This paper focuses on the theoretical and empirical 
understanding
of multi-agent Q-learning with linear value decomposition to explore its underlying implications.

3    NOTATIONS  AND  PRELIMINARIES

3.1    MULTI-AGENT MARKOV DECISION PROCESS (MMDP)

To  support  theoretical  analysis  on  multi-agent  Q-learning,  we  adopt  the  framework  of  
MMDP
(Boutilier, 1996), a special case of Dec-POMDP (Oliehoek et al., 2016), to model fully cooperative

2


Under review as a conference paper at ICLR 2021

multi-agent decision-making tasks.  MMDP is defined as a tuple M  =  ⟨N , S, A, P, r, γ⟩.  N  ≡

{1, . . . , n} is a finite set of agents. S is a finite set of global states. A denotes the action 
space for an

individual agent. The joint action a ∈ A ≡ An is a collection of individual actions [ai]ⁿ   . At 
each

timestep t, a selected joint action at results in a transition st₊₁  ∼ P (·|st, at) and a global 
reward
signal r(st, at). γ ∈ [0, 1) is a discount factor. The goal for MARL is tΣo construct a joint 
policy π =

denoted as Q  (s, a) = r(s, a)+γEs'∼P (·|s,a)[V π(s′)]. We use Q∗ and V ∗ to denote the 
action-value

function and the state-value function corresponding to the optimal policy π∗, respectively.

Dec-POMDP (Oliehoek et al., 2016) is a generalized model of MMDP with the consideration of
partial observability. In Dec-POMDPs, each agent can only access to its local observations rather
than the full information of global states. As infinite-horizon Dec-POMDPs is undecidable in general
(Madani et al., 1999), this paper focuses theoretical analyses on settings with full observability. 
In
practice, partial observability is not a hard constraint.  A Dec-POMDP can be transformed to an
MMDP when communication is available. Many prior efforts have been made to construct efficient
communication protocols for exchanging information among agents (Foerster et al., 2016; Das et al.,
2019; Wang et al., 2020c). By constructing belief states through extended observation scopes, these
methods can approximately transform learning problems in Dec-POMDPs to that in MMDPs. From
this perspective, we consider MMDP as a simplification of notations to make the underlying insights
more accessible.

3.2    CENTRALIZED TRAINING WITH DECENTRALIZED EXECUTION (CTDE)

Most deep multi-agent Q-learning algorithms with value decomposition adopt the paradigm of
centralized training with decentralized execution (Foerster et al., 2016). In the training phase, 
the
centralized trainer can access all global information, including global states, shared global 
rewards,
agents’ polices,  and value functions.   In the decentralized execution phase,  every agent makes
individual decisions based on its local observations.  Note that this paper considers MMDP as a
simplified setting which rules out the concerns of partial observability. Thus our notations do not
distinguish the concepts of states and observations. Individual-Global-Max (IGM) (Son et al., 2019)
is a common principle to realize effective decentralized policy execution.  It enforces the action

selection consistency between the global joint action-value Qtₒt and individual action-values [Qi]ⁿ 
  ,

which are specified as follows:

∀s ∈ S,  arg max Qtₒt(s, a) =    arg max Q₁(s, a₁), . . . , arg max Qn(s, an)    .           (1)

		

As stated in Eq. (2), the additivity constraint adopted by VDN (Sunehag et al., 2018) is a 
sufficient
condition for the IGM constraint stated in Eq. (1).  However, this linear decomposition structure
is    not a necessary condition and induces a limited joint action-value function class because the
linear number of individual functions cannot represent a joint action-value function class, which is
exponential with the number of agents.

n

(Additivity)    Qtₒt(s, a) =        Qi(s, ai).                                         (2)

i=1

3.3    FITTED Q-ITERATION (FQI) FOR MULTI-AGENT Q-LEARNING

For multi-agent Q-learning with value decomposition, we use Qtₒt to denote the global but 
factorized

value function, which can be factorized as a function of individual value functions [Qi]ⁿ   . In 
other

words, we can use [Qi]ⁿ    to represent Qtₒt.  For brevity, we overload Q to denote both of them.

In the MMDP settings, the shared reward signal can only supervise the training of the joint value
function Qtₒt, which requires us to modify the notation of Bellman optimality operator T  as 
follows:

(T Q)tₒt(s, a) = r(s, a) + γ  '        E'           Σmax Qtₒt(s′, a′)Σ .                           
(3)

Fitted Q-iteration (FQI) (Ernst et al., 2005) provides a unified framework which extends the above
operator  to  solve  high-dimensional  tasks  using  function  approximation.   It  follows  an  
iterative
optimization framework based on a given dataset D = {(s, a, r, s′)},

3


Under review as a conference paper at ICLR 2021

Σ.

 

(t)

Σ2Σ

	

where an initial solution Q⁽⁰⁾ is selected arbitrarily from a function class    . By constructing a 
specific
function class     that only contains instances satisfying the IGM condition stated in Eq. (1) 
(Sunehag
et   al., 2018; Rashid et al., 2018), the centralized training procedure in Eq. (4) will naturally 
produce

suitable individual values [Qi]ⁿ   , from which individual policies can be derived for 
decentralized

execution.

4    MULTI-AGENT  Q-LEARNING  WITH  LINEAR  VALUE  DECOMPOSITION

In the literature of deep MARL, constructing a specific value function class     satisfying the IGM
condition is a critical step to realize the paradigm centralized training with decentralized 
execution.
Linear value decomposition proposed by VDN (Sunehag et al., 2018) is a simple yet effective
method to implement this paradigm. In this section, we provide theoretical analysis towards a deeper
understanding of this popular decomposition structure. Our result is based on a multi-agent variant 
of
fitted Q-iteration (FQI) with linear value decomposition, named FQI-LVD. We derive the closed-form
update rule of FQI-LVD, and then reveal the underlying credit assignment mechanism realized by
linear value decomposition learning.

4.1    MULTI-AGENT FITTED Q-ITERATION WITH LINEAR VALUE DECOMPOSITION (FQI-LVD)

To provide a clear perspective on the effects of linear value decomposition, we make an additional
assumption to simplify the notations and facilitate the analysis.

Assumption 1 (Adequate and Factorizable Dataset).  The dataset D contains all applicable state-
action pairs (s, a) whose empirical probability is factorizable with respect to individual 
behaviors of
multiple agents. Formally, let pD(a s) denote the empirical probability of joint action a executed 
on
state s, which can be factorized to the production of individual components,

pD(a|s) =  Y pD(ai|s),      Σ pD(ai|s) = 1,     pD(ai|s) > 0,                      (5)

	

where pD(ai|s) denotes the empirical probability of the event that agent i executes ai on state s.

Assumption 1 is based on the fact that an adequate dataset is necessary for FQI algorithms to find
a feasible solution (Farahmand et al., 2010; Chen & Jiang, 2019).   In practice,  the property of
factorizable data distribution can be directly induced by a decentralized data collection procedure.
When agents perform fully decentralized execution, the empirical probability of an event (s, a) in 
the
collected dataset D is naturally factorized.

Now we define FQI with linear value decomposition as follows.

Definition 1 (FQI-LVD).  Given a dataset D, FQI-LVD specifies the action-value function class


LVD

.Q   Qtₒt(·, a) =

Σi=1

Qi(·, ai), ∀a ∈ A and

Σ∀Qi  ∈ R|S||A|

n
i=1

(6)


and induces the empirical Bellman operator T LVD:

Q(t+1)  ← T LVDQ(t)  ≡ arg min          Σ

pD(a, s′|s)

.yˆ⁽ᵗ⁾(s, a, s′) − Σ

2

Qi(s, ai)


Q∈QLVD    (s,a,s')∈S×A×S

Σ                .

 

i=1

Σ             Σ₂

where yˆ⁽ᵗ⁾(s, a, s′)  =  r(s, a) + γ maxₐ'  Q⁽ᵗ⁾ (s′, a′) denotes the sample-based regression 
target.

y⁽ᵗ⁾(s, a)  =  (T Q⁽ᵗ⁾)tₒt(s, a)  =  r(s, a) + γEs'∼    ·|s,a) Σmaxₐ'  Q     (s′, a′)Σ denotes the 
ground-

(t)

truth target value derived by Bellman optimality operator. pD(a, s′ s) = pD(a s)P (s′ s, a) denotes
the empirical probability of the event that agents execute joint action a on state s and transit to 
s′.

Qtₒt and [Qi]ⁿ    refer to the discussion of CTDE defined in Section 3.3.

4


Under review as a conference paper at ICLR 2021

The proof of Eq. (7) is deferred to Lemma 1 in Appendix A. Value-decomposition network (VDN)
(Sunehag et al., 2018) provides an implementation of FQI-LVD, in which individual value functions

[Qi]ⁿ    are parameterized by deep neural networks, and the joint value function Qtₒt can be simply

formed by their summation.

4.2    IMPLICIT CREDIT ASSIGNMENT IN LINEAR VALUE DECOMPOSITION

In the formulation of FQI-LVD, the empirical Bellman error minimization in Eq. (7) can be regarded
as a weighted linear least-squares problem, which contains n|S||A| variables to form individual

value functions [Qi]ⁿ    and |S||A|n data points corresponding to all entries of the regression 
target

y⁽ᵗ⁾(s, a). To solve this least-squares problem, we derive a closed-form solution stated in Theorem 
1,
which can be verified through Moore-Penrose inverse (Moore, 1920) for weighted linear regression
analysis. Proofs for all theorems, lemmas, and propositions in this paper are deferred to Appendix.

Theorem 1.  Let Q⁽ᵗ⁺¹⁾  = T LVDQ⁽ᵗ⁾ denote a single iteration of the empirical Bellman operator.
Then ∀i ∈ N , ∀(s, a) ∈ S × A, the individual action-value function Q⁽ᵗ⁺¹⁾(s, ai) =


E        Σy⁽ᵗ⁾ .s, a

⊕ a′

ΣΣ − n − 1

E       Σy⁽ᵗ⁾ (s, a′)Σ +w (s),                 (8)


`  evaluation of the ˛in¸dividual action ai    x

`   counterfac˛tu¸al baseline     x

	

except for agent i. The residue term w ≡ [wi]i=1  is an arbitrary vector satisfying ∀s, Σi=1 wi(s) 
=

As shown in Theorem 1, the local action-value function Q⁽ᵗ⁺¹⁾ consists of three terms. The first 
term
is the expectation of one-step TD target value over the actions of other agents, which evaluates the
expected return of executing an individual action ai. The second term is the expectation of one-step
target TD values over all joint actions, which can be regarded as a baseline function evaluating
the average performance. The arbitrary vector w indicates the entire valid individual action-value
function space. We can ignore this term because w does not affect the local action selection of each
agent and will be eliminated in the summation operator of linear value decomposition (see Eq. (2)),

compare the theoretical analysis of FQI-LVD with the empirical results of VDN to demonstrate and
verify the accuracy of our closed-form updating rule (see Eq. (8)) in Section 6.1.

Note that, if we regard the empirical probability pD(a s) within the dataset D as a default policy,
the first term of Eq. (8) is the expected value of an individual action ai, and the second term is
the expected value of the default policy, which is considered as the counterfactual baseline. Their
difference corresponds to a credit assignment mechanism called counterfactual difference rewards,
which has been used by counterfactual multi-agent policy gradient (COMA) (Foerster et al., 2018).

Implication 1.  As shown in Eq. (8), linear value decomposition implicitly implements a counterfac-
tual credit assignment mechanism, which is similar to what is used by COMA.

Compared to COMA, this implicit credit assignment is naturally served by empirical Bellman error
minimization through linear value decomposition, which is much more scalable. The extra importance
weight (n     1)/n brings our derived credit assignment to be more consistent and meaningful in the
sense that all global rewards should be assigned to agents. Consider a simple case where all joint
actions generate the same reward signals, Eq. (8) will assign 1/n unit of rewards to each agent, but
COMA will assign 0. This gap will gradually close when n becomes sufficiently large.

5    IMPROVING  THE  LEARNING  STABILITY  OF  VALUE  DECOMPOSITION

In the previous section, we have derived the closed-form update rule of FQI-LVD, which reveals the
underlying credit assignment mechanism of linear value decomposition structure. This derivation also
enables us to investigate more algorithmic functionalities of linear value decomposition in 
multi-agent
Q-learning. Although linear value decomposition holds superior scalability in multi-agent settings,
we find that FQI-LVD has the potential risk of unbounded divergence from arbitrary initialization.
To improve the stability of linear value decomposition training, we theoretically demonstrate that
on-policy data distribution or richer Q function classes can provide some convergence guarantees.
Moreover, we also utilize a concrete MMDP example to visualize our implications.

5


Under review as a conference paper at ICLR 2021


r=0

a1    a2 , r=0

19

10                                                                               

=   0.01

500

             QPLEX


          

14                                                                                                  
      

10

=   0.1

=   0.25

400

QTRAN

             QMIX


a1 =a2 =A(2)

1               r=0                 2

      =   0.5

9                                                                                                   
              =   1.0

10

Qmax

300

200

          VDN

             Optimal


a1 =a2 =A⁽¹⁾, r=1

4

10

0            200         400         600         800        1000

Iteration  t

100

0

0          20         40         60         80        100

Iteration t


(a) Two-state MMDP

(b) On-policy FQI-LVD

(c) Deep MARL


for

r(s, a)

and the action space for each agent A  ≡

.A(1)

, . . . , A

(|A|)Σ

5

.  (b) The learning curves

of   Qtₒt       of on-policy FQI-LVD on the given MMDP where the dataset is generated by different
choices of hyper-parameters ϵ for ϵ-greedy. (c) The learning curves of   Qtₒt       while running 
several
deep multi-agent Q-learning algorithms.

5.1    UNBOUNDED DIVERGENCE IN OFFLINE TRAINING

We will provide an analysis of the convergence of FQI-LVD with offline training on a dataset D.

Proposition 1.  The empirical Bellman operator T LVD  is not a γ-contraction, i.e., the following

important property of the standard Bellman optimality operator T  does not hold for TD    anymore.

∀Qtₒt, Q′tot  ∈ Q,     ǁT Qtₒt − T Q′totǁ∞ ≤ γǁQtₒt − Q′totǁ∞                      (9)

For the standard Bellman optimality operator      (Sutton & Barto, 2018), γ-contraction is critical
to derive the theoretical guarantee. In the context of FQI-LVD, the additivity constraint limits the
joint action-value function class that it can express, which deviates the empirical Bellman operator
LVD  from the original Bellman optimality operator     (see Theorem 1). This deviation is induced
by the negative importance weight (n     1)/n stated in Eq. (8) and is also known as inherent 
Bellman
error (Munos & Szepesvári, 2008), which corrupts a broad set of stability properties, including

γ-contraction.

To serve a concrete example, we construct a simple MMDP with two agents, two global states, and
two actions (see Figure 1a). The optimal policy of this MMDP is simply executing the action    ⁽¹⁾
at state s₂, which is the only way for two agents to obtain a positive reward. The learning curve of
ϵ = 1.0 (green one) in Figure 1b refers to an offline setting with uniform data distribution, in 
which
an unbounded divergence can be observed as depicted by the following proposition.

Proposition 2.  There exist MMDPs such that,  when using uniform data distribution,  the value
function of FQI-LVD diverges to infinity from an arbitrary initialization Q⁽⁰⁾.

Note that the unbounded divergence discussed in Proposition 2 would happen to an arbitrary initial-
ization Q⁽⁰⁾. To provide an implication for practical scenarios, we also investigate the 
performance of
several deep multi-agent Q-learning algorithms in this MMDP. As shown in Figure 1c, VDN (Sunehag
et  al., 2018), a deep-learning-based implementation of FQI-LVD, results in unbounded divergence.
We postpone the discussion of other deep-learning-based algorithms to the next subsection.

5.2    LOCAL AND GLOBAL CONVERGENCE IMPROVEMENTS

To improve the training stability of FQI-LVD, we investigate methods to enable local and global
convergence of value decomposition learning, respectively.

Local Convergence Improvement.    As shown in Theorem 1, the choice of training data distribution
affects the output of the empirical Bellman operator     LVD.   We find that FQI-LVD has a local
convergence property in an on-policy mode, i.e., the dataset D is accumulated by running an ϵ-greedy
policy (Mnih et al., 2015). Here we include an informal statement of local stability of FQI-LVD and
defer the precise version, its proof, and the algorithm box of on-policy FQI-LVD to Appendix C.1.

Theorem 2 (Informal).  On-policy FQI-LVD will locally converge to the optimal policy and have a
fixed point value function when the hyper-parameter ϵ is sufficiently small.

Theorem 2 indicates that multi-agent Q-learning with linear value decomposition has a convergent
region, where the value function induces optimal actions.  By combining this local stability with

6


Under review as a conference paper at ICLR 2021

Brouwer’s fixed-point theorem (Brouwer, 1911), we can further verify the existence of a fixed-point
solution for the on-policy Bellman operator    LVD. Figure 1b visualizes the performance of 
on-policy
FQI-LVD with different values of the hyper-parameter ϵ.  With a smaller ϵ (such as 0.1 or 0.01),
on-policy FQI-LVD demonstrates numerical stability, and their corresponding collected datasets are
closer to on-policy data distribution.

Global Convergence Improvement.    Linear value decomposition structure limits the joint action-
value function class    LVD, which is the origin of the deviation of the empirical Bellman operator 
   LVD,
discussed in Proposition 1. Another way to improve training stability is to enrich the 
expressiveness
of  value decomposition. We consider a multi-agent fitted Q-iteration (FQI) with a full action-value
function class derived from IGM, named FQI-IGM, whose action-value function class is as follows:


QIGM  =   ,Q   Q

tot

∈ R|S||A|n  and  Σ∀Q

R|S||A|  n

i=1

with Eq. (1) is satisfied, .         (10)

Note that    LVD         IGM indicates that linear decomposition structure stated in Eq. (2) is a 
sufficient
condition for the IGM constraint. The formal definition of FQI-IGM is deferred to Appendix C.2 and
its global convergence property is established by the following theorem.

Theorem 3.  FQI-IGM will globally converge to the optimal value function.

Theorem 3 relies on a fact that    IGM is complete in MMDP settings, i.e., inherent Bellman errors
discussed in Proposition 1 can reach zero and its empirical Bellman operator    IGM is a 
γ-contraction.
Using universal function approximation of neural networks, QPLEX (Wang et al., 2020a), a deep-
learning-based implementation of FQI-IGM, theoretically realizes the complete IGM function class.
QTRAN (Son et al., 2019) is an approximate implementation of FQI-IGM which uses soft penalties
to realize IGM constraints.  As the basis of comparison, VDN (Sunehag et al., 2018) is the deep-
learning-based implementation of FQI-LVD. An intermediate version, QMIX (Rashid et al., 2018),
establishes  a non-linear monotonic mapping between local and global value functions. The value
function class of QMIX can be summarized as follows:

QQMIX  = ,Q   Qtₒt(s, a) = f (s, Q₁(s, a₁), . . . , Qn(s, an)) and f (s, ·) is monotonic, ,      
(11)

which is known to underrepresent the IGM function class since the monotonic correspondence is
not necessary for the IGM constraint stated in Eq. (1) (Mahajan et al., 2019).  Formally, QLVD  ⊂
QQMIX  ⊂ QIGM  is a sequence of strict inclusion relations.

As shown in Figure 1c, QPLEX and QTRAN, two algorithms with representational capacity    IGM,
perform outstanding numerical stability in the proposed MDP example. By contrast, the phenomenon
of unbounded divergence happens to both VDN and QMIX, whose function classes are incomplete in
terms of the IGM constraint. This experiment is a didactic study connection between our theoretical
implications and practical algorithms.  Combining the theoretical and empirical results above, we
summarize this section by the following insights.

Implication 2.  Multi-agent Q-learning with linear value decomposition potentially suffers from the
risk of unbounded divergence from arbitrary initialization. On-policy data distribution or richer Q
function classes can improve its local or global convergence, respectively.

6    EMPIRICAL  ANALYSIS

In this section, we conduct an empirical study to connect our theoretical implications to practical
scenarios of deep multi-agent Q-learning algorithms. An empirical analysis of a didactic example, a
two-state MMDP, has been carried out in Section 5, which shows that the linear value decomposition
structure needs to improve training stability in offline mode. In order to verify other 
implications,
here we evaluate four state-of-the-art deep-learning-based methods, VDN (Sunehag et al., 2018),
QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), and QPLEX (Wang et al., 2020a) on the
matrix game proposed by QTRAN and StarCraft Multi-Agent Challenge (SMAC) benchmark tasks
(Samvelyan et al., 2019). The implementation details of four baselines and experimental settings are
deferred to Appendix F. We test all experiments with 6 random seeds and demonstrate them with
median performance and 25-75% percentiles.

7


Under review as a conference paper at ICLR 2021


1             (1)

a₂

A(2)

A(3)

1             (1)

a₂

A(2)

A(3)

1             (1)

a₂

A(2)

A(3)


A(1)

A(2)

A(3)

8

-12

-12

-12

0

0

-12

0

0

A(1)

A(2)

A(3)

-6.22

-4.89

-4.89

-4.89

-3.56

-3.56

-4.89

-3.56

-3.56

A(1)

A(2)

A(3)

-6.23

-4.90

-4.90

-4.90

-3.57

-3.57

-4.90

-3.57

-3.57


(a) Payoff of matrix game

(b) Qtot  of FQI-LVD

(c) Qtot  of VDN

Table 1: (a) Payoff matrix of the one-step game. Boldface means the optimal joint action selection
from payoff matrix. (b,c) Joint action-value functions Qtₒt of FQI-LVD and VDN. Boldface means
the greedy joint action selection from Qtₒt.


100

80

60

40

20

0

3s_vs_5z
2s3z
5m_vs_6m

100

80

60

40

20

0

QPLEX
QTRAN
QMIX
VDN

Behavior

100

80

60

40

20

0

100

80

60

40

20

0


0.0M     0.5M     1.0M     1.5M     2.0M     2.5M

Timesteps

(a) Online data collection

100

80

60

40

1c3s5z

100

80

60

40

0           60        120       180       240       300

Epochs

(b) 3s_vs_5z

QPLEX
QTRAN
QMIX
VDN

Behavior

100

80

60

40

0           60        120       180       240       300

Epochs

(c) 2s3z

100

80

60

40

0           60        120       180       240       300

Epochs

(d) 5m_vs_6m


20                                                     3c7z                                      20

10m_vs_11m

0                                                                                                0

20                                                                                             20

0                                                                                               0


0.0M     0.4M     0.8M     1.2M     1.6M     2.0M

Timesteps

(e) Online data collection

100

80

60

40

3s5z

100

80

60

40

0           60        120       180       240       300

Epochs

(f) 1c3s5z

QPLEX
QTRAN
QMIX
VDN

Behavior

100

80

60

40

0           60        120       180       240       300

Epochs

(g) 3c7z

100

80

60

40

0           60        120       180       240       300

Epochs

(h) 10m_vs_11m


20                                                          2s_vs_1sc                       20

3h_vs_4z

0                                                                                                0

20                                                                                             20

0                                                                                               0


0.0M     0.4M     0.8M     1.2M     1.6M     2.0M

Timesteps

(i) Online data collection

0           60        120       180       240       300

Epochs

(j) 3s5z

0           60        120       180       240       300

Epochs

(k) 2s_vs_1sc

0           60        120       180       240       300

Epochs

(l) 3h_vs_4z

Figure 2: (a,c,i) Constructing datasets using online data collection of VDN. (b-d,f-h,j-l) 
Evaluating
the performance of deep multi-agent Q-learning algorithms with a given static dataset on nine maps.

6.1    IS OUR CLOSED-FORM UPDATE RULE OF LINEAR VALUE DECOMPOSITION CONSISTENT
WITH THE DEEP-LEARNING-BASED EMPIRICAL RESULTS?

As shown in Theorem 1, we derive the closed-form update rule of FQI-LVD. From an optimization
perspective,  FQI-LVD and VDN share the same objective function (see Definition 1) but have
different optimization methods, i.e., arg min vs. gradient descent. Starting from a common matrix
game used by QTRAN (Son et al., 2019) and QPLEX (Wang et al., 2020a) stated in Table 1a, we
will illustrate the correctness of our closed-form formulation. This matrix game describes a simple
cooperative multi-agent, which includes two agents and three actions. Miscoordination penalties are
also considered and the optimal strategy for two agents is to perform action    ⁽¹⁾ simultaneously. 
We
adopt a uniform data distribution to conduct this didactic example.

Table 1b and 1c show the joint action-value functions of FQI-LVD and VDN, respectively.  Com-
paring with these two joint action-value functions, we find that the estimation error of VDN is only
ǁQ            − QVDNǁ∞ = 0.01, which corresponds to a 0.2% relative error.  This simulation result

strongly illustrates the accuracy of Theorem 1.  A learning curve of relative error for Table 1c is

provided in Appendix G.1. In addition, as discussed by QTRAN and QPLEX, VDN with limited
function class cannot learn the optimal policy in this didactic matrix game. The joint action-value
functions of QPLEX, QTRAN, and QMIX are deferred to Appendix G.2, where QPLEX and QTRAN
can solve this task, but QMIX cannot.

6.2    IS LINEAR VALUE DECOMPOSITION LIMITED IN OFFLINE TRAINING?

Section 5 shows that in offline training mode, linear value decomposition is limited in a didactic
MMDP  task.   In  order  to  generalize  our  implications  to  complex  domains,  we  investigate  
the

8


Under review as a conference paper at ICLR 2021

performance of deep multi-agent Q-learning in the StarCraft II benchmark tasks with offline data
collection. Recently, offline reinforcement learning has attracted great attention because it can 
equip
with multi-source datasets and is regarded as a key step towards real-world applications (Dulac-
Arnold et al., 2019; Levine et al., 2020). Differing from other related work studying distributional
shift (Fujimoto et al., 2019; Levine et al., 2020; Yu et al., 2020), we aim to adopt a diverse 
dataset to
investigate the effect of the expressiveness of a value decomposition structure on offline 
training, i.e.,
which value decomposition structure is suitable for multi-agent offline reinforcement learning. 
These
datasets are constructed by training a behavior policy of VDN (Sunehag et al., 2018) and collecting 
a
fixed number of experienced episodes during the whole training procedure.

We evaluate the learning curve of StarCraft II on nine common maps. The results are shown in Figure

2. To approximate the MMDP setting, we concatenate the global state with the local observations
for each agent to handle partial observability.  Figure 2(b-d,f-h,j-l) illustrate that VDN (Sunehag
et al., 2018) and QMIX (Rashid et al., 2018) performs poorly and cannot utilize well the offline
dataset collected by an unfamiliar behavior policy.  In contrast, QPLEX (Wang et al., 2020a) and
QTRAN (Son et al., 2019) with richer Q function class perform pretty well, which indicates that the
expressiveness of value decomposition structures dramatically affects the performance of multi-agent
offline Q-learning.  The learning curves of Behavior line are shown in Figure 2(a,e,i), which is
implemented by VDN with ϵ-greedy online data collection.  Figure 2(a,e,i) show that VDN with
online   data collection can solve these nine tasks, but cannot with offline data collection, that 
is, there is
a considerable gap between online and offline training with linear value decomposition. Although the
distribution shift (Levine et al., 2020) can be a potential cause of this gap, the remarkable 
performance
of QPLEX and QTRAN suggests that our datasets should be sufficient for offline training.

In contrast to the theoretical convergence analysis stated in Section 5, in this subsection, 
empirical
experiments aim to conduct the performance of the deep-learning-based implementation of linear
value decomposition (i.e., VDN) in the online and offline data collection settings. We have designed
several comparative experiments to demonstrate two empirical implications shared with the above
theoretical understanding (see Implication 2):  1) VDN with online data collection has superior
performance than offline data collection.  2) VDN, a deep-learning-based algorithm with a linear
value decomposition structure, has considerable limitations in offline training; while QPLEX and
QTRAN, two deep-learning-based algorithms with or approximately with complete IGM function
class, are the state-of-the-art value decomposition algorithms for multi-agent offline training.

7    CONCLUSION

This paper makes an initial effort to provide theoretical analysis on multi-agent Q-learning with 
value
decomposition. We derive a closed-form solution to the empirical Bellman error minimization with
linear value decomposition.  Based on this novel result, we reveal the implicit credit assignment
mechanism of linear value decomposition learning and provide a formal analysis of its learning
stability and convergence. We also formally show that on-policy training or a richer value function
class can improve the stability of factorized multi-agent Q-learning. Empirical results are 
conducted
with state-of-the-art deep multi-agent Q-learning with value decomposition and verify theoretical
insights in both didactic examples and complex StarCraft II benchmark tasks.

To close this paper, we connect our results with an additional related literature named relative 
overgen-
eralization pathology (Wiegand, 2003). In the empirical studies of cooperative learning, the 
behaviors
of individual agents are usually negatively affected by their uncooperative partners.  Focusing on
a similar issue, our theoretical analysis of the implicit counterfactual credit assignment provides 
a
detailed characterization to understand this pathological phenomenon in linear value decomposition,
which also provides insights for the corresponding feasible solutions. Regarding relative overgen-
eralization pathology, coordination graphs (Böhmer et al., 2020) explore a different methodology
for cooperative multi-agent Q-learning. This method allows collaborated action-selection through
communications, which does not follow the principle of IGM consistency.  In comparison with
linear value decomposition, coordination graphs use a higher-order value decomposition structure
(Castellini et al., 2019) in the view of function approximation. Supplementary to the results of 
this
paper, the value factorization of coordination graphs provides a different and promising perspective
for future studies.

9


Under review as a conference paper at ICLR 2021

REFERENCES

Alekh Agarwal, Nan Jiang, and Sham M Kakade. Reinforcement learning: Theory and algorithms.
Technical report, Technical Report, Department of Computer Science, University of Washington,
2019.

Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch.  Emergent tool use from multi-agent autocurricula.  In International Conference on
Learning Representations, 2020.

Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International
Conference on Machine Learning, 2020.

Craig Boutilier. Planning, learning and coordination in multiagent decision processes. In 
Proceedings
of the 6th Conference on Theoretical Aspects of Rationality and Knowledge, pp. 195–210. Morgan
Kaufmann Publishers Inc., 1996.

Luitzen Egbertus Jan Brouwer. Über abbildung von mannigfaltigkeiten. Mathematische Annalen, 71
(1):97–115, 1911.

Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen.  An overview of recent progress in the
study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):
427–438, 2012.

Jacopo Castellini, Frans A Oliehoek, Rahul Savani, and Shimon Whiteson.  The representational
capacity of action-value networks for multi-agent reinforcement learning. In Proceedings of the
18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1862–1864,
2019.

Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. 
In

International Conference on Machine Learning, pp. 1042–1051, 2019.

Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle
Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine
Learning, pp. 1538–1546, 2019.

Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement
learning. arXiv preprint arXiv:1904.12901, 2019.

Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.

Journal of Machine Learning Research, 6(Apr):503–556, 2005.

Amir-massoud Farahmand, Csaba Szepesvári, and Rémi Munos. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems, pp. 568–576,
2010.

Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 2137–2145, 2016.

Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients.   In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.

Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-learning
algorithms. In International Conference on Machine Learning, pp. 2021–2030, 2019.

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062, 2019.

Maximilian Hüttenrauch, Adrian Šošic´, and Gerhard Neumann. Guided deep reinforcement learning
for swarm systems. arXiv preprint arXiv:1709.06011, 2017.

Landon Kraemer and Bikramjit Banerjee.  Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190:82–94, 2016.

10


Under review as a conference paper at ICLR 2021

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Omid Madani, Steve Hanks, and Anne Condon. On the undecidability of probabilistic planning and
infinite-horizon partially observable markov decision problems. In AAAI/IAAI, pp. 541–548, 1999.

Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.  Maven:  Multi-agent
variational exploration. In Advances in Neural Information Processing Systems, pp. 7611–7622,
2019.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Eliakim H Moore.  On the reciprocal of the general algebraic matrix.  Bulletin of the American
Mathematical Society, 26:394–395, 1920.

Rémi Munos and Csaba Szepesvári. Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research, 9(May):815–857, 2008.

Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent
rl with global rewards. In Advances in Neural Information Processing Systems, pp. 8102–8113,
2018.

Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions
for decentralized pomdps. Journal of Artificial Intelligence Research, 32:289–353, 2008.

Frans A Oliehoek, Christopher Amato, et al.  A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.

Liviu Panait and Sean Luke.  Cooperative multi-agent learning: The state of the art.  Autonomous
Agents and Multi-agent Systems, 11(3):387–434, 2005.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and  Shimon  Whiteson.   Qmix:  Monotonic  value  function  factorisation  for  deep  multi-agent
reinforcement learning. In International Conference on Machine Learning, pp. 4292–4301, 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.  The
starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, pp. 2186–2188. International Foundation for Autonomous Agents
and Multiagent Systems, 2019.

Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular
mdps. In Advances in Neural Information Processing Systems, pp. 1151–1160, 2019.

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887–5896, 2019.

Xinliang Song, Tonghan Wang, and Chongjie Zhang. Convergence of multi-agent learning with a
finite step size in general-sum games. In Proceedings of the 18th International Conference on Au-
tonomous Agents and MultiAgent Systems, pp. 935–943. International Foundation for Autonomous
Agents and Multiagent Systems, 2019.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085–2087, 2018.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Jianhao Wang, Zhizhou Ren, Terry Liu, Yu Yang, and Chongjie Zhang.  Qplex:  Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020a.

11


Under review as a conference paper at ICLR 2021

Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Multi-agent reinforcement learning
with emergent roles. In International Conference on Machine Learning, 2020b.

Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable
value  functions  via  communication  minimization.   In  International  Conference  on  Learning
Representations, 2020c.

Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-agent
decomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020d.

R Paul Wiegand. An analysis of cooperative coevolutionary algorithms. PhD thesis, Citeseer, 2003.
David H Wolpert and Kagan Tumer.   Optimal payoff functions for members of collectives.   In

Modeling Complexity in Economic and Social Systems, pp. 355–369. World Scientific, 2002.

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based offline policy optimization. In Advances in Neural Information
Processing Systems, 2020.

Chongjie Zhang and Victor Lesser.  Multi-agent learning with policy prediction.  In Twenty-fourth
AAAI conference on Artificial Intelligence, 2010.

Chongjie Zhang and Victor Lesser. Coordinated multi-agent reinforcement learning in networked
distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.

12


Under review as a conference paper at ICLR 2021

A    OMITTED  PROOFS  IN  SECTION  4

Definition 1 (FQI-LVD).  Given a dataset D, FQI-LVD specifies the action-value function class


LVD

.Q   Qtₒt(·, a) =

Σi=1

Qi(·, ai), ∀a ∈ A and

Σ∀Qi  ∈ R|S||A|

n
i=1

(6)


and induces the empirical Bellman operator T LVD:

Q(t+1)  ← T LVDQ(t)  ≡ arg min          Σ

pD(a, s′|s)

.yˆ⁽ᵗ⁾(s, a, s′) − Σ

2

Qi(s, ai)


Q∈QLVD    (s,a,s')∈S×A×S

Σ                .

 

i=1

Σ             Σ₂

where yˆ⁽ᵗ⁾(s, a, s′)  =  r(s, a) + γ maxₐ'  Q⁽ᵗ⁾ (s′, a′) denotes the sample-based regression 
target.

y⁽ᵗ⁾(s, a)  =  (T Q⁽ᵗ⁾)tₒt(s, a)  =  r(s, a) + γEs'∼    ·|s,a) Σmaxₐ'  Q     (s′, a′)Σ denotes the 
ground-

(t)

truth target value derived by Bellman optimality operator. pD(a, s′ s) = pD(a s)P (s′ s, a) denotes
the empirical probability of the event that agents execute joint action a on state s and transit to 
s′.

Qtₒt and [Qi]ⁿ    refer to the discussion of CTDE defined in Section 3.3.

Lemma 1.  The empirical Bellman operator T LVD  defined in Definition 1 is equivalent to

TD   Q     ≡ arg min          Σ        pD(a, s′|s) .yˆ⁽ᵗ⁾(s, a, s′) − Σ

2

Qi(s, ai)


Q∈QLVD    (s,a,s')∈S×A×S

Σ                .

i=1

Σ             Σ₂


= arg min

Q∈QLVD    (s,a)∈S×A

Proof.  Recall Definition 1,

pD(a|s)

y⁽ᵗ⁾(s, a) −

i=1

Qi(s, ai)

.                       (12)


TD   Q     ≡ arg min          Σ        pD(a, s′|s)

.yˆ⁽ᵗ⁾(s, a, s′) − Σ

2

Qi(s, ai)


Q∈QLVD   (s,a,s')∈S×A×S

i=1


= arg min       E            yˆ⁽ᵗ⁾(s, a, s′)     Q

Q∈QLVD   (s,a,s')∼D

tot

(s, a)Σ2Σ


= arg min       E            yˆ⁽ᵗ⁾(s, a, s′)     y⁽ᵗ⁾(s, a) + y⁽ᵗ⁾(s, a)     Q

Q∈QLVD   (s,a,s')∼D

tot

(s, a)Σ2Σ

= arg min       E       Σ.yˆ⁽ᵗ⁾(s, a, s′) − y⁽ᵗ⁾(s, a)Σ2Σ


Q∈QLVD   (s,a,s')∼D

+        E

(s,a,s')∼D

Σ2 .yˆ⁽ᵗ⁾(s, a, s′) − y⁽ᵗ⁾(s, a)Σ .y⁽ᵗ⁾(s, a) − Qtₒt(s, a)ΣΣ


+        E

(s,a,s')∼D

Σ.y⁽ᵗ⁾(s, a) − Q

tot

(s, a)Σ2Σ                                      (13)

The first term is a constant since y⁽ᵗ⁾ and yˆ⁽ᵗ⁾ are fixed targets.
The second term is equal to zero since


E

(s,a,s')∼D

Σ2 .yˆ⁽ᵗ⁾(s, a, s′) − y⁽ᵗ⁾(s, a)Σ .y⁽ᵗ⁾(s, a) − Qtₒt(s, a)ΣΣ


=  2     E     Σ       E        Σyˆ⁽ᵗ⁾(s, a, s′) − y⁽ᵗ⁾(s, a)Σ

.y⁽ᵗ⁾(s, a) − Qtₒt(s, a)ΣΣ

`                      =˛¸0                                     x

13


Under review as a conference paper at ICLR 2021

The third term exactly corresponds to Eq. (12).


Lemma 2.  Considering following weighted linear regression problem

min ǁ √pT · (Ax − b) ǁ2

(15)

where A ∈  Rmn×mn, x ∈  Rmn, b, p ∈  Rmn , m, n ∈  Z⁺.  Besides, A is m-ary encoding matrix
namely ∀i ∈ [mⁿ], j ∈ [mn]

.1,     if    ∃u ∈ [n], j = m × u + ([i/mᵘ∫ mod m),

For simplicity, jᵗʰ row of A corresponds to a m-ary number ˙aj = (j)m where ˙a = a₀a₁ . . . an−₁,
with au ∈ [m], ∀u ∈ [n]. Assume p is a positive vector which follows that


pj = p(˙aj) =

u∈[n]

pu(au,j),  where pu : [m] → (0, 1) and

au∈[m]

pu(au) = 1, ∀u ∈ [n]     (17)

The optimal solution of this problem is the following. Denote i = u × m + v, v ∈ [m], u ∈ [n] and
an arbitrary vector w ∈ Rmn


x∗ = Σ   p(˙a)   b

· 1(a

= v) −  n − 1 p(˙a)b   −    1  

 1

w '  +                 w

	

'       (18)

Proof.  For brevity, denote

Aᵖ = √pT · A,         bᵖ = √pT · b                                          (19)

Then the weighted linear regression becomes a standard Linear regression problem w.r.t Aᵖ, bᵖ. To
compute the optimal solutions, we need to calculate the Moore-Penrose inverse of Aᵖ . The sufficient
and necessary condition of this inverse matrix Aᵖ,†    Rmn×mn   is the following three statements
(Moore, 1920):

(1) AᵖAᵖ,† and Aᵖ,†Aᵖ are self-adjoint                                         (20)

(2) Aᵖ = AᵖAᵖ,†Aᵖ                                                           (21)

(3) Aᵖ,† = Aᵖ,†AᵖAᵖ,†                                                    (22)

We consider the following matrix as Aᵖ,† and we prove that it satisfies all three statements.  For

∀i ∈ [mn], i = u × m + v, u ∈ [n], v ∈ [m], j ∈ [mⁿ]

Ap,† = Ap,†


i,j

i,˙aj

. p(˙a       )

n − 1 .

1 . p(˙a       )

1    nΣ−1  . p(˙a    '    )


=               −u,j

pu(au,j)

· 1(au,j = v) −     n

p(˙aj) −  m

         −u,j

pu(au,j)

+

mn u'=0

         −u ,j

pu' (au',j )

(23)


where p(˙a−u) = Qu'

u pu' (au' ).

14


Under review as a conference paper at ICLR 2021

First, we verify that AᵖAᵖ,† is a mⁿ × mⁿ self-adjoint matrix in statement (1).  For simplicity,

O(˙ai, ˙aj) = {u|au,i = au,j, u ∈ [n]}.


(ApAp,†)

=   Σ √p(˙a )[. p(˙a−u,j ) · 1(a

= a    ) −  n − 1 .p(˙a  ) −  1 . p(˙a−u,j )

u   [n]                                                

  1    nΣ−1  . p(˙a    '    )


=       Σ     √p(˙aj)p(˙ai) −  n − 1

Σ .p(˙a )p(˙a  ) −  1

Σ  √p(˙aj)p(˙ai)

	

Σ    1    nΣ−1  √p(˙aj)p(˙ai)                               


u∈[n]  mn u'=0

pu' (au',j )


=       Σ     √p(˙aj' )p(˙ai) − (n − 1).p(˙a )p(˙a  ) −  1

Σ  √p(˙aj)p(˙ai)

	


+  1

m

p(˙aj)p(˙ai)

pu(au,j)

u∈[n]                                 

=       Σ     √p(˙aj )p(˙ai) − (n − 1).p(˙a )p(˙a  )                                              
(24)

Observe that pu(au,j)  =  pu(au,i) if au,i =  au,j, thus (AᵖAᵖ,†)i,j  =  (AᵖAᵖ,†)j,i for any i, j

[mⁿ]. This proves that AᵖAᵖ,† is self-adjoint.

Second, we prove that Aᵖ,†Aᵖ is a mn × mn self-adjoint matrix and has surprisingly succinct form.
Let i = u × m + v, u ∈ [n], v ∈ [m].

1.  i = i′. Besides, O(i) = {˙a ∈ [mⁿ]|au = v}


(Ap,†Ap)

=    Σ  √p(˙a)[. p(˙a−u) · 1(a

= v) −  n − 1 √p(˙a) −   1 . p(˙a−u)


i,i

pu(au)          ᵘ               n

˙a   O(i)                                          

  1    nΣ−1  . p(˙a    ' )

m     pu(au)


+

mn u'=0

−u    ]

pu' (au' )

n−1

=    Σ     p(˙a)   −  n − 1 p(˙a) −   1    p(˙a)   +   1    Σ    p(˙a)   

Σ  .                1                      1    Σ            Σ    n − 1

	


 1

= 1 −  m −

n − 1 p

n

(au

  1  

= v) +

mn

Σ  p(˙a−u' )


  1  

+

mn

˙a∈O(i)

p(˙a−u)

u'∈[n] ˙a∈O(i)

u'/=u


1

= 1 −  m −

1

n − 1 p
n

1

(au

1

= v) +

mn

+ n − 1 mp
mn

(au

= v)

= 1 −  m + mn                                                                                       
   (25)

15


Under review as a conference paper at ICLR 2021

2.  i = u × m + v, i′ = u × m + v′, v /= v′. This implies that Q(i) ∩ O(i′) = Ø


(Ap,†Ap)

i,i

'  =    Σ

p(˙a)[     p(˙a−u)   1(a

pu(au)          ᵘ

= v) −  n − 1 √p(˙a)

˙a∈O(i')                                                                                

−   1 . p(˙a−u)        1    Σ . p(˙a    ' )

n−1

	


m     pu(au)

mn u'=0

pu' (au' )


=         Σ         p(˙a)   −  n − 1

 1

p(˙a) −

Σ     p(˙a)  


+   1    Σ

   p(˙a)           1  

+

  p(˙a)  


mn u'∈[n] ˙a∈O(i')  pu' (au' )

u'/=u

mn ˙a∈O(i')  pu(au)


=       n − 1 p
n

(au

= v′)       1

m

+ n − 1

mn

˙a∈ΣO(i')

p(˙a−u

  1  

' ) + mn

1         1

=  −  m + mn                                                                                   (26)

3.  i = u₁ × m + v₁, i′ = u₂ × m + v₂, u₁ /= u₂.


(Ap,†Ap)

i,i'  =    Σ

p(˙a)[     p(˙a−u1 )    1(a

pu (au )          ᵘ1

= v) −  n − 1 √p(˙a)


˙a∈O(i')

1        1                                             


−   1 . p(˙a−u1 )

  1    nΣ−1  . p(˙a    ' )


m     pu1 (au1 )

mn u'=0

pu' (au' )


=         Σ          p(˙a)    −  n − 1

 1

p(˙a) −

Σ      p(˙a)   


+   1     Σ

   p(˙a)           1  

+

   p(˙a)   


mn  u'∈[n] ˙a∈O(i')  pu' (au' )

u'/=u2

mn ˙a∈O(i')  pu2 (au2 )


= pu2

1

=

(au2

)      n − 1 p

n

(au2

) − pu2

(au2

) + n − 1 mp
mn

(au2

  1  

) +

mn

(27)

mn

Observe that Aᵖ,†Aᵖ is self-adjoint by equation (2,3,4) and the expression is succinct.

Third, we verify statement (2). Since we have computed Aᵖ,†Aᵖ, the verification is straightforward.

For brevity, denote Aᵖ,†Aᵖ as Aᵖ

(AᵖAᵖ)˙ₐ,i = √p(˙a)  Σ (Aᵖ)um₊ₐ  ,i

= √p(˙a) .1(∃u ∈ [n], i = um + a  ) −   1  +   1   + (n − 1)   1   Σ

= √p(˙a) · 1(∃u ∈ [n], i = um + au)                                                           (28)

Thus, AᵖAᵖ,†Aᵖ = Aᵖ.

Similarly, we can verify statement (3). Suppose i₀ = u₀ × m + v₀, we have

16


Under review as a conference paper at ICLR 2021


(AᵖAᵖ,†)

  1  

i0 ,˙a  =  mn

[     p(˙a−u)   1(a
pu(au)          ᵘ

= v)

u/=u0  v   [m]

u∈[n]                                                                                               
                 


−  n − 1 √p(˙a) −

 1 . p(˙a−u)

  1    nΣ−1  . p(˙a    ' )


n                     m     pu(au)

mn u'=0

pu' (au' )


+  Σ (1(v = v  ) −   1

+   1   )[. p(˙a−u0 )  · 1(a

= v)


−  n − 1 √p(˙a) −

 1 . p(˙a−u0 )

  1    nΣ−1  . p(˙a    ' )


n                     m     pu0 (au0 )

mn u'=0

pu' (au' )


=    1  

mn

[     p(˙a−u)   1(a
pu(au)          ᵘ

= v)

u∈[n] v∈[m]                                                                                         
         


−  n − 1 √p(˙a) −

 1 . p(˙a−u)

  1    nΣ−1  . p(˙a    ' )


n                     m     pu(au)

mn u'=0

pu' (au' )

+  Σ (1(v = v  ) −  1 )[− n − 1 √p(˙a) −  1 . p(˙a−u0 )

		


  1    nΣ−1  . p(˙a    ' )

Σ (1(v = v₀) −

 1 ). p(˙a−u0 )

· 1(au0

= v)


mn u'=0

pu' (au' )

v∈[m]

m      pu0 (au0 )

=    1    Σ . p(˙a−u) −  n − 1 √p(˙a)

1  Σ      1 . p(˙a     )        1    nΣ−1  . p(˙a    ' )                         


v   [m]                                 

  1    nΣ−1  . p(˙a    ' )

0        0

 1   . p(˙a      )

	

Clearly, we have the following relations

Σ     1 . p(˙a     )        1    nΣ−1  . p(˙a    ' )

		


Thus

0           m

v∈[m]


(AᵖAᵖ,†)

=    1    Σ . p(˙a−u) −  n − 1 √p(˙a) + (1(a

= v  ) −  1 ). p(˙a−u0 )

(32)


u∈[n]

p,
i0 ,˙a

0        0

(33)

17


Under review as a conference paper at ICLR 2021

This proves Aᵖ,† =  Aᵖ,†AᵖAᵖ,† in statement (3) and Aᵖ,† is the Moore-Penrose inverse of Aᵖ.
Since the optimal solution x∗ = Aᵖ,†bᵖ + (Imn  mn    Aᵖ,†Aᵖ)w where w     Rᵐⁿ is any vector
(Moore, 1920).

Denote xᵖ = Aᵖ,†bᵖ. We have ∀i = u × m + v

xᵖ =  Σ Aᵖ,†√p(˙a)b˙ₐ

=  Σ[. p(˙a−u) · 1(a   = v) −  n − 1 √p(˙a) −  1 . p(˙a−u)

  1    nΣ−1  . p(˙a    ' )  √         


Σ Σ   p(˙a)  

	 

n − 1

 1    p(˙a)  

 

  1    nΣ−1     p(˙a)    Σ

 

From equation (2, 3, 4), we have i = u × m + v, i′ = u′ × m + v′


p,†    p

.  1   −    1  

if u = u′

− mn         if u /= u

If we consider w as the following i₀ = u₀ × m + v₀


w    =    Σ

   p(˙a)   

b

	

(36)


Then for i = u × m + v

˙a∈O(i0 )


((I − Aᵖ,†Aᵖ)w)i

=

i0 ∈[mn]

  1  

− mn wi0

 1

+             (

m

i0 :u0 =u

  1  

−  mn )wi0

(37)

u   u0


=  Σ −   1     Σ

   p(˙a)   

b˙a

+  1  Σ   p(˙a)   b

(38)


˙a       mn u'∈[n]  pu' (au' )

m        pu(au)

˙a

Notice that this is exactly the last two terms in equation (5).  Therefore, the optimal solutions of
this weighted linear regression problem can be written as: i = u × m + v, v ∈ [m], u ∈ [n] and an
arbitrary vector w ∈ Rᵐⁿ.


x∗ = Σ   p(˙a)   b

· 1(a

= v) −  n − 1 p(˙a)b   −    1  

 1

w '  +                 w

	

'       (39)

This completes the proof.

Theorem 1.  Let Q⁽ᵗ⁺¹⁾  = T LVDQ⁽ᵗ⁾ denote a single iteration of the empirical Bellman operator.
Then ∀i ∈ N , ∀(s, a) ∈ S × A, the individual action-value function Q⁽ᵗ⁺¹⁾(s, ai) =


E        Σy⁽ᵗ⁾ .s, a

⊕ a′

ΣΣ − n − 1

E       Σy⁽ᵗ⁾ (s, a′)Σ +w (s),                 (8)


`  evaluation of the ˛in¸dividual action ai    x

`   counterfac˛tu¸al baseline     x

where we denote ai ⊕ a′ i  = ⟨a′1, . . . , a′i−1, ai, a′i+1, . . . , a′n⟩. a′ i denotes the action 
of all agents

			

except for agent i. The residue term w ≡ [wi]i=1  is an arbitrary vector satisfying ∀s, Σi=1 wi(s) 
=

18


Under review as a conference paper at ICLR 2021

Proof.  In the formulation of FQI-LVD stated in Definition 1, the empirical Bellman error minimiza-
tion in Eq. (7) can be regarded as a weighted linear least squares problem as follows: ∀s ∈ S,


min ǁ √pT · (Ax − b) ǁ2

(40)

where  let  m, n  ∈   Z⁺  denote  the  size  of  action  space  |A|  and  the  number  of  agents,  
respec-
tively;  A  ∈   Rmn×mn  denotes  the  multi-agent  credit  assignment  coefficient  matrix  of  
action-
valueΣfunctions with liΣnear value decomposition; x ∈  Rmn  denotes individual action-value func-

		

1, b     Rmn  denotes the regression target y⁽ᵗ⁾(s,  ) derived by Bellman optimality operator; p    
 Rmn
denotes the empirical probability of joint action a executed on state s,  pD(a s),  which can be
factorized to the production of individual components illustrated in Assumption 1.

Besides, A is m-ary encoding matrix namely ∀i ∈ [mⁿ], j ∈ [mn]

.1,     if    ∃u ∈ [n], j = m × u + ([i/mᵘ∫ mod m),

For simplicity, jᵗʰ row of A corresponds to a m-ary number ˙aj = (j)m where ˙a = a₀a₁ . . . an  ₁,
with au    [m],   u     [n]. According to the factorizable empirical probability pD shown in Assump-
tion 1, p is a corresponding positive vector which follows that


pj = p(˙aj) =

u∈[n]

pu(au,j),  where pu : [m] → (0, 1) and

au∈[m]

pu(au) = 1, ∀u ∈ [n]     (42)

According to Lemma 2, we derive the optimal solution of this problem is the following.  Denote

i = u × m + v, v ∈ [m], u ∈ [n] and an arbitrary vector w ∈ Rmn


x∗ = Σ   p(˙a)   b

· 1(a

= v) −  n − 1 p(˙a)b   −    1  

 1

w '  +                 w

	

'       (43)

which means ∀i ∈ N , ∀(s, a) ∈ S × A, the individual action-value function Q⁽ᵗ⁺¹⁾(s, ai) =


E

a'      p

−i

(·|s)

Σy⁽ᵗ⁾ .s, ai

′  ΣΣ −  n − 1

E

a'∼pD

(·|s)

Σy⁽ᵗ⁾ (s, a′)Σ + wi(s),              (44)

where we denote ai ⊕ a′ i  = ⟨a′1, . . . , a′i−1, ai, a′i+1, . . . , a′n⟩. a′ i denotes the action 
of all agents

			

except agent i.  The residue term w ≡  [wi]i=1  is an arbitrary vector satisfying ∀s, Σi=1 wi(s) =

B    OMITTED  PROOFS  IN  SECTION  5.1

Proposition 1.  The empirical Bellman operator T LVD  is not a γ-contraction, i.e., the following

important property of the standard Bellman optimality operator T  does not hold for TD    anymore.

∀Qtₒt, Q′tot  ∈ Q,     ǁT Qtₒt − T Q′totǁ∞ ≤ γǁQtₒt − Q′totǁ∞                      (9)

Proof.  Assume the empirical Bellman operator    LVD is a γ-contraction.  For any MMDPs, when
using a uniform data distribution, the value function of FQI-LVD will converge (Ernst et al., 2005)
because of the contraction of the distance (infinity norm) between any pair of Q.  However, one
counterexample is indicated in Proposition 2, which shows that there exists MMDPs such that, when
using              a uniform data distribution, the value function of FQI-LVD diverges to infinity 
from an arbitrary
initialization Q⁽⁰⁾. The assumption of γ-contraction is not hold and the empirical Bellman operator
T          LVD  is not a γ-contraction.

Proposition 2.  There exist MMDPs such that,  when using uniform data distribution,  the value
function of FQI-LVD diverges to infinity from an arbitrary initialization Q⁽⁰⁾.

19


Under review as a conference paper at ICLR 2021

Proof.  We consider the following MMDP with 2 agents, 2 states (s₁, s₂) and each agent (i = 1, 2)
has 2 actions A  ≡    A(1), A(2)    .  The reward function is listed below which r(sj, a) denotes 
the
reward of (sj, a), where a = ⟨a₁, a₂⟩.

r(s₁) = .0    0Σ    r(s₂) = .1    0Σ                                   (45)

Besides, the transition is deterministic.

T (s₁) = .s1      s1Σ    T (s₂) = .s2      s2Σ                                (46)

Furthermore, γ     ( ⁴ , 1). (In practice, γ is usually chosen as 0.99 or 0.95.) The following 
proves that
this MMDP will diverge for any initialization.

Denote Qᵗ(sj, ai) as the decomposed Q-value of agent i after tᵗʰ value-iteration at state sj with
action ai. Then, the total Q-value can be described as Qᵗ  (sj, a) = Qᵗ (sj, a₁) + Qᵗ (sj, a₂). For


brevity, 0ᵗʰ Q-value is its initialization.

tot                            1                             2

First, we clarify the process of each iteration. Since the value-iteration for linear decomposed 
function
class is solving the MSE problem in Lemma 2. b is target one-step TD-value w.r.t the Q-value of
the last iteration. Through described in Lemma 2, the optimal solution of this MSE problem is not
unique.  We can ignore the term of an arbitrary vector w when considering the joint action-value
functions because w does not affect the local action selection of each agent and will be eliminated 
in
the summation operator of linear value decomposition. In addition, under uniformed sampling, we
observe that pu(au) =  ¹  for any ˙a, u. Then, in equation 34

−  1    p(˙a)   +   1    Σ    p(˙a)     = 0                                           (47)

Second, we denote V ᵗ (sj) = maxₐ Qᵗ  (sj, a) and observe that ∀t ≥ 1, sj


Qᵗ (s  , a  ) =  1

Σ .r(s  , a) + γV ᵗ−¹(T (s  , a)Σ −  1  Σ 1 .r(s  , a) + γV ᵗ−¹(T (s  , a))Σ


1     j     1

2                  j

a2 ∈A

tot                j

2         4         ʲ

a∈A

tot

j

(48)

= Qᵗ (sj, a₂)                                                                                       
                         (49)

The second equation holds because the transition T  and the reward R are symmetric for both agents.
Thus, we omit the subscript of local Q-values as Qᵗ(sj, a) when t ≥ 1.

Third, we analyze the Q-values on state s₁.  Clearly, its iteration is irrelevant to s₂.  According 
to
equation 48, ∀a ∈ A, t ≥ 1

Qᵗ(s  , a) =  γ V ᵗ−¹(s  )                                                                          
      (50)


1                 2   tot

γ

=        max

1

.Qᵗ−¹(s  , a  ) + Qᵗ−¹(s  , a  )Σ                        (51)

Clearly,  when  t  ≥  1, Qᵗ  s₁, A(1)      =  Qᵗ  s₁, A(2)   .   Therefore,  we  observe  that  
Qᵗ(s₁, ·)  =

γᵗq₁, ∀t ≥ 1 where q₁ is determined by the initialization Q⁰  (s₁, a), ∀a ∈ A.

20


Under review as a conference paper at ICLR 2021

Last, we consider state s₂. It is straightforward to observe the following recursion for t      2 
from
equation 48

Qᵗ .s  , A(1)Σ =  1 (1 + 2γV ᵗ−¹(s  )) −  1 [1 + γ(3V ᵗ−¹(s  ) + V ᵗ−¹(s  ))]

=  5γ V ᵗ−¹(s  ) + 3 −  1 γᵗq


=  5γ max Qᵗ−¹(s  , a) + 3 −  1 γᵗq

(52)

4   a∈A                             8      4

Qᵗ .s  , A(2)Σ =  1 (γV ᵗ−¹(s  ) + γV ᵗ−¹(s  )) −  1 [1 + γ(3V ᵗ−¹(s  ) + V ᵗ−¹(s  ))]

=  γ V ᵗ−¹(s  ) −  1      3   t


=  γ max Qᵗ−¹(s  , a) −  1 +    γᵗq

1

(53)

4  a∈A                             8      4


We consider some δ > 0 and t   =   log      δ    . Then, t > t

6|q1 |

Qᵗ .s  , A(2)Σ ≥  γ max Qᵗ−¹(s  , a) −  1 + δ  ≥  γ Qᵗ−¹ .s  , A(2)Σ −  1 + δ

(54)

	


Consequently, Qᵗ(s₂, a₂) ≥ Qᵗδ     s₂, A(2)    , ∀t ≥ tδ by equation 54. Since t ≥ tδ
Q^t .s  , A(2)Σ = . γ Σt−tδ  .Qᵗδ  .s  , A(2)Σ −   1 + δ  Σ +  1 + δ

(55)


Furthermore, γ ∈ ( ⁴ , 1). There exists some Tδ ≥ tδ which

QTδ  .s  , A(2)Σ ≥  Q^Tδ  .s  , A(2)Σ ≥  1 + 2δ  > − 1 + 2δ

(56)


2                                           2

According to equation 52 and let δ <  ¹ .

2γ − 8              6


QTδ +1 .s  , A(1)Σ ≥  5γ QTδ  .s  , A(2)Σ + 3 −  1 γᵗq

(57)

2                            4              2                         8      4      1

5 + 10δ      3       1

> −      24      + 8 −  24 δ                                              (58)

>  1                                                                                  (59)

8


Similar to equation 54, we observer from equation 52 that ∀t > Tδ=   1   + 1

Qt .s  , A(1)Σ ≥  5γ Qt−1 .s  , A(1)Σ + 1

(60)

2                            4                2                         4

and

V ᵗ (s₂) = 2Qᵗ .s₂, A(1)Σ                                                    (61)

≥ 2 . 5γ Qᵗ−¹ .s  , A(1)Σ + 1 Σ                                 (62)


=  5γ V ᵗ−¹ (s  ) + 1

(63)

4    tot           2          4

5γ                                                                                                  
         1                                                     t

Since   4    >  1 and the initial point at Tδ=   1   + 1 is larger than  8 , this suggests that 
Vtot (s₂) will

eventually diverge.

Noticing that our proof holds with respect to any   Q⁰  (sj, a)   j        , a           . Thus, 
value-iteration
on linear decomposed function class w.r.t this MDP will diverge evnetually under any circumstances.

21


Under review as a conference paper at ICLR 2021

C    OMITTED ALGORITHM  BOX, THEOREM, AND  DEFINITION  IN SECTION  5.2

C.1    LOCAL CONVERGENCE IMPROVEMENT

Algorithm 1 On-Policy Fitted Q-Iteration with ϵ-greedy Exploration

1:  Initialize Q⁽⁰⁾.

2:  for t = 0 . . . T     1 do                                                          d T  
denotes the computation budget

3:         Construct an exploratory policy π˜t based on Q⁽ᵗ⁾.                      d i.e., ϵ-greedy 
exploration

π˜t(a|s) = Y .      + (1 − ϵ)I Σa  = arg max Q⁽ᵗ⁾(s, a′ )ΣΣ                (64)

  ϵ

i


i=1

|A|

i              i

a'i∈A

4:         Collect a new dataset Dt by running π˜t.

5:         Operate an on-policy Bellman operator Q⁽ᵗ⁺¹⁾ ← T LVDQ⁽ᵗ⁾ ≡ T LVDQ⁽ᵗ⁾.

Algorithm 1 is a variant of fitted Q-iteration which adopts an on-policy sample distribution. At 
line 3,
an exploratory noise is integrated into the greedy policy, since the function approximator generally
requires an extensive set of samples to regularize extrapolation values. Particularly, we 
investigate
a standard exploration module called ϵ-greedy, in which every agent takes a small probability to
explore actions with non-maximum values.  To make the underlying insights more accessible, we
assume the data collection procedure at line 4 can obtain infinite samples, which makes the dataset
Dt become a sufficient coverage over the state-action space (see Assumption 1). This algorithmic
framework serves as a foundation for discussions on local stability.

We consider an additional assumption stated as follows.

Assumption 2 (Unique Optimal Policy).  The optimal policy π∗ is unique.

The intuitive motivation of this assumption is to have the optimal policy π∗ be a potential stable
solution.  In situations where the optimal policy is not unique, most Q-learning algorithms will
oscillate around multiple optimal policies (Simchowitz & Jamieson, 2019), and Assumption 2 helps

us to rule out these non-interesting cases. Based on this setting, the local stability of FQI-LVD 
can be
characterized by the following lemma.

Lemma 3.  There exists a threshold δ > 0 such that the on-policy Bellman operator T LVD  is closed


in the following subspace B ⊂ QLVD, when the hyper-parameter ϵ

s

is sufficiently small.

=    Q        LVD    πQ = π∗,  max  Qtₒt(s, π∗(s))     V ∗(s)       δ

s∈S

Formally, ∃δ > 0, ∃ϵ > 0, ∀Q ∈ B, there must be T LVDQ ∈ B.

Lemma 3 indicates that once the value function Q steps into the subspace    , the induced policy
πQ will converge to the optimal policy π∗. By combining this local stability with Brouwer’s fixed-
point theorem (Brouwer, 1911), we can further verify the existence of a fixed-point solution for the
on-policy Bellman operator T LVD  (see Theorem 4).

Theorem 4 (Formal version of Theorem 2).  Besides Lemma 3, Algorithm 1 will have a fixed point
value function expressing the optimal policy if the hyper-parameter ϵ is sufficiently small.

Theorem 4 indicates that, multi-agent Q-learning with linear value decomposition has a convergent
region, where the value function induces optimal actions.  Note that     LVD  is a limited function
class, which even cannot guarantee to contain the one-step TD target    LVDQ. From this perspective,
on-policy data distribution becomes necessary to make the one-step TD target projected to a small 
set
of critical state-action pairs, which help construct the stable subspace B stated in Lemma 3.

C.2    GLOBAL CONVERGENCE IMPROVEMENT

Definition 2 (FQI-IGM).  Given a dataset D, FQI-IGM specifies the action-value function class


QIGM  =   ,Q   Q

tot

∈ R|S||A|n  and  Σ∀Q

R|S||A|  n

i=1

22

with that Eq. (1) is satisfied, .      (65)


Under review as a conference paper at ICLR 2021


and induces the empirical Bellman operator

Q(t+1)  ← T IGMQ(t)  ≡ arg min      Σ

pD(a|s) .y⁽ᵗ⁾(s, a) − Q

tot

(s, a)Σ2 ,          (66)

Q∈QIGM   (s,a)∈S×A

where y⁽ᵗ⁾(s, a) = r(s, a) + γ maxₐ'  Q⁽ᵗ⁾ (s′, a′) denotes the regression target derived by 
Bellman

optimality operator. Qtₒt and [Qi]ⁿ    refer to the interfaces of CTDE defined in Section 3.3.

Compared with FQI-LVD stated in Definition 1, the differences are the Q function class, i.e, QIGM  
vs.

QLVD.

D    OMITTED  PROOFS  OF  THEOREM  3

Lemma 4.  The empirical Bellman operator T IGM  stated in Definition 2 is a γ-contraction, i.e., the
following important property of the standard Bellman optimality operator T  will hold for T IGM.

∀Qtₒt, Q′tot  ∈ Q,     ǁT Qtₒt − T Q′totǁ∞ ≤ γǁQtₒt − Q′totǁ∞                    (67)

Proof.  We want to prove

T IGMQ  tot  = r(s, a) + γ ⟨P (s, a), VQ⟩ ,                                       (68)

where P  is transition function, VQ(·) = maxₐ∈A Qtₒt(·, a), and ⟨·, ·⟩ is inner product.  According
to Eq.  (68) and Lemma 1.5 in RL textbook (Agarwal et al., 2019), we can prove that T IGM  is a

γ-contraction. Eq. (68) indicates that the empirical Bellman error


errIGM ≡    min         Σ     p   (a|s) .y⁽ᵗ⁾(s, a) − Q

(s, a)Σ2  = 0.                  (69)

Let a∗,⁽ᵗ⁾  =  Σa∗,⁽ᵗ⁾Σn      =  arg max        y⁽ᵗ⁾(s, a).  Then, ∀y⁽ᵗ⁾(s, ·), we construct Q   
(s, a)  =


y⁽ᵗ⁾(s, a) and its corresponding local action-value functions [Qi]ⁿ

.1,     when ai = a∗,⁽ᵗ⁾,

satisfying IGM principle:


Qi(s, ai) =

i

0,     when ai /= a∗,⁽ᵗ⁾.

(70)

To avoid the multiple solutions of arg max operator in a∗,⁽ᵗ⁾, we consider the lexicographic order 
of
joint actions as the second priority. Thus, we illustrate the completeness of IGM function class in
MMDP setting from our construction. Then, Eq. (68) is held, and T IGM  is a γ-contraction in MMDP

framework.

Theorem 3.  FQI-IGM will globally converge to the optimal value function.

Proof.  Let Q∗(s, a) = maxπ  ΠQπ(s, a) where Π is the space of all policies. According to Lemma 4
and Theorem 1.4 in RL textbook (Agarwal et al., 2019), we have that


•  There exists a stationary and deterministic policy π such that Qπ

= Q∗tot.


•  A vector Q

tot

∈ R|S|×|A|    is equal to Q∗tot

if and only if it satisfies Q

tot

= .T IGMQΣ

tot.


•  ∀Q′tot  ∈ QIGM,

¨Q∗tot  − .T IGMQ′Σtot¨

= ¨.T IGMQ∗Σtot  − .T IGMQ′Σtot¨

(71)

≤γ ǁQ∗tot  − Q′totǁ∞ .                                         (72)

Thus, FQI-IGM will globally converge to optimal value function.

23


Under review as a conference paper at ICLR 2021

E    OMITTED  PROOFS  OF  APPENDIX  C.1

E.1    SOME NOTATIONS

In this section, we only consider the data distribution generated by the optimal joint policy π∗.

To simplify the notations, we use ε =    ˢ  to reformulate the exploratory policy generated by 
ϵ-greedy


exploration as follows

π˜(a|s) =

where εˆ = (|A| − 1)ε.

iY=1

|A|

.ε + (1 − εˆ)I

ai = arg max Q∗i (s, a′i)

a'i∈A

(73)

In addition, we use f (s, ·, ·) to denote the corresponding coefficient in the closed-form updating

(T LVDQ)tₒt(s, a) =   Σ  f (s, a, a′)(T Q)tₒt(s, a′)                                 (74)

a'∈An

where (   Q)tₒt = r(s, a′) + γVtₒt(s′) denote the precise target values derived by Bellman 
optimality
equation.

Formally, according to Eq. (8),


f (s, a, a′) =

in which

h⁽¹⁾(s, a, a′)
1 − εˆ

h⁽⁰⁾(s, a, a′)

+           ε           − (n − 1)

n

(1 − εˆ)

hπ∗ (s,a')

εn−hπ∗ (s,a')

,      (75)

hπ∗ (s, a) =        I[ai = πi∗(s)]                                                       (76)

i=1
n

h⁽¹⁾(s, a, a′) =        I[ai = πi∗(s)]I[ai = a′i]                                       (77)

i=1
n

h⁽⁰⁾(s, a, a′) =        I[ai /= πi∗(s)]I[ai = a′i]                                       (78)

i=1

As a reference indicating whether the learned value function produces the optimal policy, we denote

E(Q) = max Σ       max        (Qtₒt(s, π∗(s)) − Qtₒt(s, a))Σ                    (79)

Notice that π∗ denotes the optimal policy of the given MDP, so    (Q) might be negative for a
non-optimal or inaccurate value function Q.

E.2    OMITTED PROOFS

Lemma 5.  Given a dataset D generated by the optimal policy π∗ with ϵ-greedy exploration, for any
target value function Q,


we have

δ

∀δ > 0,  ∀0 < ε ≤  n2|A|n2n+1(Rmₐₓ + γǁVtₒtǁ

,                               (80)

)

∀s ∈ S,    (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))   ≤ δ,                          (81)

where (T Q)tₒt(s, a) = r(s, a) + γVtₒt(s′) denotes the regression target generated by Q.

24


Under review as a conference paper at ICLR 2021


Proof.  ∀s ∈ S,

 (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s)) 

≤  |(f (s, π∗(s), π∗(s)) − 1)(T Q)tₒt(s, π∗(s))| +         Σ

f (s, π∗(s), a′)(T Q)tₒt(s, a′) 


≤  |f (s, π∗(s), π∗(s)) − 1| +

a'∈An\{π∗(s)}

|f (s, π∗(s), a′)| ǁ(T Q)tₒtǁ∞.                      (82)

In the first term, ∀s ∈ S,

=   (n − (n − 1)(1 − εˆ))(1 − εˆ)ⁿ−¹ − 1


=   (1 + (n − 1)εˆ)

.nΣ−1 .n − 1Σ

(−1)AεˆAΣ − 1 


l

A=0

nΣ−1 .n − 1Σ             Σ


=   (1 + (n − 1)εˆ)

1 − (n − 1)εˆ+

A=2

(   1)AεˆA

l

− 1 


=   1 − (n − 1)²εˆ² + (1 + (n − 1)εˆ)

.nΣ−1 .n − 1Σ

(−1)AεˆAΣ

− 1 

l

A=2


=   εˆ² .(n − 1)² − (1 + (n − 1)εˆ)

nΣ−1 .n − 1Σ

(−1)AεˆA−²Σ 


2   2 .  2

nΣ−1 .n − 1ΣΣ

l

A=2


≤ |A|  ε      n   + 2

l

A=2

≤ |A|2ε² .n² + 2ⁿΣ

 	 


In the second term, ∀s ∈ S,

≤ ε  n  |A|  2  .                                                                                   
(83)


a'∈AnΣ\{π∗ (s)}

Σ

 

|f (s, π∗(s), a′)|

 . hπ∗ (s, a′)

Σ          hπ∗ (s,a')

n−hπ∗ (s,a') 


=

a'∈An\{π∗(s)}

≤  a'∈AnΣ\{π∗ (s)}

≤  a'∈AnΣ\{π∗ (s)}

n

.hπ∗ (s, a′) − (n − 1)(1 − εˆ)Σ (1 − εˆ)ʰπ∗ (s,a')−1εⁿ−ʰπ∗ (s,a')

2n(1 − εˆ)hπ∗ (s,a')−1εn−hπ∗ (s,a')

2nε


≤ 2nε|A|

.                                                                                                   
                    (84)

25


Under review as a conference paper at ICLR 2021

Thus ∀s ∈ S,

 (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s)) 


≤  |f (s, π∗(s), π∗(s)) − 1| +

a'∈An\{π∗(s)}

|f (s, π∗(s), a′)| ǁ(T Q)tₒtǁ∞

2    2        2   n                    n


≤ (ε  n  |A|  2   + 2nε|A|

)ǁ(T Q)tₒtǁ∞

2        n   n+1

≤ εn  |A|  2      ǁ(T Q)tₒtǁ∞

2        n   n+1

≤ εn  |A|  2      (Rmₐₓ + γǁVtₒtǁ∞)

≤ δ.                                                                                                
                                (85)

Lemma 6.  Given a dataset D generated by the optimal policy π∗ with ϵ-greedy exploration, for any
target value function Q,


we have

0 < ε                                  (1 − γ)E(Q∗)

γn3|A|n2n+4(Rmₐₓ/(1 − γ) + γǁV π∗  − V ∗ǁ∞)

,                        (86)


∀s ∈ S,   (T LVDQ)tₒt(s, π∗(s)) − V ∗(s)  ≤ γǁV π∗  − V ∗ǁ

+ 1 − γ E(Q∗),          (87)


D

where V π∗ (s) = Qtₒt(s, π∗(s)).
Proof.  ∀s ∈ S,

 (T LVDQ)tₒt(s, π∗(s)) − V ∗(s) 

tot

∞     8nγ

≤   (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))  + |(T Q)tₒt(s, π∗(s)) − V ∗(s)|

=   (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))  + |(T Q)tₒt(s, π∗(s)) − Q∗(s, π∗(s))|

=   (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))  + |(T Q)tₒt(s, π∗(s)) − (T Q∗)(s, π∗(s))|

≤   (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))  + γ|Vtₒt(s′) − V ∗(s′)|

≤   (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))  + γ|Qtₒt(s′, π∗(s′)) − V ∗(s′)|

≤   (T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))  + γǁV π∗  − V ∗ǁ∞                             (88)


Let δ =  ¹−γ E(Q∗). According to Lemma 5, with the condition

0 < ε ≤                           δ                           =          (1 − γ)E(Q∗)/(8nγ)

,          (89)


we have

n2|A|n2n+1(Rmₐₓ + γǁVtₒtǁ∞)

n2|A|n2n+1(Rmₐₓ + γǁVtₒtǁ∞)


D

Notice that

8nγ

ǁVtₒtǁ∞ ≤ ǁV ∗ǁ∞ + ǁVtₒt − V ∗ǁ∞                                   (91)

≤  Rmax   + ǁV π∗  − V ∗ǁ    .                                           (92)


The overall statement is

1 − γ

tot                    ∞


(1 − γ)E(Q∗)

γn3|A|n2n+4(Rmₐₓ/(1 − γ) + γǁV π∗  − V ∗ǁ∞)

≤           (1 − γ)E(Q∗)/(8nγ)

(93)

26


Under review as a conference paper at ICLR 2021

we have ∀s ∈ S,

LVD                       ∗                 ∗       

 (T        Q)tₒt(s, π  (s)) − V   (s)


D

≤ γǁV π∗  − V ∗ǁ

tot

+ 1 − γ E(Q∗).                                                                     (94)

Lemma 7.  For any value function Q, the corresponding sub-optimality gap satisfies

E(T Q) ≥ E(Q∗) − 2γǁVtₒt − V ∗ǁ∞                                  (95)

Proof.  With a slight abuse of notation, let s₁ and s₂ denote the next states while taking actions 
π∗(s)
and a at the state s, respectively. According to the definition,


(    Q) =                   max

(An\{π∗(s)})

≥ (s,a)∈S×max   π   (s)})

≥ (s,a)∈S×max   π   (s)})

=                   max

(s,a)∈S×(An\{π∗(s)})

((T Q)tot(s, π∗(s)) − (T Q)tot(s, a))

,(T Q∗)(s, π∗(s)) − (T Q∗)(s, a) − γ ,|Vtot(s1 ) − V ∗(s1 )| + |Vtot(s2 ) − V ∗(s2 )|ΣΣ

,(T Q∗)(s, π∗(s)) − (T Q∗)(s, a) − 2γǁVtot  − V ∗ǁ∞Σ

,Q∗(s, π∗(s)) − Q∗(s, a) − 2γǁVtot  − V ∗ǁ∞Σ

= E(Q∗) − 2γǁVtot  − V ∗ǁ∞                                                                          
                                                                                          (96)

Lemma 8.  Given a dataset D generated by the optimal policy π∗ with ϵ-greedy exploration, for any
target value function Q,


δ

∀δ > 0,  ∀0 < ε ≤  n2|A|n2n(Rmₐₓ/(1 − γ) + γǁVtₒt − V ∗ǁ

,                     (97)

)

we have ∀s ∈ S, ∀a ∈ An \ {π∗(s)},

(T LVDQ)tₒt(s, a) ≤ (T Q)tₒt(s, π∗(s)) − E(Q∗) + 2nγǁVtₒt − V ∗ǁ∞ + δ              (98)

where (T Q)tₒt(s, a) = r(s, a) + γVtₒt(s′) denotes the regression target generated by Q.

Proof.  ∀s ∈ S, ∀a ∈ An \ {π∗(s)},

(T LVDQ)tₒt(s, a) =    Σ  f (s, a, a′)(T Q)tₒt(s, a′)

a'∈An

= f (s, a, π∗(s))(T Q)tₒt(s, π∗(s))


+

a'∈An:hπ∗ (s,a')=n−1

+

a'∈An:hπ∗ (s,a')<n−1

f (s, a, a′)(T Q)tₒt(s, a′)

f (s, a, a′)(T Q)tₒt(s, a′)                   (99)

27


Under review as a conference paper at ICLR 2021

In the first term,

f (s, a, π∗(s))(T Q)tot(s, π∗(s))

. hπ∗ (s, a)                     Σ             n                            ∗


=

1 − εˆ

− (n − 1)

(1 − εˆ)   (T Q)tot(s, π  (s))

=  ,hπ∗ (s, a) − (n − 1)(1 − εˆ), (1 − εˆ)n−1 (T Q)tot(s, π∗(s))

=  ,hπ∗ (s, a) − (n − 1) + (n − 1)(|A| − 1)ε, (1 − εˆ)n−1 (T Q)tot(s, π∗(s))

≤ ,hπ∗ (s, a) − (n − 1), (1 − εˆ)n−1 (T Q)tot(s, π∗(s)) + εn|A|ǁ(T Q)totǁ∞

=  ,hπ∗ (s, a) − (n − 1), (1 + (1 − εˆ)n−1 − 1)(T Q)tot(s, π∗(s)) + εn|A|ǁ(T Q)totǁ∞


,     ∗                        ,

 nΣ−1 ,n − 1,


≤    hπ  (s, a) − (n − 1)

(T Q)tot(s, π∗(s)) + 2n

l=1

(−1)lεˆl  ǁ(T Q)totǁ∞ + εn|A|ǁ(T Q)totǁ∞


,     ∗                        ,

.nΣ−1 ,n − 1,Σ

	

≤ ,hπ∗ (s, a) − (n − 1), (T Q)tot(s, π∗(s)) + εˆn2nǁ(T Q)totǁ∞ + εn|A|ǁ(T Q)totǁ∞

≤ ,hπ∗ (s, a) − (n − 1), (T Q)tot(s, π∗(s)) + εn2n|A|ǁ(T Q)totǁ∞ + εn|A|ǁ(T Q)totǁ∞                 
                                      (100)


In the second term,

a'       n:hπΣ∗ (s,a')=n    1

f (s, a, a')(T Q)tot(s, a')


Σ                 . h⁽¹⁾(s, a, a')

h⁽⁰⁾(s, a, a')

Σ             n−1                         '


=

a'∈An:hπ∗ (s,a')=n−1

+

1 − εˆ

ε           − (n − 1)

(1 − εˆ)        ε(T Q)tot(s, a )


=

a'       n:hπ∗ (s,a')=n    1

.h(0)

(s, a, a')(1 − εˆ)

ⁿ−¹(T Q)tot(s, a') +

h⁽¹⁾(s, a, a')
1 − εˆ

− (n − 1)Σ

(1 − εˆ)

n−1

ε(T Q)tot(s, a')Σ


Σ                 .  (0)             '

n−1

'        h⁽¹⁾(s, a, a')

n−1                         Σ


≤                                      h

a'∈An:hπ∗ (s,a')=n−1

(s, a, a )(1 − εˆ)

(T Q)tot(s, a ) +

1 − εˆ

− (n − 1)  (1 − εˆ)

εǁ(T Q)totǁ∞


≤ a'       n:hπΣ∗ (s,a')=n    1

,h(0)(s, a, a')(1 − εˆ)n−1 (T Q)tot(s, a') + 2nεǁ(T Q)totǁ∞,


=                    Σ                 .

h⁽⁰⁾(s, a, a')

.nΣ−1 ,n − 1,

(−1)ˡεˆˡΣ

(T Q)tot(s, a') + 2nεǁ(T Q)totǁ∞Σ


a'∈An:hπ∗ (s,a')=n−1

=                    Σ

.h⁽⁰⁾(s, a, a')

l=0

.1 +

A

nΣ−1 ,n − 1,

(−1)ˡεˆˡΣ

(T Q)tot(s, a') + 2nεǁ(T Q)totǁ∞Σ


a'∈An:hπ∗ (s,a')=n−1

A

l=1


≤               Σ                 .

n−1

h(0)(s, a, a')(T Q)tot(s, a') +  

(−1)lεˆl  ǁ(T Q)totǁ∞ + 2nεǁ(T Q)totǁ∞Σ


a'∈An:hπ∗ (s,a')=n−1

  l=1        A


=                    Σ                 .

n−1

h(0)(s, a, a')(T Q)tot(s, a') + εˆ 

(−1)lεˆl−1   ǁ(T Q)totǁ∞ + 2nεǁ(T Q)totǁ∞Σ


a'∈An:hπ∗ (s,a')=n−1

  l=1        A


≤               Σ                 .

h(0)(s, a, a')(T Q)tot(s, a') + εˆ

.nΣ−1 ,n − 1,Σ

ǁ(T Q)totǁ∞ + 2nεǁ(T Q)totǁ∞Σ


a'∈An:hπ∗ (s,a')=n−1

A

l=1


≤ a'       n:hπΣ∗ (s,a')=n    1

,h(0)(s, a, a')(T Q)tot(s, a') + ε|A|2n−1 ǁ(T Q)totǁ∞ + 2nεǁ(T Q)totǁ∞,


=  

≤ 

a'       n:hπΣ∗ (s,a')=n    1

Σ

h⁽⁰⁾(s, a, a')(T Q)

h⁽⁰⁾(s, a, a')(T Q)

tot

(s, a') + εn|A|2n−1 ǁ(T Q)

(s, a') + εn²|A|2ⁿǁ(T Q)

totǁ∞

ǁ

+ 2n²εǁ(T Q)

totǁ∞

(101)

28


Under review as a conference paper at ICLR 2021


In the third term,

a'       n:hπΣ∗ (s,a')<n    1

≤ a'       n:hπΣ∗ (s,a')<n    1

f (s, a, a')(T Q)tot(s, a')

 f (s, a, a )(T Q)tot(s, a ) 


h⁽¹⁾(s, a, a')

h⁽⁰⁾(s, a, a')

hπ∗ (s,a')  n−hπ∗ (s,a')


=                                                                            +

1 − εˆ

ε           − (n − 1)  (1 − εˆ)                   ε

(T Q)tot(s, a )


h⁽¹⁾(s, a, a')

h⁽⁰⁾(s, a, a')

hπ∗ (s,a')  n−hπ∗ (s,a')


≤                                           1 − εˆ         +

ε           + (n − 1)  (1 − εˆ)                   ε

(T Q)tot(s, a )


≤ a'       n:hπΣ∗ (s,a')<n    1

n   1 +      1

1 − εˆ

+  1 Σ (1 − εˆ)hπ∗ (s,a')εn−hπ∗ (s,a')  (T Q)

tot

(s, a ) 


≤ a'       n:hπΣ∗ (s,a')<n    1

n .1 +  2 Σ (1 − εˆ)hπ∗ (s,a')εn−hπ∗ (s,a')  (T Q)

tot

(s, a ) 


≤ a'       n:hπΣ∗ (s,a')<n    1

≤ a'       n:hπΣ∗ (s,a')<n    1

n−hπ∗ (s,a')−1

3nεǁ(T Q)totǁ∞

tot

(s, a ) 

≤ 3nε|A|  ǁ(T Q)totǁ∞                                                                               
                                                                                                    
        (102)

Combining the above terms, we can get


(T LVDQ)tot(s, a)

= f (s, a, π∗(s))(T Q)tot(s, π∗(s)) +

a'∈An:hπ∗ (s,a')=n−1

f (s, a, a')(T Q)tot(s, a')


+

a'∈An:hπ∗ (s,a')<n−1

f (s, a, a')(T Q)tot(s, a')

≤ ,hπ∗ (s, a) − (n − 1), (T Q)tot(s, π∗(s)) + εn2n|A|ǁ(T Q)totǁ∞ + εn|A|ǁ(T Q)totǁ∞


+ 

a'       n:hπΣ∗ (s,a')=n    1

h⁽⁰⁾(s, a, a')(T Q)

tot

(s, a') + εn2 |A|2nǁ(T Q)

totǁ∞

+ 3nε|A|nǁ(T Q)

totǁ∞


≤ ,hπ∗ (s, a) − (n − 1), (T Q)

tot

(s, π∗(s)) + 

a'       n:hπΣ∗ (s,a')=n    1

h⁽⁰⁾(s, a, a')(T Q)

tot

(s, a')

+ εn2 |A|ⁿ2nǁ(T Q)totǁ∞                                                                             
                                                                                           (103)


in which

a'∈An:hπΣ∗ (s,a')=n−1

≤             Σ∗

h⁽⁰⁾(s, a, a′)(T Q)tₒt(s, a′)
h⁽⁰⁾(s, a, a′)                 max

a'∈An:hπ∗ (s,a')=n−1

(T Q)tₒt(s, a′)


a'∈An:hπ   (s,a')=n−1

= (n     hπ∗ (s, a))              max

a'∈An:hπ∗ (s,a')=n−1

(T Q)tₒt(s, a′)


π∗

∈An\{π∗(s)

(T Q)tₒt(s, a′)

= (n − hπ∗ (s, a)) ((T Q)tₒt(s, π∗) − E(T Q))                                                       
    (104)

29


Under review as a conference paper at ICLR 2021

Thus ∀s ∈ S, ∀a ∈ An \ {π∗(s)},

(T LVDQ)tot(s, a)


≤ ,hπ∗ (s, a) − (n − 1), (T Q)

+ εn2 |A|ⁿ2nǁ(T Q)totǁ∞

tot

(s, π∗(s)) + 

a'       n:hπΣ∗ (s,a')=n    1

h⁽⁰⁾(s, a, a')(T Q)

tot

(s, a')

≤ ,hπ∗ (s, a) − (n − 1), (T Q)tot(s, π∗(s)) + (n − hπ∗ (s, a))   (T Q)tot(s, π∗) − E(T Q)   + εn2 
|A|ⁿ2nǁ(T Q)totǁ∞

= (T Q)tot(s, π∗(s)) − (n − hπ∗ (s, a))E(T Q) + εn2 |A|ⁿ2nǁ(T Q)totǁ∞                               
                                                       (105)

According to Lemma 7, E(T Q) ≥ E(Q∗) − 2γǁVtₒt − V ∗ǁ∞. So ∀s ∈ S, ∀a ∈ An \ {π∗(s)},

(T LVDQ)tot(s, a) ≤ (T Q)tot(s, π∗(s)) − (n − hπ∗ (s, a))E(T Q) + εn2 |A|ⁿ2nǁ(T Q)totǁ∞

≤ (T Q)tot(s, π∗(s)) − (n − hπ∗ (s, a))   E(Q∗) − 2γǁVtot  − V ∗ǁ∞    + εn2 |A|ⁿ2nǁ(T Q)totǁ∞

≤ (T Q)tot(s, π∗(s)) − E(Q∗) + 2nγǁVtot  − V ∗ǁ∞ + εn2 |A|ⁿ2nǁ(T Q)totǁ∞

≤ (T Q)tot(s, π∗(s)) − E(Q∗) + 2nγǁVtot  − V ∗ǁ∞ + εn2 |A|ⁿ2n(Rmax  + γǁVtotǁ∞)

≤ (T Q)tot(s, π∗(s)) − E(Q∗) + 2nγǁVtot  − V ∗ǁ∞ + εn2 |A|ⁿ2n(Rmax  + γǁV ∗ǁ∞ + γǁVtot  − V ∗ǁ∞)

≤ (T Q)tot(s, π∗(s)) − E(Q∗) + 2nγǁVtot  − V ∗ǁ∞ + εn2 |A|ⁿ2n(Rmax /(1 − γ) + γǁVtot  − V ∗ǁ∞)

≤ (T Q)tot(s, π∗(s)) − E(Q∗) + 2nγǁVtot  − V ∗ǁ∞ + δ                                                
                    (106)


Lemma 9.  Let B denote a subspace of value functions

B = .Q ∈ QLVD  E(Q) ≥ 0,  ǁVtₒt − V ∗ǁ

≤    1   E(Q∗) Σ                  (107)

∞     8nγ


Given a dataset D generated by the optimal policy π∗ with ϵ-greedy exploration,

0 < ε                             (1 − γ)E(Q∗)

n3|A|n2n+4(Rmₐₓ/(1 − γ) + E(Q∗)/(8n))

we have ∀Q ∈ B, T LVDQ ∈ Bˆ ⊂ B where

(108)


Bˆ = .Q ∈ QLVD  E(Q) > 0,  ǁVtₒt − V ∗ǁ

≤    1   E(Q∗) Σ                  (109)


Proof.  According to Lemma 5, with the condition

∞     8nγ


0 < ε                                  E(Q∗)/4

n2|A|n2n+1(Rmₐₓ/(1 − γ) + E(Q∗)/(8n))

we have ∀Q ∈ B, ∀s ∈ S,

E(Q∗)/4

n2|A|n2n+1(Rmₐₓ + γǁVtₒtǁ∞)

(110)


(T LVDQ)tₒt(s, π∗(s)) − (T Q)tₒt(s, π∗(s))

which implies ∀Q ∈ B, ∀s ∈ S,

1

≤  4 E

(Q∗)                           (111)


(T LVDQ)tₒt(s, π∗(s)) ≥ (T Q)tₒt(s, π∗(s))

According to Lemma 8, with the condition

0 < ε ≤                               E(Q∗)/4

1

−  4 E

(Q∗).                          (112)


n2|A|n2n(Rmₐₓ/(1 − γ) + E(Q∗)/(8n))

E(Q∗)/4

n2|A|n2n(Rmₐₓ/(1 − γ) + γǁVtₒt − V ∗ǁ∞)

(113)

30


Under review as a conference paper at ICLR 2021


we have ∀Q ∈ B, ∀s ∈ S, ∀a ∈ An \ {π∗(s)},

(T LVDQ)tₒt(s, a) ≤ (T Q)tₒt(s, π∗(s)) − E(Q∗) + 2nγǁVtₒt − V ∗ǁ∞

+ 1   (Q∗)

4

≤ (T Q)tₒt(s, π∗(s)) − E(Q∗) + 1 E(Q∗) + 1 E(Q∗)


= (T Q)tₒt(s, π∗(s))

1

−  2 E

4                 4

(Q∗)

< (T LVDQ)tₒt(s, π∗(s))                                                                       (114)


which implies E(T LVDQ) > 0.

According to Lemma 6, with the condition

0 < ε                               (1 − γ)E(Q∗)

γn3|A|n2n+4(Rmₐₓ/(1 − γ) + E(Q∗)/(8n))

(1 − γ)E(Q∗)

γn3|A|n2n+4(Rmₐₓ/(1 − γ) + γǁV π∗  − V ∗ǁ∞)

,                       (115)

we have ∀Q ∈ B, ∀s ∈ S,


D                                                D

≤ γǁV π∗  − V ∗ǁ

+ 1 − γ E(Q∗) ≤    1   E(Q∗).           (117)


Combing Eq. (110), (113), and (115), the overall condition is

0 < ε                             (1 − γ)E(Q∗)

n3|A|n2n+4(Rmₐₓ/(1 − γ) + E(Q∗)/(8n))

(118)

Lemma 3.  There exists a threshold δ > 0 such that the on-policy Bellman operator T LVD  is closed


in the following subspace B ⊂ QLVD, when the hyper-parameter ϵ

s

is sufficiently small.

=    Q        LVD    πQ = π∗,  max  Qtₒt(s, π∗(s))     V ∗(s)       δ

s∈S

Formally, ∃δ > 0, ∃ϵ > 0, ∀Q ∈ B, there must be T LVDQ ∈ B.

Proof.  It is implied by Lemma 9.

Theorem 4 (Formal version of Theorem 2).  Besides Lemma 3, Algorithm 1 will have a fixed point
value function expressing the optimal policy if the hyper-parameter ϵ is sufficiently small.

Proof.  Notice that the state value function is sufficient to determine the target values, so the 
subspace
defined in Lemma 9 is a compact and convex space in terms of Vtₒt.  The operator     LVD  is a
continuous mapping because it only involves elementary functions. According to Brouwer’s Fixed
Point Theorem (Brouwer, 1911), there exist Q         satisfying    LVDQ        . In addition, 
according to
the definition stated in Eq. (109), the fixed point must represent the unique optimal policy since 
it

cannot lie on the boundary with E(Q) = 0.

F    EXPERIMENT  SETTINGS  AND  IMPLEMENTATION  DETAILS

F.1    IMPLEMENTATION DETAILS

We adopt the PyMARL (Samvelyan et al., 2019) implementation with default hyper-parameters to
investigate state-of-the-art multi-agent Q-learning algorithms: VDN (Sunehag et al., 2018), QMIX
(Rashid et al., 2018), QTRAN (Son et al., 2019), and QPLEX (Wang et al., 2020a).  The training
time of these algorithms on an NVIDIA RTX 2080TI GPU is about 4 hours to 12 hours, which

31


Under review as a conference paper at ICLR 2021

Map Name       Replay Buffer Size     Behaviour Test Win Rate     Behaviour Policy
2s3z                 20k episodes                         91.2%                             VDN

3s5z                 20k episodes                         77.5%                             VDN

2s_vs_1sc            20k episodes                         99.6%                             VDN

3s_vs_5z             20k episodes                         94.2%                             VDN

1c3s5z               30k episodes                         92.1%                             VDN

3c7z                 30k episodes                         94.4%                             VDN

5m_vs_6m            50k episodes                         61.7%                             VDN

10m_vs_11m          50k episodes                         88.7%                             VDN

3h_vs_4z             50k episodes                         83.1%                             VDN

Table 2: The dataset configurations of offline data collection setting.

is depended on the number of agents and the episode length limit of each map.  The performance
measure of StarCraft II tasks is the percentage of episodes in which RL agents defeat all enemy 
units
within the limited time constraints, called test win rate. The dataset providing off-policy 
exploration
is constructed by training a behavior policy of VDN and collecting its 20k, 30k or 50k experienced
episodes. The dataset configurations are shown in Table 2. We investigate five multi-agent 
Q-learning
algorithms over 6 random seeds, which includes 3 different datasets and evaluates two seeds on
each dataset. We train 300 epochs to evaluate the learning performance with a given static dataset,
of which 32 episodes are trained in each update, and 160k transitions are trained for each epoch
totally. Moreover, the training process of behavior policy is the same as that discussed in PyMARL
(Samvelyan et al., 2019), which has collected a total of 2 million timestep data and anneals the
hyper-parameter ϵ of ϵ-greedy exploration strategy linearly from 1.0 to 0.05 over 50k timesteps. The
target  network will be updated periodically after training every 200 episodes. We call this period 
of
200 episodes an Iteration, which corresponds to an iteration of FQI-LVD (see Definition 1).

F.2    TWO-STATE MMDP

In the two-state MMDP shown in Figure 1a, due to the GRU-based implementation of the finite-
horizon paradigm in the above five deep multi-agent Q-learning algorithms, we assume that two agents
starting from state s₂  have 100 environmental steps executed by a uniform ϵ-greedy exploration
strategy (i.e., ϵ  =  1).  We use this long-term horizon pattern and uniform ϵ-greedy exploration
methods to approximate an infinite-horizon MMDP paradigm with uniform data distribution. We
adopt γ  = 0.9 to implement FQI-LVD and deep MARL algorithms.  In the FQI-LVD framework,
Vmₐₓ =    ¹    = 100 as shown in Figure 1b. Figure 1c demonstrates that Optimal line is 
approximately

1−γ


99

i=0

γⁱ = 63.4 in one episode of 100 timesteps.

F.3    STARCRAFT II

StarCraft II unit micromanagement tasks consider a combat game of two groups of agents, where
StarCraft II takes built-in AI to control enemy units, and MARL algorithms can control each ally 
unit
to fight the enemies. Units in two groups can contain different types of soldiers, but these 
soldiers
in the same group should belong to the same race. The action space of each agent includes no-op,
move [direction], attack [enemy id], and stop. At each timestep, agents choose to move or attack in
continuous maps.  MARL agents will get a global reward equal to the amount of damage done to
enemy units. Moreover, killing one enemy unit and winning the combat will bring additional bonuses
of 10 and 200, respectively. The maps of SMAC challenges in this paper are introduced in Table 3 in
the episodes of 100 timesteps.

32


Under review as a conference paper at ICLR 2021

Map Name                          Ally Units                                         Enemy Units
2s3z                      2 Stalkers & 3 Zealots                        2 Stalkers & 3 Zealots

3s5z                      3 Stalkers & 5 Zealots                        3 Stalkers & 5 Zealots

2s_vs_1sc                           2 Stalkers                                       1 Spine 
Crawler

3s_vs_5z                            3 Stalkers                                             5 
Zealots

1c3s5z          1 Colossus, 3 Stalkers & 5 Zealots     1 Colossus, 3 Stalkers & 5 Zealots

3c7z                      3 Colossi & 7 Zealots                         3 Colossi & 7 Zealots

5m_vs_6m                          5 Marines                                            6 Marines

10m_vs_11m                       10 Marines                                          11 Marines

3h_vs_4z                         3 Hydralisks                                          4 Zealots

Table 3: SMAC challenges.

G    DEFERRED  TABLES  AND  FIGURES  IN  SECTION  6

G.1    THE LEARNING CURVE OF TABLE 1C

1.0%

0.8%

0.6%

0.4%

0.2%


0.0%

VDN

0            200         400         600         800       1000

Iteration t

Figure 3: The learning curve of Table 1c. Every iteration contains 200 gradient steps. The relative
error is defined as ǁQFQI⁻LVD − QVDNǁ∞/ǁQFQI⁻LVDǁ∞.

G.2    DEFERRED TABLES IN SECTION 6.1


1               (1)

a₂

A(2)

A(3)

1               (1)

a₂

A(2)

A(3)

1             (1)

a₂

A(2)

A(3)


A(1)

A(2)

A(3)

7.98

-12.18

-12.11

-12.09

-0.02

-0.03

-12.10

-0.02

-0.03

A(1)

A(2)

A(3)

8.00

-12.00

-12.00

-12.00

-0.00

0.00

-12.00

0.00

0.00

A(1)

A(2)

A(3)

-7.98

-7.98

-7.98

-7.98

-0.00

-0.00

-7.98

-0.00

-0.00


(a) Qtot  of QPLEX

(b) Qtot  of QTRAN

(c) Qtot  of QMIX

Table 4: (a-c) Joint action-value functions Qtₒt of QPLEX, QTRAN, and QMIX. Boldface means the
greedy joint action selection from Qtₒt.

33


Under review as a conference paper at ICLR 2021

H    ABLATION  STUDIES  ON  NETWORK  CAPACITY

H.1    ABLATION STUDIES IN MATRIX GAME


1             (1)

a₂

A(2)

A(3)

1               (1)

a₂

A(2)

A(3)

1               (1)

a₂

A(2)

A(3)


A(1)

A(2)

A(3)

8

-12

-12

-12

0

0

-12

0

0

A(1)

A(2)

A(3)

7.98

-12.18

-12.11

-12.09

-0.02

-0.03

-12.10

-0.02

-0.03

A(1)

A(2)

A(3)

8.00

-12.00

-12.00

-12.00

-0.00

0.00

-12.00

0.00

0.00


(a) Payoff of matrix game

(b) Qtot  of QPLEX

(c) Qtot  of QTRAN


1             (1)

a₂

A(2)

A(3)

1             (1)

a₂

A(2)

A(3)


A(1)

A(2)

A(3)

-6.24

-4.90

-4.90

-4.90

-3.57

-3.57

-4.90

-3.57

-3.57

A(1)

A(2)

A(3)

-8.03

-8.03

-8.03

-8.03

-0.01

-0.01

-8.03

-0.01

-0.01

(d) Qtot  of Large-VDN                    (e) Qtot  of Large-QMIX

Table 5: (a-c) The ground-truth payoff matrix and the joint action-value functions of QPLEX and
QTRAN. (d-e) The joint action-value functions Qtₒt of Large-VDN and Large-QMIX. Boldface
means the greedy joint action selection from Qtₒt.

To address the concern that QPLEX naturally uses more hidden parameters than VDN and QMIX,
which may also improve its representational capacity.  To demonstrate that the performance gap
between QPLEX and other methods does not come from the difference in term of the number of
parameters, we increase the number of neurons in VDN and QMIX so that they have comparable
number of parameters as QPLEX. Formally, Large-VDN and Large-QMIX have similar number of
parameters as QPLEX. The experiment results are presented in Table 5, both the “Large-” versions
of VDN and QMIX cannot represent an accurate value function in this matrix game. Increasing the
number of parameters cannot address the limitations of VDN and QMIX on representational capacity.

H.2    ABLATION STUDIES IN STARCRAFT II BENCHMARK TASKS


100

80

60

         

QPLEX

QTRAN
VDN

100

80

60

100

80

60


40               Large-VDN

             Behavior

20

0

0           60        120       180       240       300

Epochs

(a) 3s_vs_5z

40

20

0

0          60        120       180       240       300

Epochs

(b) 2s3z

40

20

0

0           60        120       180       240       300

Epochs

(c) 2s_vs_1sc


100

100

100

80                                                                                                  
 80                                                                                                 
  80

60                                                                                                  
 60                                                                                                 
  60

40                                                                                                  
 40                                                                                                 
  40

20                                                                                                  
 20                                                                                                 
  20


0

0           60        120       180       240       300

Epochs

(d) 1c3s5z

0

0           60        120       180       240       300

Epochs

(e) 3s5z

0

0           60        120       180       240       300

Epochs

(f) 5m_vs_6m

Figure 4: Evaluating the performance of Large-VDN with a given static dataset.

In addition to the ablation study in the matrix game, Figure 4 and Figure 5 present the ablation
studies in StarCraft II benchmark tasks with offline data collection. In comparison to the standard
versions of VDN and QMIX, we introduce Large-VDN and Large-QMIX which have similar number
of parameters as QPLEX. As shown in Figure 4, increasing parameters can benefit VDN in some

34


Under review as a conference paper at ICLR 2021


100

80

60

         

QPLEX

QTRAN
QMIX

100

80

60

100

80

60


40               Large-QMIX

             Behavior

20

0

0           60        120       180       240       300

Epochs

(a) 3s_vs_5z

40

20

0

0          60        120       180       240       300

Epochs

(b) 2s3z

40

20

0

0           60        120       180       240       300

Epochs

(c) 2s_vs_1sc


100

100

100

80                                                                                                  
 80                                                                                                 
  80

60                                                                                                  
 60                                                                                                 
  60

40                                                                                                  
 40                                                                                                 
  40

20                                                                                                  
 20                                                                                                 
  20


0

0           60        120       180       240       300

Epochs

(d) 1c3s5z

0

0           60        120       180       240       300

Epochs

(e) 3s5z

0

0           60        120       180       240       300

Epochs

(f) 5m_vs_6m

Figure 5: Evaluating the performance of Large-QMIX with a given static dataset.

easy maps such as 2s3z and 2s_vs_1sc, but it cannot provide fundamental improvement in harder
tasks. As shown in Figure 5, the effects of increasing parameters are rather weak for QMIX. These
experiments demonstrate that increasing the number of parameters cannot address the limitations of
VDN   and QMIX on representational capacity.

I    ADDITIONAL  EXPERIMENTS  ON  MMDP EXAMPLE


500

400

300

200

=   1.0

=   0.8

=   0.5

             =   0.2

=   0.1

        

=   0.

500

400

300

200

       

=   1.0

=   0.8

=   0.5

=   0.2

=   0.1

=   0.


100

            Optimal

100

            Optimal


0

500

400

0          20         40         60         80        100

Iteration t

(a) VDN

=   1.0

=   0.8

=   0.5

0

500

400

0          20         40         60         80        100

Iteration t

(b) QMIX

=   1.0

=   0.8

=   0.5


300

200

             =   0.2

=   0.1

        

=   0.

300

200

       

=   0.2

=   0.1

=   0.


100

            Optimal

100

            Optimal


0

0          20         40         60         80        100

Iteration t

(c) QPLEX

0

0          20         40         60         80        100

Iteration t

(d) QTRAN

Figure 6: The learning curves of   Qtₒt       while running several deep multi-agent Q-learning 
algo-
rithms with an unfactorizable dataset.

Remark    Assumption  1  on  the  factorizable  dataset  does  not  require  the  factorizability  
of  the
underlying transition and reward functions or the decomposability of the joint action-value 
function.
On the contrary, all our theorems and examples focus on the situations where the joint Q-function

35


Under review as a conference paper at ICLR 2021

cannot be perfectly factorized. In addition, Assumption 1 can be naturally satisfied when the 
dataset
is collected by decentralized execution of agents’ policies, e.g., an on-policy dataset collected 
using
ϵ-greedy exploration policies or an offline dataset collected by given decentralized policies of 
agents.
All algorithms discussed in paper, including VDN, QMIX, QTRAN, and QPLEX, learn decentralized
policies, which are executed in a decentralized manner. The theoretical implications derived in this
paper are applicable whenever such factorizable data collection procedures are carried out.

To investigate the dependency of our theoretical implications on Assumption 1,  we provide an
experiment to evaluate the performance of deep multi-agent Q-learning algortihms on unfactorizable
datasets. Figure 6 present the learning curves of VDN, QMIX, QPLEX, and QTRAN in the example
MMDP shown in Figure 1a with an unfactorizable dataset D constructed by a parameter η as follows:

₍₁₎     ₂                  0.5η + 0.25(1 − η)          0.25(1 − η)

−                                           −

As shown in Figure 6, the choice of parameter η has no impacts on the performance of QPLEX and
QTRAN, which matches the fact that Theorem 3 does not rely on the assumption of factorizable
dataset.  As the extension of Proposition 2, VDN and QMIX empirically suffer from unbounded
divergence when the dataset is not factorizable. The only exception is the case of η = 1, in which
the dataset only contains two kinds of joint actions. In this case, the given MMDP degenerates to a
single-agent MDP because agents only perform the same actions in the dataset. As a result, VDN and
QMIX would not diverge in this special situation.

36

