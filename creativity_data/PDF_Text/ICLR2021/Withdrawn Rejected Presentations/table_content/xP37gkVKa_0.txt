Table 1: Average scores in 2-player Hanabi with different search variants. Time column shows the averagewall-clock time of each method to play a game. Columns of RL-kH show performance of different methodsapplied on blueprint policies trained for different amounts of time. Each cell contains the mean averaged over5000 games. Please refer to Appendix D for the complete version of the table that includes the standard errorof mean, percentage of perfect games, and more test scenarios. LBS variants achieve a substantial fraction ofthe policy improvements of SPARTA over the blueprint at a lower computational cost.
Table 2: Result on 6-card Hanabi variant. Each cell contains the mean and standard error of mean over 5000games in the first row and percentage of perfect games (25 points) in the second row.
Table 3: Hyper-Parameters for Reinforcement Learningɑ1+β* N-1 where α = 0.1, β = 7 and N = 80. The rest of the hyper-parameters can be found inTable 3.
Table 4: Learned Belief Search Result. RL-kH means blueprint policy after k hours of training with RL.
