Table 1: Negative average test log-likelihood in nats (smaller is better)on binarized MNIST.
Table 2: Average test log-likelihood in nats (higher is better) for benchmark datasets. Entries marked with* evaluate standard deviation across 3 independent runs of the algorithm; all others are mean ± standard erroracross samples in the test dataset. TraDE achieves significantly better log-likelihood than other algorithms on alldatasets except MINIBOONE.
Table 4: Mean squared error ofregression on HEPMASS.
Table 5: Average precision for out-of-distribution detection The results forNADE, NICE and TAN were (carefully) eye-balled from the plots of (Oliva et al., 2018).
Table 6: Average test log-likelihood (nats) for HEP-MASS dataset with andwithout additive noise in thetraining data.
Table S1: Test likelihood (higher is better) using ONLY the MMD objective for training vs. TraDE.		Dataset	Only MMD	TraDEPOWER	-5.00 ± 0.24	0.73 ± 0.00GAS	-10.85 ± 0.16	13.27 ± 0.01HEPMASS	-28.18 ± 0.12	-12.01 ± 0.03MINIBOONE	-68.06	-9.49 ± 0.13BSDS300	70.70 ± 0.32	160.01 ± 0.02B Density estimation with few dataMMD regularization especially helps when we have few training data. Test likelihoods (higher isbetter) after training on a sub-sampled MINIBOONE dataset are: -55.62 vs. -55.53 (100 samples)and -49.07 vs. -48.90 (500 samples) for TraDE without and with MMD-penalization, respectively.
Table S2: Hyper-parameters for benchmark datasets.
Table S3: Dataset information. In order to ensure that results are comparable and same setups and datasplits are used as previous works, we closely follow the experimental setup of (Papamakarios et al., 2017) fortraining/validation/test dataset splits. In particular, the preprocessing of all the datasets is kept the same as thatof (Papamakarios et al., 2017). Note that, these setups were used on all other baselines as well.
