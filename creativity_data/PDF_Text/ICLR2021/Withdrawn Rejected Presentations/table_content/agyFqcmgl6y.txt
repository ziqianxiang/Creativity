Table 1: Sample efficiency and test accuracy with different training sample sizes. DEAR-lin and -nlr denotethe model with linear and nonlinear f. Line 1 is unsupervised; 2-3 are semi-supervised; others are supervised.
Table 2: Distributional robustness. The worst-case and average test accuracy(a) CelebA	(b) PendulumMethod	WorstAcc(%)	AvgAcc(%)	WorstAcc(%)	AvgAcc(%)ResNet	59.12±i.78	82.12±o.26	60.48±2.73	87.40±o.89DEAR-Iin-10%	-71.40±o.47-	^^81.04±o.i4	63.93±i.33-	^^89.70±o.63DEAR-nlr-10%	70.44±i.02	81.94±o.3i	65.59±i.90	90.19±0.63ReSNet-multi	~^59.17±4.o2^^	^^82.05±o.25	61.70±4.o2^^	^^87.20±i.ooS-VAE	60.54±3.48	79.51±o.58	20.78±4.45	84.26±ι.3iS-β-VAE	63.85±2.09	80.82±o.i9	44.12±9.73	86.99±i.78S-TC-VAE	64.93±3.30	81.58±o.i4	35.50±5.57	86.64±ι.i5DEAR-lin	76.05±o.70	83.56±o.09	74∙95±i.26	93.61±o.i3DEAR-nlr	71.37±o.66	83.81±o.08	72.48±o.74	93.11±o.i45.2.2	Distributional robustnessWe manipulate the training data to inject spurious correlations between the target label and somespurious attributes. On CelebA, We regard mouthqpen as the spurious factor; on Pendulum, Wechoose background^color ∈ {blue(+), white(-)}. We manipulate the training data such that thetarget label is more strongly correlated with the spurious attributes, i.e., the target label and thespurious attribute of 80% of the examples are both positive or negative, while those of 20% examplesare opposite. For example, in the manipulated training set, 80% smiling examples in CelebA havean open mouth; 80% corrupted examples in Pendulum are masked with a blue background. The test
Table 3: SAGAN architecture (k = 100 and Ch = 32).
