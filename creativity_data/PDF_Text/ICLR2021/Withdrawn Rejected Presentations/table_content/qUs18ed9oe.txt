Table 1: We report the fraction of unsafe behavior policies encountered during training across differ-ent OpenAI safety environments for the policy updates across 2e7 training timesteps. LBPO obtainsfewer constraint violations consistently across all environments.
Table 2: Cumulative return of the converged policy for each safety algorithm normalized by PPOâ€™sreturn. Negative returns are clipped to zero. LBPO tradeoffs return for better constraint satisfaction.
