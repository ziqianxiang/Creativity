Table 1: Adaptive preconditioners (analyzed methods are bolded), with G0 = 0 and β1, β2 ∈ [0, 1). In practice, a small I is added to ensure Ak 0. *: We use the PyTorch implementation in experiments which includes bias correction.			Optimizer	Gk	(Vk= Vfik (Wk))	Ak	βAdaGrad	Gk-1 +diag(VkVk>)	G1k/2	0RMSProp	β2Gk-1+(1-β2)diag(VkVk>)	G1k/2	0Adam	(β2Gk-i + (1 - β2)diag(VkVk>))/(1 - βk)	Gk/2	β1AMSGrad*	(β2Gk-i + (1 - β2 )diag(VkVk>))/(1 - βk)	max{Ak-1, Gk/2 }	β1RMSProp and Adam maintain an exponential moving average of past stochastic gradients, but asReddi et al. (2018) pointed out, unlike AdaGrad, the corresponding preconditioners do not guaranteethat Ak+1 Ak and the resulting per-dimension step-sizes do not go to zero. This can lead tolarge fluctuations in the effective step-size and prevent these methods from converging. To mitigatethis problem, they proposed AMSGrad, which ensures Ak+1 Ak and the convergence of iterates.
Table 2: ResUlts for smooth, Convex functions.
Table 3: Summary of notationConcept	Symbol	Concept	SymbolIteration counter, maximum	k, T	General preconditioner	AkIterates, minimum	* Wk,W*	Preconditioner bounds	[amin, amax]Step-size	ηk	Maximum smoothness	LmaxFunction value, minimum	f(W),f*	Dimensionality	dStoch. function value, minimum	fi(W),fi*	Diameter bound	D		Variance	σ2 = Ei[fi(W*) —fi*]A Setup and assumptionsWe restate the main notation in Table 3. We now restate the main assumptions required for ourtheoretical resultsWe assume our objective f : Rd → R has a finite-sum structure,1nf (w) = - Efi(W),	⑷ni=1and analyze the following update, with ik selected uniformly at random,Wk+1 = Wk — ηk A-1 mk	； mk = βmk-ι + (1 — βNfik (Wk)	(Update rule)where ηk is either a pre-specified constant or selected on the fly. We consider AdaGrad and AMSGradand use the fact that the preconditioners are non-decreasing i.e. Ak Ak-1. For AdaGrad, β = 0.
