Table 1: Classification test error (%) on CIFAR10 using SGD, SGD + NM, and SRSGD. We reportthe results of SRSGD with two restarting schedules: linear (lin) and exponential (exp). The numbersof iterations after which we restart the momentum in the lin schedule are 30, 60, 90, 120 for the1st, 2nd, 3rd, and 4th stage. Those numbers for the exp schedule are 40, 50, 63, 78. We include thereported results from (He et al., 2016b) (in parentheses) in addition to our reproduced results.
Table 2: Classification test error (%) on CIFAR100 using SGD, SGD + NM, and SRSGD. We reportthe results of SRSGD with two restarting schedules: linear (lin) and exponential (exp). The numbersof iterations after which we restart the momentum in the lin schedule are 50, 100, 150, 200 for the1st, 2nd, 3rd, and 4th stage. Those numbers for the exp schedule are 45, 68, 101, 152. We include thereported results from (He et al., 2016b) (in parentheses) in addition to our reproduced results.
Table 3: On CIFAR10/100 (%), SRSGD training with only 100 epochs achieves comparable classifi-cation errors (%) to the SGD baseline training with 200 epochs.
Table 4: Test errors on CIFAR10 (%) of Pre-ResNet-110 (Left)/290 (Right) using different optimizers.
Table 5: Single crop validation errors (%) on ImageNet of ResNets trained with SGD baseline andSRSGD. We report the results of SRSGD with the increasing restarting frequency in the first twolearning rates. In the last learning rate, the restarting frequency is linearly decreased to 1. For baselineresults, we also include the reported single-crop validation errors (He et al., 2016c) (in parentheses).
Table 6: Comparison of single crop validation errors on ImageNet (%) between SRSGD training withfewer epochs and SGD training with full 90 epochs.
Table 7: Restarting frequencies for CIFAR10 and CIFAR100 experiments	CIFAR10	CIFAR100Linear schedule Exponential schedule	Fl = 30, F2 = 60, F3 = 90, F4 = 120 (r = 2) Fl = 40, F2 = 50, F3 = 63, F4 = 78 (r = 1.25)	Fl = 50, F2 = 100, F3 = 150, F4 = 200 (r = 2) Fl = 45 , F2 = 68, F3 = 101, F4 = 152 (r = 1.50)D.2 ImageNetThe ImageNet dataset contains roughly 1.28 million training color images and 50K validation colorimages from 1000 classes Russakovsky et al. (2015). We run our ImageNet experiments on ResNet-50, 101, 152, and 200 with 5 different seeds. Following He et al. (2016a;b), we train each model for90 epochs with a batch size of 256 and decrease the learning rate by a factor of 10 at the 30th and 60thepoch. The initial learning rate is 0.1, the momentum is 0.9, and the weight decay rate is 1 × 10-5.
Table 8: Restarting frequencies for ImageNet experiments	ImageNetLinear schedule	F1 = 40, F2 = 80, F3: linearly decayed from 80 to 1 in the last 30 epochsD.3 Training ImageNet in Fewer Number of Epochs:Table 9 contains the learning rate and restarting frequency schedule for our experiments on trainingImageNet in fewer number of epochs, i.e. the reported results in Table 6 in the main text. Othersettings are the same as in the full-training ImageNet experiments described in Section D.2 above.
Table 9: Learning rate and restarting frequency schedule for ImageNet short training, i.e. Table 6 inthe main text.
Table 12: Discriminator loss (i.e. Earth Moving distance estimate) of the WGAN with gradientPenaIty trained on MNIST with SGD SGD + NM and SRSGD. In all experiments, We use the initiallearning rate of 0.01, which is reduced by a factor of 10 at epoch 200 and 300. All models are trainedfor 350 eP°chs∙ The momentum for sgd and sgd + nm is set to 0∙9∙ The restart schedule inSRSGD is Setto 60 120,and180.____________________________________________________TaSk	SGD	SGD + NM	SRSGD	Improv ement over SGD/SGD + NMMNIST	0.71 ± 0.10	。58 ±。03	0.44 ± 0.06	0.27/0.14IO1------ SGD + Momentum ------ SGD + NesterovMomentUm ------ SRSGDIO0o 50	100 150 200 250 300 350EPochFigure 8: Earth Moving distance estimate (i.e. discriminator loss) vs. training epochs of WGAN withgradient penalty trained with SGD (red), SGD + NM (green), and SRSGD (blue) on MNIST.
Table 13: Learning rate (LR) schedule for the ablation study of error rate vs. reduction in trainingepochs for CIFAR10 experiments, i.e. Figure 4 in the main text and for CIFAR100 experiments, i.e.
Table 14: Classification test error (%) of SGD short training (100 epochs), SGD full training (200epochs), SRSGD short training (100 epochs), and SRSGD full training (200 epochs) on CIFAR10.
Table 15: Classification test error (%) of SGD short training (100 epochs), SGD full training (200epochs), SRSGD short training (100 epochs), and SRSGD full training (200 epochs) on CIFAR100.
Table 16: Test error vs. number of training epochs for CIFAR10Network	110(90 less)	125 (75 less)	140 (60 less)	155 (45 less)	170 (30 less)	185(15 less)	200 (full trainings)Pre-ResNet-110	5.37 ± 0.11	5.27 ± 0.17	5.15 ± 0.09	5.09 ± 0.14	4.96 ± 0.14	4.96 ± 0.13	4.93 ± 0.13Pre-ResNet-290	4.80 ± 0.14	4.71 ± 0.13	4.58 ± 0.11	4.45 ± 0.09	4.43 ± 0.09	4.44 ± 0.11	4.37 ± 0.15Pre-ResNet-470	4.52 ± 0.16	4.43 ± 0.12	4.29 ± 0.11	4.33 ± 0.07	4.23 ± 0.12	4.18 ± 0.09	4.18 ± 0.09Pre-ResNet-650	4.35 ± 0.10	4.24 ± 0.05	4.22 ± 0.15	4.10 ± 0.15	4.12 ± 0.14	4.02 ± 0.05	4.00 ± 0.07Pre-ResNet-1001	4.23 ± 0.19	4.13 ± 0.12	4.08 ± 0.15	4.10 ± 0.09	3.93 ± 0.11	4.06 ± 0.14	3.87 ± 0.07Table 17: Top 1 single crop validation error vs. number of training epochs for ImageNetNetwork	60 (30 less)	65 (25 less)	70 (20 less)	75 (15 less)	80 (10 less)	90 (full trainings)ResNet-50	25.42 ± 0.42	25.02 ± 0.15	24.77 ± 0.14	24.38 ± 0.01	24.30 ± 0.21	23.85 ± 0.09ResNet-101	23.11 ± 0.10	22.79 ± 0.01	22.71 ± 0.21	22.56 ± 0.10	22.44 ± 0.03	22.06 ± 0.10ResNet-152	22.28 ± 0.20	22.12 ± 0.04	21.97 ± 0.04	21.79 ± 0.07	21.70 ± 0.07	21.46 ± 0.07ResNet-200	21.92 ± 0.17	21.69 ± 0.20	21.64 ± 0.03	21.45 ± 0.06	21.30 ± 0.03	20.93 ± 0.13Table 18: Test error vs. number of training epochs for CIFAR100Network	110 (90 less)	125 (75 less)	140 (60 less)	155 (45 less)	170 (30 less)	185 (15 less)	200 (full trainings)Pre-ResNet-110	24.06 ± 0.26	23.82 ± 0.24	23.82 ± 0.28	23.58 ± 0.18	23.69 ± 0.21	23.73 ± 0.34	23.49 ± 0.23Pre-ResNet-290	21.96 ± 0.45	21.77 ± 0.21	21.67 ± 0.37	21.56 ± 0.33	21.38 ± 0.44	21.47 ± 0.32	21.49 ± 0.27Pre-ResNet-470	21.35 ± 0.17	21.25 ± 0.17	21.21 ± 0.18	21.09 ± 0.28	20.87 ± 0.28	20.81 ± 0.32	20.71 ± 0.32Pre-ResNet-650	21.18 ± 0.27	20.95 ± 0.13	20.77 ± 0.31	20.61 ± 0.19	20.57 ± 0.13	20.47 ± 0.07	20.36 ± 0.25Pre-ResNet-1001	20.27 ± 0.17	20.03 ± 0.13	20.05 ± 0.22	19.74 ± 0.18	19.71 ± 0.22	19.67 ± 0.22	19.75 ± 0.11
Table 17: Top 1 single crop validation error vs. number of training epochs for ImageNetNetwork	60 (30 less)	65 (25 less)	70 (20 less)	75 (15 less)	80 (10 less)	90 (full trainings)ResNet-50	25.42 ± 0.42	25.02 ± 0.15	24.77 ± 0.14	24.38 ± 0.01	24.30 ± 0.21	23.85 ± 0.09ResNet-101	23.11 ± 0.10	22.79 ± 0.01	22.71 ± 0.21	22.56 ± 0.10	22.44 ± 0.03	22.06 ± 0.10ResNet-152	22.28 ± 0.20	22.12 ± 0.04	21.97 ± 0.04	21.79 ± 0.07	21.70 ± 0.07	21.46 ± 0.07ResNet-200	21.92 ± 0.17	21.69 ± 0.20	21.64 ± 0.03	21.45 ± 0.06	21.30 ± 0.03	20.93 ± 0.13Table 18: Test error vs. number of training epochs for CIFAR100Network	110 (90 less)	125 (75 less)	140 (60 less)	155 (45 less)	170 (30 less)	185 (15 less)	200 (full trainings)Pre-ResNet-110	24.06 ± 0.26	23.82 ± 0.24	23.82 ± 0.28	23.58 ± 0.18	23.69 ± 0.21	23.73 ± 0.34	23.49 ± 0.23Pre-ResNet-290	21.96 ± 0.45	21.77 ± 0.21	21.67 ± 0.37	21.56 ± 0.33	21.38 ± 0.44	21.47 ± 0.32	21.49 ± 0.27Pre-ResNet-470	21.35 ± 0.17	21.25 ± 0.17	21.21 ± 0.18	21.09 ± 0.28	20.87 ± 0.28	20.81 ± 0.32	20.71 ± 0.32Pre-ResNet-650	21.18 ± 0.27	20.95 ± 0.13	20.77 ± 0.31	20.61 ± 0.19	20.57 ± 0.13	20.47 ± 0.07	20.36 ± 0.25Pre-ResNet-1001	20.27 ± 0.17	20.03 ± 0.13	20.05 ± 0.22	19.74 ± 0.18	19.71 ± 0.22	19.67 ± 0.22	19.75 ± 0.11G Impact of Restarting Frequency for ImageNet and CIFAR 1 00G. 1 Implementation DetailsFor the CIFAR10 experiments on Pre-ResNet-290 in Figure 5 in the main text, as well as theCIFAR100 and ImageNet experiments in Figure 14 and 15 in this Appendix, we vary the initialrestarting frequency F1. Other settings are the same as described in Section D above.
Table 18: Test error vs. number of training epochs for CIFAR100Network	110 (90 less)	125 (75 less)	140 (60 less)	155 (45 less)	170 (30 less)	185 (15 less)	200 (full trainings)Pre-ResNet-110	24.06 ± 0.26	23.82 ± 0.24	23.82 ± 0.28	23.58 ± 0.18	23.69 ± 0.21	23.73 ± 0.34	23.49 ± 0.23Pre-ResNet-290	21.96 ± 0.45	21.77 ± 0.21	21.67 ± 0.37	21.56 ± 0.33	21.38 ± 0.44	21.47 ± 0.32	21.49 ± 0.27Pre-ResNet-470	21.35 ± 0.17	21.25 ± 0.17	21.21 ± 0.18	21.09 ± 0.28	20.87 ± 0.28	20.81 ± 0.32	20.71 ± 0.32Pre-ResNet-650	21.18 ± 0.27	20.95 ± 0.13	20.77 ± 0.31	20.61 ± 0.19	20.57 ± 0.13	20.47 ± 0.07	20.36 ± 0.25Pre-ResNet-1001	20.27 ± 0.17	20.03 ± 0.13	20.05 ± 0.22	19.74 ± 0.18	19.71 ± 0.22	19.67 ± 0.22	19.75 ± 0.11G Impact of Restarting Frequency for ImageNet and CIFAR 1 00G. 1 Implementation DetailsFor the CIFAR10 experiments on Pre-ResNet-290 in Figure 5 in the main text, as well as theCIFAR100 and ImageNet experiments in Figure 14 and 15 in this Appendix, we vary the initialrestarting frequency F1. Other settings are the same as described in Section D above.
Table 19: Test error when using new learning rate schedules with less training epochs at the 2ndand 3rd learning rate for CIFAR10. We still train in full 200 epochs in this experiment. In the table,80-90-100, for example, means we reduce the learning rate by factor of 10 at the 80th, 90th, and100 th epoch.
Table 20: Test error when using new learning rate schedules with less training epochs at the 2nd and3rd learning rate for CIFAR100. We still train in full 200 epochs in this experiment. In the table,80-90-100, for example, means we reduce the learning rate by factor of 10 at the 80th, 90th, and100 th epoch.
Table 21: Top 1 single crop validation error when using new learning rate schedules with less trainingepochs at the 2nd learning rate for ImageNet. We still train in full 90 epochs in this experiment. Inthe table, 30-40, for example, means we reduce the learning rate by factor of 10 at the 30th and 40thepoch.
