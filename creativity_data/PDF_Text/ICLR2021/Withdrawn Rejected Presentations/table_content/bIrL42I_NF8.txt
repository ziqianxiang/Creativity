Table 1: The generalization issues fordecentralized deep learning on twotopologies (ResNet-20 on CIFAR-10with n ∈ {16, 32, 64} workers). Thetest top-1 accuracies are over threeseeds with fine-tuned learning rates.
Table 2: The impact of consensus distance of different phases on generalization performance (test top-1accuracy) of training ResNet-20 on CIFAR-10. The All-Reduce performance for n = 32 and n = 64 are92.82 ± 0.27 and 92.71 ± 0.11 respectively. The tuned decentralized performance (all phases on a fixed ringand WIo consensus distance control) for n = 32 and n = 64 are 91.74 ± 0.15 and 89.87 ± 0.12 respectively.
Table 3: The impact of different consensus distances on generalization for different phases of trainingResNet-20-3 on ImageNet-32. The centralized baseline performances for n= 16 and n=32 are 51.74 ± 0.06and 51.98 ± 0.37 respectively, while those of decentralized training (on a fixed ring) are 51.04 ± 0.06 and50.17 ± 0.04. The reported test top-1 accuracies are over two seeds.
Table 4: Quality propagation across training phases with different consensus distances on ResNet-20 forCIFAR-10 (Ring with n = 32). In phase-1 and phase-2, the model parameters reach inexact consensus controlledby different target consensus distance Ξ, while phase-3 performs All-ReduCe on model parameters.
Table 5: The impact of different numbers of training epochs (at phase-1) on generalization, for trainingResNet-20 on CIFAR-10 (dec-phase-1 with n = 32). The number of epochs at phase-1 is chosen from{150, 200, 250}, while the rest of the training reuses our default setup.
Table 6: The importance of phase-1 for training ResNet-20 on CIFAR-10 (n = 32), in terms of (1) targetconsensus distance and (2) the number of training epochs. In phase-2 and phase-3, we perform decentralizedtraining (w/o consensus distance control).
Table 7: Spectral gap and node degree of studied topologies.
Table 8: The effect of communication topologies and scales (ResNet-20 on CIFAR-10 with n=32). The testtop-1 accuracies are over three seeds with fine-tuned learning rates.
Table 9: The impact of consensus distance of different phases on generalization performance (test top-1accuracy) of training ResNet-20 on CIFAR-10. The centralized baseline performance for n=32 and n=64 are92.82 ± 0.27 and 92.71 ± 0.11 respectively. The performance of decentralized training (all phases on a fixedring and w/o consensus distance control) for n=32 and n=64 are 91.74 ± 0.15 and 89.87 ± 0.12 respectively.
Table 10: The effect of SlowMo for decentralized learning, for training ResNet20 on CIFAR-10 (n = 32).
Table 11: Phase-1 consensus distance control performance with fine-tuned learning rates of trainingResNet-20 on CIFAR-10 (n = 32). Setup in this table is identical to that of Table 2, except that we fine-tune the learning rate for each case from a grid of linear scaling-up factors {30, 28, 26, 24, 22}. The results areover three seeds.
Table 12:	The impact of consensus distance on generalization performance with vanilla SGD (withoutmomentum) (test top-1 accuracy) of training ResNet-20 on CIFAR-10. The All-Reduce performance for n = 32and n = 64 are 90.64 ± 0.19 and 90.58 ± 0.26 respectively. The tuned decentralized performance (all phaseson a fixed ring and w/o consensus distance control) for n=32 and n=64 are 90.30 ± 0.14 and 88.92 ± 0.23respectively. We repeat experiments for n = 32 for 3 seeds and n = 64 for 2 seeds.
Table 13:	The impact of different numbers of training epochs (at phase-2 and phase-3) on generalization,for training ResNet-20 on CIFAR-10 (ring topology with n=32). The number of epochs at phase-1 is chosenfrom {75,100,125}, while the rest of the training reuses our default setup. Experiments are run over 2 seeds.
Table 14:	The impact of half cosine learning rate schedule on generalization, for training ResNet20 onCIFAR-10 (ring topology with n=32). The inline figure depicts the uncontrolled consensus distance over thewhole training procedure through the half-cosine learning rate schedule. Only one training phase is consideredfor the consensus distance control and the numerical results in the table are averaged over 3 seeds.
Table 15: The impact of different consensus distances on optimization and/or generalization, for differentphases of training ResNet-20 on CIFAR-10 (n = 32). The table is almost identical to Table 2, except theconsensus distance is controlled by the (runtime) averaged norm of the local gradients (i.e. adaptive consensusdistance).
Table 16: The impact of quality propagation across phases (in both phase 1 and phase 2) on an undirectedtime-varying exponential graph (n = 32), similar to Table 4.
Table 17: The impact of different consensus distances at phase 2, for training ResNet-20 on CIFAR-10 withtime-varying exponential graph (n = 32). The baseline performance of using exponential graph for the entiredecentralized training is 92.64 ± 0.04. The reported test top-1 accuracies are averaged over three seeds.
