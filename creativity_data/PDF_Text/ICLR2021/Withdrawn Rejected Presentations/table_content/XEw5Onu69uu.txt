Table 1: Summary of datasets from the 2 different target domains4 ExperimentsFor the experiments we focus on the problem of predicting chemical compound graphs from 2Dimages where the fully mediating layer is represented using the planar embedding of the chemicalgraph structure we are predicting. In order to measure empirically the performance of our methodof self-labeling fully mediating representations we perform three steps. (1) We pre-train (trainingdetails in Appendix A.2) a ChemGrapher [21] model (summarized in Appendix A.1) wherefore,corresponding to the pipeline described in the work of Oldenhof et al. [21], we sample around130K chemical compounds from ChEMBL [9] in SMILES format and artificially generate, usingan RDKit fork [1], a rich labeled dataset with 2D images of chemical compounds. (2) Secondly, wetest the baseline performance of this pre-trained model on two different test sets from two differenttarget domains than the source domain of the pre-trained model. (3) Thirdly, we apply our domainadaptation method and measure performance again on the two target domains.
Table 2: Comparison performance on Maybridge data set. We observe that our approach enables toreach higher performance than the current state of the art. Most of the tools available for chemicalgraph recognition are rule based approaches for which a training dataset is not relevant.
Table 3: Summary of the layers of the segmentation networkLayer	Kernel	Nonlinearity	Padding	Dilationconv1	3x3	ReLU	1	no dilationconv2	3x3	ReLU	2	2conv3	3x3	ReLU	4	4conv4	3x3	ReLU	8	8conv5	3x3	ReLU	8	8conv6	3x3	ReLU	4	4conv7	3x3	ReLU	2	2conv8	3x3	ReLU	1	no dilationlast	1x1	none	no padding	no dilation12Under review as a conference paper at ICLR 2021Table 4: Different layers in the classification networkLayer	Kernel	Nonlinearity	Padding	Dilationdepthconv1	3x3	ReLU	1	no dilationconv2	3x3	ReLU	2	2conv3	3x3	ReLU	4	4conv4	3x3	ReLU	8	8conv5	3x3	ReLU	1	no dilation
Table 4: Different layers in the classification networkLayer	Kernel	Nonlinearity	Padding	Dilationdepthconv1	3x3	ReLU	1	no dilationconv2	3x3	ReLU	2	2conv3	3x3	ReLU	4	4conv4	3x3	ReLU	8	8conv5	3x3	ReLU	1	no dilationglobal maxpool	input size	None	no padding	no dilationlast	1x1	None	no padding	no dilationA.2 Training details for graph recognition toolTraining details of the graph recognition tool for every iteration of our method are summarized inTable 5. The input images used for training of the different networks are a mix if images from sourcedomain and upsampled rich labeled images from target domain. For pretraining of the ChemGraphermodel only images from source domain were used. The training was performed using a computenode with 2 NVIDIA v100 GPUs with 32GB of memory.
Table 5: Training details for different networksNetwork	#input images		#epochs	walltime	minibatch size	learning rate	source domain	target domain (upsampled)				Segm. network	114K	20K	5	24h	8	0.001Atom Clas.	12.4K	2.6K	2	8h	16	0.001Charge Clas.	12.4K	2.6K	2	8h	16	0.001Bond Clas.	4.4K	2.1K	2	4h	64	0.001A.3 Computational cost per rich-labeling iterationIn the following Table 6 the computational cost for 1 rich-labeling iteration is summarized includingall steps: (re)training, predicting and graph aligning rich-labeling.
Table 6: Computational costs per rich-labeling iteration	Training	Predict	Graph Aligning	Hardware	2 NVmIA v100 GPUs	1 NVJIDIA v100 GPU	Intel Xeon Gold 6240 2.6GEF	Dataset	Source+Target domain	Indigo/Maybride	Indigo	Maybridge#datapoints	see Table 5	4,000	4,000	4,000Walltime	〜44h (details Table 5)	〜2h	〜40min	〜3minA.4 Examples of cases where graph alignment failsWe would like to showcase some examples where the constrained (max 2 node substitutions or max1 edge substitution) graph alignment fails. At the same time it is important to note that our proposeddomain adaptation method is an iterative method, so if a graph alignment fails in a previous iterationit could succeed in a next one when the new model makes a new graph prediction closer to the truegraph.
