Table 1: A comparison of computational complexity and the number of parameters of the proposed optimalseparable convolution and existing approaches. The proposed optimal separable convolution is much moreefficient. In this table, C represents the channel size of convolution, K is the kernel size, H and W arethe output height and width, g is the number of groups. “Vol. RF” represents whether the correspondingconvolution satisfies the proposed volumetric receptive field condition.
Table 2: Experimental results on CIFAR10 for DARTS. The proposed optimal separable convolution (o-DARTS) generalizes well to the DARTS architecture, and achieves improved accuracy with approximately thesame FLOPs and fewer parameters. DARTS uses depth separable convolution and an optional d- is prefixed.
Table 3: Experimental results on ImageNet40 for the ResNet architecture. The proposed optimal separableconvolution (o-ResNet) achieves 4-5% performance gain over the ResNet baseline.
Table 4: Experimental results on	full ImageNet for the DARTS architecture. The proposed o-DARTS achieves				74.2% top1 accuracy with only 4.5 million parameters.					Net Arch e rc	FLOPs	#Params	Top1	Top1 Error	Top5 Top5 Error	(billion)	(million)	(%)	(%)	(%)	(%)(d-)DARTS (Liu et al., 2018)	0.530		4.72	73.3%	26.7%	91.3%	8.7%o-DARTS	0.554	4.50	74.2%	25.8%	91.9%	8.1%calculated on scales of [16, 8, 4], and the last fully-connected (FC) layer outputs 1000 categoriesfor classification. We make this modification because the ImageNet dataset has significantly moretraining samples than the CIFAR10 dataset. Experimental results are illustrated in Table 3, as can beseen, by substituting conventional convolutions with the proposed optimal separable convolutions,the resulting o-ResNet achieved 4-5% (e.g. 49.97% vs 44.93% for 56-layer and 50.72% vs 46.74%for 110-layer) performance gains comparing against the ResNet baselines. This demonstrates thatthe proposed optimal separable convolution scheme is much more efficient. For o-ResNet56 and o-ResNet110, they also have fewer parameters that could contribute to a more regularized model. Foro-ResNet20 and o-ResNet32, they have slightly more parameters because the last FC layer accountsfor a great portion of overhead for 1000 classes.
Table 5: Experimental results on CIFAR10 for the ResNet architecture with ablation studies of internal BN andnon-linearity and spatial separable configuration.
Table 6: Experimental results on CIFAR10 for the ResNet with inference time on Windows 10 Intel CPUi5-8250.
