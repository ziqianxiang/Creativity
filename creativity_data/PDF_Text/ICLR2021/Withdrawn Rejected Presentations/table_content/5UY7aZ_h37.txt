Table 1: Performance (mean±std over 4 trials) of different LSTM and Transformer models trained independentlywith the LM objective.
Table 2: Performance (mean±std over 4 trials) of different LSTM and Transformer models trained independentlyWith the classification objective.
Table 3: Performance (mean±std over 4 trials) of different LSTM and Transformer models with LM objectivewhen We apply pure distillation With T = 1.
Table 4: μ-Accuracy ↑ (mean±std over 4 trials) of different LSTM and Transformer models with classificationobjective when we apply pure distillation with T = LStudent Model	Transformer	Teacher Model		LSTM		Transformer-seq	UTransformer-seq	Transformer	0.9555 ± 0.0013	0.9556 ± 0.0006	0.9571 ± 0.0027	0.9596 ± 0.0008Transformer-seq	0.9599 ± 0.0006	0.9629 ± 0.0008	0.9679 ± 0.0005	0.9720 ± 0.0017UTransformer-seq	0.9611 ± 0.0006	0.9635 ± 0.0004	0.9688 ± 0.0008	0.9748 ± 0.0003LSTM	0.9682 ± 0.0002	0.9690 ± 0.0011	0.9741 ± 0.0004	0.9759 ± 0.0001In summary, the results from this section support the importance of recurrence for solving thistask (Tran et al., 2018; Dehghani et al., 2019). Additionally, as shown in Figures 3a and 3b, we finda decreasing trend in the variance of the models, i.e., adding more inductive biases to the modelsdecreases their variance. This is empirical evidence that supports the relation between variance of thesolutions a model converges to and its inductive biases.
Table 5: Accuracy and Expected Calibration Error (mean±std over 4 trials) of CNN and MLP trained indepen-dently on MNIST and evaluated on MNIST, MNIST-Scaled and MNIST-Translated.
Table 6: Accuracy and Expected Calibration Error (mean±std over 4 trials) of CNN and MLP trained with puredistillation with τ = 5, on MNIST and evaluated on MNIST, MNIST-Scaled and MNIST-Translated.
Table 7: Performance (mean±std over 4 trials) of different LSTM and Transformer models trained independentlywith the LM objective on the training set.
Table 8: Performance (mean±std over 4 trials) of different LSTM and Transformer models trained independentlywith the classification objective on the training set.
Table 9: Accuracy of MLPs trained through KD with CNN teachers, where the CNN teachers are trained onin-distribution (vanilla MNIST) training set, while the training set in the distillation step is either in-distortion(first row) or out-of-distribution (second and third rows). Note that during the distillation step, the student do nothave access to the ground truth labels from the training set.).
Table 10: Accuracy of MLPs trained with ground truth labels on different splits of the Corrupted-MNIST dataset.
Table 11: Accuracy of CNNs trained with ground truth labels on different splits of the Corrupted-MNIST dataset.
