Table 1: Results obtained using the edge probing classifier. We study the performance in manymodelâ€™s variants, considering small and large versions of several models. Results are shown groupedby the families of methods defined in Section 3.1.
Table 2: Tree Edition Distance against the ground truthgraph (large models used). We display both strategiesfor estimating de along with their average score.
Table 3: Average F1-score for some semantic categories revealing models strengths and weaknesses.
Table 4: Summary of our main findings and their corresponding supporting evidence.
Table 5: Accuracies for every model in this study. This table is analogous to Table 1E Pre-Training corpus comparisonFamily	Model	Pre-Training Corpus Size			Tokens	Uncompressed SizeNCE	Word2Vec	33B^^	150GB*	GloVe-42B	42B	175GB*GLM	GPT-2	10B*	40GB	T5	180B*	750GB	ELMo	0.8B^^	4GB*	BERT	3.9B	16GBCE	RoBERTa	38.7B*	160GB	XLNet	32.9B	140GB*	ALBERT	3.9B	16GBTable 6: Pre-Training corpus sizes used for each one of the studied models. The official sourcesreport corpus sizes in terms of number of tokens or uncompressed size in GB. The symbol * denotesvalues estimated by us based on official available information.
Table 6: Pre-Training corpus sizes used for each one of the studied models. The official sourcesreport corpus sizes in terms of number of tokens or uncompressed size in GB. The symbol * denotesvalues estimated by us based on official available information.
Table 7: Pearson correlation among concept-level F1-score obtained by different architectures. Weonly considered concepts with F1-scores between 0.55 an 0.9 to remove possible noisy outliers.
Table 8: F1-score and standard deviation of several semantic categories evaluated on the largerversion of each model. Each value was computed by considering the F1-score of all the conceptsthat belong to the analyzed category. This is not an extensive list and categories are somewhatimbalanced. Categories were selected based on the number of sub-categories they contained.
