Table 1: Classification accuracy on MNISTWe increase the number of training epochs for Entropy-SGD/Entropy-SGD-SALR/SGD-SALR to40 and the training epochs of SGD/SWA to 200. Experimental results are reported in Table 2.
Table 2: Classification accuracy on CIFAR10To illustrate the flexibility of our framework, we change the base optimizer SGD to ADAM andre-run all the experiments under a similar setting as that of Table 2. Results are reported in Table3. Finally, in Table 4, we report the sharpness measure of the final solution obtained by each op-timization approach. We also conduct some experiments on CIFAR-100. Results are deferred toAppendix.
Table 3: Classification accuracy on CIFAR10 using ADAM6.2	Text PredictionWe train an LSTM network on the Penn Tree Bank (PTB) dataset for word-level text prediction.
Table 4: Sharpness of final solutions (CIFAR-10, SGD)PTB	SGD	SWA	Entropy-SGD	SALRPTB-LSTM	78.4 (0.22)	78.1 (0.25)	72.15 (0.16)	71.42 (0.14)WP-LSTM	1.223 (0.01)	1.220 (0.05)	1.095 (0.01)	1.089 (0.02)Table 5: Perplexity on PTB/WPiterate. To further demonstrate the advantage of SALR framework, we plot the testing error curvesfor SGD, Entropy-SGD, Entropy-SALR and SALR in Figure 4 (Left). Interestingly, the overall trendwhen adding SALR to SGD is drastically changed with the trend smoothly and more consistentlyincreasing its performance. This can be potentially explained through the capability of SALR toquickly escape sharp regions relative to SGD and hence attain larger and more consistent rates ofimprovement across Epochs. We also plot the change of sharpness/learning rates over some SALRiterations in the last Epoch in Figure 4 (Right). This figure highlights the dynamics of both learningrates and sharpness, highlighting that the median sharpness tends to stabilize and hence leading to aproportional relationship between learning rates and sharpness.
Table 5: Perplexity on PTB/WPiterate. To further demonstrate the advantage of SALR framework, we plot the testing error curvesfor SGD, Entropy-SGD, Entropy-SALR and SALR in Figure 4 (Left). Interestingly, the overall trendwhen adding SALR to SGD is drastically changed with the trend smoothly and more consistentlyincreasing its performance. This can be potentially explained through the capability of SALR toquickly escape sharp regions relative to SGD and hence attain larger and more consistent rates ofimprovement across Epochs. We also plot the change of sharpness/learning rates over some SALRiterations in the last Epoch in Figure 4 (Right). This figure highlights the dynamics of both learningrates and sharpness, highlighting that the median sharpness tends to stabilize and hence leading to aproportional relationship between learning rates and sharpness.
Table 6: Perplexity on PTB2. We train an LSTM to perform character-level text-prediction using War and Peace (WP).
Table 7: Perplexity on War and Peace3. We add two experiments on CIFAR-10 and CIFAR-100.
Table 8: More Results on CIFAR-10/1004. We compare SALR with SmoothOut (Wen et al., 2018), a technique to smooth out sharpminima by averaging over multiple perturbed copies of the landscape. We run SALR,SmoothOut and AdamSmoothOut on CIFAR-10 and CIFAR-100 five times using ResNet44.
Table 9: SmoothOutIn conclusion, SALR can deliver improvement over a range of dataset.
