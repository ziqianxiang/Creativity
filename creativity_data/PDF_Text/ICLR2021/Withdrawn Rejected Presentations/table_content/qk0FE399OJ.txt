Table 1: Comparison between the EPS, None-aging EA and RSPS on NASBench-201 validationdataset.
Table 2: Test accuracy comparisons between EPS and other state-of-the-art NAS works reduplicatedin the NASBench-201 paper Li & Talwalkar (2019); Liu et al. (2019b); Dong & Yang (2019b;a);Peng et al. (2019). Our results on three different datasets are respectively listed in the second block.
Table 3: Comparisons between EPS and Liu et al.
Table 4: Comparison with the state-of-the-art methods (Peng et al. (2019); Liu et al. (2019b); Dong& Yang (2019b); Xie et al. (2018); Xu et al. (2019); Zela et al. (2019); Lu et al. (2019); Li &Talwalkar (2019); Yang et al. (2020); Zhang et al. (2020)) on CIFAR-10/-100. (Search time may bevaried using different GPUs.)Method	Test Err.(%)		Params (M)	Search cost (GPU days)	Search method	CIFAR-10	CIFAR-100			ENAS	2.89	1891	4.6	0.45	RLDARTSV1	3.00±0.14	-	3.3	1.5	GDARTSV2	2.76±0.09	17.54*	3.3	4	GGDAS	2.82	18.13	2.5	0.17	GSNAS	2.85±0.02	-	2.8	1.5	GPC-DARTS	2.57±0.07	-	3.6	0.1	GR-DARTS	2.95±0.21	18.01±0.26	-	1.5	GNSGANet	2.75	-	3.3	4	EARSPS	2.71	17.63*	4.3	0.7	RSCARS	2.62	-	3.6	0.4	RS&EARS-NSAS	2.64(2.50)	17.56(16.85)	3.4	0.7	RSEPS	2.52±0.08(2.41)	17.09 ±0.26(16.90)	3.5/3.6	0.33	RS&EA7Under review as a conference paper at ICLR 2021
Table 5: Comparison with the Liu et al. (2019b), Zela et al. (2019) in DARTS sub search spacesSetting	RSPS		DARTS	DARTS-ES	DARTS-ADA	EPS	S1	3.17±0.15	4.66±0.71-	3.05±0.07	3.03±0.08	2.84±0.16CIFAR-10	S2 S3	3.46±0.15 2.92±0.04	4.42±0.40 4.12±0.85	3.41±0.14 3.71±1.14	3.59±0.3 2.99±0.34	3.21±0.06 2.61±0.05	S4	89.39±0.84	6.95±0.18	4.17±0.21	3.89±0.67	3.82±0.15	S1	25.81±0.39	29.93±0.41	28.90±0.81	24.94±0.81	23.85±1.27CIFAR-100	S2	22.88±0.16	28.75±0.92	24.68±1.43	26.88±1.11	21.92±0.27	S3	24.58±0.61	29.01±0.24	26.99±1.79	24.55±0.63	22.38±0.61	S4	30.01±1.52	24.77±1.51	23.90±2.01	23.66±0.90	23.93±0.67	S1	2.64±0.09	9.88±5.50	2.80±0.09	2.59±0.07	2.49±0.04SVHN	S2	2.57±0.04	3.69±0.12	2.68±0.18	2.79±0.22	2.56±0.09	S3	2.89±0.09	4.00±1.01	2.78±0.29	2.58±0.07	2.53±0.05	S4	3.42±0.04	2.90±0.02	2.55±0.15	2.52±0.06	2.78±0.11S4: A search space contains {3 × 3 SepConv, N oise} on each edge. Also, we noticed that Ran-domNAS tend to fail on the S4 which suggests the method is difficult to distinguishes the N oiseoperation from 3 × 3 S epC onv.
Table 6: Comparison with other state-of-the-artmethods on PTB.
Table 7: Comparison between EPS and two optional approaches on NASBench-201Method	Valid acc.	Test acc.	GPU hrs.
Table 8: EPS is ran for 4 times with different random seeds. The best of these architectures for eachrun is then trained from scratch for 300 epochs.
Table 9: The GPU benchmark of the EPS in different search spaces on one Titan V GPU.
Table 11: Architectures searched on DARTS image classification search space.
Table 12:	Architectures searched on DARTS image classification sub search space (CIFAR-10).
Table 13:	Architectures searched on DARTS image classification sub search space (CIFAR-100).
Table 14:	Architectures searched on DARTS image classification sub search space (SVHN).
Table 15:	Architectures searched on DARTS PTB search space.
Table 16: NASBench validation accuracy across the 36 settings for EPS. “#i" stands for the ithsearch run.
Table 17: DARTS search space testing accuracy. “#i" stands for the ith search run. “run i" standsfor the ith validation training run.
Table 18: DARTS sub search space testing accuracy. “#i" stands for the ith search run. “run i" standsfor the ith validation training run.
