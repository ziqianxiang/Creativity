Table 1: Candidate operation set. For each type of operation including multi-head attention (MHA),feed-forward network (FFN) and separable convolution (SepConv) in each row, we list the numberof heads in MHA and the size of the intermediate layer in FFN, and kernel size in SepConv underdifferent hidden sizes (in different columns).
Table 2: Comparison of NAS-BERT models and hand-designed BERT models under different sizes(60M, 30M, 10M, 5M) on GLUE dev set. “PF” means pre-training and fine-tuning. “KD” meanstwo-stage knowledge distillation. MNLI is reported in the matched set. Spearman correlation isreported for STS-B. Matthews correlation is reported for CoLA. Accuracy is reported for othertasks.
Table 2: 1) The size of the embedding layer is the same as that of the corresponding NAS-BERTmodel; 2) We use the original BERT structure (MHA plus FFN) and keep the parameter, latency,depth and width as close as possible to the corresponding NAS-BERT model. The baseline BERTmodels in Table 2 are: BERT60 (L=10, H=512, A=8), BERT30 (L=6, H=512, A=8), BERT10 (L=6,H=256, A=4) and BERT5 (L=6, H=128, A=2) where L is the number of layers, H is the hidden size,and A is the number of attention heads.
Table 3: Results on the dev and test set of the GLUE benchmark. “*” means using data augmenta-tion. The test set results are obtained from the official GLUE leaderboard.
Table 4: The results of NAS-BERT with and without progressive shrinking (PS) on NAS-BERT60 .
Table 5: The results of different progressive shrinking approaches on NAS-BERT60 . PS-arch andPS-op denote pruning architectures and operations in progressive shrinking.
Table 6: The accuracy of the teacher models on dev set of the GLUE benchmark. The teacher modelof MobileBERT is IB-BERTLARGE which reaches the similar accuracy as original BERTLARGE .
