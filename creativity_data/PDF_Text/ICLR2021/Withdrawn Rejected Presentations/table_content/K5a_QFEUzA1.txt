Table 1: BLEU scores on WMT’14 English-French (En-Fr), WMT’16 English-German (En-De) andWMT’16 English-Romanian (En-Ro) unsupervised translation tasks.
Table 2: BLEU scores on the unsupervised IWSLT’13 English-French (En-Fr) and IWSLT’14English-German (En-De) tasks.
Table 3: BLEU comparison of CBD vs. no cross-model variants in the WMT’14 English-French(En-Fr), WMT’16 English-German (En-De) and English-Romanian (En-Ro) tasks.
Table 4: Reconstruction BLEU scores of BD and CBD in different languages for the WMTunsupervised translation tasks. Lower BLEU means more diverse.
Table 5: Comparison between the amount of real data, generated data by CBD and the duplicates perlanguage pair for the WMT’14 En-Fr, WMT’16 En-De and En-Ro unsupervised MT tasks.
Table 6: BLEU comparison of CBD vs. an ensemble of UMT agents and ensemble knowledgedistillation (Freitag et al., 2017) on WMT’14 En-Fr, WMT’16 En-De and En-Ro translation tasks.
Table 7: Comparison with other alternatives on the WMT En-Fr, Fr-En, En-De and De-En, with XLMas the base model.
Table 8: BLEU scores on the unsupervised IWSLT’13 English-French (En-Fr) and IWSLT’14English-German (En-De) tasks with varying number of agents n of GCBD.
Table 9: Percentage of tri-gram repetitions in the synthetic data generated by ensemble knowledgedistillation (Freitag et al., 2017), compared to those created by CBD; and the respective test BLEUscores in WMT’14 En-Fr, WMT’16 En-De and En-Ro unsupervised tasks.
Table 10: BLEU scores of our method and the baseline (Conneau & Lample, 2019) on thetranslationese effect (Edunov et al., 2020), in the WMT’14 English-German setup.
