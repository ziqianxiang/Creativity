Table 1: ✓ indicates that the generator could effectively learn all the data modes, while × means despite bestefforts with tuning the training suffers from mode collapse (more than a quarter of the data modes are dropped).
Table 2: Quantitative Results on the Stacked MNIST dataset: Applying our proposed dynamic multiadversarial training (DMAT) procedure to a simple DCGAN achieves perfect mode coverage, better than manyexisting methods for mode collapse.
Table 3: Quantitative Results on CIFAR10: We benchmark DMAT against other multi-adversarial baselinesas well as on several GAN architectures, observing consistent performance increase.
Table 4: BigGAN + DMAT Ablations on CIFAR10 (A) A relaxed spawning condition with small α and shortwarmup schedule that leads to large number of discriminators (>7) (B) Long warm-up schedules that spawn newdiscriminators late into training (C) A greedy strategy for assigning responsibility of fake samples ( = 0) (D)Flipping the data splitting logic with responsibilities of fake samples being random and of real being -greedyand (E) Choosing the discriminator with lowest score for updating Generator instead of soft random weighting.
Table 5: Per-class FID on CIFAR10: FID improves consistently across all classes.
