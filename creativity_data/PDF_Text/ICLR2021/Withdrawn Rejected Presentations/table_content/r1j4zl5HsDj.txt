Table 1: Average E。〜P HMethod	Accuracy (%)π* (Ours)	-T7：9SGBS	{26.5, 26.2,	27.2, 26.5,	21.4, 12.8}Uncertainty	14.3LAL	4.1Uniform	6.9	knoWn to have near-optimal sample complexity (Hanneke et al., 2014). Uncertainty sampling evenoutperforms our r-dependent baseline by a bit which in theory should not occur-we conjecture thisis due to insufficient convergence of our policies or local minima. Our second experiment constructsa distribution P based on the dataset: to draw a θ 〜P we uniformly at random select a j ∈ [1000]and sets θ, = 2p(j) - 1 for all i ∈ [d]. As shown in Table 1, SGBS and π* are the winners. LALperforms much worse in this case, potentially because of the distribution shift from P (prior wetrain on) to P (prior at test time). The strong performance of SGBS may be due to the fact thatsign(θi) = 2z*(θ)i — 1 for all i and θ 〜P, a realizability condition under which SGBS has strongguarantees (Nowak, 2011).
Table 2: Average E。〜P HMethod	Average Regretπ* (OUrS)^^	3.209SGBS	{3.180, 3.224,	3.278, 3.263,	3.153, 3.090}Uncertainty	3.027LAL	3.610Uniform	3.877	fensive jokes. We filter the dataset to only contain users that rated all 100 jokes, resulting in 14116users. A rating of each joke was provided on a [-10, 10] scale which was rescaled to [-1, 1] andobservations were simulated as Bernoulli’s like above. We then clustered the ratings of these users(see Appendix J for details) to 10 groups to obtain Z = {z(k) : k ∈ [10], z(k) ∈ {0, 1}100} wherezi(k) = 1 corresponds to recommending the ith joke in user cluster z(k) ∈ Z. Figure 7 shows thesame style plot as Figures 4,6 but for this jokes dataset, with our policy alone nearly achieving ther-dependent baseline for all r. Mirroring the construction of the 20Q prior, we construct P by uni-formly sampling a user and employing their θ to answer queries. Table 2 shows that despite ourpolicy not being trained for this setting, its performance is still among the top.
Table 3: Number of Iterations and Learning Rates15Under review as a conference paper at ICLR 2021Procedure	Hyper-parameter	Experiment				1D Threshold |X| = 25	20 Questions |X| = 100	Jester Joke |X| = 100	N	1000 X |Z|	300 × |Z|	2000 × |Z|	M	1000	500	500	L	10	30	30Init + Train +	λbinary	7.5	30	30Fine-tune	λPol-reg(regret)	.2	.8	.8	λPol-reg(fine-tune)	.3	.8	.8	λGen-reg	.05	.1	.05	λbarrier		103 (all)	Table 4: Parallel Sizes and Regularization coefficientsTo provide a general strategy of choosing hyper-parameters, we note that L, firstly, λbinary, λPol-regare primarily parameters tuned for |X | as the noisiness and scale of the gradients, and entropy overthe arms X grows with the size |X |. Secondly, λGen-reg is primarily tuned for |Z| as it penalizesthe entropy over the N arms, which is a multiple of |Z |. Thirdly, learning rate of θ is primarilytuned for the convergence of constraint ρ* into the restricted class, thus Lbarrier becoming 0 afterthe specified number of iterations during initialization is a good indicator. Finally, we choose N
Table 4: Parallel Sizes and Regularization coefficientsTo provide a general strategy of choosing hyper-parameters, we note that L, firstly, λbinary, λPol-regare primarily parameters tuned for |X | as the noisiness and scale of the gradients, and entropy overthe arms X grows with the size |X |. Secondly, λGen-reg is primarily tuned for |Z| as it penalizesthe entropy over the N arms, which is a multiple of |Z |. Thirdly, learning rate of θ is primarilytuned for the convergence of constraint ρ* into the restricted class, thus Lbarrier becoming 0 afterthe specified number of iterations during initialization is a good indicator. Finally, we choose Nand M by memory constraint of our GPU. The hyper-parameters for each experiment was tunedwith less than 20 hyper-parameter assignments, some metrics to look at while tuning these hyper-parameters includes but are not limited to: gradient magnitudes of each component, convergence ofeach loss and entropy losses for each regularization term (how close it is to the entropy ofa uniformprobability), etc.
