Table 1: Converged parameters for the HOT-DINA student modelParameter	Mean (converged)	SD (converged)θι	-0.17	0.93 =θ2	1.77	0.64θ3	-0.65	072θ4	-073	∏3θ5	-0.38	0.78θ6	-0.28	0.55θ7	-048	07θ	0:T4	0.47bi	1.25	073b	1.25	072b3	1.25	0.72b4	1.25	072b5	1.42	07ib6	1.49	0.68 —Table 1 shows converged values for a subset of HOT-DINA parameters. For example, the eightθ values refer to the 8 student proficiency parameters of the student model. For simplicity, weshow only the first 6 values of b (skill difficulty parameter) in the table. Once we obtain the modelparameters, we need two things to be done for the student simulator to be successful: given an
Table 2: Tutor state and action range for each agent typeTyPe	Tutor State	Action Range	#Actions-1-	Content area, position in curriculum	Thresholds	3, Continuous-2-	Content area, position in curriculum	BACK, SAME, NEXT, SKIP	1, Discrete-3-	Content area	Any activity in content area	1, Discrete4	None	Any activity in any content area	1, Discrete4	Evaluating learned policiesWe evaluate the learned policies along two metrics which assess the local and global impacts. Thelocal impact is the average change in reward by replacing a single historical choice of activity withthe activity chosen by the policy. The global impact is the overall change in reward per attempt byfollowing the learned policy from the first attempt. Table 3 evaluates the learned policies, per agenttype, by their impact on learning gains (expected reward) of the first 100 attempts averaged acrossthe 8 children, compared to the historical baseline of 0.0009.
Table 3: Evaluating policies based on local and global impactAgent Type	Local impact	Global impact1	:	9.98x	=	1.37x =2	108Tx	144x3	1099x	166x4	11.64x —	2.47x —From the table, we see an increasing trend for both the local and global impacts and is in-line withour beliefs that less constrained the RL agent is, the greater the impact. Interestingly, local impactsseem to exceed global impacts for all 4 cases. This is because successive attempts have independent5Under review as a conference paper at ICLR 2021rewards since the Prior(Know at step t) does not depend on the policy-proposed-action at step t - 1for local impacts. Thus, if some less-known activity allows a large one-time gain at step t - 1, thelocal impact allows it to occur multiple times in subsequent steps, beating average global gains perstep.
Table 4: Prior work and their approachesPaper	Student model	Adaptive policyDavid et al. (2016)	BKT	ThreshoIdWhitehill & Movellan (20T7T	POMDP	Policy GradientDoroUdi et al. (2017)	BKT	MasteryShenetal. (2018)	Model-Free	DQNSegal etal.(2018)	BKT	ThresholdDoroUdietal.(2019)一	BKT	Inc TimeThis paper	HOT-DIN「	PPO6	ConclusionThis paper contributes a novel framework for optimizing instructional sequencing in an intelligenttutoring system by combining knowledge tracing with deep reinforcement learning and evaluatingthe learned decision policy on historical data. We fit a simulated student on authentic log data fromreal children using RoboTutor in Tanzania, in contrast to earlier work that used synthetic data. Wetrained the student model using HOT-DINA because it is more accurate than other knowledge tracingmethods. We used Proximal Policy Optimization because it learns better than previous reinforce-ment learning methods applied previously to ITS. We use knowledge probabilities estimated by thestudent model as a state and directly optimize for learning gains which we set as the reward. Weevaluated the learned policies’ local and global impact on expected knowledge gains relative to ahistorical baseline and explained the somewhat surprising results we observed.
