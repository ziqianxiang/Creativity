Table 1: Loss (MSE) ±95% confidence intervals on multidimensional linear regression task, aver-aged over 400 meta-testing tasks. The results are presented for a varying numbers of K data pointsand a noise level of 0.01In Table 1 we show that TreeMAML outperforms the Baseline and MAML across all numbers ofdata points. Learned TreeMAML performs relatively better for a larger number of data points; thisis expected because, as the number of data points increases, the gradients used to cluster the taskswill be less affected by the noise and become more accurate, leading also to better clustering.
Table 2: Performance (MSE loss) ±95% confidence intervals on mixed regression task, averagedover 600 tasks. N-epoch results on tasks with K=10 data points and a 0.01 noise level.
