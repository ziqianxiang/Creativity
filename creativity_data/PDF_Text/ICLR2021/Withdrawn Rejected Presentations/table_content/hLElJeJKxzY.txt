Table 1: Performance of different methods in different OPenAI Gym environmentsEnv	DDQN	Perfect Demonstration		Imperfect demonstration			DQfD	DQfDD-BC	DQfD	DQfDD-BC	Average stop episode				CartPole-v0	310±85	162±170	161±170	-	-CartPole-v1	500±0	121±19	91±27	500±0	411±179Acrobot-v1	198±123	51±4	49±5	261±136	143±11LunarLander-v2	263±21	65±3	65±9	402±192	119±17Env	Average training score for last 10 episodes				CartPole-v0	193±0.6	190±10.7	194±9.7	-	-CartPole-v1	133±12.1	498±3.9	500±0.3	348±25.1	429±46.0Acrobot-v1	-97±5.3	-95±4.0	-91±2.5	-96±5.3	-95±7.8LunarLander-v2	231±6.5	219±8.7	223±6.1	234±4.4	211±20.4In all environments, our DQfDD-BC method requires the smallest number of episodes to stop train-ing and obtains almost the highest final Performance, which demonstrates the significant advantages6Under review as a conference paper at ICLR 2021of learning speed and performanCe. It needs to be pointed out that the termination Condition ofCartPole-v0 is muCh easier to be satisfied Compared to the CartPole-v1. In some luCky episodes, thetermination Condition Can be triggered in CartPole-v0, whiCh is almost impossible in CartPole-v1.
