Table 1: Performance on CIFAR-100 using different configurations of AutoSampling and baselines.
Table 2: Experiments on CIFAR-10.	Table 3: Experiments on ImageNet.
Table 4: Static vs dynamic sampling schedule on CIFAR-100 (%)Network	Sampling Type			UNIFORM	STATIC	DynamicResnet1 8	78.46±0.035	78.80±0.007	79.44±0.020Resnet50	79.70±0.023	80.21±0.014	81.53±0.088ImageNet, the mixture exploration outperforms the random exploration by a clear margin. We alsocompare our AutoSampling with some recent non-uniform sampling methods on CIFAR-100, whichcan be found in Appendix A.2.
Table 5: Transfer of sampling distributions learned by three model structures to ResNet-50 onCIFAR-100 (%). UNIFORM denotes the baseline result using uniform sampling distribution.
Table 6: Comparisons between AutoSampling and existing sampling methods on CIFAR-100Methods	Network	Baseline (%)	With method (%)	Improvement (%)DLIS	WRN-28-2	66.0	68.0	2.0AutoSampling (ours)	WRN-28-2	73.37±1.09	76.24±1.02	2.87RAIS	ResNet18	76.4	76.4	0.0AutoSampling (ours)	ResNet18	78.46±0.035	79.44±0.020	0.98A.3 Comparison between learned sampling schedules and data loss valuesTo further interpret the learned sampling schedules, we compare the sampling frequency of eachtraining image and its loss values in different epochs during training of CIFAR-100 with ResNet-18.
