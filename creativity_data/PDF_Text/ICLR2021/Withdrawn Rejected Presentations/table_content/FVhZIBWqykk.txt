Table 1: ROUGE F1 score and MoverScore (1-gram and 2-grams) results on CNN/DM. We alsoreport the size of pretraining data (P-Data) and parameters (Params) of each model.
Table 2: ROUGE Scores and MoverScores resultson the NYT50 summarization dataset.
Table 3: CIDEr results based on two trainingregimes (Cross-Entropy and Self-Critical) onthe Stanford Image-Paragraph dataset.
Table 5: Effective coverage entropy H, repetition rate and BLEU score on subsets with short andlong source sentences on English-German newstest2014.
Table 4: BLEU scores and statisti-cal confidence (Koehn, 2004) on WMTnewstest2014 for English-German andEnglish-French translation tasks.
Table 6: N-gram overlaps with lead-3.
Table 7: Human evaluation results on Represen-tativeness, Readability and Factual Correctness.
Table 8: Summarization dataset statistics.
Table 9: Ablation study. All the models have the same settings as the PG baseline, i.e., the maximumencoding steps are all set to 400 (thus the difference in ROUGE scores from the first block in Table1 in the main paper.
Table 10:	The first example from the CNN/DM test set showing the outputs ofPG + Cov. + Trigram-blocking and our model.
Table 11:	The second example from the CNN/DM test set showing the outputs of PG + Cov. +Trigram-blocking and our model. Highlighted spans are the phrases whose lengths are equal to orlonger than 3 tokens and are copied verbatim from the source document.
Table 12:	The first example from the CNN/DM test set showing the outputs of the BARTSum andBARTSum + DyDim.
Table 13:	The second example from the CNN/DM test set showing the outputs of the BARTSumand BARTSum + DyDim.
Table 14: Novel n-gram percentage onCNN/DM (PG-based models).
Table 15: Novel n-gram percentage on NYT(BERT-based models).
