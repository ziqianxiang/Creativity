Table 1: Machine Learning model HyperparametersMachine Learning Models	Best Hyperparameters	Hyperperameter boundXGBoost Random forest Bi-LSTM	N_estimators:800, DePth:5 N_estimators:700, Depth:4 Layers:2(Nodes: 128, 64), OPtimizer:Adam, DroPoUt:0.2	N_estimators:400-1000, DePth:3-7 N_estimators:400-1000, DePth:3-7 Layers:2 - 4 , Nodes: 32- 512, DroPout: 0.1 - 0.53	ExperimentsWe have approached the categorization of skills into multiple competency groups as a multi-labelclassification problem. We have created our training data at skill X competency group level i.e. foreach skill we will have 40 rows, corresponding to each competency group. For each skill-grouppair, we have tried to predict the probability of that skill belonging to that group using classifiermodels like XGBoost, Random Forest, and Bi-LSTM. Pairs of models which were compared and hada statistically significant difference in the performance are highlighted with a star in Table 3. Thedetails of all these experiments are given below.
Table 2: Machine Learning model training timeMachine Learning Models Training Time (in seconds)XGBoost	122Random forest	87Bi-LSTM	167K-means vs spectral clustering: In this experiment, we tried to see the effect of adding cluster-based features generated using K-means and spectral clustering on SkillBERT embedding. Forthis comparison, we applied XGBoost on the cluster labels and the features used in the previousexperiment where we compared different embedding approaches.
Table 3: Evaluation of result on different embedding models and feature setModel	Precision	Recall	F1-score	Class 0	Class 1	Class 0	Class 1	Class 0	Class 1*XGBoost + pre-trained BERT	98.83%	51.54%	95.85%	74.26%	97.21%	60.84%*XGBoost + Word2vec	98.06%	68.34%	97.36%	65.21%	96.53%	66.73%XGBoost + SkillBERT	99.32%	96.65%	99.47%	84.82%	99.39%	90.35%XGBoost + SkillBERT + K-means	99.27%	96.92%	99.54%	85.24%	99.40%	90.70%Random forest + SkillBERT + spectral clustering	99.28%	95.15 %	99.50%	83.48%	99.39%	88.93%XGBoost + SkillBERT + spectral clustering	99.35%	97.23%	99.48%	85.09%	99.41%	90.76%*Bi-LSTM + SkillBERT + spectral clustering	99.26%	95.86%	99.57%	86.43%	99.42%	90.90%4 ResultsResults shown in Table 3 conclude that SkillBERT improved the performance of the classificationmodel over Word2vec and pre-trained BERT. Use of XGBoost with SkillBERT based featuresgive F1-score of 90.35% for class 1 as compared to 60.83% and 66.73% of pre-trained BERT andWord2vec based features. Use of different machine learning (XGBoost and Random Forest), deeplearning (Bi-LSTM) algorithms, and clustering-based features (K-means and spectral clustering) ontop of SkillBERT is not making a statistically significant difference and the results are very similar.
Table 4: Core vs fringe skill classifier resultsClass 0	Precision Class 1	Class 2	Class 0	Recall Class 1	Class 2	Class 0	F1-score Class 1	Class 299.07%	93.19%	99.76%	99.74%	78.28%	62.45%	99.40%	85.08%	76.81%5 ConclusionIn this paper, we have addressed the problem of recruiters manually going through thousands ofapplications to find a suitable applicant for the posted job. To reduce the manual intervention, amulti-label skill classification model is developed which can classify skills into competency groupsand hence, helps in quick mapping of relevant applications to a job. For skill representation, differentword embedding models like Word2vec and BERT are used and comparison among classificationresults of different machine learning models is shown. Additionally, features like TFIDF, clusteringlabels, and similarity-based features are explored for better classification of skills. We trained BERTon a domain-specific dataset and a significant improvement is noticed while comparing the resultswith pre-trained BERT and Word2vec.
Table 5: Result for different Number of top skills similarity values in feature set (In this experiment,all the features mentioned in the experiment section "SkillBERT vs Word2vec vs Pre-trained BERT"were used and only the number of skills used for similarity value calculation were varied. As aclassifier we used XGBoost)No. of skills used	Precision		Recall		F1-score		Class 0	Class 1	Class 0	Class 1	Class 0	Class 1Top 1 skill	99.22%	95.15%	98.89%	83.92%	99.05%	89.18%Top 2 skills	99.27%	96.10%	99.26%	84.10%	99.26%	89.70%Top 3 skills	99.32%	96.65%	99.47%	84.82%	99.39%	90.35%Top 4 skills	99.21%	95.56%	99.40%	84.69%	99.30%	89.80%â‘ n-E>u9s'lljFigure 7: Scatter plot of eigenvalues to determine number of eigenvectors and clusters in spectralclusteringTable 6: Result for different embedding size (In this experiment, XGBoost was used as a classifierand bert-prob was used along with emdeddings of different sizes as independent variable. No otherfeature apart from these was used)SkillBERT embedding size	Precision		Recall		F1-score		Class 0	Class 1	Class 0	Class 1	Class 0	Class 132	98.12%	91.65%	95.47%	80.12%	96.78%	85.50%64	98.32%	91.80%	97.26%	81.10%	97.79%	86.12%
Table 6: Result for different embedding size (In this experiment, XGBoost was used as a classifierand bert-prob was used along with emdeddings of different sizes as independent variable. No otherfeature apart from these was used)SkillBERT embedding size	Precision		Recall		F1-score		Class 0	Class 1	Class 0	Class 1	Class 0	Class 132	98.12%	91.65%	95.47%	80.12%	96.78%	85.50%64	98.32%	91.80%	97.26%	81.10%	97.79%	86.12%128	99.12%	92.65%	97.47%	83.80%	98.29%	88.00%256	99.12%	92.56%	97.40%	83.79%	98.25%	87.96%11Under review as a conference paper at ICLR 2021Figure 8: Elbow method graph to determine the number of clusters in K-means clusteringTable 7: Examples of some candidate and job profilesCandidate or Job	Skill set	Probable competency groupsCandidate1	Design, knockoutjs, Corel draw	Tool design, mechanical design, front end, web developmentCandidate2	Statistical modeling, statistical process control	Statistics, production operationsJob1	Analytical skills, project execution, accounting	Financial operations, business analytics, statistics, accountsJob2	Digital marketing, cash management, ms office, ms excel, ms word, tally	Taxation, banking, statisticsA.3 SkillBERT trainingThe dataset used for training the SkillBERT model can be downloaded from here. It contains the list
Table 7: Examples of some candidate and job profilesCandidate or Job	Skill set	Probable competency groupsCandidate1	Design, knockoutjs, Corel draw	Tool design, mechanical design, front end, web developmentCandidate2	Statistical modeling, statistical process control	Statistics, production operationsJob1	Analytical skills, project execution, accounting	Financial operations, business analytics, statistics, accountsJob2	Digital marketing, cash management, ms office, ms excel, ms word, tally	Taxation, banking, statisticsA.3 SkillBERT trainingThe dataset used for training the SkillBERT model can be downloaded from here. It contains the listof skills present in job requisitions. Table 7 contains examples of some candidate and job profiles. Weleveraged Bert-Base architecture on the job-skill data to generate embeddings of size 768, details of itcan be found here. Finally, the embeddings generated using the SkillBERT model can be downloadedhere.
Table 8: Feature DescriptionFeature Name	Feature Type	Dimensionalitybert_0 - bert_127	SkillBERT Embedding	128bert-prob	SkillBERT Embedding	10-34	Spectral clustering label	35value1-value3	skill-skill similarity	3tf-idf	TFIDF	1bert_grp_sim	skill-group similarity	1core_skill_count,fringe_skill _count	group based feature	2DependentCompetency Group (40)autocad0.1-0.3Figure 9: Data format used for creating bert_ prob featurefinance0.310.90.8 0.21A.5 FeaturesThe details of features used in the training of Bi-LSTM model, which gave us the best performanceare given in Table 8 .
