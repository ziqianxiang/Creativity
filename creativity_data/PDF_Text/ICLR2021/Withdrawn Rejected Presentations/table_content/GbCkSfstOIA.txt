Table 1: Test performance on MNIST dataset with the different number of labeled data.
Table 2: Performance comparison of USPS, ETH-80 and COIL-100	USPS	ETH-80	COIL-100	Mean±STD	Mean±STD	Mean±S T DGFHF (Zhu et al., 2003)	72.57 (±8.72)	71.86 (±5.50)	71.86 (±5.50)LLGC (Zhou et al., 2004)	85.34 (±6.52)	81.62 (±2.71)	86.36 (±3.78)LNP (Wang and Zhang, 2007)	86.57 (±6.25)	80.86 (±2.68)	86.97 (±3.49)SLP (Nie et al., 2010)	85.89 (±5.25)	80.30 (±2.89)	87.36 (±3.38)PN-LP (Zoidi et al., 2016)	80.23 (±6.24)	72.41 (±4.42)	87.18 (±3.83)CD-LNP (Zhang et al., 2015a)	73.47 (±9.22)	70.11 (±4.83)	82.91 (±4.81)SparseNP (Zhang et al., 2015c)	89.93 (±2.42)	82.81 (±2.02)	76.36 (±5.34)ProjLP (Zhang et al., 2015b)	89.68 (±3.22)	82.39 (±1.95)	90.41 (±3.10)AdaptiveNP (Jia et al., 2016)	82.37 (±5.99)	73.29 (±4.34)	78.42 (±4.54)ALP-TMR (Zhang et al., 2020)	91.12(±3.04)	82.26 (±1.95)	91.51 (±4.13)MCMC (ours)	94.28 (±2.38)-	93.38 (±5.91)	94.74 (±3.18)6	ConclusionIn this paper, we proposed a new approach, which we called Maximum Cluster Margin classifier(MCMC), for semi-supervised learning. The key idea is that we define a novel loss function forclustering results. In particular, the quality of embedding latent space plays an important role forsemi-supervised learning. In our embedding latent space, we guide our network to gather the sampleswhich should be in the same categories. Simultaneously, pushing these clusters as far as possible, and
