Table 1: Max Average Return over 10 trials of 2M time steps (4M for Ant-v1). First two maximumvalues for each task are bolded. Â± corresponds to half a std over trials.
Table 2: Common hyperparameter choices of PPO and PPO-PeVFA.
Table 3: Training details for PPO-PeVFA, including value approximation of historical policies andpolicy representation learning. CL is abbreviation for Contrastive Learning and AUX is for auxiliaryloss of action prediction. In our experiments, grid search is performed for the best hyperparamterconfiguration regarding terms with multiple alternatives (i.e., {}).
