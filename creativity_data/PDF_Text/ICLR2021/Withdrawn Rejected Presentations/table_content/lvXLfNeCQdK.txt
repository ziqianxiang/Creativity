Table 1: Test errors (Standard / PGD / Verified error) of IBP, CROWN-IBP (β = 1), CAP, andOURS on MNIST, CIFAR-10, and SVHN. Bold and underline numbers are the first and secondlowest verified error.
Table 2: Test errors (Standard / PGD / Verified error) of OURS and CROWN-IBP1→0 on CIFAR-10.
Table 3: Performance (in terms of errors) of the variants of CROWN-IBP (β = 1). Note that 0/0.25,0/0.5, and CAP-IBP start with looser bounds but they have more smooth landscape, which leads toa better performance than CROWN-IBP (β = 1) (highlighted with underline).
Table 4: Test errors (Standard / Verified error) compared to the best errors reported in the literature.
Table 5: Test errors (Standard / PGD / Verified error) of IBP, CROWN-IBP (β = 1), CAP, andOURS on MNIST, CIFAR-10, and SVHN. See Appendix A for all the other settings, same as inTable 1. Bold and underline numbers are the first and second lowest verified error.
Table 6: Test errors of OURS with different β- and κ-scheduling on MNIST and CIFAR-10.
Table 7: Test errors of OURS with different numbers of gradient update steps in (17) on CIFAR-10.
Table 8: Comparison of the performance (Standard / PGD / Verified error) depending on varioustrain . Here, we use κ-scheduling from 0 to 0.
Table 9: Comparison of the performance (Standard / PGD / Verified error) of the models trained withtrain and 1.1train. Here, we use κ-scheduling from 0 to 0.
