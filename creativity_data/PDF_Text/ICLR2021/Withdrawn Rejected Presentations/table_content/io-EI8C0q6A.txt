Table 1: CommonVoice phoneme error rate (PER). Models are pretrained on either one language(pt = 1) or 10 languages (pt = 10); and fine-tuned on each language (ft = 1) or all languages(ft = 10). D indicates the pretraining data, LS for English LibriSpeech (100h or 360h), BBLall forBABEL (1070h), CVEn for English CommonVoice (1350h), CVmo for monolingual (see number ofpretraining hours per language) and CVall for multilingual (1350h). Languages can be high-resource(es, fr, it) or low-resource (e.g. ky, sv,tr, tt). Baseline results * are from Riviere et al. (2020).
Table 2: BABEL results using character error rate (CER) on in-pretraining languages. Baselineresults are from Cho et al. (2018) and use the same amount of data as our multilingual models.
Table 3: BABEL results on out-of-pretraininglanguages (CER). XLSR-10 provides strongrepresentations for languages not seen duringpretraining, outperforming monolingual modelspretrained specifically on these languages.
Table 4: BABEL results on out-of-pretraininglanguages using word error rate (WER).
Table 5: Impact of language similarity on cross-lingual transfer. We simulate a low-resourcelanguage scenario by using only 5 hours of Italian CommonVoice data and add 50 hours from anotherlanguage for pretraining. We fine-tune on 1 hour of Italian supervised data.
