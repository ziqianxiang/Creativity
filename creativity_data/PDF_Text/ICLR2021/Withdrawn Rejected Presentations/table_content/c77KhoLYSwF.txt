Table 1: Various experimental designs used in data poisoning research.
Table 2: Benchmark success rates (%) (best in each column is in bold).
Table 3: Hyperparameters.
Table 4: Baseline performance.
Table 5: Data normalization and augmentation + ADAM.
Table 6: Data normalization and augmentation + SGD.
Table 7: ReSNet-18 victims.
Table 8: Poisons crafted with ε = 8/255.
Table 9: TranSfer learned victims.
Table 10: Success rates (%) with varying dataset sizes and number of poisons.
Table 11: Success rates (%) with varying dataset size in the benchmark setting.
Table 12: Black-box victim.		Attack	Success Rate (%)	Diff. From Baseline (%)FC	4.00 ± 5.00	-88.00CP	16.00 ± 3.67	-72.00CLBD	86.00 ± 3.47	0.00HTBD	2.00 ± 5.00	-67.00Table 13: Success with specific class pairs.
Table 13: Success with specific class pairs.
Table 14: Success on flipped targets.
Table 15: Success rates (%) of backdoor attacks with varying patch sizes.
Table 16: CIFAR-10models.
Table 18: TinyImageNet models (first 100 classes).
Table 19: TinyImageNet models (all classes).
Table 20: CIFAR-10 models With TrainSet SiZe 2500.
Table 21: Success rates (%) of backdoor attacks with swapped patch images.
Table 22: CIFAR-10 Transfer learning benchmark tests with varying budget.
Table 23: Google Images case study.
Table 24: Complete CIFAR-10 transfer learning benchmark results.
Table 25: Complete CIFAR-10 from-Scratch benchmark results.
Table 26: Complete TinyImageNet benchmark results.
