Table 1: Details of Tiingo and Stockwits datasetsDataset Nodes Avg hyperedges Training days Validation days Testing daysTiggo	91	691	529	151	76Stocktwits	91	177.1	712	203	103direction of spectral methods, considering different subsets of nodes in the same hyperedge mayhave different structural importance, Li & Milenkovic (2017) proposed inhomogeneous hypergraphpartitioning model to assign different costs to different hyperedge cuts. Li & Milenkovic (2018)defined the notion of p-Laplacians which constitute the basis of new spectral hypergraph clusteringmethods. Chan et al. (2018) considered a stochastic diffusion process and introduces a new hyper-graph Laplacian operator generalizing the Laplacian matrix of graphs. Yadati et al. (2019) simplifiedhypergedges into simple edges with mediators and demonstrate the effectiveness through detailedexperiments. The other way, spatial methods, Gilmer et al. (2017) raised a Message Passing NeuralNetworks (MPNN) framework which learns a message-passing algorithm and aggregate features fornode representation. Feng et al. (2019b) introduced the first hypergraph deep learning method hy-pergraph neural network (HGNN). However, most of the existing works focus on static hypergraphstructure which has little effort on optimizing the hypergraph structure during the learning process.
Table 2: Performance on Tiingo and Stocktwits dataset with hidden size 128 of TE								Dataset	Tiingo				Stocktwits			Models	MAE	MAPE	MSE	Promote	MAE	MAPE	MSE	PromoteDA-RNN	0.1869	0.4786	0.0467	55.37%	0.1855	1.0950	0.0384	71.55%HAN	0.1466	0.3017	0.0548	42.43%	0.1272	0.5517	0.0414	61.29%RSR	0.0946	0.2596	0.0183	7.57%	0.0893	0.1870	0.0158	18.19%DHGCN	0.0984	0.2754	0.0216	15.08%	0.0845	0.1782	0.0156	14.86%DyHCN	0.0873	0.2533	0.0160		0.0732	0.1660	0.0118	of different models for prediction, we use three evaluation metrics, the mean squared error (MSE),								mean absolute error (MAE) and mean absolute percentage error (MAPE).								Baselines To evaluate the result of our proposed DyHCN model, we compare the experiment resultwith the traditional time series, NLP-based, graph-based and hypergraph-based model: 1) DA-RNNHsu et al. (2009) One of the state-of-the-art models for time series prediction. 2) HAN Hu et al.
Table 3: Performance on Tiingo and Stocktwits dataset with hidden size 128 of TEDataset		Tiingo		Stocktwits		Models	MAE	MAPE	MSE	MAE	MAPE	MSEDyHCN(no inner)	0.1303	0.3988	0.0347	0.0745	0.1667	0.0122DyHCN(no outer)	0.0887	0.2613	0.0165	0.0732	0.1695	0.0120DyHCN(no TE)	0.0982	0.2754	0.0218	0.0842	0.1784	0.0160HGCN(with TE)Feng et al. (2019b)	0.2943	0.8025	0.1771	0.2943	0.8025	0.1771DyHCN	0.0873	0.2533	0.0160	0.0732	0.1660	0.0118of results with different hidden size, the performance has no significant difference, indicating thatthe hidden size of LSTM is not the major factor for model prediction.
