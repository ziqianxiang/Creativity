Table 1:fa hi hu it nl Pt ro Sk Sl Sv Vi ZhX	XX	XXXXXXXX XX	XXXXSeventeen target languages and five tasks used in our experiments. English is used as thesource language. ar: Arabic, bg: Bulgarian, da: Danish, de: German, es: Spanish, fa: Persian, hi:Hindi, hu: Hungarian, it: Italian, nl: Dutch, pt: Portuguese, ro: Romanian, sk: Slovak, sl: Slovene,sv: Swedish, vi: Vietnamese, zh: Chinese.
Table 2: Model scores selected based on LMS for NER, POS, QA, RE, and ARL. En-Dev /Pivot-Dev / 100-Target / All-Target: model selection based on the highest F1 of English dev set/ Pivot language dev set (pivot language in bracket) / 100 target language dev set examples / tar-get language dev set. LMS: model selection based on the highest scores for the target language:arg maxm s(m, ltarget); “# All-Target” is the number of labeled target-language sentences used formodel selection in the All-Target oracle.
Table 3: Model-specific feature analysis. We use mBERT models in the meta-dev set for analysis.
Table 4: Language embedding analysis across lang2vec, syntax, and no language embedding.
Table 5: Statistics of the imbalanced dataset. Number of instances and the total positive/negativeratio.
Table 6: F1 scores for relation extraction and argument role labeling on the test set. En-Dev/100-Target/All-Target: model selection based on the highest F1 of English dev set/100 target languagedev set examples/target language dev set. Ours: model selection based on the highest scores for thetarget language: arg maxm s(m, ltarget).
Table 7: Model scores (mean ± sd) selected based on LMS for POS and QA over 5 runs. Boldindicates the best score and underline indicates the second best. * indicates the LMS/Pivot-Dev isstatistically significantly (p-value ≤ 0.05) higher than En-Dev.
Table 8: XLM-RoBERTa experiment: F1 of relation extraction and argument role labeling on theimbalanced dataset. Model selection results are based on XLM-RoBERTa-base models in the meta-test set.
Table 9: Multi-task analysis using additional training data in the target language from another task.
Table 10: Model selection results for QA (F1). A varying number of models to train LMS from 6 to120.
