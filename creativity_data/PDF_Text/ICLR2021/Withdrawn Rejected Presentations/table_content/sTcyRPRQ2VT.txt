Table 1: Examples of Normal, EntityPairOveral and SingleEntityOveralp text snippets	TyPe	Sentence	TripletsNormal	London is the capital of UK	(London, CaPitaLof, UK)EntityPairOverlap	London is the capital of UK and its largest city	(London, CaPitaLof, UK) (London, largest_city_of, UK)SingleEntityOverlap	HeathroW airport is located at London which is the capital ofUK	(Heathrow airport, located_at, London) (London, CaPitaLof, UK)To achieve this they follow an encoder-decoder based approach by having an encoder neural networkto read sequentially each xs ∈ Xhs = encoder(hs-1, xs)where hs is the state of the encoder at time s and a decoder neural network to produce each yt ∈ Ygiven the current state gt and the previous predicted symbol yt-1g1 = hsgt = decoder(gt-1 , yt-1)P(yt|y1, y2, ...,yt-1,X) = softmax(gt)Encoder and decoders can be recurrent neural networks (Cho et al., 2014) or convolutional basedneural networks (Gehring et al., 2017). In addition, an attention mechanism can also be incorporatedinto the encoder (Bahdanau et al., 2014; Luong et al., 2015) for further boosting of the model’sperformance. Lately, Transformer architectures (Vaswani et al., 2017; Devlin et al., 2018; Liu et al.,2019; Radford et al., 2018) , a family of models whose components are entirely made up of attentionlayers, linear layers and batch normalization layers, have established themselves as the state of the artfor sequence modeling, outperforming the typically recurrent based components. Seq2seq models
Table 2: Datasets’s statisticsDataset	# instances	# entity types	# relation types	size of “relation” languageWebNLG	-23794-	45	47	70-NYT-	-70029-	12	28	31DocRED	30289	6	—	96	—	511	—of methods that can combine the NER and relation extractions tasks into a single model (Zhenget al., 2017; Zeng et al., 2018; Fu et al., 2019). Our task is related to the relation extraction taskas we also interested in extraction of relation types from a text. Nevertheless, as we place our taskin the beginning of the KG generation process, we do not have available any information about theposition of the entities in the text neither we attempt to identify them. We solely focus on the relationtypes and the acquisition of the domain’s metagraph in order to utilize the proper models in the nextsteps of the pipeline, which also means to utilize the proper NER or relation extraction models forthe domain of interest.
Table 3: Comparison of CNN model, RNN model and Transformer-based methods on WebNLG,NYT and DocRED datasets. *The architecture of the CNN and RNN models has been modifiedto exclude the component which provides information about the position of the entities in the textsnippet.
Table 4: Evaluation of metagraph’s reconstruction on the three datasets using CNN, RNN andTransformer-based models. *The architecture of the CNN and RNN models has been modifiedto exclude the component which provides information about the position of the entities in the textsnippet.
Table 5: Performance of CNN, RNN and Transformer-based methods on instances from NYTdatasets with more than 1 relation in them. *The architecture of the CNN and RNN models hasbeen modified to exclude the component which provides information about the position of the enti-ties in the text snippet.
Table 6: Comparison of CNN, RNN and Transformer-based methods on WebNLG, NYT and Do-cRED datasets for the relation type extractiont task. *The architecture of the CNN and RNN modelshas been modified to exclude the component which provides information about the position of theentities in the text snippet.
Table 7: Evaluation of metagraph’s reconstruction on WebNLG dataset using CNN, RNN andTransformer-based models. *The architecture of the CNN and RNN models has been modifiedto exclude the component which provides information about the position of the entities in the textsnippet.
