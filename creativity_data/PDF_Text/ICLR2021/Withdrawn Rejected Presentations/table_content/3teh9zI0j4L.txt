Table 1: Samples of a MLE-trained transformer LM when fed with different types of prefixes. Theprompt is “= Frank Headlam = Air Vice Marshal Frank Headlam , CB , CBE ( 15 July 1914 ”, whichis the beginning of an article in the wiki-103 test set. The generation length is fixed to 30. To savespace, we omit the long prefix and only show the last few words. The examples are not cherry-picked.
Table 2: EB-M measurements with BLEU on the wiki-103 dataset. MLS refers to the LSTM model,and MTF refers to the transformer model. BLEU(M |H) refers to BLEU(PMW|lH+1:l+lgen, PDWl+1:l+lgen).
Table 3: EB-C measurements with dJS for the transformer synthetic setting. We have also tried longerprefix length (e.g., 80 or 100), and get very similar observations.
Table 4: Human ratings of length-30 generations with prefixes of different length from PD or PM .
Table 5: Samples of a MLE-trained LSTM LM when fed with different types of prefixes. The promptis “the development of women ’s football in africa faces several challenges , including limited accessto education , poverty amongst”. The generation length is fixed to 30. To save space, we omit thelong prefix and only show the last few words.
Table 6: Samples of a MLE-trained transformer LM when fed with different types of prefixes. Theprompt is “= Angel of Death ( Slayer song ) = ” Angel of Death ” is the opening track”. Thegeneration length is fixed to 30.
Table 7: EB-C measurements with dTV as the metric for the transformer synthetic setting.
Table 8: EB-M measurements with different metrics for the transformer LM on the wiki-103 dataset.
