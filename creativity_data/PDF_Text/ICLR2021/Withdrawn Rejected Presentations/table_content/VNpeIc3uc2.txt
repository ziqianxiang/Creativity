Table 1: CNN architecture details (width multiplier = 2)Number ofCells	Max. Long-Range Link Candidates a)	Depth	Width Multiplier3	[10,35,50] [20,45,75] [30,50,100] [40,60,120] [50,70,145]	31	23	[20,40,70] [30,50,100] [40,80,125] [50,105,150] [60,130,170]	40	23	[25,50,90] [35,80,125] [50,105,150] [70,130,170] [90,150,210]	49	23	[30,80,117] [50,110,150] [70,140,200] [90,175,250] [110,215,300]	64	2Table 2: CNN architecture details (width multiplier = 1)Number ofCells	Max. Long-Range Link Candidates a)	Depth	Width Multiplier3	[5,8,12] [10,30,50] [30,40,70] [41,61,91] [50,90,110]	31	13	[5,9,12] [11,31,51] [31,41,71] [41,62,92] [50,90,109]	40	13	[5,10,11] [11,31,52] [31,41,73] [42,62,93] [50,90,109]	49	13	[5,10,12] [11,32,53] [31,42,74] [42,62,94] [49,90,110]	64	1Again, we conduct several experiments for different {dc, wc, tc} values which yield many randomCNN architectures. The random long-range link creation process is the same as that in MLPs and, forCNN experiments, we have repeated all experiments three times with different random seeds. Specificnumbers used for {dc, wc, tc} are given in Tables 1, 2, and 3. Each row in all tables represents adifferent {dc, wc, tc} configuration. Of note, all CNNs use ReLU activation function and Batch Normlayers.
Table 2: CNN architecture details (width multiplier = 1)Number ofCells	Max. Long-Range Link Candidates a)	Depth	Width Multiplier3	[5,8,12] [10,30,50] [30,40,70] [41,61,91] [50,90,110]	31	13	[5,9,12] [11,31,51] [31,41,71] [41,62,92] [50,90,109]	40	13	[5,10,11] [11,31,52] [31,41,73] [42,62,93] [50,90,109]	49	13	[5,10,12] [11,32,53] [31,42,74] [42,62,94] [49,90,110]	64	1Again, we conduct several experiments for different {dc, wc, tc} values which yield many randomCNN architectures. The random long-range link creation process is the same as that in MLPs and, forCNN experiments, we have repeated all experiments three times with different random seeds. Specificnumbers used for {dc, wc, tc} are given in Tables 1, 2, and 3. Each row in all tables represents adifferent {dc, wc, tc} configuration. Of note, all CNNs use ReLU activation function and Batch Normlayers.
Table 3: CNN architecture details (width multiplier = 3)Number ofCells	Max. Long-Range Link Candidates a)	Depth	Width Multiplier3	[10,30,50] [40,60,90] [70,90,130] [100,120,170] [130,150,210]	31	33	[11,31,51] [42,62,92] [72,93,133] [103,123,173] [133,153,212]	40	33	[11,31,52] [43,63,93] [73,95,135] [104,124,176] [134,154,214]	49	33	[12,32,52] [44,64,95] [76,96,136] [106,126,178] [135,156,216]	64	3H	Additional ResultsH. 1 More MNIST training convergence resultsWe pick two groups of three models each - (X, Y, and Z) - and - (D, E, and F) and plot their trainingaccuracy vs. epochs. Models X and Y have similar NN-Mass but Y has more #Params and depththan X. Model Z has far fewer layers and nearly the same #Params as X, but has higher NN-Mass.
Table 4: Description of our generated Synthetic DatasetsDataset name	Description: Training Set, i ∈ [1,60000]; Test Set, i ∈ [1,12000]Seg20	Feature: [Xi,Xi], Label:匕，Xi = Sample(击[|_击C, |_击C + 1]), Yi = [ 20 C mod2	T .	.	Seg30	FeatUre: [Xi,Xi], Label:匕,Xi = Sample(击[|_击C, |_击C + 1]), Yi = [ 30 Cmod2	Circle^^(Cir^ cle20)	Feature: [Xii, X2i], Label:匕，Xii = Li * cos(rand_num), X2i = Li * Sin(rand_num), Li = Sample(表][赤C,匕=b20Cmod2we quantitatively analyze the above results by generating a linear fit between test accuracy vs.
Table 5: Exploiting NN-Mass for Model Compression on CIFAR-10 Dataset. All our experimentsare reported as mean ± standard deviation of three runs. DARTS results are from [Liu et al. (2018)].
