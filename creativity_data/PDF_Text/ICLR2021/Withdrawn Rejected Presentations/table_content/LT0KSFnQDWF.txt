Table 1: Graph classification accuracy on TUD Dataset. First, Second, Third best methods arehighlighted. For GSN, the best performing structure is shown. * Graph Kernel methods.
Table 2: MAE on ZINC. ‘-EF’: Ad-ditional Edge Features, ‘-r’: randomfeatures. *Our implementation.
Table 3: ROC-AUC on ogbg-molhiv. ‘-AF’: AdditionalFeatures, ‘-VN’: message passing with a virtual node.
Table 4: Comparison between DeepSets and GSN with the same structural featuresDataset	MUTAG	PTC	Proteins	NCI1	Collab	IMDB-B	IMDB-M	ZINC	MOL-HIVDeepSets	93.3±6.9	66.4±6.7	77.8±4.2	80.3 ±2.4	80.9 ±1.6	77.1 ±3.7	53.3 ±3.2	0.288 ±0.003	77.34±1.46# params	3K	2K	3K	10K	30K	51K	68K	366K	3.4MGSN	92.8±7.0	68.2±7.2	77.8±5.6	83.5± 2.0	85.5±1.2	77.8±3.3	54.3±3.3	0.108 ±0.018	77.99±1.00# params	3K	3K	3K	10K	52K	65K	66K	385K	3.3M6	ConclusionIn this paper, we propose a novel way to design structure-aware graph neural networks. Motivatedby the limitations of traditional GNNs to capture important topological properties of the graph,we formulate a message passing scheme enhanced with structural features that are extracted bycounting the appearances of prominent substructures, as domain knowledge suggests. We show boththeoretically and empirically that our construction leads to improved expressive power and attainsstate-of-the-art performance in real-world scenarios. In future work, we will further explore theexpressivity of GSNs as an alternative to the k-WL tests, as well as their generalisation capabilities.
Table 5: MAE on ZINC. Experiments with the same parameter budget (100K)Method	Test MAEGIN*	0.388±0.013MPNN-EF	0.324±0.006GSN	0.193±0.010GSN-EF	0.154±0.002Generalisation: empirical observations: We repeat the experimental evaluation on ZINC usingdifferent fractions of the training set. We compare our best baseline model (MPNN-EF in Table 2)against GSN with the best performing substructure (GSN-EF in Table 2). In Figure 6, we plot thetraining and test errors of both methods. Regarding the training error, GSN consistently performsbetter, following our theoretical analysis on its expressive power. More importantly, GSN manages togeneralise much better even with a small fraction of the training dataset. Observe that GSN requiresonly 20% of the samples to achieve approximately the same test error that MPNN achieves whentrained on the entire training set.
Table 6: Disambiguation scores δ on ZINC for different substructure families and their maximumsize k. Size k = 0 refers to the use of the original node features only.
Table 7: Graph Classification accuracy on various social and biological networks from the TUDDatasets collection https://chrsmrrs.github.io/datasets/. Graph Kernel methodsare denoted with an *. For completeness we also include methods that were evaluated on potentiallydifferent splits. The top three performance scores are highlighted as: First, Second, Third.
Table 8: Chosen hyperparameters for each of the two GSN variants for each dataset.
Table 9: Chosen substructures for ogbg-molhivModel	GSN-AF	GSN-VN	GSN-VN -AFfeatures	edges (GSN-e)	vertices (GSN-v)	edges (GSN-e)substructure type	graphlets	graphlets	graphletssubstructure family	cycles	cycles	cyclesk	12	6	6et({v, u}) = We ∙ ein({u, v}). The baseline model is a modification of GIN that allows for edgefeatures: for each neighbour, the hidden representation is added to an embedding of its associatededge feature. Then the result is passed through a ReLU non-linearity which produces the neighbour’smessage. Formally, the aggregation is as follows:ht+1(v) = UPt+1 卜t(v) + X σ (ht(u) + et({v, u})))	(13)A stronger baseline is also proposed by the authors: in order to allow global information to bebroadcasted to the nodes, a virtual node takes part in the message passing (-VN setting in Table 3 ofthe main paper). The virtual node representation, denoted as Gt, is initialised as a zero vector G0and then Message Passing proceeds as follows:hV = ht(v) + Gt, ht+1(v) = UPt+1ht(v) + X σ(ht(u) + et({v,(14)Gt+1 = MLPt+1Gt+	hht(u)
Table 10: ROC-AUC on ogbg-molhiv. Performance with different substructures and skip connec-tions for the structural identifiersMethodGSN (k = 6)GSN (k = 8)GSN- skip (k = 6)GSN- skip (k = 8)Training Validation Test94.29 ± 3.3894.33 ± 2.3893.77 ± 3.3893.97 ± 2.7086.58 ± 0.8485.59 ± 0.8286.44±1.0985.30 ± 1.0177.99 ±± 1.0078.20 ± 1.6978.07±0.8378.55 ± 1.25
