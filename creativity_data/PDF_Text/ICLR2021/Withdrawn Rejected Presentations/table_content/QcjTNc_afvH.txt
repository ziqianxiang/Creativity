Table 1: Comparison on reconstruction and generation quality across different resolutions of thestylemap. Mean squared error (MSE) is in [-1, 1] scale and learned perceptual image patch similar-ity (LPIPS) measures reconstruction accuracy with the encoder. Frechet Inception Distance (FID)measures the quality of randomly generated samples from the standard Gaussian distribution. Thehigher resolution helps accurate reconstruction, validating the effectiveness of stylemap. We observethat 8 Ã— 8 stylemap already provides accurate enough reconstruction and accuracy gain and after-ward improvements get visually negligible. Although FID varies differently across datasets, possiblydue to the different contextual relationship between locations for generation, the stylemap does notseriously harm quality of the images.
Table 2: Comparison with the baselines for real image projection. Runtime covers the end-to-endinterval of projection and generation in seconds. Protocols for MSE and LPIPS are the same withTable 1. FIDlerp measures quality of the images interpolated on the style space as a proxy for potentialquality of the manipulated images. Our method allows real-time manipulation of real images whileachieving the best reconstruction accuracy and the best quality of the interpolated images. AlthoughImage2StyleGAN produces the smallest reconstruction error, it suffers from minutes of runtime andpoor interpolation quality which are not suitable for a practical editing. Its flaws can be found in thefigure: deviating identity, odd changes on neck and background, and sudden changes on eyes. SEANis not applicable to AFHQ due to no existence of segmentation masks for training. The horizontalline between methods separates optimization-based methods and encoder-based methods.
Table 3: Comparison with the baselines for local image editing. Average precision (AP) is measuredwith the binary classifier trained on real and fake images (Wang et al., 2020). The lower AP indicatesthat manipulated images are more indistinguishable from real images. MSEsrc and MSEref measureerror from the source image outside the mask and from the reference image inside the mask, respec-tively. Compared with the baselines, our method seamlessly composes the two images giving betterreconstructions.
Table 4: Losses for training each network.
