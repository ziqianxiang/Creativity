Table 1: Mean 10-fold cross validation accuracy on five graph datasets for kernel and unsupervisedmethodsRW (Gartner et al., 2003)	50.7 ±0.3	34.7 ±0.2	-	74.2 ±0.4	-SP (Borgwardt & Kriegel, 2005)	55.6 ±0.2	38.0 ±0.3	64.1 ±0.1	75.07 ±0.5	78.7 ±3.9GK (Shervashidze et al., 2009)	65.9 ±1.0	43.9 ±0.4	77.3 ±0.2	71.67 ±0.6	74.9 ±3.8WL (Shervashidze et al., 2011)	72.3 ±3.4	47.0 ±0.5	68.8 ±0.4	72.92 ±0.6	76.4 ±2.4DGK (Yanardag & Vishwanathan, 2015)	67.0 ±0.6	44.6 ±0.5	78.0 ±0.4	75.7 ±0.5	-MLG (Kondor & Pan, 2016)	66.6 ±0.3	41.2 ±0.0	-	76.3 ±0.7	-GCN (Kipf & Welling, 2017)	74.0 ±3.4	51.9 ±3.8	50.0 ±0.0	76.0 ±3.2	-GAT (Velickovic et al., 2018)	70.5 ±2.3	47.8 ±3.1	85.2 ±3.3	--GIN-0 (Xu et al., 2019b)	75.1 ±5.1	52.3 ±2.8	92.4 ±2.5	76.2 ±2.8	-GIN- (Xu et al., 2019b)	74.3 ±5.1	52.1 ±3.6	92.2 ±2.3	75.9 ±3.8	-VGAE (Kipf & Welling, 2016) SUB2VEC (Adhikari et al., 2018)	64.9±0.38 55.3 ±1.5	38.9±0.46 36.7 ±0.8	- 71.5 ±0.4	72.4±0.42 -	76.3±0.34 -GRAPH2VEC (Narayanan et al., 2017)	71.1 ±0.5	50.4 ±0.9	75.8 ±1.0	73.3 ±2.1	-INFOGRAPH (Sun et al., 2020)	73.0 ±0.9	49.7 ±0.5	82.5 ±1.4	75.0 ±1.3	76.9 ±1.3MVGRL (Hassani & Khasahmadi, 2020)	74.2 ±0.7	51.2 ±0.5	84.5 ±0.6	75.9 ±1.9	78.3 ±1.7ALATION-INVERSE-GCN	73.2 ±1.9	50.4 ±1.3	83.9 ±1.3	75.0 ±1.2	77.8 ±1.3ALATION-GCN	74.2 ±1.4	49.6 ±1.2	84.6 ±1.2	74.9 ±1.2	77.9 ±1.0OURS	76.0 ±1.3	51.5 ±1.4	86.0 ±1.7	76.1 ±1.4	79.1 ±1.5Data and baselines We use five graph classification benchmarks including IMDB-Binary, IMDB-
Table 2: Comparison of different methods on social recommendation tasksDatasets	Ciao		Douban	-	ILS	RMSE	ILS	RMSEsRGCNN (Monti et al., 2017)	1.63%	1.183	6.03%	0.801GC-MC (Berg et al., 2018)	1.09%	1.061	3.90%	0.734GraphRec (Fan et al., 2019)	1.24%	1.062	8.27%	0.754OURS	0.48%	1.071	2.65%	0.745Ablation study We investigate the role of each component in our GDN. We let the three variantsshare the same depth of layers and parameter size. As observed in Table 4, when decoding by pureinverse of GCN, the performance is just comparable to that of GCN decoders and outperformed byour GDN decoders in all 5 datasets, indicating the effectiveness of this hybrid design.
Table 3: The effect of GDN with various graph generation methodsDatasets	MUTAG			PTC-MR			ZINC		-	logp(A|Z)	AUC	AP	log p(A|Z)	AUC	AP	log p(A|Z)	AUC	APVGAE (Kipf & Welling, 2016)	-1.156	0.869	0.645	-1.366	0.566	0.433	-1.033	0.551	0.288VGAE with GDN	-1.114	0.880	0.678	-1.351	0.760	0.602	-0.997	0.862	0.613Graphite (Grover et al., 2019)	-1.140	0.868	0.632	-1.362	0.564	0.437	-1.043	0.559	0.288Graphite with GDN	-1.104	0.882	0.681	-1.347	0.773	0.613	-0.989	0.851	0.593recommendation accuracy, we report Root Mean Squared Error (RMSE). For recommendation di-versification (Ziegler et al., 2005), we report Intra-List Similarity (ILS). For the definition of ILSand detailed hyperparameter settings, please refer to Appendix C.
Table 4: Statistics of the datasets used in graph classificationDatasets	IMDB-BIN	IMDB-MULTI	REDDIT-BIN	PROTEINS	DD(No.Graphs)	1000	1500	2000	1113	1178(No.Classes)	2	3	2	2	2(Avg.Nodes)	19.8	13.0	508.5	39.1	284.3(Avg.Edges)	193.1	65.9	497.8	72.8	715.7Table 5: Statistics of the datasets used in social recommendationDataset	Users	Items	Train/Test Ratings	Rating Density	Social Connections	Social DensityDouban	3,000	3,000	123,202/13,689	1.52%	2,690	0.03%Ciao	7,317	1,000	39,279/16,892	0.77%	111,781	0.21%A Detailed Model ConfigurationFor the unsupervised graph-level representation experiments, we use the original graph structure inthe decoders of the proposed graph autoencoder framework. The C parameter of LIBSVM (Chang &Lin, 2011) is selected from {10-3, 10-2, . . . , 102, 103}. The depth of GNN layers in the encoders isset to 2. The output dimension of the first layer is chosen from {64, 128, 256}. The output dimensionof the second layer is chosen from {16, 32}. The number of clusters K is chosen from {16, 32}. Thenumber of epochs is chosen from [15, 20]. The batch size is set to 1. We set λA = 0 and λX = 1.
Table 5: Statistics of the datasets used in social recommendationDataset	Users	Items	Train/Test Ratings	Rating Density	Social Connections	Social DensityDouban	3,000	3,000	123,202/13,689	1.52%	2,690	0.03%Ciao	7,317	1,000	39,279/16,892	0.77%	111,781	0.21%A Detailed Model ConfigurationFor the unsupervised graph-level representation experiments, we use the original graph structure inthe decoders of the proposed graph autoencoder framework. The C parameter of LIBSVM (Chang &Lin, 2011) is selected from {10-3, 10-2, . . . , 102, 103}. The depth of GNN layers in the encoders isset to 2. The output dimension of the first layer is chosen from {64, 128, 256}. The output dimensionof the second layer is chosen from {16, 32}. The number of clusters K is chosen from {16, 32}. Thenumber of epochs is chosen from [15, 20]. The batch size is set to 1. We set λA = 0 and λX = 1.
Table 6: Statistics of the datasets used in graph generationDatasets	MUTAG	PTC-MR	ZINC(No.Graphs)	188~~	344	5000(Avg.Nodes)	17.9	14.3	23.1(Avg.Edges)	19.8	14.7	49.7Social recommendation Social recommendations assume a social network among users and theseuser-user interactions are helpful in boosting the recommendation performance (Jamali & Ester,2010). As GNNs have been proven to be capable of learning on graph data, recently recommenda-tion methods based on GNNs have shown impressive results on recommendation accuracy. Repre-sentative works include sRGCNN (Monti et al., 2017), GC-MC (Berg et al., 2018), and GraphRec(Fan et al., 2019). Specifically, GC-MC (Berg et al., 2018) models recommendations as link predic-tions using graph autoencoder. Though successful, GC-MC uses an MLP decoder and fails to dealwith noisy ratings (Candes & Plan, 2010).
