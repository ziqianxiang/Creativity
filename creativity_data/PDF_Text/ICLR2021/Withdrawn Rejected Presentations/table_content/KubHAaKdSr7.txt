Table 2: A summary of the best results when modifying 32 facts with constraints on T-REx for various models.
Table 3: Fine-tuning BERT-Base without constraints on the modified supporting evidences DM of T-REx. AMis the accuracy on 32 modified facts from the T-REx benchmark and AF\S is the accuracy on the unmodifiedfacts. The results are averaged over 5 independent runs with standard error in parentheses. RI denotes startingfrom a randomly initialized model with no pretraining. See § 4.4 for the definition of FT and FTM.
Table 4: Comparing the results of finetuning with constraints on the supporting evidence of |M| = 512modified facts with and without the supporting evidences for the unmodified facts in every mini-batch (T-RExbenchmark). We report the results after averaging over 5 independent runs with standard error in parentheses.
Table 5: Results for finetuning different components of a FaE on the |M| = 32 modified facts of T-REx undera range of constraints (FT+FTM setting). ∆Af∖s is the drop in accuracy on unmodified facts. We report theresults with AM closest to the accuracy on the modified facts achieved by the BERT-Large model (77.50%).
Table 6: Results for modifying a pretrained BERT-Base model using kNN-LM on |M| = 32 facts fromT-REx. is defined in Eq. 10, which is the maximum allowable distance for using the nearest neighbor predic-tion. By comparison, if we modify the 0th Transformer block for the same BERT-Base model, we can obtainAF\S=27.78%/23.51%/17.69% and AM=15.63%/58.13%/71.25% with δ =1e-3/2e-3/4e-3, respectively.
