Table 1: Notation of loss terms.
Table 2: Adult DatasetMethod	Error(%)	EOIn-processing Post-processing	16.8 16.9	0.048 0.049Basel: all real	15.9	0.179Base2: all fake	-15.8-	0.141Base3: random	-16.0-	0.180Base4: balance	-15.8-	0.157GFT 一	16.1	0.044Table 3: COMPAS DatasetMethod	Error(%)	EOIn-processing Post-processing	32.9 32.8	0.034 0.039Base1: all real	32.1	0.224Base2: all fake	-32.3-	0.231Base3: random	-32.7-	0.247Base4: balance	-32.0-	0.229GFT 一	32.8	0.028Table 4: CelebA DatasetMethod	Error(%)	EOBase1: all real	21.0	0.426Base2: all fake	-40-	0.610
Table 3: COMPAS DatasetMethod	Error(%)	EOIn-processing Post-processing	32.9 32.8	0.034 0.039Base1: all real	32.1	0.224Base2: all fake	-32.3-	0.231Base3: random	-32.7-	0.247Base4: balance	-32.0-	0.229GFT 一	32.8	0.028Table 4: CelebA DatasetMethod	Error(%)	EOBase1: all real	21.0	0.426Base2: all fake	-40-	0.610Base3: random	-23.5-	0.417Base4: balance	-19.6-	0.362Base5: fix ratio	-207-	0.242Base6: reverse	-19.2-	0.171GFT 一	19.1	0.098We report the maximum among the false positive difference and true positive difference betweenprotected and unprotected groups. We compared our method with exponentiated-gradient reductionbased in-processing algorithm (Agarwal et al., 2018) and score-based post-processing algorithm
Table 4: CelebA DatasetMethod	Error(%)	EOBase1: all real	21.0	0.426Base2: all fake	-40-	0.610Base3: random	-23.5-	0.417Base4: balance	-19.6-	0.362Base5: fix ratio	-207-	0.242Base6: reverse	-19.2-	0.171GFT 一	19.1	0.098We report the maximum among the false positive difference and true positive difference betweenprotected and unprotected groups. We compared our method with exponentiated-gradient reductionbased in-processing algorithm (Agarwal et al., 2018) and score-based post-processing algorithm(Hardt et al., 2016). In addition to these two methods, we also compared the GFT with four differentbaselines. Base1 denotes the model that trains with all original examples, which is also an uncon-strained classifiers. Base2 denotes the model that trains using all counterfactual examples. Base3is the model that trains with a random combination of original and counterfactual examples. Base4is the model trained with a balance combination of original and counterfactual examples, whichguarantees the proportion of protected and unprotected group in the training set to be the same.
Table 5: Our counterfactual generative model architecture. Conv1.x, Conv2.x and Conv3.x denoteconvolution units that may contain multiple convolution layers. E.g., [4×4, 64, 2]×3 denotes 3cascaded convolution layers with 64 filters of size 4×4 and stride 2.
