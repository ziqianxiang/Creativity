Table 1: Summary of human-normalized scores in 49 Atari games. BEBU, BEBU-UCB, BEBU-IDSand OEB3 are trained for 20M frames with RTX-2080Ti GPU for 5 random seeds.
Table 3: Algorithmic Comparison of Related Works	Bonus or Poste- rior Variance	Update Method	Uncertainty CharacterizationEBU (Lee et al., 2019)	-	backward update	-Bootstrapped DQN (Osband et al., 2016)	bootstrapped	on-trajectory update	bootstrapped distributionBEBU (base of our work)	bootstrapped	backward update	bootstrapped distributionOptimistic LSVI (Jin et al., 2020)	closed form	backward update	optimismOEB3 (ours)	bootstrapped	backward update	optimismUBE (O’Donoghue et al., 2018)	closed form	on-trajectory update	posterior samplingBayesian-DQN (Azizzadenesheli et al., 2018)	closed form	on-trajectory update	posterior sampling14Under review as a conference paper at ICLR 2021Algorithm 3 BEBU & BEBU-UCB & BEBU-IDS	1	: Input: Algorithm Type (BEBU, BEBU-UCB, or BEBU-IDS)2	Initialize: replay buffer D, bootstrapped Q-network Q(∙; θ) and target network Q(∙; θ一)3	: Initialize: total training frames H = 20M, current frame h = 04	: while h < H do5	Pick a bootstrapped Q-function to act by sampling k 〜Unif {1,..., K}6	: Reset the environment and receive the initial state s07	: for step i = 0 to Terminal do8	:	if Algorithm type is BEBU then
Table 4: Hyper-parameters of BEBU for MNIST-MazeHyperparameters	Value	Descriptionstate space	28 × 28 × 2	Stacking two images sampled from MNIST dataset with labels according to the agent’s current location.
Table 5: Hyper-parameters of BEBU for Atari gamesHyperparameters	Value	Descriptionstate space	84 × 84 × 4	Stacking 4 recent frames as the input to network.
Table 6: Raw scores for Atari games. Each game is trained for 20M frames with a single RTX-2080TiGPU. Bold scores signify the best score out of all methods.
Table 7: Comparison of scores in Montezuma's Revenge.
