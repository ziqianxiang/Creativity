Table 1: Experimental results of DPP for MNIST dataset using LeNet architecturesNetwork	Model	Remaining parameters (%)	Accuracy (%)	DPP (This work)	1.95	97.90LeNet300-100	(Liu et al., 2020)	2.24	98.03	Non-pruned baseline	100	98.16	DPP (This work)	1.51	99.07LeNet5-Caffe	(Liu et al., 2020)	1.68	98.94	Non-pruned baseline	100	99.184.2 VGG-16 AND MOBILENET V1 ON CIFAR- 1 0 AND CIFAR- 1 00We evaluate DPP for structured pruning feature maps and the fully-connected layers on VGG-16(Simonyan & Zisserman, 2015), which consists of 13 convolutional layers. Additionally, we use twofully-connected layers. For this experiment, we use case (c) for the convolutional stage, and case (a)for the fully-connected stage (Table 2).
Table 2: Experimental results of DPP for CIFAR-10 and CIFAR-100 datasets using VGG-16 andMobileNet v1 architecturesDataset	Network	Model	Remain.(%)	Accuracy(%)	Fixed SparsityCIFAR-10	VGG-16	DPP (This work)	12.14	93.36	Yes		(Liu et al., 2020)	8.82	93.93	No		Magnitude-based	10	88.59	Yes		Non-pruned baseline	100	93.75	-	MobileNet v1	DPP (This work)	36.95	93.14	Yes		Magnitude-based	37	92.7	Yes		Non-pruned baseline	100	93.67	-CIFAR-100	VGG-16	DPP (This work)	18.8	70.32	Yes		Magnitude-based	19	68.9	Yes		Non-pruned baseline	100	70.40	-	MobileNet v1	DPP (This work)	40	72.35	Yes		Magnitude-based	40	70.19	Yes		Non-pruned baseline	100	72.50	-4.3 Sparse B inarization on MNISTWe then turn to assess the performance of DPP for networks with binary parameters. Note that this ispossible with DPP since it does not rely on the magnitude of the weights for sparsification.
Table 3: Experimental results of DPP for MNIST dataset using binary LeNet architecturesNetwork	Model	Remaining parameters (%)	Accuracy (%)LeNet300-100	DPP (This work)	21	96.81	Non-pruned baseline	100	97.9LeNet5-Caffe	DPP (This work)	4.1	98.36	Non-pruned baseline	100	98.14.4 Analysis of sparsity belief over timeWe visualize the metrics in figure 3, normalized by the upper bound of the entropy to faciliatestraightforward comparison between layers. Recall that high entropy of the average mask indicatesdiversity among the neurons in a layer, low conditional entropy indicates confidence in a specificpattern and high mutual information indicates both diversity and confidence.
