Table 1: The configuration space for the regularization cocktail regarding the explicit regularizationhyperparameters of the methods and the conditional constraints enabling or disabling them. (BN:Batch Normalization, WD: Weight Decay, DO: Drop-Out, SC: Skip Connection, MB: Multi-branchchoice, SS: Shake-Shake, SD: Shake-Drop, LA: Lookahead Optimizer, SWA: Stochastic WeightAveraging, SE: Snapshot Ensembles, MU: Mix-Up, CM: Cut-Mix, CO: Cut-Out, and AT: FGSMAdversarial Learning)et al., 2019) as the main deep learning framework for our work and we extended the AutoDL-framework Auto-Pytorch (Mendoza et al., 2018; Zimmer et al., 2020) with our implementations forthe regularizers, as shown in Table 1.
Table 2: The configuration space of the training and model architecture hyperparameters.
Table 3: The search space of the training and model hyperparameters for the gradient boostingestimator of the Auto-Sklearn tool.
Table 4: Datasets. The collection of datasets used in our experiments, combined with detailedinformation for each dataset.
Table 5: Top-5 baselines. The test set performance for the Regularization Cocktail against the Top-5Most Frequent (Top-5 F) and the Top-5 Highest Ranks (Top-5 R) baselines.
Table 6: Detailed Table of Results. The test set performance for the plain network, individualregularization methods and for the regularization cocktails.
Table 7: Results of Experiment 3 The performances of the Regularization Cocktail and the GBDTalgorithm over the different datasets.
