Table 1: Human evaluations on sentiment, content preservation, and fluency.
Table 2: Examples of transferring along five different axes of style. The same model is used acrossall examples, with no additional training. Words deleted from the input are red, and words added inthe output are blue. Within each category, a fixed tiny set of exemplars is chosen, and fixed deltascale and tuning rates are used. The exemplars and settings are provided in Appendix A.2.
Table 3: Examples of dialect-sensitive completion (λ=8, add:40-70%, delete:。％). In each case, theinput text consists of an unfinished phrase, for example: “My favorite food: ”. The three exemplarsused for each dialect are the same as those used for the transfers in Table 2, as listed in Table 7.
Table 4: Examples of shortening (add:0-5%, delete:40-90%), using the first five sentences from theWikipedia article “Artificial neural network”. For each sentence, the target style is extracted directlyfrom the input text, and no delta is added.
Table 5: Random augmentations of input text “What’ll the weather be tomorrow?”, using randomstyle vector deltas with components sampled from N (0, 0.08).
Table 6: Emotiveness transfer exemplars. Transfer settings: λ=9, add/delete rates: 0-100%.
Table 7: Dialect transfer exemplars. Transfer settings: λ=8, add/delete rates: 10-30%.
Table 8: Politeness transfer exemplars. Transfer settings: λ=5, add/delete rates: 20-50%.
Table 9: Formality transfer exemplars. Transfer settings: λ=4, add/delete rates: 40-80%.
Table 10: Sentiment transfer exemplars. Transfer settings: λ=3, add/delete rates: 0-100%.
