Table 1: Forward KL loss comparisons for symmetric noise matrix estimations with CIFAR-10 as theunderlying clean dataset. “AP” means anchor point. We removed anchor-like data up to 70% in thesame manner described in (Xia et al., 2019). An average loss reduction is achieved from 0.27 to 0.17for 40% anchor-like instances removal when comparing with baselines not using clean samples. Wealso improved MPEIA and GLC by using their estimates as priors. Smaller loss values are bold-faced.
Table 2: Forward KL loss comparisons for pairwise noise transition matrix estimations with CIFAR-10 as the underlying clean dataset. In the table, “no” means no sample removal, “hardest” and “easiest”mean a 90% random sample removal from the hardest class, cat, and the easiest class, frog. Weconstantly perform the best for averaged noise levels. Smaller loss values are bold-faced.
Table 3: Forward KL loss comparisons for Clothing1M subset. Ours-1 has the lowest forward KLloss among all baseline models. When 0.5% Dclean is used, we (ours-3) improved GLC.
Table 4: A summary of the main differences while training CNN to generate LID sequences forCIFAR-10, when different amounts of anchor-like instances are removed.
Table 5: Neural network (and its usage), training losses and training epochs of all the baselines forCIFAR-10. Some methods, S-model, T-Revision, ours-1, ours-2 and ours-3, require “prior model” toinitialize noise transition matrices for formal estimation. While some method, T-Revision, trains NNwith special losses in its main method.
Table 6: Neural network (and its usage), training losses and training epochs of all the baselines forClothing1M subset. Respective prior models and losses are summarized.
