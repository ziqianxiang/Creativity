Table 1: KSD between the HMC samples and the target distribution for the baselines and the 4variations of our method on each of the synthetic target distributions.
Table 2: Average test set marginal log-likelihood and its standard error for different models for					MNIST and Fashion MNIST estimated using HAIS. For models with a scale factor, we also report					the final scale after training. For comparison, we report the best log-likelihood values from previous					works that tune HMC hyperparameters and use the same architecture.							MNIST			Fashion MNISTModel	Scale	Mean	Standard Error	Scale	Mean Standard ErrorVAE	-	-85.08	0.2172	-	-108.54	0.6010DReG-IWAE	-	-83.73	0.2109	-	-104.48	0.5841α=0	1.0	-83.48	0.2101	1.0	-104.08	0.5834α=1	1.0	-82.46	0.2073	1.0	-103.57	0.5826α = 0 & SKSD	6.79	-81.91	0.2042	5.58	-103.18	0.5802α = 1&SKSD	3.90	-81.94	0.2045	3.59	-102.29	0.5748Hoffman (2017)	-	-81.74	0.2046 =	-	-103.04	0.5804Salimans et al. (2015)	-	-81.94	-		Caterini et al. (2018)	-	-82.62	-		significantly better performance than VAE or IWAE, showing that reducing the approximation biasof the variational distribution with HMC greatly helps model learning. Furthermore, we see thatadding the scale factor to our model significantly improves performance as this avoids degeneratebehaviour when training the HMC hyperparameters. We note that, without any scaling, α = 1outperforms α = 0 due to α = 0 resulting in a too narrow initial distribution. With scaling, the
Table 3: P-values of the Wilcoxon test comparingthe model with HMC parameters tuned by maxELT& SKSD with the baseline models based on the KL-divergences of the marginals. Bold means maxELT &SKSD improves on the baseline at the given p-value.
Table 4: Unnormalized log densities for the 2D distributions used in the first experiment.
Table 5: -E (T)qφ[logp*(x)] values for the baselines and the 4 variations of our method on eachof the synthetic test distributions. The ground truth value -Ep [log p* (x)] is found using rejectionsampling.
Table 6: Training time for synthetic 2D problems on CPU (sec / 100 iters).
Table 7: p-values for paired t-tests on test log-likelihood values on the MNIST dataset.
Table 8: p-values for paired t-tests on test log-likelihood values on the Fashion MNIST dataset.
Table 9: Training Time for DLGM on GPU (sec / 1000 iters).
Table 10: P-values of the Wilcoxon test comparing models with initial distribution trained withα = 0-divergence. Values in bold mean that the test showed the top row model has lower KL-divergences than the left column model, i.e. the top row model is the better model at the significancelevel given by the p-value.
Table 11: P-values of the Wilcoxon test comparing models with initial distribution trained withα = 1-divergence. Values in bold mean that the test showed the top row model has lower KL-divergences than the left column model, i.e. the top row model is the better model at the significancelevel given by the p-value.
Table 12: P-values of the Wilcoxon test comparing models with initial distribution trained withα = 2-divergence. Values in bold mean that the test showed the top row model has lower KL-divergences than the left column model, i.e. the top row model is the better model at the significancelevel given by the p-value.
Table 13: P-values of the Wilcoxon test comparing models with initial distribution trained withmaximum likelihood. Values in bold mean that the test showed the top row model has lower KL-divergences than the left column model, i.e. the top row model is the better model at the significancelevel given by the p-value.
