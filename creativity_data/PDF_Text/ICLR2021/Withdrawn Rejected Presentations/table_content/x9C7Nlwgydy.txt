Table 1: Different Ways to generate ensemblesData Representation	Clustering algorithmsDifferent data preprocessing techniques	Multiple clustering algorithms (k-means, GMM, etc)Subsets of features	Same algorithm With different parameters or initial- izationsDifferent transformations of the features	Combination of multiple clustering and different parameters or initializationsFred & Jain (2005) discuss different Ways to generate an ensemble of clusterings Which are tabulatedin Table 1. In our proposed algorithm, We focus on choosing of data representation to generatecluster ensembles.
Table 2: Clustering evaluation metricsDataSetS	ImageNet-10			ImageNet-Dogs			STE10			CIFAR10			CIFAR100-20		MethodS\Metrics	ACC	NMI	ARI	ACC	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARIDCCM	0.710	0.608	0.555	0.383	0.321	0.182	0.482	0.376	0.262	0.623	0.496	0.408	0.327	0.285	0.173GATCluster	0.762	0.609	0.572	0.333	0.322	0.200	0.583	0.446	0.363	0.610	0.475	0.402	0.281	0.215	0.116PICA	0.870	0.802	0.761	0.352	0.352	0.201	0.713	0.611	0.531	0.696	0.591	0.512	0.337	0.310	0.171ConCURE	0.922	0.877	0.852	0.452	0.447	0.288	0.728	0.634	0.554	0.720	0.608	0.520	0.385	0.377	0.223We performed additional experiments to compare ConCURL to PICA when trained only on the“train” split for STL-10, CIFAR-10 and CIFAR-100. From Table 3, we observe that ConCURLachieves better clustering on all metrics. From Table 2 and 3, we can observe that increasing the datafor training improves the performance of the algorithm. In order to understand the generalization of7Under review as a conference paper at ICLR 2021the algorithm, in Table 4, we show results for the case when the models were trained only on “train”split and evaluated on “test data” that was not used during training. We can observe that we performbetter when evaluated on the test data.
Table 3: Clustering evaluation metrics: Models trained using only train split, and evaluation on trainsplitDatasets	STL10	CIFAR10	CIFAR100-20		Methods'Metrics	Acc NMI ARI	Acc NMI ARI	Acc NMI ARIPICA	^0494~0.424~0.297	-0.559~O.5TO~0.402	^0395~0.273~0Γ138^ConCURL	0.623	0.514 0.428	0.705 0.545 0.507	0.366	0.351	0.195Table 4: Clustering evaluation metrics: Models trained using only train split, and evaluation on testsplit of the dataDatasets	ImageNet-10	ImageNet-Dogs	STL10	CIFAR10	CIFAR100-20				Methods∖Metrics	Acc NMI ARI	Acc NMI ARI	Acc NMI ARI	Acc NMI ARI	Acc NMI ARIPICA 1	^0758^^0724^^0.602	0.375^^0399~020T	"■Q.484^^0.422~0.293	^Q559^^0504~0.393	^0^^^0276^^0.138-COnCURL	0.864	0.840 0.770	0.455 0.477	0.274	0.611	0.498	0.410	0.693	0.527	0.488	0.363	0.354 0.1934.2	Effect of Number of random transformations and Embedding sizeIn order to study the sensitivity of the algorithm to the random transformations, we performed anablation study for STL10 trained on the train split (Table 5). Recall that M is number of transfor-mations used in algorithm 1 and M = 30, 50 yield good results for the two types of transformations(random projections and diagonal transformations). We can also observe that there isn’t a large dif-ference between results obtained using either of the transformations. The results fluctuate with amargin of ± 0.06 and still outperform the other baselines in almost all cases. In the first column ofthe Table 5, we used diagonal transformations, and varied the number of transformations. The sec-
Table 4: Clustering evaluation metrics: Models trained using only train split, and evaluation on testsplit of the dataDatasets	ImageNet-10	ImageNet-Dogs	STL10	CIFAR10	CIFAR100-20				Methods∖Metrics	Acc NMI ARI	Acc NMI ARI	Acc NMI ARI	Acc NMI ARI	Acc NMI ARIPICA 1	^0758^^0724^^0.602	0.375^^0399~020T	"■Q.484^^0.422~0.293	^Q559^^0504~0.393	^0^^^0276^^0.138-COnCURL	0.864	0.840 0.770	0.455 0.477	0.274	0.611	0.498	0.410	0.693	0.527	0.488	0.363	0.354 0.1934.2	Effect of Number of random transformations and Embedding sizeIn order to study the sensitivity of the algorithm to the random transformations, we performed anablation study for STL10 trained on the train split (Table 5). Recall that M is number of transfor-mations used in algorithm 1 and M = 30, 50 yield good results for the two types of transformations(random projections and diagonal transformations). We can also observe that there isn’t a large dif-ference between results obtained using either of the transformations. The results fluctuate with amargin of ± 0.06 and still outperform the other baselines in almost all cases. In the first column ofthe Table 5, we used diagonal transformations, and varied the number of transformations. The sec-ond column of the Table 5 contains results with a fixed dimension of random projection (=64), andvaries the number of transformations. The third column of the Table 5 contains results with a fixednumber of transformations (=100), and varies the dimension of projection (the original embeddingsize is 256).
Table 5: Ablations on the number of transformations and dimension of random projections. Theseresults were obtained by training ConCURL on train split of STL10 for 1000 epochs.
Table 6: Dataset SummaryDataset	Classes	Train Data	Test Data	ResolutionImageNet-10 (Deng et al., 2009)	10	13000	500	224× 224Imagenet-Dogs (Deng et al., 2009)	-15-	-19500-	-750-	224× 224STL-10(Coates etal.,2011)	-10-	-5000-	-8000-	96× 96--CIFAR10 (KriZhevSky et al., 2009)-	-10-	-50000-	-10000	-32× 32-CIFAR100-20 (KriZhevSky et al., 2009)	-20-	-50000-	-10000	-32× 32-CUB (Wahetal., 2011)	-200-	-5994-	-5794-	224× 224CaIteCh-101 (Fei-Fei et al., 2004) 一	-101-	-7020-	-1657-	224× 224Intel	6	-14034-	-3000-	128 X 128AwA2 AwA2(Xian et al., 2018) 一	50	29865	7457	224 X 224A.2 Algorithm - Pseudo CodeThe pseudo code for our algorithm is given in Algorithm 1.
Table 7: More datasets: Clustering evaluation metrics of ConCURL on train and test splitsDatasets	Intel	CaltechIOI	CUB	AwA2			Split'Metrics	ACC NMI ARI	Acc NMI ARI	Acc NMI ARI	Acc NMI ARITrain	0.910~0801 ~0.800	0.339~0.651 ~0.223	^0!27~0.452~0.033	0.539~0.681 ~0.448Test	0.899	0.783	0.776	0.361	0.691	0.218	0.113	0.442	0.025	0.542	0.685	0.446A.6 Ablation study on the lossesIn this subsection, we study the effect of weights α, β and γ on the final metrics.Results for theweight configuration corresponding to α = 1, β = 1, γ = 1 is what is shown in the main paper. Forthe case of (α = 1, β = 0, γ = 0), we computed the cluster accuracy, NMI, ARI by computing theembeddings of all the data output by the representation learning algorithm used for L1 (here Grillet al. (2020)). Then we computed a K-means clustering on the embeddings (the target projectionlayer embeddings in this case) to obtain a partition of the data, and follow the same procedurementioned in A.4. For all the experiments in this section, we trained only on the train split of thedatasets.
Table 8: Cluster metrics evaluation on data points that available during trainingDatasets	ImageNet-10			ImageNet-Dogs			STL10		Methods∖Metrics	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARIɑ = 1,β = 0,γ = 0	0.818	0.843	0.757	0.492	0.464	0.289	0.29	0.31	0.137ɑ = 1,β = 1,γ = 0	0.905	0.875	0.841	0.400	0.386	0.245	0.565	0.503	0.373α = 1,β =1,γ =1	0.922	0.877	0.852	0.452	0.447	0.288	0.623	0.514	0.428Intel			Caltech101			CUB			AwA2		Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI0.889	0.764	0.758	0.348	0.641	0.212	0.134	0.460	0.041	0.528	0.713	0.4160.907	0.797	0.795	0.309	0.634	0.190	0.128	0.454	0.034	0.539	0.684	0.4610.910	0.801	0.800	0.339	0.651	0.223	0.127	0.452	0.033	0.539	0.681	0.448Table 9: Cluster metrics evaluation on data points that were not available during trainingDatasets	ImageNet-10			ImageNet-Dogs			STL10		Methods∖Metrics	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARIɑ = 1,β = 0,γ = 0	0.782	0.778	0.630	0.444	0.507	0.279	0.276	0.299	0.130ɑ = 1,β = 1,γ = 0	0.884	0.867	0.811	0.408	0.441	0.238	0.560	0.484	0.362α = 1,β =1,γ =1	0.864	0.840	0.770	0.455	0.477	0.274	0.611	0.498	0.410Intel			Caltech101			CUB			AwA2		Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI0.893	0.772	0.765	0.325	0.644	0.136	0.128	0.453	0.034	0.552	0.701	0.413
Table 9: Cluster metrics evaluation on data points that were not available during trainingDatasets	ImageNet-10			ImageNet-Dogs			STL10		Methods∖Metrics	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARIɑ = 1,β = 0,γ = 0	0.782	0.778	0.630	0.444	0.507	0.279	0.276	0.299	0.130ɑ = 1,β = 1,γ = 0	0.884	0.867	0.811	0.408	0.441	0.238	0.560	0.484	0.362α = 1,β =1,γ =1	0.864	0.840	0.770	0.455	0.477	0.274	0.611	0.498	0.410Intel			Caltech101			CUB			AwA2		Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI0.893	0.772	0.765	0.325	0.644	0.136	0.128	0.453	0.034	0.552	0.701	0.4130.903	0.787	0.785	0.325	0.667	0.181	0.117	0.445	0.027	0.541	0.681	0.4560.899	0.783	0.776	0.361	0.691	0.218	0.113	0.442	0.025	0.542	0.685	0.446A.7 t-sne plotsIn figure 3, we show t-sne plot of the ImageNet-10 embeddings obtained from ConCURL trainedmodel. One can clearly see the separation between various clusters with the exception of airline andairship clusters. Airline and airship clusters are mixed together on leftmost and righmost part of thet-sne plot.
