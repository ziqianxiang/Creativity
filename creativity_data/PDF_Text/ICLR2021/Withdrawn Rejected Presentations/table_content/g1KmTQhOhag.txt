Table 1: Performance of baseline and memory models on WMT-14 DE-EN translation. Values representan average of BLEU 4 scores for 3 runs of every model evaluated on 2000 samples from WMT-14 DE-ENValidation set.______________________________________________________________________________________________Small models 4 layers per encoder/decoder, 20 epochs		Base models 6 layers per encoder/decoder, 10 epochs	Transformer (baseline)	19.01	Transformer (baseline)	24.65MemTransformer 5	19.17	-	-MemTransformer 10	19.15	MemTranSformer 10	25.07MemTransformer 20	19.14	MemTranSformer 20	25.58MemBottleneck Transformer 10	11.20	MemCtrl Transformer 20	24.13MemBottleneck Transformer 20	10.41	MemCtrl Shared Transformer 20	25.73MemBottleneck Skip Transformer 20	16.45	-	-3.1	Performance metricsThe main hypothesis of the study says that adding memory to multilayered encoder-decoder archi-tectures should result in better performance for sequence processing tasks such as machine transla-tion. BLEU scores for WMT-14 DE-EN translation task (Bojar et al., 2014) are presented in Table 1.
Table 2: Memory lesions. Performance of the models trained with memory gradually degrades if the memorysize is changed during inference.
Table 3: Memory extension. Values represent an average of BLEU 4 scores for 3 runs of every model.
Table 4: Memory augmentation for the language modeling task. Average performance after training onWikiText-103(Merity et al., 2016) over 3 runs.
Table 5: Results on GLUE dev set with [mem] tokens added only for end task fine-tuning. Each [mem]was randomly initialized and trained only on the GLUE task. All runs were repeated 5 times and average scoresare reported. +pool stands for using concatenation of max and avg pooling over outputs for [mem] tokensinstead of the output from [CLS] token for classification.
