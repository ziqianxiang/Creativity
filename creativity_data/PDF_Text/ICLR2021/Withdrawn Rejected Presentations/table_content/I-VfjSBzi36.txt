Table 1: Comparison between randomly-pruned modelsand EarlyBERT on 4 GLUE tasks. We prune 4 heads ineach layer.
Table 2: Performance of EarlyBERT (fine-tuning) compared with different baselines.
Table 3: Ablation of regularization strength Î».
Table 4: Ablation of pruning ratios on self-attentionheads and intermediate neurons.
Table 5: Performance of EarlyBERT (pre-training) compared with BERT baselines.
