Table 1: Hyperparameters for the Omniglot sequential tasks experimentHyperparameter	BomLA	BomVIPosterior regulariser λ	0.1	-Precision initialisation values	10-4 〜10-2	-Covariance initialisation values	-	exp(-10)Number of Monte Carlo samples	-	5Number of batch M	1	1Number of query samples per class (meta-evaluation)	15	15Number of epochs per task	50	50Number of inner SGD steps in meta-training (k)	5	5Inner SGD learning rate (α)	0.1	0.1Outer loop optimiser	Adam	AdamOuter loop learning rate	0.001	0.001Number of tasks sampled for meta-evaluation	100	100Number of inner SGD steps in meta-evaluation (k)	10	1017Under review as a conference paper at ICLR 2021C.2 Pentathlon: sequential datasetsWe use the model architecture proposed by Vinyals et al. (2016) in this experiment, as we did forthe sequential tasks experiment. Tables 2 and 3 are the hyperparameters used in this experiment.
Table 2: Hyperparameters for the pentathlon experiment (same value for all datasets)Hyperparameter	BomLA	BomVIPosterior regulariser λ	(various values)	-Precision initialisation values	10-4 〜10-2	-Number of tasks sampled for Hessian approx.	5000	-Covariance initialisation values	-	exp(-5)Number of Monte Carlo samples	-	20Meta-batch size M	32	32Number of query samples per class	15	15Number of iterations per dataset	5000	5000Outer loop optimiser	Adam	AdamOuter loop learning rate	0.001	0.001Number of tasks sampled for meta-evaluation	100	100Table 3: Hyperparameters for the pentathlon sequential datasets experiment (individual datasets)Hyperparameter	Omniglot	CIFAR-FS	miniImageNet	VGG-Flowers	AircraftNumber of inner SGD steps in meta-training (k)	1	5	5	5	5Inner SGD learning rate (α)	0.4	0.1	0.1	0.1	0.1Outer learning rate decay schedule	-	×0.1 halfway	×0.1 halfway	×0.1 per 1000 iterations	×0.1 halfwayNumber of inner SGD steps in meta-evaluation	3	10	10	10	1018
Table 3: Hyperparameters for the pentathlon sequential datasets experiment (individual datasets)Hyperparameter	Omniglot	CIFAR-FS	miniImageNet	VGG-Flowers	AircraftNumber of inner SGD steps in meta-training (k)	1	5	5	5	5Inner SGD learning rate (α)	0.4	0.1	0.1	0.1	0.1Outer learning rate decay schedule	-	×0.1 halfway	×0.1 halfway	×0.1 per 1000 iterations	×0.1 halfwayNumber of inner SGD steps in meta-evaluation	3	10	10	10	1018Under review as a conference paper at ICLR 2021o o OLnoLn OLn o o o oC b m 寸 寸 Cnnnel Nge 寸Sd-yVdD iθuθ6quj∣∕u∕ 山 sjθλλo∣z∣-99λOOOcρ∙m⅛6jdj∣vωEM 6.Eu«一vsazu-gvɪs① IU¾u ① 6elu-∕u∕EFigure 3:	Meta-evaluation accuracy across 3 seed runs on each dataset along meta-training. Higheraccuracy values indicate better results with less forgetting as we proceed to new datasets. B omLAwith λ = 100 gives better performance in the off-diagonal plots (retains performances on previouslylearned datasets), and has a minor performance trade-off in the diagonal plots (learns less well on
