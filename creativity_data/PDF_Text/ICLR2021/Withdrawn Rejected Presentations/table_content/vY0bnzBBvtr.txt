Table 1: Regret comparisons for Q-learning algorithms on episodic MDPAlgorithm	Regret	Time	SpaceQ-learning+Bernstein bonus Jin et al. (2018)	O(√H3 SAT)	O(T)	O(SAH)Aggregated Q-learning Dong et al. (2019)	θ(√H4 MT + eT) 1	O(MAT)	O(MT)Full-Q-learning (FQL)	θ(√H4 T)	O(SAT)	O(SAH)Elimination-Based Half-Q-learning (HQL)	θ(√H 6 T)	O(SAT)	O(SAH)2	PreliminariesWe consider an episodic Markov decision process, MDP(S , A,H,P,r), where S is the set of stateswith |S| = S, A is the set of actions with |A| = A, H is the constant length of each episode, P is theunknown transition matrix of distribution over states if some action y is taken at some state x at steph ∈ [H], and rh : S×A→[0, 1] is the reward function at stage h that depends on the environmentrandomness Dh . In each episode, an initial state x1 is picked arbitrarily by an adversary. Then, ateach stage h, the agent observes state xh ∈S, picks an action yh ∈ A, receives a realized rewardrh(xh,yh), and then transitions to the next state xh+1, which is determined by xh, yh, Dh. At thefinal stage H, the episode terminates after the agent takes action yH and receives reward rH . Thennext episode begins. Let K denote the number of episodes, and T denote the length of the horizon:T = H × K, where H is a constant. This is the classic setting of episodic MDP, except that in theone-sided-feedback setting, we have the environment randomness Dh, that once realized, can helpus determine the reward/transition of any alternative feasible action that “lies on one side” of ourtaken action (Section 2.1). The goal is to maximize the total reward accrued in each episode.
Table 2: Comparison of cumulative costs for backlogged episodic inventory controlOPT	FQL	HQL Aggregated QL QL-UCBH	K	mean	SD	mean	SD	mean	SD	mean	SD	mean	SD	100	88.2	4.1	103.4	6.6	125.9	19.2	406.6	16.1	3048.7	45.01	500	437.2	4.4	453.1	6.6	528.9	44.1	1088.0	62.2	4126.3	43.7	2000	1688.9	2.8	1709.5	5.8	1929.2	89.1	2789.1	88.3	7289.5	57.4	100	257.4	3.2	313.1	7.6	435.1	17.6	867.9	29.2	7611.1	46.73	500	1274.6	6.1	1336.3	10.5	1660.2	48.7	2309.1	129.8	10984.0	73.0	2000	4965.6	8.3	5048.2	13.3	5700.6	129.1	7793.5	415.6	22914.7	131.1	100	421.2	3.3	528.0	10.4	752.6	32.9	1766.8	83.8	11238.4	140.05	500	2079.0	8.2	2204.0	13.1	2735.1	114.1	4317.5	95.8	15458.1	231.8	2000	8285.7	8.3	8444.7	16.4	9514.4	364.2	13373.0	189.2	40347.0	274.6Table 2 shows that both FQL and HQL perform promisingly, with significant advantage over theother two algorithms. FQL stays consistently very close to the clairvoyant optimal, while HQLcatches up rather quickly using only one-sided feedback. See more experiments in Appendix F.
