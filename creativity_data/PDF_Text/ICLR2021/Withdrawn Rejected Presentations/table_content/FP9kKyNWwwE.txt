Table 1: The network architecture optimized for every meta-dataset.
Table 2: Hyper-parameter search space for the meta-datasets. The name of each the meta-datasets isinspired by the most prominent hyper-parameter, highlighted in red.
Table 3: Results on several zero-shot HPO benchmarks. The numbers reported are the averagenormalized regret for 120 tasks on each meta-dataset evaluated as the average of a 5-fold cross-validation scheme. We report the best results in bold and UnderIine the second best.
Table 4: Normalized regret for state-of-the-art transfer learning HPO methods for up to 80 trials afterinitialization with 20 configurations.
Table 5: Final results for zero-shot HPO for different variations of our model optimized with differentobjectives. The numbers reported are the average normalized regret after 20 trials.
Table 6: Encoding of the different hyper-parameters used in the meta-dataset.
Table 7: Summary of the 120 UCI datasets used to generate the meta-datasets.
Table 8: The network architecture optimized for every meta-dataset.
Table 9: Final results of each model optimized on the different meta-datasets. The numbers reportedare the average normalized regret after 50 trials on held-out validation sets for the zero-shot task.
Table 10: Final results for different variants of our sequential model optimization policy. The numbersreported are the average normalized regret after 80 trials on held-out validation after initializationwith the exact same 20 configurations suggested by the our zero-shot approach.
Table 11: Final results of our universal response model optimized with different auxiliary weights.
