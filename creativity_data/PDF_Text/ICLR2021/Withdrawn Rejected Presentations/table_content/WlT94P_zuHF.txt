Table 1: Perplexity scores (lower is better) of the three networks: Transformer-QL, Transformer-XL and Compressive Transformer (Comp-Transformer). The third column shows the segmentlength (ns), memory length (nm) and compressed memory length (ncm) of the test model. Theforth column shows the window length nw of the test model. Note that, for Transformer-XL andTransformer-QL, the window length is ns + nm and for Compressive Transformer the windowlength is ns + nm + ncm . The average test context length nc has been shown in the fifth column.
Table 2: Improvement in test perplexity score (lower is better) of Transformer-QL over Transformer-XL for three different model dimensions. The forth and fifth columns show the test perplexityobtained by Transformer-QL and Transformer-XL respectively.
Table 3: Relative improvements in perplexity scores (lower is better) obtained by Transformer-QLover Transformer-XL on WikiText-103 dataset. The second column shows the test segment (ns) andmemory (nm) length. The third and forth column respectively show the average test context length(nc) of Transformer-XL and Transformer-QL network.
Table 4: Statistics of the datasets used in the experiments.
Table 5: Comparison of Transformer-QL with Multi-scale Transformer (MS-Transformer). Thethird and forth column respectively show the segment length (ns) and the memory length (nm) usedduring training. The fifth, sixth and seventh columns respectively show the segment, memory andthe window (nw = ns + nm) length used to compute the text perplexities.The eighth column showsthe average context length (nc) of the test models.
