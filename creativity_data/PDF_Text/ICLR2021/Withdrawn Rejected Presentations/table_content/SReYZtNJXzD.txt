Table 1: Test accuracy (%) on CIFAR-100. The best results on each block are bolded. Italic row isthe most basic baseline where examples have identical weights.
Table 2: Accuracy (%) of DM and otherbaselines on CIFAR-10 under symmetriclabel noise. The best results on each blockare bolded. Number format of this tablefollows MentorNet. ’-’ denotes result wasnot reported.
Table 3:	Experiments on sentiment classificationof movie reviews. Due to the space shortage, theresults of variants after DN are in the brackets.
Table 4:	Results of common stochastic optimis-ers. Adam (Kingma & Ba, 2015) is an adaptivegradient method. We report three settings of it.
Table 5: Accuracy (%) on Clothing1M. The leftmost block’s results are from SL while the middleblock’s are from Masking. Results of corresponding variants after DN are in the brackets.
Table 6: Video retrieval results on MARS dataset. All other methods use GoogLeNet V2 except that DRSA and CAE use more complex ResNet-50.			Metric	DRSA CAE OSM+CAA	Our Trained Results			CCE	GCE MAE MSE	DMmAP (%)	65.8	67.5	72.4	58.1	31.6	12.0	19.6	72.8CMC-1 (%)	82.3	82.4	84.7	73.8	51.5	26.0	39.3	84.34.3	Robust video retrievalDataset and evaluation settings. MARS (Zheng et al., 2016) contains 20,715 videos of 1,261 per-sons. There are 1,067,516 frames in total. Because person videos are collected by tracking anddetection algorithms, abnormal examples exist as shown in Figure 3 in the supplementary material:Some frames contain only background or an out-of-distribution person. Exact noise type and rateare unknown. We use 8,298 videos of 625 persons for training and 12,180 videos of the other 636persons for testing. We report the cumulated matching characteristics (CMC) and mean averageprecision (mAP) results (Zheng et al., 2016).
Table 7: Results of DM and other standard regularisers on CIFAR-100. We set r = 40%, i.e., thelabel noise is severe but not belongs to the majority. We train ResNet-44. We report the average testaccuracy and standard deviation (%) over 5 trials. Baseline is CCE without regularisation.
Table 8: The test accuracy (%) of DM and other standard regularisers on Vehicles-10. We trainResNet-44. Baseline denotes CCE without regularisation. We test two cases: symmetric label noiserate is r = 40%, and clean data r = 0.
Table 9: Is it feasible to correct the labels of noisy training data? Our results demonstrate theeffectiveness of label correction using DM. When retraining from scratch on the relabelled trainingdata, we do not adjust the hyper-parameters β and λ. Therefore, the reported results of retraining onrelabelled datasets are not the optimal. In label correction, the original labels are replaced by theircorresponding predictions.
Table 10: Results of CCE, DM on CIFAR-10 under noisy labels. For every model, we show itsbest test accuracy during training and the final test accuracy when training terminates, which areindicated by ‘Best’ and ‘Final’, respectively. We also present the results on corrupted training setsand original intact one. The overlap rate between corrupted and intact sets is (1 - r). When λis larger, β should be larger for adjusting emphasis variance. The intact training set serves as anindicator of meaningful fitting and we observe that its accuracy is always consistent with the finaltest accuracy.
