Table 1: Statistics for different language pairsshowing the number of (parallel) sentences in thetrain/dev/test datasets. The test data is from WMT-14 for De-En and Ru-En, and WMT17 for Zh-En.
Table 2: BLEU scores achieved by bench-marked models on the WMT-14 (De-En,Ru-En) and the WMT-17 (Zh-En) testsets.
Table 3: MT output errors indicative of source texts with hard-to-translate phenomena (WMT-19).
Table 4: Pronoun evaluation: Rankings of the different models for each language pair, obtainedfrom our evaluation procedure. Through manual analysis, % for the following types of errors arereported: Anaphora - instances of Gender Copy, Named Entity and Language specific errors.
Table 5: Coherence and Read-ability evaluation: Rankings ofthe different models for eachlanguage pair, obtained fromour evaluation procedure.
Table 6: Lexical consistency evaluation: Rankings of the different models for each language pair,ranked by the % of samples that are Consistent compared to inconsistent samples, then by % ofconsistent samples in the Full testset. Also shown are the results of manual error analysis on a subsetof the translations for Synonyms, Related words, Omissions, Named Entity, Random translation.
Table 7: Discourse connective evaluation: Rankings of the different models for each language pair,ranked first by their Accuracy and then by the percentage where ANY connective is produced. Eachset of rankings is followed by the results of the manual analysis on a subset of the translation datafor Omissions, Synonyms, Mistranslations.
Table 8: Results of the re-trained pronoun scoring model.
Table 9: Models ranked according to their performance (best to worst) in anaphora according to theevaluation model, with BLEU score for comparison. Model scores given here are obtained by sub-tracting the score for the model translation from the score for the reference translation, and summingthe score differences across the dataset. Hence, smaller model scores indicate better performance(closer to reference scores).
Table 10: Results of the re-trained coherence model.
Table 11: Models ranked according to their performance (best to worst) in coherence according ourevaluation, with BLEU for comparison. Coherence scores given here are obtained by subtracting thescore for the model translation from the score for the reference translation, and summing the scoredifferences across the dataset. Hence, smaller model scores indicate better performance (closer toreference scores).
Table 12: Results of the connective classification model.
Table 13: Connective-wise results for the connective user studies. The table also shows the numberof times the Reference / System translation was chosen (summed for both annotators). The Tiecolumn shows the number of times the users showed no preference. Note that ties are not includedin the agreement. Other samples not included were the ones marked as invalid by the annotators dueto misalignment errors, severe grammatical issues, etc.
Table 14: Discourse phenomena: Anaphora (restricted to anaphoric pronouns), LexicalConsistency, and Discourse Connectives in popular NMT datasets (for English). The column ANYshows the proportion of sentences which contain any of the listed phenomena.
Table 15: Configuration parameters for training Han model, taken from the authors’ repositoryhttps://github.com/idiap/HAN_NMT/Parameters	Values	Step 1: Sentence-level NMT		-encoder_type	transformer-decoder_type	transformer-enc_layers	6-dec_layers	6-IabeLsmoothing	0.1-rnn_size	512-Position_encoding	--dropout	0.1-batch_size	4096-Start_decay_at	20-epochs	20-max_generator_batches	16-batch_type	tokens-normalization	tokens-accum_count	4-optim	adam
Table 16: Configuration parameters for training San, Anaph, Concat, S2S models. Parameters ofAnaph are taken from the original paper Voita et al. (2018b) and parameters of San are taken fromthe authors’ repository: https://github.com/THUNLP-MT/Document-Transformerand user manual for the THUMT library which provides the basic Transformer model: https://github.com/THUNLP-MT/THUMT/blob/master/UserManual.pdf. Parameters whichare not listed were left as default. Note that the max-update for Zh-En was set to 400,000 due tothe larger dataset size.
Table 17: Examples for the types of errors found in the translations. S: denotes source, T: denotesmodel translations while R: denotes reference translations.
