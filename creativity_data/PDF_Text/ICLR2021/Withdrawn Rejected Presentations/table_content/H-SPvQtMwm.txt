Table 1: Overview of all Synthesizing Functions.
Table 2: Experimental Results on WMT’14 English- SIZER variants are competitive with TransformersGerman, WMT’14 English-French Machine Trans- for this task. On LM1b, We find that the Randomlation tasks and Language Modeling One Billion Synthesizers perform within 1-2 PPL points away(LM1B). f denotes original reported results in from the vanilla Transformer model. The best per-(Vaswani et al., 2017).	forming model is the Synthesizer (D+V), whichachieves the best performance on this setting.
Table 3: Experimental results on Abstrac-tive Summarization (CNN/Dailymail) andDialogue Generation (PersonaChat). We re-port on RL (Rouge-L), B4 (Bleu-4), Met.
Table 5: Experimental results (dev scores) on multi-task language understanding (GLUE bench-mark) for small model and en-mix mixture. Note: This task has been co-trained with SUPerGLUE.
Table 6: Experimental results (dev scores) on multi-task language understanding (SuperGLUEbenchmark) for small model and en-mix mixture. Note: This task has been co-trained with GLUE.
Table 4: Validation PerPlexity scores on C4 dataset (Raffel et al., 2019). All models are at aPProxi-mately similar Parameterization.
Table 7: Results on Encoding only tasks (accu-racy).
Table 8: Results for additional Synthesizer variants on WMT EnDe (BLEU scores)A.3 Effect of Number of HeadsWe also investigate the impact of the number of heads on performance. We trained three RandomSynthesizer models for the small version of the machine translation tasks using the T5 frameworkwithout pretraining. For simplicity, evaluation is done via greedy decoding. We report scores on thedevelopment set. We are mainly interested in relative performance and not absolute numbers. Table9 reports the results on varying the number of heads on performance.
Table 9: Effect of number of heads on multi-task MT. Increasing the number of heads improvesperformance.
