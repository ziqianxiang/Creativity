Table 1: Comparison of the extra class method (ours) with various other out-of-distribution detectionmethods when trained on CIFAR-10 and CIFAR-100 and tested on other datasets. All numbers fromcomparison methods are sourced from their respective original publications. For our method, we alsoreport the standard deviation over five runs (indicated by the subscript), and treat the performanceof other methods within one standard deviations as equivalent to ours. For fair comparison with theMahalanobis detector (MAH) Lee et al. (2018), we use results when their method was not tunedseparately on each OoD test set (Table 6 in Lee et al. (2018).
Table 2: DAC vs OpenMax. The OpenMax implementation was based on code avail-able at https://github.com/abhijitbendale/OSDN and re-implemented by us in Py-Torch Paszke et al. (2019).
Table 3: DAC vs OE for NLP Classification task. OE implementation was based on code available athttps://github.com/hendrycks/outlier-exposureone expects that such methods will cause the DNN to predict with less confidence when presentedwith inputs from a different distribution or from novel categories; we compare against the followingmethods:•	Softmax Thresholding This is the simplest baseline, where OoD samples are detected bythresholding on the winning softmax score; scores falling below a threshold are rejected.
Table 4: The performance of DAC as an OoD detector, evaluated on various metrics and comparedagainst competing baselines. All experiments used the ResNet-34 architecture, except for MCDropout, in which case we used the WideResNet 28x10 network. ↑ and J indicate that higher andlower values are better, respectively. Best performing methods (ignoring statistically insignificantdifferences)on each metric are in bold.
