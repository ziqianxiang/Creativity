Table 1: We augment programs with 11 automated source-to-source compiler transformations. 10 of the 11 transformations arecorrect-by-construction and do not modify operational semantics.
Table 2: Type inference accuracy on TypeScript programs in the Hellendoorn et al. (2018) dataset.
Table 3: Results for different settings of the code summarization task: supervised training with 81kfunctions, masked language model pre-training and contrastive pre-training with fine-tuning.
Table 4: Code clone detection results withcosine similarity probe. Contrastive and hy-brid representations are predictive of func-tionality, with +6.2%, +10% AUROC overtextual similarity (edit distance).
Table 6: Ablating compiler transformations used during contrastive pre-training. The DeepTyperBiLSTM is pre-trained with constrastive learning for 20k steps, then fine-tuned for type inference.
Table 5: Compiler data augmentationsdegrade performance when training su-pervised models from scratch.
Table 7: If local representations are learned, transferring part of the Contrastive MLP head improvestype inference. The encoder is a 2-layer BiLSTM (d=512), with a 2-layer MLP head for bothpre-training purposes and type inference. The mean hidden state representation is optimized for 10kiterations for the purposes of this ablation.
Table 8: Contrasting global, sequence-level representations outperforms contrasting local representa-tions. We compare using the terminal (global) hidden states of the DeepTyper BiLSTM and the meanpooled token-level (local) hidden states.
Table 9: Training time and decoder depth ablation on the method name prediction task. Longerpre-training significantly improves downstream performance when a shallow, 1 layer decoder is used.
