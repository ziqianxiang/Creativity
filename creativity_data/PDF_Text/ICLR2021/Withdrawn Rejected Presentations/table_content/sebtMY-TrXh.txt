Table 1:	Generalization: next word predic-tion of unbiased sentences at test time. An un-biased sentence is encoded and decoded by eachmodel. Color means that the word was incor-rectly reconstructed. Blue means that the sen-tence complies with the bias and purple meansthat the incorrect reconstruction is still unbi-ased. Most reconstructions seem grammaticallycorrect. In practice AriEL also made errors.
Table 2:	Generation: output of the decoderwhen sampled uniformly in the latent space.
Table 3: Evaluation of continuous sentence embeddings on the toy dataset. Results for a latentspace of d = 16 and d = 512. Each experiment is run 5 times. AriEL, achieves almost perfectperformance in most metrics, especially in validity, which quantifies how many random sampleswere decoded into a unique and grammatical sentence. Transformer performed exceptionally, exceptfor validity. All methods improved their performance increasing d, particularly in validity, but stillachieved less than one third the performance of AriEL. VAE is the second best in validity, supportingour hypothesis, that volume coding facilitates retrieval of information by random sampling.
Table 4: Performance on the GuessWhat?!Questioner data. For the real dataset the pat-tern is repeated: AriEL shows that a largervalue of valid sentences is possible. Transformer 16 gave better results than whennlayers was increased from 2 to 20 to increaseits learnable parameters from 588K to 2,666K.
Table S1: Description of vocabulary used.
