Table 1: Top-1 accuracy ofResNet-32 on long-tailed CIFAR-10 and CIFAR-100. Rows with f denoteresults directly copied from ZhoU et al. (2020). * denotes results reproduced with the authors code.
Table 2: Top-1 accuracy of ResNet-50 on iNat-uralist 2018. Rows with fdenote results directlycopied from their original paper. We present re-sults when training for 90 / 200 epochs.
Table 3: Top-1 accuracy of on large-scale long-tailed datasets ImageNet-LT for different back-bone architectures. Rows with fdenote results di-rectly copied from Kang et al. (2020). * denotesresults reproduced with the authors code.
Table 4: Comprehensive results on the most skewed long-tailed CIFAR-100 (imbalance ratio: 100).
Table 5: Comprehensive results on ImageNet-LT with different backbone networks (ResNet-50,ReSNext-50).__________________________________________________________________Method	Many	ResNet-50		All	ResNext-50			All		Medium	Few		Many	Medium	Few	Cross Entropy	64.0	33.8	5.8	41.6	65.9	37.5	7.7	44.4Decouple-cRT	58.8	44.0	26.1	47.3	61.8	46.2	27.4	49.5Decouple-LWS	57.1	45.2	29.3	47.7	60.2	47.2	30.3	49.9BBN	56.2	46.6	14.1	45.9	58.1	47.2	15.6	47.1Ours (S = 1)	53.5	45.2	41.8	47.9	55.2	46.7	39.0	49.2Ours (S = 10)	54.5	45.0	41.9	48.2	56.5	46.1	39.4	49.34.2	Ablation StudyTo find the optimal setting of S, which is the hyper-parameter controlling when to switch, we in-vestigate the S value and corresponding results are shown in Table 6. Interestingly, our methodachieves comparable results despite different values of S, indicating S is not dataset/distributiondependent or sensitive. This is consistent with our motivation: memorization of tail classes willnot seriously disrupt the learned representation with smaller learning rate. Thus, once there is a CRsampling during the small learning rate training, model could jointly fine-tune both feature extractorand classifier to achieve better generalization, regardless of the specific value of S.
Table 6:	Determining of the optimal S onlong-tailed CIFAR-10 (imbalance ratio: 50)and CIFAR-100 (imbalance ratio: 50).
Table 7:	Determining of switching strategies onlong-tailed CIFAR-10 (imbalance ratio: 50).
Table 8: Quantitative results of the regularity of each class on long-tailed CIFAR-10 (imbalanceratio: 50). All indexes are calculated based on the cumulative learned events and forgetting events.
Table 9: Comprehensive results on long-tailed CIFAR-10 (imbalance ratio: 50) with combinationsof different data samplers used in different stages.
Table 10: Sampling weights in the switching stage based on cardinality and irregularity respectivelyon long-tailed CIFAR-10 (imbalance ratio: 50).
Table 11: Comparisons between Decouple learning paradigm and our learning paradigm on long-tailed CIFAR-10 (imbalance ratio: 50), where Decouple indicates fixing the backbone and re-trainthe classifier from scratch while We continue tojoint train both of them.
Table 12: Feature quality of Decouple learning paradigm and our switching learning paradigm onlong-tailed CIFAR-10 (imbalance ratio: 50). We firstly train the model with standard and switchingprocedure respectively, then re-train the classifier with different data samplers with backbone fixed.
Table 13: Total error (bias2+variance) of different methods on the test set of long-tailed CIFAR-10(imbalance ratio: 50)._____________________________________________________________________Method	Test Accuracy ↑	Bias2 J	Variance J	Total Error JCross Entropy (IB only)	0.779	0.049	0.168	0.217Cross Entropy (CR only)	0.763	0.056	0.183	0.239Decouple-cRT	0.807	0.037	0.148	0.185BBN	0.822	0.032	0.146	0.178Ours (S = 1)	0.829	0.029	0.142	0.171A.7 Learning Rate SchedulingIn Section 4.3 we visualize the learning speed of different classes to reveal the effectiveness ofstage-wise constant learning rate scheduler during training. The observations lead to an interestinghypothesis for explaining why we should mainly memorizing tail class samples in the small learningrate stage. We provide more details here.
Table 14: Test performance of models trained with various learning rate schedulers on long-tailedCIFAR-10 (imbalance ratio: 50).________________________________________________________Standard			Our		Optimizer	Learning Rate	Test Accuracy	Optimizer	Learning Rate	Test AccuracySGD	Stage-wise	77.9	SGD	Stage-wise	82.9SGD	0.1	75.5	SGD	0.1	77.7SGD	0.02	76.1	SGD	0.02	78.3SGD	0.01	75.0	SGD	0.01	77.7SGD	0.001	65.9	SGD	0.001	66.121Under review as a conference paper at ICLR 202180604020⅛ω 6u⊂ra⅛co AUB_n<_>。B(a) lr = 0.1(b) lr = 0.02(c) lr = 0.01(d) lr = 0.001cisO (5000 samples):lsl (3237 samples)cls7 (238 samples)
