Table 1: Mean invariance co-complexity values for all 4 networks over 4 transformations, rotation,scale, shear, and translation. We approximate the computation of the invariance co-complexity in (6)by taking 1000 randomly initialized networks in each case (for the supremum in (6)). For fairness, allarchitectures share approximately the same number of parameters (around 16k).
Table 2: Mean dissociation co-complexity values for all networks. Note that CNN and its variantsmaintain the dissociation co-complexity at a similar level to that of MLP.
Table 3: Mean dissociation co-complexity values for all networks. Note that CNN and its variantsmaintain the dissociation co-complexity at a similar level to that of MLP.
Table 4: Shown are the architectural details of all neural networks used in our experiments. Scale-Conv represents scale-equivariant convolution and P4Conv-Z2 and P4Conv-Z4 represents rotationequivariant convolution (as detailed in Cohen & Welling (2016a)). For the ScaleConv layers, notethat 2.2:1 represents the ratio of the maximum to the minimum scale of the filters. A total of 4 scalepathways were chosen for those layers. FC represents the fully connected layers. Networks arechosen such that overall they share a similar parametric count (shown within the brackets).
