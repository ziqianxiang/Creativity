Table 1: Description of the modules and their functionalities. We denote P as the parameter to instantiate eachmodule, H as the dialogue history, Q as the question of the current dialogue turn, and V as video input.
Table 2: AVSD test results: The visual features are: I (I3D), ResNeXt-101 (RX), Faster-RCNN (FR), C (captionas a video input). The audio features are: VGGish (V), AclNet (A). Xon PT denotes models using pretrainedweights and/or additional finetuning. Best and second best results are bold and underlined respectively.
Table 3: Ablation analysis of VilNMN with different model variants on the test split of the AVSD benchmark#	Model Variant	BLEU4	CIDErA	Full VilNMN	0.421	1.171B	,→ No video NMNs; + vanilla text→video attention	0.415	1.159C	,→ No dial. NMNs; + response→history attention	0.412	1.151D	,→ No dial. NMNs; + response→concat(history+question) attention	0.411	1.133E	,→ No dial. NMNs; + HREDLSTM (history) + question attn.	0.414	1.153F	→ No dial. NMNs; + HREDGRU(history) + question attn.	0.415	1.138entity-sensitive dialogues such as chit-chat and open-domain dialogues. Please see Appendix C foradditional analysis of performance breakdown by dialogue turns and video lengths.
Table 4: Experiment results on the TGIF-QA benchmark. The visual features are: ResNet-152 (R), C3D (C),Flow CNN from two-stream model (F), VGG (V), ResNeXt-101 (RX).
Table 5: Summary of DSTC7 AVSD and TGIF-QA benchmark	#	Train	Val.	Test	Dialogs	7,659	1,787	1,710AVSD	Turns	153,180	35,740	13,490	Words	1,450,754	339,006	110,252	Count QA	24,159	2,684	3,554TGIFQA	Action QA	18,428	2,047	2,274	Trans. QA	47,434	5,270	6,232	Frame QA	35,453	3,939	13,691B.2	Training DetailsWe use a training batch size of 32 and embedding dimension d = 128 in all experiments. WhereTransformer attention is used, we fix the number of attention heads to 8 in all attention layers. Inneural modules with MLP layers, the MLP network is fixed to 2 linear layers with a ReLU activationin between. In neural modules with CNN, we adopt a vanilla CNN architecture for text classification(without the last MLP layer) where the number of input channels is 1, the kernel sizes are {3, 4, 5},and the number of output channels is d. We initialize models with uniform distribution (Glorot &Bengio, 2010). During training, we adopt the Adam optimizer (Kingma & Ba, 2015) and a decayinglearning rate Vaswani et al. (2017) where we fix the warm-up steps to 15K training steps. We employdropout (Srivastava et al., 2014) of 0.2 at all networks except the last linear layers of question parsersand response decoder. We train models up to 50 epochs and select the best models based on the
Table 6: Performance breakdown in BLEU4 and CIDEr(a) by dialogue turn between model variants B and E.
