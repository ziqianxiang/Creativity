Table 1: Quantitative results for the MNIST experiment. We report results for three differ-ent models with varying number of dimensions in the latent space: 3, 16, and 64. For eachone we explore four training setups, a regular VAE (γ = 0) and three intensities of GR-VAE(γ ∈ {1, 10, 100}). We then report the reconstruction loss, the silhouette score of the test sam-ples in the latent space, and the Hamiltonian. Furthermore we train two downstream model: k-NNand a classification tree, and we report their average F1 scores over a 5-fold cross-validation.
Table 2: Results on the text representations. Accuracy results for the text classification task in theCora, CiteSeer, and PubMed dataset. In this particular experiment GR-VAE model was trained withequally weighted factors (γ = 1) of the loss components (reconstruction, KL-divergence and graphregularization).
Table 3: Results on text representations with partial graph observability. Accuracy results forthe text classification task in the Cora, CiteSeer, and PubMed dataset. In this particular experimentDGI was trained using the same batching procedure as GR-VAE model was trained with differentvalues for the graph regularizer weight, γ . Cora					CiteSeer			PubMed		Model	GR γ	Latent space size										20	100	250	20	100	250	20	100	250DGI (VAE)	-	0.738	0.723	0.746	0.611	0.650	0.635	0.457	0.558	0.620DGI (GR-VAE)	0.5	0.701	0.732	0.744	0.663	0.616	0.612	0.384	0.418	0.448DGI (GR-VAE)	1	0.73	0.756	0.761	0.612	0.644	0.646	0.429	0.431	0.410DGI (GR-VAE)	2	0.705	0.734	0.764	0.624	0.626	0.630	0.522	0.477	0.312DGI (GR-VAE)	10	0.527	0.7	0.576	0.597	0.635	0.654	0.594	0.499	0.377DGI (GR-VAE)	100	0.326	0.49	0.361	0.301	0.426	0.355	0.442	0.541	0.524DGI			0.738			0.611			0.722	non-existent. Ultimately, this regularizer can lower signal-to-noise ratio in cases where a particulartopology is irrelevant.
Table 4: Results for chemical reactions experiment. We report the accuracy on the downstreamreaction task. The annotations in parenthesis specify details about the encoder: Finetuned denotesthat the VAE or GR-VAE has been finetuned on chemical reaction data (on a different split from thedownstream reactions), in the case of the DGI the annotation references to which VAE model wasused for encoding the SMILES. For each instance of the GR-VAE we display which γ we used intraining.
Table 5: Shortest Hamiltonian Path (SHP) distance to an ordered chain. This table displaysthe distance from each SHP to an ordered chain (0 to 9) using FastDTW. For reference, the averagedistance of a random connected path is 30.03 ± 6.5 (computed with 1000 random sequences).
Table 6: Shortest Hamiltonian Paths. Full chains obtained when running SHP over the classcentroids of the samples in the latent space. We can see that with the biggest value of the regularizer(γ = 100) SHPs recover the original chain used for the constraint.
