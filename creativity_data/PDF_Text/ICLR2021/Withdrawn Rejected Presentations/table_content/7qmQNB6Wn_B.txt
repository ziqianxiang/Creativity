Table D.1:	Hyperparamter setup18Under review as a conference paper at ICLR 2021	State dim.	Action dim.	β	ThresholdSparseHalfCheetah-v1	17	6	0.02	5.0SparseHopper-v1	11	3	0.04	1.0SparseWalker2d-v1	17	6	0.02	1.0SparseAnt-v1	111	8	0.02	1.0	State dim.	Action dim.	β	Delay DHumanoidStandup-v1	376	17	1	DelayedHalfCheetah-v1	17	6	0.2	20DelayedHopper-v1	11	3	0.2	20DelayedWalker2d-v1	17	6	0.2	20DelayedAnt-v1	111	8	0.2	20Table D.2: State and action dimensions of Mujoco tasks and the corresponding βE	S imulation SetupWe compared our DAC algorithm with the SAC baselines and other RL algorithms on various typesof Mujoco tasks with continuous action spaces (Todorov et al., 2012) in OpenAI GYM (Brockmanet al., 2016). For fairness, both SAC/SAC-Div and DAC used a common hyperparameter setup thatbasically follows the setup in (Haarnoja et al., 2018a). Detailed hyperparameter setup and environ-
Table D.2: State and action dimensions of Mujoco tasks and the corresponding βE	S imulation SetupWe compared our DAC algorithm with the SAC baselines and other RL algorithms on various typesof Mujoco tasks with continuous action spaces (Todorov et al., 2012) in OpenAI GYM (Brockmanet al., 2016). For fairness, both SAC/SAC-Div and DAC used a common hyperparameter setup thatbasically follows the setup in (Haarnoja et al., 2018a). Detailed hyperparameter setup and environ-ment description are provided in Appendix D, and the entropy coefficient β is selected based onthe ablation study in Section 6.3. For the policy space Π we considered Gaussian policy set widelyconsidered in usual continuous RL. For the performance plots in this section, we used deterministicevaluation which generated an episode by deterministic policy for each iteration, and the shadedregion in the figure represents standard deviation (1σ) from the mean.
Table F.1: Max average return of DAC algorithm and SAC baselines for fixed α setup	DAC (α = 0.5)	DAC (α = 0.8)	DAC (α-adapt.)	SAC	SAC-Div	202491.81	170832.05	197302.37	167394.36	165548.76HumanoidS	±25222.77	±12344.71	±43055.31	±7291.99	±2005.85Del. HalfCheetah	6071.93±1045.64	6552.06±1140.18	7594.70±1259.23	3742.33±3064.55	4080.67±3418.07Del. Hopper	3283.77±112.04	2836.81±679.05	3428.18±69.08	2175.31±1358.39	2090.64±1383.83Del. Walker2d	4360.43±507.58	3973.37±273.63	4067.11±257.81	3220.92±1107.91	4048.11±290.48Del. Ant	4088.12±578.99	3535.72±1164.76	4243.19±795.49	3248.43±1454.48	3978.34±1370.23Table F.2: Max average return of DAC algorithms and SAC baselines for adaptive α setupF.2 Comparison to RND and MaxEntWe first compared the pure exploration performance of DAC to random network distillation (RND)(Burda et al., 2018) and MaxEnt (Hazan et al., 2019), which are state-of-the-art exploration methods,on the continuous 4-room maze task described in Section 6.1. RND adds an intrinsic reward rint,tto MDP extrinsic reward rt as rRND,t = rt + cintrint,t based on the model prediction error rint,t =||f(st+1) - f(st+1)||2 of prediction network f and random target network f for given state st+1.
Table F.2: Max average return of DAC algorithms and SAC baselines for adaptive α setupF.2 Comparison to RND and MaxEntWe first compared the pure exploration performance of DAC to random network distillation (RND)(Burda et al., 2018) and MaxEnt (Hazan et al., 2019), which are state-of-the-art exploration methods,on the continuous 4-room maze task described in Section 6.1. RND adds an intrinsic reward rint,tto MDP extrinsic reward rt as rRND,t = rt + cintrint,t based on the model prediction error rint,t =||f(st+1) - f(st+1)||2 of prediction network f and random target network f for given state st+1.
Table F.3: Max average return of DAC and other RL algorithms22Under review as a conference paper at ICLR 2021G Ablation StudiesHere, we provide more ablation studies for remaining delayed Mujoco tasks. Fig. G.2 shows theaveraged learning curves of α, DJαS, and H(π) of DAC considering α-adaptation, where the con-trol coefficient c is -2.0d and d = dim(A). Fig. G.2 shows the performance of DAC consideringα-adaptation with control coefficient c = 0, -0.5d, -1.0d, and -2.0d. Fig. G.3 shows the per-formance of DAC with α = 0.5 and β = 0.1, 0.2, 0.4. Fig. G.4 shows the performance ofSAC, SAC-Div with KL-divergence (SAC-Div(KL)), SAC-Div with JS-divergence (SAC-Div(JS)),and DAC to see the effect of JS divergence on the performance as explained in Section 6.3. Otherhyperparameters follow the default setup given in Table D.1.
