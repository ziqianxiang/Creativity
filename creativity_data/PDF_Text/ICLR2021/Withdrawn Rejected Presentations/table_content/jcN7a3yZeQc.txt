Table 1: Comparison of Max Average Return over 5 trials of 1 million samples. The maximum value is marked bold for each task. ± corresponds to a single standard deviation over trials.					Environments	D2Q	TD3	Methods SAC	DDPG	PPOHalfCheetah	9958.3 ± 935.70	9636.95 ± 859.06	8895.96 ± 3316.5	8577.29	1795.43Hopper	3364.34 ± 583.72	3223.75 ± 514.2	2100.67 ± 1051.6	2020.46	2164.70Walker2d	4727.20 ± 444.71	4582.82 ± 525.60	3475.15 ± 1508.71	1843.85	3317.69Ant	5264.69 ± 632.90	4373.44 ± 1000.33	3250.49 ± 1157.94	1005.30	1082.20Reacher	-3.78 ± 0.32	-3.6 ± 0.56	NA	-6.51	-6.18InvPendulum	1000 ± 0.0	1000 ± 0.0	NA	1000	1000InvDoublePendulum	9200.6 ± 186.22	8911.04 ± 750.58	NA	7741.28 ± 2195.87	8977.94dently trained critics to mitigate the overestimation effect. Averaged-DQN Anschel et al. (2017)takes the average of previously learned Q-values estimates, which results in a more stable trainingprocedure, as well as reduces approximation error variance in the target values. A clipped Double Q-learning called TD3 Fujimoto et al. (2018) extends the deterministic policy gradient Silver & Lever(2014); Lillicrap et al. (2015) to address overestimation bias. In particular, TD3 uses the minimumof two independent critics to approximate the value function suffering from overestimation. Softactor critic Haarnoja et al. (2018) takes a similar approach as TD3, but with better exploration withmaximum entropy method. Maxmin Q-learning Lan et al. (2020) extends Double Q-learning andTD3 to multiple critics to handle overestimation bias and variance.
