Table 1: Continuous-action environment descriptions.
Table 2: Continuous-action environment training hyper-parameters.					Hyper-parameter	SR-DICE	DualDICE	GradientDICE	Direct-SR	Deep TDOptimizer	Adam	Adam	Adam	Adam	Adamψπ, Qπ Learning rate	3e - 4	-	-	3e - 4	3e - 4w Learning rate	3e - 4	-	-	3e - 4	-f Learning rate	-	5e - 5	1e-5	-	-w Learning rate	-	5e - 5	1e-5	-	-u Learning rate	-	-	1e-2	-	-ψπ, Qπ Mini-batch size	256	-	-	256	256w, f, w, u, Mini-batch size	2048	2048	2048	2048	-ψπ, Qπ Target update rate	0.005	-	-	0.005	0.005Visualizations. We graph the log MSE between the estimate of R(π) and the true R(π), wherethe log MSE is computed as log 0.5(X - R(π))2. We smooth the learning curves over a uniformwindow of 10. Agents were evaluated every 1k time steps and performance is measured over 250ktime steps total. Markers are displayed every 25k time steps with offset for visual clarity.
Table 3: Training hyper-parameters for the Atari domain.
