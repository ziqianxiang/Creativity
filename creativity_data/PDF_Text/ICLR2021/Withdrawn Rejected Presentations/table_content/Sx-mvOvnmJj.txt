Table 1: Top-1 accuracy of the decoupling models (cRT and LWS) for ResNet families trained on the ImageNet-LT dataset. We vary the augmentation strategies (with or without mixup α = 0.2) on both two stages.
Table 2: Ablation study for all proposed modules	00 46.i7 H on long-tailed CIFAR-100, IF=100. MU: applying		46.93 46.89 46.87	I：mixup just in Stage-1.	>l: shift learning on batch	o.ι	46.i3 π⅞a	46.83 47.04 46.94				normalization. LAS: label-aware smoothing. 0.2	46.11 	 FY		46.60 46.76 46.96	146.6Module	LT CIFAR-100	0 3				46.08 46.45 EI⅞⅞	MU SL LAS	100	50	10 	 0.4		46.4		46.12 46.50 46.05 0.3	0.4	0.5 V nf twn hv∏prn	区I	区I	区I 0	区I	区I I	I	区I I	I	I	41.2	46.0	58.5 44.2	50.6	62.2	o.5 45.3	51.4	62.8	0.0 0.1	0.2 47.0	52.3	63.0 	 Fionrp S, AhlatiCn Qtnr		46.2 arameters	1 and K in label-aware smoothing.		BN with shift, Acc. 45.3%	BN w/o shift, Acc. 44.2%0.60.30.0-0.3-0.62	4	6	8 IO 12	14	162	4	6	8	10	12	14	16The Frist BN (bnl)Figure 6: Visualization of the changes in the running mean μ and variance σ2. The ResNet-32 based model
Table 3: Top-1 accuracy (%) for ResNet-32 models trained on long tailed CIFAR-10 and CIFAR-100.
Table 4: Top-1 accuracy (%) on ImageNet-LT (left), iNaturalist 2018 (center) and Place-LT (right).
Table 5: Detailed experiment settings on five benchmark datasets. LR: learning rate, BS: batch size,WD: weight decay, and LRS: learning rate schedule, ∆W: learning rate ratio of ∆W.
Table 6: Comprehensive accuracy results on ImageNet-LT with different backbone networks (ResNet-50, ResNet-101 & ResNet-152) and training 180 epochs.
Table 7: Comprehensive accuracy results on iNaturalist 2018 with ResNet-50 and training 200 epochs.
Table 8: Detailed accuracy results on Places-LT, starting from an ImageNet pre-trained ResNet-152.
