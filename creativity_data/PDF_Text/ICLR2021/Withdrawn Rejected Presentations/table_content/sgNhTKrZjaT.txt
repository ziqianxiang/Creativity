Table 1: Hyperparameter table for VAE training on BreakoutParameter	Valuetimesteps	1 X 107environments	16batch size	32tmax	2048K	10ci	1.0C2	0.5C3	0.0Y	0.99λ	0.95network	FC 64 (tanh) -	FC 64 (tanh)optimizer	ADAMlearning rate	3 X 10-4Parameter	Valuetimesteps	1 X 107environments	16batch size	32
Table 2: Policy hyperparameters of PPOfixed Table 3: Policy hyperparameter table ofand PPOadapt	PPOVAE12Under review as a conference paper at ICLR 2021A.3 CHOOSING APPROPRIATE VALUES FOR κIn equation 3, we introduced the hyperparameter κ to balance VAE and PPO gradients. We foundempirically, that tuning κ is straight forward and requires only few trials. In order to simplify thesearch for κ, one can evaluate gradient magnitudes of the different losses at the point where they aremerged at U. Our experiments showed PPO’s gradients to be significantly smaller, thus scaling upthe loss function was appropriate. This will likely differ if the networks are configured differently.
