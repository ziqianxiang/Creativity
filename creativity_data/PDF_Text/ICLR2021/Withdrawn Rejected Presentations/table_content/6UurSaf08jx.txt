Table 1: Experiments on the impact on SAFE vs a regular linear projection using Transformer-base on the WMT’14 EN-DE machine translation benchmark3.3	Sandwich-style Parameter SharingWeight sharing techniques, despite being surprisingly effective, have been relatively unexplored sofar with regard to Transformer Encoder-Decoder Models. However, this has been shown to be apowerful technique for leveraging models with large capacity and less memory usage/computation(Dehghani et al., 2018; Lan et al., 2020; Wu et al., 2019).
Table 2: Experiments performed on WMT’14 EN-DE using different parameter sharing techniques.
Table 3: Results for machine translation on WMT’14 EN-DE and WMT’16 EN-RO task, for ourbase models. Note that the * superscript indicates results from Kasai et al. (2020).
Table 4: Results on the WMT’14 EN-DE for our large models(2018) who propose using deep representations for NMT. For language modeling, we compare to thebase Transformer-XL (Dai et al., 2019) and Deep Equilibrium Model (Bai et al., 2019), which alsoemploys parameter sharing. Lastly, for our summarization task, we compare with specialized archi-tectures such as Pointer-Generator Networks (See et al., 2017), and Convolutional Seq2Seq-basedmodels (Fan et al., 2018), as well as the Transformer model from Edunov et al. (2019).
Table 5: Results on the Wikitext- 1 03 (Merity et al., 2016) language modeling benchmark.
Table 6: Results on the CNN-Daily Mail Summarization task (Nallapati et al., 2016; See et al., 2017)by 0.8 and 0.3 BLEU, respectively. The result is within 0.1 BLEU from the Transformer-big model(210M params), despite a 70% parameter reduction. We believe that these results demonstrate theempirical efficacy of the techniques leveraged in the Subformer.
