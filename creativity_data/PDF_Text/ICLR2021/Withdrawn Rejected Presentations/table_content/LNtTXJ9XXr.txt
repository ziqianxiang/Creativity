Table 1: Clean and robust accuracy of ResNet-18 modelstrained under different settings on CIFAR-10. All robustaccuracy results are obtained using the E = 8/255 '∞ ball.
Table 2: The clean and robust accuracy using different combination coefficient p on CIFAR-10 withResNet-18. The 1st block uses adversarial trained models with and without further standard fine-tuning of batch normalization (BN). The 2nd block uses standard trained models with and withoutfurther adversarial finetuning of BN. All robust accuracies are obtained using e = 8/255 '∞ ball.
Table 3: Comparison on CIFAR-10/100 over ResNet-18, DenseNet-121, Preact-18, and ResNeXt-29. Models are trained for 20 and 100 epochs using normal Batch Normalization (BN), AdvProp andour RobMask. RobMask shows a significant performance improvement on all model architectures.
Table 4: Comparison on ImageNet over ResNet-18. Models are trained for 105 epochs using normalBatch Normalization (BN), AdvProp and our RobMask. RobMask shows a significant performanceimprovement on all model architectures.
Table 5: Robust accuracy under different levels of PGD '∞ attacks on CIFAR-10 with ResNet-18architecture. RobMask clearly outperforms AdvProp and standard adversarial training in all the testperturbation strengths except = 8/255 on which AdvProp and standard adversarial training aretrained.
Table 6: Robust accuracy under different levels of '∞ Autoattacks on CIFAR-10 with ResNet-18architecture. RobMask clearly outperforms AdvProp and standard adversarial training in all the testperturbation strengths except = 8/255 on which AdvProp and standard adversarial training aretrained.
Table 7: Comparison on CIFAR-10 over DenseNet-121 on AdvProp and its extensions. Models aretrained for 100 epochs.
Table 8: RobMask results on CIFAR-10/100 over ResNet-18, DenseNet-121, Preact-18, andResNeXt-29. Models are trained for 20 and 100 epochs.
Table 9: Cosine similarity on under every batch normalization layer under standard fine-tuned train-ing on adversarial trained modelA.3 Adversarial masking visuliazation12Under review as a conference paper at ICLR 2021κ!li∙IRllsl(a) Adversarial training with further stan-	(b) Adversarial trainingdard finetuning of BNFigure 4: Illustration of the Adversarial Masking effect. We mark several feature maps (red andgreen boxes) are blocked out or magnified when comparing (a) and (b), which can be viewed as aselection mask on “non-robust” and “robust” features.
