Table 1: Classification test accuracy, number of parameters (Par), and FLOPs on CIFAR-10 betweenPED and those of the state-of-the-art methods. Par and FLOPs are in million.
Table 2: The top-1 test accuracy, number of parameters (Par), and FLOPs on CIFAR-100 betweenPED and those of the state-of-the-art methods. Par and FLOPs are in million. “〜” means approximatevalue. The accuracy reported in W. et al. (2020) is given by 0.6924 ± 0.24.
Table 3: Classification test accuracy, number of parameters (Par), and FLOPs on SVHN data setbetween PED and those of the state-of-the-art methods. Par and FLOPs are in million.
Table 4: The top-1 and top-5 test accuracy, number of parameters (Par), and FLOPs on ImageNetdata set between PED and those of the state-of-the-art methods. FLOPs is in billion.
Table 5: Comparison of classification test accuracy on Tiny-ImageNet data set, number of parameters,and FLOPs between PED and those of the state-of-the-art unstructured methods. “-" means noreported value. ∆ and J indicate the change and dropping in percentage of the test accuracy withrespect to the base (full) model, respectively.
Table 6: The results of pruning in intermediate stages for ResNet32 and Tiny-ImageNet data set.
Table 7: The results of pruning in intermediate stages for ResNet56 and CIFAR-10 data set.
Table 8: The results of pruning using random selection of units in intermediate stages for ResNet56and CIFAR-10 data set.
Table 9: The results of pruning using units with the largest energy values in intermediate stages forResNet56 and CIFAR-10 data set.
Table 10: The results of pruning in intermediate stages for ResNet164 and CIFAR-10 data set.
Table 11: The results of pruning in intermediate stages for DenseNet100-k12 and CIFAR-10 data set.
Table 12: The results of pruning in intermediate stages for ResNet164 and CIFAR-100 Data Set.
Table 13: The results of pruning in intermediate stages of execution of our algorithm for DenseNet100-k12 and CIFAR-100 data set.
Table 14: Comparison of classification test accuracy using ResNet164 on SVHN data set, number ofparameters, and number of FLOPs between the various stages of proposed pruning method and thoseof the state-of-the-art deep compression methods.
Table 15: The results of pruning in intermediate stages of execution of our algorithm for ResNet50and ImageNet data set.
