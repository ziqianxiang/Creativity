Table 1: Examples of training samples. <s2> is a special <s> for k=2. <m> is short for <mask>.
Table 2: The BLEU scores of our proposed HRT and the baseline methods on four tasks. Unlessotherwise stated, the used beam size is 5. “?” denotes dynamic iterations. “20L” stands for using a20-layer encoder. “NPD” is short for Noisy Parallel Decoding. All HRT models only iterate onceby non-autoregression.
Table 3: Performance against different datastrategies. Cd=1 represents decoding theHRT model in an autoregressive manner.
Table 4: The effect of training by differentchunk sizes. Latency is tested in batch sizeof 16 using Cd=k and bat=bmp=1.
Table 5: Ablation study on WMT’16 En→Ro test set.
Table 6: Time-consuming of decoding newstest2014 by different batch sizes on Titian X GPU.
Table 7: Performance of autoregressive models in the synthetic experiment.
Table 8: Compare hybrid-regressive translation (HRT) to autoregressive translation (AT), itera-tive refinement based non-autoregressive translation (IR-NAT), and semi-autoregressive translation(SAT). Q(i) denotes the computation cost in autoregressive mode when producing the i-th token(e.g., the prefix length is i - 1). Qb(i) denotes the computation cost in non-autoregressive modewhen producing i tokens by one shot with a beam size of b. I=4 〜10, k is generally 2.
Table 9: Apply the optimization methods used in HRT training to MP. BLEU scores are evaluatedon the WMT’14 En→De test set.
