Table 1: Adjusted mutual information (AMI) under transformations. We reported the mean and stdover 10 runs. ‘J’, ‘T’, ‘R’ stand for jittering, translation and rotation respectively.
Table 2: Few-shot classification accuracyBaseline		ModelNet40					Sydney10				5-way		10-way		5-way		10-way		10—shot	20-shot	10-shot	20-shot	10-shot	20-shot	10-shot	20-shot3D-GAN	55.8±10.7	65.8±9.9	40.3±6.5	48.4±5.6	54.2±4.6	58.8±5.8	36.0±6.2	45.3±7.9FoldingNet	33.4 ±13.1	35.8±18.2	18.6±6.5	15.4±6.8	58.9±5.6	71.2±6.0	42.6±3.4	63.5±3.9Latent-GAN	41.6±16.9	46.2±19.7	32.9±9.2	25.5±9.9	64.5±6.6	79.8±3.4	50.5±3.0	62.5±5.1PointCapsNet	42.3±17.4	53.0±18.7	38.0±14.3	27.2±14.9	59.4±6.3	70.5±4.8	44.1±2.0	60.3±4.9PointNet++	38.5±16.0	42.4±14.2	23.1±7.0	18.8±5.4	79.9±6.8	85.0±5.3	55.4±2.2	63.4±2.8PointCNN	65.4±8.9	68.6±7.0	46.6±4.8	50.0±7.2	75.8±7.7	83.4±4.4	56.3±2.4	73.1±4.1PointNet, Rand	52.0±12.2	57.8±15.5	46.6±13.5	35.2±15.3	74.2±7.3	82.2±5.1	51.4±1.3	58.3±2.6PointNet, cTree	63.2±10.7	68.9±9.4	49.2±6.1	50.1±5.0	76.5±6.3	83.7±4.0	55.5±2.3	64.0±2.4PointNet, OcCo	89.7±6.1	92.4±4.9	83.9±5.6	89.7±4.6	77.7±8.0	84.9±4.9	60.9±3.7	65.5±5.5DGCNN, Rand	31.6 ±9.0	40.8±14.6	19.9±6.5	^^16.9±4.8	58.3±6.6	76.7±7.5	48.1±8.2	76.1±3.6DGCNN, Jigsaw	34.3±4.1	42.2±11.0	26.0±7.5	29.9±8.2	52.5±6.6	79.6±6.0	52.7±3.3	69.1±2.6DGCNN, cTree	60.0±8.9	65.7±8.4	48.5±5.6	53.0±4.1	86.2±4.4	90.9±2.5	66.2±2.8	81.5±2.3DGCNN, OcCo	90.6±2.8	92.5±6.0	82.9±4.1	86.5 ±7.1	79.9±6.7	86.4±4.7	63.3±2.7	77.6±3.93.5 Object Classification ResultsWe now compare OcCo against prior initialization approaches on object classification tasks. Table 3compares OcCo-initialization to random (Rand) and (Sauder & Sievers, 2019)’s (Jigsaw) initialization
Table 3: Comparison between OcCo , Jigsaw and Rand initialization on 3D object recognitionbenchmarks. After confirming the scores from (Qi et al., 2017a; Wang et al., 2019b; Uy et al., 2019;Sauder & Sievers, 2019) are reproducible, we reported the mean and standard error over three runs.
Table 4: Overall point prediction accuracy (mAcc) and mean intersection of union (mIoU) onShapeNetPart. We reported the mean and standard error based on three runs.
Table 5: Overall point prediction accuracy (mAcc) and mean class intersection of union (mIoU) onthe S3DIS averaged across 6-cv-fold over three runs. OcCo encoders are pre-trained on ModelNet40.
Table 6: Statistics of occluded datasets for OcCo Pre-trainingName	#ofClass	# of Object	# of Views	# of Points/ObjectShaPeNet Occluded (PCN)	8	-30974-	8	1045ModelNet Occluded (OcCo )	40	-12304-	10	2008518Under review as a conference paper at ICLR 2021The reason of choosing these two datasets for benchmarking is, ShapeNet Occluded is the out-of-domain data for the models pre-trained on ModelNet Occluded, and vice versa. We believe it willgive us sufficient information on which occluded dataset should be preferred the OcCo pre-training.
Table 7: Performance of OcCo pre-trained models with different pre-trained datasetsOcCo Settings		Classification Accuracy	Encoder	Pre-Trained Dataset	ModeINet OC	ShapeNet OcPointNet	ShapeNet Oc	810	94.1	ModeNet Oc	856	95.0PCN	ShapeNet Oc	816	94.4	ModelNet Oc	851	95.1DGCNN	ShapeNet OC	867	94.5	ModelNet Oc	89.1	—	95.1From Table 7, we see that the OcCo models pre-trained on ShapeNet Occluded do not perform aswell as the ones pre-trained on ModelNet Occluded in most cases. Thus in our experiments, wereports the results pre-trained on ModelNet Occluded.
Table 8: Statistics of classification datasetsName	TyPe	# Class	# Training	# Testing(MN40) ModelNet40 (Wu et al., 2015)	synthesized	-40-	9,843	-2,468-(SN10) ScanNet10 (Dai et al., 2017a)	real scanned	-10-	6,110	-1,769-(SO15) ScanObjectNN (Uy et al., 2019)	real scanned	15	2,304	576To make a comprehensive and convincing comparison, we follow the similar procedures from(Achlioptas et al., 2018; Han et al., 2019; Sauder & Sievers, 2019; Wu et al., 2016; Yang et al., 2018),to train a linear Support Vector Machine (SVM) to examine the generalisation of OcCo encodersthat are pre-trained on occluded objects from ModelNet40. For all six classification datasets, wefit a linear SVM on the output 1024-dimensional embeddings of the train split and evaluate it onthe test split. Since Sauder & Sievers (2019) have already proven their methods are better than theprior, here we only systematically compare with theirs. We report the results2 in Table 9, we cansee that all OcCo models achieve superior results compared to the randomly-initialized counterparts,demonstrating that OcCo pre-training helps the generalisation both in-domain and cross-domain.
Table 9: linear SVM on the output embeddings from random, Jigsaw and OcCo initialised encodersDataset	PointNet			PCN 				DGCNN				Rand	Jigsaw	OCCo	Rand	Jigsaw	OCCo	Rand	Jigsaw	OCCoShapeNet10	91.3	91.1	93.9	88.5	91.8	94.6	90.6	91.5	94.5ModelNet40	70.6	87.5	88.7	60.9	73.1	88.0	66.0	84.9	89.2ShapeNet Oc	79.1	86.1	91.1	72.0	87.9	90.5	78.3	87.8	91.6ModelNet Oc	65.2	70.3	80.2	55.3	65.6	83.3	60.3	72.8	82.2ScanNet10	64.8	64.1	67.7	62.3	66.3	75.5	61.2	69.4	71.2ScanObjectNN	45.9	55.2	69.5	39.9	49.7	72.3	43.2	59.5	78.3F Re-Implementation Details of ”Jigsaw” Pre-Training MethodsIn this section, we describe how we reproduce the ’Jigsaw’ pre-training methods from (Sauder &Sievers, 2019). Following their description, we first separate the objects/chopped indoor scenes into33 = 27 small cubes and assign each point a label indicting which small cube it belongs to. We thenshuffle all the small cubes, and train a model to make a prediction for each point. We reformulate thistask as a 27-class semantic segmentation, for the details on the data generation and model training,please refer to our released code.
Table 10: Accuracy comparison between OcCo and prior pre-training baselines Alliegro et al. (2020)on 3D object recognition benchmarks. ModelNet40-20% means only 20% of training data are used.
Table 11: Sample efficiency with randomly-initialized and OcCo-initialized models.
Table 12: Detailed Results on Part Segmentation Task on ShapeNetPartShapes	PointNet				PCN				DGCNN			Rand*	Jigsaw	OCCo	Rand	Jigsaw	OCCo	Rand*	Jigsaw*	OCComean (point)	83.7	83.8	84.4	82.8	82.8	83.7	85.1	85.3	85.5Aero	83.4	83.0	82.9	81.5	^-82y1-	82.4	84.2	84.1	84.4Bag	78.7	79.5	77.2	72.3	74.2	79.4	83.7	84.0	77.5Cap	82.5	82.4	81.7	85.5	67.8	86.3	84.4	85.8	83.4Car	74.9	76.2	75.6	71.8	71.3	73.9	77.1	77.0	77.9Chair	89.6	90.0	90.0	88.6	88.6	90.0	90.9	90.9	91.0Earphone	73.0	69.7	74.8	69.2	^-69y1-	68.8	78.5	80.0	75.2Guitar	91.5	91.1	90.7	90.0	89.9	90.7	91.5	91.5	91.6Knife	85.9	86.3	88.0	84.0	83.8	85.9	87.3	87.0	88.2Lamp	80.8	80.7	81.3	78.5	78.8	80.4	82.9	83.2	83.5Laptop	95.3	95.3	95.4	95.3	95.1	95.6	96.0	95.8	96.1Motor	65.2	63.7	65.7	64.1	64.7	64.2	67.8	71.6	65.5Mug	93.0	92.3	91.6	90.3	90.8	92.6	93.3	94.0	94.4Pistol	81.2	80.8	81.0	81.0	81.5	81.5	82.6	82.6	79.6Rocket	57.9	56.9	58.2	51.8	51.4	53.8	59.7	60.0	58.0Skateboard	72.8	75.9	74.2	72.5	71.0	73.2	75.5	77.9	76.2Table	80.6	80.8	81.8	81.4	81.2	81.2	82.0	81.8	82.8
