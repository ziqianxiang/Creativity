Table 1: Top-1 and Top-5 accuracy of VGG-16 on ImageNet dataset (Deng et al., 2009) in variousnumber formats; delta (∆) indicates the accuracy drop as compared to FP32Weight	Activation	Top-1 (∆)	Top-5 (∆)FP32	FP32	71.59%	90.38%(1,5,2,15)	(1,4,3,7)	69.89% (-1.70%)	89.29% (-1.09%)(1,4,3,7)	(1,4,3,7)	70.86% (-0.73%)	90.02% (-0.36%)(1,4,3,15)	(1,4,3,7)	70.96% (-0.63%)	90.10% (-0.28%)(1,3,4,3)	(1,4,3,7)	70.18%(-1.41%)	89.56% (-0.82%)(1,3,4,7)	(1,4,3,7)	71.19% (-0.40%)	90.12% (-0.26%)(1,3,4,7)	(1,4,3,7)+(0,4,4,7)	71.19% (-0.40%)	90.14% (-0.24%)In addition to weights, it is certainly worth finding out best-fit formats for activations as well. Forthose attempts made above, activations are always in FFP8(1, 4, 3, 7) for two reasons: 1) the overalldistribution of activations is wider than that of weights, and 2) the maximum magnitude of activa-tions is much bigger than that of weights. The detailed distribution of activations will be given inSection 3.4 later. Meanwhile, it is also worth noting that a large set of commonly used activationfunctions always produce nonnegative outputs, e.g., ReLU, ReLU6, and sigmoid. That is, if thoseoutputs are represented in any signed format, a half of the code space is actually wasted. It may notbe a problem for 32-bit and 16-bit formats with long enough fraction bits; however, it is indeed aserious issue for any 8-bit format, which merely has 256 available codes in total. Since VGG-16utilizes ReLU as its activation function, it is feasible to select signed FFP8(1, 4, 3, 7) for the first
Table 2: Top-1 and Top-5 accuracy of VGG-16 after layer-wise optimizationWeight	Activation	Top-1 (∆)	Top-5 (∆)-FP32-	FP32	71.59%	90.38%(1,3,4,7)	(1,4,3,7)+(0,4,4,7)	71.19% (-0.40%)	90.14% (-0.24%)(1,3,4,7)	(1,3,4,6)+(0,4,4,7)	71.38%(-0.21%)	90.33% (-0.05%)(1,2,5,3)	(1,3,4,6)+(0,4,4,7)	71.24% (-0.35%)	90.22% (-0.16%)(1,2,5,*)	(1,3,4,6)+(0,4,4,7)	71.48%(-0.11%)	90.27% (-0.11%)(1,2,5,*)	(1,3,4,6)+(0,4,4,*)	71.48%(-0.11%)	90.32% (-0.06%)Table 3: The FFP8 format of each layer in VGG-16 after layer-wise optimizationLayer	Weight	Activation	Layer	Weight	Activation1	(1,2,5,3)	(1,3,4,6)	8	(1,2,5,5)	(0,4,4,8)2	(1,2,5,4)	(0,4,4,11)	9	(1,2,5,5)	(0,4,4,8)3	(1,2,5,4)	(0,4,4,10)	10	(1,2,5,6)	(0,4,4,8)4	(1,2,5,5)	(0,4,4,10)	11	(1,2,5,5)	(0,4,4,7)5	(1,2,5,4)	(0,4,4,9)	12	(1,2,5,5)	(0,4,4,7)6	(1,2,5,5)	(0,4,4,9)	13	(1,2,5,6)	(0,4,4,8)7	(1,2,5,4)	(0,4,4,9)			3.5	Accuracy Improvement via Model RetrainingAll FFP8 weights in our previous experiments are simply converted from pre-trained weights gen-erated from a typical FP32 training framework. However, it is reported that quantization-aware
Table 3: The FFP8 format of each layer in VGG-16 after layer-wise optimizationLayer	Weight	Activation	Layer	Weight	Activation1	(1,2,5,3)	(1,3,4,6)	8	(1,2,5,5)	(0,4,4,8)2	(1,2,5,4)	(0,4,4,11)	9	(1,2,5,5)	(0,4,4,8)3	(1,2,5,4)	(0,4,4,10)	10	(1,2,5,6)	(0,4,4,8)4	(1,2,5,5)	(0,4,4,10)	11	(1,2,5,5)	(0,4,4,7)5	(1,2,5,4)	(0,4,4,9)	12	(1,2,5,5)	(0,4,4,7)6	(1,2,5,5)	(0,4,4,9)	13	(1,2,5,6)	(0,4,4,8)7	(1,2,5,4)	(0,4,4,9)			3.5	Accuracy Improvement via Model RetrainingAll FFP8 weights in our previous experiments are simply converted from pre-trained weights gen-erated from a typical FP32 training framework. However, it is reported that quantization-awaremodel retraining can usually improve the accuracy (Jacob et al., 2017), which motivates us to checkwhether it can successfully apply to our work. We first train the ResNet-18 model in an FP32 frame-work and the corresponding Top-1 accuracy is 69.76%. Next, an FFP8-based inference is performedunder the following settings: 1) all weights are in FFP8(1, 3, 4, 7), 2) activations of the first layer arein FFP8(1, 3, 4, 6), and 3) activations of the other layers are in FFP8(0, 4, 4, 7). The Top-1 accuracyfor the above configuration is down to 69.44%. Then, a quantization-aware retraining process of 15epochs is applied and the resultant Top-1 accuracy goes back to 69.76%. Note that the retraining isdone simply in a typical FP32 framework without the need of special training skills.
Table 4: Accuracy results of various models under different format configurationsCfg.	Weight	Activation	VGG-16 Top-1 / Top-5	ResNet-50 Top-1 / Top-5	ResNet-34 Top-1 / Top-5	ResNet-18 Top-1 / Top-5A	FP32^^	FP32	71.59/90.38	76.13 / 92.86	73.31 / 91.42	69.76 / 89.08B	(1,4,3,7)	(1,4,3,7)	70.86 / 90.02	75.24 / 92.52	72.39 / 90.97	68.70 / 88.50C	(1,3,4,7)	(1,4,3,7)	71.19 / 90.12	75.38 / 92.68	72.81 / 91.16	69.25 / 88.80D	(1,3,4,7)	(1,3,4,6)+(0,4,4,7)	71.38 / 90.33	75.85 / 92.81	73.12 / 91.33	69.44 / 88.937Under review as a conference paper at ICLR 2021FP32FMABFP16 to AFP32Wgt. / Act.
