Table 1: Optimization perspectives. (a) Classical binarization methods tie each binary weight b to an associated real-valued latent variablew, and quantize each weight by only considering its associated real-valued by using the sign function. (b) Rather than updating the weightsindependent of each other, recent work uses filter-weight statistics when updating the binary weights. (c) Our proposed optimization methoddoes not focus on using the sign function, but rather flips the binary weights based on the distribution of the real weights, thus the binaryweight updates depend on the statistics of the other weights through the rank of w. (d) Recent work moves away from using the sign of thelatent variables, and instead trains the binary network with bit sign flips, however they still consider independent weight updates.
Table 2: Architecture variations. Accuracy comparison ofsign (Rastegari et al. 2016), IR-Net (Qin et al. 2020b) and our bi-half on Conv2/4/6/8 networks using Cifar-10, over 5 repetitions.
Table 3: Comparison with state-of-the-art (a): ImageNet re-sults. We show Top-1 and Top-5 accuracy on ImageNet for a num-ber of state-of-the-art binary networks. Sign is our baseline bycarefully tuning the hyper-parameters. Our proposes bi-half mod-el consistently outperforms the other binarization methods on thislarge-scale classification task.
