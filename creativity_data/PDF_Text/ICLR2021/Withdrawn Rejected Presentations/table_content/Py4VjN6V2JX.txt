Table 1: Lip reading comparison with SOTA. LRW (top-1 accuracy, the higher the better) LRS(WER, the lower the better). The supervised approaches (Superv.) directly train the models onthe dataset listed in the table. The self-supervised approaches (SSL) and Ours pretrain the modelson the datasets listed in the table, and then finetune the model on LRW and LRS2 with the sameprotocol. Underline refers the best results from of the supervised approaches. Blue color highlightsthe comparison of ours with the self-supervised approaches under the same setting.
Table 2: Comparison with SOTA approaches on deepfake detection. For training on DFDC, weuse the same training and testing list as Chugh et al. (2020). Mittal et al. use the same number ofrandomly sampled training data (180K). All the numbers of the other SOTA approaches are collectedfrom Chugh et al. (2020). All the self-supervised approaches (SSL) are pretrained on K-AV-15K,and finetuned and tested on the same training and testing set of DFDC.
Table 3: Comparison of SOTA approaches on action classification and sound classification. Wespecify pretraining dataset and the number of samples used if they are reported in the original papers(N/A: not available). We highlight the best results and the Second best results.
Table 4: Comparison with existing self-supervised pretraining approaches on lip reading and deep-fake detection. All the results are computed based on the implementation by us. We pretrain eachSOTA model with the same pretext task proposed in their paper. “Moment. Cont.”: momentum con-trast; “Pred. Cont.”: predictive contrast; “Dense. Pred. Cont.”: dense predictive contrast; “Glo-locNCE”: global-local NCE; “AVS”: audio-visual sync; “Rot”: rotation detectionModel	Feature	LRS	LRW	DFDC	UCF101	HMDB51w/o loc.	Glob.	70.9	=	65.3	=	67.9	82.3	57.1Ours	Glob.	47.6(( 23.3)	86.8 (↑ 21.5)	92.6 (↑ 24.7)	89.2 (↑ 6.9)	59.9 (↑ 2.8)Ours	Loc.	-46.5	-88.9	95.9	88.5	58.3Ours	Glob.+Loc.	45.1	89.2	96.7	90.1	61.3Table 5: The roles of global and local information on different benchmarks. “Ours”: our model thatpretrained with the whole objective. “Ours w/o loc.”: our model that pretrained without the localcontrastive objective. “Glob.”: features extracted from the global encoder. “Loc.”: features extractedfrom the local encoder. Underline refers to the best results from the other approaches.
Table 5: The roles of global and local information on different benchmarks. “Ours”: our model thatpretrained with the whole objective. “Ours w/o loc.”: our model that pretrained without the localcontrastive objective. “Glob.”: features extracted from the global encoder. “Loc.”: features extractedfrom the local encoder. Underline refers to the best results from the other approaches.
Table 6: Comparison of models pretrained with and without using MIL for local contrast.
Table 7: Evaluation on the role of attention mechanism in our approach. "V" uses only visualsequence and "V+A” uses both visual and audio sequence for finetuning on downstream tasks.
