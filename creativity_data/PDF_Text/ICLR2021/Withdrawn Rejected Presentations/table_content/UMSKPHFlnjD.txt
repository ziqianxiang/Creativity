Table 1: Effect of dense connection on COCO val2017 (%)				Method	AP	AP0.5	AP0.75FPN baseline	36.8	58.0	40.0Without Dense connection(ours)	37.7	59.6	40.3Dense connection(ours)	37.8	60.0	40.64.2	Implementation DetailsOur model is end-to-end trained based on the torchvision detection module (Paszke et al., 2019),using SGD with 0.9 momentum, 0.0005 weight decay for gradient optimization. We train detectorson a single NVIDIA titan xp GPU with the mini-batch size of one image. Unless specified, ResNet-50 pretrained on imagenet (Deng et al., 2009) is taken as the backbone networks on this dataset.
Table 2: Effect of dense connection with different pooling size on COCO val2017 (%)Pooling Size	AP	AP0.5	AP0.75(128,192)	37.8	60.0	40.6(64,96)	38.0	60.1	41.0(32,48)	37.8	59.7	40.9(16,24)	37.7	59.7	40.2Table 3: Effect of different choices of the attention module on COCO val2017 (%)Attention AP	AP0.5	AP0.75conv	38.2	59.9	41.0fc1	37.5	58.8	40.6fc2	37.6	59.4	40.1conv+fc1	37.7	59.5	40.3conv+fc2	37.7	59.2	40.6conv+fc1+fc2	37.9	59.4	40.9feature map tensor of the object proposal. In addition, we connected three parallel output branchesafter the squeeze-excitation module, denote as ”conv+fc1+fc2”. Simultaneously, we also exploreddifferent combinations of attention on single “fc” layer. The results reported in Table 3 indicate thatassembling global context information multiple times in the box head network will cause ambiguity,and mapping it to the low-dimensional features of the local information can better learn the relation-ship between the global context and the local receptive field. By using attention on conv we got the
Table 3: Effect of different choices of the attention module on COCO val2017 (%)Attention AP	AP0.5	AP0.75conv	38.2	59.9	41.0fc1	37.5	58.8	40.6fc2	37.6	59.4	40.1conv+fc1	37.7	59.5	40.3conv+fc2	37.7	59.2	40.6conv+fc1+fc2	37.9	59.4	40.9feature map tensor of the object proposal. In addition, we connected three parallel output branchesafter the squeeze-excitation module, denote as ”conv+fc1+fc2”. Simultaneously, we also exploreddifferent combinations of attention on single “fc” layer. The results reported in Table 3 indicate thatassembling global context information multiple times in the box head network will cause ambiguity,and mapping it to the low-dimensional features of the local information can better learn the relation-ship between the global context and the local receptive field. By using attention on conv we got thebest AP value, which exceeded FPN baseline by 1.4% on COCO’s standard AP metric.
Table 4: Effect of different reduction ratios r on COCO val2017 (%)ratio AP	AP0.5	AP0.754	37.8	59.7	40.58	38.2	59.9	41.016	37.9	59.6	40.67Under review as a conference paper at ICLR 2021Table 5: Effect of Bottom-up and Top-down on COCO val2017 (%)Method AP	AP0.5	AP0.75Bottom-up	37.4	58.7	40.2Top-down	38.2	59.9	41.0Table 6: Comparisons with FPN baseline on Cityscapes datasets with ResNet-50 backbone (%)Method	AP	AP0.5	AP0.75	APs	APm	APlFPN baseline DGCA(ours)	36.2	63.6	34.8	10.6	30.9	51.3 37.4	63.8	38.2	13.0	31.9	52.2COCO val 2017, where Double-Head(with DGCA) means to assemble our method on Double-HeadRCNN (Wu et al., 2020b) based on mmdetection (Chen et al., 2019b), note that in this method weonly applied our method to the fully connected head and we re-implemented Double-Head RCNNusing two Titan XP GPUs with one image per GPU(Schedule_1x). Our method can achieve Contin-uous gains on different backbone networks(1.4% improvement with ResNet-50 and 0.6% improve-ment with ResNet-101) and model(0.7% compared to Double-Head). Table 8 shows the comparison
Table 5: Effect of Bottom-up and Top-down on COCO val2017 (%)Method AP	AP0.5	AP0.75Bottom-up	37.4	58.7	40.2Top-down	38.2	59.9	41.0Table 6: Comparisons with FPN baseline on Cityscapes datasets with ResNet-50 backbone (%)Method	AP	AP0.5	AP0.75	APs	APm	APlFPN baseline DGCA(ours)	36.2	63.6	34.8	10.6	30.9	51.3 37.4	63.8	38.2	13.0	31.9	52.2COCO val 2017, where Double-Head(with DGCA) means to assemble our method on Double-HeadRCNN (Wu et al., 2020b) based on mmdetection (Chen et al., 2019b), note that in this method weonly applied our method to the fully connected head and we re-implemented Double-Head RCNNusing two Titan XP GPUs with one image per GPU(Schedule_1x). Our method can achieve Contin-uous gains on different backbone networks(1.4% improvement with ResNet-50 and 0.6% improve-ment with ResNet-101) and model(0.7% compared to Double-Head). Table 8 shows the comparisonbetween our method with the state-of-the-art methods on MS COCO 2017 test-dev. Compared withFPN baseline, our method is better at detecting small and medium targets, which is due to the in-creased connection between global context and local information.
Table 6: Comparisons with FPN baseline on Cityscapes datasets with ResNet-50 backbone (%)Method	AP	AP0.5	AP0.75	APs	APm	APlFPN baseline DGCA(ours)	36.2	63.6	34.8	10.6	30.9	51.3 37.4	63.8	38.2	13.0	31.9	52.2COCO val 2017, where Double-Head(with DGCA) means to assemble our method on Double-HeadRCNN (Wu et al., 2020b) based on mmdetection (Chen et al., 2019b), note that in this method weonly applied our method to the fully connected head and we re-implemented Double-Head RCNNusing two Titan XP GPUs with one image per GPU(Schedule_1x). Our method can achieve Contin-uous gains on different backbone networks(1.4% improvement with ResNet-50 and 0.6% improve-ment with ResNet-101) and model(0.7% compared to Double-Head). Table 8 shows the comparisonbetween our method with the state-of-the-art methods on MS COCO 2017 test-dev. Compared withFPN baseline, our method is better at detecting small and medium targets, which is due to the in-creased connection between global context and local information.
Table 7: Object detection results (bounding box AP) on COCO val2017.
Table 8: Object detection results (bounding box AP) on COCO test-dev.
