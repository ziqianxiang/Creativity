Table 1: Experimental results on PACS dataset (* denotes the model selected on the target domain).
Table 2: Experimental results on the office-home datasetNetwork	Method	A	C	P	R	Average	ERM	58.46	41.51	70.71	73.35	61.01-ResNet -18	IRM	59.02	41.33	68.27	72.55	60.29	D-SAM	58.03	44.37	69.22	71.45	60.77	JiGen	53.04	47.51	71.47	72.79	61.20	InvarNorm	60.41	43.98	70.64	74.32	62.46ReSNet	ERM	66.03	48.25	74.37	78.00	66.67-50	InvarNorm	67.73	49.14	75.32	79.01	67.51Results We report the domain generalization results on Office-home dataset compared with recentstate-of-the-arts in Table 2. We can see that our InvarNorm consistently outperforms our baselineERM on different network architectures, which proves the flexibility of our InvarNorm. Comparedwith D-SAM and JiGen (Carlucci et al., 2019) that implicitly regularize the network to increase itsdomain generalization ability, the improvement of InvarNorm shows that learning a domain invariantrepresentation on the normalization space is effectiveness.
Table 3: Experimental results on VLCS datasetMethod	V	L	C	S	AverageERM	73.40	59.92	97.35	68.35	74.75IRM	74.11	60.62	97.13	68.57	75.08InvarNorm	74.92	62.71	97.55	68.31	75.87Results In Table 3, we can draw similar observations with the results in PACS and Office-home thatour InvarNorm effectively enhances model generalization ability compared with baseline method.
Table 4: Comparison the performance gain of AdaBN on different models.
Table 5: Comparison of different normalization on PACS and Office-home dataset. (sgl denotes thebest results using the domain-specific branch, ens denotes the ensemble results in DSON, whereensemble operation requires 3 X inference time compared with a single model.)Method	P	A	C	S	Avg	A	C	P	R	AvgDSBN	78.6	66.2	95.5	70.2	~T66~	59.0	45.0	72.7	72.0	62.2IBN	75.3	73.0	92.0	77.4	79.4	55.4	44.8	68.3	72.0	60.1SN	82.5	76.8	93.5	80.8	83.4	54.1	45.0	64.5	71.4	58.8DSON(sgl)	78.7	75.7	95.4	79.5	82.3	-	-	-	-	-DSON(ens)	84.7	77.7	95.9	82.2	85.1	59.4	45.7	71.8	74.7	62.9BN	80.0	74.3	94.6	74.5	-80.9-	58.4	41.5	70.7	73.4	61.0InvarNorm	81.7	75.5	96.1	79.0	83.1	60.9	44.0	70.6	74.3	62.5BN+IN	81.5	76.4	96.7	80.3	83.7	56.3	41.3	67.5	73.7	59.7InvarNorm + IN	83.5	77.5	96.2	81.7	84.8	61.9	43.9	70.5	74.5	62.85 ConclusionWe propose a new learning formulation and training pipeline in learning an invariant neural net-work based on batch normalization. Specifically, our formulation guides the neural network to learninvariant representation by penalizing the normalization statistics change due to the marginal dis-tribution shift and rescaling parameter change due to the conditional distribution shift. InvarNormprovides a new perspective in identifying the the domain bias representation and robustify the rep-resentation to only contain invariant features. Our Extensive experimental results on three domain
