Table 1: Comparison of Terminologies between Game Theory and GANGame Theory terminology ∣ GAN terminologyplayer	generator/ discriminatorstrategy	the parameter setting of generator/ discriminator e.g. πg and πd the sequence of parameters (strategies) till epoch tpolicy	e.g. (πg1,πg2, ..., πgt) Note: Not used in DO-GAN.
Table 2: Inception scores (higher is better) and FID scores (lower is better). The mean and standarddeviation are drawn from running 10 splits on 10000 generated images. The magenta values are theimprovements of the DO-GAN variants compared with their counterparts.
Table 3: Training Hyperparameters	GAN	DCGAN	SNGAN	SGANGenerator Learning Rate	0.0002	0.0002	0.0002	0.0001Discriminator Learning Rate	0.0002	0.0002	0.0002	0.0001batch size	64	64	64	100Adam: beta 1	0.5	0.5	0.5	0.5Adam: beta 2	0.999	0.999	0.999	0.999We implement our proposed method with Python 3.7, Pytorch=1.4.0 and Torchvision=0.5.0. We setthe hyperparameters as the original implementations. We present the hyperparameters set in Table 3.
Table 4: Runtime of DO-GAN on 2D Gaussian Dataset with s = 5, 10, 15Support Set Size	Runtime (GPU hours)s=5	> 1s = 10	0.5627s = 15	0.9989Epoch 20000Figure 9: Training evolution on 2D Gaussian Dataset with s = 5, 10, 15Epoch 2000014Under review as a conference paper at ICLR 2021E Generated images of CelebA and CIFAR- 1 0In this section, we present the training images of CelebA and CIFAR-10 datasets.
