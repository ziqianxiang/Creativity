Table 1: Test Accuracy on some image datasets	MNIST	KMNIST	SVHN	CIFAR10Neural DNF	99.08%	95.43%	90.13%	67.91%standard DNN	99.12%	95.86%	90.74%	70.43%standard DNN (without Binarization)	99.11%	96.02%	91.45%	71.45%SENN	98.48%	92.64%	90.79%	71.09%SENN (without Binarization)	98.50%	91.46%	92.15%	72.32%Now regarding the interpretability, the problem here is, we can in principle inspect3 the meaning ofconcept and rules of Neural DNF, but since we have no constraints on φ, the extracted features areonly highly discriminative and is not guaranteed to be aligned with human perceptible concepts. Asymbolic DNF that operates on non-interpretable representations is still non-interpretable. We willrevisit this interpretable representation problem in next section; but despite tje first-stage, here wecan still compare the interpretability of the second-stage model. Despite many qualitive argumentsfor favouring the interpretability of the symbolic DNF (appendix B.3), we can further quantitivelyevaluate the faithfulness of models’ interpretability, a critical critera that measures how faithful theexplanation for a particular prediction are to the underlying computation of prediction. We adopt thefaithfulness metric proposed by (Melis and Jaakkola, 2018) which measures the correlation of thechange of the prediction (class probabilities) and the change of the explanation (relevance scores offeatures) upon perturbation of test examples.4 We report the faithfulness metric on test set for DNN,SENN and Neural DNF in fig. 3. For DNN, we use the coefficients of final linear layer as rele-
Table 2: Test Accuracy on CUB dataset	test acc	test acc with human interventionNeural DNF	61.94%	100.00%Concept bottleneck model	72.38%	100.00%Blackbox DNN	74.69%	N/AWhile losing quite much in accuracy, the symbolic nature enables the Neural DNF to do the thingsthe concept bottleneck model cannot. We show in fig. 4 (top) a correctly predicted sample for theclass of ‘Chestnut sided Warbler’ and in fig. 4 (middle) an incorretly predicted sample and howhuman intervention can correct the prediction. DNF rules are a very human-readable format, thatthe rules are very sparse involving only a few concepts and the logical operation is intuitive tounderstand. The DNF rules also alleviate the burden of human intervention compared to the conceptbottleneck, as only a few concepts are used in the decision rule, a human user can only check andcorrect these concepts indicated in the rules and does not have to go through every concepts which7Under review as a conference paper at ICLR 2021，DNF (expressed as IF-THEN rules)Prediction for a test sampleground truth: a Chestnut sided Warbler (testset id 4738)Feature Extractor(Inception-V3 architecture)
