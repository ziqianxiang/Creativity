Table 1: Accuracy of VGG19 and ResNet32 on CIFAR-10/100 at different pruning ratios.
Table 2: Accuracy of ResNet50 on ImageNet at different pruning ratios. ProbMask steadily beatsprevious state-of-the-art methods on Hessian-based pruning, weight magnitude pruning, dense-to-sparse training and SParse-to-sparse training. RIGL improves with the help of ERK (Erdos-Renyi-Kernel) but will result in doubling the FLOPs at inference time, so we put it in Figure 2).
Table 3: Comparing layerwise sparsity budget and global sparsity budget of ProbMask on ResNet32.
Table 4: The bold-face probability learning rate 6e-3 is the only hyperparameter obtained by gridsearch on CIFAR-10 experiments on a small size network Conv-4 (Frankle & Carbin, 2018) andapplied directly to larger datasets and networks. This demonstrates the generality of our proposedProbMask to different datasets, different networks and different tasks, i.e., pruning networks andfinding supermasks. Other hyperparameters are applied following the same practice of previousworks (Ramanujan et al., 2020; Kusupati et al., 2020; Liu et al., 2018; Zhu & Gupta, 2017). Thechannels of ResNet32 for CIFAR experiments are doubled following the same practice of Wanget al. (2020). The temperature annealing scheme follows the same practice of Xie et al. (2018)A.2 Applying ProbMask for Finding Supermasks in Randomly WeightedNeural NetworksPrevious works on supermasks, i.e, subnetworks achieving good performance with weights fixed atrandom state, focus on sparsity region [10%, 90%]. Here, we would like to explore the performanceof supermasks with higher sparsity, [90%, 99%]. We conduct experiments on modern architetureResNet32 and dataset CIFAR-100, a harder task than CIFAR-10 where a large portion of previousexperiments are conducted. In this experiment, weights are fixed at initialization state by KaimingNormal He et al. (2015). Hyperparameters follow the same as previous CIFAR experiments. Ac-cording to Figure 6, we observe that ProbMask easily scales to ultra sparse region with about 50%accuracy and 2% remaining weights, while state-of-the-art method edge-popup Ramanujan et al.
Table 5: ResNet32 on CIFAR-100 for finding supernetsDataset		CIFAR-100		Ratio	90%	95%	98%	99%ReSNet32	75.94	-	-	-edge-popup	62.94	53.07	26.6	15.39ProbMask	63.67	59.51	48.59	37.91A.3 Proof for equation 3Proof. The PDF (probability density function) of Gumbel(μ, 1) isf(z; μ) = e3μi-(Zi).	(8)The CDF (cumulative distribution function) of Gumbel(μ, 1) isF(z； μ) = e-e-”"μ.	(9)We just need to prove that∀i, P (log(si) - log(1 - si) + g1,i - g2,i ≥ 0) = si.	(10)g1,i and g2,i are two Gumbel(0, 1) random variables sampled for si. The probability is taken withrespect to g1,i and g2,i. si can be seen as a constant in the following proof.
