Table 1: A breakdown of VQA models by indicating which method is used with respect to theirvision, language, inference, and training components. Refer to Appendix A for a detailed descriptionof these methods.
Table 2: Sample and computational efficiency for pixel attention and object-centric vision represen-tations.
Table 3: Performance of DePe and other VQA models using 100% and 10% QA pair supervision.
Table 4: Absolute validation accuracy gains by adding pre-training loss weights. The gains werecomputed against the baseline method of α = β = 1.
Table 5: LXMERT CLEVR performance on different subsets of data and configurations.
Table 6: Description of functional arguments and constants.
Table 7: Implementation details for each modular function. mdet, mnum, mattr correspond to differ-ent parts in the memory representing attentional masks, numerical results, and attributes. parg is thedistribution of functional arguments produced by the question parser, while Prel and Pattr are rela-tion and attribute predictions given by the perception module. Hyper-parameters D = 50, τ = 0.25and γ = 0.5 and attribute functions are split further by each attr ∈ {shape, color, material, size}.
