Table 1: Comparison between PFPN and DISCRETE on four benchmarks using PPO/DPPO whileVarying the resolution of eaCh aCtion dimension.Training stoPs when a fixed number of samPles ismet as shown in Figure 1. RePorted numbers denote final PerformanCe aVeraged oVer 5 trials ± std.
Table 3: Lower bound of the minimum error that DISCRETE attains with different action resolu-tions in the one-step bandit task.
Table 4: Default Hyperparameters in Baseline PFPN BenchmarksTable 4 lists the default hyperparameters used in all of our experiments. Regarding PPO and A2C, inall Roboschool tasks except for the HumanoidBulletEnv-v0 one, we use a single worker thread; forHumanoidBulletEnv-v0 and DeepMimic tasks, we exploit the advantage of distributed training anduse DPPO (synchronous PPO) and A3C (asynchronous A2C) with multiple worker threads, whileIMPALA is natively multi-thread.
Table 5: Performance comparison of PPO/DPPO using PFPN, uniform discretization (DISCRETE),Gaussian baselines (Gaussian), Gaussian baselines with a larger policy network (Gaussian-Big), andpolicies of fully state-dependent Gaussian mixture model (GMM). Reported numbers denote finalperformance averaged over 5 trials ± std after training is done. In each test case, the number of thepolicy network parameters is listed in parentheses below the reported performance.
