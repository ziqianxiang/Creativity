Table 1: Uncertainties (_u) and positives (_p) of 6 sample disease findings from two labelers.
Table 2: Averaged classification accuracy (with 5 noisy label sets) on CIFAR-10 testing set (clean).
Table 3: Classification AUCs for 14 findings in Chest X-Rays using the testing set of MIMIC-CXR (with NLP generated labels) and OpenI dataset (hand-labeled GT). E-cardio: enlarged-cardiomediastinum; Pneum-x: pneumothorax. More details in Table 6 of the Appendix.
Table 4: Averaged classification AUCs for 14 findings using the MIMIC-CXR 1K hand-labeledTest set. negbio_u1, negbio_u0, chexpert_u1, and chexpert_u0 are derived from the original NLPmined labels by setting the uncertainty to either 1 or 0. See all diseasesâ€™ AUCs in the Appendix.
Table 5: Averaged AUCs with the baseline R50 from four label sets.
Table 6: Classification AUCs for 14 findings in both Chest X-ray images and associated text reportsusing the testing set of MIMIC-CXR (with NLP generated labels) and OpenI dataset (hand-labeledGT). E-cardio: enlarged-cardiomediastinum; Pneum-x: pneumothoraxA.4 Additional Experiments on Chest X-ray Images and Textual ReportsIn addition to the classification results using image only shown in the main text, we show the completeresults for classifying the image and text together on the testing set of MIMIC-CXR (with NLPmined image labels) and OpenI (with hand-labeled labels). We observe similar results on the image-text classification task in comparison to image-only ones. The data from OpenI dataset are from adifferent institute to the one of MIMIC-CXR data (our training data). Therefore, the domain gapshall be considered when we examine the performance. Table 6 additionally shows the AUCs foreach disease findings. Indeed, the text report contains more information about the disease diagnosis(maybe more than the image itself). We also observe increase of the overall AUCs. In this case,our proposed meta-training with attention-on-label scheme also helps to boost the classificationperformance with a significant amount. As pointed out before, TieNet achieves better classificationresults in some of the categories since they adopted a more complicated text embedding network(hard to implement with no open code released) and we believe better results could be obtained if ourlearning process was applied on the same model.
Table 7: Classification AUCs for 14 findings in Chest X-Ray image and text report using the testingset of MIMIC-CXR (with NLP generated labels) and MIMIC-CXR 1K hand-labeled Test set.
