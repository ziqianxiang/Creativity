Table 1: Comparison of techniques used for diverse, conditional generation. The majority of themethods insert additional loss terms, while some of them even require additional networks to betrained to achieve diverse generation results. MVP learns a non-deterministic mapping withoutadditional networks or loss terms, thus simplifying the training. Nevertheless, as we empiricallyexhibit in sec. H.7, dedicated works that tackle diverse generation can be used in conjunction withthe proposed MVP to further boost the diversity of the synthesized images.
Table 2: Comparison of attributes of polynomial-like neural networks. Even though the architecturesof Karras et al. (2019); Chen et al. (2019); Park et al. (2019) were not posed as polynomial expansions,we believe that their success can be (partly) attributed to the polynomial expansion (please checksec. F for further information). Π-Net and StyleGAN are not designed for conditional data generation.
Table 3: SymbolsSymbol	RoleN	Expansion order of the polynomialk	Rank of the decompositionsZi , ZII	Inputs to the polynomialn,ρ W rn,ρs	Auxiliary variables	Parameter tensor of the polynomialUrns , C, β	Learnable parameters*	Hadamard product4Under review as a conference paper at ICLR 20213.1	Two input variablesGiven two input variables 1 zι, Zn P Kd where K J R or K J N, the goal is to learn a functionG : Kdxd → Ro that captures the higher-degree interactions between the elements of the two inputs.
Table 4: Quantitative evaluation on class-conditional generation with resnet-based generator (i.e.,SNGAN). Higher Inception Score (IS) (Salimans et al., 2016) (lower Frechet Inception Distance(FID) (Heusel et al., 2017)) indicates better performance. The baselines improve the IS of SNGAN,however they cannot improve the FID. Nevertheless, SNGAN-MVP improves upon all the baselinesin both the IS and the FID.
Table 5: Quantitative evaluation on class-conditional generation with Π-Net-based generator. In CI-FAR10, there is a considerable improvement on the IS, while in Cars196 FID drops dramatically withMVP. We hypothesize that the dramatic improvement in Cars196 arises because of the correlationsof the classes. For instance, the SUV cars (of different carmakers) share several patterns, whichare captured by our high-order interactions, while they might be missed when learning differentnormalization statistics per class.
Table 6: Quantitative evaluation on super-resolution with Π-Net-based generator on Cars196. Thetask on the left is super-resolution 16^, while on the right the task is super-resolution 8^. Ourvariant of SPADE, i.e., SPADE-MVP (details in sec. G), vastly improves the original SPADE. Thefull two-variable model, i.e., MVP, outperforms the compared methods.
