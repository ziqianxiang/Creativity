Table 1: The comparisons of four fitting functions in terms of RMSE and R-Square.
Table 2: The number of videos, the number of target categories and the detailed settings for optimizationplanning on HMDB51, UCF101, ActivityNet, SS-V1, SS-V2, Kinetics-400 and Kinetics-600 datasets.
Table 3: The comparisons between optimization planning (OP) and hand-tuned strategies with different 3DConvNets on Kinetics-400 dataset. The number in the bracket denotes the best number of epoches, which isachieved by grid-search for hand-tuned strategies and adaptively determined for our optimization planning.
Table 4: The comparisons between optimization planning and hand-tuned strategy with DG-P3D on HMDB51(split 1), UCF101 (split1), ActivityNet, SS-V1, SS-V2 and Kinetics-400 datasets. The backbone is ResNet-50pre-trained on ImageNet. The time cost for grid search/optimization planning is reported with 8 NVidia Titan VGPUs in parallel.
Table 5: Performance comparisons with the state-of-the-art methods with RGB input on (a) UCF101 (3splits)&HMDB51 (3 splits) and (b) ActivityNet.
Table 6: Performance comparisons With the state-of-the-art methods With RGB input on SS-V1 and SS-V2.
Table 7: Comparisons with state-of-the-art methods on Kinetics-400 & Kinetics-600. The computationalcomplexity is measured in GFLOPs Ã— vieWs and the vieWs represent the number of clips sampled from the fullvideo during inference. * In vieW that it is not that fair to directly compare irCSN pre-trained on IG65M (65MWeb videos) and other methods, here We report the performance of irCSN pre-trained on Sports1M.
Table 8: Computational cost of different 3D ConvNets architecture.
