Table 1: Supervised results on GLUE dataset. Darker color indicates more improvement. Better in color.
Table 2: Semi-supervised results on GLUE dataset (CoLA, SST-2, STS-B, MNLI). Results for QNLI, MRPC,and QQP are in the Supplementary Material (SM). Darker color indicates more improvement. Better in color.
Table 3: Image-Text retrieval results with Recall@K (R@K). Upper panel: Flickr30K, lower panel: MSCOCO.
Table 4: Variational language modeling results on PTB (left) and Yelp dataset (right).
Table 5: Results for different K choices on GLUE dataset. Th 80% method are also listed as acomparison.
Table 6: Semi-supervised results on GLUE dataset (QNLI, MRPC, and QQP).
Table 7: Variational language modeling results on PTB (left) and Yelp dataset (right).
Table 8: Results on GLUE dataset.
Table 9: Results for different Î» choices on GLUE dataset.
Table 10: Supervised results on GLUE dataset with BERT-large model.
Table 11: VQA validation dataset resultsC Training detailsImage-Text Retrieval For the Flickr30K data, we train the model for 30 epochs. The initiallearning rate is set to 0.0002, and decays by a factor of 10 after 15 epochs. For MS-COCO data,we train the model for 20 epochs. The initial learning rate is set to 0.0005, and decays by 10 after10 epochs. We set the batch size to 128, and threshold the maximum gradient norm to 2.0 forgradient clipping. We also set the dimension of the GRU and joint embedding space to 1024, and thedimension of the word embedding to 300.
