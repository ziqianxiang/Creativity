Table 1: Evaluation on Atari games. @N represents the amount of RL interaction utilized. Mdn is the medianof human-normalized scores, M is the mean, > 0 is the number of games with better than random performance,and > H is the number of games with human-level performance. On each subset, we mark as bold the highestscore. Since different papers report different results of supervised RL e.g SimPLe, we choose the best availableresults and contrast them to APT’s results. The results of VISR are cited from Hansen et al. (2020) as the sourcecode is not publicly available. Raw scores of each Atari game given Table 5 (Appendix). Top: data-limited RL.
Table 2: Comparison of fine-tuning representation, fine-tuning both representation and RL agent on Atari games.
Table 3: Comparison of fine-tuning on continuous control environments in OpenAI Gym. Maximum value foreach task is bolded. ± corresponds to a single standard deviation over 3 runs with random seed.
Table 4: Comparison of fine-tuning on DMControl. Models are pre-trained on Cheetah, Hopper, and Walker,and subsequently fine-tuned on respective downstream tasks. The ’sparse’ denotes reward is sparse. APTsignificantly outperforms baselines in most of sparse reward tasks.
Table 5: Comparison of raw scores of each method on Atari games. Results are averaged over five randomseeds. ■ denotes dense reward hard exploration games. ■ denotes sparse reward hard exploration games. @Nrepresents the amount of RL interaction utilized at fine-tuning phase.
Table 6: The action repeat hyper-parameter used for each environment.
Table 7: Hyper-parameters in the DeepMind control suite experiments.
Table 8: Hyper-parameters in the Atari suite experiments.
