Table 1: Comparison of applied approximations vs related low-cost neural network training works.
Table 2: Memory-related properties of variables used during training. To obtain the exemplaryquantities of total memory given, BinaryNet was trained for CIFAR-10 classification with Adam.
Table 3: Test accuracy of non-binary and BNNs using standard and proposed training approachesfor various models and datasets optimized with Adam. Results for our training approach applied tothe former are included for reference only; we do not advocate for its use with non-binary networks.
Table 4: Memory footprint and per-batch energy consumption of the standard and our proposedtraining schemes for various models using the Adam optimizer.
Table 5: Accuracy, memory and energy imPacts of moving from standard to our ProPosed data reP- resentations. We include block floating-Point ∂X to illustrate the imPortance of dynamic range over Precision for its rePresentation. For these exPeriments, BinaryNet was trained to classify CIFAR-10.							OPtimizer	Data tyPe		Batch norm.	ToP-1 test accuracy		Memory saving (×)1	Energy saving (×)1	∂W	∂Y		%	∆ (PP)1			float32	float32	l2	88.74	—	—	—	float16	float16	l2	88.71	-0.03	2.00	1.09	bool	float16	l2	87.93	-0.81	2.27	1.10Adam	bool	int52	l2	81.12	-7.62	2.50	4.32	bool	po2_5	l2	89.47	0.73	2.50	4.01	bool	po2_5	l1	87.64	-1.10	2.50	4.01	bool	po2_5	ProPosed	88.59	-0.15	3.60	4.36	float32	float32	l2	88.52	—	—	—	float16	float16	l2	88.54	0.02	2.00	1.09SCn ʊ/itb SGD with	bool	float16	l2	87.35	-1.17	2.31	1.10	bool	int5	l2	81.89	-6.63	2.59	4.40momentum	bool	po2_5	l2	89.08	0.56	2.59	4.06	bool	po2_5	l1	88.69	0.17	2.59	4.06	bool	po2_5	ProPosed	87.45	-1.07	4.07	4.45	float32	float32	l2	91.38	—	—	—	float16	float16	l2	91.36	-0.02	2.00	1.09	bool	float16	l2	90.54	-0.84	2.37	1.10
Table 6: Test accuracy, memory footprint and per-batch energy consumption of the standard and ourproposed training schemes for ResNetE-18 classifying ImageNet with Adam used for optimization.
