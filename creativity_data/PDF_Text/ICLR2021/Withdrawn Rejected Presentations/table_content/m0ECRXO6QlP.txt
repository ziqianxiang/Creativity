Table 1: Validation accuracy of a ResNet50 pre-trained on ImageNet with access to 10% of the labels (left table)and 1% of labels (right table). All SimCLR implementations are pre-trained for a certain number of epochs andthen fine-tuned on the available labels. Using SuNCEt to leverage available labels during pre-training (not onlyfine-tuning) accelerates training, and produces better models with much less compute.
Table 2: Evaluating transfer learning performance of a ResNet50 pre-trained on ImageNet. Using SuNCEtto leverage available labels during pre-training (not just fine-tuning) always improves transfer relative toself-supervised pre-training with same number of pre-training epochs.
Table 3: Validation accuracy of a ResNet50 pre-trained on ImageNet with access to 10% of labels.
Table 4: Non-parametric inference. Validation accuracy of a ResNet50 pre-trained on ImageNet for 400-epochswith access to 10% of labels using SimCLR+SuNCEt with the default SimCLR hyper-parameters. Inference isconducted non-parametrically by computing the similarity of validation images to the 10% labeled train images.
Table 5: Validation accuracy of a ResNet50 pre-trained on ImageNet with access to 100% of labels, using theLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLSimCLR, using an unsupervised batch-size of 4,096 samples and a supervised batch-size of 1,280 samples.
