Table 1: FID (smaller is better) and Disentanglement (larger is better) scores are shown. We compareWGAN (Arjovsky et al., 2017), DMWGAN (Khayatkhoei et al., 2018), β-VAE (Higgins et al.,2016), InfoGAN (Chen et al., 2016) with our model. The mean and std. values are computed from10 (MNIST) and 5 (3D-Chair) replicated experiments.
Table B.1: Q-WGAN architecture used for MNIST dataset (Q-β-VAE architecture is similar. Seebelow for the differences.)Generator	DiscriminatorInput(8)	Input(1,28,28)Full(1024), BN,LReLU(0.2)	Conv(c=64, k=4, s=2, p=1), BN, LReLU(0.2)Full(6272), BN,LReLU(0.2)	Conv(c=128, k=4, s=2, p=1), BN, LReLU(0.2)ReshaPeTo(128,7,7)	ReshapeTo(6272)ConvTrs(C=64, k=4, s=2,p=1), BN, LReLU(0.2)	Full(1024), BN, LReLU(0.2)ConvTrs(C=32, k=4, s=2,p=1), BN, LReLU(0.2)	Full(1)ConvTrs(c=1, k=3, s=1, p=1), Tanh	11Under review as a conference paper at ICLR 2021B.1.1	Notes on the Other Compared ModelsOverall, we match the architecture of other models with our model for fair comparison. Somedifferences to note are:•	DMWGAN: We used 10 generators. Each generator has the same architecture as oursexcept the number of features or the channels are divided by 4, to match the number oftrainable parameters. Note that 4 is the suggested number from the original paper.
Table B.2: Q-WGAN architecture used for 3D-Chair dataset.
Table B.3: Q-WGAN architecture used for UT-Zap50k dataset.
