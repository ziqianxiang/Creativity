Table 1: Automatic evaluation results. OOC: Out-of-candidate. Pass: Good candidates thatpass the Response Filter. Slct.: Persuasive candidates selected by the Response Imitator. Strag.:Candidates with strategies. The baselines only generate one response, so metrics that involve multiplecandidates such as OOC do not apply and are left blank. *p<0.05, **p<0.01.
Table 2: Human evaluation results. Nonrep.: Nonrepetitiveness. Const: Consistency. Fluc.:Fluency. Pers.: Persuasiveness. All.: Overall experience. Dnt.: Average donation. DntP.: Donationprobability. *p<0.05, **p<0.01.
Table 3:	Dialogue examples from RFI and RFI - RL with ratings. For RFI, it attempts to persuadewith various strategies; the persuasive utterances with strategies are highlighted (in the order ofcredibility appeal, emotion appeal and foot-in-the-door). Compared to RFI, the responses from RFI -RL are shorter with fewer persuasion strategies.
Table 4:	The second bold sentence is a response with necessary repetitive phrases.
Table 5: Dataset Statistics of the PersuasionForGood dataset.
Table 6:	Another dialogue example from our RFI model. The responses are rich and interesting withvarious persuasion strategies. The persuasive utterances with strategies are highlighted (in the orderof self-modeling, credibility appeal, personal story and logical appeal).
Table 7:	Another dialogue example from our RFI - RL model. Compared to RFI, the responses areshorter and less persuasive with occasional inconsistency (e.g. “It is very easy to persuade peopleto donate”). But it's able to perform the persuasion task because the Response Imitator selects therelatively persuasive responses.
Table 8:	One dialogue example from our RFI - RL - Demo model. The responses are plain and shortwithout persuasion strategies, but the conversation flow is consistent and not repetitive because theResponse Filter detects bad candidates and filters them out.
Table 9:	One dialogue example from the baseline ARDM. The sentences are very repetitive and notconsistent with the context.
