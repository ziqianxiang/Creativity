Table 1: Retrieval results for Flickr30K, comparing only methods that use raw images as input, andvector representations for the embedding space. Additional methods can be found in Appendix A.
Table 2: Retrieval results for COCO, comparing only methods that use raw images as input, andvector representations for the embedding space. Additional methods can be found in Appendix A.
Table 3: VQA Results for three different reader models on COCO val-set. Vanilla - models usepre-trained BERT model. PT - Pre-Training with the knowledge source. XTRA-10C - training viaour method using the knowledge source indicated and alignment model trained on that knowledgesource, using 10 retrieved captions. 5-GT - training with the 5 ground truth captions.
Table 4: VQA performance using models trained with 10 retrieved caption, and evaluating with-out any retrievals (”unplugged”). The highest decrease in performance occurs for the in-domain(COCO) knowledge source where retrieved examples are most informative.
Table 5: Retrieval results for Flickr30K. Top - methods that use raw images as input, and vectorrepresentations for the embedding space. Bottom Methods that use detection features or sequencesimilarity measures.
Table 6: Retrieval results for COCO. Top - methods that use raw images as input, and vector rep-resentations for the embedding space. Bottom Methods that use detection features or sequencesimilarity measures.
