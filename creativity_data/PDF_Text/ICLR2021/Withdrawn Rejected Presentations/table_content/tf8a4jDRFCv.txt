Table 1: Different functions achievable by varying the parameters in the formulation in Eq. 2â€¢	LAF performs comparably to PNA on random graph generation tasks, outperformingseveral graph neural networks architectures including GAT (Velickovic et al., 2018) andGIN (Xu et al., 2019)The rest of this work is structured as follows. In Section 2 we define the LAF framework and showhow appropriate parametrizations of LAF allow to represent a wide range of popular aggregationfunctions. In Section 3 we discuss some relevant related work. Section 4 reports synthetic and real-world experiments showing the advantages of LAF over (sets of) predifined aggregators. Finally,conclusions and pointers to future work are discussed in Section 5.
Table 2: Results on the Point Cloud classification task. Accuracies with standard deviations (calcu-lated on 5 runs) for the ModelNet40 dataset.
Table 3: Results on Text Concept Set Retrieval on LDA-1k, LDA-3k, and LDA-5k. Bold valuesdenote the best performance for each metric.
Table 4: Results on the Multi-task graph properties prediction benchmark. Results are expressed inlog 10 of mean squared error.
