Table 1: Summary of different methods for DP training of neural networksOptimizers	Privacy	Speed	Memory	Generalizability-Non-DP SGD-	-X-	J	-J-	JDP-SGD-Vanilla	-✓-	-X-	-J-	JDP-SGD-MUltiPle	-✓-	-J-	X	JDP-SGD-OUter	-✓-	-J-	-J-	XDP-SGD-JL	J	J	J	J 一As we can see, none of these approaches for speeding up DP-SGD completely solve the problem and fall short in at leastone dimension. In this work, we propose a new algorithmic framework based on JL-projections for fast differentiallyprivate training of deep neural networks, which bypasses the expensive step of exactly computing per-sample gradientnorms. We summarize this discussion in Table 1.
Table 2: Seconds Per ePoch to train the network from PaPernot et al. (2020) (7 conVolutional layers and 1 fully-connectedlayers) with 623,146 Parameters. We set σ = 0.9, C = 2, B = 256, η = 0.1, E = 20.
Table 3: Seconds per epoch to train our RNN with 598,274 parameters. We set β1 = 0.9, β2 = 0.999, σ = 0.6, C =1,B=256,η=0.001,E=20.
Table 4: 2-layer CNN in OpacusTensorflow Privacy with 26,010 parameters and batch size of 256. AlexNet inKrizhevsky et al. (2012) (5 convolutional layers and 3 fully-connected layers) with 21,598,922 parameters and batchsize of 64. The double vjp implementation is in PyTorch and the rest of them are in Tensorflow.
Table 5: VGG13 (10 convolutional layers and 3 fully-connected layers). This network contains 32,394,562 parameters.
