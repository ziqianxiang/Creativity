Table 1: The average correlation between gradient flow measures and generalization performanceMeasure Correlation to Test Loss Correlation to Test Accuracy	Sparse	Dense	Sparse	Dense||g||1 (5)	0.3705	0.3940	0.3551	0.3750||g||2 (5)	0.3732	0.3181	0.3167	0.3840egf1 (6)	0.4155	0.4168	0.3992	0.4041egf2 (6)	0.4373	0.3323	0.3833	0.3774||g||1 (5)	0.4030	0.4411	0.4286	0.3720||g ||2 (5)	0.3998	0.4008	0.3974	0.3913egf1 (6)	0.4362	0.4506	0.4418	0.3821egf2 (6)	0.4048	0.4121	0.4142	0.3990We compare the average absolute Kendall Rank correlation between different formulations ofgradient flow and generalization. The subscript denotes the p-norm (l1 or l2 norm). We see thatEGF has higher absolute correlation when compared to standard gradient flow measures. We alsosee that is consistent across Fashion MNIST (see Appendix B.1).
Table 2: Different network configurations for sparse and dense comparisonsConfigurationVariantsOptimizersRegularization/Normalization methodNumber of hidden layersDense WidthActivation functionsBatch SizeLearning RateSGD , SGD with mom (0.9) ,Adagrad , RMSprop and AdamNo regularization, L2/Weight Decay , Data Augmentation ,Skip Connections and Batchnorm.
Table 3: Wilcoxon Signed Rank Test Results for ReLU networks with four hidden layers, trained onCIFAR-100, using different learning rates. We use a p-value of 0.05, the bold values indicate wheresparse networks perform better than dense networks in a statistical significance manner (reject H0from 4), while non-bold values indicate that it is possible dense networks have the same or better testaccuracy in that configuration. The performance results for these networks are presented in Figure2a, 12 and 10a .
Table 4: The average correlation between gradient flow measures and generalization performancefor FMISTâ€”	Measure	Correlation to Test Loss		Correlation to Test Accuracy			Sparse	Dense	Sparse	DenseS	||g||1 (5)	0.3259	0.2522	0.3536	0.3487	||g ||2 (5)	0.3207	0.2702	0.3139	0.3318	egf1 (6)	0.3534	0.2522	0.3748	0.3487	egf2 (6)	0.3672	0.3017	0.2314	0.3335Gradient Flow for Dense and Sparse MLPs with Four Hidden Layers on CIFAR-100Figure 6: Gradient Flow in CIFAR-100 using ||g ||1C Detailed Results for SC-SDCIn this section, we presented the detailed results for our experiments.
Table 5: Test Accuracy summary for CIFAR-10 with low learning rate (0.001)(a)	One Hidden Layer	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	54.537 +/- 0.91	55.259 +/- 0.36	56.098 +/- 2.395	55.497 +/- 3.022	54.565 +/- 0.909	55.269 +/- 0.545	54.279 +/- 2.034	55.981 +/- 0.349Adam	51.391 +/- 1.066	52.476 +/- 0.48	37.955 +/- 2.874	42.404 +/- 6.494	48.671 +/- 0.766	49.312 +/- 1.033	52.668 +/- 1.275	53.321 +/- 0.892RMSProp	48.999 +/- 0.76	51.507 +/- 0.817	36.289 +/- 4.053	40.62 +/- 7.105	47.7 +/- 1.926	48.145 +/- 2.468	52.581 +/- 1.617	53.045 +/- 1.282SGD	53.714 +/- 1.564	55.023 +/- 0.48	57.091 +/- 1.524	55.089 +/- 3.08	53.775 +/- 1.559	55.091 +/- 0.46	54.364 +/- 2.363	55.996 +/- 0.696SGD with mom(0.9)	53.774 +/- 1.968	54.746 +/- 0.804	56.98 +/- 1.027	57.849 +/- 0.754	54.083 +/- 1.72	55.051 +/- 1.06	54.684 +/- 2.083	55.951 +/- 0.604(b)	Two Hidden Layers	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	54.179 +/- 1.92	55.333 +/- 0.558	59.739 +/- 3.03	58.967 +/- 4.313	53.813 +/- 1.783	55.123 +/- 0.301	53.703 +/- 2.711	56.471 +/- 0.317Adam	52.003 +/- 0.91	53.785 +/- 1.146	42.095 +/- 7.723	49.468 +/- 7.948	50.588 +/- 0.683	51.681 +/- 0.668	57.541 +/- 1.525	57.886 +/- 1.171RMSProp	51.352 +/- 1.198	53.499 +/- 1.5	46.872 +/- 4.916	51.919 +/- 5.436	50.382 +/- 0.602	50.52 +/- 0.918	57.432 +/- 1.512	57.974 +/- 1.151SGD	52.895 +/- 1.918	53.905 +/- 1.735	59.893 +/- 2.16	55.865 +/- 5.475	52.931 +/- 1.852	53.941 +/- 1.724	54.153 +/- 2.259	56.459 +/- 0.413SGD with mom(0.9)	53.777 +/- 1.583	53.357 +/- 3.18	61.814 +/- 1.959	62.367 +/- 2.177	53.835 +/- 1.728	53.26 +/- 3.309	56.081 +/- 2.158	57.575 +/- 0.428(c)	Four Hidden Layers	No Regularization			Data Augmentation		Dense		L2 Sparse		Batchnorm			Skip Connections			Dense	Sparse		Dense	Sparse					Dense	Sparse		Dense		Sparse
Table 6: Test Loss summary for CIFAR-10 with low learning rate (0.001)(a)	One Hidden Layer	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	1.768 +/- 0.156	1.677 +/- 0.216	1.274 +/- 0.066	1.29 +/- 0.084	1.712 +/- 0.127	1.615 +/- 0.18	1.564 +/- 0.033	1.548 +/- 0.079Adam	142.604 +/- 46.942	102.403 +/- 61.214	11.263 +/- 6.312	9.691 +/- 7.296	6.296 +/- 1.944	5.542 +/- 1.923	6.242 +/- 0.969	6.144 +/- 0.972RMSProp	70.0 +/- 15.672	62.098 +/- 25.008	12.061 +/- 8.758	25.578 +/- 26.538	4.582 +/- 1.216	5.503 +/- 1.938	6.644 +/- 1.002	7.31 +/- 1.16SGD	1.977 +/- 0.168	1.616 +/- 0.161	1.253 +/- 0.04	1.306 +/- 0.084	1.939 +/- 0.166	1.595 +/- 0.151	1.699 +/- 0.069	1.663 +/- 0.017SGD with mom(0.9)	3.076 +/- 0.56	2.598 +/- 0.047	1.438 +/- 0.112	1.407 +/- 0.113	2.42 +/- 0.413	2.092 +/- 0.055	2.208 +/- 0.122	2.145 +/- 0.021(b)	Two Hidden Layers	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	2.962 +/- 0.329	2.53 +/- 0.679	1.178 +/- 0.067	1.198+/- 0.107	2.621 +/- 0.2	2.151 +/- 0.442	2.62 +/- 0.089	2.161 +/- 0.339Adam	116.26 +/- 25.299	143.085 +/- 69.247	2.006 +/- 0.305	2.232 +/- 0.474	3.698 +/- 0.234	3.379 +/- 0.4	7.469 +/- 1.768	7.309 +/- 1.877RMSProp	148.115 +/- 30.097	161.511 +/- 74.983	2.192 +/- 0.179	2.832 +/- 0.981	4.339 +/- 0.144	3.985 +/- 0.514	7.416 +/- 1.368	7.193 +/- 1.495SGD	2.917 +/- 0.488	2.008 +/- 0.366	1.157 +/- 0.051	1.268 +/- 0.145	2.809 +/- 0.47	1.959 +/- 0.336	2.41 +/- 0.249	2.01 +/- 0.134SGD with mom(0.9)	4.054 +/- 0.818	3.971 +/- 1.438	2.442 +/- 0.452	1.79 +/- 0.503	2.95 +/- 0.574	2.948 +/- 1.025	2.788 +/- 0.359	2.376 +/- 0.128(c)	Four Hidden Layers	No Regularization		Data Augmentation		L2		Batchnorm		Skip Connections		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	Sparse
Table 7: Test Accuracy summary for CIFAR-100 with low learning rate (0.001)(a)	One Hidden Layer	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	26.76 +/- 0.886	27.39 +/- 0.644	26.958 +/- 2.287	26.22 +/-2.659	27.023 +/- 1.035	27.588 +/- 0.874	21.491 +/- 1.413	22.974 +/- 0.299Adam	23.016 +/- 1.688	24.185 +/- 0.657	13.401 +/- 1.361	16.3 +/- 4.214	21.771 +/- 1.059	22.191 +/- 0.493	24.255 +/- 2.204	25.111 +/- 0.906RMSProp	22.115 +/- 1.05	23.764 +/- 0.737	12.805 +/- 1.663	15.415 +/- 3.953	20.953 +/- 1.012	22.063 +/- 0.747	24.009 +/- 2.011	24.715 +/- 1.092SGD	26.911 +/- 1.177	27.043 +/- 1.958	26.525 +/- 1.234	24.081 +/- 3.158	27.051 +/- 1.155	27.086 +/- 2.041	21.014 +/- 2.475	23.002 +/- 0.659SGD with mom(0.9)	25.155 +/- 2.71	26.243 +/- 1.264	29.007 +/- 1.125	29.577 +/- 1.318	25.663 +/- 2.55	26.632 +/- 1.299	21.649 +/- 2.645	23.565 +/- 0.631(b)	Two Hidden Layers	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	28.21 +/- 1.497	28.748 +/- 1.701	28.773 +/- 2.741	27.423+/-4.207	28.495 +/- 1.572	29.063 +/- 1.805	24.932 +/- 3.263	27.318 +/- 1.216Adam	20.423 +/- 1.323	23.28 +/- 2.231	12.156 +/- 4.33	17.349 +/- 6.332	20.977 +/- 0.507	22.596 +/- 1.217	28.15 +/- 1.968	28.558 +/- 1.696RMSProp	18.585 +/- 2.791	21.013 +/- 4.295	12.551 +/- 4.858	16.499 +/- 5.901	21.065 +/- 0.566	21.967 +/- 1.24	28.151 +/- 2.113	28.132 +/- 1.841SGD	27.125 +/- 1.31	24.099 +/- 7.827	27.951 +/- 1.625	22.25 +/- 6.112	27.271 +/- 1.262	24.039 +/- 7.938	23.861 +/- 4.083	26.622 +/- 1.036SGD with mom(0.9)	26.972 +/- 2.252	26.289 +/- 3.761	31.985 +/- 2.717	31.633 +/- 2.95	27.328 +/- 2.375	26.621 +/- 3.86	25.421 +/- 3.022	27.196 +/- 1.268(c)	Four Hidden Layers	No Regularization		Data Augmentation		Dense	L2	Sparse	Batchnorm		Skip Connections		Dense	Sparse	Dense	Sparse				Dense	Sparse	Dense	Sparse
Table 8: Test Loss summary for CIFAR-100 with low learning rate (0.001)(a)	One Hidden Layer	No Regularization		Data Augmentation		L2		Batchnorm		Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	3.715 +/- 0.262	3.581 +/- 0.301	3.147 +/- 0.1	3.177 +/- 0.116	3.631 +/- 0.215	3.491 +/- 0.255	3.922 +/- 0.107	3.846 +/- 0.171Adam	440.694 +/- 189.721	342.236 +/- 207.693	38.232 +/- 23.824	84.171 +/- 110.671	46.989 +/- 22.993	26.927 +/- 22.742	15.53 +/- 2.784	15.77 +/- 2.959RMSProp	261.914 +/- 81.721	358.131 +/- 234.544	39.084 +/- 24.3	743.694 +/- 668.044	37.436 +/- 14.38	48.654 +/- 47.815	15.824 +/- 1.809	16.68 +/- 1.186SGD	3.655 +/- 0.03	3.305 +/- 0.141	3.184 +/- 0.062	3.283 +/- 0.134	3.617 +/- 0.03	3.289 +/- 0.13	4.015 +/- 0.101	3.867 +/- 0.052SGD with mom(0.9)	7.038 +/- 1.577	5.627 +/- 0.09	3.538 +/- 0.221	3.373 +/- 0.195	5.608 +/- 1.201	4.62 +/- 0.016	5.008 +/- 0.495	4.609 +/- 0.039(b)	Two Hidden Layers	No Regularization		Data Augmentation			L2		Batchnorm		Dense	Sparse	Dense		Sparse	Dense	Sparse	Dense	SparseAdagrad	4.687 +/- 0.772	4.178 +/- 0.871	3.128 +/-	0.081	3.154 +/- 0.152	4.351 +/- 0.559	3.884 +/- 0.623	4.17 +/- 0.129	3.784 +/- 0.154Adam	441.033 +/- 76.035	493.788 +/- 215.952	4.593 +/-	0.368	5.09 +/- 1.28	12.361 +/- 0.424	9.121 +/- 2.275	17.406 +/- 4.665	17.608 +/- 4.662RMSProp	346.064 +/- 99.04	460.908 +/- 212.828	4.157 +/-	0.293	6.024 +/- 1.985	13.274 +/- 0.776	11.817 +/- 3.779	16.036 +/- 2.394	17.701 +/- 1.539SGD	4.395 +/- 0.151	3.551 +/- 0.445	3.048 +/-	0.077	3.327 +/- 0.31	4.303 +/- 0.142	3.53 +/- 0.434	4.293 +/- 0.714	3.733 +/- 0.025SGD with mom(0.9)	8.28 +/- 2.146	7.393 +/- 1.534	6.507 +/-	1.228	4.739 +/- 1.349	6.048 +/- 1.465	5.623 +/- 1.288	5.096 +/- 0.873	4.337 +/- 0.068(c)	Four Hidden LayersNo Regularization			Data Augmentation			L2 Sparse	Batchnorm		Skip Connections		Dense	Sparse	Dense	Sparse	Dense		Dense	Sparse	Dense	Sparse
Table 9: Test Accuracy summary for CIFAR-10 with high learning rate (0.1)(a) Four Hidden LayersNo Regularization	Data Augmentation	L2	Batchnorm	Skip Connections	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	19.469 +/- 10.563	31.377 +/- 15.801	22.62 +/- 16.083	37.885 +/- 17.664	19.739 +/- 12.699	40.133 +/- 14.52	56.194 +/- 1.319	57.536 +/- 0.712	15.615 +/- 9.853	30.756 +/- 17.113Adam	10.0 +/- 0.0	10.0 +/- 0.005	10.0 +/- 0.0	10.0 +/- 0.0	9.998 +/- 0.012	9.98 +/- 0.036	53.191 +/- 2.687	54.964 +/- 1.88	9.999 +/- 0.003	10.001 +/- 0.005RMSProp	9.999 +/- 0.003	10.001 +/- 0.01	10.0 +/- 0.0	10.001 +/- 0.003	10.0 +/- 0.0	10.191 +/- 0.578	53.328 +/- 1.022	53.62 +/- 1.056	10.0 +/- 0.004	10.001 +/- 0.011SGD	57.603 +/- 1.546	56.487 +/- 3.866	51.81 +/- 6.396	60.065 +/- 1.465	8.255 +/- 19.659	30.631 +/- 26.518	59.191 +/- 1.805	60.295 +/- 0.949	57.599 +/- 1.302	56.398 +/- 2.893SGD with mom(0.9)	9.303 +/- 2.608	12.872 +/- 14.06	10.0 +/- 0.0	19.086 +/- 18.81	10.579 +/- 10.646	35.2 +/- 23.74	57.822 +/- 1.012	59.058 +/- 0.886	9.334 +/- 2.582	9.117 +/- 9.037Table 10: Test Loss summary for CIFAR-10 with high learning rate (0.1)(a) Four Hidden Layers	No Regularization		Data Augmentation		Dense	L2 Sparse	Batchnorm		Skip Connections		Dense	Sparse	Dense	Sparse			Dense	Sparse	Dense	SparseAdagrad	4.332 +/- 3.705	6.299 +/- 3.469	2.036 +/- 0.344	1.895 +/- 0.263	3.21 +/- 1.888	3.186 +/- 1.288	5.979 +/- 0.403	5.19 +/- 1.109	3.934 +/- 2.999	5.788 +/- 3.694Adam	2.328 +/- 0.066	6.323 +/- 11.003	2.303 +/- 0.0	2.303 +/- 0.0	405749.405 +/- 1051430.43	343515.597 +/- 1330067.381	51.96 +/- 34.783	60.026 +/- 18.807	4.174 +/- 7.207	2.366 +/- 0.17RMSProp	1339.999 +/- 5147.876	275.11 +/- 831.374	2.303 +/- 0.0	2.303 +/- 0.002	212.06 +/- 812.388	43851.25 +/- 122539.555	40.846 +/- 17.869	107.835 +/- 42.366	30.677 +/- 104.926	349.709 +/- 1315.569SGD	6.609 +/- 1.155	7.01 +/- 2.525	4.964 +/- 1.373	3.744 +/- 1.348	0.339 +/- 0.897	1.659 +/- 1.237	3.758 +/- 0.389	3.254 +/- 0.341	4.634 +/- 0.791	4.674 +/- 1.132SGD with mom(0.9)	2.138 +/- 0.615	3.166 +/- 4.273	2.303 +/- 0.0	2.129 +/- 0.359	1.797 +/- 0.975	2.175 +/- 0.895	6.208 +/- 0.244	5.396 +/- 0.905	2.149 +/- 0.595	4.388 +/- 8.92824Under review as a conference paper at ICLR 2021
Table 10: Test Loss summary for CIFAR-10 with high learning rate (0.1)(a) Four Hidden Layers	No Regularization		Data Augmentation		Dense	L2 Sparse	Batchnorm		Skip Connections		Dense	Sparse	Dense	Sparse			Dense	Sparse	Dense	SparseAdagrad	4.332 +/- 3.705	6.299 +/- 3.469	2.036 +/- 0.344	1.895 +/- 0.263	3.21 +/- 1.888	3.186 +/- 1.288	5.979 +/- 0.403	5.19 +/- 1.109	3.934 +/- 2.999	5.788 +/- 3.694Adam	2.328 +/- 0.066	6.323 +/- 11.003	2.303 +/- 0.0	2.303 +/- 0.0	405749.405 +/- 1051430.43	343515.597 +/- 1330067.381	51.96 +/- 34.783	60.026 +/- 18.807	4.174 +/- 7.207	2.366 +/- 0.17RMSProp	1339.999 +/- 5147.876	275.11 +/- 831.374	2.303 +/- 0.0	2.303 +/- 0.002	212.06 +/- 812.388	43851.25 +/- 122539.555	40.846 +/- 17.869	107.835 +/- 42.366	30.677 +/- 104.926	349.709 +/- 1315.569SGD	6.609 +/- 1.155	7.01 +/- 2.525	4.964 +/- 1.373	3.744 +/- 1.348	0.339 +/- 0.897	1.659 +/- 1.237	3.758 +/- 0.389	3.254 +/- 0.341	4.634 +/- 0.791	4.674 +/- 1.132SGD with mom(0.9)	2.138 +/- 0.615	3.166 +/- 4.273	2.303 +/- 0.0	2.129 +/- 0.359	1.797 +/- 0.975	2.175 +/- 0.895	6.208 +/- 0.244	5.396 +/- 0.905	2.149 +/- 0.595	4.388 +/- 8.92824Under review as a conference paper at ICLR 2021Figure 16: Test Accuracy for CIFAR-100 with 0.001 Learning RateFigure 17: Test Accuracy for CIFAR-10 with 0.1 Learning RateTable 11:	Test Accuracy summary for CIFAR-100 with high learning rate (0.1)(a) Four Hidden LayersNo Regularization	Data Augmentation	L2	Batchnorm	Skip Connections	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	4.832 +/- 3.339	11.731 +/- 7.078	10.304 +/- 7.42	14.827 +/- 8.334	5.383 +/-5.543	14.899 +/- 9.16	28.804 +/- 1.905	29.826 +/- 1.857	5.885 +/- 3.829	11.649 +/- 6.443Adam	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	0.999 +/- 0.007	22.995 +/- 1.308	23.792 +/- 2.673	1.0 +/- 0.0	1.001 +/- 0.003RMSProp	1.0 +/- 0.0	1.001 +/- 0.004	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.008	24.424 +/- 1.281	25.13 +/- 1.514	0.999 +/- 0.003	0.999 +/- 0.005
Table 11:	Test Accuracy summary for CIFAR-100 with high learning rate (0.1)(a) Four Hidden LayersNo Regularization	Data Augmentation	L2	Batchnorm	Skip Connections	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	Sparse	Dense	SparseAdagrad	4.832 +/- 3.339	11.731 +/- 7.078	10.304 +/- 7.42	14.827 +/- 8.334	5.383 +/-5.543	14.899 +/- 9.16	28.804 +/- 1.905	29.826 +/- 1.857	5.885 +/- 3.829	11.649 +/- 6.443Adam	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	0.999 +/- 0.007	22.995 +/- 1.308	23.792 +/- 2.673	1.0 +/- 0.0	1.001 +/- 0.003RMSProp	1.0 +/- 0.0	1.001 +/- 0.004	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.0	1.0 +/- 0.008	24.424 +/- 1.281	25.13 +/- 1.514	0.999 +/- 0.003	0.999 +/- 0.005SGD	25.719 +/- 11.926	23.833 +/- 11.53	11.46 +/- 5.91	22.79 +/- 10.489	28.546 +/- 8.056	24.077 +/- 11.917	30.83 +/- 2.898	31.749 +/- 1.631	26.638 +/- 9.891	28.838 +/- 3.675SGD with mom(0.9)	1.0 +/- 0.0	5.53 +/- 6.788	1.0 +/- 0.0	8.005 +/- 9.069	4.471 +/- 5.342	12.283 +/- 10.29	31.041 +/- 2.264	31.778 +/- 1.709	1.003 +/- 0.009	1.689 +/- 2.47725Under review as a conference paper at ICLR 2021Figure 18: Gradient Flow for CIFAR-10 with 0.1 Learning RateTable 12:	Test Loss summary for CIFAR-100 with low learning rate (0.1)(a) Four Hidden LayersNo RegularizationDense	SparseData AugmentationDense	SparseL2Dense	Sparse
Table 12:	Test Loss summary for CIFAR-100 with low learning rate (0.1)(a) Four Hidden LayersNo RegularizationDense	SparseData AugmentationDense	SparseL2Dense	SparseBatchnormDense	SparseDenseSkip ConnectionsSparseAdagradAdamRMSPropSGDSGD with mom(0.9)7.667 +/- 4.9474.664 +/- 0.201
