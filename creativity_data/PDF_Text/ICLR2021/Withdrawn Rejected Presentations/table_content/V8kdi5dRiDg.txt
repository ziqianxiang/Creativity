Table 1: The comparison of unsupervised object segmentation methods. For our model, we report theperformance averaged over ten runs. For the best model, we also report the standard deviation values.
Table 2: The comparison of unsupervised saliency detection methods. For BigBiGAN and E-BigBiGAN we report the mean values over 10 independent runs.
Table 3: The comparison of E-BigBiGAN to the WSOL state-of-the-art. For E-BigBiGAN we reportthe mean values over 10 independent runs. Despite being completely unsupervised, E-BigBiGANperforms on par with the WSOL methods, which were trained under more supervision.
Table 4: Comparison of our method with the weakly-supervised BigGAN-based approach.
Table 5: Impact of different components in the E-BigBiGAN pipeline.
Table 6: Comparison of masks generation with supervised U2Net-guided synthetic labelingB GANs latent segmentationWe have shown that on pair with BigGAN, the BigBiGAN has a latent direction responsible for imagesegmentation. This remains true for other state-of-the-art generative models. As an additional experi-ment, we explored the latent space of StyleGAN2 (Karras et al. (2020)) trained on LSUN-Churchdataset (Yu et al. (2015)) and following Voynov & Babenko (2020) successfully reveal directions thathave different effects on foreground / background pixels drastically increasing foreground red channelwhile keeping background colors closer to the original. The direction and examples of saliency masksfor LSUN-Church are visualized in Figure 6. So, both StyleGAN2 and BigBiGAN can differentiatebetween object/background, being unsupervised. Note, however, that the BigBiGANâ€™s domain ismuch broader, and its synthetic data can be used for a wider range of tasks.
