Table 1: Hyper-parameters in data generationHyper-parametersBlock depth (bdepth)Block size (bsize)Block number (bnum)Expression complexity (Ec)Descriptionmaximal level of nested control dependencies (‘loop’ or ‘branches’).
Table 2: Accuracy (%) comparison between N-Bref and alternative methods in (a) type recovery, and (b)AST generation. Ins2AST is a previous neural-based program decompiler (only code sketch generation stage).
Table 3: Ablation study of N-Bref on AST generation. ‘-ensemble’ refers to disable the separation of datatype recovery and AST generation.
Table 4: Hyper-parameters chosen for each neural network model	Lang2logic	Ins2AST	Baseline Transformer	N-BrefBatch Size	50	50	16	16	—Number of encoder layer	2	2	3	N 1=3Number of decoder layer	2	一	2	—	3	N2=2, N3=2Number of head (d)	-	-	8	8	—Encoder Type	LSTM	N-ary LSTM	Multi-head Attention	Multi-head Attention +GNNDecoder TyPe	Tree LSTM	Tree LSTM	Multi-head Attention	Multi-head Attention+GNNHidden States Size	256			Embedding size	256			Gradient clip threshold	1.0			13Under review as a conference paper at ICLR 2021B Leetcode s olutions examplesWe present the examples of the tested Leetcode solutions in Figure 1 and 2. The tasks that are testedincludes ”Isomorphic Strings”, ”Multiply Strings”, ”Longest Palindromic Substring”, ”ImplementstrStr()”, ”ZigZag Conversion”. Many easy problems are too short (e.g. ”Length of the last word”)to justify the performance of N-Bref and some of them use their own-defined functions which isbeyond the scope of N-Bref.
Table 5: Comparison between N-Bref and baseline transformer using graph edit distance acrossdatasets.
Table 6: Performance on code vulnerability detection.
Table 7: Performance of code similarity accuracy.
