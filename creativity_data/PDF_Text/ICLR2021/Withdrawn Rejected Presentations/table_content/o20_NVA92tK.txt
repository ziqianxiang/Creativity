Table 1: ImageNet-200 and ImageNet-R top-1 error rates. ImageNet-200 uses the same 200 classesas ImageNet-R. DeepAugment+AugMix improves over the baseline by over 10 percentage points.
Table 2: SVSF classification error rates. Networks are robust to some natural distribution shifts butare substantially more sensitive the geographic shift. Here Diverse Data Augmentation hardly helps.
Table 3: DeepFashion Remixed results. Unlike the previous tables, higher is better since all values aremAP scores for this multi-label classification benchmark. The “OOD” column is the average of therow’s rightmost eight OOD values. All techniques do little to close the IID/OOD generalization gap.
Table 4: A highly simplified account of each hypothesis when tested against different datasets.
Table 5: ImageNet-C Blurs (Defocus, Glass, Motion, Zoom) vs Real Blurry Images. All values areerror rates and percentages. The rank orderings of the models on Real Blurry Images are similar tothe rank orderings for “ImageNet-C Blur Mean,” so ImageNet-C,s simulated blurs track real-worldblur performance.
Table 6: A highly simplified account of each hypothesis when tested against different datasets. Thistable includes ImageNet-A results.
Table 7: ImageNet-200 and ImageNet-Renditions error rates. ImageNet-21K and WSL Pretrainingtest the Pretraining hypothesis, and here pretraining gives mixed benefits. CBAM and SE test theSelf-Attention hypothesis, and these hurt robustness. ResNet-152 and ResNeXt-101 32×8d test theLarger Models hypothesis, and these help. Other methods augment data, and Style Transfer, AugMix,and DeepAugment provide support for the Diverse Data Augmentation hypothesis.
Table 8: Clean Error, Corruption Error (CE), and mean CE (mCE) values for various models andtraining methods on ImageNet-C. The mCE value is computed by averaging across all 15 CE values.
Table 9: ImageNet-A top-1 accuracy.
Table 10: Various distribution shifts represented in our three new benchmarks. ImageNet-Renditionsis a new test set for ImageNet trained models measuring robustness to various object renditions.
Table 11: Number of images in each training and test set. ImageNet-R training set refers to theILSVRC 2012 training set (Deng et al., 2009). DeepFashion Remixed test sets are: in-distribution,occlusion - none/slight, occlusion - heavy, size - small, size - large, viewpoint - frontal, viewpoint -not-worn, zoom-in - medium, zoom-in - large. StreetView StoreFronts test sets are: in-distribution,capture year - 2018, capture year - 2017, camera system - new, country - France.
Table 12: Clean Error, Corruption Error (CE), and mean CE (mCE) values for DeepAugment ablationson ImageNet-C. The mCE value is computed by averaging across all 15 CE values.
Table 13: DeepAugment ablations on ImageNet-200 and ImageNet-Renditions.
