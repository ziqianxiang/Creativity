Table 1: FVD on BAIRMethod2	FVD Q)SV2P	262.5LVT	125.8SAVP	116.4VideoGen (ours)	112.5DVD-GAN-FP	109.8TrIVD-GAN-FP -	103.3Video Transformer	94 ± 2Table 2: Comparing FVD and FVD* values forBAIR on different transformer architectures. C =single-frame conditional, U = unconditional. TheAxial Attention model follows the transformer ar-chitectures described in Ho et al. (2019b). GPTand GPT Small follow the same architecture, butGPT Small containing half the number of layersas GPT. FVD* is computed similar to FVD, butusing reconstructed dataset examples instead of theoriginal dataset examples	bits/dim	FVD	FVD*
Table 2: Comparing FVD and FVD* values forBAIR on different transformer architectures. C =single-frame conditional, U = unconditional. TheAxial Attention model follows the transformer ar-chitectures described in Ho et al. (2019b). GPTand GPT Small follow the same architecture, butGPT Small containing half the number of layersas GPT. FVD* is computed similar to FVD, butusing reconstructed dataset examples instead of theoriginal dataset examples	bits/dim	FVD	FVD*Axial (C)	3.94	170.1	133.3GPT Small (U)	4.53	230.4	187.0GPT (U)	4.08	191.7	164.2GPT (C)	2.95	112.5	94.24.3	BAIR Robot PushingFor BAIR, the VQ-VAE downsamples by a factor of 4 over space, and 2 over time (32x reduction),with 4 attention residual blocks. We use a codebook of 2048 codes, each 256-dim embeddings. Thetransformer prior has a hidden size of 384 and 22 layers. For single-frame conditioning, we jointlytrain a ResNet-34 encoder that encodes the frame to a 512-dim vector.
Table 4: Ablation on positional encodingsTable 3: Ablation on attention in VQ-VAE. FVD ______________________________________________________is with reconstructed examples			Position Encoding	bits/dim Q)	FVD (1)VQ-VAE Architecture	NMSE ⑷	FVD Q)	Sin-Cosine	3.94	232			None	5.96	1697No Attention	0.014	156	Temporal Only	5.84	1360With Attention	0.010	125	Spatial Only	4.82	300			Spatial + Temporal	4.53	230significantly better result. Table 2 shows the results of training a smaller unconditional transformer onBAIR, where it achieves both a worse bits per dim and FVD (230) compared to the larger transformer(192).
Table 3: Ablation on attention in VQ-VAE. FVD ______________________________________________________is with reconstructed examples			Position Encoding	bits/dim Q)	FVD (1)VQ-VAE Architecture	NMSE ⑷	FVD Q)	Sin-Cosine	3.94	232			None	5.96	1697No Attention	0.014	156	Temporal Only	5.84	1360With Attention	0.010	125	Spatial Only	4.82	300			Spatial + Temporal	4.53	230significantly better result. Table 2 shows the results of training a smaller unconditional transformer onBAIR, where it achieves both a worse bits per dim and FVD (230) compared to the larger transformer(192).
Table 5: Hyperparameters of VQ-VAE encoder and decoder models for Moving MNIST, ViZDoom(HGS = Health Gathering Supreme), and BAIRI Moving MNIST		ViZDoom (HGS)	BAIR / ViZDoom (Battle2)Input size	16 X 64 X 64	16 x 64 x 64	16 x 64 x 64Latent size	4 x 16 x 16	4x 16x 16	8x 16x 16β (commitment loss coefficient)	0.25	0.25	0.25Batch size	64	64	64Learning rate	7 x 10-4	7 x 10-4	7 x 10-4Hidden units	240	240	240Residual units	128	128	128Residual layers	2	4	4Uses attention	No	Yes	YesCodebook size	512	2048	2048Codebook dimension	64	256	256Encoder filter size	3	3	3Upsampling conv filter size	4	4	4Training steps	20k	75K	75kA.2 Prior NetworksTable 6: Hyperparameters of prior networks for Moving MNIST, ViZDoom (HGS), BAIR andViZDoom (Battle2).
Table 6: Hyperparameters of prior networks for Moving MNIST, ViZDoom (HGS), BAIR andViZDoom (Battle2).
