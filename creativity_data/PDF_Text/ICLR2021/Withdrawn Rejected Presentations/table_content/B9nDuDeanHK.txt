Table 1: Experiments of VGG-7, ResNet-20, and ResNet-56 on Cifar10, and ResNet-18 on Ima-geNet. We put the results of baseline of full-precision networks and BWNs in Table.2. FP indicatesthe first full-precision Conv layer which is not quantized according to the common practice. VGG-7has 6 Conv layers, and we use the quantized bit numbers for each layer to indicate how many se-lected quantized kernels are used. ResNet-20 and ResNet-56 have three groups, each group sharesthe same number of channels which are 16, 32, 64 in order. We assign the same quantized bit numberfor each group. ResNet-18 has four groups which have channels of 64, 128, 256, 512. CR indicatesCompressed Ratio. Acc is the top-1 test accuracy on Cifar-10 and top-1 validation accuracy on Im-ageNet. The accuracy is reported as the mean of the best accuracy during training of 5 runs withdifferent random seeds. More results are displayed in Appendix.H.
Table 2: This is a performance table of different quantization methods with different network archi-tectures on different datasets with different training methods. ”Baseline” means the network withfull precision as the baseline. ”w/o WD” means without weight decay, and ”w/o SF” means withoutusing scaling factors. SF = 1 means we fix scaling factors in all layers to 1. LR×10 means wemagnify learning rate 10 times. Test Acc(Validation Acc on ImageNet) means top 1 accuracy ontesting set(validation set).
Table 3: Experiments of VGG-7, ResNet-20, and ResNet-56 on Cifar10, and ResNet-18 on Im-ageNet. FP indicates the first full-precision conv layer which is not quantized according to thecommon practice. VGG-7 has 6 conv layers, and we use the quantized bit numbers for each layer toindicate how many selected quantized kernels are used. ResNet-20 and ResNet-56 has three groups,each groups share the same number of channels which are 16, 32, 64 in order. We assign the samequantized bit number for each group. ResNet-18 has four groups which has channels of 64, 128,256, 512. CR indicates Compressed Ratio. Acc is the top 1 test accuracy on Cifar-10, and top 1validation accuracy on ImageNet. The accuray is reported as the mean of the best accuracy duringtraining among 5 runs with different random seeds.
Table 4: Experiments of QBN on ResNet-20. We use (mean ± std) to report the results.
Table 5: ResNet-18 on ImageNet funetuned base on a pretrained BWN.
Table 6: We train several networks using the normal top frequent binary-kernels, and those leastfrequent binary-kernels named ”Reverse” in the table since they use the order of reversed top 128frequent kernels. 10.0% in the table indicates the training cannot converge from the beginning.
Table 7: Using different source layers of the selected kernels. The Source Layer is where we extractthe most frequent binary kernels, and target QBN is the network we apply QBN to.
