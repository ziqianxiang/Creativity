Table 1: Task performance on MNIST for different leader’s learning speeds. Note that multitaskperformance gradually worsens as the leader learns faster, being the case where no iterative updatesare performed clearly unstable.
Table 2: Task performance on MNIST for Rotograd and a baseline method using the same architec-ture with different number of parameters. As observed, the effect of negative transfer diminishes asthe model capacity increases.
Table 3: Test results on the MNIST dataset for different methods. The arrow on the each column’stitle describes whether higher is better or the other opposite.
Table 4: Task performance results on the ChestX-ray14 dataset for different methods. All tasks areclassification tasks.
