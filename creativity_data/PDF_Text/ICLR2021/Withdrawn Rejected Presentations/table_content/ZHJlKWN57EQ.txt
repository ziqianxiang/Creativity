Table 1: Model accuracy bottleneck for the standard pure 16-bit training algorithm. Thisalgorithm shows validation accuracy gap compared to 32-bit training. In an ablation of this algo-rithm, we use 32-bit model weights and turn off nearest rounding only on model weight updates.
Table 2: Pure 16-bit training can match 32-bit training on model accuracy. With stochasticrounding or Kahan summation for model weight updates, pure 16-bit training can attain 0.1% lowerto 0.2% higher absolute value for validation accuracy metrics across applications.
