Table 1: Evaluated workload model size and training time. Training time is measured when runningon 1 V100 GPU. CIFAR-10 is trained with batch size 128 for 320 epochs. SWB-300 and SWB-2000are trained with batch size 128 for 16 epochs.
Table 2: CIFAR-10 accuracy (%) with different batch size. Across runs, learning rate is set as 0.1for first 160 epochs, 0.01 for the next 80 epochs and 0.001 for the last 80 epochs. Model accuracyconsistently deteriorates when batch size is over 1024. Bold text in each row represents the highestaccuracy achieved for the corresponding model, e.g., EfficientNet-B0 achieves highest accuracy at91.92% with batch size 1024.
Table 3: CIFAR-10 comparison for batch size 2048, 4096 and 8192, with learning rate set as 0.2,0.4 and 0.8 respectively. All experiments are conducted on 16 GPUs (learners), with batch size perGPU 128, 256 and 512 respectively. Bold texts represent the best model accuracy achieved given thespecific batch size and learning rate. When batch size is 8192, DPSGD significantly outperformsSSGD. The batch size 128 baseline is presented for reference. bs stands for batch-size, lr stands forlearning rate.
Table 4: Heldout loss comparison for SSGD and DPSGD, evaluated on SWB-300 and SWB-2000.
Table 5: CIFAR-10 with batch size 8192. By reducing learning rate, SSGD can escape early trapsbut still lags behind DPSGD. Bold text in each column indicates the best accuracy achieved for thatmodel across different learning rate and optimization method configurations. DPSGD consistentlydelivers the most accurate models.
Table 6: Decreasing learning rate for SWB-300 and SWB-2000 (bs stands for batch-size). Bold text ineach column indicates the best held-out loss achieved across different learning rate and optimizationmethod configurations for the corresponding batch size. DPSGD consistently delivers the mostaccurate models. *The learning rate used here corresponds to batch size 256 baseline learning rate,and we still adopt the same learning rate warmup, scaling and annealing schedule. Thus when thislearning rate reduces by x, the overall effective learning rate also reduces by x.
