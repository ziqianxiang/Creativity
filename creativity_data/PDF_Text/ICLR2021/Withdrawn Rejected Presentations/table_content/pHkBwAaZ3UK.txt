Table 1: Interesting statistics of the datasets used in this paper. (*: multi-label task)	cora	citeseer	pubmed	github	PPi*# Nodes	2,708	3,327	19,717	37,700	56,944# Links	5,429	4,732	44,338	289,003	818,716# Classes	7	6	3	2	121*# Features	1,433	3,703	500	600	50Avg. Degree	2.00	1.42	2.25	7.67	28.8Table 2: Performances of GRARF and baselines on node classification tasks. (% micro-f1)	cora	citeseer	pubmed	github	ppiVanilla GCN (Kipf & Welling, 2017)	87.40 ± 0.36	78.09 ± 0.35	86.18 ± 0.10	81.72 ± 0.14	59.26 ± 0.03GraphSAGE (Hamilton et al., 2017)	82.02 ± 0.31	70.23 ± 0.51	86.26 ± 0.33	78.09 ± 1.23	56.97 ± 0.94GAT (Velickovic et al., 2017)	87.80 ± 0.11	76.43 ± 0.73	85.39 ± 0.12	81.78 ± 0.52	47.29 ± 2.30GIN (Xu et al., 2019)	85.08 ± 0.83	73.87 ± 0.52	84.88 ± 0.17	79.05 ± 0.88	60.35 ± 1.14MixHop (Abu-El-Haija et al., 2019)	88.70 ± 0.19	74.59 ± 0.27	85.60 ± 0.10	79.68 ± 1.00	58.55 ± 0.14Random RF	86.93 ± 1.22	77.31 ± 1.37	86.87 ± 1.30	79.81 ± 7.82	63.27 ± 2.38GRARF	88.34 ± 0.82	79.24 ± 0.88	88.20 ± 0.33	87.30 ± 2.54	66.54 ± 1.12representations are set as d = 128 and an identical two-layer setup is used in all baselines. Hyper-parameters such as learning rates in all models are tuned to achieve best performances on thevalidation sets. All neural models are trained with adequate epochs with the early-stopping strategy.
Table 2: Performances of GRARF and baselines on node classification tasks. (% micro-f1)	cora	citeseer	pubmed	github	ppiVanilla GCN (Kipf & Welling, 2017)	87.40 ± 0.36	78.09 ± 0.35	86.18 ± 0.10	81.72 ± 0.14	59.26 ± 0.03GraphSAGE (Hamilton et al., 2017)	82.02 ± 0.31	70.23 ± 0.51	86.26 ± 0.33	78.09 ± 1.23	56.97 ± 0.94GAT (Velickovic et al., 2017)	87.80 ± 0.11	76.43 ± 0.73	85.39 ± 0.12	81.78 ± 0.52	47.29 ± 2.30GIN (Xu et al., 2019)	85.08 ± 0.83	73.87 ± 0.52	84.88 ± 0.17	79.05 ± 0.88	60.35 ± 1.14MixHop (Abu-El-Haija et al., 2019)	88.70 ± 0.19	74.59 ± 0.27	85.60 ± 0.10	79.68 ± 1.00	58.55 ± 0.14Random RF	86.93 ± 1.22	77.31 ± 1.37	86.87 ± 1.30	79.81 ± 7.82	63.27 ± 2.38GRARF	88.34 ± 0.82	79.24 ± 0.88	88.20 ± 0.33	87.30 ± 2.54	66.54 ± 1.12representations are set as d = 128 and an identical two-layer setup is used in all baselines. Hyper-parameters such as learning rates in all models are tuned to achieve best performances on thevalidation sets. All neural models are trained with adequate epochs with the early-stopping strategy.
