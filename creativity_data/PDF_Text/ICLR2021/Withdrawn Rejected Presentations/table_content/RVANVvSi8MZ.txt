Table 1: Comparison of WLGCNet and previous state-of-the-art models including WL (Sher-vashidze et al., 2011), PSCN (Niepert et al., 2016), DGCNN, SAGPool (Lee et al., 2019), DIFF-POOL, g-U-Net, and GIN on graph classification datasets. We report the graph classification accura-cies (%) on PROTEINS, D&D, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI5K, COLLAB,and REDDIT-MULTI12K datasets._______________________________________________________________________________	PROTEINS	D&D	IMDBM	RDTB	RDT5K	COLLAB	RDT12Kgraphs	1113	1178	1500	2000	4999	5000	11929nodes	39.1	284.3	13	429.6	508.5	74.5	391.4classes	2	2	3	一	2	5	3	11WL	75.0 ± 3.1	78.3 ± 0.6	50.9 ± 3.8	81.0 ± 3.1	52.5 ± 2.1	78.9 ± 1.9	44.4 ± 2.1DGCNN	75.5 ± 0.9	79.4 ± 0.9	47.8 ± 0.9	-	-	73.8 ± 0.5	41.8 ± 0.6PSCN	75.9 ± 2.8	76.3 ± 2.6	45.2 ± 2.8 一	86.3 ± 1.6	49.1 ± 0.7	72.6 ± 2.2	41.3 ± 0.8DIFFPOOL	76.3	80.6	-	-	-	75.5	47.1SAGPool	71.9	76.5	-	-	-	-	-g-U-Net	77.6 ± 2.6	82.4 ± 2.9	51.8 ± 3.7	85.5 ± 1.3	48.2 ± 0.8	77.5 ± 2.1	44.5 ± 0.6GIN	76.2 ± 2.8	82.0 ± 2.7	52.3 ± 2.8	92.4 ± 2.5	57.5 ± 1.5	80.6 ± 1.9	-WLGCNet	78.9 ± 4.2	83.8 ± 2.8	56.1 ± 3.6	94.1 ± 2.2	58.2 ± 3.2	83.1 ± 7.9	50.3 ± 1.5we stack multiple blocks, each of which consists ofa WLGCL and a pooling layer (Gao & Ji, 2019).
Table 2: Comparison of WLGCL and the WLGCL using native implementation (denoted asWLGCLn). We evaluate them on simulated data with different graph sizes in terms of the num-ber of nodes and the number of edges. All layers output 64 feature channels. We report the numberof multiply-adds (MAdd), the amount of memory usage, and the CPU execution time. We describethe input graph size in the format of “number of nodes / number of edges”.
Table 3: Comparison of WLGCNet and previ-ous state-of-the-art models on relatively smalldatasets. We report the graph classification accu-racies on MUTAG, PTC, and IMDBB datasets.
Table 4: Comparison of WLGCNet, the net-work using the same architecture as WLGCNetwith GCN layers (denoted as WLGCNetg), thenetwork using the same architecture as WLGC-Net with regular line graph convolution layers(denoted as WLGCNetl). We report the graphclassification accuracies on REDDIT-BINARY,REDDIT-MULTI5K, and REDDIT-MULTI12K.
