Table 1: AUROC values for MNIST dataset. The best performing method in each column is in bold.
Table 2: AUROC values for Fashion-MNIST dataset. The best performing method in each columnis in bold. For the interest of space, we omit class names in the table and list the names here instead:{T-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot}We also demonstrate that our solution works well in the multi-class setting where more than oneclass can constitute the normal set. In Table 4, p denotes the number of classes constituting thenormal dataset and the percentage denotes the proportion of anomalies in the test set. In everyscenario, our proposed metohd is able to outperform state-of-the-art methods in both AUROC andF1 metrics.
Table 3: AUROC values for CIFAR-10 dataset. The best performing method in each column is inbold. For the interest of space, we omit class names in the table and list the names here instead:{airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck}Yet, the performance boost indicates that there is some additional benefits to be gained from rotatingimages. We speculate that since convolutional neural networks are not rotation-invariant by design,each rotation gets considered as a different domain of normality which the CPC module needs todeal with. Finally, we show that ensembling the scores of different rotations during inference isbeneficial (iii vs. v , iv vs. vi).
Table 4: AUC and F1 scores for COIL-100 dataset.
Table 5: Utilized datasets summaryA.2 Implementation DetailsFor all datasets, we resize the image to 128 × 128 and extract patches of size 32 × 32 where 16pixels overlap between patches. This results in a 7 × 7 grid of patches for a single image. Duringtraining, each patch is randomly cropped by 28 × 28 and zero-padded to recover the original size.
Table 6: Preliminary experiments on CIFAR-10.
