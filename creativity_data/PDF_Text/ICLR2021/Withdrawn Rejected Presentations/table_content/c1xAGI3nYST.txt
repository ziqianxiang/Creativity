Table 2: Generative performance on CIFAR-10Table 1: Generative performance on CelebA-64Model	FID；	Model	FID；NCP-VAE (ours)	5.25	NCP-VAE (ours)	24.08NVAE (Vahdat & Kautz, 2020)	13.48	NVAE (Vahdat & Kautz, 2020)	51.71RAE (Ghosh et al., 2020)	40.95	RAE (GhoSh et al., 2020)	74.162s-VAE (Dai & Wipf, 2018)	44.4	2s-VAE (Dai & WiPf, 2018)	72.9WAE (Tolstikhin et al., 2018)	35	Perceptial AE (Zhang et al., 2020)	51.51PerCePtial AE(Zhang et al., 2020)	13.8	EBM (DU & Mordatch, 2019)	40.58Latent EBM (Pang et al., 2020)	37.87	Latent EBM (Pang et al., 2020)	70.15COCO-GAN (Lin et al., 2019)	4.0	Style-GANv2 (KarraS et al., 2020)	3.26QA-GAN (Parimala & ChannaPPayya, 2019)	6.42		Denoising Diffusion Process (Ho et al., 2020)	3.17NVAE-Recon (Vahdat & Kautz, 2020)	1.03	NVAE-Recon (Vahdat & Kautz, 2020)	2.67Table 3: Generative results on CelebA-HQ-256		Table 4: Likelihood results on MNIST in nats	Model	FID；	Model	NLL；NCP-VAE (ours)	24.79	NCP-VAE (ours)	78.10NVAE (Vahdat & Kautz, 2020)	40.26	NVAE-small (Vahdat & Kautz, 2020)	78.67GLOW (Kingma & DhariWaL 2018)	68.93	BIVA (Maal0e etal., 2019)	78.41Advers. LAE (Pidhorskyi et al., 2020)-	19.21	DAVE++ (Vahdat et al., 2018b)	78.49PGGAN (Karras et al., 2017)	8.03	IAF-VAE (Kingma et al., 2016)	79.10
Table 1: Generative performance on CelebA-64Model	FID；	Model	FID；NCP-VAE (ours)	5.25	NCP-VAE (ours)	24.08NVAE (Vahdat & Kautz, 2020)	13.48	NVAE (Vahdat & Kautz, 2020)	51.71RAE (Ghosh et al., 2020)	40.95	RAE (GhoSh et al., 2020)	74.162s-VAE (Dai & Wipf, 2018)	44.4	2s-VAE (Dai & WiPf, 2018)	72.9WAE (Tolstikhin et al., 2018)	35	Perceptial AE (Zhang et al., 2020)	51.51PerCePtial AE(Zhang et al., 2020)	13.8	EBM (DU & Mordatch, 2019)	40.58Latent EBM (Pang et al., 2020)	37.87	Latent EBM (Pang et al., 2020)	70.15COCO-GAN (Lin et al., 2019)	4.0	Style-GANv2 (KarraS et al., 2020)	3.26QA-GAN (Parimala & ChannaPPayya, 2019)	6.42		Denoising Diffusion Process (Ho et al., 2020)	3.17NVAE-Recon (Vahdat & Kautz, 2020)	1.03	NVAE-Recon (Vahdat & Kautz, 2020)	2.67Table 3: Generative results on CelebA-HQ-256		Table 4: Likelihood results on MNIST in nats	Model	FID；	Model	NLL；NCP-VAE (ours)	24.79	NCP-VAE (ours)	78.10NVAE (Vahdat & Kautz, 2020)	40.26	NVAE-small (Vahdat & Kautz, 2020)	78.67GLOW (Kingma & DhariWaL 2018)	68.93	BIVA (Maal0e etal., 2019)	78.41Advers. LAE (Pidhorskyi et al., 2020)-	19.21	DAVE++ (Vahdat et al., 2018b)	78.49PGGAN (Karras et al., 2017)	8.03	IAF-VAE (Kingma et al., 2016)	79.10NVAE-Recon (Vahdat & Kautz, 2020)	0.45	VampPrior AR dec. (Tomczak & Welling)	78.45
Table 3: Generative results on CelebA-HQ-256		Table 4: Likelihood results on MNIST in nats	Model	FID；	Model	NLL；NCP-VAE (ours)	24.79	NCP-VAE (ours)	78.10NVAE (Vahdat & Kautz, 2020)	40.26	NVAE-small (Vahdat & Kautz, 2020)	78.67GLOW (Kingma & DhariWaL 2018)	68.93	BIVA (Maal0e etal., 2019)	78.41Advers. LAE (Pidhorskyi et al., 2020)-	19.21	DAVE++ (Vahdat et al., 2018b)	78.49PGGAN (Karras et al., 2017)	8.03	IAF-VAE (Kingma et al., 2016)	79.10NVAE-Recon (Vahdat & Kautz, 2020)	0.45	VampPrior AR dec. (Tomczak & Welling)	78.45improves the NVAE FID of 51.71 to 24.08. On MNIST, although our latent space is much smaller, ourmodel outperforms previous VAEs. NVAE has reported 78.01 nats on this dataset with a larger latent space.
Table 6: Effect of SIR sample size and LD iterations. Time-N is the time used to generate a batch of N images.
Table 5: # groups & gener-ative performance in FID]# groups	NVAE	NCP-VAE6	33.18	18.6815	14.96	5.9630	13.48	5.25to LD if we set the number of proposal samples in SIR equal to the number LD iterations. Tab. 6 reportsthe impact of these parameters. We observe that increasing both the number of proposal samples in SIRand the LD iterations leads to a noticeable improvement in FID score. For SIR, the proposal generation andthe evaluation of r(z) are parallelizable. Hence, as shown in Tab. 6, image generation is faster with SIRthan with LD (LD iterations are sequential). However, GPU memory usage scales with the number of SIRproposals, but not with the number of LD iterations. Interestingly, SIR, albeit simple, performs better thanLD when using about the same compute.
Table 7: Generative performance on CelebA-64with the RAE (Ghosh et al., 2020) architectureTable 8: Likelihood results on MNIST on sin-gle latent group model with architecture fromLARS (Bauer & Mnih, 2019) & SNIS (LawsonModel	FID；	et al., 2019) (results in nats)	VAE w/ Gaussian prior	48.12	Model	NTJ T I2s-VAE (Dai & Wipf, 2018)	49.70		NLL；WAE (Tolstikhin et al., 2018)	42.73	VAE w/ Gaussian prior	84.82RAE (Ghosh et al., 2020)	40.95	VAE w/ LARS prior (Bauer & Mnih, 2019)	83.03NCP w/ Gaussian prior as base	41.28	VAE w/ SNIS prior (Lawson et al., 2019)	82.52NCP w/ GMM prior as base	39.00	NCP-VAE	82.826	ConclusionsThe prior hole problem is one of the main reasons for VAEs’ poor generative quality. In this paper, we tackledthis problem by introducing the noise contrastive prior (NCP), defined by the product of a reweighting factorand a base prior. We showed how the reweighting factor is trained by contrasting samples from the aggregateposterior with samples from the base prior. Our proposal is simple and can be applied to any VAE to increaseits prior’s expressivity. We also showed how NCP training scales to large hierarchical VAEs, as it can be donein parallel simultaneously for all the groups. Finally, we demonstrated that NCPs improve the generativeperformance of state-of-the-art NVAEs by a large margin, closing the gap to GANs.
Table 8: Likelihood results on MNIST on sin-gle latent group model with architecture fromLARS (Bauer & Mnih, 2019) & SNIS (LawsonModel	FID；	et al., 2019) (results in nats)	VAE w/ Gaussian prior	48.12	Model	NTJ T I2s-VAE (Dai & Wipf, 2018)	49.70		NLL；WAE (Tolstikhin et al., 2018)	42.73	VAE w/ Gaussian prior	84.82RAE (Ghosh et al., 2020)	40.95	VAE w/ LARS prior (Bauer & Mnih, 2019)	83.03NCP w/ Gaussian prior as base	41.28	VAE w/ SNIS prior (Lawson et al., 2019)	82.52NCP w/ GMM prior as base	39.00	NCP-VAE	82.826	ConclusionsThe prior hole problem is one of the main reasons for VAEs’ poor generative quality. In this paper, we tackledthis problem by introducing the noise contrastive prior (NCP), defined by the product of a reweighting factorand a base prior. We showed how the reweighting factor is trained by contrasting samples from the aggregateposterior with samples from the base prior. Our proposal is simple and can be applied to any VAE to increaseits prior’s expressivity. We also showed how NCP training scales to large hierarchical VAEs, as it can be donein parallel simultaneously for all the groups. Finally, we demonstrated that NCPs improve the generativeperformance of state-of-the-art NVAEs by a large margin, closing the gap to GANs.
Table 9: Hyper-parameters for training the binary classifiers.
