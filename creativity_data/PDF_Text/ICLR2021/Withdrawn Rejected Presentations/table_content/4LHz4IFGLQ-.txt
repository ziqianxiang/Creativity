Table 1: An example 2-dimensional embedding.
Table 2: Embedding evaluation task performance comparison between CBOW and DSAW with thebest tuned hyperparameters. In all tasks, higher scores are better. Best results in bold.
Table 3: Paraphrasing of the source words returned by the LAMA planner. See Table 18-19 in theappendix Sec. C.6 for more examples.
Table 4: Embedding evaluation task performance with updated implementations. In all tasks, higherscores are better. Best results are in bold.
Table 5: Results with old implementations for reference.
Table 6: Per-category accuracies for the text classification task. (new results)Data (Num. documents)	Sentence len.		200		500		1000		avg.	med.	CBOW	DSAW	CBOW	DSAW	CBOW	DSAWSCI (1994)	276	229	.976	.952	.983	.988	.990	.995COMP (1981)	341	255	.809	.938	.892	.980	.931	.984SPORT (1987)	383	269	.902	.941	.952	.985	.971	.993RELI (1995)	447	324	.996	.976	.999	.994	.999	.995MS (9142)	17	17	.770	.629	.773	.666	.741	.704Total (17099)			.890	.867	.920	.908	.920	.930Table 7: Per-category accuracies for the text classification task. (old results)19Under review as a conference paper at ICLR 2021B.6 Rebuttal Experiments: Detailed Results on Word Similarity withAverage-Based CBOWWe re-evaluated CBOW model after fixing the word vector aggregation from summation to averaging.
Table 7: Per-category accuracies for the text classification task. (old results)19Under review as a conference paper at ICLR 2021B.6 Rebuttal Experiments: Detailed Results on Word Similarity withAverage-Based CBOWWe re-evaluated CBOW model after fixing the word vector aggregation from summation to averaging.
Table 8: Word similarity results compared by the datasets (new results).
Table 9: Word similarity results compared by the datasets (old results).
Table 10: Downstream task performance of DSAW models using Logistic vs. Gaussian weightinitialization. The better initialization under the same hyperparameter set is highlighted in bold.
Table 11: Downstream task performance of for SG and SG-BTL.
Table 12: Schematic diagram of SG-BTL.
Table 13: Overall performance of the Hybrid models.
Table 14: Word similarity results compared by the datasets (CBOW and DSAW).
Table 15: Word Analogy accuracies compared by each category. Data from the best performingCBOW and DSAW models with the embedding size 1000.
Table 16: Word Analogy accuracies compared by each category, comparing the effect of the differ-ent ordering of +, - operations in DSAW. Data from the best performing DSAW model with theembedding size 1000.
Table 17: Per-category accuracies for the text classification task. We observed that the continu-ous embeddings performs well in MS, which has shorter sentences, and relatively worse in longersentences, except RELI.
Table 18: (Part 1) Paraphrasing of the source words returned by the LAMA planner . 300 sourcewords are randomly selected from the 4000th to 8000th most frequent words. Note*: Louisville,Cleveland and Pittsburgh are the central city of Kentucky, Ohio and Pensilvania, respectively.
Table 19: (Part 2) Paraphrasing of the source words returned by the LAMA planner . 300 sourcewords are randomly selected from the 4000th to 8000th most frequent words. Note*: Puerto Ricoand Taiwan are both islands; Puerto Rico and Argentina both speak spanish.
Table 20: Comparison of different zero-shot definition construction mechanism. Note*: Puerto Ricoand Taiwan are both islands; Puerto Rico and Argentina both speak spanish.
