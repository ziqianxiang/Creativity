Table 1: Mean/patch-wise/block-wise/pixel-wise AUSE and correlation between L1 loss and un-certainty, and NLL on SR benchmark dataset Set 14. Our infer-transformation, infer-dropout andinfer-noise are compared with MC-dropout (Gal & Ghahramani, 2016), deep ensemble (Lakshmi-narayanan et al., 2017) and log likelihood maximization (LLM) (Zhang et al., 2019). MC-drop1 usesthe output of the original model as prediction while MC-drop2 uses the mean of output samples fromthe re-trained model (with added dropout) as prediction. Models evaluated: SRGAN and SRresnet.
Table 2: Mean/patch-wise/block-wise/pixel-wise AUSE and correlation between L1 loss and uncer-tainty, and NLL on NYU Depth Dataset V2. Our infer-transformation, infer-dropout and infer-noiseare compared with MC-dropout (Gal & Ghahramani, 2016). MC-drop1 uses the output of the originalmodel as prediction while MC-drop2 uses the mean of output samples from the re-trained model(with added dropout) as prediction. Models evaluated: FCRN model.
Table 3: Comparison of our methods and baselines as well as performance bound (PB). We mark thebest performance bound in bold face and the best method by underlining. Model evaluated: SRGAN.
Table 4: The second application of active learning on our generated uncertainty maps to improve theperformance with more efficiency. Here we select samples with high uncertainty yields better resultsthan select randomly. Results evaluated in Set 14 on SSIM, PSNR and L1 loss.
Table 5: Mean/patch-wise/block-wise/pixel-wise AUSE and correlation between L2 loss and uncer-tainty on SR benchmark dataset Set 14. Our infer-transformation, infer-dropout and infer-noise arecompared with MC-dropout (Gal & Ghahramani, 2016) and deep ensemble (Lakshminarayanan et al.,2017). MC-drop1 uses the output of the original model as prediction while MC-drop2 uses the meanof output samples from the re-trained model (with added dropout) as prediction. Models evaluated:SRGAN trained with L2 loss and adversarial loss and SRresnet trained with L2 loss.
Table 6: Mean/patch-wise/block-wise/pixel-wise AUSE and correlation between L2 loss and un-certainty on NYU Depth Dataset V2. Our infer-transformation, infer-dropout and infer-noise arecompared with MC-dropout (Gal & Ghahramani, 2016). MC-drop1 uses the output of the originalmodel as prediction while MC-drop2 uses the mean of output samples from the re-trained model(with added dropout) as prediction. Models evaluated: FCRN model for depth estimation.
Table 7: Comparison of our methods and baselines as well as performance bound (PB). We mark thebest performance bound in bold face and the best method by underlining. Model evaluated: SRGAN.
Table 8: Correlation of uncertainty and cross-entropy loss, comparing using entropy with the baselineMC-dropout, models evaluated are Densenet for the segmentation on CamVid dataset, UNET forsegmentation on SNEMI3D dataset and Resnet for classification on CIFAR100 dataset.
