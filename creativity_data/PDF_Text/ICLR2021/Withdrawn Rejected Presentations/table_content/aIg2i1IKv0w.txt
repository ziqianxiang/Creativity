Table 1: Results from the Background Challenge on ImageNet-9 using ResNet-50. Our method out-performs the relevant baselines across all three datasets. The difference between Mixed-same andMixed-rand is referred to as the background gap (BG-gap), which indicates average robustnessto varying backgrounds from different image sources.
Table 2: Multi-source domain generalization results (%) on the VLCS dataset with ResNet-18 as thebase network. All reported numbers are averaged over three runs.
Table 3: Average and worst-group accuracies for CelebA and Waterbird benchmark datasets. Meth-ods without group-level supervision (i.e. with âœ“) are preferable. * refers to methods with Inception-ResNetV2 backbone instead of ResNet-50. CIM outperforms both supervised and unsupervisedmethods on the CelebA dataset as well as unsupervised methods on the Waterbirds dataset. It alsoachieves favorable performance relative to supervised methods on Waterbirds.
Table 4: Hyper-parameters for our CIM and CIM* methods.
Table 5: Results from the Background Challenge on ImageNet-9 using ResNet-50. The differencebetween Mixed-same and Mixed-rand is referred to as the background gap (BG-gap), whichindicates average robustness to varying backgrounds from different image sources.
Table 6: Modelnet40 (Wu et al., 2015) point cloud classification results.
Table 7: Ablation study of CIM on the CelebA dataset. Rand x- corresponds to the commoncontrastive learning strategy i.e. using random negative samples and augmented version of the inputas positives. Only m refers to applying m on the input without any contrastive regularization. Onlyx+ and Only x- refer to leveraging only positive or negative terms in our contrastive Gramian loss,respectively. The highlighted column shows the worst group accuracy.
