Table 1: Results across all datasets and baselines under optimal hyperparameter tuning (settingsshown). Note that we report the standard deviation of the runs instead of standard deviation of themean (i.e. standard error) which is often reported instead. The former is higher than the latter by afactor of the square root of the number of trials (10).
Table 2: Ensemble results for all datasets. In all settings, the optimal m (number of subnetworks)is 5. We see that compared to the other methods presented, ensembling does well in both predictiveperformance and in reducing churn. It does come at a cost, however: the model is effectively 5 timeslarger, making both training and inference more expensive.
Table 3: Ablation on k-NN label smoothingâ€™s hyperparameters: a, b, and k for the SVHN dataset.
