Table 1: Mean average error of different approaches on Boston Housing price dataset	ReLU	Sigmoid	LeakyReLUVanilla	2.97	-^3.16	2.85Decov	2.77	2.99	2.80Ours (direct)	2.72	2.97	2.82Ours (det)	2.68	2.87	2.83Ours (logdet)	2.64	2.83	2.77Table 1 reports the results in terms of the mean average error for the different approaches over theBoston Housing price dataset. First, we note that employing a diversification strategy (ours andDecov) boosts the results compared to the Vanilla approach for all types of activations. The threevariants of our approach, i.e., the within-layer approach, consistently outperform the Decov lossexcept for the LeakyReLU where the latter outperforms our direct variant. Table 1 shows that thelogdet variant of our approach yields the best performance for all three activation types.
Table 2: Test error rates on CIFAR10	ReLU	Sigmoid	LeakyReLUVanilla	32.04 ± 0.57	33.78 ± 0.64	30.99 ± 0.27Decov	30.98 ± 0.25	32.22 ± 0.51	30.70 ± 0.35Ours (direct)	31.28 ± 0.49	31.69 ± 0.51	30.86 ± 0.75Ours (det)	31.28 ± 0.60	32.92 ± 0.49	30.93 ± 0.44Ours (logdet)	31.26 ± 0.41	32.61 ± 0.46	30.70 ± 0.256	ConclusionsIn this paper, we proposed a new approach to encourage ‘diversification’ of the layer-wise featuremap outputs in neural networks. The main motivation is that by promoting within-layer activationdiversity, neurons within the same layer learn to capture mutually distinct patterns. We proposed anadditional loss term that can be added on top of any fully-connected layer. This term complements8Under review as a conference paper at ICLR 2021Table 3: Test error rates on CIFAR100	ReLU	Sigmoid	LeakyReLUVanilla	65.81 ± 0.42	78.52 ± 0.40	64.90 ± 0.22Decov	65.26 ± 0.21	77.08 ± 0.47	64.57 ± 0.23Ours (direct)	64.95 ± 0.32	76.91 ± 0.91	64.85 ± 0.23Ours (det)	64.90 ± 0.40	77.79 ± 0.29	64.46 ± 0.40
Table 3: Test error rates on CIFAR100	ReLU	Sigmoid	LeakyReLUVanilla	65.81 ± 0.42	78.52 ± 0.40	64.90 ± 0.22Decov	65.26 ± 0.21	77.08 ± 0.47	64.57 ± 0.23Ours (direct)	64.95 ± 0.32	76.91 ± 0.91	64.85 ± 0.23Ours (det)	64.90 ± 0.40	77.79 ± 0.29	64.46 ± 0.40Ours (logdet)	64.95 ± 0.17	77.70 ± 0.61	64.49 ± 0.19the traditional ‘between-layer’ feedback with an additional ‘within-layer’ feedback encouraging di-versity of the activations. We theoretically proved that the proposed approach decreases the esti-mation error bound, and thus improves the generalization ability of neural networks. This analysiswas further supported by experimental results showing that such a strategy can indeed improve theperformance of neural networks in regression and classification tasks. Our future work includesextensive experimental analysis on the relationship between the distribution of the neurons outputand generalization.
