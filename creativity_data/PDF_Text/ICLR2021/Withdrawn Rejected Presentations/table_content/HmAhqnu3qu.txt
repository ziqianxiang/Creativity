Table 1: Results for a single-task model trained in a classical supervised manner (Cl), and a linearclassifier trained on the embeddings produced by our meta-learning strategies (iSAME, eSAME).
Table 2: Results fora single-task model trained in a classical supervised manner, a fine-tuned model (trained on all three tasks, and fine-tuned on the two shown tasks), and a linear classifier trained on node embeddings learned with our proposed strategies (iSAME, eSAME) in a multi-task setting.													Task					Dataset						GC	NC	LP	ENZYMES			PROTEINS	DHFR				COX2				GC	NC	LP	GC NC LP	GC	NC	LP	GC	NC	LPClassical End-to-End Training												X			51.6			73.3	71.5			76.7			X			87.5		72.3		97.3			96.4			X			75.5	85.6			98.8			98.3Fine-Tuning												X	X		48.3	85.3		73.6 72.0	66.4	92.4		80.0	92.3	X		X	49.3		71.6	69.6	80.7	65.3		58.9	80.2		50.9	X	X		87.7	73.9	80.4 81.5		80.7	56.6		87.4	52.3iSAME (ours)												X	X		50.1	86.1		73.1 76.6	71.6	94.8		75.2	95.4	X		X	50.7		83.1	73.4	85.2	71.6		99.2	77.5		98.9	X	X		86.3	83.4	79.4 87.7		96.5	99.3		95.5	99.0X	X	X	50.0	86.5	82.3	71.4 76.6 87.3	71.2	95.5	99.5	75.4	95.2	99.2	eSAME (ours)													X	X		51.7	86.1		71.5 79.2	70.1	95.7		75.6	95.5	X		X	51.9		80.1	71.7	85.4	70.1		99.1	77.5		98.8
Table 3: ∆m (%) results for a classical multi-task model (Cl), a fine-tuned model (FT; trained onall three tasks and fine-tuned on two) and a linear classifier trained on the node embeddings learnedusing our meta-learning strategies (iSAME, eSAME) in a multi-task setting.
Table 4: Results of a neural network trained on the embeddings generated by a multi-task model, toperform a task that was not seen during training by the multi-task model. “x,y ->z” indicates thatthe multi-task model was trained on tasks x and y, and the neural network is performing task z.
