Table 1: Features of related datasets that: 1) test com-positional generalization and reasoning, and 2) are pro-cedurally gnerated. We compare the datasets along thefollowing axis: Inspectable Rules (IR), Diversity, Com-positional Generalization (CG), Modality and if the fol-lowing training setups are supported: Supervised, Meta-learning, Multitask & Continual learning (CL).
Table 2: Aggregate statistics of theworlds in GraphLog. Statistics for eachindividual world are in the Appendix.
Table 3: Multitask evaluation performance whentrained on different data distributions (catego-rized based on their similarity of rules: Similar(S) containing similar worlds and a mix of simi-lar and dissimilar worlds (D))Easy World Medium World Hard WorldWorld difficultyGAT-E-GATGCN-E-GATParam-E-GATGAT-RGCNGCN-RGCNParam-RGCNFigure 3: We categorize the datasets in terms oftheir relative difficulty (see Appendix). We ob-serve that the models using E-GAT as the com-position function consistently work well.
Table 4: Inductive performance on data splits marked by difficultyIn Section A.4 we introduced the notion of difficulty among the tasks available in GraphLog . Here,we consider a set of experiments where we perform multitask training and inductive testing onthe worlds bucketized by their relative difficulty (Table 4). We sample equal number of worldsfrom each difficulty bucket, and separately perform multitask training and testing. We evaluate theaverage prediction accuracy on the datasets within each bucket. We observe that the average multitaskperformance also mimics the relative task difficulty distribution. We find GAT-E-GAT modeloutperforms other baselines in Easy and Medium setup, but is outperformed by GAT-RGCN modelin the Difficult setup. For each model, we used the same architecture and hyperparameter settingsacross the buckets. Optimizing individually for each bucket may improve the relative performance.
Table 5: Convergence performance on 3 held out datasets when pre-trained on easy, medium andhard training datasetsC.2 Multitask Pre-training by task similarityIn the main paper (Section 5.2) we introduce the setup of performing multitask pre-training onGraphLog datasets and adaptation on the datasets based on relative similarity. Here, we performfine-grained analysis of few-shot adapatation capabilities of the models. We analyze the adaptationperformance in two settings - when the adaptation dataset has complete overlap of rules with thetraining datasets (group=1.0) and when the adaptation dataset has zero overlap with the trainingdatasets (group=0.0). We find RGCN family of models with a graph based representation functionhas faster adaptation on the dissimilar dataset, with GCN-RGCN showing the fastest improvement.
Table 6: Results on Single-task supervised setup for all datasets in GraphLog. Abbreviations: NC:Number of Classes, ND: Number of Descriptors, ARL: Average Resolution Length, AN: Averagenumber of nodes, AE: Average number of edges, D: Difficulty, AGG: Aggregate Statistics. List of models considered : M1: GAT-EGAT, M2:GCN-E-GAT, M3: Param-E-GAT, M4: GAT-RGCN, M5: GCN-RGCN and M6: Param-RGCN.
