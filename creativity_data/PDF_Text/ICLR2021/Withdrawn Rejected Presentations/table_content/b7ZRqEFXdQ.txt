Table 1: Summary of experimental datasets.
Table 2: The NLLoracle performance of different models (τ = 1) on the synthetic dataset with thesequence length of 20 and 40 respectively. For NLL, the lower, the better.
Table 3: BLEU and NLLgen performance on MS COCO image captions with τ = 0.1 for theproposed models with MSA and MDA. For BLEU scores, the higher, the better.
Table 4: The BLEU and NLLgen performance on EMNLP2017 WMT News dataset with tempera-tures of 1 for the proposed models with MSA and MDA.
Table 5: Mean and standard deviation results of human evaluation w.r.t different models on MSCOCO Image Caption dataset. Note that “Real” indicates the real data samples.
Table 6: The NLLgen performance of different models (τ = 1) on the synthetic dataset with thesequence length of 20 and 40 respectively. For the NLL score, the lower, the better.
Table 7: The human evaluation scale from 1 to 5 with corresponding criteria and example sentences.
Table 8: Samples of baseline models and real dataset on MS COCO Image Captioning dataset.
Table 9:	Randomly sampled 10 samples trained on MS COCO dataset, with MDA (top row) andMSA (bottom row). We can observe that the phrases “a man” and “motorcycle” occur too manytimes in the samples generated by MSA-enhanced models. Thus, models with MSA lack the diver-sity of text whereas those with MDA perform well on both the diversity and quality.
Table 10:	Randomly sampled 10 samples trained on EMNLP2017 WMT News dataset, with MDA(top row) and MSA (bottom row).
