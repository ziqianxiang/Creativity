Table 1: Comparing log likelihood (standard error) of test data for independent GPs (GP) vs. MOGPwith n latent functions (n-MOGP) on collected saliency measurements from CIFAR-100 training.
Table 2: Performance (standard error) against percentage of neurons/filters pruned per layer withvarying λ for tested algorithms.
Table 3: Ablation study showing performance (standard error) vs. varying early pruning hyperpa-rameters: MOGP variational inducing points (Ind. pnts.), MOGP latent functions (Lat. func.), Tstep.
Table 4: BEP vs. SNIP and Grasp on ResNet-50 on ImageNet dataset. We vary percentage of residualunit sequence channels (Seq) and filters pruned (Lyr). ‘Train’ refers to wall time during networktraining, ‘Prune’ refers to pruning modeling/inference overhead. Benchmark wall time for ResNet-50is 55h on our hardware. Benchmark performance is 75.7% for unpruned ResNet-50.
Table 5: Comparing BEP with PruneTrain on ResNet-50 on ImageNet dataset. PruneTrain uses astronger ResNet-50 baseline with Top-1 76.2% performance. BEP uses a 75.7% baseline. ‘Train’refers to wall time during network training, ‘Prune’ refers to pruning modeling/inference overhead.
Table 6: Notations used elsewhere in the paper.
Table 7: Comparing log likelihood (standard error) of test data for Independent GPs (GP) vs. MOGPwith n latent functions (n-MOGP) on collected saliency measurements from CIFAR-10 training.
