Table 1: Here we compare AWAC (averaged over 4 seeds) and AWAC + MB2PO (averaged over4 seeds) to recent offline model-free and model-based RL algorithms. We report the normalizedscore where 100 is the performance of a fully trained SAC policy and 0 is the performance of auniform random policy. For the other methods, we report the results from their own papers or theoriginal D4RL paper. ”-expert” results for MOPO were not included in the original paper and thusare omitted here. We include the stand deviation for our results and for previous results if reported.
