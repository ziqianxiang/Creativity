Table 1: encouraging layer sparsity can reduce the prediction error and the model complexity3.1.1	S imulation FrameworkWe generate data according to the model in (2). The most outside activation function f1 is the identityfunction, and the coordinates of all other activation functions f2 , . . . , fl are ReLU functions. Theinput vectors x1, . . . , xn are jointly independent and standard normally distributed in d dimensions;the noise random variables u1, . . . , un are independent of the input, jointly independent, and standardnormally distributed in one dimension. For a given sparsity level sW ∈ [0, 1], a vector s ∈{0, 1}l-1 with independent Bernoulli distributed entries that have success parameter sW is generated.
Table 2: layer sparsity can reduce model complexity without sacrificing classification accuracyachieve the same accuracies, but the refitted version can be trained much faster. These observations arecommensurate with the observations in (Frankle & Carbin, 2019), who study refitting of connection-sparse networks.
