Table 1: Neighbourhood Distillation results on CIFAR-10 for different values of multiplier k. OurNeighbourhood Distillation method reaches the same accuracy as Knowledge Distillation (KD) andperforms better than retraining from scratch (CE). As k decreases, fine-tuning becomes critical inorder to recover good accuracy. Our method is 2.3× faster than Knowledge Distillation on GPU.
Table 2: Distillation results on ResNet-50 for neighbourhoods of 2 and 3 layers with different mul-tipliers. Neighbourhood Distillation (ND) performs similarly to Knowledge Distillation (KD) andbetter than retraining from scratch (CE). As the gap between the student and the teacher increases,fine-tuning becomes necessary to recover good model accuracy. Our method is 3.6× faster.
Table 3: Classification accuraciesand GPU Time comparison be-tween Neighbourhood Distillationand Knowledge Distillation.
Table 4: Classification accuracies for different data-free dis-tillation methods. Different values of k are used for the stu-dent architecture including k = 1 (self-distillation). Wecompare accuracies obtained after different training meth-ods: fully-supervised cross-entropy loss (CE), GaussianNoise Neighbourhood Distillation (G-ND), Gaussian NoiseNeighbourhood Distillation with Gaussian Noise Finetuning(G-ND+FT), Gaussian Noise Knowledge Distillation (GN-KD) and Zero-Shot Knowledge Distillation (Nayak et al.,2019) (ZSKD). Gaussian Noise Neighbourhood Distillationsurpasses all other data-free distillation methods we com-pared to.
Table 5: Impact of regularizing the teacher model on Neighbourhood Distillation. Accuracies areobtained before fine-tuning. Using a regularized teacher improves the accuracy of the student beforefine-tuning.
Table 6: Ablation Study. Impact of fine-tuning (FT) and look-ahead (2-LA) on final test accuracyfor ResNet-20. While fine-tuned model always perform better than pre-finetuned models, the use oflook-ahead tightens the gap between pre and post finetuned accuracy.
Table 7: Comparison of training time in GPU hours for ResNet models.
