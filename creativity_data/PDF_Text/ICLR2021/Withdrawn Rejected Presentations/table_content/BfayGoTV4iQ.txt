Table 1: Few-shot classification results on OmniglotOmniglot	(way, shot)Algorithm	Encoder	Train Data	(5,1)	(5,5)	(20,1)	(20,5)Training from Scratch (Hsu et al., 2019)	N/A	Omniglot	52.50 ± 0.84	74.78 ± 0.69	24.91 ± 0.33	47.62 ± 0.44CACTUs-MAML (Hsu et al., 2019)	Conv4	Omniglot	68.84 ± 0.80	87.78 ± 0.50	48.09 ± 0.41	73.36 ± 0.34CACTUs-ProtoNet (Hsu et al., 2019)	Conv4	Omniglot	68.12 ± 0.84	83.58 ± 0.61	47.75 ± 0.43	66.27 ± 0.37AAL-ProtoNet (Antoniou & Storkey, 2019)	Conv4	Omniglot	84.66 ± 0.70	88.41 ± 0.27	68.79 ± 1.03	74.05 ± 0.46AAL-MAML (Antoniou & Storkey, 2019)	Conv4	Omniglot	88.40 ± 0.75	98.00 ± 0.32	70.20 ± 0.86	88.30 ± 1.22UMTRA (Khodadadeh et al., 2019)	Conv4	Omniglot	83.80	95.43	74.25	92.12Random CNN	Conv4	N/A	67.96 ± 0.44	83.85 ± 0.31	44.39 ± 0.23	60.87 ± 0.22Conv-VAE	Conv4	Omniglot	77.83 ± 0.41	92.91 ± 0.19	62.59 ± 0.24	84.01 ± 0.15Conv-VAE	Conv4	Quickdraw	81.49 ± 0.39	94.09 ± 0.17	66.24 ± 0.23	86.02 ± 0.14SketchEmbedding (Ours)	Conv4	Omniglot*	94.88 ± 0.22	99.01 ± 0.08	86.18 ± 0.18	96.69 ± 0.07SketchEmbedding (Ours)	Conv4	Quickdraw*	96.96 ± 0.17	99.50 ± 0.06	91.67 ± 0.14	98.30 ± 0.05MAML (Supervised) (Finn et al., 2017)	Conv4	Omniglot	94.46 ± 0.35	98.83 ± 0.12	84.60 ± 0.32	96.29 ± 0.13ProtoNet (Supervised) (Snell et al., 2017)	Conv4	Omniglot	98.35 ± 0.22	99.58 ± 0.09	95.31 ± 0.18	98.81 ± 0.07Stroke data used for trainingQuickdraw: Stroke-based image sketching The Quickdraw (Jongejan et al., 2016) dataset con-sists of 345 classes of each with 70,000 examples, produced by human players participating in thegame “Quick, Draw!”. Examples from the Quickdraw dataset are shown in Figure 2b. The input
Table 2: Few-shot classification results on mini-ImageNetmini-ImageNet(way, shot)Algorithm	Backbone	Train Data	(5,1)	(5,5)	(5,20)	(5,50)Training from Scratch (Hsu et al., 2019)	N/A	mini-ImageNet	27.59 ± 0.59	38.48 ± 0.66	51.53 ± 0.72	59.63 ± 0.74CACTUs-MAML (Hsu et al., 2019)	Conv4	mini-ImageNet	39.90 ± 0.74	53.97 ± 0.70	63.84 ± 0.70	69.64 ± 0.63CACTUs-ProtoNet (Hsu et al., 2019)	Conv4	mini-ImageNet	39.18 ± 0.71	53.36 ± 0.70	61.54 ± 0.68	63.55 ± 0.64AAL-ProtoNet (Antoniou & Storkey, 2019)	Conv4	mini-ImageNet	37.67 ± 0.39	40.29 ± 0.68	-	-AAL-MAML (Antoniou & Storkey, 2019)	Conv4	mini-ImageNet	34.57 ± 0.74	49.18 ± 0.47	-	-UMTRA (Khodadadeh et al., 2019)	Conv4	mini-ImageNet	39.93	50.73	61.11	67.15Random CNN	Conv4	N/A	26.85 ± 0.31	33.37 ± 0.32	38.51 ± 0.28	41.41 ± 0.28Conv-VAE	Conv4	mini-ImageNet	23.30 ± 0.21	26.22 ± 0.20	29.93 ± 0.21	32.57 ± 0.20Conv-VAE	Conv4	Sketchy	23.27 ± 0.18	26.28 ± 0.19	30.41 ± 0.19	33.97 ± 0.19Random CNN	ResNet12	N/A	28.59 ± 0.34	35.91 ± 0.34	41.31 ± 0.33	44.07 ± 0.31Conv-VAE	ResNet12	mini-ImageNet	23.82 ± 0.23	28.16 ± 0.25	33.64 ± 0.27	37.81 ± 0.27Conv-VAE	ResNet12	Sketchy	24.61 ± 0.23	28.85 ± 0.23	35.72 ± 0.27	40.44 ± 0.28SketchEmbeddingt (ours)	Conv4	Sketchy*	38.61 ± 0.42	53.82 ± 0.41	63.34 ± 0.35	67.22 ± 0.32SketchEmbedding (ours)	ResNet12	Sketchy*	40.39 ± 0.44	57.15 ± 0.38	67.60 ± 0.33	71.99 ± 0.3MAML (supervised) (Finn et al., 2017)	Conv4	mini-ImageNet	46.81 ± 0.77	62.13 ± 0.72	71.03 ± 0.69	75.54 ± 0.62ProtoNet (supervised) (Snell et al., 2017)	Conv4	mini-ImageNet	46.56 ± 0.76	62.29 ± 0.71	70.05 ± 0.65	72.04 ± 0.60
Table 3: Effect of pixel loss coefficient α on Omniglot few-shot classification		Table 4: ResNet-101 45-way classification score on 1-shot generated sketches of seen and unseen classes.		αmax	20-way 1-shot Acc.	Generation Method	Seen	Unseen0.00	87.17 ± 0.36	Original Data	97.66	96.090.25	87.82 ± 0.36	Conv-VAE	76.28 ± 0.93	75.07 ± 0.840.50	91.39 ± 0.31	SketchEmbedNet	81.44 ± 0.95	77.94 ± 1.070.75	90.59 ± 0.32		—	0.95	89.77 ± 0.32			approaches (Figure 6). In the settings shown, none of the models have seen any examples fromthe character class, or the parent alphabet. Furthermore, the drawer has seen no written charactersduring training and is trained only on the Quickdraw dataset. Visually, our generated images betterresemble the support examples and the variations by stretching and shifting strokes better preservesthe semantics of each character. Generations in pixel space may disrupt strokes and alter the characterto human perception. This is especially true for written characters as they are frequently defined by aspecific set of strokes instead of blurry clusters of pixels.
Table 5: Few-shot classification accuracy of all αmax valuesαmax	25k	50k	75k	100k	125k	150k	175k	200k	225k	250k	275k	300k0.00	89.35	87.94	88.73	88.46	88.01	88.04	88.23	87.73	88.03	87.86	87.65	87.170.25	89.21	90.39	90.20	89.75	87.78	88.37	88.64	88.05	87.98	88.41	88.15	87.820.50	90.48	89.58	89.81	89.02	90.68	91.24	90.26	90.94	91.12	91.30	91.12	91.390.75	91.39	89.95	89.56	89.81	89.95	90.79	91.02	91.09	91.82	90.76	91.42	90.590.95	90.23	90.15	90.10	89.55	90.27	92.37	92.27	90.29	91.58	91.02	89.73	89.77E Intra-alphabet Lake SplitThe creators of the Omniglot dataset and one-shot classification benchmark originally proposed anintra-alphabet classification task. This task is more challenging than the common Vinyals split ascharacters from the same alphabet may exhibit similar stylistics of sub-components that makes visualdifferentiation more difficult. This benchmark has been less explored by researchers; however, westill present the performance of our SketchEmbedding model against evaluations of other few-shotclassification models on the benchmark. Results are shown in Table 6.
Table 6: Few-shot classification results on Omniglot (Lake split)Omniglot (Lake split)			(way, shot)			Algorithm	Backbone	Train Data	(5,1)	(5,5)	(20,1)	(20,5)Conv-VAE	Conv4	Quickdraw	73.12 ± 0.58	88.50 ± 0.39	53.45 ± 0.51	73.62 ± 0.48SketchEmbedding (Ours)	Conv4	Quickdraw	89.16 ± 0.41	97.12 ± 0.18	74.24 ± 0.48	89.87 ± 0.25SketchEmbedding (Ours)	ResNet12	Quickdraw	91.03 ± 0.37	97.91 ± 0.15	77.94 ± 0.44	92.49 ± 0.21BPL (Supervised) (Lake et al., 2015; 2019)	N/A	Omniglot	-	-	96.70	-ProtoNet (Supervised) (Snell et al., 2017; Lake et al., 2019)	Conv4	Omniglot	-	-	86.30	-RCN (Supervised) (George et al., 2017; Lake et al., 2019)	N/A	Omniglot	-	-	92.70	-VHE (Supervised) (Hewitt et al., 2018; Lake et al., 2019)	N/A	Omniglot	-	-	81.30	-16Under review as a conference paper at ICLR 2021Unsurprisingly, our model is outperformed by supervised models and does fall behind by a moresubstantial margin than in the Vinyals split. However, our SketchEmbedding approach still achievesrespectable classification accuracy overall and greatly outperforms a Conv-VAE baseline.
Table 7: Random Seeding on Few-Shot Classification results on Omniglot (Conv4)(way, shot)Seed	(5,1)	(5,5)	(20,1)	(20,5)1	96.45	99.41	90.84	98.082	96.54	99.48	90.82	98.103	96.23	99.40	90.05	97.944	96.15	99.46	90.50	97.995	96.21	99.40	90.54	98.106	96.08	99.43	90.20	97.937	96.19	99.39	90.70	98.058	96.68	99.44	91.11	98.189	96.49	99.42	90.64	98.0610	96.37	99.47	90.50	97.9911	96.52	99.40	91.13	98.1812	96.96	99.50	91.67	98.3013	96.31	99.38	90.57	98.0414	96.12	99.45	90.54	98.0315	96.30	99.48	90.62	98.05Average	96.37 ± 0.12	99.43 ± 0.02	90.69 ± 0.20	98.07 ± 0.05Table 8: Random Seeding on Few-Shot Classification results on Omniglot (ResNet12)
Table 8: Random Seeding on Few-Shot Classification results on Omniglot (ResNet12)(way, shot)				Seed	(5,1)	(5,5)	(20,1)	(20,5)1	96.61	99.58	91.25	98.582	96.37	99.52	90.44	98.403	96.04	99.58	89.86	98.274	96.44	99.50	90.76	98.405	95.95	99.52	89.88	98.296	95.63	99.45	89.28	98.177	96.24	99.52	89.90	98.348	95.41	99.45	88.75	98.059	96.04	99.49	89.70	98.2410	95.40	99.41	88.91	98.0511	95.82	99.51	89.67	98.2412	96.25	99.51	90.21	98.2813	95.84	99.53	89.71	98.1814	96.04	99.56	89.89	98.3115	96.04	99.57	89.97	98.32Average	96.00 ± 0.31	99.51 ± 0.04	89.89 ± 0.56	98.27 ± 0.1217
Table 9: Random Seeding on Few-Shot Classification results on mini-ImageNet(way, shot)				Seed	(5,1)	(5,5)	(5,20)	(5,50)1	37.15	52.99	63.92	68.722	39.38	55.20	65.60	69.793	39.40	55.47	65.94	70.414	40.39	57.15	67.60	71.995	38.40	54.08	65.36	70.086	37.94	53.98	65.24	69.657	38.88	55.71	66.59	71.358	37.89	52.65	63.42	68.149	38.25	53.86	65.02	69.8210	39.11	55.29	65.99	69.9811	37.39	52.88	63.66	68.3312	38.24	53.91	65.19	69.8213	38.62	53.84	63.83	68.6914	37.73	53.61	64.22	68.4115	39.50	55.23	65.51	70.25Average	38.55 ± 0.45	54.39 ± 0.63	65.14 ± 0.59	69.69 ± 0.56G	Data processing
Table 10: Model comparisons between generative autoregressive models that produce pixel or vectorsketch drawings.
Table 11: Few-shot classification results on OmniglotOmniglot			(way, shot)			Algorithm	Backbone	Train Data	(5,1)	(5,5)	(20,1)	(20,5)Training from scratch (Hsu et al., 2019)	N/A	omniglot	52.50 ± 0.84	74.78 ± 0.69	24.91 ± 0.33	47.62 ± 0.44Random CNN	Conv4	N/A	67.96 ± 0.44	83.85 ± 0.31	44.39 ± 0.23	60.87 ± 0.22Conv-VAE	Conv4	omniglot	77.83 ± 0.41	92.91 ± 0.19	62.59 ± 0.24	84.01 ± 0.15Conv-VAE	Conv4	Quickdraw	81.49 ± 0.39	94.09 ± 0.17	66.24 ± 0.23	86.02 ± 0.14Conv-AE	Conv4	Quickdraw	81.54 ± 0.40	93.57 ± 0.19	67.24 ± 0.24	84.15 ± 0.16β-VAE (β = 250) (Higgins et al., 2017)	Conv4	Quickdraw	79.11 ± 0.40	93.23 ± 0.19	63.67 ± 0.24	84.92 ± 0.15k-NN (Hsu et al., 2019)	N/A	omniglot	57.46 ± 1.35	81.16 ± 0.57	39.73 ± 0.38	66.38 ± 0.36Linear Classifier (Hsu et al., 2019)	N/A	omniglot	61.08 ± 1.32	81.82 ± 0.58	43.20 ± 0.69	66.33 ± 0.36MLP + Dropout (Hsu et al., 2019)	N/A	omniglot	51.95 ± 0.82	77.20 ± 0.65	30.65 ± 0.39	58.62 ± 0.41Cluster Matching (Hsu et al., 2019)	N/A	omniglot	54.94 ± 0.85	71.09 ± 0.77	32.19 ± 0.40	45.93 ± 0.40CACTUs-MAML (Hsu et al., 2019)	Conv4	omniglot	68.84 ± 0.80	87.78 ± 0.50	48.09 ± 0.41	73.36 ± 0.34CACTUs-ProtoNet (Hsu et al., 2019)	Conv4	omniglot	68.12 ± 0.84	83.58 ± 0.61	47.75 ± 0.43	66.27 ± 0.37AAL-ProtoNet (Antoniou & storkey, 2019)	Conv4	omniglot	84.66 ± 0.70	88.41 ± 0.27	68.79 ± 1.03	74.05 ± 0.46AAL-MAML (Antoniou & storkey, 2019)	Conv4	omniglot	88.40 ± 0.75	98.00 ± 0.32	70.20 ± 0.86	88.30 ± 1.22UMTRA (Khodadadeh et al., 2019)	Conv4	omniglot	83.80	95.43	74.25	92.12sketchEmbedding (Ours)	Conv4	omniglot	94.88 ± 0.22	99.01 ± 0.08	86.18 ± 0.18	96.69 ± 0.07sketchEmbedding-avg (Ours)	Conv4	Quickdraw	96.37	99.43	90.69	98.07
Table 12: Few-shot classification results on mini-ImageNetmini-ImageNet			(way, shot)			Algorithm	Backbone	Train Data	(5,1)	(5,5)	(5,20)	(5,50)Training from scratch (Hsu et al., 2019)	N/A	mini-ImageNet	27.59 ± 0.59	38.48 ± 0.66	51.53 ± 0.72	59.63 ± 0.74UMTRA (Khodadadeh et al., 2019)	Conv4	mini-ImageNet	39.93	50.73	61.11	67.15CACTUs-MAML (Hsu et al., 2019)	Conv4	mini-ImageNet	39.90 ± 0.74	53.97 ± 0.70	63.84 ± 0.70	69.64 ± 0.63CACTUs-ProtoNet (Hsu et al., 2019)	Conv4	mini-ImageNet	39.18 ± 0.71	53.36 ± 0.70	61.54 ± 0.68	63.55 ± 0.64AAL-ProtoNet (Antoniou & storkey, 2019)	Conv4	mini-ImageNet	37.67 ± 0.39	40.29 ± 0.68	-	-AAL-MAML (Antoniou & storkey, 2019)	Conv4	mini-ImageNet	34.57 ± 0.74	49.18 ± 0.47	-	-Random CNN	Conv4	N/A	26.85 ± 0.31	33.37 ± 0.32	38.51 ± 0.28	41.41 ± 0.28Conv-VAE	Conv4	mini-ImageNet	23.30 ± 0.21	26.22 ± 0.20	29.93 ± 0.21	32.57 ± 0.20Conv-VAE	Conv4	sketchy	23.27 ± 0.18	26.28 ± 0.19	30.41 ± 0.19	33.97 ± 0.19Random CNN	ResNet12	N/A	28.59 ± 0.34	35.91 ± 0.34	41.31 ± 0.33	44.07 ± 0.31Conv-VAE	ResNet12	mini-ImageNet	23.82 ± 0.23	28.16 ± 0.25	33.64 ± 0.27	37.81 ± 0.27Conv-VAE	ResNet12	sketchy	24.61 ± 0.23	28.85 ± 0.23	35.72 ± 0.27	40.44 ± 0.28sketchEmbedding-avg (ours)	Conv4	Sketchy*	37.01	51.49	61.41	65.75sketchEmbedding-best (ours)	Conv4	sketchy*	38.61 ± 0.42	53.82 ± 0.41	63.34 ± 0.35	67.22 ± 0.32sketchEmbedding-avg (ours)	ResNet12	sketchy*	38.55	54.39	65.14	69.70sketchEmbedding-best (ours)	ResNet12	sketchy*	40.39 ± 0.44	57.15 ± 0.38	67.60 ± 0.33	71.99 ± 0.3MAML (supervised) (Finn et al., 2017)	Conv4	mini-ImageNet	46.81 ± 0.77	62.13 ± 0.72	71.03 ± 0.69	75.54 ± 0.62
