Table 1: Masked Language Model perplexities on BLLIP datasets of different models.
Table 2: The unsupervised parsing performance of different models.
Table 3: Fraction of ground truth constituents that were predicted as a constituent by the modelsbroken down by label (i.e. label recall)Relations MLM Constituency	Stanford	Conll	PPL	UF1	UAS	UUAS	UAS	UUASparent+dep	60.9 (1.0)	54.0 (0.3)	46.2 (0.4)	61.6 (0.4)	36.2 (0.1)	56.3 (0.2)parent	63.0 (1.2)	40.2 (3.5)	32.4 (5.6)	49.1 (5.7)	30.0 (3.7)	50.0 (5.3)dep	63.2 (0.6)	51.8 (2.4)	15.2 (18.2)	41.6 (16.8)	20.2 (12.2)	44.7 (13.9)Table 4: The performance of StructFormer with different combinations of attention masks.
Table 4: The performance of StructFormer with different combinations of attention masks.
Table 5: The performance of StructFormer on PTB dataset with different mask rates. Dependencyparsing is especially affected by the masks. Mask rate 0.3 provides the best and the most stableperformance.
