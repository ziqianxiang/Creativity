Table 1: Combined p values for feature visualization and lesion experiments: Each p value is acombination of p values for all of a network’s sub-clusters created with the Fisher or Chi Squaredmethod. For ImageNet networks, the values reflect results for single networks, but for all others,values (separately) give a median across 5 independently trained replicates. We do not ascribeany particular epistemic significance to the threshold of 0.05 (also note that significance thresholdsnormally need to be adjusted when taking a median of p values) but we bold all p values less thanit. In Appendix A.7, we present corrections for multiple comparisons. In Appendix A.6, we showresults for k = 8 and k = 18. Feature Visualization (Top): Results indicate tests for visualizationsproduced with Lucid. Lesions (Bottom): Results indicate tests involving lesions of sub-clusters.
Table 2: Results of Lesion tests for randomly initialized mid-sized VGG networks: As in Table1, each cell gives a median for 5 independent networks, and values less than 0.05 are bolded.
Table 3: Combined p values for lesion experiments with k = 8 (top) and k = 18 (bottom): Table1b replicated with alternate choices of the number of clustering centers for the same networks.
Table 4: Significance of results in table 1, using the Benjamini-Hochberg procedure to ensure thatthe false discovery rate per group—that is, the expectation under the data-generating distribution ofthe proportion of results declared significant that came from the null distribution—is below 0.05,where all the Fisher tests are grouped together and all the chi squared tests are separately groupedtogether.
Table 5: Significance of results in table 1, using the Holm-Bonferroni method to ensure that thefamily-wise error rate—that is, the probability under the data-generating distribution that any resultis falsely declared significant— is less than 1/20.
Table 6: Combined p values testing for differences in variance and coefficient of variationquartiles in feature visualization experiments: Compare to table 1a. In the activational variancecolumns, p values are shown comparing true sub-cluster activations to random sub-cluster activa-tions. As in table 1a, p values for non-ImageNet networks are medians among 5 trials, and p valuesless than 0.05 are bolded. In the coefficients of variance columns, quartiles for the CoVs for truesub-clusters are shown.
Table 7: Table of sub-clusters of an MLP network trained on Fashion-MNIST using pruningand dropout. The network is partitioned into 8 clusters.
Table 8: Table of sub-clusters of an VGG network trained on CIFAR-10 using dropout. Thenetwork is partitioned into 8 clusters.
Table 9: Dependency information of pairs of “important” sub-clusters of an MLP trained onFashion-MNIST with dropout. X and Y represent sub-clusters, X being the one earlier in thenetwork. They are numbered by first their layer and then their cluster number. The network ispartitioned into 8 cluster.
