Table 1: Throughput of various models for batches of size 32. Stars (*) indicate usage of gradientaccumulation due to GPU memory constraints. For sequences of length 440, batches of size 16 wereused for standard attention. For sequences of length 904, batches of size 4 were used.
Table 2: The top 16 heads in ProtBERT-BFD whose attention maps gave the most precise contactmaps across 500 validation families. Most of the top performing heads are found in the last layer.
Table 3: 6 families chosen for hyperparameter optimization for single-layer attention.
Table 4: 10 families chosen for hyperparameter optimization for factored attention1000Training data distributiontrRosetta datasettraining data800-0jz>u ① nb0)5 BZ>ug① əj J。u⅛u ① ~∣600-400 -200 -27 2B 2θ 210	211	212	213	214# of sequences in MSAFigure 10: The length and MSA size distribution for our 748 family subset (red) compared to thefull 15,051 families in the trRosetta dataset selected for trainingA.6.2 Producing Contact MapsA PDB structure gives 3D coordinates for every atom in a structure. We use Euclidean distancebetween the beta carbons to define distance between any pair of positions. A pair of positions wherethis distance is less than 8A is declared to be a contact.
