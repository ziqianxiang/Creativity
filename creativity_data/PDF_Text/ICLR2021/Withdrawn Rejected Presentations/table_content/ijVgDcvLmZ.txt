Table 1:	Payoff of matrix gameTable 2:	QTRAN-altTable 3:	QMIX^∖~U2 ui、∖	A	B	CA	-8-	-12	~Λ2~B	^-Γ2-	0	0~o~C	--T2-	0	~0~Q2 Qι^^	3.3	0.1	0.14.7	8.0	-12.0	-12.0-0.1	-12.0	0.0	0.0-0.1	-12.0	0.0	0.0Q2 Qι^^	-5.6	0.1	0.1-6.6	-8.1	-8.1	-8.102	-8.1	0.0	0.00.1	-8.1	0.0	0.0Table 4:	FSV, α0 = 0.01Table 5:	FSV, α0 = 0.1Table 6:	FSV, α0 = 1Q2 Qi^^	3.3	-0.7	-0.04.7	8.0	-12.0	-12.0
Table 2:	QTRAN-altTable 3:	QMIX^∖~U2 ui、∖	A	B	CA	-8-	-12	~Λ2~B	^-Γ2-	0	0~o~C	--T2-	0	~0~Q2 Qι^^	3.3	0.1	0.14.7	8.0	-12.0	-12.0-0.1	-12.0	0.0	0.0-0.1	-12.0	0.0	0.0Q2 Qι^^	-5.6	0.1	0.1-6.6	-8.1	-8.1	-8.102	-8.1	0.0	0.00.1	-8.1	0.0	0.0Table 4:	FSV, α0 = 0.01Table 5:	FSV, α0 = 0.1Table 6:	FSV, α0 = 1Q2 Qi^^	3.3	-0.7	-0.04.7	8.0	-12.0	-12.007	-12.0	0.0	0.0
Table 3:	QMIX^∖~U2 ui、∖	A	B	CA	-8-	-12	~Λ2~B	^-Γ2-	0	0~o~C	--T2-	0	~0~Q2 Qι^^	3.3	0.1	0.14.7	8.0	-12.0	-12.0-0.1	-12.0	0.0	0.0-0.1	-12.0	0.0	0.0Q2 Qι^^	-5.6	0.1	0.1-6.6	-8.1	-8.1	-8.102	-8.1	0.0	0.00.1	-8.1	0.0	0.0Table 4:	FSV, α0 = 0.01Table 5:	FSV, α0 = 0.1Table 6:	FSV, α0 = 1Q2 Qi^^	3.3	-0.7	-0.04.7	8.0	-12.0	-12.007	-12.0	0.0	0.00.7	-12.0	0.0	0.0
Table 4:	FSV, α0 = 0.01Table 5:	FSV, α0 = 0.1Table 6:	FSV, α0 = 1Q2 Qi^^	3.3	-0.7	-0.04.7	8.0	-12.0	-12.007	-12.0	0.0	0.00.7	-12.0	0.0	0.0^^Qr Qi	7.2	-0.5	-0.50.9	8.0	-11.8	-11.705	-11.8	-0.0	0.00.5	-11.8	-0.0	0.0Q2 Qi^^	0.3	0.1	0.17.7	-80-	~4Γ	--49-00		0.1	-0.0-0.1		-0.1	-0.0value and the optimal action due to the limitation of additivity and monotonicity structures whileFSV and QTRAN successfully represent all the joint action values. In addition, even if α is notannealed to very small, FSV correctly approximated the optimal joint action values because wedirectly estimate λ when α and αi tend to 0, which relaxes the constraints of the function class toguarantee the correct structure during the training process.
Table 5:	FSV, α0 = 0.1Table 6:	FSV, α0 = 1Q2 Qi^^	3.3	-0.7	-0.04.7	8.0	-12.0	-12.007	-12.0	0.0	0.00.7	-12.0	0.0	0.0^^Qr Qi	7.2	-0.5	-0.50.9	8.0	-11.8	-11.705	-11.8	-0.0	0.00.5	-11.8	-0.0	0.0Q2 Qi^^	0.3	0.1	0.17.7	-80-	~4Γ	--49-00		0.1	-0.0-0.1		-0.1	-0.0value and the optimal action due to the limitation of additivity and monotonicity structures whileFSV and QTRAN successfully represent all the joint action values. In addition, even if α is notannealed to very small, FSV correctly approximated the optimal joint action values because wedirectly estimate λ when α and αi tend to 0, which relaxes the constraints of the function class toguarantee the correct structure during the training process.
Table 6:	FSV, α0 = 1Q2 Qi^^	3.3	-0.7	-0.04.7	8.0	-12.0	-12.007	-12.0	0.0	0.00.7	-12.0	0.0	0.0^^Qr Qi	7.2	-0.5	-0.50.9	8.0	-11.8	-11.705	-11.8	-0.0	0.00.5	-11.8	-0.0	0.0Q2 Qi^^	0.3	0.1	0.17.7	-80-	~4Γ	--49-00		0.1	-0.0-0.1		-0.1	-0.0value and the optimal action due to the limitation of additivity and monotonicity structures whileFSV and QTRAN successfully represent all the joint action values. In addition, even if α is notannealed to very small, FSV correctly approximated the optimal joint action values because wedirectly estimate λ when α and αi tend to 0, which relaxes the constraints of the function class toguarantee the correct structure during the training process.
Table 7: training result for Max of Two Quadratics game	opt	sub-opt	otherFSV	^^0^^	0	-0-MADDPG	2^Γ~	-18-	-0--QMIX-	2^Γ~	-18-	-0-VDN	-0-	16	4results indicate that, a more explorative policy and correct estimation of Q-values are both neededto overcome the relative overgeneralization problem. Using a centralized critic like MADDPG toguide the decentralized actors will mislead the policy gradients because it averages the Q-valuesbased on others’ policies (?). Using individual Q-values to guide actors requires the full expressiveability of factorizable tasks where QMIX and VDN fail to estimate individual Q-values correctlydue to the structural limitation as shown in Sec5.1 and QTRAN losts its tractability for continuoustasks. To enable better exploration in joint action space, Wei et al. (2018) adopt multi-agent softq-learning to avoid the relative overgeneralization problem, but it still uses a centralized critic whichsuffers scalability and it’s very sensitive to how the temperature parameter anneals. It’s clear that,FSV utilizes value function factorization method to get correct estimation of individual Q-valuesand carries exploration with a more explorative energy-based policy can achieve 100% success rate.
Table 8: hyper-parameterssettings	discrete	continuousFSV		layer number of λ	2	2unit number of hidden layer in λ	64	64layer number of w, b	1	1unit number of hidden layer in w, b	0	64layer number of actor	0	2unit number of hidden layer in actor	0	64learning rate of actor	0	3e - 4α decay scheme	linear decay from 1 to 0.01	Automate Entropy AdjustmentQMIX and VDN		layer number of actor	0	2unit number of hidden layer in actor	0	64learning rate of actor	0	3e-47.2 Implementation DetailsIn discrete tasks, We follow the PyMARL Samvelyan et al. (2019) implementation of VDN, QMIXand QTRAN, where the hyper-parameters of are the same in SMAC Samvelyan et al. (2019). Weillustrate the special hyper-parameters of FSV in Table 8 and others are the default settings in Py-MARL. In discrete tasks, we extend VDN and QMIX to the actor-critic framework. Specifically,
