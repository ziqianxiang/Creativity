Table 1: Comparison of median test accuracy on various benchmark datasets. We report the meanand standard deviation over three runs with different random seeds and splits, and also report themean of the best accuracy in parentheses. The best scores are indicated in bold. We denote methodshandling unlabeled out-of-class samples (i.e., open-set) as “Open-SSL”.
Table 2: Comparison of median test accuracy on 9 super-classes of ImageNet, which are obtainedby grouping semantically similar classes in ImageNet; Dog, Reptile, Produce, Bird, Insect, Food,Primate, Aquatic animal, and Scenery. We report the mean and standard deviation over three runswith different random seeds and splits. The best scores are indicated in bold. We denote methodshandling unlabeled out-of-class samples (i.e., open-set) as “Open-SSL”.
Table 3: Ablation study on three main components of our method: the detection criterion (“Detect”),auxiliary loss (“Aux. loss”), and auxiliary BNs (“Aux. BNs”). We report the mean and standarddeviation over three runs with different random seeds and a fixed split of labeled data.
Table 4: Comparison of the median test accuracy of ResNet-50 trained on out-of-class samples andtheir soft-labels of the CIFAR-10 benchmarks with 4 labels per class. We denote the new setting ofminimizing with the auxiliary loss as “Aux. loss only”. We report the mean and standard deviationover three runs with different random seeds and splits of labeled data.
Table 5: Comparison of the median test accuracy of Wide-ResNet-28-2 and ResNet-50 on CIFAR-100+ TinyImageNet benchmark over baseline methods. The best scores are indicated in bold. We denotemethods handling unlabeled out-of-class samples (i.e., open-set) as “Open-SSL”.
Table 6: Comparison of the median test accuracy on the CIFAR-10 + SVHN benchmark with 400labels per class over baseline methods. The best scores are indicated in bold.
Table 7: Super-classes used in 9 benchmarks from ImageNet dataset. The class ranges are inclusive.
Table 8: Comparison of median test accuracy on the CIFAR-100 + TinyImageNet benchmark with(a) 4 and (b) 25 labels per class, over various hyperparameters τ and λ.
Table 9: Comparison of the median test accuracy for the use of out-of-class samples on the CIFAR-scale benchmarks with 4 labels per class. We denote whether using out-of-class samples for trainingas “w/ out-of-class”. We report the mean and standard deviation over three runs with different randomseeds and splits. The best scores are indicated in bold.
Table 10: Comparison of detection methods on the CIFAR-Animals + CIFAR-Others benchmarkwith 4 labels per class under various evaluation metrics. We denote our detection method without theprojection header as “Ours w/o header”. The best scores are indicated in bold.
Table 11: Comparison of the median test accuracy on the CIFAR-Animals + CIFAR-Others benchmarkwith 4 labels per class among various detection methods. We denote our detection method withoutthe projection header as “Ours w/o header”. We report mean and standard deviation over three runswith different random seeds and a fixed split of labeled data. The best scores are indicated in bold.
Table 12: The detection performance across different (a) proportions of out-of-class and (b) detectionthresholds in CIFAR-Animals + CIFAR-Others benchmark with 4 labels per class.
