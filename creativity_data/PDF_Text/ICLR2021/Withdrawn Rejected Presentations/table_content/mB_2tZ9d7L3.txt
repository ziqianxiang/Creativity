Table 1: The accuracy rate (%) comparison of applying our AFI module to different residual stagesof ReSNet-32 and ReSNet-110. BeSt results are marked in bold.
Table 2: The accuracy rate(%) of AFI-MobileNetV2 with various of the self-attention model inCIFAR-100.
Table 3: The architecture details of AFI-ResNet-6N+2 for CIFAR dataset. The operations andfeature shapes are listed inside the brackets and the number of stacked blocks is shown outside.
Table 4: Accuracy rates (%) on CIFAR-100 dataset. All results are reproduced by ourselves for afair comparison. Our network results are bold in the table. The FLOPs are calculated by assumingthe batch size of 32._________________________________________________________Model	C100	Params	FLOPsResNet-32 (He et al., 2016)	71.16	472.76K	2.23GAFI-ResNet-32	71.09	378.23K	1.78GAFI-ResNet-32-B	72.57	668.98K	3.15GResNet-110 (He etal.,2016)	73.73	1.74M	8.17GAFI-ResNet-110	74.03	1.35M	6.23GAFI-ResNet-110-B	75.69	2.57M	12.06GShUfleNetV2 (Ma et al., 2018)	70.71	0.94M	1.32GAFI-ShUfleNetV2	72.06	0.95M	1.32GAFI-ShUfleNetV2-B	72.24	1.29M	1.76GMobileNetV2 (Sandler et al., 2018)	75.20	2.41M	3.03GAFI-MobileNetV2	75.94	2.25M	2.67GAFI-MobileNetV2-B	77.08	2.84M	3.16GResNeXt-29(32×4d)(Xie etal., 2017)	78.44	4.87M	24.95GAFI-ResNeXt-29(32 ×4d)	79.37	4.26M	21.74GAFI-ResNeXt-29(32 × 4d)-B	一	79.85	5.23M	25.90Gand automobiles. CIFAR-100 requires the network to classify pictures into 100 categories. We train
Table 5: The table shows the accuracy rates (%) of networks on the ImageNet validation set. Our re-sults are marked in bold. All results are reproduced for a fair comparison. The FLOPs are calculatedby assuming the batch size of 32.
Table 6: The comparison of the memory usage of AFI-ResNet-50 and ResNet-50 in the training andtesting process. The batch size is 64.
