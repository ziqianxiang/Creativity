Table 1: Defining terms from neuroscience weuse and their parallels in Deep Learning.
Table 2: Comparing the spacial complexity of Bractivate with various state-of-the-art UNet architectures. Thetop group represents manually-designed models. The middle row comprises differentiable search. The bottomis ours, based on dendritic branching. We report the results as ”Dice (# Params).”Model	I	I LUng	EM	NucUNet (Ronneberger et al., 2015)	0.925 (7.7e6)	0.811 (7.7e6)	0.833 (7.7e6)R2-UNet (Alom et al., 2018)	0.596 (9.5e7)	0.464 (9.5e7)	0.049 (9.5e7)Attn-UNet (Oktay et al., 2018)	0.954 (3.2e7)	0.937 (3.2e7)	0.721 (3.2e7)UNet++ (Zhou et al., 2018)	0.903 (9.0e6)	0.846 (9.0e6)	0.841 (9.0e6)WideUNet (Zhou et al., 2018)	0.888 (9.3e6)	0.811 (9.3e6)	0.828 (9.3e6)NasUNet (Weng et al., 2019)	∣	I 0.934 (1.2e5)	0.729 (4.8e5)	0.774 (1.2e5)BraCtivate	∣	I 0.942 (3.1e4)	0.929 (4.8e5)	0.878 (4.8e5)Figure 7 highlights how Bractivate achieves comparable performance to larger models when ini-tialized with Xavier initialization (Glorot & Bengio, 2010). Table 2 highlights how Bractivate issignificantly smaller than many of the other state-of-the-art models: it exchanges high spatial com-8Under review as a conference paper at ICLR 2021Plexity for more skip connections, as these branches allow information to propagate through salientblocks in the network. For domain-specific tasks, high parameters reduce the signal: noise ratio inthe network; simpler models like Bractivate rely on powerful skip connections, analogous to den-drites, to carry most of the signal. Because these connections consist of simple concatenation or
Table 3: GPU Run-time for all three datasets with time measured in hours averaged over three trials with imagedimensions of 128 × 128.
