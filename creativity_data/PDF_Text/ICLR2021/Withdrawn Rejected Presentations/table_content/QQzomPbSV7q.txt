Table 1: Recall@k evaluated on MNIST dataset. The train classes are digits 0-5 and the test classesare digits 6-9model	MNIST Train Digits			MNIST Test Digits			R@1	R@5	R@10	R@1	R@5	R@10Triplet	42.01	87.51	96.56	35.16	80.86	93.26EPS + Triplet [ours]	65.78	93.57	97.38	42.31	83.86	93.61clusters is by using K-means algorithm on the embedding space, with K equal to the number ofclasses. However, this prevents from the measurement capturing more diverse solutions in whichhomogeneous clusters appear only when using larger amount of clusters. Regular NMI preferssolutions with class-collapsing. Therefore, we increase the number of clusters in the NMI evaluation(denote it by NMI+) we also report the regular NMI score.
Table 2: Recall@k and NMI performance on Cars196 and CUB200- 2011. NMI+ indicate the NMImeasurement when using 10 (number of classes) clusters. Our EPS method improves in all cases. *:Our re-implemented version with the same embedding dimension.
Table 3: Recall@k and NMI performance on Omniglot dataset. In both cases the training was donewith only language labels. Right: evaluation on language labels. Left: evaluation on letter labels.
Table 4: Results of semi-hard with/without EPS on the Omniglot training dataset. Without EPS thenetwork feet almost perfectly to the training set. However, using EPS results in batter performanceson the letters fine-grained task.
Table 5: Std of Recall@1 results. Each model was trained 8 times with different random seeds.
Table 6: Results of Multi-similarity loss with Embedding size 512 (as in Wang et al. (2019)). UsingEPS improve results in both cases.
