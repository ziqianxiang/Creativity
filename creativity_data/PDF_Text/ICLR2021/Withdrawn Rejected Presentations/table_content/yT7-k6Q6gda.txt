Table 1: Using a 10-30x smaller learning rate (Baseline) results in up to 9% degradation in testaccuracy on popular image classification benchmarks (c.f. to optimal η*). Adding Fisher penalty (FP)substantially improves generalization and closes the gap to η*. We do not use data augmentation withCIFAR-10 and CIFAR-100 to ensure that using a small learning rate does not lead to under-fitting.
Table 2: Fisher penalty (FP) improves generalization in 4 out of 5 settings when applied with theoptimal learning rate η* and trained using standard data augmentation. In 3 out of 5 settings thedifference between FP and η* is small (below 1%), which is expected given that FP is aimed atreproducing the regularization effect of large η, and we compare to training with the optimal η*.
Table 3: Fisher Penalty (FP) and GPr both reduce memorization competitively to mixup. We measuretest accuracy at the best validation point in training with either 25% or 50% examples with noisylabels in the CIFAR-100 dataset.
Table 4: The maximum value of Tr(F) along the optimization trajectory for experiments on CIFAR-10or CIFAR-100 included in Table 1.
Table 5: Time per epoch (in seconds) for experiments in Table 1.
Table 6: The final epoch training accuracy for experiments shown in Table 1. Experiments with smalllearning rate reach no lower accuracy than experiments corresponding to a large learning rate η*.
