Table 1: Average generation time in sec-onds per token for generating sequences oflength 256.
Table 2: Human and automatic evaluation for sentiment on book text generation (rated for positivity, bookresemblance and fluency all on a scale of 1-5), with key results in bold. For human evaluation, we averagethree annotations on generations from 50 prompts for each model, where prompts are from the start of bookchapters, and are a minimum of 150 char. For automatic evaluation, we use a RoBERTa classifier trained onSST-2 (Socher et al., 2013) to measure label fidelity (how often the sample is classified as having the same labelas the control code), and measure the perplexity of generations under GPT-2 to compute perplexity scores. Wecompare using a CC-LM as a GeDi to guide GPT2-XL (GeDi-guided), vs. direct class conditional generation(CC-LM). Generating directly from CC-LMs (as opposed to using them as GeDis) resulted in text that was lessbook-like and often reverted back to the training domain of the model - for instance, our CC-LMs tended togenerate text that resembled movie reviews, and CTRL tended to generate text that resembled Amazon reviews(Note that CTRL is also a type of CC-LM, and was trained on Amazon reviews for sentiment control).
Table 3: Human and automatic evaluation of toxicity. We collect 3 annotations of toxicity labels (where weclassify sample based on majority) and linguistic fluency scores (scale of 1-5) for 100 samples for each model.
Table 4: Automatic label fidelity on topics, measured by how often a RoBERTa classifier’s label matches thecontrol code used to generate the sample. We trained 4 different class-conditional language models, each with1 class held out and we consider direct CTRL-style generation (CC-LM), and GeDi-guided generation fromthese models. “training set class” label fidelity averages the label fidelities from 3 models trained with thegiven class as one of the training classes. The “zero-shot class” label fidelity for each class uses generationsfrom the model trained on the other 3 classes, using a zero-shot control code for the desired class. We includeresults from raw GPT-2-XL as a baseline to show how much GeDi and CC-LM are influencing generation. Wefind that GeDi is able to influence generation more effectively than CC-LM when conditioning on both trainingclasses and held out classes.
Table 5: Label fidelity and perplexity scores for the weighted decoding heuristic, filtering heuristic,and combined weighted decoding filtering heuristic.
Table 6: RoBERTa-based toxicity and perplexity scores for the weighted decoding heuristic, filteringheuristic, and combined weighted decoding filtering heuristic.
Table 7: Statistical significance p-values for sentiment results in Table 2. We use a Wilcoxon signedrank test for paired measures, since all models generate from the same set of prompts (and becausea non-parametric test is appropriate for an ordinal scale). All p-values are 2-tailed and compare thealigned models in first two columns for positivity, book resemblance, and fluency.
Table 8: Statistical significance p-values for toxicity results in Table 3. For toxicity, we use a 2proportions z-test. For fluency we use a Wilcoxon signed rank test for paired measures as in Table7. All p-values are 2-tailed and compare the aligned models in first two columns for toxicity andfluency.
Table 9: Controlling the sentiment/tone of generation with GeDi (greedy decoding). Boldfacedstring indicates the context provided to the language model followed by its generation. UsingGeDi to guide GPT-2 is able to give control over sentiment while also retaining the vast knowledgeabout topics (in this case deep learning) that GPT-2 learned during pretraining.
Table 10: Controlling the sentiment/tone of generation with GeDi (greedy decoding). Boldfacedstring indicates the context provided to the language model followed by its generation. UsingGeDi to guide GPT-2 generation gives control over sentiment.
Table 11: Controlling the sentiment/tone of generation with GeDi and CC-LMs (greedy decod-ing). Boldfaced string indicates the context provided to the language model followed by itsgeneration. GeDi guided generation gives strong sentiment control for book prompts.
Table 12: Generation with raw GPT-2 and PPLM when controlling the sentiment/tone of the text(greedy decoding). Boldfaced indicates the context provided to the language model followed byits generation. Like GeDi, PPLM was able to achieve domain transfer of sentiment. However,GeDi led to 30x faster generation.
Table 13: Controlling the sentiment/tone of generation with CTRL (greedy decoding). Boldfacedindicates the context provided to the language model followed by its generation. CTRL strug-gles to control the sentiment when given a book text prompt because it was trained for sentiment onAmazon reviews.
Table 14: Controlling the sentiment/tone of generation with class-conditional language models(greedy decoding). Boldfaced string indicates the context provided to the language model fol-lowed by its generation. Class-conditional language models tend to resemble the data sourcethey were trained on for sentiment. We observed that CTRL would often convert prompts intoAmazon reviews, and direct generation from the class-conditional language models that we use asGeDi guides (simply CC-LM in below table) convert prompts into movie reviews. Underlinedtext indicates the parts of the generation that resemble reviews. The CC-LMs also produces post-processing artifacts from the IMDb dataset they were trained on. When using these same CC-LMs asGeDis to guide generation, training domain artifacts are cancelled out from contrasting the positiveand negative distributionsModel	GenerationCTRL (negative) (greedy)	12, Heinlein One Simmons didn’t know what to make of the sound he’d heard in Herb’s voice. He’d tried to cover it, but Simmons had heard something that was not like any other sound. It was a voice he couldn’t quite place. He could tell it wasn’t human. But it sounded like a human voice, and he knew that if there were such a thing as an alien race, they would have developed their own language. And so did he. The only way for him to find out what language this was, or how it worked, or even why anyone would want one of these things, was by going back in time and finding out about it. So he went back in time, and found himself on the planet Earth. And then his ship crashed. And when he woke up, all his memories were gone. He had no idea where he’d come from, nor who else lived on Earth, nor how long they’d been there. But now he’s here, with no memory at all. Except for one thing. His name. John Scalzi. That’s it.I don’t think I’ve ever read anything more depressing than this. It’s almost too depressing. If you can get past that, though, it’s actually pretty good. The characters are interesting, though some are just plain annoying. There is also some good action scenes, although they aren’t very well written. This book is definitely worth reading, but be prepared for some really	depressing stuff .
Table 15: Detoxification of LM generation with GeDi (greedy decoding) when triggers from RealToxicity Prompts (filtered as described in Section 5.2) are used. Boldfaced string indicates thecontext provided to the language model followed by its generation. GPT-2 and GPT-3 completethese with text containing toxicity in the form of sexually explicit references, profanity or insultinglanguage including curse words.
Table 16: Controlling topic of generation (zero-shot) with GeDi (greedy decoding). This particulartopic GeDi (from Section 5.3) was trained as CC-LM on only three classes: science, sports and busi-ness. The topics of Space, Fire, and History were not a part of the GeDi training set. Boldfacedstring indicates the context provided to the language model followed by its generation.
Table 17: Controlling topic of generation (zero-shot) with GeDi (greedy decoding). This particulartopic GeDi (from Section 5.3) was trained as CC-LM on only three classes: science, sports andbusiness. The topics of World, Cars, Climate were not a part of the GeDi training set. Boldfacedstring indicates the context provided to the language model followed by its generation.
Table 18: Controlling topic of generation (zero-shot) with the CTRL model (greedy decoding).
