Table 1: Precision@K scores (P@K) corresponding to K=10 and 50 for the IMA model and base-lines for the task of retrieval of similar multimodal MNIST-TIDIGITS paired data.
Table 2: Precision@K scores at K=10 for the proposed IMA model and other baselines for the taskof same-emotion utterance retrieval from IEMOCAP (for happy, angry, sad, neutral emotions).
Table 3: Experimental setup for digits (MNIST-TIDIGITS) and emotion classification (IEMOCAP)Parameters Dataset	Digits (MNIST-TIDIGrrS)	Emotion (IEMOCAP)Learning Rate (eta)range	{0.0625,0.125,0.25, 0.5,1.0,2.0,4.0,8.0}	{0.0625,0.125,0.25, 0.5,1.0,2.0,4.0,8.0}Train set size	50000	TTBValidation set size	15000	526Test set size	10000	-NeuronS/layer	50	50A.4 Model Sizes of IMA and baselinesThe IMA model has the advantage of having only M encoders and O(M) loss terms to optimizecompared to the JMVAE-KL and MVAE models. That in conjunction with no stochasticity assump-tion (present in VAEs) results in fewer autoencoder parameters to train compared to JMVAE-KLand MVAE, even when the encoder and decoder sizes are the same. Table 4 lists the model sizes forthe IMA model and the baseline models for the experimental settings described in Section 5. TheIMA model has slightly higher parameters compared to MVAE, however that increase comes withthe ability of the importance networks to learn uncorrelated noise across modalities.
Table 4: Number of parameters of IMA model compared with the JMVAE-KL and MVAE baselinesModel Component	IMA	JMVAE-KL	MVAEUnimodal Encoders	753000	783000	783000Multimodal Encoders	0	1309300	0Decoders	754280	1217360	1217360Importance Network	723402	0	0Total Size	2230682	3309660 —	200036013Under review as a conference paper at ICLR 2021Table 5: Test set accuracies obtained by the multimodal embeddings from the proposed IMA modeland other baselines on MNIST-TIDIGITS dataset. Accuracies are shown both for 2D and 50Drepresentations. Multimodal embeddings from the proposed model outperform unimodal and fusionapproaches without importance-based weightingModel		2D					50D				overall	image noise	speech noise	overall	image noise	speech noiseIMA (importance weights)	83.88	^6929	82.20	96.94	86.99	94.46IMA (no importances)	79.88	59.86	63.13	96.84	86.54	93.61IMA (unimodal image)	76.90	12.06	83.05	86.92	9.41	95.31IMA (unimodal speech)	62.26	66.00	10.80	82.88	90.80	11.70JMVAE-KL SUzUki et al.(2016)	53.06	23.24	14.40	97.9	93.85	93.22
Table 5: Test set accuracies obtained by the multimodal embeddings from the proposed IMA modeland other baselines on MNIST-TIDIGITS dataset. Accuracies are shown both for 2D and 50Drepresentations. Multimodal embeddings from the proposed model outperform unimodal and fusionapproaches without importance-based weightingModel		2D					50D				overall	image noise	speech noise	overall	image noise	speech noiseIMA (importance weights)	83.88	^6929	82.20	96.94	86.99	94.46IMA (no importances)	79.88	59.86	63.13	96.84	86.54	93.61IMA (unimodal image)	76.90	12.06	83.05	86.92	9.41	95.31IMA (unimodal speech)	62.26	66.00	10.80	82.88	90.80	11.70JMVAE-KL SUzUki et al.(2016)	53.06	23.24	14.40	97.9	93.85	93.22MVAE WU & Goodman (20Γ8T	41.32	39.69	10.38	98.18	^495~	92.37Table 6: Emotion recognition accuracies for the best model on the validation set achieved by IMAmodel on IEMOCAP dataset, along with JMVAE-KL and MVAE baseline models.
Table 6: Emotion recognition accuracies for the best model on the validation set achieved by IMAmodel on IEMOCAP dataset, along with JMVAE-KL and MVAE baseline models.
