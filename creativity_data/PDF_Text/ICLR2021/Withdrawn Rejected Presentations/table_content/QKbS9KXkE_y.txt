Table 1: Quantitative results indicating the diversity of options used (entropy), and clustering accuracyin action and state spaces (silhouette score), with and without information asymmetry (IA), andwith or without limited number of switches. Higher values indicate greater separability by option /component.
Table 2: Hyperparameters - OpenAI gymB.2	Action and Temporal Abstraction ExperimentsShared across all algorithms, we use 3-layer convolutional policy and Q-function torsos with [128, 64,64] feature channels, [(4, 4), (3, 3), (3, 3)] as kernels and stride 2. For all multitask domains, we buildon information asymmetry and only provide task information as input to the high-level controller andtermination conditions to create additional incentive for the options to specialize. The Q-function hasaccess to all observations (see the corresponding tables in this section). We follow (Riedmiller et al.,2018; Wulfmeier et al., 2020) and assign rewards for all possible tasks to trajectories when addingdata to the replay buffer independent of the generating policy.
Table 3: Hyperparameters. Values are taken from the OpenAI gym experiments with the abovementioned changes.
Table 4: Action space for the Sawyer Stacking experiments.
Table 5: Observations for the Sawyer Stacking experiments. The TCP’s pose is represented as itsworld coordinate position and quaternion. In the table, m denotes meters, rad denotes radians, and qrefers to a quaternion in arbitrary units (au).
Table 6: Action space for the Sawyer Ball-in-Cup experiments.
Table 7: Observations for the Sawyer Ball-in-Cup experiments. In the table, m denotes meters, raddenotes radians, and q refers to a quaternion in arbitrary units (au). Note: the joint velocity andcommand represent the robot’s internal state; the 3 degrees of freedom that were fixed provide aconstant input of 0.
Table 8: Observations for the go to one of 3 targets task with ’Ball’ and ’Ant.
