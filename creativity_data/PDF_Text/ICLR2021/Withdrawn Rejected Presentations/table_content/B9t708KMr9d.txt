Table 1: Comparision between message passing modelsTraining	Prediction	Feature	Label	Feature	LabelLPA		-^X^^		XGCN	X		X	APPNP	X		X	GCN-LPA	X	X	X	UniMP (Ours)	X	X	X	XTo unify the feature and label propagation, there are mainly two issues needed to be addressed:Aggregating feature and label information. Since node feature is represented by embeddings,while node label is a one-hot vector. They are not in the same vector space. In addition, thereare different between their message passing ways, GNNs can propagate the information by diverseneural structures likes GraphSAGE (Hamilton et al., 2017), GCN (Kipf & Welling, 2016) and GAT(Velickovic et al., 2017). But LPAs can only pass the label message by graph adjacency matrix.
Table 2: Dataset statistics of OGB node property predictionName	Node	Edges	Tasks	Split Rate	Split Type	Task Type	Metricogbn-products	2,449, 029	61, 859, 140	1	8∖02∖88	Sales rank	Multi-class class	Accuracyogbn-proteins	132,534	39, 561, 252	112	65\16\19	Species	Binary class	ROC-AUCogbn-arxiv	169,343	1, 166, 243	1	78\08\14	Time	Multi-class class	AccuracyDatasets. Most of the frequently-used graph datasets are extremely small compared to graphs foundin real applications. And the performance of GNNs on these datasets is often unstable due to severalissues including their small-scale nature, non-negligible duplication or leakage rates, unrealisticdata splits (Dwivedi et al., 2020; Hu et al., 2020). Consequently, we conduct our experiments onthe recently released datasets of Open Graph Benchmark (OGB) (Hu et al., 2020), which overcomethe main drawbacks of commonly used datasets and thus are much more realistic and challenging.
Table 3: The hyper-paramerter setting of our model	ogbn-Products	ogbn-proteins	ogbn-arxivSamPling_method	NeighborSamPling	Random Partition	Full-batchnum_layers	3	7	3hidden_size	128	64	128num_heads	4	4	2dropout	0.3	0.1	0.3lr	0.001	0.001	0.001Weight_decay	*	*	0.0005label_rate	0.625	0.5	0.6254.2	Comparison with SOTAsBaseline and other comparative SOTAs are provided by OGB leaderboard. Some of the includingresults are conducted officially by authors from original papers, While the others are re-implementedby communities. And all these results are guaranteed to be reproducible With open source codes.
Table 4: Results for ogbn-productsModel	Test Accuracy	Validation Accuracy	ParamsGCN-Cluster (Chiang et al., 2019)	0.7897 ± 0.0036	0.9212 ± 0.0009	206, 895GAT-Cluster	0.7923 ± 0.0078	0.8985 ± 0.0022	1, 540, 848GAT-NeighborSampling	0.7945 ± 0.0059	-	1, 751, 574GraphSAINT (Zeng et al., 2019)	0.8027 ± 0.0026	-	331, 661DeeperGCN (Li et al., 2020)	0.8090 ± 0.0020	0.9238 ± 0.0009	253, 743UniMP	0.8256 ± 0.0031	0.9308 ± 0.0017	1, 475, 605Table 5: Results for ogbn-proteinsModel	Test ROC-AUC	Validation ROC-AUC	ParamsGaAN (Zhang etal., 2018)	0.7803 ± 0.0073	-	-GeniePath-BS (LiU et al., 2020b)	0.7825 ± 0.0035	-	316, 754MWE-DGCN	0.8436 ± 0.0065	0.8973 ± 0.0057	538, 544DeepGCN (Li et al., 2019)	0.8496 ± 0.0028	0.8921 ± 0.0011	2, 374, 456DeeperGCN (Li et al., 2020)	0.8580 ± 0.0017	0.9106 ± 0.0016	2, 374, 568UniMP	0.8642 ± 0.0008	0.9175 ± 0.0007	1, 909, 1046Under review as a conference paper at ICLR 2021Table 6: Results for ogbn-arxivModel	Test Accuracy	Validation Accuracy	Params
Table 5: Results for ogbn-proteinsModel	Test ROC-AUC	Validation ROC-AUC	ParamsGaAN (Zhang etal., 2018)	0.7803 ± 0.0073	-	-GeniePath-BS (LiU et al., 2020b)	0.7825 ± 0.0035	-	316, 754MWE-DGCN	0.8436 ± 0.0065	0.8973 ± 0.0057	538, 544DeepGCN (Li et al., 2019)	0.8496 ± 0.0028	0.8921 ± 0.0011	2, 374, 456DeeperGCN (Li et al., 2020)	0.8580 ± 0.0017	0.9106 ± 0.0016	2, 374, 568UniMP	0.8642 ± 0.0008	0.9175 ± 0.0007	1, 909, 1046Under review as a conference paper at ICLR 2021Table 6: Results for ogbn-arxivModel	Test Accuracy	Validation Accuracy	ParamsDeePerGCN (Li et al., 2020)	0.7192 ± 0.0016	0.7262 ± 0.0014	1, 471, 506GaAN (Zhang et al., 2018)	0.7197 ± 0.0024	-	1, 471, 506DAGNN (LiU et al., 2020a)	0.7209 ± 0.0025	-	1, 751, 574JKNet (XU et al., 2018b)	0.7219 ± 0.0021	0.7335 ± 0.0007	331, 661GCNII (Chen et al., 2020)	0.7274 ± 0.0016	-	2, 148, 648UniMP	0.7311 ± 0.0021	0.7450 ± 0.0005	473, 4894.3	Ablation studies on Graph Transformer and Masked Label PredictionIn this section, we will conduct extensive studies to identify the improvements from different com-
Table 6: Results for ogbn-arxivModel	Test Accuracy	Validation Accuracy	ParamsDeePerGCN (Li et al., 2020)	0.7192 ± 0.0016	0.7262 ± 0.0014	1, 471, 506GaAN (Zhang et al., 2018)	0.7197 ± 0.0024	-	1, 471, 506DAGNN (LiU et al., 2020a)	0.7209 ± 0.0025	-	1, 751, 574JKNet (XU et al., 2018b)	0.7219 ± 0.0021	0.7335 ± 0.0007	331, 661GCNII (Chen et al., 2020)	0.7274 ± 0.0016	-	2, 148, 648UniMP	0.7311 ± 0.0021	0.7450 ± 0.0005	473, 4894.3	Ablation studies on Graph Transformer and Masked Label PredictionIn this section, we will conduct extensive studies to identify the improvements from different com-ponents of our unified model. To get a fair comparison, we re-implement classical GNN methodslike GCN and GAT, following the same sampling methods and model setting shown in Table 3. Thehidden size of GCN is head_num*hidden_size since it doesn't have head attention. We also changedifferent inputs for our models to study the effectiveness of feature and label propagation.
Table 7: Ablation studies on models with different inputs.
Table 8: The tuned hyperparamerters of our model	ogbn-prdouct	ogbn-proteins	ogbn-arxivSamPling_method	NeighborSamPling	Random Partition	Full-batchnum_layers	[3*, 4]	[3, 5, 7*, 9]	[3*, 4]hidden_size	[128*, 256]	[32, 64*,128]	[128*, 256]num_heads	[4*,2]	[6, 4*, 2]	[2*, 1]dropout	[0.3*]	[0, 0.1*,0.3]	[0.1, 0.3*]lr	[0.01, 0.001*]	[0.01, 0.001*]	[0.1, 0.001*]weight-decay	-	-	[0, 0.0005*]IabeLrate	[0.625*]	[0.375, 0.5*, 0.625]	[0.125, 0.25, 0.375, 0.5, 0.625*, 0.75, 0.875]C Margin Similarity functionGiven an attention weight αij from GAT or Graph Transformer, which can represent the connec-tion tightness between source node i and distance node j, we employ the Circle Loss (Sun et al.,2020) and make a slight change on it to build our Margin Similarity Function (MSF), measuringthe connection tightness between the neighbors nodes with same labels. For each center node i andits neighbors j, k ∈ N (i), we take the measurement task as a pair similarity problem in which thecenter node’s neighbors with same label are positive samples and the others are negative samples,calculating their connection tightness as following:1NMSF = N Elog ( 1 +	E	eαi,j - eαi,k
