Table 1: Averaged prediction errors and their standard errors (based on 5-fold validation).
Table 2: Prediction error ± standard error: YearPredictionMSD datasetMethods	d = 10	d=20	d=30	d=40SDRL	8.8 ± 0.1	9.2 ± 0.8	9.2 ± 0.8	8.8 ± 0.1SPCA	10.6 ± 0.1	10.4 ± 0.1	9.6 ± 0.1	10.2 ± 0.1PCA	10.6 ± 0.1	10.4 ± 0.1	10.3 ±0.1	10.2 ± 0.1OLS	———9.6 ±0.1———tive methods including convolutional networks (CN) and distance correlation autoencoder (dCorAE)(Wang et al., 2018). With CN, we use the feature extractor by dropping the cross entropy layer ofthe DenseNet trained for classification. The MNIST and FashionMNIST datasets consist of 60k and10k grayscale images with 28 × 28 pixels for training and testing, respectively, while the CIFAR-10dataset contains 50k and 10k colored images with 32 × 32 pixels for training and testing, respec-tively. For the learning from scratch strategy, the representer network Rθ has 20 layers for MNISTdata and 100 layers for CIFAR-10 data. We apply the transfer learning technique to the combinationof SDRL and CN on CIFAR-10 (Krizhevsky et al., 2009). The pretrained WideResnet-101 model(Zagoruyko & Komodakis, 2016) on the Imagenet dataset with Spinal FC (Kabir et al., 2020) isadopt for Rθ . The discriminator network Dφ is a 4-layer network. The the architecture of Rθ andmost hyperparameters are shared across all four methods - SDRL, CN, SDRL+CN and dCorAE.
Table 3: Classification accuracy for MNIST and FashionMNISTd		MNIST			FashionMNIST				SDRL	dCorAE	CN	SDRL	dCorAE	CNd=	16	99.41	99.58	99.39	94.44	94.18	94.21d=	32	99.61	99.54	99.45	94.18	93.89	94.41d=	64	99.56	99.53	99.49	94.13	94.24	94.38Table 4: Classification accuracy for CIFAR-10 datad		Learning from scratch			Transfer learning				SDRL	dCorAE	CN	SDRL	CN	SDRL+CNd=	16	94.29	94.15	94.21	97.52	97.44	97.68d=	32	94.58	94.18	94.92	97.33	97.79	97.96d=	64	94.46	94.66	95.09	97.49	97.90	97.917 Conclusion and future workIn this work, we formulate a framework for sufficient and disentangled representation learning andconstruct an objective function characterizing conditional independence and disentanglement. Thisenables us to learn a representation with the desired properties empirically. We provide statisti-cal guarantees for the learned representation by deriving an upper bound on the excess risk of theobjective function.
Table 4: Classification accuracy for CIFAR-10 datad		Learning from scratch			Transfer learning				SDRL	dCorAE	CN	SDRL	CN	SDRL+CNd=	16	94.29	94.15	94.21	97.52	97.44	97.68d=	32	94.58	94.18	94.92	97.33	97.79	97.96d=	64	94.46	94.66	95.09	97.49	97.90	97.917 Conclusion and future workIn this work, we formulate a framework for sufficient and disentangled representation learning andconstruct an objective function characterizing conditional independence and disentanglement. Thisenables us to learn a representation with the desired properties empirically. We provide statisti-cal guarantees for the learned representation by deriving an upper bound on the excess risk of theobjective function.
Table A1: Hyper-parameters for simulated examples, where s varies according to epochTask	λ	d	n	Ti	T2	s								0-150	151-225	226-500Regression	1.0	2 or 1	64	1	500	3.0	2.0	1.0Classification	1.0	2	64	1	500	2.0	1.5	1.0Table A2: MLP architectures for Dφ and Rθ in regressionLayers	Dφ		Rθ		Details	Output size	Details	Output sizeLayer 1	Linear, LeakyReLU	16	Linear, LeakyReLU	16Layer 2	Linear	1	Linear, LeakyReLU	8Layer 3			Linear	dTable A3: MLP architecture for Dφ of simulated classification examples and the benchmark clas-sification datasetsLayers	Details	Output sizeLayer 1	Linear, LeakyReLU	64Layer 2	Linear, LeakyReLU	128Layer 3	Linear, LeakyReLU	64Layer 4	Linear	1Table A4: DenseNet architecture for Rθ in the simulated classification examplesLayers	Details	Output size
Table A2: MLP architectures for Dφ and Rθ in regressionLayers	Dφ		Rθ		Details	Output size	Details	Output sizeLayer 1	Linear, LeakyReLU	16	Linear, LeakyReLU	16Layer 2	Linear	1	Linear, LeakyReLU	8Layer 3			Linear	dTable A3: MLP architecture for Dφ of simulated classification examples and the benchmark clas-sification datasetsLayers	Details	Output sizeLayer 1	Linear, LeakyReLU	64Layer 2	Linear, LeakyReLU	128Layer 3	Linear, LeakyReLU	64Layer 4	Linear	1Table A4: DenseNet architecture for Rθ in the simulated classification examplesLayers	Details	Output sizeConvolution	3 X 3 Conv	24 × 20 × 20	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 1	36 × 20 × 20Transition Layer 1	BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv	30 × 10 × 10	BN, 1 × 1 Conv	
Table A3: MLP architecture for Dφ of simulated classification examples and the benchmark clas-sification datasetsLayers	Details	Output sizeLayer 1	Linear, LeakyReLU	64Layer 2	Linear, LeakyReLU	128Layer 3	Linear, LeakyReLU	64Layer 4	Linear	1Table A4: DenseNet architecture for Rθ in the simulated classification examplesLayers	Details	Output sizeConvolution	3 X 3 Conv	24 × 20 × 20	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 1	36 × 20 × 20Transition Layer 1	BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv	30 × 10 × 10	BN, 1 × 1 Conv	Dense Block 2	BN, 3 × 3 Conv ×	18× 10× 10Transition Layer 2	BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv	15 × 5 × 5	BN, 1 × 1 Conv	Dense Block 3	BN, 3 × 3 Conv × 1	27 × 5 × 5Pooling	BN, ReLU, 5 × 5 Average Pool, Reshape	27Fully connected	Linear	2
Table A4: DenseNet architecture for Rθ in the simulated classification examplesLayers	Details	Output sizeConvolution	3 X 3 Conv	24 × 20 × 20	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 1	36 × 20 × 20Transition Layer 1	BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv	30 × 10 × 10	BN, 1 × 1 Conv	Dense Block 2	BN, 3 × 3 Conv ×	18× 10× 10Transition Layer 2	BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv	15 × 5 × 5	BN, 1 × 1 Conv	Dense Block 3	BN, 3 × 3 Conv × 1	27 × 5 × 5Pooling	BN, ReLU, 5 × 5 Average Pool, Reshape	27Fully connected	Linear	214Under review as a conference paper at ICLR 2021A.2 Real datasetsRegression: In the regression problems, hyper-parameters are presented in Table A5. The Adamoptimizer with an initial learning rate of 0.001 and weight decay of 0.0001 is adopted. The MLParchitectures of Dφ and Rθ for the YearPredictionMSD data are shown in Table A6.
Table A5: Hyper-parameters for YearPredictionMSD dataDataset	λ	d	n	T1	T2	sYearPredictionMSD	1.0	10, 20, 30, 40	64	1	500	1.0Table A6: MLP architectures for Dφ and Rθ for YearPredictionMSD dataLayers	Dφ		Rθ		Details	Output size	Details	Output sizeLayer 1	Linear, LeakyReLU	32	Linear, LeakyReLU	32Layer 2	Linear, LeakyReLU	8	Linear, LeakyReLU	8Layer 3	Linear	1	Linear	dClassification: For the classification problems, hyper-parameters are shown in Table A7. We againuse Adam as the SGD optimizers for both Dφ and Rθ. Specifically, learning rate of 0.001 and weightdecay of 0.0001 are used for Dφ in all datasets and for Rθ on MNIST (LeCun et al., 2010). Wecustomized the SGD optimizers with momentum at 0.9, weight decay at 0.0001, and learning rate ρin Table A8 for FashionMNIST (Xiao et al., 2017) and CIFAR-10 (Krizhevsky et al., 2012). For thetransfer learning of CIFAR-10, we use customized SGD optimizer with initial learning rate of 0.001and momentum of 0.9 for Rθ . MLP architectures of the discriminator network Dφ for MNIST,FashionMNIST and CIFAR-10 are given in Table A3. The 20-layer DenseNet networks shown inTable A9 were utlized for Rθ on the MNIST dataset, while the 100-layer DenseNet networks shownin Table A10 and A11 are fitted for Rθ on FashionMNIST and CIFAR-10.
Table A6: MLP architectures for Dφ and Rθ for YearPredictionMSD dataLayers	Dφ		Rθ		Details	Output size	Details	Output sizeLayer 1	Linear, LeakyReLU	32	Linear, LeakyReLU	32Layer 2	Linear, LeakyReLU	8	Linear, LeakyReLU	8Layer 3	Linear	1	Linear	dClassification: For the classification problems, hyper-parameters are shown in Table A7. We againuse Adam as the SGD optimizers for both Dφ and Rθ. Specifically, learning rate of 0.001 and weightdecay of 0.0001 are used for Dφ in all datasets and for Rθ on MNIST (LeCun et al., 2010). Wecustomized the SGD optimizers with momentum at 0.9, weight decay at 0.0001, and learning rate ρin Table A8 for FashionMNIST (Xiao et al., 2017) and CIFAR-10 (Krizhevsky et al., 2012). For thetransfer learning of CIFAR-10, we use customized SGD optimizer with initial learning rate of 0.001and momentum of 0.9 for Rθ . MLP architectures of the discriminator network Dφ for MNIST,FashionMNIST and CIFAR-10 are given in Table A3. The 20-layer DenseNet networks shown inTable A9 were utlized for Rθ on the MNIST dataset, while the 100-layer DenseNet networks shownin Table A10 and A11 are fitted for Rθ on FashionMNIST and CIFAR-10.
Table A7: Hyper-parameters for the classification benchmark datasetsDataset	λ	d	n	T1	T2	sMNIST	1.0	16, 32, 64	64	1	300	0.1FashionMNIST	1.0	16, 32, 64	64	1	300	1.0CIFAR-10	1.0	16, 32, 64	64	1	300	1.0CIFAR-10 (transfer learning)	0.01	16, 32, 64	64	1	50	1.0B	Appendix: ProofsIn this appendix, we prove Lemmas 2.1 and 4.1, and Theorems 4.2 and 4.3.
Table A8: Learning rate ρ varies during training.
Table A9: Architecture for MNIST, reduced feature size is dLayers	Details	Output sizeConvolution	3 X 3 Conv	24 × 28 × 28	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 2	48 × 28 × 28Transition Layer 1	BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv	24× 14 × 14	BN, 1 × 1 Conv	Dense Block 2	BN, 3 × 3 Conv × 2	48 × 14 × 14Transition Layer 2	BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv	24× 7× 7	BN, 1 × 1 Conv	Dense Block 3	BN, 3 × 3 Conv × 2	48 × 7 × 7Pooling	BN, ReLU, 7 × 7 Average Pool, Reshape	48Fully connected	Linear	dTable A10: Architecture for FashionMNIST, reduced feature size is dLayers	Details	Output sizeConvolution	3 × 3 Conv	24 × 28 × 28	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 16	216×28× 28Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv		108× 14× 14	BN, 1 × 1 Conv	
Table A10: Architecture for FashionMNIST, reduced feature size is dLayers	Details	Output sizeConvolution	3 × 3 Conv	24 × 28 × 28	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 16	216×28× 28Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv		108× 14× 14	BN, 1 × 1 Conv	Dense Block 2	BN, 3 × 3 Conv × 16	300 × 14 × 14Transition Layer 2 BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv		150 × 7× 7	BN, 1 × 1 Conv	Dense Block 3	BN, 3 × 3 Conv × 16	342 × 7 × 7Pooling	BN, ReLU, 7 × 7 Average Pool, Reshape		342Fully connected	Linear	dTable A11: Architecture for CIFAR-10, reduced feature size is dLayers	Details	Output sizeConvolution	3 × 3 Conv	24 × 32 × 32	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 16	216 × 32 × 32Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv		108× 16× 16	BN, 1 × 1 Conv	
Table A11: Architecture for CIFAR-10, reduced feature size is dLayers	Details	Output sizeConvolution	3 × 3 Conv	24 × 32 × 32	BN, 1 × 1 Conv	Dense Block 1	BN, 3 × 3 Conv × 16	216 × 32 × 32Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv		108× 16× 16	BN, 1 × 1 Conv	Dense Block 2	BN, 3 × 3 Conv × 16	300× 16× 16Transition Layer 2 BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv		150×8×8	BN, 1 × 1 Conv	Dense Block 3	BN, 3 × 3 Conv × 16	342 × 8 × 8Pooling	BN, ReLU, 8 × 8 Average Pool, Reshape		342Fully connected	Linear	d16Under review as a conference paper at ICLR 2021B.2	Proof of Lemma 4.1Proof. Our proof follows KezioU (2003). Since f(t) is convex, then ∀t ∈ R, we have f (t) = f **(t),wheref**(t)=sup{st- f*(s)}s∈R
