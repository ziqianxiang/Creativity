Table 1: Evaluation results comparing BC, offline SAC, BCQ, BEAR, BRAC-v, CQL(H) and our AdaPT-ACon four datasets. All results are reported as the average undiscounted return of the learned policy over threeindependent trials with random seeds. Here we omit BRAC with policy regularization (BRAC-p) (Wu et al.,2019) because BRAC-v generally obtains higher performance than BRAC-p. The results of offline SAC aregiven in Appendix E due to the limitation of space.
Table 2: The performance of the partially trained policies and corresponding perfor-mance thresholds for each task.
Table 3: The correction coefficient and maximum target divergence used on eachtask for our AdaPT-AC algorithm.
Table 4: Evaluation results of offline SAC on four datasets. All results arereported as the average undiscounted return of the learned policy over threeindependent trials with random seeds.
