Table 1: Comparison with existing question generation methods on the test set of SQuAD Split1 andSplit2. Models marked as ‘*’ indicate results we reproduced.
Table 2: Application of ASGen to other existingquestion generation models. BL-4, MTR, RG-Lindicate BLEU-4, METEOR, ROUGE-L.
Table 3: Comparison with existing questiongeneration methods on the test set of MSMARCO and NewsQA. (L) indicate (Large).
Table 5: Average of 10 human evaluationscores over 50 randomly picked samples fromSQuAD. Each column indicates Syntax (ST),Semantics (SM), Context-Relevance (CR) andAnswer-Relevance (AR) in the range 1 to 5.
Table 4: Ablation of pre-training methods, i.e.,pre-training on NS, ASGen, and ASGen withoutconditioning on a given answer (w/o A), on thetest set of SQuAD splits. “Wiki” indicates the sen-tence generation score on Test-Wiki.
Table 6: Mean absolute error (MAE)between prediction Kpred and ground-truth Kgt on the test set of SQuADApproachThresholding on LogitsFixed-K (Kpred = 5)DynamiC-K (ASGen)MAESplit1 Split22.31	1.121.92	0.991.24	0.764.2	Downstream MRC Task PerformanceTo show the effeCtiveness of the generated synthetiC data, we train MRC models on generated data,before fine-tuning on the downstream data. As shown in Table 7, the synthetiC data generated by‘BertGen (Large) + ASGen’ Consistently improves the performanCe of BERT (Large, WWM) bya signifiCant margin. Pre-training BERT on synthetiC data improves F1 sCores by 1.8 on SQuAD-v1.1 and 5.6 on SQuAD-v2.0 for BERT (Large), and 0.7 on SQuAD-v1.1 and 2.5 on SQuAD-v2.0for BERT (WWM). SynthetiC data also improves BERT+CLKT performanCe on KorQuAD. Also,to show improvement due to our pre-training method in the downstream MRC task, we Compare
Table 7: Comparison of downstream MRCtask EM/F1 scores after pre-training on thegenerated synthetic data (syn data). Thescores are obtained from the dev set ofSQuAD-v1.1 and SQuAD-v2.0, and the devset and the test set of KorQuAD (KQD).
Table 8: Comparison of downstream MRC task EM/F1 scores using the synthetic data from differentpre-training methods. The scores are obtained from SQuAD-v1.1 and SQuAD-v2.0 dev set.
Table 9: Examples of questions generated on SQUAD-V1.1 development set. We compare generatedquestions from 'BertGen + ASGen' with 'BertGen + NS’. Colored Text indicates given answers.
Table 10: Manual categorization of the reasoningtype for 150 randomly sampled answerable ques-tions generated questions on Wikipedia. Note thateach example can be assigned to multiple types.
Table 11: Manual categorization of the rea-soning type for unanswerable questions.
Table 12: Additional experiments on the effectiveness of ASGen on the test set of SQuAD Split3.
Table 13: Ablation study of applying ASGen to question generation model on Natural Ques-tions (Kwiatkowski et al., 2019) short answer dataset. The scores are obtained from the dev set.
Table 14: Ablation study of applying our method to ELECTRA (Clark et al., 2020) on SQuAD-v2.0dev set after pre-training on the generated synthetic data using ASGen with Small-Wiki.
Table 15: EM/F1 scores of the BERT (Large) fine-tuned on QUASAR-T dataset. The used syntheticdata is generated from ASGen trained on SQuAD-v1.1 (Full-Wiki).
Table 16: Comparison of predicting K answers with downstream BERT (Large) MRC results onSQuAD-v2.0 dev set after pre-training on each generated synthetic data using corresponding answergeneration approach with Small-Wiki.
Table 17: The performance of our method on limited-data domain (BioASQ). Note that the scoresof question generation are obtained from BioASQ factoid-type 6b. All experiments were conductedusing the official source code of Yoon et al. (2020).
Table 18: Application of ASGen to T5 Model with Limited Pre-Training dataTest set on Split1	BLEU-4	METEOR	ROUGE-LT5 (Small)	-15.6^^	23.3	37.1T5 (Small) + ASGen (Small-Wiki)	16.5	24.0	38.4Test set on Split2	BLEU-4	METEOR	ROUGE-LT5 (Small)	-18.8^^	25.2	40.5T5 (Small) + ASGen (Small-Wiki)	19.2	25.9	41.3Table 19: Central tendency and variation for human evaluation scores. ± is 95% confidence interval.
Table 19: Central tendency and variation for human evaluation scores. ± is 95% confidence interval.
Table 20: Central tendency and variation for the score of our approach, BertGen(Large) + ASGen,on downstream SQuAD-v1.1 and v2.0 dataset. ± is standard deviation.
Table 21: Distribution over the number of answers in SQuAD-v1.1 dataset.
