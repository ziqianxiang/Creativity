Table 1: Comparisons of different methods on CIFAR-100. “W” and “A” represent the averagequantization bitwidth of the weights and activations, respectively.
Table 2: Comparisons on ImageNet. “*” denotes that we get the results from the figures in (vanBaalen et al., 2020) and “-" denotes that the results are not reported. Moreover, “W” and “A”represent the average quantization bitwidth of the weights and activations, respectively.
Table 3: Effect of the bit-sharing scheme. We report the testing accuracy, BOPs, and search costs onCIFAR-100. The search costs are measured on a GPU device (NVIDIA TITAN Xp).
Table 4: Effect of the one-stage compression. We report the results of ResNet-56 on CIFAR-100.
Table 5: Effect of the alternative training scheme. We report the results of ResNet-56 on CIFAR-100.
Table 6: Resource-constrained compression on BitFusion. We evaluate the proposed ABS under thelatency- and energy-constrained and report the Top-1 and Top-5 accuracy on ImageNet.
Table 7: Comparisons of the search costs on CIFAR-100. The search costs are measured on a GPUdevice (NVIDIA TITAN Xp).
Table 8: Comparisons of different methods w.r.t. memory footprints. We compress ResNet-56using different methods and report the results on CIFAR-100.
Table 9: Comparisons of different methods with MobileNetV3 on CIFAR-100.
