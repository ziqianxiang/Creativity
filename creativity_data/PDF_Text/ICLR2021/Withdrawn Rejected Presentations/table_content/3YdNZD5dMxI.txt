Table 1: FID of the synthesized samples (lower is better), averaged over 5 random sets of samples.
Table 2: FID of the synthesized samples when conditioned on the ground truth labels. For SB-GAN, we train the entire model end-to- end and extract the trained SPADE.			Table 3: Average perceptual evaluation scores when each evaluators has selected a quality score in the range of 1 (terrible quality) to 4 (high quality) for each image.			SPADE	SB-GAN	ProGAN	BigGAN	SB-GANCityscapes-5k	72.12	60.39	2.08	-	2.48Cityscapes-25k	60.83	54.13	2.53	2.27	2.61ADE-Indoor	50.30	48.15	2.35	1.96	2.494.2	Quantitative evaluationTo provide a thorough empirical evaluation of the proposed approach, we generate samples foreach dataset and report the FID scores of the resulting images (averaged across 5 sets of generatedsamples). We evaluate SB-GAN both before and after end-to-end fine-tuning, and compare ourmethod to two strong baselines, ProGAN (Karras et al., 2017) and BigGAN (Brock et al., 2019).
Table 4: Ablation study of various components of SB-GAN. We report FID scores of SB-GANbefore fine-tuning, fine-tuning only the semantic bottleneck synthesis component, fine-tuning onlythe image synthesis component, and full end-to-end fine-tuning. Experiments are performed on theCityscapes-5K dataset at a resolution of 128 Ã— 256.
