Table 1: Transfer learning test set results on SentEval for English models. Baseline models includeBERT-based (BERT and SBERT) and non-BERT models (SkipThought, InferSent and USE).
Table 2: Performance (accuracy) of multilingual models trained with monolingual data on XEVAL.
Table 3: Performance (accuracy) of models trained with cross lingual data on XEVAL. mUSEonly supports 16 languages, Underline indicates the language is not supported by mUSE. We testwith multiple strategies for multitask pretraining: [S1]: CMLM → BR; [S2]: CMLM+BR; [S3]:CMLM → CMLM+BR. [f-mBERT] denotes finetuning mBERT with CMLM and BR.
Table 4: Classification accuracy on the Amazon Reviews dataset. The experiments examine the zero-shot cross-lingual ability of multilingual models. We explore both freezing/updating the weights ofthe multilingual encoder during training on English data.
Table 5: Tatoeba results (retrieval accuracy) for each language. Our model CMLM+BR achieves thebest results on 30 out of 36 languages.
Table 6: Average retrieval accuracy on 36 languages of multilingual representations model with andwithout principal component removal (PCR) on Tatoeba dataset.
Table 7: Ablation study of CMLM designs, including the number of projection spaces, architectureand sentence representations. The experiments are conducted on SentEval.
