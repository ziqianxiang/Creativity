Table 1: Checkpoint ranking results on Group I, the checkpoints of mixed supervision (GFLOPSexcludes a forward Pass on training data, Which takes 3.04e5 GFLOPS Shared by all methods)Method	ReCall@1	Rel@1	ReCall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-000-	96.97	-25.00-	98.79	23.56	18.44	4.95E4Linear (5 epoch)	25.00	98.79	50.00	98.94	49.77	32.33	4.97E4Linear (converged)	50.00	99.63	75.00	99.65	68.97	53.43	5.33E4Fine-tune (1 epoch)	25.00	97.45	25.00	97.66	30.25	22.15	6.51E5Fine-tune (5 epoch)	0.00	91.09	25.00	98.61	48.19	36.78	4.28E6MI (α=0.01)	0.00	64.67	0.00	87.96	2.39	-0.31	1.62E5(Poole et al., 2019) MI (α=0.50)	0.00	66.71	25.00	90.31	-4.91	-13.05	1.62E5MI w/ PCA (α=0.01)	0.00	89.45	50.00	99.27	16.16	20.67	5.58E4MI w/ PCA (α=0.50)	0.00	86.49	25.00	94.28	-24.72	-16.06	5.58E4LEEP (Nguyen et al., 2020)	一	—	一	—	一	一	一N LEEP	75.00	99.65	75.00	99.65	84.30	76.00	12.85Table 2: Checkpoint ranking results on Group II, the checkpoints at different pre-training stages(GFLOPS excludes a forward pass on training data, which takes 3.04E5 GFLOpS shared by all)Method	ReCall@1	Rel@1	Recall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-0:00-	96.46	-25.00-	98.79	27.01	24.24	4.95E4Linear (5 epochs)	50.00	99.57	100.00	100.00	55.07	51.28	4.97E4Linear (converged)	75.00	99.95	100.00	100.00	79.30	68.60	5.33E4
Table 2: Checkpoint ranking results on Group II, the checkpoints at different pre-training stages(GFLOPS excludes a forward pass on training data, which takes 3.04E5 GFLOpS shared by all)Method	ReCall@1	Rel@1	Recall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-0:00-	96.46	-25.00-	98.79	27.01	24.24	4.95E4Linear (5 epochs)	50.00	99.57	100.00	100.00	55.07	51.28	4.97E4Linear (converged)	75.00	99.95	100.00	100.00	79.30	68.60	5.33E4Fine-tune (1 epoch)	25.00	99.05	25.00	99.47	19.61	15.52	6.51E5Fine-tune (5 epochs)	25.00	99.55	100.00	100.00	68.47	58.33	4.28E6MI (α=0.01) (Poole et al., 2019)	0.00	94.84	25.00	97.43	-29.41	-17.81	1.62E5MI (α=0.50)	0.00	96.66	0.00	97.03	-11.36	-10.21	1.62E5MI w/ PCA (α=0.01)	50.00	99.60	75.00	99.85	52.14	51.34	5.58E4MI w/ PCA (α=0.50)	0.00	96.68	50.00	99.52	23.73	17.09	5.58E4LEEP (Nguyen et al., 2020)	75.00	99.44	75.00	99.90	50.36	55.49	378.31N LEEP	100.00	100.00	100.00	100.00	72.84	67.49	12.95Table 3: CheckPoint ranking results on GrouP III, the checkPoints of heterogeneous architectures(GFLOPS excludes a forward pass on training data, which takes 2.73E5 GFlOpS shared by all)Method	ReCall@1	Rel@1	Recall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-25.00-	98.17	-25.00-	99.35	30.14	13.80	3.37E4Linear (5 epoch)	25.00	98.98	25.00	99.63	33.45	18.95	3.38E4Linear (converged)	25.00	99.66	25.00	99.72	63.55	36.91	3.62E4
Table 3: CheckPoint ranking results on GrouP III, the checkPoints of heterogeneous architectures(GFLOPS excludes a forward pass on training data, which takes 2.73E5 GFlOpS shared by all)Method	ReCall@1	Rel@1	Recall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-25.00-	98.17	-25.00-	99.35	30.14	13.80	3.37E4Linear (5 epoch)	25.00	98.98	25.00	99.63	33.45	18.95	3.38E4Linear (converged)	25.00	99.66	25.00	99.72	63.55	36.91	3.62E4Fine-tune (1 epoch)	0.00	98.28	25.00	99.80	17.61	11.59	4.43E5Fine-tune (5 epoch)	25.00	98.62	25.00	99.68	25.72	15.72	2.91E6MI (α=0.01) (Poole et al., 2019)	25.00	98.29	25.00	99.34	4.42	2.94	1.30E5MI (α=0.50)	25.00	98.36	25.00	99.37	-9.79	-6.81	1.30E5MI w/ PCA (α=0.01)	0.00	99.18	50.00	99.82	61.94	38.83	5.56E4MI w/ PCA (α=0.50)	0.00	96.34	0.00	98.47	33.17	21.26	5.56E4LEEP (Nguyen et al., 2020)	25.00	97.36	75.00	99.90	42.99	45.06	247.56N LEEP	25.00	99.66	25.00	99.70	66.94	51.14	12.68If we train the linear classifiers till convergence, they become the second best checkpoint rank-ing method in Groups I and II. Note that the linear classifiers’ accuracies, i.e., the ranking scores,imply the linear separability of the features extracted by the checkpoints. Recall that the mutual in-formation with PCA feature dimension reduction is among the second best in Group III. Since bothmethods measure the feature representations’ quality by the downstream tasks’ labels, we conjecturethat the quality of the features is a strong indicator of the checkpoints’ final fine-tuning performance
Table 5: Comparison results on all checkpoints in Group I, II, III (GFLOPS excludes a forward passon training data, Which takes 2.73E5 GFLOPS Shared by all).__________________________________Method	ReCall@1	Rel@1	ReCall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-000-	99.13	-25.00-	99.46	22.30	13.42	4.45E4Linear (5 epochs)	0.00	99.13	25.00	99.21	42.99	31.64	4.47E4Linear (converged)	25.00	99.42	50.00	99.73	76.22	61.22	4.79E4Fine-tune (1 epoch)	0.00	96.69	0.00	98.16	3.84	6.50	5.85E5Fine-tune (5 epochs)	0.00	99.49	0.00	99.49	27.20	27.16	3.84E6MI (α=0.01)	0.00	77.50	0.00	81.44	1.12	7.16	1.52E5(Poole et al., 2019) MI (α=0.50)	0.00	66.51	0.00	90.07	-4.05	-14.22	1.52E5MI w/ PCA (α=0.01)	0.00	89.18	50.00	99.84	12.14	20.99	5.57E4MI w/ PCA (α=0.50)	0.00	97.07	0.00	98.70	-14.03	-2.39	5.57E4LEEP (Nguyen et al., 2020)	一	—	一	—	一	一	一N LEEP	50.00	99.47	50.00	99.78	83.71	68.18	12.8613Under review as a conference paper at ICLR 2021A.3 Comparison results on all checkpoints in Groups I, II, IIITo obtain a comprehensive analysis, we also consolidate the checkpoints from Group I, II and IIIinto one group (including 41 checkpoints in total) and then apply the ranking methods on it. Table 5shows the comparison results. The results further evaluate our observations in Section 4 of the main
Table 6: Comparison results on Group IV (GFLOPS excludes a forward pass on training data, whichtakes 6.27E5 GFLOPS Shared by all).____________________________________________________Method	Recall@1	Rel@1	Recall@3	Rel@3	Pearson	Kendall	GFLOPSLinear (1 epoch)	-0.00-	98.58	-25.00-	98.99	46.75	27.27	1.021E5Linear (5 epochs)	0.00	98.72	75.00	99.95	59.27	41.32	1.023E5Linear (converged)	25.00	99.81	75.00	99.95	82.17	73.48	1.06E5Fine-tune (1 epoch)	0.00	96.19	25.00	99.34	29.64	21.21	1.34E6Fine-tune (5 epochs)	75.00	99.98	75.00	99.94	69.19	50.00	8.81E6MI (α=0.01) (Poole et al., 2019)	0.00	97.25	75.00	98.46	12.96	13.21	1.62E5MI (α=0.50)	25.00	98.60	50.00	99.54	30.16	18.21	1.62E5MI w/ PCA (α=0.01)	0.00	99.85	75.00	99.95	51.85	48.91	5.58E4MI w/ PCA (α=0.50)	0.00	95.99	50.00	98.41	48.64	44.31	5.58E4LEEP (Nguyen et al., 2020)	25.00	99.52	75.00	99.72	54.54	46.43	378.31N LEEP	75.00	99.98	100.00	100.00	83.22	73.80	12.95A.4 Group IV: Supervised ResNet101sWe incorporate another group of checkpoints, including 12 ResNet101 (He et al., 2016) modelspre-trained by fully supervised learning on ImageNet (Deng et al., 2009), iNaturalist (Van Hornet al., 2018), and Places-365 (Zhou et al., 2017). We obtain the checkpoints in the same way as wehave done for Group II, but with ResNet101 architecture. We want to study how different modelarchitecture and model size affect the ranking quality.
Table 7: Absolute fine-tuning accuracy on Group I.
Table 8: Absolute fine-tuning accuracy on Group II.
Table 9: Absolute fine-tuning accuracy on Group III.
Table 10: Absolute fine-tuning accuracy on Group IV.
