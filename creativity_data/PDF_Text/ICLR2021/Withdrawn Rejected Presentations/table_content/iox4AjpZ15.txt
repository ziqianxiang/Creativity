Table 1: Comparison in representation and invertible quality on MNIST datasetsDataset	Algorithm	RMSE	MNE	Trust	Cont	Kmin	Kmax	l-MSE	Acc	MLLE	-	-	0.6709	0.6573	1.873	6.7e+9	36.80	0.8341	t-SNE	-	-	0.9896	0.9886	5.156	324.9	48.07	0.9246	ML-Enc	-	-	0.9862	0.9927	1.761	58.91	18.98	0.9326	VAE	0.5263	33.17	0.9712	0.9703	5.837	130.5	22.79	0.8652MNIST	TopoAE	0.5178	31.45	0.9915	0.9878	4.943	265.3	24.98	0.8993	ML-AE	0.4012	16.84	0.9893	0.9926	1.704	57.48	19.05	0.9340	i-ML-Enc (L)	0.0457	0.5085	0.9906	0.9912	2.033	60.14	18.16	0.9316	INN	0.0615	0.5384	0.9851	0.9823	1.875	22.38	7.494	0.9176	i-RevNet	0.0443	0.4679	0.9118	0.8785	13.41	142.5	6.958	0.9901	i-ResNet	0.0502	0.6422	0.9149	0.8922	1.876	19.28	10.78	0.9925	i-ML-Enc(L-1)	0.0407	0.5085	0.9986	0.9973	1.256	5.201	5.895	0.95806Under review as a conference paper at ICLR 2021(C) Half SPere (PCA) --()-*+,, )	--()-*+,, )<	()-*+,⑹ MNIST (PCA)	--()-*+,, )<	()-*+,	A-BC*(e) COIL-20 (PCA)	--()-*+,, )4	()-*+,	A-SCEFigure 4: Visualization of invertible NLDR results of i-ML-Enc compared to ML-Enc and t-SNE.
Table 2: Brief introduction to the configuration of datasets for method comparison.
Table 3: Comparison in embedding and invertible quality on Swiss roll and Half-spheres datasets.
Table 4: Comparison in embedding and invertible quality on USPS, FMNIST, and COIL-20 datasets.
Table 5: Comparison of manifold learning difficulties of interpolation datasetsDataset	Class	Train set	Dimension	Entropy	Texture	KNN	LogisticUSPS	10	9298	^^56	5.479	0.5097	0.9589	0.9381MNIST(256)	10	9298	256	1.879	10.51	0.9493	0.9099MNIST(784)	10	20000	784	1.598	39.75	0.9515	0.8943KMNIST	10	20000	784	2.911	33.01	0.9141	0.6471FMNIST	10	20000	784	4.115	24.75	0.8133	0.798416Under review as a conference paper at ICLR 2021K≤5K≥20K≤10K≥25K≤5K≥20K≤10K≥25Figure 9: Visualization of kNN interpolation results of i-ML-Enc on image datasets with k ≤ 10 andk ≥ 20. For each row, the upper part shows results of i-ML-Enc while the lower part shows the rawinput images. Both the input and latent results transform smoothly when k is small, while the latent
Table 6: Ablation study of the proposed loss terms in i-ML-Enc on five image datasets.
