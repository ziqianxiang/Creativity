Table 1: Evaluation on the basic dataset and the simple dataset					Method	MSE	PSNR	SSIMEvaluation on basic dataset	BPVAE	0.017	18.250	0.544	VAE	0.016	18.282	0.543Evaluation on simple dataset	BPVAE VAE	0.007 0.0346	22.392 14.831	0.909 0.6014 RESULTS4.1	Does BPVAE know what it doesn’ t know?To investigate whether VAEs have a good understanding of the distribution of training data, we carryout reconstruction experiments under multiple conditions. Despite the variety of choices for settingsof reconstruction experiment, here we take the following setting as an example: CIFAR10 as the basicdataset for training and MNIST as the simple dataset. After training VAEs and BPVAEs separately,we generated reconstructed images during the inference process. As shown in Figure 4, we visualizedthe results of standard VAEs and our proposed BPVAEs in comparison. It is evident that BPVAEsobtain much better performance than standard VAEs on MNIST, while these two models achievecomparable results on CIFAR10. The great performance of BPVAEs on MNIST can be attributed tothe effective capacity of the extra introduced s-priors, which can assist BPVAEs of capturing externalfeature representation for the data from simple dataset, in which case VAEs failed due to lack ofvarious latent priors with strong capacity.
Table 2: AUROC and AUPRC for detecting OOD inputs using likelihoods of BPVAE, likelihood ofVAE, and other baselines on FashionMNIST vs. MNIST datasets.___________________Model	AUROC	AUPRCBPVAE(OUrS)	1.000	1.000Standard VAE	0.012	0.113Likelihood Ratio(μ, λ) Ren et al. (2019)	0.994	0.993ODIN Liang et al. (2018)	0.752	0.763Mahalanobis distance Lee et al. (2018)	0.942	0.928Ensemble, 20 classifiers Lakshminarayanan et al. (2017)	0.857	0.849WAIC,5 models Choi et al. (2018)	0.221	0.4010.070.060.050.040.030.020.010.00——CIFARlO(Train)——CIFARlO(Test)
Table 3: AUROC and AUPRC for detecting OOD inputs using likelihoods of BPVAE and VAE, andother baselines on CIFAR10 vs. SVHN datasets._______________________________________________Model	AUROC	AUPRCBPVAE(OurS)	1.000	1.000Standard VAE	0.037	0.214Likelihood Ratio(μ, λ) Ren et al. (2019)	0.930	0.881ODIN Liang et al. (2018)	0.938	0.926Mahalanobis distance Lee et al. (2018)	0.728	0.711Ensemble, 20 classifiers Lakshminarayanan et al. (2017)	0.946	0.916WAIC,5 models Choi et al. (2018)	0.146	0.363——CIFARlO(Train)——CIFARlO(Test)—FashionMNIST(Test)——GTSRB(Test)——IMAGENET(Test)——KMNiSTCTest)MNIST(Test)——OMNIGLOT(Test)SVHN(Test)0.06
