Table 1: Increase in throughput for ImageNet training with chunked local updates vs pipelined back-prop. Backprop batch size a × b, where a is the microbatch size and b is the number of microbatchesover which gradients are accumulated.
Table 2: Total data communicated between IPUs, for a single step of ResNet34 training on Ima-geNet, over 4 IPUs. Results are for batch size 32 × 8 for pipelined backprop and 32 for chunkedlocal parallelism. All measurements in MB.
Table 3: Memory consumption in MB, for ResNets training on ImageNet with pipelined backpropand local parallelism. “Average” denotes the mean memory over IPUs, “Max” is the value forthe IPU which consumed most memory. “Recomputation” refers to activation recomputation. Forpipelined backprop, the number of microbatches over which gradients were accumulated was in allcases 2 × the number of IPUs.
