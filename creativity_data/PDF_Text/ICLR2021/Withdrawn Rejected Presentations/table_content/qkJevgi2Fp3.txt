Table 1: Complexity per layer and minimum number of sequential operations of various architec-tures. n is the sequence length, dx is the input dimension, and d is the order. First three rows are asreported in Vaswani et al. (2017).
Table 2: IMDB test accuracy. The first four rows are from Gu et al. (2020), where all the models arereported to use 256 hidden units.
Table 3: QQP test accuracies. Split1 and split2 LSTM scores are as reported in Shen et al. (2018)and Sharma et al. (2019), respectively. The number of parameters for split2 (800k) is not reportedbut is a conservative estimate.
Table 4: Comparison of MRPC results. First six rows are from Wang et al. (2018). We reportaccuracy/F1 as in that work as well.
Table 5: Comparison of results for the SNLI dataset. First row is from Bowman et al. (2015).
Table 6: Comparison of psMNIST results. All but last two rows as reported in Voelker et al. (2019).
Table 7: Training time comparison (in seconds). We use models containing 102k parameters (sameas the original LMU model), set the batch size to 100, and use a single GTX 1080 to obtain thefollowing numbers. By “Total Time” we refer to the time it takes for the training accuracy to cross95%.
