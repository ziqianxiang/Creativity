Table 1: Dataset summary.
Table 2: F1 score comparison of models for sequence labeling on different datasets. All models (ex-cept CVT and SeqVAT) use the same BERT encoder. F1 score of our model for each task is followedby standard deviation and percentage improvement (↑) over BERT with few-shot supervision.
Table 3: F1 score comparison of models for sequence labeling on multilingual datasets using thesame BERT-Multilingual-Base encoder. F1 score of MetaST for each task is followed by standarddeviation in parentheses and percentage improvement (↑) over BERT with few-shot supervision.
Table 4: F1 scores of different models with 200 labeled samples for each task. The percentageimprovement (↑) is over the BERT model with few-shot supervision.
Table 5: Variation in model performance on varying K labels / slot on SNIPS dataset with 39 slots.
Table 6: Ablation analysis of our framework MetaST with10 labeled examples per slot on SNIPS and CoNLL03 (EN).
Table 7: Varying proportion of unlabeled datafor MetaST with 10 labels per slot.
Table 8: Email Dataset.
Table 9: Wikiann (En) Dataset.
Table 10: MIT Restaurant Dataset.
Table 11: CoNLL2003 (EN)Method	Shots (4 Slot Types)	-5	10	20	100-Full-supervision	BERT	87.67Few-shot Supervision	BERT	64.80	70.77	73.89	80.61Few-shot Supervision + unlabeled dataMethod	Shots (3 Slot Types × 41 languages)	-5	10	20	100-Full-supervision	BERT	87.17Few-shot Supervision	BERT	77.68	79.67	82.33	85.70Few-shot Supervision + unlabeled dataMean Teacher	64.55	68.34	73.87	79.21VAT	64.97	67.63	74.26	80.70Classic ST	67.95	72.69	73.79	81.82BOND	69.42	72.79	76.02	80.62MetaST	73.34	76.65	77.01	82.11
Table 12: Multilingual CoNLL03.
Table 13: Multilingual Wikiann15Under review as a conference paper at ICLR 2021A.4 Implementations and Hyper-parameterWe do not perform any hyper-parameter tuning for different datasets. The batch size and maxi-mum sequence length varies due to data characteristics and are as shown in Tbale 14. The hyper-parameters are as shown in Table 14.
Table 14: Batch size, sequence length and BERT encoder choices across datasetsBERT attention dropout	0.3BERT hidden dropout	0.3Latest Iteration R in labeled data acquisition	5BERT output hidden size h	768Steps for fine-tuning teacher model on labeled data	2000Steps T for self-training model on unlabeled data	3000Mini-batch S	5Re-initialize Student	YPseudo-label Type	HardWarmup steps	20learning rate α	5e-5Weight-decay	5e-6Table 15: Hyper-parameters.
Table 15: Hyper-parameters.
