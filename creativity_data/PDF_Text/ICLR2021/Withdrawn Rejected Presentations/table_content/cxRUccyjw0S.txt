Table 1: Multi-modal domain translation on the three domains of AFHQ. Metrics: classification ac-curacy of class labels from content codes (y J c), classification of original class from the translatedimage (y7- J Xij), translation fidelity (FID) and translation diversity (LPIPS).
Table 2: Class-supervised disentanglement of 10K classes on CelebA. Representation results: clas-Sification accuracy of class labels from content codes (y J C) and the error of landmarks regressionfrom class codes (R(ey) -â†’ c). Identity = FaceNet cosine similarity, Expression = RMSE on faciallandmarks, Head Pose = yaw, pitch, roll angle errors, Rec = reconstruction error (LPIPS). * Indicatesloss of head pose and expression details.
Table 3: Male-to-Female translation results on CelebA-HQ.
Table 4: Generator architecture based on StyleGAN2. StyleConv and ModulatedConv use the in-jected latent code composed of the content, class and style representations.
Table 5: Discriminator architecture based on StyleGAN2.
Table 6: Encoder architecture based on StarGAN-v2. Note that we do not use any domain-specificlayers. D is the dimension of the content, class or style code respectively.
Table 7: Encoder shallow architecture with low receptive field for settings in which classes onlyexhibit a low level intra style variation (e.g. CelebA).
