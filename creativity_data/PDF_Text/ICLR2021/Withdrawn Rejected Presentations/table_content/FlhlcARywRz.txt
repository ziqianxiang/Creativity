Table 1: Zero-shot cross-dataset object detection performance on the validation sets of datasets thatwere not seen during training. The metric is mEAP@0.5 (see the text for a definition). We compareto models trained on each single training dataset (top 4 rows), a unified detector with dataset-specificoutput classifier (5th row), a unified detector with our learned unified label space (6th row), and aunified detector with the human expert label space (7th row). For reference, we show an “oracle”model that is trained on the training set of each test dataset on the bottom row. The columns refer totest datasets.
Table 2: Evaluation of unified label spaces. We measure mAP on the validation sets of the trainingdatasets. We compare to a language-based baseline and a manual unification by a human expert.
Table 3: Detection mAP on the validation sets of the training datasets. We show the performanceof our unified model, the disjoint label-space detector with the oracle head at test time, and dataset-specific models (the last row, where each column is from a different model).
Table 4: Test set performance on RVC datasets: COCO test-challenge set, OpenImages challenge2019 test sets (shown in public test set/ private test set), Mapillary test set, and Objects365 validationset. Top: results of RVC challenge participants. Bottom: the published state-of-the-art performanceon each specific dataset (without model ensembles or test-time augmentation). Objects365 wasinitially part of the challenge but was removed in the final evaluation.
Table 5: Instance segmentation performance on six training datasets and two new datasets (KITTIand WildDash). We show mask mAP when the test dataset is included in training, and show maskmEAP when the testing on new datasets.
Table 6: Leaderboard of RVC instance segmentation challenge. We show results on the test set foreach datasets (test-challenge for COCO and Private test set for OPenImages).
Table 7: Summarize of datasets we used. Top: datasets we used in training, which are from ECCV2020 RVC challenge; Bottom: datasets we used for zero-shot cross dataset testing. We list thefeatures of each dataset in the last column.
Table 8: Ablation experiments on training losses. We start with a sigmoid cross entropy loss, and addour multi-dataset loss and hierarchical-aware loss (for OpenImages only) one by one. Experimentsare conducted on ResNet-50 with 2× schedule.
Table 9: Ablation experiments on data sampling. We compare sampling by the original size of eachdataset (first row) to evenly sampling across datasets, and validate the effectiveness of class-awaresampling for Objects365 and OpenImages (second row).
Table 10: Ablation studies on trianing schedule. We train the unified detector on and each dataset-specific models for different training schedules and show mAP on each training datasets.
Table 11: Table of notations used in the algorithms. For each notation, we also list their dimensionand the value range.
