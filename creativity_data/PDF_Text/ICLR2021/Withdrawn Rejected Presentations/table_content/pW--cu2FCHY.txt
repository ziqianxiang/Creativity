Table 1: Complexity comparison with different Transformers: Reformer (Kitaev et al., 2020), Syn-thesizer (Tay et al., 2020a), Linear Transformer (Katharopoulos et al., 2020) (only variants thatsupport the causal mode are shown). Here T, d denote the sequence length and feature dimension,respectively.
Table 2: NLL results on CIFAR10, evaluated by bits/dim, the lower the better. Speed and memoryare measured during training time, with a batch size of 32 across 8 V100 GPUs. AFT achieve thestate-of-the-art result in this setting, with much fewer parameters, better speed and significantly lessmemory.
Table 3: Enwik8 results, measured in bits per character (bpc), the lower the better. Baselines com-pared are Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020a) (itâ€™s best performing denseversion) and Linear Transformer (Katharopoulos et al., 2020). Speed and memory are measuredduring training time, with a batch size of 128 on a 8 V100 GPU node.
Table 4: Training and testing bpc w.r.t. the local window size for AFT-local-learned.
Table 5: WMT 2014 English-to-German Translation.
Table 6: Image super resolution results On CelebA. Our AFT mod-els outperform the PiXelReCurSiVe baseline (Dahl et al., 2017) inbits/dim (the lower the better), and show clear advantages in param-eter efficiency and memory saving over Image Transformers (Par-mar et al., 2018), with comparable or even better performance.
Table 7: Results on ShapeNetCore v2, evaluated by bits/dim, the lower the better.
