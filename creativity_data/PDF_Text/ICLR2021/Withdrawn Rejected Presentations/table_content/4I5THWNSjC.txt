Table 1: Comparison with selected efficient networks on ImageNet. Statistics on referenced base-lines are cited from original papers. See Appendix C for detailed comparison incl. training recipes.
Table 2: Latency measurements on Google Pixel 3XL for different models.
Table 3: Different disturbance applied to the combination coefficients.			Disturbance	Correct	TOP- 1	Mean	Uniform	ShuffledBasisNet-MV2	78.2	73.9 (-4.3)	67.2 (-11.0)	67.2 (-11.0)	56.5 (-21.7)BasisNet-MV3	79.8	77.8 (-2.0)	69.5 (-10.3)	69.7 (-10.1)	58.1 (-21.7)4.8	Understanding the learned BasisNet modelsVisualizing the specialization of basis models. We visualized the combination coefficient vectorson ImageNet validation set to better understand the effectiveness of model synthesis. In Fig. 7we show visually similar and distinct categories, as well as the combination coefficients of 15-thlayer. The lightweight model chooses the same specialist to better handle the subtleties between dogbreeds, but for visually distinct categories the synthesized models are very different (curves in (B)bottom do not coincide). In Fig. 7 (D) we show the coefficients for all images using t-SNE. Thedog categories form a single cluster while the others reside in very different clusters. We also finddifferent bases are activated by fine-grained visual patterns, e.g. fluffy dogs mainly activate 2nd baseand short-haired dogs use 14th base. More qualitative examples are shown in Appendix E.
Table 4: Complete comparison of different efficient networks on ImageNet classification. For base-lines, We cite statistics on ImageNet from original papers. Our results are bolded.
Table 5: Detailed comparison of BasisNet-MV2 with MobileNetV2.
Table 6: Detailed comparison of BasisNets with different number of bases.
Table 7: Comparison of BasisNet with CondConv.
Table 8: Performance drop When Shuffled disturbance was applied at different layer.
