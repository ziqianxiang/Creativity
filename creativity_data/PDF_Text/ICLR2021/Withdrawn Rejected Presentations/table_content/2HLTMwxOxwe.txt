Table 1: Main results. We report the AUROC and the FPR@95. The best RETO and best baseline metricsare highlighted for each setting. For the corrupted CIFAR data sets, we report the average and the worst-casevalues, over all corruption types. For all instances, we used a training set of 40,000 labeled samples and a testset of size 20,000 with an equal number of ID and OOD samples.
Table 2: OOD detection hardness.
Table 3: Generalization on held out test set.
Table 4: AUROC for an early-stopped binary classifier trained to separate the training set from thetest set.
Table 5: Extended results on all setting (including a breakdown for all corruption types). For RETOand vanilla ensembles, we train 5 ResNet20 models for each setting.
Table 6: Experiments with a test set of size 1000, with an equal number of ID and OOD test samples.
Table 7: Results on all setting where we used ensembles of 5 VGG16 models for RETO and vanilla.
Table 8: Results on MNIST/FashionMNIST settings. For RETO and vanilla ensembles, we train 53-hidden layer MLP models for each setting.
Table 9: Results for Outlier Exposure, when using the same corruption type, but with a higher/lowerseverity, as OOD data seen during training.
Table 10: OOD detection performance on CIFAR10 vs CIFAR10v2ID data	OOD data	kNN	DPN	Vanilla Ensembles	OE	Mahal.	Mahal-T AUROC ↑ /FPR@95 φ	MCD	RETO (rand init)	RETO (pretrained)CIFAR10	CIFAR10v2	0.53 / 0.93	0.63 / 0.91	0.64 / 0.87	0.64 / 0.88	0.55 / 0.92	0.56 / 0.93	0.58 / 0.90	0.91 / 0.20	0.76 / 0.74Furthermore, our OOD experiments (as shown in Table 10) show that most baselines are able todistinguish between the two datasets, with RETO achieving the highest performance. The methodswhich require OOD data for tuning (Outlier Exposure and DPN) use CIFAR100 for tuning.
