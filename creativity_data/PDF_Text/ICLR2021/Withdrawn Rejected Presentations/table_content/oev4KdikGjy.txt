Table 1: Mutual information between VAE latent spaces (ZA) and the CIFAR-10 test set (I(ZA; X)),ʌand the CIFAR-10 test set as reconstructed by a baseline VAE (I(ZA; X)) for VAEs trained witha range of MSDAs. MixUp prevents the model from learning about specific features in the data.
Table 2: Image classification accuracy for our approach, FMix, against baselines for: PreAct-ResNet18 (ResNet), WideResNet-28-10 (WRN), DenseNet-BC-190 (Dense), PyramidNet-272-200+ ShakeDrop + Fast AutoAugment (Pyramid). Parentheses indicate author quoted result.
Table 3: Classification performance for a ResNet101 trained on ImageNet for 90 epochs with a batchsize of 256, and evaluated on ImageNet and ImageNet-a, adversarial examples to ImageNet. Notethat Zhang et al. (2017) (MixUp) use a batch size of 1024 and Yun et al. (2019) (CutMix) train for300 epochs, so these results should not be directly compared.
Table 4: Classification performance for FMix against baselines on Bengali grapheme classification.
Table 5: Classification performance of FMix and baselines on sentiment analysis tasks.
Table 6: Mean and standard deviation divergence scores on CIFAR-10H, using the PreAct ResNet18model trained on CIFAR-10.
Table 7: Classification performance for our approach, FMix, against a baseline for a PointNet (Qiet al., 2017) on ModelNet10 (Wu et al., 2015)Data setModelBaselineFMixMixUpCutMixModelNet10 PointNetI 89.10±0.3289.57±0.44ResNet (α=1.0)CommandsResNet (α=0.2)97.69±0.0498.59±0.0398.44±0.0698.46±0.0898.31±0.0898.46±0.08
Table 8: General experimental details present in all experiments. Batch Size (BS), Learning Rate(LR). Schedule reports the epochs at which the learning rate was multiplied by0.1. * Adam optimiserused.
