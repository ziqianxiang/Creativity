Table 1: RMSE and NLL of models on UCI datasets. Datasets listed in order of increasing size.
Table 2: PRR and OOD detection ScoreSData	Model	PRR H[E]	(↑) V[y]	I	AUC-ROC (↑) K	v[μ]	ENSM	-	0.32 ± 0.02	-	0.58 ± 0.01	0.56 ± 0.02wine	EnD2	0.30 ± 0.02	0.30 ± 0.02	0.65 ± 0.01	0.65 ± 0.01	0.65 ± 0.03	NWPN	0.30 ± 0.03	0.30 ± 0.03	0.64 ± 0.01	0.64 ± 0.01	0.53 ± 0.02	ENSM	-	0.23 ± 0.01	-	0.64 ± 0.02	0.62 ± 0.02power	EnD2	0.23 ± 0.01	0.23 ± 0.01	0.66 ± 0.01	0.66 ± 0.01	0.50 ± 0.02	NWPN	0.20 ± 0.02	0.20 ± 0.02	0.56 ± 0.01	0.56 ± 0.01	0.31 ± 0.02	ENSM	-	0.64 ± 0.00	-	0.55 ± 0.0	0.62 ± 0.0mSd	EnD2	0.63 ± 0.00	0.63 ± 0.00	0.50 ± 0.0	0.50 ± 0.0	0.65 ± 0.0	NWPN	0.64 ± 0.00	0.64 ± 0.00	0.73 ± 0.0	0.73 ± 0.0	0.75 ± 0.05	Monocular Depth Estimation ExperimentsHaving eStabliShed that the propoSed methodS work on par with or better than enSemble methodSon the UCI dataSetS, we now examine them on the large-Scale NYU Depth v2 (Nathan Silberman& FerguS, 2012) and KITTI (Menze & Geiger, 2015) depth-eStimation taSkS. In thiS Section, thebaSe model iS DenSeDepth (DD), which defineS a U-Net like architecture on top of DenSeNet-169featureS (AlhaShim & Wonka, 2018). The original approach trainS it on inverted targetS uSing acombination of L1, SSIM, and Image-Gradient loSSeS. We replace thiS with NLL training uSing aGauSSian model, which yieldS mean and preciSion for each pixel (Single). We alSo uSe original targetSfrom the dataSet. The reSt of the data pre-proceSSing, augmentation, optimization, and evaluation
Table 3: Predictive Performance comparisonMethod	NYUv2 Predictive Performance							KITTI Predictive Performance							δι(↑)	δ2(↑)	δ3(↑)	reKD	rmse(φ)	loglOa)	NLLa)	δι(↑)	δ2(↑)	δ3(↑)	reIa)	rmse(φ)	loglOa)	NLLa)DD	0.847	0.972	0.993	0.124	0.468	0.054	-	Il 0.886		0.965	0.986	0.093	4.170	-	-ENSM 5	0.862	0.975	0.994	0.117	0.438	0.051	0.76 Il 0.932		0.989	0.998	0.073	3.355	0.032	1.94Single	0.852	0.971	0.993	0.122	0.456	0.053	5.74	0.924	0.987	0.997	0.078	3.545	0.034	2.98NWPN	0.842	0.968	0.992	0.126	0.472	0.055	-1.60	0.920	0.986	0.997	0.077	3.525	0.035	1.52EnD	0.851	0.971	0.993	0.122	0.458	0.053	9.11	0.915	0.984	0.997	0.079	3.936	0.036	3.27MD-EnD	0.858	0.972	0.992	0.121	0.451	0.051	8.48	0.925	0.987	0.997	0.079	3.446	0.034	2.30EnD2	0.855	0.972	0.993	0.120	0.451	0.052	-1.47	0.928	0.988	0.998	0.075	3.367	0.033	1.42DER	0.847	0.969	0.992	0.125	0.464	0.053	-1.04	0.926	0.986	0.997	0.078	3.552	0.034	1.71In table 4 we assess all models on the task of out-of-domain input detection. Two OOD test-datasetsare considered: LSUN-church (LSN-C) and LSUN-bed (LSN-B) (Yu et al., 2015), which consist ofimages of churches and bedrooms. The latter is most similar to NYU Depth-V2 and more challengingto detect. OOD images are center-cropped and re-scaled to the in-domain data. With regards toKITTI, which consists of outdoor images of roads and displays a large range of depth in each image,the OOD data is far closer to the camera than the in-domain data as a result of a crop-and-scaleprepossessing. Examples of this a provided in appendix D.4.
Table 4: OOD detection % AUC-ROC (↑) comparisonMethod	OOD	H[E]	NYUv2 vs LSUN OOD Detection			v[μ]	H[E]	KITTI vs LSUN OOD Detection			v[μ]			V[y]	I I	K			V[y]	I I	K	Single		0.69	0.69	-	-	-	0.02	0.02	-	-	-NWPN		0.801	0.801	0.199	0.199	0.799	0.999	0.999	1.0	1.0	1.0EnD		0.646	0.646	-	-	-	0.003	0.003	-	-	-EnD2	LSN-B	0.724	0.733	0.817	0.806	0.770	0.015	0.017	0.887	0.868	0.040DER		0.722	0.732	0.717	0.638	0.728	0.028	0.030	0.029	0.005	0.035MD-EnD		-	0.630	-	0.488	0.502	-	0.004	-	0.448	0.033ENSM		-	0.723	-	0.672	0.745	-	0.032	-	0.822	0.097Single		0.845	0.845	-	-	-	0.023	0.023	-	-	-NWPN		0.993	0.993	0.003	0.003	0.992	0.994	0.994	1.0	1.0	0.998EnD		0.703	0.703	-	-	-	0.004	0.004	-	-	-EnD2	LSN-C	0.882	0.893	0.964	0.952	0.928	0.018	0.020	0.834	0.806	0.036DER		0.877	0.877	0.791	0.675	0.878	0.034	0.035	0.035	0.009	0.040MD-EnD		-	0.698	-	0.119	0.422	-	0.012	-	0.506	0.031ENSM		-	0.887	-	0.696	0.886	-	0.036	-	0.779	0.098Figure 3: Comparison of uncertainty measures between ensembles and EnD2. For two input images,we demonstrate the difference between prediction and ground truth (Error), measures of Total Variance(Total) and EPKL (Knowledge) obtained from ensembles and our model. The left and right images
Table 5: Description of UCI datasets.
Table 6: Prediction performance metrics of models on six UCI datasets.
Table 7: PRR SCoreS on all SiX UCI datasets.
Table 8: OOD Detection (ROC-AUC) of models on UCI datasets.
Table 9: RKL ablation on NYU with training OOD KITTLMethod		Predictive Performance							OOD Detection ROC-AUC(↑)(H[E])		δι (↑)	δ2(↑)	δ3(↑)	I rel(J)	rmse(J)	log10 Q) I NLLQ)		Single		I 0.852	0.971	0.993	I 0.122	0.456	0.053 I	5.74 I	I	0.69NWPN , γ	=0	0.852	0.971	0.993	0.122	0.450	0.052	-1.46	0.691NWPN , γ	= 0.005	0.819	0.957	0.987	0.148	0.496	0.06	-1.69	0.803NWPN , γ	= 0.01	0.805	0.952	0.986	0.148	0.515	0.061	-1.57	0.81NWPN , γ	= 0.025	0.665	0.905	0.975	0.197	0.666	0.085	-0.84	0.866NWPN , γ	= 0.05	0.556	0.849	0.956	0.240	0.821	0.106	0.05	0.876D.4 Additional OOD detection experimentsWe also provide additional OOD detection results, where models trained on NYU detect KITTI OODdata, and vice versa. Note, we do not evaluate NWPN in this scenario, as there two datasets representthe training data. The results generally follow the same trends as those outline in the main paper.
Table 10: RKL ablation on KITn With training OOD NYU.
Table 11: OOD Detection (ROC-AUC) of NYU models on KITTI and vice-versa.
