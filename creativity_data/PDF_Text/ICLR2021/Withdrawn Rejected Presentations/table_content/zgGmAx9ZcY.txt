Table 1: 2-Conv. Layer network results on CIFAR10 and Fashion-MNIST with cross entropy loss.
Table 2: AlexNet results on CIFAR100 and TinyImageNet200 with cross entropy loss. Se-rial/Parallel indicates serial training and parallel training scheme during the backwards pass. Paral-lel+ indicates that the method is capable of update unlocking.
Table 3: 2-Conv. Layer network architecture used in the first set of experiments. BN stands forbatch normalization.
