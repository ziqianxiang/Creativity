Table 1: Dataset StatisticsDataSet	#VerticeS	#EdgeS	#LabelS	#FeatureS	% of VerticeS in TrainVal/TeSt-Reddit-	231,443	11,606,919	-41	602	70/20/10Amazon2M	2,683,902	48,336,954	31	100	70/20/10During each epoch the vertices at the frontier of each subgraph use the approximated neighbors tocapture the neighborhood information. Recall that during training the vertices at the frontier in eachsubgraph only need to capture information from their immediate neighbors (1 or 2-hop neighbors atmost). Our approximate subgraphs capture this information. In the next epoch this information fromone subgraph must move deeper into other subgraphs which are not present on the local machine. Thisis where the parameter aggregation at the end of an epoch plays a role in communicating knowledgeacross subgraphs. Thus intuitively we capture both the local knowledge at each subgraph boundaryduring an epoch, and then transmit that knowledge across subgraphs between epochs.
Table 2: Results for training GraphSAGE (GS) and KW-GCN (KW) using Reddit.
Table 3: Training KW-GCN on Amazon2M untilconvergence on five workers.
Table 4: Training GraphSAGE on Amazon2Mfor a fixed time (2386 s) on 20 workers.
