Table 1: Retrieval performance on human proof logsTerm Freq.	av rel max rank	recall@16	recall@32	recall@64	recall@128boolean	0.24	-0:15-	-019-	-025-	031logarithm	0.35	01	-0T3-	-0!7-	0.21natural	0.46	0.06	0.08	0.09	0.11In information retrieval literature, notions such as tf-idf (Manning et al., 2008) have been used toretrieve relevant documents corresponding to a query. We view premise selection as a retrievalproblem, thinking of the current goal we are trying to prove as the query, and the knowledge base ofpreviously proven theorems (premises) as the documents from which we would like to retrieve.
Table 2: Tactic success rates with extra random parameters, 1 second timeout.
Table 3: Examples of theorems in the benchmark (com-pressed for brevity)Alternative characterization of orthogonal matrices.
Table 4: Benchmark statisticsSPHt	# of TheoremsTraining	10214Validation	3225Testing	3184Total	16623 â€”Training and evaluation. During training, we generate data for training by trying to prove state-ments in the training set of 10,214 theorems. We train for 8 million steps. Details of our hardwaresetup and hyperparameters are in the Appendix. For evaluation of all our experiments trained in thereinforcement learning setup, we focus on the number of statements proven in the held-out validationset of 3,225 theorems. We run a continuous evaluation on samples of the validation set as well as afinal evaluation on the full validation set. These metrics are:6Under review as a conference paper at ICLR 2021	Final validation	Cumulative validationPure human imitation		Bansal et al. (2019)	32.65%	-Paliwal et al. (2020)	49.95%	-Human RL		Bansal et al. (2019)	38.9%	not reported
