Table 1: Quantitative results of Sim2SG when evaluated on the target domain in CLEVR environment.
Table 2: Left (resp. right): Source and target domains have different (resp. similar) appearance but similar (resp. different) content distribution. An the evaluations are on the target domain.					Trained on	mAP @0.5 IoU	Recall@20	Trained on	mAP @0.5 IoU	Recall@20Source only	0.675	0.339	Source only	1.0	0.76Source + c,label	0.923	0.646	Source + a	0.97	0.722Source + a	0.938	0.938	Source + c,label	1.0	0.996Figure 2: Qualitative results of Sim2SG on the target domain for CLEVR (first row), Dining Sim(second row) and Drive-Sim environments (last three rows). First column shows that the source onlybaseline fails to either detect objects or have high number of false positives (mislabels) leading topoor scene graph. Our method detects objects better, has way fewer false positives and ultimatelygenerates more accurate scene graphs as shown in second and third column respectively. Objects arecolor coded. For better visibility, we only show partial scene graph for Drive-Sim.
Table 3: Evaluation on KITTI hard when training Drive-Sim synthetic environment. The class specificAP and mAP are reported at 0.5 IoU.
Table 4: Quantitative results of Sim2SG on a target domain in Dining-Sim environment.
Table 5: Dining-Sim ablations. Left (resp. right): Source and target domains have different (resp.
Table 6: Evaluation on three modes of KITTI : easy (top), moderate (middle), hard (bottom) whentraining Drive-Sim synthetic environment. The class specific AP and mAP are reported at 0.5 IoU.
