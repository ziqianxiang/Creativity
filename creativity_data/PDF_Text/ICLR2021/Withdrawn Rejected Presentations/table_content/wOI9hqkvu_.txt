Table 1: Differences between various GANs. Autoregressive: using autoregressive generators.
Table 2: Generation performance on COCO and SNLI datasets. Bold scores indicate the best perfor-mance in all models while underline scores are the best in non-pretrained models.
Table 4: Performance with different discriminatorarchitectures on COCO dataset.
Table 5: Unsupervised decipherment. # and* are reported in Chen et al. (2018) and Heet al. (2020), respectively.
Table 3: Comparison to autoregressive(AR) generators on SNLI dataset.
Table 6: Results for sentence manipulation (modifying a sentence containing a source word to theone containing a target word). OV = Offset Vector; GD = Gradient Descent (Ours).
Table 7: Cases of sentence interpolation. Gray words indicate unchanged parts compared with thepreceding sentence.
Table 8: Smoothness of sentences with different lengths. We convert one sentence to the next one byadding a new sampled latent variable to the end (or the front) of the latent variable sequence. Graywords indicate unchanged parts compared with the preceding sentence.
Table 9: Hyper-parameters of NAGAN on the syn-thetic data and the real data.		Table 10: Results on the synthetic data. NA- GAN are tagged with droprates in inferenceHyperparameter	Value	(Training droprate is always 0.25).
Table 11: Generation performance on COCO and SNLI datasets with mean and variance of threeruns with different random seeds.
Table 12: Parameter size of all models when vo-cabulary size is 4838. Note that NAGAN do notneed word embeddings because it receives latentvariables as inputs.
Table 13: Cases of unconditional generationon the COCO dataset.
Table 14:	Cases of unsupervised decipherment on the WordSub dataset. We shoW the decipheredtext and the golden ansWer, Where the ciphertext is omitted. Red words indicates the deciphermenterrors.
Table 15:	Training and inference latency of an autoregressive Transformer and NAGAN on the COCOdataset. All results are evaluated with batch size of 32.
