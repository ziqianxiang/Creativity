Table 1: Overview of graph datasets, divided to three frequency groupsDatasets	|V|	|E|	d	∣c∣	r(Y)	r(X)	TypeCora	2,708	5,278	1,433	7	0.23 ± 0.04	0.91 ± 0.10	CitationCiteseer	3,327	4,676	3,703	6	0.27 ± 0.03	0.81 ± 0.19	CitationPubmed	19,717	44,327	500	3	0.55 ± 0.02	0.87 ± 0.07	CitationAmz-Photo	7,487	119,043	745	8	0.25 ± 0.04	0.82 ± 0.04	Co-purchaseAmz-Computer	13,381	245,778	767	10	0.27 ± 0.05	0.83 ± 0.04	Co-purchaseWisconsin	251	450	1703	5	0.87 ± 0.08	0.89 ± 0.23	WebCornell	183	277	1703	5	0.86 ± 0.11	0.86 ± 0.32	WebTexas	183	279	1703	5	0.98 ± 0.03	0.84 ± 0.32	WebChameleon	2,277	31,371	2325	5	0.81 ± 0.05	0.99 ± 0.01	WikipediaBipartite	2,000	50,182	50	2	2.0 ± 0.00	1.0 ± 0.00	Synthetic6.2	Vertex ClassificationWe compare our method with some of the best models in the current literature. Two layersMLP (our model without graph filters), GCN (Kipf & Welling, 2017), SGC (Wu et al., 2019),and APPNP (Klicpera et al., 2019) are used as a baseline. Geom-GCN-(I,P,S) (Pei et al., 2020),JKNet+DE (Xu et al., 2018; Rong et al., 2019), and GCNII (Chen et al., 2020a) are currently amongthe best models. We implement the Chebyshev polynomial filter as in (Defferrard et al., 2016) andset λmax = 1.5. The Literature section of Table 2 and 3 shows the best results found in the literaturewhere these models are set at the recommended hyper-parameters and recommended variants for
Table 2: Vertex classification accuracy for low-frequency datasetsMethods		Datasets						Cora	Citeseer	Pubmed	Photo	ComputerOur experiments (Average over 10 runs of stratified 0.6/0.2/0.2 splits)						MLP		75.01 ± 1.33	73.24 ± 1.28	83.56 ± 0.44	85.05 ± 1.62	80.42 ± 0.73SGC (k = 2)		87.15 ± 1.57	75.00 ± 0.93	87.97 ± 0.35	93.67 ± 0.68	90.87 ± 0.43APPNP (α = 0.2)		88.07 ± 1.32	76.71 ± 0.88	88.21 ± 0.37	94.70 ± 0.50	91.16 ± 0.44GCNII (0.5, 0.5)		86.21 ± 1.40	76.86 ± 1.29	89.77 ± 0.52	92.57 ± 0.61	88.71 ± 0.55SGF-Cheby (λmax =	2.0)	88.42 ± 1.60	76.85 ± 1.01	87.74 ± 0.37	91.26 ± 1.76	89.71 ± 0.55SGF-Cheby (λmax =	1.5)	30.05 ± 0.60	21.11 ± 0.03	41.72 ± 2.99	26.79 ± 1.82	36.99 ± 0.03SGF		88.97 ± 1.21	77.58 ± 1.11	90.12 ± 0.40	95.58 ± 0.55	92.15 ± 0.41Literature (Best result among their variants)						GCN		85.77	73.68	88.13	(not avail.)	(not avail.)GAT		86.37	74.32	87.62	(not avail.)	(not avail.)Geom-GCN		85.27	77.99	90.05	(not avail.)	(not avail.)APPNP		87.87	76.53	89.40	(not avail.)	(not avail.)JKNet+DE		87.46	75.96	89.45	(not avail.)	(not avail.)GCNII		88.49	77.13	90.30	(not avail.)	(not avail.)Table 3: Vertex classification accuracy for midrange and high frequency datasetsMethods		Datasets				
Table 3: Vertex classification accuracy for midrange and high frequency datasetsMethods		Datasets						Wisconsin	Cornell	Texas	Chameleon	BipartiteOur experiments (Average over 10 runs of stratified 0.6/0.2/0.2 splits)						MLP		83.72 ± 3.40	80.13 ± 4.59	80.30 ± 5.55	45.63 ± 1.88	48.34 ± 1.67SGC (k = 2)		56.27 ± 6.79	53.37 ± 5.41	51.49 ± 6.75	26.51 ± 2.44	48.07 ± 1.47APPNP (α = 0.2)		71.02 ± 5.98	74.55 ± 4.49	66.95 ± 6.02	54.58 ± 1.67	50.89 ± 1.08GCNII (0.5, 0.5)		71.57 ± 5.13	74.47 ± 5.42	73.78 ± 6.72	55.81 ± 1.55	49.70 ± 1.75SGF-Cheby (λmax =	2.0)	76.28 ± 4.23	69.32 ± 5.67	77.59 ± 4.36	70.16 ± 2.08	100.0 ± 0.00SGF-Cheby (λmax =	1.5)	52.34 ± 6.11	59.25 ± 3.14	62.22 ± 5.43	28.71 ± 3.19	100.0 ± 0.00SGF		87.06 ± 4.66	82.45 ± 6.19	80.56 ± 5.63	58.77 ± 1.90	100.0 ± 0.00Literature (Best results among their variants)						GCN		45.88	52.70	52.16	28.18	(not avail.)GAT		49.41	54.32	58.38	42.93	(not avail.)Geom-GCN		64.12	60.81	67.57	60.90	(not avail.)APPNP		69.02	73.51	65.41	54.30	(not avail.)JKNet+DE		50.59	61.08	57.30	62.08	(not avail.)GCNII		81.57	76.49	77.84	62.48	(not avail.)Results in Table 3 also suggest that the ability to adapt of the state of the art model GCNII is sensitiveto its parameters α and θ. In our experiment, we fix the θ parameter to 0.5 for all datasets, while in their
Table 4: Vertex classification accuracy comparison between horizontal and vertical stackingDatasets	Number of stacked filters					16	32	64	#Iteration	TimeSGF (Average over 10 runs of stratified 0.6/0.2/0.2 splits)					Cora	88.97 ± 1.21	88.70 ± 1.29	88.75 ± 1.07	234.5	115.4 msPubmed	90.12 ± 0.40	89.93 ± 0.55	88.34 ± 0.67	357.9	205.1 msWisconsin	87.06 ± 4.66	85.51 ± 4.84	86.17 ± 4.41	502.5	98.6 msCornell	82.45 ± 6.19	80.55 ± 6.58	81.14 ± 4.50	615.7	98.7 msSGF-Horizontal (Average over 10 runs of stratified 0.6/0.2/0.2 splits)					Cora	88.34 ± 1.70	88.48 ± 1.41	88.08 ± 1.65	765.9	107.1 msPubmed	87.38 ± 0.38	87.27 ± 0.40	87.10 ± 0.37	603.8	130.6 msWisconsin	84.03 ± 2.39	78.42 ± 6.70	60.19 ± 4.96	1046.5	122.1 msCornell	64.95 ± 6.02	56.84 ± 5.97	56.83 ± 6.08	666.8	81.5 msX%	Oa2	Oa3	0 α4(a) Vertical Filter Stacking(b) Horizontal Filter StackingFigure 5: Block diagram example of order-4 stacked filters. Both of these models can learn order-4polynomial as the filtering function.
Table 5: Test accuracy when α and β are initialized randomlyMethods	Datasets					Cora	Citeseer	Pubmed	Photo	ComputerSGF (0.5)	88.97 ± 1.21	77.58 ± 1.11	90.12 ± 0.40	95.58 ± 0.55	92.15 ± 0.41SGF (U[-1,1])	88.47 ± 1.40	77.50 ± 1.88	88.23 ± 1.12	92.23 ± 0.53	87.15 ± 3.63	Wisconsin	Cornell	Texas	Chameleon	BipartiteSGF (0.5)	87.06 ± 4.66	82.45 ± 6.19	80.56 ± 5.63	58.77 ± 1.90	100.0 ± 0.00SGF (U[-1,1])	88.66 ± 3.40	79.13 ± 1.60	79.67 ± 3.62	57.83 ± 2.47	100.0 ± 0.002.0Init.
