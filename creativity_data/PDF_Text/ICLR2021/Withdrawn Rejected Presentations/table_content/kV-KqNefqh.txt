Table 1: Quantitative evaluation. Note that Rel denotes the exploitation of relation informationduring training, and Img indicates the requirement of real images (instead of layouts) for training.
Table 2: Ablation studies of LT-Net on COCO-Stuff. Note that Lrel and denote the RelationConsistency Loss and confidence score weight, respectively. For each added component, we trainthe LT-Net for 50 epochs and report the results on the test set.)Model	C. Box mIOU (↑)	R.Box 小 mIOU (I)	FID (；)	R-pre. (↑)Baseline	43.12 ±0.04	-	80.89 ±6.53 0.30 ±0.02+ Lrel	45.33 ±0.03	-	60.65 ±0.54 0.33 ±0.02+ VT-CAtt	46.45 ±0.05 49.57 ±0.13		56.09 ±0.19 0.34 ±0.02+ €	46.42 ±0.06 49.72 ±0.03		55.74 ±0.91 0.35 ±0.024.4	Ablation StudiesFinally, we perform ablation studies on our model design. More precisely, we demonstrate theeffectiveness of our model by incrementally adding each component to the baseline model, whichonly contains Object/Relation Predictor P and Layout Generator G without relation consistencyLrel . Additionally, we assess the each component of our relation-aware and object-discriminativeembeddings as described in Sect. 3.2, and the supporting results can be seen in Appendix A.2.3.
Table 3: Descriptions of the COCO-stuff and VG-MSDN datasets. Note that, #Img and #Rel repre-sent the total number of images and that of relation pairs in the dataset, respectively. In the last twocolumns, Obj and Pred denote the numbers of unique object classes and predicates, respectively.
Table 4: Ablation studies on our input embedding in terms of the prediction accuracy for the maskedword, object ID and PoP ID. We show that the uses of both object and PoP ID embeddings aredesirable for exploiting the relation/object (as our LT-Net does).
