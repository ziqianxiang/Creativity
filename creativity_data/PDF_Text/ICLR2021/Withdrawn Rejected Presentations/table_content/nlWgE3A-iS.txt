Table 1: Episodic reward average for Dreamer, ReaPER, and the individual components of ReaPER (Con-trastive loss, Ll and Prioritized Episodic replay, noted Contrast, Ll and PER respetively), as a function ofenvironment steps, the conjunction of Ll regularization and Contrastive loss is also shown (LlContrast). Re-wards are averaged across the cartpole balance, cartpole swingup, reacher easy, cup catch, finger spin, walkerwalk, walker run, and cheetah run environments in DMControl. ReaPER consistently outperforms the otheroptions. Prioritized episodic replay by itself does not improve upon the Dreamer baseline, but is effective inconjunction with the rest of the ReaPER pipeline. Conversely, LlContrast under-performs both Ll and Contrastme⅛ods over all tested hyperparameters, showing that these methods do not compose well unless paired withprioritized episodic replay.
Table 2: Time-to-reward comparison between Dreamer and ReaPER on 8 environments in DMCon-trol benchmark. Values indicate the ratio between average number of steps required to reach rewardtarget (expressed as percentage of maximum reward achieved by the RemER agent). Average num-ber of steps computed over the seeds that reached the target, so the estimate is Optunistic. Largernumbers indicate longer times for Dreamer to reach the target reward, numbers PreCeeded by >indicate that Dreamer did not achieve the target before 500⅛ timesteps.
