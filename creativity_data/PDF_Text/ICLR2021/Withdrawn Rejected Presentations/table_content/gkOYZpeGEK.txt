Table 1: The runtime foreach algorithm using MNISTdataset. UMAP and UMATO takemuch less time than MulticoreT-SNE (Ulyanov (2016)) when testedon a Linux server with 40-coreIntel Xeon Silver 4210 CPUs. Theruntimes are averaged over 10 runs.
Table 2: Quantitative results of UMATO and six baseline algorithms. The hyperparametersof the algorithms are chosen to minimize KL0.1. The best one is in bold and underlined, and therunner-up is in bold. Only first four digits are shown for conciseness.
Table 3: The average value of normalized Procrustes distance between diverse dimensionalityreduction techniques over four datasets. In all real-world datasets, UMATO has shown the mostrobust embedding results over different initialization methods. Although the UMATO results inthe highest normalized Procrustes distance in the Spheres dataset, the embedding results look quitesimilar (Figure 4). The winner is in bold.
Table 4: Analysis of the KL divergence and cross-entropy loss function for imposing penaltieswhen updating the positions of points in low-dimensional space. (Upper table) The KL diver-gence and the first term of the cross-entropy function impose a big penalty when wij is small but vijis large. (Lower table) In contrast, the second term of cross-entropy function imposes a big penaltywhen vij is small but wij is large.
Table 5: Quantitative evaluation of UMATO and UMATO with multi-phase optimizations.
Table 6: The normalized Procrustes distance between two projection results by the percentageof sub-samples. From four dimensionality reduction techniques, we measured the normalized Pro-crustes distance to check the projection stability using the Flow Cytometry dataset. The winner is inbold.
Table 7: Local quality metrics of UMATO and the baseline algorithms. Although the valuesare changing a little bit depending on the number of nearest neighbors, when comparing the resultof k = 10 and k = 15, the ranks barely change. The winner is in bold and underlined, and therunner-up in bold.
