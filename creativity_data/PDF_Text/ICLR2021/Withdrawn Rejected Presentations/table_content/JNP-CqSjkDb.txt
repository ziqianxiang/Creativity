Table 1: Model Configurations	Configurations		AFQMC	TNEWS	CMNLI		BiLSTM	1	1	1	#Layer	ReZero	4	4	4		Transformer	4	4	4Word2Vec		StarSaber	4	4	4		-BiLSTM-	1.45M	1.45M	1.45M	#Parameter	ReZero	3.97M	3.97M	3.97M		Transformer	3.97M	3.98M	4.35M		StarSaber	2.89M	2.89M	2.89M		ReZero	6	6	12	#Layer	Transformer	6	6	12		StarSaber-1	12	12	24Pretrained		StarSaber-2	6	6	12		ReZero	18.19M	19.81M	36.37M	#Parameter	Transformer	18.20M	19.82M	36.75M		StarSaber-1	13.47M	15.09M	27.30M		StarSaber-2	7.17M	8.79M	14.69M5Under review as a conference paper at ICLR 2021
Table 2: DataSet StatiSticSDataSet	#Training Sample	#Dev Sample	#TeSt Sample	#CharacterSAFQMC	34334	4316	3861	1.14MTNEWS	53360	10000	10000	1.63MCMNLI	391782	12426	13880	22.24MTable 3: AccUracy(%) on datasets	ModelS	AFQMC		TNEWS		CMNLI			Dev	Test	Dev	Test	Dev	Test	BiLSTM	69.00	69.90	51.01	51.17	58.55	67.30Word2Vec	ReZero	69.05	69.90	49.20	50.03	58.82	67.30	TranSformer	69.00	69.90	50.23	50.70	57.75	67.30	StarSaber	69.05	69.96	51.22	50.85	57.69	67.30	ReZero	69.97	70.27	52.59	52.69	69.44	69.43Pretrained	TranSformer	70.32	70.24	50.78	51.64	71.14	67.30	StarSaber-1	70.85	70.68	53.25	53.41	69.67	69.43	StarSaber-2	70.18	70.22	52.28	52.55	69.44	69.43From Table 2 we can See that both AFQMC and TNEWS are dataSetS of medium Size and CMNLIiS a larger dataSet compared to the other two. AFQMC iS a dataSet of Sentence Similarity, rep-reSented in a binary claSSification taSk. TNEWS iS a dataSet of text claSSification which haS 15claSSeS in total. We don’t USe the keywordS provided in order to make compariSion with re-
Table 3: AccUracy(%) on datasets	ModelS	AFQMC		TNEWS		CMNLI			Dev	Test	Dev	Test	Dev	Test	BiLSTM	69.00	69.90	51.01	51.17	58.55	67.30Word2Vec	ReZero	69.05	69.90	49.20	50.03	58.82	67.30	TranSformer	69.00	69.90	50.23	50.70	57.75	67.30	StarSaber	69.05	69.96	51.22	50.85	57.69	67.30	ReZero	69.97	70.27	52.59	52.69	69.44	69.43Pretrained	TranSformer	70.32	70.24	50.78	51.64	71.14	67.30	StarSaber-1	70.85	70.68	53.25	53.41	69.67	69.43	StarSaber-2	70.18	70.22	52.28	52.55	69.44	69.43From Table 2 we can See that both AFQMC and TNEWS are dataSetS of medium Size and CMNLIiS a larger dataSet compared to the other two. AFQMC iS a dataSet of Sentence Similarity, rep-reSented in a binary claSSification taSk. TNEWS iS a dataSet of text claSSification which haS 15claSSeS in total. We don’t USe the keywordS provided in order to make compariSion with re-SUltS in XU et al. (2020). And CMNLI iS a NatUral LangUage Inference dataSet containing 3claSSeS for each Sample. OUr reSUltS are Shown in Table 3. ReSUltS achieved by different largepretrained modelS are Shown in Table 4. It can be Seen that none of thoSe large-Scale modelScan achieve aStoniShing performance. ThiS iS dUe to the conStrUction approach USed by CLUE.
Table 4: Accuracy for large-scale pretraining models in XU et al. (2020)Models	AFQMC		TNEWS		CMNLI		Dev	Test	Dev	Test	Dev	TestBert-base	74.16	73.70	56.09	56.58	79.47	79.69BERT-wwm-ext-base	73.74	74.07	56.77	56.86	80.92	80.42ERNIE-base	74.88	73.83	58.24	58.33	80.37	80.29RoBERTa-large	73.32	74.02	57.95	57.84	82.40	81.70XLNet-mid	70.73	70.50	56.09	56.24	82.21	81.25RoBERTa-wwm-ext	74.30	74.04	57.51	56.94	80.70	80.51RoBERTa-wwm-large-ext	74.92	76.55	58.32	58.61	83.20	82.12For AFQMC, all models with Word2Vec in fact output zero for every sample(class labels are zeroand one). This may be due to distribution imbalance in such a dataset. Only those pretrained onescan classify a small fraction of samples into the positive class. Thus an improvement of 0.41% isuneasy to achieve. Another insteresting phenomenon appearing in CMNLI is that the gap betweenthe development set and the test set is surprisingly large. For models with Word2Vec, the gapreaches up to 9.61%. For TNEWS, the input sentence is only the title of a passage. In this dataset,StarSaber-1 outperforms ReZero by 0.72% while StarSaber-2 differs from ReZero by only 0.14%.
Table 5: Results of StarSaber-gate. Accuracy format:Test acc(Dev acc)	AFQMC	TNEWS	CMNLI#Parameter	13.47M~~	15.09M	27.29MAccuracy(%)	70.27(70.62)	53.56(53.04)	69.43(69.95)We can compare the results here with the results in Table 3. With the number of layers, the hiddensize and the input size fixed, gates certainly help improve the performance. But when parameters are7Under review as a conference paper at ICLR 2021equally many, that is our implementation of StarSaber-1 which has twice the number of layers, gatesdon’t show any superiority. For simplicity and compactness, we drop all gates. We may also wantto drop the gates in LSTM and replace them with a weighted residual connection instead, which issimpler and more efficient. And this weight itself can also be parametrized by a simple non-linearfunction of hidden vectors.
Table 6:	Results of StarSaber implemented with PE. Accuracy format:Test acc(Dev acc)AFQMC TNEWS	CMNLI-#Parameter	11.89M	13.51M	24.14M-AccUracy(%) 70.06(70.06)	52.05(52.13) 67.30(66.67)5.3 Effectiveness of direct pathsDirect paths seem unecessary in StarSaber since we already have a residual connection. However,from the perspective of fixed point, ifwe drop these direct paths in each layer, the model will finallyconverge to the same fixed point for whatever inputs. In order to check whether these direct pathsare practical or not, we remove all of them and again increase the number of layers to equalize thenumber of parameters. From the results of CMNLI in Table 7, we can clearly see the benefits theybring.
Table 7:	Results of StarSaber without direct paths. Accuracy format:Test acc(Dev acc)AFQMC TNEWS	CMNLI-#Parameter	10.32M	11.94M	20.99M-ACCUracy(%)	69.75(70.16)	52.76(52.31) 67.30(69.00)6 ConclusionThis paper proposes a framework to transform RNN-based models to attention-based ones. With theperspective to view attention as a way to construct a word relation graph, we transform the vanillaRNN to StarSaber, by defining a set of equations. Other variants of RNN can also be transformedin the same way, such as LSTM and GRU discussed above. In this way, we reduce the numberof parameters in Transformer by dropping the FFN layer. Experiments on three datasets and theablation study show the effectiveness of our model and framework.
