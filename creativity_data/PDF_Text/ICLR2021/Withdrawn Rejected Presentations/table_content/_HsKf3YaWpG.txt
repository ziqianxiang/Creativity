Table 1: Influence of Feature Space Uniformity on Generalization. We study the influence of feature spaceuniformity on the generalization capabilities in ZSDA by matching the feature space to prior distributions r(z)of increasing uniformity (left to right). We report mean accuracy and standard deviation over 5 runs on the taskof MNIST → USPS and USPS → MNIST zero-shot domain adaptation using ResNet-18.
Table 2: Meta-Learning. 1) Comparison of several meta-learning algorithms on four few-shot learning bench-marks: Omniglot (Lake et al., 2019), Double MNIST (LeCun, 1998), CIFAR-FS (Krizhevsky et al., 2009) andMini-Imagenet Vinyals et al. (2016). We test with multiple regularization techniques such as Dropout, L2 regu-larization and compare directly against uniformity-alignment (U-A) as proposed by Wang & Isola (2020). Themodels are evaluated with and without uniformity regularization (U R) and we report the mean error rate over5 seeds. No hyperparameter tuning is performed on the meta-learner and we use the exact hyperparametersas proposed in the original paper. 2) Application of uniformity regularization with Universal RepresentationTransformer Layers (Liu et al., 2020) on Meta-Dataset to improve further upon the state-of-the-art performanceof URT. Numbers listed in blue represent the current state-of-the-art on the MetaDataset tasks.
Table 3: Deep Metric Learning (Zero-Shot Generalization). 1) Evaluation of uniformity regularization(UR) on strong deep metric learning baseline objectives with a ResNet-50 backbone (He et al., 2016) on twostandard benchmarks: CUB200-2011 (Welinder et al., 2010) & CARS196 (Krause et al., 2013). We reportmean Recall@1 and Normalized Mutual Information (NMI). All baseline scores are taken from Roth et al.
Table 4: Zero-Shot Domain Adaptation. Comparison of several zero-shot domain adaptation strategies onthe digit recognition task. The models are evaluated with and without uniformity regularization (UR) and wereport the mean accuracy and standard deviation over 5 random seeds. The results for Adversarial DomainDiscriminative Adaptation (ADDA) and the “Source Only” + LeNet backbone are taken directly from Tzenget al. (2017). “Target Only” refers to a model directly being trained and evaluated on the target distribution. Weperform no hyperparameter tuning, and the exact hyperparameters are used as in Tzeng et al. (2017).
Table 5: Out-of-Distribution Generalization. Comparison of OOD detection performance for various net-works with mean accuracy and standard deviation over 5 seeds. To generate OOD samples, we perform randomtranslations, rotations, and scaling over each test image. We use UR to denote uniformity regularization.
