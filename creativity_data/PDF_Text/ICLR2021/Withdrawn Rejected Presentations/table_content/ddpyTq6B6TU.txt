Table 1: Statistics of six classification tasks in CLUE benchmark. #Train, #Dev, #Test are the size oftraining, development and test sets, respectively. #L is the number of labels. Sequences are simplydivided into two categories according to their length: “Long” and “Short”.
Table 2: CLUE test results scored by the evaluation server4.
Table 3: Details of the templates.
Table 4: Comparison of statistical meth-ods, the sentence embedding model and pre-trained contextualized language models onthe FAQ dataset. “Acc.” represents Top-1 ac-curacy.
Table 5: Results on NLG according to humanjudgment. “R” and “CM” represent the per-centage of paragraphs that are “relevant” and“convey the meaning”, respectively.
Table 6: Examples of the retrieved paragraphs and corresponding comments from the judges.
