Table 1: The basic hyper-parameters, we used the Adam optimizer with learing rate α = 0.0000625and = 1.5 × 10-4 , before training the online policy, we let the initialized random policy make20K steps to collect some transitions and the capacity for replay buffer is 1M. During the trainingprocess, we sample 32 transitions from the replay buffer and update the online policy every foursteps. The reward is clipped into [-1, 1] and Relu is adopted as the activation function. For replayprioritization we use the recommended proportional variant, with importance sampling from 0.4 to1, the prioritization ω is set to 0.5. In addition, we employ Natoms = 51, Vmin = -10, Vmax = 10for distributional RL and n = 3 for multi-step returns. Finally the count-base bonus is set to 0.01Table 2 lists the rest hyper-parameters for experiments on GYMIC. Since there are 46 clinical fea-tures in this environment, we stack 4 consecutive states to compose a 184-dimensional vector as theinput for the state encoder fonline(fdeployed or ftarget). The state encoder is a 2-layer MLP withhidden size 128.
Table 2: Extra hype-parameters for the experiments in GYMIC, we stack 4 consecutive states andadopt a 2-layer MLP with hidden size 128 to extract the feature of states.
Table 3: Additional hyper-parameters for experiments in Atari games. Observations are grey-scaledand rescaled to 84 × 84 4 consecutive frames are staked as the state and each action is acted fourtimes. And we limit the max number of frames for an episode to 108K. The state encoder consistsof 3 convolutional layers.
Table 4: We list the value of the switching cost and reward of different criteria when the environment takes 1.5 million steps and 3 million steps. "Reward”corresponds to the absolute value of the reward, and "Gap” denotes the difference between the reward under a specific criterion and "None." And ("Switching Cosf,corresponds to the switching cost under a criterion at this time step.
