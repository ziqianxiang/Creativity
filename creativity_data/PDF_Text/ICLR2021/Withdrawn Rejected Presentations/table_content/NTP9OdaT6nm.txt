Table 1: Metrics averaged over the last 25 episodes of training in Safety Gym environments with PPO-Lagrangian methods, normalized relative to unconstrained PPO metrics. Cost rate is the accumulatedcost regret over the entirety of training.
Table 2: Atari reward shaping with state augmentation, choosing hyperparameters that minimizeconstraint violations per episode. “Dense” refers to whether the dense cost term was used and “rewardshaping” refers to the fixed reward shaping coefficient λ.
Table 3: Atari reward shaping with state augmentation, choosing hyperparameters that maximizecumulative reward per episode. “Dense” refers to whether the dense cost term was used and “rewardshaping” refers to the fixed reward shaping coefficient λ.
Table 4: Mean per-episode MuJoCo rewards and violations with soft dense constraints and constraintstate augmentation. ToP row displays the reward shaping coefficient λ.
Table 5: Atari results with hard constraints, choosing hyperparameters which maximize reward whenapplying action shaping in training and evaluation, only in training, or only in evaluation.
