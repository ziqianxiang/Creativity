Table 1: Performances on classification tasks in CLUE in terms of accuracy (%). The numbers inboldface denote the best results of tasks. Average accuracies of models are also given. Numbersof parameters (param) and time complexities (cmplx) of models are also shown, where l, n, and ddenote layer number, sequence length, and hidden representation size respectively. The tasks withmarkt are those with data augmentation.
Table 2: Performances on MRC tasks in CLUE in terms of F1, EM (Exact Match) and accuracy.
Table 3: State-of-the-art results of Chinese base models in CLUE.
Table 4: Performance on the tasks in GLUE. Average score over all the tasks is slightly differentfrom the official GLUE score, since we exclude WNLI. CoLA uses Matthew’s Corr. MRPC andQQP use both F1 and accuracy scores. STS-B computes Pearson-Spearman Corr. Accuracy scoresare reported for the other tasks. Results of MNLI include MNLI-m and MNLI-mm. The othersettings are the same as Table 1.
Table 5: Performance on three English MRC tasks. We use EM and F1 to evaluate the performanceof teXt detection, and rePort accuracies for RACE, on both develoPment set and test set.
Table 6: State-of-the-art results of English base models in GLUE. Each task only rePorts one scorefollowing Clark et al. (2020), and we rePort the average EM of SQuAD1.1 and SQuAD2.0 ondevelopment set. AMBERT* is pre-trained with a corpora with size comparable to that of RoBERTa(160G uncomPressed teXt). Scores with ? are rePorted from the Published PaPers.
Table 7: Performances on the development sets of CLUE, GLUE, SQuAD and RACE withAMBERT-Single or Our BERT (better one) for inference. CN-Models and EN-Models denote Chi-nese and English pre-trained models respectively. CoLA uses Matthew’s Corr. We report EM ofCMRC2018 and the average EM of SQuAD1.1 and SQuAD2.0. The other metrics are all accura-cies.
Table 8: Performances on the development sets of CLUE, GLUE and RACE with different regu-larization coefficients in fine-tuning. CN-Models and EN-Models stand for Chinese and Englishpre-trained models respectively. CoLA uses Matthew’s Corr. The other metrics are accuracies.
Table 9: The rate of coarse-grained tokens (those not included in fine-grained vocabulary) in coarse-grained tokenization.
Table 10: Hyper-parameters for pre-trained AMBERT.		Hyperparam	Chinese AMBERT	English AMBERTNumber of Layers l	12	12Hidden Size d	768	768Sequence Lengh n	512	512FFN Inner Hidden Size	3072	3072Attention Heads	12	12Attention Head Size	64	64Dropout	0.1	0.1Attention Dropout	0.1	0.1Warmup Steps	10,000	10,000Peak Learning Rate	1e-4	1e-4Batch Size	512	1024Weight Decay	0.01	0.01Max Steps	1m	500kLearning Rate Decay	Linear	LinearAdam	1e-6	1e-6Adam β1	0.9	0.9Adam β2	0.999	0.999C.2 Hyper-parameters in Fine-tuning
Table 11: Hyper-parameters for fine-tuning of Chinese tasks.
Table 12: Hyper-parameters for fine-tuning of English tasks.
Table 13: Case study for sentence matching tasks in both English and Chinese (QNLI and CMNLI).
