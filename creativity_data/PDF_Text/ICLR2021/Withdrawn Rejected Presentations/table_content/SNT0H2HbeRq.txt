Table 1: Results obtained on the CIFAR-C andCIFAR-P test sets by networks that embed thepush-pull layer. For each push-pull model, the re-sults are to be compared with its baseline networkwithout inhibition. E indicates the classificationerror on the original test set. The mCE, rCE andmFR are normalized by the corresponding valuesachieved by the baseline AlexNet.
Table 2: Corruption error (CE) and flip rate (FR) of WideResNet models with and without the push-pull layer, trained using different data augmentations.
Table 3: Results obtained on the ImageNet-C and ImageNet-P by ConvNets that embed the push-pull layer as a substitute of different convolutional layers. For each augmented model with push-pulland anti-aliasing layers, the mCE and FR results are normalized by those of the baseline network.
Table 4: Results on CIFAR-C, for models that make a combined use of the push-pull filters with theanti-aliasing filters.
Table 5: Results on the corruptions in CIFAR-C, for models that make a combined use of the push-pull filters with cutout and AutoAugment (AA) data augmentation techniques.
Table 6: Detailed comparison of results on the corruptions in ImageNet-C achieved by models thatdeploy push-pull and anti-aliasing-filters.
Table 7: Flip probability per perturbation achieved on the CIFAR-P data set by networks augmentedwith push-pull filters and anti-aliasing filters.
Table 8: Flip probability per perturbation achieved on the CIFAR-P data set by networks augmentedwith push-pull filters and trained with different data augmentation techniques. AA stands for Au-toAugment.
Table 9: Flip probability results per perturbation achieved on the ImageNet-P data set by ResNetmodels augmented with push-pull filters, compared with those achieved by ResNet models thatdeploy anti-aliasing filters.
