Table 1: Here, we show Target Training performance against attacks that minimize perturbation,for which Target Training does not use adversarial samples. Target Training exceeds performanceof default, unsecured classifier. Target Training also exceeds the performance of classifier trainedwith Adversarial Training that uses adversarial samples. For several attacks, Target Training evenexceeds the accuracy of default classifier on non-adversarial samples in CIFAR10 (84.3%).
Table 2: Using adversarial samples in training, Target Training performs slightly better thanAdversarial Training against attacks that do not minimize perturbation.
Table 3: Target Training breaks the transferability of attacks generated from unsecured classifier bymaintaining high accuracy against attacks generated using the unsecured classifier in attacks thatminimize perturbation for CIFAR10 and MNIST. For attacks that do not minimize perturbation,Target Training breaks the transferability in MNIST, and partially in CIFAR10.
Table 4: Target Training performance against adaptive adversarial attack, for CIFAR10 and MNIST.
Table 5: Architectures of Target Training classifiers for CIFAR10 and MNIST datasets. For theconvolutional layers, we use L2 kernel regularizer. Notice that the final Dense.Softmax layers inboth models have 20 output classes, twice the number of dataset classes. The default, unsecuredclassifiers have the same architectures, except the final layers have 10 output classes: Dense.Softmax10.
Table 6: Expanded comparison of performance against attacks that do not minimize perturbationshows that both Target Training and Adversarial Training can defend against attacks, the adversarialsamples of which have not been used in training. Bold font has been used to highlight where suchdefense was effective. Both Target Training and Adversarial Training defend against some attacksthat they have not been trained for, but not all. Unsecured classifier performance also provided as aperformance baseline. Target Training appears to be slightly better at defending against attacks, thesamples of which have not been used in training.
Table 7: Here, we show Target Training performance on original, non-adversarial samples. WhenTarget Training is not using adversarial samples against attacks with no perturbation, Target Trainingeven exceeds unsecured classifier accuracy on non-adversarial samples (Adversarial Training is notapplicable here because it needs adversarial samples). When Target Training is using adversarialsamples against attacks with perturbation, Target Training equals Adversarial Training performance.
Table 8: Class output probabilities for Target Training on original, and adversarial samples fromMNIST. Adversarial samples generated with CW-L2(κ = 0). Zero probability values and probabil-ity values rounded to zero have been omitted.
Table 9: Class output probabilities for Target Training on original, and adversarial samples fromCIFAR10. Adversarial samples generated with CW-L2(κ = 0). Zero probability values and proba-bility values rounded to zero have been omitted. The two highest class probabilities for each imageare made bold. The deer (fifth image) appears to be misclassified as a horse.
