Table 1: Inference time (seconds) of D-Minv/LD-Minv and exact Minv for different matrix size withbatch size 100. The efficiency advantage of our LD-Minv/D-Minv is obvious through all cases.
Table 2: - log10(Test Loss) (the bigger the better) on the test data for LD-Minv and D-Minv withvarious condition number κ. D-Minv performs better when κ = 1. However, LD-Minv shows itsstrong robustness in the more complex case κ 1. The network parameters. e.g., K and L, mainlydetermines the final results of LD-MinV rather than the Condition number.___________________Condition Number κ	1	2	3	4	5	10	50	100	1000D-Minv	7.64	2.83	2.49	2.36	2.30	2.27	2.26	2.26	2.25LD-Minv	5.95	5.01	4.19	3.62	3.49	3.30	3.21	3.22	3.21Stiefel manifold. Here Si is a diagonal matrix whose diagonal entries are i.i.d. and obey a uniformdistribution U(1∕κ, 1) with distribution boundaries being [1∕κ, 1]. We let K = 4,L = 10 andthe batch size be 100. We show the results in Table 2. Not surprisingly, D-Minv suffers fromill-condition problems, e.g., the large condition number deteriorates the performance of D-Minv.
Table 3: Training loss and SVD MSE on the test data for D-SVD with various depth Ksvd . The firsttwo rows are the quantities that measured without training which, to certain extent, are equivalent tothe results of the original optimization algorithm (Wen & Yin, 2013; Li et al., 2019). The last tworows are the quantities measured after training on the same test data. The decline of the loss and theMSE is obvious, which demonstrates the matters of learning.
Table 4: Averaged PSNR and Inference Time(s) on the test images randomly selected from Schmidt& Roth (2014). Here fL-ADMM denote the L-ADMM given in Eq. (13) with fixed αk, γk for all k,aL-ADMM represents the L-ADMM with adaptive αk = max{0.98k α0, }, γk = max{0.98k γ0, }with =1e-4. When D-NbD in Eq. (14) utilizes D-Minv and LD-Minv to perform Minv during theforward propagation, we denote them by D-NbD(D-Minv) and D-NbD(LD-Minv), respectively.
