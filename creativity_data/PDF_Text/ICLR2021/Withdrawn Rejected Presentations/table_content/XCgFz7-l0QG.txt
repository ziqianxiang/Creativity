Table 1: Results of experiment on the MNIST datasetModel	method	#Parameters	accuracy on test setteacher	from scratch	-^636010^^	98.14student	from scratch	3985	87.62student	original KD	3985	88.04student	proposed method	3985	91.454.3 CIFAR- 1 0 classificationThe second experiment is conducted on the CIFAR10 dataset with two popular network structures asthe teacher and the student networks. In this experiment, we used the inception v3 (Szegedy et al.,2016) network as the teacher and mobileNet v2 (Sandler et al., 2018) as the student. The teacher isapproximately 9 times bigger than the student. We repeated the previous experiment on CIFAR10by using these two networks. Table 2 shows the results of this experiment.
Table 2: Results of experiment on CIFAR10 datasetModel	method	#parameters	accuracy on test setinception v3 (teacher)	from scratch	21638954	95.41%mobilenet (student)	from scratch	2236682	91.17%mobilenet (student)	original KD	2236682	91.74%mobilenet (student)	proposed method	2236682	92.60%7Under review as a conference paper at ICLR 20214.4	GLUE TASKSThe third experiment is designed based on General Language Understanding Evaluation (GLUE)benchmark (Wang et al., 2018) and roBERTa family language models (Liu et al., 2019; Sanh et al.,2019). The GLUE benchmark is a set of nine language understanding tasks, which are designed toevaluate the performance of natural language understanding systems. roBERTa models (roBERTa-large, roBERTa-base, and distilroBERTa) are BERT (Devlin et al., 2018) based language under-standing pre-trained models where roBERTa-large and roBERTa-base are the cumbersome versionswhich are proposed in (Liu et al., 2019) and have 24 and 12 transformer layers respectively. distil-roBERTa is the compressed version of these models with 6 transformer layers and has been trainedbased on KD procedure proposed in (Sanh et al., 2019) with utilizing the roBERTa-base as theteacher. The general procedure in GLUE tasks is fine-tuning the pre-trained models for its down-stream tasks and the average performance score. Here, we fine-tuned the distilroBERTa model based
Table 3: Results of experiment on GLUE tasksModel (Network)	ColA	SST-2	MRPC	STS-B	QQP	MNLI	QNLI	RTE	WNLI	ScoreroBERTa-large (Teacher)	60.56	96.33	89.95	91.75	91.01	89.11	93.08	79.06	56.33	85.82DistilroBERTa (Student)	56.61	92.77	84.06	87.28	90.8	84.14	91.36	65.70	56.33	78.78Our DistilroBERTa (Student)	60.49	92.51	87.25	87.56	91.21	85.1	91.19	71.11	56.33	80.304.5	GLUE tasks with few sample pointsIn this experiment, we modified the previous experiment slightly to investigate the performance ofthe proposed method in the few data sample scenario. Here we randomly select a small portionof samples in each data set and fine-tuned the distilroBERTa based on these samples. For CoLA,MRPC, STS-B, QNLI, RTE, and WNLI, 10% of data samples and for SST-2, QQP, and MNLI 5%of them in the dataset are used for fine-tuning the student model.
Table 4: Results of few sample experiment on GLUE tasksModel (Network)	ColA	SST-2	MRPC	STS-B	QQP	MNLI	QNLI	RTE	WNLI	ScoreroBERTa-large (Teacher)	60.56	96.33	89.95	91.75	91.01	89.11	93.08	79.06	56.33	85.82DistilroBERTa (Student)	43.82	91.05	76.96	81.51	84.92	75.88	83.94	52.07	56.33	71.90Our DistilroBERTa (Student)	44.11	91.74	77.20	82.82	85.32	76.75	84.34	56.31	56.33	72.765 ConclusionIn this paper, we have introduced the backward KD method and showed how we can use the back-ward knowledge of teacher model to train the student model. Based on this method, we could easilylocate the diverge areas between teacher and student model in order to acquire auxiliary samplesat those areas with utilizing the gradient of the networks and use these samples in the training pro-cedure of the student model. We showed that our proposal can be efficiently applied to the KDprocedure to improve its performance. Also, we introduced an efficient way to apply backward KDon discrete domain applications such as NLP tasks. In addition to the synthetic experiment whichis performed to visualize the mechanism of our method, we tested its performance on several imageand NLP tasks. Also, we examined the extremely small student and the few sample scenarios in twoof these experiments. We showed that the backward KD can improve the performance of the trainedstudent network in all of these practices. We believe that all auxiliary samples do not have the samecontribution to improving the performance of the student model. Also perturbing all data samplescan be computationally expensive in large datasets.
