Table 1: Training BERT on English Wikipedia and BookCorpus. Our method with half of a millioniterations matches the baseline performance with one million iteration steps, and outperforms thebaseline with half of a million iterations.
Table 2: Training BERT on the XLNet dataset. Our method with half of a million iterations matchesthe baseline performance with one million iteration steps, and outperforms the baseline performancewith half of a million iterations.
Table 3: Results from different untying points τ and thresholds ρ. Models are trained for 500kiterations on English Wikipedia and BookCorpus.
Table 4: We group several consecutive layers as a weight sharing unit instead of sharing weightsonly across original layers. A × B means grouping A layers as a unit which is being shared with Btimes. Models are trained for 500k iterations on English Wikipedia and BookCorpus.
