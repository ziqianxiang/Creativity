Table 1: Accuracies of four CNN-models on the face vs. non-face task, averaged over three runs.
Table 2: Accuracies and entropies of capsule network models on the face vs. non-face task, averagedover three runs.	_Method	Mean of accuracy	Standard devia- tion of accuracy	Mean of entroPy	Standard deviation of entroPyUnregCaPSI	99.96%	0.01%	-229	-O1schcaps1	99.44%	0.08%	"-0.004	"-0.001equalcaps1	99.96%	0.00%	^0	Éª^SdacaPSI	99.98%	0.01%	-273	-O1UnregCaPS2	99.99%	0.01%	-209	-0050.4caps2	96.63%	2.03%	"-0.012	-OT80.8caps2	99.29%	0.59%	0.0002	0.0001SChCaPS2	99.93%	0.07%	"-0.005	"-0.004equalcaps2	99.99%	0.00%	5.0	5.0SdacaPS2				For the second architecture, we train four models. These are an unregularised model, a modelregularised by weighting the entropy-loss by 0.4 and the margin-loss by 0.6, a model regularised byweighting the entropy-loss by 0.8 and the margin-loss by 0.2 and a model regularised according to aschedule that increases the weight of the entropy-loss while reducing the weight of the margin-lossas the training continues. We use the terms unregcaps2, 0.4caps2, 0.8caps2, schcaps2 to denotethese models.
Table 3: Mean activations of the predictions for face dataModel	Mean acti- vation for faces	Mean activation for trans- formed faces	Model	Mean acti- vation for faces	Mean activation for trans- formed facesCNN1	0.633	0.633	sdacaps1	0.730	0.632CNN2	T:00	0.984	unregcaps2	0.730%	0.679CNN3	0.632	0.592	0.4caps2	0.714	"-0.391CNN4	0.629	0.628	0.8caps2	0.725	0.4836Unregcaps1	0.730	0.646	schcaps2	0.729	"-0.591schcaps1	0.727	0.546	equalcaps2	0.729	0.719equalcaps1	0.727	0.714	sdacaps2		This experiment is an answer to papers (Paik et al., 2019), (Gu & Tresp, 2020) that show thatrouting algorithms are not required for performance. While deep learning models for computervision do not need a routing-like mechanism for achieving high performance, we see that suchmodels need not learn the compositionality of their inputs. The CNN-models perform very poorlyin this regard, showing no change in the activations. This is both in models that have max-pooling,and those which do not use it. Max-pooling is thought to reduce the ability of CNNs to learn spatialrelationships due to the fact that the position of the original activation is not preserved, and thiscould cause a loss of spatial relationships among activations. We see that even when no pooling isused, CNNs do not detect learn compositional structures well.
Table 4: Accuracies and entropies of unregularised and regularised capsule network models onCIFAR10, averaged over three runs.
Table 5: Accuracies and entropies of unregularised and regularised capsule network models onFashionMNIST, averaged over three runs.
