Table 1: Optimized PAC-Bayes bounds using different methods. T-nm and R-nm represents networkF-nm trained with true/random labels. TESTER. gives the empirical generalization gap. BASErepresents the bound given by the algorithm proposed by Dziugaite & Roy (2017). Appr, Iter, andIter.M represents the bound given by our algorithms.
Table 2: DatasetsDataset	# Data Points		Input Size	# Classes	Label	Train	Test			CIFAR10	50000	10000	3 X 32 X 32	10	TrueCIFAR10-R	50000	10000	3 × 32 × 32	10	RandomCIFAR100	50000	10000	3 X 32 X 32	100	TrueMNIST	60000	10000	28 X 28	10	TrueMNIST-2	60000	10000	28 X 28	2	TrueMNIST-R	60000	10000	28 X 28	10	RandomAll the datasets (MNIST, CIFAR-10, and CIFAR-100) we used are publicly available. Accordingto their descriptions on the contents and collection methods, they should not contain any personalinformation or offensive content. MNIST is a remix of datasets from the National Institute ofStandards and Technology (NIST), which obtained consent for collecting the data. However, wealso note that CIFAR-10 and CIFAR-100 are subsets of the dataset 80 Million Tiny Image (Torralbaet al., 2008) (http://groups.csail.mit.edu/vision/TinyImages/), which used automaticcollection and includes some offensive images.
Table 3: Structure of F-2002 on MNIST#	Name	Module	In Shape	Out Shape1		Flatten	(28,28)	7842	fc1	Linear(784, 200)	784	2003		ReLU	200	2004	fc2	Linear(200, 200)	200	2005		ReLU	200	2006	fc3	Linear(200, 10)	200	10		output		21LeNet5: We adopted the LeNet5 structure proposed by LeCun et al. (1998) for MNIST, and slightlymodified the input convolutional layers to adapt the input of CIFAR-10 dataset. The standard LeNet5structure we used in the experiments is shown in Table 4. We further modified the dimension of fc1and conv2 to create several variants for the experiment in Section 5.1. Take the model whose firstfully connected layer is adjusted to have 80 neurons as an example, we denote it as LeNet5-(fc1-80).
Table 4: Structure of LeNet5 on CIFAR-10#	Name	Module	In Shape	Out Shape1	conv1	Conv2D(3, 6, 5, 5)	(3, 32, 32)	(6, 28, 28)2		ReLU	(6, 28, 28)	(6, 28, 28)3	maxpool1	MaxPooling2D(2,2)	(6, 28, 28)	(6, 14, 14)4	conv2	Conv2D(6, 16, 5, 5)	(6, 14, 14)	(16, 10, 10)5		ReLU	(16, 10, 10)	(16, 10, 10)6	maxpool2	MaxPooling2D(2,2)	(16, 10, 10)	(16, 5, 5)7		Flatten	(16, 5, 5)	4008	fc1	Linear(400, 120)	400	1209		ReLU	120	12010	fc2	Linear(120, 84)	120	8411		ReLU	84	8412	fc3	Linear(84, 10)	84	10		output		Networks with Batch Normalization: In Appendix F.4 we conducted several experiments regard-ing the effect of batch normalization on our results. For those experiments, we use the existingstructures and add batch normalization layer for each intermediate output after it passes the ReLUmodule. In order for the Hessian to be well-defined, we fix the running statistics of batch normaliza-tion and treat it as a linear layer during inference. We also turn off the learnable parameters θ and β
Table 5: Structure of LeNet5-BN on CIFAR-10#	Name	Module	In Shape	Out Shape1	conv1	Conv2D(3, 6, 5, 5)	(3, 32, 32)	(6, 28, 28)2		ReLU	(6, 28, 28)	(6, 28, 28)3		BatchNorm2D	(6, 28, 28)	(6, 28, 28)4	maxpool1	MaxPooling2D(2,2)	(6, 28, 28)	(6, 14, 14)5	conv2	Conv2D(6, 16, 5, 5)	(6, 14, 14)	(16, 10, 10)6		ReLU	(16, 10, 10)	(16, 10, 10)7		BatchNorm2D	(16, 10, 10)	(16, 10, 10)8	maxpool2	MaxPooling2D(2,2)	(16, 10, 10)	(16, 5, 5)9		Flatten	(16, 5, 5)	40010	fc1	Linear(400, 120)	400	12011		ReLU	120	12012		BatchNorm1D	120	12013	fc2	Linear(120, 84)	120	8414		ReLU	84	8415		BatchNorm1D	84	8416	fc3	Linear(84, 10) output	84	1022Variants of VGG11: To verify that our results apply to larger networks, we trained a number
Table 6: Squared dot product (VTE[x])2 and spectral ratio λ1∕λ2 for fully connected layers in aselection of network structures and datasets. We independently trained 5 runs for each instance andcompute the mean, minimum, and maximum of the two quantities over all layers (except the firstlayer which takes the input with mean-zero) in all runs.
Table 7: Squared dot product (VTE[x])2 and spectral ratio λ1∕λ2 for convolutional layers in theselection of network structures and datasets in Table 6.
Table 8: Structure of E[xxT] for BN networksDataSet	Network	# fc	mean	(VT E[χ])2 min	max	mean	λ1∕λ2 min	maxMNIST	F-2002-BN	2	0.062	0.001	0.260	1.16	1.04	1.30	F-6002-BN	2	0.026	0.000	0.063	1.13	1.02	1.26	F-6004-BN	4	0.027	0.000	0.146	1.11	1.03	1.19CIFAR10	LeNet5-BN	3	0.210	0.001	0.803	1.54	1.20	1.89to the disappearance of peak in top eigenspace overlap of different models, as shown in Fig. 28. Thepeak still exists in conv1 because BN is not applied to the input.
Table 9: Full PAC-Bayes bound optimization resultsNetwork	Method	PAC-Bayes Bound	KL Divergence	SNN loss	λ (prior)	Test ErrorT-600	Prev	0.161	5144	0.028	-	0.017	Base	0.154	4612.6	0.03373	-1.3313	0.0153	Appr	0.1432	3980.6	0.03417	-1.6063	0.0153	Iter	0.1198	3766.1	0.02347	-1.2913	0.0153	Iter(D)	0.1199	3751.1	0.02366	-1.2913	0.0153	Iter.M	0.1255	3929.9	0.02494	-1.3213	0.0153T-6002	Prev	0.186	6534	0.028	-	0.016	Base	0.1921	6966.6	0.03262	-1.4163	0.0148	Appr	0.1658	5176.1	0.03468	-2.0963	0.0148	Iter	0.1456	5086.5	0.02473	-1.7963	0.0148	Iter(D)	0.1443	4956.8	0.02523	-1.7963	0.0148	Iter.M	0.1502	5024.5	0.02767	-1.8363	0.0148T-1200	Prev	0.179	5977	0.027	-	0.016	Base	0.1754	5917.6	0.03295	-1.5463	0.0161	Appr	0.1725	5318.8	0.03701	-1.8313	0.0161	Iter	0.1417	5071	0.02292	-1.4763	0.0161	Iter(D)	0.1413	5021.1	0.02316	-1.4763	0.0161	Iter.M	0.1493	5185.4	0.02576	-1.5363	0.0161
Table 10: Overlap of R(S(k)) ∖ {S(k) ∙ 1} and the top C - 1 dimension eigenspace of E[M(k)] ofdifferent layers at minima.
