Table 1: Single model perplexity for the Penn Treebank language modeling task is presented. Theasterisk (*) denotes the best perplexity on the test set for each dropout setting. The asterisk (◦)means the perplexity reported in (Gal & Ghahramani, 2016b).
Table 2: Results of out-of-distribution detection are presented. “Test accuracy” is the predictionaccuracy computed on the test set of Fashion-MNIST. For each pair of training domain (Fashion-MNIST) and non-training domain (MNIST, EMNIST, Kannada or Kuzushiji), the averaged AUCscore computed using 30 random seeds is shown with the standard deviation. The asterisk ** (resp.
Table 3: AUC score of Out-of-Distribution detection for MC dropout using 2000 sampling, Taylor approximation, and VPBNN. The standard deviation of the AUC score is calculated using 30 differ- ent random seeds. The asterisk ** (resp. *) denotes the highest (the second-highest) AUC for each pair of training and non-training dataset.				Training → non-training dataset	Method	Uncertainty	Activation func.	AUCFashion → MNIST	MC dropout	Entropy	softmax	0.866 ± 0.019			sigmoid	0.796 ± 0.024		Mean-std	Softmax	0.934 ± 0.014**			sigmoid	0.916 ± 0.022	Taylor approx.	Entropy	SOftmax	0.718 ± 0.026			sigmoid	0.682 ± 0.028		Mean-std	SOftmax	0.728 ± 0.025			sigmoid	0.775 ± 0.028	VPBNN	Entropy	SigmOid	0.806 ± 0.025	(adaptive ρ)	Mean-std	sigmoid	0.923 ± 0.026*Fashion → EMNIST	MC dropout	Entropy	softmax	0.893 ± 0.013			sigmoid	0.846 ± 0.016		Mean-std	SOftmax	0.941 ± 0.011*			sigmoid	0.937 ± 0.013	Taylor approx.	Entropy	SOftmax	0.791 ± 0.019			sigmoid	0.755 ± 0.021		Mean-std	SOftmax	0.777 ± 0.017			sigmoid	0.833 ± 0.017
Table 4: Test accuracy on the test set of Fashion-MNIST.
