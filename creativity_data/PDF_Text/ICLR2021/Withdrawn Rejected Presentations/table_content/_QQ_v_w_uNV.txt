Table 1: Three adversarial examples that successfully fool all 63 models, crafted by using universaladversarial word replacement rules discovered by our proposed algorithm.
Table 2: Adversarial transferability on AGNEWS Table 3: Adversarial transferability among vari-and MR datasets with PWWS and GA attacks. ous neural network architectures.
Table 4: Adversarial transferability of models with different input forms and embedding types.
Table 5: Attack success rates of the adversarial examples generated by applying the word replace-ment rules found by our algorithm (UAWR) and pointwise mutual information (PMI) on 63 modelswith AGNEWS and MR datasets. “Word%” denotes the average percent of words actually modified,and “Succ%” the attack success rate in terms of the number of sentences. The maximum percentageof words that are allowed to be modified was set to 30%.
Table 6: Five adversarial word replacement rules discovered from MR dataset each for the positiveand negative categories as well as their changes in the pointwise mutual information (PMI).
Table 7: All neural models under investigation.
Table 8: Different ensembles selected by human expert and algorithm.
Table 9: Attack success rate of the adversarial examples crafted by UAWR rules produced usingthe ensembles with different sizes.
Table 10: Attack success rate against maximum percentage of words that are allowed to be modified.
