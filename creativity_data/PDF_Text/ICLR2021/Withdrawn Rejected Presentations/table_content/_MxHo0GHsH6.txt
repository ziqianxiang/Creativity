Table 1: The accUracy of the biggest modelin qUantization-aware training(QAT) andprogressive bit inheritance from high bit tolow bit. Start denotes the accUracy withone epoch training, and end denotes the ac-cUracy at the end of training.
Table 2: Quantization-aware NAS performance under different bit-widths on ImageNet dataset. Bit(W/A) denotes the bit-width for both weights and activation. The number of bit for different layersis different for SPOS (Guo et al., 2019) with bit-width in the range of {1, 2, 3, 4} and APQ (Wanget al., 2020) with bit-width in the range of {4, 6, 8}. BMobi (Phan et al., 2020), BATS (Bulat et al.,2020), and OQA use the same bit-width for different layers.
Table 3: ImageNet performance under 4, 3, 2 bit. OQA4bit-M and OQA4bit-L denote medium andlarge model size in the 4 bit OQANets family respectively. @25 means we take weights from thesupernet and finetune for 25 epochs. W/A denotes the bit-width for both weights and activation.
Table 4: Top-1 Accuracy on ImageNet dataset under different settings. FLOPs is calculated withthe corresponding floating-point models. Models named Nas-then-Quantize is chosen from thepareto front with the floating-point performance while the other is chosen with 2 bit performance.
Table 5: The comparison of the search cost and retrain cost with existing quantiztion-aware nasmethods. N denotes the number of models to be deployed. The total cost is calculated with N = 40.
Table 6: The details of quantization-aware nas named SPOS (Guo et al., 2019), BMobi (Phan et al.,2020), BATS (Bulat et al., 2020), APQ (Wang et al., 2020) are given, including network architecture,search space, bit-width and quantization algorithm PACT (Choi et al., 2018), Bireal (Liu et al.,2018b), Xnor-net++ (Bulat & Tzimiropoulos, 2019), HAQ (Wang et al., 2019), LSQ (Esser et al.,2019). Group MobileNet denotes the MobileNet with group convolution in place of depthwiseconvoluton.
Table 7: MBConv refers to inverted residual block which has a ’1 × 1 pointwise - k × k depthwise-1 × 1 pointwise’ structure without SE module (Hu et al., 2018), MBConv-SE is the MBConv blockwith SE module. Channels means the number of output channels in this stage. Depth means thenumber of blocks or layers in this stage. Expand ratio refers the expand ratio of input channelswhich controls the width of the depthwise convolution. Convolution layers in the first and last hasno expand ratio. Kernel size refers to the kernel size k of the depthwise convolution.
Table 8: MBconv refers to inverted residual block which has a ’1 × 1 pointwise - k × k depthwise-1 × 1 pointwise’ structure without SE module (Hu et al., 2018), MBconv-SE is the MBconv blockwith SE module. channels means the number of output channels in this stage. Depth means thenumber of blocks in this stage. Expand ratio refers the expand ratio of input channels which controlsthe width of the depthwise convolution. convolution layers in the first and last has no expand ratio.
