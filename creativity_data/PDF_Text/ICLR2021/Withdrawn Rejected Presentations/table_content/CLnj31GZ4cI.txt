Table 1: Comparison between our approach (K-Adapter) and previous works on injecting knowl-edge into BERT.
Table 2: Results on two entity typing datasets OpenEntity and FIGER.
Table 3: Results on question answering datasets including: CosmosQA, SearchQA and Quasar-T.
Table 5: P@1 on LAMA and LAMA-UHN across Google-RE and T-REx corpora.
Table 6: Examples of generation for RoBERTaLARGE and K-Adapter. The last column reportsthe top ranked predicted tokens. Correct predictions are in bold.
Table 7: A case study for K-Adapter and RoBERTa on relation classification dataset TACRED.
