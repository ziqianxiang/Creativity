Table 1: Datasets summaryDATASET	#USER	#ITEM	#RATINGS	#DENSITY	RATING RANGE	Synthetic	50	200	variable	variable	1,2,..	., 5ML-100k	943	1,682	100,000	0.063	1,2,..	., 5ML-1M	6,040	3,706	1,000,209	0.045	1,2,..	., 5Table 2: Training settingsMETHOD	DATASET	d	M	λ	OTHERSAlternating optimization 3.1	ML-100k	3	^4^^	0.1	boosted k-rep. with 4 learners	ML-1M	3	^4^^	0	boosted k-rep. With 5 learnersRECONST-NET 3.2	ML-100k	3	T0^^	10	n1 = 10, n2 = 100, n3 = 10, drop-out= 0.1	ML-1M	4		10	nι = 15, n2 = 100, n3 = 15, drop-out= 0.1as:L=XkXck(u)g({VqTaq})-suk2+λXkakk2	(14)here g can be an arbitrary non-linear function. In figure 3 we have proposed g as two additionalhidden layers in right panel with tanh activation. Here, we usually choose n1 = n3 and equals ourexpectation from the number of soft clusters in the data. Still one can interpret the last hidden layeras the soft clustering layer.
Table 2: Training settingsMETHOD	DATASET	d	M	λ	OTHERSAlternating optimization 3.1	ML-100k	3	^4^^	0.1	boosted k-rep. with 4 learners	ML-1M	3	^4^^	0	boosted k-rep. With 5 learnersRECONST-NET 3.2	ML-100k	3	T0^^	10	n1 = 10, n2 = 100, n3 = 10, drop-out= 0.1	ML-1M	4		10	nι = 15, n2 = 100, n3 = 15, drop-out= 0.1as:L=XkXck(u)g({VqTaq})-suk2+λXkakk2	(14)here g can be an arbitrary non-linear function. In figure 3 we have proposed g as two additionalhidden layers in right panel with tanh activation. Here, we usually choose n1 = n3 and equals ourexpectation from the number of soft clusters in the data. Still one can interpret the last hidden layeras the soft clustering layer.
Table 3: RMSE comparisonMETHOD	ML-100k	ML-1M-PMF	0.952	0.883GMC (Kalofolias et al., 2014)	0.996	-Factorized EAE (Hartford et al., 2018)	0.920	0.860IGMC (Zhang & Chen, 2019)	0.905	0.857GC-MC (Berg et al., 2017)	0.910	0.832Bayesian timeSVD++ (Rendle et al., 2019)	0.886	0.816sRGCNN (Monti et al., 2017)	0.929	-GRALS (Rao et al., 2015)	0.945	-Alternating optim. (ours)	0.920	0.860RECONST-NET (ours)	0.909	0.862user va.
