Table 1: Numerical performance of DDQN on CartPole with true reward (r), noisy reward (f),surrogate reward r (Wang et al., 2020), and peer reward fpeer (ξ = 0.2). Ravg denotes averagereward per episode after convergence, the higher (↑) the better; Nepi denotes total episodes involvedin 10,000 steps, the lower (J) the better.
Table 2: BC from weak demonstrations. Our approach success-fully recovers better policies than expert.
Table 3: Comparison with single view training and CoPiEr (Songet al., 2019) on standard policy co-training.
Table A1: Numerical performance of DDQN on CartPole with true reward (r), noisy reward (rF),surrogate reward r (Wang et al., 2020), and peer reward rpeer(ξ = 0.2). Ravg denotes averagereward per episode after convergence, (last five episodes) the higher (↑) the better; Nepi denotes totalepisodes involved in 10,000 steps, the lower ⑷ the better.
Table A2: Numerical performance of DDQN on CartPole with true reward (r), noisy reward (f),surrogate reward r (Wang et al., 2020), and peer reward rpeer(ξ = 0.2). + VRT denotes the variancereduction techniques in Romoff et al. (2018) are adopted.
Table A3: The policy entropy of the PPO agent during training.
Table A4: The mean valUe of the highest action probability over 1000 steps.
