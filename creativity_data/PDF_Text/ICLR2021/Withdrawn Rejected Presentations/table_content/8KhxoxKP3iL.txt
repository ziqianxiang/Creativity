Table 1:	Testing accuracy of self-attention ablation study on the IMDB reviews dataset.
Table 2:	Testing accuracy of recurrent attention ablation study on the IMDB reviews dataset.
Table 3:	Testing accuracy of self-attention on the Noisy MNIST datasetSample sizeCNNSelf-attention-CNNEqual weight self-attention CNNn=5k	n=20k	n=60k0.874(0.001)~0,944(0.002)~0,965(0.002)0.970(0.003)	0.993(0.000)	0.996(0.000)0.900(0.001)	0,927(0.041)	0,900(0.029)From the table, we see that Attention-CNN model achieves almost perfect testing accuracy asthe original MNIST dataset. Although equal-attention CNN has the same expressiveness power,its performance is much worse than attention-CNN model. Again, It shows the usefulness ofconcentrating self-attention weights properly in the task.
