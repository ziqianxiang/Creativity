Table 1: Experment setting for PPL and global preference and its variants	Preference	Batch sampling	RewardSetting 1	PPL	Replay memory + Experts	Rt,i + Rt,eSetting 2	PPL	Replay memory	Rt,i + Rt,eSetting 3	PPL	Replay memory + Experts	Rt,iSetting 4	Global preference	Replay memory + Experts	Rt,i + Rt,ethat our active inference based approach can achieve a compatible results with current inverse RLalgorithms. Expert simulations were obtained from Open AI RL baseline zoo (Raffin, 2018).
