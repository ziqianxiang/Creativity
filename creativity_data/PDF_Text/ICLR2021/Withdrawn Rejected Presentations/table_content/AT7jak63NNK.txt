Table 1: Settings for out-of-distribution environmentsEnvironment	Meta-train tasks	Meta-test tasks	Data points for adaptation	N	MCheetah-vel-medium	100	30	200	10	100Cheetah-vel-hard	100	30	200	10	100Ant-direction	100	10	400	20	0Cheetah-negated-joints	10	10	400	10	0Ant-negated-joints	10	10	400	10	0Walker-rand-params	40	20	400	10	10013Under review as a conference paper at ICLR 2021C.3 Hyper-parametersFor the MIER experiments hyper-parameters are kept mostly fixed across all experiments, with themodel-related hyperparameters set to default values used in the Model Based Policy Optimizationcodebase (Janner et al., 2019), and the policy-related hyperparameters set to default settings inPEARL (Rakelly et al., 2019), and their values are detailed in Table 2. We also ran sweeps on somehyper-parameters, detailed in Table 3.
Table 2: Default Hyper-parameters(a) Model-relatedHyPerParameter	ValueModel arch	200-200-200--	200Meta batch size	10Inner adaptation steps	2Inner learning rate	0.01Number of cross tasks for rela-	20belling	Batch-size for cross task sam-	1e5pling	Dataset train-val ratio for model	0.8adaptation	(b) Policy-relatedHyperparameter	ValueCritic arch	300-300-300Policy arch	300-300-300Discount factor	0.99Learning rate	3e-4
Table 3: Hyper-parameter sweepsHyper-parameter	Value	Selected ValuesNumber of policy optimization steps per meta-training iteration	1000, 2000, 4000	1000Context vector dimension	5, 10	5Gradient norm clipping	10, 100	10All experiments used GNU parallel (Tange, 2011) for parallelization, and were run on GCP instanceswith NVIDIA Tesla K80 GPUS.
