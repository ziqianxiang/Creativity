Table 1: Fleiss’s Kappascores (↑): agreements on to-ken-level hallucination labelsModels Our evaluation data is generated from two reference mod-els on which we will measure hallucination (see Appendix for moredetails): (1) TranS2S (Vaswani et al., 2017) is the standard Trans-former Seq2Seq model with 6 encoder layers and 6 decoder layers.
Table 2: F1 (x100) of hallucination labels on the outputs of different systems from a machine transla-tion task (see§4.2) and the abstract summarization task (XSUM dataset). The first block are baselinemethods and the second block are our results. We highlight the best results without using reference.
Table 3: Comparisons of annotated (True) and predicted (Pred) percentage of hallucinated tokenson the benchmark test sets.
Table 4: Performance on the TranS2Sbenchmark from MT and summariza-tion by using different data as the in-PUt to the noised function N(∙). “raw”refers to the original targets in the train-ing data.
Table 5: BLEU, BLEURT and percentage of hallucinated tokens on the CWMT2017 patent testset. We compare with the noised self-training method (He et al., 2020) in the second block andsequence-level loss truncation method (Kang & Hashimoto, 2020) in the third block.
Table 6: Basic hyper-parameters of architecture for NMT models.
Table 7: Comparisons of TranS2S and MBART on Patent and COVID-19 domains. True hal. toksand Pred hal. toks are the ground-truth and our model predicted percentage of hallucinated tokensin the benchmark test set for each domain.
Table 8: Spearman‘s correlation coefficients of the sentence level hallucination scores computedby different systems with the human annotations (percentage of hallucinated tokens in a sentence).
Table 9: Triplets represent (Precision, Recall, F1 (x100)) of hallucination labels on the outputs ofdifferent systems from a machine translation task (see§4.2). The first block are baseline methodsand the second block are our results. We highlight the best results without using reference.
Table 10: Triplets represent (Precision, Recall, F1 (x100)) of hallucination labels on the abstractsummarization task (XSum dataset). The first block are baseline methods and the second block areour results. We highlight the best results without using reference.
Table 11: Error rates of hallucination predictions on target words that are synonyms (predicted byWordNet) of words in the input on XSum test sets.
Table 12: Examples of partially hallucinated outputs from the teacher MT model used in self-trainingand the hallucinated labels predicted by our system. We only highlight words with hallucinationlabels with [1].
Table 13: Examples of annotations and model predictions. [0] indicates faithful word while [1]indicates hallucinated word.
