Table 1: Summary of Complexity. Here D denotes the average degree, b denotes the batch size,snode and slayer are the number of sampled neighbors in NS and IS respectively, K is the dimensionof embedding vectors (for simplicity, assume it is the same across all layers), L is the number oflayers, N is the number of nodes in the graph, A is the adjacency matrix, T is the number ofiterations, Twait is the waiting time for LU-DGL-GCN.
Table 2: Comparison of LU-DGL-GCN with baseline methods on benchmark datasets. For smalldatasets (Cora, Citeseer, Pubmed), we set the number of layers as 2, for large OGBN-arxiv dataset,we set it as 7. We set Tlazy = 50 here for LU-DGL-GCN. Results show that LU-DGL-GCN canachieve superior efficiency without much performance sacrifice.
Table 3: Comparison of LU-DGL-GCN with different Tlazy . Note that, Tlazy = 1 corresponds tothe DGL-GNN model. Results show that with proper Tlazy , we can reduce time cost and improveaccuracy while maintaining same memory cost.
Table 4: Statistics of Benchmark DatasetDataset	Cora	Citeseer	Pubmed	OGBN-arxivNodes	2708	^3327	19717	169343Edges	5429	4732	44338	1166243Classes	7	6	3	40Feature	1433	3703	500	100In each iteration, it would choose one variable, fix the other components, then optimizing the objec-tive with respect to only the single variable. By doing so, we only need to solve a lower dimensionalminimization problem at each iteration, which would be easier. CD algorithm has been discussed invarious literatures and has been used in applications for a long time (Wright, 2015; Wu et al., 2008;Shi et al., 2016).
