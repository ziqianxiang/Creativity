Table 1: Statistics for each task in the benchmark. For the controller type, “planner” refers to ahand-designed navigation planner, “human” refers to human demonstrations, and “policy” refers torandom or neural network policies. The number of samples refers to the number of environmenttransitions recorded in the dataset.
Table 2: Normalized results comparing online & offline SAC (SAC, SAC-off), bootstrapping error reduction (BEAR), behavior-regularized actor critic with policy(BRAC-p) or value (BRAC-v) regularization, behavioral cloning (BC), advantage-weighted regression (AWR), batch-constrained Q-Ieaming (BCQ), continuousrandom ensemble mixtures (cREM), and AlgaeDICE (aDICE). Average results are reported over 3 seeds, and normalized to a score between 0 (random) and 100(expert).
Table 3: The raw, un-normalized scores for each task and algorithm are reported in the table below. These scores represent the undiscounted return obtained fromexecuting a policy in the simulator, averaged over 3 random seeds.
Table 4: Domains and dataset types contained within our benchmark. Maze2D and AntMazeare new domains we propose. For each dataset, we also include references to the source if orig-inally proposed in another work. Datasets borrowed from prior work include MuJoCo (Expert,Random, Medium), Adroit (Human, Expert), and FrankaKitchen (Complete, Partial, Mixed). Allother datasets are datasets proposed by this work.
Table 5: Our recommended partition of tasks into “training” tasks where hyperparameter tuning isallowed, and “evaluation” tasks where final algorithm performance should be reported.
