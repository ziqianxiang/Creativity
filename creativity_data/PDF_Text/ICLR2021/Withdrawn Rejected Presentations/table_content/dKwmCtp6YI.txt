Table 1: Number of language pairs out of 15 with significant differences, with respective p-values. ARRUtrefers to AR & RU being optimized only on the target side; whereas ARRUs,t denotes optimization on bothsource and target sides (relevant for directions AR-RU and RU-AR).
Table 2: Target-Train-Token-to-Parameter ratio (TTT2P ratio) for WMT’14 EN-FR and FR-EN	100	Number of lines					35,762,532		1,000	10,000	100,000	1,000,000	10,000,000	EN: num train tokens	3,248	33,768	313,154	3,123,129	30,852,455	308,640,462	1,174,344,513FR: num train tokens	3,548	36,507	339,803	3,414,959	33,865,679	343,344,536	1,327,817,765EN-FR num params	45,609,474	51,039,363	62,871,584	75,630,304	85,210,037	108,226,335	111,417,633TTT2P ratio	0.000078	0.000715	0.005405	0.045153	0.397438	3.172468	11.917483FR-EN num params	45,540,219	50,692,575	61,916,891	74,547,874	83,936,258	107,378,859	111,399,165TTT2P ratio	0.000071	0.000666	0.005058	0.041894	0.367570	2.874313	10.541771This shows that the effect we reported in § 5 also holds on these datasets: “the ratio of targettraining token count to number of parameters falls into O(10-4) for 102 lines, O(10-3) at 103,O(10-2) at 104, and O(10-1) for 105 lines and so on”.
Table 3: Target-Train-Token-to-Parameter ratio (TTT2P ratio) for IWSLT’14 DE-EN and EN-DE	Number of lines				160,239	100	1,000	10,000	100,000	DE: num train tokens	2,874	27,675	253,757	2,519,534	4,035,591EN: num train tokens	2,739	26,416	245,659	2,461,879	3,949,114DE-EN num params	45,297,348	49,410,683	53,639,825	55,189,376	55,428,584TTT2P ratio	0.000060	0.000535	0.004580	0.044608	0.071247EN-DE num params	45,405,078	49,809,797	54,300,056	56,245,643	56,564,366TTT2P ratio	0.000063	0.000556	0.004673	0.044795	0.071345K.2 Token-to-parameter ratio for non-neural monolingual LMsWe experimented also on KenLM (Heafield, 2011; Heafield et al., 2013), a non-neural LM withmodified Kneser-Ney smoothing (Kneser & Ney, 1995; Chen & Goodman, 1999), on our datasetA and found that on the word level, such a spike (or a hump) is common across all languages, seeFigure 17. The target-token-to-parameter ratio is under 1 for most of these smaller data sizes. Thisseems related to the analytical findings in Opper et al. (1990) where the pseudo-inverse solution to asimple learning problem was shown to exhibit non-monotonicity, with the peak exactly as the ratio ofdata to parameters (α) approaches 1.
Table 4: Token-to-parameter ratios on non-neural monolingual trigram LMs	lang_numlines	num_tokens	| unigrams |	| bigrams |	| trigrams |	num_params	tokens/paramsCHAR	AR_100	9079	85	925	2894	3904	2.325563525	AR_1000	123832	110	1577	8592	10279	12.04708629	AR_10000	1083517	152	3216	21479	24847	43.60755826	AR_100000	10625047	179	5114	44251	49544	214.4567859	AR_1000000	102064230	242	8517	90353	99112	1029.786807	EN_100	11730	78	806	2532	3416	3.433840749	EN_1000	159444	84	1215	5808	7107	22.43478261	EN_10000	1344001	125	2532	17181	19838	67.7488154	EN_100000	13132862	170	4231	36104	40505	324.2281694	EN_1000000	123491871	247	7126	70406	77779	1587.727677	ES_100	12374	87	781	2398	3266	3.788732394	ES_1000	171104	93	1210	5045	6348	26.95400126	ES_10000	1484804	117	2534	15462	18113	81.97449346	ES_100000	14549703	176	4261	33554	37991	382.9776263	ES_1000000	138596036	257	7217	67280	74754	1854.02836	FR_100	12456	89	836	2610	3535	3.523620934	FR_1000	179048	97	1259	5711	7067	25.33578605	FR_10000	1490983	133	2607	16282	19022	78.38203133
Table 5: Number of language pairs out of 15 with significant differences, with respective p-values. BYTE6layersis the representation with erratic ARtrg and RUtrg .
