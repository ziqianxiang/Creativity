Table 1: The average forgetting of with and without accounting for the Jacobian’s variation, on theMNIST, CIFAR and CUB200 datasets (lower is better). A higher over-parameterization coefficientimplies that the constant Jacobian assumption is more likely to hold.
Table 2: The average accuracy of several methods on the MNIST, CIFAR and CUB200 datasets.
Table 3: Overview of the properties of the SGD, OGD, OGD+, A-GEM and A-GEM-NT algorithms.
Table 4: Overview of the notions presented in the main manuscript with their respective notations.
Table 5: Classifier module of the architecture used for the CUB200 benchmark.
Table 6: Hyperparameters used across experimentsMNIST Benchmarks•	EWC, SI and MAS : we fixed the seed to 0, then performed a grid search over the regulari-sation parameter in [0.1, 1, 10, 100, 1.000, 10.000]•	SGD, OGD and OGD+ : we used the same hyperparameters as Farajtabar et al. (2019).
Table 7: Permuted MNIST : The test accuracy of models from the indicated task after being trainedon all tasks in sequence. The best Continual Learning results are highlighted in bold.
Table 8: Rotated MNIST : The test accuracy of models from the indicated task after being trained onall tasks in sequence. The best Continual Learning results are highlighted in bold.
Table 9: Comparison of the average accuracy, average forgetting, forward transfer and backwardtransfer of several methods on the Permuted MNIST benchmark.
Table 10: Comparison of the average accuracy, average forgetting, forward transfer and backwardtransfer of several methods on the Rotated MNIST benchmark.
Table 11: Comparison of the average accuracy, average forgetting, forward transfer and backwardtransfer of several methods on the Split CIFAR100 benchmark.
Table 12: Comparison of the average accuracy, average forgetting, forward transfer and backwardtransfer of several methods on the CUB200 benchmark.
