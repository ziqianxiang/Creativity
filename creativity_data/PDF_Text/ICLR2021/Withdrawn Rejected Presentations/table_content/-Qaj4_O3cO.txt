Table 1: Accuracy of VGG5 on CIFAR-10, with DCT block size =	Table 2: Accuracy(%), with timesteps indicated in parenthesis. -p and -d represent training with pixels and DCT coefficients, respectively4×4	八,	^	VGG9	VGG11	VGG13- Configuration	CIFAR-10	CIFAR-100	TinyImageNetTransform Accuracy Matrix	(%) Random	64.7 Unranked	833 Orthonormal	. PCA	83.8 DCT	83.5	-ANN-P	913	697	56.9 ANN-d	90.4	66.4	5435..51ba 90.1 (175) SNN-p	88.9 (100)	67.8 (125)	53 (250) 一SNN-d	88.2 (100)	65.1 (125)	44.6 (250) IZZ	89.94	683	52.43 (125) DCT-SNN	(48)	(48)	51.45 (48) aANN without batchnorm and maxpool to facilitate conversion bANN with batchnorm and maxpoolblocks of pixels overlap. This is equivalent to performing convolution with a kernel size of 4 and astride of 2, and increases our input dimensions by 4×. To counter this, we add an additional 2 × 2average pooling layer before the linear layers.
Table 3: Comparison of DCT-SNN to other reported results. SGB denotes Surrogate-Gradient Basedbackprop, Hybrid denotes pretrained ANN followed by SNN fine-tuning, TTFS denotes Time-To-First-Spike scheme, TL denotes tandem learning and (xC, yL) denotes an architecture with x Convlayers and y Linear layers.
Table 4: Accuracy of various temporal encoding schemes on MNISTReference	ACCUracy(%)	Timesteps(Kheradpisheh & Masquelier, 2020 )	97.4	256(Comsa et al., 2020)	97.9	not reported(Stephan et al., 2020)	85	10(Yu et al., 2013)	78	100(Xu et al., 2018)	87	not reported(Beyeler et al., 2013)	91.6	500This work	98.54	16This work	867	2This work	97.3	5To further compare the performance of the proposed DCT-SNN encoding scheme with recenttemporal methods on MNIST, we implement it on a shallow network with just 1 hidden layerconsisting of 784-100-10 neurons (all fully-connected). The results are reported in Table 4. Ascan be seen, our methods outperforms these recent temporal methods in terms of accuracy and alsoconverges at much lower timesteps.
