Table 1: Average Accuracy on classical document networksApproaches	Cora	Citeseer	PubmedGF (ZhU et al., 2003)	-680%^^	45.3%	63.0%Mani-reg (Belkin et al., 2006)	59.5%	60.1%	70.7%DeePwalk (Perozzi et al., 2014)	67.2%	43.2%	65.3%Planetoid (Yang et al., 2016)	75.7%	64.7%	77.2%Chebyshev (Defferrard et al., 2016)	81.2%	69.8%	74.4%GCN (KiPf & Welling, 2017)	81.5%	70.3%	79.0%MoNet (Monti et al., 2017)	81.7%	一	78.8%GAT (VeIickovic et al., 2018)	83.0%	72.5%	79.0%BGCN Hasanzadeh et al. (2020)	82.2%	70.0%	—ADSF (Zhang et al., 2020)	84.7%	73.8%	79.4%JAT-E	85.5±0.4%	73.8±0.4%	82.0±0.3%JAT-I	85.8±0.5%	74.3±0.4%	82.8±0.4%Table 2: Average Accuracy on OGBN-ArxivApproaches	Train	Accuracy (%) Validation	TestMLP	-63.6%^^	57.7%	55.5%Node2Vec (Grover & Leskovec, 2016)	76.4%	71.3%	70.1%GCN (KiPf & Welling, 2017)	78.9%	73.0%	71.7%GraPhSAGE (Hamilton et al., 2017)	82.3%	72.8%	71.5%
Table 2: Average Accuracy on OGBN-ArxivApproaches	Train	Accuracy (%) Validation	TestMLP	-63.6%^^	57.7%	55.5%Node2Vec (Grover & Leskovec, 2016)	76.4%	71.3%	70.1%GCN (KiPf & Welling, 2017)	78.9%	73.0%	71.7%GraPhSAGE (Hamilton et al., 2017)	82.3%	72.8%	71.5%JAT-E	81.3±0.2%	73.8±0.1%	72.6±0.06%JAT-I	81.2±0.1%	73.4±0.08%	72.9±0.2%benchmark database, as one of the testing datasets. The effectiveness of all methods is validatedvia allowing them to perform semi-supervised node classification (transductive learning) in all thebenchmarking sets and the classified nodes are evaluated using Accuracy (M icro-F1). To compareJATs impartially with other baselines, we closely follow the experimental paradigms used in therelated works (Yang et al., 2016; KiPf & Welling, 2017; VeliCkovic et al., 2018; HU et al., 2020). Weleave the details of testing datasets and experimental scenarios in appendix due to space limitation.
Table 3: Performance comparison on JATs using different structural information. C, J, or SSLmeans JAT uses Cosine or Jaccard similarity to compute the structural correlations for attentionscore computation, or only uses structural coefficents to compute attention scores.
Table 4: Characteristics of the testing datasets used in our experiments	Cora	Citeseer	Pubmed	OGBN-ArxivN	2708	-^3327^^	19717	169343|E|	5429	4732	44338	1166243D	1433	3703	500	128C	7	6	3	40Training Nodes	140	120	60	90941Validation Nodes	500	500	500	29799Test Nodes	1000	1000	1000	48603As graph transductive learning is mainly considered in our experiment to validate different ap-proaches, we set up the experiment closely following the settings in the related works (Yang et al.,2016; Velickovic et al., 2018; HU et al., 2020). All the datasets are split into three parts: training,validation, and testing. For datasets Cora, Citeseer, and Pubmed, we use only 20 nodes per class fortraining, bUt all the featUre vectors. For dataset OGBN-Arxiv, we Use a practical split strategy, seg-menting the nodes which represent the academic papers, according to pUblication years (HU et al.,2020). Papers pUblished Up to 2017 are in the training split. While papers pUblished in 2018 and2019 are Used for validation and test, respectively. The performance of different approaches areevalUated on the test split. The statistics of these benchmarking datasets are sUmmarized in Table4, where N, |E|, D, and C denote the nUmber of vertices, edges, vertex featUres, and nUmber ofclasses in each dataset, respectively.
Table 5: Performance comparison on JATs using different attention strategiesDifferent versions of JATs	Cora	Citeseer	PubmedJAT-E	85.5±0.4%	73.8±0.4%	82.0±0.3%JAT-I	85.8±0.5%	74.3±0.4%	82.8±0.4%JAT-I (node-wise)	85.4 ±0.3%	72.5±0.5%	82.6±0.2%E	DiscussionsIn this section, we perform some detailed discussions that may help one to better understand theproposed JATs.
