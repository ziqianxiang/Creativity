Figure 1: (a): Concept. A continual learner at a hospital which learns on sequence of disease prediction tasksmay want to utilize relevant task parameters from other hospitals. FCL allows such inter-client knowledgetransfer via the communication of task-decomposed parameters. (b): Challenge of FCL. Interference fromother clients, resulting from sharing irrelevant knowledge, may hinder an optimal training of target clients (Red)while relevant knowledge from other clients will be beneficial for their learning (Green).
Figure 2: Updates of FedWeIT. (a) A client sends sparsified federated parameter Bc m(ct). After that, theserver redistributes aggregated parameters to the clients. (b) The knowledge base stores previous tasks-adaptiveparameters of clients, and each client selectively utilizes them with an attention mask.
Figure 3: Configuration of task sequences:We first split a dataset D into multiple sub-tasks in456789Transmit Bb(ct,r) and A(ct-1,R) of client cc to serverCompute θ(r) 一者 Pc∈c Bct,r)Distribute θ(Gr) and{A(jt-1,R)}j∈C to client c non-IID manner ((a) and (b)). Then, we distributeMinimize EqG. (2) for sojlving each local CL problems them to multiple clients (C#). Mixed tasks fromend for	multiple datasets (colored circles) are distributedend for	across all clients ((c)).
Figure 4: Left: Averaged task adaptation during training last two (9th and 10th) tasks with 5 and 100 clients.
Figure 5:	(a) Accuracy over C2S cost. We report the relative communication cost to the original network. Allresults are averaged over the 5 clients. (b) Inter-client transfer for NonIID-50. We compare the scale of theattentions at first FC layer which gives the weights on transferred task-adaptive parameters from other clients.
Figure 6:	Left: Performance of FedWeIT with different scale of hyperparameters on Non-iid 50. Middle:Performance comparison about current task adaptation at 6th and 8th tasks during federated continual learningon NonIID-50. Right: Forgetting measure using Backward Transfer (BWT).
Figure 7:	Average Per-task Performance with error bars over the number of training epochs per com-munication rounds on Overlapped-CIFAR-100 for FedWeIT with 5 clients. All models transmit full of localbase parameters and highly sparse task-adaptive parameters. All results are the mean accuracy over 5 clients andwe run 3 individual trials. Red arrows at each point describes the standard deviation of the performance.
Figure 8:	(a) Comparison of adaption for tasks between FedWeIT and APD with 20 clients in federated continuallearning scenario (b) Comparison of adaptation for tasks between FedWeIT and APD with 100 clients in federatedcontinual learning scenario. We visualize the last 5 tasks out of 10 tasks per client. Overlapped-CIFAR-100dataset are used after splitting instances according to the number of clients (20 and 100).
Figure 9:	Forgetting analysis. Performance change over the increasing number of tasks for all tasks except thelast task (1st to 9th) during federated continual learning on NonIID-50. We observe that our method does notsuffer from task forgetting on any tasks.
Figure 10: FedWeIT with asynchronous federated continual learning on Non-iid 50 dataset. We measurethe test accuracy of all tasks per client.
