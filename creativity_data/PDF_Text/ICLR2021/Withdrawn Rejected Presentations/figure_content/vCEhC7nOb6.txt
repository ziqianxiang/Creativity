Figure 1: L2 neighborhoods with = 0.5 radius in parameter space for different parameterizations.
Figure 2: Verification of assumptions for EWN in Lin-Sep experiment: (a) shows the dataset.
Figure 3: Demonstration of Results for EWN in Lin-Sep experiment: (a) demonstrates part 1of Proposition 2, where ge is approximated by using w from the last point of the trajectory. Clearly,▽wu L stops oscillating and converges to e. (b) demonstrates part 2 of Proposition 2 and showsthat for weight vectors 5,7 and 8, Wu(t) converges in opposite direction of VwuL(w(t)). (c), (d)and (e) demonstrate Theorem 2 for EWN, where for weight vectors 5,7 and 8. The three graphs areplotted at loss values of e-200, e-250 and e-300 respectively. At each loss value, for the 3 weights,log kwu k + log kVwu Lk is approximately same.
Figure 4: (a) Network architecture for the Simple-Traj experiment . (b) Trajectories of the twoweights for EWN and Unnorm, starting from 5 different initialization points.
Figure 5:	(a) Training data for the XOR experiment. (b, c) Norm of the incoming neuron weights forthe EWN and unnormalized architectures.
Figure 6:	Variation of convergence rate of train loss with number of layers for multilayer linear netsThe asymptotic convergence rate of loss for SWN and Unnorm have already been established in Lyu& Li (2020) as Θ (~~1)1_ 2 ) ∙ For EWN, the corresponding theorem is provided belowTheore^rt 3 For E^WN under Assumntions (A1)-(A5) and lim	kr(t+1) Mt)k ——0 the followinɑeor em	. o	^ol	,	urz∙c^e∕	ʃɪssu(mL^7tι∙^ytvs (□	/∖∕/	ar匕	ɪɪɪt—^(^^)	g(t	∣ι)	g(t)	, te f^ovv^ovvi∙fv^^hold1.	∣∣w(t)k asymptotically growsat Θ ((log(d(t))L1)2.	L(w(t)) asymptotically goes down at the rate of Θ (d(t)(klg d(t))2).
Figure 7: Variation of test accuracy vs percentage of neurons pruned in first layer at different lossvalues for MNIST experimentNils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understandingbatch normalization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
Figure 8:	Verification of assumptions for SWN in Lin-Sep experiment: (a) shows the dataset.
Figure 9:	Demonstration of Results for SWN in Lin-Sep experiment: (a) demonstrates part 1of Proposition 2, where ge is approximated by using w from the last point of the trajectory. Clearly,▽wu L stops oscillating and converges to e. (b) demonstrates part 2 of Proposition 2 and showsthat for weight vectors 5,7 and 8, Wu(t) converges in opposite direction of VwuL(w(t)). (c), (d)and (e) demonstrate Theorem 2 for SWN, where for weight vectors 5,7 and 8. The three graphs areplotted at loss values of e-200, e-250 and e-300 respectively. At each loss value, for the 3 weights,log kVwu Lk - log kwu k is approximately same.
Figure 10:	(a) shows the XOR dataset. (b), (c) and (d) demonstrate that EWN weights grow sparselywhen compared to Unnorm and SWN47Under review as a conference paper at ICLR 2021E」ON(a) EWNFigure 11:	Norm of the weight vectorwhen trained on MNIST.
Figure 11:	Norm of the weight vectorwhen trained on MNIST.
