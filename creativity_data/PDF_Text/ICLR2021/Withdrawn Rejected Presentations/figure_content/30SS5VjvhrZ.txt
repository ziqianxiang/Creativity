Figure 1: The uncertainty for feed-forward NNs is evaluated. The solid line is the target function andthe training samples (Ã—) are plotted. For each method, the uncertainty is depicted as the confidenceinterval.
Figure 2: Left: LSTM. Right: the corresponding VPBNN.
Figure 3: The mean and variance of the output from the ReLU or sigmoid function for the Gaus-sian or uniform inputs are computed using VPBNN and Taylor approximation. Absolute errors ofVPBNN and Taylor approximation to the MC method are presented.
Figure 4: The architecture of NN to learn the regression function in Section 5.1. ReLU is used atthe Dense of the middle layer and the identify function is used at the Dense in the last layer.
Figure 5: The architecture of RNN used in Section 5.1. The sigmoid function and tanh function areused as the activation function.
Figure 6:	The uncertainty of Bayesian RNN is evaluated. The solid line is the target function. Foreach method, the uncertainty is depicted as the confidence interval. Upper panels: the target andestimated regression function with its uncertainty. Lower panels: the standard deviation of outputvalue at each input value.
Figure 7:	The architecture of RNN used for Language Modeling in Section 5.2. The sigmoidfunction and tanh function are used as the activation function.
Figure 8:	Neural networks to train Fashion MNIST in Section 5.3. ReLU is used in the middle denselayer, and the softmax or sigmoid function is used at the output layer.
