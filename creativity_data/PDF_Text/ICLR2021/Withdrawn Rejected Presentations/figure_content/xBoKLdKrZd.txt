Figure 1: An overview of fast neural architecture evaluators (i.e., performance estimators).
Figure 2: Dynamics of different criteria during the supernet training process.
Figure 3: (a) Influence of layer proxy. (b) Influence of channel proxy. (c) Kendallâ€™s Tau and averagerank difference of architectures in different FLOPs groups.
Figure 4: Comparison of predictor performances. x axis: Different training set size. Each linecontains the results of 3 differently randomly sampled training set. The results of using differenttraining seeds are averaged.
Figure 5: Comparison of predictors after training of5 stages. In each stage, N architecture is chosen,evaluated, and used to train the predictor along with previous architecture data.
Figure A1: The overview of predictor-based neural architecture search (NAS). The underlined de-scriptions between the parenthesis denote different methods.
Figure A2: (a)(b)(c) Kendall-tau in different FLOPs groups, the training set size is 39, 78 and 390,respectively. (d)(e)(f) Average rank difference in different FLOPs groups, the training set size is 39,78 and 390, respectively.
Figure A3: How the evaluation quality evolves along the supernet training process with differentsampling strategy. The data is also listed in Tab. 3 in the main text.
