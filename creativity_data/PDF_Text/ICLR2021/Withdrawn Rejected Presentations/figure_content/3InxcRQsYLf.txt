Figure 1: 64 Ã— 64 samples for BAIR and ViZDoom environments generated by VideoGenDeep generative models of multiple types (Goodfellow et al., 2014; van den Oord et al., 2016b; Dinhet al., 2016) have seen incredible progress in the last few years on multiple modalities includingnatural images (van den Oord et al., 2016c; Zhang et al., 2019; Brock et al., 2018; Kingma & Dhariwal,2018; Ho et al., 2019a; Karras et al., 2017; 2019; Van Den Oord et al., 2017; Razavi et al., 2019;Vahdat & Kautz, 2020; Ho et al., 2020; Chen et al., 2020), audio waveforms conditioned on languagefeatures (van den Oord et al., 2016a; Oord et al., 2017; Binkowski et al., 2019), natural language inthe form of text (Radford et al., 2019; Brown et al., 2020), and music generation (Dhariwal et al.,2020). These results have been made possible thanks to fundamental advances in deep learningarchitectures (He et al., 2015; van den Oord et al., 2016b;c; Vaswani et al., 2017; Zhang et al., 2019;Menick & Kalchbrenner, 2018) as well as the availability of compute resources (Jouppi et al., 2017;Amodei & Hernandez, 2018) that are more powerful than a few years ago. However, one notablemodality that has not seen the same level of progress in generative modeling is high fidelity naturalvideos. The complexity of natural videos requires modeling correlations across both space and timewith much higher input dimensions, thereby presenting a natural next challenge for current deepgenerative models. The complexity of the problem also demands more compute resources which canbe considered as one important reason for the slow progress in generative modeling of videos.
Figure 2: We break down the training pipeline into two sequential stages: training VQ-VAE (Left)and training a autoregressive transformer in the latent space (Right). The first stage is similar to theoriginal VQ-VAE training procedure. During the second stage, VQ-VAE encodes video data to latentsequences as training data for the prior model. For inference, we first sample a latent sequence fromthe prior, and then use VQ-VAE to decode the latent sequence to a video sample.
Figure 3: Architecture of the attention residual block in the VQ-VAE as a replacement for standardresidual blocks.
Figure 4: Moving MNIST samples conditioned on a single given frame (red).
Figure 5: VQ-VAE reconstructions for BAIR Robot Pushing. The original videos are contained ingreen boxes and reconstructions in blue.
Figure 6: Samples for BAIR Robot Pushing. (Top) shows samples conditioned on a single frame.
Figure 7: Samples for ViZDoom health gathering supreme environment. (Top) shows unconditionallygenerated samples. (Bottom) shows samples conditioned on the same action sequence (turn right andgo straight).
Figure 8: Samples for ViZDoom battle2 environment. (Top) shows unconditionally generated samples.
