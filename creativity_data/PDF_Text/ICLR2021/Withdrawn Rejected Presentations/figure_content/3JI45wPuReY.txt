Figure 1:	Average performance of the Surgeon over all 15 runs on the SVHN (top row) and CIFAR-10 (bottom row) datasets for small (left), medium (center), and large (right) starting topology, com-pared to training the starting topologies as a baseline.
Figure 2:	Left: Single run of the Surgeon on the SVHN dataset and small starting topology. Righttop: Small starting topology. Right bottom: Topology after 100 epochs training with the Surgeon.
Figure 3: Left: Single run of the CIFAR-10 dataset and large starting topology. The Surgeon man-ages to overcome the local optimum at around epoch 30. Right: Single run of the Surgeon on theSVHN dataset and large starting topology. The Surgeon is able to reduce the total parameter countby almost 2/3 while still reaching the same overall validation accuracy.
Figure 4: Example of a topology evolution performed by the Surgeon on the SVHN dataset andmedium starting topology. Early on, reducing the network size helps to increase the accuracy level(at epochs 10 and 30). Adding a whole layer in epoch 50 is still able to achieve some increasein accuracy. From epoch 60 on the network oscillates between very similar states. Ideally thisbehaviour can be used as an indicator for early stopping in future versions of the Surgeon.
