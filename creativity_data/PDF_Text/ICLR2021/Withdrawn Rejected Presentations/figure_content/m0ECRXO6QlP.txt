Figure 1: Top-5 validation accuracy of a ResNet50 pre-trained on ImageNet with access to 10% of the labels.
Figure 2: Top-1 accuracy of*^z^x^^z^z^z^z^z^z^z^z^^^Z^Z^Z^a ResNet50 pre-trained on^^^^^^^^^^^^^^^^^^^^^^^^^^^ImageNet with access to 10%*^Z^Z^^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^Z^of the labels. Orange markers depict^Z^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^SimCLR self-supervised pre-training^Z^^^^^^^^^^^^^^^^^^^^^Z^^^^^^^^^^^^^Z^^^^^^^^^^^^^ZZfollowed by fine-tuning. Blue-^Z^zZλ^Z^Z^zZλ^Z^zZZλ^Z^Z^zZλ^Z^Z^zZλ^^Z^ZZλ^Z^Z^zZ∙markers depict the combination of-^Z^zZλ^Z^Z^zZλ^zZ^ZZλ^Z^Z^zZλ^Z^Z^zZλ^Z^Z^zZλ^Z^Z^zZ∙SimCLR + SuNCEt. Using SuNCEt-^Z^^Zλ^Z^Z^zZλ^^	-^Z^^^zZλZ^Zto leverage available labels during-^Z^zZλ^Z^Z^Z^ZZ^Z^ZZλ^Z^Z^zZλ^Z^Z^zZλ^Z^Z^zZλ^Z^Z^zZ∙Ere-training (not only fine-tuning)-^zZλ^Z^Z^zZλZ-Z^zZλ^Z^Z^zZλ^zZ-^zZλ^Z^Z^zZλ^Z^zZ-Z
Figure 3: Percentage compute used by various methods to match the best SimCLR (ResNet50, ImageNet) top-1validation accuracy (top plot) and top-5 validation accuracy (bottom plot) with 1000 epochs of pre-training,followed by fine-tuning with 10% of labels. Both SimCLR + SuNCEt and SimCLR + cross-entropy use thedefault SimCLR hyper-parameters. The SimCLR+cross-entropy approach matches the best SimCLR validationaccuracy while using only 63% of the compute. These savings are lower than those provided by SuNCEt (whichonly requires -44% of pre-training to match the best SimCLR accuracy), but are significant nonetheless.
Figure 4: Training a ResNet50 with an adjusted-stem on CIFAR10 given various percentages of labeled data.
Figure 5: Training a ResNet50 on ImageNet using the SimCLR + SuNCEtcombination when given access to 10% of the labels (top sub-plot) and1% of the labels (bottom sub-plot). We examine the best top-1 and top-5validation accuracy, and the corresponding computational requirements, aswe vary the fraction of labeled samples per mini-batch. If we only use a smallfraction of labeled data in each mini-batch, then the best model accuracy drops.
Figure 6: Training a ResNet50 with an adjusted-stem on CIFAR10 given various percentages of labeled data.
Figure 7: Training a ResNet50 with an adjusted-stem on CIFAR10 given various percentages of labeled data. TheSuNCEt loss is only used for the first 100 epochs of training, and then switched off for the reaminder of training.
Figure 8: Training a ResNet50 with an adjusted-stem on CIFAR10 given various percentages of labeled data.
