Figure 1:	High Level Visualization of Proposed Model. The model processes the input tokens inmultiple temporal scales. Each scale has several transformer layers with recurrent memory. Theoutput of the last layer of one scale is compressed to form the input of the next scale. As thesegment length gets reduced because of compression, the memory length is increased to make thetotal length (i.e. segment length + memory length) of all the layers same. In the figure, the blueboxes represent hidden states of the current time step where as the red boxes represent the memorystates.
Figure 2:	TranSformer-QL Algorithm.
Figure 3:	Forward pass of Transformer-XL. The functions Compute and Shift are given in Figure 2band 2d respectively.
Figure 4:	Computation of minimum context length of a Transformer-QL model. The ns, nm respec-tively represent the segment and memory length of the scale 1 of the network.
Figure 5: Dependency of hidden states to the past tokens in a Transformer-QL network.
