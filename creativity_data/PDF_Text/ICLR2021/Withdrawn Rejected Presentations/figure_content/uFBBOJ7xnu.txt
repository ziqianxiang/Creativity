Figure 1: Temporal smoothness in the real world and in neural network training. A) Top: smoothinformation in the real world. Bottom: randomly ordered data in training neural networks.2B)Manipulating smoothness levels in training data using the ordering of training samples. Coloredrectangles indicate the amount of smoothness induced by repeating a category.
Figure 2: Neural architectures for classifying temporally smooth data. A) Test error (MSE loss)and test accuracy in SGD training of a feedforward neural network (MNIST data) across differentsmoothness levels. B) The same as A, for a neural network with leaky memory in internal represen-tations. C) The same as A, for a neural network with leaky memory and gating mechanism. Randomsampling in all 3 plots are identical and can be used as a common reference. [Curves in this fig-ure have been averaged over 5 runs with different initialization and were further smoothed using a100-iteration moving average.]the averaged results. For hyperparameters in synthetic dataset see Appendix A.2. When RMSpropwas implemented, β1 and β2 were set to 0.9 and 0.99, respectively (Ruder, 2016).
Figure 3: Unsupervised learning from data with multiple levels of smoothness. A) Example ofmultiple levels of smoothness in samples from the real world: mouth changing fast, while face-shape changes slowly. B) Multiple levels of smoothness in synthesized data: top row changes everyitem; middle row changes every 3 items; bottom row changes every 5 items. X-axis shows time,each 3-by-2 item is one sample. C) 5 Different AE models. α1, α2, and α3 show the memorycoefficient in the hidden representations. D) Reconstruction test error (MSE loss) during trainingfor individual items across 5 different AE models. All the curves in this plot have been averaged over50 runs with different random initialization. E) Comparing the “timescale- selectivity” of models,by computing the difference between the squared Pearson correlations for time-scale matching unitsand non-matching units (e.g. correlation of long-memory with slow features minus correlation oflong-memory with fast and medium features). In no-memory systems and in leaky-memory systemswith uniform memory, we measured these correlations for hidden units in the corresponding positionas those in the multi-scale leaky memory. Error bars show the mean and standard deviation across10,000 bootstraps, with 50 values per bootstrap.
