Figure 1: Comparing all_reduce and codistillation for training ResNet50 on ImageNet. WeCodistill two models using batch size 256 for each, and the model trained with all_reduce usesbatch size 512. All experiments are run on 16 GPUs. We report the (a) training loss and (b) top-1validation accuracy. We observe that compared to all_reduce, the model trained with codistilla-tion underfits, obtaining higher training loss and lower top-1 accuracy. (c) Examining the differencein parameters from initialization over the course of training further suggests that codistillation has aregularizing effect that impacts performance, since parameters remain closer to their initial values.
Figure 2: Comparing all_reduce and Codistination for ResNet50 and ResNeXt-101 models onthe ImageNet dataset, with decreasing weight decay and a shifted learning rate decay schedulecompared to Goyal et al. (2017). Both methods achieve similar values of top-1 validation accuracy.
Figure 3: Codistillation scales well across multiple values of batch size per worker. Each time wedouble the batch size per worker, we scale the learning rate schedule by a factor of two and performhalf the number of updates. We do not observe any significant degradation in (a) training loss or (b)validation accuracy across a wide range of batch sizes. The training loss and validation accuracy fordifferent batch sizes are also provided in Table A.2.
Figure 4: Comparing all_reduce and codistillation for ResNet50 and ResNeXt101 models on theImageNet dataset, using a cosine learning rate schedule (He et al., 2019). We observe that the finalvalidation performance for the two approaches is very close, confirming that codistillation worksconsistently across different learning rate schedules.
Figure 5: Comparing all_reduce and Codis-tillation using “big” transformer model onWMT’16 En-De dataset. Models trained usingall_reduce and codistillation both reach sim-ilar NLL on the validation dataset.
Figure 6: Final validation top-1 accuracy and training loss of a ResNet-50 model when trained usinga fraction of the full training data. We observe as smaller fractions of training data are used (andmodel starts overfitting), Codistination setup increasingly improves over the all_reduce setup interms of validation accuracy.
Figure 7: Evaluating the robustness of codistillation setup by varying the frequency of exchangingcheckpoints (left) and learning rate (right) when training ResNet50 on ImageNet dataset. Whilethere are some gains by using a higher frequency, the results do not degrade too much with a lowerfrequency. The learning rate can be increased from 0.1 to 0.2 without hurting the performance.
Figure 8: Comparing the training loss for the all_reduce and codistillation setups for ReSNet50and ResNeXt-101 models (respectively) on the ImageNet dataset, with decreasing weight decay anda shifted learning rate decay scheduled compared to Goyal et al. (2017). While the all_reducesetup reaches a lower training loss, the performance on the validation dataset (in terms of top-1validation accuracy) is very similar for the two setups (for both models) as shown in Fig. 2.
Figure 9: Comparing the training loss and validation accuracy (respectively) for the all_reduceand codistillation setups on the ImageNet dataset, with decreasing weight decay and a shifted learn-ing rate decay scheduled compared to Goyal et al. (2017). While the all_reduce setup reaches alower training loss, the performance on the validation dataset (in terms of top-1 validation accuracy)is very similar for the two setups.
Figure 10: Comparing the training loss and validation accuracy (respectively) for the all_reduceand codistillation setups on the ImageNet dataset, with decreasing weight decay and a shifted learn-ing rate decay scheduled compared to Goyal et al. (2017). While the all_reduce setup reaches alower training loss, the performance on the validation dataset (in terms of top-1 validation accuracy)is very similar for the two setups.
Figure 11: Comparing the training loss for the all_reduce and Codistillation setups for ResNet50and ResNeXt-101 models (respectively) on the ImageNet dataset, using the cosine learning rateschedule proposed in He et al. (2019). While the all_reduce setup reaches a lower training loss,the performance on the validation dataset (in terms of top-1 validation accuracy) is very similar forthe two setups (for both models) as shown in Fig. 4.
Figure 12: Comparing the training loss (left) and validation accuracy (right) of the all_reduceand codistillation setups with the ResNet50 model on the ImageNet dataset. The training setup is thesame as in Fig. 2(a), with one difference - the L2 weight decay is kept at a constant value of 10-4throughout training. These plots show that when we do not account for the regularization effect ofcodistillation, the codistillation model,s performance lags behind all_reduce.
Figure 13: Evaluating codistillation setup by varying the value of αk . Note that the value is notchanged during training and hence αk is denoted by α.
Figure 14: Comparing all_reduce and codistillation setups using the “big” transformer modelon the WMT’16 En-De dataset. The model trained using codistillation performs worse in terms oftraining performance, but generalizes well to the validation dataset, as seen in Fig. 5.
Figure 15:	Evaluating the training loss robustness of the codistillation setup by varying the frequencyof exchanging checkpoints (left) and learning rate (right) when training a ResNet50 model on theImageNet dataset. The training loss is robust to changes to the frequency, reaching very similarvalues across the whole range. The learning rate can be increased from 0.1 to 0.2 without hurtingthe training performance.
Figure 16:	Evaluating the robustness of codistillation for the Transformer model by varying thefrequency of exchanging checkpoints. The performance does not degrade when exchanging check-points less frequently.
Figure 17:	Evaluating the robustness of codistillation for the Transformer model by varying theinitial value of α and γ. Given the nature of geometric progression, we perform a sweep over α toachieve optimal performance for each γ ∈ {1, 1.1, 1.5}. The case of γ = 1 corresponds to keepingα constant. This case slightly under-performs as compared to the cases where we increase α overtime.
