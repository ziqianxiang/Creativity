Figure 1: We study learning policies by offline learning on a prior dataset D and then fine-tuning with onlineinteraction. The prior data could be obtained via prior runs of Rl, expert demonstrations, or any other source oftransitions. Our method, advantage weighted actor critic (AWAC) is able to learn effectively from offline dataand fine-tune in order to reach expert-level performance after collecting a limited amount of interaction data.
Figure 2: Analysis of prior methods on HalfCheetah-V2 using offline RL with online fine-tuning. (1) On-policymethods (DAPG, AWR, MARWIL) learn relatively slowly, even with access to prior data. We present ourmethod, AWAC, as an example of how off-policy RL methods can learn much faster. (2) Variants of soft actor-critic (SAC) with offline training (performed before timestep 0) and fine-tuning. We see a “dip” in the initialperformance, even if the policy is pretrained with behavioral cloning. (3) Offline RL method BEAR (Kumaret al., 2019) on offline training and fine-tuning, including a “loose” variant of BEAR with a weakened constraint.
Figure 3: Comparative evaluation on the dexterous manipulation tasks. These tasks are difficult due to their highaction dimensionality and reward sparsity. We see that AWAC is able to learn these tasks with little online datacollection required (100K samples ≈ 16 minutes of equivalent real-world interaction time). Meanwhile, mostprior methods are not able to solve the harder two tasks: door opening and object relocation.
Figure 4: Comparison offine-tuning from an initialdataset of suboptimal data ona Sawyer robot pushing task.
Figure 5: Comparison of our method and prior methods on standard MuJoCo benchmark tasks. These tasks aremuch easier than the dexterous manipulation tasks, and allow us to better inspect the performance of methods inthe setting of offline pretraining followed by online fine-tuning. SAC+BC and BRAC perform on par with ourmethod on the HalfCheetah task, and ABM performs on par with our method on the Ant task, while our methodoutperforms all others on the Walker2D task. Our method matches or exceeds the best prior method in all cases,whereas no other single prior method attains good performance on all of the tasks.
Figure 6: Comparison of prior algorithms that can incorporate prior datasets. See section A.5 for specificimplementation details. We argue that avoiding estimating ∏β (i.e., ∏β is “No")is important When learningwith complex datasets that include experience from multiple policies, as in the case of online fine-tuning, andmaintaining a constraint of some sort is essential for offline training. At the same time, sample-efficient learningrequires using Qπ for the critic. Our algorithm is the only one that fulfills all of these requirements.
Figure 7: Comparison of our method (AWAC) with CQL and AlgaeDICE. CQL and AWAC perform similarlyoffline, but CQL does not improve when fine-tuning online. AlgaeDICE does not perform well for offlinepretraining.
Figure 8: Comparison of our method (AWAC) fine-tuning on varying data quality datasets in D4RL (Fu et al.,2020). AWAC is able to improve its offline performance by further fine-tuning online.
