Figure 1: Amortization. Left: Optimization over two dimension of the policy mean, μ1 and μ3,for a particular state. A direct amortized policy network outputs a SuboPtimal estimate, yielding anamortization gap in performance. An iterative amortized policy network finds an improved estimate.
Figure 2: Estimating Multiple Policy Modes. Unlike direct amortization, which is restricted to asingle estimate, iterative amortization can effectively sample from multiple high-value action modes.
Figure 3: Mitigating Value Overestimation. Using the same value estimation setup (β = 1 inEq. 12), shown on Ant-v2, iterative amortization results in (a) higher value overestimation bias(closer to zero is better) and (b) a more rapidly changing policy as compared with direct amortiza-tion. Increasing β helps to mitigate these issues by further penalizing variance in the value estimate.
Figure 4: Policy Optimization. Visualization over time steps of (a) one dimension of the policydistribution and (b) the improvement in the objective, ∆J , across policy optimization iterations. (c)Comparison of iterative amortization with Adam (Kingma & Ba, 2014) (gradient-based) and CEM(Rubinstein & Kroese, 2013) (gradient-free). Iterative amortization is substantially more efficient.
Figure 5: Performance Comparison. Iterative amortized policy optimization performs comparablywith or better than direct amortized policies across a range of MuJoCo environments. Performancecurves show the mean and ± standard deviation over 5 random seeds.
Figure 6: Decreased Amortization Gap. Estimated amortization gaps per step for direct and itera-tive amortized policy optimization. Iterative amortization achieves comparable or lower gaps acrossenvironments. Gaps are estimated using stochastic gradient-based optimization over 100 randomstates. Curves show the mean and ± standard deviation over 5 random seeds.
Figure 7: Iterations During Training. Performance of iterative amortized policy optimization forvarying numbers of iterations during training. Increasing the number of iterations generally resultsin improvements. Curves show the mean and ± standard deviation over 4 random seeds.
Figure 8: Optimizing Model-Based Value Estimates. (a) Performance comparison of direct anditerative amortization using model-based value estimates. (b) Planned trajectories over policy opti-mization iterations. (c) The corresponding estimated objective increases over iterations. (d) Zero-shot transfer of iterative amortization from model-free (MF) to model-based (MB) estimates.
Figure 9: Amortized Optimizers. Diagrams of (a) direct and (b) iterative amortized policy opti-mization. As in Figure 1, larger circles represent probability distributions, and smaller red circlesrepresent terms in the objective. Red dotted arrows represent gradients. In addition to the state, st ,iterative amortization uses the current policy distribution estimate, λ, and the policy optimizationgradient, VλJ, to iteratively optimize J. Like direct amortization, the optimizer network parame-ters, φ, are updated using VφJ. This generally requires some form of stochastic gradient estimationto differentiate through at 〜π(ajst, O; λ).
Figure 10: Value Architecture Comparison. Plots show performance for ≥ 3 seeds for each valuearchitecture (A or B) for each policy optimization technique (direct or iterative). Note: results foriterative + B on Hopper-v2 were obtained with an overly pessimistic value estimate (β = 2.5rather than β = 1.5) and are consequently worse.
Figure 11: Model-Based Value Estimation. Diagram of model-based value estimation (shown withdirect amortization). For clarity, the diagram is shown without the policy prior network, pθ (at |sjThe model consists of a deterministic reward estimate, r(st, at), (green diamond) and a state esti-mate, st+1 |st, at, (orange diamond). The model is unrolled over a horizon, H, and the Q-value isestimated using the Retrace estimator (Munos et al., 2016), given in Eq. 13.
Figure 12:	Per-Step Improvement. Each plot shows the per-step improvement in the estimatedvariational RL objective, J , throughout training resulting from iterative amortized policy optimiza-tion. Each curve denotes a different random seed.
Figure 13:	Comparison with Iterative Optimizers. Average estimated objective over policy opti-mization iterations, comparing with Adam (Kingma & Ba, 2014) and CEM (Rubinstein & Kroese,2013). These iterative optimizers require over an order of magnitude more iterations to reach com-parable performance with iterative amortization, making them impractical in many applications.
Figure 14: 2D Optimization Plots. Each plot shows the optimization objective over two dimen-Sions of the policy mean, μ. This optimization surface contains the value function trained using adirect amortized policy. The black diamond, denoting the estimate of this direct policy, is generallynear-optimal, but does not match the optimal estimate (red star). Iterative amortized optimizers arecapable of generalizing to these surfaces in each case, reaching optimal policy estimates.
Figure 15: Test-Time Gradient-Based Optimization. Each plot compares the performance ofdirect amortization vs. direct amortization with 50 additional gradient-based policy optimizationiterations. Note that this additional optimization is only performed at test time.
Figure 16:	Additional Amortized Test-Time Iterations. Each plot compares the performance ofiterative amortization (trained with 5 iterations) vs. the same agent with an additional 5 iterations atevaluation. Performance remains similar or slightly worse.
Figure 17:	Iterations During Training. (a) Performance and (b) estimated amortization gap forvarying numbers of policy optimization iterations per step during training on HalfCheetah-v2.
Figure 18:	Distance Between Policy Means. Each plot shows the L2 distance between the estimatedpolicy means from two separate policy optimization runs at a given state. Results are averaged over100 on-policy states at each point in training and over experiment seeds.
