Figure 1: Overview. For a given network (left), binarized activations are gathered for the layers Ii, Ijfor each sample, and summarized in the binary database D (right). Rules are discovered over D,where a good rule set M is given with at the bottom right, with rules X → Y , X ∈ Ii , Y ∈ Ij .
Figure 2: Evaluation of rule quality. Top: Performance of EXPLAINN as precision and recall ondata with varying number of planted rules with mutual exclusive head items (left) and co-occurringhead items with varying noise (right). 10% noise corresponds to more noise than signal in the data.
Figure 3: MNIST Average activation of neurons for the classes in MNIST for filter 2 in the firstconvolutional layer. Overlayed are the ExplaiNN rules, where pixel groups of the same colour(e.g. purple pixels top left for classes 2, 3) belong to a single rule spanning multiple classes.
Figure 4: Neurons discriminating Huskies and Malamutes. a) Huskies and Malamutes are verysimilar looking dog breeds. b) Prototypes for rules X → Y discovered for classes X, Siberianhusky (red frame), class Malamute (yellow frame), resp. both (orange frame) and neurons Y inFC7. The neurons associated with both classes represent typical features shared between the twoclasses, those associated only with Siberian huskies show their slightly sharper, more definedhead, while those associated only with Malamutes capture their more fluffy fur.
Figure 5: Information flow for Totem poles. Example from rule cascades found for ImageNet startingat label Totem pole. For each rule X → Y , the group of neurons of the tail Y are used to generatea prototype. To discover rule cascades, we start mining rules between output and FC7 layer. Ruletails, which are neurons of FC7, are then used as heads for rules between FC7 and FC6. Similarly,rule tails are used as heads for rules between FC6 and CONV5.
Figure 6: Example database and model. A toy database D with blocks indicating where the itemsA, B, C, D occur in D, margins and relevant joint counts are given on the right. A sensible rule setM ∪ Mind = A → BC ∪ Mind is given on the right, the part of the database where the rule appliesand holds is indicated by a light respectively dark orange area.
Figure 7: Example of tail error encoding. For a given database D given in a, where blocks indicatethe occurrence of items, a good rule is given byA → BCDE. The part of the database where therule applies is indicated by the orange area. In b we show the part of the transaction were the ruleholds for varying number k of tail items that have to be present in a transaction, from all items on theleft - corresponding to a conjunction - towards just a single item on the right, which correspondsto a disjunction. In c we visualize the error encoding used to transmit the data for k = 3. Wefirst transmit the data where the rule holds, resulting in the area that is indicated by the gray block.
Figure 8: Example of the impact of noise. For a given database D given in a, where blocks indicatethe occurrence of items, a good rule is given byABC → D. Due to high noise, the simple conjunc-tive pattern language results in a bad representation on where the rule should apply, visualized onthe left of b. More relaxed definitions towards disjunctions, where we only require l items of thehead to be present in the transaction, result in much more stable representation on where the ruleapplies.
Figure 9: MNIST prototype. Prototype image for filter 2 from the first convolutional layer.
Figure 10: Filter visualizations. Activation maps (a) for the classes, the prototype of the filter (b),and discovered rules (c), over the whole dataset for filter 36 in the second convolutional layer.
Figure 11: The negative of a digit. Visualizations for filter 12 in the first convolutional layer. Thisfilter seems to capture the ’negatives’ of the handwritten digits.
Figure 12: GoogLeNet results on ImageNet. (a)Visualizations for the rules found between the labelsand the last hidden layer in GoogLeNet. The labels in the rule heads are written above the prototypeimages of the tail unit groups. Each rule tail captures some interesting features of the correspondingclasses: In the first rule the characteristic curly hair of different dog breeds is captured, the secondgroup encapsulates information about the typical colourful plumage of peacocks, the third capturesthe shape of obelisks. We provide example images of the curly haired dog breeds in (b).
Figure 13: Shared information across labels. Visualizations for the rules found between the labelsand the last fully connected layer (FC7). The labels in the rule heads are written above the prototypeimages of the tail unit groups. Each rule tail captures some interesting features of the correspondingclasses: In the first rule the characteristic face of different dog breeds is captured, the second groupencodes information about the arch structures present for both Viaduct and Triumphal arch,the third captures the red beaks surrounded by blackish feather that are shared between differentbirds, and the fourth shows typical heads and tusks of elephants.
Figure 14: The ugly face neurons. From the data for all dog breed categories, EXPLAINN discov-ered the rule between the labels {Japanese spaniel, Pekinese, Shih-Tzu, Lhasa,Affenpinscher, Pug, Brabancon griffon}, and 5 units from the FC7 layer, for whicha prototype is given in the right image. The 5 units together capture the rather characteristic face ofthe mentioned breeds. Visualizing these 5 units one by one, given on the left, gives only little insightabout the encapsulated informationWe discovered many shared traits that the networks is able to pick up across classes, which areencapsulated in groups of neurons in the last layer. For example, there are neurons that capture thered beaks of different birds, arch like structures of buildings, tusks of elephants and as an objectitself, and the ugly face of a whole group of different dog breeds (see App. Fig. 13). Even if wewould visualize all neurons individually and handpick them, without the knowledge about the rule,22Under review as a conference paper at ICLR 2021e.g. for the ugly dogs, it is very hard to interpret what they encode, and only few of them give hintsabout what the information is (see App. Fig. 14). In practice, we only visualize class prototypes,which can be very misleading, as shown in the next section.
Figure 15: The left image shows the visualization for the whole class Great Danes. This visu-alization could not highlight many characteristic features, since there is a large diversity within theclass. On the right side 3 images from the dataset is shown, along with 3 rules that ExplaiNN findsin connection with the class label. We are able to pick up trends, that are not characteristic to thewhole class, but only a subset.
Figure 16: Side dogs. The visualization for the FC6 nodes resembles animals from side view, atypical image positioning for dogs. In the images generated for associated units in the convolutionallayer, we see how the network uses different filters to put this pattern together. The first depictedfilter captures a dog head, typically found at the top left part of the image. The second and third filtersfocus on the legs. In the third row, we can see that other units for these filters are also commonlyactivated with the feature, showing that there are typical positions for legs in both the left and rightlower parts of the image. In the last three prototypes, we recognize horizontal lines, which overallresembles the typical shape and position in the image where we would expect the back of an animal.
Figure 17: Flower visualizations. For rules found between output and last fully connected layer,we visualize the neurons in the tail of the rule for the fine-tuned VGG-S network (first), the originalVGG-S network (second), and example images for the flower classes (right).
Figure 18: Diverse prototypes. Visualized are prototypes for rules found in the VGG-S networkfor ImageNet data between the output and last hidden layer. The class labels corresponding to theoutput are given above each image, the size of the group of neurons that this picture was generatedfrom is given in the bottom right.
Figure 19: More prototypes. Visualized are prototypes for rules found in the VGG-S network forImageNet data between the output and last hidden layer. The class labels corresponding to the outputare given above each image, the size of the group of neurons that this picture was generated from isgiven in the bottom right.
