Figure 1: Synthetic experiments showing the impact of step-size on the performance of AdaGrad,AMSGrad with varying step-sizes, including the default in PyTorch, and the SLS variants.
Figure 2: Comparing optimizers for multi-class classification with deep networks. Training loss (top)and validation accuracy (bottom) for CIFAR-10, CIFAR-100 and Tiny ImageNet.
Figure 3: Sketch of the line-search inequalities.
Figure 4:	Effect of step-size on the performance of adaptive gradient methods for binary classificationon a linearly separable synthetic dataset with different margins. We observe that the large variancefor the adaptive gradient methods, and the variants with SLS have consistently good performanceacross margins and optimizers.
Figure 5:	Runtime (in seconds/epoch) for optimization methods for multi-class classification usingthe deep network models in Fig. 2. Although the runtime/epoch is larger for the SLS/SPS variants,they require fewer epochs to reach the maximum test accuracy (Figure 2). This justifies the moderateincrease in wall-clock time.
Figure 6: Comparing optimization methods on image classification tasks using ResNet and DenseNetmodels on the CIFAR-10/100 datasets. For the SLS/SPS variants, refer to the experimental detailsin Appendix F. For Adam, we did a grid-search and use the best step-size. We use the defaulthyper-parameters for the other baselines. We observe the consistently good performance of AdaGradand AMSGrad with Armijo SLS. We also show the variation in the step-size and observe a cyclicpattern (Loshchilov & Hutter, 2017) - an initial warmup in the learning rate followed by a decrease orsaturation to a small step-size (Goyal et al., 2017).
Figure 7:	Comparing optimization methods on image classification tasks using variants of ImageNet.
Figure 8:	Comparing optimization methods on MNIST.
Figure 9: Comparison of optimization methods on convex objectives: binary classification on LIBSVMdatasets using RBF kernel mappings. The kernel bandwidths are chosen by cross-validation followingthe protocol in (Vaswani et al., 2019b). All line-search methods use c = 1/2 and the proceduredescribed in Appendix F. The other methods are use their default parameters. We observe the superiorconvergence of the SLS variants and the poor performance of the baselines.
Figure 10: Comparison of optimization methods for deep matrix factorization. Methods use thesame hyper-parameter settings as above and we examine the effects of over-parameterization on theproblem: minw^w? Ex〜N(0,1)∣∣ W2W1χ - Axk2 (VasWani et al., 2019b; Rolinek & Martius, 2018).
Figure 11: Ablation study comparing variants of the basic optimizers for multi-class classificationwith deep networks. Training loss (top) and validation accuracy (bottom) for CIFAR-10, CIFAR-100 and Tiny ImageNet. We consider the AdaGrad with AMSGrad-like momentum and do not findimprovements in performance. We also benchmark the performance of AMSGrad without momentum,and observe that incorporating AMSGrad momentum does improve the performance, whereas heavy-ball momentum has a minor, sometimes detrimental effect. We use SLS and Adam as benchmarks tostudy the effects of incorporating preconditioning vs step-size adaptation.
