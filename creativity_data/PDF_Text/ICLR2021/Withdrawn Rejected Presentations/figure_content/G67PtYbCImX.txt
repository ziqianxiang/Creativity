Figure 1: Active learning and search on ImageNet (top) and OpenImages (bottom). Across datasetsand strategies, SEALS with k = 100 performed similarly to the baseline approach in terms of boththe error the model achieved for active learning (left) and the recall of positive examples for activesearch (right), while only considering a fraction of the data U (middle).
Figure 2: Active learning and search on a proprietary dataset of 10 billion images. Across strategies,S EALS with k = 10, 000 performed similarly to the baseline approach in terms of both the errorthe model achieved for active learning (left) and the recall of positive examples for active search(right), while only considering a fraction of the data U (middle).
Figure 3: Measurements of the latent structure of unseen concepts in ImageNet (left) and OpenIm-ages (right). Across datasets, the k-nearest neighbor graph of unseen concepts was well connected,forming large connected components (top) for even moderate values of k. The components weretightly packed, leading to short paths between examples (bottom).
Figure 4: Active learning and search with 20 positive seed examples and a labeling budget of10,000 examples on ImageNet (top) and OpenImages (bottom). Across datasets and strategies,SEALS with k = 100 performs similarly to the baseline approach in terms of both the error themodel achieves for active learning (left) and the recall of positive examples for active search (right),while only considering a fraction of the unlabeled data U (middle).
Figure 5: Active learning and search with 50 positive seed examples and a labeling budget of10,000 examples on ImageNet (top) and OpenImages (bottom). Across datasets and strategies,SEALS with k = 100 performs similarly to the baseline approach in terms of both the error themodel achieves for active learning (left) and the recall of positive examples for active search (right),while only considering a fraction of the unlabeled data U (middle).
Figure 6: Impact of increasing k on ImageNet (|U |=639,906). Larger values of k help to closethe gap between SEALS and the baseline approach that considers all of the unlabeled data for bothactive learning (top) and active search (middle). However, increasing k also increases the candidatepool size (bottom), presenting a trade-off between labeling efficiency and computational efficiency.
Figure 7: Impact of increasing k on OpenImages (|U |=6,816,296). Larger values of k help toclose the gap between SEALS and the baseline approach that considers all of the unlabeled datafor both active learning (top) and active search (middle). However, increasing k also increases thecandidate pool size (bottom), presenting a trade-off between labeling efficiency and computationalefficiency.
Figure 8: Active learning and search on ImageNet with self-supervised embeddings from Sim-CLR (Chen et al., 2020). Because the self-supervised training for the embeddings did not use thelabels, results are average across all 1,000 classes and |U |=1,281,167. To compensate for the largerunlabeled pool, we extended the total labeling budget to 4,000 compared to the 2,000 used in Fig-ure 1. Across strategies, SEALS with k = 100 substantially outperforms random sampling in termsof both the mAP the model achieves for active learning (left) and the recall of positive examples foractive search (right), while only considering a fraction of the data U (middle). For active learning,the gap between the baseline and SEALS approaches is slightly larger than in Figure 1, which islikely due to the larger pool size and increased average shortest paths (see Figure 9).
Figure 9: Measurements of the latent structure of unseen concepts in ImageNet with self-supervisedembeddings from SimCLR (Chen et al., 2020). In comparison to Figure 3a, the k-nearest neighborgraph for unseen concepts was still well connected, forming large connected components (left) foreven moderate values of k, but the average shortest path between examples was slightly longer(right). The increased path length is not too surprising considering the fully supervised model stilloutperformed the linear evaluation of the self-supervised embeddings in Chen et al. (2020).
Figure 10: The per-class APs of SEALS were highly correlated to the baseline approaches (*-All)for active learning on ImageNet (right) and OpenImages (left). On OpenImages with k = 100 and abudget of 2,000 labels, the Pearson’s correlation (ρ) between the baseline and SEALS for the averageprecision of individual classes was 0.986 for MaxEnt and 0.987 for MLP. The least-squares fit had aslope of 0.99 and y-intercept of -0.01. On ImageNet, the correlations were even higher.
Figure 11: SEALS achieved higher APs for classes that formed larger connected components (left)and had shorter paths between examples (right) in ImageNet (top) and OpenImages (bottom).
Figure 12: MaxEnt-SEALS (k = 100) versus MaxEnt applied to a candidate pool of randomlyselected examples (RandPool). Because the concepts we considered were so rare, as is often thecase in practice, randomly chosen examples are unlikely to be close to the decision boundary, and amuch larger pool is required to match SEALS. On ImageNet (top), MaxEnt-SEALS outperformedMaxEnt-RandPool in terms of both the error the model achieves for active learning (left) and therecall of positive examples for active search (right) even with a pool containing 10% of the data(middle). On Openimages (bottom), MaxEnt-RandPool needed at least 5× as much data to matchMaxEnt-SEALS for active learning and failed to achieve similar recall even with 10× the data.
Figure 13: MLP-SEALS (k = 100) versus MLP applied to a candidate pool of randomly selectedexamples (RandPool). Because the concepts we considered were so rare, as is often the case inpractice, randomly chosen examples are unlikely to be close to the decision boundary, and a muchlarger pool is required to match SEALS. On ImageNet (top), MLP-SEALS outperformed MLP-RandPool in terms of both the error the model achieves for active learning (left) and the recall ofpositive examples for active search (right) even with a pool containing 10% of the data (middle). OnOpenimages (bottom), MLP-RandPool needed at least 5× as much data to match MLP-SEALS foractive learning and failed to achieve similar recall even with 10× the data.
Figure 14: ID-SEALS (k = 100) versus ID applied to a candidate pool of randomly selected exam-ples (RandPool). Because the concepts we considered were so rare, as is often the case in practice,randomly chosen examples are unlikely to be close to the decision boundary, and a much larger poolis required to match SEALS. On ImageNet (top), ID-SEALS outperformed ID-RandPool in termsof both the error the model achieves for active learning (left) and the recall of positive examples foractive search (right) even with a pool containing 10% of the data (middle). On Openimages (bot-tom), ID-RandPool needed at least 5× as much data to match ID-SEALS for active learning andfailed to achieve similar recall even with 10× the data.
Figure 15: Active learning and search on Goodreads with Sentence-BERT embeddings. Acrossdatasets and strategies, SEALS with k = 100 performs similarly to the baseline approach in termsof both the error the model achieves for active learning (left) and the recall of positive examples foractive search (right), while only considering a fraction of the data U (middle).
Figure 16: Active learning and search on Goodreads with a labeling budget of 100,000 examples.
Figure 17: Cumulative distribution function (CDF) for the largest connected component in theGoodreads dataset with varying values of k.
