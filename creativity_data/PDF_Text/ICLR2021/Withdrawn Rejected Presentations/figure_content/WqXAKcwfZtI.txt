Figure 1: Domain Adaptation. Alearner is trained on abundant la-beled data and is expected to per-form well in the target domain(marked as +). Decision bound-aries correspond to a 2-layers neu-ral net trained using f -DAL.
Figure 2: f-DAL framework.
Figure 3: Comparison of GDAvs AExG in a toy task with JSas divergence. G is the classof quadratic functions and His linear. AExG can acceleratethe convergence to the optimalsolution. (Appendix E)larger. We refer to this algorithm as Aggressive Extra-Gradient (AExG). We illustrate this in asimple example (Appendix E), whose convergence/trajectories are shown in Figure 3 and we willexplore AExG further in the experimental section.
Figure 4: Transfer performance of a model trained using f -DALfor different choices of divergences and different transfer tasks onthe Office-31 benchmark. Baseline is ResNet-50 w/o f -DAL. Weadditionally show the performance of DANN (Table 2). Whencompared with f-DAL (JS), We see a significant performanceboost. This is in line with our theory which suggests the use of(visual) bag of words representations a per-category domain classifier VS a discriminator.
Figure 5: left) Relative Improvement (%) AExG Vs GDA(SGD) for different choices of divergences and transfertasks on the Office-31 benchmark. Overall, we observe gains in performance among all divergences. right)Transfer curves for Pearson χ2 on the task A→W on the Office-31 benchmark (# Iter vs Acc). We can seeAExG converges faster and also obtains slightly better results. This is inline with the insights obtained fromthe theoretical results presented in Sec 4.1 and Appendix 5.
