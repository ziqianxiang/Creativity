Figure 1: Architecture of the NODE-SELECT graph neural network (NSGNN). Provided a graph, anumber of independent NSGNN filters (3 in this figure) are applied, where each provides a differentstate embedding based on their selection of propagating nodes (in blue). Finally, the output of allthe filters are then summed to create a much richer state embedding for the graph nodes.
Figure 2: Message Propagation in filter. First, the NSGNN filter selects a subset of propagatingnodes based on their global importance. Those selected vertices are represented in blue: V * ={2, 4, 5, 9}. At depth q = 0, only the selected nodes {2, 4, 5, 9} are allowed to share a proportiona(0) of their embedding. At depth q = 1, only the 1-hop neighbors of the initially selected nodesare allowed to share another proportion a(1, of their updated contribute to the message-passing.
Figure 4: (a) Impact of over-smoothing over models’ accuracy. (b) Effect of additional layers onover-smoothing.
Figure 5: Comparisons of distribution of C learned by each filter in a NODE-SELECT model trained on thePubmed dataset. The overall accuracy reached by the model is 89.2%.
Figure 6: Distribution of self-attention weights from a trained NSGNN model on the Cora dataset(random-split).
Figure 7: 3 layers in (a) and (8) layers in (b)14Under review as a conference paper at ICLR 2021Figure 8: 16 NSGNN filters85.5 -85.0 -0	5	10	15	20	25	30Number of layersFigure 9: Number of layers (filters) x Avg. Accuracy15Under review as a conference paper at ICLR 2021求 U- xws⊃wu<82.5 -80.0 -77.5 -75.0 -72.5 -70.0 -0.0	0.2	0.4	0.6	0.8T value
Figure 8: 16 NSGNN filters85.5 -85.0 -0	5	10	15	20	25	30Number of layersFigure 9: Number of layers (filters) x Avg. Accuracy15Under review as a conference paper at ICLR 2021求 U- xws⊃wu<82.5 -80.0 -77.5 -75.0 -72.5 -70.0 -0.0	0.2	0.4	0.6	0.8T valueFigure 10: Threshold (T) x AccuracyFigure 11: Threshold (T) X Size of V *16
Figure 9: Number of layers (filters) x Avg. Accuracy15Under review as a conference paper at ICLR 2021求 U- xws⊃wu<82.5 -80.0 -77.5 -75.0 -72.5 -70.0 -0.0	0.2	0.4	0.6	0.8T valueFigure 10: Threshold (T) x AccuracyFigure 11: Threshold (T) X Size of V *16Under review as a conference paper at ICLR 2021A.7 Effect of thresholdA.8 Embeddings SimilarityAUU ① nbCD」JSUOSμpdluooFigure 12: Histogram of embeddings cosine-similarity.
Figure 10: Threshold (T) x AccuracyFigure 11: Threshold (T) X Size of V *16Under review as a conference paper at ICLR 2021A.7 Effect of thresholdA.8 Embeddings SimilarityAUU ① nbCD」JSUOSμpdluooFigure 12: Histogram of embeddings cosine-similarity.
Figure 11: Threshold (T) X Size of V *16Under review as a conference paper at ICLR 2021A.7 Effect of thresholdA.8 Embeddings SimilarityAUU ① nbCD」JSUOSμpdluooFigure 12: Histogram of embeddings cosine-similarity.
Figure 12: Histogram of embeddings cosine-similarity.
Figure 13: Comparisons of over-fitting vs. good-fitting in trained GCN (a,b) and NSGNN (c,d)Our variant is less affected by over-smoothing. Particularly, the increase of additional parametersframework will generally impact our NODE-SELECT less severely than other GNN. Figure 13below contrasts the training of our NODE-SELECT with GCN on the Cora dataset. On the left,figures 13a and 13c display well-fitted GCN (2 layers) and NSGNN (3 layers/ filters). On the rightside, figures 13b and 13d illustrate the over-fitting in GCN (5 layers) and NSGNN (10 layers).
