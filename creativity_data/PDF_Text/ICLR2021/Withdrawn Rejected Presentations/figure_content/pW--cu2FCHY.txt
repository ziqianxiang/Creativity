Figure 1: AFT blocks require only element-wise and pooling operations.
Figure 2: Comparisons of efficiency between models for a forward and backward pass with batchsizeof4 on a single GPU with 32 GB of RAM.
Figure 3: Comparisons of efficiency between models for decoding sequences.
Figure 4: Proof of concept experiments for AFT-relu2, AFT-relu and AFT-softmax, tested on CI-FAR10. All three versions train well with standard optimization settings. AFT-relu2 and AFT-reluperform similarly, while AFT-relu-softmax and AFT-sigmoid-softmax are more stable and yieldssignificantly better results.
Figure 5: Image completion with test examples.
Figure 6: Upscaled imagesfrom baseline and our 2D lo-cal transformers on CelebA.
Figure 7: Upscaled images from baseline 1D/2D local Image Transformers (Parmar et al., 2018) andour AFT-local2D model trained on CelebA.
Figure 8: Point clouds generated by AFT trained on airplane point clouds.
