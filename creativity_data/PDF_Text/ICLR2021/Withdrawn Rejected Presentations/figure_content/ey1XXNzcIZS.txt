Figure 1: Different tokens are routed to different experts in token-based MoE models(a), whereasthey may be routed to the same expert based on task or some other prior in task-base MoE (b).
Figure 2: Comparing the performance of different routing strategies for Mixture-of-Experts(MoE) models on a massively multilingual dataset - We compare routing experts by tokens, andtasks (using either language pairs or target languages). Given that routing by token on the encoderand routing by task on the decoder performed the best on WMT (Table 1), we use those settingsfor the scaled up 128 expert models we compare. We split the comparison of results into (a) Xx-Enlanguage pairs and (b) En-Xx language pairs. The languages on the x-axis are sorted left-to-rightin descending order of resource size. Best seen in color. Note that the token-level MoE has 6.5Bparameters in the decoders while our task-level MoE has only 200M.
Figure 3: We record the gating decisions of our MoE model trained on internal data on a multiwayparallel dataset. The darker a cell, corresponding to, say en-sr and the 37th expert, the more theexpert is used. In (a) the encoder, tokens from all tasks (Xx-En) seem to prefer the same set of fewexperts slightly over the others; while in (b) the decoder each task (En-Xx) seems to slightly prefer afew experts over the other. Moreover, the set of experts appears to be similar for related languages.
Figure 4: Inference cost analysis. Left, encoders inference cost is small (32ms vs 1565ms) comparedto decoders for a 128 Token-MoE model with sequence lenght 64. Right, We measure the throughputof our Task-MoE model and baseline Token-MoE model across batch sizes and see that the peakthroughput of Task-MoE is 2.6 times higher. In comparison, the peak throughput of Transformer-Big with 473M parameters is 40.3k tokens/sec.
