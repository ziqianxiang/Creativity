Figure 1:	Parameters of classic orthonormal polynomial transforms, up to diagonal scaling. Cosine-III (short for the Discrete Cosine Transform, type III) is formed from the Chebyshev polynomials.
Figure 2:	Algorithms for orthogonal polynomials. The O(n2) algorithms can be parallelized. Inparticular, a version of Clenshaw’s algorithm for evaluation takes O(n) parallel time (Barrio, 2000).
Figure 3: Algorithms for polynomialevaluation and interpolation. All theinputs are vectors in Rn+1. Both algo-rithms return a vector in Rn+1. Eval-uate is the Clenshaw algorithm. Inter-polate is the dual algorithm of Higham(i988). DiVDiffS(x,y) computes thefirst row of the table of divided differ-ences of y with respect to x; see theappendix for its definition.
Figure 4:	These algorithms computethe vector-Jacobian products of Eval-uate and Interpolate. The argumentsu, v ofEvaluate are the correspondingvalues computed during the forwardpass, and U is an alias of c.
Figure 5:	Runtime comparisons of standard (JAX) and cus-tom (CUDA) implementations. The latter is orders of magni-tudes faster than the former, for both the forward and back-ward passes. Presently, algorithms involving sparse updateswithin nested loops are an edge case for compilers. We expectthis situation to improve, but our present investigation wouldnot have been possible without custom implementation.
Figure 6:	Learning both AnyPT and quantization tables(red) achieves lower loss than learning just the tables (blue).
Figure 7: Plots of Σ for classical orthogonal polynomials. These areformed from B = 1024 samples and the correct α,β,μ listed in Figure1, so Σ should be close to identity. The Chebyshev and Legendre poly-nomials behave as desired. The Hermite and Laguerre polynomials sufferin the bottom-right entries involving higher-degree polynomials. This isbecause Hermite and Laguerre μ have unbounded support, along whichhigh-degree polynomials quickly diverge. The Chebyshev and Legendreμ are supported on [-1,1]. f should ideally be bounded.
Figure 8: Mop iteratively draws a batch fromμ, forms the estimates σ and Σ, and descendson their loss. fz (w) is the loss of parame-ters w on the example z. ρ is the prior overw and D is the data distribution of z. μ isa distribution over R. Larger n trades morecomputation for better approximation. Theexpectation Ex is over X ∈ R drawn iid μ.
Figure 10: Mop on toy polynomials. Solidlines are true f *, thick dashed lines areMop using LKL, and dotted lines use LF .
Figure 11: Mop fails todistinguish parity data(colored lines) fromnoise (gray lines), nomatter the value of n.
Figure 9: Mop recover-ing β* for different Clas-sical polynomials. Thickdashed lines use LKL .
Figure 12: Variants of Evaluate andInterpolate for orthonormal polyno-mial sequences. As in Interpolate,DiVDiffs is the first row of the tableof divided differences.
Figure 13: Transpose Vandermonde multiplication for monic orthogonal (left) and orthonormal(right) polynomial sequences. These use O(n2) time using O(n) space. They are used in thefollowing algorithms for VJPs.
