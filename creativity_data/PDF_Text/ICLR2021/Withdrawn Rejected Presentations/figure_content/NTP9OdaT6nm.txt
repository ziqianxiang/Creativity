Figure 1: (a) Illustration of the formal language constraint framework operating through time. Stateis carried forward through time by both the MDP and the recognizer, DC. (b) No-1D-ditheringconstraint employed in the Atari and MUJoCo domains: .* ('r)2âˆ£(r ')2 (note, all unrepresentedtransitions return to q0).
Figure 2: Performance/conformance curves in selected Safety Gym environments with Pareto frontiersplotted per reward shaping method. We observe that using state augmentation (green) consistentlyoutperforms the baseline (blue) at all levels of reward shaping, which are anti-correlated with episodiccost and episodic return. The use of cost shaping (purple) produces gains in return at a given amountof cost only at small reward shaping values correlating to high return and cost. Consequently, thecombination of state augmentation and cost shaping inherits this behavior of being more effectivenesswhen cost/return are higher. The full set of plots is included in Appendix C.
