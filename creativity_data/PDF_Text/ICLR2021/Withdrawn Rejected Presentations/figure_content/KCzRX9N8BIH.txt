Figure 1: Optimizing likelihood parameters adapts the loss without manual hyperparameter tuningto balance accuracy and certainty.
Figure 2: Illustration of an image classifier withthree different types of likelihood temperatureconditioning: global, predicted, and data. EaChrepresents a different way to parametrize themodel's temperature.
Figure 3: An image loss function with threedifferent likelihood parameter dimensionalities.
Figure 4: A synthetic logistic regression experiment. Regressing softmax temperature reduces theinfluence of outliers (blue, bottom-left), by locally raising temperature. Thejointly optimized model(center and right panel) achieves a more accurate classification that a model trained without adaptivetemperature (left panel).
Figure 5: The data with the lowest (top) and highest (bottom) predicted temperatures in the SVHNdataset. High temperature entries are blurry, cropped poorly, and generally difficult to classify.
Figure 6: Distribution of Outlier Detection AUCacross the ODDS Benchmark. Our approaches,PCA+S and AE+S, are competitive with otherOutlier Detection systems.
Figure 7: Performance of L2 (left) and L1 (middle) regularized linear regression on a 500 dimen-Sional synthetic dataset where the true parameters, w*, are known. Dynamic Ridge (D-Ridge)and D-LASSO regression find the regularization strength that best estimates the true parameters.
