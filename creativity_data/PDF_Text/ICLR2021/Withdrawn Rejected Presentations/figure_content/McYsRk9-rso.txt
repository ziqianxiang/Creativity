Figure 1: Changes in performance without access to domain labels, relative to the multi-domainbaseline of learning an individual baseline on each dataset (sizes in %). For ResNet26 (•), perfor-mance losses on the smallest domains (yellow) are significant. Our proposed solutions (•) recover alarge portion of the gap to full domain-supervison. Best viewed in color.
Figure 2: Left: PCA over gate activation paths. DRA partitions the data into separate latent domainsin a semantically meaningful way: Omniglot (•) occupies a distinct region and also differs visuallyfrom the other datasets. Svhn (•) and Gtsrb (•) cluster together — both datasets contain real-world,but well-defined symbols (house numbers, street signs). Right: the effective distance δη that governsthe amount of exchange in WDT for each latent domain. Omniglot shares little information withother domains, Dtd (•) shares more actively.
Figure 3: Left: PCA of samples represented by their L-dimensional activation paths. DRA sharesparameters between visually similar domains art painting and photo (•,•), while isolating sketch (•).
Figure 4: Graphical models for (a) multi-domain learning, (b) latent domain learning. (c) A ResNetblock, equipped with a dynamic residual adapter (K = 2). Incoming samples x pass down threestreams: an identity function, a transformation via a large convolution f, as well as an evaluation byexpert gates g, which dynamically assigns (dashed arrows) small corrections h1 and h2.
Figure 5: PCA over ResNet26. Only Omniglot is separated from other latent domains.
