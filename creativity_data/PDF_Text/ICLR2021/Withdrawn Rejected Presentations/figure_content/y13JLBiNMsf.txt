Figure 1: Input speech waveform (a) and attention plots between speech waveform and transcription learnedby (b) soft attention, (c) GMM attention, and (d) SAGMM-tr attention.
Figure 2: Schemes of (a) conventional GMM and (b) SAGMM attention score calculation which are similarto the integral of the normal distribution on uniform and non-uniform axis spacing, respectively.
Figure 3: Attention plots for single-head SAGMM-tr model with a speech utterance with a long silence.
Figure 4: Encoder self-attention mask between query (vertical axis) and key (horizontal axis) in M = 3case.
Figure 5: Attention alignments plots from multi-head truncated GMM model.
Figure 6: Attention alignments plots from multi-head soft attention model.
Figure 7: Attention alignments plots for single-head SAGMM-tr for the utterance “Chapter eleven [longPaUseC the morrow brought a very sober looking morning the SUn making only a few efforts to appear andCatherine augured from it everything most favourable to her wishes” in LibriSpeech. The attention plot intime domain longer than the original wave due to 1-vector concatenation for EOS 2.4Figure 8: Attention alignments plots from multi-head SAGMM-tr model with adaptive window width. Theempty plots denote the pruned head in the first layerFigure 9: Attention alignments plots from multi-head SAGMM-tr attention model with fixed window width.
Figure 8: Attention alignments plots from multi-head SAGMM-tr model with adaptive window width. Theempty plots denote the pruned head in the first layerFigure 9: Attention alignments plots from multi-head SAGMM-tr attention model with fixed window width.
Figure 9: Attention alignments plots from multi-head SAGMM-tr attention model with fixed window width.
Figure 10: Encoder-decoder attention alignments plots from multi-head SAGMM-tr model with bi-directional encoder in machine translation experiment.
Figure 11: Encoder self-attention alignments plots from multi-head SAGMM-tr model with uni-directionalencoder in machine translation experiment.
Figure 12: Encoder-decoder attention alignments plots from multi-head SAGMM-tr model with uni-directional encoder in machine translation experiment.
