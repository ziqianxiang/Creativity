Figure 1: Multi-hop attention diffusion. Con-sider making a prediction at nodes A and D.
Figure 2: MAGNA Architecture. EachMAGNA block consists of attention computa-tion, attention diffusion, layer normalization,feed forward layers, and 2 residual connec-tions for each block. MAGNA blocks can bestacked to constitute a deep model. As illus-trated on the right, context-dependent attentionis achieved via the attention diffusion process.
Figure 3: Analysis of MAGNA. (a) Influence of MAGNA on Laplacian eigenvalues. (b) Effect ofdepth on performance. (c) Effect of hop number K on performance. (d) Effect of teleport probabilityÎ±.
Figure 4: Attention weightson Cora dataset.
