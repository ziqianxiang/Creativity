Figure 1: The Skill-Action (SA) Architecture(b) Network Architecture of SAIn this section, we define the dynamics (Markov process) of SA. We first introduce MDP notations.
Figure 2: Episodic Returns. X-axis is time step. Y-axis is Episodic ReturnIt is extremely interesting that SA shows two completely different kinds of behaviors on infinite andfinite horizon environments. According to previous option framework implementations (Klissarov2All implementations are from DAC’s open source repo https://github.com/ShangtongZhang/DeepRL/tree/DAC.
Figure 3: Skill Duration PatternsOn infinite horizon environments as shown in Figure 2 (a), SA’s performance significantly outper-forms all baselines by a large margin in various aspects. For episodic return, e.g., HumanoidStandup,all option implementations barely converge, while SA is 240% better than DAC and AHP4. For con-vergence, SA has the fastest convergence speed. On the first two environments, which are alsoreported in DAC, SA only takes 40% of time steps of DAC and AHP to reach similar episodic re-turns. This acceleration is because: 1) SA is MDP formulated, the skill policy is updated at eachtime step; 2) SA only has one action policy decoder; 3) the action decoder learns to decode skillcontext vectors whichever skill is activated. For stability, all 10 runs of SA converges to a similarlevel while the other have much larger standard deviations. This property is theoretically justifiedby Proposition 3.5 and further discussed in Appendix A.1.
Figure 4: Interpretation of Skill Context Vectors4.4	Transfer LearningWe follow DAC (Zhang & Whiteson, 2019) and run 6 pairs of transfer learning tasks constructed inDAC based on DeepMind Control Suite (Tassa et al., 2020). Each pair contains two different tasks.
Figure 5: Performance on DAC transfer learning tasksOn the transfer learning (the second) task, SA’s performance ranks the first in 5 out of 6 environ-ments. This shows SA’s advantages on reusing experience and knowledge across tasks. On the firsttask, SA’s performance is also among the best algorithms in all environments. This further validatesSA’s advantage on single task as observed in section 4.1.
Figure 6: Performance of Ten OpenAI Gym MuJoCo Environments.
Figure 7: Duration of 4 options during 430 training episodes of HalfCheetah.
Figure 8: Activated option sequences of 4 independent HalfCheetah runs.
Figure 9: Heatmap of all 4 skill context vectorsAs the first step, we follow Sabour et al. (2017) to interpret what property each dimension of theskill context vector in Figure 9 encodes by perturbing each dimension and decode perturbed skillcontext vectors into primary actions. Specifically, we perturb one dimension by adding a range ofperturbations [-0.1, 0.09] by intervals of 0.01 onto it while keep the other dimensions fixed. Afterperturbation, each skill context vector dimension has 20 perturbed vectors. We then use the actionpolicy decoder to decode all those vectors into primary actions and see how the perturbation affectsthe primary action. As an illustration, we plot Dimension 0’s all 20 perturbed results in Figure 10.
Figure 10: Perturbation on the Dim 0With visualization of perturbation results in hand, we can interpret what property each dimensionencode by inspecting relationships between perturbations and primary actions. In Figure 10, as anexample, it is clear that changes on Dim 0 has opposite effect on the back leg and front leg: a largervalue on Dim 0 will assign the back leg a larger torque while the front leg a smaller one, and viceversa. This means Dim 0 is has a focus point property: it focuses torque on only one leg.
Figure 11: Interpretation of Skill 1 and Skill 2Subfigures in Figure 11 can be interpreted in the same manner as Figure 10. As an example, fromFigure 9 we can see that Skill 1 has a significant small value on Dim 11. In Figure 11, it shows thata smaller Dim 11 will twist the front leg forward and back foot forward while twist back thigh, backshin backward. Composition of these movements is a back leg landing property. Similarly, we caninterpret that Dim 15 is a front leg landing property and Dim 22 is a balancing property. Therefore,Skill 1 is focusing on landing from all positions.
Figure 8: as an all-weather skill, Skill 2 is the most frequently executed one and has the longestduration. From time to time, when the Cheetah needs to land and balance itself, Skill 1 will beexecuted. However, since landing skill does not provide power of moving forward and thus haslower returns to continue, once the body is balanced the Cheetah will quickly stop Skill 1’s executionand keep running with Skill 2.
Figure 12: An Illustration of the SMDP Option Framework. An option ot-1 is selected by masterpolicy P (ot-1 |st-1) at time step t - 1. At time step t, termination function βot-1 (st) determinesto continue option ot-1. So that there is no random variable ot at time step t compared to there arerandom variables o at every time step in MDP formulation (figure 13).
Figure 13: PGM of the MDP Option FrameworkThe termination policy distribution P(bt|st, ot-1) : S × O → B can be formulated as a mixturedistribution6 conditioned on option vector (the one-hot vector) ot-1 and state st .
