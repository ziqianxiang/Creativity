Figure 1: In this visual-thermal pair, the two inputs are displaced (visualised in Figure 9), a phe-nomenon which often occurs in real-world scenarios. This is a failure case even for state-of-the-arts(Pixel-to-Pixel SR de Lutio et al. (2019) and DeepJF Li et al. (2017)), as they are based on perfectalignment. CMSR adapts to the given misalignment, and corrects it as a part of the SR process,producing a sharp result (41.069 dB PSNR). The deformed RGB image is presented in Figure 9captured by different sensors placed in the exact same position, and taken at the exact same time. Asto be shown, in real-life scenarios perfect alignment of multiple sensors is often hard to achieve. Inour work, we present new means to allow the two modalities to be moderately misaligned, namelyweakly aligned. Our network contains a learnable deformation component that implicitly alignsdetails in the two images together. More specifically, our architecture includes a deformation modelthat aligns details from the RGB image to the target modality in a coarse-to-fine manner, before theyare fused together. The network does not use any explicit supervision for the deformation sub-task,but rather optimizes the deformation parameters to adhere to the super-resolution goal. Figures 1and 7 present cases where a weakly aligned pair causes state-of-the-arts methods to fail, whereasour method produces high-quality super-resolved output.
Figure 2: CMSR successfully ignores details that appear only in the RGB image (a standing man).
Figure 4: Training process. The RGB image first goes through a deformation step which aligns it tothe target modality (in blue). Then, random patches are selected by an augmentation step (in Red)and down-sampled (in green). The patches are used to train the CMSR network (in orange) and thedeformation parameters. The loss function is measured between the super-resolved output and theinput target modality images.
Figure 3: CMSR performs three-waysummation; two of the resulting fea-ture maps, one from each modality,are summed together with the origi-nal modality input that is naively up-sampled, in a residual manner.
Figure 6: We compare our method to its baseline method, ZSSR (Shocher et al. (2017)), as well as toanother cross-modality method, VTSRGAN (Almasri et al. (2018a)) on a visual-thermal pair fromthe ULB17-VT evaluation. On the right, the output of Feature-Extractor 2 (Figure 3) is given asthe learned RGB residual which is added to our output. This RGB residual resembles an edge-map;it is artifact-free and contains no unwanted textures3.3	InferenceAt inference time, we use the trained CMSR network anddeformation parameters, to perform SR on the entire tar-get modality image guided by the RGB modality image(see Figure 5).
Figure 5: Inference. During inference,the learned deformation parameters andthe CMSR component are used to up-sample the original LR modality inputimage, guided by the HR RGB input im-age.
Figure 7: This misaligned visual-depth pair is taken from our Shuffled-Middlebury dataset evalu-ation. Compared to cross-modality state-of-the-arts Pixel-to-Pixel SR (de Lutio et al. (2019)) andDeep Joint Filtering (Li et al. (2017)), who struggle to produce a clear result, our method succeedsin this task (32.239 dB PSNR / 0.9403 SSIM) thanks to its alignment capabilities.
Figure 8: We evaluated CMSR using different trans-formation layers. In the leftmost column, the resultingdeformed RGB image is given. In the other columnswe show the resulting alignment, visualized throughblending of the R-G (Red-Green) channels of the afore-mentioned deformed RGB image, together with theGround-Truth thermal image.
Figure 9: We evaluated CMSR on aseverely misaligned visual-thermal pair,(a) and (b), with both global and local dis-placements. We overlaid the images, oncebefore training (c), and once after trainingthe network (d). CMSR deformed its RGBinput on-the-fly to better alignment, rely-ing solely on our SR loss.
Figure 10: We compared CMSR both to its single-modality baseline, ZSSR, (Shocher et al. (2017))and to competing cross-modality methods, VTSRCNN, VTSRGAN (Almasri et al. (2018a)) andDeep Joint Filtering, (Li et al. (2017)) on the NIR modality, in the task of x 4 SR. Our method,CMSR, is able to produce better super-resolved images visually and numerically, despite not beingpreviously trained.
Figure 11: The visual-depth pairs from theMiddlebury dataset (top row) and the visual-thermal pairs from the ULB17-VT dataset (bot-tom row) show strong multi-modal registration.
Figure 12: Two examples of Weakly Alignedmodality pairs. To visualize the misalignment,we overlaid them with semi-transparency. Note,the ghosting effect where cross-modal misalign-ment occurs.
Figure 13: We let CMSR perform4x SR on a Weakly Aligned visual-thermal pair, with different transfor-mation layers, averaged across 5 runs.
Figure 14: CMSR uses its RGB input conservatively.
