Figure 1: VAE KL divergence undergoes as a phase transition as the model as we decrease the amountof smoothing to recover the original model at ρ = 1. The figure shows top layer KL divergence vs. ρ,where each point is a different model after 100000 steps of training with a fixed ρ. The plots showthe KL divergence values for the original unsmoothed model after training on the smoothed version.
Figure 2: Training 4 layer VAE models on static MNIST with validation ELBO on the left andKL(q(z4|z3)||N (0, 1)) on the right. The VAE shows posterior collapse, while HVAE avoids italongside improved validation ELBO.
Figure 3: VAEs (above) exhibit suddenly many inactive units when increasing the depth to 2 layersand the width beyond 20 neurons, while collapsing completely after 3 layers and 60 neurons. Theproposed Hermite VAEs (below) retain for nearly all cases full latent activity, while maintainingsimilar ELBOs (not shown). Trained on MNIST for 100K steps for practical reasons (normalconvergence if left training), with 2 deterministic, 200-neuron tanh layers before every stochasticlayer.
