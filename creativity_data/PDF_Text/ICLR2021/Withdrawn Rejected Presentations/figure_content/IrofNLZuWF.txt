Figure 1: We empirically evaluate the second moment (in blue) and variance (in orange) of stochasticgradients during the training of neural networks. We observe that the magnitude of these quantitieschanges significantly as iteration count increases, ranging from 10 times (ResNet) to 106 times(Transformer). This phenomenon motivates us to consider a setting with non-stationary noise.
Figure 2: Left: The injected noise intensity over iterations. Middle:Average loss trajectory over10 runs for four different algorithms: standard baseline, idealized baseline, Alg 1 and Alg 2. Thecurve (idealized vs standard) confirms that adopting step sizes inverse to the noise level lead to fasterconvergence and less variations. Right: Average and standard deviation of function suboptimality.
Figure 3: The left two plots show the accuracy of training ResNet18 on Cifar10 dataset. The righttwo plots present the negative log-likelihood loss for LSTM Language modelling from Merity et al.
Figure 4: We retrain the models with Adam optimizer and evaluate the second moment (in blue)and variance (in orange) of stochastic gradients for the Cifar10 experiments and PTB experiments inFigure 1.
