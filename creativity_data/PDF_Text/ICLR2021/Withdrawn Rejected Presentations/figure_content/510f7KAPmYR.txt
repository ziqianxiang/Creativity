Figure 1: Left - adding a new weight connection from N01 to Nii. Center - adding a placeholdernode N22 to the network by adding connections to N30 and N31. Right - forming a new layer withN40, N44 without affecting the network,s outputs.
Figure 2:	Dynamic search space granularity that varies from layer-level operations down to individualweights and biases.
Figure 3:	Network architecture template consisting of variable convolutional layers between reduc-tions followed by variable linear layers.
Figure 4: Empirical loss and test accuracy results after 500 epochs of training for evolutionarynetworks constrained to 0.5, 1.0, 2.0, and 5.0 million parameters.
Figure 5: Training results compared to a fixed architecture VGG-like network and a NAS algorithmthat randomly adds new parameters, and training results with various architecture learning rates.
