Figure 1: Deep Free Energy Network (FENet) calculation process. The left side shows how tocalculate the Free Energy using data at hand. The right side shows how to calculate the ExpectedFree Energy for RL with latent imagination.
Figure 2:	Comparison of FENet to PlaNet and ”PlaNet with demonstrations”. Plots show test per-formance over learning iterations. The lines show means and the areas show standard deviationsover 10 trajectories.
Figure 3:	Comparison of FENet to PlaNet and ”PlaNet with demonstrations” in sparse-reward set-tings, where agents do not get rewards less than 0.5. Plots show test performance over learningiterations. FENet substantially outperforms PlaNet. The lines show means and the areas show stan-dard deviations over 10 trajectories.
Figure 4: Comparison of FENet to imitation learning methods when only suboptimal experts areavailable in Cheetah-run. Plots show test performance over learning iterations. Behavioral Cloningimitation methods cannot surpass the suboptimal expert’s return which FENet successfully sur-passes. The lines show means and the areas show standard deviations over 10 trajectories.
Figure 5: Comparison of FENet (imitation RL) to other learning strategies (ablation studies:imitation-pretrained RL, RL only, and imitation only with FENets). Plots show test performanceover learning iterations. The lines show means and the areas show standard deviations over 10trajectories.
Figure 6: Image-based control tasks used in our experiments.
