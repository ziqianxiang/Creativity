Figure 1: Target modification includes OR (LS and CP), and LC (SeIfLC and Non-selfLC). Assumethere are three training classes. q is the one-hot target. U is a uniform label distribution. P denotesa predicted label distribution. The target combination parameter is âˆˆ [0, 1].
Figure 2: Comparison of setting using different schemes. Experiments are done on CIFAR-100with asymmetric label noise r = 0.4. For data-dependent items, mean results are reported.
Figure 3: Comprehensive learning dynamics on CIFAR-100 with asymmetric label noise r = 0.4.
Figure 4: Learning dynamics on CIFAR-100 under asymmetric noisy labels. We show all iterationsonly in (a) and (d). In the others, we show the second half iterations, which are of higher interest. Asthe noise rate increases, the superiority of ProSelfLC becomes more significant, i.e., avoiding fittingnoise in the 2nd row and leading to better generalisation in the 1st row.
Figure 5: The changes of entropy statistics and ProSelfLC at training. We store a model every 1000iterations to monitor the learning process. For data-dependent metrics, after training, we split thecorrupted training data into clean and noisy subsets according to the information about how thetraining data is corrupted before training. Finally, we report the mean results of each subset.
