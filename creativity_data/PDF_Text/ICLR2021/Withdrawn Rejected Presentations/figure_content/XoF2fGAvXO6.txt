Figure 1: Example (passage, query, answer) from DROP and outline of our method: executing noisyprogram obtained from dependency parsing of query by learning date/number entity specific crossattention, and sampling and execution of discrete operations on entity arguments to reach the answer.
Figure 2: Modeling the interaction between the passage and (left) the program, & (right) its num-ber/date entities. For each program step k, they respectively yield (i) Stacked Span Prediction Logitsand (ii) Attention over Number/Date entities for each passage token. The linear combination of thesetwo gives the expected distribution over entities, Tknum and Tkdate for the step kStacked Span PredictionLogit for each program stepBERT Contextualized PassageEncoding for program step ⅛=1Passage toNumber/DateAttention for eachprogram stepInductive Bias JorEntity SpecificInformation Retrieval1 is X1 = (‘which is the longest’); X2 = (‘goal by Carpenter’, X1); Answer = Discrete-Reasoning(‘which isthe longest’, X2). Each program step consists of two types of arguments (i) Query Span Argumentobtained from the corresponding node, indicates the query segment referred to, in that program stepe.g., ‘goal by Carpenter’ in Step 2 (ii) Reference Argument(s) obtained from the incoming edges tothat node, refers to the previous steps of the program that the current one depends on e.g., X1 in Step
Figure 3: Operator & Argument Sampling Network and RL framework over sampled discrete actionsRewardfunction•	Entity-Type Predictor Network, an Exponential Linear Unit (Elu) activated fully-connected layerfollowed by a softmax that outputs the probabilities of sampling either date or number types.
Figure 4: t-SNE plot of DROP-num-Test questions.
Figure 5: (a) Training trend showing the Recall@top-k and all actions, accuracy of Operator andEntity-type Predictor, estimated based on noisy psuedo rewards (Appendix A.6), (b) Module-wiseperformance (using pseudo-reward) on DROP-num-Test, (c) Bucketing performance by total numberof passage entities for WNSMN, and the best performing NMN and GenBERT model from Table 1.
