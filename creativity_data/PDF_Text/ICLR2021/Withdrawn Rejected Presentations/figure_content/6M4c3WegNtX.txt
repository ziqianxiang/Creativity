Figure 1: t-SNE visualization: fivedifferent architectures, each trainedwith 20 different initializations.
Figure 2: Illustration of one iteration of NES-RE. Network architectures are represented as coloredbars of different lengths illustrating different layers and widths. Starting with the current population,ensemble selection is applied to select parent candidates, among which one is sampled as the parent.
Figure 3: NLL vs. ensemble sizes on CIFAR-10, CIFAR-100 and Tiny ImageNet with and withoutrespective dataset shifts (Hendrycks & Dietterich, 2019) over DARTS search space.
Figure 4: NLL vs. budget K on CIFAR-10/100 and Tiny ImageNet with and without respectivedataset shifts over the DARTS search space. Ensemble size is fixed at M = 10.
Figure 5: ECE vs. dataset shift severity on CIFAR-10,CIFAR-100 and Tiny ImageNet over the DARTS searchspace. No dataset shift is indicated as severity 0. Ensem-ble size is fixed at M = 10.
Figure 7: Ensemble, average base learner and oracle en-semble NLL for ImageNet-16-120 on the NAS-Bench-201search space. Ensemble size M = 3.
Figure 6: Average base learner loss and oracle ensemble loss (see Section 3 for definitions) onCIFAR-10/100 and Tiny ImageNet. Recall that small oracle ensemble loss generally corresponds tohigher diversity. These findings are qualitatively consistent across datasets and also over shifted testdata. See Appendix C.1.
Figure 7: Results on Fashion-MNIST with varying ensembles sizes M . Lines show the mean NLLachieved by the ensembles with 95% confidence intervals.
Figure 8: Average base learner loss for NES-RS, NES-RE and DeepEns (RS) on Fashion-MNIST.
Figure 9: Oracle ensemble loss for NES-RS, NES-RE and DeepEns (RS) on Fashion-MNIST. Linesshow the mean NLL and 95% confidence intervals.
Figure 10: Entropy of predicted probabilities when trained on CIFAR-10 over the DARTS searchspace.
Figure 11: t-SNE visualization: predictions of base learners in two ensembles, one with fixedarchitecture and one with varying architectures.
Figure 12: NLL vs. ensemble sizes on CIFAR-10, CIFAR-100 and Tiny ImageNet with varyingdataset shifts (Hendrycks & Dietterich, 2019) over DARTS search space.
Figure 13:	Classification error rate (between 0-1) vs. ensemble size on DARTS search space.
Figure 14:	Average base learner and oracle ensemble NLL acrosson CIFAR-10 over DARTS search space.
Figure 15: Average base learner and oracle ensemble NLL across ensemble sizes and shift severitieson CIFAR-100 over DARTS search space.
Figure 16: Average base learner and oracle ensemble NLL acrosson Tiny ImageNet over DARTS search space.
Figure 17:	Ensemble NLL vs. budget K. Ensemble size fixed at M = 10.
Figure 18:	Ensemble error vs. budget K. Ensemble size fixed at M = 10.
Figure 19: High fidelity NLL vs. budget K on CIFAR-10 and CIFAR-100 with and without respectivedataset shifts over the DARTS search space. Ensemble size is fixed at M = 10.
Figure 20: High fidelity classification error vs. budget K on CIFAR-10 and CIFAR-100 with andwithout respective dataset shifts over the DARTS search space. Ensemble size is fixed at M = 10.
Figure 21:	Results on CIFAR-10 (Hendrycks & Dietterich, 2019) with varying ensembles sizesM and shift severity. Lines show the mean NLL achieved by the ensembles with 95% confidenceintervals. See Appendix C.1.3 for the definition of NES-RE-0.
Figure 22:	Loss vs. ensemble size for NES and deep ensembles (with/without ensemble selectionover initializations). The left plot shows that NES-RE outperforms all other methods across ensemblesizes. The right plot shows that ensembles produced by NES algorithms also consistently have higherdiversity (as indicated by smaller oracle ensemble loss). See Appendix C.3 for details.
Figure 23:	Plots show NLL vs. ensemble sizes comparing NES to the baselines introduced inAppendix C.4 on CIFAR-10 and CIFAR-100 with and without respective dataset shifts (Hendrycks &Dietterich, 2019).
Figure 24:	Average base learner loss and oracle ensemble loss for NES and the baselines introducedin Appendix C.4 on CIFAR-10 and CIFAR-100. Recall that small oracle ensemble loss generallycorresponds to higher diversity.
