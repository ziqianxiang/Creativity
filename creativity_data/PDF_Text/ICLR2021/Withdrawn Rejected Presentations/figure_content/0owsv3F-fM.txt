Figure 1: Illustration of mapping functions with and without sequential structure from the targetdomain (left) to the source domain (right). Shaded regions denote data distributions, where thedarker the color, the higher the probability. In Fig. 1(a), both st and s0t are “realisic” source domaininstances, but only st is correct. Since they are of similar probabilities, distribution matching maymap ot to any of them. In RL, the policy may output unreliable actions when taking these incorrectlymapped states as inputs. In Fig. 1(b), a sequential structure can help rule out the wrong mappingbased on trajectory contexts.
Figure 2: Illustration of the generation in the real world and simulation domains respectively. Allnodes are random variables. Shaded nodes are observable variables. The solid line denotes the gen-eration process and dashed lines denote the inference process. There is an unpaired correspondencebetween two state trajectories surrounded by the rounded rectangle. Note that we include policy πin both generation process, which corresponds to the edge from state st to action at .
Figure 3: Model structure with embed-DMDM is first trained using batches of transition tuplescollected in the simulator. During the training pro-CeSS of the mapping function, the dynamics model is trained online using D@ = {(^t, at, st+ι 〜p(s0∣^t, at)} and is periodically updated to the mapping function. That is, We reset the simulator tothe mapped states ^t and then rollout with a one-step oracle simulation transition to get st+ι. Theoptimization objective of the dynamics model is an MSE loss in Eq. 8. Algorithm 2 demonstrates adetailed training and updating procedure of the dynamics model and the entire mapping function.
Figure 4: A visual illustration of (a) original images (b) reconstructed images (c) re-rendered imagesby setting the simulator to mapped states.
Figure 5: Training Curves of different methods on Cross-modal domain adaptationmapping images to a predefined interpretable and meaningful state spaCe is more diffiCult than end-to-end poliCy learning.
Figure 6: Ablation studies by removing embed-DM and RNN in CODASSinCe CODAS Contains multiple ideas, we ConduCt additional experiments to understand the Contri-bution of eaCh Component to the overall performanCe. We ConduCt ablation studies to demonstrate(1) if sequential struCture matters in the Cross-modal domain adaptation in RL and (2) if embed-DM8Under review as a conference paper at ICLR 2021improves the long-horizon inference. Since the embed-DM is based on sequential structure, we onlytest on two ablated variants, i.e., CODAS w/o embed-DM and w/o both embed-DM and RNN.
Figure 7: Illustration of full netWork structure. Blue parts denote mapping function qφ ; YelloWand green parts denote reconstruction function pθ . Green part is alWays fixed in the entire trainingprocess.
Figure 8: Examples of rendered images of MuJoCo environments(a) Pendulum(b) HalfCheetah17Under review as a conference paper at ICLR 2021D Extra Experiment ResultsD. 1 Average reward ratio of all environmentsMethod CODAS(ours) GAN BC(max) BC(final)Reward Ratio ∣	70.1%	∣ 42.8% ∣ 47.83% | 34.91%D.2 Performance of the Optimal PolicyTable. 3 shows the mean value and standard deviation of the un-discounted cumulative return of100 trajectories collected by the optimal policy trained on states using PPO. The maximum episodelength is set to 1000. Full training curves of policies trained on state space are provided in Fig. 9.
Figure 9: Training curves of policy on state space and image space.
Figure 11: Reward ratio in Hopper with small dynamics mismatches.
Figure 12: Training curves of CycleGAN.
Figure 13: Training curves of GAN with action loss.
Figure 14: An example of failed distribution matching under a linear transformation.
