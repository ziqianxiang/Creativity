Figure 1: Training and interpretation of a Potts model on a single protein family. The proteins areall loops formed by one blue and one yellow amino acid locking together. The MSA for this familyaligns these critical yellow and blue amino acids. For the trained MRF on this MSA, the weightmatrix W4,1 has the highest values due to the evolutionary constraint that blue and yellow covaryfor those positions. In this case, the highest predict contact recapitulates a true contact.
Figure 2: Predicted contact maps and Precision at L for each model on PDB entry 2BFW. Blueindicates a true positive, red indicates a false positive, and grey indicates a false negative.
Figure 3: Model performance evaluated on MSA depth and reference length.
Figure 4: Contact precision for all models stratified by the range of the interaction.
Figure 5: Examining impact of number of heads on precision at L/5. Left: Comparing performanceof Potts and 128 heads over each family shows comparable performance. Right: Precision at L/5drops off slowly until 32 heads, then steeply declines beyond that.
Figure 6: Factored attention with 4 heads(b)	Training dynamics of models on 3n2a.
Figure 7: A single set of frozen valuematrices can be used for all families.
Figure 8: The tree on the right depicts evolution of a protein family. The protein at the root is theancestral protein, and the five proteins at the leaves are its present-day descendants. The alignmenton the left is the corresponding Multiple Sequence Alignment of observed sequences.
Figure 9: MSA for sequences from Figure 8 compared to a padded batch of the same sequences.
Figure 10: The length and MSA size distribution for our 748 family subset (red) compared to thefull 15,051 families in the trRosetta dataset selected for trainingA.6.2 Producing Contact MapsA PDB structure gives 3D coordinates for every atom in a structure. We use Euclidean distancebetween the beta carbons to define distance between any pair of positions. A pair of positions wherethis distance is less than 8A is declared to be a contact.
Figure 11:	The total number of contacts for a structure as a function of protein length follows alinear trend. (slope = 2.64, R2 = 0.929)18Under review as a conference paper at ICLR 2021S 0.8 -ra+-»raQ∏3①(ΛS. 0.6-+-»cS①-g"1Λ能 0.4-
Figure 12:	The empirical CDF of number of per-residue contacts for 3,747,101 residues in 15,051structures in the trRosetta dataset.
Figure 13:	Reducing the number of heads causes a much steeper decrease in precision at L.
Figure 15: Effect of head size on factored attention precision at L and L/5 over 748 families.
Figure 14: Effect of number of heads on correlation between the order-4 weight tensors for factoredattention (see Equation 4) and Potts (see Section 3).
Figure 16: 4 heads has degraded performance for precision at L.
Figure 17:	Factored attention trained with a single set of frozen value matrices performs comparablyto Potts, evaluated on precision at L across 748 families.
Figure 18:	Number of parameters versus length for MRF models.
Figure 19: APC has a significant positive effect on the performance of Potts and factored attention.
Figure 20: Effect of loss on precision at L over many families. Pseudolikelihood has a uniform butsmall benefit over masked language modeling for both models.
Figure 21: Training on unaligned families degrades performance on almost all families.
Figure 22: The addition of a single-site term to either factored or standard attention produces littleadditional benefit.
