Figure 1: Left panel: ReLU and Parametric Softplus. Right panel: the first derivatives for ReLU and ParametricSoftplus. Compared to ReLU, Parametric Softplus is smooth with continuous derivatives.
Figure 2: Visualizations of smooth activation functions and their derivatives.
Figure 3: Smooth activation functions improve adversarial training. Com-pared to ReLU, all smooth activation functions significantly boost robustness,while keeping accuracy almost the same.
Figure 4: Scaling-up EfficientNet inSAT. Note EfficientNet-L1 is not con-nected to the rest of the graph becauseit was not part of the compound scalingsuggested by (Tan & Le, 2019).
