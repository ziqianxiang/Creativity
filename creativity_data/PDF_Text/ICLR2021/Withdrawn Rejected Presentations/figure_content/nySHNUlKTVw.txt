Figure 1: (a) A growing graph with sequentially added subgraphs. (b) Framework of IGL. At eachtime, we train on the new subgraph together with selected or generated vertices and edges from theold graph within restricted size, and aim to perform on the entire observed graph.
Figure 2: Pipeline of our proposed methods for IGL, where we can choose the sample-based orcluster-based strategy, generating a graph for learning to update current model at each time.
Figure 3: Results of ablation studies in node classification. (a) Results of Reddit with differentlength of growing graph sequence (in log scale). (b) Results of Reddit under different memoryconstraint to size of the graph for learning. (c) Results of Pubmed using different backbone models.
Figure 4: Graph feature visualization using t-SNE in node classification of Cora at time 5 and 10.
Figure 5: Classification accuracy on the entire observed graph at each time.
Figure 6:	AUC on the entire observed graph at each time.
Figure 7:	The average degree of subgraph and entire graph throughout tasks in benchmarks.
Figure 8: Feature visualization using t-SNE of all methods training node classification on Cora.
Figure 9: Visualization of node classification results on Cora. We show the results at time 2, 4,6, 8, 10 in one row for each method. For the appeared nodes, we draw the training nodes in red, anddraw nodes classified into correct/wrong category in green/blue. Then unappeared nodes until noware in gray.
