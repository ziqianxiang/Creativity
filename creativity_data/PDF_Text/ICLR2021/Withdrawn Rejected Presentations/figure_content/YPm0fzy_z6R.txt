Figure 1: Overall architecture of S GDNET. (a) Given a signed graph G and initial node featuresX, SGDNET with multiple SGD layers produces the final embeddings H(L), which is fed to a lossfunction under an end-to-end framework. (b) A single SGD layer learns node embeddings based onsigned random walk diffusion. (c) Our diffusion module aggregates the features of node v so thatthey are similar to those connected by + edges (e.g., node u), and different from those connectedby - edges (e.g., node t). Also, it injects the local feature (i.e., the input feature of each module) ofnode v at each aggregation to make the aggregated features distinguishable.
Figure 2: Feature diffusion by signed random walks in SGDNet. (a) Signed random walks properlyconsider edge signs. (b) The positive and the negative feature vectors p(vk) and m(vk) are updated fromthe previous feature vectors and the local feature vector hVl) as described in Equation (2).
Figure 3: Effect of SGDNet’s feature diffusion compared to state-of-the-art SGCN. The performanceof SGDNET is boosted while that of SGCN degrades as the number K of diffusion steps increases.
Figure 4: Effect of local injection ratio C of SGDNET. A relatively small value (0.15 〜0.35) of Cis the best for the Bitcoin-Alpha and Bitcoin-OTC (small) datasets while c around 0.5 shows betteraccuracy for the Slashdot and Epinions (large) datasets.
Figure 5: Experimental results on Wikipedia dataset.
Figure 6: Effect of the embedding dimension of each model.
