Figure 1: Illustrations of an graph (a), its corresponding line graph (b), and its incidence graph (c).
Figure 2: An illustration of a graph (a), its corresponding line graph (b), and its weighted linegraph (c). Here, we consider a graph with 4 nodes and 4 edges as illustrated in (a). The numbersshow the node degrees in the graph. In figure (b), a line graph is constructed with self-loops. Eachnode corresponds to an edge in the original graph. In the regular line graph, the weight of eachedge is 1. Figure (c) illustrates the weighted line graph constructed as described in Section 3.2. Theweight of each edge is assigned as defined in Eq.(1).
Figure 3: An illustration of our proposed weighted line graph convolution layer. We consider aninput graph with 4 nodes and each node contains 2 features. Based on the input graph, We firstlyconstruct the weighted line graph with features as described in Section 3.2. Then We apply two GCNlayers on the original graph and the weighted line graph, respectively. The edge features in the linegraph are transformed back into node features and combined with features in the original graph.
Figure 4: An illustration of the weighted line graph convolution network. The input graph is anundirected attributed graph. Each node in the graph contains two features. Here, we use a GCNlayer to produce low-dimensional continuous feature representations. In each of the following twoblocks, we use a layer and a layer for feature learning and graph coarsening, respectively. We use amulti-layer perceptron network for classification.
Figure 5: Comparison of WLGCNets with dif-ferent depths on PTC, PROTEINS, and REDDIT-BINARY. We report the classification accuracies.
