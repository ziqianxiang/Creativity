Figure 1: An example of buffers. Circle represents worker, and the number is worker ID. There are15 workers and 5 buffers. The gradient received from worker_s is stored in buffer_{s mod 5}.
Figure 2: Average top-1 test accuracy w.r.t. epochs when there are no Byzantine workers (the firstrow), 3 Byzantine workers (the second row) and 6 Byzantine workers (the last row), respectively.
Figure 3: Average perplexity w.r.t. epochs with 1 Byzantine worker. Subfigures (a) and (b) arefor RD-attack, while Subfigures (c) and (d) for NG-attack. Due to the differences in magnitudeof perplexity, y-axes of Subfigures (a) and (c) are in log-scale. In addition, Subfigures (b) and (d)illustrates that BASGD converges with only a little loss in perplexity compared to the gold standard.
Figure 4:	Average training loss w.r.t. epochs when there are no Byzantine workers. The aggregationfunction in BASGD is set to be median (left) and trimmed-mean (right), respectively.
Figure 5:	Average training loss w.r.t. epochs in face of random disturbance attack (left) and negativegradient attack (right), when the number of Byzantine workers r = 3. Some curves do not appearin the figure, because the value of loss function is extremely large or even exceeds the range offloating-point numbers.
Figure 6:	Average training loss w.r.t. epochs in face of random disturbance attack (left) and negativegradient attack (right), when the number of Byzantine workers r = 6. Some curves do not appearin the figure, because the value of loss function is extremely large or even exceeds the range offloating-point numbers.
