Figure 1: Few-shot image generation. Our method generates novel and high-quality samples in anew domain with a small amount of training data. (Top) Diverse random samples from adapting aFFHQ-pretrained StyleGAN2 to toddler images from the CelebA dataset (with only 30 images)using our method. (Bottom) Smooth latent space interpolation between two random seeds shows thatour method produces novel samples instead of simply memorizing the 30 images. Please see thesupplementary video for more results.
Figure 2: Comparing methods for GANadaption. Learnable parameters aredenoted in red. (a) TransferGAN (TGANfor simplicity) (Wang et al., 2018) andFreezeD (Mo et al., 2020) retrain allweights W in a layer. SSGAN (Noguchi &Harada, 2019) and FSGAN trainsignificantly fewer parameters per layer.
Figure 3: Effects of singular values. We visualize FSGAN’s adaptation space by magnifying thetop 3 singular values σ0, σ1, σ2 from SVD performed on style and conv layers of a StyleGAN2(Karras et al., 2019a; 2020) pretrained on FFHQ. In mapping layer 4 (style4), the leading σs changethe age, skin tone, and head pose. In synthesis layer 2 (conv8×8), face dimensions are modified interm of face height/size/width. In synthesis layer 9 (conv1024×1024), the face appearance changes infiner pixel stats such as saturation, contrast, and color balance.
Figure 4: Problem with FID as a few-shot metric.
Figure 5: Close-domain adaptation (FFHQ→CelebA). Models adapted from a pretrainedStyleGAN2 using 〜30 target images (left-most column) of (a) CelebA ID 4978 and (b) CelebA ID3719. The proposed FSGAN generates more natural face images without noticeable artifacts.
Figure 6: Far-domain adaptation (Photo→Art). Comparing FSGAN with alternative GANadaptation methods in the photo-to-art setting. (a) FSGAN more effectively alters building layoutsand adds landscape in the foreground to match the Van Gogh paintings, maintaining better spatialcoherency. (b) FSGAN adopts features from the Portraits dataset (hats, beards, artistic backgrounds),while other methods primarily alter image textures. (c) FSGAN transforms natural hair and facialfeatures to imitate the anime target while retaining spatial consistency. Note the occurrence of pinkhair in our generated images, which does not exist in the few-shot target but is visually consistent.
Figure 7: N-Shot settings (FFHQ-Portraits): (a) Mo et al. (2020) withlimited timesteps preserves diversity at all n-shots, but producesU undesired artifacts and limited adaptation (e.g. sunglasses remain). (b). Mo et al. (2020) with increased timesteps produces quality adaptationF with 100 shots, but degenerates at ≤50 shots. (c) FSGAN (ours) is robustto n-shot settings, producing high-quality adaptation even at N=5.
