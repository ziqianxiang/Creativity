Figure 1: Illustration of UCB-bonus by using bootstrapped estimates in the regression task. The greendots represent 60 data points. (a) Regression curves of 20 neural networks. (b) Mean estimate (black)and uncertainty measurement (shadow). (c) The optimistic value (red) and mean value (black).
Figure 2: Visualizing UCB-bonus in Breakoutpendix D for the detailed specifications. The code is available at https://bit.ly/33jv1ab.
Figure 3: The change of meanUCB-bonus in BreakoutTo understand the proposed UCB-bonus, we use a trained OEB3agent to interact with the environment for an episode in Break-out and record the UCB-bonuses at each step. The curve inFigure 2 shows the UCB-bonuses of the subsampled steps inthe episode. We choose 8 spikes with high UCB-bonuses andvisualize the corresponding frames. The events in spikes cor-respond to scarcely visited areas or crucial events, which areimportant for the agent to obtain rewards: digging a tunnel (1),throwing the ball to the top of bricks (2,3), rebounding the ball(4,5,6), eliminating all bricks and starting a new round (7,8).
Figure 4: Results of 200K steps training of MNIST maze with different wall-density setup.
Figure 5: An example MNIST maze (left) andthe UCB-bonuses in the agentâ€™s path (right).
Figure 6: Visualization of UCB-bonus in Mnist-maze(d)E.2 BreakoutIn Breakout, the agent uses walls and the paddle to rebound the ball against the bricks and eliminatethem. We use a trained OEB3 agent to interact with the environment for an episode in Breakout. Thewhole episode contains 3445 steps, and we subsample them every 4 steps for visualization. The curvein Figure 7 shows the UCB-bonus in 861 sampled steps. We choose 16 spikes with high UCB-bonusesand visualize the corresponding frames. The events in spikes usually mean meaningful experiencesthat are important for the agent to get rewards. In step 1, the agent is hoping to dig a tunnel and getrewards faster. After digging a tunnel, the balls appear on the top of bricks in steps 2, 3, 4, 5, 6, 9,and 12. Balls on the top are easier to hit bricks. The agents rebound the ball and throw it over thebricks in steps 7, 8, 10, and 11. In state 13, the agent eliminates all bricks and then comes to a new19Under review as a conference paper at ICLR 2021round, which is novel and promising to get more rewards. The agents in steps 14, 15, and 16 reboundthe ball and try to dig a tunnel again. The UCB-bonus encourages the agent to explore the potentiallyinformative and novel state-action pairs to get high rewards. We record 15 frames after each spike forfurther visualization. The video is available at https://youtu.be/VptBkHyMt8g.
Figure 7: Visualization of UCB-bonus in BreakoutE.3 MsPacmanIn MsPacman, the agent earns points by avoiding monsters and eating pellets. Eating an energizercauses the monsters to turn blue, allowing them to be eaten for extra points. We use a trainedOEB3 agent to interact with the environment for an episode. Figure 8 shows the UCB bonus inall 708 steps. We choose 16 spikes to visualize the frames. The spikes of exploration bonusescorrespond to meaningful events for the agent to get rewards: starting a new scenario (1,2,9,10),changing direction (3,4,13,14,16), eating energizer (5,11), eating monsters (7,8,12), and enteringthe corner (6,15). These state-action pairs with high UCB-bonuses make the agent explore theenvironment efficiently. We record 15 frames after each spike, and the video is shown at https://youtu.be/C_8NHKpBNXM.
Figure 8: Visualization of UCB-bonus in MsPacman20Under review as a conference paper at ICLR 2021F Raw Scores of all 49 Atari GamesTable 6: Raw scores for Atari games. Each game is trained for 20M frames with a single RTX-2080TiGPU. Bold scores signify the best score out of all methods.
Figure 9: Relative score of OEB3 compared to BEBU in percents (%).
Figure 10: Relative score of OEB3 compared to BEBU-UCB in percents (%).
Figure 11: Relative score of OEB3 compared to BEBU-IDS in percents (%).
