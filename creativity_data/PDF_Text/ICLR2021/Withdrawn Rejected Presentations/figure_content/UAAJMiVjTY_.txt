Figure 1: Example of MetaAbd’s abduction-induction learning. Given training examples, back-ground knowledge of abducible primitives and probabilistic facts generated by a perceptual neuralnet, M etaAbd learns an abductive logic program H and abduces relational constraints (implementedwith the CLP(Z) predicate “#=”1) over the input images; it then uses them to efficiently prune thesearch space of the most probable pseudo-labels z (in grey blocks) for training the neural network.
Figure 2: Prolog code for MetaAbd. It recursively proves a series of atomic goals in three ways:(1) deducing them from background knowledge; (2) abducing a possible grounded expression (e.g.,relational constraint) to satisfy them (bold fonts); (3) matching them against the heads of meta-rules and form an augmented program or prove it with the current program. Finally, the abducedgroundings Abd are used for searching the best pseudo-labels z; the probability of Abd is used forcalculating the score function in Equation 5.
Figure 3: Learned programs and thetime efficiency of abduction.
