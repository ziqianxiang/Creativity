Figure 1: The setup and result of Experiment 1. The CIFAR-10 train set is labeled as either Animalsor Objects, with label noise affecting only cats. A WideResNet-28-10 is then trained to 0 train erroron this train set, and evaluated on the test set. Full experimental details are in C.21Under review as a conference paper at ICLR 2021There are several notable things about this experiment. First, the error is localized to cats in test setas it was in the train set, even though no explicit cat labels were provided. Second, the amount oferror on the cat class is close to the noise applied on the train set. Thus, the behavior of the classifieron the train set generalizes to the test set in a certain sense. This type of similarity in behavior wouldnot be captured solely by average test error — it requires reasoning about the entire distribution ofclassifier outputs. In our work, we show that this experiment is just one instance of a different type ofgeneralization, which we call “Distributional Generalization”. We now describe the mathematicalform of this generalization. Then, through extensive experiments, we will show that this type ofgeneralization occurs widely in existing machine learning methods: neural networks, kernel machinesand decision trees.
Figure 2: Feature Cali-bration. (A) CIFAR-10with p fraction of class0 → 1 mislabeled. Actualp vs. observed noise in theclassifier outputs. (B) Mul-tiple feature calibration onCelebA.
Figure 3: Agreement Property on CIFAR-10/100.
Figure 4: Distributional Generalizationremain close. That is, regularization prevents the kernel from fitting the noise on both the train andtest sets in a similar way. Full experimental details are given in Appendix B, including an analogousexperiment for neural networks on CIFAR-10, with early-stopping in place of regularization (Fig-ure 22). These experiments suggests that Distributional Generalization is a meaningful notion evenfor non-interpolating classifiers.
Figure 5: Distributional Generalization in Experiment 2. Joint densities of the distributionsinvolved in Experiment 2. The top panel shows the joint density of labels on the train set:(CIFAR_Class (x), y). The bottom panels shows the joint density of classifier predictions onthe test set: (CIFAR_Class (x), f (x)). Distributional Generalization claims that these two jointdensities are close.
Figure 6: Joint density of (y, Class(x)), top, and (f (x), Class(x)), bottom, for test samples (x, y)from Experiment 2 for an MLP.
Figure 7: Feature Calibration for Constant Partition L: The CIFAR-10 train and test sets areclass-rebalanced according to (A). Interpolating classifiers are trained on the train set, and we plot theclass-balance of their outputs on the test set. This roughly matches the class-balence of the train set,even for poorly-generalizing classifiers.
Figure 8: Feature Calibration with original classes on CIFAR-10: We train a WRN-28-10 onthe CIFAR-10 dataset where we mislabel class 0 → 1 with probability p. (A): Joint density of thedistinguishable features L (the original CIFAR-10 class) and the classification task labels y on thetrain set for noise probability p = 0.4. (B): Joint density of the original CIFAR-10 classes L and thenetwork outputs f(x) on the test set. (C): Observed noise probability in the network outputs on thetest set (the (1, 0) entry of the matrix in B) for varying noise probabilities pTo show that this is not dependent on the particular class used, we also show that the same holds for arandom confusion matrix. We generate a sparse confusion matrix as follows. We set the diagonal to0.5. Then, for every class j, we pick any two random classes for and set them to 0.2 and 0.3. Wetrain a WRN-28-10 on it and report the test confusion matrix. The resulting train and test densitiesare shown in Figure 9. As expected, the train and test confusion matrices are close, and share thesame sparsity pattern.
Figure 9: Feature Calibration with random confusion matrix on CIFAR-10: Left: Joint densityof labels y and original class L on the train set. Right: Joint density of classifier predictions f (x)and original class L on the test set, for a WideResNet28-10 trained to interpolation. These two jointdensities are close, as predicted by Conjecture 1.
Figure 10: Feature Calibration for Decision trees on UCI (molecular biology). We add labelnoise that takes class 2 to class 1 with probability p ∈ [0, 0.5]. The top row shows the confusionmatrix of the true class L(x) vs. the label y on the train set, for varying levels of noise p. The bottomrow shows the corresponding confusion matrices of the classifier predictions f (x) on the test set,which closely matches the train set, as predicted by Conjecture 1.
Figure 11: Decision trees on UCI (wine). We add label noise that takes class 1 to class 2 withprobability p ∈ [0, 0.5]. Each column shows the test and train confusion matrices for a given p. Notethat this decision trees achieve high accuracy on this task with no label noise (leftmost column).
Figure 12: Decision trees on UCI (mushroom). We add label noise that takes class 0 to class 1 withprobability p ∈ [0, 0.5]. Each column shows the test and train confusion matrices for a given p. Notethat this decision trees achieve high accuracy on this task with no label noise (leftmost column).
Figure 13: Feature Calibration for multiple features on CelebA: We train a ResNet-50 to performbinary classification task on the CelebA dataset. The top row shows the joint distribution of this tasklabel with various other attributes in the dataset. The bottom row shows the same joint distributionfor the ResNet-50 outputs on the test set. Note that the network was not given any explicit inputsabout these attributes during training.
Figure 14: Coarse partitions as distinguishable features: We consider a setting where the originalclasses are not distinguishable, but the a superset of the classes are.
Figure 15: TV distance between (L(x), f (x)) and (L(x), y) vs. ε for the distinguishable features0.1	0.2	0.3	0.4	0.5	0.6distinguishable features systematically, we train a networks to classify (C at, Dog, Plane, Ship)with changing number of samples. Networks with fewer samples have larger ε since they are worseat classifying the distinguishable features. Then, we use the same setup and number of samplesrespectively to train networks on the binary task and measure the TV-distance between (L(x), f (x))and (L(x), y) in this task. The results are shown in Figure 15. As predicted, the TV distance is upperbounded by ε.
Figure 16: Agreement Property for Myrtle Kernel on CIFAR-10.
Figure 17: Agreement Property for Kernels on Fashion-MNIST. For two classifiers trained ondisjoint train sets, the probability they agree with each other (on the test set) is close to their testaccuracy.
Figure 18: Agreement Property on UCI. For two decision trees trained on disjoint train sets, theprobability they agree with each other (on the test set) is close to their test accuracy. Each pointcorresponds to one UCI task, and error bars show 95% Clopper-Pearson confidence intervals inestimating population quantities.
Figure 19: Histogram of sample-hardnesses.
Figure 20: Pointwise agreement histogram.
Figure 21: Distributional Generalization. Train (left) and test (right) confusion matrices for kernelSVM on MNIST with random sparse label noise. Each row corrosponds to one value of inverse-regularization parameter C . All rows are trained on the same (noisy) train set.
Figure 22: Distributional Generalization for WideResNet on CIFAR-10. We apply label noisefrom a random sparse confusion to the CIFAR-10 train set. We then train a single WideResNet28-10,and measure its predictions on the train and test sets over increasing train time (SGD steps). Thetop row shows the confusion matrix of predictions f(x) vs true labels L(x) on the train set, and thebottom row shows the corresponding confusion matrix on the test set. As the network is trained forlonger, it fits more of the noise on the train set, and this behavior is mirrored almost identically on thetest set.
