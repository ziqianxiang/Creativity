Figure 1: (Left) The predict-and-denoise framework: First, a denoiser is learned using syntheticperturbations of a large number of unlabeled outputs. Second, a base predictor composed with thedenoiser is learned with labeled data. Composing with a denoiser allows the base predictor to besimpler, improving generalization. (Middle) Univariate regression example where a staircase functionrequires a complex linear spline fit. (Right) A simple linear function can fit a staircase function whencomposed with a denoiser which projects onto the valid outputs (the integers).
Figure 2:	(Left-Middle) Example pseudocode and code from the synthetic dataset. Since thepseudocode is ambiguous, variable types and whether to instantiate a variable must be inferred. (Right)Random corruption used to train a denoiser from corrupted to valid code. The denoiser must inferthe correct type of var_5 from other lines.
Figure 3:	Generated letters A-J for 10 randomly selected fonts. (a) The direct predictor makes blurry out-puts with many artifacts. (b) The composed predictor (base + denoiser) makes clearer outputs with moredistinct font patterns. (c) The improvement comes from leveraging output structure learned by the de-noiser. This allows the base predictor to produce blurrier outputs corresponding to a lower norm model.
Figure 4:	Test MSE on font image generation. (Left) Results when L2 regularization strength istuned with the validation set. (Right) Varying L2 regularization strength (1e-6 to 1e-2) for directand composed predictors. While similar at low regularization, increasing the regularization strengthimproves the composed predictor while hurting the direct predictor.
Figure 5:	(Left-Middle) Example input and output of the base predictor on the synthetic dataset.
