Figure 1: Tokenization and input representation generation of long-term, short-term user be-havior and profile data. To form behavioral words, we discretize long-term behavior into 24-hourintervals and segment short-term sequences when there is a time interval larger than 30 minutes be-tween two actions. The word embeddings of the attribute IDs in each action are first concatenated.
Figure 2: Pretraining of UserBERT. The representation vectors in input sequences are randomlymasked (zeroed-out), and then the masked input is passed through UserBERT. The model is trainedto reconstruct the attributes in these masked ‘words’. For each attribute, an output layer is connectedto the hidden representations at the masked positions, and it is learned via minimizing the predictionerrors of multi-label classifications.
Figure 3: Fine-tuning performance comparison for the user targeting task. ROC4UC and ac-curacy results on two use cases, predicting new users for two different services.
Figure 4: ROC-AUC comparison betweenTransformer-based MTL models with differentnumbers of labeled data.
Figure 5: Performance comparison betweenUserBERT with and without pretraining on usertargeting taskthis task only have few labeled data. Classification performance in terms of accuracy and ROC-AUCare shown in Figure 3. The LSTM model, which sequentially models user behavior, has relativelylow accuracy. One possible explanation is that the sequential order of user actions does not provideuseful information for this task. From our experience the user targeting task focuses on patterns fromrelatively static user preferences. The Wide&Deep model shows competitive performance, whichis reasonable since our exploratory analysis indicates that user profiles are important features. Theperformance of the Transformer-based models reveal that the underlying explanatory factors for thistask can be captured by attention networks. UserBERT outperforms other models in both use casesby a substantial margin. We hypothesize that, compared to Transformer-based MTL, the learning ofthe UserBERT is not limited by the multiple training tasks and is able to learn more expressive andgeneric representations from the input.
Figure 6: ROC-AUC comparison on attribute Pre-diction task.
