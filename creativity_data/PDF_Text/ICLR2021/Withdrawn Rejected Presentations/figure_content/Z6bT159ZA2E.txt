Figure 1: Upper bound AP (in red) and scores of the best models using the COCO evaluation tool(in orange; FCOS on VOC and FASHION datasets, and Hybrid Task Cascade on COCO). The blacksolid line on the COCO panel belongs to the winning entry on the latest (2019) challenge on COCOval2019 provided at https://competitions.codalab.org/competitions/20794#results(this result was not available at the time of this study). See also https://paperswithcode.com/sota/object-detection-on-coco-minival. Notice that the gap at AP50 is almost closed on COCO. Thereis, however, still a large gap between the performance of the state of the art object detection models and theempirical upper bound. The gap is wider over higher IOU thresholds and small objects. See Fig. 11 in Appx D.
Figure 2: Top: Illustration of thecontext surrounding an object,Bottom:	Object recognitionaccuracy. Top rows: testingon the canonical object size(used in the rest of the paper;See Appendix C for confusionmatrices.). Bottom rows: trainingand testing are the same, forexample, a classifier is trainedon the object-only case 0.6 andis then tested on the object-onlycase 0.6. The best accuracy ineach row is highlighted in bold.
Figure 3: A: Illustration ofour setup for finding boxeswith IOU ≥ γ with the targetbox (corresponding to αβ =2γ∕(1 + γ); αβ = 2/3 forIOU = 0.5), B: The solutionsare 4 curves represented by Eqs.
Figure 4: Top: UAPand Model APs over thePASCAL VOC dataset usingVOC (left) and COCO APevaluation codes (right).
Figure 5: Detection APs over MS COCO dataset borrowed from the Detectron2 benchmark. The black dashline corresponds to the best model among the models we analyzed (the score shown with “*”). The black solidline shows the most recent results (the score shown with “+”). See Appx. D for results over the MMDetection.
Figure 6: Correlation between the classification accuracy andupper bound AP. The higher the accuracy, the better the UAP.
Figure 7: Top: Sample images placedin white and white noise backgroundsas well as the cropped objects (whichwere shown to the models in isolation).
Figure 8: Quantifying the contribution of error types inmodels using the COCO analysis tool (@ IOU=0.5).
Figure 9: Illustration of AP calculation.
Figure 11: APs over COCO dataset borrowed from the MMDetection benchmark. We add CenterNet resultsto MMDetection.
Figure 12: A state-of-the-art object detector (Faster-RCNN; trained on COCO dataset) is able to detect multipleobjects in a living-room (a), but it fails to detect a transplanted object (elephant) out of its context. Rosenfeldet al. (2018) showed that a transplanted object a) may occasionally become undetected orbe detected with sharpchanges in confidence, b) may be classified as another object, and/or c) cause other objects to switch identity,bounding box, or disappear (image reproduced from (Rosenfeld et al., 2018)).
Figure 13: Sample images used in the context analysis experiments. Top row) single objects in whitebackground, single objects in white noise background, Bottom row) cropped objected (no resizing), croppedand resized objects (width=300).
Figure 14: Complete results of the context analysis experiments.
Figure 15: Error analysis of models based on the object size over the MS COCO dataset.
