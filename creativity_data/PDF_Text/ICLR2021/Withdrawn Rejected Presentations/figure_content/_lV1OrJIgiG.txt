Figure 1: In each training episode, we randomly select a task T to initialize environment simulationand feed the corresponding task context cT to the agent. We use a joint state space o ∈ R12 as inputto the agent, consisting of position R3 , orientation R3 , and translational and rotational velocity R6 .
Figure 3: We use yellow or circles to indicate predicted states or other quantities, and grey or squaresfrom actual interactions. (left) Applying MCTS with hypermodel to search for behavioral policy andvalue, and act with a sampled action. (right) Computing targets and backpropagating from loss. Theblue line indicates n-step relabelling. We only illustrate backpropagation for one reward node forsimplicity. The solid red line is to emphasize the gradient flow from the auxiliary model loss to themeta network’s weight ψ. The dashed red line is the gradient from task loss.
Figure 4: Zero-shot evaluation performance on 13 × 13 maps from distance 1 to 15. (a) Success rateof all agents. (b) Lengths of successful trajectories of MMN and MAH.
Figure 5: Study of performance on (a) different map sizes and (b) perturbation strategies.
Figure 6: Trajectories from hierarchical navigation in zero-shot on 13 × 13 maps. The top-rightcorner is the start, and the bottom-left is the goal. Other darker cells are generated subgoals withdistance 5. The first row is for MMN and second row is for MAH. For the first 4 tasks (columns),MMN successfully reached the goals, while MAH failed. Both methods failed in the last task.
Figure 7: Multi-task training performance of MMN and MAH.
Figure 8: Ablation study of n-step relabelling.
Figure 9: Ablation study of different hyperparameters in evaluation.
