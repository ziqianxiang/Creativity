Figure 1: Difference in ME measurement for changes in the radius r used to measure the complexity con-tained in a local neighborhood.
Figure 2: ME measures of the convolutional kernel weights during model training using r = 1 for color-FERET and UTKFace datasetsFigure 3: Confusion matrix for the five classes of the balanced colorFERET dataset; Asian-M-E: AsianMiddle-Easternconverged and greater improvements can be achieved with manual HP fine tuning. Furthermore, MCCEtrained models generally converge faster taking an average of 176 and 204 epochs compared to 199 and 268epochs for the colorFERET and UTKFace datasets respectively. Faster convergence along with higher lossessuggests an enhanced learning capacity of the MCCE models, which can be improved with exploration ofadditional techniques to improve convergence such as learning rates.
Figure 3: Confusion matrix for the five classes of the balanced colorFERET dataset; Asian-M-E: AsianMiddle-Easternconverged and greater improvements can be achieved with manual HP fine tuning. Furthermore, MCCEtrained models generally converge faster taking an average of 176 and 204 epochs compared to 199 and 268epochs for the colorFERET and UTKFace datasets respectively. Faster convergence along with higher lossessuggests an enhanced learning capacity of the MCCE models, which can be improved with exploration ofadditional techniques to improve convergence such as learning rates.
Figure 4: Confusion matrix for the five classes of the unbalanced UTKFace datasetLoss8 -MCCE and CCE loss1	11	21	31	41	51	61	71	81	91	101	111	121	131	141	151	161	171	181	191	201	211	221	231	241	251â– ^^^-CF_MCCE_loss	CF_CCE_loss	UTK_Face_MCCE_loss	UTK_Face_CCE_lossEpochsFigure 5: Loss curves for MCCE and CCE loss functions for colorFERET and UTKFace datasets6 Conclusion and Future WorkIn this paper, we proposed a novel extension to the commonly used Categorical Cross Entropy (CCE) lossfunction known as Maximum Categorical Cross Entropy (MCCE). While CCE evaluates the probabilitydistributions of the CNN predicted and ground truth class labels, MCCE extends this evaluation to includethe entropic distribution of convolutional kernel weights during model training. MCCE provides a robustnoise-averse method of calculating model loss since partial knowledge of the entropic distribution of theinput data is determined by a priori and large divergences from the maximum are penalized during modeltraining.
Figure 5: Loss curves for MCCE and CCE loss functions for colorFERET and UTKFace datasets6 Conclusion and Future WorkIn this paper, we proposed a novel extension to the commonly used Categorical Cross Entropy (CCE) lossfunction known as Maximum Categorical Cross Entropy (MCCE). While CCE evaluates the probabilitydistributions of the CNN predicted and ground truth class labels, MCCE extends this evaluation to includethe entropic distribution of convolutional kernel weights during model training. MCCE provides a robustnoise-averse method of calculating model loss since partial knowledge of the entropic distribution of theinput data is determined by a priori and large divergences from the maximum are penalized during modeltraining.
