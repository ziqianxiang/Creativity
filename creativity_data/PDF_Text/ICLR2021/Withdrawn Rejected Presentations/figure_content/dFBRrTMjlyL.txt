Figure 1: Results on synthetic data with different activation functions. “-GPN” denotes the functionis Gaussian-Poincare normalized. ∣∣x(l) ∣∣2 denotes the l2 norm of the outputs of the l-th layer. ndenotes the width. ∣∣ dWI) ∣∣f is the FrobeniuS norm of the gradient of the weight matrix in the l-thlayer.
Figure 2: Histogram of φ0(hi(l)). The values of φ0(hi(l)) are accumulated for all units, all layers andall samples in the histogram.
Figure 3: Test accuracy (percentage) during training on CIFAR-10. “-BN” denotes that batch nor-malization is applied before the activation function.
Figure 4: Results on synthetic data with different activation functions. “-GPN” denotes the functionis Gaussian-Poincare normalized. ∣∣x(l)∣∣2 denotes the l2 norm of the outputs of the l-th layer. ndenotes the width. ∣∣ ∂W) ∣∣f is the Frobenius norm of the gradient of the weight matrix in the l-thlayer.
Figure 5: Results on synthetic data with different activation functions. “-GPN” denotes the functionis Gaussian-Poincare normalized. ∣∣x(l)∣∣2 denotes the l2 norm of the outputs of the l-th layer. ndenotes the width. ∣∣ ∂Wi) IIf is the Frobenius norm of the gradient of the weight matrix in the l-thlayer.
Figure 6: Histogram of φ0(hi(l)). The values of φ0(hi(l)) are accumulated for all units, all layers andall samples in the histogram. Except for ELU, none of them has values concentrated around one.
Figure 7: Gradient norm ratio for different layer width on synthetic data. The ratio is definedas maxi || 般ɑ)∣∣f/ mini ∣∣ ∂WEι IIf∙ The width ranges from 100 to 1500. The error bars showstandard deviation.
Figure 8: Test accuracy (percentage) during training on MNIST.
Figure 9: Test accuracy (percentage) during training on CIFAR-10.
Figure 10: Gradient norm ratio during training on MNIST. Horizontal axis denotes the mini-batchupdates. Vertical axis denotes the gradient norm ratio maxi ∣∣ ∂∂EEi) ∣∣f / mini k ∂∂EE) ∣∣f ∙ The gradientvanishes (∣ ∂VE) ∣∣f ≈ 0)for ReLU and LeakyReLU during training and hence the plots are empty.
Figure 11: Gradient norm ratio during training on MNIST. Horizontal axis denotes the mini-batchupdates. Vertical axis denotes the gradient norm ratio maxi ∣∣ ∂∂EEi) ∣∣f / mini k ∂∂EE) ∣∣f ∙ The gradientvanishes (∣ ∂VE) ∣∣f ≈ 0) for GELU and GELU-BN during training and hence the plots are empty.
Figure 12: Gradient norm ratio during training on CIFAR-10. Horizontal axis denotes the mini-batch updates. Vertical axis denotes the gradient norm ratio max? ∣∣ ∂V‰IIf/ mini k a∂EEi) IIf∙ Thegradient vanishes (∣ a∂EEi) IIf ≈ 0) for ReLU and LeakyReLU during training and hence the plotsare empty.
Figure 13: Gradient norm ratio during training on CIFAR-10. Horizontal axis denotes the mini-batch updates. Vertical axis denotes the gradient norm ratio maxi ∣∣ ∂V‰IIf/ mini k a∂EEi) IIf∙ Thegradient vanishes (∣ 盗/)∣f ≈ 0) for GELU, GELU-BN and GELU-GPN-BN during training andhence the plots are empty. For GELU-GPN-BN, both gradient exploding and gradient vanishing areobserved.
