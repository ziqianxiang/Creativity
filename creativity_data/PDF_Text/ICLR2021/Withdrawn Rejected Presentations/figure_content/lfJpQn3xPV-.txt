Figure 1: A temporal graph Gt where new vertices with potentially new classes appear over time.
Figure 2: Distributions tdiff k of time differences (y-axis) for PharmaBio (left), DBLP-easy (center)and DBLP-hard (right) within the k-hop neighborhood of each vertex for k = {1, 2, 3} (x-axis).
Figure 3: Results for Q1: Distribution Shift under Static vs Incremental Training. Comparisonof static models (solid lines) and incrementally trained models (dashed lines) on PharmaBio (left),DBLP-Easy (center), and DBLP-Hard (right). Average accuracy of 10 runs (y-axis) per time step(x-axis). Error regions are 95% confidence intervals computed with 1,000 bootstrap iterations.
Figure 4: Results for Q2: Training with Warm vs Cold Restarts in an online scenario with 200epochs training over the window per time step. Average accuracy of 10 runs (y-axis) per time step(x-axis). Error regions are 95% confidence intervals computed with 1,000 bootstrap iterations.
Figure 5:	Distribution of vertices per year, degree distributions, label distributions, for our temporalgraph datasets.
Figure 6:	Hyperparameter choices for experiment Q1. Static methods were trained for 400epochs on 25% of the data before the first evaluation time step. Incremental methods were trainedwith warm restarts for 200 epochs per time step using a window size of3 (DBLP) and 4 (PharmaBio)Method	Training	Layers	Hidden Size	Learning RateMLP	static	2	64	10-3 [10-4,10-1]MLP	incremental	2	64	10-3 [10-4,10-1]GS-Mean	static	2	64	10-3 [10-4,10-1]GS-Mean	incremental	2	64	10-3 [10-4,10-1]GAT	static	2	64	10-3 [10-4,10-1]GAT	incremental	2	64	10-2 [10-4,10-1]For each dataset, we chose the boundaries for our evaluation time steps [tstart, tend], such that 25%of the total number of vertices lie before tstart, and tend is the final time step. For PharmaBio (1985-2016), that is tstart = 1999, and for both DBLP variants (1990-2015), that is tstart = 2004. Databefore tstart may be used for training, depending on the window size. Regarding changes in the classset (distribution shift), DBLP-easy has 12 venues in total, including one bi-annual conference andfour new venues appearing in 2005, 2006, 2007, and 2012. DBLP-hard has 73 venues, includingone discontinued, nine bi-annual, six irregular venues, and 23 new venues.
Figure 7:	Hyperparameter choices for experiment Q2. All methods are supplied with 200 epochsper time step. Learning rate optimization is performed on DBLP-easy.
Figure 8:	Hyperparameter choices for experiment Q3. All methods are supplied with 200 epochsper time step. We separately optimize hyperparameters for each window size and for each restartconfiguration on DBLP-easy.
Figure 9: Detailed results per time step for experiment Q3. Comparison of different temporalwindow sizes in an online scenario with 200 incremental training epochs per time step with eithercold (Top) or warm restarts (Bottom) and varying temporal window sizes. 95% CI not shown forreasons of better visualization. Average accuracy of 10 different runs (y-axis) per timestep (x-axis).
