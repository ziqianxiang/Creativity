Figure 1: Left: Generic model architecture, common to all tasks. The feature vector xt at each timestep t is linearly projected to a vector ut of the same dimensionality d as the internal representationvectors of the model and is fed to the first self-attention layer to form the keys, queries and valuesafter adding a positional encoding. Right: Training setup of the unsupervised pre-training task.
Figure 2: Dataset: BeijingPM25QUanty. Left: Root Mean Squared Error of a fully supervisedtransformer (orange circles) and the same model pre-trained (blue diamonds) on the training setthrough the unsupervised objective and then fine-tuned on available labels, versus the proportion oflabeled data in the training set. Right: Root Mean Squared Error of a given model as a function ofthe number of samples (here, shown as a proportion of the total number of samples in the trainingset) used for unsupervised pre-training. For supervised learning, two levels of label availabilityare depicted: 10% (purple circles) and 20% (green squares) of all training data labels. Note that ahorizontal axis value of 0 means fully supervised learning only, while all other values correspond tounsupervised pre-training followed by supervised fine-tuning.
Figure 3: Root Mean Squared Error of a given model as a function of the number of samples (here,shown as a proportion of the total number of samples in the training Set) used for unsupervised pre-training. Two levels oflabel availability (used for supervised learning) are depicted: 10% (left panel)and 20% (right panel) of all training data labels. Note that a horizontal axis value of 0 means fullysupervised learning only, while all other values correspond to unsupervised pre-training followed bysupervised fine-tuning.
Figure 4: Masking schemes within our transformer encoder framework: for implementation of fore-casting objective (left), for an alternative unsupervised learning objective involving a single noisedistribution over time steps, applied synchronously to all variables (right).
Figure 5: Top: Imputation of missing values in the test set of BenzeneConcentration dataset. Thecontinuous blue line is the ground truth signal, the light blue circles indicate the values hiddenfrom the model and the orange dots its prediction. We observe that imputed values approximatetrue values very well, even in cases of rapid transitions and in cases where many contiguous valuesare missing. Bottom: Same, shown for 5 different dimensions of the time series (here, these areconcentrations of different substances) as columns and 4 different, randomly selected samples asrows.
