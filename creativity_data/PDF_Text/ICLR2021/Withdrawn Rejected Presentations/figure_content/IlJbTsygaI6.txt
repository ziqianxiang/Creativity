Figure 1: Interaction between a HAC-General hierarchical agent with 2 levels of policies and theenvironment. The goal-picking policy at the top produces goals and the goal-reaching policy atthe bottom interacts with the environment to reach those goals in at most H steps. The environmentproduces a reward and the state changes at each interaction. The objective of the goal-picking policyat the top is to pick goals that maximize the reward that is collected, while the objective of the goal-reaching policy at the bottom is to reach the goal states and collects at least the desired amount ofreward dictated in the goal.
Figure 2: HAC's optimality illusion: the goal-reaching policy ‚àèbeiow doesn,t reach the originalaction/goal g but after replacing the original goal/action by the hindsight action S it appears optimal.
Figure 3: Now that rewards matter, reaching the goal isn,t enough to guarantee optimality: thehighest-reward path to the goal must be picked.
Figure 4: HAC-General re-creates optimality by re-defining goals as a (state, desired reward) pair.
Figure 5: Evolution of the performance of the agent during training in the Mountain Car and LunarLander environments. Each algorithm is executed 5 times per environment: the bold line representsthe mean reward and the shaded regions display the standard deviation. HAC-General With Teacherperformance exceeds the success threshold defined by the environment and outperforms both com-peting algorithms, showing both its effectiveness despite being in a more general and harder settingthan HAC and the usefulness of leveraging a black box expert during training. Note: training stopsonce the success threshold is reached, since it indicates the agent can solve the task.
Figure 6: Explanations produced by the hierarchical agent at different time steps of an episode. Thered or yellow rectangle indicates the current goal. A plan composed of goals g1 , g2, ... is generatedgiven the current state and shown as progressively darker green rectangles. A video version of theexplanations is provided in the supplementary material.
Figure 7: The three types of transitions created by the Hindsight Actor-Critic algorithm: hindsightaction transitions (replace the action by the state that was reached by the policy below), hindsightgoal transition (replace action; replace the goal by a state reached by the policy during the episode)and subgoal testing transition (if the policy at level L picks an action/goal the policy below fails toreach, occasionally attribute that action a very low reward).
Figure 8: The trained hierarchical agent runs for a hundred episodes and for each episode we mea-sure the percentage of goals that were reached during the episode. The distribution of those percent-ages makes it possible to better understand how often the goals are reached. In the Mountain Carenvironment the goal is almost always reached (92 % median, 92% average) and in the Lunar Landerenvironment the goal is reached the majority of the time (96 % median, 74% average) despite goalsbeing much more difficult to pick and reach.
