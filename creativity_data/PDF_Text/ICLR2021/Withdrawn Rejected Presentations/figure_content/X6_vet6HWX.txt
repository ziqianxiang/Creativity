Figure 1: Test loss for the regression task with a linear network: (a) task-unrelated, (b) task-related,and (c) task-related + task-unrelated cases. The legends report the amount of unnecessary dimensions.
Figure 2: Test accuracy for classification task with a linear network: (a) task-unrelated, (b) task-related, and (c) task-related + unrelated cases. Plots (a) and (c) show the double descent phenomenon.
Figure 3: AUTC curves for different MLPs on the classification task: (a) task-unrelated, (b) task-related, and (c) task-related + unrelated case. In green MLP with ReLU non-linearity and cross entropyloss, in beige and pink linear MLPs with cross entropy and mean square error losses respectively, inblue exact solution from Figure 2.
Figure 4: AUTC curves for the MLP with non-linear ReLU activations and cross entropy loss on themixed Gaussians dataset, for different distributions of task-unrelated dimensions.
Figure 5:	log AUTC curves of DCNNs and MLP with ReLU and cross entropy loss on SyntheticMNIST datasets. The log AUTC is computed in the range n^ ∈ [1,103]. For cases (a)-(c), on theright, examples for each dataset.
Figure 6:	Natural MNIST. (a) top: log AUTC of DCNNs across different filters and max-poolingsizes at the first layer computed on ntr in the range [50, 1000]. (a) bottom: test accuracy acrossDCNNs for a number of training examples n柝 = 20 and different amount of unnecessary inputdimensions. (a) right: examples for different amount of unnecessary input dimensions, from the topto the bottom, the edge MNIST sizes are 200, 150, 80, and 28. (b-c) distribution of max activationvalues, as We move from the edges of the hidden representations of test images to their centers, acrossthe three convolutional layers.
Figure 7: Accuracy on Stanford Dogs with different amount of unnecessary dimensions. An examplefor each of the five cases given as input to ResNet-18 (left). AUTC curves of ResNet-18 modelstrained on the fives cases (right).
Figure 8: Regression problem with corrupted output: (a) prediction error in absence of unnecessarydimensions, as function of the output corruption; (b) analysis of prediction error as function of thevariance of unnecessary components; on the y-axis, difference between test errors in presence (wirrelevant) and absence (w/o irrelevant) of task-unrelated dimensions, a blue curve for each level ofoutput corruption. (b)values of σinput, the presence of task-unrelated dimensions does not have any effect on the test error.
Figure 9: AUTC curves for the multivariate Gaussian distributions, as we decrease the distance Dbetween two classes: (a) task-unrelated, (b) task-related, (c) task-related/unrelated cases.
Figure 10: An example from each version of the Synthetic MNIST dataset. (a) task-unrelated case,(b) task-related case, (c) task-related/unrelated case, the image size is fixed at 200.
Figure 12: log AUTC on Natural MNIST. All the networks have an average global pooling operationafter the three convolutional layers.
Figure 13: AUTC on SST2 dataset, SPlitted on training set.
