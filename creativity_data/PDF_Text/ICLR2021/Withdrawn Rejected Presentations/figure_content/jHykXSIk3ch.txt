Figure 1: Illustration of the spherical perspective forSGD. The loss function L of a NN w.r.t. the parame-ters xk ∈ Rd of a neuron followed by a BN is radiallyinvariant. The neuron update xk → xk+1 in the origi-nal space, with velocity ηk VL(xk), corresponds to anupdate uk → uk+1 of its projection through an expo-nential map on the unit hypersphere Sd-1 with velocityηke kVL(uk)k at order 2 (see details in Section 2.3).
Figure 2: (a) Effect of the radial part of ck on the displacement on Sd-1 ; (b) Example of anisotropy andsign instability for the deformation ψ(VL(uk)) = VL(uk) 0	ULUk"d k k^L(uk) k(where | ∙ | is the element-wiseabsolute value) occurring in Adam’s first optimization step; (c) Different contribution in ck⊥ of two past gradientsV1 and V2 of equal norm, depending on their orientation. Illustration of the transport of V1 from uk-1 to uk :Γuukk-1 (V1) (cf. Appendix D.2 for details)4.2 Empirical studyTo study empirically the importance of the identified geometric phenomena, we perform an ablationstudy: we compare the performance (accuracy and training loss speed) of Adam and variants thatneutralize each of them. We recall that AdamW neutralizes (b2) and that AdamG neutralizes all ofabove phenomena but loses the scheduling effect identified in Eq. 14. To complete our analysis, weuse geometrical tools to design variations of Adam which neutralizes sequentially each phenomenonwhile preserving the natural scheduling effect in Theorem 2. We neutralize (a) by replacing theelement-wise second-order moment, (b1) and (b2) by transporting the momentum from a currentpoint to the new one, (c) by re-scaling the momentum at step k. The details are in Appendix. D.2.
Figure 3: Training speed comparison with ResNet20 on CIFAR10. Left: Mean training loss over all trainingepochs (averaged across 5 seeds) for different Adam variants. Right: Zoom-in on the last epochs. Please refer toTable 2 for the corresponding accuracies.
