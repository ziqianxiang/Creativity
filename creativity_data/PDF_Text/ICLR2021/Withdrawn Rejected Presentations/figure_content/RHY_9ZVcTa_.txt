Figure 1: Left and Middle: Two learned DNN representation functions fe】(B),札(B) visualizedon held-out data B. The DNNs are word embedding models Mnih and Teh (2012) trained on theBillion Word Dataset (Chelba et al., 2013) (see Appendix A.1 for code release and training details).
Figure 2: Deep Supervised Classification. (a) Data distribution for a linearly identifiable K-wayclassification problem. (b) Mean (centered) CCA between the learned representations over thecourse of training. After approx. 4000 iterations, CCA finds a linear transformation that rotate thelearned representations into alignment, up to optimization error. (c) Learned representations aftertransformation via optimal linear transformation. The first dimension of the first model’s featurespace is plotted against the first dimension of second. The learned representations have a nearly linearrelationship, modulo estimation noise.
Figure 3: Self-Supervised Representation Learning. Error bars are computed over 5 pairs ofmodels. (a) Input data. Two patches are taken (one from top half, and one from the bottom half) ofan image at random. Using a contrastive loss, we predict the identity of the bottom patch encodingfrom the top. (b) Linear similarity of learned representations at checkpoints (see legend). As modelsconverge, linear similarity increases. (c) Linear similarity as we increase the amount of data forfθ and gθ. Error bars are computed over 5 pairs of models. (d) As we increase model size, linearsimilarity after convergence increases for both fθ and gθ .
Figure 4: Text Embeddings by GPT-2. GPT-2 results. Representations of the last hidden layer(which is identifiable), in addition to three earlier layers (not necessarily identifiable) for fourGPT-2 models. For each representation layer, SVCCA is computed over to all pairs of models,over which correlation coefficients were averaged. SVCCA was applied with 16, 64, 256 and 768principal components. The learned representations in the last, identifiable layer more correlated thanrepresentations learned in preceding layers.
