Figure 1: The framework of DG-Net. Left: For a training batch, each sample performs differentforward paths that are determined by the sample-dependent macro connectivity. Right: Nodeoperations at the micro level. Here we illustrate a node with 2 active input edges and output edges.
Figure 2: The procedure of updating the adjacency matrix and the proposed buffer for storing. A nodeobtains the weights of input edges from the row (blue) and storing weights to output edges saving inthe column (green). The matrices are saved in a buffer that supports batch training efficiently.
Figure 3: Different routing methods with different locations of routers.
Figure 4: Different initialization schemes for routers. The positive bias initializes the connections asexistence, obtains lower training loss in the early training procedure, and achieves higher validationaccuracy than negative and zero biases.
Figure 5: The distribution of mean of weights of edges in different graphs/stages. Darker colorsrepresent larger weights.
Figure 6: The distribution of standard deviation of weights of edges in different graphs/stages.
Figure 7: Visualization of the learned connectivity for different input samples.
