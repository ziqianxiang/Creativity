Figure 1:	ERMAS trains planners that are robust to reality gaps in these testbed multi-agent simula-tions. (Left) In the repeated bimatrix game, a pair of agents navigates a 2D landscape. Both agentsand the planner receive rewards based on visited coordinates. Brighter squares indicate higher pay-off. (Right) In the spatiotemporal economic simulation, 4 heterogenous agents perform labor, traderesources, earn income, and pay taxes according to the schedules set by the planner.
Figure 2:	Validating ERMAS Agent-Adv-Search in constrained repeated bimatrix games. Eachpoint in the above scatter plots describes the average outcome at the end of training for the agents(x-coordinate) and the planner (y-coordinate). Error bars indicate standard deviation. (Left) The bi-matrix reward structure encodes a social dilemma featuring a low-agent-reward/high-planner-rewardNash equilibrium, which is where vanilla MARL converges. Agents trained with ERMAS deviatefrom this equilibrium in order to reduce planner reward. governs the extent of the allowable de-viation. (Middle) As increases, the average per-timestep regret experienced by the agents alsoincreases. Each average is taken over the final 12 episodes after rewards have converged. (Right)Using fixed values of λ (rather than allowing it to update, as in the full algorithm) distorts perfor-mance and prevents agents from reaching the same -equilibria discovered with learned λ.
Figure 3: Planner and agent rewards in the repeated bimatrix game over training time. Each linerepresents an average over 10 seeds, with error bars indicating standard error. The lines correspondto runs of ERMAS and are colored according to β, the weight of the meta-learning term of Equation13. (Left) Planner performance over training episodes. (Middle) Agent 1 performance over trainingepisodes. (Right) Agent 2 performance over training episodes.
Figure 4: Robust planner learning in the repeated bimatrix game. Average planner performance asa function of the reward perturbation coefficient Q in the test environment. Each point representsan average over 5 seeds, with error bars indicating standard error. Vertical dashed lines denote thetraining environment Q. (Left) Average planner performance at convergence. (Right) Performancerelative to the vanilla MARL baseline, which is trained without a robustness objective.
Figure 5: Learning robust optimal taxation in an economic simulation. Results are plotted followingthe same conventions in Figure 4. Due to the intractability of other robustness algorithms in thissetting, we instead compare against different planner choices. “US Federal” uses a fixed tax schemeadapted from the 2018 US Federal income tax rates. “Saez” uses an adaptive, theoretical formula toestimate optimal tax rates.
