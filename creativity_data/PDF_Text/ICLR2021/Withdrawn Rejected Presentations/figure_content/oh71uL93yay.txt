Figure 1: A graph with two classes of nodes,while white nodes are unlabeled (Figure 1a). Toclassify nodes, our model will increase the con-necting strength among nodes within the sameclass, thereby increasing their feature/label influ-ence on each other. In this way, our model is ableto identify potential intra-class edges (bold linksin Figure 1b) and strengthen their weights.
Figure 2: Node embeddings of Zachary’s karate club network trained on a node classification task(red vs. blue). Figure 2a visualizes the graph. Node coordinates in Figure 2b-2e are the embeddingcoordinates. Notice that GCN does not produce linearly separable embeddings (Figure 2b vs. Figure2c), while GCN-LPA performs much better even in the presence of noisy edges (Figure 2d vs. Figure2e). Additional visualizations are included in Appendix D.
Figure 3: Sensitivity to thenumber of LPA iterations onCiteseer dataset.
Figure 4: Sensitivity to λ(weight of LPA loss) on Cite-seer dataset.
Figure 5: Training time Perepoch on random graphs.
Figure 6: Visualizationof learned edge weightsin Coauthor-CS dataset.
Figure 7: An illustrating example of label propagation in LPA. Suppose labels are propagated forthree iterations, and no self-loop exists. Blue nodes are labeled while white nodes are unlabeled. (a)va’s label propagates to v1 (yellow arrows). Note that the propagation of va’s label to v3 is cut offsince v3 is labeled thus absorbing va ’s label. (b) va ’s label that propagated to v1 further propagatesto v2 and vb (yellow arrows). Meanwhile, va’s label is reset to its initial value then propagates fromva again (green arrows). (c) Label propagation in iteration 3. Purple arrows denote the propagationof va ’s label starting from va for the third time. (d) All possible paths of length no more than threefrom va to vb containing unlabeled nodes only. Note that there is no path of length one from va tovb.
Figure 8: Visualization of GCN and GCN-LPA with 1 〜4 layers on karate ClUb network.
