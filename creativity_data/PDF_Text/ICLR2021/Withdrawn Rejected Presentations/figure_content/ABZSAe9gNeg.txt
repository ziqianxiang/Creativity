Figure 1: Block diagram of DP-CTGAN model. CTGAN model.
Figure 2: Teacher Discriminator of PATE-4	Evaluation: Metrics, Infrastructure and Public BenchmarksWe focus on two sets of metrics in our benchmarks: one for comparing the distributional similarityof two datasets and another for comparing the utility of synthetic datasets given a specific predictivetask. These two dimensions should be viewed as complementary, and in tandem they capture theoverall quality of the synthetic data.
Figure 3: Real Car F1 Score: 0.97Figure 4: Mushroom pMSEExperimental Results: Public Datasets We ran experiments on five public real datasets, whichhelped inform the applied scenario discussed in Section 5. Full details of the experiments can befound in the Appendix, Figures 25-30. We will refer to the datasets as Adult, Car, Mushroom,Bank and Shopping, and will discuss a handful of the results here in terms of their machine learningutility and their statistical similarity to the real data. The individual synthesizer’s are color codedconsistently across plots, and their performance is tracked according to dataset (So “dpctgan_car”tracks the graphed metric for DPCTGAN on the Car dataset).
Figure 4: Mushroom pMSEExperimental Results: Public Datasets We ran experiments on five public real datasets, whichhelped inform the applied scenario discussed in Section 5. Full details of the experiments can befound in the Appendix, Figures 25-30. We will refer to the datasets as Adult, Car, Mushroom,Bank and Shopping, and will discuss a handful of the results here in terms of their machine learningutility and their statistical similarity to the real data. The individual synthesizer’s are color codedconsistently across plots, and their performance is tracked according to dataset (So “dpctgan_car”tracks the graphed metric for DPCTGAN on the Car dataset).
Figure 5: ML evaluation results for internal dataset(b) RegressionWe ran our evaluation suite on the applied internal data scenarios to generate the synthetic data fromeach DP synthesizer and benchmark standard ML models. We also applied a Logistic Regressionclassifier with differential privacy from IBM. (Chaudhuri et al., 2011; diffprivlib) to the real dataas a baseline. Figure 5a shows the ML results from our evaluation suite. As expected, as the pri-vacy budget increases, performance generally improves. DP-CTGAN had the highest performancewithout the QUAIL enhancement. QUAIL, however, improved the performance of all synthesizers.
Figure 6: Privacy budgetFigure 7: Privacy budgetPeeling Back QUAIL: Analysis of via clustering and direct comparison By first assessing theTSNE clustering in Figures 8 and 9, we see that not only is the synthetic data produced by QUAILvery similar to the real data, but the accuracy of the labeling for the embedded model (in this case,DPLR) is also very similar. Further investigation into data scale revealed that the QUAIL methodtakes advantage of allocating excess epsilon when datasets are large. As datascale increases, thesensitivity of the differentially private model decreases and so less epsilon can be used more effi-ciently. Thus, we see that an exaggerated difference between DPLR embedded in QUAIL (with anepsilon of 2.4) and DPLR with an epsilon of 3.0 for a dataset of 20,000 samples. In this case, theembedded DPLR model accuracy suffers, and so does the learning utility of the produced syntheticdata. Conversely, as we increase the data size to 50,000 and 100,000 samples, we see that the in-ternal model (with epsilon 2.4) can match the performance of the vanilla model (with epsilon 3.0).
Figure 7: Privacy budgetPeeling Back QUAIL: Analysis of via clustering and direct comparison By first assessing theTSNE clustering in Figures 8 and 9, we see that not only is the synthetic data produced by QUAILvery similar to the real data, but the accuracy of the labeling for the embedded model (in this case,DPLR) is also very similar. Further investigation into data scale revealed that the QUAIL methodtakes advantage of allocating excess epsilon when datasets are large. As datascale increases, thesensitivity of the differentially private model decreases and so less epsilon can be used more effi-ciently. Thus, we see that an exaggerated difference between DPLR embedded in QUAIL (with anepsilon of 2.4) and DPLR with an epsilon of 3.0 for a dataset of 20,000 samples. In this case, theembedded DPLR model accuracy suffers, and so does the learning utility of the produced syntheticdata. Conversely, as we increase the data size to 50,000 and 100,000 samples, we see that the in-ternal model (with epsilon 2.4) can match the performance of the vanilla model (with epsilon 3.0).
Figure 8: Privacy budget = 3.0Figure 9: Privacy budget = 3.0ι.o-∣----DPLR (in QUAIL) vs. DPLR (vanilla) vs. Random Forest trained on QUAIL Synthetic Data--------------------∣-ι.othat PATECTGAN had better utility and statistical similarity in scenarios with high privacy budget( >= 3.0) when compared to the other synthesizers we benchmarked. Conversely, with low privacybudget ( <= 1.0) we found that DPCTGAN had better utility, but PATECTGAN may still be betterin terms of statistical similarity.
Figure 9: Privacy budget = 3.0ι.o-∣----DPLR (in QUAIL) vs. DPLR (vanilla) vs. Random Forest trained on QUAIL Synthetic Data--------------------∣-ι.othat PATECTGAN had better utility and statistical similarity in scenarios with high privacy budget( >= 3.0) when compared to the other synthesizers we benchmarked. Conversely, with low privacybudget ( <= 1.0) we found that DPCTGAN had better utility, but PATECTGAN may still be betterin terms of statistical similarity.
Figure 10: Privacy budget = 3.0method	classification		regression		6: 3	6: 6	6: 3	6: 6dpgan	1130	3950	1300	4335quail_dpgan_p0.9	^^0	58	202	^^03quail_dpgan_p0.5	^305		444	1323dpctgan	15689	24808	3989	14029quail_dpctgan_p0.9	^T99	759	198	^318quail_dpctgan_p0.5	4478		1171	3964Pategan	~Γ1	-206	224	^389quail_pategan_p0.9	^T8	^T5	152	^T56quail_pategan_p0.5	^31	-69	165	^^10PateCtgan	^T60	-449	263	-474quail_patectgan_p0.9	^T6	^T9	152	^T55quail_patectgan_p0.5	56	131	185	270Table 1: Time Performance Analysis of QUAIL compared to other synthesizers (time is shown inseconds)4.	Using pMSE. pMSE can be used alongside ML utility metrics to balanced experiments. pMSEconcisely captures statistical similarity, and allows practitioners to easily balance utility against thedistributional quality of their synthetic data.
Figure 11: Budget E = 1.0Figure 12: Budget E = 3.0Figure 13: Budget E = 10.0Figure 14: Budget E = 1.0Figure 15: Budget E = 3.0Figure 16: Budget E = 10.0D	EvaluationsD.1 Description of InfrastructureOur experimental pipeline provides an extensible interface for loading datasets from remote hosts,specifically from the UCI ML Dataset repository (Dua & Graff, 2017). For each evaluation dataset,15Under review as a conference paper at ICLR 2021Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0
Figure 12: Budget E = 3.0Figure 13: Budget E = 10.0Figure 14: Budget E = 1.0Figure 15: Budget E = 3.0Figure 16: Budget E = 10.0D	EvaluationsD.1 Description of InfrastructureOur experimental pipeline provides an extensible interface for loading datasets from remote hosts,specifically from the UCI ML Dataset repository (Dua & Graff, 2017). For each evaluation dataset,15Under review as a conference paper at ICLR 2021Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0
Figure 13: Budget E = 10.0Figure 14: Budget E = 1.0Figure 15: Budget E = 3.0Figure 16: Budget E = 10.0D	EvaluationsD.1 Description of InfrastructureOur experimental pipeline provides an extensible interface for loading datasets from remote hosts,specifically from the UCI ML Dataset repository (Dua & Graff, 2017). For each evaluation dataset,15Under review as a conference paper at ICLR 2021Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-
Figure 14: Budget E = 1.0Figure 15: Budget E = 3.0Figure 16: Budget E = 10.0D	EvaluationsD.1 Description of InfrastructureOur experimental pipeline provides an extensible interface for loading datasets from remote hosts,specifically from the UCI ML Dataset repository (Dua & Graff, 2017). For each evaluation dataset,15Under review as a conference paper at ICLR 2021Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary process
Figure 15: Budget E = 3.0Figure 16: Budget E = 10.0D	EvaluationsD.1 Description of InfrastructureOur experimental pipeline provides an extensible interface for loading datasets from remote hosts,specifically from the UCI ML Dataset repository (Dua & Graff, 2017). For each evaluation dataset,15Under review as a conference paper at ICLR 2021Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novel
Figure 16: Budget E = 10.0D	EvaluationsD.1 Description of InfrastructureOur experimental pipeline provides an extensible interface for loading datasets from remote hosts,specifically from the UCI ML Dataset repository (Dua & Graff, 2017). For each evaluation dataset,15Under review as a conference paper at ICLR 2021Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,
Figure 17: Budget = 1.0Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 18: Budget = 3.0Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 19: Budget = 10.0Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 22: Budget = 10.0Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 20:	Budget = 1.0Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 21:	Budget = 3.0Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 23: Budget = 1.0Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 24: Budget = 3.0Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 25: Budget = 10.0we launch a process that synthesizes datasets for each privacy budget (s) specified on each syn-thesizer specified. Once the synthesis is complete, the pipeline launches a secondary processthat analyzes the synthetic data, training classifiers and running the previously mentioned novelmetrics. The run is launched, and the results are logged, using MLFlow runs (Zaharia et al.,2018) with an Azure Machine Learning compute-cluster backend. Our compute used CPU nodesSTANDARD.NC24r (24 Cores, 224 GB RAM, 1440 GB Disk) and GPU nodes GPU (4 XNVIDIA Tesla K80). We highly encourage future work into hyperparameter tuning for differen-tially private machine learning tasks, and believe our evaluation pipeline could be of some use inthat effort.
Figure 26: Data distribution of the internal dataset for various attributes. Included to highlight theimbalanced nature, difficulty of supervised learning problem.
Figure 27: pMSE evaluation results for internal dataset. PATECTGAN performed best in both cases.
Figure 28: PATECTGAN demonstrated better performance at higher epsilons. QUAIL synthesizersperformed best at low epsilon privacy values.
Figure 30: DPCTGAN outperformed other synthesizers by significant margins with Adult, both interms of ML utility and statistical similarity.
Figure 29: Mushrooms AUC-ROC curve demonstrated that the QUAIL synthesizers might not gen-eralize particularly well.
Figure 31: As the most complex benchmark dataset, Bank presented a particular challenge. Theresults are difficult to interpret, and would require further experimentation to draw conclusions.
Figure 33: We see similar results here to our Bank dataset. Bank and Shopping appear to have beentoo complex for the synthesizers at the relatively low epsilon budgets provided.
Figure 32: Despite the noisy plots, at higher epsilon values, based F1-Scores and this pMSE plot, itdoes appear as though DPCTGAN and PATECTGAN improved on the other synthesizers.
