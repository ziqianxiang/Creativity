Figure 1: Mapping of VAE to implicit isometric embedding.
Figure 2: PDFs of three variables to generate a toy dataset.
Figure 3: Scattering plots of the data generation probability (x-axis) versus four estimated prob-abilities (y-axes) for the downward-convex loss. y-axes are (a) p(μ(x)), (b) exp(-Lx/β), (c)aX/2p(μ(x)) Qj σj(χ), and (d) aX/2 exp(-Lχ/β).
Figure 4: Graph of σj(x) -2average and 2 σ7∙㈤2Dj (Z) inVAE for CelebA dataset.
Figure 5: Graph of σj(x) -2average and ∙∣ σj∙(x)2Dj (z) inVAE for CelebA dataset withexplicit decomposed loss.
Figure 6: Dependency ofdecoded image changes withzj = -2 to 2 on the averageof σj(x) .
Figure 7: Projection of the volume element from the implicit orthonormal space to the isometricspace and input space. Vn (∙) denotes n-dimensional volume.
Figure 8:	Plots of the data generation probability (x-axis) versus estimated probabilities (y-axes) forthe square error loss. y-axes are (a) p(μ(χ)), (b) exp(-Lχ/β), and (c) p(μ(χ)) Qj。人况).
Figure 10: Scale factor ax for thedownward-convex loss and upward-convex loss.
Figure 9:	PDFs of three variablesto generate a Ramp dataset.
Figure 11: Property measurements of the Mix dataset using the square error loss. λ is changed from1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 12: Property measurements of the Mix dataset using the downward-convex loss. λ is changedfrom 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 13: Property measurements of the Mix dataset using the upward-convex loss. λ is changedfrom 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 14: Property measurements of the Ramp dataset using the square error loss. λ is changedfrom 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 15: Property measurements of the Ramp dataset using the downward-convex loss. λ ischanged from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 16: Property measurements of the Ramp dataset using the upward-convex loss. λ is changedfrom 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 17: Property measurements of the Norm dataset using the square error loss. λ is changedfrom 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 18: Property measurements of the Norm dataset using the downward-convex loss. λ ischanged from 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 19:	Property measurements of the Mix dataset using the upward-convex loss. λ is changedfrom 1 to 1, 000. Var(zj) denotes the estimated variance, given by the average of σj-(2x).
Figure 20:	Ratio of the estimated variances Var(z3)/Var(z1) and Var(z2)/Var(z1) for the threedatasets and three coding losses at λ = 100. Var(zj ) denotes the estimated variance, given by theaverage of σj-(2x) .
Figure 21: Dependency of Coding Loss onβ for Mix, Norm, and Ramp dataset usingsquare loss.
Figure 22: Dependency of Transform loss /Coding Loss Ratio on β for Mix, Norm, andRamp dataset using square loss.
Figure 23: Traversed outputs for all the component, changing zj from -2 to 2. The latent variableszj are numbered in descending order by the estimated variance σj-2 shown in Figures 4 and 5.
Figure 24: Graph of σj∙(x)-2 average and ∣σj∙(x)2Dj (Z) in CelebA dataset. The bottleneck size andλ are set to 256 and 10000, respectively.
Figure 25: Graph of σj∙(x)-2 average and 2σj∙(x)2Dj (Z) in MNIST dataset.
Figure 26: Graph of 1 (1 +in the BCE approximation.
Figure 27: Probability for a symbol with mean μ and noise σ2Using log(1 + X) = X + O(x2) expansion, Rμσ is derived as:Rzj = - log Pzj ' 2 (μj(x)2 + σj(x)2 - log σj(x)2 - log ∏) = DKLj(X)G) + 2 log ɪ. (8I)When Rzj and DKLj(x)(∙) in Eq. 2 are compared, both equations are equivalent except a smallconstant difference 2 log(πe∕6) ` 0.176 for each dimension. As a result, KL divergence for j-thdimension is equivalent to the rate for the uniform quantization coding, allowing a small constantdifference.
