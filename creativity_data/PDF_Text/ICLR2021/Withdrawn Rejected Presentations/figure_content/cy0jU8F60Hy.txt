Figure 1: Illustration of minimizing the CT divergence Cψ,θ(μ, ν) between N(0,1) and N(0,eθ). Left:Evolution of the CT divergence, its parameters and forward and backward costs, and corresponding Wassersteindistance; Middle: Gradients of the CT w.r.t. θ or φ. The 2D trace of (θ, eφ) is marked with red arrows. Right:The forward, backward, and CT values against θ when eφ is optimized to a small value, which show combiningforward and backward balances mode covering and seeking, making it easier for θ to move towards its optimum.
Figure 2: Illustration of how minimizing the ACT diver-gence between the empirical distribution of a generator andthat of a bimodal Gaussian mixture, whose 5000 randomsamples are given, helps optimize the generator distribu-tion towards the true one. Top: Plots of the ACT diver-gence C(^n, VM), forward ACT cost C(μN → VM), back-ward ACT cost C(^n — ^m), and Wasserstein distanceW2(μN, ^m)2, where N = M = 5000. Bottom: The PDFof the true data distribution μ(dx) = PX (x)dx (red) and thegenerator distribution ν(dy) = pθ (y)dy (blue, visualized viakernel density estimation) at different training iterations.
Figure 3: Top: Plot of the sample Wasserstein distance W2(μ5000, ^5000)2 against the number of trainingepochs, where the generator is trained With either W2(μN, VN)2 or the ACT divergence between μN and VN,with the mini-batch size set as N = 20 (left), N = 200 (middle), or N = 5000 (right); one epoch consists of5000/N SGD iterations. Bottom: The fitting results of different configurations, where the KDE curves of thedata distribution and the generative one are marked in red and blue, respectively.
Figure 4: Comparison of the generation quality on 8-Gaussianmixture data: one of the 8 modes has weight ρ and the rest modeshave equal weight as 1-ρ.
Figure 5: Fitting 1D bi-modal Gaussian (top) and 2D8-Gaussian mixture (bottom) by interpolating betweenthe forward ACT (γ = 1) and backward ACT (γ = 0).
Figure 6: Generated samples of the deep generative model that adopts the backbone of SNGAN but is optimizedwith the ACT divergence on CIFAR-10, CelebA, and LSUN-Bedroom. See Appendix B for more results.
Figure 7: Analogous plot to Fig. 6 for Left: LSUN-Bedroom (128x128) and Right: CelebA-HQ (256x256).
Figure 8: For the univariate normal based toy example specified in (17), we plot the forward,backward, and CT values against θ at four different values of φ, which show combining forward andbackward balances mode covering and seeking, making it easier for θ to move towards its optimum.
Figure 9: Top: The risk for the forward and backward conditional transport plans to degeneratew.r.t. the value of φ based on CT analysis. Bottom: The forward conditional transport plan betweentwo discrete sets of S = 500 points; among the S data points, one point is an outlider. The ACTnavigators are optimized with SGD over mini-batches, whose elements are randomly sampled (withreplacement) from their corresponding discrete sets and the mini-batch size varies from 50 to 500.
Figure 10: On a 8-Gaussian mixture data, comparison of generation quality and training stabilitybetween two mini-max deep generative models (DGMs), including vallina GAN and WassersteinGAN with gradient penalty (WGAN-GP), and two mini-max-free DGMs, whose generators aretrained under the sliced Wasserstein distance (SWD) and the proposed ACT divergence, respectively.
Figure 11: Analogous plot to Fig. 10 for the Swiss-Roll dataset.
Figure 12: Analogous plot to Fig. 10 for the Half-M∞n dataset.
Figure 13: Analogous plot to Fig. 10 for the 25-Gaussian mixture dataset.
Figure 14: Visual results of ACT for generated samples (blue dots) compared to real samples (reddots) on Swiss Roll, Half Moons, 8-Gaussian mixture, and 25-Gaussian mixture. The second andthird columns map the data points and their corresponding navigator logits by color; The fourth andfifth columns map the generated points and their corresponding navigator logits by color.
Figure 15: Visual results of GAN for generated samples (blue dots) compared to real samples (reddots) on Swiss Roll, Half Moons, 8-Gaussian mixture, and 25-Gaussian mixture. The second and thethird columns map the data points and their corresponding discriminator logits by color; The fourthand fifth columns map the generated points and the corresponding discriminator logits by color.
Figure 16:	Visual results of generated samples on MNIST and CIFAR-10 using pixel-wise transportcost, with DCGAN (standard CNN) backbone.
Figure 17:	Visual results of generated samples on MNIST and CIFAR-10 using pixel-wise transportcost, with SNGAN (ResNet) backbone. The Inception and FID scores are not shown due to poorvisual quality.
Figure 18:	Visual results of generated samples with the perceptual similarity (Zhang et al., 2018)with four different training configurations.
Figure 19: Visual results of using a standard cross-entropy discriminator loss in lieu of ACT diver-gence to train the critic of ACT.
Figure 20:	Unconditional generated samples and inception scores of MNIST, with DCGAN (standardCNN) backbone.
Figure 21:	Unconditional generated samples and FIDs of CIFAR-10, with DCGAN (standard CNN)backbone.
Figure 22: Unconditional generated samples and FIDs of CIFAR-10, with SNGAN (ResNet) back-bone.
Figure 23: Conditional generated samples and FIDs of CIFAR-10, with SNGAN (ResNet) backbone.
Figure 24: Generated samples and FIDs of CelebA, with DCGAN (standard CNN) backbone.
Figure 25: Generated samples and FIDs of CelebA, with SNGAN (ResNet) backbone.
Figure 26:	Generated samples and FIDs of LSUN, with DCGAN (standard CNN) backbone.
Figure 27:	Generated samples and FIDs OfLSUN, with SNGAN (ResNet) backbone.
