Figure 1: The φ(q)〜q and f(χ)〜X of SSELU (a) & (b) and ISELU (C) & (d) under different λ.
Figure 2: (1 + δq)〜q ofsSELU & lSELU under dif-ferent e.
Figure 3: The absolute value of the output mean under different input mean and variance.
Figure 4: The distribution of the second-moment of forward pre-activation and the Frobenius normof backward gradient on weights.
Figure 5: The distribution of output activations of sSELU and lSELU in different layers.
Figure 6: Influence of λ on1 + δq(38)We plot the relationship between the maximum (1 + δq) under q ∈(0, 2] and λ in Figure 6. Obviously, the maximum (1 + δq) decreaseswhen λ gets larger. This observation is quite intuitive. The 1 + δicharacterizes the relative deviation between E[f2(x)] and E[(df(x)/dx)2]E[x2]. For the positivepre-activations, we have22∞	e-Z-	∞	e—⅞-E[f 2(x+)] = J	λ√qz √= dz = λ J	√qz √= dz = E[(df (x+)∕dx+)2]E [x+].	(39)Hence, the deviation is contributed only by the negative part. With a larger λ, the positive activationsare scaled up, thus the negative activations have to be scaled down to preserve the overall secondmoment. Therefore, the negative part contributes less to the overall second moment, and the relativedeviation between E[f2 (x)] and E[(df(x)∕dx)2]E[x2] gets smaller. All in all, a larger λ leads tosmaller δq, and a smaller δq reduces the gradient explosion rate (1 + δq).
Figure 7: The μNι-ι with Increasing Fan-in.
