Figure 1: Illustration of our attention-augmented encoder. First, a stack of three image frames arepassed as input to a sequence of convolutional layers inside feature encoder (blue box). The featureencoder produces two sets of feature maps: (i) non-attentional features xf with shape C × Hf × Wf;(ii) attentional features xg with shape C × Hf × Wf. Attention block (orange box) takes in the twosets of feature maps as input, and outputs an attended copy of xf . Non-attentional and attentionalfeatures are re-weighted and added together via a residual connection.
Figure 2: Performances of DeepMind control suite tasks trained with SAC using different encoderarchitectures for pixel inputs. Our attention network (blue curve) significantly outperforms the base-line encoder, and achieves similar final performance with SAC trained from state inputs.
Figure 3: Learning curves on DeepMind control suite. The solid line and shaded regions representthe mean and standard deviation, respectively, across three runs. Our attention network alone, with-out any representation learning, world model fitting, or data augmentation techniques, performs onpar with the state-of-the-art methods.
Figure 4: Behaviour cloning performance comparison of our architecture (w/ Attention Module) andthe architecture used in RAD (Laskin et al., 2020) (w/o Attention Module). Additionally, we userandom cropping data augmentation when training the network with the RAD architecture. Sinceour architecture have additional encoders that add more parameters, we also include a wider versionof the architecture used in RAD (Filter Size = 64) that has more parameters than our architecture(Filter Size = 32) for comparison. Even with random cropping, the architecture used in RAD under-performs our architecture. See more details of the offline experimental setup in Appendix A.
Figure 5: Visualization of outputs from the proposed attention module. Our agent demonstrates aselective feature-extraction scheme that pays more attention to the most relevant locations for eachchosen action.
Figure 6: We evaluate the ablation encoder p0 on a subset of visual control tasks, and demonstratethat, although the original shallow encoder p uses a smaller capacity, it outperforms the ablationencoder consistently across the tested domains.
Figure 7: Ablation illustrations and learning curves on Walker-walk, Cheetah-run, Finger-spin andHopper-stand.
Figure 8: (a) Illustration of direction addition and concatenation, as alternatives to the adaptive scal-ing method used in our main architecture. (b) Performance comparison between using the ablationcombination methods and adaptive scaling using σ.
