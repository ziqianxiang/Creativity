Figure 3: Relative informationand n comparison. For each valueof n we observe a specific orderbetween the compression ratios.
Figure 4: Information gain andFLOPs comparison. It is observedthat different compression ratiosare optimal (in terms of FLOPs)for different desired informationgain.
Figure 1: n-accuracy and FLOPscomparison. Itis observed that dif-ferent compression ratios are opti-mal (in terms of FLOPs) for differ-ent desired n-accuracy.
Figure 2: Log error and FLOPscomparison, where the error issimply 1 - n-accuracy/100 andthe binary logarithm is used.
Figure 5: n-accuracy and information gaincomparison for the first 3 compression ratios.
Figure 6: n-accuracy and information gaincomparison for the last 3 compression ratios.
Figure 7: Relative information and compres-sions comparison for different values of n.
Figure 8: n-accuracy on Tiny ImageNet with various shared-label prediction methods.
Figure 9: Relative information on Tiny ImageNet with variousshared-label prediction methods.
Figure 10: The unified CNN-LSTM architectureA.2 Long Short Term Memory Networks (LSTM)As mentioned earlier, since the objective is to characterize the high-order label dependency in thesame sequence (data points embedding in the same sequence share the same label), we employ longshort term memory (LSTM) neurons (Hochreiter & Schmidhuber, 1997) as our recurrent neurons.
Figure 11: n-accuracy and FLOPs comparison. It is observedthat different compression ratios are optimal (in terms of FLOPs)for different desired n-accuracy.
Figure 12: Log error and FLOPs comparison, where the error issimply 1 - n-accuracy/100 and the binary logarithm is used.
Figure 13: Relative information and n comparison. For eachvalue of n we observe a specific order between the compressionratios.
Figure 14: Information gain and FLOPs comparison. It is ob-served that different compression ratios are optimal (in terms ofFLOPs) for different desired information gain.
Figure 15: n-accuracy and information gain comparison. Weobserve that there exists a very high correlation between the twomeasures in our experimental setup.
Figure 16: n-accuracy and FLOPs comparison. It is observedthat different compression ratios are optimal (in terms of FLOPs)for different desired n-accuracy.
Figure 17: Log error and FLOPs comparison, where the error issimply 1 - n-accuracy/100 and the binary logarithm is used.
Figure 18: Relative information and n comparison. For eachvalue of n we observe a specific order between the compressionratios.
Figure 19: Information gain and FLOPs comparison. It is ob-served that different compression ratios are optimal (in terms ofFLOPs) for different desired information gain.
Figure 20: n-accuracy and information gain comparison. Weobserve that there exists a very high correlation between the twomeasures in our experimental setup.
Figure 21: n-accuracy on MNIST with various shared-label pre-diction methods.
Figure 22: Relative information on MNIST with various shared-label prediction methods.
