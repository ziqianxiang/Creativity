Figure 2: The network structure for the proposed implementation i-ML-Enc. The first L - 1 layersequidimensional mapping in the green dash box are the first stage which achieves s-sparse, andthey have an inverse process in the purple dash box. (a) is a layer of nonlinear homeomorphismtransformation (red arrow). (b) linearly transforms (blue arrow) s-sparse representation in Rm intoRs0 as the second stage. (c) are the extra heads by linear transformations. (d) indicates the paddingzeros of the l-th layer to force d(l) -sparse.
Figure 3: Sparsity and clustering effect.
Figure 4: Visualization of invertible NLDR results of i-ML-Enc compared to ML-Enc and t-SNE.
Figure 5: (a) shows the proposed geodesics interpolation on a manifold; (b) reports the MSE loss of 1to 10 nearest neighbors interpolation results on interpolation datasets.
Figure 6: The results of kNN interpolation in latent space. For each dataset, the upper row showsthe latent result, while the lower shows the input result. The latent results show more noise but lessoverlapping and pseudo-contour than the input results.
Figure 7: Visualization of invertible NLDR results of i-ML-Enc with comparison to Isomap, ML-Enc,and t-SNE on Swiss roll and five real-world datasets. The target dimension s0 = 2 for all datasets,and the high-dimensional latent space are visualized by PCA. For each row, the left five cells showthe NLDR and reconstruction process in the first stage of i-ML-Enc, and the right four cells show2D results for comparison. ML-Enc and t-SNE show great clustering effects but drop topologicalinformation. Compared to the classical DR method Isomap (preserving the global geodesic distance)and t-SNE (preserving the local geometry), the representations learned by i-ML-Enc preserves therelationship between clusters and the local geometry within clusters.
Figure 8: Visualization of reconstruction results of i-ML-Enc on six image datasets. For each cell, theupper row shows results of i-ML-Enc while the lower row shows the raw input images. We randomlyselected 10 images from different classes to demonstrate the bijective property of i-ML-Enc.
Figure 9: Visualization of kNN interpolation results of i-ML-Enc on image datasets with k ≤ 10 andk ≥ 20. For each row, the upper part shows results of i-ML-Enc while the lower part shows the rawinput images. Both the input and latent results transform smoothly when k is small, while the latentresults show more noise but less overlapping and pseudo-contour than the input results when k islarge. The latent interpolation results show more noise and less smoothness when the data manifoldbecomes more complex.
Figure 11: Visualization analysis of the representation from the L - 1-th layer of i-ML-Enc. (a) and(b) show the margin of the non-diagonal elements and the diagonal elements with W7T W7 trained onSwiss roll (50x50) and MNIST (784x784). These elements are divided into 7 orders of magnitudeafter min-max normalization, indicating the large margine of the diagonal and non-diagonal elements.
