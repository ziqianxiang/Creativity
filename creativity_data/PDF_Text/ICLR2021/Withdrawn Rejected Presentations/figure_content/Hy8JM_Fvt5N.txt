Figure 1: Hypothesis: bi-half maximizes the entropy. Entropy of binary weights and activations. We compare our bi-half method to sign(Rastegari et al. 2016) and IR-Net (Qin et al. 2020b). (a) Entropy of the binary weights during training for Conv2 on Cifar10. (b) Entropy ofthe network activations for ResNet-18 on Cifar100. Our bi-half model can guarantee maximum entropy during training for the binary weightdistribution and it is able to better maximize the entropy of the activations.
Figure 2: Hypothesis: bi-half maximizes the entropy. Bit flip-s during training. We compare the bit flips during training in ourbi-half with the sign (Rastegari et al. 2016) and IR-Net (Qin et al.
Figure 3: Empirical analysis (a): Effect of hyper-parameters. We show the effect of weight decay and learning rate decay on binaryweights flips using the Conv2 network on Cifar-10. Carefully tuning these hyper-parameters is important for adequately training the binarynetworks.
Figure 4: Empirical analysis (c): Optimization benefits. We train our bi-half model 100 times on Cifar-10 and plot the distribution of thelosses and accuracies over the 100 repetitions. We compare our results using optimal transport to the results using the standard sign function.
Figure 5: Empirical analysis (b): Which bit-ratios are pre-ferred? We varying the bit-ratios on Cifar-10 and Cifar-100 usingConv2 the choice of the priorppos under the Bernoulli distribution.
Figure 6: Empirical analysis (c): Optimization benefits. Bi-half regularization: 2D example for a 12-parameter fully connected binarynetwork σ(w2lσ(w1 Tx + bι)), where σ(∙) is a sigmoid nonlinearity. Weights are in {-1,1}. (a) Enumeration of all decision boundaries for12 binary parameters (4096 = 212 combinations). (b) Weight combinations and unique solutions when using our bi-half constraint. (c) Theweight combinations and unique decision boundaries for various bit-ratios. When the number of negative binary weights is 6 on the x-axis,we have equal bit-ratios, which is the optimal ratio. Using the bi-half works as a regularization, reducing the search-space while retaining themajority of the solutions.
Figure 7: Architecture variations: Different architectures on Cifar-100. We evaluate on Cifar-100 over 5 different architectures: VG-G16 (Simonyan and Zisserman 2015), ResNet18 (He et al. 2016), ResNet34 (He et al. 2016), InceptionV3 (Szegedy et al. 2016), Shuf-fleNet (Zhang et al. 2018). We compare sign (Rastegari et al. 2016), IR-Net (Qin et al. 2020b) and our bi-half . The 1/32 and 1/1 indicatethe bit-width for weights and for activations, where 1/1 means we quantize both the weights and the activations to binary code values. Ourmethod achieves competitive accuracy across different network architectures.
Figure 8: Comparison with state-of-the-art (b): Pruned networks. Test accuracy of Conv2/4/6/8 on CIFAR-10, and ResNet-18 on CIFAR-100 when varying the % pruned weights. We compare with the MPT baseline (Diffenderfer and Kailkhura 2021) using binary weight maskingand the sign function. Having equal +1 and -1 ratios is also optimal when the networks rely on pruning and that our optimal transportoptimization can easily be adapted to work in combination with pruning.
