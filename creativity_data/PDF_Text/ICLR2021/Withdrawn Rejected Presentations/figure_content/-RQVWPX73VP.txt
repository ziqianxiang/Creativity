Figure 1: Left: The whole computation graph for K-shot meta-RL, where θ is the meta-policyparameter, D is the trail buffer storing transitions and context in trail phase, T is the task samplefrom task distribution P(T), V π is the value function evaluating policy π. Straight lines representfor forward path and curved lines are backward path. K-shot Meta-RL optimizes the average returnafter policy adaptation using trail buffer. Right: The LSTM context encoder structure. A transitionis first embeded by a linear layer and fed through LSTM to form context ct at timestep t. The hiddenstate will be kept between different episodes.
Figure 2: The 2d navigation task result. The policy converged, and the performance is close to maxscore in task.
Figure 3: meta-policy and adaptation in 2d navigation, where pink circles are the goals green circleis zero reward area. Agent not in the right part corresponding the current goal or in green circle willget zero reward. In each task agent has three trails to collect data, then perform the test phase.
Figure 4: meta-policy and adaptation in mujoco environments. The performance is compared withprevious gradient-based algorithms. The performance is better than the previous algorithms.
