Figure 1: Causal diagram in 2-agent MarkovGame: (a) Standard MARL, (b) IntroducingHowever, direct optimization of the objective func-tion (2) is not easy. Fig. 1(a) shows the causaldiagram of the considered system model describedin Section 3 in the case of two agents with de-centralized policies. Since we consider the caseof no explicit dependency, the two policy distribu-tions can be expressed as π1(at1∖st) and π2(at2∖st).
Figure 2: Overall operation of the proposed VM3-AC. We only need the operation in the red boxafter training.
Figure 3: Performance of MADDPG (blue), MA-AC (green), MAVEN (purple), IMOA (black), andVM3-AC (the proposed method, red) on multi-walker environments (a)-(b), predator-prey (c)-(e), andcooperative navigation (f). (MW, PP, and CN denote multi-walker, predator-prey, and cooperativenavigation, respectively)decentralized policy for each agent. 2) Multi-agent actor-critic (MA-AC) - a variant of VM3-AC(β = 0) without the latent variable. 3) Multi-agent variational exploration (MAVEN) (Mahajan et al.
Figure 4: The positions of four agents after five time-steps after the episode begins in the early stageof the training: 1st row - VM3-AC and 2nd row - MA-SAC. The figures in column correspond to adifferent seed. The black squares are the preys and each color except black shows the position ofeach agent.
Figure 5: (a) and (b): VM3-AC (red), VM3-AC without latent variable (orange), and MA-SAC (cyan)and (c) and (d): performance with respect to the temperature parameteris shown in Table 3. It is seen that the zero vector replacement method yields almost the sameperformance and enables fully decentralized execution without performance loss.
Figure 6: Considered environments: (a) Multi-walker, (b) Predator-prey, and (c) Cooperativenavigationin the same position and increase the value of R1. Thus, the different coordinated behavior is neededas N and C change. The observation of each agent consists of relative positions between agents andother agents and those between agents and the preys. Thus, each agent can access to all informationof the environment state. The action of each agent is two-dimensional physical action. We setR1 = 10 and T = 100. We simulated the environment with three cases: (N = 2, M = 16, C = 1),(N = 3,M = 16,C = 1) and (N = 4,M = 16,C = 2).
