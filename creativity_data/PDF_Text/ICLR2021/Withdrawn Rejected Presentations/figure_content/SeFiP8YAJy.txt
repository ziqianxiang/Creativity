Figure 1: Training paradigm based on adjoined networks. The original and the compressed versionof the network are trained together with the parameters of the smaller network shared across both.
Figure 2: (Top) Standard 2d-convolution operation. The inputs, outputs and convolution filters are all3d volumes. (Bottom) Adjoint convolution operation. The convolution layer receives two inputs inp1and inp2. Standard convolution operation is applied on inp1 to get out1. For the second input, only afraction of the conv filters are used (rest can be treated as zero or omitted) to get out2.
Figure 3: (Left) Plot of validation cross-entropy loss of the adjoint-16 full and standard resnet50network on CIFAR-100. (Right) Plot of validation cross-entropy loss of the adjoint-4 full and standardresnet50 network on ImagenetAdjoint-full vs standard				Network	Dataset	Mask matrix (M)	Adjoint-full	StandardReSnet-50	Cifar-10	ai6	^9T34	^9025	Cifar-100	ai6	69.07	65.31	Imagenet	a4	75.84	73.41	Imagewoof	a4 * r0.9	86.3	85.2	Pets	_O2		87	85.31ReSnet-18	Cifar-10	a，8	""90.26	ɪɪj	Cifar-100	a4	66.84	61.39	Imagewoof	a4	84.16	83.35	Pets	_O8		86.33	84.84Table 2: The last two columns show the accuracies (in %) of the network trained in the adjointfashion and the same network trained in the standard way. In all cases the adjoint network exceedsthe accuracy of the standard full network. aα, rβ are as in Table 1. Detailed results can be found inthe appendix .
Figure 4: (Left) Plot of validation cross-entropy loss of the adjoint-16 full and standard + dropoutresnet50 network on CIFAR-100. (Right) Plot of validation cross-entropy loss of the adjoint-4 fulland standard + dropout resnet50 network on ImagenetAdjoint-full Vs standard + dropouts				Network	Dataset	Mask matrix (M)	Full	Std + dropoutReSnet-50	Cifar-10	a16	-9T34	^90∏	Cifar-100	ai6	69.07	66.66	Imagenet	a4	75.84	69.54	Imagewoof	a4 * r0.9	86.3	82.76	Pets	a4	86.73	84.7ReSnet-18	Cifar-10	08	-90.26	-89.04	Cifar-100	a4	66.09	64.3	Imagewoof	a4	84.16	82.1	Pets	a		86.33	82.61Table 3: The last two columns show the accuracies (in %) of the network trained in the adjoint fashionvs the same network trained in the standard way using dropouts. In all cases the adjoint networkexceeds the accuracy of the standard one. aα, rβ are as in Table 1. Details can be found in theappendix .
Figure 5:	Architecture diagram for resnet50 network used in this paper. conv(ni, no) is a combinationof convolution layer with ni input and no output channels followed by a batch norm and relu layer.
Figure 6:	A ResBlock with l layers and input ni and output no.
Figure 7: Validation loss for the various training paradigms for resnet50 trained on Cifar-10. All theadjoint plots correspond to the bigger network.
Figure 8: Validation loss for the various training paradigms for resnet50 trained on Cifar-10. Valida-tion loss for the various training paradigms for resnet50 trained on Cifar-10. All the adjoint plotscorrespond to the bigger network.
Figure 9: Validation loss for the various training paradigms for resnet18 trained on Cifar-10. All theadjoint plots correspond to the bigger network.
Figure 10: Validation loss for the various training paradigms for resnet18 trained on Cifar-10. All theadjoint plots correspond to the bigger network.
Figure 11: Validation loss for the various training paradigms for resnet50 trained on Cifar-100. Allthe adjoint plots correspond to the bigger network.
Figure 12: Validation loss for the various training paradigms for resnet50 trained on Cifar-100. Allthe adjoint plots correspond to the bigger network.
Figure 13: Validation loss for the various training paradigms for resnet18 trained on Cifar-100. Allthe adjoint plots correspond to the bigger network.
Figure 14:	Validation loss for the various training paradigms for resnet18 trained on Cifar-100. Allthe adjoint plots correspond to the bigger network.
Figure 15:	Validation loss for the various training paradigms for resnet50 trained on Imagenet. Allthe adjoint plots correspond to the bigger network.
Figure 16: Validation loss for the various training paradigms for resnet50 trained on imagewoof. Allthe adjoint plots correspond to the bigger network.
Figure 17: Validation loss for the various training paradigms for resnet50 trained on imagewoof. Allthe adjoint plots correspond to the bigger network.
Figure 18: Validation loss for the various training paradigms for resnet18 trained on imagewoof. Allthe adjoint plots correspond to the bigger network.
Figure 19: Validation loss for the various training paradigms for resnet18 trained on imagewoof. Allthe adjoint plots correspond to the bigger network.
Figure 20: Validation loss for the various training paradigms for resnet50 trained on oxford-pets Allthe adjoint plots correspond to the bigger network.
Figure 21: Validation loss for the various training paradigms for resnet50 trained on oxford-pets. Allthe adjoint plots correspond to the bigger network.
Figure 22: Validation loss for the various training paradigms for resnet18 trained on oxford-pets. Allthe adjoint plots correspond to the bigger network.
Figure 23: Validation loss for the various training paradigms for resnet18 trained on oxford-pets. Allthe adjoint plots correspond to the bigger network.
Figure 24: Validation cross entropy loss for various regularization functions. The networks weretrained using Adj-4 mask matrix on cifar-100 using resnet-18.
