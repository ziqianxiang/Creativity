Figure 2: Accuracy predictions of a small super-network (x axis) and Bench201 results (y axis,in percent) for the top-50, top-250 (left and center columns) and 1000 randomly sampled (rightcolumn) benchmark networks on the cifar10-valid data set, ordered by accuracy. While all fiveoperations are available in the top row, Zero is masked in the bottom row.
Figure 1: ANAS-BenCh-201cell with operations on the or-ange edges3.3	TrainingIn our experiments we train various NAS-Bench-201 networks. Small variants have 2 cells per stage(total of 8 cells, with 3 stages and 2 fixed cells for spatial reduction) and 32 channels in the firstcell, which is roughly similar to common topology sharing methods. Medium sized networks have4 cells per stage and start with 64 channels.
Figure 3: Counting how many top-N Bench architectures use an operation at any position in thearchitecture (top plot in each of the two rows and three columns) and their total usage (each bottomplot) over the top-N networks and all possible architectures (X axis). The operations are Zero (green),Skip (blue), 1×1 Conv (orange), 3×3 Conv (cyan) and 3×3 Avg Pool (purple). One operation ismasked in each of the six pairs, except for the top left one. As an example, if3 of the top-10 networksuse at least one Pool operation, together a total of 4 Pool operations, and have all 5 operationsavailable, the usage is 3/10 = 0.3 while the share is 4/(5 ∙ 10) = 0.08.
Figure 4: Measuring ranking correlations and average accuracy (y axis) of the top N bench networks(x axis) in different search spaces (color), plotting the mean and std of five runs. We used small sized(S) super-networks in the top row and medium (M) sized ones in the bottom row.
Figure 5:	Adding Linear Transformers (LT) to Skip and Skip+Pool (color).
Figure 6:	Further variations to the super-network training (color).
Figure 7:	See Figure 4, the left column is kept the same, adding masking combinations with the1×1 Convolution (center column) and Skip (right column)13Under review as a conference paper at ICLR 202150	100	150	250	500 random1000Top N bench networksN d/ U=■=声N d。！，U=-=声0.40.20.0AOeJnO:?B UO=∙P=∙Λ10	2550	100	150	250	500 random729Top N bench networksS	τa=0.07-▼一,	S,	LT={Skip}	Ta= 0.50-*-1	S,	LT={Skip, Pool}	Ta= 0.71
Figure 8:	See Figure 5, further search space subsets with Linear Transformers.
Figure 9:	See Figure 9, the left column is kept the same, adding masking combinations with the1×1 Convolution (center column) and Zero+Pool (right column)CifarlOOlmaαeNetl6-120Figure 10:	The accuracy values (x axis) of all architectures on the NAS-Bench-201 data sets andtheir respective accuracy on the other three data sets (color, y axis).
Figure 10:	The accuracy values (x axis) of all architectures on the NAS-Bench-201 data sets andtheir respective accuracy on the other three data sets (color, y axis).
Figure 11:	Additional metrics for small super-networks that start with 32 (left) or 96 channels(right).
Figure 12: As Figure 11, adding linear transformers.
Figure 13:	Visualizing some metrics, similar to Figure 2, of the small and wide super-networks(Figure 12, left, green)17Under review as a conference paper at ICLR 2021N doɪaq'aM IX0.40.20.0-0.24 2 0 2 4. . . . .
Figure 14:	Super-networks with 1 to 5 normal cells per stage. Smaller sized weight-sharing super-networks are generally easier to train and better predictors in the full search space.
