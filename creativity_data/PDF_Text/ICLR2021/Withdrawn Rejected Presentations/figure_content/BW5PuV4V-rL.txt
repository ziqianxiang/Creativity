Figure 1: Undesirable solutions during SGD, visualized for MNIST with component weights πk .
Figure 2:	Exemplary results for centroids learned by SGD, trained on full images.
Figure 3:	Visualization of centroids after exemplary training runs (3 epochs) on high-dimensionaldatasets for sEM: Fruits 360 (left, 30 000 dimensions) and SVHN (right, 3 000 dimensions). Com-ponent entries are displayed “as is”, meaning that low brightness means low RGB values. Visibly,many GMM components remain unconverged, which is analogous to a sparse-component solutionand explains the low log-likelihood values especially for these high-dimensional datasets.
Figure 4: Visualization of different centered Gaussian function gk states controlled by σ.
Figure 5: Trend of clustering capabilities for SEM and SGD trained GMM. Comparison by Davies-Bouldin score (smaller is better) and Dunn index (higher is better) for the datasets MNIST, Fash-ionMNIST, NotMNIST, Devanagari and SVHN. The lines visualize the average metric score/indexvalues of 10 repetitions with its standard deviation.
