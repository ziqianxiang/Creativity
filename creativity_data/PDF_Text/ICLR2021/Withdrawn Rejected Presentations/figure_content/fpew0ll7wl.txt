Figure 1: Comparison of supervised (top-left), continual (top-middle), and few-shot learning (top-right) with Ned (bottom). The learner (grey box) accumulates data (dotted path), trains on given data(filled nodes), then is evaluated (empty nodes). The size of the node indicates the scale of the trainingor evaluation. Each color represents a different set of classes.
Figure 2: (a) Compares the rolling accuracy of various methods over the stream of data. Exemplar Tuning per-forms the best at all stages of the stream. (b) Compares the accuracy of NCM on novel classes across networkarchitectures. Contrary to prevailing thought, we find that deeper networks overfit less to the pretrain classes.
Figure 3: (a) Accuracy of standard training with MoCo & supervised pretraining. Surprisingly, MoCo accuracydecreases during the initial streaming phase. (b) ROC curves for unseen class detection. MDT outperforms allother OOD methods evaluated in Ned. (c) Standard training accuracy curve for a range of training frequenciesand epochs/training phase showing that over training can lead to lower accuracy. MACs (X total gradient updates.
Figure 4: The distribution of samples over the classes for Sequences 1 - 5. Classes with less than 50samples are considered in the tail and samples with greater than or equal to 50 samples are consideredin the head for the purpose of reporting.
Figure 5: The accuracy for the in-distribution (IND) and out-of-distribution (OOD) samples as the thresholdfor considering a sample out-of-distribution varies. The horizontal axis is the threshold value, and the verticalaxis is the accuracy. Intersection of the IND and OOD curves at a higher accuracy generally indicates betterout-of-distribution detection for a given method.
Figure 6: The plot compares the accuracy and MACs for various update strategies when fine-tuning.
