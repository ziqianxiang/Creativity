Figure 1: Glorot/He Initialization for a Sparse NN. All neurons in a dense NN layer (a) have the samefan-in, whereas in a sparse NN (b) the fan-in can differ for every neuron, potentially requiring samplingfrom a different distribution for every neuron. The initialization derivation/fan-out variant are explainedfurther in Appendix A.1. (c) Std. dev. of the pre-softmax output of LeNet5 with input sampled from anormal distribution, over 5 different randomly-initialized sparse NN for a range of sparsities.
Figure 2: Gradient Flow of Sparse Models during Training. Gradient flow during training averagedover multiple runs, ‘+‘ indicates training runs with our proposed sparse initialization and Small Densecorresponds to training of a dense network with same number of parameters as the sparse networks.
Figure 3: Effect of Mask Updates in Dynamic Sparse Training. Effect of mask updates on the gradientnorm. RigL Inverted chooses connections with least magnitude. We measure the gradient norm before andafter the mask updates and plot the ∆. ‘+‘ indicates proposed initialization and used in MNIST experiments.
Figure 4: Lottery Tickets Are Biased Towards the Pruning Solution, Unlike Random Initialization.
Figure 5: MDS Embeddings/L2 Distances: (a, d): 2D Multi-dimensional Scaling (MDS) embeddingof sparse NNs with the same connectivity/mask; (b, e): the average L2-distance between a pruning solutionand other derived sparse networks; (c, f): linear path between the pruning solution (α = 1.0) and LT/scratchat both initialization, and solution (end of training). Top and bottom rows are for MNIST/LeNet5 andImageNet-2012/ResNet-50 respectively.
Figure 6: Glorot/He Initialization for a Sparse NN. (Glorot et al., 2010; He et al., 2015) restrict theoutputs of all neurons to be zero-mean and of unit variance. All neurons in a dense NN layer (a) have thesame fan-in/fan-out, whereas in a sparse NN (b) the fan-in/fan-out can differ for every neuron, potentiallyrequiring sampling from a different distribution for every neuron. The fan-in matrix contains the valuesused in Eq. (1) for each neuron.
Figure 7: Gradient Flow of Sparse LeNet-5s during Training. Gradient flow during training averagedover multiple runs, ‘+‘ indicates training runs with our proposed sparse initialization. ‘+Liu‘ indicatesinitialization proposed by Liu et al., 2019. ‘He‘ suffix refers to He initilization where ‘scale=2‘ and ‘fanin‘options are used for the variance scaling initialization.
Figure 8: Effect of Mask Updates in Dynamic Sparse Training. Effect of mask updates on the gradientnorm. We measure the gradient norm before and after the mask updates and plot the ∆. ‘+‘ indicatesproposed initialization and used in MNIST experiments.
Figure 9: Sparse 300-100 MLP experiments. Gradient floW during training averaged over multiple runs,‘+‘ indicates training runs With our proposed sparse initialization.
Figure 10: Hessian spectrum before and after mask updates: (left) SET (right) RigL. Similar to Ghorbaniet al., 2019, we estimate the spectral density of Hessian using Gaussian kernels.
Figure 11: MNIST Hessian spectrum experiments.
