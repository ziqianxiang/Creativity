Figure 1: Results for different methods in SYN-S (left with 28 items), SYN-M (middle with 280items), and SYN-L (right with 1400 items). One time step represents one interaction step, where ineach interaction step the model recommends 3 items to the user and the user interacts with one ofthem. In all cases, REN models with diversity-based exploration lead to final convergence, whereasmodels without exploration get stuck at local optima.
Figure 3: Rewards over time on MovieLens-1M (left), Trivago (middle), and Netflix (right). Onetime step represents 10 recommendations to a user, one hour of data, and 100 recommendations to awusiethr foonrlyMtohveierLeleenvsa-n1cMe,(Tfirrisvta) gaon,daunndcNerettafliinxt,yre(tshpiercdt)ivteerlym., barely increases over time in SYN-L. Incontrast, the full REN-G works in both SYN-S and SYN-L. This is because without the uncertaintyterm, REN-1,2 fails to effectively choose items with uncertain embeddings to explore. REN-1,3ignores the diversity in the latent space and tends to explore items that have rarely been recommended;such exploration directly in the item space only works when the item number is small, e.g., in SYN-S.
Figure 2: Ablation study on differentterms of REN. ‘REN-1,2,3’ refers todifferent base models are able to achieve higher long-termrewards compared to their non-REN counterparts.
Figure 4: Rewards over time on MovieLens-1M for 500 users (left), 750 users (middle), and all 6,040users (right). One time step represents 10 recommendations to a user.
