Figure 1: Incoming exposures are split into Etirain and Evial. Etirain is aggregated with Ptrain and isiused for detection training. L is updated with samples from Etirain labeled as K + 1. The change inmodel accuracy is assessed using Pval to identify novelty and class identity. Finally, L is updatedwith the predicted label; samples from Etirain and Evi al are saved to their respective exemplars.
Figure 2: Graphs depicting the average accuracy decrease for repeated vs. non-repeated classes atvarious values of class-imbalance. As the ratio of class-imbalance increases, the accuracy changefor non-repeated classes remains low, with the exception at high ratios when very few samples arepresent during the model update (red). The accuracy decrease for repeated classes is amplified as theratio of class-imbalance increases (blue).
Figure 3: Visualization depicting the behavior of class feature distances as more classes are learnedby the network. The optimal threshold computed by optimizing the F-score is the midpoint of thelearned and novel class values when all 100 classes are incorporated. However, this threshold willmistakenly recognize repeated classes as new classes early on during incremental training.
Figure 4: TSNE plot depicting a model which has been trained to classify lamp vs. boy. An incomingexposure with class girl is introduced to the model. Comparing the feature distributions of this model,boy and girl have almost identical distributions; the OOD detector mistakenly assesses them to besame class (left). In contrast, our method first attempts to learn a feature space where the two classesare separable and identifies boy and girl correctly (right).
Figure 5: Visualizations comparing accuracy (left) and number of classes detected (right) for theMNIST benchmark.
Figure 6: Visualizations comparing accuracy (left) and number of classes detected (right) for theSVHN benchmark.
Figure 7: Visualizations comparing accuracy (left) and number of classes detected (right) for theCIFAR-10 benchmark.
Figure 8: Visualizations comparing accuracy (left) and number of classes detected (right) for theCIFAR-100 benchmark.
Figure 9: Visualizations comparing accuracy (left) and number of classes detected (right) for theCRIB benchmark.
Figure 10: Visualizations depicting iLAP’s performance on the MNIST benchmark with 100, 50, and20 exemplar sizes (left to right).
Figure 11: Visualizations depicting iLAP’s performance on the SVHN, CIFAR-10, and CIFAR-100benchmark with a exemplar size of 50.
Figure 12: Visualizations comparing accuracy (left) and number of classes detected (right) for theCIFAR-100 benchmark without the use of pre-trained weights.
