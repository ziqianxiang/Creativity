Figure 1: As the difference between Qvalues for action a and the baseline ac-tion reaches e (in this case e = 2), theloss increases to ∞.
Figure 2: OPenAI Safety Environments: PointGoal1, PointPush2, CarGoal2, DoggoPuSh2that LBPO is more robust than CPO and SDDPG to Q-function errors, hence making it a preferablealternative, especially when function approximation is used. Finally, We show that LBPO allowsflexible tuning of the amount of risk-aversion for the agent.
Figure 3: Each point corresponds to a particularsafety method applied to a certain safety environ-ment. The x-axis shows the fraction of constraintviolations encountered by the behavior policy dur-ing training and y-axis shows the policy perfor-mance normalized by the corresponding environ-ment’s PPO return.
Figure 5: Increasing β parameter for the barrier increases the risk aversion of the agent as Can beseen in the plots above.
Figure 4: An analysis of the robustness of safeRL algorithms CPO, SDDPG, and LBPO to finitesample Q function errors for a simple didactic en-vironment. Constraint violations in CPO and SD-DPG increase quickly as the number of on-policysamples used to estimate the Q function decreases.
