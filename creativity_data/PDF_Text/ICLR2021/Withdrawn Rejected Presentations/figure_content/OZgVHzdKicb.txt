Figure 1: BaycRL: Illustration of how Bayesian classifiers learnedwith normalized maximum likelihood can be used to provide infor-mative learning signal during learning. The human user providesexamples of successful outcomes, which are then used in combina-tion with on policy samples to iterate between training a classifierwith NML and training RL with this reward.
Figure 2: An idealized illustration of a well-shaped reward and the solutions that various classifier trainingschemes might provide. Red bars represent visited states near the initial state s0 and green pluses represent theexample success examples s+ . (a) The ideal reward would provide learning signal to encourage the policy tomove from the start states s0 to the successful states s+. (b) When training a success classifier with MLE, theclassifier may output zero (or arbitrary) probabilities when evaluated at new states. (c) Tabular CNML will givea prior probability of 0.5 on new states. (d) When using function approximation, CNML with generalizationwill provide a degree of shaping towards the goal.
Figure 3: Diagram of using meta-NML to train a classifier. Meta-NML learns an initialization that can quickly adapt to new data-points with arbitrary labels. At evaluation time, it approximates theNML probabilities (right) fairly well with a single gradient stepevaluate these questions, we evaluate our proposed algorithm BayCRL with the following setup. Fur-ther details and videos can be found at https://sites.google.com/view/baycrl/home6.1	Experimental SetupWe start off by understanding the algorithm behavior by evalu-ating it on maze navigation problems, which require avoidingseveral local optima before truly reaching the goal. Then, toevaluate our method in more complex domains, we consider threerobotic manipulation tasks that were previously covered in Singhet al. (2019a) with a Sawyer robot arm: door opening, tabletopobject pushing, and 3D object picking. As we show in our re-sults, exploration in these environments is challenging and usingnaively chosen reward shaping often does not solve the problemat hand. More details on each environment and their associatedchallenges are available in Appendix A.4.1.
Figure 4: We evaluate on two mazeenvironments and three roboticstasks: (a) the agent must navigatearound an S-shaped corridor, (b) theagent must navigate a spiral corridor,(c) the robot must push a puck to lo-cation, (d) the robot must raise a ran-domly placed tennis ball to location,(e) the robot must open the door aspecified angle.
Figure 5: BayCRL outperforms prior goal-reaching methods on all our evaluation environments. BayCRL alsoperforms better or comparably to a heuristically shaped hand-designed reward that uses Euclidean distance,demonstrating that designing a well-shaped reward is not trivial in these domains. Shading indicates a standarddeviation across 5 seeds. For details on the success metrics used, see Appendix A.4.2.
Figure 6: Ablative analysis ofBayCRL. The amortization frommeta-learning and access to goalexamples are both important com-ponents for performance.
Figure 7: Visualization of reward shapingfor 3D Pick-and-Place at various z values(heights). BayCRL learns rewards that pro-vide a smooth slope toward the goal, adaptingto the policy and guiding it to learn the task,while the MLE classifier learns a sharp andpoorly shaped decision boundary.
Figure 8: Plots of visitations and state coverageover time for BayCRL vs. VICE. BayCRL ex-plores a significantly larger portion of the statespace and is able to avoid local optima.
Figure 9: Graphical Model framework for Control as Inference. et correspond to auxiliary event variablesrepresenting successfully accomplishing the taskA.2 Detailed Description of Meta-NMLWe provide a detailed description of the meta-NML algorithm described in Section 5, and the detailsof the practical algorithm.
Figure 10: Figure illustrating the meta-training procedure for meta-NML.
Figure 11: Comparison of adapting to a query point (pictured on left with the original dataset) at test time forCNML with and without importance weighting. The version without importance weighting is more unstableboth in terms of overall batch loss and the individual query point loss, and thus takes longer to converge. Thespikes in the red lines occur when that particular batch happens to include the query point, since that pointâ€™sproposed label (y = 1) is different than those of nearby points (y = 0). The version with importance weightingdoes not suffer from this problem because it accounts for the query point in each gradient step, while keepingthe optimization objective the same.
Figure 12: Comparison of idealized (discrete) NML and meta-NML rewards on data from the Zigzag Maze Task.
Figure 13: Average absolute difference between MLE and meta-NML goal probabilities across the entire mazestate space from Figure 12 above. We see that meta-NML learns a model initialization whose parameters canchange significantly in a small number of gradient steps. Additionally, most of this change comes from the firstgradient step (indicated by the green arrow), which justifies our choice to use only a single gradient step whenevaluating meta-NML probabilities for BayCRL.
Figure 14: A comparison of the rewards given by various classifier training schemes on the 2D Zigzag maze.
Figure 15: Performance of BayCRL compared to other algorithms according to ground truth distance metrics.
Figure 16: Comparison of BayCRL, VICE, and SAC with sparse rewards on a discrete, randomized variantof the Zigzag Maze task. BayCRL is still able to solve the task on a majority of runs due to its connectionto a count-based exploration bonus, whereas ordinary classifier methods (i.e. VICE) experience significantlydegraded performance in the absence of any generalization across states.
Figure 17: Visualization of theDouble-Sided Maze environment.
Figure 18: Performance of BayCRL, VICE, and SAC with sparse rewards on a double-sided maze where somesparse reward states are not provided as goal examples. BayCRL is still able to find the sparse rewards, thusreceiving higher overall reward, whereas ordinary classifier methods (i.e. VICE) move only towards the providedexamples and thus are never able to find the additional rewards. Standard SAC with sparse rewards, also includedfor comparison, is generally unable to find the goals. The dashed gray line represents the location of the goalexamples initially provided to both BayCRL and VICE.
Figure 19: Plot of visitations for BayCRL vs. VICE on the double-sided maze task. BayCRL is initially guidedtowards the provided goals in the bottom left corner as expected, but continues to explore in both directions, thusallowing it to find the hidden sparse rewards as well. Once this happens, it focuses on the right side of the mazeinstead because those rewards are easier to reach. In contrast, VICE moves only towards the (incomplete) set ofprovided goals on the left, ignoring the right half of the maze entirely and quickly getting stuck in a local optima.
