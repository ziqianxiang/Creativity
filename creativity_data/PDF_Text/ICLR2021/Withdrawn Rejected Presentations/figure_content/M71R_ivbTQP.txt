Figure 1: Inference chain by NeuroChains for ResNet-50 (pre-trained on ImageNet) when applied to 20 testimages of “dalmatian” and ”strawberry”. Top: Thesub-network retains only 13/67 layers and 75/26560filters of the ResNet-50. The scores for selected fil-ters are represented using the colormap on the top-right. Middle: The per-layer featuremaps gener-ated by SMOE (Mundhenk et al., 2019) show a cleartrends of firstly extracting the local patterns (dots ondalmatian and strawberry) and gradually covering aglobal shapes of the classes. Bottom: Filters with thelargest scores are visualized using the method by Er-han et al. (2009). In shallower layers, L4B1C2_48and L4B1C3_524 capture a more local black spot pat-tern of the dalmatian, L3B1C3_683 captures the eyesand nose patterns; L3B1C3_818 extracts the local colorpattern of strawberry. In the last bottleneck layer,L4B3SC769 and L4B3SC1457 capture the global pat-terns of dalmatian,s black and white fur; L4B3C3,456and L4B3SC_511 captures the main shape and color
Figure 2: Left: Size and fidelity (how well the sub-networks preserve the original DNN’s outputs) of 1500sub-networks extracted by NeuroChains for ResNet-50 in different case studies (local regions). The x-axisrefers to the number of retained filters, while the y-axis is the induced decrease of probability on the original-DNN predicted class. It shows that NeuroChains can usually find very small sub-networks and meanwhilepreserve the original DNN’s outputs. Right: Faithfulness (filter score and degeneration by removing thefilter) of 783 sub-networks, each extracted by NeuroChains on 20 uniform samples randomly drawn fromtwo classes. The x-axis refers to the filter scores, while the y-axis denotes the decrease of the sub-network’sprobability on the original-DNN predicted class after removing a filter from the sub-network. It shows that thesub-networks suffer from more degeneration if removing a filter with higher score (magenta line and the shadedareas show strong linear correlation between the two). Hence, the scores faithfully reflect the importance offilters in explaining the original DNNs.
Figure 3: Averaged test fidelity (stability) of 1500sub-networks extracted by NeuroChains for VGG-19when applied to unseem images in nearby local regions.
Figure 4: Inference chain by NeuroChains for VGG-19 when applied to images of “Indigo bunting” and”sorrel”. Left: The sub-network retains only 9/16 layers and 118/4480 filters of the VGG-19. Middle: In theSMOE featuremaps, the eyes of both indigo bunting and sorrel, the feathers of indigo bunting, and the legs ofsorrel are gradually located as the key features. Right: In shallow layers, filters extract local patterns like eyes(L23」98, L23_205 and L25_506) and feathers (L25_294 and L25.413). In the last two layers, L32_30 capturesthe eyes of sorrel (big and with eyelids) while L32_352 shows the eyes and the whole head of the bird; L32.455captures the pattern of sorrel legs; L34_490 captures the contour of sorrel,s main body. It implies an inferencechain for Indigo bunting: L10_105 → L19」03 → L23-205 → L25.506 and L25_294 → L32-352 → L34_483.
Figure 5:	Robustness of original model (LEFT) vs. extracted sub-networks (RIGHT) under different attacks.
Figure 6:	Firing behaviour (featuremaps) of filters in the original network and the sub-network.
Figure 7: Inference chain by NeuroChains for VGG-19 when applied to images of “Castle” and ”Stone Wall”which confuse DNN models. Left: The sub-network retains only 10/16 layers and 138/4480 filters of the VGG-19. Middle: In the SMOE featuremaps, the wall of castle and the windows of the wall are gradually locatedas the wrong key features. Right: For castle: L14_96 → L19_145 → L23_216 and L23_220 → L28_283→ L32,351; For stone wall: L14_20 → L19-364 → L23-449 → L28-378 → L32.45. Filters like L19_364,L23.216 and L28_378 confuse the model and cause wrong predicts since they capture the patterns of bothclasses.
Figure 8: Histogram of the quotient metric in Eq. (9) computed over all the 783×20 samples (LEFT).
Figure 9: Mean±std of L2 distance in the the penultimate-layer representation space between eachsample and its K-nearest neighbours from the penultimate-layer representation space (blue) andReLU pattern space (red).
Figure 10: Case studies of an image, its 10-nearest neighbours in the output space of penultimatelayer (Top), and its 10-nearest neighbours in the raw input space in terms of Hamming distancebetween first-layer ReLU patterns (Bottom).
Figure 11: Case studies of SMOE generated heatmaps for the original network and the NeuroChainsextracted sub-network.
Figure 12: Comparison of NeuroChains and magnitude-based pruning on the capability of preserv-ing the original network’s output distribution (smaller KL divergence means better preservation)over 783×20 uniform samples.
Figure 13: Statistics of the output discrepancy between sub-networks extracted by NeuroChainsand the original network: VGG-19 (Left) and ResNet-50 (Right). The x-axis refers to the numberof filters in sub-DNNS, the y-axis is the logrithm of KL-divergence between the output distribu-tions produced by the sub-networks and the original network. The KL-divergence for most samplesare small, indicating the sub-networks preserve the original network’s output distribution for mostsamples.
Figure 14: Histogram of stability of sub-networks extracted by NeuroChains for ResNet-50. Thex-axis refers to 20K for the K-nearest neighbours of the 20 samples used to extract the sub-network)in the penultimate-layer representation space, while the y-axis is the test fidelity (averaged over allsub-networks), i.e., the accuracy of sub-networks in preserving the predicted class by the originalnetwork on the unseem K-nearest neighbours.
Figure 15: Left: Scatterplot with a jointly density estimate of the performance of sub-networks ex-tracted by NeuroChains for VGG-19. Each point corresponds to a sample. The x-axis refers to thenumber of filters in the sub-network, the y-axis measures the decrease of probability on the origi-nal network predicted class. For VGG-19, most sub-networks’ output probabilities drop very littleregardless of how many filters are retained. Right: Scatterplot with a jointly density estimate offaithfulness of sub-DNNs extracted by NeuroChains for VGG-19. The x-axis refers to the scalingscore of removed filter, the y-axis is the decrease of average probability for the original-DNN pre-dicted class compared with the complete sub-DNNs. For VGG-19, it seems the higher the score, themore the probability drops. The slope of the magenta line is the linear (Pearson) correlation, whilethe shaded area around the line represents the confidence interval.
Figure 16: Scatterplot with a jointly density estimate of faithfulness of sub-networks extractedby NeuroChains for VGG-19 (Left) and ResNet50 (Right). The x-axis refers to the scaling score(weight) of removed filter in the sub-networks, the y-axis is the logrithm of KL-divergence betweenthe outputs of the new sub-networks (after removal) and the original sub-networks (before removal).
Figure 17: Inference chain by NeuroChains for VGG-19 when applied to images of “bald eagle” and ”castle”.
Figure 18: Inference chain by NeuroChains for VGG-19 when applied to images of “kangaroo” and”banana”.
Figure 19: Inference chain by NeuroChains for Resnet-50 when applied to images of “pineapple” and ”leop-ard”. Top: The sub-network retains only 17/67 layers and 157/26560 filters. Middle: The per-layer fea-turemaps generated by SMOE. Both the body and leaves of the pineapple are highlighted. The special skintexture is enough for ResNet-50 to identify leopard. Bottom: Filters with the largest scores. By observ-ing the patterns in the activation maximization result and the highlighted regions in the featuremap, we canfind that some filters extract different local patterns appearing at different parts of pineapple. For example,L4B1C3_1010 capture the texture and the color of the main body, L3B1CL154 capture the patterns of the leafpart. It is interesting to see that L4B3C2_107 is the accurate descriptor of the main body and the leaf partsand thus provide nearly orthogonal features. For leopard, the skin marked with black spots is its most obviousfeature. L4B1CL274 extracts the basic texture and color while L4B3SC1538 and l4b3CL247 really showthe skin pattern of the leopard. It shows an inference chain for pineapple: L2B1SC342 → L3B1C3_806 andL3B1CL154 → L4B1C3_1010 → L4B3C2_107.
Figure 20: Inference chain by NeuroChains for ResNet-50 when applied to images of “chain” and ”volcano”.
