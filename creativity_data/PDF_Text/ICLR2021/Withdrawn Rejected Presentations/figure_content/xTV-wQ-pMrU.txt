Figure 1: Permutations as a self-supervised technique can handle a variety of modalities with mini-mal changes to the network architecture. Dotted layers indicate weight sharing across input patches.
Figure 2: Data processing and upstream training procedures. All permutations are sampled on thefly, during training. To obtain embeddings reusable for downstream tasks, we truncate the networkby removing the last few layers. This truncation depends on the task and is explained in Section 3.
Figure 3: Performance of our permutation-based pretraining over 3 audio tasks when varying thenumber of data points in the downstream task.
Figure 4: Performance on the NSynth datasetâ€™s downstream tasks, as a function of the number offrequency bands used for the pretext task on the audio experiment. With lower number of patches,the upstream task can become too easy to lead to good feature learning, risking overfitting.
