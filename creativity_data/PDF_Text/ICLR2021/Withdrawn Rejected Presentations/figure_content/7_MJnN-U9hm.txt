Figure 1: Comparison of randomedge pruning and random input-output path selection for differentnetwork densities. Architecture:VGG19. Dataset: CIFAR-10.
Figure 2: Two paths be-tween the same input-output units - the leftpath’s EWP is much lowerthan the right path’s EWP.
Figure 3: Comparing GraSP with a network formed by a set of paths with higher initial gradientmagnitude. The plots show classification accuracy, gradient norm, and training error (10% networkdensity). Network: VGG19. Dataset: CIFAR-100.
Figure 4: Comparing sparse networks formed by a) lower EWP paths, b) random paths, and c)higher EWP paths in terms of classification accuracy, initial gradient norm, and training error (10%network density). Network: VGG19. Dataset: CIFAR-100.
Figure 5: Accuracy compar-isons for very low networkdensities. Network: VGG19.
Figure 6: Jaccard Similarity structural comparisons between sparse MLPs on the MNIST image-transformation.
Figure 7: Equation (5)P(kSk0>0)=1-P(kSk0=0)=1-(1-P(nj∈S))N×L	(7)P(kSk0 >0) =1-(1-2 × xN(1 - xN))N×L(8)Figure 8 shows the probability of the presence of at least one stubunit in the entire network for varying width values and a depth ofthree. We can see in Figure 8 if we keep increasing x the probabilityof the presence of at least one stub unit keeps increasing.
Figure 8: Equation (8)2.	We can observe for all width values P(kSk0 > 0) is an increas-ing function as network density decreases and hits one very quickly.
Figure 9: Two paths havingextreme EWP valuesG(wk3)G(yk3)∂L∂Wk3∂L∂yk3	∂L	∂y4 I =	∂y4 χ	∂Wk3 IIIIIX i	∂L	X dyi+1I ∂yk3 I	∂yi+1	∂LdyoutX w4 X yk3X∂L l+1.∂yι+1X Wki(11)(12)When we analyze the node gradient equations (G(y13) and G(y23)), we can observe,
Figure 10: Examples of the input and the output for the transformation taskNetwork Architectures and Hyper-parameters: We use constant width MLP networks, withwidth equal to 100, 200, 300 and 400. Only 1000 examples per class were used in training for 20epochs. A learning rate of 0.001 was used with an exponential decay factor of 0.95 after every epoch,Adam optimizer, batch size of 32, and MSE as the loss. Test performance was evaluated with MSEmetric on the entire test set. ReLU in combination with batch-normalization were implemented aftereach layer, including the output layer.
Figure 11: Comparison of SynFlow pruning algorithm with SNIP and GraSP. The row indicates theplots for constant width networks with widths 100, 200, 300 and 400 from left to right.
Figure 12: Comparison of the data-agnostic SynFlow pruning algorithm to the biased random walkFigure 11 shows the comparison between the data-agnostic SynFlow algorithm to the SNIP andGraSP algorithms. Here we can clearly observe that the SynFlow algorithm performs better than theother algorithms for very high level of sparsity, whereas when the density increases the performancesstart to coincide with each other. This can be explained by the fact that in the cases of very highsparsity also the SynFlow algorithm preserves the effective flow of information.
