Figure 1: (a) The learned optimizer architecture proposed in Andrychowicz et al. (2016) consistingof a per-parameter LSTM. (b) The learned optimizer architecture proposed in Metz et al. (2019b)consisting of a per-parameter fully connected (feed-forward, FF) neural network with additionalinput features. (c) The learned optimizer architecture proposed in this work consisting of a per-tensorLSTM which exchanges information with a per-parameter feed forward neural network (FF). TheLSTMs associated with each tensor additionally share information with each other (shown in gray).
Figure 2: (a) Our proposed learned optimizer has a greater sample efficiency than existing methods.
Figure 3: Learned optimizer performance as compared to a battery of different baselines. For eachbaseline, we show a box plot over 100 different tasks. We show both a training set (solid color)which the optimizer has seen, and a test set (hatched light color) which is sampled from the samedistribution but never used for outer-training. Values to the right of the dashed line indicate thelearned optimizer outperforming the corresponding baseline. The learned optimizer outperforms anysingle baseline optimizer with a fixed hyperparameter configuration (marked global, with a singleTrial). We also outperform per-task tuning of baselines, when tuning is done only over a modestnumber of hyperparameter configurations (for instance, learning rate tuned Adam which uses 14trials). Note that the learned optimizer undergoes no per-task tuning and only makes use of one trial.
Figure 4: Implicit regularization in the learned optimizer. Panels show optimization trajectories overa 2D loss surface, f (x, y) = 2 (X - y)2, that has a continuum of solutions along the diagonal. Left:Trajectories of the Adam optimizer go to the nearest point on the diagonal. Right: Trajectories of thelearned optimizer go towards the diagonal, but also decay towards a single solution at the origin, as ifthere were an added regularization penalty on the magnitude of each parameter.
Figure 5: We show outer-generalization of the learned optimizer in a controlled setting by varyinghyperparameters of the task being trained. We vary parameters around two types of models. Toprow: A two hidden layer, 32 unit fully connected network trained on MNIST with batch size 128.
Figure 6: Learned optimizers are able to optimize ResNet models. (a) Inner-learning curves usingthe learned optimizer to train a small ResNet on CIFAR-10. We compare to learning rate tunedAdam and Momentum. Solid lines denote test performance, dashed lines are train. (b) Inner-learningcurves using the learned optimizer to train a small ResNet on 64x64 resized ImageNet. We compareto learning rate tuned Adam and Momentum. (c) Performance averaged between 9.5k and 10kinner-iterations for Adam and Momentum as a function of learning rate, on the same task as in (b).
Figure 7: The learned optimizers can be used to train themselves about as efficiently as hand designedmethods. On the x-axis we show number of weight updates done to the learned optimizer. Onthe y-axis we show outer-loss. Each point consists of inner-training 100 models, each with fiverandom initializations trained for 10k inner-iterations. We show the average validation performancepost-normalization averaged over all tasks, seeds, and inner-training steps. Each line represents adifferent randomly initialized learned optimizer. In orange we show Adam with a learning rate of3 * 10-5 the same value used to train the optimizers in this work. In green, We show Adam with alearning rate of 10-3, the learning rate that performed best in this 10k outer-iteration regime. In bluewe show our learned optimizer.
Figure 8: When using a learned optimizer to train itself, we find good performance early in training,but diverge after 10k outer-iterations. On the x-axis we show number of weight updates done tothe learned optimizer. On the y-axis we show outer-loss. Each point consists of inner-training 100models, each with five random initializations trained for 10k inner-iterations. We show the averagevalidation performance post-normalization averaged over all tasks, seeds, and inner-training steps.
Figure 9: High level / system diagram for infrastructure used. See Appendix H for more information.
