Figure 1: Schematic diagram of Gradient-DDlearning with w ∈ R2 . Rather than updatingw directly along the gradient of the MSPBE(arrow), the update rule selects wn that min-imizes the MSPBE while satisfying the con-StraintkVW - VWn-IIID ≤ μ (shaded ellipse).
Figure 2:	Performance in the random walk task depends on step size. (a), Constant step size αn =α0. (b), Tapering step size αn = α0(103 + 1)/(103 + n). In (a) and (b), state space size 10 (left)or 20 (right). GDD(c) denotes the Gradient-DD with c. The curves are averaged over 20 runs, witherror bars denoting standard deviations across runs.
Figure 3:	Performance of Gradient-DD in the random walk task. From left to right in each subfigure:the size of state space is 10 (α0 = 0.1), 20 (α0 = 0.2), 50 (α0 = 0.5). The curves are averaged over20 runs, with error bars denoting standard deviations across runs.
Figure 4:	Performance of Gradient-DD in the Boyan Chain task with 20 features. Note that the caseGradient-DD(4), i.e. c = 4, is not shown when it does not converge.
Figure 5: Bairds off-policy counterexample. From left to right: 7-state, 100-state, and 500-state.
Figure 6:	Performance of Gradient-DD in the random walk task under Case 3. Tapering step sizeαn = α0(103 + 1)/(103 + n) and κ is allowed to be dependent on n: κn = cαn. From left to rightin each subfigure: the size of state space is 10 (α0 = 0.1), 20 (α0 = 0.2), 50 (α0 = 0.5). Resultsare averaged over 20 runs, with error bars denoting standard deviations across runs.
Figure 7:	Performance of Gradient-DD in the random walk task when the initial values are set 0.
Figure 8:	Performance under various βn in the random walk task with 20 states. αn = α0(103 +1)/(103 + n) with α0 = 0.2. From left to right in each subfigure: the size of state space is βn =an∕4, βn = a.n/2, βn = α. and βn = 2αn. Results are averaged over 20 runs, with error barsdenoting standard deviations across runs.
Figure 9:	Performance of Gradient-DD in the Boyan Chain task with 20-features under Case 3.
