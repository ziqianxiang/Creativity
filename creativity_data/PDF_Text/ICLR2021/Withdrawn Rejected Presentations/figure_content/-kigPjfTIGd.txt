Figure 1: Stage 2 parametrization For upsampling stages, we adopt the DVD-GAN (Clark et al.,2019) base architecture and re-purpose it to be conditional on the low-resolution samples. Thisincludes replacing ConvGRUs with 3D convolutions, adding skip connections to xlwi and adding amatching discriminator that discriminates real or generated (xlw , xwi) pairs.
Figure 2: Randomly selected SSW-GAN 128x128/50 frame samples for Kinetics-600: We show randomsamples from our Kinetics-600 128x128/12 model unrolled to generate 50 frames. Each row shows frames fromthe same sample at different timesteps. We observe that the generations remain fairly consistent through timewhile the frame quality does not degrade.
Figure 3: Unrolling DVD-GAN We show samples from a 128x128/6 DVD-GAN baseline (our re-implementation) trained on Kinetics-600 and unrolled to generate 128x128/50 videos. While samples arevalid for the first 6 frames, they become motionless and degrade thereafter.
Figure 4: Matching discriminator We show here a random sample from our two-stage model on Kinetics with(MD) and without the matching discriminator (no MD). For each sample we show the output of the first stageand the corresponding second stage output. We observe that, while the no MD model generates plausible localsnippets at stage 2, it does not remain coherent. Our model generates a coherent sample because it is groundedin the low resolution input.
Figure 5: Random 128x128/100 BDD100K samples: We show samples from our three-stage BDD100Kmodel. Each row shows a different sample over time. Despite the two stages of local upsampling, the framequality does not degrade noticeably through time.
Figure 6: Comparison of recurrent layers We compare two variants of the same generator, one witha single ConvGRU layer per generator block and one with a separable 3D convolution per generatorblock. On the left we show the evolution of the FVD score during training, and on the right we showthe Inception Score. Both scores are normalized to the [0, 1] range where 1 is the highest scoreobtained by these models and 0 the lowest. Both models have similar behaviour and computationalcosts, but the 3D convolution processes inputs in parallel.
Figure 7: Additional samples for Kinetics 128x128/12 We show additional samples from our two-stage Kinetics 128x128/12 model unrolled to generate 128x128/50 videos. More samples can befound in the supplementary videos.
Figure 8: Additional samples for BDD 128x128/100 We show additional samples from our three-stage BDD 128x128/12 model unrolled to generate 128x128/100 videos.
Figure 9: Power Spectrum Density (PSD) plots for different time steps We show a comparisonof the PSD between the original data and our generations at different steps in the predictions. Weobserve that our generations have a similar PSD to that of the original data, even at the end of thegeneration, indicating that the generations do not blur over time significantly.
Figure 10: Pairs of samples from stage 1 and their corresponding stage 2 output We show a fewexamples from our 128x128/12 two-stage model trained on Kinetics-600 and unrolled to generate128x128/50 videos. For each example, we show the first stage low resolution generation and thecorresponding stage 2 upsampling. Stage 2 outputs refine the details of the first stage generations butretain the overall scene structure.
Figure 12: Samples from Kinetics-600 classes with low motion content20Under review as a conference paper at ICLR 2021H Joint training of multiple s tagesOur proposed method consists of multiple stages trained independently and sequentially. This allowsus to reduce the memory requirements, as we do not have to keep the intermediate activations tobackpropagate through the full model, and allows us to define stages with larger memory requirementsthan feasible if the model was trained end-to-end. However, our approach does not require each stageto be trained independently. Furthermore, with end-to-end training we could fit an equivalent fullmodel in a single optimization round, thus reducing the training time, and potentially finding a bettersolution. Additionally, we might not need to discriminate the output of intermediate stages nor usematching discriminators. In this section we report some initial results investigating the joint trainingof the different stages.
Figure 13: 64x64/12 samples from the jointly trained model on Kinetics-600 We show unrolledsamples from a variant of our model with all stages trained jointly. The output of the first stage is nota valid generation anymore, but the second stage learns to generate valid video snippets. Samples donot exhibit lot of motion.
