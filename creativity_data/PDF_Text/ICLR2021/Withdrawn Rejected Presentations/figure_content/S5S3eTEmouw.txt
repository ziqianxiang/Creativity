Figure 1: Comparing the standard meta-RL setting (left), which includes on-policy and off-policy meta-RL,with offline meta-RL (right). In standard meta-RL, new interactions are sampled from the environment duringboth meta-training and meta-testing, potentially storing experiences in a replay buffer (off-policy meta-RL). Inoffline meta-RL, a batch of data is provided for each training task Ti . This data could be the result of prior skillslearned, demonstrations, or other means of data collection. The meta-learner uses these static buffers of data formeta-training and can then learn a new test task when given a small buffer of data for that task.
Figure 2: MACAW policy ar-chitecture. Solid lines showthe forward pass; dashed linesshow gradient flow during thebackward pass during adapta-tion only; the advantage headis not used in the outer looppolicy update.
Figure 3: Comparing MACAW with (i) an offline variant of PEARL (Rakelly et al., 2019), a state-of-the-artoff-policy meta-RL method, (ii) an offline multi-task training + fine tuning method based on AWR (Peng et al.,2019), and (iii) a meta-behavior cloning baseline. Shaded regions show one standard error of the mean reward offour seeds. MACAW is the only algorithm to consistently outperform the imitation learning baseline, and alsolearns with the fewest number of training steps in every environment (note the log x axis).
Figure 4: Left: Ablating MACAW’s enriched policy update when varying the quality of the inner loopadaptation data. Solid lines correspond to MACAW, dashed lines correspond to MACAW without the auxiliarypolicy loss (equivalently, MAML+AWR with weight transforms). Both perform similarly with good qualityadaptation data (orange), but the policy adaptation step without the auxiliary loss begins to fail as adaptationdata is increasingly sub-optimal (blue and red). Bad, medium, and good data correspond to the first, middle, andlast 500 trajectories from the lifetime replay buffer of the behavior policy for each task; see Appendix D forlearning curves of the individual offline policies. Center: Ablating MACAW’s weight transform layer in thesame experimental setting as the cheetah-velocity experiment in Figure 3. Without the additional expressiveness,learning is much slower and less stable. Right: Train task sparsity split performance of MACAW, OfflinePEARL, and Offline MT+fine tune. Each curve corresponds to the performance of a method as the number oftasks available for training is varied. MACAW shows the most consistent performance when different numbersof tasks are used, performing well even when only three tasks are used for training.
Figure 5: Ablating the weight transformation in MACAW on the MuJoCo benchmark environments.
Figure 7: Average success rates of MACAW, PEARL, and MT + fine-tuning (with 20 fine-tuning steps) on the5 test tasks the Meta-World ML45 suite of continuous control tasks. Dashed line shows final PEARL averagesuccess rate after 10m training steps.
Figure 8: Learning curves for offline policies for the 4 different MuJoCo environments used in theexperimental evaluations. Each curve corresponds to a policy trained on a unique task. Various levelsof smoothing are applied for the purpose of easier visualization.
Figure 9: Learning curves and success rates for all tasks in the MetaWorld 45 benchmark. Each curvecorresponds to a policy trained on a unique task. Various levels of smoothing are applied for thepurpose of plotting.
