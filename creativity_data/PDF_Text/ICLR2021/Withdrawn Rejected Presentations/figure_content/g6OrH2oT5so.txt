Figure 1: PoisonedDoors1Under review as a conference paper at ICLR 2021For some randomly chosen 2 ≤ j ≤ N (sampled each episode), thereward behind dj is 2 but for all i ∈ {2, . . . , N} \ j the reward behind di is -2. Without knowledgeof j , the optimal policy is to always enter the correct code to open d1 obtaining an expected rewardof 1. In contrast, if the expert is given the privileged knowledge of the door dj with reward 2, it willalways choose to open this door immediately. It is easy to see that an agent without knowledge of jattempting to imitate such an expert will learn open a door among d2 , . . . , dN uniformly at randomobtaining an expected return of -2 ∙ (N - 3)/(N - 1). Training with reward-based RL after this‘warm start’ is strictly worse than starting without it: the agent needs to unlearn its policy and then, bychance, stumble into entering the correct code for door d1 , a practical impossibility when M is large.
Figure 2: Effect of partial observability in a 1-dimensional gridworld environment. (a) The twostart states and actions space for 1D-Lighthouse with N = 4. (b) A trajectory of the agent followinga hypothetical random policy. At every trajectory step we display output probabilities as per theshortest-path expert (πexp) for each state. (c/d) Using the same trajectory from (b) we highlight thepartial-observations available to the agent (shaded gray) under different filtration function f1, f2.
Figure 3: MiniGrid base tasks and model overview. (a) WC: Navigation with wall obstacles,with additional expert and environmental challenges. We test up-to 25 × 25 grids with 10 walls.
Figure 4: Evaluation following (Dodge et al., 2019). As described in Section 4.3, we plot expectedvalidation reward of best-found model (y-axis) over an increasing budget of # training runs, each withrandom hyperparameter values (x-axis). Clearly, larger E[Reward] with fewer # training runs is better.
Figure 5: “Less intelligent” teachers. Learning fi-partial policies using f j -optimal experts 2D-LH.
Figure 6: PointNav in AIHabitat.
