Figure 1: Overview. Our proposed GraPhNorm is shown along the upper branch. Each step in thisbranch can boost the performance of GNNs: subtracting graph mean has preconditioning effect;introducing a learnable shift avoids the expressiveness degradation; further scaling to unit normenjoys “scale-invariant” property (IOffe & Szegedy, 2015; Hoffer et al., 2018; Arora et al., 2018). Incomparison, BatchNorm in the lower branch suffers from heavy batch noise. Overall, GraphNormsignificantly surpasses BatchNorm in training speed (Figure 4) and enjoys good generalizationperformance (Table 1).
Figure 2: Singular value distribution of Q and QN for sampled graphs in different datasets usingGIN. More visualizations for different types of graphs can be found in Appendix D.1Normalization. Generally, given a set of values {χι, χ2,…，Xm}, a normalization operation firstshifts each Xi by the mean μ, and then scales them down by standard deviation σ: Xi → Y xi-μ + β,where Y and β are learnable parameters, μ = / Pm=I Xi and σ2 = 煮 Pm=I (Xi - μ) . Themajor difference among different existing normalization methods is which set of feature values thenormalization is applied to. For example, in computer vision, BatchNorm (Ioffe & Szegedy, 2015) isthe de facto method that normalizes the feature values in the same channel across different samples inthe batch. In NLP, LayerNorm (Ba et al., 2016) is more popularly used, which normalizes the featurevalues at each position in a sequence separately. In GNN literature, as the aggregation function issimilar to the convolutional operation, BatchNorm is usually used. Xu et al. (2019) uses BatchNormin the GIN model, where the BatchNorm is applied to all values in the same feature dimension acrossthe nodes of all graphs in the batch.
Figure 3: Batch-level statistics are noisy for GNNs. We plot the batch-level/dataset-levelmean/standard deviation of the first (layer 0) and the last (layer 3) BatchNorm layers of differ-ent model checkpoints for a five-layer GIN on PROTEINS and a ResNet18 on CIFAR10. The batchsize of all experiments are set to 128. More visualizations for different types of graphs can be foundin Appendix D.2.
Figure 4: Training performance of GIN/GCN with GraphNorm and BatchNorm on different tasks.
Figure 5: Ablation study of the parameter α. Left panel: Sampled graphs with different topologicalstructures. Right panel: training curves of GIN/GCN using GraphNorm with or without α (α = 1).
Figure 6:	Singular value distribution of Q and QN. Graph samples from PROTEINS, NCI1,MUTAG, PTC, IMDB-BINARY, COLLAB are presented.
Figure 7:	Batch-level statistics are noisy for GNNs (Examples from PTC, NCI1, MUTAG, IMDB-BINARY datasets). We plot the batch-level mean/standard deviation and dataset-level mean/standarddeviation of the first (layer 0) and the last (layer 3) BatchNorm layers in different checkpoints. GINwith 5 layers is employed.
Figure 8:	Batch-level statistics are noisy for GNNs of different depth. We plot the batch-levelmean/standard deviation and dataset-level mean/standard deviation of different BatchNorm layers(from layer 0 to layer 3) in different checkpoints. We use a five-layer GIN on PROTEINS andResNet18 on CIFAR10 for comparison.
Figure 9:	Batch-level statistics are noisy for GNNs of different batch sizes. We plot the batch-level mean/standard deviation and dataset-level mean/standard deviation of different BatchNormlayers (layer 0 and layer 3) in different checkpoints. Specifically, different batch sizes (8, 16, 32, 64)are chosed for comparison. GIN with 5 layers is employed.
Figure 10:	Training performance of GIN/GCN with GraphNorm, BatchNorm , LayerNorm andwithout normalization on PROTEINS, NCI1, PTC and MUTAG datasets.
