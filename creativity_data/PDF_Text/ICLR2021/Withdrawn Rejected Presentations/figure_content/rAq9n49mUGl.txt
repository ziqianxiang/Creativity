Figure 1: Example toy model signals and corresponding corrections learned via the hybrid trainingprocedure in algorithm 1. Top-left: noisy true signal Y overlaid with noiseless uncorrected X andcorrected |X + gδ (X) | signals, with X given by Fβ=4 in equation (8); difference between true Fβ=2and learned |X + gδ (X)| is inset. Bottom-left: noisy true signal Y overlaid with noisy correctedsignal (Hybrid); learned correction gδ(X) and noise level exp(g(X)) is inset. Top-right & bottom-right: distributions of learned gδ and exp(g) over the validation X data.
Figure 2: Left: example noisy MRI signal Y compared with noisy corrected signal (Hybrid), trainedvia the hybrid training algorithm 1; distributions of learned gδ and exp(g ) over the validation Xdata is inset, as well as the learned correction gδ (X) and signal difference for the data shown. Top-right: comparison of true Y vs. learned signal amplitude distributions for the uncorrected F (t; θ)from equation (9), GAN, MMD GAN, and hybrid-trained GAN signal models over the validationX and Y data, as well as the distribution differences compared to Y . Bottom-right: comparison oftrue vs. learned T2 distributions - analagous to regularized inverse Laplace transforms - and T2 dis-tribution differences compared to Y for the same four signal models. The hybrid training algorithmproduces signal amplitude and T2 distributions which are most similar to the true distributions.
Figure 3: Example parameter maps computed using CVAE (top row) and NNLS (bottom row).
Figure 4: Example learned stochastic corrections for the toy problem using traditional, MMD, andhybrid GANs. The top row of plots for each model shows deterministic corrections, and the bottomrow shows the stochastic corrections.
Figure 5: Example learned stochastic corrections for the MRI problem using traditional, MMD, andhybrid GANs. The top row of plots for each model shows deterministic corrections, and the bottomrow shows the stochastic corrections.
