Figure 1: (a) Factor graph for the CNF with measure φ = (vι ∨v2 ∨v4)∧(-v1 ∨v2 ∨-v3)∧(v3 ∨v4),where solid lines are the positive incidences of vi in uj , and dashed lines are the negative incidencesof vi in uj ; (b) the decomposition of the factor graph according to the positive and negative re-lations, which are used for crossing-attention; (c) meta-paths are used for the self-attention amongliterals or clauses.
Figure 2: Our Heterogeneous Graph Transformer architecture consists of a set of encoders anddecoders connected sequentially, as in (b). Its encoder and decoder architectures are shown in (a)and (c), respectively.
Figure 4: Our model's test speeds compared tothose of PDP, with the speedup of WalkSAT asthe baseline metric. The x-axis indicates the sixdata distribution, upon which We test the mod-els. The y-axis measures the speedup of themodels, w.r.t. WaIkSAT's performance.
Figure 3: The learning curve of HGT (GT) and thatof RLSAT (RL). The x-axis measures the numberof epochs trained. The y-axis measures the valida-tion score in percentage. Both models are trainedon {KC: color3(10, 0.5), KV: cover3(7, 0.5), KQ:clique3(10, 0.1)}.
