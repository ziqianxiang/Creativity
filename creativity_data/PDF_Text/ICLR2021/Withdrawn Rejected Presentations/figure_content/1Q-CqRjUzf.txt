Figure 1: ImageNet ablation study: We plot the effect of the entropy regularizer (top10), for varyingα, and Co-distillSKL for varying β, on the prediction entropy, accuracy and churn. These plots showsthe complementary nature of these methods in reducing churn. While entropy regularizer reduceschurn by reducing the prediction entropy, Co-distillSKL reduces churn by improving the agreementbetween two models, hence increasing the entropy.
Figure 2: ImageNet: Left - effect of the entropy regularizer and our proposed variant of co-distillation,Co-distillSKL, on the prediction confidence. Right - comparison with Co-distillCE (Anil et al., 2018).
Figure 3: CIFAR-100 ablation study: Similar to Figure 1, we plot the effect of the SKL regularizer(cf. (7)), for varying α, and our proposed variant of co-distillation (Co-distillSKL) for varying β, onthe prediction entropy, accuracy, and churn. These plots shows the complementary nature of thesemethods in reducing churn. While the regularizer reduces churn, by reducing the prediction entropy,Co-distillSKL reduces churn by improving the agreement between two models, hence increasing theentropy.
Figure 4: Visualisation of entropy regularised loss (eq. (6)) in binary case. On the left panel is theregularised log-loss (1 — α)∙— logP + α ∙ Hbin(P) which accepts a probability in [0,1] as input.
Figure 5: Visualisation of temperature scaling on binary logistic lau. We plot ´ log σpf{τq forvarying temperature parameter τ .
