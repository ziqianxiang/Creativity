Figure 1: An illustration of the deep probabilistic pruning framework (for layer i) for dynamicmasking of weights W with a stochastic mask MÎ¦(i) . This mask is generated by sampling from acategorical distribution with probabilities that are jointly learnt with the weights W. The * symbolindicates element-wise multiplication.
Figure 2: Visualization of the three scenarios for which we add experiments with DPP. All adoptedvalues are illustrative. a. Fully-connected weight pruning (example 3 inputs 4 outputs, K = 1), b.
Figure 3: PruningDiversity metrics for both the first layer, I (m(1) , d), and the second I (m(2) , d), ofLeNet300-100 on MNIST. The AveragePruneEntropy and EntropyAverageMas are also shownfor both layers.
