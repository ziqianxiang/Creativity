Figure 1: GraphLog setup: A set of rules(grounded in propositional logic) is par-titioned into overlapping subsets. It isused to define unique worlds. Within eachworld Wk, several knowledge graphs gik(governed by rule set ofWk) are generated.
Figure 2: Overview of training process: (a):Sample multiple graphs from GW. (b): Con-vert the relational graph into extended graphGW. Note that edges of different color (denot-ing different types of relations) are replacedby a node of same type in GW. (c): Learn rep-resentations of the relations (r) using fr (ex-tended graph as the input). In case of Parammodels, the relation representations are pa-rameterized via an embedding layer and theextended graph is not created. (d, e): Thecomposition function takes as input the querygi, u, v and the relational representation r. (f):The composition function predicts the relationbetween the nodes u and v .
Figure 3: We categorize the datasets in terms oftheir relative difficulty (see Appendix). We ob-serve that the models using E-GAT as the com-position function consistently work well.
Figure 4: We run multitask experiments over anincreasing number of worlds to stress the capac-ity of the models. We report the average of testset performance across the worlds that the modelhas trained on so far. All the models reach theiroptimal performance at 20 worlds, beyond whichtheir performance starts to degrade.
Figure 5: We evaluate the effect of changingthe similarity between the training and the eval-uation datasets. The colors of the bars depictshow similar the two distributions are while they-axis shows the mean accuracy of the modelon the test split of the evaluation world. We re-port both the zero-shot adaptation performanceand performance after convergence.
Figure 6: Evaluation of models in continual learning setup. In Figure 6(a), the blue curve shows theaccuracy on the current world and the orange curve shows the mean of accuracy on all previouslyseen worlds. Training on new worlds, degrades modelâ€™s performance on the past worlds (catastrophicforgetting). In Figure 6(b), sharing representation function reduces catastrophic forgetting.
Figure 7: Figure 7(a) represents graphs drawn at random from different worlds available in GraPhLog. Nodes in red denote the query nodes, and the nodes in blue denote the shortest path among the querynodes. Figure 7(b) represents the degree distribution of all graphs from all worlds in GraphLog .
Figure 8: We perform fine-grained analysis of few shot adaptation capabilities in Multitask setting.
Figure 9: We evaluate the effect of k-shot adaptation on held out datasets when pre-trained on easy,medium and hard training datasets, among the different model architectures. Here, k ranges from 0to 40.
Figure 10: Curriculum Learning strategy in Continual Learning setup of GraphLog. Figure 10(a)presents the current task accuracy (blue) and mean of all previous task accuracy (orange). Figure10(b) presents the mean of previous task accuracy when either the composition function or therepresentation function is shared for all worlds.
