Figure 1: The proposed scalable Transformers (ST) can flexibly change different layersâ€™ featurewidth to meet different computational constraints. (Middle) The widest Transformer of a scal-able Transformers. (Top) A narrower sub-Transformer with fixed input-output feature dimensionand varying attention feature dimensions for different layers. The weights of sub-Transformersare directly cropped from the widest Transformer without re-training. (Bottom) A narrower sub-Transformer with the same input-output and attention feature dimension for all layers.
