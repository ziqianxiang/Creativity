Figure 2: Examples of the four environments used in our experiements: ColorSwitch, Pushblock,Crafting and Cartpole.
Figure 4: Hypothesis prediction accuracy on both triplet and non-triplet hypotheses (1-1 ratio) for theColorSwitch, Pushblock, Crafting and Cartpole environments, using RHyp reward for training.
Figure 5: Hypothesis accuracies with intrinsic pretraining versus triplet pretraining (purple).
Figure 6: (Left) Results on training an RL using RHyp with oracle predictor on the Crafting environ-ment. (Right) Results training just hypothesis prediction on oracle policy.
Figure 7: Network architecture for our policy network (left) and prediction network (right)D. 1 Network AblationIn Figure 8 we see the results of our network architecture ablation. As we can see, our new policyarchitecture described previosuly clearly outperforms a standard MLP policy network on the language-condition pretraining task. We also see that the transformer architecture outperforms the LSTM andMLP model on the final task when we hold the policy network constant.
Figure 8: (left) policy network ablation (right) prediction network ablation.
Figure 9: Hypothesis prediction accuracy on both triplet and non-triplet hypotheses for the ColorSwitch, Pushblock, Crafting and Cartpole environments, using RHyp reward for training.
Figure 10: Pretraining Reward for ColorSwitch for intrinsic motivation. Showing mean and variancebands on 25 random seeds.
Figure 11: Pretraining Reward for Pushblock for intrinsic motivation. Showing mean and variancebands on 25 random seeds.
Figure 12: Pretraining Reward for Crafting for intrinsic motivation. Showing mean and variancebands on 25 random seeds.
