Figure 1: High level overview of the proposed benchmark. Left block: from the left, the learner acquirestask-specific information from each set, one-by-one, without being allowed to view previous sets (memoryconstraint). The learner can store knowledge in a shared memory bank and use it in a classification model. Onthe rightmost side, future tasks are inaccessible to the learner. On the bottom, the same process is repeated onthe second support set. Note that the first support set is now inaccessible. Right block: once the learner hasviewed all support sets, it is given an evaluation set (target set) containing new examples of classes contained inthe support-sets, and tasked with producing predictions for those samples. The evaluation procedure has accessto the target set labels, and can establish a generalization measure for the model.
Figure 2: Visual representation of the four continual few-shot task types. Each row corresponds to a task withNumber of Support Sets, NSS=4, and a defined Class-Change Cnterval (CCI). Given a sequence of support sets,Sn , the aim is to correctly classify samples in the target set, T. Colored frames correspond to the associatedsupport set labels.
Figure 3: Accuracy (percentage) on the Omniglot and SlimageNet test set for different values of Number ofSupport Sets Per Task (NSS), Class Change Interval (CCI) equal 1, and with/without overwrite.
Figure 4: ATM (Across-Task Memory) and MAC (Multiply-Accumulate Computations) costs for a variety ofNSS (Number of Support Sets Per Task). ProtoNets are the superior method across the board. In terms ofATM it is worth noting that methods such as MAML++ H and SCA tend to become incrementally cheaper thanMAML++ L as the number of support sets increases. Whereas in terms of MACs MAML++ H and SCA arethe most expensive by an order of magnitude or more compared to MAML++ L and ProtoNets.
