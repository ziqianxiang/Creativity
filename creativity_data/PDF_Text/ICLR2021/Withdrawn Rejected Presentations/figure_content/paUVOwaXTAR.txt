Figure 1: Left: conventional hard weight sharing approach. The model consists of a shared featureextractor followed by the task-specific task solvers (e.g. logits layer in case of classification). Theonly place where the model can specialize to each task is in its solver, which might not be enoughfor sufficiently distinct tasks. Right: Compositional model. We use a shared set of trainable mod-ules (we call them templates) available for all tasks; each layer’s weights are generated as a linearcombination of the templates weights, and the linear combination weights (mixture weights ξ) aretask-specific. In this case, the templates can not only be shared across different tasks, they also canbe reused multiple times within the same task. (best viewed in color).
Figure 2: Self-organization of modules for multi-task and domain-adaption. In (a) co-training 5natural image datasets with 32 modules (CIFAR- 1 00 and FOOD101 not shown), first 5 layers werereused across all learned models (shown with dashed rectangles). In (b) co-training 9 modules on 9tasks: 3 handwritten digit datasets EMNIST, MNIST and KMNIST with the original images andtwo image augmentations (boundary extraction and blurring). The model learned to use differentinput transformation for each image augmentation, while sharing the top layers for each task. Onthe other hand, for each fixed augmentation, the model learned to share first few layers across alldifferent tasks.
