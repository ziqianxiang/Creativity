Figure 1: End-to-End Architecture of R-MONet. (a) A spatial attention network similar to S4Netreceives the input image to generate foreground objects’ bounding boxes {tk} and segmentationintermediate output {αk}. The scope {sk} represents the portion of the undecomposed scene. Withthe initial scope s0 = 1, the stick-breaking process will transform {αk} into the segmentationmasks {mk } (K-1 masks for the foreground objects and 1 mask for the background) and keepPkK=1 mk = 1. VAE encoder takes the image and {mk} as the inputs to compute representationlatents over segmentation and input image {zk }. (b) VAE decoder reconstructs different objects{Xk} and their masks {mk} with {zk} for representation learning. (C) To train the RPN in theinference network, {mk}k∈[1,K-1] will be passed into the Multi-Otsu thresholding method to gen-erate the pseudo bounding boxes {fi}. The image, {mk} and reconstructed objects {Xk} will formVAE ELBO loss. The KL divergence between {m∙k} and reconstructed segmentation masks {fnk}forces VAE to model the segmentation masks distribution in spatial attention module. The details ofthe loss is in Section 3.5.
Figure 2:	Qualitative comparison of scene reconstruction, bbox and foreground segmentation masksof MONet, MONet(ResNet18 + FPN), MONet(ResNet18 + FPN + UNet), IODINE, SPACE, R-MONet(ROI Align), R-MONet(Lite), R-MONet(UNet) on CLEVR dataset. More results can beseen in Appendix AMONetMONet(ResNetl8+FPN)MONet(ResNetl8+FPN+UNet)IODINESPACER-MONet(ROI Align)R-MONet(Lite)R-MONet(UNet)Figure 3:	Qualitative comparison of scene reconstruction, bbox and foreground segmentation masksof MONet, MONet(ResNet18 + FPN), MONet(ResNet18 + FPN + UNet), IODINE, SPACE, R-MONet(ROI Align), R-MONet(Lite), R-MONet(UNet) on Multi-dSprites dataset. More results can
Figure 3:	Qualitative comparison of scene reconstruction, bbox and foreground segmentation masksof MONet, MONet(ResNet18 + FPN), MONet(ResNet18 + FPN + UNet), IODINE, SPACE, R-MONet(ROI Align), R-MONet(Lite), R-MONet(UNet) on Multi-dSprites dataset. More results canbe seen in Appendix A9Under review as a conference paper at ICLR 2021ReferencesChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, MattBotvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-tation. arXiv preprint arXiv:1901.11390, 2019.
Figure 4: Additional qualitative comparison oftation masks on CLEVR dataset.
Figure 5: Additional qualitative comparison oftation masks on CLEVR dataset.
Figure 6:	Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on CLEVR dataset.
Figure 7:	Additional qualitative comparison oftation masks on CLEVR dataset.
Figure 8:	Additional qualitative comparison oftation masks on CLEVR dataset.
Figure 9: Additional qualitative comparison oftation masks on CLEVR dataset.
Figure 10:	Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on CLEVR dataset.
Figure 11:	Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on Multi-dSprites dataset.
Figure 12:	Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on Multi-dSprites dataset.
Figure 13:	Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on Multi-dSprites dataset.
Figure 14:	Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on Multi-dSprites dataset.
Figure 15: Additional qualitative comparison oftation masks on Multi-dSprites dataset.
Figure 16: Additional qualitative comparison of scene reconstruction, bbox and foreground segmen-tation masks on Multi-dSprites dataset.
Figure 17: Qualitative comparison of R-MONet(UNet)’s masked object reconstruction, bbox andforeground segmentation masks at epoch 480, epoch 864, epoch 7008, epoch 19296 on CLEVRdataset. In the initial stage like epoch 480, ROIs are random across the image. The spatial attentionnetwork tends to learn segmentation inside the ROIs. After spatial attention network learns the roughsegmentation masks (epoch 864), the pseudo ground truth bboxs generated from rough segmentationmask can guide object detection branch to find more accurate ROIs. In this stage, if segmentationmasks contain more than one object, pseudo ground truth bbox will separate them with Multi-Otsualgorithm. At the middle stage (epoch 7008), the evolving segmentation masks help VAE to learnobject appearance representations. In the last stage (epoch 19296), segmentation masks, bboxs andobject appearance representations from VAE keep evolving at the same time.
Figure 18: Qualitative comparison of MONet’s masked object reconstruction, bbox and foregroundsegmentation masks at epoch 47040 on CLEVR dataset. During training, since the loss of MONetdoes not prevent multiple objects showing up in the same mask, MONet tends to distribute theobjects with the same reconstructed color into the same mask. This problem may slow down MONettraining.
Figure 19: Quantitative training convergence speed comparison between MONet, R-MONet(Lite)and R-MONet(UNet). We compared the ARI performance growth speed in terms of batches andtraining time. On CLEVR dataset and Multi-dSprites dataset, R-MONet converges much faster thanMONet. This may be because MONet uses the entire scene as the ROI and it is harder for attentionnetwork to obtain a proper segmentation for each object compared with regional ROI in R-MONet.
Figure 20: Qualitative comparison of R-MONet(UNet)’s (without the self-supervised loss and ob-ject detection branch) masked object reconstruction, bbox and foreground segmentation masks onCLEVR dataset. This model performs segmentation on the entire image and generates object masksin parallel. The ARI is nearly zero since all object segmentations are in one mask. The spatial atten-tion module is good at segmenting objects from the background but unable to separate objects fromeach other. This proves that the loss of MONet does not prevent multiple objects from showing upin the same mask and the effectiveness of proposed self-supervised loss.
Figure 21: Qualitative comparison of R-MONet(UNet) masked object reconstruction, bbox andforeground segmentation masks on MS-COCO 2017 dataset (Lin et al. (2014)). We use the pretrained ResNet18+FPN on ImageNet (Deng et al. (2009)) provided in torchvision(Marcel & Rodriguez (2010)). Unfortunately, the proposed model can not achieve the same perfor-mance as the supervised models. The VAE used can not generate complex object in the scene. Itfocuses more on the region which has high contrast with its surrounding areas.
Figure 22: The feature map flow of the improved segmentation head. (a) The input image ispassed into ResNet18 and conv{1-5} layers are extracted for the later use. The conv{2-5} layersare passed into FPN for the bounding boxes generation in RPN. After the FPN, the conv5 layer ispassed into a MLP to compress its feature space. During the region feature selection, only conv{1-3} layers are transformed with ROI Masking method. The output of different layers are combinedtogether like UNet to preserve low level features. (b) The original ROI Masking method enlargesthe selected region by applying the ternary masks to the feature maps and it is only applied to thefeatures from the conv3 layer. The transformed feature maps keep the conv3’s dimensions. (c)The ROI Align method only extracts the feature maps in the selected area and transforms them intothe fixed dimensions. The segmentation head is similar to the one used in the Mask R-CNN whenadapting ROI Masking and ROI Align.
