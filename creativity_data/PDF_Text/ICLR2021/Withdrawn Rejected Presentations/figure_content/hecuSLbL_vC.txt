Figure 1: The variation of the train accuracy onthe memorised samples from the first task as afunction of overparameterization (higher is bet-ter). The forgetting decreases with overparame-terization, as stated in Theorem 2.
Figure 2: The variation of the train accuracyon the memorised samples from each task, afterthe model was trained on all tasks in sequence(higher is better). We vary the hidden size as aproxy for overparameterization.
Figure 3: The variation of the train accuracy on the memorised samples from the each task, after themodel was trained on all tasks in sequence (higher is better). We vary the hidden size as a proxy foroverparameterization.
Figure 4: The variation of the train accuracy on the memorised samples from the first task, after themodel was trained on all tasks in sequence (higher is better). We vary the memory size per task from100 to 300.
Figure 5: Test accuracy on the 10 first tasks of Rotated MNIST, for SGD, OGD, OGD+ and A-GEM.
Figure 6: Test accuracy through tasks for different Continual Learning methods on the MNIST andCIFAR100 and CUB200 benchmarks. The y-axis is truncated for clarity. We report the mean andstandard deviation over 5 independent runs. The test error is measured at the end of each task.
