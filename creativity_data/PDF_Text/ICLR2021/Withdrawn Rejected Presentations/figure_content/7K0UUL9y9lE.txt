Figure 1: We compare attention weights using exp(τ (x - 1))with the collision probability of concatenating τ hyperplane hashes(Charikar, 2002) (1 - arccos(x)∕π)τ for T = 8. We plotexp(τ (x - 1)) so that the range is between 0 and 1 but withoutchanging the actual attention weights in softmax. We also plot thederivative of exponential function and of collision probability, aswell as a lower bound we will use later during backpropagation.
Figure 2: Overview of YOSO-Attention. The hash table stores the sum of values associated with hashed keys.
Figure 3: (a) The left two plots are results on MLM and SOP for 512 sequence length. We report MLMvalidation perplexity and SOP validation loss for each 2K training steps. (b) The middle two plots are resultsfor MLM and SOP when using different number of hashes on validation. Since the runtime of YOSO-Attentionis linear with respect to the number of hashes, these two plot directly reflect the equivalent relation betweenperformance vs inference time. (c) The right two plots are results on on MLM and SOP for 4096 sequencelength. YOSO-x means the model is pretrained with YOSO-Attention using x hashes with E being expectation.
Figure 4: Attention matrices generated by self-attention and YOSO-Attention with different hash settingsusing the same input. Notice that the patterns are preserved well.
Figure 5: (a) The relative error in the left plot is defined asE[ kE[N-YoSo(Q,K,V)]-N-YoSo(Q,k,v)k∞ ] The relative error is esE[	∣∣E[N-YOSO(Q,K,V)]∣∣∞	]. The relatlve error Is es-timated by computing E[N-YoSo(Q, K, V )] based on collisionprobability, then estimating N-YoSo(Q, K, V ) multiple times,finally computing the mean of relative error of each estimate asan estimation of the outer expectation. (b) The runtime per tokenis estimated by estimating N-YoSo(Q, K, V ) multiple times andmeasuring the total time elapsed and then dividing the total time bynumber of iterations and sequence length to get runtime per token.
