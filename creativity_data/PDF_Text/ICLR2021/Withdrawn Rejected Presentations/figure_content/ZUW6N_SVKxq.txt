Figure 1: Generative models for two layer (L = 2) deep GPs. (Top) Generative model for adeep GP, with a kernel that depends on the Gram matrix, and with Gaussian-distributed features.
Figure 2: Visualisations of a single prior sample of the kernels and Gram matrices as they passthrough the network. We use 1D, equally spaced inputs with a squared exponential kernel. As wetransition K(G'-ι) → g`, We add “noise" by sampling from a Wishart (top) or an inverse Wishart(bottom). As we transition from g` to K(G'), we deterministically transform the Gram matrixusing a squared-exponential kernel.
Figure 3:	A series of generative models for a standard, finite BNN. Top. The standard model, withfeatures, f`, and weights W` (Eq. 41). Middle. Integrating out the weights, the distribution overfeatures becomes Gaussian (Eq. 44), and We explicitly introduce the kernel, k`, as a latent variable.
Figure 4:	A series of generative models for an infinite network with bottlenecks. First row. Thestandard model. Second row. Integrating out the weights. Third row. Integrating out the features,the Gram matrices are Wishart-distributed, and the kernels are deterministic. Last row. Eliminatingall deterministic random variables, we get a model equivalent to that for DGPs (Fig. 1 bottom).
Figure 5: Eigenvalue histograms for a single sample from the labelled distribution, with N = 2000.
Figure 6: Samples from a one-layer (top) and a two-layer (bottom) deep IW process prior (Eq. 16).
Figure 7: The additional flexibility in a one-layer deep IW process can be used to capture mismatchin the kernel. We plot five posterior function samples from trained IW processes in the first row,and samples from trained GPs below. We generate different sets of training data from a GP withdifferent kernel bandwidths (0.5, 1, 2, 5, 10) across columns, while we keep the kernel bandwidth inall models being 1.
