Figure 1: Working principle of CHEF. An ensemble of Hebbian learners is applied to the upperlayers of a trained neural network. Distilling information from different layers of abstraction is calledrepresentation fusion. Each Hebbian learner is iteratively optimized and the results are combined.
Figure 2: 5-shot and 50-shot top-1 accuracies (along with 95% confidence intervals) of differentresidual blocks and the output layer of an Imagenet-Pretrained ResNet-18 and the ensemble result(Orange, “ens”)on the four different datasets of the cross-domain few-shot learning benchmark. Forcomparison, also the ResNet-10 ensemble results (green) are included.
Figure 3: 20-shot top-1 accuracies (along with 95% confidence intervals) of different residual blocksand the output layer of an Imagenet-Pretrained ReSNet-18 and the ensemble result (Orange, “ens”)onthe four different datasets of the cross-domain few-shot learning benchmark. For comparison, alsothe ResNet-10 ensemble results (green) are included.
Figure 4: Ablation study of the ConV-4 architecture on the miniImagenet and tieredImagenet datasetsfor 1-shot and 5-shot. The plots show the individual performances of Hebbian learners acting onsingle layers and their ensemble performance along with 95% confidence intervals. The labels on thex-axis indicate how far the respective layer is from the output layer.
Figure 5: Ablation study of the ResNet-12 architecture on the miniImagenet and tieredImagenetdatasets for 1-shot and 5-shot. The plots show the individual performances of Hebbian learners actingon single layers and their ensemble performance along with 95% confidence intervals. The labels onthe x-axis indicate how far the respective layer is from the output layer.
