Figure 1: CMNIST with varying label noise θy. Under high label noise (θy > .2), where thespurious feature color correlates to label more than shape on the train data, IRM(eEIIL) matches orexceeds the performance of IRM(eHC) on the test set without relying on hand-crafted environments.
Figure 2: We examine subgroup sufficiency—whether calibration curves match across demographicsubgroups—on the ConfoundedAdult dataset. Whereas ARL is not subgroup-sufficient (a), EIIL in-fers worst-case environments and regularizes their calibration to be similar (b), ultimately improvingsubgroup sufficiency (c). This helps EIIL generalize better to a shifted test set (e) compared withARL (d). Note that neither method uses sensitive group information during learning.
Figure 3: MSE of the causal feature v and non-causal feature z. IRM(eEIIL)applied to the ERMsolution (Black) out-performs IRM based on the hand-crafted environment (Green vs. Blue). Toexamine the inductive bias of the reference model Φ, we hard code a model Φα-SPURIOUS where αcontrols the degree of spurious feature representation in the reference classifier; IRM(eEIIL) out-performs IRM(eHC) when the reference Φ focuses on the spurious feature, e.g. with Φ as ERM orα-SPURIOUS for high α.
