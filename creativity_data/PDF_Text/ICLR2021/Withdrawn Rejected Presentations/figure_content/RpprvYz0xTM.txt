Figure 1: Overview of our end-to-end framework. For multi-modal videos, our framework consistsof two feature encoders f and fa, two MLPS h and ha for contrastive learning, one fusion layer η,and two linear heads φl and φu for classification and clustering. The training signal for φu is obtainedby WTA on-the-fly. For single-modal images, the audio encoder f。is omitted and η turns to be anidentity mapping function.
Figure 2: t-SNE visualization of features from unlabelled data on CIFAR-10, using the modelpretrained on the labelled data with fully supervised training.
Figure 3: t-SNE visualization of features from unlabelled data on CIFAR-10, using our modelafter end-to-end training on both labelled and unlabelled data.
Figure 4: t-SNE visualization of features from unlabelled data on Kinetics-400, using the modelpretrained on the labelled data with fully supervised training.
Figure 5: t-SNE visualization of features from unlabelled data on Kinetics-400, using our modelafter end-to-end training on both labelled and unlabelled data.
