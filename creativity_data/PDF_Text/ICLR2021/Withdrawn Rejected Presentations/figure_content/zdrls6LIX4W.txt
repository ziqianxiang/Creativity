Figure 1: (a) A Markov chain of joint policies representing the inherent non-stationarity of MARL.
Figure 2: Adaptation performance during meta-testing in mixed incentive ((a), (b)), competitive(c), and cooperative (d) environments. The results show that Meta-MAPG can successfully adaptto a new and learning peer agent throughout the Markov chain. Mean and 95% confidence intervalcomputed for 10 random seeds for ((a), (b), (c)) and 5 random seeds for (d) are shown in figures.
Figure 3: (a) Adaptation performance with a varying number of trajectories. Meta-MAPG achievesthe best AUC in all cases and its performance generally improves with a larger K . Mean and 95%confidence interval are computed for 10 seeds. (b) and (c) Visualization of j’s initial policy for indistribution and out of distribution meta-testing, respectively, where the out of distribution split has asmaller overlap between the policies used for meta-training/validation and those used for meta-testing.
Figure 4: (a) Adaptation performance with opponent modeling (OM). Meta-MAPG with OM usesinferred policy parameters for peer agents, computing the peer learning gradient in a decentralizedmanner. (b) Adaptation performance with a varying number of agents in RPS. Meta-MAPG achievesthe best AUC in all cases. (c) Ablation study for Meta-MAPG. Meta-MAPG achieves significantlybetter performance than ablated baselines with no own learning gradient and no peer learning gradient.
Figure 5: 2-Agent HalfChee-tah domain, where two agentsare coupled within the robotand control the robot together.
Figure 6: IPD meta-learning setup. An agent j’s policy is initialized randomly from the initialpersona population p(φ0-i) that includes various cooperating and defecting personas. The agent jthen updates its policy throughout the Markov chain, requiring an agent i to adapt with respect to thelearning of j .
Figure 7: Visualization of a teammate j ’s ini-tial expertise in the 2-Agent HalfCheetah do-main, where the meta-test distribution has athe meta-train/val and from 475 and 500 iterationsas the meta-test distribution (see Figure 7). We con-struct the distribution with the gap to ensure that themeta-testing distribution has a sufficient difference tothe meta-train/val so that we can test the generaliza- sufficient difference to meta-train/val.
Figure 8: Learning paths on the zero-sumgame. The standard approach with the station-ary assumption diverges, resulting in worseperformance for both agents. In contrast, anapproach that considers the learning processof the other agents, such as LOLA (Foersteret al., 2018a), converges to the equilibrium.
Figure 9:	Action probability dynamics with Meta-PG in IPD with a cooperating persona peerLOLA-DiCE Agent Markov Chain Dynamics.8S.420.0.0.0.
Figure 10:	Action probability dynamics with LOLA-DiCE in IPD with a cooperating persona peerREINFORCE Agent Markov Chain Dynamics.8.64.20.0.0.0.
Figure 11: Action probability dynamics with REINFORCE in IPD with a cooperating persona peerCooperate8 6 4 20.0.0.0.
Figure 12: Action probability dynamics with Meta-MAPG in IPD with a cooperating persona peer19Under review as a conference paper at ICLR 2021G.2 RPSMeta-PG Agent Markov Chain DynamicsPeer Agent Markov Chain DynamicsFigure 13: Action Probability Dynamics with Meta-PG in RPS with a scissors persona opponentɪ Peer Agent Markov Chain Dynamicsɪ LOLA-DiCE Agent Markov Chain Dynamics8 6 4 20.0.0.0.
Figure 13: Action Probability Dynamics with Meta-PG in RPS with a scissors persona opponentɪ Peer Agent Markov Chain Dynamicsɪ LOLA-DiCE Agent Markov Chain Dynamics8 6 4 20.0.0.0.
Figure 14: Action Probability Dynamics with LOLA-DiCE in RPS with a scissors persona opponentIElNFORCE Agent MarkOV ChaM DynarnicSɪ Peer Agent Markov Chain Dynamics8 6 4 20.SS0.
Figure 15: Action Probability Dynamics with REINFORCE in RPS with a scissors persona opponentMeta-MAPG Agent Markov Chain DynamicsPeer Agent Markov Chain DynamicsFigure 16: Action Probability Dynamics with Meta-MAPG in RPS with a scissors persona opponent20Under review as a conference paper at ICLR 2021H Hyperparameter DetailsWe report our hyperparameter values that we used for each of the methods in our experiments:H. 1 Meta-MAPG and Meta-PGHyperparameter	ValueTrajectory batch size K	4,8, 16, 32, 64Number of parallel threads	5Actor learning rate (inner)	1.0, 0.1Actor learning rate (outer)	1e-4Critic learning rate (outer)	1.5e-4Episode horizon H	150Max chain length L	7GAE λ	0.95Discount factor Y	0.96Table 3: IPD
Figure 16: Action Probability Dynamics with Meta-MAPG in RPS with a scissors persona opponent20Under review as a conference paper at ICLR 2021H Hyperparameter DetailsWe report our hyperparameter values that we used for each of the methods in our experiments:H. 1 Meta-MAPG and Meta-PGHyperparameter	ValueTrajectory batch size K	4,8, 16, 32, 64Number of parallel threads	5Actor learning rate (inner)	1.0, 0.1Actor learning rate (outer)	1e-4Critic learning rate (outer)	1.5e-4Episode horizon H	150Max chain length L	7GAE λ	0.95Discount factor Y	0.96Table 3: IPDHyperparameter	ValueTrajectory batch size K	^64Number of parallel threads	5
