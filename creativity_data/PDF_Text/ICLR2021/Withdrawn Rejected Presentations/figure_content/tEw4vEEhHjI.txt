Figure 1: Toy classification with a BNN and our method. Shade represents confidence, the suffix“ZO” stands for “zoomed-out”. Far away from the training data, vanilla BNNs can still be overcon-fident (a, c). Our method fixes this issue while keeping predictions unchanged (b, d).
Figure 2: The construction of our kernel in 1D, as the limiting covariance of the output of a Bayesianlinear model with D ReLU features. Grey curves are ReLU features while thin red curves aresamples. Red shades are the ±1 standard deviations of those samples.
Figure 3: Variance of f (6) as a function of x*. When f is a function over neural network repre-sentations of the data (b), it captures the data region better than when f is only defined on the inputspace (a). Increasing the kernel hyperparameters (here we assume they have the same value for alllayers) makes the low-variance region more compact around the data (c).
Figure 4: Rotated-MNIST results (averages of 10 predictions). x-axes are rotation angles. In (a), allmethods achieve similar accuracies.
Figure 5: Average confidence as a function of α. Top: the vanilla LLL. Bottom: LLL with RGPR.
Figure 6: Toy regression with a BNN and additionally, our RGPR. Shades represent ±1 standarddeviation.
