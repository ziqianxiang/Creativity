Figure 1: Model Overview a) The encoder transforms the raw state into an internal state representa-tion φt. The state representation φt is used by the decoder, Λ(∙, at), and ψ(∙,at). The decoder tries toreconstruct the raw input st from the state representation φt . Λ and ψ produce one output per action,with the former predicting matrices and the latter predicting vectors. Also, both Λ and ψ use twohidden layers to process φt before their final layers. b) Reward prediction by dotting the current stateφt , produced by the encoder, and reward weight w.
Figure 2: Environments a) A graphical representation of the Axes environment. The agent, shownas a red square, must traverse to various goal locations marked with the letter "G". The eight goallocations are split between training, shown as blue boxes, and testing, shown as green boxes. b) Arendering of the Reacher task. The agent controls the robotic Sawyer arm to move the end-effector toa 3D point in space. The eight goal locations are shown as balls. Training goals as green, and testgoals as red.
Figure 3: Performance in Axes and Reacher environments. The propose model, with the non-linearreward, is shown as the green line, while the linear variant is shown as the blue line. The non-linearvariant outperforms the linear variant while differing by only the inclusion of the Λ component,controlled with the β hyperparameter. We assume the worse than random performance of the linearvariant is due to the model learning a degenerate state representation and therefore policy.
Figure 4: Transfer Performance on the Axes and Reacher environments. Performance on the Axesenvironments is lower than training as the test tasks are on average farther from the agent’s initialposition.
Figure 5: Visualization of Lambda Function on half-random Axes. a) The half-random variant ofAxes. b) The learned expected future correlation of one feature with itself along A's diagonal isvisualized over the entire state space. The first column is the max value of Λ over the actions. Theremaining columns, from left to right, correspond to each action: left, up, right, and down. Red andblue correspond to maximal and minimal values.
Figure 6: Guided Exploration: The Λ component of the proposed model is used to guide explorationduring transfer. By using Λ the agent explores in directions with large variance in the state space.
Figure 7: Additional Experimentsperformance is This primary motivation was to understand if there are any confounding factors in theperformance of the linear and non-linear models.
