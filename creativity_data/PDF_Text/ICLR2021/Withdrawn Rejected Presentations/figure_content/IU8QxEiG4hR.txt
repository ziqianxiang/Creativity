Figure 1: SBEVNet overview. We first extract the image features given the target and referenceimage. Using the pair of features, we create a disparity feature volume. We then reduce the disparityfeature volume along with the height and warp it in the bird’s eye view layout space. On a parallelbranch of the network, we apply inverse perspective mapping (IPM) on the reference image andits features. We concatenate the IPM RGB, IPM feature, and the stereo BEV features. The BEVrepresentation is then used to estimate the semantic map through a U-Net. Visibility mask is used toapply the supervised loss only at the locations in the BEV which are in the view of the front camera.
Figure 2: Qualitative results on the test set of the CARLA and the KITTI dataset. The majormistakes in the predictions are annotated by a blue rectangle.
Figure 3: Illustration of mapping the disparity space to bird’s eye view space and inverse perspectivemapping. (a) The operation maps different disparities and x to the BEV space in order to match theground truth. We also show an example layout warped to the disparity space. (b) The inverseperspective mapping operation maps pixels of the reference image to the BEV space in order tomatch the ground truth. The same mapping can be applied to the image features as well.
Figure 4: SBEVNet-CMD overview. We first extract the image features given the target and refer-ence image. Using the pair of features, we create a stereo BEV representation. During training time,we apply inverse perspective mapping (IPM) on the image features which is used to predict the BEVlayout separately. We minimize the L1 distance between first K channels of both the BEV featuremaps. During inference time, we only use the stereo BEV representation to estimate the semanticmap.
Figure 5: Performance as a function of maximum distance from the camera. We consider the pixelsin the BEV layout which are atmost a certain distance away from the camera.
Figure 6: Performance as a function of minimum distance from the camera. We consider the pixelsin the BEV layout which are atleast a certain distance away from the camera.
Figure 7: Performance of the system as a function of amount of training data used.
