Figure 1: Our method learns the 6D grasping policy with a goal-auxiliary task using an egocentriccamera (goals denoted as green forks). We combine imitation learning with a planner (green tra-jectory) and reinforcement learning for known objects. When finetuning the policy with unknownobjects, we use hindsight goals from successful episodes (red trajectory) as supervision. The policylearned in simulation can be successfully applied to the real world for grasping unseen objects.
Figure 2: Illustration of our point aggregation procedure. We show the RGB, depth, foregroundmask and point cloud at time t, and the aggregated point cloud UP to time t.
Figure 3: Our network architecture with the PointNet++ network (Qi et al., 2017) for feature extrac-tion. Both the actor and the critic are regularized to predict grasping goals as auxiliary tasks. Notethat We use quaternions in grasping goal prediction and Euler angles for the action.
Figure 4: (a) Learning curves of different BC models. (b) Learning curves of different RL models.
Figure 5: Success and failure of real-world 6D grasping using our policy trained in simulation.
Figure 6: Complex contacts between the robot gripper with different objects during grasping.
Figure 7: Grasp prediction examples on several ShaPeNet objects. The red fork denotes the gripperPose predicted from our policy network.
