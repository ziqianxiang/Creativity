Figure 1: Preference function is expected to have smooth behavior over the space of items.
Figure 2: Mirroring, shifting and scaling.
Figure 3: Neural network inspired from equation 13.
Figure 4: Training process for different methodsva.
Figure 5: The performance of the clustering techniques on synthetic dataA	AppendixA. 1 Clustering evaluationIn Section 3.1.1, we proposed the k-representation clustering and its boosted version. Here, we studytheir performances via experiments on synthetic data. We recall that the mentioned methods do notexplicitly penalize miss-clustering; instead, they minimize the within-cluster reconstruction loss. Asa result, we expect these methods to perform fairly well when the clusters are distinguishable. Tomeasure the matching between the identified clusters and the original ones, we employ the AdjustedRank Index (ARI) (look at Vinh et al. (2010)). For two clusterings c1 and c2 with the same domainU, if we form the contingency matrix N with the (i, j) element as |{u, c1(u) = i, c2(u) = j}|,then, ARI defined based on the elements, column and row sums of N, intuitively shows the rateof agreement between the two clusterings if u is randomly chosen from U . It takes the maximumvalue 1 for identical clusterings and the minimum value 0 when the clusterings are perceived asfully random with respect to each other. As explained, ARI has the advantage of comparing twoclusterings even with different number of clusters.
