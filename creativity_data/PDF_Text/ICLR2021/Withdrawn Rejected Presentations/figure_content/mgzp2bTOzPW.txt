Figure 1: Permutation language model. It is the combination of Autoregressive (AR) and Autoen-coding (AE) language model, which flexibly learn the context of current token.
Figure 2: Syntactic relevance factorization sampling.
Figure 3: Two-stream self-attention mechanism. The attention mask is used to obtained the factor-ization, and then the context features are learned by autoregressive model, the blue box represent thecontent and the purple box indicates the position.
Figure 4: Sequence length corresponding to different granularity.
Figure 5: Analysis of sentence length and polysemy.
