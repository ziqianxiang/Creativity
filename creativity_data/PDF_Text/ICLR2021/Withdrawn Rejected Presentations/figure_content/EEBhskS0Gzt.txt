Figure 1: Neural network architectures considered in previous and current work. (A) Neural networkarchitecture from Musslick et al. (2016). (B) Adapted neural architecture used for the simulationexperiments described in the main text.
Figure 2: Network structure for minimal basis set (left) and tensor product (right) representationsand the effects of multitasking in each. Red cross indicates error in execution of task because ofinterference whereas green check-mark indicates successful execution.
Figure 3: Effect of varying representational overlap during serial task training. Overlap varies fromhaving completely separate representations for each task (0%, red) to completely sharing represen-tations between tasks (100%, purple). (A) Comparison of learning speed of the networks. Accuracyfor executing only one task per input pattern (training condition) increases faster for high amountsof overlap. (B) Average error for each task when executed in parallel with other tasks (testing con-dition; e.g. the first three bars show the error for Task 1 when executed in parallel with Task 4 fordifferent amounts of representational sharing). Higher amounts of overlap lead to higher error inparallel processing due to interference. For 0% overlap, there is some amount of residual error dueto interference because the network was never trained to execute two tasks in parallel. (C) Cor-relation of convolutional layer representations between Tasks 3 and Tasks 4 computed using theaverage representation for each layer across all the data. We show results for the tasks involving theconvolutional network, as those are the more complex tasks we are interested in.
Figure 4: Effect of serial vs parallel task training. (A) Comparison of learning speed of the networks(cf. Figure 3). (B) Comparison of the error in average task performance over all data when paralleliz-ing compared to serializing (the lack of a bar indicates no error). (C) Correlation of convolutionallayer representations between Tasks 3 and Tasks 4 computed using the average representation foreach layer across all the data. We show results for the tasks involving the convolutional network.
Figure 5: Evaluation of meta-learning algorithm. (A) Comparison of all methods on trade-off in-duced in original environment. (B) Comparison of all methods on trade-off induced in environmentwhere noise is added to inputs. (C) Percent of trials for which meta-learner picks to do single-taskingin both environments.
