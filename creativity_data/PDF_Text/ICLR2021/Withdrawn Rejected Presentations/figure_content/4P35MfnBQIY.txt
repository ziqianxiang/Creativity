Figure 1: Distribution of the correctness rate of past interactions when the response correctnessof current interaction is fixed, for 4 knowledge tracing benchmark datasets. Orange (resp. blue)represents the distribution of correctness rate (of past interactions) where current interaction’s re-sponse is correct (resp. incorrect). x axis represents previous interactions’ correctness rates (valuesin [0, 1]). The orange distribution lean more to the right than the blue distribution, which shows themonotonicity nature of the interaction datasets. See Section A.2 for details.
Figure 2: Augmentation strategies and corresponding bias on model’s predictions (predicted cor-rectness probabilities). Each tuple represents question id and response of the student’s interaction (1means correct). Replacing interactions with similar questions (Q1 , Q3 to Q01 , Q03) does not changemodel’s predictions drastically. Introducing new interactions with correct responses (Q0, Q00) in-creases model’s estimation , but deleting such interaction (Q1, 1) decreases model’s estimation.
Figure 3: Performances with various sizes of training data under the DKT model. x axis stands forthe portion of the training set we use for training (relative to the full train set) and y axis is the AUC.
Figure 4: Response correctness prediction for a student in the ASSISTmentsChall dataset. Werandomly insert interactions with correct responses (interactions with yellow boundaries). In case ofthe vanilla DKT model, the predictions for the original interactions (especially the interactions withgreen boundaries) are decreased, even if the student answered more questions correctly. However,such problem is resolved when we train the model with monotonicity regularization (with the lossLtot = Lori + Lcojins + 100 ∙ Lreg-Cojins). Unlike the vanilla DKT model, predicted correctnessprobabilities for the original interactions are increased after insertion.
Figure 5: Performances (AUCs) of the DKT model for each augmentation and correspondingregularization with different augmentation probabilities (αaug)) and regularization loss weights(λreg-aug).	The hyperparameters are searched over	αaug	∈	{0.1,	0.3,	0.5}	and	λreg-aug	∈{1, 10, 50, 100}. For each dataset, each column represents results with replacement, correct inser-tion, incorrect insertion, correct deletion, and incorrect deletion, from left to right. We set λaug = 1for all cases. We use question-random replacement for ASSISTments2015 dataset.
