Figure 1: Validation BLEU AUCC and test BLEU for IWSLT (high is good). Comparison of regulartransformer and reservoir transformer with FFN or Transformer reservoir layers added.
Figure 2: Validation BLEU AUCC and test BLEU for WMT (high is good). Comparison of regulartransformer and reservoir transformer with FFN or Transformer reservoir layers added.
Figure 3: Validation BPC AUCC and test BPC on the enwik8 language modelling task (low is good).
Figure 4: Downstream RoBERTa performance on SST-2 (left) and MultiNLI-matched (right).
Figure 5: IWSLT comparison of normal v frozen v backskipped4.4	BackskippingWith the reservoir transformers as described above, we obtain better efficiency by skipping the “gra-dient application” matrix addition step in some of the layers (i.e., updating the weights). One stepfurther would be to investigate skipping the entire backward pass for reservoirs altogether, whichwould save us from having to do the much more expensive matrix multiplication for these layersthat is required for the propagation of gradients. We report on preliminary experiments where in thebackward pass we replace the gradients for the layer Li going into the reservoir Li+1 with a noisyestimate (Jaderberg et al., 2017; Czarnecki et al., 2017). Promisingly, Oktay et al. (2020) recentlyasked “why spend resources on exact gradients when we’re going to use stochastic optimization?”and show that you can do randomized auto-differentiation quite successfully.
Figure 6: IWSLT comparison of different hybrid architectures with different reservoir layers.
Figure 7: IWSLT validation AUCC and test BLEU with 6-layer decoder.
Figure 8: IWSLT with 2-layer decoder using different freezing strategy.
Figure 9: RoBERTa Reservoir Results, Pre-training versus downstream task plot for 12 layerRoBERTa. MNLI-m (left). SST-2 (right).
Figure 10: RoBERTa Reservoir Results, Training plot for 12 layer RoBERTa (left). AUCC result(right).
Figure 11: Validation BLEU AUCC and test BLEU for IWSLT (high is good). Comparison ofregular transformer and reservoir transformer with FFN or Transformer reservoir layers added.
Figure 12: Validation BLEU AUCC and test BLEU for WMT (high is good). Comparison of regulartransformer and reservoir transformer with FFN or Transformer reservoir layers added.
Figure 13: Validation BPC AUCC and test BPC on the enwik8 language modelling task (low isgood). Comparison of regular and reservoir transformers for varying depths.
Figure 14: Downstream RoBERTa performance on SST-2 (left) and MultiNLI-matched (right).
Figure 15: IWSLT with 2-layer decoder validation plot (upper left). WMT with 24-layer decodervalidation plot (upper right). Enwik8 with 48-layer decoder validation plot (lower left). RoBERTawith 12-layer decoder validation plot (lower right).
