Figure 1: Comparison among three different manners of advantage estimation & update. πa,t repre-sents the policy of agent a at iteration t and there are n agents in total. Lines with arrow representpolicy updates and ep denotes the update epoch. In single iteration, synchronous update takes onlyone epoch while asynchronous update takes n epochs. In advantage estimation, policies need tobe estimated jointly, and the dashed boxes contain joint polices used for advantage estimation incorresponding update. Particularly, synchronous estimation requires other agents’ future polices.
Figure 2: One step process of marginal advantage Aa(St, Sa1(U1,t))’s estimation with TD residual.
Figure 3: Test wining rate vs. training steps of various methods on SMAC benchmarksthe other policy based MARL algorithm COMA, our algorithm shows considerable improvementin 7 tasks of 8. According to the results, in homogeneous & symmetric tasks such as 3m and 8m,our algorithm converges after 1 million steps of training and reach approximate 100 percent testwining rate. For homogeneous & asymmetric tasks(10m_vs_11m) and simple heterogeneous &symmetric tasks such as 3s5z and 1c3s5z, our algorithms converges slower, and the wining ratefluctuates slightly during training. However, our algorithm also reaches approximate 100 percenttest win rate after 3 million steps of training. For different micro-trick tasks, the performance andconvergence speed of our algorithm varies greatly. While in harder tasks as 10m_vs_11m, MMM2and 2m_vs_1z, COMA algorithm shows no performance. The wining rate after training is testedand shown in table 2. Our algorithm also shows the best performance in most of the tasks.
Figure 4: Display of learned cooperative strategies.
