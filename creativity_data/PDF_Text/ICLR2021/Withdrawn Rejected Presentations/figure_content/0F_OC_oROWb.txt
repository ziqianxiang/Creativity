Figure 1: Gradient descent vs sampling. In gradient descent we estimate the gradient at a given pointand take a small step in the opposite direction direction of the gradient. In contrast, for samplingbased methods we explicitly compute the function at different points and then chose the point wherethe function is minimum.
Figure 2: Notation for variables in the network architecture are shown in this figure.
Figure 3: Accuracy versus the number of times all the weights are updated for RSO and SGD onthe MNIST and on the CIFAR-10 data set. The results demonstrate that the number of times allthe weights need to be updated in RSO is typically much smaller than the corresponding number inSGD. Note that since the weight update step for RSO is linear in the number of parameters of thenetwork, each weight update step is significantly more expensive. For example, on MNIST, SGDtakes 10 seconds per epoch while RSO takes 12 minutes. On CIFAR10, RSO takes 52 minutes perepoch while SGD takes 29 seconds.
Figure 4: Sequential updates versus updating all the weights in convolution layers in parallel orupdating the neurons in parallel. Sequential updates perform the best for both data sets. The parallelupdate scheme enables using distributed learning techniques to reduce wall-clock time to reachconvergence for MNIST. For CIFAR-10, the gap between sequential and parallel may be closed byrunning the parallel setup on a longer learning schedule using distributed learning hardware.
