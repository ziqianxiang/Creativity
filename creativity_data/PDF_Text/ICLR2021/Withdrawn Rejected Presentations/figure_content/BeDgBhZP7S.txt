Figure 1: Delta of accuracy over training epochs between the Frequent and Long Tail for varyingamounts of long-tail samples. We see a clear maxima happen for each model and dataset. Thesemaxima are the points where first derivatives for both sets match and is where we define the phasechange between learning frequent patterns and learning long-tail elements.
Figure 2: Difference in accuracy over remaining training epochs between the regularly trained modeland models where ’Keep’ was applied to all layers for MiniAlexNet - CIFAR-10. We can see thatthe impact on the Frequent set is moderate, while for the Long Tail Set the impact on accuracy isdramatic.
Figure 3: Difference in accuracy over remaining training epochs between the regularly trained modeland models where ’Keep’ was applied to all layers for Resnet18 - CIFAR-10. We can see that theimpact on the Frequent set is greater than with MiniAlexNet, yet the impact on the Long Tail set isstill more acute. More depth seems to increase the interference effect of long-tail samples on patternmining weights.
Figure 4: Difference in accuracy over remaining training epochs between the regularly trained modeland models where ’Purge’ was applied to all layers for MiniAlexNet on CIFAR-10. We can see thatthe impact is drastic for all sets, not only for the Long Tail. We hypothesize this has to do with themodel optimizing itself in spite of these weights.
Figure 5: Difference in accuracy in the Long Tail set over remaining training epochs between the reg-ularly trained model and models where ’Keep’ was applied to a selection of layers for MiniAlexNeton CIFAR-10. We can see the effect of restoring weights in the first layers has much more impacton accuracy than in the final classifier layers as would be expected.
Figure 6: Samples from the Frequent set classified after applying Keep. To the left are samples thatwere classified incorrectly; to the right, samples correctly classified. Incorrectly classified samplesshow harder examples, usually closeups or with an atypical color palette. To the right, we see easierexamples where we get a full view of the class being predicted (car, plane).
Figure 7: Difference in Accuracy of applying Keep for different Phase Change Points on the Fre-quent Set. The earlier the PCP, the higher the differences. PCPs are 5, 1, 8 for MiniAlexNet -CIFAR10, MiniAlexNet - MNIST and Resnet-18 - CIFAR-10 respectively.
Figure 8: Difference in Accuracy of applying Keep for different Phase Change Points on the LongTail Set. The earlier the PCP, the higher the differences. PCPs are 5, 1, 8 for MiniAlexNet -CIFAR10, MiniAlexNet - MNIST and Resnet-18 - CIFAR-10 respectively.
Figure 9: Difference in accuracy in the Long Tail set over remaining training epochs between theregularly trained model and models where ’Purge’ was applied to all layers starting from the PCP.
Figure 10: Accuracy obtained by both Frequent and Lont Tail sets in CIFAR10 in the Mini-Alexnetmodel. The Delta curve shows the difference between the accuracy obtained in both sets.
Figure 11: Accuracy obtained by both Frequent and Lont Tail sets in CIFAR10 in the Resnet-18model. The Delta curve shows the difference between the accuracy obtained in both sets.
Figure 12: Accuracy obtained by both Frequent and Lont Tail sets in MNIST in the Mini-Alexnetmodel. The Delta curve shows the difference between the accuracy obtained in both sets.
Figure 13: Proportion of parameters that are non-convergent relative to the Phase Change Pointfor different amount of long tail examples and models. Increased long-tail samples show muchmore non-convergent parameters, which suggests greater use of parameters. We relate convergentparameters to those that learn frequent patterns.
Figure 14: Proportion of parameters that are non-convergent relative to the initialization point fordifferent levels of long-tail samples and models. Increased amount of long-tail samples show muchmore non-convergent parameters, which suggests greater use of parameters.
