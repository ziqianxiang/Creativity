Figure 1: Parallelization in deep learning - (a) data, (b) model, (C) pipeline and (d) local parallelism.
Figure 2: A comparison forward progagation and backward propagation patterns for the architec-tures considered in this work - (a) backpropagation, (b) greedy local updates, (C) overlapping localupdates, and (d) chunked local updates.
Figure 3: Pareto optimal curves showing the cost vs time tradeoff for an 8-layer, 4096 unit MLPtrained on CIFAR-10 reaching a particular cutoff in training loss. We find that under no circumstanceis backprop the most efficient method for training. ‘ X ' symbol denotes trained models.
Figure 4: Total compute cost vs. serial compute cost (walltime) Pareto curves computed fromvalidation loss for a 6M parameter parameter transformer. We find that for high loss cutoffs (e.g.
Figure 5: Total compute cost vs walltime frontier for ResNet50 models trained on ImageNet. Weshow the cost/time to reach a certain cutoff measured on validation accuracy (top) and training accu-racy (bottom). With low cutoffs (50%, 60%, 70%), modest speedups can be obtained on validationperformance. With higher cutoffs (74%) however backprop is optimal. In the subsequent table, weshow the best accuracies reached for each method across all configurations. We find that the leastparallelized method, Two Chunk Greedy, is the only local method competitive with backprop onvalidation accuracy.
Figure 6: Properties and trade-offs of local parallelism. (a) The cosine similarity between backprop-agation gradients and greedy local gradients for a 5 layer convolutional neural network. Gradientsin the last two layers are identical to, or converge towards, those from backpropagation. Earlierlayer local gradients are increasingly dissimilar to those from backpropagation but are still descentdirections. (b) An ablation of the number of layers per chunk for ResNet18 trained on CIFAR-10.
Figure 7: First layer filters taken at the end of training normalized by the min and max value per filter.
Figure 8:	Pareto optimal curves showing the cost vs time tradeoff for an 8-layer MLP trained onCIFAR-10 with different number of units. From top to bottom we show 4096, 1024, 256, and 64hidden unit MLPs. We continue find that under no circumstance is backprop the most efficientmethod for training. × denote trained models. In the following table we show the best performanceachieved for each different model. We find large models are able to near perfectly minimize this loss.
Figure 9:	Cost wallclock trade off curves for a larger transformer model. We find for high losscutoffs (e.g. 5.), significant speedups (around 4×) can be obtained. For cutoffs of 4.0, and 3.9speedups (around 2×) are still possible, but only with the overlapping method. For even lower cutoffs, 3.8, we find the majority of our models are unable to obtain this loss. In the subsequent tablewe show the best archived validation loss maximized across all configurations.
Figure 10:	Cost wallclock time frontier for ResNet18 models trained on ImageNet. We show thecost/time to reach a certain cutoff measured on validation accuracy (top) and training accuracy (bot-tom). We find with low cutoffs modest speedups can be obtained on validation performance. In thetable we report the best achieved accuracy over all hyperparameters.
Figure 11: Execution traces for ResNet34 on ImageNet, batch size 8 × 8 for backprop and 8 forchunked local parallelism. Green blocks denote inter-IPU communication, pink and red indicatescomputation and light blue represents exchange of data between tiles on the same IPU. Each swimlane represents the execution trace for one of four IPUs, within which there exist a hierarchy of oper-ations spread vertically within the lane, with the lowest-level operation in the hierarchy highlightedin darker pink. The horizontal axis is hardware cycles. The dashed blue and orange boxes in (a)cover the ramp-up and ramp-down phases of pipelined backprop respectively, and a steady state stepis highlighted with the green dashed box and expanded in (b).
