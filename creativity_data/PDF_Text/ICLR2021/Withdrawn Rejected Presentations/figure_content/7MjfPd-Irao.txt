Figure 1: RIDE rewards the agent for taking actions that lead to large impact, which is measured bythe change in the observation embedding φ.
Figure 2: EC rewards the agent for taking actions that lead to observations different from ones storedin the current episodic memory.
Figure 3: Two time steps t and t0are independently sampled from the set{0, . . . , k }, and the input observationis perturbed temporally to generate twocorrelated views. The embedding net-work φ(∙) is trained to maximize agree-ment.
Figure 4: MultiRoomN12S10. Theagent (red) has to reach the goal(green) by passing through multi-ple rooms.
Figure 5: Performance of exploration methods on diverse MiniGrid tasks. Note that EC-SimCLR,the episodic memory extension of RIDE-SimCLR, performs the best on all tasks.
Figure 6: The ROC curves show that the cosine similarity in RIDE-SimCLR is predictive of temporaldistance between states, as expected. Unsurprisingly, the `2 distance in RIDE is not.
Figure 7: Intrinsic reward heatmaps for opening doors (green), moving forward (blue), or turningleft/right (red) on a random instance of MultiRoomN7S8. The top row corresponds to RIDE and thebottom corresponds to RIDE-SimCLR. A is the agent starting position, G is the goal position andD are doors.
Figure 8: Heatmap of state visitation count on the MultiRoomN10S6 task with a random policy andpolicy trained with RIDE-SimCLR in the absence of any extrinsic reward.
Figure 9: Performance on level 1 of Mario with intrinsic reward only.
Figure 10: Intrinsic reward during training on diverse MiniGrid tasks.
Figure 11: RIDE intrinsic reward heatmaps for opening doors (green), moving forward (blue), orturning left/right (red) on a random instance of KeyCorridorS4R3. A is the agent starting position,G is the goal position and D are doors.
