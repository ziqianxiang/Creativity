Figure 1: Image manipulation by text instruction. Each input contains a reference image and atext instruction. The results are synthesized images by our model.
Figure 2:	Method overview. Given an input image X and a text instruction t, the proposed TIM-GAn first predicts a spatial attention mask M (where to edit, Section 3.2) and a text operator fhow(how to edit, Section 3.1). The image feature φχ is then modified by the text operator 九。0 on thepredicted mask M. Finally, the edited image y is synthesized from the manipulated image feature φy.
Figure 3:	Where and how to edit. (a) The calculation of spatial mask M from text feature φtwhereand image feature φx . (b) The proposed text-adaptive routing mechanism executes various paths astext operators. The operator is parameterized by (α, β, γ) generated from text feature φthow.
Figure 4:	User preference studies. We present manipulated images on the Clevr and abstract scenedatasets and ask the users to select the one which (a) is more realistic and (b) is more semanticallyrelevant to the ground-truth image.
Figure 5:	Where and how to edit. (a) We visualize the predicted self-attention weights and spatialattention masks. The self-attention weights are labeled above each word, and highlighted if theweights are greater than 0.2. (b) We show the t-SNE visualization of the routing parameters αpredicted from different types of instructions on the Clevr dataset.
Figure 6: Selected generation results. We show the manipulation results by different approaches onthe Clevr (top), Abstract scene (middle), and Cityscapes (bottom) datasets.
Figure 7: Examples of the generated image by our model on Clevr.
Figure 8: Examples of the generated image by our model on Abstract Scene.
Figure 9: Examples of the generated image by our model on Cityscapes.
Figure 10: Retrieval Results. For each row, top-5 retrieved images are shown. The correct image ishighlighted in the green box.
