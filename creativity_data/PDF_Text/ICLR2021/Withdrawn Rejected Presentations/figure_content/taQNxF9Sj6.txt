Figure 2: hprev is added as an additional keyand value to one self-attention layer. Arrowsshow which positions can pass informationto which other positions.
Figure 1: Augmenting a pretrained trans-former with a recurrence module, allowingreduction of attention computation as well assimpler processing of longer contexts.
Figure 3: Varying degree of overlap while evaluating a transformer with a window size of 3. Thegreen (top) circles are outputs, and the blue (bottom) circles are inputs.
Figure 4: Effect of window size on perfor-mance on PG-19 validation set.
Figure 5: Relationship between FLOPs andperplexity. Curves range over window sizesfrom 200 to 600.
