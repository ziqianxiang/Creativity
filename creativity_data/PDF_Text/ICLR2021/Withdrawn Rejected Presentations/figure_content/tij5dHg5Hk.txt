Figure 1: Framework diagram of RAFT and BYOL. The online network is composed of an encoderfθ and an extra predictor qw. The Mean Teacher fξ is the EMA of the encoder fθ. In BYOL, theloss is computed by minimizing the distance between the prediction of one view xi and anotherview x2 ’s representation generated by the MT. In RAFT, we optimize two objectives together: (i)minimize the representation distance between two samples from a positive pair and (ii) maximizethe representation distance between the online network and its MT.
Figure 2: Analysis on the legitimacy of RAFT and Why it's more favorable. (a) Diagram demon-strating how RAFT conceptually works: if two samples, updating directions are opposite, MT helpspushing them away at the next several iterations. Here Zi = qw (fθ(xi)), Z = fξ(xi), i = 1, 2.
Figure 3: Visualization of the representation distribution evolution on CIFAR10 training set. Weproject the representation fθ(x) to 2-D dimension using PCA (Wold et al., 1987) and then normalizeto a unit sphere. The width of the circle shows the density of the data points that are projected tothat particular position. Two dots residing on each side of the blue line across the circle representstwo augmented views of the same data. (a) Supervised learning has no restriction on the uniformityof representation space. (b) BYOL with predictor evenly projects the data to different positions. (c)BYOL w/o predictor tends to project the huge portion of the data to the same position. (d) BYOL’supper bound BYOL0 also effectively disperses representations on the sphere. (e) Our RAFT showsthat minimizing/maximizing Lcross-model has similar effect on the final representation distribution.
Figure 4: Training behaviors of BYOL with varying structures of the predictor. For detailed expla-nation on the model architecture, refer to Appendix C. (a) Evolution of training loss LBYOL. Whentaken out the predictor, the training loss quickly converges to 0 (red curve, BYOL-NP). Replacingthe MLP-predictor (blue curve, BYOL-MLPP) with the linear predictor (green curve, BYOL-LP)will not cause the collapse even though I is an apparent solution for collapse. Furthermore, ini-tializing the linear predictor with I forces the loss quickly approaching to 0 at beginning, while itrecovers from the seemingly collapse after 10-20 epochs of training (orange curve, BYOL-LPI). (b)Evolution of the representation uniformity. BYOL with predictor consistently optimizes the unifor-mity of the representation distribution even though the uniformity is not explicitly included in theloss term. One interesting fact to note here is that the uniformity loss is optimized with a constantrate with linear predictor (green curve, BYOL-LP; orange curve, BYOL-LPI) after certain phase oftraining. (c) linear evaluation protocol on CIFAR10. Different structures of the predictor provideclose performance on the downstream classification task.
Figure 5: The evolution traces of Lalign(qw ◦ fθ, fξ) and Luniform(fθ) in BYOL0 and our proposedRAFT. (a) Evolution trace of BYOL0 -NP. Increasing β (weight of Lcross-model, regularizer for Lalign)does not prevent the failed regularization: Lalign converges to 0 quickly. (b) Evolution trace ofRAFT-NP. Small value of β doesn’t effectively regularize Lalign, but increasing the weight helps.
