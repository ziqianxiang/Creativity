Figure 1: An agent (smiley)should reach one of 12 goals(stars) in a grid world. Learn-ing to reach a goal in the topright corner helps him to learnabout the other goals in thatcorner. However, learningto reach the green stars (bot-tom left corner) at the sametime gives conflicting objec-tives, hindering training. Taskclustering resolves the issue.
Figure 2: Left: Mean reward and 95% confidence interval (shaded area) from 10 trials when trainingon the chain environment. Right: Task assignment (dots) and task specific reward (color) over thecourse of training the two policies in our approach. Each plot shows one of the policies/estimatedclusters. The assignments converge to the natural clustering reflected by the goal location.
Figure 3: Left: Mean reward and 95% confidence interval (shaded area) from 10 trials when trainingon the grid-world environment depicted in Figure 1. Right: Task assignment (dots) and task specificreward (color) over the course of training for the n = 4 policies (estimated clusters) in our approach.
Figure 4: Left: Mean reward and 95% confidence interval (shaded area) from 10 trials when trainingon the pendulum environment. The curves are smoothed by a rolling average to dampen the noiseof the random starting positions. For (Yu et al., 2020) we used 12 trials out of which 3 failed toconverge and were excluded. Right: Task assignment (dots) and task specific reward (color) from asample run. Two policies focus on long and short, while the others focus on medium lengths.
Figure 5: Evaluation of the BipedalWalker experiments. The shaded areas show the 95% confidenceinterval on the mean task reward. Left: Track and field task set; 6 tasks with varying objectives.
Figure 6:	The results of our experiments on a subset of the Atari Learning Environment games. Thereward is averaged across 3 trials and the shaded region shows the standard deviation of the mean.
Figure 7:	Ablations for different number of policies n. Shaded areas show the 95% confidenceinterval of the mean reward from 10 trials each. Left: Corner-grid-world tasks. Right: Pendulumtasks, learning curves smoothed.
Figure 8:	Comparison of our approach against randomly assigning tasks to policies at the start oftraining. Shaded areas show the 95% confidence interval of the mean reward. Left: Corner-grid-world tasks, 10 trials each. Right: Pendulum tasks, 10 trials each, learning curves smoothed.
Figure 9: Shown are the assignments from 4 randomly picked trials on the track and field Bipedal-Walker task set.
Figure 10: Shown are the assignments from 4 randomly picked trials on the first BipedalWalker taskset. l refers to the lenghts of the legs, o refers to the frequency of obstacles.
Figure 11: Shown are the assignments of all three trial that were run on the set of Atari games. Thecolor represents the human-normalized score per game.
Figure 12: Show is the difference between using random assignments or our EM approach fordifferent numbers of policies. On the left the development during training is shown, on the right theaverage performance gap over the last 10% of the training is visualised.
