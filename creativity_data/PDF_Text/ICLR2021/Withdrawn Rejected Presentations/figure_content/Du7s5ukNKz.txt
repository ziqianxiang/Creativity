Figure 1: Illustration of weakly supervised policy learning and our PeerPL solution with correlatedagreement (CA). We use Y to denote a weak supervision, be It a noisy reward, or a noisy demonstra-tion. Eva stands for an evaluation function. “Peer Agent” corresponds to weak supervisions.
Figure 2: Learning curves of DDQN on CartPole with true reward (r) ■, noisy reward (r) ■,surrogate reward (Wang et al., 2020) (r)	, and peer reward (fpeer, ξ = 0.2) ■.
Figure 3: Learning curves of BC on Atari. Standard BC ■, PeerBC (ours) ■, expert ■.
Figure 4: Policy co-training on control/Atari. Single view H, Song et al. (2019) , PeerCT (ours) ■.
Figure A1: Learning curves on CartPole game with true reward (r) , noisy reward (r) , surrogatereward (Wang et al., 2020) (r)	, and peer reward (FPeer, ξ = 0.2) ■. Each experiment is repeated10 times with different random seeds.
Figure A2: Learning curves of DQN on CartPole game With Peer reward (Fpeer) ■ under differentchoices of ξ (from 0.1 to 0.4).
Figure A3: Learning curves of DDPG (Lillicrap et al., 2015) on Pendulum with true reward (r) ■,noisy reward (r) ■, and peer reward (Fpeer) ■.
Figure A4: Learning curves of DQN on CartPole game With Peer rewards (FPeer) ■. Here, a lineardecay ξ is applied during training procedure (initial ξ = 0.4). Compared to static ξ = 0.4, the lineardecay Peer Penalty stabilizes the convergence of RL algorithms.
Figure A5: Sensitivity analysis of ξ for PeerBC on Pong With behavior cloning ■, PeerBCvaries from 0.2 to 0.5 and 1.0), expert ■, and SQIL ■ reported by SQIL (Reddy et al., 2019).
Figure A6: Learning curves of DDQN on CartPole with true reward (r) ■, noisy reward (r) ■, noisyreward + VRT ■, surrogate reward (Wang et al., 2020) (r) ■, surrogate reward + VAT ■ and peerreward (FPeer, ξ = 0.2) ■.
Figure A7: The policy entropy of the PPO agent during training. The imperfect expert model istrained for 0.2 × 107 timesteps as the red line indicates.
