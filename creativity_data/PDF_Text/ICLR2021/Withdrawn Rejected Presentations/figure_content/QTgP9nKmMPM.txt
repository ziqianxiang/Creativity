Figure 1: High level framework of conven-tional deepGCN (upper) and layerwise deep-GCN (lower). The aggregation step (A) cor-responds to F H(l-1) operation and the trans-formation step corresponds to σ(∙W⑴)oper-ation. In conventional training, both steps aredone for every iteration, but for sequential layer-wise training, we can decouple these two opera-tions to conduct the transformation step in everyiteration, but the aggregation step only once atthe beginning of each layer, which results in thedemonstrated time saving.
Figure 3: Comparison of sequential and parallel training. Results show that parallel can quicklycatches up to sequential training within a few epochs.
