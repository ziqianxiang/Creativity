Figure 1: A basic image reconstruction task. One can either use a state-of-the-art PGAN (Karraset al., 2017), optimizing the latent code to match the generation to the input, or use our simplerAugIntAE. Both succeed on the training set of adult faces (left), but on the novel domain of babyfaces (right), the best PGAN reconstruction ages the baby. Ours is far more faithful.
Figure 2: A VAE (left) and WGAN-GP (right) trained on MNIST (top row) successfully recovertraining images (row 2) and synthesize new ones (row 3). They struggle to represent images fromnovel classes (EMNIST letters, rows 4 and 5), even when latent codes are optimized directly, as anoracle encoder (row 6). VAE reconstructions use the mean embedding μ directly and do not samplein latent space. Best viewed digitally.
Figure 3: Randomly chosen AE image reconstructions for unseen classes. From left to right: MNISTgeneralizes to EMNIST, Omniglot train generalizes to Omniglot test, and CIFAR-10 generalizes toCIFAR-100, but MNIST does not generalize trivially to white noise. Bottom row: nearest (in pixelspace) training image is shown for comparison. Best viewed digitally.
Figure 4: Training for AugIntAE. For every training image, two sets of augmentation parametersρ (rotation, translation, etc.) are sampled, plus a mixing coefficient α. The network must recoverthe mixed transformation image from the same mixture in latent space. An auxiliary adversarialdiscriminator improves quality.
Figure 5:	Examples of interpolation between handwritten character pairs from novel classes (EM-NIST left, Omniglot test right). AugIntAE interpolates smoothly, preserves the desired shape, andunlike the AE/IntAE baselines, produces minimal artifacts even when transformations are non-affine(far left). Best viewed digitally.
Figure 6:	Comparison to few-shot generators. Each row corresponds to a distinct set of 5 seedimages (left). Neural Statistician and DAGAN overfit to the training data. The AE successfullygenerates letters but introduces visual artifacts. AugIntAE more effectively removes these artifactsthan the baseline IntAE. Results from AE-based models are midpoint interpolations between seedpairs. Best viewed digitally.
Figure 7: Ablation study on forms of data augmentation. Each row corresponds to a distinct set of 5seed images (left); results are midpoint interpolations between seed pairs. Each individual form ofaugmentation improves over the AE baseline (fewer artifacts), but none approach the performanceof using them all together (right). See, for example, the “S” in the top-right corner of each plot. Bestviewed digitally.
Figure 8: CelebA interpolations involving simultaneous pose and lighting shifts. AE and IntAEdarken the nose unrealistically (left) or produce transparent artifacts (right), while AugIntAE accu-rately navigates the lighting/pose shift. Best viewed digitally.
Figure 9: Six seed images (left) are sufficient to produce a variety of novel images (right). Thefifteen displayed here represent the midpoints of the fifteen unique pairwise interpolation paths. Were-emphasize that this network was trained only on male faces. Best viewed digitally.
Figure 10: Examples of interpolation between image pairs from novel classes (CIFAR-100). AugIn-tAE interpolates smoothly with minimal transparency and color artifacts. Examples are hand-pickedso that interpolation is possible. Best viewed digitally.
Figure 11:	L2 pixel reconstruction error as a function of noise frequency, [0, 1]-scaled. Dotted linesrepresent reconstruction loss on novel domain images (EMNIST for the MNIST model, CIFAR-100for the CIFAR-10 model, etc.
Figure 12:	Attempted reconstructions of single-channel uniform noise. First row is noise, second isthe MNIST model, third is the Omniglot model. Both models clearly struggle to abandon a learnedpenstroke prior.
Figure 13: Attempted reconstructions of three-channel uniform noise. First row is noise, secondis the Celeb-A model, third is the CIFAR-10 model. As noise frequency climbs, the networksprogressively abandon the input signal and attempt to extract a lower-frequency pattern.
Figure 14: Black digits on white background are seed images, and white digits on black backgroundare novel synthesized images. Neural statistician (left) and DAGAN (right) produce the desiredbehavior on training classes. Thus failures on novel classes clearly represent a failure to generalize.
Figure 15:	Illustrative EMNIST pairs. Each image pair contains artifacts in the AE/IntAE modelsthat AugIntAE is able to avoid.
Figure 16:	Illustrative Omniglot pairs. Each image pair contains artifacts in the AE/IntAE modelsthat AugIntAE is able to avoid.
Figure 17: Illustrative Celeb-A pairs. Each image pair contains artifacts in the AE/IntAE modelsthat AugIntAE is able to avoid. AEs generally produce transparency and silhouette artifacts aroundthe hairline, mouth, and chin, while IntAEs produce nonsmooth interpolations, unrealistic headcontours, and/or color artifacts. We note that AugIntAE is prone to smoothing away fine details,sometimes to a greater degree than AE.
Figure 18: Illustrative CIFAR-100 pairs. AEs are largely indistinguishable from pixel fade, whileIntAEs produce color and texture artifacts. AugIntAE does not always successfully preserve seman-tic content through the interpolation, but the interpolation at least usually makes sense. Of particularnote are the apples on the left-hand side: rather than fade away the stem, the AugIntAE modelretracts it into the apple! However, we again note that the more sensible AugIntAE interpolationcomes at the cost of smoothing away fine details.
Figure 19:	Random EMNIST pairs. AugIntAE is superior in some cases, and gives roughly equalperformance in the remainder.
Figure 20:	Random Omniglot pairs. AugIntAE successfully avoids many visual artifacts, but alsotends to oversimplify complex shapes.
Figure 21: Random Celeb-A pairs. Pixel fade is actually a reasonably strong baseline on Celeb-Abecause of how the images are aligned, so in many cases the models all arrive at the same reasonablygood interpolation.
Figure 22: Random CIFAR-100 pairs. There is clear room for future work - in some cases theAugIntAE output is unacceptably blurry, and the AugIntAE interpolation frequently fails to preservesemantic content.
