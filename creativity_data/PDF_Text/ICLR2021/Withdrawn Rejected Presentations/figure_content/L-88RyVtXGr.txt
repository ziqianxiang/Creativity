Figure 1: Illustration of the proposed filter basis sharing method. A normal convolution layer in (a)can be decomposed into a filter basis Wbasis and coefficients a as in (b). A filter basis is a genericbuilding block that can be shared by repetitive convolution layers as in (c). Some layer-specificcomponents (Wbasis and aâ‘´)can be combined for better expressiveness of recursive layers.
Figure 2: The flows of gradients in 4 shared bases of ResNet34-S16U1 at the same epoch. Forcomparison, orthogonal regularization and batch normalization (BN) following the bases are turnedon and off. In (b) and (c), BNs and orthogonal regularization, respectively, improve the flow ofgradients. In (d), when both BNs and orthogonal regularization are applied simultaneously, thestrongest flow of gradients is observed. This trend is consistently observed during the training.
Figure 3: Cosine similarities of bases and coefficients of ResNet34-S16U1 (2-th and 3-th groups.) Inthe upper row, X and Y axis are indexes to the shared/unique components of the bases. The first 32 and64 basis components of the 2-th and 3-th groups are shared by 6 and 10 recursive convolution layers,respectively. The others are non-shared unique basis components of those layers. Orthogonalityregularization is applied only to shared components. The lower row shows corresponding coefficientsin the residual block groups. Brighter color corresponds to higher similarity.
Figure 4: Testing errors vs. the number of parameters and FLOPs on CIFAR-100. The number ofshared basis components (s), and non-shared basis components (u) are varied. Using more sharedbasis components results in better performance. In contrast, using more non-shared components doesnot always improve performance.
Figure 5: Block structure of ResNet34 and a shared basis. For basic blocks, a shared basis is definedby decomposing original 3x3 convolution filters.
Figure 6: Block structure of ResNet50 and shared bases. In the shared bottleneck block, the first1x1 convolution and the second 3x3 convolution are considered as two bases of the bottleneck block.
Figure 7: Block structure of MobileNetV2 and shared bases. In the shared block, the first 1x1pointwise convolution and the second 3x3 depthwise convolution are considered as two bases of theblock. During training, orthogonal regularization is applied to these bases separately.
