Figure 1: L-conv layer architecture. Li only acton the d spatial dimensions, and Wi only act onthe mι feature. For each i, this action is analogousto a Graph Convolutional Network with d nodesand mι features per node.
Figure 2: Results on four datasets with two variant: “Rotated” and “Rotated and scrambled”. In allcases L-ConV performs best. On MNIST, FC and CNN come close, but using 5x more parameters.
Figure 3: Comparison of one and two layer performance of L-conv (blue), CNN without pooling(orange), CNN with Maxpooling after each layer (green), fully connected (FC) with structure similarto L-conv (red) and shallow FC, which has a single hidden layer with width such that the total numberof parameters matches L-conv (purple). The labels indicate number of layers and layer architecture(e.g. “2 L-conv” means two layers of L-conv followed by one classification layer). Left and middleplots show test accuracies on CIFAR100 with rotated and scrambled images, and on the originalCIFAR100 dataset, respectively. The plot on the right show the number of parameters of each model,which is the same for the two datasets.
