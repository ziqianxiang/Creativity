Figure 1: Comparison of optimization methods for ResNet18 Training on CIFAR10.
Figure 2: Comparison of optimization methods for VGG19 training on CIFAR100.
Figure 3: Comparison of optimization methods for two-layers LSTM training on WikiText-2.
Figure 4: Comparison of optimization methods for six-layers LSTM training on SWB-300.
Figure 5: The growth of quantity Pit=1 kzi k in Adam+0.1 for the first 10 epochs and then annealed by V0.5 for another 10 epochs, with momentum 0.9.
Figure 6: Comparison of Adam+ and Adam+ with Fixed StepsizeFigure 7: log(Pit=1 kzik) versus log(t)Adagrad and momentum SGD. If we use the same learning rate scheduler of SGD for Adam+, thenAdam+ performs the best.
Figure 7: log(Pit=1 kzik) versus log(t)Adagrad and momentum SGD. If we use the same learning rate scheduler of SGD for Adam+, thenAdam+ performs the best.
