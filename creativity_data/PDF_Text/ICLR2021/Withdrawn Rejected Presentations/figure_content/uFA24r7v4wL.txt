Figure 1: Comparison among different approaches of distributed GCN training for large graphs.
Figure 2: Illustrating vanilla partition parallel GCN training, where large graphs are partitionedinto smaller sub-graphs (see inner nodes) with each can be fit into one GPU memory. But exces-sive boundary nodes (in orange) are introduced in each sub-graph by GCNsâ€™ neighbor aggregation,which not only drastically increases memory cost of each subgraph but also incurs heavy commu-nication overhead between subgraphs, rendering such partition parallelism ineffective.
Figure 3: Throughput comparison on Redditdata. Each partition uses one GPU, exceptCAGNET(c=2) uses doubled GPUs.
Figure 5: Application-level memory usage re-duction achieved by BDS-GCN. Reduction areagainst baselines without sampling in each num-ber of partitions.
Figure 4: Training time breakdown of BDS-GCN with different boundary sampling rates.
Figure 6: Convergence comparison between unsampled full-graph distributed training (i.e., BDS-GCN with p=1) and boundary-node sampled distributed training (i.e., BDS-GCN with p < 1) overdifferent numbers of partitions on Reddit.
