Figure 1: (Left) Calibration error values for two ResNet models A and B on CIFAR-10 test set.
Figure 2: (Left) Rejection results on CIFAR-10 for decreasing uncertainty threshold comparing H(p)and maxp as uncertainty metric. In both cases, the top-1 error decreases strictly monotonicallywith decreasing threshold. (Right) Out-of-distribution detection for CIFAR-10 → CIFAR-100. Thenormalized entropy H(p) as measure of uncertainty can be used to robustly detect OoD data.
Figure 3:	Calibration diagrams for a toy experiment with a degenerated model constantly predictingthe marginal probabilities p = (0.6, 0.4) in a binary classification task. ECE and MMCE onlyconsider maxp and fail at capturing the miscalibration of class 2 with p(c = 2) = 0.4, butacc(c = 2) = 0. The red bars show the measured miscalibration. Uncertainty is given as normalizedentropy. The left diagram is computed using ECE as MMCE does not involve binning.
Figure 4:	Toy experiment with two random models A and B in a binary classification task. UCE isless sensitive to number of bins used in the estimator and provides a consistent ranking of the models.
Figure 5:	Calibration error vs. softmax temperature on CIFAR-10. All metrics provide inconsistentranking of models over τ . The metrics ECE, UCE and MMCE have a narrow region in which theoptimal temperature for all models can be found. They show more a consistent ranking before andafter the point of optimal temperature. This allows comparison of calibration of models if theyare all over- or under-confident. However, all metrics fail at comparing underconfident models tooverconfident models. Even at optimal temperature, Brier score and NLL fail at comparing calibrationof models with different accuracy, as the metrics are always lower for models with better accuracy.
Figure 6:	Calibration error vs. softmax temperature on CIFAR-100.
Figure 7:	Calibration error vs. softmax temperature from SWAG trained with different regularizationon CIFAR-10. Both MMCE and UCE regularization lead to less overconfident models and reducemiscalibration (optimal temperature is closer to τ = 1). Entropy regularization leads to underconfidentmodels and is not as effective as MMCE and UCE regularization on CIFAR-10. MMCE and UCEregularization at optimal temperature outperform entropy regularization at optimal temperature forall metrics except Brier score.
Figure 8:	Calibration error vs. softmax temperature from SWAG trained with different regularizationon CIFAR-100. In this experiment, entropy regularization without temperature scaling (τ = 1)was surprisingly effective and outperforms MMCE and UCE regularization. However, at optimaltemperature both MMCE and UCE regularization outperform entropy regularization for all metrics.
Figure 9:	Rejection results on CIFAR-100 for decreasing uncertainty threshold comparing H(p) andmax p as uncertainty metric. In both cases, the top-1 error decreases monotonically with decreasingthreshold.
Figure 10: Binning estimator sample distribution for ResNet-34 on CIFAR-10 (left) and for ResNet-50 on CIFAR-100 (right) with M = 15 bins. ECE and UCE use fixed bin widths and ACE uses anadaptive binning scheme. The number of samples per bin for ECE and UCE are similar on CIFAR-10.
