Figure 1: An overview of our PROTECTOR pipeline. (a) The perturbation classifier Cadv correctly categorizesrepresentative attacks of different types. (b) An illustration of the trade-off in Theorem 2, where an adversarialexample fooling Cadv (the 'âˆž sample marked in red) becomes weaker to attack the second-level Mp models.
Figure 2: PCA for different types of adver-sarial examples on MNIST.
Figure 3: Illustration of the effect of random noise forgenerating adversarial examples. Note that the notion ofsmall and large perturbations is only used to illustratethe scenario in Figure 3, and in general none of theperturbation regions subsumes the other.
