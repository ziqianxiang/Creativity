Figure 1: Median and standard error (over 100 runs) of objective value (f (θ)), accumulated update size(PT=1 l∣gt/√vt||2) and total change in adaptive learning rate (PT=1 ||房一√v= ∣∣ι) for Adam, AMS-Grad, MAdam on the problem defined in Eq. 12.
Figure 2: Results on NQM. The left figure shows the mean and standard error of the loss under differentlearning rates η, computed over 100 runs at each point. We select the best β for ADAM at each η . The bestresults (mean and variance) of Adam and MAdam are 1.84e-3 (2.51e-4) and 4.05e-3 (4.84e-4) respectively.
Figure 3: Training loss, validation accuracy and step size of various optimization methods on SST-2. Alloptimizers here use λ = 0.1. ADAM and LAPROP use (η, β)=(1e-5, 0.98), MADAM and LAMADAM use(η,β, β)=(4e-5, 0.5, 0.98), ADAM-η0 and LAPROP-η0 use (η, β)=(1.6e-5, 0.98).
Figure 5: Distribution of effective step size of AdaBound and MAdam at iteration 10000, 30000 and 60000on IWSLT’14. Red lines indicate the clipping range of AdaBound. On the top/bottom are results of Ad-aBound/MAdam with learning rates 5e-4/1.25e-3.
Figure 6: More results on the Noisy Quadratic Model.
Figure 7: Training loss, test accuracy and average step size on CIFAR10.
Figure 8: Training loss, validation BLEU and average step size on IWSLT’14 DE-EN, trained with η=5e-4, λ=1e-2, β=0.999 for LAPROP and η=1.5e-3, λ=1e-2, β=0.5, /3=0.999 for LaMAdam, and η=4.375e-4,λ=1e-2, e=0.999 for LAPROPR.	一optimizer in Table 5.1, except for the result of Adam , which was copied from Liu et al. (2020)but uses the same hyperparameters except for the learning rate and weight decay. For LaProp,MADAM and LAMADAM, we choose learning rates from {1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 6e-3, 8e-3}and weight decay from {0.003, 0.006, 0.01, 0.012, 0.02, 0.03}, and found the best combinations forLAPROP, MADAM and LAMADAM are (2e-3, 0.03), (5e-3, 0.012) and (6e-3, 0.012). For SGD, wechoose learning rate from {0.05, 0.1, 0.2} and weight decay from {5e-5, 7e-5, 1e-4}, and found thebest combination to be (0.1, 7e-5).
