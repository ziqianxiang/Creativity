Figure 1: The representation learning pipeline of TimeAutoML. There are totally eight modulesforming this pipeline, namely data augmentation, encoder fen, attention, decoder fde, similarity se-lection fsim, estimation network fest, EM estimator ∕em, auxiliary classification network 九展.And∕bce represents the binary cross entropy computation, D(Xi, χjec), E(yj and Lself(Xi) representthe reconstruction loss, energy and self-supervised contrastive loss of input sample Xi, respectively.
Figure 2: AUC scores of TimeAutoML on Figure 3: Anomaly interpretation via analy-ECGFiveDays dataset when irregular sam- sis in latent space.
Figure A1: AUC scores of TimeAutoML and state-of-the-art anomaly detection methods on univari-ate datasets when irregular sampling rate β varies from 0 to 0.7.
Figure A2: AUC scores of TimeAutoML and state-of-the-art anomaly detection methods on multi-variate datasets when irregular sampling rate β varies from 0 to 0.7.
Figure A3: Irregular sampling on Sine curves.
Figure A4: The average rank of our method and state-of-the-art models. Our method is an unsu-pervised representation learning method, combined with other classification algorithm can reach anexcellent classification performance. We combine GBDT with our model in the experiment. A lowervalue of average rank represents a better classification performance.
Figure A5: The sensitivity analysis about weighting factors λ1 and λ2, for each combination ofλ1 ∈ {0.00001, 0.0001, 0.001, 0.01, 0.1, 1} and λ2 ∈ {0.001, 0.01, 0.1, 1, 10}.
Figure A6: The sensitivity analysis about Beta distribution priors α° and β0, for each combinationof αo ∈	{5,10,15,	20, 25,	30}	and g°	∈	{5,10,15,	20, 25,	30}.	一H Appendix H: Motivation of involving Gaus s ian Mixture ModelThe idea of TimeAutoML is to map the original time series into the latent space representation andthe generated representation can be used to carry out many downstream machine learning tasks, suchas anomaly detection, clustering, classification and so on.
