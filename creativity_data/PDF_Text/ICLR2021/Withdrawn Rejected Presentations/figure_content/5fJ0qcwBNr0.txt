Figure 1: Gradient values on different models. The X-axis is the index of layers and Y-axis denotesthe average absolute gradient value in each layer. The dotted line shows the average of all absolutegradients in a model. “Baseline” has extremely low gradient values on all layers and its gradients aremultiplied by 1000 for clear presentation. Even with advanced training techniques, the vanishing andexploding problem is not addressed completely. The gradients between the top layer and the bottomlayer are still in orders of magnitude in “LayerNorm”, “Residual”, and “ResNorm”.
Figure 2: From left to right, we demonstrate the charts of gradient values at initialization and attraining stages, gradient correlations at initialization, and at training stages. “Baseline” and “Layer-Norm” with either lower gradient correlations or lower gradient values achieve worse convergenceperformance. “ResNorm” and “Residual” with higher gradient correlations or higher gradient values,obtain better convergence performance.
Figure 3: Test accuracy of different mod-els on CIFAR10.
Figure 4: Higher kernels brings more steady performance. Each dot represents a single architecturewith its kernel (X-axis) and accuracy (Y-axis). For each dataset, we draw two plots with a differentgranularity of kernel ranges.
