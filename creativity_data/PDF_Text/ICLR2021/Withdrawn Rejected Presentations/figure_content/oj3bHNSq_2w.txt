Figure 1: Median Frechet Inception Distance during training for ten runs on MNIST, CIFAR-10,CAT 1282 and FFHQ 5122, using very simple convolutional GANs. The shaded areas show mini-mum and maximum value during training for the cost formulations. MM-nsat is best overall, suffersless from gradual mode dropping and trains reliably on the more challenging datasets.
Figure 2: Scaling factors as a func-tion of the discriminator output, ata scale emphasizing asymptotic be-haviors. MM-GAN’s scaling factorcauses G’s gradient to vanish whenDp(G(Z)) → 0, which corresponds tothe maximum value of its cost.
Figure 3: Update step size for an arti-ficial gradient gt = exp(-0.001t) withβ1 = 0.99, β2 = 0.999 and α = 1. Ingreen and red, simulated and approxi-mated update magnitudes, using equa-tions 8 and 11. Updates vanish evenwhile gt .
Figure 4: Real and generated samples on aclassic toy 2D mixture of Gaussians dataset: asample is here a coordinate pair. In blue, NS-GAN samples exhibiting mode dropping be-havior, here in the process of hopping betweenmodes. In red, samples using the unmodifiedMM-GAN cost function. MM-GAN trains wellon this toy problem despite its saturating costfunction and shows reasonable coverage of allmodes.
Figure 5: Gradient magnitudes in the earlyphase of training on MNIST with 4-layer fullyconnected networks. Top two: NS-GAN work-ing properly and unmodified MM-GAN in itssaturating failure mode, ultimately freezing pa-rameters. Bottom two: MM-nsat, and MM-GAN with G’s Adam parameter reduced β2 =0.999 → 0.99, both avoiding saturation.
Figure 7: Samples from training runs for CAT1282 using the DCGAN architecture with spec-tral normalization in the discriminator and self-attention. All NS runs collapse in terms of di-versity, compared to only one of the MM-nsatruns. We show the least favorable comparisonfor MM-nsat, i.e. the run with latest collapsefor NS and earliest collapse for MM-nsat. Foreach of these runs, we hand pick the best look-ing batch of samples generated during training.
Figure 6: Generated samples after 1000 epochsfor MNIST with 4-layer fully connected net-works. MM-nsat is strikingly better in termsof mode coverage.
Figure 8: Left: FID, an estimated distance between generated and real data based on Inceptionactivations. Right: DJSCD, the Jensen-Shannon divergence between class distributions in generatedand real data. For various networks using the MNIST and CIFAR-10 datasets, we plot the medianvalues for ten runs during training. The shaded area indicates maximum and minimum values. NSand MM are the traditional cost functions; variants use normalized gradients, reduced β2 for G’sAdam optimizer and the non-saturating rescaling from eq 6.
Figure 9: FID during training with additional diagnostics for 4-layer convolutional (top) and fully-connected (bottom) networks on MNIST, using (left) NS-GAN and (right) MM-nsat for the param-eter updates. For each run, we calculate both the NS-GAN and MM-nsat gradients at each iterationand plot their relative norms (green) and cosine similarity (orange). We plot the running means, withthe actual, noisy values indicated by the shaded areas.
Figure 10: FID during training with additional diagnostics for 4-layer convolutional (top and middle)and fully-connected (bottom) networks on MNIST, using (left) MM-GAN and (right) MM-nsat forthe parameter updates. For each run, we calculate both the MM-GAN and MM-nsat gradients at eachiteration and plot their relative norms (green) and cosine similarity (orange). We plot the runningmeans, with the actual, noisy values indicated by the shaded areas. Since both Conv-4 MM-GANproduces both good and bad results frequently, we show diagnostics for both cases.
Figure 11: Gradient magnitudes in the early phase of training on MNIST with 4-layer fully con-nected networks: as in fig 5, but including additional tests for MM-GAN with modified β2 hy-perparameters for D’s and G’s optimizers. As suggested by our model in section 2.4, MM-GANsaturation when using Adam depends on β2 values causing gradients to vanish. In particular, a lowervalue of β2 for G than D disrupts the feedback mechanism that otherwise brings training to a halt.
Figure 13: MNIST samples for 4-layer convolutional (left) and fully connected networks (right),using the saturating MM-GAN cost. Top to bottom show samples during training for a single run:we reuse the same set of latents z to generate samples each time. Cross-reference with figure 12,showing diagnostics for the same runs. These samples show the behavior of the MM-GAN generatorwhen it falls into its saturating failure mode: G’s outputs effectively cease to change due to negligibleparameter updates.
Figure 14: Left: FID, an estimated distance between generated and real data based on Inceptionactivations. Right: DJSCD, the Jensen-Shannon divergence between class distributions in generatedand real data. Copying the CNN settings from Miyato et al. (2018) for the CIFAR-10 dataset, we plotthe median values for ten runs during training. The shaded area indicates maximum and minimumvalues.
Figure 15: Left: FID, an estimated distance between generated and real data based on Inceptionactivations. Right: DJSCD, the Jensen-Shannon divergence between class distributions in generatedand real data. For various networks using the MNIST and CIFAR-10 datasets, we plot the medianvalues for ten runs during training. The shaded area indicates maximum and minimum values. Costsare the traditional MM-GAN and NS-GAN (Goodfellow et al., 2014), as well as our MM-nsat fromeq 6 and linear combinations as given in eq 24.
Figure 16: Left: FID, an estimated distance between generated and real data based on Inceptionactivations. Right: DJSCD, the Jensen-Shannon divergence between class distributions in generatedand real data. For various networks using the MNIST and CIFAR-10 datasets, we plot the medianvalues for ten runs during training. The shaded area indicates maximum and minimum values. Mod-ified cost functions behave as predicted by their scaling factors according to theory in section 2.3.
Figure 17: Left: FID, an estimated distance between generated and real data based on Inceptionactivations. Right: DJSCD, the Jensen-Shannon divergence between class distributions in generatedand real data. For various networks using the MNIST and CIFAR-10 datasets, we plot the medianvalues for ten runs during training. The shaded area indicates maximum and minimum values. NS isGoodfellow’s non-saturating cost; MM-nsat is from eq 6; Hinge and LS are alternative formulationsfor comparison, which use different costs for both D and G.
Figure 18: Scatter plot DJSCD vs FID for 4-layer convolutional networks on CIFAR-10, using MM-nsat and NS cost functions. We regularize D with zero-centred real data gradient penalty, sweepingacross different values of the weighting constant λ used in D’s cost function, and train for 1000epochs. Each dot represents a single run, while the larger circles show the mean value for thetwenty different runs for each value of λ. The dotted lines indicate each cost function’s trajectoryfor increasing values of λ. With very weak regularization, MM-nsat is far stronger than NS, butfor increasingly strong regularizations, the two costs converge in terms of performance. Their bestresults (λ = 100) are similar and match weakly regularized MM-nsat in terms of FID, but withimproved class balance. Performance progressively degrades with stronger regularization.
Figure 19: Real and generated samples on modified 2D mixture of Gaussians: modes ordered alongan expanding spiral from the origin, making in excess of two revolutions. Density of modes increasesalong this spiral. In blue, NS-GAN. In red, samples using the unmodified MM-GAN cost function.
Figure 20: Scatterplots DJS vs FID with trendlines for 20 training runs, using linear combinationsof the NS and MM-nsat costs (eq 24), on the MNIST-1K dataset created by combining triplets ofsamples from MNIST into 3-channel RGB images. Left: zero centred gradient penalty and spectralnormalization. Right: no spectral normalization and relaxed zero centred gradient penalty.
Figure 21: FID during training for ten runs on CAT 1282 with two different network architectures.
Figure 23: FID, an estimated distance between generated and real data based on Inception activa-tions. For various resolutions using the FFHQ dataset, we plot the median values for three runsduring training. The shaded area indicates maximum and minimum values. Results for FFHQ10242 has been left out: neither MM-nsat or NS-GAN achieve FID values meaningfully better thanrandomly initialized networks. For resolutions from 322 to 5122, training collapse is universal forNS-GAN while MM-nsat is stable and achieves similar FID for low resolutions and much better FIDfor high resolutions.
Figure 24: FFHQ 5122 samples for 9-layer convolutional networks. Subfigures (a-c) show NS-GAN’s early stopping performance, its abrupt, catastrophical mode collapse and its final, nonsensi-cal samples. Subfigure (d) shows final samples for MM-nsat, which fall well short of photo-realism,but are of higher quality than the NS-GAN early stopping samples and fairly good given the exceed-ingly simple network architecture.
