Figure 1: Meta-RL setting: Given a new environment and task, the agent is allowed to first explore and gatherinformation, and then must use this information to solve the task in subsequent exploitation episodes.
Figure 2: (a) Coupling between the exploration policy πexp and exploitation policy πtask. These policies areillustrated separately for clarity, but may be a single policy. Since the two policies depend on each other (forgradient signal and the τexp distribution), it is challenging to learn one when the other policy has not learned. (b)DREAM: πexp and πtask are learned from decoupled objectives by leveraging a simple one-hot problem ID duringmeta-training. At meta-test time, the exploitation policy conditions on the exploration trajectory as before.
Figure 3: (a) Sample complexity of learning the optimal exploration policy as the action space |A| grows (1000ʌ ʌseeds). (b) Exploration Q-values Qexp(a). The policy arg maxa Qexp(a) is optimal after the dot. (c) Exploitationvalues given optimal trajectory Vtask(T?xp). (d) Returns achieved on a tabular MDP With |A| 二 8 (3 seeds).
Figure 4: Didactic grid worlds to stress testexploration. (a) Navigation. (b) Cooking.
Figure 6: Cooking results: only Dream achieves optimal reward on training problems (left) logged every10 meta-training trials, and on generalizing to unseen problems (middle). 3D visual navigation results: onlyDream reads the sign and solves the task (right).
Figure 5: Navigation results. Only Dreamoptimally explores all buses and the map.
Figure 8: Examples of different distracting bus and map problems. (a) An example distracting bus problem.
Figure 9: Three example cooking problems. The con-tents of the fridges (color-coded) are different in differ-ent problems.
Figure 10: Examples of Dream’s learned exploration behavior. (a) Dream learns the optimal explorationbehavior on the distraction variant: riding 3 of the 4 helpful colored buses, which allows it to infer thedestinations of all colored buses and efficiently reach any goal during exploitation episodes. (a) Without theinformation bottleneck, Dream also explores the unhelpful gray buses, since they are part of the problem. Thiswastes exploration steps, and leads to lower returns during exploitation episodes. (c) Dream learns the optimalexploration on the map variant: it goes to read the map revealing all the buses’ destinations, and then ends theepisode, though it unnecessarily rides one of the buses.
Figure 11: Dream’s learned encodings of the exploration trajectory and problems visualized with t-SNE (van derMaaten & Hinton, 2008).
Figure 12:	Dream learns the optimal explo-ration policy, which learns the fridges’ con-tents by going to each fridge and using thepickup action.
Figure 13:	Cooking without goals results. OnlyDream learns the optimal policy, achieving ~2xmore reward than the next best approach.
