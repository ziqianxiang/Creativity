Figure 1: The complexity oftransforming the feature Tl to theoutput, i.e. H(Σl , . . . , ΣL).
Figure 2: The negative correlation between the complexity oftransformations H(Σl) andtheentanglementTC(Σl) of DNNswith similar activation rates.
Figure 3: The change of the transformation complexity and the activation rates in traditional DNNswithout skip-connections.
Figure 4: The change of the transformation complexity in residual networks.
Figure 5: The change of the transformationcomplexity along with the increase of the taskcomplexity n.
Figure 6: The change of H(Σl) and I(X; Σl)in VAEs learned on the MNIST dataset and theCIFAR-10 dataset.
Figure 7: The complexity of transformations and the gap between the training loss and the testingloss of the learned minimum-complexity DNNs. The left-most point in each subfigure at λ = 0refers to DNNs learned by only using the task loss.
Figure 8: Decrease of the testing loss along with the increas of the weight of the complexity loss.
Figure 9: The complexity calculated with different values of a. The trend of the change of thecomplexity was consistent when we used different values of α, which enabled fair comparisonsbetween different DNNs.
