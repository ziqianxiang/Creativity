Figure 1: Contrastive training method. The objective is to reconstruct the storyline. Sentencesare presented in their original order. Given an anchor sentence x, the algorithm should identify thecontext sentence x+ out of negative samples x1-, x2-. Sentences are encoded using separate views,which are composed within a pairwise distance matrix. A softmax classifier outputs the probabilityof each sentence pair to share the same context. In practice, we consider two distance matrices giventhe view f or g used to encode the anchor sentence and the target samples. We estimate the finalprobability as the average of the two resulting probabilities.
Figure 2: (left) The sentence is parsed in constituency and the tree is binarized. The applicationof the N-Ary Tree LSTM on the obtained structure is represented. (right) The sentence is parsedin dependency and a Child-Sum Tree LSTM model is recursively applied. This example illustratesthe structural difference between these two views. Dependency parsing is articulated around theverb ”filled”, which is the root node. In constituency, subject and verb are connected through theroot node. The two architectures differ as the N-Ary Tree LSTM is structured as a binary tree anddifferentiate the left and right children while the Child-Sum Tree LSTM might have an arbitrarynumber of unordered nodes.
Figure 3: Projection of the embeddings from the SUBJ task. (left) The DEP, CONST model isused (right) We train a Quick-thought model using scripts from Logeswaran & Lee (2018)7 on theUMBC dataset. Both dimension reductions are performed using the UMAP algorithm. In bothcases, samples appear well separated given their labels.
