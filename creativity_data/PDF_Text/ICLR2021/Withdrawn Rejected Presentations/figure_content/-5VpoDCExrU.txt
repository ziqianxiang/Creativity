Figure 1: Sample telecommunication log file including three log entries. Telecommunication logsare complex and diverse because they involve various devices and software.
Figure 2: Unsupervised anomaly detection Left: t-SNE embedding of our vector representationsfor log sequences in HDFS dataset. Yellow points mark normal log sequences and blue pointsmark abnormal log sequences (based on human expert annotation). The red plus sign marks thecenter of the distribution. Note that abnormal sequences are mostly clustered and are far from thecenter. Center: K-means clustering of our vector representations. Clusters are sorted decreasinglyaccording to their size. Note that abnormal examples are concentrated in the smallest clusters.
Figure 3: Left Masked language modeling loss (cross entropy) during test. Left bar shows test lossusing a model that is not trained to use time. Middle bar and right bar show test loss on a model thatis trained using timestamp. In the middle bar, timestamp for the masked word is also masked. In theright bar, timestamp for the masked bar is revealed. Right: Comparison of different techniques ohHDFS dataset. Our model significantly outperform previous models.
Figure 4: Predicting whether or not an event type will appear within the next t seconds from now.
Figure 5: For better readability we have visualized each log template with a letter. Left: Given aquery log (highlighted blue), we use a language model to predict its probability twice: Once includ-ing and once excluding lj . in both cases, log entries prior to lj are included. Right: Visualizationof a few examples. Query log is highlighted blue. Previous logs are highlighted green according totheir causal score C(lj , li). This visualization helps identify logs that help predict the query log themost.
Figure 6: Left: A sample query window, top-5 matches based on our embedding and top-5 matchesbased on edit distance. We have replaced each log template with a unique letter and a unique color.
