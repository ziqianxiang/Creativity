Figure 1: An example image that is randomly shuffled after being divided into 2 × 2, 4 × 4 and 8 × 8patches respectively.
Figure 2: The leftmost is an image in the ImageNet, the right three are the corresponding images inthe Stylized-ImageNet.
Figure 3: First row: the adversarial examples and the labels predicted by Defective CNNs. Secondrow: the original images and the ground truth labels. Third row: the adversarial examples andthe labels predicted by standard CNNs. Attack method is MIFGSM (Dong et al., 2017) and theperturbation scales are '∞ ∈ {16/255, 32/255}. More details can be found in Appendix B.
Figure 4: Defense performance against additive Gaussian noise. p-Bottom means applying defectiveconvolutional layers with keep probability p to the bottom layers of a standard ResNet-18.
Figure 5: Relationship between success defense rates against adversarial examples generated byPGD and test accuracy with respect to different keep probabilities. Each red star represents a specifickeep probability with its value written near the star.
Figure 6: CIFAR-10 dataset. First row: the adversarial examples generated by defective CNNs andthe predicted labels. Second row: original images. Third row: the adversarial examples generatedby the standard CNN and the predicted labels.
Figure 7: Tiny-ImageNet dataset. First row: the adversarial examples generated by defective CNNsand the predicted labels. Second row: original images. Third row: the adversarial examplesgenerated by the standard CNN and the predicted labels.
Figure 8: The architecture of ResNet-18C.2 RESNET-50Similar to ResNet-18, ResNet-50 (He et al., 2016) contains 5 blocks and each block contains several1 × 1 and 3 × 3 convolutional layers (i.e. Bottlenecks). In our experiment, we apply defectiveconvolutional layers to the 3×3 convolutional layers in the first three “bottom” blocks. The defectivelayers in the 1st block are marked by the red arrows in Figure 9.
Figure 9: The architecture of ResNet-5022Under review as a conference paper at ICLR 2021C.3 DenseNet- 1 2 1DenseNet-121 (Huang et al., 2017) is another popular network architecture in deep learning re-searches. Figure 10 shows the whole structure of DenseNet-121. It contains 5 Dense-Blocks, eachof which contains several 1 × 1 and 3 × 3 convolutional layers. Similar to what we do for ResNet-50,we apply defective convolutional layers to the 3 × 3 convolutional layers in the first three “bottom”blocks. The growth rate is set to 32 in our experiments. vspace-0.1inBottomDense-BlockOFigure 10: The architecture of DenseNet-121C.4 SENET-18SENet (Hu et al., 2017a), a network architecture which won the first place in ImageNet contest 2017,is shown in Figure 11. Note that here we use the pre-activation shortcut version of SENet-18 and weapply defective convolutional layers to the convolutional layers in the first 3 SE-blocks.
Figure 10: The architecture of DenseNet-121C.4 SENET-18SENet (Hu et al., 2017a), a network architecture which won the first place in ImageNet contest 2017,is shown in Figure 11. Note that here we use the pre-activation shortcut version of SENet-18 and weapply defective convolutional layers to the convolutional layers in the first 3 SE-blocks.
Figure 11: The architecture of SENet-18C.5 VGG-19VGG-19 (Simonyan & Zisserman, 2014) is a typical neural network architecture with sixteen 3 × 3convolutional layers and three fully-connected layers. We slightly modified the architecture byreplacing the final 3 fully connected layers with 1 fully connected layer as is suggested by recentarchitectures. Figure 12 shows the whole structure of VGG-19. We apply defective convolutionallayers on the first four 3 × 3 convolutional layers.
Figure 12: The architecture of VGG-1923Under review as a conference paper at ICLR 2021C.6 WideResNet-32Based on residual networks, Zagoruyko & Komodakis (2016) proposed a wide version of residualnetworks which have much more channels. In our experiments, we adopt the network with a widthfactor of 4 and apply defective layers on the 0th and 1st blocks. Figure 13 shows the whole structureof WideResNet-32.
Figure 13: The architecture of WideResNet-32D Training Details on CIFAR-10 and MNISTTo guarantee our experiments are reproducible, here we present more details on the training processin our experiments. When training models on CIFAR-10, we first subtract per-pixel mean. Then weapply a zero-padding of width 4, a random horizontal flip and a random crop of size 32 × 32 ontrain data. No other data augmentation method is used. We apply SGD with momentum parameter0.9, weight decay parameter 5 × 10-4 and mini-batch size 128 to train on the data for 350 epochs.
Figure 14: Train and test curve of standard and defective ResNet-18 on CIFAR-10 and MNISTMNISTE	Attack approachesIn this subsection, we describe the attack approaches used in our experiments. We first give anoverview of how to attack a neural network in mathematical notations. Let x be the input to theneural network and fθ be the function which represents the neural network with parameter θ . Theoutput label of the network to the input can be computed as c = arg maxi fθ (x)i. In order toperform an adversarial attack, we add a small perturbation δx to the original image and get an24Under review as a conference paper at ICLR 2021adversarial image xadv = x + δx . The new input xadv should look visually similar to the originalx. Here We use the commonly used '∞-norm metric to measure similarity, i.e., We require that∣∣δχ∣∣ ≤ e. The attack is considered successful if the predicted label of the perturbed image Cadv =arg maxi fθ(xadv)i is different from c.
