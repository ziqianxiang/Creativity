Figure 1: (a) Empirical generalization gaps when varying the matrix norm regularization coefficientμ in (9). Consistent with the theoretical results, the gap reduces as μ increases for every e value usedfor training. (b) Comparison of test accuracy of neural networks trained with different coefficients λand μ under weight PGD attack (200 steps) with e perturbation level. AUC refers to the area undercurve score. Joint training with the two theory-driven terms as described in (9) indeed yields moregeneralizable and robust neural networks against weight perturbations.
Figure 2: Empirical generalization gaps when altering the matrix norm regularization coefficient μin (9).
Figure 3: Comparison of test accuracy of neural networks trained with different coefficients λ andμ against weight PGD attack (100 steps) with perturbation level e.
Figure 4: Comparison of test accuracy (%) trained With different coefficients λ and μ against weightPGD attack (200 steps) with = 0.01 and 0.02. The experiment setting follows Figure 1(b).
Figure 5: Test accuracy under different weight perturbation level with 200 attack steps. The modelsare trained using the alternative loss described in Section D.4.
Figure 6: Statistics associated with generalization bounds for standard model and robust modeltrained using equation (9). The experiment setting follows Figure 1(a). In the first column, wepresent test error under weight PGD attack with perturbation level . The curve with = 0 (noweight perturbation) corresponds to the standard generalization setting. In the second column, wepresent the product of spectral norms of the weights matrices, related to bounds in (Bartlett et al.,2017). In the third column, we show spectral norm of the product of the weight matrices, related tobounds in (Barron & Klusowski, 2018). In the fourth column, we show the product of spectral normsof the weights matrices divided by the spectral norm of the product of the weight matrices. Notably,we present each value into the logarithm function. For the standard model, both types of boundincrease with respect to training set size and are shown to be vacuous, consistent with the resultsin (Nagarajan & Kolter, 2019). For the robust model, both types of bound exhibit same decreasingtrend as the test error and therefore are non-vacuous. Moreover, these two bounds demonstratesimilar scaling behavior (nearly constant log ratio) in both standard and robust models.
Figure 7: Ablation study of the robust model with (λ = 0.01, μ = 0). The experiment setting isthe same as Figure 6. In the absence of the generalization regularization term in equation (9), thegeneralization bounds still show decreasing trend with the test error.
