Figure 1: The input tensor is divided into a number of line (1D) or plane (2D) slices. Each normal-ization technique slices the input tensor differently and each slice is normalized independently ofthe other slices. SeqNorm sequentially slices and normalizes the tensor as GroupNorm and then asGhostNorm.
Figure 2: Comparison of the loss landscape on MNIST between BatchNorm, GhostNorm, and thebaseline.
Figure 3: Comparison of the loss landscape on CIFAR-10 between BatchNorm, GhostNorm, andthe baseline.
Figure 4:	Python code for GhostNorm in PyTorch.
Figure 5:	Python code for accumulating gradients in PyTorch.
Figure 6: Comparison of the loss landscape on MNIST between the baseline, BatchNorm, andGhostNorm with different GM values. The last figure (bottom right) depicts the misclassificationerror on the testing set during training.
Figure 7: Comparison of the loss landscape on CIFAR-10 between BatChNorm and GhostNormwith different GM values. The last figure (bottom right) depicts the misclassification error on thetesting set during training.
