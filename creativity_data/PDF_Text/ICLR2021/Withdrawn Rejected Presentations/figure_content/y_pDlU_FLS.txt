Figure 1: Learned optimizers outperform well tuned baselines on three different tasks: (a) linearregression, (b) the Rosenbrock function, and (c) training a neural network on the two moons dataset.
Figure 2: Visualizing optimizer behavior with update functions (see §3.2 for details) for differentcommonly used optimization techniques. (a) Gradient descent is a (stateless) linear function, whoseslope is the learning rate. (b) Gradient clipping saturates the update, beyond a threshold. (c) Mo-mentum introduces a vertical offset depending on the accumulated velocity (colors indicate differentvalues of the accumulated momentum). (d) RMSProp changes the slope (effective learning rate) ofthe update (colors denote changes in the state variable, the accumulated squared gradient).
Figure 3: Momentum in learned optimizers. Each column shows the same phenomena, but for opti-mizers trained on different tasks. Top row: Projection of the optimizer state around a convergencepoint (black square). Inset: the total variance of the optimizer states over test problems goes to zeroas the trajectories converge. Middle row: visualization of the update functions (§3.2) along the slowmode of the dynamics (colored lines correspond to arrows in (a)). Along this dimension, the effecton the system is to induce an offset in the update, just as in classical momentum (cf. Fig. 2c). Bot-tom row: Eigenvalues of the linearized optimizer dynamics at the convergence fixed point (blacksquare in (a)) plotted in the complex plane. The eigenvalue magnitudes are momentum timescales,and the color indicates the corresponding learning rate. See §4.1 for details.
Figure 4:	Gradient clipping in a learned optimizer. Top row: The update function computed at theinitial state saturates for large gradient magnitudes. The effect of this is similar to that of gradientclipping (cf. Fig. 2b). Bottom row: the empirical density of encountered gradients for each task(note the different ranges along the x-axes). Depending on the problem, the learned optimizer cantune its update function so that most gradients are in the linear portion of the function, and thus notuse gradient clipping (seen in the linear regression task, left column) or can potentially use more ofthe saturating region (seen in the Rosenbrock task, middle column).
Figure 5:	Learning rate schedules mediated by autonomous dynamics. Top row: Low-dimensionalprojection of the dynamics of the learned optimizer in response to zero gradients (no input). Theseautonomous dynamics allow the system to learn a learning rate schedule (see §4.3). Bottom row:Effective learning rate (measured as the slope of the update function) as a function of iteration duringthe autonomous trajectories in the top row. We only observe a clear learning rate schedule in thelinear regression task (left column), which includes both a warm-up and decay. For context, dashedlines indicate the best (tuned) learning rate for momentum.
Figure 6:	Learning rate adaptation in learned optimizers. Top row: Approximate fixed points (col-ored circles) of the dynamics computed for different gradients reveal an S-curve structure. Middlerow: Update functions (§3.2) computed at different points along the S-curve (corresponding to ar-rows from the top row). The effect of moving towards the edge of the S-curve is to make the updatefunction more shallow (thus have a smaller effective learning rate, cf. Fig. 2d). The effect is similaralong both arms; only one arm is shown for clarity. Bottom row: Summary plot showing the effec-tive learning rate along each arm of the S-curve, for negative (red) and positive (green) gradients.
Figure 7: A learned optimizer that recovers momentum on the linear regression task. (a) Eigenvalues of theJacobian of the optimizer dynamics evaluated at the convergence fixed point. There is a single eigenmode thathas separated from the bulk. (b) Another way of visualizing eigenvalues is by translating them into optimizationparameters (learning rates and momentum timescales), as described in Appendix B. When we do this for thisparticular optimizer, we see that the slow eigenvalue (momentum timescale closest to one) also has a largelearning rate. These specific hyperparameters match the best tuned momentum hyperparametrs for this taskdistribution (gold star). (c) When we extract and run just the dynamics along this single mode (orange dashedline), we see that this reduced optimizer matches the full, nonlinear optimizer (solid line) almost exactly.
Figure 8: Schematic of a learned optimizer.
Figure 9: Performance summary. Each panel shows the meta-objective (average training loss) over64 random test problems for baseline and learned optimizers. Error bars show standard error. Thelearned optimizer has the lowest (best) meta-objective on each task.
Figure 10: Hyperparameter selection for linear regression.
Figure 11: Hyperparameter selection for Rosenbrock.
Figure 12: Hyperparameter selection for training a neural network on two moons data.
