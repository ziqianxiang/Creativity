Figure 1: Proposed neural architecture for multi-source domain adaptation: Multi-source Information-regularized Adaptation Network (MIAN). Multi-source and target domain input data are fed into theencoder. We denote arbitrary source domains as Si and Sj without loss of generality. The domaindiscriminator outputs a logit vector, where each dimension corresponds to each domain. CNN andFCN refers to convolutional neural networks and fully connected neural networks, respectively.
Figure 2: (a)~(b): Test accuracies for (a) MNIST-M and (b) SVHN as target domains. (c)~(d):Variance of stochastic gradients after 1000 steps for (c) MNIST-M and (d) SVHN as target domainsin log scale. Less is better.
Figure 3: (a) Proxy A-distance. (b)~(c) Ablation study on the objective of domain discriminator.
Figure 4:	Comparison of existing and proposed MDA models. (a) Existing multiple-discriminatorbased methods align each pairwise source and target domain but may fail by neglecting the domainshift between source domains. It also may suffer from unstable optimization and lack of resource-efficiency. (b) Our proposed model mitigates suggested problems by unifying domain discriminators.
Figure 5:	Network architectures. BN denotes Batch Normalization (Ioffe & Szegedy (2015)) andSVD denotes differentiable SVD in PyTorch for MIAN-γ (Section E)Batch Spectral Penalization (BSP, Chen et al. (2019)), Adversarial Discriminative Domain Adaptation(ADDA, Tzeng et al. (2017)), Maximum Classifier Discrepancy (MCD, Saito et al. (2018)), DeepCocktail Network (DCTN, Xu et al. (2018)), and Moment Matching for Multi-Source DomainAdaptation (M3SDA, Peng et al. (2019)).
Figure 6: (a)〜(b): tSNE visualization (a) before and (b) after adaptation. Representations fromtarget domain (SVHN) are shown in red. Digit Class labels are shown with CorresPonding numbers.
Figure 7: (a): SVD-entropy analysis. (Office-31; Source domain: DSLR) (b): Comparisons betweenBSP and DBSP. (Office-31; DSLR → Amazon)Table 6: Ablation study of decaying batch spectral penalization and annealing information regulariza-tion (Office-31). For accurate assessment of extent to which performance improvement is causedby each strategies, γ is fixed as 0 in Annealing-β, and β is fixed as 0.1 in Decaying-γ. Results fromAnnealing-β are reported in main paper.
