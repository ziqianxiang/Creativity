Figure 1: FORK includes three neural networks, thepolicy network Aφ , the system model Fθ , and the re-ward model Rη .
Figure 2: TD3-FORK versus TD3-MT(二) wo, where r is the moving averageof cumulative reward (per episode), and r0 is a predefined goal, w0 is the initial weight, and (a)10 = a if0 ≤ a ≤ 1, = 0 if a < 0 and = 1 if a > 1. The loss function with adaptive weight becomesL(φ) = E -Qψ (st, Aφ(st)) - wRη(st, Aφ(st)) - wγRη (sSt+1, Aφ (sSt+1)) - wγ2 Qψ (sSt+2, Aφ(sSt+2)) .
Figure 3: The six environment used in our experiments4.2	Implementation DetailsTerminology. step (or time): one operation, e.g. training Actor with a mini-batch; episode: a single-run ofthe environment from the beginning to the end, consisting of many steps; and instance: the entire trainingconsisting of multiple episodes.
Figure 4: Learning curves of six environments. Curves were smoothed uniformly for visual clarity.
Figure 5: Training losses under the two different reward networksA.2 Adaptive Weights versus Fixed WeightsWe compared TD3-FROK with the fixed weights, named as TD3-FORK-F, where the weight is chosen tobe 0.4. TD3-FORK performs the best in four out of the six environments. TD3-FORK-F has a worseperformance than TD3 on Walker2d-v3. We therefore proposed and used the adaptive weight because ofthis observation.
Figure 6: Learning curves of TD3, TD3-FORK-F and TD3-FORK. Curves are smoothed uniformly forvisual clarity.
Figure 7: Learning curves of the six environments. Under each algorithm, Actor and Critic, together, weretrained for 1.5 million steps. Curves are smoothed uniformly for visual clarity.
Figure 8: Learning curves of TD3-FORK, TD3, TD3-FORK-S, TD3-FORK-Q and TD3-FORK-DQ. Curvesare smoothed uniformly for visual clarity.
