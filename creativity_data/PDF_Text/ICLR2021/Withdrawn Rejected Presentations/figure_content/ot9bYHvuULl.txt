Figure 1: (a) and (b) are visualizations of projections for the ASWD and the SWD between two2-dimensional Gaussians. (c) and (d) are distance histograms for the ASWD and the SWD between two100-dimensional Gaussians. Figure 1(a) shows that the injective neural network embedded in the ASWDlearns data patterns (in the X-Y plane) and produces well-separate projected values (Z-axis) betweendistributions in a random projection direction. The high projection efficiency of the ASWD is evidentin Figure 1(c), as almost all random projection directions in a 100-dimensional space lead to significantdistances between 1-dimensional projections. In contrast, random linear mappings in the SWD oftenproduce closer 1-d projections (Z-axis) (Figure 1(b)); as a result, a large percentage of random projectiondirections in the 100-d space result in trivially small distances (Figure 1(d)), leading to a low projectionefficiency in high-dimensional spaces.
Figure 2: The first and third columns are target distributions. The second and fourth columns are log2-Wasserstein distances between the target distribution and the source distribution. The horizontal axisshow the number of training iterations. Solid lines and shaded areas represent the average values and 95%confidence intervals of log 2-Wasserstein distances over 50 runs. A more extensive set of experimentalresults can be found in Appendix F.1.
Figure 3: FID scores of generative models trained with different metrics on CIFAR10 and CELEBAdatasets with L= 1000 projections. The error bar represents the standard deviation of the FID scores atthe specified training epoch among 10 simulation runs.
