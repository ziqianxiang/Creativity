Figure 1: Overview of the proposed general radiance field.
Figure 2: The CNN module to extract pixelfeatures.
Figure 3: Qualitative results of SRNs and our GRF for 50-view reconstruction of chairs and cars.
Figure 4: Qualitative results of SRNs (without retraining) and our GRF for 50-view reconstructionof unseen cars. SRNs fails to recover faithful shapes for unseen cars without retraining.
Figure 5: Qualitative results of our GRF for novel scene generalization.
Figure 6: Qualitative results on SynthetiC-NeRF (1st row) and real-world datasets (2nd row).
Figure 7: Visualization of Max Attention Map from Multiple Input Images of a Novel Object forInferring a Novel View.
Figure 8: Qualitative results of our GRF when being fed with a variable number of views of a novelobject. The red circle highlights that the tail of the car is able to be recovered given more visual cuesfrom more input images.
Figure 9: Qualitative results of SRNs and our GRF for 50-VieW reconstruction of chairs and cars.
Figure 10:	Qualitative results of our GRF for novel VieW depth and RGB estimation on the SynthetiC-NeRF dataset.
Figure 11:	Qualitative results of our GRF for novel view depth and RGB estimation on the SynthetiC-NeRF dataset.
Figure 12:	Qualitative results of our GRF for novel view depth and RGB estimation on the SynthetiC-NeRF dataset.
Figure 13:	Qualitative results of our GRF for novel view depth and RGB estimation on the SynthetiC-NeRF dataset.
Figure 14:	Qualitative results of our GRF for novel view depth and RGB estimation on the real-worlddataset.
Figure 15:	Qualitative results of our GRF for novel view depth and RGB estimation on the real-worlddataset.
Figure 16:	Qualitative results of our GRF for novel view depth and RGB estimation on the real-worlddataset.
Figure 17:	Qualitative results of our GRF for novel view depth and RGB estimation on the real-worlddataset.
