Figure 1: Schematic overview comparing our proposed method with existing alternatives and chan-nel pruning. Channel pruning has a fundamentally different goal compared to ours, i.e., trainingslimmable nets. PareCO jointly optimizes both the architectures and the shared weights.
Figure 2: Comparisons among PareCO, Slim, and TwoStage. C10 and C100 denote CIFAR-10/100.
Figure 3: A latency-vs.-error	Figure 4: Prediction error vs. inference memory footprint forview of Figure 2m.	MobileNetV2 and ResNet-18 on ImageNet.
Figure 5: Ablation study for binary search and the number of gradient descent updates per fulliteration using ResNet20 and CIFAR-100. Experiments are conducted three times and we plot themean and standard deviation.
Figure 6: Comparing the width-multipliers between PareCO and Slim. The title for each plot denotesthe relative differences (PareCO - Slim) and the numbers in the parenthesis are for PareCO.
