Figure 1: Comparison of LSGAN and QRGAN generator loss. LSGAN generator minimizes thedifference of the means of distributions. QRGAN generator minimizes the difference of thequantile values of distributions.
Figure 2: (Top) Ring-8 experiment result. (Bottom) Grid-25 experiment result.
Figure 3: Contour plots of discriminator output spaceMode collapse occurred on NSGAN and LSGAN in both Ring-8 and Grid-25 experiments. How-ever, mode collapse is not observed on our proposed method (QRGAN) experiments. Mode collapseoccurs by various reasons. One of the most decisive reason is local minima in the space created bydiscriminator. To analyze the discriminator output space at a situation which discriminator over-whelms generator, we trained discriminator by various GAN methods with dummy generator whichoutputs samples from uniform distribution. The color is the discriminator output value (higher ismore realistic). Blue dots are dummy generator output, sampled from uniform distribution. Wepick some blue dots and draw red arrows (force) by normalized computed gradients by generatorloss. The contour plots of discriminator ourput spaces are shown in Fig. 3. As we can see from thecontours, very steep slope appears only near real samples and very gentle slope appear for samplesoutside. Thus, gradient norm is very small (the arrow length is short). Most gradient directionsare not desirable for fake samples. Looking at the colors, discriminator guides proper Wassersteindistance minimization. Slopes are not too gentle fake samples. Red arrows consistently head toreal samples. Like LSGAN case, gradients are too gentle for fake samples and gradient directionsare decided by noise, thus not desirable. Generator is likely to suffer from local minima. In com-mon, discriminator is modeled to predict probability. For the optimal discriminator, there are nooutput difference between real samples. This behavior is same to fake samples, too. In real, noisemakes difference, thus gradient directions are noisy. Then, generator is not trained to make realisticsamples. This works very similarly for LSGANs and bounded (which use finite targets) QRGANs,
Figure 4: CIFAR-10 generation experiment results7Under review as a conference paper at ICLR 2021(a) QRGAN compared to NSGAN and LSGAN(b) QRGAN compared to WGAN-GPFigure 5: LSUN-Bedroom generation experiment results(a) QRGAN compared to NSGAN and LSGAN(b) QRGAN compared to WGAN-GPFigure 6: Cats generation experiment resultsTable 1: The minimum FID summaryMinimum FID	CIFAR-10	LSUN-Bedroom	CatsNSGAN	34:16-	28:29-	11.37LSGAN	34.53	30.39	13.68WGAN-GP	30.36	19.76	229.8QRGAN	30.93	22.45	10.464 RemarksWe presented the proposed QRGAN, a new method to train GANs that minimize 1-Wassersteindistance between real data distribution and fake data distribution using quantile regression. Bymodeling a discriminator to predict quantiles, more details could be learnt. We explained how modecollapse is created, and what is the desired characteristics of discriminator output space. We showed
Figure 5: LSUN-Bedroom generation experiment results(a) QRGAN compared to NSGAN and LSGAN(b) QRGAN compared to WGAN-GPFigure 6: Cats generation experiment resultsTable 1: The minimum FID summaryMinimum FID	CIFAR-10	LSUN-Bedroom	CatsNSGAN	34:16-	28:29-	11.37LSGAN	34.53	30.39	13.68WGAN-GP	30.36	19.76	229.8QRGAN	30.93	22.45	10.464 RemarksWe presented the proposed QRGAN, a new method to train GANs that minimize 1-Wassersteindistance between real data distribution and fake data distribution using quantile regression. Bymodeling a discriminator to predict quantiles, more details could be learnt. We explained how modecollapse is created, and what is the desired characteristics of discriminator output space. We showedthat LSGAN discriminator creates sharp local minima, thus it can get stuck in mode collapse. On thecontrary, WGAN-GP did not create any sharp local minima. We found an appropriate regularizationmethod from this analysis. The result has shown improved generation quality than previous state ofthe art GAN methods.
Figure 6: Cats generation experiment resultsTable 1: The minimum FID summaryMinimum FID	CIFAR-10	LSUN-Bedroom	CatsNSGAN	34:16-	28:29-	11.37LSGAN	34.53	30.39	13.68WGAN-GP	30.36	19.76	229.8QRGAN	30.93	22.45	10.464 RemarksWe presented the proposed QRGAN, a new method to train GANs that minimize 1-Wassersteindistance between real data distribution and fake data distribution using quantile regression. Bymodeling a discriminator to predict quantiles, more details could be learnt. We explained how modecollapse is created, and what is the desired characteristics of discriminator output space. We showedthat LSGAN discriminator creates sharp local minima, thus it can get stuck in mode collapse. On thecontrary, WGAN-GP did not create any sharp local minima. We found an appropriate regularizationmethod from this analysis. The result has shown improved generation quality than previous state ofthe art GAN methods.
Figure 7: The MLP-based model architecture used for MoG experiments11Under review as a conference paper at ICLR 2021A.2 CIFAR- 1 0 ArchitectureGeneratorCoπvT(ln-dlm=100, OLrt_dlm=256, kemel_8lze=4, StrWe=1, PaddlngW)BatehNonnReLUDiscriminatorFigure 8: Architecture for CIFAR-1012Under review as a conference paper at ICLR 2021A.3 Checkerboard artifacts and resize-convolutionFigure 9: Checkerboard artifacts. (Top) Checkerboard artifacts has caused because kernel size isnot divisible by stride. (Bottom) No checkerboard artifacts appeared by using resize-convolution.
Figure 8: Architecture for CIFAR-1012Under review as a conference paper at ICLR 2021A.3 Checkerboard artifacts and resize-convolutionFigure 9: Checkerboard artifacts. (Top) Checkerboard artifacts has caused because kernel size isnot divisible by stride. (Bottom) No checkerboard artifacts appeared by using resize-convolution.
Figure 9: Checkerboard artifacts. (Top) Checkerboard artifacts has caused because kernel size isnot divisible by stride. (Bottom) No checkerboard artifacts appeared by using resize-convolution.
Figure 10: Discriminant model vs generative modelThere are many attempts to train neural-network-based generative models like Variational Autoen-coders (VAE), Generative Adversarial Networks (GANs), and Adversarial Autoencoders. Kingma& Welling (2014) Kingma & Welling (2014) Makhzani et al. (2016)B.2	The Kullback-Leibler (KL) divergenceThe Kullback-Leibler (KL) divergence is defined as follows,DKL(P|Q) = p(x)logdx(11)where both P and Q are assumed to be continuous, is an integral probability metric (IPM) to measurediversity between P and Q. The KL divergence can be infinite when there are points where P (x) =0 and Q(x) > 0. In that case, gradients calculated by KL divergence tends to be zero, then traininggets slow.
Figure 11: 1-Wasserstein distance is also called Earth Mover’s distanceB.4	Variational AutoencoderB.4.1	AutoencoderAutoencoder is a neural network architecture used to learn latent representations in an unsupervisedmanner. It consists of encoder and decoder networks, and usually encoder’s output dimension issmaller than input dimension, called bottleneck.
Figure 12: Autoencoder architectureB.4.2	Variational AutoencoderIn VAEs, they make the encoder to output parameters of a tractable distribution such as normal dis-tribution. By doing so, we can calculate Kullback-Leibler (KL) Divergence of the code distributionand a prior distribution. For given encoder E, decoder D, prior distribution p, the objective is:min JVAE(θ) = 2 (X — D(Z； θ)) + KL(Zkp)(18)where, μ, σ = E(X; θ) and Z 〜N(μ,σI).
Figure 13: Variational Autoencoder architectureIn addition, reparameterization trick is applied for backpropagation. Z = μ + σ.e, where e 〜p. InVAEs, the KL divergence regularization assumes every code distribution to bep even when differentinput is given. This leads to input aliasing to the decoder and worsen blurry outputs.
Figure 14: The Generative Adversarial Networks framework architectureB.5.2	Difficulty of Training GANsFigure 15: (Upper, Unrolled GAN) the GAN training is successful without mode collapse. (Lower,Standard GAN) the GAN training is failed, and this type of GAN failure is called mode collapsethat generator outputs very similar images Metz et al. (2017)GANs are difficult to train because the networks compete each other. Because discriminator is usedfor generator loss calculation, the loss can oscillate and be unstable. In addition, cross entropy lossused in the original GANs often saturates. When failed to train GANs, mode collapse problem oftenoccurs.
Figure 15: (Upper, Unrolled GAN) the GAN training is successful without mode collapse. (Lower,Standard GAN) the GAN training is failed, and this type of GAN failure is called mode collapsethat generator outputs very similar images Metz et al. (2017)GANs are difficult to train because the networks compete each other. Because discriminator is usedfor generator loss calculation, the loss can oscillate and be unstable. In addition, cross entropy lossused in the original GANs often saturates. When failed to train GANs, mode collapse problem oftenoccurs.
Figure 16: Reinforcement learningsequence to find best policy. Q - Learning is learning Q-values iteratively. Q - value is definedas,∞Q(s,a) = E[X γtRt∣s,a]t=0(21)and this can be represented by Bellman equation,Q(s, a) = R + E[γQ(s0, a0)]	(22)where s is current state, a is the action, r is reward, a0 is the next action, and γ is a hyperparametercalled discount factor to discount future rewards exponentially.
Figure 17: C51’s performance compared to human and DQN Bellemare et al. (2017)19Under review as a conference paper at ICLR 2021B.6.3	Quantile Regression Deep Q Network (QR-DQN)QR-DQN Dabney et al. (2017) is proposed to fix the non-convergence problem of C51. The authorshypothesize the probability distribution function (PDF) consists of N dirac delta functions. We canoptimize the PDF by changing the supports of the dirac delta functions. Likewise, the CDF can bealso optimized by quantiles.
Figure 18: The CDF which is represented by 3 Dirac delta functions. Area filled by red representsthe 1-Wasserstein distance. The 1-Wasserstein distance is proportional to the sum of differencebetween quantile values at each quantile fractions. Therefore, quantile regression is the equivalentto 1-Wasserstein distance minimization.
