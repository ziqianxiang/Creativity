Figure 1: Transfer learning setup: (1) Upstream models Pre-training of models from randomlyinitialized weights on the (large) upstream datasets; (2) Model search Either downstream task in-dependent or by running a proxy task, i.e. fixing the weights of all but the last layer and training alinear classifier or deploying a kNN classifier on the downstream dataset; (3) Downstream trainingUnfreezing all the weights, optimizing both the pre-defined ones and a new linear classification layeron the downstream dataset.
Figure 2: Model search methods: (A) Task-agnostic methods do not see the downstream task, pro-ducing the same ranking of models for all possible tasks (e.g., using the highest ImageNet accuracy);(B) Task-aware methods deploy a proxy (e.g., linear evaluation) for each model on user’s dataset,without referring to meta data used during pre-training; (C) Meta-learned task-aware methods de-ploy a proxy for each model on user’s dataset, together with meta data used during pre-training. Forexample, one could use some notion of similarity between tasks, as in Achille et al. (2019).
Figure 3: Task-agnostic strategies. Relative regret (r(m), cf. Section 3) With B = 1 (transparent)and B = 2 (solid) on the ALL, RESNET-50 and EXPERT pools, bearing in mind that there is onlyone task-agnostic model in Expert. By definition, task-agnostic strategies exclude experts yieldinghigh regret on the ResNet-50 and Expert pools, particularly on natural or structured datasets.
Figure 4: Task-aware strategies (linear). Relative regret for B = 1 (transparent) and B = 2 (solid)on the All, ResNet-50, and Expert pools. Compared to task-agnostic strategies, we observeimprovement on natural datasets (except SVHN) and on restricted pools (except dSpr-Loc), dueto its ability to properly choose experts.
Figure 5: Task-agnostic (positive if better) vs Task-aware (linear) (negative if better) for B = 1.
Figure 6: Hybrid linear (positive if better) vs Linear evaluation (negative if better) for B = 2. Weobserve that hybrid linear significantly outperforms linear with the same budget on the All pool.
Figure 7: Optimal picks as a function of the computational budget. The number of picked models(relative) with zero regret across three representative pools. We note that hybrid linear outperformsall other methods on All, whilst being comparable with the linear strategy on restricted pools wherelinear alone already performs well. Here, the task-agnostic oracle refers to a method which ranksmodels based on their average accuracy across all datasets (more details Section 5.4).
