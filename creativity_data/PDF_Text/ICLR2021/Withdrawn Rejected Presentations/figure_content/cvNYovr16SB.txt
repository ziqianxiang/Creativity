Figure 1: Unsupervised pre-training for deep RL on DM-Control. After pre-training (e.g. on ImageNet or in Chee-tah reward free environment), the agent fine-tunes the pre-trained representation or initialization to achieve higher task-specific rewards (e.g. let the Cheetah run faster). ImageNetpre-training denotes training MoCo on downsampled Ima-geNet. Count-based pre-training means training RL agentwith only count-based exploration signal. The training detailsare in Appendix Section F.1. The results show none of the twomethods outperforms training from scratch.
Figure 2: Diagram of the proposed method Unsupervised Active Pre-Training: it consists of contrastiverepresentation learning on data collected by the agent (equation (1)) and RL optimization to maximize particlebased entropy (equation (5)). After pre-training, the task-agnostic encoder fθ and the RL policy initializationcan be fine-tuned for different downstream tasks to maximize task-specific reward.
Figure 3: Evaluation on DeepMind Control suite. Models are pre-trained on Cheetah, Hopper, and Walker, andsubsequently fine-tuned on respective downstream tasks. The ’sparse’ denotes reward is sparse. The scores ofeach environment given in Table 4 (Appendix).
Figure 4: Comparison of fine-tuning representation, fine-tuning both representation and RL agent, and ablatedbaselines. Models are pre-trained on Hopper and subsequently fine-tuned on downstream tasks. The ’sparse’denote reward is sparse. Both variants of APT outperform training from scratch and other baselines.
Figure 5: Comparison between fine-tuning representations learned on single environment and multiple environ-ments. We apply APT to all the three environments.
Figure 6: Comparison of entropy between different state explorationmethods. The results are averaged over 5 random trials. Error bardenotes one standard derivation.
