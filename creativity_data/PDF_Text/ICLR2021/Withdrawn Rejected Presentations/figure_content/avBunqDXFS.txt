Figure 1: Unlike standard replay (scheme A) which requires a substantial memory budget, we explore thepotential of an unlabeled datastream to serve as replay and significantly reduce the required memory budget.
Figure 2: Diagram of DistillMatch, our proposed algorithm for semi-supervised continual learning. We useunlabeled data from the agent’s environment to ground the current model in the past task while simultaneouslylearning new knowledge. This is done with a combination of local distillation, consistency regularization, andpseudo-labeling unlabeled data with confident predictions from an Out-of-Distribution Detector.
Figure 3: Ω (%) VS Coreset Size for Base, with GDand DM (no corset) plotted horizontally. We show baserequires roughly 935 and 338 stored images to matchthe performance of DM and GD, respectfully. This isequivalent to 0.23 and 0.08 stored images per unlabeledimage (calculated using the number of unlabeled imagesper task in this scenario, which is 4,000).
