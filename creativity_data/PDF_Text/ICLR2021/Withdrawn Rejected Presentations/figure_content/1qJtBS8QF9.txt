Figure 1: GVCLN architecture3Under review as a conference paper at ICLR 20213.2	View-Consistency LearningWe consider applying view-consistent learning to the view 1 and view 2 in Figure 1. Inspiredby contrastive learning, Chen et al. (2020) inputs the original sample and the sample after dataaugmentation (flipping or cropping, etc.) into two same views respectively. Since data augmentationdoes not change the label information of sample, the output representations of two views are similar.
Figure 3: all loss of Cora 0.5%Figure 2: accuracy and loss of Cora 0.5%Figure 4: Cora with 5.6% Figure 5: Citeseer with 3.6% Figure 6: PubMed with 0.3%7Under review as a conference paper at ICLR 2021Fig. 2 and Fig. 3 show the accuracy curve and loss function curve of the Cora dataset when the labelrate is 0.5%. It can be seen that adding a pseudo-label when the epoch of training is 200 is helpfulfor the accuracy and loss function decline.
Figure 2: accuracy and loss of Cora 0.5%Figure 4: Cora with 5.6% Figure 5: Citeseer with 3.6% Figure 6: PubMed with 0.3%7Under review as a conference paper at ICLR 2021Fig. 2 and Fig. 3 show the accuracy curve and loss function curve of the Cora dataset when the labelrate is 0.5%. It can be seen that adding a pseudo-label when the epoch of training is 200 is helpfulfor the accuracy and loss function decline.
Figure 4: Cora with 5.6% Figure 5: Citeseer with 3.6% Figure 6: PubMed with 0.3%7Under review as a conference paper at ICLR 2021Fig. 2 and Fig. 3 show the accuracy curve and loss function curve of the Cora dataset when the labelrate is 0.5%. It can be seen that adding a pseudo-label when the epoch of training is 200 is helpfulfor the accuracy and loss function decline.
Figure 7: t-SNE of Cora raw featuresFigure 8: t-SNE of Cora features with filtering5 ConclusionsWe propose a new model, GVCLN, to solve the node classification problem in the case of lowlabel rate. GVCLN adopts a dual-view structure to view-consistent learning. For the two viewers,we use the graph convolutional layer and the graph attention layer, respectively, and finally passthrough a non-linear graph convolutional layer. Because the graph convolutional layer of viewer1 is relatively simple, it can quickly learn to obtain node representations, and then increases therepresentation ability of graph attention layer by supervised the consistency loss function in thelearning. In contrast, the graph attention layer of viewer 2 is more complicated, it can prevent theaddition of pseudo-labels from making the GVCLN unstable, and prevent the pseudo-label errorsamplify step by step. Thus, We propose a view-consistency learning method, and carry out relevantpractices on the task of graph node classification. Good results can be obtained on the three citationdatasets on all label rate and without validation.
Figure 8: t-SNE of Cora features with filtering5 ConclusionsWe propose a new model, GVCLN, to solve the node classification problem in the case of lowlabel rate. GVCLN adopts a dual-view structure to view-consistent learning. For the two viewers,we use the graph convolutional layer and the graph attention layer, respectively, and finally passthrough a non-linear graph convolutional layer. Because the graph convolutional layer of viewer1 is relatively simple, it can quickly learn to obtain node representations, and then increases therepresentation ability of graph attention layer by supervised the consistency loss function in thelearning. In contrast, the graph attention layer of viewer 2 is more complicated, it can prevent theaddition of pseudo-labels from making the GVCLN unstable, and prevent the pseudo-label errorsamplify step by step. Thus, We propose a view-consistency learning method, and carry out relevantpractices on the task of graph node classification. Good results can be obtained on the three citationdatasets on all label rate and without validation.
