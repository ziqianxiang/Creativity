Figure 1: A schematic view of width transfer. N denotes the network to be optimized, D denotes thetraining dataset, NN and D denote the projected network and dataset. P(∙) is a projection functionparameterized by tunable knobs s. A is the width-optimization algorithm and E(∙) is a simpleextrapolation function to match the dimensions of W* to w*.
Figure 2: A schematic view of different width optimization strategies in existing algorithms (left)and a cartoon illustration of the two extrapolation strategies for scaling the optimized width multi-pliers from lower dimensions to higher dimensions (right). Best view in color.
Figure 3: We consider transferring the optimized widths obtained from smaller models and extrap-olate that to the largest model (1.732× wide or 4× deep). We plot the ImageNet top-1 accuracy foruniform baseline, transferred width E(W*), and direct optimization w* (the leftmost points). Onthe x-axis, we plot the width optimization overhead saved by using width transfer.
Figure 4: We compare the two extrapolationstrategies using DMCP for both ResNet18 andMobileNetV2. We can observe that both stack-average-block and stack-last-block perform simi-larly.
Figure 5: We consider transferring the optimized widths obtained from smaller training data con-figurations to the largest training data configuration (320 × 320 input resolution or full data size).
Figure 6: Width transfer with compound projection. By using width transfer, we can achieve 320×width optimization overhead reduction for width optimization while remain competitive comparedto direct optimization for the top-performing algorithm, DMCP.
Figure A1: The average optimized width for ResNet18 and MobileNetV2. They are averaged acrossthe optimized widths in Section 4.1. We plot the mean in solid line with shaded area representingstandard deviation.
Figure A2: PairWiSe cosine similarity between W* and E(W*) for different width optimizationalgorithms and projection strategies. Within each methods (diagonal blocks), w* and E(W*) aregenerally similar.
