Figure 1: Our proposed framework for sense-aware next token1and masked token prediction tasks.
Figure 2: An example of English-Japanese sense-level joint training, which shows two possibleJapanese translations (至艮行 and 岸)of the English word bank. hk,L is a contextual representation ofbank in finance context and ck0,2 is the cluster center for this sense. ca,1, ca,2, cb,1, cb,2 are differentsense cluster centers of the two Japanese translations, among which cb,2 is the closest to hk,L afterdimension reduction through PCA. Our sense level objective (Eq. 6) moves sense clusters for bank(organization) and 至艮行(organization) closer to each other.
Figure 3: Next token and masked token prediction tasks of language models. For simplicity, we onlyshow the forward language model in next token prediction.
Figure 4: We visualize sense vectors of English monolingual model (SaELMo) in a two dimensionalPCA, and show the vectors close to two different sense vectors of word may in (a) and (b).
Figure 5:	We visualize all sense vectors of en-jp bilingual model (Bi-SaELMo) in a two dimensionalPCA, and show the vectors close to two different sense vectors of word bank.
Figure 6:	We visualize sense vectors of English monolingual model (SaELMo) in a two dimensionalPCA, and show the vectors close to two different sense vectors of word us in (a) and (b).
Figure 7:	We visualize all sense vectors of en-de bilingual model (Bi-SaELMo) in a two dimensionalPCA, and show the vectors close to two different sense vectors of word may.
