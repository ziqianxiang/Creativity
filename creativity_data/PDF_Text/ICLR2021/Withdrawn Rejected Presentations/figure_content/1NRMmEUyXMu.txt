Figure 1: MBRL versus L3P (World Model as a Graph). MBRL does step-by-step virtual rolloutswith the world model and quickly diverges from reality when the planning horizon increases. L3Pmodels the world as a graph of sparse multi-step transitions, where the nodes are learned latentlandmarks and the edges are reachability estimates. L3 P succeeds at temporally extended reasoning.
Figure 2: An overview of L3P, which learns a small number of latent landmarks for planning. Themain components of our method are: learning reachability estimates (via Q-learning and regression),learning a latent space (via an auto-encoder with reachability constraints), learning latent landmarks(via clustering in the latent space), graph search on the world model and online planning.
Figure 3: For both Point and Ant, during training, the initialization state distribution and the goalproposal distribution are uniform around the maze. During test time, the agent is asked to traversethe longest path in the maze. The success rate on the test environment is reported in Figure 4. Thisenvironment demonstrates L3Pâ€™s ability to generalize to longer horizon goals during test time.
Figure 4: Test time success rate vs. total number of timesteps, on maze and robotic manipulationenvironments. During test time, new more difficult goals are selected. L3P shows more robustgeneralization much more quickly than other methods. For every environment except PointmMaze,L3P is the only algorithm that consistently solves the task.
Figure 5: Visualizing planning on AntMaze at test time. Read images from upper left to bottom right.
Figure 6: We consider two environments involving a fetch robot, a block, and a box. In Box-aside-PickAndPlace, the fetch must learn to pick and place the block while avoiding collision with the box.
