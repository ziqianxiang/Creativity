Figure 1: Video Texture Synthesis. Prior video prediction (Xu et al., 2020) and generation (Vondrick et al.,2016; Lee et al., 2019; Mallya et al., 2020) fail to generate long sequences and high resolution images. Classicvideo textures (SChodl et al., 2000) (middle) can generate infinite sequences by resampling frames, but usesfixed representations which are not robust to varying domains. Our method (right) learns a representation andnon-parametric method for infinite video generation based on resampling frames from an input video.
Figure 2: Contrastive Video Textures: We extract overlapping segments from the video and fit a bi-grammodel trained using NCE loss which learns representations for query/target pairs such that given a querysegment Vi, φ(Vi) is similar to positive segment ψ(Vi+1) and dissimilar to negative segment ψ(Vj) wherej ∈ [1, ...N] and j 6= i, i + 1.Video Texture Synthesis. During inference, we start with a random segmentVt [1], compute φ(Vt) and ψ(Vj) and calculate the edge weights as similarity between φ(Vt) and ψ(Vj). Werandomly traverse to (purple arrow) one of the edges that has high probability to reach [2]. We denote higherweight edges in green and lower weighted edges in red and the thickness correlates with the probability. Noedge indicates zero similarity. The randomly chosen segment [2] is appended to the output as Vt+1 and theprocess is repeated (shown by the orange arrow) with [2] as the query.
Figure 3: To play the videos, please view in Acrobat Reader and click on the figure. Videos are also includedat this [website]. Qualitative comparison of our method (Contrastive) to the baselines Classic, Classic+ andClassic++ are shown. The red bar at the bottom indicates the part of the original video being played. Theresults from all 3 baselines are choppy. Our method selects good transitions in the video as can be seen by thered bar moving but the transition is seamless.
Figure 4: Qualitative comparison of audio conditioned video textures synthesized by Classic, Random, VisualRhythm and Beat (VRB) and our Contrastive model. (A) The conditioning audio waveform shows a gap inthe audio where no music is being played. Our model is able to pick up on that and the corresponding videothat is synthesized has hands in the air and no strumming. However both Random and Classic+Audio showstrumming, which is not in sync with the audio. The result using VRB shows the person talking. (B) Theconditioning audio waveform has the same chord repeated twice. The video synthesized by our model reflectsthis, and we observe the same frames (1 and 2) repeated again. Classic+Audio plays the note just once andRandom plays a different note. VRB result contains a region without audio where the person isn’t playinganything, which is out of sync with the conditioning audio.
Figure 5: Left. Transition probability matrix for two different videos (in each row) for both classic andcontrastive methods. Right. Number of transitions vs Sigma for Classic and Number of transitions vsTemperature for Contrastive.
Figure 6: The figure shows frames from two different videos synthesized by our method. Red bar indicatesposition of the original video being played. The transition happens at the third frame and is seamless in bothcases. The first is a forward jump and the second is a backward jump.
