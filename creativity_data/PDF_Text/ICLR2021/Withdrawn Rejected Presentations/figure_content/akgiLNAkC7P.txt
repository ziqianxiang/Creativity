Figure 1: The TwoBridges environment. (a) The expert avoids the lion and takes the upper bridge.
Figure 2: The environments used in the experiments. Note that nominal agents are not aware of theconstraints shown.
Figure 3: Performance of agents during training on their respective constrained (true) environments.
Figure 4: Walker2din the nominal environment. (For the LapGridWorld this is the number of times the agent attemptsto move in the anti-clockwise direction.) For the Walker2d experiment, we observed 0 constraintviolations throughout training. This is because, in practice, ζθ usually acts conservatively comparedto the true constraint function (by also constraining state-action pairs that are close to the true con-strained ones). Additional details on these experiments, including hyperparameters, can be found inAppendix A.4. As can be seen, over the course of training, the agent’s true reward increases and itsnumber of constraint violations go down.
Figure 5: Ablation study results. All plots were smoothed and (except in (c)) averaged over 3 seeds.
Figure 6: Heatmap of ζθ for each of the four possible actions in TwoBridges (in clockwise directionfrom top-left): right, left, down, up.
Figure 7: Performance of agent during training on the constrained (true) environment for differentvalues for α.
