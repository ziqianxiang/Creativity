Figure 1: Illustration of the proposed Counterfactual Self-Training (CST) framework. There aretwo sales records (observational data) shown in the table, i.e., , Customer A was offered $2 andbought an item; Customer B was offered $1 and did not buy. The question marks in the tablesrepresent the counterfactual outcome which we do not observe. For all these unseen counterfactualoutcomes, pseudo-labels which are colored in red in the tables are imputed by a model and areused to augment the observational data. The model is subsequently updated by training on both theimputed counterfactual data and the factual data. This iterative training procedure continues until itconverges.
Figure 7: Loss Curves for Synthetic DatasetsFigure 11: Loss Curves for Multi-Label DatasetsNX X -(r^i logfθ(Xi,p) + (1- r^i)log(1- fθ(Xi,P)))	(12)i=1 p∈P∖Piri,p 〜Bern(fθ(x%p)),∀i,pSince r ∈ {0,1}, by taking expectation over r,NEr XX -(* logfθ(Xi，。)+。— 3)log(1 - fθ(Xi,p)))	(13)i=1 p∈P∖piN= X X - fθ(Xi,p) logfθ(Xi,p) + (1 - fθ(Xi,p)) log(1 - fθ(Xi,p))	(14)i=1 p∈P∖piwhich equals to the entropy term defined on fθ , thus Our CSI-RI framework can be viewed as avariant of entropy regularization in semi-supervised learning (Grandvalet & Bengio, 2005). Sincewe aim to simulate a randomized trail, the hyper-parameter in Grandvalet & Bengio (2005) is setto 1 in CST. Instead of taking the argmax imputation which is commonly used in classification EM(CEM) (Amini & Gallinari, 2002) that minimizes the objective, we impute a randomly assigned13Under review as a conference paper at ICLR 2021
Figure 11: Loss Curves for Multi-Label DatasetsNX X -(r^i logfθ(Xi,p) + (1- r^i)log(1- fθ(Xi,P)))	(12)i=1 p∈P∖Piri,p 〜Bern(fθ(x%p)),∀i,pSince r ∈ {0,1}, by taking expectation over r,NEr XX -(* logfθ(Xi，。)+。— 3)log(1 - fθ(Xi,p)))	(13)i=1 p∈P∖piN= X X - fθ(Xi,p) logfθ(Xi,p) + (1 - fθ(Xi,p)) log(1 - fθ(Xi,p))	(14)i=1 p∈P∖piwhich equals to the entropy term defined on fθ , thus Our CSI-RI framework can be viewed as avariant of entropy regularization in semi-supervised learning (Grandvalet & Bengio, 2005). Sincewe aim to simulate a randomized trail, the hyper-parameter in Grandvalet & Bengio (2005) is setto 1 in CST. Instead of taking the argmax imputation which is commonly used in classification EM(CEM) (Amini & Gallinari, 2002) that minimizes the objective, we impute a randomly assigned13Under review as a conference paper at ICLR 2021label with a larger probability to be the CEM solution. This step is very similar to deterministic
