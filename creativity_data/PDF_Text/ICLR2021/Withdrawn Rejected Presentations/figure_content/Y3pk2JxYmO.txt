Figure 1: Demonstration of our partial back-propagation framework during training. The networkin the figure has 4 blocks. For each block, we introduce additional projection head and contrastiveloss. In this optimization step, we randomly choose to start back-propagation from the contrastiveloss of the second block.
Figure 2: Demonstration of hard pair selection when M = 2. From all the four pairs, the pair withthe highest contrastive loss is selected and passed to the following blocks. The procedure introduceslittle computational overhead except for a forward pass through the projection head. In comparison,a smaller feature map significantly decreases the computational cost of the whole network.
Figure 3: Linear classification on ImageNet. The x-axisshows the training time (days), and the y-axis shows the top-1 accuracy for the linear classifier.
Figure 4: The three update scheme de-scribed in context.
