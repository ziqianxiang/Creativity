Figure 1: An example using the Python front-end of gtn to compute the ASG loss function andgradients. The inputs to the ASG function are all gtn.Graph objects.
Figure 2: The graphs used to construct the ASG sequence criterion. The arc label “p:r/w” denotesan input label p, an output label r and weight w. Graphs with just “p/w” are acceptors.
Figure 3: The primary difference between ASG and CTC is the inclusion of the blank token graph(a) which allows for optional transitions on <b> and results in the CTC alignment graph (b).
Figure 4: Transition graph examples. The n-gram score is w and β is the back-off weight.
Figure 5: An individual sub-word-to-graphemetransducer (a) for the token “th” used to con-struct a lexicon L which is used to make the de-composition graph (b) for the label “the”.
Figure 6: The convolutional transducer with areceptive field size of 3 and a stride of 2. Eachoutput is the forward score of the compositionof a kernel graph with a receptive field graph.
Figure 7: Validation CER with (dashed) and without (solid) marginalization as a function of (a-c)the number of word pieces and (d) the overall sub-sample factor using 1000 word pieces.
Figure 8: The Viterbi word piece decomposition with 1000 tokens for (a) “_know" and (b) “posi-tion”. The decompositions are computed on the training set images prior to cropping the words.
Figure 9: Graphs for the convolutional WFST.
Figure 10: A few individual token graphs used to construct variants of, for example, CTC. Theindividual token graphs are combined to create the overall token graph T =(T + ... + TC )*.
Figure 11: The number of word pieces with occurrences c in the given range. For each datasetwe use 1,000 word piece tokens and count the number of occurrences in the training text usingthe decomposition for each word provided by SentencePiece. The WSJ training text contains 639kwords, the LibriSpeech training text has 990K words and the IAM training text has only 54K words.
