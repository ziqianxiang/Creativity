Figure 1: Excerpt of a toy data set with two decision boundaries (left) and respective robustnesscurves (right). The data is separated perfectly by one smooth boundary (blue robustness curve), andone squiggly boundary (orange robustness curve). We indicate margins around the boundaries atdistances ε and 2ε. Selecting a single perturbation threshold is not sufficient to decide which classifieris more robust.
Figure 2: '∞ robustness curves (left plot) and '2 robustness curves (right plot) resulting from differenttraining methods (indicated by label), optimized for different threat models (indicated by label). Thedashed vertical lines visualize the three point-wise measures from Table 1. The models are trainedand evaluated on the full training-/test sets of CIFAR-10. The curves allow us to reliably comparethe robustness of the classifiers, unbiased by choice of perturbation threshold.
Figure 3: '∞ robustness curves for multiple data sets. Each curve is calculated for a different modeland a different test data set. The data sets are indicated by the labels. The models are trained withMMR + AT, Threat Models: MNIST: '∞(ε = 0.1), FMNIST: '∞(ε = 0.1), GTS: '∞(ε = 4/255),CIFAR-10: '∞(ε = 2/255). The curves for MNIST and FMNIST both show a change in slope,which can not be captured with point-wise measures and could be a sign of overfitting to the specificthreat models for which the classifiers were optimized for.
Figure 4: '∞ robustness curves (left plot) and '2 robustness curves (right plot) resulting from differenttraining methods (indicated by color and label), optimized for different threat models (indicated bylabel). The models are trained and evaluated on the full training-/test sets of CIFAR-10. The curvesallow us to reliably compare the transfer of robustness of the classifiers across distance functions,unbiased by choice of threat model.
Figure 5: Minimum inter-class distances of all data sets considered in this work, measured in '∞(left), `2 (middle), and `1 (right) norm. See Table 2 for size and dimensionality. The shapes of thecurves and the threshold from which any classifier must necessarily trade of between accuracy androbustness differ strongly between data sets.
Figure 6: Example of a data distribution and two linear classifiers such that the `2 robustness curvesintersect, but not the '∞ robustness curves.
Figure 7: Visualization of four images from CIFAR-10 (top row), together with adversarial examples(bottom row), calculated with PGD (Madry et al. 2018) for a model trained with MMR + AT, ThreatModel: '∞(ε = 2/255). The resulting perturbation sizes of the adversarial examples are (from leftto right) 17/255, 18/255, 18/255, 18/255. Even for perturbation sizes far greater than popular choicesof point-wise measures, adversarial examples can be very hard to detect for humans.
Figure 8: '∞ robustness curves for two state-of-the-art robust models with a large architecture(WideResNet-28-10). The labels indicate the training method (Sehwag2020Hydra: (Sehwag etal. 2020), Wu20Adversarial: (Wu et al. 2020)). The trained models are taken from Croce, An-driushchenko, Sehwag, et al. (2020). The models are trained on the full training set of CIFAR-10,and robustness curves are based on a sample of 1000 points from the test set.
