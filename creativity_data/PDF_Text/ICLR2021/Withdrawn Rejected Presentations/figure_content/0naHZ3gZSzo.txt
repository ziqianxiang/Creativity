Figure 1: Comparison of different hyperparameter optimization algorithms for l2-regularized logisticregression sharing the same legend. (a)-(c): Test error. (d)-(f): Suboptimality. (g)-(i): kVf (λ)k2.
Figure 2: CompariSon of different hyperparameter optimization algorithmS for 2-layer CNN, VGG-16 and ReSNet-152 Sharing the Same legend. (a)-(C): TeSt error. (d)-(f): Suboptimality. (g)-(i):∣∣Vf (λ) ∣∣2. (Larger figures can be found in the supplement material.)SpeCifiCally, the training of modern DNN iS uSually an intriguing proCeSS, involving multiple heuriStiChyperparameter schedules, e.g. learning rate with exponential weight decay. Instead of intuitivesettings, we propose to apply epoch-wise learning rates and jointly optimize these hyperparameters.
Figure 3: Comparison of different hyperparameter optimization algorithms for data hyper-cleaningsharing the same legend, where “HP” is the abbreviation of hyperparameters (a)-(b): Suboptimality.
