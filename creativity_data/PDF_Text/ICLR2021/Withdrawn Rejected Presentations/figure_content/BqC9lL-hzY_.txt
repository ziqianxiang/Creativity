Figure 1: We disentangle the raw feature into feature compo- Figure 2: The network for thenents of different complexity orders. We further visualize and disentanglement of reliable featureanalyze the feature components using some generic metrics. components.
Figure 3: Significance of feature componentsdisentangled by different disentangler nets.
Figure 6: Significance (Pcl)) and reliability (P(I)叫 of the disentangled feature components.
Figure 7: (left) The effectiveness of feature componentsaeffective; (right) The significance of feature components beingOverfitted αOVerfit.
Figure 8: Comparisons betweenthe original DnN, the compressedDNN, and the distilled DNN.
Figure 9: Improvements of the classification accuracy based on Φ(l)(x). (left) The accuracy im-provement of different DNNs learned on the CIFAR-10 dataset with l = 7 using different number oftraining samples. (right) The accuracy improvement with different values ofl. Here ResNet-32 waslearned on the CIFAR-10 dataset, and ResNet-34 was learned on the CUB200-2011 dataset and theStanford Dogs dataset, respectively.
Figure 10: The architecture of the disentangler net.
Figure 11: Significance of feature components in DNNs learned for different tasks using disentan-gler nets w/o skip-connections. We simply revised the disentangler nets introduced in Figure 10 byremoving the skip-connections, so as to obtain the stacked disentangler nets.
Figure 12: Significance of feature components using disentangler nets w/o skip connections of dif-ferent widths. It shows that distributions of feature components generated by the simply stackeddisentangler nets were similar to the distributions generated by the residual disentangler nets inFigure 4. This consistency demonstrated the trustworthiness of the metric P⑴.
Figure 13: The network for the disentanglement of reliable feature components.
Figure 14: Visualization of feature Components of different Complexity orders on the CUB200-2011dataset.
Figure 15:	Significance of feature components in DNNs learned on the CUB200-2011 dataset andthe Stanford Dogs dataset.
Figure 16:	Reliability of feature components in DNNs learned on the CUB200-2011 dataset and theStanford Dogs dataset.
Figure 17:	(left) Effectiveness of feature components。!2己也"The top-right sub-figure shows theShapley value 夕fain; (right) Significance of feature components being over-fitted。0?例行「The top-right sub-figure shows the Shapley value 夕overfit.
Figure 18:	Relationship between the feature complexity and the accuracy.
