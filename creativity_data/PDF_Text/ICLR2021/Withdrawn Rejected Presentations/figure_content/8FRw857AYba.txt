Figure 1: (a): The RL part of qd-rl operates over time steps while its QD part operates at thecontroller level, considering the MDP as a black box. (b) One qd-rl iteration consists of threephases: 1) A new population of solutions is sampled from the QD Pareto Front or from the Map-Elites grid. 2) These solutions are mutated by an off-policy RL agent: half of the solutions areoptimized for quality and the other half for diversity. The RL agent leverages one shared critic foreach objective. 3) The newly obtained solutions are evaluated in the environment. Transitions arestored in a replay buffer while final scores and BDs of solutions are stored in the QD archive.
Figure 2: Evaluation environments. Though they may look similar, the state and action spaces inPOINT-MAZE are two-dimensional, whereas they are 29 Ã— 8 in ANT-MAZE.
Figure 3: Learning curves of qd-td3 versus ablations and baselines. In point-maze and ant-trap, the performance is the highest return. In ant-maze, it is minus the highest distance to thegoal. In 3a and 3b, all results are averaged over 5 seeds. In 3c and 3d, curves show the best 2 seedsout of 5 for all algorithms.
Figure 4: Coverage maps of q-td3 (left) and qd-td3 (right) in point-maze and ant-trap.
Figure 5: Gradients maps on point-maze and ant-maze. The black lines represent the mazewalls, the arrows depict gradient fields and the square indicates the maze exit. Both settings presenthighly deceptive gradients: naively following them leads into a wall.
Figure 6: Coverage map of the point-maze environment for all ablations. Each dot corresponds tothe position of an agent at the end of an episode. Dots corresponding to the oldest actors are in bluewhile the newest are in purple.
Figure 7: Coverage map of the ant-trap environment. Each dot corresponds to the position of anagent at the end of an episode. Dots corresponding to the oldest agents are in blue while the newestare in purple.
