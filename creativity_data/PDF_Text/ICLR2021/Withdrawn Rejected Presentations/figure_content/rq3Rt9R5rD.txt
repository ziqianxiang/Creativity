Figure 1: Our image translation network, BalaGAN, is designed to handle imbalanced input do-mains, e.g., a set of dog images that is much richer than that of wolves. We “balance” the domainsby converting them into multiple modes reflecting their “styles” and train a GAN over all mode pairsto learn a multitude of intra- and inter-mode cross-translations. During inference, the network takesa source (e.g., a dog) and a reference image (e.g., a wolf) to produce a new image following the“style/mode” of the reference while resembling the source in a pixel-wise manner.
Figure 2: An illustration of BalaGAN’s architecture.
Figure 3: Applying CycleGAN and BalaGAN on the dog → wolf and woman → men translationtasks, by training with decreasing number of images in the target domain. The numbers above thetable indicate the number of target domain images that were used for training.
Figure 4: Visual (left) and FID(I) (right) results of CycIeGAN and BalaGAN applied on thecar → red-car translation task, by training with decreasing number of images in the target domain.
Figure 5: (a) Results of BalaGAN applied with varying values of k. Below each column we specifythe number of modalities that the translation network was trained on, that is k + 1. (b) FID(1) of ourablation study applied on the dog → wolf translation task with k = 16 which is the number of dogs’breeds. Rows and columns notation are explained in 4.4 (c) Visual results of ablation study.
Figure 6: Various methods applied on AFHQ dataset, which is balanced, to translate dogs to cats.
Figure 7: (a) BalaGAN on the AFD trained on 1000 wolves using 40 modalities. (b) Human percep-tual study in the imbalanced setting. We present the percentage of users that chose the correspondingimage as the preferred one. (c) Users preferences for the AFHQ dataset in a balanced setting. (d)FID (1) of our method applied on AFD in an imbalanced setting. The number of modalities is k +1.
Figure 8: Modalities that are found by clustering the representations obtained by BalaGAN’s en-coder, compared to those that obtained by a VAE.
Figure 9: Modalities that were obtained by BalaGAN for (a) AFHQ dataset and (b) CelebA dataset.
Figure 10: Clusters that were obtained by training the encoder with two different sets of augmenta-tions. Each column corresponds to a cluster.
Figure 11: Applying CycleGAN, MUNITn , CUT and BalaGAN on the dog → wolf translationtask, by training with decreasing number of images in the target domain. The numbers above thetable indicate the number of target domain images that were used for training. As can be seen,BalaGAN achieves the best results for the imbalanced setting. The results of CUT resemble those ofCycleGAN, but the performance decline in CUT is more significant. MUNIT struggles in learningthe varied wolves distribution out of a small target domain.
Figure 12: Applying CycleGAN, MUNITn, CUT and BalaGAN on the women → men translationtask, by training with decreasing number of images in the target domain. The numbers above thetable indicate the number of target domain images that were used for training. As can be seen,BalaGAN is almost agnostic to decrease in the size of the target domain. While MUNIT struggledin learning the varied wolves distribution, here it produces better results since the distribution ofmen’s faces is not as varied.
Figure 13: Results of various methods applied on AFHQ in the balanced setting.
Figure 14: As our translation network is applied in latent space, it is possible to interpolate betweentwo given reference images. This significantly increases the ability of enriching the target domain.
Figure 15: BalaGAN applied on the dogs→wolves translation task. We trained our method over10,000 dogs and 1000 wolves, using 40 modalities.
Figure 16: BalaGAN applied on the balanced dogs→cats translation task. We decomposed both thesource and target domains into 30 modalities.
Figure 17:10,000 womenapplied on thewomen→men translation task. We trained our method overand 1000 men, using 30 modalities.
