Figure 1: Using the rates provided by Corollary 1 leads to linear convergence for (Stochastic) Ada-grad in the noiseless linear regression setting also considered in Xie et al. (2020). (a, b) Noiselesslinear regression on 2000 examples in 20 dimensions. (c, d) Noiseless linear regression on 200examples in 1000 dimensions.
Figure 2: Using the adaptive rate provided by Corollary 1 with L approximated by L(t) =.99 kf W(t))l leads to convergence for Adagrad in the noisy linear regression setting (60 exam-ples in 50 dimensions with uniform noise applied to the labels). (a) 1 hidden layer network withLeaky ReLU activation Xu et al. (2015) and 100 hidden units. (b) 1 hidden layer network withx + sin(x) activation with 100 hidden units. All networks were trained using a single Titan Xp, butcan be trained on a laptop as well.
