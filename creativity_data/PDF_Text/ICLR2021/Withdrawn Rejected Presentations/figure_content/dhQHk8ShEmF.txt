Figure 1: When deploying an image classification system (OOD detector G(x) + image classifier f (x)) in anopen world, there can be multiple types of out-of-distribution examples. We consider a broad family of OODinputs, including (a) Natural OOD, (b) L∞ OOD, (c) corruption OOD, and (d) Compositional OOD. A detaileddescription of these OOD inputs can be found in Section 5.1. In (b-d), a perturbed OOD input (e.g., a perturbedmailbox image) can mislead the OOD detector to classify it as an in-distribution sample. This can trigger thedownstream image classifier f(x) to predict it as one of the in-distribution classes (e.g., speed limit 70). Throughadversarial training with informative outlier mining (ATOM), our method can robustify the decision boundaryof OOD detector G(x), which leads to improved performance across all types of OOD inputs. Solid lines areactual computation flow.
Figure 2: On CIFAR-10, we train a DenseNet with objective (4) for 100 epochs without informative outliermining. At epoch 30, we randomly sample 400,000 data points from Doauutxiliary, and plot the OOD score frequencydistribution (a). We observe that the model quickly converges to solution where OOD score distribution becomesdominated by easy examples with score closer to 1, as shown in (b). Therefore, training on these easy OODdata points can no longer help improve the decision boundary of OOD detector. (c) shows the hardest examplesmined from TinyImages w.r.t CIFAR-10.
Figure 3: An illustration example to explain Why Uχ helps to get a good detector Gr. With Uχ, We canprune away hypotheses Gr for any r ≥ 1.9. Thus, the resulting detector Gr can detect OOD samples from QXsuccessfully and robustly.
Figure 4: Examples of four types of OOD samples.
Figure 5: On CIFAR-10, we train the model with objective (4) for 100 epochs without informative outliermining. For every 10 epochs, we randomly sample 400,000 data points from the large auxiliary OOD datasetand use the current model snapshot to calculate the OOD scores.
