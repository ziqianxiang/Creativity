Figure 1: We plot the average values of EdD [w(s, a)]and (1 - γ)Es0,a0 [f(s0, a0)] output by DualDICE ona task with an identical behavior and target policy, suchthat the true value of both terms is 1. The 10 individualtrials are plotted lightly, with the mean in bold. Theestimates of DualDICE matches our hypothesis thatDualDICE overestimates f (s0 , a0) as propagating up-dates through the MDP occurs at a much slower rate.
Figure 2: Off-policy evaluation results on the continuous-action MuJoCo domain using the “easy” experimentalsetting (500k time steps and σb = 0.133). The shaded area captures one standard deviation across 10 trials.
Figure 3: Off-policy evaluation results on the continuous-action MuJoCo domain using the “hard” experimentalsetting (50k time steps, σb = 0.2, random actions with p = 0.2). The shaded area captures one standarddeviation across 10 trials. This setting uses significantly fewer time steps than the “easy” setting and thebehavior policy is a poor estimate of the target policy. Again, we see SR-DICE outperforms the MIS methods,demonstrating the benefits of our proposed decomposition and simpler optimization. This setting also showsthe benefits of deep RL methods over MIS methods for OPE in high dimensional domains, as deep TD performsthe strongest in every environment.
Figure 4: We plot the log MSE for off-policy evaluation in the image-based Atari domain. The shaded areacaptures one standard deviation across 3 trials. We can see the MIS baselines diverge on this challenging en-vironment, while the remaining methods perform similarly. Perhaps surprisingly, on most games, the naivebaseline of using R(∏b) from the behavior policy outperforms all methods by a fairly significant margin. Al-though the estimates from deep RL methods are stable, they are biased, resulting in a higher MSE.
Figure 5: Ablation study results for the HalfCheetah task. We default to the “hard” setting wherever possible.
Figure 6: Off-policy evaluation results for Pendulum and Reacher. The shaded area captures one standarddeviation across 10 trials. Even on these easier environment, we find that SR-DICE outperforms the baselineMIS methods.
Figure 7: Off-policy evaluation results on HalfCheetah examining the value of differing representations addedto the baseline MIS methods. The experimental setting corresponds to the “hard” setting from the main body.
Figure 8: Off-policy evaluation results on HalfCheetah evaluating the performance benefits from larger networkcapacity on the baseline MIS methods. “Big” refers to the models with an additional hidden layer. The exper-imental setting corresponds to the “hard” setting from the main body. The shaded area captures one standarddeviation across 10 trials. We find that there is no clear performance benefit from increasing network capacity.
Figure 9: Results measuring the log MSE between the estimated density ratio and the ground-truth on a simple5-state MDP domain with three feature sets. The shaded area captures one standard deviation across 10 trials.
Figure 10: The average run time of each off-policy evaluation approach in minutes. Each experiment is run for250k time steps and is averaged over 3 seeds. SR-DICE and Direct-SR pre-train encoder-decoder for 30k timesteps and the deep successor representation 100k time steps.
Figure 11: We plot the log MSE for off-policy evaluation in the image-based Atari domain, using an episode-dependent noisy policy, where e = 0.2 with P = 0.8 and e = 0 with P = 0.2. This episode-dependent selectionensures sufficient state-coverage while using a stochastic policy. The shaded area captures one standard devia-tion across 3 trials. Markers are not placed at every point for visual clarity.
Figure 12: We plot the log MSE for off-policy evaluation in the image-based Atari domain, using a distinctbehavior policy, trained by a separate algorithm, from the target policy. This experiment tests the ability togeneralize to a more off-policy setting. The shaded area captures one standard deviation across 3 trials. Markersare not placed at every point for visual clarity.
