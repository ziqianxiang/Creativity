Figure 1: (a) Examples of non-complex images from ImageNet synthesized by the state-of-the-art BigGAN model (Brock et al., 2019). Although these samples look decent, the complex scenessynthesized by BigGAN (e.g., from the Cityscapes dataset) are blurry and defective in local structure(e.g., cars are blended together) (b). Zoom in for more detail. (c) A complex scene synthesizedby our model respects both local and global structural integrity of the scene. (d) Schematic ofour unconditional Semantic Bottleneck GAN. We progressively train the adversarial segmentationsynthesis network to generate realistic segmentation maps from scratch, then synthesize a photo-realistic image using a conditional image synthesis network. End-to-end coupling of these twocomponents results in state-of-the-art unconditional synthesis of complex scenes.
Figure 2: Schematic of Semantic Bottleneck GAN. Starting from random noise, we synthesize asegmentation layout and use a discriminator to bias the segmentation synthesis network towards re-alistic looking segmentation layouts. The generated layout is then provided as input to a conditionalimage synthesis network to synthesize the final image. A second discriminator is used to bias theconditional image synthesis network towards realistic images paired with real segmentation layouts.
Figure 3: Images synthesized on Cityscapes-5K. Best viewed on screen; zoom in for more detail.
Figure 4: Images synthesized on Cityscapes-25K. Best viewed on screen; zoom in for more detail.
Figure 5: Images synthesized on ADE-Indoor. This dataset is very challenging, causing modecollapse for the BigGAN model (3rd row). In contrast, samples generated by SB-GAN (1st row) aregenerally of higher quality and much more structured than those of ProGAN (2nd row).
Figure 6: Architectural differences between our unconditional semantic bottleneck synthesis net-work and the conditional semantic layout synthesis network in Hong et al. (2018) and Li et al.
Figure 7: Segmentations and their corresponding images synthesized by SB-GAN trained on theCityscapes-25K dataset.
Figure 8: Segmentations and their corresponding images synthesized by SB-GAN trained on theCityscapes-25K dataset.
Figure 9: Segmentations and their corresponding images synthesized by SB-GAN trained on theADE-Indoor dataset.
Figure 10: Segmentations and their corresponding images synthesized by SB-GAN trained on theADE-Indoor dataset.
Figure 12: The effect of fine-tuning (FT) on the baseline setup for ADE-Indoor dataset. Analo-gously to the results on Cityscapes-25K, we observe improvements in both the global structure ofthe segmentations and the performance of semantic image synthesis.
Figure 13: The effect of SB-GAN on improving the performance of the state-of-the-art semanticimage synthesis model (SPADE) on ground truth segmentations of Cityscapes-25K validation set.
Figure 14: The effect of SB-GAN on improving the performance of the state-of-the-art semanticimage synthesis model (SPADE) on ground truth segmentations of ADE-Indoor validation set. ForSB-GAN, we train the entire model end-to-end, extract the trained SPADE sub-network, and syn-thesize samples conditioned on the ground truth labels.
