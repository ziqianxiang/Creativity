Figure 1: Many parents face this task after school ends - who picks up the kid, and who getsgroceries? The pictorial symbols represent propositions, which are true or false depending on thestate of the environment. The arrows in (c) represent subpolicies, and the colors of the arrows matchthe corresponding transition in the FSA. The boxed phone at the beginning of some of the arrowsrepresents how these subpolicies can occur only after the agent receives a phone call.
Figure 2:	Performance on the satisfaction and composability experiments, averaged over all tasks.
Figure 3:	In this environment, the agent must either pick up the kid or go grocery shopping, andthen go home. This is equivalent to the OR task. Starting at S0, the greedy algorithm picks the nextstep through the FSA with the lowest cost (in this case, picking up the kid), which leads to a higheroverall cost. The LOF algorithm finds the optimal path through the FSA.
Figure 4: LOF and RM both require an environment MDP E and an automaton T that specifies atask.
Figure 5: In RM, subpolicies are learned for each state of the automaton. In this case, in state S0, asubpolicy is learned that goes either to the shopping cart of the kid, whichever is closer. In state S1,the subpolicy goes to the house.
Figure 6:	LOF has two steps. In (a) the first step, logical options are learned for each subgoal. In (b)the second step, a metapolicy is found using Logical Value Iteration.
Figure 7:	What distinguishes LOF from RM is that the logical options ofLOF can be easily composedto solve new tasks. In this example, the new task is to go home or pick up the kid, then go groceryshopping. Logical Value Iteration can find a new metapolicy in 10-50 iterations without needing torelearn the options.
Figure 8: FSA for the sequential task. The LTL formula is ♦(a ∧ ♦(b ∧ ♦(c ∧ ♦h))) ∧ !o. Thenatural language interpretation is “Deliver package a, then b, then c, and then return home h. Andalways avoid obstacles o”.
Figure 9: FSA for the IF task. The LTL formula is (♦(c ∧ ♦a) ∧ !can) ∨ (♦a ∧ ♦can) ∧ !o.
Figure 10: FSA for the OR task. The LTL formula is ♦((a ∨ b) ∧ ♦c) ∧ !o. The natural languageinterpretation is “Deliver package a or b, then c, and always avoid obstacles o”.
Figure 11: FSA for the composite task. The LTL formula is (♦((a ∨ b) ∧ ♦(c ∧ ♦h)) ∧ !can) ∨(♦((a ∨ b) ∧ ♦h) ∧ ♦can) ∧ !o. The natural language interpretation is “Deliver package a or b,and then c, unless c gets cancelled, and then return to home h. And always avoid obstacles”.
Figure 12:	All satisfaction experiments on the delivery domain. Notice how for the composite andOR tasks (Figs. 12c and 12d), the Greedy baseline plateaus before LOF-VI and LOF-QL. Thisis because Greedy chooses a suboptimal path through the FSA, whereas LOF-VI and LOF-QLfind an optimal path. Also, notice that RM takes many more training steps to achieve the optimalcumulative reward. This is because for RM, the only reward signal is from reaching the goal state.
Figure 13:	Satisfaction experiments for the reacher domain, without RM results. The results areequivalent to the results on the delivery domain.
Figure 14: Satisfaction experiments for the reach domain, including RM results. RM takes signifi-cantly more training steps to train than the other baselines, although it eventually reaches and sur-passes the cumulative reward of the other baselines. This is because for the continuous domain, weviolate some of the conditions required for optimality when using the Logical Options Framework-in particular, the condition that each subgoal is associated with a single state. In a continuousenvironment, this condition is impossible to meet, and therefore We made the subgoals small spher-ical regions, and we only made the subgoals associated with specific Cartesian coordinates and notvelocities (which are also in the state space). Meanwhile, the optimality conditions of RM are looserand were not violated, which is why it achieves a higher final cumulative reward.
Figure 15: All composability experiments for the delivery domain.
Figure 16: All composability experiments for the reacher domain.
