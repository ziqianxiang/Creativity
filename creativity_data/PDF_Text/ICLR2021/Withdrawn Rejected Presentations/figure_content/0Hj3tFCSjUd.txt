Figure 1: Retrosynthesis and SMILES.
Figure 2: EBM framework for retrosynthesis. Given the product as input, the EBM framework(1) represents it as SMILES or a graph, (2) designs and trains the energy function Eθ, (3) ranks re-actant candidates with the trained energy score Eθ* , and (4) identifies the top K reactant candidates.
Figure 3: Dual model. (a) Learning consists of training three transformers: prior p(X) (green),likelihood p(y|X) (blue), and backward p(X|y) (orange). Dual model penalizes the divergencebetween forward p(X)p(y|X) and backward direction p(y|X) with Dual constraint (highlighted).
Figure 4: Dual ranking improves upon translation proposal. Left and right column are the topthree candidates from translation proposal and dual re-ranking of the proposal. Ground truth (GT) isgiven at the top and is labeled orange in the middle. By dual re-ranking, the GT ranks the first place,whereas the 3rd place in the proposal. Note that the first place in the proposal is only one atomdifferent from GT (Br vs I), indicating the dual model is able to identify small changes in structure.
Figure 5: Dual ranking improves upon translation proposal. Another example. Descriptions seeFig 4A.11 ALTERNATIVE OF SMILES: DEEPSMILES AND SELFIESIn this section, we explore the effect of prepossessing procedure of sequence-based model, e.g. inlinerepresentation of molecular graph, in effecting performance of sequence-based model. In particular,deepSMILES (Dalke, 2018) and SELFIES (Krenn et al., 2019) are alternatives to SMILES. Withoutloss of fairness, we evaluated these representations using Ordered sequential model (Sec 3.1.2)Theresults indicate SMILES work the best. We speculate the reason are deepSMILES and SELFIES areon average longer than SMILES, leading to higher probability of making mistakes on token leveland therefore low sequence-level accuracy.
