Figure 1:	We perturb the intermediate outputs at regular locations in the network using a gaussiannoise of amplitude and measure the effect of these perturbations on the accuracy of the model.
Figure 2:	(a) Computation graphs for Neighbourhood Distillation. Top: the teacher and the studentneighbourhoods receive activations from the root of the teacher network. The student neighbour-hood is trained to reproduce the output of the teacher. Bottom: the teacher and student outputs arepropagated to the head of the teacher network and additional activations between teacher and studentnetworks are compared. The look-ahead loss gives an additional training signal for the student toreproduce the teacher. (b) Example of teacher and student neighbourhoods.
Figure 3: Trade-off curve between number of parameters and final accuracy for ResNetV1-20 andResNetV1-50 models. Models found with Student Search are obtained by recombining units ofdifferent sizes that were distilled independently with Neighbourhood Distillation. Width multipliersrefers to applying a uniform multiplier on all blocks. Bottleneck multipliers refers to applying auniform multiplier on the inner layers of all blocks. Student Search efficiently finds better studentarchitectures than naively applying uniform transformations to the teacher.
Figure 4: Error accumulation caused by perturbing network layers. The bar plot shows the individualaccuracy drop for a network with perturbed weights for one layer. The blue line plot shows thecumulative sum of errors caused by perturbing a given number of layers from left to right. The blackline plot shows the empirical error accumulation evaluated on the test set after perturbing a givennumber of layers. (a) Individual accuracy drop of each layer is < 0.1% and the errors accumulatesub-linearly. (b) Individual accuracy drop of each layer is 〜1% and errors accumulate linearly. (C)Individual accuracy drop of each layer is high (〜2%) and the errors accumulate super-linearly.
Figure 5: Drop in accuracy after perturbing different ResNetV1-50 models trained with GaussianNoise N (0, σ) introduced after each unit. is the amplitude of perturbations introduced at test time.
Figure 6: Impact of applying a multiplier k on the distilled accuracy of different Residual Units. x-yrefers to unit y of block x of the ResNet. For each unit, accuracy remains unchanged as long as themultiplier remains above a certain threshold. This thresholding effect can be observed for differentresidual units, but each unit has a different threshold.
Figure 7: Student Search for Sparsification in ResNetV1-50. Models found with Student Searchare obtained by recombining layers with different sparsity rates that were distilled independentlywith Neighbourhood Distillation. Student Search efficiently finds a better student architectures fordistillation than naively applying uniform sparsity rates.
Figure 8: Student Search with Neighbourhood Distillation. A Teacher Network is broken down intoseveral neighbourhoods. Each Neighbourhood can be used to train several student neighbourhoodsindependently from the rest. Selected student neighbourhoods are then merged back into a singleStudent Network. This allows us to explore a large search space without having to retrain all models.
