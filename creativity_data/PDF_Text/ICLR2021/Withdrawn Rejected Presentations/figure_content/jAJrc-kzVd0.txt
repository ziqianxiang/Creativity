Figure 1: a. Illustration of the “Linear GridWorld” example: there are N grids and 4 actions (north,south, east, west). Reward for entering the goal state (cheese) is 1; reward is 0 elsewhere. b-c.
Figure 2: Results of Q-learning and soft Q-learning in Maze. a-c. surprise (the magnitude of TDerror) v.s. absolute value ofEVB (left), EIV (middle) and PIV (right) in Q-learning. d-f. Theoreticalupper bound and g-i. lower bound v.s. absolute value of EVB, EIV and PIV in soft Q-learning. Thered line is the identity line.
Figure 3: Results of DQN and soft DQN in CartPole. a-c. surprise (the magnitude of the TD error)v.s. absolute value of EVB (left), EIV (middle) and PIV (right) in DQN. d-f. Theoretical upperbound and g-i. lower bound v.s. absolute value of EVB, EIV and PIV in soft DQN. The red line isthe identity line.
Figure 4: Learning curve of soft DQN (blue lines), and soft DQN with prioritized experience replayin term of soft TD error (PER, orange lines) and the theoretical upper bound of value metrics ofexperience (VER, green lines) on nine Atari games.
Figure 5: Maze environment and learning curves.
Figure 6: Illustration on the difference between VER and PER in soft Q-learning. VER uses the theo-retical upper bound as priority (Pmax * |TDsoft |), which balances the TD error and the “on-poIicyness”of the experience. Depicted are the theoretical upper bound (left), |TD| (middle), and the policy term(right) of50 experiences from the replay buffer in the maze (upper panel) and CartPole (lower panel),ordered by the theoretical upper bound.
