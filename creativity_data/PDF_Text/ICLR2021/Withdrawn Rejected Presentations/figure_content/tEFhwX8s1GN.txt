Figure 1: The step size vs. iterationfor nonconvex functions: in a neighbor-hood of the solution, the learning ratecan be a constant value below a thresh-old.
Figure 2: Left: the range of spectral radius where IGD converges, it can be seen there are a widerange t for convergence. Middle: the Log error ∣∣x - X∣∣2. The error line in blue grows slowly afteran initial rapid rise. This implies that in the presence of noise, large t-values can still calculate xclose to x*. Right: calculated solution of a large radius (3.0),the computed values of χ(t) do notsignificantly deviate from the noisy data X within the large t-value.
Figure 3: Different learning rate for the vanilla SGD in the ResNet56 model on CIFAR10 andCIFAR100. Models are trained with learning rate from 0.2 to 1.
Figure 4: The performance of several popular models on CIFAR10 and CIFAR100. Figures fromleft to right are for ResNet56, ResNet110,VGG16 and DenseNet, respectively.
Figure 5: Left: Train error for ResNet56 on ImageNet32; Middle: Top-1 error; Right: Top-5 error.
Figure 7: LSTM on Penn Treebank Dataset. For Fig. (a) and Fig. (b), the left shows Train Perplexityand the right shows Test Perplexity.
Figure 8: Resnet56 on CIFAR-10.
Figure 9: Resnet110 on CIFAR-10.
Figure 10: VGG16 on CIFAR-10.
Figure 11:	DenseNet on CIFAR10.
Figure 12:	Resnet56 on CIFAR100.
Figure 13:	Resnet110 on CIFAR100.
Figure 14: VGG16 on CIFAR100.
Figure 15: DenseNet on CIFAR100.
