Figure 1: Comparing the generaliza-tion and training losses of our proposedpersonalized model with the globalmodels of FedAvg and SCAFFOLD byincreasing the diversity among the dataof clients on MNIST dataset with a lo-gistic regression model.
Figure 2: Comparing the the performance of APFL with FedAvg (APFL with α = 0) and SCAF-FOLD on the MNIST dataset. Top row is the training loss and the bottom row is the generalizationaccuracy on training and validation data, respectively. In (a), the accuracy lines of SCAFFOLD andFedAvg global models are removed since their low values degrade the readability of the plot.
Figure 3: Comparing the APFL with adaptive α and the localized FedAvg. The left figure is thetraining performance, and the right one is the accuracy of these models on local validation data.
Figure 4: Evaluating the effeCt of sampling on APFL and FedAvg algorithm using the MNISTdataset that is non-IID with only 2 Classes per Client with logistiC regression as the loss. The firstrow is training performanCe on the loCal model of FedAvg and personalized model of APFL withdifferent sampling rates from {0.3, 0.5, 0.7}. The seCond row is the generalization performanCeof models on loCal validation data, aggregated over all Clients. It Can be inferred that despite thesampling ratio, APFL Can superbly outperform FedAvg.
Figure 5: The results of applying FedAvg and APFL (with adaptive α) on an MLP model using EM-NIST dataset, which is naturally heterogeneous. APFL achieves the same training loss of localizedFedAVG, while outperforms it in validation accuracy.
Figure 6: Comparing the effect of fine-tuning with the local model of FedAvg and with the personal-ized model of APFL on the synthetic datasets. The model is trained for 100 rounds of communicationwith 97 clients, and then 3 clients will join in fine-tuning the global model based on their own data.
