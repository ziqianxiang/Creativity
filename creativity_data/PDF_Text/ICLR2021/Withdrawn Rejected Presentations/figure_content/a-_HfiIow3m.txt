Figure 1: Schematic overview bi-modal VAEs using a PoE and additional network structures thatare capable of semi-supervised learning without requiring a two step learning procedure. VAEVAE(a) and (b) are by Wu & Goodman (2019), JMVAE is by Suzuki et al. (2017), MVAE is by Wu &Goodman (2018), and SVAE is our newly proposed model. Each triangle stands for an individualneural network, the colors indicate the two different modalities.
Figure 2: MNIST-SVHNreconstruction for fully su-pervised VAEVAE.
Figure 3: MNIST-Split image reconstructions of a top half and a bottom half given (a) the top half;(b) the bottom half of the original image.
Figure 4: MNIST-Split dataset. Accuracy of an oracle network applied to images reconstructedgiven (a) the full image (both halves) (b) the top half (c) the bottom half.
Figure 5: The SVAE and VAEVAE network architectures for 3-modalities case. The number ofparameters is kn2 for SVAE and kn2n-1 for VAEVAE, where n is the number of modalities and kis the number of parameters in one encoding network.
Figure 6: MNIST-Split-3 dataset, reproducing the logic of MNIST-Split for the images splitted inthree parts.
Figure 7: Performance on MNIST-SVHN for different supervision levels. (a) Joint coherence, ashare of generated images with the same digit class; (b) Cross-coherence, accuracy of SVHN recon-structions given MNIST; (c) Cross-coherence, accuracy of MNIST reconstructions given SVHN.
Figure 8: CUB Images-Captions dataset. Performance metrics for different supervision levels. (a)Joint coherence, the correlation between images and labels reconstructed from the randomly sam-pled latent vectors; (b) Cross-coherence, the correlation of the reconstructed caption given the image;(c) Cross-coherence, the correlation of the reconstructed image given the caption.
Figure 9: Examples of image and caption reconstructions given one modality input for SVAE andVAEVAE. Given that the caption can be broad (e.g., ”this bird is black and white and has a longpointy beak” in the example), it can fit many different images. In this case, the image from thecaption reconstruction tends to better fit the description than the original image. The same goesfor images: one of the reconstructed images has a bird with a red belly which got reflected in thegenerated caption even though it was not a part of the original caption.
