Figure 1: Inputs to the edge probing classifier correspond to the model embeddings M(x) and M(y)of concepts x and y, respectively. To account for the different dimensionality of the tested models,M (x) and M (y) are projected into a common lower dimensionality space using a linear layer. Theresulting embeddings x0 and y0 are concatenated into a vector, which is fed into a Multi-LayerPerceptron that is in charge of predicting if the concept pair is related or not.
Figure 2: A reconstructed graph usingMCM over BERT-large. The graphs for allthe models can be found in Appendix B.
Figure 3: Semantic factors with a high (top charts) or low (bottom charts) impact on F1-score, alongwith their 90% confidence intervals. Charts only display ranges where at least 100 samples existed.
Figure 4: F1-score for hypernym prediction across each model layer.
Figure 5:	The Princeton WordNet Gloss Corpus provides sentences with manually annotated map-pings of words to their corresponding WordNet Synset (concept / sense).
Figure 6:	Each triplet is used to create related and unrelated pairs of words according to the relation-ship hypernym of. We create six edge probing pairs, and therefore, the edge probing classifier willneed to identify the pair’s components and the order in which the words were declared in the pair.
Figure 7: Ground Truth Knowledge Graph(a) Word2Vec reconstruction using MCM	(b) Word2Vec reconstruction using TIM(c) GloVe-42B reconstruction using MCM	(d) GloVe-42B reconstruction using TIMFigure 8: Knowledge graph reconstruction using Word2Vec and GloVe.
Figure 8: Knowledge graph reconstruction using Word2Vec and GloVe.
Figure 9: Knowledge graph reconstruction using GPT-2 and T5.
Figure 10: Knowledge graph reconstruction using ELMo, BERT and RoBERTa.
Figure 11: Knowledge graph reconstruction using XLNet and ALBERT.
Figure 12: Relative depth in the WordNet graph: For each synset, we compared F1 with depthscore (0 % for the root and 100 % for leaves) measuring differences between higher/lower levelconcepts. As seen here, NCE-based methods are significantly impacted by this variable.
Figure 13: Graph distance between concepts: We measured the impact of the number of “hops”that separate two tested concepts on pair-wise F1 score. This chart reveals a strong correla-tion of all the models in this aspect. As an example of this phenomenon, closer relations suchas hchihuahua, dogi are, in general, considerably easier to capture than distant relations such ashchihuahua, entityi. For details on how we implement the distance in WordNet, check AppendixA.2.
Figure 14: Concept frequency: In this Figure we evaluate if frequent concepts are easier or harderto capture for these models. The frequency was computed by counting occurrences in the 38 GB ofOpenWebText Corpus (http://Skylion007.github.io/OpenWebTextCorpus). Thechart shows that uncommon and highly frequent concepts are harder to be modeled. In particu-lar, NCE-based models and T5 are more sensitive to this factor.
Figure 15: Number of parent and child nodes: We studied if the number of parents or child nodeshave an impact on F1-scores. The same phenomenons show up when we analyze only direct parentsor children.
Figure 16: Number of Senses and Sense Ranking: We studied if models are impacted by multi-sense concepts such as “period”, and by their sense ranking (how frequent or rare those senses are).
Figure 17: Number of sibling nodes and number of synonyms: We discover that these factors havevery low impact on F1-scores. The fact that siblings are not confused leads to similar conclusionsas the ones provided in Figure 16.
Figure 18: Analysis of F1-score correlations among models. A horizontal line in these charts wouldrepresent perfect independence.
