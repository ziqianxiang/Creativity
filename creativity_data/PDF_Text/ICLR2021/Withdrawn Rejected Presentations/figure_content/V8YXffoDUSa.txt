Figure 1: a A recurrent ResNet implementing a simple error-corrective inverse model. The predic-tion based on the current estimate Z is compared to the input X (Via positive and negative errorsP and n). The error is used to update Z, whereas X is simply retained throughout all blocks. bTrajectories of the estimates Zι, Z2 across blocks in a feedforward network, iterative steps in anthe error-corrective algorithm, residual blocks in a trained ResNet, and residual blocks in a trainedrecurrent ResNet. All four methods converge to the correct estimate (indicated by black ’x’). cDropping out the fourth block (unbroken line) has a minor impact on the ResNet. d If the last blockis iteratively applied to the final estimate, the value diverges for both the residual and the recurrentnetwork (broken lines), indicating that they do not learn a convergent structure.
Figure 2: Iterative convergence in ResNets with standard training. a The different perturbation meth-ods (early read-out for determining convergence, dropping-out blocks for determining recurrence,and additional evaluations of the last block for determining divergence) are illustrated for the threestages of the ResNet. The x axis depicts the residual block targeted by the perturbation and the yaxis the error rate resulting from the corresponding perturbation (chance performance at 90%). Forclarity, one of the six instances is emphasized in the plots. b The resulting index values for eachstage (small translucent dots) and their averages across instances (large dots). c The error rate forthe individual network instances is plotted against Convergence and Divergence Index.
Figure 3: Iterative convergence and performance of coupled ResNets. a Effect of gradient couplingand initialization (rec.: recurrent; non.-rec.: non-recurrent) on indices of iterative convergence forarchitectures with different numbers of channels. b Effect of gradient coupling and initialization onthe performance on CIFAR-10 and Digitclutter-5. c Relationship between performance and iterativeconvergence, i.e., Convergence (left) and Divergence Index (right). Models with the same numberof parameters are visualized by the same color and individual lines. Results in a, c are on CIFAR-10.
Figure 4: Iterative convergence and performance of improperly convergent ResNets. a Effects ofgradient coupling and initialization (rec.: recurrent; non.-rec.: non-recurrent) on iterative conver-gence indices. b Error rates on CIFAR-10 as a function of gradient coupling and initialization.
Figure 5:	This figure depicts the evolution of mean error rate on training (left) and validation data(right) during training. This pertains to 16 channels in the first stage across all combinations ofcoupling parameter and initialization.
Figure 6:	a The indices of iterative convergence as based on accuracy with respect to the predictionsmade by the unperturbed model rather than the ground truth. b,c The evolution of crossentropy andthe Euclidean distance between intermediate representations and the final representation across thenumber of evaluated blocks and additional evaluations.
Figure 7:	This figure plots the effective parameter count of the networks studied in the main textagainst the raw parameter count.
Figure 8: Performance of training variations on CIFAR-10. a The effect of initializing batchnormwith γ = 0.1 instead ofγ = 1. b The effect of using a triangular kernel for gradient coupling insteadof a uniform kernel. c A variation of gradient coupling where the first five blocks in each stage wereuncoupled.
Figure 9: This figure plots the error rate of all models trained on CIFAR-10 against their raw andeffective parameter count.
Figure 10: This figure plots the indices of iterative convergence against the coupling parameters forthe different training variations.
Figure 11: This figures plots the indices of iterative convergence for the properly convergent ResNetstrained on CIFAR-10.
Figure 12: This figures depicts the performance of properly convergent ResNets on CIFAR-10.
Figure 13: a Performance of gradient-coupled ResNets on variations of Digitclutter with a differentnumber of overlapping digits and different size of training data. b Performance of gradient-coupledResNets on CIFAR-100, CIFAR-10 with few training data, and MIST.
