Figure 1: The k-SP constraint improves sample efficiency of RL methods in sparse-reward tasks bypruning out suboptimal trajectories from the trajectory space. Intuitively, the k-SP constraint meansthat when a policy rolls out into trajectories, all of sub-paths of length k is a shortest path (under adistance metric defined in terms of policy, discount factor and transition probability; see Section 3.2for the formal definition). (Left) MDP and a rollout tree are given. (Middle) The paths that satisfythe k-SP constraint. The number of admissible trajectories is drastically reduced. (Right) A pathrolled out by a policy satisfies the k-SP constraint if all sub-paths of length k are shortest paths andhave not received non-zero reward. We use a reachability network to test if a given (sub-)path is ashortest path (See Section 4 for details).
Figure 2: An example observation of (a) FourRooms-11×11, (b) KeyDoors-11×11 in MiniGrid,(c) GoalLarge in DeepMind Lab, and (d) the maze layout (not available to the agent) of GoalLarge.
Figure 3:	Progress of average episode reward on MiniGrid tasks. We report the mean (solid curve)and standard error (shadowed area) of the performance over six random seeds.
Figure 4:	Progress of average episode reward on DeepMind Lab tasks. We report the mean (solidcurve) and standard error (shadowed area) of the performance over four random seeds.
Figure 5: (Left) 7×7 Tabular four-roomsdomain with initial agent location (red) andthe goal location (green). (Right) The trajec-tory space reduction ratio (%) before and af-ter constraining the trajectory space for var-ious k and ∆t with k-SP constraint. Evena small k can greatly reduce the trajectoryspace with a reasonable tolerance ∆t.
Figure 6:	Transition count maps for baselines and SPRL: (a), (b), and (c) are in reward-free (lightgreen) while (d) is in reward-aware (dark green) setting. In reward-free settings (a-c), we showrewarding states in light green only for demonstration purpose. The location of agent’s initial state(orange) and rewarding states (dark green) are fixed. The episode length is limited to 500 steps.
Figure 7:	Average episode reward of SPRL with varying k =1, 3, 10, 30 as a function of environmentsteps for DeepMind Lab tasks. Other hyper-parameters are kept same as the best hyper-parameter.
Figure 8:	Average episode reward of SPRL with varying k =3, 5, 7 as a function of environmentsteps for MiniGrid tasks. Other hyper-parameters are kept same as the best hyper-parameter. Thebest performance is obtained with k = 5.
Figure 9:	Average episode reward of SPRL with varying ∆t =1, 2, 3, 5 as a function of environmentsteps for DeepMind Lab tasks. Other hyper-parameters are kept same as the best hyper-parameter.
Figure 10: Average episode reward of SPRL with varying ∆t =10, 15, 25, 50 as a function ofenvironment steps for MiniGrid tasks. Other hyper-parameters are kept same as the best hyper-parameter. The best performance is obtained with ∆t = 25.
Figure 11: Average episode reward of SPRL with varying observation stacking dimension of 0, 1, 5,10 as a function of environment steps for DeepMind Lab tasks. Other hyper-parameters are kept sameas the best hyper-parameter. The best performance is obtained without stacking (i.e., #stack=0).
Figure 12: Average episode reward of SPRL with varying observation stacking dimension of 0, 1, 5as a function of environment steps for MiniGrid tasks. Other hyper-parameters are kept same as thebest hyper-parameter. The best performance is obtained without stacking (i.e., #stack=0)A.3 S tacking observationThe CMDP with k-SP constraint becomes the (k + 1)-th order MDP as shown in Eq. (8). Thus,in theory, the policy should take currrent state st augmented by stacking the k previous statesas input: [st-k, st-k+ι..., st], where [∙] is a stacks the pixel observation along the channel (i.e.,color) dimension. However, stacking the observation may not lead to the best empirical resultsin practice. Figure 11 and 12 show the performance of SPRL on DeepMind Lab and MiniGriddomains with varying stacking dimension. For stack=m, we stacked the observation from t - m tot: [st-m, st-m+1, . . . , st]. We experimented up to m = k: up to m = 10 for DeepMind Lab andm = 5 for MiniGrid. The result shows that stacking the observation does not necessarily improves theperformance for MDP order greater than 1, which is often observed when the function approximationis used (e.g., Savinov et al. (2018b)). Thus, we did not augment the observation in all the experiments.
Figure 13:	The accuracy of the learned reachability network on (a) FourRooms-7×7 (b) FourRooms-11×11, (c) KeyDoors-7×7 and (d) KeyDoors-11×11 in MiniGrid in terms of environment steps.
Figure 14:	The accuracy of the learned reachability network on (a) GoalSmall (b) ObjectMany, and(c) GoalLarge in DeepMind Lab in terms of environment steps.
Figure 15:	The accuracy of the learned reachability network on (a) FourRooms-7×7 (b) FourRooms-11×11, (c) KeyDoors-7×7 and (d) KeyDoors-11×11 in MiniGrid in terms of environment steps.
Figure 16: The performance of SPRL with and without curriculum learning of RNet on (a)FourRooms-7×7 (b) FourRooms-11×11, (c) KeyDoors-7×7 and (d) KeyDoors-11×11 in MiniGrid.
Figure 17:	The performance of SPRL with and without curriculum learning of RNet on (a) GoalSmall(b) ObjectMany, and (c) GoalLarge in DeepMind Lab in terms of environment steps.
Figure 18:	The accuracy of the learned reachability network with and without curriculum learning ofRNet on (a) GoalSmall (b) ObjectMany, and (c) GoalLarge in DeepMind Lab in terms of environmentsteps.
