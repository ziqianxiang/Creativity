Figure 1: Pure exploration task: Continuous 4-room maze6.1	Pure Exploration ComparisonIn order to see the exploration performance of DAC (α = 0.5) as compared to the SAC baselines,we compare state visitation on a 100 × 100 continuous 4-room maze task. The maze environmentis made by modifying a continuous grid map available at https://github.com/huyaoyu/GridMap, and it is shown in Fig. 1(a). State is (x, y) position in the maze, action is (dx, dy)bounded by [-1, 1], and the agent location after the action becomes (x + dx, y + dy). The agentstarts from the left lower corner (0.5, 0.5) and explores the maze without any reward, and Fig. 1(b)shows the mean number of new state visitations over 30 seeds, where the number of state visitationis obtained for each integer interval. As seen in Fig. 1(b), DAC visited much more states thanSAC/SAC-Div, which means that the exploration performance of DAC is superior to that of the SACbaselines. In addition, Fig. 1(c) shows the corresponding state visit histogram of all seeds. Here,as the color of the state becomes brighter, the state is visited more times. Note that SAC/SAC-Divrarely visit the right upper room even at 500k time steps for all seeds, but DAC starts visiting the6Under review as a conference paper at ICLR 2021right upper room at 5k time steps and frequently visit the right upper room at 500k time steps. Thus,Fig. 1(c) clearly shows that DAC has better sample-efficiency for exploration than SAC/SAC-Div.
Figure 2: Performance comparison: Fixed α case(c) SparseWalker-v1(d) SparseAnt-v1Figure 3: α-skewed JS divergence for DAC and SAC/SAC-DivFixed α case: In order to see the advantage of the sample-aware entropy regularization for rewardedtasks, we compare the performance of DAC with α = 0.5 and the SAC baselines on simple MDPtasks: SparseMujoco tasks. SparseMujoco is a sparse version of Mujoco and the reward is 1 if theagent exceeds the x-axis threshold, otherwise 0 (Hong et al., 2018; Mazoure et al., 2019).
Figure 3: α-skewed JS divergence for DAC and SAC/SAC-DivFixed α case: In order to see the advantage of the sample-aware entropy regularization for rewardedtasks, we compare the performance of DAC with α = 0.5 and the SAC baselines on simple MDPtasks: SparseMujoco tasks. SparseMujoco is a sparse version of Mujoco and the reward is 1 if theagent exceeds the x-axis threshold, otherwise 0 (Hong et al., 2018; Mazoure et al., 2019).
Figure 4: Performance comparison: Adaptive α case(a) Weighting factor α (b) Control coefficient c (c) Entropy coefficient β (d) JS divergenceFigure 5: Averaged learning curve for ablation studyof Rηα, we used regularization for α learning and restricted the range of α as 0.5 ≤ α ≤ 0.99 forα adaptation so that a certain level of entropy regularization is enforced. Here, we consider morecomplicated tasks: HumanoidStandup and delayed Mujoco tasks (DelayedHalfCheetah, Delayed-Hopper, DelayedWalker2d, and DelayedAnt). HumanoidStandup is one of high-action dimensionalMujoco tasks. Delayed Mujoco tasks suggested by (Zheng et al., 2018; Guo et al., 2018) have thesame state-action spaces with original Mujoco tasks but reward is sparsified. That is, rewards for Dtime steps are accumulated and the accumulated sum is delivered to the agent once every D timesteps, so the agent receives no reward during the accumulation time. The performance results aver-aged over 5 random seeds are shown in Fig. 4. The result of the max average return of these Mujocotasks for DAC and SAC/SAC-Div is provided in Table F.2 in Appendix F.1. As seen in Fig. 4, allversions of DAC outperform SAC. Here, SAC-Div also outperforms SAC for several tasks, but theperformance gain by DAC is much higher. In addition, it is seen that the best α depends on thetasks in the fixed α case. For example, α = 0.8 is the best for DelayedHalfCheetah, but α = 0.5 isthe best for DelayedAnt. Thus, we need to adapt α for each task. Finally, DAC with α-adaptationhas the top-level performance for most tasks and the best performance for HumanoidStandup andDelayedHopper tasks. Further consideration for α is provided in Section 6.3.
Figure 5: Averaged learning curve for ablation studyof Rηα, we used regularization for α learning and restricted the range of α as 0.5 ≤ α ≤ 0.99 forα adaptation so that a certain level of entropy regularization is enforced. Here, we consider morecomplicated tasks: HumanoidStandup and delayed Mujoco tasks (DelayedHalfCheetah, Delayed-Hopper, DelayedWalker2d, and DelayedAnt). HumanoidStandup is one of high-action dimensionalMujoco tasks. Delayed Mujoco tasks suggested by (Zheng et al., 2018; Guo et al., 2018) have thesame state-action spaces with original Mujoco tasks but reward is sparsified. That is, rewards for Dtime steps are accumulated and the accumulated sum is delivered to the agent once every D timesteps, so the agent receives no reward during the accumulation time. The performance results aver-aged over 5 random seeds are shown in Fig. 4. The result of the max average return of these Mujocotasks for DAC and SAC/SAC-Div is provided in Table F.2 in Appendix F.1. As seen in Fig. 4, allversions of DAC outperform SAC. Here, SAC-Div also outperforms SAC for several tasks, but theperformance gain by DAC is much higher. In addition, it is seen that the best α depends on thetasks in the fixed α case. For example, α = 0.8 is the best for DelayedHalfCheetah, but α = 0.5 isthe best for DelayedAnt. Thus, we need to adapt α for each task. Finally, DAC with α-adaptationhas the top-level performance for most tasks and the best performance for HumanoidStandup andDelayedHopper tasks. Further consideration for α is provided in Section 6.3.
