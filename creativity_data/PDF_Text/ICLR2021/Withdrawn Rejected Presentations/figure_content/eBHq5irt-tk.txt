Figure 1: A resolution of double descent. We replicate the double descent behaviour of deep neuralnetworks using a ResNet18 (He et al., 2016) on CIFAR-100, where train loss decreases to zero withsufficiently wide model while test loss decreases, then increases, and then decreases again.
Figure 2: Left: Effective dimensionality as a function of model width and depth for a CNN onCIFAR-100. Center: Test loss as a function of model width and depth. Right: Train loss as afunction of model width and depth. Yellow level curves represent equal parameter counts (1e5, 2e5,4e5, 1.6e6). The green curve separates models with near-zero training loss. Effective dimensionalityserves as a good proxy for generalization for models with low train loss.
Figure 3: Left: A comparison of prior and pos-terior distributions in a linear regression setting,demonstrating the decrease in variance referred toas posterior contraction. Right: Functions sam-pled from the prior and posterior distributions.
Figure 4: We see a similarly strong relative relationship between effective dimensionality and test lossover a wide range of regularization parameters z. Each point is the effective dimensionality computedfor a regularization parameter (varying colors) and a particular width and depth, as in Figure 2. Weuse only models with low training loss (above the green partition in Figure 2) for this plot.
Figure 5: The effective dimensionality of the posterior covariance over parameters and the function-space posterior covariance. Red indicates the over-parameterized setting, yellow the critical regimewith k â‰ˆ n, and green the under-parameterized regime. In both models we see the expected increasein effective dimensionality in parameter space and decrease in effective dimensionality of the Hessian.
Figure 6: Swiss roll data. Left: Adam trained feed-forward, fully connected classifier. Center:Differences in original and perturbed classifier when parameters are perturbed in low curvaturedegenerate directions. Right: Differences in the original and perturbed classifier when parametersare perturbed in high curvature directions. Note the perturbation in the center plot is approximately100 times the size of that of the plot on the right!4.2	Function-Space HomogeneityThe previous section has demonstrated that parameters can be undetermined by the data; we nowshow that the function induced by the model is unchanged in these directions.
Figure 7: Comparing effective dimensionality as a generalization measure to the path-norm, logpath-norm, PAC-Bayes, and magnitude aware PAC-Bayes flatness measures from Jiang et al. (2020)for double descent (a) without and (b) with label noise. For models with low train loss, effectivedimensionality most closely follows both test error and test loss.
