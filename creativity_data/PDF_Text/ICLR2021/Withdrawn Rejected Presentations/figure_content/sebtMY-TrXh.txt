Figure 1: Sentence embedding with arith-metic coding and AriEL. In this example,the generating context-free grammar (CFG)is S → A∖B∖AA∖AB∖AC∖BC∖ABC∖BCC,and the bar plot on top indicates the fre-quency of those sentences in the dataset, asan extra bias to the language. AC (middle)encodes any sequence of this CFG over asingle dimension within [0,1], and the fre-quency of the sentence determines the lengthassigned on that segment. AriEL is a mul-tidimensional extension of AC (here in 2D),where the frequency information is preservedin the volumes. The Language Model pro-vides the boundaries where the next sym-bols are to be found. For a 2D latent space,d = 2, the axis to split to find symbol si isdi = i mod d. In the image ⅛ = 0 and⅛ = 1 represent the horizontal and verticalaxis.
Figure 2: Interpolations between randompoints in the latent space of AriEL, anddiversity of the sentences generated in be-tween. For low dimensions all sentences arevery densely packed, and in the extreme ofone dimension, all sentences are found follow-ing one given dimension. As the dimensional-ity increases, the sentences are redistributed in[0, 1]d and less sentences are found in a givendirection. The lower bound at 0.746 is relatedto the language complexity.
Figure S1: Random-sampling-based generation in the first row, and encoding of input sen-tences in the remaining rows. A sentence is represented by a point in the latent space. First rowshows the proportion of grammatically correct sentences that can be decoded by random uniformsampling the latent space. AriEL sampled almost only grammatical sentences (ungrammatical areso few that are placed on top in the plot). Transformer mainly yielded ungrammatical sentences,while AE and VAE were able to produce many grammatical sentences (ungrammatical are below,otherwise they would cover up the grammatical). Each dot is labeled according to how many ad-jectives the sentence generated has. Second and third rows show the clusters of points in the latentspace for the test sentences as they are mapped by the encoders. All models seem to shift the clustersto some degree according to the number of adjectives in the sentence, in the second row. A similarconclusion applies to the third row, that shows where sentences of different length are encoded. Forall panels, we searched subjectively for the dimensions that would better reveal some clustering,with the help of PCA. We scaled all latent representations between [0,1] for visualization.
Figure S2: Radar Chart of the Quantitative Assessment. Latent space of R16 on the left andR512on the right. Training was performed on biased sentences. The metrics are defined in Method-ology: Generalization is measured by prediction accuracy of unbiased sentences (PAU), Predictionby prediction accuracy of biased sentences (PAB), grammar accuracy (GA) and bias accuracy (BA)and Generation by uniqueness (U), validity (V), vocabulary coverage (VC) and grammar coverage(GC). AriEL excels in all the 8 metrics. Most importantly AriEL outperforms every other methodin Generation Validity (V) and it doesn’t require a large latent space to do so (R16 similar to R512).
Figure S3: Algorithms for AriEL encoder and decoder B stands for bound, and Bup and Blowfor the upper and lower bounds that define the AriEL volumes, the blue color identifies the lines withthe major differences between encoder and decoder and PLM identifies the Language Model insideAriEL. Its cumulative distributions (cup, clow) are used to define the limits of the volumes and itssize (range). (Left) AriEL Encoding: from sentence to continuous space. Finally the volumes arerepresented by their central point z for simplicity. (Right) AriEL decoding: from continuous spaceto sentence. z is used to identify which volume has to be picked next.
