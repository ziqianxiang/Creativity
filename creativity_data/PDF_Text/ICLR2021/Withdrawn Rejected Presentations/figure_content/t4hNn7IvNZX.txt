Figure 1: Accuracies of mod-els trained on MNIST underdifferent levels of `2 attacks.
Figure 2: (a) gives the distance between the robustness certificate (yellow) and the worst-case performanceon testing data (pink) with an example on MNIST. The gap between the two lines indicates the tightness ofour certificate (Eq. 3). (b) compares the performance of two models trained With different c(∙)s. The classifiertrained with the noise included in the cost has better performance overall. (c) compares the performance ofNAL with WRM on MNIST, CNN (ELU) under different γs. NAL overall has better performance than WRM.
Figure 3: NAL outperforms baselines on CIFAR-10, VGG-16 and Tiny ImageNet, ResNet-18. (a),(c) aretrained on ELU models under different γs. For the same γ, NAL exceeds WRM. (b),(d) are trained on ReLUmodels with γ = 1.5 and the corresponding ε. NAL yields the highest robustness under different levels ofattack. STN has the highest clean accuracy.
Figure 4:	NAL versus baselines on MNIST, CNN. (a) NAL versus WRM for different γs. (b) γ = 0.25. (c)γ = 1.5. (d) γ = 3. Equivalent εs are used in SmoothAdv and TRADES.
Figure 5:	NAL versus baselines on CIFAR-10, ResNet-18. (a) NAL versus WRM for different γs. (b) γ = 0.25.
Figure 6:	The comparison between the ReLU model (pink) and the ELU model (yellow) on CIFAR-10, ResNet-18 with γ = 1.5 and σ = 0.1. The ReLU model converges faster than the ELU model.
