Figure 1: An input image is transformed into the output image using our framework: we firstgenerate an EPS map as intermediate representation to then generate the output image in the targetdomain with an image-to-image translation GAN. Object classes of the EPS map are visualizedbased on the color scheme of Cordts et al. [Cordts et al. (2016a)]. The input image shown here forillustration is taken from the Cityscapes data set.
Figure 2: EPS maps are generated by combining a semantic segmentation map and an edge mapgenerated by two state-of-the-art networks.
Figure 3: Examples of synthesized images from various input sources CARLA, Cityscapes, FCAV,and KITTI comprising real and rendered data. The EPS maps are extracted from the input imagesand then used as the only input to synthesize the output images.
Figure 4: Illustration of the IoU scores per class considering CARLA, Cityscapes, FCAV, and KITTI(left), and the dependence of the IoU score on different smoothing radii (right).
Figure 5: Illustration of the effect caused by different edge carving levels and the corresponding IoUscore. The input image shown here for illustration is taken from the KITTI data set.
Figure 6: Illustration of the effect caused by warping semantic segmentations using different inten-sities, and the corresponding IoU score. The input image shown here for illustration is taken fromthe CARLA data set.
Figure 7: Illustration of the effect caused by smoothing the input image. A Kuwahara filter is appliedwith different radii. The input image is taken from the FCAV data set.
