Figure 1: Distributions of users over time. Left: A distribution which remains constant over time,following the i.i.d assumption. Right: Auto-induced Distributional Shift (ADS) results in a changein the distribution of users in our content recommendation environment. (see Section 5.2 for details).
Figure 2: The widely studied problems of reinforcement learning (RL) with state s, action a, rewardr tuples, and i.i.d. supervised learning (SL) with inputs x, predictions y and loss l (a,d) are free fromincentive problems. We focus on cases where there are incentives present which the learner is notmeant to pursue (b,c). Lines show paths of influence. The learner may have incentives to influenceany nodes descending from its action, A, or prediction, y. Which incentives are undesirable (orange)or desirable (cyan) for the learner to pursue is context-dependent.
Figure 3: Types of incentives, and their rela-tionship to ADS.
Figure 4: Context swapping (right).
Figure 5: (A) Values of 妨 in the supervised learning(SL) unit test. Larger values mean sacrificing presentperformance for future performance (i.e. non-myopicexploitation of ADS). (B) Average level of non-myopiccooperate behavior observed in the RL unit test forHI-ADS, with two meta-learning algorithms (B1) PBTand (B2) REINFORCE. Lower is better, since the goalis for non-myopic incentives to remain hidden. Despitethe inner loop being fully myopic (simple MLP in theSL test, γ = 0 in RL test), in all cases outer-loop (OL)optimizers reveal HI-ADS (top rows). Context swap-ping significantly mitigates HI-ADS (bottom rows).
Figure 6: Q-learning can fail the unit test, play-ing 〜80-90% cooperate in3of5 experiments(bottom row). Each column represents an indepen-dent experiment. Q-values for the cooperateand defect actions stay tightly coupled in thefailure cases (col. 1,2,5), while in the cases passingthe unit test (col. 3,4) the Q-value of cooperatedecreases over time.
Figure 7: Content recommendation experiments. Left: using Population Based Training (PBT)increases accuracy of predictions faster, leads to a faster and larger drift in users’ interests, P (y|x),(Center); as well as the distribution of users, P (x), (Right). Shading shows std error over 20 runs.
Figure 8: Average level of non-myopic (i.e. cooperate) behavior learned by agents in the unit testfor HI-ADS. Despite making the inner loop fully myopic (γ = 0), population-based training (PBT)can cause HI-ADS, leading agents to choose the cooperate action (top row). context swappingsuccessfully prevents this (bottom row). Columns (from left to right) show results for populationsof 10, 100, and 1000 learners. In the legend, “interval” refers to the interval (T) of PBT (see Sec.
Figure 9: The same experiments as Figures 6, 10, run for 50,000 time-steps instead of 3000, toillustrate the persistence of non-myopic behavior.
Figure 10: More independent experiments with Q-learning, exactly following Figure 6. Q-learningfails the unit test in a total of 10/30 experiments (including those from Figure 6).
Figure 11: More independent experiments with Q-learning, exactly following Figure 6, except alsousing context swapping. This leads to a 100% success rate on the unit test.
Figure 12: Amount of auto-induced covariate shift (left) and auto-induced concept shift (right) as afunction of performance (accuracy) averaged over all trials, learners, and time-steps. Only relativelystrong learners (those which achieve accuracy > 60%) exhibit HI-ADS.
Figure 13: Context swapping doesn’t have the desired effect in the content recommendationenvironment.
Figure 14: Content recommendation results for different values of α1, α2.
Figure 15: Offline Q-learning can also reveal HI-ADS, when pooling data from different (policy,environment) pairs. Yellow regions represent policy pairs for which Q(C) > Q(D), resulting innon-myopic behavior.
