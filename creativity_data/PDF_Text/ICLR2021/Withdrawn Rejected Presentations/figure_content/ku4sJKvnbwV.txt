Figure 1: Collocation-based planning. Each image shows a full plan at that optimization step.
Figure 2: Latent Collocation (LatCo). Left: Our latent state-space model, with an encoder q(z|o)and a latent state-space dynamics model p(zt+ι |zt, at)〜N(μ(zt, at),σ(zt, at)). A reward modelr(zt) predicts the reward from the latent state. The model is trained with a variational lower boundto reconstruct the observations (not shown). Right: comparison of LatCo and shooting methods.
Figure 3: LatCo executed trajectories on the three considered tasks. In all these sparse reward settingsrequiring temporally extended reasoning, LatCo is able to optimize effective plans and execute them.
Figure 4: Planned and executed trajectories on the SawyerPush task. LatCo produces a feasible and effective plan, andis able to execute it. Shooting struggles to solve this task asdense or shaped rewards are not provided.
Figure 5: Dynamics violation and reward predic-tions of planned trajectories over the course of 200optimization steps on the obstacle task. Earlier on,the planner explores high-reward regions, converg-ing to the goal state. Eventually, as the dynamicsconstraint is enforced, the plan becomes feasible,while maintaining a high predicted reward.
Figure 7: Additional optimization curves. The dynamics coefficient (magnitude of Lagrange multipli-ers) increases exponentially as the dynamics constraint is enforced, and eventually convergesA Model Architecture and Training DetailsWe use the latent dynamics and reward mod-els from PlaNet (Hafner et al., 2019) with de-fault hyperparameters. We set the image sizeto 64x64 and action repeat to 1 for both point-mass and Meta-World models. For every N = 1episode collected, we train for It = 15 iterations.
Figure 6: Visualization of collocation-based plan-ning. Left: shooting-based planning searches foractions that maximize rewards of the transitions asB Planning DetailsCEM. we optimize for 100 iterations. In eachiteration, 10000 action sequences are sampledand the distribution is refit to the 100 best sam-ples. For this baseline, we have manually tunedthe batch size and number of iterations, and wereport the best results.
Figure 8: Visualization of the reward predictor forthe Sawyer Pushing task. The output of the rewardpredictor is shown for each object position on the2D table. We see that the reward predictor correctlypredicts a value of 1 at the goal, and low valuesotherwise. In addition, there is a certain amount ofsmoothing induced by the reward predictor, whichcreates a gradient from the start to the goal posi-tion. This explains why gradient-based planning isapplicable even in this sparse reward task.
