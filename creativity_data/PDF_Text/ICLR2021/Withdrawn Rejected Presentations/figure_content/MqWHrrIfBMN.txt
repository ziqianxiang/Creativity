Figure 1: Schematic comparison of heavy aggregation of the conventional FL and our proposed edgelearning scenario.
Figure 2: We incorporate model ensemble at each edge node and temporal memory at the core tostore maximum m proceeding edge models. In Phase 1, to obtain an ensemble of edge models fromone identical copy of the core model, a different input batch sampling is applied for each edge model(M1 to M3). In the KD phase (Phase 2), the returned ensemble of edge models and the edge modelsfrom the previous (M-1) rounds stored in the memory are used as teachers.
Figure 3:	Intuitive interpretation of the difference between cloned KD and independent KD on animaginary 2-dimensional parameter space. The properties of the two types of distillation are writtenin text.
Figure 4:	Venn diagrams of correctly classified samples after KD with a fixed teacher. In both cases(a) and (b), the teacher model was dispatched to and re-collected from the edge. Results on CIFAR-100 with ResNet-32.
Figure 5: The left table shows four variants of independently trained edge model (I-) and variantsof cloned edge model (C-). The next two letters indicate whether proposed methods are applied ornot with ’P’ (plain) indicating no method is applied. The final accuracy of the table and the accuracycurve all indicate that cloned distillation (solid line) is crucial. Refer to Section 5.1 for more details.
Figure 6: Scores that show cloned distillation is more easy-to-follow than independent knowledgedistillation. Results on CIFAR-100.
Figure 7: Diagram and results of using one core and two edges with lagged response on CIFAR-100.
Figure 8: Robustness of the method when noisy edge model is included on CIFAR-100.
Figure 9: Experiment results quantifying how much the core model actually learns from the edgedata information. The diagram below depicts how the score is computed. Those scores are calculatedon the edge dataset. A core model shows improvement on edge dataset after re-collecting. Resultson CIFAR-100 with ResNet-32.
