Figure 1: ResNet-50 We vary the width of a ResNet-50 trained on ImageNet for various flavors ofAlgebraNet and the real-valued baseline. We separate based on algebras with up to 1:1 multiplies tovalues loaded compute density (top), and greater than a 1:1 density (bottom). In the left columns,we show parameters and ImageNet top-1 accruacy. We count parameters as the total number of realvalues e.g. a complex number counts as two parameters, M2(R) and H both count as 4, etc. All runsuse batch norm (Ioffe and Szegedy, 2015), unless explicitly stated. Right: For the same algebras,we compare the number of floating-point operations (multiply-adds) required at inference. Unlikeall previously considered algebras, for M2 (R), we find equivalent computational costs comparedto real-valued networks at baseline performance. M2 (C) improves performance compared to thepreviously considered H with the same compute density.
Figure 2: MobileNet-v1, Left: We vary the width of a MobileNet-v1 trained on ImageNet for asubset of the considered AlgebraNets and the real-valued baseline. Right: For the same algebras, wecompare the number of FLOPs (multiply-adds) required at inference.
Figure 3: Pruning ResNet-50, Left: Using magnitude pruning, we prune entire tuples of M2 (R)-ResNet50 to between 50 and 90% sparsity. Different widths correspond to different curves, pointsalong each curve are different sparsity levels. In green, we show baseline magnitude-pruning resultsfrom (Gale et al., 2019). Right: For the same pruned networks, we show the FLOP efficiency.
