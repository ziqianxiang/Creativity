Figure 1: Consider an image recognition dataset where the goal is to classify the task, T , as paintingor not painting, and the attribute, A, as woman or man. Note that in this dataset women are correlatedwith painting, and men with not painting. In this work we are particularly concerned with errors thatcontribute to the amplification of bias (red and yellow in the figure), i.e., those that amplify thetraining correlation. We further disentangle these errors into those that amplify the attribute to taskcorrelation (i.e., incorrectly predict the task based on the attribute of the person; shown in yellow)versus those that in contrast amplify the task to attribute correlation (shown in red).
Figure 2: Different fairness metrics vary in how they respond to model errors. In our image dataset(Fig. 1) of predicting someone who is a woman or man to be painting or not, we consider a classifierthat always predicts the task correctly for men, but varies for women. The x-axes of the graphscorrespond to the fraction of women predicted to be painting, where the ground-truth is 2/3, and themodel does not predict false positives before this point. The first two metrics, FPR and TPR differ-ence, only capture one of under- or overclassification. The next two metrics are symmetric around2/3, being unable to differentiate under- or overclassification. Both bias amplification metrics areable to distinguish between under- and overclassification.
Figure 3: Bias amplificationvarying as a function of pro-portion of images classified tobe men (controlled by classifi-cation threshold).
Figure 4: BiasAmp→ changes as a result of three conditionsof images: full, noisy person mask, full person mask. Asless of the person is visible, A→T decreases from less hu-man attribute visual cues to bias the task prediction. T→Aincreases because the model must rely on visual cues fromthe task to predict the attribute.
Figure 5: Illustrative captions from the Equalizer model (Hendricks et al., 2018), which decreasesT → A bias amplification in these captions from the Baseline, but inadvertently increases A → T .
Figure 6: We investigate the consistency of various metrics by looking at 95% confidence intervalsas we manipulate two independent variables: model architecture (left three graphs), and majority tominority groups ratio (right graph). The top row is for the attribute of “big nose”, and bottom rowis for “young.” For model architecture, across 5 runs, the accuracy measure of average precisionretains a consistent ranking across models, but two different fairness measures (FPR difference andA → T bias amplification) have overlapping intervals. This does not allow us to draw conclusionsabout the differing fairness of these models. However, across-ratio differences in bias amplificationare significant enough to allow us to draw conclusions about the differing levels of fairness.
Figure 7: How the fair-ness metrics of BiasAmpA→Tand FPR difference vary aswe change the threshold forCOMPAS’s risk prediction.
