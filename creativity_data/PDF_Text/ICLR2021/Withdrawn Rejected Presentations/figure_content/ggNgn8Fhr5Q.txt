Figure 1: Example reconstructions from a deterministic Neural Process (CNP) with varying repre-sentation sizes. For the Gaussian Process example, reducing the representation size removes higherfrequency components. For the Fourier series example, the CNP instead limits the effective interval.
Figure 2: Influence of the context on the learned representations in a deterministic Neural Process(CNP) and a variational Neural Process (NP) with Dr = 128 and data coming from a GaussianProcess prior with an EQ kernel with l = 0.2 (top half, kernel description in Eq. (10)) or a Fourierseries with K = 19 as described by Eq. (11) (bottom half). x refers to the input space (i.e. thedomain), y to the output space (i.e. the co-domain). Representation channels are ordered by theiraverage magnitude. We find that the learned representations are almost always anti-symmetric acrossy = 0. The CNP exhibits an oscillating behaviour along the x-axis, with varying frequency acrossrepresentation channels. In contrast, the NP seems to mostly partition the input space into separateregions, only exhibiting oscillating behaviour in channels with lower magnitude. The representationchannels in this example were selected for illustrative purposes. Note that each panel is normalizedseparately, so color values are not comparable.
Figure 3: Deterministic Neural Process (CNP) trained to be a band-pass and a band-stop filter. Wetrain three CNP models on Fourier series, where we only show certain frequencies during trainingfor the models that should serve as frequency filters. The bottom row shows the distribution ofFourier components in the training data (left) and after applying each model to the reference data(right). The models only learn to represent frequencies seen during training, and as a result act likeband-pass and band-stop filters. The top rows show an example of this. See Fig. A.8 for the sameexperiment using a variational Neural Process.
