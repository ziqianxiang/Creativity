Figure 1: The 10 RLBench tasks used for evaluation. Current state-of-the-art reinforcement learningalgorithms catastrophically fail on all tasks, whilst our method succeeds within a modest number ofsteps. Note that the positions of objects are placed randomly at the beginning of each episode.
Figure 2: Summary and architecture of our method. RGB and organised point cloud crops are madeby extracting pixel locations from our Q-attention module. These crops are then fed to a continuouscontrol RL algorithm that suggests next-best poses that is trained with a confidence-aware critic.
Figure 3: Visualising the Q values across 4 different points in time on 6 tasks. At each step, RGBand organised point cloud crops are made by extracting pixel locations that have the highest Q-value.
Figure 4: Keyframe selection and demo aug-mentation, where the black line representsa trajectory, ’!’ represents keyframes, anddashed blue lines represent the augmentedtransitions to the keyframes.
Figure 5: Visualising RGB observations of keyframes from the keyframe selection process on 4tasks. Here k is the keyframe number.
Figure 6: Learning curves for 10 RLBench tasks. Methods include Ours (ARM), SAC (Haarnojaet al., 2018), TD3 (Fujimoto et al., 2018), and QT-Opt (Kalashnikov et al., 2018). ARM uses the3-stage pipeline (Q-attention, next-best pose, and control agent), while baselines use the 2-stagepipeline (next-best pose and control agent). All methods receive 200 demos which are stored in thereplay buffer prior to training. Solid lines represent the average evaluation over 5 seeds, where theshaded regions represent the min and max values across those trials.
Figure 7: Ablation study'put_rubbish_in_bin' task.
