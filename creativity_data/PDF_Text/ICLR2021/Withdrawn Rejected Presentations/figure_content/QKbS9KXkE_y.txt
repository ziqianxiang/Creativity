Figure 1: Graphical model for flat policies (left), mixture policies (middle) - introducing a type ofaction abstraction, and option policies (right) - adding temporal abstraction via autoregressive options.
Figure 2: Representation of the dynamic programming forward pass - bold arrows represent connec-tions without switching. Left: example with two options. Right: extension of the graph to explicitlycount the number of switches. Marginalization over the dimension of switches determines componentprobabilities. By limiting over which nodes to sum at every timestep, the optimization can be targetedto fewer switches and more consistent option execution.
Figure 3: Results on OpenAI gym. Dashed black line represents DAC (Zhang & Whiteson, 2019),dotted line represents Option-Critic (Bacon et al., 2017) and solid line represents IOPG (Smith et al.,2018) (lines reflect the approximate results after 2 × 106 steps from (Zhang & Whiteson, 2019)).
Figure 4: Ball-In-Cup and Stacking. Left: Environments. Right: Example agent observations.
Figure 5: Results for option policies, mixture policies, and single Gaussian policies (respectivelyHO2, RHPO, and MPO) in multi-task domains with pixel-based ball-in-cup (left) and pixel-basedblock stacking (right). All four tasks displayed use sparse binary rewards, such that the obtainedreward represents the number of timesteps where the corresponding condition - such as the ball is inthe cup - is fulfilled. Please see Appendix B for further details and additional tasks.
Figure 6: Sequential transfer experiments with limited option switches. Left: BIC. Right: Stack. Wesee considerable improvements for limited switches. In addition, we visualize the actual agent optionswitch rate in the environment to directly demonstrate the constraint’s effect.
Figure 7: Results on OpenAI gym with/without conditioning of option probabilities on past actions.
Figure 8: Block stack-ing results with differenttrust-region constraints.
Figure 9: Analysis on Ant locomotion tasks, showing histogram overoptions, and t-SNE scatter plots in action space colored by option.
Figure 10: Qualitative results for the three bodies (Ball, Ant, Quad) without limited switches, bothwith and without IA, obtained over 100 evaluation episodes. Left: the histogram over differentoptions used by each agent; Centre to right: scatter plots of the action space, state space, and taskinformation, colored by the corresponding option selected. Each of these spaces has been projectedto 2D using t-SNE, except for the two-dimensional action space for Ball, which is plotted directly.
Figure 11: Complete results on pixel-based ball-in-cup experiments.
Figure 12:	Complete results on pixel-based stacking experiments.
Figure 13:	Complete results on multi-task block stacking with and without conditioning terminationconditions on tasks.
Figure 14: Complete results on OpenAI gym with and without conditioning component probabilitieson past executed actions. For the off-policy (top) and on-policy case (bottom). The on-policyapproaches uses data considerably less efficiently and the x-axis is correspondingly adapted.
Figure 15: Complete results on block stacking with varying trust-region constraints for both termina-tion conditions β and the high-level controller πC .
Figure 16: Ablation results comparing inferred options with sampled options during learning (sam-pled) and during execution (executed). The ablation is run with five actors instead of a single one asused in the OpenAI gym experiments in order to generate results faster.
Figure 17: The environment used for simple locomotion tasks with Ball (left), Ant (center) andQuadruped (right).
