Figure 1: Illustration of optimization behaviorwith BN and WD. Angular update ∆t representsthe angle between the updated weight Wt and itsformer value Wt+1.
Figure 2: Performance of layer.2.0.conv2 from Resnet50 in SGD and SGDM, respectively. In (a),(d), semitransparent line represents the raw value of ||gt||2 or ht, while solid line represents theaveraged value within consecutive 200 iterations to estimate the expectation of ||gt||2 or ht; In (b),(e), blue solid lines shows the raw value of weight norm ||wt ||2 , while dashed line represent thetheoretical value of weight norm computed in Theorem 1, 2 respectively. The expectations E∣∣g∣∣2and Eh are estimated as solid lines (a) and (d) respectively; In (c), (f), red lines represent raw valueof angular update during training, dashed lines represent the theoretical value of angular updatecomputed by λ∕2λp and ,2λη/ (1 + α) respectively.
Figure 3: angular update in the last two stages is smaller than its theoretical value. This phenomenoncan be well interpreted by our theory: according to Theorem 1, 2, when equilibrium condition isreached, theoretical value of weight norm satisfies ||wt112 æ 4∕λ, therefore when learning rate is8Under review as a conference paper at ICLR 2021(a) Angular update in Imagenet(d) Angular update in MSCOCO(c) Weight norm in Imagenet(e) Angular update in MSCOCO (f) Weight norm in MSCOCO(rescaled)Figure 3: In (a),(b),(d),(e), solid lines with different colors represent raw value of angular updateof weights from all convolutional layer in the model; In (a), (d), training setting rigorously followsGoyal et al. (2017); He et al. (2019) respectively; In (b), (e), weight norm is dividied by √10 aslong as learning rate is divided by 10; In (c), (f), weight norm is computed on layer.1.0.conv2 inResnet50 backbone. Blue line represent original settings, orange line represent rescaled settings.
Figure 3: In (a),(b),(d),(e), solid lines with different colors represent raw value of angular updateof weights from all convolutional layer in the model; In (a), (d), training setting rigorously followsGoyal et al. (2017); He et al. (2019) respectively; In (b), (e), weight norm is dividied by √10 aslong as learning rate is divided by 10; In (c), (f), weight norm is computed on layer.1.0.conv2 inResnet50 backbone. Blue line represent original settings, orange line represent rescaled settings.
Figure 4: Simulation of SGD (Eq.(130)), η = 0.1, λ = 0.001, xo = 10, et 〜U(-3, 3). Orangelines represent the square of unit gradient norm; blue solid lines represent simulated value of weightnorm square; black dashed lines represent theoretical value of weight norm square24Under review as a conference paper at ICLR 2021Figure 5: Simulation ofSGDM (Eq.(131)), η = 0.1, λ = 0.001, xo = 10, Q 〜U(-3,3), α = 0.9.
Figure 5: Simulation ofSGDM (Eq.(131)), η = 0.1, λ = 0.001, xo = 10, Q 〜U(-3,3), α = 0.9.
Figure 6: ||贝 ||2, ht and norm of weight from IayerL0.conv2 in ReSnet50 backbone. (a),(b) present∣∣g∣∣2, h in multi-state learning rate schedule experiments discussed in Section 5.2. semitranspar-ent line represents the raw value of ∣∣gt∣∣2 and ht, solid line represents the averaged value withinconsecutive 200 iterations to estimate the expectations E∣∣gt∣∣2 and Eht. (c) presents the empiricalvalue of weight norm and its theoretical value in standard and rescaled cases. The theoretical valueis computed by estimated expectations Eht in (a), (b) respectively.
Figure 7: Angular update of weights from layer1.0.conv2 in ResNet50. The blue lines representthe angular update of weights within a single iteration in when batch settings is B = 1024(4096);The red lines represent the accumulated angular update within 4(16) iterations in smaller batchsetting(B = 256).
Figure 8: Enlargement ratio of gradients’ norm of weights from layer1.0.conv2 when batch sizeincreases . ||g(k) || represents the gradient’s norm computed using k samples(not average).
Figure 9: The angular update ∆t of MobileNet-V2 (Sandler et al., 2018) and ShuffleNet-V2+ (Maet al., 2018). The solid lines with different colors represent all scale-invariant weights from themodel; The dash black line represents the theoretical value of angular update, which is computed byJ 12λα. Learning rate η is initialized as 0.5, and divided by 10 at epoch 30, 60, 80 respectively; WDcoefficient λ is 4 × 10-5; Momentum parameter α is set as 0.9.
