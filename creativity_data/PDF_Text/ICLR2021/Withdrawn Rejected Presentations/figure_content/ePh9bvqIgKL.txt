Figure 1: Random acti-vation function initializa-tion. The initial popula-tion consists of randomsamples of two kinds ofcomputation graphs, ran-domly initialized with theoperators in Table 1. Inthis manner, the searchstarts with simple graphsand gradually expands tomore complex forms.
Figure 2: Evolutionary operations on activation functions. In an ‘Insert’ mutation, a new operatoris inserted in one of the edges of the computation graph, like the Swish(x) in (b). In a ‘Remove’mutation, a node in the computation graph is deleted, like the addition in (c). In a ‘Change’ mutation,an operator at a node is replaced with another, like addition with multiplication in (d). These firstthree mutations are useful in refining the function locally. In contrast, in a ‘Regenerate’ mutation (e),every operator in the graph is replaced by a random operator, thus increasing exploration.
Figure 3: Parameterizationof activation functions. Inthis example, parameters areadded to k = 3 randomedges, yielding the parametricactivation function aσ(β∣x∣ -arctan(γx)).
Figure 4: Progress of PANGAEA on three different neuralnetworks. Evolution quickly discovered activation functionsthat outperform ReLU (shown at x = 0), and continued toimprove throughout the experiment. The plots show the high-est validation accuracy of all activation functions evaluatedso far after 100 epochs of training. Notable discovered ac-tivation functions are identified with a star and annotated.
Figure 5: Adaptation of parametric activation functions overtime and space. Top: The parameters change during training,resulting in different activation functions in the early and latestages. The plots were created by averaging the values ofα, β , and γ across the entire network at different trainingepochs. Bottom: The parameters are updated separately ineach channel, inducing different activation functions at dif-ferent locations of a neural network. The plots were createdby averaging α, β , and γ at each layer of the network afterthe completion of training.
Figure 6: CIFAR-100 test accuracy for different neural networks and activation functions. Accuracywith ReLU is shown in blue, and accuracy with the specialized activation functions in red. The relativeimprovement of the specialized functions over ReLU is shown as a dotted green line, according to theaxis values on the right of each plot. Left: The depth of Wide ResNet is fixed at 10, and the widthvaries from 1 to 16. Center: The depth of Wide ResNet varies from 10 to 34, while the width is fixedat four. Right: The depth of Preactivation ResNet ranges from 20 to 164. The width and depth of anetwork can affect how much a specialized activation function outperforms ReLU.
