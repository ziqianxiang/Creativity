Figure 1:	Spatial resolution tunable in convolution phase (bottom, with 3 resolution level examplesillustrated), but vanishes in the pooling phase (top, which mixes all the node vectors into one vector).
Figure 2:	Over-delicate structural resolution may hamper the generalization performance.
Figure 3:	Three steps of the SLIM network. Sub-structure embedding: extract k-hop local sub-graphsaround each node and embed them in a new space. Substructure landmarking: compute sub-structurerepresentatives through unsupervised clustering of sub-structures across graphs. Identity-preservinggraph pooling: project each graph on the common set of sub-structure landmarks for final prediction.
Figure 4: End-to-end training architecture of the SLIM network.
Figure 5: Accuracy vs structural resolution K.
Figure 6: Algorithm stability on the Mutagdataset. More examples in Appendix G.
Figure 7: Impact of spatial resolution and the unsupervised losses on the prediction result.
Figure 8: Two pairs of molecules from different classes of MUTAG data, with their respectivesubstructure-interaction-patterns marked on top of the learned structural landmarks. Red and bluecolors signify the class labels.
Figure 9: Testing accuracy of different algorithms over the training epochs.
Figure 10: Average wall-clock time comparisons of SLIM and other methods.
