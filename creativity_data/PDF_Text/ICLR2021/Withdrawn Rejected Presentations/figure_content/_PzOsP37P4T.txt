Figure 1: Reparameterization in stochastic computational graphs.
Figure 2: (a) Approximation of GEN GS in terms of choices of the truncation level n and thetemperature τ in Poisson(7). As sub-figures go from left to right, the truncation level grows. Hence,the popped-out sticks, implying remaining probability of the right side, disappears if the truncationlevel is large enough. As sub-figures go from top to bottom, the temperature decreases, and thePMF of truncated distributions and the original distributions becomes similar. Appendix D providesthe fine-grained PMF of GENGS. (b) On the x-axis, TD(λ, n) → D(λ) as truncation level n ↑ ∞,according to Proposition 3. Then, TD(λ, n) can be reparameterized by the Gumbel-Max trick with alinear transformation T as in Proposition 4. On the y-axis, as temperature T 1 0, GenGS (π, T) →TD(λ, n), where π is a computed PMF value of TD(λ, n), according to Theorem 5.
Figure 3: Synthetic example performance curves in log scale: (Top Row) losses, variances, and biasesof gradients for Poisson; (Middle Row) losses for Binomial, Multinomial, and NegativeBinomial;(Bottom Row) variances of gradients for Binomial, Multinomial, and NegativeBinomial. We utilizethe cumulative average for smoothing the curves, and the curves with confidence intervals and thecurves without smoothing are in Appendix J.
Figure 4: Reconstructed images by VAEs with various gradient estimators. GenGS shows theclearest images among other gradient estimators with better reconstruction.
Figure 5: (Left) A graphical notation of NVPDEFwith generative process (θ) and inference network(φ). The multi-stacked latent layers have λi asa prior distribution parameter. (Right) A neuralnetwork diagram of NVPDEF: diamond nodes in-dicate the auxiliary random variable for the repa-rameterization trick.
Figure 6: Fine-grained PMF of GenGS for Poisson(7).
Figure 7: Visualization of GenGS reparameterization of explicit inference version. Note that wecompute PMF values from the infered parameter λ.
Figure 8: Visualization of GenGS reparameterization of implicit inference version. Note that we donot infer the distribution parameter λ, and the PMF values are computed directly, instead.
Figure 9:	Synthetic example performance curves in log scale: (Top Row) Losses, variances, and biasesof gradients for Poisson; (Middle Row) Losses for Binomial, Multinomial, and NegativeBinomial;(Bottom Row) Variances of gradients for Binomial, Multinomial, and NegativeBinomial. We utilizethe cumulative average for smoothing the curves, and we also provide confidence intervals together.
Figure 10:	Synthetic example performance curves in log scale: (Top Row) Losses, vari-ances, and biases of gradients for Poisson; (Middle Row) Losses for Binomial, Multinomial,and NegativeBinomial; (Bottom Row) Variances of gradients for Binomial, Multinomial, andNegativeBinomial. We do not smoothen the curves to show the convergence of losses in this figure.
