Figure 1: High-level architecture of our approach. Each agent runs GameDistill by performingsteps (1), (2), (3) individually to obtain two oracles per agent. During game-play(4), each agent(with SQLoss) takes either the action suggested by the cooperation or the defection oracleof exploitation, it is drawn to defect, both to maximize the reward (through exploitation) and mini-mize its loss (through being exploited). To increase the likelihood of cooperation, it is important toreduce instances of exploitation between agents. We posit that, if agents either mutually cooperate(CC) or mutually defect (DD), then they will learn to prefer C over D and achieve a socially desir-able equilibrium. (for a detailed illustration of the evolution of cooperation, see Appendix C, whichis part of the Supplementary Material)Motivated by ideas from human psychology that attribute cooperation in humans to the status-quobias (Guney & Richter, 2018), we introduce a status-quo loss (SQLoss) for each agent, derivedfrom the idea of imaginary game-play (Figure 2). Intuitively, the loss encourages an agent to imag-ine an episode where the status-quo (current situation) is repeated for several steps. This imaginedepisode causes the exploited agent (in DC) to perceive a continued risk of exploitation and, there-fore, quickly move to (DD). Hence, for the exploiting agent, the short-term gain from exploitation(DC) is overcome by the long-term loss from mutual defection (DD). Therefore, agents movetowards mutual cooperation (CC) or mutual defection (DD). With exploitation (and subsequently,the fear of being exploited) being discouraged, agents move towards cooperation.
Figure 2: Intuition behind Status - Quo-aware learner. At each step, the SQLoss encourages anagent to imagine the consequences of sticking to the status-quo by imagining an episode where thestatus-quo is repeated for κ steps. Section 2.3 describes SQLoss in more detail.
Figure 3: (a) Average NDR values for different learners in the IPD game. SQLearner agents obtaina near-optimal NDR value (-1) for this game. (b) Average NDR values for different learners in theIMP game. SQLearner agents avoid exploitation by randomising between H and T to obtain anear-optimal NDR value (0) for this game.
Figure 4: (a) Probability that an agent will pick a coin of its color in Coin Game. (b) Representationof clusters obtained after GameDistill. Each point is a t-SNE projection of the 100-dimensionalfeature vector output by the GameDistill network for an input sequence of states. The figure onthe left is colored based on rewards obtained by the Red and Blue agents. The figure on the right iscolored based on clusters learned by GameDistill.
Figure 5: Illustration of two agents (Red and Blue) playing the dynamic games: (a) Coin Game andthe (b) Stag-Hunt Game(b)A.1 Coin GameFigure 5a illustrates the agents playing the Coin Game. The agents, along with a Blue or Red coin,appear at random positions in a 3 × 3 grid. An agent observes the complete 3 × 3 grid as input andcan move either left, right, up, or down. When an agent moves into a cell with a coin, it picks thecoin, and a new instance of the game begins where the agent remains at their current positions, buta Red/Blue coin randomly appears in one of the empty cells. If the Red agent picks the Red coin,it gets a reward of +1, and the Blue agent gets no reward. If the Red agent picks the Blue coin, itgets a reward of +1, and the Blue agent gets a reward of -2. The Blue agent’s reward structure issymmetric to that of the Red agent.
Figure 6: Representation of the clusters learned by GameDistill for Coin Game. Each point is at-SNE projection of the feature vector (in different dimensions) output by the GameDistill networkfor an input sequence of states. For each of the sub-figures, the figure on the left is colored based onactual rewards obtained by each agent (r1 |r2). The figure on the right is colored based on clustersas learned by GameDistill. GameDistill correctly identifies two types of trajectories, one forcooperation and the other for defection.
Figure 7: t-SNE plot for the trajectory embeddings obtained from the Stag Hunt game along withthe identified cooperation and defection clusters.
Figure 8: Illustrative predictions of the oracle networks learned by the Red agent usingGameDistill in the Coin Game. The numbers in red/blue show the rewards obtained by the Redand the Blue agent respectively. The cooperation oracle suggests an action that avoids picking thecoin of the other agent while the defection oracle suggests an action that picks the coin of the otheragent16Under review as a conference paper at ICLR 2021D.5 Results for the Iterated Stag Hunt (ISH) using SQLossWe provide the results of training two SQLearner agents on the Iterated Stag Hunt game in Fig-ure 9. In this game also, SQLearner agents coordinate successfully to obtain a near-optimal NDRvalue (0) for this game.
Figure 9: NDR values for SQLearner agents in the ISH game. SQLearner agents coordinatesuccessfully to obtain a near optimal NDR value (0) for this game.
Figure 10: NDR values for SQLearner agents in the Chicken game. SQLearner agents coordinatesuccessfully to obtain a near optimal NDR value (-1.12) for the game.
