Figure 1: For fixed initial training set logits Z0, plotting learning curves against ηt = β2ηt causesthe learning curves to collapse to the learning curve of the linearized model at early times (right), incontrast to un-scaled curves (left). Models with large β follow linearized dynamics the longest.
Figure 2: The timescale Tz depends only on ∣∣Z0∣∣f in units of η = β2η (left, inset). Tz dependslinearly on ∣Z0∣F, up to an O(1) coefficient which saturates at large and small ∣Z0∣F (left, main).
Figure 3: The time to deviation from linearized dynamics, Tnl, has large deviation over β and kZ0kF(left), which can be largely explained by linear dependence on β (right), in units of η = β2η. Thereis an O(1) dependence on kZ0kF which is consistent across varying β for fixed kZ0kF.
Figure 4: Optimal learning rate η* for WRN on CIFAR10 scales as 1∕β.
Figure 5: Properties of early learning dynamics, which affect generalization, can be determined bylocation in the β-∣∣Z0∣∣F phase plane (a). At optimal learning rate η*, small β and larger ∣∣Z0∣∣Fleads to slower early learning (b), and larger β increases time before nonlinear dynamics contributesto learning. Large ∣Z0 ∣F has poorly conditioned linearized dynamics. Generalization for a wideresnet trained on CIFAR10 is highly sensitive to β, and relatively insensitive to ∣Z0 ∣F outside poorconditioning regime. Final logit variance is relatively insensitive to parameters (c).
Figure 6: Dependence of test accuracy for various architectures with β tuning. (a) For WRN withbatchnorm, trained on CIFAR10, the optimal β ≈ 10. Without batchnorm, the performance of thenetwork can be nearly recovered with β-scaling alone with β ≈ 10-2. Even poorly conditionednetworks (achieved by increasing weight scale σw) recover performance. (b) For β < 10-2, learningis less stable, as evidenced by low average performance but high maximum performance (over 10random seeds). (c) We see similar phenomenology on the IMDB sentiment analysis task trained withGRUs - where average-case best performance is near β = 1 but peak performance is at small β .
Figure 7: Tz/kZ0∣∣F is highly correlated with ∣∣ΘΘ(Y - σ(Z0))k-1, with ΘΘ computed in the infinitewidth limit (in units of effective learning rate η = β2η). Ratio between normalized timescales atlarge and small ∣Z0 ∣F depends on nonlinearity (left and middle), as well as training set statistics(right, CIFAR10 with shuffled labels).
