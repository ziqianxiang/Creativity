Figure 1: (Left) EBM generated 128x128 unconditional CelebA-HQ images. (Right) 128x128 unconditionalLSUN Bedroom Images.
Figure 2: Illustration of our overall proposed framework for training EBMs. EBMs are trained with contrastivedivergence, where the energy function decreases energy of real data samples (green dot) and increases the energyof hallucinations (red dot). EBMs are further trained with a KL loss which encourages generated hallucinations(shown as a solid red ball) to have low underlying energy and high diversity (shown as blue balls). Red/greenarrows indicate forward computation for while dashed arrows indicate gradient backpropogation.
Figure 3: Illustration of our multi-scaleEBM architecture. Our energy function overan image is defined compositionally as thesum of energy functions on different resolu-tions of an image.
Figure 4: Randomly selected unconditionalCelebA-HQ samples from our trained EBMFigure 5: Visualization of Langevin dynamics sampling chains on an EBM trained on CelebA-HQ 128x128.
Figure 5: Visualization of Langevin dynamics sampling chains on an EBM trained on CelebA-HQ 128x128.
Figure 6: Output samples of running Langevin Dynamics from a fixed initial sample (center of square), with orwithout intermittent data-augmentation transitions. Without data-augmentation transitions, all samples convergeto same image, while data augmentations enables chains to seperateOur improvements are largely built on top of the EBMS training framework proposed in (Du &Mordatch, 2019). We use a buffer size of 10000, with a resampling rate of 5% with L2 regularizationon output energies. Our approach is significantly more stable than IGEBM, allowing us to removeaspects of regularization in (DU & Mordatch, 2019). We remove the clipping of gradients in Langevinsampling as well as spectral normalization on the weights of the network. In addition, we addself-attention blocks and layer normalization blocks in residual networks of our trained models. Inmulti-scale architectures, we utilize 3 different resolutions of an image, the original image resolution,half the image resolution and a quarter the image resolution. We report detailed architectures in theappendix. When evaluating models, we utilize the EMA model with EMA weight of 0.999.
Figure 7: Illustration of InceptionScore over long run chains with or with-out data augmentation/KL loss.
Figure 8: Illustration of very low temperature samplesfrom our model with KL loss and data augmentation(top) on CIFAR-10 and CelebA-HQ compared to with-out (bottom). After a large number of sampling steps,models trained without KL/data augmentation convergeto stranges hues in CIFAR-10 and random textures onCelebA-HQ. In contrast, due to better mode exploration,adding both losses maintain naturalistic image modes onboth CIFAR-10 and CelebA-HQ.
Figure 9: The KL loss significantly improvesthe stability of EBM training. Stable EBM train-ing occurs when the energy difference is roughlyzero (indicated by dashed black line). We find thatwithout using the KL loss term EBM training di-verges quickly with different additions to networkarchitecture, while with the KL loss training is sta-ble. Spectral normalization improves stability oftraining, but addition of components such as layernormalization also destabilizes training.
Figure 10: Examples of compositionality in face attributes (left) and different renderings of a shape (right). Ourmodel is able to construct high resolution globally coherent compositional renderings, including fine detail suchas lighting and reflections (right).
Figure 11: Architecture of models on different datasets.
Figure 12: Plots of the gradient magnitude of LKL and LCD across training iterations. Influences and relativemagnitude of both loss terms stays constant through training.
Figure 13:	Generations on MNIST with backpro-pogation through 1 step of Langevin sampling.
Figure 14:	Generations on MNIST with backpro-pogation through all steps of Langevin sampling.
Figure 16: Illustrations of compositional generations from (Vedantam etal., 2018). Generations are significantlymore blurry than our generations.
Figure 15: Illustration of collapsed sampling from an EBM.
Figure 17: Randomly selected unconditional LSUN bed 128x128 samples from our trained EBM.
Figure 18: Randomly selected unconditional CIFAR-10 samples from our trained EBM.
