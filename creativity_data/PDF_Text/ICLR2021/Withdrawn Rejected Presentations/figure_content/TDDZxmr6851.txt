Figure 1: Large learning rates lead to large weight movement and better performance. (a) A visualizationof gradient descent dynamics derived in our analytic model. A 2D slice of parameter space is shown, wherelighter color indicates higher loss and dots represents points visited during optimization. Initially, the loss growsrapidly while local curvature decreases.  Once curvature is sufficiently low, gradient descent converges to aflat minimum.  We call this the catapult effect.  See Figures S2 and S1 for more details.  (b) Confirmation ofour predictions in a practical deep learning setting.  Line shows the test accuracy of a Wide ResNet trainedon CIFAR-10 as a function of learning rate, each trained for a fixed number of steps. Dashed lines show ourpredictions for the boundaries of the large learning rate regime (the catapult phase), where we expect optimalperformance to occur. Maximal performance is achieved between the dashed lines, confirming our predictions.
Figure 2: Early time catapult dynamics. (a,b,c) A 3 hidden layer fully-connected network with ReLU non-linearity with width 2048 trained on MNIST (ηcrit  = 6.25). (d,e,f) Wide ResNet 28-10 trained on CIFAR-10(ηcrit  = 0.18). Both networks are trained with vanilla SGD; for more experimental details see Appendix A. (a,d)Early time dynamics of the training loss for learning rates in the linear and catapult phases. (b) Maximum valueof the loss as a function of the learning rate. (e) Early time dynamics of the curvature for learning rates in thelinear and catapult phase. (c,f) λt  measured at t  η = 250 (for FC) and t  η = 30 (for WRN), as a function oflearning rate. Training diverges for learning rates in the shaded region.
Figure 3: Models perform best with a large learning rate. Test accuracy vs learning rate for (a,b) a CNNtrained on CIFAR-10 using SGD with batch size 256 and L2 regularization (ηcrit       10−⁴) and (c,d) WRN28-10trained on CIFAR-10 using SGD with batch size 1024, L2 regularization, and data augmentation (ηcrit       0.14);see Appendix A for details. (a,c) have a fixed compute budget: (a) 437k steps and (b) 12k steps. (b,d) have beenevolved for a fixed amount of physical time: (b) was evolved for 475/η steps (purple) and evolved for 50k moresteps at learning rate 2   10−⁵ (red) and (d) was evolved for 3360/η steps with learning rate η (purple) and thenevolved for 4800 more steps at learning rate 0.035 (red). In all cases, optimal performance is achieved aboveηcrit and close to the expected maximum learning rate, in agreement with our predictions.
Figure S1: Visualization of training dynamics in all three phases. In the lazy phase, the network isapproximately linear in its parameters, and converges exponentially to a global minimum.  In thecatapult phase, the loss initially grows, while the weight norm and curvature decrease. Once thecurvature is low enough, optimization converges. In the divergent phase, both the loss and parametermagnitudes diverge. (a)-(d) Loss surface and training dynamics visualized in a 2d linear subspace.
Figure S2: Empirical results for the gradient descent dynamics of the warmup model with n = 10³,for which ηcrit      1. (a) Training loss for different learning rates. (b) Maximum NTK eigenvalue as afunction of time. For η > 1, λt decreases rapidly to a fixed value. (c) Maximum NTK eigenvalueat        t = 25/η. The shaded area indicates learning rates for which training diverges empirically. Theresults are presented as a function of t · η (rather than t) for convenience.
Figure S3: The convergence time diverges when the learning rate is close to the critical value ηcrit,indicated by the solid green line. The measured exponents (shown in parentheses) are close to thepredicted value of -1. Experiment involves the warmup model of Section 3 with width 16,000.
Figure S4: Final accuracy versus learning rate for a fully-connected 1 hidden layer ReLU network,trained on 512 samples of MNIST with full-batch gradient descent until training accuracy reaches 1or 700k physical steps (see Appendix A for details). We used a subset of samples to accentuate theperformance difference between phases. The optimal performance is obtained when the learning rateis      above ηcrit, and close to ηmₐₓ.
Figure S5:  Test accuracy vs learning rate for WRN28-10 and CIFAR100 with vanilla SGD, L₂regularization, data augmentation, label smoothing and batch size 1024. The critical learning rate isηcrit      0.4. (a) Evolved for 38400 steps. (b) Evolved for 96000/η steps with learning rate η (blue)and then evolved for 7200 more steps at learning rate 0.01 (red).
Figure S7: WRN28-10 on CIFAR10 without L₂. Same setup as 3d but evolved for longer times.
Figure S8: Test accuracies for a larger L₂ CIFAR10 experiment like that of the main section.  (a)WRN CIFAR-10 7200 steps as in figure 3c. (b) WRN CIFAR10 2400 physical steps and then 4800more steps at learning rate 0.01 as in figure 3d.
Figure S9: Training accuracies for the performance experiments. Smaller learning rates have highertraining accuracy when compared in physical time.  However, they still perform worse for a fixednumber of steps. (a) WRN CIFAR-10 12000 steps as in figure 3c. (b) WRN CIFAR10 3360 physicalsteps as in figure 3d. (c) WRN CIFAR100 38400 steps as in figure S5a.(d) WRN CIFAR100 96000physical steps as in figure S5b.
