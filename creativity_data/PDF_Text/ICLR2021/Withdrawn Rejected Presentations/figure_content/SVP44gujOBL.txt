Figure 1: A geometrical interpretation of gradient steps forthe understanding of equation 1.
Figure 2:	Learning curves of experiments 1 and 2 comparing DCL+, DCL-, and vanilla SGDs. Errorbars signify the standard error of the mean (STE) after 30 independent trials.
Figure 3:	Learning curves for experi-ment 2 with varying pace(t) = bkNcfor DCL+. The parameter k needs tobe finely tuned for improving the gen-eralization of the network. A low kvalue exposes only examples with lessnoise to the network at every epochwhereas a high k value exposes mostof the dataset including highly noisyexamples to the network. A moder-ate k value shows less noisy examplesalong with some examples with moder-ate level of noise to the learner. Here, amoderate k = 0.6 generalizes the best.
Figure 4: Top 10 images with the highest standard deviation values (top row) and top 10 imageswith the lowest standard deviation values (bottom row) in CIFAR-100 dataset.
Figure 5: Learning curves for experiments 3 (top) and 4 (bottom). Error bars represent the STE after25 independent trials.
Figure 6: Bars represent the final mean top-1 test accuracy (in %) achieved by models in experiments3 - 7. Error bars represent the STE after 25 independent trials for experiments 3,4 and 7, and 10independent trials for experiments 5 and 6.
Figure 7: Relation of P and Stddev values of examples over training epochs 1, 5, and 100 for exper-iments 1 (top row) and 2 (bottom row).
Figure 8: Bars represent the final mean top-1 test accuracy (in %) achieved by models. Error barsrepresent the STE after 10 independent trials.
