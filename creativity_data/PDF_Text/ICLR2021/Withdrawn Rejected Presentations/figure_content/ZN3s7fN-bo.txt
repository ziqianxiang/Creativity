Figure 1: Tool Comparison Contrasting between a representative tool for debugging RL in the existingecosystem (L), and Vizarel (R), highlights the difference in design intent between both systems. The formerwas designed for the supervised learning setting, and has shown promise for use in reinforcement learning.
Figure 2: State + Action Viewports (T) Visualizing the state viewport for the inverted pendulum task. Thisrepresentation overlayed with another state viewport similar to (b), provides the user with better intuition aboutthe correspondence between states and actions for non image state spaces. (B) Visualizing the action viewportfor the Pong environment (Bellemare et al., 2013). Hovering over instantaneous timesteps dynamically updatesthe state viewport (4.1.1) and shows the corresponding rendered image for the selected state. This representationprovides the user with intuition about the agent policy, and could help subsequent debugging.
Figure 3: Replay Buffer Viewport Projecting the contents of the replay buffer into a 2D space for easiernavigation, and clustering states based on similarity. This viewport provides a visual representation for replaybuffer diversity and can help in subsequent debugging. Hovering over points in the replay buffer dynami-cally updates the generated state viewport (4.1.1), and shows the rendered image for the corresponding state(animation depicted using overlay).
Figure 4: Distribution Viewport Using the lasso tool to select a group of points (dashed gray line) inthe replay buffer viewport (4.2.1), dynamically updates (dashed red line) the distribution viewport (4.2.2) bycomputing and plotting the distribution of values for the specified tensor (e.g. actions or rewards).
Figure 5: Trajectory Viewport Selecting points in the replay buffer viewport (4.2.1), causes the trajectoryviewport (4.2.3) to dynamically update and plot the absolute normalized TD error values over the length ofthe trajectory. Hovering over points in the trajectory viewport, allows the user to view a rendering of the statecorresponding to that timestep in the generated state viewport (4.1.1).
Figure 6: Vizarel Workflow Diagram Typical steps during policy debugging, and how the designed systemfits into this workflow. The system takes as input a policy saved during a checkpoint and evaluates the policythrough a specified number of rollouts. This data is then visualized through viewports specified by the user,that are used for debugging the policy through making guided changes.
Figure 7: Spatio-Temporal Interaction Visual-izing the replay buffer viewport (4.2.1) (spatial view),and trajectory viewport (4.2.3) (temporal view), alongwith overlays to independently track image renderingsof states in both as a state space viewport (4.1.1). Nav-igating between these viewports allows the user to ob-serve both agent spatial and temporal behavior, whichcould facilitate better insights during debugging.
Figure 8: Comparing TD error along an agent trajectory Visualizing the trajectory viewport (4.2.3),allows the user to compare the TD error at different timesteps along the trajectory, along with the associatedstate viewport (4.1.1). An example interaction is visualized here by hovering over regions of potential interestin the trajectory viewport. This simultaneous view allows the user to easily compare and draw similaritiesbetween action sequences which cause large changes in TD error.
Figure 9: Tensor Comparison Viewport Selecting points (dashed green line) in the replay buffer viewport(4.2.1), and generating (dashed red line) the tensor comparison viewport, allows the user to compare tensors(e.g. actions or states), where dimensions of higher variance are automatically highlighted. This could lead tofaster debugging in environments where each dimension corresponds to physically intuitive quantities.
Figure 10: Visualizing the replay buffer for hard exploration tasks Tasks such as Montezumaâ€™srevenge are classic examples of hard exploration tasks. Here we show how the replay buffer viewport, can helpvisualize the distribution of data samples in the replay buffer.
