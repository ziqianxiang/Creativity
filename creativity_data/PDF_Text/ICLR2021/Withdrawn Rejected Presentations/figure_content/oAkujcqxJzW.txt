Figure 1: Examples of Flexibility Networksapplied (Feng et al., 2017). Therefore, researchers have mostly focused on finding general guide-lines and heuristics (Jordan & Graves, 1995; Chou et al., 2010; Simchi-Levi & Wei, 2012; Fenget al., 2017). While the heuristics in literature are generally effective, in many applications, even aone percent gain in a FDP solution is highly significant (Jordan & Graves, 1995; Chou et al., 2010;Feng et al., 2017) and this motivates us to investigate reinforcement learning (RL) as a new approachfor solving FDPs. In addition, FDPs fall into the class of two-stage mixed-integer stochastic pro-grams, which often arise in strategic-level business planning under uncertainty (see, e.g., Santosoet al. (2005)). Therefore, RL’s success in FDPs can lead to future interest in applying RL to morestrategic planning problems in business. We believe that RL can be effectively applied in FDPsand more strategic decision problems for two reasons. First, RL is designed to solve problems withstochastic rewards, and the stochasticity of strategic planning problems may even help RL to exploremore solutions (Sutton et al., 2000). Secondly, the heuristics mostly relied on, and therefore, are alsoconstrained by human intuitions, whereas RL algorithms with deep neural networks may uncoverblind spots and discover new designs, or better, new intuition altogether similar to the successfulapplications of RL in Atari games and AlphaGo Zero (Silver et al., 2017).
Figure 2: Comparison of RL with other heuristic methods over four test scenarios.
Figure 3: Training curves with different values Ω with and without VR.
Figure 4: Training curves for meta-training, and fast adaptions.
Figure 5: Training curve for different values of K.
