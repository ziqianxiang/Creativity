Figure 1: Left, the wall observation and cue received by the network at each timestep. Right, theentangled predictive task the LSTM network is pre-trained on in order to generate a non-metric mapof the maze environment.
Figure 2: For the joint task to be learned by the LSTM network, we introduce secondary cue points,where the same cue tone as that played at the primary cue point will be repeated if and only if theagent has proceeded in turning in the direction corresponding to the cue tone frequency given at theprimary cue location. The agent is free to choose the next action to be taken when traversing themaze at either the choice point at the top of the stem of the maze or at the secondary cue locations.
Figure 3: Left: Success rate (proportion of direct traversals to reward locations) of each set oftraining paradigms on the reward task, averaged over 10 initial conditions and random wall coloursusing optimal rate of epsilon decay for each paradigm, each shown with a 95% confidence interval.
Figure 4: A-D) Top row: Activity maps showing well isolated place fields of four LSTM units(acting as place cells) indicated in dotted regions after the reward task. Place fields determined bycontiguous locality with average activity exceeding 30% peak unit activity during a single left tra-jectory followed by a right trajectory. Bottom row: LSTM unit activity exceeding 60% of previouslyaveraged peak unit activity for the given neuron when agent run from bottom of maze stem to topof stem and given a low frequency (left) cue tone halfway up the stem, then stationary at choicepoint with LSTM network repeatedly receiving observation from choice point for timesteps there-after (shown in addition to previously determined unit place fields in dotted regions). A, B) Strongextrafield firing contiguously from cue to choice point. C, D) High extrafield firing at choice pointwhile agent is paused at top of stem. E) Place fields (determined from average activity on bothtrajectories) of four LSTM units outlined in dotted areas after reward based task. High levels ofconsistent extrafield firing at primary and secondary cue points in 56% of LSTM units.
Figure 5: LSTM inferred agent position after pre-training on maze. The agent is run from the start attimestep 1 to timestep 4 where it receives a low frequency cue (indicating a left turn). At timestep 9the agent is stopped at the top of the maze stem and the LSTM is given the environment observationfrom this location for the remainder of the shown timesteps. The inferred position then moves leftaccording to the cue with the position seeming to jump abruptly between timesteps 15 and 16. Theinferred position then moves back to the starting position at timestep 26. We observe an analogousinferred forward moving representation on the right side of the maze with a high frequency cue.
Figure 6: LSTM representation after reward training. As previously, we run the agent from thestart position to the top of the stem of the maze at timestep 9 with a low frequency (left) cue toneat timestep 4. Again, the agent is stopped at this position with the LSTM network receiving theenvironment observation from this position for the remainder of the shown timesteps. As with thenetwork purely trained on the predictive task, the representation moves in the direction correspond-ing to the frequency of the given cue tone. Then between timesteps 14 and 15, the inferred positionjumps from the return arm with active reward sites to the alternate arm, with the inferred positionmoving from this position to the start location fairly consistently. Then the inferred position jumpsagain at timestep 32 to the rewarding return arm and moves constantly to the start position.
Figure 7: UMAP manifold of LSTM network dynamics of complete left trajectory (dark blue) andcomplete right trajectory (red) shown along with manifold of dynamics when agent run from startlocation to choice point with left cue (light blue) and right cue (pink) given at cue point and agentpaused in place at the top of the maze stem. A few timesteps after the agent is paused, the dynamicsof the left cue paused agent (light blue) switches manifold path abruptly from running alongsidethe complete left trajectory path (blue) and joins the right trajectory path (red), following this formany timesteps before ultimately resulting at the same manifold end position as the complete lefttrajectory manifold path (blue). This is analogous for the right cue paths (red and pink).
Figure 8: Histograms showing LSTM unit discrimination index for turn direction selectivity (DIturn)vs task phase selectivity (DIphase). A highly negative selectivity index for turn direction indicates aneuronal unit which exhibits high levels of selectivity (uniquely high network activity) for a leftwardtrajectory and a highly positive selectivity index indicates selectivity for a rightward trajectory. Anegative selectivity for task phase indicates a neuron which is highly selective for the choice (re-trieval) phase of the goal based task whereas a positive index indicates a neuron which is highlyselective for the cue (encoding) phase of the task.
Figure 9: A, B, C) Top row: well isolated place fields of three LSTM units indicated in dotted regionsafter pre-training. Place fields determined by contiguous locality with average activity exceeding30% peak field activity during a single left trajectory followed by a right trajectory. Bottom row:agent run from bottom of maze stem to top of stem (and given a low frequency cue tone halfway upthe stem) and paused at choice point with LSTM network repeatedly receiving observations fromchoice point for timesteps thereafter. A) Strong extrafield firing at choice point with some activity atthe cue point. B) Extrafield activity at choice point and at position below. C) High extrafield firingat cue point before agent pauses at top of stem.
Figure 10: Place fields of four LSTM units, starting from i = 0 where the network has been pre-trained on the sensory prediction task, drifting forwards towards reward locations throughout rewardtraining (where i is the number of training iterations). The place fields ultimately rest at maze rewardlocations at the end of reward training (i = 1400).
