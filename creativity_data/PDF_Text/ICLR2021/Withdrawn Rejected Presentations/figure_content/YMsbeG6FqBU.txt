Figure 1: A part of Kuhn poker. Terminal utilities shown for the first player.
Figure 2: An average reward per modulations scored against opponent πt as a function of time(measured in acting steps). The brown curve is a random uniform policy (i). Cyan, orange and blueare (ii) with ∈ 0.0, 0.01, 0.05 respectively. Pink, green and yellow are (iii) with ∈ 0.0, 0.01, 0.05.
Figure 3: NFSP and MC-RCFR on the Leduc Poker, II-GoofSpiel with 5 cards and Liars DiceWe compare empirical convergence to approximate Nash equilibria using a model-free sampledform of regression CFR (34) (MC-RCFR). Trajectories are obtained using outcome samplingMCCFR (20), which uses off-policy importance sampling to obtain unbiased estimates ofimmediate regrets r, and average strategy updates s, and individual (learned) state-action1 T	/Cf ，	1	∙	ʌ	T i A 1	T ∙	1 i ∙	1baselines (27) to reduce variance. A regressor then predicts R and a policy is obtainedvia Eq. 1, and similarly for the average strategy. Each episode, the learning player i playswith an -on-policy behavior policy (while opponent(s) play on-policy) and adds everydatum (s,r,π(R)) to a data set, D, With a retention rule based on reservoir sampling soit approximates a uniform sample of all the data ever seen. MC-RCFR is related, but notequivalent to, a variant of DeepCFR (5) based on outcome sampling (OS-DeepCFR) (31).
Figure 4: ARMAC results on Leduc, II-Goofspiel, and Liar’s Dice. The y-axis is NashConv of theaverage strategy ∏t. The x-axis is number of epochs. One epoch consists of 100 learning steps. Eachlearning step processes 64 tra jectories of length 32 sampled from replay memory. The final valuereached by the best runs are 0.18 (Leduc), 0.5 (II-Goofspiel), and 0.095 (Liar’s Dice).
Figure 5: ARMAC results in No-Limit Texas Hold’em trained with FCPA action abstractionevaluated using LBR-FC metric. The y-axis represents the amount LBR-FC wins agains theARMAC-trained policy. The x-axis indicate days of training. The left graph shows the learningcurve in a linear scale, while the right one shows the same curve in a log-log scale.
Figure 6: The (a) Multi-headed network architecture, and (b) Exploration example.
Figure 7: Performance on Breakout (left) and Montezuma Revenge (right). Results are shown fortwo seeds.
