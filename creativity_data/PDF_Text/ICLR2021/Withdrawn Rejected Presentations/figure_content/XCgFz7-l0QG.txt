Figure 1: (a) Minimization Step: Using the teacher model knowledge for training the student inKD (utilizing forward knowledge) (b) Maximization Step: Augmenting the input dataset x withauxiliary data samples x0 which is generated by the back propagation of gradient through bothnetworks (utilizing backward knowledge)maps (Sun et al., 2019; Sun et al.; Jiao et al., 2019), gradients of the network outputs w.r.t theinputs (Czarnecki et al., 2017; Srinivas & Fleuret, 2018)), and matching decision boundaries forclassification tasks (Heo et al., 2019). using this additional information might be useful to get thestudent network performance closer to that of the teacher.
Figure 2: Visualizing the data insufficiency issue for the original KD algorithm. (a) behaviour ofthe teacher and the student function after training with KD loss. (b) divergence areas between theteacher and the student networks. (c) behaviour of l2 - norm loss function between teacher and thestudent and the idea of obtaining auxiliary data samples.
Figure 3: General procedure of utilizing auxiliary samples in NLP models. Here x is the one-hotvector of input tokens, W is the embedding matrix, and z is the embedding vector of x.
Figure 4: Visualizing the generation of auxiliary samples and their utilization in training the studentmodel.
