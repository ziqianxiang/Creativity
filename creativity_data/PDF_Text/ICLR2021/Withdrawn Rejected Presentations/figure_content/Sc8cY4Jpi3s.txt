Figure 1: Timeline illustrating the design of the optimization algorithm. Preconditioner statistics (Lt and Rt)are computed at each step by the accelerators. Preconditioners (L 1/4 and R 1/4) are only computed every N stepsand this computation is distributed to all available CPU cores.
Figure 2: Results for a Transformer model on WMT'14 en→fr, trained with batch size of 1536. (a) Testlog-perplexity vs. number of steps; the algorithm converges 1.95x faster in steps, while being only ≈ 16%slower per step. This allows the method to attain a particular log-perplexity in 40% less wall-time. (b) Detailedbreakdown of latency of a single step (Appendix G.6). Diagonal AdaGrad optimizer: 134ms, Shampoo: 145ms(all layers except embedding and softmax layers) and 155ms (all layers). Preconditioner computation is pipelinedand distributed over CPUs, thus not adding any overhead, and transfer latency (≈100ms) is amortized overhundreds of steps.
Figure 3: Impact of Shampoo extensions on WMT’14 en→fr training: (a) preconditioning applied to all layersexcept embedding and softmax layers, vs. applied to all layers; (b) preconditioning with fully-connected layerspartitioned into sub-blocks; (c) varying interval between preconditioner updates.
Figure 4: Test log-perplexity of a Transformer-Big model on WMT’14 en→fr. (a) Shampoo converges fasterthan AdaGrad (≈ 2x faster in steps), and allows larger learning rates; due to the large overhead in step time, thisresults in only 30% improvement in wall-time. (b) Larger batch sizes reduce the optimizer overhead from 40% to19%, resulting in an end-to-end improvement of 41% in wall-time for convergence.
Figure 5: (a) Shampoo reaches a target AUC of 80.25% in half as many steps with preconditioning embeddinglayers improving the results, and achieves a new state-of-the-art AUC of 80.56%; (b) Shampoo converges in≈ 16% fewer steps, and achieves ≈ 1% higher MLM accuracy than the baseline on BERT-Large.
Figure 6: Benchmarks on computing inverse-pth root for statistics of varying dimensions (left), and the conditionnumbers for Lt of a layer in the transformer model over time (right). We find that the coupled Newton iterationmethod can effectively utilize the CPUs and give large walltime improvements compared to SVD (that relies onbidiagonal divide-and-conquer). These were measured without warmstart which provides additional speedup ofupto 4x by reducing the number of iterations to the solution.These were measured on Intel Skylake CPUs. Notethat since 〜log2( P K(Lt)) bits of precision are lost in computing P-th roots, 64-bit arithmetic becomes necessary.
Figure 7: Minimum (dashed) and maximum (solid) singular values for statistics matrices of theembedding, softmax and intermediate attention query projection layers.
