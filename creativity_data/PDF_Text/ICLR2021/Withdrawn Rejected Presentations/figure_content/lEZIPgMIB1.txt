Figure 1: Overview of UMAP (A → B) and Parametric UMAP (A → C). (A) The first stage ofthe UMAP algorithm is to compute a probabilistic graphical representation of the data. (B) Thesecond stage of the UMAP algorithm is to optimize a set of embeddings to preserve the structure ofthe fuzzy simplicial complex. (C) The second stage of UMAP, learning a set of embeddings whichpreserves the structure of the graph, is replaced with a neural network which learns a set of neuralnetwork weights (parameters) that maps the high-dimensional data to an embedding. Both B and Care learned through the same loss function.
Figure 2: An outline of the varients of UMAP used in this paper. Solid lines represent neuralnetworks. Dashed lines represent error gradients.
Figure 3: An example of semi-supervised learning with UMAP on the moons dataset. (A) A de-cision contour learned over the moons dataset with 3 labeled datapoints from each class. Unlableddatapoints are shown in grey and labeled datapoints are shown in red and blue. The decision contouris shown in the background using the ’coolwarm’ colormap. (B) The learned embeddings in thejointly trained network. (C) UMAP loss over the unlabeled training dataset. (D) Classifier accuracyfor the training and validation set.
Figure 4: Example comparison of 2D projections from Parametric UMAP and each baseline. (A)3D buffalo. (B) Mouse retina single cell transcriptomes. (C) Fashion MNIST. 2D projections of theremaining datasets are given in Fig. 12.
Figure 5: Training times comparison between UMAP and Parametric UMAP. All results were ob-tained with up to 32 threads on a machine with 2 AMD EPYC Rome 7252 8-Core CPU running at3.1 GHz and a Quadro RTX 6000.
Figure 6:	Comparison of embedding speeds using parametric UMAP and other embedding algo-rithms on a held-out testing dataset. Embeddings were performed on the same machine as Figure11. Values shown are the median times over 10 runs.
Figure 7:	Reconstruction accuracy measured as mean squared error (MSE). MSE is shown relativeto each dataset (setting mean at 1).
Figure 9: Baseline classifier with an additional UMAP loss with different numbers of labeled train-ing examples. Non-parametric UMAP projections of the UMAP graph being jointly trained areshown in the bottom right of each panel. Error bars show SEM.
Figure 10: Comparison of baseline classifier, augmentation, and augmentation with an additionalUMAP loss.
Figure 11: SSL using UMAP over the learned latent graph, computed over latent activations in theclassifier.
Figure 12: Comparison of projections from multiple datasets using UMAP, UMAP in Tensorflow,Parametric UMAP, Parametric UMAP with an Autoencoder loss, Parametric t-SNE, t-SNE, a VAE,an AE, and PCA. (a) Moons. (B) 3D buffalo. (c) MNIST (d) Cassin’s vireo song segments (e) Mouseretina single cell transcriptomes. (f) Fashion MNIST (g) CIFAR10. The Cassin’s vireo dataset usesa dynamic time warping loss and an LSTM network for the encoder and decoder for the neuralnetworks. The image datasets use a convnet for the encoder and decoder for the neural networks.
Figure 13: Trustworthiness scores for five datasets using 2- and 64-dimensional projections usingeach projection method. 64-dimensional t-SNE is not shown due to limitations in high-dimensionalprojections with t-SNE. Trustworthiness is computed over 10,000 samples of the training dataset.
Figure 16: Silhouette scores for five datasets using 2- and 64-dimensional projections using eachprojection method. 64-dimensional t-SNE is not shown due to limitations in high-dimensional pro-jections with t-SNE.
Figure 17: Clustering results. Comparisons are based upon the Normalized Mutual Information(NMI) between labels and clusters. Each dataset shows the NMI for the best clustering chosen onthe basis of its silhouette score.
Figure 18: Reconstruction speed. Reconstructions are performed on the same machine as in Fig 6.
Figure 19: Non-parametric UMAP projections of activations in the last layer of a trained classifierfor MNIST, FMNIST, and CIFAR10. For each dataset, the top row shows the ground truth labels onabove, and the model’s predictions below, in a light colormap. On top of each projection, the labeleddatapoints used for training are shown in a darker colormap.
