Figure 1: Schematic illustration of our proposed approach. (a) We train a neural network usingstandard techniques to obtain a point estimate of the weights. (b) We identify a small subset of theweights. (c) We estimate a posterior distribution over the selected subnetwork via Bayesian inferencetechniques. (d) We make predictions using the full network of mixed Bayesian/deterministic weights.
Figure 2: Predictive distributions (mean ± std) for 1D regression. The numbers in brackets denotethe number of parameters over which inference was done (out of 2600 in total). WaSSerStein-basedsubnetwork inference maintains richer predictive uncertainties at smaller parameter counts.
Figure 3: Mean test log-likelihood values obtained on UCI datasets across all splits. Differentmarkers indicate models with different numbers of weights. The horizontal axis indicates the numberof weights over which full covariance inference is performed. 0 corresponds to MAP parameterestimation, and the rightmost setting for each marker corresponds to full network inference.
Figure 4: Results on the rotated MNIST (left) and the corrupted CIFAR (right) benchmarks ofOvadia et al. (2019), showing the mean ± std of the error (top) and log-likelihood (bottom) acrossthree different seeds. Subnetwork inference retains better uncertainty calibration and robustness todistribution shift than point estimated networks and other Bayesian deep learning approaches.
Figure 5: Rotated MNIST (left) and Corrupted CIFAR10 (right) results for deep ensembles (Lak-shminarayanan et al., 2017) with large numbers of ensemble members (i.e. up to 55). Horizontalaxis denotes number of ensemble members, and vertical axis denotes performance in terms of log-likelihood. Straight horizontal lines correspond to the performance of our method, as a reference.
Figure 6:	Rejection-classification plots. We simulate a realistic OOD rejection scenario (Filos et al.,2019b) by jointly evaluating our models on an in-distribution and an OOD test set. We allow ourmethods to reject increasing proportions of the data based on predictive entropy before classifyingthe rest. All predictions on OOD samples are treated as incorrect. Following (Nalisnick et al., 2019),we use CIFAR10 vs SVHN and MNIST vs FashionMNIST as in- and out-of-distribution datasets,respectively. Note that the SVHN test set is randomly sub-sampled down to a size of 10,000.
Figure 7:	Full MNIST rotation and CIFAR10 corruption results, for ResNet-18, reporting predictiveerror, log-likelihood (LL), expected calibration error (ECE) and brier score, respectively (from top tobottom).
Figure 8:	MNIST rotation results for ResNet-50, reporting predictive error, log-likelihood (LL),expected calibration error (ECE) and brier score. We choose a subnetwork containing only 0.167%(39,190 / 23,466,560) of the parameters of the full network. We see that subnetwork inference stillresults in an improvement in the calibration of predictive uncertainty. As expected, however, forResNet-50 the improvement over MAP is smaller than for ResNet-18 where we were able to choosea subnetwork containing 0.38% of the parameters.
