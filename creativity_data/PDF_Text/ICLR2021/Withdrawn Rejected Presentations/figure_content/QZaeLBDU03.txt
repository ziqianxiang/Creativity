Figure 1: The defender starts with an uniform random strategy (-0); it switches to a possibleconfiguration of a software system with equal probability in each state. Then, the defender, uponinteractions with an environment and simulation of an adversary in their head, adapts its strategy atevery step and finally converges to the Strong Stackelberg Eq. (SSE) of the BSMG yielding x*.
Figure 2: The defenderâ€™s value in the four states of the BSMG modeling the MTD for web-applicationswhen using BSS Q-learning (blue) compared to Uniform Random Strategy (orange) (34), a Bayesianversion of EXP-Q learning (green) (13) and the optimal stage-agnostic strategy (red) (6).
Figure 4: Values in the BSMG-based MTD for IDS placement when using BSS Q-learning (blue)compared to Uniform Random Strategy (orange), EXP-Q learning (green) and Nash Q-learning (red).
Figure 3: Run-time of the learn-ing agents for 80 episodes.
Figure 5: Situating BSMGs in the land-scape of game-theoretic models that cap-ture incomplete information.
Figure 6: Comparing BBS-Q with URS (orange) and EXP-Q learning (green) on MTD for web-applications with switching costs incorporated in the transition function of BSMGs.
Figure 7: An example cloud system highlighting its network structure, the attacker and defender(admin) agents and the possible attacks and monitoring mechanisms (20).
