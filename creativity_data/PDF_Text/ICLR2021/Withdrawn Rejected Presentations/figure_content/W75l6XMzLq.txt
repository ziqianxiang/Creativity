Figure 1: Tasks in the open-ended environment with sparse binary rewards. They include Pushing,Sliding and Pick-and-Place with a Fetch robotic arm as well as different in-hand object manipula-tions with a Shadow Dexterous Hand. The agent obtains a reward of 1 if it achieves the desired goalwithin some task-specific tolerance and 0 otherwise.
Figure 2:	Learning curves for variants with multiple goals. All curves shown are trained with defaultDDPG hyper-parameters. It shows that at an early stage, our method (labeled MultiGoalER) learnsmore efficiently and can maintain a stable high success rate.
Figure 3:	Ablation studies on hyper-parameter L. All curves shown are trained with default DDPGhyper-parameters.
