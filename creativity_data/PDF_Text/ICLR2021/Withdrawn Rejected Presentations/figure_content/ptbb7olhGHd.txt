Figure 1: Tweets pertaining to a company were collected from Twitter (1) and passed through FinBERT, afinancial sentiment classifier, to produce 10 sentiment based features (2). Price data from Yahoo finance wasthen collected to use as targets (3). We use the price data and sentiment features to create a dataset (4). Wethen train probabilistic forecasting models (5) on the collected dataset. Lastly, we adversarially manipulateTwitter based features in our testing set to obtain an adverserial testing set (6) using a gradient attack method(7). We then test the original and adversarial stock forecasts (8) using several metrics that simulate a portfolio’sperformance to determine the robustness of our model and data.
Figure 2: Log return of Tesla stock over our testing set for DeepAR-ST. Note the similar but shifted shape ofour prediction and the target.
Figure 3: Probability distribution of log return of Tesla stock over our testing set for GPVAR-3. The faintgreen line within the probability distribution represents the mean prediction of 100 samples of our distribution.
Figure 4: Log return of Tesla stock over our testing set for DeepAR-G. Compared to DeepAR-ST, the predic-tions have a lower variance due to Gaussian distribution having smaller tails then Student-t distribution.
Figure 5: Plotting the normal and adversarial versions of the covariates used in the testing set. From top tobottom, the plots represent the following covariates: general sentiment score, volume of tweets, average likes,average retweets, positive sentiment score, negative sentiment score, neutral sentiment score, percentage ofpositive tweets, percentage of negative tweets and percentage of neutral tweets. The goal of the perturbationhere was to decrease the mean of DeepAR-G. Note the perturbations of neutral score during between day 10 to35, or average retweets at around day 65 or 82.
Figure 6: Change in predictions after adversarially decreasing the mean of DeepAR-G.
Figure 7: Log return of Tesla stock across our whole dataset. A red vertical line separates the training andtesting sets. Note the difference in average magnitude of log returns in the training set compared to the testingset.
Figure 8: Overview of the DeepAR model (figure taken from Salinas et al. (2020)). Inputs zi,t-1and xi,t as well as the previous RNN hidden state hi,t-1 are fed to the RNN’s current state tocompute hi,t for each time step t. The RNN’s output is then mapped to the parameters φi,t governingthe likelihood function l(zi,t∖φi,t) associated with a specific distributional assumption over zi,t.
