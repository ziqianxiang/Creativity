Figure 1: Sampling-based uncertainty mechanisms on toy datasets. In contrast to MC dropout (left),the second-moment loss (right) induces uncertainties that capture aleatoric uncertainty. The groundtruth data is shown in red. Each grey line represents the outputs of one of 200 random sub-networksthat are obtained by applying dropout-based sampling to the trained full network. For details on thedata sets (‘toy-hf’, ‘toy-noise’), the neural architecture and the uncertainty methods please refer tosection 4 and references therein.
Figure 3: Expected calibration errors (ECEs) of different uncertainty methods under i.i.d. conditions(first and second panel) and under various kinds of data shift (third to sixth panel, see text for details).
Figure 4: Shown is the value of the loss component L2 as given by eq. (3) over μdrop and σdropdescribing the implicit dropout ensemble. The blue line shows the position of the minima of L2for fixed values of σdrop. Clearly visible are the global minima at σdrop = 0 and the bifurcation atσdrop = 2/n.
Figure 5: The second-moment loss induces uncertainties σtotai = σdr0p + ∣fθ - h%)|. The relativecontribution of both components ("fraction_dropout_std", "fraction_spread")is shown for three ex-emplary datasets (top: toy-noise, middle: superconduct, bottom: protein) and i.i.d. (train: blue, test:orange) as well as non-i.i.d. data splits (test-label: red, test-pca: yellow).
Figure 6: Visualisation of the components (columns) of the second-moment loss for selected testdatasets (rows). The prediction residual fθ(Xi) - yi (first column), model spread fg(xi) - fθ (Xi)(second column), a scatter plot ofboth quantities (third column) and Ifg(Xi) — fθ (Xi) ∣-∣fθ(Xi) — yi|(fourth column) are shown. The chosen datasets from top to bottom are: toy-hf, wine-red, power,california.
Figure 7: Scheme of two non-i.i.d. splits: a PCA-based split in input space (left) and label-basedsplit in output space (right). While datasets appear to be convex here, they are (most likely) not inreality.
Figure 8: Root-mean-square errors (RMSEs) of different uncertainty methods under i.i.d. conditions(first and second panel) and under various kinds of data shift (third to sixth panel, see text fordetails). SML (‘ours’) is compared to 6 benchmark approaches. Each blue cross is the mean overRMSE values from 13 UCI regression datasets. Orange line markers indicate median values. Thegray vertical bars reach from the 25% quantile (bottom horizontal line) to the 75% quantile (tophorizontal line).
Figure 9: Negative log-likelihoods (NLLS) of different uncertainty methods under i.i.d. conditions(first and second panel) and under various kinds of data shift (third to sixth panel, see text for details).
Figure 10: Wasserstein distances of different uncertainty methods under i.i.d. conditions (first andsecond panel) and under various kinds of data shift (third to sixth panel, see text for details). SML(‘ours’) is compared to 6 benchmark approaches. Each blue cross is the mean over WS values from13 UCI regression datasets. Orange line markers indicate median values. The gray vertical barsreach from the 25% quantile (bottom horizontal line) to the 75% quantile (top horizontal line).
Figure 11: Prediction residuals (x-axis) and predictive uncertainty (y-axis) for a hypothetical idealuncertainty mechanism. The Gaussian errors are matched by Gaussian uncertainty predictions at theexact same scale. 68.3% of all uncertainty estimates (plot points) lie above the orange 1σ-lines and99.7% of them above the blue 3σ-lines.
Figure 12: Prediction residuals (respective x-axis) and predictive uncertainty (respective y-axis) fordifferent uncertainty mechanisms (columns) and datasets (rows). Each light blue dot in each plotcorresponds to one test data point. Realistic uncertainty estimates should lie mostly above the blue3σ-lines.The datasets toy-noise, naval, abalone and superconduct are shown, from top to bottom.
Figure 13: Expected calibration errors (ECEs) for SML-trained networks with hyper-parametersβ = 0.1, 0.25, 0.5, 0.75, 0.9 under i.i.d. conditions (first and second panel) and under various kindsof data shift (third to sixth panel, see text for details). Each blue cross is the mean over ECE valuesfrom 13 UCI regression datasets. Orange line markers indicate median values. The gray verticalbars reach from the 25% quantile (bottom horizontal line) to the 75% quantile (top horizontal line).
Figure 14: Wasserstein distances for SML-trained networks with hyper-parameters β =0.1, 0.25, 0.5, 0.75, 0.9 under i.i.d. conditions (first and second panel) and under various kinds ofdata shift (third to sixth panel, see text for details). Each blue cross is the mean over WS valuesfrom 13 UCI regression datasets. Orange line markers indicate median values. The gray verticalbars reach from the 25% quantile (bottom horizontal line) to the 75% quantile (top horizontal line).
Figure 15: Dependencies between the three uncertainty measures ECE, Wasserstein distance andKolmogorov-Smirnov distance. Uncertainty methods are encoded via plot markers, data splits viacolor. Datasets are not encoded and cannot be distinguished (see text for more details). Each plotpoint corresponds to a cross-validated trained network. The clearly visible deviations from idealcorrelations point at the potential of these uncertainty measures to complement one another.
