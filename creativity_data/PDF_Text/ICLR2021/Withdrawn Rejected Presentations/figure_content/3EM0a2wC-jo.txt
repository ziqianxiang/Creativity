Figure 1: Architecture of the DAF-Net. Grey boxes represent fixed computations; white boxes representneural networks with adjustable parameters; those with internal vertical bars represent a replication of the samecomputation on slot values in parallel. Red lines indicate information derived from an input observation, greenlines indicate information derived some hypothesis slots values, and blue lines indicate information derived fromcounts on each hypothesis slot.
Figure 2: Visualizations of Dynamic and Gaussian domains. Observations are transparent while while groundtruth states are boldedModel	Slots	Ground Truth Clusters		äºŒ	Z		3	5	7	10	0.162 (0.001)	0.214 (0.001)	0.242 (0.001)DAF-Net	20	0.175 (0.001)	0.195 (0.001)	0.213 (0.001)	30	0.188 (0.001)	0.197 (0.001)	0.205 (0.001)Set Transformer	-	0.261 (0.001)	0.279 (0.001)	0.282 (0.001)Vector Quantization	-	0.171 (0.001)	0.199 (0.001)	0.205 (0.001)Table 3: Quantitative evaluation of DAF-Net on distributions with different numbers of true components andhypothesis slots at test time with 30 observations. In all cases, DAF-Net is trained with 3-component problems,10 slots, and 30 observations. We compare with an offline set transformer trained with different numbers ofproblem components as well as with vector quantization.
Figure 3: Results on two image-based association tasks (left: MNIST, right: airplanes). At the top of eachis an example training problem, illustrated by the true objects and an observation sequence. Each of thenext rows shows an example test problem, with the ground truth objects and decoded slot values. The threehighest-confidence hypotheses for each problem are highlighted in red, and correspond nicely to the ground-truthobjects.
Figure A1: Plots of inferred number of components using a confidence threshold in DAF-Netcompared to the ground truth number of clusters (DAF-Net is trained on only 3 clusters). We considertwo scenarios, a noisy scenario where cluster centers are randomly drawn from -1 to 1 (left) and ascenario where all added cluster components are well seperated from each other (right). DAF-Net isable to infer more clusters in both scenarios, with better performance when cluster centers are moredistinct from each other.
Figure A2: Illustration of the clustering process. Decoded value of hypothesis (with size correspond-ing to confidence) shown in red, with ground truth clusters in black. Observations are shown inblue.
Figure A3: Plot of slots (left), and what slot each input assignsthe highest attention towards (right) (each slot is colored differently,with assigned inputs colored in the same way). Note alignment ofregions on the right with point density on the left.
Figure A4: Plots of the magnitude ofrelevance weights with increased obser-vation number on different distributionswith higher standard deviation (noise).
Figure A5: Architecture of different models.
Figure A6: Architectures of encoder and decoder models on image experiments.
