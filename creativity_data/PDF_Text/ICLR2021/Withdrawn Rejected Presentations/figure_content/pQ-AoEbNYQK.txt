Figure 1: An overview of DiffAutoML. We first sample the DA operations for each sample basedon the data transformation parameters τ . Then, a child network is sampled based on the architectureparameters α, which will be used to process transformed mini-batch. Training loss is calculated toupdate the neural network parameter θ by an optimizer with the hyper-parameters η . Parameters τand α are updated based on the training loss, while η is updated with the validation loss to achievedifferentiable end-to-end joint optimization of automated DA, NAS and HPO.
Figure 2: Comparison of validation accuracy of DSNAS and DiffAutoML over (a) the trainingepochs and (b) the run time. NAS optimization in DiffAutoML can be divided into searching stagewhere α is optimized with others, and tuning stage where α is fixed while other modules are stillupdated.
Figure 3: Searched architectures with different AutoML components. The network architecture isdivided into foUr components shown in the above figUre with the help of the dotted line. The firstblock of each component has the stride nUmber of 2 and the rest blocks have the stride nUmber of 1.
Figure 4: Choice blocks in search space. From left to right are Choice_1, Choice_2, Choice_3, andChoice _4.
