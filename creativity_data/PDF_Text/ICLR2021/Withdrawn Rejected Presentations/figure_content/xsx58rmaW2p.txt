Figure 1: The evolution of alignment of per-example gradients of a ResNet-18 during training asmeasured with m-coherence on samples of size m = 50,000 on 3 variants of ImageNet with differentamounts of label noise. Our main finding is that coherence not only decreases in the course of training(as expected from Lemma 4 when examples get fit), but it also increases. A high peak is reachedrapidly with real labels (in the first 100 steps) and a much lower peak is reached slowly with randomlabels (over many epochs). Horizontal lines for m-coherence are shown at 1 (the orthogonal limit)and at m. Vertical lines indicate sharp reductions in learning rate. Light dots show the results of 4other runs to understand sensitivity w.r.t. randomness in initialization and mini-batch construction.
Figure 2: The early trajectory of an EfficientNet-B0model shows a similar increase as ResNet. The overalltrajectories are also similar, and it is interesting to notethat we get similar values for coherence although thetwo architectures are very different.
Figure 3: The evolution of alignment of per-example gradients of a EfficientNet-B0 network duringtraining as measured with m-coherence on samples of size m = 50,000 on 3 variants of ImageNetwith different amounts of label noise. We note that the results are qualitatively in agreement withwhat we see on ResNet, i.e., in both real and random cases we see coherence increase. A high peak isreached rapidly with real labels (in the first 150 steps) and a much lower peak is reached slowly withrandom labels (over many epochs). Horizontal lines for m-coherence are shown at 1 (the orthogonallimit) and at m. Vertical lines indicate sharp reductions in learning rate.
Figure 4: The evolution of alignment of per-example gradients of a ResNet-18 during training asmeasured with m-coherence on samples of size m = 50,000 on 3 variants of ImageNet with differentamounts of label noise. This figure is similar to Figure-1, but also includes m-coherence plots forImageNet test dataset (in column 1 and 2).
Figure 5: The expected gradients in the numerator and denominator for α (not m-coherence)corresponding to Figure 1. Note that even when the expected gradient is flat (as may be inferred evenfrom the slope of the loss function), there is activity in the denominator which gets picked up with αor m-coherence particularly, if the scale is set appropriately w.r.t. to the orthogonal limit.
Figure 6: To understand the effect of m for the values of α and m-coherence, we plot these valuesfor m = 50,000 and m = 100,000 for the ResNet-18 training on 0% noise. In both plots we showhorizontal lines for the orthogonal limit (which is different for the two samples in the α plot since it is1/m, but the same in the m-coherence plot since it is 1 in both cases) and the perfect alignment case(which is the same in the α plot since it is 1, and is different in the m-coherence plot since it is m.)5 4 3 2 1 0OooooouaJqo，区80O 50 IOO 150 200 250 30015.012.510.07.55.02.575% noise(ISt epoch)♦ TrainingTest
Figure 7: Evolution of m-coherence for 25% and 75% label noise (under the same settings asFigure 1). This confirms the pattern discussed in the main text that with increasing noise, the rate atwhich coherence is created in early training slows down.
