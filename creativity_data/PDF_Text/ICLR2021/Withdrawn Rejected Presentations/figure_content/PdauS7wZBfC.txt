Figure 1: Top: Backpropagation on a chain. Backprop proceeds backwards sequentially and explicitlycomputes the gradient at each step on the chain. Bottom: Predictive coding on a chain. Predictions,and prediction errors are updated in parallel using only local information.
Figure 2: Top: The computation graph of the nonlinear test function VL = tan(√θv0) + sin(v2).
Figure 3: Training and test accuracy plots for the Predictive Coding and Backprop CNN onSVHN,CiFAR10, and CiFAR10 dataest over 5 seeds. Performance is largely indistinguishable.
Figure 4: Test accuracy plots for the Predictive Coding and Backprop RNN and LSTM on theirrespective tasks, averaged over 5 seeds. Performance is again indistinguishable from backprop.
Figure 5: Training loss plots for the Predictive Coding and Backprop CNN on SVHN,CIFAR10, andCIFAR10 dataset over 5 seeds.
Figure 8: Training losses for the predictive coding and backprop RNN. As expected, they areeffectively identical.
Figure 9: Computation graph and backprop learning rules for a single LSTM cell.
Figure 10: The LSTM cell computation graph augmented with error units, evincing the connectivityscheme of the predictive coding algorithm.
Figure 11: Divergence between predictive coding and numerical gradients as a function of sequencelength.
Figure 12: Number of iterations to reach convergence threshold as a function of sequence length.
