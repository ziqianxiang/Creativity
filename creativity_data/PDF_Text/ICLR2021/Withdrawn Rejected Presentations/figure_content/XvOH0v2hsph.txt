Figure 1: Rank correlation (with final test accuracy) performance of the sum of training losses,SoTL (blue) and SoTL-E (red), and those of validation losses (purple), SoVL (solid) and SoVL-E(dash dot), as well as that of final test loss (black) for 5000 random architectures in NASBench-201on three image datasets. Note the correlation performances of final test loss and SoVL-E near theend of training get surprisingly poor for CIFAR10/100. We explain this in Section 5.1.
Figure 2: Rank correlation performance of various baselines: SoTL-E, SoTL, SoVL, Val Acc andLcSVR for 5000 random architectures in NASBench-201 on three image datasets (a) to (c) and for552 randomly wired architectures on FLOWERS102 (d). In all cases, our SoTL-E achieves superiorrank correlation with the true test performance in much fewer epochs than other baselines. We shadethe region T > 100; this shaded region is less interesting in NAS where we want to use as fewertraining epochs as possible to maximise the speed-up gain compared to full evaluation T = Tend .
Figure 3: Rank correlation performance of various baselines: SoTL-E, SoTL, SoVL, VAccES, VAc-cES(EMA) and TLmini for 100 random architectures from DARTS search space on CIFAR10. Wetest the three training set-ups used in (Liu et al., 2019): (a) the search phase (over 200 architec-tures), (b) the retraining phase for CIFAR10 and (c) the retraining phase for ImageNet. Our SoTL-Eachieves superior rank correlation in much fewer epochs than other baselines. Note also that the sumof training loss (SoTL/SoTL-E) gives better test performance estimation than individual training loss(TLmini).
Figure 4: Model selection among 69 random graph generator hyperparamters on RandWiredNNdataset using (a) our SOTL-E and (b) VAccES. We use each hyperparameter value to generate 8architectures and evaluate their true test accuracies after complete training. The mean and standarderror of the test performance across 8 architectures for each hyperparameter value are presented asTest Acc (yellow) and treated as ground truth (Right y-axis). We then compute our SoTL-E=1 esti-mator for all the architectures by using their first T < 250 epochs of training losses. The mean andstandard error of SoTL-E scores for T = 10, . . . , 90 are presented in different colours (Left y-axisof (a)). The rank correlation between the mean Test Acc and that of SoTL-E for various T is shownin the corresponding legends in (a). The same experiment is conducted by using early-stoppedvalidation accuracy (VAccES) for performance estimation (b). With only 10 epochs of training, ourSoTL-E estimator can already capture the trend of the true test performance of different hyperparam-eters relatively well (Rank correlation= 0.851) and can successfully identify 24-th hyperparamtersetting as the optimal choice. The prediction of best hyperparameter by VAccES is less consistentand the rank correlation scores of VAccES at all epochs are lower than those of SoTL-EAUeJnUUeκοh-ŋ 5 ŋ 5 O 59 8 8 7 7 6-6070TestAcc σ=250)
Figure 5: NAS performanCe of Regularised Evolution (RE) (Top row) and TPE (Bottom row) inCombined with final validation aCCuraCy (Val ACC (T=200)), early-stopping validation aCCuraCy (ValACC (T=50)) and our estimator SoTL-E on NASBenCh-201. SoTL-E leads to the fastest ConvergenCeto the top performing arChiteCtures in all Cases.
Figure 6: Example on a simple Bayesian linear regression problem. We see that the sum over traininglosses gives an estimator for the lower bound L of model evidence, and that the SoTL measure ismore effective than the final training loss at distinguishing the two models M1 and M2 .
Figure 7: Rank correlation performance of the sum of training losses over E most recent epochs(SoTL-E) on the NASBench-201 dataset. Different E values are investigated for 5000 randomarchitectures in NASBench-201 on three image datasets. In all three cases, smaller E consistentlyachieves better rank correlation performance in the early training phase with E = 1 being the bestchoice.
Figure 8: Rank correlation performance of the sum of training losses over E most recent epochs(SoTL-E) on the DARTS dataset. Different E values include those < 1 are investigated for 100random architectures in DARTS search space under three different evaluation set-ups. In all threesettings, smaller E in general achieves better rank correlation performance in the early training phasewith E = 1 again being the best choice. The performance of E < 1 is not stable and deterioratesfrom E = 1.
Figure 9:	Training losses, validation losses and validation accuracies of three example architectureson CIFAR100. The average of the training losses, validation losses and validation accuracies overthe final 10 epochs is presented in the subcaption of each architecture.
Figure 10:	Rank correlation performance of the sum over training losses, SoTL (red), the sumover training accuracy, SoTAcc (blue), the sum over validation losses, SoVL (purple) and the sumof validation accuracy, SoVAcc (green) for 5000 random architectures in NASBench-201 on threeimage datasets. Note SoTL denotes the summation from epoch 0 to epoch T and SoTL-E denotes thesummation over the most recent epoch T. The same applies for those of SoVL-E, SoTAcc/SoTAcc-Eand SoVAcc/SoVAcc-E. The results on CIFAR10 and CIFAR100 confirm the discussion in Section5.2 and in the subsection above; as the training proceeds, the validation loss can become poorlycorrelated with the validation/test accuracy while the training loss is still perfectly correlated withthe training accuracy. Thus, another baseline to check against is the sum over validation accuracy,SoVAcc/SoVAcc-E. It’s expected that SoVAcc-E should converge to a perfect rank correlation (=1)with the true test performance at the end of the training. However, the results in (a), (b) and (c) showthat our proposed estimator SoTL-E can consistently outperform SoVAcc-E in the early and middlephase of the training (roughly T ≤ 150 epochs). This reconfirms the usefulness of our estimator.
Figure 11: Mean and 5 standard error of training losses and validation losses on 5000 architectureson different NASBench-201image datasets. (a) shoWs the training curves and (b) shoWs the numberof architectures Whose training losses go beloW 0.1 as the training proceeds. Many architecturesreach very small training loss in the later phase of the training on CIFAR10 and CIFAR100, thus mayoverfitting on these tWo datasets. But all the architectures suffer high training losses on IMAGENET-16-120, Which is a much more challenging classification task, and none of them overfits.
Figure 12: NAS performance of Random Search (RS) in combined With final validation accu-racy (Final Val Acc), early-stop validation accuracy (ES Val Acc) and our estimator SoTL-E onNASBench-201. SoTL-E enjoys competitive convergence as ES Val Acc and both are faster thanusing Final Val Acc.
