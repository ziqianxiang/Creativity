Figure 1: Intra- and inter-temporal attention. The relevance of each input sequence element can beassessed by inspecting their associate intertemporal attention value.
Figure 2: Pre-hoc temporal attention module. This module completely replaces the original outputmodule.
Figure 3: Post-hoc Temporal Attention Module. This module extends the original output moduleoperations.
Figure 4: Sample bAbI task records with its predicted inter-temporal attention values in the secondcolumn5 Conclusions and DiscussionThis work successfully shows that the temporal attention modules to a recurrent model with externalmemory increases performance in QA tasks and, simultaneously, adds interpretability attributes pre-viously unavailable in the original model. In the best configuration, an average accuracy error equalto 21,9% is observed, which corresponds to a 26% improvement compared to the error obtained bythe base model called the Entity Network.
Figure 5: bAbI task #18 training record example. Itâ€™s composed by a facts sequence{fact1 , . . . , fact5 }, a query q and the answer ans to that question. The supporting facts have ared background color.
