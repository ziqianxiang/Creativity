Figure 1: Training T for 3000 iterations with batch size 100 [(a), (b)] and 10 [(c), (d)]. For (a)and (c), colored lines represent the estimation for I(X; Y), EPXY [T], and log(EPX0PY [eT]) of eachbatch. We also added the ideal MI for (a) and exponential moving average (EMA) of I(X; Y) withspan 10 (decay rate 2/11) for (c). We can see that MINE fails to generate a meaningful estimationon batch size 10. Outputs of the neural network for each sample are shown in (b) and (d). As thereare 100 samples per iteration for (b) and 10 for (d), total number of outputs is 300000 and 30000,respectively. Note that the base-10 logarithmic scale is used on the y-axis for (c) and (d).
Figure 2: (a) Training T with different batch sizes 100, 200, 400, and 800. We show the histogramof I(X; Y ) of the final 1000 batches. (b) Histogram of network outputs of marginal samples atdifferent iterations. The probability of the joint case to occur is N, hence the proportionally smallarea of joint cases. A black vertical line is drawn at 0 to assist visually.
Figure 3: (a) Training T with the same settings as Fig. 1d with ReMINE. We can see that ReMINEsuccessfully avoids both drifting and network output explosion problem. (b) Same setting as (a), butused the optimizer from Section 5.2. We suspect Adam shows better performance as it accumulatesprevious gradients. (c) Same setting as (b) except we used micro-averaging strategy with slidingwindow of size 1000. (d) Training T with the same settings as Fig. 1d, but used the same optimizerfrom (b). Outputs are more stabilized compared to Fig. 1d, but still fails to make a stable estimate.
Figure 4: (a) Applying ReMINE for 1500 iterations, with different Cs and λ = 0.1. (b) Histogramof network outputs for the marginal samples at different iterations. We set C = 0 and λ = 0.1.
Figure 5: Comparing the loss surface for varying lambda. Per each batch, we averaged the networkoutputs of joint samples to estimate j, and non-joint cases of marginal samples to estimate j.
Figure 6: Estimation performance on 20-D Gaussian. Similar to Poole et al. (2019), we increase ρevery 4000 iterations. The estimated MI (light) and smoothed estimation with exponential movingaverage (dark) are plotted for each methods, and theoretical bounds are plotted by dotted lines.
Figure 7: Self-consistency tests on CIFAR-10 (Krizhevsky et al., 2009). We report the average resultof 10 repeated runs. Dotted lines indicate theoretical bounds for type 1, and ideal ratio for type 2, 3.
Figure 8: (a) Comparing the joint and non-joint case outputs of ReMINE with λ = 0.01 whenbatch size is 1 for 5000 iterations. We can see j → 0 and j → -击=-50. (b) Comparing thejoint and non-joint case outputs of MINE for 5000 iterations. As the statistics network strugglesto diverge into two different values, it becomes numerically unstable, hence failing in the middleof the training. (c) Comparing the estimation performance of different λs on varying batch sizes1,2, ∙ ∙ ∙ 100. The dotted line represents the true MI.
Figure 9: Visualizing network outputs on the 1-D correlated Gaussian dataset. Each axis representsa paired value x and y of each sample, and the color represents network outputs for (a) marginal and(b) joint samples.
Figure 10: Bias, variance, MSE of estimators on 20-D correlated Gaussian datasetFigure 11: Estimation performance on 20-D correlated Gaussian dataset for additional estimators15Under review as a conference paper at ICLR 2021	MI		2	4	6	8	10	CCMI	-0.65	-1.24	-2.55	-4.18	-6.00	InfoNCE	-0.30	-1.00	-2.31	-4.00	-5.89	JS	-0.57	-0.72	-0.94	-1.21	-1.61	KL	-4.35	-5.25	-6.46	-7.52	-8.83Bias	MINE	-0.15	-0.24	0.32	33.81		Mixed KSG	-1.49	-2.99	-4.51	-6.08	-7.64	NWJ	-0.20	-0.36	-0.74	-1.47	-2.70	SMILE (τ = 10)	-0.15	-0.26				SMILE+JS (τ = 10)	-0.19	-0.26	-0.34	-0.51	-0.74	TNCE	-0.19	-0.93	-2.26	-3.98	-5.88	TUBA	-0.17	-0.33	-0.74	-1.42	-2.85	Iα (α = 0.01)	-0.16	-0.28	-0.53	-1.01	-1.91	ReMINE (C=0, λ = 1)	-0.16	-0.27	-0.49	-0.62	-0.51	ReMINE-L1 (C=0, λ = 1)	-0.25	-0.27	-0.33	0.78	40.44	ReMINE-J (C=0, λ = 1)	-0.16	-0.27	-0.49	-0.65	-0.56
Figure 11: Estimation performance on 20-D correlated Gaussian dataset for additional estimators15Under review as a conference paper at ICLR 2021	MI		2	4	6	8	10	CCMI	-0.65	-1.24	-2.55	-4.18	-6.00	InfoNCE	-0.30	-1.00	-2.31	-4.00	-5.89	JS	-0.57	-0.72	-0.94	-1.21	-1.61	KL	-4.35	-5.25	-6.46	-7.52	-8.83Bias	MINE	-0.15	-0.24	0.32	33.81		Mixed KSG	-1.49	-2.99	-4.51	-6.08	-7.64	NWJ	-0.20	-0.36	-0.74	-1.47	-2.70	SMILE (τ = 10)	-0.15	-0.26				SMILE+JS (τ = 10)	-0.19	-0.26	-0.34	-0.51	-0.74	TNCE	-0.19	-0.93	-2.26	-3.98	-5.88	TUBA	-0.17	-0.33	-0.74	-1.42	-2.85	Iα (α = 0.01)	-0.16	-0.28	-0.53	-1.01	-1.91	ReMINE (C=0, λ = 1)	-0.16	-0.27	-0.49	-0.62	-0.51	ReMINE-L1 (C=0, λ = 1)	-0.25	-0.27	-0.33	0.78	40.44	ReMINE-J (C=0, λ = 1)	-0.16	-0.27	-0.49	-0.65	-0.56	CCMI	0.24	0.03	0.02	0.01	0.00
Figure 12: Self-consistency tests of JS, Iα, and InfoNCE on CIFAR-10. We use the average resultof 10 repeated runs. Dotted lines indicate theoretical bounds for type 1, and ideal ratio for type 2, 3.
Figure 13: Self-consistency tests on MNIST.
Figure 14: Self-consistency tests on CIFAR-10 with ResNet18.
Figure 15: CMI estimation results on Model 1 (n = 20000). The dotted line represents the idealCMI value. We rerun the same experiment 10 times, and plot mean (dark) and standard deviation(light).
