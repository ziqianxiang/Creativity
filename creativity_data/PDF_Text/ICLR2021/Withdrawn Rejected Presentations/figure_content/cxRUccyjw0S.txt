Figure 1: Examples of entanglement in state-of-the-art image translation models. Current ap-proaches and their architectural biases tightly preserve the original structure and generate unreliabletranslations. Our model disentangles the high level content and captures the target style faithfully.
Figure 2: A sketch of the entire method. In the disentanglement stage, we optimize over a singleclass embedding per class and a single content embedding per image, as well as the parameters ofthe style encoder and the generator. In the synthesis stage, we optimize all modules in an amortizedfashion, using the learned class and content embeddings as targets of the new encoders. During thisstage, an additional adversarial discriminator is trained to increase the image fidelity.
Figure 3: Comparison against best baselines on AFHQ (zoom in for better resolution). Class (i.e.
Figure 4: CelebA: Class (i.e. facial identity) and style (e.g. illumination) are guided by the referenceimage, while the content (e.g. head pose and expression) of the source image should remain intact.
Figure 5: CelebA-HQ Male/Female: Our method makes more significant changes to the face thanFader Networks while preserving the similarity to the original identity better than mGANprior.
Figure 6: More qualitative results on AFHQ.
Figure 7: More qualitative results on AFHQ.
Figure 8: More qualitative results on CelebA. Top row: Original content images. Left column:Original class and style images.
Figure 9: More qualitative results on CelebA. Top row: Original content images.
Figure 10: More qualitative results on CelebA-HQ.
Figure 11: Qualitative examples from the ablation analysis; (i) Only a single translation is possi-ble without modeling style, which often yields entangled results. (ii) Disentanglement is achievedwithout adversarial loss. (iii) Adversarial loss contributes to visual fidelity.
Figure 12: Visualization of the three factors modeled by our method. Changing the class (e.g. Tocat, dog, wild) while leaving the style intact affects high level semantics of the presented animal.
Figure 13: Visualization of the three factors modeled by our method. Top row and left columnpresent original images. Translating the images in each column to the class of the image on the leftcolumn (a) affects the person identity. Borrowing the style from the image on the left column (b)transfers illumination and class-specific attributes as hair color. In both cases the head pose and thefacial expression remain intact.
Figure 14: Evidence for the inductive bias conferred by latent optimization on AFHQ (a validationof the discovery presented in Gabbay & Hoshen (2020)). Latent optimization starts with randominitialized content codes and preserves the disentanglement between content and class along theentire training. In contrast, a randomly initialized content encoder outputs entangled codes. Inorder to result in disentangled content codes, it is required to remove the class information from thecontent during the course of optimization. In this plot we can see that the optimization is in practiceunsuccessful at doing so.
Figure 15: Failure cases where the mouth and tongue are transferred over from the style image.
