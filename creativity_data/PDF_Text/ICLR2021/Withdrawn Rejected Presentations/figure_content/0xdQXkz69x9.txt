Figure 1: Example task with CM = 4	1	2	3	4尤*	卓		ɪ	J而y*	meter	watch	stopwatch	clock4 classes with N = 4 and M = 4.
Figure 2: Range of considered attacks on meta-learning based few-shot image classifiers. f and gdenote the classifier and trained meta-learner, respectively. Each diagram depicts how an attack isapplied and includes an expression for the attack,s computation. Adversarially perturbed quantitiesare denoted with a tilde. The prime (0) marks in the Transfer attack indicate that the attack istransferred to a different meta-learner and classifier than the attack is derived from.
Figure 3: Pairs of images from the miniImageNet dataset where the left is unperturbed, while theright is adversarially perturbed by a PDG Support attack with = 0.05, γ = 0.0015, and L = 100.
Figure 4: The relative drop in classification accuracy for a variety of attacks against MAML andProtoNets models in the 5-way miniImageNet configuration, averaged over 500 tasks. For SupportGeneral (1x), M=N, and for Support General (10x), M=10N. All support images were perturbed.
Figure 5: (a) The relative drop in 5-way, 5-shot classification accuracy of ProtoNets as the numberof poisoned classes and poisoned shots within those classes are varied using three different methodsof crafting the poisoned images. Darker colors indicate a stronger attack. Attacks were calculatedwith = 0.05, γ = 0.0015, L = 200, M = 13N, averaged over 250 tasks. (b) Relative drop in5-way classification accuracy for ProtoNets and MAML using a support attack as M is varied, for1-shot and 5-shot. Both the targeted, all and untargeted, single loss strategies were considered andthe more effective was chosen for each model. The attacks are generated using PGD with = 0.05,γ = 0.0015, L = 100, and half the classes and shots in the support set adversarially perturbed.
Figure 6: The relative drop in model accuracy for a variety of attacks using the CNAPs algorithmon META-DATASET with = 0.05, γ = 0.0015, L = 100, averaged over 500 tasks, with all classes,but only 20% of the shots poisoned.
Figure 7: Relative drop in 5-way classification accuracy when transferring adversarial support at-tacks as a function of poisoned shots and classes using CNAPs, which uses a ResNet18 network,to fine-tuners that use ResNet18 and MNASNet networks on ILSVRC 2012 from Meta-Dataset.
