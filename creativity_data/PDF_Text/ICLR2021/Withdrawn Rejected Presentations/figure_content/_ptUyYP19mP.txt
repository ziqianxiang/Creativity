Figure 1: A Hypothetical Demonstration of how exploration is done in BeBold versus Random NetworkDistillation (Burda et al., 2018b), in terms of distribution of intrinsic rewards (IR). BeBold reaches the goal bycontinuously pushing the frontier of exploration while RND got trapped. Note that IR is defined differently inRND (1/N(St)) versus BeBold (max(1∕N(st+ι) — 1/N(st), 0), See Eqn. 3), and different color is used.
Figure 2: MiniGrid Environments. Left: a procedurally-generated OMFull environment. Right: BeBoldsolves challenging tasks which previous approaches cannot solve. Note that we evaluate all methods for 120Msteps. AMIGo gets better results when trained for 500M steps as shown in (Campero et al., 2020).
Figure 3: Results for various hard exploration environments from MiniGrid. BeBold successfully solves allthe environments while all other baselines only manage to solve two to three relatively easy ones.
Figure 4: Normalized visitation counts N(St)/Z (Z is a normalization constant) for the location of agents.
Figure 6: Ablation Study on BeBold comparing with RND with episodic intrinsic reward. BeBold significantlyoutperforms RND with episodic intrinsic reward on all the environments.
Figure 7: Results for tasks on NetHack. BeBold achieves the SoTA results comparing to RND and IMPALA.
Figure 8: Results for CNN-based and RNN-based model on MonteZuma’s Revenge. BeBold achieves goodperformance.
Figure 9: On policy state density heatmaps Pn (St). BeBold continuously pushes the frontier ofexploration from Room1 to Room7.
Figure 10:	Results for BeBold Part 1 and all baselines on all static tasks.
Figure 11:	Results for BeBold Part 2 and all baselines on all static tasks.
