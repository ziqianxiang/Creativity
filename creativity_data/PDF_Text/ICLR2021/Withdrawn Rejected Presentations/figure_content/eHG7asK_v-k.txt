Figure 1: The relationship of discounted returns ηi for an agent i given the different joint policy pairs,where πi is the current policy, πi′ is the simultaneously updated policy. Given πi, the monotonic im-provement against fixed opponent can be easily measured: ηi(πi′ , π−i) ≥ ηi(πi, π−i). However, dueto        the simultaneous learning, the improvement of ηi(πi′ , π′ i) is unknown compared to ηi(πi, π−i).
Figure 2: Comparisons between independent trust region learner and multi-agent trust region learner.
Figure 3: Overview of the multi-agent trust region learning phases in two-agent games.  It can beeasily extended to the n-agent case by solving the n-agent two-action matrix form meta-game.
Figure  4:   Learning  dynamics  ofMATRL in matching pennies game.
Figure 5:  Learning curves in discrete and continuous tasks.  The solid lines are average episodereturns with 10 random seeds for each model, and the light color areas are the error bar.
Figure 6: MATRL/IL versus MATRL/IL in the two-agent pong game. For each setting, the grids arepair-wise performance (average scores) by pitting their ten checkpoints against one another, yellowmeans higher score.
Figure 7: Running time of 20,000 environ-ment steps (including 50 gradient steps)for the algorithms in 2-4 agents games.
Figure 8: Multi-agent discrete and continuous action tasks: (a) 2-agent checker (discrete), (b) 4-agentswitch (discrete), (c) 3-agent MuJoCo hopper (continious).
Figure 9:  (a)-(b):  Learning dynamics of two games using MATRL. c:  Extra learning curves in4-Agent Ant multi-agent MuJoCo task.
Figure 10: Pong game in Atari 2600.
