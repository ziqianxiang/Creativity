Figure 1: (a) Overview of the two SSL algorithms we study in this paper: SimCLR (W1 = W2 = W, nopredictor, NCE Loss) and BYOL (Wι has an extra predictor, W2 is a moving average), (b) Detailed Notation.
Figure 2: Overview of Sec. 4. (a) To analyze the functionality of the covariance operator V砧 [KKι(zo)(Eqn. 3), We assume that Nature generates the data from a certain generative model with latent variable zo andz0, while data augmentation takes x(zo,z0), changes z0 but keeps zo intact. (b) Sec. 4.1: one layer one neuronexample. (c) Sec. 4.2: two-layer case where V[Kι] and V[K2] interplay. (d) Sec. 4.3: Hierarchical Latent TreeModels and deep ReLU networks trained with SimCLR. A latent variable Zμ, and its corresponding nodes Nμin multi-layer ReLU side, covers a subset of input x, resembling local receptive fields in ConvNet.
Figure 3: (a) Two 1D objects under translation: (a1) Two different objects 11 (zo = 1) and 101 (zo = 2)located at different locations specified by z0. (a2) The frequency table for a neuron With local receptive field ofsize 2. (b) In two-layer case (Fig. 2(c)), V[Kι] and V[K2] interplay in two-cluster data distribution.
Figure 4: (a) The Hierarchical Latent Tree Model (HLTM). (b) Correspondence between latentvariables and nodes in the intermediate layer of neural networks. (c) Definition of Vj, Sj and a* inTable. 6.
Figure 5: A visualization of BYOL dynamics in low dimensions. Left: Black arrows denote the vector fieldof the flow in the w1 and w2 of plane online and predictor weights in Eqns. 186 and 187 when the targetnetwork weight θ is fixed to 1. For all 3 panels, λs = 1, λd = 1/2, and τo = τp = τt = 1, and allvectors are normalized to unit length to indicate direction of flow alone. The red curve shows the hyperoblicmanifold of stable fixed points w2w1 = θλdλs-1, while the red point at the origin is an unstable fixed point.
Figure 6: The Hierarchical Latent Tree ModeI(HLTM) used in our experiments (Sec. G.2 and Sec. 6).
Figure 7: Top row: Without `2 normalization, training with SimCLR and Lsimp leads to fast growthof the weight magnitude over time (each curve is one training curve out of 30 trials with differentrandom seeds). Furthermore, this growth is super exponential due to the interplay between topand bottom layers, as suggested by the dynamics in Eqn. 8. Note that the y-axis is in log scale.
Figure 8: When |w2,j| is high, the corresponding node j is highly selective to one specific cluster ofthe data generative models. On the other hand, those node j with low selectivity has very small w2,jand does not contribute substantially to the output of the network. Training with Lτnce (Left Plot)seems to yield stronger selectivity than with Lsimp (Right Plot).
Figure 9: Ablation of how the Frobenius norm of the covariance operator OP changes over training,under different factors: depth L, sample range of ρ*ν (ρμν 〜Uniform[delta」ower, 1]) and over-parameterization ∣Nμ∣. Top row: covariance operator of immediate left latent variable of the rootnode z0; Bottom row: covariance operator of immediate right child of the root node z0.
Figure 10: Ablation of how the Frobenius norm of the covariance operator OP changes over training.
