Figure 1: Left: The standard case. Right: The concatenated inputs construction. Plots of theTest MSE verses number of samples (pre-concatenation) for min-norm ridgeless regression, whered = 30. Following Nakkiran (2019), inputs are drawn X 〜N(0,Id), target y = θx + N(0, σ2),where θ are the parameters, ∣∣θ∣∣2 = 1, σ = 0.1. θ = Xty.
Figure 2: Top row: Loss against number of parameters. Bottom row: Error against number ofparameters. In order from left to right: 1. Input is 32 × 32 image, label is one-hot vector and lossis Cross Entropy, 2. Input is 64 × 32 image (two of the same image stacked on top of each other),label is one-hot vector and loss is Cross Entropy, 3. Input is 64 × 32 image (two different 32 × 32images stacked on top of each other), label is two-hot vector of values 0.5 and 0.5 and loss is CrossEntropy. All models trained with batch size of 100 for 1000 epochs on a subset (n = 4000) of theMNIST dataset. Adam optimizer is used with learning rate = 0.001, β1 = 0.9, β2 = 0.999. Modelsare feedforward neural networks with a single layer of hidden units with ReLU activation.
Figure 3: Left: Standard one-hot vector setup, Right: Concatenated inputs construction. Top: Loss. Bottom:Error. All models trained with batch size of 128 for 400 epochs with 15% label noise on the CIFAR10 dataset.
Figure 4: Left: Standard one-hot vector setup, Right: Concatenated inputs construction. All models trainedwith batch size of 128 for 400 epochs with 15% label noise on the CIFAR100 dataset. Label noise is definedby changing p% of the samples to random wrong labels and is applied prior to training, and prior to stackingfor two-hot vectors. Adam optimizer is used with learning rate = 0.0001, β1 = 0.9, β2 = 0.999. Models areResNet18 architecture where the width k is varied.
Figure 5: Left: Standard one-hot vector setup, Right: Concatenated inputs construction. Models are ResNet18architecture on CIFAR-10 where the error during training is plotted above.
Figure 6: Left: Standard one-hot vector setup, Right: Concatenated inputs construction. Models are ResNet18architecture on CIFAR-100 where the error during training is plotted above.
Figure 7: Left: Bias and Variance against width k of ResNet-34 in the standard case. Middle: Biasand Variance against width k of ResNet-34 in the concatenated inputs construction. Right: Test erroragainst width k of ResNet-34 in the standard case and concatenated inputs construction.
