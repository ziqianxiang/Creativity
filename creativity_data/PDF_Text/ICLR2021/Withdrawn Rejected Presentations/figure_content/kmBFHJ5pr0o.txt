Figure 1: TA/RA comparison between DAT-FGSM and DAT-hɑr γλt r∙cτnτιιιfInC TlCriQQ Inr∙rαoQQQ I hɑ O	ɪber of computing nodes increases. The LSGD vs. node-GPU configurations. Left: (CIFAR-10, ResNet-.	.,similar conclusion holds for DAT-FGSM 18). Right: (ImageNet, ResNet-50).
Figure 2: RA against PGD attacks for model trained by DAT-PGD, DAT-FGSM, and AT following (ImageNet, ResNet-50)in Table 1. (Left) RA versus different perturbation sizes (overthe divisor 255). (Right) RA versus different steps.
Figure 3: Fine-tuning ResNet-50 (pre-trainedon ImageNet) under CIFAR-100. Here DAT-PGD is used for both pre-training and fine-tuning at 6 nodes with batch size 6 × 128.
Figure A1: TA/RA of Fast AT with CLR versus batch sizes.
Figure A2: Training accuracy and objective value (loss) of DAT-PGD against training epochs. (a) DAT-PGDfor (CIFAR-10, ResNet-18) using 6 × 1 computing configuration and 6 × 2048 batch size. (b) DAT-PGD for(ImageNet, ResNet-50) using 6 × 6 computing configuration and 6 × 512 batch size.
Figure A3: RA against different PGD attacks for the model trained by DAT-PGD, DAT-FGSM, and AT under(CIFAR-10, ResNet-18). (Left) RA against PGD attacks with different perturbation sizes (over the divisor 255).
Figure A4: RA against different C&W attacks for the model trained by DAT-PGD, DAT-FGSM, and AT underthe setting (CIFAR-10, ResNet-18) and (ImageNet, ResNet-50), respectively. Here for ease of C&W attackgeneration at ImageNet, we randomly select 1000 test ImageNet images (1 image per class) to generate C&Wattacks.
Figure A5: Fine-tuning ResNet-50 (pre-trained on ImageNet) under CIFAR-10. Here DAT-PGD is used forboth pre-training and fine-tuning at 6 nodes with batch size 6 × 128.
