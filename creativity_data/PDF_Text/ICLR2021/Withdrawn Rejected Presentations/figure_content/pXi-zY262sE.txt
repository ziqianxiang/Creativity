Figure 1: The flow of GraVeR. (1) We first train a classifier using the training set, (2-3) do early-stopping using the validation set, (3) extract fine-tuned word embeddings, (4) change the numberof random maskers according to the validation performance, (5) add weighted bias to fine-tunedembedding W0, (6) add the maskers to a portion of W0 , and lastly, (7) re-train the classifier with W0from the very first step. We repeat the process until all the vocabularies are masked.
Figure 2: Training curves in the classification datasets. As GraVeR makes the model fit to thevalidation set, the performance on the test set increases.
