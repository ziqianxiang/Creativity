Figure 1: We show results on a 1D regression dataset of a sinusoid curve. In green are data points ofthe dataset and in blue the prediction including uncertainty (two standard deviations). As expected,vDUQ reverts to the prior away from the training data, while Deep Ensembles extrapolates arbitrarilyand confidently. The uncertainty is the posterior variance in the case of vDUQ, and the varianceacross the ensemble element predictions in Deep Ensembles.
Figure 2: On the left we visualize a 2D classification task where the classes are two Gaussian blobs(drawn in green), and a large grid of unrelated points (colored according to their log-probabilityunder the data generating distribution). In the center we see the features as computed by an un-constrained model. On the right the features computed by a model with residual connections andspectral normalization. As we can see, the objective for the unconstrained model introduces a largeamount of distortion of the space, largely collapsing the input to a single line, making it almostimpossible to use distance-sensitive measures on these features. In contrast, a constrained mappingstill makes the classes more separable, while maintaining the relative distances of the other points.
Figure 3: We show uncertainty results on the two moons classification dataset. Yellow indicateshigh confidence, while blue indicates uncertainty. In Figure 3a, a simple feed forward resnet with asoftmax output is certain everywhere except on the decision boundary. In Figure 3b, we see that asimple feed forward model in combination with DKL has large areas of certainty, even away fromthe training data. In Figure 3c, we show vDUQ with a residual network as feature extractor incombination with spectral normalization, showing close to ideal uncertainty on this dataset.
Figure 4: The same setup as in 1b, but insteadof visualizing the uncertainty directly, we vi-sualize samples from the joint posterior. Asexpected, the functions become significantlyless smooth away from the data as expectedusing the Matern V = 1 kernel.
Figure 5: A density of the Lipschitz values inbatch normalization layers, averaged across15 WRN models that were trained with Soft-max output and without Spectral Normal-ization (exactly following Zagoruyko & Ko-modakis (2016)). We see that many of theconstants are significantly above 1, invalidat-ing the claim that Batch Normalization re-duces the Lipschitz constant of the network.
Figure 6: The same setup as in Figure 1b, but with noise sampled from N (0, 0.5) instead ofN(0,0.05).
