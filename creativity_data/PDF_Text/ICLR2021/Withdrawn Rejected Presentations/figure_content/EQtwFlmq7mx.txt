Figure 1: CIFAR10: cross entropy lossFigure 2: CIFAR10: prediction accuracyalternative to SPPA-GN where we implement the per-iteration update rule based on well-knownstochastic optimization algorithms, preferably with the ones that take less number of iterations toconverge like L-BFGS (Liu and Nocedal, 1989).
Figure 2: CIFAR10: prediction accuracyalternative to SPPA-GN where we implement the per-iteration update rule based on well-knownstochastic optimization algorithms, preferably with the ones that take less number of iterations toconverge like L-BFGS (Liu and Nocedal, 1989).
Figure 3: MNIST: cross entropy loss	Figure 4: MNIST: prediction accuracy4	ExperimentsWe show some real data experiments to demonstrate effectiveness of the proposed SPPA implementa-tions in classification and regression problems respectively. We compare our proposed algorithmswith three variants of SGD: the original version with various step strategies, SGD with momentum(Sutskever et al., 2013), Adagrad (Duchi et al., 2011), and Adam (Diederik P. Kingma, 2014), withthe default settings suggested in their original papers. We used the PyTorch (Paszke et al., 2019)implementation of the algorithms and used the same platform for implementing our customizedoptimization algorithm.
Figure 5: Boston housing pricesFigure 6: Ames housing prices4.2	Regression with SPPA-GNIn this subsection, we compare the performance of SPPA in two regression problems. Using theleast squares loss, the problem becomes an instance of nonlinear least squares, thus our proposedSPPA-GN algorithm could be applied. The inner loop in SPPA-GN algorithm is run at most 10 timesfor the results presented below.
Figure 6: Ames housing prices4.2	Regression with SPPA-GNIn this subsection, we compare the performance of SPPA in two regression problems. Using theleast squares loss, the problem becomes an instance of nonlinear least squares, thus our proposedSPPA-GN algorithm could be applied. The inner loop in SPPA-GN algorithm is run at most 10 timesfor the results presented below.
