Figure 1: (a) A Rule-based Representation Learner example. The dashed box shows an example ofa discrete logical layer and its corresponding rules. (b) A simplified computation graph of GradientGrafting. Arrows with solid lines represent forward pass while arrows with dashed lines representbackpropagation. The green arrow denotes the grafted gradient, a copy of the gradient representedby the red arrow. After grafting, there exists a backward path from loss function to the parameter Î¸.
Figure 2: Scatter plot of F1 score against log(#edges) for 3 datasets for all 5 folds.
Figure 3:	Training loss of 3 compared discrete model training methods and Gradient Grafting withor without improved logical activation functions on 3 data sets.
Figure 4:	Logical rules obtained from RRL trained on the bank-marketing data set.
Figure 5: Decision mode for the fashion data set summarized from rules of RRL.
Figure 6: Scatter plot of F1 score against log(#edges) for 10 datasets for all 5 folds.
