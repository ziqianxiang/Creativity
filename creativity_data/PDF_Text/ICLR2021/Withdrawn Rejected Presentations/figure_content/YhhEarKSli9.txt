Figure 1: Inference methods to classify Y given data X under latent Z and semi-labeled nuisance S.
Figure 2: Model accuracy across different datasets. AutoBayes offers significant gain.
Figure 3: Bayes-Ball algorithm basic rules (Shachter, 2013). Conditional nodes are shaded.
Figure 4: Full-chain Bayesian graph and inference models for Z-first or S-first factorizations.
Figure 5: Example Bayesian graphs for data generative models under automatic exploration. Bluearrows indicate generative graph for decoder networks. Thick circled S specifies the requirement ofS-conditional decoder, which is less-convenient when learning unlabeled nuisance datasets.
Figure 6:	Z-first and S-first inference graph models relevant for generative models D-G, J, and K.
Figure 7:	Overall network structure for pairing generative model K and inference model Kz.
Figure 8:	Task classification accuracy across different graphical models (with standard deviation).
Figure 9:	Task classification accuracy for Stress dataset.
Figure 10:	Task classification accuracy across subject ID for Stress dataset.
Figure 11:	Task classification accuracy as a function of time complexity for Stress dataset.
