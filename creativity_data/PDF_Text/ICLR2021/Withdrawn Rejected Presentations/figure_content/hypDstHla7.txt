Figure 1: Neuron activity is analyzed in neural networks trained for target reaching of multi-jointrobotic manipulators utilizing end-to-end Deep Q-Learning. We train the network in a simplifiedenvironment of a robot with 2 to 4 controllable joints operating in vertical space (right, initial con-figuration with joints θi). Transferability to a sophisticated robotic simulation (left) with motionsrestricted to vertical space and the robot finger tips supposed to reach the green sphere could bedemonstrated.
Figure 2: Neuron activation analysis for randomly initialized, trained and pruned networks on a threejoint manipulator (averages over 500 sample episodes, 20 trained agents). Top: Distance measuredbetween all-to-all (all neurons in a network are correlated among each other, left) neurons and layer-wise (for every neuron only neurons on this layer are considered for correlation, right) indicate abell-shaped distribution with higher mean in the first and last layer. Pruning sharpens the bell-shape,increasing the mean, but reducing very high distance scores. Bottom: Clustering Dendrograms aregenerated based on the distance measures for an exemplary trained network. Untrained networksshow very similar clusters, trained network highlights cluster groups and pruning reduces neuronswhile increasing cluster distance. The first layer generally keeps the most distinct clusters, thepenultimate layer the strongest neuron reduction.
Figure 3: Evaluation of heuristic cluster based network pruning on the example of a three jointmanipulator. Left: Even though the initial network accuracy decreases rapidly, the training duration(Number of episodes executed until the validation set is passed) only increases significantly withcluster thresholds larger than 0.3. As a trade off between minimal network size after pruning andefficient training τ = 0.2 has been picked as the optimal cluster threshold and was applied forall further experiments. Right: The first reduction step demonstrates the strongest reduction for allnetworks initialized with different initial neuron count per layer. Layers with more than 128 neuronsare reduced to a very similar neuron count in the first pruning step.
Figure 4: Analysis of inter network mappings: Sets of two net-works are trained on robots with different number of joints. Aprojection matrix P that reflects the network similarity is calcu-lated to compute a from the source network neuron activation bwith minimal difference to a.
Figure 5: Projection of neuron activation between networks trained for variable joint robot ma-nipulators.(data averaged with 5 networks each, 3 pruning steps). Top: Benchmark of mappingtechnique and evaluation metrics: On the example of multiple three joint manipulator networks, wefind linear mapping with λ = 50 the superior mapping approach in contrast to greedy mapping. Inparticular, coverage and normalized error indicate mapping quality well in comparison to untrainednetworks. A layer-wise linear mapping with λ = 50 is not optimal, but strongest correlations can befound between corresponding layers. This is represented in the higher diagonal values in the table ofnormalized average layer distances on the right. Here, layer 1 and 6 show best mapping. (initializa-tion with 6 layers each 256 neurons before pruning, random nets with average pruned network sizeof s = [46 22 16 13 8 20] neurons per layer). Middle: Neuron activation correlations ofnetworks trained on robots with different joint count (2-4 refers to a mapping from networks trainedfor a 2 joint to a 4 joint robotic arm): The mapping error gets higher with increased difference injoint numbers, the coverage accordingly decreases. Mapping a network with higher complexity intolower complex ones performs slightly better than vice versa. In this study the mappings 4-2 areclosest to the performance of the native 3-3 mappings. This mapping is influenced by a proper trans-formation of sensory inputs to the increased number of input neurons on the first layer. The resultsare demonstrated for balanced mapping as θ10 = θ1 , θ20 = θ3 , θ2 = 0, θ4 = 0 (4b) which performsbetter than in contrast to naive mapping θ10 = θ1 , θ20 = θ2 , θ3 = 0, θ4 = 0 (4a). (results as mean of25 mappings). Bottom: Mean layer to layer projections: Networks trained for more similar robotsshow better layer to layer mappings. The first layer of the source network shows high utilization
