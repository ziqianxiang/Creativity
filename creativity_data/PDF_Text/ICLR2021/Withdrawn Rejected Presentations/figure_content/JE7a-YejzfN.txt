Figure 1: Quantifying difficulty by using the largest eigenvalue of the Fisher information metric(FIM) a) We show that contrast sets and counterfactual examples aren’t necessarily concentratednear the decision boundary as shown in this diagram. Difficult examples are the ones shown in greenregion (close to the decision boundary) and this is the region where we should evaluate model fragility.
Figure 2: Left figure: The difficult examples on the FIM test set are challenging for the classifiers withaccuracy between 1-5%. For the easy examples, (low FIM λmax) the accuracy ranges between 60-100%. Right figure: We plot the classifier accuracy with respect to the l2 norm of the perturbation inembedding space. For difficult examples, the classifier accuracy drops below 5% at small perturbationstrengths (l2 norm < 0.3)4.2.2	BERTBERT: Transfer learned models like BERT capture rich semantic structure of the sentence. They arerobust to changes like actor names and tend to rely on semantically relevant words for classifyingmovie reviews. As we can see from Table 6 difficult BERT examples, even with multiple positivewords, tend to predict a negative sentiment when only one of the positive word is substituted. Evenwith words like “fantastic”, “terrific” and “exhilarating”, just changing “best” to “worst” changedthe entire sentiment of the movie review. Easy examples for BERT require multiple simultaneousword substitutions to change the sentiment as can be seen in Table 6. Unlike CNN models, BERT issignificantly more robust to meaningless substitutions like actor names.
Figure 3: On YelpReviewPolarity we observe a correlation coefficient of -0.4 between minimumperturbation strength and the difficulty score. Similarly, we observe a correlation of 0.35 between thedifficulty score and empirical success of random word substitutions. This suggests that fim eigenvaluecaptures perturbation sensitivity in both embedding space and word substitutions.
Figure 4: a) Distribution of difference in largest eigenvalue of FIM of the original and the perturbedsentence in contrast set and counterfactual examples for BERT and CNN. With a mean near 0, theseperturbations are not difficult for the model. Adhoc perturbations are thus not useful for evaluatingmodel robustness.
Figure 5: Here we take the Joulin et al. (2017) text classification approach ( and plot classifieraccuracy as a function of the step size of the perturbation. The number of examples in the x-axisrepresents the number of examples based on the eigenvalue. So, 200 refers to the 200 easy examplesand 200 difficult examples. We then perturb these examples in the direction of the eigenvector andcheck if the classifier prediction flipped. As can be seen that the classifier accuracy remains very highfor easy examples and is significantly low for difficult examples. On the second diagram, for easyexamples, the classifier still exhibits minimal performance drop when the weight of the perturbationalong the eigenvector is increased. Thus our method is dataset agnostic.
Figure 6: We use the eigenvector with the largest eigenvalue as a perturbation in embedding space.
Figure 7: We notice the linear relationship between the log of eigenvalue and the percentage ofsuccessful word flips. Since the length of each document varies drastically for these documentclassification problems, we decide the number of words to flip as 10% of the document length withthe vocabulary of the dataset. We then measure the percentage of predictions whose classificationlabel changes and the log of fim eigenvalue. The pv alueandtherv aluearereportedatthetop.
Figure 8: a) Histogram of eigenvalues of dev sets of original IMDb examples, contrast sets andcounterfactual examples. The significant overlap between the three distributions indicates that thecounterfactual and contrast set examples might not be as difficult as previously believed. The tailend of all three distributions contain the difficult examples. b) Distribution of difference in largesteigenvalue of FIM of the original and the perturbed sentence in contrast set and counterfactualexamples. With a mean near 0, these perturbations are not necessarily more difficult for the model.
