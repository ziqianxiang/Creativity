Figure 1: Probabilistic graphi-cal model illustrating the staterepresentation learning prob-lem, estimating state represen-tation Z from original state S.
Figure 2: (left) A representation that aliases the states s0 and s3 into a single state maximizes Jstatebut is not sufficient to represent the optimal policy which must choose different actions in s0 ands3 to reach s2 which yields reward. (right) Values of Jstate and Jfwd for a few representative staterepresentations, ordered by increasing I(Z; S). The representation that aliases s0 and s3 (plottedwith a diamond) maximizes Jstate , but the policy learned with this representation may not be optimal(as shown here). The original state representation (plotted with a star) is sufficient.
Figure 3: (left) In this MDP, a representation that aliases the states s0 and s1 into a single statemaximizes Jinv, yet is not sufficient to represent the optimal policy, which must distinguish betweens0 and s1 in order to take a different action (towards the high-reward states outlined in green). (right)Values of Jinv and Jfwd for a few selected state representations, ordered by increasing I(Z; S). Therepresentation that aliases s0 and s1 (plotted with a diamond) maximizes Jinv, but is not sufficient tolearn the optimal policy. Note that this counterexample holds also for Jinv + I(R; Z).
Figure 4: (left) Original catcher game in whichthe agent (grey paddle) moves left or right to catchfruit (yellow square) that falls from the top of thescreen. (right) Variation catcher-grip in which theagent is instantiated as a gripper, and must openthe gripper to catch fruit.
Figure 5: (top) Policy performance using learned representations as state inputs to RL, for the catcherand catcher-grip environments. (bottom) Error in predicting the positions of ground truth stateelements from each learned representation. Representations maximizing Jinv need not representthe fruit, while representations maximizing Jstate need not represent the gripper, leading theserepresentations to perform poorly in catcher and catcher-grip respectively.
Figure 6: Graphical model for Lemma 1, depicting true states S, states in the representation Z,actions A, rewards R, and the variable X (which We will interpret as the sum of future rewards in theproof of Proposition 1).
Figure 7: performance of policies obtained from a Q-function trained to predict Q*, given staterepresentations learned by each Mi objective, in the (left) catcher environment and (right) catcher-grip environment. insufficient objectives Jinv and Jstate respectively perform worse than sufficientobjective Jfwd .
Figure 8: Example 64x64 pixel observations withbackground distractors.
Figure 9: (top) Policy performance using learned representations as state inputs to RL, for the catcherand catcher-grip environments with background distractors. (bottom) Error in predicting the positionsof ground truth state elements from each learned representation. Representations maximizing Jinvneed not represent the fruit, while representations maximizing Jstate need not represent the gripper,leading these representations to perform poorly in catcher and catcher-grip respectively.
