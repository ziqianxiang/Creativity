Figure 1:	High scores in brain predictivity can be achieved with few supervised updates (logx-axes). A Average brain predictivity of models trained with a range of supervised updates (epochs ×images). Each dot is a different hypothesis of how the ventral visual stream might have developed andshows the adult brain-likeness score that is achieved by that model. Fairly brain-like representationsare already realized with few supervised updates, relative to a fully trained model (black dot; seealso Figure 6). Standardly trained CORnet-S is set to be 100% score in brain predictivity on thebenchmarks (0.42 absolute). A "pixels" baseline achieves 7% brain predictivity score (0.03 absolute).
Figure 2:	"At-birth" synaptic connectivity yields reasonable scores in brain predictivity. ASpecifying the initial weight distribution: Kaiming Normal (KN, He et al., 2015) samples froma generic Gaussian. Weight Compression (WC) compresses trained weights into low-parameterclustered distributions that weights can be initialized from. B Visualization of WC compressedparameters: Gabor filters for first layer and cluster centers for following layers with kernel size > 3.
Figure 3:	Training only critical layers reduces the number of updated synapses while maintain-ing high brain predictivity. A We could naively reduce the parameters of a fully-trained model byfreezing layers from the bottom up, training only the top layers ("Downstream Training DT"; graybox). We instead propose Critical Training (CT) which only trains down-sampling layers (blue box).
Figure 4:	High brain predictivity can be achieved with few supervised synaptic updates througha combination of training reductions (log x-axis). A By reducing updates with a combination offewer supervised updates (Figure 1), improved initialization WC (Figure 2), and training only down-sampling layers CT (Figure 3), the resulting models (dark blue dots; fewer supervised updates alonein light blue) maintain high brain predictivity scores while requiring only a fraction of supervisedsynaptic updates compared to standard CORnet-S (black dot, top right). B Comparison betweenWC-initialized models trained with CT versus standardly initialized models training all weights, whenvarying training epochs and labeled images. Colors represent their percent point difference in brainpredictivity scores. WC+CT improve performance in regimes with few epochs and images.
Figure 5:	Dissecting training re-ductions. A Transfer to other net-works. We sample from WC ini-tializations determined on CORnet-S, followed by Critical Training ofonly down-sampling layers. B Ab-solute scores on individual bench-marks of combinations of initializa-tion (KN/WC, Figure 2), and withcritical training (CT, Figure 3) tech-niques.
Figure 6: Findings generalize acrosshyperparameters. Notations like inFigure 1. To test whether fast learn-ing could be achieved by purely chang-ing hyperparameters, we evaluated dif-ferent regularizations (weight decay0.001 and 0.00001 vs. 0.0001), learn-ing rates (initial 1.0 and 0.01 vs. 0.1in the main manuscript) and learningrate schedules ("Scheduler alternative"decreases learning rate more agres-sively). Trends are qualitatively sim-ilar across all choices, and quantita-tively nearly optimal with respect tofast convergence to high brain predic-tivity for the hyperparameters used inthe manuscript.
Figure 7: Alternative weightcompression methods Compar-ison of different initializationsthat compress weights, "at birth"i.e. without any training (gray)and after training critical layers(shades of blue) for 6 epochs. Ourbest clustering-based approachWC achieved similar results asthe Mixture Gaussian approach(〜3 percent points mean differ-ence) but leads to more diverseclusters. Performance drops whensolely sampling weights from ker-nel based normal distributions(Kernel normal) and additionallydisabling the Gabor prior (No Ga-bor prior)frequency and C is a scaling factor.
Figure 8: Detailed analysis of WC+CT. A When reducing the number of supervised synaptic updates,adding critical training (dark grey) and adding weight compression initialization (dark blue) bothimprove scores in brain predictivity at the same number of supervised synaptic updates, in comparisonto a model with standard initialization and training all weights (bright blue). B Brain predictivitiesfor the WC+CT model when trained with a range of epochs and labeled images. C Same as B, butfor a standardly initialized (KN) model training all weights.
Figure 9: Individual brain benchmark scores for WC+CT model A Individual brain predictivitiyscores of WC+CT models trained with a range of epochs on all images. These models score especiallyhigh on V1, V2 and V4 already after one epoch in comparison to a model with standard initializationtraining all weights. IT and Behavior benchmarks continuously improve over later epochs as well butfall short of a fully trained model. B Like A, but with models trained until convergence on differentnumbers of labeled images, up to the full dataset of 1.28M images (rightmost points). As in A we see> 80% V1, V2 and V4 scores with only 100,000 images. For comparable IT and behavioral scores,more images are required.
