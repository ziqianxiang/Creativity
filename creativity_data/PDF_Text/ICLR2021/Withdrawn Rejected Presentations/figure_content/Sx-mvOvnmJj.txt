Figure 1: Reliability diagrams of ResNet-32. From left to right: the plain model trained on the originalCIFAR-100 dataset, the plain model, cRT, and LWS trained on long-tailed CIFAR-100 with IF=100.
Figure 2: Classifier weight norms for the ImageNet-LT validation set when classes are sorted by descendingvalues of Nj . Left: weight norms of cRT with or without mixup. Right: weight norms of LWS with or withoutmixup. (light shade: true norm, dark lines: smooth version)have shown significant improvement over mixup. Thulasidasan et al. (2019) found that CNNs trainedwith mixup are significantly better calibrated. Label smoothing (Szegedy et al., 2016) is anotherregularization technique that encourages the model to be less over-confident. Unlike cross-entropycomputes loss upon the ground truth labels, label smoothing computes loss upon a soft version of thelabel, which can relieve the over-fitting and increase calibration and reliability (Muller et al., 2019).
Figure 3: Violin plot of predicted probability distributions for different parts of classes, head (more than 100images), medium (20 to 100 images), and tail (less than 20 images) on LT CIFAR-100, IF=100. The upper halfpart in light blue: LWS (cross-entropy). The bottom half part in deep blue: LWS (label-aware smoothing).
Figure 4: Reliability diagrams of ResNet-32 trained on LT CIFAR-100, IF=100. From left to right: cRT withmixup, LWS with mixup, LWS with mixup and shifted BN, and MiSLAS. It is better to look together with Fig. 1.
Figure 6: Visualization of the changes in the running mean μ and variance σ2. The ResNet-32 based modelis trained on LT CIFAR-100 with imbalanced factor 100. Left: μ and σ2 in the first BN of ResNet-32, whichcontains 16 channels. Right: μ and σ2 in the last BN of ResNet-32, which contains 64 channels.
Figure 7: Reliability diagrams on CIFAR10 with 15 bins. From left to right: plain ResNet-32 modeltrained on the original CIFAR-10 dataset, plain model, cRT, LWS, and MiSLAS trained on long-tailedCIFAR-10 with imbalanced factor 100.
Figure 8: Reliability diagrams on ImageNet with 15 bins. From left to right: plain ResNet-50model trained on the original ImageNet dataset, plain model, cRT, LWS, and MiSLAS trained onFigure 9: Reliability diagrams of ResNet-152 trained on Places-LT with 15 bins. From left to right:cRT, LWS, cRT with mixup, LWS with mixup, and MiSLAS.
Figure 9: Reliability diagrams of ResNet-152 trained on Places-LT with 15 bins. From left to right:cRT, LWS, cRT with mixup, LWS with mixup, and MiSLAS.
Figure 10: Ablation study of two hyperparameters 1 and K in label-aware smoothing. Our label-aware smoothing (orange square) outperforms cross-entropy (green square) by a large margin on bothlong-tailed CIFAR-10 (left) and long-tailed CIFAR-100 (right).
Figure 11: Function visualization and accuracy of Eqn. (3) (left) and Eqn. (7) (right).
