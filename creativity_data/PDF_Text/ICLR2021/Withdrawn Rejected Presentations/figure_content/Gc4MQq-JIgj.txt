Figure 1: The trajectories produced by the the policy trained by our proposed method ((a) and (d)),4-step MPC ((b), (e)), and the policy trained with penalized DQN ((c) and (f)). The trajectories oncircular circuit were produced by the policies trained on the original circuit. S represents the initialposition of the agent. The red marks represents the places at which the agent crashed into the wall.
Figure 2: (a) Map of FrozenLake8x8-v0. Here the symbols ‘S’, ‘G’, ‘H’ and ‘F’ denote the positionof start, goal, hole to be avoided, and frozen surface, respectively. (b) Heat map of the threat functionof optimal R-MDP baseline policy. The warmer color represents the higher value of the thresholdfunction at the corresponding position and action. The threshold tends to be higher around the hole.
Figure 3: (a) Illustration of Jam task. The light blue circles are obstacles and the yellow circle is theagent. Three shaded corners are safe-zones. The arrow attached to each object shows its directionof movement. (b) Heat map of the trained threat function whose value at point (x, y) represent thethreat when the obstacle (light blue) is placed at (x, y) with the same velocity. (c) and (d) are the heatmaps of the sum of the threat functions of all moving obstacles. The heat value at (x, y) is the threatposed to the agent at (x, y) when it is moving in the direction indicated at the left bottom corner.
Figure 4: Comparison of multiple CMDP methods in terms of rewards (left panels) and crash rate(right panels) in diffferent environments (a) Point Gather, (b) Circuit and (c) Jam.
