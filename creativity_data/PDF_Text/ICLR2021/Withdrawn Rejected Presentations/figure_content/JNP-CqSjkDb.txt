Figure 1: The model structure of a 1layer-Starber4	Experiments4.1	A Special Type of Pretraining: Effectiveness of Masked LanguageAlthough we don’t pretrain on an enormous dataset, we can still improve the performance by utiliz-ing the common pretraining task. In practice, we pretrain our model on a collection of all the inputswithout their labels from the training, development and test set. We use dynamic mask (Liu et al.,2019), which is to mask different positions of a sample at every epoch. The reconstruction loss iscomputed on all positions of a sequence instead of only those masked ones. Masking probability inevery position is set to be 0.3. Different from BERT, we don’t have a MASK symbol. Instead everymasked position is replaced with a word uniformally selected from the whole vocabulary.
