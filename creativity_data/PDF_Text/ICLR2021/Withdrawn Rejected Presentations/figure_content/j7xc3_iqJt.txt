Figure 1: Mem2Mem architecture: Sentence-level representations from the document encoder are reducedto a fix sized encoder memory ME. The encoder memory is transferred to the decoder memory MD and isread using a RAM like mechanism. The resulting memory readout vector is used to condition sentence andword-level attention. The decoder hidden states and the memory readout vector are then used to update thememory state MD via a gated write operation (zi).
Figure 2: Multi-head memory write attention matrix A and memory read attention matrix Ψ. In (a),rows denote memory heads or slots and columns indicate input sentence indices. In (b), columnsindicate decoding time steps.
Figure 3: The effect of regularization on memory compression. Examples of the multi-head encodermemory write attention matrix A are illustrated. Rows denote memory heads or slots and columnsindicate input sentence indices. Note that the regularization loss L(comp) removes the redundancyover different memory heads and guides each slot to focus on a single sentence.
Figure 4: The effect of regularization on memory read. Examples of the decoder memory readattention matrix Ψ are illustrated. Rows denote memory heads or slots and columns indicate decod-ing time steps. Note that the regularization loss L(read) encourages the model to fully utilize thecompressed memory representations.
