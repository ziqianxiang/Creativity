Figure 1: Visual depiction of Thm. 1 with a (random)generator G : R2 7→ R3 . Left: generator input spacepartition Ω made of PolytoPal regions. Right: gener-ator image I m(G) which is a continuous piecewiseaffine surface comPosed of the PolytoPes obtainedby affinely transforming the PolytoPes from the in-Put sPace Partition (left) the colors are Per-region andcorresPond between left and right Plots. This input-space-partition / generator-image / per-region-affine-mapping relation holds for any architecture employingpiecewise affine activation functions. Understandingeach of the three brings insights into the others, as wedemonstrate in this paper.
Figure 2: Impact of dropout and dropconnect on the intrinsic dimension of the noise induced generators fortwo “drop” probabilities 0.1 and 0.3 and for a generator G with S = 6, D = 10, L = 3 with varyingwidth D1 = D2 ranging from 6 to 48 (x-axis). The boxplot represents the distribution of the per-regionintrinsic dimensions over 2000 sampled regions and 2000 different noise realizations. Recall that the intrinsicdimension is upper bounded by S = 6 in this case. Two key observations: first dropconnect tends to producingDGN with intrisic dimension preserving the latent dimension (S = 6) even for narrow models (D1 , D2 ≈ S),as opposed to dropout which tends to produce DGNs with much smaller intrinsic dimension than S. As aresult, if the DGN is much wider than S, both techniques can be used, while in narrow models, either none ordropconnect should be preferred.
Figure 3: DGN withdropout trained (GAN)on a circle dataset (bluedots); dropout turns aDGN into an ensembleof DGNs (each dropoutrealization is drawn in aApplication: Effect of Dropout/Dropconnect. Noise techniques, such as different color).
Figure 4: Left: final training negative ELBO (smaller is better) for a MLP DGN (S = 100 in red) on MNISTwith varying width (y-axis) and dropout rates (x-axis) of the DGN (decoder); we observe that the impact ofdropout on performances depends on the layer widths (CIFAR10 results can be seen in Fig. 9). Right:Minimalapproximation error (E*) for a target linear manifold (S* = 5, in red), With increasing dataset size from 100to 1000 (blue to green) and different latent space dimensions S (x-axis), the E* = 0 line is depicted in black.
Figure 5:	Visualization of a single basis vectors [Aω].,k before andafter learning obtained from a region ω containing the digits 7, 5, 9,and 0 respectively, for GAN and VAE models made of fully con-nected or convolutional layer (see Appendix E for details). We ob-serve how those basis vectors encodes: right rotation, cedilla exten-sion, left rotation, and upward translation respectively allowing in-terpretability into the learn DGN affine parameters.
Figure 6:	Left: Depiction of the DGN image I m(G) for different values of smoothing β; clearly the smooth-ing preserves the overall form of the generated surface while smoothing the (originally) non-differentiableparts. No other change is applied on G. Right: Negative ELBO (lower is better, avg. and std. over 10 runs) fordifferent DGNs (VAE trained) with CNN architecture with varying smoothing parameters β . Regardless of thedataset, the trend seems to be that smoother/less-smooth models produce better performance in CNNs.
Figure 7: Distribution of the per-region log-determinants (bottom row) for DGN trained ona bimodal distribution with varying per modevariance (first row), demonstrating how the datamultimodality and concentration pushes the per-region determinant of Aω to greatly increase inturn leading to large amplitude layer weights andimpacting the stability of the DGN learning (alsorecall Lemma 1). Additional experiments aregiven in Fig. 15 in Appendix D.6.
Figure 8: Histograms of the DGN adjacent region angles for DGNs withtwo hidden layers, S = 16 and D = 17, D = 32 respectively and vary-ing width d` on the y-axis. Three trends to observe: increasing the widthincreases the bimodality of the distribution while favoring near 0 angles; in-creasing the output space dimension increases in the number of angles nearorthogonal; the amount of weight sharing between the parameters Aω andAω0 of adjacent regions ω and ω0 makes those distributions much more con-centrated near 0 than with no weight sharing (depicted in blue), leading to anEO overall well behave manifold. Additional figures are available in Fig. 11.
Figure 9: Reprise of Fig. 431503100D.2 Example of Determinantsσ1 = 0, σ2 ∈ {1, 2, 3}Figure 10: Distribution of log(Pdet( AT Aω)) for 2000 regions ω with a DGN with L = 3,S = 6,D = 10 andweights initialized with Xavier; then, half of the weights’ coefficients (picked randomly) are rescaled by σ1 andthe other half by σ2. We observe that greater variance of the weights increase the spread of the log-determinantsand increase the mean of the distribution.
Figure 10: Distribution of log(Pdet( AT Aω)) for 2000 regions ω with a DGN with L = 3,S = 6,D = 10 andweights initialized with Xavier; then, half of the weights’ coefficients (picked randomly) are rescaled by σ1 andthe other half by σ2. We observe that greater variance of the weights increase the spread of the log-determinantsand increase the mean of the distribution.
Figure 11: Reproduction of Fig. 8. Histograms of the largest principal angles for DGNs with onehidden layer (first two rows) and two hidden layers (last two rows). In each case the latent spacedimension and width of the hidden layers is in the top of the column. The observations reinforce theclaims on the role of width and S versus D dimensions.
Figure 12: The columns represent different widths d` ∈ {6, 8,16, 32} and the rows correspond torepetition of the learning for different random initializations of the GDNs for consecutive seeds.
Figure 13: Randomly generated digits from the trained GAN (top) and trained VAE(bottom) modelsfor the experiment from Fig. 5. Each row represents a model that was training on a different randominitialization (8 runs in total) which produced the result in Table 1.
Figure 14: Randomly generated digits from the trained CONV GAN (top) and trained CONVVAE(bottom) models for the experiment from Fig. 5. Each row represents a model that was trainingon a different random initialization (8 runs in total) which produced the result in Table 1.
Figure 15: Reproduction of Fig. 7 for multiple standard deviations and multiple randomseeds. Each column represent a different standard deviation of the two Gaussians σ ∈{0.002, 0.01, 0.05, 0.1, 0.3, 1, 2} and each row is a run with a different seed. As can be seen inall cases (except when lack of convergence) the distribution of the determinants support the claimand relate with the Entropy of the true distribution (blue points).
