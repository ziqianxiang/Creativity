Figure 1: Proposed GSA module: The keys, queries, and values (generated using 1 Ã— 1 convolutions)are processed by the content attention and positional attention layers in parallel. The positionalattention layer is split into column-only and row-only positional attention layers, which use learnedrelative position embeddings Rc and Rr as keys. Finally, the outputs of the content and positionalattention layers are summed to generate the output of the GSA module. Here, BN denotes batchnormalization (Ioffe & Szegedy, 2015), and PA stands for positional attention.
Figure 2: Comparison between ResNet-{38, 50, 101} structure-based CNNs and GSA networks.
Figure 3: Comparison between ResNet-{38, 50, 101} structure-based CNNs and GSA networks.
