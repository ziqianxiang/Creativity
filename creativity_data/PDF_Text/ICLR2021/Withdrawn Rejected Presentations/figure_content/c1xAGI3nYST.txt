Figure 1: We propose anEBM prior using the prod-uct of a base prior p(z) anda reweighting factor r(z),designed to bring the baseprior closer to the aggre-gate posterior q(z).
Figure 2: NCP-VAE is trained in two stages. In the first stage, we train a VAE using the original VAEobjective. In the second stage, we train the reweighting factor r(z) using noise contrastive estimation (NCE).
Figure 3: Randomly sampled images from NCP-VAE with the temperature t for the prior.
Figure 4: (a) Classification loss for binary classifiers on latent variable groups. A larger final loss upontraining indicates that q(z) and p(z) are more similar. (b) The effective sample size vs. the final loss valueat the end of training. Higher effective sample size implies similarity of two distributions.
Figure 5: Residual blocks used in the binary classifier. We use s, p and C to refer to the stride parameter,the padding parameter and the number of channels in the feature map, respectively.
Figure 6: Query images (left) and their nearest neighbors from the CelebA-HQ-256 training dataset.
Figure 7: Query images (left) and their nearest neighbors from the CelebA-HQ-256 training dataset.
Figure 8: Additional samples from CelebA-64 at t = 0.7.
Figure 9: Additional samples from CelebA-HQ-256 at t = 0.7.
Figure 10: Selected good quality samples from CelebA-HQ-256.
Figure 11: Qualitative results on mixture of 25-Gaussians.
Figure 12: Additional samples from CelebA-64 at t = 0.7.
