Figure 1: Fine-tuning the checkpoints in Group I on four downstream tasks. We keep the bestfine-tuning accuracy for each checkpoint-task pair after hyper-parameter sweeping. For better visu-alization, the values are offset by their mean (cf. Table 7 in Appendix for the absolute values). (Bestviewed in color. Red: generative models. Black: From-Scratch. Green: self-supervised models.
Figure 2: N LEEP’ checkpoint ranking performance, evaluated by Kendall’s τ , on Groups I and II inNeuCRaB. We vary the PCA feature dimension and the number of Gaussian components in GMM.
Figure 3: Difference between the fine-tuning accuracy of each checkpoint and the mean fine-tuningaccuracy on Group IV. Black bar means From-Scratch. Red, green and orange bars represent Ima-geNet models, iNaturalist models and Places365 models, respectively. Img-90k means the check-point obtained by early stopping at the 90k-th iteration on ImageNet, and so on.
Figure 4: Difference between the fine-tuning accuracy of each checkpoint and the mean fine-tuningaccuracy on Group II. Black bar means From-Scratch. Red, green and orange bars represent Ima-geNet models, iNaturalist models and Places365 models, respectively. Img-90k means the check-point obtained by early stopping at the 90k-th iteration on ImageNet, and so on.
Figure 5: Difference between the fine-tuning accuracy of each checkpoint and the mean fine-tuningaccuracy on Group III. The colors of bars represent the models trained with different architectures.
