Figure 1:	Training Curves on MuJoCo benChmarks with SAC-based algorithms. We set PMOE withK = 4 in all the experiments exCept HalfCheetah-v2 with K = 2 and HumanoidStandup-v2 withK = 10.
Figure 2:	Training Curves on MuJoCo benChmark with PPO-based algorithms. We set a largernumber, K = 8, for the high-dimensional environments Humanoid-v2 and HumanoidStandup-v2,and K = 4 for other environments . The results show that our PMOE has Comparable performanCewith the baseline on low-dimension environments, but signifiCantly better than the baseline onhigh-dimensional environments.
Figure 3: Trajectories of the agents with our method and the baselines in the target-reaching en-vironment. We fix the reset locations of target, obstacles and agent. (a), (b), (c) and (d) visualizethe 10 trajectories collected with methods involving: original SAC, gating operation with SAC,back-propagation-all PMOE (discussed in Sec. 3.4) and back-propagation-max PMOE, respectively.
Figure 4: Visualisation of distinguishable primitives learned with PMOE using t-SNE plot on Hopper-v2 environment. The states are first clustered as in (b). Then actions within the same state clusterare plotted with t-SNE as in (a) and (c) for the gating method and our approach, respectively. Ourmethod clearly demonstrates more distinguishable primitives for the policy.
Figure 5: Visualisation of exploration trajectories in the initial training stage for the target-reachingenvironment. The initial 10K steps (the grey region on the learning curves in (b)) of explorationtrajectories are plotted in (a) and (c) for our PMOE method (red) and SAC (blue), respectively. Thegreen rectangle is the target region.
Figure 6:	Comparison of different numbersof primitives K in terms of average returns onHumanoidStandup-v2 environment. For each case,we conduct 5 runs and take the means. The perfor-mance increases when K increase from 2 to 10, butdecreases if K keep increasing from 10 to 16.
Figure 7: Visualisation of the target-reaching environmentThe immediate reward function for each time step is defined as:(100, if the agent reaches the target;- 10, if the agent collides with edges or obstacles;	(19)||v||2 , otherwise.
Figure 8: Visualisation of the probabilities of each primitive over the time steps in the MuJoCoHalfCheetah-v2 environment. The y-axis shows the probabilities of different primitives.
Figure 9: Visualisation of the actions at the selected 5 time steps in one period. The y-axis showsthe probabilities of different primitives. This result shows that the primitives develop distinctspecialisations, with the primitive 0 becomes the most active when the front leg touches the ground,while the primitive 1 becomes the most active when the leg leaves the ground.
Figure 10: We plot the t-SNE(b) State Observations(c) Oursvisualisation for other 5 MuJoCo environments: Ant-v2,HumanoidStandup-v2, Humanoid-v2, HalfCheetah-v2 and Walker2d-v2. Parameters and other detailsare the same as the setting mentioned in Sec 4.2.
Figure 11: Comparison with Gumbel-softmax and REINFORCE in the MuJoCo tasks Hopper-v2 andHumanoidStandup-v2. We found our method PMOE-SAC has a better performance.
Figure 12: Comparison with PPO in the MuJoCo task HalfCheetah-v2. We found our methodPMOE-PPO has a better performance.
Figure 13: Comparison with different K and different amounts of entropy regularisation. Ourapproach can be considered as a kind of entropy regularisation method and the number of primitivesis positively correlated with the entropy of the policy. The larger number of primitives with smallerentropy has a similar performance to the smaller number of primitives with larger entropy.
