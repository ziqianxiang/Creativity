Figure 1: Computation of loss in the training loop when augmented with Quantile Regularization(QR). Parts in Red are the ones that constitute QR. Total Loss = Loss - Entropy2	Background and DefinitionsBefore we proceed with definitions, we state the notation followed in the rest of paper. X , Y denotesthe input and output space respectively. X, Y denote random variables modelling inputs and outputs.
Figure 2: Fig. 2a shows 256 training data points. Fig. 2b shows the calibration mapping learnt usingIsotonic Calibration with 32 samples. Fig. 2c shows predicted PDF at test time before applyingIsotonic Calibration and truncation and shift in mean that happens if the mapping in Fig. 2b isapplied . It also shows that such truncation can affect the support. Fig. 2d shows the resulting PDFafter Isotonic Calibration(d) PDF after Isotonic Calibration4.1	Differential Entropy EstimationWe need the differential entropy estimator for deriving our calibration loss function. There is a richliterature for entropy estimation. A brief overview about non-parametric entropy estimation canbe found in (Beirlant & Dudewicz, 1997). We use sample-spacing entropy estimation originallyproposed in (Vasicek, 1976). A k spacing of random variable is defined as the amount of probabilitymass between ordered samples that are k - 1 samples apart. Let S be a one-dimensional r.v. withCDF F and assume We are given n samples {si}n=ι 〜S and k s.t. 1 ≤ k ≤ n. Sample spacingentropy estimation is based on the following observation of k-spacings of random variablesEs [F(1 * * * s(i+k)) - F(Sei))] = nɪɪThere are many formulations of sample spacing entropy estimators but We use the one in (Learned-Miller et al. (2003); Equation 8 ) Which is1 n-k n+ 1H(S) = n-k X log [F (s(i+k) - s(k))]n - i=1
Figure 3: Dashed line (y=x) indicates perfect Calibration. The closer to dashed line, the better(d) White Wine18Under review as a conference paper at ICLR 2021(a) Airfoil(b) BostonPap-Pald0.20.00.0	0.2	0.4	0.6	0.8	1.0actual(c) RedPap-PaId0.20.00.0	0.2	0.4	0.6	0.8	1.0actual(d) Yacht HydrodynamicsFigure 4: Dashed line (y=x) indicates perfect Calibration. The closer to dashed line, the better19
Figure 4: Dashed line (y=x) indicates perfect Calibration. The closer to dashed line, the better19Under review as a conference paper at ICLR 2021C.3 VARYING QUANTILE REGULARIZATION PARAMETER LIn the following sections, we show how quantile regularization parameter L affects both calibrationerror and Root Mean Square Error (RMSE) for dropout-VI in Sec. C.3.1 and for deep ensembles inSec. C.3.2. We do so by fixing spacing value to k = √n, where n is batch-size and varying LC.3.1 for dropout3	4QR reg parameter L1.51.05求」Ota uoμpq=3Figure 5: On X-axis we have Calibration Error (%). On Y-axis we have the QR-reg parameterL. Each curve with same color represents calibration error for a particular dataset as We vary QR-reg parameter L for {1,2,3,4, 5} . Spacing value is set to k = √n. Here model is Dropout.
Figure 5: On X-axis we have Calibration Error (%). On Y-axis we have the QR-reg parameterL. Each curve with same color represents calibration error for a particular dataset as We vary QR-reg parameter L for {1,2,3,4, 5} . Spacing value is set to k = √n. Here model is Dropout.
Figure 6: On X-axis we have Root Mean Square Error (RMSE). On Y-axis we have that QR-reg pa-rameter L. Each curve with same color represents RMSE for a particular dataset as we vary QR-regparameter L for {1, 2, 3,4,5} . Spacing value is set to k = √n. Here the model is Dropout.
Figure 7:	On X-axis we have Calibration Error (%). On Y-axis we have the QR-reg parameter L.
Figure 8:	On X-axis we have Root Mean Square Error (RMSE). On Y-axis we have the QR-regparameter L. Each curve with same color represents RMSE for a particular dataset as we varyQR-reg parameter L for {1, 2,3,4,5}. Spacing value is set to k = √m. Here the model is DeepEnsemble. Datasets are divided into two groups based on the scale of RMSE , which is useful forviewing the plots. On left, plot is for {airfoil, boston, concrete, yacht, protein }. On right, plot is for{fish, kin8nm, red, white}.
Figure 9:	On X-axis We have Cailibration Error (%). On Y-axis We have the spacing multi-plier t. Resultant Spacing value for spacing multiplier t is k = t.√n. Each curve with samecolor represents Cailibration Error (%) for a particular dataset as We vary spacing multiplier t for{1, 2, 3, 4, 5} . Here the model is Dropout. Quantile Regularization parameter is fixed at L = 1.
Figure 10: On X-axis we have Root Mean Square Error (RMSE). On Y-axis we have the spacingmultiplier t. Resultant Spacing value for spacing multiplier t is k = t.√n. Each curve with samecolor represents RMSE for a particular dataset as we vary spacing multiplier t for {1, 2, 3, 4, 5} .
Figure 11:	On X-axis We have Cailibration Error (%). On Y-axis We have the spacing multi-plier t. Resultant Spacing value for spacing multiplier t is k = t.√n. Each curve with samecolor represents Cailibration Error (%) for a particular dataset as We vary spacing multiplier t for{1, 2, 3, 4, 5} . Here the model is Deep Ensemble. Quantile Regularization parameter is fixed atL = 5. Datasets are divided into tWo groups based on the scale of Cailibration Error (%) , Which isuseful for vieWing the plots. On left, plot is for {airfoil, boston, concrete, yacht }. On right, plot isfor {fish kin8nm, red, White,protein,year}.
Figure 12:	On X-axis we have Root Mean Square Error (RMSE). On Y-axis we have the spacingmultiplier t. Resultant Spacing value for spacing multiplier t is k = t.√n. Each curve with samecolor represents RMSE for a particular dataset as we vary spacing multiplier t for {1, 2, 3, 4, 5}. Here the model is Deep Ensemble. Quantile Regularization parameter is fixed at L = 5.
Figure 13: Base Represents FC-DenseNet57 in Fig. 13a and QR represents FC-DenseNet 57 trainedwith Quantile Regularization. Base Represents FC-DenseNet103 in Fig. 13b and QR represents FC-DenseNet103 trained with Quantile Regularization. Ideal line is y = x line which represents perfectcalibration. The More closer the curve to diagonal line the better.
Figure 14: Base+iso Represents FC-DenseNet57 in Fig. 14a after isotonic claibration and QR rep-resents FC-DenseNet 57 trained with Quantile Regularization after isotonic calibration. Base Rep-resents FC-DenseNet103 in Fig. 14b and QR represents FC-DenseNet103 trained with QuantileRegularization. Ideal line is y = x line which represents perfect calibration. The More closer thecurve to diagonal line the better.
