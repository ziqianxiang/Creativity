Figure 1: Our proposed Synthesizer model architecture.
Figure 2: Histogram of Encoder and Decoder Attention Weights on MT (WMT EnDe). L denotesthe layer number and Enc/Dec denotes encoder or decoder.
Figure 3: Init Decoderweights (Reference)A.5 What patterns do Synthesizers learnIn this section, we perform a deeper analysis of the Synthesizer model.
Figure 4: Visual analysis of Synthetic Attention (decoder) on WMT EnDe.
Figure 5: Visual analysis of Synthetic Attention (encoder) on WMT EnDe.
