Figure 1: Representative generated images (of resolution 128 × 128) using our proposed algorithm.
Figure 2: A description of generative procedure in our method. (a) schematic of an auto-encoder (b) kernelrepresentations of latent space and noise from prior (c) operator learning and inferencing (d) solve pre-imageand decode the generated latent representation to obtain final generation result in data spaceRemark 1. A bounded latent space with specified structure provides several benefits over one withoutany such constraints. First, it guarantees that the Gram matrix constructed with a (positive-definite)kernel function is bounded, which is required for a valid empirical estimation of the mean embeddingin Def. 2 Song et al. (2009). Moreover, the projection onto the hypersphere also helps ensure thepositive-definiteness of kernels constructed using the latent representations, which is discussed in3.2. At this point, the requirement of geodesic completeness and convexity may not be apparent butthese properties will be useful for sample generation.
Figure 4: The geodesic interpolation algorithmProperties. If KZ is indeed invertible, the empirical meanembedding of samples generated using the proposed al-gorithm (assuming the preimage is exact) is equal to theempirical mean embedding of the latent representations,indicating a match in distribution. Further, we use locally-weighted Frechet mean on sphere to construct approxi-mate preimage of sample in RKHS. Since a closed-formSolUtion for the weighted Frechet mean does not exist onthe sphere, we propose to use a simple and efficient al-gorithm, namely the geodesic interpolation (gI) Salehianet al. (2015), that uses the geodesic on a hypersphere toiteratively computes the weighted FreChet mean FreChet (1948) of top Y latent representations (seeFig. 4). The algorithm has the following properties: (i) the geodesically completeness of the latentspace guarantees that the geodesic interpolation is well-defined (ii) the geodesic convexity of the la-tent space guarantees that the output of “gI” algorithm lies on the latent space (iii) the “gI” algorithmconverges asymptotically to the FreChet mean Salehian et al. (2015).
Figure 3: Step by step details of our sample generation algorithm.
Figure 5: 10k samples from MNIST dataset (left to right) (a) projected on S2 shown in (θ, φ) using auto-encoder, and 10K generated samples using (b) RBF (c) arccos (d) NTK. Minimum regularization has beenapplied to all kernels to ensure invertibility. Color of sampled points represents the class of their ’nearest’ pointin the feature space.
Figure 6: Generations (in red box) and training samples corresponding to the top-5 latent represen-tations used in geodesic interpolation. It can be observed that the samples with top kernel valuesindeed share high visual similarity.
Figure 7: Comparison of different sampling techniques using AE trained on CelebA 64x64. Left toright: (1) Interpolation among 10 random latent points (2) samples of SRAE+Glow (3) samples oftwo-stage VAE (4) samples of kPF-flow using 10k latent pointsBrain Imaging dataset: In this section, we present results on generating high-resolution (160 ×196 × 160) 3D brain images from ADNI consists of 183 samples from group AD (diagnosed asAlzheimer’s Disease) and 291 samples from group CN (control normals). In this setting (wheren = 474 d = 5017600), it is often hard to model the data distribution using either variationalmethods or flow-based methods due to the high variance of the data or the memory-inefficiency ofthe operations. However, benefiting from the regularized and linear nature of our kernel operatorWilloughby (1979); Arora et al. (2020), we can still generate high-quality samples in such resolu-tion that are in-distribution. As before, we present the comparative results with respect to the VAEmodel. The generated samples presented in Fig. 8 clearly demonstrate that proposed method gen-erates sharper images. To evaluate whether these results are also scientifically meaningful (and notmerely visually pleasing), we tested consistency between statistical group difference testing on thereal images (groups were AD and CN) and the same testing performed on the generated samples. Weperformed a FWER corrected two-sample t-test Ashburner & Friston (2000) in a manner consistentwith standard practice Ashburner & Friston (2000); Winkler et al. (2014). The results (see Fig 8)show that while there is a deterioration in regions identified to be affected by disease (and different8Under review as a conference paper at ICLR 2021
Figure 8: Left. Top: data, generated samples of Middle: VAE samples, Bottom: kPF samples. Standard Glowfailed to fit into system memory for image of this resolution. Right. Statistically significant regions Top: data,Bottom: samples are shown in negative log p-value thresholded at p < 0.01.
Figure 10: Top to bottom: no latent spacewith kPF, (2) interpolation in Fourier space,(3) Glow as transfer operator, (4) kPF astransfer operatorWe further show the advantage of using a regularizedAE over a regular one. The regularization scheme weapplied is derived from Ghosh et al. (2020), where weapply spectral normalization on all encoder-decodertransformations. Regularization has been shown tolead to smoother latent spaces Alain & Bengio (2014),	MNIST	CIFAR	CelebASAENTK-kPF 10k	22.3	133.6	41.2SRAENτκ-kPF 10k	15.0	123.3	39.9Table 2: vanilla vs. regularized AE.
Figure 11: Randomly picked samples from (1) VAE (2) 2-Stage VAE (3) SAENTK-kPF 10k (4)SRAENTK-kPF 10k. All models trained with the architecture in Ghosh et al. (2020) without tuningfor hyperparamtersA.6 CHOICE OF γIn the inference stage, our proposed method finds the approximate preimage of the transferred kernelembeddings by interpolating among the top γ latent representations of the training samples weightedby their kernel values. The choice of γ therefore has implications on the generation quality. FromFigure 12, we can observe that, in general, FID worsens as γ increases. This observation aligns withour intuition of preserving only the local similarities in kernel embeddings, and similar idea has14Under review as a conference paper at ICLR 2021been used in the literature Kwok & Tsang (2004). However, significantly decreasing γ leads to theundesirable result where the generator merely generates the training samples (in the extreme casewhere γ = 1, generated samples will just be reconstructions of training samples). Therefore, in ourexperiments, we choose γ = 10 to achieve a balance between generation quality and the distance totraining samples.
Figure 12: FID versus choice of γA.7 Experimental specificationsFor all experiments in Table 1 except the GAN experiments, we adapted a custom ResNet imple-mentation where each block has residual connection→ BatchNorm → Swish → 3x3 Conv → BatchNorm → Swish → 3x3 Conv →The detailed architecture is given by the following table	MNIST	CIFAR-10	CelebAEncoder	5x5 conv, stride 1			ResBlock32 X 2 ResBlock64 × 2 ResBlock128 × 2 ResBlock256 × 2	ResBlock32 × 2 ResBlock64 × 2 ResBlock128 × 2 ResBlock256 × 2	ResBlock32 × 2 ResBlock64 × 2 ResBlock128 × 2 ResBlock256 × 2 ResBlock512 × 2Decoder	ResBlock256 × 2 ResBlock128 × 2 ResBlock64 × 2 ResBlock32 × 2	ResBlock256 × 2 ResBlocki28 × 2 ResBlock64 × 2 ResBlock32 × 2	ResBlock512 × 2 ResBlock256 × 2 ResBlock128 × 2 ResBlock64 × 2 ResBlock32 × 2	5x5 conv, stride 1		Table 4: Detailed network architecture for experiments in Table 1. Subscript denotes the number ofinput channels. Upsampling and downsampling are performed using strided convolutions.
