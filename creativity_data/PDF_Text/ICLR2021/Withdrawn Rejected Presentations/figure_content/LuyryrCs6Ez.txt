Figure 1: Concept Space. Three example concepts (rows) along with schematic positive examples. Actualscenes are rendered in multiple ways including the CLEVR renderer (Johnson et al., 2016) (see Figure 2). Right:The grammar of variables, quantifiers, functions and operators to induce compositional concepts.
Figure 2: CURI Task. Given as input a support setDsupp, with positive and negative examples correspond-ing to concept, the model has to infer the concept andproduce accurate predictions on novel images (right).
Figure 3: Language of thought. All valid (type-consistent) compositions of functions are potential complexconcepts in our dataset. Note that the functions are illustrated for the case of images and schemas. Location,size, shape etc. correspond to different properties for sounds.
Figure 4: Baseline models (left). Different choices for the encoder f (u) parameterization explored in the paper.
Figure 5: Compositionality Gap. Different splits (x-axis) plotted w.r.t performance of the strong oracle (greenline) and weak oracle (red line) on the mAP (top) and Accuracy (bottom) evaluated on respective test splits(using hard negatives in support and query sets). Difference between the two is the compositionality gap (compgap). Yellow: shows the (best) relation-net model on schema inputs, purple: shows the model on image inputs,and gray: shows the model on sound inputs. Error bars are std across 3 independent model runs.
Figure 6: Qualitative Example of an Episode in CURI dataset. Best viewed zooming in, in color.
Figure 7: Qualitative Example of an Episode in CURI dataset. Best viewed zooming in, in color.
Figure 8: Qualitative Example of an Episode in CURI dataset. Best viewed zooming in, in color.
Figure 9: Qualitative Example of an Episode in CURI dataset. Best viewed zooming in, in color.
Figure 10: Qualitative Example of an Episode in CURI dataset. Best viewed zooming in, in color.
Figure 11: Qualitative Example of an Episode in CURI dataset. Best viewed zooming in, in color.
Figure 12:	Histogram of properties found in inputs u in the dataset. We notice that the properties are all largelyuniform with bias in x and y-coordinates towards the center of the image.
Figure 13:	Histogram of number hypotheses with same evaluation signatures.
Figure 14: mAP on validation for hard negatives (y-axis) vs number of training steps (x-axis) for relation networkmodels on images with different dimensionality for the object embedding oi .
Figure 15: mAP on validation for hard negatives (y-axis) vs number of training steps (x-axis) for relation networkmodels on images with different learning rates.
Figure 16: mAP on validation for hard negatives (y-axis) vs number of training steps (x-axis) for relation networkmodels on images with different amounts of language usage by varying the parameter α.
Figure 17: mAP on validation for hard negatives (y-axis) vs number of training steps (x-axis) for relation networkmodels on schemas with different amounts of language usage by varying the parameter α.
Figure 18: mAP on validation for hard negatives (y-axis) vs number of training steps (x-axis) for concat poolingmodels on schemas with different amounts of language usage by varying the parameter α.
Figure 19: mAP (top) and accuracy (bottom) metrics for the different splits presented in the paper when easynegatives are used (ordered by their corresponding comp gap). Yellow: shows the schema relation-net modelwhile purple: shows the image relation-net model. Notice that compared to Fig. 5 in the main paper, the compgap is smaller and the models appear to be substantially closer to the strong oracle in this setting compared towhen we use hard negatives.
