Figure 1: Illustration of Generalized Message Aggregation FunctionsIn this work, we analyze the performance of GCNs on large-scale graphs. In particular, we look atthe effect of aggregation functions in performance. We unify aggregation functions by proposing anovel Generalized Aggregation Function (Figure 1) suited for graph convolutions. We show howour function covers all commonly used aggregations (mean, max, and sum), and its parameters canbe tuned to learn customized functions for different tasks. Our novel aggregation is fully differen-tiable and can be learned in an end-to-end fashion in a deep GCN framework. In our experiments,we show the performance of baseline aggregations in various large-scale graph datasets. We thenintroduce our generalized aggregation and observe improved performance with the correct choiceof aggregation parameters. Finally, we demonstrate how learning the parameters of our generalizedaggregation, in an end-to-end fashion, leads to state-of-the-art performance in several OGB bench-marks. Our analysis indicates the choice of suitable aggregations is imperative to the performanceof different tasks. A differentiable generalized aggregation function ensures the correct aggregationis used for each learning scenario.
Figure 2: Training loss of PlainGCN, ResGCN and ResGCN+Depth & Normalization. In our experiments, we find normalization techniques play a crucial role intraining deep GCNs. Without normalization, the training of deep network may suffer from vanishinggradient or exploding gradient problem. We apply normalization methods such as BatchNorm (Ioffe& Szegedy, 2015) or LayerNorm (Ba et al., 2016) to normalize vertex features. In addition to this,we also propose a message normalization (MsgNorm) layer to normalize features on the messagelevel, which can significantly boost the performance of networks with under-performing aggregationfunctions. The main idea of MsgNorm is to normalize the features of the aggregated message m(vl) ∈RD by combining them with other features during the vertex update phase. Suppose we apply theMsgNorm to a simple vertex update function MLP(h(vl)+m(vl)). The vertex update function becomesas follows:(l)hl+I) = φ(I)(hVl), mVl)) = MLP(hVl) + S ∙ khVl)k2 ∙	)	⑸km(vl)k2where MLP(∙) is a multi-layer perceptron and S isa learnable scaling factor. The aggregated messagem(Vl) is first normalized by its `2 norm and then scaled by the `2 norm of h(Vl) by a factor of s. Inpractice, we set the scaling factor S to be a learnable scalar with an initialized value of 1. Notethat when S = km(Vl) k2/kh(Vl) k2, the vertex update function reduces to the original form. In ourexperiment, we find MsgNorm boosts performance of under-performing aggregation functions suchas mean and PowerMean on ogbn-proteins more than 1%. However, we do not see any significant
Figure 3: Learning curves of 112-layer DyResGEN with SoftMaxSum-Agge(∙).
Figure 4: Learning curves of 112-layer DyResGEN with PowerMeanSum-Aggp (∙).
