Figure 1: Performance results of the selection method against base and shaped agentsgot stuck in a local optimum of not moving at all. The CartPole environment shows that selectionis able to maintain optimal solutions despite perturbations in learned behaviors. The selection agentwas able to quickly re-converge to the solution each time a fluctuation occurred. This also under-lines the dynamic nature of our selection method for automatic reward adaptation. In response topoor behaviors encountered during learning, the selector was able to re-discover the correct shapingreward signals and re-learn the optimal solution. The Pendulum environment was quickly solved byall three agents. It acts as a baseline check for the selection method and demonstrates that selectiondoes not lead the agent to other solutions with suboptimal performance.
Figure 2: Comparisons in performance between our selection idea and the automatic reward adap-tation method presented in Fu et al. (2019)When applied to these versions of the problems, the selector was still able to identify the correctshaping reward signals to listen to. The selection agent converged to the correct policy in both theCartPole and MountainCar problems, often faster than the Fu agent. In CartPole, selection arrivedat a better solution than Fu earlier in learning. In MountainCar, selection learned a policy that per-formed slightly worse than that learned by Fu’s method, but discovered it earlier. Later in learning,the selection agent maintained competitive performance despite lacking the extra time to computeoptimal usage of available shaping reward signals. After reaching convergence, the selection agent’sperformance remained close to that of the Fu agent. While there was some fluctuation in the pol-icy, the selection method was consistently able to adjust the agent back towards the optimal policy.
