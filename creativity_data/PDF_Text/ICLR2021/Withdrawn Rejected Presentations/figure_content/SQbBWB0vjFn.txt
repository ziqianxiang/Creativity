Figure 1: (a): local dependencies of the blue point captured by a 3 × 3 regular convolution operator;(b): long-range dependencies of the blue point highlighted in distant locations and captured by ourlong-range module; (c) and (d) show that our long-range module can flexibly adjust the numberof focused sampling locations, related to the blue point, to capture long-range dependencies; and(e) presents eight sample results generated by our lightweight long-range network at 256 × 256.
Figure 2: (a): architecture of the proposed lightweight long-range generative adversarial networks;the red dashed box indicates the generation of metadata; (b): implementation of the long-rangemodule in spatial direction; (c): implementation of the long-range module in channel-wise direction.
Figure 3: Qualitative results generated by our method on FFHQ (left), CUB (middle), and ImageNetChurch (right). The top row shows the images that are used to provide metadata for the model.
Figure 4: Qualitative comparison of three methods on the FFHQ (top row), CUB (middle), andImageNet Church (bottom) datasets.
Figure 5: FID values at different epochs on the FFHQ (top) and ImageNet Church (bottom) datasets.
Figure 6: Architecture of the residual block.
Figure 7: Architecture of the upsampling block.
Figure 8: Additional results generated by our method on the FFHQ dataset at 256 × 256.
Figure 9: Additional results generated by our method on the CUB bird dataset at 256 × 256.
Figure 10: Additional results generated by our method on the ImageNet dataset at 256 × 256. Thecategories are cat, dog, beer bottle, and bike.
Figure 11: Additional results generated by our method on the ImageNet dataset at 256 × 256. Thecategories are baobab, valley, bus, and tiger.
