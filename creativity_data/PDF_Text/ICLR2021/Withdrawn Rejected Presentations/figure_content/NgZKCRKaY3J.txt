Figure 1: Bias in ECEBIN for perfectly cali-brated models. Simulated data from a perfectlycalibrated model with confidence scores mod-eled to ResNet-110 CIFAR-10 output (He et al.,2016; Kangsepp, 2019). We show a reliabilitydiagram for a sample of size n = 200 and thedistribution of ECEBIN scores computed across106 independent simulations. Even though themodel is perfectly calibrated, ECEBIN systemati-cally predicts large calibration errors.
Figure 2: Bias affects which recalibration algorithm is preferred. For ten models, we reportwhich recalibration method is determined to be superior based either on ECEBIN or ECEsweep. Thewide bar indicates the superior method using entire validation set (mean of X instances); narrow barseach use a random sample of 10% of the original validation set. Recalibration methods tested arehistogram binning, temperature scaling, and isotonic regression.
Figure 3: Curves controlling true calibra-tion error. Our ability to measure calibrationis contingent on both the confidence scoredistribution (e.g., f(X)ã€œBeta(2.8,0.05))and the true calibration curve (e.g., EY [Y |f(X) = c] = c2.
Figure 4: ECEBIN with equal width binning canoverestimate TCE and the optimal number ofbins depends on number of samples.
Figure 5: Maximum likelihood fits to empirical datasets illustrate large skew in their densitydistribution and calibration function. For each dataset, (a) confidence distributions were fit witha two-parameter beta distribution and (b) calibration curves were fit via generalized linear modelsacross multiple model families, with the best model selected via the Akaike information criterion(details in Appendix A). Across models, the dataset source systematically affects both curves. (c)We plot the overall quality of the fits by computing the ECEBIN on the original data vs. the ECEBINaveraged over 1000 simulated trials. Curves well-fit to the data should lie close to the identity line.
Figure 6: EMsweep is less biased than alternative calibration metrics. We plot bias versusnumber of samples n for calibration metrics on simulated data drawn from the CIFAR-10, CIFAR-100, and ImageNet fits (Section 5.1). The dataset the model was trained on has a greater influence onbias than the model architecture. Metrics based on equal mass binning consistently outperform equalwidth binning. Exploiting monotonicity in the EMsweep metric helps the most at small sample sizes.
Figure 7: Bias in calibration estimation increases as TCE decreases. We Plot average ECE(%) for EW (left) and EMsweep (right) versus the TCE (%), with varying samPle size and scoredistributions. The estimator bias is systematically worse for better calibrated models, and the effect ismore egregious with fewer samPles. At n = 200 samPles, dePending on the score distribution, an EWestimate of 12% could either corresPond to 5% or 8% TCE. The EMsweep metric is able to mitigatethe bias and ambiguity in calibration error estimation to a certain extent.
Figure 8: Probability of failing to de-tect miscalibration (type II error, ormiss rate), plotted as a function of TCEfor various sample sizes (n), with typeI error rate fixed at 0.05. EMsweep(dashed lines) obtains a significantlylower failure rate than EW (solid lines).
Figure 9: Bias for various calibration metrics assuming curves fit to CIFAR-10 ResNet-110output. We plot bias for various calibration metrics using both equal-width binning (left column)and equal-mass binning (right column) as we vary both the sample size n and the number of bins b.
Figure 10:	Bias for various calibration metrics assuming curves fit to CIFAR-100 Wide ResNet-32 output. We plot bias for various calibration metrics using both equal-width binning (left column)and equal-mass binning (right column) as we vary both the sample size n and the number of bins b.
Figure 11:	Bias for various calibration metrics assuming curves fit to ImageNet ResNet-152output. We plot bias for various calibration metrics using both equal-width binning (left column)and equal-mass binning (right column) as we vary both the sample size n and the number of bins b.
Figure 15: Bins chosen by equal mass ECESWEEP method. We plot equal mass ECEBIN % versusnumber of bins for various sample sizes n. We highlight the TCE with a horizontal dashed line andshow the average number of bins chosen by the ECESWEEP method for different sample sizes withvertical dashed lines. When the model is uncalibrated (left) ECESWEEP chooses a bin number thatis close to optimal. However, for perfectly calibrated models (right), the optimal number of bins issmall (<=4), and ECESWEEP does not do a good job of selecting a good bin number. The incorrectbin selection may partially explain why ECESWEEP still has some bias for perfectly calibrated models.
