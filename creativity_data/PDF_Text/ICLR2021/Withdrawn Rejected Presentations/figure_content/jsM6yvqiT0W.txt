Figure 1: Post-calibration training curves on a WideResNet-28-10 on CIFAR-100. Temperature scalingminimizes the training and validation NLL reasonably well (and improves the ECE), but still underfits theNLL. Matrix scaling learns a higher-capacity matrix calibrator and minimizes the training NLL better, butdoes not improve the ECE since the calibrator does not maintain the accuracy and is encouraged to improve theaccuracy instead of calibration. Our Neural Rank-Preserving Transforms (NRPT) learns a higher-capacitycalibrator that preserves the accuracy, and improves both the training/validation NLL as well as the ECE.
Figure 2: Comparison between TS, LTS, and NRPT in the calibration abilities. Each dot in (b) is obtained byoptimizing a weighted combination of the NLL and ECE loss. Shaded area in (a) and crosses in (b) indicate thestandard deviation over 4 random seeds.
Figure 3: Illustration of the PRR metric.
Figure 4: Accuracy among most-confident examples on ImageNet.
