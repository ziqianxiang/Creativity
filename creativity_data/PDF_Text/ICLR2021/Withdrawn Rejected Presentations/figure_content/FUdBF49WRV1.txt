Figure 1: Overview of the steps required to aggregate messages in the direction of the eigenvectors.
Figure 2: Possible directional flows in different types of graphs. The node coloring is a potentialmap and the edges represent the gradient of the potential with the arrows in the direction of theflow. The first 3 columns present the arcosine of the normalized eigenvectors (acos φ) as nodecoloring, and their gradients represented as edge intensity. The last column presents examples ofinductive bias introduced in the choice of direction. (a) The eigenvectors 1 and 2 are the horizontaland vertical flows of the grid. (b) The eigenvectors 1 and 2 are the flow in the longest and second-longest directions. (c) The eigenvectors 1, 2 and 3 flow respectively in the South-North, suburbs tothe city center and West-East directions. We ignore φ0 since it is constant and has no direction.
Figure 4: Realization of a radius-1 convolution using the proposed aggregators. Ix is the inputfeature map, * the convolutional operator, Iy the convolution result, and Bi = B(Vφi).
Figure 5: Test set results using a parameter budget of 〜100k, with the same hyperparameters asCorso et al. (2020). The low-frequency Laplacian eigenvectors are used to define the directions,except for CIFAR10 that uses the coordinates of the image. For brevity, we denote dxi and avi asthe directional derivative Bdix and smoothing Baiv aggregators of the i-th direction. We also denoteposi as the i-th eigenvector used as positional encoding for the mean aggregator.
Figure 6: Fine-tuned results of the DGN model against models from Dwivedi et al. (2020) and Huet al. (2020): GCN (Kipf & Welling, 2016), GraphSage (Hamilton et al., 2017), GIN (Xu et al.,2018a), GAT (VelickoVic et al., 2017), MoNet (Monti et al., 2017), GatedGCN (Bresson & Laurent,2017) and PNA (Corso et al., 2020). All the models use 〜100k parameters, except those with *who use 300k to 1.9M. In ZINC the DGN aggregators are {mean, dx1, max, min}, in PATTERN{mean, dx1, av1}, in CIFAR10 {mean, dx1, dx2, max}, in MolHIV {mean, dx1, av1, max, min}.
Figure 7: Accuracy of the Various models using data augmentation with a complex architectureof 〜 100k parameters and trained on 10% of the CIFAR10 training set (4.5k images). An angleof X corresponds to a rotation of the kernel by a random angle sampled uniformly in (-x°,x°)using definition 5 with F1,2 being the gradient of the horizontal/Vertical coordinates. A noise of100x% corresponds to a distortion of each eigenVector with a random noise uniformly sampled in(-χ ∙ m,χ ∙ m) where m is the average absolute value of the eigenvector,s components. The meanbaseline model is not affected by the augmentation since it does not use the underlining Vector field.
Figure 8: Illustration of an example pair of graphs which the 1-WL test cannot distinguish butDGNs can. The table shows the node feature updates done at every layer. MPNN with mean/sumaggregators and the 1-WL test only use the updates in the first row and therefore cannot distinguishbetween the nodes in the two graphs. DGNs also use directional aggregators that, with the vectorfield given by the first eigenvector of the Laplacian matrix, provides different updates to the nodesin the two graphs.
