Figure 1: Experimental results for the MNIST dataset. (a) The median, min, and max of the log of variance ofthe stochastic gradient estimators for two different mini-batch sizes (distinguished by colors) and five differentinitial weights. The solid lines show the median of all five initial weights while the highlighted regions showthe min and max of the log of variance. (b) The log of the training and validation loss vs epochs. (c) The log oftraining and validation error vs epochs. Here error is defined as one minus predicting accuracy. The plot doesnot show the epochs if error equals to zero. (d) The gap of accuracy on training and test sets vs epochs startingfrom epoch 100.
Figure 2: Experimental results for the Graduate Admission dataset. Left: log (Var (gb∣F0)) andlog (Var (VLPwbq ∣F0)) vs iteration t for 4 different mini-batch sizes. Right: The log of polynomial Val-ues when fitting polynomials on selected mini-batch sizes at certain iterations.
Figure 3:	Experimental results for the Synthetic dataset. Left: log Var gtb,1 ∣F0and log (Var(VWIL(We1, Wtb2) ∣F0))	VS iteration	t.	Right:	log `var (gb,2∣F0))	andlog (Var(V w2 L(Wtb,1 ,W1,2) ∣F0)) VS iteration t.
Figure 4: Experimental results for the MNIST dataset. Left: The median, min, and max of the log of varianceof the stochastic gradient estimators for two different mini-batch sizes (distinguished by colors) and five dif-ferent initial weights. The solid lines show the median of all five initial weights while the highlighted regionsshow the min and max of the log of variance. Right: The gap of accuracy on training and test sets vs epochsstarting from epoch 100.
Figure 5: Experimental results for the MNIST dataset. Left: The log of the training and validation loss vsepochs. Right: The log of training and validation error vs epochs. Here error is defined as one minus predictingaccuracy. The plot does not show the epochs if error equals to zero.
Figure 6: Experimental results for the XLNet model on the Yelp dataset. Left: The variance of stochasticgradient estimators vs epochs. Middle: The training and validation loss vs epochs. Right: The training andvalidation error vs epochs.
