Figure 1: The networks found by the MDL learner for (a) the identity task and (b) the previous charactertask (same result for all training sets of length 10, 20, 50, and 100). The input (arriving to the yellow cellon the left) is directly fed into the output (blue cell on the right) with a weight of 1, either through a direct,‘contemporary’ connection (a), or through a cross-time connection in (b).
Figure 2: The network found by the MDL learner for the exactly 1 task. The network keeps track of the numberof 1’s seen so far in the middle input cell (see the recurrent connection that makes it persists in memory), andresets it when a # is input (see the -1 connection from the top input cell).
Figure 3: The network found by the MDL learner for (a) the at least 1 task and (b) the between 3 and 6 taskfor the largest training set.
Figure 4: The network found by the MDL learner for (a) the anbn, and (b) the anbncn task for the largesttraining set. In (a), the bottom right cell takes care of the counting: it decreases at each new input a (byincrements of 3), and decreases at each new increment of b (by increment of 3: +6 directly from the middleinput cell and -3 indirectly through the bottom left input cell). In network (b), the self-loop cell handles countingof all three symbols, first by decreasing by the number of a’s, then increasing for each b, then decreasing againfor c’s; the output cell values at each time step align with the counter’s value to create the correct probabilitydistribution.
Figure 5: The network found by the MDL learner for the addition task, trained on 400 pairs of numbers. Thefirst digit is added to the second digit, and the sum is squared in place. Next, a hidden cell (in yellow) with arecurrent connection was evolved to take care of the carry-over. The network reaches 100% accuracy on a testset consisting of all pairs of numbers up to 250, and is in fact provably correct for any arbitrary pair of numbers.
Figure 6: Cross-entropy of different learners across all the tasks presented here. For easier visualization, thecross-entropy is normalized (divided) by the cross-entropy of a learner predicting uniform probability for alloutputs. At the other extreme, the green line represents an optimal baseline as the cross-entropy of a learnerwhich would have captured the underlying task perfectly well. The gray lines represent the various RNNcompetitors, and the blue line is the MDL learner. The x-axis distributes the various tasks (identity, previouscharacter, addition, exactly n, at least n, between m and n, anbn, anbncn), for increasing sizes of the trainingset. Corresponding numbers are given in Appendix F.
