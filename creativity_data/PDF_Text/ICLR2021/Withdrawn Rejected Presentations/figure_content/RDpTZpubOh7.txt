Figure 1: A simple level of the SafeLife environment containing an agent (»), a SPaWner (器),crates (口)，and cells of life. The agent's goal is to remove unwanted red cells (prune task) andto create new patterns of life in the blue squares (append task). Once the agent has satisfactorilycompleted its goals it can leave via the level exit (皿).Note that all level boundaries wrap; they havetoroidal topology.
Figure 2: A co-training framework for safety aware RL training. The task agent, Aθ , determines theaction taken in the environment and the resulting trajectories. The virtual safe agent, Zψ , receives thesame state as Aθ and makes a suggestion for a safe action given the state. The distance between theaction probabilities of the Aθ and Zψ is captured in the Distribution Loss, Ldist . Zψ learns how tomaximize the safety objective on its own set of environments in parallel to Aθ .
Figure 3: Length ChamPion for SafeLife Suite of 1. prune-still (a-c), 2. append-still (d-f), 3. prune-dynamic (g-i), 4. append-dynamic (j-l) tasks evaluated for 100 testing environments every 100,000stePs on Episode Length (left column) where shorter is better, Performance Ratio (middle column)where higher is better, and Episodic Side Effect (right column) where lower is better5	ResultsThe results of the exPeriments shown in Figure 3 demonstrate that a virtual safety agent trained onone task in the SARL framework can generalize zero-shot to other tasks and environment settings inthe SafeLife suite, while maintaining comPetitive task and side effect scores comPared to the baselinemethod. This allows us to abstract the notion of safety away from the environment sPecific side effectmetric, and also increase the overall samPle efficiency of the SARL method for subsequent trainingruns. The SARL methods that are trained from scratch also show comPetitive task and side effectscores comPared to the baseline method.
