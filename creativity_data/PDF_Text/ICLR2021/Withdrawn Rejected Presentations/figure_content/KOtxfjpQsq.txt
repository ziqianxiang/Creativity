Figure 1: Learning curves. In each figure, the vertical axis represents returns, and the horizontal axis representsnumbers of training samples (x1000). The meta-policy and meta-model are fixed and evaluated interms of their average return on 50 episodes at every 5000 training samples for L2A and 1000 trainingsamples for the other methods. In each episode, the task is initialized and changed randomly. Eachmethod is evaluated in six trials, and average returns on the 50 episodes is further averaged over thetrials. The averaged returns and their standard deviations are plotted in the figures. We also plot thebest performances of PEARL and M3PO trained for a longer-term as PEARL-long and M3PO-long.
Figure 2: Learning curves of M3PO. In each figure, the vertical axis represents returns, and the horizontalaxis represents numbers of training samples (x1000). The meta-policy and meta-model are fixed andevaluated their expected returns on 50 episodes per 1000 training samples. We run experiments byvarying the rollout length k of M3PO. In these experiments, the values of hyperparamters exceptk are the same as those described in Table 1. Each case is evaluated in six trials, and the averagereturn on the 50 episodes is further averaged over the trials. The averaged expected returns and theirstandard deviations are plotted in the figures. Dashed lines represent M3PO’s learning curves inFigure 1. k = x → y means k values linearly changes from x to y during leaning.
Figure 3: Example of a meta-policy learned by M3PO with 200k samples in Humanoid-direc.The humanoidis highlighted in accordance with a task (red: move to left, and blue: move to right). The figures aretime-lapse snapshots, and the first figure on the left is the snapshot at the beginning time. Figuresshow the policy successfully adapting to the task change.
Figure 4: The outline of Model-based meta-RL formulation. Here, τ is a task, s0 is hidden state factors, o is an observation, h is a history, a is an action and r is a reward.
Figure 6: An example of branched rollouts with k = 3. Here, the blue dots represent trajectories contained inan environment dataset Denv and the yellow dots represent fictitious trajectories generated in accor-dance with a current policy π under a predictive model pθ .
Figure 7: The learning curve of vanilla PEARL (vPEARL), PEARL with RNN-traj (PEARL-tran), M3PO, andits variants in which the vPEARL and PEARL-tran are used for meta-policies. In each figure, thevertical axis represents expected returns and the horizontal axis represents the number of trainingsamples (x1000). The meta-policy and meta-model (and a predictive model) were fixed and theirexpected returns were evaluated on 50 episodes at every 1000 training samples for the other methods.
Figure 8: Environments for our experiment28Under review as a conference paper at ICLR 2021A.12 Complementary experimental resultsAUUedə-osoFigure 9: Discrepancy between Eπφ,p [R] and Eπφ,pθ [R] in Theorem 1. The vertical and horizontal axesrepresent the discrepancy value and m ∈ [0, 1], respectively.
Figure 9: Discrepancy between Eπφ,p [R] and Eπφ,pθ [R] in Theorem 1. The vertical and horizontal axesrepresent the discrepancy value and m ∈ [0, 1], respectively.
Figure 10: The local change in m0 with respect to π versus training sample size. In each figure, the verticalaxis represents the local change of the meta-model error (ddm0) and the horizontal axis representsthe training sample size (x1000). The red-dotted line is the linear interpolation of the blue dots,which shows the trend of the local change decreasing as the training sample size grows.
Figure 11: Transition of model errors on training. In each figure, the vertical axis represents empirical valuesof m and the horizontal axis represents the number of training samples (x1000). We ran five trialswith different random seeds. The result of the x-th trial is denoted by Trial-x. We used the negativeof log-likelihood of the meta-model on validation samples as the approximation of m . The figuresshow that the model error tends to decrease as epochs elapse.
Figure 12: Transition of meta-policy divergence on training. In each figure, the vertical axis represents empir-ical values of π and the horizontal axis represents the number of training samples (x1000). We ranthree trials with different random seeds. The result of the x-th trial is denoted by Trial-x. For π ,we used the empirical Kullback-Leibler divergence of πθ and πD . Here, πD has the same policynetwork architecture with πθ and is learned by maximizing the likelihood of actions in D (Denv forAlgorithm 2).
Figure 13: Learning curve of PEARL and M3PO in a long-term training. In each figure, the vertical axisrepresents expected returns and the horizontal axis represents the number of training samples(x50000). The meta-policy and meta-model were fixed and their expected returns were evaluatedon 50 episodes at every 50,000 training samples. Each method was evaluated in three trials, and theresult of the x-th trial is denoted by method-x. Note that the scale of the horizontal axis is largerthan that in Figure 1 by 50 times (i.e., 4 in this figure is equal to 200 in Figure 1).
Figure 14: Comparison of GHP-MDP (Algorithm 1 in Perez et al. (2020)) and M3PO. The figures show learn-ing curves of GHP-MDP and M3PO. In each figure, the vertical axis represents expected returnsand the horizontal axis represents the number of training samples (x1000). GHP-MDP was eval-uated in two trials, and each trial was run for three days in real-times. Due to the limitation ofcomputational resources, we could not run this experiment as many days and trials as other exper-iments. The expected returns of GHP-MDP in each trial (denoted by “Trial-1” and “Trial-2”) areplotted in the figure. The results of M3PO is referred to those in Figure 1. From the comparisonresult, we can see that M3PO achieves better sample efficiency than GHP-MDP.
Figure 15: Learning curve of M3PO and M2PO. In each figure, the vertical axis represents expected returns andthe horizontal axis represents the number of training samples (x1000). The meta-policy and meta-model (and a predictive model) were fixed and their expected returns were evaluated on 50 episodesat every 1000 training samples for the other methods. In each episode, the task was initialized andchanged randomly. Each method was evaluated in at least five trials, and the expected return on the50 episodes was further averaged over the trials. The averaged expected returns and their standarddeviations are plotted in the figures.
Figure 16: Transition of TD-errors (Q-function error) on training. In each figure, the vertical axis representsempirical values of m and the horizontal axis represents the number of training samples (x1000).
Figure 17: Learning curve of PEARL, M3PO and M3PO-h in a long-term training. In each figure, the verticalaxis represents expected returns and the horizontal axis represents the number of training samples(x50000). The meta-policy and meta-model were fixed and their expected returns were evaluatedon 50 episodes at every 50,000 training samples. Each method was evaluated in three trials, and theresult of the x-th trial is denoted by method-x. Note that the scale of the horizontal axis is largerthan that in Figure 1 by 50 times (i.e., 4 in this figure is equal to 200 in Figure 1).
Figure 18: High-level summary of what we do in this paper based on previous works. Our core contributionis proposing a model-based meta-reinforcement learning (RL) algorithm (i.e., Algorithm 2) withperformance guarantee. To achieve this, we first generalize various existing model-based meta-RLsettings as solving a partially observable Markov decision process (POMDP) (Section 4). We thenconduct theoretical analysis by extending the theorems proposed in Janner et al. (2019). We extendtheir theorems (Theorems 4.1, 4.2 and 4.3) into our POMDP setting (Sections 5.1 and 5.2). As apart of our extension, we refine Theorem 4.2 and 4.3 in order to strictly guarantee the performanceof branched rollouts (Section A.7). We also compare these theorems in order to motivate usingbranched rollouts for model-based meta-RL (Sections 5.2 and A.6). We finally propose a practicalalgorithm based on the result of our theoretical anyways (Section 6).
