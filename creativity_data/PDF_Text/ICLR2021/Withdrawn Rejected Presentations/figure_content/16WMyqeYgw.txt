Figure 1: Average episode rewards in the Cartpole balancing problem. The curves and the shadowedareas represent the means and the quartiles over 9 independent runs. The models are evaluated for10 evaluation episodes every 200 training episodes.
Figure 2: Improvement in normalized scores of V-DQN over DDQN in 200M framesιo3-∣=^IO2 -IO1 -IO0--ioɔ H-IOH-叫-IO，」Lossebn-SU-UDUU<COEU□x=sv」。0。-nBSSVSJep>u一8sX-UsoqdUMOaNdnSRURVsənbeəsOcU6u 支
Figure 3:	Improvement in normalized scores of TD-DQN over DDQN in 200M framesmini-batch of 32 samples over prioritized replay buffer every 4 training steps. The target network isupdated every 30K steps.
Figure 4:	The mean and median of the normalized training curve over all 55 Atari gamesused to quantify the rate of convergence of the return series for specific state-action pairs. The returndynamics of value-based reinforcement learning is particularly susceptible to diverging as valueimprovements beget policy ones. This paper introduces a new two-stream network architecture toestimate both weighted variance/TD errors; both our techniques (V-DQN and TD-DQN) outperformDDQN on the Atari game benchmark. We have identified two promising directions for future workin this area: 1) unifying exploration architectures to ameliorate the cold start issues and 2) adaptingour exploration strategies to other deep reinforcement learning models.
Figure 6: Smoothed return sequences (a) and corresponding σ values (b) over visitations.
Figure 5: Raw return sequences (a) and corresponding σ values (b) over visitations.
Figure 7: Residual return sequences (a) and corresponding σ values (b) over visitations.
Figure 8: Training curve on Atari games from a single training run each. Episodes start with up to30 no-op actions. Each data point is an average of episode rewards from 500K frames of evaluationruns, and smoothed over 10 data points.
