Figure 1: Spatio-temporal self-similarity (STSS) representation learning. STSS represents eachspatio-temporal position (query) as its similarities (STSS tensor) with its neighbors in space and time(neighborhood). STSS allows to take a generalized, far-sighted view on motion, i.e., both short-termand long-term, both forward and backward, as well as spatial self-motion. Our method learns toextract a rich and effective motion representation from STSS without additional supervision.
Figure 2: Overview of our self-similarity representation block (SELFY). SELFY block takes asinput a video feature tensor V, transforms it to a STSS tensor S, and extracts a feature tensor Ffrom S. It then converts F to the same size as the input V via the feature integration, and combinesit with the input V to produce the final STSS representation Z. See text for details.
Figure 3: Feature extraction from STSS. For a spatio-temporal position (t, x, y), each methodtransforms (L, U, V ) volume of STSS tensor S into (L, CF). See text for details.
Figure 4: Basic blocks and their combinations. (a)spatio-temporal convolution block (STCB), (b) SELFY-sblock, and (c-f) their combinations.
Figure 5: Results of the robustness experiment. (a) and (b) show top-1 accuracy of SELFYNetvariants (Table 3a) with different occlusions and motion blurs, respectively. (c) shows qualitativeexamples that SELFYNet ({-3,â€¦,3}) answers correct, while SELFYNet ({l}) fails.
Figure 6: Comparison with non-local approaches (Wang et al., 2018; Liu et al., 2019) andcorrelation-based methods (Kwon et al., 2020; Wang et al., 2020). From the top, non-local block,CP module, MS module, Correlation block and SELFY block are illustrated.
Figure 7: Qualitative results of two SELFYNets on SS-V1. Each subfigure visualizes predictionresults of the two models with Grad-CAM-overlaid RGB frames. The correct and wrong predictionsare colorized as green and red, respectively.
