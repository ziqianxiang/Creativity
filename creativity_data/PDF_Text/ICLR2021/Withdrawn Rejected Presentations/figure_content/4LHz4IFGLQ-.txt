Figure 1: (Left) Traditional 3-gram CBOW with negative sampling. (Middle) 3-gram CBOW seenas a sequence of continuous state manipulations. (Negative sampling is not shown) (Right) 3-gramDiscrete Sequential Application of Words model. BN=Batch Normalization, BC=Binary Concrete.
Figure 2: Back-To-Logit architecturecube-like graph (Payan, 1992) is a graph class originating from graph theory. Asai & Muise (2020)identified that state transition graphs of STRIPS planning problems is equivalent to directed cube-likegraph. A cube-like graph G(S, D) = (V, E) is a simple2 undirected graph defined by the sets S and D.
Figure 3: (Left) A graph representing a 3-dimensional cube which is a cube-like graph. (Right) Agraph whose shape is identical to the left, but whose unique node embeddings are randomly shuffled.
Figure 4: Scatter plot of the best analogy accuracies with CBOW (x-axis) and DSAW (y-axis) foreach dataset category. Each path indicates the performance change caused by the different analogymethod ADD, Ignore-A, Only-B. The path tends to move toward top-right, indicating that bothembeddings are utilizing the differential information in the vectors a and a* for analogy, not justthe neighborhood structure of b and a*.
Figure 5: The shortcoming of addingcontinuous vectors in a cosine vectorspace.
Figure 6: PCA plots of words/phrases in continous/discrete embeddings (best on computer screen).
Figure 7: Plotting the compositional phrases with CBOW (left) and DSAW (right): Long thin solidcylindrical pasta = spaghetti, from https://en.wikipedia.org/wiki/Spaghetti.
Figure 8: Plotting the compositional phrases with CBOW (left) and DSAW (right): Adult malecattle = ox, from https://en.wikipedia.org/wiki/Ox.
Figure 10: Plotting the compositional phrases with CBOW (left) and DSAW (right): Italian luxurysports car manufacturer = Ferrari, from https://en.wikipedia.org/wiki/Ferrari.
Figure 9: Plotting the compositional phrases with CBOW (left) and DSAW (right): Quadrupedalruminant mammal = sheep, from https://en.wikipedia.org/wiki/Sheep.
Figure 11:	The list of words used for the paraphrasing experiments.
Figure 12:	The density of add/delete effects (left, right) in the best DSAW model trained withE = 200, where x-axis is the word index sorted according to the frequency (frequent words areassigned the smaller indices), cut off at 32000-th word. We observe that rare words tend to havemore effects.
Figure 13: (left) Cumulative plot of the number of solutions found at the total time t, and (right)the same statistics based on the actual search time, i.e., the runtime excluding the time for the inputparsing and initialization.
Figure 14: The same plot as Table 13, but the length is restricted to be larger than 2.
Figure 15: List of function words and number-related words.
