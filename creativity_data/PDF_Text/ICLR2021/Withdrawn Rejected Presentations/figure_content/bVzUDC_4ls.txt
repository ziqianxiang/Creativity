Figure 1: Empirical characterization of numerical error of different implementationsFigure 2: Illustration of our method. Since the verifier does not model the floating point arith-metic details of the implementation, their decision boundaries for the classification problem diverge,which allows us to find adversarial inputs by crossing the boundary via numerical error fluctuations.
Figure 2: Illustration of our method. Since the verifier does not model the floating point arith-metic details of the implementation, their decision boundaries for the classification problem diverge,which allows us to find adversarial inputs by crossing the boundary via numerical error fluctuations.
Figure 3: The quasi-safe images with respect to which all implementations are successfully attacked,and corresponding adversarial images24-core processor. We train the small architecture from Xiao et al. (2019) with the PGD adversaryand the RS Loss on MNIST and CIFAR10 datasets. The trained networks achieve 94.63% and44.73% provable robustness with perturbations of 'âˆž norm bounded by 0.1 and 2/255 on the twodatasets respectively, similar to the results reported in Xiao et al. (2019). Our code will be madepublicly available after the review process.
