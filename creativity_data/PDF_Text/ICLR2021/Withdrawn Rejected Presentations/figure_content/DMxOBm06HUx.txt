Figure 1: An overview of AMBERT, showing the process of creating multi-grained representations.
Figure 2: Distances between representations of fine-grained and coarse-grained encoders (represen-tations of [CLS]) in AMBERT-Combo and AMBERT. CD and ED stand for cosine dissimilarity(one minus cosine similarity) and normalized Euclidean distance respectively.
Figure 3: Attention weights of first layers of Our BERT (word/phrase), AMBERT-Hybrid and AM-BERT, for English and Chinese sentences.
Figure 4: Attention maps of first layers of fine-grained BERT models for English and Chinesesentences. The Chinese sentences are “商店里的兵乓球拍卖完了(Table tennis bats are sold out inthe shop)”, “北上京城施展平生报复(Go north to Beijing to fulfill the dream)”, “南京市长江大桥位于南京 (The Nanjing Yantze River bridge is located in Nanjing)”. Different colors representattention weights in different heads and darkness represents weight.
Figure 5: Attention maps of first layers of coarse-grained BERT models for English and Chinesesentences. Note that tokenizations may have errors.
