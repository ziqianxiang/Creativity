Figure 1: An illustration of our proposed feature-based performance estimation strategy. Top: TheCNN architecture can be considered as composition of a feature extractor and a linear classifier.
Figure 2: Performance estimation for different architectures including VGG-16, ResNet-18, ResNet-50, and MobileNetV2 on CIFAR-10/CIFAR-100 dataset. Original denotes the original test accuracyat different epochs. Ours denotes the performance estimated by our method using the checkpointsup to the given epochs. Our method can reach an accuracy closer to the final accuracy at the earlystage of training, which indicates a much more accurate performance estimate than the baseline.
Figure 3: Performance estimation for different training budgets from 25% (50 epochs) to 100% (200epochs). The linear learning rate schedule is adjusted accordingly.
Figure 4: Classification results of test images during training ResNet-18 on CIFAR-100. We sample200 test images to observe if their deep features can be correctly classified by (a) the optimal linearclassifiers, and (b) the classifier ensembles at each epoch. Light gray pixels denotes that image iscorrectly classified, and black pixels denotes a wrong classification. The horizontal axis indicatesthe image indices, sorted by their accuracy for better visualization. The vertical axis indicates thetraining epochs up to 200 at the bottom.
Figure 5: Relative ranking measured by Kendall’s τ coeffi-cient and the true performance. Original denotes the originalaccuracy metrics at different epochs. Ours denotes the pre-dicted performance by our method using the checkpoints upto the given epochs. For fair comparison, we slightly shiftOurs rightwards because of the computational overhead.
Figure 6: Regret over time of random search, HyperBand, and BOHB with and without our per-formance estimation for searching in NAS-Bench-201. We use log scale on both axes for bettervisualization.
