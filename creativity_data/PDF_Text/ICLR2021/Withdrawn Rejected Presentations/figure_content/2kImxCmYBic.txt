Figure 1: ‘DPmm’ and ‘DPrt’ family trees	root Category						,DP10,			,DPo6'			,DPo3'			,1010,			I赞 If		ParentS					[ ,DPo6,]		[]		[]				SiblingS		「		□			[]			U					auntsuncles			[]			[]			[]			[,1010,]					COUSinS			[ 'NArw']			[ 'NArw']			[ 'NArw']			[ 'NArw' ]			I赞		Children		[]			[ ,DPo3,]		□		□			niecesnephews			[]		「			[]			U					COWorkerS			[]			[]			[,1010,]			U					friends			[]			[]		[]			U		PrOCeSScliet						DPOd			Ord3			DPOd			1010	returned columns					column_ord3_DPod_1010 (1010 returns a set) column_NArw		(support entries for DP10)		(support entries for DP10)		(support entries for DP10)Figure 2: ‘DP10’ family treesnoise source as implemented is Gaussian, the application is capped from extreme outliers at half ofrange (e.g. +/- 0.5) and based on whether an input entry is above or below the midpoint, positiveor negative noise respectively is scaled to ensure maintained original range in returned data basedon values of input entry. Parameters are also accepted to indicate what ratio of input will receiveinjection. Similar options are available for Laplace distribution noise profiles.
Figure 2: ‘DP10’ family treesnoise source as implemented is Gaussian, the application is capped from extreme outliers at half ofrange (e.g. +/- 0.5) and based on whether an input entry is above or below the midpoint, positiveor negative noise respectively is scaled to ensure maintained original range in returned data basedon values of input entry. Parameters are also accepted to indicate what ratio of input will receiveinjection. Similar options are available for Laplace distribution noise profiles.
Figure 3: ‘dxdt’ family trees7	Integer SetsInteger feature sets of unknown origin may present a particular challenge for automated encodings,as these may be associated with a diverse set of interpretations, and may originate from continu-ous variables, counters, discrete relational variables (e.g. small/medium/large), or possibly even anordinal categoric encoding (Stevens et al., 2020). The Automunge philosophy for these kind of am-biguities is to simply redundantly encode in a manner suitable for each [Table 7, Fig 4], defering toa training operation for determining relevancy.
Figure 4: ‘ntgr’ family trees8	ExperimentsSome experiments were run to evaluate impact of various normalizations and transformations. TheHiggs data set (Baldi et al., 2014) was selected based on scale and predominantly numeric fea-ture types. The Higgs application is tabular data for binary classification sourced from high energyphysics domain to evaluate subatomic particle interactions to distinguish between background noiseand traces of Higgs boson interactions. The origin paper noted some details of architectures (5layers with 300 nodes in each) that served as basis for the experiments, and we used a 0.6M sizevalidation set out of the 8.8M samples. A departure was made from the origin paper in use of aphased learning rate with the fastai (Howard & Gugger, 2020) “fit_one_cycle” learner for tabulardata, partly for convenience along with time and resource constraints. Since the interest was primar-ily to evaluate relative performance between different preprocessing methods, no significant attemptwas made to train models to maximum performance or to venture beyond double descent, instead theprimary tuning aspect was selecting an epoch depth beyond which additional did not have improvedvalidation metrics. The experiments were repeated with different size subsets of the training data,including full data set, 5% samples, and 0.25% samples to represent scenarios with underservedtraining data. Learning rates deferred to the fastai default finder, and the evaluation metric was se-lected as area under the receiver operating characteristic curve (ROC AUC) to be consistent with theorigin demonstration.
