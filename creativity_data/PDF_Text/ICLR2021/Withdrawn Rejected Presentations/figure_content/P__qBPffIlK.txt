Figure 1: Visualization of privatization mechanisms. A logistic regression model is fitted to theoptimal decision boundary (blue). Minimizing the log likelihood (a) leads to a model that transformspoints to the opposite category. Maximizing the entropy of the adversary (b) leads to transformationsthat make the adversary uncertain (closer to the decision boundary). Our two-step approach (c)achieves a stronger privacy by first maximizing the entropy as in (b), and then transforming the imageto a random, new value for the sensitive attribute s (purple arrows). Small grey arrows show thenegative gradient of the loss function with regards to the input variable, and orange circles illustratesthe distortion budget.
Figure 2: The trade-off curve between privacy loss and utility score for the baseline (likelihood), theimproved baseline (entropy) and our method on the synthetic dataset. Upper left corner is better.
Figure 3: Results on the synthetic dataset. The censored representations by (a) baseline with filtertrained to minimize log-likelihood of adversary, (b) the baseline filter trained to maximize entropy,and (c) filter trained to maximize entropy and then generating a new synthetic attribute.
Figure 4: Privacy vs. utility trade-off curve where the sensitive attribute is smiling (top left), gender(top right), lipstick (bottom left), age (bottom right). Our approach with negative entropy lossconsisitently outperforms all other approaches on the attributes explored, and our method withlog-likelihood loss outperforms the baseline with log-likelihood loss on all explored attributes.
Figure 5: Qualitative results for the sensitive attribute smile. In the first four columns: = 0.001,and in the last four columns: = 0.01. From top to bottom row: input image (x), censored image(x0), censored image with synthetic non-smile (x00 , s0 = 0), censored image with synthetic smile(x00, s0 = 1). The model is able to generate a synthetic smiling attribute while maintaining much ofthe structure in the image. These images were generated from a model trained using 128x128 pixels.
Figure 6: Overview of the training setup (left) and the network architecture used in the filter and thegenerator (right).
Figure 7: Qualitative results for the sensitive attribute gender. In the first four columns: = 0.001,and in the last four columns: = 0.01. From top to bottom row: input image (x), censored image(x0), censored image with synthetic female gender (x00 , s0 = 0), censored image with synthetic malegender (x00, s0 = 1). The model is able to generate a synthetic gender while maintaining much of thestructure in the image. These images were generated from a model trained using 128x128 pixels.
