Figure 1: Our solution pipeline consists of four stages as captioned in the legend and described inSection 3. We first train an image steganography encoder and decoder and then use the encoder toembed artificial fingerprints into the GAN training data. We then train a GAN model in the defaultway. Finally, we decode the fingerprints from the deepfakes; we justify that fingerprints can betransferred from real data to the deepfakes.
Figure 2: Samples for Table 1 on CelebA. See more samples on LSUN in Figure 7 in Appendix.
Figure 3: Zoom-in needed. Red plots show the fingerprint detection in bitwise accuracy w.r.t. theamount of perturbations over ProGAN trained on CelebA. In the left four plots (robustness againstimage perturbations), blue dots represent detection accuracy on the fingerprinted real training im-ages, which serve as the upper bound references for the red dots. See Figure 8 in Appendix foradditional results over ProGAN trained on LSUN. In the right two plots (robustness against modelperturbations), blue dots represent FID of generated images from the perturbed models.
Figure 4: Perturbed image samples from the fingerprinted ProGAN and the corresponding finger-print detection accuracy. The detection still performs robustly (bitwise accuracy â‰¥ 0.75) even whenthe image quality heavily deteriorates w.r.t. each perturbation.
Figure 6: Steganography decoder architecture.
Figure 8: Red plots show the fingerprint detection in bitwise accuracy w.r.t. the amount of per-turbations over ProGAN trained on LSUN. Blue dots represent detection on the fingerprinted realtraining images, which serve as the upper bound references for the red dots. This is supplementaryto Figure 3 in the main text.
Figure 7: samples for Table 1 on LSUN, supplementary to Figure 2 in the main text.
