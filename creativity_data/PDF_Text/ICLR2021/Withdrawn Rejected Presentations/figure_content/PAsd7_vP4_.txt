Figure 1: Learning curves of PPO/DPPO using PFPN (blue) compared to Gaussian policies (red)and DISCRETE policies (green). Solid lines report the average and shaded regions are the minimumand maximum cumulative rewards achieved with different random seeds during training.
Figure 2: Comparison of SAC using PFPN (blue) and Gaussian policies (red). DISCRETE is notshown as the categorical distribution cannot be applied to SAC.
Figure 3: Evolution of how particles are distributed along four action dimensions during trainingof the DeepMimicWalk task with DPPO. The depicted four dimensions represent the target righthip joint position expressed in an axis-angle representation. Each action dimension is normalizedbetween -1 and 1. Particles are initially distributed uniformly along a dimension (dark colors) andtheir locations adaptively change as the policy network is trained (light colors). The training stepsare measured by the number of samples exploited during training.
Figure 4: Comparison of motion generated by PFPN and DISCRETE in DeepMimicWalk taskduring one step cycle. Both PFPN and DISCRETE are trained with DPPO using the best resolutionparameters from Table 1 (35 particles and 200 bins, respectively). PCA embedding of the trajectoriesof the agentâ€™s two feet are shown at the right and their time sequence expansions at the bottom ofeach method. Lines with "x" are the ground truth trajectories extracted from the motion capturedata that the agent learns to imitate.
Figure 5:	(a) One-step bandit task with asymmetric reward landscape. The reward landscapeis defined as the gray line having two peaks asymmetrically at -0.25 and 0.75. The probabilitydensities of stochastic action samples drawn from PFPN (blue) and Gaussian policy (red) are countedafter training with a fixed number of iterations. (b) Illustration of 2D multi-goal navigation. Left:one-step trajectories generated by PFPN and Gaussian policy via stochastic sampling after training.
Figure 6:	Training curves on continuous control tasks from the Roboschool and DeepMimic envi-ronments using on-policy policy gradient algorithms and IMPALA with v-trace correction.
Figure 7:	Motion trajectories of end effectors (four feet) of the ant agent in AntBulletEnv-v0 taskwith PFPN-PPO, Gaussian-based PPO and PPO with fixed, uniform discretization (DISCRETE). Weapply PCA to visualize the 2D trajectories; the trajectories are measured by the relative positions ofeffectors with respect to the root link of the agent.
Figure 8: Evolution of particles distributed on action dimensions for the four feet joints of the agentin AntBulletEnv-v0 task during training using PPO.
Figure 9:	Torque Patterns generated by Gaussian-based PPO and PFPN-PPO on AntBulletEnv-v0.
Figure 10:	Learning curves on continuous control tasks using DDPG in Roboschool and OPenAIGym environments.
Figure 11:	Comparison of PFPN to Gaussian baselines and fully state-dependent mixture of Gaus-sians (GMM). GMM in SAC uses the reparameterization trick described in Section 3.4 for state-action value based optimization.
Figure 12:	Cumulative rewards obtained during training as a function of the actual wall clock time.
Figure 13:	Sensitivity of PFPN to the number of particles and resampling strategies and hyper-parameters on DeepMimicWalk task using DPPO. (a) Comparison of PFPN using 35 particles peraction dimension, which is the default parameters used in above benchmarks, to that using 5 (red),10 (green), 50 (yellow) and 100 (azure) particles. (b)-(d) show the performance of PFPN with 35particles on each action dimension but different resampling strategies or hyperparameters, where theblue line is the default hyperparameters used in our benchmark tests.
Figure 14: Learning curves of PPO/DPPO using PFPN with the default resampling process (blue)compared to that without resampling (green).
