Figure 1: ResNet Architecture is shown in the top. Process of obtaining a sub-FC-DNN by ignoring skip(retaining block) or retaining skip (ignoring block) is shown in the bottom.
Figure 2: Shows weight sharing and rotational symmetry of internal variables and the output after pooling in aCNN. Left most cartoon uses a GAP layer, and the other two cartoons use max-pooling. Circles are nodes andthe 1/0 in the nodes indicate the gating. Pre-activations/node output are shown in brown/purple.
Figure 3: Shows a deep gated network (DGN). The soft-ReLU enables gradient flow into the feature network.
Figure 4: Cfi , Civ ,i âˆˆ [4] are the convolutional layers, Which are folloWed by global-average-pooling (GAP)layer then by a dense layer (Df/Dv), and a softmax layer to produce the final logits.
Figure 5: Hidden layer outputs for a fixed random input to the value network of DGN with permuted gating.
