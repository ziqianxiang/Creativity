Figure 1: Predictions from SSL VAE methods on half-moon binary classification task, with accuracy in lowercorner. Each dot indicates a 2-dim. feature vector, colored by predicted binary label. Top: 6 labeled examples(diamond markers), 994 unlabeled. Bottom: 100 labeled, 900 unlabeled. First 4 columns use C = 2 encodingdimensions, last 2 use C = 14. M2 (Kingma et al., 2014) classification accuracy deterioriates when increasingmodel capacity from 2 to 14, especially with only 6 labels (drop from 98.1% to 80.6% accuracy). In contrast,our CPC VAE approach is reliable at any model capacity, as it better aligns generative and discriminative goals.
Figure 2: Semi-supervised learning of 2-dim. encodings of MNIST digits, with accuracy in lower corner. Allmethods use 100 labeled examples and 49,900 unlabeled examples. Each observed image x is mapped to itsmost likely 2-dim. encoding z and colored by true label y. Labeled examples are emphasized. Where applicable,we also show class decision boundaries. Baselines (from left): 2-stage unsupervised VAE-then-MLP (Sec. 2.2)and a “supervised” VAE maximizing joint likelihood log p(x, y) (a special case of our PC method with λ = 1,Sec. 3.1). Our methods: Prediction constrained VAE (PC-VAE with λ = 25, Sec. 3.1) and consistent predictionconstrained VAE (CPC-VAE, Sec. 3.2). Competitors: M2 from Kingma et al. (2014), which intentionallydecouples label y from “style” z, has limited accuracy due to imbalance of discriminative and generative goals.
Figure 3: Formalization of our consistency-constrained VAE as a decision network (Cowell et al., 2006). Circularnodes are random variables, including the latent VAE code Z that generates observed features x. Shaded nodesare observed, including the class labels y for some data (left). Square decision nodes indicate predictions ywof class labels that depend on the amortized inference network qφ (z | x). Diamonds indicate losses (negativeutilities) that influence the variational prediction of labels and latent variables. Generative likelihood: Likestandard VAEs, our generalizations choose generative model parameters θ and variational posteriors qφ tomaximize the ELBO L (orange). Prediction accuracy: Unlike previous semi-supervised VAEs, we do notdirectly model the probabilistic dependence of labels y on z and/or x. We instead treat label prediction asa decision problem, with application-motivated loss 'S (red), that constrains the approximate VAE posteriorqφ (z|x) (and therefore the encodings and generative model). Prediction consistency: For unlabeled data (right),we cannot directly evaluate the quality of predictions. However, we do know that if two observations X and Xare generated from the same latent code z, they should have identical labels; otherwise, the model cannot havehigh accuracy. The loss 'c (blue) enforces the consistency of such predictions. Aggregate consistency: By thelaw of large numbers, we also know that aggregate label frequencies for unlabeled data should be close to thefrequencies π observed in labeled data. The loss 'A (green) enforces this constraint, and penalizes degeneratepredictors that satisfy 'c by predicting the same label yw for most or all of the unlabeled data.
Figure 4: Sampled reconstructions used to compute the consistency loss during training. Top: Original image.
Figure 5: Visualizations of generative model performance on SVHN for a prediction and consistency constrainedVAE incorporating latent affine transformations, trained on the SVHN dataset with all labels. (5a) Samples fromthe learned generative model conditioned on class (more are shown in supplement B). Samples are chosen viarejection sampling in the latent space with a threshold of 95% confidence in the target class. (5b) Reconstructionsof images in the held-out test set. Each triplet shows the original image (left), the reconstruction (middle), andan aligned reconstruction (right) obtained by fixing the learned affine transform variables to the global mean.
Figure 6: CelebA dataset results. Left: Test accuracy on the 4-class CelebA SSL task (1000 labeled trainingimages, 159,770 unlabeled; error bars show std. dev. of 10 labeled set samples). CPC-VAE improves on thelabeled-set only WRN, and especially beats the unsupervised VAE which poorly separates the classes in thelatent space. Right: Class-conditional samples of the 4 possible classes from our semi-supervised CPC VAE.
Figure 7: Prior distribution for latent parameters ∑(i) = tanh(z(i)) used to represent affine transformations.
Figure 8: Class-conditional samples of the 10 possible digit classes in the MNIST dataset. Each column showsmultiple samples from one specific digit class. From left to right, each panel shows samples from a standardunsupervised VAE, our CPC-VAE, and model M2 (Kingma et al., 2014). All models use a 2-dimensional latentcode, and are trained on the MNIST dataset with 100 labeled examples (10 per class).
Figure 9: Class-conditional samples of the 10 possible digit classes in the SVHN dataset, extending figure 5a.
Figure 10: Sensitivity of test accuracy to the constraint (lagrange multiplier) hyperparameters λ and γfigure 10. All runs used our best consistency-constrained model for MNIST using dense networks.
