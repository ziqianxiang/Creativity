Figure 1: DACT-BERT adds an additional classification layer after each Transformer block, alongwith a sigmoidal confidence function. DACT-BERT combines the Transformer hidden state and theoutputs and confidences of all earlier layers into an accumulated answer an . Later, during inference,the model is halted once an â‰ˆ aN .
Figure 2: Attention entropy distribution per layer in the backbone for DACT-BERT (blue) and Dee-BERT (red) for three different GLUE tasks. Each point represent the entropy for one attention headin each layer and the line shows the mean entropy for all the attentions in a given layer.
Figure 3: Output prediction attribution to each input token. Two samples from the MRPC task areshown with their attributions obtained both for DeeBERT and DACT-BERT. The color intensityshows the degree on how much each token contributes to the final output. Green is used for tokensthat have positive attribution to the output, while red means tokens with negative attribution to theoutput.
Figure 4: Halting (top) and output (bottom) attribution to each input token. For one sample fromthe MRPC task. The tokens attributions towards the halting value are computed for every haltingneuron, after each transformer block. For both, the color intensity shows the degree on how mucheach token contributes to the final output. Green is used for tokens that have positive attribution tothe output, while red means on tokens with negative attribution to the halt or output.
Figure 5: The number of times DACT-BERT (blue) and DeeBERT (red) quit computation at aspecific layer in the MRPC task.
