Figure 1: Normalized dot product of averagednoisy gradients over 100 iterations. Stationar-ity depends on the learning rate: η = 1 corre-sponds to stationarity (purple), while η =0.1corresponds to non stationarity (orange). De-tails in Section 2.
Figure 2: The architecture of SplitSGD. The initial learning rate is η and the length of the first singlethread is t1 . If the diagnostic does not detect stationarity, the length and learning rate of the nextthread remain unchanged. If stationarity is observed, we decrease the learning rate by a factor γ andproportionally increase the length.
Figure 3: Histogram of the gradient coherence Qi (for the second pair of windows, normalized) ofthe Splitting Diagnostic for linear and logistic regression. The two left panels show the behavior inTheorem 3.1, the two right panels the one in Theorem 3.2. In orange we see non stationarity, while inpurple a distribution that will return stationarity for an appropriate choice of w and q .
Figure 4: (left) comparison between Splitting and pflug Diagnostics on linear and logistic regression.
Figure 5: Performance of SGD, Adam, FDR and SplitSGD in training different neural networks.
Figure 6: Sensitivity analysis for SplitSGD with respect to the parameters w and q, appearing as thelabels of the x-axis in the form (w, q). In the convex setting (left) we consider the log loss achievedafter 100 epochs, while for deep neural networks (right) we report the maximum of the test accuracy.
