Figure 2: Input and output of backdoored CelebA autoencoder.
Figure 3: An overview of the training of backdoored GANs.
Figure 4: Evaluation of our backdoor attack on GANs for all three datasets. Figure 4a comparesthe performance of the backdoored GANs with clean ones when generating the original distribution.
Figure 5: Visualization of a clean GAN (Figure 5a) and a backdoored GANs on clean input (Fig-ure 5b) and backdoored input (Figure 5c), using the CelebA dataset.
Figure 6: Visualization of the output of the backdoored MNIST GAN. Figure 6a shows the cleanoutput, Figure 6b shows the backdoored output when a distribution is set as the target, and Figure 6cFigure 7:	Visualization of the output of the backdoored CIFAR-10 GAN. Figure 7a shows the cleanoutput, Figure 7b shows the backdoored output when a distribution is set as the target, and Figure 7cshows the backdoored output when a single image is used as the target.
Figure 7:	Visualization of the output of the backdoored CIFAR-10 GAN. Figure 7a shows the cleanoutput, Figure 7b shows the backdoored output when a distribution is set as the target, and Figure 7cshows the backdoored output when a single image is used as the target.
Figure 8:	Visualization of the output of the backdoored CelebA GAN. Figure 8a shows the cleanoutput, Figure 8b shows the backdoored output when a distribution is set as the target, and Figure 8cshows the backdoored output when a single image is used as the target.
Figure 9: Visualization of the backdoored CIFAR with MNIST set as target. Figure 9a shows theclean output and Figure 9b shows the target output, i.e., the output when the input is backdoored.
