Figure 1: Expected posterior utility w.r.t. optimization iterations for three casesnon-linear case with estimatedoptimization iterationsoptimization curves for user 10	20	40	60	80 IQOoptimization iterationsoptimization curves for user 2= 0.26⅛∙≡ 0.24ω0.0.22P(υ¥ 0.20d0.186	20	40	60	80	100OPtlmIZatIon CUrVeS for USer 3---- with exact value
Figure 2: Optimization curves w.r.t. different numbers of samples per estimationIn this section, we present the experiment results to demonstrate the effectiveness of the two pro-posed technical solutions absorbed into our framework, i.e., the effectiveness of the ADMM-basedsolution for maximizing expected posterior utility and the importance sampling technique for pos-terior utility estimation.
Figure 3: Influence of user preference and item varianceactivation function over the inner product of item representation and user representation (i.e., wu).
Figure 4: Learned policy w.r.t. different levels of user preference0.0050.0100.0150.0200.025i,ι i,2	i,3“4	15	-6	"7 f,B 19	-10	"11	”12	-13	『14	-15	-16Figure 5: Learned policy w.r.t. different levels of user uncertainty0.018不0.016 O0.014 I&0.012 onP0.010 °aσ
Figure 5: Learned policy w.r.t. different levels of user uncertainty0.018不0.016 O0.014 I&0.012 onP0.010 °aσ0.008]We also provide analysis about how different levels of user preference would affect the final learnedpolicy over items. For a randomly sampled user, We vary W0 from -1.0 to 1.5 and keep the valuesof other dimensions unchanged. We also randomly pick 16 items from candidate item set, sort theseitems according to the value of dimension 0 of the item mean vector and denote the sorted itemsas iι, i2,∙∙∙,ii6. For each case (with specific value for W0), we record the final learned policy(recommendation probabilities) over the picked items and present the heat map in Figure 4 Wheredarker color represents larger recommendation probability. As shown in Figure 4, with the increase
