Figure 1: Schematic of the BCPNN architecture with three input HCs and one hidden HC. A. Theprobabilistic graphical model illustrating the generative process with each random variable (circle)representing a HC. The input HCs are observable (shaded circle) and the hidden HC is latent (opencircle). The naive Bayes assumption renders the likelihood of generating inputs factorial. B. Theequivalent neural network where the input HCs are binary (gray circle), hidden HC is multinomial(gray box), and hidden MCs within the HC are the discrete values of the hidden variable (gray circleinside the box).
Figure 2: A: Bias regulation mechanism. For generating the figure, khalf =-5 and pM axE nt =0.01was used. B: The schematic of the network used for unsupervised learning. In this network, theinput layer contains nine binary HCs (grey circles on the left), and the hidden layer contains threeHCs (grey boxes), each of which contains four MCs (grey circles inside the boxes).The existenceof a connection between an input HC and hidden HC is shown as a blue strip, i.e., Mij =1. Theinput-hidden weights are shown as yellow dots and are present only when a connection alreadyexists.
Figure 3: A. Histogram of weights from the input layer to hidden layer. The horizontal axis hasthe minimum to maximum value of the weights as the range, and the vertical axis is in log scale. B.
Figure 4: Receptive fields of different unsupervised learning methods. For each model, the positiveand negative values are normalized, such that blue, white, and red represent the lowest, zero, andhighest value of weights. A. BCPNN: Each row corresponds to a randomly chosen HC and theconstituent MCs of BCPNN. First column shows the receptive field of HC (black means Mij =1).
