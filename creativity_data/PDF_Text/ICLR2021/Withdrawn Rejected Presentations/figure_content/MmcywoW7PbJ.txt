Figure 1: Framework of GPIM. We jointly train the abstract-level policy πμ and the discriminatorqφ to understand skills which specify task objectives (e.g., trajectories, the final goal state), anduse such understanding to reward the goal-conditioned policy for completing such (rendered) tasks.
Figure 2: Graphical model.
Figure 3: Disentanglement of policy πθ.
Figure 5: Goals and learned behaviors by GPIM: Dots in 2D navigation (left subfigure, x-y goal) andatari games denote different final goal states, and Curves with same Color represent CorrespondingtrajeCtories; Goals in 2D navigation (right subfigure, Color-shape goal) and objeCt manipulation aredesCribed using the text at the top of the diagram, where the purple lines imply the behaviors; InmujoCo tasks, the first (swimmer, half Cheetah and fetCh) and third (swimmer and half Cheetah) rowsrepresent the expert trajeCtories, and eaCh row below represents the Corresponding behavior.
Figure 6: Expert behaviors and learned behaviors.
Figure 7: Performance (normalized distance to goals vs. training steps) of our GPIM and threebaselines, where GPIM generally achieves smaller distances to goals in comparison to baselines.
Figure 8: Abstract-level policy ∏μ gradually ex-plores the environment, generating more difficultgoals. qφ encourages ∏θ to gradually mimic ∏*.
Figure 9: (Left) The maze environment and reward functions. The heatmaps depict (the learningprocess of) the reward function conditioned on the specific task reaching the left-bottom star (RIGand DISCERN) or ”imitating” the trajectory induced by abstract-level policy (GPIM). Specifically,the learning process of DISCERN’s reward function refers to the learning process of embeddingstate. Note that the reward functions of baselines are conditioned on the goals, while GPIM’s rewardfunction is conditioned on the skill ω . So, the induced trajectory by GPIM conditioned on the sameskill refines over training steps (at the abstract-level), as shown in the bottom. (Right) Learningcurves for GPIM and the enhanced baselines (DIAYN+RIG and DIAYN+DISCERN). Comparedwith our model, baselines ignoring the dynamic of the maze environment exhibit poor performance.
Figure 10: 2D navigation with (a) DIAYN-Imitator, (b) DIAYN-Goal, and (c) our proposed GPIM.
Figure 11: Distribution of sampled goals for the goal-Conditioned poliCy, where the initial state is[5,2] (the red star) and the actor step is 20 for each rollout. (a) The goals (images) are obtained bydecoding 30 sampled latent goals zg in VAE framework; (b) The goals (colored dots) are sampledfrom the agent’s behaviors by random exploration; (c) The goals (colored dots) are rendered fromthe states induced by our abstract-level policy. (d): Evaluation on reaching user-specified goals,where GPIM significantly outperforms baselines.
Figure 12: Ablation study and disentanglement.
Figure 13: Ablation study on mujoco tasks.
Figure 14: GridWorld tasks. Y-axis is the percent-age of different rooms that the robot arrives in.
Figure 15: GridWorld tasks. The Y-axis is the percentage of different rooms that agent arrives in.
Figure 16: Expert behaviors and learned behaviors. The four expert trajectories are described para-metrically as: (x, y, z) = (log10(t + 1) + t/50, sin(t)/5 + t/5, t/5), (x, y, z) = (t/5, cos(t)/5 -1/5 + t/5, sin(t)/5), (x, y, z) = (cos(t)/5 + t/50 - 1/1, sin(t)/5 + t/5, t/5), and (x, y, z) =(sin(t) - sin(2t)/2, -t/5, cos(t)/2 - cos(2t)/2).
Figure 17: Discovered goal-conditioned behaviors. (f-h): The left subfigure shows the expert be-haviors (goals); The middle subfigure shows the learned behaviors by GPIM; The right subfigure isthe stacked view of goals and GPIM behaviors.
