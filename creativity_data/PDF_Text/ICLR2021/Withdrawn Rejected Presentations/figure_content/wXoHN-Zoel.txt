Figure 1: Total recovery from training bias by enforcing riskparity. In this simple example, the training bias P - P * isalways orthogonal to the risk parity constraint (blue line)because P and P * are probability distributions. Thus if thetraining bias does not affect the risk profiles (i.e. P satisfiesAssumption 3.3), then enforcing risk parity allows us to to-tally overcome the training bias. Unfortunately, to show anexample in which the risk decomposes into recoverable andnon-recoverable parts, We need (at least) two more dimen-sions.
Figure 2: Decision heatmaps for (left) baseline on train data from P ; (center left) fair classifier ontrain data from P; (center right) baseline on test data from P *; (right) fair classifier on test data fromP*. Decision boundary of the fair classifier has larger slope better accounting for the group a = 1underrepresented in the train data. Consequently its performance is better on the unbiased test data.
Figure 4: Example in which enforcing algorithmic fairness harms OOD generalization. The triangleis the set of risk profiles, and the dotted left side of the triangle intersects the fair constraint (i.e.
