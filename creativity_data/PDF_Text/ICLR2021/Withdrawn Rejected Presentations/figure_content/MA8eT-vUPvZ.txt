Figure 1: A schematic of the ARM problem setting and approach, described in detail in Section 3. Left: Duringtraining, we assume access to labeled data along with group information z, which allows us to construct trainingdistributions that exhibit group distribution shift. For example, a training distribution may place uniform mass ononly a single user’s examples. We use these training distributions to learn a model that is adaptable to distributionshift via a form of meta-learning. We detail the specific adaptation procedures (orange box) that we considerin Section 3 and Figure 2. Right: We perform unsupervised adaptation to different test distributions, withoutrequiring zero shot generalization to shift as in prior methods. If the test shifts we observe are similar to thosesimulated by the training distributions, e.g., we deploy the model to new end users at test time, then we expectthat we can effectively adapt to these test distributions for better performance.
Figure 2: Schematics of the two broad classes of approaches we consider. Left: In the contextual approach,x1, . . . , xK are summarized into a context c, and we propose two methods for this summarization, either througha separate context network or using batch normalization activations in the model itself. c can then be used by themodel to infer additional information about the input distribution. Right: In the gradient based approach, anunlabeled loss function L is used for gradient updates to the model parameters, in order to produce parametersthat are specialized to the test inputs and can produce more accurate predictions.
Figure 3: Visualizing VAE sam-ples conditioned on differentvalues of y (x axis) and c (yaxis). The VAE learns to usec to represent rotations.
Figure 4: In the streaming setting, ARM methods reach strong performance on rotated MNIST (left) and TinyImageNet-C (right), after fewer than 10 and 25 data points, respectively, despite meta-training with batch sizesof 50 for both domains. This highlights the ability of the trained models to adapt with small test batches.
Figure 5: Visualizing one batch of 50 imagesfrom a FEMNIST test user. The ARM-CMLmodel, using the entire batch, is able to suc-cessfully adapt to output the correct label “a”on the ambiguous example, shown enlarged,whereas other models incorrectly output “2”.
Figure 6: During inference for ARM-CML, the contextnetwork produces a vector ck for each input image xkin the batch, and the average of these vectors is used asthe context c is input to the prediction network. Thiscontext may adapt the model by providing helpful infor-mation about the underlying test distribution, and thisadaptation can aid prediction for difficult or ambiguousexamples. During training, we compute the loss of thepost adaptation predictions and backpropagate throughthe inference procedure to update the model.
