Figure 1: Layer structure of the ReduNet: fromone iteration of gradient ascent for rate reduction.
Figure 2: Original samples and learned representations for 2D and 3D Mixture of Gaussians. We visualizedata points X (before mapping) and features Z (after mapping) by scatter plot. In each scatter plot, each colorrepresents one class of samples. We also show the plots for the progression of values of the objective functions.
Figure 3: Heatmaps of cosine similarity between data Xshift/learned features Zshifb MCR2 loss, and distancebetween shift samples and subspaces. For (a), (b), (e), (f), we pick one sample from each class and augmentthe sample with its every possible shifted ones, then calculate the cosine similarity between these augmentedsamples. For (d), (h), we first augment each samples in the dataset with its every possible shifted ones, thenwe evaluate the cosine similarity (in absolute value) between pairs across classes: for each pair, one sample isfrom training and one sample is from test which belong to different classes.
Figure 4: Cosine similarity (absolute value) for 2D and 3D Mixture of Gaussians. Lighter colorimplies samples are more orthogonal.
Figure 5: Learning mixture of Gaussians in S1 and S2. (Top) For S1, we set σ1 = σ2 = 0.1;(Bottom) For S2, we set σ1 = σ2 = σ3 = 0.1.
Figure 6: Learning mixture of Gaussian distributions with more than 2 classes. For both cases, weuse step size η = 0.5 and precision = 0.1. For (a), we set iteration L = 2, 500; for (b), we setiteration L = 4, 000.
Figure 7: Visualization of signals in 1D. Blue dots represent the sampled signal used for trainingwith dimension n = 150. Red curves represent the underlying 1D function (noiseless). (Left) Onesample from class 1; (Right) One sample from class 2.
Figure 8: Cosine similarity (absolute value) of training/test data as well as training/test representa-tions for learning 1D functions.
Figure 9: Histogram of Cosine similarity between pairs sampled from different Classes for learning1D funCtion. The histogram of Cosine similarity between training data Xc as well as representationsZc Vs. testing (shifted) data Xc0 as well as (shifted) representations Zc0, where we let c denote theClass index and c 6= c0 .
Figure 10: Cosine similarity (absolute Value) of training/test data as well as traning/test representa-tions for learning rotational inVariant representations on MNIST.
Figure 11: Histogram of Cosine similarity between pairs sampled from different Classes for learningrotational inVariant representations on MNIST. The histogram of Cosine similarity between trainingdata Xc as well as representations Zc Vs. testing (shifted) data Xc0 as well as (shifted) representa-tions Zc0 , where we let c denote the Class index and c 6= c0 .
Figure 12: Examples of rotated images of MNIST digits for testing rotation invariance, each rotatedby18。.(Left) digit ‘0'. (Right) digit'1’.
Figure 13: Examples of translated images of MNIST digits (with stride=7) for testing cyclic trans-lation invariance of the ReduNet. (Left) digit ‘0’. (Right) digit ‘1’.
