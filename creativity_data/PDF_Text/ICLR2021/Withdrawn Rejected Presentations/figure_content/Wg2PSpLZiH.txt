Figure 1:	Examples of images with two different caption text pieces from the MS COCO captiondataset, where some captions are more fine-grained than the others that are more abstract.
Figure 2:	The overall framework of SemVLP. We use a shared Transformer network and a pluggablecross-modal attention module to support both the low-level and high-level semantic alignments inan iterative learning framework.
Figure 3: Results w.r.t different two-stream architectures for aligning high-level semantics on VQAand NLVR2 development set (best performance achieved at the semantic level of Ls = 6).
