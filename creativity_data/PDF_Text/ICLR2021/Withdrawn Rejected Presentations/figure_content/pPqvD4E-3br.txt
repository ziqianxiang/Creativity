Figure 1: The output probability (yc) distributionsof a ResNet-32 classifier trained on artificially im-balanced CIFAR-10 for positive (left) and negativesamples (right). For frequent classes, the predicteddistribution skews towards 1; for infrequent classes,it skews towards 0. Classifiers trained using PLM(bottom) reduces this bias, when compared to clas-sifers trained with binary cross-entropy (top).
Figure 2: The test accuracy for each class on CIFAR-10 with varying levels of imbalance. PLM leadsto improved generalization on minority classes, especially when there is a large amount of imbalance.
Figure 3: The class-wise F1 score on the MSCOCO dataset. The classes are ordered based on thenumber of samples in the training set. The bars represent the results achieved by using PLM, andthe points are the results with BCE. Not only does PLM improves results on the 15 most infrequentclasses, but also it improves scores on difficult classes, like backpack, bench, and cell phone.
Figure 4: The change of the ratio over time on im-balanced MultiMNIST. The ratio for tail classestend to increase while the ratio for head classestend to decrease.
Figure 5: Error rates on CIFAR-10 and CIFAR-100 (both with ρ = 100). This ablation measuresthe effect of λ. For these experiments, the initialratio is the dataset's ratio rc,1 = rc.
Figure 6: The class-wise precision and recall scores on the MSCOCO dataset. The classes are orderedbased on the number of samples in the training set. The bars represent the results achieved by usingthe PLM method, and the points are the results when using BCE.
