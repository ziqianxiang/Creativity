Figure 1: The overall frameworks of existing works on combining quantization and NAS and ourmethod. (a) denotes directly converting the best searched floating-point architecture to quantization.
Figure 2: Comparison of the parato models of NAS-then-Quantize and OQA. (a) OFA FP supernetis used for NAS and LSQ is used as the quantization method. @25 denotes finetuning for 25 epoch.
Figure 3: Comparison with the state-of-the-art quantization methods (LSQ , QKD, SAT) in variousnetwork(MobileNetV2/V3, EfficientNet-B0) on the ImageNet dataset.
Figure 4: The quantization accuracy drop of shallow-fat and deep-slim subnets which are sampledfrom FP 4/2 bit supernets on the ImageNet dataset.
Figure 5: Comparison of the Top-1 validation accuracy on ImageNet dataset between the FP pareto,3 bit pareto, and 2 bit pareto. The pareto is selected from the corresponding supernet and the accu-racy is also obtained from the supernet.
Figure 6: The probability distribution oftotal depth.
Figure 7: Architecture visualization of OFANet and our searched OQANets. ’MB E3 K3' indicates’mobile block with expansion ratio 3, kernel size 3x3'. From top to bottom, there are FP OFANet,4-bit OQANet and 2-bit OQANet. There are under similar computation cost, around 220M FPFLOPs.
