Figure 1: (a) t-SNE visualization of offline and online samples from the walker2d-mediumdataset from the D4RL benchmark (Fu et al., 2020), and online samples collected by a CQL agenttrained on the same dataset. (b) We visualize the log-likelihood estimates of offline and onlinesamples observed in walker2d-medium task, based on a VAE pre-trained on the offline dataset.
Figure 2: Illustrations of our framework. (a) For updating the agent during fine-tuning, we drawa balanced number of samples from both offline and online replay buffers initially, but focus onexploiting online samples at a later training stage. (b) We first train an ensemble of N independentoffline RL agents with the given offline dataset. Then, we distill the ensemble of policies into asingle policy, such that the policy may receive a more accurate learning signal from the Q-ensemble.
Figure 3:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during onlinefine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval, respec-tively, across four runs. Dotted lines indicate the performance of offline RL agents before onlinefine-tuning.
Figure 4:	Performance on walker2d tasks from the D4RL benchmark (Fu et al., 2020) with andwithout balanced replay. Specifically, we consider two setups: Uniform replay, where offline andonline samples are sampled uniformly from the same buffer for updates, and Online only, wherethe offline agent is fined-tuned using online samples exclusively.
Figure 5:	We compare BRED (using ensemble distillation) to its variant, an ensemble of independentpolicies (i.e., no distillation), on walker2d tasks. One can observe that ensemble distillationimproves both stability and performance during online fine-tuning.
Figure 6: Performance on walker2d tasks from the D4RL benchmark (Fu et al., 2020) with vary-ing ensemble size N âˆˆ {1, 2, 5}. We observe that performance of BRED improves as N increases.
Figure 7:	Performance of CQL-ft methods on (a) halfcheetah-medium-replay, (b)hopper-medium-replay, and (c) walker2d-medium-replay tasks from the D4RLbenchmark (Fu et al., 2020) during online fine-tuning. The solid lines and shaded regions repre-sent mean and 95% confidence interval, respectively, across four runs.
Figure 8:	Fine-tuning performance on walker2d-medium task when using (a) SAC, (b) SAC-D, and (c) TD3 for fine-tuning pre-trained offline CQL agents. The solid lines and shaded regionsrepresent mean and 95% confidence interval, respectively, across four runs.
Figure 9:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 10:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 11:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 12:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 13:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 14:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 15:	Performance on MuJoCo tasks from the D4RL benchmark (Fu et al., 2020) during on-line fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval,respectively, across four runs.
Figure 16:	Variance in Q-values for 10 sampled actions averaged across mini-batch during fine-tuning. The solid lines and shaded regions represent mean and 95% confidence interval, respectively,across four runs.
Figure 17: Comparison of Q-function estimates and true Q functions for BRED and Online-only inWalker2d tasks. We see that BRED shows low variance, and suffers less from overestimation bias.
