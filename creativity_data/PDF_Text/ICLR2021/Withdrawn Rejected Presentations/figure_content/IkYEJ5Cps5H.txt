Figure 1: Illustration of a channel pruning procedure that leads to inactive weights. When j-th outputchannel of l-th convolution weights W.(? is pruned, i.e. Wij) = 0c1-ιKIKl, then the j-th featuremap of l-th layer Xj(l) should also be 0. Consequently, Xj(l) yields inactive weights Wj(l+1). Notethat we use W(lj- to denote the tensor W.(j).., following the indexing rules OfNUmPy (Van Der Waltet al., 2011).
Figure 2: A comparison of the greedy channel pruning method and our pruning method. Parallelo-grams represent feature maps and squares represent 2-D filters of convolution weights. Gray squaresare filters which account for the objective. The numbers on each squares represent the absolute sumof weights in the filter.
Figure 3: Input channel activation = r(l-1) , shape column activation = q(l) , and the correspond-ing mask (= A(l)) for l-th convolution layer, where A(l) = r(—1)③ q⑴.
