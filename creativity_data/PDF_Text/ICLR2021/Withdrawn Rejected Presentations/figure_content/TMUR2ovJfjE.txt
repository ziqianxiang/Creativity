Figure 1: Two examples where the same low-complexity classifier shows similar fit on trainingsamples, but in (a) the LGF has no constraints (large generator space) and in (b) the LGF is constrainedby smoothness and other invariances (small generator space).
Figure 2: Invariance co-complexity values for the relevant networks on MNIST, when the datapointpairs S and S0 are separated by varying degrees of rotational, scale, shear and translational shifts.
Figure 3: Plots depicting the average generalization gap (difference between test and training error)when 2-layer neural networks of variable hidden neuron number H, are chosen as the label generatingfunction, and a single layer neural network is chosen as the classifier. The generalization gap trend isplotted for two scenarios, (a) m = 10 and (b) m = 100, where m is the number of training examples.
