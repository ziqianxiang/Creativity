Figure 1: An example to highlight the discrepancies between the Synthetic Data (Rows 1 and 3) andReal-Life Data (Rows 2 and 4). In rows 1 and 2, We show sequences of the word order being typedwith the same number of frames between keypresses sampled. The frames with green boxes indicateones in which a key was pressed, i.e, in the first frame for first two rows, the key o was pressed.
Figure 2: An overview of our training procedure. A single training iteration takes a pair of syn-thetic and real-life videos. We disentangle them into style and content representations, and createfour combinations of feature representations. For example, real style paired with synthetic content.
Figure 3: Distribution of phrase and video lengths for our datasets. The number of real-life data-Points (229) is significantly less than synthetic (60,409).
Figure 4: t-sne plots for the outputs of ES (Left), EC (Center), and M (Right).
Figure 5: From left to right: the distribution of Qwerty-D, TER, and METEOR scores, respectively.
Figure 6: An example of the outputs of CycleGAN (ZhU et al., 2017). The top is the output ofCycleGAN and the bottom is the original real-life image.
