Figure 1: Overview of methods connected by the introduced framework of action and perceptionas divergence minimization. Each latent variable leads to a mutual information term between saidvariable and the data. The mutual information with past inputs explains representation learning. Themutual information with future inputs explains information gain, empowerment, and skill discovery.
Figure 2: Action and perception minimize the joint KL divergence to a unified target distributionthat can be interpreted as a learning probabilistic model of the system. Given the target, perceptionaligns the agent’s beliefs with past inputs while actions align future inputs with its beliefs. There aremany ways to specify the target, for example as a latent variable model that explains past inputs andpredicts future inputs and an optional reward factor that is shown as a filled square.
Figure 3: Maximum Entropy RLFigure 3 shows the actual and target distributions for maximum entropy RL. The input sequence isx =. {xt} and the action sequence is a =. {at }. In the graphical model, these are grouped into pastactions and inputs ax< , future actions a> , and future inputs x> . The actual distribution consists ofthe fixed environment dynamics and a stochastic policy. The target consists of a reward factor, anaction prior that is often the same for all time steps, and the environment dynamics,Actual： Pφ(x, a) = IIt p(xt I xi：t-i,ai：t-i)Pφ(at | xi：t,ai：t-i),environment	policyTarget:	T(x, a) & Qt exp(r(xt)) p(xt ∣ xi：t-i,ai：t-i) T(at).
Figure 4: Variational InferenceFigure 4 shows variational inference for the exampleof supervised learning using a BNN (Denker et al.,1987; MacKay, 1992a; Blundell et al., 2015). The inputs are images x =. {xi} and their classesy =. {yi } and we infer the latent parameters w as a global representation of the data set (Alemiand Fischer, 2018). The parameters depend on the inputs only through the optimization processthat produces φ. The target consists of a parameter prior and a conditional likelihood that uses theparameters to predict classes from images,Actual：	Pφ(χ, y, W) = pφ(w) IIi p(χi,yi),belief	dataTarget:	T(x, y, W) & T(W) Qi T(yi | Xi,w).
Figure 5: Amortized InferenceFigure 5 shows amortized inference on the example of a VAE (Kingma and Welling, 2013; Rezendeet al., 2014). The inputs are images x =. {xi} and we infer their latent codes z = {zi}. Theactual distribution consists of the unknown and fixed data distribution and the parameterized encoderpφ (zi | xi). The target is a probabilistic model defined as the prior over codes and the decoderthat computes the conditional likelihood of each image given its code. We parameterize the targethere, but one could also introduce an additional latent variable to infer a distribution over decoderparameters as in Appendix A.1,Actual：	Pφ(χ, Z) = IIi P(Xi)pφ(zi | Xi),data encoderTarget：	τφ(x, Z) = Qi τφ(xi | Zi) T(Zi).
Figure 7: ControlWe describe behavior as an optimal control problemwhere the agent chooses actions to move its distri-bution of sensory inputs toward a preference distri-bution over inputs that can be specified via rewards(Morgenstern and Von Neumann, 1953; Lee et al.,2019b). We first cover deterministic actions that leadto KL control (Kappen et al., 2009; Todorov, 2008)and input density exploration (Schmidhuber, 1991;Bellemare et al., 2016; Pathak et al., 2017).
Figure 8: EmpowermentActual pFigure 8 shows stochastic control with an expressive target that captures correlations between inputsand actions. The input sequence is x =. {xt} and the action sequence is a =. {at}. In the graphicalmodel, these are grouped into past actions and inputs ax<, future actions a>, and future inputs x>.
Figure 9: Skill DiscoveryTarget τFigure 9 shows skill discovery with the input sequence x =. {xt}, action sequence a =. {at}, and thesequence of temporally abstract skills z =. {zk }. The graphical model groups the sequences into pastand future variables. The actual distribution consists of the fixed environment, an abstract policy thatselects skills by sampling from a fixed distribution as shown here or as a function of past inputs, andthe low-level policy that selects actions based on past inputs and the current skill. The target consistsof an action prior and a reverse predictor for the skills and could further include a reward factor,Actual:Target:Pφ(x, a, Z) = QT=K Pφ(Zk) QT=1 pφ(at | x1:t, a1:t-1, zbt/KC ) p(xt | x1:t—1,a1:t —1),abstract policy	policy	environmentTφ(x,a,z) & QT=K Tφ(zk I x) QT=I T(at).
Figure 10: Information GainTarget τFigure 10 shows information gain exploration on the example of latent model parameters anddeterministic actions. The inputs are a sequence x =. {xt } and the latent parameters are a globalrepresentation w. The graphical model separates inputs into past inputs x< and future inputs x>. Theactual distribution consists of the controlled dynamics and the parameter belief. Amortized latentstate representations would include a link from x< to Z . Latent policy parameters would include a21Under review as a conference paper at ICLR 2021link from w to x> . The target distribution is a latent variable model that explains past inputs andpredicts future inputs, as in Appendix A.3. The target could further include a reward factor,Actual：	Pφ(x, W) = pφ(w)∏t pφ(xt | xi：t-i),belief controlled dynamicsTarget:	T(x, W) = T(W) Qt T(Xt | xι.-ι,w).
