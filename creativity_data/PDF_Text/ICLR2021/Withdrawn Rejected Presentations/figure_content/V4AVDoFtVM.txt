Figure 1: Generalized Policy Iteration (GPI) with function approximation. Left: GPI with con-ventional value function approximator Vφ. Right: GPI with PeVFA V (Xn) (Sec. 3) where extrageneralization steps exist. The subscripts of policy ∏ and value function parameters φ, θ denote theiteration number. The squiggle lines represent non-perfect approximation of true values.
Figure 2: Illustrations of value generalization with PeVFA among policies. Each circle representsthe value function of a policy. (a) Global generalization: values can generalize to unlearned policies(∏0 ∈ ∏ι) from already learned policies (∏ ∈ ∏o). (b) Local generalization: values of previouspolicies ({πi}it=0) can generalize to the successive policy (πt+1) along policy improvement path.
Figure 3: Empirical evidences of two kinds of generalization of PeVFA. (a) Global generalization:PeVFA shows comparable value estimation performance on the testing policy set (red) after learningon the training policy set (blue) while maintains reasonable order of optimality. (b) Local general-ization: PeVFA (Vθ (χπ)) shows lower losses (i.e., closer distance to approximation target) thanconvention VFA (Vφπ) before and after the training for values of successive policies along policyimprovement path, which demonstrates Corollary 2. The left axis is for approximation loss (loweris better) and the right axis is for average return as a reference of the learning process of PPO policy.
Figure 4: Framework of policy representation training. Policy network parameters (OPR) or policystate-action pairs (SPR) are fed in policy encoder with permutation-invariant (PI) transformations,producing representation Xn. Xn can be trained by gradients from PeVFA value approximation (i.e.,End-to-End), as well as with optional auxiliary action recovery loss and unsupervised contrastivelearning, shown with separation by the gray dotted line.
Figure 5: t-SNE visualizationfor OPR (E2E) of 6k policiesfrom 5 trials (denoted by differ-ent markers) in Ant-v1. Policiesare colored by average return.
Figure 6: 2D Pointer Walker. (a) The heat map of the utility function of the 2D plane. The darkerregions have higher utilities. (b) Demonstrative illustrations of trajectories generated by 30 syntheticpolicies, showing diverse behaviors and patterns. Each subplot illustrates the trajectories generatedin 50 episodes by a randomly synthetic policy, with different colors as separation. For each trajectory(the same color in one subplot), transparency represents the dynamics along timesteps, i.e., fullytransparent and non-transparent denotes the positions at first and last timesteps.
Figure 7: An illustration of ar-chitecture of PeVFA network. FCis abbreviation for Fully-connectedlayer.
Figure 8:	Complete empirical eVidence of local generalization of PeVFA across 7 MuJoCo tasks.
Figure 9:	Empirical eVidence of local generalization of PeVFA on InVertedPendulum-V1 and Ant-V1 with different learning rates of policy, i.e., {0.0001, 0.001, 0.005}. Results are aVeraged oVer 6trials.
Figure 10:	Examples of policy geometry. (a) An arbitrary policy, where p(s, a) is sampled fromN(0, 1) for a joint space of 40 states and 40 actions and then normalized along action axis. Statesare squeezed into the range of [-1, 1] for clarity. (b) A synthetic continuous policy with p(s, a) =(1 - a5 + s5) exp(-s2 - a2) for a joint space of s ∈ [-2, 2] and a ∈ [-2, 2] (each of whichare discretized into 40 ones) and then normalized along action axis. (c) A general demo of neuralnetwork policy, generated from an arbitrary policy (as in (a)) over a joint space of 200 states and100 actions with some smoothing skill. States are squeezed into the range of [-1, 1] for clarity andthe probability masses of actions under each state are normalized to sum into 1.
Figure 11:	An illustration for policy encoder of Origin Policy Representation (OPR) for a two-layerMLP. h1 , h2 denotes the numbers of hidden units for the first and second hidden layers respectively.
Figure 12:	Examples of data augmentation on policy network parameters for Origin Policy Repre-sentation (OPR). Left: an example of original policy network. Middle: dropout-like random masksare performed on original policy network, where gray dashed lines represent the weights maskedout. Right: randomly selected weights are corrupted by random noises, denoted by orange lines.
Figure 13:	Evaluations of PPO-PeVFA with end-to-end (E2E) trained OPR and SPR in MuJoCocontinuous control tasks. The results demonstrate the effectiveness of PeVFA and two kinds ofpolicy representation, answering the Question 1. The results are average returns and the shadedregion denotes half a standard deviation over 10 trials.
Figure 14:	Evaluations of PPO-PeVFA with OPR and SPR trained through contrastive learning(CL) in MuJoCo continuous control tasks. The results are average returns and the shaded regiondenotes half a standard deviation over 10 trials.
Figure 15:	Evaluations of PPO-PeVFA with OPR and SPR trained through auxiliary loss of actionprediction (AUX) in MuJoCo continuous control tasks. The results are average returns and theshaded region denotes half a standard deviation over 10 trials.
Figure 16:	An overall view of performance evaluations of different algorithms in MuJoCo contin-uous control tasks. The results are average returns and the shaded region denotes half a standarddeviation over 10 trials.
Figure 17: Results for PPO-PeVFA with End-to-End (E2E) OPR on Ant-v1 with respect to thenumber of historical policies that PeVFA is trained on. The setting varies at the numbers of historicalsamples generated by recent policies to different degrees, e.g., 50k denotes 50k steps of state-actionsamples from recent historical policies used for the training of PeVFA. The results are averagereturns and the shaded region denotes half a standard deviation over 10 trials.
Figure 18: Visualizations of end-to-end (E2E) learned Origin Policy Representation (OPR) forpolicies collected during 5 trials (denoted by different kinds of markers). In total, about 6k policiesare plotted for HalfCheetah-V1 (a-b) and 12k for Ant-V1 (c-d). In each subplot, t-SNE and PCA2D embeddings are at left and right respectiVely. In performance View, each policy (i.e., marker) iscolored by its performance eValuation (aVeraged return). In process View, each policy is colored byits corresponding iteration ID during GPI process.
Figure 19: Visualizations of end-to-end (E2E) learned Surface Policy Representation (SPR) forpolicies collected during 5 trials (denoted by different kinds of markers). In performance View, eachpolicy (i.e., marker) is colored by its performance eValuation (aVeraged return). In process View,each policy is colored by its corresponding iteration ID during GPI process.
