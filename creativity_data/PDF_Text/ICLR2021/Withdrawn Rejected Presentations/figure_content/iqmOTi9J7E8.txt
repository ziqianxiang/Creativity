Figure 1: Split inference setup. Client runs M1 locally andsends the features z = M1(x) to the server. The server pre-dicts the intended attribute as ypub = M2 (z). An adversarytrains a separate model M3 to predict the private attribute asypri = M3 (z).
Figure 2: (left): Training and inference for defense 1 (client removes null content of z). (right): Training andinference for defense 2 (client removes low-energy content of z). zo is the feature vector in which the nullcontent is obfuscated. M1 is constructed by augmenting M1 with the module that removes the signal content.
Figure 3: (left) Remaining signal content at the output of each layer of the main model, (right) private accuracyfor different split layers. The content of the input significantly decreases in deeper layer. Similarly, the privateaccuracy decreases, indicating that the discarded content contained information relevant to the private attribute.
Figure 4: The effect of removing the null content on the private accuracy with different split layers.
Figure 6: Privacy accuracy versus split layer.
Figure 5: The effect of the number of preserved features (m0) on the public and private accuracy when thenetwork is split at the input of CONV-3 layer.
Figure 7: Comparison between our method and feature pruning. Both methods reduce the private accuracy.
Figure 8: Comparison between our method and adversarial training. For a given privacy level, our methodprovides higher utility (higher public accuracy) compared to adversarial training method.
Figure 9: Accuracy on public (“smiling”) and private attributes. Our method obfuscates the feature vectorwithout the knowledge of private attribute at training or inference times. Adversarial training method maximizesthe accuracy on “smiling,” while minimizing the accuracy on “gender.” As can be seen, our method reducesaccuracy on all private attributes. The adversarially trained model successfully reduces accuracy on “gender”attribute, but fails to remove information of other attributes. This highlights the applicability of our method inpractical settings as a generic obfuscator compared to specialized techniques such as adversarial training.
Figure 10: Comparing the performance of ourmethod on the 10-FC and 10-CONV networks.
