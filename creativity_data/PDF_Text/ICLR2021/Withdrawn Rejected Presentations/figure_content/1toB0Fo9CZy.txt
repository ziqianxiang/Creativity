Figure 1: (a) A SPD cell structure composed of4 SPD nodes, 2 input node and 1 output node. Initially the edgesare unknown (b) Mixture of candidate SPD operations between nodes (c) Optimal cell architecture obtainedafter solving the relaxed continuous search space under a bi-level optimization formulation.
Figure 2: (a) Distribution of edge weights for operation selection using softmax, sigmoid, and sparsemax onFrechet mixture of SPD operations. (b) Derived SParSemaX architecture by the proposed SPDNetNAS. Bettersparsity leads to less skips and poolings compared to those of other NAS solutions shown in Appendix Fig.5.
Figure 3: (a)-(b) Normal cell and Reduction cell for multiple dimensionality reduction respectivelyA.2 Effect of adding nodes to the cellExperiments presented in the main paper consists of N = 5 nodes per cell which includes two inputnodes, one output node, and two intermediate nodes. To do further analysis of our design choice,we added nodes to the cell. Such analysis can help us study the critical behaviour of our cell designi.e, whether adding an intermediate nodes can improve the performance or not?, and how it affectsthe computational complexity of our algorithm? To perform this experimental analysis, we usedHDM05 dataset (Muller et al., 2007). We added one extra intermediate node (N = 6) to the celldesign. We observe that we converge towards an architecture design that is very much similar interms of operations (see Figure 4). The evaluation results shown in Table (7) help us to deduce thatadding more intermediate nodes increases the number of channels for output node, subsequentlyleading to increased complexity and almost double the computation time.
Figure 4:	(a)-(b) Optimal Normal cell and Reduction cell with 6 nodes on the HDM05 datasetis in-line with the experiments conducted by Liu et al. (2018b). We then transfer the optimizedarchitectures from the singe cell search directly to the multi-cell architectures for training. Hence, thesearch time for all our experiments is same as for a single cell search i.e. 3 CPU days. Results for thisexperiment are provided in Table 8. The first row in the table shows the performance for single cellmodel, while the second and third rows show the performance with multi-cell stacking. Remarkably,by stacking multiple cells our proposed SPDNetNAS outperforms SPDNetBN Brooks et al. (2019) bya large margin (about 8%, i.e., about 12% for the relative improvement).
Figure 5:	(a), (b) Derived architecture by using softmax and sigmoid on the FreChetmixtUre of SPD operations.
Figure 6: (a) Validation accuracy of our method in comparison to the SPDNet and SPDNetBN on RADARdataset. Clearly, our SPDNetNAS algorithm show a steeper validation accuracy curve. (b) Test accuracy on 10%,33%, 80%, 100% of the total data sample. It can be observed that our method exhibit superior performance.
Figure 7: (a) Loss function curve showing the values over 200 epochs for the RADAR dataset (b) Loss functioncurve showing the values over 100 epochs on the HDM05 dataset.
Figure 8: Weighted Riemannian Pooling: Performs multiple weighted FreChet means on the channels of theinput SPDB.2	Average and Max PoolingIn Figure 9 we show our average and max pooling operations. We first perform a LogEig map on theSPD matrices to project them to the Euclidean space. Next, we perform average and max pooling onthese Euclidean matrices similar to classical convolutional neural networks. We further perform anExpEig map to project the Euclidean matrices back on the SPD manifold. The diagram shown inFigure 9 is inspired by Huang & Van Gool (2017) work. The kernel size of AveragePooling.reducedand MaxPooling_reduced is set to2or4 for all experiments according to the specific dimensionalityreduction factors.
Figure 9: Avg/Max Pooling: Maps the SPD matrix to Euclidean space using LogEig mapping, does avg/maxpooling followed by ExpEig mapB.3	Skip ReducedFollowing Liu et al. (2018b), we defined an analogous of Skip operation on a single channel for thereduced cell (Figure 10). We start by using a BiMap layer —equivalent to Conv in Liu et al. (2018b),to map the input channel to an SPD whose space dimension is half of the input dimension. We furtherperform an SVD decomposition on the two SPDs followed by concatenating the Us, Vs and Dsobtained from SVD to block diagonal matrices. Finally, we compute the output by multiplying theblock diagonal U, V and D computed before.
Figure 10:	Skip Reduced:Maps input to two smaller matrices using BiMaps, followed by SVD decompositionon them and then computes the output using a block diagonal form of U’s D’s and V’sB.4 Mixed Operation on SPDsIn Figure 11 we provide an intuition of the mixed operation we have proposed in the main paper.
Figure 11:	Detailed overview of mixed operations. We simplify the example by taking 3 nodes (two inputnodes and one output node) and two candidate operations. Input nodes have two channels (SPD matrices), weperform channelwise weighted FreChet mean between the result of each operation (edge) where weights a'sare optimized during bi-level architecture search optimization. Output node 3 is formed by concatenating bothmixed operation outputs, resulting in a four channel node.
