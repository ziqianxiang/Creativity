Figure 1: The long-tail distribution of sentence piece tokens in the curated multilingual dataset. Thehead classes are tokens with high frequency, otherwise, they are classified as tail classes.
Figure 2: Overview of the Adapt-and-Adjust framework. The layer norm is omitted to save space.
Figure 3: Parameter transfer from a pre-trained multilingual language model to a speech recognitiondecoder. The dotted line shows the transfer direction from a specific module of the model.
Figure 4: Dual-Adapters. The orange box presents the active language-specific adapter and the bluebox presents the common adapter. The figure on the right shows the language adapter structure.
Figure 5: Encoder structure.
Figure 6: Decoder structure.
Figure 7:	Class Frequency. We only show labels more than 20kFigure 8:	Class Frequency with Balance Sampling. We only show labels more than 20k14Under review as a conference paper at ICLR 2021D Ablation study on Inference Phase Logit AdjustmentFigure 9 depicts the CERs of the systems with different τ for inference phase logit adjustmentwith the best A2 configuration: balanced sampling with mBERT and Dual-Adapters. We chooseone language from each group, and the other languages demonstrate the same trend. In Figure 9, theadvantage of the logit adjustment is clearly shown compared to the baseline without logit adjustment(τ = 0). All of the languages achieve their best CER with a τ value of τ = [0.3, 0.4]. Performancesaturates with heavier smoothing (τ >= 0.5).
Figure 8:	Class Frequency with Balance Sampling. We only show labels more than 20k14Under review as a conference paper at ICLR 2021D Ablation study on Inference Phase Logit AdjustmentFigure 9 depicts the CERs of the systems with different τ for inference phase logit adjustmentwith the best A2 configuration: balanced sampling with mBERT and Dual-Adapters. We chooseone language from each group, and the other languages demonstrate the same trend. In Figure 9, theadvantage of the logit adjustment is clearly shown compared to the baseline without logit adjustment(τ = 0). All of the languages achieve their best CER with a τ value of τ = [0.3, 0.4]. Performancesaturates with heavier smoothing (τ >= 0.5).
Figure 9:	Comparison of models with different T.
Figure 10: Validation losses.
Figure 11: Closer look at validation losses.
