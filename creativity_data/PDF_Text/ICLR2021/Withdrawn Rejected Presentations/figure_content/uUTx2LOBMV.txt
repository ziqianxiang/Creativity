Figure 1: The schematic of tensor networks based on MPS. Left is GTNs and right is DTN.
Figure 2: The diagram about the model architecture of TextTN.
Figure 3: On four classification tasks (MR, CR, MPQA and Subj), we calculate the entanglemententropy and evaluate the accuracy of TeXtTN, respectively, with different bond-dimensions set from 5to 30.
Figure 4: The schematic diagram of tensors and tensor contraction12Under review as a conference paper at ICLR 2021Figure 5: The schematic diagram of Tensor-Train decomposition and Matrix Product State尸(S)= WlΦ(S)ClaSS □□□□Figure 6: The diagram about the computing of singular values for the entanglement entropy. The left isthe position of separation in SentenCe-DTN (the position is v〔n/2j), and the right is the representationof singular values in the process of SVD for sentence-DTN.
Figure 5: The schematic diagram of Tensor-Train decomposition and Matrix Product State尸(S)= WlΦ(S)ClaSS □□□□Figure 6: The diagram about the computing of singular values for the entanglement entropy. The left isthe position of separation in SentenCe-DTN (the position is v〔n/2j), and the right is the representationof singular values in the process of SVD for sentence-DTN.
Figure 6: The diagram about the computing of singular values for the entanglement entropy. The left isthe position of separation in SentenCe-DTN (the position is v〔n/2j), and the right is the representationof singular values in the process of SVD for sentence-DTN.
Figure 7:	The diagram of all-function learning for tensor network. (a) The conditional probabilityfunctions for the output in different position S%(i ∈ {1,..., n}). (b) Getting an all-function f (S) bycombining all functions f i(S) from different positions.
Figure 8:	The entanglement entropy in different training epochs on four datasets, including MR, CR,MPQA and Subj when the initialized training.
