Figure 1: Overview of our attack. Given a poisoned classifier, we construct a robustified smoothedclassifier using Denoised Smoothing (Salman et al., 2020). We then extract colors or cropped patchesfrom adversarial examples of this robust smoothed classifier to construct novel triggers. Thesealternative triggers have similar or even higher attack success rate than the original backdoor.
Figure 4: Backdoor patterns in adversarial examples ( = 20) for robustified poisoned classifiers,where each poisoned model has a different color trigger.
Figure 3: Backdoor triggers usedin our analysis.
Figure 6: Comparison of different forms of adversarial examples ( = 20) from a binary poisonedclassifier on ImageNet.
Figure 7: Results for attacking a robustified poisoned multi-class classifier obtained through Bad-Net (Gu et al., 2017). The attack success rate of the original backdoor Trigger A is 72.60%. Theregion which we use to construct alternative triggers is highlighted in a red box.
Figure 8: Results of applying our attack on an ImageNet clean classifier.
Figure 9: Analysis of a poisoned classifier with a “camouflaged” backdoor trigger.
Figure 10: Results of attacking two poisoned classifiers in TrojAI dataset.
Figure 11:	Results for attacking three binary poisoned classifiers obtained by three backdoor attacks.
Figure 12:	Results for attacking multi-class poisoned classifiers on ImageNet obtained byHTBA (Saha et al., 2020) and CLBD (Turner et al., 2019).
Figure 13: Results of applying our attack on an ImageNet clean classifier (binary).
Figure 14: Results of attacking 8 poisoned classifiers in the TrojAI dataset.
Figure 15:	Results of attacking two clean classifiers in the TrojAI dataset.
Figure 16:	Adversarial examples ( = 20 in l2 norm) of a robustified poisoned classifier in theTrojAI dataset. Below each image is the class predicted by the original poisoned classifier.
Figure 17: Comparison of different adversarial examples ( = 20) of a robustified binary poisonedclassifier on ImageNet.
Figure 18:	Effects of enhanced visualization techniques on adversarial examples of a robustifiedImageNet binary poisoned classifier.
Figure 19:	Comparison of adversarial examples generated with/without regularization.
Figure 20: Interface of interactive tool we develop for TrojAI dataset.
Figure 21: Sample saliency maps of a poisoned classifier on clean images.
Figure 22: Adversarial examples of robustified poisoned classifiers with different fixed trigger loca-tions during training.
Figure 23: Results of attacking a poisoned ImageNet classifier with 10 classes. The success rate ofthe original backdoor is 59.71%.
