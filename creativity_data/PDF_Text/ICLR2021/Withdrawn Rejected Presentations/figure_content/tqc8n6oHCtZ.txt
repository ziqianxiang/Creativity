Figure 1: Illustration of (a) word-vector elimination process in PoWER-BERT (Goyal et al., 2020)and (b) Drop-and-Restore process in Length-Adaptive Transformer. Yellow box and blue boxesimply the output of embedding layer and transformer layers, respectively. Green boxes mean vectorsdropped in lower layers and restored at the last layer. Red box is the task-specific layer. Thoughword-vectors in the middle could be eliminated (or dropped), remaining vectors are left-aligned forthe better illustration. In this case, the number of transformer layers is four.
Figure 2: Pareto curves ofF1 score and FLOPs on SQuAD 1.1 (Rajpurkaret al., 2016). We apply the proposed method to BERTBase (solid lines) andDistilBERT (dotted lines). For each model, we draw three curves using(1) standard finetuned transformer with constant-rate length reduction, (2)Length-Adaptive Transformer with constant-rate length reduction, and(3) Length-Adaptive Transformer with length configurations obtainedfrom the evolutionary search.
Figure 3: Correlationbetween FLOPs andlatency with differentlength configurations.
Figure 4: Exam-ple of area underPareto curve as theevolutionary search oflenth configurationsproceeds.
