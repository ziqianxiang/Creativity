Figure 1: Architecture of QTRAN++To be specific, QMIX express the mixing network fmiχ as a fully-connected network with non-negativeweight θ(s) ≥ 0. The non-negative weight is state-dependent since it is obtained as an output ofhypernetworks (Ha et al., 2017) with the underlying state as input.
Figure 2: Learning process of QTRAN and QTRAN++. When Qtran is in overall larger than Qjt,QTRAN decreases Qtran only for the optimal action U. On the other hand, our method efficientlylearns the transformed action-value estimator through additional constraint for non-optimal actions.
Figure 3: Median test win rate with 25%-75% percentile, comparing QTRAN++ with baselines. Allthe results are averaged over five independent runs. Each plot corresponds to different scenario names,e.g., 3s5z. Scenarios with additional negative rewarding mechanism are indicated by “(negative)” inthe name of scenarios, e.g., 3s5z (negative).
Figure 4: Median test win rate with 25%-75% percentile, comparing QTRAN++ with its modifica-tions. All the results are averaged over five independent runs. Each plot corresponds to differentscenario names, e.g., 3s5z. Scenarios with additional negative rewarding mechanism are indicated by“(negative)” in the name of scenarios, e.g., 3s5z (negative).
Figure 5: Illustration of the policies learned using QTRAN++ and QMIX in negative scenarios.
Figure 6:	Median test win rate with 25%-75% percentile, comparing QTRAN++ to baselines usingmore training steps. All the results are averaged over five independent runs.
Figure 7:	Experimental result on the multi-domain Gaussian squeeze environment. Partial observabil-ity indicates the amount of observation provided for each agent.
Figure 8:	Experimental result on the multi-domain Gaussian squeeze environment. Partial observabil-ity indicates the amount of observation provided for each agent.
