Figure 1: Illustration of different methods for processing long sequences. Each square representsa hidden state. The black-dotted boxes are Transformer layers. (a) is the sliding-window-basedmethod to chunk a long sequence into short ones with window size 3 and stride 2. (b) builds cross-sequence attention based on sliding window over pre-selected positions (red-dotted boxes). (c)hashes the hidden states into different buckets by randomly-initialized vectors. (d) is our proposedapproach to cluster the hidden states. Our final model is a combination of (a) and (d) that processesboth local and global context.
Figure 2: An overview of proposed Transformer layer. (a) Sliding-Window layer over a sequence.
