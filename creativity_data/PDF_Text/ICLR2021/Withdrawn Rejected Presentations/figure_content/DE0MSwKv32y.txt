Figure 1: Agent (gray circle) is moving in a grid-world. Itcan move through the door (blue) to other rooms, if it entera trap (red) the episode is terminated. (Genuine) one-steploop arise when agent attempts to walk into the wall. In theexample, the model imagines that performing action "right"will not result in environment change (false loop). In factchoosing this action would result in moving to the next room.
Figure 2: Map of ToyMontezumaRevenge statespace. The only reward is given after reachingthe goal room marked with G. To obtain it, agentneeds to gather several keys (marked in yellow)and go through doors (marked in blue). Step-ping on a trap (marked in red) results in episodeAs for performance metrics, we use minimal dis-tance to the goal. It is measured in steps and termination.
Figure 3: ToyMontezumaRevenge, comparison between planners augmented with TBV, ε-greedy,and no top-level exploration mechanism. Results are averaged over 10 random seeds, shaded areasshow 95% confidence intervals.
Figure 4: Tower of Hanoi, both MCTS and BestFS quickly find the solution if augmented with TBV.
Figure 5: Numerical measurement of influenceof TBV on BestFS in the presence of a falseloop in ToyMR. The upper (blue) line presentsthe frequency of reaching the open doors in thefirst room for the agent without TBV. This agentnever enters the door due to a false loop error.
Figure 6: False loop verification in the Towerof Hanoi. We consider two agents: MCTS withTBV (orange) and ε-greedy (blue) exploration(presented also on Figure 4). The plots showshow often agent performed action which falselyseemed to be a loop in the state-action graph.
Figure 7: Tower of Hanoi puzzle. For the firsttime, the agent finds himself in a position tomove the fourth disk but mistakenly believesthat it is not possible, resulting in a false-loop.
Figure 8: Performance of TBV BestFS on the Tower of Hanoi domain with different values of QantileRankA.5	Random TBV rejection analysisIn our preliminary experiments we have seen that without random rejection of TBV mechanism theagents often tends to get stuck in cycles with high ensemble disagreement and is unable to leavesuch cycle until the end of episode (since model is not updated in during the episode). In Figure9 we present the performance in Tower of Hanoi for different values of frequency of random TBV19Under review as a conference paper at ICLR 2021Figure 9: Performance of TBV BestFS on the Tower of Hanoi domain with different frequencies ofrandom rejection of TBVrejection (while the rest of parameters is same as in our best experiments). It can be seen, that if thefrequency of TBV override (i.e. frequency with which we allow TBV to change planners action) isbetween 0.5 and 0.9 we obtained best results. Also, TBV is not very sensitive to this parameter aslong it is in an appropriate range of values.
Figure 9: Performance of TBV BestFS on the Tower of Hanoi domain with different frequencies ofrandom rejection of TBVrejection (while the rest of parameters is same as in our best experiments). It can be seen, that if thefrequency of TBV override (i.e. frequency with which we allow TBV to change planners action) isbetween 0.5 and 0.9 we obtained best results. Also, TBV is not very sensitive to this parameter aslong it is in an appropriate range of values.
Figure 10: Performance of RND and PPO on Tower of Hanoi with longer training.
