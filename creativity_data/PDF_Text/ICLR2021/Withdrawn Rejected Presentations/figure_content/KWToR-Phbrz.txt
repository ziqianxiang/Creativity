Figure 1: DiVE encodes the input image (left) to explain into a latent representation Z. Then Zis perturbed by and decoded as counterfactual examples. During training, LCF finds the set ofthat change the ML model classifier outcome while Ldiv and Lprox enforce that the samples arediverse while staying proximal. These are four valid counterfactuals generated from the experimentin Section 4.4. However, only the bottom row contains counterfactuals where the man is still baldas indicated by the oracle or a human. These counterfactuals identify a weakness in the ML model.
Figure 2: Beyond trivial explanations. The rate of successful explanations (y-axis) plotted againstembedding similarity (x-axis) for all methods. For both metriCs, higher is better, i.e., the most valu-able explanations are in the top-right Corner. For eaCh method, we ran an hyperparameter sweepand denote the mean of the performanCes with a dot. The Curves are Computed with KDE. Theleft plot shows the performanCe on CelebA and the other two plots shows the performanCe for in-distribution (ID) and out-of-distribution (OOD) experiments on Synbols . All DiVE methods outper-form xGEM+ on both metriCs simultaneously when Conditioning on successful counterfactuals. Inboth experiments, DiVEFisher and DiVEFisherSpeCtral improve the performanCe over both DiVERandomand DiVE.
Figure 3: Qualitative results of DiVE, Progressive Exaggeration (PE) [53], and xGEM [26] for the“Smiling” attribute. Each column shows the explanations generated for a target probability outputof the ML model. The numbers on top of each row show the actual output of the ML model.
Figure 4: Qualitative results of DiVE, Progressive Exaggeration (PE) [53], and xGEM+ for the“Young” attribute. Each column shows the explanations generated for a target probability output ofthe ML model. The numbers on top of each row show the actual output of the ML model.
Figure 5: Bias detection experiment. Each column presents an explanation for a target “Smiling”probability interval. Rows contain explanations produced by PE [53], xGEM+ and our DiVE. (a) ofagender-unbiased classifier, and (b) corresponds to explanations of a gender-biased “Smile” classifier.
Figure 6: Successful counterfactual generations for different instantions of DiVE. Here, the originalimage was misclassified as non-smiling. All methodologies were able to correctly add a smile to thewoman.
Figure 7: Labelling interface. The user is presented with a counterfactual image and has to chooseif the target attribute is present or not in the image.
Figure 8: Sample of the synbols dataset.
