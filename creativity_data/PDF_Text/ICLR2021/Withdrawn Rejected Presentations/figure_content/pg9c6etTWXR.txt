Figure 1: Predictive uncertainty of a (a) MAP-trained, (b) Laplace-approximated (LA), and (c)LULA-augmented Laplace-approximated (LA+LULA) neural network on regression (top) and clas-sification (bottom) tasks. Black curve represent predictive mean and decision boundary, and shaderepresents ±3 standard deviation and confidence in regression and classification, respectively. MAPis overconfident and LA can mitigate this. However, LA can still be overconfident away from thedata. LULA improve LA’s uncertainty further without affecting its predictions.
Figure 2: An illustration of the proposed construction. Rectangles represent layers, solid linesrepresent connection between layers, given by the original weight matrices WM(1A)P, . . . , WM(LA)P. Theadditional units are represented by the additional block at the bottom of each layer. Dashed linescorrespond to the free parameters Wc(1), . . . , Wc(L-1), while dotted lines to the zero weights.
Figure 3: LULA compared to DPN on the Rotated-MNIST benchmark.
Figure 4: Even when their weights are assigned randomly, LULA units improve the vanilla Laplacein terms of UQ. Fine-tuning the LULA weights improve it even further, in particular in terms ofconfidence far from the data—trained, LULA yields less confident prediction in this region.
