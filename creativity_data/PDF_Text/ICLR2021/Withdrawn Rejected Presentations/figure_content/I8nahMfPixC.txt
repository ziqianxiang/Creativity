Figure 1: Performances of adversarial training (Def-adv) and our ADD-Defense on MNIST (a) andthe illustration of the invariant information (b). The defenses are trained with adversarial examplesgenerated from untargeted FGSM attack. “Adv” denotes adversarial examples.
Figure 2: An overview of ADD-Defense. We remove perturbation-specific inforamtion generatedfrom known attacks by a perturbation discriminator. Next, We make the representation generalizefor unknown attacks by matching it with a Gaussian prior. In this way, We can obtain a perturbation-invariant representation (PIR). A classifier is exploited to preserve class-specific information. Inputan adversarial example Xu generated from an unknown attack, we can extract the PIR and utilize itto generate an example which have a correct prediction for the target model Mt.
Figure 3: Performances of defenses on MNIST. The known attacks are the PGD attacks with e0.15 through 40 iterations (a) and DDN attacks (b).
Figure 4: Performances of defenses against RP2 attack (a) and the restored examples against STuattack (b). The known attacks are attack 1 and attack2, which generate different adversarial patches.
Figure 5: Performances of defenses against BPDA on MNIST (a) and AUtoAttack (b). For BPDAattack, the defenses are trained based on L∞-PGD with e = 0.15. The dotted line represents theaccuracy of the attacked target model and the solid line represents the accuracy of the target modelusing defenses. For AutoAttack, our defense is trained based on L∞-PGD with e = 0.15 and 0.3 onMNIST and L∞-PGD with e = 0.03 on CIFAR-10.
Figure 6: Illustration of examples distribution. “un” indicates Untargeted attack and "ta“ indicatestargeted attack. The top images are the distributions of original examples and adversarial examples.
Figure 7:	The illustration of restored examples on MNIST (a) and CIFAR-10 (b). The red superscriptrepresents the wrong classification and the green superscript represents the correct classification.
Figure 8:	Adversarial examples generated by various attacks on MNIST dataset (a). The restoredimages of our defense (b) and APE-GAN (c) on MNIST dataset.
Figure 9:	Adversarial examples generated by various attacks on Fashion-MNIST dataset (a). Therestored images of our defense (b) and APE-GAN (c) on Fashion-MNIST dataset.
Figure 10:	Adversarial examples generated by various attacks on CIFAR-10 dataset (a). The restoredimages of our defense (b) and APE-GAN (c) on CIFAR-10 dataset.
Figure 11: Adversarial examples generated by various attacks on SVHN dataset (a). The restoredimages of our defense (b) and APE-GAN(C) on SVHN dataset.
Figure 12: Performance against attacks on SVHN. The defense is removed some different compo-nents, such as the perturbation discriminator and the prior distribution.
Figure 13: Adversarial examples generated with different adversarial patches (a) and the restoredimages of our defense (b) on LISA dataset.
Figure 14: Adversarial examples generated from ST and FSW attack on MNIST on the top threerows. The restored examples of our defense ADD-Defense, improved framework ADD-Defense-Sand APE-GAN are shown on the next rows.
Figure 15: Performance against attacks with various maximum perturbation values. The rose linerepresents the accuracy of the target model to the adversarial examples, and the cerulean line repre-sents the accuracy after defense. Fig (b) is an enlarged view of the cerulean part in Fig (a)images in first, sixth, eighth and ninth columns are easily mistaken for the number "8, 9, 3, 9" invisual perception.
Figure 17: The images in BPDA experiment. The SUbscPIRt “adv” denotes the adversarial examplesof BPDA for different defenses. The subscPIRt “def” denotes the restored images of defense. Fromthe first row to the third row, the maximum number of iterations is 1,10 and 20 respectively.
Figure 18: Illustration of hardness inversion on MNIST dataset. “-u” indicates untargeted attack and”-t” indicates targeted attack. The two colored lines represent the results based on different knownattacks respectively. Fig(a) shows the results against targeted and untargeted attacks and Fig(b)shows the results against white-box and black-box attacks. The accuracy against targeted attacksand black-box attacks is higher than that against untargeted attacks and white-box attacks.
Figure 19:	The examples with non-malicious perturbation. The target images in figure (left) arethe adversarial examples generated by untargeted C&W attack. The target images in figure (right)are the adversarial examples generated by untargeted DDN attack. The images on fourth and fifthrows reflect the difference in pixels between the generated images and the target images. The bluepixels indicate that the pixel value in generated images is lower than that in the target images, andred pixels indicates that pixel value in generated images is higher than that in the target images.
Figure 20:	The misclassified reconstructed images on MNIST dataset. The images reconstructed byour defense are shown on the second row. In visual perception, the number reflected in the image issimilar to the error class.
