Figure 1: From left to right, the Hopper, Half-Cheetah, and Ant environments we use to test ouralgorithm.
Figure 2: Top row: Average cumulative reward under swaps for one adversary training (left) andthree-adversary training (right). Each square corresponds to 20 trials. In the three adversary case,each square is the average performance against the adversaries from that seed. Middle row: (Left)Playing the agent trained against 1 adversary against the adversaries from the three adversary case.
Figure 3: Average reward for Ant, Hopper, and Cheetah environments across ten seeds and across thevalidation set (top row) and across the holdout test set (bottom row). We compare vanilla PPO, thedomain randomization oracle, and the minimax adversary against RAP of size three and five. Barsrepresent the mean and the arms represent the std. deviation. Both are computed over 20 rollouts foreach test-set sample. The std. deviation for the test set are not reported here for visual clarity due tothe large variation in holdout test difficulty.
Figure 4: Average reward across ten seeds on each validation set parametrization - friction coefficienton the x-axis and mass coefficient on the y-axis. DR refers to domain randomization and X Adv is anagent trained against X adversaries. Top row is Hopper and bottom row is Half Cheetah.
Figure 5: Average reward for Hopper across varying adversary number.
Figure 6: Average reward for Ant, Hopper, and Cheetah environments across ten seeds and across thevalidation set (top row) and across the holdout test set (bottom row). We compare vanilla PPO, thedomain randomization oracle, and the minimax adversary against RAP of size three and five. Barsrepresent the mean and the arms represent the std. deviation. Both are computed over 20 rollouts foreach test-set sample. The std. deviation for the test set are not reported here for visual clarity due tothe large variation in holdout test difficulty.
Figure 7: Labelled Body Segments of HopperTable 2: Hopper Holdout Test DescriptionsTest	Body WBh Friction Coeff 1.3	Body with Friction Coeff 0.7A	Torso, Leg	Floor, Thigh, FootB	Floor, Thigh	Torso, Leg, FootC	Foot, Leg	Floor, Torso, ThighD	Torso, Thigh, Floor	Foot, LegE	Torso, Foot	Floor, Thigh, LegF	Floor, Thigh, Leg	Torso, FootG	Floor, Foot	Torso, Thigh, LegH		Thigh, Leg		Floor, Torso, FootC.1 HopperThe Mujoco geom properties that we modified are attached to a particular body and determine itsappearance and collision properties. For the Mujoco holdout transfer tests we pick a subset of thehopper ‘geom’ elements and scale the contact friction values by maximum friction coefficient, 1.3.
Figure 8: Labelled Body Segments of Cheetah13Under review as a conference paper at ICLR 2021Table 3:	Cheetah Holdout Test Descriptions. Joints in the table receive the maximum frictioncoefficient of 0.9. Joints not indicated have friction coefficient 0.1TestABCDEFGHGeom With Friction Coeff 0.9Torso, Head, FthighFloor, Head, FshinBthigh, Bshin, BfootFloor, Torso, HeadFloor, Bshin, Ffoot
Figure 9: Labelled Body Segments of Antcheetah ‘geom’ elements and scale the contact friction values by maximum friction coefficient, 0.9.
Figure 10: Ant Heatmap: Average reward across 10 seeds on each validation set (mass, friction)parametrization.
Figure 11: Average reward for Half Cheetah environment across ten seeds. Top row shows theaverage reward when trained with a ‘bad’ friction parametrization which lead to DR not learning arobust agent policy, and bottom row shows the average reward when trained with a ‘good’ frictionparametrization.
Figure 12: Left: heatmap of the performance of the half-cheetah domain randomized policy acrossthe friction and mass value grid. Right: Left: heatmap of the performance of the half-cheetah domainrandomized policy across the friction and mass value grid where the agent policy is an LSTM.
Figure 13: The DeepMind Control catch task. The cup moves around and attempts to get the ball tofall inside.
Figure 14: (Top left) 0 adversary, (top right) 1 adversary, (bottom left) 3 adversary, (bottom right) 5adversaries for variations of cup and ball mass.
Figure 15: Two transfer tests for the bandit task. On both tasks the 4 adversary case has improvedperformance relative to RARL while domain randomization performs terribly on all tasks. Barsindicate one std. deviation of the performance over 100 trials.
Figure 16: Wall-clock time vs. reward for varying numbers of adversaries. Despite varying adversarynumbers, the wall-clock time of 1, 3, 5, and 7 adversary runs are all the same.
Figure 17: Iterations vs. reward for varying numbersnumbers, the wall-clock time of 1, 3, 5, and 7 adversary500	600	700of adversaries. Despite varying adversaryruns are all the same.
