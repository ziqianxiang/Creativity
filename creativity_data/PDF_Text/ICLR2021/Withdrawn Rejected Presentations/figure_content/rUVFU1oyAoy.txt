Figure 1: Geometric illustration ofNon-Convex Continual Learning (NCCL). In the continual learn-ing setting, the model parameter starts from the moderate local optimal point for the previouslylearned tasks XP. Over T iterations, we expect to reach the new optimal point XPuc which has agood performance on both previously learned and current tasks. In t-th iteration, the model param-eter xt encounters either VgJt,pos3) or Vghneg(χt). These two different cases indicate whether(fit (Xt), VgJt (xt)〉is positive or not. To prevent χt from escaping the feasible region, i.e., catas-trophic forgetting, we propose the theoretical condition on learning rates for f and g.
