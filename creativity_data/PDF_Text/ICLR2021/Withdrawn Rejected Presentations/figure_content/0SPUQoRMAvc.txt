Figure 1: The overview of the proposed architecture. An extra semantic branch is proposedto offer contextual information for depth representations. Then, we enhance the local depth fea-ture representation via sampling and enhancing semantic edge-based point features via the proposedSemantic-guided Edge Enhancement module (SEEM). The global feature representation is promotedby the proposed semantic-guided multi-layer self-attention (SA), which explores pixels feature cor-relations between fused depth and semantic features.
Figure 2: The semantic-guided edge enhancement module (SEEM). The proposed method firstsamples and extracts the semantic edge-located point set which consists of the edge points, disturbededge points and random points. Then, the points features are extracted via bilinear sampling and fedinto the point feature enhancement module which includes a set of 1-D convolutions, to enhance thelocal point depth feature representations independently.
Figure 3: Qualitative comparison on KITTI 2015. Our method shows its distinct advantagesagainst other methods in generating sharp depth borders as well as thin structures that is well-alignedwith the semantic contextual information.
Figure 4: Qualitative results of the proposed method against the baseline. The third columnrefers to the semantic guidance, in which the black and white area denote the foreground and back-ground, and the red dots are the sampled points of the proposed SEEM module.
Figure 5: Semantic category-level improvement. We use the AbsRel metric for evaluation. Bluerefers to our methods and red denotes the baseline method (Godard et al., 2019). Benefited fromthe proposed SEEM module and semantic-guided multi-level attentions, our method improvementsconsistently on most of semantic categories.
Figure 6: Qualitative results on Cityscapes (Cordts et al., 2016). All methods are trained on KITTI(Geiger et al., 2012) to evaluate the generalization abilities. Ours-C ityp and Ours-C ityGT refer toresults generated using input pseudo and GT semantic label, respectively.
Figure 7:	Qualitative ablation results. “B” denotes the baseline method. “B+SEEM” and “B+SA”are two ablated versions of our methods. As highlighted by the green and blue boxes, the twoproposed contributions complement each other to produce predictions with the best accuracy.
Figure 8:	Qualitative results of off-the-shelf semantic models. “E” denotes the error map, “CSV ”and “CSV + K” refer to results generated by model MCSV and MCSV+K. “F ull” means fullsemantic label with 19 categories, while “Bi” stands for the binary label containing only back-ground/foreground categories.
Figure 9: Additional qualitative comparisons on KITTI dataset. All models are trained on KITTI(Geiger et al., 2012) Eigen split (Eigen et al., 2014).
Figure 10: Additional qualitative comparisons on Cityscapes. All models are trained on KITTI(Geiger et al., 2012) Eigen split (Eigen et al., 2014). Ours-Cityp and Ours-CityGT refer to resultsgenerated using input pseudo and GT semantic label, respectively.
