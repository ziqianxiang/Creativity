Figure 1: Policy architecture. The policy starts with an empty memory bank. An agent executes thepolicy in a task. If it fails, the trajectory is added to a memory bank. When the agent performs thetask again, the policy adapts the output based on the information from memory. The policy uses theTransformer to extract information from each trajectory and a gated attention mechanism to mergethe information of multiple trajectories.
Figure 2: Examples of the 2D Gridworld, 3D Miniworld, UR Robot Reacher environments. (a): thetop-down view of an example of 2D Gridworld. (b) and (c) show the 3D miniworld environment. (b):first-person view (observation input) for the agent. (c): top-down view of the environment, which isnot available to the agent. (d) and (e) show the UR robot reacher environment. (d) is an exampleobservation input to the policy. (e) shows UR robot reacher setup.
Figure 3: Learning in the Gridworld environment. (a): learning curves in training. Our method(Memory) learns significantly faster than SNAIL. PPO-finetune and PPO-finetune + Attention failto learn to solve the tasks. (b): testing results on new tasks in the Gridworld. (c) and (d): testingresults (5 episodes) on 200 new tasks (2D gridworld) as the training progresses. Success: tasks thatare successful. Fail(hit > 0) and End Diff : tasks where the agent failed, hit at least one trap within 5episodes, and ended up in a different state (a different trap or does not hit any block) in each episode.
Figure 4: (a): learning curves of the policies that store failures of different length in the memory. (b):comparing learning curves of our method (Memory), SNAIL, and SNAIL with concatenated memory.
Figure 5: Testing results forUR Robot Reacher taskstecture as Memory, but add both the successes and failures to the same memory bank. We performedthe ablation experiments in the 2D Gridworld and 3D Miniworld environments.
Figure 6: Ablation on adding successful experiences. (a) and (b) show the learning curves duringtraining for the 2D Gridworld and 3D Miniworld environments. (c) and (d) show the testingperformance of the trained policies.
