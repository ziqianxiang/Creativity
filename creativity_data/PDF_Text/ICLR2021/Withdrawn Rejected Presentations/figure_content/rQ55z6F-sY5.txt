Figure 1: The illustration depicts 3 different approaches of recognising the object of interest. (From left toright) The first person portrays the approach of traditionally trained models. In the middle, is our adopted ap-proach for recognition. The illustration only shows one of the many queries asked. Lastly, individual responsesfrom domain experts represent an ensemble of one-class models.
Figure 2: (a) The proposed 2 stream model for INN. The joint image-label representation(h) is computedfrom image(z) and label(Ïˆ) representations. This representation is solely responsible for driving the binaryprediction of the model. (b) Last layers of a traditionally trained model.
Figure 4: Grad-CAM visualisations on the training images for the STL-10 dataset. Red region indicates theareas contributing highest towards the prediction of the model.
Figure 5: Grad-CAM visualisation for different activations used in the label encoder.
Figure 6: Umap plotFigure 7: Accuracy vs d Figure 8: Accuracy on CUB-200The results indicate that for training larger datasets we are required to employ networks with com-paratively larger latent dimensions.
Figure 7: Accuracy vs d Figure 8: Accuracy on CUB-200The results indicate that for training larger datasets we are required to employ networks with com-paratively larger latent dimensions.
