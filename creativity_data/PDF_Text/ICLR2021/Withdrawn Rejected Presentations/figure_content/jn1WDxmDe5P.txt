Figure 1: This figure shows an outline of our method. The green and blue dashed lines show twopossible inputs to the clustering. The green part shows the high dimensional input (e.g. images)feature extraction using an auto-encoder and the blue line shows the low dimensional input. Theright part of the figure shows the prediction of k and how we update the controller network.
Figure 2: Results on MNIST & Fashion-MNIST datasets. The green square shows the predicted kvalue by Meta-K and the red circle shows the maximum value on the specific metric.
Figure 3: The graphs of cumulative return for MNIST & FMNIST datasets0	20	40	60	80	100epochsFMIST5 ConclusionIn this paper, we proposed a self-supervised approach to find the optimal number of clusters forany dataset in the K-Means algorithm. We showed in our experiments that our method is able topredict the number of clusters effectively without any direct supervision. Even though our methodis able to predict the number of clusters in most scenarios with distinctive features, it would bechallenging if the input features are compact and not separable. Another limitation of our work isthat it is dependent on the silhouette score, and the method would perform poorly if silhouette scoredoesnâ€™t achieve a good clustering evaluation. However our method is not limited to silhouette scoreand any other clustering evaluation metric can be used as well. Another point is that our methodis dependent on the feature extraction network and if the encoder network is not trained properly,the controller training would also be challenging. We plan to improve our approach by using othermetrics than silhouette score, adaptive layers, and Gaussian Mixture Models (GMMs) to have anend-to-end pipeline for fully automated clustering.
