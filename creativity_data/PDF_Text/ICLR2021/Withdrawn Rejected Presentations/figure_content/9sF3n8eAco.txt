Figure 1: Various floating-point formats: (a) conventional ones, and (b) the proposed FFP8 format3 Flexible 8-Bit Floating-Point (FFP8) Format3.1	Definition of the FFP8 FormatIn this subsection, a flexible 8-bit floating-point (FFP8) format, which leads to more accurate in-ference outcomes of deep neural networks, is presented. A typical floating-point format consists ofsign (s), exponent (e), and fraction (f) fields, and the bit length of each field is specified as x, y, z,respectively. Besides, one more parameter, exponent bias (b), is required to completely specify afloating-point number. Conventionally, b is always implicitly set to 2y-1 - 1. In this paper, an n-bitfloating-point format is denoted as (x, y, z, b) or (x, y, z), where n = x + y + z. If b is missing, thedefault value is implicitly used.
Figure 2: The range windows of various 8-bit floating-point formats versus the weight distributionof VGG-16; (a) in FP8(1, 4, 3), (b) in FP8(1, 5, 2), (c) in FFP8(1, 4, 3, 15), (d) in FFP8(1, 3, 4, 7)higher precision whereas 4.6% of leftmost weights are not included in the range window. However,those out-of-the-window weights can be regarded as some sort of pruned weights. That is, moreinvestigation should be further conducted to determine whether FFP8(1, 3, 4, 7) or FFP8(1, 4, 3, 15)is better for the weights in VGG-16. One way or another, it is clear that the proposed FFP8 formatscan achieve certain improvement that the conventional 8-bit formats cannot.
Figure 3: The process converting FP32 inputs into FFP8 outputs with the given format parametersTable 1: Top-1 and Top-5 accuracy of VGG-16 on ImageNet dataset (Deng et al., 2009) in variousnumber formats; delta (∆) indicates the accuracy drop as compared to FP32Weight	Activation	Top-1 (∆)	Top-5 (∆)FP32	FP32	71.59%	90.38%(1,5,2,15)	(1,4,3,7)	69.89% (-1.70%)	89.29% (-1.09%)(1,4,3,7)	(1,4,3,7)	70.86% (-0.73%)	90.02% (-0.36%)(1,4,3,15)	(1,4,3,7)	70.96% (-0.63%)	90.10% (-0.28%)(1,3,4,3)	(1,4,3,7)	70.18%(-1.41%)	89.56% (-0.82%)(1,3,4,7)	(1,4,3,7)	71.19% (-0.40%)	90.12% (-0.26%)(1,3,4,7)	(1,4,3,7)+(0,4,4,7)	71.19% (-0.40%)	90.14% (-0.24%)In addition to weights, it is certainly worth finding out best-fit formats for activations as well. Forthose attempts made above, activations are always in FFP8(1, 4, 3, 7) for two reasons: 1) the overalldistribution of activations is wider than that of weights, and 2) the maximum magnitude of activa-tions is much bigger than that of weights. The detailed distribution of activations will be given inSection 3.4 later. Meanwhile, it is also worth noting that a large set of commonly used activationfunctions always produce nonnegative outputs, e.g., ReLU, ReLU6, and sigmoid. That is, if thoseoutputs are represented in any signed format, a half of the code space is actually wasted. It may notbe a problem for 32-bit and 16-bit formats with long enough fraction bits; however, it is indeed aserious issue for any 8-bit format, which merely has 256 available codes in total. Since VGG-16
Figure 4: Weight distributions of (a) wholemodel, (b) first layer, (c) Layer 6, (d) last layerFigure 5: Activation distributions of (a) whole model, (b) first layer, (c) Layer 6, (d) last layer6Under review as a conference paper at ICLR 2021Table 2: Top-1 and Top-5 accuracy of VGG-16 after layer-wise optimizationWeight	Activation	Top-1 (∆)	Top-5 (∆)-FP32-	FP32	71.59%	90.38%(1,3,4,7)	(1,4,3,7)+(0,4,4,7)	71.19% (-0.40%)	90.14% (-0.24%)(1,3,4,7)	(1,3,4,6)+(0,4,4,7)	71.38%(-0.21%)	90.33% (-0.05%)(1,2,5,3)	(1,3,4,6)+(0,4,4,7)	71.24% (-0.35%)	90.22% (-0.16%)(1,2,5,*)	(1,3,4,6)+(0,4,4,7)	71.48%(-0.11%)	90.27% (-0.11%)(1,2,5,*)	(1,3,4,6)+(0,4,4,*)	71.48%(-0.11%)	90.32% (-0.06%)Table 3: The FFP8 format of each layer in VGG-16 after layer-wise optimizationLayer	Weight	Activation	Layer	Weight	Activation1	(1,2,5,3)	(1,3,4,6)	8	(1,2,5,5)	(0,4,4,8)2	(1,2,5,4)	(0,4,4,11)	9	(1,2,5,5)	(0,4,4,8)3	(1,2,5,4)	(0,4,4,10)	10	(1,2,5,6)	(0,4,4,8)4	(1,2,5,5)	(0,4,4,10)	11	(1,2,5,5)	(0,4,4,7)5	(1,2,5,4)	(0,4,4,9)	12	(1,2,5,5)	(0,4,4,7)
Figure 5: Activation distributions of (a) whole model, (b) first layer, (c) Layer 6, (d) last layer6Under review as a conference paper at ICLR 2021Table 2: Top-1 and Top-5 accuracy of VGG-16 after layer-wise optimizationWeight	Activation	Top-1 (∆)	Top-5 (∆)-FP32-	FP32	71.59%	90.38%(1,3,4,7)	(1,4,3,7)+(0,4,4,7)	71.19% (-0.40%)	90.14% (-0.24%)(1,3,4,7)	(1,3,4,6)+(0,4,4,7)	71.38%(-0.21%)	90.33% (-0.05%)(1,2,5,3)	(1,3,4,6)+(0,4,4,7)	71.24% (-0.35%)	90.22% (-0.16%)(1,2,5,*)	(1,3,4,6)+(0,4,4,7)	71.48%(-0.11%)	90.27% (-0.11%)(1,2,5,*)	(1,3,4,6)+(0,4,4,*)	71.48%(-0.11%)	90.32% (-0.06%)Table 3: The FFP8 format of each layer in VGG-16 after layer-wise optimizationLayer	Weight	Activation	Layer	Weight	Activation1	(1,2,5,3)	(1,3,4,6)	8	(1,2,5,5)	(0,4,4,8)2	(1,2,5,4)	(0,4,4,11)	9	(1,2,5,5)	(0,4,4,8)3	(1,2,5,4)	(0,4,4,10)	10	(1,2,5,6)	(0,4,4,8)4	(1,2,5,5)	(0,4,4,10)	11	(1,2,5,5)	(0,4,4,7)5	(1,2,5,4)	(0,4,4,9)	12	(1,2,5,5)	(0,4,4,7)6	(1,2,5,5)	(0,4,4,9)	13	(1,2,5,6)	(0,4,4,8)7	(1,2,5,4)	(0,4,4,9)			
Figure 6: Memory-efficient system architectures: off-chip external data (a) in BFP16, (b) in FFP8;and (c) an FFP8 to FP32 hardware converterIn addition to image classification, we also want to know whether the proposed FFP8 format per-forms equally well in other application domains. Here, we examine two more applications: semanticsegmentation and ECG check. First, FCN32s is a popular CNN model for semantic segmentation(Long et al., 2014). If FCN32s is in FP32, the mIOU is 63.63% using the VOC2011 dataset (Ever-ingham et al.). Alternatively, if all the weights are in FFP8(1, 4, 3, 14) and all the activations are inFFP8(1, 4, 3, 2), the resultant mIOU would be 63.45%, a slight drop of 0.18%. Second, an LSTMmodel for ECG check (Physionet, 2017), has also been tested. If the LSTM model is in FP32, thecheck accuracy is 81.12%. If all the weights are in FFP8(1, 3, 4, 6), the first-layer activations are inFFP8(1, 3, 4, 5), and activations in the other layer are in FFP8(1, 4, 3, 16), the resultant accuracywould be 81.53%, an accuracy gain of 0.41%. The experimental results once again demonstrate thatFFP8 performs very well in these two categories as well.
