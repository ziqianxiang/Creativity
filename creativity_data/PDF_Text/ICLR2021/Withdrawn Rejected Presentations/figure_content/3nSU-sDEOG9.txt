Figure 1: The overall framework of a RL agent equipped with ESCE. Agents receive environmental rewardsand calibrated signals from ESCE as its total rewards. The policy network and ESCE are two independentmodels. The ESCE examines each state and provides calibrated rewards when the state is considered as anempirical sufficient state. Within the ESCE training process, state vectors are automatically labeled with ourproposed novel labeling method, and are then stored in corresponding pools. The ESCE network is updatedwith overfitting training, where phase one is a binary classification with data from pools and Rnegative pools,and phase two is the proposed overfitting training updated with Rnegative data alone.
Figure 2: 2(a) A schematic diagram of decision boundary after phase-one overfitting training: the classifierwill try to make most of the samples correctly distributed on both sides of the boundary. However, there arestill quite a few promiscuous samples falsely allocated due to label ambiguity. 2(b) A schematic diagram ofdecision boundary after phase-two overfitting training: all state vectors labeled with Rnegative are excludedfrom Rnegative boundary, which matches the definition of ESD-policy. 2(c) A schematic diagram of sensitiveSampling: the actual ESD-policy may change with the policy’s updating. The state vectors inside the grayregion were insufficient states, and then turn into empirical sufficient states. The ESCE is primarily updatedwith these dynamic samples.
Figure 3: 3(a) The calibrated rewards and environmental rewards are encoded in red and yellow, and the valueestimation (Critic) trained with calibrated rewards and environmental rewards are encoded in green and red,respectively. The blue line displays that ESCE identifies a empirical sufficient state when the agent hit thepellet with the edge of the bat (right bat), which significantly increases transverse velocity for the agent to winthe game, and makes it unsolvable for its opponent. However, the value of baseline (red) increases continuouslyuntil receiving the reward returned by the environment, which is far away from the decisive state. To the bestof our knowledge, prior approaches could not make such precise prediction (blue) on critical states. 3(b) InFishingDerby-v0, most states are identified when the agents’ hook close to fishes or a fish is already hooked.
