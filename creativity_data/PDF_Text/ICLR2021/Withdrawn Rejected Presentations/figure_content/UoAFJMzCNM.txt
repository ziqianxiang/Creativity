Figure 1: SDFP framework for N Players. Each NN block has the architecture in Fig. 11.
Figure 2: Sample efficiency between FBSDEframework w/ and w/o invariant layer.
Figure 3: Comparison of SDFP and analytic solution for the inter-bank problem. Both the state (left)and control (right) trajectories are aligned with the analytic solution (represented by dots).
Figure 4: Time and memory complexity comparison between batch, iterate and invariant layer+batchimplementations. Time complexity is measured by per-iteration time.
Figure 5: RSE and total loss comparison between our FBSDE framework and that of baseline Han& Hu (2019)experiments hereon correspond to the symmetric SDFP implementation in Algorithm 1 We alsotest sample efficiency and generalization capability of the invariant layer on a 50-agent problemtrained over 100 stages. The number of initial states is limited during the training and the evaluationcriterion is the terminal cost of the test set in which the initial states are different from the initialstates during training. Fig. 2 showcases the improvement in sample efficiency and generalizationperformance of invariant layer. We suspect this is due to the network needing to learn with respectto a specific permutation of the input, whereas permutation invariance is built into the networkarchitecture with invariant layer.
Figure 6: RSE and total loss trajectory comparison between our FBSDE framework and that of Han& Hu (2019) w/ and w/o invariant layer for 500 agents.
Figure 7: RSE and training loss trajectory comparison between our FBSDE framework and theextension of Pereira et al. (2019) w/ and w/o invariant layer for 500 agents.
Figure 8: Terminal time step state X and control U distribution of ith agent for linear and superlin-ear dynamics.
Figure 9: One belief space racing trajec-tory. The solid line represents the meanand the circles represent the variance.
Figure 10: Invariant layer architecture.
Figure 11: FBSDE Network for a Single Agent. Note that the same FC is shared across all timesteps.
Figure 12: Superlinear dynamics comparison between baseline and our algorithmF InterbankBy pluging the running cost to the HJB function, one can have,Vi,t + inf	Xla(X -	Xj ) +	u2]Vxj	+ uui	- qui (X -	Xi)	+	景X	- Xi)I2	+ui∈Ui	«	2	2	∕C1∖Lj=I	_|	(21)2tr(Vxx,i^^T) = 0∙By computing the infimum explicitly, the optimal control of player i is:ui (X, t) = q(X — Xi)-Vχ,i(X, t). The final form of HJB can be obtained asVi,t + 2tr(Vxx,i ςςT) + a(X - Xi)Vx,i + £[a(X - Xj ) + uj ]Vx,jj=i	(22)+ 2(X - Xii))- 2(q(X - Xi)- Vx,i)2 = 0Applying Feynman-Kac lemma to equation 22, the corresponding FBSDE system isdX(t) = (f (X(t), t) + G(X(t), t)u(t))dt + Σ(t, X(t))dWt,	X(0) = xodVi = -[∣(X - Xi)2 - 1(q(X - Xi) - Vx,i)2 + Ui]dt + VT∑dW,	V(T) = g(X(T)).
Figure 13: 2 car racing problem with 8 second time horizon.
Figure 14: The gradient path of FBSDE model w/ and w/o importance sampling. The figure on theleft is FBSDE with importance sampling and the figure on the right is FBSDE without importancesampling. One can identify that the framework with importance sampling would lead to long chainof gradient.
Figure 15: The training loss and RSE of LSTM backbone FBSDE architecture w/ and w/o impor-tance samplingWhere η(t), μ(t) are two scalar functions. The optimal control under this ansatz is:a?(t, X )= q + η(t)(1 - 9) (X - Xi)	(26)By pluginging the ansatz into HJB function derived in 22, one can have,η⑴=2(a + q)η(t) + (I -1)η2(t) - (e - q2),η(T) = c,1	1N 2	(27)μ(t) = - 2σ2(1-ρ2)(1- -)η(t),μ(T) = 0.
Figure 16: The Total loss and RSE of 1000 agents simulationwith 1E-3 learning rate for all simulations.
