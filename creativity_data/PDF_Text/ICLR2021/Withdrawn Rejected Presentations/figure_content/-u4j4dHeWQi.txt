Figure 1: An illustration for motivation of GSRL.
Figure 2: GSRL allows joint optimization of map construction and agent exploration from scratch.
Figure 3: An illustration of exploration strategy.
Figure 4: Illustrations of optimal goals. States surrounded by red circles and connected by red arrowsare optimal solution path, which can be obtained by the shortest path planning algorithm on the fullyexplored graph Gfull in (a). When the graph is not fully explored (G0e in (b)), we generate the optimalsolution path hindsight, where we regard the state with the highest value in the next episode (G0e+1 in(c)) as the target state, and shortest path to it as the optimal solution path (Pe+1 = hs0, s1 , s3, s6, s9iin (d)). We define the optimal goal g* as the reachable target state in the optimal solution path inepisode e (e.g., s10 in Gfull and s6 in G0e).
Figure 5:	Visualization of robotic manipulation environments.
Figure 6:	Learning curves of GSRL, HER, MAP, GoalGAN, and CHER on various environmentswith 10 random seeds, where the solid curves depict the mean, the shaded areas indicate the standarddeviation, and dashed horizontal lines show the asymptotic performance.
Figure 7: An illustrated example of notations in Graph Structured Reinforcement Learning (GSRL).
Figure 8: An illustrated example of Graph Structured Reinforcement Learning (GSRL).
Figure 9: An illustrated example of the proof of Proposition 1 and the key difference between theproposition in this paper and the one in the previous work.
Figure 10: An illustrated example of the relationship between optimal goal and boundary. In thefully explored graph Gfull, the red circled states together show the optimal solution path (Pfull =hs0, s1, s3, s6, s9, s10i) with terminal one (s10) for the optimal goal in (a). In any other not fullyexplored state-transition graph G0e at the beginning timestep of any episode e in (b), we regard thereachable state in the dashed line circle (s6) through planning in the next episode G0e+1 in (c) as theoptimal goal in (d).
Figure 11: An illustrated example for relationship between certainty and number of visited states.
Figure 12: Visualization of robotic manipulation environments.
