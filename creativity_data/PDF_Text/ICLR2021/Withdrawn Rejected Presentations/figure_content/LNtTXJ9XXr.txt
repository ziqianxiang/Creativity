Figure 1: Batch statistics in the first batch normalization (BN) layer of an adversarial trainedResNet18 model on CIFAR10, with and without further standard fine-tuning of BN (orange andblue lines, respectively). The running mean μ and variance σ, as well as the rescaling shift param-eter β are almost the same (overlapped in the figure), while the rescaling weight γ has a significantdifference, which has a notable contribution to the clean and robustness trade-off.
Figure 2: Illustration of (a) Adversarial Masking hypothesis, and (b) RobMask method for im-proving the generalization performance. Instead of just using a single masking for both clean andadversarial examples, we use the linear combination of k primary rescaling parameters {wj }jk=1and {wj0 }jk=1 to incorporate different perturbation strength i.
Figure 3: Training curve on DenseNet-1219Under review as a conference paper at ICLR 2021ReferencesAbien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprintarXiv:1803.08375, 2018.
Figure 4: Illustration of the Adversarial Masking effect. We mark several feature maps (red andgreen boxes) are blocked out or magnified when comparing (a) and (b), which can be viewed as aselection mask on “non-robust” and “robust” features.
