Figure 1: A Comparison between hierarchical convolutions and convolutions with ARF. Left: GCNswith ARFs better focus on critical nodes and filter out noises in large neighborhoods. Right: ARFsmore efficiently explore long-distance dependencies.
Figure 2: The general architecture of GRARF. Red and blue arrows together demonstrate the trainingprocess. Blue arrows along demonstrate the prediction process.
Figure 3: Examples and results of two designed experiments. (a)(b) The denoising experiment. (c)(d)The long-distance dependency experiment. GRARF displays robustness to noises in both experiments,whereas DerformanCes of vanilla GCNs and GATs dro≈ã dramaticallyFigure 4: Analysis of behaviors in GAT and GRARF. (a) A box-plot of attention weights on corawith regard to node degrees, where medians, Q1s, Q3s and a 95% intervals are displayed. (b)-(d)Histograms of attention weights assigned to true nodes and noises in the denoising experiments(r = 1), with different node degrees (d = 2, 5, 10). (e) The ratios of noises in constructed ARFs withregard to node degrees, in the denoising experiments (r = 1).
Figure 4: Analysis of behaviors in GAT and GRARF. (a) A box-plot of attention weights on corawith regard to node degrees, where medians, Q1s, Q3s and a 95% intervals are displayed. (b)-(d)Histograms of attention weights assigned to true nodes and noises in the denoising experiments(r = 1), with different node degrees (d = 2, 5, 10). (e) The ratios of noises in constructed ARFs withregard to node degrees, in the denoising experiments (r = 1).
