Figure 1: Existing hand-designed robust losses and our meta-learned robust loss. Left: ConventionalCross-Entropy (CE), Mean Absolute Error (MAE) (Ghosh et al., 2017), and label-smoothing (Pereyraet al., 2017). Middle: Generalised Cross Entropy (GCE) (Zhang & Sabuncu, 2018), Symmetric CrossEntropy (Wang et al., 2019). Right: Our learned loss.
Figure 2: t-SNE visualisation of penultimate layer Resnet18 features after learning on CIFAR-10with 40% symmetric label noise. Left: model trained by CE. Middle: model trained by FW. Right:model trained by Our learned loss.
Figure 3: Generalisation of learned loss to varying noise-levels. Left: 2MLP-MNIST, Middle:4CNN-MNIST, Right: VGG11-CIFAR10.
