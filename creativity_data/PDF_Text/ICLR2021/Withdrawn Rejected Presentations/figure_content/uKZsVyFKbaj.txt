Figure 1: An example of an 8 × 8 cell board of Life over six time steps, evolving in time from left toright. White pixels are considered alive and black pixels are considered dead.
Figure 2: Neural network architecture diagrams for the 1-step minimal model and L(n, m). On theleft, the 1-step minimal architecture consists of an input layer that feeds into a convolutional layerwith two 3 × 3 filters with ReLU activation, then into a convolutional layer with one 1 × 1 filter withReLU activation, fed into a similar convolutional layer but with sigmoid activation for decoding. Onthe right, the architecture for L(n, m) consists of the same as the minimal model, except where thefirst two hidden layers consist of 2m and m filters respectively, and are repeated n times, where m isthe factor of overcompleteness (see text).
Figure 3: Measured probability that the m times overcomplete n-step-Life architecture learnssuccessfully. Each line corresponds with a particular n. Each point plots the percentage of 64instances of L(n, m) with random initializations sampled from a unit normal distribution that learnedthe rules of n-step-Life after 1 million training examples. We train instances of L(n, m) for values1 ≤ n ≤ 5 and 1 ≤ m ≤ 24, excluding many combinations due to computational constraints. Forn > 1, none of the instances of L successfully learned with the minimal(m = 1) architecture. As nincreases, the degree of overcompleteness required for consistent converges increases rapidly.
Figure 4: First five graphs: Binary cross-entropy loss over the duration of training of the 64 networkstrained to solve the n-step-Life problem for 1 ≤ n ≤ 5. The horizontal axis corresponds to number ofepochs of training. For clarity, we omit from the graphs the loss of networks that eventually divergedto a degenerate state where the networks predicted all cells to be dead, regardless of the input Lifeconfiguration. These examples converge on a loss that is well over 1. Last graph: Average earliestpoint of convergence of L(n, m) for m = 8 and 1 ≤ n ≤ 4. Note that n = 5 is excluded because noinstances of L(5, 8) converge. We compute the earliest point of convergence for each network byobserving the first epoch where the loss falls below 0.01, indicating that the network has reached astable 100% accuracy.
Figure 5: Left: Fraction of converged L(1, 1) networks with weights initialized with a k-signperturbation of a converged solution to the 1-step-Life problem. Center: Same as left except weightsinitialized with k-sign perturbation of the original initial weights that converged to the solution. Right:Same as center except weights initialized with a uniform perturbation of the original initial weights.
Figure 6: Top: Fraction of minimal networks that converge to a solution to the 1-step-Life problem(left and center) and 2-times overcomplete networks (right) when trained with datasets of agiven d-density dataset. The left and center graphs refer to the same training configurations,however, the left graph includes the rate of convergence for datasets with d between 0.1 and0.9 with 0.05 intervals while the center graph has data for d between 0.2 and 0.5 with 0.0125 intervals.
