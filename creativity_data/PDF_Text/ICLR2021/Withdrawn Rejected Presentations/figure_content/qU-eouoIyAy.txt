Figure 1: The mapping between sensory stimuli (left) and fMRI recordings (right). Neural encodingseeks to find a transformation from stimulus to the observed brain response via a latent representation(middle). Conversely, neural decoding seeks to find the information present in the observed brainresponses by a mapping from brain activity back to the original stimulus.
Figure 2: Schematic illustration of the HYPER framework. Face images are generated from randomlysampled latent features z âˆˆ Z by a face-generating GAN, as denoted by the dotted box. These facesare then presented as visual stimuli during brain scanning. Next, a linear decoding model learnsthe mapping from brain responses to the original latent representation, after which it predicts latentfeatures Z for unseen brain responses. Ultimately, these predicted latent features are fed to the GANfor image reconstruction.
Figure 3: Voxel mask: 4096 most active voxelsbased on highest z-statistics within the averagedz-map from the training set responses, resulting ina distributed network of activity.
Figure 4: Semantic face editing of a stimulus im-age from the testing set (number 7) by varying thelatent vector along a separation boundary.
Figure 5: Results of the five models that predict latent vectors from brain data. The highest per-formance is achieved by our first model which is trained on latent representations alone and nointermediate layer activations. A. Four testset stimuli (left) and their five corresponding modelreconstructions. B. The average latent similarity, feature similarity, and structural similarity (Y axis)of the five models (X axis). C. Five graphs display the Pearson correlation coefficients (Y axis)between true and predicted semantic feature scores of the five models (X axis) for each visual feature.
Figure 6: Results of model 0 that is trained on only the latent vectors. Here, we display testing setsamples 1-18 for Subject 1 and 19-36 for Subject 2. Image reconstructions (left) versus perceivedimages (right). Interpolations visualize similarity regarding the underlying latent representations.
Figure 7: Qualitative results of our approach compared to (21) and the eigenface approach inreconstructing image 26, 28, and 36 (arbitrarily chosen). The model columns display the best possibleresults. For (21), this displays reconstructions directly decoded from the 1024-dimensional latentrepresentation of this method. For the eigenfaces approach, this shows reconstructions directlyobtained from the 512 principal components.
Figure 8: A. Layer activations that corresponded most with each voxel are mapped on the brain oftwo subjects respectively. In pink, borders are indicated between the primary visual cortex, the earlyvisual cortex, the dorsal stream visual cortex, ventral stream visual cortex, and MT+ complex andneighboring visual areas, as based on (4). B. Distribution of layers assigned to voxels across differentregions of interest. C. Activation similarity of different layers.
