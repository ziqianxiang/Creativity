Figure 1: Theory and meta-learned exploration in a two-arm Gaussian bandit. Left: Bayes optimalexploration behavior for different lifetimes and across uncertainty conditions σl , σp. Right: Meta-learned exploration behavior using the RL2 (Wang et al., 2016) framework. There exist two behavioralregimes (learning by exploration and a heuristic non-explorative strategy) for both the theoreticalresult and the numerical meta-learned behaviors. The amount of meta-learned exploration is averagedboth over 5 independent training runs and 100 episodes for each of the 400 trained networks.
Figure 2: Bimodality of the reward landscape. Upper row. The Bayesian model predicts a bimodaldependence of the expected return on the policy. Parameters for lifetime T = 100 in the non-learning(left) and learning regime (right) and close the transition edge (middle). Bottom row. Distribution ofthe mean number of explorative pulls in 1000 separatively trained network with different randomseeds. Close to the edge, the networks fall into two classes: networks either abandon all exploration(peak at n = 0) or explore and learn. Away from the transition, all 1000 networks adopt a similarstrategy.
Figure 3: Recurrent dynamics of two meta-trained bandits for task conditions that favour learning(blue) and not learning (red), for a lifetime T = 100. Each bandit is tested both for the case wherethe deterministic arm is the better option and for the case where the stochastic arm is the better option.
Figure 4: Grid navigation task with 3 different rewards, which differ in the amount of reward and theuncertainty in location. Top-Left: Task formulation. Bottom-Left: Learning curves (T test = 100)for different training lifetime. We plot the median, 25th and 75th percentile of the return distributionaggregated over 10 independent training runs and 10 evaluation episodes. Agents trained on a shorterlifetimes gradually generalize worse to the larger test lifetime. Right: Relative state occupancy ofmeta-learned exploration strategies for different training lifetimes during meta-learning (averagedover 100 episodes of length 100). With increasing lifetime the agent explores larger parts of the statespace and actively avoids suboptimal object location transition.
Figure 5: Lifetime dependence of the performance and the visitation counts of the goal locationsfor an RL2 agent trained on random 6 × 6 grid worlds and evaluated on T test = 100. For smalllifetimes the meta-learned strategy only exploits the small safe object. For larger lifetimes the agentfirst explores the more uncertain medium (and later high) reward object locations. The displayedstatistics (median, 25th/75th percentile) are aggregated over 5 independent training runs and 500evaluation episodes.
Figure 6: Episode return for agentstrained on T train and tested on Ttest.
Figure 7: Characteristic trajectories for three different types of meta-learned strategies. Top tobottom: Episode rollouts (T test = 100) for inner loop training lifetimes T train = {10, 20, 50}.
Figure 8: Two-arm Gaussian bandit. trials n of the overall lifetime T exploring the second stochasticarm. This assumption is justified since our correspondingmeta-learning agent may easily encode the deterministic natureof the fixed arm 0 and therefore only explore the second non-deterministic arm.2 The expectedcumulative reward of such a two-phase exploration-exploitation policy can then be factorized asfollows:Erμ,rTXt=1Erμ,r+ Eμ,r	E rt n,μt=n+1(-1) × n + Eμ [(T - n)p(μ > 0)μ]where μ denotes the maximum a posteriori (MAP) estimate of μ after n trials:μ = -5— [-1 × Pp + n × Pl × r].
Figure 9: Learning to learn and not to learn. Left to right. The meta-learned exploration strategyis visualized for different increasing checkpoints throughout meta-learning (T = 100). The initialbehavior is random, but as training progresses two distinct regimes (adaptive versus heuristic) emerge.
