Figure 1: To reference the node at the bottom,we use the notation np with p = {1, 3, 2}. Weuse curly brackets {} to write a path, and anglebrackets〈•〉for the concatenation of paths.
Figure 2: (Left) Example of hyper-planes wp, formed through the sum of δp . The hyper-planeW{1,3,2,1} associated to the class n{i,3,2,i} is in green, the construction with the δ's in blue, andall intermediate w in red. (Right) Riemannian versus “projected” gradient descent. Riemannianoptimization follows approximately geodesics, while projected gradient steps can jump very far fromThis can be avoided, for example, by deploying the regularization parameter (or weight decay)independently for each kδp k. However, it is costly in terms of hyper-parameter estimation.
Figure 3: Illustration of a Riemanian gradient step on a manifold M. In blue, the projection operatorfrom the ambient space (in our case, Rd) to the tangent space Txk M. This maps the standard gradientof the function, Vf, to its Riemannian gradient gradf. Then, in red We have the retraction thatmaps vector from the tangent space Txk M to the manifold M. This converts the gradient stepxk - hgradf (xk) (that belongs to the tangent space) to xk+1 (that belongs to the manifold).
Figure 4: Visualization of 2-dimensional embedding vector. We added the fully connected layer (Rm×2 ,e.g., for ResNet18, m = 512) prior to the last FC layer. (a) Baseline, (b) Baseline (Multitask), and proposedparameterization ((c) Hierarchy, (d) +Manifold, and (e) +Riemann).
Figure 5: Visualization of a high dimensional embedding vector using t-SNE on 2D plane. (a) Baseline,(b) Baseline (Multitask), and proposed parameterization ((c) Hierarchy, (d) +Manifold, and (e) +Riemann).
Figure 6: Visualization of a high dimensional embedding vector on 2D plane using PCA. (a) Baseline (b)Baseline (Multitask), and proposed parameterization ((c) Hierarchy, (d) +Manifold, and (e) +Riemann).
