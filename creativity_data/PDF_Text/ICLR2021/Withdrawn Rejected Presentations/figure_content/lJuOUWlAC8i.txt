Figure 1: Commonsense question answering augmented with external graph knowledge. Underlinedwords and phrases are recognized concepts. To correctly answer this question, it's desirable that the modelhas access to commonsense knowledge like (print, Requires, use paper), (paper, HasProperty,expensive), which is not presented in the context. This calls for the integration of contextualized common-sense knowledge.
Figure 2: Architecture of a typical neural-symbolic model for commonsense reasoning.
Figure 3: Overview of our HGNâ€™s graph module. We jointly learn the graph structure and networkparameters. Darkness of edges indicate their weights. Red variables are updated in the previous step.
Figure 4: Ablation studies. (a)(b) Performance of HGN and baseline models with different amountof training data; (c) Performance of different model variants.
Figure 5:	Case I: Unrelated extracted facts are filtered out.
Figure 6:	Case II: Helpful generated facts are incorporated.
