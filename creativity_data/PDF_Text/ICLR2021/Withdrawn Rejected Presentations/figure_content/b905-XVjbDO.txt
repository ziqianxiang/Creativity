Figure 1: An illustration of a ReLU layer N :R2 → R3, x = N (z), that is not globally injec-tive. Differently colored regions in the z-space aremapped to regions of the same color in the x-space.
Figure 2: Illustration of the DSS definition. Left: A configuration of 4 vectors in R2 that do not havea DSS w.r.t. all x ∈ R2. In this case, the vectors do not generate an injective layer. The set of labelsindicate which wj have positive inner product with vectors in the wedge; there are two wedges withonly one such {wj}; we have ReLU(W x1) = ReLU(Wx2). Center: A configuration where fourvectors have a DSS for all x ∈ R2 . These vectors correspond to a minimally-expansive injectivelayer; see Corollary 2. Right: A plot of kReLU(W x)k1 where W is given as in the left figure. Notethat x 7→ ReLU(W x) is linear in every wedge.
Figure 4: A visualization of the indicesin (8) in two dimensions. The blue re-gion is a kernel C of width O, the pinkregion is the zero-padded box of widthP , and D is the offset of the kernel Cin the pink box. The entire signal is thegreen box of width N .
Figure 5: An illustration of an injective deepneural network that avoids expansivity as de-scribed by Corollary 3. White trapezoids areexpansive weight matrices satisfying Theo-rem 1, and the blue trapezoids are randomprojectors that reduce dimension while pre-serving injectivity.
