Figure 1: Convergence plot for the Sphere and Ill-Conditioned Sphere with 30 training variablesand 32 hyperparameters, with varying algorithms and JD with varying gradient probabilities. Notethat Joint Descent significantly outperforms the traditional algorithms that alternate between trainingand tuning and thereby produce a sawtooth-like training curve. Furthermore, the optimal gradientprobability is around 0.5, even when the number of hyperparameters is 32.
Figure 2:	Objective convergence plot for Accelerated Joint Descent with the same setting as Fig 1.
Figure 3:	Training accuracy for Joint Descent for the MNIST dataset with hyperparameters: learningrate, ReLU slope, eLU cofficient. The ReLU slope is set to 1, implying a linear activation, but themodel learns to set the ReLU slope to be close to 0 to achieve competitive performance.
