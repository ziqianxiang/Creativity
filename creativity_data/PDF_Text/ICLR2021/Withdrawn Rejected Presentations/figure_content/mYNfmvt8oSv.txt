Figure 1: Visual illustrations of the proposed dense-ConneCtions based D2RL modification to the policy ∏φ(∙)and Q-ValUe Qθ(∙) neural network architectures. The inputs are passed to each layer of the neural networkthrough identity mappings. Forward pass corresponds to moving from left to right in the figure. For state-basedenvs, st is the observed simulator state and there is no convolutional encoder.
Figure 2: The effect of increasing the num-ber of fully-connected layers to parameter-ize the policy and Q-Networks for Soft-Actor Critic (Haarnoja et al., 2018) on Ant-v2 in the OpenAI Gym Suite (Brockmanet al., 2016). It is evident that performancedrops when increasing depth after 2 layers.
Figure 3: OpenAI Gym benchmark environments with SAC. Comparison of the proposed D2RL and thebaselines on a suite of OpenAI-Gym environments. We apply the D2RL modification to SAC (Haarnoja et al.,2018). The error bars are with respect to 5 random seeds. The results on Humanoid env are in the Appendix.
Figure 4: OpenAI Gym benchmark environments with TD3. Comparison of the proposed variation D2RLand the baselines on a suite of OpenAI-Gym environments. We apply the D2RL modification to TD3 (Fujimotoet al., 2018). The error bars are with respect to 5 random seeds.
Figure 5: Challenging selected manipulation and locomotion environments. Comparison of the proposedvariation D2RL and the baselines on a suite of challenging manipulation and locomotion environments. Weapply the D2RL modification to the SAC (Haarnoja et al., 2018), HER (Andrychowicz et al., 2017), andHIRO (Nachum et al., 2018) algorithms and compare relative performance in terms of average episodic re-wards with respect to the baselines. The task complexity increases from Fetch Reach to Fetch Slide. JacoReach is challenging due to high-dimensional torque controller action space, AntMaze requires exploration tosolve a temporally extended problem, and Furniture BlockJoin requires solving two tasks- join and lift sequen-tially. The error bars are with respect to 5 random seeds. Some additional results on the Fetch envs are in theAppendix.
Figure 6: Ablation studies with a SAC agenton the Ant-v2 env in the Open AI Gym suite.
Figure 7: Complete set of experiments with HER (Andrychowicz et al., 2017) and DDPG (Qiu et al., 2019) onthe Fetch robot. Using D2RL continues to outperform the baseline on the three environments considered.
Figure 8: Illustrations of some of the challenging robotic control environments used for our experiments. InFetch slide, a Fetch robot arm with one finger must be controlled to slide a puck to a goal location. In Jackoreach, a Jaco robot with a three finger gripper must be controlled to reach the red brick. In Ant maze, an Antwith four legs must be controlled to navigate a maze. In Baxter block join-lift, one arm of a Baxter robot witha two finger gripper must be controlled to join two blocks and lift the combination above a certain height.
