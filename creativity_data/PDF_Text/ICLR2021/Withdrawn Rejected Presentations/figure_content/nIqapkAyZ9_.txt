Figure 1: Feature embeddings scattered over the 2D unit circle. In (a), the features are polarizedacross a single axis; the singular value of the principal (horizontal) axis is large while singular valueof the secondary (vertical) axis is small, respectively. In (b), the features are spread uniformly acrossboth dimensions; both singular values are comparably large. (c) depicts the PCA analysis of a toy2D Gaussian dataset to demonstrate our intuition. The principal component (green) has the highesteigenvalue, i.e., the axis with the highest variation, while the second component (red) has a smallereigenvalue. Maximizing all eigenvalues promotes data dispersion across all dimensions. In thispaper, we maximize the mean singular value to regularize the feature embedding and avoid a modelcollapse.
Figure 2: Quantitative evaluation on Stanford CARS196. X and Y-axis denote the learning rate lrand recall@1 performance, respectively.
Figure 3: The SVMax regularizer mitigates model collapse in a GAN trained on a toy 2D mixtureof Gaussians dataset. Columns show heatmaps of the generator distributions at different trainingsteps (iterations). The final column shows the groundtruth distribution. The first row shows thedistributions generated by training a vanilla GAN suffering a model collapse. The second rowshows the generated distribution when penalizing the generator’s fake embedding with the SVMaxregularizer. The third and fourth rows show two distributions generated using an unrolled-GANwith and without the SVMax regularizer, respectively. This high resolution figure is best viewed ona screen with zoom capabilities.
Figure 4: Qualitative feature embedding evaluation using the MNIST dataset projected onto the 2Dunit circle. The first row shows the feature embedding learned using a vanilla contrastive loss andthe second row applies the SVMax-regularized. A random subset of the test split is projected forvisualization purpose. Different colors denote different classes. The regularized feature embed-ding spreads out uniformly and rapidly. The supplementary material shows the feature embeddingevolves vividly up to 200 epochs. This high resolution figure is best seen on a screen.
Figure 5: Quantitative evaluation on CUB-200-2011 with various batch sizes b = {288, 72} andembedding dimensions d = {256, 64} to demonstrate the stability of our hyperparameter. λ = 1 forcontrastive loss and λ = 0.1 for triplet loss.
