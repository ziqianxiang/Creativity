Figure 1: A. Representation of the general experimental design used here and in following experi-ments. B. Samples of the six datasets used here. Leek10 and Leek2 are shown in their entirety. C.
Figure 2: A. Experimental design for Experiment 2. B. Results in terms of normalized accuracy (0is random chance, 100 is perfect performance). Standard deviation for each condition is shown inSection A.13.4	Experiment 3: Training on the whole canvas is not enoughIt is conceivable that in the previous experiments, and in similarly designed experiments in theliterature (Kauderer-Abrams, 2017; Gong et al., 2014; Chen et al., 2017; Blything et al., 2020),vanilla networks failed to show invariance to translation because they were tested on locations wherethey had not seen any items. In which case, pretrained networks succeeded not because they hadacquired the deep property of translation invariance from the visual environment, but simply becausethey had been trained on the whole canvas. To test this hypothesis we separated the canvas in 9equilateral areas (58Ã—58 pixels), and within each area, only 2 of the total classes were presented. TheFigure 3: Experimental design for Experiment 3 and results. A. After fine-tuning on MNIST, thenetwork did not generalize on untrained locations. B. When tested on the same dataset (EMNIST18),but without using classes segregation, letters were only recognised when presented on the area theywere trained on, so mean accuracy was low. In both cases, the heatmaps are averaged across allclasses.
Figure 3: Experimental design for Experiment 3 and results. A. After fine-tuning on MNIST, thenetwork did not generalize on untrained locations. B. When tested on the same dataset (EMNIST18),but without using classes segregation, letters were only recognised when presented on the area theywere trained on, so mean accuracy was low. In both cases, the heatmaps are averaged across allclasses.
Figure 4: Experimental design and results of condition 1 and 2 in Experiment 4. On Condition1, the heatmap shows a certain degree of generalization on untrained area, but not complete. OnCondition 2, the network only accurately predicted classes on the area in which they were presentduring training.
Figure 5: A. Cosine similarity across several horizontal displacements. B. Cosine similarity atdifferent stages of the experimental setup for experiments in Section 3.3. The bottom part of thefigure indicates the different stages at which the analysis is performed.
Figure 6: Average results across each repetition for Experiment 1. Error lines indicate one standarddeviation. Hatches indicate the condition in which fine-tuning had the same items as pretraining(corresponding to the diagonal in Figure 2)A.2 Training on limited translations: further testsA.2. 1 Condition 1: inter-class analysisWe showed that a CNN was only partially able to generalize on untrained locations (Section 3.5).
Figure 7: Heatmaps for each EMNIST class for the Condition 1 of Experiment 3. The network seemto be perfectly invariant to translation for some classes.
Figure 8: Condition 1 and 2 from Experiment 3. Here, instead of testing on the pretrain dataset, wefine-tuned on a new dataset (MNIST) and tested on the fully-translated version of it. We also showthe heatmap for each class, showing a discrete degree of inter-class variability and perfect translationfor some classes.
Figure 9:	Condition 3 for Experiment 3. This condition was split in 4 sub-conditions, with increasingnumber of letters trained on the whole canvas. Heatmaps are shown for each group.
Figure 10:	Cosine similarity analysis before and after 1-location fine-tuning, for each pretrainingfully-translated dataset, showing that in almost every condition the network acquired translationinvariance, but it was some times disrupted by fine-tuning.
Figure 11: Cosine Similarity value across the whole network, for several displacements, for thevanilla network and a network pretrained on ImageNet.
Figure 12: Cosine Similarity values across the whole network, at the most extreme displacement,for the vanilla network and a network pretrained on ImageNet.
Figure 13: Accuracy of a fully-connected network pretrained on fully-translated MNIST or Leek10,and fine-tuned on 1-location MNIST, Leek10, or Leek2. The network pretrained on Leek10 wasable to exhibit translation invariance for Leek2, but the other combinations failed, probably due tocatastrophic interference being more problematic for a non-convolutional network.
