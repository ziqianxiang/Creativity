Figure 1: Masked PredictiVe Coding Withfour-fold downsampletemporal downsampling rate r, Transformer encoder projectsthe output of last encoder layer to dimension Xo ∈ Rt∕r,d×r.
Figure 2: CER(%) with each layer of pre-trained Transformerencoder. Results on HKUST is pre-trained with Didi Callcenterand results on AISHELL is pre-trained with Open MandarinTable 5: MPC + APC is the model pre-trained with APC 50% oftime and MPC 50% of time. Relative Error Rates Reduction(%)is calculated with HKUST baseline without MPCPre-training Data	Objective	CER	RERR	MPC	28.0	4.56Didi Callcenter - 1K	APC	27.8	5.12	MPC + APC	27.2	7.32	MPC	27.6	5.80Didi Callcenter	APC	27.9	4.72	MPC + APC	26.8	8.46stage. Previous work on APC showed a future prediction stepof 5 gave best results on Transformer decoder [13]. We im-plemented APC with the same future prediction step and setswitching probability p to 0.5. Experiments were conducted onHKUST dataset with Transformer CTC model and the results ofdifferent pre-training objectives are presented in Table 5. Whenused alone, APC and MPC got similar improvements. Combin-
