Figure 1: Overview of Neural DNF and the proposed BOAT AlgorithmBackwardPass:The rule-based Neural DNF is particularly suited for classification tasks that require the logicalcompositions of certain concepts. We demonstrate that with experiments on a toy dataset called2D-XOR, an extension of the XOR problem which has received special interest in the literature(Minsky and Papert, 1969). We consider 2D-XOR as a minimal example to show the benefits ofNeural DNF’s vertical integration, because 2D-XOR requires a model to both learn the right high-level features (concepts) from the raw data and the right XOR function. We further apply NeuralDNF to image datasets in two scenarios: In the first scenario, we use a regular deep network asfeature extractor with no further constraints. We show that Neural DNF can successfully learn boththe feature extractor and the logical rules for classification, and achieve competitive accuracy. Notethat in this case there is no guarantee that the extracted features are meaningful to human. In thesecond scenario, we constrain the feature extractor to produce human-aligned interpretable featuresby enforcing an auxiliary concept loss based on human concept annotations. In this scenario, NeuralDNF becomes highly interpretable that the interpretability enables human to easily interact with andmanipulate the learned model, such as performing human-intervention on the extracted features toimprove accuracy, or slightly tweaking the model to recognize an imaginary class that does not existin the dataset. In conclusion, our experiments show that Neural DNF achieves accuracy comparablethat of blackbox deep learning models while offering an interpretable symbolic DNF representation,
Figure 3: Evaluating the faithfulness of explanations on test setvance scores. Alternatively, for DNN we can also utilize some representitive post-hoc interpretationmethods (also known as attribution methods): Guided Backprop (GB) (Springenberg et al., 2014),3We do provide an anecdotal example on the explanations Neural DNF can derive in (appendix fig 4) asa direct comparison to the explanations provided by linear models (e.g., the MNIST example from the self-explaining network (Melis and Jaakkola, 2018)) and explains the benefits of a symbolic DNF.
Figure 4: (top) illustration ofa correct prediction of Neural DNF for class ‘Chestnut sided Warbler’;(middle) illustration of human interaction: an incorrect prediction of Neural DNF for class ‘Chestnutsided Warbler’ can be corrected by human intervention; (bottom) illustration of manipulating alearned Neural DNF to classify an imaginary class ‘Blue-crown Chestnut sided Warbler’.
