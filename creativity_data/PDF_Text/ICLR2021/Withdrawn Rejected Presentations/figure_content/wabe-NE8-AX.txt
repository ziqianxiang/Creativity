Figure 1: Computing a vector-Fisher-vector product v>Fv, for a 10-fold classification model de-fined by model, can be implemented with the same piece of code for 2 representations of the FIMusing NNGeometry, even if they involve very different computations under the hood.
Figure 2: Schematic description of NNGeometry’s main componentstraining or a test set, called the Gram Matrix (Kw)ij = kw (xi, xj). Note that in the case where theoutput space is multidimensional with dimension c, then Kw is in fact a 4d tensor.
Figure 3:	Residual 丁q"2 and cos angle between V and v0 = (FpprOx + λI) (F + λI) V fora 24M parameters Resnet50 at different points during training on TinyImagenet, using differentapproximations Fapprox of F, for V uniformly sampled on the unit sphere (higher is better).
Figure 4:	Cos angle between FV and FapproxV fora 24M parameters Resnet50 at different points dur-ing training on TinyImagenet, using different approximations Fapprox of F, for V uniformly sampledon the unit sphere (higher is better).
Figure 5: Relative difference between v>Fv and v>Fapproxv for a 24M parameters Resnet50 atdifferent points during training on TinyImagenet, using different approximations Fapprox of F, for vuniformly sampled on the unit sphere (higher is better).
Figure 6: Relative difference of trace computed using Fapprox and F (lower is better). As we observe,all 3 representations PMatDiag, PMatQuasiDiag and PMatEKFAC estimate the trace very ac-curately, since the only remaining fluctuation comes from Monte-Carlo sampling of the FIM. On theother hand, the estimation provided by PMatKFAC is less accurate.
Figure 7: NTK analysis for 50 examples of class c1 and 50 examples of class c2 at various points dur-ing training. (top row) Gram matrix of the NTK. Each row and column is normalized by 1diag (G)for better visualization. We observe that the NTK encodes some information about the task later intraining, since it highlights intra-class examples. (bottom row) Examples are projected on the 1st 2principal components of the Gram Matrix at various points during training. While points are merelymixed at initialization, the NTK adapts to the task and becomes a good candidate for kernel PCAsince examples become linearly separable as training progresses.
