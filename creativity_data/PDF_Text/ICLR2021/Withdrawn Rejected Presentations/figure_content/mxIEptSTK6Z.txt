Figure 1: Three different types of Measurement-Importance (a) EM-EI (Edge Measurement, EdgeImportance): Weight importance is calculated based on weight measurement. (b) EM-NI (EdgeMeasurement, Node Importance): Neuron importance is calculated based on weight measurement.
Figure 2: Classification accuracy of continual learning on Split Cifar10. SI (Zenke et al., 2017),MAS (Aljundi et al., 2018a) and UCL (Ahn et al., 2019) show critical changes in their performanceas the order of tasks changes.
Figure 3: Distribution of Weight Importance. Note that range of weight importance value is normal-ized by dividing the standard deviation value into the average activation value. This is based on thefirst task of split CIFAR 10.(task order: 2-0-1-3-4)(a) Average Activation Value based weight importance distributionfor the 6 layers). On the other hand, our proposed regularization loss term assigns 26% of totalimportance to layer 1. Furthermore, our method avoids assigning excessive importance to certainlayer(26%, 16%, 16%, 15%, 15%, 12%).
Figure 4: Results on Split MNIST benchmark. Here, VCL indicates VCL(without coreset)(Nguyenet al., 2017).
Figure 5: LA accuracy on Split CIFAR10.
Figure 6:	TA accuracy on Split CIFAR10As Figure 5 shows, our method outperforms all other methods with large margins. Also, 5b showsthat our algorithm is more robust to the order of tasks. In Figure 6, we achieve overall highestaccuracy than others. Accuracy drop in TA accuracy measurement of our method is natural because6Under review as a conference paper at ICLR 2021weight importance based methods inevitably increase the number of high importance weights loos-ing their plasticity gradually. On the contrary, other compared methods show increasing accuracy. Itlooks like that their plasticities are relatively high, however, it implicitly also indicates that their sta-bilities are quite limited possibly because weight importance based continual learning is not workingappropriately. Note that TA accuracy of task1 in Figure 6a for other methods are relative quite lowcompared to TA accuracy of task5. Proposed method shows better stability in the order of tasksand also has a low degree of forgetting. In our method, average degraded degree of performance islowest as 1.23%, whereas SI is 18.06%, UCL is 7.35%, and MAS is 22.89% .
Figure 7:	Degree of Interference on Split CIFAR10. It is calculated by each task’s first learnedaccuracy - accuracy after learning the last task3.3 SPLIT CIFAR10-100We evaluate our method on Split CIFAR10/100 benchmark where each task has 10 consecutiveclasses. We use the same multi-headed setup as in the case of Split CIFAR10. We train our networkfor 100 epochs with α = 0.5. We fix task 1 as CIFAR10 due to the difference in the size of datasetbetween CIFAR10 and CIFAR100. The order of task 2 to task 11 that consists of CIFAR100 israndomly shuffled. Our algorithm shows better stability showing best accuracy values in old tasks.
Figure 8: LA accuracy on CIFAR10-100.
Figure 9: TA accuracy on CIFAR10-100.
Figure 10: Degree of Interference on Split CIFAR10-100. It is calculated by each task’s first learnedaccuracy - accuracy after learning the last task.
Figure 11: The performance on CIFAR10 and CIFAR10-100 with doubled channel. Accuracy in-creases when we use a doubled channel network.
Figure 12: Concept of Evaluation.
Figure 13: Comparison Performance on Split CIFAR10. Taks Order: T5 → T4 → T3 → T2 → T1.
Figure 14: Comparison Performance on Split CIFAR10 with Ours and UCL. Taks Order: T5 →T4 → T3 → T2 → T1. Note that both performance of Ours and UCL increase when weight re-initialization is applied.
Figure 15: Weight Importance Maps of Continual Tasks11Under review as a conference paper at ICLR 2021Figure 16: Another representation of split cifar10 LA accuracy. To describe both accuracy and stan-dard deviation at once, we change the form of graph. However, it is difficult to indicate differenceof accuracy and standard deviation.
Figure 16: Another representation of split cifar10 LA accuracy. To describe both accuracy and stan-dard deviation at once, we change the form of graph. However, it is difficult to indicate differenceof accuracy and standard deviation.
Figure 17: Representation on each task’s data with the model learned the last task.
Figure 18: Representation on Task 1 data as learning step progresses.
Figure 19: Normalized Weight importance distribution of each convolution layer. 19a representsnormalized average of weight importance of each convolution layer. 19b indicates normalizedaverage/std of weight importance of each convolution layer. Our method relaxes the tendency toconsolidate weights of earlier layers. This is based on the first task of split CIFAR 10.(task order:2-0-1-3-4)9 7 5 3 19 9 9 9 9(％)AωEJn3<Taskl Task2 Task3 Task4 Task5 Task6 Task7 Task8 Task9 TasklOTask Order■ SI →-MAS UCL - EWC →∙VCL -BBGD →-OURSFigure 20: Permuted MNIST Average Accuracy. Due to the lack of memory capacity, we evaluateVCL using multi-layer peceptrons with two hidden layers with 100 ReLU activations.
Figure 20: Permuted MNIST Average Accuracy. Due to the lack of memory capacity, we evaluateVCL using multi-layer peceptrons with two hidden layers with 100 ReLU activations.
Figure 21: LA accuracy on Tiny ImageNet.
Figure 22:	TA accuracy on Tiny ImageNet.
Figure 23:	Degree of Interference on split Tiny ImageNet. Itis calculated by each task’s first learnedaccuracy - accuracy after learning the last task.
