Figure 1: Effect of over-parameterization on training of constrained normalizing flow on mixture ofGaussian dataset for number of hidden layers m = 1600, 64003	ExperimentsFull details of experimental setup and additional results on constrained normalizing flow as well asresults on unconstrained normalizing flow are given in appendix G.
Figure 2:	Comparison of data distribution and generated data for mixture of Gaussian and betadistributionsFigure 3:	Effect of over-parameterization on training of unconstrained normalizing flow on mixtureof Gaussian and mixture of beta distributions39Under review as a conference paper at ICLR 2021Figure 4: Effect of over-parameterization on training of small sized constrained normalizing flowG.2 Results for constrained normalizing flowIn Sec. 2.1, we suggested that high overparameterization may adversely affect training for constrainednormalizing flows. We now give experimental evidence for this in Figs 4, 5 and 6. We use Gaussiandistribution as a base distribution for all our experiments of constrained normalizing flow. In Fig.4,we see that for neural network with m = 100 and m = 400, the training loss decreases stably. InFigs. 5 and 6, we see that as we increase the learning rate, training becomes more stable for larger m.
Figure 3:	Effect of over-parameterization on training of unconstrained normalizing flow on mixtureof Gaussian and mixture of beta distributions39Under review as a conference paper at ICLR 2021Figure 4: Effect of over-parameterization on training of small sized constrained normalizing flowG.2 Results for constrained normalizing flowIn Sec. 2.1, we suggested that high overparameterization may adversely affect training for constrainednormalizing flows. We now give experimental evidence for this in Figs 4, 5 and 6. We use Gaussiandistribution as a base distribution for all our experiments of constrained normalizing flow. In Fig.4,we see that for neural network with m = 100 and m = 400, the training loss decreases stably. InFigs. 5 and 6, we see that as we increase the learning rate, training becomes more stable for larger m.
Figure 4: Effect of over-parameterization on training of small sized constrained normalizing flowG.2 Results for constrained normalizing flowIn Sec. 2.1, we suggested that high overparameterization may adversely affect training for constrainednormalizing flows. We now give experimental evidence for this in Figs 4, 5 and 6. We use Gaussiandistribution as a base distribution for all our experiments of constrained normalizing flow. In Fig.4,we see that for neural network with m = 100 and m = 400, the training loss decreases stably. InFigs. 5 and 6, we see that as we increase the learning rate, training becomes more stable for larger m.
Figure 5:	Effect of over-parameterization on training of constrained normalizing flow on mixture ofGaussian dataset for number of hidden layers m = 1600, 640041Under review as a conference paper at ICLR 20210	20	«	60 SO 100Number of epochsMixture of Beta Distributions (I.r. =0.025)20 Φ £0 SO 100Number of epochs12108 6 4 2M,≡06UeUU J。UnoU 2一Mixture of Beta Distributions (l.r.=0.1)m=1600m=640020	40	60 SO 100Number of epochsm=1600Mixture of Beta Distributions (l.r.≡0.025)O 20 -W 60 SO IOCNumberofepochs
Figure 6:	Effect of over-parameterization on training of constrained normalizing flow on mixture ofbeta distribution dataset for number of hidden layers m = 1600, 640042Under review as a conference paper at ICLR 2021fl4 ι口弧1Mixture of Gaussian Distributions (l.r.=0.0125)2o Mixture of Gaussian Distributions (l.r.=0.1)---------------------------------m=1600L8 '	---- m=6400Number of epochsFigure 7:	Effect of over-parameterization on training of constrained normalizing flow on mixture ofGaussian dataset for epochs=1000 and number of hidden layers m = 1600, 640043Under review as a conference paper at ICLR 2021□Lemma H.2. Let X1, X2, ..., Xn be independent random variables from N (0, σ2), then with at least1 - -1 probability, following holds.
Figure 7:	Effect of over-parameterization on training of constrained normalizing flow on mixture ofGaussian dataset for epochs=1000 and number of hidden layers m = 1600, 640043Under review as a conference paper at ICLR 2021□Lemma H.2. Let X1, X2, ..., Xn be independent random variables from N (0, σ2), then with at least1 - -1 probability, following holds.
