Figure 1: Examples of generated summarizations on Java (red blocks) and Python (blue blocks)test set. Transformer is a vanilla Transformer, Transformer (full) implements relative positionalencoding and copy mechanism from Ahmad et al. (2020).
Figure 2: Overall structure of our model. Encoder consists of multiple GN-Transformer blocks.
Figure 3: (a) Simplest fully-connected graph representation of sequence. Each node corresponds toa token. Self-loops are omitted. (b) Deep blue nodes are AST for the statement ’a[i]=4+2’, shallowblue nodes shows the correspondence between AST and source code. (c) Standard SCG structure.
Figure 4:	(a) Average code length of subsets of samples under different sentence BLEU scorethresholds on the test set of Java dataset. (b) The average path length and clustering coefficient ofour SCG structures are calculated on the test set of Java dataset and Transformer.
Figure 5: (a) Introducing shortcut edges for AST node ’AdditiveExpr’ in Variant 1, the three tokennodes ’4’, ’+’, ’2’ will connect with it. We marked token nodes by shallow blue, AST nodes by deepblue, edges from token nodes to AST nodes by red, orange for the converse direction. (b) In Variant2, all token nodes are fully connected with each other.
Figure 6: Example of building connections for subtokens. Edges between other tokens and AST areomitted. The figure also shows how shortcut edges are connected to subtoken nodes. (a) Connectionfor original token node. (b) Connection for subtoken nodes. All subtoken nodes copied the sameedges from the original token node.
