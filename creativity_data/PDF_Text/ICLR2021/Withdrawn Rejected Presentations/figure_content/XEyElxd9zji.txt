Figure 1: Feedforward networks vs RNNs(b) RNNsynapse gets an increment. Over the seven decades since Hebb, many forms of plasticityhave been observed experimentally and/or formalized analytically, many of them quitesophisticated and complex, see Dayan and Abbott (2001) for an exposition. All of themdictate a change - increment or decrement - in the synaptic weight of a synapse (i,j) providedneurons i and j both fired in some pattern. Intuitively, the decision for the application of aplasticity rule takes place at the post-synaptic neuron j , since j receives information fromthe firing of both i and itself. This is consistent with our understanding of the molecularmechanisms that determine synaptic strength, all of which are complex chemical phenomenataking place at (the dendrite of) j .
Figure 2: On the standard MNIST data set, we trained the same underlying RNN withT = 1, |V | = 1000 with an output layer plasticity rule, and separately with GD (using thestandard Adam optimizer, learning rate 10-2) on the output weights. Note that we did notoptimize hyperparameters such as batch size and learning rate. This is only meant to showthat plasticity-based training is competitive with gradient methods.
Figure 3: Comparison of various models on different datasets trained using the same plasticityrules. We first learned a plasticity rule for the output weights of a small feedforward network(i.e. |V | = 100, T = 1) on the Halfspace dataset, and a plasticity rule for the graph weights ofa small recurrent network (i.e. |V | = 100, T = 3) on the ReLU1 dataset. We then hold theserules fixed and use them to re-train both of the small models and additionally large models(|V | = 1000) on all six datasets, restricting MNIST and Fashion-MNIST to only a random10,000 training examples in the interest of fair comparison. Averages of 10 re-trainings foreach model/dataset combination are shown above.
Figure 4: Adversarial perturbations on MNIST (left) and FashionMNIST (right) for aGD-trained network, and a plasticity-trained network. Original images are in the top row.
Figure 5: The same network with |V | = 1000, cap = 500 was trained with plasticity rules forT = 1, 3, and with GD. For each trained network, we generate adversarial data-sets withincreasing perturbation magnitude.
Figure 6: Updating on all examples provides similar results as updating only on misclassifieddata for a T = 3, |V| = 1000, k = 500 simple RNN on MNIST.
