Figure 1: Adversarial attacks against ResNet152 over the giant panda image using FGSM (Goodfellowet al., 2015), PGD-40 (Madry et al., 2017)(α=8∕255), DeePFool (Moosavi-Dezfooli et al., 2016) and Carlini-Wagner (Carlini & Wagner, 2017) attacks. The second columns in panels show the difference (L2) between theoriginal image (not shown) and the adversarial one (values shifted by 128 and clamPed). The edge maP (usingCanny edge detector) remains almost intact at small Perturbations. Notice that edges are better Preserved forthe PGD-40. See APPx. A for a more detailed version of this figure, and also the same using the Sobel method.
Figure 2: Sample images from the datasets. Numbers in parenthesesdenote the number of classes.
Figure 3: Left) Average results of the EAT defense on all datasets (last cols. in tables). Middle and Right)Comparison of natural (Orig. model column; solid lines) vs. adversarial training averaged over all datasets.
Figure 4: Results of GSD method.
Figure 5: A) Background subtraction together with edge detection improves robustness (here against theFGSM attack). Noisy data is created by overlaying a digit over white noise (noise×(1-mask)+digit). B) De-fending backdoor attacks. An almost invisible pattern (with intensity 10/255 of the digit intensity) is added tohalf of the samples from one class, which are then relabeled as another class. Notice that the Edge model isnot confused over the edge maps (right panels) since edge detection removes the pattern. In case of a visiblebackdoor attack, background subtraction can help discard the irrelevant region. See Appx. I for more details.
Figure 6: Classification accuracy over naturally distorted images.
Figure 7: Edge-guided adversarial training (EAT). In its simplest form, adversarial training is per-formed over the 2D (Gray+Edge) or 4D (RGB+Edge) input (i.e., number of channels; denoted asImg+Edge). In a slightly more complicated form (B), first for each input (clean or adversarial), theold edge map is replaced with the newly extracted one. The edge map can be computed from theaverage of only image channels or all available channels (i.e., image plus edge).
Figure 8: Adversarial attacks against ResNet152 over the giant panda image using 4 prominentattack types: FGSM (Goodfellow et al., 2015) and PGD-40 (Madry et al., 2017) (α=8∕255) fordifferent perturbation budgets ∈ {8, 16, 32, 64}, as well as DeepFool (Moosavi-Dezfooli et al.,2016) and Carlini-Wagner (Carlini & Wagner, 2017). The second column in each panel shows thedifference (L2) between the original image (not shown) and the adversarial one (values shifted by128 and clamped). For DF and CW, values are magnified 20x and then shifted. The edge map (usingthe Canny edge detector) remains almost intact at small perturbations. Notice that edges are betterpreserved for the PGD-40 attack. See Appx. A for results using the Sobel method.
Figure 9: As is in Fig. 1 in the main text but using the Sobel edge detector. As it can be seen edgemaps are almost invariant to adversarial perturbation.
Figure 10: Illustration of adversarial perturbation over the image as well as its edge map. The firstrow in each panel shows the clean or adversarial image (under the FGSM attack). The second rowshows the perturbed edge map (i.e., the edge channel of the the 2D or 4D adversarial input). Thethird row shows the redetected edge map from the attacked gray or rgb image (i.e., calculated onlyfrom the image channels and excluding the edge map itself).
Figure 11: Samples images from Sketch and Icons-50 datasets, perturbed with FGSM = 8/255,and their corresponding edge maps using Canny edge detection.
Figure 12: Top) Adversarial example generated for the giant panda image using the FGSM at-tack (Goodfellow et al., 2015). Bottom) Adversarial examples generated for AlexNet from Szegedyet al. (2014). (Left) is a correctly predicted sample, (center) difference between correct image, andimage predicted incorrectly magnified by 10x (values shifted by 128 and clamped), (right) adversar-ial example (i.e., left image + middle image). Even though the left and right images appear visuallythe same to humans, the left images are correctly classified by a DNN classifier while the right im-ages are misclassified as “ostrich, Struthio camelus”. Notice that in all of these images the overallimage structure and edges are preserved.
Figure 13: A) Classification of a standard ResNet-50 of (a) a texture image (elephant skin: onlytexture cues); (b) a normal image of a cat (with both shape and texture cues), and (c) an image witha texture-shape cue conflict, generated by style transfer between the first two images, B) Accuracyand example stimuli for five different experiments without cue conflict, and C) Sample images fromthe Stylized-ImageNet (SIN) dataset created by applying AdaIN style transfer to an ImageNet image(left). Figure compiled from Geirhos et al. (2018).
Figure 14: An example visual illusion simultaneously depicting a portrait of a young lady or anold lady. While fooling humans takes a lot of effort and special skills are needed, deep models aremuch easier to be fooled. In this example, the artist has carefully added features to make the portraitlook like an old lady while the new additions will not negatively impact the look of the younglady too much. For example, the right eyebrow of the old lady (marked in red below) does notdistort the ear of the young lady too much. See https://medium.com/@jonathan_hui/adversarial-attacks-b58318bb497b for more details.
Figure 15: Classification results based on shape vs. texture. The left-most column shows the imagepresented to a model. The second column in each row names the object from which the shapewas sampled. The third column names the object from which the textured silhouette was obtained.
Figure 16: Comparison of natural (the Orig. column in the tables; solid curves) vs. adversarialtraining (blue dashed-dot curves). The accuracy at = 0 (for adversarial training) is averaged overdifferent robust models (three over MNIST and two over others; corresponding to clean columns intables). Left column) Average over MNIST and Fashion MNIST datasets, Right column) Averageover all datasets. Results show a clear advantage of using edges. Over MNIST and FashionMNIST,the model trained on edges alone leads to a trade-off between accuracy and robustness. Img+edgemodel does worse than the Image model but its performance is recovered after adversarial training.
Figure 17: Breakdown of natural training (the Orig. row in Tables) over datasets.
Figure 18: Top) GSD with a classifier trained on images generated (by pix2pix) only from theedge maps of the clean images, Bottom) GSD with edge maps derived from adversarial examples.
Figure 19: Sample images alongside their corruptions with 5 severity levels.
Figure 20: Edges for images in Fig. 19.
Figure 21: Performance of models against natural image corruptions over the TinyImageNet dataset.
Figure 22: Performance of models against natural image corruptions over the GTSRB dataset.
Figure 23: Performance of models against natural image corruptions over the Icons-50 dataset.
Figure 24: Examples of invisible (left) and visible (right) backdoor attacks (e.g., Brown et al.
Figure 25: Application of the Img and Edge models over Original, noisy, and background-subtractedMNIST digits. Noisy data is created by overlaying a digit over white noise (noise×(1-mask)+digit).
Figure 26: Application of the Img and Edge models over Original, noisy, and background-subtractedFashionMNIST data. Noisy data is created by overlaying an object over white noise (noise×(1-mask)+object). The FGSM attack is used here. We find that background subtraction together withedge detection improves robustness.
Figure 27: Similar to Fig. 5 in the main text with the difference that here the noise model is trainedover the noisy data. Removing the perturbations on the image background (via background subtrac-tion) improves the robustness.
Figure 28: Masking the FGSM attack perturbations (i.e., keeping the altered pixels on the image oredge only regions).
Figure 29: Sample images from the Boundary attack.
Figure 30: Two sample adversarial images (FGSM) along with their edge maps using HED andCanny edge detection methods.
Figure 31: Top: our pipeline to approximate the Canny edge detector and our approach for craftingadversarial examples, Bottom: Sample digits and their generated edge maps.
Figure 32:	PyTorch code of our pipeline shown in Fig 31.
Figure 33:	Top: Performance of the adaptive attack, Bottom: Samples adversarial images and theiredge maps using the Canny edge detector.
