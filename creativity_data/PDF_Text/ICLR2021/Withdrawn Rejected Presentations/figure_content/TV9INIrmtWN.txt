Figure 1: Illustration of the PhysEnv domain (Du & Narasimhan, 2019). Here, the agent (darkblue) needs to navigate to the goal (red) while avoiding enemies (light blue). Typically the fullenvironment states (top) are provided to the RL algorithm to learn. We consider the case where onlyobservations from under a hard attention window (bottom) are available. The attention’s location(yellow square) at each time step is controllable by the agent. Using only partial observations, theagent must learn to represent its surroundings and complete its task.
Figure 2: The agent starts with a representation (μt) from which it attempts a reconstruction (Tt)of the likely current full (unobserved) state st. Then, it picks an attention location (lt) and receivesan observation of the state from under the attention (ot). We assume that the size of the attentionwindow is known to the agent. The observation is written into μt to form 哨.The entire map isthen stepped through an internal world model to update the dynamical objects, forming μt+ι, therepresentation for the next step.
Figure 3: (left) Object-wise breakdown of reconstruction error in gridworld. (right) % of frames inwhich the object occurs, i.e. the frequency with which the attention is focuses on a particular object.
Figure 4: Average testing reward for goal seeking task in PhysEnv (left) and gridworld (right).
Figure 5: Visualizations of glimpse behavior in gridworld environment. Each sub-figure is the sametesting episode at different stages of training, with the rows corresponding to the time within theepisode. In each subfigure, the first column shows the full (unobserved) state. The second columnis a heatmap showing the glimpse policy, i.e. the likelihood the glimpse agent will select a locationfor placing attention. The location that was sampled is indicated in the first column by the yellowsquare. The final, third, column is the reconstruction T based on the agent,s current current beliefabout the environment.	14Under review as a conference paper at ICLR 202115Under review as a conference paper at ICLR 2021Next we show qualitative comparisons of the glimpse policy and state reconstruction between ourmethod and the baselines. Each subfigure in figures 7 and 8 shows the same episode of length 20played out with different glimpse agents. The first column is our method, the second shows a uni-form random policy for the glimpse agent, the third shows a glimpse agent trained on environment(task) rewards, and lastly the fourth shows the glimpse following the agent. In both environments,it can be seen that our method learns to focus on dynamic objects within the environment and theresulting state reconstruction shows that these objects are preserved in the memory.
Figure 7: Glimpse behavior for fully trained models of our method and the baselines in gridworld.
Figure 8: Glimpse behavior for fully trained models of our method and the baselines in PhysEnv.
Figure 9: (left) PhysEnv environment. (right) Gridworld environment. Reconstruction error (L2)from the full state during training of the DMM + glimpse agent. All approaches have converged by200k iters, with our approach having the lowest error, i.e. the most faithful reconstruction of the fullunobserved state.
Figure 10: (left) PhysEnv environment. (right) Gridworld environment. Episodic reward for glimpseagent during training (higher is better).
Figure 11: State reconstruction and observation reconstruction errors in gridworld environment.
