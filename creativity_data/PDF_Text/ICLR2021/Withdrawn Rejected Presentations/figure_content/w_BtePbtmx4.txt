Figure 1: Comparing Localized Updates and SGD-based BP•	Localized Learning Rule Formulation. We formulate a computationally efficient localizedlearning rule and highlight the clear runtime benefits when compared to SGD.
Figure 2: Overview of the Learning Mode Selection AlgorithmOverall, based on the impact of localized learning on both runtime and accuracy, we find that a goodlearning mode selection algorithm should favor application of localized learning to a contiguousgroup of initial layers, while ensuring fewer or no localized learning epochs in later layers. We further4Under review as a conference paper at ICLR 2021impose an additional constraint on top of this spatio-temporal pattern. Specifically, we allow eachlayer to transition from one learning mode to another at most once during the entire training process.
Figure 3:	Progression ofLocalized→SGD transition layerepochs.
Figure 4: Analyzing memory footprint andbatch-size variationwhich reduces the frequency of gradient aggregation between devices and alleviates the communi-cation overhead. At epoch 33, the memory footprint per GPU reduces to less than 5 GB, allowingtraining with an increased mini-batch size of 128 per GPU from epoch 33 onwards. The doublingof the batch-size provides an additional 6% runtime improvement, when measured across the entiretraining period. We note that other training techniques such as training with stochastic depth cannotexploit this feature, due to minimal reduction in memory footprint.
Figure 5: Compute efficiency vs. accuracy trade-off on the ImageNetdataset for a) ResNet50 and b) MobileNetV2increase to 1.38 ×-1.47× when around 1.5% loss in accuracy is tolerable.
Figure 6: Impact of (a) tshift and (b) Lmax on accuracy and indicates that accuracy is SeVerely im-SlUUJ	' maxruntime savings on the ImageNet dataset for ResNet50 pacted for extremely smallvalues oftshift that are less than 3%. The ac-curacy is largely stable in the regime of tshift between 5-12%, and begins to experience smalldegradations again when tshift exceeds 12%. These trends can be explained by analyzing the rate atwhich the transition layer progresses, and the number of layers transitioning to localized updates inan epoch for different tshift values. Smaller values of tξihift (<3%) give rise to low values of k (〜1-2epochs), the minimum number of epochs that must elapse before the transition layer can shift again.
