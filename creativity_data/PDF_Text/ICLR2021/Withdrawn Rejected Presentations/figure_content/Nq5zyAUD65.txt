Figure 1: Smooth activations (top) and their gradients (bottom) for different β parameter values.
Figure 2: Three dimensional surfaces showing network outputs as function of 2-dimensional inputsin [-6, 6], with 5 hidden layers of dimensions [256, 128, 64, 32, 16] activated by ReLU, SmeLU withdifferent β parameters, and no activation (linear). Layers apply weight normalization, and there is noclipping of large activations. Matrices of all hidden layers are equal for all figures and are randomlydrawn from a standard normal N (0, 1) distribution. Top: Different activations and SmeLU β values.
Figure 3: Relative PD; ∆r1, expressed in [%] of the positive label prediction as function of losschange [%] from a ReLU baseline for different activations on real validation data. The X-axis is:left: logarithmic cross entorpy loss, right: ranking loss.
Figure 4: PD ∆1 as function of error rate on MNIST dataset for different activations with differentβ parameters. Left: with Adagrad optimizer, right: with an SGD optimizer.
Figure 5: Three dimensional surfaces showing network outputs as function of 2-dimensional inputsin [-6, 6], with 5 hidden layers of dimensions [256, 128, 64, 32, 16] with no activation (linear), andactivated by ReLU, and SmeLU with different β parameters. Matrices of all hidden layers areequal for all figures and are randomly drawn from a standard normal N (0, 1) distribution. Top:ReLU, SmeLU different β and linear, no clipping of pre-activation values with L2 norm 1 weightnormalization. Second Row: weight normalization with activation clipping to [-6, 6]. Third Row:no clipping, no weight normalization. Bottom Row: clipping, no weight normalization.
Figure 6: Three dimensional surfaces with identical configuration to Fig. 5 for Swish, Softplus,GELU, and SELU, with different β parameters, all with weight normalization and no clipping. Top:Swish and Softplus. Bottom: GELU and SELU.
Figure 7: Three dimensional surfaces with identical configuration to Fig. 5 for no activation, ReLUand Smelu (β = 0.5) with weight and layer normalizations, with no clipping. Top: Linear withdifferent weight and layer norms. Middle: ReLU with different weight and layer norms. Bottom:SmeLU with different layer norms.
Figure 8: Different activations and their gradients for β = 1.
Figure 9: SELU (top) and CELU (bottom) activations and gradients with different β parametervalues.
Figure 10: SoftPlus activations and gradients with different β parameter values.
Figure 11: Swish, GELU, Mish, and TanhExp activations and gradients with different β parametervalues.
Figure 12: SmeLU, Generalized SmeLU, Sigmoid-RESCU, and SmoothRelu activations and gradi-ents with different α, β and gradient parameter values.
Figure 13: PD ∆2 aS function of error rate on MNIST dataSet for different activationS with differentβ parameterS. Left: with Adagrad optimizer, right: with an SGD optimizer.
Figure 14: PD ∆1L as function of error rate on MNIST dataset for different activations with differentβ parameters. Left: with Adagrad optimizer, right: with an SGD optimizer.
Figure 15: PD ∆H as function of error rate on MNIST dataset for different activations with differentβ parameters. Left: with Adagrad optimizer, right: with an SGD optimizer.
