Figure 1: Distribution of `2norm of top [√pi] of attentionweightsBaseline model: To train the baseline model, we first flatten theinput one large vector of dimension 130 × 100 and pass it to a 1-hidden layer MLP with h hiddenunits. The model is trained using binary cross entropy loss.
Figure 2:Noisy MNISTdatasetonly 670k number of parameters. This fact also guarantees that the superiority of our model is notfrom training a bigger model.
Figure 3: Number of linear regions inlog scale v.s. sparsityinterpret that attention mechanisms help us reduce unnecessary non-linearity inside the landscape.
