Figure 1: A diagram on the left specifies several properties demonstrated by finite-width mod-els. As plots on the right demonstrate, our novel IC-MF limit model satisfy all of these prop-erties, while MF and NTK limit models, as well as sym-default limit model presented in thepaper violate some of them. Left: A band of scaling exponents (qσ, q) that lead to dynamicallystable model evolutions in the limit of infinite width, as well as dashed lines of special properties thatcorresponding limits satisfy. Three colored points correspond to limit models that satisfy most ofthese properties. Right: Training dynamics of three models that correspond to color points on the leftplot, as well as of initialization-corrected mean-field model (IC-MF), which does not correspond toany point of the left plot. These models are results of scaling of a reference model of width d = 27(black line) up to width d = 216 (colored lines). Solid lines correspond to the test set, while dashedlines are for the train set. Note that we have added a small vertical displacement to all curves in orderto make them visually distinguishable. See Appendix F for details.
Figure 2: Initialization-corrected mean-field (IC-MF) limit captures the behavior of a givenfinite-width network best among other limit models. We plot a KL-divergence of logits of differentinfinite-width limits of a fixed finite-width reference model relative to logits of this reference model.
Figure 3: Test accuracy of different limit models, as well as of the reference model. Setup: We train aone hidden layer network on subsets of the CIFAR2 dataset of different sizes with SGD with varyingbatch sizes.
Figure 4: Mean kernel diagonals E X〜D(ηaKa,d(x, x) + nWKw,d(x, x)) of different limit models, aswell as of the reference model. Setup: We train a one hidden layer network on subsets of the CIFAR2dataset of different sizes with SGD with varying batch sizes. Data expectations are estimated with 10test data samples.
Figure 5: Mean absolute logits E X〜D ∖f (x)∖ of different limit models, as well as of the referencemodel. Setup: We train a one hidden layer network on subsets of the CIFAR2 dataset of differentsizes with SGD with varying batch sizes. Data expectations are estimated with 10 test data samples.
Figure 6: Mean absolute logits relative to kernel diagonals E X〜D∖fd(x)∕(^^2Ka,d(x, x) +nWKw,d(x, x))∖ of different limit models, as well as of the reference model. Setup: We train aone hidden layer network on subsets of the CIFAR2 dataset of different sizes with SGD with varyingbatch sizes. Data expectations are estimated with 10 test data samples.
Figure 7: KL-divergence of different limit models relative to a reference model. Setup: We train aone hidden layer network on subsets of the CIFAR2 dataset of different sizes with SGD with varyingbatch sizes.
Figure 8: Left: KL-divergence of different limit models relative to a reference model. Right:Accuracies on the test set of different limit models as well as of the reference model. Setup: We traina one hidden layer network on the full CIFAR10 dataset with SGD with batches of size 100.
Figure 9: Left: we plot a KL-divergence of logits of different infinite-width limits of a fixed finite-width reference model relative to logits of this reference model. KL-divergences are estimated usinggaussian fits with 10 samples. Right: same, for probabilities instead of logits. KL-divergences areestimated using beta-distribution fits with 10 samples. Setup: we train a one hidden layer networkwith SGD on CIFAR2 dataset; see Appendix F for details.
