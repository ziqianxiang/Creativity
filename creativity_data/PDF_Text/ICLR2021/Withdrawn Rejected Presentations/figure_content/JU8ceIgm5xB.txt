Figure 1: A demonstration of our approach in vision (left) and dialogue (right). (left) Given twoaugmentations X and y, we fork X into two subviews, χ0 which is an exact copy of X and χ00, aninformation-restricted view obtained by occluding some of the pixels in x0. We can maximizeI(x; y) ≥ I(x00; y) +1(x0; y|x00) using a contrastive bound by training x00 to be closer to y than toother images from the corpus, and by training x0 to be closer to y than to samples from p(y∣x00),i.e. we can use x00 to generate hard negative samples for x0. The conditional MI term encourages theencoder to imbue the representation of x0 with information it shares with y beyond the informationalready in x00. (right) χ0 and y represent past and future in a dialogue respectively and x00 is the“recent past”. In this context, the encoder is encouraged to capture long-term dependencies that cannotbe explained by the most recent utterances.
Figure 2: We plot the value of the MI estimated by INCE and INCES bounds for three Gaus-sian covariates x0 , x00 , y as function of the number of negative samples K. We sample differentcovariances for a fixed true MI (green horizontal line) and report error bars. “InfoNCE” com-putes INCE(x0, x00; y); “InfoNCES” computes INCE(x00; y) + ICNCE(x0; y|x00); “InfoNCES IS”computes INCE(x00; y) + IIS(x0; y|x00).
