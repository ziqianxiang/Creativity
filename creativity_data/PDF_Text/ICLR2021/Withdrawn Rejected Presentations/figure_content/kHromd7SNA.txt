Figure 1: An animation produced by our method.
Figure 2: Overview of our model. The region predictor returns heatmaps for each part in thesource and the driving images. We then compute principal axes of each heatmap, to transformeach region from the source to the driving frame through a whitened reference frame. Region andbackground transformations are combined by the pixel-wise flow prediction network. The targetimage is generated by warping the source image in a feature space using the pixel-wise flow, andinpainting newly introduced regions, as indicated by the confidence map.
Figure 3: Comparison of motion/part representations. Regression-based keypoint representationsdo not provide consistent detection between frames (marked with red). Additionally, backgroundmotion leaks into one or several detected keypoints. Our PCA-based regions (with and withoutbackground motion) correctly identify meaningful parts, are consistent between frames, and useadditional regions more effectively.
Figure 4: Qualitative comparisons. We show representative examples of articulated animation usingour method and FOMM (Siarohin et al., 2019a), on two datasets of articulated objects: TED-talks(left) and TaiChiHD (right). Zoom in for greater detail.
Figure 5: Mean test-time absolute rotationerror, as a function of training set size.
Figure 6: Examples of synthetic rectangle dataset.
