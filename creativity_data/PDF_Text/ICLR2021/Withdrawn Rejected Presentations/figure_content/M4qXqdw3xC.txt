Figure 1: An illustration of how CNNs use posi-tion information to resolve boundary effects. Weplace CIFAR-10 images in random locations on acanvas of 0’s (black) or 1’s (white). We evaluate ifa ResNet-18, trained w/ or w/o padding for seman-tic segmentation, can segment the image region.
Figure 2: We consider two location dependant tasksdesigned to investigate the boundary effects in CNNs.
Figure 3: Sample pair generation reflect-ing two semantic concepts.
Figure 4: Left: Location dependant image classification (left two) and segmentation (right two).
Figure 5: Left: Filter activation visualization for the classification task on CIFAR-10 with a whitebackground and 5 × 5 grid size. It is clear that zero padding provides richer information and largeractivations downstream, particularly at locations near the boundary (e.g., L = 5). Right: Samplepredictions of semantic segmentation on different locations of a 7 × 7 grid under three backgroundsettings. Confidence maps are plotted with the ‘cividis’ colormap, where yellow and dark blueindicates higher and lower confidence, respectively.
Figure 6: Comparison of filter activations for the location dependant segmentation task trainedwithout padding, 5 × 5 grid size, L = 13, and three canvas colors, black, white, and mean. Noticethe large activations in the background region for black, contrasting that of white and mean.
Figure 7: Example predictions on the Cityscapes validation setwhen training w/ and w/o padding. Best viewed zoomed in.
Figure 8: (a) the input image, (100 × 100). (b) the output of the convolution with padding, (100 × 100).
Figure 9: An illustration of our proposed grid settings (k = 3) with all three different canvas colorsfor the location dependant tasks.
Figure 10: Location dependant image classification (left) and semantic segmentation (right).
Figure 11: Location dependant image classification (left two) and segmentation (right two).
Figure 12: t-SNE (Maaten & Hinton, 2008) visualization of the CIFAR-10 test set classificationlogits for a 7 × 7 grid. Examples of a single input are given in the top row, while the embeddingvisualizes the entire dataset (bottom two rows).
Figure 13: Sample predictions of image segmentation on all the locations of a 5 × 5 grid under themean canvas setting. Confidence maps are plotted with the ‘viridis’ colormap, where yellow and darkblue indicates higher and lower confidence, respectively.
Figure 14: Example predictions on the Cityscapes validation set when training w/ and w/opadding. Best viewed with zoom.
Figure 15: An illustration of the evaluationregions used for the analysis in Table 11 andFig. 7.
Figure 16: Performance comparison ofDeepLabv3 network with respect to various im-age regions and padding settings.
Figure 17: Sample training images generated using Cutout under two different canvases.
