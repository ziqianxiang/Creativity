Figure 1: MetaCURE’s meta-training pipeline (With prediction error intrinsic). Sharing buffers fortWo policies achieves high sample efficiency as both policies can be off-policy trained With historyexperiences. Batches from B and Benc are sampled from the same task. πe ’s Q function is omittedfor brevity.
Figure 2: Evaluation of MetaCURE and several meta-RL baselines on various sparse-reward con-tinuous control task sets. MetaCURE achieves superior final performance as well as high sampleefficiency, while none of the baseline algorithms manage to solve all five tasks. MetaCURE-PEslightly outperforms MetaCURE-IG.
Figure 3: Adaptation visualization of MetaCURE-PE and PEARL on Point-Robot-Sparse. Theagent is given four episodes to perform adaptation. Trajectories in dark purple are trajectories ofthe first adaptation episode, while the light yellow trajectories are trajectories of the last episode.
Figure 4:	Visualization of MetaCURE-PE and PEARL on the Walker-Vel-Sparse tasks. The solid redline is the target velocity, and the region bounded by red dash lines represents velocities that get in-formative rewards. While PEARL keeps its belief unchanged during an entire episode, MetaCURE-PE first efficiently explores the goal velocity before performing good exploitation policies.
Figure 5:	(a) Learning curves of MetaCURE-IG and MetaCURE-PE on Point-Robot-Sparse-Noise.
Figure 6:	Evaluation of MetaCURE and several meta-RL baselines on Reacher-Goal-Sparse andWalker-Rand-Params.
Figure 7:	Visualization of MetaCURE and PEARL on the Cheetah-Vel-Sparse tasks.
Figure 8:	(a) Learning curves of MetaCURE-IG and MetaCURE-PE on Point-Robot-Sparse-Random. (b) Adaptation visualization of MetaCURE-IG. The agent gets transmitted when it touchesthe green circle. (c) Adaptation visualization of MetaCURE-PE. It fails to obtain good explorationbehaviors, as it is interested in the randomness of being transmitted.
Figure 9:	MetaCURE without intrinsic rewards cannot explore effectively during meta-training andperforms poorly.
Figure 10:	(a) Point-Robot-Sparse with a sub-optimal goal. The sub-optimal goal is illustrated ingreen, which requires no adaptation exploration but offers lower returns. The cross indicates theagent’s initial position. Blue circles indicate optimal goals. (b) Learning curves of MetaCUREw/o intrinsic rewards on the illustrated task set. (c) Adaptation visualization of MetaCURE withintrinsic rewards (curiosity intrinsic). MetaCURE finds the optimal goal, and leads to higher fi-nal performance. (d) Adaptation visualization of MetaCURE without intrinsic rewards. Explorerwithout intrinsic rewards maximizes average extrinsic return, leading to inefficient exploration.
