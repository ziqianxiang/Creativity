Figure 1: Model Architectures of (a) VSDN-F (filtering); (b) VSDN-S (smoothing).
Figure 2:	Visualization for human skeleton interpolation of different models.
Figure 3:	Training processes of our models with respect to the different number of sampled latentstate trajectories. (UP: training set; Bottom: validation set)4.3 Quantitative StudiesIn order to better understand the properties of VAE and IWAE losses in training VSDNs, we conductcomprehensive quantitative evaluation by varying the number of sampled trajectories when comput-^ https://cdiac.ess-dive.lbl.gov/epubs/ndp/ushcn/monthly _doc.html8Under review as a conference paper at ICLR 2021ing these losses. We visualize the LVAE and LIKW AE of VSDN trained for 40 epoches on theHuman3.6M dataset in Figure 3. As the VSDN-S contains both forward and backward ODE-RNNs,it is more difficult to train than VSDN-F. The looseness of LVAE further increases the training dif-ficulty and results in a worse lower bound of VSDN-S (VAE). Therefore, VSDN-S (VAE) requiresmore epochs to converge during the training. For the other cases, we observe that the LIKW AE istighter than LV AE in training when the number of trajectories is small. Therefore, LIKW AE hasfaster convergence in training our models. For large number of trajectories, LV AE has similar tight-ness as LIKW AE in training set.
Figure 4: A example to show the noise injection problem of R(Xt).
