Figure 1: The hard attention network architecture we consider, consisting of an RNN core (yellow), alocation network (light blue), a glimpse embedder (dark blue), and a classifier (red). ht is the RNNhidden state after t steps. The network outputs distributions over where to attend (lt) at each timestep, and over the class label (θ) after T steps.
Figure 2: A near-optimal glimpse sequence being generated for the task of inferring the attribute‘Male’. Top row: A heatmap of estimated expected posterior entropy for each possible next glimpselocation lt . The red cross marks the minimum, which is chosen as the next glimpse location. Bottomrow: Observed parts of the image after taking each glimpse.
Figure 3: Attentional variational posterior CNN. An RGB image and l1:t are processed to create anembedding of the information gained from glimpses 1 to t. This embedding is fed into an imageclassifier to obtain an approximation of p(0|yi：t,li：t).
Figure 4: Comparison of glimpse locations chosen by RAM and PS-NOGS on the CelebA-HQ testset for three classification tasks. For each t ∈ {1, 2, 3, 4, 5}, we show an image where each pixel’scolour corresponds to how often it was observed at this time step during testing. The outlines areproduced by averaging outputs from a face detector across the dataset. For t = 1, each networklearns a single location which it attends to on every test image. This is expected behaviour as the firstlocation is chosen before taking any glimpses, and therefore before being able to condition on theimage. RAM appears to then fail to learn to direct the later glimpses, attending almost uniformlyacross the image. In contrast, PS-NOGS distributes these glimpses broadly over the salient regions.
Figure 5: Number of training iterations for each CelebA-HQ attribute before a validation cross-entropy loss within 0.01 of the best is achieved. On average, PS-NOGS trains almost 7× faster thanRAM, the fastest method without supervision sequences. PS-NOGS also exhibits greatly reducedvariance in the training time. Attributes are sorted by the mean training time.
Figure 6: CUB validation accuracy over training.
