Figure 1: The CC sparsity constraint and the associated accelerator datapath for general matrixmultiplication (GEMM), in comparison with BB sparsity. (a) An illustration of CC sparsity imposedon an 8 × 8 weight matrix with G = 4, s = 1. See main text for details. (b) GEMM with a CC-sparseweight matrix, illustrated for the same pattern as in (a). (c) CC-sparse GEMM dataflow for the case in(b). Illustrated is a single operation involving elements highlighted with red rectangles in (b). Partialsum H is persistent in PE’s register (output-stationary). (d, e) Similar to (b, c), for BB sparsity. Seemain text for details.
Figure 2: Impact of the intragroup sparsity constraint. (a) Probabilities of a nonzero weight failingto be allocated an encoding slot, for various group sizes and sparsity levels PS of 2, 4, 7, and 15.
Figure 3: (a) The weight distributions in the original (red) and proposed (blue) dropout rampingmodel training. Solid lines: weights from the pruning pool. Dashed lines: weights protected frompruning. The dropout ratio ramping causes more weights in the pruning pool to shrink toward 0. (b)Efficiency-accuracy tradeoff for the ImageNet classification task compared among various lightweightand pruned network architectures. The horizontal axis corresponds to the total operation counts(in FLoPs). For MobileNetV1, MobileNetV2, and ShuffleNetV1, we also include the results frommodels with the width multiplied by various scales. MobileNetV2 with intragroup sparsity: Yellowtriangles: G = 8, s = 2, with width scales of 1.0, 1.4 and 2.0, trained for 120 epochs; Red invertedtriangles: G = 8 with s = 1, 2, 3and4, trained for 400 epochs.
