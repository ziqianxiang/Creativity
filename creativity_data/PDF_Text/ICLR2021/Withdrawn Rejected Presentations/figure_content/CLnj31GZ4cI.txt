Figure 1: (a) Pre-trained language models inject multiple kinds of knowledge with multi-task learn-ing. Model parameters need to be retrained when injecting new kinds of knowledge, which mayresult in the catastrophic forgetting (b) Our K-ADAPTER injects multiple kinds of knowledge bytraining adapters independently on different pre-train tasks, which supports continual knowledgeinfusion. When we inject new kinds of knowledge, the existing knowledge-specific adapters willnot be affected. KIA represents the adapter layer and TRM represents the transformer layer, both ofwhich are shown in Figure 2.
Figure 2: Structure of the adapter layer (left). Theadapter layer consists of two projection layers andN=2 transformer layers, and a skip-connectionbetween two projection layers.
