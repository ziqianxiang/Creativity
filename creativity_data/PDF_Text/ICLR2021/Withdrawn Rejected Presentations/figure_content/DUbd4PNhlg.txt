Figure 1: In the experiment, we set the dimension d = 10 and the number of training samples n = 200.
Figure 3: Subfigure (a) shows w(k),k(1 + η(3∣∣w*k2 - 3||w(k)∣∣2)) (i.e. component A of w#kK]) versusiteration t for each neuron k. Subfigure (b) plots 2η PjK6=k((wt(j))>wt(k))wt(j),k + ηwt(k),k PjK6=k kwt(j) k2 (i.e.
Figure 5: Subfigure (a) shows w(k),k (1 + η(3∣∣w*k2 - 3||w(k)∣∣2)) (i.e. component A ofw#(kK),,kt) versus iteration t for each neuron k. Subfigure (b) plots 2η PjK6=k((wt(j))>wt(k))wt(j),k +ηwt(k),k PjK6=k ∣wt(j) ∣2 (i.e. component B of w#(kK),,kt) versus iteration t for each neuron k. Subfigure(c) plots the norm of component C of w(kK⊥, while subfigure (d) plots the norm of component D of(k),⊥w#K,,t for each neuron k. Our empirical findings show that the components due to interaction of theother neurons (i.e. component B and D) are small (notice that the scale of the vertical axis of (a) and(b), (c) and (d) are different) compared to their counterparts (i.e. component A and C respectively),which suggests that θ," U 10-4 on (10) empirically. Top row: number of neurons K = 10. Bottomrow: number of neurons K = 3.
Figure 6: The perpendicular component ∣w(kK⊥ ∣∣ of each k over iterations t. We see that theperpendicular component remains small.
Figure 7: Gradient descent with different values of the step size. Note that the scales of the horizontalaxes are different. Both the step size η and the degree of over-parametrization affect the time thatgradient descent enters the linear convergence regime. A larger step size η and a larger number ofneurons K help gradient descent to make progress faster.
Figure 8: Gradient descent with η = 0.5. We see that gradient descent cannot converge to zerotraining (and testing) error. That is, the step size is too large to converge to a global optimal solution.
Figure 9: We plot the number of iterations required for the metric vec(w* q> -W#K)>V2f (Wt#K)vec(w*q> — WaK) to be positive (the y-axis) under different values of thestep size η (the x-axis).
