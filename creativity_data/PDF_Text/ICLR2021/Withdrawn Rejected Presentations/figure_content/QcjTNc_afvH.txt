Figure 1: While a traditional mapping network produces style vectors to control feature maps, wecreate a stylemap with spatial dimensions, which not only makes the projection of a real image muchmore effective at inference, but also enables local editing. The style map w is resized to w+ throughconvolutional layers to match the spatial resolution of each feature in the synthesis network. Here“A” stands for a learned affine transform, and “Mod” indicates modulation consisting of element-wise multiplication and addition.
Figure 2: Our local editing starts with a learned encoder for fast image-to-stylemap projection. Weestimate the stylemaps w and we of the original x and the reference xe, and transform them to mul-tiple resolutions through the learned stylemap resizer. For each resolution, we calculate the convexcombination of the two stylemaps using the user-defined binary mask m. Finally, the learned gen-erator produces the output using the spatially-mixed stylemaps. The right one shows an examplegenerated using our method.
Figure 3: Local editing comparison across different resolutions of the stylemap. Regions to be dis-carded are faded on the original and the reference images. 4 × 4 suffers from poor reconstruction.
Figure 4: Local editing comparison on CelebA-HQ. The first two baselines even fail to reconstructuntouched region. In-Domain GAN inversion poorly blends the two images, leaking colors to faces,hair, or background, respectively. SEAN locally transfers coarse structure and color but significantlyloses details. Ours seamlessly transplants the target region from the reference to the original.
Figure 5: Local editing comparison on AFHQ. Each row blends the two images with vertical, hor-izontal and custom masks, respectively. Our method seamlessly composes two species with well-preserved details resulting in non-existing creatures, while others tend to lean towards one species.
Figure 6: Examples of unaligned transplantation. StyleMapGAN allows composing arbitrary numberof any regions. Note that the size of the tower and eyes are automatically adjusted regarding thesurroundings. The masks are specified on 8 × 8 grid and the stylemaps are blended on w space.
Figure 7: Local editing comparison across different mapping network structures in the generator. Theautoencoder method without a mapping network is most unnatural in a modified image. The mappingnetwork with convolutional layers has more natural results than the autoencoder. Nevertheless, dueto its bad reconstruction quality, it suffers from preserving the characteristics of original images. OurMLP mapping network is natural in local editing and preserves the original image well. Also, evenif the eye part is not properly inserted like the animal image, it naturally creates it.
