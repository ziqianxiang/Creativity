Figure 1: (A+B) 2D visualization methods, saliency extraction method (left) and activation-maximization (right), are applied to a 3DConv and a 3TConv. The 2D results are somewhat subjectiveto interpretation, especially the activation-maximization. For saliency extraction it is not alwaysclear why the network is finding that particular part of the image salient. In (B) we extract thetemporal parameters and use them to enrich the interpretation of the 2D results. For each channel acombined explanation is generated: the saliency extraction method (left), the activation-maximization(right) and the set of temporal parameters are plotted (bottom). In the right-most plot indicating thetranslation parameters, blue denotes the starting position and red denotes the ending position. Forthe chosen video where the target class is "Pushing Hand Away" we can see that the high-activationchannels contain translation parameters that correspond with what is happening in the original video.
Figure 2: Distributions of the temporal parameters of pretrained 3TConv versions of GoogLeNet andResNet18. Each network has been trained on the Jester dataset for gesture recognition and on theUCF101 dataset for action recognition. Each separate panel represents the distributions of all thelearned temporal parameters across a single model after training on the indicated dataset. In eachseparate panel, three distributions are displayed: (i) the distribution for the scale parameter, (ii) thedistribution for the rotation parameters, and (iii) the joint distribution for the translation parameters.
Figure 3: Distributions of the temporal parameters for the 3TConv versions of GoogLeNet andResNet18 that were trained from scratch. Each network has been trained on the Jester dataset forgesture recognition and on the UCF101 dataset for action recognition. Each separate panel representsthe distributions of all the learned temporal parameters across a single model after training on theindicated dataset. In each separate panel, three distributions are displayed: (i) the distribution for thescale parameter, (ii) the distribution for the rotation parameters, and (iii) the joint distribution for thetranslation parameters. Note that the axis scales are equal for each panel so that a direct comparisoncan be made, between models and well as between datasets.
Figure 4: Comparison LeNet-5-3D and LeNet-5-3T on the video-MNIST dataset, while varying thenumber of samples per class that the models had access to during training.
Figure 5: An example of the toy dataset that we created called Video-MNIST. Ten classes in total,each has a different appearance and dynamic behavior. Each sequence contains 30 frames showingan affine transformation on a single original digit moving in a 28 Ã— 28 pixel frame. The class-specificaffine transformations are restricted to scale, rotation and x, y translations.
