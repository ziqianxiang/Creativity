Figure 1: Overview of our method at test time. Source and driving masks ms and m(d) are gen-erated using the mask generator m. The identity-perturbation operator Ptest is then applied to thedriver’s mask, and along with a scaled-down version of the source’s image D(s) and the source’smask ms, they are fed into the mask refinement network r, in order to generate the driver’s refinedmask md. Next, the refined mask md, the source’s mask ms and the scaled-down source’s imageD(s) are fed into the low-res generator `, which generate the initial prediction c. Finally, the scaled-up refined mask U(md), the source image s, the initial prediction c and the scaled-up source’s maskU(ms) are fed into the high-res generator h, in order to generate the final prediction f.
Figure 2: The method at train time. The driving image d is augmented using A. We generate thedriver’s refined mask md as in test time, except that the operator Ptrain is applied instead of Ptest.
Figure 3:	Intermediate results generated by our method. The final generated frame f is comparedto FOMM and to ablation modelS. From left to right: Source frame s, Source maSk ms, drivingframe d, driving maSk m(d), perturbed driving maSk PteSt(m(d)), refined driving maSk md, low-res prediction c, high-res prediction f, FOMM result, and the ablations: no_pert, which drops Ptest,no_ref, which omits the mask refinement r, and no_id, which omits both.
Figure 4:	Sample results on the three benchmarks. We use the exact same samples as evaluated byFOMM (Siarohin et al., 2019b).
