Figure 1: (a) In training, we sample teams from a set of compositions. The coach observes the entireworld and coordinates different teams via broadcasting strategies periodically; (b) A team withdynamic composition can be viewed as a sequence of fixed composition team, thus the proposedtraining generalizes to dynamic composition; (c) Our method is at the star position within MARL.3the CTDE constraint is too restrictive as each agent only has access to its own decisions and partialenvironmental observations at test time - See Section 3.1 for an example where this requirementcauses failure to learn. The CTDE constraint can be relaxed either by 1) allowing all agents to com-municate with each other (Zhang et al., 2018) or 2) having a special “coach” agent who distributesstrategic information based on the full view of the environment (Stone & Veloso, 1999). The formercase is typically too expensive for many CTDE scenarios (e.g., battery-powered drones or vehicles),while the latter case of having a coach may be feasible (e.g., satellites or watchtowers to monitor thefield in which agents operate). In this work, we focus on the latter approach of having a coach tocoordinate the agents.
Figure 2: The coach-player network architecture. Here, GRU refers to gated recurrent unit (Chunget al., 2014); MLP refers to multi-layer perceptron; FC refers to fully connected layer. Both coachand players use multi-head attention to encode information. The coach has full view while playershave partial views. ha encodes agent a's history. ha combines the most recent strategy z^ = zt-t%τto predict the individual utility Qa. The mixing network combines all Qas to predict Qtot.
Figure 3: An example episode up to t = 30 with communication interval T = 4. Here, ca isrepresented by rgb values, ca = (r, g, b, v). For illustration, we set agents rgb to be one-hot but itcan vary in practice. (i) an agent starts at home; (ii) the invader (black) appears while the agent (red)goes to the red resource; (iii) another agent is spawned while the old agent brings resource home;(iv) one agent goes for the invader while the other for resource; (v-vi) a new agent (blue) is spawnedand goes for the blue resource while other agents (red) are bringing resources home.
Figure 4: Training curves for Resource Collection. (a) comparison against A-QMIX, AI-QMIX andCOPA without the variational objective. Here we choose T = 4; (b) ablations on the communicationinterval T. All results are averaged over 5 seeds.
Figure 5: The varying sensitivityto communication frequency.
