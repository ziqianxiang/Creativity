title,year,conference
 Bayesian filtering unifies adaptive and non-adaptive neural network optimizationmethods,2020, In Advances in Neural Information Processing Systems 33
 Second Order OptimizationMade Practical,2020, arXiv preprint: 2002
 Stochastic Runge-Kutta methods and adaptive SGD-G2 stochasticgradient descent,2020, arXiv preprint: 2002
 Does Adam optimizer keep close to the optimalpoint?,2019, arXiv preprint: 1911
 BGADAM: Boosting based Genetic-Evolutionary ADAM for Convolu-tional Neural Network Optimization,2019, arXiv preprint: 1908
 Neural Optimizer Search withReinforcement Learning,2017, In 34th International Conference on Machine Learning
 Training Neural Networks for and byInterpolation,2020, In 37th International Conference on Machine Learning
 CoolMomentum: A Method for Stochastic Optimizationby Langevin Dynamics with Simulated Annealing,2020, arXiv preprint: 2005
 Practical Gauss-Newton Optimisation for DeepLearning,2017, In 34th International Conference on Machine Learning
 Stochastic gradient descent tricks,2012, In Neural networks: Tricks ofthe trade
 Closingthe generalization gap of adaptive gradient methods in training deep neural networks,2020, In 29thInternational Joint Conference on Artificial Intelligence
 On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization,2019, In 7th International Conference on LearningRepresentations
 NAMSG: An Efficient Method For Training Neural Networks,2019, arXiv preprint:1905
 Momentum with Variance Reduction for Nonconvex Composition Optimiza-tion,2020, arXiv preprint: 2005
 On Empirical Comparisons of Optimizers for Deep Learning,2019, arXiv preprint: 1910
 AdaBatch: Adaptive Batch Sizes forTraining Deep Neural Networks,2017, arXiv preprint: 1712
 An Adaptive and Momental BoundMethod for Stochastic Learning,2019, arXiv preprint: 1910
 Incorporating Nesterov Momentum into Adam,2016, In 4th International Conference onLearning Representations
 diffGrad: An Optimization Method for Convolutional NeuralNetworks,2020, IEEE Transactions on Neural Networks and Learning Systems
 Stochastic Gradient Methods withLayer-wise Adaptive Moments for Training of Deep Networks,2019, arXiv preprint: 1905
 Practical Quasi-Newton Methods for Training DeepNeural Networks,2020, In Advances in Neural Information Processing Systems 33
 Deep Learning,2016, MIT Press
 ShamPoo: Preconditioned Stochastic TensorOPtimization,2018, In 35th International Conference on Machine Learning
 DeeP Residual Learning for ImageRecognition,2016, In IEEE Computer Society Conference on Computer Vision and Pattern Recognition
 Slowing Down the Weight Norm Increase in Momentum-based Optimizers,2020, arXivpreprint: 2006
 Biased Stochastic First-Order Methods for ConditionalStochastic Optimization and Applications in Meta Learning,2020, In Advances in Neural InformationProcessing Systems 33
 Second-order Information in First-order OptimizationMethods,2019, arXiv preprint: 1912
 Nostalgic Adam: Weighting More of the Past GradientsWhen Designing the Adaptive Learning Rate,2019, In 28th International Joint Conference on ArtificialIntelligence
 Adaptive Gradient Methods Can BeProvably Faster than SGD after Finite Epochs,2020, arXiv preprint: 2006
 Adaptive Learning Rate via CovarianceMatrix Based Preconditioning for Deep Neural Networks,2017, In 26th International Joint Conferenceon Artificial Intelligence
 TAdam: A Robust StochasticGradient Optimizer,2020, arXiv preprint: 2003
 Gradient-only line searches: An Alternative to Probabilistic LineSearches,2019, arXiv preprint: 1903
 Gravilon: Applications of a New GradientDescent Method to Machine Learning,2020, arXiv preprint: 2008
 Improving Generalization Performance by Switchingfrom Adam to SGD,2017, arXiv preprint: 1712
 AdaX: Adaptive Gradient Descent withExponential Long Term Memory,2020, arXiv preprint: 2004
 PAGE: A Simple and OptimalProbabilistic Gradient Estimator for Nonconvex Optimization,2020, arXiv preprint: 2008
 On the variance of the adaptive learning rate and beyond,2020, In 8th International Conference onLearning Representations
 Wall crossing invariants from spectral networks,2017, Annales Henri Poincare
 SGDR: Stochastic Gradient Descent with Warm Restarts,2017, In 5thInternational Conference on Learning Representations
 Adaptive Gradient Methods with DynamicBound of Learning Rate,2019, In 7th International Conference on Learning Representations
 Quasi-hyperbolic momentum and Adam for deep learning,2019, In 7thInternational Conference on Learning Representations
 Probabilistic Approaches to Stochastic Optimization,2018, Ph
 MTAdam: Automatic Balancing of Multiple Training Loss Terms,2020, arXivpreprint: 2006
 Optimizing Neural Networks with Kronecker-Factored Approxi-mate Curvature,2015, In 32nd International Conference on Machine Learning
 Variants of RMSProp and Adagrad with LogarithmicRegret Bounds,2017, In 34th International Conference on Machine Learning
 Parabolic Approximation Line Search for DNNs,2020, In Advancesin Neural Information Processing Systems 33
 DADAM: A Consensus-basedDistributed Adaptive Gradient Method for Online Optimization,2019, arXiv preprint: 1901
 CProp: Adaptive Learning Rate Scaling from PastGradient Conformity,2019, arXiv preprint: 1912
 On the Convergence of Adam and Beyond,2018, In 6thInternational Conference on Learning Representations
 L4: Practical loss-based stepsize adaptation for deep learning,2018, InAdvances in Neural Information Processing Systems 31
 Practical Bayesian Learning ofNeural Networks via Adaptive Subgradient Methods,2018, arXiv preprint: 1811
 Domain-independentDominance of Adaptive Methods,2019, arXiv preprint: 1912
 No more pesky learning rates,2013, In 30th InternationalConference on Machine Learning
 VR-SGD: A Simple Stochastic Variance Reduction Method for MachineLearning,2020, IEEE Trans
 Cyclical Learning Rates for Training Neural Networks,2017, In IEEE Winter Conferenceon Applications of Computer Vision
 Super-Convergence: Very Fast Training of Neural NetworksUsing Large Learning Rates,2017, arXiv preprint: 1708
 Adathm: Adaptive Gradient Method Based on Estimates ofThird-Order Moments,2019, In 4th IEEE International Conference on Data Science in Cyberspace
 S-SGD: SymmetricalStochastic Gradient Descent with Weight Noise Injection for Reaching Flat Minima,2020, arXiv preprint:2009
 Adaloss: Adaptive Loss Functionfor Landmark Localization,2019, arXiv preprint: 1908
 Lecture 6,2012,5â€”RMSProp: Divide the gradient by a runningaverage of its recent magnitude
 Calibrating the Adaptive Learning Rate to ImproveConvergence of ADAM,2019, arXiv preprint: 1908
 Compo-sitional ADAM: An Adaptive Compositional Solver,2020, arXiv preprint: 2002
 Attention Is All You Need,2017, In Advances in Neural InformationProcessing Systems 30
 Scheduledrestart momentum for accelerated stochastic gradient descent,2020, arXiv preprint: 2002
 SAdam: A Variant of Adamfor Strongly Convex Functions,2020, In 8th International Conference on Learning Representations
 HyperAdam: A Learnable Task-Adaptive Adam forNetwork Training,2019, In 33rd AAAI Conference on Artificial Intelligence
 Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates,2019, arXiv preprint:1911
 A Walk with SGD,2018, arXivpreprint: 1802
 Momentum-based variance-reduced proXimal stochastic gradient method for compositenonconveX stochastic optimization,2020, arXiv preprint: 2006
 Structured StochasticQuasi-Newton Methods for Large-Scale Optimization Problems,2020, arXiv preprint: 2006
 ADAHESSIAN:An Adaptive Second Order Optimizer for Machine Learning,2020, arXiv preprint: 2006
 Large Batch Training of Convolutional Networks,2017, arXivpreprint: 1708
 Large Batch Optimization for Deep Learn-ing: Training BERT in 76 minutes,2020, In 8th International Conference on Learning Representations
 Stochastic Gradient Methods with Block DiagonalMatriX Adaptation,2019, arXiv preprint: 1905
 AdaptiveMethods for NonconveX Optimization,2018, In Advances in Neural Information Processing Systems 31
 ADADELTA: An Adaptive Learning Rate Method,2012, arXiv preprint: 1212
 Noisy Natural Gradient asVariational Inference,2018, In 35th International Conference on Machine Learning
 YellowFin and the Art of Momentum Tuning,2019, In MachineLearning and Systems
 GADAM: Genetic-Evolutionary ADAM for Deep Neural NetworkOptimization,2018, arXiv preprint: 1805
 Normalized Direction-preserving Adam,2017, arXivpreprint: 1709
 Stochastic Normalized Gradient Descent with Momen-tum for Large Batch Training,2020, arXiv preprint: 2007
 ADAMT: A Stochastic Optimization with TrendCorrection Scheme,2020, arXiv preprint: 2001
 LaProp: a Better Way to Combine Momentumwith Adaptive Gradient,2020, arXiv preprint: 2002
