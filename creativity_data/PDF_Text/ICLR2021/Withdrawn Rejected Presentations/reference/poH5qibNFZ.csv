title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Greedy layerwise learning can scaleto imagenet,2018, CoRR
 On the efficacy of knowledge distillation,2019, In Proceedingsofthe IEEE International Conference on Computer Vision
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Context-dependent pre-trained deep neuralnetworks for large-vocabulary speech recognition,2011, IEEE Transactions on audio
 Large scale distributed deep networks,2012, InAdvances in neural information processing systems
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Mask r-cnn,2017, In Proceedings oftheIEEE international conference on computer vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning deep resnet blockssequentially using boosting theory,2017, CoRR
 Self-supervised knowledge distillationusing singular value decomposition,2018, In The European Conference on Computer Vision (ECCV)
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 In-datacenter performance analy-sis of a tensor processing unit,2017, In Proceedings of the 44th Annual International Symposium onComputer Architecture
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Learning multiple layers of features from tiny images,2009, Technical report
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Progressive neural architecture search,2017, CoRR
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Data-free knowledge distillation for deepneural networks,2017, arXiv preprint arXiv:1710
 Greedy infomax for biologically plausibleself-supervised representation learning,2019, CoRR
 Zero-shot knowledge distillation in deep networks,2019, arXiv preprintarXiv:1905
 Training neural networks with local error signals,2019, arXivpreprint arXiv:1901
 Understanding the exploding gradient prob-lem,2012, CoRR
 Efficient neural architecturesearch via parameter sharing,2018, CoRR
 Faster R-CNN: towards real-timeobject detection with region proposal networks,2015, CoRR
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Learning representations by back-propagating errors,1986, nature
 Weight normalization: A simple reparameterization toaccelerate training of deep neural networks,2016, CoRR
 Deep neural networks for acoustic modeling inspeech recognition,2012, IEEE Signal processing magazine
 Bidirectional attentionflow for machine comprehension,2016, arXiv preprint arXiv:1611
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Residual networks are exponential en-sembles of relatively shallow networks,2016, CoRR
 Extracting andcomposing robust features with denoising autoencoders,2008, In Proceedings of the 25th internationalconference on Machine learning
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Knowledge projection for deep neural networks,2017, arXivpreprint arXiv:1710
 Learning transferable architecturesfor scalable image recognition,2017, CoRR
