title,year,conference
 Sanitychecks for saliency maps,2018, In Proc
 Towards robust interpretability with self-explainingneural networks,2018, In Proc
 Towards better understanding ofgradient-based attribution methods for deep neural networks,2018, In Proc
 Sam: The sensitivity of attribution methods tohyperparameters,2020, arXiv preprint arXiv:2003
 Controlling the false discovery rate via knockoffs,2015, TheAnnals of Statistics
 Interpreting black box models via hypothesistesting,2019, arXiv:1904
 PCANet: A simpledeep learning baseline for image classification,2015, IEEE Transactions on Image Processing
 Explaining imageclassifiers by counterfactual generation,2019, In Proc
 This lookslike that: deep learning for interpretable image recognition,2019, In Proc
 Learning to explain: Aninformation-theoretic perspective on model interpretation,2018, In Proc
 L-shapley and c-shapley: Efficientmodel interpretation for structured data,2019, In Proc
 Robust attribution regular-ization,2019, In Proc
 Real time image saliency for black box classifiers,2017, In Proc
 Adversarial localization network,2017, In Proc
 Interpretable explanations of black boxes by meaningful perturba-tion,2017, In Proc
 Towards automatic concept-basedexplanations,2019, In Proc
 Counterfactual visualexplanations,2019, Proc
 Towards a deepand unified understanding of deep neural models in nlp,2019, In Proc
 Deep residual learning for imagerecognition,2016, In Proc
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, In Proc
 A benchmark for interpretabilitymethods in deep neural networks,2019, In Proc
 Convolutional neural networks for sentence classification,2014, Proc
 On the accuracy of influencefunctions for measuring group effects,2019, In Proc
 Imagenet classification with deep convolu-tional neural networks,2012, In Proc
 The mythos of model interpretability,2016, arXiv:1606
 DeepPINK: reproducible featureselection in deep neural networks,2018, In Proc
 A unified approach to interpreting model predictions,2017, In Proc
 Universaladversarial perturbations,2017, In Proc
 A theoretical explanation for perplexing behaviors ofbackpropagation-based visualizations,2018, In Proc
 Glove: Global vectors for wordrepresentation,2014, In Proc
 Why should i trust you?: Explaining thepredictions of any classifier,2016, In Proc
 In Proc,2011, of ICML
 Restricting the flow: Informationbottlenecks for attribution,2020, 2020
 Grad-cam: Visual explanations from deep networks via gradient-based localiza-tion,2016, arXiv:1611
 Toward generating a new intrusiondetection dataset and intrusion traffic characterization,2018, In Prof
 Learning important features throughpropagating activation differences,2017, In Proc
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv:1409
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2013, arXiv:1312
 Understanding impacts of high-order lossapproximations and features in deep learning interpretation,2019, arXiv:1902
 When explanations lie: Why many modified bpattributions fail,2020, In Proc
 Smoothgrad:removing noise by adding noise,2017, arXiv:1706
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Visualizing the impact of feature attributionbaselines,2020, Distill
 Intriguing properties of neural networks,2013, arXiv:1312
 Representer point selectionfor explaining deep neural networks,2018, In Proc
 Interpreting neural networks using flip points,2019, arXivpreprint arXiv:1903
 Ï„ is introduced byEqn,2017, 3 in Section A6
