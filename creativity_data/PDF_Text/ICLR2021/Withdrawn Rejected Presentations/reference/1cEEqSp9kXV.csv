title,year,conference
 Exponentially many local minima for singleneurons,1996, In Advances in Neural Information Processing Systems 8
 A systematic search method for obtaining multiple local op-timal solutions of non-linear programming problems,1996, IEEE Transactions on Circuits and SystemsI: Fundamental Theory and Applications
 Quasi-stability regions of nonlinear dynamical systems:Theory,1996, IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in Neural Information Processing Systems
 Sharp minima can generalize fordeep nets,2017, In Proceedings of the 34th International Conference on Machine Learning
 Essentially no barriersin neural network energy landscape,2018, In Proceedings of the 35th International Conference onMachine Learning (PMLR)
 Deep ensembles: A loss landscape per-spective,2020, arXiv:1912
 Comparison of particle swarm optimization and back-propagation as training algorithms for neural networks,2003, In Proceedings of the 2003 IEEE SwarmIntelligence Symposium
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Flat minima,1997, Neural Computation
 Densely con-nected convolutional networks,2017, In IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 An emPirical analysis of deeP network losssurfaces,2016, arXiv:1612
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, In The 32th International Conference on Machine Learning(ICML)
 A hybrid of genetic algorithm and particle swarm optimization for recurrentnetwork design,2004, IEEE Transactions on Systems
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations (ICLR)
 Deep learning,2015, Nature
 A dynamical trajectory-based methodology for systemati-cally computing multiple optimal solutions of general nonlinear programming problems,2004, IEEETransactions on Automatic Control
 Tuning of the structure andparameters of a neural network using an improved genetic algorithm,2003, IEEE Transactions onNeural Networks
 Boosted convolutional neural networks,2016, In British Machine Vision Conference (BMVC)
 Trust-tech-based expectation maxi-mization for learning finite mixture models,2008, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Meal: Multi-model ensemble via adversariallearning,2019, In AAAI
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv 1409
 Elite: Ensemble of optimal input-pruned neural networks usingtrust-tech,2011, IEEE Transactions on Neural Networks
 Horizontal and vertical ensemble with deep representa-tion for classification,2013, arXiv:1306
 Local minimafound in the subparameter space can be effective for ensembles of deep convolutional neuralnetworks,2020, Pattern Recognition
 Cyclicalstochastic gradient mcmc for bayesian deep learning,2020, In ICLR
 Enhanced elite-load: A novel cmpsoatt methodologyconstructing short-term load forecasting model for industrial applications,2020, IEEE Transactions onIndustrial Informatics
