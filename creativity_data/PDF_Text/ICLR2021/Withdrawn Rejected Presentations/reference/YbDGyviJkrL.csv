title,year,conference
 Generative pretraining from pixels,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Dual attentionnetwork for scene segmentation,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 A novel spatial-temporal predictionmethod for unsteady wake flows based on hybrid deep neural network,2019, Physics of Fluids
 Openfoam: A c++ library for complexphysics simulations,2007, In International workshop on coupled methods in numerical dynamics
 Hamiltonian systems and transformation in hilbert space,1931, Proceedings of thenational academy of sciences of the united states of america
 Linear predictors for nonlinear dynamical systems: Koopman operatormeets model predictive control,0005, Automatica
 Data-driven spectral analysis of the koopman opera-tor,1063, Applied and Computational Harmonic Analysis
 Pattern formation by interactingchemical fronts,1993, Science
 Prediction of laminar vortex shedding over a cylinder usingdeep learning,2017, arXiv preprint arXiv:1712
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 A practical guide to applying echo state networks,2012, In Neural networks: Tricksofthe trade
 Deep learning for universal linear embeddingsof nonlinear dynamics,2018, Nature communications
 Recurrent neural networkarchitecture search for geophysical emulation,2020, arXiv preprint arXiv:2004
 context2vec: Learning generic context embed-ding with bidirectional lstm,2016, In Proceedings of the 20th SIGNLL conference on computationalnatural language learning
 On numerical approximations of the Koopman operator,2020, arXiv preprintarXiv:2009
 Efficient estimation of word represen-tations in vector space,2013, arXiv preprint arXiv:1301
 Deep dynamicalmodeling and control of unsteady fluid flows,2018, In Advances in Neural Information ProcessingSystems
 Complex patterns in a simple system,1993, Science
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Improving language under-standing by generative pre-training,2018, OpenAI Blog
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Tensorized transformer for dynamical systems modeling,2020, arXivpreprint arXiv:2006
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Graph attention networks,2017, arXiv preprint arXiv:1710
 Latent space physics: Towards learning thetemporal evolution of fluid flow,2019, In Computer Graphics Forum
 Huggingfaceâ€™s transformers: State-of-the-art natural language processing,2019, ArXiv
 Self-attention generativeadversarial networks,2019, In International Conference on Machine Learning
