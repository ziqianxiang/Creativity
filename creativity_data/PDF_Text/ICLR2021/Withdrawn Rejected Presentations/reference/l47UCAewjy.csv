title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2016, arXiv preprint arXiv:1611
 Training a 3-node neural network is np-complete,1989, In Advancesin neural information processing systems
 Interpretable explanations of black boxes by meaningful perturba-tion,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Stiffness: A new perspective ongeneralization in neural networks,2019, arXiv preprint arXiv:1901
 Learning generative con-vnets via multi-grid modeling and sampling,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Estimating information flow in deep neural networks,2019, In InternationalConference on Machine Learning
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 beta-vae: Learning basic visual concepts with aconstrained variational framework,2017, ICLR
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Learning how to explain neural networks: Patternnet and patternat-tribution,2017, arXiv preprint arXiv:1705
 Similarity of neuralnetwork representations revisited,2019, arXiv preprint arXiv:1905
 Tiny imagenet visual recognition challenge,2015, CS 231N
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 A tutorial on energy-basedlearning,2006, Predicting structured data
 On the computational efficiency of trainingneural networks,2014, In Advances in neural information processing systems
 A unified approach to interpreting model predictions,2017, In Advancesin Neural Information Processing Systems
 How to construct deeprecurrent neural networks,2013, arXiv preprint arXiv:1312
 Searching for activation functions,2017, arXivpreprint arXiv:1710
 ”why should I trust you?”： Explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD international conferenceon knowledge discovery and data mining
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Deep inside convolutional networks: visualising imageclassification models and saliency maps,2017, arXiv preprint arXiv:1312
 Maximally informative hierarchical representations of high-dimensional data,2015, In Artificial Intelligence and Statistics
 New theory cracks open the black box of deep learning,2017, In Quanta Magazine
 Information-theoretic analysis of generalization capability of learn-ing algorithms,2017, In Advances in Neural Information Processing Systems
 Understanding neuralnetworks through deep visualization,2015, arXiv preprint arXiv:1506
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 Architectural complexity measures of recurrent neural networks,2016, In Advancesin neural information processing systems
 Object detectorsemerge in deep scene cnns,2015, In ICLR
 Learning deepfeatures for discriminative localization,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
