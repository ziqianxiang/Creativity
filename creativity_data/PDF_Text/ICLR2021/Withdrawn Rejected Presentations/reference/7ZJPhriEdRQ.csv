title,year,conference
 Generating sentences from a continuous space,2015, arXiv preprint arXiv:1511
 InfoGAN:Interpretable representation learning by information maximizing generative adversarial nets,2016, InProc
 Diagnosing and enhancing VAE models,2019, In Proc
 The usual suspects? Reassessing blame for VAE posteriorcollapse,2020, In Proc
 Fromvariational to deterministic autoencoders,2020, In Proc
 Generative adversarial nets,2014, In Proc
 Im-proved training of Wasserstein GANs,2017, In Proc
 Lagging inferencenetworks and posterior collapse in variational autoencoders,2019, In Proc
 beta-VAE: Learning basic visual concepts with aconstrained variational framework,2017, In Proc
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proc
 An introductionto variational methods for graphical models,1999, Machine Learning
 Adam: A method for stochastic optimization,2015, In Proc
 Auto-encoding variational Bayes,2014, In Proc
 Gradient-based learning applied todocument recognition,1998, Proc
 MAE: Mutual posterior-divergence regulariza-tion for variational autoencoders,2019, In Proc
 Spectral normalization forgenerative adversarial networks,2018, In Proc
 Preventing posterior collapse withdelta-VAEs,2019, In Proc
 Generating diverse high-fidelity images withVQ-VAE-2,2019, In Proc
 How does batch nor-malization help optimization? In Proc,2018, Advances in Neural Information Processing Systems(NeurIPS)
 Controllable variational autoencoder,2020, In Proc
 Lad-der variational autoencoders,2016, In Proc
 Wasserstein auto-encoders,2018, In Proc
 Neural discrete representation learn-ing,2017, In Proc
 Spherical latent spaces for stable variational autoencoders,2018, 2018
 InfoVAE: Balancing learning and inference invariational autoencoders,2019, In Proc
