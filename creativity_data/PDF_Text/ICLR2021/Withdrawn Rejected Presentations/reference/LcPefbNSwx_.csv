title,year,conference
 Nonlinear Programming: Analysis and Methods,2003, Dover Publications
 Advances in optimizingrecurrent networks,2013, In IEEE International Conference on Acoustics
 The convergence of a class of double-rank minimization algorithms: 2,1970, the newalgorithm
 Deep generative image modelsusing a laplacian pyramid of adversarial networks,2015, In Advances in Neural Information ProcessingSystems
 A family of variable-metric methods derived by variational means,1970, MathComp
 A rapidly convergent descent method for minimization,1963, ComputerJ
 Laplacian reconstruction and refinement for semantic seg-mentation,2016, 2016
 Deep residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Deep residual learning for image recog-nition,2016, 2016b
 Accelerating stochastic gradient descent using predictive variance reduc-tion,2013, News in physiological ences
 Accelerating stochastic gradient descent using predictive variance reduc-tion,2013, News in physiological ences
 Adam: A method for stochastic optimization,2014, Computer ence
 Learning multiple layers of features from tiny images,2009, ComputerScience Department
 Imagenet classification with deep convo-lutional neural networks,2012, pp
 Deep laplacian pyramidnetworks for fast and accurate super-resolution,2017, In IEEE Conference on Computer Vision andPattern Recognition
 Accelerated parallel optimization meth-ods for large scale machine learning,2014, Computer ence
 On the momentum term in gradient descent learning algorithms,1999, Neural Netw
 A stochastic gradient method with an exponen-tial convergence rate for finite training sets,2012, In International Conference on Neural InformationProcessing Systems
 Conditioning of quasi-newton methods for function minimization,1970, Math
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv 1409
 A survey of optimization methods from amachine learning perspective,2019, IEEE Transactions on Cybernetics
 On the importance of initialization and momentumin deep learning,2013, 30th International Conference on Machine Learning
 Lecture 6e rmsprop: Divide the gradient by a runningaverage of its recent magnitude,2012, COURSERA: Neural networks for machine learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Adadelta: An adaptive learning rate method,2012, Computer ence
