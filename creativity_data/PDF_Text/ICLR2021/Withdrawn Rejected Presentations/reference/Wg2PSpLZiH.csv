title,year,conference
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Unified language model pre-training for natural language understandingand generation,2019, In 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)
 Cross-modal retrieval with correspondence autoen-coder,2014, In Proceedings of the 22nd ACM international conference on Multimedia
 Unsupervised representation learning bypredicting image rotations,2018, In ICLR 2018
 Rich feature hierarchies for ac-curate object detection and semantic segmentation,2014, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Canonical correlation analysis: Anoverview with application to learning methods,2004, Neural computation
 Learning by abstraction: The neural state machine,2019, InAdvances in Neural Information Processing Systems
 Albert: A lite bert for self-supervised learning of language representations,2019, In InternationalConference on Learning Representations
 Stacked cross attention forimage-text matching,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Visualbert: A simpleand performant baseline for vision and language,2019, arXiv preprint arXiv:1908
 Oscar: Object-semantics aligned pre-training for vision-languagetasks,2020, arXiv preprint arXiv:2004
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Fully convolutional netWorks for semanticsegmentation,2015, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Vilbert: Pretraining task-agnostic visiolinguis-tic representations for vision-and-language tasks,2019, In Advances in Neural Information ProcessingSystems
 Im2text: Describing images using 1 millioncaptioned photographs,2011, In Advances in neural information processing systems
 Faster r-cnn: ToWards real-time objectdetection With region proposal netWorks,2015, In Advances in neural information processing systems
 Vl-bert: Pre-trainingof generic visual-linguistic representations,2019, arXiv preprint arXiv:1908
 A corpus forreasoning about natural language grounded in photographs,2018, arXiv preprint arXiv:1811
 Lxmert: Learning cross-modality encoder representations from trans-formers,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Struct-bert: Incorporating language structures into pre-training for deep language understanding,2019, arXivpreprint arXiv:1908
 Multi-modal mutual topic reinforcemodeling for cross-media retrieval,2014, In Proceedings of the 22nd ACM international conference onMultimedia
 Tied transformers: Neural machinetranslation with shared encoder and decoder,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 From image descriptions to visualdenotations: New similarity metrics for semantic inference over event descriptions,2014, Transactionsof the Association for Computational Linguistics
 Ernie-vil: Knowledge enhanced vision-language representations through scene graph,2020, arXiv preprintarXiv:2006
