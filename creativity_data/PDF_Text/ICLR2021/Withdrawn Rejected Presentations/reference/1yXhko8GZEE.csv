title,year,conference
 Banach wasserstein gan,2018, In NeurIPS
 A convergence theory for deep learning via over-parameterization,2019, In ICML
 Towards principled methods for training generative adversarialnetworks,2017, In ICLR
 Wasserstein gan,2017, In ICML
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 The mechanics of n-player differentiable games,2018, arXiv preprint arXiv:1802
 Scaling learning algorithms towards AI,2007, In Large Scale KernelMachines
 Acloser look at the optimization landscapes of generative adversarial networks,2019, arXiv preprintarXiv:1906
 Began: Boundary equilibrium generative adversar-ial networks,2017, arXiv preprint arXiv:1703
 Neural photo editing withintrospective adversarial networks,2016, arXiv preprint arXiv:1609
 Magan: Margin adaptationfor generative adversarial networks,2017, arXiv preprint arXiv:1704
 Training gans withoptimism,2018, In ICLR
 Generative modeling using the slicedwasserstein distance,2018, In CVPR
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Generative adversarial nets,2014, In NeurIPS
 Im-proved training of wasserstein gans,2017, In NeurIPS
 Topics in matrix analysis,1994, Cambridgeuniversity press
 Provable benefit of orthogonal initialization in opti-mizing deep linear networks,2020, arXiv preprint arXiv:2001
 Stacked generativeadversarial networks,2017, In CVPR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In NeurIPS
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Sliced-wasserstein autoencoder: Anembarrassingly simple generative model,2018, arXiv preprint arXiv:1804
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Mmd gan:Towards deeper understanding of moment matching network,2017, In NeurIPS
 Preconditioned stochasticgradient langevin dynamics for deep neural networks,2015, arXiv preprint arXiv:1512
 Towards understanding the dy-namics of generative adversarial networks,2018, In ICML
 Dualing GANs,2017, In NeurIPS
 Unrolled generative adversarialnetworks,2017, In ICLR
 Conditional generative adversarial nets,2014, arXiv preprintarXiv:1411
 cgans with projection discriminator,2018, arXiv preprintarXiv:1802
 Spectral normalizationfor generative adversarial networks,2018, In ICLR
 Fisher gan,2017, In NeurIPS
 Mcgan: Mean and covariance feature matchinggan,2017, arXiv preprint arXiv:1702
 f-gan: Training generative neural samplersusing variational divergence minimization,2016, In NeurIPS
 Conditional image synthesis with auxil-iary classifier gans,2017, In International conference on machine learning
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing systems
 Improved generatorobjectives for gans,2016, arXiv preprint arXiv:1612
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2016, In ICLR
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In Advances in neural information processing systems
 Improved techniques for training gans,2016, In D
 Sliced wasserstein generativemodels,2019, In CVPR
 Disentangling trainability and gener-alization in deep neural networks,2020, 2020
 The unusual effectiveness of averaging in gan training,2019, In ICLR
 Self-attention generativeadversarial networks,2019, In International Conference on Machine Learning
