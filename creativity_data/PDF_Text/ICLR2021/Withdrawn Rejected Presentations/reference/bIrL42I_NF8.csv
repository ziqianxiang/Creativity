title,year,conference
 Stochastic gradient push fordistributed deep learning,2019, In International Conference on Machine Learning
 Optimization methods for large-scale machinelearning,2018, Siam Review
 A downsampled variant of imagenet as analternative to the cifar datasets,2017, arXiv preprint arXiv:1707
 Dual averaging for distributed optimization:Convergence analysis and network scaling,2012, IEEE Transactions on Automatic Control
 Multi30k: Multilingual english-german image descriptions,2016, arXiv preprint arXiv:1605
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations
 Advances and open problems infederated learning,2019, arXiv preprint arXiv:1912
 Decentralized deep learning witharbitrary communication compression,2020, In International Conference on Learning Representations
 Aunified theory of decentralized SGD with changing topology and local updates,2020, In InternationalConference on Machine Learning
 Learning multiple layers of features from tiny images,2009, 2009
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2018, In International Conference on Machine Learning
 Extrapolation for large-batch training indeep learning,2020, In International Conference on Machine Learning
 Swarmsgd: Scalable decentralized sgd with local updates,2020, arXiv preprint arXiv:1910
 Distributed subgradient methods for multi-agent optimization,2009, IEEETransactions on Automatic Control
 Optimalalgorithms for non-smooth distributed optimization in networks,2018, In Advances in Neural InformationProcessing Systems
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 A simple and fast distributed acceleratedgradient method,2019, In OPT2019: 11th Annual Workshop on Optimization for Machine Learning
 The error-feedback framework: Better rates forSGD with delayed gradients and compressed communication,2019, CoRR
 Problems in decentralized decision making and computation,1984, PhD thesis
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 MATCHA: speedingup decentralized SGD via matching decomposition sampling,2019, arXiv preprint arXiv:1905
 Imagenet training inminutes,2018, In Proceedings of the 47th International Conference on Parallel Processing
