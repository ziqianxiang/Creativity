title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information Processing systems
 Neural optimizer search with rein-forcement learning,2017, arXiv PrePrint arXiv:1709
 Discovering governing equations from databy sparse identification of nonlinear dynamical systems,2016, Proceedings of the national academy ofsciences
 Gating creates slow modes and controlsphase-space complexity in grus and lstms,2020, arXiv PrePrint arXiv:2002
 Data-driven discoveryof coordinates and governing equations,2019, PrOceedingS of the NatiOnaI Academy of Sciences
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv PrePrint arXiv:1406
 On empirical comparisons of optimizers for deep learning,2019, arXiv PrePrintarXiv:1910
 Discovering symbolic models from deep learning with inductive biases,2020, arXivPrePrint arXiv:2006
 Adaptive subgradient methods for online learning andstochastic optimization,2011, JOUrnaI of machine Iearning research
 Adam: A method for stochastic optimization,2014, arXiv PrePrintarXiv:1412
 Theory of gating in recurrent neuralnetworks,2020, arXiv PrePrint arXiv:2007
 Learning to optimize,2016, arXiv PrePrint arXiv:1606
 An exponential learning rate schedule for deep learning,2019, arXivPrePrint arXiv:1910
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivPrePrint arXiv:1608
 Aggregated momentum: Stabilitythrough passive damping,2018, arXiv PrePrint arXiv:1804
 Learning gradient descent: Better generalization and longerhorizons,2017, arXiv Preprint arXiv:1703
 How recurrent networks implement contextual process-ing in sentiment analysis,2020, arXiv preprint arXiv:2004
 Uni-versality and individuality in neural dynamics across large populations of recurrent networks,2019, InAdvanceS in neural information ProceSSing systems
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine Iearning
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine Iearning
 Some methods of speeding up the convergence of iteration methods,1964, USSRComPUtationaI MathematicS and MathematicaI Physics
 On the convergence of adam and beyond,2019, arXivPrePrint arXiv:1904
 A stochastic approximation method,1951, The annals ofmathematical StatiStics
 Reverse-engineering recur-rent neural network solutions to a hierarchical inference task for mice,2020, bioRxiv
 No more pesky learning rates,2013, In InternationalConference on Machine Learning
 Acceleration via symplectic discretizationof high-resolution differential equations,2019, In AdvanceS in NeUral Information ProceSSing Systems
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE WinterConference on APPIicationS of ComPUter ViSion (WACV)
 A differential equation for modeling nesterovâ€™saccelerated gradient method: Theory and insights,2014, In AdvanceS in neural information ProceSSingsystems
 Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks,2013, NeUral computation
 Learned optimizers that scale andgeneralize,2017, arXiv Preprint arXiv:1703
 A lyapunov analysis of momentum methodsin optimization,2016, arXiv Preprint arXiv:1611
 Understanding short-horizon bias instochastic meta-optimization,2018, arXiv Preprint arXiv:1803
 Why gradient clipping acceler-ates training: A theoretical justification for adaptivity,2020, In International COnferenCe on LearningRepreSentations
