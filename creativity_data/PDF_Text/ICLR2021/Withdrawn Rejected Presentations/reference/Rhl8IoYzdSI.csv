title,year,conference
 Mixture density networks,1994, 1994
 Weight uncertainty inneural networks,2015, arXiv preprint arXiv:1505
 Modelling het-erogeneous distributions with an uncountable mixture of asymmetric laplacians,2019, In Advances inNeural Information Processing Systems
 Xgboost: A scalable tree boosting system,2016, In Proceedings of the22nd acm sigkdd international conference on knowledge discovery and data mining
 A note on the summation of Chebyshev series,0891, Math
 The wycash portfolio management system,1992, ACM SIGPLAN OOPS Messenger
 Implicit quantile networks for distributional re-inforcement learning,2018, In International Conference on Machine Learning
 Numerical methods in scientific computing,2008, Vol
 Error analysis of an algorithm for summing certain finite series,0263, J
 Dropout as a Bayesian approximation: Representing modeluncertainty in deep learning,2015, arXiv:1506
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Probabilistic backpropagation for scalable learning ofbayesian neural networks,2015, In ICML
 Probabilistic backpropagation for scalable learn-ing of bayesian neural networks,2015, In International Conference on Machine Learning
 Multilayer feedforward networks are uni-versal approximators,1989, Neural networks
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Advances in Neural Information ProcessingSystems
 Classification and regression by randomforest,2002, R news
 On the decay rate of Chebyshev coefficients,2017, Appl
 Error analysis for polynomial evaluation,1974, Math
 A practical monte carlo implementation of bayesian learning,1996, In Advancesin Neural Information Processing Systems
 Stop explaining black box machine learning models for high stakes decisions anduse interpretable models instead,2019, Nature Machine Intelligence
 Hidden technical debt inmachine learning systems,2015, In Advances in neural information processing systems
 Frequentist uncertainty estimates for deep learning,2018, BayesianDeep Learning workshop NeurIPS
 Single-model uncertainties for deep learning,2019, In Advancesin Neural Information Processing Systems
 Bayesian uncertainty estimation for batch nor-malized deep networks,2018, arXiv preprint arXiv:1802
 Is Gauss quadrature better than Clenshaw-Curtis? SIAM Rev,0036,
 Supersparse linear integer models for optimized medical scoringsystems,2016, Machine Learning
 Unconstrained monotonic neural networks,2019, In Advances in NeuralInformation Processing Systems
 A multi-horizon quantile recurrentforecaster,2017, arXiv preprint arXiv:1711
