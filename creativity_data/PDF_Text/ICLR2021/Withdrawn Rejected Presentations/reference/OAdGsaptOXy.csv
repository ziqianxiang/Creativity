title,year,conference
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2019, In Proceedings of the 2019 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies
 Language modelsare unsuPervised multitask learners,2019, OpenAI Blog
 XLNet:Generalized Autoregressive Pretraining for Language Understanding,2019, In Advances in Neural InformationProcessing Systems
 Bert rediscovers the classical nlP PiPeline,2019, arXiv preprintarXiv:1905
 Transformers as soft reasoners over language,2020, arXivpreprint arXiv:2002
 From’f’to’a’on the nyregents science exams: An overview of the aristo Project,2019, arXiv preprint arXiv:1909
 Languagemodels are few-shot learners,2020, 2020
 Retrieval-augmented generation forknowledge-intensive nlP tasks,2020, arXiv preprint arXiv:2005
 Barack’s wife hillary:Using knowledge graPhs for fact-aware language modeling,2019, In Proceedings of ACL
 The curious case of neural textdegeneration,2019, arXiv preprint arXiv:1904
 Scaling laws for neural language models,2020, arXiv preprintarXiv:2001
 Entitiesas experts: Sparse memory access with entity supervision,2020, arXiv preprint arXiv:2004
 Latent relation language models,2020, InProceedings of AAAI 2020
 Entity Linking in Queries: Efficiencyvs,2017,Effectiveness
 What do you learn from context? probingfor sentence structure in contextualized word representations,2019, arXiv preprint arXiv:1905
 Word-entity duet representations for document ranking,2017, InProceedings of the 40th International ACM SIGIR Conference on Research and Development in InformationRetrieval
 Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,2019, In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics
 Momentum contrast for unsupervisedvisual representation learning,2019, arXiv preprint arXiv:1911
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, arXiv preprint arXiv:1705
 Entity tracking improves cloze-style reading compre-hension,2018, In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 What does BERT look at? ananalysis of BERT’s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing andInterpreting Neural Networks for NLP
 Stereoset: Measuring stereotypical bias in pretrainedlanguage models,2020, arXiv preprint arXiv:2004
 Leading conversational search by suggesting useful questions,2020, In Proceedings of The WebConference 2020
 Grounded conversation generation asguided traverses in commonsense knowledge graphs,2020, In Proceedings of ACL
