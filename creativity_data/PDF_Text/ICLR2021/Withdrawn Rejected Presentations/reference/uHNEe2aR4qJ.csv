title,year,conference
 Critical learning periods in deep neuralnetworks,2017, arXiv preprint arXiv:1711
 Task2vec: Task embedding for meta-learning,2019, InProceedings ofthe IEEE International Conference on Computer Vision
 Curriculum learning,2009, InProceedings of the 26th annual international conference on machine learning
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Development of language-specific phoneme representations in the infant brain,1998, Natureneuroscience
 Eigenvalues of covariance matrices: Application toneural-network learning,1991, Phys
 Note on learning rate schedules for stochastic optimization,1991, InAdvances in neural information processing systems
 Neuroplasticity subserving motor skill learning,2011, Neuron
 Asymptotics of wide networks from feynman diagrams,2019, arXiv preprintarXiv:1909
 Thedifficulty of training deep architectures and the effect of unsupervised pre-training,2009, In ArtificialIntelligence and Statistics
 Towards robust evaluations of continual learning,2018, arXiv preprintarXiv:1805
 850-855 vol,1999,2
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Simplifying neural nets by discovering flat minima,1995, InAdvances in Neural Information Processing Systems 7
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Continual learning for robotics,2019, arXiv preprint arXiv:1907
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Revisiting batch normalizationfor practical domain adaptation,2016, arXiv preprint arXiv:1603
 Bad global minima exist andSGD can reach them,2019, CoRR
 The stability-plasticity dilemma: Inves-tigating the continuum from catastrophic forgetting to age-limited learning effects,2013, Frontiers inpsychology
 Toward understanding catastrophic forgetting in continual learning,2019, arXiv preprintarXiv:1908
 Centering neural network gradient factors,1998, In Neural Networks: Tricks of theTrade
 Towards flatter losssurface via nonmonotonic learning rate scheduling,2018, In UAI
 Convolutional neural networks applied tohouse numbers digit classification,2012, In Proceedings of the 21st International Conference on PatternRecognition (ICPR2012)
 Lifelong machine learning systems: Beyond learningalgorithms,2013, In 2013 AAAI spring symposium series
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE Winter Conferenceon Applications ofComputer Vision (WACV)
 Training very deep networks,2015, InAdvances in neural information processing systems
 Lifelong robot learning,1995, Robotics and autonomous systems
 The Unreasonable effectiveness of the forget gate,2018, arXivpreprint arXiv:1804
 A perspective view and sUrvey of meta-learning,2002, Artificialintelligence review
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2018, arXivpreprint arXiv:1803
