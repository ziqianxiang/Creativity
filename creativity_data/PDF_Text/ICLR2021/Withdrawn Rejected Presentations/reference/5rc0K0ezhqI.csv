title,year,conference
 Deep variational informationbottleneck,2016, arXiv preprint arXiv:1612
 Learning representations for neural network-basedclassification using the information bottleneck principle,2019, IEEE Transactions on Pattern Analysisand Machine Intelligence
 Mutual information neural estimation,2018, In International Conferenceon Machine Learning
 Learning deep architectures for ai,2009, Foundations and trendsR in MachineLearning
 A renyi entropy convolution inequality with application,2002, In 200211th European Signal Processing Conference
 Understanding disentangling in β-vae,2018, arXiv preprint arXiv:1804
 Efficient and scalable bayesian neural nets with rank-1factors,2020, arXiv preprint arXiv:2005
 The conditional entropy bottleneck,2020, arXiv preprint arXiv:2002
 Ceb improves model robustness,1099, Entropy
 The Conditional Entropy Bottleneck,2019, Submission to ICLR 2019
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Fromvariational to deterministic autoencoders,2019, arXiv preprint arXiv:1903
 An empirical investi-gation of catastrophic forgetting in gradient-based neural networks,2013, arXiv preprint arXiv:1312
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, Lecture Notes in Computer Science
 beta-vae: Learning basic visual concepts with aconstrained variational framework,2016, 2016
 Connectionist learning procedures,1990, In Machine learning
 Bayesian active learning forclassification and preference learning,2011, arXiv preprint arXiv:1112
 Multi-sample dropout for accelerated training and better generalization,2019, arXiv preprintarXiv:1905
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Batchbald: Efficient and diverse batchacquisition for deep bayesian active learning,2019, In Advances in Neural Information ProcessingSystems
 Learning multiple layers of features from tiny images,2009, 2009
 Adversarial machine learning at scale,2017, 2017
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Multivariate information transmission,1954, Transactions of the IRE Professional Groupon Information Theory
 Deepfool: a simple andaccurate method to fool deep neural networks,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Regularizing deep neuralnetworks by noise: Its interpretation and optimization,2017, In Advances in Neural InformationProcessing Systems
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 On variationalbounds of mutual information,2019, arXiv preprint arXiv:1905
 Learning and generalization with the informationbottleneck,2010, Theoretical Computer Science
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Deep learning and the information bottleneck principle,2015, In 2015IEEE Information Theory Workshop (ITW)
 On mutualinformation maximization for representation learning,2019, arXiv preprint arXiv:1907
 Regularization of neuralnetworks using dropconnect,2013, In International conference on machine learning
 Regularization of neuralnetworks using dropconnect,2013, In International conference on machine learning
 A theory of usableinformation under computational constraints,2020, arXiv preprint arXiv:2002
 A new outlook on shannon’s information measures,1991, IEEE transactions oninformation theory
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Deep mutual learning,2018, InProceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
