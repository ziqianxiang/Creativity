title,year,conference
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, In Maria FlorinaBalcan and Kilian Q
 Exact information propagation through fully-connectedfeed forward neural networks,2018, arXiv preprint arXiv:1806
 A comprehensive and modu-larized statistical framework for gradient norm equality in deep neural networks,2020, arXiv preprintarXiv:2001
 Model compression and hardwareacceleration for neural networks: A comprehensive survey,2020, Proceedings of the IEEE
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Batch normalization: Accelerating deep network trainingby reducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Self-normalizingneural networks,2017, In Advances in neural information processing systems
 Towards understanding regularizationin batch normalization,2018, arXiv preprint arXiv:1809
 An empirical study of batch normalization and group normalization inconditional computation,2019, arXiv preprint arXiv:1908
 Mixedprecision training,2018, In International Conference on Learning Representations
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In Advances in neural information processing Systems
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 The singular values of convolutional layers,2019, InInternational Conference on Learning Representations
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 l1-norm batch nor-malization for efficient training of deep neural networks,2018, IEEE transactions on neural networksand learning systems
 mixup: Beyond em-pirical risk minimization,2018, In International Conference on Learning Representations
 Fixup initialization: Residual learning withoutnormalization,2019, arXiv preprint arXiv:1901
