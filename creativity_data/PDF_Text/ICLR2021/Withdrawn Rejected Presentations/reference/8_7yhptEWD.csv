title,year,conference
 A learning rule for asynchronous perceptrons with feedback in a combinatorialenvironment,1990, In Artificial neural networks: concept learning
 Deep equilibrium models,2019, In Advances in NeuralInformation Processing Systems
 Multiscale deep equilibrium models,2020, arXivpreprintarXiv:2006
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 Neural ordinarydifferential equations,2018, In Advances in neural information processing systems
 Deep convolutional net-works as shallow gaussian processes,2018, arXivpreprint arXiv:1808
 Dynamical isometry and a mean field theory of lstms and grus,2019, arXiv preprintarXiv:1901
 Why do deep residual networks gener-alize better than deep feedforward networks?-a neural tangent kernel perspective,2020, arXiv preprintarXiv:2002
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 Finite versus infinite neural networks: an empirical study,2020, arXivpreprint arXiv:2007
 Bayesian deep convolutional networks with manychannels are gaussian processes,2018, arXiv preprint arXiv:1810
 Generalization of back propagation to recurrent and higher order neural net-works,1988, In Neural information processing systems
 A hybrid method for nonlinear equations,1970, Numerical methods for nonlinearalgebraic equations
 Deep informationpropagation,2016, arXiv PreprintarXiv:1611
 Monotone operator equilibrium networks,2020, arXiv preprintarXiv:2006
 Tensor programs i: Wide feedforward or recurrent neural networks of any architectureare gaussian processes,2019, arXiv preprint arXiv:1910
 Tensor programs ii: Neural tangent kernel for any architecture,2020, arXiv preprintarXiv:2006
