title,year,conference
 Meta-learning deepenergy-based memory models,2019, arXiv preprint arXiv:1910
 Mine: mutual information neural estimation,2018, arXiv preprintarXiv:1801
 Self-supervised gans viaauxiliary rotation loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Exponentialfamily estimation via adversarial dynamics embedding,2019, In Advances in Neural InformationProcessing Systems
 Residualenergy-based models for text generation,2020, arXiv preprint arXiv:2004
 Implicit generation and generalization in energy-based models,2019, arXivpreprint arXiv:1903
 Model based planning with energy based models,2019, arXivpreprint arXiv:1909
 Energy-based models foratomic-resolution protein conformations,2020, arXiv preprint arXiv:2004
 Meta-learning and universality: Deep representations and gradientdescent can approximate any learning algorithm,2017, arXiv:1710
 Learning generative convnetsvia multi-grid modeling and sampling,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Improvedtraining of Wasserstein gans,2017, In NIPS
 Reinforcement learning Withdeep energy-based policies,2017, arXiv preprint arXiv:1702
 A baseline for detecting misclassified and out-of-distributionexamples in neural networks,2016, arXiv preprint arXiv:1610
 Products of experts,1999, International Conference on Artificial Neural Networks
 Training products of experts by minimizing contrastive divergence,2002, NeuralComput
 Denoising diffusion probabilistic models,2020, arXiv preprintarXiv:2006
 Selective exPerience rePlay for lifelong learning,2018, arXiv preprintarXiv:1802
 DeeP directed generative models with energy-based Probabilityestimation,2016, arXiv preprint arXiv:1606
 Adam: A method for stochastic oPtimization,2015, In ICLR
 Maximum entroPy generatorsfor energy-based models,2019, arXiv preprint arXiv:1901
 Wasserstein introsPective neural networks,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Stein variational Policy gradient,2017, arXivpreprint arXiv:1704
 Unifying non-maximum likelihood learning objectives with minimum kl contraction,2011, InAdvances in Neural Information Processing Systems
 Unrolled generative adversarialnetworks,2016, 11 2016
 SPectral normalization forgenerative adversarial networks,2018, arXiv preprint arXiv:1802
 On the anatomy of mcmc-based maximum likelihood learning of energy-based models,2019, arXiv preprint arXiv:1903
 Learning non-convergent non-persistent short-run mcmc toward energy-based model,2019, In Advances in Neural InformationProcessing Systems
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2016, In ICLR
 Experiencereplay for continual learning,2019, In Advances in Neural Information Processing Systems
 A contrastive divergence for combining variationalinference and mcmc,2019, arXiv preprint arXiv:1905
 Deep boltzmann machines,2009, In David A
 Deep energy estimatornetworks,2018, arXiv preprint arXiv:1805
 Deep unsupervisedlearning using nonequilibrium thermodynamics,2015, arXiv preprint arXiv:1503
 Learning neural random fields with inclusive auxiliary generators,2018, arXivpreprint arXiv:1806
 Training restricted boltzmann machines using approximations to the likelihoodgradient,1064, In Proceedings of the 25th international conference on Machine learning
 Pixel recurrent neural networks,2016, InICML
 Generative models ofvisually grounded imagination,2018, In ICLR
 A theory of generative convnet,2016, InInternational Conference on Machine Learning
 Synthesizing dynamic patterns by spatial-temporalgenerative convnet,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Integrating episodic memory into a reinforcementlearning agent using reservoir sampling,2018, arXiv preprint arXiv:1806
 Lsun:Construction of a large-scale image dataset using deep learning with humans in the loop,2015, arXivpreprint arXiv:1506
001 at eachiteration,2020, The data augmentation transform consists of color augmentation of strength 1
 Thus theKL loss serves as a regularizer to prevent EBM sampling from collapsing,2018, In the absence of the KLloss
