title,year,conference
 There are many con-sistent explanations of unlabeled data: Why you should average,2019, ICLR
 Mutual information neural estimation,2018, In International Conferenceon Machine Learning
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Sensitivity and generalization in neural networks: an empirical study,2018, arXiv preprintarXiv:1802
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 On varia-tional bounds of mutual information,2019, arXiv preprint arXiv:1905
 Improved deep metric learning with multi-class n-pair loss objective,2016, In Advances inneural information processing systems
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 On mutualinformation maximization for representation learning,2019, arXiv preprint arXiv:1907
 Understanding contrastive representation learning through align-ment and uniformity on the hypersphere,2020, arXiv preprint arXiv:2005
 Principal component analysis,1987, Chemometrics andintelligent laboratory systems
 Local aggregation for unsupervised learningof visual embeddings,2019, In Proceedings of the IEEE International Conference on Computer Vision
