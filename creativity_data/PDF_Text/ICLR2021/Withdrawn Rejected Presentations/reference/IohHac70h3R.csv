title,year,conference
 Convex Optimization,0521, Cambridge University Press
 On the generalization ability of on-linelearning algorithms,2004, IEEE Transacitons on Information Theory
 Deep residUal learning for imagerecognition,2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Nostalgic adam: Weighting more of the past gradientswhen designing the adaptive learning rate,2019, arXiv preprint arXiv: 1805
 Adam: A method for stochastic optimization,2015, Proceedings ofthe 3rd International Conference on Learning Representations (ICLR)
 Adax: Adaptive gradient descent withexponential long term memory,2020, arXiv preprint arXiv:2004
 Microsoft COCO:common objects in context,2014, CoRR
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, Proceedings of 7th International Conference on Learning Representations
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 Rmsprop: Divide the gradient by a running average of itsrecent magnitude,2012, COURSERA: Neural networks for machine learning
 Adaptive methodsfor nonconvex optimization,2018, Advances in Neural Information Processing Systems 31
 Adashift:Decorrelation and convergence of adaptive learning rate methods,2019, Proceedings of 7th InternationalConference on Learning Representations (ICLR)
1 Proof of Theorem 4,2021,2 and Theorem C
1 and the drop out probability was 0,2015,2
