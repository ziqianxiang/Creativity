title,year,conference
 Towards modular algorithminduction,2020, arXiv preprint arXiv:2003
 Learning to represent programswith graphs,2018, In International Conference on Learning Representations
 Finding and counting given length cycles,1997, Algorithmica
 Computing receptive fields of convolutional neuralnetworks,2019, Distill
 Diffusion-convolutional neural networks,2016, In Advances in neuralinformation processing systems
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 A pencil-and-paper algorithm for solving sudoku puzzles,2009, Notices of the AMS
 Graphonomy:Universal human parsing via graph transfer learning,2019, In CVPR
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Neural GPUs learn algorithms,2015, arXiv preprint arXiv:1511
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Learning multiple layers of features from tiny images,2009, 2009
 Improved quantum algorithm for triangle finding via combinatorial arguments,2014, In2014 IEEE 55th Annual Symposium on Foundations of Computer Science
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 There is no 16-clue sudoku: Solving thesudoku minimum number of clues problem via hitting set enumeration,2014, Experimental Mathematics
 Neural network for graphs: A contextual constructive approach,2009, IEEE Transactionson Neural Networks
 Solving every sudoku puzzle,2009, Preprint
 Recurrent relational networks,2018, In Advances in NeuralInformation Processing Systems
 Improving neural program synthesis with inferredexecution traces,2018, In Advances in Neural Information Processing Systems
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Mastering chess and shogiby self-play with a general reinforcement learning algorithm,2017, arXiv preprint arXiv:1712
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Graph attention networks,2017, arXiv preprint arXiv:1710
 Satnet: Bridging deep learning andlogical reasoning using a differentiable satisfiability solver,2019, arXiv preprint arXiv:1905
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Acomprehensive survey on graph neural networks,2019, arXiv preprint arXiv:1901
 Multi-scale context aggregation by dilated convolutions,2015, arXivpreprint arXiv:1511
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 Root mean square layer normalization,2019, In Advances in NeuralInformation Processing Systems
 8 depicts an example of an unfilled Sudoku puzzle with 17 givens and its solution obtainedby the Matrix Shuffle-Exchange network,2018, For training
