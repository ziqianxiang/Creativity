title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Entropy-sgd: Biasing gradient descent into widevalleys,2017, In ICLR
 Angular visual hardness,2020, In ICML
 A closer look atfew-shot classification,2019, arXiv preprint arXiv:1904
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2017, arXiv preprint arXiv:1712
 Model-agnostic meta-learning for fast adaptation of deepnetworks,2017, In ICML
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In AISTATS
 Implicit bias of gradient descent on linearconvolutional networks,2018, In NeurIPS
 Flat minima,1997, Neural Computation
 Iterative algorithms for gram-Schmidt orthogonalization,1989, Computing
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In ICML
 Speeding up convolutional neural networks withlow rank expansions,2014, In BMVC
 Orthogonal deep neural networks,2019, TPAMI
 Dynamic filter networks,2017, In NeurIPS
 Deep learning without poor local minima,2016, In NeurIPS
 Imagenet classification with deep convolutionalneural networks,2019, In NeurIPS
 Cheap orthogonal constraints in neural networks: Asimple parametrization of the orthogonal and unitary group,2019, In ICML
 Visualizing the loss landscape ofneural nets,2018, In NeurIPS
 Efficient riemannian optimization on the stiefel manifold via thecayley transform,2020, In ICLR
 Algorithmic regularization in over-parameterized matrixsensing and neural networks with quadratic activations,2018, In COLT
 Network in network,2015, arXiv preprint arXiv:1312
 Learning towardsminimum hyperspherical energy,2018, In NeurIPS
 Neural similarity learning,2017, In NeurIPS
 Deephyperspherical learning,2017, In NeurIPS
 Pointnet: Deep learning on point sets for 3dclassification and segmentation,2017, In CVPR
 Deep isometric learning for visualrecognition,2020, arXiv preprint arXiv:2006
 Random features for large-scale kernel machines,2019, In NeurIPS
 Stochastical approximation of smooth convex bodies,2004, Mathematika
 No bad local minima: Data independent training error guarantees formultilayer neural networks,2020, arXiv preprint arXiv:1605
 Factorized convolutional neural networks,2017, In ICCVWorkshops
 Full-capacity unitaryrecurrent neural networks,2016, In NeurIPS
 Diverse neural network learns true target functions,2017, In AISTATS
