title,year,conference
 Knowledgedistillation from internal representations,2019, arXiv preprint arXiv:1910
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Wasserstein gan,2017, arXiv preprintarXiv:1701
 Remixmatch: Semi-supervised learning with distribution alignment and augmentationanchoring,2019, arXiv preprint arXiv:1911
 Mixmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems
 Vicinal risk minimization,2001, InAdvances in neural information processing systems
 On the efficacy of knowledge distillation,2019, In Proceedings ofthe IEEE International Conference on Computer Vision
 Feature-map-level online adversarialknowledge distillation,2020, In Proceedings of International Conference on Machine Learning
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Mixup as locally linear out-of-manifold regulariza-tion,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 A com-prehensive overhaul of feature distillation,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 Knowledge transfer via distillationof activation boundaries formed by hidden neurons,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Paraphrasing complex network: Network compressionvia factor transfer,2018, In Advances in Neural Information Processing Systems
 Lit: Learned intermediaterepresentation training for model compression,2019, In International Conference on Machine Learning
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, arXiv preprint arXiv:2002
 Teacher-student compression with generative adversar-ial networks,2018, arXiv preprint arXiv:1812
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Improved knowledge distillation via teacher assistant,2020, AAAI Conference on ArtificialIntelligence
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Correlation congruence for knowledge distillation,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Fitnets: Hints for thin deep nets,2015, In International Conference on LearningRepresentations
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Transformation invariancein pattern recognitionâ€”tangent distance and tangent propagation,1998, In Neural networks: tricks ofthe trade
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Knowledge transfer with Jacobian matching,2018, In Jennifer Dy andAndreas Krause (eds
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE International Conference on Computer Vision
 Kdgan: Knowledge distillation with generativeadversarial networks,2018, In Advances in Neural Information Processing Systems
 Knowledge distillation meets self-supervision,2020, In European Conference on Computer Vision
 Wide residual networks,2016, In BMVC
 Paying more attention to attention: Improving theperformance of convolutional neural networks via attention transfer,2017, In International Conferenceon Learning Representations
 mixup: Beyond empiricalrisk minimization,2018, International Conference on Learning Representations
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Knowledge distillation by on-the-fly native ensemble,2018, InAdvances in neural information processing systems
