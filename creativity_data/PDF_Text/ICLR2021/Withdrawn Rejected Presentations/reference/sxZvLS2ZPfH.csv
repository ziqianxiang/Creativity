title,year,conference
 XNLI: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Adam: A method for stochastic optimization,2015, In ICML
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 The third international Chinese language processing bakeoff: Word segmenta-tion and named entity recognition,2006, In Proceedings of the Fifth SIGHAN Workshop on ChineseLanguage Processing
 Deep contextualized word representations,2018, In Proc
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, 2019
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 Attention is all you need,2017, In NIPS
