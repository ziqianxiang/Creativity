title,year,conference
 Rezero is all you need: Fast convergence at large depth,2020, arXiv preprintarXiv:2003
 vq-wav2vec: Self-supervised learning ofdiscrete speech representations,2019, arXiv preprint arXiv:1910
 The perceptron: A model for brain functioning,1962, i
 The use of the area under the roc curve in the evaluation of machine learningalgorithms,1997, Pattern recognition
 Freezeout: Accelerate trainingby progressively freezing layers,2017, arXiv preprint arXiv:1706
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, arXiv preprintarXiv:2005
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Masked language modeling for pro-teins via linearly scalable long-context transformers,2020, arXiv preprint arXiv:2006
 Super-vised learning of universal sentence representations from natural language inference data,2017, arXivpreprint arXiv:1705
 Geometrical and statistical properties of systems of linear inequalities with ap-plications in pattern recognition,1965, IEEE transactions on electronic computers
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Depth-adaptive transformer,2019, arXivpreprint arXiv:1910
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Deep randomized neural networks,2020, In Recent Trendsin Learning From Data
 Echo state neural machine translation,2020, arXiv preprintarXiv:2002
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Information processing using transient dynamics of semiconductor lasers sub-ject to delayed feedback,2013, Selected Topics in Quantum Electronics
 Extreme learning machine: theory andapplications,2006, Neurocomputing
 Adaptive nonlinear system identification with echo state networks,2003, In Advances inneural information processing systems
 Effects of noise on convergence and generalization inrecurrent networks,1995, In Advances in neural information processing systems
 An analysis of noise in recurrent neural networks:convergence and generalization,1996, IEEE Transactions on neural networks
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, arXiv preprint arXiv:2006
 Convolutional neural networks for sentence classification,2014, arXiv preprintarXiv:1408
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Reservoir computing approaches to recurrent neural net-work training,2009, Computer Science Review
 Event-driven randomback-propagation: Enabling neuromorphic deep learning machines,2017, Frontiers in neuroscience
 Randomizedautomatic differentiation,2020, arXiv preprint arXiv:2007
 Learning and generalization characteristicsof the random vector functional-link net,1994, Neurocomputing
 Improving transformer models by reordering theirsublayers,2019, arXiv preprint arXiv:1911
 Languagemodels are unsupervised multitask learners,2018, 2018
 Random features for large-scale kernel machines,2008, In Advances inneural information processing systems
 Weighted sums of random kitchen sinks: Replacing minimizationwith randomization in learning,2009, In Advances in neural information processing systems
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Compact hard-ware for real-time speech recognition using a liquid state machine,2007, pp
 Green ai,2019, arXiv preprintarXiv:1907
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Recent advances in physical reser-voir computing: A review,2019, Neural Networks
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Bert rediscovers the classical nlp pipeline,2019, arXivpreprint arXiv:1905
 Deep image prior,2018, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 No training required: Exploring random encoders for sentenceclassification,2019, arXiv preprint arXiv:1901
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 Language modeling teaches you more than translation does:Lessons learned through auxiliary syntactic task analysis,2018, In Proceedings of the 2018 EMNLPWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
