title,year,conference
 Thefifth PASCAL recognizing textual entailment challenge,2009, In TAC
 Closing the gener-alization gap of adaptive gradient methods in training deep neural networks,2018, arXiv preprintarXiv:1806
 On the convergence ofa class of adam-typealgorithms for non-convex optimization,2019, ICLR
 The PASCAL recognising textual entailmentchallenge,2006, In Machine learning challenges
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 The second pascal recognising textual entailment challenge,2006, In Proceedings of theSecond PASCAL Challenges Workshop on Recognising Textual Entailment
 Deep residual learning for image recog-nition,2016, In CVPR
 Adam: A method for stochastic optimization,2015, In Yoshua Bengioand Yann LeCun (eds
 Albert: A lite bert for self-supervised learning of language representations,2020, ICLR
 The Winograd schema challenge,2011, InAAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning
 On the variance of the adaptive learning rate and beyond,2020, ICLR
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv:1907
 Adaptive gradient methods with dynamicbound of learning rate,2019, ICLR
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv:1910
 On the convergence of adam and beyond,2018, ICLR
 No more pesky learning rates,2013, In ICML
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Attention is all you need,2017, In NeurIPS
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In NAACL
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Neurips
 Understanding short-horizon bias instochastic meta-optimization,2018, arXiv:1803
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In ICLR
 Adaptive meth-ods for nonconvex optimization,2018, In NeurIPS
 ADADELTA: an adaptive learning rate method,2012, CoRR
 Which algorithmic choices matter at which batch sizes? insightsfrom a noisy quadratic model,2019, In NeurIPS
 Why adam beats sgd for attention models,2019, arXiv:1912
 Freelb: Enhancedadversarial training for natural language understanding,2020, In ICLR
 Adabelief optimizer: Adapting stepsizes by the belief inobserved gradients,2020, In NeurIPS
 Laprop: a better way to combine momentum withadaptive gradient,2020, arXiv:2002
