title,year,conference
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,0027, Proceedings of the National Academy ofSciences
 Bayesian experimental design: A review,1995, StatisticalScience
 Numerical methods forA-optimal designs with a sparsity constraint for ill-posed inverse problems,2012, ComputationalOptimization and Applications
 Batch mode active learning and itsapplication to medical image classification,2006, In Proceedings of the 23rd International Conferenceon Machine Learning
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 More data can hurt for linear regression: Sample-wise double descent,2019, arXivpreprint arXiv:1912
 Deepdouble descent: Where bigger models and more data hurt,2020, In 8th International Conference onLearning Representations
 Optimal regularization canmitigate double descent,2020, arXiv preprint arXiv:2003
 Neural tangents: Fast and easy infinite neural networks in Python,2020, InInternational Conference on Learning Representations
 Bayesian batchactive learning as sparse subset approximation,2013, In Advances in Neural Information ProcessingSystems
 Optimal design of experiments,2006, SIAM
 Active learning for convolutional neural networks: A core-setapproach,2021, In 6th International Conference on Learning Representations
 Single shot active learning using pseudo annotators,2019, PatternRecognition
 Active learning via transductive experimental design,2017, InProceedings of the 23rd International Conference on Machine Learning
