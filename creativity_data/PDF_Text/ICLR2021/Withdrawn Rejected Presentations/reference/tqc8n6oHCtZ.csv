title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Once for all: Train one network and specialize it for efficientdeployment,2019, arXiv preprint arXiv:1908
 Funnel-transformer: Filtering out sequentialredundancy for efficient language processing,2020, arXiv preprint arXiv:2006
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Depth-adaptive transformer,2019, arXivpreprint arXiv:1910
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Dynabert: Dynamic bert with adaptive width anddepth,2020, arXiv preprint arXiv:2004
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Fastbert: a self-distillingbert with adaptive inference time,2020, arXiv preprint arXiv:2004
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the aaai conference on artificial intelligence
 Learning ordered representations with nesteddropout,2014, In International Conference on Machine Learning
 Green ai,2019, arXiv preprintarXiv:1907
 The righttool for the job: Matching model and instance complexities,2020, arXiv preprint arXiv:2004
 Compression of neural machinetranslation models via pruning,2016, arXiv preprint arXiv:1606
 Megatron-lm: Training multi-billion parameter language models using gpu modelparallelism,2019, arXiv preprint arXiv:1909
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Multi-scaletransformer language models,2020, arXiv preprint arXiv:2005
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Flops as a direct optimization objective forlearning sparse neural networks,2018, arXiv preprint arXiv:1811
 Branchynet: Fast inferencevia early exiting from deep neural networks,2016, In 2016 23rd International Conference on PatternRecognition (ICPR)
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Transformers: State-of-the-artnatural language processing,2019, arXiv preprint arXiv:1910
 Deebert: Dynamic early exiting foraccelerating bert inference,2020, arXiv preprint arXiv:2004
