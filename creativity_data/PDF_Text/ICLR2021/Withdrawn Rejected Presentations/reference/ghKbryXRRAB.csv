title,year,conference
 Controlling computation versus quality forneural sequence models,2020, CoRR
 What does BERT lookat? An analysis of BERT’s attention,2019, CoRR
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 What BERT is not: Lessons from a new suite of psycholinguistic diagnostics forlanguage models,2020, Transactions of the
 Reducing transformer depth on demand withstructured dropout,2020, In International Conference on Learning Representations
 Assessing BERT’s syntactic abilities,2019, CoRR
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Dynabert: Dynamic bertwith adaptive width and depth,2020, In Advances in Neural Information Processing Systems
 Revealing the dark secrets ofBERT,2019, In Proceedings of the Conference on Empirical Methods in Natural Language Processingand International Joint Conference on Natural Language Processing
 Building machines that learn and think likepeople,2017, Behavioral and brain sciences
 ALBERT: A lite BERT for self-supervised learning of language representations,2020, In 8thInternational Conference on Learning Representations
 FastBERT: a self-distilling BERT with adaptive inference time,2020, In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics
 RoBERTa: A robustly optimized BERT pre-training approach,2019, CoRR
 Distributed rep-resentations of words and phrases and their compositionality,2013, In Advances in Neural InformationProcessing Systems 26
 WordNet: A lexical database for english,1995, Communications of the ACM
 Deep contextualized word representations,2018, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Improving transformer models by reordering theirsublayers,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Lan-guage models are unsupervised multitask learners,2018, 2018
 Visualizing and measuring the geometry of BERT,2019, In Advances in Neural InformationProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019
 Theright tool for the job: Matching model and instance complexities,2020, In Proc
 An analysis of attention mechanisms: The case ofword sense disambiguation in neural machine translation,2018, In Proceedings of the Third ConferenceonMachine Translation
 BERT rediscovers the classical NLP pipeline,2019, InProceedings of the 57th Conference of the Association for Computational Linguistics
 What do you learnfrom context? probing for sentence structure in contextualized word representations,2019, In 7thInternational Conference on Learning Representations
 A multiscale visualization of attention in the transformer model,2019, In Proceedings of the57th Conference of the Association for Computational Linguistics
 DeeBERT: Dynamic early exitingfor accelerating BERT inference,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 An analysis of BERT indocument ranking,2020, In Proceedings of the 43rd International ACM SIGIR conference on researchand development in Information Retrieval
