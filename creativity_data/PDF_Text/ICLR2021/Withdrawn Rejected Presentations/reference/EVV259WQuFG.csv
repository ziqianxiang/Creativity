title,year,conference
 Neural Reading Comprehension and Beyond,2018, PhD thesis
 Gestalt: a stacking ensemble for squad2,2020,0
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In Proceedings of the 55th Annual Meetingof the Association for Computational Linguistics
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 SentencePiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing: System Demonstrations
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Ms marco: A human generated machine reading comprehension dataset,2016, November 2016
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Bidirectional attentionflow for machine comprehension,2017, In 5th International Conference on Learning Representations
 Attention is all you need,2017, In I
 Making neural qa as simple as possible butnot simpler,2017, Proceedings of the 21st Conference on Computational Natural Language Learning(CoNLL 2017)
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, In Advancesin Neural Information Processing Systems 32
 Hierarchicalattention networks for document classification,2016, In Proceedings of the 2016 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Machine reading comprehension: a literaturereview,2019, CoRR
 Retrospective reader for machine reading comprehen-sion,2020, ArXiv
 Machine reading comprehension: The role ofcontextualized language models and beyond,2020, arXiv preprint arXiv:2005
 Neural structural correspondence learning for domain adaptation,2017, InProceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)
