title,year,conference
 Convergence of alternating optimization,2003, Neural
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Efficient training of bertby progressively stacking,2019, In International Conference on Machine Learning
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 schubert: Optimizing elements of bert,2020, arXiv preprintarXiv:2005
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Alternating minimizations converge to second-orderoptimal solutions,2019, In International Conference on Machine Learning
 K-bert: En-abling language representation with knowledge graph,2019, arXiv preprint arXiv:1909
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Structbert: Incor-porating language structures into pre-training for deep language understanding,2019, arXiv preprintarXiv:1908
 Large batch optimization for deep learning: Trainingbert in 76 minutes,2019, arXiv preprint arXiv:1904
