title,year,conference
 A little is enough: Circumventing defenses fordistributed learning,2019, arXiv preprint arXiv:1902
 signSGD withmajority vote is communication efficient and fault tolerant,2018, arXiv preprint arXiv:1810
 Draco: Byzantine-resilient distributed training via redundant gradients,2018, arXiv preprint arXiv:1803
 Aggregathor: Byzantine machine learning via robustgradient aggregation,2019, Conference on Systems and Machine Learning (SysML) 2019
 Robust federated learning in aheterogeneous environment,2019, arXiv preprint arXiv:1906
 Advances and open problems infederated learning,2019, arXiv preprint arXiv:1912
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 The hidden vulnerability ofdistributed learning in byzantium,2018, arXiv preprint arXiv:1802
 Detox: A redundancy-based framework for faster and more robust gradient aggregation,2019, arXiv preprint arXiv:1907
 Generalized Byzantine-tolerant SGD,2018, arXivpreprint arXiv:1802
 Fall of Empires: Breaking Byzantine-tolerant SGD byInner Product Manipulation,2019, arXiv preprint arXiv:1903
 ByRDiE: Byzantine-resilient distributed coordinate descent fordecentralized learning,2019, IEEE Transactions on Signal and Information Processing over Networks
 Byzantine-robust distributedlearning: Towards optimal statistical rates,2018, arXiv preprint arXiv:1803
 Defending against saddle pointattack in byzantine-robust distributed learning,2018, arXiv preprint arXiv:1806
