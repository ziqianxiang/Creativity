title,year,conference
 Classification from pairwise similarity and unlabeleddata,2018, In ICML
 Rademacher and gaussian complexities: Risk bounds andstructural results,2002, JMLR
 Deep learning for classical Japanese literature,2018, arXiv preprint arXiv:1812
 Classificationfrom triplet comparison data,2020, Neural Computation
 Analysis of learning from positive andunlabeled data,2014, In NeurIPS
 Convex formulation for learning frompositive and unlabeled data,2015, In ICML
 Learning with multiplecomplementary labels,2020, In ICML
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Loss de-composition and centroid estimation for positive and unlabeled learning,2019, TPAMI
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Deep residual learning for imagerecognition,2016, In CVPR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Learning from complementarylabels,2017, In NeurIPS
 Binary classification for positive-confidencedata,2018, In NeurIPS
 Complementary-labellearning for arbitrary losses and models,2019, In ICML
 Active classification with com-parison queries,2017, In FOCS
 Adam: A method for stochastic optimization,2015, In ICLR
 Positive-unlabeledlearning with non-negative risk estimator,2017, In NeurIPS
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 On the minimal supervision fortraining any binary classifier from only unlabeled data,2019, In ICLR
 Mitigating overfitting in supervisedclassification from two unlabeled datasets: A consistent risk correction approach,2020, In AISTATS
 Progressive identifi-cation of true labels for partial-label learning,2020, In ICML
 Learning from cor-rupted binary labels via class-probability estimation,2015, In ICML
 Learning withnoisy labels,2013, In NeurIPS
 Read-ing digits in natural images with unsupervised feature learning,2011, In NeurIPS Workshop on DeepLearning and Unsupervised Feature Learning
 Squared-lossmutual information regularization: A novel information-theoretic approach to semi-supervisedlearning,2013, In ICML
 Learning with confident examples: Rankpruning for robust classification with noisy labels,2017, In UAI
 Learning from crowds,2010, JMLR
 Semi-supervised auc optimization based onpositive-unlabeled learning,2018, MLJ
 Multiple instancelearning convolutional neural networks for object recognition,2016, In ICPR
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In NeurIPS
 Combating noisy labels by agreement: Ajoint training method with co-regularization,2020, In CVPR
 Whose voteshould count more: Optimal integration of labels from labelers of unknown expertise,2009, In NeurIPS
 Fashion-mnist: A novel image dataset for bench-marking machine learning algorithms,2017, arXiv preprint arXiv:1708
 Uncoupled regression from pairwisecomparison data,2019, In NeurIPS
 Noise-tolerantinteractive learning using pairwise comparisons,2017, In NeurIPS
 Regression with com-parisons: Escaping the curse of dimensionality with ordinal information,2020, JMLR
 Learning with biased comple-mentary labels,2018, In ECCV
 Multi-instance learning by treating instances as non-iidsamples,2009, InICML
 Introduction to semi-supervised learning,2009, Synthesis Lectureson Artificial Intelligence and Machine Learning
