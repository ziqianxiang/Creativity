title,year,conference
 Riemannianwalk for incremental learning: Understanding forgetting and intransigence,2018, In Proceedings of theEuropean Conference on Computer Vision (ECCV)
 Continual learning with tiny episodicmemories,2019, arXiv preprint arXiv:1902
 Continual learning: A comparative study on how to defy forgettingin classification tasks,2019, arXiv preprint arXiv:1909
 Towards robust evaluations of continual learning,2018, arXiv preprintarXiv:1805
 Pathnet: Evolution channels gradient descent in super neuralnetworks,2017, arXiv preprint arXiv:1701
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Continual learning via neural pruning,2019, arXivpreprint arXiv:1903
 An empirical investi-gation of catastrophic forgetting in gradient-based neural networks,2013, arXiv preprint arXiv:1312
 Re-evaluating continual learningscenarios: A categorization and case for strong baselines,2018, arXiv preprint arXiv:1810
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcomingcatastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Limitations of the empirical fisher approxima-tion,2019, arXiv preprint arXiv:1905
 Lca: Loss change allocation for neuralnetwork training,2019, In Advances in Neural Information Processing Systems
 Learn to grow: Acontinual structure learning framework for overcoming catastrophic forgetting,2019, arXiv preprintarXiv:1904
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 New insights and perspectives on the natural gradient method,2014, arXiv preprintarXiv:1412
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 Continual learning by asymmetricloss approximation with single-side overestimation,2019, In Proceedings of the IEEE InternationalConference on Computer Vision
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 Connectionist models of recognition memory: constraints imposed by learning andforgetting functions,1990, Psychological review
 icarl:Incremental classifier and representation learning,2017, In Proceedings of the IEEE conference onComputer Vision and Pattern Recognition
 Topmoumoute online natural gradientalgorithm,2008, In Advances in neural information processing systems
 Continual learning with deep generativereplay,2017, In Advances in Neural Information Processing Systems
 ImProving and under-standing variational continual learning,2019, arXiv preprint arXiv:1905
 Three scenarios for continual learning,2019, arXiv preprintarXiv:1904
 Continuallearning with hyPernetworks,2019, arXiv preprint arXiv:1906
