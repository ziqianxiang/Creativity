title,year,conference
 YouTube-8M: A large-scale video classificationbenchmark,2016, arXiv preprints
 Variancereduction in SGD by distributed importance sampling,2016, In ICLR Workshop
 Adaptive importance sampling to accelerate training of a neural proba-bilistic language model,2008, IEEE Transactions on Neural Networks
 Curriculum learning,2009, In ICML
 Understanding batch normal-ization,2018, In NIPS
 Active bias: Training moreaccurate neural networks by emphasizing high variance samples,2017, In NIPS
 Exact sampling of determinantal pointprocesses with sublinear time preprocessing,2019, In NIPS
 Learning what data to learn,2017, In ICLRWorkshop
 Curriculum-guided hindsightexperience replay,2019, In NIPS
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In AISTATS
 Automatedcurriculum learning for neural networks,2017, In ICML
 Deep residual learning for image recog-nition,2016, In CVPR
 Flat minima,1997, Neural Computation
 Long short-term memory,1997, Neural Computation
 Efficient diversified mini-batch selectionusing variable high-layer features,2019, In ACML
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Not all samples are created equal: Deep learning withimportance sampling,2018, In ICML
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InICLR
 Adam: a method for stochastic optimization,2015, In ICLR
 Learning mUltiple layers of featUres from tiny images,2009, 2009
 Self-paced learning for latent variablemodels,2010, In NIPS
 Adaptive active learning for image classification,2013, In CVPR
 Online batch selection for faster training of neUral networks,2016, InICLR Workshop
 Accelerating minibatch stochastic gradient descent Using typ-icality sampling,2019, IEEE Transactions on Neural Networks and Learning Systems (Early Access)
 Efficient neUral architectUresearch via parameter sharing,2018, In ICML
 LangUagemodels are UnsUpervised mUltitask learners,2019, OpenAI Blog
 Prioritized experience replay,2016, InICLR
 Training region-based object detectorswith online hard example mining,2016, In CVPR
 Very deep convolUtional networks for large-scale imagerecognition,2015, In ICLR
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine Learning
 The general inefficiency of batch training for gradientdescent learning,2003, Neural Networks
 Wide residual networks,2016, In BMVC
 Active mini-batch samPlingusing rePulsive Point Processes,2019, In AAAI
 Stochastic oPtimization with imPortance samPling for regularized lossminimization,2015, In ICML
 Neural architecture search with reinforcement learning,2017, In ICML
