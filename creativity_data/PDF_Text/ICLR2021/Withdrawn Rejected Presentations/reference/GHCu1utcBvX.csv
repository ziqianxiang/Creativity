title,year,conference
 Deep learning of representations: Looking forward,2013, In International Conference onStatistical Language and Speech Processing
 The consciousness prior,2017, arXiv preprint arXiv:1709
 Understanding disentangling in β-vae,2018, arXiv preprint arXiv:1804
 Syntactic structures,1957, Walter de Gruyter
 The compositionality papers,2002, Oxford University Press
 A survey on concept driftadaptation,2014, ACM computing surveys (CSUR)
 Permutation eqivariant models forcompositional generalization in language,2020, In ICLR
 Scan: Learning hierarchical compositional visual concepts,2018, In ICLR
 β-vae: Learning basic visual concepts with a con-strained variational framework,2017, In International Conference on Learning Representations (ICLR)
 Learning todecompose and disentangle representations for video prediction,2018, In Advances in Neural InformationProcessing Systems
 Clevr: A diagnostic dataset for compositional language and elementary visualreasoning,2017, In CVPR
 Measureing compositional generalization: a comprehensive method on realisticdata,2020, In ICLR
 Disentangling by factorising,2018, In International Conference onMachine Learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 More systematic than claimed: Insights on the scan tasks,2018, OpenReview
 Variational inference of disen-tangled latent concepts from unlabeled observations,2017, In International Conference on LearningRepresentations (ICLR)
 Generalization without systematicity: On the compositional skillsof sequence-to-sequence recurrent networks,2018, In International Conference on Machine Learning
 Compositional generalization through meta sequence-to-sequence learning,2019, InAdvances in Neural Information Processing Systems
 Buildingmachines that learn and think like people,2017, Behavioral and Brain Sciences
 Human few-shot learning of compositionalinstructions,2019, arXiv preprint arXiv:1901
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, Transactions of the Association for Computational Linguistics
 Challenging common assumptions in the unsupervised learning of dis-entangled representations,2019, In International Conference on Machine Learning
 Disentangling factors of variations using few labels,2020, In ICLR
 Rearranging the familiar: Testing compositionalgeneralization in recurrent networks,2018, arXiv preprint arXiv:1807
 Rethinking eliminative connectionism,1998, Cognitive psychology
 The algebraic mind: Integrating connectionism and cognitive science,2003, MIT press
 Society of mind,1986, Simon and Schuster
 Universal grammar,1970, Theoria
 A survey on domain adaptationtheory,2020, arXiv preprint arXiv:2004
 Recurrent neural networks can learn to implement symbol-sensitivecounting,1998, In Advances in Neural Information Processing Systems
 Compositional generalization in a deep seq2seqmodel by separating syntax and semantics,2019, arXiv preprint arXiv:1904
 Task representations in neural networks trained to perform many cognitive tasks,2019, Natureneuroscience
