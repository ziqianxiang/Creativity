title,year,conference
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Evaluating NLP Models via Contrast Sets,2004, arXiv:2004
 Shortcut Learning in Deep Neural Networks,2004, arXiv:2004
 Breaking nli systems with sentences that requiresimple lexical inferences,2018, arXiv preprint arXiv:1805
 Annotation Artifacts in Natural Language Inference Data,1803, arXiv:1803
 Bag of tricks for efficient textclassification,2017, ArXiv
 How much reading does reading comprehension require?a critical investigation of popular benchmarks,2018, arXiv preprint arXiv:1808
 Learning the Difference that Makes aDifference with Counterfactually-Augmented Data,2020, arXiv:1909
 Convolutional neural networks for sentence classification,2014, arXiv preprint arXiv:1408
 Right for the wrong reasons: Diagnosing syntacticheuristics in natural language inference,2019, arXiv preprint arXiv:1902
 Adversarialnli: A new benchmark for natural language understanding,2019, arXiv preprint arXiv:1910
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, In Advances in Neural Information Processing Systems
 Huggingfaceâ€™s transformers: State-of-the-artnatural language processing,2019, ArXiv
 Theadversarial attack and detection under the fisher information metric,2019, In Proceedings of the AAAIConference on Artificial Intelligence
