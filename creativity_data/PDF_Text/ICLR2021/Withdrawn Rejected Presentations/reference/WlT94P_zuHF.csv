title,year,conference
 Memory transformer,2020, CoRR
 Generating long sequences with sparsetransformers,2019, CoRR
 Masked language modeling for pro-teins via linearly scalable long-context transformers,2020, CoRR
 Transformer-XL: Attentive language models beyond a fixed-length context,2019, In Proceedingsofthe 57th Conference of the Association for Computational Linguistics
 Funnel-Transformer: Filtering out sequen-tial redundancy for efficient language processing,2020, CoRR
 Injecting hierarchy with U-Net transform-ers,2019, CoRR
 GMAT: global memory augmentation for transformers,2020, CoRR
 Transformers areRNNs: Fast autoregressive transformers with linear attention,2020, CoRR
 Reformer: The efficient transformer,2020, In 8thInternational Conference on Learning Representations
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting,2019, In Advances in Neural Information Processing Systems 32: NeurIPS 2019
 Pointer sentinel mixturemodels,2017, In 5th International Conference on Learning Representations
 SimpleBooks: Long-term dependency book dataset with simplified English vocab-ulary for word-level language modeling,2019, CoRR
 Hierar-chical transformers for long document classification,2019, In IEEE Automatic Speech Recognition andUnderstanding Workshop
 Multi-scaletransformer language models,2020, CoRR
 Cluster-former: Clustering-based sparse transformer for long-range dependencyencoding,2020, CoRR
 Linformer: Self-attentionwith linear complexity,2020, CoRR
 BP-Transformer: Modellinglong-range context via binary partitioning,2019, CoRR
 Big bird: Transformersfor longer sequences,2020, CoRR
 HIBERT: Document level pre-training of hierarchicalbidirectional transformers for document summarization,2019, In Proceedings of the 57th Conferenceof the Association for Computational Linguistics
