title,year,conference
 Language gans falling short,2018, In NIPS Workshop
 Efficientsoftmax approximation for gpus,2016, arXiv perprint arXiv:1609
 On calibration of modern neuralnetowrks,2017, In ICML
 Distilling the knowledge in a neural network,2014, InNIPS
 Long short-term memory,1997, Neural computation
 Toward con-trolled generation of text,2017, In ICML
 Categorical reparameterization with gumbel-softmax,2017, InICLR
 Dynamic evaluation of neuralsequence models,2017, arXiv preprint arXiv:1709
 Learning when to concentrate or divertattention: Self-adaptive attention temperature for neural machine translation,2018, In EMNLP
 Softmax q-distribution estimation for structured prediction: A theoretical interpretation for raml,2017, arXivpreprint arXiv:1705
 Building a large annotatedcorpus of english: The penn treebank,1993, In Computational linguistics
 Pointer sentinel mixturemodels,2017, In ICLR
 Regularizing and optimizing lstm lan-guage models,2018, In ICLR
 Empirical eval-uation and combination of advanced language modeling techniques,2011, In INTERSPEECH
 Automatic differentiation inPyTorch,2017, In NIPS Autodiff Workshop
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, arXiv preprint arXiv:1909
 Breaking the softmaxbottleneck: A high-rank RNN language model,2018, In ICLR
 Heated-up softmaxembedding,2018, arXiv preprint arXiv:1809
