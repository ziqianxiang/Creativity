title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Communication complexity of distributed convex learning andoptimization,2015, In Advances in neural information processing systems
 Practical secure aggregation for privacy-preserving machine learning,2017, In Proceedings of the 2017 ACM SIGSAC Conference on Computerand Communications Security
 Towardsfederated learning at scale: System design,2019, arXiv preprint arXiv:1902
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT’2010
 Leaf: A benchmark for federated settings,2018, arXiv preprint arXiv:1812
 On the outsized importance of learning rates in local updatemethods,2020, arXiv preprint arXiv:2007
 Momentum improves normalized SGD,2020, arXiv preprintarXiv:2002
 On the ineffectiveness of variance reduced optimization for deeplearning,2019, In Advances in Neural Information Processing Systems
 Compiling machine learning programs viahigh-level tracing,2018, Systems for Machine Learning
 Differentially private federated learning: A clientlevel perspective,2017, arXiv preprint arXiv:1712
 FedBoost: Communication-efficientalgorithms for federated learning,2020, In 37th International Conference on Machine Learning (ICML)
 Training keyword spotting models on non-iid datawith federated learning,2020, arXiv preprint arXiv:2005
 Byzantine-robust learning on heterogeneousdatasets via resampling,2020, arXiv preprint arXiv:2006
 The Non-IID data quagmireof decentralized machine learning,2019, arXiv preprint arXiv:1910
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Error feedbackfixes SignSGD and other gradient compression schemes,2019, In 36th International Conference onMachine Learning (ICML)
 SCAFFOLD: Stochastic controlled averaging for on-device federatedlearning,2020, In 37th International Conference on Machine Learning (ICML)
 Tighter theory for local SGD onindentical and heterogeneous data,2020, In Proceedings of AISTATS
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 A unifiedtheory of decentralized SGD With changing topology and local updates,2020, In 37th InternationalConference on Machine Learning (ICML)
 Federated optimiza-tion: Distributed machine learning for on-device intelligence,2016, arXiv preprint arXiv:1610
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Agnostic federated learning,2019, arXivpreprint arXiv:1902
 Communication trade-offs for synchronized dis-tributed SGD with large step size,2019, In 33rd Conference on Neural Information Processing Systems(NeurIPS)
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Aide: Fastand communication efficient distributed optimization,2016, arXiv preprint arXiv:1608
 Communication-efficient distributed optimization usingan approximate newton-type method,2014, In International conference on machine learning
 Local SGD converges fast and communicates little,2019, International Conferenceon Learning Representations (ICLR)
 The error-feedback framework: Better rates forSGD with delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 On the importance of initial-ization and momentum in deep learning,2013, In International conference on machine learning
 Understanding Unin-tended memorization in federated learning,2020, arXiv preprint arXiv:2006
 Hybrid stochastic gradi-ent descent algorithms for stochastic nonconvex oPtimization,2019, arXiv preprint arXiv:1905
 Fast and faster convergence of SGD for over-Parameterized models and an accelerated PercePtron,2018, arXiv preprint arXiv:1810
 Tackling the objective in-consistency Problem in heterogeneous federated oPtimization,2020, arXiv preprint arXiv:2007
 SlowMo: ImProvingcommunication-efficient distributed sgd with slow momentum,2020, International Conference onLearning Representations (ICLR)
 Minibatch vs local SGD for heteroge-neous distributed learning,2020, arXiv preprint arXiv:2006
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Large batch oPtimization for deePlearning: Training bert in 76 minutes,2019, In International Conference on Learning Representations
 On the linear sPeeduP analysis of communication efficient mo-mentum sgd for distributed non-convex oPtimization,2019, arXiv preprint arXiv:1905
 Parallel restarted SGD with faster convergence and lesscommunication: Demystifying why model averaging works for deeP learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 AdaPtive meth-ods for nonconvex oPtimization,2018, In Advances in neural information processing systems
 Why ADAM beats SGD for attention models,2019, arXiv preprintarXiv:1912
 Federatedlearning with non-iid data,2018, arXiv preprint arXiv:1806
 Parallelized stochastic gradientdescent,2010, In Advances in neural information processing systems
 The second termcan be removed since η ≤ L,2021, Taking expectation on both sides and using the update variance boundLemma 9
