title,year,conference
 Constrained policy optimization,2017, arXivpreprint arXiv:1705
 Solving rubikâ€™s cube with arobot hand,2019, arXiv preprint arXiv:1910
 Constrained markov decision processes with total cost criteria: Lagrangian approachand dual linear program,1998, Mathematical methods ofoperations research
 Safe model-basedreinforcement learning with stability guarantees,2017, In Advances in neural information processingsystems
 Risk-constrained markov decision processes,2010, In 49th IEEE Conferenceon Decision and Control (CDC)
 A lyapunov-based approach to safe reinforcement learning,2018, In Advances in neural information processingsystems
 Lyapunov-based safe policy optimization for continuous control,2019, arXiv preprintarXiv:1901
 Safe exploration in continuous action spaces,2018, arXiv preprint arXiv:1801
 Qt-opt: Scalable deepreinforcement learning for vision-based robotic manipulation,2018, arXiv preprint arXiv:1806
 Being optimistic to beconservative: Quickly learning a cvar policy,2019, arXiv preprint arXiv:1911
 Benchmarking Safe Exploration in Deep Reinforce-ment Learning,2019, 2019
 Trust regionpolicy optimization,2015, In International conference on machine learning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Optimizing the cvar via sampling,2014, arXiv preprintarXiv:1404
 Worst cases policy gradients,2019, arXivpreprint arXiv:1911
 Safe exploration for interactive machinelearning,2019, In Advances in Neural Information Processing Systems
 Grandmasterlevel in starcraft ii using multi-agent reinforcement learning,2019, Nature
 Safe reinforcement learning in constrained markov decision pro-cesses,2020, arXiv preprint arXiv:2008
 Cautious adapta-tion for reinforcement learning in safety-critical settings,2020, arXiv preprint arXiv:2008
