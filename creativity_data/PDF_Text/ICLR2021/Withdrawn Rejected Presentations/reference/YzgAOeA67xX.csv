title,year,conference
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, arXiv preprint arXiv:1806
 Mxnet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, arXiv preprint arXiv:1512
 Comparing biases for minimal network construction withback-propagation,1989, In Advances in neural information processing systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Neural networks for machine learning lecture6a overview of mini-batch gradient descent,2012, 2012
 Long short-term memory,1997, Neural computation
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Adam: A method for stochastic optimization,2015, 3rd InternationalConference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 Reconciling modern deep learning with traditionaloptimization analyses: The intrinsic learning rate,2020, Advances in Neural Information ProcessingSystems
 On the variance of the adaptive learning rate and beyond,2019, In International Conference onLearning Representations
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Adaptive gradient methods with dynamicbound of learning rate,2019, 7th International Conference on Learning Representations
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 On the convergence of adam and beyond,2019, 6thInternational Conference on Learning Representations
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 A diffusion theory for minima selection: Stochasticgradient descent escapes from sharp minima exponentially fast,2020, arXiv preprint arXiv:2002
 Adai: Separating theeffects of adaptive learning rate and momentum inertia,2020, arXiv preprint arXiv:2006
 Adaptive methodsfor nonconvex optimization,2018, In Advances in neural information processing systems
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
 Heavy-ball method in nonconvex optimization problems,1993, ComputationalMathematics and Modeling
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
