title,year,conference
 TensorFlow: Large-scale machine learning on heterogeneoussystems,2015, 2015
 Structured pruning of deep convolu-tional neural networks,2017, 13(3):1—18
 A survey of model compression andacceleration for deep neural networks,2017, 2017
 Learning FastAlgorithms for Linear Transforms Using Butterfly Factorizations,2019, volume 97
 Exploitinglinear structure within convolutional networks for efficient evaluation,2014, In Advances inNeural Information Processing Systems
 QuicK-means: Accelerationof K-means by learning a fast transform,2019, 2019
 Understanding the difficulty of training deep feedforwardneural networks,2010, In Proceedings of the Thirteenth International Conference on ArtificialIntel ligence and Statistics
 Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification,2015, 2015
 Deep Residual Learning forImage Recognition,2016, pp
 Adam: A method for stochastic optimization,2014, In arXivPreprint arXiv:1412
 Fastfood — Approximating Kernel Expansions inLoglinear Time,2013, pp
 Speeding-up convolutionalneural networks using fine-tuned CP-decomposition,2015, In 3rd International Conference onLearning Representations
 Gradient-based learningapplied to document recognition,1998, 86(11):2278—2324
 Sparseconvolutional neural networks,2015, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Rethinking theValue of Network Pruning,2018, 2018
 Variational Dropout SparsifiesDeep Neural Networks,2017, 2017
 TensorizingNeural Networks,2015, 2015
 Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets,2013, pp
 Very Deep Convolutional Networks for Large-ScaleImage Recognition,2015, 2015
 Compressing deep neural networks withsparse matrix factorization,2019, 2019
 Deep Fried Convnets,2015, 2015
