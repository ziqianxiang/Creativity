title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning
 Comparing Dynamics : Deep NeuralNetworks versus Glassy Systems,2018, In International Conference on Machine Learning
 Geometry of Energy Landscapes and the Optimizabil-ity of Deep Neural Networks,2020, Physical Review Letters
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National AcademyOfSciencesofthe UnitedStatesofAmerica
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 On Lazy Training in Differentiable Program-ming,2019, In Neural Information Processing Systems
 Random deep neural networks are biasedtowards simple functions,2019, Advances in Neural Information Processing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Disentangling feature and lazytraining in deep neural networks,2019, arXiv preprint arXiv:1906
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of Machine Learning Research
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Polylogarithmic width suffices for gradient descent to achieve arbi-trarily small test error with shallow ReLU networks,2019, arXiv preprint arXiv:1909
 Imagenet classification with deep convo-Iutional neural networks,2012, In Advances in Neural Information Processing Systems
 Deep learning,2015, Nature
 Gradient Descent Maximizes the Margin of Homogeneous Neural Net-works,2020, In International Conference on Learning Representations
 On the number of linearregions of deep neural networks,2014, In Advances in Neural Information Processing Systems
 Generalization in Deep Networks: The Role of Distancefrom Initialization,2017, In Advances in Neural Information Processing Systems
 A Modern Take on the Bias-Variance Tradeoff in Neural Net-works,2019, In Workshop on Identifying and Understanding Deep Learning Phenomena
 Exploring Gener-alization in Deep Learning,2017, In Advances in Neural Information Processing Systems
 The roleof over-parametrization in generalization of neural networks,2019, In International Conference onLearning Representations
 Gradient Descent can Learn LessOver-parameterized Two-layer Neural Networks on Classification Problems,2019, arXiv preprintarXiv:1905
 Deep learning generalizes becausethe parameter-function map is biased towards simple functions,2019, In International Conference onLearning Representations
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Advances in NeuralInformation Processing Systems
 Depth-width tradeoffs in approximating natural functions with neuralnetworks,2017, In International Conference on Machine Learning
 Practical Bayesian Optimization of MachineLearning Algorithms,2012, In Advances in Neural Information Processing Systems
 UnderstandingDeep Learning Requires Rethinking of Generalization,2017, In International Conference on LearningRepresentations
 Gradient descent optimizes over-parameterized deep ReLU networks,2020, Machine Learning
