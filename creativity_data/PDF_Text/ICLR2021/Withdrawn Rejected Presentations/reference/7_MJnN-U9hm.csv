title,year,conference
"â€œlearning-compression"" algorithms for neural netpruning",2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Dynamic network surgery for efficient dnns,2016, InAdvances in neural information processing systems
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in neural information processing systems
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Pruning versus clipping in neural networks,1989, Physical Review A
 A simple procedure for pruning back-propagation trained neural networks,1990, IEEEtransactions on neural networks
 Radix-net: Structured sparse matrices for deep neural net-works,2019, In 2019 IEEE International Parallel and Distributed Processing Symposium Workshops(IPDPSW)
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Importance estimationfor neural network pruning,2019, In Proceedings of the IEEE Conference on Computer Vision andPatternRecognition
 Lookahead: a far-sighted alternative ofmagnitude-based pruning,2020, arXiv preprint arXiv:2002
 Deep expander networks: Efficient deepnetworks from graph theory,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Stochasticnet: Forming deepneural networks via stochastic connectivity,2016, IEEE Access
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Mean field residual networks: On the edge of chaos,2017, In Advancesin neural information processing systems
 Drawing early-bird tickets: Towards more efficienttraining of deep networks,2019, arXiv preprint arXiv:1909
