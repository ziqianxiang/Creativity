title,year,conference
 On the cross-lingual transferability ofmonolingual representations,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Multilingual alignment of contextual word repre-sentations,2020, In International Conference on Learning Representations
 Syntactic structures,2002, Walter de Gruyter
 Cross-lingual language model pretraining,2019, In Advances inNeural Information Processing Systems
 Un-supervised cross-lingual representation learning at scale,2019, arXiv preprint arXiv:1911
 Association for Computational Linguistics,1600, doi:10
 Cross-lingualpre-training based transfer for zero-shot neural machine translation,2020, In Proceedings of the AAAIConference on Artificial Intelligence
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Multilingual denoising pre-training for neural machine translation,2020, arXivpreprint arXiv:2001
 Automatic domain adaptation for pars-ing,2010, In Human Language Technologies: The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguistics
 Efficient estimation of word represen-tations in vector space,2013, arXiv preprint arXiv:1301
 Polyglot contextual representations improvecrosslingual transfer,2019, In Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Towards robust cross-domain domain adaptation for part-of-speech tagging,2013, In Proceedings of the Sixth International Joint Conference on Natural LanguageProcessing
 Probing prior knowledge needed in challengingchinese machine reading comprehension,2019, arXiv preprint arXiv:1904
 OPUS-MT â€” Building open translation services for theWorld,2020, In Proceedings of the 22nd Annual Conferenec of the European Association for MachineTranslation (EAMT)
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Clue: A chinese language understanding evaluation benchmark,2020, arXivpreprint arXiv:2004
 Unsupervised cross-lingual transfer ofword embedding spaces,2018, In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Adversarial training for unsupervisedbilingual lexicon induction,1959, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 ChID: A large-scale Chinese IDiom dataset for clozetest,2019, In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
 The United Nations parallel cor-pus v1,2016,0
