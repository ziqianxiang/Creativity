title,year,conference
 Do GANs learn the distribution? some theoryand empirics,2018, In International Conference on Learning Representations
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In KamalikaChaudhuri and Ruslan Salakhutdinov (eds
 An Introduction to Numerical Analysis,1989, John Wiley and Sons
 Empirical study of benefits ofoverparameterization in single-layer latent variable generative models,2020, In Proceedings of the 37thInternational Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, In Proceedings of the 35th International Conference onLearning Representations
 Generative adversarial nets,2014, In Zoubin Ghahramani
 Neural autoregressiveflows,2018, In Jennifer G
 Neural tangent kernel: Con-vergence and generalization in neural networks,2018, In Samy Bengio
 Auto-encoding variational bayes,2014, In Yoshua Bengio andYann LeCun (eds
 Normalizing flows: An introduction and review of currentmethods,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
 SGD learns one-layernetworks in WGANs,2020, In In Proceedings of the 37th International COnference on Machine Learning
 Are ganscreated equal? a large-scale study,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 A vector-contraction inequality for rademacher complexities,2016, In InternationalConference on Algorithmic Learning Theory
 Benefits of jointly training au-toencoders: An improved neural tangent kernel analysis,2019, CoRR
 On the dynamics of gradientdescent for autoencoders,2019, In The 22nd International Conference on Artificial Intelligence andStatistics
 Normalizing flows for probabilistic modeling and inference,2019, ArXiv
 Variational inference with normalizing flows,2015, volume 37 ofProceedings ofMachine Learning Research
 Maximum of a sequence of gaussian random variables,2012, 2012
 Optimal Transport for Applied Mathematicians,2015, Calculus of Variations
 Understanding machine learning: From theory toalgorithms,2014, Cambridge university press
 Coupling-basedinvertible neural networks are universal diffeomorphism approximators,2020, ArXiv
 Sylvester normalizingflows for variational inference,2018, In proceedings of the Conference on Uncertainty in ArtificialIntelligence (UAI)
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, CambridgeSeries in Statistical and Probabilistic Mathematics
