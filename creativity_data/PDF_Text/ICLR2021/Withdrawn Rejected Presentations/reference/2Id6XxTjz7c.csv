title,year,conference
 Language models are few-shot learners,2020, arXiv:2005
 Towards the limit of network quantization,2017, InInternational Conference on Learning Representations (ICLR)
 Extremely low bit Transformer quantization for on-device neuralmachine translation,2020, arXiv preprint arXiv:2009
 Predicting parameters in deeplearning,2013, In Advances in neural information processing systems
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, arXiv:1810
 HAWQ: Hessianaware quantization of neural networks with mixed-precision,2019, arXiv:1905
 Deep Learning,2016, MIT Press
 Network sketching: exploiting binarystructure in deep CNNs,2017, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Deep residual learning for imagerecognition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Distilling the knowledge in a neural network,2015, InNIPS Deep Learning and Representation Learning Workshop
 Loss-aware binarization of deep networks,2017, InInternational Conference on Learning Representations (ICLR)
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Adaptive quantization of neural networks,2018, In International Conferenceon Learning Representations (ICLR)
 Learning multiple layers of features from tiny images,2009, Technical report
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, arXiv preprint arXiv:1808
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems
 Practical bayesian optimization,2008, University of Alberta
 Building a large annotatedcorpus of English: ThePennTreebank,1993, Comput
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 Low-rank matrix factorization for deep neural network training with high-dimensional output targets,2013, InICASSP
 Model compression via distillation and quantiza-tion,2018, In International Conference on Learning Representations (ICLR)
 A call for clarity in reporting bleu scores,2018, arXiv preprint arXiv:1804
 On the compression ofrecurrent neural networks with an application to LVCSR acoustic modeling for embedded speechrecognition,2016, In ICASSP
 XNOR-Net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 And the bit goesdown: Revisiting the quantization of neural networks,2020, In International Conference on LearningRepresentations
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Huggingfaceâ€™s transformers: State-of-the-artnatural language processing,2019, ArXiv
 Improving neuralnetwork quantization without retraining using outlier channel splitting,2019, arXiv:1901
 Trained ternary quantization,2017, InInternational Conference on Learning Representations (ICLR)
