title,year,conference
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, arXiv preprint arXiv:2002
 Sharp minima can generalizefor deep nets,2017, In Doina Precup and Yee Whye Teh
 Characterizing implicit bias in termsof optimization geometry,2018, In Jennifer Dy and Andreas Krause
 How implicit regularization of neural networksaffects the learned function - part i,2019, arXiv preprint arXiv:1911
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In S
 The implicit bias of gradient descent on nonseparable data,2019, In AlinaBeygelzimer and Daniel Hsu
 On large-batch training for deep learning: Generalization gap andsharp minima,2017, In International Conference on Learning Representations
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, In H
 Gradient descent quantizes ReLU networkfeatures,2018, arXiv preprint arXiv:1803
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Geometry ofoptimization and implicit regularization in deep learning,2017, arXiv preprint arXiv:1705
 A function space view of boundednorm infinite width ReLU nets: The multivariate case,2020, In International Conference on LearningRepresentations
" Minimum ""norm"" neural networks are splines",2019, arXiv preprintarXiv:1910
 On the spectral bias of neural networks,2019, In International Conferenceon Machine Learning
 Exact solUtions to the nonlinear dynamicsof learning in deep linear neUral networks,2014, In YoshUa Bengio and Yann LeCUn
 Towards Understanding generalization of deep learning:Perspective of loss landscapes,2017, arXiv preprint arXiv:1706
 Understandingdeep learning reqUires rethinking generalization,2017, In International Conference on LearningRepresentations
 A type of generalization error indUced byinitialization in deep neUral networks,2019, arXiv preprint arXiv:1905
