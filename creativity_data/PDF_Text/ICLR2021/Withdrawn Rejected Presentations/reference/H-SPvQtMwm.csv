title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Long short-term memory-networks for machinereading,2016, arXiv preprint arXiv:1601
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Effective approaches to attention-based neural machine translation,2015, arXiv preprint arXiv:1508
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 A decomposable attentionmodel for natural language inference,2016, arXiv preprint arXiv:1606
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Fixed encoder self-attention patterns intransformer-based machine translation,2020, arXiv preprint arXiv:2002
 Relevance of unsupervised met-rics in task-oriented dialogue for evaluating natural language generation,2017, CoRR
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Memory networks,2014, arXiv preprintarXiv:1410
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
