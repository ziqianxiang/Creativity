title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Adversarial classification,2004, InProceedings of the tenth ACM SIGKDD international conference on Knowledge discovery anddata mining
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Minority reports defense: Defending against adversarialpatches,2020, arXiv preprint arXiv:2004
 Universaladversarial perturbations,2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Local gradients smoothing: Defense againstlocalized adversarial attacks,2019, In 2019 IEEE Winter Conference on Applications of ComputerVision (WACV)
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Ensemble adversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Attention is all you need,2017, 2017
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Patchguard: Prov-able defense against adversarial patches using masks on small receptive fields,2020, arXiv preprintarXiv:2005
 Clipped bagnet: Defendingagainst sticker attacks with clipped bag-of-features,2020, 2020
