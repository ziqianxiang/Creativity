title,year,conference
 Fine-grained analysisof sentence embeddings using auxiliary prediction tasks,2016, arXiv preprint arXiv:1608
 Leveraging linguisticstructure for open domain information extraction,2015, In ACL
 Freebase: a collabo-ratively created graph database for structuring human knowledge,2008, In SIGMOD
 Comet: Commonsense transformers for automatic knowledge graph construction,2019, arXivpreprint arXiv:1906
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Toward an architecture for never-ending language learning,2010, In AAAI
 What does bert lookat? an analysis of bert’s attention,2019, arXiv preprint arXiv:1906
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Commonsense knowledge mining frompretrained models,2019, In EMNLP
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Differentiable reasoning over a virtual knowledge base,2020, arXiv preprintarXiv:2002
 Identifying relations for open informationextraction,2011, In EMNLP
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Measuring massive multitask language understanding,2020, arXiv preprintarXiv:2009
 A structural probe for finding syntax in word representa-tions,2019, In NAACL
 Yago2: A spatiallyand temporally enhanced knowledge base from wikipedia,2013, Artificial Intelligence
 Attention is not explanation,2019, arXiv preprint arXiv:1902
 Generalizationthrough memorization: Nearest neighbor language models,2019, arXiv preprint arXiv:1911
 Semi-supervised classification with graph convolutional net-works,2016, arXiv preprint arXiv:1609
 ALBERT: A lite BERT for self-supervised learning of language representations,2019, CoRR
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, TACL
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Distant supervision for relation extractionwithout labeled data,2009, In ACL
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Knowledge enhanced contextual word representations,2019, arXiv preprintarXiv:1909
 How context affects language models’ factual predictions,2020, arXivpreprint arXiv:2005
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Hopfieldnetworks is all you need,2020, arXiv preprint arXiv:2008
 Open language learning forinformation extraction,2012, In EMNLP
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, arXiv
 A cross-lingual dictionary for english wikipedia con-cepts,2012, 2012
 What do you learn from con-text? probing for sentence structure in contextualized word representations,2019, arXiv preprintarXiv:1905
 Attention is all you need,2017, In NIPS
 Visualizing attention in transformerbased language models,2019, arXiv preprintarXiv:1904
 Bertology meets biology: Interpreting attention in protein language models,2020, arXiv preprintarXiv:2006
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, In NIPS
 Knowledge graph and text jointlyembedding,2014, In EMNLP
 Probase: A probabilistic taxonomyfor text understanding,2012, In Proceedings of the 2012 ACM SIGMOD International Conference onManagement of Data
 Xlnet: Generalized autoregressive pretraining for language understanding,2019,	CoRR
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In ICCV
 Then a Wikipedia anchor to theWikidata item dictionary is constructed and used to further link the entities to Wikidata,2014, If the heador tail is a pronoun
