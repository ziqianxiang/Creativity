title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Clt for linear spectral statistics of large-dimensional samplecovariance matrices,2004, The Annals of Probability
 On the convergence of the spectral empirical process of wignermatrices,2005, Bernoulli
 Limit of the smallest eigenvalue of a large dimensional samplecovariance matrix,2008, In Advances In Statistics
 On asymptotics of eigenvectors of large samplecovariance matrix,2007, The Annals of Probability
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine learningand the bias-variance trade-off,2018, stat
 Subspace fitting meets regression:The effects of supervision and orthonormality constraints on double descent of generalizationerrors,2020, arXiv preprint arXiv:2002
 A model of double descent for high-dimensional binary linear classification,2019, arXiv preprint arXiv:1911
 Exact expressions for double descentand implicit regularization via surrogate random design,2019, arXiv preprint arXiv:1912
 Gradient descent provably optimizesover-parameterized neural networks,2018, In International Conference on Learning Representations
 Generalization error of generalized linear models in high dimensions,2020, arXiv preprintarXiv:2005
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Asymptotic errors for convex penalized linearregression beyond gaussian matrices,2020, arXiv preprint arXiv:2002
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Precise tradeoffs in adversarial trainingfor linear regression,2020, arXiv preprint arXiv:2002
 Analytic study of double descent in binary classification:The impact of loss,2020, arXiv preprint arXiv:2001
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing SyStemS
 The implicit regularization of ordinary leastsquares ensembles,2020, In International Conference on Artificial Intelligence and Statistics
 Minimizers of the empirical risk and risk mono-tonicity,2019, In Advances in Neural Information Processing Systems
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime,2019, arXivpreprint arXiv:1911
 More data can hurt for linear regression: Sample-wise double descent,2019, arXivpreprint arXiv:1912
 Central limit theorem for signal-to-interference ratio of reduced ranklinear receiver,2008, Ann
 Asymptotic normality and confidence intervals for derivatives of2-layers neural network in the random features model,2020, In Neural Information Processing Systems
 On the number of variables to use in principal component regression,2019, InAdvances in Neural Information Processing Systems
