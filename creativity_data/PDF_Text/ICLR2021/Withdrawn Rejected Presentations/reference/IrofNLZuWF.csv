title,year,conference
 The case forfull-matrix adaptive regularization,2018, arXiv preprint arXiv:1806
 Minimax regret bounds for reinforcement learning,2017, InInternational Conferences on Machine Learning (ICML)
 Optimization methods for large-scale machine learning,2018, SiamReview
 Convex optimization: Algorithms and complexity,2014, arXiv preprint arXiv:1405
 On the convergence of a class of ADAM-type algorithms fornon-convex optimization,2019, In International Conference on Learning Representations (ICLR)
 Spider: Near-optimal non-convex optimization via stochasticpath integrated differential estimator,2018, In Proceedings of Advances in Neural Information ProcessingSystems (NIPS)
 Optimal non-asymptotic bound of the Ruppert-Polyak averaging withoutstrong convexity,2017, arXiv preprint arXiv:1709
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Online optimization: Competing withdynamic comparators,2015, In Artificial Intelligence and Statistics
 Accelerated gradient descent escapes saddle points faster thangradient descent,2018, In Conference On Learning Theory
 On variance reduction for stochastic smooth convex optimization withmultiplicative noise,2019, Mathematical Programming
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 On the convergence of stochastic gradient descent with adaptive stepsizes,2019, InThe 22nd International Conference on Artificial Intelligence and Statistics
 On the variance of the adaptive learningrate and beyond,2019, arXiv preprint arXiv:1908
 Towards better understandingof adaptive gradient algorithms in generative adversarial nets,2020, In International Conference onLearning Representations (ICLR)
 On the adequacy of untuned warmup for adaptive optimization,2019, arXiv preprintarXiv:1910
 Regularizing and optimizing LSTM language models,2018, InInternational Conference on Learning Representations (ICLR)
 Online optimization in dynamicenvironments: Improved regret rates for strongly convex problems,2016, In 2016 IEEE 55th Conferenceon Decision and Control (CDC)
 Non-asymptotic analysis of stochastic approximation algorithms formachine learning,2011, In Proceedings of Advances in Neural Information Processing Systems (NIPS)
 Problem complexity and method efficiency in optimization,1983, Wiley
 Making gradient descent optimal for strongly convexstochastic optimization,2012, In International Conferences on Machine Learning (ICML)
 Escaping saddle points with adaptive gradientmethods,2019, In International Conferences on Machine Learning (ICML)
 Why adam beats sgdfor attention models,2019, arXiv preprint arXiv:1912
 Why gradient clipping accelerates training: A theoreticaljustification for adaptivity,2020, In International Conference on Learning Representations
 On the convergence of adaptive gradient methods fornonconvex optimization,2018, arXiv preprint arXiv:1808
 Adashift: Decorrelation and convergenceof adaptive learning rate methods,2019, In International Conference on Learning Representations(ICLR)
 A sufficient condition for convergences of adam andrmsprop,2019, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
