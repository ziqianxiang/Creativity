title,year,conference
 Negative eigenvalues of thehessian in deep neural networks,2018, 2018
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Natural gradient works efficiently in learning,1998, Neural computation
 Improving the convergence of back-propagation learning withsecond order methods,1988, In Proceedings of the 1988 connectionist models summer school
 Convergence rate analysis ofa stochastic trust region method for nonconvex optimization,2016, arXiv preprint arXiv:1609
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Adaptive cubic regularisation methods forunconstrained optimization,2011, part i: motivation
 How Much Patience to You Have?: AWorst-case Perspective on Smooth Noncovex Optimization,2012, Science and Technology FacilitiesCouncil Swindon
 Improved preconditioner for hessian free optimization,2011, In NIPSWorkshop on Deep Learning and Unsupervised Feature Learning
 Stochastic optimization using a trust-regionmethod and random models,2018, Mathematical Programming
 Trust region methods,2000, SIAM
 Exploiting negative curvature in deterministic and stochasticoptimization,2017, arXiv preprint arXiv:1703
 Escaping saddles withstochastic gradients,2018, arXiv preprint arXiv:1803
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing systems
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 Gradientdescent can take exponential time to escape saddle points,2017, In Advances in Neural InformationProcessing Systems
 Escaping from saddle points-online stochasticgradient for tensor decomposition,2015, In COLT
 Why momentum really works,2017, Distill
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, In International Conference on Machine Learning
 Training feedforward networks with the marquardtalgorithm,1994, IEEE transactions on Neural Networks
 Reducing the dimensionality of data with neuralnetworks,2006, science
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Sub-sampled cubic regularization for non-convex opti-mization,2017, In International Conference on Machine Learning
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Gradient descent onlyconverges to minimizers,2016, In Conference on Learning Theory
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, arXiv preprint arXiv:1805
 Stochastic second-order methods fornon-convex optimization with inexact hessian and gradient,2018, arXiv preprint arXiv:1809
 Inefficiency of k-fac for large batch size training,2019, arXiv preprint arXiv:1903
 Deep learning via hessian-free optimization,2010, In ICML
 New insights and perspectives on the natural gradient method,2014, arXiv preprintarXiv:1412
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Revisiting small batch training for deep neural networks,2018, arXivpreprint arXiv:1804
 Second-order stagewise backpropagation for hessian-matrixanalyses and investigation of negative curvature,2008, Neural Networks
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 Automatic differentiation in pytorch,2017, 2017
 Fast exact multiplication by the hessian,1994, Neural computation
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Stochastic cubicregularization for fast nonconvex optimization,2017, arXiv preprint arXiv:1711
 Characteristic vectors of bordered matrices with infinite dimensions i,1993, In TheCollected Works of Eugene Paul Wigner
 Second-order oPtimization for non-convex machine learning: An emPirical study,2017, arXiv preprint arXiv:1708
 Newton-tyPe methods for non-convexoPtimization under inexact hessian information,2017, arXiv preprint arXiv:1708
 Inexact non-convexnewton-tyPe methods,2018, arXiv preprint arXiv:1802
 Adadelta: an adaPtive learning rate method,2012, arXiv preprint arXiv:1212
