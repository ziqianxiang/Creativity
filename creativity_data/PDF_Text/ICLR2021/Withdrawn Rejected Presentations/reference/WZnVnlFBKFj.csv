title,year,conference
 Update aware device schedulingfor federated learning at the wireless edge,2020, In Proc
 COLA: Decentralized linear learning,2018, In Proc
 Adam: A method for stochastic optimization,2017, arXiv:1412
 Randomized distributed mean estimation: Accuracy vs com-munication,2018, Frontiers in Applied Mathematics and Statistics
 Federated optimization: Distributed optimiza-tion beyond the datacenter,2015, arXiv:1511
 Federatedlearning: Strategies for improving communication efficiency,2016, In Proc
 Learning multiple layers of features from tiny images,2009, InTechnical Report
 Machine learning at the wireless edge: Distributed stochasticgradient descent over-the-air,2020, IEEE Transactions on Signal Processing
 Communication-efficient learning of deep networks from decentralized data,2017, In Proc
 Hogwild: A lock-free approach to paralleliz-ing stochastic gradient descent,2011, In Advances in Neural Information Processing Systems(NIPS)
 DoubleSqueeze: Parallel stochastic gra-dient descent with double-pass error-compensated compression,2019, In Proc
 Scheduling policies for federatedlearning in wireless networks,2020, IEEE Transactions on Communications
 Fed-erated learning with non-IID data,2018, arXiv:1806
 On the convergence properties of a K-step averaging stochastic gradi-ent descent algorithm for nonconvex optimization,2018, In Proc
