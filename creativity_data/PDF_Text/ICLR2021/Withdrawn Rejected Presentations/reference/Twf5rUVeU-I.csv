title,year,conference
 Introduction to Numerical Continuation Methods,0898, Society forIndustrial and Applied Mathematics
 Greedy layer-wise trainingof deep networks,2006, In Proceedings of the 19th International Conference on Neural InformationProcessing Systems
 Visual Reconstruction,0262, MIT Press
 Optimization methods for large-scale machinelearning,0036, SIAM Review
 Large displacement optical flow: descriptor matching in variationalmotion estimation,2011, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Semi-Supervised Learning,2006, Adaptive computation andmachine learning
 Identifying and attacking the saddle point problem in high-dimensional non-convex optimization,2014, In Z
 Transferringoptimality across data distributions via homotopy methods,2020, In International Conference onLearning Representations (ICLR)
 Iterative non-linear dimensionality reduction withmanifold sculpting,2007, In Advances in Neural Information Processing Systems
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Optimization by simulated annealing,1983, Science
 An adaptive accelerated proximal gradient method and its homo-topy continuation for sparse optimization,2014, In Eric P
 A survey on transfer learning,1041, IEEE Trans
 On the importance of initializationand momentum in deep learning,2013, In Sanjoy Dasgupta and David McAllester (eds
 Outlier path: A ho-motopy algorithm for robust svm,2014, In Eric P
 Fast and faster convergence of sgd forover-parameterized models and an accelerated perceptron,2019, In Kamalika Chaudhuri andMasashi Sugiyama (eds
 How transferable are features in deepneural networks? In Z,2014, Ghahramani
