title,year,conference
 Lyapunov exponents of nonlinear stochastic systems,1988, In Nonlinear StochasticDynamic Engineering Systems
 Stability of stochastic gradientdescent on nonsmooth convex losses,2020, arXiv preprint arXiv:2006
 Sgd-qn: Careful quasi-newton stochastic gra-dient descent,2009, 2009
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Handwritten digit recognition with a back-propagation net-work,1990, In Advances in neural information processing Systems
 Automatic learning rate maximization byon-line estimation of the hessianâ€™s eigenvectors,1993, In Advances in neural information processingsystems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Understanding theeffects of data parallelism and sparsity on neural network training,2020, CoRR
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Deep learning theory review: An optimal controland dynamical systems perspective,2019, arXiv preprint arXiv:1908
 Deep learning via hessian-free optimization,2010, In ICML
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Fast large-scale optimization by unifyingstochastic gradient and quasi-newton methods,2014, In International Conference on Machine Learning
 Optimization for deep learning: theory and algorithms,2019, arXiv preprintarXiv:1912
 Why gradient clipping acceleratestraining: A theoretical justification for adaptivity,2019, arXiv preprint arXiv:1905
