title,year,conference
 Towards causal vqa: Revealing and reducingspurious correlations by invariant and covariant semantic editing,2020, In CVPR
 A simple framework forcontrastive learning of visual representations,2020, In ICML
 Commonsense reasoning and commonsense knowledge in artificialintelligence,2015, Communications of the ACM
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Dimensionality reduction by learning an invariantmapping,2006, In CVPR
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Compositional attention networks for machine rea-soning,2018, 2018
 Semantic sentence matching with densely-connectedrecurrent and co-attentive information,2019, In AAAI
 Microsoft coco: Common objects in context,2014, In ECCV
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, In Neurips
 Representation learning With contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 A Web-based question ansWering system for effectivee-learning,2007, In ICALT 2007
 Normalization: A preprocessing stage,2015, arXiv preprintarXiv:1503
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Faster r-cnn: ToWards real-time objectdetection With region proposal netWorks,2015, In Neurips
 Lxmert: Learning cross-modality encoder representations from trans-formers,2019, In EMNLP
 Unbiased scene graphgeneration from biased training,2020, In CVPR
 Attention is all you need,2017, In NeuriPs
 Unsupervised feature learning via non-parametric instance discrimination,2018, In CVPR
 Automatic question ansWer-ing system for consumer products,2016, In IntelliSys
 Reclor: A reading comprehension datasetrequiring logical reasoning,2020, In ICLR
 Cross-modality relevance for reasoning on lan-guage and vision,2020, 2020
