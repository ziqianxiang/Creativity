title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Approximation and estimation bounds for artificial neural networks,1994, MachineLearning
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in neural information processingsystems
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 A priori estimates of the population risk for residualnetworks,2019, arXiv preprint arXiv:1903
 A priori estimates of the population risk for two-layer neuralnetworks,2019, Communications in Mathematical Sciences
 Barron spaces and the compositional function spaces for neuralnetwork models,2019, arXiv preprint arXiv:1906
 Delving deep into rectifiers: SUrpassinghUman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural tangent kernel: Convergence and gen-eralization in neUral networks,2018, In Advances in neural information processing systems
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 A mean field view of the landscape of two-layers neuralnetworks,2018, In Proceedings of the National Academy of Sciences
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Parameters as interacting particles: long time convergenceand asymptotic error scaling of neural networks,2018, In Advances in neural information processingsystems
 Spurious local minima are common in two-layer ReLU neural net-works,2018, In International Conference on Machine Learning
 Mean field analysis of neural networks: A centrallimit theorem,2020, Stochastic Processes and their Applications
 An analytical formula of population gradient for two-layered ReLU network and itsapplications in convergence and critical point analysis,2017, In International Conference on MachineLearning
