title,year,conference
 Etc: Encoding long and structured data in transformers,2020, In Empirical Methods in NaturalLanguage Processing (EMNLP)
 A bert baseline for the natural questions,2019, arXivpreprint arXiv:1901
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Masked language modeling for pro-teins via linearly scalable long-context transformers,2020, arXiv preprint arXiv:2006
 Yinyang k-means:A drop-in replacement of the classic k-means with consistent speedup,2015, In International conferenceon machine learning (ICML)
 Long short-term memory,1997, Neural Computation
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, arXiv preprint arXiv:2006
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations (ICLR)
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations (ICLR)
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Pointer sentinel mixturemodels,2017, In International Conference on Learning Representations (ICLR)
 Densely connected attention propagation forreading comprehension,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 Head-driven phrase structure grammar parsing on penn treebank,2019, InAssociation for Computational Linguistics (ACL)
