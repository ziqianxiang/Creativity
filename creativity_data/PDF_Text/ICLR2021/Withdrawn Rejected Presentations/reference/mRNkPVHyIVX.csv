title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Unlabeled dataimproves adversarial robustness,2019, In NeurIPS
 Certified adversarial robustness via randomizedsmoothing,2019, In ICML
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In ICML
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In ICML
 Explaining and harnessing adversarialexamples,2015, In ICLR
 Countering adversarialimages using input transformations,2018, In ICLR
 A baseline for detecting misclassified and out-of-distributionexamples in neural networks,2017, In ICLR
 Using pre-training can improve model robustnessand uncertainty,2019, In ICML
 Deep anomaly detection with outlierexposure,2019, In ICLR
 Using self-supervised learningcan improve model robustness and uncertainty,2019, In NeurIPS
 Certifiedrobustness to adversarial examples with differential privacy,2019, In IEEE Symposium on Security andPrivacy (SP)
 A simple unified framework for detectingout-of-distribution samples and adversarial attacks,2018, In NeurIPS
 Towards robust neural networks viarandom self-ensemble,2018, In ECCV
 Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images,2015, In CVPR
 Adversarial robustness through locallinearization,2019, In NeurIPS
 Overfitting in adversarially robust deep learning,2020, InICML
 ImageNetLarge Scale Visual Recognition Challenge,2015, In IJCV
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, InNeurIPS
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, In ICLR
 Intriguing properties of neural networks,2013, In ICLR
 On adaptive attacks toadversarial example defenses,2020, In arXiv:2002
 Improvingadversarial robustness requires revisiting misclassified examples,2020, In ICLR
 Fast is better than free: Revisiting adversarial training,2020, InICLR
 Lsun:Construction of a large-scale image dataset using deep learning with humans in the loop,2015, InarXiv:1506
 Wide residual networks,2016, In arXiv:1605
 Defense against adversarial attacks using feature scattering-basedadversarial training,2019, In NeurIPS
 Attacks which do not kill training make adversarial learning stronger,2020, In arXiv:2002
