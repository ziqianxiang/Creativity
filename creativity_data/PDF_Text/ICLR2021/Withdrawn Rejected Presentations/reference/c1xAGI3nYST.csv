title,year,conference
 A learning algorithm for Boltzmann ma-chines,1985, Cognitive science
 Convolutional image captioning,2018, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Sequential latent spaces for modelingthe intention during diverse image captioning,2019, In Proceedings of the IEEE International Conference onComputer Vision
 Discriminatorrejection sampling,2018, In International Conference on Learning Representations
 Resampled priors for variational autoencoders,2019, In The 22nd InternationalConference on Artificial Intelligence and Statistics
 Large scale gan training for high fidelity natural imagesynthesis,2018, arXiv preprint arXiv:1809
 Variational lossy autoencoder,2016, arXiv preprint arXiv:1611
 Diagnosing and enhancing vae models,2018, In International Conference on LearningRepresentations
 Juke-box: A generative model for music,2020, arXiv preprint arXiv:2005
 Implicit generation and modeling with energy based models,2019, In Advances inNeural Information Processing Systems
 Latent constraints: Learning to generate conditionallyfrom unconditional generative models,2018, In International Conference on Learning Representations
 Som-vae:Interpretable discrete representation learning on time series,2018, In International Conference on LearningRepresentations
 From varia-tional to deterministic autoencoders,2020, In International Conference on Learning Representations
 Generative Adversarial Networks,2014, arXiv:1406
 Towards con-ceptual compression,2016, In Advances In Neural Information Processing Systems
 PixelVAE: A latent variable model for natural images,2016, arXiv preprint arXiv:1611
 Noise-contrastive estimation: A new estimation principle for Un-normalized statistical models,2010, In Proceedings of the Thirteenth International Conference on ArtificialInteUigence and Statistics
 Gans trainedby a two time-scale update rule converge to a local nash equilibrium,2017, In Advances in neural informationprocessing systems
 Training products of experts by minimizing contrastive divergence,2002, Neural computation
 Learning and relearning in boltzmann machines,1986, Paralleldistributed processing: Explorations in the microstructure of cognition
 Denoising diffusion probabilistic models,2020, arXiv preprintarxiv:2006
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEE conferenceon computer vision and pattern recognition
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In International Conference on Machine Learning
 Semi-supervised learning withnormalizing flows,2020, In ICML
 Training gener-ative adversarial networks with limited data,2020, arXiv preprint arXiv:2006
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2014, In The International Conferenceon Learning Representations (ICLR)
 Semi-supervised learn-ing with deep generative models,2014, In Advances in Neural Information Processing Systems
 Improvedvariational inference with inverse autoregressive flow,2016, In Advances in Neural Information ProcessingSystems
 Glow: Generative flow with invertible 1x1 convolutions,2018, In S
 Learning hierarchi-cal priors in vaes,2019, In Advances in Neural Information Processing Systems
 Classification using discriminative restricted boltzmann machines,2008, InProceedings of the 25th international conference on Machine learning
 Energy-inspired models: Learning withsampler-induced distributions,2019, In Advances in Neural Information Processing Systems
 Deep learning face attributes in the wild,2015, InProceedings of the IEEE international conference on computer vision
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXiv preprintarXiv:1608
 BIVA: A very deep hierarchy of latentvariables for generative modeling,2019, In Advances in neural information processing systems
 Adversarial autoen-coders,2015, arXiv preprint arXiv:1511
 Adversarial variational bayes: Unifying variational au-toencoders and generative adversarial networks,2017, In 34th International Conference on Machine Learning(ICML)
 Autoregressive energy machines,2019, arXiv preprint arXiv:1904
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Learning latent space energy-basedprior model,2020, arXiv preprint arXiv:2006
 Waveflow: A compact flow-based model for rawaudio,2020, ICML
 Searching for activation functions,2017, arXiv preprintarXiv:1710
 Taming vaes,2018, arXiv preprint arXiv:1810
 Stochastic backpropagation and approxi-mate inference in deep generative models,2014, In International Conference on Machine Learning
 Discrete variational autoencoders,2016, arXiv preprint arXiv:1609
 Distribution matching in variational infer-ence,2018, arXiv preprint arXiv:1802
 Ladder varia-tional autoencoders,2016, In Advances in neural information processing systems
 Variationalautoencoder with implicit optimal priors,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Wasserstein auto-encoders,2018, In International Conferenceon Learning Representations (ICLR 2018)
 Metropolis-hastings generativeadversarial networks,2019, In International Conference on Machine Learning
 NVAE: A deep hierarchical variational autoencoder,2020, In Neural InformationProcessing Systems (NeurIPS)
 Undirected graphical models as approximateposteriors,2020, In International Conference on Machine Learning (ICML)
 Neural discrete representation learning,2017, In Advances in NeuralInformation Processing Systems
 On the necessity andeffectiveness of learning the prior of variational auto-encoder,2019, arXiv preprint arXiv:1905
 Perceptual generative autoen-coders,2020, In International Conference on Machine Learning
