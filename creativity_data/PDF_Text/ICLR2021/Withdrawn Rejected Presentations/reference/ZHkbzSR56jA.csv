title,year,conference
 Byzantine stochastic gradient descent,2018, In Advances inNeural Information Processing Systems
 Advances inasynchronous parallel and distributed optimization,2020, Proceedings of the IEEE
 A little is enough: Circumventing defenses fordistributed learning,2019, In Advances in Neural Information Processing Systems
 signSGD withmajority vote is communication efficient and fault tolerant,2019, In International Conference on LearningRepresentations
 Machine learning with adversaries: Byzantinetolerant gradient descent,2017, In Advances in Neural Information Processing Systems
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings oftheInternational Conference on Computational Statistics
 Distributed statistical machine learning in adversarial settings:Byzantine gradient descent,2017, Proceedings of the ACM on Measurement and Analysis of ComputingSystems
 Large scale distributed deep networks,2012, InAdvances in Neural Information Processing Systems
 Recent advances in algorithmic high-dimensional robuststatistics,2019, arXiv preprint arXiv:1911
 The hidden vulnerability of distributed learning inbyzantium,2018, In International Conference on Machine Learning
 Tradingredundancy for communication: Speeding up distributed SGD for non-convex optimization,2019, InProceedings of the International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
 Communication-efficient distributed dual coordinate ascent,2014, In Advances inNeural Information Processing Systems
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Advancesand open problems in federated learning,2019, arXiv:1912
 Communication efficient distributedmachine learning with the parameter server,2014, In Advances in Neural Information Processing Systems
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 An accelerated proximal coordinate gradient method,2014, InAdvances in Neural Information Processing Systems
 averaging in distributed primal-dual optimization,2015, In Proceedings of the InternationalConference on Machine Learning
 Scaling-up distributed processing of datastreams for machine learning,2020, arXiv preprint arXiv:2005
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Communication-efficient distributed optimization usingan approximate newton-type method,2014, In Proceedings of the International Conference on MachineLearning
 Slim-dp: a multi-agent systemfor communication-efficient distributed deep learning,2018, In Proceedings of the 17th InternationalConference on Autonomous Agents and MultiAgent Systems
 Zeno: Distributed stochastic gradient descent withsuspicion-based fault-tolerance,2019, In Proceedings of the International Conference on MachineLearning
 Zeno++: Robust fully asynchronous SGD,2020, InProceedings of the International Conference on Machine Learning
 Trading computation for communication: Distributed stochastic dual coordinateascent,2013, In Advances in Neural Information Processing Systems
 Adversary-resilient distributed and decentralizedstatistical inference and machine learning: An overview of recent advances under the byzantinethreat model,2020, IEEE Signal Processing Magazine
 Byzantine-robust distributedlearning: Towards optimal statistical rates,2018, In Proceedings of the International Conference onMachine Learning
 Defending against saddle pointattack in byzantine-robust distributed learning,2019, In Proceedings of the International Conference onMachine Learning
 On the linear speedup analysis of communication efficientmomentum SGD for distributed non-convex optimization,2019, In Proceedings of the InternationalConference on Machine Learning
 Parallel restarted SGD with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Linear convergence with condition number indepen-dent access of full gradients,2013, In Advances in Neural Information Processing Systems
 Asynchronous distributed admm for consensus optimization,2014, InProceedings of the International Conference on Machine Learning
 Proximal SCOPE for distributedsparse learning,2018, In Advances in Neural Information Processing Systems
 Parallelized stochastic gradientdescent,2010, In Advances in Neural Information Processing Systems
