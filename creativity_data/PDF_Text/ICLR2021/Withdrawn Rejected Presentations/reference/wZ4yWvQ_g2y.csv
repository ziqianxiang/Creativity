title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Proxylessnas: Direct neural architecture search on target taskand hardware,2018, In International Conference on Learning Representations
 Once-for-all: Train onenetwork and specialize it for efficient deployment,2019, arXiv preprint arXiv:1908
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 The pascal recognising textUal entailmentchallenge,2006, In Machine Learning Challenges
 NeUral architectUre search: A sUrvey,2018, arXivpreprint arXiv:1808
 RedUcing transformer depth on demand withstrUctUred dropoUt,2019, In International Conference on Learning Representations
 Single path one-shot neUral architectUre search with Uniform sampling,2019, arXiv preprintarXiv:1904
 Depthwise separable convolutions for neuralmachine translation,2018, In International Conference on Learning Representations
 Applying depthwise separable andmulti-channel convolutional neural networks of varied kernel size on semantic trajectories,2020, NeuralComputing and Applications
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Large memory layers with product keys,2019, In Advances in Neural Information ProcessingSystems
 Albert: A lite bert for self-sUpervised learning of langUage representations,2019, In InternationalConference on Learning Representations
 Darts: Differentiable architectUre search,2018, InInternational Conference on Learning Representations
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 Light-paff: A two-stage distillation framework for pre-training and fine-tuning,2020, arXiv preprintarXiv:2004
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Finding fasttransformers: One-shot neural architecture search by component composition,2020, arXiv preprintarXiv:2008
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Sample-efficient neuralarchitecture search by learning action space,2019, arXiv preprint arXiv:1906
 Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers,2020, arXiv preprintarXiv:2002
 Structured pruning of large language models,2019, arXivpreprint arXiv:1910
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In NAACL
 Bert-of-theseus: Compressingbert by progressive module replacing,2020, arXiv preprint arXiv:2002
 Bignas: Scaling up neural archi-tecture search with big single-stage models,2020, arXiv preprint arXiv:2003
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
