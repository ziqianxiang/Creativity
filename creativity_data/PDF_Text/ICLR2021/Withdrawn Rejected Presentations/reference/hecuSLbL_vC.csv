title,year,conference
 Regret Bounds for Lifelong Learning,2017, InAarti Singh and Jerry Zhu (eds
 Neural networks with a self-refreshing memory: Knowledgetransfer in sequential learning tasks without catastrophic forgetting,2000, Connection Science
 Fine-grained analysisof optimization and generalization for overparameterized two-layer neural networks,2019, CoRR
 Rademacher and gaussian complexities: Risk bounds andstructural results,2003, J
 Exploiting task relatedness for multiple task learning,2003, In BernhardScholkopf and Manfred K
 Understanding regularisation methods for continual learning,2020,	ArXiv
 On the inductive bias of neural tangent kernels,2019, In NeurIPS
 Efficientlifelong learning with a- GEM ,2019, In International Conference on Learning Representations
 Learning to learn arounda common mean,1016, In S
 Kernel ridge vs,2017, principal component regression:Minimax bounds and the qualification of regularization operators
 Gradient descent provablyoptimizes over-parameterized neural networks,2018, CoRR
 Uncertainty-guidedcontinual learning with bayesian neural networks,2020, In International Conference on LearningRepresentations
 Orthogonal gradient descent for continuallearning,2019, ArXiv
 Catastrophic forgetting in connectionist networks,1999, Trends in cognitive sciences
 An empiricalinvestigation of catastrophic forgeting in gradient-based neural networks,2014, CoRR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, CoRR
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Overcoming catastrophic forgettingin neural networks,2016, Proceedings of the National Academy of Sciences
 Overcoming catastrophic forgetting inneural networks,2017, Proceedings of the National Academy of Sciences
 Optimal continual learning has perfect memoryand is np-hard,2020, ArXiv
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep convo-lutional neural networks,2012, In F
 Gradient-based learning applied todocument recognition,1998, In Proceedings of the IEEE
 Lifelong learning with dynamicallyexpandable networks,2018, ArXiv
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2019, CoRR
 Alleviating catastrophic forgetting usingcontext-dependent gating and synaptic stabilization,2018, Proceedings of the National Academy ofSciences
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, Psychology of Learning and Motivation
 Toward understanding catastrophic forgetting in continual learning,2019, ArXiv
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 Connectionist models of recognition memory: constraints imposed by learning andforgetting functions,1990, Psychological review
 Online structured laplace approximationsfor overcoming catastrophic forgetting,2018, In Proceedings of the 32nd International Conference onNeural Information Processing Systems
 Overcoming catastrophicforgetting with hard attention to the task,2018, CoRR
 Generative replay with feedback connections as a generalstrategy for continual learning,2018, arXiv preprint arXiv:1809
 Three scenarios for continual learning,2019, arXiv preprintarXiv:1904
 Regularization matters: Generalization andoptimization of neural nets v,2019,s
