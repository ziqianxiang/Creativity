title,year,conference
 The Oxford handbook of random matrixtheory,2011, Oxford University Press
 Large deviations for wigner’s law and voiculescu’s non-commutative entropy,1997, Probability theory and related fields
 Pattern recognition and machine learning,2006, springer
 Entropy-SGD: Biasing gradi-ent descent into wide valleys,2016, arXiv preprint arXiv:1611
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Open problem: The landscape of the losssurfaces of multilayer networks,2015, In Conference on Learning Theory
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Gpy-torch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration,2018, In Advancesin Neural Information Processing Systems
 An investigation into neural net optimizationvia Hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Curvature is key: Sub-sampled loss surfaces and the implications for large batchtraining,2020, arXiv preprint arXiv:2006
 Flat minima,1997, Neural Computation
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 SubsPace inference for bayesian deeP learning,2019, Uncertainty in ArtificialIntelligence (UAI)
 Three factors influencing minima in SGD,2017, arXiv preprintarXiv:1711
 Universal statistics of fisher information indeeP neural networks: Mean field aPProach,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2016, arXivpreprint arXiv:1609
 DeeP learning,2015, nature
 APProximating sPectral densities of large matrices,2016, SIAMReview
 Distribution of eigenvalues forsome sets of random matrices,1967, Matematicheskii Sbornik
 DeeP learning via Hessian-free oPtimization,2010, In ICML
 Second-order optimization for neural networks,2016, PhD thesis
 OPtimizing neural networks with Kronecker-factored aPProxi-mate curvature,2015, In International conference on machine learning
 Training deeP and recurrent networks with Hessian-free oPti-mization,2012, In Neural networks: Tricks of the trade
 The Lanczos and conjugate gradient algorithms in finite pre-cision arithmetic,2006, ActaNumerica
 The full spectrum of deep net Hessians at scale: Dynamics with sample size,2018, arXivpreprint arXiv:1811
 Measurements of three-level hierarchical structure in the outliers in the spectrum ofdeepnet hessians,2019, arXiv preprint arXiv:1901
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 Automatic differentiation inPytorch,2017, 2017
 Fast exact multiplication by the Hessian,1994, Neural computation
 The spectrum of the fisher information matrix of a single-hidden-layer neural network,2018, In Advances in Neural Information Processing Systems
 Eigenvalues of the Hessian in deep learning: Singu-larity and beyond,2016, arXiv preprint arXiv:1611
 Empirical analysis ofthe Hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Krylov subspace descent for deep learning,2012, In Artificial Intelligenceand Statistics
 Characteristic vectors of bordered matrices with infinite dimensions i,1993, In TheCollected Works of Eugene Paul Wigner
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 How sgd selects the global minima in over-parameterized learning:A dynamical stability perspective,2018, In Advances in Neural Information Processing Systems
 Hessian-based anal-ysis of large batch training and robustness to adversaries,2018, In Advances in Neural InformationProcessing Systems
 Pyhessian: Neural networksthrough the lens of the hessian,2019, arXiv preprint arXiv:1912
	Let HP×P be a symmetric matrix with eigenvalues λ1 ≥ ,2012,
3 and 3,2016,3respectively and it also gets the spectral mass at 0 wrong
