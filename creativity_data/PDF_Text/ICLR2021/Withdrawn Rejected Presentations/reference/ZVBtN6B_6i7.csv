title,year,conference
 Adaptive input representations for neural language modeling,2019, InICLR
 Neural machine translation by jointlylearning to align and translate,2015, In ICLR
 Deep equilibrium models,2019, In Advances in NeuralInformation Processing Systems
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Finding structure in time,1990, Cogn
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Accessinghigher-level representations in sequential transformers with feedback memory,2020, arXiv preprintarXiv:2002
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Efficient softmax approxima-tion for gpus,2017, In ICML
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Long short-term memory,1997, Neural computation
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Large memory layers with product keys,2019, In Advances in Neural Information ProcessingSystems
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Pointer sentinel mixtUremodels,2016, arXiv preprint arXiv:1609
 RecurrentneUral network based langUage model,2010, In Eleventh annual conference of the international speechcommunication association
 Replication and analysis of ebbinghausâ€™ forgetting curve,2015, PLoS ONE
 Stabilizing transformers for reinforcement learning,2019, ArXiv
 Bio-logical structure and function emerge from scaling unsupervised learning to 250 million proteinsequences,2019, bioRxiv
 Recipes for building an open-domain chatbot,2020, arXivpreprint arXiv:2004
 Efficient content-based sparseattention with routing transformers,2020, arXiv preprint arXiv:2003
 Maze-base: A sandbox for learning from games,2015, ArXiv
 Aug-menting self-attention with persistent memory,2019, arXiv preprint arXiv:1907
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attention Withlinear complexity,2020, arXiv preprint arXiv:2006
 The psychology and neuroscience of forgetting,2004, Annu
 Long-term feature banks for detailed video understanding,2019, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Pay less attention WithlightWeight and dynamic convolutions,2018, In International Conference on Learning Representations
 Bp-transformer: Modellinglong-range context via binary partitioning,2019, arXiv preprint arXiv:1911
 A reward discount of 0,1024,98 is used
