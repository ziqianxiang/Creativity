title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, In NeurIPS
 Online learning rate adaptationwith hypergradient descent,2017, ICLR
 Automatic differentiation inmachine learning: a survey,2018, JMLR
 Adaptive subgradient methods for online learning and stochasticoptimization,2011, JMLR
 Forward and reverse gradient-based hyperpa-rameter optimization,2017, In ICML
 Convolutional neural networks at constrained time cost,2015, In CVPR
 Deep residual learning for image recognition,2016, In CVPR
 Linear convergence of gradient and proximal-gradient methodsunder the Polyak-IojasieWicz condition,2016, In Joint European Conference on Machine Learning andKnowledge Discovery in Databases
 Adam: A method for stochastic oPtimization,2015, ICLR
 Learning multiPle layers of features from tiny images,2012, University ofToronto
 Using statistics to automate stochastic oPtimization,2019, In Advances inNeural Information Processing Systems
 Gradient-based learning aPPlied to documentrecognition,1998, Proceedings of the IEEE
 An exPonential learning rate schedule for deeP learning,2019, In InternationalConference on Learning Representations
 On the variance of the adaPtive learningrate and beyond,2019, In ICLR
 Sgdr: Stochastic gradient descent With Warm restarts,2017, 2017
 AdaPtive gradient methods With dynamic bound of learningrate,2018, In ICLR
 Learning gradient descent: Better generalization and longer horizons,2017, InICML
 Gradient-based hyPerParameter oPtimization throughreversible learning,2015, In ICML
 Reading digits in natural imageswith unsupervised feature learning,2011, In NIPS Workshop on Deep Learning and UnsupervisedFeature Learning 2011
 Adaptive restart for accelerated gradient schemes,2015, Foundations ofcomputational mathematics
 On the convergence of adam and beyond,2018, In ICLR
 L4: Practical loss-based stepsize adaptation for deep learning,2018, InNeurIPS
 An overview of gradient descent optimization algorithms,2016, arXiv:1609
 Local gain adaptation in stochastic gradient descent,1999, In 1999 Ninth InternationalConference on Artificial Neural Networks ICANN 99
 Optimization for deep learning: theory and algorithms,2019, arXiv:1912
 Gain adaptation beats least squares,1992, In Proceedings of the 7th Yale workshop onadaptive and learning systems
 Fast online policy gradient learning with smd gain vectoradaptation,2006, In NeurIPS
