title,year,conference
 Katyusha: The First Direct Acceleration of Stochastic Gradient Methods,2017, InSTOC
 Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Op-timization,2018, In Proceedings of the 35th International Conference on Machine Learning
 How To Make the Gradients Small Stochastically,2018, In Proceedings of the 32ndConference on Neural Information Processing Systems
 Natasha 2: Faster non-convex optimization than SGD,2018, In Advances in NeuralInformation Processing Systems
 The importance of better models in stochastic optimization,2019, Proceedingsof the National Academy of Sciences
 Nonlinear Programming,1999, Athena Scientific
 Incremental proximal methods for large scale convex optimization,2011, Mathematicalprogramming
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2014, In Proceedings of the3rd International Conference on Learning Representations
 Escaping from saddle pointsâ€”online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Mnist handwritten digit database,2010, 2010
 Non-convex finite-sum optimization viascsg methods,2017, In Advances in Neural Information Processing Systems
 Stochastic proximal iteration: a non-asymptotic improvement uponstochastic gradient descent,2014, Preprint
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Minimisation methods for training feedforward neural networks,1994, Neuralnetworks
 Incremental constraint projection-proximal methods fornonsmooth convex optimization,2013, SIAM J
 First-order stochastic algorithms for escaping from saddle pointsin almost linear time,2018, In Advances in Neural Information Processing Systems
