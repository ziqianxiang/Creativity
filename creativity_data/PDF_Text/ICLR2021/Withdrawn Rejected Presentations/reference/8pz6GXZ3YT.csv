title,year,conference
 Stronger generalization bounds for deepnets via a compression approach,2018, In Jennifer Dy and Andreas Krause (eds
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprint arXiv:2007
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprint arXiv:2007
 Learning to prune deep neural networks via layer-wise optimalbrain surgeon,2017, In Advances in Neural Information Processing Systems
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, In International Conference on MachineLearning
 Rigging the lottery: Makingall tickets winners,2020, International Conference on Machine Learning
 Stabilizing the lotteryticket hypothesis,2019, arXiv preprint arXiv:1903
 Linear mode connectivityand the lottery ticket hypothesis,2019, arXiv preprint arXiv:1912
 Guaranteed recovery of one-hidden-layer neural networks viacross entropy,2018, arXiv preprint arXiv:1802
 Second order derivatives for network pruning: Optimal brain surgeon,1993, InAdvances in neural information processing systems
 Network trimming: A data-driven neuronpruning approach towards efficient deep architectures,2016, arXiv preprint arXiv:1607
 Tensor factorization via matrix factorization,2015, InArtificial Intelligence and Statistics
 Optimal brain damage,1990, In Advances in neural informationprocessing systems
 Rethinking the value of networkpruning,2018, In International Conference on Learning Representations
 On the computational efficiency of training neuralnetworks,2014, In Advances in neural information processing SyStemS
 Proving the lottery ticket hypothesis:Pruning is all you need,2020, arXiv preprint arXiv:2002
 Variational dropout sparsifies deep neural net-works,2017, In International Conference on Machine Learning
 Comparing rewinding and fine-tuning in neural networkpruning,2019, In International Conference on Learning Representations
 Spurious local minima are common in two-layer relu neural networks,2018, InInternational Conference on Machine Learning
 Theoretical insights into the optimization land-scape of over-parameterized shallow neural networks,2018, IEEE Transactions on Information Theory
 Data-free parameter pruning for deep neural networks,2015, arXiv preprintarXiv:1507
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Picking winning tickets before training by preservinggradient flow,2019, In International Conference on Learning Representations
 Designing energy-efficient convolutional neural networksusing energy-aware pruning,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Drawing early-bird tickets: Toward more efficient training of deepnetworks,2019, In International Conference on Learning Representations
 Understanding deeplearning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Guaranteed convergence of train-ing convolutional neural networks via accelerated gradient descent,2020, In 2020 54th Annual Conferenceon Information Sciences and Systems (CISS)
 Fast learning of graph neural networkswith guaranteed generalizability:one-hidden-layer case,2020, In 2020 International Conference on MachineLearning (ICML)
 Learning one-hidden-layer convo-lutional neural networks via accelerated gradient descent with generalizability guarantees,2020, submitted toIEEE Transactions on Neural Networks and Learning Systems
 Non-vacuous generaliza-tion bounds at the imagenet scale: a pac-bayesian compression approach,2018, In International Conference onLearning Representations
