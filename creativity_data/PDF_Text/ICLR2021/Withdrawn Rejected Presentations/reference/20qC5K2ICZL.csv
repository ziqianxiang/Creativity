title,year,conference
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, In ICML
 Robust loss functions under label noise for deepneural networks,2017, In AAAI
 Explaining and harnessing adversarialexamples,2015, In ICLR
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In NeurIPS
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In ICLR
 Dimensionality-driven learning with noisy labels,2018, InICML
 Decoupling” when to update” from” how to update”,2017, InNeurIPS
 Noise tolerance under risk minimization,2013, IEEE Transactions onCybernetics
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR 2015
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In conference on empirical methods in natural language processing
 Joint optimization frameworkfor learning with noisy labels,2018, In IEEE CVPR
 Symmetric crossentropy for robust learning with noisy labels,2019, In IEEE ICCV
 Learning from massive noisylabeled data for image classification,2015, In IEEE CVPR
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference 2016
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 GSL and GLC are provided with one percent trusted data,2021, In contrast
