title,year,conference
 A largeannotated corpus for learning natural language inference,2015, In EMNLP
 ELECTRA: Pre-training text encoders as discriminators rather than generators,2020, In ICLR
 Supervisedlearning of universal sentence representations from natural language inference data,2017, arXivpreprint arXiv:1705
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Convolutional neural networks for sentence classification,2014, arXiv preprintarXiv:1408
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Distributed representations of sentences and documents,2014, InInternational conference on machine learning
 A structured self-attentive sentence embedding,2017, arXiv preprintarXiv:1703
 Multi-task deep neural networksfor natural language understanding,2019, arXiv preprint arXiv:1901
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 An efficient framework for learning sentencerepresentations,2018, arXiv preprint arXiv:1803
 Distributedrepresentations of words and phrases and their compositionality,2013, In NIPS
 A scalable hierarchical distributed language model,2009, InAdvances in neural information processing systems
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Improving languageunderstanding by generative pre-training,2018, Technical report
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 ERNIE: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 Attention is all you need,2017, In NIPS
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In 2018EMNLP Workshop BlackboxNLP
 Huggingfaceâ€™s transformers: State-of-the-art natural language processing,2019, ArXiv
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Ambert: A pre-trained language model with multi-grainedtokenization,2020, arXiv preprint arXiv:2008
 ERNIE: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
