title,year,conference
 Surprise-based intrinsic motivation for deep reinforcementlearning,2017, arXiv preprint arXiv:1703
 Computational theories of interaction and agency,1996, MitPress
 Intrinsically motivated learning in natural and artificialsystems,2013, Springer
 Openai gym,2016, arXiv preprint arXiv:1606
 Exploration by random networkdistillation,2018, arXiv preprint arXiv:1810
 Intrinsically motivated reinforcementlearning,2005, In Advances in neural information processing systems
 Off-policy actor-critic,2012, arXiv preprintarXiv:1205
 Taming the noise in reinforcement learning via softupdates,2015, arXiv preprint arXiv:1512
 Addressing function approximation error inactor-critic methods,2018, arXiv preprint arXiv:1802
 Learning self-imitating diverse policies,2018, arXivpreprint arXiv:1805
 Q-prop:Sample-efficient policy gradient with an off-policy critic,2016, arXiv preprint arXiv:1611
 Interpolated policy gradient: Merging on-policy and off-policy gradient esti-mation for deep reinforcement learning,2017, In Advances in Neural Information Processing Systems
 Generative adversarial self-imitationlearning,2018, arXiv preprint arXiv:1812
 Reinforcement learning withdeep energy-based policies,2017, arXiv preprint arXiv:1702
 Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,2018, arXiv preprintarXiv:1801
 Soft actor-critic algorithms and appli-cations,2018, arXiv preprint arXiv:1812
 Provably efficient maximum entropyexploration,2019, In International Conference on Machine Learning
 Vime:Variational information maximizing exploration,2016, In Advances in Neural Information ProcessingSystems
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Tsallis reinforce-ment learning: A unified framework for maximum entropy reinforcement learning,2019, arXiv preprintarXiv:1902
 Continuous control with deep reinforcement learning,2015, arXivpreprint arXiv:1509
 Exploration in model-based reinforcement learning by empirically estimating learning progress,2012, In Advances in neuralinformation processing systems
 Count-based explo-ration in feature space for reinforcement learning,2017, arXiv preprint arXiv:1706
 Leveragingexploration in off-policy algorithms via normalizing flows,2019, arXiv preprint arXiv:1905
 Human-levelcontrol through deep reinforcement learning,2015, Nature
 Trust-pcl: An off-policytrust region method for continuous control,2017, arXiv preprint arXiv:1707
 Combining policygradient and q-learning,2016, arXiv preprint arXiv:1611
 Curiosity-driven explorationby self-supervised prediction,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition Workshops
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Reinforcement learning: An introduction,1998, The MIT Press
 Robot trajectory optimization using approximate inference,2009, In Proceedings of the26th annual international conference on machine learning
 Sample efficient actor-critic with experience replay,2016, arXiv preprintarXiv:1611
 Scalable trust-regionmethod for deep reinforcement learning using kronecker-factored approximation,2017, In Advances inneural information processing systems
 On learning intrinsic rewards for policy gradientmethods,2018, In Advances in Neural Information Processing Systems
 Modeling purposeful adaptive behavior with the principle of maximum causalentropy,2010, PhD thesis
 Maximum entropy inversereinforcement learning,2008, 2008
 Qπold canbe estimated by diverse policy evaluation,2014, For estimation of Rπold
