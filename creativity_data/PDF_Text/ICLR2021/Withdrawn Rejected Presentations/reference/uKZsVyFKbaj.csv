title,year,conference
 On the Optimization of Deep Networks: ImplicitAcceleration by Overparameterization,1802, arXiv:1802
 Theloss surfaces of multilayer networks,2015, In Artificial intelligence and statistics
 MetaInit: Initializing learning by learning to initialize,2019, InAdvances in Neural Information Processing Systems
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Cellular automata as convolutional neural networks,2019, Physical Review E
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in neural information processing systems
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, In Advances in neural information processing systems
 Neural gpus learn algorithms,2015, arXivpreprint arXiv:1511
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Optimal brain damage,1990, In Advances in neuralinformation processing Systems
 Measuring the Intrinsic Dimensionof Objective Landscapes,1804, arXiv:1804
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 All you need is a good init,2016, arXiv:1511
 A deep learning based approach to reduced order modelingfor turbulent flow control using lstm neural networks,2018, arXiv preprint arXiv:1804
 The Effect of NetworkWidth on Stochastic Gradient Descent and Generalization: an Empirical Study,1905, arXiv:1905
 Neural arithmeticlogic units,2018, In Advances in Neural Information Processing Systems
