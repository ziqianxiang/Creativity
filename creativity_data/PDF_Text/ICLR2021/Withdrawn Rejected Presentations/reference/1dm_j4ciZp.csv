title,year,conference
 Probabilistic performance pro-files for the experimental evaluation of stochastic algorithms,2010, In Proceedings of the 12th annualconference on Genetic and evolutionary computation
 Algorithms for hyper-parameteroptimization,2011, In Advances in neural information processing systems
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprintarXiv:1910
 Dawnbench: An end-to-end deep learning bench-mark and competition,2017, Training
 Benchmarking optimization software with performance pro-files,2002, Mathematical programming
 Bohb: Robust and efficient hyperparameter opti-mization at scale,2018, arXiv preprint arXiv:1807
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Using a thousand optimization tasks to learn hyperparameter search strategies,2020, arXivpreprint arXiv:2002
 Spectral normalizationfor generative adversarial networks,2018, arXiv preprint arXiv:1802
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Deepobs: A deep learning optimizer benchmarksuite,2019, arXiv preprint arXiv:1903
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, arXiv preprint arXiv:1904
 Adaptive meth-ods for nonconvex optimization,2018, In Advances in neural information processing systems
 Tbd: Benchmarking and analyzing deep neural networktraining,2018, arXiv preprint arXiv:1803
