title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information processing systems
 Unsupervisedlabel noise modeling and loss correction,2019, arXiv preprint arXiv:1904
 Meta-learning with differen-tiable closed-form solvers,2018, arXiv preprint arXiv:1805
 A theoretical analysis of the number of shots in few-shotlearning,2019, arXiv preprint arXiv:1909
 Probabilistic model-agnostic meta-learning,2018, InAdvances in Neural Information Processing Systems
 Meta-learning with warped gradient descent,2019, arXiv preprint arXiv:1909
 Classification in the presence of label noise: a survey,2013, IEEEtransactions on neural networks and learning systems
 Hybrid attention-based prototypical networksfor noisy few-shot relation classification,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Dynamic few-shot visual learning without forgetting,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In Advances in neural information processingsystems
 Empirical bayes transductive meta-learning with synthetic gradients,2020, arXivpreprint arXiv:2004
 On the diffusion approximation of nonconvexstochastic gradient descent,2017, arXiv preprint arXiv:1705
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Learning deep netWorks from noisy labels Withdropout regularization,2016, In 2016 IEEE 16th International Conference on Data Mining (ICDM)
 How do humans teach: On curriculum learning and teachingdimension,2011, In Advances in neural information processing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Siamese neural networks for one-shotimage recognition,2015, In ICML deep learning workshop
 Learning multiple layers of features from tiny images,2009, 2009
 Self-paced learning for latent variablemodels,2010, In Advances in Neural Information Processing Systems
 Meta dropout: Learning to perturblatent features for generalization,2019, In International Conference on Learning Representations
 Meta dropout: Learning to perturb latent features forgeneralization,2020, In International Conference on Learning Representations
 Meta-sgd: Learning to learn quickly for few-shotlearning,2017, arXiv preprint arXiv:1707
 A variational analysis of stochastic gradientalgorithms,2016, In International conference on machine learning
 Concept learning with energy-based models,2018, arXiv preprint arXiv:1811
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprintarXiv:1511
 Meta-learning with implicitgradients,2019, In Advances in Neural Information Processing Systems
 Optimization as a model for few-shot learning,2016, 2016
 Few-shot learning with embedded classmodels and shot-free meta training,2019, In Proceedings of the IEEE International Conference onComputer Vision
 Meta-learning with memory-aUgmented neUral networks,2016, In International conference on machinelearning
 Few-shot learning with graph neural networks,2018, InInternational Conference on Learning Representations
 On modulating the gradientfor meta-learning,2020, In European Conference on Computer Vision
 Prototypical networks for few-shot learning,2017, InAdvances in neural information processing systems
 Self-paced dictionary learning for image classification,2012, InProceedings of the 20th ACM international conference on Multimedia
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Few-shot learning: A survey,2019, arXiv preprintarXiv:1904
 Symmetric crossentropy for robust learning with noisy labels,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 On thenoisy gradient descent that generalizes as sgd,2019, 2019
 Automatedrelational meta-learning,2020, arXiv preprint arXiv:2001
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Caml:Fast context adaptation via meta-learning,2018, 2018
