title,year,conference
 Communication complexity of distributed convex learning andoptimization,2015, In Advances in Neural Information Processing Systems 28 (NIPS 2015)
 Demystifying parallel and distributed deep learning: An in-depthconcurrency analysis,2019, ACM Computing Surveys (CSUR)
 A tightbound for set disjointness in the message-passing model,2013, In Proc
 Convex Optimization: Algorithms and Complexity,2015, now Publishers
 Determinism vs,1992, nondeterminism in multiparty communicationcomplexity
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Fully quantized distributed gradient descent,2017, Master S thesis
 Scaling distributed machine learning with the parameterserver,2014, In Proc
 On maintaining linear convergence of distributedlearning and optimization under limited communication,2019, In Proc
 Optimalalgorithms for smooth and strongly convex distributed optimization in networks,2017, In Proceedings ofthe 34th International Conference on Machine Learning (ICML 2017)
 Fundamental limits of online and distributed algorithms for statistical learning andestimation,2014, In Advances in Neural Information Processing Systems 27 (NIPS 2014)
 Distributed meanestimation with limited communication,2017, In Proceedings of the 34th International Conference onMachine Learning (ICML 2027)
 DoubleSqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In Proc
