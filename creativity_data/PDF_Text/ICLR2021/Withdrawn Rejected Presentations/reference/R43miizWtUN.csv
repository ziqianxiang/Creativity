title,year,conference
 Convolutional networks on graphs for learning molecularfingerprints,2015, In Advances in neural information processing systems
 Fast graph representation learning with PyTorch Geometric,2019, InICLR Workshop on Representation Learning on Graphs and Manifolds
 Stability of graph neural networks to relativeperturbations,2020, In International Conference on Acoustics
 Neuralmessage passing for quantum chemistry,2017, In Proceedings of the 34th International Conference onMachine Learning (ICML)
 Transient networks of spatio-temporalconnectivity map communication pathways in brain functional systems,2017, NeuroImage
 Expander graphs and their applications,2006, Bulletinof the American Mathematical Society
 Open graph benchmark: Datasets for machine learning on graphs,2020, arXivpreprint arXiv:2005
 Understanding attention and generalizationin graph neural networks,2019, In Advances in Neural Information Processing Systems 32
 Expander graphs in pure and applied mathematics,2012, Bulletin of the AmericanMathematical Society
 Sparse super-regular networks,1770, In 18th IEEEInternational Conference On Machine Learning And Applications (ICMLA)
 Fakenews detection on social media using geometric deep learning,2019, arXiv:1902
 Deep eXpander networks: Efficient deepnetworks from graph theory,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Keep it simple: Graph autoencoderswithout graph convolutional networks,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv:2006
 Deep reinforcement learning withgraph-based state representations,2020, arXiv preprint arXiv:2004
 Learning structured sparsity in deepneural networks,2016, In Advances in neural information processing systems
 Acomprehensive survey on graph neural networks,2020, IEEE Transactions on Neural Networks andLearning Systems
 Graph convolutional networks for text classification,2019, InProceedings ofthe AAAI Conference on Artificial Intelligence
 Since a simple removal of lineartransforms will result in an exponential growth in the dimension of hidden representation,2021, One19Under review as a conference paper at ICLR 2021convenient solution is to replace concatenation with a summation so that the dimension of either mor h remains unchanged after the iteration
