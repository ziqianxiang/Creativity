title,year,conference
 Language Models are Few-Shot Learners,2020, Arxiv PreprintArxiv:2005
 BERT: Pre-training of Deep BidirectionalTransformers for Language Understanding,2018, Arxiv Preprint Arxiv:1810
 Data-Efficient Image Recognitionwith Contrastive Predictive Coding,2019, Arxiv Preprint Arxiv:1905
 Long Short-Term Memory,1997, Neural Computation
 Multilayer Feedforward Networks are Universal Approx-imators,1989, Neural Networks
 Unsupervised Feature Extraction by Time-Contrastive Learning andNonlinear ICA,2016, In Advances in Neural information Processing Systems
 Nonlinear ICA Using Auxiliary Variables and GeneralizedContrastive Learning,2018, Arxiv Preprint Arxiv:1805
 Learning representations for counterfactual inference,2016, InInternational conference on machine learning
 Variational Autoencoders and Nonlinear ICA: AUnifying Framework,2019, Arxiv Preprint Arxiv:1907
 ICE-BeeM: Identifiable ConditionalEnergy-based Deep Models,2020, Arxiv Preprint Arxiv:2002
 Adam: A Method for Stochastic Optimization,2014, Arxiv PreprintArxiv:1412
 Generating Wikipediaby Summarizing Long Sequences,2018, Arxiv Preprint Arxiv:1801
 Causal effect inferencewith deep latent-variable models,2017, In Advances in Neural Information Processing Systems
 Distributed Representations of Wordsand Phrases and their Compositionality,2013, In Advances in Neural information Processing Systems
 A Scalable Hierarchical Distributed Language Model,2009, In Advances inNeural information Processing Systems
 A Fast and Simple Algorithm for Training Neural Probabilistic LanguageModels,2012, Arxiv Preprint Arxiv:1206
 Improving Language Understanding byGenerative Pre-training,2018, 2018
 Language Models are Unsuper-vised Multitask Learners,2019, Openai Blog
 SVCCA: Singular Vector CanonicalCorrelation Analysis for Deep Learning Dynamics and interpretability,2017, In Advances in Neuralinformation Processing Systems
 CNN Features Off-the-Shelf: AnAstounding Baseline for Recognition,2014, In Proceedings of The Ieee Conference On Computer Visionand Pattern Recognition Workshops
 Improved Deep Metric Learning with Multi-class N-Pair Loss Objective,2016, In Advances inNeural information Processing Systems
 Extracting and Composing Robust Featureswith Denoising Autoencoders,2008, In Proceedings of The 25th international Conference On MachineLearning
 Huggingfaceâ€™s Transformers: State-of-the-art Natural LanguageProcessing,2019, Arxiv
 XLNET: GeneralizedAutoregressive Pretraining for Language Understanding,2019, Arxiv Preprint Arxiv:1906
2 C ontinued: Case of Context RepresentationFUNCTION gOur derivation of identifiability of ge is similar to the derivation of fe ,2021, The primary difference is thatthe normalizing constants in Equation (6) do not cancel out
