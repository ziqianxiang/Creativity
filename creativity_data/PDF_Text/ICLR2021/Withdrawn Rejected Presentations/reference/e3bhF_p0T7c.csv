title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Width provably matters in optimization for deep linear neural networks,2019, InInternational Conference on Machine Learning
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, In International Conference onMachine Learning
 Gradientdescent can take exponential time to escape saddle points,2017, In Advances in neural informationprocessing systems
 Gradient descent findsglobal minima of deep neural networks,2019, In Proceedings of the 36th International Conference onMachine Learning
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Uniform convergence may be unable to explain general-ization in deep learning,2019, In Advances in Neural Information Processing Systems
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
