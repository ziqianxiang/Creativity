title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Towards evaluating the robustness of neural networks,2017, In 2017ieee symposium on security and privacy (sp)
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 On the effectiveness of intervalbound propagation for training verifiably robust models,2018, arXiv preprint arXiv:1810
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Deepfool: a simple andaccurate method to fool deep neural networks,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Certified defenses against adversarialexamples,2018, arXiv preprint arXiv:1801
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, InAdvances in Neural Information Processing Systems
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Adversariallyrobust generalization just requires more unlabeled data,2019, arXiv preprint arXiv:1906
 Efficient neural networkrobustness certification with general activation functions,2018, In Advances in neural informationprocessing systems
