title,year,conference
 The implicit regularization of stochastic gradientflow for least squares,2020, arXiv preprint arXiv:2003
 Levy Processes,1996, Cambridge University Press
 Asymptotics of stationary solutions ofmultivariate stochastic recursions with heavy tailed inputs and related limit theorems,2012, StochasticProcesses and Their Applications
 Stochastic gradient and langevinprocesses,2019, arXiv preprint arXiv:1907
 Quantitative propagationof chaos for sgd in wide neural networks,2020, arXiv preprint arXiv:2007
 On the ineffectiveness of variance reduced optimization for deeplearning,2019, In Advances in Neural Information Processing Systems
 Fractional L6vy-driven Ornstein-Uhlenbeck processes andstochastic differential equations,2011, Bernoulli
 Competing with the empirical riskminimizer in a single pass,2015, In Conference on Learning Theory
 Implicit renewal theory and tails of solutions of random equations,1991, Annals ofApplied Probability
 Concentration inequalities for random matrix products,2020, LinearAlgebra and its Applications
 Flat minima,1997, Neural Computation
 Multiplicative noise and heavy tails in stochasticoPtimization,2020, arXiv preprint arXiv:2006
 On the diffusion aPProximation of nonconvexstochastic gradient descent,2019, Annals of Mathematical Science and Applications
 Acceleratingstochastic gradient descent,2017, In Proc
 Three factors influencing minima in SGD,2017, arXiv preprint arXiv:1711
 Accelerating stochastic gradient descent using Predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 ImProving generalization Performance by sWitching fromAdam to SGD,2017, arXiv preprint arXiv:1712
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2016, arXivpreprint arXiv:1609
 The largelearning rate Phase of deeP learning: the cataPult mechanism,2020, arXiv preprint arXiv:2003
 A variational analysis of stochasticgradient algorithms,2016, In International Conference on Machine Learning
 Traditional and heavy-tailed self regularization in neuralnetWork models,2019, arXiv preprint arXiv:1901
 On estimating the tail indexand the spectral measure of multivariate α-stable distributions,2015, Metrika
 First exit time analysisof stochastic gradient descent under heavy-tailed gradient noise,2019, In Advances in Neural InformationProcessing Systems
 Non-Gaussianity ofstochastic gradient noise,2019, arXiv preprint arXiv:1910
 Once more on comparison of tail index estimators,2011, arXivpreprint arXiv:1104
 Understanding Machine Learning: From Theory toAlgorithms,2014, Cambridge University Press
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Onthe heavy-tailed theory of stochastic gradient descent for deep neural networks,2019, arXiv preprintarXiv:1912
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, In International Conference on Machine Learning
 Optimal Transport: Old and New,2009, Springer
 Why ADAM beats SGD for attention models,2019, arXiv preprintarXiv:1912
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from minima and regularization effects,2018, arXiv preprintarXiv:1803
 Let us show the tail-index α is strictly decreasing in dimension d,2021, Since aiare i
 The proof is complete,2011,	□Proof of Corollary 9
