title,year,conference
 Nice: Noise injection and clamping estimation for neuralnetwork quantization,2018, arXiv preprint arXiv:1810
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Deep learning with low precision byhalf-wave gaussian quantization,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Pact: Parameterized clipping activation for quantized neural networks,2018, arXiv:Computer Vision and Pattern Recognition
 Kronecker products and shuffle algebra,1981, IEEE Transactions on Computers
 Multi-objective optimization,2014, In Search methodologies
 Learned step size quantization,2020, 2020
 Hmq: Hardware friendly mixed precisionquantization block for cnns,2020, arXiv preprint arXiv:2007
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Condensenet: Anefficient densenet using learned group convolutions,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Trained quantization thresholds foraccurate and efficient fixed-point inference of deep neural networks,2019, arXiv: Computer Vision andPattern Recognition
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Quantizing deep convolutional networks for efficient inference: Awhitepaper,2018, arXiv preprint arXiv:1806
 Learning multiple layers of features from tiny images,2009, 2009
 Efficient non-uniform quantizer for quantized neural network targeting reconfigurable hardware,2018, arXiv preprintarXiv:1811
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Differentiabledynamic normalization for learning deep representation,2019, In International Conference on MachineLearning
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Discovering low-precision networks close to full-precision networks for efficient embedded inference,2018, arXiv preprint arXiv:1809
 8-bit inference with tensorrt,2017, In GPU technology conference
 Wrpn: wide reduced-precisionnetworks,2017, arXiv preprint arXiv:1709
 Convolutional neural networks usinglogarithmic data representation,2016, arXiv preprint arXiv:1603
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Mixed precision dnns: All you need is a goodparametrization,2020, In International Conference on Learning Representations
 Haq: Hardware-aware automatedquantization with mixed precision,2019, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 Releq: An automatic reinforcement learning approach for deep quantizationof neural netWorks,2018, 2018
 Under-standing straight-through estimator in training activation quantized neural nets,2019, InternationalConference on Learning Representations
 Blendedcoarse gradient descent for full quantization of deep neural netWorks,2019, Research in the MathematicalSciences
 Lq-nets: Learned quantization forhighly accurate and compact deep neural netWorks,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Interleaved group convolutions,2017, InProceedings of the IEEE International Conference on Computer Vision
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 1 Evaluation on ImageNetThe ImageNet dataset consists of 1,2021,2M training and 50K validation images
