title,year,conference
 A continuous-time view of early stopping for leastsquares regression,2019, AISTATS
 The implicit regularization of stochastic gradientflow for least squares,2020, In International Conference on Machine Learning
 Tight nonparametric convergence rates forstochastic gradient descent under the noiseless linear model,2020, arXiv preprint arXiv:2006
 Training with noise is equivalent to tikhonov regularization,1995, Neural computation
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InAdvances in neural information processing systems
 Distilling the knowledge in a neural network,2015, InNeurIPS Deep Learning and Representation Learning Workshop
 Flat minima,1997, Neural Computation
 On the diffusion approximation of noncon-vex stochastic gradient descent,2019, Annals of Mathematical Sciences and Applications
 Quasi-potential as an implicit regularizerfor the loss function in the stochastic gradient descent,2019, arXiv preprint arXiv:1901
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Mentornet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In International Conferenceon Machine Learning
 On large-batch trainingfor deep learning: Generalization gap and sharp minima,2017, In In International Conference onLearning Representations (ICLR)
 Self-knoWledge distilla-tion: A simple Way for better generalization,2020, arXiv preprint arXiv:2006
 Gradient descent With early stopping isprovably robust to label noise for overparameterized neural netWorks,2020, In International Conferenceon Artificial Intelligence and Statistics
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Statistical inference usingsgd,2018, In AAAI
 Unifying distillationand privileged information,2016, In In International Conference on Learning Representations (ICLR)
 Beyond least-squares: Fast rates for regularized empirical risk minimization through self-concordance,2019, InConference on Learning Theory
 Read-ing digits in natural images With unsupervised feature learning,2011, In NeurIPS Workshop on DeepLearning and Unsupervised Feature Learning
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural netWorks,2014, In International Conference on LearningReporesentations (ICLR)
 A tail-index analysis of stochastic gradientnoise in deep neural netWorks,2019, arXiv preprint arXiv:1901
 Symmetric cross en-tropy for robust learning With noisy labels,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 On thenoisy gradient descent that generalizes as sgd,2020, In International Conference on Machine Learning(ICML)
 Sphmc: Spectral hamiltonian monte carlo,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 Knowledge distillation meets self-supervision,2020, In European Conference on Computer Vision
 Understanding deep learning requiresrethinking generalization,2017, In International Conference on Learning Representations
 Cyclicalstochastic gradient mcmc for bayesian deep learning,2019, In International Conference on LearningRepresentations
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, In ICML
