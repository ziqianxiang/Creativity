title,year,conference
 Greedy layer-wise training of deepnetworks,2007, In Advances in neural information processing systems
 Evolution strategies - a comprehensive introduction,2004, Natural Computing
 Gradient descent provably optimizes over-parameterized neural networks,2019, ICLR
 Weight agnostic neural networks,2019, In Advances in Neural Information ProcessingSystems
 Deep residual learning for image recognition,2016, In 2016 IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR)
 Training products of experts by minimizing contrastive divergence,2002, Neural computation
 A fast learning algorithm for deep belief nets,2006, Neuralcomputation
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, Tech Report
 Learning overparameterized neural networks via stochastic gradient descent onstructured data,2018, In Advances in Neural Information Processing Systems
 Darts: Differentiable architecture search,2018, arXiv preprintarXiv:1806
 Putting an end to end-to-end: Gradient-isolated learningof representations,2019, In Advances in Neural Information Processing Systems
 Learning internal representations by errorpropagation,1985, Technical report
 Evolution strategies as a scalablealternative to reinforcement learning,2017, arXiv preprint arXiv:1703
 On the importance of initialization andmomentum in deep learning,2013, In International conference on machine learning
 Training neuralnetworks without gradients: A scalable admm approach,2016, In International conference on machine learning
 Learning curves for stochastic gradient descent in linear feedfor-ward networks,2004, In Advances in Neural Information Processing Systems 16
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
