title,year,conference
 Meta-learning by adjusting priors based on extended PAC-Bayes theory,2018, InInternational Conference on Machine Learning
 Learning to learn by gradient descentby gradient descent,2016, arXiv
 Making a science of model search: Hyperparameteroptimization in hundreds of dimensions for vision architectures,2013, In International Conference onMachine Learning
 Weight uncertainty inneural networks,2015, arXiv
 Concentration inequalities : a nonasymptotictheory of independence,2013, chapter 2
 Stochastic gradient hamiltonian monte carlo,2014, InInternational Conference on Machine Learning
 Learning to Learn without Gradient Descent by GradientDescent,2017, In International Conference on Machine Learning
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In International Conference on Machine Learning
 Probabilistic model-agnostic meta-learning,2018, InAdvances in Neural Information Processing Systems
 Multivariate time series imputation withvariational autoencoders,2019, arXiv preprint arXiv:1907
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In International Conference on Machine Learning
 Neural processes,2018, arXiv
 Pac-bayesian theorymeets bayesian inference,2016, In Advances in Neural Information Processing Systems
 Model selection in bayesian neural networks via horseshoepriors,2017, arXiv
 Practical variational inference for neural networks,2011, In Advances in Neural InformationProcessing Systems
 A primer on PAC-Bayesian learning,2019, arXiv
 Probabilistic backpropagation for scalablelearning of bayesian neural networks,2015, In International Conference on Machine Learning
 Learning To Learn Using GradientDescent,2001, In International Conference on Artificial Neural Networks
 Stochastic neUral network with kronecker flow,2020, In International Conferenceon Artificial Intelligence and Statistics
 AUto-encoding variational bayes,2014, In International Conferenceon Learning Representations
 Adaptiveand safe bayesian optimization in high dimensions via one-dimensional sUbspaces,2019, In InternationalConference on Machine Learning
 Contextual gaussian process bandit optimization,2011, In Advances inNeural Information Processing Systems
 Accurate uncertainties for deep learningusing calibrated regression,2018, In International Conference on Machine Learning
 Human-level concept learningthrough probabilistic program induction,2015, Science
 Bandit Algorithms,2020, Cambridge University Press
 Stein Variational Gradient Descent: A General Purpose BayesianInference Algorithm,2016, arXiv
 Structured and efficient variational deep learning with matrixgaussian posteriors,2016, In International Conference on Machine Learning
 Some PAC-Bayesian theorems,1999, Machine Learning
 Swissfel: the swiss x-rayfree electron laser,2017, Applied Sciences
 On First-Order Meta-Learning Algorithms,2018, arXiv
 Expressivepriors in bayesian neural networks: Kernel combinations and periodic functions,2020, In Uncertainty inArtificial Intelligence
 Rethink and redesign meta learning,2018, arXiv
 Deep bayesian bandits showdown: An empiricalcomparison of bayesian deep networks for thompson sampling,2018, In International Conference onLearning Representations
 Noise Regularization for Conditional Density Estimation,2019, arXiv preprintarXiv:1907
 Promp: Proximalmeta-policy search,2019, In International Conference on Learning Representations
 PACOH: Bayes-Optimal Meta-Learning withPAC-Guarantees,2020, arXiv
 Prototypical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems
 Gaussian processoptimization in the bandit setting: No regret and experimental design,2009, In International Conferenceon Machine Learning
 Calibrating general posterior credible regions,2018, Biometrika
 On the likelihood that one unknown probability exceeds another in view ofthe evidence of two samples,1933, Biometrika
 Matching networks for oneshot learning,2016, In Advances in Neural Information Processing Systems
 Understanding priors inBayesian neural networks at the unit level,2019, In International Conference on Machine Learning
 Function space particle optimization forbayesian neural networks,2019, arXiv
 Inferring latent taskstructure for multitask learning by multiple kernel learning,2010, BMC bioinformatics
