title,year,conference
 Unreasonable effectiveness of learning neural networks: From accessi-ble states and robust ensembles to basic algorithmic schemes,0027, Proceedings of the National Academyof Sciences
 Large scaledistributed deep networks,2012, In NIPS
 How neural networks find generalizable solutions: Self-tuned annealing indeep learning,2020, ArXiv
 Escaping from saddle points—online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Deep residual learning for imagerecognition,2015, CVPR
 Deep neuralnetworks for acoustic modeling in speech recognition,2012, Signal Processing Magazine
 Flat minima,1997, Neural Computation
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, CoRR
 Squeeze-and-excitation networks,2018, CVPR
 Densely connected convolutionalnetworks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Finding flatter minima With sgd,2018, In ICLR (Workshop)
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 ADAM: a method for stochastic optimization,2015, In International Conferenceon Learning Representations (ICLR)
 Imagenet classification With deep convolu-tional neural netWorks,2012, In Advances in neural information processing systems
 Can decentralizedalgorithms outperform centralized algorithms? A case study for decentralized parallel stochasticgradient descent,2017, In NIPS
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2018, In ICML
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, ICML
 Attention is all you need,2017, In I
 Aggregated residualtransformations for deep neural networks,2017, CVPR
 ReducingBERT pre-training time from 3 days to 76 minutes,2019, CoRR
 Model accuracy and runtime tradeoff in distributed deeplearning: A systematic study,2016, In IEEE International Conference on Data Mining
 Distributed deep learning strategies for automatic speech recognition,2019, In ICASSP’2019
 A highly efficient distributed deep learningsystem for automatic speech recognition,2019, In INTERSPEECH’2019
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, CVPR
 Energy-entropy competitionand the effectiveness of stochastic gradient descent in machine learning,1362, Molecular Physics
