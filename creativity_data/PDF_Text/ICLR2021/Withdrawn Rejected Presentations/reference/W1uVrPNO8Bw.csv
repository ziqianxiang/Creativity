title,year,conference
 Net-trim: Convex pruning of deepneural networks with performance guarantee,2017, In Advances in Neural Information ProcessingSystems (NIPS)
 Sparsely activated networks,2020, arXiv preprintarXiv:1907
 Non-equlibrium Thermodynamics,1962, North Holland Publishing
 A study of gradient variance indeep learning,2020, arXiv preprint arXiv:2007
 The lotteryticket hypothesis at scale,2020, arXiv preprint arXiv:1903
 Identity mappings in deep residualnetworks,2016, In ECCV
 Flat minima,1997, Neural Computation
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations (ICLR)
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InICLR
 Imagenet classification with deep convo-lutional neural networks,2012, In F
 Inducing and exploiting activa-tion sparsity for fast neural network inference,2020, In International Conference on Machine Learning(ICML)
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems (NIPS)
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2020, arXiv preprint arXiv:1907
 Runtime neural pruning,2017, In Advances in NeuralInformation Processing Systems (NIPS)
 Very deep convolutional networks for large-scale image recogni-tion,2015, In ICLR
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In ICLR
 Thermal diffusion potentials and the Soret effect,1954, Nature
 How noise affects the hessian spectrum in overparameterizedneural networks,2019, arXiv preprint arXiv:1910
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
