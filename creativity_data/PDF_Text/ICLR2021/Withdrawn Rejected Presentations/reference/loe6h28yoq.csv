title,year,conference
 The vulnerability of learning to adversarial perturbation increases withintrinsic dimensionality,2017, In 2017 IEEE Workshop on Information Forensics and Security (WIFS)
 Deep k-nn for noisy labels,2020, In ICML
 The security of machinelearning,2010, Machine Learning
 Bagging classifiersfor fighting poisoning attacks in adversarial classification tasks,2011, In International workshop onmultiple classifier systems
 Poisoning attacks against support vector machines,2012, InICML
 Security evaluation of pattern classifiers underattack,2013, IEEE transactions on knowledge and data engineering
 Poisoning behavioral malware clustering,2014, In Proceedings of the 2014 workshopon artificial intelligent and security workshop
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Nearest neighbor pattern classification,1967, IEEE transactions oninformation theory
 Histograms of oriented gradients for human detection,2005, In 2005 IEEEcomputer society conference on computer vision and pattern recognition (CVPRâ€™05)
 Why do adversarial attacks transfer? explaining transferabilityof evasion and poisoning attacks,2019, In USENIX Security Symposium
 On the consistency of exact and approximate nearestneighbor with noisy data,2018, Arxiv
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2017, arXiv preprint arXiv:1708
 Discovering informative patterns and datacleaning,1996, In Proceedings of the 3rd International Conference on Knowledge Discovery and DataMining
 Metapoison: Practicalgeneral-purpose clean-label data poisoning,2020, arXiv preprint arXiv:2004
 Intrinsic certified robustness of bagging againstdata poisoning attacks,2020, arXiv preprint arXiv:2008
 Deep partition aggregation: Provable defense against generalpoisoning attacks,2020, arXiv preprint arXiv:2006
 Data poisoning attacks on factorization-based collaborative filtering,2016, In NeurIPS
 Data poisoning against differentially-private learners:Attacks and defenses,2019, In International Joint Conference on Artificial Intelligence
 The security of latent dirichlet allocation,2015, In Artificial Intelligence andStatistics
 Using machine teaching to identify optimal training-set attacks onmachine learners,2015, In AAAI
 Towards poisoning of deep learning algorithms with back-gradientoptimization,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Fast rates for a knn classifier robust to unknown asymmetric labelnoise,2019, arXiv preprint arXiv:1906
 Certified robustness tolabel-flipping attacks via randomized smoothing,2020, In ICML
 On the robustness of deep k-nearest neighbors,2019, In 2019 IEEESecurity and Privacy Workshops (SPW)
 When doesmachine learning FAIL? generalized transferability for evasion and poisoning attacks,2018, In USENIXSecurity Symposium
 Intriguing properties of neural networks,2014, In ICLR
 Spectral signatures in backdoor attacks,2018, In Advancesin Neural Information Processing Systems
 Evaluating the robustness ofnearest neighbor classifiers: A primal-dual perspective,2019, arXiv preprint arXiv:1906
 Analyzing the robustness of nearest neighborsto adversarial examples,2018, In International Conference on Machine Learning
 Supportvector machines under adversarial label contamination,2015, Neurocomputing
 Robustness for non-parametric classification: A generic attack and defense,2020, In International Conference on ArtificialIntelligence and Statistics
