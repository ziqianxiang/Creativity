title,year,conference
 Tensorflow: A system for large-scalemachine learning,2016, In OSDI
 Learning to learn by gradient descent by gradient descent,2016, In Advances inNeural Information Processing Systems
 Re-evaluating evaluation,2018, In Advancesin Neural Information Processing Systems
 Open-ended learning in symmetric zero-sum games,2019, arXiv preprintarXiv:1901
 Deepmind lab,2016, arXiv preprintarXiv:1612
 Neural optimizer search with reinforcementlearning,2017, 2017
 Learning a synaptic learning rule,1990, UniVerSitede Montreal
 Algorithms for hyper-parameteroptimization,2011, In J
 A mean field theory of quantized deep networks:The quantization-depth trade-off,2019, arXiv preprint arXiv:1906
 Food-101 - mining discriminative compo-nents with random forests,2014, In European Conference on Computer Vision
 The OpenCV Library,2000, Dr
 Smash: one-shot modelarchitecture search through hypernetworks,2017, arXiv preprint arXiv:1708
 Bayesian optimization in alphago,2018, arXiv preprint arXiv:1812
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprint arXiv:1910
 A downsampled variant of imagenet as analternative to the cifar datasets,2017, arXiv preprint arXiv:1707
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Quantifying generalizationin reinforcement learning,2018, arXiv preprint arXiv:1812
 Density estimation using real nvp,2016, arXivpreprint arXiv:1605
 Incorporating nesterov momentum into adam,2016, 2016
 Finding structure in time,1990, Cognitive science
 Bohb: Robust and efficient hyperparameter optimiza-tion at scale,2018, arXiv preprint arXiv:1807
 Stochastic gradient methods withlayer-wise adaptive moments for training of deep networks,2019, arXiv preprint arXiv:1905
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Generating sequences with recurrent neural networks,2013, arXiv preprint arXiv:1308
 Coco: A platformfor comparing continuous optimizers in a black-box setting,2016, arXiv preprint arXiv:1603
 On the selection of initialization and activationfunction for deep neural networks,2018, arXiv preprint arXiv:1805
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Reducing the dimensionality of data with neuralnetworks,2006, science
 Long short-term memory,1997, Neural computation
 Universal statistics of fisher information indeep neural networks: mean field approach,2018, 2018
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Tabular benchmarks for joint architecture and hyperparameteroptimization,2019, arXiv preprint arXiv:1905
 Learning curve predictionwith bayesian neural networks,2016, 2016
 Imagenet classification with deep convo-Iutional neural networks,2012, In Advances in neural information processing Systems
 Parallel architecture andhyperparameter search via successive halving and classification,2018, arXiv preprint arXiv:1805
 Deep learning,2015, nature
 Learning to optimize neural nets,2017, arXiv preprint arXiv:1703
 Hyperband:A novel bandit-based approach to hyperparameter optimization,2016, arXiv preprint arXiv:1603
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Learning gradient descent: Better generalization and longerhorizons,2017, arXiv preprint arXiv:1703
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Gotta learn fast: Anew benchmark for generalization in rl,2018, arXiv preprint arXiv:1804
 DeepWeeds: A Multiclass Weed Species Image Dataset forDeep Learning,2019, Scientific Reports
 Solving rubikâ€™s cube with a robot hand,2019, arXiv preprint
 Masked autoregressive flow for densityestimation,2017, In Advances in Neural Information Processing Systems
 Scalable hyperpa-rameter transfer learning,2018, In Advances in Neural Information Processing Systems
 Critical initialisation for deepsignal propagation in noisy rectifier neural networks,2018, In S
 Searching for activation functions,2017, 2017
 Meta-learning for evolutionary parameteroptimization of classifiers,2012, Machine learning
 Unit tests for stochastic optimization,2013, arXivpreprint arXiv:1312
 On learning how to learn learning strategies,1995, 1995
 Deepobs: A deep learning optimizer benchmarksuite,2019, International Conference on Learning Representations
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Measuring the effects of data parallelism on neural network training,2018, arXiv preprintarXiv:1811
 On thetunability of optimizers in deep learning,2019, arXiv preprint arXiv:1910
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Scalable bayesian optimization using deep neuralnetworks,2015, In International conference on machine learning
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Multi-task bayesian optimization,2013, In Advances inneural information processing systems
 Freeze-thaw bayesian optimization,2014, arXivpreprint arXiv:1406
 DeepMind control suite,2018, Technical report
 Meta-dataset: A datasetof datasets for learning to learn from few examples,2019, arXiv preprint arXiv:1903
 Deep reinforcement learning with double q-learning,2016, In Thirtieth AAAI conference on artificial intelligence
 Openml: Networked science inmachine learning,2013, SIGKDD Explorations
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, arXiv preprint arXiv:1905
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Learning hyperparameter optimizationinitializations,2015, In 2015 IEEE international conference on data science and advanced analytics(DSAA)
 Sequential model-free hyperparametertuning,2015, In 2015 IEEE international conference on data mining
 No free lunch theorems for optimization,1997, IEEEtransactions on evolutionary computation
 Understanding short-horizon bias instochastic meta-optimization,2016, pp
 Sun database: Large-scale scenerecognition from abbey to zoo,2010, In 2010 IEEE Computer Society Conference on Computer Visionand Pattern Recognition
 Mean field residual networks: On the edge of chaos,2017, In AdvancesIn Neural Information Processing Systems
 Thevisual task adaptation benchmark,2019, arXiv preprint arXiv:1910
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 Neural architecture search with reinforcement learning,2017, InternationalConference on Learning Representations
 This is extremely computationally expensive and in practice approximationsmust be used,2021, One common family of approximations is short horizon based methods
 Considerable improvements havebeen made in language Melis et al,2018, (2017)
