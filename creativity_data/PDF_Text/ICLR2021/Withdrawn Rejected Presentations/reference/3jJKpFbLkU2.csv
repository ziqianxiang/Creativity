title,year,conference
 Deep pnml: Predictive normalized maximum likelihood for deepneural networks,2019, arXiv preprint arXiv:1904
 Residuals and influence in regression,1982, New York: Chapman and Hall
 Fogel and M,2018, Feder
 Fogel and M,2018, Feder
 Gal and Z,2015, Ghahramani
 A swiss army infinitesimaljackknife,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 A tutorial introduction to the minimum description length principle,2004, 6 2004
 Benchmarking neural network robustness to common corruptionsand perturbations,2019, arXiv preprint arXiv:1903
 Flat minima,1997, Neural Computation
 Averaging Weights Leads toWider Optima and Better Generalization,2018, In UAI
 Worst-case bounds for Gaussian process models,2006, InAdvances in neural information processing Systems
 Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles,2016, Advances in Neural Information Processing Systems
 A simple baseline for bayesianuncertainty in deep learning,2019, In Advances in Neural Information Processing Systems
 Conditional NML universal models,2007, In 2007 Information Theory andApplications Workshop
 Fisher information and stochastic complexity,0018, IEEE Transactions on InformationTheory
 A scalable laplace approximation for neural networks,2018, In 6thInternational Conference on Learning Representations
 Bayesian network structure learning usingfactorized NML universal models,2008, In 2008 Information Theory and Applications Workshop
 Universal sequential coding of single messages,1987, Problems of Information Transmission
 Aggregating Strategies,1990, In Proceedings of the Third Annual Workshop on ComputationalLearning Theory
