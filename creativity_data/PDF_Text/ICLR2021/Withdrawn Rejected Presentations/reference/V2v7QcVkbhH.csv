title,year,conference
 Meta-learning with differ-entiable closed-form solvers,2018, arXiv preprint arXiv:1805
 A closerlook at few-shot classification,2019, arXiv preprint arXiv:1904
 Image deformationmeta-networks for one-shot learning,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Adversarially robust few-shot learning: A meta-learning approach,2019, arXiv
 Unraveling meta-learning: Understanding feature representations for few-shot tasks,2020, arXivpreprint arXiv:2002
 Maxup: A simple way to improvegeneralization of neural network training,2020, arXiv preprint arXiv:2002
 A closer look at featurespace data augmentation for few-shot intent classification,2019, arXiv preprint arXiv:1910
 Transductive few-shot learningwith meta-learned confidence,2020, arXiv preprint arXiv:2002
 Meta-learning withdifferentiable convex optimization,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Task augmentation by rotating for meta-learning,2020, arXivpreprint arXiv:2003
 Tadam: Task dependent adaptivemetric for improved few-shot learning,2018, In Advances in Neural Information Processing Systems
 Meta-learning requires meta-augmentation,2020, arXivpreprint arXiv:2007
 Self-augmentation: Generalizing deep net-works to unseen classes for few-shot learning,2020, arXiv preprint arXiv:2004
 Prototypical networks for few-shot learning,2017, InAdvances in neural information processing Systems
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Donâ€™toverlook the support set: Towards improving generalization in meta-learning,2020, arXiv preprintarXiv:2007
 Meta-learningwithout memorization,2019, arXiv preprint arXiv:1912
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
