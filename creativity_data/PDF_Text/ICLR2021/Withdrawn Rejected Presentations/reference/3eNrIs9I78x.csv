title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Sub-dominant dense clusters allow for simple learning and high computational performance in neuralnetworks with discrete synapses,2015, Physical review letters
 Unreasonable effectiveness of learning neural networks: Fromaccessible states and robust ensembles to basic algorithmic schemes,2016, Proceedings of the NationalAcademyofSciences
 The tradeoffs of large scale learning,2008, In Advances in neuralinformation processing systems
 On-line learning for very large data sets,2005, Applied stochastic modelsin business and industry
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Theloss surfaces of multilayer networks,2015, In Artificial intelligence and statistics
 Distributed deep learning using syn-chronous stochastic gradient descent,2016, arXiv preprint arXiv:1602
 Large scale distributed deep networks,2012, InAdvances in neural information processing systems
 Sharp minima can generalizefor deep nets,2017, International Conference on Machine Learning
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Fast rates for empirical risk minimization of strict saddleproblems,2017, JMLR: Workshop and Conference Proceedings
 Flat minima,1997, Neural Computation
 Batch normalization: Accelerating deep network training by re-ducing internal covariate shift,2015, International Conference on International Conference on MachineLearning
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Visualizing the loss land-scape of neural nets,2018, In Advances in Neural Information Processing Systems
 Exploring general-ization in deep learning,2017, In Advances in neural information processing systems
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 The impact of local geometry and batch size on stochastic gradient descent for noncon-vex problems,2017, arXiv preprint arXiv:1709
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 A bayesian perspective on generalization and stochastic gradientdescent,2017, International Conference on Learning Representations
 Vc dimension of neural networks,1998, NATO ASI Series F Computer and SystemsSciences
 Identifying generalizationproperties in neural networks,2018, arXiv preprint arXiv:1809
 Smoothout:Smoothing out sharp minima to improve generalization in deep learning,2018, arXiv preprintarXiv:1805
