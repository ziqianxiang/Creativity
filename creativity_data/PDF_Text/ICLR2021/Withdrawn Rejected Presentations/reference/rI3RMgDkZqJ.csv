title,year,conference
 Constrained policy optimization,2017, InInternational Conference on Machine Learning (ICML)
 Optimality and approximationwith policy gradient methods in Markov decision processes,2019, arXiv preprint arXiv:1908
 Global optimality guarantees for policy gradient methods,2019, arXivpreprint arXiv:1906
 A note on the linear convergence of policy gradient methods,2020, arXivpreprint arXiv:2007
 Neural temporal-difference and q-learningprovably converge to global optima,2019, arXiv preprint arXiv:1905
 Fast global convergence ofnatural policy gradient methods with entropy regularization,2020, arXiv preprint arXiv:2007
 A Lyapunov-based approach to safe reinforcement learning,2018, In Advances in Neural Information ProcessingSystems (NIPS)
 Lyapunov-based safe policy optimization for continuous control,2019, arXiv preprintarXiv:1901
 Safe exploration in continuous action spaces,2018, arXiv preprint arXiv:1801
 A tale of two-timescale reinforcement learning withthe tightest finite-time bound,2019, arXiv preprint arXiv:1911
 Mastering the real-time strategy game starcraft ii,2019, 2019
 Provablyefficient safe exploration via primal-dual policy optimization,2020, arXiv preprint arXiv:2003
 Natural policy gradientprimal-dual method for constrained Markov decision processes,2020, In Proc
 Gradient descent provably optimizes over-parameterized neural networks,2018, In In Proc
 Global convergence of policygradient methods for the linear quadratic regulator,2018, arXiv preprint arXiv:1801
 A general safety framework for learning-based control in uncertain roboticsystems,2018, IEEE Transactions on Automatic Control
 Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor,2018, In In Proc
 Approximately optimal approximate reinforcement learning,2002, InProc
 A natural policy gradient,2002, In Proc
 On the sample complexity of actor-criticmethod for reinforcement learning with function approximation,2019, arXiv preprint arXiv:1910
 Algorithms for stochastic optimization with functional orexpectation constraints,2016, arXiv preprint arXiv:1604
 Derivative-free methods for policy optimization: guarantees for linear quadraticsystems,2018, arXiv preprint arXiv:1812
 Stochas-tic variance-reduced policy gradient,2018, In International Conference on Machine Learning (ICML)
 Constrainedreinforcement learning has zero duality gap,2019, In In Proc
 On the finite-time convergence ofactor-critic algorithm,2019, In Optimization Foundations for Reinforcement Learning Workshop atAdvances in Neural Information Processing Systems (NeurIPS)
 Benchmarking safe exploration in deep reinforcementlearning,2019, arXiv preprint arXiv:1910
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Adaptive trust region policy optimization: Globalconvergence and faster rates for regularized mdps,2019, arXiv preprint arXiv:1909
 Hessian aided policygradient,2019, In International Conference on Machine Learning (ICML)
 Mastering the game of go withouthuman knowledge,2017, Nature
 Responsive safety in reinforcement learning bypid lagrangian methods,2020, In In Proc
 Policy gradientmethods for reinforcement learning with function approximation,2000, In Proc
 Reward constrained policy optimization,2018, In InProc
 Neural policy gradient methods: Globaloptimality and rates of convergence,2019, arXiv preprint arXiv:1909
 Non-asymptotic convergenceof adam-type reinforcement learning algorithms under Markovian sampling,2020, arXiv preprintarXiv:2002
 An improved convergence analysis of stochastic variance-reduced policy gradient,2019, In Proc
 Sample efficient policy gradient methods with recursivevariance reduction,2020, In Proc
 Improving sample complexity bounds for actor-criticalgorithms,2020, arXiv preprint arXiv:2004
 Projection-basedconstrained policy optimization,2019, In In Proc
 Provably global convergence ofactor-critic: A case for linear quadratic regulator with ergodic cost,2019, In Proc
 Convergent policy optimization for safereinforcement learning,2019, In In Proc
