title,year,conference
 Matching theBlanks: Distributional Similarity for Relation Learning,2019, In ACL
 Ask the Right Questions: Active Question Reformulation withReinforcement Learning,2018, In ICLR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Integrating GraPh Con-textUalized Knowledge into Pre-trained LangUage Models,2019, In arXiv preprint arXiv:1912
 Parameter-Efficient Transfer Learning forNLP,2019, In ICML
 Cosmos QA: Machine readingcomPrehension with contextUal commonsense reasoning,2019, In EMNLP
 Negated LAMA: Birds cannot fly,2019, arXiv preprintarXiv:1911
 InformingUnsUPervised Pretraining with external lingUistic knowledge,2019, arXiv preprint arXiv:1909
 Sensebert: Driving some sense into bert,2019, arXiv preprint arXiv:1908
 Fine-grained entity recognition,2012, In Jorg Hoffmann and Bart Selman(eds
 Design challenges for entity linking,2015, TACL
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Deep contextualized word representations,2018, In NAACL
 Knowledge enhanced contextual word representations,2019, In EMNLP
 Improving language under-standing by generative pre-training,2018, OpenAI Blog
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Bidirectional AttentionFlow for Machine Comprehension,2016, ArXiv
 An attentive neural architec-ture for fine-grained entity type classification,2016, In Proceedings of the 5th Workshop on AutomatedKnowledge Base Construction(AKBC)
 Attention is all you need,2017, In NeurIPS
 KEPLER:A Unified Model for Knowledge Embedding and Pre-trained Language Representation,2019, arXivpreprint arXiv:1911
 Pretrained Encyclopedia:Weakly Supervised Knowledge-Pretrained Language Model,2020, In ICLR
 Position-aware Attention and Supervised Data Improve Slot Filling,2017, In EMNLP
 Graph Convolution over Pruned DependencyTrees Improves Relation Extraction,2018, In EMNLP
 ERNIE: EnhancedLanguage Representation with Informative Entities,2019, In ACL
 Aligning Books and Movies: Towards Story-Like Visual Explanations by WatchingMovies and Reading Books,2015, In ICCV
