title,year,conference
 On the convergence of nesterov’s accelerated gradient methodin stochastic settings,2020, arXiv preprint arXiv:2002
 Robust acceleratedgradient methods for smooth strongly convex functions,2018, arXiv preprint arXiv:1805
 A universallyoptimal multistage accelerated stochastic gradient method,2019, In Advances in Neural InformationProcessing Systems
 Advances in optimizingrecurrent networks,2013, In 2013 IEEE International Conference on Acoustics
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Convex optimization: Algorithms and complexity,2014, arXivpreprintarXiv:1405
 Decaying momentum helps neural network training,2019, arXivpreprint arXiv:1910
 On acceleration with noise-corruptedgradients,2018, arXiv preprint arXiv:1805
 First-order methods of smooth convexoptimization with inexact oracle,2014, Mathematical Programming
 Incorporating Nesterov momentum into Adam,2016, 2016
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Monotonicity and restart in fast gradient methods,2014, In 53rd IEEEConference on Decision and Control
 Why momentum really works,2017, Distill
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Long short-term memory,1997, Neural Computation
 Understanding generalization through visualizations,2019, arXiv preprint arXiv:1906
 Primal-dual subgradient methods for minimizing uniformlyconvex functions,2014, arXiv preprint arXiv:1401
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2017, arXiv preprint arXiv:1711
 On the insufficiency of existingmomentum schemes for stochastic optimization,2018, In 2018 Information Theory and ApplicationsWorkshop (ITA)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 A generic acceleration framework for stochastic compositeoptimization,2019, In Advances in Neural Information Processing Systems
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 MNIST handwritten digit database,2010, 2010
 An adaptive accelerated proximal gradient method and its homotopycontinuation for sparse optimization,2014, In International Conference on Machine Learning
 On the variance of the adaptive learning rate and beyond,2020, In International Conference onLearning Representations
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Optimal methods of smooth convex minimization,1985, USSRComputational Mathematics and Mathematical Physics
 Gradient methods for minimizing composite functions,2013, Mathematical Programming
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Convex analysis,1970, Number 28
 Computational complexity versusstatistical performance on sparse recovery problems,2015, arXiv preprint arXiv:1506
 A differential equation for modeling nesterov’saccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
 On the importance of initializationand momentum in deep learning,2013, In International Conference on Machine Learning
 Lecture 6,2012,5—RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Principal component analysis,1987, Chemometrics andIntelligent Laboratory Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Deep learning with elastic averaging sgd,2015, InAdvances in Neural Information Processing Systems
 Parallelized stochastic gradientdescent,2010, In Advances in Neural Information Processing Systems
 number of epoch reduction in CIFAR100 training,1001, The dashed lines aretest errors of the SGD baseline
65 ± 0,2021,14	23
