title,year,conference
 Natasha 2: Faster non-convex optimization than sgd,2018, In Advances in neuralinformation processing systems
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, In International Conference on Learning Representations
 Spider: Near-optimal non-convex opti-mization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 Finding structure with randomness:Probabilistic algorithms for constructing approximate matrix decompositions,2011, SIAM review
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In C
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Aunified convergence analysis for shuffling-type gradient methods,2020, arXiv preprint arXiv:2002
 Gradient methods for the minimisation of functionals,1963, Ussr Computational Mathematicsand Mathematical Physics
 Perturbation bounds for matrix square roots and pythagorean sums,0024, LinearAlgebra and its Applications
 Escaping saddle pointswith adaptive gradient methods,2019, arXiv preprint arXiv:1901
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Adaptive methodsfor nonconvex optimization,2018, In Advances in neural information processing systems
 On the convergence of adaptivegradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
