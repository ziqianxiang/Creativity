title,year,conference
 Net-trim: Convex pruning of deepneural networks with performance guarantee,2017, In NeuIPS
 Convex Analysis and Monotone Operator Theory inHilbert Spaces,2019, Springer
 Deep rewiring: Trainingvery sparse deep networks,2018, In ICLR
 What is the state ofneural network pruning? InI,2020, Dhillon
 Languagemodels are few-shot learners,2020, arXiv preprint arXiv:2005
 Path-level network transformationfor efficient architecture search,2018, arXiv preprint arXiv:1806
 ProxylessNAS: Direct neural architecture search on target taskand hardware,2019, In ICLR
 Once-for-all: Train one networkand specialize it for efficient deployment,2020, In ICLR
" ""Learning-Compression‚Äù algorithms for neuralnet pruning",2018, In CVPR
 A back-propagation algorithm with optimal use of hidden units,1989, In NeurIPS
 A block-iterative surrogate constraint splitting method for quadratic signalrecovery,2003, IEEE TSP
 A Douglas-Rachford splitting approach tononsmooth convex variational signal recovery,2007, IEEE JSTSP
 Deep neural network structures solvingvariational inequalities,2020, SVVA
 Lipschitz certificates for layered networkstructures driven by averaged activation operators,2020, SIMODS
 A fixed point framework for recovering signals fromnonlinear transformations,2020, arXiv preprint arXiv:2003
 NeST: A neural network synthesis tool based on agrow-and-prune paradigm,2019, IEEE TC
 Transformer-XL: Attentivelanguage models beyond a fixed-length context,2019, In ACL
 Sparse networks from scratch: Faster training without losingperformance,2020, arXiv preprint arXiv:1907
 Speech-transformer: A no-recurrence sequence-to-sequencemodel for speech recognition,2018, In ICASSP
 Rigging the lottery: Making all ticketswinners,2020, In ICML
 Dynamic network surgery for efficient dnns,2016, In NeurIPS
 Deep-speech: Scaling up end-to-end speech recognition,2014, arXiv preprint arXiv:1412
 Optimal brain surgeon and general networkpruning,1993, In ICNN
 Deep residual learning for imagerecognition,2016, In CVPR
 Searchingfor mobilenetv3,2019, In ICCV
 Densely connected convolutional networks,2017, InCVPR
 Speeding up convolutional neural networkswith low rank expansions,2014, In BMVC
 Pro-gressive skeletonization: Trimming more fat from a network at initialization,2020, arXiv preprintarXiv:2006
 Optimal brain damage,1990, In NeurIPS
 SNIP: Single-shot network prUning basedon connection sensitivity,2019, In ICLR
 A signal propagationperspective for prUning neUral networks at initialization,2020, In ICLR
 Differentiable sparsification for deep neUral networks,2019, arXiv preprint arXiv:1910
 PrUning filters forefficient convnets,2017, In ICLR
 Provable filter pruningfor efficient neural networks,2020, In ICLR
 Mcunet: Tiny deeplearning on iot devices,2020, In NeurIPS
 Dynamic model pruningwith feedback,2020, In ICLR
 Rethinking the value ofnetwork pruning,2019, In ICLR
 Bayesian compression for deep learning,2017, InNeurIPS
 Learning sparse neural networks through l0regularization,2018, In ICLR
 Learning compact recurrent neural networks,2016, InICASSP
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In ICCV
 Pointer sentinel mixturemodels,2017, In ICLR
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature Communications
 Variational dropout sparsifies deep neuralnetworks,2017, In ICML
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In ICML
 Skeletonization: A technique for trimming the fat from anetwork via relevance assessment,1989, In NeurIPS
 Structured bayesianpruning via log-normal multiplicative noise,2017, In NeurIPS
 N-BEATS: Neural basisexpansion analysis for interpretable time series forecasting,2020, In ICLR
 Librispeech: An ASR corpus based on publicdomain audio books,2015, In ICASSP
 Lookahead: A far-sighted alternative ofmagnitude-based pruning,2020, In ICLR
 Languagemodels are unsupervised multitask learners,2019, 2019
 Regularized evolution for imageclassifier architecture search,2019, In AAAI
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In ICLR
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In CVPR
 Pruning neural networks withoutany data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Butterfly transform:An efficient fft based neural architecture design,2020, In CVPR
 Picking winning tickets before training bypreserving gradient flow,2020, In ICLR
 Espnet:End-to-end speech processing toolkit,2018, In Interspeech
 Learning structured sparsity indeep neural networks,2016, In NeurIPS
 FBNet:Hardware-aware efficient convnet design via differentiable neural architecture search,2019, In CVPR
 Autoprune: Automatic network pruning byregularizing auxiliary parameters,2019, In NeurIPS
 Good subnetworksprovably exist: Pruning via greedy forward selection,2020, In ICML
 Slimmable neural networks,2019, InICLR
 Learning transferable architecturesfor scalable image recognition,2018, In CVPR
