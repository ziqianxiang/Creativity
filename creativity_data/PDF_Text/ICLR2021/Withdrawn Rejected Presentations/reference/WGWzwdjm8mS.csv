title,year,conference
 Interference and generalization in temporaldifference learning,2020, arXiv preprint arXiv:2003
 Coherent gradients: An approach to understanding generalization in gradientdescent-based optimization,2020, In International Conference on Learning Representations
 On the role of data inpac-bayes bounds,2020, arXiv preprint arXiv:2006
 Stiffness: Anew perspective on generalization in neural networks,2019, arXiv preprint arXiv:1901
 Rethinking generalization of neural models:A named entity recognition case study,2020, arXiv preprint arXiv:2001
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks,2020, arXiv preprint arXiv:2006
 The break-even point on optimization trajectories of deep neuralnetworks,2020, arXiv preprint arXiv:2002
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Gradient descent with early stop-ping is provably robust to label noise for overparameterized neural networks,2019, arXiv preprintarXiv:1903
 Toward understanding catastrophic interferencein value-based reinforcement learning,2019, Optimization Foundations for Reinforcement LearningWorkshop at NeurIPS
 Early stopping without avalidation set,2017, arXiv preprint arXiv:1703
 Some pac-bayesian theorems,1999, Machine Learning
 Extreme memorization via scale of initial-ization,2020, arXiv preprint arXiv:2008
 Uniform convergence may be unable to explain generaliza-tion in deep learning,2019, In Advances in Neural Information Processing Systems
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 The impact of the mini-batch size on the variance of gradients instochastic gradient descent,2020, arXiv preprint arXiv:2004
 Learning to learn without forgetting by maximizing transfer and minimizing interfer-ence,2018, arXiv preprint arXiv:1810
 A survey on data collection for machine learning:a big data-ai integration perspective,2019, IEEE Transactions on Knowledge and Data Engineering
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 On early stopping in gradient descent learn-ing,2007, Constructive Approximation
 Gradient diversity: a key ingredient for scalable distributed learning,2017, arXiv preprintarXiv:1706
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
