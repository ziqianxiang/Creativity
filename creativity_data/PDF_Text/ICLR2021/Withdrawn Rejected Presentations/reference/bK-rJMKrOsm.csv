title,year,conference
 Attention augmentedconvolutional networks,2019, In The IEEE International Conference on Computer Vision (ICCV)
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, In International Conference on LearningRepresentations
 BERT: pre-training ofdeep bidirectional transformers for language understanding,2019, In Jill Burstein
" Foundations of the PARAFAC procedure: Models and conditions for an""explanatory"" multi-modal factor analysis",1970, UCLA Working Papers in Phonetics
 Automatic differentiation inpytorch,2017, 2017
 In Hanna M,2019, Wallach
 Some mathematical notes on three-mode factor analysis,1966, Psychometrika
 Attention is all you need,2017, In Isabelle Guyon
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Huggingfaceâ€™stransformers: State-of-the-art natural language processing,2019, ArXiv
