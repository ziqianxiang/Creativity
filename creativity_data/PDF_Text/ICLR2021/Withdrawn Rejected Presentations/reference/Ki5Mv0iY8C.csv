title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Theory iii: Dynamics and generalization in deep networks-a simple solution,2019, arXiv preprintarXiv:1903
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Visualising basins of attractionfor the cross-entropy and the squared error neural network loss functions,2020, Neurocomputing
 Lof: identifying density-based local outliers,2000, In Proceedings of the 2000 ACM SIGMOD international conference onManagement of data
 Essentially no barri-ers in neural network energy landscape,2018, arXiv preprint arXiv:1803
 Largemargin deep networks for classification,2018, In Advances in neural information processing systems
 Flat minima,1997, Neural Computation
 Understanding generalization through visualizations,2019, arXiv preprintarXiv:1906
 Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks,2020, arXiv preprint arXiv:2006
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 On the relation betWeen the sharpest directions of dnn loss and the sgd step length,2018, arXivpreprint arXiv:1807
 Predicting the generalization gapin deep netWorks With margin distributions,2018, arXiv preprint arXiv:1810
 Fantasticgeneralization measures and Where to find them,2019, arXiv preprint arXiv:1912
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Visualizing the loss land-scape of neural nets,2018, In Advances in Neural Information Processing Systems
 Exploring general-ization in deep learning,2017, In Advances in neural information processing systems
 Empirical analysis ofthe hessian of over-parametrized neural netWorks,2017, arXiv preprint arXiv:1706
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Normalized flat minima: Exploring scaleinvariant definition of flat minima for neural networks using pac-bayesian analysis,2019, arXiv preprintarXiv:1901
 Statistical learning theory,1998, Adaptive and learning systems for signalprocessing
 Identifying generalizationproperties in neural networks,2018, arXiv preprint arXiv:1809
 Pyhessian: Neural networksthrough the lens of the hessian,2019, arXiv preprint arXiv:1912
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
