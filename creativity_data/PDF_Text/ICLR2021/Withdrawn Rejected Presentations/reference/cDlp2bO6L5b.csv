title,year,conference
 Reconciling modern machine-learning practice and theclassical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Surprises in high-dimensional ridgelessleast squares interpolation,2019, arXiv preprint arXiv:1903
 An analytic theory of generalization dynamics and transfer learningin deep linear networks,2019, In International Conference on Learning Representations (ICLR)
 The generalization error of random features regression: Precise asymp-totics and double descent curve,2019, arXiv preprint arXiv:1908
 A survey on transfer learning,2009, IEEE transactions on knowledge and dataengineering
 To transfer or not to transfer,2005, InNIPS workshop on transfer learning
 A jamming transitionfrom under-to over-parametrization affects loss landscape and generalization,2018, arXiv preprintarXiv:1810
 Random matrix theory and wireless communications,2004, Now PublishersInc
 On the number of variables to use in principal component regression,2019, InAdvances in Neural Information Processing Systems (NeurIPS)
 Taskonomy: Disentanglingtask transfer learning,2018, In IEEE conference on computer vision and pattern recognition (CVPR)
1Let us emphasize two special cases of the general result in Theorem 3,2019,1
