title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Large scale distributed neural network training through online distillation,2018, In Proceedingsof the 6th International Conference on Learning Representations
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Extracting tree-structured representations of trainednetworks,1995, In Advances in Neural Information Processing Systems 8
 Extracting Comprehensible Models from Trained Neural Networks,1996, PhDthesis
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Finding structure in time,1990, Cognitive Science
 Ensemble distillation for neural machinetranslation,2017, CoRR
 Born-again neural networks,2018, In Proceedings of the 35th International Conference on MachineLearning
 Deep Learning,2016, MIT Press
 Theoretical limitations of self-attention in neural sequence models,2020, Transactions ofthe Association for Computational Linguistics
 Modelingrecurrence for transformer,2019, arXiv preprint arXiv:1904
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Sequence-level knowledge distillation,2016, In Proceedings of the2016 Conference on Empirical Methods in Natural Language Processing
 Adam: A method for stochastic oPtimization,2014, CoRR
 Scalable syntax-aware language models using knowledge distillation,2019, In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics
 Syntactic structure distillation Pretraining for bidirectional encoders,2020, arXiv preprintarXiv:2005
 Content and cluster analysis: Assessing rePresentationalsimilarity in neural systems,2000, Philosophical Psychology
 Assessing the ability of lstms to learn syntax-sensitive dePendencies,2016, Transactions of the Association for Computational Linguistics
 ImProving multi-task deeP neuralnetworks via knowledge distillation for natural language understanding,2019, ArXiv
 The need for biases in learning generalizations,1980, Technical report
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Mnist-c: A robustness benchmark for computer vision,2019, arXiv preprintarXiv:1906
 Towards understanding knowledge distillation,2019, volume 97 ofProceedings ofMachine Learning Research
 Language models are unsupervised multitask learners,2019, OpenAI Blog
 Data-free parameter pruning for deep neu-ral networks,2015,	In Proceedings of the 26th British Machine Vision Conference
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Multilingual neural machinetranslation with knowledge distillation,2019, arXiv preprint arXiv:1902
 Distilling task-specific knowledge from bert into simple neural networks,2019, ArXiv
 The importance of being recurrent for modelinghierarchical structure,2018, In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Similarity-preserving knowledge distillation,2019, ArXiv
 Attention is all you need,2017, In Advances in neural informationprocessing systems 30
 No free lunch theorems for optimization,1089, IEEE Transactions onEvolutionary Computation
 Revisit knowledge distillation: ateacher-free framework,2019, arXiv preprint arXiv:1909
