title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 High-dimensional dynamics of gener-alization error in neural networks,2020, Neural Networks
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Particular formulae for the moore-penrose inverseof a columnwise partitioned matrix,2007, Linear algebra and its applications
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine learningand the bias-variance trade-off,2018, stat
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In Advances in Neural Information Processing Systems
 A finite sample analysis of the double descent phe-nomenon for ridge function estimation,2020, arXiv preprint arXiv:2007
 Deep neural tangent kernel and laplace kernel have the same rkhs,2020, arXivpreprint arXiv:2009
 Subspace fitting meets regression:The effects of supervision and orthonormality constraints on double descent of generalizationerrors,2020, In ICML
 Model selection for regularized least-squares algorithm in learning theory,2005, Foundations of Computational Mathematics
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Linearized two-layersneural networks in high dimension,2019, arXiv preprint arXiv:1904
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Precise tradeoffs in adversarial trainingfor linear regression,2020, In Conference on Learning Theory
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Just interpolate: Kernel “ridgeles” regression can general-ize,2019, Annals of Statistics
 Minimizers of the empirical risk and risk mono-tonicity,2019, In Advances in Neural Information Processing Systems
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Deepdouble descent: Where bigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Optimal regularization canmitigate double descent,2020, arXiv preprint arXiv:2003
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2015, In ICLR (Workshop)
 Random features for large-scale kernel machines,2008, In Advances inneural information processing Systems
 Consistency of interpolation with laplace kernels is a high-dimensional phenomenon,2019, In Conference on Learning Theory
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Benign overfitting in ridge regression,2020, arXiv preprintarXiv:2009
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, In Advances in Neural Information ProcessingSystems
 On the number of variables to use in principal component regression,2019, InAdvances in Neural Information Processing Systems
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
 The double descent occurs when the model com-plexity reaches and increases beyond the interpolation threshold,2020, Most previous works focused onproving an upper bound or optimal rate for the risk
