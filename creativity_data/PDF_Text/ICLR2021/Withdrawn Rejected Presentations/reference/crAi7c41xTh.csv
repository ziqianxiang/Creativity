title,year,conference
 Dropout: Explicit forms and capacity control,2020, arXivpreprint arXiv:2003
 Implicit regularization in deep matrix factorization,2019, InAdvances in Neural Information Processing Systems
 Implicit regularization for deep neural networksdriven by an ornstein-uhlenbeck like process,2019, arXiv preprint arXiv:1904
 Stochastic gradient and langevin processes,2019, arXivpreprint arXiv:1907
 Nonconvex optimization meets low-rank matrix factorization: Anoverview,2019, IEEE Transactions on Signal Processing
 A note on lazy training in supervised differentiable programming,2018, arXivpreprint arXiv:1812
 Algorithmic regularization in learning deep homogeneous models:Layers are automatically balanced,2018, In Advances in Neural Information Processing Systems
 Gradient descent finds global minima of deepneural networks,2018, arXiv preprint arXiv:1811
 Escaping from saddle pointsonline stochastic gradient fortensor decomposition,2015, In Conference on Learning Theory
 Matrix completion has no spurious local minimum,2016, In Advances inNeural Information Processing Systems
 Limitations of lazy training of two-layersneural network,2019, In Advances in Neural Information Processing Systems
 The implicit bias of depth: How incremental learningdrives generalization,2019, arXiv preprint arXiv:1909
 Implicit regular-ization in matrix factorization,2017, In Advances in Neural Information Processing Systems
 Characterizing implicit bias in terms of optimizationgeometry,2018, arXiv preprint arXiv:1802
 Implicit bias of gradient descent on linearconvolutional networks,2018, In Advances in Neural Information Processing Systems
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, arXiv preprint arXiv:1502
 Neural tangent kernel: Convergence and generalization inneural networks,2018, In Advances in neural information processing systems
 Gradient descent aligns the layers of deep linear networks,2018, arXiv preprintarXiv:1810
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 On large-batch trainingfor deep learning: Generalization gap and sharp minima,2016, arXiv preprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Efficient backprop,2012, In Neural networks: Tricksof the trade
 On generalization error bounds of noisy gradient methods for non-convexlearning,2019, arXiv preprint arXiv:1902
 Learning overparameterized neural networks via stochastic gradient descent onstructured data,2018, In Advances in Neural Information Processing Systems
 Algorithmic regularization in over-parameterized matrix sensing andneural networks with quadratic activations,2017, arXiv preprint arXiv:1712
 Towards explaining the regularization effect of initial large learningrate in training neural networks,2019, In Advances in Neural Information Processing Systems
 Gradient descent maximizes the margin of homogeneous neural networks,2019, arXivpreprint arXiv:1906
 Implicit self-regularization in deep neural networks: Evidencefrom random matrix theory and implications for learning,2018, arXiv preprint arXiv:1810
 Dynamic of stochastic gradient descent withstate-dependent noise,2020, arXiv preprint arXiv:2006
 On dropout and nuclear norm regularization,2019, arXiv preprintarXiv:1905
 On the implicit bias of dropout,2018, arXiv preprint arXiv:1806
 Generalization bounds of sgld for non-convex learning:Two theoretical viewpoints,2017, arXiv preprint arXiv:1707
 Generalization bounds of sgld for non-convex learning:TWo theoretical viewpoints,2018, In Conference on Learning Theory
 Convergence ofgradient descent on separable data,2018, arXiv preprint arXiv:1803
 Lexicographic and depth-sensitivemargins in homogeneous and non-homogeneous deep models,2019, arXiv preprint arXiv:1905
 Addinggradient noise improves learning for very deep networks,2015, arXiv preprint arXiv:1511
 Information-theoretic gen-eralization bounds for sgld via data-dependent estimates,2019, In Advances in Neural InformationProcessing Systems
 Path-sgd: Path-normalized optimization in deepneural networks,2015, In Advances in Neural Information Processing Systems
 The-ory of deep learning iii: explaining the non-overfitting puzzle,2017, arXiv preprint arXiv:1801
 Non-convex learning via stochastic gradient langevindynamics: a nonasymptotic analysis,2017, arXiv preprint arXiv:1702
 Exponential convergence of langevin distributions and theirdiscrete approximations,1996, Bernoulli
 Measuring theeffects of data parallelism on neural network training,2018, arXiv preprint arXiv:1811
 On the importance of initialization and momentumin deep learning,2013, In International conference on machine learning
 Rethinking the inception architecturefor computer vision,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Implicit regularization for optimal sparse recovery,2019, InAdvances in Neural Information Processing Systems
 Regularization matters: Generalization and optimization ofneural nets vs their induced kernel,2019, In Advances in Neural Information Processing Systems
 The implicit and explicit regularization effects of dropout,2020, arXivpreprint arXiv:2002
 How noise affects the hessian spectrum in overparameterized neuralnetworks,2019, arXiv preprint arXiv:1910
 The marginal value of adaptivegradient methods in machine learning,2017, In Advances in Neural Information Processing Systems
 Kernel and rich regimes in overparametrized models,2020, arXiv preprint arXiv:2002
 A diffusion theory for deep learning dynamics: Stochastic gradientdescent escapes from sharp minima exponentially fast,2020, arXiv preprint arXiv:2002
 Fluctuation-dissipation relations for stochastic gradient descent,2018, arXiv preprintarXiv:1810
 Understanding deep learning requiresrethinking generalization,2016, arXiv preprint arXiv:1611
 A hitting time analysis of stochastic gradient langevin dynam-ics,2017, arXiv preprint arXiv:1702
 The anisotropic noise in stochastic gradient descent: Itsbehavior of escaping from sharp minima and regularization effects,2019, 2019
 In the setting of Theorem 3,2021,1
 Lemma D,2021,3 and Lemma D
