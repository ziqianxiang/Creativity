title,year,conference
 On the robustness of interpretability methods,2018, arXivpreprint arXiv:1806
 Machine bias,2016, ProPublica
 Making fair ML software using trustworthyexplanation,2020, arXiv preprint arXiv:2007
 Tutorial on variational autoencoders,2016, ArXiv
 Dropout as a Bayesian Approximation: Representing ModelUncertainty in Deep Learning,2016, In Proceedings of The 33rd International Conference on MachineLearning
 Fooling neural network interpretations via adversar-ial model manipulation,2019, In Advances in Neural Information Processing Systems
 Accountable algorithms,2017, University of Pennsylvania Law Review
 The mythos of model interpretability,2016, CoRR
 A unified approach to interpreting model predictions,2017, In Advancesin Neural Information Processing Systems 30
 Generating datausing Monte Carlo dropout,2019, In 2019 IEEE 15th International Conference on Intelligent ComputerCommunication and Processing (ICCP)
 Fast learning in networks of locally-tuned processing units,1989, NeuralComputation
 ”Why should I trust you?”: Explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 The intuitive appeal of explainable machines,2018, Fordham LawReview
 A value for n-person games,1988, In Alvin E
 Explaining prediction models and individual predictions withfeature contributions,2013, Knowledge and Information Systems
 The COMPAS dataset is described in Section 4,2021,1
