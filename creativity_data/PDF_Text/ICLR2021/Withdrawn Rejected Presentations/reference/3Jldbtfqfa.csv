title,year,conference
 Distilling neural networks for greener and faster de-pendency parsing,2020, In Proceedings of the 16th International Conference on Parsing Technologiesand the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies
 Do deep nets really need to be deep? In Z,2014, Ghahramani
 Un-supervised cross-lingual representation learning at scale,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
 Knowledge distillationfor sequence model,2018, In Proc
 Sequence-level knowledge distillation,2016, In Proceedings of the2016 Conference on Empirical Methods in Natural Language Processing
 GCDT: Aglobal context enhanced deep transition architecture for sequence labeling,2019, In Proceedings ofthe 57th Annual Meeting of the Association for Computational Linguistics
 Stack-pointer networks for dependency parsing,2018, In Proceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers)
 XtremeDistil: Multi-stage distillation formassive multilingual models,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Cross-lingual name tagging and linking for 282 languages,2017, In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers)
 Viable dependency parsing as Se-quence labeling,2019, In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies
 Multilingual neural machine translation withknowledge distillation,2019, In International Conference on Learning Representations
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Structure-levelknowledge distillation for multilingual sequence labeling,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
