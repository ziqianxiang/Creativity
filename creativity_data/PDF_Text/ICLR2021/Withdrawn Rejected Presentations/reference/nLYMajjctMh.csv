title,year,conference
 Practical secure aggregation for privacy-preserving machine learning,2017, In Proceedings of the 2017 ACM SIGSAC Conference on Computerand Communications Security
 LibSVM: A library for support vector machines,2011, ACMTransactions on Intelligent Systems and Technology (TIST)
 Variational federated multi-task learning,2019, arXiv preprintarXiv:1906
 Large scale distributed deep networks,2012, In Advancesin Neural Information Processing Systems
 SAGA: a fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in Neural InformationProcessing Systems
 Semi-cyclic stochastic gradientdescent,2019, In International Conference on Machine Learning
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Optimal mini-batch and step sizes forSAGA,2019, In International Conference on Machine Learning
 Optimal decentralized distributedalgorithms for stochastic convex optimization,2019, arXiv preprint arXiv:1911
 Stochastic quasi-gradient methods: vari-ance reduction via Jacobian sketching,2018, arXiv:1805
 Variance re-duced stochastic gradient descent with neighbors,2015, In Advances in Neural Information ProcessingSystems
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems 26
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 SCAFFOLD: stochastic controlled averaging for on-device federatedlearning,2019, arXiv preprint arXiv:1910
 First analysis of local GD on hetero-geneous data,2019, In NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality
 Tighter theory for local SGD onidentical and heterogeneous data,2020, In The 23rd International Conference on Artificial Intelligenceand Statistics (AISTATS 2020)
 Adaptive gradient-based meta-learning methods,2019, InAdvances in Neural Information Processing Systems 32
 Federated optimiza-tion: distributed machine learning for on-device intelligence,2016, arXiv:1610
 Don't jump through hoops and removethose loops: SVRG and Katyusha are better without the outer loop,2020, In Proceedings of the 31stInternational Conference on Algorithmic Learning Theory
 Communication-efficient algorithms for decentralizedand stochastic optimization,2018, Mathematical Programming
 Federated learningof deep networks using model averaging,2016, arXiv preprint arXiv:1602
 A stochastic decoupling method for minimizing thesum of smooth and non-smooth functions,2019, arXiv preprint arXiv:1905
 Private federated learning with domainadaptation,2019, arXiv preprint arXiv:1912
 L-SVRG and L-Katyusha with arbitrary sampling,2019, arXivpreprint arXiv:1906
 SAGA with arbitrary sampling,2019, In The 36th InternationalConference on Machine Learning
 Coordinate descent with arbitrary sampling II: Expected sep-arable overapproximation,2016, Optimization Methods and Software
 AIDE: fastand communication efficient distributed optimization,2016, arXiv:1608
 Local SGD converges fast and communicates little,2020, In International Conferenceon Learning Representations
 Distributed stochastic multi-tasklearning with graph regularization,2018, arXiv preprint arXiv:1802
 Minibatch vs local sgd for heteroge-neous distributed learning,2020, arXiv preprint arXiv:2006
 Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks,2019, arXiv preprintarXiv:1912
 Deep learning with elastic averaging sgd,2015, InAdvances in neural information processing systems
 A convex formulation for learning task relationships in multi-tasklearning,2010, Uncertainty in Artificial Intelligence
 Stochastic optimization with importance sampling for regularizedloss minimization,2015, In Proceedings of the 32nd International Conference on Machine Learning
2 from Lemma D,2019,2 by applying Theorem 3
