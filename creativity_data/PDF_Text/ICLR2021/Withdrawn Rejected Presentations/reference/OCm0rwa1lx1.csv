title,year,conference
 Adaptive input representations for neural language modeling,2019, InICLR
 The best of both worlds: Combiningrecent advances in neural machine translation,2018, arXiv preprint arXiv:1804
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Gated feedback recurrentneural networks,2015, In International conference on machine learning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Pre-trained language model representations forlanguage generation,2019, arXiv preprint arXiv:1903
 Controllable abstractive summarization,2017, arXivpreprint arXiv:1711
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Efficient SoftmaX approximationfor gpus,2017, In ICML
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Theoretical limitations of self-attention in neural sequence models,2020, Transactions ofthe Association for Computational Linguistics
 Modelingrecurrence for transformer,2019, arXiv preprint arXiv:1904
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Teaching machines to read and comprehend,2015, In Proc
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, In Advances in neural information processing systems
 Neural gpus learn algorithms,2015, arXivpreprint arXiv:1511
 Generalizationthrough memorization: Nearest neighbor language models,2019, arXiv preprint arXiv:1911
 Dynamic evaluation of trans-former language models,2019, arXiv preprint arXiv:1904
 Simple recurrent units for highlyparallelizable recurrence,2017, arXiv preprint arXiv:1709
 Layer normalization,2016, arXiv preprintarXiv:1607
 Rouge: A package for automatic evaluation of summaries,2004, In Text summarizationbranches out
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 Spatial localization does not require the presence of local cues,1981, Learning andmotivation
 Stabilizing transformers for reinforcement learning,2019, ArXiv
 On the turing completeness of modern neuralnetwork architectures,2019, arXiv preprint arXiv:1901
 Com-pressive transformers for long-range sequence modelling,2020, In International Conference on LearningRepresentations
 Efficient content-based sparseattention with routing transformers,2020, arXiv preprint arXiv:2003
 Get to the point: Summarization withpointer-generator networks,2017, arXiv preprint arXiv:1704
 Neural machine translation of rare words withsubword units,2016, In ACL (1)
 Dense information flow for neuralmachine translation,2018, In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Adaptive attentionspan in transformers,2019, In ACL
 The importance of being recurrent for modelinghierarchical structure,2018, arXiv preprint arXiv:1803
 Attention is all you need,2017, In NIPS
 R-transformer: Recurrent neural networkenhanced transformer,2019, arXiv preprint arXiv:1907
