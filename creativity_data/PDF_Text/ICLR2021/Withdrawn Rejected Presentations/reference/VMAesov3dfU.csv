title,year,conference
 Neural module networks,2016, In IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Convergence analysis of a momentum algorithm with adaptivestep size for non convex optimization,2019, arXiv preprint arXiv:1911
 The consciousness prior,2017, arXiv preprint arXiv:1709
 Optimization methods for large-scale machinelearning,2018, SIAMReview
 The Architecture of Cognition: Rethinking Fodor and Pylyshyn’sSystematicity Challenge,2014, MIT Press
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Syntactic structures,1957, Walter de Gruyter
 Convergence guarantees for rmsprop and adam innon-convex optimization and an empirical comparison to nesterov acceleration,2018, arXiv preprintarXiv:1807
 Adaptive subgradient methods for online learning andstochastic optimization,2011, JMLR
 The compositionality papers,2002, Oxford University Press
 Recurrent independent mechanisms,2019, arXiv PreprintarXiV:1909:10893v2
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Deep residual learning for image recognition,2016, In CVPR
 Tracking the worldstate with recurrent entity networks,2016, arXiv preprint arXiv:1612
 β-vae: Learning basic visual concepts with a con-strained variational framework,2017, In International Conference on Learning Representations (ICLR)
 Measuring compositional generalization: A comprehensivemethod on realistic data,2020, In International Conference on Learning Representations
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Modular networks: Learning to decompose neuralcomputation,2018, In Advances in Neural Information Processing Systems
 Imagenet classification with deep convolutional neuralnetworks,2012, In NIPS
 Generalization without systematicity: On the compositional skillsof sequence-to-sequence recurrent networks,2018, In International Conference on Machine Learning
 Compositional generalization through meta sequence-to-sequence learning,2019, InAdvances in Neural Information Processing Systems
 Buildingmachines that learn and think like people,2017, Behavioral and Brain Sciences
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep learning,2015, nature
 Compositional continuallanguage learning,2020, In International Conference on Learning Representations
 Challenging common assumptions in the unsupervised learning of dis-entangled representations,2019, In International Conference on Machine Learning
 On the convergence of the lms algorithm with adaptive learning rate for linearfeedforward networks,1991, Neural Computation
 Rethinking eliminative connectionism,1998, Cognitive psychology
 The algebraic mind: Integrating connectionism and cognitive science,2003, MIT press
 Society of mind,1986, Simon and Schuster
 Universal grammar,1970, Theoria
 On the convergence of adam and beyond,2018, In ICLR
 Routing networks and thechallenges of modular and compositional computation,2019, arXiv preprint arXiv:1904
 Learning representations byback-propagating errors,1986, nature
 Compositional generalization in a deep seq2seqmodel by separating syntax and semantics,2019, arXiv preprint arXiv:1904
 Relational recurrentneural networks,2018, arXiv preprint arXiv:1806
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Deep playground,2016, online demo
 Teaching pre-trainedmodels to systematically reason over implicit knowledge,2020, arXiv preprint arXiv:2006
 Automatic Speech Recognition,2012, Springer
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 On the convergence of adaptivegradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, arXiv preprint arXiv:1808
 A subgradientcient condition forconvergences of adam and rmsprop,2018, arXiv preprint arXiv:1811
