title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 The committee machine: Computational to statistical gaps in learning a two-layersneural network,2018, In S
 Understanding batch normal-ization,2018, In Advances in Neural Information Processing Systems
 Separating the effects of batch normalization on cnntraining speed and stability Using classical adaptive filter theory,2020, arXiv preprint arXiv:2002
 Learning mixtUres of gaUssians,1999, In 40th Annual Symposium on Foundations ofComputer Science (Cat
 Gradient descent finds globalminima of deep neUral networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neUral networks,2018, In International Conference on Learning Representations
 Statistical Mechanics of Learning,0521, CambridgeUniversity Press
 Unsupervised learning of finite mixture models,2002, IEEETransactions on pattern analysis and machine intelligence
 Guaranteed recovery of one-hidden-layer neural networksvia cross entropy,2020, IEEE Transactions on Signal Processing
 Learning one-hidden-layer neural networks with land-scape design,2018, In International Conference on Learning Representations
 Modelling the influence ofdata strUctUre on learning in neUral networks: the hidden manifold model,2019, arXiv preprint arXiv:1909
 Speech recognition with deep recur-rent neural networks,2013, In 2013 IEEE international conference on acoustics
 Learning mixtures of spherical gaussians: moment methods andspectral decompositions,2013, In Proceedings of the 4th conference on Innovations in TheoreticalComputer Science
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Data clustering: 50 years beyond k-means,2010, Pattern recognition letters
 Score function features for discriminativelearning: Matrix and tensor framework,2014, arXiv preprint arXiv:1412
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Imagenet classification with deep convo-lutional neural networks,2012, In Proc
 Tensor factorization via matrix factoriza-tion,2015, In Artificial Intelligence and Statistics
 Efficient backprop,1998, InNeural Networks: Tricks of the Trade
 Deep learning,2015, Nature
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In S
 Convergence analysis of two-layer neural networks with ReLU activa-tion,2017, In Advances in Neural Information Processing Systems
 Adding one neuron can eliminate all badlocal minima,2018, In Advances in Neural Information Processing Systems
 On the computational efficiency of trainingneural networks,2014, In Advances in neural information processing systems
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Dynamicalmean-field theory for stochastic gradient descent in gaussian mixture classification,2020, Arxiv preprintArxiv: 2006
 Weight space structure and internal representations: Adirect approach to learning and generalization in multilayer neural networks,1995, Phys
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Contributions to the mathematical theory of evolution,1894, Philosophical Transactions ofthe Royal Society of London
 A study of gaussian mixture models of color andtexture features for image classification and segmentation,2006, Pattern Recognition
 Learning representations byback-propagating errors,1988, Cognitive modeling
 Generalization in a large committee machine,1992, Europhysics Letters(EPL)
 Statistical mechanics of learning in a large committee machine,1993, InAdvances in Neural Information Processing Systems
 Statistical mechanics of learn-ing from examples,1992, Phys
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 Statistical analysis of finite mixturedistributions,1985, Wiley
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Data-dependence of plateau phenomenon in learning with neu-ral network â€” statistical mechanical analysis,2019, In H
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Fast learning of graphneural networks with guaranteed generalizability: One-hidden-layer case,2020, arXiv preprintarXiv:2006
 Improved linear convergenceof training cnns with generalizability guarantees: A one-hidden-layer case,2020, IEEE Transactionson Neural Networks and Learning Systems
 Learning one-hidden-layer relunetworks via gradient descent,2019, In The 22nd International Conference on Artificial Intelligenceand Statistics
 Learning non-overlapping convolutional neuralnetworks with multiple kernels,2017, arXiv preprint arXiv:1711
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural Information Processing Systems
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
