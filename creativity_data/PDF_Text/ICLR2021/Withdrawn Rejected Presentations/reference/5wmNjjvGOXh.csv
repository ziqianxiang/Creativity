title,year,conference
 Deep rewiring: Trainingvery sparse deep networks,2018, In International Conference on Learning Representations
 Fast training and model compression of gated rnns via singularvalue decomposition,2018, In 2018 International Joint Conference on Neural Networks (IJCNN)
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Finding structure in time,1990, Cognitive science
 Rigging the lottery:Making all tickets winners,2020, In International Conference on Machine Learning
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in neural information processing systems
 Improving neural language models witha continuous cache,2017, In International Conference on Learning Representations
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Deep read: A reading comprehensionsystem,1999, In Proceedings of the 37th annual meeting of the Association for Computational Linguistics
 Long short-term memory,1997, Neural computation
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In International Conference on Learning Representations
 SPeeding uP convolutional neural networkswith low rank exPansions,2014, arXiv preprint arXiv:1405
 Pruning versus cliPPing in neural networks,1989, Physical Review A
 Recurrent continuous translation models,2013, In Proceedings of the2013 Conference on Empirical Methods in Natural Language Processing
 Efficientneural audio synthesis,2018, In International Conference on Learning Representations
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Dynamic evaluation of neuralsequence models,2018, In International Conference on Machine Learning
 Optimal brain damage,1990, In Advances in neuralinformation processing Systems
 SNIP: SINGLE-SHOT NETWORKPRUNING BASED ON CONNECTION SENSITIVITY,2019, In International Conference on LearningRepresentations
 A signal propagationperspective for pruning neural networks at initialization,2020, In International Conference on LearningRepresentations
 Dynamic sparsetraining: Find efficient sparse network from scratch with trainable masked layers,2020, In InternationalConference on Learning Representations
 Intrinsically sparse long short-term memory networks,2019, arXiv preprint arXiv:1901
 Sparse evolutionary deep learning with over one million artificial neurons oncommodity hardware,2020, Neural Computing and Applications
 Topological insights into sparse neural networks,2020, In Joint EuropeanConference on Machine Learning and Knowledge Discovery in Databases
 Bayesian compression for deep learning,2017, InAdvances in Neural Information Processing Systems
 Learning sparse neural networks throughl_0 regularization,2017, In International Conference on Learning Representations
 Transformed l1 regularization forlearning sparse deep neural networks,2019, Neural Networks
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 On the state of the art of evaluation in neural languagemodels,2018, In International Conference on Learning Representations
 Pointer sentinel mixturemodels,2017, In International Conference on Learning Representations
 Regularizing and optimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 Recurrentneural network based language model,2010, In Eleventh annual conference of the international speechcommunication association
 A topological insight into restricted boltzmann machines,1573, Machine Learning
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature Communications
 Sparsemaps: convolutional networks with sparsefeature maps for tiny image classification,2019, Expert Systems with Applications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In International Conference on Machine Learning
 Exploring sparsity in recurrentneural networks,2017, In International Conference on Learning Representations
 Deep expander networks: Efficient deepnetworks from graph theory,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural networks,2014, ICLR
 Ordered neurons: Integrating treestructures into recurrent neural networks,2019, In International Conference on Learning Representations
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Learning intrinsic sparse structures within long short-term memory,2018, InInternational Conference on Learning Representations
 Feed-forward neural network training using sparse representation,2019, ExpertSystems with Applications
 Breaking the softmax bottle-neck: A high-rank RNN language model,2018, In International Conference on Learning Representations
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
