title,year,conference
 Gradient `1 regularization for quantization robustness,2020, In International Conference onLearning Representations
 Gradient descent on neural networks typically occurs at the edge of stability,2021, InSubmitted to International Conference on Learning Representations
 Coherent gradients: An approach to understanding generalization in gradientdescent-based optimization,2020, In International Conference on Learning Representations
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Sharp Minima Can Generalize ForDeep Nets,2017, In Proceedings of the 34th International Conference on Machine Learning
 Deep Residual Learning for ImageRecognition,2015, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Flat minima,1997, Neural Computation
 Densely connected convolutionalnetworks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In S
 Three Factors Influencing Minima in SGD,2017, 2017
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations
 Beyond synthetic noise: Deep learning oncontrolled noisy labels,2020, In ICML
 FantasticGeneralization MeasUres and Where to Find Them,2020, In International Conference on LearningRepresentations
 On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,2017, In 5thInternational Conference on Learning Representations
 Learning mUltiple layers of featUres from tiny images,2009, Technical report
 Tiny imagenet visUal recognition challenge,2015, 2015
 Dividemix: Learning With noisy labels as semi-sUpervised learning,2020, In International Conference on Learning Representations
 Implicit regUlarization in deep learning,2017, 2017
 Contractive aUto-encoders: Explicit invariance dUring featUre extraction,2011, In ICML
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference (BMVC)
 Understand-ing deep learning requires rethinking generalization,2016, In International Conference on LearningRepresentations
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Residual learning without normalization via betterinitialization,2019, In International Conference on Learning Representations
