title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Implicit regularization in deep learning: A view from functionspace,2020, arXiv preprint arXiv:2008
 Practical gauss-newton optimisation for deeplearning,2017, In International Conference on Machine Learning
 Backpack: Packing more into backprop,2019, InInternational Conference on Learning Representations
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Fast ap-proximate natural gradient descent in a kronecker factored eigenbasis,2018, In Advances in NeuralInformation Processing Systems
 Efficient per-example gradient computations,2015, arXiv preprint arXiv:1510
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, In International Conference on Machine Learning
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 Efficient per-example gradient computationsin convolutional neural networks,2019, arXiv preprint arXiv:1912
 Topmoumoute online natural gra-dient algorithm,2008, In Advances in neural information processing systems
 Woodfisher: Efficient second-order approximations for modelcompression,2020, In Advances in Neural Information Processing Systems
 Scalable trust-regionmethod for deep reinforcement learning using kronecker-factored approximation,2017, In Advances inneural information processing systems
