title,year,conference
 Towards better understandingof gradient-based attribution methods for deep neural networks,2018, In International Conferenceon Learning Representations
 On identifiability in transformers,2020, In International Conference on LearningRepresentations
 What does bert lookat? an analysis of bert’s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Real time image saliency for black box classifiers,2017, In NIPS
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter ofthe Association for Computational Linguistics: Human LanguageTechnologies
 When bert forgets hoW to pos:Amnesic probing of linguistic properties and mlm predictions,2020, arXiv preprint arXiv:2006
 Assessing bert’s syntactic abilities,2019, arXiv preprint arXiv:1901
 A structural probe for finding syntax in Wordrepresentations,2019, In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies
 Attention is not explanation,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Analysing neural language models:Contextual decomposition reveals default reasoning in number and gender assignment,2019, InProceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)
 The emergence of number and syntax units in lstm language models,2019, InProceedings of the 2019 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Influence-directedexplanations for deep convolutional networks,2018, In 2018 IEEE International Test Conference (ITC)
 Open sesame: Getting inside bert’s linguisticknowledge,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and InterpretingNeural Networks for NLP
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, Transactions of the Association for Computational Linguistics
 Linguisticknowledge and transferability of contextual representations,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Influence paths forcharacterizing subject-verb number agreement in LSTM language models,2020, Association forComputational Linguistics
 Targeted syntactic evaluation of language models,2018, In Proceedingsof the 2018 Conference on Empirical Methods in Natural Language Processing
 Visualizing and measuring the geometry of bert,2019, In H
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 What do you learn from context?probing for sentence structure in contextualized word representations,2019, In 7th InternationalConference on Learning Representations
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Proceedings of the31st International Conference on Neural Information Processing Systems
 Analyzing the structure of attention in a transformer languagemodel,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and InterpretingNeural Networks for NLP
