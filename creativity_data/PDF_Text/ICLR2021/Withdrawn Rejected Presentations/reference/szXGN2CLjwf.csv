title,year,conference
 Finding approxi-mate local minima faster than gradient descent,2017, In Proceedings of the 49th Annual ACM SIGACTSymposium on Theory of Computing
 Variance reduction for faster non-convex optimization,2016, InInternational conference on machine learning
 Closing the gener-alization gap of adaptive gradient methods in training deep neural networks,2018, arXiv preprintarXiv:1806
 On the convergence ofa class of Adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Momentum improves normalized SGD,2020, arXiv preprintarXiv:2002
 Spider: Near-optimal non-convex op-timization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 Sharp analysis for nonconvex SGD escaping fromsaddle points,2019, arXiv preprint arXiv:1902
 Long short-term memory,1997, Neural computation
 How to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Non-convex finite-sum optimization viaSCSG methods,2017, In Advances in Neural Information Processing Systems
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Adaptive bound optimization for online convex opti-mization,2010, arXiv preprint arXiv:1002
 Stochastic variancereduction for nonconvex optimization,2016, In International conference on machine learning
 On the convergence of Adam and beyond,2019, arXivpreprint arXiv:1904
 English conversational telephone speech recognition by humans and machines,2017, InInterspeech
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Stochastic compositional gradient descent: algorithmsfor minimizing compositions of expected-value functions,2017, Mathematical Programming
 AdaGrad stepsizes: Sharp convergence over noncon-vex landscapes,2019, In International Conference on Machine Learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Large batch optimization for deeplearning: Training BERT in 76 minutes,2019, arXiv preprint arXiv:1904
 On complexity of finding stationarypoints of nonsmooth nonconvex functions,2020, arXiv preprint arXiv:2002
 Distributed deep learning strategies for automatic speech recognition,2019, In ICASSPâ€™2019
 Stochastic nested variance reduction for nonconvexoptimization,2018, In Advances in Neural Information Processing Systems
