title,year,conference
 A learning algorithm for Boltz-mann machines,1985, Cognitive Science
 Greedy layerWise learning can scaleto ImageNet,2019, In 36th International Conference on Machine Learning
 Decoupled greedy learning ofCNNs,2019, arXiv preprint arXiv:1901
 Demystifying parallel and distributed deep learning: An in-depthconcurrency analysis,2018, arXiv preprint arXiv:1802
 On the optimization of a synap-tic learning rule,1992, In Preprints Conf
 Learning a Synaptic Learning Rule,1990, Universityof Montreal
 Dota 2 With largescale deep reinforcement learning,2019, arXiv preprint arXiv:1912
 Language models are feW-shot learners,2020, arXiv preprintarXiv:2005
 Revisiting dis-tributed synchronous SGD,2016, arXiv preprint arXiv:1604
 Mxnet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, arXiv preprint arXiv:1512
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Bidirectional parallelfiber plasticity in the cerebellum under climbing fiber control,2004, Neuron
 Torch7: A matlab-like environmentfor machine learning,2011, In BigLearn
 Distributed deep learning using syn-chronous stochastic gradient descent,2016, arXiv preprint arXiv:1602
 Large scaledistributed deep networks,2012, In Advances in Neural Information Processing Systems 25 (NIPS2012)
 Large scale distributed deep networks,2012, InAdvances in neural information processing systems
 ImageNet: A large-scale hierarchicalimage database,2009, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)
 Local unsupervised learning for imageanalysis,2019, arXiv preprint arXiv:1908
 Meta-learning biologically plausible semi-supervised update rules,2019, bioRxiv
 PipeDream: Fast and efficient pipeline parallel DNN training,2018, arXivpreprint arXiv:1806
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)
 Data-efficient image recognition with contrastive predictive coding,2019, arXivpreprint arXiv:1905
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Neural networks and physical systems with emergent collective computationalabilities,1982, Proceedings ofthe National Academy ofSciences
 GPipe: Easy scalingwith micro-batch pipeline parallelism,2018, arXiv preprint arXiv:1811
 Training neural networks using features replay,2018, InAdvances in Neural Information Processing Systems 31 (NeurIPS 2018)
 Decoupled parallel backpropagation withconvergence guarantee,2018, arXiv preprint arXiv:1804
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Relating STDP to BCM,2003, Neural Computation
 Decoupled neural interfaces using synthetic gradients,2017, In 34thInternational Conference on Machine Learning
 Caffe: Convolutional architecture for fast feature embed-ding,2014, arXiv preprint arXiv:1408
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 ImageNet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems 25 (NIPS 2012)
 Unsupervised learning by competing hidden units,2019, Proceedingsof the National Academy of Sciences
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, arXiv preprint arXiv:2006
 Putting an end to end-to-end: Gradient-isolatedlearning of representations,2019, In Advances in Neural Information Processing Systems 32 (NeurIPS2019)
 Revisiting small batch training for deep neural networks,2018, arXivpreprint arXiv:1804
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Meta-learning up-date rules for unsupervised representation learning,2018, In International Conference on LearningRepresentations
 Using a thousand optimization tasks to learn hyperparameter search strategies,2020, arXivpreprint arXiv:2002
 Continual learning of recurrentneural networks by locally aligning distributed representations,2020, IEEE Transactions on NeuralNetworks and Learning Systems
 Performance analysis of a pipelined back-propagation parallel algorithm,1993, IEEE Transactions on Neural Networks
 Languagemodels are unsupervised multitask learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Learning internal representationsby error propagation,1985, Technical report
 Bio-inspired hashingfor unsupervised similarity search,2020, arXiv preprint arXiv:2001
 Optimal unsupervised learning in a single-layer linear feedforward neural net-work,1989, Neural Networks
 Gradient estimation usingstochastic computation graphs,2015, In Advances in Neural Information Processing Systems
 Horovod: fast and easy distributed deep learning in Ten-sorFlow,2018, arXiv preprint arXiv:1802
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Mastering the game of gowithout human knowledge,2017, nature
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Alphastar: Mas-tering the real-time strategy game starcraft ii,2019, DeepMind blog
 An efficient implementationof the back-propagation algorithm on the connection machine CM-2,1989, In Advances in NeuralInformation Processing Systems 2 (NIPS 1989)
1 but using a different number of hidden units,1024, Inaddition to 4096 units
