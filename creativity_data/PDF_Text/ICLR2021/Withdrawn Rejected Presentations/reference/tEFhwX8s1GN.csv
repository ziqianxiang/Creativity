title,year,conference
 Finding approxi-mate local minima faster than gradient descent,2017, In Proceedings of the 49th Annual ACM SIGACTSymposium on Theory of Computing
 How to make the gradients small stochastically: Even faster convex and non-convex sgd,2018, In Advances in Neural Information Processing Systems
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 A downsampled variant of imagenet as analternative to the cifar datasets,2017, arXiv preprint arXiv:1707
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in neural information processing systems
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, Cited on
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Making the last iterate of sgd informationtheoretically optimal,2019, volume 99 of Proceedings of Machine Learning Research
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning,2020, arXiv preprintarXiv:2003
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Convergence of a stochastic gradient method with momentumfor nonsmooth nonconvex optimization,2020, arXiv preprint arXiv:2002
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Regularizing and optimizing lstm lan-guage models,2017, arXiv preprint arXiv:1708
 Revisiting activation regularization for lan-guage rnns,2017, arXiv preprint arXiv:1708
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint arXiv:1308
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2018, arXiv preprint arXiv:1810
 Speech commands: A dataset for limited-vocabulary speech recognition,2018, arXivpreprint arXiv:1804
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
