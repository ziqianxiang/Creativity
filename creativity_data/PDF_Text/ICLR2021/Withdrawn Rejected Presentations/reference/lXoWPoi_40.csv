title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Shaping the learning landscape in neuralnetworks around wide flat minima,2019, CoRR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Multi-branch attentive transformer,2020, arXiv preprint arXiv:2006
 Towards understanding generalization in gradient-based meta-learning,2019, arXiv preprint arXiv:1907
 Flat minima,1997, Neural Computation
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Limitations of the empirical fisher approxi-mation,2019, arXiv preprint arXiv:1905
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Information geometry of orthogonal initializations and train-ing,2018, arXiv preprint arXiv:1810
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Identifying generalizationproperties in neural networks,2018, arXiv preprint arXiv:1809
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, In International Conference on Learning Representations
