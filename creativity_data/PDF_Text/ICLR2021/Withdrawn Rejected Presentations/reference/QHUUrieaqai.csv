title,year,conference
 HOList: AnEnvironment for Machine Learning of Higher Order Logic Theorem Proving,2019, In 36th InternationalConference on Machine Learning
 Learning toReason in Large Theories without Imitation,2019, arXiv preprint arXiv:1905
 Charles Sanders Peirce: Logic,2015, In The InternetEncyclopedia of Philosophy
 Language models are few-shot learners,2020, CoRR
 Improving Graph Neural Network Representations of Logical Formulae withSubgraph Pooling,2019, arXiv preprint arXiv:1911
 BERT: pre-training ofdeep bidirectional transformers for language understanding,2019, In Jill Burstein
 Unified Language Model Pre-training for Natural Language Understandingand Generation,2019, In Advances in Neural Information Processing Systems
 Enhancing SAT solvers with glue variable predictions,2020, arXiv preprintarXiv:2007
 Long Short-Term Memory,1997, Neural computation
 GamePad: A learning environmentfor theorem proving,2019, In 7th International Conference on Learning Representations
 Spanbert:Improving pre-training by representing and predicting spans,2020, Transactions of the Associationfor Computational Linguistics
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun
 Deep learning for symbolic mathematics,2020, In 8thInternational Conference on Learning Representations
 Learning heuristics for quantifiedboolean formulas through reinforcement learning,2020, In International Conference on LearningRepresentations
 Modelling High-Level MathematicalReasoning in Mechanised Declarative Proofs,2020, arXiv preprint arXiv:2006
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Effective approaches to attention-basedneural machine translation,2015, In LlUiS Marquez
 Universal linguistic inductivebiases via meta-learning,2020, Proceedings of CogSci
 Learning Music Helps You Read: Using transfer to study lin-guistic structure in language models,2020, In Proceedings of the 2020 Conference on Empirical Methods11Under review as a conference paper at ICLR 2021in Natural Language Processing (EMNLP)
 Mathematical Reasoning viaSelf-supervised Skip-tree Training,2020, arXiv preprint arXiv:2006
 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2020, J
 Analysing mathematicalreasoning abilities of neural models,2019, In Proceedings of International Conference on LearningRepresentations (ICLR)
 Thegraph neural network model,2008, IEEE Transactions on Neural Networks
 Enhancing the transformer with explicit relational encoding for math problem solving,2019, CoRR
 Guiding High-Performance SAT solvers with Unsat-CorePredictions,2019, In International Conference on Theory and Applications of Satisfiability Testing
 MASS: masked sequence to sequencepre-training for language generation,2019, In 36th International Conference on Machine Learning
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 First Neural Conjecturing Datasets and Experiments,2020, In ChristophBenzmuller and Bruce Miller
 Learning Branching Heuristics for Propositional ModelCounting,2020, CoRR
 Attention is All you Need,2017, In Proceedings of Advances in NeuralInformation Processing Systems (NeurIPS)
 Premise selection for theorem proving bydeep graph embedding,2017, In Advances in Neural Information Processing Systems
 Exploration of neural ma-chine translation in autoformalization of mathematics in mizar,2020, Proceedings of ACM SIGPLANInternational Conference on Certified Programs and Proofs
 INT: An Inequality Benchmark for EvaluatingGeneralization in Theorem Proving,2020, arXiv preprint arXiv:2007
 Learning to Prove Theorems via Interacting with Proof Assistants,2019, InProceedings of International Conference on Machine Learning (ICML)
 PEGASUS: pre-training withextracted gap-sentences for abstractive summarization,2020, In 37th International Conference onMachine Learning
