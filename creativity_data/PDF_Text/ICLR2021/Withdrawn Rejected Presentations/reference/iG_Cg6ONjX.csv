title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2016, arXiv preprint arXiv:1611
 Approximation by superpositions ofa sigmoidal function,1989, Mathematics of control
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Universal function approximation by deep neural nets with bounded width and reluactivations,2019, Mathematics
 Complexity of linear regions in deep networks,2019, arXiv preprintarXiv:1901
 Deep relu networks have surprisingly few activation patterns,2019, InAdvances in Neural Information Processing Systems
 A framework for the construction of upper bounds on the numberof affine linear regions of relu feed-forward neural networks,2019, IEEE Transactions on InformationTheory
 Multilayer feedforward networks areuniversal approximators,1989, Neural networks
 Densely connectedconvolutional networks,2017, In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Deep learning,2015, nature
 Deep vs,2016, shallow networks: An approximation theoryperspective
 Notes on the number of linear regions of deep neural networks,2017, Sampling TheoryAppl
 On the number of response regions of deepfeed forward networks with piece-wise linear activations,2013, arXiv preprint arXiv:1312
 Empirical bounds on linear regions of deep rectifier net-works,2018, arXiv preprint arXiv:1810
 Bounding and counting linearregions of deep neural networks,2018, In International Conference on Machine Learning
 Error bounds for approximations with deep relu networks,2017, Neural Networks
21 is easy to verified,2021, For BnÎ³(k) 
7 The proof of Proposition 4Proof,2021, Consider any input region D
