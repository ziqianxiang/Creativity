title,year,conference
 Distributed delayed stochastic optimization,2011, In Advances inNeural Information Processing Systems
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 signsgd:Compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Pattern recognition and machine learning,2006, springer
 Mxnet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, arXiv preprint arXiv:1512
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 On the convergence of adamand adagrad,2020, arXiv preprint arXiv:2003
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Deep learning,2016, MIT press
 Statistical learning with sparsity: the lassoand generalizations,2015, CRC press
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Analysis of quantized models,2019, In InternationalConference on Learning Representations
 A linear speedup analysis of distributed deep learning with sparse andquantized communication,2018, In Advances in Neural Information Processing Systems
 Error feedbackfixes signsgd and other gradient compression schemes,2019, arXiv preprint arXiv:1901
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Decentralized stochastic optimizationand gossip algorithms with compressed communication,2019, arXiv preprint arXiv:1902
 Efficient mini-batch training forstochastic optimization,2014, In Proceedings of the 20th ACM SIGKDD international conference onKnowledge discovery and data mining
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Accelerating federated learning via momentumgradient descent,2020, IEEE Transactions on Parallel and Distributed Systems
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 The error-feedback frameWork: Better rates for sgdWith delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Sparsified sgd With memory,2018, InAdvances in Neural Information Processing Systems
 Reinforcement learning: An introduction,2018, MIT press
 Deepsqueeze:Parallel stochastic gradient descent With double-pass error-compensated compression,2019, arXivpreprint arXiv:1907
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing Systems
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, In International Conference on Learning Representations
 A sufficient condition for conver-gences of adam and rmsprop,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
