title,year,conference
 Safe reinforcement learning via shielding,2018, In Proceedings of AAA118
 Using verified control envelopes for safe controller design,2014, In 2014American Control Conference
 Model-checkingalgorithms for continuous-time Markov chains,2003, IEEE Transactions on software engineering
 OpenAI gym,2016, arXiv preprint arXiv:1606
 Non-markovian rewardsexpressed in LTL: guiding search via reward shaping,2017, In Proceedings of the Tenth InternationalSymposium on Combinatorial Search
 Model checking,2001, MIT Press
 Safe exploration in continuous action spaces,2018, arXiv preprint arXiv:1801
 Temporallogic monitoring rewards via transducers,2020, 2020
 Cautious reinforcementlearning with logical constraints,2020, arXiv preprint arXiv:2002
 Contracts: specifying behavioralcompositions in object-oriented systems,1990, ACM Sigplan Notices
 Quantitative analysis and model checking,1997, In Proceedings
 Shielded decision-makingin mdps,2018, arXiv preprint arXiv:1807
 Applying ‘design by contract’,1992, Computer
 Policy invariance under reward transformations:Theory and application to reward shaping,1999, In Proceedings of the Sixteenth International Conferenceon Machine Learning (ICML 1999)
 Optlayer-practical constrained opti-mization for deep reinforcement learning in the real world,2018, In 2018 IEEE International Conferenceon Robotics and Automation (ICRA)
 Benchmarking safe exploration in deep reinforcementlearning,2019, arXiv preprint arXiv:1910
 Trial without error:Towards safe reinforcement learning via human intervention,2017, arXiv preprint arXiv:1707
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Reinforcement learning: An introduction,2018, MIT press
 An inductive synthesis frameworkfor verifiable reinforcement learning,2019, In Proceedings of the 40th ACM SIGPLAN Conference onProgramming Language Design and Implementation
