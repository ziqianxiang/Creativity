title,year,conference
 A large anno-tated corpus for learning natural language inference,2015, In Empirical Methods in Natural LanguageProcessing (EMNLP)
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Efficient Softmax approxima-tion for gpus,2017, In International Conference on Machine Learning (ICML)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations (ICLR)
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Image transformer,2018, In International Conference on Machine Learning (ICML)
 Glove: Global vectors for wordrepresentation,2014, In Empirical Methods in Natural Language Processing (EMNLP)
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (ACL)
 Novel positional encodings to enable tree-based transformers,2019, InAdvances in Neural Information Processing Systems (NeurIPS)
 The evolved transformer,2019, In International Conference onMachine Learning (ICML)
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Empirical Methods in Natural Language Processing (EMNLP)
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition (CVPR)
 Why self-attention? atargeted evaluation of neural machine translation architectures,2018, In Empirical Methods in NaturalLanguage Processing (EMNLP)
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Attention is not not explanation,2019, In Empirical Methods in NaturalLanguage Processing (EMNLP)
 Huggingfaceâ€™s transformers: State-of-the-art natural language processing,2019, ArXiv
 Lite transformer with long-short rangeattention,2020, In International Conference on Learning Representations (ICLR)
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems (NeurIPS)
2 Pre-trainingAs introduced in Devlin et al,2000, (2019)
