title,year,conference
 Sparsely-connected neural networks: towardsefficient vlsi implementation of deep neural networks,2016, arXiv preprint arXiv:1611
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 An analysis of deep neural networkmodels for practical applications,2016, arXiv preprint arXiv:1605
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 Bnn+: Improvedbinary network training,2019, In NeurIPS19 Workshop on Energy Efficient Machine Learning andCognitive Computing
 Com-pressing low precision deep neural networks using sparsity-induced regularization in ternary net-works,2017, In International Conference on Neural Information Processing
 Learning sparsenetworks using targeted dropout,2019, ArXiv
 A method for the construction of minimum-redundancy codes,1952, Proceedings ofthe IRE
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 MNIST handwritten digit database,2010, 2010
 Deep learning,2015, Nature
 Bi-real net:Enhancing the performance of 1-bit cnns with improved representational capability and advancedtraining algorithm,2018, In Proceedings of the European Conference on Computer Vision
 Learning sparse neural networks throughl0 regularization,2018, In International Conference on Learning Representations (ICLR)
 An algorithm for nonlinear optimization problems with binaryvariables,2010, Computational Optimization and Applications
 Binary neuralnetworks: A survey,2020, Pattern Recognition
 Very deep convolutional networks for large-scale imagerecognition,2015, In 3rd International Conference on Learning Representations
 Deep learning for the internet of things,2018, Computer
