title,year,conference
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, arXiv preprint arXiv:1806
 Instance-sensitive fully convolutionalnetworks,2016, In European Conference on Computer Vision
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Incorporating nesterov momentum into adam,2016, 2016
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Long short-term memory,1997, Neural computation
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Nostalgic adam: Weighting more of the past gradientswhen designing the adaptive learning rate,2018, arXiv preprint arXiv:1805
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 An exponential learning rate schedule for deep learning,2019, arXivpreprint arXiv:1910
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Adaptive bound optimization for online convex opti-mization,2010, arXiv preprint arXiv:1002
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Rprop-a fast adaptive learning algorithm,1992, In Proc
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Revisiting the sibling head in object detector,2020, arXivpreprint arXiv:2003
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Adashift:Decorrelation and convergence of adaptive learning rate methods,2018, arXiv preprint arXiv:1810
