title,year,conference
 Neural learning in structured parameter spaces-natural riemannian gradient,1997, InAdvances in neural information processing systems
 Convex optimization,2004, Cambridge university press
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing systems
 Incorporatingsecond-order functional knowledge for better option pricing,2001, In Advances in neural informationprocessing systems
 Why momentum really works,2017, Distill
 Deep learning,2016, MIT press
 Speech recognition with deep recurrentneural networks,2013, In 2013 IEEE international conference on acoustics
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Long short-term memory,1997, Neural computation
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Learning multiple layers of features from tiny images,2009, Technical report
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Human-level controlthrough deep reinforcement learning,2015, Nature
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Learning representations byback-propagating errors,1986, nature
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Escaping saddle pointswith adaptive gradient methods,2019, arXiv preprint arXiv:1901
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 TenSor2tensor for neural machinetranslation,2018, arXiv preprint arXiv:1803
 Grandmaster level instarcraft ii using multi-agent reinforcement learning,2019, Nature
 The general inefficiency of batch training for gradientdescent learning,2003, Neural networks
 Adaptive methodsfor nonconvex optimization,2018, In Advances in neural information processing systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
