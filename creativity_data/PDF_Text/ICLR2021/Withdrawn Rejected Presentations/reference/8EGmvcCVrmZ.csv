title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Learning and inference in hierarchical mod-els with singularities,2003, Systems and Computers in Japan
 Resolution of Singularities and the Generalization Error withBayesian Estimation for Layered Neural Network,2005, In IEICE Trans
 Stochastic complexities of reduced rank regression in Bayesianestimation,2005, Neural Networks
 Stronger generalization bounds for deep nets viaa compression approach,2018, In 35th International Conference on Machine Learning
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Introduction to statistical learning the-ory,2003, In Summer School on Machine Learning
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In Advances in Neural Information Processing Systems
 Entropy-SGD: Biasing gradi-ent descent into wide valleys,2017, In International Conference on Learning Representations
 Gradient descent provably optimizesover-parameterized neural networks,2018, In International Conference on Learning Representations
 Flat minima,1997, Neural Computation
 The No-U-Turn sampler: adaptively setting path lengthsin hamiltonian monte carlo,2014, J
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Three factors influencing minima in SGD,2017, arXiv preprintarXiv:1711
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Rethinking parameter counting indeep models: Effective dimensionality revisited,2020, arXiv preprint arXiv:2003
 Variational Bayes Solution of Linear Neural Networksand Its Generalization Performance,2007, Neural Computation
 Towards understanding the role of over-parametrization in gen-eralization of neural networks,2019, In International Conference on Learning Representations (ICLR)
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Exploring general-ization in deep learning,2017, In Advances in neural information processing systems
 The spectrum of the Fisher information matrix of a single-hidden-layer neural network,2018, In Advances in Neural Information Processing Systems
 Theory of deep learning III: explaining the non-overfittingpuzzle,2018, CoRR
 Swish: a self-gated activation function,2017, arXivpreprint arXiv:1710
 Singularity of the Hessian in deep learning,2016, CoRR
 A Bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Information matrices and generalization,1906, arXiv:1906
 Algebraic Geometry and Statistical Learning Theory,2009, Cambridge UniversityPress
 Mathematical Theory of Bayesian Statistics,2018, CRC Press
 Energy-entropy competitionand the effectiveness of stochastic gradient descent in machine learning,2018, Molecular Physics
