title,year,conference
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Compressing deep convolutional net-works using vector quantization,2014, arXiv preprint arXiv:1412
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 A fast learning algorithm for deep beliefnets,2006, Neural Computation
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
