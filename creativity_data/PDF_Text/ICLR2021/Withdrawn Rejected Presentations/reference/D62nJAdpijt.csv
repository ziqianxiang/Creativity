title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv Preprint arXiv:1802
 How tobackdoor federated learning,2020, In Intemational Conference on ArtificiaI Inteiligence and StatiStics
 Strip:A defence against trojan attacks on deep neural networks,2019, arXiv PrePrint arXiv:1902
 Explaining and harnessing adversarialexamples,2015, Intemational ConferenCe on Learning RePreSentations
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2017, arXiv PrePrint arXiv:1708
 Model-reuse attacks on deeplearning systems,2018, In PrOCeedingS of the 2018 ACM SIGSAC Conference on COmPUter andCOmmUniCatiOnS Security
 Gradient-based learning appliedto document recognition,1998, PrOCeedingS of the IEEE
 Certified ro-bustness to adversarial examples with differential privacy,2019, In 2019 IEEE SymPOSiUm on SeCUrityand PriVaCy (SP)
 Certified adversarial robustness withadditive noise,2019, In AdVanCeS in NeUrai InfOrmatiOn PrOCeSSing Systems
 Trojaning attack on neural networks,2017, 2017
 A tale of evil twins: Adversarial inputs versus poisoned models,2020, In Proceedingsof ACM SAC COnference on COmPUter and COmmUniCatiOnS (CCS)
 Scalable differentialprivacy with certified robustness in adversarial learning,2020, In 37th International COnferenCe onMaChine Learning
 Backdooring and poisoning neural networks with image-scalingattacks,2020, arXiv PrePrint arXiv:2003
 Intriguing properties of neural networks,2014, International Conference on LearningRepresentations
 Ensemble adversarial training: Attacks and defenses,2017, arXiv Preprint arXiv:1705
 Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,2019, NeUralCleanse: Identifying and Mitigating Backdoor AttackS in NeUral Networks
 Dba: Distributed backdoor attacks against feder-ated learning,2019, In International Conference on Learning RePreSentations
