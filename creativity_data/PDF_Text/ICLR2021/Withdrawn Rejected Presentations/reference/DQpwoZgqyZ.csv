title,year,conference
 The capacity of feedforward neural networks,2019, Neural networks
 The description length of deep learning models,2018, In Samy Bengio
 Emnist: Extending mnistto handwritten letters,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 Evolutionary data measures: Understandingthe difficulty of text classification tasks,2018, In Anna Korhonen and Ivan Titov (eds
 Capacity and trainability in recurrentneural networks,2017, In 5th International Conference on Learning Representations
 A kernel method for the two-sample-problem,2006, In Bernhard Scholkopf
 In search of lost domain generalization,2020, arXiv preprintarXiv:2007
 Identity mappings in deep resid-ual networks,2016, In Bastian Leibe
 Complexity measures of supervised classification problems,2002, IEEETrans
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 An information-theoretic definition of similarity,1998, In Jude W
 Troubling trends in machine learning scholarship,2019, ACMQueue
 Ablation studiesin artificial neural networks,2019, arXiv preprint arXiv:1901
 Readingdigits in natural images with unsupervised feature learning,2011, NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Domain adaptation via transfercomponent analysis,2009, In Craig Boutilier (ed
 Adapting visual category models to newdomains,2010, In European conference on computer vision
 Deephashing network for unsupervised domain adaptation,2017, In (IEEE) Conference on Computer Visionand Pattern Recognition (CVPR)
 Information-theoretic probing with minimum description length,2020, arXivpreprint arXiv:2003
 Learning and evalu-ating general linguistic intelligence,2019, arXiv preprint arXiv:1901
 Measuring information transfer in neural net-works,2020, arXiv preprint arXiv:2009
