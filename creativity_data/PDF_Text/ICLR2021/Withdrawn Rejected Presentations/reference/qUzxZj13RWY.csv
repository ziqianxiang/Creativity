title,year,conference
 On-line learning rate adaptation with hypergradient descent,2018, In International Conference on Learning9Under review as a conference paper at ICLR 2021Representations
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks ofthe trade
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Stochastic gradient descent tricks,2012, In Neural networks: Tricks ofthe trade
 Incorporating nesterov momentum into adam,2016, 2016
 Deep learning,2016, MIT press
 Matrix analysis,2012, Cambridge university press
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Tensor decompositions and applications,2009, SIAM review
 Deep learning,2015, nature
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, In Advances in Neural Information Processing Systems
 Sgdr: Stochastic gradient descent with warm restarts,2017, 2017
 Adaptive gradient methods with dynamic bound oflearning rate,2019, In International Conference on Learning Representations
 Training deep networks without learning rates throughcoin betting,2017, In Advances in Neural Information Processing Systems
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Tensor decomposition for signal processing and machine learning,2017, IEEETransactions on Signal Processing
 Very deep convolutional networks for large-scale image recogni-tion,2015, In International Conference on Learning Representations
 Convolutional neural networkswith low-rank regularization,2016, jan 2016
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionand pattern recognition
 On compressing deep models by loWrank and sparse decomposition,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
