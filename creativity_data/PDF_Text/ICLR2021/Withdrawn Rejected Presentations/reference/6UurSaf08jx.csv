title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Adaptive input representations for neural language modeling,2019, InInternational Conference on Learning Representations
 The best of bothworlds: Combining recent advances in neural machine translation,2018, In Proceedings of the 56thAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 Exploiting deep repre-sentations for neural machine translation,2018, Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing
 Pre-trained language model representations forlanguage generation,2019, In Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Reducing transformer depth on demand withstructured dropout,2020, In International Conference on Learning Representations
 Compressing Large-Scale Transformer-Based Mod-els: A Case Study on BERT,2020, arXiv:2002
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations
 SNIP: Single-shot Network PruningBased on Connection Sensitivity,2018, In International Conference on Learning Representations
 ROUGE: A package for automatic evaluation of summaries,1013, In Text SummarizationBranches Out
 Deep contextualized word representations,2018, In Proc
 Fully Quantized Transformer for ImprovedTranslation,2019, ArXiv
 Get to the point: Summarization withpointer-generator networks,2017, Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 The evolved transformer,2019, arXiv preprintarXiv:1901
