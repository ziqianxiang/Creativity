title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv Preprint arXiv:1802
 Decision-based adversarial attacks: Reliableattacks against black-box machine learning models,2017, arXiv Preprint arXiv:1712
 Towards evaluating the robustness of neural networks,2017, In 2017ieee SympoSiUm on SeCurity and privacy (sp)
 Unlabeleddata improves adversarial robustness,2019, In AdVanCeS in Neural Information ProCeSSing Systems
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, arXiv preprint arXiv:2003
 Advertorch v0,2019, 1: An adversarial robustnesstoolbox based on pytorch
 Robust physical-world attacks on deep learningvisual classification,2018, In PrOCeedingS of the IEEE COnferenCe on COmputer ViSiOn and PatternRecognition
 Motivatingthe rules of the game for adversarial example research,2018, arXiv preprint arXiv:1807
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 On the effectiveness of intervalbound propagation for training verifiably robust models,2018, arXiv preprint arXiv:1810
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 The fusiform face area: a module inhuman extrastriate cortex specialized for face perception,1997, JOUrnaI of neuroscience
 Auto-encoding variational bayes,2013, arXiv PreprintarXiv:1312
 Autoen-coding beyond pixels using a learned similarity metric,2016, In International Conference on machineIearning
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Domain generalization with adver-sarial feature learning,2018, In Proceedings of the IEEE Conference on COmpUter VisiOn and PatternRecognition
 Characterizing adversarial subspaces usinglocal intrinsic dimensionality,2018, arXiv preprint arXiv:1801
 Visualizing data using t-sne,2008, JOUrnaI of machineIearning research
 Domain generalization via invariantfeature representation,2013, In International COnferenCe on MaChine Learning
 Simple black-box adversarial perturbationsfor deep networks,2016, arXiv preprint arXiv:1612
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Extending defensive distillation,2017, arXiv preprintarXiv:1705
 Towards the science ofsecurity and privacy in machine learning,2016, arXiv preprint arXiv:1611
 Foolbox: A python toolbox to benchmark therobustness of machine learning models,2017, arXiv preprint arXiv:1707
 Decoupling direction and norm for efficient gradient-based l2 adversarial attacks anddefenses,2019, In PrOCeedings of the IEEE COnferenCe on COmpUter VisiOn and Pattem Recognition
 On pruning adversarially robustneural networks,2020, arXiv preprint arXiv:2002
 Ape-gan: Adversarial perturbationelimination with gan,2017, arXiv preprint arXiv:1707
 A hilbert space embedding fordistributions,2007, In International COnference on Algorithmic Learning Theory
 Mat: A multi-strength adversarial training method to mitigate adversarial attacks,2018, In 2018IEEE CompUter Society AnnUal SymposiUm on VLSI(ISVLSI)
 Intriguing properties of neural networks,2013, arXiv PrePrint arXiv:1312
 Ensemble adversarial training: Attacks and defenses,2017, arXiv PrePrint arXiv:1705
 Stronger and faster wasserstein adversarialattacks,2020, arXiv PrePrint arXiv:2008
 Defending against physically realizable attackson image classification,2019, arXiv PrePrint arXiv:1909
 SPatially trans-formed adversarial examples,2018, arXiv PrePrint arXiv:1801
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv PrePrint arXiv:1708
 Towards deePer understanding of variationalautoencoding models,2017, 2017
