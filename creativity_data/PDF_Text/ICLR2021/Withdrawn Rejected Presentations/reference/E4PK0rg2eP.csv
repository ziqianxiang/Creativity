title,year,conference
 A Simple but Tough-to-Beat Baseline for SentenceEmbeddings ,2017, In Proceedings of ICLR
 Language Models are Few-Shot Learners,2020, 2020
 Multitask Learning,1997, Machine Learning
 Semi-Supervised Sequence Learning,2015, In Proceedings of NIPS
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2019, In Proceedings of NAACL
 Language-agnostic BERT Sentence Embedding,2020, arXiv:2007
 Catastrophic forgetting in connectionist networks,1999, Trends in cognitive sciences
 Compressing Large-Scale Transformer-Based Mod-els: A Case Study on BERT,2020, arXiv:2002
 Compressing BERT: Studying the Effectsof Weight Pruning on Transfer Learning,2020, In Proceedings of Rep4NLP 2020 Workshop at ACL2020
 Learning distributed representations of sentencesfrom unlabelled data,2016, In Proceedings of ACL
 Parameter-efficient transfer learning fornlp,2019, In Proceedings of ICML
 Universal Language Model Fine-tuning for Text Classifica-tion,2018, In Proceedings of ACL
 Categorical Reparameterization with Gumbel-Softmax,2017, InProceedings of ICLR
 Overcoming catastrophic forgettingin neural networks,2017, Proceedings ofthe National Academy ofSciences
 Skip-Thought Vectors,2015, In Proceedings of NeurIPS
 ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2020, InProceedings of ICLR
 Distributed Representations of Sentences and Documents,2014, InProceedings of ICML
 Mixout: Effective Regularization to Fine-tune Large-scale Pretrained Language Models,2020, In Proceedings of ICLR
 Overcomingcatastrophic forgetting by incremental moment matching,2017, In Advances in Neural InformationProcessing Systems
 LinguisticKnowledge and Transferability of Contextual Representations,2019, In Proceedings of ACL
 Multi-Task Deep Neural Networksfor Natural Language Understanding,2019, In Proceedings of ACL
 RoBERTa: A Robustly Optimized BERT Pre-training Approach,2019, arXiv:1907
 Learning Sparse Neural Networks throughL0 Regularization,2018, In Proceedings of ICLR
 The Concrete Distribution: A ContinuousRelaxation of Discrete Random Variables,2017, In Proceedings of ICLR
 Learned in translation:Contextualized word vectors,2017, In Proceedings of NeurIPS
 Regularizationtechniques for fine-tuning in neural machine translation,2017, In Proceedings of EMNLP
 Efficient Estimation of Word Repre-sentations in Vector Space,2013, arXiv:1301
 ContinualLifelong Learning with Neural Networks: A Review,2018, arXiv:1802
 Deep Contextualized Word Representations,2018, In Proceedings of NAACL
 MAD-X: An Adapter-based Frame-work for Multi-task Cross-lingual Transfer,2020, arXiv:2005
 Improving language under-standing by generative pre-training,2018, 2018
 LanguageModels are Unsupervised Multitask Learners,2019, 2019
 How fine can fine-tuning be? Learning efficient language mod-els,2020, In Proceedings of AISTATS
 ZeRO: Memory Optimiza-tions Toward Training Trillion Parameter Models,2019, arXiv:1910
 Efficient Parametrization of Multi-domain Deep Neural Net-works,2018, In Proceedings of CVPR
 Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,2019, In Proceedings of EMNLP
 Poor Man’s BERT: Smaller andFaster Transformer Models,2020, arXiv:2004
 Movement Pruning: Adaptive Sparsity byFine-Tuning,2020, arXiv:2005
 It’s Not Just Size That Matters: Small Language Models AreAlso Few-Shot Learners,2020, arXiv:2009
 Continual Learning with Deep Gener-ative Replay,2017, In Proceedings of NeurIPS
 Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Par-allelism,2019, arXiv:1909
 BERT and PALs: Projected attention layers for efficientadaptation in multi-task learning,2019, In Proceedings of ICML
 Patient Knowledge Distillation for BERT ModelCompression,2019, In Proceedings of EMNLP
 BERT Rediscovers the Classical NLP Pipeline,2019, InProceedings of ACL
 Well-Read Students Learn Better:On the Importance of Pre-training Compact Models,2019, arXiv:1908
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2019, In Proceedingsof ICLR
 Towards Universal ParaphrasticSentence Embeddings,2016, In Proceedings of ICLR
 Huggingface’s transformers: State-of-the-art natural language processing,2019, ArXiv
 Learning universal sentence representationswith mean-max attention autoencoder,2018, In Proceedings of EMNLP
 An Unsupervised SentenceEmbedding Method byMutual Information Maximization,2020, In Proceedings of EMNLP
 Masking as an Efficient Alternative toFinetuning for Pretrained Language Models,2020, arXiv:2004
