title,year,conference
 Sparse communication for distributed gradient descent,2017, InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
 QSGD: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems 
 The convergence of sparsified gradient methods,2018, In S
 Scaling up machine learning: Parallel anddistributed approaches,2011, Cambridge University Press
 Convex optimization: Algorithms and complexity,2015, Foundations and TrendsÂ®in Machine Learning
 Stabilization of linear systems with limited information,2001, IEEEtransactions on Automatic Control
 Fast distributed coordinate descent forminimizing non-strongly convex losses,2014, IEEE International Workshop on Machine Learning forSignal Processing
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Neuralcollaborative filtering,2017, In Proceedings of the 26th International Conference on World Wide Web
 Nonconvex variance reduced optimization with arbitrarysampling,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Distributed learning withcompressed gradients,2018, arXiv preprint arXiv:1806
 ADAM: A Method for Stochastic Optimization,2015, In ICLR
 Randomized distributed mean estimation: accuracy vs communi-cation,2018, Frontiers in Applied Mathematics and Statistics
 Efficientlarge-scale distributed training of conditional maximum entropy models,2009, In Y
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 On-chip training of recurrent neural networks withlimited numerical precision,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in Neural Information Processing Systems
 AIDE:Fast and communication efficient distributed optimization,2016, CoRR
 A stochastic approximation method,1951, The Annals of MathematicalStatistics
 Scaling distributedmachine learning with in-network aggregation,2019, CoRR
 1-bit stochastic gradient descent andapplication to data-parallel distributed training of speech dnns,2014, In Interspeech 2014
 Communication-efficient distributed optimization usingan approximate Newton-type method,2014, In Eric P
 Local SGD converges fast and communicates little,2018, CoRR
 Scalar quantization for relative error,2011, In 2011 Data CompressionConference
 Distributed meanestimation with limited communication,2017, In Proceedings of the 34th International Conference onMachine Learning
 DoubleSqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In Kamalika Chaudhuri andRuslan Salakhutdinov (eds
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
 Parallelized stochastic gradientdescent,2010, In J
