Table 1: Comparing performance in terms of accuracy, forgetting, and learning accuracy acrossmethods after training on the last task (averaged over 5 runs). ↑ indicates higher, J lower is better.
Table 2: Comparing performance in terms of average accuracy, forgetting, and learning accuracy forsequential finetuning after training on the last task. ↑ indicates higher is better, J indicates lower isbetter. All metrics are averaged across 5 runs. Overall, we observe that models pre-trained on diversecorpora (RoBERTa-base) undergo less forgetting across both 5 and 15 diverse tasks. Augmentingthe Finetune baseline with SAM (Section 5) results in performance competitive with SOTA, andaugmenting the ER baseline with SAM often outperforms SOTA.
Table 3: Average sharpness (lower is flatter) of minima across tasks in a 100 dimensional randomsubspace. Pre-training (PT) leads to flat minima for each task in training by an order of magnitude.
Table 4: 5-dataset statistics. |Train|, |Dev|, |Test| denotes the number of examples in train, dev, testsplits respectively. |L| denotes the number of classes for each task.
Table 5: 15-dataset-NLP: Task/Dataset description and statistics. All tasks are either single sentenceor sentence pair classification. |Train|, |Dev|, |Test| denotes the number of examples in train, dev, testsplits respectively. |L| denotes the number of classes for each tasks.
Table 6: Average sharpness (lower is flatter) of minima across tasks in a 100 dimensional randomsubspace. ResNet-18-PT (w/ PT) has lower sharpness in comparison to ResNet-18-R (w/o PT).
Table 7: Average sharpness (lower is flatter) of tasks minima. DistilBERT-PT (w/ PT) reduces thesharpness in comparison to DistilBERT-R (w/o PT).
