Table 1: Models trained and evaluated on 224 × 224 ImageNet-1k only. See more in Appendix A.
Table 2: Throughputs measured on an RTX8000 GPU using batch size 64 and fp16. ConvMixers andResNets trained ourselves. Other statistics: DeiT (Touvron et al., 2020), ResMLP (Touvron et al.,2021a), Swin (Liu et al., 2021b), ViT (Dosovitskiy et al., 2020), MLP-Mixer (Tolstikhin et al., 2021),Isotropic MobileNets (Sandler et al., 2019). We think models with matching colored dots (•) are infor-mative to compare with each other. ^Throughput tested, but not trained. Activations: ReLU, GELU.
Table 3: Small ablation study of training a ConvMixer-256/8 on CIFAR-10.
Table 4: An investigation of ConvMixer design parameters h, d, p, k and weight decay on CIFAR-1012Under review as a conference paper at ICLR 2022C Weight VisualizationsFigure 4: Patch embedding weights for a ConvMixer-1024/20 with patch size 14 (see Table 2).
