Table 1: Likelihood estimates. We approximate the joint, marginal, and crossmodal likelihoods(defined in App. B) of test samples using 500 importance weighted samples. All models maximizethe likelihood of image feature vectors and caption feature vectors. We report average and standarddeviation over five runs per model. (higher is better)Dataset and Model	log p(X1, X2)	logp(X1)	log p(X2)	log p(X1∖X2)	log p(X2∖X1)CUB MVAE (Wu and Goodman, 2018) MMVAE (Shi et al., 2019) HMVAE (this work)	-2046.0 ± 6.6 -2633.5 ± 47.7 -1424.8 ± 14.8	-1902.3 ± 8.8 -2075.5 ± 8.2 -1854.8 ± 14.2	142.8 ± 16.1 180.9 ± 10.5 439.6 ± 2.0	-3594.0 ± 337.0 -3227.2 ± 145.1 -2382.9 ± 19.9	-3024.9 ± 297.4 -2022.2 ± 77.5 -879.7 ± 31.8Oxford Flower MVAE (Wu and Goodman, 2018) MMVAE (Shi et al., 2019) HMVAE (this work)	-2471.3 ± 59.4 -3029.9 ± 50.5 -2069.1 ± 40.0	-2433.4 ± 22.5 -2575.5 ± 8.6 -2497.6 ± 21.7	215.4 ± 21.9 141.2 ± 26.0 442.2 ± 27.7	-3067.7 ± 67.5 -3179.3 ± 44.2 -3009.3 ± 38.0	-5282.8 ± 835.1 -3454.0 ± 218.4 -1400.0 ± 77.14.3	Image generationIn this section, we maximize the likelihood of images X1 ∈ R3×64×64 and caption feature vec-tors X2 ∈ R1024. Because the data is more complex than in the previous section, we now investi-gate deeper hierarchies for the HMVAE1 and incorporate additional baselines with multiple latentvariables (MDVAE and MHVAE). Note that the deep HMVAE employs the fewest parameters (seeTable 4 for an overview) and is the only model that does not use a warmup scheme for the KL-divergence loss (S0nderby et al., 2016).
Table 2: Oxford Flower All models maximize the likelihood of images and caption features vectors.
Table 3: Feature generation on CUB/Oxford Flower - MVAE and MMVAE. Both decodersparameterize normal distributions.
Table 4: Image generation on Oxford Flower - number of trainable parameters. The secondcolumn represents the number of latent variables. One of the reason the deeper HMVAE has fewerparameters than the shallow HMVAE lies in the dense layer that maps between spatial (unimodal)and vector (shared) representations. The deeper HMVAE achieves a lower spatial resolution andwhich minimizes the parameters of this layer. Note that this single layer can easily reach more thanthree million parameters, see the official MVAE implementation at www.github.com/mhw32/multimodal-vae-public/blob/master/celeba/model.py. In general, we want toprimarily ensure that the HMVAE does not improve performance due to excessive capacity.
Table 5: Image generation on Oxford Flower - MVAE and MMVAE. The image decoder gener-ates scalars, which are used to compute the binary cross-entropy with the ground truth. The captiondecoder parameterizes a normal distribution. We use the Swish activation function (Ramachandranet al., 2017).
Table 6: Image generation on Oxford Flower - MDVAE. For both models, the encoder producesa single tensor of size D. We split this tensor into two parts that represent g ∈ R100 and zi ∈ R8for i ∈ {1, 2}, respectively. The decoder for xi uses a concatenation of zi and g as input, whereagain i ∈ {1, 2}. For the image VAE, the decoder generates scalars, which are used to compute thebinary cross-entropy with the ground truth. For the caption VAE, the decoder parameterize normaldistributions.
Table 7: Image generation on Oxford Flower - MHVAE (1)Networks for x1: We follow Vasco et al. (2020) and use the miniature DCGAN architecture (Radfordet al., 2015). The decoder generates scalars, which are used to compute the binary cross-entropy withthe ground truth. Networks for x2 : The decoder for the second modality parameterizes a normaldistribution.
Table 8: Image generation on Oxford Flower - MHVAE (2)The networks for g → Z and h → Z are identical across modalities - except for the final outputdimension which depends on the latent size, where z1 ∈ R128 and z2 ∈ R48.
