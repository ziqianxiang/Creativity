Table 1: Max total average return within certain environmental steps (mean±std over 5 trials).
Table 2: Robustness comparison of the learned policies on HaIfCheetah-v2. Best performing poli-cies from Figure 2 are tested (4e7 timestep limitation for ARS and ES). Each policy is evaluated on400 trajectories in the same environment and the average return over 5 policies are listed.
Table 3: Hyperparameters of ZOAC for the learning curves shown in Figure 2Environment	Inv.D.P.-v2		Hopper-v2		HalfCheetah-v2		Ant-v2Policy type	Linear	Neural	Linear	Neural	Linear	Neural	Linear ∣ NeuralNum. of workers n	4	8	-4-	-8-	4	8	8Rollout Iength N 一	10		-20-	-10-	20		20Train frequency H	16		16	32	16		256Para. noise std. σ	0.02		0.04		0.04	0.06	0.02Batch size L	64				128		Num. of epoches M	8				4		Actor optimizer	Adam(aactor = 0.005, βι = 0.9, β? = 0.999)						Critic optimizer	Adam(acritic = 0.0003, βι = 0.9, β2 = 0.999)						Discount factor Y	0.99						GAE coeff. λ	0.95						Proof. (1) Variance bound for ES gradient estimatorsUnder the setting described in Section 3.2, the state-value under policy πθ+σ is estimated by theaccumulative return over NH timesteps, which is denoted as VnH+σ'. The isotropic Gaussian noiseadded to the policy can be presented as e = (s,⑶ …,6d)>, where q ~ N(0,1), l ∈ {1, 2,…,d}.
Table 4: Robustness comparison of the learned policies. Best performing policies from Figure 2are tested (4e7 timestep limitation for ARS and ES in HalfCheetah-v2 and Ant-v2 till convergence).
Table 5: Dimension of the state space, action space and parameter space of different policies.
