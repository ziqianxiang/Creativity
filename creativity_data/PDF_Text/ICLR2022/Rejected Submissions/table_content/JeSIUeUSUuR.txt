Table 1: Essential statistics of DNNs comPared (more details in APPendix D.4).
Table 2: Error rates (%) on CIFAR and STL10 datasets.
Table 3: : The top-1 and top-5 error rates on ImageNet32. On this dataset, among MLPMixer (i) fori = 4, 8, 12, 16, i = 8 gives the best results,far fewer parameters (e.g., MLPMixer (4) vs MLPMixer (1)+HanMixer (16)). On ImageNet32, ourHan/MLP-Mixer combination model clearly outperforms pure MLPMixers and offers a competitiveperformance with WideResNet while using only 40% parameters. In summary, the benefits of usingHanMixers have been made evident in this set of experiments.
Table 4: Dataset statistics: N is the number of samples and dim the dimension of data vectors.
Table 5: SGD parameters.
Table 6: Adam/AdamW parameters.
Table 7: Ablation study on a 100 × 17 network framework: Effects of layer and activation types onperformance. H denotes Householder and FC denotes fully-connected layers.
Table 8: Model specifications and statistics of datasets (Elevators and Cal Housing)HSPOIN0.140.120.10.080.06—HanNet testing*FCNet1 testing0	50	100	150	200	250	300Number of EpochsHanNet testingFCNetI testing0	50	100	150	200	250	300	. 0Number of Epochs—HanNet testing*FCNet1 testing—HanNet testing*FCNet1 testing0	50	100	150	200	250	300
Table 9: The architecture on one MLP-block or Han-block. For simplification, in our experiments, allhidden MLP weights are square matries.
