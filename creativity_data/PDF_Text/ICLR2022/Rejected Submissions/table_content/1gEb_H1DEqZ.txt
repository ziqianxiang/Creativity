Table 1: Leaderboard results on GLUE benchmark. The number below each task denotes the numberof training examples. F1 scores are reported for QQP and MRPC, Spearman correlations are reportedfor STS-B, and accuracy scores are reported for the other tasks.
Table 2: Accuracy on ReClor and LogiQA dataset. The public methods are based on large models.
Table 3: Main results on the dev and test set for DocRED. * indicates that the results are taken fromNan et al. (2020). Intra- and Inter-F1 indicates F1 scores for the intra- and inter-sentence relationsfollowing the setting of Nan et al. (2020).
Table 4: Ablation studies of Prophet on the test set of GLUE dataset.
Table 5: Results on GLUE test set when replacing facts with named entities and key the relationsunchanged.
Table 6: Distribution of context length on dev set of MNLI-matched and MNLI- mismatched dataset.
