Table 1: Ablation of the clean (top) and PGD-50-10 (bottom) accuracy when changing N-FGSMhyperparameters - noise level k and FGSM step size a. Results are averaged over 3 seeds. Allmodels are evaluated with PGD-50-10 attack and test = train = 8/255.
Table 2: Ablation of the PGD-50-10 accuracy for single-step methods when increasing the train. Allmodels are evaluated with PGD-50-10 attack and test = 8/255. Note that considering the trade-offbetween clean and robust accuracy, all methods perform best when training with the same epsilon tobe applied at test time.
Table 3: Comparison of “long” (Rice et al., 2020) and “fast” (Wong et al., 2020) training schedulesfor N-FGSM and GradAlign. GradAlign does not seem to benefit from the long training schedule.
Table 4: Comparison of the clean (top) and PGD-50-10 (bottom) accuracy across different values ofstep-size α and noise magnitude for the Uniform and Gaussian distributions with = 8/255. Forevery value of k, we use a Gaussian with matching standard deviation. We observe that when wematch the standard deviation, both distribution perform similarly.
