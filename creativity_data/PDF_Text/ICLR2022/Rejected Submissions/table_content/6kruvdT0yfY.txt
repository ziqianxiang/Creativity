Table 1: Effectiveness validation experiment on PASCAL VOCLoss function	mIoU	mAcc	aAccLseparability	75.93	85.47	94.59Lseparability + Lcompact	76.25	86.1	94.61Lseparability + L compact + Lbackground	77.35	86.37	94.584	ExperimentWe comprehensively evaluate the effectiveness of C + 1 classifier on semantic segmentation, andthen transfer to object detection with some minor adjustments. During inference we use the outputof softmax layer for classification.
Table 2: Effect of three different center representationsCenter representation	mIoU	mAcc	aAccLearnable	76.68	86.29	94.71Moving average	77.35	86.70	94.87Shared	76.99	86.57	94.794.2	Semantic segmentation4.2.1	Ablation studyFor ablation study, we use VOC+Aug as the training set. All images are resized to 512x512 as input.
Table 3: Weights’ effect of LcompactWeights for Lcompact	mIoU	mAcc	aAcc0.01	76.82	86.43	94.630.1	77.35	86.70	94.871	75.22	85.25	94.39Table 4: Weights’ effect of LbackgroundWeights for Lbackground	mIoU	mAcc	aAcc0.001	75.93	86.04	94.510.0001	77.35	86.70	94.87	0.00001		76.47	85.91	94.71power being 0.9. Batch size is set to 16. We use the same setting for the all ablation study unlessspecified otherwise.
Table 4: Weights’ effect of LbackgroundWeights for Lbackground	mIoU	mAcc	aAcc0.001	75.93	86.04	94.510.0001	77.35	86.70	94.87	0.00001		76.47	85.91	94.71power being 0.9. Batch size is set to 16. We use the same setting for the all ablation study unlessspecified otherwise.
Table 5: The performance details of all classes on PASCAL VOC with and without C + 1 lossCategory	L separability		LC+1		Category	Lseparability		LC+1		mIoU	mAcc	mIoU	mAcc		mIoU	mAcc	mIoU	mAccaeroplane	90.13	96.01	92.03	95.38	diningtable	57.14	60.24	54.87	57.38bicycle	41.75	91.74	41.56	95.38	dog	84.18	93.75	86.26	94.38bird	86.62	92.32	87.71	94.17	horse	84.86	88.88	84.74	92.98boat	70.92	87.90	72.43	87.27	motorbike	84.51	91.09	83.90	92.30bottle	76.63	87.79	78.37	89.66	person	84.99	91.61	85.08	91.08bus	94.68	97.49	95.05	96.99	PottedPlant	60.82	70.29	59.70	70.70car	86.40	93.06	86.03	93.85	sheep	83.93	92.37	88.47	91.98cat	90.50	94.84	93.10	96.94	sofa	46.29	53.87	55.80	65.52chair	36.36	55.11	37.44	54.77	train	90.21	94.10	89.40	93.97cow	86.65	90.30	87.68	90.18	tv/monitor	62.69	75.31	70.45	80.79background	94.21	97.33	94.35	97.32					Table 6: Experiment results on PASCAL VOC by DeeplabV3+ with ResNet101 as the backboneLoss	mIoU	mAcc	aAccDeePLabV3+ Chen et al.(2018a)	78.62	86.55	95.22DeePLabV3++oUrS	80.00	87.99	95.50with ResNet101 as the backbone, including center representation and iteration. For OCRNet+ours,we set the weights of Lcompact and Lbackground are set to 0.1 and 0.0001 respectively, and HR-
Table 6: Experiment results on PASCAL VOC by DeeplabV3+ with ResNet101 as the backboneLoss	mIoU	mAcc	aAccDeePLabV3+ Chen et al.(2018a)	78.62	86.55	95.22DeePLabV3++oUrS	80.00	87.99	95.50with ResNet101 as the backbone, including center representation and iteration. For OCRNet+ours,we set the weights of Lcompact and Lbackground are set to 0.1 and 0.0001 respectively, and HR-Net+FCN+ours with 0.01 and 0.0001 respectively. Results in table 7 shows that our C+1 classifier isalso applicable to other semantic segmentation algorithms. Especially on the OCRNet, our methodcan improve the baseline with 0.99 mIoU.
Table 7: Experiment results on different SOTA semantic segmentation algorithmsMethod	mIoU	mAcc	aAccHRNet48+FCN SUn et al.(2019)	76.23	84.95	94.66HRNet48+FCN+OUrs	76.96	85.38	94.81OCRNet YUan et al. (2020)	77.14	85.92	94.94	OCRNet+ours		78.13	86.92	95.098Under review as a conference paper at ICLR 2022Table 8: Experiment results on PASCAL ContextMethod	mIoU	mAcc	aAccDeePLabV3+ Chen et al.(2018a)	47.34	57.40	74.18DeePLabV3++oUrs	47.81	58.29	74.68Table 9: Experiment results on LIPMethod	Flip-test	mIoU mAcc aAccHRNet SUn et al. (2019)	N Y	"33.42^^65.20^^86.80 54.53	65.77	87.30HRNet+oUrs	N Y	■33.76^^6563^^86.81 54.78	66.19	87.324.4	Object detectionObject detection. In this section, we migrated our approach to the object detection task. Nowadays,existing methods for object detection can be divided into anchor-based and anchor-free according towhether anchors are needed. Among them, the anchor-based methods are quite different from the
Table 8: Experiment results on PASCAL ContextMethod	mIoU	mAcc	aAccDeePLabV3+ Chen et al.(2018a)	47.34	57.40	74.18DeePLabV3++oUrs	47.81	58.29	74.68Table 9: Experiment results on LIPMethod	Flip-test	mIoU mAcc aAccHRNet SUn et al. (2019)	N Y	"33.42^^65.20^^86.80 54.53	65.77	87.30HRNet+oUrs	N Y	■33.76^^6563^^86.81 54.78	66.19	87.324.4	Object detectionObject detection. In this section, we migrated our approach to the object detection task. Nowadays,existing methods for object detection can be divided into anchor-based and anchor-free according towhether anchors are needed. Among them, the anchor-based methods are quite different from thesegmentation task in the classification head. Specifically, semantic segmentation is the classificationof pixels. The target to be classified corresponds to the feature map position one-by-one, whilethe anchor-based methods needs to classify the anchor boxes, which is multiple to the feature mapposition. Therefore, we choose to verify our approach on the FCOS Tian et al. (2019) model whichis the most similar method in object detection to the FCN-based segmentation network.
Table 9: Experiment results on LIPMethod	Flip-test	mIoU mAcc aAccHRNet SUn et al. (2019)	N Y	"33.42^^65.20^^86.80 54.53	65.77	87.30HRNet+oUrs	N Y	■33.76^^6563^^86.81 54.78	66.19	87.324.4	Object detectionObject detection. In this section, we migrated our approach to the object detection task. Nowadays,existing methods for object detection can be divided into anchor-based and anchor-free according towhether anchors are needed. Among them, the anchor-based methods are quite different from thesegmentation task in the classification head. Specifically, semantic segmentation is the classificationof pixels. The target to be classified corresponds to the feature map position one-by-one, whilethe anchor-based methods needs to classify the anchor boxes, which is multiple to the feature mapposition. Therefore, we choose to verify our approach on the FCOS Tian et al. (2019) model whichis the most similar method in object detection to the FCN-based segmentation network.
Table 10: Experiment results on COCOMethod	AP	AP50	AP75	APS	APM	APLFCOS(R50) Tian et al. (2019)	38.5	57.7	41.0	21.9	42.8	48.6FCOS(R50)+oUrs	39.0	58.5	41.6	23.1	43.0	49.5FCOS(X101-64x4d) Tian et al. (2019)	42.6	62.3	45.6	25.7	46.3	54.6FCOS(X101-64x4d)+oUrs	43.0	62.9	46.1	27.2	46.8	54.59Under review as a conference paper at ICLR 2022ReferencesAbhijit Bendale and Terrance E Boult. Towards open set deep networks. In Proceedings of the IEEEconference on computer vision and pattern recognition, pp. 1563-1572, 2016.
