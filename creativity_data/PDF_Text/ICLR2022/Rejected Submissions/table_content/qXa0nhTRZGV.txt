Table 1: An ablation study for different variations of SAM on a two-class CIFAR-10 dataset. Theerror rates are averaged over three random seeds and reported with the standard deviationNAME	GRADIENT FORMULA LSall (w)	ERROR RATE * 5ERM	1	pn=1 '0(yi∙ fxi (w))Vfxi (w)	13.47% ± 0.80SAM DIRECTION	1 pn=1 '0(yi∙ fxi (w))Vfχi (wsam)	11.30% ± 0.68SAM DERIVATIVE	1 £乙 '(yi ∙ fχ<wsAM))Vfxi(W)	7.45% ± 0.28SAM	1	pn=1 '0(yi∙ fxi(wsAM))Vfxi(WSAM)	9.02% ± 0.34Does the loss derivative change explain the behavior of a non-linear model? To further supportour gradient reweighting interpretation for non-linear models, we show that changing only the lossderivative '0(yi ∙ fxJwsAM)) (See Eq. (7)) and discarding the change of the direction leads to themain benefit in SAM. For this, we train a ResNet-18 on a two-class CIFAR-10 under 80% labelnoise with early stopping and study different ways to modify the gradient RLS (W) used on eachiteration of training. We report results in Table 1 where we observe that SAM direction leadsonly to a small improvement compared to ERM (13.47% to 11.30%) while SAM DERIVATIVE ishelping most leading to 7.45% test error. This gives more evidence that the main improvement fromusing SAM comes from changing the derivative of the loss '0(yi ∙ fxi (WSAM)) and not the directionRfxi (wSAM) which is coherent with our gradient reweighting interpretation of SAM. We furtherreport extra experimental details and the plots over epochs in App. D.
Table 2: Test error of CE + SAM and GCE + SAM under label noise compared to methods from theliterature. Using SAM with the GCE loss works better against label noise compared to using SAMwith the CE loss as predicted by our gradient reweighting interpretation of SAM.
