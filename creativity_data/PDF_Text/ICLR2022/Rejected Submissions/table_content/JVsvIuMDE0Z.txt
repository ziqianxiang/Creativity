Table 1: Hyperparameters used in our experimentsHyperparameter		Value	Optimizer	Adam	Learning rate	3e-4	Batch size	256TD3	Target update rate	5e-3	Policy noise std	0.1	Policy noise clip	0.5	Policy update frequency	2	Hidden layers	2Architecture	Hidden units	256	Activation function	ReLU	Number of networks N	10REDQ	Randomly sampled networks M	2	Number of updates G	20Offline BC	Î±offline	0.4Adaptive BC	-Kp	3e-5	Kd	8e-5A Ablation on ensemblesIn our experiments, we use ensembles to represent the critic network. In 5, we compare the training
