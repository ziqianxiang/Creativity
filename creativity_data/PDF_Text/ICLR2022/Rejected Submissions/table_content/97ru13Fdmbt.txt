Table 1: Evaluation results in terms of 95% confidence intervals resulting from 20 independenttraining runs. Results correspond to the checkpoint that obtained the best prediction performance onvalidation data throughout training. The lower the values of ρ the better.
Table 2: Top-1 accuracy of standard and groupmonotonic models.
Table 3: AUC-ROC (the higher the bet-ter) for the detection of adversariallyperturbed data instances.
Table 5: Prediction performance of models trained on generated data in spaces of growing dimen-sion (D) and number of monotonic dimensions (|M |). Different regularization strategies do notaffect prediction performance. The performance gap consistently observed across the evaluationsets highlights the shift between the two sets of points. The lower the values of RMSE the better.
Table 6: Fraction of monotonic points ρ for models trained on generated data in spaces of growingdimension (D) and number of monotonic dimensions (|M |). Different regularization strategies iseffective on only one of Prandom or Ptest, while Ωmiχup seems effective throughout conditions. Thelower the values of ρ the better.
Table 7: Description of datasets used for empirical evaluation.
Table 8: Top-1 accuracy obtained by both standard and group monotonic models on sub-samples ofCIFAR-10. Predicition performance obtained by classifiers defined by the total activations is upperbounded by the performance obtained at the output layer for monotonic models.
Table 9: Top-1 accuracy of auxiliary classifiers evaluated on data created by occluding patchesdeemed irrelevant by explanation heat-maps given by different models. The performance of mono-tonic classifiers when constrained to consider only the feature maps within the slice correspondingto their prediction is further reported and shown to closely math the performance of cases where thefull set of features is considered.
