Table 1:	Results under oracle trainingand evaluation. Both here an in Table 2we report the cumulative expected engage-ment under a variety of different prefer-ences (as explained in Sec. 5). The val-ues are averaged across 1000 trajectories(Standard errors areall < 0.1). By looking atthe safe shift metrics r(uo) and r(utnd), Wesee that penalized systems stay significantlycloser to safe shifts than unpenalized ones.
Table 2:	Effect of estimating evaluations and sim-ulating training for LTV. We see that the esti-mated evaluations of trained systems strongly cor-relate with the oracle evaluations (and importantly,maintain their relative orderings). A similar effectoccurs when training in simulation rather than bytraining with on-policy data collected from real users.
