Table 1: Comparison of Testing Accuracy for Different Methods (mean ± standard deviation, in %).
Table 2: Comparison of Testing Accuracy for DifferentMethods (mean ± standard deviation, in %).
Table 3: ComParison of Testing ToP-1 Ac-curacy for Different θ of LSR and TSLA(in %).
Table 4: The estimated value of δ for different data setsdaqset	Stanford Dogs CUB-2011 CIFAR-100 CIFAR-100 (Pretrained)e	0.16	0.13	055	0.380.300.250.20value of δ——CUB-2011---Stanford Dogs0.150.100.050.00^----~~--——~~--~~--~~~~--~~--10 20 30 40 50 60 70 80 90# epochFigure 4: The estimated values of δ for different data sets.
Table 5: The hyper-parameters θ and S for different models and datasets*Model		θ			S			CUB	Dogs	CIFAR	CUB	Dogs	CIFARResNet-50	0.4	0.5	^^04	^70^^	50	180VGG	0.4	0.4	0.4	50	30	180MobileNet v2	0.4	0.5	0.5	50	30	180Inception v3	0.1	0.1	0.1	80	40	180EfficientNet	0.1	0.1	0.3	20	30	180*s: TSLA drops offLSR after S epochs.
Table 6: Comparison of Testing Accuracy (%) for Different Models on CUB-2011Model	Top-1 accuracy			Top-5 accuracy			baseline	LSR	TSLA	baseline	LSR	TSLAResNet-50	80.15	82.76	82.83	-94.87^^	95.08	95.55VGG	80.58	81.14	81.71	95.46	95.31	95.62MobileNet v2	78.44	79.91	80.14	94.58	94.39	94.94Inception v3	79.08	80.12	80.19	94.63	95.24	95.19EfficientNet	81.12	80.95	81.55	95.53	95.25	95.56Table 7: Comparison of Testing Accuracy (%) for Different Models on Stanford DogsTop-1 accuracy	Top-5 accuracyModel	baseline	LSR	TSLA	baseline	LSR	TSLAResNet-50	88.14	89.50	89.90	^9904^^	99.10	99.42VGG	85.96	86.42	86.64	98.61	98.72	98.85MobileNet v2	83.05	83.62	84.34	97.98	97.68	98.40Inception v3	87.10	87.30	87.48	98.93	98.82	98.90EfficientNet	85.72	85.71	86.26	98.80	98.60	98.78Table 8: Comparison of Testing Accuracy (%) for Different Models on CIFAR-100Model	Top-1 accuracy			Top-5 accuracy			baseline	LSR	TSLA	baseline	LSR	TSLAResNet-50	76.63	77.52	77.71	-93.99^^	93.08	94.17
Table 7: Comparison of Testing Accuracy (%) for Different Models on Stanford DogsTop-1 accuracy	Top-5 accuracyModel	baseline	LSR	TSLA	baseline	LSR	TSLAResNet-50	88.14	89.50	89.90	^9904^^	99.10	99.42VGG	85.96	86.42	86.64	98.61	98.72	98.85MobileNet v2	83.05	83.62	84.34	97.98	97.68	98.40Inception v3	87.10	87.30	87.48	98.93	98.82	98.90EfficientNet	85.72	85.71	86.26	98.80	98.60	98.78Table 8: Comparison of Testing Accuracy (%) for Different Models on CIFAR-100Model	Top-1 accuracy			Top-5 accuracy			baseline	LSR	TSLA	baseline	LSR	TSLAResNet-50	76.63	77.52	77.71	-93.99^^	93.08	94.17VGG	72.99	74.09	74.22	90.59	90.37	91.70MobileNet v2	74.42	76.64	76.72	93.12	93.13	94.01Inception v3	79.95	79.71	79.87	95.23	94.76	95.19EfficientNet	69.73	71.38	71.68	90.50	90.48	91.9315Under review as a conference paper at ICLR 2022Besides, we train ResNet-50 over ImageNet with the total epochs of 180. The initial learning rate isset to be 0.2 for all algorithm and divide them by 10 every 60 epochs as suggested in (He et al., 2016;
Table 8: Comparison of Testing Accuracy (%) for Different Models on CIFAR-100Model	Top-1 accuracy			Top-5 accuracy			baseline	LSR	TSLA	baseline	LSR	TSLAResNet-50	76.63	77.52	77.71	-93.99^^	93.08	94.17VGG	72.99	74.09	74.22	90.59	90.37	91.70MobileNet v2	74.42	76.64	76.72	93.12	93.13	94.01Inception v3	79.95	79.71	79.87	95.23	94.76	95.19EfficientNet	69.73	71.38	71.68	90.50	90.48	91.9315Under review as a conference paper at ICLR 2022Besides, we train ResNet-50 over ImageNet with the total epochs of 180. The initial learning rate isset to be 0.2 for all algorithm and divide them by 10 every 60 epochs as suggested in (He et al., 2016;Zagoruyko & Komodakis, 2016). The mini-batch size of 512 is used. The momentum parameterand weight decay are set to be 0.9 and 10-4 respectively. We fix the smoothing strength θ = 0.1and the drop epoch s = 120 for TSLA. The top-1 accuracy are presented in Table 9, showing thatTSLA and LSR are comparable.
Table 9: Comparison of Testing Accuracy (%) for ImageNetTop-1 accuracy	Top-5 accuracyModel baseline LSR―TSLA―baseline LSR―TSLAResNet-50	76.35	76.96 77.09 ∣ 93.14	93.55 93.69B Technical LemmaRecall that the optimization problem is1nmiw Fs(w) := - £'(yi,f(w； Xi))w∈	n i=1where the cross-entropy loss function ` is given by'(y,f (w； x)) = X -yi log (	"(w； X))y,	；	yi g Ki=1	j=1 exp(fj (w； X))(7)(8)If we setp(w； x) = (Pi(w； x),...,Pk (w； x)) ∈ RK, Pi(w； x) = — log [	KXMfiwX))	j , (9)j=1 exp(fj (w； X))the problem (7) becomes1n
