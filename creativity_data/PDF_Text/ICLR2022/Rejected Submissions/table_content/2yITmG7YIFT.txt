Table 1: Activation functions and their communication Figure 1: A comparison of different ac-costs. Since ReLU cannot be implemented efficiently in tivation functions.
Table 2: Summary of results on all the datasets using the standard unstructured dense layer. Wereport test accuracy except Criteo where we report AUC-PR due to its skewed label distribution.
Table 3: Test accuracy on the four problems with cosine activation function. For Criteo where thelabel distribution is skewed, we report AUC-PR.
Table 4: Test accuracy of the LeNet-5 model (LeCun et al., 1998) on Fashion-MNIST for each com-bination of activation function and type of the weight matrix. For Criteo, we use a standard feed-forward MLP with four hidden layers, each containing 1024 hidden units. Our proposed Hadamardstructure works better than the low rank approach in most cases, for all activation functions, whilebeing competitive to the standard dense layer (where no structure is imposed).
Table 5: Test accuracy of the LeNet-5 model (LeCun et al., 1998) on MNIST for different optimiza-tion methods, learning rates, and activation functions. The optimization method used affects theperformance of each activation function differently. The cosine activation offers a generic, robustconstruct that can be trained without the need of precisely choosing the learning rate.
Table 6: Experimental results on Higgs. Vary the numbers of hidden layers.
Table 7: Test accuracy on all the datasets using the proposed Hadamard layer. We report test accu-racy except Criteo where we report AUC-PR due to its skewed label distribution.
Table 8: Test accuracy on all the datasets using low-rank dense layers. We report test accuracyexcept Criteo where we report AUC-PR due to its skewed label distribution.
Table 9: Comparison of Hadamard transform against other structures imposed on the weight matrix.
