Table 1: Datasets: The question answering (left) and sentiment analysis (right) datasets in ourexperiments. Left: Query source (Q), Context source (C), mean query length (|Q|), and whether thequery was written independently from the context (Q⊥C). Right: mean review length (|R|) and thepercent representation of negative (-), neutral (N) and positive (+) labels.
Table 2: Optimal Domain Search: The optimal distribution of examples is shown per target domain,in 2k increments. The underlined value indicates the “Single source Domain” (2k in-domain, 8ksource domain) that gave best results. On the right we show the F1 score for this optimal distribution,the mean score across all distribution combinations, the best Single source Domain, and the BestAcquisition Function (from Figure 1). Typically allocating optimal domain budgets and the bestacquisition functions both performed strongly.
Table 3: Hyperparameter selection for task models.
Table 4: Hyperparameter selection for DAL discriminators.
Table 5: MQRA F1 scores from each active learning method over every training set size and targetdomain. The best performances are bolded and underlined.
Table 6: Sentiment accuracy scores from each active learning method over every training set size andtarget domain. The best performances are bolded and underlined.
