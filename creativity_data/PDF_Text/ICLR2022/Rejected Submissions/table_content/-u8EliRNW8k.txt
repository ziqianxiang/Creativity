Table 1: Adopted architectures for the KWS and SE tasks. In the KWS setting, (S) and (L) denotesmall model and large model respectively.
Table 2: Performance comparison on KWS tasks in terms of accuracy (%).*The size is reportedfrom the entire KWS model on the V2-35 task. Due to the different numbers of keywords, the valuevaries from task to task, although the backbone is the same. #The Gflops is calculated on V2-35task.
Table 3: Performance of speech-MLP on KWS tasks with different configurations.
Table 4: SE results on VoiceBank+Demand dataset.*The size of T-GSA was estimated from thestructure reported in the original paper, which involves 10 transformer layers, and the dimension ofthe input/output is 1024.
Table 5: Experimental settings on keyword spotting	Parameter	Value	win」en	512	win_hop	160Feature Extraction	fft」en	480	n_mel	80	n_mfcc	40	window	hann	Time masks	2Spec Aug	Time mask bin	[0, 15]	Frequency Masks	2	Frequency mask bin	[0, 7]	initlr	1.0e-03(V2-35)/1.0e-02(LW)	label smoothing	0.1	Schedule	cosineLearning Parameters	end lr	1.0e-05(V2-35)/1.0e-04(LW)	Optimizer	AdamW	Weight Decay	1.0e-4	Dropout	0.1A.1 System Architecture
Table 6: Experimental settings on speech enhancement	Parameter	Value	win」en	512	win」en	480Feature Extraction	win_hop	160	fft_len	512	window	hann	initlr	1.0e-02	Schedule	cosineLearning Parameters	T end lr	3000 1.0e-04	Warmup	30	Optimizer	AdamW	Epoch	1000Figure 4: The system architecture used in the speech enhancement experiment.
