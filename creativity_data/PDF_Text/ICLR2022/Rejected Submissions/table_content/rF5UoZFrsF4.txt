Table 1: Comparison of VUT with several existing multi-modal modeling works.
Table 2: Datasets.
Table 3: Comparison of three models for UI Object Detection on the validation dataset.
Table 4: Results for the Widget Captioning task						Configurations	BLEU-1	BLEU-2	BLEU-3	BLEU-4	ROUGE	CIDErSOTA(Wang et al., 2021)	-^65.5-	-^458-	-^324-	-^251-	-^486-	61.3Screen Summarization alone	68.7	49.4	31.6	19.4	53.8	64.3Summarization + Object Detection	68.9	50.8	33.5	21.4	54.9	65.64 tasks (without Object Detection)	68.2	49.4	32.2	20.2	53.5	56.8All 5 tasks	67.7	49.2	32.1	20.1	53.9	65.1Table 5: Results of the Screen Summarization task.
Table 5: Results of the Screen Summarization task.
Table 6: Results for the Language Command Grounding task.
Table 7: Results for the Tappability task.
Table 8: Accuracy for the UI Object Detection task when different tasks are jointly learned.
Table 9: Ablation Study Results for Focus WeightD UI Object TypesWe process the RICO dataset for the UI Object Detection task. As discussed in the paper, amongthe 64,462 screens of the original dataset, we particularly use those verified by human raters forvalidation and test datasets. For each element on the screen, we extract its attributes such as its UIobject type, its bounding box position on the screen, whether it is clickable or enabled. We excludeall the elements that are marked as invisible as they have no correspondence with pixels on the screen.
Table 10: Parameter size (millions) comparison between the 5-task joint model and single task model.
Table 11: Inference time (ms) comparison between the 5-task joint model and single task model foreach task. The time is calculated by averaging the inference times on the test set. There is little timeoverhead for the joint model to perform each task, compared to each task-specific model.
