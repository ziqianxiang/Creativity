Table 1: Out-of-sample TS forecasting. Global parameters of all models are trained on M4 exceptfor the entries with target in the name, which are instead trained on the target dataset. We evaluatethe 10 selected models both individually (top10), reporting mean Â± 95% confidence interval of theperformance metric, and as an ensemble which takes the median of the predictions (ens10). Bestperformance for each column is in bold and does not take into account the models with target in thename since they use all the TS in the target to learn the global parameters.
Table 2: Number of global parameters of the single models for the monthly frequency. From theGluonTS implementation. NBEATS-SH and NBEATS-NSH contain 30 residual blocks with (SH)and without (NSH) shared weights as described in Oreshkin et al. (2020).
Table 3: sMAPE on M3. All models except local ones are trained on M4 except the ones with targetin the name, which are instead trained on the target dataset.
Table 4: MAPE on tourism. All models except local ones are trained on M4 except the ones withtarget in the name, which are instead trained on the target dataset.
Table 5: sMAPE on M4 in the non-transfer setting. All models are trained on the target dataset.
Table 6: Source-targets combination for DeepAR and Meta-GLAR. A model per row is trained onthe dataset in the source column and tested on all the ones in the target column. The length of theforecast horizon follows each dataset preceded by a slash. Note that the length of the forecast horizoncan differ between source and target.
Table 7: Time lags used by Meta-GLAR, DeepAR and ablations for each time-frequency. These arethe defaults in the GlUonTS implementation of DeePARFrequency	Lags	Yearly Quarterly Monthly Weekly	[1, 2, 3,4, 5, 6, 7] [1,2,3,4,5,6,7,8,9, 11, 12, 13] [1,2,3,4,5,6,7,11,12,13,23,24,25,35,36,37] [1,2,3,4,5,6,7,8,12,51,52,53,103,104,105, 155, 156, 157]Daily	[1,2,3,4,5,6,7,8,13,14,15,20,21,22,27,28,29,30,31,56, 84, 363, 364, 365, 727, 728, 729, 1091, 1092, 1093] [1,2,3,4,5,6,7,23,24,25,47,48,49,71,72,73,Hourly	95, 96, 97, 119, 120, 121, 143, 144, 145, 167, 168, 169, 335, 336, 337, 503, 504, 505, 671, 672, 673, 719, 720, 721]Table 8: Hyperparameters for the random search. The first three entries are respectively the numberof steps, minibatch size and learning rate used by the optimizer. Context mult multiplies the horizonlength of the training dataset to give the final context length for the model. Representation dimis the dimension of the input to the last linear layer of the model (which is the dimension of therepresentation). Min history length is the minimum number of observations that must be present inthe context window of the training TS slices.
Table 8: Hyperparameters for the random search. The first three entries are respectively the numberof steps, minibatch size and learning rate used by the optimizer. Context mult multiplies the horizonlength of the training dataset to give the final context length for the model. Representation dimis the dimension of the input to the last linear layer of the model (which is the dimension of therepresentation). Min history length is the minimum number of observations that must be present inthe context window of the training TS slices.
