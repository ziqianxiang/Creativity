Table 1: Hyper-parameters information for for DQN models used in E1 to E2Hyper-parameters	ValuesHidden layers	{1, 2}Hidden units	{16, 32, 64, 128}Learning rate	{1 × e-3, 5 × e-4}DQN training iterations	{100, 500, 1k, 2k}Batch size	{64}B roader ImpactThere are also some limitations of the proposed PMS as one of the preliminary attempts on modelselection for offline reinforcement learning. When the benchmarks environments (excluded Atarigames) are based on simulated environments to collect the true policy (Barth-Maron et al., 2018;Siegel et al., 2019), more real-world-based environments could be customized and studied in fu-ture works. For example, one experimental setup needs to be carefully controlled in clinical set-tings (Tang & Wiens, 2021) or resilience-oriented (Yang et al., 2021) reinforcement learning.
Table 2: Hyper-parameters information for for DQN models used in E3 to E4Hyper-parameters	ValuesConvolutional layers	{ 2, 3}Convolutional units	{16, 32}Hidden layers	{ 2, 3}Hidden units	{64, 256, 512}Learning rate	{1 × e-3, 5 × e-4}DQN training iterations	{4M, 4.5M, 5M}Batch size	{64}Table 3: Hyper-parameters information for double DQN (DDQN) models (Van Hasselt et al., 2016) with aprioritized replay (Schaul et al., 2015) used in E5 to E6.
Table 3: Hyper-parameters information for double DQN (DDQN) models (Van Hasselt et al., 2016) with aprioritized replay (Schaul et al., 2015) used in E5 to E6.
