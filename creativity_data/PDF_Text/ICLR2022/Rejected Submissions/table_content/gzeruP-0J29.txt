Table 1: Performance overview of proposed FAST-BAT vs. the baselines FAST-AT (Wong et al., 2020) andFast-AT-GA (Andriushchenko & Flammarion, 2020) on (CIFAR-10, PreActResNet-18). All methods arerobustly trained under two perturbation budgets = 8/255 and 16/255 over 20 epochs. We use the early-stoppolicy (Rice et al., 2020) to report the model of best robustness for each method. The evaluation metrics includerobust accuracy (RA) against PGD-50-10 attacks (50-step PGD attack with 10 restarts) (Madry et al., 2018) at= 8/255 and 16/255 (test-time is same as the train-time), RA against AutoAttack (AA) (Croce & Hein,2020) at = 8/255 and 16/255, and computation time (per epoch). The result a±b represents mean a andstandard deviation b over 10 random trials. All experiments are run on a single Tesla-P100 GPU.
Table 3: SA and RA on ImageNet.
Table 2: SA, RA-PGD and RA-AA of different robust training methods in the setup (CIFAR-10, PARN-18training with = 8/255) and (CIFAR-10, PARN-18 training with = 16/255), respectively. All the results areaveraged over 5 independent trials with different random seeds.
Table 4: Performance of different robust training methods under different model types. All the models are bothtrained and evaluated with the same perturbation strength .
Table 5: RA of robust PARN-18 trained by the four differ-ent methods against adaptive attacks (‘RA-PGD’ column)and transfer attacks (‘Transfer Attack’ columns). Naturallytrained PARN-18, PARN-50, and WRN-16-8 are used assurrogate models for PGD-20 attack with e = 8/255.
Table A1: Performance of FAST-BAT with differentparameter λ. We train and evaluate with the same attackbudget = 16/255 on CIFAR-10 to show the influencebrought by λ.
Table A2: Performance of FAST-BATwith different α2choices on CIFAR-10. Models are trained and evaluatedwith the same attack budget ( = 16/255). Here α1 isset as the cyclic learning rate and thus, is not a constantparameter. α2 is always set proportionate to α1 forsimplicity.
Table A3: Performance of FAST-BAT with differentlinearization schemes. Besides 1-step PGD without sign(PGD w/o Sign), we further generate linearization pointwith the following methods: uniformly random noise[-, ]d (Uniformly Random); uniformly random cor-ner {-, }d (Random Corner); and perturbation from1-step PGD attack with 0.5 as attack step (PGD).
Table A4: Performance of Hessian-free and Hessian-aware FAST-BATon CIFAR-10. We train and evaluatewith the same attack budgets = 8/255 and = 16/255 to show the influence brought by Hessian matrix.
Table A5: Performance of FAST-ATand FAST-BATwith different activation functions on CIFAR-10. ReLU,Swish and Softplus are taken into consideration. For Fast-BAT, we compare the Hessian-free and Hessian-awareversion to verify the influence of Hessian matrix. The results are averaged over 3 independent trials.
