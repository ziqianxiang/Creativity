Table 1: Trained networks for generating APs		MNIST		F-MNIST	Architecture	Optimizer	acc.	loss	acc.	lossFC	SGD	0.95	0.19	0.85	0.42	Adam	0.98	0.07	0.88	0.33CNN	SGD	0.93	0.24	0.84	0.45	Adam	0.99	0.05	0.92	0.24with batch	SGD	0.99	0.03	0.92	0.22normalization	Adam	0.98	0.06	0.92	0.25epochs due to the fact that generating ADEs with highly accurate and complicated architecturesbecame adversely expensive from a computational point of view. After training the networks, I gen-erated APs only with the zero and noise feed methods, given that the AAI approach is considerablymore time-consuming. For presenting the results, I hand-picked one of the two methods based onthe clarity of the patterns and trying to include more diversity.
