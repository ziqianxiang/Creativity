Table 1: Quantitative result for several attacks and their defense by poison immunity with s = 0.75,showing avg. poison success with standard error (where all trials have equal outcomes, we report theworst-case error estimate 5.59%). Additional details about each attack threat model can be found inthe Appendix E.2. The proposed defense significantly decreases success rates over a wide range ofattacks and scenarios without any hyperparameter changes. This table evaluates gradient matching(GM) with both squared error (SE) and cosine similarity (CS). The evaluation column with thestrongest attack uses gradient matching with cosine similarity.
Table 2: Avg. poison success for various defenses against backdoor triggers attacks, for an attack viaa 4x4 patch on 5% of training data. Top table: Baseline and defenses via filtering and differentialprivacy. Bottom table: Variations of adversarial training against poisons. We evaluate each on a noisycheckerboard patch (Noise) and a patch with semantic meaning, a firefox logo (Sem).
Table 3: This is the same table as Table 1 in the main body, with additional information of naturalvalidation accuracy and timing for each run. Best viewed on screen. All shown timings are wall-clocktime for an NVIDIA GTX-2080ti with 4 assigned CPUs. Some timings are missing (due to machineheterogeneity), but can be inferred from other rows in the same block (i.e. all transfer experimentstake roughly the same amount of time for the "Strongest Attack" defense).
Table 4: Avg. poison success for various defenses against backdoor triggers attacks, for an attackvia a 4x4 patch on 5% of training data. Extended version of Table 2, but note the flip in rows andcolumns. Best viewed on screen.
Table 5: Additional quantitative result for several attacks and their defense by poison immunity withs = 0.75, showing avg. poison success with standard error (where all trials have equal outcomes, wereport the worst-case error estimate 5.59%). Additional details about each attack threat model can befound in the Appendix E.2.
Table 6: Defenses against feature collision via (Aghakhani et al., 2020) in the transfer setting for abudget of 1% and bound of Îµ = 16.
Table 7: Transfer* refers to the explicit setting of Zhu et al. (2019). The proposed defense significantlydecreases success rates, even in this setting.
