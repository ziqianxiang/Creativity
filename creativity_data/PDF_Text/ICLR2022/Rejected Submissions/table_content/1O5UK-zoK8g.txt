Table 1: (a) Segmentation accuracy using IaBN. We report the mean IoU (%) on three target domains(Cityscapes, BDD, IDD) across both backbones. t-BN denotes train BN (Ioffe and Szegedy, 2015),while p-BN refers to prediction-time BN (Nado et al., 2020). (b) ECE (%) for IaBN and MC-Dropout(Gal and Ghahramani, 2016). We report scores for three target domains (Cityscapes, BDD, IDD)across both backbones. We trained the networks on GTA in both cases (cf. supplemental material forresults with SYNTHIA training).
Table 3: Mean IoU (%) with TTA (Simonyan and Zisserman, 2015) and our Seg-TTT. We reportscores across both source domains (GTA, SYNTHIA) and three target domains (Cityscapes, BDD,IDD) for both backbones.
Table 2: Mean IoU (%) comparison of IaBN to Comparison to other related work. We addi-alternative normalization strategies: SN (Luo et al., tionally compare IaBN to alternative normaliza-2019) and BIN (Nam and Kim, 2018).					tion strategies proposed in the literature: Batch- Instance Normalization (BIN; Nam and Kim,Method	CS	BDD	IDD	Mean	2018) and Switchable Normalization (SN; LuoSN	31.75	33.60	31.60	32.32	et al., 2019). Although both of these techniquesBIN	34.57	32.68	30.22	32.49	may appear technically similar to our IaBN,IaBN (ours)	37.54	32.79	34.21	34.85	these approaches were developed for different purposes. SN was only shown to improve in-										domain accuracy, while BIN tackles domainadaptation for image classification. Our work studies domain generalization. Furthermore, bothmethods modify the model architecture before training, while IaBN works with any pretrained seman-tic segmentation model. We implemented both in our segmentation model based on the ResNet-50backbone. We trained these approaches on GTA in an identical setup as IaBN. From results in Table 2,we observe that IaBN outperforms both BIN and SN by a significant margin in terms of mean IoU ofthe target domains.
Table 4: Mean IoU (%) comparison to state-of-the-art domain generalization methods for bothsource domains (GTA, SYNTHIA) as well as three target domains (Cityscapes, Mapillary, BDD). Wecompare to IBN-Net (Pan et al., 2018), Yue et al. (2019), ASG (Chen et al., 2020) and CSG (Chenet al., 2021). In-domain training to obtain the upper bounds uses our baseline DeepLabv1 and followsthe same schedule as with the synthetic datasets. ⑴ and。)denote the use of FCN (Long et al.,2015) and DeepLabv2 (Chen et al., 2018a) architectures, respectively.
Table 5: (a) Segmentation accuracy using IaBN. We report the mean IoU (%) on three target domains(Cityscapes, BDD, IDD) across both backbones. As before, t-BN denotes train BN (Ioffe and Szegedy,2015), while p-BN refers to prediction-time BN (Nado et al., 2020). (b) ECE (%) for IaBN andMC-Dropout (Gal and Ghahramani, 2016). We report scores for three target domains (Cityscapes,BDD, IDD) across both backbones. We trained the networks on SYNTHIA in both cases.
Table 6: The role of the augmentation type in Seg-TTT. We report mean IoU (%, ↑) and runtime (ms,1)for TTA (Simonyan and Zisserman, 2θ15) and our Seg-TTT for the GTA source domain and theCityscapes target domain for the ResNet-50 backbone.
Table 7: Runtime (ms) with TTA (Simonyan and Zisserman, 2015) or Seg-TTT. We report the runtimefor both ResNet-50 and ResNet-101 on three dominant resolutions of 2048 × 1024, 1280 × 720, and1920 × 1080, corresponding to the target domains Cityscapes, BDD, and IDD, respectively.
Table 8: Mean IoU (%) using our IaBN + Seg-TTT integrated with DeepLabv3+ (Chen et al., 2018b)based on a ResNet-50 and ResNet-101 backbone as well as with HRNet-W18 and HRNet-W48 (Wanget al., 2021b). We observe substantial improvements of the segmentation accuracy on all three targetdomains (Cityscapes, BDD, and IDD) after training on GTA.
Table 9: Mean IoU (%) comparison to state-of-the-art domain generalization methods for bothsource domains (GTA, SYNTHIA) as well as three target domains (Cityscapes, Mapillary, BDD). Wecompare to RobustNet (Choi et al., 2021) and FSDR (Huang et al., 2021).⑴ and (计)denote the useof FCN (Long et al., 2015) and DeepLabv3+ (Chen et al., 2018b) architectures, respectively.
