Table 1: Comparison of test error at the end of training for different setups and learning rate schedules.
Table 2: Comparison of performance between a simple learning rate decay and a "complex" decayamong tasks: "complex" means cosine decay for vision tasks and linear decay for NLP and RL. ForNLP and RL tasks higher metrics imply better performance, while for vision tasks, lower error denotesbetter performance. None of these tasks (except for the vision task with L2 used as a reference) haveweight norm bouncing nor an advantage from non-simple schedules. We have averaged the RL tasksover 3 runs and their difference is compatible with noise. See S1 for the individual GLUE scores, asit is common we have omitted the problematic WNLI. We use test accuracy = 100- test error, sothat for all metrics in these table, higher is better.
