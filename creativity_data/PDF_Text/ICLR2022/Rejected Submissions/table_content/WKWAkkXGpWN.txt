Table 1: Results (Per-class Accuracy) on family tree reasoning benchmarks. Models are tested ondomains with 100 objects. Minus mark means the model cannot scale up in training datasets ofcorresponding sizes or cannot handle ternary predicates.
Table 2: Results (AUC-PR) on real-world knowledge graph inductive reasoning datasets from GraIL.
Table 3: Results (AUC-PR) of transductive linkprediction on real-world knowledge graphs.
Table 4: Comparison of different samplers. Table 5: Comparison (per-class accuracy) for differentlabel calibration methods.
Table 6: Hyper-parameters for SpaLoc. The definition of depth and breadth are illustrated in Figure 1.
Table 7: Comparison of different sparsification losses.
