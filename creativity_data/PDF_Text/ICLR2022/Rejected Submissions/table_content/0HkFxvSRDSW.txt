Table 1: Performance of three parameter sharing strategies on different scenarios. Warm-up refers tothe reward value point where the strategies start to differentiate. + represents the additional rewardgained based on warm-up performance. The left side and right side of the / represent the rewardgained at the half training steps and the full training steps respectively. The best performance ineach scenario is marked in bold red. More detailed analysis can be found in Sec. 5.1.
Table 2: Different vision scopes (6-9-18) impact the model performance. The scope should belarger than 6, which is the attack scope for agents. Xrepresents the scope with the best performance.
Table 3: Policy-based role diversity influences the performance of different parameter sharing strate-gies on the MPE[28] and SMAC[39] benchmarks. The best performance in each scenario is markedX. Asterisks denote the algorithms that are not significantly different. Q diversity curves can bereferred to Fig. 18.
Table 4: Different role diversities on different scenarios from SMAC. The minimum value of onecolumn is labeled in green and the largest value is labeled in red. Detailed analysis can be found inAppx. E.
Table 5: Model performance including selective parameter sharing as a supplement to Table. 1. Thegrouping result is provided in the last column.
Table 6: Policy-based role diversity influence the performance of different parameter sharing strate-gies on the MPE [28] and SMAC [39] benchmarks.
