Table 1: PPO IMPALA-CNN evaluations (mean returnat 50M steps) on Dodgeball. Learning curves can befound in Appendix C.1, Figure 13.
Table 2: Supernet training rewards on Dodgeball. Learning curves can be found in Appendix B.
Table 3: Computational efficiency in terms of wall-clock time, achieved on a V100 GPU. For the RL cases(PPO + Rainbow), all networks use depths of 16 × 3. Training cost in RL is defined as the wallclock time takento reach 25M steps, rounded to the nearest 0.5 GPU day. We have also included reported time for DARTS in SL(Liu et al., 2019) as comparison.
Table 4: Average normalized rewards across all 16 environments w/ PPO, using the normalization method from(Cobbe et al., 2020). Full details and results (including Rainbow) are presented in Appendix D.
Table 5: Normalized Rewards in ProcGen across different search methods, evaluated at 25M steps with depths64 × 3. Largest scores on the specific environment (as well as values within 0.03 of the largest) are bolded.
