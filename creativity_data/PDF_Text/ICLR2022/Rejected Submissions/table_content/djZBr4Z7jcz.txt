Table 1: Investigating the Closed/analytic solutions of linear models. dMat(∙) denotes a diagonalmatrix, diag(X) is the vector on the diagonal of X. Λ is the (hyperparameter) diagonal matrix as acoefficient of the regularization term. W * (or P *, Q*, etc) is the optimal solution for correspondingcase, except for cases 9-12, where Wc is the low-rank closed-form solution.
Table 2: The performance comparison between different regularizations. We highlight the best resultsin bold and underline the 2nd best results for each metric. See Table 1 for the notation.
Table 3: Investigating the weight ordering of Matrix FactorizationModel	ML-20M			Netflix			MSD			Recall@20	ReCan@50	nDCG@100	Recall@20	Recall@50	nDCG@100	Recall@20	ReCan@50	nDCG@100MFZLRR Weighted	-0.3806-	-0.5175-	0.4102	-0.3484-	-0.4320-	0.3797	-0.2508-	-0.3390-	0:3037MF sorted	0.3017	0.4507	0.3361 —	0.2860	0.3801	0.3265 —	0.2288	0.3148	0.2802 —9Under review as a conference paper at ICLR 2022ReferencesCharu C. Aggarwal. Recommender Systems: The Textbook. Springer, 1st edition, 2016. ISBN3319296574.
Table 4: Time complexity analysis for the models listed in Table 1. X ∈ Rm×n. Assume m > n.
Table 5: ml-20m, DLAE full rank, parameter tuning on validation dataset by nDCG@100			λ								800	900	1000	1100	1200	1300P	0.1	0.42024	0.42063	0.42073	0.42102	0.42131	0.4212	^02^	0.43132	0.43139	0.43154	0.4314	0.43147	0.43136	^03^	0.43203	0.43211	0.43214	0.43206	0.43203	0.43196	^01^	0.43001	0.43001	0.42995	0.42996	0.42984	0.42978	~0Γ^	0.42754	0.42745	0.42729	0.42718	0.42715	0.42704Table 6: netflix, DLAE full rank, parameter tuning on validation dataset by nDCG@100			 λ										800	900	1000	1100	1200	1300	1400P	0.2	0.3904	0.3904	0.39027	0.39024	0.3902	0.3903	0.39018	0.25	0.39247	0.39252	0.39248	0.39249	0.3925	0.3925	0.39256	0.3	0.39359	0.39359	0.39366	0.39358	0.39362	0.39368	0.39369	0.35	0.39402	0.39403	0.394	0.39405	0.39399	0.39403	0.39397	0.4	0.39399	0.39393	0.39395	0.39393	0.39389	0.39388	0.39387	0.45	0.39346	0.3935	0.39343	0.39344	0.39338	0.3933	0.39329	0.5	0.39249	0.39241	0.39247	0.39241	0.39242	0.3923	0.39224E.3 Matrix Factorization with Dropout and Hyperparameter TuningCavazza et al. (2018) show that optimization with dropout (allowing rank d to be optimized) is
Table 6: netflix, DLAE full rank, parameter tuning on validation dataset by nDCG@100			 λ										800	900	1000	1100	1200	1300	1400P	0.2	0.3904	0.3904	0.39027	0.39024	0.3902	0.3903	0.39018	0.25	0.39247	0.39252	0.39248	0.39249	0.3925	0.3925	0.39256	0.3	0.39359	0.39359	0.39366	0.39358	0.39362	0.39368	0.39369	0.35	0.39402	0.39403	0.394	0.39405	0.39399	0.39403	0.39397	0.4	0.39399	0.39393	0.39395	0.39393	0.39389	0.39388	0.39387	0.45	0.39346	0.3935	0.39343	0.39344	0.39338	0.3933	0.39329	0.5	0.39249	0.39241	0.39247	0.39241	0.39242	0.3923	0.39224E.3 Matrix Factorization with Dropout and Hyperparameter TuningCavazza et al. (2018) show that optimization with dropout (allowing rank d to be optimized) isequivalent to solving a matrix approximation problem with nuclear norm:1dmin IIX - PQIlF + d - ∙ £ |典||2 ∙∣∣QTII2P,Q,d	F	p k=1	2 k 2	(47)mYn IIX - YIlF + — I∣Y∣∣2Table 7: msd, DLAE full rank, parameter tuning on validation dataset by nDCG@100			λ								10	20	30	40	50	60
Table 7: msd, DLAE full rank, parameter tuning on validation dataset by nDCG@100			λ								10	20	30	40	50	60P	0.3	0.38514	0.38515	0.38517	0.38505	0.38492	0.38474	^04^	0.38596	0.38599	0.38602	0.386	0.38597	0.38592	^0^	0.38556	0.38555	0.38553	0.38557	0.38553	0.38549	~Q6~	0.38382	0.3838	0.38381	0.38374	0.38373	0.3836624Under review as a conference paper at ICLR 2022and the solution is given by:X S=VD UΣVTY * = P * ∙ Q*=U ∙ Sμ(Σ) ∙ VTSμ(σ) = max(σ 一 μ, 0)μ = ∣1ι P 浦Xσi(X)p + (1 - p)d i=1where d denotes the largest integer such that:σd(X) > P +(1 -p)ddXσi(X)
Table 8: ml-20m, matrix factorization with dropout, hyper parameter tuning by nDCG@100 onvalidation dataset and its induced rank .____________________________________________P	0.9	0.99	0.995	0.996	0.997induced rank d	10	-200-	-385-	-467-	-602-nDCG@100~	0.29723	0.39369	0.40045	0.40046	0.39925Table 9: netflix, matrix factorization with dropout, hyper parameter tuning by nDCG@100 onvalidation dataset and its induced rank .____________________________________________P	0.9	0.99	0.996	0.997	0.998induced rank d	9	-209-	-524-	-653-	-883-nDCG@100~	0.26026	0.35462	0.36453	0.36495	0.36406Table 10: msd, matrix factorization with dropout, hyper parameter tuning by nDCG@100 onvalidation dataset and its induced rank .____________________________________________P	0.99	0.999	0.9995	0.9999	0.99995induced rank d	-249-	-2054-	3783	11380	19308nDCG@100~	0.18986	0.28532	0.307	0.32634	0.30995E.4 ResourcesOur code are mainly implemented in Numpy 1.19, Pytorch 1.7.1 on CUDA 11.0. Our experimentsare performed on nodes with two sockets, each containing a 24-core Intel(R) Xeon(R) Platinum 8268CPU @ 2.90GHz and 4 GeForce RTX 3090 24GB memory GPU.
Table 9: netflix, matrix factorization with dropout, hyper parameter tuning by nDCG@100 onvalidation dataset and its induced rank .____________________________________________P	0.9	0.99	0.996	0.997	0.998induced rank d	9	-209-	-524-	-653-	-883-nDCG@100~	0.26026	0.35462	0.36453	0.36495	0.36406Table 10: msd, matrix factorization with dropout, hyper parameter tuning by nDCG@100 onvalidation dataset and its induced rank .____________________________________________P	0.99	0.999	0.9995	0.9999	0.99995induced rank d	-249-	-2054-	3783	11380	19308nDCG@100~	0.18986	0.28532	0.307	0.32634	0.30995E.4 ResourcesOur code are mainly implemented in Numpy 1.19, Pytorch 1.7.1 on CUDA 11.0. Our experimentsare performed on nodes with two sockets, each containing a 24-core Intel(R) Xeon(R) Platinum 8268CPU @ 2.90GHz and 4 GeForce RTX 3090 24GB memory GPU.
Table 10: msd, matrix factorization with dropout, hyper parameter tuning by nDCG@100 onvalidation dataset and its induced rank .____________________________________________P	0.99	0.999	0.9995	0.9999	0.99995induced rank d	-249-	-2054-	3783	11380	19308nDCG@100~	0.18986	0.28532	0.307	0.32634	0.30995E.4 ResourcesOur code are mainly implemented in Numpy 1.19, Pytorch 1.7.1 on CUDA 11.0. Our experimentsare performed on nodes with two sockets, each containing a 24-core Intel(R) Xeon(R) Platinum 8268CPU @ 2.90GHz and 4 GeForce RTX 3090 24GB memory GPU.
