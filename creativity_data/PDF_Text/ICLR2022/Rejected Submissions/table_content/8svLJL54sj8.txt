Table 1: TasksTask Dataset	Model	Batch SizesCIFAR10	Wide ResNet	{256, 2048}CIFAR100	Wide ResNet	{256, 2048}Fashion MNIST	Max pool CNN ReLU	{256, 2048}Fashion MNIST	Max pool CNN tanh	{256, 2048}Fashion MNIST	Simple CNN	{256, 2048}ImageNet	ResNet50	{512, 1024, 2048}LM1B	Transformer	{2048}MNIST	Max pool CNN relu	{256, 2048}MNIST	Max pool CNN tanh	{256, 2048}MNIST	Simple CNN	{256, 2048}SVHN (no extra)	Wide ResNet	{256, 1024}WMT15 German-English	xformer	{64}uniref50	Transformer	{128}Table 2: 4-dimensional input searchspace (see text for more details)Hyperparameter	Range	Scaling	n		[10-5, 10]	Log	p		-[0.1, 2.0]-	Linear
Table 2: 4-dimensional input searchspace (see text for more details)Hyperparameter	Range	Scaling	n		[10-5, 10]	Log	p		-[0.1, 2.0]-	Linear1 - β	[10-3, 1.0]	Log	λ		[0.01, 0.99j~	LinearWe collected two types of data: matched and unmatched data. Matched data used the same set ofuniformly-sampled hyperparameter points across all tasks and unmatched data sampled new pointsfor each task. All other training pipeline hyperparameters were fixed to hand-selected, task-specificdefault values. All of our tasks are classification problems, so they all used the same trainingloss, although occasionally task-specific regularization terms were added. For each trial (trainingrun for a single hyperparameter point), we recorded validation error (both cross entropy error andmisclassification rate). In many cases, poor optimizer hyperparameter choices can cause training todiverge. We detected divergent training when the training cost became NaN and then marked thetrial but did not discard it. Please see the Appendix, supplementary material, and code (Onomous,2021) for additional details about the tasks and training procedure. The different tuning tasks varyin difficulty and numbers of data points, but generally there are roughly 500 matched datapointsand 1500 unmatched datapoints per tuning task. For unmatched data only, we attempted to generateroughly similar numbers of non-divergent points across tasks, so tasks with a higher probability of
Table 3: The mean and standard error of best validation error rates (%) for each test task in the offlineoptimizer hyperparameter tuning experiments. Meta BO methods including MIMO and HyperBOvariants (H* NLL and H* KL) have access to training tasks that do not share the same task dataset asthe test task. We show results of the top 5 methods, and we highlight the lowest error rates in bold.
Table 4: The mean and standard error of best validation error rates (%) for each test task in the offlineleave-one-out experiments. We show results of the top 6 methods, and we highlight the lowest errorrates in bold.
Table 5: NLLs on 23 tasks and (pseudo) KL divergences on matching datasets with trained andrandomly initialized GP models. The NLL of randomly initialized model (No training) on all tasks is148211.2. The KL value of randomly initialized model (No training) is 2177.2. Training on a subsetof a sub-dataset in the test task (Single task) often leads to much worse marginal likelihood on theentire sub-dataset. Training on irrelevant tasks (H*) achieves much lower (pseudo) KLs on matchingdatasets and lower NLLs for both the test task only and all tasks.
