Table 1: Sentence accuracy on the training and test sets for the random, productivity and system-aticity splits of the PCFG dataset.
Table 2: Sentence accuracy achieved by the seq2seq and iterative decoding transformers with relative					attention (r = 8) on the training set and on	multiple test sets of the cartesian product dataset.							Iterative		Split	Seq2seq	short inputs row token		long inputs row token	train (up to 5 numbers/letters)	100%^^	100%	100%	100%	100%test (up to 5 numbers/letters)	97.8%	100%	100%	100%	100%test (6 numbers, 5 letters)	14.3%	89.2%	100%	100%	100%test (5 numbers, 6 letters)	12.2%	0%	99.5%	0%	100%test (6 numbers/letters)	1.1%^^	0%	98.7%	0%	100%To analyze the compositional generalization ability of seq2seq and iterative decoding transformerson the cartesian product dataset, we consider the following experimental setup. Both transformersare trained on samples with up to five numbers and letters. Then, they are tested on the four differenttest sets described in Section 2.3.2: up to five numbers and letters; six numbers and five letters; fivenumbers and six letters; and six numbers and letters. The second, third and fourth test sets can beseen productivity tests. Additionally, we only report results for transformers with relative attention(relative radius r = 8) as they were the best performing architecture in our experiments.
Table 3: Sentence accuracy achieved by the seq2seq and iterative decoding transformers with relativeattention (r = 8) on the training and test sets of the MCD1 split of the CFQ dataset.
Table 4: Vocabulary size, training and test samples, and number of training steps for all seq2seq anditerative decoding datasets.
