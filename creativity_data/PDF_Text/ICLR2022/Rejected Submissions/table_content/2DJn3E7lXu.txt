Table 1: The Kendall’s Tau correlation of Lookup Tables and Linear Regression (in brackets, usingonly 124 training samples) across metrics and devices. Lookup Tables perform only marginallybetter on the FPGA and Eyeriss devices, but considerably worse in all other cases. More detailedstatistics are available in Appendix E.
Table 2: Hyper-parameter ranges and default values of the configurable predictors17Under review as a conference paper at ICLR 2022D NAS-Bench-201 / HW-NAS-Bench cell designFigure 5: Basic NAS-Bench-201 / HW-NAS cell design. Each of the six orange paths is finalizedwith exactly one out of five candidate operations {Zero, Skip, Convolution 1× 1, Convolution 3×3,Average Pooling 3×3}.
Table 3: Kendall’s Tau test correlation for Linear Regression, XGBoost, and Lookup Table (LUT)on all HW-NAS-Bench datasets (rows), for different amounts of available training data (columns),tested on the remaining 625 samples. The Lookup Table model is tested on all 15625 architectures.
Table 4: Kendall’s Tau test correlation for Linear Regression and XGBoost on the five usedTransNAS datasets (rows), for different amounts of available training data (columns), tested onthe remaining 256 samples. The Lookup Table model (LUT) is tested on all 3256 architectures.
Table 5: P-values of different distributions, trying to fit the distribution of all predictor mistakesaccording to a t-test. Larger values are better, but comparing many empirically sampled points witha true density function tends to push the p-values to 0.
Table 6: How a trained XGB predictor deviates from the ground-truth values for different architec-ture subsets, akin to Figure 2. While they are not exactly the same, they still resemble the distributionover the entire test set (top plot, 625 samples). One noteworthy exception is when no Conv3x3 op-erations are used at all, in which case the standard deviation is considerably smaller.
