Table 1: Left: TV distance between recovered topic posterior and true topic posterior for differenttopic models. Right: Major topic(s) recovery rate for different topic models. The 95% confidenceinterval is reported in both tables.
Table 2: Left: TV distance between recovered topic posterior and true topic posterior of our self-supervised learning approach versus posterior inference via Markov Chain Monte Carlo assuming aspecific prior for α = 1. Right: Major topic recovery rate of our approach versus posterior inferencevia Markov Chain Monte Carlo assuming a specific prior for α = 1. In both the Left and Righttable, the 95% confidence interval is reported.
Table C.1: TV between our recovered topic posterior and the true topic posterior of our self-supervised learning approach versus topic inference via Markov Chain Monte Carlo assuming aspecific prior for α = 1, 3, 5, 7, 9. The 95% confidence interval is reported.
Table C.2: Major topic recovery rate of our approach versus posterior inference via Markov ChainMonte Carlo assuming a specific prior for α = 1, 3, 5, 7, 9. We report the 95% confidence interval.
Table C.3: Major topic recovery rate of our approach versus posterior inference via VariationalInference assuming a specific prior for α = 1, 3, 5, 7, 9. We report the 95% confidence interval.
Table C.4: Topic posterior recovery loss of our self-supervised learning approach, measured in TotalVariation distance, for all four types of documents and α = 1, 3, 5, 7, 9. The number of trainingepochs ranges from 25 to 200, and we sample 60K new training documents in every 2 epochs,which corresponds to 720K to 6M training documents.
Table C.5: Fully-connected neural network’s performance in the LDA α = 1 scenario.
Table C.6: Fully-connected neural network’s performance in the CTM α = 1 scenario with 4096hidden dimensions.
Table C.7: Attention-based neural network’s performance on CTM documents for α = 1, 3, 5.
Table C.8: 4-layer attention-base neural network’s performance on CTM documents when attentionlayer’s dimension varies, for α = 3, 5.
Table D.1: Test Accuracy for different representation extraction method. We fixed the rest of hyper-parameters to be: 5000 embedding dimension, 150 epochs, 0.0002 learning rate, sampling 4 wordsin labels, weight decay of 0.01 and resample rate of 2.
