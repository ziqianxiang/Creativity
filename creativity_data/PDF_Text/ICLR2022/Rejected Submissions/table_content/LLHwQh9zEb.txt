Table 1:	Results for reaction outcome prediction on USPTO_480k_mixed andUSPTO_STEREO_mixed. Best results for complete columns are highlighted in bold.
Table 2: Retrosynthesis results on USPTO_full. Templ.: reaction templates used; Map.: atom-mapping required; Aug.: output data augmentation used. Best results are highlighted in bold.
Table 3: Retrosynthesis results on USPTO_50k without reaction type. Templ.: reaction templatesused; Map.: atom-mapping required; Aug.: output data augmentation used. Best results for eachgroup of rows are highlighted in bold.
Table 4: Ablation study for Graph2SMILES on USPTO_50k.
Table 5: Atom and bond features.
Table 6: Results for reaction outcome prediction on USPTO_480k_separated for methods excludedin Section 4.3, sorted by top-1 accuracy. Best results are highlighted in bold.
Table 7: Statistics of USPTO datasets used.
Table 8: Hyperparameter setting used in the experiments for different datasets. Best settings selectedbased on validation are highlighted in bold if multiple values have been experimented.
Table 9: Published results on USPTO_50k without reaction type, that demonstrate the effectivenessof additional features or techniques for the Transformer model and variantsMethod	Top- 1	Top- 3	Top- 5	Top- 10EBM reranking using Transformer (Sun et al., 2020) on template-free proposal	53.6	70.7	74.6	77.0on proposal constrained by templates	55.2	74.6	80.5	86.9GTA, non-augmented (Seo et al., 2021) without cross-attention	46.8	65.2	70.5	74.9with cross-attention (using atom-mapping)	47.3	66.7	72.3	76.5DMP, with pretrained Transformer encoder (Zhu et al., 2021) Transformer baseline	42.3	61.9	67.5	72.9fusion with pretrained ChemBERTa encoder	43.9	62.2	68.0	73.1fusion with pretrained DMP encoder	46.1	65.2	70.4	74.3Augmented Transformer (Tetko et al., 2020) x20 augmentation for products only	42.5	-	-	-x20 augmentation for both products and reactants	48.0	-	-	-SCROP (Zheng et al., 2020) Transformer baseline	43.3	59.1	64.0	67.0reranked with syntax corrector	43.7	60.0	65.2	68.7Latent Transformer, non-augmented (Chen et al., 2020) no latent variable (N=1)	42.0	57.0	61.9	65.7with latent variable (N=2)	42.1	60.0	64.9	70.3For all groups of rows in Table 9, the first row of numbers are for the Transformer variants where fea-tures or techniques of interest are not used, and the rest of the rows otherwise. In turn, these resultsillustrate the effectiveness of using templates (EBM), atom-mapping (GTA), pretraining (DMP),output-side augmentation (Augmented Transformer), syntax-based reranking (SCROP) and varia-
Table 10: Graph2SMILES results on USPTO_50k without reaction type with latent variablesModel	Top-1	Top-3	Top-5	Top-10Graph2SMILES (D-GCN)				no latent variable (N=1)	52.9	66.5	70.0	72.9with latent variable (N=2)	52.0	70.2	75.2	79.5Graph2SMILES (D-GAT)				no latent variable (N=1)	51.2	66.3	70.4	73.9with latent variable (N=2)	50.3	68.8	73.7	77.7As an illustration of how Graph2SMILES can benefit from additional features or techniques, webriefly experiment with one of them, latent variable modeling similar to Chen et al. (2020) and Kimet al. (2021), using N = 2 latent classes with a uniform prior. As in Table 10, this noticeably increasesthe top-n accuracies for both D-GCN and D-GAT, which can be particularly relevant for multi-stepplanning.
