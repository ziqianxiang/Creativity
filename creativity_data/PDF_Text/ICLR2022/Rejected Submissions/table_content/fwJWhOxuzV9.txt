Table 1: Normalized scores for each dataset and each algorithm. PDT results are competative with other moresophisticated TD-learning methods that leverage the reward-free as well as the reward-annotated datasets. (i.e. CQL+BC)DomainDatasetNum trajectories BC DT PDT (ours) CQL CQL+BC		202		0.82	0.79 ± 0.1	1.05	1.02	medium-replay-v2	25	0.85	0.54	0.80 ± 0.06	0.80	0.93		10		0.13	0.55 ± 0.05	-0.09	0.82Halfcheetah		1000		0.94	0.92 ± 0.04.93	1.00	1.00	medium-v2	25	0.93	0.89	0.91 ± 0.04	0.92	0.96		10		0.84	0.93 ± 0.032	0.83	0.92		2000		0.74	0.75 ± 0.067	0.26	0.50	medium-eXPert-v2	25	0.63	0.41	0.75 ± 0.0769	0.27	0.42		10		0.33	0.63 ± 0.077	0.34	0.43		2039		0.50	0.34 ± 0.025	0.80	0.55	medium-replay-v2	25	0.24	0.20	0.17 ± 0.0230	0.03	0.12		10		0.21	0.02 ± 0.027	0.09	0.17Hopper		2187		0.54	0.55 ± 0.053	0.64	0.61	medium-v2	25	0.43	0.44	0.36 ± 0.08	0.54	0.57		10		0.40	0.56 ± 0.042	0.57	0.56
Table 2: Ablation on effect of reward masking during pre-trainingtask	number of trajectories	PDT (w/ masking) PDT (wo/ masking)		3214	0.59	0.47hopper-medium-expert-v2	25	0.45	0.45	10	0.48	0.38	2000	0.75	0.78halfcheetah-medium-expert-v2	25	0.75	0.71	10	0.63	0.59	2191	0.99	0.97Walker2d-medium-expert-v2	25	0.98	0.98	10	0.97	0.94fine-tuning with frozen self-attention layer and fine-tuning with both frozen self-attention and feed-forwardlayers. The later choice was proposed recently on Lu et al. (2021).
