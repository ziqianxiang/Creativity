Table 1: Success rates of different methods on Meta-world environments with K = 3. Each datapoint comes from the success rate of 20 tests.
Table 2: Rewards of different methods on four unseen tasks in Kitchen environment with K = 4.
Table 3: Success rates of different sub-skill number in5.3 Ablation StudiesIn this section we perform ablation stud-ies on different components of DMIL toprovide effects of different parts. Due tolimited space, we put ablations on fine-tuning steps, bi-level meta-learning pro-Meta-world environments.
Table 4: Success rates of DMIL-High, DMIL-Low, DMIL and OptionGAIL on Meta-world envi-ronments with K = 3. Each data point comes from the success rate of 20 tests.
Table 5: Ablations of sub-skill number K in Kitchen environments.
Table 6: Ablation studies of the fine-tuning steps in Meat-world experiments with K = 5.
Table 7: K = 10variants	ML10				ML45				Meta-training		Meta-testing		Meta-training		Meta-testing		1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shotDMIL	0.795	0.94	0.52	0.57	0.713	0.92	0.21	0.48DMIL_nc	0.788	0.96	0.32	0.56	0.703	0.927	0.17	0.35Gap	0.007	-0.02	0.2	0.01	0.01	-0.007	0.04	0.13D.6 Effects of Hard/Soft EM ChoicesIn DMIL, we use hard EM algorithm to train the high-level network. One may think about to usesoft cross entropy loss to train the high-level network to get better results. We perform this ablationstudy in the following table 8. We can see that a soft cross entropy training wonâ€™t help increase thewhole success rates. This may comes from that, usually we use soft cross entropy (such as labelsmoothing) to prevent over-fitting. However, in our situation, this may cause under-fitting, sincetraining on such a large scale of diverse manipulation tasks is already very difficult. Future workscan seek more comparisons about this choice.
Table 8: Ablation about hard/soft EM choices with K = 5 in the Meta-world environments.								variants	ML10				ML45				Meta-training		Meta-testing		Meta-training		Meta-testing		1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shotDMIL	0.795	0.94	0.52	0.57	0.713333	0.92	0.21	0.48DMIL-Soft	0.37	0.65	0.33	0.46	0.235556	0.43	0.1	0.32Gap	0.425	0.29	0.19	0.11	0.477778	0.49	0.11	0.16Figure 5: The training loss of the high-level network with a softmax shows a trend of rising first andthen falling.
Table 9: DMIL hyper-parameters.
Table 10: MIL hyper-parameters.
Table 11: MLSH hyper-parameters.
Table 12: PEMIRL hyper-parameters.
