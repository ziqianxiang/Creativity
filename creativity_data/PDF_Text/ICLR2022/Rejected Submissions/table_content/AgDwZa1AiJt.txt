Table 1: Performance of two-stage inference procedure onCIFAR-100. The student employs class-specific distillationwith |Lin | = L0 = 30, with in-domain referring to the in-stances from the classes in Lin . During inference we, use anappropriate class-based delegation method. See Fig. 2 for theidentity of the distillation approaches. Fraction denotes thefraction of test instances where the student model makes thefinal prediction. Unlike margin-based delegation, for giventeacher and student models, we obtain a single value of (Ac-curacy, Fraction) tuple with class-based delegation.
Table 2: (Lite) ResNet models for CIFAR-100. Table 3: (Lite) MobileNetV3 models for ImageNet.
Table 4: Performance of two-stage inference procedure on ImageNet-1k. The student model is ob-tained by a class-specific distillation approach with |Lin | = L0 = 300, with in-domain referring tothe instances belonging to the classes in Lin . CD-I and CD-III denote the class-based distillationapproaches defined in Sec. 4.1 and Sec. A.2. Baseline refers to the standard distillation from (3) witha = 0 and b = 1. The inference procedure employs an appropriate class-based delegation for eachdistillation approach. Fraction denotes the fraction of test instances where the student model makesthe final prediction. Note that, for standard distillation (Baseline), the student cannot delegate anyexamples to the teacher via class-based delegation.
