Table 1: Explanatory Learning vs Program Synthesis paradigm. Performance comparison of adata-driven vs ground-truth interpreter in a CRN. The last column shows the tag prediction accuracyof the learned I, when provided with the correct rule.
Table 2: Number of training epochs for each training regimen.
Table 3: T-Acc and NRS for different training regimens.
Table 4: Computational cost of our models at test time to tag s new structures. In Odeen r=24,794,b=32, s=1,176. Notice how CRNs offer a good balance between computational efficiency and per-formance, this trade-off is regulated by a single parameter, the number of beams.
Table 5: Computational cost of our models at test time to produce the textual rule in outputModel	Computational Cost	T(s)	R-AccExv src	r ∙ b ∙ I	47.8	0.99CRN [300B]	300 ∙CG + 300 ∙ b ∙I	0.72	0.77CRN [10B]	10 ∙CG + 10 ∙ b ∙I	0.35	0.35Emp [300b]	300∙ Emp-C	0.41	0.07Emp [10b]	10∙ Emp-C	0.10	0.07Emp [1b]	1∙ Emp-C	0.10	0.07D Odeen example gamesIn this section we propose a collection of qualitative results showing a series of Odeen games fromthe test set and how they are solved by the proposed models. For each model, we report the pre-dicted rule (if output by the model), the accuracy on the structures labeling (T-acc), and a mark thatindicates whether the nearest rule is the correct one (NRS). All the models are trained on 10000structures with 1438 rules.
