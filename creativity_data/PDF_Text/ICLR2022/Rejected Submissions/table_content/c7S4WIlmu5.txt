Table 1: BEIR Benchmark. We report nDCG@10 on the test sets of the datasets from the BEIRbenchmark for bi-encoder methods without re-ranker. We also report the average (“Average”) andnumber of datasets where a method is the best (“Best on”) over the entire BEIR benchmark (excludingthree datasets because of their licence). We evaluate retrievers after pre-training on unsupervised datawith our contrastive learning and finetuning on MS-MARCO (“Ours”). Bold is the best overall.
Table 3: MoCo vs. SimCLR. In this table, we report nDCG@10 on the BEIR benchmark forSimCLR and MoCo, after fine-tuning on the MS-MARCO dataset.
Table 4: Impact of data augmentions. nDCG@10 after fine-tuning on MS-MARCO.
Table 5: Training data. We report nDCG@10 after fine-tuning on MS-MARCO.
Table 6: Finetuning. We report nDCG@10 after fine-tuning BERT and our model on MS-MARCO.
Table 7: BEIR Benchmark. We report Recall@100 on the test sets of the datasets from the BEIRbenchmark for bi-encoder methods without re-ranker. We also report the average (“Avg.”) and numberof datasets where a method is the best (“Best on”) over the entire BEIR benchmark (excluding threedatasets because of their licence). We evaluate retrievers after pre-training on unsupervised data withour contrastive learning and finetuning on MS-MARCO (“Ours”). Bold is the best overall.
Table 8: Recall@k on the test sets of NaturalQuestions and TriviaQA. For Inverse Cloze Task andMasked Salient Spans we report the results of Sachan et al. (2021b). The Masked Salient Spansmodel uses annotated entity recognition data.
