Table 1: Results of GuacaMol distribution-learning benchmarks on QM9 and ZINC250K. KL Divrefers to KL Divergence and FCD refers to FreChet ChemNet Distance.
Table 2: Comparison of the top-3 property scores found by each modelMethod	Penalized logP ↑				QED ↑				1st	2nd	3rd	Validity	1st	2nd	3rd	ValidityZINC250K	4.52	4.30	4.23	100.0%	0.948	0.948	0.948	100.0%JT-VAE	5.30	4.93	4.49	100.0%	0.925	0.911	0.910	100.0%GCPN	7.98	7.85	7.80	100.0%	0.948	0.947	0.946	100.0%MRNN	8.63	6.08	4.73	100.0%	0.844	0.796	0.736	100.0%GraphAF	12.23	11.29	11.05	100.0%	0.948	0.948	0.947	100.0%GA3	12.25	12.22	12.20	100.0%	0.946	0.944	0.932	100.0%GP-VAE	13.95	13.83	13.65	100.0%	0.948	0.948	0.948	100.0%Constrained Property Optimization This task concentrates on improving the property scores ofgiven molecules with the constraint that the similarity between the original molecule and the modi-fied molecule is above a threshold δ. Following Jin et al. (2018); You et al. (2018); Shi et al. (2020),we choose 800 molecules with lowest Penalized logP in the test set of ZINC250K for optimizationand Tanimoto similarity with Morgan fingerprint (Rogers & Hahn, 2010) as the similarity metric.
Table 3: Comparison of the mean and standard deviation of improvement of each model on con-strained property optimization.
Table 4: Runtime cost for JT-VAE, GraphAF and our GP-VAE on the ZINC250K dataset. Inferencetime is measured with the generation of 10,000 molecules. Avg Step denotes the average number ofsteps each model requires to generate a molecule.
Table 5: The top-3 property scores found by GP-VAE without certain modules.
Table 6: Comparison of GP-VAE without certain modules on constrained property optimizationModel	δ=0.2		δ = 0.4		δ= 0.6		Improvement	Success	Improvement	Success	Improvement	SuccessGP-VAE	6.42±1.86	99.9%	4.19±1.30~~	98.9%	2.52±1.12	90.3%- piece	2.33±1.46	74.8%	2.12±1.36	50.9%	1.87±1.12	27.0%- two-step	3.36±1.58	98.6%	2.72±1.24	82.0%	1.88±1.05	45.1%8Under review as a conference paper at ICLR 20225	Analysis5.1	Graph Piece StatisticsWe compare the statistical characteristics of the vocabulary of JT-VAE that contains 780 substruc-tures and the vocabulary of graph pieces with a size of 100, 300, 500, and 700. Figure 5 shows theproportion of substructures with different numbers of atoms in the vocabulary and their frequenciesof occurrence in the ZINC250K dataset. The substructures in the vocabulary of JT-VAE mainlyconcentrate on 5 to 8 atoms with a sharp distribution. However, starting from substructures with 3atoms, the frequency of occurrence is already close to zero. Therefore, the majority of substructuresin the vocabulary of JT-VAE are actually not common substructures. On the contrary, the substruc-tures in the vocabulary of graph pieces have a relatively smooth distribution over 4 to 10 atoms.
Table 7: Parameters in the graph piece variational autoencoderModel Param	Description	Valueeatom	Dimension of embeddings of atoms.	50Common	epiece	Dimension of embeddings of pieces.	100epos	Dimension of embeddings of postions. The max position is set to be 50.	50EncoderdhdGdzDimension of the node representations hv	300The final representaion of graphs are projected to dG .	400Dimension of the latent variable.	56Number of iterations of GIN.	4Decoder	Hgru	Hidden size of GRU.	200Predictor dp	Dimension of the hidden layer of MLP.	20014Under review as a conference paper at ICLR 2022Table 8: Training hyperparametersParam	Description	Valuelr	Learning rate	0.001α	Weight for balancing reconstruction loss and predictor loss	0.1βinit	Initial weight of KL Divergence	0
Table 8: Training hyperparametersParam	Description	Valuelr	Learning rate	0.001α	Weight for balancing reconstruction loss and predictor loss	0.1βinit	Initial weight of KL Divergence	0βmax	Max weight of KL Divergence	0.01k lwarmup	The number of steps for one stage up in β	1000βstage	Increase of β every stage	0.002Property Optimization We use gradient ascending to search in the continuous space of latentvariable. For simplicity, we set a target score and optimize the mean square error between the scoregiven by the predictor and the target score just as in the training process. The optimization stopsif the mean square error does not drop for 3 iterations or it has been iterated to the maxstep. Wenormalize the Penalized logP in the training set to [0, 1] according to the statistics of ZINC250K.
