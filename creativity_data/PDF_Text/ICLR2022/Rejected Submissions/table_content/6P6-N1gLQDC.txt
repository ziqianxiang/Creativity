Table 1: Quality of Learned Interpretations. Ground Truth SCM (left;normal) versus NOTEARSfrom Zheng et al. (2018) (right;italics). The considered Why-questions can be found pronounced aswell as formal within Fig.2 respectively. Subtle differences between interpretations exist e.g., thelast interpretation is right on the top-level but for the wrong reasons (T → Z instead of T → R).
Table 2: ’Humans vs Algorithms’. Top: Edge plots per example where the bars denote the averagevalue of given relation and the errors confidence intervals. Bottom: The SCI generated for the twohuman variants (H1, H2 from Fig.4) against an algorithm representative (NT, Zheng et al. (2018)).
Table 3: Pronunciation Scheme. Right shows the natural language reading of a rule’s activation.
Table 4: More NIM-based SCI. We prove Thm.3 for general NIM while pointing to some examplemethods from the existing literature on NIM. Here we show the results of running the methodsMi, 1:NT (Zheng et al., 2018), 2:CGNN (Goudet et al., 2018), 3:DAG-GNN (Yu et al., 2019) onthe four data sets DeUtscher Wetterdienst (DW, Mooij et al. (2016)), Causal Health (CH, ZeceVicet al. (2021a)), Mileage (M), and Recovery (R, Charig et al. (1986)) for the respective queries. Assuggested, the methods are interpretable and reVeal insights onto the learned causal semantics, whileVarying signficantly in quality in terms of accuracy relatiVe to the ground truth (GT). Independentof accuracy, ”No causal interpretation . . . ” occur when the CEM estimate of Mi contains no causalpath to the queried variable X i.e., PaX = 0 (supported through GT sparsity). We also show GTinterpretations that require a negatiVe ’no answer’ response by Mi . The corresponding appendixsections cover a detailed elaboration for these NIM-based SCI (Thm.2).
