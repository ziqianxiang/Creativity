Table 1: I-PROVEN versus original PROVEN for probabilistic robustness certificate onMNIST and CIFAR-10 models. I-PROVEN achieve significant improvements (1.6×-8.5×)on the tightness of probabilistic robustness certificate without additional computation cost.
Table 2: Average interval lengths per layer	Layer 0	Layer 1	Layer 2	Layer 3	Layer 40 001	0 01	PROVEN	0.002	0.049	0.043	0.045	0.564= 0.001, Q = 0.01	I-PROVEN	0.002	0.011	0.009	0.009	0.2030 005 Q 0 01	PROVEN	0.010	0.247	0.324	0.522	7.478= 0.005, Q = 0.01	I-PROVEN	0.010	0.054	0.052	0.055	1.1460 001 Q 0 0001	PROVEN	0.002	0.049	0.043	0.045	0.588= 0.001, Q = 0.0001	I-PROVEN	0.002	0.013	0.011	0.011	0.2374.3	Comparison to other methodsIn the earlier section, we compared I-PROVEN to PROVEN in terms of their robustnesscertificates. Now, we will show comparisons to IBP and a simple Monte Carlo approachbased on (Anderson & Sojoudi, 2020). IBP does not generally perform well for arbitrarynetworks, so we use IBP-trained models in our experiment. In particular, we trained threeCIFAR-10 CNN models A, B, and C. Model A was trained with standard loss, Model B wastrained with an IBP loss term with ramping up to 2.2/255, and Model C was trained withan IBP loss term with ramping up to 8.8/255. We use two different , 2/255 and 8/255,for each model and certifier. Q is fixed at 1%. We apply these certifiers on 1000 images fromthe validation dataset.
Table 3: IBP, PROVEN, I-PROVEN, and the Monte Carlo method on a CIFAR-10 classifier.
