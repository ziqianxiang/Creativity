Table 1: Results on Classification Tasks from IndicGLUE BenchmarkModel	pa	hi	bn	or	as	gu	mr	avgWikipedia Section Title Prediction Independent Baseline mBERT (Dhamecha et al., 2021)	76.48	80.81	82.85	28.29	-	78.58	84.38	71.90XLM-R (Kakwani et al., 2020)	70.29	76.92	80.91	68.25	56.96	27.39	77.44	65.45IndicBERT base (Kakwani et al., 2020)	67.39	74.02	80.11	57.14	65.82	68.79	72.56	69.40IndicBERT large (Kakwani et al., 2020)	77.54	77.80	82.66	68.25	56.96	52.23	77.44	70.41Ours								Baseline	74.33±0.83	78.18±0.33	81.18±0.28	74.35±1.2	76.70±0.83	76.37±0.53	79.10±0.84	77.17XLM-Indic	77.55±0.61	82.24±0.18	84.38±0.29	81.47±0.99	81.74±0.82	82.39±0.27	82.74±0.52	81.78Test statistics								δ	3.22	4.06	3.20	7.12	5.04	6.02	3.64	4.61p-value	0.0004	0.0004	0.0004	0.0004	0.0004	0.0004	0.0004	-ρ	1	1	1		1	1	1	1	-News Category Classification Independent Baseline XLM-R (Kakwani et al., 2020)	94.87	-	98.29	97.07	-	96.15	96.67	96.61mBERT (Kakwani et al., 2020)	94.87	-	97.71	69.33	-	84.62	96.67	88.64IndicBERT base (Kakwani et al., 2020)	97.44	-	97.14	97.33	-	100.00	96.67	97.72IndicBERT large (Kakwani et al., 2020)	94.87	-	97.71	97.60	-	73.08	95.00	91.65Ours								Baseline	96.83±0.19	-	98.14±0.14	98.09±0.16	-	98.80±0.43	99.58±0.25	98.30XLM-Indic	97.90±0.17	-	97.99±0.22	98.77±0.12	-	99.40±0.54	99.47±0.21	98.70
Table 2: Results on Public DatasetsLanguage	Dataset	IndicBERT-base	Baseline		XLM-Indic		δ	p-value	Test Statistics			Accuracy	F1-score	Accuracy	F1-Score			ρ	Standardized effect size F1-scoreArticle Genre Classification									hi	BBC News	74.60	77.28±1.5	46.47±4.6	79.14±0.60	48.59±0.27	2.12	0.4363	0.62	0.19bn	Soham News Article Classification	78.45	93.22±0.49	91.12±0.85	93.89±48	91.75±0.73	0.63	0.05031	0.78	0.46gu	INLTK Headlines	92.91	90.41±0.69	88.82±0.73	90.73±0.75	89.14±0.88	0.32	0.5457	0.59	0.15mr	INLTK Headlines	94.30	92.21±0.23	89.23±0.54	92.04±0.47	89.09±0.78	-0.14	0.6665	0.43	0.10Sentiment Analysis									hi	IITP Product Reviews	71.32	76.33±0.84	74.04±0.99	77.18±0.77	74.53±0.98	0.49	0.3865	0.63	0.21hi	IITP Movie Reviews	59.03	65.91±2.2	65.26±2.2	66.34±0.16	65.87±1.6	0.61	0.5457	0.59	0.15Discourse Mode Classification									hi	MIDAS Discourse	78.44	78.39±0.33	47.26±6.2	78.54±0.91	63.80±3.2	16.54	0.00004	1	0.83orange indicates baseline and XLM-Indic are equal and blue indicates XLM-Indic is better6Under review as a conference paper at ICLR 2022Article Genre, Sentiment & Discourse Mode Classification: We evaluate the modelson publicly available sequence classification datasets. Most of these classification tasks arehighly skewed. Thus we report F1 scores in addition to accuracy on these tasks. Weencourage others to do so in the future. As stated earlier, apart from δ and ρ, we also
Table 3: Test accuracy on CSQAModel	pa	hi	bn	or	as	gu	mr	avgCloze-style QA (Zero Shot) Independent Baseline IndicBERT base (Our Evaluation)	29.33	30.76	28.45	30.38	29.98	81.50	29.71	37.16OursBaseline	31.04	36.72	35.19	34.63	33.92	59.86	36.14	38.21XLM-Indic	32.77	38.52	36.38	36.00	37.36	70.22	39.53	41.54δ	1.73	1.8	1.19	1.37	3.44	10.36	3.39	3.33orange indicates baseline and XLM-Indic are equal and blue indicates XLM-Indic is better than baseline4 Why Transliteration WorksIn this section, we analyze why XLM-Indic performs better than the baseline model fromtwo perspectives, tokenization quality, and CLRS.
Table 4: Hyperparameters for all tasksTask	TPU	Batch Size	Learning Rate	Weight Decay	Dropout	Epochs	Warmup RatioNews Category Classification	False	16	2e-5	0.01	0.1	20	0.10Wikipedia Section-Title Prediction	True	256	2e-5	0.01	0.1	3	0.10Named Entity Recognition	True	512	2e-5	0.01	0.1	20	0.10BBC Hindi News Classification	False	16	2e-5	0.01	0.1	20	0.10Soham Bangla News Classification	False	16	2e-5	0.01	0.1	8	0.10iNLTK Headlines Classification	False	256	2e-5	0.01	0.1	20	0.10IITP Movie Review	False	64	5e-5	0.01	0.25	20	0.10IITP Product Review	False	16	5e-5	0.01	0.5	20	0.10MIDAS Discourse Mode	False	32	2e-5	0.01	0.5	20	0.10A.5 Dataset DetailsHere we provide the corpus size details.
Table 5: Pretraining dataset detailsLanguage	Pretraining corpus size in GB	Language Sub-family	Scripthi	8.9	Central Indo-Aryan	Devanagaribn	5.8	Eastern Indo-Aryan	Bengali-Assamesemr	1.4	Southern Indo-Aryan	Devanagarine	1.2	Northern Indo-Aryan	Devanagarisi	0.783	Insular Indo-Aryan	Sinhalagu	0.705	Western Indo-Aryan	Gujaratipa	0.449	Northwestern Indo-Aryan	Gurmukhior	0.18	Eastern Indo-Aryan	Oriyaas	0.069	Eastern Indo-Aryan	Bengali-Assamesesa	0.036	Sanskrit	Devanagaribpy	0.0017	Eastern Indo-Aryan	Bengali-Assamesegom	0.0017	Southern Indo-Aryan	Devanagaribh	0.000034	Eastern Indo-Aryan	Devanagarimai	0.000011	Eastern Indo-Aryan	Devanagari15Under review as a conference paper at ICLR 2022A.6 Public datasets accuracy test statisticsThe MWU p-values and test statistics for public datasets accuracy is given in Table 6. We
Table 6: Public datasets test statistics of accuracyLanguage	Dataset	Test Statistics					δ	p-value	ρ	Standardized effect sizeArticle Genre Classification					hi	BBC News	1.86	0.0088	0.87	0.62bn	Soham News Article Classification	0.67	0.0090	0.57	0.62gu	INLTK Headlines	0.32	0.6249	0.57	0.12mr	INLTK Headlines	-0.17	0.3503	0.36	0.22Sentiment	Analysis				hi	IITP Product Reviews	0.85	0.04099	0.79	0.48hi	IITP Movie Reviews	0.15	0.8941	0.52	0.031Discourse Mode Classification					hi	MIDAS Discourse	0.15	0.7561	0.45	0.073orange indicates baseline and XLM-Indic are equal and blue indicates XLM-Indic is betterA.7 Case Study on Soham Bangla News Classification:In order to better understand the contrast between classification decision of XLM-Indic andbaseline model, using the Layer Integrated Gradients method (Sundararajan et al., 2017) asa local self-explaining method, we performed multiple out of distribution adversarial exam-ples to check the input features attribution difference between XLM-Indic and the baseline.
