Table 1: Ablation of results for the baseline model modified by removing different components toreveal their underlying contribution to the modelMethod	Trancos		Mall		Penguins		UCSD		MAE	R2	MAE	R2	MAE	R2	MAE	R2Lrank + Lgan	5.89	0.76	3.62	0.41	7.43	0.52	5.04	0.47L gan	13.19	-0.15	4.80	0.00	13.70	-0.08	7.70	-0.45Lrank	9.47	0.51	5.46	-0.14	7.72	0.58	7.04	-0.155.1.1	Sampling Image Ranking DataThere are presently no well-established image ranking datasets available for weakly supervised ob-ject counting and localization benchmarking. Given this, all image ranking datasets used for evaluat-ing our experiments must be curated. We experiment with the object counting and detection datasetsoutlined above, and reformulate all of the available datasets as ranking datasets as follows. Given acounting datasetDcount = {xi ∈ Rh,w,d, ci ∈ N}iN=c0,where d is the number of channels and ci is the object count in image xi , we randomly sampleN =	2, 000 image pairs	(xi, xj)	and calculate their pairwise ranking as	rij	=	ci	≥	cj.	Thisprovides us with a curated ranking dataset:Drank = {(xi , xj )n , (rij )n ≡ (ci ≥ cj )n}n=1 .
Table 2: Comparison of the test error and annotation time for state-of-the-art counting methods onthe TRANCOS crowd counting dataset. When methods use the standard dot-map training set, theirannotation times are equivalent.
Table 3: Comparison of the test error and annotation time for state-of-the-art counting methods onthe MALL crowd counting dataset.
Table 4: Comparison of the test error and annotation time for state-of-the-art counting methods onthe USCD crowd counting dataset.
