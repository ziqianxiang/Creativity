Table 1: ResNet-56 on CIFAR-10: Comparison between IMP and pruning stable methods for goalsparsity levels of 90%, 95% and 98%, denoted in the main columns. Each subcolumn denotes theTop-1 accuracy, the theoretical speedup and the actual sparsity achieved by the method. Each rowcorresponds to one method, where we denote the time needed when compared to regular trainingnext to the method’s name. All results include standard deviations where zero or near-zero valuesare omitted. The two highest accuracy values are highlighted for each sparsity level.
Table 2: Exact training configurations used throughout the experiments for IMP. For pruning sta-ble methods we use the same settings, with the following exceptions: (1) since momentum playsa crucial part in the justification of GSM, we tune it over 0.9, 0.99 and 0.995 and (2) any addi-tional hyperparameters of the respective methods as well as weight decay were tuned as indicatedin Subsection A.2. We note that others have reported an accuracy of around 80% for WRN28x10trained on CIFAR-100 that we were unable to replicate. The discrepancy is most likely due to aninconsistency in PyTorch’s dropout implementation.
Table 3: Overview of sparsification methods. CS, STR and DST control the sparsity implicitlyvia additional hyperparameters. IMP is the only method that is pruning instable by design, i.e.,it loses its performance right after the ultimate pruning. Further, IMP is the only method that issparsity agnostic throughout the regular training; the sparsity does not play a role while training toconvergence. All other methods require training an entire model when changing the goal sparsity.
Table 4: ResNet-56 on CIFAR-10: Results of the comparison between IMP and pruning stablemethods for the sparsity range between 90% and 99.5%. The columns are structured as follows:First the method is stated, where IMP+ denotes the unrestricted version of IMP. Secondly, we denotethe time needed when compared to regular training of a dense model, e.g. LC needs 1.14 times asmuch runtime as regular training. The following columns are substructured as follows: Each columncorresponds to one goal sparsity and each subcolumn denotes the Top-1 accuracy, the theoreticalspeedup and the actual sparsity reached. All results include standard deviations, where we omit zeroor close to zero results. Missing values (indicated by —) correspond to cases where we were unableto obtain results in the desired sparsity range, i.e., there did not exist a training configuration withaverage final sparsity within a .25% interval around the goal sparsity and the closest one is too faraway or belongs to another column.
Table 5: WideResNet on CIFAR-100: Results of the comparison between IMP and pruning stablemethods for the sparsity range between 90% and 99.5%. The columns are structured as follows: Firstthe method is stated, where IMP+ denotes the unrestricted version of IMP. Secondly, we denote thetime needed when compared to regular training of a dense model, e.g. LC needs 1.14 times as muchruntime as regular training. The following columns are substructured as follows: Each columncorresponds to one goal sparsity and each subcolumn denotes the Top-1 accuracy, the theoreticalspeedup and the actual sparsity reached. All results include standard deviations, where we omit zeroor close to zero results. Missing values (indicated by —) correspond to cases where we were unableto obtain results in the desired sparsity range, i.e., there did not exist a training configuration withaverage final sparsity within a .25% interval around the goal sparsity and the closest one is too faraway or belongs to another column.
