Table 1: 28 × 28 MNIST classificationModel	Error rateFC, 2 layers (256 hiddens each)	1.69%Convolutional networks, 2 layers	1.21%RAM, 6 glimpses, 8 X 8, 1 scale	1.12%RAM, 7 glimpses, 8 × 8, 1 scale	1.07%Our method (Entropy), 6 glimpses, 8 × 8, 1 scale	1.05%Our method (Entropy+Context), 6 glimpses, 8 × 8, 1 scale	1.01%7Under review as a conference paper at ICLR 2022Since the digits in MNIST are centered, there is no need to use top-down mechanism “where to look”to initialize the search. Thus, we only leverage the bottom-up strategy as RAM, but we introducenew constrains in Eqs. 11 and 12 and want to test whether these new features get performance gainor not. The evaluation result on the MNIST test dataset is shown in Table 1. Using 6 glimpses,patch size 8 × 8 and 1 scale (where m = 1), our method with entropy beats RAM under the sameparameter setting. It demonstrates that the entropy constraint in Eq. 12 is helpful. In addition,we tested whether the context constraint in Eq. 11 contributes or not, and it also shows that it canimprove the classification task with more gain.
Table 2: 100 × 100 Cluttered Translated MNIST classificationModel		Error rateFC, 2 layers (256 hiddens each)	57.3%Convolutional, 5 layers	49.5%RAM,1 glimpses, 8 X 8,1 scale	88.6%RAM,1 glimpses, 12 × 12, 1 scale	87.3%RAM, 6 glimpses, 8 × 8, 1 scale	79.3%RAM, 6 glimpses, 12 × 12,4 scales	29.8%RAM, 8 glimpses, 12 × 12,4 scales	24.7%Our method, 6 glimpses, 12 × 12, 4 scale	26.3%Our method (with Entropy constraint), 6 glimpses, 12 × 12, 4 scale	23.6%Our method (with EntroPy+Context), 6 glimpses, 12 × 12, 4 scale	20.3%Our method, 8 glimpses, 12 × 12, 4 scales	20.7%Our method (with Entropy+Context), 8 glimpses, 12 × 12, 4 scales	17.3%Sequential multi-digit recognition: since the core network is based on recurrent neural networks,we can easily extend it to handle multi-digit recognition Goodfellow et al. (2014); Ba et al. (2015).
Table 3: Whole sequence recognition error rates on 54 × 54 MNIST multi-digit SVHN datasetModel	Error rate11 layer CNN Goodfellow et al. (2014)	3.96%10 layer CNN Ba et al. (2015)	4.11%Single DRAM Ba et al. (2015), 18 glimpses, 12 × 12, 2 scales	5.1%Our method (Entropy), 18 glimpses, 12 × 12, 2 scales	4.58%Our method (Entropy+Context), 6 glimpses, 18 glimpses, 12 × 12, 2 scales	4.03%We take the same protocol as Goodfellow et al. (2014) on street view house number recognitiontask. Specifically, we crop 64 x 64 images with multi-digits at the center and then randomly sample8Under review as a conference paper at ICLR 2022Table 4: The recognition accuracy on CIFAR10 dataset with PGD attack.
Table 4: The recognition accuracy on CIFAR10 dataset with PGD attack.
