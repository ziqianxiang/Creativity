Table 1: Verified Error fraction on a subset (100 classes) of the ImageNet dataset. Results are on the held-outtest set of the dataset and correspond to training the classifier with the same during training. S.D. is over fourrandom seeds of training. 100 classes is the average value of 100 unit tests over all the 100 classes while minclass and max class respectively correspond to the minimum and maximum verified errors for unit tests overeach class. Alongside each we represent the % with respect to the nominal value it corresponds to (the nominalvalue of is 0.02, so = 0.005 is 25% of 2.0). Lower is better.
Table 2: Comparison of pixel-based variation for CROWN-IBP and AuditAI on 100 classes of the ImageNetdataset. We tabulate the values below relative to the nominal values of the corresponding latent codes (i.e.
Table 3: Verified Error fraction on the CheXpert dataset. Results are on the held-out test set of the dataset andcorrespond to training the classifier with the same during training. S.D. is over four random seeds of training.
Table 4: Domain shift study. Evaluation on the NIH Chest X-Rays dataset, after being trained on the CheXpertdataset. We perform certified training of the models with = 1.0 on CheXpert and deploy it on 5000 images ofa different dataset to quantify evaluation performance under potential domain shift. We tabulate the values ofprecision, recall, and overall accuracy. Higher is better.
Table 5: Verified Error fraction on the FFHQ and LSUN datasets. For FFHQ, the task is to classify whethereyeglasses are present, and the unit tests correspond to variations with respect to pose and expression of theface. For LSUN Tower, the task is to classify whether green vegetation is present on the tower, and the unit testscorrespond to variations with respect to clouds in the sky and brightness of the scene. Results are on the held-outtest set of the dataset and correspond to training the classifier with the same during training. S.D. is over fourrandom seeds of training. Lower is better.
Table 6: In this experiment, we pre-train the classifier through certified training with = 1.0 and performverification for different values of . Verified Error fraction on the FFHQ and LSUN datasets. For FFHQ, thetask is to classify whether eyeglasses are present, and the unit tests correspond to variations with respect to poseand expression of the face. For LSUN Tower, the task is to classify whether green vegetation is present on thetower, and the unit tests correspond to variations with respect to clouds in the sky and brightness of the scene.
