Table 1: The results of fusing and finetuning residual networks on CIFAR10 dataset. The resultsshown are mean ±std. deviation over 5 seeds. The detailed results are reported in Table 11.
Table 2: The results of heterogeneous model transfer from RESNET18 to RESNET34. Experimentsare run 5 times and details can be found in Table 12.
Table 3:	The results of fusing teacher and student VGG models on CIFAR10 dataset . More resultscan be found in Table 13.
Table 4:	Knowledge distillation from teacher MA into student model MB. Results in the last row aremean ±std. deviation over different temperatures. The detailed results are reported in Table 14.
Table 5: The hyperparameters for training different network architectures.
Table 6: The optimal CLA for different combinations of layer representation and cost function.
Table 7: Performance comparison between different combinations of mapping and balancing methods.
Table 8: Performance comparison between two layer balancing methods on skill transfer task.
Table 9: Finetuning RESNET hyperparame-ters.
Table 10: Finetuning teacher-student modelshyperparameters.
Table 11: The results of finetuning RESNET from different choices of initialization across 5 differentseeds.
Table 12: The results of model transfer from MB to MA across 5 different seeds.
Table 13: Finetuning teacher-student VGG across 5 different seeds. The results reported in Table 3are run with the random seed of 42.
Table 14: Distillation results for different temperatures. Each entity is the best accuracy obtained byvarying the loss-weight factor. The last two rows report the best and average performance across 5different temperatures.
