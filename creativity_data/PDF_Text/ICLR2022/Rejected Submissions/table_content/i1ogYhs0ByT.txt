Table 1: Test Accuracy (%) of Transformer-MGK compared with the baseline softmax transformer on the LRAbenchmark. Our Transform-MGKs outperform softmax transformers while using half the number of heads,having less parameters, and requiring less FLOPs (see Figure 3 for details). Results are averaged over 5 runs.
Table 2: Test Accuracy (%) of Transformer-MLK compared with the linear transformer on the LRA.OUrTransform-MLKs achieve comparable/better accuracy than the baselines while using half the number of heads,having less parameters, and requiring less FLOPs (see Figure 3 for details). Results are averaged over 5 runs.
Table 3: Perplexity (PPL) on WikiText-103 of Transformer-MGK and MLK compared to the baselines. BothTransformer-MGK and MLK achieve comparable or better PPL than the baselines while using only half thenumber of heads. When using the same number of heads, our models significantly improve the baselines.
Table 4: Performance of Transformer-MGK using different inference/learning techniques on LRA benchmark.
Table 5: Ablation study on the impact of the mixture of keys, the Gaussian distance, and the key shifting on theLRA retrieval task. We denote the softmax Transformer by Softmax. All Softmax models (i.e., Softmax 2 heads,Softmax 1 head, Softmax MGK 1 head, and Softmax sMGK 1 head) use dot product to compute the attentionscores. All Gaussian models (i.e., Gaussian MGK 1 head, Gaussian sMGK 1 head, and Gaussian 1 head) useGaussian distance to compute the attention scores. We denote MGK with key shifting by sMGK. Here MGK isused to denote our approach of using a mixture of keys at each timestep.
Table 6: Comparing the GPU memory footprint and computational time overhead (seconds/iteration) betweenour 4-head Transformer-MGKs/MLKs and the 8-head softmax/linear transformer baselines trained on the LRAretrieval task at test time. Our Transformer-MGKs/MLKs save much more memory and have significantlysmaller wall-clock time compared to the baselines. Here we use a batch size of 32 for each iteration.
Table 7: The learned miXing coefficient πjr of all heads and layers in the 1-head Transformer-MGKs trained onthe LRA retrieval task. Here we use the same πj1, πj2 for all time step j = 1, . . . , N.
Table 8: Test Accuracy (%) of 6-head Transformer-MGKs/MLKs compared with the baseline 12-head softmaxand linear transformers on the retrieval task. Our 6-head Transformer-MGKs/MLKs significantly outperformsoftmax and linear transformers, respectively, while being more efficient in terms of computational cost, modelsize, and memory usage.
Table 9: Machine translation BLEU scores of 2-head Transformer-MGKs on the IWSLT14 De-En dataset isbetter than or equivalent to that of the 4-head baseline.
