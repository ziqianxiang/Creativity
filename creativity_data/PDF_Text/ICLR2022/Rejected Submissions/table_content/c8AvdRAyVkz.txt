Table 1: Notation Table (Elaborated in Section 2)X is input y is label α is step size L is objective	∆x = ∂L∕∂X	∆θ = ∂L∕∂θθA(i)	model parameter θ trained for i epochs by method AfA(θ, X, y)	perturbation generated by method A to attack model θ on X, yωχ	perturbation initialization as parameterized by FGSM+ and APARTGBi (A)	perturbation strength of fa(θA)), calculated as its gap to fs (θA)) on model θA)Acc(θAI), fB (θCi2), ∙))	accuracy of θA1 when attacked by perturbations generated by fs (∙) for θCi2)(a) FGSM-Generated Perturbations.
Table 2: Model Performance of WideResNet34-10 on the CIFAR-10 dataset.
Table 3: Model Performance of Pre-ResNet18 on the CIFAR-10 dataset.
Table 4: Model Performance of Pre-ResNet18 on the CIFAR-100 dataset.
Table 5: Model Performance of ResNet50 on the ImageNet dataset. + indicates single precisiontraining.
Table 6: Ablation study of APART on		the CIFAR-10 dataset With Pre-ResNet20.		Training Methods		Clean Data	PGD-20 Attack	AAAPART		82.80%	51.30%	45.92%APART (W/o layer-Wise perturbation)		83.64%	47.28%	43.10%APART (W/o perturbation initialization)		82.42%	50.10%	45.10%Specifically, we summarize results on the CIFAR-10 in Table 2 and Table 3, CIFAR-100 in Table 4,and ImageNet in Table 5. Comparing to Free-8, S+FGSM, and F+FGSM, APART achieves consistentperformance improvements against PGD-20, AutoAttack, and C&W attack for both Pre-ResNet18and WideResNet34-10. As to ATTA-10 and PGD-10, APART achieves slightly worse performancewith 4+ times speedup. Among all methods, F+FGSM is the fastest method, and APART significantlyimproves the model robustness without significant computation overheads. For example, F+FGSMtakes 122 secs/epoch for training WideResNet34-10 on the CIFAR-10 dataset, and APART takes 162secs/epoch to achieve a 7.52 absolute accuracy improvement under the PGD-20 attack and a 4.78absolute accuracy improvement under the AutoAttack.
