Table 1: The statistics ρ(xeadv) and twoself-supervised tasks. The dataset is CI-FAR10 and the network is WideResNet-34-10. Adversarial budget ε = 0.031Tasks E(ρ(eadv)) P(ρ(Xadv) > 0)Rotation~~0.15	68.51%VFlip 0.22	72.16%Rotation	Vertical Flip-0.5	0.0	0.5	1.0	-1.0	-0.5	0.0	0.5	1.0p(×βdv)	p(×βdv)Figure 1: Empirical cdf of ρ(eadv) on CIFAR10 andWideResNet-34-10. Adversarial budget ε = 0.031Vertical Flip (VFlip) Prediction is a self-supervised task similar to rotation prediction and hasalso been used for self-supervised learning (Saito et al., 2020). In essence, we make two copies ofthe input image and flip one copy vertically. The head gvflip then contains a 2-way fully-connectedlayer followed by a softmax layer and predicts whether the image is vertically flipped or not. Thecorresponding loss for an image x isLvfliP(X) = ―5 X : I 2 * 4og(GvfliP(XV )v ) ,	(13)2 v∈Vwhere V = {fliPPed, not fliPPed} is the oPeration set and GvfliP = gvfliP ◦ E. Xv denotes and
Table 2: Robust test accuracy on CIFAR10 of the test-time fine-tuning on both the online settingand the offline setting. We use the WideResNet-34-10 with an '∞ budget ε = 0.031. AT meansadversarial training and FT stands for fine-tuning. We underline the accuracy of the strongest attackand highlight the highest accuracy among them.
Table 3: STL10 results of test-time fine-tuning. We use a ResNet18 with an '∞ budget ε = 0.031.
Table 4: Tiny ImageNet results of test-time fine-tuning. We use a ResNet18 with an '∞ budgetε = 0.031. We underline the accuracy of the strongest attack and highlight the highest accuracyamong them.
Table 5: Ablation study on the online test-timefine-tuning. The dataset is CIFAR10 and the taskis the “Rotation + VFlip”. All attacks are stan-dard attacks. Removing the LSS or LR results inlower robust accuracy than the full method. SAstands for Square Attack.
Table 6: Ablation study on the online test-timefine-tuning. The dataset is CIFAR10 and the taskis the “Rotation + VFlip”. All attacks are stan-dard attacks. SA stands for Square Attack.
Table 7: Accuracy on clean images. Networks are trained with corresponding meta adversarialtraining.
Table 8: Robust test accuracy on CIFAR10 of the online test-time fine-tuning. We use the sameWideReSNet-34-10 as in Table 2, which is trained with '∞ budget 0.031. The inputs are in the '∞ball of ε = 0.015. The self-supervised task is the ensemble of rotation and vertical flip.
Table 9: Experiments to rule out the possibility of label leaking. We use the WideResNet-34-10trained with '∞ budget ε = 0.031 and show the robust test accuracy on CIFAR10 of the onlinetest-time fine-tuning. The self-supervised task is the ensemble of rotation and vertical flip.
Table 10: Accuracy on transfer attack on CIFAR10.
Table 11: Accuracy on expectation attack on CIFAR10 with the ensemble of rotation and verticalflip task.
Table 12: Accuracy on RayS on CIFAR10 with the ensemble of rotation and vertical flip task.
Table 13: Average inference time for each instance using different methods.
