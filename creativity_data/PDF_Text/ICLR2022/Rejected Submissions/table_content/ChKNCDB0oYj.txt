Table 1: Results for our main configuration of mistake-driven image classification with progressiveSpinalNet, Wide-ResNet backbone and sharpness-aware minimization (WRN+SELU+SAM).
Table 2: Experiments performed for number of worst performing classes: We changed the nWPCto see its effect on the final accuracy.
Table 3: Experiments performed for number of samples to generate: We changed the generatedsamples to see its effect on the final accuracy.
Table 4: Backbone Models: We exchange Wide-ResNet-101 for EfficientNet-B7. Abbreviations:WRN - Wide-ResNet, ENB7 - EfficientNet B7Configuration	CIFAR-10	Caltech-101	Cars	Heritage	HAM10000WRN + SELU + Adam	98.56	98.10	88.85	95.65	93.81w/o Mistake-Driven Training	98.45	97.52	88.46	94.72	93.50ENB7 + SELU + Adam	98.56	95.23	88.83	94.15	93.42w/o Mistake-Driven Training	98.23	94.63	87.97	92.10	93.17Optimizer Our full setup includes Sharpness-Aware Minimization (SAM) to minimize the lossduring training. As an alternative baseline, we choose the Adam training technique (Kingma &Ba, 2014) instead. The results are shown in Table 5. Again, mistake-driven training and GAN-based augmentation of the worst-performing classes boost the final accuracy in all cases, exceptfor the Architectural Heritage dataset. Here, we observe a slight decrease of 0.35% in accuracyfrom mistake-driven training in combination with SAM. We also observe the benefit of using SAM,which by itself leads to better performance on all datasets compared to standard Adam. This isespecially visible for the Stanford Cars dataset, where using SAM improves the accuracy by 7.49%respectively 7.75% when mistake-driven training is also used.
Table 5: Optimizer: We exchange sharpness-aware minimization for standard Adam.
Table 6: Optimizer: We exchange the SELU activation function for GELU and RELU.
Table 7: Experimental results of mistake-driven image classification in different configurations.
