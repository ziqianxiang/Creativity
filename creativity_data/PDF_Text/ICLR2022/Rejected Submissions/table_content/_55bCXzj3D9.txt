Table 1: Comparing the number of trainable parameters in each fine-tuning method.
Table 2: Dataset - Projects used for customization3.2	RQ1: INTRINSIC EVALUATION METRICSRQ1 : Do custom models obtain better performances on intrinsic metrics, such as BLEU andperplexity, w.r.t. the baseline? To begin, we investigate how the different model customizationapproaches described in Sec. 2 score on intrinsic metrics such as BLEU and perplexity. All approachesentail fine-tuning the baseline model to the dataset of a specific project, with the choice of parametersbeing tuned depending on the approach taken. The four variants are trained independently until thebest validation loss is achieved. We report the BLEU4 score and the mean perplexity per token on thetest fold, for all the 20 projects. Next, we perform statistical tests to investigate whether the observeddifferences between the baseline and custom models are significant, as well as differences amongthe customization approaches. Specifically, we rely on the Kruskal-Wallis test, a non-parametricstatistical test.
Table 3: The BLEU score and perplexity for the customization methods evaluated on the 20 projectsin our test set.
Table 4: Kruskal-Wallis Test p-values testing the significance of the pairwise hypothesis that onecustomization method is superior than another. Custom strategies are significantly better than baseline.
