Table 1: Performance comparison (in %WER) onAMI evaluation sets using w2-BERT with randommasking (baseline) and with ATM using high, lowand mixed confidence scores from the scorer. TheFT is done with AMI.
Table 2: Cross analysis of ATM perfor-mance (in %WER) using AMI and LSscorers. The FT is done on LS-100 toevaluate the test and test-other, whilethe FT is done on AMI to evaluate us-ing IHM-eval and SDM-evalEvalset	LS-SCorer	AMI-sCorertest	3.89	3.93test-other	8.92	9.68IHM-eval	12.52	12.3SDM-eval	27.34	27.00by improving the %WER from 27.34 to 27.00. Surprisingly, our results on Table 2, shows that theresults on IHM-eval using an LS-scorer are comparable to the AMI-scorer. Evaluation on test andtest-other shows that LS-scorer is better than AMI-scorer on both sets. Based on these observa-tions, we choose the LS-scorer as the universal scoring model for all ATM based pre-trained modelsregardless of the target domain (eg: AMI) used in our experiments. Table 2 shows that althoughmatching the scorer to the target domain improves the performance, the difference is not significant.
Table 3: %WER of ATM+S by fine-tuning on AMI using w2v-BERT-L modelMSM arch.	TyPe	IHM-eval	SDM-evalw2v-BERT-L	Baseline	13.38	31.63	Baseline+S	13.14	28.16	ATM	12.52	27.34	ATM+S	13.05	27.19ate the value of utterance-level loss scaling by re-weighting utterances in the context of both base-line MSM (i.e., without ATM frame selection) and ATM (ATM+S). These results are in Table 3.
Table 4: %WER obtained by FT with LS-960 using w2v-BERT-XL model using baseline, ATMand ATM+S. The results show the impact of our proposed approach on matched condition sinceLibrispeech evaluation sets are treated as closer to Libri-light PT domain.
Table 5: %WER obtained by FT with AMI using w2v-BERT-XL model using baseline, ATM andATM+S. Evaluation is done on AMI test sets to highlight the effect on mismatched condition.
Table 6: Comparison with state-of-the-art results on SpeechStew. The FT is done on SpeechStewand the results are evaluated using Kaldi scoring to match published results. Note that the model hasnever seen any CHiME-6 data, and we use it as an example for zero-shot learning mode.
Table 7: %WER of ATM by varying the frames selection based on high, low and mixed confidence(conf.) scores using w2v-BERT-L on all evaluation sets on Librispeech.
Table 8: Performance comparison of different MSM architectures with and without applying ATMon all evaluation sets on Librispeech.
Table 9: %WER on Librispeech evaluation sets using ATM with utterance scaling and frame scaling.
Table 10: %WER on Commonvoice using models FT with speechstew.
Table 11: %WER on Commonvoice using models FT with speechstew.
