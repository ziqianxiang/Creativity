Table 1: Median test aCCUraCy over 3 seeds on the VTAB-1k benChmark Using pretrained (top)ResNet-50 and (bottom) ViT-B/16 baCkbones. RegUlarization baselines that Use all layers are indi-Cated with the +All prefix. ”*” indiCates resUlts obtained from Zhai et al. (2019). Fine-tUning resUltsfor ViT-B/16 are obtained Using Code and CheCkpoints provided by Dosovitskiy et al. (2021).
Table 2: Hyper parameters selected for the VTAB-1k benchmark tasks when using pretrainedResNet-50 backbone. F: fraction of features kept, LR: learning rate, Steps: Training Steps.
Table 3: Hyper parameters selected for the VTAB-1k benchmark tasks when using pretrained ViT-B/16 backbone. F: fraction of features kept, LR: learning rate, Steps: Training Steps.
Table 4: Relative cost of HEAD2TOE when compared with FINETUNING and LINEAR. F is thefraction of features kept, N is the total number of features and C is the number of classes. Onaverage Head2Toe requires 0.5% of the FLOPs required by FineTuning during the adaptation.
Table 5: Standard deviation of test aCCUraCy over 3 seeds on the VTAB-1k benChmark Using pre-trained (top) ResNet-50 and (bottom) ViT-B/16 backbones. The mean colUmn averages the standarddeviations for each dataset.
