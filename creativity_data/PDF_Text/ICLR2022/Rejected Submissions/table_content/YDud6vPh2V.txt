Table 1: Evaluated Learning Rates in the Object Collection EnvironmentParameter Description	Valuesα	Learning rate of the Q, ψ, and ξ-function	{0.0025, 0.005, 0.025}αw, αR	Learning rate of the reward weights or the reward model	{0.025, 0.05, 0.075}β	Learning rate of the One-Step SF Model	{0.2, 0.4, 0.6}tasks: QL ≈ 1h, SFQL (O) ≈ 4h, MFξ (O) ≈ 68h, MB ξ (O) ≈ 67h, SFQL ≈ 5h, MFξ ≈ 14h,and MB ξ ≈ 18h. Please note, the reported times do not represent well the computational complexityof the algorithms, as the algorithms were not optimized for speed, and some use different softwarepackages (numpy or pytorch) for their individual computations.
Table 2: Evaluated Learning Rates in the Racer EnvironmentParameter Description	Valuesa Learning rate of the Q, ψ, and ξ-function	{0.0025,0.005,0.025,0.5}αw Learning rate of the reward weights	{0.025, 0.05, 0.075}Computational Resources: Experiments were conducted on the same cluster as for the objectcollection environment experiments. The time for evaluating one repetition of a certain parametercombination over the 37 tasks depended on the algorithm: QL ≈ 9h, SFQL (O) ≈ 70h, SFQL ξ≈ 73h, and CMF ξ ≈ 88h. Please note, the reported times do not represent well the computationalcomplexity of the algorithms, as the algorithms were not optimized for speed, and some use differentsoftware packages (numpy or pytorch) for their individual computations.
