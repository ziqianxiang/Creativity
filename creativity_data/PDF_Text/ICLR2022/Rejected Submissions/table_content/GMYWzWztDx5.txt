Table 1: Searching for learning rates on our dataset results in higher values than reported in Brownet al. (2020), providing stronger baselines to compare to our NormFormer architecture.
Table 2: Zero-Shot Accuracy for Causal LMs for the following tasks: HS: HellaSwag, PI: PIQA,WG: WinoGrande, SC: StoryCloze, OB: OpenBookQA. PPL is validation perplexity during pretrain-ing. GPT-3 (paper) results taken from Brown et al. (2020). Horizontal lines group compute-matchedruns. High LR corresponds to using a larger learning rate than reported in Brown et al. (2020). λresidindicates whether residual scaling was used. λresid did not help at 1.3B scale, as shown in 2, butthat run is not compute matched so it is not included here. Model size (∣θ∣)is reported in millions ofparameters.
Table 3: Masked LM: Pretraining validation perplexity (PPL) and fine-tuned performance on GLUEtasks for Pre-LN and NormFormer models. Note that models are trained for an equal amount ofcompute, which is less than the publicly-released roberta-base models.
Table 4: 125M parameter Language Modeling Validation perplexities after 470 V100 Hours ofpretraining. Removing any of our proposed additions degrades performance (Rows 2-5). Addingmore normalization inside the Multi Headed Attention (Row 6) does not impact perplexity at a fixednumber of updates, but reduces throughput such that the model can only complete 87,500 updates vs.
Table 5: Longer Warmup: increase LR Warmup to 6,000 steps (from 500). GPT3: increase sequencelength to 2048, increase dropout to 0.1, increase training budget to 1,000 V100 hours. Grad Clip:clip gradient norms at 0.1. NormFormer outperforms the baseline in all settings.
Table 6: Wikitext 103 results following Baevski & Auli (2019). Steps to Target PPL: atwhat percentage of the 280K steps did the model reach 18.70 perplexity. Final PPL: Best Per-plexity. A100 Hours Cost of reaching Target PPL.
Table 7: Hyperparameters for ablations in Tables 4 and 7. This train budget allows the baselinemodel to run for 100,000 updates.
Table 8: Stability and Performance for different architectures for 1.3B parameter CLMs, see Sec-tion 9.1 for details.
