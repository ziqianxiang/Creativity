Table 1: Performance of DML model evaluated on adversarial samples generated by different objec-tives. The threatened model is a pretrained Triplet-D model. Adversarial samples are generated fromtriplet, alignment or uniformity loss. Lower score indicates better quality of adversarial samples.
Table 2: Performance of metric learning models on clustering and retrieval tasks averaged over 5runs on CUB200-2011 and CARS196. Our Adversarial DML models, ADML+A, and ADML+U,outperform the rest models. The model settings and training parameters are same for all models.
Table 3: Performance of DML models on clustering and retrieval tasks averaged over 5 runs onOnline-products and In-shop. Our Adversarial DML models, ADML+A, and ADML+U, outperformthe rest models in most cases. The model settings and training parameters are same for all models.
Table 4: Robustness performance of DML models and ADML models against adversarial samplesgenerated by attacking alignment loss.
Table 5: Robustness with different adversarial training strength λ on CUB200-2011 under the align-ment attacks. The metric is Recall@1.
Table 6: Recall@1 of different metric learning losses with ADML methods on CUB200-2011	Vanilla	ADML+T	ADML+A	ADML+U	ADML+A+UTriplet	62.29	63.68	65H~~	64.72	63.17Margin	62.48	64.26	65.92^^	65.61	64.32Multi-similarity	62.71	64.45	-^66Γ3^^	65.58	64.39Info-NCE	61.42	62.74	64.01	63.85	62.91A mixture of alignment and uniformity attacks. In this experiment, we study the effect of differentweight of alignment and uniformity objectives in ADML+A+U. The backbone is DML with multi-similarity loss. The model settings are the same as in Sec. 5.3. Specifically, we assign weight (1 -β)to the alignment objective and β to the uniformity objective. The results are shown in Table 7. Whenβ increases, the performance of the ADML model first decreases and then increases, which indicatesthat attacking alignment and uniformity loss separately can lead to better results.
Table 7: Recan@1 of ADML+(1 - β) A+β U on CUB200-2011 With different β.
Table 8: Comparison of DML models trained with the positive metric losses on CUB200-2011 andCARS196. We calculate the average distance (Avgdist), average distance of positive pairs (Avgdist-Pos), and average distance of negative pairs (AvgdistNeg) with the embedded samples.
Table 9: Comparison of DML models trained with the negative metric losses and uniformity regu-larization on CUB200-2011 and CARS196. All negative metric losses except Linear achieve com-parable performance to uniformity regularization. The details of models and training settings are inSec. C.2._____________________________________________________________________________________________CUB200-2011				CARS196			Avgdist	HE(s=0)	G-HE(s=1)	Avgdist	HE(s=0)	G-HE(s=1)ImageNet	0.7220	0.3368	0.5939	0.6557	0.4326	0.6497Linear	1.2398	-0.1670	0.2535	1.3194	-0.1686	0.2449Triplet	1.3877	-0.3254	0.1496	1.4039	-0.3376	0.1420Margin	1.3929	-0.3297	0.1465	1.4016	-0.3363	0.1426MS	1.3825	-0.3221	0.1509	1.3985	-0.3338	0.1441HE regularization (s=0)	1.4001	-0.3347	0TT438	1.4064	-0.3395	0.1411HE regularization (s=1)	1.4009	-0.3355	0.1432	1.4068	-0.3398	0.1407G-HE regularization (s=1)	1.4028	-0.3369	0,1424	1,4077	-0,3406	0,14020.0	0.5	1.0	1.5	2.0distance(a)	ImageNet0.0	0.5	1.0	1.5	2.0distance(b)	G-HE (s=1)
