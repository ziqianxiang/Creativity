Table 1: The structure of the networkLayer	Input shape	Output shape	Kernel	Activation	Dilation rateDilated conv	(m × h × d)	(m × h × 8)	(1 × 3)	Relu	1Dilated conv	(m × h × 8)	(m × h × 8)	(1 × 3)	Relu	1Corr layer	(m × h × 8)	(m × h × 1)	([m+1] × 1)	Relu	-Dilated conv	(m × h × 9)	(m × h × 16)	(1 × 3)	Relu	2Dilated conv	(m × h × 16)	(m × h × 16)	(1 × 3)	Relu	2Corr layer	(m × h × 16)	(m × h × 1)	([m+1] × 1)	Relu	-Dilated conv	(m × h × 17)	(m × h × 16)	(1 × 3)	Relu	4Dilated conv	(m × h × 16)	(m × h × 16)	(1 × 3)	Relu	4Corr layer	(m × h × 16)	(m × h × 1)	([m+1] × 1)	Relu	-Causal conv	(m × h × 17)	(m × 1 × 16)	(1 × [h - 28])	Relu	-1 × 1 conv	(m × 1 × 17)	(m × 1 × 1)	(1 × 1)	Softmax	-Finally, it is necessary to discuss some connections with the recent work of Zhang et al. (2020), wherethe authors propose an architecture that also takes both sequential and cross-asset dependency intoconsideration. Their proposed architecture, from a high level perspective, is more complex than oursin that theirs involves two sub-networks, one LSTM and one CNN, whereas ours is built solely onCNN. Our architecture is thus simpler to implement, less susceptible to overfitting, and allows formore efficient computation. The most noticeable difference between their design and ours is at thelevel of the Corr layer block, where they use a convolution with a m × 1 kernel to extract dependency
Table 2: The average (and standard deviation) performances using three data sets.
Table 3: The average (and standard dev.) performances over random asset permutation in Can-data.
Table 4: The average (and std. dev.) performances as a function of the number of assets in Can-data.
Table 5: The average (and std. dev.) performances as a function of commission rate (CR) in Can-data.
