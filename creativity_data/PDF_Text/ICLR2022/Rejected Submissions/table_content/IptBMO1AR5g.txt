Table 1: ResNet-18 on CIFAR-10		Model	Test AccuracyBaseline with Weight-Decay	94.00 ± 0.47Jacobian	89.23 ± 1.02DropBlock	89.23 ± 0.44Sanker’s Method for Full Network	88.05 ± 0.22Sanker’s Method for Middle Network	88.13 ± 0.12Confidence Penalty	94.01 ± 0.40Label Smoothing	94.26 ± 0.26SEHT-D (maxIter=1, prob=0.01)	94.35 ± 0.18SEHT-D (maxIter=1, prob=0.05)	94.37 ± 0.27SEHT-D (maxIter=1, prob=0.0l) + Label Smoothing	94.38 ± 0.24cutout	94.02 ± 0.22mixup	95.39 ± 0.13SEHT-D (maxIter=1, prob=0.01) + mixup	95.45 ± 0.064.1.2	CIFAR- 1 00CIFAR-100 dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each.
Table 2: WRN-28-10 on CIFAR-100Model	Top-1 Accuracy	Top-5 AccuracyBaseline with Weight-Decay Confidence Penalty Label Smoothing SEHT-D(maxIter=1, prob=0.01)	73.79 ± 2.68 73.46 ± 1.21 74.15 ± 0.92 74.92 ± 0.77	92.01 ± 1.32 92.16 ± 0.58 90.40 ± 0.73 92.66 ± 0.54Confidence Penalty + Dropout Label Smoothing + Dropout SEHT-D(maxIter=1, prob=0.01) + Dropout	74.80 ± 0.91 72.89 ± 1.57 77.75 ± 0.37	93.09 ± 0.51 90.43 ± 0.97 94.38 ± 0.09cutout SEHT-D(maxIter=1, prob=0.01) + cutout mixup SEHT-D(maxIter=1, prob=0.01) + mixup	76.70 ± 0.79 77.09 ± 0.37 78.38 ± 0.31 78.59 ± 0.46	93.72 ± 0.40 93.98 ± 0.21 94.37 ± 0.31 94.46 ± 0.30Our experiments on Image Classification shows that our Hessian regularization method outperformsother regularization methods and can be efficiently combined with data augmentation methods.
Table 3: LSTM on Wiki-Text2Model	Validation Perplexity	Test PerplexityBaseline with Dropout	101.82 ± 0.32	95.65 ± 0.19Confidence Penalty	101.39 ± 0.32	95.57 ± 0.11Label Smoothing	99.58 ± 0.11	95.03 ± 0.58SEHT-D	100.69 ± 0.53	94.86 ± 0.508Under review as a conference paper at ICLR 2022We also tested with a 2-layer GRU (Cho et al., 2014). The size of word embeddings is 512 andthe number of hidden units per layer is 512. We run every algorithm for 40 epochs, with batchsize 20, gradient clipping 0.25 and Dropout ratio 0.3. We perform a grid search over Dropoutratios {0, 0.1, 0.2, 0.3, 0.4, 0.5} and find 0.3 to work best. We tune the initial learning rate from{0.001, 0.01, 0.1, 0.5, 1, 10, 20, 40} and decrease the learning rate by factor of 4 when the vali-dation error saturates. We find initial learning rate 20 works best, same as LSTM. Parametersare initialized from a uniform distribution [-0.1, 0.1]. For label smoothing, we perform a gridsearch over weight values {0.001, 0.005, 0.01, 0.05, 0.1} and find 0.05 to work best. For the confi-dence penalty, we perform a grid search over weight values {0.001, 0.005, 0.01, 0.05, 0.1} and find0.005 to work best. For our Hessian regularization, we perform a grid search over weight values{0.001, 0.005, 0.01, 0.05, 0.1}. Weight 0.001 works best. We set probability values 0.01. Averagesand 95% confidence intervals are estimated over 5 distinct runs.
Table 4: GRU on Wiki-Text2Model	Validation Perplexity	Test PerplexityBaseline with Dropout	119.04 ± 4.67	111.64 ± 3.67Confidence Penalty	116.40 ± 0.17	109.27 ± 0.05Label Smoothing	117.47 ± 0.48	110.46 ± 0.87SEHT-D	116.21 ± 0.60	109.03 ± 0.30Our experiments on Language Modelling demonstrate that all these three regularization methodscan improve models, while our SEHT-D is the best.
