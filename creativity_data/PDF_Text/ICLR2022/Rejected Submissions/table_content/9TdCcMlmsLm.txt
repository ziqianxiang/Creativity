Table 1:	Language perplexity results averagedacross topics. The lower, the more fluent the gen-erated continuation sentences.
Table 2:	Average sentence generation time cost.
Table 3: BLEU results on the E2E val/test sets.
Table 5: Beam search results on entailment generation, in the format val/test. â†‘4 indicateshigher/lower is better. *SQL (single) achieves zero in H1/H2 as it generates a single token.
Table 6: Prompt generation results. Note that some of the numbers from GeDi are low because thetopics are tokenized into two subword tokens, which the model was not trained with.
Table 7:	Entailment generation samples from SQL (beam search, validation dataset).
Table 8:	Prompt samples from SQL.
Table 9:	Prompt samples from MLE+PG.
Table 10:	Prompt samples from GeDi.
Table 11:	Prompt samples from PPLM.
