Table 1: Summary of findings in the ablation anal-ysis of distillation’s subgroup performance (§3). Ina range of different settings, distillation is seen tohurt the hardest subgroup accuracy (worst-k classaccuracy or worst subgroup according to an at-tribute), despite improving the average accuracy(upper rows). Decreasing number of labels helpsimprove the hardest classes (bottom row).
Table 2: Summary of effect of distillation on different teacher and student architectures consideredfor the ImageNet dataset. The comparison is with respect to the one-hot (i.e., non-distilled) student.
Table 3: Summary of student’s average accuracy using one-hot and distilled labels. Worst k denotesaccuracy over the worst k classes. Global and adaptive temperatures αy selected using a held out devset. The proposed AdaAlpha technique improves both mean and worst class accuracy over vanilladistillation. For AdaMargin on CIFAR-100 LT and ImageNet LT, we observed divergence duringtraining, presumably due to this method being sensitive to the selection of hyperparameters, which inturn are estimated on very small number of examples per class.
Table 4:	Ablations of design choices in the proposed methods: 1) remove distillation signal from thebottom 10% of classes, according to confidence; 2) randomly shuffle per-class α values; 3) weightdistillation based on StUdent and teacher confidence ZhoU et al. (2021).
Table 5: ResNet Architecture configurations used in our experiments (He et al., 2016). [*] Note thatImageNet ResNet-50 uses larger blocks with 3 convolutional layers per residual block compared toResNet-18 and 34. We refer to He et al. (2016) for more details.
Table 6: Self-distillation experiments (from Resnet-50 to Resnet-50) on the iNaturalist dataset VanHorn et al. (2018) with student’s average accuracy using one-hot and distilled labels. Worst 20denotes accuracy averaged over worst 20 classes ∆20 denotes the difference between the meanaccuracy and the worst 20 classified classes. The proposed AdaMargin technique improves mean andworst-class accuracy over both one-hot training and standard distillation.
Table 7: Difference between distillation and one-hot performance on Adult dataset. Here, subgroupsare defined by the sex and label. ∆ refers to the difference between the distilled and one-hot student’saccuracy on the subgroup.
Table 8: Difference between distillation and one-hot performance on Adult dataset. Here, subgroupsare defined by the sex, race, and label. ∆ refers to the difference between the distilled and one-hotstudent’s accuracy on the subgroup.
Table 9: Results of repeated distillation on CIFAR-100 LT. Using a distilled student as teacher for asubsequent round of distillation is seen to further hurt worst-class accuracy.
