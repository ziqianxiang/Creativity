Table 1: CIFAR10-4k: Results for 4k labeled images and an unlabeled dataset containing 41kCIFAR10 training and 1 million respectively 10 million unlabeled images from 80MTI or 1MLSUN images. MixMatch (MM) and DS3L perform worse than the “plain” baseline (shown inred). ODST+ and ODST outperform all methods in terms of accuracy. For out-of-distribution de-tection, we report the average false positive rate (FPR) at 95% TPR. ODST+ has a FPR more than30% better than the closest competitors FixMatch (FM) and MTCF.
Table 2: CIFAR10-50k: We show test accuracy and FPR@95TPR as out-of-distribution detectionperformance. ODST+ has the best improvement (1.26%) and final test error (1.93%) for Resnet50and is the only method which improves for the Pyramid272 architecture by 0.29% with 1.43% testerror whereas NSST and NSST+ degrade in test performance.
Table 3: CIFAR100-45k: ODST (+) have the best accuracy for both model architectures. SSL-results in red are worse than using labeled data only.
Table 4: CIFAR10-4k + 1M TI: Breakdown of the individual iterations of self-training schemesin the CIFAR10-4k setting with an unlabeled pool diluted with 1M samples from 80MTI. The finalmodel is chosen based on validation accuracy and marked in black.
Table 5: CIFAR10-4k + 10M TI: Breakdown of the individual iterations of self-training schemesin the CIFAR10-4k setting with an unlabeled pool diluted with 10M samples from 80MTI. The finalmodel is chosen based on validation accuracy and marked in black.
Table 6: CIFAR10-4k + 1M LSUN: Breakdown of the individual iterations of self-training schemesin the CIFAR10-4k setting with an unlabeled pool diluted with 1M samples from LSUN. The finalmodel is chosen based on validation accuracy and marked in black.
Table 7: CIFAR10-50k: Additionally to the results from the main paper, we include robustness tocommon corruptions evaluated on CIFAR10-C (C10-C). The last row ”Iteration” refers to which ofthe 3 self-training iterations is chosen based on validation set accuracy.
Table 8: CIFAR100-45k: Additionally to the results from the main paper, we include robustness tocommon corruptions evaluated on CIFAR100-C (C100-C). The last row ”Iteration” refers to whichof the 3 self-training iterations is chosen based on validation set accuracy.
Table 9: Ablation: Influence of the size of the OOD-validation set on test accuracy and out-of-distribution detection performance. ODST does not use any additional OOD-dataset and for ODST+we use 2k and 50k additional OOD samples from CIFAR100.
Table 10: Ablation: Comparison of different choices for the labels of non-selected images in U \ I.
Table 11: Ablation: We compare our ODST+ method to a model trained with the 500k-Ti datasetfrom Carmon et al. (2019). 500k-Ti was created by sampling 50k images per class from 80MTIwith the goal of improving adversarial robustness. However, we show that a ResNet50 classifiertrained with the same parameters is not able to match the performance of ODST+ in terms of testset accuracy, corruption robustness or out-of-distribution detection.
