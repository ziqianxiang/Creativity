Table 1: Comparison of policy improvement step for existing approaches versus DiME. Adjustments to thesingle-objective setting to accommodate multiple objectives are highlighted in red.
Table 2: Offline RL results for state-of-the-art approaches (left columns), and our baselines and DiME witheither a single trade-off across all tasks (middle) or the best trade-off per task (right). Tasks are from RLUnplugged (above) and D4RL (below). For DiME multi we include standard error bounds across ten randomseeds. Numbers within 5% of the best score are in bold. DiME performs on par or better than LS across alltasks. Even when the trade-off is fixed across all tasks (for a more fair comparison with prior work), DiME stilloutperforms state-of-the-art on the RL Unplugged tasks. For RL Unplugged, the results for BRAC are fromGulcehre et al. (2020); CRR is from Wang et al. (2020), referred to as “CRR exp” in their paper; and MZU(MuZero Unplugged) is from Schrittwieser et al. (2021). For D4RL, the results for the italicized algorithms arefrom Fu et al. (2020). fNote that the CRR results are for the best checkpoint from training, whereas all otherresults denote performance after a fixed number of learning steps. Per-task results for D4RL are in Appendix D.
Table 3: Default hyperparameters for all approaches, with decoupled KL-constraint on mean and covariance ofthe policy M-step.
Table 4: Hyperparameters that are either setting-specific or differ from the defaults in Table 3.
Table 5: Results for LS and DiME (average cumulative reward) obtained for the best tradeoff per algorithm,on tasks from D4RL. The results for the italicized algorithms are taken from Fu et al. (2020); these are thebest-performing algorithms from their comprehensive evaluation. Numbers within 5% of the best score are inbold.
