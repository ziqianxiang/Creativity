Table 1: The implemented FL algorithms for comparisonAlgorithm	Aggregation method	Rule of selectionFEDAVG (McMahan et al., 2017) CFCFM (Wu et al., 2021b) FEDAVG-RP (Li et al., 2019) FEDPROX (Li et al., 2020) FEDADAM (Leroy et al., 2019) AFL (Goetz et al., 2019) FedProf (ours)	full aggregation full aggregation partial (Scheme II) partial aggregation partial with momentum partial with momentum full/partial aggregation	random selection submission order random selection weighted random by data ratio random selection local loss valuation weighted random by score5.1	Experiment SetupWe built our simulated FL system and implemented the algorithms based on the Pytorch framework(Build 1.7.0). We first set up a Small-scale Task (S-Task) to learn a multi-layer feed-forward networkmodel from decentralized sensor data for predicting carbon monoxide (CO) and nitrogen oxides(NOx) emissions using the GasTurbine3 dataset. Next, we set up a Large-scale Task (L-Task) totrain a CNN over a large population of user devices for image classification (EMNIST4). In bothtasks, data sharing is not allowed between any parties. The data are non-IID across the end devices.5We introduce a diversity of noise into the local datasets on end devices to simulate the discrepancyin data quality. The S-Task is performed over 50 sensor clients and 50% of them produce noisy data(including 10% invalid). The population for L-Task contains 1000 end devices across which the dataspread with strong class imbalance - roughly 60% of the samples on each device fall into the sameclass. 15% of the local datasets in the L-Task are irrelevant images whereas another 45% of themare low-quality images (blurred or affected by salt-and-pepper noise). Considering the populationof the clients, the setting of the selection fraction C is based on the scale of the training participantssuggested by Kairouz et al. (2019). For both tasks, the clients are heterogeneous in terms of bothperformance and communication bandwidth. More experimental settings are listed in Table 4 inAppendix E where the environment setup is also given in details.
Table 2: The results of running the S-Task. The best accuracy is achieved by running for longenough. Other metrics are recorded upon reaching the target accuracy (80% for the S-Task).
Table 3: The results of running the L-Task. The best accuracy is achieved by running for longenough. Other metrics are recorded upon reaching the target accuracy (90% for the L-Task).
Table 4: Experimental setup.
