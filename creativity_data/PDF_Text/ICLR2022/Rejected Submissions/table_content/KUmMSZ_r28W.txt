Table 1: Results for D4RL benchmarks. Each number is the normalized score computed as (score -random policy score) / (expert policy score - random policy score) of the policy at the last iteration oftraining, averaged over 5 random seeds, ± standard deviation. Results of MOPO (Yu et al., 2020),BEAR (Wu et al., 2019) and CQL (Kumar et al., 2020) are reported from their respective papers.
Table 3:	Policys ∈ Rs,a ∈ RaDense layer 256, ReLUDense layer 256, ReLUDense layer 256, ReLUdense → 1Table 4:	(Double)Q Functions ∈ Rs,a ∈ RaAffine TransformationDense layer 256, ReLUDense layer 256, ReLUDense layer 256, LinearTable 5:	DiscriminatorTable 6: Hyper-parameter Settings of ParPIParameter	Policy	Q-function	Discriminatoroptimizer	Adam	Adam	AdamLearning rate	3∙10-4	3∙10-4	3∙10-4discount(γ)	0.99	n/a	n/areplay buffer size	106	n/a	n/aentropy target	—dim(A)	n/a	n/a
Table 4:	(Double)Q Functions ∈ Rs,a ∈ RaAffine TransformationDense layer 256, ReLUDense layer 256, ReLUDense layer 256, LinearTable 5:	DiscriminatorTable 6: Hyper-parameter Settings of ParPIParameter	Policy	Q-function	Discriminatoroptimizer	Adam	Adam	AdamLearning rate	3∙10-4	3∙10-4	3∙10-4discount(γ)	0.99	n/a	n/areplay buffer size	106	n/a	n/aentropy target	—dim(A)	n/a	n/aMCMC steps	n/a	n/a	5MCMC step size	n/a	n/a	3 ∙10-4gradient steps	1	1	1target update interval	1	1	1H Offline Algorithm DetailsFor completeness, we list the detailed algorithms for ParPI +MOPO in Algorithm 2. We use rollout
Table 5:	DiscriminatorTable 6: Hyper-parameter Settings of ParPIParameter	Policy	Q-function	Discriminatoroptimizer	Adam	Adam	AdamLearning rate	3∙10-4	3∙10-4	3∙10-4discount(γ)	0.99	n/a	n/areplay buffer size	106	n/a	n/aentropy target	—dim(A)	n/a	n/aMCMC steps	n/a	n/a	5MCMC step size	n/a	n/a	3 ∙10-4gradient steps	1	1	1target update interval	1	1	1H Offline Algorithm DetailsFor completeness, we list the detailed algorithms for ParPI +MOPO in Algorithm 2. We use rolloutlength 5 for all tasks, and the same penalty coefficients reported by Yu et al. (2020).
Table 6: Hyper-parameter Settings of ParPIParameter	Policy	Q-function	Discriminatoroptimizer	Adam	Adam	AdamLearning rate	3∙10-4	3∙10-4	3∙10-4discount(γ)	0.99	n/a	n/areplay buffer size	106	n/a	n/aentropy target	—dim(A)	n/a	n/aMCMC steps	n/a	n/a	5MCMC step size	n/a	n/a	3 ∙10-4gradient steps	1	1	1target update interval	1	1	1H Offline Algorithm DetailsFor completeness, we list the detailed algorithms for ParPI +MOPO in Algorithm 2. We use rolloutlength 5 for all tasks, and the same penalty coefficients reported by Yu et al. (2020).
