Table 1: Storage and compute complexity of the methods used in our experiments.
Table 2: List of experimentsHyperparameter tuning: We empirically fine-tune the hyperparameters and select the best foreach update scheme. We have made a comprehensive list of all the learning rates for the gradientand adaptive gradient based algorithms in Table 4 in the Appendix. The additional parameters aredefined as follows:5Under review as a conference paper at ICLR 2022•	ADAM: We apply an perturbation of 1.0 × 10-6. β0 and β1 are chosen to be 0.9 and 0.999,respectively.
Table 3: List of experimentsTestbed and software: All experiments were conducted using open-source software PyTorch(Paszke et al. (2019)), SciPy (Virtanen et al. (2020)) and NumPy (Harris et al. (2020)). We usean Intel Core i7-8700 CPU with a clock rate of 3.20 GHz and an NVIDIA RTX 2080 Ti graphicscard.
