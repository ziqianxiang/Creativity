Table 1: Final accuracy and runtime averaged over 10 runs, with standard deviation. “Samemethod” refers to using the training method for inference, while “full graph” uses the wholegraph for inference. LBMB achieves similar accuracy as previous methods when used fortraining, while using significantly less time per epoch and without requiring expensive fullgraph inference. LBMB is up to 500x faster than using the full graph for inference, atcomparable accuracy. Other inference methods are substantially slower or less accurate.
Table 2: Number of batches for graph partition-based batching.
Table 3: Hyperparameters for LADIESNodes per layerModel Dataset	Train ValidationGCN^"^ogbn-arxiv	42336^^84 672GCN ogbn-products 204 085 306 128GCN Reddit 90 000 150 000Table 4: Hyperparameters for neighbor samplingNumber of batchesModel	Dataset	Train	Validation	Test Number of nodes	GCN	ogbn-arxiv	12	8~~	8	6, 5, 5GCN	ogbn-products	20	4	200	5, 5, 5GCN	Reddit	8	4	4	12, 12GAT	ogbn-arxiv	8	4	4	8, 7, 5GAT	ogbn-products	1000	150	8000	15, 10, 10GAT	Reddit	400	400	400	20, 20Baseline hyperparameters. For Cluster-GCN the number of batches are the same as forour graph partition-based batching variant. Table 3 shows the hyperparameters for LADIES,Table 4 for neighbor sampling, and Table 5 for GraphSAINT-RW. To ensure that every nodeis visited exactly once during GraphSAINT-RW inference we use the validation/test nodesonly as root nodes of the random walks.
Table 4: Hyperparameters for neighbor samplingNumber of batchesModel	Dataset	Train	Validation	Test Number of nodes	GCN	ogbn-arxiv	12	8~~	8	6, 5, 5GCN	ogbn-products	20	4	200	5, 5, 5GCN	Reddit	8	4	4	12, 12GAT	ogbn-arxiv	8	4	4	8, 7, 5GAT	ogbn-products	1000	150	8000	15, 10, 10GAT	Reddit	400	400	400	20, 20Baseline hyperparameters. For Cluster-GCN the number of batches are the same as forour graph partition-based batching variant. Table 3 shows the hyperparameters for LADIES,Table 4 for neighbor sampling, and Table 5 for GraphSAINT-RW. To ensure that every nodeis visited exactly once during GraphSAINT-RW inference we use the validation/test nodesonly as root nodes of the random walks.
Table 5: Hyperparameters for GraphSAINT-RWBatch sizeModel	Dataset	Walk length Sample coverage Number of steps Train Val/Test				GCN	ogbn-arxiv	2	100	4	25 000	10 000GCN	ogbn-products	2	100	16	80 000	5000GCN	Reddit	2	100	8	23 000	6000GAT	ogbn-arxiv	2	100	8	17 500	10 000GAT	ogbn-products	2	100	1024	14 000	100GAT	Reddit	2	100	400	1600	6015Under review as a conference paper at ICLR 2022C Additional experimental resultsMain memory usage. Since LBMB saves the preprocessed dataset to memory, it can havea rather different main memory footprint than the normal dataset. To illustrate this, considerthe Reddit dataset. LBMB uses 8 auxiliary nodes per primary node on this dataset, and thetrain and validation sets of Reddit contain 177k nodes (of a total of 233k nodes). If there wasno overlap, this would translate to 1.4 million auxiliary nodes. However, our method aims atincreasing precisely this overlap between auxiliary nodes in a batch (see Sec. 3). With graphpartitioning, we end up with 466k auxiliary nodes, and with PPR batching we have 416kauxiliary nodes, across all batches. The number of auxiliary nodes determines the size of
Table 6: Main memory usage (GiB). In some cases, LBMB uses more main memory thanprevious methods due to overlapping batches (e.g. on ogbn-products). However, it canalso reduce memory requirements because it ignores irrelevant parts of the graph (e.g. onReddit). Note that we chose hyperparameters in a way that keeps GPU memory usageroughly constant between methods (as opposed to main memory usage).
Table 7: Methods and hyperparameters for selecting aux-iliary nodes for GCN on ogbn-products. LBMB is veryrobust to this choice. We did observe a slightly lower vali-dation accuracy for a low alpha (0.05), so we recommendusing 0.25.
