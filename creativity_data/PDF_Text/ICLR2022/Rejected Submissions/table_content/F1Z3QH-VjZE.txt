Table 1: Performance comparison on CelebA dataset. CelebA-single indicates the scenario in whicha single attribute (gender) is employed. CelebA-multi refers to the case considering two attributes:gender and hair color. Baseline I is a non-fair algorithm building upon the base framework with thehinge loss (Lim & Ye, 2017; Tran et al., 2017), and trained with the aggregated data Dbias ∪ Dref.
Table 2: Robustness of the proposed approach to the reference set size down to 1%. This is evaluatedon CelebA-single. All the other settings and baselines are the same as those in Table 1. Our algorithmoffers more robust performances both in fairness and intra FID.
Table 3: Performance comparison with other fairness regularizers on CelebA-single with the 10%reference set size. “JSD-based” refers to a regularization method based on Jensen-Shannon divergenceimplemented via Goodfellow et al. (2014). “KLD-based” is the one built upon Kullback-Leiblerdivergence (NoWozin et al., 2016). “χ2-based"represents the one implementing Pearson-χ2 diver-gence (Mao et al., 2017). “WD-based” refers to a regularization with Wasserstein distance (Gulrajaniet al., 2017). “Female” (or “Male”) refers to intra FID for female (or male) group. For each measure,We mark the best result in bold and the SeCond-best With underline.______________________JSD-based KLD-based	χ2-based	WD-based TVD-basedFemale	12.80 ± 1.499	15.24	±	0.371	16.01	±	1.601	16.51	±	1.244	9.51 ±	1.069Male	17.83 ± 1.173	22.47 ±	0.331	25.25	±	1.877	24.54	±	2.731	14.29 ±	1.354Fairness	0.087 ± 0.012	0.077 ±	0.019	0.058	± 0.019	0.047 ±	0.033	0.057 ±	0.0125 ConclusionWe introduced a TVD-based optimization frameWork for a fair generative model that Well tradeoffsthe fairness performance (quantified as fairness discrepancy) against sample quality (reflected in intraFID). Inspired by the equivalence betWeen the TVD and function optimization, We also developed anequivalent three-player optimization Which can readily be implemented via neural-net parameteri-zation. Our algorithm offers better performances than the state of the art both in fairness and intraFID, exhibiting more significant performances particularly for practically-relevant scenarios Wherethe access to balanced dataset is limited. One future Work of interest is to push forWard for morechallenging scenarios Where the reference dataset is not available.
Table 4: Number of samples used in CelebA. CelebA-single indicates the scenario in which a singleattribute (gender) is employed. CelebA-multi refers to the case considering two attributes: genderand hair color. For CelebA-single, we consider 9 : 1 female-vs-male samples for training set. ForCelebA-multi, we consider 85 : 15 non-black-hair vs black-hair samples. The reference set sizeindicates a ratio relative to training set.
Table 5: Number of samples used in UTKFace and FairFace. All the settings are the same as those inTable 4. For UTKFace, we consider 9 : 1 white-vs-non-white samples for training set. For FairFace,we consider 9 : 1 ratio white-vs-black samples.
Table 6: A modified ReSNet18 architecture for attribute classifiers.
Table 7: BigGAN architecture for generator and discriminator. Here ch represents the channel widthmultiplier: ch = 64 for 64 × 64 images of our interest. “ResBlock up” means a Generator ResidualBlock where the input is passed through a ReLU activation and then upsampled, followed by two3 × 3 convolutional layers with another ReLU activation in between. “ResBlock down” refers to aDiscriminator Residual Block in which the input is passed through two 3 × 3 convolutional layerswith a ReLU activation in between, and then downsampled. Upsampling is implemented via nearestneighbor interpolation, and downsampling is via average pooling. “ResBlock up/down n → m”refers to a ResBlock with n input channels and m output channels.
Table 8: Hyperparameters used for CelebA experiments. We use the same betas for (G, D, Dfair).
Table 9: Hyperparameters used for CelebA experiments with small reference set sizes. We share(β1,β2) = (0,0.999)for(G,D,Dfair).
Table 10: Hyperparameters used for UTKFace and FairFace experiments. We employ the same betasfor (G, D, Dfair).
Table 11: Hyperparameters used for experiments with other fairness regularizers. We employ thesame betas for (G, D, Dfair). “JSD-based” refers to a regularization method based on Jensen-Shannondivergence (Wong & You, 1985) implemented via Goodfellow et al. (2014). “KLD-based” is theone built upon KUllback-Leibler divergence (KUllback & Leibler, 1951; NoWozin et al., 2016). “χ2-based” represents the one implementing Pearson-χ2 divergence (Pearson, 1900; Mao et al., 2017).
Table 12: Hyperparameters used for experiments with other fairness regularizers. “JSD-JSD” indicatesthe case where both base optimization (the first term in equation 5) and regularization are basedon Jensen-Shannon divergence (Wong & You, 1985). “χ2-χ2" refers to the one that implementsPearson-χ2 divergence (Pearson, 1900) for base optimization as well as for regularization. “WD-WD”represents the one that employs Wasserstein distance (Wasserstein, 1969) both for base optimizationand regularization.
Table 13: Performance comparison with fine-tuning method on CelebA-single with 10% referenceset. Baseline I is a non-fair algorithm building upon the base framework with the hinge loss (Lim& Ye, 2017; Tran et al., 2017), and trained with the aggregated data Dbias ∪ Dref. Baseline II is thesame non-fair algorithm yet trained only with the small yet balanced reference dataset Dref . Choiet al. is the state of the art (Choi et al., 2020). “Female” (or “Male”) refers to intra FID for female(or male) group. The lower intra FID, the more realistic and diverse samples. “Fairness” is fairnessdiscrepancy introduced by Choi et al. (2020); see equation 1 for the definition. The lower, the fairer.
Table 14: Performance comparison on CelebA-single with 10% reference set size. Choi et al. (2020)corresponds to the state of the art. “Intra FID" refers to FreChet Inception Distance (Heusel et al.,2017) computed within each group (Miyato & Koyama, 2018; Zhang et al., 2019), and we provideresults for minority group herein. The lower intra FID, the more realistic and diverse samples.
Table 15: Performance comparison on UTKFace dataset where a race attribute is considered: whitevs. non-white. Baseline I is a non-fair algorithm building upon the base framework with the hingeloss (Lim & Ye, 2017; Tran et al., 2017), and trained with the aggregated data Dbias ∪ Dref. BaselineII is the same non-fair algorithm yet trained only with the small yet balanced reference datasetDref. Choi et al. is the state of the art (Choi et al., 2020). “White” (or “Non-white”) refers tointra FID (Miyato & Koyama, 2018; Zhang et al., 2019) for white (or non-white) group. The lowerintra FID, the more realistic and diverse samples for that group. “Fairness” is fairness discrepancyintroduced by Choi et al. (2020); see equation 1 for the definition. The lower, the fairer. The referenceset size indicates a ratio relative to training data.
Table 16: Performance comparison on FairFace dataset. All the settings and baselines are the same asthose in Table 15, except for different types of demographics: white vs. black.
Table 17: Complete results on CelebA-single in which a single attribute (gender) is employed.
Table 18: Complete results on CelebA-multi experiments where two attributes are considered: genderand hair color. All the settings and baselines are the same as those in Table 17 yet here we considerintra FID values for four groups: (i) female with non-black hair; (ii) male with non-black hair; (iii)female With black-hair; (iv) male With black hair.______________________________Reference set size	10%	25%Female W/ non-black hair	9.25 ± 0.040	7.74 ± 0.082Male W/ non-black hair	12.70 ± 0.090	11.61 ± 0.065Baseline I Female W/ black hair	8.93 ± 0.049	8.14 ± 0.043Male W/ black hair	10.83 ± 0.046	9.76 ± 0.067Fairness	0.245 ± 0.002	0.224 ± 0.002Female W/ non-black hair	26.96 ± 0.065	19.22 ± 0.087Male W/ non-black hair	35.35 ± 0.116	25.73 ± 0.131Baseline II Female W/ black hair	24.51 ± 0.085	17.15 ± 0.078Male W/ black hair	29.66 ± 0.088	21.92 ± 0.056Fairness	0.136 ± 0.001	0.107 ± 0.001Female W/ non-black hair	15.86 ± 0.048	13.11 ± 0.032Male W/ non-black hair	18.78 ± 0.058	14.78 ± 0.058Choi et al. Female W/ black hair	16.88 ± 0.047	12.85 ± 0.039Male W/ black hair	16.27 ± 0.042	13.68 ± 0.062Fairness	0.090 ± 0.001	0.063 ± 0.002
Table 19: Complete results on CelebA-single with the reference set size down to 1%. All the othersettings and baselines are the same as those in Table 17.
Table 20: Performance comparison with other fairness regularizations on CelebA-single with the10% reference set size. “JSD-JSD” indicates the case where both base optimization (the first termin equation 5) and regularization are based on Jensen-Shannon divergence (Wong & You, 1985;GoodfelloW et al., 2014). “χ2-χ2"refers to the one that implements Pearson-χ2 divergence (Pearson,1900; Mao et al., 2017) for base optimization as well as for regularization. “WD-WD” representsthe one that employs Wasserstein distance (Wasserstein, 1969; Gulrajani et al., 2017) both for baseoptimization and regularization. “Female” (or “Male”) refers to intra FID for female (or male) group.
