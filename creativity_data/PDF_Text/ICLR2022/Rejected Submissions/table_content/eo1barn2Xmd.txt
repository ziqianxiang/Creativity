Table 1: Sk and yk for Naive BFGS and for BFGS with momentum for L = 2 ∣∣θk2	Naive BFGS	BFGS with momentumSk	θk+1 - θk	(I- βI)(θk + l - θk)yk	θk+1 - θk + (nk+1 - nk)	(1 - β2)(θk+1 - θk) + (1 - β2)(nk+1 - nk)RV	2σ2	(1-β2)∙2σ2	θk+1-θk	θk+1 -θkablation study in Sec. 5.2). To stabilize the optimization, a common solution is to use a separate largebatch of data when estimating sk and yk (Moritz et al., 2016; Chang et al., 2019) in order to reducestochastic noise. However, this dramatically increases the computation cost and negates performancegains in wall-clock time.
Table 2: Computations and Memory in SGD, KFAC, and SLIM-QN	SGD	KFAC	sL-BFGS	SLIM-QNComputation				Fwd&Bwd	bCfb	αιkθk+ γbCfb + L P(d3 +(喇)3)	ɑ2 ∣θ∣ +2bCfb + 1 bHCfb	bCfb + α3 ∣θ∣Opt	α kθk	“o ∣∣θ∣∣ + 2 P(di + 暨)l&l	α0 ∣θ∣ +2M∣θ∣	α0 ∣θ∣ +2M∣θ∣Memory				Fwd&Bwd	bMfb	bMfb + βι ∣∣θk	bMfb + β2 ∣θ∣ + 1 bHMft	bMfb + β3 ∣θ∣Opt	βo kθk	βo ∣∣θk+2P(d2 + (曙)2)	β0 ∣θ∣ +2M∣θ∣	β0 ∣θ∣ +2M∣θ∣• di:	input dim in layer i. b: batch size. bH: batch size for the Hessian approx. kθik: #params in layer i.			4.2 Compute and Memory CostAs mentioned before, SLIM-QN aims to reduce the complexity of approximating the Hessian. In thissection, we summarize the compute and memory cost of SGD, KFAC, stochastic L-BFGS (sL-BFGS)variants(Chang et al., 2019) and SLIM-QN, and demonstrate the cost advantage of SLIM-QN.
Table 3: Hyperparameters for SGD, KFAC, SLIM-QN on ResNet-50/ImageNetOptimizer	lr	momentum	wd	damping ∣ β1∕β2		L	MSGD	0.1	0.9	0.0005	-	-	-	-KFAC	0.1	0.9	0.0002	0.001	-	50	-SLIM-QN	0.1	0.9	0.0002	0.05	0.9/0.9	30	10β1∕β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (s, y)B.2	ViTWe use a small Vision Transformer model with 6 layers, 8 attention heads, a patch size of 16, andboth hidden and MLP dimension of 512 for a total of about 10M parameters. We train for 90 epochswith linear learning rate warmup in the first 5 epochs, and decay the learning rate at 80 epochs forSLIM-QN. For SGD, we train for 100 epochs, decaying the learning rate at 30, 60, 90 epochs. Abatch size of 1024 is used for both algorithms. We perform 3 runs with different random seeds. Table4 shows the selected hyperparameters.
Table 4: Hyperparameters for SGD and SLIM-QN on ViT/ImageNetOptimizer	lr	momentum	Wd	damping	β1∕β2	L	MSGD	0.1	0.9	0.0001	-	-	-	-SLIM-QN	0.1	0.9	0.0	0.01	0.99/0.99	100	20βι /β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (s, y)							C Experiments on CIFAR- 1 0C.1 ResNet- 1 8Hyperparameter is shown as in Table 5. Initial learning rate is 0.1, decaying by a factor of 10 at 150thepoch. For both SGD and SLIM-QN we used a linear learning rate warmup for 5 epochs. Batchsize is 256 for both SGD and SLIM-QN. Figure 5 shows convergence on ResNet-18 using SGD andSLIM-QN. On small dataset CIFAR-10, SGD and SLIM-QN both deliver fast convergence at earlystages, while SLIM-QN is slightly better. Moreover, SLIM-QN achieves more faster convergenceand high validation accuracy at later stages.
Table 5:	Hyperparameters for SGD, SLIM-QN on ResNet-18/CIFAR-10Optimizer	I lr	I momentum	I Wd	I damping ∣ β1∕β2		I L	I MSGD SLIM-QN	0.1 0.1	0.9 0.9	0.0005 0.0005	- 0.05	- 0.9/0.9	- 100	- 10βι/β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (S, y)Figure 5: Convergence on ResNet-18/CIFAR-10 using SGD and SLIM-QN.
Table 6:	Hyperparameters for SGD, SLIM-QN on ViT/CIFAR-10 with batch size 1024Optimizer ∣	lr	I momentum	I Wd	I damping	I βι /β I	L I	MSGD	0.1	0.9	0.0001	-	-	-	-SLIM-QN	0.025	0.9	0.0001	0.01	0.99/0.99	100	10βι/β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (S, y)D	Stochastic Training using the Classical L-BFGSIn this section, we demonstrate that the classical L-BFGS suffers convergence instability in stochastictraining, even using large batch sizes. We train ResNet-18 on CIFAR-10, and vary batch size from 64to 2048. Learning rate decays from 0.1 to 0.001 in 100 epochs. Weight decay is 0.0005. The Hessianapproximation is updated using the L-BFGS formula for every 50 iterations, with at most 10 historyvectors.
Table 7:	Hyperparameters for SGD, SLIM-QN and block-wise SLIM-QN on ResNet-18/CIFAR-10Optimizer	∣	Ir	∣	momentum	∣ Wd ∣ damping ∣	β1 /β2	∣ L IMblock-wise SLIM-QN	∣	0.1	∣	0.9	∣ 0.0003 ∣	0.01	∣	0.9/0.9	∣ 50 ∣ 10βι/β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (S, y)18Under review as a conference paper at ICLR 2022F Hyperparameter TuningSLIM-QN involves additional hyperparameters besides common parameters in SGD: learning rate,momentum, and weight decay. While tuning the hyperparameters is not a huge burden since someparameters are fixed in all the experiments, such as lower and upper threshold for restrainingeigenvalues in the Hessian: σL , σH, and length of history vectors: M . For the rest of parameters:damping (τ0), momentum (β1, β2) and update frequency (L) for the Hessian, it is easy to conducta grid search on a small models and datasets, explore how these parameters affect optimization,then apply them to large-scale model training. For example, after training on CIFAR-10, we foundthat small L improves generalization performance, and large damping stabilizes optimization butcauses optimizer to behave like SGD. With these notions, it is easy to tune these parameters, and thenachieve optimal convergence performance and accuracy.
