Table 1: Results on OpenAI Gym Mujoco domains. CG denotes Cheetah-Gravity, CB denotesCheetah-Bodyparts, HG denotes Hopper-Gravity, HB denotes Hopper-Bodyparts, WG denotesWalker-Gravity, WB denotes Walker-Bodyparts.
Table 2: Room type probability distributionA.4.2 Box-jumping TaskWe use a simplified version of jumping task (des Combes et al., 2018) as a simple testbed for theproposed algorithm VBLRL. We select a random position of obstacle between 15 〜33 for eachtask. The 4-element state vector describes the (x, y) coordinates of the agent’s current position, andits velocity in the x and y directions. The agent can choose from two actions: jump and right. Thereward function for this box-jumping task is:Rt = I{st reach the right wall} 一 I{st+ι hit the obstacle} + Xt ∙ I{st+ι not hit the obstacle} (11)15Under review as a conference paper at ICLR 2022Object type probability ∣ Blue ball ∣ Green box ∣ Purple boxRoom 1	0	0.3	0Room 2	0	0.2	1Room 3	0.6	0	0Room 4	0	0	0Table 3: Object type probability distributionA.4.3 OpenAI Gym Mujoco DomainsFigure 4: Left: Walker-2D; Middle: Halfcheetah; Right: HopperSimilar to (Mendez et al., 2020), we evaluated on the HalfCheetah, Hopper, and Walker-2D environ-ments. For the gravity domain, we select a random gravity value between 0.5g and 1.5g for each task.
Table 3: Object type probability distributionA.4.3 OpenAI Gym Mujoco DomainsFigure 4: Left: Walker-2D; Middle: Halfcheetah; Right: HopperSimilar to (Mendez et al., 2020), we evaluated on the HalfCheetah, Hopper, and Walker-2D environ-ments. For the gravity domain, we select a random gravity value between 0.5g and 1.5g for each task.
Table 4: Reward functions for OpenAI Gym domainsepisodes of each new tasks, the agent hasn’t collected enough samples of the new task, which resultsin overfitting problems when training the task-specific. Thus, we use the world-model posteriorinstead to do the first few rounds of predictions and let the task-specific model begin training aftercollecting enough samples. The world-model has lower possibility of overfitting as its training datacomes from all the previous tasks and has much larger quantity. We list the other implementationdetails below. The planning horizons are selected from values suggested by previous model-basedRL papers (Chua et al., 2018; Wang et al., 2019).
Table 5: Room type probability distributionFor BOSS and BLRL, we set the number of sampled models K = 5, and γ = 0.95, ∆ = 0.01 forvalue iteration.
