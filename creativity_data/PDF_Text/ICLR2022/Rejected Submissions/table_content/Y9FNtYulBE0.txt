Table 1: Comparison with the baseline models for AUC of each class and average AUC. For eachcolumn, bold values denote the best results.
Table 2: Disease localization under varying IoU on the NIH Chest X-ray dataset. Please notethat since our model doesn’t use any ground truth bounding box information, to fairly evaluate theperformance of our model, we only consider the previous methods’ results under the same setting,5 ConclusionIn this paper, we propose an end-to-end knowledge-guided cross-attention Transformer, namedCheXT that can jointly model abnormality classification and localization without the supervisionof localization annotation of chest X-rays. Our approach differs from previous studies in thechoice of universal modeling transformer, the use of radiomic features as prior knowledge, and afeedback loop for image and radiomic features to mutually interact with each other. Additionally,the project aims to mitigate current gaps in radiology by making prior knowledge more accessible toimage data analytic and diagnostic assisting tools, with the hope that this will increase the model’sinterpretability. Experimental results demonstrate that our method outperforms the state-of-the-artalgorithms, especially for the disease localization task, where our method can generate more accuratebounding boxes.
