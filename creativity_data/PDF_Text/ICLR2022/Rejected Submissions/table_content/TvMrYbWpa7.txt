Table 1: Parameter counts and decoding complexity for our various global scale-space-flow models.
Table 2: Rate-distortion performance of InstA and baselines on several datasets. We summarize therate-distortion performance as BD-rate relative to the HM reference implementation of H.265 in therespective setting (lower is better).
Table  3:  Overview  of  neural  instance-adaptive  data  compression  methods.   We  categorize  theseworks by the data modality and by the adaptation of receiver-side components.   The number ofadapted network parameters on the decoder side is a proxy for adaptation flexibility, although flex-ibility is also affected by other measures such as the frequency of adaptation, or the quantization ofupdates.
Table 4:  Architecture differences between the original SSF model published by Agustsson et al.
Table 5:  Architecture details for our scale-space-flow models.  In the top, we show the number ofchannels for the latent and hyperlatent variables, z, which are the basis for the compressed code; inthe bottom, we show the number of channels in the codec and hypercodec networks in the encoderand decoder. We use the same model architecture and size for I-frame, scale-space-flow and residualnetworks.
Table 6: BD-rate of several codecs (rows) compared to different reference codecs (columns) in thelow-latency setting. Lower numbers are better, the best results on each dataset are shown in bold.
Table 7: BD-rate of several codecs (rows) compared to different reference codecs (columns) in theB-frame setting. Lower numbers are better, the best results on each dataset are shown in bold.
