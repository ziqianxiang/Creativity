Table 1: Summary of the datasets.
Table 2: Clustering results (%) of various methods on five benchmark datasets. The best and secondbest results are shown in bold and underline, respectively.
Table 4: Ablation studies for different self-supervised learning frameworks, and positive sampling(PS) strategy, and prototypical contrastive loss for NCC. The best and second best results are shownin bold and underline, respectively.
Table A1: Clustering results (%) for fair comparisons. We train NCC to demonstrate its effective-ness for fair comparisons with the following settings: 1) we exclude test set from the whole dataset;and 2) we use an original image size (224) for ImageNet-10 and ImageNet-Dogs. All results weretrained with ResNet-34. There is no clear margin for CIFAR-10 and CIFAR-20 datasets with dif-ferent splits, while significant improvements can be observed for ImageNet-10 and ImageNet-Dogsdatasets. Considering that NCC has already achieved state-of-the-art performance against previouswork in Table 2, these results further demonstrate the superiority of NCC.
Table A2: Clustering results (%) on the subsets of ImageNet. We strictly follow the settingsin (Van Gansbeke et al., 2020): we have adopted the same 50, 100, and 200 classes from ImageNet,clustered on the training set and tested on the validation set. We have used the same experimentalsettings as the other benchmarked datasets and trained NCC with ResNet-50 for 300 epochs. Wenote that SCAN has used the pre-trained model of MoCo trained on the full ImageNet for 800epochs. The results are directly referred from their published paper including k-means with pre-trained MoCo1 , SCAN after the clustering step2, and SCAN after the self-labeling step3. Withmuch fewer training epochs and training data, NCC still produces better performance with a clearmargin, demonstrating the superiority of NCC.
Table A3: Clustering results (%) on Tiny-ImageNet (Le & Yang, 2015). Tiny-ImageNet is alsoa subset of ImageNet, with a total of 200 classes and 500 images for each class. We trained NCCwith same settings in Table 2 except for ResNet-18 and the image size 96 × 96. Even though with asmaller backbone and image size (CC used ResNet-34 and the image size 224 × 224), our NCC stillproduces promising improvements over previous state-of-the-art methods.
Table A4: Clustering results (%) on long-tailed datasets of different self-supervised learning frame-works and our proposed NCC. We built the long-tailed version of CIFAR-10 and CIFAR-20, termedCIFAR-10-LT and CIFAR-20-LT using the codes of (Tang et al., 2020), which follows (Zhou et al.,2020; Cao et al., 2019). Specifically, they were built upon the training datasets under the control ofdata imbalance ratio Nmin/Nmax = 0.1 for the data distribution, where N is the number of samples ineach class. The samples in the long-tailed datasets are almost all in the head of distributions. MoCocannot handle this problem well due to the class collision issue, as a results, the samples in the headwill be pushed away and the ones in the tail will be mixed together. The rest three methods do notneed the negative examples so that they outperform MoCo v2 by a large margin. By introducingthe positive sampling and ProtoCL, we can further boost and stabilize the performance of vanillaBYOL.__________________________________________________________________________________CIFAR-10-LT	CIFAR-20-LTMethods	NMI	ACC	ARI	NMI	ACC	ARIMoCo v2	46.7±0.1	33.4±0.3	27.7±0.0	31.2±0.3	28.2±0.2	16.1±0.3BYOL	51.6±1.0	41.3±0.4	30.8±0.4	41.9±0.4	34.6±0.5	22.3±1.0NCC	55.3±0.4	43.9±0.1	36.3±0.3	44.6±0.2	39.0±0.7	27.3±0.2w/o ProtoCL	53.1±0.7	42.7±0.4	31.6±0.8	43.4±0.8	35.1±0.6	24.0±0.1Table A5: Clustering results (%) on different ResNet architectures. With the deeper ResNet net-works, there are little performance gain and even drops for MoCo v2 and BYOL. On the contrary,NCC achieves significant improvement with small standard deviation for clustering, demonstrating
Table A5: Clustering results (%) on different ResNet architectures. With the deeper ResNet net-works, there are little performance gain and even drops for MoCo v2 and BYOL. On the contrary,NCC achieves significant improvement with small standard deviation for clustering, demonstratingits superior stability and performance against the baseline MoCo v2 and BYOL. The best and secondbest results are shown in bold and underline, respectively.
