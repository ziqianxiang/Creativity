Table 1: Approximation errors for sin(πxι +-+ ∏Xn) by neural networks(a) n = 1Figure 2: Training errors for sin(πxι +----+ ∏Xn), single hidden layerand diagonal TMAF neural networks with 3 hidden layers and 400 neurons per layer (except thefirst and last layers). We also consider the tri-diagonal TMAF (denoted by Tri-diag TMAF, seeequation 7) with 3 hidden layers and 300 neurons per layer. The training datasets are 20000 input-output data pairs where the input data are randomly sampled from the interval [-1, 1] based onuniform distribution.
Table 2: Error comparison for sin(100πx) + cos(50πx) + sin(πx)For this challenging problem, we note that the diagonal TMAF and tri-diagonal TMAF producehigh-quality approximations (see Figures 5 and 4b) while ReLU and parametric ReLU are not ableto approximate the highly oscillating function within reasonable accuracy, see Figure 4b and Table2. Moreover, it is observed from Figure 6 that ReLU and parametric ReLU actually approximate thelow frequency part of the target function. To capture the high frequency, ReLU-type neural networksare clearly required to use much more neurons, introducing significantly amount of weight and biasparameters. On the other hand, increasing the number of intervals in TMAF only lead to a few moretraining parameters.
Table 3: Evaluation accuracy for CIFAR and MNIST(SSO-)601(a) Training loss(b) Classification accuracy-sso-)6q(a) Training lossFigure 9: Comparison among ReLU, Para-ReLU and TMAF for CIFAR-100Figure 8: Comparison among ReLU, Para-ReLU and TMAF for CIFAR-10(b) Classification accuracyorg/10.1016/j.jcp.2019.109136. URL https://www.sciencedirect.com/science/article/pii/S0021999119308411.
