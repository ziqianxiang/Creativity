Table 1: Our method with a linear subspace (LCS+L) and a point subspace (LCS+P) compared toLEC (Liu et al., 2017), NS (Yu et al., 2018), and US (Yu & Huang, 2019). Note that “ArbitraryCompression Levels" refers to arbitrarily fine-grained post-deployment compression. ∣ω∣ denotesthe number of network parameters, |b| denotes the number of BatchNorm parameters, and n denotesthe number of compression levels for models that don’t support arbitrarily fine-grained compression.
Table 2: Results for structured sparsity. Note that models of a particular architecture and sparsitylevel all have the same runtime characteristics (memory, FLOPS, and runtime), so we only reportone value. Runtime was measured on a MacBook Pro (16-inch, 2019) with a 2.6 GHz 6-Core IntelCore i7 processor and 16GB 2667 MHz DDR4 RAM. Memory consumption refers to the size ofmodel weights in the currently executing model.
Table 3: Results for unstructured sparsity in the high sparsity regime. Note that models of a particu-lar architecture and sparsity level all have the same runtime characteristics (memory and FLOPS), sowe only report one value. Runtime was not measured because it requires specialized hardware. So,we follow the standard practice of only reporting memory and flops. Memory consumption refers tothe size of model weights in the currently executing model.
Table 4: Results for unstructured sparsity in the wide sparsity regime. Note that models of a partic-ular architecture and sparsity level all have the same runtime characteristics (memory and FLOPS),so we only report one value. Runtime was not measured, because it requires specialized hardware(so most unstructured pruning works report memory and flops). Memory consumption refers to thesize of model weights in the currently executing model.
Table 5: Results for quantization. Note that models of a particular architecture and quantization bitwidth all use the same memory, so we only report one value. Runtime was not measured, becauseit requires specialized hardware. Memory consumption refers to the size of model weights in thecurrently executing model.
