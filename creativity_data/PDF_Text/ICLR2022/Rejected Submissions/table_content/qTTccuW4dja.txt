Table 1: Evaluation of continuous sentence embeddings on the toy dataset(d = 16, 512). Each experiment is run 5 times. AriEL, achieves almost perfect perfor-mance in most metrics, especially in validity, which quantifies how many random sampleswere decoded into a unique and grammatical sentence. Transformer performed exception-ally, except for validity. All methods improved their performance increasing d, particularlyin validity, but still achieved less than one third the performance of AriEL. VAE is thesecond best in validity, supporting the hypothesis, that volume coding facilitates retrievalof information by random sampling.
Table 2: Performance on the Guess-What?! Questioner data. For the realdataset the pattern is repeated: AriELshows that a larger value of valid sentencesis possible. Transformer 16 gave better re-sults when nlayers = 2 than nlayers = 20,that was tested to increase its learnable pa-rameters from 588K to 2,666K.
Table 3:	Generalization: next wordprediction of unbiased sentences attest time. Blue means that the incor-rect reconstruction complies with the biasand purple means that it is still unbiased.
Table 4:	Generation: output of the de-coder when sampled uniformly in thelatent space. Red defines grammaticallyincorrect generations according to the CFGthe models are trained on. AriEL producesan extremely varied set of grammaticallycorrect sentences, most of which keep thebias of the training set. Transformer re-veals itself to be hard to control via randomsampling of the latent space, since it almostnever produces correct sentences with thismethod. AE and VAE manage to produceseveral different sentences, the latter pro-ducing more non grammatical, but as wellmore varied grammatical ones.
Table 5: AriEL samples using GPT-2 as Language Model. Even for a large languagemodel such as the small GPT-2, 117M parameters, over 50K subwords, and 100 symbolssentences, AriEL results in high quality samples for a wide range of latent dimensions (SM11). Floating point precision does not seem to be of concern up to this limit.
Table S1: Description of vocabulary used.
Table S2:	Generalization: next wordprediction of unbiased sentences attest time. An unbiased sentence is encodedand decoded by each model. Color meansthat the word was incorrectly reconstructed.
Table S3:	Generation: output of thedecoder when sampled uniformly inthe latent space. Red defines grammat-ically incorrect generations according to theCFG the models are trained on. AriEL pro-duces an extremely varied set of grammati-cally correct sentences, most of which keepthe bias of the training set. Transformer re-veals itself to be hard to control via randomsampling of the latent space, since it almostnever produces correct sentences with thismethod. AE and VAE manage to produceseveral different sentences, the latter pro-ducing more non grammatical, but as wellmore varied grammatical ones.
Table S4: AriEL samples using GPT-2 as Language Model. Even for a large languagemodel such the small GPT-2, which has 117M parameters, and over 50K subwords vocab-ulary, AriEL folding resulted in high quality samples for a wide range of latent dimensions.
Table S5: AriEL samples training the Language Model on the real dataset Guess-What?!. Most samples created through AriEL seem easily interpretable if not fully gram-matically correct. Transformer appears to be very difficult to sample from the latent space,and it produces very poor language for d = 16, and more reasonable even though hardlyinterpretable for d = 512. AE and VAE seem to produce more reasonable sentences, stilloften hardly interpretable, and less diverse than AriEL.
