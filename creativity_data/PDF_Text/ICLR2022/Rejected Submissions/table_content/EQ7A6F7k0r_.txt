Table 1: Empirical results on the MNIST test dataset under the noiseless quantum circuit setting.
Table 2: Empirical results on the MNIST test dataset under the noisy quantum circuit setting.
Table 3: Comparing performance of QTN-VQC with and without activation function.
Table 4: Comparing performance of QTN-VQC with fewer qubits.
Table 5: Comparing performance of different TT-ranks for QTN-VQCTT-ranks	ParamS	CE	ACC (%){1,2, 2,1}	328	0.3090	91.43 ± 0.51{1,4, 4,1}	768	0.3082	91.46 ± 0.53{1,6, 6,1}	1464	0.3079	91.47 ± 0.52-B.2	A comparison of convergence ratesNext, we analyze the computational complexity for TTN for QTN-VQC. In more detail, given theTT-ranks {r1, r2, ..., rK}, a multi-dimensional tensor W is factorized into several K -order tensorsWk ∈ Rrk-1 ×Ik×Jk ×rk, the computational complexity of the feed-forward process is in the scale ofO(K maxk Ik maxk Jk (maxk rk)K). In contrast, the computational overhead for a dense layer isin the scale of O( k Ik k Jk). It means that smaller TT-ranks can reduce the computational costfor QTN-VQC, which explains that smaller TT-ranks {1, 2, 2, 1} is configured in our experimentsof QTN-VQC.
Table 6: Comparing performance of different TT-ranks for QTN-VQCModels	Dense-VQC	AlexNet-VQC	QTN-VQCTime/epochs (mins)	58	一	75	61C Experiments of Labeled Faces in the Wild (LFW)C.1 Experimental setupsThe LFW is a dataset for the task of unconstrained face recognition, which is composed of 13000images with the shape of [154, 154, 3]. The shape of We randomly split all the datasets into 11000training data, 2000 test data. 16 qubits are used for VQC, and the shape of the input tensor is set as22 × 147 × 22. The other settings are kept the same as the configurations for the MNIST task.
Table 7: Simulation results on the LFW test dataset under a noiseless circuit condition.
Table 8: Empirical results on the LFW test dataset under the noisy quantum circuit setting.
