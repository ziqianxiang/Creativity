Table 1: GLUE test results for single 6-layer transformer models. GAL establishes a new state of the arton KD for NLP. Baselines: BERT-Theseus (Xu et al., 2020), BERT-PKD (Sun et al., 2019a), tinyBERT (Jiaoet al., 2019) tinyRoBERTa (Rashid et al., 2021), DistilRoBERTa (Sanh et al., 2019), and DistilRoBERTa + KD(standard KD) and DistilRoBERTa + RT (round-trip translation to generate unlabeled text). Accuracy scoreson MNLI-matched/MNLI-mismatched are reported for MNLI, Matthew‘s correlation is reported for CoLA,F1/Accuracy scores are reported for QQP and MRPC, Pearson/Spearman correlations are reported for STS-B,and Accuracy is reported for QNLI and RTE.
Table 2: RoBERTa base and GAL self-training results on GLUE dev sets, averaged across 5 independent runs.
Table 3: RoBERTa-large with GAL self-training and SoTA methods evaluated on GLUE test sets. The benefitof GAL on single models is larger than ensembles. It appears that self-training reduce the variance of models.
Table 4: RoBERTa-base and GAL results on four tabular datasets from the UCI repository. Accuracy isreported for these datasets.
Table 5: Few-shot learning results for GPT-J (6B) (Wang & Komatsuzaki, 2021) on four NLP datasets. Accu-racy is reported for these datasets.
Table 6: GAL with various GPT-2model sizes on GLUE dev sets. NAindicates a vanilla RoBERTa basemodel.
Table 7: Synthetic data from class-conditional LMs underperforms GAL and RoBERTa on GLUE dev sets.
Table B.1: Classification error rates on CIFAR-10 test set with varying amounts of syntheticdata for three different model architectures. Re-ported results are the average of 3 independentruns.
Table B.2: Classification error rates on Fashion MNIST test set with varying amounts of synthetic data forthree different model architectures. Results reported are the average over 3 independent runs.
Table C.1: QNLI: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examplesin parenthesis.
Table C.2: QQP: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examplesin parenthesis.
Table C.3: RTE: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examplesin parenthesis.
Table C.4: MRPC: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examplesin parenthesis.
Table C.5: MNLI: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examplesin parenthesis.
Table C.6: SST-2: Two labeled examples, along with 3 nearest neighbors (based on RoBERTa representations)from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examplesin parenthesis.
Table D.1: Summary of the three sets of tasks used for evaluation of GAL. STS-B is a regression task, so#classes is not applicable.
Table D.2: 3 examples of input and labels for the Drybean tabular task.
Table E.1: For each dataset we report the number of unique n-grams in (the original dataset, the syntheticdataset, shared between the two).
Table F.1: Performance of RoBERTa annotation and conditioning labels on 100 random examples from thesynthetic RTE dataset generated by a class-conditional LM.
Table G.1: GLUE test results of different GPT-2 models.
Table H.1: Training details for NLP tasks.
Table H.2: Training details for tabular tasks.
Table H.3: Training details for KD on GLUE benchmark.
Table H.4: Training details for CV experimentsParameter	Description	Valueτ	Pseudo-Iabeling confidence threshold	0.95batch size	Number of labeled images per batch	64μ	Ratio between number of unlabeled and labeled images in each batch	7images per epoch	Number of labeled images per epoch	65536#epoch	Number of epochs of training	200lr	learning rate max value (10 epochs warmup then cosine decay)	0.03weight decay	Weight decay regualrization coefficient	5.00 × 10-4momentum	Nesterov momentum for SGD optimizer	0.90I Additional DetailsIn Tables I.1, and I.2, we present some descriptive statistics of our CIFAR-10 synthetic image datasetto complement the samples shown in Figure C.1 and to help shed some light on the nature of theimages generated by the NCSN network.
Table I.1: Unfiltered CIFAR-10 synthetic data statistics sorted by Count. The Class pseudo-label for eachsynthetic image is first obtained using a teacher model trained on the original CIFAR-10 data. Count denotesthe number of images per class in the entire synthetic dataset (500K images). Confidence statistics shows themean and standard deviation of the teacher model confidence score aggregated over each class.
Table I.2: Filtered CIFAR-10 synthetic data statistics sorted by Count. The Class pseudo-label is first obtainedfor each synthetic image using a teacher model trained on the original CIFAR-10 data. The dataset is thenfiltered based on the teacher confidence score where only images with confidence ≥ τ = 0.95 are retained.
