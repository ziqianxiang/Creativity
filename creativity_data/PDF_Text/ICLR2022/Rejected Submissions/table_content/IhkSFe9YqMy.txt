Table 1: Mean value of the total reward of agent in different tasksEnvironment	HalfCheetah-v2	SWimmer-v2	Walker2d-v2	Hopper-v2Random	-283± 29	Γ±4	2±2	19 ± 6DDPG	7790±2058	84 ± 26	920 ± 550	1313 ± 867DDPGERM	8415 ± 1161	94 ± 24	933 ± 578	1548 ± 861SAC	9452± 984	41 ± 2	3163 ± 951	2856 ± 502SACERM	10219±645	40 ± 3	3564 ± 1216	2883 ± 543TD3	7240± 1455	58 ± 31	2568 ± 733	1939 ± 1442TD3ERM	8795± 1023	48 ± 13	3305 ± 966	2553 ± 852In all the experiments, the most time-consuming part of the algorithm is calculating thesimilarity between states. To solve this problem, we accelerate the process by using matrixoperation, which expands the dimension of current state Sc to be consistent with the di-mension of key states buffer, so we calculate the similarity with the matrix Sk composed ofall key states quickly. Therefore, in the case of a small increase in time consumption, theperformance of the algorithm is significantly improved, especially on the HalfCheetah task.
Table 2: Dimensionality of the MuJoCo tasks environment: the dimensionality of the under-lying physics model dim(s)， number of action dimensions dim(a) and observation dimensionsdim(o).
Table 3: The description of the Mujoco tasks environmentTask name	Brief descriptionHalfCheetah-v2	The agent should move forward as quickly as possible with a cheetah like body that is constrained to the plane (Wawrzynski & Tanwani, 2013).
