Table 1: Multi-class out-of-distribution detection results using the maximum softmax probability(MSP) baseline (Hendrycks & Gimpel, 2017), the confidence branch detector of DeVries & Taylor(2018), and our maximum logit baseline. All values are percentages and average across five out-of-distribution test datasets. Full results on individual OOD test datasets are in the Appendix.
Table 2: Results on Species. Models and the processed version of ImageNet-21K (ImageNet-21K-P)are from Ridnik et al. (2021a). All values are percent AUROC. Species enables evaluating anomaly de-tectors trained on ImageNet-21K and evades class overlap issues present in prior work. Using Speciesto conduct more controlled experiments without class overlap issues, we find that contrary to recentclaims (Fort et al., 2021), simply scaling up Vision Transformers does not make OOD detection trivial.
Table 3: Multi-label out-of-distribution detection comparison of the Isolation Forest (iForest), LocalOutlier Factor (LOF), Dropout, logit average, maximum softmax probability, and maximum logitanomaly detectors on PASCAL VOC and MS-COCO. The same network architecture is used for allthree detectors. All results shown are percentages.
Table 4: Results on the CAOS benchmark. AUPR is low across the board due to the large classimbalance, but all methods perform substantially better than chance. MaxLogit obtains the bestperformance. All results are percentages.
Table 5: B is for the maximum softmax probability baseline, M is for maximum logit, D is for themethod in DeVries & Taylor (2018), and K is our own KL method described below. Both M and Kare ours. Results are on ImageNet and Places365. All values are percentages and are rounded so that99.95 rounds to 100.
