Table 1: Top-1 accuracy under linear evaluation on ImageNet. All methods are based on ResNet-50 pre-trained with two 224×224 views. Evaluation is on a single crop and all best results are inbold. ,f, denotes improved reproduction from SimSiam.
Table 2: Semi-supervised learning on Ima-geNet. All models are finetuned with 1% and10% training examples. Results for the su-pervised method are from Zhai et al. (2019).
Table 3: Transfer learning on image clas-sification task. We report top-1 accuracy oniNat18 and Places-205 datasets, and classifi-cation mAP on VOC07.
Table 4: Transfer Learning on object detection and instance segmentation task. All modelsuse the C4-backbone. We benchmark finetuned representations on the object detection task onVOC07+12 using Faster R-CNN and on the detection and instance segmentation task on COCOusing Mask R-CNN(1× schedule).
Table 5: Influence of hierarchical projection head and cross contrastive loss. We report the top-1accuracy of models trained with different choices on hierarchical projection head, cross contrastiveloss, and multi predictor.
Table 6: The levels and layers of projectionhead. We examine the effect of different lev-els and layers(in one level) for the hierarchi-cal projection head. We do not apply a ReLUactivation nor a batch normalization on the fi-nal linear layer of our MLPs.
Table 7: Learning rate of Predictor. We re-port top-1 accuracy at 100 epochs when ap-plying a multiplier λl to the low-level predic-tor and λh to the high-level predictor learn-ing rate. ’fixed’ denotes using constant learn-ing rate.
Table 8: Top-1 accuracy under linear evaluation on ImageNet. ,f, denotes improved reprodUc-tion from SimSiam.
Table 9: Memory footprint and Trainingspeed. ’Memory’ refers to the memory foot-print on a single gpU dUring training. ’ME’means momentUm encoder.
Table 10: Top-1 accuracy under differentway of cross-correlation. All method Use 3level hierarchical projection head.
