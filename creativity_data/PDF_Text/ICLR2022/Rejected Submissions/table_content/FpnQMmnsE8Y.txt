Table 1: Our method compared with multiscale deepequilibrium models (Bai et al., 2020) on CIFAR10and CIFAR100 classification. At the same number ofmodel parameters, we achieve 3% - 6% improvementwith 15 - 25x less inference time. Inference time ismeasured by milliseconds per image.
Table 2: At the same number of backbone pa-rameters on CIFAR100, using local RPGs atblock-level improves accuracy. Using a globalRPG further improves the accuracy. RPG alsooutperforms baseline methods.
Table 3: ImageNet and CIFAR100 top-1 classification accuracy versus the number of back-boneparameters for our ResNet-RPG and plain ResNet. Our ResNet-RPG consistently achieves higherperformance at the same number of parameters.
Table 4: Pose estimation performance (pa-rameter size) on MPII human pose com-pared with CPM (Wei et al., 2016). Themetric is PCKh@0.5.
Table 5: Multitask regression errors on S3DIS withsub-net architecture as Ramamonjisoa & Lepetit (2019).
Table 6: Comparison with fine-grained pruning forreducing model size. Compared with IMP (Frankleet al., 2019) on CIFAR10, RPG achieves higherpruned accuracy and similar accuracy drops.
Table 7: Comparison with coarse-grained prun-ing for reducing FLOPs and inference speed.
Table 8: RPG increases the model generalizability. (a) ResNet with RPG has the lower gap betweentraining and validation set on ImageNet classification. The metric is training accuracy minusvalidation accuracy. Lower is better. (b) Using RPG for pose estimation also decreases the trainingand validation performance GAP. The metric is training PCK@0.5 minus validation PCK@0.5. Loweris better. (c) ResNet with RPG has higher performance on out-of-distribution dataset ObjectNet(Barbu et al., 2019). The model is trained on ImageNet only and directly evaluated on ObjectNet.
