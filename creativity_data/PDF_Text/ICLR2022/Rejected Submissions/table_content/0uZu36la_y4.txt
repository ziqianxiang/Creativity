Table 1: Robust accuracy on CIFAR10 under early stopping without temporal ensembling. Thenumber in parenthesis indicates the epoch of piecewise constant step-size decay. For CFOL-AT toconsistently converge it was necessary to decay earlier than the usual decay at epoch 75. Fortunately,known regularization techniques mitigate this problem which leads to our main results in Section 5.
Table 2: Accuracy on CIFAR10. For both clean test accuracy (accclean) and robust test accuracy(accrob) we report the average, 20% worst classes and the worst class. We compare our method(CFOL-AT) with standard adversarial training (ERM-AT) and two baselines (LCVaR-AT and FOL-AT). CFOL-AT significantly improves the robust accuracy for both the worst class and the 20% tail,while only incurring a small reduction in the average robust accuracy in comparison with ERM-AT.
Table 3: Clean test accuracy (accclean) and robust test accuracy (accrob) on CIFAR100 and STL10.
Table 4: Summary of methods where N is the number of samples and k is the number of classes.
Table 5: Robust accuracy after early stopping ERM-AT based on different metrics of the Validationset. The indicated performance is on the test set. Notice that We only improVe the Worst classmarginally While suffering significantly in terms of aVerage accuracy if We early stop based on theWorst class.
Table 6: Comparison betWeen standard adVersarial training (ERM-AT) With and Without temporalensembling (TE). TE improVes the robust accuracy on the Worst class While maintaining the aVeragerobust accuracy. HoWeVer, TE leads to a drop in terms of the clean accuracy.
Table 7: For fair comparison we also consider early stopping based on the worst class accuracy onthe hold-out set. As can be observed the results for CFOL-AT do not differ significantly from earlystopping using the average robust accuracy, so standard training setups do not have to be modifiedfurther.
Table 8: Comparison between different sizes of '∞-ball attacks on CIFAR10. The same constraintis used at both training and test time. When the attack size is increased beyond the usual 8/255constraint we still observe that CFOL-AT increases the robust accuracy for the weakest classeswhile taking a minor drop in the average robust accuracy. Interestingly, the gap between ERM-ATand CFOL-AT seems to enlarge. See Appendix C.2 for more detail on the attack hyperparameters.
Table 9: Model performance on CIFAR10 under AutoAttack (Croce & Hein, 2020). The modelsstill uses 7 steps of PGD at training time with a '∞-constraint of 8∕255. Only at test time is the attackexchanged with AutoAttack under the same constraint. CFOL-AT is robust to AutoAttack in thesense that the worst class performance is still improved. However, as expected, the performance isworse for both methods in comparison with their respective 20-step PGD based attacks at test time.
Table 10: Reweighted variant of CFOL. Algorithm 1 samples from the adversarial distributionp. Alternatively one can sample data points uniformly and instead reweight the gradients for themodel using p. In expectation, an update of these two schemes are equivalent. To see why,observe that in CFOL the model has access to the gradient VθLy,i(θ). We can obtain an un-biased estimator by instead reweighting a uniformly sampled class, i.e. Ey〜p,i [VθLy,i(θ)] =Ey〜unif(k),i [kpyVLy,i(θ)]. With classes sampled uniformly the unbiased estimator for the ad-versary becomes Ly『= l{y,=y}Lyk ∀y0. Thus, one update of CFOL and the reweighted variantare equivalent in expectation. However, note that we additionally depended on the internals of themodel’s update rule and that the immediate equivalence we get is only in expectation. We test thereweighted variant of CFOL-AT on CIFAR10 and observe similar results as for CFOL-AT.
Table 11: Hyperparameter exploration for CFOL-AT, LCVaR-AT and FOL-AT.
