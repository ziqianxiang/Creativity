Table 1: Notations used in this paper.
Table 2: Comparative FID values. SRAE indicatesan autoencoder with hyperspherical latent spaceand spectral regularization following Ghosh et al.
Table 3: FID values for few samples settingdensity approximation on CelebA.
Table 4: FIDs of samples generated by Nystrom-approximated NTK-kPF on CelebA. ndenotes the size of the training data subset we consider in computing kPF, while v denotesthe size of selected landmark points for NyStrOm approximation. Without approximation,we cannot fit the kernel matrices onto a GPU with 11GB RAM when n > 10, 000. It isworth noting that the approximated kPFs can perform similarly to the full kPF even withV < 0.05n, which indicates that Nystrom approximation does not sacrifice much in terms ofperformance while delivering significant efficiency gain.
Table 5: FID table for different kernel configurations.
Table 6: Model architecture for computer visionexperiments. Subscript denotes the number of out-put channels and superscript denotes the windowsize of the convolution kernel. Batch normaliza-tion and activation is applied between each pair ofconvolution layersTable 7: Model architecture for exper-iments for image generation on brainimaging dataset. Subscript denotesthe number of output channels. Up-sampling and downsampling are per-formed using strided convolutions.
Table 7: Model architecture for exper-iments for image generation on brainimaging dataset. Subscript denotesthe number of output channels. Up-sampling and downsampling are per-formed using strided convolutions.
