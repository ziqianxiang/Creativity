Table 1: Disentanglement quantitative results for the encoder (enc) and the decoder (dec). NΓindicates the number of separated syntactic roles, and D measures concentration in a single variable.
Table 2: Resampling a specific latent variable for a sentence. The ID column is an identifier for theexample.
Table 3: Swapping the value of a specific latent variable between two sentences. The SSR (SwappedSyntactic Role) column indicates the syntactic role that has been swapped.
Table 4: Disentanglement results for structured latent variable models on SNLI.
Table 5: Disentanglement results for the Yelp datasetModel	I~~β~∏	Denc	I	Nrenc	∏	Ddec	I	Nrdec	ISequence VAE	-03- 0.4	- -	- -	-0.44 1.21(0.06)	-2.20 2.25(0.50)PB 一	-	-033^--		-	-ours-4	-03-	-048	-2.00	-0T8	-230	0.4	0.54	04)	3.00	)0)	0.23	03)	2.40	55)ours-8	^03^	-044~04)"^	3.80^^3T	0.17(0.04)	^280J4)^	0.4	0.57(0.26)	3.40	55)	0.15	10)	2.40	89)o -	0.42	0.29	0.086	0.049	0.7	o -	0.39	0,3	0,3	0.075	-0.3EL -	0.15	0.2	0.36	0.21	-0.6	L -	0.24	0.085	0.16	0.096	-o.3cCN -	0.0012	0	0	0.0025	-0.5	Z -	0.015	0.008	0.018	0.011	-o,2Je -	0.059	0.07	0.37	0.31	-0.4	I e -	0.25	0.059	0.13	0.082												-0.2Cy-	0.0037	0.017	0.12	0.16	-0.3	y-	0.029	0.017	0.03	0.033												-0.1€9 -	0.034	0.044	0.29	0.34	-0.2	9 -	0.085	0.042	0.081	0.083	9 -	0.013	0.036	0.21	0.29	-0.1	9 -	0.05	0.025	0.056	0.061	-0.1Cts--	0.73	0.097	0.∞88	0		Z -	0.32	0.056	0.078	0.057	-0.0£	subj	verb	dobj	Pobj	00 -v.v		subj	verb	dobj	PObj	Figure 6:	Encoder influence heatmap for
Table 6: Example syntactic role extractions from both SNLI and YelpSource	Sentence	SUbj		verb	dobj		PObjYelp	i was originally told it would take _num_ mins .	It	told	_ num _ mins	Yelp	slow , over priced ,i'll go else- where next time .	i	""gθ		Yelp	we will not be back	we			Yelp	terrible .				Yelp	at this point they were open and would be for another hour .	they			this pointSNLI	people are outside playing base- ball.		people		baseball	SNLI	two dogs pull on opposite ends of a rope .	two dogs	pull	opposite ends of a rope	a ropeSNLI	a lady lays at a beach .	a lady	lays		a beachSNLI	people are running through the streets while people watch .	people	running		the streetsSNLI	someone prepares food into bowls	someone	prepares	food	bowlsE	Training Details and Hyper-Parameter SettingsOur ADVAE’s hyper-parameters Our model has been set to be large enough to reach a low re-construction error during the initial reconstruction phase of the training. We use 2-layer Transformerswith 4 attention heads and a hidden size of 192. Contrary to Vanilla VAEs, our model seems toperform better with high values of NZ. Therefore, we set our latent vector to a size of 768, and divideit into 96-dimensional variables for our NZ = 8 model and to 192-dimensional latent variables forour NZ = 4 model. No automated hyper-parameter selection has been done afterward.
Table 7: Complete decoder disentanglement scores for SNLIModel	βJLΛ	Ddec	Nrdec	δγ dec,verb	δγ dec,subj	δγ dec,dobj	δγ dec,pobj	-03-	0.68(0.22)	2.80(0.45)	0.19(0.04)	0.35(0.18)	0.06(0.03)	0.07(0.03)ours-4	0.4	0.81(0.05)	3.00 (0.00)	0.21(0.04)	0.47(0.03)	0.06(0.02)	0.07(0.02)	-03-	0.60(0.10)	3.00 (0.00)	0.17(0.04)	0.31(0.08)	0.05(0.04)	0.07(0.04)ours-8	0.4	0.63(0.35)	2.80(0.45)	0.17(0.10)	0.32(0.18)	0.05(0.04)	0.08(0.05)	-03-	0.60(0.09)	2.40 (0.55)	0.24 .06)	0.03(0.04)	0.03(0.02)	0.31(0.03)Sequence VAE	0.4	1.28(0.24)	1.40 (0.55)	0.45(0.12)	0.23(0.02)	0.02(0.02)	0.57(0.11)	-03-	0.12(0.10)	3.00 (0.70)	0.01(0.01)	007(0.06)	0.01(001)	0.03(0.03)Transformer VAE	0.4	0.11(0.04)	3.20 (0.44)	0.03(0.02)	0.04(0.04)	0.01(001)	0.02(0.01)Table 8: Complete encoder disentanglement scores for SNLIModel	βJLΛ	DenC	Nrenc	∆Γe nc,verb	δγ enc,subj	δγ enc,dobj	δγ enc,pobj	0f3T~	1.30(0.09)	3.00 (0.00)	0.28 .05)	0.65(0.02)	0.08(0.03)	0.29(0.03)ours-4	0.4	1.46(0.33)	3.00 (0.00)	0.38 .12)	0.64(0.10)	0.14(0.04)	0.30(0.10)	-03-	1.36(0.13)	3.40 (0.89)	0.44 .12)	0.60(0.18)	0.21(0.08)	0.11(0.06)ours-8	0.4	1.44 .79)	3.40 (0.55)	0.42 .23)	0.61(0.34)	0.17(0.10)	0.23(0.16)Average Position	-	0.98 (-)	3.00 -)	。12(-)	0.70(-)	。12(-)	O04(-)G Disentanglement Heatmaps Over the Entire Range of SyntacticRoles and PoS TagsWe report decoder and encoder heatmaps for all the syntactic roles following the Stanford Depen-
Table 8: Complete encoder disentanglement scores for SNLIModel	βJLΛ	DenC	Nrenc	∆Γe nc,verb	δγ enc,subj	δγ enc,dobj	δγ enc,pobj	0f3T~	1.30(0.09)	3.00 (0.00)	0.28 .05)	0.65(0.02)	0.08(0.03)	0.29(0.03)ours-4	0.4	1.46(0.33)	3.00 (0.00)	0.38 .12)	0.64(0.10)	0.14(0.04)	0.30(0.10)	-03-	1.36(0.13)	3.40 (0.89)	0.44 .12)	0.60(0.18)	0.21(0.08)	0.11(0.06)ours-8	0.4	1.44 .79)	3.40 (0.55)	0.42 .23)	0.61(0.34)	0.17(0.10)	0.23(0.16)Average Position	-	0.98 (-)	3.00 -)	。12(-)	0.70(-)	。12(-)	O04(-)G Disentanglement Heatmaps Over the Entire Range of SyntacticRoles and PoS TagsWe report decoder and encoder heatmaps for all the syntactic roles following the Stanford Depen-dencies (SD; De Marneffe & Manning, 2008) annotation scheme of Ontonotes, which was used totrain our Spacy2 parser, in Figures 9 and 10. For the sake of extensiveness and to make sure wedid not draw results from some parser biases, we also report the same heatmaps but using UDPipe2.0 (Straka, 2018), which uses UD type annotations12, in Figures 13 and 14. Finally, we also report12A widely adopted annotation scheme derived from Stanford Dependencies.
Table 9: More examples where we resample a specific latent variable for a sentence.
Table 10: Reconstruction loss and Kullback-Leibler values.
Table 11: Disentanglement quantitative results on SNLI for a larger grid of Nz values.
