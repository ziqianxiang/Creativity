Table 1: Comparison with ERM baseline results from Gulrajani & Lopez-Paz (2020). We compareour approach with the baseline in terms of the OOD accuracy Accood. Note that our approachamounts to fine-tuning SWSL-ResNext101-32x4d (Yalniz et al., 2019) with different learning ratesand employing the models selection procedure described in Section 2. Note that, for each benchmark,we treat one of the four domains as the OOD domain and fine-tune the model on the remaining threedomains. We report results for all four choices for the OOD domain on each benchmark.
Table 2: Evaluation of four techniques (label smoothing, AutoAugment, PatchGaussian, and SAM)for OOD generalization. We use the same pre-trained model (SWSL-ResNext101-32x4d) acrossall settings. The number inside the parentheses after the method name represents the value of thetechnique-specific hyperparameter, e.g., PatchGaussian (1.0) corresponds to employing PatchGaus-sian (Lopes et al., 2019) with σ = 1.0. We highlight the best two OOD accuracies for each datasetwith bold text.
Table 3: Top-1 accuracy and Top-5 accuracy of pre-trained models considered in this paper.
Table 4: Comparing ImageNet pre-trained models with different model sizes. We evaluate bothin-distribution accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy).
Table 5: Comparing BiTm models with different model sizes. We evaluate both in-distributionaccuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy).
Table 6: Comparing SWSL models with different model sizes. We evaluate both in-distributionaccuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy).
Table 7: Comparing ViTs models with different model sizes. We evaluate both in-distributionaccuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy).
Table 8: Comparing models trained from scratch with different model sizes. We evaluate bothin-distribution accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy).
Table 9: In-discribution accuracy evaluation of four techniques (label smoothing, AutoAugment,PatchGaussian, and SAM) for OOD generalization. We use the same pre-trained model (SWSL-ResNext101-32x4d) across all settings. The number inside the parentheses after the method namerepresents the value of the technique-specific hyperparameter, e.g., PatchGaussian (1.0) correspondsto employing PatchGaussian (Lopes et al., 2019) with σ = 1.0.
Table 10: Out-of-discribution accuracy evaluation of four techniques (label smoothing, AutoAugment,PatchGaussian, and SAM) for OOD generalization. We use the same pre-trained model (SWSL-ResNext101-32x4d) across all settings. The number inside the parentheses after the method namerepresents the value of the technique-specific hyperparameter, e.g., PatchGaussian (1.0) correspondsto employing PatchGaussian (Lopes et al., 2019) with σ = 1.0. We highlight the best two OODaccuracies for each dataset with bold text.
Table 11: In-discribution accuracy evaluation of four techniques (label smoothing, AutoAugment,PatchGaussian, and SAM) for OOD generalization. We use the same pre-trained model (SWSL-ResNext101-32x4d) across all settings. The number inside the parentheses after the method namerepresents the value of the technique-specific hyperparameter, e.g., PatchGaussian (1.0) correspondsto employing PatchGaussian (Lopes et al., 2019) with σ = 1.0.
