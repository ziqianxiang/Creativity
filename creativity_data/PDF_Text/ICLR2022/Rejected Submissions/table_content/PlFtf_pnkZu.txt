Table 1: Comparison of different models. X/Y : source/target input. Layer-Wise: layer-wise coordination;TopOnly: use topmost-layer source encodings; Src-Src Mask: the intra-source masking schema, either fullyvisible (Full) or causal (Causal); Parameter Sharing: share parameters between source and target.
Table 2: Statistics of different datasets. M/B: million/billion; SO/TO: source-original/target-original test sets;Web: in-house web-crawled datasets; BIL/MUL: the data is used for bilingual/multilingual experiments.
Table 3: Translation quality of different models for En→XX, XX→En and zero-shot language pairs on OPUS-100. Models are trained in the Transformer big setting, aligned with 14-layer EncDec, containing about 412Mparameters (excluding embedding and softmax layers). During training, we perform oversampling with a tem-perature of 5. We list average BLEU for High, Med, Low and All language groups. We also show averageBLEU and translation language accuracy (ACC) for zero-shot test sets. D: LMs + Deep; W: LMs + Wide.
