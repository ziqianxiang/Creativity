Table 1: Performance comparison between our proposed approach BWCP and other methods onCIFAR-10. “Baseline Acc.” and “Acc.” denote the accuracies of the original and pruned models,respectively. “Acc. Drop” means the accuracy of the base model minus that of pruned models (smalleris better). “Channels J”, “Model Size J”, and “FLOPs J” denote the relative reductions in individualmetrics compared to the unpruned networks (larger is better). ‘*’ indicates the method needs a extrafine-tuning to recover performance. The best-performing results are highlighted in bold.
Table 2: Performance of our proposed BWCP and other pruning methods on ImageNet using basemodels ResNet-34 and ResNet-50. ‘"‘indicates the pruned model is fine-tuned.
Table 3: Effect of BW, Gumbel-Softmax (GS),and sparse Regularization in BWCP. The resultsare obtained by training ResNet-56 on CIFAR-10dataset. ‘BL' denotes baseline model.
Table 4: Effect of regularization strength λ1 andλ2 with magnitude 1e - 4 for the sparsity loss inEqn.(7). The results are obtained using VGG-16on CIFAR-100 dataset._________________________λ1	λ2	Acc. (%)	Acc. Drop	FLOPs J (%)1.2	0.6	73.85	-0134-	33.531.2	1.2	73.66	-0.15	35.921.2	2.4	73.33	0.18	54.190.6	1.2	74.27	-0.76	30.672.4	1.2	71.73	1.78	60.75NVIDIA GTX1080Ti. We evaluate the inference time using ResNet-50 with a mini-batch of32 (1) onGPU (CPU). GPU inference batch size is larger than CPU to emphasize our method‘s acceleration onthe highly parallel platform as a structured pruning method. We see that BWCP has 29.2% inferencetime reduction on GPU, from 48.7ms for base ResNet-50 to 34.5ms for pruned ResNet-50, and21.2% inference time reduction on CPU, from 127.1ms for base ResNet-50 to 100.2ms for prunedResNet-50.
Table 5: Effect of the number of BW modules onCIFAR-10 dataset trained with ResNet-56. ‘# BW’indicates the number of BW. More BW modulesin the network would lead to a lower recognitionaccuracy drop with comparable computation con-sumption.
Table 6: Runing time comparison during training between BWCP, vanilla BN and SCP. The proposedBWCP achieves better trade-off between FLOPs reduction and accuracy drop although it introducesa little extra computational cost during training. ‘F’ denotes forward running time (s) while ‘F+B’denotes forward and backward running time (s). The results are averaged over 100 iterations. TheGPU is NVIDIA GTX1080Ti. The CPU type is Intel Xeon E5-2682 v4.
Table 7: Performance of our BWCP on different base models compared with other approaches onCIFAR-100 dataset.______________________________________________________________Model	Mothod	Baseline Acc. (%)	Acc. (%)	Acc. Drop	Channels J (%)	Model Size J (%)	FLOPs J (%)	Slimming* (Liu et al., 2017)	77.24	-74.52-	2.72	60	29.26	47.92ResNet-164	SCP (Kang & Han, 2020)	77.24	76.62	0.62	57	28.89	45.36	BWCP (Ours)	77.24	76.77	0.47	41	21.58	39.84	Slimming* (Liu et al., 2017)	74.24	-73.53-	0.71	60	54.99	50.32DenseNet-40	Variational Pruning (Zhao et al., 2019)	74.64	72.19	2.45	37	37.73	22.67	SCP (Kang & Han, 2020)	74.24	73.84	0.40	60	55.22	46.25	BWCP (Ours)	74.24	74.18	0.06	54	53.53	40.40VGGNet-19	Slimming* (Liu et al., 2017)	72.56	-73.01-	-0.45	50	76.47	38.23	BWCP (Ours)	72.56	73.20	-0.64	23	41.00	22.09	Slimming* (Liu et al., 2017)	73.51	-73.45-	0.06	40	66.30	27.86VGGNet-16	Variational Pruning (Zhao et al., 2019)	73.26	73.33	-0.07	32	37.87	18.05	BWCP (Ours)	73.51	73.60	-0.09	34	58.16	34.46where ∂L(∑n2) denotes the gradient w.r.t. X back-propagated through ∑n2_1obtain the gradient w.r.t. ∑n2 as given byTo calculate it, we first	∂ L	=Y γ∂^T+β ∂fT	(20)
