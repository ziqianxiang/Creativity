Table 1: MNIST/F-MNIST comparison for plain, AT, GAIRAT, WMMR (αtrain = 0.1, αtest = 2),MAIL (γ = 5, β = 0.05) and BiLAW using standard robust loss. Best result is underlined andbolded and second best is bolded.
Table 2: CIFAR-10 comparison for AT, GAIRAT, WMMR, non-parametric weighting, and BiLAWwith standard adversarial training (BiLAW) and with TRADES loss (BiLAW-TRADES). We performAA on 1000 samples in the test set. Best result is underlined and bolded and second best is bolded.
Table 3: CIFAR-100 comparison for AT,GAIRAT, WMMR, and BiLAW with theTRADES loss. We perform AA on 1000samples in the test set. The best resultis underlined and bolded and the secondbest is bolded.
Table 4: Architectures for main experiments for number of classes nc.
Table 5: Architectures for main experiments for number of classes nc.
Table 6: Ablation experiments: capacity of the axuiliary weight prediction networkCapacity of ω		CIFAR10				Clean	PGD	PGD - Clean128 hidden units	86.1	58.9	27.2	―64	83.6	57.4	26.2	二64 — 64	85.8	58.6	27.1256		85.7	57.7	28	In Table 6, we evaluate the influence of the auxiliary network capacity, i.e. the choice of ω. Aswith training robust classifiers, the architecture of the network influences the clean-robust tradeoff,with smaller capacity networks reducing the gap between clean and robust performance, and largernetworks increasing the gap.
Table 7: Ablation experiments: TRADES coefficient 1∕λ1∕λ	CIFAR10		Clean	PGD1∕λ = 6	86.1	58.91∕λ = 1	87.4	52.51∕λ = 5	86.9	57.61∕λ = 10	83.8	57.9Table 8: Ablation experiments: ∆, input to the auxiliary networkNetwork input	CIFAR10		Clean	PGDmulticlass margin (Def. 3)	86.1	58.9margin (Def. 2)	84.1	54.6^ '	86.9	57.9^ '-'		85.4	53.8In Table 8, we show that the choice of input to the auxiliary neural network to predict the sampleweights has a significant impact. In particular, we show the necessity of using the multi-class marginto achieve superior clean and robust test accuracy. Surprisingly, conditioning the weight on the robustloss also leads to good performance, better than the margin , and employing a learnable map for eitherthe class-aware and class-unaware outperforms heuristic methods (e.g., WMMR and MAIL).
Table 8: Ablation experiments: ∆, input to the auxiliary networkNetwork input	CIFAR10		Clean	PGDmulticlass margin (Def. 3)	86.1	58.9margin (Def. 2)	84.1	54.6^ '	86.9	57.9^ '-'		85.4	53.8In Table 8, we show that the choice of input to the auxiliary neural network to predict the sampleweights has a significant impact. In particular, we show the necessity of using the multi-class marginto achieve superior clean and robust test accuracy. Surprisingly, conditioning the weight on the robustloss also leads to good performance, better than the margin , and employing a learnable map for eitherthe class-aware and class-unaware outperforms heuristic methods (e.g., WMMR and MAIL).
