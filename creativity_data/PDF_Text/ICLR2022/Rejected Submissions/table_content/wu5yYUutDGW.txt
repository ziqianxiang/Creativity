Table 1: Comparison with other algorithms. f and ∣ denote that the numbers are copied from (Raoet al., 2020) and (Huang et al., 2020), respectively. ? indicates the methods exploiting additionalmodalities or semantics (e.g., audio, place, cast). The best numbers are highlighted in bold.
Table 2: Average precision (AP) comparison with pre-training baselines. Note that SimCLR (NN)corresponds to our reproduced ShotCoL using SimCLR as the constrastive learning scheme.
Table 3: Ablation study on varying combinations of pretext tasks for pre-training. The best scores are highlighted in bold.									Pretext Tasks					Evaluation Metric					SSM	CGM	PP	MSM	AP	mIoU	AUC-ROC	F1	SumP1	X				42.57 ±0.29z	40.12 ±0.50	84.11 ±0.15	30.83 ±0.79	197.63P2		X			36.76 ±0.02	40.59 ±0.18	82.06 ±0.04	30.94 ±0.32	190.35P3			X		36.55 ±0.04	39.58 ±0.05	81.36 ±0.03	29.96 ±0.04	187.45P4				X	13.33 ±0.23	29.80 ±0.39	64.65 ±0.98	18.68 ±0.39	126.45P5	X	X			-55.77 ±0.05	48.19 ±0.21	90.19 ±0.03	43.17 ±0.39	237.32P6	X		X		56.04 ±0.08	49.00 ±0.16	90.13 ±0.02	44.74 ±0.29	239.91P7		X	X		38.09 ±0.03	41.25 ±0.10	82.85 ±0.01	32.24 ±0.24	195.43P8	X			X	54.39 ±0.07	47.54 ±0.18	89.72 ±0.03	42.48 ±0.22	234.13P9		X		X	39.49 ±0.04	41.71 ±0.12	83.27 ±0.02	32.85 ±0.20	197.32P10			X	X	38.53 ±0.07	40.85 ±0.15	82.78 ±0.04	31.47 ±0.16	193.63P11		X	X	X	-41.02 ±0.07	40.89 ±0.10	83.79 ±0.02	31.53 ±0.18	197.23P12	X		X	X	56.10 ±0.08	49.10 ±0.17	90.09 ±0.03	45.42 ±0.30	240.71P13	X	X		X	56.20 ±0.06	48.00 ±0.17	90.13 ±0.01	43.24 ±0.27	237.57P14	X	X	X		56.26 ±0.02	48.42 ±0.33	90.25 ±0.01	43.98 ±0.58	238.91P15	X	X	X	X	-56.26 ±0.04	49.50 ±0.11	90.27 ±0.02	45.70 ±0.24	241.73Table 4: Ablations to check the impact of pseudo-boundary discovery strategies, the number ofneighboring shots and longer pre-training. The best scores are in bold.
Table 4: Ablations to check the impact of pseudo-boundary discovery strategies, the number ofneighboring shots and longer pre-training. The best scores are in bold.
Table 5: Comparison of existing video scene segmentation datasets. Note that we brought the tablefrom (Rao et al., 2020) with an update on the MovieNet-SSeg dataset.
Table 6: Comparison between the shot-level pre-training and the proposed pre-training approaches.
Table 7: Comparison between our method and shot-level pre-training baselines on BBC and OVSDdatasets. The numbers mean AP.
Table 8: Scene clustering quality measured by normalized mutual information (NMI) metric.
Table 9: Ablation study on the combination of boundary-aware pretext tasks measured by NMI.
