Table 1: WideResNet28-10 trained on C100. See Appendix C for the cross-validated hyperparameters.
Table 2: WideResNet28-10 trained on C10. See Appendix C for the cross-validated hyperparameters.
Table 3: Evaluation of our method considering a version without any regulariser to bound localsmoothness (Mix-MaxEnt), using Spectral Normalization (Mix-MaxEnt-SN) and using Stable RankNormalization (Mix-MaxEnt-SRN). The hyperparameters are set as in Table 4.
Table 4: Hyperparameters selected via cross-validation (90/10 split with stratified sampling) forall the methods we trained from scratch. We tuned T and σ0 by minimising the ECE. All otherhyperparameters have been tuned to maximise the Accuracy.
Table 5: ResNet50 trained on C10. The cross-validated hyperparameters are provided in Appendix C.
Table 6: ResNet50 trained on C100. The cross-validated hyperparameters are provided in Appendix C.
Table 7: Calibration metrics for all networks and all datasets, without temperature scalingleaves the ranking among methods mostly unchanged. Details about the cross-validation procedureused and the temperature T used are in Section C.
Table 8: OOD detection performance considering MaximUm Probability Score (MPS), Desmpter-Shafer/Energy (DS) and Entropy (H) as Uncertainty metrics.
Table 9: OOD detection performance when using Feature Density-based estimation methods. Theresults are reported for WideResNet28-10, trained on CIFAR10. Stabilisation legend: alone denotesonly the addition of I , LRP denotes Low-Rank Pseudoinverse, γ is the fraction of the explainedvariance.
Table 10: OOD detection performance when using Feature Density-based estimation methods. Theresults are reported for WideResNet28-10, trained on CIFAR100. Stabilisation legend: alonedenotes only the addition of I, LRP denotes Low-Rank Pseudoinverse, γ is the fraction of theexplained variance.
Table 11: WideResNet28-10, Mix-MaxEnt with low α or controlled interpolation factorF Additional visualization of the Maximum Entropy effect ofMix-MaxEntIn Figure 7 we report plots similar to the ones in Section 5.2.2. As already explained, Mixup makesthe network less confident both close and far away from the training data, for this reason it results inbetter calibration (as it alleviates the overconfidence of DNN) but worse OOD detection performance(because both IND and OOD samples have higher entropy, hence are more difficult to distinguish).
Table 12: WideResNet28-10, loss: CE(PI,y) - H(PO) where H(PO) is the entropy over all thelabels.
Table 13: WideResNet28-10, Mix-MaxEnt (+SRN) mixing samples both within and between classesMethods	Accuracy (↑) ECE ⑴ AdaECE ⑷	C-Accuracy (↑) C-ECE ⑷ C-AdaECE ⑴	CIFAR	SVHN AUROC (↑) AUPR (↑) AUROC (↑) AUPR (↑)C10WRN Iα =0.1	96.49	1.88	1.89	78.45	13.57	13.55	89.39	89.64	96.87	98.43α =0.2	96.21	2.08	2.05	77.63	14.38	14.36	89.76	89.72	96.51	98.11α =0.3	96.69	1.67	1.63	77.91	13.41	13.39	90.47	90.38	98.26	99.19α =0.4	96.59	1.74	1.74	78.23	13.14	13.12	90.07	90.03	97.47	98.84α =15	96.59	1.78	1.76	75.85	16.60	16.58	89.15	89.47	95.96	98.13α =20	96.51	1.82	1.82	77.33	14.55	14.53	89.64	89.90	94.55	97.37α =30	96.67	1.78	1.74	76.53	15.79	15.78	89.48	89.80	97.84	98.86C100 WRN Iα =0.3	82.01	5.58	5.48	53.11	19.72	19.68	81.04	77.20	84.51	91.43α =0.4	82.12	5.73	5.61	54.00	17.08	17.04	80.45	77.14	88.57	93.99α =5	82.32	4.83	4.79	52.13	18.01	17.97	80.68	76.98	87.11	92.89α =15	82.06	5.02	4.89	51.97	16.96	16.91	81.85	78.40	82.17	90.41α =20	81.73	5.54	5.37	52.65	19.96	19.92	82.05	78.29	79.10	86.44α =30	82.05	5.16	5.09	52.47	18.14	18.09	80.17	76.81	82.91	89.73Table 14: WideResNet28-10, Mix-MaxEnt with only within-class mixing. The missing α hyperpa-rameters are still training.
Table 14: WideResNet28-10, Mix-MaxEnt with only within-class mixing. The missing α hyperpa-rameters are still training.
