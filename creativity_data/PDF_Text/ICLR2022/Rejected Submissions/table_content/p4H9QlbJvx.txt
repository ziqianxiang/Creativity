Table 1: Top-1 accuracy comparison of different implementations of the L1-norm pruning (Li et al.,2017) on ImageNet. We adopt the torchvision models as unpruned models for fair comparison.
Table 2: Top-1 accuracy comparison between scratch training (“Scratch”) and L1-norm pruning (Liet al., 2017) on ImageNet. “PR” means pruning ratio. *We adopt the official torchvision mod-els as unpruned models. “Finetuned-1” and “Finetuned-2” refers to two finetuning LR schedules(“Finetuned-1”: 90 epochs, initial 10-3, decay 30/60/75; “Finetuned-2”: 90 epochs, initial 10-2,decay 45/68). Best results are in bold, second best Underlined, each averaged by 3 random runs.
Table 3: JSVs (Jacobian singular values) of orthog-onal initialization (Saxe et al., 2014) on differenttypes of neural networks on MNIST dataset. Note,only the linear MLP network can achieve dynamicalisometry exactly (i.e., all the JSVs equal to 1).
Table 4: Mean JSV of the first 10 epochs under different finetuning settings. Epoch 0 refers to themodel just pruned, before any finetuning. Pruning ratio is 0.9. Note, with OrthP, the mean JSV is 1because OrthP can achieve exact isometry.
Table 5: Test accuracy (%) of the first 10 epochs corresponding to Tab. 4 under different finetuningsettings. Epoch 0 refers to the model just pruned, before any finetuning. Pruning ratio is 0.9.
Table 6: Summary of finetuning LR setups corresponding to the 4 proposed hypotheses in Sec. 3.3.
Table 7: Test accuracies (%) of the 4 hypotheses in Tab. 15 with MLP-7-Linear network on MNIST.
Table 8: Test accuracies (%) of pruning ResNet56 on CIFAR10. Unpruned accuracy: 93.78%. Eachsetting is randomly run 3 times. “Acc. gain” means the accuracy gain of initial LR 10-2 over 10-3.
Table 9: Test accuracies (%) of L1-norm pruning with MLP-7-ReLU network on MNIST. Accuracyof unpruned model: 98.16%. Each setting is randomly run 5 times, mean accuracy and stddevreported. Hyper-parameters: batch size 100, SGD optimizer, weight decay 0.0001, LR schedule:initial LR 0.01, decayed at epoch 30 and 60 by factor 0.1, total epochs: 90. The LR schedule is usedfor both scratch training and finetuning.
Table 10: Test accuracy comparison between 2 pruning schemes and 4 scratch training schemesin (Crowley et al., 2018). Network: WRN-40-2 (unpruned accuracy: 95.08%, params: 2.24M).
Table 11: Training setting summary in finetuning. For the SGD solver, in the parentheses are themomentum and weight decay.
Table 12: Mean JSV and test accuracies (%) of MLP-7-Linear on MNIST under different pruningratios. Each result is randomly run for 3 times. We report the mean accuracy and (std). “ft.” is shortfor finetuning.
Table 13: Test accuracies (%) of applying StrongReg to pruning MLP-7-Linear network on MNIST.
Table 14: Mean JSV of the first 10 epochs under different finetuning settings. Epoch 0 refers to themodel just pruned, before any finetuning. Pruning ratio is 0.9. Note, with OrthP, the mean JSV is 1because OrthP can achieve exact isometry.
Table 15: Test accuracy (%) of the first 10 epochs corresponding to Tab. 14 under different finetuningsettings. Epoch 0 refers to the model just pruned, before any finetuning. Pruning ratio is 0.9.
