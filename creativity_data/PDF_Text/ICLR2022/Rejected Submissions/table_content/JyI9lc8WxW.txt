Table 1: Analysis of data augmentation. Self-supervised training is performed on Cifar- 1 00,whereas the learned features are evaluated at(32×32) on CIFAR- 1 00 and FLOWERS-102. Augmen-tation techniques include variations in hue and saturation (H&S), brightness and contrast (B&C),Planckian-based chromaticity (P), and random grayscale conversions (G). The accuracy refers to theresults of the linear classifiers trained with features extracted from different backbones.
Table 2: Comparison training different contrastive learning models. Self-supervised training is per-formed on CIFAR- 1 00, whereas the learned features are evaluated at (32 × 32) on CIFAR- 1 00 andFlowers- 1 02. We reported the best configurations obtained on SimSiam model, and retrainedother two models, SimCLR and Barlow Twins, with those selected configurations.
Table 3: Analysis on downstream tasks. Self-supervised training is performed on Tiny-ImageNetat (64 × 64). The comparison has been done considering multiple datasets as the downstream task.
