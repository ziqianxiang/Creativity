Table 1: Multilingual cardiac signal captioning performance of pre-training methods. Resultsare shown on the test set across five seeds. The standard deviation is shown in brackets. For clarity,we have highlighted our method in gray. We find that RTLP performs on par with state-of-the-artlanguage pre-training methods, MLM and MARGE.
Table 2:	Number of instances (number of patients) used during training. These represent samplesizes for all 12 leads.
Table 3:	Number of language-specific tokens in each datasetDataset âˆ£ de el	en	es	fr it	PtPTB-XL I 2206	2662	1606	1950	1974	1866	201013Under review as a conference paper at ICLR 2022B	Translation DetailsIn this section, we outline the steps taken to translate the ECG reports originally found in the PTB-XL dataset. We remind readers that although these ECG reports are a mixture of English and Ger-man, they are predominantly in the latter. As a result, we treat German as the source language fromwhich we translate the reports to other languages. More specifically, we follow these steps.
Table 4: Encoder architecture used for experiments conducted on the PTB-XL dataset. P, Cin, andCout represent the kernel size, number of input channels, and number of output channels, respec-tively. A stride of 3 was used for all convolutional layers. M represents the dimension of the finalrepresentation. We only use layer 5 when performing supervised pre-training. When captioning,layer 4 outputs T temporal features.
Table 5: Decoder architecture used for experiments conducted on the PTB-XL dataset. E = 300represents the dimension of the representations from the encoder and the representations of thedecoder tokens. H = 4 represents the number of heads used in each of the self and cross-attentionmodules. Clang represents the number of tokens in a specific language.
Table 6: Batchsize and learning rates used for training. The Adam optimizer was used for allexperiments.
