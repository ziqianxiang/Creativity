Table 1: Results of baselines and proposed variants compared to state of the art, without ensembling.
Table 2: Performance compared to state of the art under different ensembling setups. p is the ratio ofoutliers inside the training data.
Table 3: An ablation of SimSiam showing the importance of both the normalization schemes proposedin Section 2.3 and 2.4, especially for in low batch-size training.
Table 4: OOD detection performance of various pre-trained backbones on SVHNfair comparison, we also state the relative computational budget (training or inference) for each. Theproposed feature ensembles offer big computational savings compared to CSI and STOC at roughlythe same scores. Lastly, improvements by ensembling are more significant when there are outliers inthe training data.
Table 5: One-Class Classification Summary results reported in the literature on various datasets, plussome of our results; all figures are AUC. * indicates methods trained on external additional data,which may overlap in scope with the “unseen” OOD data. ** indicates method using test-time dataaugmentation / ensembling during evaluation, which can involve drastically slower inference.
Table 6: Different feature ensembling methods: SimSiam w norm on Cifar10. Note that itemsspanning multiple columns imply summation of the corresponding features.
Table 7: Different feature ensembling methods: SimSiam w norm on Cifar100	k-Cos	k-Cos (Mah)	c-Cos	c-Cos (Mah)	GDEconv_block_2	79.62	82:40	64.71	82.32	conv_block_3	81.08	80.22	73.38	80:14	conv_block_4	88.92	89.47	82.83	89.42	conv_block_1 (1x1)	71.63	71:53	68.56	68:12	68.17conv_block_2 (1x1)	73.85	72.67	70.60	70:12	69.79conv_block_3 (1x1)	74.97	7700	68.97	74:47	73.50conv_block_4 (1x1)	86.68	89:27	75.68	88.43	88.39head_layer_1	73.88	8105	48.36	80:63	head_layer _2	65.19	72.33	—	41.26	46.18	—	-All Conv blocks-		90.65				89.61			-All Conv blocks-		90.57					All blocks	87.37	90.16	75.65	89.12	All blocks		90.20				89.10			All blocks	90.35				Ens.	90.3	—				Table 8: Different feature ensembling methods: SimCLR w norm on Cifar1017Under review as a conference paper at ICLR 2022
Table 8: Different feature ensembling methods: SimCLR w norm on Cifar1017Under review as a conference paper at ICLR 2022Figure 7: Same experiment as Figure 3 but after adding the normalization proposed in Section 2.3.
Table 9: CIFAR10 Per class results18Under review as a conference paper at ICLR 2022	p=0	p=0.05	p=0.10	∆ (p=0.1-p=0)SimSiam(W norm)	91.54	88.08	86.21	5.33SimSiam(w/o norm)	89.56	85.21	81.79	777SimCLR(no neg, W norm)	89.27	83.1	77.49	rrτ8SimCLR(no neg, w/o norm)	86.49	71.1	63.5	25:99SimCLR (neg aug, W norm)	92.91	86.1	83.38	953SimCLR (neg aug, w/o norm)	91.12	81.5	77.94	13.18Pretrained IN18	92.63	90.9	89.3	3.33Pretrained IN50	-	94.45	92.18	89.49	4.96Pretrained IN152	95.82	94.48	91.39	4.43DROC (Sohn et al., 2020)	89	76.5	73.0	16.0	DROC (DA)(SOhn et al., 2020)		92.5	85.0	80.5	12.0CSI (results from (Han et al., 2021))	94.3	88.2	84.5	9.8Semi-Supervised (5% labeled) ELSA (Han et al.)	85.7	83.5	81.6	41Semi-Supervised (5% labeled) ELSA+ (Han et al.)	95.2	93.0	91.1	4.1Table 10: CIFAR10 pollution experiments. p is the ratio of outlier data inside the training set. the lastcolumn is the loss in performance between training with clean data and a 10% polluted data. Clearly,
Table 10: CIFAR10 pollution experiments. p is the ratio of outlier data inside the training set. the lastcolumn is the loss in performance between training with clean data and a 10% polluted data. Clearly,our proposed modifications reduces the drop in all cases. With SimSiam we beat standard ELSAwhich uses 5% labeled data to maintain robustness to pollution, and come close to ELSA+, whichuses TTA and other tricks from CSI on top of the baseline SSL approach.
Table 11: Detailed ablation study. Here p is the ratio of outlier data inside the training set. Norm iswhether normalization is applied or not. Ens. is our proposed feature ensembling. Best results are inbold. Second best results are underlined.
