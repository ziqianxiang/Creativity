Table 1: Results for the original CIFAR-10 test set, CIFAR-10.1, and CIFAR-10-C. MEME outper-forms TTT despite not making any training assumptions. ?Results from Sun et al. (2020).
Table 2: Test results for ImageNet-C, ImageNet-R, and ImageNet-A. MEME achieves new state-of-the-art performance on each benchmark for ResNet-50 models for the single test point setting. ForRVT*-small, MEME substantially improves performance across all benchmarks and reaches a newstate of the art for ImageNet-C and ImageNet-R.
Table 3: Ablating the adaptation objective to test pairwise cross entropy and conditional entropy (CE)based adaptation. MEME generally performs the best, indicating that both encouraging invarianceacross augmentations and confidence are helpful in adapting the model.
Table 4: Evaluating the episodic version of Tent with a batch size of 1, which corresponds to a simpleentropy minimization approach for the test time robustness setting. This approach also uses singlepoint BN adaptation, and entropy minimization does not provide much, if any, additional gains.
Table 5: Ablating the augmentation functions to test standard augmentations (random resized croppingand horizontal flips). When changing the augmentations used, the post-adaptation performancegenerally does not change much, though it suffers the most on CIFAR-10-C.
Table 6: ImageNet-A results for the ResNext-101 models.
Table 7: Test error (%) on CIFAR-10-C level 5 corruptions.
Table 8: Test error (%) on CIFAR-10-C level 4 corruptions.
Table 9: Test error (%) on CIFAR-10-C level 3 corruptions.
Table 10: Test error (%) on CIFAR-10-C level 2 corruptions.
Table 11: Test error (%) on CIFAR-10-C level 1 corruptions.
