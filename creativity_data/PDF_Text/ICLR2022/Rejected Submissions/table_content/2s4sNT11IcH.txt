Table 1: Effects of different per-sample clippings on deep learning with DP-GD, assumingno screening happens in global clipping. Here “Yes/No” means guaranteed or not and theloss refers to the training set. “Loss convergence” is conditioned on H(t) 0 (see (2.1)).
Table 2: Linear algebra properties of NTK by different clipping methods, assuming noscreening happens in global clipping. Here ‘Yes/No’ means guaranteed or not.
Table 3: Calibration metrics ECE and MCE by non-DP (no clipping) and DP optimizers.
