Table 1: Time complexity and computation breakdown of ViT (Dosovitskiy et al., 2020) and DeiT(Touvron et al., 2021). L is the number of transformer blocks, N is the number of input tokens(patches), and M is the feature dimensionality. All models take input images of size 224 × 224except ViT-B/384, which uses 384 × 384. The softmax-attention layers constitute a fraction (15% orless) of the total compute, whereas fully-connected layers (MLP and projections) spend over 80%.
Table 2: Best cost-accuracy trade-off achieved by PoWER-BERT and the proposed Token Poolingvia varying sparsity level and feature dimensionality.
Table 3: ResUlts of aPPlying Token Pooling and PoWER-BERT on DeiT-S model. The models aregroUPed by K described in APPendix C. The integer list denotes the maximal nUmber of tokens re-tained after each transformer block. These nUmbers do not take into accoUnt the classification token,which is always retained. ThUs, “0” means that only the classification token remains. AdditionalfloPs (denoted by the Parentheses) are dUe to clUstering.
Table 4: ResUlts of aPPlying Token Pooling and PoWER-BERT on DeiT-e318 model. The mod-els are groUPed by K described in APPendix C. The integer list denotes the maximal nUmber oftokens retained after each transformer block. These nUmbers do not take into accoUnt the classifi-cation token, which is always retained. ThUs, “0” means that only the classification token remains.
Table 5: ResUlts of aPPlying Token Pooling and PoWER-BERT on DeiT-e252 model. The mod-els are groUPed by K described in APPendix C. The integer list denotes the maximal nUmber oftokens retained after each transformer block. These nUmbers do not take into accoUnt the classifi-cation token, which is always retained. ThUs, “0” means that only the classification token remains.
