Table 1: Quality of the Taylor series approximation. For bothdata sets, we trained a (sampling) VAE and evaluate the Pearsoncorrelation between the true and estimated variances. For theCelebA data set, we also evaluate the Frobenius norm betweenthe true and estimated covariance matrices. We evaluate ourapproximation and compare it to different numbers of samples.
Table 2: Quantitative evaluation for the sampling-freeVAE, where we include the performance of the sampling-free VAE with shorter training such that the total trainingbudget matches the total full training time of the sam-pling VAE. Results are reported for a 16 dimensionallatent space and averaged over 3 runs.
Table 3: Evaluation on MNIST (left) and Omniglot (right) with different methods and numbers ofsamples. The metric is the NLL on the test set (smaller is better) averaged over 5 runs.
Table 4: Posterior collapse experiment: VAEs trained on MNIST with a 200-dimensional latentspace. Full batch training on a subset of 1 000 MNIST images for a controlled setting where forthe sampling-free VAE no stochastic effects are present, and for the sampling approach only thestochasticity of the reparameterization is present. Results are averaged over 10 runs.
Table 5: Table 2 but with standard deviations.
