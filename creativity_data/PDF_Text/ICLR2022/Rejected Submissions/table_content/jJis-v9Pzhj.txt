Table 1: Test accuracy and standard deviation over five (ten for STL-10) runs in brackets on various datasetsof recent methods for PU learning. The row ”+PL” refers to an uncertainty-unaware pseudo-labeling baselinethat uses the average prediction of an ensemble of networks as selection criterion, while the bottom row refersto our uncertainty-aware solution. We used 1,000 labeled positives for training except for CIFAR-10, wherewe also report the performance with 3,000 labeled positives, and STL-10, where we used the official ten cross-validation folds. Bold font indicates highest performance. * scores as reported by Chen et al. (2020b) and Huet al. (2021) due to unavailability of source code.
Table 2: Test accuracy of our framework on the CIFAR-10 dataset with a selection bias on the positive labelswhen using the nnPU and nnPUSB losses. Our framework improves over the base PU loss in both cases; inparticular, PUUPL with nnPU loss achieved better performance than the nnPUSB loss alone.
Table A.3: Network architecture used for the CIFAR-10 experimentsDataset	Tot. Pos.	Tot. Neg.	Train Lab.	Train Unlab.	Val. size	Test SizeMNIST	30,508	29,492	-^1,000^^	54,000	5,000	10,000F-MNIST	30,000	30,000	1,000	54,000	5,000	10,000CIFAR-1。	20,000	30,000	1,000	44,000	5,000	10,000STL-1。	?	?	3,600	105,400	5,000	8,000IMDb	12,500	12,500	1,000	19,000	5,000	25,000Table B.1: Total size of the datasets and the data splits.
Table B.1: Total size of the datasets and the data splits.
