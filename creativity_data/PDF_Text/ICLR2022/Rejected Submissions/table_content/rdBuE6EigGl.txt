Table 1: List of all the hyperparameters and the search range associated with each of them. Those marked with an asterisk (*) refer to the dual architectures only.		Name	Description	ValuesNum epochs	Number of training epochs.	{100, 300}Learning rate	Learning rate.	[10-6, 10-3]Batch size	Batch size.	{32, 128}Seq len	Sequence length.	{10, 25, 50}Embedding units	Size of the embedding layer.	{400, 850}Recurrent units	Size of the recurrent layers.	{400, 850, 1150}LSTM layers	Number of recurrent layers.	{1, 2, 3}Dual units *	Size of the dual layer.	{400, 850}Embedding L2reg	L2 regularization applied to the Embedding and output layers.	{0, 10-6, 10-5}Rec. input L2reg	L2 regularization applied to the input weights of the recurrent layer.	{0, 10-6, 10-5}Rec. L2reg	L2 regularization applied to the recurrent weights of the recurrent layer.	{0, 10-6, 10-5}Activation L2reg	L2 regularization applied to the recurrent layers output.	{0, 10-6, 10-5}Dual L2reg*	L2 regularization applied to dual layer.	{0, 10-6, 10-5}Rec. input Dropout	Dropout before the first recurrent layer.	[0.0, 0.5]Rec. Dropout	Dropout for the linear transformation of the recurrent state.	[0.0, 0.5]Rec. internal Dropout	Dropout between the recurrent layers.	[0.0, 0.5]Rec. output Dropout	Dropout after the last recurrent layer.	[0.0, 0.5]Dual input Dropout*	Dropout before the dual layer.	[0.0, 0.5]
Table 2: Validation and test word-level perplexity obtained for each of the experimental configura-tions on the PTB (top) and the WT2 (bottom) datasets.
Table 3: Best validation and test word-level perplexity scores reported in the literature for the PennTreebank dataset, with and without dynamic evaluation. Missing values in the last two columnscorrespond to works where the dynamic evaluation approach was not considered. The last row inthe table displays the results obtained with our Dual mdLSTM network.
