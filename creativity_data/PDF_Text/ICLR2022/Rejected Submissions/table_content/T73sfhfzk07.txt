Table 1: NotationModel mistakes come from different sources: irreducible errors, low model capacity, wrong modelingassumptions, sample distribution shift, and so on. For each type of errors, we define a formal problemsetup and develop methods to deal with it. Those setups are not mutually exclusive and their methodscould complement each other.
Table 2: List of used datasetsDataset	Train size	Validation size	Test size	Outlier for	ReferenceCIFAR-Id~~	50000 ~~	500	-9500-	—	(15)ImageNet-IK	≈ 1.28 * 106-	3000	47000	—	(27)tiny ImageNet	—	500	-9500-	CIFAR-10~~	(27)ImageNet-O	—	600	-1400-	ImageNet-IK	(12)ImageNet-R	—	600	29400	ImageNet-IK	(8)ImageNet-A	—	600	-6900-	ImageNet-IK	(12)ImageNet-C	—	3000 —	47000	ImageNet-IK	(9)In our experimental study out-of-distribution detectors are applied to popular neural network archi-tectures trained on CIFAR-10 or ImageNet-1K collections, such as ResNet-18 and ResNet-50. Thedetailed information about the models used for each setup could be found in Tab. 3. All experimentswere performed with the PyTorch framework, due to a blind-review we attached source code tosupplementary materials and will provide the link to the repository with the camera-ready paperversion.
Table 3: List of used neural networksArchitecture	Training set	Number of parameters	Accuracy (test set)	ReferenceWide-ReSNet-28-10	CIFAR-10	≈ 36.5M	962	(29)ReSNet-18	ImageNet-IK	≈ 11.5M	697	(6)ReSNet-50	ImageNet-IK	≈ 25.5M	76.1	(6)The following out-of-distribution detectors were used as competitors to our method:1.	Maximum Softmax Probability (MSP), also referred as a baseline, described in (10). Thismethod uses negative maximum softmax probability as an out-of-distribution score.
Table 4: Wide-Resnet-28-10. CIFAR-10 vs tiny-ImageNet. ROC-AUC values.
Table 5: ImageNet-like datasets. ROC-AUC values.
Table 6: Ablation study for G-part. ROC-AUC values.
Table 7: Ablation study for SG-part. ROC-AUC values.
