Table 1: Classification results on the synthetic dataset.
Table 2: Results in the distillation setting.
Table 3: Results on the CIFAR-10/CIFAR-100 datasets.
Table 4: Results on several natural language understanding tasks in the GLUE benchmark.
Table 5: Active learning on CIFAR-100. Terms rnd, mgn,and k-ctr refer to random, margin, and k-center, respectively.
Table 6: Benefits of learning the best kernel.
Table 7: Effects of normalization.
Table 8: Results on the CIFAR-10/CIFAR-100 datasets with different backbones.
Table 9: Results on the transfer learning datasets.
Table 10: Results on the CIFAR-10/100 datasets with the square loss.
Table 11: Effect of rectification of the embeddings.
Table 12: Different activation functions on the coefficient vector. Note that the kernelized classifieris unstable when no activation function is used, this agrees with the theoretical analysis.
