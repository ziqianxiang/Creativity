Table 1: β vs. sketch size for convergenceβ	ɪr	"θɪ	~3Γ		^05^	^06^	0.7	~8Γ	~9Γsmallest sketch size	~09~	~08~	~0?T	~06~	~0T~	~0T~	0.3	~02^	~0T^{10, 20, 30, 40} in Fig. 1(b). It is observed that power law is not satisfied. However, in Fig. 1(c)we find that the error vectors in a variant of EFSGD, termed partial EFSGD (Abdi & Fekri, 2020)(see Alg. 4 in Apdx. F due to space limitation), follow broken power law, that is, a piece-wise linearfunction in the log-log plot. Partial EFSGD adds part of the compression error, i.e., (1 - β)eit forβ ∈ [0, 1), to stochastic gradients before compressing. As we shall see in numerical experiments,the update in partial EFSGD is helpful to save more memory when working with count sketch.
Table 2: β vs. sketch size for convergence for unscaled random block k gradient compressorβ	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9smallest sketch size			~O~	~O~	~O~	~0TT	~05~	0.3	~Q3~	~0Γ^Experiments are repeated 3 times and averaged results are re-ported. In the implementation of count sketches, the tensor trickcan be employed to avoid computing too many hashes, which im-proves the runtime. In the tensor trick, instead of using element-wise compressing, we view the error vector as a matrix, and treatone row or column as an “entry” of the compressed vector, result-ing in a tensor sketch (matrix when v = 1). More specifically,Figure 7: ConEF with row orcolumn hashes has similar per-suppose the error vector lives in Rd, whose matrix view is Rn×mwith d = mn. It takes n (resp. m) hash computation in a row-(resp. column-) tensor trick, while mn hashes are needed withoutecdowluimthno-ubtasefdortemnasnocret.ricks are not observed;this trick. Differences on test accuracy between row- orsee Fig. 7.
Table 3: Numerical results for ReSNet18Algorithm	test accuracy	runtime (min)	memory saving (MB)SGD	93:20	54:83	-rbSGD	8658	40:12	48:2EFSGD	9252	40:41	0ConEF 60%	92:53	4245	31:5ConEF 90%	92.02	—	4245 —	44:1Table 4: Numerical results for WRN-28-10Algorithm	test accuracy	runtime (min)	memory saving (MB)SGD	96ΓT2	139:70	-rbSGD	8292	7088	122:1EFSGD	95:28	7∏3	0ConEF 60%	96:00	72∏	76:2ConEF 90%	95∙23	—	7L98 —	101:5and ConEF-CS. For ConEF-CS with 60% saved memory, we choose β = 0.9; and for ConEF-CSwith 90% memory saving we use β = 0.95. For rbSGD, we test step sizes from {0.05.0.1, 0.2} and0.1 is chosen.
Table 4: Numerical results for WRN-28-10Algorithm	test accuracy	runtime (min)	memory saving (MB)SGD	96ΓT2	139:70	-rbSGD	8292	7088	122:1EFSGD	95:28	7∏3	0ConEF 60%	96:00	72∏	76:2ConEF 90%	95∙23	—	7L98 —	101:5and ConEF-CS. For ConEF-CS with 60% saved memory, we choose β = 0.9; and for ConEF-CSwith 90% memory saving we use β = 0.95. For rbSGD, we test step sizes from {0.05.0.1, 0.2} and0.1 is chosen.
Table 5: Numerical results for LSTMAlgorithm	perplexity	runtime (hour)	memory saving (MB)SGD	-9272-	757	-rbSGD	-120.07	145	2625EFSGD	-9829-	149	0ConEF 60%	-9776-	T5I	180:3ConEF 80%	98:59	L50 一	220222Under review as a conference paper at ICLR 2022Sso- u。QeP=e>2.00	5	10	15	20	25	30	0	5	10	15	20	25	30epoch	epoch(a) train	(b) validationFigure 8: Performance of ConEF with unscaled random block gradient compressor on a transformer.
