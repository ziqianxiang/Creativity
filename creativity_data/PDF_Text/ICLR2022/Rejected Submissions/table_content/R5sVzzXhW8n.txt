Table 1: Comparison of test accuracies of each method under instance-based label noise. CE (un-fixed f with random init.) is a common baseline adopted in the literature which trains a randominitialized DNN using CE loss on noisy dataset.
Table 2: Comparing CE (fixed encoder) with robust losses on CIFAR10Method	0.2	Symm 0.4	0.6		0.8SCE	87.63	85.34	80.07	53.81APL	89.22	86.05	79.78	55.06Peer	90.70	88.29	82.10	33.03CE (fixed encoder)	91.06	90.73	90.2	88.24(Zhang & Sabuncu, 2018) is not convex with respect to linear classifier. Second, if the gap betweenSSL and SL is large, then the encoder should not be fixed since supervised features can exhibit betterperformance. However, if the gap between SSL and SL is very small, it is guaranteed by Theorem1 and Theorem 2 that CE on noisy dataset can approximates SL on clean dataset when encoder isfixed. The observation can be seen in Figure 3 (a). Thus, the above argument suggests that whetherto fix the encoder or not is determined by the gap between SSL and SL for a given dataset.
Table 3: Comparing different SSL methods on CIFAR10 with symmetric label noiseMethod	Symm label noise ratio 0.2	0.4	0.6	0.8CE (fixed encoder with SimCLR init) CE (fixed encoder with MoCo init)	91.06 90.73	90.2	8824^ 91.55 91.12 90.45	88.51It can be observed that different SSL methods have very similar results, which further verifies ourTheorems in the paper.
Table 4: The best epoch (clean) test accuracy for each method on CIFAR100-N.
Table 5: Comparing CE (fixed encoder) with baseline (CE with random initialized unfixed encoder)on CIFAR10 with asymmetric label noiseMethod	Asymm label noise ratio 0.1	0.2	0.3	0.4CE (baseline, unfixed encoder with random init) CE (fixed SSL pre-trained encoder)	90.69	88.59 86.14 80TÎ“^ 90.85	89.92 88.25 82.46Instance-dependent label noise: Instance-dependent label noise behaves like symmetric labelnoise, i.e., the noisy labels may exist in all the classes. However, the noise ratio for each classis not the same and dependent on the features. From Section F, when noise ratio for each class isnot the same, the Bayes classifier is not consistent. However, from Table 1 and Section C we the-oretically and experimentally show that simple down-sampling strategy can approximate conditionin Theorem 1 and Theorem 5 which is helpful for instance level label noise.
