Table 1: DataSet statistics.
Table 2: kNN classification errors. Lower is better. Here, (x/y) uses x as the normalization and yas the metric. The last column reports the average relative performance to the normalized BOW.
Table 3: kNN ClaSSification errors With Clean data. LoWer is better. The Same notations as in Table 2.
Table 4:	kNN classification errors with all words and the original datasets. Lower is better. The lastcolumn reports the average relative performance to the normalized BOW (the first row in Table 2).
Table 5:	kNN classification errors With all Words and clean data. LoWer is better. The last columnreports the average relative performance to the normalized BOW (the first row in Table 3).
Table 6: Weighted kNN classification errors. Lower is better. The last column reports the averagerelative performance to the normalized BOW with the standard kNN (the first row in Table 2).
Table 7: kNN ClaSSification errors With Clean data. LoWer is better. The Same notations as in Table 2.
Table 8: kNN classification errors with 5 dimensional embeddings. Lower is better. The first tworows are the Same as Table 2 for reference.________________________________________________________	bbcsport	twitter	recipe	ohsumed	classic	reuters	amazon	20news	rel.
