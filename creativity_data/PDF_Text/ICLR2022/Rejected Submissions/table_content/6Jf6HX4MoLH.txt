Table 1: Comparing planning accuracy, and median time and vertices for the Point Robot Model onunseen environments of the same size as the training data.
Table 2: Comparing planning accuracy, and median time and vertices for Point Robot on maps of thedifferent sizes of the Random Forest environment.
Table 3: Comparing planning accuracy, and me- Table 4: Comparing planning accuracy, and me-						dian time and vertices for the Point Robot Model				dian time and vertices for Dubins Car Model for		on real world map.				the Random Forest Environment.		Planner	Accuracy Time (sec)		Vertices	Planner	Accuracy	Time (sec) VerticesRRT*	20/20	2.507	1630	SST	~~100%^^	3.313	1200MPT-RRT*	17/20	0.538	455	MPT-SST	87.6%	2.617	932MPT-RRT*-EE	20/20	0.946	804	MPT-SST-EE	100%	2.712	1150costmap to provide solutions in real time. Further hand-tuned heuristics are required to restrict theplanning space for kinematically constrained systems (Dolgov et al., 2010) otherwise they get stuckin local minima (LaValle & James J. Kuffner, 2001). We have shown through our experiments thatthe MPT aided planners require no such sub-sampling, and can learn to restrict search spaces byleveraging data. Since MPT is agnostic to the underlying planner, graph-based searches can also beused to search the highlighted space.
Table 5: Network architecture of the Fea-In this section, we detail the network architecture used in ture Extractorfor MPT in our experiments. The Transformer architecture is similar to the ones proposed in Vaswani et al. (2017). We used 6 layers of encoder block, each consisting of 3	Layer	Dimension	2D Convolution	[2, 6, 5, 0]heads. The dimension of the keys and queries was set at		512, and the dimension of the value was set at 256. The	2D Maxpool	[2]architecture of our feature extractor is given in Table 5.	ReLU	[6, 16, 5, 0]For the convolution layer, the dimensions in the brackets	2D Convolution	represent [Input Channel Size, Output Channel Size, Ker-	2D Maxpool ReLU Convolution	[2]nel Size, Stride], and for the Maxpool layer, it represents the Kernel Size		[16, 512, 5, 5]B.2 UNETThe UNet architecture we use is similar to the one used by Ronneberger et al. (2015). The networkconsists of 4 blocks of down convolution, and 4 blocks of up convolution. Each down convolutionblock consists of a 3 × 3 convolution, followed by batch norm, ReLU and a 2 × 2 of max pool layer.
Table 7: Comparing planning accuracy,planning time, and number of verticesin the tree for the Point Robot on adown sampled Maze EnvironmentAlgorithm	NEXT-KSAccuracy	28.28%Time (sec)	3.021Vertices	387C Position EncodingWe observed that the MPT modeltrained on the position encodingproposed by Vaswani et al. (2017)showed signs of overfitting. Themodel fails to highlight the regionnear the boundary of the trainingmap size (See Fig.9 (Left)). Oneway to resolve this overfitting is totrain on maps of various sizes toensure that the network observesdifferent position encoding. How-
Table 6: Comparing planning accuracy, planning time, andnumber of vertices in the tree for Point Robot on maps of thedifferent sizes of the Random Forest environment for modeltrained without random shifting of positional encoding.
