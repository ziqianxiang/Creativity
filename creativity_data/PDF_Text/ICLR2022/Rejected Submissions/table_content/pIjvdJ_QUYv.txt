Table 1: Test accuracy of PrivHFL on different fractions of participating clients and differ-ent numbers of query data. We also report the model accuracy trained on the local dataset (i.e.,baseline described in Section 3.1), and the model accuracy retrained on the private data-based query.
Table 2: Runtime (sec) of the three steps in PrivHFL's secure querying protocol. The runtime ofsecure prediction represents the query time of each client on different models. CIFAR10 and SVHNhave the same time due to the same input size and model architecture.
Table 3: Runtime (sec) and communication cost (MB) of secure prediction for different meth-ods on CIFAR10. CrypTFlow2 (Rathee et al., 2020) is the SOTA 2-party protocol that containstwo variants (OT-based and HE-based) and CryptGPU (Tan et al., 2021) is the SOTA 3-party GPU-friendly protocol. We report the cost of a single prediction on three models.
Table 4: Runtime (sec) of secure prediction for HE-Transformer and our PrivHFL on MNISTas the batch size increases. He-Transformer is the 2-party secure prediction protocol used in CaPC(Choquette-Choo et al., 2021).
Table 5: Impact of the coefficient λ in mixup on CIFAR10.
Table 6: Comparison with prior works on properties necessary for federated learning						Framework	Privacy		Usability		Efficiency		Data Privacy	Model Privacy	Model Heterogeneity	w/o Dataset Dependency	GPU Compatibility	Protocol EfficiencyBonawitz et al. (2017)	✓	X	X	✓	X	✓Bell et al. (2020)	✓	X	X	✓	X	✓Sav et al. (2021)	✓	✓	X	✓	X	XJayaraman & Wang (2018)	✓	X	X	✓	X	✓Li & Wang (2019)	X	✓	✓	X	✓	-Choquette-Choo et al. (2021)	✓	✓	✓	X	X	XLin et al. (2020)	X	X	✓	X	✓	-Sun & Lyu (2021)	X	✓	✓	X	✓	✓Diao et al. (2021)	X	X	✓	✓	✓	-This work	✓	✓	✓	✓	✓	✓D. 1 Heterogeneous federated learningFederated learning achieves collaboration among clients via sharing model gradients. While suc-cessful, it still faces many challenges, among which, of particular importance is the heterogeneitythat appear in all aspects of the learning process. This consists of system heterogeneity (Diao et al.,2021), model heterogeneity (Li & Wang, 2019) and statistical heterogeneity (Zhu et al., 2021).
Table 7: Experiment setting of different datasets during local model training.
