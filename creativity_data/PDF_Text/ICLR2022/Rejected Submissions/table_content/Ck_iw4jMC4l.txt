Table 1: Transfer learning from a frozen ResNet-18 architecture pretrained on ImageNet-1k to othercomputer vision datasets. An MLP head with two layers of pre-activation width of either 438, 512, or650 (depending on activation function, to keep the number of params approximately constant) wastrained, without re-training the pretrained base model. Trained with SGD, cosine annealed LR 0.01,for 25 epochs. Mean (standard error) of n=5 random initializations of the MLP (same pretrainednetwork). Bold: best. Underlined: second best. Italic: no significant difference from best (two-sidedStudent’s t-test, p> 0.05). Background: linear color scale from ReLU baseline (white) to best (black).
Table 2: Dataset summaries.
Table 3: Hyper-parameter random search parameters.
Table 4: Hidden pre-activation widths and number of trainable parameters used in transfer learningexperiments (corresponding to results shown in Table 1).
Table 5: Transfer learning with from a frozen ResNet-18 architecture pretrained on ImageNet-1k toother computer vision datasets. As with Table 1, but in this case we show results where the MLP hasa pre-activation width of w=512. Note that although the pre-activation width is constant, the numberof parameters in the network is not consistent between experiments. The number of parameters isshown in Table 6. Mean (standard error) of n=5 inits of the MLP (same pretrained network).
Table 6: Number of trainable parameters used in transfer learning experiments (corresponding toresults shown in Table 5).
Table 7: Performance of SRAN-based models on the I-RAVEN dataset (Hu et al., 2020). Bold: best.
Table 8: Performance of TMN-based networks at compositional zero-shot learning (CZSL) on theMIT-States dataset. Mean (standard error) of n=5 random initializations.
Table 9: Performance of TMNx networks at compositional zero-shot learning (CZSL) on the MIT-States dataset. Pre-activation width of 64 or 70 (depending on activation function, to keep the numberof parameters approximately constant). Mean (standard error) of n=5 random initializations. Bold:best. Underlined: second best. Italic: no significant difference from best (two-sided Student’s t-test,p> 0.05). Background: color scale from second-worst in column to best, linear with accuracy value.
