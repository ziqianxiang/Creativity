Table 1: Memory and computation scaling with sequence length during training and inference.
Table 2: Parameter counts and compute (forward pass) for one layer of the network, per token.
Table 3: LMU model details. N refers to the number of non-embedding and trainable parameters;d is the embedding dimension; q and θ define LMU’s order and the length of the sliding window;q0 is the number of rows in the L matrix. We adjust the post-FFN inner ration to obtain the rightparameter count for comparison to transformers - it's usually set to something in between 1.9 and2.1.
