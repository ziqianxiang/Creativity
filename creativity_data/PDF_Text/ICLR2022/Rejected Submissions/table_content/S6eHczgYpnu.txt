Table 1: Log-likelihood of the MDN adaptation methods on simulated channel variationsSource channel	Target channel	#Target samples	Proposed		Transfer		Transfer-last-layer				median	95% CI	median	95% CI	median	95% CI		80	-0.49	(-3.89, 0.45)	-6.97	(-17.92, -1.72)	-2.09	(-6.50, -0.21)AWGN	Uniform fading	160	-0.43	(-2.32, 0.48)	-1.65	(-4.41, -0.14)	-0.79	(-1.90, 0.13)		240	-0.58	(-1.94, 0.52)	-0.74	(-2.07, 0.25)	-0.35	(-1.32, 0.51)		320	-0.22	(-2.27, 0.63)	-0.40	(-1.35, 0.31)	-0.19	(-1.16, 0.54)		80	1.17	(0.68, 1.33)	-1.78	(-6.88, 0.22)	-0.64	(-6.86, 0.96)AWGN	Ricean fading	160	1.26	(0.51, 1.39)	0.37	(-1.10, 0.88)	0.55	(-0.71, 1.22)		240	1.31	(-0.09, 1.39)	0.91	(0.28, 1.17)	1.00	(0.42, 1.29)		320	1.27	(0.70, 1.41)	1.07	(0.73, 1.22)	1.14	(0.86, 1.32)		80	-0.53	(-3.77, 0.49)	-11.48	(-26.06, -3.15)	-5.77	(-16.20, -2.27)Ricean fading	Uniform fading	160 240	-0.10 -0.59	(-3.68, 0.74) (-5.44, 0.68)	-2.91 -1.24	(-5.48, -0.91) (-2.21, -0.22)	-1.45 -0.71	(-3.72, 0.12) (-1.43, 0.21)		320	-0.41	(-3.57, 0.68)	-0.43	(-1.51, 0.21)	-0.23	(-1.15, 0.35)Baseline Methods. We evaluate the following two baseline methods for adapting the MDN. 1)A new MDN is initialized using the weights of the MDN trained on the source dataset, and it isadapted using the target dataset. 2) Same as baseline 1, but only the weights of the final layer areadapted (fine-tuned) using the target dataset. The above methods are referred to as transfer andtransfer-last-layer respectively. We used the Adam optimization method (Kingma & Ba, 2015) for200 epochs, with a batch size of 10 or 0.01 times the target dataset size, whichever is larger. Table 2
Table 2: Number of parameters being optimized by theMDN adaptation methods.
Table 3: Commonly used notations and definitionsNotation	DescriptionS ∈ S := {1,…,m} 1s,m or simply 1s, s ∈ S x ∈ X ⊂ Rd with |X| = m y ∈ Rd K ∈{1, ∙∙∙ ,k} Eθe(1s) Dθd(y) = [Pθd(I | y), …，Pθd(m | y)]T Sb(y) = argmaxs∈S Pθd (s | y) Pθc(y I χ) φ(x) = Mθc (x) y = hθc (x, z) Z ∈ R' fθ(1s) = Dθd (hθc (Eθe (1s), z)) ψT = [ψτ,…，ψT] gxi and gx-i1, i ∈ [k], x ∈ X DKL(p, q) N(∙∣ μ, ∑) Cat(pι,…，Pk) II(C) kxkp	Input message or class label. Usually m = 2b, where b is the number of bits. One-hot-coded vector of a message s, with 1 at position s and zeros elsewhere. Encoded representation or symbol vector corresponding to an input message. Channel output that is the feature vector to be classified by the decoder. Categorical random variable denoting the mixture component of origin. Encoder NN with parameters θe mapping a one-hot-coded message to a symbol vector in Rd. Decoder NN with parameters θd mapping the channel output into probabilities over the message set. MAP prediction of the input message by the decoder. Conditional density (generative) model of the channel with parameters θc . Mixture density network that predicts the parameters of a Gaussian mixture. Transfer or sampling function corresponding to the channel conditional density. Random vector independent of x that captures the stochasticity of the channel. Input-output mapping of the autoencoder with combined parameter vector θT = [θeT, θcT, θdT]. Affine transformation parameters per component used to adapt the MDN. Affine and inverse-affine transformations between the component densities of the Gaussian mixtures. Kullback-Leibler divergence between the distributions p and q. Multivariate Gaussian density with mean vector μ and covariance matrix Σ. Categorical distribution with pi ≥ 0 and Pi pi = 1. Indicator function mapping a predicate c to 1 if true and 0 if false. `p norm of a vector x.
Table 4: Architecture of the Encoder, MDN channel,and Decoder neural networks. FC - fully connected(dense) layer;㊉ denotes layer concatenation; ELU -exponential linear unit; m - number of messages; d -encoding dimension; k - number of mixture components;nh - size of a hidden layer.
Table 5: Conditional log-likelihood of the MDN adaptation methods on test data from the targetdomain. The source and target domain datasets are generated from random, class-conditionalGaussian mixtures. Same GMM refers to the special case where there is no distribution change. Forthe case #components mismatched, the number of components in the source and target Gaussianmixtures is different.
Table 6: Conditional log-likelihood of the proposed MDN adaptation for different fixed values ofthe hyper-parameter λ. This is compared with the automatic selection of λ based on the validationmetric.
