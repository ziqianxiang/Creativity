Table 1: Different bound criteria on the second largest eigenvalue t2 of the bi-adjacency matricesBI-ADJACENCY EIGENVALUE (eb)	AVERAGE DEGREE (db)Unweighted (Mi)	UE: t2(Mi) ≤ 2，ti(M，- 1	UD: t2(Mi) ≤ PdavgL(Mi) - 1 + PdavgR(Mi) - 1Weighted (Wi)	WE: t2(Wi) ≤ 2Ptι(Wi) - 1difference on bound ∆s = (2√tι - 1 - t2)∕t2	Δr = (PdavgL - 1 + PdavgR-^ - t2)∕t24.2 Ramanujan Graph Property Preserving Pruning AlgorithmIn this section, we describe a modification of the iterative pruning algorithm that preserves theRamanujan graph property of a pruned neural network. In iterative pruning algorithms, pruning isemployed once the training is complete (i.e. after it converged to an optimum or it reached certainepochs). The pruned network is trained again with its initial weight values to perform the nextpruning iteration. The magnitude of a score function θ is used to identify the victim weights. Theweights are made to zero if their score function values lie in the bottom p percentile. In the IterativeMagnitude Pruning (IMP) Frankle & Carbin (2019), magnitude of the weight is used as the scorefunction. More sophisticated score functions are used in SynFlow (Tanaka et al., 2020), SNIP (Leeet al., 2018).
Table 2: Results of different pruning algorithms under the Ramanujan Graph property preservation						Pruning Algorithm	α	Lenet/MNIST		α	Conv4/CIFAR10			Density	Test Accuracy		Density	Test AccuracyWithout Pruning	0.0	100.0	97:16	0.0	100.0	85.86IMP	1.5	3.16	93:88	1.0	10.0	81.91IMP	2.0	1.0	45.39	2.0	1.0	10IMP-Bound	-	3.6	96.74	-	10.4	81.8SNIP	1.5	3.16	79	1.0	10.0	80.3SNIP	2.0	1.0	49.8	2.0	1.0	64.26SNIP-Bound	-	7.64	95.41	-	10.0	79.8SynFlow	1.5	3.16	95.92	1.0	10.0	82.5SynFlow	2.0	1.0	49.11	2.0	1.0	69.2SynFlow-Bound	-	1.33	93.82	-	1.15	64.5Comparison of pruning approaches We show that consideration Ramanujan graph propertiesbenefits network pruning algorithms. We consider three popular network pruning algorithms - (i)Iterative Magnitude Pruning (IMP) Frankle & Carbin (2019), (ii) the iterative verion of Single-shot Network Pruning based on Connection Sensitivity (SNIP) Lee et al. (2018), (iii) Synaptic flowbased pruning SynFlow Tanaka et al. (2020) and show the results of test accuracy achieved withsimilar densities with and without the bound condition. To do this experimentation, we choose thea fix compression ratio α to achieve the desired density of the pruned network as 10-α × 100, and
Table 3: Hyper-parameter settings for experimenting LTH using iterative magnitude based pruningLenet (on MNIST) Conv4 (on CIFAR10)Optimizer Training Iterations Batch size Learning Rate Pruning epochs Model initialization	Adam 20000 60 0.0012 50 Kaiming Normal	Adam 25000 60 0.0003 50 Kaiming NormalConv Layers FC layers	300,100,10	64,64, pool 128, 128, pool 256, 256, 10pruning epochs(in comparison)	60	6018UnderreVieW as a ConferenCe PaPersICLR 2022A∙7 DlFFERENT PARAMETERS VALUES WITH RESPECT To THE RAMANUJAN GRAPH BASEDPRUNING ALGoRITHMTabIe RePreSeIltatiVe result Ofthe ParameterS foundthe last PnnIg epoch Of synl—BoundAIgOrithm on LeIlet/MNISTLl	unweighted	(力 1, tι2, da∕υg, daAJgR, eb	，db, △5, Z∖r)	(31.84, 14.32, 235.20, 3.07, 11.11, 16.74, -0.22, 0.17)^TΓ^	weighted	(力 1, tl2, davgL∙) ^javgR∙> C	M 曲,△5)	(19.59, 1.37, 31.63, 12.10, 8.62, 8.87, 5.29)^TΓ^	Score	(力 1,力2, da,∙vgL, da,∙vgR, C	M db, △5)	(14.07, 6.96, 99.42, 1.30, 7.23, 10.47, 0.04 )^TΓ^		(layer-wise density, remaining/total parameters)		(0.01,2352/235200)	~T2~	unweighted	(力 1, tl2, davg, davgR∙)	，db, △5, Z∖r)	(12.41, 5.75, 3.37, 30.00, 6.76, 6.92, 0.17, 0.20)	=~UΓ	weighted	(力 1,力2, d<wg2√, dcι算gR, C	M 曲,△5)	一	(11.40, 1.34, 19.66, 6.55, 6.45, 6.68, 3.82)T2~	Score	(力 1,力2, da,∙vgL, da,∙vgR, C	M db, △5)	(43.27, 21.31, 11.23, 99.93, 13.00, 13.14, -0.39 )T2~		(layer-wise density, remaining/total parameters)		(0.01,300.0/30000)	~T3~	unweighted	(力 1,力2, daA)g，dcι>vgR, eb	,db, △5, Ar)	(29.83, 0.00, 89.00, 10.00, 10.74, 12.38, 10356908.93, 11940047.67Γ
