Table 1: Top-1 accuracies for ViT-B/16 pre-trained and fine-tuned on ImageNet-1k with or withoutthe proposed negative augmentation.
Table 2: Top-1 accuracies for ViT-B/16 pre-trained and fine-tuned on ImageNet-1k using Rand-Augment (Cubuk et al., 2020) or AugMix (Hendrycks et al., 2020b). The proposed negative aug-mentation is added on top of either positive augmentation. See Table 8 in Appendix for a full tablewith three losses for each patch-based transformation. Patch-based negative augmentation is com-plementary to “positive” data augmentation.
Table 3: Top-1 accuracies of ViT-B/16 pretrained on ImageNet-21k and finetuned on ImageNet-1k.
Table 4: Effect of patch-based negative augmentation in pre-training and fine-tuning stages. Top-1accuracies of ViT-B/16 pretrained and fine-tuned on ImageNet-1k. Under ‘Stage’ we denote whichtraining stage patch-based negative augmentation is used.
Table 5: Training details following (Dosovitskiy et al., 2021).
Table 6: Models using a different hyperparameter λ than the default value (1.5).
Table 7: Hyperparameters in patch-based transformations.
Table 8: Patch-based negative augmentation is complementary to “positive” data augmentation.
Table 9: Effect of patch-based negative augmentation in pre-training and fine-tuning stages. Top-1accuracies of ViT-B/16 pretrained and fine-tuned on ImageNet-1k. Under ‘Stage’ we denote whichtraining stage patch-based negative augmentation is used. The best result under each setting ishighlighted in bold.
Table 10: Effect of positive augmentation in pre-training and fine-tuning stages. Top-1 accuraciesof ViT-B/16 pretrained on ImageNet-21k and fine-tuned on ImageNet-1k. Under ‘Stage’ we denotewhich training stage Rand-Augment (Cubuk et al., 2020) is used.
Table 11: Top-1 accuracies for ViT-B/16 pre-trained and fine-tuned on ImageNet-1k using Rand-Augment (Cubuk et al., 2020) or AugMix (Hendrycks et al., 2020b) in both pre-training and fine-tuning. The proposed negative augmentation is added on top of either positive augmentation. Patch-based negative augmentation is complementary to “positive” data augmentation.
Table 12: Effect of patch-based negative augmentation in contrastive loss regularization. Top-1accuracies of ViT-B/16 trained with or without patch-based negative augmentation.
Table 13: Patch-based negative augmentation effectively reduce models’ texture bias on ConflictStimuli (Geirhos et al., 2018). A higher texture accuracy indicates the model has a higher biastowards texture. The “texture accuracy” is defined as the percentage of images that are classified asthe “texture” label, provided the image is classified as either “texture” or “shape” label.
