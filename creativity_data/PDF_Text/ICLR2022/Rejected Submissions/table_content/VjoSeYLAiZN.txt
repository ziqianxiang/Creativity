Table 1: PSNR (dB) values by different algorithms on 10 scenes in the simulation dataset. Ourapproach is denoted by v1: SRN w/o rescaling pair, v2: SRN with 1 rescaling pair, and v3: SRNwith 2 rescaling pair. Notably, v2 and v3 are provided to further reduce the FLOPs of v1 (seeTab. 3) while still exhibiting a competitive performance over seven state-of-the-art methods.
Table 2: SSIM values by different algorithms on 10 scenes in the simulation dataset, which showsthe consistency with the above metric PSNR. Both of PSNR and SSIM indicate the promising per-formance of proposed SRN on syhthetic testing data.
Table 3: Analysis efficiency of the deeplearning-based algorithms, with a spa-tial size of input as 256×256. The anno-tation v1: SRN w/o rescaling pair, v2:SRN with 1 rescaling pair, and v3: SRNwith 2 rescaling pair.
Table 4: Averaged PSNR/dB (↑), SSIM (↑), model size/M ⑷ and FLOPs/G ⑷ on Simplified Res-Block (without Batch Bormalization) and normal ResBlock (containing Batch Normalization) insimulation dataset. Batch size is kept as 4, learning rates is initialized as 0.0004 and will be reducedby half every 50 epochs throughout the experiment. Apparently, there is no benefit for adding BatchNormalization layer.
Table 5: Averaged PSNR (dB) andSSIM values for different learning ratein simulation experiment, where batchsize is kept as 4 and the learning rates isreduced by half every 50 epochs.
Table 7: PSNR (dB) and SSIM values on 1024×1024×28 simulation testing data. Reconstruction isconducted by SRN v2 model (i.e., the model with one rescaling pair), where optimal hyperparametersettings argued in Section A.2 are leveraged in the training phase.
Table 6: Averaged PSNR (dB) andSSIM values on different batch sizes insimulation dataset. Initial learning rateis kept as 0.0004 and reduced by halfevery 50 epochs.
Table 8: PSNR (dB) and SSIM values on 256×256×28 simulation testing set. To create a well-trained model that can easily adapt to diverse masks, We train SRN v2 model with masks gener-ated from the same distribution, during which optimal hyperparameter settings (i.e., learning rate is0.0004, batch SiZe is 4) argued in Section A.2 are leveraged.
