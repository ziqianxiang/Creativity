Table 1: Performance of hand-crafted MAKPConv vs KPConv on ModelNet40 and SemanticKITTI.
Table 2: Wide & Deep Predictor achieves best prediction quality over SemanticKITTI.
Table 3: Overall Accuracy (OA) on ModelNet40.
Table 4: mIOU on sequence 08 (validation split) of SemanticKITTI. Latency is measured withNVIDIA TITAN X Pascal on a single SemanticKITTI scene containing ~60K points. Here, rednumbers denote computation time and blue numbers denote processing time. + : Results from Zhouet al. (2021). *: Results from Tang et al. (2020).
Table 5: Comparison of various predictor-based NAS methods with Random Search baseline. Re-sults are derived from the top-5 NAS models discovered by each method.
Table 6: Structure of hand-crafted MAKPConv. "1/2" Table 6 illustrates the detailed archi- strides means upsampling by 2×. Note that stage 1 〜7 is tecture of our hand-crafted MAKP- Utilized to perform classification, and stage 8〜11is utilized Conv. The hand-crafted MAKPConv to perform semantic segmentation.	contains a total of 17 MAKPConv 	 l-∖ll/Λt` r`lo(`(`ir`iol-i /ʌɪi <ιɪiH Λ or]									Stage	Depth	Stride	In Width	Out Width	E	K	■ ■ ■ ■ ■ ■ ■■ ■ ■ ■ ■ ■ . ≡ a ■ . ≡ . . « ... ≡ a a ■ a	■ ■ . , ditional MAKPConv blocks for se- mantic segmentation. The positional setting (i.e., depth, block width) of hand-crafted MAKPConv strictly fol- lows MobileNetV2, which is con- sidered as the state-of-the-art hand- crafted model with remarkable effi- ciency. For structural settings, We employ an expansion factor of 3 for all MAKPCOnv layers. In addition, We leverage a kernel size of 7 for each MAKPConv block for design efficiency. As a result, MAKPConv uating the performance of NAS-crafted	1 2 3 4 5 6 7 8 9 10 11	-1- 2 3 4 3 3 1 1 1 1 1	-1- 2 2 2 1 2 1 1/2 1/2 1/2 1/2	-16- 16 24 32 64 96 160 416 160 96 64	-16- 24 32 64 96 160 320 160 96 64 32	丁 3 3 3 3 3 3 3 3 3 3	7j~ 7 7 7 7 7 7 7 7 7 7	serves as a strong baseline to compare with when eval MAKPConv models.								A.2 MAKPConv Design SpaceWe present the detailed settings of the MAKPConv design space in Table 7. Despite the search ofblock depth, block width, expansion factor and kernel size, we also incorporate the search of optionalattention into MAKPConv design space to trade-off the overhead in Neighbor-Kernel Attention andits performance gain.
Table 7: Configuration of the MAKPConv design space. "1/2" strides means upsampling by2×.
Table 8: 6-fold cross-validation results on S3DIS.
Table 9: mIoU per class on S3DIS Area-5.
Table 10: Operator-level latency breakdown of our searched model. We list the critical operatorshere and categorize less important operator as ’Other’. In column Type, C denotes computation-bounded operation and M denotes memory-bounded operation.
