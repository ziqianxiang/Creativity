Table 1: Performance comparison with BERTBASE on∆W	UV	589.8K	92.09	80.79	0.575	0.891∆W	UV	294.9K	92.09	80.49	0.580	0.892∆W	U V + S2	298.0K	92.78	80.64	0.588	0.895BLEU score, 55.56 versus 55.29, with half of the trainable parameters. Such a phenomenon indi-cates that adding a sparse matrix to the low-rank decomposition can make substantial improvement.
Table 2: comparison of different decomposition on GPT-2. The formulas of decomposition are reported.
Table 3: Performance comparison of different methods on BERTBASE on GLUE benchmarks. The star sign(*) in the sparsity column indicates that it represents structured sparsity.
Table 4: Performance comparison of different methods on GPT-2 on E2E, WebNLG and DART. LoRAt:reproduced results. The star sign (*) in the sparsity column indicates that it represents structured sparsity.
Table 5: Performance comparison of different methodson DeBERTa-large on CoLA, MNLL MRPC and RTE.
Table 6: Performance comparison on BERTBASE withdifferent masks embedded._______________________Methods	# Trainable Parameters	Sparsity in Pretrained Weights	Dataset						SST-2	MNLI	CoLA	STS-BFine-tune	110M	0%	92.32	82.12	57.02	88.97W Θ S1	-55M	50%	91.28	81.97	0.562	0.887W Θ S1 + UV	589.8K	50%	90.66	81.13	0.566	0.884W + UV + S2	592.9K	0%	91.97	80.86	0.580	0.893W Θ S1 + UV + S2	592.9K	50%	90.83	81.41	0.567	0.888),two DSEE,s variants (W Θ Si + UV) andW + UV + S2), DSEE (W S1 + UV + S2). The results are collected in Table 6. We cansee that: ❶ NO embedded sparsity in the pretrained weights yields the overall best performance.
Table A7: Hyper-parameters we used on different datasets.
Table A8: Performance comparison of different methods on BERTBASE on GLUE benchmarks. The first rowof each method indicates the mean accuracy, and the second row indicates the standard deviation.
Table A9: Inference time of different methods on BERTBASE on GLUE benchmarks. The sparsity with starsigns indicates that it is a structured sparsity.
