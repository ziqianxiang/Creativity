Table 1: GCN on Cora. Same experimental setting as in the previous experiment. We randomlypartition the labelled nodes into training set and test set for 100 times. Different partitions areranked in increasing order of the distance between training set and test set. Metrics are evaluated inthree cases: (1) “Random”: all 100 ways of partitions are used; (2) Top 50 %: top half of the rankedpartitions are used; (3) Top 30 %: top 30 % of the ranked partitions are used. Mean distance is theaverage graph distance between training set and test set. Values in the bracket indicate the variance.
Table 2: Test accuracy of different models on Cora. Same experimental setting as in Table 1.
Table 3: Model accuracy of different models trained on Cora. An initial labelled set (k=10) isselected by different strategies, and test set includes the rest of labelled vertexes. We run eachstrategy 10 times on each model and compute average test accuracy and variance (in brackets).
Table 4: Test accuracy of different models on Citeseer	GCN	GAT	GraphSAGERandom	56.72 (5.41)	53.78 (4.73)	56.52 (4.85)Top 50%	61.48 (5.15)	54.33 (4.24)	61.78 (3.60)Top 30%	63.03 (4.14)	54.63 (3.65)	62.38 (3.47)F.4 Initial Data LabellingIn this set of experiments related to Table 3, all three models, GCN, GAT and GraphSage, have 3layers with hidden dimensions of 16 and Relu as the activation function. We use 2 attention headsfor GAT. We trained the model with Adam optimizer with a learning rate of 0.01, weight decayof 0.0005 and drop-out of 0. We keep all other hyperparameters as default. For each training, wetrained the model in 400 epochs and record the best performance from each training. For the “cover”strategy following the greedy algorithm in (Hochbaum, 1996), we greedily select the next vertexesto be the vertexes whose r-hop neighborhood has the most uncovered vertexes. If there are morethan one candidates, we randomly pick one from the candidate set, and then we update and repeatthe selection procedure. The additional experimental results on Citeseer and PubMed are providedin Table 5 and Table 6.
Table 5: Model accuracy (%) of different models trained on Citeseer data set. An initial labelled set(k=20) is selected by different strategies, and the test set includes the rest of lablled vertexes. We runeach strategy 20 times on each model and compute its average performance and variance. ‘Cover’is our strategy that solves (1) with r = 2.
Table 6: Model accuracy of different models trained on PubMed data set. An initial labelled set(k=20) is selected by different strategies, and the test set includes the rest of lablled vertexes. We runeach strategy 20 times on each model and compute its average performance and variance. ‘Cover’is our strategy that solves (1) with r = 2.
