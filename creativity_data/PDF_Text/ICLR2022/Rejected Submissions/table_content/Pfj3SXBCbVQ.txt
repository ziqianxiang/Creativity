Table 1: Datasets partitionsDataset	Train	Val	TestEuroparl v7 (ES-EN)	1.9M∕100k∕50k	5000	5000Europarl v7 (DE-EN)	1,8M / 100k / 50k	5000	5000Europarl v7 (CS-EN)	635k / 100k / 50k	5000	5000CommonCrawl (ES-EN)	1.8M / 100k	5000	5000SciELO (ES-EN)	575k / 120k	4961	4961SciELO (PT-EN)	116k	3738	3738NewsCommentary (DE-EN)	357k / 35k	5000	5000IWLST’16 (DE-EN)	196k	993	1305Multi30K (DE-EN)	29k	1014	1000All the values in this Table indicate the number of sentences.
Table 2: Neural architecturesModel	Parameters	HyperparametersLSTM Fully CNN Transformer small Transformer standard	6.8M (S) — 27.8M (L) 6.5M (S) — 27.4M (L) 4.1M (S) — 25.0M (L) 44.9M (S) — 92.7 (L)	3 layers / 256 emb / 256 hid / bidir+attn 6x(256, 3) (enc/dec) 3 layers / 8 heads / 256 dim / 512 ffnn 6 layers / 8 heads / 512 dim / 2048 ffnnCommon hyperparamters: Loss function: CrossEntropy (without label smoothing). Optimizer:Adam (Kingma & Ba (2015)) or NAG. Batch size: 4096 tokens/batch, Clip-norm: 0.1. Maximumepochs: 100 epochs with early stopping. Beam width: 5.
