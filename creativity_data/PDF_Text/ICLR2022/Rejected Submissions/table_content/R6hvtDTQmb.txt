Table 1: Comparison of AdaMomentum and clas-sic adaptive gradient methods in mt and Vt in (1).
Table 2: FID score (1) of Spectral Normalized Generative Adversarial Network on CIFAR-10 (KrizheVsky & Hinton, 2009) dataset. f is reported in ZhUang et al. (2020).
Table 3: Test accuracy (%) of CNNs on CIFAR-10 dataset. The best in Red and second best in blue.
Table 4: Top-1 test accuracy (%) on ImageNet (Russakovsky et al., 2015) dataset.
Table 5: Test perplexity (1) results of LSTMs on Penn Treebank (Marcus et al., 1993) dataset.
Table 6: BLEU score (↑) on IWSTL’14 DE-EN (Cettolo et al., 2014) dataset.
Table 7: Well tuned hyperparameter configuration of the adaptive gradient methods for CNNs onCIFAR-10._______________________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentumStepsize α	0.001	0.001	0.001	0.001	0.001	0.001	0.001β1	~09	0.9	0.9	09	0.9	0.9	0.9β2	0.999-	0.999	0.999	-0999-	0.999	0.999	0.999Weight decay	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	-10-8-	10-8	10-8	-10-8-	10-8	10-8	10-8CIFAR datasets The values of the hyperparameters after careful tuning of the reported results of the adap-tive gradient methods on CIFAR-10 in the main paper is summarized in Table 7. For SGDM, the optimalhyperparameter setting is: the learning rate is 0.1, the momentum parameter is 0.9, the weight decay param-eter is 5 × 10-4. For Adabound, the final learning rate is set as 0.1 (matching SGDM) and the value of thehyperparameter gamma is 10-3.
Table 8: Well tuned hyperparameter configuration of the adaptive gradient methods for CNNs onImageNet.
Table 9: Well tuned hyperparameter configuration of adaptive gradient methods for 1-layer-LSTMon Penn Treebank dataset.____________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentumStepsize α	0.001	0.001	0.01	0.01	0.001	0.001	0.001β1	09	0.9	0.9	09	0.9	0.9	0.9β2	-0999-	0.999	0.999	-0999-	0.999	0.999	0.999Weight decay	1.2 × 10-4	1.2 × 10-4	1.2 × 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	-10-12-	10-12	10-8	10-8-	10-12	10-16	10-1626Under review as a conference paper at ICLR 2022120notoo90807060口一 XWdsd u≡∙llO 25	50	75	1∞ U5 150	175	200Epoch
Table 10: Well tuned hyperparameter Configuration of adaptive gradient methods for 2-layer-LSTMon Penn Treebank dataset.____________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBOund RAdam AdaBelief AdaMomentumStepsize α	0.01	0.001	0.01	0.01	0.001	0.01	0.001β1	O	0.9	0.9	0.9	0.9	0.9	0.9β2	-0.999-	0.999	0.999	-0.999-	0.999	0.999	0.999Weight deCay	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	-10-12	10-12	10-8	-10-8-	10-12	10-12	10-16Table 11: Well tuned hyperparameter Configuration of adaptive gradient methods for 3-layer-LSTMon Penn Treebank dataset.____________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentumStepsize α	0.01	0.001	0.01	0.01	0.001	0.01	0.001β1	0.9	0.9	0.9	0.9	0.9	0.9	0.9β2	-0.999-	0.999	0.999	-0.999-	0.999	0.999	0.999Weight deCay	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	-10-12-	10-12	10-8	-10-8-	10-12	10-12	10-1627Under review as a conference paper at ICLR 2022The training perplexity curve is illustrated in Figure 5. We can clearly see that AdaMomentum is able to makethe perplexity descent faster than SGDM and most other adaptive gradient methods. In experimental settings,
Table 11: Well tuned hyperparameter Configuration of adaptive gradient methods for 3-layer-LSTMon Penn Treebank dataset.____________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentumStepsize α	0.01	0.001	0.01	0.01	0.001	0.01	0.001β1	0.9	0.9	0.9	0.9	0.9	0.9	0.9β2	-0.999-	0.999	0.999	-0.999-	0.999	0.999	0.999Weight deCay	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	-10-12-	10-12	10-8	-10-8-	10-12	10-12	10-1627Under review as a conference paper at ICLR 2022The training perplexity curve is illustrated in Figure 5. We can clearly see that AdaMomentum is able to makethe perplexity descent faster than SGDM and most other adaptive gradient methods. In experimental settings,the size of the word embeddings is 400 and the number of hidden units per layer is 1150. We employ dropoutin training and the dropout rate for RNN layers is 0.25 and the dropout rate for input embedding layers is 0.4.
Table 12: Well tuned hyperparameter configuration of adaptive gradient methods for transformer onIWSTL’14 DE-EN dataset.
