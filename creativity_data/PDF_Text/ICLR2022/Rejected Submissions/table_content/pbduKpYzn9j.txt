Table 1: Effect of initialization. Inheriting both style module and convolution naturally obtainsbetter results than random initialization. Surprisingly, we find that inheriting only style module isthe best solution. With this initialization, RGB+LPIPS and RGB+LPIPS+LD obtains 0.38 and 0.51improvements, respectively, compared to their corresponding results when inherit both modules.
Table 2: How to deal with style module in StyleGAN2 distillation. The style module is comprisedof MLPs. The numbers inside and outside the “[]” is the number of channels in each layer and thenumber of layers, respectively. The style module of the teacher is [512]*8. The FLOPs saving onlyconsiders FLOPs reduction in the style module.
Table 3: Ablation study about relation mimicking.
Table 4: Comparison with SOTA. "1”(“f")denotes the lower (higher) the better. For the FID andPPL of Full-Size, GAN slimming and CAGAN, we directly use the results reported in CAGAN (Liuet al., 2021). Since CAGAN did not release their computation details of PSNR and LPIPS, wecompute these two metrics using our own implementation. We also list the PSNR and LPIPS resultsreported in CAGAN in “()”. The PSNR and LPIPS of GAN slimming are blank due to the lack ofits well-trained checkpoint. Bold font denotes the results that outperform CAGAN. “heter” denotesheterogeneous setting where the student is not a subnet of the teacher.
