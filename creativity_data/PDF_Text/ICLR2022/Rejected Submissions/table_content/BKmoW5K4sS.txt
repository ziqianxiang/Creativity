Table 1: Test accuracy and fairness gap of unconstrained models and fair models trained on biasedtraining data - COMPAS, Adult and synthetic datasets. When = 0, the models are trained on Dc .
Table 2: List of NotationsSymbol DescriptionWhere it is definedXYXY(X,Y)DNDcnDpPrDDkDtestSSδ∆
Table 3: Distribution of data points in clean training dataset and Dk - COMPAS dataset.
Table 4: Distribution of data points in clean training dataset and Dk - Adult dataset.
Table 5: Distribution of data points in clean training dataset and Dk - Synthetic dataset.
Table 6: Distribution of data points in clean training dataset and Dk - MEPS dataset.
Table 7: Test accuracy of unconstrained models and fair models (Hardt et al., 2016; Agarwal et al.,2018; Rezaei et al., 2020; Cotter et al., 2019; Zhang et al., 2018) in the presence of adversarial bias -all datasets, for = 0.1. We report and compare the accuracy of the unconstrained models and fairmodels in the benign ( = 0) and adversarial bias setting. When = 0, the models are trained on Dc .
Table 8: Fairness gap of target models on test data for = 0.1. Fair models are trained using the reduction approach (Agarwal et al., 2018). The fairness gap ∆ is defined in (1). The numbers reflect how unfair the model is with respect to the protected group in the test data. For fair models, compare numbers with δ (the guaranteed fairness gap on training data). The farther apart ∆ and δ are, the less the fairness generalization is on test data.					Dataset	Strategies		Unconstrained Fair	Fair	Fair			Model	(δ = 0.1)	(δ = 0.05)	(δ = 0.01)	Benign		0.18±0.04^^0.14±0.05	0.08±0.04	0.03±0.02	Random Sampling		0.18±0.04^^0.14±0.04	0.08±0.04	0.03±0.02	Label flipping		0.19±0.04	0.07±0.04	0.07±0.02	0.10±0.03Synthetic	Adv. sampling (Alg. 2, λ	= 0)	0.17±0.05^^0.08±0.05	0.08±0.04	0.07±0.04	Adv. sampling (Alg. 1, λ	= 1000)	-	0.05±0.03	0.04±0.03	0.04±0.02	Adv. sampling (Alg. 2, λ	= 100)	-	0.07±0.04	0.09±0.04	0.09±0.04	Adv. labeling (Alg. 2, λ =	0)	0.29±0.10^^0.07±0.04	0.10±0.04	0.11±0.06	Adv. labeling (Alg. 1, λ =	1000)	-	0.08±0.03	0.15±0.03	0.14±0.06	Adv. labeling (Alg. 2, λ =	100)	0.12±0.05	0.15±0.05	0.14±0.07	Benign		0.21±0.07^^0.11±0.06	0.08±0.05	0.06±0.04	Random Sampling		0.19±0.07^^0.08±0.03	0.10±0.03	0.11±0.05	Hard examples		0.19±0.08	0.09±0.03	0.11±0.03	0.13±0.05	Label flipping		0.23±0.07	0.09±0.04	0.08±0.03	0.07±0.04COMPAS	Adv. sampling (Alg. 2, λ	= 0)	0.26±0.08^^0.19±0.07	0.25±0.07	0.30±0.07	Adv. sampling (Alg. 1, λ	= 1000)	-	0.16±0.05	0.20±0.07	0.22±0.09	Adv. sampling (Alg. 2, λ	= 100)	-	0.29±0.06	0.33±0.08	0.37±0.09	Adv. labeling (Alg. 2, λ =	0)	0.28±0.08^^0.13±0.05	0.15±0.07	0.19±0.08
Table 9: Test accuracy of unconstrained models and fair models in the presence of adversarial bias -COMPAS for = 0.1. We report and compare the accuracy of the unconstrained models and fairmodels in adversarial bias setting when the clean training data and the target model’s architectureare unknown. Instead, a substitute dataset and a substitute model are used in Algorithm 1 andAlgorithm 2. We also report the relative accuracy drop in parenthesis.
