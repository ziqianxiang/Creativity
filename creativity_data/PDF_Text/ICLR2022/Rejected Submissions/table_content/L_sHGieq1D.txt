Table 1: Evaluation of the proposed AdvStyle on different methods (Baseline, IBN-Net (Pan et al.,2018) and ISW(Choi et al., 2021)) and backbones (MobileNetV2 (Sandler et al., 2018), ResNet-50 (He et al., 2016), and ResNet-101). All models are trained on the GTAV training set and testedon CityScapes (C), BDD-100K (B), and Mapillary (M) validation sets.
Table 2: Results of using SYNTHIA as the source domain. The backbone is ResNet-101.
Table 3: Comparison of different augmentations. Source: GTAV; Backbone: ResNet-50.			and ISW for all baCkbones. Third, when adding AdvStyle, the results of IBN-Net and ISW Can befurther imProved for all settings. For examPle, when using ResNet-101 as the baCkbone, AdvStyleimProves the average mIoU of IBN-Net and ISW by 6.09% and 6.53%, resPeCtively. In Table 2, weshow the results of using SYNTHIA as the sourCe domain and also observe Clear imProvements forAdvStyle. These results verify the Prominent advantage of our AdvStyle on different models.
Table 4: Comparison of different style-aware methods. Source: GTAV; Backbone: ResNet-50.				Comparison of Different Style-Aware Methods. In Table 4, we compare AdvStyle with threestyle-aware augmentation methods, including MixStyle (Zhou et al., 2021), CrossStyle (Tang et al.,2021) and RandStyle. MixStyle mixes the styles of two samples with a convex weight whileCrossStyle directly swaps the styles of two samples. RandStyle can be regarded a reduction ofour AdvStyle, which randomly adds Gaussian noise into the style feature. All style-aware methodsare implemented on the image-level for fair comparison. We can find that (1) all style-aware meth-ods can consistently improve the performance on all target domains and (2) AdvStyle achieves thebest results. The first finding verifies the effectiveness of augmenting image styles and the secondfinding shows the benefit of learning adversarial styles over other style-aware methods for domaingeneralized semantic segmentation.
Table 5: Comparison with state-of-the-art domain generalization methods. All models use the GTAVas the source domain. For each backbone, models with the same ID are implemented with the samebaseline. Models of “ID=I, II and IV” use the whole set (24,966) for training while models of“ID=III” only use the training set (12,403). The absolute gain of each model is calculated overthe corresponding baseline. § denotes extra using the ImageNet images. * indicates using the besttrained checkpoints for evaluating each target domain.
Table 6: Accuracy of single domain generaliza-tion on Digits. MNIST is used as the training set,and the results on different testing domains are re-ported in different columns.
Table 7: Accuracy of single domain general-ization on PACS. One domain (name in col-umn) is used as the training (source) data andthe other domains are used as the testing (tar-get) data.
Table 8: Results of using GTAV and SYNTHIA as the source data. The backbone is ResNet-50.
