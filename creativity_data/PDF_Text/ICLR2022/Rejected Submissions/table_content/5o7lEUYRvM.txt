Table 1: Summary of stochastic (and deterministic) representations for the network weights w, whichcorrespond to popular BNN approaches. These distributions apply either to the whole network or perlayer. Note Σθ is typically factorized in practice. While we use Radial BNNs rather than MFVI, weinclude it for completeness. Rank1 references specifically the Gaussian realization.
Table 2: Accuracies for the rotated MNIST experiment. Means and standard errors over ten seeds.
Table 3: Accuracies for the rotated MNIST experiment. Means and standard errors over ten seeds.
Table 4: Accuracies for the rotated MNIST experiment. Means and standard errors over ten seeds.
Table 5: Log-likelihoods for the rotated MNIST experiment. Means and standard errors over tenseeds. Best results within archetype in boldface, best results overall in blue.
Table 6: Log-likelihoods for the rotated MNIST experiment. Means and standard errors over tenseeds. Best results within archetype in boldface, best results overall in blue.
Table 7: Log-likelihoods for the rotated MNIST experiment. Means and standard errors over tenseeds. Best results within archetype in boldface, best results overall in blue.
Table 8: Expected calibration errors for the rotated MNIST experiment. Means and standard errorsover ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 9: Expected calibration errors for the rotated MNIST experiment. Means and standard errorsover ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 10: Expected calibration errors for the rotated MNIST experiment. Means and standard errorsover ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 11: Accuracies for the corrupted CIFAR10 experiment. Means and standard errors over tenseeds. Best results within archetype in boldface, best results overall in blue.
Table 12: Log-likelihoods for the corrupted CIFAR10 experiment. Means and standard errors overten seeds. Best results within archetype in boldface, best results overall in blue.
Table 13: Expected calibration errors for the corrupted CIFAR10 experiment. Means and standarderrors over ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 14: Accuracies for the CIFAR10 adversarial attack experiment. Means and standard errors overten seeds. Best results within archetype in boldface, best results overall in blue.
Table 15: Log-likelihoods for the CIFAR10 adversarial attack experiment. Means and standard errorsover ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 16: Expected calibration errors for the CIFAR10 adversarial attack experiment. Means andstandard errors over ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 17: Accuracies for the corrupted CIFAR100 experiment. Means and standard errors over tenseeds. Best results within archetype in boldface, best results overall in blue.
Table 18: Log-likelihoods for the corrupted CIFAR100 experiment. Means and standard errors overten seeds. Best results within archetype in boldface, best results overall in blue.
Table 19: Expected calibration errors for the corrupted CIFAR100 experiment. Means and standarderrors over ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 20: Accuracies for the CIFAR100 adversarial attack experiment. Means and standard errorsover ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 21: Log-likelihoods for the CIFAR100 adversarial attack experiment. Means and standarderrors over ten seeds. Best results within archetype in boldface, best results overall in blue.
Table 22: Expected calibration errors for the CIFAR100 adversarial attack experiment. Means andstandard errors over ten seeds. Best results within archetype in boldface, best results overall in blue.
