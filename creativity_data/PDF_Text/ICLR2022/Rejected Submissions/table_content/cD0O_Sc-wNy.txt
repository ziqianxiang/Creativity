Table 1: Accuracy comparison between RS-MCTS (Ours), Random choice scheduling (Random), HeuristicScheduling (Heuristic), and Equal Task Schedule (ETS) with various memory selection methods evaluatedacross all datasets. We use ’S’ and ’P’ as short for ’Split’ and ’Permuted’ for the datasets. The replay memorysize is M = 10 and M = 100 for the 5-task and 10/20-task datasets respectively. We report the mean andstandard deviation averaged over 5 seeds. RS-MCTS performs better or on par than the baselines on mostdatasets and selection methods, where MoF yields the best performance in general.
Table 2: Accuracy comparison in the 1 example per class memory setting evaluated across all datasets. We use’S’ and ’P’ as short for ’Split’ and ’Permuted’ for the datasets. RS-MCTS has replay memory size M = 2and M = 50 for the 5-task and 10/20-task datasets respectively. The baselines replay all available memorysamples. We report the mean and standard deviation averaged over 5 seeds. RS-MCTS performs on par withthe best baselines on all datasets except S-CIFAR-100.
Table 3: Performance comparison with ACC and BWT metrics for all datasets between RS-MCTS and thebaselines with various memory selection methods. The memory size is set to M = 10 and M = 100 forthe 5-task and 10/20-task datasets respectively. We report the mean and standard deviation of ACC and BWT,where all results have been averaged over 5 seeds. RS-MCTS performs better or on par than the baselines onmost datasets and selection methods, where MoF yields the best performance in general.
Table 4: Performance comparison with ACC and BWT metrics for all datasets between RS-MCTS and thebaselines in the setting where only 1 sample per class can be replayed. The memory sizes are set to M = 10and M = 100 for the 5-task and 10/20-task datasets respectively. We report the mean and standard deviationof ACC and BWT, where all results have been averaged over 5 seeds. RS-MCTS performs on par with the bestbaselines for both metrics on all datasets except S-CIFAR-100.
