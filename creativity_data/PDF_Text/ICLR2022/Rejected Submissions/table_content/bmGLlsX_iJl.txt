Table 1: Imputation results on UCI datasets - RMSE (loWer is better).
Table 2: Imputation results on image datasets - RMSE (lower is better)	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9	GAIN	.1029	.1184	.1399	.1495	.1723	.1794	.2167	.2200	.2710MNIST	MisGAN	.1083	.1117	.1184	.1227	.1311	.1388	.1512	.1906	.2621	MCFloW	.0835	.0879	.0894	.0941	.1027	.1119	.1251	.1463	.2020	EMFloW	.0726	.0775	.0832	.0901	.0986	.1100	.1260	.1504	.1951	GAIN	.1025	.1090	.1103	.1073	.1094	.1202	.1217	.1426	.5388CIFAR-10	MisGAN	.1577	.1434	.1478	.1326	.1588	.1824	.2036	.2660	.3011	MCFlow	.1083	.1112	.1179	.1273	.1340	.1387	.1466	.1552	.1702	EMFloW	.0444	.0479	.0525	.0575	.0619	.0689	.0782	.0926	.1188Table 3: Classification accuracy on imputed image datasets (higher is better)	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9MNIST	MCFloW	7.9894	.9878	.9878	.9871	.9840	.9806	.9659	7933Γ	T7732^	EMFlow	.9894	.9884	.9882	.9878	.9860	.9824	.9696	.9253	.7502CIFAR-10	MCFloW	.8352	.7081	.5525	.4166	.3406	.2820	.2476	.2194	.1875	EMFloW	.9085	.8974	.8783	.8535	.8116	.7446	.6214	.4868	.3127To make the comparison more objective, both models use the same normalizing flow with six affinecoupling layers. We also follow the authors’ suggestion for the the hyperparameter selection ofMCFlow throughout this section. Additionally, we also present the benchmarks of other state-of-artmodels including GAIN (Yoon et al., 2018) and MisGAN (Li et al., 2019) for more comprehensive
Table 3: Classification accuracy on imputed image datasets (higher is better)	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9MNIST	MCFloW	7.9894	.9878	.9878	.9871	.9840	.9806	.9659	7933Γ	T7732^	EMFlow	.9894	.9884	.9882	.9878	.9860	.9824	.9696	.9253	.7502CIFAR-10	MCFloW	.8352	.7081	.5525	.4166	.3406	.2820	.2476	.2194	.1875	EMFloW	.9085	.8974	.8783	.8535	.8116	.7446	.6214	.4868	.3127To make the comparison more objective, both models use the same normalizing flow with six affinecoupling layers. We also follow the authors’ suggestion for the the hyperparameter selection ofMCFlow throughout this section. Additionally, we also present the benchmarks of other state-of-artmodels including GAIN (Yoon et al., 2018) and MisGAN (Li et al., 2019) for more comprehensivecomparisons.
Table 4: Information of UCI datasets.
Table 5: Congeniality of imputation models (loWer is better).
Table 6: Imputation results of MIWAE on UCI datasets - RMSE (lower is better).
Table 7: Imputation results of MIWAE on MNIST - RMSE (lower is better)	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9MNIST	-MIWAE-	.1022	.1057	.1120	.1185	.1194	.1233	.1409	.1672	.2275	EMFloW	.0726	.0775	.0832	.0901	.0986	.1100	.1260	.1504	.1951Table 8: Classification accuracy of MIWAE on imputed MNIST (higher is better)	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9MNIST	-MIWAE-	.9792	.9878	.9683	.9672	.9521	.9469	.9237	.9331	.7005	EMFloW	.9894	.9884	.9882	.9878	.9860	.9824	.9696	.9253	.7502G.3 Effect of Initial ImputationEMFlow is an iterative imputation framework, and the nearest-neighbor (NN) imputation is per-formed for image datasets at the beginning as warm-start in all the previous experiments. In thissection, we compare NN imputation and zero imputation as the starting point of EMFlow and showsthe convergence of the training loss, the RMSE on the test set, and the Frobenius norm of the co-variance matrix in the latent space. As shown in Figure, 5, the difference between NN imputationand zero imputation is negligible and the convergence under both cases is fast.
Table 8: Classification accuracy of MIWAE on imputed MNIST (higher is better)	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9MNIST	-MIWAE-	.9792	.9878	.9683	.9672	.9521	.9469	.9237	.9331	.7005	EMFloW	.9894	.9884	.9882	.9878	.9860	.9824	.9696	.9253	.7502G.3 Effect of Initial ImputationEMFlow is an iterative imputation framework, and the nearest-neighbor (NN) imputation is per-formed for image datasets at the beginning as warm-start in all the previous experiments. In thissection, we compare NN imputation and zero imputation as the starting point of EMFlow and showsthe convergence of the training loss, the RMSE on the test set, and the Frobenius norm of the co-variance matrix in the latent space. As shown in Figure, 5, the difference between NN imputationand zero imputation is negligible and the convergence under both cases is fast.
