Table 1: Comparison of classifier accuracy in % for different pretraining settings. Scores betterthan the SOTA model (LaplaceNet) are in bold. ”DRL” pretraining is our proposed representationlearning, and ”VDRL” the respective version which uses a probabilistic encoder.
Table 2: FID and Inception Score for different interpolations between maximum likelihood trainingand training based on the adversarial λ0. pKL = 1.0 corresponds to original training.
Table 3: FID for different initial noise scales evaluated on 20k generated samples.
Table 4: Classifier accuracy in % with and without DRL as pretraining of the classifier when trainingfor 100 epochs only.
Table 5: Comparison of classifier accuracy in % for different pretraining methods in the case of fewsupervised labels when training for 100 epochs only.
Table 6: Classifier accuracy in % for autoencoder pretraining compared with the baseline and scorematching as pretraining. No mixup is applied for this ablation study.
Table 7: Evaluation of classifier accuracy in %, including the setting of using mixup during pretraining(right column). DRL pretraining is our proposed representation learning, and ”Mixup-DRL” therespective version which additionally applies mixup during pretraining. ”VDRL” instead uses aprobabilistic encoder.
