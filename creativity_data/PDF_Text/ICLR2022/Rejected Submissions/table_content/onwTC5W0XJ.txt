Table 1: This table summarizes a list of all the notations used in the paper.
Table 2: Brief summary of the datasets used in the experiments.
Table 3: Quantitative results comparing proposed CFCN variants with the state of the art. We seethat in the case of small datasets, CFCN models outperform the baselines in almost every metric.
Table 4: Detailed experimental setup for each dataset.							Sr.	Dataset	Filters in conv. layers	DO	BS	LR	Epochs	α1.	Oxford IIIT Pets	128,128,64,64,32,32,16,16	0.3	64	1e-4	500	12.	Aeroplane-Cow	128,128,64,64,32	0.3	16	1e-4	200	1.53.	Brain MRI	64,64,64,32,32,32	-	32	1e-5	150	34.	IDRiD	32,32,16,16,8,8	0.3	5	1e-4	200	0.8In this section we describe the detailed experimental setup used for training different models oneach dataset. We have implemented all the models using Tensorflow v2.2. In all the experiments,we used custom CNN architectures for each dataset. Further, for each dataset, all the experimentswere run by maintaining an identical experimental setup for all the models. Input images from allthe models except the IDRiD dataset were resized to the shape 96×96 while those belonging to theIDRiD dataset were resized to the shape 250×175. The number of filters used in each convolutionallayers of these architectures is given in Tab. 4. Every convolutional layer is followed by a batchnormalization layer and later by a dropout layer (except for the Brain MRI dataset). We use adropout(DO) value of 0.3. The last dropout layer is followed by a convolutional layer with 1×1filter size and 16 filters in the case of CFCN-F models and ‘C ’ filters in the case of CFCN-C, where‘C ’ is the number of classes in the given dataset. As IDRiD is a multi-label classification dataset,the 1times1 convolutional layer in this case has 3 filters, for both CFCN-F and CFCN-C. The 1×1convolutional layer is then followed by a flatten layer in the case of CFCN-F, while for the CFCN-C models, we use global average pooling. In the case of CFCN-F the flatten layer is followed by
