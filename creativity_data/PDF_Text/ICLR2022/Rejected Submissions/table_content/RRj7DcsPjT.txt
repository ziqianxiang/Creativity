Table 1: Summary of prediction accuracy on different datasets. The accuracy is reported in the formof a percentage (%).
Table 2: Summary of datasets. Each undirected edge is counted once. Each node in ogbn-proteinhas 112 binary labels. “D.” refers to the average degree of the graph. “Feat” refers to the number offeatures. “Split Ratio" refers to the ratio of training/validation/test data.
Table 3: The time complexity for computation for L-layer GCN training by layer-wise samplingand node-wise sampling . The first column refers to the matrix operation type, nodes aggregation orlinear transformation. ____________________________________________________Layer-wise Node-wiseNodes Aggregation	O(ScpL	O(SbLp)Linear Transformation	θ(sp2L)	θ(sbL-1p2)For simplicity, we suppose that the number of hidden variables in each layer is fixed as p, the sameas the dimension of H (0). The batch size and the numbers of nodes sampled in each layer equalare set all equal to a fixed constant s. We assume the number of sampled neighbors per node innode-wise sampling is b. We denote the maximal degree of all the nodes in the graph as c. Thecomputational cost of the propagation comes from two parts: the linear transformation, a densematrix product, H(I)W(I) and the node aggregation, a sparse matrix product, P(I)(H(I)W(I)).
Table 4: Average sampling time (in milliseconds) per batch for layer-wise methods and vanilla node-wise method (GraphSAGE). 512 and 1024 (followed with the dataset name) indicate the number ofnode sampled for one layer. The “f” and “d” in “LADIES+f+d” denotes “flat” and “debiased”respectively.
Table 5: Average training time (in milliseconds) per batch for a 2-layer GCN.
Table 6: Average training time (in milliseconds) per batch for a 3-layer GCN.
