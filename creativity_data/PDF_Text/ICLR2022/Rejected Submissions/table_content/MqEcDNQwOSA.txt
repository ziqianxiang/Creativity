Table 1: Overview of the neural language networks. The configurations of each networks whereE is the number of the embeddings, d is embedding vector size, and ∣θ∣, ∣θemb∣ are the number ofparameters. We assume that the k-sub-embedding is allocated randomly by Algorithm 1.
Table 2: Results of randomly allocating sub-embedding on GLUE. We compare the performanceof RoBERTaMEDIUM and randomly allocated k-sub-embeddings on GLUE benchmark.
Table 3: Clustered k-sub-embedding Results on GLUE. The networks of 3-sub-embedding areenhanced using the pretrained network where M is the number of each sub-embedding.
Table 4: Results on cross-lingual classification. We evaluate base network and k-sub-embeddingthat is fixed to k = 3. E denotes the number of embeddings in each network.
Table 5: Hyperparameters of RoBERTaMEDIUM and XLM-RMEDIUM. The details of pretrainingmodels. Most of the hyperparameters adapt from RoBERTa.
Table 6: Hyperparameters to evaluate on GLUE and XNLI. The details of fine-tuning hyperpa-rameters.
Table 7: Statistics of each monolingual corpus. We extract 15 languages from CommonCrawl.
