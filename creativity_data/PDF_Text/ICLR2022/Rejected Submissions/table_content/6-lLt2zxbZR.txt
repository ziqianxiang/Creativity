Table 1: Performance of various transformer models (large versions), Fine-tuning performed onWinogrande.
Table 2: Comparison of albert-xxlarge-v2 to best reported model in Zhou et al. (2020).
Table 3: PLL zero-shot performance on Winograd (Levesque et al., 2012) and Winogrande (train-xl) (Sakaguchi et al., 2020) data sets for a number of recent large language models. We have sortedby Winograd scores, ascending. Model size in Pytorch .bin from https://huggingface.co/models.
Table 4: PLL zero-shot performance on TimeDial data set (Qin et al., 2021) for a num-ber of recent large language models, with highest fine-tuned score from that paper in ital-ics. Scored using the Salazar et al. (2020) library but with normalization by tokenizedlength. Dataset filtered to examples with tokenized length less than 450 tokens. Model fea-tures are reported from https://huggingface.co/models with model size in Pytorch.bin. ‘kws’(short for 'keep weird spaces') indicates that the TimeDial dataset is used as original pre-sented at https://raw.githubusercontent.com/google-research-datasets/TimeDial/main/test.json. ‘not kws’ indicates the application of a string function to in-put that removes spaces before punctuation symbols.
Table 5: Zero-shot, single-take evaluation results Oflanguage models that have only been exposed to pre-training corpora via original objective function (i.e., withouta selection process based on multiple experimental configurations) across various eomɪnonsense tasks. Human performance as well as the best textitmultiple-takemodel Ma et al. (2021) are included for reference. In the case of Ma et al. (2021), multiple takes corresponded selecting from variants of Roberta-large pre-trainedon different Knowledge Graphs, and reporting the best-performing variant in the zero-shot setting.
