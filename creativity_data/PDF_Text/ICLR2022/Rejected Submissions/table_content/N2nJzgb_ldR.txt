Table 1: Experiments on Long Range Arena benchmark, the best model is boldface, the doubleunderline is a top-2 result. Results for Performer and Linear Transformer (ReLU) are copied fromSPE (Liutkus et al., 2021), except experiments with FastRPB. We mark experiments that failed dueto memory limitations as OOM (Out of Memory). Since RPE is compatible only with the OriginalTransformer, we marked other experiments as Not Applicable (N/A). RPE is N/A for CIFAR sinceplain RPE is designed for 1D sequences. We marked experiments that were too long to train as"-”.
Table 2: MNIST F1 score. All the experiments run on 4 Nvidia Tesla T4.
Table 3: Benchmark results on LRA with experiment setup proposed in SPE (Liutkus et al., 2021).
Table 4: Best Learning Rate values obtained from 20 runs of Bayesian hyperparameters searchB Proposition proofsB.1 Circulant matricesTo design a more efficient positional encoding method, we leveraged circulant matrices - a subclassof matrices with some special properties due to their relation to the Fast Fourier Transform (FFT)and circular convolution Bamieh (2020). Here we will only focus on the property that allows per-forming matrix-vector product fast and efficient in terms of speed and memory. Given an vectorc = (c0, c1, ..., cn-1) we will define the associated n × n circulant matrix C = circ(c) which firstcolumn is exactly c, and each subsequent column is obtained by a circular shift of the previouscolumn:c0ciC2CCn-1	Cn-2	…。1、C0	Cn-1	C2Ci C0	C3(13)∖Cn-1	Cn-I2	Cn-3	…	)It can be shown that for every vector x of size n matrix-vector product Cx requires only O(n log n)
