Table 1: 2 Categories of experimental settings. Problem settings with ] and ? indicate to uselinear neural network and to be considered almost in NTK regime respectively. For hyperparametersearch, we conduct Bayesian optimization for all experimental settings. We describe the furtherdetailed configurations of hyperparameters and other settings for the experiment in Appendix C.2.
Table 2: Experiments: WorkloadsModel	Dataset	Batch size	Step Budget	Epoch2-NN w/o SC (Thomas et al., 2020)	TinyMNIST	512	11343	1203-LNN Wl SC	TinyMNIST	8192	300	603-LNN w/o SC	TinyMNIST	8192	300	603-NN w/ SC	TinyMNIST	8192	300	603-NN w/o SC	TinyMNIST	8192	300	606-LNN w/ SC	MNIST	8192	300	606-LNN w/o SC	MNIST	8192	300	606-NN w/ SC	MNIST	8192	300	606-NN w/o SC	MNIST	8192	300	60Simple CNN Base (Shallue et al.1 2019)	MNIST	256	9350	606-LNN w/ SC	CIFAR-10	256	10205	606-LNN w/o SC	CIFAR-10	256	10205	60VGG-16 w/o BN (Simonyan & Zisserman, 2015)	CIFAR-10	128	78000	250ResNet-8 w/o BN (Shallue et al.1 2019)	CIFAR-10	256	15800	120ResNet-8 w/o BN (Shallue et al.1 2019)	一	CIFAR-100	256	15800	120Table 3: Hyperparameter Search Range for TinyMNIST Dataset ExperimentsModel	η	P	δ	λ	γ2-LNN w/o SC	[1e-3, 1e-1]	[1e-2,1]	[1e-2, 1]	[0]	[0, 0.999]
Table 3: Hyperparameter Search Range for TinyMNIST Dataset ExperimentsModel	η	P	δ	λ	γ2-LNN w/o SC	[1e-3, 1e-1]	[1e-2,1]	[1e-2, 1]	[0]	[0, 0.999]3-NN w/o and w/ SC	[1e-4, 1e-1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]3-LNN w/o and w/ SC	[1e-4, 1e-1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]Table 4: Hyperparameter Search Range for MNIST Dataset Experiments					Model	η	P	δ	λ	γ6-NN w/o and w/ SC	[1e-4, 1e-1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]6-LNN w/o and w/ SC	[1e-4, 1e-1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]Simple CNN	[1e-4, 1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]Table 5: Hyperparameter Search Range for CIFAR10 Dataset Experiments					Model	η	P	δ	λ	γ6-LNN w/o and w/ SC	[1e-4, 1e-1]	[0.5,1]	[0.5, 1]	[0]	[0, 0.999]ResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]VGG-16 w/o BN	[1e-3, 1e-0]	[0.5,1]	[0.5, 1]	[1e-4, 1e-1]	[1e-4, 0.999]Table 6: Hyperparameter Search Range for CIFAR100 Dataset ExperimentModel	η	P	δ	λ	γResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]17Under review as a conference paper at ICLR 2022
Table 4: Hyperparameter Search Range for MNIST Dataset Experiments					Model	η	P	δ	λ	γ6-NN w/o and w/ SC	[1e-4, 1e-1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]6-LNN w/o and w/ SC	[1e-4, 1e-1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]Simple CNN	[1e-4, 1]	[5e-1,1]	[5e-1, 1]	[0]	[0, 0.999]Table 5: Hyperparameter Search Range for CIFAR10 Dataset Experiments					Model	η	P	δ	λ	γ6-LNN w/o and w/ SC	[1e-4, 1e-1]	[0.5,1]	[0.5, 1]	[0]	[0, 0.999]ResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]VGG-16 w/o BN	[1e-3, 1e-0]	[0.5,1]	[0.5, 1]	[1e-4, 1e-1]	[1e-4, 0.999]Table 6: Hyperparameter Search Range for CIFAR100 Dataset ExperimentModel	η	P	δ	λ	γResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]17Under review as a conference paper at ICLR 2022C.3 Distribution of Train Loss and Generalization Gapmin LoSSIoTGeneralization. Gapmin LoSS
Table 5: Hyperparameter Search Range for CIFAR10 Dataset Experiments					Model	η	P	δ	λ	γ6-LNN w/o and w/ SC	[1e-4, 1e-1]	[0.5,1]	[0.5, 1]	[0]	[0, 0.999]ResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]VGG-16 w/o BN	[1e-3, 1e-0]	[0.5,1]	[0.5, 1]	[1e-4, 1e-1]	[1e-4, 0.999]Table 6: Hyperparameter Search Range for CIFAR100 Dataset ExperimentModel	η	P	δ	λ	γResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]17Under review as a conference paper at ICLR 2022C.3 Distribution of Train Loss and Generalization Gapmin LoSSIoTGeneralization. Gapmin LoSSFigure 7: Distribution of training loss and generalization gap on the trained modelsGeneralization. Gap18Under review as a conference paper at ICLR 2022D	Additional Experimental Results
Table 6: Hyperparameter Search Range for CIFAR100 Dataset ExperimentModel	η	P	δ	λ	γResNet-8 w/o BN	[1e-6, 1e+1]	[0.5,1]	[0.5, 1]	[1e-5, 1e-4]	[1e-4, 0.999]17Under review as a conference paper at ICLR 2022C.3 Distribution of Train Loss and Generalization Gapmin LoSSIoTGeneralization. Gapmin LoSSFigure 7: Distribution of training loss and generalization gap on the trained modelsGeneralization. Gap18Under review as a conference paper at ICLR 2022D	Additional Experimental ResultsD. 1 Full Results of Correlation between Generalization Gap and TIC LowerB ound EstimatesWe summarize these results, evaluated with three different correlation coefficients, in Table 7. Theseresults are the calculated 3 types of correlation coefficients for the plot shown in figure 4a, 4b and 2c. Furthermore, the relationship between these correlation coefficients and the values of the ratios of
Table 7: Correlation: TIC estimates Tr(C(θ))/Tr(F(θ)) and generalization gapModel	Dataset	Spearman's	Kendall’s τ	Pearson’s Correlation2-NN	Tiny MNIST	-^-0.456^^	-0.313	-0.3093-NN	Tiny MNIST	-0.631	-0.44	-0.7663-NN w/ SC	Tiny MNIST	-0.19	-0.137	-0.3473-LNN	Tiny MNIST	0.277	0.238	0.2563-LNN w/ SC	Tiny MNIST	0.932	0.795	0.8986-NN	MNIST	~~0.882~~	0.708	0.4256-NN w/ SC	MNIST	0.969	0.87	0.4786-LNN	MNIST	0.682	0.465	0.7746-LNN w/ SC	MNIST	0.593	0.512	0.848Simple CNN	MNIST	0.923	0.553	0.7636-LNN	CIFAR10	~~0.951	0.82	0.8886-LNN w/ SC	CIFAR10	0.976	0.877	0.965VGG-16 w/o BN	CIFAR10	0.904	0.725	0.933ResNet-8 w/o BN	CIFAR10	0.912	0.766	0.983ResNet-8 w/o BN	CIFAR100	0.966	0.855	0.97819Under review as a conference paper at ICLR 2022∙0∙5∙0∙5
Table 8: Correlation: TIC Estimates Tr(F (θ)-1C(θ)) and Generalization GapDataset	Model	Spearman’s	Kendall’s τ	Pearson’s CorrelationTiny MNIST	2-NN	~~0.532^^	0.38	0.27Tiny MNIST	3-NN	-0.62	-0.427	-0.679Tiny MNIST	3-NN w/ SC	0.258	0.175	-0.332Tiny MNIST	3-LNN	0.292	0.271	0.305Tiny MNIST	3-LNN w/ SC	0.886	0.75	0.922Table 9: Correlation: TIC Estimates Tr(Fblockdiag(θ)-1Cblockdiag(θ)) and Generalization GapDataset	Model	Spearman’s	Kendall’s τ	Pearson’s CorrelationTiny MNIST	2-NN	0.524~~	0.372	0.288Tiny MNIST	3-NN	-0.549	-0.366	-0.622Tiny MNIST	3-NN w/ SC	0.364	0.244	-0.08Tiny MNIST	3-LNN	0.26	0.257	0.252Tiny MNIST	3-LNN w/ SC	0.937	0.823	0.944Table 10: Correlation: TIC Estimates Tr(Fdiag(θ)-1Cdiag(θ)) and Generalization Gap				Dataset	Model	Spearman’s	Kendall’s τ	Pearson’s CorrelationTiny MNIST	2-NN	^^-0.309^^	-0.23	-0.234Tiny MNIST	3-NN	-0.297	-0.203	-0.415Tiny MNIST	3-NN w/ SC	-0.176	-0.128	-0.415Tiny MNIST	3-LNN	0.275	0.237	0.288
Table 9: Correlation: TIC Estimates Tr(Fblockdiag(θ)-1Cblockdiag(θ)) and Generalization GapDataset	Model	Spearman’s	Kendall’s τ	Pearson’s CorrelationTiny MNIST	2-NN	0.524~~	0.372	0.288Tiny MNIST	3-NN	-0.549	-0.366	-0.622Tiny MNIST	3-NN w/ SC	0.364	0.244	-0.08Tiny MNIST	3-LNN	0.26	0.257	0.252Tiny MNIST	3-LNN w/ SC	0.937	0.823	0.944Table 10: Correlation: TIC Estimates Tr(Fdiag(θ)-1Cdiag(θ)) and Generalization Gap				Dataset	Model	Spearman’s	Kendall’s τ	Pearson’s CorrelationTiny MNIST	2-NN	^^-0.309^^	-0.23	-0.234Tiny MNIST	3-NN	-0.297	-0.203	-0.415Tiny MNIST	3-NN w/ SC	-0.176	-0.128	-0.415Tiny MNIST	3-LNN	0.275	0.237	0.288Tiny MNIST	3-LNN w/ SC	0.932	0.796	0.91825Under review as a conference paper at ICLR 2022D.3 Additional Results of Practical-Scale ExperimentsIn this section, we present results of practical-scale setting that we have not presented in the mainpaper. We observed that in a small-scale setting, DNNs with a large number of layers and SC tendto have a high rank correlation in the TIC estimator and a strong correlation with the generalization
Table 10: Correlation: TIC Estimates Tr(Fdiag(θ)-1Cdiag(θ)) and Generalization Gap				Dataset	Model	Spearman’s	Kendall’s τ	Pearson’s CorrelationTiny MNIST	2-NN	^^-0.309^^	-0.23	-0.234Tiny MNIST	3-NN	-0.297	-0.203	-0.415Tiny MNIST	3-NN w/ SC	-0.176	-0.128	-0.415Tiny MNIST	3-LNN	0.275	0.237	0.288Tiny MNIST	3-LNN w/ SC	0.932	0.796	0.91825Under review as a conference paper at ICLR 2022D.3 Additional Results of Practical-Scale ExperimentsIn this section, we present results of practical-scale setting that we have not presented in the mainpaper. We observed that in a small-scale setting, DNNs with a large number of layers and SC tendto have a high rank correlation in the TIC estimator and a strong correlation with the generalizationgap. Since it is not computationally feasible to compute TIC in practical DNNs, this section de-tails experimentalresults on the performance of two approximate estimators of TIC using diagonalapproximation and its lower bound.
Table 11: Full Details of Runtime Measuments Expenmemnt. Unit: secondDataset	Model	Exact w/FC	Block Diag w/FC	Diag w/FC	Lower Bound w/FC	Exact w/HC	Block Diag w/HC	Diag w/HC	Lower Bound w/HC	Lower Bound w/HC by Hutchinson,s MethodTiny MNIST	2-wide NN	10.9093	6.7516	3.7273	1.9069	105.4282	96.0647	92.979	82.0081	2.7814Tiny MNIST	3-NN	3.8641	3.8435	3.7937	T994	23.9083	22.4055	21.7705	22.4203	2.5705Tiny MNIST	3-NN w/ SC	3.8999	3.8568	3.8239	2.0225	26.2303	24.9561	25.0151	24.9217	2.8932Tiny MNIST	3-LNN	3.8656	T847	3.7991	1.9521	23.188-	22.2685	22.1641	21.6511	2.8645Tiny MNIST	3-LNN w/ SC	3.8771	3.8962	3.8159	1.9594	23.8571	22.8106	23.0338	22.0721	2.7198MNIST	6-NN	-Wa	-Wa	3.7235	1.9172	-Wa	-Wa	-Wa	-Wa	2.8949MNIST	6-NN w/ SC	-Wa	-Wa	3.6813	1.9459	-Wa	-Wa	-Wa	-Wa	3.3679MNIST	6-LNN	-Wa	-Wa	3.4822	1.9148	-Wa	-Wa	-Wa	-Wa	2.8089MNIST	6-LNN w/ SC	-Wa	-Wa	T502-	1.9329	-Wa	-Wa	-Wa	-Wa	2.9382MNIST	Simple CNN	-Wa	-Wa	4.7059	2.6337	-Wa	-Wa	-Wa	-Wa	8.7958CIFAR10	6-LNN	-Wa	-Wa	3.6742	1.9295	-Wa	-Wa	-Wa	-Wa	4.1043CIFAR10	6-LNN w/ SC	-Wa	-Wa	3.7003	1.9453	-Wa	-Wa	-Wa	-Wa	2.8996CIFAR10	ReSNet8	-Wa	~n∕a	12.4677	10.0031	~n∕a	-Wa	~n∕a	-Wa	90.9633CIFAR10	VGG16 ~	-Wa	-Wa	13.3077	11.3721	-Wa	-Wa	-Wa	-Wa	60.716CiFARlOO 一	ReSNet8	n/a	N/A	12.4292	9.9655	—	N/A	n/a	N/A	n/a	一	88.7423Under review as a conference paper at ICLR 2022D.5 EXPERIMENTS WITH d/n CHANGES WITHIN TINYMNISTWe created a restricted dataset, SmallTinyMNIST, which uses only 5% of the TinyMNIST data.
Table 12: Additional problem settings are highlighted in bold textCategory	Problem Setting: Dataset & Model	Ratio: d/n	TinyMNIST on 2-NN w/o SC	0.09Small Scale	TinyMNIST on 3-NN w/o SC, 3-NN w/ SC	0.02Data Size <1MB	TinyMNIST on 3-LNN w/o SC, 3-LNN w/ SC	0.02Model <50KB	SmallTinyMNIST on 3-LNN w/o SC	0.36	TinyMNIST on Wide 3-NN WISC	13.90We conducted training evaluations using SmallTinyMNIST to investigate the relationship betweenthe TIC estimator and the generalization gap for patterns with relatively large d/n ratios.
Table 13: Correlation: TIC estimates Tr(C(θ))/Tr(F(θ)) and generalization gapModel	Dataset	Spearman's	Kendall’s τ	Pearson’s Correlation3-LNN	Tiny MNIST	^^0.277^^	0.238	0.2563-LNN	Small Tiny MNIST	0.806	0.622	0.8113-NN w/ SC	Tiny MNIST	-0.19	-0.137	-0.3473-Wide NN w/ SC	Tiny MNIST	0.834	0.656	0.96733Under review as a conference paper at ICLR 20220.0400.035Generalization Gap vs TIC Estimate0∙0∙0∙dp0UOIJPZWəuə00.0000.005SmllTinyMNIST on 3-LNNTinyMNIST on 3-LNNTinyMNIST on 3-NN w/ SCTinyMNIST on Wide 3-NN w/ SCO
