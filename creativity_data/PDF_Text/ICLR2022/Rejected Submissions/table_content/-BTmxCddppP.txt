Table 1: Accuracy on the in-distribution (CIFAR-10/CIFAR-100) and FPR@95%TPR for varioustest out-distributions of different OOD methods with OpenImages as training out-distribution (resultsfor the test set of OpenImages are not used in the mean FPR). Lower false positive rate is better.
Table 2: Evaluation (same metrics as in Table 1) of models trained with shared and separaterepresentations. Shared training benefits both the classifier and the binary discriminators.
Table 3: AUC for CIFAR-10 vs. various out-distributions of different methods that have access toonly unlabelled CIFAR-10 data during training. Shown are the scores obtained from the likelihoodsof the PixelCNN++ from Ren et al. (2019),	Mean	SVHN	LSUN	CelebA Smooth		C-100	Open	80M	UNIModel	AUC	AUC	AUC	AUC	AUC	AUC	AUC	AUC	AUCLh PixelCNN++	57.05	07.14	89.02	57.77	77.93	52.96	70.52	43.99	100.00LH VAE	57.97	20.98	83.03	48.05	91.67	51.32	55.62	55.09	100.00Lh Regret VAE	52.24	87.36	35.73	70.69	14.84	53.03	50.81	53.21	94.11BinDisc Uniform	45.90	71.22	34.67	35.07	47.13	47.32	44.70	41.17	100.00E.2 Likelihood Ratios as a binary discriminatorIn Section 3.1, we discussed that for the Bayes optimal solutions of their training objectives, the ratioof the likelihoods of two density estimators for different distributions is as an OOD detection scoringfunction equivalent to the prediction of a binary discriminator between the two distributions. In orderto find out which role this equivalence plays in practise, we train a binary discriminator betweenCIFAR-10 as in-distribution and the background distribution obtained by mutating 10% of the pixlesof in-distribution images as described in Ren et al. (2019). In Table 4, we compare the OOD detectionperformance of this discriminator with likelihood ratios estimated with PixelCNN++ (Salimans et al.,19Under review as a conference paper at ICLR 20222017) as trained with the code of Ren et al. (2019) setting L2 regularization as 10, and with the
Table 4: AUROC for CIFAR-10 vs. various out-distributions. The Likelihood Ratio Ren et al.
Table 5: Accuracy on the in-distribution (CIFAR-10/CIFAR-100) and AUC for various test out-distributions of the different OOD methods with OpenImages as training out-distribution for whichthe FPRs are shown in Table 1.
Table 6: AUC evaluation of the models trained with shared and separate representations from Table 2.
Table 7: Accuracy on the in-distribution (CIFAR-10/CIFAR-100) and FPR@95%TPR for varioustest out-distributions of different OOD methods with 80 Million Tiny Images as training out-distribution (shown results for test set of 80M are not used for computing the mean FPR). Lower falsepositive rate is better. CelebA makes no sense as test out-distribution for CIFAR-100 as it containsman/woman as classes. Plain, OE, BGC and Shared have been trained using the same architectureand training parameters/schedule. s1 , s2 , s3 are the scoring functions introduced in Section 3.2. Ourbinary discriminator (BinDisc) resp. the combination with the shared classifier (S hared Combi)performs similar/better than Outlier Exposure (Hendrycks et al., 2019a).
Table 8: Accuracy on the in-distribution (CIFAR-10/CIFAR-100) and AUC (AUROC) for varioustest out-distributions of different OOD methods with 80 Million Tiny Images as training out-distribution (shown results for test set of 80M are not used for computing the mean AUC). Therelative performance of the different methods measured in AUC is similar to what we observed withthe FPR@95%TPR measure in Table 7.
Table 9: FPR@95%TPR for CIFAR-10/CIFAR-100 as in-distribution with SVHN as trainingout-distribution.
Table 10: AUC for CIFAR-10/CIFAR-100 as in-distribution with SVHN as training out-distribution.
Table 11: Out-of-distribution detection evaluation for various ResNet50 models trained on RestrictedImageNet in terms of AUC and FPR@95%TPR. The last column (NOTRIN) refers to the remainingclasses from the ILSVRC2012 validation split that are not part of Restricted ImageNet and that wereused as the training out-distribution;it does not contribute to the mean test FPR/AUC. As all modelsuse the train split of NotRIN as training-out distribution.
Table 12: Effect of varying λ during training for OE (Hendrycks et al., 2019a), the s3 scoringfunction for models with background class and SHARED COMBI s3. Shown are test accuracy andFPR@95%TPR with OpenImages as training out-distribution.
Table 13: Effect of varying λ during training for OE (Hendrycks et al., 2019a), the s3 scoring functionfor models with background class and S HARED COMBI s3. Shown are test accuracy and AUC(AUROC) with OpenImages as training out-distribution.
Table 14: Mean μ and standard deviation σ of the FPR@95%TPR measure for different methodsand scoring functions over five runs each for models with OpenImages as training out-distribution.
Table 15: Mean μ and standard deviation σ of the AUROC measure for different methods andscoring functions over five runs each for models with OpenImages as training out-distribution. Thetraining details are the same as for the results shown in Table 5.
