Table 1: Datasets	Adult	Amazon	KDD UPsening	Kick	KDD Internet	Click	Higgs	Marketing	Default	HEPMASS#SamPles	49K	-33K-	50K	73K	10K	400K	11KK	45K	-30K-	840K#features	15	10	231	36	69	â€”	12	28	16	23	25benchmarks (Bentejac et al., 2021) with the use of default settings and an efficient integrated Cate-gorical feature handler. We train each model for B = 5000 iterations with the learning rate set insuch a way that the cross validated optimal point is close to the 2500-th iteration to ensure conver-gence of the training proceess. Datasets used in this investigation and their properties are listed inTable 1, their links can be found in references. Most of them are taken from the list of benchmarksof the original CatBoost paper (Prokhorenkova et al., 2017) and the proposed tuned hyperparametersfrom the paper were used.
Table 2: Quality estimation, 0-1 loss / logloss, relative error change	Adult	Amazon	KDD UPSeHing	Kick	KDD InternetBaseline	0.1264/0.2723	0.0447/0.1400	0.0494/0.1666	0.0496 /0.2857	0.1004/0.2202Adaptive pruning	-0.24%/-0.24%	-1.37%/-0.53%	-0.20%/-0.10%	+0.11% /-0.19%	-2.46% /-0.52%	CliCk	HiggS	Marketing	Default	HepmassBaseline	0.1564 /0.3916	0.2364/0.4810	0.0926/0.1937	0.1865/0.4327	0.1258/0.2768Adaptive pruning	+0.04% / -0.03%~	-0.14% /-0.14%	-2.27%/-0.71%	-2.50% /-0.07%~	-0.17% /-0.16%validation protocol monotonically decreases with the number of clusters, as it was expected, and itgives no insight about the optimal cluster count and possible improvement compared to the baseline.
