Table 1: Comparison of clustering and fairness performance on MNIST-USPS, Reverse-MNIST andHAR. HAR consists of multi-state PSV that baselines with dashes are not applicable. Bold results arethe best results among all the baselines except the guaranteed fairness results which are marked withblue. Note we report our average performance results after 10 trials and the term optimal refers to theclustering giving the ground truth labels and the corresponding balance.
Table 2: Deep-clustering data: hyperparameter summary	HAR	reverse-MNIST	MNIST-USPSLearning rate	0.0002	0.0005	0.0005Batch size	256	256	256Train epochs	60	80	50Weight decay α	0.0001	0.0001	0.0001Parameter β	4.00	6.00	4.00Parameter γ	1.00	1.00	1.0014Under review as a conference paper at ICLR 2022Table 3: Traditional tabular data: hyperparameter summary	Census	Credit	BankLearning rate	0.0002	0.0002	0.0005Batch size	256	256	256Training epochs	30	50	30Weight decay α	0.0001	0.0001	0.0001Parameter β	4.00	5.00	4.00Parameter γ	1.00	1.00	1.00For the traditional fair clustering data sets, we use a stacked fully connected neural network withintermediate layers as d - 50 - 50 - k, note the total number of features d for Census, Credit and
Table 3: Traditional tabular data: hyperparameter summary	Census	Credit	BankLearning rate	0.0002	0.0002	0.0005Batch size	256	256	256Training epochs	30	50	30Weight decay α	0.0001	0.0001	0.0001Parameter β	4.00	5.00	4.00Parameter γ	1.00	1.00	1.00For the traditional fair clustering data sets, we use a stacked fully connected neural network withintermediate layers as d - 50 - 50 - k, note the total number of features d for Census, Credit andBank are 5, 3, 14 and k = 2 is the number of clusters. Moreover, we have listed the hyperparametersthat we used for these three data sets in Table 3. We set the same values for hyper-parameters α andγ. Since these tabular data sets only have limited features we observe that fewer training epochs areneeded to train our model.
Table 4: We report the average running time (second) over 10 trials.
