Figure 1: (a) Overview of the proposed method. We encode the inputs to the policy — including observations,goals, and action histories — as templated English phrases. The phrases are sent to an embedding layer and apre-trained language model to predict the subsequent action. The embedding layer fφ can either be a embed-ding layer of a pre-trained language model (Experiment 1) or be a learned embedding layer (Experiment 2). (b)Results. We summarize the main results of Experiment 1 (“Transformer from scratch” and “Pretrained trans-former (natural encoding)”), Experiment 2A (“Pretrained transformer (random encoding)”) and Experiment 2B(“Pretrained transformer (learned encoding)”).
Figure 2: VirtualHome is a 3D interactive environment simulating household activities. It provides a symbolicgraph representation of the partial observation. The action space changes over time based on the observation.
Figure 3: Pre-trained language models for interactive imitation learning. The objects in the current ob-servation, the goal predicates, and history actions are first serialized as templated English phrases. We extractthe tokens and their features from the phrases and send them to the pre-trained language model. The output ofpre-trained language model are summarized into a context feature vector by average pooling which is then usedfor verb and object prediction.
Figure 4: Comparisons of the proposed method and baselines on different testing subsets. “MLP-N”,“MLP-1”, and “LSTM” are baselines without using transformer (Vaswani et al., 2017). “LM (scratch) w/oHist” and “LM (ft) w/o Hist” are based on the transformer architecture but do not use history in the input fordecision making. “LM (scratch)” and “LM (ft) (Ours)” are based transformer and uses history in the input.
Figure 5: Comparisons of pre-trained languagemodels using natural and unnatural strings.
Figure 6: Comparisons oɪpre-traineɑ language en-codings and learned encodings. “LM (ft) (Ours)” usesthe pre-trained language encodings. In “rep Goal”, weuse the learned encoding for goal and the pre-trainedlanguage encodings for history and observation. Simi-larly, “rep Hist” and “rep Obs” use the the learned en-coding for history and observation, respectively. “repGoal-Hist-Obs” uses the learned encoding for goal, his-tory, and observation.
Figure 7: Predicted trajectory and actions for a given household task. The policy learned by fine-tuningthe pre-trained language model successfully finishes the task described in the goal predicates. We highlight thekey actions in the map, where the agent is finding, grabbing, or placing objects in the target positions.
Figure A1: Object encoding. In VirtualHome, the partial observation o of the environment state can be repre-sented as a scene graph, with nodes representing objects and edges describing their spatial relationships. Eachobject node in the observation has a name, a state description, and world coordinates. Object name encoding:for each object node, we serialize its object name as an English phrase. For each word in the English phrase, weextract its tokens and features using the tokenizer and the embedding layer of the pre-trained language model,respectively. We take the averaged features of all the English tokens in the object name and obtain a “name”feature fio,name for each object node. Object state encoding: there are six types of object states in the wholetraining dataset, including, “clean”, “closed”, “off”, “on”, “open”, and “none”. Thus for each object node, weuse a 6-dim vector to represent its state. This state vector is then passed through a fully-connected layer togenerate a state feature fio,state of object oi . Object position encoding: we concatenate the world coordinates{oi,x, oi,y, oi,z} of each object and their spatial distance to the agent {ax, ay, az} to generate a position vector[oi,x, oi,y, oi,z, oi,x - ax, oi,y - ay, oi,z - az]. This position vector is then passed through two fully-connectedlayers with a ReLU layer in the middle to generate a position feature fio,position of object oi. The final feature fioof each object node is obtained by passing the concatenation of its name feature fio,name, state feature fio,state,and position feature fio,position through a fully-connected layer.
Figure A2: Model architecture of the propose model in the main paper Section 6.
Figure A3: Model architecture of “rep Goal” used in the main paper Section 7.2.
Figure A4: Model architecture of “rep Hist” used in the main paper Section 7.2.
Figure A5: Model architecture of “rep Obs” used in the main paper Section 7.2.
Figure A6: Model architecture of “rep Goal-Hist-Obs” used in the main paper Section 7.2.
Figure A7: Attention weights of a layer named “Head 3 Layer 2”. We show attention weights on two differ-ent tasks. We find that “Head 3 Layer 2” is able to capture objects in the goal predicates, such as “wineglass”and “cutleryfork” in the left figure, and “pancake” and “chicken” in the right figure (the figures are cropped forvisualization).
Figure A8: Attention weights of layers named “Head 1 Layer 2” (left) and “Head 4 Layer 11” (right).
Figure A9: Regression planner. Given a task described by goal predicates, the planner generates an actionsequence to accomplish this task. The agent has a belief about the environment, i.e. an imagined distributionof object locations. As the agent explores the environment, its belief of the world becomes closer to the realworld. At every step, the agent updates its belief based on the latest observation (see (Puig et al., 2020)), findsa new plan using the regression planner, and executes the first action of the plan. If the subtask (described bythe goal predicate) has been finished, the agent will select a new unfinished subtask, otherwise, the agent willkeep doing this subtask until finish it.
