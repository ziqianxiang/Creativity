Figure 1: Training and inference pipeline of our proposed conditional generative quantile net-work for a univariate component. The quantile level uis sampled from the unit interval and pushedinto standard normal. During training, our smooth parameterization learns an optimal coupling be-tween the standard normal and the target distribution, and converges to a convex potential. Duringinference, the gradients are computed via auto-differentiation to generate the respective quantileY 〜V. The convex potential gradient ▽夕X is a comonotonic push-forward mapping of μ to V.
Figure 2: Qualitative results on toy datasets. (a.) Estimated quantile curves of thethree-component Gaussian mixture model. (b.) Density estimation on Eight Gaussians,Half Moons, and Spirals with 1, 2, and 3 classes respectively.
Figure 3: Qualitative results of generated images. For the MNIST samples, we conditionallygenerate the handwritten digits by conditioning on a digit for each row. (a.) Directly learning amapping onto the image space of MNIST. (b.) Using a VAE, the quantile network maps to the latentspace. (c.) Novel combination of attribute generation on CelebA.
