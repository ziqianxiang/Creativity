Figure 1: Overview of VLAF2 captioning model. (a) Leveraging linguistic ability from BERT fordescribing novel objects. (b) Preserving adequacy and fidelity of novel object captions via CLIP.
Figure 2: Learning to caption novel objects with linguistic fluency. For caption-labeled image xi, Weimpose the SeqUence-to-SeqUenCe objective ยฃยง2ยง for training. For uncaptioned image Xu, we exploitBERT to improve the wordings of the generated caption yU, and the refined caption is denoted as yU.
Figure 3: Learning to caption novel objects with improved visual-linguistic adequacy and fidelity.
Figure 4: Example results and comparisons for image captions produced by VinVL and ours interms of fluency, fidelity and adequacy. Note that both utilize VIVO for novel object detection.
Figure 5:	Example results and comparisons for image captions produced by VinVL and ours interms of fluency, fidelity and adequacy. Note that both utilize VIVO for novel object detection.
Figure 6:	Example results and comparisons for image captions produced by VinVL and ours interms of fluency, fidelity and adequacy. Note that both utilize VIVO for novel object detection.
Figure 7:	False captions misled by the wrong object detection tags.
