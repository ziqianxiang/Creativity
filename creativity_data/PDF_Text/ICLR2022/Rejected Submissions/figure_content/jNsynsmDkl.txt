Figure 1: Left: Overall structure of our MulCon framework. It consists of three main components:label-level embedding network that uses a label-wise attention block with an Encoder (Enc(∙)) forextracting label-level embeddings gi from input images, a set of independent classifiers fc for multi-label prediction, and a projector (Proj(∙)) to map label-level embeddings to a latent space for Con-trastive learning. Right: Detail of the label-wise attention block. It takes image features ri from(Enc(∙)) as input and returns label-level embedding gi as output. The module contains several self-attention blocks, a multi-headed attention block and a set of learnable label embedding.
Figure 2: MulCon has two steps during training: pretrain-ing and contrastive finetuning. The first step is to train thelabel-level embedding network with binary cross-entropyloss (LBCE) to effectively decompose an input image intoseveral semantic components so that the first componentcorresponds to the first label, etc. The second step is tofinetune the previously trained network with contrastive loss(LLLCL) and LBCE to improve the quality of label-levelembedding.
Figure 3: t-SNE visualization forimage components trained withonly LBCE (top) and with thecombination of LBCE and LLLCL(down).
Figure 4: Top-4 related images retrieved given an query image and label on COCO dataset. Theresults for our full model (MulCon) are on the left, and the results for our model without contrastiveloss (MulCon with BCE only) are on the right. The label under each retrieved image is the onecorresponding to the embedding closest to the picked query embedding.
Figure 5: Top-4 related images retrieved given an query image and multiple labels on COCO dataset.
Figure 6: Visualization of attention maps. The top row includes the input image and the selectedattention map for the ground-truth labels. The bottom row includes the multi-headed attention mapsfor class “person”.
Figure 7: Visualization of attention maps. In each row, the left most image is input image, and theremains are the mean attention maps (from 4 head) for its ground truth classes.
