Figure 1: Solution Methods for MDP relative to state-action space size. The blue area denotesproblems that are tractable using exact methods. The red area is the subset of problems where deepneural network based Q-learning is the most used solution method. The gray area is the subset ofproblems we aim to tackle.
Figure 2: Cartpole solution policy ∏ (x, v,θ,w) from our algorithm.
Figure 3: Gridworld example: The green square is the starting state and the orange state is the goal.
Figure 4: Side by Side comparison of optimal policy π* and the tree policy implied by our algorithm.
Figure 5: Cartpole state space and diagram.
Figure 6: The point (M, N) with reward 1 represented by a •. A configuration of the shortest pathtrajectory such that points in every partition have the same direction. This is optimal as the metricinduced by d1 (x, y) is the Manhattan distance.
Figure 7: Optimal policy ∏ in the aggregation of MDP in Figure 7.
