Figure 1: Transformerencoder architectureVaswani et al. (2017).
Figure 2:	Scaleddot product attentionVaswani et al. (2017).
Figure 3: ViT architecture (Dosovitskiy et al., 2020).
Figure 4: Towards asparse transformer?2We have neglected the constraint diag(C) = 0 for ease of exposition8Under review as a conference paper at ICLR 20225.3	Self-expressiveness versus Self-AttentionNotice from equation 11 that self-expressiveness can be interpreted as a self-attention mechanismwhere the query qj = xj is expressed as a linear combination of all values vi = xi, i = 1, . . . , N,with attention coefficients cij determined by the queries qj = xj and the keys ki = xi . However,we note that self-expressive coefficients (SEC) are more general than self-attention coefficients.
