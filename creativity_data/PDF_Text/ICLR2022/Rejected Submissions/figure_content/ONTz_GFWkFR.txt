Figure 1: By scheduling Y , we can observe thatincreasing the regularization (i.e., reducing Y) re-duces the approximation bias and increases thepoint of convergence, which improves the model.
Figure 2: Learned MNIST manifolds. Left:sampling-free VAE. Right: sampling VAE. Thelatent space is 2 dimensional and the displayedrange is [-5, 5]. Training details are in the sup-plementary material.
Figure 3: Reconstructions and traversals for theCelebA (left) and 3D Chairs (right) data sets.
Figure 4: Quantitative evaluation of FID (left) and log marginal likelihood (right) on the CelebAdata set for different sizes of the latent space. On FID, the sampling-free Gaussian VAE significantlyoutperforms the sampling Gaussian VAE and performs on par regarding the log marginal likelihood.
