Figure 1: (a) We define a novel problem setting, where standard-training (ST) users, which might belimited by data or computational resources, can “share" robustness from adversarial-training (AT)users who can afford it. (b) Comparison of robustness on a varying portion of AT users, where a5-domain digit recognition dataset is distributed to 50 users in total and details are in Appendix C.7.
Figure 2: Models are trained with decoupled BN layers on Digits dataset. bn1 is the first BN layer inthe network. (a) Results on SVHN. (b) The relative statistics are compared on MNIST versus SVHN.
Figure 3:	Evaluating FRP performance with different FRP settings.
Figure 4:	Comparison of robustness transfer approaches by domains.
Figure 5: Illustration of the Dual-BN (DBN) layer and the copying operation for robustness propaga-tion.
Figure 6: Visualization of samples.
Figure 7: The convergence curves and parameters sensitivity of λ, C and γ. C is for regularizationand γ is for RBF-kernel used in SVM whose performance is evaluated on Digits domains.
Figure 8: Dataset sizes for userswhen the global seed is set as 1.
Figure 9: Experiments with varying data size.
Figure 10: Logits of standard-trained models visualized by t-SNE on DomainNet.
Figure 11: Logits of standard-trained models visualized by t-SNE on Digits.
Figure 12: Vary the number of involved users per communication round. The validation accuracy iscomputed by averaging users’ accuracy. For AT users, the RA is used while SA is used for ST users.
Figure 13: Robustness and accuracy by the increasing totalnumber of users as 25, 50, 150, and 200. The larger scatter inthe left figure indicates more users.
Figure 14: Correlation of statistic differences of mean (top) and log-variance (bottom) in BN layers.
Figure 15: Evaluation of the noise-independence as-sumption (left two) and its effects on users’ RA im-provement from debiasing (right two). Estimated bythe Pearson coefficient, R presents the degree to whichnoise statistics (e.g., μr - μ) are correlated to datastatistics (e.g., μ).
