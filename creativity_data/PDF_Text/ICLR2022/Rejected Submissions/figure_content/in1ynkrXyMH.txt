Figure 1: Toy example of feed-forward and introspection process. The visual post-hoc explanationsin the sensing is from Grad-CAM (Selvaraju et al., 2017) while the explanations in the reflectionstage are our own. The written text is for illustrative purpose only.
Figure 2: Scatter plot with performance accuracy vs expected calibration error. Ideally, networks arein top left. Introspectivity increases performance accuracy while decreasing calibration error.
Figure 3: (a) ResNet-18 on CIFAR-10C. (b) Expected calibration error across 5 challenge levels inbrightness and saturate distortions. Note that both these distortions do not affect the performance ofthe network and their feed-forward accuracy is high. The improvement in accuracy is statisticallyinsignificant. However, introspection decreases the ECE across challenge levels.
Figure 4: Introspective feature visualizations. The images in the leftmost column are the input x. Therepresentative images are for illustrative purposes and are not used to extract features.
Figure 5: For the input image on the left, the VWL J(yI, 5) are shown on the right. Each image is avisualization of the 50 × 10 gradient matrix. All images are sparse except in the prediction row 5 andintrospective question row i.
Figure 6: Introspective performance gains over Feed-Forward networks of a) ResNets-18,34,50,101,b) Level-wise averaged results across ResNets-18,34,50,101Challenae Levelsshot noise....Feed-ForwardintrospectiveFigure 7: Introspective performance gains over Feed-Forward Resnet-18 across distortions and levelslevel-wise increase for each network. Note that, an Introspective ResNet-18 performs similarly to aFeed-Forward ResNet-50.
Figure 7: Introspective performance gains over Feed-Forward Resnet-18 across distortions and levelslevel-wise increase for each network. Note that, an Introspective ResNet-18 performs similarly to aFeed-Forward ResNet-50.
Figure 8: ECE vs distortion levels across 12 separate distortions from CIFAR-10C for ResNet-18.
Figure 9: Introspective vs. Feed-Forward accuracy of ResNet-18 across training epochs on (a) CIFAR-10 original testset, (b) CIFAR-10C Motion Blur Testset on all 5 challenge levels, (c) CIFAR-10CGaussian Noise Testset on all 5 challenge levels.
Figure 10: Introspective vs. Feed-Forward accuracy of ResNet-18 across training epochs when (a)f (∙) and H(∙) are trained on the same training set (b) H(∙) is trained on a separate held-out validationsetTraining, Testing, and Results in Fig. 9a In this experimental setup, ResNet-18 is trained for200 epochs. The model states at multiples of 3 epochs from 1 to 200 are stored. This provides 67states of f (∙) along its training process. Each f (∙) is tested on CIFAR-10 testset and the recognitionaccuracy is plotted in blue in Fig. 9a). The introspective features rx for all 67 states are extractedfor the 50,000 training samples. These rχ are used to train 67 separate H(∙) of structure providedin Table 3 with a similar training setup as in Section C.1. The rx from the 10, 000 testing samplesare extracted individually for each of the 67 f (∙) states and tested. The results are plotted in red inFig. 9a). Note the sharp spikes at epochs 60 and 120 where there is a change in the learning rate.
Figure 11: Introspective vs. Feed-Forward accuracy of ResNet-18 across training rounds for state-of-the-art techniques in an active learning setting. The query batch size per round is 1000. The trainsetis CIFAR-10 and testset is Gaussian Noise from CIFAR-10C.
