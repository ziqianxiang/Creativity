Figure 1: Illustration of (a) pre-training for VL-PTMs with masked language modeling (MLM)head, (b) vanilla fine-tuning with new classification (CLS) head, and (c) our colorful cross-modalprompt tuning (CPT) framework that reformulates visual grounding into a fill-in-the-blank problemwith reused MLM head. Only square parts of relevant image regions are shown for illustration.
Figure 2: CPT framework for predicate classification by filling-in-the-blank with reused MLM head.
Figure 3:	Results of utilizing different colors for visual grounding, including (a) an overall evaluationof top-6 colors from different models, and (b) a zoom-in study of aligned individual colors.
Figure 4:	Case study. The bounding boxes given by image region proposals (olive), ground-truthannotation (pink), CPT (green), and fine-tuning baseline (yellow) are highlighted accordingly.
Figure 5: Experimental results with dif-ferent color transparencies.
Figure 7: Visualization of grounding results. First row: zero-shot setting. Second row: fully su-pervised setting. FT: fine-tuning. The bounding boxes given by image region proposals (olive),ground-truth annotation (pink), CPT (green), and fine-tuning baseline (yellow) are highlighted ac-cordingly. Some images are cropped for better visual effects.
Figure 8: Outlook for adapting cross-modal prompt tuning (CPT) to other tasks.
