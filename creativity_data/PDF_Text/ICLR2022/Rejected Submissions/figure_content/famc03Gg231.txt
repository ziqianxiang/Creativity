Figure 1: BFGS fails to fit a wavepacket when the initial guess is toofar from the true value. A neuralnetwork trained on the same objec-tive learns to fit all examples.
Figure 2: (a) Minimization of the function L = kyk22 with y = (x1, x22) in x space. Contoursof L are shown in gray. Solid lines are the optimization trajectories of gradient descent (GD),Newton’s method (N), and physical gradients (PG), with infinitesimal step sizes. Circles representthe first 10 iterations with constant step size. (b) Comparison of the same optimization methods bytheir respective update steps and properties: whether they can adapt to function sensitivity (S), areHessian-free (HF) and take nonlinearities into account (NL). H denotes the Hessian w.r.t. θ. Theproperties of PG are explained in section 2.2.
Figure 3: Network trained on single-parameter optimization (section 3.1) using Adam in combina-tion with various gradient schemes. Running average over 1000 mini-batches.
Figure 4: Optimization of Poisson’s equation to reconstruct a desired output state (section 3.2).
Figure 5: Optimization involving the heat equation (section 3.3). (a) Example from the data set:observed distribution (y*), inferred solutions, ground truth solution (x*). (b) Optimization curvesfor one example. (c) Learning curves.
Figure 6: Incompressible fluid flow (section 3.4). (a) One example from the data set: initial markerdistribution (mo); simulated marker distribution after time t using ground-truth velocity (y*) andnetwork predictions (y.); predicted initial velocities (x∙); ground truth velocity (x*). (b) Optimiza-tion curves with η = 1, averaged over 4 examples. (c) Learning curves, running average over 64mini-batches.
Figure 7: Convergence visualization in x space for one example i. The point xn = fθ (y) representsthe current solution estimate. The grey line and area mark all {χ∣L(χ) < L(χn)} and x* is aminimum of L. Xn = P(Xn) is the next point in the optimization trajectory of X and the green andblue circles around it represent open sets with radii e and ||Xn - Xn∣∣2, respectively. In the areashaded in orange, the distance to Xn decreases but L increases.
Figure 8: Learning curves of the network trained to fit wave packets. The performance of L-BFGS-B and random guessing are shown on the same data for reference. The left graph shows the objective||y - y*∣∣2 and the right two graphs show the X-SPace deviation from the true solution in position ξoand amplitude A.
Figure 9: Gradients for minimizing sin(α∕x), shown for a = 1. From top to bottom: Supervisedlearning with xo = 1, Gradient descent, inverse gradient, Newton,s method, physical gradient,objective function. Singularities for small X are not properly resolved in the IG / Newton plots.
Figure 10: Networks trained on single-parameter optimization using Adam and various gradientschemes. Each figure shows the learning curves for a network initialized with a fixed seed between5 and 8. The X axis denotes the number of training iterations, Solid lines show the running averageover 1000 mini-batches.
Figure 11: Inverse problems involving Poisson’s equation. Top: Three examples from the data set,from left to right: observed target (y*), simulated observations resulting from network predictions(Adam y, Adam+PG y), predicted solutions (Adam x, Adam+PG x), ground truth solution (x*).
Figure 12: Inverse problems involving the heat equation. Top: Two examples from the data set. Thetop row shows observed target (y*) and simulated observations resulting from inferred solutions. Thebottom row shows the ground truth solution (x*) and inferred solutions. From left to right: groundtruth; gradient descent (GD), L-BFGS-B (BFGS) and inverse physics (Inv.phys.), running for 100iterations each, starting with x0 = 0; Networks trained for 10k iterations. Bottom: Neural networklearning curves for two random network initializations, measured in terms of ||x - χ*∣∣ι.
Figure 13: Three example inverse problems involving the Navier-Stokes equations. For each exam-ple, the ground truth (GT) and neural network reconstructions using Adam with physical gradient(A+PG) and pure Adam training (Adam) are displayed as rows. Each row shows the initial veloc-ity v0 ≡ x as well as five frames from the resulting marker density sequence m(t), at time stepst ∈ {0, 0.5, 1, 1.5, 2}. The differences of the Adam version are especially clear in terms of v0.
Figure 14: Measured time per neural network training iteration for all experiments, averaged over64 mini-batches. Step times were measured using Python's perf_counter() function and in-clude data generation, gradient evaluation and network update. In all experiments, the computa-tional cost difference between the various gradients is marginal, affecting the overall training timeby less than 10%.
