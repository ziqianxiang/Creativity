Figure 1: An illustrative example of coherence. Randomly selected examples of class “3” fromour “halves-MNIST” dataset (top) and visualizations of neuron clusters in the first layer of networkstrained to output the modular sum of digits in the images (bottom). We trained multilayer perceptronson a version of the MNIST dataset (LeCun et al., 1998) in which the images were two half-widthdigits side-by-side and the labels were their sum modulo 10. We clustered their first-layer neuronsusing approaches presented in Section 3.1. We then used a correlation-based method from Watanabe(2019) to create visualizations of the clusters. Almost all show selectivity to one half of the input. InAppendix A.4, we detail our approach and compare to visualizations of random sets of units.
Figure 2: Our procedural pipeline. The first three steps generate a partitioning of the network into“subclusters” which we analyze using (4a) lesion and (4b) feature visualization methods to measureimportance and coherence compared to random subclusters. Finally, (not shown in the pipeline), weaggregate results to produce Fisher-Bates p values and effect measures. These final steps are shownin Figure 4.
Figure 3: Fisher-Bates p values for (1) lesion-based experiments measuring importance via overallaccuracy drops (Acc. Drop) and coherence via the range of class-wise accuracy drops (Class Range);and (2) feature visualization-based experiments measuring coherence via the optimization score offeature visualizations (Vis Score) and the entropy of network outputs (Softmax H). Each subfigurecorresponds to a type of network, where each row represents a partitioning method and each columna modularity proxy measurement. The Fisher-Bates p values are shown on a log scale rounded tonearest integer. If a p value is significant at an α = 0.05 level after Benjamini Hochberg correction(p ≤ pcrit = 0.025 ≈ 10-1.6), then its background is shaded. Otherwise, “n.s.” (not significant) isreported. Results are calculated as described in Section 3.2.
Figure 4: Our extended procedural pipeline. This figure expands Figure 2 and shows the successivesteps after generating a partitioning of subclusters (step 3 in Figure 2). After performing either lesionor feature visualization analysis, the results from each true cluster and its random clusters areaggregated to produce p values and effect measures. For simplicity, only the analysis for the lesionexperiment is presented, but the same pipeline is used for the feature vis experiments.
Figure 5: Histograms of feature visualization percentiles for four partitionings of VGG CIFAR-10 network. A VGG CIFAR-10 network is partitioned using four methods ({weights, activations}× {global, local}) and analyzed with the feature visualization experiment to produce the collectionof percentiles for each subcluster. This figure shows the percentile distribution for each clusteringas a histogram with a single percentile value per bin. Recall that a lower percentile means that atrue subcluster is disproportionately often more coherent than random subclusters while controllingfor layer and size. Under the null hypothesis, percentiles distribute uniformly. Visual inspectiondemonstrates that, for example, Weights / Global, Act / Global, and Act / Local deviate from theuniform distribution. In this paper, we use the Fisher-Bates procedure to conduct this hypothesistesting rigorously. Indeed, Figure 3 shows that these clustering of the VGG CIFAR-10 network aresignificant when aggregated over five models.
Figure 6: Comparison of true and random subcluster visualizations for the first layer of MLPs trainedon the halves-MNIST dataset. The first rows show visualizations for true subclusters, and the bottomfour show visualizations for random ones of the same size. Each panel gives results for a differentapproach to clustering: (a) weights/global, (b) weights/local, (c) activity/global, and (d) activity/local.
Figure 7: Example feature visualizations for true and random sub-clusters: In the left columnare shown true sub-cluster visualizations, and in the right column are visualizations of sub-clustersof random neurons of the same size in the same layer. (a) MLP, MNIST; (b) CNN, MNIST; (c)CNN-VGG, CIFAR-10; (d) VGG-16, ImageNet; (e) VGG-19, ImageNet.
