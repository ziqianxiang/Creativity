Figure 1: Left: a baseline Pre-LayerNorm transformer layer. Center: NormFormer, with the threeproposed additions in bold. Right: a single attention head with our proposed HeadScale operationapplied prior to the output projection with trainable parameters γi . * When applied, residual scalingimpacts the second residual connection in each layer.
Figure 2: Pretraining perplexity on held-out validation data for Causal and Masked Language Mod-els as a function of training compute (GPU days). The blue stars show the point where a modelmatches the baseline’s lowest perplexity.
Figure 3: Average L1 norm of gradients to the second fully connected weight for layers 0,1,6,10 and11, early in training.
Figure 4: Distribution of learned scaling parameters in three of the added operations. For FFN LN,earlier layers receive downscaled inputs, keeping their gradients in the same range as the gradientsof later layers. This plot is discussed in detail in Section 6.
Figure 5: LR Stability Test: learning rate starts from 0 and linearly increases by 5e-5 at each train-ing step until training destabilizes. NormFormer reaches a higher learning rate before destabilizing.
Figure 6: Average λresid weights at each layer of different sized CLMs in the NormFormer+λresidsetting. Depth is layer number / total layers.
Figure 7: Change in grad norm with each operation of NormFormer compared to the baseline.
