Figure 1: Our neural acoustic fields learn an implicit representation for acoustic propagation. (a) A 3D top-downview of the room. (b) Walkable regions shown in grey. (c) and (d) NAF captures the directional nature of sound.
Figure 2: Overview of NAF. Given a listener position and an emitter location, We first query a grid for localfeatures using bilinear interpolation. We compute the sinusoidal embedding of the positions, phase, and time,and query a discrete embedding matrix using the orientation and left/right ear. These features are fed to animplicit decoder. Our method is trained with a MSE loss with impulse responses.
Figure 3: Qualitative Visualization of Test Set Impulse Response Prediction. Left: Example log-STFT ofimpulse responses and predictions from NAF. Right: (a)-(c) Three examples of ground truth waveform of theimpulse response, and the corresponding NAF generated waveform reconstructed with Griffin-Lim.
Figure 4: Qualitative Visualization of Neural Acoustic Fields. (a) The 3D structure of the room. (b)Walkable regions shown in grey. (c) Strength of predicted impulse response given a emitter location, lightercolor indicates louder sound.
Figure 5: Local Geometric Conditioning.
Figure 6: Qualitative Grid Visualization. Visualization of the learned grid features with principal componentanalysis. When trained using acoustic information, the grid learns highly structured information. When usingNeRF, the grid is highly noisy. Structure emerges when we train both frameworks together.
Figure 7: Qualitative Visualization of Cross-Modal Image Generation. Qualitative comparison betweenNeRF learned jointly with a NAF with RGB and acoustic supervision, and NeRF learned with only RGBsupervision. We observe fewer floating artifacts when jointly training with audio. (a)-(c) Three views from"Large 1". (d)-(f) Three views from "Large 2".
Figure 8: Qualitative Visualization of Sound Localization. Our NAFS Can localize where a sound is comingfrom given listener samples. (a)-(d) Given unperturbed audio, the NAF can model the sound by placing thesource anywhere in the scene. For each grid location as emitter, we evaluate how well the listener responsematches the observation. The actual emitter location is shown in red. Lighter color indicates that if an emitterwas placed here, there is lower error compared to observations, which matches well with the real emitter location.
Figure A1: Additional Qualitative Predictions of NAF. Qualitative visualization of the loudnessmap as predicted by NAF across four different rooms.
Figure A2: Architecture of the model that uses emitter and listener specific local geometry condition-ing.
Figure A3: Architecture of the model that uses no local geometry conditioning.
Figure A4: A room the emitter-listener probes. (a) The 3D structure of a room. b The probes markingthe location of emitters/listeners.
Figure A5: Visualization using interpolated sound. We show the loudness at different locationsusing (a)-(b) nearest-neighbor interpolation and (c)-(d) linear interpolation.
Figure A6: Qualitative Visualization of impulse waveforms at unseen emitter/HStener locations. Fromleft to right, we show the ground truth waveform, the Griffin-Lim recovered waveform, and the waveform learnedby a network in the time domain for two locations (a)-(b) not seen in training.
Figure A7: We interpolate linearly between left/right latents. Here we show two examples (a)-(b)where we take linear steps between the latent representing the left ear, and the latent representing theright ear. Pay attention to the onset (left) of each spectrogram.
