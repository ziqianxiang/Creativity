Figure 1: (Game 1:) Convergenceto the cooperative NE€	∆θ*(s = 1)	ratio (mean ± std)%-0^	-433.3-	(47.8± 5.1)%^00T	-979.3-	(66.3± 4.3)%^wτ	2498.6	(77.4± 2.8)%Table 3: (Game 1:) Relationship of convergence ratio and .
Figure 2: (Game 2:) Starting froma close neighborhood of a fullymixed NEFigure 4: (Game 2:) NE-gap formultiple runsFigure 3: (Game 2:) Total rewardfor multiple runsGame 2: coordination game Our second numerical example studies the empirical performance ofexact gradient play for an identical-reward game which is a special class of Markov potential game.
Figure 4: (Game 2:) NE-gap formultiple runsFigure 3: (Game 2:) Total rewardfor multiple runsGame 2: coordination game Our second numerical example studies the empirical performance ofexact gradient play for an identical-reward game which is a special class of Markov potential game.
Figure 3: (Game 2:) Total rewardfor multiple runsGame 2: coordination game Our second numerical example studies the empirical performance ofexact gradient play for an identical-reward game which is a special class of Markov potential game.
Figure 5: (Game 3:) State-based coordination game, rewards are nonzeroif both players locate at the same shaded gridsFigure 6: Total reward J (θ(t) )keeps increasing*a	%b	NCPlayer 1I 0.2501	0.0465	0.01450.0457	0.0363	0.07440.0143	0.0734	0.4450ya Vb ycPlayer 1Figure 8: NE-gap convergesto a value close to zero. Herethe NE-gap is measured byFigUre 7： MarginaldistribUtiOn dθ (si,x ,s2,χ)and dy (sι,y ,s2,y)	maxi maχ(s,ai)Aiθ (s,ai)Game 3: state-based coordination game OUr third nUmerical example stUdies the empiricalperformance of the sample-based learning algorithm, Algorithm 1. Here we consider a generalizationof coordination game (Game 2) where the two players now try to coordinate on a 2D grid. Thetwo-player state-based coordination game on a 3 × 3 grid is defined as follows： the state space is
Figure 6: Total reward J (θ(t) )keeps increasing*a	%b	NCPlayer 1I 0.2501	0.0465	0.01450.0457	0.0363	0.07440.0143	0.0734	0.4450ya Vb ycPlayer 1Figure 8: NE-gap convergesto a value close to zero. Herethe NE-gap is measured byFigUre 7： MarginaldistribUtiOn dθ (si,x ,s2,χ)and dy (sι,y ,s2,y)	maxi maχ(s,ai)Aiθ (s,ai)Game 3: state-based coordination game OUr third nUmerical example stUdies the empiricalperformance of the sample-based learning algorithm, Algorithm 1. Here we consider a generalizationof coordination game (Game 2) where the two players now try to coordinate on a 2D grid. Thetwo-player state-based coordination game on a 3 × 3 grid is defined as follows： the state space isgiven by S = S1 × S2, S1 = S2 = Sx × Sy = {xa,xb,xc} × {ya,yb,yc}, action space is given byA = A1 × A2, A1 = A2 = {Stay, Left, Right, Up, Down}, i.e., agent can choose to stay at cUrrent
Figure 8: NE-gap convergesto a value close to zero. Herethe NE-gap is measured byFigUre 7： MarginaldistribUtiOn dθ (si,x ,s2,χ)and dy (sι,y ,s2,y)	maxi maχ(s,ai)Aiθ (s,ai)Game 3: state-based coordination game OUr third nUmerical example stUdies the empiricalperformance of the sample-based learning algorithm, Algorithm 1. Here we consider a generalizationof coordination game (Game 2) where the two players now try to coordinate on a 2D grid. Thetwo-player state-based coordination game on a 3 × 3 grid is defined as follows： the state space isgiven by S = S1 × S2, S1 = S2 = Sx × Sy = {xa,xb,xc} × {ya,yb,yc}, action space is given byA = A1 × A2, A1 = A2 = {Stay, Left, Right, Up, Down}, i.e., agent can choose to stay at cUrrentgrid or move left/right/Up/down to its neighboring grids. We assUme that there is random noise dUringthe transition; for example, if agent 1 is staying at the middle grid (xb, yb), then：P (s1,x =	xa |s1,x	= xb, a1	= Stay) = P (s1,x	=	xc |s1,x	=	xb, a1 =	Stay) = ,P (s1,x =	xb |s1,x	= xb, a1	= Stay) = 1 - 2,P (s1,y =	ya |s1,y	= xb, a1	= Stay) = P (s1,y	=	yc |s1,y	=	xb, a1	=	Stay) = ,P (s1,y =	yb |s1,y	= yb, a1	= Stay) = 1 - 2.
