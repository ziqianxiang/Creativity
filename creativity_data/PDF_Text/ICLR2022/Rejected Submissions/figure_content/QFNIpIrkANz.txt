Figure 1: (a) Probabilistic graphical model of a transition under influence of the index variable E andlatent variable W . The stable conditional is highlighted in blue. (b) Spurious correlations assumingstate-only formulation. (c) Spurious correlations assuming wrong edge orientation Ot → st+1.
Figure 2: Gridworld ERM vs IRM reward recovery. (a) expert trajectory datasets: 1st group (blue)400 trajectories, 2nd group (white): 25 trajectories, 3rd group (green): 3 trajectories. (b) MaxEnt IRLERM baseline (c) MaxEnt IRL ERM baseline with L2 regularization coefficent 1e-3 (d) MaxEnt IRLwith IRM penalty, λ = 0.01, (e) MaxEnt IRL with IRM penalty, λ = 0.055.2	Adversarial setting: PyBullet robot environmentsSetup In this section, the experiments are performed in PyBullet (Ellenberger, 2018-2019) gym(Brockman et al., 2016) environments, which are an open-source alternative to the MuJoCo physics7Under review as a conference paper at ICLR 2022simulator. We generate the demonstration datasets by training policies using the PPO algorithm(Schulman et al., 2017) 1 2 * and varying the dynamics of the environments. An environment is definedas an instance of the variable E, which has an impact on the prior distribution of transitions (andby extension, trajectories) p(s, a, s0) and p(τ) in a minibatch of rolled out on-policy and experttransitions. We have used 10 expert trajectories for every environment for the physical parametermodification experiments. For the goal intervention experiments, we have used a proportion of 1, 3and 10 trajectories for the three goal directions respectively.
Figure 3:	Policy performance of the ERM vs IRM methods on out-of-distribution settings in thephysical parameter intervention settingResults on physical parameter interventions We test the performance using the policy rolloutmethod with respect to the ground truth reward. We compare the policies trained using the adversarialapproach with and without the invariance penalty. The evaluation environment setting is chosen to beoutside of the value range presented to the model at training time. We can observe in Fig. 3 that theIRM-regularized version of AIRL outperforms the baseline in a zero-shot generalization setting inboth of the locomotion environments (CustomAntMuJoCo and CustomHalfCheetahMuJoCo)2), where interventions were performed on the hind leg length (2x and 3x of original length) andfriction coefficients respectively. The friction coefficients have been modified to have a value of(1.5, 0.1, 0.1) and (2.0, 0.6, 0.6) tangential, torsional and rolling coeffients respectively. In constrast,we can see that for the CustomReacherPyBulletEnv environment, where the demonstrationswere recorded by policies acting on the environment with a varying gravity coefficient, both modelsperform similarly. We attribute this to the fact that the gravity plays little role in the context of theReacher environment.
Figure 4:	Policy performance of the ERM vs IRM methods on out-of-distribution environments in agoal intervention setting1stable-baselines3 implementation22*MuJoCo refers to the environment (MDP) specification. The simulation engine is PyBullet.
