Figure 1: Learning low-frequency (left) and high-frequency (right) cubic polynomials over the hy-percube d = 30, using KRR with HFC (FC), HGFCP (FC-GP), HCK (CK), HωCK (CK-LP) and HGCPK(CK-GP), and regularization parameter λ = 0+ . We report the average and the standard deviationof the test error over 5 realizations, against the sample size n.
Figure 2: Decay of the weights κj for different lenfths ω for local average pooling (on the left) andbandwidths σ2 for pooling with Gaussian filter (on the right), for d = 101.
Figure 3: Impact of downsampling on the eigenvalues κj . On the left, we fix ω = 25 (d = 200,q = 30, r = 1) and increase δ from 1 (no downsampling) to 40. On the right, we compare ∆ = 1(continuous line) and ∆ = ω (dashed lines), with d = 150,q = 20,r = 1.
Figure 4: Heatmap of the eigenvectors {vj }j∈[d] ordered from highest associated eigenvalue (bot-tom) to lowest (top), for d = 200, q = 30, r = 1, ω = 25, and ∆ = 1 (left), ∆ = ω = 25 (middle)and ∆ = 40 (right).
Figure 5: Test error of kernel ridge regression with and without downsampling. We report the testerror of one realization, against the sample size n. On the left, we consider a unique architectureq = 10 and ω = ∆ = 5, and compare HωCK (continuous line) versus HωCK,∆ (dashed line) whenlearning cyclic q-local polynomials of degree 2, 3 and 4. On the right, we consider a unique cyclicq-local polynomial of degree 3 for fixed q = 10, ω = 10 and ∆ ∈ {1, 3, 6, 10}.
Figure 6: Learning cyclic polynomials of degree 2 (top), 3 (middle) and 4 (bottom) over the hy-percube d = 30, using KRR with HFC (FC), HGFCP (FC-GP), HCK (CK), HωCK (CK-LP) and HGCPK(CK-GP),regularization parameter λ = 0+ and h(χ) = Pk∈[7] 0.2 ∙ Xk. We report the average andthe standard deviation of the test error over 5 realizations, against the sample size n.
