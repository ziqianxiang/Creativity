Figure 1: The collision effect inlatent space-based BO tasks Whenhaving 100 data points and observa-tions. Because the data points aroundthe optimum severely collided, BO ismisguided to the sub-optimum. Dif-ferent from noise Which normally as-sumed to be Gaussian, the collisioncould be much more divergent. Seeextended discussion in Appendix B.1.
Figure 3: Experiment results on six pre-collected datasets. Each experiment is repeated at leasteight times.The colored area around the mean curve denotes the √σ=. Here σ denotes the empiricalstandard deviation. n denotes the number of cases repeated in experiments. Note that the BO andTPE implementation we tested can not finish in reasonable time for figure 3a and figure 3b.
Figure 5: Illustrate the 1-D latent space of Feynman III.9.52 dataset. The second row shows theratio that the penalty define as equation 1 is non-zero. The third row shows point-wise estimation ofλ. 5a shows a regularized latent space with a few observable collisions. 5b shows a non-regularizedlatent space with bumps of collisions especially around the maxima among the observed data points.
Figure 6: These curves show the network design test results. The collision value shown here is thepenalty term proposed in equation 1. The x-axis denotes the neural network’s general complexity.
Figure 7: Illustrate regression task on Rastrigin-2D dataset. 7a shows a non-regularized latent spaceafter sufficiently trained as is demonstrated in 7b. 7b shows the NLL of the training process. 7ashows the corresponding MSE.
Figure 8: Network graph of a (L + 1)-layer dense network with D input units and 1 output units. Inour experiments, L is set to be 3.
Figure 9: Simple regret under different parameter settings on the Rastrigin 2D dataset. The coloredarea represents the standard error of the tests at certain iteration. Each experiments are repeatedeight times. The figure shows that a moderately large λ suffices to achieve decent performance interms of simple regret. We believe that the wide range of objective values of the test dataset, whichotherwise would hurt the optimization performance, can be regularized by the collision penalty. Thecurves demonstrate the decent performance of DW LOCo as long as the parameters are not set tobe too small.
Figure 10: Experiment results on six synthetic & real datasets. Each experiment is repeated at leasteight times. The shaded area around the mean curve denotes the -√n. Here σ denotes the empiricalstandard deviation. n denotes the number of cases repeated in experiments. As illustrated in thefigure, the random-embedding-based methods have been significantly outperformed by LOCo andDW LOCo. We place the discussion over REMBO here for two reasons. Firstly, there has beenseveral problems about REMBO as discussed in section 2. Secondly, the experiments are conductedon tasks where the effective dimensions are at a similar scale as the dimensionality of the originalinputs and doesn’t align with the assumption of REMBO.
