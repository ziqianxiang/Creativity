Figure 1: Example merge conflict represented through standard diff3 (left) and token-level diff3(center), and the user resolution (right). The merge conflict resolution takes the token-level edit b.
Figure 2: An overview of the MergeBERT architecture. From left to right: given conflictingprograms A, B and O token-level differencing is performed first, next, programs are tokenized andthe corresponding sequences are aligned (a|o and o|a , b|o, and o|b). We extract edit steps for eachpair of token sequences (∆ao and ∆bo). Four aligned token sequences are fed to the multi-inputencoder neural network, while edit sequences are consumed as type embeddings. Finally, encodedtoken sequences are summarized into a hidden state which serves as input to classification layer. SeeAlgorithm 1 for details about merge resolution decoding. Parts of the neural networks colored in blueare finetuned, the rest are transferred from pretrained encoder and frozen.
Figure 3: Summary of merge conflict resolution labels in our dataset for TypeScript: label distributionfor merge conflicts extracted with the standard (line-level) diff3 algorithm, right.
Figure 4: Summary of merge conflict resolution labels in our dataset for TypeScript: label distributionfor merge conflicts extracted with token-level differencing algorithm.
Figure 5: MergeBERT model trained from scratch as compared to finetuning training for sequenceclassification downstream task with the encoder weights transferred and frozen during finetuning.
Figure 6: MergeBERT input representation. The input embeddings are a sum of the token embeddings,the type embeddings and the position embeddings. Type embeddings are extracted from the editsequence step that represent how to turn the a|o sequence into the o|a11.4	Implementation DetailsWe pretrain a BERT model with 6 encoder layers, 12 attention heads, and a hidden state size of768. The vocabulary is constructed using byte-pair encoding method (Sennrich et al., 2016) andthe vocabulary size is 50000. We set the maximum sequence length to 512. Input sequences coverconflicting regions and surrounding code (i.e., fragments of Pref and Suff) up to a maximumlength of 512 BPE tokens. The backbone of our implementation is HuggingFace’s RobertaModeland RobertaForSequenceClassification classes in PyTorch, which are modified to turnthe model into a multi-input architecture shown in Fig. 2.
Figure 7: An example of a file with multiple conflicting regions.
Figure 8: Example real-world merge conflict resolved by MergeBERT. (Top) merge conflict repre-sented through the standard diff3, (middle) corresponding token-level conflicts, and (bottom) theuser resolution.
