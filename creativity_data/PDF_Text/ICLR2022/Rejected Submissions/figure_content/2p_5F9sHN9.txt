Figure 1: An adversarial subspace. (Left) For any given input example, we define asubspace where the boundary is closest to the input. The white arrows depict our sampling0.25-I 0.0-exemplarscheme, which uniformly samples directions within the adversarial subspace where we measurethe distance to the decision boundary. The perturbation images are rescaled for visualization,but their 02 length is indicated on the axes. Each color corresponds to the class categories fordeer (class 4), dogs (class 5), and horses (class 7). This visualization is for an adversariallytrained ResNet-50. (Right) We then measure the curvature of the decision boundary, bothin the high-dimensional input space and in the adversarial subspace.
Figure 2: Distances to the decision boundary. The distance to the decision boundaryfor random perturbations within an adversarial subspace grows sub-linearly with increaseddimensionality. (Upper left) The dots indicate the adversarial perturbation length for thatdimension (i.e. along the perturbation axis), averaged across images. The box plots show thedistance to the decision boundary for uniformly-sampled directions within the n-dimensionaladversarial subspace, where n is increasing along the horizontal axis. The box indicatesthe interquartile range, the whiskers indicate the 10% and 90% data percentiles, and thesolid horizontal lines indicate the mean. (Lower left) Same as the above, except now thevertical axis indicates the ratio of the largest distance to the decision boundary divided bythe length of the given subspace dimension,. That this is small indicates that we rarelysampled a boundary point more than 50% farther away than the largest adversarial dimension.
Figure 3: Boundary visualizations. Each subplot visualizes a two dimensional cross-section of the decision space for a naturally trained network. Each cross-section is spannedby the vectors corresponding to the largest distance coefficient found for a given inputimage (visualized as black arrows) and the according adversarial perturbation of the largestdimension of the subspace from which the distance coefficient was retrieved. The red dotsindicate the position of the described adversarial perturbation. The number in the top rightcorner depicts the distance coefficient of the cross-section (as reported in Figure 2, bottomleft panel). Areas of the cross-section that lie outside the image bounds are shaded.
Figure 4: Decision boundary curvature. All plots include data for 50 correctly classifiedtest images with evenly distributed labels and the first 10 adversarial subspace dimensions.
Figure 5: Adversarial Class Composition. (Left) Within the adversarial subspaces ofrandomly chosen test images we quantified how many unique adversarial classes exist. Inmost cases adversarial training increases the diversity of alternate classes close to any testimage. (Right) Each plot shows the distribution of nearby alternate classes for each originclass with natural and adversarial training. Adversarial training results in a more evendistribution of nearby classes for all categories except bird and ship.
Figure 6: CIFAR-10 example adversarials. The panels show adversarial examples fora given input image. Column pairs are sorted by adversarial dimension. Within a pair,each column includes examples for the natural and robust models, respectively. The labelsbelow the images indicate the class assigned by the model. The numbers in brackets are the£2-norms of the respective images.
Figure 7:	Ratio of out of bounds samples. Weuniformly sample the distance to the decision bound-ary with increasing dimensionality per input image andadversarial subspace dimension. We report the frac-tion of directions pointing to out-of-bounds boundaries,averaged across 100 images.
Figure 8:	Perturbation lengths and dimensionality. (left) The subfigure shows thedistribution of the n-th found adversarial perturbations across images. (right) We tested atdistance increments of 0.001 how many adversarial dimensions were found at that distanceper image. The lines represent the means over all test images and the color shaded areasshow the 10th and 90th percentiles.
Figure 9: Distributions of distances to decision boundary. (A,B) Distributions fornatural model. (C,D) Distributions for adversarially trained model. (A-C) We show thecomplete set histograms corresponding to Figure 2 dimensions. (C,D) Additionally, weprovide the mean-normalized histograms to corresponding to the histograms in A and C toemphasize the smooth increase in variance of distances with increasing dimensionality.
Figure 10:	Distances to the decision boundary. Compare with Figure 2.
Figure 11:	Perturbation lengths and dimensionality. Compare with Figure 8.
Figure 12:	Distances to the decision boundary. Compare with Figure 2.
Figure 13: Perturbation lengths and dimensionality. Compare with Figure 8.
Figure 14: Ratio of out of bounds samples. Compare with Figure 7.
Figure 15:	Ratio of out of bounds samples. Compare to Figure 7.
Figure 16:	MNIST distances to the Decision Boundary. We repeated the distanceanalysis from Figure 2 on the MNIST dataset Only test images with at least 8 adversarialdimensions were included in this FigureE Seed consistencyFigure 21: Sampling size in adversarialsubspaces. The line shows the mean runningdifference in distance to the decision boundarymeans across 100 samples as one increases thesample size from 5 to 100. The shaded areadepicts the standard deviation from the meanline.
Figure 21: Sampling size in adversarialsubspaces. The line shows the mean runningdifference in distance to the decision boundarymeans across 100 samples as one increases thesample size from 5 to 100. The shaded areadepicts the standard deviation from the meanline.
Figure 17: MNIST boundary visualizations. This figure is the MNIST version of Figure3. Note that for MNIST, the decision boundary lies outside the valid pixel range more oftenthan for CIFAR.
Figure 18: MNIST perturbation lengths and dimensions. Compare with Figure 8.
Figure 19: MNIST example adversarials. Compare to Figure 6.
Figure 20: MNIST decision boundary curvature. Compare with Figure 4of adversarial subspaces and transferable subspaces are therefore grounded on ill-definedsubspaces, spanned by non-orthogonal vectors.
Figure 22:	Optimization consistency on MNIST. The bars show number of samplesfor which at least n dimensions were found during optimization. Different colors indicatedistinct seeds for model training.
Figure 23:	Distance consistency on MNIST. Box plots show the length of the adversarialvector as one increases dimensionality. The data is computed for a single image, where thevariation comes from five different seeds used to initialize the networks before training. Notethat the variation is much smaller than the variation across images for a single seed, as canbe seen in Figure 16.
Figure 24: Clipping causes non-orthogonality. The histograms show angles betweenvectors that define “subspaces" in the GAAS method proposed by Tramer et al. (2017) (left)and distance to decision boundary estimations by He et al. (2018).
