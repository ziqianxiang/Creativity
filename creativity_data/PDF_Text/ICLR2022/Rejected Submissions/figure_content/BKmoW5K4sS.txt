Figure 1: Test accuracy of unconstrained and fair models in the presence of adversarial bias -COMPAS dataset. The x-axis is the ratio of the size of Dp to the size of clean dataset Dc , andreflects the contamination level of the training set. We compare the impact of adversarial bias withbaselines and poisoning attacks against unconstrained models, for various . The difference betweentest accuracy at = 0 (benign setting) and a larger value reflects the impact of the bias. Constantprediction always outputs the majority label in a clean dataset. The enforced fairness level δ is 0 and0.01 for the fair model (Hardt et al., 2016) and fair model (Agarwal et al., 2018), respectively.
Figure 2: Fairness gap on the unconstrained model with respect to the training data - COMPASdataset. An unconstrained model is learned on the training data that includes Dp generated by variousalgorithms. The fairness gap is defined in (1). The numbers reflect how unfair this unconstrainedmodel is with respect to the protected group on the training data.
Figure 3: Effect of fairness level δ on robustness across groups in the presence of adversarial bias 一COMPAS dataset. For a given , training dataset is the same for all the algorithms and Dp is generatedusing Alg. 2 with λ = 100. Fair models are trained using reduction approach (Agarwal et al., 2018).
Figure 4: Effect of fairness level δ on training accuracy across groups in the presence of adversarialbias - COMPAS dataset. For a given , Dp is the same for all algorithms (generated using Alg. 2 withλ= 100).
Figure 5: Training accuracy for group-based classifier - COMPAS dataset. We train models onlyon the majority or minority training data and compare their training accuracy. For a given , Dp isgenerated using Alg. 2 (λ = 100).
Figure 6: Test accuracy of unconstrained and fair models under adversarial bias - COMPAS dataset.
Figure 7: Test accuracy of unconstrained and fair models under adversarial bias - Adult dataset. Theenforced fairness level δ is 0 and 0.01 for the fair model (Hardt et al., 2016) and fair model (Agarwalet al., 2018), respectively.
Figure 8:	Test accuracy of unconstrained and fair models under adversarial bias - Synthetic dataset.
Figure 9:	Test accuracy of unconstrained and fair models under adversarial bias - MEPS dataset. Theenforced fairness level δ is 0 and 0.01 for the fair model (Hardt et al., 2016) and fair model (Agarwalet al., 2018), respectively.
Figure 10: Effect of fairness level δ on robustness across groups under adversarial sampling andadversarial labeling bias - COMPAS dataset. The reductions approach (Agarwal et al., 2018) is usedto train fair models. The majority group (whites) contributes 61% of the test data.
Figure 11:	Effect of fairness level δ on robustness across groups under adversarial sampling andadversarial labeling bias - Adult dataset. The majority group (males) contributes 66% of the test data.
Figure 12:	Effect of fairness level δ on robustness across groups under adversarial sampling andadversarial labeling bias - Synthetic dataset. The majority group contributes 54.4% of the test data.
Figure 13:	Effect of fairness level δ on robustness across groups under adversarial sampling andadversarial labeling bias - MEPS dataset. The majority group contributes 58.9% of the test data. Thereductions approach (Agarwal et al., 2018) is used to train fair models.
Figure 14:	Test accuracy of unconstrained and fair models with respect to equal opportunity in thepresence of adversarial bias - COMPAS dataset. The x-axis e is the ratio of the size of DP to the sizeof clean dataset Dc , and reflects the contamination level of the training set. We compare the impactof adversarial bias with baselines and adversarial bias against unconstrained models, for various e.
Figure 15:	Accuracy of clean training data and Dp under adversarial sampling and adversariallabeling bias - COMPAS dataset. The reductions approach Agarwal et al. (2018) is used to train allfair models.
Figure 17: Accuracy of clean training data and Dp under adversarial sampling and adversarial labelingbias - Synthetic dataset. Fair models are trained using reduction approach (Agarwal et al., 2018).
Figure 19:	Fairness gap on the unconstrained model with respect to the training data - COMPASdataset. An unconstrained model is learned on the training data that includes adversarial biasgenerated by Alg. 2 (λ = 100). The fairness gap ∆ is defined in (1). The numbers reflect how unfairthis unconstrained model is with respect to the protected group on the training data.
Figure 20:	Fairness gap on the unconstrained model with respect to the training data - Adult dataset.
Figure 21:	Fairness gap on the unconstrained model with respect to the training data - Syntheticdataset. An unconstrained model is learned on the training data that includes adversarial biasgenerated by Alg. 2 (λ = 100).
Figure 22:	Fairness gap on the unconstrained model with respect to the training data - MEPS dataset.
Figure 23: Distribution of DP - COMPAS dataset. We report the protected attribute (s = 0 forwhites and s = 1 for blacks) and labels of data points in Dp for various . For every value of , thenumber for each combination of the protected attribute and label reflects the percentage of pointswith this combination in DP . Algorithm 2 with λ = 0 is the same as the attack algorithm proposed inSteinhardt et al. (2017).
Figure 24: Distribution of DP - Adult dataset. We report the protected attribute (S = 0 for males ands = 1 for females) and labels of data points in Dp for various . Algorithm 2 with λ = 0 is the sameas the attack algorithm proposed in Steinhardt et al. (2017).
Figure 25: Distribution of the DP - Synthetic dataset. Algorithm 2 with λ = 0 is the same as theattack algorithm proposed in Steinhardt et al. (2017).
Figure 26: Distribution of the DP - MEPS dataset. We report the protected attribute (S = 0 for non-white ands = 1 for white) and labels of data points in Dp for various . For this dataset, (s = 0, y = -) represents thesmallest subgroup.
