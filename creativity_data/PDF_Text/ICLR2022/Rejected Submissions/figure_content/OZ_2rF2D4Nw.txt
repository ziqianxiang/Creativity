Figure 1: (a) A typical work flow when using Kokoyi; (b) Jupyter notebook example of a multi-layerRNN encoder; (c) The corresponding auto-generated module in PyTorch.
Figure 2:	Grammar of KOKOYI IR. e and vrepresents expression and variable, respectively.
Figure 3:	(a) Attention module in kokoyi-lang; (b) Python code generated by Kokoyi; (c)Intermediate representation of the program.
Figure 4: Relative training speedv.s. PyTorchFigure 5: Per-iteration training time under different batch sizesfor MLP (left) and Transformer (right).
Figure 5: Per-iteration training time under different batch sizesfor MLP (left) and Transformer (right).
Figure 6:	Classification of the papers sampled from several recent conferences12Under review as a conference paper at ICLR 2022B S tudy of Transformer CodeTo quantify the actual Line-of-code saving of Kokoyi on the Transformer model, we compare theimplementation in PyTorch and Kokoyi side-by-side in Figure 7. Overall, a large portion ofthe PyTorch implementation is for manipulating shapes of the tensors to align with the batchingrequirement of the subsequent operations. By contrast, these non-math parts are completely absentin the Kokoyi implementation thanks to the flexible and succinct tensor constructor syntax. Yet,Kokoyi can still achieve similar training speed by its auto-batching design.
Figure 7:	Comparison of the multi-head attention code in PyTorch and Kokoyi. We hightlight theparts of shape manipulation that are saved by Kokoyi.
