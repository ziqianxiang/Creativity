Figure 1: Graphical model for duration modification. γ and θ are the model parameters inferredduring training. Attention At is conditionally independent of target length T given X and Mcoding of the similarity matrix. While simple, DTW requires both the source and target utterancesto be known a priori. Hence, it cannot be used for on-the-fly modification of new signals.
Figure 2: Binary attention masks with 3 different slopes.
Figure 3: Neural network architecture used for the sequence-to-sequence speech generation. Theencoder and decoder modules consist of 10 identical blocks. Projection layers are simple feed-forward layers without any non-linearity to project input features in high dimension.
Figure 4: Comparing error in length prediction using encoder embeddings.
Figure 5: Comparing alignment similarity between attention map and DTW.
Figure 8: MOS of speech generated by our model evaluated via AMT.
Figure 10: Transformer model architecture.
Figure 11:	(a) Length prediction of target utterances and (b) measuring similarity of attention map toDTW cost matrix. The model is trained without masking constraint imposed in the attention layer.
Figure 12:	(a) Length prediction of target utterances and (b) measuring similarity of attention mapto DTW cost matrix. The model is trained without residual connection in the final decoder layer.
Figure 13: Examples of alignment path obtained using encoder-decoder attention map (left) andground truth DTW backtracking procedure (right). The source sequence lies on the x-axis and thetarget/generated sequence lies on y-axis. Note that, unlike DTW, the convolutional neural networkdoes not use the ground truth target utterance. The attention map and the DTW similarity matrixexhibits similar block structure arising due to the short-term stationarity of speech signals.
Figure 14: Examples of alignment path obtained using encoder-decoder attention map (left) learnedwithout masking constraint and ground truth DTW backtracking procedure (right). The sourcesequence lies on the x-axis and the target/generated sequence lies on y-axis. The attention map nolonger exhibits the block structure and is distributed uniformly across the frames of source sequence.
Figure 15: Examples of alignment path obtained using encoder-decoder attention map (left) learnedwithout residual connection and ground truth DTW backtracking procedure (right). The source se-quence lies on the x-axis and the target/generated sequence lies on y-axis. Once again, the attentionmap no longer exhibits the block structure and is distributed uniformly on y-axis in most cases.
