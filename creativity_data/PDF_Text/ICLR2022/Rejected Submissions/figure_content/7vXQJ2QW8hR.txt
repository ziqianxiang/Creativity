Figure 1: (a) Input space partitioning presents how deeper layers successively subdivide the space,where the newly introduced boundaries are in dark and previously built ones are in grey. We seethat: (i) the turning point of splines in later layers are exactly located at previous ones, and (ii)splines in the final classification layer are exactly the decision boundary (denoted as blue lines).
Figure 2: Classification task pruning using FCNets, where the blue lines represent subdivisionsin the first layer and the red lines denote the last layerâ€™s decision boundary. We see that: (i) pruningindeed removes redundant subdivision lines so that the decision boundary remains an X -shape until80% nodes are pruned; and (ii) ideally, one blue subdivision line would be sufficient to provide twoturning points for the decision boundary, e.g., visualization at 80% sparsity.
Figure 3: Classification task pruning using ConvNets, where to produce these visuals, We choosetwo images from different classes to obtain a 2-dimensional slice of the 764-dimensional input space(grid depicted on the left). We thus obtain a low-dimensional depiction of the subdivision splinesthat we depict in blue for the first layer, green for the second convolutional layer, and red for thedecision boundary of 6 vs. 9 (based on the left grid). We consistently find that only a fraction ofsplines are necessary to provide the turning points of final decision boundary.
Figure 4: Visual-ization of splinetrajectories, whichmainly adaptduring early phaseof training demon-strating the lotteryticket hypothesisfor DN partitions.
Figure 5: Visualization of the early-bird (EB) phenomenon, which can be leveraged to largely reducethe training costs due to the less training of costly overparametrized DNs.
Figure 6: Left: a small (L = 2, D1 = 5, D2 = 8) DN inputspace partition. Middle: the pruning criteria as in Eq. (2).
Figure 8: Additional visualization of the partitioning and subdivision layer after layer, where eachnode introduces a spline in the input space which is depicted under the current partitioning with ahighlighted path linked via a dotted line.
Figure 9: Accuracy vs. efficiency trade-offs of lottery initialization and layerwise pretraining.
Figure 10: K-means experiments on atoy mixture of 64 Gaussian.
Figure 11: Left: Depiction of a simple (toy) univariate regression task with target function beinga sawtooth with two peaks. Right: The `2 training error (y-axis) as a function of the width of theDN layer (2 layers in total). In theory, only 4 units are requires to perfectly solve the task at handwith a ReLU layer, however we see that optimization in narrow DNs is difficult and gradient basedlearning fails to find the correct layer parameters. As the width is increased as the difficulty ofthe optimization problem reduces and SGD manages to find a good set of parameters solving theregression task.
Figure 13: visualization of the early-bird (EB) phenomenon when using training samples, testingsamples, and random samples, respectively.
Figure 14: Visualization of spline trajectories using FCNet with Leaky ReLU activation functions.
Figure 15: Visualization of the early-bird (EB) phenomenon when training PreResNet-101 withLeaky ReLU activation functions.
