Figure 1: A schematic of our method. We wish to ensemble M feature extractors over a trainingdataset of n images, M MLPs are initialized as well as representations for each of the n images. Thetraining objective is for all of the features of the M models to be recoverable by feeding the learnedrepresentations through the respective MLP. The MLPs and learned representations are simultane-ously optimized via gradient descent, using a cosine loss. At inference time, the MLPs are frozen,and solely the image representation is optimized.
Figure 3: Our method applied to non-ImageNetdatasets, leveraging the generalization of ourpretrained feature extractors. Performance isimproved across all datasets.
Figure 2: Nearest neighbor accuracies on thevalidation split of ImageNet. Our method im-proves over all baselines by over 2%.
Figure 4: Ensembling varied models. We ob-serve that our method seems to capture spe-cialties/strengths of the component feature ex-tractors, particularly the symbolic-dataset effi-cacy of RotNet.
Figure 5: Despite not utilizing the consistencyof the supervised classification objective, ourmethod effectively combines supervised modelsto improve upon the performance of all other en-sembles considered.
Figure 6: Paralleling the efficacy of self-distillation (Zhang et al., 2019), our “ensem-bling" method proves to provide performancegains even whenjust one model is employed.
Figure 7: We experiment with a single BarlowTwins model on our generalization benchmark.
Figure 8: The MLPs do not necessarily needto be trained on the target dataset, but can betransferred from ImageNet to other downstreamdatasets. Here no parameter tuning is done oneach dataset, gradients are computed solely todirectly learn the representations. We find infact that this transfer approach maintains theperformance of directly learning new MLPs.
Figure 9: In the single-model case, transfer-ring φ still provides benefit over the baseline,but is less effective than learning the MLPs per-dataset.
Figure 10: Ablation of MLP depth, possibly sug-gesting that the low-rank tendency of deeper net-works serves as a regularizer on the learned rep-resentations. This results in improved representa-tion quality with network depth up to 6 layers.
Figure 11: Sorted singular value curves for ourmethod vs. the baseline features in an apples-to-apples setting (learning φ restricted to non-negative). Our method learns features with amore balanced set of singular values, indicatinga more uniformly spread bounding space.
Figure 12: Mean per-dataset accuracy, singleBarlow Twins model on varied dataset bench-mark. Learning rate ablation.
Figure 13: Mean per-dataset accuracy, singleBarlow Twins model on varied dataset bench-mark. Batch size ablation. Linear learning ratescaling rule followed.
Figure 14: Linear regression accuracies with cross-validated L2-regularization. Relative perfor-mance of our method is worse compared to the k-NN evaluation setting.
Figure 15: The normalized maximum similarity (measure of how close each test point’s nearestneighbor in the trainset is relative to average) for each method-dataset pair. Dashed lines indicatethe median for each distribution We see that the proposed approach generally has higher normalizedmaximum similarities, indicating relatively tighter clustering behavior.
