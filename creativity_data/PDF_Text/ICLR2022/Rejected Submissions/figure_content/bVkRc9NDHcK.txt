Figure 1: Hiding and uncovering audio data inside an image via VLVQ. Left: Original image. Right:Image with the audio data hidden inside. Bottom: 19 seconds of hidden audio extracted from theimage on the right.
Figure 2: Difference between the original image and the final container image containing 19 secondsof audio, inferenced using the same model with different conditions. The additional artifact encodingfor the STFT features (top part of the difference) becomes more apparent with larger values of γ,a coefficient used to trade-off between audio quality and image fidelity. (Left: γ = 0.1. Centerγ = 1.0. Right: γ = 10.0)Bao, 2005) have been applied to the task. Deep learning based approaches such as Huu et al. (2019)encodes STFT features of audio into the container image with a convolutional architecture. Ye et al.
Figure 3: The overall process of the VLVQ framework. Iterative encoding on H minimizes thedistance between original container image and the modified container image (Limg). Iterative de-coding on F minimally updates the container image (Ldec), while making the reconstructed audiodata to be as close to the ground truth (Lstft).
Figure 4: Conditioning module of the VLVQ framework. The architecture is conditioned on γ viathe FiLM layer (Perez et al., 2018), and the loss term is conditioned on γ via linear combination.
Figure 5: Evaluation of encoding variable number of chunks inside the VLVQ framework. Lightercolor indicates the encoding of later audio chunks. Despite being trained on maximum three chunks,the model can be reasonably extrapolated to encode longer sequences.
Figure 6: Conditional model (curve).
