Figure 1: Our proposed method FedPNAS consists of (1) a federated learning phase, where eachclient updates both the base component (ψb) and the personalized component (ψp) the architectureusing the FedPNAS update (Section 3.3) and sends its parameters to the central server for aggrega-tion; and (2) a fine-tune phase, where each client updates only the personalized component of thearchitecture using standard gradient update.
Figure 2: Feature mapping down the componentstacks of our architecture space. Every base celltakes as inputs (a) the outputs from its immediatepredecessor and (b) the one before it through askip-ahead connection. On the other hand, everypersonalization cell takes as input only the outputfrom the previous cell.
Figure 3: Plotting average classification accuracy of various methods against no. training epochs onheterogeneous tasks derived from (a) MNIST dataset; and (b) CIFAR-10 dataset. Figure (c) comparescumulative running time of various methods against no. training epochs on CIFAR-10 dataset.
