Figure 1: Aggregation weight entropy distribution of graphs. Low entropy means high degree ofconcentration, vice versa. An entropy of zero means all aggregation weights are on one source node.
Figure 3: Averaged accuracy (%) on graphs with noisy edges for 20 runs. Best view in colors.
Figure 2: Averaged accuracy (%) on CSBM(sparse split) for 20 runs. Best view in colors.
Figure 4: A tiny example of illustration of graph gradient and graph divergence. Best view in colors.
Figure 5: Aggregation weight entropy distribution of homophilic benchmark graphs. Low entropymeans high degree of concentration, vice versa. An entropy of zero means all aggregation weightsare on one source node.
Figure 6: Aggregation weight entropy distribution of heterophilic benchmark graphs. Low entropymeans high degree of concentration and vice versa. An entropy of zero means all aggregationweights are on one source node.
Figure 7:	The curves of training loss and testing accuracy for p = 1.
Figure 8:	Visualization of node embeddings for Cora dataset using t-SNE (van der Maaten & Hinton,2008)(a) 1âˆ™0GNN.
Figure 9: Visualization of node embeddings for Computers dataset using t-SNE.
Figure 10: Visualization of node embeddings for Chameleon dataset using t-SNE.
Figure 11: Visualization of node embeddings for Wisconsin Dataset using t-SNE.
