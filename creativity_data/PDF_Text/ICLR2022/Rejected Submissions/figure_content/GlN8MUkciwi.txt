Figure 1: Video Retrieval from Title and comments. We show the top 4 videos retrieved for theambiguous title “Look what I found!”, and from left to right we progressively add more commentswhich our model uses to refine the results.
Figure 2: Method Overview. We introduce a context adapter module that uses inputs of the auxiliarymodality to adapt the embedding of another branch. With this module the model is able to accept ordiscount information.
Figure 3: Varying Number of Comments. Themodel can effectively benefit from additional aux-iliary information and models trained With morecomments become better at extracting informa-tion even When feWer are available during testing.
Figure 4: Failure Case. A heatmap showing thesimilarities between the image adapted with dif-ferent comments (rows), and captions (columns).
Figure 5: Examples of retrieved video thumbnails when adapting the text branch.
Figure 6:	Examples of retrieved titles when adapting the visual branch.
Figure 7:	Visualising comment saliency. We show the title and thumbnail for three videos, andshow the ranked saliency of comments when adapting using the Text branch (left) and Image branch(right). Comments mentioning topics relevant to the title or image are ranked highly, while irrelevantcomments are lower.
Figure 8: We show a histogram of comment statistics on KineticsComments.
Figure 9: Examples of failure cases where using comments confounds the model and leads to a moremismatched retrieved thumbnail.
Figure 10: We show a diagram of the feature extraction and Context Adapter Module for the case ofadapting the Video Feature. Multi-Head Self-Attention is performed on the input tokens (which arethemselves video or textual features) as part of a transformer architecture consisting of two ResidualAttention blocks. Finally the output token corresponding to the Video Feature is passed through afinal linear layer and added to the original feature in a residual fashion.
