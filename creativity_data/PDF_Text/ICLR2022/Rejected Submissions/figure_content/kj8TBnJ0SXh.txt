Figure 1: Facial detail hallucination and rendering. Given a single input image, a target expression(in this case ‘Happy’), and an initial detailed geometry extracted using FDS (Chen et al., 2019) (shownin column ‘Reconstruction’ and row ‘Detailed Shading’) as input, our method hallucinates facialgeometric details consistent with the target expression. The hallucinated details are added to thesmooth proxy geometry (marked as ‘Proxy Shading’, also extracted using FDS (Chen et al., 2019)), togive a detailed geometry with facial details consistent with the target expression (in column ‘Happy’and row ‘Detailed Shading’). The detailed geometry is then rendered using Neural Rendering to givethe final image (first row of the column labelled ‘Happy’). A zoom-in of one of the predicted facialdetails and its render is shown in column ‘Predicted Facial Detail’ and ‘Render of Predicted FacialDetail’ respectively. (Electronic zoom recommended)1	IntroductionModelling the geometry of the human face continues to attract great interest in the computer visionand computer graphics communities. Strong PCA-based priors make 3D morphable models (3DMMs)(Blanz et al., 1999) robust, but at the same time over-regularize them. Thus, they fail to capture finefacial details, such as the wrinkles on the forehead when the eyebrows are raised or bumps on thecheeks when one smiles. Additionally, the lack of diversity in the texture space of most available3DMMs makes it very hard to generate realistic renderings that capture the large variations of colorand texture we observe in human faces. Recent methods (Tewari et al., 2019; 2018; Tran & Liu,2018; Tran et al., 2019; Zhu et al., 2020; Booth et al., 2017; Dou et al., 2017; Jackson et al., 2017;Kim et al., 2018; Genova et al., 2018) address this by learning richer shape and expression spaces
Figure 2: Detail Hallucination Network. Given a tar-get expression y, DetH outputs a detail hallucinationD(Iy)H and a detail mask D(Iy)M. The detail hallu-cination is combined with the input detail map D(Ix)using the detail mask to give the final hallucinated facialgeometric detail map D(Iy).
Figure 3: The Rendering Network. The Rendering Network, R first predicts a Neural Texture Map NTMfrom the given input texture map T using NTex φ. The NTM is then rasterized using both the proxy (geometryw/o details) and the detailed geometry and input into an image rendering network Tex2Im. Tex2Im generates arendering of the details ID and a low-resolution image ILR containing only detail-invariant image texture. Theyare added together in the final rendered image Iy.
Figure 4: Expression Change. Here we show the results of detail hallucination and rendering as the expressionchanges. The first column is the input image, the second column is the reconstruction (the detailed shadingunder column ‘Recons’ is generated by (Chen et al., 2019)) and the subsequent columns are the results of thehallucinated details and their renders by our method under different expressions. The first image row is theoutput rendering and the second is the shading of the detailed geometry. As one can see DetH is able to generaterealistic details depending on the expression being manifested and R is able to faithfully render them to theimage space. We zoom-in on a subset of details in the final column for greater clarity. (Please view in highresolution)3.3	Rendering NetworkThe rendering network, R, consists of two subnetworks: (1) The Neural Texture prediction networkNTex φ and (2) The image rendering network Tex2Im.
Figure 5: View Consistency. In this figure we demonstrate the consistency of the details rendered by R. Asubset of the hallucinated details from DetH are marked with blue rectangles. As can be seen, R renders thedetails in a consistent manner across views. (Please view in high resolution)4	ResultsWe train the detail hallucination network, DetH, and the rendering network R, on 9,000 imagesfrom the FFHQ dataset (Karras et al., 2019). Additionally, 3,000 frames were sampled from the MUG(Aifanti et al., 2010) and the ADFES (Van Der Schalk et al., 2011) datasets to speed-up trainingof the detail hallucination network DetH. Due to memory constraints, DetH and R are trainedindependently. Upon publication we will release the code. Below, we show the FaceDet3D’s resultson expression change, results on view consistency, a comparison with DECA (Feng et al., 2021) andAblation studies.
Figure 6: Qualitative Comparison with DECA(Feng et al., 2021) The row label shows the expres-sion being animated, the first column is the input im-age, the second is the detailed geometry hallucinatedby FaceDet3D for the expression being animated, the3rd column is the render generated by FaceDet3D of itshallucinated detailed geometry, the 4th column is the de-tailed geometry generated by DECA for the expressionbeing animated and the 5th column is render generatedby DECA. As can be seen by comparing columns 2 and4, the details generated by FaceDet3D are significantlymore consistent with the expression than those gener-ated by DECA, whose details are more generic (see textfor details). Further, a comparison between the 3rd andthe fifth column of the figure show that the renders ofFaceDet3D are significnatly more realistic than those ofDECA. (Please view in high resolution)the hallucinated details from DetH as the expression changes. The second row of images showsthe images rendered without details, this is done by setting the hallucinated detail map to zero. Asseen by comparing the skin appearance marked with the red rectangles, in the first row the skin
Figure 7: Details vs. No Details. We com-pare the results of changing the expressionboth with and without hallucinating details.
Figure 8: Ablating AugW and DSL. Weshow that without AugW and DSL detailsare not rendered to the image space. (Pleaseview in high resolution)PCA space cannot capture the rich details of the human face across a variety of input identities andexpression edits.
