Figure 1:  (Top) Distribution of algorithms’ normalized scores (Section 4.1) on all unconstrainedproblems split by objective class. Higher is better. NN+MILP matches or outperforms NN+RegEvoon  22/30  problems.   An  alternate  plot  where  scores  correspond  to  area  under  the  best-observedreward curve (AUC) can be found in Appendix H. (Bottom) Best observed reward as a functionof iteration for an example problem in each class, averaged over 20 trials (bands indicate    1sd).
Figure 2:  (a) Best observed reward as a function of iteration for typical constrained problem (Sec-tion 4.3), averaged over 20 trials (bands indicate    1sd).  Initial randomly sampled set of 50 pointsis omitted. Distribution of normalized final scores and more examples can be found in Appendix H.
Figure 3:  Best observed reward as a function of iteration for all TfBind (left) and BBOB (right)instances, comparing the use of binary and integer variables.  For TfBind, the categorical variablesare transformed to integer with an arbitrary ordering, and for BBOB, we use the given ordering ofthe problem. Note that the error region is large here since we aggregate all of the instances of eachclass.
Figure 4:  (Left) Best observed reward as a function of iteration for random instances based on theIsing model with the subset equality constraints described in Section 4.3. (Right) Distribution of thealgorithms’ normalized scores on the same constrained Ising model problems. Higher is better.
Figure 5:  Best observed reward as a function of iteration for random instances based on the Isingmodel with 200 (left) and 400 (right) binary variables and the subset equality constraints describedin Section 4.3, except that the number of pairs of subsets is k = 20 (left) and k = 40 (right).
Figure 6:  Best observed reward as a function of iteration for random instances based on the Isingmodel with 200 (left) and 400 (right) binary variables and the subset equality constraints describedin  Section  4.3,  except  that  the  number  of  pairs  of  subsets  is  restricted  to  k  =  10.   FC(16)  andFC(32) represent the runs where the surrogate neural network has a single layer of 16 and 32 ReLUsrespectively.
Figure 7:  Best observed reward as function of iteration for three graph partitioning instances fromMINLPLib (negated for maximization), with 60, 120, and 180 binary variables respectively.  Blacklines show the best known feasible solution to the problem (as of October 1, 2021).  Colored linesshow the average over 20 trials, while bands indicate    1 sd.  Note that bands that exceed the blackline are an artifact of the symmetric nature of standard deviation, and do not necessarily mean a trialfound an improved solution.
Figure 8: Distribution of algorithms’ normalized AUC scores (Section H.1) on unconstrained prob-lems split by objective function class. Higher is better. Relative performance of algorithms in termsof AUC is similar as best-observed reward (Figure 1).
Figure  9:  Distribution  of  algorithms’  normalized  scores  (Section  4.1)  on  constrained  problems.
Figure 10:  Distribution of MILP acquisition problem solve times as a function of iteration splitby objective class for unconstrained problems (Section 4.2) and for all constrained problems (Sec-tion 4.3).  Line and bands show the median and 5th/95th percentile range over all trials of all prob-lems in a class.
Figure 11:  Best observed reward as a function of iteration for the first half of all unconstrainedproblems (Section 4.2), averaged over 20 iterations (bands indicate    1sd). Dashed grey lines in thefirst  50 steps indicate the initial randomly sampled dataset, common to all methods except RBFOpt,which performs its own initialization.
Figure 12:  Best observed reward as a function of iteration for the second half of all unconstrainedproblems (Section 4.2), averaged over 20 iterations (bands indicate    1sd). Dashed grey lines in thefirst  50 steps indicate the initial randomly sampled dataset, common to all methods except RBFOpt,which performs its own initialization.
Figure 13: Best observed reward as a function of iteration for the first half of all constrained prob-lems (Section 4.3), averaged over 20 iterations (bands indicate    1sd). Initial randomly sampled setof 50 points is omitted.
Figure 14:  Best observed reward as a function of iteration for the second half of all constrainedproblems (Section 4.3), averaged over 20 iterations (bands indicate    1sd). Initial randomly sampledset     of 50 points is omitted.
