Figure 1: Illustration of norm and bias unbalance at the end of a CIFAR10 continual experiment. 5tasks with 2 classes each. (left) the norm of each output vector at the end of the last task, (middleleft) bias at the end of the last task, and (middle right) the difference between the last task’s bias andcurrent bias. We can see a clear unbalance in norm and bias for the last active vectors (classes 8 and9). The middle right figure shows us that the imbalance of bias is primarily due to the modification ofbias from the previous task (classes 6 and 7). (right) Illustration of Forgetting: we plot the differencebetween the first 50 weights after task 44 and after task 45 in the scenario Core10Mix. Task 45 iscomposed of one class different from 44’s one. Those two figures illustrate the impact of the masking.
Figure 2: Illustration of interference for the linear layer: We plot (left) the angles (in degree) betweenthe output vectors Ai , (middle) the mean angle between the latent vectors z of each class and theAi vectors and (right) ratio between the mean angle with wrong classes, and the mean angle of thetarget class: a high value indicates a higher risk of interference. In this experiment, there is a highrisk of interference between examples for class 4 and examples of class 1 or 3, as pictured by a darkred cell. See appendix F for a similar experiment for WeightNorm.
Figure 3: Experiments realized on 8 different task orders. We plot the test accuracy on the full testset for each epoch. We compare the different parameterizations of the linear layer. A vertical linerepresent a task transition. The notation _Masked denotes layer trained with single masking while_GMasked denotes layer trained with group maskingreduces the risk of interference as described in section 3. The only difference between these layers isthat in WeightNorm, the predictions are weighted by the norm of the latent vectors (cf Coslayer eq. 4,WeightNorm eq. 3), which scales the gradients thus may change the learning dynamics.
Figure 4: Comparison between layers trained by gradient descent and layers trained without gradients.
Figure 5: Test accuracy on task 0 in CIFAR100Lifelong Scenario.
Figure 8: Code to reproduce the scenarios used in the paper with continuum library.
