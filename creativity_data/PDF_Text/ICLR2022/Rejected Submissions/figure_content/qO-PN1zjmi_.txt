Figure 2: Left: Sketch of the SSND setting. Middle and Right: OOD detection with a diverse ensemble.
Figure 3: Varying degrees of ensemble disagreement and how that influences What regions are flagged OOD.
Figure 4: Regularization prevents individual models trained on S ∪ (U, C) from fitting (U—C, c).
Figure 5: Accuracy during fine-tuning a model pretrained on S (epoch 0 indicates values obtained with theinitial pretrained weights). The samples in (UOOD , c) are fit first, while the model reaches high accuracy on(UID , c) much later. We fine-tune for at least one epoch and then early stop when the validation accuracy startsdecreasing after 7 epochs (vertical line). The model is trained on SVHN[0:4] as ID and SVHN[5:9] as OOD.
Figure 6: Left: AUROC averaged over all scenarios in the medical OOD detection benchmark. The values forthe baselines are computed using the code from Cao et al. (2020). Right: The AUROC of ERD as the numberand proportion of ID (CIFAR10[0:4]) and OOD (CIFAR10[5:9]) samples in the unlabeled set are varied.
Figure 7: Cartoon illustration showing a diverse ensemble of linear binary classifiers. We compare OODdetection performance for two aggregation scores: (H ◦ Avg) (Left) and (Avg ◦ ρ) with ρ(f1 (x), f2 (x)) =asgn(f1(x))=sgn(f2 (X)) (Right). The two metrics achieve similar TPRs, but using (H ◦ AVg) instead of our score,(AVg◦ ρ), leads to more false positives, since the former simply flags as OOD a band around the averaged model(solid black line) and does not take advantage of the ensemble’s diversity.
Figure 8: Relying only on the randomness of SGD and of the weight initialization to diversifymodels is not enough, as it often yields similar classifiers. Each column shows a different predictortrained from random initializations with Adam. All models have the same 1-hidden layer MLParchitecture.
Figure 9: Distribution of disagreement scores on ID and OOD data for an ensemble that is notdiverse enough (Left), and an ensemble With regularized disagreement (Right). Note that MCD isearly-stopped using oracle OOD data. ID=CIFAR10[0:4], OOD=CIFAR10[5:9].
Figure 10: (a) Data samples for the MNIST/FashionMNIST splits. (b) Data samples for the CI-FAR10/SVHN splits.
Figure 11: Left: Samples from ImageNet and ObjectNet taken from the original paper by Barbuet al. (2019). Right: Data samples for the corrupted CIFAR10-C data set.
Figure 12: Samples from the medical image benchmark. There are 3 ID data sets containing frontaland lateral chest X-rays and retinal images. Hard OOD samples contain images of diseases that arenot present in the training set.
Figure 13: AUROC averaged over all scenarios in the medical OOD detection benchmark Cao et al.
Figure 14: AUROC averaged over the novel-class scenarios in the medical OOD detection bench-mark Cao et al. (2020), i.e. only use case 3.
Figure 15: Comparison between ERD and the various baselines on the NIH chest X-ray data set, foruse case 1 (top), use case 2 (middle) and use case 3 (bottom). Baselines ordered as in Figure 13.
Figure 16: Comparison between ERD and the various baselines on the PC chest X-ray data set, foruse case 1 (top), use case 2 (middle) and use case 3 (bottom). Baselines ordered as in Figure 13.
Figure 17: Comparison between ERD and the various baselines on the DRD fundus imaging dataset, for use case 1 (top), use case 2 (middle) and use case 3 (bottom). Baselines ordered as inFigure 13.
Figure 18: AUROCs obtained with an ensemble of WRN-28-10 models, as the initial learning rateand the batch size are varied. We used the hardest setting, CIFAR100:0-50 as ID, and CIFAR100:50-100 as OOD.
Figure 19: The AUROC of ERD as the number and proportion of ID (CIFAR10) and OOD (SVHN)samples in the unlabeled set are varied.
Figure 20:	Accuracy measured while fine-tuning a model pretrained on S (epoch 0 indicates values obtainedwith the initial pretrained weights). The samples in (UOOD , c) are fit first, while the model reaches highaccuracy on (UID , c) much later. We fine-tune for at least one epoch and then early stop when the validationaccuracy starts decreasing.
