Figure 1: (left) The pre-training phase: A transformer predicts actions conditioned on states and rewards thatare masked and is trained on the un-annotated large offline dataset. (right) The fine-tuning phase where asmall fraction of the data is reward-labeled, we unmask the reward tokens and predict the reward as well asactions during fine-tuning on the downstream task.
Figure 2: Aggregate results of the normalized scores across all domains and datasets. PDT achieves leading aggregatescores in the low-label regimes and is competetive with BC regularized CQL despite not learning a value funcction.
Figure 3:	Left: Ablation on effect of reward prediction on pointmass maze task, Right: Prediction ofreturns-to-go of different discrete states in the PointMaze environmentIn this experiment, we use our model to output the best possible return-to-go given different starting locationsin the maze. In Figure 3 (right) a darker square indicates a bin that represents a better return-to-go. Thismeans that if the point mass starts in that state, it will be able to quickly reach the goal square of (9, 1). Asexpected, states closer to (9, 1) have a better reward-to-go.
Figure 4:	Ablation on the effect of which PDT parameters to fine-tune. Numbers presented show the average normalizedscore over all medium and medium-expert mujoco experiments presented in our main results.
