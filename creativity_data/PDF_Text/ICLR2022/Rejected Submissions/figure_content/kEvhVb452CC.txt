Figure 1: Transformed ResNets strike a strong accuracy-robustness balance. Our models (red)significantly outperform the original ResNet-RS models (dark blue) they were initialized from whenevaluated on ImageNet-1k. They also exhibit solid performances on various robustness benchmarks(ImageNet-C, A and R, and FGSM adversarial attacks from left to right).
Figure 2: T-CNNs reach much higher performance and robustness at equal training time. Totaltraining time (original training + finetuning) is normalized by the total training time of the ResNet50-RS.
Figure 3: Robustness is most improved for strong and blurry corruption categories. We reportthe relative improvement between the top-1 accuracy of the T-ResNet50-RS and that of the ResNet50-RS on ImageNet-C, averaging over the different corruption categories (left) and corruption severities(right).
Figure 4: The later layers effectively escape the convolutional configuration. A: top-1 accuracythroughout the 50 epochs of fine-tuning of a T-ResNet270-RS. B: size of the receptive field of thevarious heads h (thin lines), calculated as αh-1 (see Eq. 3). Thick lines represent the average over theheads. C: depicts how much attention the various heads h (thin lines) pay to positional information,through the value of σ(λh) (see Eq. 7). Thick lines represent the average over the heads.
Figure 5: GPSA layers combine local and global attention in a complementary way. We depictedthe attention maps of the four GPSA layers of the T-ResNet270-RS, obtained by feeding the imageon the left through the convolutional backbone, then selecting a query pixel in the center of the image(red box). For each head h, we indicate the value of the gating parameter σ(λh) in red (see Eq. 7). Ineach layer, at least one of the heads learns to perform content-based attention (σ(λh) = 0).
Figure 6:	The larger the learning rate, the lower the test accuracy dips, but the faster it climbsback up. We show the dynamics of the ResNet50, fine-tuned for 50 epochs at resolution 224, forthree different values of the maximal learning rate.
Figure 7:	Performance at different test-time resolutions, for the finetuned models with andwithout SA. The ResNet50-RS and ResNet101-RS models are finetuned at resolution 224, and allother models are finetuned at resolution 320.
