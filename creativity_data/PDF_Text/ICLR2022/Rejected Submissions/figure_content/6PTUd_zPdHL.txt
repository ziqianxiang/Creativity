Figure 1: Overview of the proposed architecture: A CNN predicts scores for an image, which arethen ranked by a differentiable ranking algorithm returning the probability distribution for each rankin matrix P . The rows of this distribution correspond to ranks, and the columns correspond to therespective classes. In the example, we use a 50% top-1 and 50% top-2 loss, i.e., PK = [.5, .5, 0, 0, 0].
Figure 2: ImageNet-1K accuracy improvementsfor all ResNeXt-101 WSL model sizes (32x8d,32x16d, 32x32d, 32x48d). Blue is the originalmodel and orange is with top-k fine-tuning.
Figure 3: Effects of varying the ratio between top-1 and top-5 (left) and varying the size of dif-ferentially ranked subset m. Both experiments are done with the differentiable Sinkhorn rankingalgorithm (Cuturi et al., 2019). On the left, m = 16, on the right, Î± = 0.75. Averaged over 5 runs.
Figure 4:	Minimum ranks after a splitter cascade resulting from the transitive closure of the swaps.
Figure 5:	A (5, 16) selection network constructed with the method described in the text. The num-bers on the wires are the minimum ranks (starting at 0) that can be occupied by the values on thesewires. Red crosses mark where wires can be excluded, green check marks where a top rank is de-termined. Swaps in blocks of equal color belong to the same splitter cascade. Swaps in gray boxeswould be needed for full splitter cascades, but are not needed to determine the top 5 ranks.
