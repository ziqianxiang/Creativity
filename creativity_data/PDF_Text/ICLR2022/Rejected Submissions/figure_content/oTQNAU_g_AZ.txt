Figure 1: Five bimanual manipulation tasks. (a) Rearrange the blocks to goal positions. (b) Stack the blocksinto a tower. (c) Open the box and put the block inside. (d) Open the green door and push the block throughthe wall to the goal position. There are springs on both the box in (c) and the door in (d), which will closeautomatically without external force. Thus it requires one robot to hold the box cover and the door. (e) Lift upand rotate a bar with two arms to a target configuration, where the gripper is locked as closed, so it cannot bedone by one arm. More details about our environments are in Appendix A.
Figure 2): (i) the parameters of the state encoder for agent i itself fi,i(∙); (ii) the parameters of theother agents fi,j (∙), (1 ≤ j ≤ N, i = j) (shared); (iii) the parameters of all the interaction regionsfi,j(∙), (N + 1 ≤ j ≤ N + M) (shared). In this way, our model can be extended to environmentswith different number of interaction regions and agents. We represent the policy for agent i as,∏Φi (ai|s) = hi(fi,i(si)+LayerNorm(gi(vi))),	(5)4Under review as a conference paper at ICLR 2022where gi(∙) is one fully connected layer to further process the attention embedding vi, which encodesthe relationship between agent i and all the state entities (including all agents and interaction regions).
Figure 2: Our model framework. We use attention mecha-nism to combine all embedded representations from agentsand interaction regions. The output of attention module, to-gether with another embedded vector from si are summedtogether with L. The combined feature is fed into a 2-layerMLP hi to output ai . The intrinsic loss is computed from theattention probability αi and encourages the agents to attendto different sub-tasks.
Figure 3: Performances of different methods on two bimanual manipulation tasks, Open Box and Place (2on the left) and Push with Door (2 on the right). We consider two reward settings for each task, (i) a sparsereward (right in each group), where agents only receive a success reward when all the goals are reached; (ii)an informative reward (left in each group), where agent will additionally receive a reward for reaching eachindividual goal in addition to the final success reward.
Figure 4: Ablation studies on the value of λ. Our method is generally robust to the choice of λ, when even islarge (e.g., λ=0.2). In our practice, we choose λ=0.05 for all the experiments.
Figure 5: Visualization of attention αi .
Figure 6: Visualization of bimanual manipulation. For each object, we represents its goal as a transparent dotin the same color. (a) Both arms are picking up objects and alternatively stacking them into a tower; (b) Tostack two tower, each arm is working on one tower that is close to it; (c) We show the two arms can collaboratewithout conflict to pick up the 8 objects to their target locations.
Figure 8: The environments used in our experimentsA.1 Environment DescriptionsPush with Door, Figure 8(a). The two robots are placed on both sides of a 100cm × 70cm table,opposite each other (all robot manipulation environments are same for this setting). The goal is topush a block through a sliding door and make it reach the target position on the other side of the door.
