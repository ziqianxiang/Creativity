Figure 1: An illustrative figure for com-paring POMDPs considered in conventionalRL literature and those in our proposed set-ting. By performing active feature acquisi-tion, the partial observability is determinedby the feature acquisition policy besides theevolvement of the system.
Figure 2: Observation decoder and be-lief inference model for the partially ob-servable sequential VAE. Shaded nodesrepresent the observed variables. Theinference model filters information overthe partial observations and actions, topredict both the observed and unob-served features.
Figure 3: Performance curves on the bouncing ball+ domain: a: episodic number of observationsacquired by the Ï€f ; b: task rewards w/o cost. Our proposed method outperforms the non-sequentialbaselines in learning the task as well as acquiring less observations; c: Ablation study on bouncingball+ to illustrate the effect of learning the feature acquisition policy. Each method is run with 10random seeds. Our proposed approach outperforms the random baseline significantly in terms of taskperformance.
Figure 4: Seq-PO-VAE reconstruction for the online trajectories upon convergence (better to viewenlarged). Each block of three rows corresponds to the results for one trajectory. In each block, thethree rows (top-down) correspond to: (1) the partially observable input selected by acquisition policy;(2) the ground-truth full observation; (3) reconstruction from Seq-PO-VAE. The green boxes remarkthe frames where ball is not observed but our model could impute its location. Key takeaways: (1) ourlearned acquisition policy captures model dynamics ; (2) Seq-PO-VAE effectively impute the missingfeatures (i.e., ball can be reconstructed even when they are unobserved from consequent frames).
Figure 5: Performance curves for the compared approaches on Sepsis. The curves are derived undercost value of 0.01. Overall, our method converges to treatment policy with substantially better rewardcompared to the baselines.
Figure 6:	The task reward and episodic no. fea-ture acquisitions derived upon convergence forour method, when training our model under dif-ferent cost values. We also present the results for abaseline that trains the task policy under a randomfeature acquisition strategy. The actively learnedfeature acquisition leads to significant advantagecompared to random acquisition.
Figure 7:	This figure shows the total number offeatures acquired (y-value) to obtain certain re-ward standard (x-value), when training the taskpolicy using different representation learning ap-proaches. The presented methods adopt a costc = 0.01. Our method results in the best achiev-able reward (max x-value) with lowest slope.
Figure 8: Imputation results for different VAE models. We select 9 trajectories obtained from thetrained End-to-End policy. Each block corresponds to the results for one trajectory (better to viewenlarged). The five rows in one block are (top-down): (1) partial observations acquired by the agent;(2) ground-truth image with full observation; (3) Imputation by NoSeq-ZI (partial); (4) Imputation byNoSeq-ZI (full); (5) Imputation by Seq-PO-VAE (ours). Our model can often successfully predict theballs location even if it is not present in the acquired observation. Hence it successfully employs itslearned knowledge of the dynamics. In contrast, the non-sequential model (obviously) fails to predictthe balls location when the ball is not present in the observation.
Figure 9: Cost-performance trade-off investigation. Each row corresponds to the performance interms of task reward and episodic number of acquisitions obtained for a specific method (see legends).
Figure 10:	Comparison result between our proposed method and the non-sequential VAE baselinemodels under different values for cost. Each curve is derived from 3 independent runs.
Figure 11:	Average number of observations acquired in each episode when training our proposedmodel under different cost values.
Figure 12: Two example trajectories for illustrating how our method works on the Sepsis medicaldomain. The acquisition policy is trained with a cost of 0. Each block corresponds to one trajectoryand the four rows correspond to the four measurement features being considered for active featureacquisition. Each dot indicates the employment of feature acquisition on the corresponding measure-ment feature at the presented time point. In each trajectory, we demonstrate the ground-truth signalover time as well as the imputed signal over time predicted by our proposed Seq-PO-VAE model.
