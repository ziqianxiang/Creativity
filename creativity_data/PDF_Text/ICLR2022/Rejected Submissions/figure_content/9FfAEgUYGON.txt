Figure 1: Mismatched No More is a model-based RL algorithm that learns a policy, dynamics model, andclassifier. The classifier distinguishes real transitions from model transitions. The policy and dynamics modelare jointly optimized to sample transitions that yield high return and look realistic, as estimated by the classifier.
Figure 2: Two Didactic Experiments. (Left) We apply MnM to a navigation task with transition noise thatmoves the agent to neighboring states with equal probability. MnM solves this task more quickly than Q-learningand VMBPO. The dynamics learned by MnM are different from the real dynamics, changing the transition noise(blue arrows) to point towards the goal. (Right) We simulate function approximation by a learning model that isforced to make the same predictions for groups of 3 Ã— 3 states, resulting in a model that is inaccurate aroundobstacles. The classifier term compensates for this function approximation error by penalizing the policy fornavigating near obstacles.
Figure 3: Lower Bounds and Risk Seeking. (Left) On a simple 3-state MDP with stochastic transition in onestate (red arrows), MnM converges to the reward-maximizing policy while VMBPO learns a strategy with lowerrewards and higher variance (as predicted by theory). (Right) We apply value iteration to the gridworld fromFig. 2a to analytically compute various objectives. As predicted by our theory, the MnM objective is a lowerbound on the expected return, whereas the VMBPO objective overestimates the expected return.
Figure 5: Comparison on Robotics Tasks: We compare MnM to MBPO and SAC on simulated control tasks.
Figure 4: Environments: Our experi-ments included tasks from four benchmarks:(clockwise from top-left) OpenAI Gym,DM Control, Metaworld, and ROBEL.
Figure 7: Optimistic Dynamics: (Left) On the Pusher-v2 task, the MnM dynamics model makes the puckmove towards the puck move towards the gripper before being grasped. (Right) On the HalfCheetah-v2task, the MnM dynamics model helps the agent stay upright after tripping.
Figure 6: Model exploitation: The very large Q valuesof MBPO suggest model exploitation, which our methodappears to avoid.
Figure 8: Alternative Model Learning Objectives: Using the DClawScrewFixed-v0 task, we compareMnM and MBPO (Janner et al., 2019) to two additional model learning objectives suggested in the literature,VAML (Farahmand et al., 2017) and value-weighted maximum likelihood (Lambert et al., 2020). MnM (ourmethod) outperforms these alternative approaches.
Figure 9:	Ablations Experiments: Compared with MBPO (orange line), MnM uses a GAN-like model (redline) with a model optimism term and modifies the reward function.
Figure 10:	MnM trains stably. Despite resembling a GAN, the MnM dynamics model trains stably, with thevalidation MSE decreasing steadily throughout training. Different colors correspond to different random seeds ofMnM. The dashed line corresponds to the minimum validation MSE of a maximum likelihood dynamics model.
Figure 11: Environments: Our experiments look at three locomotion tasks from OpenAI Gym (Brockmanet al., 2016), the inverted pendulum task from DM Control (Tassa et al., 2018), four manipulation tasks fromMetaworld (Yu et al., 2020a), and four dextrous manipulation tasks from Robel (Ahn et al., 2020). The Robeltasks use the same dynamics but different reward functions, so we only include an image of one task.
