Figure 1: Training loss and test accuracy of TranSformer-MGK/MLK vs. softmax/linear transformer on theretrieval task, which has the longest average sequence-length and attention span among the LRA tasks (Tay et al.,2021). The impressive performance of Transformer-MGK/MLK on this challenging task validates the capabilityof our models to capture long-range dependencies via learning a diversity of attention patterns.
Figure 2: (Left) Visualization of attention matrices in the baseline 4-head softmax transformer (left), 4-headTransformer-MGK (middle), and 2-head Transformer-MGK (right) trained on the document retrieval task.
Figure 3: Computational cost (FLOPs) and the number of parameters of Transformer-MGK vs the baselinesoftmax transformer (Left) and Transformer-MLK vs. the baseline linear transformer (Right). The efficiencyadvantage of Transformer-MGK/MLK over the baselines in both metrics grows with the number of head.
Figure 4: Validation perplexity of Transformer-MGK vs. the softmax transformer (Left) and Transformer-MLKvs. the linear transformer (Right) for language modeling on WikiTeXt-103.
Figure 5: Visualization of attention matrices from 8-head softmax transformer (Left), 8-head Transformer-MGK(Middle), and 4-head Transformer-MGK (Right) trained on WikiText-103 language modeling. Here, we plot theattention matrices for all heads and layers in the models. The sequence length is 256 and the size of each matrixis 256×256.
Figure 6: Rank distributions of attention matrices from 8-head Softmax transformer (Left), 8-head Transformer-MGK (Middle), and 4-head Transformer-MGK (Right) trained on WikiText-103 language modeling. The rankhistograms are computed from 1000 attention matrices at each layer. The attention matrices from Transformer-MGKs have higher ranks than those from softmax transformers. This implies that Transformer-MGKs havemore diverse attention patterns, which allows us to reduce the number of heads in Transformer-MGKs.
Figure 7: Model complexity (Top) and computational cost (Bottom) of different inference and learning methodsfor Transformer-MGK trained on the document retrieval task. While computational costs are almost the same,Transformer-sMGK has more advantage in model size, comparing to Transformer-MGK, Transformer-MGKHard-E, and Soft-E. The naming is as explained in Section 3.4 in the main text.
Figure 8: Visualization of attention matrices in the 4-head linear transformer baseline (Left), 4-head Transformer-MLK (Middle), and 2-head Transformer-MLK (Right) trained on the document retrieval task. Here, the sequencelength is 4000, and the size of each matrix is 4000×4000.
Figure 9: Training and test loss/aCCUraCy of TranSformer-MGK vs. SoftmaX transformer (Left) and Transformer-MLK vs. linear Transformer (Right) on the retrieval task, which has the longest average sequence-length andattention span among the LRA tasks (Tay et al., 2021). In training, We apply early stopping to avoid overfitting.
Figure 10: Validation perpleXity of the Transformer-MGK vs. the softmaX transformer (Left) and theTransformer-MLK vs. the linear transformer (Right) for language modeling on WikiTeXt-103. Training convergeson this task after 500000 iterations, equivalent to 115 epochs .
Figure 11: Weight matrices WK for computing the keys, for all heads and layers, in the 2-head SoftmaXtransformer baseline (Left), the 1-head Transformer-MGK with 2 keys (Middle), and the 1-head Transformer-sMGK with 2 keys (Right) trained on the LRA retrieval task. Here, the dimension of each head D = 32 and thatof input xi is Dx = 64. Hence, each weight matriX has the shape of (64, 32).
Figure 12: Key embeddings K for all heads and layers of the 2-head SoftmaX transformer baseline (Left), the1-head Transformer-MGK with 2 keys (Middle), and the 1-head TranSfOrmer-SMGK with 2 keys (Right) trainedon the LRA retrieval task. Here the dimension D of each head is 32, and we plot the key embeddings of the first100 tokens in a randomly chosen sequence. Hence, each key matrix has the shape of (100, 32).
Figure 13: Computational cost (FLOPs) for each training iteration of Transformer-MGK vs. the baselinesoftmax transformer (Left) and Transformer-MLK vs. the baseline linear transformer (Right) on the LRAretrieval task. The efficiency advantage of Transformer-MGK/MLK over the baselines grows with the number ofheads. Here, the batch size of each training iteration is 32.
Figure 14: Computational cost (measured in FLOPs) per training iteration of different inference and learningmethods for Transformer-MGK trained on the LRA retrieval task. Computational costs are almost the same forTransformer-sMGK, Transformer-MGK, Transformer-MGK Hard-E, and Soft-E. The naming is as explained inSection 3.4 in the main text.
