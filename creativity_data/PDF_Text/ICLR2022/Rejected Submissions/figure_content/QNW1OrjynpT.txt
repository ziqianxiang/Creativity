Figure 1: Assessing memory retrieval in neural language models. In language, memory of pastcontext is required for ongoing comprehension. How are memory systems of neural LMs organized,via learning, for processing? In our paradigm, language models processed English text in which alist of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the firstpresentation of the list to its second presentation. The two lists of nouns were embedded in a vignettewhich consisted of a context string, first list, an intervening text, and the second noun list. To probethe properties of neural LM memory, we measured LM surprisal on the second list while varying: a)set size, b) the structure of the second list, c) the length of the intervening text, and d) the content andstructure of the intervening text.
Figure 2: Median surprisal (over N list = 230) broken down per token position in second lists ofarbitrary nouns and semantically coherent nouns. Negative values on x-axis represent 4 tokens ofprompt string that introduced the second list: “(she) read the list again”. The 0-index marks the firstnoun in the list. Line style and hue denote manipulation of the second list relative to the first list.
Figure 3: Verbatim token retrieval for varying number of tokens being retrieved (left) and the lengthof the intervening text (right). Reported is proportion of list-median surprisal on second relative tofirst list of nouns. Points show group median (over N list = 230). Error bars denote 95% confidenceinterval around the median (bootstrap estimate). For set size manipulation, intervening text is fixed at26 tokens. For intervening text manipulation, set size is fixed at 10 tokens.
Figure 4: LM memory retrieval for different intervening texts. We plot relative list-median surprisalover all non-initial tokens in lists. Points show group median (over N list = 230). Error bars denote95% confidence interval around the median (bootstrap estimate). Note that in the top-row plots y-axisstarts at 60%.
Figure 5:	Repeat surprisal for randomly initialized transformer LM and a transformer with permutedattention weights. Reported is relative list-median surprisal over all non-initial tokens in lists only.
Figure 6:	LSTM verbatim token retrieval for varying number of tokens being retrieved at short(3-token) intervening text. Reported is proportion of list-median surprisal on second relative to firstlist of nouns (repeat surprisal). Points show group median (over N list = 230). Error bars denote 95%confidence interval around the median (bootstrap estimate).
Figure 7: LM memory retrieval for models of different depths. Reported is relative list-mediansurprisal over all non-initial tokens in lists. Points show group median (over N list = 230). Errorbars denote 95% confidence interval around the median (bootstrap estimate). Note that in these plotsy-axis starts at 70%.
Figure 8:	Wikitext-103 transformer memory retrieval results for control vignettes. Top left: in thisvignette the subject ‘Mary’ is replaced with ‘John’. Top right: in this vignette the colon introducingthe list is replaced with a comma. Bottom: the preface string is randomly permuted. Reported isrelative list-median surprisal over all non-initial tokens in lists. Points show group median (over N list= 230). Error bars denote 95% confidence interval around the median (bootstrap estimate). Note thatin these plots y-axis starts at 70%.
Figure 9:	Results for paradigms with additional control vignettes: A) the subject ‘Mary’ is replacedwith ‘John’, B) the colon token introducing the list is replaced with a comma, C) the tokens in thepreface string are randomly permuted and, D) the tokens in the prompt string are randomly permuted.
Figure 10:	Verbatim token retrieval for varying number of tokens being retrieved (left) and the lengthof the intervening text (right). Reported is proportion of list-median surprisal on second relative tofirst list of nouns. Points show group mean (over N list = 230). Error bars denote 95% confidenceinterval around the mean (bootstrap estimate). For set size manipulation, intervening text is fixed at26 tokens. For intervening text manipulation, set size is fixed at 10 tokens.
