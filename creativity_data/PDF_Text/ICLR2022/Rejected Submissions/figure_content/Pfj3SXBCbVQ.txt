Figure 1: The frequency of the first 100 tokens of a large subword vocabulary (32k tokens) and asmall subword vocabulary (350 tokens). The first distribution has a much longer tail than the seconddistribution, which translates to a lower average relative frequency per token (see first Figure).
Figure 2: As we limit the training data (left to right), we see that quasi character-level Transformersperforms better than their large subword vocabularies versions. Similarly, this phenomenon seemsto occur regardless of the language (top to bottom).
Figure 3: The advantages of quasi character-level Transformers in data-poor environments are con-sistent when models are trained on different languages and domains. From left to right we have theresults from models trained on the CommonCrawl, SciELO, and NewCommentary datasets, whichbelong to different domains. In each of those figures, we see that when the training data is lim-ited (two columns on the left of each figure), quasi character-level models outperform their largesubword-level counterparts, regardless of the domain.
Figure 4: The green lines refer to the best and worst runs of the models with large subword-levelvocabularies, while the blue lines refer to the best run of the quasi character-level models. In the leftfigure, we see that quasi character-level Transformers consistently outperform the ones with largesubword vocabularies. Then, in the middle figure we see that for the LSTMs, this phenomenon is notas strong as with Transformers. Finally, in the right figure, we can see how quasi character-CNNsdo not present these advantages as clearly as in the other models.
Figure 5: Vocabularies seem to have a strong impact on the catastrophic forgetting effects. Whilethe quasi character-level model lost 12.3pts (see Figure 5a), the large subword-level model only lost0.7pts (see Figure 5b)Vocabulary size: ~32k tokens40Health	Hâ†’B(Voc. domain=H)	(Voc. domain=H)(b) Large subword-level vocabulary8Under review as a conference paper at ICLR 2022From Figure 5, we can infer that vocabularies seem to have a strong impact on the effects of catas-trophic forgetting because character-level vocabularies seem to make models more susceptible to thecatastrophic forgetting problem than large subword-level vocabularies.
Figure 6: Transformers with quasi character-level vocabularies (left figures) appear to be more con-sistent between domains than Transformers with large subword-level vocabularies (right figures)Even though quasi character-level Transformers achieve better consistency across domains, theysuffer more severely the catastrophic forgetting problem than Transformers with large subword vo-cabularies. We believe that by using specially designed regularization techniques to address thisissue such as LwF (Li & Hoiem (2016)) or EWC (Kirkpatrick et al. (2016)) these problems couldbe mitigated, leading to more robust and consistent models.
