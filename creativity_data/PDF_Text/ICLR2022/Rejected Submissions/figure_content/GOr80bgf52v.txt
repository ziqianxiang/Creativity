Figure 1: PyBullet environment with a UR5 arm.
Figure 3: Breakdown of tasks in Cubes (top row) and Shapes (bottom row) environments. In Cubes,we train on the task of making a stack of four blocks and test on two wall stacking and two stairstacking tasks. In shapes, we instantiate 16 tasks with structures of height of three and a roof andtop, and split them 50/50 training/testing. We provide expert demonstrations for training tasks andtest zero-shot generalization in prediction on testing tasks.
Figure 4: Schema of our pipeline for creating cropped object images. We concatenate an RGB imagewith horizontal and vertical coordinate grids. Using a bounding box with added padding, we cropan 18×18 image from both the RGB component and the coordinate grid components. By observingthe coordinate grids, our agent known where in the image the object was cropped. Note that thecoordinate grids are derived from object bounding boxes, not the actual (x,y,z) object positionsin the environment. Hence, we do not need to know the ground-truth object positions in order togenerate our factored states. We add two additional coordinate grids by mirroring the vertical andhorizontal grids (similar to positional encodings in Locatello et al. (2020)).
Figure 5: Action sequence ranking Hits@1 conditioned on action noise . The noise level controlsthe degree to which negative action sequences differ from the position one (Section 3.1). Left:results for training tasks; right: results for zero-shot generalization. We compare Factored WorldModel against baselines from Table 1. Means and 95% confidence intervals over 5 random seedsare reported.
Figure 6: Block position prediction error as a function of the number of predicted time steps (Left:Cubes, right: Shapes). The validation set used in this figure contains noisy trajectories for trainingtasks, whereas the errors reported in Table 1 are for optimal trajectories that reach the goal of eachtask. We compare Factored World Model against baselines from Table 1. Means and 95% confidenceintervals over 5 random seeds are reported.
Figure 7: Visualizing action attention weights for a sequence of building a stack of four blocks and awall. Each bar in the histogram is associated with a particular object by color. (a) the agent executesa pick action at t = 1, t = 3, t = 5 and a place action at t = 2, t = 4, t = 6. (b) pick action att = 1, t = 3, t = 5, t = 7, t = 9 and place action at t = 2, t = 4, t = 6, t = 8, t = 10.
Figure 8: Left: our real-world experimental setup includes a UR5 arm with a Robotiq gripper, twoRGB cameras facing the workspace (Camera 1 and 2) and one RGB camera to take an image of anobject the robot is holding (Camera 3). The gripper moves inside of the box in the bottom-left cornerafter every successful pick action. Top-right: an in-hand image taken by Camera 3. Bottom-right:the four images represent a single factored state of the environment with a tower of four blocks. Eachimage is centered on one block starting from the top block going to the bottom. State factorizationis explained in Figure 4.
Figure 9: Visualization of feature maps learned by C-SWM with heuristic action factorization. Thefirst two columns show the two views of the environment provided to the model. The next sixcolumns show the 18×18 feature maps for each object slot given by the C-SWM object extractor.
