Figure 1: CLAFusion strategy: In the first step, the cross-layer alignment problem is solved for two pre-trained models. Two corresponding layers are surrounded by two rounded rectangles of the same color. Based onthe optimal mappings obtained in the previous step, CLAFusion balances the number of layers (adding layers inthis figure). The red circles and lines indicate the newly added neurons and weights (Zero weights are omitted).
Figure 2: Cross-layer alignment example: Two pre-trained neural networks are given in (a). Model Ahas 4 hidden layers of size 5, 4, 3, 1 while model B has 3 hidden layers of size 3, 2, 1. (b) shows the costmatrix (squared Euclidean distance) between layer representations (number of neurons) for hidden layers of twonetworks. Three color cells represent the solution of the CLA problem. Note that the upper-left and lower-rightcells are automatically chosen by the constraints on the first and last hidden layers. (c) visualizes the optimalmapping. Two rounded rectangles of the same colors represent two corresponding layers in the optimal CLA.
Figure 3: Layer balancing examples: In (a), a new layer is added between hidden layers 1 and 2 of model B.
Figure 4: The performance (global test accuracy on the whole MNIST dataset) of the fused model when fusingthe specialist model A and the generalist model B in various weight proportions of model B. All results areaveraged across 5 seeds. Detailed results are given in Table 8.
Figure 5: The average performance of skill transfer using (a) adding layers and (b) merging layers as the layerbalancing method.
