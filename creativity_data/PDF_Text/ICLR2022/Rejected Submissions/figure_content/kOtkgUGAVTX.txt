Figure 1: This work deals with unsuper-vised skill discovery through mutual infor-mation maximization. We introduce Con-trastive Intrinsic Control (CIC) — a new un-supervised RL algorithm that explores andadapts more efficiently than prior methods.
Figure 2: Qualitative visualizations of unsupervised skills discovered in Walker, Quadruped, and Jaco armenvironments. The Walker learns to balance and move, the Quadruped learns to flip upright and walk, and the 6DOF robotic arm learns how to move without locking. Unlike prior competence-based methods for continuouscontrol which evaluate on OpenAI Gym (e.g. Eysenbach et al. (2019)), which reset the environment when theagent loses balance, CIC is able to learn skills in fixed episode length environments which are much harder toexplore (see Appendix K).
Figure 3: Architecture illustrating the practical implementation of CIC . During a gradient update step, randomτ = (s, s0 ) tuples are sampled from the replay buffer, then a particle estimator is used to compute the entropyand a noise contrastive loss to compute the conditional entropy. The contrastive loss is backpropagated throughthe entire architecture. The entropy and contrastive terms are then scaled and added to form the intrinsic reward.
Figure 4: We report the aggregate statistics using stratified bootstrap intervals (Agarwal et al., 2021) for 12downstream tasks on URLB with 10 seeds, so each statistic for each algorithm has 120 seeds in total. We findthat overall, CIC achieves leading performance on URLB in terms of the IQM, mean, and OG statistics. Asrecommended by Agarwal et al. (2021), we use the IQM as our primary performance measure. In terms ofIQM, CIC improves upon the next best skill discovery algorithm (APS) by 91% and the next best algorithmoverall (ProtoRL) by 26%.
Figure 5: We visualize the contributions to the CIC intrinsic reward from the entropy and disciminator termsacross the three URLB domains. Since H(τ) and H(τ |z) terms are on different scales we set the hyperpa-rameter α = 0.9 to weight the two terms equally for the Walker and Quadruped tasks. For Jaco, we find thatdiscriminator-only CIC (α = 0.0) is sufficient because random exploration results in meaningful behaviorssince the arm is fixed to the table and therefore can’t fall. While the CPC intrinsic reward increases throughouttraining, the entropy reward decreases and settles at a non-zero value. Without explicit entropy maximization,the entropy of CIC approaches zero. Compared to APT, CIC achieves smaller entropy as expected since thediscriminator counteracts the entropy term. Compared to DIAYN, CIC achieves substantially higher entropy.
Figure 6: Design choices for pre-training and adapting with skills have significant impact on performance.
Figure 7: Learning curves for finetuning pre-trained agents for 100k steps. Task performance is aggregated foreach domain, such that each curve represents the mean normalized scores over 4 × 10 = 40 seeds. The shadedregions represent the standard error. CIC surpasses the performance of the prior state-of-the-art on Walker andJaco tasks while tying on Quadruped. CIC is the only algorithm that performs consistently well across all threedomains.
Figure 8: A gridworld example motivating the need for large skill spaces. In this environment, we place anagent in a 10 × 10 gridworld and provide the agent access to four discrete skills. We show that the mutualinformation objective can be maximized by mapping these four skills to the nearest neighboring states resultingin low behavioral diversity and exploring only four of the hundred available states.
Figure 9: Learning curves for finetuning pre-trained agents for 100k steps. Task performance is aggregated foreach domain, such that each curve represents the mean normalized scores over 4 X 10 = 40 seeds. The shadedregions represent the standard error. CIC surpasses the performance of the prior state-of-the-art on Walker andJaco tasks while tying on Quadruped. CIC is the only algorithm that performs consistently well across all threedomains.
Figure 10: Qualitative visualization of DIAYN and CIC pre-training on the Walker and Quadruped domainsfrom URLB. Confirming findings in prior work Zahavy et al. (2021), we also find that DIAYN policies producestatic but non-trivial behaviors mapping to “yoga” poses while CIC produces diverse and dynamic behaviorssuch as walking, flipping, and standing. Though it’s hard to see from these images, all the DIAYN skills getstuck in frozen poses while the CIC skills are producing dynamic behavior with constant motion.
Figure 11: To empirically demonstrate issues inherent to competence-based exploration methods, We runDIAYN (Eysenbach et al., 2019) and compare it to ICM (Pathak et al., 2017) and a Fixed baseline wherethe agent receives an intrinsic reWard of 1.0 for each timestep and no extrinsic reWard on both OpenAI Gym(episode resets when agent loses balance) and DeepMind Control (DMC) (episode is fixed for 1k steps) Hopperenvironments. Since Gym and DMC reWards are on different scales, We normalize reWards based on themaximum reWard achieved by any algorithm ( 1k for Gym, 3 for DMC). While DIAYN is able to achievehigher extrinsic reWards than ICM on Gym, the Fixed intrinsic reWard baseline performs best. HoWever, onDMC the Fixed and DIAYN agents achieve near-zero reWard While ICM does not. This is consistent Withfindings of prior Work that DIAYN is able to learn diverse behaviors in Gym (Eysenbach et al., 2019) as Wellas the observation that DIAYN performs poorly on DMC environments (Laskin et al., 2021)22Under review as a conference paper at ICLR 2022L Pseudocode for the Contrastive Discriminator in CICCIC consists of two terms I(T; Z) = H(T) - H(T|z) ≥ H(T) + E[log q(τ|z)] the entropy H(T) isestimated with a particle estimator Singh et al. (2003); Liu & Abbeel (2021a) while the discrimina-tor q(T |z) is estimated with a contrastive loss introduced in this work. Note that contrastive learningin CIC is different than prior vision-based contrastive learning such as CURL Laskin et al. (2020b),since we are not performing contrastive learning over augmented images but rather over state tran-sitions and skills. The contrastive objective in CIC is used for unsupervised learning of behaviorswhile in CURL it is used for unsupervised learning of visual features.
