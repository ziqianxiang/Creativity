Figure 1: l2 norm of the logit values (a), l2 norm of the last layer weights (b), histogram of per-sample gradient magnitude in the first epoch (c), and divergence histogram of optimisation path inaggregated clipped gradients from the optimisation path in unclipped ones (d) when we train theclassifier on CIFAR10 using cross-entropy loss or our proposed loss function. Minimising crossentropy loss using DP-SGD leads to logit exploding (a) and weight exploding (b). However, ourproposed loss function prevents logit exploding (a) and weight exploding (b). Therefore, per-samplegradient magnitudes (c) obtained based on our proposed loss function is smaller than per-samplegradient magnitudes obtained based on cross entropy loss. In addition to this, the per-sample gradi-ents of our proposed loss function is much more condensed. Finally, direction of the gradients (d)of our proposed loss function is more aligned with the unclipped gradients.
Figure 2: Analysing the smoothness ofthe landscape of our proposed loss func-tion and cross-entropy on MNIST. Ourloss function obtains smaller σmax (themaximum of the first singular value),making it more smooth than cross en-tropy.
Figure 3:	Deployment of our loss function in DP-SGD training of an end-to-end classifier for 5 runs.
Figure 4:	Deployment of our loss function in DP-SGD training of a public feature extractor followedby a classifier for 5 runs. Comparison between the test accuracy of CE-T-PFE-CNN (Tramer &Boneh, 2021) ( ) and O-T-PFEjCNN, ours ( ), as a function of privacy budget ε.
Figure 5:	l2 norm of weights (first row) and pre-activations (second row) in CE-T-CNN (-)and O-T-CNN (—) using CIFAR10 (C) and FaShiOnMNIST (F). See Appendix C for results ofMNIST and other layers.
Figure 6: Histogram ofl2 norm of per-example gradient in CE-T-CNN and O-T-CNN on CIFAR10.
Figure 7:	Cosine similarity between aggregated of clipped per-example gradients and aggregated ofunclipped per-example gradients in first epoch of CE-T-CNN (-) and O-T-CNN (-----).
Figure 8:	Confusion matrices that show per-class training accuracy of O-T-CNN (first column),CE-T-CNN (second columns) and non-private SGD approach (last columns). The first and secondrow show confusion matrices of CIFAR10 at epoch 2 and 20, respectively. See Appendix D for theresults of other epochs as well as FashionMNIST and MNIST datasets.
Figure 9:	Ablation study of DP-SGD training of an end-to-end classifier (first row) and a publicfeature extractor followed by a classifier (second row) for 5 runs. Loss functions under study: CE,SSE, SSE+CE, SSE+Focal and SSE+Focal+Regulariser.
Figure 10: Test accuracy of O-T-CNN as a function of hyperparameter values of our loss function.
Figure 11:	l2 norm of model weights in CE-T-CNN (-) and O-T-CNN (-) using CIFAR10,FashionMNIST and MNIST.
Figure 12:	Pre-activation l2 norm per layer of CE-T-CNN (-) and O-T-CNN (--) on CIFAR10,FashionMNIST and MNIST.
Figure 13:	Confusion matrices that show per-class training accuracy of O-T-CNN (first column),CE-T-CNN (second columns) and non-private SGD approach (last columns) using CIFAR10, Fash-ionMNIST and MNIST.
Figure 14:	Histogram ofl2 norm of per-example gradient in CE-T-CNN and O-T-CNN on CIFAR10(top row), FashionMNIST (middle row) and MNIST (bottom row).
Figure 15:-DP-SGD training of an end-to-end classifier using SSE , SAE and HUber loss(β = .1 , β = .2 , β = .3 , β = .4 , β = .5 , β = .6 , β = .7 ,β = .8	, β = .9----and β =1-------).
