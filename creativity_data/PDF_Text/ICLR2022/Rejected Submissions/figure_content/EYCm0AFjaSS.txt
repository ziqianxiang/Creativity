Figure 1: ZerO initialization for ResNet. We add extra skip connections and modify their locationsin the standard ResNet. We also apply Hadamard transforms (defined at Definition 1) when there isan expansion in the channel dimension. We omit Relu and batch normalization here, see Figure 4for a detailed design. A partial-identity 1x1 convolutional kernel is defined at Equation 1.
Figure 2: Left: the forward dynamic of the zero-initialized example network at initialization. Solidand dashed blue lines represent non-zero signals. Right: we represent the signals in the first layer atinitialization on a standard 2-dimensional basis. Dashed lines represent the signals before the Reluand solid lines represent the signals after the Relu, which are activations x1.
Figure 3: Left: final weight distributions. Right: weight correlations during training. Each settingachieves 98% test accuracy on MNIST dataset except for ZerO Init (no transform).
Figure 4: Left: associated operations for each convolutional layer. Right: an illustration of a1x1x3x5 partial-identity convolutional kernel.
Figure 5: Training error over the first 5epochs on ImageNet.
Figure 6: Test accuracy of the pruned networks trained with various initialization methods.
Figure 7: Measuring gradient norms of ResNet-18 for the first 400 iterations.
