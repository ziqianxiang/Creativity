Figure 1: (a) We introduce Worst-off DRO, an invariant learning algorithm for partial group labeled data (asshown in (iii)). This is in contrast with other settings, such as (unsupervised) DRO [14] where (i) no group labelsare available at train time, or Group DRO [30] when (ii) group labels for all training examples are required. (b)Worst-off DRO finds the worst-case group assignment for missing data with a marginal distribution constraint,which may be given as side information, or estimated from labeled counterparts, and optimizes similarly to thatof Group DRO. The constraint set defined by the marginal distribution includes the ground-truth group labelswith high probability, ensuring the training objective of Worst-off DRO to be an upper bound of that of GroupDRO with ground-truth group labels of entire data.
Figure 2: Increasing the labelled samples. Minority group accuracies are plotted at different countsof the labelled samples in the training dataset. Both, Group DRO (Partial) and Worst-off DROalgorithms improve the minority group accuracies with more training labels. Also, the Worst-offDRO method has higher accuracy values than Group DRO method. The aggregate group accuraciesare shown in the Appendix Figure 5.
Figure 3:	Minority Group vs. Average Accuracy. Evaluations for different hyper-parameterchoices are plotted for Worst-off DRO and Group DRO (Partial) methods. Models from Worst-off DRO training are concentrated in the top-right corner of the plots. This is desirable indicating ahigh accuracies across the two metrics. For model selection from among the possible choices, weadopt the NVP procedure (see Section 4).
Figure 4:	Progression of group weights. The evolution of q-values (see Algorithm 1) is plotted foreach group. The q-values for the minority groups increases gradually while those of the majoritygroups reduce. A high q-value indicates that the corresponding group receives a higher weightrelative to other groups. In the plots, minority group is indicate by a * on q.
Figure 5: Increasing the labelled samples - Average Group Accuracy. We plot the average groupaccuracies as a function of labelled samples. These accuracies remain fairly similar as the count oflabelled samples grows.
Figure 6: Varying the parameter in the constraints. The marginal constraint is gradually relaxedby increasing the parameter. The accuracies in the plots are computed on the test sets. Performanceof the models with â‰¤ 0.01 are similar, however, the accuracies drop when increasing beyond 0.01threshold.
