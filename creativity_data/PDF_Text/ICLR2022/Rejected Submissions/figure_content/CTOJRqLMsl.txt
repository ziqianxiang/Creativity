Figure 1: Geometric illustration of Non-Convex Continual Learning (NCCL). In the continuallearning setting, the model parameter starts from the moderate local optimal point for the previouslylearned tasks XP. Over T iterations, we expect to reach a new optimal point XPuc which has agood performance on both previously learned and current tasks. In the t-th iteration, the modelparameter xt encounters either Vgjt,pos(xD or VgJtneg(Xt). These two different cases indicateswhether(/几(xt), NgJt (xt)i is positive or not. To prevent χt from escaping the feasible region, i.e.,catastrophic forgetting, we impose a theoretical condition on learning rates for f and g.
