Figure 1: Despite performance con-vergence, the network continues tolous diffusion where c corresponds to the anomalous dif-fusion exponent. Standard Brownian motion correspondsto c = 1. Similar observation were made by Baity-Jesiet al. [14] who noticed distinct phases of the training tra-jectory evident in the dynamics of the global displace-ment and Chen et al. [15] who found that the exponentof diffusion changes through the course of training. Aparallel observation is given by Hoffer et al. [18] for thebeginning of training, where they measure the global dis-move through parameter space. Weplot the squared Euclidean norm for thelocal and global displacement (δk and∆k) of five classic convolutional neu-ral network architectures. The networksare standard Pytorch models pre-trainedon ImageNet [16]. Their training is re-sumed for 10 additional epochs. Weshow the global displacement on a log-
Figure 2: Oscillatory dynamics inlinear regression. We train a linearnetwork to perform regression on theCIFAR-10 dataset by using an MSEloss on the one-hot encoding of the la-bels. We compute the hessian of theloss, as well as its top eigenvectors.
Figure 3: The training trajectory behaves isotropically, regardless of the training loss. Weresume training of a pre-trained ResNet-18 model on ImageNet and project its parameter trajec-tory (black line) onto the space spanned by the eigenvectors of its pre-trained Hessian q1, q30 (witheigenvalue ratio ρ1 /ρ30 ` 6). We sample the training and test loss within the same 2D subspaceand visualize them as a heatmap in the left and center panels respectively. We visualize the mod-ified loss computed from the eigenvalues (ρ1 , ρ30) and optimization hyperparameters according toequation (10) in the right plot. Note the projected trajectory is isotropic, despite the anisotropy ofthe training and test loss.
Figure 4: Implicit velocity regulariza-tion defined by the inverse Hessian.
Figure 5: Phase space oscillations aredetermined by the eigendecomposi-tion of the Hessian. We visualize theprojected position and velocity trajecto-ries in phase space. The top and bottompanels show the projections onto q1 andq30 respectively. Oscillations at differ-ent rates are distinguishable for the dif-ferent eigenvectors and were verified bycomparing the dominant frequencies inthe fast Fourier transform of the trajec-tories.
Figure 6: Understanding how the hyperparameters of optimization influence the diffusion. Weresume training of pre-trained ResNet-18 models on ImageNet using a range of learning rates, batchsizes, and momentum coefficients, tracking ∣∣δtk2 and k ∆t k2. Starting from the default hyperparam-eters, namely η = 1e 一 4, S = 256, and β = 0.9, we vary each one while keeping the others fixed.
Figure 7: The fitted rate of anomalous diffusion increases with the length of trajectory fitted.
