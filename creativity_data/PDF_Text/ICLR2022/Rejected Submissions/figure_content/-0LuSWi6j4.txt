Figure 1: Mean-squared error, ELBO, and gradient norm for the standard VAE with trainable vari-ance versus a VAE targeted to achieve the same distortion. The former exhibits unstable training.
Figure 2: Samples from a standard VAE with a trainable variance versus a VAE targeted to achievethe same distortion. Both have poor sample quality despite achieving good ELBOs (Figure 1).
Figure 3: Distortion-targeted VAE with a non-negligible distortion target γ = 16, trained on SVHN.
Figure 4: Reconstructions from distortion-targeted VAEs for various choices of γ. Choosing γ = 8suffices to achieve a high-quality reconstruction and requires encoding significantly less information(rate) into the latent space than γ = 0.5.
Figure 5:	Samples from distortion-targeted VAEs for various choices of γ. Lower rates result ineasier modeling but blurrier samples. Higher rates make the modeling task much more challenging,causing poor sample quality despite good reconstructions (Figure 4)We optimize this constrained objective using a Lagrange multiplier (Zhao et al., 2018). We note thatthe rate term DKL (qφ(zo:T | x) k Pθ (zo：T)) is an upper bound on the mutual information I(X; Z0：T)under the distribution defined by pθ(x)qg(zo：T | x) (AIemietaL,2018). By varying the choice of Y,we can modulate the amount of information about x that is stored in in the latent code z0:T. Figure 4shows the reconstructions along with the rate of the learned model. Models with high rate containconsiderably more information about x in the latent space and adds additional burden that the prior4Under review as a conference paper at ICLR 20210c□-□ θ>4e6ΘNFigure 6:	Rate, conditional log-likelihood lnpθ(x | z0:T), and ELBO as a function of the distortionachieved by various VAEs on SVHN. The ELBO favors high-rate models, which has poor samplequality (Figure 5).
Figure 6:	Rate, conditional log-likelihood lnpθ(x | z0:T), and ELBO as a function of the distortionachieved by various VAEs on SVHN. The ELBO favors high-rate models, which has poor samplequality (Figure 5).
Figure 7: Samples from tWo-stage training for SVHN and CelebA. The first roW shoWs samples fromthe primary VAE. The next four rows show samples from secondary VAEs with various choices ofY. The secondary VAE makes near-imperceptible visual changes to the primary VAE,s samples. Thelast row shows samples from a VAE that directly targets a small distortion. In contrast to two-stagetraining, directly targeting a small distortion yields poorer sample quality.
Figure 8: Test set ELBO from two-stage training as a function of the final distortion achieved.
Figure 9: Mean-Squared Error over the first 100000 iterations. All models quickly converge to thetargeted distortion values.
Figure 10: Reconstructions for SVHN and CelebA as a function of the distortion target γ and train-ing iterations. Despite achieving the same distortion (Figure 9), earlier iterations capture higher-frequency information.
