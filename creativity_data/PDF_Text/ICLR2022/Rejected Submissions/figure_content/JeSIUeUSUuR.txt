Figure 1: Landscape of kFL(x, W, b)k2. Rows 1 to 3 are for FCNets with 3 activations: Sigmoid(L = 2,4,6, 10, 15), ReLU and ABS (L = 2, 10, 20, 40, 60). Row 4 is for HanNets (to be defined)with the same L values as for ReLU and ABS.
Figure 2: Variability measured by V3 in (4) for FCNet-ReLU (left) and FCNet-ABS (middle) andHanNets (right). Each bar represents a geometric mean of 3000 parameter samples. As depth Lgrows, the width d decreases so that the total number of model parameters is approximately 4000.
Figure 3: Overall structure of the tested Han/MLP-Mixer model (LM can be zero).
Figure 4: The top row consists 3 functions (from left to right): the target binary function, a functiontrained from a FCNet and one from a HanNet, respectively. The bottom row: (a) the training set with25% of data points, (b) and (c) are top views of the two trained solutions (rounded to 0 or 1), alongwith their test accuracies. The training accuracies are nearly 100% for both.
Figure 5: (Left) FCNet: the best testing accuracy is UP to 87%. (Mid) HanNet: the best testingaccuracy is over 99%. (Right) FCNet vs HanNet: the number of model parameters in each line isfixed, resPectively, at 4000, 8000 and 20000 for FCNet-s, FCNet-m and FCNet-l, and at 1000, 3000,5000 for the 3 lines for HanNets. In all exPeriments, the training errors are near zero. The Plot showsthe existence of significant gaPs in generalization ability between FCNets and HanNets.
Figure 6: Sizes of C- and G-matrices for ReLU and absolute-value activations15Under review as a conference paper at ICLR 2022The right plot of Figure 6 is for the comparison of two different activation functions, ReLU, andabsolute-value. Starting from a random point-pair, We plot the C-matrices computed on a set ofrandom weight-bias pairs as specified with initialization multipliers，2/d and，1/d for ReLU andabsolute-value, respectively. The plot shoWs clearly that ReLU, With considerably smaller C-matrices,is much more prone to the negative influence of C2C than the absolute-value is. At L = 1000, theC-matrix for ReLU is more than three orders of magnitude smaller than that for absolute-value.
Figure 7: Checkerboard: left plot for data points; right plot for labels; colors match binary labels.
Figure 8: FCNet-ReLU vs FCNet-ABS on Checkerboard. The 1st-row contains computed trainingand testing losses, and the 2nd-row training and testing accuracies. Columns 1 to 3 (from left toright) are for results corresponding to the number of model parameters equal to 1600, 2400, and 3200,respectively (recall that the training set contains 1640 samples). Each group of curves depicts themean and variance of 10 random runs.
Figure 9:	TWo implementations of the channel mixing module using MLP-Mixer and Han-Mixer,respectively. HH denotes Householder.
Figure 10:	(a) Testing aCCuraCy in FCNet and HanNet. (b) Training set: violet-Colored points arethose whose 10% labels are flipped (baCkground Color represents their original label). (C) FCNet: thebest testing aCCuraCies are 86.5% in the model trained from (b). (d) HanNet: the best testing aCCuraCyis 95.2% .
Figure 11:	All figures show testing performance where the top row is for the Elevators dataset; andthe bottom row for Cal Housing. From left to right: (1) HanNet (red) vs. FCNet1 (blue) on 80%training data, (2) then on 20% training data; (3) HanNet (red) vs FCNet2 (blue) on 80% training data,(4) then on 20% training data. Note that HanNet testing results are repeated.
Figure 12: Visualization of variability on FCNet-Sigmoid with 2,4,6,10,15 hidden layers.
Figure 13: Visualization of variability on FCNet-ReLU with 2,10,20,40,60 hidden layers.
Figure 14: Visualization of variability on FCNet-ABS with 2,10,20,40,60 hidden layers.
Figure 15: Visualization of variability on HanNet with 2,10,20,40,60 hidden layers.
