Figure 1: Training loss and validation perplexity during GPT-2 pre-training (baseline without curricu-lum learning) under different model sizes, batch sizes (and LR), and sequence lengths. All subfiguresshare the same legend as 1(b). In 1(b) and 1(d), the purple line for “1.5B, Bsz 512, Seqlen 1K” istruncated (the whole training takes 341 hours).
Figure 2: Training loss and validation perplexity during GPT-2 117M pre-training, comparing thebaseline and curriculum learning under different batch sizes/LR and sequence lengths. All sub figuresshare the same legend as 2(b) (“CL 60K” means applying curriculum learning to the first T =60Ksteps).
Figure 3: Training loss, validation perplexity, and Adam variance norm/max element during GPT-2 1.5B Seqlen 1K pre-training, comparing the baseline and curriculum learning under differentbatch sizes/LR. Also compare with two related works (“2-stage CL” (Press et al., 2020) and “BszWarmup” (Brown et al., 2020)) described in Section 5.4. These two related works are not presentedin 3(b) and 3(d) because they are performed on different hardware. All subfigures share the samelegend as 3(f) (“CL 45K” means applying curriculum learning to the first T =45K steps).
Figure 4: Step-wise and token-wise validation perplexity during GPT-2 117M Seqlen 1K pre-trainingwith batch size 512 and different curriculum learning duration. (“CL 20K” means applying curriculumlearning to the first T =20K steps).
Figure 5:	Validation perplexity and learning rate during GPT-2 1.5B Seqlen 1K pre-training withbatch size 512, comparing the baseline and curriculum learning under different learning rate decayschedules (“CL 270K” means applying curriculum learning to the first T =270K steps).
Figure 6:	Training loss during GPT-2 pre-training, comparing the baseline, related works andcurriculum learning.
Figure 7: Step-wise training loss during GPT-2 1.5B Seqlen 1K pre-training (first 3K steps only) withbatch size 2K, seed 1236, and different learning rates for baseline and curriculum learning (“CL 8K”means applying curriculum learning to the first T=8K steps).
Figure 8: Training loss, Adam momentum l1 norm, and Adam variance l1 norm/max element duringGPT-2 1.5B Seqlen 1K pre-training (first 5K steps only) with batch size 4K, comparing the baselineand curriculum learning under different gradient clipping levels. Grad clip 1.0 indicates that theglobal gradient l2 norm is clipped to 1.0. All subfigures share the same legend as 8(d) (“CL 45K”means applying curriculum learning to the first T =45K steps).
Figure 9: Step-wise training loss during GPT-2 1.5B Seqlen 1K pre-training (first 8K steps only)with batch size 4K (the same hyperparameters as the second set in Section 3), comparing baseline,curriculum learning, and curriculum learning plus inserting 10% of full-length sequences every 375steps.
