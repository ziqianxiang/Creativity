Figure 1: Overview of the Iterative Memory Network (IMN) architecture. The Iterative MemoryUpdate module (in purple) illustrates the repeated memory update process. The details about themulti-way attention and the Memory Enhancement module are on the left.
Figure 3: Ablation study on the model structure5.4	Ablation StudyWe conduct ablation study about the model structure and report the results in Fig.3. We remove theMemory Enhancement module to produce the ablation model IMN(w/o. m.e.). We further removethe cross with the target item to product the ablation model IMN(delayed cross & w/o. m.e.). Wereplace the Euclidean distance with another Hadamard distance calculation for the ablation modelIMN(w/o. Euclidean dist.). We remove the iterative update process to produce IMN(w/o. iterativewalk), which becomes the same as DIN. We replace the attention with average pooling to produceIMN (w/o. attention), which becomes identical to YouTube DNN. The following are our findings:â€¢	The iterative update process models intra-sequence dependencies, which benefits the model per-formance significantly. IMN(full) has a remarkable improvement over IMN(w/o. iterative walk),on par with the improvement of IMN(w/o. iterative walk) over IMN(w/o.attention). It demon-strates the effectiveness of our proposal to model intra-sequence dependencies in addition totarget-sequence dependencies as in DIN.
Figure 4: Computational and memory efficiency analysisof signal traversal paths. SASRec is O(1) due to the pairwise comparisons in the self-attentionmechanism. Our IMN also is O(1) since the memory vector with the sequence information in-teracts with sequence items through the multi-way attention. There is no intra-sequence signalpassing in DIN and UBR4CTR. DIEN and MIMN are at O(L) due to the sequential nature.
Figure 5: IMN performances and attention entropy with increasing sequence walk iterationsTable 3: Online A/B result, with Raw denoting the raw data and Impr the relative improvement.
