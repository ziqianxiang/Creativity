Figure 1: Data poisoning attacks require a new approach to adversarial training to robustify machinelearning models against this threat model.
Figure 2: Visualization of the effects of poisoning attacks against an undefended and a defendedmodel. Top: Feature collisions via Aghakhani et al. (2020). Bottom: Gradient matching as in Geipinget al. (2021). The target image is marked by a black triangle and is originally part of the class coloredblue. The poisoned images are colored red and are part of the class colored green. The x-y axis ineach diagram corresponds to a projection of the principal direction separating both classes, whilethe confidence in the original target class is marked on the z-axis. The defended models generate afeature space in which poisons (red) behave consistently with the robust model, prevening collsionsin Fig. 2b and alignment in Fig. 2d, so that the target (black) remains correct.
Figure 3: Avg. Poison Success versus validation accuracy for various defenses against the gradientmatching attack of (Geiping et al., 2021) in the from-scratch setting. The baseline undefended modelis shown in blue, the proposed defense in red. The differentially private SGD is shown for noisevalues from 0.0001 to 0.01. The proposed defense is a strong trade-off of robustness and accuracy.
Figure 4: Feature Defense. A robustbase model can withstand feature colli-sion attacks even when fine-tuning non-robustly on poisoned data. Note howno collision forms around target and themodel remains robust.
Figure 5: Effectiveness of the Defense against large attack budgets for the case of a defense againstgradient matching in the from-scratch setting on a ResNet-18. In this scenario, the attack alreadysucceeds when 1% of the data is poisoned, but the attacker can increase their budget further andpoison more of the data. The defense is still effectively reduces poison success even with large levelsof poisoned data with a roughly logarithmic scaling. Note that 10% is the limit for this attack onCIFAR-10, for which all data points from the poisoned class are modified.
Figure 6: 3D Visualization of a feature collision attack (via Poison-Frogs) against an undefendedand a defended model. The defended model significantly hinders feature collisions. The target imageis marked by a black triangle and is originally part of the class marked in blue. The poisoned imagesare marked in red and are part of the class marked in green.
Figure 7: 3D Visualization of the effects of a feature collision (Bullseye Polytope) attack against anundefended and a defended model. The defended model significantly hinders feature collisions. Thetarget image is marked by a black triangle and is originally part of the class marked in blue. Thepoisoned images are marked in red and are part of the class marked in green. Notably the strongcollision seen in the baseline is inhibited by the defense.
Figure 8: 3D Visualization of the effects of a gradient matching attack (Witches’ Brew) against anundefended and a defended model. The defended model significantly hinders feature collisions. Thetarget image is marked by a black triangle and is originally part of the class marked in blue. Thepoisoned images are marked in red and are part of the class marked in green.
Figure 9: 3D Visualization of the effects of a backdoor trigger patch attack against an undefendedand a defended model. The target trigger is applied to a number of target images shown in black andis originally part of the class marked in blue. The poisoned images are marked in red and are part ofthe class marked in green. Note how the black datapoints are associated with the poison class in theundefended case, but correctly associate with the target class in the defended case.
Figure 10: 3D Visualization of the effects of a gradient matching attack (Witches’ Brew with squaredloss) against an undefended and a defended model. The defended model significantly hinders featurecollisions. The target image is marked by a black triangle and is originally part of the class marked inblue. The poisoned images are marked in red and are part of the class marked in green.
Figure 11: 2D Visualization of a feature collision attack (via Poison-Frogs) against an undefendedand a defended model. The defended model significantly hinders feature collisions. The target imageis marked by a black triangle and is originally part of the class marked in blue. The poisoned imagesare marked in red and are part of the class marked in green.
Figure 12: 2D Visualization of the effects of a feature collision (Bullseye Polytope) attack againstan undefended and a defended model. The defended model significantly hinders feature collisions.
Figure 13: 2D Visualization of the effects of a gradient matching attack (Witches’ Brew) againstan undefended and a defended model. The defended model significantly hinders feature collisions.
Figure 14: 2D Visualization of the effects of a backdoor trigger patch attack against an undefendedand a defended model. The target trigger is applied to a number of target images shown in black andis originally part of the class marked in blue. The poisoned images are marked in red and are part ofthe class marked in green. Note how the black datapoints are associated with the poison class in theundefended case, but correctly associate with the target class in the defended case.
Figure 15: 2D Visualization of the effects of a gradient matching attack (Witches’ Brew with squarederror) against an undefended and a defended model. The defended model significantly hinders featurecollisions. The target image is marked by a black triangle and is originally part of the class marked inblue. The poisoned images are marked in red and are part of the class marked in green.
