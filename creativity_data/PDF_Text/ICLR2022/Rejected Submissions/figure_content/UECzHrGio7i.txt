Figure 1: Reward vs. fraction of corruptions in Lunar Lander environment. Shaded region representsone standard deviation for 50 trials. We fix the sample size N = 4000 for the demonstration data set,and vary the fraction of corruptions up to 30%. Our algorithm Robust Behavior Cloning (RBC) oncorrupted demonstrations has nearly the same performance as BC on expert demonstrations (this isthe case when = 0), which achieves expert level. Also, it barely changes when grows larger. Bycontrast, the performance of vanilla BC on corrupted demos fails drastically.
Figure 2: Offline Imitation Learning on four different continuous control tasks with demonstrationdata of size 60000. We vary the corruption ration = 10%, 20%. For every 5 epochs, we evaluatethe current policy in the environment for 20 trials, and the shaded region represents the standarddeviation. Vanilla BC on corrupted demonstrations fails to converge to expert policy. Using therobust counterpart Algorithm 1 on corrupted demonstrations has good convergence properties. Sur-prisingly, our RBC on corrupted demonstrations has nearly the same reward performance of usingBC on expert demonstrations.
