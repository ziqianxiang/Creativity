Figure 1: Overview of the proposed SDA-FL framework. Before the federated learning, the syn-thetic data from all clients are sent to the PS to construct a global synthetic dataset. In each iteration,every client first downloads the global model and updates the pseudo labels of the synthetic data forlocal training. The local models are then uploaded to the PS for pseudo label updating and modelaggregation. Lastly, the PS updates the global model wt+1 with the updated synthetic dataset.
Figure 2: Test accuracy of different methods for federated semi-supervised learning on MNIST,FashionMNIST, and Cifar-10 classification tasks.
Figure 3: Test accuracy and FID as a function of the privacy budget. We run three trails, and reportthe mean and the standard deviation of the test accuracy. The FIDs of the real samples in MNIST,FashionMNIST, and Cifar-10 are 10.54, 23.17, and 42.70, respectively, which are much larger thanthose of the synthetic data. It illustrates that the differentially private synthetic data, while of lowimage quality, still effectively help to improve the training performance with non-IID data.
Figure 4: Visual comparison between synthetic data and the original Cifar-10 dataset. The syntheticdata are sampled from the generator trained with privacy budget = 200.
Figure 5: Test accuracy on FashionMNIST with Figure 6: Test accuracy on FashionMNIST withvarying server update steps. The ∞ steps mean different rounds of pseudo label updating.
Figure 7: Test accuracy and FID on Fashion- Figure 8: Test accuracy and FID on Fashion-MNIST, under varying training rounds for the MNIST, under varying samples for training theGANs.	GANs locally.
Figure 9: Test accuracy onFashionMNIST, under varyingsizes of the synthetic dataset.
Figure 10: Test accuracy onFashionMNIST, under varyingλ2 in the local update.
Figure 11: Test accuracy onFashionMNIST, under varyingthresholds.
