Figure 1: Adversarial training framework of auto-encoding inverse reinforcement learning. Theauto-encoder computes the reconstruction error for these two mini-batches of data examples andoptimize the objectives. The surrogate reward function provides the signal to the agent.
Figure 2: The clean data points lie neara low dimensional manifold illustrated withthe bold black line. The gray circle showsthe noisy sampling process. And the auto-encoder learns to denoise the noisy data toclean points.
Figure 3: Mean and standard deviation return of the evaluation policy over 10 rollouts and 5 seeds,learning from non-noisy expert demonstrations, reported every 100k timesteps. The return is interm of the environment’s ground truth reward.
Figure 4: t-SNE visualization of latent representations of the reward function on Walker2d for differ-ent methods with hyper-parameter preplexity = 30.0. Top row: on non-noisy expert data, bottomrow: on noisy expert data (Gaussian noise (0, 0.5)).
Figure 5: Scaled normalized re-wards for different levels in the tra-jectory space on Walker2d.
Figure 6: Illustrations for locomotion tasks we used in our experiments: (a) Swimmer; (b) Hopper;(c) Walker; (d) HalfCheetah; (e) Ant; (f) Humanoid.
Figure 7: Mean and standard deviation return of the evaluation policy over 10 rollouts and 5 seedslearning from noisy expert demonstrations, reported every 100k timesteps. The return is in term ofthe environment’s ground truth reward.
Figure 8: t-SNE visualization of latent representations of the reward function on Walker2d for dif-ferent methods. Top row: on non-noisy expert data (preplexity = 5.0); Second row: bottom row:on non-noisy expert data (preplexity = 50.0); Third row: on noisy expert data (preplexity = 5.0);Bottom row: on noisy expert data (preplexity = 50.0).
Figure 9: Mean and standard deviation return of the evaluation policy over 10 rollouts and 5 seedslearning from non-noisy expert demonstrations, reported every 100k timesteps. The return is in termof the environment’s ground truth reward.
Figure 10: Mean and standard deviation return of the evaluation policy over 10 rollouts and 5 seedslearning from non-noisy expert demonstrations, reported every 100k timesteps. The return is in termof the environment’s ground truth reward.
Figure 11: More stable training procedure: norm of weights for the policy network and the corre-sponding variance of policy gradients for non-noisy and noisy expert data. Top: Walker2d; Middle:Swimmer; Bottom: Hopper.
Figure 12: Mean and standard deviation return of the evaluation policy over 10 rollouts and 5 seeds,learning from non-noisy expert demonstrations, reported every 100k timesteps. The return is interm of the environment’s ground truth reward.
