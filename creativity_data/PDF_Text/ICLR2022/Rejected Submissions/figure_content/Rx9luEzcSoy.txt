Figure 1: Overview of our work. In both untrained and pretrained DNN-based image priors, westudy the existence of Lottery Image Prior (LIP) that could transfer to various image inverse prob-lems such as denoising, inpainting, super-resolution, and/or compressive sensing restoration.
Figure 2: LIP visual results: inpainting (row 1), super-resolution (rows 2/3) and denoising (row 4).
Figure 3: Experimental results of finding LIP in setting i (i.e., DIP). The first row of the figuresummarizes the LTH IMP training loops and the second row denotes the evaluation of found LIP.
Figure 4: Experiments of the rewind strategy (background task: denoising). Note that we train themodel with N epochs in IMP and ReWindj means rewinding the ticket parameter to θj, the weightsafter j% × N steps of training.
Figure 6: Experimental Results of Multi LIP on images from same/different domains. We compareMulti LIP with LIP and random prune methods. The background task is denoising in setting i.
Figure 5: We evaluate differ-ent single-image LIPs and themulti-image LIP on Baby imageto measure LIP’s transferability(cross images).
Figure 7: Transferability (cross tasks) experimental results. We study the transferability of denois-ing LIP on the restoration tasks such as inpainting and super-resolution (SR); we also study theinpainting and SR LIP on the denoising task. We consider two SR scale factors = 4, 8.
Figure 8: Transferability experiments. We test the denoising LIP on MNIST and CIFAR10 datasets(left 2 figures). Note that we replace the last convolutional layer of DIP models with the linear layerand load the same initial weights. We evaluate the CIFAR LIP on the denoising task.
Figure 9: Visual results of compressed sensing using LIPs found in a pre-trained PGAN.
Figure 10: Visual results of inpainting using LIPs found in a pre-trained PGAN.
Figure 11: Layer-wise sparsity ratio results of LIP, SNIP and randomly pruned tickets. Note thatwe summarize the sparsity ratio of each layer: the ratio of the number of parameters whose valuesare equal to zero to the number of total parameters of the layer. And the x-axis of these figuresis composed of the serial numbers of model layers. We sampled subnetworks with four differentsparsities (sparsity = 36%, 59%, 89%, 95%) to observe.
Figure 12: The learning curve plots when using different subnetworks towards the DIP task. Inthe figure, S denotes the sparsity of the model. We compare both PSNR and SSIM values. Forfair comparisons, we trained these subnetworks on the denoising task on the Baby image with 3000iterations, then trained in isolation (the iteration number is recommended by (Chen et al., 2020c) tocapture the ”early-stopping” phenomenon of DIP), and summarized their performances.
Figure 13: Learning curves using four different training targets: a clean image (Baby.png), the sameimage added with noises, the same randomly scrambled and white noise. Note that we use fourdifferent models: the LIP subnetwork (S = 89%), randomly pruned subnetwork (S = 89%), SNIPsubnetwork (S = 89%) and the dense model (S = 0%). And we trained them in isolation in thesame experimental settings for 10000 iterations.
Figure 14: Images used in plotting the curves of experiments.
Figure 15: Evaluating the reconstruction images of different subnetworks (LIP, SNIP and randompruning) by FFT (Fast Fourier Transformation) to check whether the high frequency informationhas been lost during pruning. Note that we experimented on the Baby.png. We found that com-pared with random pruning, LIP and SNIP can both maintain the high frequency information of theground-truth. For example, the LIP and SNIP subnetworks both maintain mostof the high-frequencyinformation of the ground-truth at the sparsity 79%, but the LIP could also performs well at the spar-sity 59% where the SNIP loses more high-frequency information.
