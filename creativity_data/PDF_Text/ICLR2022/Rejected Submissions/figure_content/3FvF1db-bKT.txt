Figure 1: A schematic depiction of our local augmentation. The purple and yellow circles on thegraph correspond to the central node and its augmented neighbors respectively. After augmenting theneighborhood, we exploit the initial and the generated feature matrix as input for downstream GNNs.
Figure 2: (a) The original graph. (b) EP-B exploits the neighbors to reconstruct the central nodeâ€™sembedding. (c) GraphSAGE encourages nearby nodes to have similar embeddings. (d) Given therepresentation of the central node, our aim is to infer the representations of the connected distributionof neighbors.
Figure 3: The distribution of the attribute bin ofthe inference neighbors vs. the distribution of theattribute bin of the original neighbors, with KLdivergence = 0.0026. The value of each featurebin is the sum of the attribute values of multiple di-mensions of the feature vector. We split the featurevector into multiple feature bins.
Figure 4: GCN and LA-GCN architectures. The difference between GCN and LA-GCN architecturesis that the LA-GCN has an additional convolutional layer for X and it uses a concatenation operationto mix the hidden representations.
Figure 5: LA-GCNII architecture. The difference between GCNII and LA-GCNII is that the LA-GCNII has an additional MLP layer for X and it uses a concatenation operation to mix the hiddenrepresentations.
