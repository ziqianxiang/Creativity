Figure 1: Induced distances for f : R2 → R2, f(x0, x1) = (max(x0, 0), max(x1, 0)), (a pair ofReLUs). Let a, b > 0. Then the fiber f-1 ({(a, b)}) is the point {(a, b)}. The fiber f-1 ({(a, 0)})is the ray {(a, y) : y ≤ 0}. The fiber f-1 ({(0, 0)}) is the quadrant {(x, y) : x ≤ 0, y ≤ 0}. ForU = (1,4) and V = (9, -5), We have εf (u, v) = 5, U = (4,0), V = (4, -5), and z = (4,0).
Figure 2: RobuStneSS of the repreSentation obtained from the first linear layer of a 3-layer FCNNuSing different training procedureSInput spaceBenign trainingAdv. train, C = 1.0Adv. train, C = 2.0Adv. train, C = 3.0TRADES, c = 1.0TRADES, c = 2.0TRADES, c = 3.0(a) MNISTssol yportne-ssorCInput spaceBenign trainingAdv. train, C = 2.0Adv. train, e = 3.0TRADES, c = 2.0TRADES, c = 3.03	3.5	4	4.5	5	5.5	6(b) Fashion MNIST
Figure 3: RobuStneSS of repreSentationS obtained from different layerS of a 3-layer FCNN traineduSing benign trainingboth the MNIST andFaShion MNIST dataSetS. AS expected, the linear layerS of modelS with benigntraining are the leaSt robuSt, with robuStneSS increaSing with the budget train uSed during training.
Figure 4: Robustness of representations obtained from different layers of a 3-layer FCNN trainedusing PGD adversarial training with = 2.0Input space -M-Benign training -θ-Adv. train, C = 1.0 —I—Adv. train, c = 2.0 -ɪ-Adv. train, e = 3.0TRADES, c = 1.0 -+2	2.5	3	3.5	4TRADES, C = 2.0TRADES, c = 3.0 -■(a) MNISTssol yportne-ssorCssol yportne-ssorC2	2.5	3	3.5	4	4.5	5	5.5	6(b) Fashion MNISTInput space -M-First linear -θ-First ReLU *Second Linear
Figure 5: Robustness of the representation obtained from the second ReLU layer of a 3-layer FCNNusing different training procedures.
Figure 6:	Robustness of the representations obtained from fully connected networks with layers ofdecreasing size.
Figure 7:	0 - 1 Loss: Robustness of the representation obtained from the first linear layer of a3-layer FCNN using different training proceduresF	Additional ResultsF.1 Impact of reduction of network sizeIn Figure 6, we compare the robustness of representations obtained from fully connected networkswith decreasing layer sizes. The ‘Regular’ network is the one used throughout, while the ‘Small’and ‘Smaller’ networks have corresponding layers that are 2× and 4× narrower respectively. Wecan clearly see that as the width of the feature extractor decreases, so does its robustness.
Figure 8:	0 - 1 Loss: Robustness of representations obtained from different layers of a 3-layerFCNN trained using benign trainingInput space ->t-First linear -θ-First ReLU +Second LinearSeCond ReLU *~ssol 1-0ssol 1-0Input space ->t-First linear -θ-First ReLU +Second LinearSecond ReLU *~2	2.5	3	3.5	4	4.5	5	5.5	6	2	2.5	3	3.5	4	4.5	5	5.5	6(a) MNIST	(b) Fashion MNISTFigure 9:	0 - 1 Loss: Robustness of representations obtained from different layers of a 3-layerFCNN trained using PGD adversarial training with = 2.0ssol 1-0Input space -M-
Figure 9:	0 - 1 Loss: Robustness of representations obtained from different layers of a 3-layerFCNN trained using PGD adversarial training with = 2.0ssol 1-0Input space -M-Benign training -θ-Adv. train, C = 1.0 -∙~Adv. train, C = 2.0 -t-Adv. train, C = 3.0TRADES, C = 1.0 -+TRADES, C = 2.0 -*TRADES, C = 3.0 -■ssol 1-0Input spaceBenign trainingAdv. train, e = 2.0Adv. train, e = 3.0TRADES, c = 2.0TRADES, C = 3.02	2.5	3	3.5	4	4.5	5	5.5	6	2	2.5	3	3.5	4	4.5	5	5.5	6(a) MNIST	(b) Fashion MNIST
Figure 10:	0 - 1 Loss: Robustness of the representation obtained from the second ReLU layer of a3-layer FCNN using different training procedures.
Figure 11:	Robustness of the representation obtained from the first linear layer of a 4-layer CNNusing different training procedures.
