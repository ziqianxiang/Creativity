Figure 1: Adversarial Surprise is a multi-agent competition in which two policies take turns con-trolling a single agent. The Explore policy acts first, and tries to put the agent into surprising,high entropy states. On its turn, the Control policy tries to minimize surprise by finding familiar,low-entropy, predictable states. As training continues, the competition drives the agents to learnincreasingly complex behaviors. In the above example, the Control policy eventually learns to pickup a key and lock the door to prevent the Explore agent from taking it into the room with randomlymoving objects (a noisy TV state).
Figure 2: State coverage in stochastic BMDPs: the number of rooms (out of 4) explored both (b)cumulatively over training, and (c) within an episode. AS explores better than the state-of-the-artexploration methods RND and ASP, which become distracted by noisy elements. AS outperformsSMiRL, which minimizes observation surprise by turning to face the wall, or remains in dark rooms.
Figure 4: Zero-shot transfer in Atari: Each method is trained in Atari using only intrinsic reward,then transferred zero-shot to optimizing game reward. Plots show game reward, where error bars arethe 95% Confidence Interval (CI) of 5 seeds. The games reward behavior such as staying alive andshooting enemies, so obtaining higher reward indicates the agent has learned meaningful behaviors.
Figure 3: State coverage in Doom:the heatmaps show log p(s) over thefirst 1000 training steps, where s is theagentâ€™s x-axis position in Doom. Blackareas indicate p(s) = 0, meaning theagent has never visited these coordi-nates. These results show that AS fullyexplores the state space, while RNDfails to explore both edges and SMiRLfails to explore the right edge.
Figure 5: Zero-shot transfer in Doom.
Figure 6: Q2. Emergence of Complexity: theaverage number of times the agent flip a switch tostop lights from flashing. ASP and RND do notlearn to press the switch, while SMiRL and ASboth press the switch a similar number of times.
Figure 7: Q3. Emergence of Complexity: a) An example environment which contains a keythat can be used to lock a door separating the agent from stochastic elements. b) We observe tworelatively short phase transitions separating three learning phases with three clearly distinguishablebehaviors: randomly exploring, going to the dark room, locking the agent in the dark room (left).
Figure 9: The four Atari environments we used to test each method.
