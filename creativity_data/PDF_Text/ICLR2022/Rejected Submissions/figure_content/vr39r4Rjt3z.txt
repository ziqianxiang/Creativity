Figure 1: Different Classifier Network Designspaper, the authors compartmentalised their backbone network. They employed a fixed feature ex-tractor and only fine-tuned their classifiers. Unlike the conventional multiple layer perceptron (MLP)classifier of Figure 1(a) used in most studies (such as Mai et al. (2020)), their classifier which theycalled highway connection networks (HCNs) had the peculiar design shown in Figure 1(b). AnHCN included a two-layer perceptron with highway connection (Srivastava et al., 2015) followedby a softmax layer. We used red to denote fully connected layers, yellow for softmax, and blue forthe HCN gated unit. They demonstrated that HCNs were more robust against forgetting than MLPs;and even showed that naively sequential learning with HCNs could often outperform MLP Classi-fiers with continual learning techniques applied. Moreover, they found that highway connectionswere specifically required and could not be replaced with residual connections (He et al., 2016).
Figure 2: Differences in the Inferences and Updates of the HCN and MHC ClassifiersBut does the entirety of weight WLγ need to be updated at all times? Two prior studies, the lotteryticket hypothesis (LTH) (Frankle & Carbin, 2019) and the uncertainty-regularised continual learning(UCL) (Ahn et al., 2019), suggested not. In network pruning, LTH found that only a small subsetof the dense network needed to be updated to achieve competitive performances. Whereas UCLshowed that continual learning could be achieved by regularising specific nodes in the weight thatdeviated from a pre-determined distribution. They both considered the structural relationship amongnodes in the base learner. A similar approach can be applied to HCN.
Figure 3: Longitudinal Changes in Activation Values After Task ChangingIn order to demonstrate the effectiveness of LWNs over MLPs, we prepared a toy example which firsttrained both classifiers with MNIST (LeCun et al., 1998) and then with FashionMNIST (Xiao et al.,2017). Both classifiers had the hidden dimensions H = 100 with the two layer designs shown inFigures 1(d) and 1(a) respectively. Both datasets contained grey-scale images on 28 × 28 pixel boxeswith 60K and 10K images for training and testing. After pre-training both classifiers on MNIST, weexternally stored a batch of MNIST images BMNIST and applied them on the classifiers to collect theoutput activations of the second layer a(L0). When we fine-tuned the classifiers for FashionMNIST,we re-applied BMNIST and collected the new activation a(Lξ) for each ξ-th update. We then plottedthe changes in activation in Figures 3(a) and 3(b). See Appendix A for more details in the setup.
Figure 4: Longitudinal Changes in Activation Values After Task Changing0.75-0.50，	l	l	l	l	- 0.00200	400	600	800	1000Iterations(b) LWN in MLPIn order to understand the reason why BN was disadvan-tageous for continual learning, we repeated the experi-ments in Section 5 where we first trained BN in MLP forMNIST and then fine-tuned for FashionMNIST. We illus-trated the results in Figure 4. This figure was formattedfollowing the style of Figure 3. BN in MLP had muchTable 3: Results on Perm-MNISTClassifier	ACC (%)	FA1 (%)MLP	:66.4	67.9LWNinMLP	684	713BN in MLP	44.3	38.3deeper colours than LWN in MLP. In addition, the divergence was Diffact = 97.66 in BN in MLPand forgetting was hence 41%(= [1 - 69.03/97.66] × 100%) more severe than LWN in MLP.
Figure 6: Longitudinal Changes in Activation Values After Task ChangingFolloWing the experimental setup as described in Appendix A, We further compared LWNs With 100and With 50 hidden dimensions. As shoWn in Figure 6, there Were more patches of deeper colours inan LWN With 50 hidden dimensions than there Were in an LWN With 100 hidden dimensions. Afterfine-tuning both classifiers, We found that an LWN With 100 hidden dimensions had a divergencescore of Diffact = 69.03 While an LWN With 50 hidden dimensions had Diffact = 37.34. AfterWeighting it equally across the number of dimensions, it Was Diffact = 0.69(= 69.03/100) perdimension for the former and Diffact = 0.75(= 37.34/50) per dimension for the latter. ForgettingWas hence 8.00%(= [1 - 0.69/0.75] × 100%) less severe in the former setting. This Was becausethat LWN exploited the hidden dimension in neural networks — the higher the dimensionality, themore evenly the features could be redistributed.
