Figure 1: The fully connected version of the original predictron implemented with 16 depth layerswith individual weights. Each fully connected layer has 128 neurons, except for the output layerswhich have 1. s is the input state while s0:K are the abstract states. V 0:K are the estimated valuesfor each abstract state. r0:K-1 are the expected abstract rewards received for transitioning fromone abstract state to the next. Y0:K-1 is the expected abstract discount factors to apply for eachabstract step. λ0:K-I is the expected eligibility assigned to each abstract step. As the abstract stepsis arbitrarily long, γ and λ can vary as well.
Figure 2: Here, the results from the objective function used to train the RL based policies arecompared. The results are based on 50 test runs for each setup of each tested environment.
Figure 3: Additional observations regarding mean lateness (Top) and the number of completed parts(Bottom) are shown here. The data is shown with the inner quartiles (colored boxes), the median(black lines), the mean (white dots), and the min and max values (whiskers). Both the mean latenessand the number of completed parts is reported for all parts completed after the initial two full systemruns, which are removed to align the observation with the objective function.
