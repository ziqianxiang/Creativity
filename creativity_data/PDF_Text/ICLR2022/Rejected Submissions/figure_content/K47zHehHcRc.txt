Figure 2: (Top) Traversals for autoencoder models on MNIST. (Bottom) Response error measuredon each of the six latent dimensions when traversing along the last dimension (= 5). Each line(in different colors) represents a sample drawn from the aggregate posterior. The horizontal axistracks the dimension being traversed. The vertical axis records the difference between response andoriginal sample along the corresponding dimension. For unregularized models, the traversals areperformed in a range whose extremities are identified by the aggregate marginal posterior Q(Z).
Figure 4: Diagram comparing the structure of an autoencoder with (right) and without (left) theExplicit Causal Latent Block. Instead of a single space modelling the representation, an XBlockemploys a noise or information space N and a causal or structure space Z.
Figure 6: Traversals for the autoencoder model in 3 variants, namely: Standard (left), ’X’ (center)and ’XC-I’ (right). The different variants are comparable in terms of generation quality. However,the Xnets show a higher degree of modularity between the dimensions. We hypothesise that theXblock favours independence. We include the full traversal plots in the Additional material (18).
Figure 7: Input reconstructions (first row) and prior samples (second row) for the autoencoder modelin 3 variants, namely: Standard, ’X’ and ’XC-I’. The prior samples are obtained through hybridsampling from the aggregate posterior distribution.
Figure 8: MMD between noises ag-gregate posterior and hybrid sam-pling distribution, measured usingan Inverse Multiquadratic kernel(IMQ). The distributions are ob-tained by recording the MMD met-ric over all S models configurations(including different random seeds).
Figure 9: Diagram illustrating interventional consistency.
Figure 10: Architectural skeleton used for all models. The input is fed to a convolutional network(ConvNet) to obtain its latent representation (in green). This is then fed to a second convolutionalnetwork (UpsampleNet), equipped with bilinear upsampling layers, that produces the output image.
Figure 11: Comparison of reconstruction quality of S models.
Figure 12: Comparison of S models on generated samples quality.
Figure 13: Comparison of S models on traversals quality.
Figure 14: Comparison of v32 models on reconstruction quality.
Figure 15: Comparison of v32 models on the generated samples quality.
Figure 16: Comparison of v32 models on the interpolation samples quality.
Figure 17: Comparison of v32 models(b)	XAE v32on traversals quality (1).
Figure 18: Comparison of v32 models on traversals quality (2).
Figure 19: BetaVAE traversal response errors.
Figure 20: AVAE traversal response errors.
Figure 21: XVAE traversal response errors.
Figure 22: XCVAE-I traversal response errors.
Figure 23: AE traversal response errors.
Figure 24: XAE traversal response errors.
Figure 25: XCAE-I traversal response errors.
Figure 29: Self-consistency score for each noise variable of the autoencoder family. Results areshown for both the S and Standard model sizes.
Figure 28: Summary of self-consistency for multiple random seeds and model versions. The score isreduced to a single number by averaging across the dimensions. Note that for the variational modelsthe score is obtained by sampling from the prior distribution, while for the AE and SAE familiesposterior sampling is used.
Figure 35: Double intervention matrix on N (left) and Z (right) for a XCAE-I S model on MNIST.
Figure 34: Hybridisation on v32 models representation. The left column corresponds to the basesample, the middle column to the presented alternative and the hybridisation result for each dimen-sion is shown on the right.
