Figure 1: Comparing the DP amplification observed by decreasing batch probability (given by e-t)to the amplification we observe from decreasing Px(1) (given by (1 - t)/t)2We say ”mostly” as this is true upto a point. In particular we have the expectation of the training datasetsize decreases with smaller dataset sampling probabilities, and thus the lowest possible batch sampling rate1/n, where n in the training set size, increases in expectation6Under review as a conference paper at ICLR 20224.2	Usefulness for a DefenderWe now explain one course of action a defender can take in light of this new privacy amplificationfor MI. In particular note that an upper bound on Pχ* (1) translates to an upper bound on the relationfound in Theorem 1 (as the bound is monotonically increasing with Pχ*(1)); hence one can, inpractice, focus on giving smaller upper-bounds on Px* (1) to decrease MI positive accuracy.
Figure 2: Comparing the upper bound to MI performance we achieved to that given by Erlingssonet al. (2019) and Sablayrolles et al. (2019) (note Pχ* ⑴=0.5 here). In particular note We are tighterfor all .
Figure 3: Our upper bound on MI positive accuracy as a function of Px* (1)C TablesPaper	Analytic Form	TyPeYeom etal. (2018)	ee∕2	GeneralErlingsson et al. (2019)	1 — e-e∕2	GeneralSablayrolles et al. (2019)	Px*(I) + e/4	Positive AccuracyOurWork	(1 + e-'Pχ* (0)「 I1 +	Pχ*(i))	Positive AccuracyTable 1: Bounds found in prior work.
Figure 4: Impact of threshold on positive accuracy and accuracy.
Figure 5: Sablayrolles et al. (2019) upper bound on MI positive accuracy as a function of Pχ* ⑴compared to our bound. Note that we are still tighter for all probabilities.
Figure 6: MI advantage13Under review as a conference paper at ICLR 2022□election Capacity vs. Expected Training Set SizeFigure 7: Comparing our deletion capacity trend to the trend Sekhari et al. (2021) describes. Inparticular our number of deletions degrades with training size while theirs increasing.
Figure 7: Comparing our deletion capacity trend to the trend Sekhari et al. (2021) describes. Inparticular our number of deletions degrades with training size while theirs increasing.
