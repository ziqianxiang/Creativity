Figure 1: The SPN structureused to estimate a distribu-tion as a mixture of class-conditional densities. Eachclass-conditional density is inturn represented by a sub-SPN. Classification is donevia Bayes’ rule.
Figure 2: Left: Illustration of our gradient-based approach. Arrow indicates perturbation basedon a gradient step. Right: An example on MNIST. The first row corresponds to the first gradientstep for perturbing the prediction irregardless of the manifold. This perturbation removes somecharacteristics of the current class and adds some characteristics of the counterfactual class. Theresulting sample u yields the desired class but obviously deviates from the training sets (It looksneither like a 1 nor 7). The second row corresponds to the second gradient step for generatingin-distribution counterfactual by pushing the intermediate sample u to a region with higher density.
Figure 3: Counterfactual examples on MNIST across several classes and methods. The top rowindicates the original class and the target class.
Figure 5: Individual illustration of the two gradients used in our approach on two commonly used 2Ddatasets. From left to right, the figures are: the training data for classification task, SPN’s predictionon this set, the contour lines of SPN’s decision boundary, i.e. logS(X|y0) - logS(X|y1), and itsgradient, the contour lines of SPN’s density on X. The orange data points are class y0 and the bluedata points are y1 . Note that each figure on a row are plotted under the same scale, and the gradientin separate figures do not have one-to-one correspondence.
