Figure 1: Left: Traditional multi-task learning framework (that learns shared feature representations)vs. Right: our proposed multi-task oriented generative modeling (that learns a shared generativemodel across various visual perception tasks)(b) Multi-task oriented generative modelingOur MGM addresses this question by proposing a general framework that uses synthesized imagespaired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks.
Figure 2: Architecture of our proposed multi-task oriented generative modeling (MGM) framework.
Figure 3: Joint training of the multi-task network and the image generation network. The multi-tasknetwork provides useful feature representation to guide the image generation process, while thegeneration network refines the shared representation through back-propagation.
Figure 4: Visualization and error comparison of the multi-task prediction outputs in the 50% datasetting. The prediction results of MGM is quite close to the ground-truth, significantly outperformingthe state-of-the-art results.
Figure 5: Performance change with different ratios of weakly labeled data. Joint learning significantlyimproves the performance. The performance of MGM keeps increasing with the number of theweakly labeled synthesized images, achieving results almost comparable to that of MGMr trainedwith all the available WeakIy Iabeled real images._________________________________________________________________Model	SS Q)	DE Q)	SN Q)	ETQ)	ReQ)	PC Q)-ST-	0.120	1.768	0.157	0.228	0.703	0.462MT	0.112	1.747	0.169	0.241	0.704	0.436MGM	0.108	1.715	0.152	0.201	0.699	0.417Table 5: Mean test losses for six tasks on Tiny-Taskonomy. Again, our MGM outperforms thebaselines, indicating its flexibility, generability, and scalability.
Figure 6: More visualizations of the multi-task predictions for MGM and the compared baselines.
Figure 7: Generated images of SAGAN and MGM for the Tiny-Taskonomy dataset. After jointlytraining, the images are not visually realistic, but they are helpful to improve the downstream taskperformance.
Figure 8: Paired RGB and semantic segmentation labels. This paired data indicates that the oracleannotator we used in the pilot study is effective.
