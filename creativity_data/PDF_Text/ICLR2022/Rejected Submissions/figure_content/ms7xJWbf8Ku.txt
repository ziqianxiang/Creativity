Figure 1: Wikipedia BERT pre-training dataset sequence length histograms (token count excludingpadding) for different maximum sequence lengths. Based on the Wikipedia article dump from Oc-tober 1st 2020. The theoretical speed-up relates to not using any padding tokens and not having anyoverhead from processing the different lengths.
Figure 2:	Attention mask code sample [left] and example zero-one mask [right].
Figure 3:	Vectorized unpacking of the sequence loss. White rectangles correspond to padding.
Figure 4: Comparison of learning curves for packed and unpacked processing, where all experimentsconverged to the target accuracy within the same number of training samples(3 million). [left] sameeffective batch size (ebs is batch size times packing factor), [middle] different heuristic adjustmentsof the hyperparameters (batch size 1500 for all runs, such that ebs for packed runs is 1500 * 2), and[right] realized speed-up from packing (in excess of desired 2x).
Figure 5: Comparison of the theoretical speed-up achievable as the number of accelerators is in-creased.
Figure 6: Left: Speed-up from un-padding on 8 GPUs closely resembles a Gumbel distribution.
Figure 7: Comparison of learning curves with and without mask or positional embedding adjustmentin our packed BERT approach. The grey accuracy baseline to reach is 72.1%.
Figure 8: Visualization of the residual of the NNLS packing problemmin k(wA) ∙ x — (Wb)Il2x∈Rms.t. x ≥ 0(18)We should not significantly penalize a deficit in short sequence lengths (smaller than 8 tokens) asadding up to 8 tokens of padding is not much overhead. Similarly, a surplus in long sequences isnot worrisome because the amount of padding needed to achieve a sequence length of 512 is small.
Figure 9: Visualization of the weighted residual of the NNLS packing problem21Under review as a conference paper at ICLR 2022E.5 Discussion of residual weight choiceThis section discusses the choice and effect of the weighting parameters in the NNLSP packingalgorithm. To simplify the problem of selecting reasonable defaults for the residual weights, weuse just two parameters to completely describe the weights: an “offset” parameter and a “weight”parameter. Originally, all sequence length residuals are given the same weight of 1. This resultsin a packing with leftover long sequences, because there are not enough short sequences to packthem with. To reduce the residual on long sequences, we could either increase the residual weighton long sequences or reduce the weight on short sequences. We chose to reduce the weight on shortsequences. Specifically, sequence lengths up to the “offset” length have a reduced “weight”. Theother residual weights stay at 1.
Figure 10: SQuAD 1.1 BERT pre-training dataset sequence length histogram for maximum se-quence length of 384.
Figure 11: GLUE dataset sequence length histograms for maximum sequence length of 128.
Figure 12:	LibriSpeech sequence length histograms of preprocessed audio data [top] as well as targettext data [bottom].
Figure 13:	Abstract length distribution in PubMed.
Figure 14:	Comparison of learning curves for packed and unpacked processing with reduced batchsize for the packed approach.
Figure 15: Comparison of learning curves for packed and unpacked processing with heuristicsapplied.
Figure 16:	Comparison of learning curves for packed and unpacked processing in the optimizedsetup.
