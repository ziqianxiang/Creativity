Figure 1: Speedup/Accuracy of augmenting 30% coresets compared to original max-loss policy for(a) ResNet20 trained on CIFAR10 and (b) ResNet32 trained on SVHN.
Figure 2: Histogram of the singular values of the Jacobian matrix for augmented vs. original (a)MNIST/1 hidden-layer MLP, (b) CIFAR10/ResNet20, and (c) SVHN/ResNet32. (d) Effect of strongvs. weak augmentation on MNIST/MLP at initialization. We sampled 2 and 4 transformations fromrotation, translation, contrast, brightness, etc. for normal vs. strong augmentation.
Figure 3: Training ResNet20 on full data and augmented coresets extracted from CIFAR10. (a)Intersection between elements of coresets of size 30% and maximum loss subsets of the same size.
Figure 4: Accuracy improvement and speedups by augmenting subsets found by our method vs. max-loss and random, over improvement of full data augmentation (F.A.) compared to no augmentation(N.A.). The Figure shows the results for CIFAR10 with ResNet20 in terms of (a) speedup and (b)accuracy, and results for SVHN with ResNet32 in terms of (c) speedup and (d) acuracy.
Figure 6: Intersection between max-loss and coresets in the top N points selected aggregated acrossthe entire training process. Here, we show the increasing overlap between max-loss and coresetpoints as N grows.
Figure 7: Qualitative evaluation of coreset and max-loss points.
Figure 5: Supplementary plots for Figure 3c: Training on coreset and its augmentation compared torandom baseline, measured using test accuracy against percentage of data used on CIFAR10 datasetacross various subset sizes. Accuracy and percentage of data used are measured at every epoch andaveraged over 5 runs.
