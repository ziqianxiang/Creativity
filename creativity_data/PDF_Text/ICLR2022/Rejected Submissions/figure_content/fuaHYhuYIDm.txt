Figure 1: A high-level illustration of MAGNEx. The features of input i are scored by the explainer.
Figure 2: magnex for experiments with text. The input is passed through bert fine-tuned for thetask and then a gru further contextualizes the embeddings before each of them is passed through ashared linear layer to produce a score for each token. bert is not updated during training.
Figure 3: Explanations produced by magnex (a) and lime (b) for a review in the imdb test set.
Figure 4: Explanations produced by magnex (a) and lime (b) for a question (green box) of squadvalidation set. Filtering these to produce binary scores as we do during evaluation creates the reducedinputs i) who wrote about the great pest ##ile ##nce in 1893 ? the historian francis aidan gas ##quetwrote about the ‘ great pest ##ile and ii) who wrote about ##ile ##nce 1893 ? the francis gas ##quetwrote great 1893 suggested “ some adopt black interpretation other roman ##1. Global contextallows magnex to produce comparatively well formed inputs to present to the pre-trained model.
Figure 5: Explanations for a sample image (a) in the mnist dataset with explanations generated bymagnex (b) and lime (c) when explaining a Random Forest.
Figure 6: Explanations for a sample image (a) in the mnist dataset with explanations generated bymagnex (b) and lime (c) when explaining a cnn.
