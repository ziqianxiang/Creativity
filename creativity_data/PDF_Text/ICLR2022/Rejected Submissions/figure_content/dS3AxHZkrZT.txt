Figure 1: Motivation of our orthogonal loss. Two situations are illustrated, where G(Zi) and G(Zj)are a pair of generated pseudo anomalies. Indicated by right formulas, 1) the euclidean-distance-basedloss Ld e.g., dispersion loss (Ngo et al., 2019), stay the same for G(Zi) given di = d2; and 2) theorthogonal loss Lorth penalizes more on G(Zi) given a smaller angle between (G(Zi), G(Zj)).
Figure 2: Experiments conducted on 2D synthetic data where Xn 〜 N((20,20), 3), Za 〜N((0,0), 8), Xa = G(Za). (a) Bad-GAN, (b) Bad-GAN + pull-away term ((ZhaO et al., 2016),an orthogonal regularization without decentralizing samples), (c) Bad-GAN + Lorth, (d) Taichi-GAN(Bad-GAN + Good-GAN + Lorth). The green, red, and blue dots represent real inliers, reall outliers,and generated pseudo outliers, respectively. We use α = 0.9 in Bad-GAN (here real anomalies score1) following the setting in FenCeGAN for MNIST. To illustrate the effect of a Good-GAN, We includethree anomaly samples {(10,10), (12,8), (8,12)} to illustrate the efficacy of angular diversity and aGood-GAN in (d). The bar on the right encodes the anomaly score in color for reference.
Figure 3: (a) Schematic visualization of the proposed method. The upper section illustrates thescenario without anomalies, which targets generating diverse samples. The bottom section includesfew labeled anomalies (red dots), which targets generating samples under the guidance of few labeledanomalies while keeping high diversity. (b) Training (upper) and testing (bottom) framework. Underthe hood of the entire Taichi-GAN framework, the Bad-GAN components are marked in black, whilethe additional Good-GAN components are in red. The discriminator of the Bad-GAN (i.e., Dbd) isused during the inference phase to score the anomalies.
Figure 4: Generated pseudo anomalies among multiple datasets. The first row shows inlier examples.
Figure 5: (a) Different weights of δ on MNIST. (b) Different weights of δ on Fashion-MNIST, δ = 0represents the Good-GAN is disfunCtioned.
Figure 6: Anomaly detection pipeline for BraTS20. The upper left is examples of origin brain slices,i.e., two normal cases (upper) and one anomaly case (lower). Note that only normal cases are usedfor training F-anoGAN. Orange components (upper) are pre-processing steps for creating inputs(processed difference map) for the Taichi-GAN. The post-process includes three steps motivated fromBaur et al. (2018): 1) threshoding with 0.3, 2) Gaussian Blur the image with kernel size 5 × 5 andstandard deviation values [1, 1], 3) median filter with kernel size 5 × 5. The right bottom is inputs ofTaichi-GAN and the left bottom are generated pseudo anomalies by Taichi-GAN. In the inferencestage, the anomaly score of each slice is obtained using Dbd.
