Figure 1: Our algorithm consists of two iterative learning procedures: 1. Get context dynamics{ct-i }tT=0 from the joint simulator and previous policies πoi ld, πo-ldi . 2. Train policy πi with datasampled from the local simulator conditioned on context dynamics {ct-i}tT=0.
Figure 3: Average samples needed bydifferent algorithms in 100-SKUs en-vironment to reach the median perfor-mance of baselines. The lower thevalue, the higher the sample-efficiencyfor the algorithm.
Figure 2: Training curves of different algorithmsin 5-SKUs environment. “IR” means the environ-ment only provides individual rewards and “w/ocontext” denotes the algorithm does not use infor-mation related to shared resource as parts of itsinputs. The X-axis t also takes in samples fromlocal simulation for CD-PPO.
Figure 4: A diagram to illustrate an inventory dynamics in two time steps.
Figure 5: Training curves on N50-C500.
Figure 6: Training curves on N50-C2000G.2 Ablation Studies for Context AugmentationIn this section, we aim to seek a better way to augment the context dynamics in order to boostperformance of our algorithm. We ponder the following two questions:Q1: Which is a better way to augment the original data, adding noise or using a Deep predic-tion model?To answer this question, we set out experiments on environment N5-C100 with our algorithm CD-PPO, using either context trajectories generated by a deep LSTM prediction model, or simply addinga random normal perturbation to the original dynamics trajectories, both on 3 different seeds. Asshown in Figure 7, those runs with deep prediction model generated dynamics enjoy less std and bet-ter final performance. This could result from that the diversity of deep model generated trajectoriessurpasses that of random perturbation.
Figure 7: Training curves of CD-PPO with different augmentation methods24Under review as a conference paper at ICLR 2022Figure 8: Training curves of CD-PPO with varied ratio of augmented dataQ2: Does dynamics augmentation improve the performance of the algorithm? If so, how muchshould we perturb the original data?We run similar experiments on environment N5-C100 with CD-PPO, in which the local simulatoris ingested with a mixture of original dynamics data and LSTM generated data. The ratio of per-turbed dynamics data varies from 0% to 100%. And we found that the algorithm turns out the bestperformance when we use fully generated data, as shown in Figure 8.
Figure 8: Training curves of CD-PPO with varied ratio of augmented dataQ2: Does dynamics augmentation improve the performance of the algorithm? If so, how muchshould we perturb the original data?We run similar experiments on environment N5-C100 with CD-PPO, in which the local simulatoris ingested with a mixture of original dynamics data and LSTM generated data. The ratio of per-turbed dynamics data varies from 0% to 100%. And we found that the algorithm turns out the bestperformance when we use fully generated data, as shown in Figure 8.
