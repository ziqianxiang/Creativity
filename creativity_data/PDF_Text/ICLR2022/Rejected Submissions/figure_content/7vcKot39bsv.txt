Figure 1: The learning curves on CIFAR-10and CIFAR-100. Left: ResNet18 on CIFAR-10.
Figure 2: Adai shows better Generalization inthe comparison with Momentum and Adam un-der similar convergence speed. DenseNet121 onCIFAR-100.
Figure 3: ResNet50 on ImageNet. Left Subfigure: Top 1 Test Error. Right Subfigure: Training Loss.
Figure 4: The test errors of ResNet18 on CIFAR-10 under various learning rates and weight decay.
Figure 5: Flat Minima Selection: Adai ≈ M omentum Adam. The log-scale mean escapetime - log(Γ) with the 95% confidence interval is displayed. We empirically verify that - log(Γ) =O(k-1) holds for Momentum and Adai but does not hold for Adam. Instead, we observe that一 log(Γ) = O(k-2) holds better for Adam. While Adai and Momentum favor flat minima similarly,Adai may escape loss valleys slightly faster than Momentum.
Figure 6: We verified Equation (3) by training pretrained and random three-layer fully-connectednetworks on MNIST (LeCun, 1998). Top Row: Pretrained Models. Bottom Row: Random Models.
Figure 7: The Stochastic Gradient Noise Analysis (Xie et al., 2021b). The histogram of the normof the gradient noises computed with ResNet18 on CIFAR-10. Subfigure (a) follows Simsekli et al.
Figure 8: The illustration of Kramers Escape Problems (Xie et al., 2021b). Assume there are twovalleys, Sharp Valley a1 and Flat Valley a2 . Also Col b is the boundary between two valleys. a1 andaa are minima of two neighboring valleys. b is the saddle point separating the two valleys. c locatesoutside of Valley a1.
Figure 9: The test errors of ResNet18 on CIFAR-10 and VGG16 on CIFAR-10 under various weightdecay. Left: ResNet18. Right: VGG16. The optimal test performance corresponds to λ = 0.0005.
Figure 10: Generalization and Convergence Comparison. Subfigures (a)-(b): ResNet18 and VGG16on CIFAR-10. Subfigures (c)-(d): DenseNet121 and GoogLeNet on CIFAR-100. Top Row: Testcurves. Bottom Row: Training curves. Adai with η = 1 and η = 0.5 converge similarly fast to SGDwith Momentum and Adam, respectively, and Adai generalizes significantly better than SGD withMomentum and Adam.
Figure 11: Convergence comparison by training VGG16 on CIFAR-10 for 1000 epochs with the fixedlearning rate. When they converge similarly fast, Adai converges in a lower training loss in the end.
Figure 12:	Language Modeling. The learning curves of Adai, SGD (with Momentum), and Adam forLSTM on Penn TreeBank. The optimal test perplexity of Adai, SGD, and Adam are 74.3, 74.9, and74.3, respectively. Adai and Adam outperform SGD, while Adai may lead to a lower training lossthan Adam and SGD.
Figure 13:	The expected minima sharpness analysis of the weight-perturbed training loss landscapeof ResNet18 on CIFAR-10. The weight noise scale is the standard deviation of the injected Gaussiannoise. The minima learned by Adai and SGD are more robust to weight noise. Obviously, Adai andMomentum can learn much flatter minima than Adam in terms of the expected minima sharpness.
