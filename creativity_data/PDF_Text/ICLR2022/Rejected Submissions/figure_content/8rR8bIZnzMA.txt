Figure 1: Overview of using DGT for link prediction. Given snapshot graphs {Gι, G2} as input,⑴we first generate the temporal union graph with the considered max shortest path distance DmaX = 5,and its associated (2) temporal connection encoding and (3) spatial distance encoding. Then, theencodings are mapped into ATC, ASD for each node pairs (i, j) using a fully connected layer. Topredict whether an edge exists 由 future graph G3, we first (4) sample target nodes and context nodes,and then apply (5) DGT to encode target nodes and context nodes separately.
Figure 2: Overview of the pre-training. Given snapshot graphs {Gι, G2} as input, We first generatethe temporal union graph. Then, We sample the target node Vtgt and two different set of context nodesVctx, Vctx. After that, we apply DGT on {Vtgt, Vctx} and {Vtgt, Vctx} to output HL))and H(L). Tothis end, we optimize Lview(Θ) by maximizing the similarity between H(gJ and H^t), and optimizeLreCon(Θ) by recovering snapshot graphs using H(L).
Figure 3: Comparison of the Micro - and Macro-AUC score of DGT with and without pre-training.
Figure 4: Comparison of DGT with baselines across multiple time steps, where the Macro-AUCscore is reported in the box next to the curves.
Figure 5:	Comparison of DGT with baselines across multiple time steps, where the Macro-AUCscore is reported in the box next to the curvesA.4 Results on noisy datasetIn this section, we study the effect of noisy input on the performance of DGT using UCI and Yelpdatasets. We achieve this by randomly selecting 10%, 20%, 50% of the node pairs and changingtheir connection status either from connected to not-connected or from not-connected to connected.
Figure 6:	Relationship between entropy and mutual information (Figure 2.2 of Cover & Thomas(2006)).
Figure 7: Suppose we want to compute the pairwise self-attention between nodes {v1,...,v5 }. Wecan first split all self-attentions into multiple chunks, and iteratively compute the self-attention valuein each chunk. For example, at the first iteration, We first compute the self attention between node{vι,..., v3} (in blue) and store it in memory (in yellow). Then, at the second iteration, We computethe attention between node {v1,...,v3} and node {v4, v5}.
