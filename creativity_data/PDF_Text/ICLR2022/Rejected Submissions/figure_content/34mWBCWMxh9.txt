Figure 1: Spatial smoothing improvesboth accuracy and uncertainty (NLL).
Figure 2: Comparison of three different Bayesian neural network inferences: canonical BNNinference, temporal smoothing (Park et al., 2021), and spatial smoothing (ours). In this figure, x0 isobserved data, Pi is predictions p(y|xo, Wi) or p(y∣Xi, Wi), ∏i is importances ∏(x∕xo), and N isensemble size.
Figure 3: Spatial smoothing improves both accuracy and uncertainty across a whole range ofensemble sizes. We report the predictive performance of ResNet-18 on CIFAR-100.
Figure 4: Stages of CNNs such as ResNet (left) and thestages incorporating spatial smoothing layer (right).
Figure 5: Spatial smoothing layers reduce feature map variances, suggesting that they ensemblefeature map points. We provide standard deviation of feature maps by block depth with ResNet-50 onCIFAR-100. c1 to c4 and s1 to s4 each stand for stages and spatial smoothing layers, respectively.
Figure 6: MC dropout adds high-frequency noises, and spatial smoothing filters high-frequencysignals. In these experiments, we use ResNet-50 for ImageNet. Left: Frequency mask Mf withw = 0.1π. Middle: Diagonal components of Fourier transformed feature maps at the end of thestage 1. Right: The accuracy against frequency-based random noise. ResNets are vulnerable tohigh-frequency noises. Spatial smoothing improves the robustness against high-frequency noises.
Figure 8: Both GAP and spatial smoothing smoothen the loss landscapes. To demonstrate this,we present the loss landscape visualizations of ResNet-18 models with MC dropout on CIFAR-100.
Figure 7: Both GAP and spa-tial smoothing suppress largeHessian eigenvalue outliers,i.e., they flatten the loss land-scapes. Compare with Fig. 8.
Figure 9: Spatial smoothing also improves predictive performance on large datasets. We reportpredictive performance of ResNet-50 on ImageNet.
Figure 10: Spatial smoothingimproves the robustness. SeeFig. E.3 for more details.
