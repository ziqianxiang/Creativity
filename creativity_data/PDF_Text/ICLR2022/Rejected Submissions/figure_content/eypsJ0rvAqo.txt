Figure 1: LAMB’s scaling coefficients (ct(l) in (1)) and variance norms for different layers in the firstBertLayer during BERT-Large pre-training seqlen 128 (5993 steps in total). We set the lower/upperbound of the scaling coefficient (cmin and cmax in (1)) at 0.01 and 0.3.
Figure 2: Sample-wise convergence speed for BERT-Large pre-training with LAMB and 1-bit LAMB.
Figure 3: Scalability of 1-bit LAMB with NCCL-based backend for BERT-Large pre-training onV100 GPUs. LAMB lines represent the throughput at 1-bit LAMB’s warmup stage (i.e., baselineLAMB). 1-bit LAMB lines represent the throughput at compression stage. Annotations representthe highest speedup achieved in each figure. Note that this is the speedup between warmup andcompression stage. The end-to-end speedup also depends on the percentage of warmup. All figuresshare the same legend as 3(g).
Figure 4:	LAMB’s scaling coefficients (ct(l) in (1)) for different layers during BERT-Large pre-trainingseqlen 128 (5993 steps in total). Since all 24 BertLayer have similar patterns, we just present the firstone. We set the lower/upper bound of the scaling coefficient (cmin and cmax in (1)) at 0.01 and 0.3.
Figure 5:	LAMB’s variance norms for different layers during BERT-Large pre-training seqlen 128.
Figure 6: 1-bit LAMB scaling ratios (rt(l) in Algorithm 1) for different layers during BERT pre-training sequence length 128. Since all 24 BertLayer have similar patterns, we just present the firstone. The number of warmup steps is 1K out of 5993 total steps. We set the clipping configurationsof the scaling ratio (rmin, rmax, rthreshold in Algorithm 1) at 0.5, 4.0, 0.1.
Figure 7: Comparing MPI and NCCL-based compressed communication backend implementationsbased on performance of BERT pre-training seqlen 128.
Figure 8: Sample-wise training loss and testing accuracy for ResNet-50 on CIFAR100.
