Figure 1: Our drawing agent can accomplish four different tasks.
Figure 2: Generator model. At each time step, the policy networkreceives a canvas and outputs two distributions for Bezier curve pa-rameters and stop/continue decision. When the ‘continue’ decision issampled, the resulting stroke is rendered and added to the final output.
Figure 3: Quality of generated MNIST characters as training progresses (i.e. policy is updated) fromleft to right.
Figure 4: Omniglot unconditional samples. For randomly sampled generations, four closest samples(in terms of pixelwise L2 distance) from the training dataset are presented.
Figure 5: Randomly sampled uncon-ditional generations for the Omniglotdataset.
Figure 6: MNIST reconstructions. For each sample on the left hand side of the columns, parsingprocesses are demonstrated. Colors represent the order of the strokes. (pink: first stroke, green:second stroke, blue: third stroke)Figure 7: Omniglot reconstruction. For each sample on the left hand side of the columns, resultingreconstructions are demonstrated.
Figure 7: Omniglot reconstruction. For each sample on the left hand side of the columns, resultingreconstructions are demonstrated.
Figure 8:	New exemplar generation. Given an unseen character from a new alphabet (highlighted inred boxes), the model generated 9 exemplars.
Figure 9:	Novel sample generation conditioned on a type. Given 10 characters from an alphabet,our model produced 20 new samples.
Figure 10: FID values for unconditional generations of Omniglot dataset throughout the trainingprocess. The experiment is repeated over 3 seeds.
Figure 11:	LPIPS values for each alphabet in the test set calculated from sampled exemplars (a),SSIM and L2 values for each alphabet in the test set calculated from sampled exemplars (b).
Figure 12:	LPIPS values for each alphabet in the test set calculated from novel samples produced(a), L2-SSIM values for each alphabet in the test set calculated from novel samples produced (b).
