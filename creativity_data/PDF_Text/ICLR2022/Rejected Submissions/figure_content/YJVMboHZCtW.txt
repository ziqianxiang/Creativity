Figure 1: Algorithm decision boundary variability on CIFAR-10 and CIFAR-100. (a) Examples offake CIFAR-10 images generated by conditional BigGAN. (b) Examples of fake CIFAR-100 imagesgenerated by conditional BigGAN. (c) Scatter plots of algorithm DB variability to accuracy on testset with different architectures and training strategies on CIFAR-10 and CIFAR-100. The colors ofblue, red, and yellow points denote the architectures of VGG-16 (VGG), ResNet-18 (ResN), andWideResNet-28 (WRN), respectively. The shapes of •, N, and designate the training strategies ofstandard training (S), non-data-augmentation training (N), and adversarial training (A), respectively.
Figure 2: (a) Plots of algorithm DB variability and test error as functions of training time (LR islearning rate). (b) Scatter plots of test error to algorithm DB variability (LR is learning rate). Thepoints are collected from different epochs. Each curve and point is calculated and then averaged on10 trials.
Figure 3: (a) Plots of algorithm DB variability and test error as functions of training sample sizeon CIFAR-10 and CIFAR-100. (b) Plots of algorithm DB variability and test error as functions oftraining time with the existence of 20% label noise on CIFAR-10 and CIFAR-100. Each curve iscalculated and then averaged on 10 trials.
Figure 4: (a) The η- curves on CIFAR-10 with different training sample sizes 2000 (m2000), 2000(m2000), 10000 (m10000), 20000 (m20000), and 50000 (m50000), respectively. (b) The η- curves onCIFAR-100 with different training sample sizes. (c) The schematic diagram of the η- curves w.r.t.
Figure 5: (a) Plots of test error as a function of training time (LR is learning rate). (b) Plots ofalgorithm DB variability as a function of training time (LR is learning rate). Each curve is calculatedand then averaged on 10 trials.
