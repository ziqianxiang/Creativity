Figure 1: The VUT model architecture contains two Transformer models, which take image, structureand language input, and three task heads for achieving five distinct UI modeling tasks.
Figure 2: The distribution of the 21 UI elements at the log10 scale. The proportion that each type isused as an inner versus a leaf node is shown within each bar.
Figure 11: Examples for the UI Object Detection task. In the ground-truth screens, the boundingboxes of inner objects are highlighted in orange and those of leaf objects are shown in blue. In thepredictions, we render the bounding boxes of predicted objects as inner (orange) versus leaf (blue)based on the dominant use of the predicted object type, according to Figure 2.
Figure 15: Examples for the Language Command Grounding task. The object located by the modelis highlighted with a blue bounding box in each screenshot.
Figure 19: Examples for the Screen Summarization task. We here display one of the 5 references(ground-truth summaries) created by human annotators for each screen.
Figure 23: Examples for the Widget Captioning task. The target element is highlighted via a bluebounding box. We here show one of the three references (ground-truth captions) created by humanannotators for each target element.
Figure 27: Examples for the Tappability Prediction task. The questioned element is highlighted witha blue bounding box.
