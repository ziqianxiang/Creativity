Figure 1: The training pipeline of OCN. Our model is composed of a controller (circle) and a optionspool (rectangles). The controller and options are randomly initialized, which means each optiondoes not correspond to a meaningful subtask. After behavior cloning, both options and controllersare induced (marked blue) and the options correspond to meaningful subtasks from demonstrations(e.g., get wood). Then we freeze the parameters in the options and re-initialize the controller. Thecontroller is trained to adapt to the new environment with HRL (marked red).
Figure 2: An example of OCN. The controller c models the task make bridge. Three optionsseparately model subtasks get iron, get wood or make at factory.
Figure 3: The three different phase of OCN: (a) At the first time step, the controller selects an optionoi; The option oi outputs the first action a1. (b) If the previous option oi predict that the subtask isnot finish; The option oi then continue outputs action at; The controller hidden state is copied fromprevious time step. (c) If the previous option oi predict that the subtask is done; The controller thenselects a new option oj and updates the controller hidden state; The new option oj outputs actionat . Blue arrows represent probability distributions output by controller and options. Red arrowsrepresent recurrent hidden states between time steps.
Figure 4: The learning curve of different methods on three finetuning tasks of S1. dense meansdense reward setting. sparse means sparse reward setting.
Figure 5: The learning curve of different methods on three finetuning tasks of S2. OMPN is notincluded because it does not learn an explicit set of options.
Figure 6: The learning curve of different methods on three finetuning tasks of Dial. dense meansdense reward setting. sparse means sparse reward setting.
Figure 7: Comparison of unsupervised trajectory parsing results during the imitation phase withOMPN (Lu et al., 2021). The F1 score with tolerance (Left) and Task Alignment (Center) show thequality of learned task boundaries. The normalized mutual information (Right) between the emergedoption selection ptc and the ground-truth shows that OCN learns to associate each option to onesubtask. T=1 means that the temperature term in the controller is removed.
Figure 8: Comparison of parsing results during different K at F1 scores with tolerance, task alignaccuracy and NMI.
Figure 9: Comparison of prediction accuracy of actions and the returns during different K13Under review as a conference paper at ICLR 2022F I F I I IEnnaM EmhM EannM, Iron Grass# GoldWoodA Agentswitchenvoption1option2option3action■Zii 7 ⅛B Bzib 7 ≡l Bz∣ι ♦ ≡∣LM	匚Hq工	τ≡1 次 τ⅞ 工 次 TW/ Iron Grass# Gold
Figure 10: Visualizations of different tasks in S1. Different colored lines correspond to differentsubtasks.
Figure 11: Visualization of a task in S2.
