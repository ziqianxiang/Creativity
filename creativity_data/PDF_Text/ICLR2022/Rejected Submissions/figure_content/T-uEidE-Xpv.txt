Figure 1: (a): In contrastive instance learning, the features from different transformations of the same inputimage are compared to each other. (b): However BNN can yield the binary activations AB and full-precisionactivations AF (i.e. two transformations of an image both from the same BNN) in the same forward pass, thusthe BNN can act as two image transformations in the literature of contrastive learning.
Figure 2: Feeding two images into a BNN, and obtaining the three pairs of binary and full-precision activations.
Figure 3: t-SNE [26] visualization of the activations representing for random 10 classes in CIFAR-100. Everycolor represents a different class. We can clearly witness the improvement of our method for learning betterbinary representations.
Figure 4: In-depth analysis on different aspects of the proposed approach including a comparison of learnedcorrelation maps from different methods (a-d), the effect of number of negative samples in contrastive mutualinformation maximization (c-f), training and testing curves (g), and a comparison of fine-tuning results (h).
Figure 5: Results of depth estimation and segmentation.
