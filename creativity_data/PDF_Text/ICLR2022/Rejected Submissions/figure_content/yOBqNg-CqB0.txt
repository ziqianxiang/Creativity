Figure 1: Neither pair of textshas common words. WMDcan choose a similar sen-tence appropriately, whereasthe BOW distance cannot dis-tinguish these cases.
Figure 2: kNN classification errors reported in the original WMD paper (Figure 3 in (Kusner et al.,2015)). Lower is better. WMD outperformed the naive baselines by significant margins.
Figure 3: kNN classification errors in our re-evaluation. Lower is better. The shaded bars are theperformance without normalization. WMD is comparable to classical baselines with normalization.
Figure 4: Histograms of distances between matched word embeddings. (Top) 300-dimensionalembeddings. (Bottom) 5-dimensional embeddings.
Figure 5: Scatter plot of WMD and L1/L1 BOW. (Top) 300-dimensional embeddings. WMD ismostly determined by L1/L1 BOW. (Bottom) 5-dimensional embeddings.
