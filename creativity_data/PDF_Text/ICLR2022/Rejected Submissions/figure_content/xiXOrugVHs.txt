Figure 1: An overview of the top-down transformer. Suppose a document with 7 tokens is the inputs to themodel, as shown on the bottom left. The bottom-up inference is achieved with local self-attention (N1 layers) asshown in the left panel. To initialize the top-level representations, we pool bottom-up-inferred token represen-tations with either equal weights or adaptive weights (see Section 2.3 for details). Top-level representations arethen updated with full self-attention (N2 layers) to capture global context. They are then used to update bottom-up-inferred token representations, accounting for the top-down update for token representations, as shown inthe middle panel. The final token representations are attended by the decoder to generate a summary. Note thatinference is used in the sense of statistical inference for latent variables and does not imply no training.
Figure 2: An illustration of local self-attention. It illustrates local self-attention of 9 tokens with window size4. Each token attends 2 tokens on the left and 2 tokens on the right, as long as there are sufficient right and leftneighbors. The attended nearby tokens are in light green. Each token also attends itself, as indicated by darkgreen. White color indicates absence of attention.
Figure 3: Ablation on attention window size with PubMed. The window sizes tested are 32, 64, 128, 256, 512,1024.
