Figure 1: Illustration of the advantages with explicit orthogonal low-rank geometry regularization.
Figure 2: An illustration of the proposed meta-learned orthogonal low-rank embedding. The inputimages of both support set (blue, yellow, and green boxes) and query set (red box) are all firstmapped to feature vectors by a universal feature extractor, where an orthogonal low-rank geometry isimposed. The features at each task then go through the adaptive orthogonal low-rank transformation,whose parameters are adapted by samples of each task, and achieve higher intra-class similarity andinter-class orthogonality. Finally, an adaptive subspace projection is used for each class, where theprojection matrices are inferred directly in a closed form.
Figure 3: Moving average of the accuracyat different p when updating the adaptiveorthogonal low-rank transformation.
Figure 4: Visualization of the feature space of Zp while updating θ in Ψ for 10 iterations. Featurevectors from three classes in a 5-way FSL task are embedded with PCA, and visualized in threecolors. Viewing angles are adjusted for better visualizations. The value of the OLE loss and theaccuracy at each iteration are noted in the figure.
