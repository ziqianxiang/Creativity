Figure 1: Attention matrices from the hand-coded Transformer implementing binary addition withn = 8.
Figure 2:	Attention matrices from the hand-coded vs. trained Transformer implementing binaryaddition with n = 8.
Figure 3:	Test set accuracy ofa 1-layer 1-head Transformer on binary addition ofupto n-bit numberswith different positional encodings.
Figure 4: Test set accuracy ofa 1-layer 1-head Transformer on binary addition ofupto n-bit numberswith different training set sizes.
