Figure 1: Locality-based mini batching. The primary (e.g. training) nodes are indicated bypentagons. These nodes are first partitioned into batches, e.g. using graph partitioning. Wethen use local clustering to select the auxiliary nodes of each batch, e.g. the neighbors withtop-k personalized PageRank (PPR) scores. Finally, we generate a batch using the inducedsubgraph of all selected nodes, but only calculate the outputs of the primary nodes we chosewhen partitioning. Batches can overlap and do not need to cover the whole graph.
Figure 2: Convergence of validation accuracy in log. time. Average and 95 % confidenceinterval of 10 runs. LBMB converges the fastest in 8 of 9 cases.
Figure 3: Training convergence in log. time for GCN on ogbn-products with smaller trainingsets. The gap in convergence speed between LBMB and the baselines grows larger for smalltraining sets, since LBMB scales with training set size and not with overall graph size.
Figure 4: Test accuracy and log. inference time for a fixed GNN. LBMB consistently providesthe best accuracy versus time trade-off (top-left corner).
Figure 5: Convergence per epoch. LBMB convergesperforming any sampling.
Figure 6: Training convergence per epoch for smaller training sets. All methods convergesimilarly fast per training step, demonstrating once again the importance of a fast time pertraining step.
Figure 7: Convergence per Figure 8: Convergence per Figure 9: Batch scheduling fortime for training GCN on epoch for batching methods GAT on ogbn-arxiv. Optimalogbn-arxiv. Both graph par- when training GCN on ogbn- batch order prevents down-titioning and PPR batching arxiv. PPR-based partition- ward spikes in accuracy andlead to faster convergence ing also converges the fastest leads to higher final accuracy.
Figure 10: Final test accuracy(LBMB inference) when train-ing LBMB with PPR batchingwith different numbers of pri-mary nodes per batch (GCN onogbn-arxiv). LBMB is ratherinsensitive to this choice.
