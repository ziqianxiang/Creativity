Figure 1: Despite being equal-size and class-balanced, the high-quality half and low-quality half ofthe CIFAR-10 training set have drastically different behaviors in adversarial training.
Figure 2: Correlation between the rankings of the examples (scaled by the total number of examples)based on learning stability estimated by different training settings. Spearman’s rank coefficient (ρ) isannotated at the upper left of each subfigure. Here we consider adversarial training methods includingPGD-AT, TRADES and MART (Wang et al., 2020), and neural architectures including pre-activationResNet-18, VGG-19 (Simonyan & Zisserman, 2015) and WRN-16-10.
Figure 3: Examples selected from CIFAR-10 training set by learning stability. The top row showsthe original images annotated with their true labels and the bottom row shows the correspondingadversarially-perturbed images annotated with their labels predicted by a model.
Figure 4: Best robust test accuracy obtained when removing different fractions of training exampleseither randomly or in an ascending order of data quality.
Figure 5: Robust test accuracy obtained at the best and last checkpoints by adversarial training whenremoving training examples either randomly or in an ascending order of data quality. The robustoverfitting can be captured by the visual gap between the dashed and solid curves of the same color.
Figure 6: Best robust test accuracy obtained and evaluated against PGD attack and AA when removingtraining examples either randomly or in an ascending order of data quality. The overestimation gapcan be captured by the visual gap between the dashed and solid curves of the same color.
Figure 7: Standard test accuracy obtained by standard training and adversarial training when removingdifferent fractions of training examples either randomly or in ascending order of data quality. Thecross-generalization gap can be captured by the visual gap between the dashed and solid curves ofthe same color.
Figure 8: Correlation between data quality and three problems in adversarial training shown byconducting controlled experiments on WRN-28-10.
Figure 10: The distribution of learning stability given different training epochs.
Figure 11: Correlation between data quality rank and other measurementsHere we show that the prediction probability of an example is correlated with its data quality rank.
Figure 12: Scatter plots of the quality ranks of training examples based on prediction probabilitiesobtained by different training settings. The prediction probability of an example is consistent acrossrandom initializations, different methods and models.
Figure 13:	Scatter plots of the quality ranks of training examples based on minimum perturbationsobtained by different training settings. The minimum perturbation of an example is consistent acrossrandom initializations, different methods and models.
Figure 14:	Scatter plots of the quality ranks of training examples based on 1st learned epochs obtainedby different training settings. The 1st learned epoch refers to the first epoch when an exampleis classified correctly under adversarial attack, which is consistent across random initializations,different methods and models.
Figure 15: Fine tune the model obtained by standard training with adversarial examples generated bytargeted attack on either all the training examples or the high-quality subset among them. When theattack in adversarial training is primarily targeted to “Airplane”, the test accuracy of “Airplane” afterfine-tuning is significantly lower, while the test accuracies of other classes are comparable. Instead,when the attack is targeted to “Automobile”, the test accuracy of “Automobile” after fine-tuning issignificantly lower. In contrast, when fine tuning the model only on the high-quality subset, suchdifference is not significant.
Figure 16: Fine tune the best model obtained in adversarial training with targeted attack on eitherall the training examples or the high-quality subset among them. When the attack in adversarialtraining is primarily targeted to “Airplane”, the robust test accuracy of “Airplane” after fine-tuning issignificantly lower, while the performance of other classes are comparable. We can observe similarpatterns when the attack is targeted to “Automobile” or “ship”. In contrast, when fine tuning themodel only on the high-quality subset, the difference is insignificant.
Figure 17: The overestimation gap, namely the difference between PGD-10 and Auto Attack evalua-tion, generated by adding examples of two competing classes into a high-quality subset.
Figure 18: The overestimation gap generated by adding examples of two non-competing classes intoa high-quality subset.
