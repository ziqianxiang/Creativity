Figure 1: Kernel regression on synthetic data. We simulate data from a nonlinear regression modelwith Gaussian additive noise and fit kernel regression using a polynomial kernel. We run both SGDand GD for two step size schemes, see details in Appendix H.1. In the first plot, we show thedirectional bias by Rayleigh Quotient(RQ):= kKb22 (same as Theorem 5 and 7). The SGD indeedkbk2converges in the direction of a larger RQ, which matches our Theorems 5 and 7. In the third plot weshow the prediction error of the solution paths, and the SGD does have lower prediction error thanGD, even GD has smaller training loss than SGD. This supports Theorem 11.
Figure 2: The experiment of a small ResNet-like Neural Network on FashionMNIST. In (a), wefollow Wu et al. (2021) to use the Relative Rayleigh Quotient(RRQ) as the measurement of theconvergence direction, where higher RRQ means that the convergence direction is closer to the largereigenvalue direction of the Hessian. The SGD with moderate step size has higher RRQ than the GDwith either moderate step size or small step size, which supports the theory in Theorems 5 and 7. Itisalso interesting to observe SGD with a small step size also has a different directional bias comparedwith SGD with a moderate step size. In (b), we plot the testing accuracy from 20 repetitions ofexperiments, the test accuracy (inside bracket) of SGD with moderate step size is higher than theother cases, and we have Wilcoxon signed-rank test to confirm that the difference is significant at0.01 level. The test accuracy validates Theorem 11. For more details of the experiments, the ranktest, and more experiments, see Appendix H.2.
Figure 3: Use more step sizes in SGD/GD. The test accuracy is evaluated once every 500 iterations,and inside the bracket is the average of the last 5 test accuracy values.
