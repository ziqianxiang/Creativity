Figure 1: The comparison between the model discrepancy increase factor δι and the communicationcost decrease factor 1 - λι for a) CIFAR-10 and b) CIFAR-100 training.
Figure 2: The number of communications at the individual layers. The communications are countedduring the whole training (non-IID data).
Figure 3: The total data size (communication cost) that correspond to Figure 2. The data sizecomparison clearly shows where the performance gain OfFedLAMA comes from.
Figure 4: The learning curves of CIFAR-10 (ResNet20) training (128 clients). a): The curves forIID data distribution. b): The curves for non-IID data distribution (α = 0.1). FedAvg (x) indicatesFedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of xand the interval increase factor ofy. As the aggregation interval increases, FedAvg rapidly loses theconvergence speed, and it results in achieving a lower validation accuracy within the fixed iterationbudget. In contrast, FedLAMA effectively increases the aggregation interval while maintaining theconvergence speed.
Figure 5: The learning curves of CIFAR-100 (WideResNet28-10) training (128 clients). a): Thecurves for IID data distribution. b): The curves for non-IID data distribution (α = 0.1). FedAvg(x) indicates FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the baseinterval of x and the interval increase factor ofy. While FedAvg significantly loses the convergencespeed as the aggregation interval increases, FedLAMA has a marginl impact on it which results in ahigher validation accuracy.
Figure 6: The learning curves of FEMNIST (CNN) training. FedAvg (x) indicates FedAvg withthe interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of X and the intervalincrease factor of y. FedLAMA curves are not strongly affected by the increased aggregation intervalwhile FedAvg significantly loses the convergence speed as well as the validation accuracy.
