Figure 1: Hypothetical images from the MNIST dataset(b) P(7) = 51%tion stays in the same. In the second scenario the the output of the network for class 7 decreasesfrom 85% to 45%, and results in the classification changing from 7 to 9. When considering the twodefinitions, a small change in the output leads to no change in the classification and a large changein the output leads to a change in classification and so robustness and classification robustness bothagree with each other.
Figure 2: Experiments that show how the two networks trained with LR and SR constraints performevaluated against different definitions of robustness underlying the attack.
Figure 3: Experiments that show how different choices of a constraint loss affect standard robustness of neuralnetworks, for PGD attacks with various values.
Figure 4: Experiments that show how adversarial training, training with data augmentation, and training withconstraint loss affect standard robustness of neural networks, for PGD attacks with various values.
Figure 7: Experiments that show how adversarial training, training with data augmentation, and training withconstraint loss affect strong classification robustness of neural networks, for varying sizes of the PGD attack(measured by values).
Figure 8: Experiments that show how different choices of a constraint loss affect standard robustness of neuralnetworks, for varying sizes of the PGD attack (measured by values).
Figure 9: Experiments that show how different choices of a constraint loss affect lipschitz robustness of neuralnetworks, for varying sizes of the PGD attack (measured by values).
Figure 10: Experiments that show how different choices of a constraint loss affect strong classification robust-ness of neural networks, for varying sizes of the PGD attack (measured by values).
Figure 11: Experiments that show a comparison of how training with data augmentation, and training withconstraint loss affect strong classification robustness of neural networks, for varying sizes of the PGD attack(measured by values) of our standard network with a bigger architecture.
Figure 12: Experiments that show how the networks trained with Adversarial Training perform when evalu-ated against different definitions of robustness underlying the attack.
Figure 13: Experiments that show how the networks trained with SR constraints perform when evaluatedagainst different definitions of robustness underlying the attack.
Figure 14: Experiments that show how the networks trained with LR constraints perform when evaluatedagainst different definitions of robustness underlying the attack.
