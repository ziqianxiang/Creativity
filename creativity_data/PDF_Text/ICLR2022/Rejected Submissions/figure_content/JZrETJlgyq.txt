Figure 1: The overall framework of the proposed NCC in an EM framework.
Figure 2: Illustration of the proposed techniques compared to negative sampling. (a) Negative sam-pling in contrastive learning giving rise to class collision issue. (b) The proposed positive samplingencouraging the alignment between neighbors of one view with another one. (c) The proposed pro-totypical contrastive loss encouraging prototypical alignment and prototypical uniformity.
Figure 3: Visualization of feature representations learned by NCC on CIFAR-10 with t-SNE. Dif-ferent colors denote the different semantic classes. Zoom in for better view.
Figure 4: Visualization of feature representations learned by different representation learning frame-works and our proposed NCC on CIFAR-10 with t-SNE. Zoom in for better view.
Figure A1: Visualizations of NMIs and STDs by applying PCL (Li et al., 2021a) to MoCo andBYOL on CIFAR-10. Compared to PCL+BYOL, PCL+MoCo performs more stable during cluster-ing with a more uniform distribution of representations. The decreasing STDs (standard deviationof '2-normalized features) also indicate that PCL+BYOL suffers from the representation collapse.
Figure A2: The effect of the hyperparameter r. NCC performs K-means clustering for every r epoch.
Figure A3: The effect of the hyperparameter σ for positive sampling. Taking a look at σ 〜[0,10-3],although introducing the positive sampling into BYOL causes a slight drop on CIFAR-10, the clus-tering performance becomes more stable as evidenced by the standard deviation. This is because theneighbors of one sample are regarded as positive examples. Besides, the performance for CIFAR-20has increased over baseline BYOL with the standard deviation reduced. These results indicate thatpositive sampling can improve the stability of BYOL. However, when σ is too large, the performancebecomes unstable and drops a lot. It is not surprised since during positive sampling with large σ, theinstances from other clusters could be sampled and regarded as positive examples. Therefore, wesuggest setting σ to a small value, saying (0, 10-3].
Figure A4: The effect of the hyperparameter λpci for the ProtoCL. The results suggest that NCCis robust to different loss weights of ProtoCL loss on CIFAR-20. However, the higher loss weightleads to instability on CIFAR-10. The possible reason is that CIFAR-20 is more diverse and hasmore semantic classes than CIFAR-10 (100 versus 10). Anyway, We suggest that the loss weightfor ProtoCL loss can be set to [0.01,0.1] for different situations, which has demonstrated superiorperformance on both two datasets.
Figure A5: Visualizations for the outlier points produced by the model at 1000-th epoch on CIFAR-10 with t-SNE.
Figure A6: The effect of the predefined number of clusters K . We reported NMIs following (Caronet al., 2018). We note that the predefined K of NCC during the training of NCC is the same asthe K in k-means clustering process for evaluation. To further demonstrate the influences of over-clustering, we also reported the results of vanilla BYOL during k-means clustering. The resultsdemonstrate that both BYOL and NCC have the same over-clustering behavior on these two datasets;that is, opposite trends on these two datasets. Specifically, over-clustering leads to the performancedrop for CIFAR-10, but it leads to performance increase for CIFAR-20. However, our NCC canstill produce large improvements over BYOL with the same predefined number of clusters. We notethat the opposite results are due to the significant difference between these two datasets. Althoughhaving the same number of samples, CIFAR-10 has 10 distinct classes while CIFAR-20 has, in fact,100 classes but uses 20 super-classes instead. If the representations are well aligned within the samesemantic clusters, the over-clustering would try to destroy the structures of the clusters and push thesemantically similar examples away, which certainly compromises the clustering performance.
Figure A7: Visualization of training and cluster statistics for BYOL and NCC: a) normalized mutual in-formation (NMI)1 between the clustering results and ground-truth labels; b) standard deviation (STD)2of `2 -normalized features to evaluate the uniformity, where the higher indicates the more uniform rep-resentations; c) cluster imbalance ratio3 computed by Nmin /Nmax, where N is the number of samplesin each class; and d) cluster statistics, or the sorted number of samples in each cluster for the modelat 1000-th epoch on CIFAR-10. Taking a look at NMIs and STDs during the training stage in (a) and(b), NCC produces higher NMIs with stable and higher STDs, while BYOL performs unstable with theSTDs gradually decreased. The results indicate that NCC yields a more uniform representation spacewith the samples well-clustered. On the other hand, the uniform representations of NCC can also avoidthe collapse of k-means clustering at the same time. As shown in (c), the k-means clustering process ofNCC produces more balanced clusters with a higher cluster imbalance ratio. On the contrary, the clus-ters of BYOL are highly imbalanced, which is consistent with the unstable NMIs and decreasing STDs.
Figure A8: The effect of the different projection dimension. NCC achieves consistent and significantperformance improvement over BYOL regardless of different projection dimension.
Figure A9: The effect of the different data augmentation. It is not surprised to see the performancedrops for both BYOL and NCC when removing some data augmentations. On the contrary, theclustering results suggest that NCC still performs more stable and is robust to data augmentations.
