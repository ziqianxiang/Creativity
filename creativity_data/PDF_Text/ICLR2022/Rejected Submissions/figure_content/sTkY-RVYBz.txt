Figure 1: The Counterbalancing Teacher (CT) model’s architecture. In the first step, the teacherencoder (i.e., unnormalized) Gθ is trained to map input x into a label. In the second step, we freezeGθ, remove its classification head cGθ, and train the student encoder (i.e., with batch-norm) Fζ whileregularizing its learned representations rFζ using distance function Lct and rGθ. Stopping gradientsoperation is shown by 8. Classifier heads and class probabilities are denoted by c and p, respectively.
Figure 2: (a) In the MNIST classification task, the model without BN (NoBN) achieves highclassification accuracy even if the frequent training features are missing during inference. (b)Ablation results on the “None” test set. (c) The model with BN (2nd row) puts more emphasis on thered/blue squares as they are the frequent features while the NoBN model (3rd row) does not.
Figure 3: CIFAR-10-C error. For both the models, CT achieves lower corruption errors.
Figure 4: Corruption (left) and clean (right) errors by increasing adaptation batch size on CIFAR-10-C.
Figure 5: Histogram plots of class ‘2’ vs. ‘3’.
