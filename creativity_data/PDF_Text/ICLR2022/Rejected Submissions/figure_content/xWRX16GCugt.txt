Figure 1: Example of a simple tree of settingsthat shows the core principle of our framework:research settings can be organized into a hierar-chy based on their assumptions. Methods havea set of basic assumptions which correspond totheir target setting (dashed arrows), and can beapplied onto any of their descendants. In thistrivial example, both MLP and ConvNet clas-sifiers are applicable to the MNIST classifica-tion setting and their performances are directlycomparable, even though they were created forslightly different settings. Sequoia applies thisprinciple to the field of Continual Learning.
Figure 2: Sequoia - The Continual Learning Research Tree. Continual learning research set-tings can be organized into a tree, in which more general settings (parents) are linked with morerestricted settings (children) by the differences in assumptions between them. Settings generallybecome more challenging the higher they are in this hierarchy, as less information becomes avail-able to the method. The central portion of the tree shows the assumptions specific to CL, while thehighest lateral branches indicate the choice of either supervised or reinforcement learning, which weconsider to be orthogonal to CL. By combining either with the central assumptions, settings fromContinual SL and Continual RL can be recovered to the left and right, respectively.
Figure 3: Incremental Supervised Learning results. Final performanCe (vertiCal axis) is plottedagainst runtime (horizontal axis). The methods aChieving the best trade-off lie Closer to the top-leftof the figures. Task-InCremental and Class-InCremental results are presented on the top and bottomrow, respeCtively. The dotted line shows ChanCe aCCuraCy for eaCh setting-dataset Combination. ForeaCh methods, several trials are presented depending on metriCs Composed of linear Combination offinal performanCe and (normalized) runtime. GEM and GDumb aChieve the best tradoff, althoughthe latter Cannot make prediCtions in an online manner and thus serves more as a referenCe point.
Figure 4: Impact of the RL backbone algorithm in Traditionaland Incremental RL. Final performance (vertical axis) is plottedagainst online performance (horizontal axis). The bubbles’ sizeindicates the normalized runtime of the methods. The methodsachieving the best trade-off lie closer to the top-right of the fig-ures and have smaller bubble size. Datasets are presented in eachrow and settings are presented in each column. For each method,several trials are presented depending on metrics composed oflinear combination of final performance and online performance.
Figure 5: UML Diagram showing the main abstractions of Sequoia.
Figure 6: UML Diagram of the CL assumptions hierarchy. The CRL and CSL branches are notshown, but follow an identical structure.
Figure 7: Split-Synbols. Example of Synbols character to classify.
Figure 8: Continual-MonsterKong. We display the 8 tasks that constitute the benchmark in chrono-logical order. The first two tasks test the agent’s ability to jump between platforms, the second twotest its ability to climb ladders and the last four combine both skills.
Figure 9: Incremental Supervised Learning results. Transpose of Figure 3 for improved read-ability. Final performance (vertical axis) is plotted against runtime (horizontal axis). The methodsachieving the best trade-off lie closer to the top-left of the figures. The dotted line shows chanceaccuracy for each setting-dataset combination. For each methods, several trials are presented de-pending on metrics composed of linear combination of final performance and (normalized) runtime.
Figure 10: Impact of the RL backbone algorithm in Traditional and Incremental RL. Largerversion of Figure 4 for improved readability. Final performance (vertical axis) is plotted againstonline performance (horizontal axis). The bubbles’ size indicates the normalized runtime of themethods. Datasets are presented in each row and settings are presented in each column. For eachmethod, several trials are presented depending on metrics composed of linear combination of finalperformance and online performance. The methods achieving the best trade-off lie closer to thetop-right of the figures and have smaller bubble size. In general, we observe a trade-off betweenperformance and runtime, a tendency also observed in Figure 9. Another interesting trade-off canbe observed between final performance and online performance. E.g., in both MonsterKong bench-marks, DQN achieves the best final performance whereas PPO achieves the best online performance.
Figure 11: PPO’s Transfer matrix in Continual-MonsterKong. Each cell at row i and columnj indicates the test performance on task j after having learned tasks 0 through i. The contents ofeach cell correspond to the average reward per episode obtained in the test environment for thecorresponding task. Positive numbers above the diagonal indicate generalization to unseen tasks,which is achievable by design in the Continual-Monsterkong benchmark.
