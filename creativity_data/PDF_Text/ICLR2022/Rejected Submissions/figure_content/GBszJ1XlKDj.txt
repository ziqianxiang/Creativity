Figure 1:	Figures for the synthetic medium scale MDP. (a): Relative change of the policy kπnew -π kF /kπ kF during training of Algorithm 1 compared with PMD and GPMD in Zhan et al. (2021),with the logarithmic scale used for both axes. Notice that Algorithm 1 converges in 6-7 iterationsto 10-12 in all cases while PMD and GPMD take more than 104 iterations. (b) - (e): Blue: Theconvergence of log |log ∣∣π - π* kF| in the training process with the KL divergence, the reverse KLdivergence, the Hellinger divergence and the α-divergence with α = -3, respectively. Green: A linethrough the origin with slope log 2. Comparison of the convergence plots with the green referencelines shows a clear quadratic convergence for Algorithm 1.
Figure 2:	Figures for the synthetic medium scale MDP. (a): Relative change of the policykπnew - π kF /kπ kF in the training process of Algorithm 1. Logarithmic scale is used for the verticalaxis. (b) - (e): Blue: The convergence of log |log ∣∣π - π* kF| in the training process with the KLdivergence, the reverse KL divergence, the Hellinger divergence and the α-divergence with α = -3,respectively. Green: A line through the origin with slope log 2.
