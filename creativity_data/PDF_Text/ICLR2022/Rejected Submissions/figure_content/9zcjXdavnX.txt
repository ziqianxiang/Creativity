Figure 1: QRS. β is a scalar parameter thatcontrols the quality/efficiency trade-off. Theblue shaded area depicts the truncated distri-bution that QRS samples from.
Figure 2: Estimation of sampling quality (TVD(p, pβ) and DKL(p, pβ)), efficiency (acceptance rate),and the trade-off between them for a QRS sampler when using a proposal q = Poisson(λ = 10) toapproximate p = Poisson(λ = 11) over five independent experiments with 10M samples.
Figure 3: GPT-2 generations containing the term “Wikileaks”. We use GPT-2 small, GPT-2 smallconditioned on various prompts and the fine-tuned model obtained through DPG (Khalifa et al., 2021)as proposal distributions q for the QRS algorithm. Points in the left-upper corner correspond toTVD(p, q) before QRS, while the curves show TVD(p, pβ) as a function of the acceptance rate.
Figure 4: Estimation of the divergence from the EBM (TVD and KL to p), moments of featuresfemale and science, and divergence from the original base model (KL from a) to debias GPT-2biographies talking about scientists. We also show samples from running the QRS sampler at 10-3acceptance rate. Samples are cut off at 40 (subword) tokens and are manually chosen to show twomale and two female biographies, for constraint satisfaction (moment matching) results refer to thegraph. We color words that fire our female or science features.
Figure 5: TVD(p, pβ) running the QRS sampler at various acceptance rates to generate paraphrasesof three sequences (top). We show some example paraphrases from both the proposal distributionq(x) (round-trip NMT) as well as the QRS sampler pβ at an acceptance rate of 10-5 (bottom).
Figure 6: Visualization ofMain Proof. The left panel shows the unnormalized distributions P and Pe,the right panel their normalized versions P and pβ. On the right panel, the area under the P (resp. thePβ) curve represents the total p-mass (resp Pe -mass) of X, namely 1. To simplify visual comparison,the figure assumes that Z = 1, in other words that P = p; then Ze ≤ 1 and Pe is Pe moved up bythe constant factor 去.The TVD between P and Pe is equal to the area between the two curves aboveCe, but also to the area between the two curves above Ce. This last area is included inside the areabelow the P curve above Ae, which is the visual counterpart of equation (16).
Figure 7: Metropolis-Hastings: Theorem 7.4. copied from (Robert & Casella, 2004). Here f isthe target distribution. Note the difference between (i) and (ii). Property (i) is concerned with thef -expectation of R.V. h, and considers the average over the T first elements of a single chain, whichconverges to the expectation for increasing T . Property (ii) is concerned with the TVD distancebetween the target distribution f and the distribution obtained by repeatedly running an n-step chainand outputting the n-th element. This distance converges to 0 for increasing n.
Figure 8: We compare the QRS and IMH samplers in practice by taking 1,000 samples at 3 orders ofacceptance rate for the constraint 50% female and 100% science. We experiment with a version ofIMH in which we use a fixed burn-in of 1000 and set thinning as to obtain the desired acceptancerate, as well as a version in which we reset the chain after 10, 100, or 1,000 samples. For the latterversion we can also estimate perplexity as the samples are i.i.d. We do not show the percentage ofunique samples for the latter version, but note this is 100% for both QRS and IMH-reset.
Figure 9: We show importance sampling estimates of TVD(p, .), an upper-bound on TVD(p, pβ),DKL(p||.), DKL(.||a) and feature moments as a function of acceptance rate. We show three distribu-tional constraints on GPT-2 Biographies and two pointiwse constraints on GPT-2 small. As proposaldistribution we make use of a DPG model trained for each constraint separately. We show separatelines for the target moments and the moments realized by the EBMs, revealing slight inaccuraciesin the EBM moments for some constraints. We also show observed moments for 50k QRS samplesobtained at acceptance rates 10-1, 10-2, and 10-3.
