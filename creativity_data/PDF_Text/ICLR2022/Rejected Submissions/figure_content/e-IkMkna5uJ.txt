Figure 1: Noise fitting shows when and how much a network fits a target frequency function.
Figure 2: Linear interpolation measurements on functions of varying frequency. Left: Interpo-lation experiment between images of the same class on an oracle function: the true one-hot labelperturbed by radial wave label smoothing of variable frequency. As the oracle function increasesin frequency, the interpolating paths become less smooth. Right: Summary of this interpolationexperiment via a discrete Fourier transform; as the frequency of the oracle function increases, so doesthe proportion of the DFT magnitude allocated to the high frequency components.
Figure 3: Modern CNNs exhibit spectral bias toward low frequencies, but larger networkslearn high frequencies more readily. Higher min noise fitting denotes that the label smoothingnoise function is learned more readily. Left: Min noise fitting for a shake_shake_96 model with radialwave label smoothing of varying frequency; lower frequencies are learned more readily. Center:Min noise fitting for wide resnets of variable width at frequency 0.039; the larger model learnsthis frequency more readily. Right: Min noise fitting for shake-shake models of variable width atfrequency 0.039; larger models learn this frequency more readily.
Figure 5: Weight decay inhibits learning high frequencies. Results here are from ShakeEhake_32using the radial wave label smoothing methodology at frequency 0.038 (Left) and linear interpolationwithin and between classes (Right). Both measurement techniques show that increasing the penaltyon the norm of the weights produces a stronger spectral bias, in the form of diminished fitting of highfrequency noise and a lower-frequency learned function.
Figure 6: Implicit regularization in the form of dataset size and Mixup augmentation producesa lower-frequency learned function, but RandAugment and AutoAugment can increase func-tion frequency. Left: Interpolation experiment for wrn_160 trained with varying training set sizes:training with more examples produces a lower-frequency learned function, particularly within-class.
Figure 7: Self-distillation produces a studentmodel with a lower-frequency learned functionthan its teacher. The teacher ShakeÂç´hake_96model is trained on one-hot labels and stoppedearly when training loss reaches a threshold. Thestudent is trained to fit the pseudolabels producedby the teacher until the same training loss thresholdis achieved, at which point it has higher validationaccuracy than the teacher.
Figure 8: All six models we tested exhibit spectral bias. Here we show noise fitting when trainingeach model with different frequencies of radial wave label smoothing.
Figure 9:	All six models we tested exhibit sensitivity to variations of low (but nonzero) imagefrequency, which are dominant in natural images. Here we show noise fitting when training eachmodel with label smoothing of frequency 0.038 in various unit norm direction corresponding toFourier basis images indexed by frequency k.
Figure 10:	Spectral bias is evident via interpolation when training with radial wave labelsmoothing at various frequencies. Results here parallel those in Figure 8 but using the linearinterpolation methodology.
Figure 11:	Spectral bias is evident via interpolation when training as usual and comparingcheckpoints at different epochs.
Figure 12:	Weight decay increases spectral bias, producing a lower-frequency learned function.
Figure 13:	Increasing the number of training examples reduces the frequency of the learnedfunction.
Figure 14:	Mixup augmentation produces a lower-frequency learned function on all modelstested. On some models, RandAugment and AutoAugment produce a higher-frequency learnedfunction.
Figure 15:	Modest Mixup augmentation produces a lower-frequency learned function, butMixup that is too strong can induce higher frequencies.
Figure 16:	Self-distillation produces a lower-frequency student compared to its teacher.
