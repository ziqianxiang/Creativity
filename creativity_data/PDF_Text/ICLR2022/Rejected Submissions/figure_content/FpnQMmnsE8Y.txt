Figure 1: We propose a recurrent parameter generator (RPG) that shares a fixed set of parametersin a ring and use them to generate parameters of different parts of a neural network, whereas in thestandard neural network, all the parameters are independent of each other, so the model gets bigger asit gets deeper. Left: The third section of the model starts to overlap with the first section in the modelring, and all later layers share generating parameters for possibly multiple times. Right: Employingthe Recurrent Parameter Generator (RPG) for ResNet could reduce the model parameters to any size.
Figure 2: We demonstrate the effectiveness of RPGS on various applications including imageclassification (Left), human pose estimation (Middle), and multitask regression (Right). A networkcan either have a global RPG or multiple local RPGs that are shared within blocks or sub-networks.
Figure 3: Recurrent parameter generators at multiple scales. Upper: A global RPG is used forgenerating convolution kernels for the entire ReSNet18. Lower: Four local RPGs are each responsiblefor generating convolution kernels within each corresponding building block of the ResNet18.
Figure 4: CIFAR100 accuracy versus the backboneparameter size for plain ResNet and RPG. RPGonly has a 0.2% drop with 50% Res34 parameters.
Figure 5: Ablation studies of permutation and re-flection matrices of Res34-RPG. Having both ma-trices gives the highest performance.
