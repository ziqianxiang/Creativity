Figure 1: (a) L2 equilibrium gaps and (b) rewards of Algorithm 1 on the matrix team task, and (c) L2 equilibriumgaps and (d) rewards of Algorithm 2 on the Markov team task. “Current” denotes the actual policy trajectory{(μh,νk)}HH=KKk=i, while “Average” represents the auxiliary policies {(μh,琮)}H=KKk=ι. In “Independent”,each agent runs a naive single-agent Q-learning algorithm independently, by taking greedy actions w.r.t its localQ-function estimates. Shaded areas denote the standard deviations of the equilibrium gap or reward.
Figure 2: (a) L2 equilibrium gaps and (b) rewards of Algorithm 1 on the matrix team task given inTable 1. “Current” denotes the actual strategy trajectory, while “Average” represents the uniformlysampled strategy pair. Shaded areas denote the standard deviations of the equilibrium gap or reward.
Figure 3: (a) L2 equilibrium gaps and (b) rewards of Algorithm 2 on the Markov team task givenin Table 2. “Current” denotes the actual policy trajectory {(μh,νh)}H=Kk=「while “Average”represents the auxiliary policies {(μh, Vh)}HKKk=ι. Shaded areas denote the standard deviations ofthe equilibrium gap or reward.
