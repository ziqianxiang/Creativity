Figure 1: The network,s inputs is X. The outputsare the inducing points Z, the mean vector m andthe Cholesky factor, L, of q(uIXi).
Figure 2: Toy data set with N = 200 points. Initial and final locations for the inducing points areshown on the top and bottom of each figure. In IDSGP, the inducing points correspond to the pointdrawn with star. The posterior mean and standard deviation of full GP are shown with blue and browndashed lines, respectively. VSGP method with M = 4. IDSGP with M = 2 and a neural networkwith 2 layers with 50 units. SWSGP with M = 4 and 2 neighbors. SOLVE with M1 = M2 = 2.
Figure 3: Banana classification data set with N = 5300 points. The final location of inducing pointsare shown inside the figures. For IDSGP, we show the location of inducing points related to the greencolored point. VSGP with M = 4. IDSGP with M = 2 and a neural network with 2 hidden layerseach contains 50 hidden nodes. SWSGP with M = 4 and 2 neighbors. SOLVE with M1 = M2 = 2.
Figure 4: Negative log-likelihood on the test set for each method as a function of the training time inseconds, in log10 scale, for the Yellow taxi and the Airline delays datasets. Best seen in color.
Figure 5: Toy regression example by varying number of inducing points Mx ={2, 4, 8, 16, 32, 64, 128} with location of initial and final inducing points for an arbitrary selectedpoint x from training sets. The mean and standard deviation of full GP prediction are shown withdashed blue and brown lines, respectively. The blue lines and the dashed red lines are the mean andstandard deviation of IDSGP.
Figure 6: Toy regression example by varying number of inducing points Mu, Mv =(2,4,8,16,32,64,128} with location of initial and final inducing points. The mean and standarddeviation of full GP prediction are shown with dashed blue and brown lines, respectively. The bluelines and the dashed red lines are the mean and standard deviation of SOLVE.
Figure 7: Toy regression example by varying number of inducing points M ={2, 4, 8, 16, 32, 64, 128} with location of initial and final inducing points. The mean and standarddeviation of full GP prediction are shown with dashed blue and brown lines, respectively. The bluelines and the dashed red lines are the mean and standard deviation of VSGP.
Figure 8: Toy regression example by varying number of the neighbor inducing points Mc ={2, 4, 8, 16, 32, 64, 128} and total number of inducing points M = 128, with location of initial andfinal inducing points. The mean and standard deviation of full GP prediction are shown with dashedblue and brown lines, respectively. The blue lines and the dashed red lines are the mean and standarddeviation of SWSGP.
Figure 9: Toy regression example by varying number of inducing points M ={2, 4, 8, 16, 32, 64, 128} with location of initial and final inducing points. The mean and standarddeviation of full GP prediction are shown with dashed blue and brown lines, respectively. The bluelines and the dashed red lines are the mean and standard deviation of VSGP*.
Figure 10: (left) Test RMSE for each method as a function of the training time in seconds, in log10scale, for the Yellow taxi dataset. (right) Prediction error on the test set for each method as a functionof the training time in seconds, in log10 scale, for the Airlines Delays dataset. Best seen in color19Under review as a conference paper at ICLR 2022D.6 Neural network trained via maximum likelihoodIn this subsection we show extra experiment results on the UCI datasets when using a neural networktrained via maximum likelihood. The architecture of the neural network is the same as the one of thenetwork used in the proposed method IDSGP. Training is done using ADAM. The learning rate usedis 0.001. The mini-batch size is the same for the GP-based methods. In regression, we use the neuralnetwork to predict the mean and variance of the Gaussian predictive distribution. In classification, weuse a sigmoid activation function. The average test negative log-likelihood obtained in each problemis shown in Table 9 and Table 10. The results are high-lighted in bold-face when the NN performsworse than any other GP based method. The tables show that, in the case of regression problems,most of the times the neural network performs worse than the GP based methods. In the case ofclassification problems, the performance of the neural network is worse in those problems in whichthe error is higher according to Table 6. By contrast, in those problems in which the accuracy isalmost equal to 100%, there are no differences or it performs slightly better.
