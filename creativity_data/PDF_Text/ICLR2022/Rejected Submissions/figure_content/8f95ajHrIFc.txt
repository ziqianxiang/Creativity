Figure 1: In this study we make connection betweentwo popular paradigms for fine-tuning sequence genera-tion models according to preferences Reward Maximiza-tion (RM) and Distribution Matching (DM).
Figure 2: Values of reward, advantageand the baseline for first 1000 epochsof a pointwise constraint experiment.
Figure 3: Evaluation metrics: Dkl(p, ∏θ) (] better), E∏θφ(x) (↑ better), Dkl (∏θ , a) (] better), Self-BLEU-5(]better), and Distinct-1 (↑ better) aggregated over 6 pointwise constraints experiments (tasks 1-6) for policiesobtained from GDC++, GDC, Ziegler and Reinforce. See Figure 6 for aggregated distributional constraintsexperiments, and Figures 7-10 in the Appendix for a detailed view on each experiment. And a Table 5 view forfinal results of each run.
Figure 4: E∏θ φ(x) or μ per constraint (↑ better) and Dkl (p, ∏θ) (] better) as a function of the number ofsamples reported for task 1 (a) and task 8 (b). We report the number of samples (i.e. the number of epochs timesthe batch size) for a fair comparison of convergence speed. GDC++ is consistently superior across all batchsizes in terms of convergence and constraint satisfaction. The effect is more conspicuous with small batch sizes.
Figure 5: Comparison between GDC and GDC++using a set of Variance diagnosis metrics on two experi-ments for pointwise and distributional constraints.
Figure 6: Evaluation metrics: average μ (↑ better), Dkl(p∣∏θ) (] better), Dkl (∏θ |a) (] better), Self-BLEU-5(]better), and Distinct-1 (↑ better) on aggregated four distributional constraints experiments: Task 7: a singledistributional constraint, Task 8 and Task 9: a two hybrid constraint pairs, Task 10: Multiple Distributionalconstraints. For policies obtained from GDC++ and GDC. Average μ was computed for each experiment bymapping Eχ~qφi(x) for each constraint i onto a [0,1] interval and averaging over constraints. See Figures 9-10in for a detailed view on each experiment.
Figure 7:	Evaluation metrics E∏θ φ(x), KL(p∣∏θ) (] better), KL(∏θ |a) (] better), Self-BLEU-5 (] better), andDistinct-1 (↑ better) for three constraints types: Task 1: Word "amazing" Fig.(a), Task 2: Word "wikileaks"Fig.(b) and Task 3: Wordlist "politics" Fig.(c) for policies obtained from GDC++, GDC, Ziegler and Reinforce.
Figure 8:	Evaluation metrics E∏θφ(x), KL(p∣∏θ) (] better), KL(∏θ|a) (] better), Self-BLEU-5 (] better),and Distinct-1 (↑ better) for three pointwise constraints experiments: Task 4: Wordlist "science" Fig.(a),Task 5: classifier +ve sentiment Fig.(b) and Task 6: Classifier -ve sentiment Fig.(c) for policies obtainedfrom GDC++, GDC, Ziegler and Reinforce.
Figure 9:	Constraint satisfaction μ (↑ better) for four distributional constraints types: Task 7: a singledistributional constraint Fig.(a). Task 8 and Task 9: a two hybrid constraint pairs Fig.(b) & Fig.(c) Task10: Multiple Distributional constraints Fig.(d). For policies obtained from GDC++ and GDC. The dashedHorizontal bars denote the desired moments 口匕.
Figure 10:	Evaluation metrics: KL(p∣∏θ) (] better), KL(∏θ |a) (] better), Self-BLEU-5 (] better), and Distinct-1 (↑ better) four distributional constraints types: Task 7: a single distributional constraint Fig.(a). Task 8,9: atwo hybrid constraint pairs Fig.(b) and Fig.(c), Task 10: Multiple Distributional constraints Fig.(d), for policiesobtained from GDC++ and GDC.
