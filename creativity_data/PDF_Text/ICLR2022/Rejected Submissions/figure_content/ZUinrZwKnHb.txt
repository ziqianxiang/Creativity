Figure 1: We choose two examples to show differences between the naive and supervised self-attention patterns. The association reference for the naive self-attention is the average of all layers.
Figure 2: Model overview. The model architecture consists of three parts: a regular ResNet, aregular Transformer encoder, and several transposed convolutional layers. Two types of loss functionare leveraged to supervise the model training. The final output of the model is supervised by thegroundtruth keypoint heatmaps. One of the immediate self-attention layers is sparsely supervisedby the instance masks. In particular, we sample the rows of the attention matrix of the chosenattention layer according to the visible keypoint locations of each human instance, reshape theminto 2D-like maps, and then use the mask of each instance to supervise the average map. In thisfigure, we only show a few keypoints of each instance for simplicity.
Figure 3: Self-Attention based Grouping. When the foundedkeypoints in a skeleton induce a stronger attention attractionto an unmatched keypoint, this candidate will be assignedto this skeleton. The blue edges (thick) have totally higherattention scores than the green edges (slim).
Figure 4: Localization errors analysis on COCO validation set.
Figure 5: The convergences on the heatmap loss and instance mask loss when trained with andwithout supervising self-attention.
Figure 6: The convergences on the heatmap loss and mask loss when supervising the self-attentionin different layer depths.
Figure 7: The architecture designs for supervising shared self-attention and independent self-attention.
Figure 8: The convergences on the heatmap loss and mask loss when trained with supervising sharedself-attention and independent self-attention.
Figure 9: Qualitative visualization results predicted by our pure bottom-up model. For each image,we show the original image plotted with human poses and masks. And, for each image, we alsoshow the learned attention areas from the views of 4 sampled keypoints, each location of whichhas been annotated by a white color pentagram. Redder areas mean higher attention scores.
