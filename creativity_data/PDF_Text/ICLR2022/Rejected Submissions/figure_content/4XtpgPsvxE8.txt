Figure 1: The p-values of the null hypothesisthat statistical methods perform equal to or bet-ter than deep learning methods, evaluated fordatasets of different sizes. The red line displaysthe significance level α = 5%.
Figure 2: The relative improvement of the bestdeep learning method versus the best classicalmethod on all benchmark datasets. The red lineis fitted on the visualized datapoints via linear re-gression in the log-space.
Figure 3: Illustration of non-dominated sorting. The layers show the partitioning of the data inPareto fronts. The numbers depict the overall rank by computing the -net within each layer.
Figure 4: Hypervolume errorwhen iteratively choosing mod-els via different model selectionapproaches. Errors are averagedover all 44 benchmark datasetsand five random seeds.
Figure 5: Visualization of forecast latency and nCRPS for mod-els trained and evaluated on the “M4 Yearly” dataset. Each colordescribes a family of models with dots representing different hy-perparameter configurations and training times (checkpoints fordeep learning models). Crosses show the default hyperparame-ter configuration and the maximum training time for the dataset.
Figure 6: Comparison of individual forecasting models, hyper-ensembles and latency-constrainedensembles. The axes show the average relative latency and nCRPS compared to the unconstrainedensemble. Results are averaged across all 44 benchmark datasets. Light crosses show default config-urations of deep learning models, bold crosses show their corresponding hyper-ensembles. Classicalmodels are not colored as most can only be found (far) beyond the range of the plot.
Figure 7: Correlation matrix of all benchmark methods’ nCRPS ranks across all benchmark datasets.
