Figure 3: Improving KL estimation between two standard Normals via KMM-DRE using sDREQ1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00S(b) Varying σ (bandwidth is 0.02)Robustness and Generalization beyond Gaussian Distribution: We rigorously test the robust-ness of SDRE by evaluating it on the following setups. (1) p and q as 1D truncated normal dis-tributions to evaluate it on another setup with distributions with finite support. (2) Randomizedmean parameters to ensure that the sDRE model does not benefit from the symmetry of the dis-tributions around 0 in Table 2. (3) p and q as Student-T distributions with randomized means totest the performance of sDRE under model mismatch. In all these settings, sDRE model is able toreliably estimate the true KL-divergence demonstrating that it is highly robust. Please see Table 5 inAppendix G for results and additional details.
Figure 4: SpatialMultiOmniglot representation learning results(b) Representation learning accuracyacters from different Omniglot alphabets and v is a n × n grid containing the next characters ofthe corresponding characters in u. After learning, we evaluate the representations from the encoderwith a standard linear evaluation protocol (Oord et al., 2018). For the model, as in TRE, we usea separable architecture commonly used in MI-based representation learning literature and modelthe densities with log p(u, v) = g(u)TWf(u), where g and f are 14-layer convolutional ResNets(He et al., 2015). We construct the scaling distributions via dimension-wise mixing. In this exper-iment, we only compare to the single ratio baseline and TRE because (Rhodes et al., 2020, Figure4) already demonstrated that it significantly outperforms both Contrastive Predictive Coding (CPC)(Oord et al., 2018) and Wasserstein Predictive Coding (WPC) (Ozair et al., 2019) models of repre-sentation learning on the same exact task.
Figure 5:	TRE vs SDRE on p = N (-1, 0.1) and q = N(1, 0.2) using 3 intermediate distributions p1, p2,p3constructed using the linear-combination construction.
Figure 6:	sDRE vs TRE telescoping on mixtures of distributions with finite support13Under review as a conference paper at ICLR 2021Figure Label	p	q	TRE Pk	sDRE ma	N(0,1e-6)	N(0,1)	Linear Mixing with α = [6.10e-05, 0.0078, 0.13] Linear Mixing with α = [0.053, 0.11,	Mixture of Linear Mixing with α = [6.10e-05, 0.0078, 0.13]b	N (-1, 0.08)	N(2, 0.15)	0.16, 0.21, 0.26, 0.31, 0.37, 0.42, 0.47, 0.53, 0.58, 0.63, 0.68, 0.74, 0.79, 0.84, 0.89, 0.95] Linear Mixing with α = [0.03, 0.07, 0.1, 0.14, 0.17, 0.21, 0.24, 0.28, 0.31, 0.34,	C(0, 1)c	N (-2, 0.08)	N(2, 0.15)	0.38, 0.41, 0.45, 0.48, 0.52, 0.55, 0.59, 0.62, 0.66, 0.69, 0.72, 0.76, 0.79, 0.83, 0.86, 0.9, 0.93, 0.97] Linear Mixing with	C(0, 1)d	N (-5, 1)	N(5,1)	α = [0.11, 0.22, 0.33, 0.44, 0.55, 0.66, 0.77, 0.88]	C(0, 2)Table 3: Experiment configurations for Figure 7C Appendix1D density ratio estimation taskIn Section 5.1 we used KL-divergence as the evaluation metric to assess the accuracy of the densityratio estimation of each of the models, BC-DRE, TRE, F-DRE and sDRE. Since KL-divergenceonly evaluate the density ratio over samples from p, here we provide the plots of the log densityratio for all the models over a larger interval to better capture their behavior.
Figure 7: 1D density ratio estimation task. The configurations of the 1D Gaussians are the following incolumn-wise order: (a) p = N (0, 1e-6), q = N (0, 1), KL= 13.32 (b) p = N (-1, 0.08), q = N(2, 0.15),KL = 200.27 (c)p = N (-2, 0.08), q = N (2, 0.15), KL = 355.82 (d)p = N(-5,1),q = N (5, 1), KL= 50.0. The models are the following row-wise: single ratio BC-DRE, TRE, F-DRE, and SDRE.
Figure 8: Uncertainty quantification for SDRE estimator. Here, p = N (-1.0, 0.1), q = N (1.0, 0.2) andm = C(0, 1). We plot the 3x standard deviation around the mean in light blue.
Figure 9: Diagnostic plot for a high dimensional experiment.
Figure 10: Diagnostic plot for a high dimensional experiment with randomized means.
Figure 11: Mutual information estimation and representation learning with varying K.
Figure 12: SpatialMultiOmniglot representation learning results with same encoder for fandg.
Figure 13: sDRE is able to match the ground truth log-ratio and correctly decays all the valuestowards -∞ if they don’t fall under the support of p.
Figure 14: Improving KL estimation between two standard Normals via KMM-DRE using sDRE(b) Varying σ (bandwidth is 0.02)Huang et al., 2006; Sugiyama et al., 2012). The main advantage of this method is that the ratioestimators can be computed in closed form and therefore are computationally more efficient.
