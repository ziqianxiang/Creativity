Figure 1: Illustration of the learning strategy in an ideal setting, using synthetic (uniformly distributednoise) data. We replace the 0-1 loss with cross-entropy loss and train logistic regression modelsfor both f and g. (a) shows that the informative and the uninformative supports are separated. (b)demonstrates that the model derived with majority-uninformative data has reasonable performance onthe informative portion. (c) shows that the predictor has high accuracy in the informative region, butlow accuracy in the uninformative region. In (d) and (e), the selector trained with fS九 successfullyrecovers informative support thus resulting in low selective risk.
Figure 2: Illustration of Algorithm 1. By up-weighing the informative datapoints, the algorithmprogressively improves the classifier. d) shows the sum weight of all informative data over weightsum of all data, i.e "iPf；I Yi ( See Y in Algorithm 1).
Figure 3: Weight of informative data as a function of training epoch and the ratio of informative data.
Figure 4: Ablation Study on Hyper-parameter β - Our Method.
Figure 5: Ablation Study on Hyper-parameter o - DeepGambler.
Figure 6: Ablation Study on Hyper-parameter a and λ - SelectiveNet.
