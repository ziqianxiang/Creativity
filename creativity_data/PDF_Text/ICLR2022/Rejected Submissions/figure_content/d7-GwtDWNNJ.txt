Figure 1: Schematic diagram of the GDN architecture obtained via algorithm unrolling.
Figure 2: Left: After training on RG graphs of size N = 68, GDNs are capable of maintainingperformance on RG graphs orders of magnitude larger. Missing data at N = 4000 corresponds tooverwhelmed our memory resources. The simplest model GDN-share-1 improves performance withincreasing graph size. Right: Reduction in MAE (%) over mean prior of SCs for different lobes.
Figure 3: MIMO Filter: Layer k takes a tensor Ak ∈ RCikn ×N×N and outputs a tensor Ak+1 ∈RCokut ×N ×N. The i-th slice [Ak]i,:,: is called the i-th input channel and [Ak+1]j,:,: is called the j-thoutput channel.
Figure 4: The four lobes in the brains cortex.
