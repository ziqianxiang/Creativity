Figure 1: Maximum Entropy Population: We train each agent in the population to maximize itstask reward as well as the population entropy reward to attain a maximum entropy population.
Figure 2: Overcooked environment: From left to right, the layouts are Cramped Room, AsymmetricAdvantages, Coordination Ring, Forced Coordination, and Counter Circuit.
Figure 3:	Performance comparison: Train and test performances on the matrix game. Shown arethe results for Best Responses (BRs) to MEP agents, BRs to TrajeDi populations, BRs to baselinepopulations, and individual agents. MEP allows faster learning compared to TrajeDi and others.
Figure 4:	Mean episode reward and population entropy learning curve: The left two plots are themean episode reward and population entropy with entropy reward weight α = 0 in the AsymmetricAdvantage layout. The right two plots show the quantities with α = 0.01 in the same layout.
Figure 5: Performance comparison and ablation test: Average episode rewards over 400 timestep(1 min) trajectories for different methods, with standard error over 5 different random seeds, pairedwith the proxy human HP roxy . The hashed bars with the slash (/) show results with the startingposition of the agents switched. Figure (a) shows the performance comparison among MEP andbaselines including SP and PBT. Figure (b) shows the ablation tests, where we use MEPα=0 andMEPβ=0 to denote the MEP model without the population entropy reward and without the prioritizedsampling mechanism, respectively. Figure (c) shows the performance comparison with TrajeDi.
Figure 6: Performance with real humansThere is a recent related work on zero-shot coordination with a diversified population, which iscalled TrajeDi (Lupu et al., 2021). To the best of our knowledge, TrajeDi is the most related work.
Figure 7: Performance comparison: We did an extensive hyper-parameter search for TrajeDi. MEPconverges faster than TrajeDi under all the parameters.
