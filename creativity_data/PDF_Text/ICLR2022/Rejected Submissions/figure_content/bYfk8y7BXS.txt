Figure 2:	Box plots of model selection performancefrom offline learning in each DRL algorithms for E2.
Figure 3:	Sensitivity analysis for different trainingdata size. PMS attains the best performance and hasthe least sensitivity.
Figure 5: DQN environments in our studies: (a) Ei: FrozenLake-v0; (b) E2: Banana Collectors (3D geomet-rical navigation task); (c) E3: Pong-v0; (d) E4: Breakout-v0; (e) E5: HalfCheetah-v1; (f) E6: WaIker2d-v1.
Figure 6: Policy selection using top-k ranking re-gret score in E1 (Frozen Lake).
Figure 7: Policy selection using top-k ranking pre-cision in E1 (Frozen Lake).
Figure 8: Policy selection using top-k ranking re-gret score in E2 (Banana Collector).
Figure 9: Policy selection using top-k ranking pre-cision in E2 (Banana Collector).
Figure 10: Policy selection using top-k ranking re- Figure 11: Policy selection using top-k ranking pre-gret score in E3 (Pong).	cision in E3 (Pong).
Figure 12: Policy selection using top-k ranking re-gret score in E4 (Breakout).
Figure 13: Policy selection using top-k ranking pre-cision in E4 (HalfCheetah-v1).
Figure 15: Policy selection using top-k ranking Pre-cision in E5 (HalfCheetah-v1).
Figure 14: Policy selection using top-k ranking re-gret score in E5 (HalfCheetah-v1).
Figure 16: Policy selection using top-k ranking re-gret score in E6 (Walker2d-v1).
Figure 17: Policy selection using top-k ranking pre-cision in E6 (Walker2d-v1).
Figure 18: Different Î± for PMS selection.
Figure 19: Different O for PMS selection.
