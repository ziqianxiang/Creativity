Figure 1: An overview of the MAMA approach. (a) shows the architecture of meta-attack with an RNNoptimizer. (b) represents the model-agnostic optimization process. As an example, given defense mod-els {f1, f2, f3}, meta-train/meta-test could include different divisions as {f1, f2}/{f3}, {f1, f3}/{f2}, or{f2 , f3 }/{f1 }. The parameters φ of the optimizer will be updated by accumulating the gradients from meta-train and meta-gradients from meta-test.
Figure 2: The accuracy curves of HYDRA w.r.t. Figure 3: Cross-data generalization evaluation With dif-training iterations of BMA with different λ.	ferent numbers of training data points on CIFAR-10.
Figure 4: Comparing the performance of different optimizers based on PGD adversary against four defensemodels. The iteration needed for training the automated optimizer (BMA) is also marked at 20. The resultsalso evidence that BMA can directly extend attack iterations from 20 to a longer step.
Figure 5: The running time of AA,ODI, and BMA With a mini-batchof 100 data points on CIFAR-10.
