Figure 1: ‘Flat policy learning’ vs. proposed ‘Hierarchical policy learning’. The flat policy learning has beenemployed in prior arts, training an agent to directly learn the low-level actions. In contrast, our hierarchicalpolicy decomposes a long-horizon task into multiple subtasks and leverages the high-level abstract planning,which enables an agent to better address long-horizon planning.
Figure 2: Model Architecture. The model architecture comprises of three main components (1) policy com-position controller for defining the policy structure based on the given instructions, (2) a master policy (MP)responsible for navigation, and (3) a set of interaction policies (IP) specialising in the manipulation tasks. Itddenotes an RGB frame from an explorable direction, d ∈ [0, D], at the time step, t where d = 0 indicates theegocentric direction. We encode Id using a pretrained ResNet and acquire a visual feature, vd. Xi denotes eachstep-by-step instruction. lT,v, lT,m denotes the encoded instruction for the ‘interactive perception module’ and'action prediction module' respectively. lτ:T +ι,n denotes the encoded 'subtask' instruction described in Sec.
Figure 3: Loop Escape Module. The objective of the agent at the current time step is to move to a target object(a garbage can). (a) and (b) show an example of a deadlock state and the behavior of loop escape module whenfinding the target object, respectively. Each dark-blue square denotes the position of the agent. ◦ denotes thetarget object that the agent should navigate to. -→ denotes the view direction of the agent. The dashed ◦ and -→indicate that the target object is invisible to the agent due to occlusion. -→ denotes actions taken by the agenta path taken in a deadlock state. In a deadlock state, the loop escape module cancels the current action thatresults in the deadlock state, ×, and takes a random navigation action, -→.
Figure 4: Hierarchical policy learns faster and more efficient action sequences. Plot (a) shows the learningcurves (success rates vs epochs) of the hierarchical and flat policy agents for unseen and seen environments.
Figure 5: The sensitivity curve for the value of W for loop escape module (LEM). Y-axis denotes thesuccess rate for the unseen validation split.
Figure 6: Plot shows the learning curve (subgoal success rates vs epochs) for the seven subgoal policies de-scribed in Sec. A.1GOAL: Heat and chill a potato.
Figure 7: Inference of the task ‘Heat and Chill a potato.’ using hierarchical policy vs. using flat policy.
Figure 8: Pickup Object interaction policy agent completes a subgoal task ‘Take the remote.’ in an unseenenvironment.
Figure 9: Put Object interaction policy agent completes a subgoal task “Put the plunger below the sink.” in anunseen environment.
Figure 10: Clean Object interaction policy agent completes a subgoal task “Clean the mug in the sink.” in anunseen environment.
Figure 11: Cool Object interaction policy agent completes a subgoal task “Chill the apple in the refrigerator.
Figure 12: Toggle Object interaction policy agent completes a subgoal task “Turn on the lamp.” in an unseenenvironment.
Figure 13: Heat Object interaction policy agent completes a subgoal task “Heat the apple slice in the mi-crowave.” in an unseen environment.
Figure 14: Slice Object interaction policy agent completes a subgoal task “Make slices of the bread.” in anunseen environment.
