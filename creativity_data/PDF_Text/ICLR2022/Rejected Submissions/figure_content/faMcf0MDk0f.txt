Figure 1: The main differences between previous work and BoolNet. BoolNet uses 1-bit featuremaps and a shifted sign function reducing memory requirements and the need for 32-bit operations.
Figure 2:	Comparison between a conventional binary convolution block with 32-bit data flow (a) andour proposed binary convolution block with 1-bit data flow (b).
Figure 3:	The detailed architecture of BoolNet. To enhance the information flow, we modify thebaseline architecture in two aspects: a) Reducing information loss with our multi-slices binaryconvolution. b) Strengthening the information propagation by using split and concatenate operations.
Figure 4: Comparison between BoolNet and state-of-the-art BNNs. The energy consumption iscalculated through hardware simulations.
Figure 5: The training and validation accuracy curves of our proposed Progressive Weight Binarization.
Figure 6:	The training and validation accuracy curves of our trainings discussed in the comparison tothe state-of-the-art BNNs in Section 4.5.
Figure 7:	A theoretical memory usage comparison of one convolution block between BoolNet andprevious work. Actual numbers can differ during implementation, but BoolNet shows significantlylower memory usage, especially in early stages, even when using our Multi-slice strategy with k = 4.
Figure 8: Hardware data flow comparison between Bi-RealNet and BoolNet.
