Figure 1: Graphical model for a hi-erarchical Gaussian process.
Figure 2: (a) The performance profile for outperforming the median of best error rates at the 100thBO iteration. (b) BO convergence of all methods: the median and 20/80 percentiles of the regrets onerror rates over 115 BO runs: 23 tasks and each with 5 repeats of different random seeds. (c) A violinplot on the vertical slices of (b) at the 100th iteration; the white dot is the median and the black line isthe 20/80 percentile. Overall, HyperBO methods H* NLL and H* KL are able to achieve the lowestregret on error rate on the majority of tasks with fewer iterations.
Figure 3: Medians and 20/80 per-centiles of regrets on best valida-tion error rates for methods thatuses models trained on 3 to 23training tasks.
Figure 5: Results of running BO methods in the online setting on 3 different tasks. The image basedtasks use best validation error rate as objective while the text based ones including Uniref50 use bestvalidation CE loss. In all 3 tasks, HyperBO methods achieved better results.
Figure 4: Medians and 20/80percentiles of simple regrets formethods that uses models trainedon 0.2% to 90% of data in eachtask.
Figure 6: Performance profiles for outperforming the median of best error rates at the (a) 25th BOiteration, (b) 50th BO iteration and (c) 100th BO iteration.
Figure 7: The left most is a summary of the BO convergence of all methods: the median and 20/80percentiles of the regrets on error rates over 115 BO runs: 23 tasks and each with 5 repeats of differentrandom seeds. We also show violin plots on its two vertical slices at 50th and 100th iteration, wherethe white dot is the median and the black line is the 20/80 percentile. Overall, HyperBO methods H*NLL and H* KL are able to achieve the lowest regret on error rate on the majority of tasks.
Figure 8: Aggregated BO results on 23 tasks (all in Table 1 except ImageNet ResNet50 2048 becauseof insufficient data) that uses models trained on 3 to 23 training tasks. Note that the models are nevertrained on the data from the test task that we run BO on. If the number of training tasks is less than23, we first remove the tasks that involve the same task dataset as the test task and then remove othersrandomly until we reach the designated number of training tasks. The top left shows the median and20/80 percentiles of regret on best validation error rate for each method. The rest are violin plotsshowing the regret for MIMO, H* NLL and H* KL, where white dots indicate the median and blacklines the 20/80 percentiles.
Figure 9: Aggregated BO results on 23 tasks (all in Table 1 except ImageNet ResNet50 2048 becauseof insufficient data) that uses models trained on 0.2% to 90% of data in each task. Note that themodels are never trained on the data from the test task that we run BO on. The top left is the medianand 20/80 percentiles of simple regret in log scale. The rest of the figures are simple regret violinplots for MIMO and H* NLLFigure 10: Aggregated leave-one-out BO convergence results on 23 tasks, each with 5 repeats usingdifferent random seeds. The left most is the median and 20/80 percentiles of the regrets on error rates.
Figure 10: Aggregated leave-one-out BO convergence results on 23 tasks, each with 5 repeats usingdifferent random seeds. The left most is the median and 20/80 percentiles of the regrets on error rates.
Figure 11: Leave-one-out log regret mean and standard deviation results on ImageNet ResNet50512, LM1B Transformer 2048, WMT XFormer 64 and Uniref50 Transformer 128. All methods wererepeated 5 times with different random seeds to initialize their models. In LM1B Transformer 2048,H* NLL and H* KL disappeared around 60 to 80 BO iterations because they reached 0 regret.
Figure 12: Results of running BO methods in the online setting on 9 different tasks. The image basedtasks all use best validation error rate as objective while the text based tasks (LM1B, Uniref50 andWMT) use best validation ce loss. HyperBO methods achieved better results in 7 out of 9 tasks.
Figure 13: We compare the Peformance of 3 different objective functions in HyPerBO under 5settings of acquisition functions. For EI and PI, using KL as the objective for HyperBO is slightlybetter than NLL or NLL+KL. However, different conclusions can be drawn for UCB2, UCB3 andUCB4. Nevertheless, all HyPerBO variants still outPerform the best alternatives.
Figure 14: We compare the performance of 5 different acquisition functions under 3 settings ofobjectives in HyperBO. Overall, PI and EI outperform UCB with different coefficient values. ButHyperBO with UCB variants still outperforms STBOH, which is roughly the best baseline accordingto the main results in Fig. 6.
