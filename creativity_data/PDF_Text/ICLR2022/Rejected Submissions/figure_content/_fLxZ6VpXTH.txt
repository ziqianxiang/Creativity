Figure 1: Example results obtained by SLIL (Ours) and baselines on mode coverage. (a): A Reachertask, with four targets in different colors. (b)-(g) show the mode coverage (i.e, state distribution) withexpert policy (b), our SLIL policy (c), DRIL policy (d), GAIL policy (e), PWIL policy (f), and SLILimplemented with SoftFlow (Soft-SLIL) (g). (h) show the earth mover’s distance (EMD) (Ling &Okada, 2007) between expert and learner policy state distributions. All the distributions are visualizedusing kernel density estimation (KDE) (Sheather & Jones, 1991). None of the compared approachessolve the mode collapse problem.
Figure 2: LIL framework.
Figure 5: Illustration of FFJORD trainedon 2D manifold (top), on 1D manifold(middle), and our DCNF trained on 1Dmanifold (bottom).
Figure 6: Illustration of DCNF.
Figure 7: Results of SLIL (Ours)and baselines on mode coverage inHalfCheetah2. All the distributionsare visualized using KDE (Sheather& Jones, 1991).
Figure 8: EMD vs scaled return in tasks with multiplemodes. The x-axis is the EMD (Ling & Okada, 2007)between expert and learner policy state distribution. They-axis is the expected return (i.e, total reward), scaled sothat the expert achieves 1 and a random policy achieves 0.
Figure 9: Test state log likelihood using PEω*learned from our DCNF and CNF.
Figure 10: Performance of learner policies in tasks with one mode. The y-axis is the expected return(i.e, total reward), scaled so that the expert achieves 1 and a random policy achieves 0.
Figure 11: SLIL (Ours) performance with differ-ent gating parameters λ in Walker.
Figure 12: Policy performances with differentlearning rates using GAIL and our SLIL in Walker.
Figure 13: Policy performance over environmentinteraction numbers (SLIL vs GAIL) in Walker.
