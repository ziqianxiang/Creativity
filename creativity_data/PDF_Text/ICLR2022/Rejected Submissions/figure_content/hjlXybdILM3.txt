Figure 1: We apply SimpleBits to a variety of tasks to aid neural network understanding. As aper-image simplifier, applied during training (left), it investigates the trade-off curve between sim-plification and accuracy. It can also be used as a post-hoc analysis tool after training (center),illuminating features of images that are crucial to the trained classifier. When combined with datacondensation (right), the original data set can be effectively reduced both in size and in complexity.
Figure 2: Visualization of the bits-per-dimension (bpd) measure for image complexity, sorted fromlow to high. Image samples are taken from MNIST, Fashion-MNIST, CIFAR10 and CIFAR100,in addition to a completely black image sample. bpd is calculated from the density produced by aGlow (Kingma & Dhariwal, 2018) model pretrained on 80 Million Tiny Images.
Figure 4:	Selection of Simplified Training Images on CIFAR10. Simplified images from settingswith varying simplification loss weight 位sim. We observe that at lower bits per dimension color isretained only for some images such as green color for frog or blue sky for the plane. In this low bitregime, texture remains discernable for the cat and frog images.
Figure 5:	Results for Training Image Simplifications on real Datasets. Dots show results for trainingwith different loss weights for the simplification loss. Images with less bits per dimension lead toreduced accuracies, this already happens for smaller bpd-reductions for more complex datasets likeCIFAR10 than for less complex ones like SVHN.
Figure 6: Dataset condensation accuracies (when retraining with the condensed dataset) vs. datasimplicity. Top: Each dot represents a data condensation experiment run with a particular weightfor the simplification loss, which results in more or less complex datasets. Accuracies can be retainedeven with substantially reduced bits per dimension. In the 1-image-per-class case (top left figure),arrows highlight the settings that are visualized in the bottom figure. Bottom: Condensed datasetswith varying simplification loss weight. Each row represents the whole condensed dataset (1 imageper class), with high (top row) or low (bottom row) bits per dimension. Lower bits per dimensiondatasets are visually simpler and smoother while retaining the accuracy.
Figure 7: Condensed dataset for pleural effusion and gender prediction from chest radiographs inMIMIC-CXR. Condensed images for the classes look very similar. Color-coded mixed rightmostimages reveal the differences between the classes. Green highlighted region at the lower end of thelung consistent with typical radiologic features for pleural effusion (white region indicating fluid onlungs), red highlighted around lung for gender indicate smaller lung volume for the female class.
Figure 8: Post-hoc simplifications of misclassified CIFAR-10 examples. For each, simplified imagereveals plausible causes for the misclassification. We subsequently made alterations to compensatefor the cause (from left to right: removing black dots, removing tree top, removing color, removinghigh frequency texture, removing circle, and removing color), and are able to revert the predictionsto the true class. We also show color-coded saliency maps for expected gradients (Erion et al.,2021) and prediction difference (Zintgraf et al., 2017) for comparison (red: evidence for and blue:evidence against the predicted class). SimpleBits reveals more information than saliency methods.
Figure S1: Visualization of the bits-per-dimension (bpd) measure for image complexity, sorted fromlow to high. Image samples are taken from MNIST, Fashion-MNIST, CIFAR10 and CIFAR100, inaddition to a completely black image sample. bpd is calculated from the density produced by aGlow (Kingma & Dhariwal, 2018) model pretrained on 80 Million Tiny Images, a PixelCNN modeltrained on CIFAR10, and a diffusion model trained on CIFAR10.
Figure S2: Tradeoff between PNG storage space and accuracies. Note that the PNG bpd file sizesdo not show the maximally possible savings, these can be seen from the bpd values in the mainmanuscript.
Figure S3: Learning curves for retraining on simplified images on CIFAR10.
Figure S4: Comparison between SimpleBits and two simpler baselines: In the first one, the simplifiernetwork is trained to simultaneously reduce bpd of the simplified image and the mean squared errorbetween the simplified and the original image. In the second one, gaussian blurring is applied tothe input images, different runs vary in the standard deviation used to create the gaussian blurringkernel. In the third one, we use JPEG compression with varying quality levels. Tradeoff curves areworse for the baselines than for SimpleBits .
Figure S5: Uncurated set of simplified images with 位sim = 2.0, 6 per class.
Figure S6: Uncurated set of correctly predicted simplified images with 位sim = 2.0, 6 per class.
Figure S7: Correctly predicted simplified images with strongest color with 位sim = 2.0, 6 per class.
Figure S8: Selected simplified images that highlight potentially spurious features. Two leftmostimages are correctly predicted, two rightmost images are incorrectly predicted.
Figure S9: Dataset condensation results with varying simplification loss weight. Top: Individualdots represent accuracies for setting with different simplification loss weights. Accuracies can beretained even with substantially reduced bits per dimension. For 1 image per class, arrows highlightthe settings that are visualized below. Below: Condensed datasets with varying simplification lossweight. Per dataset, showing condensed datasets with high (top row) and low (bottom row) bitsper dimension. Lower bits per dimension datasets are visually simpler and smoother while mostlyretaining accuracies.
