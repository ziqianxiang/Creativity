Figure 1: Comparing the accuracy of different attention methods with ResNet-50 and ResNet-101 asbackbone in terms of accuracy and network parameters. The circles reflects the network parametersand FLOPs of different models. Our proposed network achieves higher accuracy while having lessmodel complexity.
Figure 2:Figure 2: A detailed Illustration of DMSANetWe compare our network architecture with Resnet (Wang et al., 2017), SENet (Hu et al., 2018b) andEPSANet (Zhang et al., 2021) in Figure 3. We use our DMSA module in between 3 × 3 convolutionand 1 × 1 convolution. Our network is able to extract features at various scales and aggregate thoseindividual features before passing through the attention module.
Figure 2: A detailed Illustration of DMSANetWe compare our network architecture with Resnet (Wang et al., 2017), SENet (Hu et al., 2018b) andEPSANet (Zhang et al., 2021) in Figure 3. We use our DMSA module in between 3 × 3 convolutionand 1 × 1 convolution. Our network is able to extract features at various scales and aggregate thoseindividual features before passing through the attention module.
Figure 3: Illustration and comparison of ResNet, SENet, EPSANet and our proposed DMSANetblocks.
