Figure 1: For the same magnitude of step taken (same ball radius), a larger norm of parameters leadsto a smaller change in angle.
Figure 2: Zt(S) when training a convolutional network on CIFAR10 and a fully connected networkon MNIST.
Figure 3: Norm of the parameters (layer 3) and norm of the gradient when training a convolutionalnetwork on CIFAR106.2 Generalization bound and test errorWe show in this section the usefulness of considering the normalized loss for bounding the test error.
Figure 4: The bound obtained from the Euclidean distance is much worse than the bound obtainedfrom our normalized distance. However, the generalization bound is still vacuous. The network is a6-layer fully connected network trained on MNIST.
Figure 5: The generalization bound from Theorem 6 (α = 1.0) when training with different amountsof label noise. The network is a 6-layer fully connected network trained on MNIST with SGD. Thefinal test accuracies are: 98.56% (label noise ratio = 0.0), 96.06% (label noise ratio = 0.1), 91.28%(label noise ratio = 0.2) and 84.22% (label noise ratio = 0.3).
Figure 6: ζt(S) when training with different amounts of label noise. The network is a 6-layer fullyconnected network trained on MNIST with SGD.
Figure 7: The generalization bound from theorem 6 (α = 1.0 and α = 0.9) is estimated whentraining with Adam and SGD. The network is a 6-layer fully connected network trained on MNISTwith 10% of label noise. For Adam the test accuracy is 95.38% and for SGD, the test accuracy is96.06%. Those are the best test accuracies that could be obtained after tuning the learning rate foreach respective algorithm.
