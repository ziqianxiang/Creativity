Figure 1: Selcted 256 × 256 video sample generated by HARP on RoboNet (Dasari et al., 2019).
Figure 2: Illustration of our approach. We first train a VQ-GAN model that encodes frames intodiscrete latent codes. Then the discrete codes are flattened following the raster scan order, and acausal transformer model is trained to predict the next discrete codes in an autoregressive manner.
Figure 3: 256 × 256 future frames predicted by HARP on action-free Meta-World (Yu et al., 2020).
Figure 4: 256 × 256 future frames predicted by HARP on action-conditioned RoboNet (Dasari et al.,2019). The model predicts the next ten frames conditioned on the first two frames and the future tenactions. We observe that our model can predict the movement of a robot arm surrounded by variousobjects of different colors and shapes. More videos predicted by HARP are available in Appendix E.
Figure 5: 256 × 256 future frames predicted by HARP trained on Kinteics-600 dataset (Carreiraet al., 2018) using ImageNet pre-trained VQ-GAN model. Videos are licensed under CC-BY and at-tribution can be found in Appendix G. More videos predicted by HARP are available in Appendix F.
Figure 6: Failure cases in our experiments. (a) Interaction with the objects is ignored. (b) The modelrepeats the first frame while a person is moving right in the ground-truth frames.
Figure 7: Action-conditioned HARP.
Figure 8: Test loss during the training of HARP with and without data augmentation on KITTIdriving dataset (Geiger et al., 2013). We observe that the model easily overfits to the training dataset,and data augmentation helps mitigate the overfitting problem.
Figure 10: 64 × 64 future frames predicted by HARP on Meta-World dataset.
Figure 11: 256 × 256 future frames predicted by HARP on RoboNet dataset.
Figure 12: 256 × 256 future frames predicted by HARP on Kinetics dataset.
