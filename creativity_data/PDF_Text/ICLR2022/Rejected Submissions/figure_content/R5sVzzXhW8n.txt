Figure 1: Illustration of different learning paths.
Figure 2: Tsne visualization of the self-supervised encoder outputs f (X) on DogCat (Kaggle),CIFAR10 and CIFAR100 (Krizhevsky et al.). Different colors stand for different classes. SimCLR(Chen et al., 2020) is selected for SSL training. Quality of encoders: (a)	(b)	(c).
Figure 3: (a) (b) (c): Performance of CE on DogCat, CIFAR10 and CIFAR100 under symmetricnoise rate. For each noise rate, the best epoch test accuracy is recorded. The blue line representstraining with fixed encoder and the red line represents training with unfixed encoder; (d): test ac-curacy of CIFAR10 on each training epoch under symmetric 0.6 noise rate. We use ResNet50 forDogCat and ResNet34 for CIFAR10 and CIFAR100. SimCLR is used to deploy SSL pre-training.
Figure 4: In this training framework, We adopt CE for SL training and InfoNCE for SSL training.
Figure 5: Experiments with respect to the regularizer on CIFAR10. ResNet34 is deployed for allthe experiments. (a) (b): Encoder is pre-trained by SimCLR. Symmetric noise rate is 20% and 40%,respectively; (c) (d): Encoder is randomly initialized. Symmetric noise rate is 40% and 60%, re-spectively. The value of hyper-parameters and other detailed setting in the experiments are reportedin the Appendix.
Figure 6: Visualizing decreased gap by down-sampling strategy.
Figure 8: Ablation study of using the regularizer to train DNN on noisy dataset.
Figure 7: Comparing difference choices of distance measure in Equation (4). Type 1 denotes usingl2 norm to calculate distance between SL features and square l2 norm to calculate distance betweenSSL features, which is adopted in our paper. Type 2 denotes using l2 norm to calculate distance forboth SL and SSL features.
Figure 9: Tsne visualization of supervised features before linear classifier on CIFAR10 under 20%,60% and 80% symmetric label noise, respectively.
