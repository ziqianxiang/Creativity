Figure 1: The architecture of hierarchical variational auto-encoder. The HLSM decoder generates aadjacency matrix with latent variables, which are sparsified by the community membership (blackdashed arrows). The GCN encoder learns variables using not only the feedforward priors fromprevious layers (black solid arrows), but also the skipping likelihood of the inputs (orange arcs).
Figure 2: (a) An illustration for the residual decomposition with two influential neighbors (theinfluence strengths for other neighbors are assumed to be 0). The residual for zi can be decomposedas components at the direction of neighbor zj and zk, and rij , rik are the corresponding influencestrengths. (b) An example for the standard and actual distances in a two-dimensional latent space.
Figure 3: Sensitivity analysis for link prediction on the Political blogs dataset. (a) The plain VAE(green) tends to collapse as the number of layers increases, while the performance of DLSM (blue) ismuch more stable and continuously rises until more than 4 layers. (b) The performance of DLSM(blue solid line) reaches peak when v is about 0.9, and is consistently better than that of the ablatedvariant -S (black dashed line).
Figure 4: Visualizations of the latent positions learned on the Emails network using a 2D t-SNEprojection. Points in different colors denote nodes from the ground-truth communities.
Figure 5: Illustration of the community membership with different priors learned on the Emailsnetwork.
Figure 6: Probability density distributions of the degrees and node random factors learned by DLSM.
