Figure 1: We propose a multi-stage approach for learning object-centric generative scene models: (1) Motionsegmentation detects moving objects in the input videos. The predicted (noisy) segmentation masks are usedto (2A) extract object crops for training a generative object model and (2B) extract backgrounds for training agenerative background model. (3) A scene model combines the object and background models to sample novelscenes, permitting interventions on variables such as the background and the number and locations of fish.
Figure 2: The FISHBOWL dataset: each video contains 128 frames with ground truth segmentation; renderingsand masks (without occlusion) of every fish and background are provided in the validation and test sets.
Figure 3: Causal graph for our scenemodel; circles and diamonds denoterandom and deterministic quantities.
Figure 4: Qualitative results from the object model using another object as augmentation. Left: Object modeltrained on the masks predicted by motion segmentation. Right: Object model trained using ground truth seg-mentations. Top: Reconstructions of validation set elements. Middle: Reconstructions of validation set el-ements that are occluded by 0.5 ± 0.05. Bottom: objects sampled from the respective model. The shownreconstructions and samples are not cherrypicked and do not use entropy filtering as in the scene model.
Figure 5: Samples from the training dataset in comparison to the scene model using the background and objectmodels trained on the motion segmentation and ground truth segmentation, respectively.
Figure 6: Interventions on the scene properties of a sample from the unsupervised scene model. Top to bottom:(i) Varying the number of objects, (ii) varying the object identities, (iii) varying the scales of individual objects,(iv) changing backgrounds while keeping objects constant, (v) re-sampling locations of all objects.
Figure 7: The modularity of our multi-stage approach allows to easily exchange individual compo-nents. Here, we replace the image-modeling part of the object model with a GAN instead of thepreviously used VAE. As in our main model, this allows meaningful traversals/interventions sinceobjects and their positions/sizes are presented as individual entities.
Figure 9: Reconstructions from the object model for input crops with different occlusion levels (0.0 = noocclusion, 1.0 = fully occluded). For each occlusion level, the input images and the respective reconstructionsare shown. Both model variants are trained using another fish as artificial occlusion during training. Left:Object model trained using the motion segmentation masks. Right: Object model trained using the groundtruth unoccluded masks.
Figure 10: Samples from the object model using another input object as augmentation during training. Theseare the same models as used for Fig. 4 in the main paper. Left: Object model trained on the motion segmentation.
Figure 11: Input samples to the background β -VAE. Objects were removed by applying an ensembleof foreground-background segmentation algorithms. Images are resized to 96 × 64px to trade-offtraining speed for sample quality.
Figure 12: Two object “reconstructions”. Object latents are obtained from a reference sample. Ourmodular scene model makes it straightforward to vary locations of single objects, exchanging singleobjects, or changing the background without affecting the output sample (top to bottom).
Figure 13: Additional samples from the scene model. Depicted samples use different (reconstructed)backgrounds, samples from the object model are obtained from the standard normal prior and fil-tered with 150 bit entropy threshold at τ = 0.2, sample sizes are constrained on a reference trainingsample, object positions are sampled independently from a uniform prior. Samples are not cherryp-icked.
Figure 14: Conditional samples from the model. Fish appearances (qualitatively, mainly the bright-ness) now vary according to the background sample.
Figure 15: Conditional sampling of object locations based on a reference scene. For matching latentssampling, we extract the object latents and positions from the reference scene using the motionsegmentation model. Samples obtained by matched sampling are more similar to the referencescene than samples obtained by unconstrained sampling from the model priors.
Figure 16: Details of entropy filtering when using the object model. (A1) Distribution of mask entropies fora given foreground model, estimated over 25,000 randomly sampled objects. The distribution typically peaksaround 100 to 200 bits, makeing this a reasonable range for picking the cut-off. (A2) The 128d latent vectorof objects is reasonably correlated with the entropy (R2 = 0.45), making it possible to alter the prior to samplefrom to encourage low entropy samples. (B) 64 samples with the lowest entropy after drawing 1000 randomsamples from the object models and sorting according to entropy. While this strategy encourages some non-plausible, low entropy objects (row 4 and 6), it generally filters the dataset for samples with sharp boundariesand good visual quality. (C) 64 samples with the highest entropy after drawing 1000 random samples from theobject models and sorting according to entropy. With a few exceptions of plausible samples (e.g. in row 2),most of the samples should be rejected in the scene model.
Figure 17: Samples from a baseline GAN (Mescheder et al., 2018) trained on frames from the training set ofthe FISHBOWL dataset. Top: Input frames from the dataset. Bottom: Samples generated by the GAN.
Figure 19: Scenes from the Fishbowl dataset reconstructed by SPACE (Lin et al., 2020b). The model is trainedusing the official implementation provided by the authors with the default parameters. The two variants shownuse grid sizes of 4x4 and 8x8, respectively.
Figure 20: Scenes from the Fishbowl dataset reconstructed by SPACE (Lin et al., 2020b). The glimpse encoderand decoder where trained on the candidate objects given by the motion segmentation using the object modelloss proposed in this work. Afterwards, the remaining components where trained end-to-end using the officialimplementation provided by the authors with the default parameters.
Figure 21: Qualitative results of GENESIS-v2 applied on the FISHBOWL dataset. The reconstruction in thesecond row look somewhat blurry, but capture all major structure in the input images shown in the first row.
Figure 22: Qualitative results of GENESIS-v2 trained on the Fishbowl dataset using the default VAE objectiveinstead of the GECO objective. Sampling from the model works much better in this setting; the model failshowever to segment the scene into the indiviual objects.
Figure 23: Qualitative results when using the GENESIS-v2 object decoder as object model within our modulartraining approach using the same loss and training schedule. Reconstructions and samples look worse than withthe original object model, hinting at the larger capacity of our object model being necessary for our dataset.
Figure 24: Qualitative results when using the GENESIS-v2 object decoder as object model within our modulartraining approach using a larger weight of the KL divergence in the VAE training loss. At the prize of worsereconstructions, the samples from the model can be substantially improved this way.
Figure 25: Results from our model trained on the RealTraffic dataset compared to results from other modelstrained on this dataset.
