Figure 1: Overall strategy for incorporating rotation and permutation equivariance into deep neuralnetworks using attention mechanisms and geometric products. (a) In the simplest form, our proposedstructure uses an attention mechanism over the bond lengths of a cloud of points, each of whichcarries a value as commonly used in graph neural networks. (b) Geometric products can be used tosummarize pairs, triplets, or larger tuples of vectors in a systematic and geometrically meaningfulway. Rotation equivariance can be attained by producing rotation-invariant or -covariant quantities ineach layer of a model. Here, attention over pairs of bonds—represented by two-dimensional tiles—is used. (c) An attention mechanism reduces the set of generated geometric products to enforcepermutation equivariance; the learned attention maps can provide insights into how models operate.
Figure 2: Unit cells of crystal structure prototypes chosen for the structure identification benchmark.
Figure 3: Sample pairwise attention maps for four training data molecules (malonaldehyde, aspirin,benzene, and uracil) after filtering out low-attention pairs. The attention maps indicate how stronglythe pair of atoms joined by the line affect the representation of the atom indicated with a star, withlighter lines indicating greater influence. Qualitatively, more complex bonding environments suchas those on the right tend to have longer-range attention interactions than the simpler environmentson the left.
Figure 4: Performance of rotation-equivariant geometric algebra attention networks compared toa naive transformer-based architecture using dot product attention. Rotation-equivariant modelstypically improve both (a) the speed of convergence during training and (b) final accuracy obtained.
Figure 5: Visual overview of model architectures used in this work. Architectures presented are for(a) crystal structure classification, (b) molecular force regression, and (c) coarse-grained backmap-ping.
