Figure 1: Translation OOD (larger, hollow, shapes with 50 vertices on a 256x256 grid (trained on64x64 grid). Ground truth (GT) and autoencoder (AE) predictions displayed.
Figure 2: Comparison of OOD rotation between AE trained with high diversity and GT acrossmultiple times.
Figure 4: POLARAE architecture: The input image is passed through a polar transform modulewhich consists of polar origin predictor followed by conversion to log-polar space. The obtainedpolar representation is passed through a fully convolutional autoencoder and finally the output ispassed through an inverse transform module to convert the image back to cartesian coordinate space.
Figure 5: Effect of diversity,iteration on IID rotation quality for AE.
Figure 6: Comparison of iteratively trained rotation networks (It = 7, 8, 9) with networks that weretrained to rotate a specified angular distance in a single forward pass. For example, if the networkIt = 9 experienced between 1 and 9 iterative passes during training, then the comparison networkwas trained to rotate an object by 9 minimal rotations (9MR or 95 radians) in a single pass. Forhigher values of iterations/MR, the iteratively trained networks have improved performance overtime, as indicated by MSE, despite rotating shapes using many more steps.
Figure 7: Comparison of OOD rotation quality between ground truth (GT), AE across time.
Figure 8: Comparison of OOD rotation quality between ground truth (GT), VAE, POLARAE acrossmultiple times.
Figure 9: Effect of diversity vs iteration for POLARAE on OOD rotation(b) Location7h⅛⅛?(a) MNIST(b) Shape StructurePOLARAEFigure 10: Comparison of OOD rotation quality between AE, POLARAE across time.
Figure 10: Comparison of OOD rotation quality between AE, POLARAE across time.
Figure 11: Comparison of OOD scaling quality between ground truth (GT), VAE, AE, POLARAEacross multiple times.
Figure 12: Comparison of OOD rotation quality of 3D shapes across time.
Figure 13: Comparison of OOD scaling quality between ground truth (GT), AE across time.
Figure 14: Effect of diversity vsNumber of Iterations (∕t) during trainingNumber of Iterations (∕t) during training(a) Size (Same grid)(b) ShapeFigure 15: Effect of diversity vs iteration for POLARAE on OOD scalingNumber of Iterations (∕t) during trainingNumber of Iterations (∕t) during training(a) Size (Same grid)(b) ShapeFigure 16:	Effect of diversity vs iteration for POLARAE on OOD rotation13Under review as a conference paper at ICLR 2022t=I	t=LO	匕2 O	t=30	匕4。	仁5。9ΞES□QE3国 NUEl ESHt=l	t=10	t=20	t=30	t=40	仁5。POLARAE、、|4E3□QQ
Figure 15: Effect of diversity vs iteration for POLARAE on OOD scalingNumber of Iterations (∕t) during trainingNumber of Iterations (∕t) during training(a) Size (Same grid)(b) ShapeFigure 16:	Effect of diversity vs iteration for POLARAE on OOD rotation13Under review as a conference paper at ICLR 2022t=I	t=LO	匕2 O	t=30	匕4。	仁5。9ΞES□QE3国 NUEl ESHt=l	t=10	t=20	t=30	t=40	仁5。POLARAE、、|4E3□QQFigure 17:	Comparison of OOD rotation on hollow shapes.
Figure 16:	Effect of diversity vs iteration for POLARAE on OOD rotation13Under review as a conference paper at ICLR 2022t=I	t=LO	匕2 O	t=30	匕4。	仁5。9ΞES□QE3国 NUEl ESHt=l	t=10	t=20	t=30	t=40	仁5。POLARAE、、|4E3□QQFigure 17:	Comparison of OOD rotation on hollow shapes.
Figure 17:	Comparison of OOD rotation on hollow shapes.
Figure 18: Comparison of OOD scaling on hollow shapes.
Figure 19: Top: Mean squared error (MSE) on I.I.D. test set at each time step for 9 trained rotationnetworks. The label ‘It. = N’ indicates that during training, the number of iterations of the newtorkon a given batch was sampled uniformly between 1 and N. Bottom: the same plot blown up tovisualize the first 6 time-steps. Of note, networks trained with higher iterations actually have aworse MSE at the first time step, but achieve a much better MSE in the long run. Confidenceintervals represent standard error between 3 identically trained networks.
