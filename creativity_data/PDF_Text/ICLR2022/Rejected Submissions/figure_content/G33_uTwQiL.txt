Figure 1: An overview of EVFN. For a many-body system X, we first centralize the positions topreserve translation equivariance. Then we introduce a tuple of edge-level complete basis Fij totransform the geometric tensors xi into SO(3)-invariant scalars. Afterwards, the scalar embeddingstij , pre-defined node features hi and edge features eij are fed to the Graph Transformer to learnedgewise embeddings mij . Finally, a vectorization block transforms the edgewise embeddings intonodewise vector fields Vi .
Figure 2:	Results of GCN with various data augmentation degrees on the POS datasetThe Impact of the Number of Training samples To further explore the impact of the numberof training data for both non-equivariant and equivariant models, we re-train GCN and EVFN withthe different number of training samples on the POS dataset and summarize the results in Figure3. In particular, under the same data splitting setting described in Section 4.1, we choose to samplethe trajectories with different time intervals {10-2,1.5 ∙ 10-3, 5 ∙ 10-4,3 ∙ 10-4}, correspondingto 150, 1000, 3000 and 5000 training labels, respectively. The time interval of the test set remainsthe same as before (i.e., 10-3). Note that we avoid introducing new trajectories into the datasetbecause it may influence the problem complexity of the original interpolation and extrapolationtasks. As shown in Figure 3 (a) and (b), with the number of samples growing, the interpolationperformance of GCN becomes better but its extrapolation becomes worse, indicating that the GCN-based neural ODE may easily overfit to the interpolation task. Compared to GCN, EVFN achievesmore robust performance to the different number of training samples. As shown in Figure 3 (c)and (d), introducing more training samples in the original coordinate frame cannot enhance theequivariant capacity of GCN efficiently, which demonstrates that SO(3) augmentation is a morereasonable choice to realize equivariance for non-equivariant methods.
Figure 3:	Results for different number of training trajectories on the POS datasetTable 5: The impact of different normalization constants. SE(3)-T denotes the SE(3)-Transformermodel.
Figure 4: Visualizations of generated conformations. For each molecule randomly selected fromGEOM-Drugs dataset, we sample multiple conformations and show the best-aligned ones with thereference ones.
