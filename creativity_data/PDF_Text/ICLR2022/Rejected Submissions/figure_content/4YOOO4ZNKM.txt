Figure 1: (a) The contrastive SSL framework with model augmentation. We apply the model aug-mentation to the encoder, which constructs two views for contrastive learning. (b) the neuron mask-ing augmentation. We demonstrate the neuron masking for the Feed-Forward network. (c) thelayer dropping augmentation. We add K FFN layers after the encoder and randomly drop M lay-ers (dash blocks) during each batch of training. And (d) the encoder complementing augmentation.
Figure 2:	Performance comparison betweeen SASRec and SRMA in HR@5 and NDCG@5 w.r.tdifferent values of neuron masking probability p on Sports and Toys dataset.
Figure 3:	The NDCG@5 performance w.r.t. different K and M for layer dropping augmentation onSports and Toys dataset.
