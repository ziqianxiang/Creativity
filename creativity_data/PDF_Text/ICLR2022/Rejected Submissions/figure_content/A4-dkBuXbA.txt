Figure 1: Excitation and squeeze density plot showing weight distribution across all trials for par-ticipant 3 with number of time slices P = 6 where attention weights (x-axis) were calculated usingEq. 5 and y-axis represents the density of the distributionZooHO9HSS 或 090jgX09Zo0 寸 O9Zκj 寸。9Ztn b 09Zeo∙TG9Z8"T09∣∙M8"T09ZOO"T09ZO□∙T09Z8∙T09ZC□∙T09IIZmm寸寸599ZZg66三三二胃胃口雪苫日日胃胃二月黑已雪月目^丹丹田寓寓IqRR fjjf;J田田用宗宗初以斛盅盅两两幅温格精益兄我留SamplesFigure 2: Example heatmap produced by the guided grad-CAM looking at the first convolutionallayer, corresponding to participant 3 imagining a right hand movement (ω = 400 and S = 500);note that the features were encoded in a time invariant manner as discussed earlier7Under review as a conference paper at ICLR 20224 DiscussionIn this study, two different window sizes (ω = 200 and ω = 400) using either S = 500 or S = 1000(2s or 4s intervals) were supplied to the model for training and testing, and results reported are forthe unseen test data. The model achieved mean κ = 0.60 ± 0.02, representing performance thatexceeds state-of-art CRAM (κ = 0.45), and the winner of BCI IV 2A competition (κ = 0.57), assummarized in Table 2. The baseline yielded identical performance using ω = 200 or ω = 400 withσ = 50 and S = 500 (Table 1: Baseline), implying that classification could be performed twiceas quickly since it used half as much data (i.e., window size 400 over a 4s interval would require adelay of 4s and an input of 1.6s input per one step of classification, compared with a 2s delay and
Figure 2: Example heatmap produced by the guided grad-CAM looking at the first convolutionallayer, corresponding to participant 3 imagining a right hand movement (ω = 400 and S = 500);note that the features were encoded in a time invariant manner as discussed earlier7Under review as a conference paper at ICLR 20224 DiscussionIn this study, two different window sizes (ω = 200 and ω = 400) using either S = 500 or S = 1000(2s or 4s intervals) were supplied to the model for training and testing, and results reported are forthe unseen test data. The model achieved mean κ = 0.60 ± 0.02, representing performance thatexceeds state-of-art CRAM (κ = 0.45), and the winner of BCI IV 2A competition (κ = 0.57), assummarized in Table 2. The baseline yielded identical performance using ω = 200 or ω = 400 withσ = 50 and S = 500 (Table 1: Baseline), implying that classification could be performed twiceas quickly since it used half as much data (i.e., window size 400 over a 4s interval would require adelay of 4s and an input of 1.6s input per one step of classification, compared with a 2s delay and0.8s input per one step of classification), representing slightly better than the current real-time EEGbased classification applications. A further advantage of shorter windows and intervals is that lessmemory is required to store the signals, further increasing the potential for its use in real-time orembedded BCIs. The data augmentation method proposed, in which extra training samples weregenerated by exchanging the first and last 2s intervals (note: this was not done to the final testingset used to evaluate the model’s performance) led to the highest classification accuracy, with a mean
