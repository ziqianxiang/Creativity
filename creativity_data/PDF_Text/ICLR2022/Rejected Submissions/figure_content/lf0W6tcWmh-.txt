Figure 1: Test accuracy obtained with Resnet-18 (R18) and WideResnet16 (WR16) on CIFAR-10 and CIFAR-100. The architectures are trained using GD/GD+M (a) and SGD/ SGD+M (b) for 300 epochs to ensure zerotraining error. (c)-(d) respectively display the training loss and test accuracy by R18 with GD/GD+M onCIFAR-10. To isolate the effect of momentum, we turn off data augmentation, dropout and batch normalization.
Figure 2: DataSet equation (D) 2D. EaCh data-point is Xi = [ci ∙ w*, di ∙ n] ∈ R4 for some Ci ,d ∈ R. Weproject these points in the 2D space (SPan(w*), span(n)). The feature is w* and the noisy patch is in span(n).
Figure 3: (a): Training loss (b) test accuracy on large margin data and (c) test accuracy on the small margin datain the synthetic setting. While GD and GD+M get zero training loss, GD+M generalizes better on small margindata than GD. Setting: 20000 training data, 2000 test data, d=30, number of neurons=5, number of patches=5.
Figure 4: Training (a) and test (b) accuracy obtained with Resnet-18 on CIFAR-10 dataset with artificiallygenerated small margin data. The architectures are trained using GD/GD+M for 300 epochs to ensure zerotraining error. Data augmentation, dropout and batch normalization are turned off. (SM) stands for the testaccuracy obtained by the algorithm on the small margin data. Results are averaged over 5 runs with bestscheduled learning rate and weight decay for each individual algorithm separately.
