Figure 1: Select an activation with DartsIn this section, we introduce related techniques briefly.
Figure 2: ForWard propagation under α in DHPO.
Figure 3: Deep convolution neural network to be tunedIn the experiment, we apply DHPO to two classical models, i.e. deep convolutional neural networksand MLp(multi-layer perceptron), which include various kinds of hyper-parameters. DNN(deepconvolution neural network) is a lightweight convolution network designed with reference to Kuni-hiko & Fukushima (1980); Lecun & Bottou (1998); Behnke & Sven (2003). Even though DNN isnot the best model to process images, it is a good network which can distinguish the performence ofvarious HPO approaches since it has a large number of hyper-parameters. DNN’s network structureis as shown in Figure 3, which is a lightweight implementation. And MLP is composed of 4 fullyconnected layers. DNN is used to evaluate HPO for convolution layer and pooling layer. MLP isused to evaluate HPO for fully connected layer.
Figure 4: Loss sequence in the training sessionAs shown in Figure 4, DHPO shows a better loss sequence on MNIST, SVHN and AGARICUS. Itcan be seen that the loss of DHPO is not dominant at the beginning, and then slowly drops to thelowest of all models. This corresponds to the two stages of DHPO in the actual training process.
Figure 5: Loss sequence comparison in 500 epochs training.
Figure 6: case study on MNISTIn some singleton of DHPO, five groups of hyper-parameters are given out by DHPO along with thetraining session. We evaluate these five groups of hyper-parameters on DNN chronologically andvisualize the performances in Figure 6. The later hyper-parameters have stronger convergence capa-bilities according to the loss. Performance(accuracy mean) and stability(accuracy std) under thesehyper-parameters also shows an increasing trend. As the training session progresses, the quality ofhyper-parameters gradually increases, which demonstrates that DHPO is an effective HPO method.
Figure 7: The forward propagation of pooling layerWe take Maximum pooling and average pooling as candidates. We will prove Darts is effective inthis occasion. θpooitype ∈ {MaxPool, AvgPool}. α ∈ R2. According to Darts, the output of thispooling layer is calculated as formula 2 visualized in Figure 7.
