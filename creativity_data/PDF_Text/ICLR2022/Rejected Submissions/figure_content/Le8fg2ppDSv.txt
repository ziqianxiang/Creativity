Figure 1:	Examples of generated summaries for a Newsroom article using both BART and a 2-decoderHYDRASUM model. Longer copied sequences (denoting extractive behavior) are underlined. For HydraSum,summaries from different mixtures of decoders differ in degree of abstractiveness, specificity, and length.
Figure 2:	HYDRASUM's training and generation workflow for a 2-decoder version. We consider two trainingsettings, namely guided and unguided, which differ in how the gate weights used to combine different decodersare obtained. During generation, summaries can be sampled from individual decoders or their mixture.
Figure 3: Our proposed HYDRASUM architecture. The decoder network incorporates multiple decoders and agating mechanism is used to combine their output probabilities in a mixture-of-experts formulation.
Figure 4: Graphs plot the 2gram overlap of the baseline and HYDRASUM decoders. Compared to the baseline,D0 decoder samples summaries from a distribution that more closely resembles the reference distribution.
Figure 5: Examples of generated summaries using the unguided setting for Newsroom dataset. Long extrac-		tive sequences are underlined, additional details that increase the specificity of summaries are in bold.		observation from Table 1, D1 generates a highly extractive summary whereas D0 generates an ab-stractive summary with less copying. In the second example, We observe a difference in specificityof the generated summaries. D0 summary includes additional details like Jenson Button's professionand his wife,s name, compared to the more less specific summary generated by D0.
Figure 6: 2gram overlap and specificity of CNN summaries generated using different values of g in the guidedsetting. The top graphs are obtained by mixtures of decoders from the abstractiveness-controlled model; thebottom graphs are from the specificity-controlled model. These graphs provide evidence that properties likeabstractiveness and specificity can be varied by varying the gate probabilities in the guided setting.
Figure 7: 2gram overlap and specificity of CNN and Newsroom summaries generated using a combinationof specificity-controlled and abstractiveness-controlled decoders.
Figure 8: 2gram overlap and specificity of Newsroom and XSum summaries generated using differentvalues of g in the guided setting. The graphs show that properties like abstractiveness and specificity can becontrolled by sampling from a mixture of the 2 decoders corresponding to the chosen style.
Figure 9: Example of multi-feature control by HYDRASUM		Baseline Bart	HYDRASUM D0	HydraSum D1	HydraSum MixCNN	4.4/4.4/4.2/.88	4.3/4.4/4.2/.86	4.3/4.3/4.0/.89	4.4/4.3/4.2/.87Newsroom	4.3/4.4/4.2/.9	4.4/4.4/4.2/.92	4.2/4.4/4.1/.91	4.4/4.5/4.3/.90XSUM	4.3/4.4/4.2/.77	4.3/4.3/4.2/.81	4.1/4.4/4.2/.81	4.2/4.5/4.3/.80Table 11: Comparison of human-rated Relevance/Coherence/Grammaticality/Factuality scoresof HydraSum models under the unguided setting and the baseline Bart model.
Figure 10: Interface of the Mechanical Turk Tasksummarization model, using control variables like keywords (He et al., 2020), queries (AbaCha et al.,2021), or even explicit fine-grained content plans e.g. entity-chains (Narayan et al., 2021; Elsaharet al., 2021). These approaches primarily provide ’content-plans’ as additional textual inputs to themodel, and hence there task formulation differs significantly from our paper. Instead, we focus on adifferent subset of surface-level summary properties, like length, specificity, readability, etc. that donot target content selection per se. Recently, GeDi (Krause et al., 2021) proposed using small LMsas generative discriminators for specific attributes (e.g. toxicity) to guide the generation of largermodels. Similar class-conditional language models approaches (CC-LMs) have been previouslyproposed (Keskar et al., 2019; Ficler & Goldberg, 2017) to fine-tune models on specific attributes.
