Figure 1: Losses along the lines of the SGD training processes exhibit a parabolic shape. Theloss of the direction defining mini-batch (green) is excluded from the distribution of mini-batchlosses to show that it is significantly different. This makes line searches on it unfavorable. Inaddition, the parabolic property articulates stronger for the full-batch loss (red); thus, this workaims to approximate it efficiently with a parabola. The figure is created with code and data fromMutschler & Zell (2021).
Figure 2: Several metrics to compare update step strategies on the full-batch losses along 10,000lines measured by Mutschler & Zell (2021): 1. update step sizes, 2. accumulated loss improvementper step given as: l(0) - l(supd) where supd is the update step of a specific optimizer. This is thelocally optimal improvement to the minimum of the full-batch loss along a line. The right plot showsalmost proportional behavior between the optimal update step and the negative gradient norm of thedirection defining mini-batch loss. The LABPAL&SGD version of our approach performs almostoptimal on ground truth data. Results LABPAL&NSGD are almost identical and thus omitted.
Figure 3: Training process on the problem of which the empirical observations were inferred (ResNet-20 trained on 8% of CIFAR-10 with SGD). LABPAL&NSGD and LABPAL&SGD outper-form SGD. Interestingly LABPAL&NSGD estimated huge λs, whereas supds are decreasing6Under review as a conference paper at ICLR 2022terestingly huge λs of up to 80, 000 are estimated, whereas supds are decreasing. LABPAL&SGDshows similar performance as SGD; however, it seems beneficial to ignore gradient size informationas the better performance of LABPAL&NSGD shows.
Figure 4: Performance comparison on CIFAR-100 of our approach LABPAL in the SGD and NSGDvariants against several line searches and SGD. Optimal hyperparameters for CIFAR-10 found witha detailed grid search are reused (Appendix G.1). Here, our approaches surpass the other approacheson training loss, validation, and test accuracy. Columns indicate different models. Rows indicatedifferent metrics. Results for CIFAR-10, ImageNet and SVHN are given in appendix Figures 8, 9and 10. The batch-size used is 128.
Figure 5: Performance comparison at batch size 10 on CIFAR-10. Left: reusing the same hyper-parameters as for batch size 128. Right: applying the directly estimable noise adaptation factor tothe LABPAL approaches and performing a grid search for optimal hyperparameters for the otherapproaches. On validation and test accuracy the LABPAL approaches outperform SGD, whereas ontraining loss they compete with it. (For more details see Appendix Figure 11 & 12). PLS curves areincomplete since training failed.
Figure 6:	Sensitivity analysis of parameters of LABPAL&SGD. ThePAL&NSGD described in Figure 7 are also valid for LABPAL&NSGD.
Figure 7:	Sensitivity analysis of parameters of LABPAL&NSGD. The default parameters are: ap-proximation batch size Ba = 1280, SGD steps s = 1000, step size adaptation α = 1.8, batch sizeschedule k = (0:1, 75000:2, 112500:4), momentum β = 0, maximal step size = 1.0, noise-factor= 1. For Ba the factor 128 is multiplied with is given on the x axis.
Figure 8:	Performance comparison on CIFAR-10 of our approach LABPAL in the SGD and NSGDvariants against several line searches and SGD. Optimal hyperparameters are found with an elaborategrid search. Our approaches challenge and often outperform the other approaches on training loss,validation, and test accuracy. Columns indicate different models. Rows indicate different metrics.
Figure 9:	Performance comparison of top 1 error on IMAGENET of our approach LABPAL inthe SGD and NSGD variants against SLS and SGD. Optimal hyperparameters are found with anelaborate grid search. Optimal hyperparameters found with a detailed grid search for CIFAR-10 arereused. Our approaches challenge the other approaches on training loss and test accuracy. SLS failsshortly after the beginning of the training due to too high estimated learning rates. For LABPAL theadaptation factor introduced in Section 4.3 is applied.
Figure 10: Performance comparison on SVHN of our approach LABPAL in the SGD and NSGDvariants against several line search and SGD. Optimal hyperparameters found with a detailed gridsearch for CIFAR-10 are reused. Our approaches challenge and often surpass the other approacheson training loss, validation, and test accuracy. Columns indicate different models. Rows indicatedifferent metrics.
Figure 11: Performance comparison of several models on CIFAR-10 with batch size 10. The samehyperparameters are used as for batch size 128 (see Figure 8). PAL and PLS fail in this scenario.
Figure 12: Performance comPariSon of Several modelS on CIFAR-10 with batch size 10. For theLABPAL aPProacheS only the noiSe factor iS adaPted according to equation 7. For all other aP-ProacheS, a grid Search iS Performed to find the beSt hyPerParameterS for thiS Scenario. (See AP-Pendix G.1). In comPariSon to Figure 11, now, the LABPAL aPProacheS Perform comPetitive onthe training loSS and SurPaSS the other aPProacheS on validation and teSt accuracy. PLS PlotS areincomPlete Since the training failed after Some StePS.
Figure 13: Left: Training time comparison on CIFAR-10. SGD, SLS, and PAL show similartraining times. GOLSI, and both variants of LABPAL are slightly slower (up to 19.6%). However, aslightly longer training time is acceptable if less time has to be spent in hyper-parameter tuning. PLSis significantly slower. Note that in comparison to SGD, the implementations of the other optimizersare not optimized on CUDA level. Right: Maximum allocated memory comparison on CIFAR-10.
