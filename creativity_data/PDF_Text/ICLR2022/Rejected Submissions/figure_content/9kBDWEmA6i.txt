Figure 1: The model trained on DS1 ∪DS2 (green) exhibits poor validation metrics on the validationsets of DS1 and DS2 taken individually. It does not outperform the baseline trained on DS1 (blue)on test set DS3. Its predictions are shifted towards low values on benign and even malignant images.
Figure 2: Selection of samples from 4 DSs, DS0, DS1, DS2, DS3, while alternating the labels 0, 1within a DS. A sampling pattern of length Nmin = 37 was used to select the samples. It was builtfrom the numbers of samples having the rarest labels (1 in this case) in DSi： nFn for i = 0, •…3.
Figure 3: Validation metrics of DNNs trained on 2 DSs (S1 , Ma) and (S2, Ma). First column:global metrics. Second column: local metrics on DS (S1, Ma). Third column: local metrics on DS(S2 , Ma ). First row: WLL / image. Second row: AUC per image.
Figure 4: Global and local ROC curves on the validation sets DSs (S1, Ma) and (S2, Ma) forDNNs trained with: ”Bal. labels”, ”Bal. Segments” and ”P. sampling” (left to right). In black theglobal ROC curve is presented. In blue (resp. green) the ROC curves obtained on the validation setsof DSs (S1, Ma) (resp. (S2, Ma)). OPs at threshold 0.2, 0.4, 0.6, 0.8 are added on each ROC curve.
Figure 5: Full model architecture composed of a Backbone and a classifier. The Backbone is incharge of building a meaning full feature representation. The classifier is in charge of predictingimage wise probability of malignancy. The Backbone is first pre-trained to classify (224, 224)patches into 5 classes: healthy tissue, benign / malignant soft tissue lesion, benign / malignantcalcification. The whole model is then fine-tuned to classify (1152, 832) mammography images into2 classes: benign and malignant. The number of convolution kernels of each convolution block aregiven as well as the shapes of the intermediate feature maps.
Figure 6: Global and local ROC curves on: (S3, Ma) and (S3, Mb). Black: global ROC curve onS3, Ma) ∪ (S3, Mb). Blue (resp green): ROC curve on (S3 , Ma) (resp. (S3, Mb)).
