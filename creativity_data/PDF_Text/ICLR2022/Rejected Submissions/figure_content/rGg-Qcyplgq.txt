Figure 1: Illustration of the N-Chain environment starting from state s2 . To emphasize the stochas-ticity, the reward of state s4 was set as a mixture model composed of two Gaussian distributions.
Figure 2: (a) Empirical return distribution of DLTV during training in N-Chain environment. Thedashed lines denote the exact mean, and the dots on the x-axis denote the perturbed mean of eachaction. No-op actions are not shown for visibility. (b) Total count of performing true optimal action.
Figure 3: (Left) Empirical return distribution plot in N-Chain environment. Since QR-DQN doesnot depend on other criterion, the dots are omitted. (Right) Mean and standard-deviation differencebetween each algorithm and ground truth distribution.
Figure 4: (a) Evaluation curves on LunarLander-v2. All curves are the average of three random seedsand the shaded area represents the standard deviation. We smoothed the curve over 5 consecutivesteps. (b) Hitting time to reach a given threshold.
Figure 5: Evaluation curves on Atari games. We smoothed all curves over 10 consecutive steps withthree random seeds. In case of Pong-v4, we resize the x-axis, since it can easily obtain the optimalpolicy with few interactions due to its environmental simplicity.
Figure 6: Pipeline of PDBOO.
