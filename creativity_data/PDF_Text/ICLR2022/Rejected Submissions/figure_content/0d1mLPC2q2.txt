Figure 1: Test error rate of ResNet20 on CIFAR-100 when trained for different numbers of epochs(the teacher is ResNet56 for KD). Each result isobtained by averaging 3 random runs (shaded areaindicates the std) “Flip” refers to horizontal flip;“Crop” refers to random crop. Both are standarddata augmentation schemes in classification. Theoptimal number of training epochs and its test er-ror are highlighted in red.
Figure 2: Interplay between knowledge distillation (KD) and data augmentation (DA). (a) Illustra-tion of the difference of supervised target between the KD loss and cross-entropy (CE) loss. Aninput is transformed to different versions (called views in this paper) owing to data augmentation.
Figure 3: Test error rate of WRN-16-2 and VGG8 on CIFAR-100 when trained for different numbersof epochs, using KD or cross-entropy (CE) loss, with or without data augmentation (DA). Everyerror rate is averaged by 3 random runs (shaded area indicates the stddev). Consistent with Fig. 1,when DA is used, the optimal number of epochs is postponed and postponed more for KD than CE.
Figure 4: Mean KL divergence ratio r (Eq. (4))over iterations on different datasets. The itera-tions are normalized into range [0, 1] for easycomparison since the total numbers of iterationsare different on the 3 datasets.
Figure 5: ImageNet CutMix samples where the main object in one of the images is no longer vis-ible after CutMix augmentation. Below each sample, the first is the target probability assigned byCutMix and the second is the top-5 predicted probabilities by the teacher. These examples can bemisleading when cross-entropy loss is used, but not for KD, as explained in the text.
