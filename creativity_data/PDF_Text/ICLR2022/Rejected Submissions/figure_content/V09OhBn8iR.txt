Figure 2: Accuracy vs. |m|/|D|. Figure 3: Confusion matrices for |M|/|D| = 99% and 95%.
Figure 4: Loss landscapes of the model trained on the biased training set. (a) and (b) represents theper-sample and aggregated loss landscapes. Top and bottom row depict the results of the case. Thered arrows are the convergence point and minimal point. We draw the loss landscapes based on ∣∣Vθk.
Figure 5: Acc. drops.	Figure 6: Entropy vs Loss.
Figure 7: Examples of MNIST variants (Left: CM, Right: WM). Figure 8: Cartoon.
Figure 9:	Simple convolutional networks for the variants of the MNIST task and Cartoon data.
Figure 10:	De-noising module and threshold plots for various conditions. Data= CM,Noise={0%, 1%, 10%} and Bias={99.5%, 1%, 5%, 10%}.
Figure 11:	De-noising module and threshold plots for various conditions.
Figure 12:	Hyperparameter sensitivity. We train models on CM With diffculty 0.0001 under learningrate η ∈ {0.005, 0.01, 0.02} and epoch e ∈ {50, 100}.
Figure 13:	Accuracy on each (color, label)-pair. Red color corresponds to bad accuracy, While bluecolor denotes higher accuracy.
Figure 14: Pairwise sampling probability under colored MNIST with 0.0001 difficulty. Blue colorindicates higher sampling probability.
Figure 15: Plots of class activation map (CAM) for each method of same inputs, sampled from theMajor (M) and Minor (m) sets.
