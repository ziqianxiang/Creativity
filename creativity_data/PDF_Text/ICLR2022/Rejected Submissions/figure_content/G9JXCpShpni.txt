Figure 1: The iterated batch MBRL loop. Preai : (st, at) 2 ，右十］is the real system (So Line 7is What dominates the cost) andP : (st, at) 2 st+1 can be the real system or the system model inROLLOUT. S0 is the distribution of the initial state of the real system. π(0) : st 2 at is an initial(typically random) policy and in ROLLOUT π : st 2 at is any policy. N is the number of episodes;T is the length of the episodes; L is the planning horizon and n is the number of planning trajectoriesused by the actor policies π(τ). τ = 1, . . . , N is the episode index Whereas t = 1, . . . , T is thesystem (or model) access step index. Learn is a supervised learning (probabilistic or deterministictime-series forecasting) algorithm applied to the collected traces and Actor is a Wrapper of thevarious techniques that We experiment With in this paper (Fig 2). An Actor typically updatesπ(τ -1) using the model P(τ) in an n × L planning loop, but it can also access the initial policy π(0)and the trace ∪ττ0=1T(τ0) collected on Preal up to episode τ.
Figure 2: Model-based Actors (policies executed on the real system). RSACTOR is a classicalrandom shooting planner that uses the random policy π(0) for all rollouts. Since the random policyhas no value estimate, only TotalReward can be used as Value in Fig 3/Line 3 of Heteroge-NEOUSRS. GUIDE&EXPLOREACTOR first learns a Dyna-style guide policy ξ on the model p (moreprecisely, updates the previous guide contained in πprev). It can also use the traces T collected onthe real system. It then “decorates” the guide by (possibly n different) exploration strategies (Fig 4),and runs these reactive guide&explore policies [ρi]in=1 in the HETEROGENEOUSRS planner, eitherusing the raw TotalReward or the total reward bootstrapped by the value estimate of the guidepolicy ξ (BOOTSTRAP) in Fig 3/Line 3. ALPHAZEROACTOR calls ALPHAZERO which plans andexplores internally, using Monte-Carlo tree search, with a budget of L × n simulator calls.
Figure 3: Value estimates on rollout traces and HeterogeneousRS: random shooting witha set of policies. TOTALREWARD and B OOS TRAP are two ways to evaluate the value of a rollouttrace. The latter adds the value of the last state to the total reward, according to a value estimateV : s → R+, weighted by a hyperparameter α. They are called in Line 3 of HETEROGENEOUSRSwhich is a random shooting planner that accepts n different shooting policies [ρi]in=1 for the nrollouts used in the search. As usual, it returns the first action a* = T(i*)[1,2] of the best traceT (i*) = ((s*, a*),..., (ST, aT)). Its parameters are the shooting policies [ρi]n=ι, the model p, andthe number n and length L of rollouts, but to properly define it, we also need the state s which weplan from, so we use a double argument list ()[].
Figure 4: Exploration strategies. HEATINGEXPLORE heats the guide action distribution ξ(a∣s)to n different temperatures, and EPSGREEDYEXPLORE changes the best action to a random ac-tion ∏(0) 2 a with different probabilities. The temperatures [Ti]n=ι and probabilities [εi]n=ι arehyperparameters.
Figure 5: Learning curves obtained with different agents. Mean reward is between 0 (hanging) and4 (standing up). Episode length is T = 200, number of epochs is N = 100 with one episode perepoch. Except for DynaZero, mean reward curves are averaged across ten seeds. Areas withlighter colors show the 90% confidence intervals.
Figure 6: Mean asymptotic rewards (MAR) of DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε)obtained for different values of ε. Error bars and areas in lighter colors represent the 90% confi-dence intervals. When ε is close to 0 the performance is close to DQN while for ε close to 1 theperformance is close to the RSACTOR agent. Our multi-ε exploration strategy is able to select thebest ε automatically and dynamically.
Figure 7: Performance obtained with RSActor on the real Acrobot system for different planninghorizons L and number of generated rollouts n. The plot shows the mean rewards obtained for sev-eral randomly initialized episodes of 200 steps. The error bars give the associated 90% confidenceintervals. Note that since Acrobot has a discrete action space with three actions, the total numberof different rollouts for h = 10 is n = 310 = 59,049. The performance shown for h = 10 andn = 100,000 thus only requires n = 59,049 rollouts.
