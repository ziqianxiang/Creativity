Figure 1: We show examples for a good (green) and a bad (red) score for each metric. (a) sanesaliency depends on network parameters, tested by randomizing weights and biases. (b) faithfulsaliency correlates with predictive accuracy, tested by perturbing the input sequence. (c) sensitivesaliency of the predicted class in one sample is different from others. (d) for robust saliency smallchanges to input data cause only small effects. (e) stable saliency of all class samples has a lowvariance and standard deviation. (f) saliency should be localized on the predicted segment.
Figure 2: Pairwise Pearsoncorrelations of scores (alsoplotted in Fig. 7b). Every met-ric is independent of all others.
Figure 3: Results for the localization metric on the tooltracking dataset and with all methods on U-time. Weshow detailed values for the localization metric in (a)and examples of saliency maps in (b).
Figure 4: (a): shows aggregated results for TCN and FCN over all classification datasets withoutnormalization ( as we found that model architectures generally lead to similar scores); (b): shows theinfluence of datasets on the scores, when we aggregate for each dataset over {model architectures,methods}. Similarity metrics for saliency maps yield different results in different domains.
Figure 5: Scores as a heatmap for TCNon FordB. Columns with high relative scores(bright squares) indicate good visualizations.
Figure 6: We show the localization metric results for all methods on the tool tracking dataset andwith the bi-LSTM model. (a) shows the results for the localization metric. (b) shows exemplarysaliency maps.
Figure 7: (a) shows scores separately over all {model, method } combinations. (b) removes thedatasetsâ€™ bias in order to assess the performance of individual visual interpretability metrics inde-pendent from datasets. (c) shows results for different model architectures, aggregated across datasetsand visual interpretability methods. Model architecture has a comparatively small influence on theperformance, which speaks to their generalization capabilities.
Figure 8: Pairplots of the correlations of metric scores with each other. No combination of metricshas a meaningful correlation with each other, proving that they provide independently useful signals.
Figure 9: Examples of saliency maps for the TCN architecture on the FordB dataset for class 0.
Figure 10: Examples of saliency maps for the TCN architecture on the FordB dataset for class 1.
