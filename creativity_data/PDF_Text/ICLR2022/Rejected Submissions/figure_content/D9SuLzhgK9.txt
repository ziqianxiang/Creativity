Figure 1: Gradient ratio against FID score (a) and number of epochs (b) obtained with DCGAN on CIFAR-10.
Figure 2: (a) FID score against the number of epochs of a Resnet WGAN-GP trained on CIFAR-10 withAdam, AdaLR, and AdaDir. AdaLR performs slightly better than Adam while AdaDir performs very poorly.
Figure 3: FID score of a Resnet WGAN-GPmodel trained with l-nSGDA, g-nSGDA, Adam,and SGDA against the batch size in the CIFAR-10dataset. At small batch sizes, the best perform-ing models are those trained with nSGDA meth-ods. As the batch size increases, the performanceof nSGDA methods worsens and Adam performsbetter. Lastly, models trained with nSGDA con-sistently outperform those trained with SGDA.
Figure 4: FID scores obtained when training a Resnet WGAN-GP using Adam, l-nSGDA, g-nSGDA, andSGD on different datasets. In all these datasets, l-nSGDA, g-nSGDA and Adam perform approximately aswell. SGDA performs much worse. The models are trained with batch-size 64.
Figure 5: Learning process of SGDA (a,b,c) and learned modes by nSGDA (d) with mD = mG = 5, d =1000. We see that in the nSGDA model, each neuron solely learns one of the modes.
