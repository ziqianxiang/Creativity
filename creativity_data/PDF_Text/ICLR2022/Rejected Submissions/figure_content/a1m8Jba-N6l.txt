Figure 1: We train a fully-connected network on three synthetic datasets for binary classification,with 1-mixup through 32-mixup regularization (α = 1). The plotted functions show the networkoutput and demonstrate that higher k-mixup better captures local structure (visible through less blur,increased contrast) while retaining reasonable, even smoothing between the classes.
Figure 2: Optimal transport couplings and vicinal datasets for k = 1 (left) and k = 32 (right) in 3simple datasets. In the bottom row, α = 1 was used to generate vicinal datasets of size 512.
Figure 3: Test accuracy on toy datasets, averaged over 5 Monte Carlo trials.
Figure 4: Test accuracy on UCI datasets using fully connected networks.
Figure 5:	Results for MNIST with a LeNet architecture (no mixup performance: 99.0%), averagedover 20 Monte Carlo trials (±.02 confidence on test performance). Note that k-mixup doubles theimprovement of 1-mixup over ERM.									k	α = .05	a = . 1	a = .2	a = .5	a = 1	α = 10	α = 100	1	94.785	95.025	95.27	95.645	95.63	94.82	94.085	2	94.8	94.92	95.285	95.645	95.79	95.105	94.14	4	94.76	94.99	95.3	95.65	95.745	95.205	94.315	8	94.79	94.925	95.255	95.625	95.815	95.285	94.61	16	94.68	94.98	95.215	95.595	95.735	95.33	94.92	32	94.74	94.905	95.11	95.465	95.675	95.375	95.165(a)	Performance (test accuracy)k	α = .05	α=.1	a = .2	a = .5	a = 1	α =10	α =100	29.5	58.2	107.1	198.1	^280^	624.1	803.12-	27.0	54.0	94.4	192.6	^768	564.9	721.2~	22.3	58.6	87.3	163.0	^406	494.3	637.68-	24.1	43.1	78.4	141.7	T995^	431.3	554.816	16.8	32.6	70.7	114.3	!866	365.2	468.732	14.9	28.4	50.4	96.9	^1424	288.6	371.7(b)	Average squared distance of vicinal distribution from training set.
Figure 6:	Results for CIFAR-10 with Resnet18 architecture (no mixup performance: 94.4%), averagedover 20 Monte Carlo trials (±.03 confidence on test performance). Difference between best k-mixupand best 1-mixup is 0.17% for fixed high α (α = 100), the improvement increases to 1.2%.
Figure 8: Test accuracy on CIFAR-100and SVHN, averaged over 20 MonteCarlo trials (±.03 confidence).
Figure 7: Training convergence of k = 1 and k = 32 mixup on CIFAR-10, averaged over 20 randomtrials. Note that both train at roughly the same rate (k = 32 slightly faster), the train accuracydiscrepancy is due to the more class-accurate vicinal training distribution created by higher k-mixup.
Figure 9: CIFAR-10 test accuracy for DenseNet-BC-190 and WideResnet-101 architectures. ForDenseNet, the difference between best k-mixup and best 1-mixup is 0.44%, for fixed high α (α = 4),the improvement increases to 0.65%. For WideResnet, the difference between best k-mixup and best1-mixup is 0.28%, for fixed high α (α = 4), the improvement increases to 1.25%.
Figure 10: Google Speech Commandstest accuracy using LeNet architecture,averaged over 20 Monte Carlo trials(±.014 confidence).
Figure 11: Manifold mixup test accuracyon CIFAR-10 using Resnet18 architec-ture, averaged over 20 Monte Carlo trials(±.03 confidence).
Figure 12: Adversarial robustness: accuracy on white-box FGSM adversarial attacks.
