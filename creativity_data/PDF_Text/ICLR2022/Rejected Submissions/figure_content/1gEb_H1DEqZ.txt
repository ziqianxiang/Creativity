Figure 1: How the facts and logical graph constructed from raw text inputs. Edges in red denotesadditional edges added in the logical graph, while text with green indicates the sentence-level logicalconnectives which will be mentioned in ยง4.
Figure 2: An illustration about pre-training methods used in Prophet. The model takes the text,extracted fact and the randomly sampled node pairs in the logical graph as the input. The model ispre-trained with three novel objectives. One is the standard masked language modeling applied tosententious connectives, the others are fact alignment and logical path prediction.
Figure 3: Heatmap of the attention matrix of vanilla BERT and our implemented Prophet for thesentence "However, they met harsh suppression after the Bolthevik government was stabilized.".
Figure 4: Accuracy of different context length on MNLI-match (left) and MNLI-mismatch (right)dev set. There are approximate 1000 samples in each intervals.
Figure 5: We take an example from RTE dataset, and use Prophet and BERT-base to predict thelabel of the relations among two given sentences.
