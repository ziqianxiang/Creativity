Figure 1: In the (a) object collection environment, ξ-learning reached the highest average rewardper task for (b) linear, and (d) general reward functions. The average over 10 runs per algorithm andthe standard error of the mean are depicted. (c) The performance difference between ξ-learning andSFQL is stronger for general reward tasks that have high non-linearity, i.e. where a linear rewardmodel yields a high error. SFQL can only reach less than 50% of MF ξ-learning's performance intasks with a mean linear reward model error of 1.625.
Figure 2: (a) Example of a reward function for the racer environment based on distances to its 3markers. (b) ξ-learning reaches the highest average reward per task. SFQL yields a performance evenbelow QL as it is not able to model the reward function with its linear combination of weights andfeatures. The average over 10 runs per agent and the standard error of the mean are depicted.
Figure 3: Object collec-tion environment from (Bar-reto et al., 2017) with 3 objecttypes: orange, blue, pink.
Figure 4: MF ξ-learning outperforms SFQL in the object collection environment by Barreto et al.
Figure 5: Total return over all tasks in each evaluated condition. The tables show the p-values ofpairwise Mann-Whitney U tests between the agents. See the text for more information.
