Figure 1: Example distribution movement: far away from decision boundary(left) vs close to deci-sion boundary(right)Example 1. Figure 1 shows two extreme cases of adversarial example X0 distribution movementfrom clean X distribution given decision boundary in a simplified 2D scenario. When X is far awayfrom decision boundary, the direction of attack or movement towards worst case example underverifiable adversary is same for all the x. In this case, PP(XIy)) ≈ 1. In the second scenario, assumethe decision boundary is a perfect semicircle representing the local curvature of decision boundaryand perturbed examples from both classes follow a uniform distribution, then the examples with0-0 label in the outer arc condensed into the inner arc and vice versa for examples with 0+0 labelmoving to the external arc. Due to the existence of curvature, the area of the outer arc Sin is4Under review as a conference paper at ICLR 2022greater than the inner arc SOut. Therefore, from Theorem 1 the expectation of PP(XIIy) is greaterthan 1. Another detailed example is illustrated in Appendix B. In this later example, the uniformperturbation assumption replaced the post-mapping uniform distribution assumption, which is morerealistic for adversarial scenario.
Figure 2: Left: Re-weighting examples based on vulnerability, Right: Auto-tuning : customize theperturbation for mis-classified examplesWe parameterize the importance weight in the following form:3i = e-*(ImargiMfθ,χi,yi⑹I) + α 〜s(fθ, Xi,yi)	(7)where γ and α are both positive hyper-parameters to balance the effect of re-weighting. The fullloss function after re-weighting is then defined as:1κ-nn	1nEl(ZK,y) + (I - K)I(ZK,y)i=1	ωi i=1(8)where κ is a hyper-parameter to balance the trade-off between clean loss and re-weighted robustnessloss.
Figure 3: Distribution of margin: vanilla robust training vs re-weighting4.2.2	EFFECTS OF AUTO-TUNINGIn robust training, a train usually produces good defensive model for eval which is slightly smallerthan itself. Practitioners refrain from using an even larger due to the marginal gain of robustnessaccuracy with the cost of sacrificing natural accuracy.
Figure 4: An ideal example illustrating importance weight expectation around decision boundarywith curvatureExample 2. We assume label-balanced data x ∈ R2 and y ∈ {+, -} and positive constant σ, theclean data With label 1 were uniformly distributed in the inner ring Cin = {x∣σ < ∣∣xk2 < 2σ}and data with label 0 were uniformly distributed in the outer ring Cout = {x∣2σ < ∣∣x∣2 <3σ}. Then p(y = +) = p(y = -) = 0.5 and clean data distribution is: p(x|y) =[3∏σ2 if y = +,σ < kχk2 <2σ5∏σ2 if y = -, 2σ < Ilxk2 < 3σ10 otherwiseassume the linear classifier f : R → R uses r = ∣x∣2 as feature and outputs the logit for probability,s.t. the probability after sigmoid function is pf (y|x)where s = r + 2σify=+ify=-The negative log-likelihoodNLL = -E(x,y)〜p(x,y)[log(Pf (y|x 川= -p(y = +)	p(x|y = +)log(pf (y = +|x))dx-p(y = -)	p(x|y = -)log(pf (y = -|x))dx-0.5-0.5
