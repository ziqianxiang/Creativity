Figure 1: Knowledge distillation (a,b) seeks to copy the end-to-end functionality of one or moreteachers but often fails to transfer (c). We propose a simple yet effective method to learn a consol-idated representation from N teachers that transfers well to downstream tasks (d). Given a largeunlabeled proxy dataset, we train a single student model using multi-task distillation with a separateclassifier head for each of the teacher tasks. To limit student forgetting and representation collapse,we always include an additional generalist teacher (ImageNet model). We show that the resultingconsolidated representation transfers better to downstream tasks than any of the individual teachers(including the generalist).
Figure 2: Exp. 1 with N = 1, 5-shot linear SVM downstream transfer. Left: Comparing differentrepresentations on ten downstream tasks. Teacher-related downstream tasks in bold. Performancerelative to ImageNet representation baseline. Excerpt (2 of 10 teacher domain scenarios). Right:Tally of comparisons among all ten Dt1 domains. On teacher-related downstream tasks, we outper-form or match ImageNet-pretrained, and on other tasks we match ImageNet. Traditional distill oftenunderperforms ours (7/10 related, 10/10 unrelated) and ImageNet (3-4/10 related, 10/10 unrelated).
Figure 3:	Exp. 2 with N > 1 multi-model merging, 5-shot linear SVM downstream transfer. Left:In the same domain, we are able to consolidate from models with different architectures and improvetransfer performance over every single teacher. Right: We can consolidate different domain modelsand improve over the ImageNet representation. See appendix for full comparisons.
Figure 4:	Exp. 3 Excerpt of N = 1, fine-tuning results. Showing full-shot (transfer to all classesfor Flowers and Birds). Our conclusions are the same between fine-tuning and fixed representation(Fig. 2). See appendix for the full results (few-shot, full-shot, more datasets).
Figure 5:	Additional studies. 5-shot linear SVM downstream transfer. Left: Tuning loss weightsresult in different balance between related and unrelated downstream task performance. Right:Places365 as proxy has similar results as ImageNet proxy. However, training on Places365’s super-vised labels or using a less general proxy (iNaturalist) hinders performance.
Figure 6:	Exp. 1 Full results for Fig. 2a (N1 single task-specific teacher, 5-shot linear SVMdownstream transfer). All 10 Dti cases. This extensive set of experiments have the same conclusionsas the main paper: we match or outperform ImageNet representation while traditional distill oftenunderperforms. See also Table 3 for a comparison tally.
Figure 7:	Exp. 2 Merging N > 1 same-domain ResNet18 teachers, 5-shot linear SVM downstreamtransfer. Part 1/2 of full results for Fig. 3a. We are able to consolidate from models with differentarchitectures (ResNet50 φt0 and ResNet18 φti) and improve transfer performance over every singleteacher.
Figure 8:	Exp. 2 Merging N > 1 same-domain ResNet50 teachers, 5-shot linear SVM downstreamtransfer. Part 2/2 of full results for Fig. 3a. Our conclusions generalize to using all ResNet50teachers.
Figure 9:	Exp. 2 Merging N > 1 different domain ResNet50 teachers, 5-shot linear SVM down-stream transfer. Full results for Fig. 3b (N > 1 multiple model merging, multiple domains). We canconsolidate different domain models and improve over the ImageNet representation and (for mostrelated/unrelated downstream datasets) multi-task traditional distillation.
Figure 10:	Exp. 3 N = 1 (first 4 graphs) and N > 1 (last graph) teacher(s), 5-shot fine-tuningdownstream transfer. Part 1/2 of full results for Fig. 4. The same conclusions as the fixed represen-tation scenario in Figs. 2, 6 hold.
Figure 11:	Exp. 3 N = 1 (first 4 graphs) and N > 1 (last graph) teacher(s), full-shot fine-tuning downstream transfer. Part 2/2 of full results for Fig. 4. The same conclusions as the fixedrepresentation scenario in Figs. 2, 6 hold.
Figure 12:	Exp. 5 N = 1, 5-shot linear SVM downstream transfer, different proxies. Full results forFig. 5b. Conclusions similar - as the proxy, Places365 is similar to ImageNet, but a narrower-ScoPediNaturalist underperforms.
