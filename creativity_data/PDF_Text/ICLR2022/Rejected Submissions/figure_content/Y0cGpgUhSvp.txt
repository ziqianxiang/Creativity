Figure 1: Properties of different training data selection functions: reducible loss (ours), irreducibleloss, loss, and gradient norm. Left: We score the points in each batch with each selection functionand sum the score of the top 20% of points, then divide it by the total sum across all points. Acrossselection functions, the set of top-scoring points is responsible for most of the total; this suggests thatmany points are already learned (redundant) and can be skipped. Right: Proportion of selected pointswith label noise applied. We added 10% uniform label noise, i.e., we randomly switched each point’slabel with 10% probability. Selecting the 10% “hardest” points (high loss or gradient norm) givesbatches where > 10% of the labels are mislabeled. We can avoid noisy points by selecting points thatare “easy” (low irreducible loss) or have high reducible loss. Results are averaged over 15 epochs.
Figure 2: Reducible loss is robust to a variety of label noise patterns, while other selection methodsdegrade. A step corresponds to lines 6 - 11 in Algorithm 1. Lines correspond to means and shadedareas to minima and maxima across 3 random seeds.
Figure 3: Gradient steps required to achieve a given test accuracy (lower is better). Left column:The speedup of RHOLS over uniform sampling is greatest on a large-scale web scraped datasetwith noisy labels (12.5x). Middle column: Speedups are still substantial on clean datasets. Rightcolumn: Applying 10% uniform label noise to training data degrades other methods but increasesthe speedup of our method. A step corresponds to lines 6 - 11 in Algorithm 1. Lines correspond tomeans and shaded areas to minima and maxima across 3 random seeds.
Figure 4: CINIC-10 with half of the classes absent (irrelevant) in the test and holdout sets. Left:steps required to reach a given target accuracy on the test set. Right: Proportion of selected pointswhose labels do appear in the holdout and test sets. RHOLS and irreducible loss select exclusivelythe relevant classes. ROHLS provides a large speedup over existing baselines. Lines correspond tomeans and shaded areas to standard deviations across 3 random seedsselection achieves after epoch 50 in just 2 epochs, providing a speedup of 12.5x. Furthermore,RHOLS’ reaches a higher final accuracy (74%) than uniform selection (66%).
Figure 5: The irreducible loss model can be small, trained on little data, and reused acrosstarget models and hyperparameters. The x-axis shows after how many fewer epochs RHOLSreaches the highest accuracy that uniform selection reaches during 100 epochs (with -nb = 0.1). AllnBrows but the first (‘Default’) use a small CNN as irreducible loss model (in addition to the otherexperiment conditions indicated). Each dot represents an experiment with a specific combination ofirreducible loss model and target model (multiple seeds per combination shown as separate dots).
