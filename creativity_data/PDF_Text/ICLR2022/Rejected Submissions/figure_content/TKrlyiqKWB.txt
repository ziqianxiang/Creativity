Figure 1: CSNs support single (a) or multiple (b-d) classification by projecting encodings into con-cept subspaces. Subspaces may exhibit a range of relationships from parallel (b) to orthogonal (d).
Figure 2: A 2D latent space for fair classifi-cation. The prototypes for high and low in-come (X) are perpendicular to the prototypesfor applicant sex (circles).
Figure 3: 2D latent space for hi-erarchical digit classification createsclusters around even and odd proto-types (circles on the right and left, re-spectively) and digit prototypes (X).
Figure 4: Decoded prototypes for digit classification provide visualizations of how images are clas-sified.
Figure 5: Decoded prototypes for the bolt classification task, plotted within an axis-normalized 3Dspace. Using 16 prototypes for the 8-label classification task reveals bolt-specific actions as well asa variety of motions such as screwing in the bolt and moving towards it.
Figure 6: 2D learned latent spaces for the Adult, German, Fashion, and Digit datasets, showingprototypes from two concepts (×’s and black dots). Fair tasks USed orthogonal concept spaces (top);hierarchical tasks created multi-level clusters (bottom).
Figure 7: Without alignment loss guidance, CSNs may learn undesirable relationships between con-cepts like age (circles denote prototypes) and credit (Xs denote prototypes) in the German dataset(left). In an artificial weather scenario (right), CSNs can be guided to learn the right causal relation-ship between temperature and precipitation.
