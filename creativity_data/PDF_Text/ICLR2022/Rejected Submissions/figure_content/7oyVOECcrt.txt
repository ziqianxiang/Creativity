Figure 1: The architecture of a layer in a locally permutation equivariant graph network is presented.
Figure 2: Bases for mappings to and from different order permutation representations, where ρkis a k-order representation. Each color in a basis indicates a different parameter. ρ0 → ρ0 is amapping from a 0-order representation to a 0-order representation, i.e. a graph level label to graphlevel label, and has 1 learnable parameter. ρ1 → ρ1 is a mapping from a 1-order representation to a1-order representation, i.e. a node level label to node level label, and has 2 learnable parameters, onemapping node features to themselves and the other mapping node features to other nodes. Further,there are mappings between different order representation spaces and higher order representationspaces.
Figure 3: (a) Regular repre-sentation. (b) Restricted rep-resentation.
Figure 4: Computational cost of global and local permutation equivariant models with the samenumber of model parameters for datasets with varying average size graphs. For the local equivari-ance case local neighbourhoods were computed using 1-hop neighbourhoods.
Figure 5: The initial graph on the left is isomorphic to the top graph on the right. On the other hand,the initial graph on the left is non-isomorphic to the bottom graph on the right. This is because thereis no structure preserving mapping that has an inverse between the two graphs.
Figure 6:	The general framework for building models with permutation equivariance through rep-resentations of the permutation group is presented. In general for work on graphs the input repre-sentation ρj = ρ2, but this could vary if the input was a set or other data structure. Then for eachlayer a choice of k value can be made which is the number of hops to consider when extracting sub-graphs. Also, for each layer the permutation representation type can be chosen where the choice ofρι ㊉ ρ2 ㊉•…㊉ Pi is the order of representations required for that layer. Choosing larger order repre-sentations increases the expressivity of the model, although this increases the bases space increasingthe computational cost.
Figure 7:	A visualisation of the training accuracy for benchmark graph classification datasets across10-folds. Each plot shows the training accuracy as a percentage, where a higher percentage is better.
Figure 8: Comparison between out LPEGN method and other methods on benchmark graph clas-sification datasets. Results for the LPEGNN method are presented as a histogram of the 10-foldruns. Each other method is given as a Gaussian distribution with mean and standard deviation as ispresented in Table 1. In addition, we display the sum of the distributions for all other methods toshow how our method compares.
Figure 9: Presented is the percentage of ranking wins across the seven datasets for the LPEGN. Aresults above 50% means the LPEGN method beats the other method across the majority of datasets.
Figure 10: Histograms showing the number of occurrences of each size sub-graph in the MUTAGdataset for different choices of k value, where k is the number of hops to take away from the centralnode when selecting nodes to be included within a sub-graph.
Figure 11: (a) Global permutation equivariance and some of the possible representation choices.
