Figure 1: CSC visualization: A signal x is generated by a superposition of few atoms from a convo-lutional dictionary D. Each entry of Γ corresponds to a certain shift of a limited support filter.
Figure 2: 2D CSC visualization: Γ is an H×W×C tensor, composed of H×W “needles”, denotedas γ, each of size 1×1×C. Since Γ is sparse, the bulk of its entries are zeros. A given γ at locationi, j in Γ contributes to a patch located in the corresponding location in the output image x. Eachnon-zero entry γi in γ defines the coefficient for the filter ai in a superposition that creates a patch.
Figure 3: Our interpretation (green) of generators as handling two separate sub-tasks - GS producesa sparse representation vector and GI maps it into an image. The depicted architecture contains 4blocks and the stated tensors’ dimensions are typical for synthesizing 32 × 32 × 3 images.
Figure 4: An illustrative experiment on the effects of applying our method during the training of asimple DCGAN (Radford et al., 2015) on CIFAR-10 dataset. (a): Sparsity levels and FID compari-son between non-regularized, L1-based regularization and a constraint-based regularization (L0 andL0,∞) on a single-layer CSC and a L0,∞ constraint applied on the ML-CSC. The sparsity level isthe percentage of non-zeros in the representation Γ. As can be seen, promoting sparsity leads to im-proved performance. (b): A spatial sparsity distribution comparison of Γ, obtained by the differentsparsifying techniques. Each pixel in the above figure represents the mean sparsity attained in thecorresponding needle of the sparse tensor Γ. As demonstrated above, L0 and L1 lead to a globalsparsity which is imbalanced locally, while L0,∞ forces such a balance. Since most of the objectsin CIFAR-10 are centered, applying L0 or L1 regularizations leads to denser needles at the center.
Figure 5: A visualization of the intermediate representations in an encoder-decoder architecture withskip-connections.
Figure A: A visualization of the dictionary atoms in the ML-CSC setup obtained for DCGAN trainedon the CIFAR-10 database: (a) The 128 atoms of D1, each of size 4 × 4 × 3. (b) The 128 atoms ofDeff = D1D2, each of size 8 × 8 × 3.
Figure B: A visualization of the performance improvement obtained by the proposed sparsity-inducing methods across various GAN architectures.
