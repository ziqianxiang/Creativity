Figure 1: Complete design of orthogonal convolutional layer as a cascade of convolutions, whose filtercoefficients are depicted in each block. The matrix Q is orthogonal and U(')'s are column-orthogonal.
Figure 2: SC-Fac: A pipeline for designing orthogonal convolutional layer.⑴ An orthogonalconvolution h[n] is equivalent a paraunitary system H(z) in the spectral domain (Theorem 2.1). (2)The paraunitary system H(Z) is multiplications of factors characterized by (column-)orthogonal ma-trices ({U(')}-=-L, Q, {U(')}L=ι) (Equation (2.2a), Theorem B.5). (3) These orthogonal matricesare parameterized by skew-symmetric matrices using exponential map.
Figure 3: Up and down sampling. In (a), the sequence x[n] isup-sampled into x↑2[n]. In (b), x[n] is down-sampled into x0|2 [n]with even entries (red) and x1|2[n] with odd entries(blue).
Figure 4: Run-time and memory comparison using WideResNet22 on Tesla V100 32G. x-axis indicates thewidth factor (channels = base_channels × factor). Our SC-Fac is the most computationally and memory-efficientfor wide architectures and is the only method that scales to width factor to 10 on WideResNet22. We alsocompare with an ordinary network with regular convolutions and ReLU activations. Note that SC-Fac has thesame inference speed as a regular convolution — the overhead is from the GroupSort activations.
Figure 5: A framework for designing orthogonal convolutional layers. In Appendix C.2, we unifyvariants of orthogonal convolutions in the spectral domain and show that their designs reduce to con-structing paraunitary systems. In Appendix C.3, we show that a paraunitary system can be constructedwith different approaches: our approach and BCOP (Li et al., 2019b) represent the paraunitary usingorthogonal matrices, while CayleyConv (Trockman & Kolter, 2021) and SOC (Singla & Feizi, 2021)directly parameterizes it using unconstrained parameters. In Appendix C.4, we investigate variousparameterizations for orthogonal matrices, such as matrix exponential, Cayley transform, and BjGrCkorthogonalization.
Figure 6: Variants of residual blocks. In our experiments, we combine (a) & (b) to construct anorthogonal ResNet, and (c) & (d) to construct an orthogonal ShuffleNet. In Proposition 4.1, we provethe Lipschitzness of these building blocks. Since composition of Lipschitz functions is still Lipschitz,it implies that a network constructed by these building blocks is also Lipschitz.
Figure 7: Effect of the Lipschitz margin 0 for WideResNet22-10. It shows a trade-off betweenclean and robust accuracy with different margins for multi-class hinge loss. As shown, the trainingand test accuracy become higher with larger margin, but the robust accuracy decreases after 0 = 0.1.
Figure 8: Random samples from SC-Fac Residual Flow trained on MNIST.
