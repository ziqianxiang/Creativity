Figure 1: Controllable generative models with sample-level style control. (a) The informationcontained in a sample can be divided into content (i.e., the text) and style (i.e., all other informationbesides content). (b) Our goal during inference is to generate samples containing target contentA in the style of sample B. Notice that sample B generally contains a different content. (c) Thereexists a training-inference mismatch when learning these models in typical unsupervised trainingof controllable generative models. During training, the same sample is used as content input andstyle input, whereas during inference, content and style inputs are from different samples, i.e., thereference style sample contains a different content than the target content. The mismatch leads toincorrect content generation during inference. (d) To mitigate the training-inference mismatch, theproposed style equalization takes unpaired samples as input during both training and inference. Ittransforms the style of sample B to that of A by estimating their style difference.
Figure 2: Model used in the paper. (a) an overview of the entire model, which includes a styleencoder (in green), content attention, and decoder (in gray). Note that the input content c can be theoutput of a content-embedding network (used in speech synthesis) or one-hot encoding of characters(used in handwriting synthesis). at is the output of the content attention at time t, which is a linearcombination of the elements in c. (b) the proposed style encoder without style equalization. (c) theproposed style encoder with the style equalization. φ computes the vector δ that encodes the styledifference between x0 and x. M applies this transformation to x0 to match the style of x.
Figure 3: Handwriting generation results and evaluation. The reference style examples are shownin black, and the outputs are shown in blue.
Figure 4: Style attention weights. The figure shows the style attention weights of two models trainedon LibriTTS during inference on unseen styles with (a) style equalization always applied duringtraining (100%) and (b,c,d) style equalization applied to 50% of the training batches.
Figure 5: Overview of the model. (a) shows an overview of the entire model without style equal-ization (used during inference since δ = φ(xr, xr) = 0). It includes a style encoder (in green), acontent attention, a decoder (in gray), and a LSTM at the bottom. Note that the input content c canbe the output of a content-embedding network (used in speech synthesis) or one-hot encoding ofcharacters (used in handwriting synthesis). at is the output of the content attention at time t, whichis a linear combination of the elements in c. (b) shows an overview of the entire model with styleequalization (used during training or during interpolation). φ computes the vector δ that encodes theamount of style transformation between x0 and x. M applies this transformation to x0 to match thestyle of x. Please see Table 4 for details about individual blocks.
Figure 6: Parallel and nonparallel text generation results. In the figure, the reference examples areshown in black, and the generated results are shown in blue.
