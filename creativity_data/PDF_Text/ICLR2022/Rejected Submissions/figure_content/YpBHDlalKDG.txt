Figure 1: Our learning system is robust and versatile, supporting various simulation methods,robot designs, and locomotion tasks with continuously controllable target velocity and heights.
Figure 2: 3D agents collection. These agents are designed with simple stacked cubes or complexhandcrafted meshes.
Figure 3: 2D agents collection.
Figure 4: Framework overview. Simulation instances are batched and executed in parallel onGPUs. The whole system is end-to-end differentiable, and We use gradient-based algorithms to op-timize the neural network controller weights. Each time step involves evaluating a NN controllerinspired by SIREN Sitzmann et al. (2020), and a differentiable simulator time integration imple-mented using DiffTaichi Hu et al. (2020). Simulation states are fed back to the initial state pool toimprove the richness of training sets and thereby the robustness of the resulted NN controller. Atailored loss function (as shown in section 3.2) is designed for each task.
Figure 5: Summary of the ablation study. Here we show the summary of ablation study on agentAlpaca. Each iteration indicates one training iteration, which is composed of 1000 steps of physicalsimulation and one step of update on weights of the controller. The result show that the Full methodachieves the best performance. For more results of ablation studies on other agents, please check theappendix.
Figure 6: Gradient Analysis. The plots show the gradient distribution of different agent. The x-axis represent the sum of gradients norm. The values are drawn in log scale fora clear visualization.
Figure 7:	Activation functions ablation study. The figure show the validation loss between choicesof different activation functions. From left to right, the subplot shows the results of Task, Run andJump loss, respectively.
Figure 8:	Input features ablation study. Full-PS, Full-SV and Full-TG indicate the models trainedwithout Periodic Signal, State Vector or Targets respectively.
Figure 9:	Comparison on different agents. Both our method and PPO run on GPUs. The solid anddashed lines show the validation loss of our method and PPO, respectively. The agent design is alsoshown in the figure. Springs marked in red are actuators.
Figure 10: Network architecture.
Figure 11: Left: Our system allows the user to control the agentâ€™s motion interactively. Right: thetrajectory of a walking quadruped agent under user control.
Figure 12: Left: Training loss for zero friction, .4 friction and sticky contact models. Middle:Agent suffers from sticky surface and cannot move further. Right: Agent moves smoothly towardsright.
Figure 13: Manipulation task showcase. The upper plot shows the snapshots of scenario 'Juggle'and the lower one shows that of the 'Dribble and Shot'.
Figure 14: Training curve of the 'Juggle' task. The loss is defined as the distance between centerof the object and the target point.
Figure 15: Validation loss on different agents and targets. The plot shows the validation losson different targets combinations for agents. The target velocity ranges from -0.08 to 0.08 and thetarget heights are 0.1, 0.15 and 0.20. Each subplot shows the task, running and jumping loss for oneagent given a pair of specified target velocity and height.
Figure 16: Summary of the ablation study for agents. The subplots show the normalized valida-tion loss for task (weighted summation of different losses), running and jumping from top to bottomrow. The subplots in different columns show the results of different agents.
