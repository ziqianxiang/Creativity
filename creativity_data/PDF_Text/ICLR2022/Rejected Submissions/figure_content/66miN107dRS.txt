Figure 1: Comparison of conventional contrastive learning (CL) and the proposed Contrastive Attraction andContrastive Repulsion (CACR) framework. For conventional CL, given a query, the model randomly takes onepositive sample to form a positive pair and compares it against multiple negative pairs, with all samples equallytreated. For CACR, using multiple positive and negative pairs, the weight of a sample (indicated by point scale)is contrastively computed to allow the query to not only more strongly pull more distant positive samples, butalso more strongly push away closer negative samples.
Figure 2: Illustration of the CACR framework. The encoder extracts features from samples and the conditionaldistributions help weigh the samples differently given the query, according to the distance of a query X and itscontrastive samples x+, x-. Z means element-wise multiplication between costs and conditional weights.
Figure 3: Conditional entropy H(X- |X) w.r.t. epoch on CIFAR-10 (left) and linearly label-imbalancedCIFAR-10 (right). The maximal possible conditional entropy is indicated by a dotted line.
Figure 4: (Supplementary to Figure 3) Conditional entropy H(X- |X) w.r.t. training epoch onCIFAR-100 (left) and linear label-imbalanced CIFAR-100 (right). The maximal possible conditionalentropy is indicated by a dotted line.
Figure 5: (Supplementary to Figure 3) Conditional entropy H(X- |X) w.r.t. training epoch onexponential label-imbalanced CIFAR-10 (left) and CIFAR-100 (right). The maximal possibleconditional entropy is indicated by a dotted line.
Figure 6: Illustration of positive/negative samples and their corresponding weights. (Left) For a query augmentedfrom the original dog image, 4 positive samples are shown, with their weights visualized as the blue distribution.
Figure 7:	The t-SNE visualization of the latent space at different training epochs, learned by CL loss(top) and CACR loss (bottom). The picked query is marked in green, with its positive samples markedin blue and its negative samples marked in red. The circle with radius t- is shown as the black dashedline. As the encoder gets trained, we can observe the positive samples are aligned closer to the query(Property 1), and the conditional differential entropy H(X- |X) is progressively maximized, drivingthe distances d(fθ(x), fθ(x-)) towards uniform (Lemma 2).
Figure 8:	The linear classification results of training with different sampling size on small-scaledatasets. The training batch size is proportional to the negative sampling size.
Figure 9: The linear classification results of training with different positive sampling size on CIFAR-10, CIFAR-100 and ImageNet-1K. An AlexNet encoder is applied on CIFAR-10 and CIFAR-100;ResNet50 encoder is applied on ImageNet.
Figure 10: Comparison of training efficientcy: Linear classification with learned representations w.r.t.
