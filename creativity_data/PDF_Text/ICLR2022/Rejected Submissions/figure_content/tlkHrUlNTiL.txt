Figure 1: DGN is a setup to understand the role of gating in DNNs with ReLUs. The DLGN setup completelydisentangles and re-arranges the computations in an interpretable manner. The surprising fact that a constant 1input is given to weight network of DLGN is justified by theory and experiments in Sections 3.1 and 3.2.
Figure 2: Shows the DGN on the left. Training: In the case of fixed learnt gates, the feature network ispre-trained using yf as the output, and then the feature network is frozen, and the value network is trained withi^DGN as the output. In the case of fixed random gates, the feature network is initialised at random and frozen,and the value network is trained with Jdgn as the output. In the case of fixed gates, hard gating G(q) = H{q>o}is used. Standalone Training: both feature and value network are initialised at random and trained togetherwith Jdgn as the output. Here, soft gating G(q) = ι+eχp(-β∙q) is used to allow gradient flow through featurenetwork. On the right side is the CNN-GAP and its DGN used in (Lakshminarayanan & Singh, 2020). InCNN-GAP, C1, C2, C3, C4 are convolutional layers, FC1, FC2 are fully connected layers. In CNN-GAP-DGN,Gl, l = 1, 2, 3, 4 are the gates of layers, the superscripts f, and v stand for feature and value network respectively.
Figure 3: Here the gates G1 , G2, G3, G4 are generated by the feature network and are permuted asGi1 , Gi2 , Gi3 , Gi4 before applying to the value network. C1 , C2 , C3 , C4 have 128 filters each. Table Iand II: All columns (except the last) show the % test accuracy on CIFAR-10 and CIFAR-100, and % of DNNperformance recovered by DLGN is in the last column. Table I: For each dataset, the top row has results forvanilla models without permutations (the results are averaged over 5 runs) and the bottom row has results of4! - 1 = 23 permutations (except the identity) for each model (the results are averaged over the 23 permutations).
Figure 4: C1GAP and C4GAP have width = 512 to make them comparable to VGG-16 whose maximum widthis 512. The ensemble size is 16 to match the 16 layers of VGG-16. Note that C4GAP in Figure 3 has width=128.
Figure 5: Illustration of Definition 2.1 and Proposition 2.1 in a toy network with 2 layers, 2 gatesper layer and 4 paths. Paths pi andp are 'on' and paths p3 andp4 are 'off’. The value, activity andfeature of the individual paths are shown. y is the summation of the individual path contributions.
Figure 6: Shows the permutations 1 - 12 C4GAP-DLGN in Table I of Figure 3. The top left is theidentity permutation and is the vanilla model.
Figure 7: Shows the permutations 13 -GaLUGaLUGaLUGaLUGAP)sGaLUGaLUGaLUGaLU∣→ FC → y(X)xv → Cv -> Cv ->	C3Xf → C1VC2C3fGaLUG1	G2xv ―C Cv -> p -*■ C2
Figure 8: Shows VGG-16 (left), VGG-16-DLGN (middle), VGG-16-DLGN-SF(right).
Figure 9: Shows a comparison of the gated linear network (GLN) and the value network of DLGN.
