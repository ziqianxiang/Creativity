Figure 1: (a) Target networks use an additional set of parameters, θ (pink), to estimate target valueswhich are then used in the Bellman error. The target network stabilizes the Q-value estimates, butis not up-to-date. (b) In our approach, we use the up-to-date θ to estimate the Q-value for theBellman error (see the rightmost block), and add a κ weighted Functional Regularization (FR) lossto stabilize the current Q-value estimate (parameterized by θ) by comparing it to the old Q-values(parameterized by θθ).
Figure 2: Four Rooms Comparison. We compare the performance of different Deep Q-learningalgorithms. The reward position is kept fixed throughout training (upper-left room), but the agent’sposition is randomized at the beginning of every episode to bypass exploration difficulties. In Figure(a), we can see that FR DQN (blue dashed) quickly and stably completes the task. The other methodsstruggle to get a good combination of speed and stability, e.g., the two green methods are either fastor stable. All methods minimize the squared Bellman error equally well, as shown in Figure (b),however our method approximates the true value function an order of magnitude more accuratelythan the other methods as shown in Figure (c). Figure (d) shows the stability of FR DQN where theperformance is not too sensitive to changes in the regularization parameter (κ).
Figure 3: DQN Atari Comparison. Performance curves for the subset of Atari games from Mnihet al. (2013). The returns are averaged over 10 trials using the -greedy policy with = 0.05.
Figure 4: Double DQN Atari Comparison. Performance curves for the subset of Atari games fromMnih et al. (Mnih et al., 2013). The baseline (dotted blue line) and 4 different FR weights (solidlines) average returns over 10 trials using -greedy with = 0.05.
Figure 5: We benchmark the different algorithms on different Four Room environment sizes. Aswe increase the size of the Four Room environment, the reward becomes sparser and the task moredifficult to complete. We observe that FR DQN scale more gracefully to larger environment sizes.
Figure 6: The average regret can be noisy since certain runs might not complete the task, e.g. 2seeds do not complete the task for DQN on the size 19. Thus, we also report the median regret over500k iterations. See Table 1 for the hyper-parameter used for each algorithm.
Figure 7: DQN with different target update period.
Figure 8: FR DQN with different regularization weight.
Figure 9: Polyak DQN with different parmaeter τ .
Figure 10: Deep Mellow with different temperature parameter.
Figure 11: Return.
Figure 12: Value function of the 4 Rooms environments estimated via tabular Q-learning.
