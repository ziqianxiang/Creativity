Figure 1: Illustrations of the resource-constrained setting(b) Histogram of rewards in online data col-lected by agents trained with online versus of-fline featuresmay take sufficient time to generate an abundance of features by post-processing high-dimensionalmeasurements.
Figure 3: Results on D4RL datasets. 2Figure 2: Results on RC-D4RL datasets. 1Difficulty	Baseline (TD3+BC)	Transfer (0.5,0.5)	Transfer (0.0,1.0)	Difficulty	True-BC	PredictiveexPert	-29.8 %	-32.5 %	-39.6 %	exPert	-32.1 %	-18.5%medium-rePlay	-33.2 %	-22.4 %	-14.1 %	medium-rePlay	30.7 %	31.0%Table 2: We summarize the loss in performance by notusing the offline features for the Baseline (TD3+BC),Transfer (0.5,0.5) and (0.0,1.0) as a % change over theTeacher score on RC-D4RL HalfCheetah-v2.
Figure 2: Results on RC-D4RL datasets. 1Difficulty	Baseline (TD3+BC)	Transfer (0.5,0.5)	Transfer (0.0,1.0)	Difficulty	True-BC	PredictiveexPert	-29.8 %	-32.5 %	-39.6 %	exPert	-32.1 %	-18.5%medium-rePlay	-33.2 %	-22.4 %	-14.1 %	medium-rePlay	30.7 %	31.0%Table 2: We summarize the loss in performance by notusing the offline features for the Baseline (TD3+BC),Transfer (0.5,0.5) and (0.0,1.0) as a % change over theTeacher score on RC-D4RL HalfCheetah-v2.
Figure 4: FQE estimated rewards on the Ads data. Only the relative metrics are presented to hidebusiness-sensitive information. 10 different seed values are tried, and the average values are pre-sented. Transfer (0.0, 1.0) outperforms the baseline, Transfer (1.0, 0.0), 7 times out of 10.
Figure 5: Simulation of high resolution and low resolution sensors.
Figure 6: Illustration of the proposed methodC.1 Dataset collection procedure: RC-D4RLFor the dataset collection using policies trained with limited features, we follow a similar protocol asthe D4RL data collection procedure (Fu et al., 2020). In the D4RL benchmark suite, offline datasetswith different difficulty levels are collected based on the exploration capacity of the data-collectingpolicy or behavior policy. The difficulty levels are denoted as medium-replay, medium, medium-expert and expert. For each combination (environment, difficulty, dimension, seed) in Table 1, wetrain an expert policy by training a TD3 (Fujimoto et al., 2018) algorithm for 1 million timestepsusing the limited features, and using the base hyperparameters of the TD3 algorithm. We then trainthe medium policy by training the TD3 algorithm until the agent reaches roughly half of the rewardachieved by the expert policy for this combination. These policies are deployed in the simulator andthe interactions are logged to generate the expert and medium datasets. Note that all the features arelogged in the datasets. The medium-replay data comprises of the environment interactions recorded15Under review as a conference paper at ICLR 2022during the training of the medium policy. The medium-expert dataset is the concatenation of themedium and expert policies. Thus, we generate in total 240 3 offline RL datasets. Note that we didnot perform hyperparameter tuning to compute the expert data collection policy and only used thebase hyperparameters used in the TD3 paper4. We can observe the variation in the quality of thedatasets with the dimensionality of online features and the difficulty from Table C.6 which reinforces
Figure 8: Results on D4RL datasets.
Figure 7: Results on RC-D4RL datasets.
Figure 9: Hopper(b) medium(c) medium-expert(d) expert(a) medium-replay(b) mediumFigure 11: Walker2dFigure 10: HalfCheetah(c) medium-expert(d) expertC.6 Dataset SummaryWe present the summary of the RC-D4RL datasets.
Figure 11: Walker2dFigure 10: HalfCheetah(c) medium-expert(d) expertC.6 Dataset SummaryWe present the summary of the RC-D4RL datasets.
Figure 10: HalfCheetah(c) medium-expert(d) expertC.6 Dataset SummaryWe present the summary of the RC-D4RL datasets.
Figure 12:	RC-D4RL Hopper-v2 experiments summary. We plot the mean of the rewards and theerror bars represent the standard deviation across the 3 random seeds for a given dataset.
Figure 13:	RC-D4RL HalfCheetah-v2 experiments summary. We plot the mean of the rewards andthe error bars represent the standard deviation across the 3 random seeds for a given dataset.
Figure 14:	RC-D4RL Walker2d-v2 experiments summary. We plot the mean of the rewards and theerror bars represent the standard deviation across the 3 random seeds for a given dataset.
