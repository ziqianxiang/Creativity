Figure 1: (A) Triangular construction is built by Prioritizing interPretability of a neural network.
Figure 2: (A1-3) Three triangular constructions (orange plots) using different values of a. Higher aresults in more step-wise plots and more constant values around the data samples (blue points). (B1)Plots of NNs approximated using triangular construction (smooth green plots) over scatter plots ofthe corresponding true data (green open circles). The parameters are as the following A = 1, λ =-1,C = 0 for all, (B2) A1 = 0.1, B1 = 20, C1 = 1 (B3) A1 = 0.1, B1 = 10, C1 = 1.
Figure 3: (A) Double selective activation with different parameters. (B) SQANN schematic. Eachlayer (Nk , αk ) is stylized as a collection of neurons. A neuron stores the main “fingerprint” innucleus ηl<k> (dark brown) and its corresponding “output” in nucleus αl<k> (dark red). Whenstrong activation is detected, the signal will be redirected to the dark red nucleus αl<k>. (C) SQANNused for a simple classification. (Left) The large filled dots are training samples, x marks are testsamples. Bright red indicates y = 1.0, dark red y = 0.5. (Right) Same as left but test samples thatare interpolated (i.e. no strong activation) are annotated with red open circles; colored lines indicatewhich two training samples are used for the interpolation. Lines are marked with different colorsand styles for clarity. (D) Construction of SQANN when (D.1) admission occurs: a new neuron isintroduced, creating more connection analogous to mammalian brains. (D.2) collision occurs.
Figure 4: (A) Training/test (circles/x marks) data for demonstration. Smaller/larger test data spreadmeans test samples are closer/further to/from training samples. (B) Boxplots for data whose dis-tributions are similar to (A). Column 1(3): (fractional) errors on test data samples. Column 2(4),(fractional) errors on test data samples excluding interpolated samples. Column 5: no. of datasamples whose predicted values are interpolated. (B.2) Similar to top, but for (A.2).
Figure 5: Green circles: training data samples used in TNN. Green and red x: test data samples.
Figure 6: (A) Strong activation at layer 2 node 3 (yellow circle with red boundary). The output istaken as [α2]3 (B) No strong activation, only 2 moderate activations (circles with darker shades oforanges). The output shown is a weighted average.
Figure 7: More a1 , a2 variations of double selective activation.
Figure 8: (A) Visualization ofa SQANN node η in the activation space, marked as red x. Black opencircles are the fingerprints of other nodes that are already integrated into SQANN. (B) A data samplewhose activation lies within the red/orange region such as red/blue plus mark is strongly/moderatelyactivating the particular node η. Otherwise, it is weakly activating it, e.g. green plus mark. A datasetand SQANN construction settings are desirable if for every training data u, there is a reasonablylarge d such that the probability that complication occurs is pcu ≈ 0. When a data sample x0 stronglyactivates node η (red plus mark), it will cause collision. If this happens, x0 will be integrated into thecollision layer as η0, i.e. collision is resolved, hence shifting the effective shape of activation shapeof η shape (blue arrow). What used to strongly excite η might now excite η0 more strongly (say, if itis nearer to η0).
Figure 9: (A) Training samples are in closed circles, test samples are in x. (B) Open circles showsamples that need interpolation, and the corresponding lines point towards the samples from whichthey are interpolated.
