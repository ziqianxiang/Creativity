Figure 1: Quantizing the weights of a single network layer, using a linear combination ofK different quantizers Q “ {Q1,Q2,…，Qk}. Note, that each quantizer uses a differentbitwidth, i.e., n “ {n1,n2, ∙∙∙ , nκ}. The resulting quantized weights qk are multiplied withattention values ak P r0, 1s that reflect the importance of the corresponding quantizationfunction Qk(∙). The attention values are optimized during training according to algorithm 1).
Figure 2:	The quantization error of BR and DQA for different temperatures T . In particular,a low temperature corresponds to 2bit quantization, while a high temperature means amixture of float32 and 2bit quantization or a mixture of 2/4/8bit quantization, respectively.
Figure 3:	The evolution of the attention values ak and the resulting quantization functionfor the first layer of a quantized ResNet18 trained on CIFAR100.
Figure 4:	The loss surfaces for a randomly initialized ResNet18, with float32, 2bit or mixed2,4,8bit quantized weights, evaluated on the CIFAR10 dataset. 2bit quantized weights yielda non-smooth loss surface with many local minima, what is undesirable for optimization.
