Figure 1: Diagram of Triangular Dropout application in training (Left) and used during test ordeployment (Right). During test, a layer width can be simulated by masking the layer output, orrealized by copying relevant weights into a narrower architecture.
Figure 2: Reconstruction loss versus encoding size for a variety of methods. Triangular Dropout(red) is most similar to independent autoencoders trained at specific encoding sizes (black). PCAAEmodels produced during progressive training are shown in blue. Light grey and light blue show thereconstruction loss of the full-size standard autoencoder and PCAAE models, respectively, when thelatent space is ablated down to a given size.
Figure 3: Qualitative analysis of MNIST reconstructions for all methods. Colored borders corre-spond to line color in Figure 2. A: Reconstructions from individual autoencoders at selected sizes,with original inputs on the right. B: Width 32 autoencoder with latent features ablated to desiredsize. C: Reconstructions from PCAAE checkpoint models produced by progressively training latentchannels towards a full width of 32. D: Width 32 PCAAE with latent features ablated to desiredsize. E: Reconstructions from Triangular Dropout trained once at size 32, with smaller size encod-ings created via ablation.
Figure 4: Validation accuracy of VGG19 with Triangular Dropout as the two hidden layers in theclassification MLP are modified in width after training. Several batch sizes are tested, reduced fromthe full-sized square batch (bold). For comparison, the reported validation accuracy of the originalVGG19 is also shown (teal).
Figure 5: Curves describe imitator performance on each environment as the Triangular Dropoutlayers are reduced in width after training, using architectures shown on the far left. We only showwidths 1 through 24 for readability. Horizontal lines indicate reference performance of originalexperts (black), ablated policies with at least 90% expert performance (teal), and retrained policiesusing the compressed architectures (magenta).
Figure 6: MNIST reconstruction loss for the widths 1 through 32 of autoencoders trained withTriangular Dropout and a variety of available latent widths.
Figure 7: Reconstructions of previously unseen CelebA images on various architectures using Tri-angular Dropout at the encoding layer.
Figure 8: PPO training curves for expert policiesFigure 9: PPO training curves for retrained policies having a compressed middle layer.
Figure 9: PPO training curves for retrained policies having a compressed middle layer.
Figure 10: PPO training curves for retrained policies having all layers compressed.
Figure 11: Average top speed, average reward, and architecture details for all agents in the RLexperiments.
