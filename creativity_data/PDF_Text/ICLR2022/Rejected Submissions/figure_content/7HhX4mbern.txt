Figure 1: Reservoir computing via random Signature layers. Given an input time-series, our approach can beseen as random Signature layer, acting as a reservoir, extracting a set of random features preserving the strongtheoretical properties of the Signature of the input path. A simple linear readout can the be applied to the soextracted features to perfrom arbitrarily downstream tasks.
Figure 2: To enhance intuition, We show the Signatures and Randomized Signatures for X (smoothed BroW-nian Motion). Note that Xt = St1 + X0 for each t, and that S2 and S3 get smaller and smaller in magnitude,yet more and more regular (smoother, as a result of iterated integrals). The Randomized Signatures resemble amix (i.e. a random linear combination of) the true Signatures: Z1 resembles a shifted version of -S1, While Z2resembles a combination of S1 and S3 . Indeed, Thm. 2 guarantees that Randomized Signatures approximatethe full Signature With no need for computation of iterative integrals. We validate this experimentally in Sec. 4.
Figure 3: Randomized Signature approx- Figure 4: Performance of the Randomized Signature autoencoder.
Figure 5: Tumor Growth Kinetics stimulated with Scaled Squared BM (left) - Step Function (center) - BM(right). Model is only trained on scaled squared BMs, so the input in the central plot is out of distribution.
Figure 6:	Electrochemical Battery Model: Example of fit for a Train Sample (left) - Predictions on Test Samplefor different k (left) - Comparison with NNARX (right)X is drawn from the battery. We use the open source NASA Prognostic Model Package (Teubertet al., 2021) to simulate voltage trajectories given input current control paths. On a fixed equallyspaced partition D of [0, 500], we model the input current with step functions taking values 0 or 1on random sub-interval of [0, 500]. To test the robustness of our approach, we train a Ridge Regres-sion to map NT rain = 1000 instances of k-dimensional Randomized Signature of the controls intothe respective solutions to which we add white noise. The left of Figure 6 shows the comparisonbetween the ground truth and our prediction on an in-sample trajectory. In the same figure, we com-pare the generated time series for several choices of k on a test sample along with the path of theinput control. In Figure 6 we show the performance of our method in comparison with a nonlinearneural ARX (NNARX) — which is often used in the modern control theory/system identificationliterature as a benchmarch (see e.g. (Masti and Bemporad, 2018)), given its strong theoretical guar-antees (Schoukens and Ljung, 2019). Table 8 in Appendix reports the performances in terms ofMSE on 1000 test samples for our models compared to two versions of NNARX: NNARX 22 whichhas around 1000 trainable parameters just like our model with k = 1000, and NNARX 1000 whichmatches our best model in terms of MSE but has around 106 trainable parameters. Details arereported in Appendix B.2.
Figure 7:	Randomized Signature vs. Truncated Signature model. (Left) Number of trainable parametersfor Randomized Signature is significantly smaller regardless of the number of controls. (Center) TruncatedSignature is much slower than Randomized Signature in high dimensions. (Right) As opposed to RandomizedSignature, the performance of Truncated Signature degrades as the number of controls increases.
Figure 8: Mean rank of the modified version of Rocket described in the text and a number of deep learningbaselines (results taken from (Fawaz et al., 2019)) on 128 datasets from the UCR archive. Our method isreferred to as “rocket_ours”. The other methods are, from left to right: Time Le-Net (Le GuenneC et al., 2016),Multi Channel Deep Convolutional Neural Network (Zheng et al., 2014), Time Warping Invariant Echo StateNetwork (Tanisaro and Heidemann, 2016), Time-CNN (Zhao et al., 2017), Multi Layer PerCeptron (Wang et al.,2017), Encoder (Serra et al., 2018), Fully Convolutional Neural Network (Wang et al., 2017), Residual Network(Wang et al., 2017).
Figure 9: Double Well: Randomized Signatures (left) - Test Sample (right).
Figure 10: 4-Dimensional Ornstein-UhlenbeCkProcess: Test Sample.
Figure 11:	Double Well: Irregularly Sampled Test Sample (left) - Fractional Geometric BrownianMotion Test Sample (right).
Figure 12:	Enzyme-Substrate Reactions stimulated with Squared Brownian Motion (left) - StepFunction (right).
Figure 13: Out-of-Sample comparison of NCDE(nchannels , nnodes) models, Randomized Signature modeland Ground Truth.
