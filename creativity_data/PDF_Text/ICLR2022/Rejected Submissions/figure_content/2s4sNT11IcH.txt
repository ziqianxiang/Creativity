Figure 1: Illustration of global (upper) and local per-sample clipping (lower) in Algorithm 1.
Figure 2: Attack model's ROC-AUC on entireCIFAR10 in Section 6.2. non-DP AUC, 0.717;DP-SGDlocal, 0.644; DP-SGDglobal, 0.648.
Figure 3: Loss (left) and accuracy (right) on MNIST with 4-layer CNN under differentclipping methods, batch size 256, learning rate 0.15, noise scale 1.1, clipping norm 1.0; forglobal clipping, we choose Z = 210 as the maximum gradient bound, (, δ) = (2.32, 10-5).
Figure 4: Reliability diagrams (left for non-DP; middle for global clipping; right for localclipping) on MNIST with 4-layer CNN.
Figure 5: Loss (left and middle) and accuracy (right) on CIFAR10 with 5-layer CNN underdifferent clipping methods, batch size 250, learing rate 0.05, noise scale 1.3, Z = 75, clippingnorm 1.5 (flat). For layerwise clipping, global: [1.5, 0.3] per layer (1.5 for weights, 0.3 forbiases); local: [1.5, 1.5], (, δ) = (1.96, 10-5).
Figure 6: Reliability diagrams (left for non-DP; middle forConfidenceConfidenceclipping; right for localclipping) on CIFAR10 with 5-layer CNN.
Figure 7: Loss (left), accuracy (middle) and calibration after switching clipping (right) onSNLI with pre-trained BERT, batch size 32, learning rate 0.0005, noise scale 0.4, Z = 1000,clipping norm 0.1, (, δ) = (1.25, 1/550152).
Figure 9:Performance of DP optimizersIO06 XlO-12×10q10q	101Epochunder different clipping methods on the WineQuality with Z = 400 (left) and the California Housing datasets with Z = 2000 (right).
