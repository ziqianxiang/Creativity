Figure 1: End-to-end overview of pBE with E=6 experts, M =2 partitions, sparsity of K=2, anda “last-2” configuration. Top: pBE contains a sequence of ViT blocks, followed by alternating p-MoE and ViT blocks. images are split into patches whose linear embeddings are processed by eachblock. Here, We show 1 embedding for each of three images (□, □, □). In practice, We have manyembeddings including a special class embedding, as in Dosovitskiy et al. (2021). Bottom left: ina p-MoE block, We replace the ViT block’s MLP With parallel partitioned expert MLPs, see (3). Em-beddings are tiled (□) in the first p-MoE block only. The effect of the routing weights is not depicted.
Figure 2: The effeCt of inCreasing statiC (M) and adaptive (K) ensembling. ImageNet performanCefor ViT-S/32 models. Yellow indiCates better performanCe; purple indiCates worse performanCe.
Figure 3: ViT (o) and V-MoE (V)ensembles of size M ∈ {1, 2, 4}(denoted by markers of inCreas-ing size) for S/32 (O), B/32 (O),L/32 (O), L/16 (◊), and H/14 (◊)on ImageNet.
Figure 4:	ImageNet NLL (left, center left) and mean 10-shot error across datasets (center right,right). We provide zoomed-in plots of the highlighted areas. The dashed lines show Pareto frontiers.
Figure 5:	Quality of uncertainty estimates. ImageNet ECE (left), near (center left) and far (cen-ter right, right) OOD detection, measured by false positive rate at 95% precision (Fort et al., 2021).
Figure 6: NLL in the presence of distribution shift for models trained on ImageNet. For ImageNet-C, we provide a zoomed-in plot of the highlighted area. The dashed lines represent Pareto frontiers.
Figure 7: Estimated φ compared to the ImageNet NLL values for our ViT models. We also includethe tangent at the points corresponding to each ViT model to indicate the gradients at those points.
Figure 8: Comparison for the impact on ImageNet NLL of variations in K, E and M. The underly-ing model is ViT-S/32.
Figure 9:	Extended results for Figure 3 to a selection of other tasks and metrics. We see that in mostcases, ensembles tend to help ViT and V-MoE equally.
Figure 10:	Extended few-shot results from Figure 4 with an additional aggregation method andnumbers of shots.
Figure 11:	Extended OOD detection the results from Figure 5 with an additional OOD dataset andmore metrics.
Figure 12: Extended OOD detection the results from Figure 5 with Cifar100 as the in-distributiondataset, an additional OOD dataset, and more metrics.
Figure 13: Extended results from Figures 4, 5 and 6 with additional metrics.
Figure 14: Results for Cifar10 and Cifar10-C.
Figure 15: Results for Cifar100.
Figure 16: Results for Oxford Flowers 102.
Figure 17:	Results for Oxford IIIT Pet.
Figure 18:	Results for V-MoE with K ∈ {1,2,4,8} and pBE with K ∈ {1, 2, 4}. Models withlarger values of K have larger FLOPs.
Figure 19:	Results for pBE with M = 4 and K ∈ {1, 2}.
Figure 20: Replication of Figure 2a, averaged over two new random seeds, showing the effect on LLof increasing static (M) and adaptive (K) ensembling. ImageNet performance for ViT-S/32 models.
Figure 21:	The effect of λ, which controls the strength of the load balancing loss as described inappendix A of Riquelme et al. (2021), on NLL, Error, ECE and diversity, for pBE and V-MoE. Theresuls are averaged over three random seeds. All models have a ViT-B/32 architecture.
Figure 22:	The influence of the noise standard deviation of the expert MLPs’ initial random weights.
