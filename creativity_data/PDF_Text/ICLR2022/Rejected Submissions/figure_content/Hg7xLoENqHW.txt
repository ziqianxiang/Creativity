Figure 1: A schematic illustration. Suppose anupdate is constrained by a divergence with re-spect to current πt (gray). MD is performed inthe space defined by Ψω. ∏t+ι is recovered inthe desired space ∆A (solid curve) using VΩ*.
Figure 2: The policy πt minimizes an online cost Dω(∙k∏E,t) associated with a convex regularizersuch as negative Shannon and Tsallis entropies. The left example shows that the number of updatesof πt+1 vary by the step sizes η of MD formulation. The plots on the right show that DQ (πt k∏E) varyby η (averaged over 10 different trials). The solid red lines represent the baselines of DQ(πE,t k∏E).
Figure 3: Illustrations of mirror descent imitation learning in the (a) t-th iteration and (b) (t + 1)-th iteration. Consider that {πE,t}∞=1 is a random process and Πe,t and πE,t+1 are sampled from aneighborhood of πE with respect to a norm. The MD step is taken in the interval of πt and ΠE,t. Notethat by decreasing the step size of updates, the region of πt+1 (blue) shrinks.
Figure 5: The average Bregman divergence measured on the log scale in multi-armed bandits at theaction size of |A| = 103. The shaded area represents 95% confidence interval for five runs.
Figure 6: (a) The multi-goal environment, MD-AIRL trajectories, and the ground-truth rewardsare shown. (b) The information entropies for the probabilities of achieving four goals. The x-axis indicates the q value of the Tsallis entropy regularizers. The Shannon entropy regularizer isconsidered by the case of q = 1. (c) The top of each column shows regularized reward surfacesobtained by πν. The middle and bottom show regularized rewards from πφ and the policy πθ .
Figure 7: Average scores in MuJoCo benchmarks. The x-axis indicates the q value of the Tsallisentropy regularizers. Shaded regions indicate 95% confidence intervals for four different runs. Top:4 demonstrations. Bottom: 100 demonstrations.
Figure 8: Average scores during training with 4 demonstrations (Tsanis regularize] T with q = 2).
Figure 9: Schematic illustrations of MD-AIRL reward models for discrete (left) and continuouscontrol (right)C.2 Multi-goal environmentLet the 2D coordinate denote the position of a point mass on the environment. In the multi-goalenvironment, the agent is initially located according to the normal distribution N (0, (0.1)2I). Thefour goals are located at (6, 0), (-6, 0), (0, 6), and (0, -6), where the agent can move a maximum of23Under review as a conference paper at ICLR 20221 unit per timestep for each coordinate. The ground-truth reward is given by the difference betweensuccessive values of a Gaussian mixture depicted as Figure 10.
Figure 10: Visualization of the multi-goal environment.
