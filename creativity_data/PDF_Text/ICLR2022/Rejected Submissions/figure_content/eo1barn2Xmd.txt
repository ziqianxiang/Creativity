Figure 1: Optimization using SGD, Naive BFGS,BFGS with momentum and Exact Newton.
Figure 2: Training loss (left) and validation accuracy (right) of SLIM-QN, KFAC and SGD onImageNet using ResNet-50 model. The model trained with SLIM-QN benefits from faster early-stageconvergence and achieve comparable generalization performance as SGD. We plot the mean andstandard error over 3 runs with different random seeds.
Figure 3: Training loss (left) and validation accuracy (right) of SLIM-QN and SGD on ImageNetusing a Vision Transformer model. The model trained with SLIM-QN benefits from faster early-stageconvergence and achieve better generalization performance compared to SGD. We plot the mean andstandard error over 3 runs with different random seeds.
Figure 4: Ablation analysis for SLIM-QN on ResNet-18/CIFAR-10 (batch size: 256).
Figure 5: Convergence on ResNet-18/CIFAR-10 using SGD and SLIM-QN.
Figure 6: Convergence on ViT/CIFAR-10 using SGD and SLIM-QN.
Figure 7: Convergence on ResNet-19/CIFAR-10 using the classical BFGS.
Figure 8: Block-wise SLIM-QN for distributed systems. Models are divided into blocks, which arethen optimized by SLIM-QN in multiple nodes.
Figure 9: Convergence on ResNet-18/CIFAR-10 using SGD, SLIM-QN, and block-wise SLIM-QNEpochsE.1 Convergence GuaranteesIn this section, we prove that block-wise SLIM-QN also converges in a linear rate given assumptionin Sec. 4.1. Furthermore, the convergence property is guaranteed in any arbitrary way of dividingmodels.
