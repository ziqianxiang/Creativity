Figure 2: Our proposed self-supervised learning pipeline for combinatorial optimization. The prob-ability of selecting each element is predicted by a graph neural network, and an accurate estimationof the combinatorial objective score is achieved via near-discrete TopK solution by our GS-TopK.
Figure 1: Atoy example to explain Lemma 2: Se-lect top-3 items from [1.0, 0.8, 0.601, 0.6, 0.4, 0.2]and we compare GS-TopK and SOFT-TopK con-cerning the gap to a discrete solution w.r.t. differ-ent τ, σ configurations. Here the gap to discretesolutions is tighten by a larger σ and a smallerτ for GS-TopK, compared to SOFT-TopK whosegap is larger and can only be controlled by τ .
Figure 3: In discrete clustering, comparison of the estimated objective score (red) and the real one(blue). Our GS-ToPK achieves the best accurate estimation of the objective score.
Figure 4: Four conditions considered in our proof. It is worth noting that xi, xj must not lie betweenxk, xk+1, because we define xk, xk+1 as two adjacent items in the original sorted list.
