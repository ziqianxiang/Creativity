Figure 1: Illustration of our framework. (a) An example multilayer perceptron (MLP) GA is mappedto a directed line graph GB , which is governed by an edge dynamics B. Each node (dichromaticsquare) of GB is associated with a synaptic connection linking two neurons (in different colors) fromdifferent layers of GA . (b) A diagram of transfer learning from the source domain (left stack) toa target domain (right stack). The pre-trained model is modified by adding additional layers, i.e.
Figure 2: Learning curves of five representative pre-trained models w.r.t accuracy (row 1) and βeff(row 2). A regularized linear model h(∙; θ) (blue curve in row 3) is estimated with Bayesian ridgeregression using a few of observations of βeff on training set and validation accuracy I during earlyTrain ∖β∖Train ∖β∖0.92-	*'βS0.90-0,88 7 t0=4,BΓC=45io-2' " io-1......ioo ''Train ∖β∖fine-tuning. The starting epoch t0 of observations affects the fit of h, and is automatically determinedaccording to BIC, and the true test accuracy at epoch 50 is predicted with I * = h(0; θ*).
Figure 3: (a) Our βeff based prediction of the validation accuracy versus the true test accuracy atepoch 50 of seven representative pre-trained models. Each shape is associated with one type ofpre-trained models. Distinct models of the same type are marked in different colors. Because theaccuracy of AlexNet is much lower than others, we exclude it for better visualization. Its predictedaccuracy is 0.871, and the true test accuracy is 0.868. If it is included, ρ = 0.93 > 0.92. (b) Impactsof the starting epoch t0 of the observations and (c) the number of training samples on the rankingperformance of our βeff based approach.
