Figure 1: From left to right: Ground truth image X in the data space, corrupted image d in the dataspace (random masking m, Gaussian blur C, additive white Gaussian noise n), posterior mean Hin_the latent space with reconstruction uncertainty δr, model uncertainty δm and the restored imageg(H) (decoded posterior mean) in the data space. We have included the encoding of the uncorrupteddata f(x) (illustrated by the shaded white bars in the third column). Top row: data sample from theMNIST-dataset (ground truth label: 4). Bottom row: data sample from the FashiOn-MNIST-dataset(ground truth label: 2 (pullover)). We can classify d using the posterior mean H as the autoencoder'slatent space is supervised (note the highlighted max. activation responsible for classification). Weare able to classify and quantify model uncertainty δm with the Mahalanobis-distance in the latentspace (note the highlighted min. activation responsible for classification). Strong overlapping for theFaShiOn-MNIST-example of the 1 ∙ σ error bars of δr across different classes indicates that no reliableand confident classification is possible due to heavy corruption.
Figure 2: Concept visualization: Steps involved in classifying corrupted data and quantifyinguncertainty of the reconstruction, as described in Section 2.1. The arrow from dto P(h|d) graphicallyillustrates the process of statistically inferring the latent space activations from corrupted data d usingMGVI. H includes all samples drawn from the posterior distribution in the latent space. Subsequently,the sampling mean and sampling standard deviation are determined from H to classify corrupteddata samples and quantify their reconstruction uncertainty. Model uncertainty is determined via theMahalanobis distance in the latent space. We do not depict convolution C and masking m in thefigure above for simplicity.
Figure 3: Illustration of the latent space structure of a semi-supervised autoencoder and the M-distance as a classifier based on MNIST. For this visualization, the 10-dimensional latent spaceactivations are mapped to a two-dimensional subspace using a Principal Component(PC) analysis(Wold et al., 1987). For an arbitrary corrupted datum d, the inferred posterior mean H in the latentspace is marked accordingly. To classify H, the M-distance is computed to every cluster in the latentspace to obtain δmi for all ten classes. The shortest distance arg min(δm) serves as the classification.
Figure 4: Accuracy and uncertainty (reconstruction uncertainty δr and model uncertainty δm) ofclassifications of data samples of the MNIST dataset (for Fashion-MNIST see Figure 10 in theAppendix A) at different noise levels (left column), different masking levels (middle column), anddifferent convolution levels (right column) exploiting the supervised latent space structure (top row)and the M-distance (bottom row) as classifying features.
Figure 5: Uncertainty based Receiver Operator Characteristics (U-ROC) of the proposed identifier offalse classifications for different noise levels α of our method in comparison with MC dropout andwith EDL. In this experiment, the formulation of the e.g. "TRUE NEGATIVE" case would be: Basedon the uncertainty value, the sample is correctly detected as afalse classification - the "lie detector"works. Samples are taken from the MNIST-dataset. Top left: corrupted datum at α = 0.1. Bottomleft: corrupted datum at α = 1.0. The irregularity in the U-ROC-Curve of the dropout model is dueto the stochastic nature of MC dropout. Bottom right: Evaluation of Accuracy (ACC) for all givennoise values α and the Area under the Curve (AUC) for all U-ROCs.
Figure 6: Exemplary visualization of corruption through noise. Experiments cover the entire noiserange from α = 0.01 to α = 10.
Figure 7: Exemplary visualization of isolated masking. Experiments cover the entire masking rangefrom β = 0 to β = 14 and additional noise at α = 0.1. The experiment layout of masking is adoptedfromγ= 1	γ= 1.5	γ= 2	γ= 10Figure 8: Exemplary visualization of corruption through convolution. Experiments cover the entireconvolution range from γ = 0.1 to γ = 10.
Figure 8: Exemplary visualization of corruption through convolution. Experiments cover the entireconvolution range from γ = 0.1 to γ = 10.
Figure 9: Exemplary visualization of different masking levels applied in experiments with additivenoise α = 0.1 on top. Experiments cover the entire masking range from β = 0 to β = 14.
Figure 10: Accuracy and uncertainty (reconstruction uncertainty δr and model uncertainty δm ) ofclassifications of data samples of the Fashion-MNIST dataset (for MNIST see Figure 4) at differentnoise levels (left column), different masking levels (middle column), and different convolution levels(right column) exploiting the supervised latent space structure (top row) and the M-distance (bottomrow) as classifying features.
