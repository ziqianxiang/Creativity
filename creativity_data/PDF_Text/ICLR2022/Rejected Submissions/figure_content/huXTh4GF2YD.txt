Figure 1: Given (a) a latent feature space, we demonstrate (b) closed-set and (c) open-set classificationexamples, where KKCs and UUCs are known and unknown classes, respectively.
Figure 2: PH and PI (Ours)At training time, note that each PI (x, C) = 0.5 builds an auxiliary decision boundary between theC-th class KKC sample and the others, where Lbg,u ensures a majority of KUC samples locatedoutside the entire class-wise boundaries. However, Lbg,u can be insufficient to achieve robust UUCrejection and closed-set classification results, since it does not control correctly classified KKCsamples to be located inside the corresponding class-wise boundaries. Thus, in addition to Lcf, weimpose a regularization Lbg,k on KKC data to maintain closed-set classification performance andenhance the gap between KKC and KUC data in terms of the Euclidean distance. By formulatingLbg,k = E(xk,y)〜Dt [-l(y = C) log(Pι(xk, C))], where C = arg maxi∈{1,…°} PI(xk, i), we defineour BCR loss as Lbg = Lbg,k + Lbg,u and call Lbg the class-inclusion loss. In summary, we useL = Lcf + λLbg = Lcf + λ(Lbg,k +Lbg,u)=E(Xk ,y)~Dt [-log Pd (y |xk) - λEχb 〜Db H (y = C)Iog(PI (Xk ,C)) + log(1- PI (Xb )川 ⑼as our total loss, where C = argmaxi∈{1,…，c} PI(xk, i) and PI(xb) = maxi∈{1,…，c} PI(xb, i).
Figure 3: t-SNE results of vanilla Softmax classifiers and distance-based classifiers.
Figure 4: t-SNE results of regularized Softmax classifiers and distance-based classifiers.
