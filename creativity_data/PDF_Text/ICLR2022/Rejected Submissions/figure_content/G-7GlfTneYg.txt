Figure 1: The neural and cognitive model of how human brain understand and restore distorted speech.
Figure 2: Overview of the proposed VoiceFixer system.
Figure 3: The architecture of ResUNet, which output has the same size as input.
Figure 4: The architecture and training scheme of TFGAN, whose generator is later used as vocoder. Thegenerator takes mel spectrogram as input and upsampled it into waveform. Both output waveform and its STFTspectrogram are used to compute loss. We employ both time and frequency discriminators for discriminativetraining.
Figure 5: Box plot of the MOS scores on generalspeech restoration task. Red solid line and greendashed line represent median and mean value.
Figure 6: Box plot of the MOS scores on speech super-resolution, declipping, dereveberation and denoising.
Figure 7: The architecture____________________ Low QualitySpectrogramJ	BatChNOrm | Linear0 128 -> 256BatchNormJ	BidireCtionaI GRU 256 -> 256 layer1I	BidireCtionaI GRU 256 -> 256 layer2I RelU I BatChNOrm | LinearI 512-> 256RelU I BatChNorm | Linear2 256-> 128!Restoration Mask------------------------Hg)IRestored Spectrogram(b) BiGRUof DNN and Bi-GRUThe DNN and BiGRU We use are shown in Figure 7. DNN is a six layers fully connected networkwith BatchNorm and ReLU activations. The DNN accept each time step of the low-quality spec-trogram as the input feature and output the restoration mask. Similarly, for the BiGRU model, wesubstitute some layers in DNN to a two-layer bidirectional GRU to capture the time dependency
Figure 8:	The structure of T-discriminator.
Figure 9:	The structure of F-discriminator.
Figure 10: Comparison between different restoration mothods. The unprocessed speech is noisy, reverberant,and in low-resolution. The leftmost spectrogram is the unprocessed low-quality speech and the rightmost isthe target high-quality spectrogram. In the middle, from left to right, the figures show results processed byone-stage SSR dereveberation model, SSR denoising model, GSR model and VoiceFixer based GSR model.
Figure 11: Comparison between different model on four different tasks using simulated data.
Figure 12: Restoration on the audios either collected from the internet or recorded by ourselves.
