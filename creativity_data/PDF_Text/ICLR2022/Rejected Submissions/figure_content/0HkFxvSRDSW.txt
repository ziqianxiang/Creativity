Figure 1: (a) Performance of MARL algorithm 1 & 2 combined with different parameter sharing,communication, and credit assignment strategies varies from candidates 1 to n. (b) Using RoleDiversity to describe each task, We can ensure the best combination of different strategies.
Figure 2: (a) An illustration of how policy-based role difference varies in one game(4m_vs_3z). Adetailed explanation of how it varies can be found in Sec. 3.1. (b) Instance curve of the trajectory-based role difference in different battle scenarios according to observation overlap. Trajectory-basedrole difference is larger in 3s_vs_5z but smaller in 4m_vs_5m. (c) Contribution-based role diversityin different battle scenarios represented by Q value. The contribution-based role diversity is largerin 1s1m1h1M_vs_5z but smaller in 4m_vs_3z.
Figure 3: Performance curves include Q ValUe-based(first row) and policy gradient-based(seCondrow) credit assignment with Shared and No shared parameter sharing strategies.
Figure 4: Histogram of (a) model performance when adopting communication mechanism comparedto baseline (w/o communication). (b) model performance with different vision scope (6-9-18), wherescope 9 is the standard setting in SMAC. Grey dots represent the observation overlap. Larger theoverlap, the smaller the trajectory-based role diversity. Detailed analysis can be found in Sec. 5.2circle. Therefore, it is critical to determine when and how to accept the extra information providedvia communication mechanism in the context of cooperative MARL. As discussed in Sec. 4, thepattern of different agentsâ€™ support sets for policy optimization can determine whether or not theextra information is needed. Notably, the similarity of these support sets is largely dependent onthe trajectories of different agents, corresponding to the trajectory-based role diversity defined inSec. 3.2. Small trajectory-based role diversity corresponds to a similar support set pattern, whichmeans that forming a concentrated input is preferred for policy optimization. Experiment results intable. 7 and Fig. 4 further prove that scenarios with larger observation overlap are more suitable forcommunication. Detailed setting of communication mechanism on SMAC can be found in Appx. GTo prove that small trajectory-based role diversity prefers obtaining extra information via commu-nication and vice versa, we conduct extra experiments to study the relationship between the patternof the input observation (support set) and the model performance by shrinking the vision scope (r ineq. 14). The results can be found in table 2. The results show that the model performance is stronglyrelated to the vision scope and the trajectory-based role diversity determines whether the small orlarge vision is preferred. Small trajectory-based role diversity prefers large vision scope, indicat-ing the similar pattern of support set is better. Large trajectory-based role diversity prefers small
Figure 5: A multi-agent visual language navigation task. Agents are initialized in different locationsand the target description is given. Agents need to cooperate with each other to find the targetlocation according to the description as soon as possible.
Figure 6: Using real images as observation overlap percentage calculation. TWo methods are Pro-posed including vector-wise cosine similarity and channel-wise threshold-based similarity percent-age. A detailed discussion can be found in Sec. c.3it is also possible that We can get observation overlap directly based on real image MARL tasks. AsshoWed in Fig. 6. Passing the input image to pre-trained cNN/Transformer backbone and gettingits feature, We can use cosine similarity or channel-Wise similarity to compute the overlap betWeendifferent observation features as daT0 ,a1 in Eq. 3. HoWever, these methods can bring large noise tothis metric. Moreover, hoW to stabilize the reinforcement learning With real pictures as input is stillunder investigation. in addition, it is rare in MARL tasks that the only information provided in thetraining stage is one single image. Location and communication are necessary auxiliary informationto help learn the coordination of agents in most MARL tasks. Therefore, simply using the raW imageto calculate the observation overlap can be a choice, but not the best choice.
Figure 7: An illustration of different role based on MPE.
Figure 8: Illustration of policy-based role, trajectory-based role, and contribution-based role on realscenarios from SMAC.
Figure 9: An overview of how knowledge sharing works with the MARL framework. Fully shared,partly shared, no shared (separated), and selectively shared[8] strategies are shown here. The samecolor indicates the same policy function part across different agents. Dash line represent only sharingno gradient backpropagation.
Figure 10: Communication works as a supplement part for MARL under the CTDE framework.
Figure 11: Credit assignment method focuses on assigning the proper individual reward from thetotal reward to update.
Figure 14: Policy learning curve with different vision scope (6-9-18).
Figure 15: Observation overlap curve of one episode game on different battle scenarios. Thepolicy is trained using VDN[48] and no parameter sharing. We also provide the curve of gameprogress(equals to the enemy health), ally in scope and ally alive. All values are normalized from 0to 1.
Figure 16: Policy based role diversity(real) in one episode.
Figure 17: Policy based role diversity(semantic) in one episode.
Figure 18: Q value curve in one episode on different scenarios.
