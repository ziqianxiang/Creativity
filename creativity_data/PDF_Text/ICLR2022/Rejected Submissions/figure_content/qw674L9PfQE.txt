Figure 1: The CLOOB architecture for image-text pairs. The image embedding Xi and the text em-bedding yi retrieve the embeddings Uxi and Uyi, respectively, from a modern Hopfield network thatstores image embeddings U = (u1 , . . . , uM ) (green boxes at the left). The image-retrieved imageembedding Uxi serves as anchor in order to contrast the positive text-retrieved image embeddingUyi with the negative text-retrieved image embedding Uyj for j 6= i. Analog, for the second modernHopfield network that stores text embeddings V = (v1, . . . , vK) (green boxes at the right).
Figure 2: Variance reduction of InfoLOOB by modern Hopfield networks. From left to right: withoutHopfield for MI 10, with Hopfield for MI 10, without Hopfield for MI 14, with Hopfield for MI 14.
Figure A1: The estimated mutual information of the InfoNCE objective saturates at the batch sizeinduced bound. The InfoLOOB objective trained with the same batch size with samples from thesame correlated Gaussian distributions following (Belghazi et al., 2018; Poole et al., 2019; Chenget al., 2020) is not limited by that bound and better estimates higher mutual information but suffersfrom higher variance. This is remedied by incorporating the modern Hopfield network.
Figure A2: Visualization of zero-shot classification of three examples from each dataset. The follow-ing datasets are used (top to bottom): ImageNet, ImageNet V2, Birdsnap, Country211, Flowers102,GTSRB, Stanford Cars and UCF101. The ground truth label is displayed above the picture. The barplots show the softmax values of the top 5 classes.
