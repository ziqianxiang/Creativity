Figure 1: (a) MLM validation loss of BERT-Small with a random subset of parameters set to zero(solid blue curve) or kept untrained (dashed orange). (b) training loss curves of BERT-Small duringpre-training of 10 epochs (757k steps) fixing a random subset of the parameter either early (orangedashed) or late (blue dash-dotted) during the training, as well as for the dense baseline (solid black).
Figure 2: Schematic illustration of pruning and re-allocation stepin a typical DynSparse training algorithm leading to an evolutionof the network representation in parameter space. The dynamicevolution of the sparsity pattern allows the DynSparse trainingalgorithm to explore a larger fraction of the network parameterscompared to static sparsity, while remaining "always sparse". Forunstructured DynSparse training, the granularity of the sparsitypattern is of block size 1×1, while for structured DynSparsetraining, the block size is chosen between 4×4, 8×8 and 16×16.
Figure 3: Pareto curve of the BERT family (Turc et al.,2019), comparing validation MLM loss of unstructuredDynSparse training (orange dotted line) with static spar-sity (solid blue line) and the dense baseline (black dashedline, the standard deviation is not visible at this scale) asa function of FLOPs. All sparsity results are obtained forpre-training with sparsity ratio 0.9, n = 160 pattern up-dates, and optimal pruning ratio pr = 0.5 (see Figure 5).
Figure 4: Comparing validation MLMloss of DynSparse training of BERT-Medium with various sparsity ratios (in-dicated by color and marker style andjoint by orange dotted line) with densetraining of BERT family (black dashedline) as a function of non-embeddingFLOPs. For all sparsity ratios, we usethe hyperparameters optimized for spar-sity ratio 0.9.
Figure 5: Characterization of the DynSparse pre-training of BERT-Medium with sparsity ratio 0.9.
Figure 6: MLM validation loss of DynSparseBERT-Medium with sparsity s = 0.9 for block sizeB = 16 as a function of the regularization coefficientfor Group Lasso regularization (solid blue) or weightdecay (orange dashed). The error bars correspond tothe standard deviation over three runs. Number ofupdates n = 80, pruning ratio pr = 0.5.
Figure 7: Block metric dependence ofDynSparse training of BERT-Medium withsparsity S = 0.9 for block size B = 16.
Figure 8: Block size dependence of thereduction in FLOPs for DynSparsetraining compared to (interpolation) ofthe dense BERT family for a given taskperformance. Values correspond to theblock sparse DynSparse training ofBERT-Base given in Table 1.
Figure 9: BERT-Medium with static unstruc-tured sparsity s = 0.9 imposed on all weightsusing Glorot (blue) or truncated normal (or-ange) initialization scheme. The marker shapeindicates whether the standard deviation ofthe weight initializiation was increased.
Figure 11: (Left panel) Fraction of explored degrees of freedom for static sparsity and unstructuredDynSparse training using gradient based (RigL) (Evci et al., 2019) vs random re-allocation (Dettmers& Zettlemoyer, 2019). (Right panel) Corresponding sparsity patterns for the first up-projection inthe feedfoward component ("Boom-up") of the second transformer block, accumulated throughouttraining, for sparsity ratio 0.9 using gradient based (RigL) and random based reallocation. A black(white) dot corresponds to a parameter being non-zero (zero) at any point during training. The darkhorizontal blocks in the RigL updates indicate a collapse due to outliers along the input dimension,which indicates that the effect arises from the activation part of the dense gradient update. Thissuggests that the collapse could be mitigated by reducing the influence of the activations duringDynSparse training update.
Figure 10: MLM loss vs pruning ratio pr timesnumber of sparsity pattern updates n for un-structured DynSparse training of BERT-Mediumwith sparsity ratio 0.9 for different values of(Top panel) pruning ratio pr (with n = 160) and(Bottom panel) sparsity pattern updates n (withpr = 0.5). Same data as in Figure 5.
Figure 12: MLM validation loss for the Top paneldense BERT family Bottom panel static BERTof different sparsity ratios between 0 and 0.9 as afunction of learning rate. The solid line correspondto a cubic fit for all data with the same sparsity ratio.
Figure 13: (Left panel) Dense fit to the optimal learning rate estimated as the position of the blacktriangles from Figure 12 for BERT-Medium with various sparsities and the dense BERT-family asa function of the number of trainable parameters N for various model sizes (indicated by symbolstyle and color) and sparsity ratios (colored crosses). The black lines indicate linear fits that arebest approximated by log(η) = -0.8383(±0.05) log(N) + 6.13(±0.7) for the sparse models andlog(η) = -0.44(±0.05) log(N) - 0.47(±0.9) for the dense models. (Right panel) Testing theprediction of the optimal sparse learning rate from Eq. 3 (markerstyle "+") on BERT-family withsparsity 0.9 and block size 16×16 (values given in Table A.12).
