Figure 1: The overall architecture of the proposed SERCNN.
Figure 2:	Illustration of stIn this study, our SE is the simple stacked vector made up of two pretrained GloVe embeddings(Pennington et al., 2014) trained on Twitter and Wikipedia 2014 + Gigaword 5 datasets, respectively:1.	GloVe Twitter (25 dimensions) trained using global word co-occurrences information byPennington et al. (2014) under an uncased setting, using 2 billion tweets, with 27 billiontokens. The resulting vector consists of 1.2 million vocabularies learned from the corpus.
Figure 3:	Recurrent Convolutional Neural Network (RCNN) using single Forward LSTMThe overall architecture of our RCNN model is visualized in Figure 3. Instead of using bidirectionalLong Short Term Memory (LSTM) as described in Lai et al. (2015), we use a generic single ForwardLSTM to learn the context, c, from the embedding vector, v . With j refers to the jth social mediapost and k refers to the kth number of words in the post, we can formulate the context for a givenword:c(wkj), h(wkj) = LSTM c(wkj-1),h(wkj-1),v(wkj)	(6)where |c| ∈ R and h is the output vector.
Figure 4: Performance of SERCNN and SELSTM trained on different number of postsFinding optimal number of posts The performance of our proposed SERCNN is almost identicalto its single embedding counterpart, but these models’ performances are topped in the Table 3. Thisobservation suggested that RCNN architecture can capture significant embedding features that areneglected by LSTM and relate these features with the context learned by LSTM in discriminating de-pression users and healthy controls. From the identical results, we can deduce that the performancereaches the state of plateau when we include all social media posts collected within a month.
