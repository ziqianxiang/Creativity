Figure 1: A comparison between typical end-to-end task-specific GNN explainers and the proposedtask-agnostic explanation pipeline. To explain a multitask model, typical explanation pipelines needto optimize multiple explainers, whereas the two-stage explanation pipeline only learns one embed-ding explainer that can cooperate with multiple lightweight downstream explainers.
Figure 2: Overviews of the self-supervised training framework for the embedding model (right) andthe architecture of the parametric explainers (left). During training, we generate random conditionvectors p as an input to the embedding explainer and mask the embeddings. The learning objectiveseeks to maximize the mutual information between two embeddings on certain dimensions.
Figure 3: Quantitative performance comparisons with baseline methods on six tasks from Molecu-leNet (top row) and PPI (bottom row). The curves are obtained by varying the threshold for selectingimportant edges.
Figure 4: Visualizations on explanations to the GNN model for the BACE task. Top 10% impor-tant edges are highlighted with red shadow. The numbers below molecules are fidelity scores whenmasking-out the top 10% important edges. Right two columns are explanations to two certain em-bedding dimensions without downstream tasks. Fidelity scores in the right two columns explainingtwo embedding dimensions are still computed for the BACE task but are just for reference.
Figure 5: Visualizations on explanations to the GNN model for the HIV task. Top 10% impor-tant edges are highlighted with red shadow. The numbers below molecules are fidelity scores whenmasking-out the top 10% important edges. Right two columns are explanations to two certain em-bedding dimensions without downstream tasks.
Figure 6: Visualizations on explanations to the GNN model for the SIDER task. Top 10% im-portant edges are highlighted with red shadow. The numbers below molecules are fidelity scoreswhen masking-out the top 10% important edges. Right two columns are explanations to two certainembedding dimensions without downstream tasks.
Figure 7: Visualizations on explanations to the synthetic dataset BA-Shapes.
