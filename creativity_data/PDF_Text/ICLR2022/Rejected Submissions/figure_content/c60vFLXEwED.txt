Figure 1: UnsUPervised image-to-image translation results of PIVQGAN with disentangled postureand identity control. In each panel, the first row has input pose images, and the first column hasreferential identity images. The second and third rows are “segmentation-like” masks automaticallylearned by PIVQGAN, and bottom-right are the synthesized images.
Figure 2: Top: Illustration of the VQSN module. (1) A base feature-map is quantized by a certainnumber (e.g. 2 or 6) of trainable embeddings. (2) The VQ embeddings are used to perform thespatial-wise affine transformation on image feature-maps. Bottom: Training scheme of PIVQGAN.
Figure 3: Qualitative comparison of PIVQGAN and otherbaselines on AFHQ dataset.
Figure 4: Left: Object-mask editing. Right: Various augmentation methods used on the model'sidentity inputs, enabling the model to learn the augment-variant attributes from only posture inputs.
Figure 5: Left: VQSN is controlled by the latent vectors, thus it has a smooth posture transactionwhen interpolating between two pose references. Right: When fixing the pose-latents and interpo-lating only the identity-latents, PIVQGAN shows a smooth transaction on identity-related attributes.
Figure 6: PIVQGAN is robust to out-domain images with unseen textures, and can generate mean-ingful in-domain counterparts from these out-domain inputs. (a,b) Out-domain images as identityinput. (c,d) Out-domain images as posture input.
