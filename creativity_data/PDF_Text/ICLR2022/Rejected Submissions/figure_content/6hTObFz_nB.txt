Figure 1: Safety RSSM. In this example, the model observes the environment for two time steps andpredicts the subsequent two time steps. at denotes the action taken at step t, ot the observation, htthe deterministic component of the latent state, Zt the posterior stochastic component, Zt the priorstochastic component, λt the violation prediction (our novel contribution, highlighted in yellow), ^tthe predicted observation and ^ the predicted reward. Circles represent stochastic variables whereassquares represent deterministic variables. An arrow from one shape to another indicates that thesource is used in the calculation of the destination.
Figure 2: Samplevisual observationfrom the VGWenvironment.
Figure 3: Reward curves for the model-based agents on fixed (left) and procedurally generated(right) instances of the VGW environment. The shaded area denotes one standard deviation.
Figure 4: Interacting with the real environment. Observations xt from the environment are encodedand a latent representation is obtained. The representation is passed to π and the shield to obtain anH-bounded safe (with respect to the agent's world model) action ^t. The circle with the plus signrepresents the concatenation operation.
Figure 5: Training a world model. Observations xt from the experience dataset D are encoded by Eand used to generate a posterior stochastic state zi . zi is concatenated with the deterministic state hiand used to predict the reward %, original observation Xi (through decoder D), and whether or nota violation has taken place λi (though network lθ). A prior stochastic state Zi is also generated witha view to matching the posterior state as closely as possible. The circle with the plus sign representsthe concatenation operation.
Figure 6: Violations per episode for agents on fixed (left) and procedurally generated (right) in-stances of the VGW environment.
Figure 7: Training reward curves for agents on psticky = 0.1 (left) and psticky = 0.5 (right) in-stances of the CD environment.
Figure 8: Violations per episode for agents on psticky = 0.1 (left) and psticky = 0.5 (right) instancesof the CD environment.
Figure 9: Comparison of latent trajectories in the agents’ learned models of the environment. Alltrajectories begin from the leftmost observation and time progresses from left to right. The bottomrow of arrows denote the actions taken just before each time step. From top to bottom, the threerows of images correspond to trajectories in the latent space of the BPS agent, the unshielded agent,and our agent.
Figure 10: Plot of violation loss Lviolation from the CD environment. As you can see, the “elbowin the curve occurs at around 60 episodes.
Figure 11: Training reward curves of agents using latent shielding with and without a shield intro-duction schedule (SIS).
