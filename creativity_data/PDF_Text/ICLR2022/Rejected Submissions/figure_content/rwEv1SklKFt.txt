Figure 1: Overview of our attack with a human in the loop. Given a poisoned classifier, we constructa robust smoothed classifier using Denoised Smoothing (Salman et al., 2020). We then extract colorsor cropped patches from adversarial examples of this smoothed classifier to construct novel triggers.
Figure 2: Visualization of some adversarial examples ( = 20/60) from two robustified poisonedclassifiers and a robustified clean classifier. Trigger A and Trigger B are shown in Figure 4.
Figure 3: Backdoor patterns in adversarial examples ( = 20) for robustified poisoned classifiers,triggers shown below adversarial images.
Figure 4: Backdoor triggersused in our analysis.
Figure 5: Comparison of different forms of adversarial examples ( = 20) from a binary poisonedclassifier on ImageNet.
Figure 6: Results for attacking a poisoned multi-class classifier obtained through BadNet (Gu et al.,2017). The attack success rate of the original backdoor Trigger A is 72.60%. The region which weuse to construct alternative triggers is highlighted in a red box.
Figure 7: Results of applying our attack on an ImageNet clean classifier.
Figure 8: Analysis of a poisoned classifier with a “camouflaged” backdoor trigger.
Figure 9: Results of attacking a poisoned classifier in TrojAIdataset.
Figure 10: Interface of interactive tool we develop for TrojAI dataset.
Figure 11:	Results for attacking three binary poisoned classifiers obtained by three backdoor attacks.
Figure 12:	Results for attacking multi-class poisoned classifiers on ImageNet obtained byHTBA (Saha et al., 2020) and CLBD (Turner et al., 2019).
Figure 13: Results of applying our attack on an ImageNet clean classifier (binary).
Figure 14: Results of applying our attack on an ImageNet clean classifier.
Figure 15: Results of attacking 8 poisoned classifiers in the TrojAI dataset.
Figure 16: Results of attacking two clean classifiers in the TrojAI dataset.
Figure 17: Adversarial examples ( = 20 in l2 norm) of a robustified poisoned classifier in the TrojAIdataset. Below each image is the class predicted by the original poisoned classifier.
Figure 18: Comparison of different adversarial examples ( = 20) of a robustified binary poisonedclassifier on ImageNet.
Figure 19: Results of attacking a poisoned ImageNet classifier with 10 classes. The success rate ofthe original backdoor is 59.71%.
