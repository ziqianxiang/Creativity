Figure 1: Visualization of the first layer of AlexNet trained by Adam and SGD on the CIFAR-10dataset. Both algorithms are run for 100 epochs with weight decay regularization and standard dataaugmentations, but without batch normalization. Clearly, the model learned by Adam is more “noisy”than that learned by SGD, implying that Adam is more likely to overfit the noise in the training data.
Figure 2: Visualization of the feature learning(mini maxrhw1,r, ξii) in the training process.
