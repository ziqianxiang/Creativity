Figure 1: The gated four-room setting, with a task re-quiring navigation from thegreen square to the star.
Figure 2: Alchemy. Placingstones in potions moves the agentthrough a latent graph of objectproperties.1Query Model. We work in the online setting, where the agent interacts with the tasks by playingpolicies from the initial state s0 . During meta-training, we allow the agent to interact with theenvironments using an unbounded number of timesteps for each trajectory before resetting. We thencompute query complexity in terms of the total number of timesteps spent in all tasks in total.
Figure 3: Illustrating α-importance. Since theblack arrow is the onlypath to the goal, it isV0f (so)-important.
Figure 4: The black arrow is not α-significant for one of the tasks, Figure 5: Using optimistic imag-but is nevertheless “covered” by optimistic imagination.	ination for exit detection.
Figure 6: Phase I con-tribution to learning πIin Figure 5, markedwith an arrow.
Figure 7: Phase II con-tribution. Optimal pol-icy state coverage (red)is insufficient for learn-ing πI , which requiresthe green region.
Figure 8: The hard instance inTheorem 5.2.
Figure 9: An MDP that does not satisfy low (α, β)-unreliability, where a* is inblue, and a1 is in red (and purple for both actions). State shading representsstate clusters, and rewards are 0 unless indicated otherwise.
Figure 10: An MDP that does not satisfy low γ-goal-reaching suboptimality,with three actions indicated by red, blue, and purple, and exits lh and rh . TheMDP satisfies (∞, 0)-unreliability, yet nevertheless exhibits high hierarchicalsuboptimality.
