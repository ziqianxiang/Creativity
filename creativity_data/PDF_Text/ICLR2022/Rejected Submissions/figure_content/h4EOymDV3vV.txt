Figure 1: Conditional score matching with a parametrized latent code is representation learning.
Figure 2: Results of a DRL model trained on MNIST (a-b) and CIFAR-10 (c-d) using uniformsampling of t. Samples are generated from a grid of latent values ranging from -1 to 1. The pointclouds visualize the latent representation of test samples, colored according to the digit class.
Figure 3: Results of a VDRL model trained with a focus on high noise levels on MNIST (a-b) andCIFAR-10 (c-d). Samples are generated from a grid of latent values ranging from -2 to 2. The pointclouds visualize the latent representation of test samples, colored according to the digit class.
Figure 4: Generated image samples for different values of tinit . Top row ((a)-(f)) uses the gaussianprior, bottom row ((g)-(l)) uses the version with an additional uniform random variable in the prior.
Figure 5: Samples and latent distribution of a model trained on MNIST using KL-divergence anduniform sampling of tA.4 Training on single timescalesTo understand the effect of training DRL on different timescales more clearly, we limit the supportof the weighting function λ(t) to a single value of t. We analyze the resulting quality of the latentrepresentation for different values of t using the silhouette score with euclidean distance based onthe dataset classes Rousseeuw (1987). It compares the average distance between a point to all otherpoints in its cluster with the average distance to points in the nearest different cluster. Thus wemeasure how well the latent representation encodes classes, ignoring any other features. Note that15Under review as a conference paper at ICLR 2022(a) Samples generated from a grid of latent valueson a range from -1 to 1Figure 6: Samples and latent distribution of a VDRL model trained on CIFAR-10 using uniformsampling of t.
Figure 6: Samples and latent distribution of a VDRL model trained on CIFAR-10 using uniformsampling of t.
Figure 7: Samples and latent distribution of a DRL model trained on MNIST using uniform samplingof σ (focus on high noise levels).
Figure 8: Samples and latent distribution of a DRL model trained on CIFAR-10 using uniformsampling of σ (focus on high noise levels).
Figure 9: Samples generated starting from xt (left column) using the diffusion model with the latentcode of another x0 (top row) as input. It shows that samples are denoised correctly only whenconditioning on the latent code of the corresponding original image x0 .
Figure 10: Samples generated using the same latent code for each generation, showing that therandomness of the code-conditional generation of DRL reduces in higher dimensional latent spaces.
Figure 11: Mean and standard deviation of silhouette scores when training a DRL model on MNIST(left) and CIFAR-10 (right) using a single t over three runs.
Figure 12: Classifier accuracies for few shot learning on given 8-dimensional representations learnedusing DRL (SM), VDRL (VSM), Autoencoder (AE) and Variational Autoencoder (VAE).
Figure 13: Comparison of the distribution of t and the respective loss with and without including theadversary λ0α .
Figure 14: Value of parameter α over training, where 2α is the slope of the adversary λ0α .
