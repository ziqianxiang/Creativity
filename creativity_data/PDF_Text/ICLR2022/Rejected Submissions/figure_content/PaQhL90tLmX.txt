Figure 1: A data pipeline for the DeepTLF framework. First, the original tabular data is used to traina gradient boosted decision trees (GBDT) model. The heterogeneous data (i.e., training as well as thetest data) are transformed by exploiting the structures of the decision trees in the ensemble. Morespecifically, the TreeDrivenEncoder algorithm distills information from trained decision trees of theGBDT model to produce homogeneous binary vectors. These vectors are then used to train a DNN.
Figure 2: A toy example of the proposed data transformation performed by the TreeDrivenEncoderalgorithm. On the left, we see two heterogeneous input feature vectors x1 and x2 from the originaldataset D, where xi ∈ R4, f1, f2, f4 are numerical features, and f3 is a categorical feature in a tabulardataset. Note there is also a missing value - for the feature f3 in x2. To encode the input data, weuse two trained decision trees on the dataset D, with 5 inner nodes in total. By evaluating the Booleanfunction in each inner node for a given input vector, we construct two homogeneous feature vectorsxb1 and xb2 , where a component of these vectors is set to 1 if the corresponding Boolean functionevaluates to true and 0 otherwise.
Figure 3: t-SNE visualizationsof original heterogeneous tab-ular default of clients dataset(top), and the same datasetafter the TreeDriVenEncodertransformation (bottom).
Figure 4: Left: Noisy labels experiment. Right: Noisy data experiment. The DNN here is identicalto the DL part in the DeePTLF. Note the test data is not corrupted. We report the ROC AUC value(higher is better). Results are averages over five trials for the telecom churn (D3) dataset.
Figure 5: A relationship between a num-ber of trees in the GBDT and the final per-formance of the proposed DeepTLF frame-work. The precise same GBDT model isused for the data encoding in the DeepTLF.
Figure 6: A multimodal data experiment onthe textual and tabular from the D7 dataset,the DNN and DeepTLF have identical DLarchitectures and training setups. Resultsare averaged over five trials.
Figure 7: The comparison of the DeepTLF (a green line) and the deep neural model (DNN) (a red line)models on validation (unseen) data. DeepTLF and DNN models have the exact same architecture.
Figure 8: A relationship between number of decision trees and the DeepTLF performance. Theaccuracy score (higher is better), ROC AUC score (higher is better), cross-entopy loss (lower is better)metrics for the same experiment. The exact same GBDT model is used for the data encoding in theDeepTLF. The results are averages over five trials for the telecom churn (D3) dataset.
Figure 9: The missing data experiment. The accuracy score (higher is better), ROC AUC score(higher is better), cross-entopy loss (lower is better) metrics for the same experiment. The exact sameGBDT model is used for the data encoding in the DeepTLF. The DNN model is identical in trainingand architecture to the DeepTLF’s DNN part. The results are averages over five trials for the telecomchurn (D3) dataset.
Figure 10: Learning rate. The results are averages over five trials for the telecom churn (D3) dataset.
Figure 11: Correlation plots for different quality measurements. The exact same GBDT model is usedfor the data encoding in the DeepTLF. The results demonstrate that there is indeed a high positiverelationship between the performance of GBDT and DeepTLF. Thus, the proposed data distillationalgorithm can successfully distill the knowledge from trees. The results are averaged over five trialsfor the telecom churn (D3) dataset.
Figure 12: A "sanity check" experiment. A comparison of the TreeDrivenEncoder and randomencoding functions. The random encoding mimics the TreeDrivenEncoder, but it selects a randomfeature and splitting value. The experiment verifies that the TreeDrivenEncoder is able to distill theknowledge using trained decision trees in a GBDT algorithm. The results are averaged over fivetrials.
