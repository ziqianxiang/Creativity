Figure 1: Illustrations of (a) Traditional ”soft” self-attention and (b) ”Hard” self-attention forthe word ”he”. In hard self-attention, ”he” concentrates all of its focus on ”John”, while in softself-attention, ”he” places a small amount of attention on the other irrelevant words also. Therefore,hard self-attention helps build a better representation for ”he”.
Figure 2: The winning tickets of con-ventional fine-tuned and SpecializedBERT-Base on MRPC.
Figure 3: Elements pruned for (a) downstream tasks and (b) Transformer architectures acrossGLUE and SQUAD in our most accurate models for each task. Here, ATTN-Approx means onlycertain attention heads inside the attention block are pruned, and FFN-Approx means only certainneurons inside the feed-forward block are pruned.
Figure 4: Change in class boundaries during Specialization on MRPC with BERT-Base. Weuse T-distributed Stochastic Neighbor Embedding (TSNE) on word embeddings right before thefinal classifier, using the 3 main components of each embedding. Here, 0 and 1 refer to sentencepairs that are semantically non-equivalent and equivalent, respectively. Specialized models showbetter class separability, and hence, are more accurate.
Figure 5: Attention patterns of Specialized models on WNLI using BERT-Base, shown at thefinal ATTN block in the model (layer 12 in the fine-tuned model, layer 9 in the Specializedmodel). WNLI has the smallest training set among all of our studied tasks, and hence, exhibitshighest sensitivity to random seeds.
