Figure 1: Top. A GP is trained on the blue points. GP uncertainty (model standarddeviation) in shaded blue. Left. Using GP variance as a proxy for EU here misses outon important regions of the input space from which useful information can be acquired.
Figure 2: Illustrating noise and bias. Ob-served y is independent noise plus trueE[Y|x] = f*(x), itself best approxi-mated by unknown f(x) in parametricset S (orange), e.g., using Bayesian pos-terior distribution (grey) over parame-ters. f is the closest function in S to f *,leading to a bias b(χ) = f * (x) - f(χ).
Figure 3: Drug Combinations. Predicted mean and uncertainty on test set. 50 test examples areordered by increasing value of true synergy score (orange). Model predictions and uncertaintiesin blue. Ensemble (a) (and MC-dropout, not shown) consistently underestimate uncertainty whileDEUP (b) captures the right order of magnitude. (c) Corr. w. res. shows correlation between modelresiduals and predicted uncertainties σ. A best-case Upper Bound on Corr. w. res. is obtained fromthe correlation between σ and true samples from N(0, σ). Ratio is the ratio between col. 1 and 2(larger is better). Log-likelihood: average over 3 seeds of per sample predictive log-likelihood.
Figure 4: Left.Maximum value reached by the different optimization methods, for the 10 dimensionalAckley function. In each run, all the methods start with the same initial 20 points. The shaded areasrepresent the standard error across 3 runs. Right. maximum value reached in the budget-constrainedsetting, on the Ackley functions of different dimensions. The error bars represent the standard erroracross 3 different runs, with different initial sets of 20 pairs. The budget is 120 function calls in total.
Figure 5: Average regret on CartPole task. Errorbars for standard error across 5 runs.
Figure 6: Spearman Rank Correlation Coefficient between the predicted uncertainty and true errorfor models trained with CIFAR-10, and evaluated on CIFAR-10-C. DEUP outperforms the baselineson all types of corruptions.
Figure 7: Top. A GP is trained to regress a function using noisy samples. GP uncertainty (modelstandard deviation) is shaded in blue. Bottom left. Using GP variance as a proxy for epistemicuncertainty misses out on more regions of the input space, when compared to Figure 1 . Bottomright. Using additional out-of-sample data in low density regions, a second GP is trained to predictthe generalization error of the first GP (total uncertainty). Using second samples from the oraclefor each of the training points, a linear regressor fits the training pairs (x, 2 (yι - y2)2) to estimatethe pointwise aleatoric uncertainty (constant in this case). The aleatoric uncertainty is subtractedfrom DEUP’s (second GP) predictions to obtain more accurate of epistemic uncertainty. Notethat no constraint is imposed on DEUP’s outputs, which explains the predicted negative values foruncertainties. In practice, if these predicted uncertainties were to be used, (soft) clipping should beused.
Figure 8: Left. Synthetic function to optimize. Right. Maximum value reached by the differentmethods on the synthetic function. The shaded areas represent the standard error across 5 differentruns, with different initial sets of 6 pairs. For clarity, the shaded areas are omitted for the two worstperforming methods. In each run, all the methods start with the same initial set of 6 points. GP-EItends to get stuck in local optima and requires more than 50 steps, on average, to reach the globalmaximum.
Figure 9: Sequential Model Optimization on the Levi N.13 functionpc,queθjc,n-2> EnE-XeW(b) Comparisons with GP-EI and Random acquisition4Plot of the function copied from https://www.sfu.ca/ ssurjano/levy13.html23Under review as a conference paper at ICLR 2022H.3 Additional details for the Ackley function experiment, for synthetic dataIN HIGHER DIMENSIONSThe Ackley function of dimension d is defined as:Ackleyd : B → Rx 7→ A exp1d+ exp I — COS cos(cxi)	— A — exp(1)d i=1where B is a hyperrectangle of Rd. (0, . . . , 0) is the only global optimizer of Ackleyd, at which thefunction is equal to 0. We use BoTorch’s default values for A, B, c, which are 20, 0.2, 2π respectively.
Figure 10: Predicted mean and uncertainty for different models on a separate test set. 50 examplesfrom the test set are ordered by increasing value of true synergy score (orange). Model predictionsand uncertainties are visualized in blue. MC-Dropout, Ensemble and DUE consistently underestimatethe uncertainty while DEUP seems to capture the right order of magnitude. Figures made using TheUncertainty Toolbox (Chung et al., 2020).
