Figure 1: An illustration of typical symmetries in a pendulum, and the corresponding transformations of thestate and action for a group equivariant transition model: (a) shows how reflection of the agent’s state results ina permutation of the action, denoted by a-1 . (b) shows how rotation of the agent’s state results in invariance ofthe action in the absence of gravity. The state transitions can be modeled as group actions (2D rotations in thisexample), which can be captured by our symmetry transformation-based transition model. Note that rotationalsymmetry can hold even when gravity is present. In this case, symmetry transformations include rotations (andreflections) that preserve the Hamiltonian. Such non-linear energy-preserving transformations of state-actionsin the pixel space can become linear in the embedding space.
Figure 2: This figure demonstrates the relationship betweentwo types of equivariance in latent variable modeling for anMDP with symmetric transition function. Green arrows (ver-tical plane) identify a diagram for transition models in anMDP homomorphism. A model T and state embedding func-tion hs that are equivariant under agent,s action makes thisdiagram commute. Red arrows (horizontal plane) identify thecommutativity diagram for a symmetric transition function ofan MDP in the latent space. Here the state-action embedding{s,α} is produced through the symmetry transformation ofanother state-action embedding {s,a}.
Figure 3: Performance profiles for different methods basedon score distributions (left), and average score distributions(right). Shaded regions show pointwise 95% confidencebands. The higher the curve, the better the method is.
Figure 4: Plots of Interquartile Mean (IQM) and Optimality Gap computed from human-normalized scores,showing the point estimates along with 95% confidence intervals (over 10 runs for all methods, 5 runs forSimPLe). A higher IQM and a lower optimality gap reflects better performance. (a) shows different methodsfor all 26 games. (b) shows different methods for 17 games. (c) shows the proposed model with different groupchoices for all 26 games. (d) shows the proposed model with different loss terms for all 26 games.
Figure 5: A schematic of the EqR model, applied to model-free RL. Green in the framework cor-responds to learning equivariance under the agent’s action and red corresponds to learning equiv-ariance of the transition model with respect to symmetry transformation of the state-action. Thiscolor scheme is consistent with Figure 2. The part of the framework that corresponds to rewardmatching and Q-learning is shown in blue and brown respectively. The arrows in the schematic aredifferentiated by their heads and are described in the legend.
