Figure 1: Graphical representations of noise models, including the class-conditional noise (CCN)model, the instance-dependent noise (IDN) model, and the proposed instance-confidence embedding(ICE) approximation of IDN. Here, X is the input feature, Y is the true label, Y is the noisy label,and C ∈ [0, 1] is a scalar confidence parameter.
Figure 2: An illustration of the transformation (N 7→ H) from the clean class-posterior pφ(Y |x) (theleftmost) to the noisy class-posterior pφ(Y |x) (the rightmost). The outer black triangle depicts theprobability simplex ∆2 projected to the 2-dimensional space for 3-class classification. We can seethat when the label is almost deterministic (N is close to a vertex), the estimation of K - 1 columns ofthe transition matrix T (x) (the two deviated vertices of the dotted triangle) has only limited influenceon the estimated noisy class-posterior pb(Y |x) (M is still close to H). This inspires us to go a stepfurther and use single-parameter approximations (N 7→ O) qθ,φ(Y |x) (Eqs. (8) and (9)).
Figure 3: An example of the learned class-posteriors using (a) the usual cross entropy withoutany modification; (b) weight decay; (c) modified confidence of the prediction (Eq. (8)) with neuralnetwork approximation for g : X → [0, 1]; and (d) instance-confidence embedding. The pointswith black edges are mislabeled instances. We can observe that noisy labels affect the decisionboundary and the model complexity. Comparing the last two panels, modifying the confidence of theprediction works better with instance embedding than neural network approximation. ICE can reducethe influence of ambiguous or mislabeled instances to improve the class-posterior estimation.
Figure 4: Ridgeline plots of the confidence C during training. The red/blue curves represent theconfidence of instances with flipped/original labels, respectively.
Figure 5: The 32 most low-confidence training examples in the MNIST and CIFAR-10 datasets,ordered left-right, top-down by increasing confidence. The index in the dataset, original label, andpredicted label are annotated above each image for verification.
Figure 6: The class-wise spectra from high-confidence (left) to low-confidence (right) trainingexamples in the MNIST (digit 3 and 4) and CIFAR-10 (cat and dog) datasets.
Figure 7: Contours of the log-likelihood w.r.t. pi and C using the linear interpolation (Eq. (8)) for Kin {2, 3, 5, 7, 10}.
Figure 8: Contours of the log-likelihood on the simplex when K = 3 using the linear interpolation(Eq. (8), top) and the power transformation (Eq. (9), bottom) for C in {0.1, 0.3, 0.5, 0.7, 0.9}.
Figure 9: Illustration of related methods, including the categorical cross-entropy (CCE), label smooth-ing (LS), soft/hard bootstrapping loss (SB/HB), and the proposed instance-confidence embedding(ICE) with the linear transformation (Eq. (8)).
Figure 10: Ridgeline plots of the confidence C during training. The density is estimated via Gaussiankernel density estimation (KDE). The red/blue curves represent the confidence of instances withflipped/original labels, respectively.
Figure 11: The 32 most low-confidence training examples in the MNIST, FMNIST, KMNIST, SVHN, CIFAR-10 and CIFAR-100datasets, ordered left-right, top-down by increasing confidence.
Figure 12: The class-wise spectra from high-confidence (left) to low-confidence (right) training examples in the MNIST, FMNIST,KMNIST, SVHN, CIFAR-10 and CIFAR-100 datasets.
Figure 13: The 32 most low-confidence training examples in the Clothing1M dataset, ordered left-right, top-down by increasingconfidence.
