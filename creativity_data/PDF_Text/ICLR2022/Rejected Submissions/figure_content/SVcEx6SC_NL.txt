Figure 1: A limitation of standard feature embeddings: it is straightforward to construct pairs ofimages (χ1,χ2) that appear completely different yet have near-identical representations.
Figure 2: Sample images highlighting the properties and applications of “robust representations”studied in this work. All of these manipulations use only gradient descent on simple, unregularized,direct functions of the representations of adversarially robust neural networks (Goodfellow et al.,2015; Madry et al., 2018).
Figure 3: Representation inversions for robust and standard models trained on the Restricted Ima-geNet dataset. We minimize (3) to find an Xinv which matches (in '2-distance) the representationof the target image (x) starting from each corresponding source image x° (top row) for an adversar-ially trained (second row) and standard (third row) model respectively. When inverting the robustrepresentation, regardless of the optimization starting point xo, the resulting inversions are percep-tually similar to the target image; in contrast, the results of inverting the standard model appear moresimilar to the (arbitrary) source image used as the seed for the optimization. Additional results inAppendix B.1, and similar results for ImageNet are in Appendix B.1.4.
Figure 4: Target: random images from the test set (col. 1-5) and from outside of the trainingdistribution (6-10); Result: images obtained from optimizing inputs (using Gaussian noise as thesource image) to minimize '2-distance to the representations of the corresponding image in the toprow. More examples appear in Appendix B.1.
Figure 5: Optimizing objective (3) with an '2-norm constraint around the source image. On thex-axis is the radius of the constraint set, and on the y-axis is the distance in representation spacebetween the minimizer of objective (3) within the constraint set and the target image, normalized bythe norm of the representation of the target image: i.e., a point (xi, yi) on the graph corresponds toyi = minkδk2≤xi kR(x + δ) - R(xtarg)k2/kR(xtarg)k2. Notably, we are unable to closely matchthe representation of the target image for the robust network until the norm constraint grows verylarge, and in particular much larger than the norm of the perturbation that the model is trained tobe robust against (ε in objective (2)). Shown are 95% confidence intervals over random choice ofsource and target images.
Figure 6: Correspondence between image-level patterns and activations learned by standard androbust models on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), We use PGD to find inputs that (locally) maximally activate a given componentof the representation vector (cf. Appendix A.6.1 for details). In the left column we have the seedinputs x0 (selected randomly), and in subsequent columns we visualize the result of the optimiza-tion (4), i.e., x0, for different activations, with each row starting from the same (far left) input x0 for(top): a robust (adversarially trained) and (bottom): a standard model. Additional visualizations inAppendix B.3, and similar results for ImageNet in B.3.2.
Figure 7: Figure reproduced from (Olah et al.,2017)—a (strongly regularized) visualization ofa few components of the representation layer ofGoogLeNet. While regularization (as well asFourier parameterization and colorspace decor-relation) yields visually appealing results, thevisualization does not reveal consistent seman-tic concepts. The situation is significantly worsewithout regularization (cf. Figure 6 bottom).
Figure 8: A visualization of the first four com-ponents of the representation layer of VGG16when regularization via random jittering and ro-tation is applied. Figure produced using the Lu-cid a visualization library.
Figure 9: Maximizing inputs x0 (found by solving (4) with x° being a gray image) and most or leastactivating images (from the test set) for two random activations of a robust model trained on theRestricted ImageNet dataset. For each activation, we plot the three images from the validation setthat had the highest or lowest activation value sorted by the magnitude of the selected activation.
Figure 11: Image interpolation using robust representations compared to their image-space coun-terparts. The former appear perceptually plausible while the latter exhibit ghosting artifacts. Forpairs of images from the Restricted ImageNet test set, we solve (5) for λ varying between zeroand one, i.e., we match linear interpolates in representation space. Additional interpolations appearin Appendix B.2.1 Figure 16. We demonstrate the ineffectiveness of interpolation with standardrepresentations in Appendix B.2.2 Figure 17.
Figure 12: Robust representations yield semantically meaningful inverses: Original: randomly cho-sen test set images from the Restricted ImageNet dataset; Inverse: images obtained by inverting therepresentation of the corresponding image in the top row by solving the optimization problem (??)starting from: (a) different test images and (b) Gaussian noise.
Figure 13: Robust representations yield semantically meaningful inverses: (Original): randomlychosen out-of-distribution inputs; (Inverse): images obtained by inverting the representation of thecorresponding image in the top row by solving the optimization problem (??) starting from Gaussiannoise.
Figure 14: Standard representations do not yield semantically meaningful inverses: (Original): ran-domly chosen test set images from the Restricted ImageNet dataset; (Inverse): images obtained byinverting the representation of the corresponding image in the top row by solving the optimizationproblem (??) starting from Gaussian noise.
Figure 15: Visualization of inputs that are mapped to similar representations by models trained onthe ImageNet dataset. Target (χ2) & Source (xi): random examples image from the test set; Robustand Standard (xi): result of minimizing the objective (3) to match (in '2-distance) the representationof the target image starting from the corresponding source image for (top): a robust (adversariallytrained) and (bottom): a standard model respectively. For the robust model, We observe that theresulting images are perceptually similar to the target image in terms of high-level features, whilefor the standard model they often look more similar to the source image which is the seed for theoptimization process.
Figure 16: Additional image interpolation using robust representations. To find the interpolationin input space, we construct images that map to linear interpolations of the endpoints in robustrepresentation space. Concretely, for randomly selected pairs from the Restricted ImageNet test set,we use (??) to find images that match to the linear interpolates in representation space (5).
Figure 17: Image interpolation using standard representations. To find the interpolation in inputspace, we construct images that map to linear interpolations of the endpoints in standard represen-tation space. Concretely, for randomly selected pairs from the Restricted ImageNet test set, weuse (??) to find images that match to the linear interpolates in representation space (5). Image spaceinterpolations from the standard model appear to be significantly less meaningful than their robustcounterparts. They are visibly similar to linear interpolation directly in the input space, which is infact used to seed the optimization process.
Figure 18: Correspondence between image-level features and representations learned by a ro-bust model on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use a constrained optimization process to identify input features that maximallyactivate a given component of the representation vector (cf. Appendix A.6.1 for details). Specif-ically, (left column): inputs to the optimization process, and (subsequent columns): features thatactivate randomly chosen representation components, along with the predicted class of the feature.
Figure 19: Correspondence between image-level features and representations learned by a ro-bust model on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use a constrained optimization process to identify input features that maximallyactivate a given component of the representation vector (cf. Appendix A.6.1 for details). Specif-ically, (left column): inputs to the optimization process, and (subsequent columns): features thatactivate select representation components, along with the predicted class of the feature.
Figure 20: Correspondence between image-level patterns and activations learned by standard androbust models on the Restricted ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use PGD to find inputs that (locally) maximally activate a given component ofthe representation vector (cf. Appendix A.6.1 for details). In the left column we have the original in-puts (selected randomly), and in subsequent columns we visualize the result of the optimization (4)for different activations, with each row starting from the same (far left) input for (top): a robust(adversarially trained) ResNet-50 model, (middle): a standard ResNet-50 model and (bottom): astandard VGG16 model.
Figure 21: Correspondence between image-level patterns and activations learned by standard androbust models on the complete ImageNet dataset. Starting from randomly chosen seed inputs(noise/images), we use PGD to find inputs that (locally) maximally activate a given component ofthe representation vector (cf. Appendix A.6.1 for details). In the left column we have the original in-puts (selected randomly), and in subsequent columns we visualize the result of the optimization (4)for different activations, with each row starting from the same (far left) input for (top): a robust(adversarially trained) ResNet-50 model, (middle): a standard ResNet-50 model and (bottom): astandard VGG16 model.
Figure 22: Visualization of the results adding various neurons, labelled on the left, to randomlychosen test images. The rows alternate between the original test images, and those same imageswith an additional feature arising from maximizing the corresponding neuron.
