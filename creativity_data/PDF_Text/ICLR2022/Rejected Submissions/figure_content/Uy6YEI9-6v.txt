Figure 1: (a) NeRF. (b) Our method.
Figure 2: We propose an object-centric neural scene representation for image synthesis. Given ascene description (a), and a repository of neural object-centric scattering functions (OSF) trainedindependently from images and frozen for each object (b), we can compose the objects into scenes(c), and render photorealistic images as we move lights (d), cameras (e), and/or objects (f). Ourframework is capable of rendering occlusions, specularities, shadows, and indirect illumination.
Figure 3: Using our method (OSFs) to render: (a) single and (b) multiple objects.
Figure 4: Sampling procedure. (a) Scene with a camera, light source, and object bounding boxes.
Figure 5: Novel lighting results.
Figure 6: Scene composition results on Furniture-Random. The models OSF, o-NeRF, ando-NeRF + S are explained in §5. Compared to o-NeRF, our model (OSF) is able to disentanglelighting-dependent and view-dependent appearance and can render shadows.
Figure 7: Comparison of OSF (ours) to Neu-ral Reflectance Fields (NRF)(Bi et al., 2020a).
Figure 8: Real-world results. NRF (Bi et al., 2020a) and NeRF (Mildenhall et al., 2020) learnon individual static scenes or objects. In contrast, we compose real-world objects and scenes usingOSFs. The objects are composed with a (a) synthetic floor and (b, c) real outdoor scenes from Real-Outdoor. Columns show different ablated versions of our model: “No Shadows, No Indirect”which considers only direct illumination; “No Indirect” which includes both direct illumination andshadows; “Indirect Only” which considers only indirect illumination. Our OSFs show the mostrealistic renderings, with accurate shadows (e.g., pony shadowing the two other objects (row a) andindirect illumination (i.e., the ground and environment illuminating the objects).
Figure 9: Composing real-world objects from Real-NRF using our OSF method. We demonstratethe effect of moving the light, camera, or objects. Note how the appearance and shadows of the ob-jects are updated across different scene configurations. Also notice that even when parts of the palmtree object and the cartoon object are cast under the pony’s shadow, they do not appear completelydark due to the indirect illumination from the floor.
Figure 10: Visualizing the effect of different numbers of indirect (secondary) rays (N ) per primarysample for our OSF model (the brightness of these images has been increased only for visualizationpurposes). Note that the noisiness of the render decreases as N increases. We find that we are ableto achieve relatively non-noisy results with approximately five samples.
Figure 11: Learned OSFs on objects from Real-NRF. The objects were captured in a dark roomwith a one-light-at-a-time setup. After training OSF on each object in this dataset, we are able torender the objects from novel viewpoints and lighting directions.
Figure 12: Ablation results on our OSF model.
Figure 13: Comparisons on scene composition on Furniture-Realistic. The models OSF, o-NeRF, and o-NeRF + S are explained in §5. Compared to o-NeRF, our model (OSF) is able todisentangle lighting-dependent appearance from view-dependent appearance for individual objects,and is able to render shadows cast by objects onto the ground correctly.
Figure 14: Complex illumination results.
Figure 15: Flowchart of our method. See §4 for more details.
