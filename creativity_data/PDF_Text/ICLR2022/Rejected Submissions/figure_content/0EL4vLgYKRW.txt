Figure 1: Analysis of MA-TD3+BC, MA-CQL, and OMAR in the medium-replay dataset fromSpread. (a) Spread. (b) Performance. (c) Performance improvement percentage of MA-CQL overthe behavior policy with varying number of agents. (d) Visualization of the Q-function landscape.
Figure 2: Performanceimprovement percentageof OMAR over MA-CQL with varying num-ber of agents.
Figure 3:	Ablation study on the effect of the regularization coefficient in different types of datasets.
Figure 4:	Ablation study on the effect of key hyperparameters in the sampling mechanism averagedover different types of datasets.
Figure 5: Multi-agent particle environments and Multi-Agent HalfCheetah.
Figure 6: Learning curves of MA-ICQ, MA-TD3+BC, MA-CQL, and OMAR in multi-agent particleenvironments (CN, PP, and W is abbreviated for cooperative navigation, predator-prey, and worldrespectively).
Figure 7: Comparison of OMAR and MA-CQL in StarCraft II micromanagement tasks.
Figure 8: Performance improvement percentage of our method over MATD3 in the online (rightpart) setting and MA-CQL in the offline setting (left part) with a varying number of agents in theSpreak task. The first, second, thrid, and fourth quadrants correspond to the online RL, offline RL,offline multi-agent RL, and online multi-agent RL settings.
Figure 9: Normalized score of OMAR and MA-CQL trained using a fraction of the entire replaydataset in cooperative navigation.
Figure 10: Performance improvement percentage of MA-CQL over the behavior policy with a vary-ing number of agents in a non-cooperative version of the Spread task.
