Figure 1: Illustration of statistical manifold obtainedfrom the 1-1 mapping between the set of point clouddata and the space of probability density functions.
Figure 2: Probability heat maps for various k (the greener, the higher) for some examples from theShapeNet dataset (Chang et al., 2015), where we set σ = k × MED for k ∈ (0, ∞). MED denotesthe median of the distances between the points in the point cloud and their nearest points.
Figure 3: Two moving point cloudswith different velocity matrices.
Figure 4: Random walks for point cloud data on the statistical manifold equipped with Euclideanmetric (upper) and info-Riemannian metric (lower). An initial shape is defined by slightly deform-ing the reference sphere with the addition of a small velocity matrix V . Then, sequences of pointclouds are generated by recursively adding randomly sampled velocity matrices normalized to havethe same norm with V under each metric. Details of implementations are in Appendix C.
Figure 5: Left: Latent space with linear and geodesic interpolants. The orange interpolants connecta wide cylinder to a tall cylinder, while the magenta interpolants connect a cylinder to a cone. Linearinterpolants and geodesic interpolants under the Euclidean and info-Riemannian metrics are drawnas dotted, dashed, and solid lines, respectively. Right: Generated point clouds from those inter-polants. To visually indicate which class generated point cloud belong to, we color these accordingto the ratio of the Chamfer distances to the nearest point cloud for each class (see Appendix D). Forexample, when it is uncertain which class a generated data belongs to (i.e., the nearest distances toeach class are similar), it is assigned some color other than blue, red, or green.
Figure 6: Latent spaces produced by regularized au-toencoders, each of which is trained with the Euclidean(Left) and info-Riemannian metric (Right). Represen-tative intra-class linear interpolants between two cylin-ders and two cones are drawn as black solid lines.
Figure 7: From left to right: latent spaces with equidistant ellipse ({z|(z - z*)TG(z*)(z - z*) = 1}for center z*) centered on some selected points and sampled points from interspaces, GaussianMixture Model (GMM) fitting results, generated samples from the GMM, and the heat map ofthe pairwise Euclidean distances in the latent space of all test data. The upper figure is a vanillaautoencoder trained without regularization, while the lower figure is trained with regularization(using the info-Riemannian metric). For the samples in the third column, we assign colors using thesame method of Section 5.1.1 to visually express which classes the samples are likely to belong to.
Figure 8: Representative shape to each class and the shape parameters required to define it. Thereare 5 shape classes including cylinder, cone, elliptic cone, ellipsoid, and box.
Figure 9: The representative five examples of the regularization experiments on the synthetic dataset.
Figure 10: The generated point clouds from the linear inteɪpolants of the regularized autoencoders with the Euclidean metric (UPPef) and info-Riemannian metric(Lower).
Figure 11: Graphs of classification accuracy versus reconstruction error measured on ModelNetdatasets. More transparent markers have larger coefficients λ; detailed values are in Table 9 andTable 10.
Figure 12: Learning curves of classification accuracy and reconstruction error measured on Model-Net datasets (ModelNet40 and ModelNet10) according to the noise levels (1%, 5%, 10%, and 20%).
Figure 13: Learning curves of classification accuracy and reconstruction error measured on Model-Net datasets (ModelNet40 and ModelNet10) according to the label rates (50%, 10%, 5%, and 1%).
Figure 14: Kernel density functions of different point cloud data (circle, ellipse, square, heart, andstar) using different kernels (Gaussian, uniform, triangular, Epanechnikov, and sigmoid). We set thecommon bandwidth of kernels σ = MED, where MED denotes the median of the distances betweenthe points in the point cloud and their nearest points.
