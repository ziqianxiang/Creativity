Figure 1: Top-1 accuracy on ImageNet vs. depth (in logscale) of various models. ParNet performs competitivelyto deep state-of-the-art neural networks while having muchlower depth. Performance of prior models is as reportedin the literature. Size of the circle is proportional to thenumber of parameters. Models are evaluated using a single224×224 crop, except for ViTB-16 and ViTB-32 (Dosovit-skiy et al., 2021), which fine-tunes at 384×384 and PReLU-net (He et al., 2015), which evaluates at 256×256. Mod-els are trained for 90 to 120 epochs, except for parameter-efficient models such as MNASNet (Tan et al., 2019), Mo-bileNet (Sandler et al., 2018), and EfficientNet (Tan & Le,2019), which are trained for more than 300 epochs. Forfairness, we exclude results with longer training, higher res-olution, or multi-crop testing.
Figure 2: Schematic representation of ParNet and the ParNet block. ParNet has depth 12 and iscomposed of parallel substructures. The width of each block in (a) is proportional to the number ofoutput channels in ParNet-M and the height reflects output resolution. The ParNet block consists ofthree parallel branches: 1×1 convolution, 3×3 convolution and Skip-Squeeze-and-Excitation (SSE).
Figure 3: Performance of ParNet increases as we increase the number of streams, input resolution,and width of convolution, while keeping depth fixed. The left plot shows that under a fixed parametercount, the most effective way to scale ParNet is to use 3 branches and high resolution. The rightplot shows the impact on performance by changing only one of the aforementioned factors. We donot observe saturation in performance, indicating that ParNets could be scaled further to increaseperformance while maintaining low depth.
Figure A1: Schematic representation of the Fusion (left) and Downsampling (right) blocks used inParNet.
