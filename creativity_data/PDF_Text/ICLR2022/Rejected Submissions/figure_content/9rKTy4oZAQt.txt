Figure 1: Example weight functions and their resulting coefficients in the policy gradient estimate(9). In these plots, outcomes increase in quality from left to right. As in CPT (Tversky & Kahneman,1992), weight coefficients are proportional to the derivative of the weight function.
Figure 2: Impact of different distributional objectives in one environment (CarButton2). The shadingin the first 2 plots (and subsequent learning curves) reflects the standard deviation associated withrunning over 5 random seeds. Left: Net reward (positive reward minus penalty) throughout learning.
Figure 3: Average episode reward (including penalty) over training for different learning approachesin three different environments. C3PO with Wang (η = 0.75) weighting outperforms others.
Figure 4: Average number of penalty events per episode (lower is better) over training for differentlearning approaches in three different environments. The horizontal lines reflect the cost levelsreached by both PPO and TRPO training with zero penalty in Ray et al. (2019).
Figure 5: Testing reward distributions (including penalty; sampling turned off) for long trainingof three Safety Gym environments.
Figure 6: Comparison of positive contributions to episode reward during training for our approach(yellow) and Lagrangian methods configured to have the same cost level.
Figure 7:	Impact of variance reduction measures on optimization of the CPT value function. Here“Base” refers to the risk-sensitive policy gradient estimate (10), “UTG” adds utility-to-go and a neuralnetwork baseline (11), “GAE” incorporates generalized advantage estimation, and “TR” implementstrust regions via clipping. Shading represents the variation over five random seeds.
Figure 8:	Impact of different distributional objectives in remaining two environments of initial trials.
Figure 9: Agent outcome distributions across trials run over increasingly cautious (η increasing)objectives. Distributions correspond to results shown in Table 1.
Figure 10: Policy entropy progression during training for three environments. Shading reflects theobserved variation over 5 random seeds.
Figure 11: Average episode reward (including penalty) over training for different unconstrainedlearning approaches in remaining three environments.
Figure 12: Average number of cost events per episode (lower is better) over training for differentunconstrained learning approaches in remaining three environments. As above, the “zero-penalty”line refers to the level reached by PPO and TRPO trained with no penalty in the reward (Ray et al.,2019).
Figure 13: Testing reward distributions (including penalty; sampling turned off) for long trainingruns in the remaining three Safety Gym environments. In five of the six environments, C3PO withη = 0.75 provides tangible benefit. A smaller η is likely required to improve performance onCarPush2.
Figure 14: Comparison of positive contributions to episode reward during training for our approach(yellow) and Lagrangian methods configured to have the same cost level. The plots for the other threeenvironments are shown in Figure 6.
Figure 15: Comparison of cost incurred (lower is better) during training for our approach andLagrangian methods configured to have the same cost level. As intended, cost levels are consistentlymatched between the methods. As above, the “zero-penalty” line refers to the level reached by PPOand TRPO trained with no penalty in the reward (Ray et al., 2019).
Figure 16: Comparison of cost incurred (lower is better) during training for our method and Con-strained Policy Optimization (Achiam et al., 2017) configured to have a matching cost limit. Resultsare consistent with Ray et al. (2019). As above, the “zero-penalty” line refers to the level reached byunconstrained PPO and TRPO trained with no penalty in the reward (Ray et al., 2019).
Figure 17: Comparison of average episode reward (including penalty) for C3PO and CPO.
