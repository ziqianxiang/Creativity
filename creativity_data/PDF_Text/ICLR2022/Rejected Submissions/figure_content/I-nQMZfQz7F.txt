Figure 1: The interaction feature prediction schemefunction is pixel-aligned: The function takes images from multiple cameras as input (e.g. stereo)and, assuming known camera poses and intrinsics, the latent representation at a certain spatial loca-tion is directly related to pixels of the images. Once learned, the interaction features can be used by atypical constrained optimal control framework to plan dexterous object-robot interaction. We adoptLogic-Geometric Programming (LGP) (Toussaint et al., 2018) as an optimization-based manipu-lation planning framework and show that this learned-feature based planning enables to computetrajectories that involve various types of interaction modes only from images. Due to the represen-tations’ generalization, the learned features are directly applicable to manipulation tasks involvingunseen objects. To summarize, our main contributions are•	To represent objects as neural implicit functions upon which interaction features are trained,•	An image-based manipulation planning framework with the learned features as constraints,•	Comparison to non pixel-aligned, non implicit function, and geometric representations,•	Demonstration in various manipulation scenarios ranging from simple pick-and-hang [videos] to longer-horizon manipulations [videos] and zero-shot imitations [videos].
Figure 2: Image encoder and 3D reprojector of the backbone network.
Figure 3: The proposed interaction feature prediction scheme for manipulation planningIn order to compute a full trajectory of the robot and the objects that it interacts with, the learnedfeatures can be integrated into any constraint-based trajectory optimization and manipulation plan-ning framework, using the features as differentiable interaction constraints. In this work, we adoptLogic-Geometric Programming (LGP) (Toussaint et al., 2018) as an optimization-based manipu-lation planning framework. In typical manipulation scenes, cameras are equipped such that theirviews cover a wide range of the environment. As shown in Fig. 3, we warp raw images of the entirescene to get object-centric images and corresponding camera extrinsics/intrinsics and compute therobot frame’s pose via forward kinematics to feed them into the network. Section 5.1 presents howthe learned constraint functions are integrated into sequential manipulation planning problems andSection 5.2 discusses the proposed warping procedure.
Figure 4: SDFs predicted by PIFO and the globalimage feature model.
Figure 5: Sequential manipulation scenarios:Single-, three-mug hanging and handover.
Figure 6: Position and Orientation ErrorsFigure 7: (a) Reference (b-c) Imitationeach interaction pose separately as in Sec. 6.1 and checked if these individually optimized poses arekinematically feasible, i.e. whether or not the inverse kinematics problems have a solution. Eventhough the mug’s initial pose was given such that the first gasping is feasible, 53 out of 100 pairsof grasp and hang poses were infeasible for the pick & hang scenario and 86 out of 100 sets for thehandover scenario, i.e., many of the sampled poses led to a collision or an infeasible robot configu-ration for hanging/handover. Some failure cases are depicted in Figs. 18-19. This will become evenworse when the whole trajectory is optimized or the mug’s initial pose is given arbitrarily. As thesequence length gets longer, not only should an exponentially larger number of planning problemsbe solved to find a set of feasible poses, but also the found poses are not guaranteed to be optimal.
Figure 7: (a) Reference (b-c) Imitationeach interaction pose separately as in Sec. 6.1 and checked if these individually optimized poses arekinematically feasible, i.e. whether or not the inverse kinematics problems have a solution. Eventhough the mug’s initial pose was given such that the first gasping is feasible, 53 out of 100 pairsof grasp and hang poses were infeasible for the pick & hang scenario and 86 out of 100 sets for thehandover scenario, i.e., many of the sampled poses led to a collision or an infeasible robot configu-ration for hanging/handover. Some failure cases are depicted in Figs. 18-19. This will become evenworse when the whole trajectory is optimized or the mug’s initial pose is given arbitrarily. As thesequence length gets longer, not only should an exponentially larger number of planning problemsbe solved to find a set of feasible poses, but also the found poses are not guaranteed to be optimal.
Figure 8: Data Generation(e) Image16Under review as a conference paper at ICLR 2022(a) Before augmentation(b) After augmentationFigure 9: Image Data AugmentationFigure 10: Reconstruction via marching cube. Red: ground truth, Blue: reconstructed(b) Test Mugs17Under review as a conference paper at ICLR 2022Figure 11: Key interaction points on the gripper and hook. Before passed to PIFO, their globalpositions are computed from the gripper's or hook's PoSe q, i.e., Pi = R(q)pi + t(q), ∀i ∈{1, ..., Nkeypoint}.
Figure 9: Image Data AugmentationFigure 10: Reconstruction via marching cube. Red: ground truth, Blue: reconstructed(b) Test Mugs17Under review as a conference paper at ICLR 2022Figure 11: Key interaction points on the gripper and hook. Before passed to PIFO, their globalpositions are computed from the gripper's or hook's PoSe q, i.e., Pi = R(q)pi + t(q), ∀i ∈{1, ..., Nkeypoint}.
Figure 10: Reconstruction via marching cube. Red: ground truth, Blue: reconstructed(b) Test Mugs17Under review as a conference paper at ICLR 2022Figure 11: Key interaction points on the gripper and hook. Before passed to PIFO, their globalpositions are computed from the gripper's or hook's PoSe q, i.e., Pi = R(q)pi + t(q), ∀i ∈{1, ..., Nkeypoint}.
Figure 11: Key interaction points on the gripper and hook. Before passed to PIFO, their globalpositions are computed from the gripper's or hook's PoSe q, i.e., Pi = R(q)pi + t(q), ∀i ∈{1, ..., Nkeypoint}.
Figure 13: Baseline Networks used for comparison.
Figure 14:	First 5 principal components from PCA on image features. The first component indicatesthe object vs. non-object areas, the second component distinguishes the handle parts, and the thirdone spots the above vs. below of the mugs, etc. Note that the network is trained only via the taskfeature supervisions.
Figure 15:	The first principal component from PCA on representation vectors of the 3D surfacepoints. It distinguishes the handles of the mugs from the other parts and is consistent across differentmugs.
Figure 16: The three-mug scenario. 60 steps of robot configurations and rigid transformations ofthree mugs are jointly optimized via the proposed manipulation framework. This optimization is a1071-dimensional decision problem (one 7DOF arm for 60 steps and one 7DOF mug for 51, 31, 11steps = 1071, the mug’s rigid transformations before grasped are not included in optimization) andis solved within 1 minute on a standard laptop.
Figure 17: The handover scenario. 30 steps of the two arms’ configurations and rigid transformationsof the mug are jointly optimize dvia the proposed manipulation framework. This optimization is a567-dimensional decision problem (two 7DOF arms for 30 steps and one 7DOF mug for 21 steps =567, the mug’s rigid transformations at the first phase are not included in optimization) and is solvedwithin 1 minute on a standard laptop.
Figure 18: IK with generative models - Pick & Hang. Separately generated poses often can notbe coordinated due to the kinematic infeasibility, i.e., the robot joint angle limits, or the collisionconstraints.
Figure 19: IK with generative models - Handover. Separately generated poses often can not becoordinated due to the kinematic infeasibility, i.e., the robot joint angle limits, or the collision con-straints.
Figure 20: 6D Pose Estimation. (b) Point clouds for ICP are obtained from depth cameras at thesame locations/orientations as the RGB cameras. The size of the point clouds is 1000. (c) Pointclouds for ICP are sampled from the surfaces of the meshes reconstructed via the learned φSDF . Thesize of the point clouds is 1000. (d) FCP uses 103 grid points for the target and 53 grid points (insmaller area) for the model, respectively.
Figure 21: 6D Pose Estimation Results - the estimated poses are applied to the green meshes. ICPeasily gets stuck at local optima while FCP produces fairly accurate poses which help F+ICP2 escapethe local optima; note that FCP does not iterate to get the results.
Figure 22: Zero-shot Imitation - reference motion. Two sets of posed images are obtained at t =10, 15.
Figure 23: Zero-shot imitation - optimized motions. The FCP constraints are imposed at t = 10, 15.
