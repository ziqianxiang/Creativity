Figure 1: Demonstrating the power of self-training for shifting the model on the Two-moon dataset.
Figure 2: Accuracy and Confidence of GIFT and iterative self-training, as a function of the numberof training iterations on different bins of the translated CIFAR10. We evaluate the accuracy forthe test data with translations between 0 and 100% (where zero means no shift, and 100% meansthe maximum possible amount of shift, i.e., IMAGE WIDTH/2). The iterative self-training modelhas 5 teacher updates over 1000 training iterations (i.e. updates happen after 0, 200, 400, 600, 800iterations). GIFT has 20 teacher updates over 1000 iterations (i.e. updates happen after 0, 50, 100, ...,950 iterations). The left two panels in each row correspond to models trained for the target datasetTranslated (0%-100%) CIFAR10. The right two panels in each row correspond to models trained forthe target dataset Translated (50%-100%) CIFAR10.
Figure 3: Accuracy of iterative self-training and GIFT on perturbations of CIFAR10 as a function ofthe number of teacher updates when the total number of training steps is 1000 and 500. Accuracies ofboth models improve by increasing the number of self-training iterations up to a threshold. Beyondthis threshold, while iterative self-training performance deteriorates, GIFT saturates and hence showsmore robustness.
Figure 4:	Effect of the number of teacher updates on the accuracy when the number of trainingsteps before each teacher update is fixed and set to 100, for different perturbations of CIFAR10. ForTranslated (0%-100%) CIFAR10 and Translated (50%-100%) CIFAR10, we see an increasing trendin accuracy as we increase the number of teacher updates for both iterative self-training and GIFT.
Figure 5:	Effect of the number of self-training iterations on accuracy when all interpolations arerepresented to the model at the same time with the total number of training steps of 1000 for differentperturbations of CIFAR10. Similar to iterative self-training, the performance improves by increasingthe number of self-training iterations up to a threshold. Beyond the threshold, the performancedeteriorates.
