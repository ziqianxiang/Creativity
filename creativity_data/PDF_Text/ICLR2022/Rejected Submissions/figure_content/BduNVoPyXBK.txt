Figure 1: We study three diverse compositional task structures in both 2D and 3D environments:(a) composing memory of object-motions, (b) composing 3D objects with a larger environment, and(c) composing memory of goal information to longer tasks made of recurring object configurations.
Figure 2: FARM learns representations for perceptual schemas that are distributed across n recurrentmodules {η(i)}. To discover perceptual schemas, FARM exploits structured observation featuresZt = φ(xt, Zt-1) ∈ Rm×dz that share dz spatio-temporal features across m spatial positions. Tocapture diverse structures, each module uses feature attention to compute coefficients for importantfeatures: fa(ti)t . This allows a module to update with important features present across all spatialpositions. The modules share information to coordinate what they represent with fs(hi)are .
Figure 3: FARM enables generalizing recall to novel spatio-temporal compositions of objectmotions. We present the success rate means and standard errors computed using 5 seeds. (a) OnlyFARM is able to go above chance performance for each setting. (b) Given recurrent features, we findthat learning perceptual schemas with either multiple modules or feature attention enables learningrecall of spatio-temporal compositions. These results suggest that spatial attention removes thebenefits of learning multiple modules when learning to recall object-dynamics.
Figure 4: FARM enables generalizing sequential active perception of 3D objects to a largerenvironment. We present the success rate means and standard errors computed using 3 seeds. (a)FARM generalizes best. (b) Our performance benefits come mainly from learning multiple modules,though feature attention slightly improves performance and lowers variance. These results suggestthat spatial attention interferes with reinforcement learning of 3D objects.
Figure 5: FARM enable generalizing memory-retention of goal-information to longer horizonsof obstacles than trained on. On the left-most panel of (a,b), we present the maximum traininglevel reached by each agent. All others panels in (a-c) show the generalization success rate. For allquantities, we present the mean and standard error computed using 10 seeds. (a) All architecturesachieve comparable training performance. FARM better generalizes to longer hallways with more dis-tractors in the densely populated setting. (b) FARM and AAA get similar generalization performancein the sparsely populated setting. (c) Using multiple modules and feature attention both improvegeneralization. These results suggest that spatial attention interferes with generalization benefits oflearning multiple modules while feature attention acts synergistically.
Figure 6: While (a-c) show that individual modules have salient activity for different events, (d)shows that different combinations of modules coordinate their activity for different events. Thisindicates that event representations are distributed flexibly across the modules. (a) Module 0commonly exhibits salient activity when the agent moves around an obstacle. (b) Module 6 showsselective activity for representing goal information. (c) Module 6 activates its attention coefficients asthe agent picks up the goal key. (d) Some modules activate together (i.e. correlate) to represent somerecurring events but activate in opposite directions (i.e. anti-correlate) to represent other recurringevents. For example, modules 2 and 6 correlate for picking up the correct key but anti-correlatefor dropping the wrong object. This is similar to when neurons in word embeddings correlate forsome words (e.g. man and king), but anti-correlate for other words (e.g. man and woman). Theseresults suggest that FARM is distributing representations for perceptual schemas across the modulesin complicated ways rather than representing individual schema with separate modules. Videos of thestate-activity and attention coefficients: https://bit.ly/3qCxatr.
