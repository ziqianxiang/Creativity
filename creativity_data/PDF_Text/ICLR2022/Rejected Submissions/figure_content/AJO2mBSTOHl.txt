Figure 1: Compact representation of the variable nomenclature and the dependencies associated witha feedforward neural network. The red node denotes the input vector, green nodes are vectors ofhidden units z , and purple node denote the observation vector. The gray arrows represent the weightsand bias Î¸ connecting the different hidden layers and magenta arrows outline the flow of informationthat takes place during the inference step.
Figure 2: Graphical representation of a neural network structure for temporal-difference Q-learningwith categorical actions. The red nodes denote state variables, green nodes are vectors of hidden unitsz, and the blue box is a compact representation for the structure of a convolutional neural network.
Figure 3: Comparison of TAGI with backpropagation on deep Q-network with experience replay. L:loss function; U : uniform distribution; randi: uniformly distributed pseudorandom integers.
Figure 4: Illustration of average rewards over 100 episodes of five runs for one million time steps forthe TAGI-based and backpropagation-based deep Q-learning.
Figure 5: Illustration of average reward over 100 episodes of three runs for five Atari games. Thenumber of epochs is used here for the comparison of TAGI and backpropagation-trained counterpartobtained by Mnih et al. (2016). Each epoch corresponds to four million frames. The environmentidentity are {Atari Game}NoFrameSkip-v4.
