Figure 1: “rQdia in a nutshell” rQdia regularizes Q-value distributions across augmentations.
Figure 2: Contrastive sampling has disadvan-tages that Q-value-based equalization does not.
Figure 3: 20 seeds DM Suite, aggregated across 6 tasks, on 100k and 500k benchmarks.
Figure 4: 10 seeds Atari ALE, aggregated across 26 environments. (a) Performance profiles withpointwise 95% confidence bands show rQdia outperforms others with a large margin especially whenτ ∈ [0, 1], namely relative to human-level performance. After non-linear scaling, the improvementof our algorithm is more pronounced. The gap between DER+rQdia and DER suggests rQdia canmajorly improve learning. (b) The bottom-right subplot shows that rQdia has a very high chance ofimprovement over all baselines, and no other baseline can have a > 50% chance of outperformingrQdia. (c) Higher mean, median, and IQM scores and lower optimality gap are better. rQdia hasthe best performance across all four metrics, indicating a more certain and substantial improvement.
Figure 5: Pytorch code for rQdia in Rainbow Atari.
Figure 6: Pytorch code for rQdia in continuous control.
