Figure 1: (a) Visualization of overemphasis in softmax and resmax over a range of parameters.
Figure 2: The non-expansion property is important for converges under GVI. On the MDP shownin (a) the action-values, if using softmax in the expected sarsa update (with τ = 1/16.55), do notconverge (b). We can see that resmax does converge in (c) with η = 0.000085. We use a step size αat time t to be 1/(bt/100, 000c + 1), which meets the usual conditions for stochastic approximationto converge (Sutton & Barto, 2018). Note that the tuples in (a) are of the form (action, probabilityof transition given action, reward). Results are smoothed with a window size of 10, as in Asadi &Littman (2017).
Figure 3: (a) Hyperparameter sensitivity for RiverSwim. (b) Hyperparameter sensitivity for Stochastic-reward RiverSwim. For both (a) and (b) means and standard errors are shown for 30 runs for eachparameter setting. The x-axis is plotted with alog scale, and shows the value of exploration parameters.
Figure 4: Replay buffer size sensitivity plots describing the effect of different sizes of replay bufferon the performance of the agent for Deep Expected Sarsa. Results shown are averaged over 30 runsfor each parameter setting. The step-size value is set to 10-4 for all of the environments. Lowervalues are better in Sparse MountainCar plot and higher values are better in the other two plots. Thevalue in the legends indicate the value of the exploration parameter of each soft-greedy operator.
Figure 5: Bar charts describing the effect of different level of exploration on the performance ofsoft-greedy operators in DQN setting and learning-curves presenting the best performance acrossdifferent exploration parameters. Results shown are averaged over 10 runs for each parameter setting.
Figure 1:	Diagram of RiverSwim and Stochastic-reward RiverSwim. Dotted lines and solid linesshow the transitions and probabilities for the left and right actions, respectively. Diagram adaptedfrom Osband et al. (2013).
Figure 2:	Hyperparameter sensitivity for DQN. Results shown are averaged over 30 runs for eachparameter setting. The x-axis is plotted with a symmetrical log scale.
Figure 3: Learning curves for best performing hyperparameter settings for DQN. Best settings(based on average return) are shown in the top left of each subplot. Results are averaged over 30runs. ε-greedy consistently reaches its best performance with ε = 0.1 across all three environments.
Figure 4:	Replay buffer size sensitivity plots describing the effect of different sizes of replay buffer onthe performance of the agent for DQN. Results shown are averaged over 30 runs for each parametersetting. The step-size value is set to 10-4 for all of the environments. Lower values are better inSparse MountainCar plot and higher values are better in the other two plots.
Figure 5:	Bar charts for hard exploration atari environments describing the effect of different levelof exploration on the performance of soft-greedy operators in DQN setting and learning-curvespresenting the best performance across different exploration parameters. Results shown are averagedover 10 runs for each parameter setting. The step-size value is set to 10-4 across all the environments.
Figure 6: Bar charts describing the effect of different level of exploration on the performance ofsoft-greedy operators in DQN setting and learning-curves presenting the best performance acrossdifferent exploration parameters. Results shown are averaged over 10 runs for each parameter setting.
