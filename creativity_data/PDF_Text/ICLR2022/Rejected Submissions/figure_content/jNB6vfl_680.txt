Figure 1: An example workflow over a three-layer network with 60 weights. Thicker lines representthe weights with high magnitude. The network is first pruned to 60% sparsity globally, then MT of10% (6 weights) is activated per layer. Since the last layer is completely pruned by GP, MT recoversthe minimum number of connections there. It distributes the slack arising from the drop in sparsityto the other layers in proportion to their existing sparsities, i.e., 2 weights for Layer 1 (sparsity 20%)and 4 weights for Layer 2 (sparsity 52%). Red color indicates changes in weights due to GP andyellow indicates changes due to MT.
Figure 2: Weights remaining in WRN-22-8 model after pruning at 99.9% sparsity. MT helps retainweights in layers that have low magnitude weights and are heavily pruned, e.g., Layers 20, 21, and22.
Figure 3: For MobileNet-V2 at 98% sparsity as well, MT is essential to retain some weights in theheavily pruned layers (Layers 55, 56, and 57) and allow the model to learn successfully.
Figure 5: The frobenius output distortion is lower forGP compared to uniform pruning on a layer by layerbasis. Thus, GP preserves outputs closer to the originalunpruned model compared to uniform pruning. Resultson WRN-22-8 on CIFAR-10 dataset.
Figure 4: Difference in architectures be-tween WRN and MobileNet. WRN doesnot have any prunable residual connectionsin the last layers (dotted lines) while Mo-bileNet does. This leads to different pruningbehaviors on the two architectures.
Figure 6: Layer-wise pruning results produced by GP on MobileNet-V2 model on CIFAR-10. Prun-ing is done on three different pre-trained models and the pruning results across the three runs arevery stable.
Figure 7: A similar trend is observed for the case of GPMT on MobileNet-V2 model on CIFAR-10as well. Pruning results on the three different pre-trained models are very stable.
