Figure 1: During fine-tuning on ImageNet, pre-trained models exhibit effective robustness (ER)while randomly-initialized models do not. However, the ER of pre-trained models diminishesthroughout the fine-tuning process and vanishes altogether at the end of training. (a) illustrates thedefinition of ER that we define more precisely in Section 3. Subfigure (b) and (c) show ImageNet testaccuracy on the x-axis for either randomly initialized or pre-trained models throughout ImageNettraining (we evaluate accuracy on checkpoints every epoch of training). (b) plots the ImageNet-V2test accuracy on the y-axis while (c) shows the effective robustness for the same models. ER can alsobe achieved by zero-shot models. In (b) and (c) we also show a BiT model that was trained on JFT andevaluated on ImageNet using a class-map between the two sets of targets (see Appendix C.7), and weshow the best performing zero-shot and linear probe CLIP models from Radford et al. (2021a). TheBiT and CLIP zero-shot models have comparable ER, but at different ID accuracy, while the linearprobe CLIP model has no ER. Standard testbed models (see Section 4) fully trained on ImageNet arein blue and are shown with 95% Clopper-Pearson confidence intervals.
Figure 2: Testbed model accura-cies ( Recht et al. (2018)) show alinear relationship in a rescaledlogit-space Taori et al. (2020). (a)shows the linear fit in logit-space,and (b) shows the same fit trans-formed back to linear-space.
Figure 3: Pre-trained models consistently have high ER across different choices of architectureson both ImageNet and CIFAR-10. We plot ER (%) versus ID accuracy (%) at every epoch oftraining as we fine tune (a) pre-trained ImageNet models on CIFAR-10 or (b,c,d) pre-trained JFTmodels on ImageNet. Figure (a) shows the ER on CIFAR-10.1 while Figures (b,c,d) show the ERon ImageNetV2, ImageNet-R, and ObjectNet, respectively. For each plot, we also show the ERand accuracy achieved by converged testbed models in blue with 95% Clopper Pearson confidenceintervals. For (a) CIFAR-10.1 and (b) ImageNetV2, we trained a randomly initialized version of allthe model architectures and we report the model with the highest maximum ER during training (solidblack curve). We observe that the pre-trained models all exhibit high ER in the middle of fine-tuning,which eventually decreases as fine-tuning proceeds. In contrast, the randomly initialized models donot have significant ER relative to the testbed models.
Figure 4: The maximum ER depends both on the pre-training and the fine-tuning dataset. (a)We evaluate BiT-R101x3 models pre-trained on varying fractions of the JFT dataset throughoutfine-tuning on the CIFAR-10 dataset, and we find that maximum CIFAR-10.1 ER increases withthe size of the pre-training dataset. Error bars represent standard deviation over 5 runs. (b) Weevaluate several BiT models of increasing size (R50x1, R101x3, R152x4) that were pretrained oneither ImageNet (S), ImageNet-21K (M), or JFT (L) throughout fine-tuning on CIFAR-10. (TheBiT-M-1K was first pre-trained on ImageNet-21K and then ImageNet-1k). Maximum CIFAR-10.1ER increases as the model size and pre-training dataset size and diversity increase. (c) The maximumCIFAR-10.1 ER also depends on the difficulty of the fine-tuning dataset. We select examples basedon the C-score Jiang et al. (2020) of CIFAR-10 and train on either the 5000 easiest, hardest or randomimages in the training set. Training on harder examples yields more ER for both pre-trained andrandomly initialized models, but lower CIFAR-10 test accuracy than training on random examples.
Figure 5: Models with high effective robustness have higher dominance probabilities than mod-els with zero effective robustness on ImageNet. We analyze the predictions of the following fourmodels: BiT-L-R152x4 Max ER (A), CLIP zero-shot (B), CLIP-LBFGS (C), and BiT-L-R152x4Converged (D). (a) For each pair of models in the testbed plus these four models, we plot theirdominance probability versus their accuracy difference. We find that the CLIP zero-shot model andthe pre-trained BiT-L-R152x4 model have higher dominance probabilities on ImageNet than any ofthe models in the testbed, but once the CLIP and BiT-L models are fine-tuned on ImageNet, they alignmore with the dominance probability distribution of the testbed models. (b) We show the pairwisecomparison of the dominance probability on ImageNet between all testbed models (not labeled) inaddition to our four models. We sort models in order of increasing top-1 accuracy on ImageNet (seeAppendix C.10 for a bigger version with labels for all models). We see that the four labeled modelsmake very different predictions relative to the testbed. (c) For each of the plotted testbed models, weplot the percent of examples that none of the other testbed models get correct. Considering the 4.8%of all examples that none of the testbed models get right, the best BiT model pre-trained on JFT gets10% of them correct and the zero-shot CLIP model gets approximately 8% correct.
Figure 6:	Multiple runs are combined by computing the ER as a binned average across the IDaccuracy (using 100 bins). The shaded area around each curve shows the standard deviation per binover 5 runs. (a) is the same data as in Figure 3(a), and (b) is the same runs Figure 3(b).
Figure 7:	Comparison of two different ways of reporting the standard deviation (std.) for themaximum ER for the experiments in Figure 4(a). (a) shows the maximum standard deviation acrossall bins of CIFAR-10 ID test accuracy, and (b) uses the standard deviation in the bin that had themaximum ER. (b) has higher variation in the std. across different runs because each bin containsdifferent number of data points, so we use (a) as a more conservative estimate as our main reportedresults.
Figure 8: Comparison of the ER of a BiT-R101x3 model that is either pre-trained on JFT or randomlyinitialized. (a) shows the ER on ImageNetV2 when trained on ImageNet, and (b) shows the ER onCIFAR-10.1 when trained on CIFAR-10. For both datasets, the randomly initialized models showno ER while the pre-trained models do exhibit ER during training, but it vanishes as the modelconverges.
Figure 9: Evaluation on ImageNet-R and ObjectNet for a BiT-L-R152x4_JFT model that is fine-tunedon ImageNet. Where the ImageNetV2 ER goes to zero at the end of fine-tuning, the pre-trainedmodel still has non-zero ER.
Figure 10: Pre-trained models show ER for all six architectures studied, while none of the randomlyinitialized models do. The amount of ER does vary across different pre-trained models, but since wedid not do the pre-training, we cannot control for the differences between architecture and optimizationin this experiment (See Section 5.3 and Appendix C.8 and C.8 for more controlled experiments onthe BiT architectures.) However, the randomly initialized models show that pre-training is crucial toget any ER at all for any of the architecture choices.
Figure 11:	The rise and fall of ER occurs for different loss functions used in training. Here, wesee that the ER during fine-tuning is incredibly similar between a model trained with MSE andcross-entropy loss.
Figure 12:	Same setup as in 10, but with varying learning rate for a fixed architecture, Wide-ResNet-50-2. The left (right) plot shows the results using pre-trained (random) weights. While there is somesmall variations from different settings, the conceptual difference between a pre-trained model andrandomly initialized weights holds true.
Figure 13:	The ER of pre-trained models is mostly insensitive to whether the full model is trainedduring fine-tuning or just the last layer. We observe this across six different model architectures, andthe only significant observed difference is that the full-model training reaches higher final accuracies,which is to be expected.
Figure 14:	Zero-shot predictions using pre-trained BiT models fine-tuned to ImageNet-1k. Wecombine the ImageNet predictions that match the relevant CIFAR-10 class by choosing either themax, mean, or sum of the probabilities. The majority of models show some amount of ER.
Figure 15: ER of a series of BiT-R101x3 models pre-trained on different amounts of data fromJFT and fine-tuned on CIFAR-10. The maximum ER and the final accuracy increases with the JFTdataset size. The model with the maximum ER used 32 million images from JFT, and as discussed inSection 5.3, we suspect the plateau after this point is caused by the fixed number of iterations usedduring pre-training, which eventually serves a bottleneck for performance improvements.
Figure 16: The maximum ER increases as a function of model architecture size for all pre-trainingdatasets. The only exception is the BiT-S-R50x1 model that has the higest maximum ER among allthe R50x1 models, and it is also higher than the BiT-S-R101x3 and BiT-S-R152x4 models.
Figure 17: The ER during fine-tuning on CIFAR-10 is mostly increasing for larger pre-trainingdataset. We show this effect across three different architecture sizes, where each plot comparesmodels pre-trained on different datasets.
Figure 18: Larger model architectures gives higher maximum ER during fine-tuning on CIFAR-10.
Figure 19: Since models fine-tuned on more difficult examples show more ER, we start training withthe full CIFAR-10 training set. We then gradually eliminate the easiest examples (where difficultyis determined by the C-score) until we only have 1,000, 5,000 or 10,000 examples left by the lasttraining epoch. (a) shows the ER during training for a ResNet-18 randomly initialized model, and(b) show the same model initialized with ImageNet pre-trained weights. In both cases this datasetschedule does not help maintain higher ER at high ID accuracies.
Figure 20: Triplet-probabilities between all models in our testbed in addition to two CLIP models andtwo BiT-L models. 99.8% of all triplet events are inside the code as defined in Mania & Sra (2020),and there is no observable violation of the distributional closeness for the effectively robust models.
Figure 21: Pairwise dominance probabilities on ImageNet between all models in the testbed inaddition to the two CLIP models, and two BiT-L models.The effectively robust models (CLIP zero-shot and BiT-L Max ER) have high dominance probability relative to all other models, which meansthat they make different predictions than all other models in the testbed. Even after fine-tuning ofthe CLIP and BiT-L model, we see from the heatmap that these models still make more differentpredictions than other models despite being different from the testbed models in Figure 5.
Figure 22: We study the effect of using buffer replay on the ER during fine-tuning on CIFAR-10 for apre-trained ResNet-18 model. We compare a standard ResNet-18 to training a ResNet-18 where wesee 1, 10 or 100 ImageNet batches for every CIFAR-10 batch. Despite minor differences in the x1,x10 and x100 curves, none of them are able to retain ER beyond the standard ResNet-18 model.
Figure 23: Using a class map from CIFAR-10 to ImageNet target classes, we can fine-tune onCIFAR-10 by initializing the prediction head to the pre-trained ImageNet model. In (a) we fine-tunethe last layer of a BiT-M-R152x4_1k model when training on only CIFAR-10, or using a replaybuffer with 1 or 100 times as many ImageNet images. In (b) we fine-tune on only CIFAR-10, butusing a L2 regularization term around the pre-trained weights, where λ is the pre-factor of the L2term in the loss function. However, none of them are able to maintain high ER at high ID accuracy.
Figure 24: Left: The ER of pre-trained BiT models during fine-tuning on CIFAR-10. We include acurve indicating what the ER would be for a model that has no accuracy gap between the original andnew test sets. Right: The ER as a percentage of the accuracy gap. As a percentage of the accuracygap, the pre-trained models are slowly decreasing during fine-tuning.
Figure 25: Same as in Figure 24 for ImageNet. Left: The ER of pre-trained BiT models duringfine-tuning on ImageNet. We include a curve indicating what the ER would be for a model that has noaccuracy gap between the original and new test sets. Right: The ER as a percentage of the accuracygap. We note that as a percentage of the accuracy gap the pre-trained models are slowly decreasingduring fine-tuning.
