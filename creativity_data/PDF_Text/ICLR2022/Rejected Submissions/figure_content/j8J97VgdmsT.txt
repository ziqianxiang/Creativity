Figure 1: FLAME-in-NeRF. Our method, FLAME-in-NeRF, models portrait videos (left) using anexpression conditioned neural radiance field with a spatial prior (middle). Once trained, FLAME-in-NeRF can reanimate the subject and the scene present in the portrait video with arbitrary facialexpressions and novel views.
Figure 2: Expression-Appearance Entanglement: Why use the Spatial Ray Prior?: Here wedemonstrate the necessity of using a Spatial Ray Prior for the reanimation of portrait videos witharbitrary facial expressions and view control. On the left we have a model that does not use a SpatialRay Prior and on the right, a model that does. As can be seen, the model without the prior generatesresults of lower quality (e.g. on the lines of the brick wall) than the model with it. Further, thedifference images show that, despite keeping the viewing direction constant, the model without thespatial prior changes the background appearance with changing expression. In contrast, the modelwith the spatial ray prior does not do so as the prior explicitly disentangles the expression and theappearance in regions of the 3D scene that do not project to the face. (Please watch the accompanyingvideo in Supplementary).
Figure 3: Overview of training FLAME-in-NeRF. First, we use DECA (Feng et al., 2021) andlandmark fitting (Guo et al., 2020) to extract per-frame camera, shape, and expression parameters.
Figure 4: FLAME induced Prior. FLAME-in-NeRF uses a silhouette rendering of the FLAMEmodel geometry (an overlay is shown above) to provide a spatial prior on rays shot through the3D scene. All points that lie on rays that intersect the silhouette, shown in red, are affected by theexpression parameters. Other points, shown in green, have their expression parameters set to zeroand are therefore unaffected by changes in expression.
Figure 5: Qualitative evaluation by reanimating subject 1: Here we show the results of reanimat-ing Subject 1 using our method, Nerfies (Park et al., 2021) and NerFACE Gafni et al. (2021) withboth expression and view changes. The first row shows the driving frame, the second row shows theresults of our method, the third shows the results of Nerfies (Park et al., 2021) and the fourth rowshows the results of NerFACE (Gafni et al., 2021). As can be seen in columns 1-4, Nerfies (Parket al., 2021) is unable to model the facial expressions correctly leading to artefacts on the face whileNerFACE (Gafni et al., 2021) is unable to model the 3D scene leading to artefatcs on the background.
Figure 6: Qualitative evaluation by reanimating subject 3: Here we show the results of reanimat-ing Subject 3 using our method, Nerfies (Park et al., 2021) and NerFACE (Gafni et al., 2021) withboth expression and view changes. The first row shows the driving frame, the second row showsthe results of our method, the third shows the results of Nerfies (Park et al., 2021) and the fourthrow shows the results of NerFACE (Gafni et al., 2021). As can be seen in columns 1-4, Nerfies isunable to model the facial expressions correctly leading to artefacts on the face while NerFACE isunable to model the 3D scene leading to artefatcs on the background. We see that FLAME-in-NeRF,in contrast with Nerfies and NerFACE, generates high-quality reanimation results with high fidelityto the driving expression and consistency across views.
