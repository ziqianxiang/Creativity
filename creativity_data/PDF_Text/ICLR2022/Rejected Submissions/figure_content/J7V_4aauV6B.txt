Figure 1: We compared Equation-(1)-basedweight decay and Equation-(2)-based weight de-cay by training ResNet18 on CIFAR-10 viavanilla SGD. In the presence of a popular learningrate scheduler, Equation-(2)-based weight decayshows better test performance. It demonstratesthat the form -ηt λθ is a better weight decay im-plementation than -λ0θ.
Figure 2: We train ResNet18 via SGD on CIFAR-10 for verifying that Convergence = O (λ-1).
Figure 3: Large-batch training (B = 16384) with various learning rates and weight decay. Notethat η = 10-3 and λ = 10-4 is the baseline choice for B = 128. Subfigure (a) and (b) show that,even slightly increasing the learning rate (by multiplying 16) is harmful to optimization convergence.
Figure 4: The learning curves of AdamS, AdamW, and Adam on CIFAR-10 and CIFAR-100. AdamSshows significantly better generalization than AdamW and Adam.
Figure 5: The scatter plot oftraining losses and test errorsduring final 40 epochs of train-ing ResNet34 on CIFAR-100.
Figure 6: The learning curvesof all adaptive gradient methodsby training ResNet34 on CIFAR-100. AdamS outperforms otherAdam variants. The test perfor-mance of other models can befound in Table 1.
Figure 7: The test errors ofVGG16 on CIFAR-10 with var-ious weight decay rates. Thedisplayed weight decay value ofAdamW has been rescaled bythe factor ≈ 0.001. A similar ex-perimental result for ResNet34is presented in Appendix C.
Figure 8: The test errors of ResNet18 on CIFAR-10. AdamS has a much deeper and wider basin neardark points (≤ 4.9%). The optimal test error of AdamS, AdamW, and Adam are 4.52%, 4.90%, and5.49%, respectively. The displayed weight decay value of AdamW has been rescaled by the factor≈ 0.001.
Figure 9: Rule 2 holds well for all Adam,AdamW, and AdamS on ResNet18. Figure 16shows similar results for VGG16, which has noscale-invariant loss landscape.
Figure 10: ResNet50 on ImageNet. The lowestTop-1 test errors of AdamS, AdamW, and Adamare 24.19%, 24.29%, and 30.07%, respectively.
Figure 11:	Language modeling under various weight decay. Note that the lower perplexity is better.
Figure 12:	The learning curves of adaptive gradient methods.
Figure 13:	Even if with similar or higher training losses, AdamS still generalizes better than AdamWand Adam. The scatter plot of training losses and test errors during final 50 epochs of training VGG16on CIFAR-10 and DenseNet121 on CIFAR-100.
Figure 14:	We compare the generalization of Adam, AdamW, and AdamS with various weight decayrates by training ResNet34 on CIFAR-100. The displayed weight decay of AdamW in the figure hasbeen rescaled by the factor ≈ 0.001. The optimal test performance of AdamS is significantly betterthan AdamW and Adam.
Figure 15:	We train ResNet18 on CIFAR-10 for 900 epochs to explore the performance limit ofAdamS, AdamW, and Adam. The learning rate is divided by 10 at the epoch of 300 and 600. AdamSachieves the most optimal test error, 4.70%.
Figure 16: Rule 2 holds well for all of Adam, AdamW, and AdamS. VGG16 on CIFAR-10.
Figure 17: The learning curves of ResNet18 and VGG16 on CIFAR-10 with cosine annealing andwarm restart schedulers. The weight decay hyperparameter: λL2 = λS = 0.0005 and λW = 0.5.
Figure 18: The test errors of ResNet18 and VGG16 on CIFAR-10 under various weight decaywith cosine annealing and warm restart schedulers. AdamS yields significantly better optimal testperformance than AdamW and Adam.
Figure 19: We compare the gen-eralization of Vanilla SGD, SGD,SGDW, and SGDS under variousweight decay hyperparameters bytraining VGG16 on CIFAR-10.
Figure 20: The test errors of ResNet18 on CIFAR-10. SGDS hasa slightly deeper blue basin near dark points (≤ 4.83%). Theoptimal choices of η and λ are very close for SGDS and SGD.
Figure 21: We compare the generalization of Vanilla SGD, SGD, SGDW, and SGDS with variousweight decay hyperparameters by training ResNet18 on CIFAR-10. The optimal weight decay ratesare near 0.0005 for all three weight implementations. The optimal performance of SGDS/SGDWis better than Vanilla SGD and SGD. For Vanilla SGD, SGD, and SGDS, we may safely chooseλL2 = λS = 0.0005. But we have to re-tune λW = 0.005 for SGDW. Hyperparameter Setting:β1 = 0 for Vanilla SGD; β1 = 0.9 for SGD, SGDW, and SGDS.
Figure 22:	Generalization analysis on SGDS and SGD with L2 regularization. HyperparameterSetting: λS = λL2 = 0.0005 and β1 = 0.9.
Figure 23:	Generalization analysis on AdaiS and Adai with L2 regularization. HyperparameterSetting: λS = λL2 = 0.0005.
