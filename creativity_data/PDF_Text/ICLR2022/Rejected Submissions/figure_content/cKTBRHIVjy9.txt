Figure 1: Overview of SUBMIX’s training protocol (left) and prediction protocol (right).
Figure 2: Perplexity for = 2, α = 2 of four privacy-preserving mechanisms (SubMix, DP-SGD, S&A, and GN-Max) using GPT-2 on Wikitext-103 with varying querybudgets B . Non-privately fine-tuning achieves a perplexity of20.0 and the pre-trained public model achieves a perplexityof 37.5 . Lower perplexity is better.
Figure 3: Perplexity of SUBMIX (k = 8) on Wikitext-103 (left) and BigPatent-G (right) asa function of ROP privacy loss e for three query budget values B . The perplexity of the pre-trainedmodel and (non-private) single model, ensemble, and fully fine-tuned models are shown for reference.
Figure 4: Perplexity of SUBMIX (k = 8) on Wikitext-103 (left) and BigPatent-G (right) asa function of query budget B for different ROP privacy losses . The perplexity of the pre-trainedmodel and (non-private) single model, ensemble, and fully fine-tuned models are shown for reference.
Figure 6: Hit rate of text extraction attacks on SUBMIX forvarying lengths of code. The # of parts is k = 3 and the # ofcodes generated is g = 100. The non-privately fine-tuned LMhas a hit rate of ≥ 0.9 for all lengths.
