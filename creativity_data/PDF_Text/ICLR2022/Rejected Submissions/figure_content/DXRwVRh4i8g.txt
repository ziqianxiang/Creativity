Figure 1: Illustration of a reachability trace network that maps states on the successful trajectory(left) of a hypothetical navigation environment to their trace labels.
Figure 2: The (a) Gridworld environment (b) U-shaped, (c) S-shaped, (d) ω-shaped and (e) Π-shapedmaze environments used for evaluation, where the red marker denotes the goal position.
Figure 3:	The trace function (center) is used to identify subgoals g0 ,g1 and g2 , corresponding towhich policies πg0, πg1 and πg2 are learned. The colors respresent the trace values (for the centerimage) or scaled value functions (for images corresponding to πg0, πg1 and πg2)this trace function, we generate subgoals (Algorithm 2) based on the temporal closeness to the goalstate. The corresponding subpolicies (visualized in Figure 3) are learned and subsequently used toguide the actions of the agent (Algorithm 3).
Figure 4:	Performances in (a) the gridworld environment and (b) the U-Maze environment over 10and 5 trials respectively.
Figure 5: (a) Environment with starting state s0, goal G and non-terminal rewards at sNT, and (b)corresponding performance over 5 trials.
Figure 6: Distribution of subgoals over 30 runs in the U-Maze environmentFigure 7: Number of subpolicies learned using reachability traces over 20 runs each for differentsubgoal trace scaling factor tφ in the U-Maze environment(for example, a subgoal with a high but inaccurate reachability trace could get selected), leading to apoor distribution of subgoals. Hence, despite the potential advantages of using high tφ , we suggestthe use of a scaling factor tφ = 1.
Figure 7: Number of subpolicies learned using reachability traces over 20 runs each for differentsubgoal trace scaling factor tφ in the U-Maze environment(for example, a subgoal with a high but inaccurate reachability trace could get selected), leading to apoor distribution of subgoals. Hence, despite the potential advantages of using high tφ , we suggestthe use of a scaling factor tφ = 1.
Figure 8: Variation of performance with different stored trajectory lengths L over 10 trialsFigure 9: Comparison of our approach with and without the availability of demonstrations over 10trials in the gridworld environment of Figure 2 (a).
Figure 9: Comparison of our approach with and without the availability of demonstrations over 10trials in the gridworld environment of Figure 2 (a).
Figure 10: (a) The environment from Figure 2(a) with a new goal location and (b) a comparison ofthe corresponding performances of standard Q-learning vs the policy reuse baseline (PPR), wheresubpolicies from Figure 3 are reused.
Figure 11: The performance of reachability traces used as an exploration bonus, compared to otherapproaches averaged over 10 trials in the gridworld environment of Figure 2 (a).
Figure 12: (b) Visualization of traces in the (a) UMaze EnvironmentFigure 13: (b) Visualization of traces in the (a) Mountain Car-v0 environmentHyperparameters	Gridworld	Point U-Maze	Other Point mazeslearning rate a	0.05	:	1e- 5	1e - 5discount factor Y	05	0.95	0.95Episode horizon H	200 一	500	50KTotal steps	1e6	2e5	1e6Stored trajectory length L	40	50	2KSize of trace buffer	500	300	1WSubgoal trace scaling factor tφ	1	1	1Epochs for training φ	100	10K	1WTrace decay parameter	0.9	一	0.99	0.9999Table 2: Hyperparameter values for our approach.
Figure 13: (b) Visualization of traces in the (a) Mountain Car-v0 environmentHyperparameters	Gridworld	Point U-Maze	Other Point mazeslearning rate a	0.05	:	1e- 5	1e - 5discount factor Y	05	0.95	0.95Episode horizon H	200 一	500	50KTotal steps	1e6	2e5	1e6Stored trajectory length L	40	50	2KSize of trace buffer	500	300	1WSubgoal trace scaling factor tφ	1	1	1Epochs for training φ	100	10K	1WTrace decay parameter	0.9	一	0.99	0.9999Table 2: Hyperparameter values for our approach.
Figure 14: Performances in the point (a) S-Maze, (b) ω-Maze and (c) π-Maze environment oversover 5 trials.
