Figure 1:	Frequency concentration of eight distinct English word embeddings. The observedfrequency concentration is similar even though trained on different data sources (e.g., Wikipedia(Wiki), Common Crawl (CC/42B/840B/Sub), and Google News (News)) and different embeddingalgorithms including GloVe, fastText, and word2vec. Each dashed red curve is linearly fitted wherelinear coefficient is presented on each legend.
Figure 2:	Frequency concentration plots for all nine different models where each model generatesn = 2000 points with d = 128. The parameter k of the Index for each node is set to 10.
Figure 3:	Top: The number of connected components as a function of inserted edges. Each col-umn shares legend. Bottom: The number of connected components as a function of γ in graphpartitioning methods Clauset et al. (2004) in log-scale. We sampled top 30K and 10K vectors fromembeddings to visualize clustering velocities, except for smaller PPI graph which has only 3,980nodes. All real-world embeddings share similar clustering velocity pattern, while model generatedspace have distinct shapes.
Figure 4:	The visualization of real-world and model generated embeddings (dim = 300) in the planeof two proposed clustering velocity indices, frequency concentration, and spatial autocorrelationindices . Real-world embeddings share similar pattern as several model generated embeddings (BA-Centroid-{NW,EW} and DPP ), while other models do not. Each model is represented with fivedistinct hyper-parameter configurations. On right, frequency concentration correlates well with thespatial autocorrelation statistic Moran’s I [-0.810] (and with 1-Geary’s C [-0.450] in Figure 9 inAppendix).
Figure 5:	Frequency concentration of four distinct embeddings of the Wikipeople graph. The ob-served frequency concentration is similar even though trained on different graph embedding algo-rithms (LLE, DeepWalk, Line, FastRP).
Figure 6:	Frequency correlation of Polyglot embedding (Al-Rfou et al., 2013) as a function ofranking of words frequency. Each embedding dimension is d = 64 and k = 10.
Figure 7: Frequency Concentration was also obseved in contextual word embeddingsC Preferential placement (PP) modelAlgorithm 2 presents the generating process of PP model.
Figure 8: The PDF of discussed statistics for each model running with 5 random seeds and distinctmodel hyper-parameter combinations as listed above. (n = 2000, d = 300)For a more intuitive interpretation, it may be easier to use C0 := 1 - C. Then, C0 ∈ [-1, 1]; C0 < 0means negative autocorrelation (checkerboard), C0 > 0 means positive autocorrelation (same-colorsquares on one side).
Figure 9: Moran’s I and Geary’s C are well correlated, and so does our proposed Frequency Con-centration.
