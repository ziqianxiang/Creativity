Figure 1: A sentence and its syntactic roles. The correspondence between syntactic roles and elementsof the predicative structure is highlighted with colors.
Figure 2: In blue, we highlight in (b) the difference between our encoder and a source-to-target MTmodel, and in (c) the difference between our decoder and a target-to-source MT model. The inputat the bottom right for the Transformer Decoders in (a) and (c) is the series of previous words forautoregressive generation. The input to our model is a series of words s, at the bottom left of (b), andits output is the reconstruction of these words in the same language, at the top right of (c).
Figure 3: Encoder influence heatmap (Γenc). Figure 4: Decoder influence heatmap (Γdec).
Figure 5: The conditional inference module linking each of the hierarchy levels in our prior with thenext level pθ (Zl |Zl-1). This module treats latent variables from previous layers as they are treated inour original decoder, and generates parameters for latent variables in subsequent hierarchy levels as itis done in our encoder.
Figure 6:	Encoder influence heatmap forYelp(Γenc).
Figure 7:	Decoder influence heatmap forYelp(Γdec).
Figure 8: The influence of latent variables on theappearance or disappearance of syntactic roles.
Figure 13: Decoder influence heatmap for all UD syntactic Roles.
Figure 14: Encoder influence heatmap for all UD syntactic Roles.
Figure 17: Encoder influenceheatmap (Γenc) whenaveraging over both layers.
Figure 15:	Encoder influenceheatmap (Γenc) when onlyusing the first layer.
Figure 16:	Encoder influenceheatmap (Γenc) when onlyusing the second layer.
Figure 18:	Encoder influence heatmap forADVAE with 16 latent variables on SNLI (Γenc).
Figure 19:	Decoder influence heatmap forADVAE with 16 latent variables on SNLI (Γdec).
