Figure 1: (left) Illustration of the standard sequential implementation for computing the hidden statem4. The input x1 is fed into the linear recurrent unit to compute the hidden state m1, which, alongwith x2, is then used to compute the next hidden state, and so on. (right) Illustration of the time-domain parallel implementation for computing the hidden state m4. The inputs x1-x4 are used tocompute the intermediate multiplies, which are then added together to compute the hidden state m4 ,all without the need for any sequential operations.
Figure 2: The LMU and implicit self-attention architecture along with output dimensions. In theillustration, n refers to the sequence length, q is the order and q0 is the reduced order, and d is theembedding dimension. Normalization layers and skip connections are not shown. One variant usesthe FFN component right after the input, and the other variant uses global attention.
Figure 3: Cross-entropy scores in nats, averaged across all the tokens in the sequence. Transformersand LSTMs fits are from Kaplan et al. (2020). Our models perform better than Transformers andLSTM models up to 1 million non-embedding parameters.
Figure 4: (left) Comparison of per-token loss of an LMU model (with global attention) and a trans-former model. (right) Per-token loss of an LMU model (without global attention) alongside thetransformerâ€™s loss.
Figure 5: Approximately matching the loss between transformers and LMUs requires 10x moretraining for the transformer. The LMU and Attention model continues to significantly outperformtransformers with 10x less training.
