Figure 1: The destructiveness of high-frequency components from natural and adversarial exampleson both standard (STD) and adversarially-trained (ADV) models. Shown above are well-trainedmodels tested with (a)-(b) images through low-pass filter and (c)-(d) frequency-swapped images.
Figure 2: The average logarithmic amplitude spectrum of (a) 1000 three-channel images and (b) theiradversarial examples generated by the standard (STD) model, where the corners represent high-freqrange, and the colorbars represent the logarithmic amplitude log(∣∙ |) (the redder the larger). And(c) denotes the difference between (b) and (a), log(|adv|) - log(|nat|) = log(|adv|/|nat|), wherethe write color of (c) represents equivalent, and the red represents |adv | > |nat|. (d) and (e) are onthe adversarially-trained (ADV) model, and (e) denotes the difference between (d) and (a).
Figure 3: Visualisation of adversarial perturbations (PGD) generated by Left: adversarially-trained(ADV) model and Right: standard (STD) model. The columns from left to right are representedby (a)-(m) respectively: (a) natural example; (b) adversarial example and (c)-(g) its perturbations(overall, average of channels, and three channels) generated by ADV model; (h) adversarial exampleand (i)-(m) its perturbations generated by STD model. Shown above are all successfully attacked,the first row is attacked as a bird from a dog, the second a cat from a frog, the third a car from a ship.
Figure 4: Further impact of local perturbations on different convolutional kernels. Shown above isadversarial perturbations with same amplitude (single channel) act on kernels with different smooth-ness and then get the local response differences of adversarial example and natural example.
Figure 5: The maximum local response differences on robust models with different smoothness. (a)shows different model smoothness affected by weight decay, (b)-(g) denote the impact of weightdecay on the local response differences, and (h) shows the model robustness and performance.
Figure 6: The destructiveness of only high-frequency components from natural and adversarial ex-amples on both standard (STD) and adversarially-trained (ADV) models. Shown above are well-trained models tested with images through high-pass filter.
Figure 7: Visualisation of adversarial perturbations (FGSM) generated by Left: adversarially-trained(ADV) model and Right: standard (STD) model.
Figure 8: Visualisation of adversarial perturbations (PGD) generated by Left: adversarially-trained(ADV) model and Right: standard (STD) model. Shown above are all unsuccessfully attacked onADV model.
Figure 9: The total absolute local response differences (per image) of some layers on adversarially-trained models with different smoothness. Among them, (a)-(f) denote the influence of differentweight decay parameters on the local response differences in each layer, and (g) shows the robustloss of training set and test set.
