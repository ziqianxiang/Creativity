Figure 1: Implicit bias of Sum-Max SAM for a diagonal networkon a sparse regression problem.
Figure 2: ERM vs SAM training overepochs. Top: standard SAM training.
Figure 3: Plots over training for a ResNet-18 trained on CIFAR-10 with 60% label noise. We canobserve that (a) test error increases when we fit the noisy samples, (b) using gradients at the SAMpoint doesn’t change the alignment with the ideal direction, (c) using gradients at the SAM pointdecreases the alignment with the noisy part.
Figure 4: SAM for a linear model ontwo Gaussians with 90% label noise.
Figure 5: An illustration of the early-learning phenomenon (Liu et al., 2020) on the training set for alinear model. Early in training the model achieves a good separation between noisy and correctlylabeled points which disappears towards the end of training.
Figure 6: Introducing a loss offset c mainly affects points with small positive (for CE and GCE) andsmall negative margin (for GCE) which mostly correspond to clean and noisy points, respectively,due to the early-learning phenomenon.
Figure 7: The effect of SAM on robustoverfitting in adversarial training is similarto its effect for noisy labels (cf. Fig. 3a).
Figure 8: Robust overfitting for a linearmodel is not observed for label-Preservingadversarial training.
Figure 9: Implicit bias of SAM on a sparse regression problem with d = 30, n = 20, Xi 〜N(0, I),K = kβ* ko = 3, yi = x>β* and 九〃(x) = x>(u Θ v). All methods are initialized at α = 0.01 andused with step-size γ = 1/L and ρ = 1/L. SumMax SGD converges to a solution which generalizesbetter (left plot) and enjoys a different implicit bias from the other methods. At the same time, allalgorithms converge to a global minimum of f at linear rate (right plot). The convergence rate isinversely proportional to the biasing effect.
Figure 10: Training and test error over training epochs for the normalized and unnormalizedformulations of SAM. We can see that both variants of SAM improve the test error to a similar level.
Figure 11:	Loss interpolations of wSAM vs wERM and wSWA vs wERM. We see that the loss interpolationfor SAM is qualitatively different from SWA.
Figure 12:	Test error over epochs for ERM, SAM direction, SAM derivative, and SAM for threerandom seeds. Although the test error for SAM derivative is oscillating, the best test error over epochs(dashed line) is lower than for other methods.
Figure 13:	Robust error over epochs for standard adversarial training compared to adversarial trainingwith GCE and with the `2 pairing term.
