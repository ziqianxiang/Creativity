Figure 1: Preferences induced by different RS policies across the same cohort of users. Insimulated environment, preferences (y axis) are in 1D, and change over time (x axis) as users interactwith the policy. On the right, the RL system drives preferences to one spot. A myopic policy (center)has a similar effect but less concentrated. These shifts are different from what we introduce as the”natural” evolution of user preferences.
Figure 2: Future preference estimation model. Given past slates and choices s0:t-1, x0:t-1, wetrain a network to predict beliefs over the current timestep preferences ut , which we then com-bine with the current slate st to predict a distribution over item choices by using the choice modelP(xt|ut,st): P(xt|s0:t,x0:t-1) = Rut P(xt|ut,st)P(ut|s0:t-1,x0:t-1). At training time, we su-pervise this with the actual choice Xt the user made for slate St - the network will learn to predictbeliefs over preferences which induce behavior which is seen in the training data.
Figure 3: Simulating futures. By iteratively using the future preferences estimation model (Fig. 2)by inputting the observables (shaded, e.g. xg6, s6:6), one can simulate how an existing user's Pref-erences would evolve in the future if they interacted with same policy π or a different policy π0 (byselecting future slates with π0).
Figure 4: Simulating Counterfactuals. To simulate What the user's preferences would have beenunder a different policy π0 , we first can use the initial preference model to estimate initial preferencesbased on later observables. We can then use the estimated initial preferences to simulate counter-factual preference trajectories for the user under π0 using a conditioned future preference estimationmodel - where estimates are conditioned on the recovered initial preference belief.
Figure 5: Ground truth human dynamics. At each timestep, the user Win receive a slate st. Giventhe user’s preferences ut, the slate st induces a distribution over item choices P(xt|st, ut) fromwhich the user samples an item Xt and receives an engagement value rt (unobserved by the RS).
Figure 6: When adding our proxies to the op-timization of our RL LTV system, the induceddistribution of preferences are closer to the NPSand initial preferences. Myopic systems insteadwill only greedily be pursuing the penalizedobjective, leading the resulting behavior to besomewhat arbitrary.
Figure 7: The actual preferences induced by theNPS policy (a random RS πrnd) among a co-hort of 1000 users (left) vs. a 1000-sampled-trajectory Monte Carlo estimate of what shiftswould be induced. Qualitatively, the estimatedpreferences seem to match the ground truth ones-although slightly more washed out.
Figure 8: BERT representation of the inference tasks. While our method is compatible with anysequence model, we choose to use a BERT transformer models. Left: estimation of the user’s futurepreferences and choice at t=2, given the interaction history history so far (this modality of predictionis closest in setup to (Sun et al., 2019)). Middle: recovering a belief over the initial preferencesand choice of the user based on later interactions. Right: conditioning on the estimate of initialpreferences ⑴ recovered from the smoothing network one Can estimate CoUnterfaCtUal preferencesand choices under slates (*) (chosen from a policy π0 we are interested in) and imagined choices(not shown dUe to space).
Figure 9: a) → b) The content can be mapped to an empirical distribution over feature space D inRd . We consider dimension d = 2 for ease of visualization. Restricting preferences and choicesto be unit vectors, one can think of them as a points on a circle: the engagement value rt willthus be related to the angle θ between ut and xt . c) We discretize this circular preference andfeature space into n = 36 bins (i.e. binning the angles) which enables to visualize distributions overpreferences and over content features as histograms over angles. d) We model slates st as categoricaldistributions over the discretized n-bin feature space.
