Figure 1: (A): Sampled summation of an array; in the dense case the proposal is important for variance reduc-tion, while in the long-tailed case, topK summands are important; (B): core recursion step of the RandomizedForward algorithm. We get topK and sample from the proposal (black and grey bars); Errors stem from thedifference (green bars) between the oracle proposal a and constructed proposal q; (C): Inferring latent stateswithin the BERT representation space. We parametrize the CRF factors with vector products; the relationsbetween states and contextualized embeddings together form a latent network (Fig. 3 and 4); (D): Experimentalprotocol; we first study the basic properties of RDP (steps 1, 2) and then integrate RDP into a LVM for inferringthe structure of the representation space (steps 3, 4). Best viewed in color.
Figure 2: Sampled Forward vs. ToPK summation (SUn et al., 2019) in different unit cases during training. Redline: target log partition. Grey line: estimates from TopK. Our method effectively corrects the bias in TopKsummation with significantly less memory, and is consistent with dense and long-tailed distributions.
Figure 3: (A1): Frequent words partake in more latent states than rare words (presumably because they arepolysemous); (A2 and A3): The distribution of states is also Zipfian, as most frequent states generate mostwords (the orange portion in A2 is almost indistinguishable); (B): t-SNE (Van der Maaten & Hinton, 2008)visualization of latent network induced from BERT; (B1): Words and their corresponding latent states. Forstates, the size of circle indicates frequency (≈ aggregated posterior probability) and color thickness meanslevel of contextualization; a state with deeper blue color tends to generate content words (whose meaning isless dependent on context); lighter blue corresponds to stopwords (which are more contextualized); words arealso colored by number of states (≈ number of linguistic roles); red color densities mean a word is generatedby several states; (B2) and (C): sample from p*(x)qφ(z|x). Our method discovers a spectrum of meaningfulstates which exhibit both morpholigical, syntactic and semantic functionalities.
Figure 4: (A1): Geometrical differences between top and tail states; most lexical variations are encoded by thetop 500 states while remaining states represent the long tail; (A3 and A4): Network topology within top 500states; in (A3) nodes are connected to their top 1 neighbor according to the transition matrix Φ (as a proxy of theempirical prior) and in (A4) according to the most frequent bigram (as a proxy of the aggregated posterior), notehow the two are correlated; (A2 and B): Highlighted bigrams and their linguistic interpretation; transitions withstopwords are more about syntax (e.g., to with infinitives or transitive verbs); transitions without stopwords aremore about specific meanings. (C): paraphrasing as latent network traversal.
Figure 5: State frequency with different N (number of states). When N = 50, the long-tail behavior is notvisible. The long-tail behavior emerges only when N is large enough (larger than 500 in our case).
Figure 6: State frequency by controlling K1 (SUm size) and K2 (sample size). We highlight that When K = 0,a pure topK summation approach would lead to posterior collapse where there exist inactive states that do nothave any density. We also notice that an increasing K consistently increases the frequency of tail states.
Figure 7: Comparison of reconstruction likelihood (log p(xt |zt, ∙)) between a randomly initialized BERT anda pretrained BERT. States induced from pretrained BERT are more meaningful than random, making it easierto reconstruct the words based on their corresponding states.
Figure 8:	Application of RDP to tree structured hypergraph. Our method consistently outperforms topKSUmmantiOn (grey lines) with less memory (as our estimates are visually closer to the red line, i.e., the truepartition).
Figure 9:	Application of RDP to entropy estimation of linear-chain CRFs. Our method consistently outper-forms topK summantion (grey lines) with less memory (as our estimates are visually closer to the red line, i.e.,the true entropy).
