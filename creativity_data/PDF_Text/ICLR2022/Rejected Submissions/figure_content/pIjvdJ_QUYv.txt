Figure 1: High-level view of PrivHFLPrivHFL follows prior HFL works (Li & Wang, 2019) and iteratively optimizes the clients, models.
Figure 2:	Secure query-data sharing protocol ∏shareSecure prediction. Recently, many works achieve secure prediction protocols in client-server set-ting (i.e., 2-party setting), such as the SOTA CrypTFlow2 (Rathee et al., 2020) or HE-Transformer4Under review as a conference paper at ICLR 2022(Boemer et al., 2019a) adopted by CaPC (Choquette-Choo et al., 2021). However, such methodsadd a huge overhead due to the use of heavy cryptographic primitives, e.g., HE and GC. The mostefficient secure prediction protocol by far is CryptGPU (Tan et al., 2021) under the 3-party setting,but it cannot be directly applied to HFL due to the inability to communicate between clients. Toachieve the efficiency of secure prediction and adapt to the communication-limited scenarios, wedesign customized protocols for the linear layers and non-linear layers2 from scratch. Figure 17 inAppendix C.2 gives a graphic depiction of end-to-end secure prediction. Below, we elaborate on theevaluation of the linear layers, ReLU and MaxPooling.
Figure 3:	Secure matrix multiplication protocol ∏Matmul(i)	Linear layers. Linear layers, evaluation follows the idea of the Beaver,s multiplication in Ap-pendix C.4.1, but we improve the communication efficiency using PRGs. Specifically, PA and theserver compute matrix multiplication ωx, where the model parameter ω is held by PA and the inputx is secret-shared between PA and the server. Given that ωx = ω[x]0 + ω[x]1, PA can computeω[x]0 locally. As shown in Figure 3, we design an efficient protocol ∏Matmul for evaluating ω[x]1.
Figure 4:	Secure DReLU protocol ∏DReLU2Deep learning models consist of a sequence of linear layers (e.g., fully-connected layers, convolutionallayers and AvgPooling) and non-linear layers (e.g., ReLU and MaxPooling).
Figure 5:	Secure result aggregation protocol ∏AggGPU Acceleration. Our protocols mainly consists of GPU-friendly vectorized secret-sharing,which can be processed by highly-optimized CUDA kernels. However, for multiplication opera-tions, existing CUDA kernels are designed to operate on floating-point inputs. In PrivHFL, Wetypically compute over integer values. To leverage optimized kernels for protocol acceleration,our PrivHFL integrates the CUDALongTensor abstract in CryptGPU (Tan et al., 2021) that embedsthe integer-valued cryptographic operations into floating-point arithmetic (refer to Appendix C.6for more details). However, CryPtGPU's protocols cannot be directly extended to PrivHFL in thecommunication-limited scenario, and hence we redesign all cryptographic protocols from scratch.
Figure 6: The performance of PrivHFL on different query datasets: the private dataset andthe synthetic datset based on mixup. We report the average accuracy of each model architecture.
Figure 7: Test accuracy of heterogeneous models after PrivHFL as the number of query dataincreases. Dashed lines represent the baseline, i.e.,the test accuracy before executing PrivHFL.
Figure 8: Runtime (sec) of batch secure prediction on CPU and GPU settings as the batch sizeincreases. We report the result of VGG-style and ResNet-style networks on CIFAR10.
Figure 9: Comparison with federated learning and heterogeneous federated learningB Additional Experimental resultsB.1	Understanding dataset expansionRecall that we instantiate the dataset expansion method by leveraging mixup (Zhang et al., 2018)in Section 3.2. To explore the effectiveness of the mixup-based datset expansion method, we vi-sualize the feature distribution of original data and synthetic samples on SVHN and CIFAR10. Asshown in Figure 11(a) and Figure 11(b), the synthetic samples cover a larger part of the featurespace and hence they should be more diverse and informative compared with original data. In otherwords, it could provide a good coverage of the manifold of natural samples. Therefore, learning thepredictions from other clients on the synthetic samples can further improve the accuracy of localmodels.
Figure 10: Overview of the results of Gussian noise, random flipping, cutmix, cutout, andmixup data augmentation strategies.
Figure 11:	Understanding the importance of dataset expansion on improving model perfor-mance. In (a) and (b), the bright colored points indicate the position of the original training data inthe feature space, and the light blue points indicate the distribution of the expanded points based onmixup in the feature space. In (c), the left and right dashed lines are the baseline test accuracy onCIFAR10 and sVHN, respectively.
Figure 12:	Using PrivHFL With active learning strategies to improve model accracy on SVHNand CIFAR10 datasets. Dashed line represents the baseline accuracy of models, and the histogramrepresents the accuracy of the model after PrivHFL based on different active learning strategies.
Figure 13: Impact of the normalization operation on the accuracy of heterogeneous modelson SVHN and CIFAR10 datasets. Dashed line represents the average accuracy of heterogeneousmodels when only private data is used for query.
Figure 14: Visualization of Non-IID-ness among clients with different Dirichlet distribution αvalues on CIFAR10 dataset. The size of scattered points indicates the number of training samplesfor a class available to that client.
Figure 15: The test accuracy of PrivHFL on SVHN and CIFAR10 with different degrees ofNon-IID-ness. The number of clients n = 20.
Figure 16: Impact of different amounts of private data on the accuracy of models on SVHNand CIFAR10. The number of clients n = 50, and the number of private data samples for eachclient ranges from 200 to 1000.
Figure 17: The whole private inference implemented across all the layers. Orange boxes rep-resent linear layers (including ConvolUtional/fully-connected/AvgPooling layers), and blue boxesrepresent non-linear layers (including ReLU/MaxPooling layers).
Figure 18: Secure PRG seed generation protocol ∏Seed20Under review as a conference paper at ICLR 2022C.5 Extending CaPC to communication-limited settingsBy carefully designing protocols, CaPC (Choquette-Choo et al., 2021) can also be extended to asetting where there is no direct communication between clients. However, as analyzed below, suchextension comes at the cost of increased communication overhead. Therefore, our PrivHFL hasgreater advantages compared to the following modified protocols.
Figure 19: Different mixup-synthetic images from the same pair of the natural images by vary-ing the mixup coefficient λ.
Figure 20: An example of secret sharing on the mixup image. x, x\ and xo are pixel matrices,owned by the querying party PQ, the server, and the answering party Pa, respectively.
