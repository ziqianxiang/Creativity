Figure 1: Performances of the policies obtained during the last 100 iteration of MEGAN: as theparameter of η = Geom(κ) grows from 0 (equivalent to an instance of GAIL) to 1 (equivalent to aninstance of MEGAN with a uniform η), we observe that the learned policies’ generated trajectoriesare increasingly similar with those generated by the expert in the sense of ρπ (classical IRL criterion),μ∏ (Generalised IRL criterion) and the cumulative discounted costs (The environment's ground truth).
Figure 2: Performances of MEGAN using a Poisson η distribution: Setting the parameter ofη = Poisson(λ) to a value around the length of the expert’s movement cycle achieved similar/betterperformances than those obtained using a uniform η distribution (Geom(1)). The expert cycle isroughly 10 frames long in the Ant environment, 25 in the Half-Cheetah, and 40 in the Hopper.
Figure 3: Performances of GAIL as we vary the discount factor γ: Neither increasing nor de-creasing the discount factor resulted in improved performances. Agnostic of the used parameter,GAIL was not able to match the expert behavior as well as MEGAN.
