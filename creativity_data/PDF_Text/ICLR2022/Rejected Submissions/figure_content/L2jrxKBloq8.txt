Figure 1: Model Overview a) The encoder transforms the raw state into an internal state represen-tation φt(s). The state representation φt(S) is Used by the decoder, An(∙,at), and ψπ(∙,at). Thedecoder tries to reconstruct the raw input st from the state representation φ)t (s). Λπ and ψπ produceone oUtpUt per action, with the former predicting matrices and the latter predicting vectors. b) Rewardprediction by dotting the cUrrent state φt (s), prodUced by the encoder, and reward weight w.
Figure 2: Environments a) A graphical representation of the Axes environment. The agent, shownas a red square, must traverse to various goal locations marked with the letter "G". The eight goallocations are split between training, shown as blue boxes, and testing, shown as green boxes. b) Arendering of the Reacher task. The agent controls the robotic Sawyer arm to move the end-effector toa 3D point in space. The eight goal locations are shown as balls. Training goals as green, and testgoals as red. c) The map layout of the Doom environment. The agent moves between rooms lookingfor a goal point. d) Images of the Doom environment.
Figure 3: Performance in the Axes and Reacher environments during training and transfer overthe last 1000 steps. Both variants are able to solve the easy environment with essentially equalperformance. In the hard environment the second-order model has higher performance than the linearmodel, and is much closer to the easy variants score.
Figure 4: Performance of the baseline linear vari-ant and our proposed second-order model in theDoom environment.
Figure 5: Visualization of Lambda Function on half-random Axes. a) The half-random variant ofAxes. b) The learned expected future correlation of one feature with itself along A's diagonal isvisualized over the entire state space. The first column is the max value of Λ over the actions. Theremaining columns, from left to right, correspond to each action: left, up, right, and down. Red andblue correspond to maximal and minimal values.
Figure 6: Guided Exploration: The Λ component of the proposed model is used to guide explorationduring transfer. By using Λ the agent explores in directions With large variance in the state space.
