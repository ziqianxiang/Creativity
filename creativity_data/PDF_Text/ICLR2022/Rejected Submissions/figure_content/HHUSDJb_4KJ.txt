Figure 1: Large-scale performance onImageNet-127 with 10% of labelsavailable. The proposed UDAL trainedfor only 1/6 of the epochs of CReST+(Wei et al., 2021) further closes the gapin accuracy to a fully supervised baselineand heavily improves on a lower boundsupervised baseline trained on 10% of thelabels.
Figure 2: Given an off-the-shelf Semi-SuPervised Learning (SSL) learner, previous works addressthe data imbalanced issue on the unsupervised loss by iterative (a) class-rebalancing sampling (Weiet al., 2021) or pseudo-label refinement (Kim et al., 2020). In this work, We propose to tackle theimbalance issue on (b) both supervised and unsupervised losses by directly performing distribution-aligned learning.
Figure 3: A variety of ablations concerning the approach of our method.
