Figure 1: Objective values andfixed-points on the HardAlias-2MDP (defined in Section 5). Thefixed-points of the absolute and Hu-ber objectives are much better prox-ies for the squared value error thanthe fixed-point of the squared Bell-man error. The dotted vertical linesindicate minima of each objective.
Figure 2: Evaluating the quality of the fixed-points ofeach objective function according to the MSVE and MAVEacross several prediction problems. Error is plotted relativeto the best representable value function. The robust lossesare better in the hard aliasing domains, the MHBE is slightlybetter in Outlier, and the MSBE is better on the classicrandom walks. Note: the error for the MSBE is clipped inHardAlias-2 (approx. 25 MAVE).
Figure 3: MSVE averaged over 100 independent trials, for each stepsize in prediction domains. Themean squared algorithms generally performed well across environments—even the adversariallychosen environments—suggesting the difficulty in minimizing the MABE. The Huber algorithmsperformed best across many environments, often displaying less sensitivity to the choice of stepsize.
Figure 4: Subplots show the distribution over 100 random seeds. The performance measure is theaverage return over the last 25% of steps for the best stepsize meta-parameter chosen per-domain.
Figure 5: Learning curves for the best meta-parameter configuration for each domain, averaged over100 random seeds. Shaded regions indicate one standard error. In Acrobot and Cart Pole, QRC-Huberand QRC have similar performance. In Acrobot and Cliff World, DQN and QRC-Huber have similarperformance. However, in Mountain Car and Lunar Lander, QRC-Huber has significantly betterperformance than both competitors. QRC-Huber is the only algorithm to reliably solve all domains.
Figure 6: MAVE averaged over 100 independent trials, for each swept stepsize in key predictiondomains. The mean squared algorithms generally performed well across environments—even theadversarially chosen environments—suggesting the difficulty in minimizing the MABE. The Huberalgorithms performed best across many environments, often displaying less sensitivity to the choiceof stepsize.
Figure 7:	Comparing the mean return over the last 25% of steps across several saddlepoint methodsagainst QRC-Huber. The saddlepoint methods generally perform very poorly, frequently findinga policy only slightly better than the random policy. These results are consistent with the findingsof Ghiassian et al. (2020) and motivate building on gradient-correction methods for nonlinearcontrol. Like QRC-Huber, GQ-Huber uses a twice differentiable estimate of the clip function and allalgorithms use the ADAM optimizer.
Figure 8:	Ablating the impact of the threshold parameter for the Huber loss function for the QRC-Huber algorithm across the benchmark domains. For three of the domains, QRC-Huber is robust tothe choice of threshold parameter with a default value of τ = 1 being a good choice. However, theMountain Car domain shows high-bimodality in performance distribution across multiple randominitializations of the neural network for smaller values of the threshold parameter.
Figure 9:	Ablating the impact of target networks on the performance of the nonlinear controlalgorithms on benchmark domains. The gradient-based methods receive much less benefit from usingtarget networks than DQN, which requires target networks to achieve above random performanceon Cart-pole and to reduce the bimodality of its performance on Mountain Car. Even with targetnetworks, DQN still exhibits large skew and bimodality in its performance distributions, indicatinginstability.
Figure 10:	Comparing algorithms on benchmark control domains with the area under the learningcurve as the performance metric. Unlike Figure 4, early learning is included in the performance metric,giving a sense of the sample complexity of each algorithm. QRC-Huber tends to perform favorablyacross all four domains compared to QRC and DQN, exhibiting much more narrow performancedistributions that are often centered around higher rewards than the competitor algorithms.
Figure 11:	Evaluating the performance of each controlalgorithm on the Minatar suite of games. The learningcurves show the scaled performance metric averagedacross domains with 95% bootstrapped confidence inter-vals about the mean. Because each point in the learningcurve has less underlying structure than the aggregateperformance metric, the confidence intervals are signifi-cantly more pessimistic than reported in Table 1. As such,the sample mean performance of QRC-Huber is slightlyhigher than QRC during early learning, but not statisti-cally significantly so in this result. Both gradient-basedalgorithms considerably outperform DQN with statisticalsignificance, indicating both less domain-sensitivity tometa-parameters as well as better absolute performance.
