Figure 1: Achieved test accuracy over differentsparsity levels of diverse unstructured and struc-tural subnetworks. Sparse models from classicalchannel-wise structural pruning algorithms (Heet al., 2017; Liu et al., 2017; Bartoldson et al.,2019; Molchanov et al., 2019) can not match thefull accuracy of the dense model (dash line).
Figure 2: Overview of our proposals including refilling, refilling+, and regrouping, which turn unstructuredsparse mask into channel-wise and group-wise structured sparse masks.
Figure 3: (Curve plots) Testing accuracy (%) over network sparsity levels (%) on Tiny-ImageNet and Im-ageNet datasets with ResNet-50 (25.56 M). (Radar plots) The end-to-end inference time saving of extremestructural winning tickets. Note that unstructured subnetworks or dense models do not have structural sparsity,and thus they are plotted as dots in the axes of accuracy in the corresponding radar plot. The rightmost plotincludes three extreme regroup tickets with accuracy drop < 1%, where “RG S: x%” indicates unstructuredsparsity before regrouping.
Figure 4: Testing accuracy (%) over network sparsity levels (%) on CIFAR-10/100 with small models Wide-ResNet-32-2 (1.86 M) and MobileNet-v1 (3.21 M).
Figure 5: (Curve plots) Testing accuracy (%) over network sparsity levels (%) on CIFAR-10/100 with largemodels VGG-16 (14.72 M) and RN-18 (11.22 M). (Radar plots) The end-to-end inference time saving ofextreme structural winning tickets. Note that unstructured subnetworks or dense models do not have structuralsparsity, and thus they are plotted as dots in the axes of accuracy in the corresponding radar plot.
Figure 6:	The layer-wise performance of convolution operations in extreme structural winning tickets of(VGG-16, C10). The first six conv. operations are omitted since there is no meaningful speedup, coincidedwith Rumi et al. (2020). Marks like “C: 2.77” indicate the layer-wise compression ratio of IMP-Regroup.
Figure 7:	(Left) Performance of structural tickets grouped from diverse initial unstructured masks. (Middle)Performance of group-wise structural tickets with different weight rewinding. (Right) Performance compar-isons between IMP-Regroup and group-aware IMP as described in Algorithm 4. Testing accuracies (%) overnetwork sparsity levels (%) are reported on (RN-18,C10).
Figure 8: Sparse mask visualizations of theextreme winning tickets from IMP (unstruc-tured), IMP-Refill (channel-wise struc-tural), and IMP-Regroup (group-wisestructural) on (VGG-16,C10). The darkercolor indicates the remaining unpruned ele-ments. (a,b,c) are the last three conv. layers.
Figure A9: Performance of structural tickets refilled by diverse channel picking criterion. Testing accuracies(%) over network sparsity levels (%) are reported on (RN-18,C10).
