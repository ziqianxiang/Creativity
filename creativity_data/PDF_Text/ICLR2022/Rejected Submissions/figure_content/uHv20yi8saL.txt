Figure 1: Trust region with respect to the clipping range, the number of agents and the numberof optimization epochs: (a) Ratio ranges for 5 agents with the number of optimization epochs; (b)Cumulative percentage of decentralized trust region as the clipping value varies; (c) Cumulativepercentage of centralized trust region as the number of agents varies (clipping at 0.1); and (d)Cumulative percentage of centralized trust region with optimization epochs (clipping at 0.1).
Figure 2: Empirical returns and trust region estimates for independent ratios clipping.
Figure 3: Contrasting IPPO and MAPPO across different maps.
Figure 4:	Test battle win mean of IPPO on maps with varied difficulty and numbers of agentsagents) and 27m_vs_30m (27 agents). However, when the clip value is too small, e.g., = 0.01 inmaps with 5 and 8 agents, the resultant trust region is also small and the update step in each iterationcan thus be too small to improve the policy. Thus, one would need to trade off between the trustregion constraint, to ensure monotonic improvement, and the policy update step, to ensure a sufficientparameter update at each iteration.
Figure 5:	Empirical test battle win mean (first row), test returns (second row) and trust regionestimates (third row) of MAPPO on maps with varied difficult and numbers of agents17Under review as a conference paper at ICLR 2022I Γ> IAilnJMVMerIonl_S_IIES5β?SJCSS-EU.
Figure 6:	Empirical returns, trust region estimates and test battle win rate for small values ofindependent ratios clipping.
Figure 7:	Joint divergence estimates and empirical returns for two types of ratio clipping at differentclipping values: 0.1 (first row), 0.3 (first row) and 0.5 (first row).
Figure 8:row), 0.3Test battle win rate for two types of ratio(first row) and 0.5 (first row).
