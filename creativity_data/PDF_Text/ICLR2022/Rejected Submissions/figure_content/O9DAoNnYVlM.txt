Figure 1: Example of a single-layer quantizedneural network with a latent weight vector h ∈Rd. h is normalized to generate we ∈ (-1, 1)d,and the output is y = σ (W>x). A binaryweight wq can be obtained by thresholding orrounding We to the discrete space D2d .
Figure 2: One round of FedVote is composed offour steps. Each worker first updates the local(k,τ)model and then sends the quantized weight Wm ,to the server. Later, the server calculates the vot-ing statistics and sends back the soft voting resultsp(k+1) to each worker.
Figure 3: Histograms of (a) model updates δm(k,)iand (b) binary weight probabilities πm(k,,iτ). Wetrained a LeNet on MNIST for a single commu-nication round.
Figure 4: Learning curves of different methods on CIFAR-10 with (a) the i.i.d. setting and (b) theDirichlet non-i.i.d. setting. Compared with the gradient quantization methods such as signSGD(Bernstein et al., 2018) and the model update quantization methods such as FedPAQ (Reisizadehet al., 2020), FedVote achieves higher accuracy given the same number of communication rounds.
Figure 5: Test accuracy versus communicationround of different methods on the non-i.i.d. CI-FAR dataset (α = 0.5). We use 100 workersand sample 20 of them in each round.
Figure 6: Test accuracy versus the number ofByzantine workers. As the number of adver-saries increases, the test accuracy of FedVotedrops rapidly.
