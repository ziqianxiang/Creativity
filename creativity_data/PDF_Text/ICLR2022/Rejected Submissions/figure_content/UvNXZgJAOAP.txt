Figure 1: (a) An outline of the attention mechanism. A query vector ht-1 is compared with an inputsequence X to figure out where to look via a multilayer perceptron (see the grey box), leading toa set of weights αt . A context vector ct is then computed for alignment by a function φ, whereattention mechanisms generally differ. Soft attention computes ct as a weighted average of X whilehard attention does it by randomly sampling an element from X. (b) Poor target representation inattention mechanism. Each element of X only represents a fixed region defined by the receptive field(see yellow boxes). It is hard to accurately represent the target without the distraction of redundant ormissing parts (see letters ‘f’ and ‘W’ for examples).
Figure 2: A generic Seq2Seq learning architecture with sharp attention. Given an input image, abackbone network breaks down it into a set of regions and extracts features from each region, leadingto a sequence of feature vectors X. An attention mechanism (see Fig. 1(a)) uses X and a hidden statevector ht-1 to compute a categorical distribution αt, based on which a region is randomly chosenand fed into a sharpener module to compute the context vector ct for clear alignment. A recurrentneural network (RNN) takes in ht-1, ct and yt-1 (the token at previous time step) to update itsinternal state, and then outputs current ht for token prediction and next iteration (dashed line).
Figure 3:	From top to bottom, we show examples of the created handwritten digit images andtranscriptions (row 2), and patches for context vector computation as well as predicted digits (failuresshown in red) at each time step (rows 3-5). Note that the patches are the receptive fields for the hardmodel and the output of the STN for the pooling based sharp models. The reference images are givenbelow the patches in the last row.
Figure 4:	Examples of recognition results for scene text recognition. All real-world testing imagesare shown as is without rescaling and grey scale conversion. We leverage the chain based sharp modelto generate the patches localised by the sharpener and predicted tokens (failures highlighted in red)across time steps. The results from other sharp models look similar to what has been shown here.
