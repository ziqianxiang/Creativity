Figure 1: LSH counts are sufficient for learning regression lines and classification hyperplanes. Here,we show the partitions that correspond to several STORM repetitions (or rows). Since the regressionline should lie inside a densely-populated region, we use sparsely-populated partitions (shown inbrown) to identify regions that contain the normal to the regression line (regression hyper-plane inhigher dimensions). Each STORM repetition prunes the space of eligible models, until we are leftwith the small feasible sparse region (shown in orange). We gain the most information when thepartition boundaries are orthogonal, which is nearly always true in high dimensions.
Figure 2: To construct a STORM sketch, we initialize a 2D array with R rows and B columns(buckets) in each row. The asymmetric LSH function li indexes row i of the sketch. We want bestmodel to hash into least count buckets. See Sections 3.3 for details.
Figure 3: Left: Shows the algorithm for optimization on the extracted labels of least count bins.
Figure 4: (a) PRP Surrogate loss for different values of p. It is convex, but less sharp than the L2loss. A key observation is that p = 4 produces the highest convexity among the family of PRPsurrogate losses. (b)Plot of slope surrogate loss for different values of p at h[Î¸, -1], [x, y]i = 0.1.
Figure 5: First 7 plots and last row represents MSE vs memory budget for ridge regression. The shadedregion represents one standard deviation. The baselines are vanilla least-square ridge regression(Least Sq. Ridge) using SGD, mean of the training set labels (Mean), Random sampling ridgeregression (RS - Ridge), and Frequent direction streaming sketch (FD Ridge) (Shi & Phillips, 2021).
Figure 6: Left: Surrogate regression loss (STORM regression loss) for different values of p. It has thehighest convexity near p = 4. Right two: Mean square error with varying sketch size with different pon two real world datasets. We observe that p=3,4 works the best.
Figure 7: Comparison of various classification-calibrated loss functions, including the STORMclassification loss.
Figure 8: (a) Regression with linear model. (b) regression with linear model and Random Fourierfeature Rahimi & Recht (2008) mapping, and (c) classification STORM losses on synthetic 2Ddatasets.
Figure 9: The random Fourier feature maps the input dimension to a higher dimension for linearmodel training. Here we map it to a 10 dimension space and project back the solution in 2D. Thefigure shows the learnt model with different STORM sketch sizes. On the given synthetic data,STORM achieves a compression of 1/256 times to 1/6660 times.
Figure 10: Left: Shows the algorithm for zero order gradient descent. Right: The compressed PRPsketch (which uses Hyperplane based gradient descent) is Rp bits in size. And on a experiment withreal data we found out that it can give acceptable model with a very less sketch size.
Figure 11: Here, we report the mean square error for our method without any regularization, whencompared with baselines at a variety of memory budgets.
