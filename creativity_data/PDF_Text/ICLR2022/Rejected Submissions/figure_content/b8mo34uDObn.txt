Figure 1: Instead of a zero-sum tradeoffbetween the ID and OOD performance oflightweight and full finetuning, ensemblingand cocktail finetuning (purple square) ap-proximately achieve the best performance ofeach, both ID and OOD.
Figure 2: Examples in the open-domain closed-book QA (wherein no text is given from which toextract the answer). The full finetuning model performs well in-distribution but not out-of-distribution,the lightweight finetuning model performs well out-of-distribution but not in-distribution, and thecocktail finetuning model performs well on both. The distribution shift (Natural Questions toWebQuestions) is due to temporal and data collection differences.
Figure 4: Ablation on XSUM in which a fullfinetuning model is distilled into a full finetuningmodel. No benefits are observed.
Figure 3: For WebNLG and XSUM, ensembles and cocktails both achieve the best of both prefix-tuning and full finetuning for some λ; for OpenQA, one can achieve approximately the best ofboth.
Figure 5: For WebNLG and XSUM, Ensembles and cocktails both achieve the best both prefix andfull finetuning for some λ; for OpenQA, one can achieve approximately the best of both.
