Figure 1: (a) Depiction of our method for learning a linear subspace of networks ω* parameterizedby α ∈ [α1, α2]. When compressing with compression function f and compression level γ, we ob-tain a spectrum of networks which demonstrate an efficiency-accuracy trade-off. (b) Our algorithm.
Figure 2: Analysis of observed batch-wise means μ and stored BatchNorm means μ during testingfor models trained with TopK unstructured sparsity. The models are trained with different targetsparsities and evaluated with various inference-time sparsities. (a)-(b): The distribution of ∣μ - μ^|across all layers. (c)-(d): The average value of ∣μ - μ^| for individual layers. (e)-(f): The correlationbetween the average of ∣μ - μ^| and test set error. Note that in (b) and (d), sparsities of 0 and 0.493produce near-identical results, thus those curves are overlapping.
Figure 3: Our method for structured sparsity using a linear subspace (LCS+L+IN) and a point sub-space (LCS+P+IN) compared to Universal Slimming (US) (Yu & Huang, 2019), Network Slimming(NS) (Yu et al., 2018), and Learning Efficient Convolutions (LEC) (Liu et al., 2017). LEC does notprovide an open-source implementation of their method for ResNet18, so we omit it. We do notallow fine-tuning or recalibration.
Figure 4: Our method for unstructured sparsity using a linear subspace (LCS+L+GN) and a pointsubspace (LCS+P+GN) compared to networks trained for a particular TopK target. The TopK targetrefers to the fraction of weights that remain unpruned during training.
Figure 5: Our method for quantization using a linear subspace (LCS+L+GN) and a point subspace(LCS+P+GN) compared to networks trained for a particular bit width target.
Figure 6: Standard evaluation of a linear subspace with network f (ω* (α), γ(α)) (Learned line), andevaluation when evaluating with reversed compression levels, f (ω*(α), γ(1 - α)) (Reversed line).
Figure 7:	Analysis of the mean absolute difference between observed batch-wise means μ and storedBatchNorm means μ during testing for CPreReSNet models trained with NS (YU et al., 2018) or US(YU & Huang, 2019). (a)-(b): The distribution of ∣μ - μ^| across all layers. (c)-(d): The averagevalue of ∣μ - μ| for each individual BatchNorm layer. (e)-(f): The correlation between the averageof ∣μ - μ| and test set error.
Figure 8:	Analysis of the mean absolute difference between observed batch-wise means μ and storedBatchNorm means μ during testing for CPreReSNet models trained with different quantization bitwidths. (a)-(b): The distribution of ∣μ - μ| across all layers. (c)-(d): The average value of ∣μ - μ^|for each individual BatchNorm layer. (e)-(f): The correlation between the average of ∣μ - μ^| andtest set error.
Figure 9:	Our method for structured sparsity using a linear subspace (LCS+L+IN) and a point sub-space (LCS+P+IN), compared to Universal Slimming (US) (Yu & Huang, 2019), Network Slimming(NS) (Yu et al., 2018), and Learning Efficient Convolutions (LEC) (Liu et al., 2017).
Figure 10:	Our method for quantization using a linear subspace (LCS+L+GN) and a point subspace(LCS+P+GN) compared to networks trained for a particular bit width target.
Figure 11: Our method for unstructured sparsity using a linear subspace (LCS+L+GN) and a pointsubspace (LCS+P+GN) compared to networks trained for a particular TopK target. The TopK targetrefers to the fraction of weights that remain unpruned during training.
