Figure 1: A data example from CONTRAQA, where contradicting information is in bold.
Figure 2: Overview of the BART-FG model, illustrated by an example sentence.
Figure 3: The QA performance for RoBERTa-largemodel with different number of fake contexts N.
Figure 4: Error analysis, showingthe QA model tends to be deceivedby the fake context generated bywhich method?threat to current QA systems. As QA models are not trained to differentiate fake contexts, they canbe easily mislead by misinformation.
Figure 5: QA performance When applying the fake con-text detector trained with different amount of data.
Figure 6: Independent evalua-tion of the detector accuracy forfake contexts generated by differ-ent methods and the benefits to QAmodels. The second to fourth rowsdenote the BART-FG model withdifferent parameter K.
Figure 7:	The Contra-QA setting (a) and the Contra-QA w/Detector setting (b).
Figure 8:	The annotation interface in the Amazon Mechanical Turk.
