Figure 1: Logistic Regression Probes allow the practitioner to observe the evolution of the interme-diate solution quality from layer to layer. The probe performance at each layer of VGG16 trainedon Cifar10. The performance is increasing layer by layer, indicating that the problem is solvedincrementally and that the inference process is evenly distributed among layers.
Figure 2: The images show the basic neural architecture with skip connection (a) and the probeperformances of each layer (b). The setup is designed to provoke the network to ”skip” the 128 layersby learning some identity-mapping analog. By observing the probe performances, this behavior canbe observed. After the initial layer, the performance degrades until reaching chance level. The probeperformances recover as soon as the skip connection is added to the layers again. We will observesimilar behavior on convolutional neural networks over the course of this work.
Figure 3:	Depending on the neural architecture, tail patterns may deviate in their appearance in probeperformance. In sequential architectures (a) the layers maintain the quality of the intermediate so-lution. If shortcut connections exist in the architecture, layers may be skipped. Skipped layers areapparent by their decaying probe performance Alain et al. (2020). This is apparent on DenseNet18(b) and ResNet34 (b) where a single DenseBlock and multiple ResidualBlocks are skipped respec-tively. All models are trained on Cifar10 at native resolution.
Figure 4:	The architecture used for the experiments in this section. Each stage consists of k blocksand has double the filters of the previous stage. The first layer of each stage is a downsampling layerwith a stride size of 2. Each building block consists of two pathways, which resemble sequences ofconvolutional layers.
Figure 5: When training MPNet with pathways of different depth, a deeper pathway is skipped for abuilding block (a), which is apparent by the deteriorating probe performance. This effect occurs onall building blocks when the depth is increased. The models are trained on Cifar10.
Figure 6: Different from tail patterns, the shallower pathway is dominating the deeper pathway evenif their receptive field sizes are identical (a). We attribute the dominating-behavior to a vanishinggradient in the deeper pathway, evident in the overall lower size of the gradients (b).
Figure 7: The MPNet architectures with only slightly different pathways in each building blockshow a coexisting behavior, which is indicated by the probe performances of the 3 × 3-pathwaypl,min and the 7 × 7 pathway pl,max increasing at a similar rate.
Figure 8:	Comparison of the representational similarity score given by the Centered Kernel Align-ment (Cortes et al., 2012; Kornblith et al., 2019) and the probe performance of MPNet architectureswith different sets of pathways in each building block. The CKA value is calculated to quantify eachlayer’s representational similarity with the final output of the network. The min and max subscriptrefer to the path with the smaller and larger kernel sizes. The CKA values are in accordance with theprobe performance, indicating that the shorter paths and the longer paths are extracting more similarfeatures in early layers of the network when compared to later layers.
Figure 9:	MPNet2(8 : 3 × 3, 8 : 3 × 3) trained on ImageNette dataset showing the correlation among8 layers deep parallel pathways. We observe that layers of the two pathways have high redundancywith their respective counterparts in the other pathway, indicating a parameter inefficiency.
