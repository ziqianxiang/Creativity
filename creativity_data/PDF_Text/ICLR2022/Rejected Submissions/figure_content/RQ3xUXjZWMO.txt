Figure 1: [Eigenvalues of the logit Hessian] Graph of the secular function v(λ) in Eq (9) whichhas zeros at the eigenvalues {λ(i)}iC=1 of M = V2zl (blue curves). We highlighted the singularitiesλ = P(i) with red vertical lines. It illustrates Theorem 1 (a) and (c).
Figure 2: [Evolution of Impurity] The impurity λ(1) increases and then decreases as P(1)increases during training. Left: The impurity of a typical example is plotted against P(1). Wetogether plot the upper bound min{p(i), Gini(P⑴)} and the lower bound 2Gini(P⑴)from Theorem1 (c) and (d). Right: The impurity is plotted against the training step. Blue curve indicates its meanvalue hλ(1)i and sky-blue area shows the 25-75% quantile range of the impurity for the training data.
Figure 3: The sharpness kHkσ and the Jacobian norm khJ ik2 show similar oscillating behavior upto a factor * which is locally constant and slowly changes during training (CIFAR-10, η = 0.04,|B| = 128; left: 0-200, middle: 1000-1200, right: 1500-1700 steps). We highlighted ∣∣H∣∣σ = 2/nwith the dashed horizontal line. Note that they have different right y-axes.
Figure 4:	Oscillatory catapult in the optimization trajectory {θ(t) } (from blue to red) of full-batch GD (McInnes et al., 2018). Left: UMAP of the model parameters trained on CIFAR-10 forthe first 500 steps. Right: Zoom-in for the oscillatory steps [100, 300]. After few steps (〜100), thesharpness reaches a threshold and the iterate shows the oscillatory behavior with iterative catapult.
Figure 5:	Three phases of Implicit Jacobian Regularization (IJR). It shows the IJR effects in theActive Regularization Period (II).
Figure 6:	The IJR effects vary depending on the hyperparameters used in the training. Theplateau of the sharpness and the n-shaped evolution of the impurity are clearly observed as expected.
Figure 7: The effectiveness of EJR (dashed lines) compared to Baseline (solid lines) during training(Left: CIFAR-10, Right: CIFAR-100) on WRN-28-10. With EJR, it mitigates the overfitting, especiallyafter each learning rate decay (undesirable decrease/increase of test accuracy/loss).
Figure 8: HeatmaP of the matrix QΛ1/2 = [√λ(1)q(1);一；√λ(C-1)q(c-1); 0] averaged over thetraining set D where M = QΛQT. Each column of QΛ1/2 visualizes the color-encoded directionof q(i) multiplied by √λ(i). We highlighted the elements q(；) and q(；+i)with the dashed boxes for0 ≤ i ≤ C - 1 (see Theorem 1 (b)).
Figure 9: There are C clusters of m = vzλ(I)Jq(I) according to the most likely class (not thetrue class). Left: (Within-class similarity) Directional data of m from Di and Dj for the twoclasses, dog (blue) and automobile (red). They are projected onto the 2D-plane spanned by the twomean vectors indicated with the arrows. We highlight the MRL ρ for each class. The directionaldata of m are concentrated within the class but separated from each other. Right: (Between-classdissimilarity) Cosine similarities between each pair of {mi}. They are mostly orthogonal, but somepairs are even negatively aligned, for example, automobile and truck. This is because the examplespredicted to be automobile mostly have the second most probable class as truck.
Figure 10: Left: Total gradient gD = hgi is aligned with the top eigenvector r of the HessianH at each step during training (Jastrzebski et al., 2019; Gur-Ari et al., 2018). They have largecosine similarities considering that they are very high-dimensional. We highlighted the cosine valuefor random m-dimensional vectors in Θ with the dashed horizontal line (about 1e-3). Middle: J q(1)(or m) is highly aligned with the gradient g for given example at each step during training.
Figure 11: Histograms of PJkJq(I)\/。and cos2(Jq(i), Jq(j)).
Figure 12:	(Left to Right): the logit norm hkz(θ(t))ki, the absolute value of the logit sumh∣1Tz(θ(t))∣i, the weight norm θ(t), the distance from the initial weight ∣∣θ(t) - θ(O)k duringtraining (every 10 steps). See together with Figure 6 (Left, solid red line) and Figure 5.
Figure 13:	(Left to Right): the logit norm h∣z(θ(t))∣i, the absolute value of the logit sumh∣iτz(θ(t))∣i, the weight norm ∣∣θ(t)∣, the distance from the initial weight ∣∣θ(t) - θ(O)Il inthe early phase of training (every step).
Figure 14:	The layerwise weight norms (6 layers from left to right and from top to bottom) ofthe 6CNN model in the early phase of training (every 10 step).
Figure 15: The relation ∣∣H∣∣σ ɑ hλ⑴〉k〈J川|2 on the CIFAR-10 dataset and the 6CNN modeltrained using GD with different learning rates η = 0.02/0.04/0.08 (from left to right). Bottomfigures are plotted for the early phase of training. Top figure is plotted for η = 0.02. Curves areplotted for every step.
Figure 16:	The relation ∣∣H∣∣σ ɑ hλ(1)ikhJ日||2 on the CIFAR-10 dataset and the VGG modeltrained using GD with different learning rates η = 0.04/0.08 (left/right) in the early phase oftraining. Curves are plotted for every step (Top: 0-1000 stpes and Bottom: 500-1000 stpes).
Figure 17:	The relation ∣∣H ∣∣σ 8 hλ(1)i∣hJ 川|2 on the CIFAR-10 dataset and the ResNet modeltrained using SGD with learning rate η = 0.04 and batch sizes |B(t) | = 128 during training(0-1000 steps). Curves are plotted for every step.
Figure 18: The relation ∣∣H∣∣σ H hλ(1)ikhJ川|2 on the CIFAR-10 dataset and the 6CNNmodel trained using SGD with fixed learning rate η = 0.04 and different batch sizes |B(t) | =512/128/32 (from left to right) in the initial phase (0-500 steps). Note that the proportionality con-stant may change according to the batch size (the smaller the batch size, the larger the proportionalityconstant). Curves are plotted for every step.
Figure 19: The relation ∣∣H∣∣σ h hλ(1)ik(J日||2 on the CIFAR-10 dataset and the VGG modeltrained using SGD with fixed learning rate η = 0.04 and different batch sizes |B(t) | =512/128/32 (from left to right) during training (0-100 epochs). Curves are plotted for everyn steps (n = 97/388/1552) where 97 = b50000/512c.
Figure 20: The relation ∣∣H∣∣σ ɑ hλ(1)ikhJ^∣∣2 on the CIFAR-100 dataset and the VGGmodel trained using SGD with fixed learning rate η = 0.1 and different batch sizes |B(t) | =128/64/32 (from left to right) in the early phase of training. Note that the proportionality constantmay change according to the batch size (the smaller the batch size, the larger the proportionalityconstant). Curves are plotted for every step (Top: 0-1000 stpes and Bottom: 500-1000 stpes).
Figure 21:	The relation ∣∣H∣∣σ <x (λ⑴i∣(Ji^∣2 on the MNISTdataSetandthe 3FCN-MNISTmodel trained using GD with different learning rates η = 0.02/0.04/0.08 (from left to right)in the early phase of training. Note that the sharpness did not reach the limit 2/n (the dashedhorizontal line). Curves are plotted for every step.
Figure 22:	The evolution of ∣∣H∣∣σ, k〈J4∣∣2, and hλ(1)i on the CIFAR-10 dataset and the VGGmodel trained using GD with the learning rates η = 0.04/0.08 (solid/dotted lines). See theFigure 6 caption together.
Figure 23: The evolution of ∣∣H ∣∣σ, kh J iiLk2, and hλ(1)i on the CIFAR-10 dataset and the 6CNNmodel trained using GD with the learning rates η = 0.02/0.04/0.08 (solid/dashed/dotted lines).
Figure 24: The evolution of ∣∣H∣∣σ, k〈J4∣∣2, and hλ(1)i on the CIFAR-10 dataset and the VGGmodel trained using SGD with the fixed learning rate η = 0.04 and the batch sizes |B(t) | =50000(GD)/512/32 (solid/dashed/dotted lines). Training with a batch size 512 shows similarevolutions to the GD training. See the Figure 6 caption together.
Figure 25: The evolution of ∣∣H ∣∣σ, k〈J〉i||2,and hλ(1)i on the CIFAR-100 dataset and the VGGmodel trained using SGD with the fixed learning rate η = 0.1 and the batch sizes |B(t) | =128/64/32 (solid/dashed/dotted lines). See the Figure 6 caption together.
Figure 26: Visualization of the optimization trajectory using UMAP. (Top) UMAP on the CIFAR-10 dataset trained with 6CNN for the first 500 steps, (Middle) on the CIFAR-10 dataset trained withVGG for the first 1000 steps, and (Bottom) on the MNIST dataset trained with 3FCN-MNIST for thefirst 1000 steps (from red to blue).
Figure 27:	Evolution of QΛ1/2 for some training steps during the training. They are visualizedfor 100/500/1000/2000/4000/6000 steps from left to right and from top to bottom.
Figure 28:	Cosine similarities of {mi}= during training. They are visualized forstep=500/1000/1500/2000/3000/4000 from left to right and from top to bottom. See the Figure9 caption.
Figure 29:	The Mean Resultant Lengths (MRL) ρ for each Di . Note that ρ is not defined for firstfew steps because some Di are empty.
Figure 30:	Directional data of m from Di and Dj. They are visualized for (i, j) = (airplane,ship)/(automobile, truck)/(dog, cat)/(deer, bird) from left to right. see the Figure 9 caption.
Figure 31: (Left) Alignment between the two vectors gD and r, and (Right) alignment of gD, rin the subspace S = span({mi}C=J using VGG with η = 0.08. See the Figure 10 caption. Theyare plotted for every 25 steps. Note that AS is not defined for first few steps (about 0-800 steps)because some Di are empty.
Figure 32:	The sharpness ∣H ∣σ and the Jacobian norm ∣J ∣2 during training with the MSE loss34Under review as a conference paper at ICLR 2022N Details of Explicit Jacobian Regularization (EJR)We first propose a simple form of EJR with the regularized loss as follows:L(θ) = L(θ) + λreg khJ ikF/C	(36)and update the model parameter asθ(HI)= θ⑴-η(Vθ L(θ㈤)+ λreg Vθ khJ ikF/C)	(37)However, this requires to build a computational graph for khJ ik2F which is inefficient for a largenetwork (e.g. WRN-28-10).
Figure 33: [Explicit Jacobian Regularization] The explicit Jacobian regularization enhances thetest accuracy. We plot the test accuracy for different learning rates η and regularization coefficientsλreg. The models are trained with batch size of |B| = 128 on CIFAR-10.
