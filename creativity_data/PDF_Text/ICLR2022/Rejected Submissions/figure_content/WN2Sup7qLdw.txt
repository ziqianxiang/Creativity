Figure 1: The architecture of our Multi-Resolution Continuous Normalizing Flow (MRCNF) method (bestviewed in color). Continuous normalizing flows (CNFs) gs are used to generate images xs from noise zsat each resolution, with those at finer resolutions conditioned (dashed lines) on the coarser image one levelabove xs+1, except at the coarsest level where it is unconditional. Every finer CNF produces an intermediateimage ys, which is then combined with the immediate coarser image xs+1 using a linear map M to form xs.
Figure 2: Tetrahedron in 3D space with 4cornersx1x2x3x41-21-21-21-21-21-21-21-21-21-21-21-4=- -- 23ggyrr■ ■- -- 23guy"-11111-21-21-21-2
Figure 4: Histogram of log likelihood per dimension of out-of-distribution datasets (TinyImageNet, SVHN, Constant) under(MR)CNF models trained on CIFAR10. As with other likelihood-based generative models such as Glow & PixelCNN, OoD datasetshave higher likelihood under (MR)CNFs.
Figure 5: (a) Example of shuffling different-sized patches of a 32×32 image: (left to right, top to bottom) 1×1, 2×2, 4×4,8×8, 16×16, 32×32 (unshuffled) (b) Bits-per-dim vs Epoch at each resolution for different MRCNF models trained onCIFAR10.
Figure 6: Generated samples from MNIST.
Figure 7: Generated samples from CIFAR10.
