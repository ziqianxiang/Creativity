Figure 1: Test accuracy achieved by IMP in relation to the total number of epochs required forretraining using either FT or SLR. For each point on the x-axis, we show the highest mean accuracyachieved by any configuration of a grid search using up to this many total retraining epochs.
Figure 2: Performance-vs.-sparsity tradeoffs for the One Shot setting on CIFAR-100 (top) and Im-ageNet (bottom). We compare the sparsity allocation methods w.r.t. the different retraining tech-niques FT (left) and SLR (right).
Figure 3: Test accuracy achieved by IMP in relation to the total number of epochs required forretraining using either FT or SLR. Each row of the plots shows a different weight decay as indicatedin the title. The pruned baselines were established by using the approach of Renda et al. (2020)using the respective weight decay and either in its original form, using LRW, or with a modifiedversion using FT..
Figure 4: Sparsity-vs.-performance tradeoffs for ResNet-56 on CIFAR-10 for IMP in the One Shotsetting for FT (left) and SLR (right) as retraining methods. The plot includes max-min confidenceintervals.
Figure 5: Sparsity-vs.-performance tradeoffs for WideResNet on CIFAR-100 for IMP in the OneShot setting for FT (left) and SLR (right) as retraining methods. The plot includes max-min confi-dence intervals.
Figure 6: Sparsity-vs.-performance tradeoffs for ResNet-50 on ImageNet for IMP in the One Shotsetting for FT (left) and SLR (right) as retraining methods.
Figure 7: Performance-vs.-theoretical speedup tradeoffs for ResNet-56, WideResNet and ResNet-50on the respective datasets. All plots depict the one shot setting.
Figure 8: Performance-vs.-sparsity and performance-vs.-theoretical-speedup tradeoffs for VGG-16on CIFAR-10 for IMP in the One Shot (above) and Iterative (below) setting for FT (left) and SLR(right) as retraining methods. In the One Shot setting the model is retrained for 30 epochs afterpruning and the iterative setting consists of 3 prune-retrain cycles with 10 epochs each. For OneShot we observe layer-collapse while the iterative splitting into less severe pruning steps avoids theproblem. Note that the total amount of retraining epochs between the two settings is identical here.
Figure 9: Increase in accuracy after retraining (above) as well as pruning stability (below) forResNet-56 trained on CIFAR-10 using different pruning stable methods. Each method was retrainedfor 30 epochs using FT. Each datapoint corresponds to the hyperparameter config with highest ac-curacy directly after pruning when considering .5% sparsity intervals between 88% and 100%. Theconfidence bands indicate the min-max-deviation around the mean with respect to different initial-ization seeds.
Figure 10: Test accuracy (above) and theoretical speedup (below) of IMP in comparison to differentpruning stable methods when training a ResNet-56 network on CIFAR-10. All methods were trainedto achieve sparsity levels of 90%, 93%, 95%, 98%, 99% and 99.5% with the exception of CS, STRand DST, where additional hyperparameter searches were necessary to obtain the curves shown here.
Figure 11: Increase in accuracy after retraining (above) as well as pruning stability (below) forWideResNet trained on CIFAR-100 using different pruning stable methods. Each method was re-trained for 30 epochs using FT. Each datapoint corresponds to the hyperparameter config with high-est accuracy directly after pruning when considering .5% sparsity intervals between 88% and 100%.
Figure 12: Test accuracy (above) and theoretical speedup (below) of IMP in comparison to differentpruning stable methods when training a WideResNet network on CIFAR-100. All methods weretrained to achieve sparsity levels of 90%, 93%, 95%, 98%, 99% and 99.5% with the exception ofCS, STR and DST, where additional hyperparameter searches were necessary to obtain the curvesshown here. Each datapoint corresponds to the hyperparameter config with highest accuracy whenconsidering .5% sparsity intervals between 88% and 100%. This holds similarly for the theoreticalspeedup, where points are selected by highest accuracy as well. The confidence bands indicate themin-max-deviation around the mean with respect to different initialization seeds.
Figure 13: Test accuracy achieved by IMP in relation to the total number of epochs required for re-training using either CLR or SLR. Each row of the plots shows a different weight decay as indicatedin the title. The pruned baselines were established by using the approach of Renda et al. (2020)using the respective weight decay and either in its original form, using LRW, or with a modifiedversion using FT.
Figure 14: Performance-vs.-sparsity (above) and performance-vs.-theoretical-speedup (below)tradeoffs for ResNet-56 on CIFAR-10 for IMP in the One Shot setting with CLR as the retrain-ing method. Each line corresponds to one pruning selection approach of interest. The model isretrained for 30 epochs after pruning.
Figure 15: Performance-vs.-sparsity (above) and performance-vs.-theoretical-speedup (below)tradeoffs for WideResNet on CIFAR-100 for IMP in the One Shot setting with CLR as the retrainingmethod.E ach line corresponds to one pruning selection approach of interest. The model is retrainedfor 30 epochs after pruning.
