Figure 1: Comparison of learning framework in terms of augmentation and architecture. BothSupCon (Khosla et al., 2020) and SelfCon use all the samples of the same ground-truth label as thepositive pairs. In all three methods, every output can be an anchor feature. Specifically, in (b) and(c), an anchor from the backbone network contrasts other features from the backbone, as well as thefeatures from the sub-network. Best seen in color.
Figure 2: Test accuracy and the estimated mu-tual information of different methods. Mutualinformation estimators are measured between theintermediate and the last features.
Figure 3: Train accuracy and test accuracy of ResNet-18 for different views and loss functions.
Figure 4: CIFAR-100 accuracyat different training epochs.
Figure 5: Gradient norm of each ResNet-18 block and con-volutional layer. We computed gradients from the SupCon loss(Left) and SelfCon-M loss (Right), both from the same initial-ized model. All convolution layers in the block are named byorder.
Figure 6: Visualizations for the feature-level multi-view generated by the sub-network. Alongwith the original image, each map visualizes the gradients from the sub-network (Left) and thebackbone network (Right), respectively. We measured the gradient of the pretrained ResNet-18 withSelfCon-S loss.
Figure 7: Qualitative examples for mitigating vanishing gradient. Along with the original image,we visualized the gradient when training with SupCon (Left) and SelfCon-M loss (Right). Note thatall the gradients are from the same model checkpoint of ResNet-18.
Figure 8: Test accuracy and the estimated mutual information of different methods. SelfCon-M*Î± denotes SelfCon-M* loss with hyperparameter a. We used ResNet-18 on CIFAR-100 datasetfor the measurements.
