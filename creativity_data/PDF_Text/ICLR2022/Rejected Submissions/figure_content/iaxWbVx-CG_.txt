Figure 1: HCCL’s framework. HCCL minimizes the cross contrastive loss between the differentlevel output of hierarchical projection head. The encoder parameters and projection parameters ofthe two branches are shared. ’=’ denotes stop gradient.
Figure 2: Comparison of different projection architectures. The projection head includes alllayers that can be shared between both branches. ’=’ denotes stop gradient.
Figure 3: Differential equation trajectoriesThe SGD iterations are thenW(t+1) - Wf) = - ηw Vwi Lw'+I)- Wy) = - ηw VW 2LP1(t+1) - P1(t) =-ηpVP1LP2(t+1) - P2(t) =-ηpVP2LClearly, the above SGD iterations correspond to highly non-linear differential equation systemsin high dimensional space. If we do not make further simplifications, there is little we can doanalytically. To get a deeper insight , we consider a special case where all matrices are diagonal ones.
Figure 4: Different way of cross-correlation on 3 level hierarchical projection head. ’=’ denotesstop gradient.
Figure 5: Cosine distance distribution of view1 and view2 features. (a): SimSiam’s projectorfeatures. (b) HCCL’s low level projector features. (c) HCCL’s low level and hightlevel projectorfeatures. (d) HCCL’s high level projector features.
Figure 6: Average cosine distance within and between classes. (a): Average cosine distance offeatures in each class. (b): Average cosine distance of features between different class.
