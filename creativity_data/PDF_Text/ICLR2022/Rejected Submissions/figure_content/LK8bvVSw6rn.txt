Figure 1: A comparison of 484 models by their AUROC (×100, higher is better) and -log(ECE)(higher is better) on ImageNet. Each marker’s size is determined by the model’s number of param-eters. A full version graph is given in Figure 8. Distilled models are better than non-distilled ones.
Figure 2: A comparison of RC-curves made by the best (ViT-L/16-384) and worst (EfficientNet-V2-XL) models we evaluated in terms of AUROC. Comparing ViT-B/32-SAM to EfficientNet-V2exemplifies the fact that neither accuracy nor AURC reflect selective performance well enough.
Figure 3: A comparison of different methods and their AUROC improvement relative to the samemodel’s performance without employing the method. Markers above the x axis represent modelsthat benefited from the evaluated method, and vice versa.
Figure 4: A comparison of different methods and their ECE improvement relative to the samemodel’s performance without employing the method. Markers above the x axis represent modelsthat benefited from the evaluated method, and vice versa. Temperature scaling can sometimes harmECE, although its purpose is to improve it.
Figure 5: Comparison of models by their overall accuracy and the coverage they are able to providea selective accuracy constraint of Top-1 99% on ImageNet. A higher coverage is better. Only ViTmodels are able to provide coverage beyond 30% for this constraint. They provide more coveragethan any other model compared to their accuracy or size.
Figure 6: OOD performance across 11 severity levels. Note how the detection performance de-creases for all models as we increase the difficulty until it reaches near chance detection performanceat the last severity (s10). The top curve belongs to ViT-L/16-384, which beats all models at everyseverity level. We also observe how the previous C-OOD benchmark, ImageNet-O does not reflectthe true OOD performance of the models, since it was designed to specifically fool ResNet-50, andso it is more difficult for models similar to ResNet-50 than other models.
Figure 7: OOD detection performance in severity 5 vs. In-Distribution uncertainty estimation per-formance. We notice that the best performing network in one task is not the best in the other, butboth belong to the same family, ViTscorrelated largest sample size family (0.74). This means that a ViT model is likely to perform wellin both ID and C-OOD. Note that the worst performing models in C-OOD detection are small,optimized models. We further discuss correlations with other factors such as accuracy, model size,input size and embedding size in Appendix M.7.
Figure 9:	A comparison of 484 models by their AUROC (×100, higher is better) and log(number ofmodel’s parameters) on ImageNet. Each dotted marker represents a distilled version of the original.
Figure 10:	A comparison of 484 models by their -log(ECE) (higher is better) and log(number ofmodel’s parameters) on ImageNet. Each dotted marker represents a distilled version of the original.
Figure 11: The RC curves for Models A and B.
Figure 12: The RC curves for the hypothetically optimal versions of Models A and B.
Figure 13:	Comparing vanilla models to those incorporating KD into their training (representedby markers with thick borders and a dot). In a pruning scenario that included distillation, yellowmarkers indicate that the original model was also the teacher. The performance of each model ismeasured in AUROC (higher is better) and -log(ECE) (higher is better).
Figure 14:	Comparing teacher models (yellow markers) to their KD students (represented by mark-ers with thick borders and a dot). The performance of each model is measured in AUROC (higheris better) and -log(ECE) (higher is better).
Figure 15:	A comparison of 484 models after being calibrated with TS, evaluated by their AUROC(×100, higher is better) and -log(ECE) (higher is better) on ImageNet. Each marker’s size is de-termined by the model’s number of parameters. ViT models are still among the best performingarchitectures for all aspects of uncertainty estimation.
Figure 16: Out of 484 models evaluated, models that were assigned a temperature higher than 1by the calibration process tended to degrade in AUROC performance rather than improve. Markersabove the x axis represent models that benefited from TS, and vice versa.
Figure 17: The relationship between temperature and the success of TS, unlike the case for AUROC,seems unrelated.
Figure 18: A comparison of 484 models by their log(number of model’s parameters) and the cover-age they are able to provide for a SAC of 99% (higher is better) on ImageNet.
Figure 19: Number of architecture parameters vs. C-OOD AUROC performance at severity level 5(median severity). The pair of numbers next to each architecture name at the legend correspond to itsSpearman correlation and the number of models tested from that architecture (family), respectively.
Figure 20: Architecture accuracy vs. C-OOD AUROC performance. In the legend, the pair ofnumbers next to each architecture name correspond to the Spearman correlation and the numberof networks tested from that architecture (family), respectively. Accuracy appears to have a highcorrelation with the C-OOD detection performance, with a Spearman correlation of 0.65. Mostarchitectures also hold this general trend except for Nest and Twins. Next to each architecture wereport the Spearman correlation value and the number of networks tested from that architecture.
Figure 21: The average relative improvement when using distillation, pretraining, semi-supervisedlearning and adversarial training. The shaded green area indicates the area of positive improvement.
Figure 22: Relative improvement gain in C-OOD detection performance when using entropy insteadof the softmax confidence signal. In median network terms, entropy offers positive improvementover softmax in most serverities except s ∈ {7, 8, 9}. The green shaded area indicates the area ofpositive improvement.
Figure 23: Relative improvement gain in C-OOD detection performance when using MC-Dropoutinstead of softmax confidence signal. We find that MC-dropout improves performance for mostnetworks in severities up to s5, and degrades performance for most networks in higher ones. Someoutlier networks get a significant improvement when switching to MC-dropout at high severity lev-els.
Figure 24: Relative improvement gain in C-OOD detection performance when using MC-dropoutentropy confidence signal. We see that MC-dropout fails to improve upon entropy in most casesacross all severity levels. This suggests that the main component in MC-dropout benefiting detectionis the usage of entropy.
Figure 25: Spearman correlation between the rankings of the models given by different severitylevel.
Figure 26: Spearman correlations between C-OOD detection AUROC and Accuracy, ID-AUROC,#Parameters, Input size, Embedding size across all severity levels.
Figure 27: An instance of viceroy butterfly predicted to be a fox squirrel by 105 models.
