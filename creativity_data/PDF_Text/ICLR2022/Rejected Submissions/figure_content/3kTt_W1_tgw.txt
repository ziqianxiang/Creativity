Figure 1: Network architecture of our proposed f -MICL. imagei : the ith image in the currentbatch; f: the function used in the f -mutual information (§2); g: feature embedding; t, t1, t2:augmentation functions drawn from the same family T of augmentations; f 0: the derivative; f *: theFenchel conjugate. The symbol ◦ denotes function composition. The sum of the two terms gives thevariational lower bound of f -mutual information. See equation 10 for more details.
Figure 3: (left and middle) Distances between pairs of normalized features within a batch. Greenregion: similar pairs. Orange region: dissimilar pairs. f -MICL gives nearly uniform distances fordissimilar pairs for the f -divergences in Table 1. For non-satisfying f -divergences such as the RKL,the features collapse to a constant and thus the distances are zero. (right) The test classificationaccuracy v.s. the batch size after training 200 epochs for all algorithms.
Figure 4: The training loss curves of various f -divergences on CIFAR-10 with 200 epochs.
Figure 5: Experiment for verifying Assumption 3. We draw the relation between the squared dis-tances kxg - yg k2 and the averaged log pg with RealNVP. The features are learned by differentalgorithms trained on CIFAR-10. (left) SimCLR; (right) f-MICL with the KL divergence.
