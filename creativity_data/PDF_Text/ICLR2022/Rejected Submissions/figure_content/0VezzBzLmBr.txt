Figure 2: In this navigation task, the agents are learning to cover all the landmarks. For the transition a fromBik , the best-matched transition b selected from Dik using de is still much different from transition a. However,the value-based distance dq will select the nearly isomorphic transition c, which is more helpful for evaluatingthe trainsition similarity.
Figure 3: Illustrations of multi-agent mujoco tasks. Different colors mean different agents (Jiang & Lu, 2021).
Figure 4: Learning curves of OTC on BCQ and AWAC.
Figure 5: Learning curves of OTC (de) on BCQ With different α.
Figure 6:	Learning curves of OTC (de) on BCQ with adaptive α and fixed α (1.0).
Figure 7:	Curves of adaptive α of OTC (de) on BCQ during online tuning. Dotted lines shoW mean values, andviolin plots shoW distributions over seeds.
Figure 8:	Learning curves of OTC, Balanced Replay, uniformly sampling from the merged dataset, and onlysampling from the online dataset.
Figure 9: Learning curves of OTC (de) on MABCQ.
