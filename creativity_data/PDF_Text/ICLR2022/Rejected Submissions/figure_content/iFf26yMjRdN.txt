Figure 1:	Three simple examples of partitioning deep learning models.
Figure 2:	More structured partial model personalization. (a) The adapter has a skip connection, thus it collapsesto the identity mapping if vi = 0; in addition, it has a bottleneck in the middle (Houlsby et al., 2019). (b) Thegeneralized additive model can be further augmented with a shared input layer for representation learning.
Figure 3: Absolute change in accuracy (percentage points) due to personalization plotted against number ofpersonal parameters (i.e., dimensionality of vi). Note that the x-axis is in log scale.
Figure 4: StackOverflow task: Scatter plot of change in training and test accuracy (percentage points) per-deviceversus the number of training samples on the device for (a) Left: full personalization with finetuning, and, (b)Right: partial personalization with the output layer.
Figure 5: Distribution of number of training samples per device for each of the tasks considered in theexperiments. For GLDv2, we do not show the long right tail, where the maximum number of data points perdevice is 1000 (cf. Table 2).
Figure 6: Left two: Distribution of change in the per-device train (left most) and test (center left) accuracydue to personalization on the StackOverflow dataset. Right two: Distribution of change in the per-device testaccuracy of partial personalization under regularization on the StackOverflow dataset: (a) center right: adapterpersonalization under `2 regularization, and, (b) rightmost: output layer personalization under dropout. Notethat the “No Reg.” and “No d/o” plots on the right two are different because they personalize different modelparts. Interpretation: The white dot in inside the violin denotes the median, while the black box enclosing thiswhite dot marks the interquartile range (i.e., 25th and 75th percentiles). The body of the violin is a kernel densityestimate of the distribution of accuracies. The lines extend out to the minimum and maximum accuracy in eachcase.
Figure 7: Scatter plot of change in accuracy (pp) per-device versus the number of training samples on thedevice for StackOverflow. Top: Training accuracy. Bottom: Test accuracy. This is the full version of Figure 4from the main paper.
Figure 8: Scatter plot of change in accuracy (pp) per-device versus the number of training samples on thedevice for GLDv2. Top: Training accuracy. Bottom: Test accuracy.
Figure 9: Scatter plot of change in accuracy (pp) per-device versus the number of training samples on thedevice with the effect of regularization. Top: '2 regularization a.k.a. weight decay. Bottom: dropout. The “best”values of the '2 regularization parameter and dropout are chosen to maximize the average test accuracy acrossall devices.
Figure 10: Change in per-device accuracy (PP) due to personalization. The solid line is the mean over 5 randomruns and the shaded area denotes the max/min across these runs. The devices are sorted in ascending order ofaccuracy change. The points in orange depict two example devices who might either be helped or harmed bypersonalization depending on the random seed.
