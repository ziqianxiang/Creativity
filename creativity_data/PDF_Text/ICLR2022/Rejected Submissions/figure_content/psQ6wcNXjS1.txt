Figure 1: Illustration of the sampling trajectories that we study in this work. The shortrun samples areinitialized from a generator that is trained in tandem with the EBM because the goal is self-containedsynthesis. Midrun and longrun samples are initiated from a high-quality starting image obtained froma pre-trained SNGAN, and we study the ability of the EBMs to preserve the quality of the input imagefrom defense and density estimation points of view. The plots show the FID score (Heusel et al., 2017)of 5,000 samples across Langevin steps. The shortrun samples improve on the generator initializationto achieve high-quality synthesis around 250 steps. The midrun samples achieve reasonably low FIDin a critical range of about 2k steps where defense can be achieved. The longrun sample maintainsreasonable synthesis across the entire trajectory, and much further. The shortrun and midrun sampleseventually produce defective results outside of their tuned window of stability.
Figure 2: Cooperative-persistent initializa-tion uses paired latent and image states thata drawn from persistent banks to learn theEBM and generator.
Figure 3: Persistent ini-tialization. Positive sam-ples are from data samplesand negative samples areMCMC samples initializedfrom a batch from an imagebank. Some states are ran-domly rejuvenated.
Figure 4: Visualization of our longruninitializatin procedure. Newly reju-venated samples must remain the theburnin bank until they have approachthe model steady-state, at which pointthey move to the update bank to beused for model gradients.
Figure 5: Comparison of cooperative learning (Xie et al., 2018) and our hybrid cooperative-persistentlearning using appearance of shortrun samples after 500 updates of the EBM using a generator withand without batch norm. The cooperative models have difficulty achieving diversity in the shortrunsamples because the EBM is unable to significantly change the appearance of initial generator images.
Figure 6: Accuracy over varying numbers of langevin steps K for both the CIFAR-10 and Imagenetexperiments. For CIFAR-10 we used Hadv = 24 in these experiments to compare against Hill et al.
Figure 7: Ablation study showing the importance of annealing. Left: Samples from a non-annealedmodel trained with the midrun method after 1500 MCMC steps. Right: Samples from a non-annealedmodel trained with the longrun method ater 100K MCMC steps. MCMC samples were initializedfrom data. This shows that rejuvenation of the midrun trajectories from data and the separation oflongrun samples into burn-in and update banks alone is not enough. Annealing ensures that samplesfrom past EBMs function as approximate samples from the current EBM, since the weights arechanging very slowly.
Figure 8: Score-based Langevin experiment on the CIFAR-10 dataset. Left: Accuracy of natural andadversarial images resulting from a BPDA+EOT defense using a score-based model with an annealedlangevin purification method for 125 samples over varying steps. Right: Samples received from thisannealed langevin diffusion process over the same sampling lengths.
Figure 9: Left: MCMC samples after 100K steps using a GLOW model (Kingma & Dhariwal,2018) trained on CIFAR-10. Right: MCMC samples after 100K steps using a conditional recoverylikelihood model (Gao et al., 2020b) trained on CIFAR-10. MCMC samples were initialized fromdata samples. Neither model can correctly approximate the distribution of probability mass forthe data density. The problem of steady-state misalignment extends beyond EBMs to many othergenerative density models. We tried several different temperatures close to 1 for the GLOW modeland found equivalent results.
Figure 10: Shortrun samples from CIFAR-10 EBM at resolution 32 × 32.
Figure 11: Shortrun samples from Celeb-A EBM at resolution 64 × 64.
Figure 12: Shortrun samples from ImageNet EBM at resolution 128 × 128.
Figure 13: Longrun samples at 100,000 steps and extremely longrun samples at 1 million stepsfor EBMs trained on three datasets. Our initialization is able to preserve a high degree of realismover the first 100K steps, and a reasonable degree of realism over very long trajectories. Whileoversaturation and distortion is noticeable for some samples using 1M steps, many samples havereasonable appearance and there is high diversity. Our method makes significant progress towardsaligning longrun samples with high-quality samples from training to ensure that the model is a validdensity.
