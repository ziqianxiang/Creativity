Figure 1: Overall architecture of the proposed approach. Blocks in blue are the only onesoptimized, while blocks in grey are fixed or frozen operations.
Figure 2: Estimated latent space dimensionality according to the fidelity parameter and itscorresponding influence on the reconstruction quality.
Figure 3: Reconstruction of an input sample with several fidelity parameters f .
Figure 4: Architecture of the encoder used in the RAVE model.
Figure 5: Overview of the proposed decoder. The latent representation is upsampled usingalternating upsampling layers and residual stack. The result is processed by three sub-networks, respectively producing waveform, loudness envelope and filtered noise signals.
Figure 6: Detailed architecture of the decoder blocks used in the RAVE model.
Figure 7: Mean Kullback Leibler divergence for each latent component between the posteriordistribution q@(z|x) estimated over the test set of the strings and VCTK datasets and theprior distribution.
Figure 8: Comparison of the estimated latent space dimensionality for two trainings ofRAVE on the Strings dataset with and without freezing the encoder during the adversarialfine tuning stage (starting at 1.106 iterations).
Figure 9: Example of timbre transfer using RAVE.
