Figure 1: An Overview of Divide and Explore. Agents are exploring different regions of state space automat-ically, and intrinsic motivations are shared across all the agents for updating.
Figure 2: Performance of D&E and baselines in MiniGrid. We plot the best agent’s performance inD&E.
Figure 3: State visitation heatmaps in the MultiRoom-N12-S10 task. All the comparison methodsonly train 1M timesteps.
Figure 4: Exploration processes of three different agents in singleton environment.
Figure 5: The performance of D&E with different number of agents.
Figure 6: The performance of D&E with different decay method.
Figure 7: Performance of D&E and baselines in MiniGrid. We plot the best agent’s performance inD&E and best performance in several independent runs of baseline algorithms.
Figure 8: The performance of D&E with different decay methods.
