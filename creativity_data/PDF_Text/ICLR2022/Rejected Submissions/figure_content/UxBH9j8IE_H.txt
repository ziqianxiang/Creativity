Figure 1: Iterative Network Pruning withRewinding Frankle et al. (2019b)connectivity structures in the individual layers. In thenext section we describe graph parameters of the net-work that determine such connectivity.
Figure 2: Examples of small feed-forward net-works having same sparsity but different connec-tivity properties; (a) disconnected network, (b)partially connected (c) strongly connected. Whenall of them are scaled to larger networks, (c) haslarge spectral gap compared to (a) and (b) signify-ing a high rate of expansion.
Figure 3: An example of bipartite graph computation for a particular convolution layer with kernelsize 2 × 2, 2 input channels, and 3 output channelsbi-parttite graph5Under review as a conference paper at ICLR 2022Now, the bounds are analyzed for both unweighted (Mi) and weighted (Wi) bipartite graphs at eachlayer Ni . Table 1 describes different bounding constraint depending on the type of the consideredgraph and bound type. We consider the bound differences (∆S and ∆R) for the eigenvalue and aver-age degree respectively. A transition of the ∆ values from positive to negative denotes a violation ofthe bounds, and thus loss of Ramanujan graph property of the bipartite graph for the correspondingpair of layers in the feed-forward neural network.
Figure 4: Results for MNIST dataset on Lenet architecture; (a) considering unweighted bi-adjacencymatrices, (b) considering weighted bi-agjacency matrices. Variation of accuracy with network den-sity is plotted for the clean and noisy test sets with increasing noise variances σ . Error bars forthe accuracy values computed over 5 runs are shown. For the layers L1, L2, and L3 the values of∆S are plotted for both unweighted and weighted representations, and ∆R for only the unweightedrepresentation. As mentioned in Table 1, ∆S and ∆R denote the difference in bounds of the eigen-values and average degrees. Transition of the ∆ values from positive to negative denote the loss ofRamanujan graph property. The plot is divided into three regimes- fully Ramanujan (gray shade),where the Ramanujan graph property holds for all the layers, partially Ramanujan (pink shade),where the property holds for some of the layers, and non-Ramanujan (no shade) where none of thelayers retain the property.
Figure 5: Results for CIFAR10 dataset on Conv4 architecture considering unweighted graph. Re-sults for the convolution layers (L2,3,4) is shown in (a), while for the FC layers (L5,6) is shown in(b). We exclude the first and last layers in this study, due to the low cardinality of one of the parts inthe bipartite graph for these layers. These layers are usually not pruned by the pruning algorithms.
Figure 6: The results for Lenet using MNIST dataset with pruning percentile pfc = 0.2 andpout = 0.1 (layer-wise-pruning); (a) eigen factors for the unweighted graph, (b) eigen factors forthe weighted graph, (c) test accuracyMNISTtteMt-Arehl WlthGIobal Prunlna (d>0.21MNIST(l∙n∙t-Arehl with Global Prunlna (d>O.21sa*qjp4M) BBUfi Cefw100.0 S1.2 2β.2 U.4 6.9	3.9	1.BNetwork Density→- LX-IembdeZLl-u∙l0-boundLi-ud∙φ-bound-→- L2-lβmbdβ2--∙ L2-u∙l0-bound-∙*∙- L2-ud∙φ-bound—L3-lβmbdB2—L3-U∙∣0-boundL3-ud∙a-bound■ < < mm-m Eecf Cefw100.0 51.2
Figure 7: The eigen factors for global pruning with pruning percentile pusing Lenet architecture2β.2 U.4 β.9	3.5	1.B	0.9	0.5Network Density(c)0.2 on MNIST datasetMNIST(l∙n∙t-Archl with Lay∙r-wls∙ Prunlnα (Eq.), U U .， ■ < IJ IJfpBqB-∙Mun- SJelEU ∙B-m-→- Li-IembdaZ-―・ Ll-u∙l0-boundLi-ud∙φ-bound-→- L2-lβmbdβ2L2-u∙l0-bound-∙*∙- L2-ud∙fl-bound→- L3-lβmbdB2_j L3-U∙∣0-boundL3-Ud∙φ-bound(a)MNIST(l∙n∙t-Archl with Lβy∙m∙ Prunlna (Eq.)
Figure 8:	The eigen factors for layer-wise pruning with pruning percentile p = 0.2 on MNISTdataset using Lenet architecture12Under review as a conference paper at ICLR 2022-→- Ll-Iambda2	-∙Ll-elg-bound	LIYeg-bound —L2-lambda2-∙≡-∙ L2-elg-bβund -*- L2-deg-bβundL3-lambda2	L3-elg-bβund	L 3-deg-boundIM-Oβ4.041.0	2β.2	22.8	20.1Network DensityIM-Oβ4.0	41.0	2β.2	22.8Network Density20.1IM-O «4.0 41.0 32.0 2β.l 2«.2 22.S 30.SNetwork DensityIM-O β4-0 «1.0 32.0 2β.l 2«.2 22.S 20.SNetwork Density(a)	(b)	(c)
Figure 9:	The eigen-factor results for the proposed pruning algorithm on MNIST dataset using Lenetarchitecture and different types of bound criteria, mentioned in Table 1; (a-b) for UE, (c-d) for UD,(e-f) for WE, (g-h) for WD (with average degree > 1)eElmv-c⅜L→- o-ɪɪ ɪɪ→- W≡0.4→- w≡ll.βeElmv-c⅜LeElmv-c⅛→- o-ɪɪ ɪɪ→- W≡0.4→- w≡ll.βaα→- v≡O.Q-→- σ≡0.4-→- σ≡O.β→- o-ɪɪ ɪɪ→- w≡o.4→- w≡ll.β10.7	7.1	s.«
Figure 10:	The test accuracy results of the proposed pruning algorithm on MNIST dataset usingLenet architecture with different noise levels in the input image using and bound criteria, mentionedin Table 1; (a) for UE, (b) for UD, (c) for WE, (d) for WD (with average degree > 1)13Under review as a conference paper at ICLR 2022A.2 More results for Conv2 architecture on CIFAR 1 0 dataset• COnV-LI，-*-- Cθ∙w-LX-∙6t,>Con*-∣A-<⅛t∙>-→- Conv-l2-*>-∙- Conv-L2-M(-)ConV-1251。100« 81.4 2β∙B 13.7 7.1 S.7 1.9 X-O 03Network Density(a)1MΛ Sl.« 2β4 1>.7 7.1 S.7 1Network Density(d)--SSC3- Esu£ Sa-W-∙- FC-L4-⅜
Figure 11:	The results for Conv2 architecture using CIFAR10 dataset with pruning percentilepconv = 0.1, pfc = 0.2, and pout = 0.1 (layer-wise-pruning); (a-c) eigen factors for the unweightedgraph, (d-e) eigen factors for the weighted graph, (f) test accuracy∞ M M W β OIP9⅞9MU∩) SJO⅞3eEVQ-W1∞ 41 16.8 6.9	2.8 1.2 0.5 0.2Network Density(a)Network Density(d)*020^'060*020(b)→- FC-L3-t2—FC-L3-et>(-)-*- FC-L3-<fb(∙)FC-L4-t2FC-L4-eb(-)-*- FC-L4-<fb(-)-→- FC-L5-t2---- FC-L5-β⅛(∙)
Figure 12:	The eigen factors for the IMP pruning algorithm on CIFAR10 dataset using Conv2 archi-tecture; (a-e) Global Pruning with p = 0.214Under review as a conference paper at ICLR 2022A.3 More results for Conv4 architecture on CIFAR 1 0 dataset(a)(d)-∙- FC-LS-*1-∙-- FC-LM (■)∣-*-∙ JC-La-<*6C)XOO.OM.440.72e.417.llU 7.S ΛΛ S.1 2.1 1.SNetwork Density(b)XOO.OM.440.72e.4X7.XlU 7.S ΛΛ S.1 2.1 1.SNetwork DensityA*3MV M⅞l(e)(f)Eetif Ceo-WFigure 13:	The results for Conv4 architecture using CIFAR10 dataset with pruning percentile
Figure 13:	The results for Conv4 architecture using CIFAR10 dataset with pruning percentilepconv = 0.1, pfc = 0.2, and pout = 0.1 (layer-wise-pruning); (a-c) eigen factors for the unweightedgraph, (d-e) eigen factors for the weighted graph, (f) test accuracy75XVIE3 &I£U2 EVQ-W168OIOOQ 63.4 40.7	26.3 17.1 11.2	7.3Network Density—Cθnv-Ll-tj-∙■— Cθnv-Ll-et>(∙)-*∙ Conv-LlYE-)-→- CQnV-L2的C0∏v-L2-et>(-)Conv-L2-<fi>(-)—C0nv-L3-tj-j C0nw-L3-β⅛(∙)-*■ COnV-L3Yb(∙)—COnv-L4-t2COnv-L4-βb(∙)
Figure 14:	The eigen factors for the IMP pruning algorithm on CIFAR10 dataset using Conv4 archi-tecture; (a-e) Layer-wise Pruning with pconv = 0.1, pfc = 0.2, and pout = 0.215Under review as a conference paper at ICLR 2022≡9⅞x6,m} Mk2u£ EVQ-W$£岂£5》Mk2u£ EVQ-W(a)10-→- C0∏v-Ll-t2-∙Cθnv-Ll-et>(∙)-*∙ Conv-LlYE-)→- Conv-L2-t2"C0nv-L2-et>(∙)-* Cθ∏v-L2-<ft>(-)—C0nv-L3-tjC0nw-L3-β⅛(∙)-*■ COnV-L3Yb(∙)—COnv-L4-tzCOnv-L4-βb(∙)-* COnV-L4><ft>(-)
Figure 15:	The eigen factors for the IMP pruning algorithm on CIFAR10 dataset using Conv4 archi-tecture; (a-e) Layer-wise Pruning with pconv = 0.1, pfc = 0.2, and pout = 0.116Under review as a conference paper at ICLR 2022A.4 More results for Conv6 architecture on CIFAR 1 0 dataset« ∞ » W OI?岂。MU3》SJE3eEVQ-Wi1210-a 6 4 2fp9⅞x6,M) ≡o*uβ⅛ EVQ-W—Co∏v-Ll-½Coπw-Ll-∙M∙J-i- Co∏v-Ll-db( )—COnv-L2，Coπw-LJ-∙fc(∙JCQ∏v-L2-(fM-)—Conv-L3-½COnv-L3-∙fc(∙)Cw∣v-L3-db( )—C0πv-L4-½COnv-L4-∙fc(∙)
Figure 16:	The eigen factors for the IMP pruning algorithm on CIFAR10 dataset using Conv6 archi-tecture; (a-e) Layer-wise Pruning with pconv = 0.1, pfc = 0.2, and pout = 0.2« ∞ » W OW£岂£5》s∙IE3eEVQ-W-∙- Cθ∏v-Ll-½→- Cθ∏w-Ll-∙M∙J-⅛-' Conv-Ll-db(∙]-∙- ConV-L2，→- Cθ∏w-LJ-∙fc(∙JConv-L2-dbt-)-∙- Cθπv-L3-taConv-L3-βfc(∙)-*- Cw∣v-L3-db( )—Cθ∏v-L4-½Conv-L4-βfc(∙)-*- Conv-L4-dbt-lCw∣v-L6-db( )-→- FC-L7-½FC-L7-∙b(∙)-*- FC-L7-<⅛(∙)
Figure 17:	The eigen factors for the IMP pruning algorithm on CIFAR10 dataset using Conv6 archi-tecture; (a-e) Layer-wise Pruning with pconv = 0.1, pfc = 0.2, and pout = 0.117Under review as a conference paper at ICLR 2022A.5 Results for VGG19≡Bsuet C&-U<PE9Mlun) EBUeL5~ BBUeL c&_m(d)(a)100.041.0 Xβ4 β.9 24 U O.S 。2 0.1NetiMork Density(b)1M.O*14> Xβ4 β.9 2.8 12 04 0.2 0.1Network Density(c)12，	LlMlL	U∙-M<-)u∙-m(-)→- UMk
Figure 18:	The eigen factors for the IMP pruning algorithm on CIFAR10 dataset using VGG19architecture and Global Pruning with p = 0.2 and lr = 0.01IOO6040⅛UE3UU< MMaF∣13.5 6.9 3.6 1.9 1.0 0.6 0.3 0.2 0.1Network DensityFigure 19: Results for the test accuracy of VGG19 on different noise levelA.6 Hyper-parameters DescriptionTable 3: Hyper-parameter settings for experimenting LTH using iterative magnitude based pruningLenet (on MNIST) Conv4 (on CIFAR10)Optimizer Training Iterations Batch size Learning Rate Pruning epochs Model initialization	Adam 20000 60 0.0012 50 Kaiming Normal	Adam 25000 60 0.0003 50 Kaiming NormalConv Layers FC layers	300,100,10	64,64, pool 128, 128, pool 256, 256, 10pruning epochs(in comparison)	60	6018UnderreVieW as a ConferenCe PaPersICLR 2022A∙7 DlFFERENT PARAMETERS VALUES WITH RESPECT To THE RAMANUJAN GRAPH BASEDPRUNING ALGoRITHMTabIe RePreSeIltatiVe result Ofthe ParameterS foundthe last PnnIg epoch Of synl—Bound
Figure 19: Results for the test accuracy of VGG19 on different noise levelA.6 Hyper-parameters DescriptionTable 3: Hyper-parameter settings for experimenting LTH using iterative magnitude based pruningLenet (on MNIST) Conv4 (on CIFAR10)Optimizer Training Iterations Batch size Learning Rate Pruning epochs Model initialization	Adam 20000 60 0.0012 50 Kaiming Normal	Adam 25000 60 0.0003 50 Kaiming NormalConv Layers FC layers	300,100,10	64,64, pool 128, 128, pool 256, 256, 10pruning epochs(in comparison)	60	6018UnderreVieW as a ConferenCe PaPersICLR 2022A∙7 DlFFERENT PARAMETERS VALUES WITH RESPECT To THE RAMANUJAN GRAPH BASEDPRUNING ALGoRITHMTabIe RePreSeIltatiVe result Ofthe ParameterS foundthe last PnnIg epoch Of synl—BoundAIgOrithm on LeIlet/MNISTLl	unweighted	(力 1, tι2, da∕υg, daAJgR, eb	，db, △5, Z∖r)	(31.84, 14.32, 235.20, 3.07, 11.11, 16.74, -0.22, 0.17)^TΓ^	weighted	(力 1, tl2, davgL∙) ^javgR∙> C	M 曲,△5)	(19.59, 1.37, 31.63, 12.10, 8.62, 8.87, 5.29)^TΓ^	Score	(力 1,力2, da,∙vgL, da,∙vgR, C	M db, △5)	(14.07, 6.96, 99.42, 1.30, 7.23, 10.47, 0.04 )^TΓ^		(layer-wise density, remaining/total parameters)		(0.01,2352/235200)	~T2~	unweighted	(力 1, tl2, davg, davgR∙)	，db, △5, Z∖r)	(12.41, 5.75, 3.37, 30.00, 6.76, 6.92, 0.17, 0.20)	=~UΓ	weighted	(力 1,力2, d<wg2√, dcι算gR, C	M 曲,△5)	一	(11.40, 1.34, 19.66, 6.55, 6.45, 6.68, 3.82)T2~	Score	(力 1,力2, da,∙vgL, da,∙vgR, C	M db, △5)	(43.27, 21.31, 11.23, 99.93, 13.00, 13.14, -0.39 )
