Figure 1: Attention mask with depth of 1 (orange) and 2 (green).
Figure 2: Masked attention heads and original attention heads for local and global dependencies.
Figure 3: Attention Locality Scores (ALS) for MaiT-Tiny: a) one masked head for all 24 layers; b)soft masking for all 24 layers5.5	deeper transformersAnother difficulty is to train deeper models because naively stacking transformer layers fails todeliver the expected performance gain as shown in Touvron et al. (2021b). One reason is attentioncollapse, i.e. the attention maps are more alike among deeper layers (Zhou et al., 2021). We findmixed masking is able to break the structural repetition in deep transformers and promotes diversityin later layers.
Figure 4: Top-1 accuracy on ImageNet for DeiT (red), MaiT (green), CaiT (yellow) and randomlymasked DeiT (blue) with various numbers of transformer blocks. Green square MaiT applies onemask heads for all layers while green star MaiT masks all three heads for the first 12 layers and onlyone masked attention head for the rest 24 layers.
Figure 5: ALS for 24-layer DeiT tiny.
Figure 6: ALS of deeper MaiT-tiny with 12, 24 and 36 layers.
Figure 7: Attention map similarity across 36 layers for DeiT-tinyIn Figure 7, 36-layer DeiT-tiny shows the highest similarity at the last 8 stages across all threeheads, with an average similarity of 0.75, whereas the similarity is quite small among the first 16layers and between the first 16 layers and the last 20 layers. This is agreement with Zhou et al.
Figure 8: Attention map similarity across 36 layers for MaiT-tiny薪F忘Xəpu - >po≡Block indexBlock indexBlock indexFigure 9: Attention map similarity across 36 layers for MaiT-tiny with mixed masking schemedepth direction is causing the attention collapse. The diversity among the heads within the samelayer doesn’t alleviate this issue.
Figure 9: Attention map similarity across 36 layers for MaiT-tiny with mixed masking schemedepth direction is causing the attention collapse. The diversity among the heads within the samelayer doesn’t alleviate this issue.
