Figure 1: The Hybrid Guided-VAE architecture. Streams of gesture events recorded using a Dy-namic Vision Sensor (DVS) are input into a Spiking Neural Network (SNN) that encodes the spatio-temporal features of the input data into a latent structure z . P and Q are pre-synaptic traces andU is the membrane potential of the spiking neuron. For clarity, only a single layer of the SNN isshown here and refractory states R are omitted. To help disentangle the latent space, a portion of theZ equal to the number of target features y* is input into a classifier that trains each latent variableto encode these features (Exc. Loss). The remaining z, noted \m are input into a different classi-fier that adversarially trains the latent variables to not encode the target features so they encode forother features instead (Inh. Loss). The latent state z is decoded back into x* using the conventionaldeconvolutional decoder layers.
Figure 2: A T-SNE plot of the zm portion of the latent space of the encoded NMNIST dataset.
Figure 3: Original (top) and reconstructed (bottom) time-surfaces for a sample gesture from eachclass. The reconstructions reflect the location of each gesture but with some smoothing of the detail.
Figure 4: (Left) A T-SNE plot of the zm portion of the latent space of the encoded DVSGesturedataset. Additionally, projections of the zm portion of the latent space of encoded new gestureswe recorded using a different DVS, and not part of the DVSGesture dataset are shown. Bottomcolor plots are the TS of the new gestures. (Right) T-SNE plot of the zm portion of the latent spacedisentanglement of three gesture classes implemented on the Intel Loihi.
Figure 5: Traversals of the latent space learned from the DVSGesture dataset. (Top) Beginning withthe right hand wave latent variable maximized and the left hand wave variable minimized, traversealong the latent space by gradually decreasing the right hand wave latent variable and increasing theleft hand wave latent variable. Note the initial TS shows a small, focal area of motion in the top-leftcorresponding to the participant’s right hand waving. (Bottom) The latent traversal along all of thenon-target z\m latent variables illustrates the relative insensitivity of the model to these features.
Figure 6: TSNE plots of the DVSGesture latent space. The images below each figure show novel gestures andwhere they are placed in the latent space by the different models.
Figure 7: A T-SNE plot of the guided zm latent space using lighting condition labels.
Figure 8: A T-SNE plot of the zm portion of the latent space of the encoded DVSGesture dataset . Sixunlabeled gestures (labeled “other” in the dataset) are shown below as time-surfaces connected to therespective embedded positions in the latent space. Each of the unclassified gestures are embeddednear gesture classes which exhibit similar motion.
