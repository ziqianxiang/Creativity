Figure 1: Experimental environments.
Figure 2: Lettuce-tomato salad recipe.
Figure 3:	Decentralized learning and centralized learning with macro-actions vs primitive-actions.
Figure 4:	Comparison of asynchronous macro-action-based actor-critic methods.
Figure 5: An example of the trajectory squeezing process in Mac-IAC. We collect each agent’shigh-level transition tuple at every primitive-step. Each agent is allowed to obtain a new macro-observation if and only if the current macro-action terminates, otherwise, the next macro-observationis set as same as the previous one. Each agent separately squeezes its sequential experiences bypicking out the transitions when its macro-action terminates (red cells). Each agent independentlytrain the critic and the policy using the squeezed trajectory.
Figure 6: An example of the trajectory squeezing process in Mac-CAC. Joint sequential experiencesare squeezed by picking out joint transition tuples when the joint macro-action terminates, in that,any agent’s macro-action termination (marked in red) ends the joint macro-action at the timestep.
Figure 7: An example of the trajectory squeezing process in Navie Mac-IACC.The joint trajectory isfirst squeezed depending on joint macro-action termination for training the centralized critic (line 18-19 in Algorithm 3). Then, the trajectory is further squeezed for each agent depending on each agent’sown macro-action termination for training the decentralized policy (line 20-23 in Algorithm 3.
Figure 8: An example of the trajectory squeezing process in Mac-IAICC: each agent learns anindividual centralized critic for the decentralized policy optimization. In order to achieve a better useof centralized information, the recurrent layer in each critic’s neural network should receive all thevalid joint macro-observation-action information (when any agent terminates its macro-action (line20-22) and obtain a new joint macro-observation). However, the critic’s TD updates and the policy’supdates still rely on each agent’s individual macro-action termination and the accumulative rewardat the corresponding timestep (line 23-26). Hence, the trajectory squeezing process for training eachcritic still depends on joint-macro-action termination but only retaining the accumulative rewardsw.r.t. the corresponding agent’s macro-action termination for computing the TD loss (the middlepart in the above picture). Then, each agent’s trajectory is further squeezed depending on its macro-action termination to update the decentralized policy.
Figure 9: Experimental environments.
Figure 10:	Decentralized learning with macro-actions vs primitive-actions in Box Pushing domain.
Figure 11:	Centralized learning with macro-actions vs primitive-actions in Box Pushing domain.
Figure 12:	Comparison of macro-action-based multi-agent actor-critic methods in Box Pushing.
Figure 13:	Experimental environments.
Figure 14:	Decentralized learning with macro-actions vs primitive-actions in Overcooked domain.
Figure 15:	Centralized learning with macro-actions vs primitive-actions in Overcooked domain.
Figure 16:	Comparison of macro-action-based multi-agent actor-critic methods in Overcooked.
Figure 17: Experimental environments.
Figure 18:	Comparison of macro-action-based multi-agent actor-critic methods in Warehouse ToolDelivery domain.
Figure 19:	Visualization of the optimal macro-action-based behaviors learned using Mac-IAICC inthe Box Pushing domain under a 14 × 14 grid world.
Figure 20:	Visualization of running decentralized policies learned by Mac-IAICC in Overcooked-A.
Figure 21: Visualization of running decentralized policies learned by Mac-IAICC in Overcooked-B.
Figure 22: Visualization of running decentralized policies learned by Mac-IAICC in Overcooked-C.
