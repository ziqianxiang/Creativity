Figure 1: The ViZDoom environment.
Figure 2: The infrastructure of our multi-stage learning system. Among the stages, the dashed linesrepresent model reuse. In our implementation, the goals are encoded into high-dimensional one-hotvectors, which are denoted as the variable Z.
Figure 3: (A) Visualization of the neuron activation for three basic competitive strategies learnedby DSC-Agent. (B) Three novel high-level tactics discovered by DSC-Agent, with their statistics ofoccurrence per match for different configurations. (C) Visualization of the value function predictedby DSC-Agent.
Figure 4: (A) and (B) Visual representation of DSC-Agent’s image processing network by usingClass Activation Mapping(CAM). (C) Visual representation of DSC-Agent’s strategic control pro-cessing network by using Class Activation Mapping(CAM).
Figure 5: Left: average Frags scores for each compared AI agent when playing against differenttesting opponents. The scores are computed by averaging over 10 episodes, each of which lasts for10 minutes. Right: a matrix showing the output distribution of the strategic policy in DSC-Agent(rows) against diverse opponents (columns).
Figure 6: (A) Comparison of HPPO, PPO and action wrapper in stage 1. (B) Evaluating the gamematching scheme used in stage 2 on affecting the final testing performance of DSC-Agent. (C)Evaluation of the importance of action decoupling, RGPS in stages 2 and 3, and strategic control.
Figure 7: DSC-Agent vs. previous top AIs in Deathmatch.
Figure 8: Map of ViZDoon track 1: we discretize the map into 20 areas, each of them is also usedto represent an opponent with a fixed strategy style.
Figure 9: Average frags/deaths per AI against 7 opponents of fixed strategy style (10 episodes of 10minutes each).
Figure 10: Averaged distribution of strategy: we carry out 10 rounds each type (the total is 20) ofbots respectively, and calculated the average strategy distribution of DSC-Agent.
Figure 11: Overview of the network structure of DSC Agent.
