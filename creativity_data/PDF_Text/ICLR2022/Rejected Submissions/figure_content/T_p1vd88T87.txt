Figure 1: Our method infers physical parameters directly from real-world videos, like the shownpendulum motion. Separated by the red line, the right half of each image shows the input frame, andthe left half shows our reconstruction based on physical parameters that we estimate from the input.
Figure 2: Overview of our approach. The dynamics in the video are modelled by an ordinary differ-ential equation, which is solved depending on unknown initial conditions z0 and unknown physicalparameters θode. The solution curve z (t; z0 , θode) is used to parametrize a time-dependent trans-formation T (z (t; z0, θode) , θ+) from the global coordinates XY of the background to the localcoordinates Xy of the moving object. The functions F(∙; θbg) and G(∙; θ∩bj) are neural networksthat model the appearance of the background and of the object, respectively. We can estimate theunknown physical parameters for a given video based on a rendering loss which penalizes the dis-crepancy between the input video frames and the rendered video. All estimated parameters andnetwork weights are shown in green in the figure.
Figure 3: Two masses spring system in which MNIST digits are connected by an (invisible) spring.
Figure 4: Prediction when training with the first N = 10 frames of sequence 1. Each image showsthe prediction of the respective method in white, and the ground truth as green overlay. For bothmethods, the prediction of images seen during training (frames 1,4,7,10) works well. For unseendata (frames 11,12,16,18,20), our method (top) leads to more reliable predictions, meaning that ourphysical parameter estimation is more accurate.
Figure 5: Rendered frames for the high-resolution (1280 × 854 pixel) lake sequence when trainingon the first 10 frames out of a total of 25 frames. While the first image is part of the training set,the two remaining images are frame 20 and 25 of the full sequence and have not been seen duringtraining. We see that our method produces realistic reconstructions of the scene even for physicalstates that are not seen during training. Best viewed on screen with magnification.
Figure 6: Prediction evaluation on the lake scene. The estimated deflection angle 夕 is close tothe groundtruth for N = 5 training frames and virtually identical for N = 10 training frames evenduring extrapolation (left). We show the time span covered by training images with transparent boxesabove the x axis. The trajectory error, measured by the MSE between the groundtruth deflectionangle and predicted deflection angle for t ∈ [0, 5], improves with the number of training frames N(right). This shows that a sufficient number of frames is necessary to constrain the model, whichcan then predict a nearly perfect trajectory.
Figure 7: Overview of our architecture for the implicit shape and appearance representations. Theinput vector X is passed through a layer of NFOUrier Fourier features (FF) to obtain the encoding Y(X).
Figure 8: Two masses spring system, where MNIST digits are connected by an (invisible) spring.
Figure 9: Two masses spring system, where MNIST digits are connected by an (invisible) spring.
Figure 10: Temporal prediction ability of our approach and the approach of Zhong & Leonard (2020)overfitted to a single sequence (baseline).We report the average MSE (Pixel MSE) of the predictedmasks for the entire sequence. The horizontal axis indicates the number of frames used for training,and the vertical axis shows the resulting error.
Figure 11: Reconstruction and prediction results for the city scene. The first frame is in the trainingset of 10 frames, while the two frames on the right are frame 20 and 25 of the sequence.
Figure 12: Reconstruction and prediction results for the apple scene. The first frame is in the trainingset of 10 frames, while the two frames on the right are frame 20 and 25 of the sequence.
Figure 13: Comparison of the visual quality of the reconstruction for the real pendulum video trainedon the sequence of 10 frames. The numbers indicate which frame of the sequence is shown. Bestviewed on screen with magnification. Please see also our supplementary video.
Figure 14: Training frames and segmentation masks for the real world video. Best viewed on a digi-tal screen with magnification. Upon closer inspection the motion blur as well as segmentation maskerror can be seen. The proposed approach can handle this real-world noise and produce compellingreconstructions and predictions shown in Fig. 13 and the supplementary video.
Figure 15: Prediction of the fully trained model of Zhong & Leonard (2020) for sequence 2 of thetest dataset (with zero control). While the prediction for the original data is perfect, the predictionfor shifting the frames by one pixel in each direction is significantly worse. This shows, that themodel does not generalize well to input frames where the pivot point of the pendulum is not in thecenter of the frame.
