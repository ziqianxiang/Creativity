Figure 1: Agent pop-ulations (robots) tra-verse the energy land-sCape (in grey) duringUPdate steps (∙) toseek energy minima(darker shade at Cen-ter). This resUlts insUrprise minimizationfrom high ( ) to lowenergy ( ) niches.
Figure 2: The EMIX architecture for learning surprise across global states.
Figure 3: Ablations for each of EMIX’s component. Whencompared to QMIX, EMIX and TwinQMIX depict im-provements in performance and sample efficiency.
Figure 4: Variation of surprise minimization with temperature β . Learning of surprise is achieved bymaking use of a suitable value of temperature parameter (β = 0.01) which controls the stability insurprise minimization by utilizing E as intrinsic motivation.
Figure 5: Task- so -many _baneling, (left) Behaviors learned by EMIX agents, (right) Behaviorslearned by QMIX agentsFigure 6: Task- 2sas_1sc, (left) Behaviors learned by EMIX agents, (right) Behaviors learned byQMIX agentsWe visualize and compare behaviors learned by surprise minimizing agents to the prior method ofQMIX. Fig. 5 presents the comparison of EMIX and QMIX agent trajectories (in yellow arrows) onthe challenging so_many_baneling task. The task consists of 27 baneling opponents which rapidlyattack the agent team on a bridge. QMIX agents naively move to the central alley of the bridge andstart attacking enemies early on. While QMIX agents naively maximize returns, EMIX agents learn adifferent strategy. EMIX agents rearrange themselves first at the corners of the bridge. Note that thesecorners provide cover from enemy’s fire. Thus, EMIX agents learn to take cover before approachingthe enemy head-on. This indicates that the surprise-robust policy is aware of the incoming fast-pacedassault.
Figure 6: Task- 2sas_1sc, (left) Behaviors learned by EMIX agents, (right) Behaviors learned byQMIX agentsWe visualize and compare behaviors learned by surprise minimizing agents to the prior method ofQMIX. Fig. 5 presents the comparison of EMIX and QMIX agent trajectories (in yellow arrows) onthe challenging so_many_baneling task. The task consists of 27 baneling opponents which rapidlyattack the agent team on a bridge. QMIX agents naively move to the central alley of the bridge andstart attacking enemies early on. While QMIX agents naively maximize returns, EMIX agents learn adifferent strategy. EMIX agents rearrange themselves first at the corners of the bridge. Note that thesecorners provide cover from enemy’s fire. Thus, EMIX agents learn to take cover before approachingthe enemy head-on. This indicates that the surprise-robust policy is aware of the incoming fast-pacedassault.
Figure 7: Variationin performance withincreasing number ofagents.
Figure 8: Variation in success rates with temperature β. A value of β = 0.01 is found to work best.
