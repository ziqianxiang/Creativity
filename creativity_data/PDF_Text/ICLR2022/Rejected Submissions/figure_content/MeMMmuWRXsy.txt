Figure 1: Left: Examples of visual control tasks in canonical (top) and distracting (bottom) environ-ments. Background, color, and camera pose distractions significantly increase observation complexity.
Figure 2: Results on pixel-based control tasks from the Distracting Control Suite in various distractionsettings. Top: No distractions. Middle: Background distractions only. Bottom Background, color,and camera distractions.
Figure 3: Top: Scores drop if recurrenceis removed. Bottom: Using negatives onlyfrom the same episode is often helpful.
Figure 4: Results on the Door Opening Task Robosuite (Zhu et al., 2020). Each column corresponds to adifferent distraction setting or robot arm type. Top: Camera observation as well as the proprioceptive state isprovided to the agent. Bottom: Only camera observation is provided, requiring the vision system to do morework.
Figure 5: Gating masks showing that CoRe learns to remove distractions. Green arrows indicate the locationsof the ball and cup, which are tiny, but CoRe can still find them. In the door opening task in Robosuite (bottomright), the lower edge of the door handle is very bright when the door is closed, indicating strong attention tothat part of the image. In addition, the agent attends to the edges of the door and the table, as well as its ownbase. Since the camera position can change randomly, the agent needs to find the robot’s position relative tothe door, for which the robot’s base and the door’s edges are important. Proprioception is provided separately.
Figure 6: Training curves for a typical run of CoRe training on cheetah, dynamic-medium setting.
Figure 7: Left: Examples from the Distracting Control Suite (Stone et al., 2021). The top row showsthe clean versions of each of the six tasks. The subsequent rows show examples from the easy,medium, and hard distraction settings. Right: Examples from the Robosuite door opening task (Zhuet al., 2020). The top row shows the clean versions of the tasks with a Panda and Jaco arm. Themiddle row shows observations with color and lighting distractions. The bottom row adds on cameradistraction.
Figure 8: Progression of validation reward with training steps on all distraction settings in theDistracting Control Suite. Top two rows are for the static setting and bottom two for dynamic. Eachplot shows easy, medium, and hard difficulty levels.
Figure 9: Additional ablation results. Left In addition to forward dynamics, inverse dynamics predictionand reward prediction improve performance. Right: Optimizing model parameters Θ using the critic loss isbeneficial.
Figure 10: Top: Performance of CoRe for different values of the KL-loss weight β. Bottom Performance of avariant in which contrastive prediction is done from the posterior latent state, rather than the prior. We can seehigher performance variance across values of β when the posterior latent state is used.
Figure 11: Results on the Robosuite Door Opening Task. All agents receive only RGB inputs.
