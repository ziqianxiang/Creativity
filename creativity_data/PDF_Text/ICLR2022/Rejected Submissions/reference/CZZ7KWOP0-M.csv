title,year,conference
 Scalable methods for 8-bit training ofneural networks,2018, arXiv preprint arXiv:1805
 Proxylessnas: Direct neural architecture search on target taskand hardware,2018, arXiv preprint arXiv:1812
 Once-for-all: Train onenetwork and specialize it for efficient deployment,2019, arXiv preprint arXiv:1908
 The best of both worlds:Combining recent advances in neural machine translation,2018, In Proceedings of the 56th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 An image is worth 16x16 words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Convolutionalsequence to sequence learning,2017, In Proceedings of the 34th International Conference on MachineLearning
 Levit: a vision transformer in convnetâ€™s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Local binary convolutional neuralnetworks,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 BossNAS: Exploring hybrid CNN-transformers with block-wisely self-supervised neuralarchitecture search,2021, In ICCV
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 The evolved transformer,2019, In International Conference onMachine Learning
 Vision transformer architecture search,2021, arXiv preprint arXiv:2106
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Alphanet: Improved trainingof supernet with alpha-divergence,2021, arXiv preprint arXiv:2102
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Addernet and its minimalist hardware design for energy-efficient artificial intelligence,2021, arXivpreprint arXiv:2101
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Lite transformer with long-shortrange attention,2020, In International Conference on Learning Representations
 Earlyconvolutions help transformers see better,2021, arXiv preprint arXiv:2106
 Weight-sharing neural architecture search: A battle toshrink the optimization gap,2020, arXiv preprint arXiv:2008
 Kernel basedprogressive distillation for adder neural networks,2020, In Advances in Neural Information ProcessingSystems
 Big bird: Trans-formers for longer sequences,2020, In Advances in Neural Information Processing Systems
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
