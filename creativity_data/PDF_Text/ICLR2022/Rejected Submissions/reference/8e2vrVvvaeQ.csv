title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2016, arXiv preprint arXiv:1611
 The security of machinelearning,2010, Machine Learning
 Recognition in terra incognita,2018, In Proceedings of theEuropean conference on computer vision (ECCV)
 Extracting training datafrom large language models,2020, arXiv preprint arXiv:2012
 Bigself-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Lowkey: leveraging adversarial attacks to protect social media usersfrom facial recognition,2021, arXiv preprint arXiv:2101
 Backdoor learning curves: Explaining backdoor poisoningbeyond inflUence fUnctions,2021, arXiv preprint arXiv:2106
 Bert: Pre-training of deepbidirectional transformers for langUage Understanding,2018, arXiv preprint arXiv:1810
 Witchesâ€™ brew: IndUstrial scale data poisoning via gradient matching,2020, arXivpreprint arXiv:2009
 Shortcut learning in deep neural networks,2020, Nature MachineIntelligence
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Momentum contrast forunsupervised visual representation learning,2020, In Conference on Computer Vision and PatternRecognition
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Metapoison: Practicalgeneral-purpose clean-label data poisoning,2020, arXiv preprint arXiv:2004
 Excessive invariancecauses adversarial vulnerability,2018, arXiv preprint arXiv:1811
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Understanding black-box predictions via influence functions,2017, InInternational Conference on Machine Learning
 Stronger data poisoning attacks break datasanitization defenses,2018, arXiv preprint arXiv:1811
 Learning multiple layers of features from tiny images,2009, 2009
 Data poisoning attacks on stochastic bandits,2019, In International Conferenceon Machine Learning
 Towards poisoning of deep learning algorithms with back-gradientoptimization,2017, In ACM Workshop on Artificial Intelligence and Security
 Readingdigits in natural images with unsupervised feature learning,2011, NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Input-aware dynamic backdoor attack,2020, arXiv preprint arXiv:2010
 Wanet-Imperceptible warping-based backdoor attack,2021, arXiv preprintarXiv:2102
 Probing neural network comprehension of natural languagearguments,2019, arXiv preprint arXiv:1907
 Right for the right reasons: Trainingdifferentiable models by constraining their explanations,2017, arXiv preprint arXiv:1703
 Justhow toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks,2021, InInternational Conference on Machine Learning
 Thepitfalls of simplicity bias in neural networks,2020, arXiv preprint arXiv:2006
 Fawkes:Protecting privacy against unauthorized deep learning models,2020, In USENIX Security Symposium
 Tensorclog: An imperceptible poisoning attack on deepneural network applications,2019, IEEE Access
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Provable defense againstdelusive poisoning,2021, arXiv preprint arXiv:2102
 Clean-label backdoor attacks,2018, 2018
 Dba: Distributed backdoor attacks against federatedlearning,2019, In International Conference on Learning Representations
 Neural tangent generalization attacks,2021, In InternationalConference on Machine Learning
