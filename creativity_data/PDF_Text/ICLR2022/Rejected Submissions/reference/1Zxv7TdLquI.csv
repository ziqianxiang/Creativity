title,year,conference
 The Theory of Dynamic Programming,1954, RAND Corporation
 Scheduled sampling for sequenceprediction with recurrent neural networks,2015, In Advances in Neural Information Processing Systems28: Annual Conference on Neural Information Processing Systems 2015
 Pixelsnail: An improved au-toregressive generative model,2018, In Proceedings of the 35th International Conference on MachineLearning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, In Proceedings ofthe 57th Conference of the Association for Computational Linguistics
 Learning energy-based models by diffusion recovery likelihood,2021, In 9th International Conference on LearningRepresentations
 No MCMC for me: Amortized sampling for fast and stable training ofenergy-based models,2021, In 9th International Conference on Learning Representations
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conferenceon Artificial Intelligence and Statistics
 The “wake-sleep” algorithm for unsupervised neuralnetworks,1995, Science
 Training products of experts by minimizing contrastive divergence,2002, NeuralComput
 A distributional approach to controlledtext generation,2021, In 9th International Conference on Learning Representations
 Deep directed generative models with energy-based probabilityestimation,2016, CoRR
 A tutorial on energy-basedlearning,2006, Predicting structured data
 ROUGE: A package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 Noise contrastive estimation and negative sampling for condi-tional models: Consistency and statistical efficiency,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Pointer sentinel mixturemodels,2017, In 5th International Conference on Learning Representations
 Learning non-convergent non-persistent short-run MCMC toward energy-based model,2019, In Advances in Neural InformationProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019
 Conditional image generation with pixelcnn decoders,2016, arXiv preprintarXiv:1606
 Learning latent spaceenergy-based prior model,2020, In Advances in Neural Information Processing Systems 33: AnnualConference on Neural Information Processing Systems 2020
 Distributional reinforcement learn-ing for energy-based sequential models,2019, CoRR
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Sequence level train-ing with recurrent neural networks,2016, In 4th International Conference on Learning Representations
 Wavenet: A generative model forraw audio,2016, In The 9th ISCA Speech Synthesis Workshop
 Pixel recurrent neural networks,2016, InInternational Conference on Machine Learning
 Energy-based open-world uncertainty modeling for confidence calibration,2021, CoRR
 Bayesian learning via stochastic gradient langevin dynamics,2011, InProceedings of the 28th International Conference on Machine Learning
 Bayesian learning via stochastic gradient langevin dynamics,2011, InProceedings of the 28th International Conference on Machine Learning
 Cooperative training ofdescriptor and generator networks,2020, IEEE Trans
 Learning neural generativedynamics for molecular conformation generation,2021, In 9th International Conference on LearningRepresentations
 Energy-based generative adversarial net-works,2017, In 5th International Conference on Learning Representations
 They believe thatthe energy-based correction of the prior noise distribution will benefit the subsequent generator’sgenerating process,2021, Furthermore
