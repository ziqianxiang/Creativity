title,year,conference
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Rethinking positional encoding in language pre-training,2020, arXivpreprint arXiv:2006
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied to document recog-nition,1998, Proceedings of the IEEE
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Listops: A diagnostic dataset for latent tree learning,2018, arXivpreprint arXiv:1804
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 On fast computation of a circulant matrix-vector product,2021, arXiv preprintarXiv:2103
 Self-attention with relative position representa-tions,2018, arXiv preprint arXiv:1803
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Attention is all you need,2017, In Advances in neural informationprocessing systems
