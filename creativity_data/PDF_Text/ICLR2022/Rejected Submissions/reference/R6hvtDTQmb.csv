title,year,conference
 MVF: Multivariate test functions library in c for unconstrained global optimiza-tion,2013, Retrieved June 2013
 Convex optimization: Algorithms and complexity,2014, arXiv preprint arXiv:1405
 On the convergence ofa class of adam-type algorithmsfor non-convex optimization,2019, In ICLR
 The global optimization problem,1978, an introduction
 Incorporating nesterov momentum into adam,2016, ICLR Workshop
 Adaptive subgradient methods for online learning and stochasticoptimization,2011, JMLR
 Generative adversarial networks,2014, NeurIPS
 Asymmetric valleys: Beyond sharp and flat local minima,2019, NeurIPS
 Deep residual learning for image recognition,2016, InCVPR
 Gans trainedby a two time-scale update rule converge to a local nash equilibrium,2017, NeurIPS
 Flat minima,1997, Neural computation
 Long short-term memory,1997, Neural Computation
 Densely ConneCted Convolutionalnetworks,2017, In CVPR
 ImProving generalization PerformanCe by switChing from adam tosgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stoChastiC oPtimization,2015, In ICLR
 Learning multiPle layers of features from tiny images,2009, Master’s thesis
 Visualizing the loss landsCaPe ofneural nets,2018, NeurIPS
 Onthe varianCe of the adaPtive learning rate and beyond,2019, ICLR
 Sgdr: StoChastiC gradient desCent with warm restarts,2017, ICLR
 AdaPtive gradient methods with dynamiC bound oflearning rate,2019, ICLR
 Building a large annotated CorPus ofEnglish: The Penn Treebank,1993, Computational Linguistics
 SPeCtral normalization for generativeadversarial networks,2018, ICLR
 First exit times of solutions of stochastic differential equations driven by multiplicative levynoise with heavy tails,2011, Stochastics and Dynamics
 Training tips for the transformer model,2018, PBML
 AdaPtive methods for nonconvexoPtimization,2018, In NeurIPS
 On the convergence of adam and beyond,2019, ICLR
 A stochastic aPProximation method,1951, The annals of mathematical statistics
 Imagenet large scale visual recognition challenge,2015, IJCV
 Descending through a crowded valley-benchmarkingdeeP learning oPtimizers,2021, ICML
 Lectures on geometric measure theory,1983, The Australian National University
 A tail-index analysis of stochastic gradient noise indeep neural networks,2019, In ICML
 On the importance of initialization andmomentum in deep learning,2013, In ICML
 Rethinking the incep-tion architecture for computer vision,2016, In CVPR
 Attention is all you need,2017, NeurIPS
 The marginal value ofadaptive gradient methods in machine learning,2017, NeurIPS
 Adabelief optimizer: Adapting stepsizes by the belief in observed gradients,2020, NeurIPS
 Online convex programming and generalized infinitesimal gradient ascent,2003, In ICML
____________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentumStepsize α	0,2022,001	0
____________________________________________________________Algorithm ∣ Adam AdamW Yogi AdaBOund RAdam AdaBelief AdaMomentumStepsize α	0,1150,01	0
