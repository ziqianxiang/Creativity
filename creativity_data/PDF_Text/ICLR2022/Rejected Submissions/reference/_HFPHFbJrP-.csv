title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Certified adversarial robustness via randomizedsmoothing,2019, In ICML
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In ICML
 A framework for robustness certification ofsmoothed classifiers using f-divergences,2020, In ICLR
 Curse of dimensionality onrandomized smoothing for certifiable robustness,2020, In ICML
 Tight certificates of adversarialrobustness for randomly smoothed classifiers,2019, arXiv preprint arXiv:1906
 Certified adversarial robustness withadditive noise,2018, arXiv preprint arXiv:1809
 Deep neural networks are easily fooled: High confi-dence predictions for unrecognizable images,2015, In Proceedings of the IEEE conference on computervision and pattern recognition
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Towards fast computation of certified robustness for relu networks,2018, In ICML
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In ICML
 Training for fasteradversarial robustness verification via inducing relu stability,2018, arXiv preprint arXiv:1809
