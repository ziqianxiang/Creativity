title,year,conference
 Deep Learning,2016, MIT Press
 Introduction to online convex optimization,2019, arXiv
 Deep residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2018, In Proceedings of the 31st Conference On Learning Theory
 Accelerating stochastic gradient descent using predic-tive variance reduction,2013, In Advances in Neural Information Processing Systems
 Adam: A method for stochastic optimization,2017, arXiv
 Deep learning,2015, Nature
 Non-convex finite-sum optimiza-tion via SCSG methods,2017, In Advances in Neural Information Processing Systems
 AEGD: Adaptive gradient decent with energy,2020,	arXiv
 An improved analysis of stochastic gradient descent withmomentum,2020, In NeurIPS
 Adaptive gradient methods with dynamic bound oflearning rate,2019, In International Conference on Learning Representations
 Some methods of speeding UP the convergence of iterative methods,1964, Z
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning Representations
 A stochastic approximation method,0003, Ann
 Very deep convolUtional networks for large-scale image recog-nition,2015, arXiv
 On the importance of initial-ization and momentUm in deep learning,2013, In Proceedings of the 30th International Conference onMachine Learning
 RMSprop: Divide the gradient by a rUnning average of itsrecent magnitUde,2012, COURSERA: Neural networks for machine learning
 Themarginal valUe of adaptive gradient methods in machine learning,2018, arXiv
 A Unified analysis of stochastic mo-mentUm methods for deep learning,2018, In Proceedings of the 27th International Joint Conference onArtificial Intelligence
 On the linear speedUp analysis of commUnication efficient mo-mentUm sgd for distribUted non-convex optimization,2019, In ICML
 Adaptivemethods for nonconvex optimization,2018, In S
 Adabelief optimizer: Adapting stepsizes by the beliefin observed gradients,2020, In Advances in Neural Information Processing Systems
