title,year,conference
 Feder-ated learning with quantized global model updates,2020, arXiv:2006
 Scalable methods for 8-bit training ofneural networks,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
 Language models are few-shotlearners,2020, In H
 DeepMood: modeling mobile phone typing dynamics for mooddetection,2017, In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining
 Deep learning withlimited numerical precision,1737, In International conference on machine learning
 Model pruningenables efficient Federated Learning on edge devices,2019, arXiv:1909
 Trainingquantized nets: A deeper understanding,2017, In Proceedings of the 31st International Conference onNeural Information Processing Systems
 FEDZIP: A compression framework forcommunication-efficient Federated Learning,2021, arXiv:2102
 FetchSGD: Communication-efficient federated learningwith sketching,2020, In International Conference on Machine Learning
 Fractionalskipping: Towards finer-grained dynamic CNN inference,2020, In Proceedings of the AAAI Conferenceon Artificial Intelligence
 The promise of edge computing,2016, Computer
 UVeQFed:Universal vector quantization for federated learning,2020, IEEE Transactions on Signal Processing
 Training deep neural networks with 8-bit floating point numbers,2018, In S
 A smart home gateway platform for data collection andawareness,2018, IEEE Communications magazine
