title,year,conference
 Curriculum learning,2009, InProceedings ofthe 26th annual international conference on machine learning
 Detecting backdoor attacks on deep neural networks byactivation clustering,2018, arXiv preprint arXiv:1811
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Certified adversarial robustness via randomizedsmoothing,2019, In International Conference on Machine Learning
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Learning models with uniform performance via distribu-tionally robust optimization,2021, The Annals of Statistics
 Sharpness-aware minimiza-tion for efficiently improving generalization,2020, arXiv preprint arXiv:2010
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2017, arXiv preprint arXiv:1708
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InAdvances in neural information processing systems
 Mentornet: Learningdata-driven curriculum for very deep neural networks on corrupted labels,2017, arXiv preprintarXiv:1712
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Beyond synthetic noise: Deep learning on con-trolled noisy labels,2020, In International Conference on Machine Learning
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Self-paced learning for latent variablemodels,2010, In Advances in neural information processing systems
 Deep partition aggregation: Provable defense against generalpoisoning attacks,2020, arXiv preprint arXiv:2006
 Neural attention distil-lation: Erasing backdoor triggers from deep neural networks,2021, arXiv preprint arXiv:2101
 Learning fromnoisy labels with distillation,2017, In Proceedings of the IEEE International Conference on ComputerVision 
 Learning deep neuralnetworks under agnostic corrupted supervision,2021, arXiv preprint arXiv:2102
 Fine-pruning: Defending against back-dooring attacks on deep neural networks,2018, In International Symposium on Research in Attacks
 Spectral normalizationfor generative adversarial networks,2018, arXiv preprint arXiv:1802
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Semidefinite relaxations for certifying ro-bustness to adversarial examples,2018, arXiv preprint arXiv:1811
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, arXivpreprint arXiv:1906
 Spectral signatures in backdoor attacks,2018, arXivPrePrint arXiv:1811
 Clean-label backdoor attacks,2018, 2018
 Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,2019, In 2019IEEE Symposium on Security and Privacy (SP)
 Rab: Provable robustness againstbackdoor attacks,2020, arXiv preprint arXiv:2003
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning
 Adversarial examples: Attacks and defenses fordeep learning,2019, IEEE transactions on neural networks and learning systems
2	Proof of theorem 1Theorem,2022, Let Remp
