title,year,conference
 Character-levellanguage modeling with deeper self-attention,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Approximation of probability distributions by convex mixtures of Gaussianmeasures,2010, Proceedings of the American Mathematical Society
 Adaptive input representations for neural language modeling,2019, InInternational Conference on Learning Representations
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Pattern recognition,2006, Machine learning
 Language models are few-shot learners,2020, In H
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Learning phrase representations using RNN encoder-decoder for statistical machine translation,2014, In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Rethinking attention withperformers,2021, In International Conference on Learning Representations
 What does BERT lookat? an analysis of BERT’s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Analyzing redundancy inpretrained transformer models,2020, arXiv preprint arXiv:2004
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Analyzing individual neurons inpre-trained language models,2020, arXiv preprint arXiv:2010
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Entropies and rates of convergence for maximum likelihood andbayes estimation for mixtures of normal densities,2001, Ann
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 Universal language model fine-tuning for text classification,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Structured attention networks,2017, arXivpreprint arXiv:1702
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Set trans-former: A framework for attention-based permutation-invariant neural networks,2019, In InternationalConference on Machine Learning
 A structured self-attentive sentence embedding,2017, CoRR
 Generating Wikipedia by summarizing long sequences,2018, arXiv preprint arXiv:1801
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Least squares quantization in pcm,1982, IEEE transactions on information theory
 Learning Word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Pointer sentinel mixturemodels,2017, In 5th International Conference on Learning Representations
 Are sixteen heads really better than one? InH,2019, Wallach
 All-but-the-top: Simple and effective postprocessing for Wordrepresentations,2018, In International Conference on Learning Representations
 ListOps: A diagnostic dataset for latent tree learning,2018, InProceedings of the 2018 Conference of the North American Chapter of the Association forComputational Linguistics: Student Research Workshop
 Fmmformer: Effi-cient and flexible transformer via decomposed near-field and far-field attention,2021, arXiv preprintarXiv:2108
 Image transformer,2018, In Jennifer Dy and Andreas Krause (eds
 Blockwiseself-attention for long document understanding,2019, arXiv preprint arXiv:1911
 Improving language under-standing by generative pre-training,2018, OpenAI report
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Efficient content-based sparseattention with routing transformers,2021, Transactions of the Association for Computational Linguistics
 Fast transformer decoding: One write-head is all you need,2019, arXiv preprintarXiv:1911
 The evolved transformer,2019, arXiv preprint arXiv:1901
 Aug-menting self-attention with persistent memory,2019, arXiv preprint arXiv:1907
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Long range arena : A benchmark for efficienttransformers,2021, In International Conference on Learning Representations
 Transformer dissection: An unified understanding for transformer’s attention via the lens ofkernel,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Analyzing the structure of attention in a transformer languagemodel,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and InterpretingNeural Networks for NLP
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 The initial value for each πjr is set to 0,2021,5
357Transformer-MGK 4 heads	53,2021,58	0
42Transformer sMGK 2 head	34,2019,69Transformer MGK 2 head	34
 It means that pG is theconvolution of f and the probability distribution G,2001, Since the space of Gaussian mixtures is densein the space of continuous probability measures (Bacharoglou
