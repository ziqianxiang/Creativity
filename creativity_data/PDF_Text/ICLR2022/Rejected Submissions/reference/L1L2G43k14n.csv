title,year,conference
 A closer look at memorization in deepnetWorks,2017, arXiv preprint arXiv:1706
 SpeCtrally-normalized margin bounds for neuralnetworks,2017, In Advances in Neural Information Processing Systems
 Kernel methods for deep learning,2009, Advances in neural informationprocessing systems
 Random deep neural netWorks are biased toWardssimple functions,2018, arXiv preprint arXiv:1812
 The structure of the genotype-phenotype map stronglyconstrains the evolution of non-coding rna,2015, Interface focus
 Input-output maps are strongly biased toWards simpleoutputs,2018, Nature Communications
 Generic predictions of output probability based oncomplexities of inputs and outputs,2020, Scientific Reports
 Deep convolutional networksas shalloW gaussian processes,2019, In International Conference on Learning Representations
 Modelling the influence of datastructure on learning in neural networks,2019, arXiv preprint arXiv:1909
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 Keeping neural networks simple,1993, In International Conference onArtificial Neural Networks
 Flat minima,1997, Neural COmputatiOn
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, arXiv preprint arXiv:1502
 Finding flatter minima with sgd,2018, In ICLR (Workshop)
 Predicting the generalization gap in deepnetworks with margin distributions,2018, arXiv preprint arXiv:1810
 Fantastic generalizationmeasures and where to find them,2019, arXiv preprint arXiv:1912
 Onlarge-batch training for deep learning: Generalization gap and sharp minima,2016, arXiv preprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 A surprising linearrelationship predicts test performance in deep networks,2018, arXiv preprint arXiv:1807
 Learning by turning: Neural architecture awareoptimisation,2021, arXiv preprint arXiv:2102
 Introduction to gaussian processes,1998, NATO ASI series
 Rethinking parameter counting in deep models:Effective dimensionality revisited,2020, arXiv preprint arXiv:2003
 From genotypesto organisms: State-of-the-art and perspectives of a cornerstone in evolutionary dynamics,2020, arXiv preprintarXiv:2002
 Some pac-bayesian theorems,1998, In Proceedings of the eleventh annual conference onComputational learning theory
 Neural networks are a priori biased towards boolean functions with low entropy,2019, arXiv preprintarXiv:1909
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Exploring generalization in deeplearning,2017, In Advances in Neural Information Processing Systems
 Sensitivityand generalization in neural networks: an empirical study,2018, arXiv preprint arXiv:1802
 Bayesian deep convolutional networks with many channels aregaussian processes,2018, arXiv preprint arXiv:1810
 A reparameterization-invariantflatness measure for deep neural networks,2019, arXiv preprint arXiv:1912
 On the spectral bias of neural networks,2018, arXiv preprint arXiv:1806
 A scaleinvariant flatness measure for deep network minima,2019, arXiv preprint arXiv:1902
 Modeling by shortest data description,1978, Automatica
 Learning representations by back-propagatingerrors,1986, Nature
 Eigenvalues of the hessian in deep learning: Singularity andbeyond,2016, arXiv preprint arXiv:1611
 The arrival of the frequent: how bias in genotype-phenotype maps can steerpopulations to local optima,2014, PloS one
 Asymptotic learning curves of kernel methods: empiricaldata vs teacher-student paradigm,2019, arXiv preprint arXiv:1905
 Expectation propagation of gaussian process classification and its application to gene expressionanalysis,2008, 01 2008
 Normalized flat minima: Exploring scale invariant definitionof flat minima for neural networks using pac-bayesian analysis,2019, arXiv preprint arXiv:1901
 Generalization bounds for deep learning,2020, arXiv preprintarXiv:2012
 Deep learning generalizes because the parameter-function map is biased towards simple functions,2018, arXiv preprint arXiv:1805
 Towards understanding generalization of deep learning: Perspective of losslandscapes,2017, arXiv preprint arXiv:1706
 Wide feedforward or recurrent neural networks of any architecture are gaussian processes,2019, In H
 A fine-grained spectral perspective on neural networks,2019, arXiv preprintarXiv:1907
 Understanding deep learning is also a job for physicists,2020, Nature Physics
 Understanding deep learningrequires rethinking generalization,2016, arXiv preprint arXiv:1611
 Energy-entropy competition and theeffectiveness of stochastic gradient descent in machine learning,2018, Molecular Physics
