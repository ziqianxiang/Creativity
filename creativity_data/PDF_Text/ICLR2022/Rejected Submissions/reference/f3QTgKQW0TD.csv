title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In International conference on machinelearning
 Synthesizing robust adversarialexamples,2018, In International conference on machine learning
 Towards evaluating the robustness of neural networks,2017, In 2017ieee symposium on security and privacy (sp)
 Unlabeled dataimproves adversarial robustness,2019, arXiv preprint arXiv:1905
 Adversarial attacks and defences: A survey,2018, arXiv preprint arXiv:1810
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Boost-ing adversarial attacks with momentum,2018, In Proceedings of the IEEE conference on computervision and pattern recognition
 Fitting a manifold oflarge reach to noisy data,2019, arXiv preprint arXiv:1910
 Adversarialexamples for semantic image segmentation,2017, arXiv preprint arXiv:1703
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations (ICLR)
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Adversarial attacks and defences competition,2018, InThe NIPSâ€™17 Competition: Building Intelligent Systems
 Magnet: a two-pronged defense against adversarial examples,2017, InACM Conference on Computer and Communications Security (CCS)
 Towards poisoning of deep learning algorithms with back-gradientoptimization,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE symposium onsecurity and privacy (SP)
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE symposium onsecurity and privacy (SP)
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, arXiv preprint arXiv:1704
