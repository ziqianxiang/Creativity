title,year,conference
 CSI NN: Reverse engineering of neUral network architec-tUres throUgh electromagnetic side channel,2019, In 28th USENIX Security Symposium
 Towards evalUating the robUstness of neUral networks,2017, In 2017 IEEESymposium on Security and Privacy
 Imagenet: A large-scale hierarchicalimage database,2009, In 2009 IEEE Computer Society Conference on Computer Vision and PatternRecognition
 The early phase of neUral network training,2020, In 8thInternational Conference on Learning Representations
 Explaining and harnessing adversarial examples,2015, In 3rdInternational Conference on Learning Representations
 Caltech-256 object category dataset,2007, 2007
 Letâ€™s agree to agree: NeUral networks share classifi-cation order on real datasets,2020, In Proceedings of the 37th International Conference on MachineLearning
 Deep residUal learning for image recognition,2016, In 2016 IEEEConference on Computer Vision and Pattern Recognition
 Stealing links from graph neUral networks,2021, In30th USENIX Security Symposium
 A baseline for detecting misclassified and out-of-distribution exam-ples in neural networks,2017, In 5th International Conference on Learning Representations
 Densely connected convolutionalnetworks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition
 High accuracy and high fidelityextraction of neural networks,2020, In 29th USENIX Security Symposium
 Entangled watermarks as adefense against model extraction,2021, In 30th USENIX Security Symposium
 PRADA: protecting against DNN model stealingattacks,2019, In IEEE European Symposium on Security and Privacy
 Protecting dnns from theft using an ensemble ofdiverse models,2021, In International Conference on Learning Representations
 Model extraction warning in mlaas paradigm,2018, InProceedings of the 34th Annual Computer Security Applications Conference
 Learning multiple layers of features from tiny images,2009, 2009
 Tiny imagenet visual recognition challenge,2015, 2015
 Defending against neural network model stealingattacks using deceptive perturbations,2019, In 2019 IEEE Security and Privacy Workshops
 Enhancing the reliability of out-of-distribution image detection inneural networks,2018, In 6th International Conference on Learning Representations
 Towards deep learning modelsresistant to adversarial attacks,2018, In 6th International Conference on Learning Representations
 Prediction poisoning: Towards defenses against DNN modelstealing attacks,2020, In 8th International Conference on Learning Representations
 Activethief: Model extrac-tion using active learning and unannotated public data,2020, Proceedings of the AAAI Conference onArtificial Intelligence
 Practicalblack-box attacks against machine learning,2017, In Proceedings of the 2017 ACM on Asia Conferenceon Computer and Communications Security
 Membership inference attacks against machinelearning models,2017, In 2017 IEEE Symposium on Security and Privacy
 Intriguingproperties of neural networks,2014, In 2nd International Conference on Learning Representations
 DAWN: dynamic adversarial watermarking of neuralnetworks,2021, pp
 Stealing machine learning models viaprediction apis,2016, In 25th USENIX Security Symposium
 Stealing hyperparameters in machine learning,2018, In 2018 IEEE Symposiumon Security and Privacy
 Cloudleak: Large-scale deep learning modelsstealing through adversarial examples,2020, In 27th Annual Network and Distributed System SecuritySymposium
 Protecting intellec-tual property of deep neural networks with watermarking,2018, In Proceedings of the 2018 on AsiaConference on Computer and Communications Security
 Hermes attack: Steal DNN models with lossless inferenceaccuracy,2021, In 30th USENIX Security Symposium
