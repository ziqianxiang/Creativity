title,year,conference
 Etc: Encoding long and structuredinputs in transformers,2020, arXiv preprint arXiv:2004
 Good-enough compositional data augmentation,2019, arXiv preprint arXiv:1904
 Pondernet: Learning to ponder,2021, arXiv preprintarXiv:2107
 The devil is in the detail: Simple tricksimprove systematic generalization of transformers,2021, arXiv preprint arXiv:2108
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Compositional generalizationin semantic parsing: Pre-training vs,2020, specialized architectures
 Deep learning,2016, MIT press
 Hybrid computing using a neural network with dynamic external memory,2016, Nature
 Measur-ing compositional generalization: A comprehensive method on realistic data,2019, arXiv preprintarXiv:1912
 Improving compositionalgeneralization in classification tasks via structure annotations,2021, arXiv preprint arXiv:2106
 Generalization without systematicity: On the compositional skillsof sequence-to-sequence recurrent networks,2018, In International conference on machine learning
 Deep learning,2015, nature
 Memorize or generalize? searching for acompositional rnn in a haystack,2018, arXiv preprint arXiv:1802
 The eos decision andlength extrapolation,2020, arXiv preprint arXiv:2010
 Making transformers solvecompositional tasks,2021, arXiv preprint arXiv:2108
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Self-attention with relative position representa-tions,2018, arXiv preprint arXiv:1803
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Hibert: Document level pre-training of hierarchicalbidirectional transformers for document summarization,2019, arXiv preprint arXiv:1905
 The error per intermediate step can be seen as the probability of making a mistakeat any given intermediate step,2022, On the left
