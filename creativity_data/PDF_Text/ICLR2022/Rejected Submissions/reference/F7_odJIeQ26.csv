title,year,conference
 Towards ahuman-like open-domain chatbot,2020, arXiv preprint arXiv:2001
 Towards solving differ-ential equations through neural programming,2018, In ICML Workshop on Neural AbstractMachines and Program Induction (NAMPI)
 Neural machine translation byjointly learning to align and translate,2014, arXiv preprint arXiv:1409
 Lan-guage models are few-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, In EuropeanConference on Computer Vision
 Retain: An interpretable predictive mo del for healthcare usingreverse time attention mechanism,2016, arXiv preprint arXiv:1608
 Marian: Fast neural machinetranslation in C++,2018, In Proceedings of ACL 2018
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Colorization transformer,2021, arXivpreprint arXiv:2102
 Deep learning for symbolic mathematics,2019, CoRR
 Object detection based on an adaptiveattention mechanism,2020, Scientific Reports
 Multilingual denoising pre-training for neural machinetranslation,2020, CoRR
 Pretrained transformers asuniversal computation engines,2021, arXiv preprint arXiv:2103
 Symbolic integration,1967, 1967
 Lxmert: Learning cross-modality encoder representations fromtransformers,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neuralinformation processing systems
 Hugging-faceâ€™s transformers: State-of-the-art natural language processing,2019, CoRR
 Stat: Spatial-temporal attention mechanism for video caption-ing,2019, IEEE transactions on multimedia
