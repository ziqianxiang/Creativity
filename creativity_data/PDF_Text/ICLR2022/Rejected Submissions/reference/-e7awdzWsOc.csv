title,year,conference
 Intrinsic Dimensionality Explains theEffectiveness of Language Model Fine-Tuning,2020, arXiv Prepr
 The Generalization-Stability Tradeoff in Neural Network Pruning,2019, ICLR
 Deep Rewiring: Trainingvery sparse deep networks,2017, 6th Int
 Sparse Networks from Scratch: Faster Training without LosingPerformance,2019, arXiv Prepr
 Rigging the Lottery:Making All Tickets Winners,2019, 37th Int
 Gradient Flow in Sparse NeuralNetworks and How Lottery Tickets Win,2020, arXiv Prepr
 Reducing Transformer Depth on Demand withStructured Dropout,2020, ICLR
 Switch Transformers: Scaling to Trillion ParameterModels with Simple and Efficient Sparsity,2021, arXiv Prepr
 Linear ModeConnectivity and the Lottery Ticket Hypothesis,2020, ICML
 The State of Sparsity in Deep Neural Networks,2019, arXivPrepr
 Dynamic ChannelPruning: Feature Boosting and Suppression,2018, 7th Int
 Block-wise DynamicSparseness,2020, arXiv Prepr
 Multiplicative Interactionsand Where to Find Them,2019, ICLR
 Top-KAST:Top-K Always Sparse Training,2020, NeurIPS
 Scaling Laws for Neural LanguageModels,2020, arXiv Prepr
 SNIP: Single-shot Network Pruningbased on Connection Sensitivity,2019, ICLR
 A Signal PropagationPerspective for Pruning Neural Networks at Initialization,2020, ICLR
 GShard: Scaling Giant Models with ConditionalComputation and Automatic Sharding,2020, arXiv Prepr
 Measuring the Intrinsic Dimensionof Objective Landscapes,2018, 6th Int
 Sparse evolutionary Deep Learning with over one million artificial neuronson commodity hardware,2019, arXiv Prepr
 Selfish Sparse RNNTraining,2021, Proc
 Learning Sparse Neural Networks throughL_0 Regularization,2018, ICLR
 A Gradient Flow Framework For Analyzing NetworkPruning,2021, ICLR
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nat
 Parameter Efficient Training of Deep Convolutional Neural Networksby Dynamic Sparse Reparameterization,2019, ICML
 GroupPruning using a Bounded-Lp norm for Group Gating and Regularization,2019, Ger
 Movement Pruning: Adaptive Sparsity byFine-Tuning,2020, NeuIPS
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, NeurIPS
 Keep the Gradients Flowing: Using GradientFlow to Study Sparse Network Optimization,2021, arXiv Prepr
 Well-Read Students Learn Better:On the Importance of Pre-training Compact Models,2019, arXiv Prepr
 Ramanujan Bipartite Graph Productsfor Efficient Block Sparse Neural Networks,2020, arXiv Prepr
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv Prepr
 DeepHoyer: Learning Sparser Neural Network with Differen-tiable Scale-Invariant Sparsity Measures,2019, ICLR
 One Shot Pruning of Recurrent Neural Networks byJacobian spectrum evaluation,2020, ICLR
