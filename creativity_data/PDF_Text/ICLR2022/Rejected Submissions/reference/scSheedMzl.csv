title,year,conference
 Data Clustering: Algorithms and Applications,2013, CRC Press
 Invariant riskminimization game,2020, In International Conference on Machine Learning
 Linear regression games: Conver-gence guarantees to approximate out-of-distribution solutions,2021, In AISTATS
 To trust or not to trust an explanation: using leaf toevaluate local linear xai methods,2021, PeerJ Computer Science
 Fairwashing explanations with off-manifold detergent,2020, In Intl
 One explanation does not fitall: A toolkit and taxonomy of AI explainability techniques,2019, arXiv:1909
 Environment inference for invariantlearning,2020, In e ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning
 A social equilibrium existence theorem,1952, Proceedings of the National Academy ofSciences
 Explanations based on the missing: Towards contrastive explanations with pertinentnegatives,2018, In Advances in Neural Information Processing Systems
 Improving simple modelswith confidence profiles,2018, In Advances in Neural Information Processing Systems
 Enhancing simple models by exploitingwhat they already know,2020, In Intl
 Strategies and games: theory and practice,1999, MIT Press
 Asymmetric shapley values: incorporating causalknowledge into model-agnostic explainability,2020, NeurIPS
 Causal shapley values: Exploitingcausal knowledge to explain individual predictions of complex models,2020, Advances in NeuralInformation Processing Systems
 A unified approach to interpreting model predictions,2017, In Advancesin Neural Information Processing Systems
 Explanation in artificial intelligence: Insights from the social sciences,2018, ArtificialIntelligence
 Thumbs up? sentiment classification usingmachine learning techniques,2002, In Proceedings ofEMNLP
 “Why should I trust you?”: Explaining thepredictions of any classifier,2016, In Proceedings of the ACM SIGKDD International Conference onKnowledge Discovery and Data Mining
 Stop explaining black box machine learning models for high stakes decisions and useinterpretable models instead,2019, Nature Mach
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2013, arXiv:1312
 Axiomatic attribution for deep networks,2017, ICML
 Causal mediation analysis for interpreting neural nlp:The case of gender bias,2020, NeurIPS
 Analysis: Article 29 working party guidelines on automateddecision making under gdpr,2018, 2018
 Notethat SHAP does not have standard errors since it is computed only once per test point,2022, The INFDvalues for SHAP are miniscule since SHAP values add up to the predictions by definition
