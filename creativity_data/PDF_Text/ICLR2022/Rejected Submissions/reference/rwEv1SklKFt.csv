title,year,conference
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Refit:A unified watermark removal framework for deep learning systems with limited data,2021, CCS
 Certified adversarial robustness via randomizedsmoothing,2019, ICML
 Adversarial robustness as a prior for learned representations,2019, arXiv preprintarXiv:1906
 Strip:A defence against trojan attacks on deep neural networks,2019, arXiv preprint arXiv:1902
 Explaining and harnessing adversarialexamples,2015, ICLR
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2020, arXiv preprint arXiv:1708
 Deep residual learning for imagerecognition,2019, CVPR
 Imagenet classification with deep convolu-tional neural networks,2019, In Advances in neural information processing systems
 Rethinking thetrigger of backdoor attack,2020, arXiv preprint arXiv:2004
 Reflection backdoor: A natural backdoor attackon deep neural networks,2020, ECCV
 Why should i trust you?: Explaining thepredictions of any classifier,2021, KDD
 Input-aware dynamic backdoor attack,2020, InH
 Rise: Randomized input sampling for explanation ofblack-box models,2019, arXiv preprint arXiv:1806
 Grad-cam: Visual explanations from deep networks via gradient-based localiza-tion,2015, ICCV
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, NeurIPS
 Denoised smoothing: Aprovable defense for pretrained classifiers,2020, NeurIPS
 Facehack: Triggering backdoored facialrecognition systems using facial characteristics,2020, arXiv preprint arXiv:2006
 Exposing backdoors in robustmachine learning models,2020, arXiv preprint arXiv:2003
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Spectral signatures in backdoor attacks,2018, NeurIPS
 Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,2019, IEEESymposium on Security and Privacy
 Practicaldetection of trojan neural networks: Data-limited and data-free cases,2020, ECCV
