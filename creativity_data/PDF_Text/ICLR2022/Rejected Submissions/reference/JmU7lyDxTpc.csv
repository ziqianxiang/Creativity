title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 High-dimensional dynamics of gener-alization error in neural networks,2020, Neural Networks
 A continuous-time view of early stopping for leastsquares regression,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 The implicit regularization of stochastic gradientflow for least squares,2020, In International Conference on Machine Learning
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, In Conference on Learning Theory
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Gener-alisation error in learning with random features and the hidden manifold model,2020, In InternationalConference on Machine Learning
 Deep learning,2016, MIT pressCambridge
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Early stopping in deep networks: Double descent andhow to eliminate it,2020, arXiv preprint arXiv:2007
 Neural tangent kernel: convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Statistical-physics-based reconstruction in compressed sensing,2012, Physical Review X
 Eigenvalues of covariance matrices: Application toneural-network learning,1991, Physical Review Letters
 Just interpolate: Kernel “ridgeless” regression can gen-eralize,0090, The Annals of Statistics
 Distribution of eigenvalues forsome sets of random matrices,1967, Matematicheskii Sbornik
 The generalization error of random features regression: preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Statistical mechanics of learning: Generalization,1995, The handbook of brain theoryand neural networks
 Statistical mechanics of generalization,1996, In Models of neuralnetworks III
 Gradient starvation: A learning proclivity in neural networks,2020, CoRR
 Fundamentals of statistical and thermal physics,2009, Waveland Press
 Statistical mechanics of learn-ing from examples,1992, Physical review A
 When and how epochwise double descent happens,2021, arXiv preprintarXiv:2108
 The nature of statistical learning theory,1998, Wiley
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 84 and by substitution of Eq,2022, 51
