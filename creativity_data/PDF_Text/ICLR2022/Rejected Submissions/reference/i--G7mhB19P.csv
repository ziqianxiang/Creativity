title,year,conference
 Neural learning in structured parameter spaces - natural riemannian gradient,1997, InM
 Natural gradient works efficiently in learning,0899, Neural Computation
 Kernelized wasserstein naturalgradient,2019, 10 2019
 Implicit regularization in deep matrixfactorization,2019, Advances in Neural Information Processing Systems
 Exact natural gradient in deep linearnetworks and its application to the nonlinear case,2018, Advances in Neural Information ProcessingSystems
 Stochas-tic training is not necessary for generalization,2021, 9 2021
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, 33rd International Conference on Machine Learning
 Implicit bias of gradient descenton linear convolutional networks,2018, Advances in Neural Information Processing Systems
 Deep residual learning for image recog-nition,2015, Proceedings of the IEEE Computer Society Conference on Computer Vision and PatternRecognition
 On the proliferation of support vectors in high dimen-sions,2020, 9 2020
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, Advances in Neural Information Processing Systems
 Fantas-tic generalization measures and where to find them,2019, In International Conference on LearningRepresentations
 Natural gradient via optimal transport,2018, Information Geometry
 A coordinate-free construction of scalable natural gradient,2018, 2018
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, 32nd International Conference on Machine Learning
 Implicit bias in deep linear classification: Initialization scale vs training ac-curacy,2020, Advances in Neural Information Processing Systems
 Revisiting natural gradient for deep networks,2013, 1st InternationalConference on Learning Representations
 The natural neural tangent kernel:Neural network training dynamics under natural gradient descent,2019, In 4th workshop on BayesianDeep Learning (NeurIPS 2019)
 Accelerating natural gradient with higher-orderinvariance,2018, 35th International Conference on Machine Learning
 Deep learning generalizes becausethe parameter-function map is biased towards simple functions,2018, 7th International Conferenceon Learning Representations
 Implicit regularization for optimalsparse recovery,2019, Advances in Neural Information Processing Systems
 Kernel and rich regimesin overparametrized models,2020, volume 125
 Fast convergence of natural gradient descentfor overparameterized neural networks,1049, Advances in Neural Information Processing Systems
