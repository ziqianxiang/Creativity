title,year,conference
 High-dimensional dynamics of gener-alization error in neural networks,2020, Neural Networks
 Limit of the smallest eigenvalue of a large dimensional samplecovariance matrix,2008, In Advances In Statistics
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Measure theory,2013, Springer
 High-dimensional asymptotics of prediction: Ridge regressionand classification,2018, The Annals of Statistics
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Eigenvalues of covariance matrices: Application toneural-network learning,1991, Physical Review Letters
 A brief prehistory ofdouble descent,2020, Proceedings of the National Academy of Sciences
 The generalization error of random features regression: Preciseasymptotics and the double descent curve,2019, Communications on Pure and Applied Mathematics
 Optimal regularization canmitigate double descent,2021, In International Conference on Learning Representations
 Principles of mathematical analysis,1976, New York
 Regularized linear regression: A preciseanalysis of the estimation error,2015, Proceedings of Machine Learning Research
 Benign overfitting in ridge regression,2020, arXiv preprintarXiv:2009
 Linear and nonlinear extension of the pseudo-inverse solu-tion for learning boolean functions,1989, EPL (Europhysics Letters)
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
