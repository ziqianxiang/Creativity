title,year,conference
 Improving out-of-distribution generalization via multi-task self-supervised pretraining,2020, arXiv preprint arXiv:2003
 Stochastic gradient descent on riemannian manifolds,2013, IEEE Transactions on AutomaticControl
 Convex optimization,2004, Cambridge university press
 Gradnorm: Gradient normalizationfor adaptive loss balancing in deep multitask networks,2018, In International Conference on Machine Learning
 Just pick a sign: Optimizing deep multitask models with gradient sign dropout,2020, In H
 Domain generalization via model-agnostic learning of semantic features,2019, arXiv preprint arXiv:1910
 Measuring andharnessing transference in multi-task learning,2020, arXiv preprint arXiv:2010
 Unsupervised domain adaptation by backpropagation,2015, In Internationalconference on machine learning
 Discretizing manifolds via minimum energy points,2004, Notices of the AMS
 Using self-supervised learning canimprove model robustness and uncertainty,2019, In H
 Rotograd: Dynamic gradient homogenization for multi-task learning,2021, arXivpreprint arXiv:2103
 Pareto efficient fairness insupervised learning: From extraction to tracing,2021, arXiv preprint arXiv:2104
 Understanding black-box predictions via influence functions,2017, In InternationalConference on Machine Learning
 Learning to generalize: Meta-learning fordomain generalization,2018, In Proceedings of the AAAI Conference on Artificial Intelligence
 Deepdomain generalization via conditional invariant adversarial networks,2018, In Proceedings of the EuropeanConference on Computer Vision (ECCV)
 Pareto multi-task learning,2019, arXivpreprint arXiv:1912
 Controllable pareto multi-task learning,2020, arXivpreprint arXiv:2010
 A learnable self-supervised taskfor unsupervised domain adaptation on point clouds,2021, arXiv preprint arXiv:2104
 Multi-task learning as multi-objective optimization,2018, In S
 Indoor segmentation and supportinference from rgbd images,2012, In European conference on computer vision
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, In H
 Unsupervised domain adaptation through self-supervision,2019, arXiv preprint arXiv:1909
 Understanding and improving information transferin multi-task learning,2020, In International Conference on Learning Representations
 In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness,2021, In InternationalConference on Learning Representations
 Multitask learning strengthens adversarial robustness,2020, 2020
 Gra-dient surgery for multi-task learning,2020, In H
 Multiobjective evolutionary algorithms: a comparative case study and thestrength pareto approach,1999, IEEE transactions on Evolutionary Computation
