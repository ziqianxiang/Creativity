title,year,conference
 Onexact computation with an infinitely wide neural net,2019, In Hanna M
 A closer look at memorization in deep networks,2017, In Doina PrecUp and Yee WhyeTeh (eds
 Rademacher and gaUssian complexities: Risk boUnds andstrUctUral resUlts,2002, JoUrnaI ofMachine Learning Research
 Reflections after refereeing papers for nips,1995, The Mathematics of Generalization
 On the global convergence of gradient descent for over-parameterized models Using optimal transport,2018, In Proceedings of the 32nd InternationalConference on NeUral Information Processing Systems
 Understanding the difficUlty of training deep feedforward neUralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 NeUral tangent kernel: Convergenceand generalization in neUral networks,2018, In Samy Bengio
 QUantifying the generalization errorin deep learning in terms of data distribUtion and neUral network smoothness,2020, NeUral Networks
 SGD on neUral networks learns fUnctions of in-creasing complexity,2019, In Hanna M
 Phase diagram for two-layer relu neuralnetworks at infinite-width limit,2021, JoUrnal OfMachine Learning Research
 The slow deterioration of the generalization error of the randomfeature model,2020, In Mathematical and Scientific Machine Learning
 Gradient descent quantizes relu networkfeatures,2018, arXiv PrePrint arXiv:1803
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, In Alina Beygelzimer and Daniel Hsu (eds
 An analytic theory of shallow networks dynamics for hinge lossclassification,2020, Advances in NeUral Information Processing Systems
 On the spectral bias of deep neural networks,2019, InternationalConference on Machine Learning
 Parameters as interacting particles: long time con-vergence and asymptotic error scaling of neural networks,2018, In Samy Bengio
 Mean field analysis of neural networks: A centrallimit theorem,2020, Stochastic Processes and their APPlications
 Training behavior of deep neural network infrequency domain,2019, InternationaI Conference on NeUraI Information Processing
 Deep frequency principle towards understanding why deeperlearning is faster,2021, In Proceedings of the AAAI Conference on ArtificiaI Intelligence
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRePresentations
