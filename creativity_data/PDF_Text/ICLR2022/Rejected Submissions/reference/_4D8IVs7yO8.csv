title,year,conference
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, CoRR
 Language models are few-shotlearners,2020, In Hugo Larochelle
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, In 9th International Conference on Learning Representations
 Learning factored representations in a deepmixture of experts,2013, arXiv preprint arXiv:1312
 Switch transformers: Scaling to trillion param-eter models with simple and efficient sparsity,2021, CoRR
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE international conference on computer vision
 Categorical reparameterization with gumbel-softmax,2017, In 5thInternational Conference on Learning Representations
 Scaling laws for neural languagemodels,2020, CoRR
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Rethinking the value ofnetwork pruning,2018, In International Conference on Learning Representations
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Ex-ploring the granularity of sparsity in convolutional neural networks,2017, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition Workshops
 Hash layers for large sparsemodels,2021, arXiv preprint arXiv:2106
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, In 5th International Conference on Learning Representations
 Learning structured sparsity indeep neural networks,2016, Advances in neural information processing systems
