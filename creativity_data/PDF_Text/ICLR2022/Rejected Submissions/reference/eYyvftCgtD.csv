title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
" Addressing ""Documentation Debt"" in machine learning research:A retrospective datasheet for BookCorpus",2021, arXiv preprint arXiv:2105
 On the opportunities and risks of foundation models,2021, CoRR
 Language models are few-shot learners,2020, arXiv preprintarXiv:2005
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 LangUage modeling with gatedconvolUtional networks,2017, In 34th International Conference on Machine Learning
 BERT: pre-training of deepbidirectional transformers for langUage Understanding,2018, arXiv preprint arXiv:1810
 Attention is not all yoU need: PUreattention loses rank doUbly exponentially with depth,2021, arXiv preprint arXiv:2103
 Rigging the lottery:Making all tickets winners,2019, arXiv preprint arXiv:1911
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 A practical survey on faster and lightertransformers,2021, arXiv preprint arXiv:2103
 Conformer: Convolution-augmentedtransformer for speech recognition,2020, arXiv preprint arXiv:2005
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Long short-term memory,1997, Neural Computation
 MobileNets: Efficient convolutional neural networks formobile vision aPPlications,2017, arXiv preprint arXiv:1704
 DeeP roots: ImProvingCNN efficiency with hierarchical filter grouPs,2017, In 2017 IEEE Conference on Computer Vision andPattern Recognition
 Dissecting the graPhcoreIPU architecture via microbenchmarking,2019, arXiv preprint arXiv:1912
 Con-vbert: ImProving bert with sPan-based dynamic convolution,2020, In Advances in Neural InformationProcessing Systems 33
 Imagenet classification with deeP convolu-tional neural networks,2012, In Advances in Neural Information Processing Systems 25
 Understanding the difficultyof training Transformers,2020, arXiv preprint arXiv:2004
 DeFINE:Deep factorized input token embeddings for neural sequence modeling,2020, In 8th InternationalConference on Learning Representations
 Languagemodels are unsupervised multitask learners,2019, 2019
 Searching for activation functions,2017, arXiv preprintarXiv:1710
 Energy and policy considerations for deeplearning in NLP,2019, In 57th Conference of the Association for Computational Linguistics
 EfficientNet: Rethinking model scaling for convolutional neuralnetworks,2019, In 36th International Conference on Machine Learning
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Lite Transformer with long-short rangeattention,2020, In 8th International Conference on Learning Representations
 Aggregated residualtransformations for deeP neural networks,2017, In 2017 IEEE Conference on Computer Vision andPatternRecognition
 Revisiting few-sampleBERT fine-tuning,2021, In International Conference on Learning Representations
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, arXiv preprint arXiv:1506
