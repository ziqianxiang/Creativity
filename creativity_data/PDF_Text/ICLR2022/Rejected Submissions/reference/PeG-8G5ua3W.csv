title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Rezero is all you need: Fast convergence at large depth,2020, arXiv preprintarXiv:2003
 Neural machine translation by jointlylearning to align and translate,2015, In Yoshua Bengio and Yann LeCun (eds
 Attentive multi-task deep rein-forcement learning,2019, 07 2019
 Language models are few-shot learners,2020, In Hugo Larochelle
 Coherent gradients: An approach to understanding generalization in gradientdescent-based optimization,2020, In International Conference on Learning Representations
 BabyAI: First steps towards grounded language learningwith a human in the loop,2019, In International Conference on Learning Representations
 Rethinking attention with per-formers,2021, In International Conference on Learning Representations
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, In International Conference on LearningRepresentations
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 An image is worth 16x16 words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Reducing transformer depth on demand withstructured dropout,2020, In International Conference on Learning Representations
 Scene memory transformer forembodied agents in long-horizon tasks,2019, In IEEE Conference on Computer Vision and PatternRecognition
 Generating sequences with recurrent neural networks,2013, CoRR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis R
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations
 Understanding the diffi-culty of training transformers,2020, arXiv preprint arXiv:2004
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 SOFT: softmax-free transformer with linear complexity,2021, CoRR
 Transformers without tears: Improving the normalization ofself-attention,2019, arXiv preprint arXiv:1910
 Stabilizingtransformers for reinforcement learning,2019, arXiv preprint arXiv:1910
 In Hanna M,2019, Wallach
 Telling bert’s full story: from local attentionto global aggregation,2020, arXiv preprint arXiv:2004
 Improving transformer models by reordering theirsublayers,2019, arXiv preprint arXiv:1911
 Learningto deceive with attention-based explanations,2019, arXiv preprint arXiv:1909
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Weight normalization: A simple reparameterization toaccelerate training of deep neural networks,2016, In Daniel D
 Efficient attention:Attention with linear complexities,2021, In IEEE Winter Conference on Applications of ComputerVision
 Mlp-mixer: An all-mlp architecture for vision,2021, CoRR
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 Attention is all you need,2017, In Isabelle Guyon
 Graph attention networks,2018, In 6th International Conference on Learning Representations
 Neural executionof graph algorithms,2020, In International Conference on Learning Representations
 Linformer: Self-attentionwith linear complexity,2020, CoRR
 Attention is not not explanation,2019, In Kentaro Inui
 Group normalization,2020, Int
 Lite transformer with long-shortrange attention,2020, In International Conference on Learning Representations
 Graph trans-former networks,2019, In Hanna M
 Deep sets,2017, In Isabelle Guyon
 Big bird: Trans-formers for longer sequences,2020, In Hugo Larochelle
 Self-attention generativeadversarial networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Predicting multicellular function through multi-layer tissuenetworks,2017, Bioinform
01-bandwidth on its own is not the problem,2022, In fact
95Entropy term strength β	0,2022,1Gradient clipping threshold	128
