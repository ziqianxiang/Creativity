title,year,conference
 Disentangling adaptivegradient methods from learning rates,2020, arXiv preprint arXiv:2002
 Revisiting resnets: Improved training and scaling strategies,2021, arXivpreprint arXiv:2103
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprint arXiv:1910
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Stochastictraining is not necessary for generalization,2021, arXiv preprint arXiv:2109
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 In-datacenter performance analysis ofa tensor processing unit,2017, In Proceedings of the 44th Annual International Symposium on ComputerArchitecture
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 MLPerf training benchmark,2019, arXiv preprint arXiv:1910
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 Descending through a crowded valley-benchmarking deep learning optimizers,2020, arXiv preprint arXiv:2007
 Optimizerbenchmarking needs to account for hyperparameter tuning,2020, In International Conference onMachine Learning
 On the origin of implicitregularization in stochastic gradient descent,2021, arXiv preprint arXiv:2101
 On the importance of initializationand momentum in deep learning,2013, In ICML
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Understanding short-horizon bias instochastic meta-optimization,2018, arXiv preprint arXiv:1803
 Image classification atsupercomputer scale,2018, arXiv preprint arXiv:1811
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, In International Conference on Learning Representations
 Which algorithmic choices matter at which batch sizes? insights froma noisy quadratic model,2019, In Advances in Neural Information Processing Systems
