title,year,conference
 Theory of reproducing kernels,1950, Transactions of the American mathematicalsociety
 Neural machine translation by jointlylearning to align and translate,2015, In 3rd International Conference on Learning Representations
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Kernel exponentialfamily estimation via doUbly dUal embedding,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 Multimodal continuous visual attentionmechanisms,2021, arXiv preprint arXiv:2104
 In H,2019, Wallach
 Behrt: transformerfor electronic health records,2020, Scientific reports
 From softmax to sparsemax: A sparse model of attentionand multi-label classification,1614, In International Conference on Machine Learning
 Sparse and continuous attention mechanisms,2020, In H
 Multi-time attention networks for irregularly sampledtime series,2021, In International Conference on Learning Representations
 Attend and diagnose:Clinical time series analysis using attention models,2018, In Proceedings of the AAAI Conference onArtificial Intelligence
 Efficient and princi-pled score estimation with nystrom kernel exponential families,2018, In International Conference onArtificial Intelligence and Statistics
 Variational learning of inducing variables in sparse gaussian processes,2009, In Artificialintelligence and statistics
 Transformer dissection: A unified understanding of transformerâ€™s attention viathe lens of kernel,2019, arXiv preprint arXiv:1908
 The latent maximum entropy principle,2012, ACMTransactions on Knowledge Discovery from Data (TKDD)
 Self-attention with functional time representation learning,2019, In H
