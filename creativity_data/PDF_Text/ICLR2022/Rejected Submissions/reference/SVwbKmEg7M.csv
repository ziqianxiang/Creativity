title,year,conference
 Unsupervised neural machinetranslation,2018, In 6th International Conference on Learning Representations
 Language models are few-shotlearners,2020, In Hugo Larochelle
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Understanding back-translation atscale,2018, In Ellen Riloff
 Scaling laws for autoregressive generative modeling,2020, CoRR
 Controllable textgeneration,2017, CoRR
 Scaling laws for neural languagemodels,2020, CoRR
 Unsupervised bitext mining andtranslation via self-trained contextual embeddings,2020, Trans
 Inducing crosslingual distributed represen-tations of words,2012, In Martin Kay and Christian Boitet (eds
 Data augmentation using pre-trained trans-former models,2020, CoRR
 Multilingual denoising pre-training for neural machine transla-tion,2020, Trans
 XLM-T: scaling up multilingual machine translation with pretrained cross-lingualtransformer encoders,2020, CoRR
 Self-training improves pre-training for few-shot learning in task-oriented dialog systems,2021, CoRR
 Cross-modelback-translated distillation for unsupervised machine translation,2021, In Marina Meila and TongZhang (eds
 A call for clarity in reporting BLEU scores,2018, In Ondrej Bojar
 LangUagemodels are UnsUpervised mUltitask learners,2019, OpenAI blog
 Deciphering foreign langUage,2011, In Dekang Lin
 Generating datasets with pretrained language models,2021, CoRR
 Self-diagnosis and self-debiasing: A proposalfor redUcing corpUs-based bias in NLP,2021, CoRR
 Improving neUral machine translation mod-els with monolingUal data,2016, In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics
 Style transfer from non-parallel text by cross-alignment,2017, In Isabelle GUyon
 MASS: masked sequence to se-quence pre-training for language generation,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov(eds
 Extracting andcomposing robust features with denoising autoencoders,2008, In William W
 Strata: Self-training withtask augmentation for better few-shot learning,2021, CoRR
 Bilingual word embeddings from non-parallel document-aligned data applied to bilingual lexicon induction,2015, In Proceedings of the 53rd Annual Meetingof the Association for Computational Linguistics and the 7th International Joint Conference onNatural Language Processing of the Asian Federation of Natural Language Processing
 Denoising based sequence-to-sequence pre-training for text generation,2019, In Kentaro Inui
 Generalizing from a few ex-amples: A survey on few-shot learning,2020, ACM Comput
 Ccnet: Extracting high quality monolingual datasets fromWeb crawl data,2020, In Nicoletta Calzolari
 mt5: A massively multilingual pre-trained text-to-text transformer,2021, InKristina Toutanova
 G-daug: Generative dataaugmentation for commonsense reasoning,2020, In Trevor Cohn
