title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Andreas Krause and Jennifer Dy (eds
 Compressingneural networks with the hashing trick,2015, In Francis Bach and David Blei (eds
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, In Z
 Sparse networks from scratch: Faster training without losingperformance,2019, CoRR
 Rigging the lottery:Making all tickets winners,2020, In Proceedings of the 37th International Conference on MachineLearning
 Compressing deep convolutionalnetworks using vector quantization,2014, CoRR
 Optimal brain surgeon and general network pruning,1993, InIEEE International Conference on Neural Networks
 Deep residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Distilling the knowledge in a neural network,2015, InNIPS Deep Learning and Representation Learning Workshop
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Speeding up convolutional neural networkswith low rank expansions,2014, In Proceedings of the British Machine Vision Conference
 Fast convnets using group-wise brain damage,2016, In 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Gradient-based learning applied to document recog-nition,1998, Proceedings of the IEEE
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems
 Layer-adaptive sparsity forthe magnitude-based pruning,2021, In International Conference on Learning Representations
 Snip: single-shot network pruningbased on connection sensitivity,2019, In 7th International Conference on Learning Representations
 A signal propa-gation perspective for pruning neural networks at initialization,2020, In International Conference onLearning Representations
 Pruning filters forefficient convnets,2017, In 5th International Conference on Learning Representations
 Learn-ing efficient convolutional networks through network slimming,2017, In 2017 IEEE InternationalConference on Computer Vision (ICCV)
 Rethinking the value ofnetwork pruning,2019, In ICLR
 Learning sparse neural networks throughl0 regularization,2018, In International Conference on Learning Representations
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature Communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In Proceedings of the 36th International Conference onMachine Learning
 Exploring sparsity in recur-rent neural networks,2017, In 5th International Conference on Learning Representations
 The roleof over-parametrization in generalization of neural networks,2019, In International Conference onLearning Representations
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, CoRR
 Very deep convolutional networks for large-scale imagerecognition,2015, In Yoshua Bengio and Yann LeCun (eds
 In H,2020, Larochelle
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, In H
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
