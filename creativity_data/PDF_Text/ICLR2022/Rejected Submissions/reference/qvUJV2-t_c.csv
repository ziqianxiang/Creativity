title,year,conference
 Training neural networks for and byinterpolation,2020, ICML
 Empirical study towards understanding line search approx-imations for training neural networks,2019, arXiv
 Big batch SGD: automatedinference using adaptive batch sizes,2016, arXiv
 ImageNet: A Large-Scale HierarchicalImage Database,2009, CVPR
 Essentially no bar-riers in neural network energy landscape,2018, ICML
 Adaptive subgradient methods for online learning andstochastic optimization,2011, J
 Qualitatively characterizing neural networkoptimization problems,2015, ICLR
 Deep residual learning for image recog-nition,2016, CVPR
 Search-ing for mobilenetv3,2019, ICCV
 Densely connectedconvolutional networks,2017, CVPR
 Gradient-only line searches: An alternative to probabilistic linesearches,2019, arXiv
 Adam: A method for stochastic optimization,2015, ICLR
 SGDR: stochastic gradient descent with warm restarts,2017, ICLR
 An empirical model oflarge-batch training,2018, arXiv
 The singularity and machine ethics,2012, In Singularity Hypotheses
 Parabolic approximation line search for dnns,2020, NeurIPS
 Readingdigits in natural images with unsupervised feature learning,2011, NeurIPS Workshop
 A stochastic approximation method,1951, Annals of Mathematical Statistics
 Cyclical learning rates for training neural networks,2017, WACV
 A bayesian perspective on generalization and stochastic gradientdescent,2018, ICLR
 A walk with sgd,2018, arXiv
