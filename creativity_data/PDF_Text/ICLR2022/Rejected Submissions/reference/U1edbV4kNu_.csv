title,year,conference
 A refined laser method and faster matrix multiplica-tion,2021, In SODA
 Layer normalization,2016, CoRR
 Demystifying parallel and distributed deep learning: An in-depth concurrency analysis,0360, ACM Comput
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Deep learningwith cots hpc systems,2013, In Sanjoy Dasgupta and David McAllester (eds
 Training deep neural networks withlow precision multiplications,2014, arXiv preprint arXiv:1412
 Coatnet: Marrying convolution andattention for all data sizes,2021, ArXiv
 8-bit approximations for parallelism in deep learning,2015, ICLR
 Diffusion models beat gans on image synthesis,2021, CoRR
 Understanding the efficiency of gpualgorithms for matrix-matrix multiplication,2004, pp
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Neocognitron: A self-organizing neural network model for a mechanism ofpattern recognition unaffected by shift in position,1980, Biological Cybernetics
 Discorsi e dimostrazioni matematiche intorno a due nuove scienze,1638, 1638
 Algorithm 799: revolve: an implementation of check-pointing for the reverse or adjoint mode of computational differentiation,2000, ACM Transactions onMathematical Software (TOMS)
 Deep residUal learning for imagerecognition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Adaptive mixturesof local experts,0899, Neural Computation
 Beyond data and model parallelism for deep neuralnetworks,2019, In A
 Large memory layers with product keys,2019, In H
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations
 Gshard: Scaling giant models with conditional computation and automaticsharding,2020, ArXiv
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In I
 Deep Gradient Compression:Reducing the communication bandwidth for distributed training,2018, In The International Conferenceon Learning Representations
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation: Research Papers
 Traininglarge neural networks with constant memory using a new execution algorithm,2020, arXiv preprintarXiv:2002
 Binary neuralnetworks: A survey,2020, CoRR
 Improving languageunderstanding by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, 2019
 Zero: Memory optimizationtowards training a trillion parameter models,2020, In SC
 Zero-infinity:Breaking the gpu memory wall for extreme scale deep learning,2021, arXiv preprint arXiv:2104
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In Bastian Leibe
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Learning representations byback-propagating errors,1986, Nature
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Mesh-tensorflow: Deep learning for supercomputers,2018, CoRR
 Megatron-lm: Training multi-billion parameter language models using gpu modelparallelism,2019, arXiv preprint arXiv:1909
 1-bit adam: Communication efficient large-scale trainingwith adamâ€™s convergence speed,2021, In Marina Meila and Tong Zhang (eds
 Attention is all you need,2017, In I
 Bppsa: Scaling back-propagation by parallel scanalgorithm,2020, In I
 Pipemare: Asynchronous pipeline parallel dnn training,2019, ArXiv
 Matrix multiplication on high-density multi-gpu architectures:Theoretical and experimental investigations,2015, volume 9137
 Our bandwidth experiments in the main paper give some idea what the networkoverhead is,2022, By using no quantization
