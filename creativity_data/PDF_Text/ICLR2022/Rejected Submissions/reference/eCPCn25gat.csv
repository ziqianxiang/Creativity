title,year,conference
 Decision transformer: Reinforcement learningvia sequence modeling,2021, arXiv preprint arXiv:2106
 Reinforcement learning as one big sequence mod-eling problem,2021, arXiv preprint arXiv:2106
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Reinforcement learning with pro-totypical representations,2021, arXiv preprint arXiv:2102
 Buildingmachines that learn and think like people,2017, Behavioral and Brain Sciences
 CLIPort: What and where pathways forrobotic manipulation,2021, In 5th Annual Conference on Robot Learning
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Language-conditioned imitation learning for robot manipulation tasks,2020, arXiv preprintarXiv:2010
 Imitating interactive intelli-gence,2020, arXiv preprint arXiv:2012
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, arXiv preprint arXiv:1908
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Pretrained transformers as universalcomputation engines,2021, arXiv preprint arXiv:2103
 Multi-modal few-shot learning with frozen language models,2021, arXiv preprint arXiv:2106
