title,year,conference
 Teaching a GAN What Not to Learn,2020, In Proc
 Measuring neu-ral net robustness with constraints,2016, In Proc
 Certifiably adversarially robust detection of out-of-distribution data,2020, In Proc
 Revisiting Out-of-Distribution Detection:A Simple Baseline is Surprisingly Effective,2021, Workshop
 ATOM: Robustifying Out-of-distribution DetectionUsing Outlier Mining,2021, In Proc
 Reliable evaluation of adversarial robustness with an ensemble of diverseparameter-free attacks,2020, In Proc
 AutoAugment: Learning AugmentationPolicies from Data,2018, arXiv:1805
 Good semi-supervised learning thatrequires a bad GAN,2017, In Proc
 OMASGAN: Out-of-Distribution Minimum AnomalyScore GAN for Sample Generation on the Boundary,2021, arXiv preprint
 Generative Adversarial Nets,2014, In Proc
 Why ReLU networks yield high-confidence predic-tions far away from the training data and how to mitigate the problem,2019, In Proc
 Deep anomaly detection with outlier exposure,2019, InProc
 The robust manifold defense: Adversarial trainingusing generative models,2017, arXiv:1712
 OOD-MAML: Meta-learning for few-shot out-of-distribution detection andclassification,2020, In Proc
 Attribution-basedconfidence metric for deep neural networks,2019, In Proc
 Quantifying perceptual distortion ofadversarial examples,2019, arXiv:1902
 Detecting OoDs as datapoints with HighUncertainty,2021, Workshop ICML
 Are all outliers alike? On Understanding theDiversity of Outliers for Detecting OODs,2021, arXiv:2103
 Why Normalizing Flows Fail to Detect Out-of-Distribution Data,2020, In Proc
 Learning multiple layers of features from tiny images,2009, 2009
 The MNIST database of handwritten digits,1998, 1998
 Training confidence-calibrated classifiers for detecting out-of-distribution samples,2018, In Proc
 A simple unified framework for detecting out-of-distributionsamples and adversarial attacks,2018, In Proc
 Mode seeking generative adversarial networks fordiverse image synthesis,2019, In Proc
 Confidence-aware learning for deep neural networks,2020, InProc
 Poisoning attacks withgenerative adversarial nets,2019, arXiv:1906
 Reading digits in natural imageswith unsupervised feature learning,2011, In Workshop
 G2D: Gen-erate to Detect Anomaly,2021, In Proc
 A General Framework For DetectingAnomalous Inputs to DNN Classifiers,2021, In Proc
 Towards Robust Artificial Intelligence Systems,2020, PhD Doctoral Thesis
 A Simple Fix to MahalanobisDistance for Improving Near-OOD Detection,2021, In Proc
 Few-Shot Learning for Defence and Security,2020, In Proc
 Certified robustness to label-flipping attacksvia randomized smoothing,2020, In Proc
 Adversarially Learned One-Class Classifierfor Novelty Detection,2018, In Proc
 Improved techniquesfor training GANs,2016, arXiv:1606
 Evidential Deep Learning to Quantify ClassificationUncertainty,2018, In Proc
 Uncertainty-aware deep classifiers using generativemodels,2020, In Proc
 Learning and evaluating representations for deepone-class classification,2021, In Proc
 Building robust classifiers through generation of confident out ofdistribution examples,2018, arXiv:1812
 Difference-Seeking GAN - Unseen Sample Generation,2020, InProc
 CSI: Novelty detection via contrastive learning on distribu-tionally shifted instances,2020, In Proc
 Uncertainty estimation using a single deep deter-ministic neural network,2020, In Proc
 Out-of-distributiondetection in classifiers via generation,2019, arXiv:1910
 Analysis ofconfident-classifiers for out-of-distribution detection,2019, arXiv:1904
 Out-of-distribution Detection in Few-shotClassification,2020, Submitted to ICLR
 Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm,2020, In Proc
