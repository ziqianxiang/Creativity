title,year,conference
 Speechstew:Simply mix all available speech recognition data to train one large neural network,2021, arXiv preprintarXiv:2104
 W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,2021, arXiv preprint arXiv:2108
 Selection via proxy: Efficient data selection for deeplearning,2019, arXiv preprint arXiv:1906
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Connectionist tem-poral classification: labelling unsegmented sequence data with recurrent neural networks,2006, InProceedings ofthe 23rd international conference on Machine learning
 Span-bert: Improving pre-training by representing and predicting spans,2020, Transactions ofthe Associationfor Computational Linguistics
 Librispeech: an asr corpusbased on public domain audio books,2015, In 2015 IEEE international conference on acoustics
 Improved noisy student training for automatic speech recognition,2020, arXiv preprintarXiv:2005
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 Attention is all you need,2017, In Advances in neural informationprocessing SyStemS
 Semi-supervised dnn training with word selectionfor asr,2017, In Interspeech
 Confidence measures for largevocabulary continuous speech recognition,2001, IEEE Transactions on speech and audio processing
