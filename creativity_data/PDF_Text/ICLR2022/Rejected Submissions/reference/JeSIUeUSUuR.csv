title,year,conference
 A multilayer neural network with piecewise-linear structure and back-propagationlearning,1991, IEEE Transactions on Neural Networks
 A downsampled variant of imagenet as analternative to the cifar datasets,2017, arXiv preprint arXiv:1707
 An imageis worth 16x16 words: Transformers for image recognition at scale,2020, In International Conferenceon Learning Representations
 Deep learning versus kernel learning: an empirical study of loss landscapegeometry and the time evolution of the neural tangent kernel,2020, arXiv preprint arXiv:2010
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Expressivity of deep neural networks,2020, arXivpreprint arXiv:2007
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Provable benefit of orthogonal initialization inoptimizing deep linear networks,2019, In International Conference on Learning Representations
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Aann: Absolute artificial neural network,2018, In 2018 3rd International Conferencefor Convergence in Technology (I2CT)
 Deep learning without poor local minima,2016, arXiv preprint arXiv:1605
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Explaining landscape connectivity of low-cost solutions for multilayer nets,2019, arXivpreprint arXiv:1906
 Deep linear networks with arbitrary loss: All local minima areglobal,2018, In International conference on machine learning
 Visualizing the loss landscapeof neural nets,2017, arXiv preprint arXiv:1712
 Canonical piecewise-linear approximations,1992, IEEE Transactions onCircuits and Systems I: Fundamental Theory and Applications
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 Efficient orthogonalparametrisation of recurrent neural networks using householder reflections,2017, In InternationalConference on Machine Learning
 The global landscape ofneural networks: An overview,2020, IEEE Signal Processing Magazine
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 On orthogonality and learningrecurrent networks with long term dependencies,2017, In International Conference on Machine Learning
 Full-capacityunitary recurrent neural networks,2016, In NIPS
 Earlyconvolutions help transformers see better,2021, arXiv preprint arXiv:2106
 Stabilizing gradients for deep neural networks via efficientsvd parameterization,2018, In International Conference on Machine Learning
