title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Model compression,2006, In KDD ’06
 Making neural machine reading comprehension faster,2019, arXiv preprintarXiv:1904
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Newsroom: A dataset of 1,2018,3 million summaries withdiverse extractive strategies
 Deep learning withlimited numerical precision,1737, In International conference on machine learning
 Teaching machines to read and comprehend,2015, Advances in neuralinformation processing systems
 Long short-term memory,1997, Neural computation
 Dynabert: Dynamic bertwith adaptive width and depth,2020, arXiv preprint arXiv:2004
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 On the effect of dropping layers ofpre-trained transformer models,2020, arXiv preprint arXiv:2004
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, arXiv preprint arXiv:1909
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Edgebert: Sentence-level energyoptimizations for latency-aWare multi-task nlp inference,2020, arXiv preprint arXiv:2011
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attentionWith linear complexity,2020, arXiv preprint arXiv:2006
 Google’s neural machine trans-lation system: Bridging the gap betWeen human and machine translation,2016, arXiv preprintarXiv:1609
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
 Bertscore: Evaluat-ing text generation With bert,2019, arXiv preprint arXiv:1904
