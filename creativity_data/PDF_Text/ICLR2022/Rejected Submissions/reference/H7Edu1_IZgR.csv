title,year,conference
 Learning to learn by gradient de-scent by gradient descent,2016, In D
 Working memory,0960, Current Biology
 Language models are few-shot learners,2020, CoRR
 Learning to learn without gradient descent by gra-dient descent,2017, In Doina Precup and Yee Whye Teh (eds
 An image is worth 16x16 words: Transformers for image recognitionat scale,2021, In International Conference on Learning Representations
 Probabilistic model-agnostic meta-learning,2018, InProceedings of the 32nd International Conference on Neural Information Processing Systems
 Generalization of reinforcementlearners with working and episodic memory,2019, In H
 Understanding the difficulty of training deep feedforward neu-ral networks,2010, In In Proceedings of the International Conference on Artificial Intelligence andStatistics (AISTATSâ€™10)
 Refresh mymemory: Episodic memory reinstatements intrude on working memory maintenance,2018, bioRxiv
 Improving transformer optimiza-tion through better initialization,2020, In Hal DaUme In and Aarti Singh (eds
 Episodic control throughmeta-reinforcement learning,2018, In CogSci
 Simple principles of metalearning,1996, Technicalreport
 Auto-prompt: Eliciting knowledge from language models with automatically generated prompts,2020, CoRR
 Match-ing networks for one shot learning,2016, In D
 Prefrontal cortex as a meta-reinforcement learn-ing system,2018, bioRxiv
 Modeling the role of working memory and episodic memoryin behavioral tasks,2008, HiPPocamPus
 Neural architecture search with reinforcement learning,2017, 2017
