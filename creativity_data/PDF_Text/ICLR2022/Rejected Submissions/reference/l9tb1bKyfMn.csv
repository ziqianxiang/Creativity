title,year,conference
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Cswin transformer: A general vision transformer backbone with cross-shapedwindows,2021, arXiv preprint arXiv:2107
 Turbotransformers: An efficient gpu servingsystem for transformer models,2020, arXiv preprint arXiv:2010
 Caltech-256 object category dataset,2007, 2007
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 Transformers in vision: A survey,2021, arXiv preprint arXiv:2101
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 A critical review of recurrent neural networksfor sequence learning,2015, arXiv preprint arXiv:1506
 Generating wikiPedia by summarizing long sequences,2018, arXiv preprintarXiv:1801
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Understanding the effective recePtivefield in deeP convolutional neural networks,2016, In Proceedings of the 30th International Conferenceon Neural Information Processing Systems
 Word sense disambiguation: Aunified evaluation framework and emPirical comParison,2017, In Proceedings of the 15th Conference ofthe European Chapter of the Association for Computational Linguistics: Volume 1
 Rethink-ing the incePtion architecture for comPuter vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Why self-attention? a targetedevaluation of neural machine translation architectures,2018, arXiv preprint arXiv:1808
 Synthesizer: Rethinking self-attention intransformer models,2020, arxiv 2020
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Non-local neural networks,2018, InProceedings of the IEEE conference on computer vision and pattern recognition
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Seg-former: Simple and efficient design for semantic segmentation with transformers,2021, arXiv preprintarXiv:2105
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Sceneparsing through ade20k dataset,2017, In Proceedings of the IEEE conference on computer vision andpattern recognition
