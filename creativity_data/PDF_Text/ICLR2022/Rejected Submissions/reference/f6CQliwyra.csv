title,year,conference
 Regret bounds for the adaptive control of linearquadratic systems,2011, In Proceedings of the 24th Annual Conference on Learning Theory
 Flambe: Structural complex-ity and representation learning of low rank mdps,2020, arXiv preprint arXiv:2006
 Optimality and approximationwith policy gradient methods in markov decision processes,2020, In Conference on Learning Theory
 Kernels for vector-valued functions:A review,2012, Foundations and TrendsÂ® in Machine Learning
 Fitted q-iteration in continuous action-spacemdps,2008, In J
 Theory of reproducing kernels,1950, Transactions of the American mathematicalsociety
 Minimax regret bounds for reinforce-ment learning,2017, In International Conference on Machine Learning
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 Scalable methods for computing state similarity in deterministic markovdecision processes,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
 Convergence of langevin mcmc in kl-divergence,2018, In AlgorithmicLearning Theory
 Deep reinforcement learn-ing in a handful of trials using probabilistic dynamics models,2018, arXiv preprint arXiv:1805
 Scal-able kernel methods via doubly stochastic gradients,2014, Advances in Neural Information ProcessingSystems
 Stochastic linear optimization under banditfeedback,2008, 2008
 Improving generalization for temporal difference learning: The successor representa-tion,1993, Neural Computation
 State aggregation learning from markov transitiondata,2018, arXiv preprint arXiv:1811
 Deepmdp:Learning continuous latent space models for representation learning,2019, In International Conferenceon Machine Learning
 Probability and random processes,2020, Oxford universitypress
 Learning latent dynamics for planning from pixels,2019, In International Conference onMachine Learning
 Towards automatic evaluation ofdialog systems: A model-free off-policy evaluation approach,2021, arXiv preprint arXiv:2102
 A shortnote on concentration inequalities for random vectors with subgaussian norm,2019, arXiv preprintarXiv:1902
 Infor-mation theoretic regret bounds for online nonlinear control,2020, arXiv preprint arXiv:2006
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Deep successorreinforcement learning,2016, arXiv preprint arXiv:1606
 Model-ensembletrust-region policy optimization,2018, arXiv preprint arXiv:1802
 Algorith-mic framework for model-based deep reinforcement learning with theoretical guarantees,2018, arXivpreprint arXiv:1807
 Certainty equivalence is efficient for linear quadraticcontrol,2019, In Proceedings of the 33rd International Conference on Neural Information ProcessingSystems
 Active learning for nonlinear system identifi-cation with guarantees,2020, arXiv preprint arXiv:2006
 Playing atari with deep reinforcement learning,2013, arXiv preprintarXiv:1312
 Model-freerepresentation learning and exploration in low-rank mdps,2021, arXiv preprint arXiv:2102
 Mcmc using hamiltonian dynamics,2011, Handbook of markov chain monte carlo
 Random features for large-scale kernel machines,2007, In Proceedingsof the 20th International Conference on Neural Information Processing Systems
 Eluder dimension and the sample complexity of optimisticexploration,2013, Advances in Neural Information Processing Systems
 Learning to optimize via posterior sampling,2014, Mathematics ofOperations Research
 Mastering the game of gowithout human knowledge,2017, nature
 Naive exploration is optimal for online lqr,2020, In InternationalConference on Machine Learning
 Real analysis,2009, Princeton University Press
 Reinforcement learning: An introduction,2018, MIT press
 Reinforcement learning with general valuefunction approximation: Provably efficient approach via bounded eluder dimension,2020, Advances inNeural Information Processing Systems
 Benchmarking model-based reinforce-ment learning,2019, arXiv preprint arXiv:1907
 Embed tocontrol: A locally linear latent dynamics model for control from raw images,2015, arXiv preprintarXiv:1506
 The laplacian in rl: Learning representations withefficient approximations,2018, arXiv preprint arXiv:1810
 Mopo: Model-based offline policy optimization,2020, arXiv preprintarXiv:2005
 Spectral state compression of markov processes,2019, IEEE transactionson information theory
 Our method achievestrong performance even comparing to pure empirical baselines,2019, To be specific
