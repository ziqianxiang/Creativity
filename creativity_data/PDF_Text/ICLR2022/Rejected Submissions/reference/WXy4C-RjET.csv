title,year,conference
 Spectrally-normalized margin bounds forneural networks,2017, CoRR
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Sharp minima can generalize fordeep nets,2017, CoRR
 Learning image and user featuresfor recommendation in social networks,2015, In Proceedings of the IEEE International Conference onComputer Vision
 On calibration of modern neuralnetworks,2017, CoRR
 Comparing biases for minimal network construction with back-propagation,1988, Advances in neural information processing systems
 The movielens datasets: History and context,2015, ACMtransactions on interactive intelligent systems (TIIS)
 Deep residual learning for imagerecognition,2015, CoRR
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, arXiv preprint arXiv:1803
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In 8th International Conference on Learning Representations
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Imagenet classification with deepconvolutional neural networks,2012, Advances in neural information processing systems
 Focal loss for denseobject detection,2017, CoRR
 SGDR: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Calibrating deep neural networks using focal loss,2020, CoRR
 Norm-based capacity control in neuralnetworks,2015, CoRR
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, arXiv preprint arXiv:1506
 Weight normalization: A simple reparameterization toaccelerate training of deep neural networks,2016, arXiv preprint arXiv:1602
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The marginalvalue of adaptive gradient methods in machine learning,2017, arXiv preprint arXiv:1705
 Onthe noisy gradient descent that generalizes as sgd,2020, In ICML
 Large-batchtraining for lstm and beyond,2019, In Proceedings of the International Conference for High PerformanceComputing
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
