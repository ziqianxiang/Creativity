title,year,conference
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, InternationalConference in Machine Learning
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, CoRR
 Algorithms for the matrix pth root,2005, NumericalAlgorithms
 Model compression and hardwareacceleration for neural networks: A comprehensive survey,2020, Proceedings of the IEEE
 Dynamic channelpruning: Feature boosting and suppression,2018, arXiv preprint arXiv:1810
 Dynamic network surgery for efficient dnns,2016, In Advancesin neural information processing systems
 Bandwidth-efficient deep learning,2018, In Proceedings of the 55thAnnual Design Automation Conference on
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv preprint arXiv:1808
 Filter pruning via geometric medianfor deep convolutional neural networks acceleration,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Iterative normalization: Beyond standardizationtowards efficient whitening,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Categorical reparameterization with gumbel-softmax,2017, 2017
 Learning multiple layers of features from tiny images,2009, Technical report
 Optimal Brain Damage,1990, In NIPS
 Pruning Filters forEfficient ConvNets,2017, In ICLR
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Learning sparse neural networks throughl_0 regularization,2017, International Conference on Learning Representation
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Dsa: Moreefficient budgeted pruning via differentiable sparsity allocation,2020, arXiv preprint arXiv:2004
 Film: Visualreasoning with a general conditioning layer,2018, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2017, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Structured probabilistic pruning forconvolutional neural network acceleration,2017, arXiv preprint arXiv:1709
 Deepmultimodal fusion by channel exchanging,2020, Advances in Neural Information Processing Systems
 Learning structured sparsity indeep neural networks,2016, In Proceedings of the 30th International Conference on Neural InformationProcessing Systems
 Deephoyer: Learning sparser neural network with differentiablescale-invariant sparsity measures,2019, arXiv preprint arXiv:1908
 Cross-channel com-munication networks,2019, In Advances in Neural Information Processing Systems
 Good subnetworksprovably exist: Pruning via greedy forward selection,2020, In International Conference on MachineLearning
 Variationalconvolutional neural network pruning,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Discrimination-aware channel pruning for deep neural networks,2018, arXiv preprintarXiv:1810
 From Eqn,2022,(11)
1 and is multiplied by 0,2019,1 after 50
