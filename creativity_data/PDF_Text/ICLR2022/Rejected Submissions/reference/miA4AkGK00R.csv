title,year,conference
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems (NeurIPS)
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Proceedings of the 35th International Conference onMachine Learning (ICML)
 First-Order Methods in Optimization,2017, Society for Industrial and Applied Mathematics
 On biased compressionfor distributed learning,2020, arXiv preprint arXiv:2002
 Curiously fast convergence of some stochastic gradient descent algorithms,2009, InProceedings of the symposium on learning and data science
 LIBSVM: a library for support vector machines,2011, ACMTransactions on Intelligent Systems and Technology (TIST)
 Linearly convergingerror compensated SGD,2020, In 34th Conference on Neural Information Processing Systems (NeurIPS)
 MARINA: Faster non-convex distributed learning with compression,2021, In International Conference on Machine Learning
 Variance-reduced methods formachine learning,2020, Proceedings of the IEEE
 SGD: General analysis and improved rates,2019, In International Conference on MachineLearning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 A better alternative to error feedback for CommUnication-efficient distributed learning,2021, In 9th International Conference on Learning Representations (ICLR)
 Natural compression for distributed deep learning,2019, arXiv preprint arXiv:1905
 Distributed second order methods with fast ratesand compressed communication,2021, arXiv preprint arXiv:2102
 Advances and open problems in federated learning,2019, arXiv preprintarXiv:1912
 Error feedback fixesSignSGD and other gradient compression schemes,2019, In 36th International Conference on MachineLearning (ICML)
 SCAFFOLD: Stochastic controlled averaging for federated learning,2020, InProceedings of the 37th International Conference on Machine Learning
 Gradient descent with compressed iterates,2019, In NeurIPS Workshopon Federated Learning for Data Privacy and Confidentiality
 Better theory for SGD in the nonconvex world,2020, arXiv preprintarXiv:2002
 Distributed learning withcompressed gradients,2018, arXiv preprint arXiv:1806
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Decentralized deep learning with arbitrarycommunication compression,2020, In International Conference on Learning Representations (ICLR)
 An optimal randomized incremental gradient method,2015, arXiv preprintarXiv:1507
 A unified variance-reduced accelerated gradient method forconvex optimization,2019, In Advances in Neural Information Processing Systems
 ANITA: An optimal loopless accelerated variance-reduced gradient method,2021, arXiv preprintarXiv:2103
 A short note of page: Optimal convergence rates for nonconvex optimization,2021, arXivpreprint arXiv:2106
 A simple proximal stochastic gradient method for nonsmooth nonconvexoptimization,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 A unified analysis of stochastic gradient methods for nonconvexfederated optimization,2020, arXiv preprint arXiv:2006
 CANITA: Faster rates for distributed convex optimization withcommunication compression,2021, arXiv preprint arXiv:2107
 ZeroSARAH: Efficient nonconvex finite-sum optimization with zerofull gradient computation,2021, arXiv preprint arXiv:2103
 Acceleration for compressed gradientdescent in distributed and federated optimization,2020, In International Conference on Machine Learning(ICML)
 PAGE: A simple and optimalprobabilistic gradient estimator for nonconvex optimization,2021, In International Conference onMachine Learning (ICML)
 A topological property of real analytic subsets,1963, Coll
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 Random reshuffling: Simple analysiswith vast improvements,2020, In H
 Bidirectional compression in heterogeneous settingsfor distributed or federated learning with partial participation: tight convergence guarantees,2020, arXivpreprint arXiv:2006
 Gradient methods for the minimisation of functionals,1963, USSR ComputationalMathematics and Mathematical Physics
 Coordinate descent with arbitrary sampling ii: Expected separableoverapproximation,2014, arXiv preprint arXiv:1412
 FedNL: Making Newton-typemethods applicable to federated learning,2021, arXiv preprint arXiv:2106
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 Understanding machine learning: from theory to algo-rithms,2014, Cambridge University Press
 Sparsified SGD with memory,2018, In Advancesin Neural Information Processing Systems (NeurIPS)
 DoubleSqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2020, In Proceedings of the 36thInternational Conference on Machine Learning (ICML)
 Spiderboost and momentum: Fasterstochastic variance reduction algorithms,2018, arXiv preprint arXiv:1810
 CSER:Communication-efficient SGD with error reset,2020, In Advances in Neural Information ProcessingSystems (NeurIPS) 
 Achieving linear speedup with partial worker participationin non-iid federated learning,2021, arXiv preprint arXiv:2101
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2016, arXiv preprint arXiv:1604
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 FedPAGE: A fast local stochastic gradient method forcommunication-efficient federated learning,2021, arXiv preprint arXiv:2108
 Consider the setting described in Example 1,2022, Let assumptions of Theorem 3 hold
 The proof is the same as for Corollary 3,2022, The only difference is that Lemma 6 is needed toupper bound the quantities 1∕θp and B∕θp
