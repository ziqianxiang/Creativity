title,year,conference
 ELECTRA: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 Revisiting pre-trained models for chinese natural language processing,2020, In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Processing: Findings
 Unified language model pre-training for natural language understandingand generation,2019, In Proceedings of the 33rd International Conference on Neural InformationProcessing Systems
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Self-knowledge distillation in natural language processing,2019, InProceedings of the International Conference on Recent Advances in Natural Language Processing(RANLP 2019)
 Pre-training universal language representation,2021, In Proceedings of the 59thAnnual Meeting of the Association for Computational Linguistics and the 11th International JointConference on Natural Language Processing (Volume 1: Long Papers)
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Wordnet: a lexical database for english,1995, Communications of the ACM
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 Mpnet: Masked and permuted pre-training for language understanding,2020, arXiv preprint arXiv:2004
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 Struct-bert: Incorporating language structures into pre-training for deep language understanding,2019, InInternational Conference on Learning Representations
 Textflint: Unified multilingual robustness evaluation toolkitfor natural language processing,2021, In Proceedings of the 59th Annual Meeting of the Associationfor Computational Linguistics and the 11th International Joint Conference on Natural LanguageProcessing: System Demonstrations
 Beyond bleu: Trainingneural machine translation with semantic similarity,2019, In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics
 Tod-bert: Pre-trainednatural language understanding for task-oriented dialogue,2020, In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Processing (EMNLP)
 Dialogue-oriented pre-training,2021, arXiv preprint arXiv:2106
 Syntax-enhanced pre-trained model,2021, In Proceedings of the59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers)
 Frustratinglysimple pretraining alternatives to masked language modeling,2021, arXiv preprint arXiv:2109
 Dialogpt: Large-scale generative pre-training for conver-sational response generation,2020, In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics: System Demonstrations
 Self-distillation as instance-specific label smoothing,2020, Advances inNeural Information Processing Systems
 SG-Net: Syn-tax guided transformer for language representation,2020, IEEE Transactions on Pattern Analysis andMachine Intelligence
 LIMIT-BERT: Linguistics in-formed multi-task bert,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: Findings
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
