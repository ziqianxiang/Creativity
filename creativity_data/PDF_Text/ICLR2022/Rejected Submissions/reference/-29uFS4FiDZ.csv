title,year,conference
 Breaking sticks andambiguities with adaptive skip-gram,2016, In artificial intelligence and statistics
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Disambiguated skip-gram model,2018, In Proceedings ofthe 2018 Conference on Empirical Methods in Natural Language Processing
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Muse: Modularizing unsupervised sense embeddings,2017, InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 Efficient non-parametric estimation of multiple embeddings per word in vector space,2014, In Proceedings of the 2014Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, 2019
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 A probabilisticmodel for learning multi-prototype word embeddings,2014, In Proceedings of COLING 2014
