title,year,conference
 Self-supervised learning by cross-modal audio-video clustering,2020, In NeurIPS
 Labelling unlabelled videos fromscratch with multi-modal self-supervision,2020, In NeurIPS
 Frozen in time: Ajoint video and image encoderfor end-to-end retrieval,2021, arXiv preprint arXiv:2104
 A short note on the kinetics-700 humanaction dataset,2019, arXiv preprint arXiv:1907
 Collecting highly parallel data for paraphrase evaluation,2011, In Proceedings ofthe 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies
 Generative pretraining from pixels,2020, InICML
 ELECTRA: Pre-training textencoders as discriminators rather than generators,2020, In ICLR
 Virtex: Learning visual representations from textual annotations,2020, arXiv preprintarXiv:2006
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2019, In ACL
 Activitynet: A large-scalevideo benchmark for human activity understanding,2015, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Preserving integrity in online social networks,2020, arXiv preprint arXiv:2009
 Billion-scale similarity search With gpus,2017, arXiv preprintarXiv:1702
 The kinetics human action video dataset,2017, arXiv preprintarXiv:1705
 Adam: A method for stochastic optimization,2015, In ICLR
 Unicoder-vl: A universal encoderfor vision and language by cross-modal pre-training,2020, In AAAI
 Visualbert: A simple andperformant baseline for vision and language,2019, arXiv preprint arXiv:1908
 Use What you have: Video retrieval usingrepresentations from collaborative experts,2019, arXiv preprint arXiv:1907
 Vilbert: Pretraining task-agnostic visiolinguistic represen-tations for vision-and-language tasks,2019, In NeurIps
 Clip4clip: An empiricalstudy of clip for end to end video clip retrieval,2021, arXiv preprint arXiv:2104
 Livebot: Generating live video comments based onvisual and textual contexts,2019, In AAAI 2019
 Audio-visual instance discrimination with cross-modalagreement,2020, arXiv preprint arXiv:2004
 Multi-modal self-supervision from generalized data transformations,2020, arXiv preprintarXiv:2003
 Support-set bottlenecks for video-text representation learning,2020, arXiv preprint arXiv:2010
 A straightforwardframework for video retrieval using clip,2021, arXiv preprint arXiv:2102
 Language models areunsupervised multitask learners,2019, OpenAI Blog
 Learning transferable visual models from natural languagesupervision,2021, arXiv preprint arXiv:2103
 Exploring the limits of transfer learning with a unified text-to-text transformer,2019, arXivpreprint arXiv:1910
 Learning multiple visual domains with residualadapters,2017, In NeurIPS
 VL-BERT: Pre-training ofgeneric visual-linguistic representations,2019, arXiv preprint arXiv:1908
 Learning video representations using contrastivebidirectional transformer,2019, arXiv preprint arXiv:1906
 Videobert: A joint model forvideo and language representation learning,2019, In ICCV
 Lxmert: Learning cross-modality encoder representations from transformers,2019, InEMNLP
 Attention is all you need,2017, In NeurIPS
 MSR-VTT: A large video description dataset for bridging video andlanguage,2016, In CVPR
 Msr-vtt: A large video description dataset for bridging video andlanguage,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Actbert: Learning global-local video-text representations,2020, In CVPR
