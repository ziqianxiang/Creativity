title,year,conference
 Beat theAI: investigating adversarial human annotation for reading comprehension,2020, Transactions of theAssociation for Computational Linguistics (TACL)
 Judge the judges: A large-scale evaluation study of neural language models for online review generation,2019, In Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Span-bert: Improving pre-training by representing and predicting spans,2020, Transactions of the Associationfor Computational Linguistics (TACL)
 Extending a parser to distant domains using afew dozen partially annotated examples,2018, In Annual Meeting of the Association for ComputationalLinguistics (ACL)
 How much reading does reading comprehension require?A critical investigation of popular benchmarks,2018, In Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Retrieval-augmented generation for knowledge-intensive NLP tasks,2020, In AnnualConference on Neural Information Processing Systems (NeurIPS)
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Adversarial augmentation policy search for domain andcross-lingual generalization in reading comprehension,2020, In Conference on Empirical Methods inNatural Language Processing (EMNLP): Findings
 Probing neural network comprehension of natural languagearguments,2019, In Annual Meeting of the Association for Computational Linguistics (ACL)
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Semantically equivalent adversarial rulesfor debugging NLP models,2018, In Annual Meeting of the Association for Computational Linguistics(ACL)
 Defending against neural fake news,2019, In Annual Conference on Neural InformationProcessing Systems (NeurIPS)
