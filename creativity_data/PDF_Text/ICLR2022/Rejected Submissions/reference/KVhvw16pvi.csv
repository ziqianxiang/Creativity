title,year,conference
 Uncertainty-based continual learning withadaptive regularization,2019, In Advances in Neural Information Processing Systems
 Online continual learning with maximal interfered retrieval,2019, In Advances in NeuralInformation Processing Systems
 Generalisation guarantees for continual learning withorthogonal gradient descent,2020, arXiv preprint arXiv:2006
 Weight uncertainty in neuralnetworks,2015, arXiv preprint arXiv:1505
 Notmnist dataset,2011, Technical report
 Riemannian walkfor incremental learning: Understanding forgetting and intransigence,2018, In Proceedings of the EuropeanConference on Computer Vision (ECCV)
 Efficient lifelonglearning with a-gem,2018, arXiv preprint arXiv:1812
 On tiny episodic memories in continual learning,2019, arXiv preprintarXiv:1902
 Closing the generalization gap ofadaptive gradient methods in training deep neural networks,2018, arXiv preprint arXiv:1806
 Lifelong machine learning,2018, Synthesis Lectures on Artificial Intelligence andMachine Learning
 Continual learning: A comparative study on how to defy forgetting in classificationtasks,2019, arXiv preprint arXiv:1909
 ImageNet: A Large-Scale Hierarchical ImageDatabase,2009, In CVPR09
 Uncertainty-guided continuallearning with bayesian neural networks,2019, arXiv preprint arXiv:1906
 Orthogonal gradient descent for continuallearning,2020, In International Conference on Artificial Intelligence and Statistics
 Model-agnostic meta-learning for fast adaptation of deepnetworks,2017, arXiv preprint arXiv:1703
 An empirical investigationof catastrophic forgetting in gradient-based neural networks,2013, arXiv preprint arXiv:1312
 Towards understanding generalization in gradient-basedmeta-learning,2019, arXiv preprint arXiv:1907
 Embracing change: Continual learningin deep neural networks,1364, Trends in Cognitive Sciences
 Re-evaluating continual learning scenarios:A categorization and case for strong baselines,2018, arXiv preprint arXiv:1810
 Gradient based memory editing for task-free continual learning,2020, arXivpreprint arXiv:2006
 Improving generalization performance by switching from adam tosgd,2017, CoRR
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcoming catastrophicforgetting in neural networks,2017, Proceedings of the national academy of sciences
 Learning multiple layers of features from tiny images,2009, 2009
 Convergence analysis of proximal gradientwith momentum for nonconvex optimization,2017, arXiv preprint arXiv:1705
 Learn to grow: A continual structurelearning framework for overcoming catastrophic forgetting,2019, arXiv preprint arXiv:1904
 Learning without forgetting,2017, IEEE transactions on pattern analysis andmachine intelligence
 Gradient episodic memory for continual learning,2017, In Advancesin neural information processing Systems
 Class-incremental learning: survey and performance evaluation,2020, arXiv preprint arXiv:2010
 Catastrophic interference in connectionist networks: The sequentiallearning problem,1989, volume 24 of Psychology of Learning and Motivation
 The stability-plasticity dilemma: Investigating thecontinuum from catastrophic forgetting to age-limited learning effects,2013, Frontiers in psychology
 Understanding therole of training regimes in continual learning,2020, arXiv preprint arXiv:2006
 Reading digits innatural images with unsupervised feature learning,2011, 2011
 Variational continual learning,2018, InInternational Conference on Learning Representations
 Continual lifelonglearning with neural networks: A review,2019, Neural Networks
 Continualunsupervised representation learning,2019, In Advances in Neural Information Processing Systems
 icarl: Incrementalclassifier and representation learning,2017, In Proceedings of the IEEE conference on Computer Vision andPattern Recognition
 An overview of gradient descent optimization algorithms,2016, arXiv preprint arXiv:1609
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Ella: An efficient lifelong learning algorithm,2013, In International Conference onMachine Learning
 Overcoming catastrophic forgettingwith agemention to the task,2018, arXiv preprint arXiv:1801
 Modular-relatedness for continual learning,2020, arXivpreprint arXiv:2011
 On training recurrent neural networks for lifelonglearning,2018, CoRR
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of its recentmagnitude
 Encoder based lifelonglearning,2017, In ICCV
 Three scenarios for continual learning,2019, arXiv preprintarXiv:1904
 Matching networks for one shotlearning,2016, In Advances in neural information processing Systems
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Reinforced continual learning,2018, In Advances in Neural Information ProcessingSystems
 Lifelong learning with dynamicallyexpandable networks,2017, arXiv preprint arXiv:1708
