title,year,conference
 QSGD:Communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 The effectivenessof reputation-based voting for collusion tolerance in large-scale grids,2014, IEEE Transactions onDependable and Secure Computing
 Machine learning with adversaries: Byzan-tine tolerant gradient descent,2017, In Advances in Neural Information Processing Systems
 Distributed trainingwith heterogeneous data: Bridging median- and mean-based algorithms,2020, In Neural InformationProcessing Systems
 Digital Image Processing 3rd Edition,2014, 2014
 Analysis of quantized models,2019, In InternationalConference on Learning Representations
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Binarizedneural networks,2016, In Advances in Neural Information Processing Systems
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Stochastic-sign sgd for feder-ated learning with theoretical guarantees,2020, arXiv preprint arXiv:2002
 Advancesand open problems in federated learning,2021, Foundations and Trends in Machine Learning
 Learning multiple layers of features from tiny images,2009, Master thesis
 Lagrangian multi-plier adaptation for rate-distortion optimization with inter-frame dependency,2015, IEEE Transactionson Circuits and Systems for Video Technology
 Ensemble distillation for robust modelfusion in federated learning,2020, Advances in Neural Information Processing Systems
 Byzantine-robust federated machine learn-ing through adaptive model averaging,2019, arXiv preprint arXiv:1909
 Binary neuralnetworks: A survey,2020, Pattern Recognition
 Line-speed and scal-able intrusion detection at the network edge via federated learning,2020, In International Federationfor Information Processing Networking Conference
 Stochastic sign descent methods: New algorithms and bettertheory,2021, In International Conference on Machine Learning
 On the byzantine ro-bustness of clustered federated learning,2020, In International Conference on Acoustics
 Feder-ated learning over wireless networks: Optimization model design and analysis,2019, In InternationalConference on Computer Communications
 Cooperative SGD: A unified framework for the design and analysisof communication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Byzantine-robust distributedlearning: Towards optimal statistical rates,2018, In International Conference on Machine Learning
