title,year,conference
 Information theory and an extension of the maximum likelihood principle,1998, InSelected papers of hirotugu akaike
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Hyperopt: a pythonlibrary for model selection and hyperparameter optimization,2015, Computational Science Discovery
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 Toward better generalization bounds with locally elasticstability,2021, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Tapas: Train-less accuracy predictor for architecturesearch,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Fast bayesian op-timization of machine learning hyperparameters on large datasets,2017, In Artificial Intelligence andStatistics
 Visualizing the loss land-scape of neural nets,2018, In S
 Random search and reproducibility for neural architecture search,2020, InUncertainty in artificial intelligence
 Noise and fluctuation of finite learning rate stochasticgradient descent,2021, In International Conference on Machine Learning
 Stochastic gradient descent as approximatebayesian inference,2017, arXiv preprint arXiv:1704
 Some pac-bayesian theorems,1999, Machine Learning
 Theory of deep learning iii: explaining the non-overfittingpuzzle,2017, arXiv preprint arXiv:1801
 Evaluation of Gaussian processes and other methods for non-linear re-gression,1997, PhD thesis
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the aaai conference on artificial intelligence
 On the predictability ofpruning across scales,2021, In International Conference on Machine Learning
 A tutorial onthompson sampling,2017, arXiv preprint arXiv:1707
 Very deep convolutional networks for large-scale imagerecognition,2015, In Yoshua Bengio and Yann LeCun (eds
 Predictive approaches for choosing hyperpa-rameters in gaussian processes,2001, Neural computation
 On the likelihood that one unknown probability exceeds another in view ofthe evidence of two samples,1933, Biometrika
 On the theory of the brownian motion,1930, Physicalreview
 Wide residual networks,2016, arXiv preprintarXiv:1605
24 in Lemma 2,2022, To do so
