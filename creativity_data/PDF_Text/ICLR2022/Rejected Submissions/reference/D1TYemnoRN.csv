title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Generalization error bounds of gradient descent for learning over-parameterized deep relu networks,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 The spectrum of kernel random matrices,2010, Annals of statistics
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow relu networks,2020, In International Conference on LearningRepresentations
 Linear convergence of gradient and proximal-gradient methods under the Polyak-IojasieWicz condition,2016, In Joint European Conference onMachine Learning and Knowledge Discovery in Databases
 Wide neural netWorks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in Neural Information Processing Systems
 Loss landscapes and optimization in over-parameterizednon-linear systems and neural netWorks,2020, arXiv preprint arXiv:2003
 Ensembles semi-analytiques,1965, IHES notes
 Foundations of machine learning,2018, MITpress
 The distribution of rademacher sums,1990, Proceedings of the AmericanMathematical Society
 Lexico-graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models,2019, arXivpreprint arXiv:1905
 Stochastic gradient descent on separabledata: Exact convergence With a fixed learning rate,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Norm-based capacity control in neuralnetWorks,2015, In Conference on Learning Theory
 A PAC-bayesian approach tospectrally-normalized margin bounds for neural netWorks,2018, In International Conference on LearningRepresentations
 The roleof over-parametrization in generalization of neural netWorks,2019, In International Conference onLearning Representations
 Gradient methods for the minimisation of functionals,1963, Ussr Computational Mathematicsand Mathematical Physics
 Pac-bayes analysisbeyond the usual bounds,2020, arXiv preprint arXiv:2006
 Non-asymptotic theory of random matrices: extreme singularvalues,1576, In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4Volumes) Vol
 The implicit bias of gradient descent on separabledata,2018, In International Conference on Learning Representations
 Benign overfitting in ridge regression,2020, arXiv preprintarXiv:2009
 Non-vacuous gen-eralization bounds at the imagenet scale: a PAC-bayesian compression approach,2019, In InternationalConference on Learning Representations
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural Information Processing Systems
 The proof is straightforWard,2010, Notice that by the inequality (11)
