title,year,conference
 cpSGD: Communication-efficient and differentially-private distributed SGD,2018, In S Bengio
 QSGD: Communication-Efficient SGD via gradient quantization and encoding,2017, In I Guyon
 signSGD withmajority vote is communication efficient and fault tolerant,2019, In International Conference on LearningRepresentations
 Language models are few-shot learners,2020, In H
 In H,2019, Wallach
 Adam: A method for stochastic optimization,2015, CoRR
 In S,2018, Bengio
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In I Guyon
 Distributed sgd with flexible gradient compression,2020, IEEE Access
 1-bit stochastic gradient descent andapplication to data-parallel distributed training of speech dnns,2014, In Interspeech 2014
 Towards more efficientstochastic decentralized learning: Faster convergence and sparse communication,2018, In Jennifer Dyand Andreas Krause (eds
 A distributed synchronoussgd algorithm with global top-k sparsification for low bandwidth networks,2019, In 2019 IEEE 39thInternational Conference on Distributed Computing Systems (ICDCS)
 Towards scalable distributed training of deep learning on publiccloud clusters,2021, In Proceedings of Machine Learning and Systems
 Compressing gradientoptimizers via Count-Sketches,2019, Proceedings of the 36th International Conference on MachineLearning
 Sparsified sgd with memory,2018, InS
 Communication-efficient distributedlearning via lazily aggregated quantized gradients,2019, In H
 DoubleSqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In Kamalika Chaudhuri andRuslan Salakhutdinov (eds
 1-bit Adam: Communication Efficient Large-ScaleTraining with Adamâ€™s Convergence Speed,2021, In Proceedings of the 38th International Conferenceon Machine Learning
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
 Gradient sparsification for Communication-Efficient distributed optimization,2018, In S Bengio
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In I
 On layer normalization in the transformer architecture,2020, InHal Daume In and Aarti Singh (eds
 Communication-Computation efficient gradient coding,2018, Proceedingsof the 35th International Conference on Machine Learning
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Double quantization for communication-efficient dis-tributed optimization,2019, In H
 Accelerating training of transformer-based language models withprogressive layer dropping,2020, In H
