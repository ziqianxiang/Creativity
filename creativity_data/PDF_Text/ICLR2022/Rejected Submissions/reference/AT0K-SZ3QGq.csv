title,year,conference
 Fed-erated learning with personalization layers,2019, arXiv preprint arXiv:1912
 Deep rewiring: Trainingvery sparse deep networks,2018, In International Conference on Learning Representations
 On bridging generic and personalized federated learning,2021, arXivpreprint arXiv:2107
 Exploiting shared repre-sentations for personalized federated learning,2021, arXiv preprint arXiv:2102
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Federated learning of a mixture of global and local models,2020, arXivpreprint arXiv:2002
 Personalized federated learning: A unified frame-work and universal oPtimization techniques,2021, arXiv preprint arXiv:2102
 Fedml: A research library and benchmarkfor federated machine learning,2020, arXiv preprint arXiv:2007
 The non-iid data quagmire ofdecentralized machine learning,2020, In International Conference on Machine Learning
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Top-kast: Top-kalways sparse training,2020, Advances in Neural Information Processing Systems
 Networked exponential families for big data over networks,2020, IEEE Access
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Lotteryfl:Personalized and communication-efficient federated learning with lottery ticket hypothesis onnon-iid datasets,2020, arXiv preprint arXiv:2008
 Sparse evolutionary deep learning with over one million artificial neu-rons on commodity hardware,2020, Neural Computing and Applications
 Three approaches forpersonalization with applications to federated learning,2020, arXiv preprint arXiv:2002
 Communication-efficientlearning of deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature Communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, International Conference on Machine Learning
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Networked federated multi-task learning,2021, arXiv preprint arXiv:2105
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Local sgd converges fast and communicates little,2018, arXiv preprintarXiv:1805
 Personalized federated learning by structured andunstructured pruning under data heterogeneity,2021, arXiv preprint arXiv:2105
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Fedcm: Federated learning withclient-level momentum,2021, arXiv preprint arXiv:2106
 Following therecommendation from Hsieh et al,2018, (2020)
 The bound canbe simplified as follows:6Nη2 XEtkQk2 ≤ 18Nη2Φt,2022,	(33)KkPutting together: Plugging all the components into Eq
