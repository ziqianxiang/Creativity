title,year,conference
 What does bert look at?an analysis of bertâ€™s attention,2019, arXiv preprint arXiv:1906
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Graphcodebert: Pre-training code representations withdata flow,2020, arXiv preprint arXiv:2009
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Logiqa: Achallenge dataset for machine reading comprehension with logical reasoning,2020, In Christian Bessiere(ed
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 The stanford corenlp natural language processing toolkit,2014, In Proceedings of 52ndannual meeting of the association for computational linguistics: system demonstrations
 Wordnet: a lexical database for english,1995, Communications of the ACM
 Language-aware truth assessment of fact candidates,2014, InProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers) 
 Reasoning with latent structure refinementfor document-level relation extraction,2020, arXiv preprint arXiv:2005
 Fact-driven logical reasoning,2021, arXiv preprintarXiv:2105
 Improving languageunderstanding by generative pre-training,2018, 2018
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Fine-tune bert fordocred with two-step process,2019, arXiv preprint arXiv:1909
 K-adapter: Infusing knowledge into pre-trained models with adapters,2020, arXivpreprint arXiv:2002
 Pretrained encyclopedia:Weakly supervised knowledge-pretrained language model,2019, arXiv preprint arXiv:1912
 Docred: A large-scale document-level relation extraction dataset,2019, arXivpreprint arXiv:1906
 Reclor: A reading comprehension datasetrequiring logical reasoning,2020, In International Conference on Learning Representations (ICLR)
