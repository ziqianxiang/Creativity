title,year,conference
 Memory-efficient adaptive optimiza-tion for large-scale learning,2019, arXiv preprint arXiv:1901
 Theoretical analysis of auto rate-tuning by batchnormalization,2018, arXiv preprint arXiv:1812
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 Extreme tensoring forlow-memory preconditioning,2019, arXiv preprint arXiv:1902
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprintarXiv:1910
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Implementation matters in deep policy gradients: A case studyon ppo and trpo,2020, arXiv preprint arXiv:2005
 Stochastic gradi-ent methods with layer-wise adaptive moments for training of deep networks,2019, arXiv preprintarXiv:1905
 Shampoo: Preconditioned stochastic tensor opti-mization,2018, In International Conference on Machine Learning
 Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,2018, arXiv preprintarXiv:1801
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Accelerat-ing stochastic gradient descent for least squares regression,2018, In Conference On Learning Theory
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 An exponential learning rate schedule for deep learning,2019, arXivpreprint arXiv:1910
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Adaptive bound optimization for online convex opti-mization,2010, arXiv preprint arXiv:1002
 Regularizing and Optimizing LSTMLanguage Models,2017, arXiv preprint arXiv:1708
 An Analysis of Neural Language Mod-eling at Multiple Scales,2018, arXiv preprint arXiv:1803
 Training tips for the transformer model,2018, arXiv preprintarXiv:1804
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning Representations
 Descending through a crowded Valley-benchmarking deep learning optimizers,2020, arXiv preprint arXiv:2007
 Escaping saddle pointswith adaptive gradient methods,2019, In International Conference on Machine Learning
 Adagrad stepsizes: Sharp convergence over nonconvexlandscapes,2019, In International Conference on Machine Learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, arXiv preprint arXiv:1904
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Aligning books and movies: Towards story-like visual explanations by watch-ing movies and reading books,2015, In Proceedings of the 2015 IEEE International Conference onComputer Vision (ICCV)
