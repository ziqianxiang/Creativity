title,year,conference
 The security of machine learning,0885, MachineLanguage
 Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an AccuracyTradeoff,2020, arXiv:2011
 DP-InstaHide: Provably Defusing Poisoning and Backdoor Attackswith Differentially Private Data Augmentations,2021, arXiv:2103
 Language Modelsare Few-Shot Learners,2020, In 34th Conference on Neural Information Processing Systems (NeurIPS 2020)
 On Evaluating Adversarial Robustness,2019, arXiv:1902
 REFIT: A UnifiedWatermark Removal Framework for Deep Learning Systems with Limited Data,2020, arXiv:1911
 Improved Regularization of Convolutional Neural Networks withCutout,2017, arXiv:1708
 STRIP: Adefence against trojan attacks on deep neural networks,2019, In Proceedings of the 35th Annual Computer SecurityApplications Conference
 Witches’ Brew: Industrial Scale Data Poisoning via Gradient Matching,2021, In InternationalConference on Learning Representations
 MaxUp: A Simple Way to Improve Generalizationof Neural Network Training,2020, arXiv:2002
 BadNets: Evaluating Backdooring Attackson Deep Neural Networks,2019, IEEEAccess
 MetaPoison: Practical General-purpose Clean-label Data Poisoning,2020, In Advances in Neural Information Processing Systems
 Stronger Data Poisoning Attacks Break Data SanitizationDefenses,2018, arXiv:1811
 Learning Multiple Layers of Features from Tiny Images,2009, 2009
 Deep Partition Aggregation: Provable Defenses against General PoisoningAttacks,2021, In International Conference on Learning Representations
 Fine-Pruning: Defending Against Backdooring Attackson Deep Neural Networks,2018, In Research in Attacks
 TowardsDeep Learning Models Resistant to Adversarial Attacks,2017, arXiv:1706
 Towards DeepLearning Models Resistant to Adversarial Attacks,2018, In International Conference on Learning Representations
 Automatic differentiation in PyTorch,2017, In NIPS 2017 AutodiffWorkshop
 Detection of Adversarial TrainingExamples in Poisoning Attacks through Anomaly Detection,2018, arXiv:1802
 Data Poisoning Won’t Save You From Facial Recognition,2021, In ICML 2021Workshop on Adversarial Machine Learning
 Adversarial Training against Location-Optimized AdversarialPatches,2020, arXiv:2005
 Just How Toxicis Data Poisoning? a Unified Benchmark for Backdoor and Data Poisoning Attacks,2020, arXiv:2006
 Certifying Some Distributional Robustness with PrincipledAdversarial Training,2018, In International Conference on Learning Representations
 Bypassing Backdoor Detection Algorithms in Deep Learning,2020, In 2020 IEEE EuropeanSymposium on Security and Privacy (EuroS P)
 On Adaptive Attacks to AdversarialExample Defenses,2020, In 34th Conference on Neural Information Processing Systems (NeurIPS 2020)
 Spectral Signatures in Backdoor Attacks,2018, In Advances inNeural Information Processing Systems 31
 Spectral Signatures in Backdoor Attacks,2018, arXiv:1811
 Clean-Label Backdoor Attacks,2018, openreview
 On Evaluating Neural Network Backdoor Defenses,2020, arXiv:2010
 RAB: Provable Robustness Against BackdoorAttacks,2020, arXiv:2003
 Mixup: Beyond Empirical RiskMinimization,2018, In International Conference on Learning Representations
