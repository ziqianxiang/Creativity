title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning (ICML)
 Globally optimal gradient descent for a ConvNet with Gaussianinputs,2017, In International Conference on Machine Learning (ICML)
 On the linear convergence of the stochastic gradient method withconstant step-size,2019, Optimization Letters
 Nonconvex optimization meets low-rank matrix factorization:An overview,2019, IEEE Transactions on Signal Processing (TSP)
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations(ICLR)
 Delving deep into rectifiers: Surpassinghuman-level performance on ImageNet classification,2015, In Conference on Computer Vision andPattern Recognition (CVPR)
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow ReLU networks,2020, In International Conference on LearningRepresentations (ICLR)
 Gradient descent finds global minima for generalizable deepneural networks of practical sizes,2019, In Annual Allerton Conference on Communication
 Memorization precedes generation: Learning unsu-pervised GANs with memory networks,2018, In International Conference on Learning Representations(ICLR)
 Semi-supervised learning with GANs:Manifold invariance with improved inference,2017, In Advances in neural information processingsystems (NeurIPS)
 Efficient BackProp,2012, InNeural networks: Tricks of the Trade
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in neural information processing systems (NeurIPS)
 A mean field analysis of deep resnetand beyond: Towards provably optimization via overparameterization from depth,2020, In InternationalConference on Machine Learning (ICML)
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, In Conference on Learning Theory
 NIST Handbook ofMathematical Functions Paperback and CD-ROM,2010, Cambridge University Press
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint arXiv:1308
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2019, IEEE Transactions on InformationTheory
 On learning over-parameterized neural networks: A functional approxi-mation perspective,2019, In Advances in neural information processing systems (NeurIPS)
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2019, In International Conference on ArtificialIntelligence and Statistics (AISTATS)
 Introduction to the Non-asymptotic Analysis of Random Matrices,2012, CambridgeUniversity Press
 Empirical evaluation of rectified activations inconvolutional network,2020, arXiv preprint arXiv:1505
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in neural information processing systems (NeurIPS)
