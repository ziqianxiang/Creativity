title,year,conference
 A unified lotteryticket hypothesis for graph neural networks,2021, In International Conference on Machine Learning
 Efficient tensor core-based gpukernels for structured sparsity under reduced precision,2021, 2021b
 Rethinking attention with per-formers,2021, In International Conference on Learning Representations
 An image is worth 16x16 words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 Block pruning for fastertransformers,2021, arXiv preprint arXiv:2109
 Soft: Softmax-free transformer with linear complexity,2021, arXiv preprintarXiv:2110
 Accelerating sparse deep neural networks,2021, arXiv preprintarXiv:2104
 Scaling neural machine translation,2018, InProceedings ofthe Third Conference on Machine Translation: Research Papers
 Sparse sinkhorn attention,2020, InInternational Conference on Machine Learning
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Long range arena : A benchmark for efficienttransformers,2021, In International Conference on Learning Representations
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Nystromformer: A nystrom-based algorithm for approximating self-attention,2021, InProceedings of the AAAI Conference on Artificial Intelligence
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
 Dropat-tention: A regularization method for fully-connected self-attention networks,2019, arXiv preprintarXiv:1907
 Ex-plicit sparse transformer: Concentrated attention through explicit selection,2019, arXiv preprintarXiv:1912
 The self-attention layer in transformer usually has multiple independent attentionheads,2022, Instead of launching one CUDA kernel for each attention head
 Our method achieves 1,1024,41 ã€œ1
