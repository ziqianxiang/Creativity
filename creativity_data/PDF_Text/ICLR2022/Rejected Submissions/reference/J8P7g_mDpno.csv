title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In ICML
 Deep rewiring: Trainingvery sparse deep networks,2018, In ICLR
 Language models are few-shot learners,2020, In NeurIPS
 An iterative pruning algorithm forfeedforward neural networks,1997, IEEE Transactions Neural Networks
 Transformer-XL: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In NeurIPS
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Rigging the lottery:Making all tickets winners,2020, In ICML
 The difficulty of training sparseneural networks,2020, arXiv preprint arXiv:1906
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 The lottery tickethypothesis at scale,2019, arXiv preprint arXiv:1903
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Levenshtein transformer,2019, In NeurIPS
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In NeurIPS
 Mask R-CNN,2017, In ICCV
 Deep residual learning for image recog-nition,2016, In CVPR
 Scaling laws for autoregressive generative modeling,2020, arXiv preprint arXiv:2010
 Sparsity indeep learning: Pruning and growth for efficient inference and training in neural networks,2021, arXivpreprint arXiv:2102
 Acceleratedsparse neural training: A provable and efficient method to find n:m transposable masks,2021, arXivpreprint arXiv:2102
 Top-KAST: Top-k always sparse training,2020, In NeurIPS
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Deep learning without poor local minima,2016, In NeurIPS
 Optimal brain damage,1990, In NeurIPS
 SNIP: Single-shot network pruning basedon connection sensitivity,2019, In ICLR
 Pruning filters forefficient convnets,2017, In ICLR
 Focal loss for dense object detection,2017, In ICCV
 Dynamic sparse training:Find efficient sparse network from scratch with trainable masked layers,2020, In ICLR
 SSD: Single shot multibox detector,2015, arXiv preprint arXiv:1512
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In ICCV
 Accelerating sparse deep neural networks,2021, arXiv preprintarXiv:2104
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature Communications
 Pruning algorithms-a survey,1993, IEEE Transactions on Neural Networks
 Faster R-CNN: Towards real-time objectdetection with region proposal networks,2015, In NeurIPS
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In ICLR
 Mo-bileNetV2: Inverted residuals and linear bottlenecks,2018, In CVPR
 Stacked U-Nets: A no-frills approachto natural image segmentation,2018, arXiv preprint arXiv:1804
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Going deeper with convolutions,2015, InCVPR
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, In NeurIPS
 The computationallimits of deep learning,2020, arXiv preprint arXiv:2007
 Training data-effiCient image transformers distillation through attention,2020, arXivpreprint arXiv:2012
 Attention is all you need,2017, In NeurIPS
 Picking winning tickets before training bypreserving gradient flow,2020, In ICLR
 Video-to-video synthesis,2018, In NeurIPS
 High-resolution image synthesis and semantic manipulation with conditional gans,2018, In CVPR
 Few-shot video-to-video synthesis,2019, In NeurIPS
 Learning structured sparsity indeep neural networks,2016, In NeurIPS
 Discovering neural wirings,2019, InNeurIPS
 Dilated residual networks,2017, In CVPR
 Learning n:m fine-grainedstructured sParse neural networks from scratch,2021, In ICLR
