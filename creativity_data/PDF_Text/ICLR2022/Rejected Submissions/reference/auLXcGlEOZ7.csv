title,year,conference
 Implicit regularization in deep matrixfactorization,2019, In Advances in Neural Information Processing Systems
 On the implicit bias of initialization shape: Beyond infinitesimalmirror descent,2021, arXiv preprint arXiv:2102
 On implicit regularization: Morse functions and applications to matrixfactorization,2020, arXiv preprint arXiv:2001
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, arXiv preprint arXiv:2002
 Implicit regularization in matrix sensing: A geometricview leads to stronger results,2020, arXiv preprint arXiv:2008
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Implicit regularization in matrix factorization,2018, In 2018 Information Theory and ApplicationsWorkshop (ITA)
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 Directional convergence and alignment in deep learning,2020, arXivpreprint arXiv:2006
 Gradient descent follows theregularization path for general losses,2020, In Conference on Learning Theory
 Towards resolving the implicit bias of gradient descentfor matrix factorization: Greedy low-rank learning,2020, arXiv preprint arXiv:2012
 Lexico-graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models,2019, InInternational Conference on Machine Learning
 Convergence of gradient descent on separable data,2019, In The 22ndInternational Conference on Artificial Intelligence and Statistics
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Implicit regularization in deep learning may not be explainable bynorms,2020, arXiv preprint arXiv:2005
 Nonlinear optimization,2011, Princeton university press
 Gradient methods never overfit on separable data,2020, arXiv preprint arXiv:2007
 Kernel and rich regimes in overparametrized models,2020, arXivpreprint arXiv:2002
 A unifying view on implicit bias in traininglinear neural networks,2020, arXiv preprint arXiv:2010
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
