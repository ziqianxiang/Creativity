title,year,conference
 Provable bounds for learning some deeprepresentations,2014, In International Conference on Machine Learning
 An empir-ical distribution function for sampling with incomplete information,1955, The annals of mathematicalstatistics
 Greedy layerwise learning can scaleto imagenet,2019, In International conference on machine learning
 Maximum likelihood estimates of monotone parameters,1955, The Annals of MathematicalStatistics
 Globally optimal gradient descent for a ConvNet with Gaussianinputs,2017, ArXiv
 Improved learning of one-hidden-layer convolutional neural networkswith overlaps,2018, ArXiv
 Gra-dient descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima,2017, ArXiv
 Implicit convex regularizers of cnn architectures: Convex optimizationof two-and three-layer networks in polynomial time,2021, In International Conference on LearningRepresentations (ICLR)
 Ober die theorie der einfachen ungleichungen,1902, J
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Learning two-layer neural networks withsymmetric inputs,2018, arXiv preprint arXiv:1810
 Learning neural networks with two nonlinear layers in polynomialtime,2017, In COLT
 Learning one convolutional layer with overlappingpatches,2018, ArXiv
 Graph implementations for nonsmooth convex programs,2008, InV
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Beating the perils of non-convexity:Guaranteed training of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 On the convergence of the method of analytic centers when applied to convex quadraticprograms,1990, Mathematical Programming
 Asymptotic properties of non-linear least squares estimators,1969, The Annals ofMathematical Statistics
 Efficient learning ofgeneralized linear and single index models with isotonic regression,2011, In NIPS
 A new polynomial-time algorithm for linear programming,1984, In Proceedings ofthe sixteenth annual ACM symposium on Theory of computing
 Convolutional neural networks for sentence classification,2014, arXiv preprint arXiv:1408
 The polynomial solvability of convexquadratic programming,1980, USSR Computational Mathematics and Mathematical Physics
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Efficient estimation in convex single indexmodels,2017, Preprint
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On the computational efficiency of training neuralnetworks,2014, In Advances in neural information processing Systems
 Rectifier nonlinearities improve neuralnetwork acoustic models,2013, In ICML Workshop on Deep Learning for Audio
 On stochastic limit and order relationships,1943, The Annals ofMathematical Statistics
 Interior path following primal-dual algorithms,1989, part ii: Convexquadratic programming
 Computational complexity of parametric linear programming,1980, Mathematicalprogramming
 Neural networks are convex regularizers: Exact polynomial-timeconvex optimization formulations for two-layer networks,2020, In International Conference on MachineLearning
 Vector-output ReLU neural networkproblems are copositive programs: Convex analysis of two layer networks and polynomial-timealgorithms,2020, arXiv preprint arXiv:2012
 Nonparametric least squares estimation of a multivariate convexregression function,2011, The Annals of Statistics
 Lectures on stochastic Program-ming: modeling and theory,2014, SIAM
 Learning ReLUs via gradient descent,2017, In NIPS
 An analytical formUla of popUlation gradient for two-layered ReLU network and itsapplications in convergence and critical point analysis,2017, In ICML
 A theory of the learnable,1984, Commun
 Principles of risk minimization for learning theory,1992, In Advances in neuralinformation processing systems
 Learning distribUtions generated byone-layer ReLU networks,2019, In NeurIPS
 An extension of karmarkar’s projective algorithm for convex qUadraticprogramming,1989, Mathematical programming
 Recovery gUaranteesfor one-hidden-layer neUral networks,2017, In ICML
