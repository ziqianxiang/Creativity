title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Spectrally-normalized margin bounds for neuralnetworks,2017, arXiv preprint arXiv:1706
 On the ability and limitations of transformersto recognize formal languages,2020, arXiv preprint arXiv:2009
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Decision transformer: Reinforcement learningvia sequence modeling,2021, arXiv preprint arXiv:2106
 Evaluating large language modelstrained on code,2021, arXiv preprint arXiv:2107
 On generalization bounds of a family of recurrent neuralnetworks,2019, arXiv preprint arXiv:1910
 What does BERT lookat? an analysis of BERTâ€™s attention,2019, arXiv preprint arXiv:1906
 Approximation by superpositions ofa sigmoidal function,1989, Mathematics of control
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory
 Recurrent independent mechanisms,2020, In International Conference onLearning Representations
 Neural production systems,2021, Advances in NeuralInformation Processing Systems
 Multilayer feedforward networks are uni-versal approximators,1989, Neural networks
 Infinite attention: Nngp andntk for deep attention networks,2020, In International Conference on Machine Learning
 Reinforcement learning as one big sequence mod-eling problem,2021, arXiv preprint arXiv:2106
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 Highly accurateprotein structure prediction with alphafold,2021, Nature
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Generating wikipedia by summarizing long sequences,2018, arXiv preprintarXiv:1801
 Pretrained transformers as universalcomputation engines,2021, arXiv preprint arXiv:2103
 Effective approaches to attention-based neural machine translation,2015, arXiv preprint arXiv:1508
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Analysis of boolean functions,2021, arXiv preprint arXiv:2105
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Approximating how single headattention learns,2021, arXiv preprint arXiv:2103
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Bert rediscovers the classical nlp pipeline,2019, arXivpreprint arXiv:1905
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 A mathematical theory of atten-tion,2020, arXiv preprint arXiv:2007
 Statistically meaningful approximation: a case study onapproximating turing machines with transformers,2021, arXiv preprint arXiv:2107
 Tensor programs ii: Neural tangent kernel for any architecture,2020, arXiv preprintarXiv:2006
 Using Lemma A,2022,2
