title,year,conference
 Delta-stn: Efficient bilevel optimization for neural networks usingstructured response jacobians,2020, CoRR
 Meta-learning with differ-entiable closed-form solvers,2018, In International Conference on Learning Representations (ICLR)
 Mathematical programs with optimization problems in theconstraints,1973, Operations Research
 A single-timescale stochastic bilevel optimizationmethod,2021, arXiv preprint arXiv:2102
 Generic methods for optimization-based modeling,2012, International Conference onArtificial Intelligence and Statistics (AISTATS)
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In Proc
 Forward and reversegradient-based hyperparameter optimization,2017, In International Conference on Machine Learning(ICML)
 Bilevelprogramming for hyperparameter optimization and meta-learning,2018, In International Conferenceon Machine Learning (ICML)
 Approximation methods for bilevel programming,2018, arXiv preprintarXiv:1802
 On differentiating parameterized argmin and argmax problems with application to bi-leveloptimization,2016, arXiv preprint arXiv:1607
 On the iteration com-plexity of hypergradient computation,2020, International Conference on Machine Learning (ICML))
 Optimizing large-scale hy-perparameters via automated learning algorithm,2021, CoRR
 Randomized stochastic variance-reduced methods for stochasticbilevel optimization,2021, arXiv preprint arXiv:2105
 On stochastic moving-averageestimators for non-convex optimization,2021, arXiv preprint arXiv:2104
 The cma evolution strategy: a comparing review,2006, Towards a new evolutionarycomputation
 A two-timescale frameworkfor bilevel optimization: Complexity analysis and application to actor-critic,2020, arXiv preprintarXiv:2007
 Convergence of meta-learning withtask-specific adaptation over partial parameter,2020, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Multi-step model-agnostic meta-learning: Convergenceand improved algorithms,2020, arXiv preprint arXiv:2002
 Bilevel optimization: Convergence analysis and enhanceddesign,2021, International Conference on Machine Learning (ICML))
 Anear-optimal algorithm for stochastic bilevel optimization via double-momentum,2021, arXiv preprintarXiv:2102
 Adam: A method for stochastic optimization,2014, InternationalConference on Learning Representations (ICLR)
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Investigating bi-level opti-mization for learning and vision from a unified perspective: A survey and beyond,2021, arXiv preprintarXiv:2101
 Min-max optimization without gradients: Convergence and applicationsto black-box evasion and poisoning attacks,2020, In International Conference on Machine Learning(ICML)
 Self-tuningnetworks: Bilevel optimization of hyperparameters using structured best-response functions,2018, InInternational Conference on Learning Representations (ICLR)
 Meta-learning with im-plicit gradients,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 An extended kuhn-tucker approach for linear bilevelprogramming,2005, Applied Mathematics and Computation
 Prototypical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems (NIPS)
 Es-maml: Simple hessian-free meta learning,2019, In International Conference on LearningRepresentations (ICLR)
 A comparative study of large-scale variants ofcma-es,2018, In International Conference on Parallel Problem Solving from Nature
 Matching networks for oneshot learning,2016, In Advances in Neural Information Processing Systems (NIPS)
 Gradient free minimax optimization:Variance reduction and faster convergence,2020, arXiv preprint arXiv:2006
 Provably faster algorithms for bilevel optimization,2021, arXivpreprint arXiv:2106
 Adversarial attacks on graph neural networks via metalearning,2019, In International Conference on Learning Representations (ICLR)
