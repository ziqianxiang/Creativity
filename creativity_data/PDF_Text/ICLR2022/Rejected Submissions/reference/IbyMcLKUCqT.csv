title,year,conference
 Learning with pseudo-ensembles,2014, Advances inneural information processing systems
 Rademacher and gaussian complexities: Risk bounds andstructural results,1532, J
 Learning invariances inneural networks,2020, arXiv preprint arXiv:2010
 Adamatch:A unified approach to semi-supervised learning and domain adaptation,2021, arXiv preprintarXiv:2106
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Self-ensembling for visual domain adapta-tion,2017, arXiv preprint arXiv:1706
 Functional regularization for representation learning: A unifiedtheoretical perspective,2020, arXiv preprint arXiv:2008
 Provable guarantees for self-superviseddeep learning with spectral contrastive loss,2021, arXiv preprint arXiv:2106
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Efficient augmentation via data subsampling,2018, arXiv preprintarXiv:1810
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Enhanced convolutional neural tangent kernels,2019, arXiv preprint arXiv:1911
 An analysis of the effect of invariance on gen-eralization in neural networks,2019, In International conference on machine learning Workshop onUnderstanding and Improving Generalization in Deep Learning
 Rotation equivariant vector fieldnetworks,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Learning with invariances in randomfeatures and kernel models,2021, arXiv preprint arXiv:2102
 Distributionally robustneural networks for group shifts: On the importance of regularization for worst-case generaliza-tion,2019, arXiv preprint arXiv:1911
 Regularization with stochastic transfor-mations and perturbations for deep semi-supervised learning,2016, Advances in neural informationprocessing systems
 Transformation invariancein pattern recognitionâ€”tangent distance and tangent propagation,1998, In Neural networks: tricks ofthe trade
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, arXiv preprint arXiv:2001
 Scale-equivariant steerable networks,2019, arXivpreprint arXiv:1910
 Improving invariance and equivariance properties ofconvolutional neural networks,2016, 2016
 Self-supervised learning with data augmentations provablyisolates content from style,2021, arXiv preprint arXiv:2106
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, Cambridge Seriesin Statistical and Probabilistic Mathematics
 Toward understanding the feature learning process of self-supervisedcontrastive learning,2021, arXiv preprint arXiv:2105
 Deep scale-spaces: Equivariance over scale,2019, arXiv preprintarXiv:1905
 Harmonicnetworks: Deep translation and rotation equivariance,2017, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 On the generalization effects oflinear transformations in data augmentation,2020, In International Conference on Machine Learning
 Wide residual networks,2016, arXiv preprintarXiv:1605
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
