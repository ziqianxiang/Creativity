title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In 3rd International Conference on Learning Representations
 Non-autoregressive electron redistribution modeling for reaction prediction,2021, In Proceedings of the38th International Conference on Machine Learning
 SMILES enumeration as data augmentation for neural network modeling ofmolecules,2017, CoRR
 Graph transformer for graph-to-sequence learning,2020, Proceedings of theAAAI Conference on Artificial Intelligence
 Deep retrosynthetic reaction prediction using local reactivity andglobal attention,2021, JACS Au
 Learning phrase representations using RNN encoder-decoderfor statistical machine translation,2014, In Proceedings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP)
 A graph-convolutional neural network model forthe prediction of chemical reactivity,2019, Chem
 Computer-assisted design of complex organic syntheses,1969, Science
 The Logic of Chemical Synthesis,1989, Wiley
 Graph transformation policy network for chemi-cal reaction prediction,2019, In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining
 Retrosynthesis withattention-based NMT model and chemical analysis of “wrong” predictions,2020, RSC Adv
 PCT: point cloud transformer,2020, CoRR
 Bridging nonlinearities and stochastic regularizers with gaussianerror linear units,2016, CoRR
 Strategies for pre-training graph neural networks,2020, In International Conference onLearning Representations
 Predicting organic reaction out-comes with Weisfeiler-Lehman network,2017, In Advances in Neural Information Processing Systems
 Junction tree variational autoencoder formolecular graph generation,2018, In Proceedings of the 35th International Conference on MachineLearning
 Geometry-aware transformerfor molecular property prediction,2021, CoRR
 Molecular transformer unifies reaction prediction andretrosynthesis across pharma chemical space,2019, Chem
 Automatic retrosynthetic route planningusing template-free models,2020, Chem
 Molecular graPhenhanced transformer for retrosynthesis Prediction,2021, Neurocomputing
 Molecule attention transformer,2020, CoRR
 Linking the neural machine translation and the Prediction of organicchemistry reactions,2016, CoRR
 Assigning unique keys to chemical com-Pounds for data integration: Some interesting counter examPles,2005, In Data Integration in the LifeSciences
 In H,2020, Larochelle
 “Found intranslation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models,2018, Chem
 Molecular transformer: A model for uncertainty-calibrated chemical re-action prediction,2019, ACS Central Science
 Gta: Graph truncated attention for retrosynthesis,2021, Proceedingsof the AAAI Conference on Artificial Intelligence
 Self-attention with relative position representa-tions,2018, In Proceedings of the 2018 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 A graph to graphs frameworkfor retrosynthesis prediction,2020, In Hal Daume III and Aarti Singh (eds
 Computer-assisted synthetic planning: The endof the beginning,2016, Angewandte Chemie International Edition
 State-of-the-art augmented NLP transformer models fordirect and single-step retro synthesis,2020, Nature Communications
 Attention is all you need,2017, In I
 Graph attention networks,2018, In International Conference on Learning Representations
 SQL-to-text generationwith graph-to-sequence model,2018, In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing
 Retroxpert: Decompose retrosynthesis prediction like a chemist,2020, In H
 Dual-viewmolecule pre-training,2021, CoRR
