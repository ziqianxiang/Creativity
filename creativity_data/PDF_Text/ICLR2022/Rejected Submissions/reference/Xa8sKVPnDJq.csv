title,year,conference
 Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models,2021, ArXiv
 Language models are few-shotlearners,2020, In H
 Rethinking attentionwith performers,2020, ICLR
 Plug and play language models: A simple approach to controlled textgeneration,2020, ArXiv
 Feature-wise transformations,2018, Distill
 Generative adversarial nets,2014, Advances in neural informationprocessing systems
 DeepBach: a steerable model for Bachchorales generation,2017, In International Conference on Machine Learning
 Training products of experts by minimizing contrastive divergence,2002, Neuralcomputation
 Compound word transformer:Learning to compose full-song music over dynamic directed hypergraphs,2021, AAAI
 Coun-terpoint by convolution,2017, In Proceedings of the International Conference on Music InformationRetrieval
 An improved relative self-attention mech-anism for transformer with application to music generation,2018, CoRR
 Tuning recurrent neural net-works with reinforcement learning,2016, CoRR
 The relativistic discriminator: a key element missing from standardgan,2018, arXiv preprint arXiv:1807
 Attributes-aware deep music transformation,2020, InProceedings of the 21st international society for music information retrieval conference
 Contrastive representation learning: Aframework and review,2020, IEEE Access
 Bachbot: Automatic composition in the style of bach chorales,2016, Masters thesis
 This time withfeeling: Learning expressive musical performance,2020, Neural Computing and Applications
 Contrastive learning for unpairedimage-to-image translation,2020, In European Conference on Computer Vision
 A hierarchical latentvector model for learning long-term structure in music,2018, ICML
 Learninga latent space of multitrack measures,2018, arXiv preprint arXiv:1806
 Music fadernets: Controllable music generation based onhigh-level features via low-level feature modelling,2020, ISMIR
 WaveNet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Neural discrete representation learn-ing,2017, NeurIPS
 Attention is all you need,2017, CoRR
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Learning interpretable representation forcontrollable polyphonic music generation,2020, ISMIR
 Musemorphose: Full-song and fine-grained music style transferwith just one transformer vae,2021, arXiv preprint arXiv:2105
