title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 What does bert look at?an analysis of bert’s attention,2019, arXiv preprint arXiv:1906
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics
 En-tities as experts: Sparse memory access with entity supervision,2020, arXiv preprint arXiv:2004
 Allennlp: A deep semantic natural languageprocessing platform,2017, 2017
 Assessing bert’s syntactic abilities,2019, arXiv preprint arXiv:1901
 Measuring systematic generaliza-tion in neural proof generation with transformers,2020, In Advances in Neural Information ProcessingSystems 33
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Long short-term memory,1997, Neural computation
 Dynamic entityrePresentations in neural language models,2017, arXiv preprint arXiv:1708
 Generalization without systematicity: On the comPositionalskills of sequence-to-sequence recurrent networks,2018, In ICML
 Sensebert: Driving some sense into bert,2019, arXiv preprintarXiv:1908
 Memorize or generalize? searching for acomPositional rnn in a haystack,2018, arXiv preprint arXiv:1802
 Linguisticknowledge and transferability of contextual rePresentations,2019, arXiv preprint arXiv:1903
 Roberta: A robustly oPtimized bert PretrainingaPProach,2019, arXiv preprint arXiv:1907
 ImProvingrobustness by augmenting training sentences with Predicate-argument structures,2020, arXiv preprintarXiv:2010
 StereoSet: Measuring stereotyPical bias in Pretrainedlanguage models,2020, ArXiv
 Dissecting contextualword embeddings: Architecture and rePresentation,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Knowledge enhanced contextual word rePresentations,2019, arXiv preprintarXiv:1909
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Word sense disambiguation: Aunified evaluation framework and empirical comparison,2017, In Proceedings of the 15th Conference ofthe European Chapter of the Association for Computational Linguistics: Volume 1
 Knowledge-aware language model pretraining,2020, arXiv preprint arXiv:2007
 Simple bert models for relation extraction and semantic role labeling,2019, arXivpreprint arXiv:1904
 Bert rediscovers the classical nlp pipeline,2019, arXivpreprint arXiv:1905
 What do youlearn from context? probing for sentence structure in contextualized word representations,2019, InProceedings of the 2019 International Conference on Learning Representations
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems 30
 Facts as experts: Adaptable andinterpretable neural memory over symbolic knowledge,2020, arXiv preprint arXiv:2007
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Huggingface’s transformers: State-of-the-artnatural language processing,2019, ArXiv
 Swag: A large-scale adversarial datasetfor grounded commonsense inference,2018, arXiv preprint arXiv:1808
 Ernie: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
 Edited by T,1989,S
