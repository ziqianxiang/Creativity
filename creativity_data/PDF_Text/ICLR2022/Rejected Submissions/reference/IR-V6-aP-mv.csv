title,year,conference
 Leveraging procedural generationto benchmark reinforcement learning,2019, arXiv preprint arXiv:1912
 Phasic policy gradient,2020, arXiv preprintarXiv:2009
 On the computational inefficiency of large batch sizesfor stochastic gradient descent,2018, arXiv preprint arXiv:1811
 Evaluating theperformance of reinforcement learning algorithms,2020, In International Conference on MachineLearning
 A natural policy gradient,2001, Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Rank score tests,2006, Circulation
 Stochastic gradient descent as approximateBayesian inference,2017, arXiv preprint arXiv:1704
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Trust regionpolicy optimization,2015, In International conference on machine learning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Mirror descent policyoptimization,2020, arXiv preprint arXiv:2005
 On the combination of independent two sample tests of wilcoxon,1960, Bull Inst InternStaist
 Which algorithmic choices matter at which batch sizes? Insightsfrom a noisy quadratic model,2019, Advances in neural information processing systems
