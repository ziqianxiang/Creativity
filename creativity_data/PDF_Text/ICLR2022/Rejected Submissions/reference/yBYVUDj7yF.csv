title,year,conference
 On principal component regression in a high-dimensional error-in-variables setting,2020, arXiv preprint arXiv:2010
 A theoretical analysis of contrastive unsupervised representation learning,2019, arXiv preprintarXiv:1902
 Modular learning in neural networks,1987, In AAAI
 Auto-association by multilayer perceptrons and singular valuedecomposition,1988, Biological cybernetics
 Auto-association by multilayer perceptrons and singular valuedecomposition,2004, Biological Cybernetics
 Rate-optimal perturbation bounds for singular subspaces with appli-cations to high-dimensional statistics,2018, The Annals of Statistics
 On the non-asymptotic concentration of het-eroskedastic wishart-type matrix,2020, arXiv preprint arXiv:2008
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 Big self-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Discrimi-native unsupervised feature learning with convolutional neural networks,2014, Advances in neuralinformation processing systems
 A selective overview of deep learning,2019, arXiv preprintarXiv:1904
 Dissecting supervised con-Strastive learning,2021, In International Conference on Machine Learning
 Measuring statistical de-pendence with hilbert-schmidt norms,2005, In International conference on algorithmic learning theory
 Solving high-dimensional partial differential equationsusing deep learning,2018, Proceedings of the National Academy of Sciences
 Provable guarantees for self-superviseddeep learning with spectral contrastive loss,2021, arXiv preprint arXiv:2106
 A broad study on the transferability of visual representations with con-trastive learning,2021, ArXiv
 Supervised contrastive learning,2020, arXiv preprintarXiv:2004
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Predicting what you already know helps:Provable self-supervised learning,2020, arXiv preprint arXiv:2008
 Choosing a point from the surface ofa sphere,1972, Annals of Mathematical Statistics
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 From principal subspaces to principal components with linear autoencoders,2018, arXivPrePrint arXiv:1804
 Facenet: A unified embedding for facerecognition and clustering,2015, 2015 IEEE Conference on ComPuter Vision and Pattern Recognition(CVPR)
 Improved deep metric learning with multi-class n-pair loss objective,2016, In NIPS
 Colored maximum variance unfold-ing,2007, Advances in Neural Information Processing Systems
 Supervised featureselection via dependence estimation,2007, In Proceedings of the 24th international conference onMachine learning
 Self-supervised learning from a multi-view perspective,2020, arXiv PrePrint arXiv:2006
 Toward understanding the feature learning process of self-supervisedcontrastive learning,2021, arXiv PrePrint arXiv:2105
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE conference on comPuter visionand Pattern recognition
 SamPle covariance matrices and high-dimensional dataanalysis,2015, Cambridge University Press Cambridge
 A useful variant of the davis-kahan theorem forstatisticians,2015, Biometrika
