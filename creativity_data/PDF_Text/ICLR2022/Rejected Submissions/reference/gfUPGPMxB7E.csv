title,year,conference
 Hindsight experience replay,2017, arXivpreprint arXiv:1707
 Model-based offline planning,2020, arXiv preprintarXiv:2008
 Scaling data-driven roboticswith reward sketching and batch reinforcement learning,2019, arXiv preprint arXiv:1909
 Actionable models:Unsupervised offline reinforcement learning of robotic skills,2021, arXiv preprint arXiv:2104
 Deepreinforcement learning from human preferences,2017, arXiv preprint arXiv:1706
 Discovering an aid policy to minimize student evasionusing offline reinforcement learning,2021, arXiv preprint arXiv:2104
 Minimax-optimal off-policy evaluation with linear functionapproximation,2020, In International Conference on Machine Learning
 Rewriting historywith inverse rl: Hindsight inference for policy improvement,2020, arXiv preprint arXiv:2002
 Replacing rewards with examples:Example-based policy search via recursive classification,2021, arXiv preprint arXiv:2103
 Guided cost learning: Deep inverse optimal controlvia policy optimization,2016, In International conference on machine learning
 Deep spatialautoencoders for visuomotor learning,2016, In 2016 IEEE International Conference on Robotics andAutomation (ICRA)
 Learning robust rewards with adversarial inverse reinforce-ment learning,2018, International Conference on Learning Representations
 Variational inverse control withevents: A general framework for data-driven reward definition,2018, Conference on Neural InformationProcessing Systems
 Variational inverse control withevents: A general framework for data-driven reward definition,2018, arXiv preprint arXiv:1805
 Off-policy deep reinforcement learning withoutexploration,2018, arXiv preprint arXiv:1812
 Emaq: Expected-maxq-learning operator for simple yet effective offline and online rl,2021, In International Conference onMachine Learning
 Multi-task deep reinforcement learning with popart,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Generative adversarial imitation learning,2016, Conference on NeuralInformation Processing Systems
 Mapping state space using landmarks for universal goalreaching,2019, Advances in Neural Information Processing Systems
 Way off-policy batch deep reinforcement learning ofimplicit human preferences in dialog,2019, arXiv preprint arXiv:1907
 Mt-opt: Continuous multi-task robotic rein-forcement learning at scale,2021, Conference on Robot Learning (CoRL)
 Morel: Model-based offline reinforcement learning,2020, arXiv preprint arXiv:2005
 Semi-supervised reward learning for offline reinforcement learning,2020, arXivpreprint arXiv:2012
 Stabilizing off-policyq-learning via bootstrapping error reduction,2019, In Advances in Neural Information ProcessingSystems 
 Conservative q-learning for offlinereinforcement learning,2020, arXiv preprint arXiv:2006
 Representation balancing offline model-basedreinforcement learning,2021, In International Conference on Learning Representations
 Multi-task batch reinforcement learning with metric learning,2019, arXivpreprint arXiv:1909
 Reinforcement learning without ground-truthstate,2019, arXiv preprint arXiv:1905
 Competitive experience replay,2019, arXivpreprint arXiv:1902
 Provably good batch reinforce-ment learning without great exploration,2020, arXiv preprint arXiv:2007
 Grounding language in play,2020, arXiv preprint arXiv:2005
 Iris: Implicit reinforcement without interaction at scale for learning control from offline robotmanipulation data,2020, In 2020 IEEE International Conference on Robotics and Automation (ICRA)
 Deployment-efficient reinforcement learning via model-based offline optimization,2020, arXiv preprintarXiv:2006
 Visualreinforcement learning with imagined goals,2018, arXiv preprint arXiv:1807
 Algorithms for inverse reinforcement learning,2000, In Proceedingsof the Seventeenth International Conference on Machine Learning
 Actor-mimic: Deep multitask andtransfer reinforcement learning,2015, arXiv preprint arXiv:1511
 Alvinn: an autonomous land vehicle in a neural network,1988, In Proceedings of the1st International Conference on Neural Information Processing Systems
 Temporal difference models: Model-free deep rl for model-based control,2018, arXiv preprint arXiv:1802
 Offline reinforcement learning fromimages with latent space models,2021, Learning for Decision Making and Control (L4DC)
 Bridging offline rein-forcement learning and imitation learning: A tale of pessimism,2021, arXiv preprint arXiv:2103
 Agnostic system identification for model-based reinforcementlearning,2012, In ICML
 Keep doing what worked: Behavioralmodelling priors for offline reinforcement learning,2020, arXiv preprint arXiv:2002
 End-to-end roboticreinforcement learning without reward engineering,2019, arXiv preprint arXiv:1904
 Cog:Connecting new skills to past experience with offline reinforcement learning,2020, arXiv preprintarXiv:2010
 S4rl: Surprisingly simple self-supervision for offline reinforcementlearning,2021, arXiv preprint arXiv:2103
 Multi-task reinforcement learning with context-based representations,2021, arXiv preprint arXiv:2102
 Policy continuation with hindsightinverse dynamics,2019, arXiv preprint arXiv:1910
 Overcoming model bias for robust offlinedeep reinforcement learning,2020, arXiv preprint arXiv:2008
 Multi-task reinforcement learning: ahierarchical bayesian approach,2007, In Proceedings of the 24th international conference on Machinelearning
 Few-shot goal inference for visuomotorlearning and planning,2018, In Conference on Robot Learning
 Knowledge transfer in multi-taskdeep reinforcement learning for continuous control,2020, 2020
 Representation matters: Offline pretraining for sequential decisionmaking,2021, arXiv preprint arXiv:2102
 Multi-task reinforcement learning with softmodularization,2020, arXiv preprint arXiv:2003
 Mopo: Model-based offline policy optimization,2020, arXiv preprint arXiv:2005
 Conservative data sharing for multi-task offline reinforcement learning,2021, arXiv preprintarXiv:2109
 Plas: Latent action space for offline reinforce-ment learning,2020, arXiv preprint arXiv:2011
 Maximum entropy inversereinforcement learning,2008, In Aaai
 Following Yu et al,2020, (2021a)
 To obtain the final expression of Proposition F,2022,1
