title,year,conference
 Language Models are Few-Shot Learners,2005, arXiv:2005
 Language Modeling with GatedConvolutional Networks,2017, arXiv:1612
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2019, arXiv:1810
 CatBoost: gradient boosting withcategorical features support,2018, arXiv:1810
 An Image is Worth 16x16 Words: Transformers for Image Recognition atScale,2021, arXiv:2010
 Deepfm: A factorization-machine based neural network for CTR prediction,2017, CoRR
 Anembedding learning framework for numerical features in ctr prediction,2021, Proceedings of the27th ACM SIGKDD Conference on Knowledge Discovery Data Mining
 RealFormer: TransformerLikes Residual Attention,2020, arXiv:2012
 Long short-term memory,1997, Neural computation
 TabTransformer: Tabular DataModeling Using Contextual Embeddings,2012, arXiv:2012
 Lightgbm: A highly efficient gradient boosting decision tree,2017, In NIPS
 Deep learning diseaseprediction model for use with intelligent robots,0045, Computers Electrical Engineering
 Decoupled Weight Decay Regularization,1711, arXiv:1711
 Autocross: Automatic feature crossing for tabular data in real-worldapplications,2019, CoRR
 Induction of decision trees,1986, MACH
 GLU Variants Improve Transformer,2020, arXiv:2002
 Autoint: Automatic feature interaction learning via self-attentive neural networks,2018, CoRR
 Attention Is All You Need,1706, arXiv:1706
