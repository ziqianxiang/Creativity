title,year,conference
 LIBSVM: a library for support vector machines,2011, ACMtransactions on intelligent systems and technology (TIST)
 SAGA: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in Neural InformationProcessing Systems
 SPIDER: Near-optimal non-convex op-timization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 Stabilized SVRG: Simple variance reductionfor nonconvex optimization,2019, In Conference on Learning Theory
 Local SGD: Unified theory and new efficientmethods,2020, arXiv preprint arXiv:2011
 MARINA: Faster non-convex distributed learning with compression,2021, In International Conference on Machine Learning
 On the convergence of local descent methods in federatedlearning,2019, arXiv preprint arXiv:1910
 Firecaffe: near-linearacceleration of deep neural network training on compute clusters,2016, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 SCAFFOLD: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 Tighter theory for local SGD onidentical and heterogeneous data,2020, In International Conference on Artificial Intelligence andStatistics
 Federated optimization:Distributed machine learning for on-device intelligence,2016, arXiv preprint arXiv:1610
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Don't jump through hoops and removethose loops: SVRG and Katyusha are better without the outer loop,2020, In Proceedings of the 31stInternational Conference on Algorithmic Learning Theory
 A unified variance-reduced accelerated gradient method forconvex optimization,2019, In Advances in Neural Information Processing Systems
 Non-convex finite-sum optimization viaSCSG methods,2017, In Advances in Neural Information Processing Systems
 On the convergence ofFedAvg on non-iid data,2019, arXiv preprint arXiv:1907
 SSRGD: Simple stochastic recursive gradient descent for escaping saddle points,2019, InAdvances in Neural Information Processing Systems
 ANITA: An optimal loopless accelerated variance-reduced gradient method,2021, arXiv preprintarXiv:2103
 A short note of PAGE: Optimal convergence rates for nonconvex optimization,2021, arXivpreprint arXiv:2106
 A simple proximal stochastic gradient method for nonsmooth noncon-vex optimization,2018, In Advances in Neural Information Processing Systems
 A fast Anderson-Chebyshev acceleration for nonlinear optimization,2020, InInternational Conference on Artificial Intelligence and Statistics
 A unified analysis of stochastic gradient methods for nonconvexfederated optimization,2020, arXiv preprint arXiv:2006
 CANITA: Faster rates for distributed convex optimization withcommunication compression,2021, arXiv preprint arXiv:2107
 ZeroSARAH: Efficient nonconvex finite-sum optimization with zerofull gradient computation,2021, arXiv preprint arXiv:2103
 Acceleration for compressed gradientdescent in distributed and federated optimization,2020, In International Conference on MachineLearning
 PAGE: A simple and optimalprobabilistic gradient estimator for nonconvex optimization,2021, In International Conference onMachine Learning
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 SARAH: A novel method for machinelearning problems using stochastic recursive gradient,2017, In International Conference on MachineLearning
 Local SGD converges fast and communicates little,2020, In International Conferenceon Learning Representations
 SpiderBoost and momentum: Fasterstochastic variance reduction algorithms,2018, arXiv preprint arXiv:1810
 Parallel restarted SGD with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Stochastic nested variance reduction for nonconvexoptimization,2018, In Advances in Neural Information Processing Systems
