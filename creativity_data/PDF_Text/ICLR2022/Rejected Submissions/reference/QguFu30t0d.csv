title,year,conference
 A little is enough: Circumventing defenses fordistributed learning,2019, arXiv preprint arXiv:1902
 Towardsfederated learning at scale: System design,2019, arXiv preprint arXiv:1902
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Leaf: A benchmark for federated settings,2018, arXivpreprint arXiv:1812
 Cronus: Robustand heterogeneous collaborative learning with black-box knowledge transfer,2019, arXiv preprintarXiv:1912
 Group knowledge transfer: Federatedlearning of large cnns at the edge,2020, arXiv preprint arXiv:2007
 Fedml: A research library and benchmarkfor federated machine learning,2020, arXiv preprint arXiv:2007
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Ad-vances and open problems in federated learning,2019, arXiv preprint arXiv:1912
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Fedmd: Heterogenous federated learning via model distillation,2019, arXivpreprint arXiv:1910
 Abnormal client behavior detectionin federated learning,2019, arXiv preprint arXiv:1910
 An exponential learning rate schedule for deep learning,2019, arXivpreprint arXiv:1910
 Not all knowledge is created equal,2021, CoRR
 Ensemble distillation for robust modelfusion in federated learning,2020, arXiv preprint arXiv:2006
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 KnoWledge inheritance for pre-trained language models,2021, arXivpreprint arXiv:2105
 Improved proteinstructure prediction using potentials from deep learning,2020, Nature
 Masteringthe game of go With deep neural netWorks and tree search,2016, nature
 Federated multi-tasklearning,2017, arXiv preprint arXiv:1705
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Efficientnet: Rethinking model scaling for convolutional neural net-Works,2019, In International Conference on Machine Learning
 Selective knowledge distillation forneural machine translation,2021, arXiv preprint arXiv:2105
 Netadapt: Platform-aware neural network adaptation for mobile applications,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Learning from multiple teacher networks,2017, InProceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery andData Mining
 Reinforcedmulti-teacher selection for knowledge distillation,2021, In Proceedings of the AAAI Conference onArtificial Intelligence
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
