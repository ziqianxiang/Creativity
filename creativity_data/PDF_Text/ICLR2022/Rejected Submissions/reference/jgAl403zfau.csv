title,year,conference
 Learning the number of neurons in deep networks,2016, InAdvances in Neural Information Processing Systems
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Constraint-aware deep neuralnetwork compression,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 cudnn: Efficient primitives for deep learning,2014, arXiv preprintarXiv:1410
 ChamNet:Towards efficient network design through platform-aware model adaptation,2019, In CVPR
 Progressive skeletonization: Trimming more fat from a network at initialization,2020, arXivpreprint arXiv:2006
 Dpp-net: Device-aware progressive search for pareto-optimal neural architectures,2018, In Proceedings of the EuropeanConference on Computer Vision (ECCV)
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Dynamic channelpruning: Feature boosting and suppression,2018, arXiv preprint arXiv:1810
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Network trimming: A data-drivenneuron pruning approach towards efficient deep architectures,2016, arXiv preprint arXiv:1607
 SNIP: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations
 Eagleeye: Fast sub-net evaluation for efficientneural network pruning,2020, In European Conference on Computer Vision
 Pruning filters forefficient convnets,2017, International Conference on Learning Representations (ICLR)
 Ssd: Single shot multibox detector,2016, In European conference on computervision
 Rethinking the valueof network pruning,2019, In International Conference on Learning Representations
 Theoretical analysis of self-training with deep networks on un-labeled data,2017, In International Conference on Learning Representations
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, Advances in neural information processing systems
 Imagenet large scale visual recognition challenge,0920, Int
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Very deep convolutional networks for large-scale imagerecognition,2015, International Conference on Learning Representations (ICLR)
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Neural pruning via growing regularization,2021, InInternational Conference on Learning Representations (ICLR)
 Netadapt: Platform-aware neural network adaptation for mobile applications,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Gate decorator: Globalfilter pruning method for accelerating deep convolutional neural networks,2019, arXiv preprintarXiv:1909
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
