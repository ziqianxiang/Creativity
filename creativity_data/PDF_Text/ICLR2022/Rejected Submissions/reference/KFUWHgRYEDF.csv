title,year,conference
 Curriculum adversarial training,2018, In Jerome Lang (ed
 Robust neural machine translation With doublyadversarial inputs,2019, In Anna Korhonen
 ELECTRA: pre-trainingtext encoders as discriminators rather than generators,2020, In 8th International Conference on LearningRepresentations
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Efficient training ofBERT by progressively stacking,2019, In Proceedings of the 36th International Conference on MachineLearning
 Explaining and harnessing adversarialexamples,2015, In Yoshua Bengio and Yann LeCun (eds
 Gpipe: Efficient training of giantneural networks using pipeline parallelism,2019, In Advances in Neural Information Processing Systems32: Annual Conference on Neural Information Processing Systems 2019
 Non-convex optimization for machine learning,2017, arXiv preprintarXiv:1712
 SMART:robust and efficient fine-tuning for pre-trained natural language models through principled regu-larized optimization,2020, In Dan Jurafsky
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In 5thInternational Conference on Learning Representations
 Pytorch distributed: Experienceson accelerating data parallel training,2020, Proc
 On gradient descent ascent for nonconvex-concave minimaxproblems,2020, In International Conference on Machine Learning
 Adversarial training for large neural language models,2020, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Adversarial training methods for semi-supervised text classification,2017, In 5th International Conference on Learning Representations
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Weakly-convex-concave min-maxoptimization: provable algorithms and applications in machine learning,2021, Optimization Methodsand Software
 Zero: Memory optimizationtowards training A trillion parameter models,2019, CoRR
 Convex analysis,2015, Princeton university press
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems 30: Annual Conference on Neural Information Processing Systems 2017
 Improving neural language modeling via adversarialtraining,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Transformers: State-of-the-art naturallanguage processing,2020, In Qun Liu and David Schlangen (eds
 Hessian-based analysisof large batch training and robustness to adversaries,2018, In Samy Bengio
 Hessian-based analysisof large batch training and robustness to adversaries,2018, In Samy Bengio
 Amata: An annealing mechanism foradversarial training acceleration,2020, CoRR
 ReducingBERT pre-training time from 3 days to 76 minutes,2019, CoRR
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, arXiv preprint arXiv:1904
 Word embedding perturbation for sentence classification,2018, CoRR
 Accelerating training of transformer-based language models withprogressive layer dropping,2020, In Hugo Larochelle
 Freelb: Enhancedadversarial training for natural language understanding,2020, In 8th International Conference onLearning Representations
