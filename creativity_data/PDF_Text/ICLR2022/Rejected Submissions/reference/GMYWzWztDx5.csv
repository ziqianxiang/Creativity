title,year,conference
 Adaptive input representations for neural language modeling,2019, InInternational Conference on Learning Representations
 Language models are few-shotlearners,2020, In H
 Un-supervised cross-lingual representation learning at scale,2020, 2020
 JUrassic-1: Technical details and evalUa-tion,2021, Technical report
 Understanding the diffi-cUlty of training transformers,2020, arXiv preprint arXiv:2004
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 A corpus and cloze evaluation for deeper under-standing of commonsense stories,2016, In Proceedings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Association for Computational Linguistics,1098, doi:10
 Shortformer: Better language modeling using shorterinputs,2020, arXiv preprint arXiv:2012
 Languagemodels are unsupervised multitask learners,2019, Technical report
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Gra-dinit: Learning to initialize neural networks for stable and efficient training,2021, arXiv preprintarXiv:2102
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2019, arXiv preprint arXiv:1506
