title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Scalable second orderoptimization for deep learning,2020, arXiv preprint arXiv:2002
 Distributed second-order optimization usingkronecker-factored approximations,2016, 2016a
 Layer normalization,2016, arXiv preprintarXiv:1607
 Implicit gradient regularization,2020, arXiv preprintarXiv:2009
 Aprogressive batching l-bfgs method for machine learning,2018, In International Conference on MachineLearning
 Practical gauss-newton optimisation for deeplearning,2017, In International Conference on Machine Learning
 On-line learning for very large data sets,2005, Applied Stochastic modelsin business and industry
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, arXiv preprint arXiv:1406
 Fastapproximate natural gradient descent in a kronecker-factored eigenbasis,2018, arXiv preprintarXiv:1806
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 A family of variable-metric methods derived by variational means,1970, Mathematicsof computation
 Practical quasi-newton methods for training deepneural networks,2020, arXiv preprint arXiv:2006
 Reducing the dimensionality of data with neuralnetworks,2006, science
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning applied to document recog-nition,1998, Proceedings of the IEEE
 A theoretical framework for back-propagation,1988, In Proceedings of the 1988 connectionist models summer school
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Deep learning via hessian-free optimization,2010, In ICML
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Training deep and recurrent networks with hessian-free opti-mization,2012, In Neural networks: Tricks of the trade
 Fastand furious convergence: Stochastic second order methods under interpolation,2020, In InternationalConference on Artificial Intelligence and Statistics
 Fast exact multiplication by the hessian,1994, Neural computation
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, Advances in neural information processing systems
 A stochastic quasi-newton method for onlineconvex optimization,2007, In Artificial intelligence and statistics
 Conditioning of quasi-newton methods for function minimization,1970, Mathematicsof computation
 On the origin of implicit regu-larization in stochastic gradient descent,2021, arXiv preprint arXiv:2101
 Fast large-scale optimization by unifyingstochastic gradient and quasi-newton methods,2014, In International Conference on Machine Learning
 Variable metricstochastic approximation theory,2009, In Artificial Intelligence and Statistics
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Adahessian: An adaptive second order optimizer for machine learning,2020, arXiv preprintarXiv:2006
