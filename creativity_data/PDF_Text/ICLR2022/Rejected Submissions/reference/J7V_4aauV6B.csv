title,year,conference
 Adam: A method for stochastic optimization,2015, 3rd InternationalConference on Learning Representations
 Adaptive gradient methods with dynamic bound oflearning rate,2019, 7th International Conference on Learning Representations
 On the convergence of adam and beyond,2018, 6thInternational Conference on Learning Representations
 On the noisy gradient descentthat generalizes as sgd,1036, In International Conference on Machine Learning
 Positive-negative momentum: Manipulatingstochastic gradient noise to improve generalization,1144, In International Conference on MachineLearning
