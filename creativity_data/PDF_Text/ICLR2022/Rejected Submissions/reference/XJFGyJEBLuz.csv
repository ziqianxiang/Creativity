title,year,conference
 Learning a deep listwise context modelfor ranking refinement,2018, In ACM SIGIR Conference on Research and Development in InformationRetrieval
 Revisiting approximatemetric optimization in the age of deep neural networks,2019, In ACM SIGIR Conference on Researchand Development in Information Retrieval
 Learning to rank using gradient descent,2005, In Proceedings of the 22nd InternationalConference on Machine Learning
 Learning to rank: from pairwiseapproach to listwise approach,2007, In International Conference on Machine Learning
 Fast ranking with additive ensembles of oblivious andnon-oblivious regression trees,2016, ACM Trans
 Knowledge distillation: Asurvey,2020, arXiv preprint arXiv:2006
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Knowledge distillation withadversarial samples supporting decision boundary,2019, In AAAI Conference on Artificial Intelligence
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Training linear svms in linear time,2006, In ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Lightgbm: A highly efficient gradient boosting decision tree,2017, In International Conference onNeural Information Processing Systems
 Sequence-level knowledge distillation,2016, arXiv preprintarXiv:1606
 Learning to rank for information retrieval,2009, Found
 Setrank: Learninga permutation-invariant ranking model for information retrieval,2020, In ACM SIGIR Conference onResearch and Development in Information Retrieval
 Introducing LETOR 4,2013,0 datasets
 A general approximation framework for direct optimization ofinformation retrieval measures,2010, Information Retrieval
 Rankdistil: Knowledge distillation for ranking,2021, InInternational Conference on Artificial Intelligence and Statistics
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Multilingual neural machinetranslation with knowledge distillation,2019, arXiv preprint arXiv:1902
 Ranking distillation: Learning compact ranking models with high per-formance for recommender system,2018, In ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining
