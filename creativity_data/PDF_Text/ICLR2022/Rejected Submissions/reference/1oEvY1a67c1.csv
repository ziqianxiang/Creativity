title,year,conference
 Dota 2 with largescale deep reinforcement learning,2019, ArXiv preprint
 Language models are few-shot learners,2020, In Hugo Larochelle
 A theory of label propagation for subpopulationshift,2021, arXiv preprint arXiv:2102
 Emerging properties in self-supervised vision transformers,2021, ArXiv preprint
 Big self-supervised models are strong semi-supervised learners,2020, In HugoLarochelle
 Self-training avoids using spuriousfeatures under domain shift,2020, In NeurIPS
 Xception: Deep learning with depthwise separable convolutions,2017, In 2017 IEEEConference on Computer Vision and Pattern Recognition
 Robustbench: a standardizedadversarial robustness benchmark,2020, ArXiv preprint
 The mnist database of handwritten digit images for machine learning research,2012, IEEESignal Processing Magazine
 A study and comparison of human and deep learningrecognition performance under visual distortions,2017, In International Conference on ComputerCommunications and Networks
 Self-ensembling for visual domainadaptation,2018, In 6th International Conference on Learning Representations
 Empirical comparison of hard and soft label propagation forrelational classification,2007, In 17th international conference on Inductive logic programming
 Generalisation in humans and deep neural networks,2018, In Samy Bengio
 Robust loss functions under label noise for deepneural networks,2017, In Satinder P
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 The many faces of robustness: A criticalanalysis of out-of-distribution generalization,2020, ArXiv preprint
 Augmix: A simple data processing method to improve robustness anduncertainty,2020, In 8th International Conference on Learning Representations
 Distilling the knowledge in a neural network,2014, InNIPS Deep Learning Workshop
 Parameter-efficient transfer learningfor NLP,2019, In Proceedings of the 36th International Conference on Machine Learning
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition
 Understanding self-training for gradual domainadaptation,2020, In International Conference on Machine Learning
 Pseudo-label: The simple and efficient semi-supervised learning method for deepneural networks,2013, In ICML Workshop : Challenges in Representation Learning (WREPL)
 Torchvision the machine-vision package of torch,2010, In ACMInternational Conference on Multimedia
 Automatic differentiation inPyTorch,2021, In NIPS Autodiff Workshop
 Increasing the robustness of dnns against image corruptions byplaying the game of noise,2020, ArXiv preprint
 Improving robustness against common corruptions by covariate shift adaptation,2020, InAdvances in neural information processing systems
 Learning adaptive loss for robustlearning with noisy labels,2020, ArXiv preprint
 A DIRT-T approach to unsuperviseddomain adaptation,2018, In 6th International Conference on Learning Representations
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, In NeurIPS
 Unsupervised domain adaptation throughself-supervision,2019, ArXiv preprint
 0 Contributors,2020, SciPy 1
 Fully test-time adaptation by entropy minimization,2020, ArXiv preprint
 Theoretical analysis of self-training withdeep networks on unlabeled data,2020, In ICLR
 Aggregated residualtransformations for deep neural networks,2017, In 2017 IEEE Conference on Computer Vision andPattern Recognition
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 Memo: Test time robustness via adaptation andaugmentation,2021, arXiv preprint arXiv:2110
 Generalized cross entropy loss for training deepneural networks with noisy labels,2018, In Samy Bengio
 Unsupervised domain adaptationfor semantic segmentation via class-balanced self-training,2018, In Proceedings of the Europeanconference on computer vision (ECCV)
 Camelyon17 contains three different test splits withdifferent domains and varying difficulty levels,2017, For evaluation
