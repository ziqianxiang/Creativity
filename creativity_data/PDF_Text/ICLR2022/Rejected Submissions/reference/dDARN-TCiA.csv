title,year,conference
 VarianceReduction in SGD by Distributed Importance Sampling,2016, arXiv:1511
 Importance Sampling Tree for Large-scale Em-pirical Expectation,2016, In International Conference on Machine Learning
 On the Ineffectiveness of Variance Reduced Opti-mization for Deep Learning,2019, Advances in Neural Information Processing Systems
	SAGA: A Fast Incre-mental Gradient Method With Support for Non-Strongly Convex Composite Ob-jectives,2014, Advances in Neural Information Processing Systems
 Variance Re-duced Stochastic Gradient Descent with Neighbors,2015, Advances in Neural Information ProcessingSystems
 Accelerating Stochastic Gradient Descent using Pre-dictive Variance Reduction,2013, Advances in Neural Information Processing Systems
 Adaptive Sampling for Incremental Op-timization Using Stochastic Gradient Descent,2015, In Kamalika Chaudhuri
 A Stochastic Gradient Method with an Expo-nential Convergence _rate for Finite Training Sets,2012, Advances in Neural Information ProcessingSystems
 Non-Uniform Stochastic Average Gradient Method for Training Conditional RandomFields,1938, In Artificial Intelligence and Statistics
 Minimizing finite sUms with the stochas-tic average gradient,2017, Mathematical Programming
 Safe Adaptive Importance Sam-pling,2017, Advances in Neural Information Processing Systems
 Stochastic Optimization with Importance Sampling for RegUlarizedLoss Minimization,1938, In International Conference on Machine Learning
