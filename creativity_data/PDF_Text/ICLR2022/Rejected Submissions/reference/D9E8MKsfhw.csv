title,year,conference
 Expert gate: Lifelong learning witha network of experts,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Notmnist dataset,2011, Technical report
 Efficientlifelong learning with a-gem,2018, In International Conference on Learning Representations
 On tiny episodic memories in continuallearning,2019, arXiv preprint arXiv:1902
 Lifelong machine learning,2018, Synthesis Lectures on Artificial Intelligenceand Machine Learning
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Ad-versarial continual learning,2020, In Andrea Vedaldi
 Online meta-learning,2019, InKamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Sharpness-aware minimizationfor efficiently improving generalization,2021, In International Conference on Learning Representations
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Universal language model fine-tuning for text classification,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Continual learning by usinginformation of each class holistically,2021, 2021
 Towardsa robust experimental framework and benchmark for lifelong language learning,2021, 2021
 Meta-learning representations for continual learning,2019, arXivpreprint arXiv:1905
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In 5thInternational Conference on Learning Representations
 Overcomingcatastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Rehearsal-free continual learning oversmall non-iid batches,2019, arXiv preprint arXiv:1907
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 Under-standing the role of training regimes in continual learning,2020, arXiv preprint arXiv:2006
 A corpus and cloze evaluation for deeper under-standing of commonsense stories,2016, In Proceedings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Readingdigits in natural images with unsupervised feature learning,2011, In Advances in Neural InformationProcessing Systems (NIPS)
 A survey on transfer learning,2009, IEEE Transactions on knowledgeand data engineering
 Collecting diverse natural language inference problems for sentencerepresentation evaluation,2018, In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Philadelphia: Linguistic Data Consortium,2019,
 Improving language under-standing with unsupervised learning,2018, Technical report
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Anatomy of catastrophic forgetting: Hiddenrepresentations and task semantics,2020, arXiv preprint arXiv:2007
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 CARER: Contex-tualized affect representations for emotion recognition,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Green ai,0001, Commun
 Toward training recurrent neural networks forlifelong learning,2020, Neural computation
 Energy and policy considerations for deeplearning in nlp,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 An empirical study on robustness to spuriouscorrelations using pre-trained language models,2020, Transactions of the Association for ComputationalLinguistics
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Efficient meta lifelong-learning with limited memory,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 A comprehensive survey on transfer learning,2021, Proceedings of the IEEE
