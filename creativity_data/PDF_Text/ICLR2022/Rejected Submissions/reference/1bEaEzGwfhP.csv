title,year,conference
 Text editing by command,2021, In Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Levenshtein Transformer,2019, arXiv:1905
 End-to-end Neural Coreference Res-olution,2017, arXiv:1707
 RoBERTa: A Robustly Optimized BERT Pre-training Approach,2019, arXiv:1907
 Felix: Flexible TextEditing Through Tagging and Insertion,2020, arXiv:2003
 Unsupervised Text Style Transfer with PaddedMasked Language Models,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 Variational inference for learning repre-sentations of natural language edits,2021, In AAAI
 Improving Language Under-standing by Generative Pre-Training,2018, pp
 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2020, arXiv:1910
 Get To The Point: Summarization withPointer-Generator Networks,2017, April 2017
 Attention is All you Need,2017, In I
 Identifying semantic edit in-tentions from revisions in Wikipedia,2017, In Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing
 Learning structural editsvia incremental tree transformations,2021, In International Conference on Learning Representations
 Defending against neural fake news,2019, arXiv preprint arXiv:1905
