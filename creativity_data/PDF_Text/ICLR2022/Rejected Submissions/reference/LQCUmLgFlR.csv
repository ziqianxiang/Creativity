title,year,conference
 A continuous-time view of early stopping for leastsquares regression,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 The implicit regularization of stochastic gradientflow for least squares,2020, In Proceedings of the 37th International Conference on Machine Learning
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences 
 Reconciling modern machine learningpractice and the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning 
 A model of double descent for high-dimensional binary linear classification,2019, arXiv preprint arXiv:1911
 Exact expressions for double descentand implicit regularization via surrogate random design,2019, arXiv preprint arXiv:1912
 High-dimensional asymptotics of prediction: Ridge regressionand classification,2018, The Annals of Statistics 
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, arXiv preprint arXiv:1806
 Efficient and scalable bayesian neural nets with rank-1factors,2020, In International conference on machine learning 
 Shape matters: Understanding theimplicit bias of the noise covariance,2020, arXiv preprint arXiv:2006
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision 
 Matrix analysis ,2012, Cambridge university press
 Measuring the intrinsic dimensionof objective landscapes,2018, arXiv preprint arXiv:1804
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In International Conferenceon Artificial Intelligence and Statistics 
 A brief prehistoryof double descent,2020, Proceedings of the National Academy of Sciences 
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Understanding overfitting peaks in generalization error: Analytical risk curves forl_2 and l_1 penalized interpolation,2019, arXiv preprint arXiv:1906
 The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime,2019, arXivpreprint arXiv:1911
 Generalization and parameter estimation in feedforward nets:Some experiments,1989, Advances in neural information processing systems 
 Optimal regularization canmitigate double descent,2020, arXiv preprint arXiv:2003
 Implicit regularization in deep learning may not be explainable bynorms,2020, arXiv preprint arXiv:2005
 Implicit regularization for optimalsparse recovery,2019, Advances in Neural Information Processing Systems 
 The statistical complexity of early-stopped mirror descent,2020, arXiv preprint arXiv:2002
 A simple generalization of a result for random matriceswith independent sub-gaussian rows,2016, arXiv preprint arXiv:1612
 Rethinking bias-variancetrade-off for generalization of neural networks,2020, In International Conference on Machine Learning
 On early stopping in gradient descent learn-ing,2007, Constructive Approximation 
 Boosting with early stopping: Convergence and consistency,2005, The Annalsof Statistics 
