title,year,conference
 Quantized compressive sampling of stochastic gradients for effi-cient communication in distributed deep learning,2020, In Proc
 QSGD:Communication-efficient SGD via gradient quantization and encoding,2017, Proc
 The convergence of sparsified gradient methods,2019, Proc
 In Proc,2018, of Intl
 signSGD withmajority vote is communication efficient and fault tolerant,2018, In Proc
 Finding frequent items in data streams,2002, InIntl
 Support-vector networks,1995, Machine learning
 Adaptive gradient quantization for data-parallel SGD,2020, Proc
 Deep residual learning for image recog-nition,2016, In Proc
 A better alternative to error feedback for CommUnication-efficient distributed learning,2020, arXiv preprint arXiv:2006
 Natural compression for distributed deep learning,2019, arXiv preprint arXiv:1905
 Error feedback fixessignSGD and other gradient compression schemes,2019, In Proc
 Deep gradient compression: Reduc-ing the communication bandwidth for distributed training,2018, In Proc
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Mixed precisiontraining,2018, In Proc
 IntSGD: Floatlesscompression of stochastic gradients,2021, arXiv preprint arXiv:2102
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Zero: Memory optimizationstoward training trillion parameter models,2020, In Proc
 Zero-shot text-to-image generation,2021, arXiv:2102
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 1-bit stochastic gradient descent andits application to data-parallel distributed training of speech dnns,2014, In Proc
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, arXiv preprint arXiv:1909
 Low-memory neural network training: A technical report,2019, arXiv preprintarXiv:1904
 A theory of the learnable,1984, Communications of the ACM
 ATOMO: Communication-efficient learning via atomic sparsification,2018, Proc
 Gradient sparsification for communication-efficient distributed optimization,2018, Proc
 TernGrad:Ternary gradients to reduce communication in distributed deep learning,2017, Proc
 Themarginal value of adaptive gradient methods in machine learning,2017, In Proc
 Error compensated quantized sgdand its applications to large-scale distributed optimization,2018, In Proc
 CSER:Communication-efficient SGD with error reset,2020, Proc
 Step-Ahead Error Feedback for Distributed Trainingwith Compressed Gradient,2021, In Proceedings of the AAAI Conference on Artificial Intelligence
 Compressed communication for distributed deeplearning: Survey and quantitative evaluation,2020, Technical report
 Gradiveq: Vector quantization forbandwidth-efficient gradient aggregation in distributed cnn training,2018, Proc
 Communication-efficient distributed blockwise mo-mentum SGD with error-feedback,2019, Proc
 As discussed in Section 4,2020,2
