title,year,conference
 Escaping saddles with stochasticgradients,1155, In International Conference on Machine Learning
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 How to escape saddle pointsefficiently,1724, In International Conference on Machine Learning
 Adam: A method for stochastic optimization,2015, 3rd InternationalConference on Learning Representations
 Adaptive gradient methods with dynamic bound oflearning rate,2019, 7th International Conference on Learning Representations
 Ageneric approach for escaping saddle points,1233, In International Conference on Artificial Intelligenceand Statistics
 On the convergence of adam and beyond,2018, 6thInternational Conference on Learning Representations
 Positive-negative momentum: Manipulatingstochastic gradient noise to improve generalization,1144, In International Conference on MachineLearning
