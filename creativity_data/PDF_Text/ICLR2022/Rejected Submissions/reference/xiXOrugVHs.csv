title,year,conference
 ETC: Encoding long and struc-tured inputs in transformers,2020, In Proceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP)
 The best of bothworlds: Combining recent advances in neural machine translation,2018, In Proceedings of the 56thAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 Summscreen: A dataset for abstrac-tive screenplay summarization,2021, arXiv preprint arXiv:2104
 Generating long sequences with sparsetransformers,2019, ArXiv
 Hierarchical multiscale recurrent neural net-works,2016, arXiv preprint arXiv:1609
 Sliding selector network with dynamic memory for extractive summarizationof long documents,2021, In Proceedings of the 2021 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies
 Long short-term memory,1997, Neural computation
 Efficient attentions forlong document summarization,2021, In Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Transformers are rnns: Fast autoregressivetransformers with linear attention,2020, In Proceedings of the International Conference on MachineLearning (ICML)
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 ROUGE: A Package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 Roberta: A robustly oPtimized bert PretrainingaPProach,2019, ArXiv
 Biva: A very deep hierarchy oflatent variables for generative modeling,2019, Advances in Neural Information Processing Systems
 Samplernn: An unconditional end-to-end neural audiogeneration model,2016, arXiv preprint arXiv:1612
 Abstractive text summarizationusing sequence-to-sequence rnns and beyond,2016, arXiv preprint arXiv:1602
 Learningmulti-layer latent variable model via variational optimization of short run mcmc for approximateinference,2020, In European Conference on Computer Vision
 On extractive and abstractiveneural document summarization with transformer language models,2020, In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Get to the point: Summarization withpointer-generator networks,1073, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Laddervariational autoencoders,2016, Advances in neural information processing systems
 Synthesizer: Re-thinking self-attention for transformer models,2021, In International Conference on Machine Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Recursively summarizing books with human feedback,2021, arXiv preprint arXiv:2109
 Unsupervised extractivesummarization by pre-training hierarchical transformers,2020, In Findings of the Association forComputational Linguistics: EMNLP 2020
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
 Pegasus: Pre-training with extractedgap-sentences for abstractive summarization,2020, In International Conference on Machine Learning
 HIBERT: Document level pre-training of hier-archical bidirectional transformers for document summarization,2019, In Proceedings of the 57thAnnual Meeting of the Association for Computational Linguistics
