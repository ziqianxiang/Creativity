title,year,conference
 Deep equals shallow for relu networks in kernel regimes,2020, arXivpreprint arXiv:2009
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Hierarchically compositionaltasks and deep convolutional networks,2020, arXiv preprint arXiv:2006
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Spherical harmonics in p dimensions,2012, 2012
 Limitations of lazytraining of two-layers neural networks,2019, arXiv preprint arXiv:1906
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Deep residual learning for image recog-nition,2016, In Conference on Computer Vision and Pattern Recognition
 The surprising simplicity of the early-time learning dynamics of neural networks,2020, arXiv preprint arXiv:2006
 On the neural tangent kernel of deep networks withorthogonal initialization,2020, arXiv preprint arXiv:2004
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Generalization and network design strategies,1989, In Connectionism in perspective
 Deep neural networks as gaussian processes,2018, In International Conference on LearningRepresentations
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in Neural Information Processing Systems
 Generalization error of random fea-tures and kernel methods: hypercontractivity and kernel matrix concentration,2021, arXiv preprintarXiv:2101
 Learning with invariances in randomfeatures and kernel models,2021, arXiv preprint arXiv:2102
 Methods for interpreting and un-derstanding deep neural networks,2018, Digital Signal Processing
 Bayesian deep convolutional networks with many chan-nels are gaussian processes,2019, In International Conference on Learning Representations
 Bayesian deep convolutional networks withmany channels are gaussian processes,2019, In International Conference on Learning Representations
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Advances In NeuralInformation Processing Systems
 Deep informationpropagation,2017, International Conference on Learning Representations
 Improved proteinstructure prediction using potentials from deep learning,2020, Nature
 Neural kernels without tangents,2020, In InternationalConference on Machine Learning
 Masteringthe game of go with deep neural networks and tree search,2016, Nature
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Feature learning in infinite-width neural networks,2020, arXiv preprintarXiv:2011
