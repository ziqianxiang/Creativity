title,year,conference
 Neural symbolic regression that scales,2021, arXivpreprint arXiv:2106
 Multiplying matrices without multiplying,2021, arXiv preprintarXiv:2106
 End-to-end object detection with transformers,2020, arXiv preprintarXiv:2005
 Learning advanced mathematical com-putations from examples,2020, arXiv preprint arXiv:2006
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Neural networks for computing eigenvalues and eigenvec-tors,1992, Biological Cybernetics
 Approximation by superpositions of a sigmoidal function,1989, Mathematics ofcontrol
 Speech-transformer: A no-recurrence sequence-to-sequencemodel for speech recognition,2018, In 2018 IEEE International Conference on Acoustics
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, arXiv preprint arXiv:2010
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Teach-ing temporal logics to neural networks,2021, arXiv preprint arXiv:2003
 Long short-term memory,1997, Neural computation
 Neural gpus learn algorithms,2015, arXivpreprint arXiv:1511
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Deep learning for symbolic mathematics,2019, arXiv preprintarXiv:1912
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Random Matrices,2004, Academic Press
 Investigating the limitations of transformers withsimple arithmetic tasks,2021, arXiv preprint arXiv:2102
 Moment-based spectral analysis of random graphs withgiven expected degrees,2017, arXiv preprint arXiv:1512
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAIblog
 A neural network for computing eigenvectors and eigenval-ues,1991, Biological Cybernetics
 Analysing mathematical rea-soning abilities of neural models,2019, arXiv preprint arXiv:1904
 Neural arithmeticlogic units,2018, arXivpreprint arXiv:1808
 Fast transformers with clustered atten-tion,2020, arXiv preprint arXiv:2007
 A recurrent neural network for real-time matrix inversion,1993, Applied Mathematics andComputation
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Big bird: Transformersfor longer sequences,2021, arXiv preprint arXiv:2007
 From zhang neural network to newton iterationfor matrix inversion,2008, IEEE Transactions on Circuits and Systems I: Regular Papers
