title,year,conference
 Understanding double descent requires a fine-grained bias-variancedecomposition,2020, In H
 High-dimensional dynamics of generalization errorin neural networks,2020, Neural Networks
 Essentialcell biology,2015, Garland Science
 Intrinsic dimension of datarepresentations in deep neural networks,2019, In Advances in Neural Information Processing Systems
 Implicit regularization in deep matrix factoriza-tion,2019, In H
 A CloserLook at Memorization in Deep Networks,2017, In Proceedings of the 34th International Conference onMachine Learning
 Explaining neural scaling laws,2021, arXiv preprintarXiv:2102
 Reconciling modern machine-learning practice and theclassical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Revisitingresnets: Improved training and scaling strategies,2021, arXiv preprint arXiv:2103
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 On the global convergence of gradient descent for over-parameterized modelsusing optimal transport,2018, In Advances in Neural Information Processing Systems 31
 Implicit bias of gradient descent for wide two-layer neural networks trainedwith the logistic loss,2020, In Conference on Learning Theory
 On lazy training in differentiable programming,2019, In Advances inNeural Information Processing Systems
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Characterizing implicit bias in terms of optimizationgeometry,2018, In International Conference on Machine Learning
 Implicit bias of gradient descenton linear convolutional networks,2018, 31
 Springer New York Inc,2001,
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Scaling laws for autoregressive generative modeling,2020, arXiv preprintarXiv:2010
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 The implicit bias of gradient descent on nonseparable data,2019, In Conference onLearning Theory
 Scaling laws for neural language models,2020, arXiv preprint arXiv:2001
 Learning multiple layers of features from tiny images,2009, 2009
 Explaining landscape connectivity of low-cost solutions for multilayer nets,2019, arXivpreprint arXiv:1906
 Bad global minima exist and sgd can reachthem,2020, In H
 A mean field view of the landscape of two-layer neuralnetworks,2018, Proceedings ofthe National Academy ofSciences
 The generalization error of random features regression: Preciseasymptotics and the double descent curve,2019, Communications on Pure and Applied Mathematics
 Deepdouble descent: Where bigger models and more data hurt,2020, In International Conference on LearningRepresentations
 A moderntake on the bias-variance tradeoff in neural networks,2018, arXiv preprint arXiv:1810
 In search of the real inductive bias: On the role of implicitregularization in deep learning,2015, In ICLR
 On connectivity of solutions in deep learn-ing: The role of over-parameterization and feature quality,2021, In Advances in Neural InformationProcessing Systems
 Classifying high-dimensional gaussianmixtures: Where kernel methods fail and neural networks succeed,2021, In ICML
 A constructive prediction of the generaliza-tion error across scales,2020, In International Conference on Learning Representations
 Parameters as interacting particles: long time convergenceand asymptotic error scaling of neural networks,2018, In Advances in Neural Information ProcessingSystems 31 
 A neural scaling law from the dimension of the data manifold,2020, arXivpreprint arXiv:2004
 Landscape connectivity and dropout stability ofsgd solutions for over-parameterized neural networks,2020, In International Conference on MachineLearning
 The implicit bias of gradient descent on separable data,2018, InInternational Conference on Learning Representations
 Residual networks behave like ensemblesof relatively shallow networks,2016, In D
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Understanding deep learning requiresrethinking generalization,2017, In ICLR
 The per-GPU batch size is set to 128 and is halved for the widest networksto fit in the GPU memory,2017, The networks are trained on 8 or 16 Volta V100 GPUs so as to keep thebatch size B equal to 1024
