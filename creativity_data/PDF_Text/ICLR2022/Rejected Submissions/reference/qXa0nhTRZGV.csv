title,year,conference
 A closer look atmemorization in deep networks,2017, arXiv Preprint arXiv:1706
 Unlabeleddata improves adversarial robustness,2019, In NeUrIPS
 Robust overfittingmay be mitigated by properly learned smoothening,2021, In InternatiOnal COnference on LearningRePreSentations
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, ICML
 Data profiling for adversarial training: On the ruin ofproblematic data,2021, arXiv PrePrint arXiv:2102
 Entropy-sgd optimizes the prior of a pac-bayes bound:Generalization properties of entropy-sgd and data-dependent priors,2018, In InternatiOnal COnferenCeon MaChine Learning
 Sharpness-aware minimizationfor efficiently improving generalization,2021, In International COnferenCe on Learning RePreSentations
 Robust loss functions under label noise for deepneural networks,2017, AAAL 2017
 Uncoveringthe limits of adversarial training against norm-bounded adversarial examples,2020, arXiv PrePrintarXiv:2010
 Speech recognition with deep re-current neural networks,2013, In 2013 IEEE international COnferenCe on acoustics
 Shape matters: Understanding the implicitbias of the noise covariance,2020, arXiv PrePrint arXiv:2006
 Identity mappings in deep residualnetworks,2016, ECCV
 Simplifying neural nets by discovering flat minima,1995, InAdvanCeS in neural information PrOCeSSing systems
 Self-adaptive training: beyond empirical riskminimization,2020, arXiv PrePrint arXiv:2002
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International COnferenCeon MaChine Learning
 Fantasticgeneralization measures and where to find them,2019, arXiv PrePrint arXiv:1912
 An analysis of noise in recurrent neural networks:convergence and generalization,1996, IEEE TranSaCtiOnS on neural networks
 Linear convergence of gradient and proximal-gradient methodsunder the polyak-Lojasiewicz condition,2016, In MaChine Learning and Knowledge DiSCOVery inDatabases
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivPrePrint arXiv:1609
 Temporal ensembling for semi-supervised learning,2016, arXiv PrePrintarXiv:1610
 Efficient backprop,2012, InNeUraI networks: TriCkS of the trade
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, arXiv PrePrint arXiv:2002
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In International COnferenCeon ArtifiCiaI Intelligence and StatiStics
 Extrapolation for large-batch training indeep learning,2020, In International COnferenCe on MaChine Learning
 Early-learningregularization prevents memorization of noisy labels,2020, AdVances in NeUraI InfOrmatiOn ProcessingSystems
 Bad global minima exist and sgdcan reach them,2019, arXiv PrePrint arXiv:1906
 A simple yet effective baseline for robust deep learning withnoisy labels,2019, arXiv PrePrint arXiv:1909
 DeePdouble descent: Where bigger models and more data hurt,2019, arXiv PrePrint arXiv:1912
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv PrePrint arXiv:1412
 ImPlicit bias of sgd for diagonallinear networks: a provable benefit of Stochasticity,2021, In AdVanCeS in NeUraI Information ProCeSSingSyStems
 Fixing data augmentation to improve adversarial robustness,2021, arXiv PrePrintarXiv:2103
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv PrePrintarXiv:1412
 Overfitting in adversarially robust deep learning,2020, In ICML
 Fixmatch: SimPlifying semi-suPervised learning withconsistency and confidence,2020, arXiv PrePrint arXiv:2001
 Robust and on-the-flydataset denoising for image classification,2020, arXiv PrePrint arXiv:2003
 Combating label noise in deep learning using abstention,2019, arXiv PrePrint arXiv:1905
 Smoothout:Smoothing out sharp minima to improve generalization in deep learning,2018, arXiv PrePrintarXiv:1805
 Kernel and rich regimes in overparametrized models,2020, InProceedingS of Thirty Third Conference on Learning Theory
 A unifying view on implicit bias in traininglinear neural networks,2021, In International COnference on Learning RePreSentations
 Understandingdeep learning requires rethinking generalization,2016, arXiv PrePrint arXiv:1611
 mixup: Beyond empiricalrisk minimization,2017, arXiv PrePrint arXiv:1710
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, AdVanceS in neural information PrOceSSing systems
 Regularizing neural networks via adversarialmodel perturbation,2021, CVPR
