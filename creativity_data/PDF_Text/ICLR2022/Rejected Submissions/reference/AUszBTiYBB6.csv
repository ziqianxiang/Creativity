title,year,conference
 Byzantine stochastic gradient descent,2018, arXiv preprintarXiv:1803
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT
 Convex optimization: Algorithms and complexity,2014, Foundations and Trends R inMachine Learning
 The non-iid data quagmire ofdecentralized machine learning,2020, In International Conference on Machine Learning
 Stochastic-sign SGD forfederated learning with theoretical guarantees,2020, arXiv preprint arXiv:2002
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Communication efficient distributedmachine learning with the parameter server,2014, Advances in Neural Information Processing Systems
 A framework for evaluating gradient leakage attacks in federated learning,2020, arXivpreprint arXiv:2004
 Byzantine-robust distributedlearning: Towards optimal statistical rates,2018, In International Conference on Machine Learning
 Federatedlearning with non-iid data,2018, arXiv preprint arXiv:1806
