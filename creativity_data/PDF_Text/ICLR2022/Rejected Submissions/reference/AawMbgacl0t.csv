title,year,conference
 Onexact computation with an infinitely wide neural net,2019, In Proceedings of the 33rd InternationalConference on Neural Information Processing Systems
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, Advances in Neural Information ProcessingSystems
 Bagging predictors,1996, Machine learning
 Coherent gradients: An approach to Understanding generalization in gradientdescent-based optimization,2019, In International Conference on Learning Representations
 RedUcing over-fitting in deep networks by decorrelating representations,2016, ICLR
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Demystifying dropoUt,2019, In International Conference onMachine Learning
 Neural tangent kernel: convergence and gen-eralization in neUral networks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Data-dependent stability of stochastic gradient descent,2018, InInternational Conference on Machine Learning
 A variational analysis of stochastic gradientalgorithms,2016, In International conference on machine learning
 Insights on representational similarity in neuralnetworks with canonical correlation,2018, NeuRIPS
 Modern neural networks generalize on smalldata sets,2018, In Proceedings ofthe 32nd International Conference on Neural Information ProcessingSystems
 Complexity control by gradient descent indeep networks,2020, Nature communications
 Svcca: Singular vectorcanonical correlation analysis for deep learning dynamics and interpretability,2017, NeuRIPS
 Theimpact of neural network overparameterization on gradient confusion and stochastic gradient de-scent,2020, In International Conference on Machine Learning
 The boosting approach to machine learning: An overview,2003, Nonlinear estimationand classification
 Residual networks behave like ensembles ofrelatively shallow networks,2016, Advances in neural information processing systems
 Gradient diversity: a key ingredient for scalable distributed learning,2018, In InternationalConference on Artificial Intelligence and Statistics
 The anisotropic noise in stochas-tic gradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InInternational Conference on Machine Learning
