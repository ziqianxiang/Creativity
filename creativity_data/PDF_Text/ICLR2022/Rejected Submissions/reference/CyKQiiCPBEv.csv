title,year,conference
 Chemberta: Large-scale self-supervised pretraining for molecular property prediction,2020, arXiv preprint arXiv:2010
 Enhanced deep-learning prediction of molecular properties viaaugmentation of bond topology,2019, ChemMedChem
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Distilling structured knowledge fortext-based relational reasoning,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 Convolutional networks on graphs forlearning molecular fingerprints,2015, arXiv preprint arXiv:1509
 Deep docking: a deep learning platform for augmentationof structure based drug discovery,2020, ACS central science
 Smiles transformer: Pre-trained molecular fingerprintfor low data drug discovery,2019, arXiv preprint arXiv:1911
 Open graph benchmark: Datasets for machine learning on graphs,2020, arXivpreprint arXiv:2005
 Ogb-lsc:A large-scale challenge for machine learning on graphs,2021, arXiv preprint arXiv:2103
 Improving the search performance of extended Con-nectivity fingerprints through activity-oriented feature filtering and application of a bit-density-dependent similarity function,2009, ChemMedChem: Chemistry Enabling Drug Discovery
 Chemformer: A pre-trainedtransformer for computational chemistry,2021, 2021
 Semi-supervised classification with graph convolutional net-works,2016, arXiv preprint arXiv:1609
 Directional message passing for molec-ular graphs,2020, arXiv preprint arXiv:2003
 Hamnet: Conformation-guided molecularrepresentation with hamiltonian neural networks,2021, arXiv preprint arXiv:2105
 Self-supervised graph transformer on large-scale molecular data,2020, arXiv preprintarXiv:2007
 Molecular transformer: a model for uncertainty-calibrated chemicalreaction prediction,2019, ACS central science
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Mobilebert:Task-agnostic compression of bert by progressive knowledge transfer,2019, 2019b
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Smiles-bert: largescale unsupervised pre-training for molecular property prediction,2019, In Proceedings of the 10thACM international conference on bioinformatics
 Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers,2020, arXiv preprintarXiv:2002
 Moleculenet: a benchmark for molecular machine learn-ing,2018, Chemical science
 Learning neural generativedynamics for molecular conformation generation,2021, arXiv preprint arXiv:2102
 Seq2seq fingerprint: An unsuperviseddeep molecular embedding for drug discovery,2017, In Proceedings of the 8th ACM internationalconference on bioinformatics
 Seq3seqfingerprint: towards end-to-end semi-supervised deep drug discovery,2018, In Proceedings of the 2018ACM International Conference on Bioinformatics
