title,year,conference
 Sorting out lipschitz function approximation,2019, InInternational Conference on Machine Learning
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, In InternationalConference on Machine Learning
 Residual flows forinvertible generative modeling,2019, Advances in Neural Information Processing Systems
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In International Conference on MachineLearning
 Nice: Non-linear independent componentsestimation,2014, arXiv preprint arXiv:1410
 Density estimation using real nvp,2016, arXivpreprint arXiv:1605
 Dizzyrnn: Reparameterizing recurrentneural networks for norm-preserving backpropagation,2016, arXiv preprint arXiv:1612
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Ffjord:Free-form continuous dynamics for scalable reversible generative models,2018, arXiv preprintarXiv:1810
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Orthogonal recurrent neural networks with scaledcayley transform,2018, In International Conference on Machine Learning
 Recurrent orthogonal networks and long-memorytasks,2016, In International Conference on Machine Learning
 The scaling and squaring method for the matrix exponential revisited,2009, SIAMreview
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Densely connectedconvolutional networks,2017, In CVPR
 Paraunitary matrices,2012, arXiv preprint arXiv:1205
 Improving training of deep neural networksvia singular value bounding,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in neural information processing systems
 Normalizing flows: An introduction and reviewof current methods,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Trivializations for gradient-based optimization on manifolds,2019, Advances inNeural Information Processing Systems
 Cheap orthogonal constraints in neural networks:A simple parametrization of the orthogonal and unitary group,2019, In International Conference onMachine Learning
 Efficient riemannian optimization on the stiefel manifold viathe cayley transform,2019, In International Conference on Learning Representations
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Complex unitary recurrent neuralnetworks using scaled cayley transform,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Spectral normalization forgenerative adversarial networks,2018, In International Conference on Learning Representations
 Normalizing flows for probabilistic modeling and inference,2019, arXiv preprintarXiv:1912
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Proceedings of the 31st InternationalConference on Neural Information Processing Systems
 Deep isometric learning forvisual recognition,2020, In International Conference on Machine Learning
 The singular values of convolutional layers,2019, InInternational Conference on Learning Representations
 Skew orthogonal convolutions,2021, arXiv preprint arXiv:2105
 Wavelets and filter banks,1996, SIAM
 Sylvesternormalizing flows for variational inference,2018, In 34th Conference on Uncertainty in ArtificialIntelligence 2018
 A comparison of design methods for 2-d fir orthogonalperfect reconstruction filter banks,1995, IEEE Transactions on Circuits and Systems II: Analog andDigital Signal Processing
 On orthogonality and learningrecurrent networks with long term dependencies,2017, In International Conference on Machine Learning
 Orthogonal convolutional neuralnetworks,2019, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Scaling provable adversarialdefenses,2018, In Proceedings of the 32nd International Conference on Neural Information ProcessingSystems
 Fast is better than free: Revisiting adversarial training,2020, InInternational Conference on Learning Representations
 All you need is beyond a good init: Exploring better solutionfor training extremely deep convolutional neural networks with orthonormality and modulation,2017, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Network deconvolution,2020, In International Conference onLearning Representations
 Stabilizing gradients for deep neural networks via efficientsvd parameterization,2018, In International Conference on Machine Learning
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
3 Interpretations of Previous ApproachesIn Theorem B,2020,5
 In Proposition D,2018,1
