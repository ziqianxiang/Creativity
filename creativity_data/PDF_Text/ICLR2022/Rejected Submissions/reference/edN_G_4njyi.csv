title,year,conference
 Optimal Client Sampling for Federated Learn-ing,2020, Workshop in NeurIPS: Privacy Preserving Machine Learning
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv
 Tighter theory for local sgd on iden-tical and heterogeneous data,2020, In Silvia Chiappa and Roberto Calandra (eds
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Learning multiple layers of features from tiny images,2009, 2009
 In I,2020, Dhillon
 On the convergence offedavg on non-iid data,2020, In International Conference on Learning Representations
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, In Kamalika Chaudhuri and Masashi Sugiyama (eds
 Adaptive federated optimization,2021, In InternationalConference on Learning Representations
 Dynamic federated learning,2020, In 2020 IEEE 21stInternational Workshop on Signal Processing Advances in Wireless Communications (SPAWC)
 Efficient algorithms for modifying and sampling from a categorical distribution,2019, CoRR
 Atomo: Communication-efficient learning via atomic sparsification,2018, In S
 Cooperative SGD: A unified Framework for the Design and Analysisof Communication-Efficient SGD Algorithms,2018, 2018
 Matcha: Speed-ing up decentralized sgd via matching decomposition sampling,2019, In 2019 Sixth Indian ControlConference (ICC)
 Tackling the objective in-consistency problem in heterogeneous federated optimization,2020, In Hugo Larochelle
 Slowmo: Improvingcommunication-efficient distributed sgd with slow momentum,2020, In International Conference onLearning Representations
 AdaGrad stepsizes: Sharp convergence over noncon-vex landscapes,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
