title,year,conference
 The effects of adding noise during backpropagation training on a generalizationperformance,1996, Neural computation
 The inverse variance-flatness relation in stochastic gradient descent is criticalfor finding flat minima,2021, Proceedings of the NatiOnal ACademy of Sciences
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In PrOCeedingS of the thirteenth international COnferenCe on artificial intelligence andStatiStics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalCOnference on COmPUter vision
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv PreprintarXiv:1207
 Three factors influencing minima in sgd,2017, arXiv PreprintarXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivPreprint arXiv:1609
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Visualizing the loss land-scape of neural nets,2017, arXiv preprint arXiv:1712
 A pac-bayesian tutorial with a dropout bound,2013, arXiv preprint arXiv:1307
 On convergence and generalization of dropout training,2020, AdVanCeSin NeUraI InfOrmatiOn PrOCeSSing Systems
 Exploring gener-alization in deep learning,2017, arXiv preprint arXiv:1706
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Dropout training as adaptive regularization,2013, AdVanceSin neural information processing systems
 How sgd selects the global minima in over-parameterized learning: Adynamical stability perspective,2018, AdVanceS in NeUraI InfOrmatiOn Processing Systems
 Embedding principle of losslandscape of deep neural networks,2021, arXiv PrePrint arXiv:2105
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2018, arXivPrePrint arXiv:1803
