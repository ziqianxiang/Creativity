title,year,conference
 Exponential decay sine wave learning rate for fast deepneural network training,2017, In 2017 IEEE Visual Communications and Image Processing (VCIP)
 Universal stagewise learning for non-convexproblems with convergence on averaged solutions,2019, In International Conference on LearningRepresentations
 Relative deviation learning bounds and generalization withunbounded loss functions,2019, Annals of Mathematics and Artificial Intelligence
 Understanding the role of momentum in non-convex optimization: Practical insightsfrom a lyapunov analysis,2020, arXiv preprint arXiv:2010
 Global convergence of the heavy-ball methodfor convex optimization,2015, In 2015 European Control Conference (ECC)
 Accelerated gradient methods for nonconvex nonlinear and stochasticprogramming,2016, Mathematical Programming
 Understanding the role of momentum in stochasticgradient methods,2019, Advances in Neural Information Processing Systems
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 A nonmonotone learning rate strategy for SGD training of deep neuralnetworks,2015, In 2015 IEEE International Conference on Acoustics
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Imagenet classification with deep convolutional neuralnetworks,2012, In Advances in Neural Information Processing Systems
 Stochastic polyak step-size for sgd: Anadaptive learning rate for fast convergence,2021, In International Conference on Artificial Intelligenceand Statistics
 Convergence of a stochastic gradient method with momentum for non-smooth non-convex optimization,2020, In International Conference on Machine Learning
 Some methods of speeding up the convergence of iteration methods,1964, Ussr computationalmathematics and mathematical physics
 A stochastic approximation method,1951, The Annals of Mathematical Statistics
 Towards flatter loss surface via nonmonotonic learningrate scheduling,2018, In UAI2018 Conference on Uncertainty in Artificial Intelligence
 On the hyperparameters in stochastic gradient descent with momentum,2021, arXiv preprintarXiv:2108
 On learning rates and Schrodinger operators,2020, arXiv preprintarXiv:2004
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE Winter Conferenceon Applications of Computer Vision (WACV)
 On the importance of initialization and momentumin deep learning,2013, In International Conference on Machine Learning
 Statistical learning theory,1998, Wiley
 On the convergence of step decay step-size for stochasticoptimization,2021, arXiv preprint arXiv:2102
 Stagewise training accelerates convergence of testing error overSGD,2019, Advances in Neural Information Processing Systems
 Wide residual networks,2016, arXiv preprint arXiv:1605
