title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Impact of spatial frequency based constraints on adversarialrobustness,2021, CoRR
 Adversarial examples are not easily detected: Bypassingten detection methods,2017, Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, ArXiv
 Keeping the bad guys out: Protecting and vaccinating deep learning with jpegcompression,2017, ArXiv
 Frequency-tuned universal adversarial attacks,2020, CoRR
 A study of the effect of JPGcompression on adversarial images,2016, CoRR
 Explaining and harnessing adversarial ex-amples,2015, CoRR
 Low frequency adversarial perturbation,2019, InUAI
 Principal component propertiesof adversarial samples,2019, ArXiv
 Adversarial logit pairing,2018, ArXiv
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In ICML
 Learning multiple layers of features from tiny images,2009, Technical report
 Detecting autoattackperturbations in the frequency domain,2021, 2021
 Distillation as a defense to adversarialperturbations against deep neural networks,2016, 2016 IEEE Symposium on Security and Privacy (SP)
 Grad-cam: Why did you say that? visual explanations from deep networks viagradient-based localization,2016, CoRR
 On the effectiveness of low fre-quency perturbations,2019, ArXiv
 One pixel attack for fooling deep neuralnetworks,2019, IEEE Transactions on Evolutionary Computation
 Intriguing properties of neural networks,2014, CoRR
 Toward few-step adversarial trainingfrom a frequency perspective,2020, Proceedings of the 1st ACM Workshop on Security and Privacy onArtificial Intelligence
 Towards frequency-based explanation for robust CNN,2020, CoRR
 Towards frequency-basedexplanation for robust cnn,2020, ArXiv
 Feature squeezing: Detecting adversarial examples in deepneural networks,2018, ArXiv
