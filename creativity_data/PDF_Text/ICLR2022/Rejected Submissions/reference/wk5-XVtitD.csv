title,year,conference
 Can language models encode perceptual structure without grounding? a case studyin color,2021, arXiv preprint arXiv:2109
 Playing text-adventure games with graph-based deepreinforcement learning,2018, arXiv preprint arXiv:1812
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Decision transformer: Reinforcement learningvia sequence modeling,2021, arXiv preprint arXiv:2106
 Textworld: A learningenvironment for text-based games,2018, In Workshop on Computer Games
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Latent semantic analysis,2004, Annual review of information science and technology
 Human instruction-following with deepreinforcement learning via transfer-learning from text,2020, arXiv preprint arXiv:2005
 Long short-term memory,1997, Neural computation
 Reinforcement learning as one big sequence mod-eling problem,2021, arXiv preprint arXiv:2106
 Multilingual constituency parsing with self-attention andpre-training,2018, arXiv preprint arXiv:1812
 Planning as search: A quantitative approach,2021, Artificial intelligence
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, arXiv preprint arXiv:1908
 Pretrained transformers as universalcomputation engines,2021, arXiv preprint arXiv:2103
 Language understanding for text-basedgames using deep reinforcement learning,2015, arXiv preprint arXiv:1506
 Algorithms for inverse reinforcement learning,2000, In Icml
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Value-agnosticconversational semantic parsing,2021, In Proceedings of the 59th Annual Meeting of the Associationfor Computational Linguistics and the 11th International Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers)
 Alvinn: An autonomous land vehicle in a neural network,1989, Technical report
 Virtualhome: Simulating household activities via programs,2018, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Watch-and-help: A challenge for social perception and human-ai collaboration,2020, arXivpreprint arXiv:2010
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Rail: Risk-averse imitation learning,2017, arXiv preprintarXiv:1707
 Behavioral cloning from observation,2018, arXiv preprintarXiv:1805
 MUlti-modal few-shot learning with frozen langUage models,2021, arXiv preprint arXiv:2106
 Attention is all yoU need,2019, arXiv preprint arXiv:1706
 HUggingfaceâ€™s transformers:State-of-the-art natUral langUage processing,2019, arXiv preprint arXiv:1910
 Xlnet: Generalized aUtoregressive pretraining for langUage Understand-ing,2019, In H
 Keep calm and explore:LangUage models for action generation in text-based games,2020, arXiv preprint arXiv:2010
 Reading and acting while blindfolded:The need for semantics in text game agents,2021, arXiv preprint arXiv:2103
 CoUnting to explore and generalize in text-based games,2018, arXiv preprint arXiv:1806
 Hierarchical task learning from langUage instrUctions with Unifiedtransformers and self-monitoring,2021, arXiv preprint arXiv:2106
