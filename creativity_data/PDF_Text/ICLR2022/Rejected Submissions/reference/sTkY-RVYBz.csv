title,year,conference
 Towards shape biased unsupervisedrepresentation learning for domain generalization,2019, arXiv preprint arXiv:1909
 Layer normalization,2016, arXiv preprintarXiv:1607
 Understanding batch normaliza-tion,2018, arXiv preprint arXiv:1806
 Domaingeneralization by solving jigsaw puzzles,2019, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Underspecificationpresents challenges for credibility in modern machine learning,2020, arXiv preprint arXiv:2011
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Deep domain generalization with structured low-rank constraint,2017, IEEETransactions on Image Processing
 Quality resilient deep neural networks,2017, arXiv preprintarXiv:1703
 Domain generalization with domain-specific aggregationmodules,2018, In German Conference on Pattern Recognition
 Batchnormalization is a cause of adversarial vulnerability,2019, arXiv preprint arXiv:1905
 Generalisation in humans and deep neural networks,2018, arXiv preprintarXiv:1808
 Shortcut learning in deep neural networks,2020, Nature MachineIntelligence
 Adversarially robust distillation,2020, InProceedings of the AAAI Conference on Artificial Intelligence
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, arXiv preprint arXiv:1903
 Using self-supervised learningcan improve model robustness and uncertainty,2019, arXiv preprint arXiv:1906
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Arbitrary style transfer in real-time with adaptive instance nor-malization,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Removing spurious features can hurt accuracy and affect groupsdisproportionately,2021, In Proceedings of the 2021 ACM Conference on Fairness
 Supervised contrastive learning,2020, arXiv preprintarXiv:2004
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Towards a theoretical understanding of batch normalization,2018, stat
 Domain generalization with adver-sarial feature learning,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Just interpolate: Kernel “ridgeless” regression cangeneralize,2020, Annals of Statistics
 Understanding regularization in batchnormalization,2018, arXiv preprint arXiv:1809
 Unified deep superviseddomain adaptation and generalization,2017, In Proceedings of the IEEE International Conference onComputer Vision
 More data can hurt for linear regression: Sample-wise double descent,2019, arXivpreprint arXiv:1912
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE symposium onsecurity and privacy (SP)
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 A simple way to make neural networks robust against diverse imagecorruptions,2020, In European Conference on Computer Vision
 Improving robustness against common corruptions by covariate shift adaptation,2020, Advancesin Neural Information Processing Systems
 Grad-cam: Visual explanations from deep networks via gradient-based localiza-tion,2017, In Proceedings of the IEEE international conference on computer vision
 Robustpointset: A dataset for benchmarking robustness of pointcloud classifiers,2020, arXiv preprint arXiv:2011
 Unbiased look at dataset bias,1521, In CVPR 2011
 Instance normalization: The missingingredient for fast stylization,2016, arXiv preprint arXiv:1607
 Examining the impact of blur onrecognition by convolutional networks,2016, arXiv preprint arXiv:1611
 BarloW twins: Self-supervisedlearning via redundancy reduction,2021, arXiv preprint arXiv:2103
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
