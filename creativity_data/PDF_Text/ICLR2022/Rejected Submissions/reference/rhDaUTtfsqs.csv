title,year,conference
 Adaptive input representations for neural language modeling,2018, InInternational Conference on Learning Representations
 Curriculum learning,2009, InProceedings of the 26th annual international conference on machine learning
 Results of thewmt17 neural mt training task,2017, In Proceedings of the second conference on machine translation
 Language models are few-shot learners,2020, In H
 Curriculum learning for language modeling,2021, arXiv preprint arXiv:2108
 Adam: A method for stochastic optimization,2015, CoRR
 Curriculum learning and minibatch bucketing in neural machinetranslation,2017, In Proceedings of the International Conference Recent Advances in Natural LanguageProcessing
 Self-paced learning for latent variablemodels,2010, In NIPS
 1-bitLAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB’s ConvergenceSpeed,2021, arXiv preprint arXiv:2104
 Carbon emissions and large neural network training,2021, arXivpreprint arXiv:2104
 Shortformer: Better language modeling using shorterinputs,2020, arXiv preprint arXiv:2012
 Improving language under-standing by generative pre-training,2018, 2018a
 Languagemodels are unsupervised multitask learners,2018, 2018b
 Better language models and their implications,2019, OpenAI Blog
 Neural network learning control of robot manipulators using gradually increasingtask difficulty,1994, IEEE transactions on Robotics and Automation
 1-bit Adam: Communication Efficient Large-ScaleTraining with Adam’s Convergence Speed,2021, In Proceedings of the 38th International Conferenceon Machine Learning
 Simple and effective curriculum pointer-generator networks for readingcomprehension over long narratives,2019, In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics
 A simple method for commonsense reasoning,2018, arXiv preprintarXiv:1806
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Variance reduction for stochasticgradient optimization,2013, In NIPS
 A comprehensive survey on curriculum learning,2020, arXivpreprint arXiv:2010
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Defending against neural fake news,2019, In Proceedings of the 33rd InternationalConference on Neural Information Processing Systems
 Reducing bert computation bypadding removal and curriculum learning,2021, In 2021 IEEE International Symposium on PerformanceAnalysis of Systems and Software (ISPASS)
 An empirical exploration ofcurriculum learning for neural machine translation,2018, arXiv preprint arXiv:1811
