title,year,conference
 Rank-one modification of theSymmetriceigenproblem,1978, Numerische Mathematik
 Gradient descent onneural networks typically occurs at the edge of stability,2021, In International Conference on LearningRepresentations
 Sharp minima can generalize fordeep nets,2017, In International Conference on Machine Learning
 Sharpness-aware minimizationfor efficiently improving generalization,2021, In International Conference on Learning Representations
 Stiffness: Anew perspective on generalization in neural networks,2019, arXiv preprint arXiv:1901
 Some modified matrix eigenvalue problems,1973, Siam Review
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Flat minima,1997, Neural computation
 Robust learning with jacobian regularization,2019, arXivpreprint arXiv:1908
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 The break-even point on optimization trajectories of deep neuralnetWorks,2020, In International Conference on Learning Representations
 Catastrophic fisher explosion: Earlyphase fisher matrix impacts generalization,2021, In Marina Meila and Tong Zhang (eds
 On the relation betWeen the sharpest directions of DNN loss and the SGD step length,2019, InInternational Conference on Learning Representations
 Fantas-tic generalization measures and where to find them,2020, In International Conference on LearningRepresentations
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In 5thInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Batch size matters: A diffusion approxima-tion framework on nonconvex stochastic gradient descent,2017, stat
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Toward a theory of optimization for over-parameterizedsystems of non-linear equations: the lessons of deep learning,2020, arXiv preprint arXiv:2003
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations
 Implicit regularization in deep learning,2017, arXiv preprint arXiv:1709
 The full spectrum of deepnet hessians at scale: Dynamics with sgd training andsample size,2018, arXiv preprint arXiv:1811
 Measurements of three-level hierarchical structure in the outliers in the spectrumof deepnet hessians,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 The effect of network width onstochastic gradient descent and generalization: an empirical study,2019, In International Conference onMachine Learning
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Eigenvalues of the hessian in deep learning: Singularityand beyond,2016, arXiv preprint arXiv:1611
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Very deep convolutional networks for large-scale imagerecognition,2015, In Yoshua Bengio and Yann LeCun (eds
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 On the origin of implicit regularizationin stochastic gradient descent,2021, In International Conference on Learning Representations
 Identifying generalizationproperties in neural networks,2018, arXiv preprint arXiv:1809
 A walk with sgd,2018, arXiv preprintarXiv:1802
 Pyhessian: Neural networksthrough the lens of the hessian,2020, In 2020 IEEE International Conference on Big Data (Big Data)
 Wide residual networks,2016, arXiv preprint arXiv:1605
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InInternational Conference on Machine Learning
