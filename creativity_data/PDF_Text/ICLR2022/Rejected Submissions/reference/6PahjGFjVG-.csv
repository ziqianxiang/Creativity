title,year,conference
 Observation of a new particle in thesearch for the Standard Model Higgs boson with the ATLAS detector at the LHC,2012, Physics LettersB
 Observation of Gravi-tational Waves from a Binary Black Hole Merger,2016, Physical Review Letters
 Byzantine stochastic gradient descent,2018, In Proceedingsof the 32nd International Conference on Neural Information Processing Systems
 Byzantine-resilient non-convex stochastic gradient descent,2021, In International Conference on Learning Representations
 Looking updata in p2p systems,2003, Communications of the ACM
 A little is enough: Circumventing defensesfor distributed learning,2019, In H
 Robust consensus in distributed networksusing total variation,2013, arXiv preprint arXiv:1309
 Machine learningwith adversaries: Byzantine tolerant gradient descent,2017, In Proceedings of the 31st InternationalConference on Neural Information Processing Systems
 Coin flipping by telephone a protocol for solving impossible problems,1983, ACM SIGACTNews
 Language models are few-shot learners,2020, In H
 Draco: Byzantine-resilient distributed training via redundant gradients,2018, In International Conference on MachineLearning
 Limits on the security of coin flips when half the processors are faulty,1986, In Proceedingsof the eighteenth annual ACM symposium on Theory of computing
 Aggregathor: Byzantine machine learning via robustgradient aggregation,2019, In The Conference on Systems and Machine Learning (SysML)
 Large scale dis-tributed deep networks,2012, In F
 On the ineffectiveness of variance reduced optimization fordeep learning,2019, In H
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances In Neural InformationProcessing Systems
 The sybil attack,2002, In IPTPS
 The hidden vulnerability of dis-tributed learning in Byzantium,2018, In Jennifer Dy and Andreas Krause (eds
 Byzantine fault-tolerance in Peer-to-Peer distributed gradient-descent,2021, arXiv preprint arXiv:2101
 Deep residual learning for imagerecognition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Byzantine-robust learning on heterogeneousdatasets via resampling,2020, arXiv preprint arXiv:2006
 Gpipe: Efficient training of giant neural networks using pipeline parallelism,2019, ArXiv
 Learning from history for byzantine robustoptimization,2020, arXiv preprint arXiv:2012
 Decentralized deep learning witharbitrary communication compression,2020, In International Conference on Learning Representations
 Albert: A lite bert for self-supervised learning of language representations,2019, ArXiv
 Pytorch distributed: Experienceson accelerating data parallel training,2020, Proc
 Deep Gradient Compression:Reducing the communication bandwidth for distributed training,2018, In The International Conferenceon Learning Representations
 Roberta: A robustly optimized bert pretrainingapproach,2019, ArXiv
 Sgdr: Stochastic gradient descent with warm restarts,2017, In InternationalConference on Learning Representations (ICLR) 2017 Conference Track
 Privacy androbustness in federated learning: Attacks and defenses,2020, arXiv preprint arXiv:2012
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 Byzantine-robust decentralized stochastic optimization overstatic and time-varying networks,2021, Signal Processing
 Zero: Memory optimizationstoward training trillion parameter models,2020, SC20: International Conference for High PerformanceComputing
 Detox:A redundancy-based framework for faster and more robust gradient aggregation,2019, InH
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Bygars: Byzantine sgd with arbitrary number ofattackers,2020, arXiv preprint arXiv:2006
 A method for obtaining digital signatures andpublic-key cryptosystems,1978, Communications of the ACM
 Dynamic federated learning model for identifying adver-sarial clients,2020, arXiv preprint arXiv:2007
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Megatron-lm: Training multi-billion parameter language models using gpu modelparallelism,2019, arXiv preprint arXiv:1909
 Revisiting unreasonableeffectiveness of data in deep learning era,2017, In Proceedings of the IEEE International Conference onComputer Vision (ICCV)
 Data poisoning attacks againstfederated learning systems,2020, In ESORICS
 Sybil nodes as a mitigation strategy against sybil attack,2014, ProcediaComputer Science
 Real-world sybil attacks in bittorrent mainline dht,2012, In 2012IEEE Global Communications Conference (GLOBECOM)
 Federated variance-reducedstochastic gradient descent with robustness to byzantine attacks,2020, IEEE Transactions on SignalProcessing
 Fall of empires: Breaking byzantine-tolerantsgd by inner product manipulation,2020, In Uncertainty in Artificial Intelligence
 Towards building a robust and fair federated learning system,2020, arXivpreprint arXiv:2011
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Efficient multi-party private setintersection against malicious adversaries,2019, In Proceedings of the 2019 ACM SIGSAC Conferenceon Cloud Computing Security Workshop
 Why are adaptive methods good for attention models?In H,2020, Larochelle
 Parallelized stochastic gradi-ent descent,2010, In J
 Alistarh et al,2020, (2018) propose ByzantineSGDand prove the convergence results for convex problems
 Let As,2022, 3
 Let As,2022, 3
 Let As,1440, 3
 Let As,1440, 3
