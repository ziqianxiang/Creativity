title,year,conference
 Improved language modeling by decoding the past,2019, In Proceedings of the 57thAnnualMeeting ofthe Associationfor Computational Linguistics
 Language models are few-shot learners,2020, In H
 Learning phrase representations using RNN encoder-decoderfor statistical machine translation,2014, In Alessandro Moschitti
 Incorporating nesterov momentum into adam,2016, In Proceedings of 4th InternationalConference on Learning Representations
 Frage: Frequency-agnosticword representation,2018, In S
 Long short-term memory,0899, Neural Computation
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In Proceedings of the 5th International Conference onLearning Representations
 Adam: A method for stochastic oPtimization,2015, In Yoshua Bengioand Yann LeCun (eds
 Building a large annotatedcorPus of english: The Penn treebank,1993, Comput
 Mogrifier LSTM,2020, In International Conference onLearning Representations
 Pointer sentinel mixturemodels,2017, In 5th International Conference on Learning Representations
 Regularizing and oPtimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 Recur-rent neural network based language model,2010, In Takao Kobayashi
 Separation of memory and processing in dual recurrentneural networks,2021, In Igor Farkass
 Using the output embedding to improve language models,2017, In Proceedingsofthe 15th Conference of the European Chapter of the Association for Computational Linguistics:Volume 2
 Languagemodels are unsupervised multitask learners,2019, 2019
 Improving neural language modeling via adversar-ial training,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
