title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Layer normalization,2016, arXiv preprintarXiv:1607
 Proxquant: Quantized neural networks via proximaloperators,2018, arXiv preprint arXiv:1810
 A simplified natural gradient learningalgorithm,2011, Advances in Artificial Neural Systems
 The relaxation method of finding the common point of convex sets and its applica-tion to the solution of problems in convex programming,1967, USSR computational mathematics andmathematical physics
 Convex optimization: Algorithms and complexity,2015, Foundations and TrendsÂ® inMachine Learning
 Metaquant: Learning to quantize by learning topenetrate non-differentiable quantization,2019, In Advances in Neural Information Processing Systems
 Deepshift: To-wards multiplication-less neural networks,2019, arXiv preprint arXiv:1905
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Extremely low bit neural net-work: Squeeze the last bit out with admm,2018, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Rotated binary neural network,2020, Advances in Neural Information ProcessingSystems
 Re-laxed quantization for discretized neural networks,2019, In International Conference on LearningRepresentations
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Parallel training of dnns with natural gradi-ent and parameter averaging,2014, arXiv preprint arXiv:1410
 Information and the accuracy attainable in the estimation of statistical param-eters,1992, In Breakthroughs in statistics
 Topmoumoute online natural gra-dient algorithm,2008, In Advances in neural information processing systems
 General relativity,2010, University of Chicago press
 Lq-nets: Learned quantization forhighly accurate and compact deep neural networks,2018, In Proceedings of the European conferenceon computer vision (EcCv)
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
