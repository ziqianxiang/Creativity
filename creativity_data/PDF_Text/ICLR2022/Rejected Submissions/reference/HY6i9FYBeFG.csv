title,year,conference
 Remixmatch: Semi-supervised learning with distribution alignment and augmenta-tion anchoring,2019, arXiv preprint arXiv:1911
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 Boosting co-teaching with compres-sion regularization for label noise,2021, arXiv preprint arXiv:2104
 Robust loss functions under label noise for deepneural networks,2017, In Proceedings of the AAAI Conference on Artificial Intelligence
 Training deep neural-networks using a noise adaptationlayer,2016, 2016
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, arXiv preprint arXiv:1802
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Robust inference viagenerative classifiers for handling noisy labels,2019, In International Conference on Machine Learning
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, arXiv preprint arXiv:2002
 Learning from noisy data with robust representationlearning,2020, 2020b
 Early-learningregularization prevents memorization of noisy labels,2020, arXiv preprint arXiv:2007
 Decoupling” when to update” from” how to update”,2017, arXivpreprint arXiv:1706
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Very deep convolutional netWorks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Fixmatch: Simplifying semi-supervised learningWith consistency and confidence,2020, arXiv preprint arXiv:2001
 Selfie: Refurbishing unclean samples for robustdeep learning,2019, In International Conference on Machine Learning
 Ngc: Aunified framework for learning with open-world noisy data,2021, arXiv preprint arXiv:2108
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Learning withfeature-dependent label noise: A progressive approach,2021, arXiv preprint arXiv:2103
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, arXiv preprint arXiv:1805
 Con-trast to divide: Self-supervised pre-training for learning with noisy labels,2021, arXiv preprintarXiv:2103
18 I 96,2021,45 ∣ 96
