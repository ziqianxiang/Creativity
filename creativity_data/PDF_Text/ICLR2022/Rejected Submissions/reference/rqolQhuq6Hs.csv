title,year,conference
 Metastability in reversiblediffusion processes I,2004, Sharp asymptotics for capacities and exit times
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In International Conference on Machine Learning
 The Heavy-Tail Phenomenon in SGD,2021, InInternational Conference on Machine Learning
 Discovering PhysicalConcepts with Neural Networks,2020, Physical Review Letters
 Three Factors Influencing Minima in SGD,2017, arXiv:1711
 Brownian motion in a field of force and the diffusion model of chemicalreactions,1940, Physica
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Statistical theory of the decay of metastable states,1969, Annals of Physics
 Deep learning,2015, Nature
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Noise and Fluctuation of Finite Learning RateStochastic Gradient Descent,2021, In International Conference on Machine Learning
 Bayesian model comparison and backprop nets,1992, In Advances in Neural InformationProcessing Systems
 Dynamic of Stochastic GradientDescent with State-Dependent Noise,2020, arXiv:2006
 Stochastic differential equations: an introduction with applications,1998, Springer
 The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Trainingand Sample Size,2018, arXiv:1811
 Measurements of three-level hierarchical structure in the outliers in the spectrum ofdeepnet hessians,2019, In International Conference on Machine Learning
 Implicit Bias of SGD for DiagonalLinear Networks: a Provable Benefit of Stochasticity,2021, arXiv:2106
 Empirical analysis ofthe hessian of over-parametrized neural networks,2017, arXiv:1706
 Approximation analysis of stochastic gradient langevin dynam-ics by using fokker-planck equation and ito process,2014, In International Conference on MachineLearning
 Machine learning the thermodynamicarrow of time,2021, Nature Physics
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, In International Conference on Machine Learning
 A Bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Stochastic gradient descent with noise of machine learning type,2021, Part II:Continuous time analysis
 Onthe Noisy Gradient Descent that Generalizes as SGD,2020, In International Conference on MachineLearning
 How SGD selects the global minima in over-parameterized learn-ing: A dynamical stability perspective,2018, In Advances in Neural Information Processing Systems
 A Diffusion Theory For Deep Learning Dynamics:Stochastic Gradient Descent Exponentially Favors Flat Minima,2021, In International Conference onLearning Representations
 UnderstandingDeep Learning Requires Rethinking of Generalization,2017, In International Conference on LearningRepresentations
 A hitting time analysis of stochastic gradientLangevin dynamics,2017, In Proceedings of Machine Learning Research
 The anisotropic noise in stochas-tic gradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InInternational Conference on Machine Learning
