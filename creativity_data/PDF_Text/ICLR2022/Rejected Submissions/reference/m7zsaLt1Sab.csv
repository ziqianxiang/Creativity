title,year,conference
 Language models are few-shot learners,2020, ArXiv
 Isotropy in the contextual embeddingspace: Clusters and manifolds,2021, In ICLR
 Refiningword representations by manifold learning,2019, In IJCAI
 Visualizing and measuring the geometry of bert,2019, In NeurIPS
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Investigating learning dynamics of bert fine-tuning,2020, InAACL
 Distributional structure,1954, WORD
 Word re-embedding via manifold dimensionality retention,2017, InEMNLP
 A structural probe for finding syntax in word representa-tions,2019, In NAACL
 Principal component analysis,2011, In International Encyclopedia of Statistical Science
 Universal text represen-tation from bert: An empirical study,2019, ArXiv
 Emergence of separable manifolds in deep language representations,2020, ArXiv
 Umap: Uniform manifold approximation and projection for di-mension reduction,2018, ArXiv
 Pointer sentinel mixturemodels,2017, ArXiv
 Contextual and non-contextual word embeddings: an in-depthlinguistic investigation,2020, In REPL4NLP
 Efficient estimation of wordrepresentations in vector space,2013, In ICLR
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Deep contextualized word representations,2018, In NAACL
 Recursive deep models for semantic compositionality over a sentiment tree-bank,2013, In EMNLP
 Perturbed masking: Parameter-free probing foranalyzing and interpreting bert,2020, In ACL
