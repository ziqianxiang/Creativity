title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 Stochastic gradient push fordistributed deep learning,2019, In International Conference on Machine Learning (ICML)
 signsgd withmajority vote is communication efficient and fault tolerant,2018, arXiv preprint:1810
 Diffusion adaptation strategies for distributed optimization andlearning over networks,2012, IEEE Transactions on Signal Processing
 On the convergence of decentralizedadaptive gradient methods,2021, arXiv preprint arXiv:2109
 Accelerating gossipsgd with periodic global averaging,2021, In Proceedings of the 38th International Conference onMachine Learning (ICML)
 Imagenet: A large-scalehierarchical image database,2009, In IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint:1810
 Next: In-network nonconvex optimization,2016, IEEE Transactions onSignal and Information Processing over Networks
 Dual averaging for distributed optimiza-tion: Convergence analysis and network scaling,2011, IEEE Transactions on Automatic control
 Periodic stochastic gradient descent with momentum for decen-tralized training,2020, arXiv preprint:2008
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Adam: A method for stochastic optimization,2014, arXivpreprint:1412
 Decentralized stochastic optimization andgossip algorithms with compressed communication,2019, In International Conference on MachineLearning
 Consensuscontrol for decentralized deep learning,2021, arXiv preprint:2102
 A decentralized proximal-gradient method with network independentstep-sizes and separated convergence rates,2019, IEEE Transactions on Signal Processing
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2018, In International Conference on Machine Learning
 Quasi-global momen-tum: Accelerating decentralized deep learning on heterogeneous data,2021, In Proceedings of the 38thInternational Conference on Machine Learning (ICML)
 On the variance of the adaptive learning rate and beyond,2019, In International Conference onLearning Representations
 Linear convergent decentralizedoptimization with compression,2020, arXiv preprint arXiv:2007
 Communication-censored admm fordecentralized consensus optimization,2019, IEEE Transactions on Signal Processing
 Diffusion least-mean squares over adaptive networks: Formulationand performance analysis,2008, IEEE Transactions on Signal Processing
 Gnsd: A gradient-tracking based non-convex stochastic algorithm for decentralized optimization,2019, In 2019 IEEE Data Science Workshop(DSW)
 Dadam: A consensus-baseddistributed adaptive gradient method for online optimization,2019, arXiv preprint arXiv:1901
 Distributed subgradient methods for multi-agent optimiza-tion,2009, IEEE Transactions on Automatic Control
 Harnessing smoothness to accelerate distributed optimization,2018, IEEE Transactionson Control of Network Systems
 On the convergence of adam and beyond,2019, arXivpreprint:1904
 Adaptive federated optimization,2020, In InternationalConference on Learning Representations
 Optimalalgorithms for smooth and strongly convex distributed optimization in networks,2017, In InternationalConference on Machine Learning
 On the linear convergence of the admmin decentralized consensus optimization,2014, IEEE Transactions on Signal Processing
 d2 : Decentralized training overdecentralized data,2018, In International Conference on Machine Learning
 Doublesqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In International Conferenceon Machine Learning
 Local adaalter:Communication-efficient stochastic gradient descent with adaptive learning rates,2019, arXiv preprintarXiv:1911
 An improved convergence analysis for decentralizedonline stochastic non-convex optimization,2020, arXiv preprint:2008
 Augmented distributed gradient methods for multi-agent op-timization under uncoordinated constant stepsizes,2015, In IEEE Conference on Decision and Control(CDC)
 On the linear speedup analysis of communication efficient mo-mentum sgd for distributed non-convex optimization,2019, In International Conference on MachineLearning
 Exact dffusion for distributed optimization andlearning - Part I: Algorithm development,2019, IEEE Transactions on Signal Processing
 DecentLaM: Decentralized momentum SGD for large-batch deep training,2021, arXiv preprintarXiv:2104
 Adaptivemethods for nonconvex optimization,2018, In S
 Onthe convergence of adaptive gradient methods for nonconvex optimization,2018, arXiv preprintarXiv:1808
