title,year,conference
 On the projected subgradient method fornonsmooth convex optimization in a hilbert space,1998, Mathematical Programming
 Large scale distributed deep networks,2012, InAdvances in neural information processing systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Communication trade-offs for local-sgd with largestep size,2019, Advances in Neural Information Processing Systems
 Convolutionalsequence to sequence learning,2017, In International Conference on Machine Learning
 Stochastic optimization with heavy-tailed noise via accelerated gradient clipping,2020, arXiv preprint arXiv:2005
 Lo-cal sgd with periodic averaging: Tighter analysis and adaptive synchronization,2019, In Advances inNeural Information Processing Systems
 Beyond convexity: Stochastic quasi-convexoptimization,2015, arXiv preprint arXiv:1507
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In Advances in Neural Information Processing Systems
 Ad-vances and open problems in federated learning,2019, arXiv preprint arXiv:1912
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 Tighter theory for local Sgd on identi-cal and heterogeneous data,2020, In International Conference on Artificial Intelligence and Statistics
 Decentralized stochastic optimizationand gossip algorithms with compressed communication,2019, In Proceedings of the 36th InternationalConference on Machine Learning
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Stability and convergence of stochastic gradient clipping: Be-yond lipschitz continuity and smoothness,2021, arXiv preprint arXiv:2102
 Building a large annotatedcorpus of english: The penn treebank,1993, Comput
 Communication-efficientlearning of deep networks from decentralized data,2017, AISTATS
 Regularizing and optimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Adaptive federated optimization,2021, ICLR
 Distributed stochastic optimization and learning,2014, In 2014 52ndAnnual AUerton Conference on Communication
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Local sgd converges fast and communicates little,2018, arXiv preprintarXiv:1805
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 Minibatch vs local sgd for heteroge-neous distributed learning,2020, arXiv preprint arXiv:2006
 The min-max complexityof distributed stochastic convex optimization with intermittent communication,2021, arXiv preprintarXiv:2102
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, arXiv preprint arXiv:1904
 On the linear speedup analysis of communication efficient mo-mentum SGD for distributed non-convex optimization,2019, In Proceedings of the 36th InternationalConference on Machine Learning
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Improved analysis of clipping algorithmsfor non-convex optimization,2020, arXiv preprint arXiv:2010
 Why gradient clipping acceleratestraining: A theoretical justification for adaptivity,2019, arXiv preprint arXiv:1905
 Understandingclipping for federated learning: Convergence and client-level differential privacy,2021, arXiv preprintarXiv:2106
 On the convergence properties of a k-step averaging stochastic gradi-ent descent algorithm for nonconvex optimization,2017, arXiv preprint arXiv:1708
 Parallelized stochastic gradientdescent,2010, In Advances in neural information processing Systems
