title,year,conference
 Variational networkquantization,2018, In International Conference on Learning Representations
 Control variates for stochasticgradient mcmc,2019, Statistics and Computing
 Scalable methods for 8-bit training ofneural networks,2018, arXiv preprint arXiv:1805
 Stochastic gradientmcmc with stale gradients,2016, arXiv preprint arXiv:1610
 Stochastic gradient hamiltonian monte carlo,2014, InInternational conference on machine learning
 Training binary multilayerneural networks for image classification using expectation backpropagation,2015, arXiv preprintarXiv:1503
 User-friendly guarantees for the langevin monte carlowith inaccurate gradient,2019, Stochastic Processes and their Applications
 Understanding andoptimizing asynchronous low-precision stochastic gradient descent,2017, In Proceedings of the 44thAnnual International Symposium on Computer Architecture
 Variance reduction in stochastic gradient langevin dynamics,2016, Advancesin neural information processing systems
 Learned step size quantization,2019, arXiv preprint arXiv:1902
 On the effects of quantisationon model uncertainty in bayesian neural networks,2021, arXiv preprint arXiv:2102
 Scal-able bayesian learning of recurrent neural networks for language modeling,2016, arXiv preprintarXiv:1611
 Deep learning withlimited numerical precision,1737, In International conference on machine learning
 Long short-term memory,1997, Neural computation
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In Proceedings of the IEEE conference on computervision and pattern recognition
 Quantizing deeP convolutional networks for efficient inference: AwhitePaPer,2018, arXiv preprint arXiv:1806
 Communication-efficient stochastic gradient mcmc for neural networks,2019, In Proceedings of the AAAI Conferenceon Artificial Intelligence
 Trainingquantized nets: A deePer understanding,2017, Neural Information Processing Systems
 Dimension-free bounds for low-Precision training,2019, Advancesin Neural Information Processing Systems
 Fixed Point quantization of deeP con-volutional networks,2016, In International conference on machine learning
 A comPlete reciPe for stochastic gradient mcmc,2015, arXivpreprint arXiv:1506
 SamPling can be fasterthan oPtimization,2019, Proceedings of the National Academy of Sciences
 Mixed Precisiontraining,2017, arXiv preprint arXiv:1710
 ComPutation error analysis of block floatingPoint arithmetic oriented convolution neural network accelerator design,2018, In Proceedings of theAAAI Conference on Artificial Intelligence
 SamPling-free learning of bayesian quantized neuralnetworks,2019, arXiv preprint arXiv:1912
 Ultra-low precision 4-bit training of deep neural networks,2020, Advances in Neural InformationProcessing Systems
 Bayesian bits: Unifying quantization and pruning,2020, arXiv preprintarXiv:2005
 Adversar-ial distillation of bayesian neural network posteriors,2018, In International Conference on MachineLearning
 Training and inference with integers in deepneural networks,2018, arXiv preprint arXiv:1802
 Cycli-cal stochastic gradient mcmc for bayesian deep learning,2020, International Conference on LearningRepresentations
 Qpytorch: A low-precisionarithmetic simulation framework,2019, arXiv preprint arXiv:1910
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
