title,year,conference
 Theoretical analysis of auto rate-tuning by batchnormalization,2018, arXiv preprint arXiv:1812
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Distributional and lq norm inequalities for polynomials overconvex bodies in Rn,2001, Mathematical research letters
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, In Conference on Learning Theory
 Momentum improves normalized sgd,2020, In International Conferenceon Machine Learning
 Understanding the role of momentum in non-convex optimization: Practical insightsfrom a lyapunov analysis,2020, arXiv preprint arXiv:2010
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 First-order methods of smooth convexoptimization with inexact oracle,2014, Mathematical Programming
 Shape matters: Understanding the implicitbias of the noise covariance,2020, arXiv preprint arXiv:2006
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Learning Theory
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 On the insufficiency of existingmomentum schemes for stochastic optimization,2018, In 2018 Information Theory and ApplicationsWorkshop (ITA)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep con-VolUtional neural networks,2012, Advances in neural information processing systems
 The two regimes of deep network training,2020, arXiv preprintarXiv:2002
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 An improved analysis of stochastic gradient descent withmomentum,2020, arXiv preprint arXiv:2007
 Colloquium Comput,2010, Complex
 Problem complexity and methodefficiency in optimization,1983, 1983
 Path-sgd: Path-normalized optimiza-tion in deep neural networks,2015, arXiv preprint arXiv:1506
 Gradient methods for the minimisation of functionals,1963, USSR ComputationalMathematics and Mathematical Physics
 Convergence rates of inexact proximal-gradientmethods for convex optimization,2011, arXiv preprint arXiv:1109
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 On the importance of initializationand momentum in deep learning,1139, In International conference on machine learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, arXiv preprint arXiv:1705
 Wide residual networks,2016, arXiv preprint arXiv:1605
