Under review as a conference paper at ICLR 2022
Pareto Navigation Gradient Descent: a First-
Order Algorithm for Optimization in Pareto Set
Anonymous authors
Paper under double-blind review
Ab stract
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
Many modern machine learning applications, such as multi-task learning, require finding
optimal model parameters to trade-off multiple objective functions that may conflict with
each other. The notion of the Pareto set allows us to focus on the set of (often infinite number
of) models that cannot be strictly improved. But it does not provide an actionable procedure
for picking one or a few special models to return to practical users. In this paper, we
consider optimization in Pareto set (OPT-in-Pareto), the problem of finding Pareto models
that optimize an extra reference criterion function within the Pareto set. This function can
either encode a specific preference from the users, or represent a generic diversity measure
for obtaining a set of diversified Pareto models that are representative of the whole Pareto
set. Unfortunately, despite being a highly useful framework, efficient algorithms for OPT-
in-Pareto have been largely missing, especially for large-scale, non-convex, and non-linear
objectives in deep learning. A naive approach is to apply Riemannian manifold gradient
descent on the Pareto set, which yields a high computational cost due to the need for eigen-
calculation of Hessian matrices. We propose a first-order algorithm that approximately
solves OPT-in-Pareto using only gradient information, with both high practical efficiency
and theoretically guaranteed convergence property. Empirically, we demonstrate that our
method works efficiently for a variety of challenging multi-task-related problems.
1	Introduction
Although machine learning tasks are traditionally framed as optimizing a single objective. Many modern
applications, especially in areas like multitask learning, require finding optimal model parameters to minimize
multiple objectives (or tasks) simultaneously. As the different objective functions may inevitably conflict
with each other, the notion of optimality in multi-objective optimization (MOO) needs to be characterized by
the Pareto set: the set of model parameters whose performance of all tasks cannot be jointly improved.
Focusing on the Pareto set allows us to filter out models that can be strictly improved. However, the Pareto
set typically contains an infinite number of parameters that represent different trade-offs of the objectives.
For m objectives `1, . . . , `m, the Pareto set is often an (m - 1) dimensional manifold. It is both intractable
and unnecessary to give practical users the whole exact Pareto set. A more practical demand is to find some
user-specified special parameters in the Pareto set, which can be framed into the following optimization in
Pareto set (OPT-in-Pareto) problem:
Finding one or a set of parameters inside the Pareto set of `1, . . . , `m that minimize a reference criterion F.
Here the criterion function F can be used to encode an informative user-specific preference on the objectives
`1 , . . . , `m , which allows us to provide the best models customized for different users. F can also be an
non-informative measure that encourages, for example, the diversity of a set of model parameters. In this
1
Under review as a conference paper at ICLR 2022
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
case, optimizing F in Pareto set gives a set of diversified Pareto models that are representative of the whole
Pareto set, from which different users can pick their favorite models during the testing time.
OPT-in-Pareto provides a highly generic and actionable framework for multi-objective learning and opti-
mization. However, efficient algorithms for solving OPT-in-Pareto have been largely lagging behind in deep
learning where the objective functions are non-convex and non-linear. Although has not been formally studied,
a straightforward approach is to apply manifold gradient descent on F in the Riemannian manifold formed by
the Pareto set (Hillermeier, 2001; Bonnabel, 2013). However, this casts prohibitive computational cost due
to the need for eigen-computation of Hessian matrices of {6}. In the optimization and operation research
literature, there has been a body of work on OPT-in-Pareto viewing it as a special bi-level optimization
problem (Dempe, 2018). However, these works often heavily rely on the linearity and convexity assumptions
and are not applicable to the non-linear and non-convex problems in deep learning; see for examples in Ecker
& Song (1994); Jorge (2005); Thach & Thang (2014); Liu & Ehrgott (2018); Sadeghi & Mohebi (2021) (just
to name a few). In comparison, the OPT-in-Pareto problem seems to be much less known and under-explored
in the deep learning literature.
In this work, we provide a practically efficient first-order algorithm for OPT-in-Pareto, using only gradient
information of the criterion F and objectives {'i}. Our method, named Pareto navigation gradient descent
(PNG), iteratively updates the parameters following a direction that carefully balances the descent on F and
{'i}, such that it guarantees to move towards the Pareto set of {'i} when it is far away, and optimize F in a
neighborhood of the Pareto set. Our method is simple, practically efficient and has theoretical guarantees.
In empirical studies, we demonstrate that our method works efficiently for both optimizing user-specific
criteria and diversity measures. In particular, for finding representative Pareto solutions, we propose an
energy distance criterion whose minimizers distribute uniformly on the Pareto set asymptotically (Hardin
& Saff, 2004), yielding a principled and efficient Pareto set approximation method that compares favorably
with recent works such as Lin et al. (2019); Mahapatra & Rajan (2020). We also apply PNG to improve the
performance of JiGen (Carlucci et al., 2019b), a multi-task learning approach for domain generalization, by
using the adversarial feature discrepancy as the criterion objective.
Related Work There has been a rising interest in MOO in deep learning, mostly in the context of multi-task
learning. But most existing methods can not be applied to the general OPT-in-Pareto problem. A large body
of recent works focus on improving non-convex optimization for finding some model in the Pareto set, but
cannot search for a special model satisfying a specific criterion (Chen et al., 2018; Kendall et al., 2018; Sener
& Koltun, 2018; Yu et al., 2020; Chen et al., 2020; Wu et al., 2020; Fifty et al., 2020; Javaloy & Valera, 2021).
One exception is Mahapatra & Rajan (2020); Kamani et al. (2021), which searches for models in the Pareto
set that satisfy a constraint on the ratio between the different objectives. The problem they study can be
viewed as a special instance of OPT-in-Pareto. However, their approaches are tied with special properties of
the ratio constraint and do not apply to the general OPT-in-Pareto problem.
There has also been increasing interest in finding a compact approximation of the Pareto set. Navon et al.
(2020); Lin et al. (2020) use hypernetworks to approximate the map from linear scalarization weights to
the corresponding Pareto solutions; these methods could not fully profile non-convex Pareto fronts due
to the limitation of linear scalarization (Boyd et al., 2004), and the use of hypernetwork introduces extra
optimization difficulty. Another line of works (Lin et al., 2019; Mahapatra & Rajan, 2020) approximate
the Pareto set by training models with different user preference vectors that rank the relative importance
of different tasks; these methods need a good heuristic design of preference vectors, which requires prior
knowledge of the Pareto front. Ma et al. (2020) leverages manifold gradient to conduct a local random walk
on the Pareto set but suffers from the high computational cost. Deist et al. (2021) approximates the Pareto set
by maximizing hypervolume, which requires prior knowledge for choosing a good reference vector.
Multi-task learning can also be applied to improve the learning in many other domains including domain
generalization (Dou et al., 2019; Carlucci et al., 2019a; Albuquerque et al., 2020), domain adaption (Sun
2
Under review as a conference paper at ICLR 2022
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
et al., 2019; Luo et al., 2021), model uncertainty (Hendrycks et al., 2019; Zhang et al., 2020; Xie et al., 2021),
adversarial robustness (Yang & Vondrick, 2020) and semi-supervised learning (Sohn et al., 2020). All of
those applications utilize a linear scalarization to combine the multiple objectives and it is thus interesting to
apply the proposed OPT-in-Pareto framework, which we leave for future work.
2	Background on Multi-objective Optimization
We introduce the background on multi-objective optimization (MOO) and Pareto optimality. For notation,
we denote by [m] the integer set {1, 2,  , m}, and R+ the set of non-negative real numbers. Let Cm =
{ω ∈ Rm， Pm=I ωi = 1} be the probability simplex. We denote by ∣∣∙∣∣ the Euclidean norm.
Let θ ∈ Rn be a parameter of interest (e.g., the weights in a deep neural network). Let `(θ) =
[`1 (θ), . . . , `m (θ)] be a set of objective functions that we want to minimize. For two parameters θ, θ0 ∈ Rn,
we write '(θ)占 '(θ0) if 'i(θ) ≥ '(θ0) for all i ∈ [m]; and write '(θ) > '(θ0) if '(θ)占 '(θ0) and
'(θ) = '(θ0). We say that θ is Pareto dominated (or Pareto improved) by θ0 if '(θ) * '(θ0). We say that θ is
Pareto optimal on a set Θ ⊆ Rn, denoted as θ ∈ Pareto(Θ), if there exists no θ0 ∈ Θ such that '(θ) A '(θ0).
The Pareto global optimal set P** := Pareto(Rn) is the set of points (i.e., θ) which are Pareto optimal on
the whole domain Rn. The Pareto local optimal set of ', denoted by P*, is the set of points which are Pareto
optimal on a neighborhood of itself:
P * := {θ ∈ Rn : there exists a neighborhood Nθ of θ, such that θ ∈ Pareto(Nθ)}.
The (local or global) Pareto front is the set of objective vectors achieved by the Pareto optimal points, e.g.,
the local Pareto front is F * = {'(θ) : θ ∈ P *}. Because finding global Pareto optimum is intractable for
non-convex objectives in deep learning, we focus on Pareto local optimal sets in this work; in the rest of the
paper, terms like “Pareto set” and “Pareto optimum” refer to Pareto local optimum by default.
Pareto Stationary Points Similar to the case of single-objective optimization, Pareto local optimum implies
a notion of Pareto stationarity defined as follows. Assume ` is differentiable on Rn . A point θ is called Pareto
stationary if there must exists a set of non-negative weights ω1, . . . , ωm with Pim=1 ωi = 1, such that θ is a
stationary point of the ω-weighted linear combination of the objectives: 'ω (θ) ：= Pm=I ωi'i(θ). Therefore,
the set of Pareto stationary points, denoted by P, can be characterized by
m
P := {θ ∈ Θ: g(θ)=0} ,	g(θ) := minm || X ωiV'i(θ)∣∣2,	⑴
ω∈ i=1
where g(θ) is the minimum squared gradient norm of 'ω among all ω in the probability simplex Cm on [m].
Because g(θ) can be calculated in practice, it provides an essential way to access Pareto local optimality.
Finding Pareto Optimal Points A main focus of the MOO literature is to find a (set of) Pareto optimal
points. The simplest approach is linear ScalarizatiOn, which minimizes 'ω for some weight ω (decided, e.g.,
by the users) in Cm . However, linear scalarization can only find Pareto points that lie on the convex envelop
of the Pareto front (see e.g., Boyd et al., 2004), and hence does not give a complete profiling of the Pareto
front when the objective functions (and hence their Pareto front) are non-convex.
Multiple gradient descent (MGD) (DeSid6ri, 2012) is an gradient-based algorithm that can converge to a
Pareto local optimum that lies on either the convex or non-convex parts of the Pareto front, depending on the
initialization. MGD starts from some initialization θ0 and updates θ at the t-th iteration by
Θt+1 - θt — ξvt,	Vt := arg max m min V'i(θt)>v - 1 ||v『∖ ,	(2)
v∈Rn	i∈[m]	2
3
Under review as a conference paper at ICLR 2022
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
where ξ is the step size and vt is an update direction that maximizes the worst descent rate among all
objectives, since V'i(θt)>V ≈ ('i(θt) - 'i(θt - ξv))∕ξ approximates the descent rate of objective ' when
following direction v . When using a sufficiently small step size ξ, MGD ensures to yield a Pareto improvement
(i.e, decreasing all the objectives) on θt unless θt is Pareto (local) optimal; this is because the optimization in
Equation (2) always yields mini∈[m] V'i(θt)>vt ≤ 0 (otherwise We can simply flip the sign of vt).
Using Lagrange strong duality, the solution of Equation (2) can be framed into
m
Vt = X3i,tV'i(θt),	where	{"i,t}m=I = arg min ∣∣Vθ'ω(θt)k .	(3)
ω∈Cm
i=1
It is easy to see from Equation (3) that the set of fixed points of MDG (which satisfy Vt = 0) coincides with
the Pareto stationary set P*.
A key disadvantage of MGD, however, is that the Pareto point that it converges to depends on the initialization
and other algorithm configurations in a rather implicated and complicated way. It is difficult to explicitly
control MGD to make it converge to points with specific properties.
3	Optimization In Pareto Set
The Pareto set typically contains an infinite number of points. In the optimization in Pareto set (OPT-in-
Pareto) problem, we are given an extra criterion function F(θ) in addition to the objectives `, and we want to
minimize F in the Pareto set of `, that is,
min F (θ).	(4)
θ∈P*
For example, one can find the Pareto point whose loss vector `(θ) is the closest to a given reference point
r ∈ Rm by choosing F(θ) = k'(θ) - r『.We can also design F to encourages '(θ) to be proportional to r,
i.e., '(θ) a r; a constraint variant of this problem was considered in Mahapatra & Rajan (2020).
We can further generalize OPT-in-Pareto to allow the criterion F to depend on an ensemble of Pareto points
{θ1 , ..., θN} jointly, that is,
min	F(θ1 , ..., θN).	(5)
θ1 ,...,θN ∈P*
For example, if F(θ1 , . . . , θN) measures the diversity among {θi}iN=1 , then optimizing it provides a set of
diversified points inside the Pareto set P*. An example of diversity measure is
F (θι,...,θN ) = E('(θι),..., '(Θn )),	with	E('ι,..., 'n ) = X k'i- 'j k-2,	(6)
i6=j
where E is known as an energy distance in computational geometry, whose minimizer can be shown to give
an uniform distribution asymptotically when N → ∞ (Hardin & Saff, 2004). This formulation is particularly
useful when the users’ preference is unknown during the training time, and we want to return an ensemble of
models that well cover the different areas of the Pareto set to allow the users to pick up a model that fits their
needs regardless of their preference. The problem of profiling Pareto set has attracted a line of recent works
(e.g., Lin et al., 2019; Mahapatra & Rajan, 2020; Ma et al., 2020; Deist et al., 2021), but they rely on specific
criterion or heuristics and do not address the general optimization of form Equation (5).
Manifold Gradient Descent One straightforward approach to OPT-in-Pareto is to deploy manifold gradient
descent (Hillermeier, 2001; Bonnabel, 2013), which conducts steepest descent of F(θ) in the Riemannian
manifold formed by the Pareto set P*. Initialized at θo ∈ P*, manifold gradient descent updates θt at the t-th
iteration along the direction of the projection of VF(θt) on the tangent space T(θt) at θt in P*,
θt+1 = θt - ξProjT (θt)(VF(θt)).
4
Under review as a conference paper at ICLR 2022
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
By using the stationarity characterization in Equation (1), under proper regularity conditions, one can
show that the tangent space T(θt) equals the null space of the Hessian matrix V2'ωt(θt), where ωt =
arg mi□ω∈cm ∣∣Vθ'ω (θt) k. However, the key issue of manifold gradient descent is the high cost for calculating
this null space of Hessian matrix. Although numerical techniques such as Krylov subspace iteration (Ma
et al., 2020) or conjugate gradient descent (Koh & Liang, 2017) can be applied, the high computational cost
(and the complicated implementation) still impedes its application in large scale deep learning problems. See
Section 1 for discussions on other related works.
4	Pareto Navigation Gradient Descent for OPT-in-Pareto
We now introduce our main algorithm, Pareto Navigating Gradient Descent (PNG), which provides a practical
approach to OPT-in-Pareto. For convenience, we focus on the single point problem in Equation (4) in the
presentation. The generalization to the multi-point problem in Equation (5) is straightforward. We first
introduce the main idea and then present theoretical analysis in Section 4.1.
Main Idea We consider the general incremental updating rule of form
θt+ι 一 θt 一 ξvt,
where ξ is the step size and vt is an update direction that we shall choose to achieve the following desiderata
in balancing the decent of {'} and F:
i)	When θt is far away from the Pareto set, we want to choose vt to give Pareto improvement to θt , moving it
towards the Pareto set. The amount of Pareto improvement might depend on how far θt is to the Pareto set.
ii)	If the directions that yield Pareto improvement are not unique, we want to choose the Pareto improvement
direction that decreases F(θ) most.
iii)	When θt is very close to the Pareto set, e.g., having a small g(θ), we want to fully optimize F (θ).
We achieve the desiderata above by using the vt that solves the following optimization:
Vt = argmin 11 ∣∣VF(θt) - v『s.t. Vθ'(θt)>v ≥ φt, ∀i ∈ [m]∖ ,	(7)
v∈Rn	2
where we want vt to be as close to VF(θt) as possible (hence decrease F most), conditional on that the
decreasing rate Vθ'i(θt)>vt of all losses ' are lower bounded by a control parameter φt. A positive φt
enforces that Vθt'i(θ)>vt is positive for all'%, hence ensuring a Pareto improvement when the step size is
sufficiently small. The magnitude of φt controls how much Pareto improvement we want to enforce, so we
may want to gradually decrease φt when we move closer to the Pareto set. In fact, varying φt provides an
intermediate updating direction between the vanilla gradient descent on F and MGD on {'i}:
i)	If φt = -∞, We have Vt = VF(θt) and it conducts a pure gradient descent on F without considering {'i}.
ii)	If φt → +∞, then Vt approaches to the MGD direction of {'i} in Equation (2) without considering F.
In this work, we propose to choose φt based on the minimum gradient norm g(θt) in Equation (1) as a
surrogate indication of Pareto local optimality. In particular, we consider the following simple design:
φ = -∞	if g(θt) ≤ e,	(8)
φt= αtg(θt) if g(θt) > e,	(8)
where e is a small tolerance parameter and αt is a positive hyper-parameter. When g(θt) > e, we set φt to be
proportional to g(θt), to ensure Pareto improvement based on how far θt is to Pareto set. When g(θt) ≤ e,
we set φt = -∞ which “turns off” the control and hence fully optimizes F (θ).
In practice, the optimization in Equation (7) can be solved efficiently by its dual form as follows.
5
Under review as a conference paper at ICLR 2022
Theorem 1. The solution vt of Equation (7), if it exists, has a form of
m
Vt = VF (θt) + X λi,tV'i(θt),
t=1
with {λi,t}tm=1 the solution of the following dual problem
mm
mam - 2 ||VF(Ot) + X λN `i(θ )||2+X -φt∙
λ∈R+	2	i=1	i=1
(9)
(10)
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
The optimization in Equation (10) can be solved efficiently for a small m (e..g, m ≤ 10), which is the case
for typical applications. We include the details of the practical implementation in Appendix B.
4.1 Theoretical Properties
We provide a theoretical quantification on how PNG guarantees to i) move the solution towards the Pareto
set (Theorem 2); and ii) optimize F in a neighborhood of Pareto set (Theorem 3). To simplify the result and
highlight the intuition, we focus on the continuous time limit of PNG, which yields a differentiation equation
dθt = -vtdt with vt defined in Equation (7), where t ∈ R+ is a continuous integration time.
Assumption 1. Let {θt : t ∈ R+} be a solution of dθt = -vtdt with vt in Equation (7); φk in Equation (8);
e > 0; and αt ≥ 0,∀t ∈ R+. Assume F and ` are continuously differentiable on Rn, and lower bounded
with F * := inf θ∈Rn F (θ) > -∞ and ' := inf θ∈Rn 'i(θ) > -∞. Assume supθ∈Rn ∣∣VF (θ)k ≤ c.
Technically, dθt = -vtdt is a piecewise smooth dynamical system whose solution should be taken in the
Filippov sense using the notion of differential inclusion (Bernardo et al., 2008). The solution always exists
under mild regularity conditions although it may not be unique. Our results below apply to all solutions.
Pareto Optimization on ` We now show that the algorithm converges to the vicinity of Pareto set quantified
by a notion of Pareto closure. For ≥ 0, let P be the set of Pareto -stationary points: P = {θ ∈
Rn : g(θ) ≤ e}. The Pareto closure of a set Pe, denoted by Pe is the set of points that perform no worse than
at least one point in P, that is,
Pe ：= ∪θ∈pe{θ},	{θ} = (θ0 ∈ Rn ： '(θ0) W '(θ)}∙
Therefore, Pe is better than or at least as good as Pe in terms of Pareto efficiency.
Theorem 2	(Pareto Improvement on `). Under Assumption 1, assume θ0 6∈ Pe, and te is the first time when
θte ∈ Pe, then for any time t < te ,
dt'i(θt) ≤ -αtg(θt),
min g(θs) ≤
s∈[0,t]
mini∈[m]('i(θo) — [*)
R0t αsds
Therefore, the update yields Pareto improvement on ` when θt 6∈ Pe and αtg(θt) > 0.
Further, if R0t αsds = +∞, then for any e > e, there exists a finite time te ∈ R+ on which the solution enters
Pe and stays within Pe afterwards, that is, we have θte ∈ Pe and θt ∈ Pe for any t ≥ te.
Here we guarantee that θt must enterPe for some time (in fact infinitely often), but it is not confined in Pe.
On the other hand, θt does not leave Pe after it first enters Pe thanks to the Pareto improvement property.
Optimization on F We now show that PNG finds a local optimum of F inside the Pareto closure Pe in an
approximate sense. We first show that a fixed point θ of the algorithm that is locally convex on F and ` must
be a local optimum of F in the Pareto closure of {θ}, and then quantify the convergence of the algorithm.
6
Under review as a conference paper at ICLR 2022
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
Lemma 1. Under Assumption 1, assume θt ∈ Pe is a fixed point Ofthe algorithm, that is, dθt = -Vt = 0,
and F, ' are convex in a neighborhood θt, then θt is a local minimum of F in the Pareto closure {θt}, that is,
there exists a neighborhood of θt in which there exists no point θ0 such that F(θ0) < F(θt) and '(θ0) W '(θt).
On the other hand, if θt ∈ Pe, We have Vt = VF(θt), and hence a fixed point with 釜=-Vt = 0 is an
unconstrained local minimum of F when F is locally convex on θt .
Theorem 3.	Let e > e and assume ge := supθ{g(θ): θ ∈ Pe} < +∞ and supt≥o a < ∞. Under
Assumption 1, when we initialize from θ0 ∈ P, we have
min dθS2 ≤ F^-F + 1 Z t αs (αsge + c√7)ds.
s∈[0,t] ds	t	t 0
In particular, if we have αt = α = const, then mins∈[o,t] ∣∣dθs∕dsk2 = O(1/t + α√g7).
If R∞ 0Ydt < +∞ for some Y ≥ 1, we have mins∈[o,t] kdθs∕ds∣2 = O(1∕t + √g7∕t1∕γ).
Combining the results in Theorem 2 and 3, we can see that the choice of sequence {αt : t ∈ R+} controls how
fast we want to decrease ` vs. F. Large αt yields faster descent on `, but slower descent on F. Theoretically,
using a sequence that satisfies αtdt = +∞ and αtγ dt < +∞ for some γ > 1 allows us to ensure that
both mins∈[o,t] g(θs) and mins∈[o,t] ∣∣dθ∕ds∣2 converge to zero. If we use a constant sequence at = α, it
introduces an O(α√g7) term that does not vanish as t → +∞. However, we can expect that ge is small when
e is small for well-behaved functions. In practice, we find that constant αt works sufficiently well.
5	Empirical Results
We introduce three applications of OPT-in-Pareto with PNG: Singleton Preference, Pareto approximation and
improving multi-task based domain generalization method. We also conduct additional study on how the
learning dynamics of PNG changes with different initialization and hyper-parameters (αt and e), which are
included in Appendix C.3. Other additional results that are related to the experiments in Section 5.1 and 5.2
and are included in the Appendix will be introduced later in their corresponding sections.
5.1	Finding Preferred Pareto Models
We consider the synthetic example used in Lin et al. (2019); Mahapatra & Rajan (2020), which consists of
two losses: 'ι(θ) = 1 - exp(- ∣θ - η∣2) and '2(θ) = 1 - exp(- ∣θ + η∣2), where η = n-1/2 and n = 10
is dimension of the parameter θ.
Ratio-based Criterion We first show that PNG can solve the search problem under the ratio constraint of
objectives in Mahapatra & Rajan (2020), i.e., finding a point θ ∈ P * ∩ Ω with Ω = {θ : r1'1(θ) = r2'2(θ)=
...=rm'm(θ)}, given some preference vector r = [ri,…,rm]. We apply PNG with the non-uniformity
score defined in Mahapatra & Rajan (2020) as the criterion, and compare with their algorithm called exact
Pareto optimization (EPO). We show in Figure 1(a)-(b) the trajectory of PNG and EPO for searching models
with different preference vector r, starting from the same randomly initialized point. Both PNG and EPO
converge to the correct solutions but with different trajectories. This suggests that PNG is able to achieve
the same functionality of finding ratio-constraint Pareto models as Mahapatra & Rajan (2020); Kamani et al.
(2021) do but being versatile to handle general criteria. We refer readers to Appendix C.1.1 for more results
with different choices of hyper-parameters and the experiment details.
Other Criteria We demonstrate that PNG is able to find solutions for general choices of F . We consider
the following designs of F: 1) weighted `2 distance w.r.t. a reference vector r ∈ R+m, that is, Fwd(θ) =
7
Under review as a conference paper at ICLR 2022
(b)
(C)	(d)
Figure 1: (a)-(b): the trajectory of finding Pareto models that satisfy different ratio constraints (shown in
different colors) on the two objectives `1 , `2 using EPO and PNG; we can see that PNG can achieve the
same goal as EPO (with different trajectories) while being a more general approach. (c)-(d): the trajectory of
finding Pareto models that minimize the weighted distance and complex cosine criterion using PNG. The
green dots indicate the converged models. We can see that PNG can successfully locate the correct Pareto
models that minimize different criteria.
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
Pm=ι('i(θ) - ri)2∕ri ； and 2) complex cosine: in which F is a complicated function related to the cosine
of task objectives, i.e., FCS = - cos(∏('ι(θ) - rι)∕2) + (cos(∏('(θ2) - r2)) + 1)2. Here the weighted '2
distance can be viewed as finding a Pareto model that has the losses close to some target value r, which can be
viewed as an alternative approach to partition the Pareto set. The design of complex cosine aims to test whether
PNG is able to handle a very non-linear criterion function. In both cases, we take r1 = [0.2, 0.4, 0.6, 0.8] and
r2 = 1 - r1 . We show in Fig 1(c)-(d) the trajectory of PNG. As we can see, PNG is able to correctly find the
optimal solutions of OPT-in-Pareto. We also test PNG on a more challenging ZDT2-variant used in Ma et al.
(2020) and a larger scale MTL problem (Liu et al., 2019). We refer readers to Appendix C.1.2 and C.1.3 for
the setting and results.
5.2	Finding Diverse Pareto Models
Setup We consider the problem of finding diversified points from the Pareto set by minimizing the energy
distance criterion in Equation (6). We use the same setting as Lin et al. (2019); Mahapatra & Rajan (2020).
We consider three benchmark datasets: (1) MultiMNIST, (2) MultiFashion, and (3) MultiFashion+MNIST.
For each dataset, there are two tasks (classifying the top-left and bottom-right images). We consider LeNet
with multihead and train N = 5 models to approximate the Pareto set. For baselines, we compare with linear
scalarization, MGD (Sener & Koltun, 2018), and EPO (Mahapatra & Rajan, 2020). For the MGD baseline,
we find that naively running it leads to poor performance as the learned models are not diversified and thus we
initialize the MGD with 60-epoch runs of linear scalarization with equally distributed preference weights and
runs MGD for the later 40 epoch. We refer the reader to Appendix C.2.1 for more details of the experiments.
Metric and Result We measure the quality of how well the found models {θ1, . . . , θN} approximate the
Pareto set using two standard metrics: Inverted Generational Distance Plus (IGD+) (Ishibuchi et al., 2015)
and hypervolume (HV) (Zitzler & Thiele, 1999); see Appendix C.2.2 for their definitions. We run all the
methods with 5 independent trials and report the averaged value and its standard deviation in Table 1. We
report the scores calculated based on loss (cross-entropy) and accuracy on the test set. The bolded values
indicate the best result with p-value less than 0.05 (using matched pair t-test). In most cases, PNG improves
the baselines by a large margin. We include ablation studies in Appendix C.2.3 and additional comparisons
with the second-order approach proposed by Ma et al. (2020) in Appendix C.2.4.
5.3	Application to Multi-task based Domain Generalization Algorithm
JiGen (Carlucci et al., 2019b) learns a domain generalizable model by learning two tasks based on linear
scalarization, which essentially searches for a model in the Pareto set and requires choosing the weight of
8
Under review as a conference paper at ICLR 2022
Data	Method	Loss		Acc	
		HV↑ (10-2)	IGD+J (10-2)	HV↑ (10-2)	IGD+J (10-2)
	Linear	7.48 ± 0.11	0.14 ± 0.034	9.27 ± 0.024	0.036 ± 0.0084
Multi-MNIST	MGD	7.69 ± 0.10	0.051 ± 0.011	9.27 ± 0.023	0.0078 ± 0.0010
	EPO	7.87±0.16	0.069 ± 0.028	9.17 ± 0.032	0.065 ± 0.018
	PNG	7.86±0.11	0.042±0.012	9.39±0.036	0.0056±0.0022
	Linear	0.38 ± 0.059	0.13 ± 0.013	4.76 ± 0.019	0.064 ± 0.012
Multi-Fashion	MGD	0.42 ± 0.064	0.046 ± 0.016	4.77 ± 0.019	0.023±0.0030
	EPO	0.36 ± 0.058	0.31 ± 0.11	4.78 ± 0.030	0.21 ± 0.020
	PNG	0.47±0.066	0.016±0.0022	4.81±0.021	0.023±0.0031
	Linear	5.01 ± 0.057	0.167 ± 0.054	8.46 ± 0.046	0.110 ± 0.035
Fashion-MNIST	MGD	5.09 ± 0.069	0.060 ± 0.029	8.40 ± 0.045	0.049±0.011
	EPO	4.60 ± 0.166	0.233 ± 0.054	8.12 ± 0.041	0.385 ± 0.077
	PNG	5.27±0.054	0.048±0.027	8.53±0.047	0.046±0.022
Table 1: Results of approximating the Pareto set by different methods on three MNIST benchmark datasets.
The numbers in the table are the averaged value and the standard deviation. Bolded values indicate the
statistically significant best result with p-value less than 0.5 based on matched pair t-test.
PACS	art paint	cartoon	sketches	photo	Avg
D-SAM	0.7733	0.7243	0.7783	0.9530	0.8072
DeepAll	0.7785	0.7486	0.6774	0.9573	0.7905
JiGen	0.8009 ± 0.004	0.7363 ± 0.007	0.7046 ± 0.013	0.9629±0.002	0.8012 ± 0.002
JiGen+adv	0.7923 ± 0.006	0.7402 ± 0.004	0.7188 ± 0.005	0.9617 ± 0.001	0.8033 ± 0.001
JiGen+PNG	0.8014±0.005	0.7538±0.001	0.7222±0.006	0.9627±0.002	0.8100±0.005
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
Table 2: Comparing different methods for domain generalization on PACS using ResNet-18. The values in
table are the testing accuracy with its standard deviation. The bolded values are the best models with p-value
less than 0.1 based on match-pair t-test.
linear scalarization carefully. It is thus natural to study whether there is a better mechanism that dynamically
adjusts the weights of the two losses so that we eventually learn a better model. Motivated by the adversarial
feature learning (Ganin et al., 2016), we propose to improve JiGen such that the latent feature representations
of the two tasks are well aligned. This can be framed into an OPT-in-Pareto problem where the criterion is
the discrepancy of the latent representations (implemented using an adversarial discrepancy module in the
network) of the two tasks. PNG is applied to solve the optimization. We evaluate the methods on PACS (Li
et al., 2017), which covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon, and Sketches).
The model is trained on three domains and tested on the rest of them. Our approach is denoted as JiGen+PNG
and we also include JiGen + adv, which simply adds the adversarial loss as regularization and two other
baseline methods (D-SAM (D’Innocente & Caputo, 2018) and DeepAll (Carlucci et al., 2019b)). For the three
JiGen based approaches, we run 3 independent trials and for the other two baselines, we report the results in
their original papers. Table 2 shows the result using ResNet-18, which demonstrates the improvement by the
application of the OPT-in-Pareto framework. We also include the results using AlexNet in the Appendix. We
refer readers to Appendix C.4 for the additional results and more experiment details.
6	Conclusion
This paper studies the OPT-in-Pareto, a problem that has been studied in operation research with restrictive
linear or convexity assumption but largely under-explored in deep learning literature, in which the objectives
are non-linear and non-convex. Applying algorithms such as manifold gradient descent requires eigen-
computation of the Hessian matrix at each iteration and thus can be expensive. We propose a first-order
approximation algorithm called Pareto Navigation Gradient Descent (PNG) with theoretically guaranteed
descent and convergence property to solve OPT-in-Pareto.
9
Under review as a conference paper at ICLR 2022
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
References
Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, and Richard Socher. Improving out-of-
distribution generalization via multi-task self-supervised pretraining. arXiv preprint arXiv:2003.13525,
2020.
Mario Bernardo, Chris Budd, Alan Richard Champneys, and Piotr Kowalczyk. Piecewise-smooth dynamical
systems: theory and applications, volume 163. Springer Science & Business Media, 2008.
Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic
Control, 58(9):2217-2229, 2013.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press,
2004.
Fabio M. Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain
generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019a.
Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain
generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 2229-2238, 2019b.
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization
for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning,
pp. 794-803. PMLR, 2018.
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir
Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 2039-2050. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/16002f7a455a94aa4e91cc34ebdb9f2d-Paper.pdf.
Timo M Deist, Monika Grewal, Frank JWM Dankers, Tanja Alderliesten, and Peter AN Bosman.
Multi-objective learning to predict pareto fronts using hypervolume maximization. arXiv preprint
arXiv:2102.04523, 2021.
Stephan Dempe. Bilevel optimization: theory, algorithms and applications. TU Bergakademie Freiberg,
Fakultat fur Mathematik Und Informatik, 2018.
Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes
Rendus Mathematique, 350(5-6):313-318, 2012.
Qi Dou, Daniel C Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-
agnostic learning of semantic features. arXiv preprint arXiv:1910.13580, 2019.
Antonio D’Innocente and Barbara Caputo. Domain generalization with domain-specific aggregation modules.
In German Conference on Pattern Recognition, pp. 187-198. Springer, 2018.
Joseph G Ecker and Jung Hwan Song. Optimizing a linear function over an efficient set. Journal of
Optimization Theory and Applications, 83(3):541-563, 1994.
Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Measuring and
harnessing transference in multi-task learning. arXiv preprint arXiv:2010.15413, 2020.
10
Under review as a conference paper at ICLR 2022
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International
conference on machine learning, pp.1180-1189. PMLR, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois Laviolette,
Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine
Learning Research, 17(59):1-35, 2016. URL http://jmlr.org/papers/v17/15- 239.html.
DP Hardin and EB Saff. Discretizing manifolds via minimum energy points. Notices of the AMS, 51(10):
1186-1194, 2004.
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can
improve model robustness and uncertainty. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
a2b15837edac15df90721968986f7f8e-Paper.pdf.
Claus Hillermeier. Generalized homotopy approach to multiobjective optimization. Journal of Optimization
Theory and Applications, 110(3):557-583, 2001.
Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance calculation in
generational distance and inverted generational distance. In International conference on evolutionary
multi-criterion optimization, pp. 110-125. Springer, 2015.
Adrian Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. arXiv
preprint arXiv:2103.02631, 2021.
Jesus M Jorge. A bilinear algorithm for optimizing a linear function over the efficient set of a multiple
objective linear programming problem. Journal of Global Optimization, 31(1):1-16, 2005.
Mohammad Mahdi Kamani, Rana Forsati, James Z Wang, and Mehrdad Mahdavi. Pareto efficient fairness in
supervised learning: From extraction to tracing. arXiv preprint arXiv:2104.01634, 2021.
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for
scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 7482-7491, 2018.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International
Conference on Machine Learning, pp. 1885-1894. PMLR, 2017.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct
2017.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for
domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,
2018a.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep
domain generalization via conditional invariant adversarial networks. In Proceedings of the European
Conference on Computer Vision (ECCV), pp. 624-639, 2018b.
Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and Sam Kwong. Pareto multi-task learning. arXiv
preprint arXiv:1912.12854, 2019.
11
Under review as a conference paper at ICLR 2022
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
Xi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong. Controllable pareto multi-task learning. arXiv
preprint arXiv:2010.06313, 2020.
Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In
Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition ,pp.1871-1880,
2019.
Zhengliang Liu and Matthias Ehrgott. Primal and dual algorithms for optimization over the efficient set.
Optimization, 67(10):1661-1686, 2018.
Xiaoyuan Luo, Shaolei Liu, Kexue Fu, Manning Wang, and Zhijian Song. A learnable self-supervised task
for unsupervised domain adaptation on point clouds. arXiv preprint arXiv:2104.05164, 2021.
Pingchuan Ma, Tao Du, and Wojciech Matusik. Efficient continuous pareto exploration in multi-task learning.
In International Conference on Machine Learning, pp. 6522-6531. PMLR, 2020.
Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with
controlled ascent in pareto optimization. In International Conference on Machine Learning, pp. 6597-6607.
PMLR, 2020.
Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya. Learning the pareto front with hypernetworks.
arXiv preprint arXiv:2010.04104, 2020.
Javad Sadeghi and Hossein Mohebi. Solving optimization problems over the weakly efficient set. Numerical
Functional Analysis and Optimization, pp. 1-33, 2021.
Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Pro-
cessing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.
cc/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support
inference from rgbd images. In European conference on computer vision, pp. 746-760. Springer, 2012.
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-
gus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596-608. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf.
Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self-
supervision. arXiv preprint arXiv:1909.11825, 2019.
Phan Thien Thach and TV Thang. Problems with resource allocation constraints and optimization over the
efficient set. Journal of Global Optimization, 58(3):481-495, 2014.
Sen Wu, Hongyang R. Zhang, and Christopher R6. Understanding and improving information transfer
in multi-task learning. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=SylzhkBtDB.
Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-
training and self-training using auxiliary information for out-of-distribution robustness. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
jznizqvr15J.
12
Under review as a conference paper at ICLR 2022
396
397
398
399
400
401
402
403
404
405
406
Junfeng Yang and Carl Vondrick. Multitask learning strengthens adversarial robustness. 2020.
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gra-
dient surgery for multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5824-5836. CUr-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
3fe78a8acf5fda99de95303940a2420c-Paper.pdf.
Linfeng Zhang, Muzhou Yu, Tong Chen, Zuoqiang Shi, Chenglong Bao, and Kaisheng Ma. Auxiliary training:
Towards accurate and robust models. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 372-381, 2020.
Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study and the
strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257-271, 1999.
13
Under review as a conference paper at ICLR 2022
407
408
409
410
411
412
413
414
415
416
417
418
A Theoretical Analysis
Theorem 1 [Dual of Equation (7)] The solution vt of Equation (7), if it exists, has a form of
m
Vt = VF (θt) + X λi,tV'i(θt),
i=1
with {λi,t}im=1 the solution of the following dual problem
1
max——
λ∈R+m 2
m
VF (θt) + X λtV'i(θt)
i=1
2m
+	λiφt,
i=1
where R+m is the set of nonnegative m-dimensional vectors, that is,	R+m	=	{λ	∈	Rm :	λi	≥ 0,	∀i	∈	[m]}.
Proof. By introducing Lagrange multipliers, the optimization in Equation (7) is equivalent to the following
minimax problem:
m
min max	∣∣VF (θt) — v∣∣2 + X λ% (φt - V'i(θt)>v).
v∈Rn λ∈Rm 2
+	i=1
With strong duality of convex quadratic programming (assuming the primal problem is feasible), we can
exchange the order of min and max, yielding
)
max
λ∈R+m
Φ(λ) :
V∈Rιn 1kVF R)-
m
v『+ X λi (φt-V'i(θt )>v)
i=1
It is easy to see that the minimization w.r.t. V is achieved when V = VF(θt) + Pm=I λiV'i(θt). CorresPond-
ingly, the Φ(λ) has the following dual form:
1
max——
λ∈R+m 2
m
VF (θt) + X %V'i(θt)
i=1
2
m
+	λiφt.
i=1
This concludes the Proof.
Theorem 2 [Pareto Improvement on `]
when θte ∈ Pe, then for any time t < te ,
Under Assumption 1, assume θ0 6∈ Pe, and te is the first time
□
dt'i(θt) ≤ -αtg(θt),
min g(θt) ≤
s∈[0,t]
mini∈[m]('i(θo)-峭
R0t αsds
Therefore, the update yields Pareto improvement on ` when θt 6∈ Pe and αtg(θt) > 0.
Further, if R0t αsds = +∞, then for any > e, there exists a finite time t ∈ R+ on which the solution enters
Pe and stays within Pe afterwards, that is, we have θte ∈ Pe and θt ∈ Pe for any t ≥ te.
Proof. i) When t < te, we have g(θt) > e and hence
dt'i(θt) = -V'i(θt)>Vt ≤ -φt = -αtg(θt),	(11)
where we used the constraint of V'i(θt)>vt ≥ φt in Equation (7). Therefore, we yield strict decent on all the
losses {'i} when ag(θ>) > 0.
14
Under review as a conference paper at ICLR 2022
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
ii)	Integrating both sides of Equation (11):
.,aas-	θ αs"θs)ds /	'i(θo)	-	'i(θt)	/	'i(θ0)	- '*
min g(θs) ≤ 0	≤	≤	.
s∈[0,t]	J。asds	R0 asds	八 asds
This yields the result since it holds for every i ∈ [m].
If 0∞tdt = +∞, then we have mins∈[0,t] g(θs ) → 0 when t → +∞. Assume there exists an > e,
such that θt never enters P at finite t. Then we have g(θt ) ≥ for t ∈ R+, which contradicts with
min∈[0,t] g (θs ) → 0.
iii)	Assume there exists a finite time t0 ∈ (te, +∞) such that θto ∈ Pe. Because e > e and g is continuous, Pe
is in the interior of Pe ⊆ Pe. Therefore, the trajectory leading to θtj ∈ Pe must pass through Pe \ Pe at some
point, that is, there exists a point t00 ∈ [t, t0), such that {θt : t ∈ [t00, t0]} 6∈ Pe. But because the algorithm can
not increase any objective ' outside of Pe, We must have '(θto) W '(θt00), yielding that θto ∈ {θt00} ⊆ Pe,
where {θt00} is the Pareto closure of {θt00}; this contradicts with the assumption.	口
Lemma 1 Under Assumption 1, assume θt ∈ Pe is a fixed point Ofthe algorithm, that is, dθtt = -Vt = 0,
and F, ' are convex in a neighborhood θt, then θt is a local minimum of F in the Pareto closure {θt},
that is, there exists a neighborhood of θt in which there exists no point θ0 such that F (θ0) < F(θt) and
'(θ0) W '(θt).
Proof. Note that minimizing F in {θt} can be framed into a constrained optimization problem:
min F(θ) s.t. `i (θ) ≤ `i (θt), ∀i ∈ [m].
θ
In addition, by assumption, θ = θt satisfies Vt = VF(θt) + Pm=I λi,tV'i(θt) = 0, which is the KKT
stationarity condition of the constrained optimization. It is also obvious to check that θ = θt satisfies the
feasibility and slack condition trivially. Combining this with the local convexity assumption yields the
result.	口
Theorem 3 [Optimization of F] Let e > e and assume ge := sup0{g(θ): θ ∈ Pe} < +∞ and
supt≥0 αt < ∞. Under Assumption 1, when we initialize from θ0 ∈ Pe, we have
min	学	Z F(^-F +	1 Z t αs	(…+	V)ds.
s∈[0,t]	ds	t	t 0
In particular, if we have at = α = const, then mins∈[0,t] ∣∣dθs∕dsk2 = O(1/t + α√g7).
If R0∞ αγdt < +∞ for some Y ≥ 1, we have mins∈[0,t] ∣∣dθs∕dsk2 = O(1∕t + √gi∕t1∕γ).
Proof. i) The slack condition of the constrained optimization in Equation (7) says that
λi,t (V'i(θt)>vt - φt) =0, ∀i ∈ [m].
This gives that
∣vtk2 = (VF (θt) + XX %,tV'i (θt)) vt
m
= VF(θt)>Vt + X	λi,tφt	//plugging Equation (12).
i=1
(12)
(13)
15
Under review as a conference paper at ICLR 2022
If θt 6∈ Pe, we have φt = atg(θt) and this gives
m
dd-F (θt) = -VF (θt)> Vt = -∣vtk2 + X λi,tφt
i=1
dθt
dt
2m
+	λi,tαtg(θt)
i=1
—
If θt is in the interior of Pe, then we run typical gradient descent of F and hence has
d2
dtF (θt) = -kvtk2
dθt
dt
—
2
If θt is on the boundary of Pe, then by the definition of differential inclusion, dθ/dt belongs to the convex
hull of the velocities that it receives from either side of the boundary, yielding that
ddtF (θt)
dθt
dt
i=1
where β ∈ [0, 1]. Combining all the cases gives
dtF(θt) ≤ -
2m
+ β X λi,tαtg(θt) ≤ -
dθt
dt
dθt
dt
2m
+	λi,tαtg(θt),
i=1
2m
+	λi,tαtg(θt).
i=1
Integrating this yields
sm[0nt] IA ≤ - /
dθ
ds
2
ds≤
F(θ0) - F*
t
F(θo) - F*
t
1 tm
+ - J ɪ2 λi,sasg(θs)ds
+ 1 / as (asge + C√g7)ds,
-0
438
439
440
where the last step used Lemma 2 with φt = αtg(θt):
m
fλi,tαt,g(θt) ≤ α2g(θt) + Cat VZgW) ≤ α2ge + cat√gl,
i=1
and here We used g(θt) ≤ ge because the trajectory is contained in Pe following Theorem 2.
The remaining results follow Lemma 4.
A.0. 1 Technical Lemmas
Lemma 2. Assume Assumption 1 holds. Define g(θ) = minω∈Cm ∣∣pm=1 ωiV'i(θ)k2, where Cm is the
probability simplex on [m]. Then for the vt and λi,t defined in Equation (7) and Equation (10), we have
m
X λi,tg(θt) ≤ max (φt + cpg(θt), 0)∙
i=1
441
Proof. The slack condition of the constrained optimization in Equation (7) says that
λi,t (V'i(θ)>vt- φt) =0, ∀i ∈ [m].
Sum the equation over i ∈ [m] and note that Vt = VF(θt) + Pm=I λi,tV'i(θt). We get
m
X %,tV'i(θt)
i=1
2m	>	m
+	X λi,tV'i(θt)	VF(θ) - X λi,tφt = 0.
i=1	i=1
(14)
—
≤
□
16
Under review as a conference paper at ICLR 2022
Define
m
Xt = X λi,tV'i(θt)
i=1
m
λt = X λi,t,
i=1
gt = g(θt) = ωm∈Cinm
Eω”'i(θt)
i=1
m
2
λi,tV'i(θ)!	VF (θt)
Then it is easy to see that Xt ≥ λ2gt. Using CaUchy-SchWarz inequality,
m
≤ kVF(θt)k X%,tV'i(θ)
i=1
≤ c√Xt,
Where We used kVF(θt)k ≤ c by Assumption 1. Combining this With Equation (14), We have
IXt - λtΦt∣ ≤ c√χt.
442 Applying Lemma 3 yields the result.
□
Lemma 3. Assume φ ∈ R, and X, λ, c, g ∈ R+ are non-negative real numbers and they satisfy
|x — λφ∣ ≤ c√x,	X ≥ λ2g.
443 Then we have λg ≤ max(0, φ + c√g).
Proof. Square the first equation, We get
f(X) := (X - λφ)2 - c2X ≤ 0,
Where f is a quadratic function. To ensure that f(X) ≤ 0 has a solution that satisfies X ≥ λ2g, We need to
have f(λ2g) ≤ 0, that is,
f(λ2g) = (λ2g - λφ)2 -c2λ2g≤0.
444 This can hold under tWo cases:
445 Case 1: λ = 0;
446 Case 2: ∣λg 一 φ∣ ≤ c√g, and hence φ 一 c√g ≤ λg ≤ φ + c√g.
Under both case, We have
λg ≤ max(0, φ + c√g).
447	口
Lemma 4. Let {αt: t ∈ R+} ⊆ R+ be a non-negative sequence with A := (R∞ αγdt)"γ < ∞, where
γ ≥ 1, and B = supt αt < ∞. Then we have
Z Z (α2 + a§) ds ≤ (B + 1)At-1/7.
t0
Proof. Let η = γ-ɪ, so that 1/n + 1∕γ = 1. We have by Holder,s inequality,
「t	/-t	ɪ/ 1/Y / -t	ɪ/n
0 αs ds ≤	0 αsγds	0 1ηds
≤ At1/n = At1-1/Y.
448
and hence
+ αs ds ≤
B+1 /1 asds ≤ (B + 1)At-1∕γ.
1Z
□
17
Under review as a conference paper at ICLR 2022
Algorithm 1 Pareto Navigating Gradient Descent
1:	Initialize θ0; decide the step size ξ, and the control function φ in Equation (8) (including the threshold
e > 0 and the descending rate {αt}).
2:	for iteration t do
m
Θt+1 一 θt - ξvt,	Vt = VF(θt) + X λi,tV'i(θt),	(15)
i=1
where λi,t = 0, ∀i ∈ [m] if g(θt) ≤ e, and {λi,t}tm=1 is the solution of Equation (10) with φ(θt) =
αtg(θt) wheng(θt) > e.
3:	end for
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
B Practical Implementation
Hyper-parameters Our algorithm introduces two hyperparameters {αt} and e over vanilla gradient descent.
We use constant sequence αt = α and we take α = 0.5 unless otherwise specified. We choose e by
e = γeo, where e° is an exponentially discounted average of ^^ Pm=IllV6@)『over the trajectory so that
it automatically scales with the magnitude of the gradients of the problem at hand. In the experiments of this
paper, we simply fix γ = 0.1 unless specified.
Solving the Dual Problem Our method requires to calculate {λi,t }tm=1 with the dual optimization problem
in Equation (10), which can be solved with any off-the-shelf convex quadratic programming tool. In this
work, we use a very simple projected gradient descent to approximately solve Equation (10). We initialize
{λi,t}tm=1 with a zero vector and terminate when the difference between the last two iterations is smaller than
a threshold or the algorithm reaches the maximum number of iterations (we use 100 in all experiments).
The whole algorithm procedure is summarized in Algorithm 1.
C Experiments
C.1 Finding Preferred Pareto Models
C.1.1 Ratio-based Criterion
The non-uniformity score from (Mahapatra & Rajan, 2020) that we use in Figure 1 is defined as
FNU(O) = X 工⑻IOg (≡) , A四=P∈rm(S'
(16)
We fix the other experiment settings the same as Mahapatra & Rajan (2020) and use γ = 0.01 and α = 0.25
for this experiment reported in the main text. We defer the ablation studies on the hyper-parameter α and γ to
Section C.3.
C.1.2 ZDT2-Variant
We consider the ZDT2-Variant example used in Ma et al. (2020) with the same experiment setting, in
which the Pareto set is a cylindrical surface, making the problem more challenging. We consider the
same criteria, e.g. weighted distance and complex cosine used in the main context with different choices
ofr1 = [0.2, 0.4, 0.6, 0.8]. We use the default hyper-parameter set up, choosing α = 0.5 and r = 0.1.
18
Under review as a conference paper at ICLR 2022
Figure 2: Trajectories of solving OPT-in-Pareto with weighted distance and complex cosine as criterion using
PNG. The green dots are the final converged models. PNG is able to successfully locate the correct models in
the Pareto set.
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
For complex cosine, we use MGD updating for the first 150 iterations. Figure 2 shows the trajectories,
demonstrating that PNG works pretty well for the more challenging ZDT2-Variant tasks.
C.1.3 General Criteria: Three-task learning on the NYUv2 Dataset
We show that PNG is able to handle large-scale multitask learning problems by deploying it on a three-
task learning problem (segmentation, depth estimation, and surface normal prediction) on NYUv2 dataset
(Silberman et al., 2012). The main goal of this experiment is to show that: 1. PNG is able to handle
OPT-in-Pareto in a large-scale neural network; 2. With a proper design of criteria, PNG enables to do
targeted fine-tuning that pushes the model to move towards a certain direction. We consider the same
training protocol as Liu et al. (2019) and use the MTAN network architecture. Start with a model trained
with equally weighted linear scalarization and our goal is to further improve the model’s performance
on segmentation and surface normal estimation while allowing some sacrifice on depth estimation. This
can be achieved by many different choices of criterion and in this experiment, we consider the following
design： F(θ) = ('seg(θ) × 'surface(θ))∕(0.001 + 'depth(θ)). Here %, 'surface and 'depth are the loss functions
for segmentation, surface normal prediction and depth estimation, respectively. The constant 0.001 in the
denominator is for numeric stability. We point out that our design of criterion is a simple heuristic and might
not be an optimal choice and the key question we study here is to verify the functionality of the proposed
PNG. As suggested by the open-source repository of Liu et al. (2019), we reproduce the result based on the
provided configuration. To show that PNG is able to move the model along the Pareto front, we show the
evolution of the criterion function and the norm of the MGD gradient during the training in Figure 3. As we
can see, PNG effectively decreases the value of criterion function while the norm of MGD gradient remains
the same. This demonstrates that PNG is able to minimize the criterion by searching the model in the Pareto
set. Table 3 compares the performances on the three tasks using standard training and PNG, showing that
PNG is able to improve the model’s performance on segmentation and surface normal prediction tasks while
satisfying a bit of the performance in depth estimation based on the criterion.
C.2 Finding Diverse Pareto Models
C.2. 1 Experiment Details
19
Under review as a conference paper at ICLR 2022
	Segmentation	Depth	Surface Normal			
Algorithm	(Higher Better) mIoU Pix Acc	(Lower Better) Abs Err Rel Err	Angle Distance (Lower Better) Mean Median	11.25	Within t。 22.5	30
Standard PNG	"^7.09	56.36 28.23	56.66	0.6143^^0.2618 0.6161	0.2632	31.46	27.37 31.06	26.50	19.51 21.06	41.71 43.41	54.61 55.93
Table 3: Comparing the multitask performance of standard training using linear scalarization with equally
weighted losses and the targeted fine-tuning based on PNG.
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
We train the model for 100 epochs using Adam op-
timizer with batch size 256 and 0.001 learning rate.
To encourage diversity of the models, following the
setting in Mahapatra & Rajan (2020), we use equally
distributed preference vectors for linear scalarization
and EPO. Note that the stochasticity of using mini-
batches is able to improve the performance of Pareto
approximation for free by also using the intermedi-
ate checkpoints to approximate P . To fully exploit
this advantage, for all the methods, we collect check-
points every epoch to approximate P , starting from
epoch 60.
C.2.2 Evaluation Metric Details
We introduce the definition of the used metric for
evaluation. Given a set P = {Θi,...,Θn } that we
use to approximate P , its IGD+ score is defined as:
Figure 3: The evolution of Criterion F and the norm
of MGD gradient when trained using PNG on NYUv2
dataset with MTAN network. PNG effectively de-
creases the criterion while ensuring the model is within
the Pareto set, since the norm of MGD gradient remains
unchanged.
一二、 f -、，八
IGD+(P)=/ q(θ, P)dμ(θ),
P*
，-a、
q(θ, P) = mm
θ∈P
- `(θ)
where μ is some base measure that measures the importance of θ ∈ P and (t)+ := max(t, 0), applied on
each element of a vector. Intuitively, for each θ, we
find a nearest θ ∈ P that approximates θ best. Here
,ι / ∖	i ∙	1	i	,ι , i ,ι , A ∙	,ι n ɪ	, ∙	ι	t`
the (•)+ is applied as we only care the tasks that θ is worse than θ. In practice, a common choice of μ can
be a uniform counting measure with uniformly sampled (or selected) models from P. In our experiments,
since we can not sample models from P, we approximate P by combining P from all the methods, i.e.,
P ≈ ∪m∈{Linear,MGD,EPO,PNG}Pm, where Pm is the approximation set produced by algorithm m.
This approximation might not be accurate but is sufficient to compare the different methods,
The Hypervolume score of P, w.r.t. a reference point 'r ∈ R', is defined as
HV(P) = μ ({' = ['ι,…,'m] ∈ Rm | ∃θ ∈P, s.t. 't(θ) ≤'t ≤ `r Vt ∈ [m]}),
where μ is again some measure. We use 'r = [0.6,0.6] for calculating the Hypervolume based on loss and
set μ to be the common Lebesgue measure. Here we choose 0.6 as we observe that the losses of the two tasks
are higher than 0.6 and 0.6 is roughly the worst case. When calculating Hypervolume based on accuracy, we
simply flip the sign.
20
Under review as a conference paper at ICLR 2022
				Loss		Acc	
				Hv↑ (10-2)	IGDJ (10-2)	Hv↑ (10-2)	IGDJ (10-2)
		ɑ =	0 0.25	7.89 ± 0.11	0.041 ± 0.012	9.39 ± 0.038	0.0056 ± 0.002
γ	0.1	α	= 0.5	7.86 ± 0.12	0.043 ± 0.012	9.39 ± 0.038	0.0056 ± 0.002
		ɑ =	二 0.75	7.84 ± 0.11	0.045 ± 0.013	9.38 ± 0.037	0.0057 ± 0.002
		Y =	0.01	7.86 ± 0.12	0.042 ± 0.012	9.39 ± 0.038	0.0056 ± 0.002
α	0.5	Y	=0.1	7.86 ± 0.12	0.043 ± 0.012	9.39 ± 0.038	0.0056 ± 0.002
		Y =	0.25	7.85 ± 0.11	0.042 ± 0.012	9.39 ± 0.036	0.0056 ± 0.002
Table 4: Ablation study based on Multi-Mnist dataset with different choice of α and γ.
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
C.2.3 Ablation Study
We conduct ablation study to understand the effect of α and γ using the Pareto approximation task on
Multi-Mnist. We compare PNG with α = 0.25, 0.5, 0.75 and γ = 0.01, 0.1, 0.25. Figure 4 summarizes the
result. Overall, we observe that PNG is not sensitive to the choice of hyper-parameter.
C.2.4 Comparing with the Second Order Approach
We give a discussion on comparing our approach with the second order approaches proposed by Ma et al.
(2020). In terms of algorithm, Ma et al. (2020) is a local expansion approach. To apply Ma et al. (2020),
in the first stage, we need to start with several well distributed models (i.e., the ones obtained by linear
scalarization with different preference weights) and Ma et al. (2020) is only applied in the second stage to
find the neighborhood of each model. The performance gain comes from the local neighbor search of each
model (i.e. the second stage).
In comparison, PNG with energy distance is a global search approach. It improves the well-distributedness
of models in the first stage (i.e. it’s a better approach than simply using linear scalarization with different
weights). And thus the performance gain comes from the first stage. Notice that we can also apply Ma et al.
(2020) to PNG with energy distance to add extra local search to further improve the approximation.
In terms of run time comparison. We compare the wall clock run time of each step of updating the 5 models
using PNG and the second order approach in Ma et al. (2020). We calculate the run time based on the
multi-MNIST dataset using the average of 100 steps. PNG uses 0.3s for each step while Ma et al. 2020 uses
16.8s. PNG is 56x faster than the second order approach. And we further argue that, based on time complexity
theory, the gap will be even larger when the size of the network increases.
C.3 Understanding PNG Dynamics
We draw more analysis to understand the training dynamics of PNG.
Different Staring Points We give analysis on PNG with different initializations showing that PNG is
more robust to the initialization than other approaches such as Lin et al. (2019). We consider the Pareto set
approximation tasks and reuse synthetic example introduced in Section 5.1. We consider learning 5 models to
approximate the Pareto front staring from two different bad starting points. Specifically, in the upper row of
Figure 4, we consider initializing the models using linear scalarization. Due to the concavity of the Pareto
front, linear scalarization can only learns models at the two extreme end of the Pareto front. The second row
uses MGD for initialization and the models is scattered at an small region of the Pareto front. Different from
the algorithm proposed by Lin et al. (2019) which relies on a good initialization, using the proposed energy
21
Under review as a conference paper at ICLR 2022
Figure 4: Evolution of models from different initialization. Upper row uses initialization with linear
scalarization and lower row uses initialization from MDG. From left to right: the evolution of models during
training. PNG is robust to initializations. In both two cases of very poor initialization, PNG is still able to
move the models so that they are eventually well distributed on the Pareto set.
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
distance function, PNG pushes the models to be equally distributed on the Pareto Front without the need of
any prior information of the Pareto front even with extremely bad starting point.
Trajectory Visualization with Different Hyper-parameters We also give more visualization on the PNG
trajectory when using different hyper-parameters. We reuse synthetic example introduced in Section 5.1
for studying the hyper-parameters α and γ. We fix α = 0.25 and vary γ = 0.1, 0.05, 0.01, 0.1; and fix
γ = 0.01 and vary α = 0.1, 0.25, 0.5, 0.75. Figure 5 plots the trajectories. As we can see, when γ is properly
chosen, with different α, PNG finds the correct models with different trajectories. Different α determines the
algorithm’s behavior of balancing the descent of task losses or criterion objectives. On the other hand, with
too large γ, the algorithm fails to find a model that is close to P*, which is expected.
C.4 Improving Multitas k Based Domain Generalization
We argue that many other deep learning problems also have the structure of multitask learning when multiple
losses presents and thus optimization techniques in multitask learning can also be applied to those domains.
In this paper we consider the JiGen (Carlucci et al., 2019b). JiGen learns a model that can be generalized to
unseen domain by minimizing a standard cross-entropy loss 6嗨 for classification and an unsupervised loss
`jig based on Jigsaw Puzzles:
'(O) = (I- ω)'class(θ) + ω'jig⑹.
The ratio between two losses, i.e. ω, is important to the final performance of the model and requires a
careful grid search. Notice that JiGen is essentially searching for a model on the Pareto front using the linear
scalarization. Instead of using a fixed linear scalarization to learn a model, one natural questions is that
22
Under review as a conference paper at ICLR 2022
Figure 5: Ablation study on OPT-in-Pareto with different ratio constraint of objectives. Upper row, from
left to right: fixing α = 0.25, γ = 0.1, 0.05, 0.01, 0.001; Lower row, from left to right: fixing γ = 0.01,
α = 0.1, 0.25, 0.5, 0.75. By comparing the figures in the first row, we find that choosing a too large γ make
the final converged model be far away from the Pareto set, which is as expected. By comparing the figures in
the second row, we find that changing α make PNG give different priority in making Pareto improvement or
descent on F . When α is larger (the right figures), PNG will first move the model to Pareto set and start to
decrease F after that.
23
Under review as a conference paper at ICLR 2022
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
whether it is possible to design a mechanism that dynamically adjusts the ratio of the losses so that we can
achieve to learn a better model.
We give a case study here. Motivated by the adversarial feature learning (Ganin et al., 2016), we propose
to improve JiGen such that the latent feature representations of the two tasks are well aligned. Specifically,
suppose that Φclass (θ) = {φclass(xi, θ)}in=1 and Φjig(θ) = {φjig(xi, θ)}in=1 is the distribution of latent feature
representation of the two tasks, where xi is the i-th training data. We consider FPD as some probability metric
that measures the distance between two distributions, we consider the following problem:
min FPD[Φclass(θ), Φjig(θ)].
θ∈P*
With PD as the criterion function, our algorithm automatically reweights the ratio of the two tasks such that
their latent space is well aligned.
Setup We fix all the experiment setting the same as Carlucci et al. (2019b). We use the Alexnet and Resnet-18
with multihead pretrained on ImageNet as the multitask network. We evaluate the methods on PACS (Li et al.,
2017), which covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches). Same to
Carlucci et al. (2019b), we trained our model considering three domains as source datasets and the remaining
one as target. We implement FPD that measures the discrepancy of the feature space of the two tasks using
the idea of Domain Adversarial Neural Networks (Ganin & Lempitsky, 2015) by adding an extra prediction
head on the shared feature space to predict the whether the input is for the classification task or Jigsaw task.
Specifically, we add an extra linear layer on the shared latent feature representations that is trained to predict
the task that the latent space belongs to, i.e.,
1n
FpD(Φclass(θ), Φjig(θ)) = min - J^lθg(σ(w>φclass(xi,θ))) + log(1 - σ(w>φciass(xi, θ))).
w,b n
, i=1
Notice that the optimal weight and bias for the linear layer depends on the model parameter θ, during the
training, both w, b and θ are jointly updated using stochastic gradient descent. We follow the default training
protocol provided by the source code of Carlucci et al. (2019b).
Baselines Our main baselines are JiGen (Carlucci et al., 2019b); JiGen + adv, which adds an extra domain
adversarial loss on JiGen; and our PNG with domain adversarial loss as criterion function. In order to run
statistical test for comparing the methods, we run all the main baselines using 3 random trials. We use the
released source code by Carlucci et al. (2019b) to obtained the performance of JiGen. For JiGen+adv, we use
an extra run to tune the weight for the domain adversarial loss. Besides the main baselines, we also includes
TF (Li et al., 2017), CIDDG (Li et al., 2018b), MLDG (Li et al., 2018a) , D-SAM (D’Innocente & Caputo,
2018) and DeepAll (Carlucci et al., 2019b) as baselines with the author reported performance for reference.
Result The result is summarized in Table 5 with bolded value indicating the statistical significant best methods
with p-value based on matched-pair t-test less than 0.1. Combining Jigen and PNG to dynamically reweight
the task weights is able to implicitly regularizes the latent space without adding an actual regularizer which
might hurt the performance on the tasks and thus improves the overall result.
24
Under review as a conference paper at ICLR 2022
Method	Art paint	Cartoon	Sketches	Photo	Avg
AleXNet					
TF	0.6268	0.6697	0.5751	0.8950	0.6921
CIDDG	0.6270	0.6973	0.6445	0.7865	0.6888
MLDG	0.6623	0.6688	0.5896	0.8800	0.7001
D-SAM	0.6387	0.7070	0.6466	0.8555	0.7120
DeepAll	0.6668	0.6941	0.6002	0.8998	0.7152
JiGen	0.6855 ± 0.004	0.6889±0.002	0.6831±0.011	0.8946 ± 0.008	0.7380 ± 0.002
JiGen + adv	0.6857 ± 0.004	0.6837 ± 0.003	0.6753 ± 0.008	0.8980 ± 0.001	0.7357 ± 0.003
Jigen + PNG	0.6914±0.005	0.6903±0.002	0.6855±0.007	0.9044±0.003	0.7429±0.002
ResNet-18					
D-SAM	0.7733	0.7243	0.7783	0.9530	0.8072
DeepAll	0.7785	0.7486	0.6774	0.9573	0.7905
JiGen	0.8009 ± 0.004	0.7363 ± 0.007	0.7046 ± 0.013	0.9629±0.002	0.8012 ± 0.002
JiGen + adv	0.7923 ± 0.006	0.7402 ± 0.004	0.7188 ± 0.005	0.9617 ± 0.001	0.8033 ± 0.001
JiGen + PNG	0∙8014±0.005	0.7538±0.001	0.7222±0.006	0.9627±0.002	0.8100±0.005
Table 5: Comparing different algorithms for domain generalization using dataset PACS and two network
architectures.
25