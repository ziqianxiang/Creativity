Under review as a conference paper at ICLR 2022
Federated Robustness Propagation: Sharing
Adversarial Robustness in Federated Learn-
ING
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) emerges as a popular distributed learning schema that
learns a model from a set of participating users without requiring raw data to be
shared. One major challenge of FL comes from heterogeneity in users, which may
have distributionally different (or non-iid) data and varying computation resources.
Just like in centralized learning, FL users also desire model robustness against
malicious attackers at test time. Whereas adversarial training (AT) provides a sound
solution for centralized learning, extending its usage for FL users has imposed
significant challenges, as many users may have very limited training data as well as
tight computational budgets, to afford the data-hungry and costly AT. In this paper,
we study a novel learning setting that propagates adversarial robustness from high-
resource users that can afford AT, to those low-resource users that cannot afford
it, during the FL process. We show that existing FL techniques cannot effectively
propagate adversarial robustness among non-iid users, and propose a simple yet
effective propagation approach that transfers robustness through carefully designed
batch-normalization statistics. We demonstrate the rationality and effectiveness
of our method through extensive experiments. Especially, the proposed method is
shown to grant FL remarkable robustness even when only a small portion of users
afford AT during learning.
1	Introduction
Federated learning (FL) (McMahan et al., 2017) is a learning paradigm that trains models from
distributed users or participants (e.g., mobile devices) without requiring raw training data to be shared,
alleviating the rising concern of privacy issues when learning with sensitive data and facilitating
learning deep models by enlarging the amount of data to be used for training. In a typical FL algorithm,
each user trains a model locally using their own data and a server iteratively aggregates users’
incremental updates or intermediate models, converging to a model that fuses training information
from all users. A major challenge in FL comes from the heterogeneity of users. One source of
heterogeneity is distributional differences in training data collected by users from diverse user
groups (Fallah et al., 2020; Zhu et al., 2021). Yet another source is the difference of computing
resources, as different types of hardware used by users usually result in varying computation budgets.
For example, consider an application scenario of FL from mobile phones (Hard et al., 2019), where
different types of mobile phones (e.g., generations of the same brand) may have drastically different
computational power.
Data heterogeneity should be carefully handled during the learning as a single model trained by
FL may fail to accommodate the differences (Yu et al., 2020). A variety of approaches have been
proposed to address the issue, such as customizing network structures (Li et al., 2020c; Arivazhagan
et al., 2019) or tailoring training strategies (Fallah et al., 2020; Dinh et al., 2020) for each user. Even
though hardware heterogeneity is ubiquitous, their impacts to FL processes and possible solutions
have received very limited attention so far.
The impacts of the two types of heterogeneity become aggravated when participating users’ desire
adversarial robustness during the inference stage, against imperceptible noise that can significantly
mislead model predictions. To address this issue, a straightforward extension of FL, federated
1
Under review as a conference paper at ICLR 2022
Figure 1: (a) We define a novel problem setting, where standard-training (ST) users, which might be
limited by data or computational resources, can “share" robustness from adversarial-training (AT)
users who can afford it. (b) Comparison of robustness on a varying portion of AT users, where a
5-domain digit recognition dataset is distributed to 50 users in total and details are in Appendix C.7.
adversarial training (FAT), can be adopted, which idea was explored in (Zizzo et al., 2020; Reisizadeh
et al., 2020). Locally, each user trains models with adversarially augmented samples, namely
adversarial training (AT) (Xie et al., 2020). As studied in central settings, the AT is data-thirsty and
computationally expensive (Shafahi et al., 2019a). Therefore, involving a fair amount of users in
FAT is essential, given the fact that each individual user may not have enough data to perform AT.
However, this implies an increasing difficulty for fitting diverse data distributions and more intensive
computation for each user, which could be 3 - 10 times more costly than the standard equivalent
(Shafahi et al., 2019a; Zhang et al., 2019). The computation overhead can be prohibitive for FL users
with limited computational budget such as mobile devices. As such, it is often unrealistic to enforce
all users in a FL process to conduct AT locally, despite the fact that the robustness is indeed a strongly
desired or even required property for all users. This conflict raises a challenging yet interesting
question: Is it possible to propagate adversarial robustness in FL so that budget-limited users can
benefit from robustness training of users with abundant computational resources?
Motivated by the question above, we formulate a novel problem setting called Federated Robustness
Propagation (FRP), as depicted in Fig. 1a. We consider a rather common non-iid FL setting that
involves budget-sufficient users (AT users) that conduct adversarial training, and budget-limited ones
(ST users) that can only afford standard training. The goal of FRP is to propagate the adversarial
robustness from AT users to ST users. Note that sharing adversarial data is prohibited for mitigating
the overhead of adversarial-data generation, due to the privacy consideration of the FL framework.
In Fig. 1b, we show that independent AT by users without FL (local AT) will not yield a robust
model since each user has scarce training data. Directly extending an existing FL algorithm FedAvg
(McMahan et al., 2017) or a heterogeneity-mitigated one FedBN (Li et al., 2020c) with AT treatments,
named as FATAvg and FATBN, give very limited capability of propagating robustness. The limitation
may arise from the following two reasons: 1) Heterogeneity of users results in distinct noise behavior
in different users, degrading the transferability of model robustness. 2) Structural differences between
clean and adversarial samples may further impede robustness propagation (Xie & Yuille, 2019).
To address the aforementioned challenges, we propose a novel method Federated Robust Batch-
Normalization (FedRBN) to facilitate propagation of adversarial robustness among FL users. 1)
We design a surprisingly simple linear method that transmits the robustness by copying batch-
normalization (BN) statistics, inspired by the strong connection between model robustness and
statistic parameters in the BN layer (Schneider et al., 2020); 2) To efficiently propagate robustness
among non-iid users, we weight and average multiple AT users’ statistics as BN for every ST user; 3)
Facing the structural difference between the clean and adversarial data, we train two separate BNs for
each data type, which are adaptively chosen at the inference stage. Our method is communication-
efficient as it only incurs an one-time additional communication after training. We conduct extensive
experiments demonstrating the feasibility and effectiveness of the proposed method. In Fig. 1b, we
highlight some experimental results from Section 5. When only 20% of non-iid users used AT during
learning, the proposed FedRBN yields robustness, competitive with the best all-AT-user baseline
(FATBN) by only a 2% drop (out of 59%) on robust accuracy. Note that even if our method with
100% AT users increase the upper bound of robustness, such a bound is usually not attainable because
of the presence of resource-limited users that cannot afford AT during learning.
2
Under review as a conference paper at ICLR 2022
2	Related Work
Federated learning for robust models. The importance of adversarial robustness in the context
of federated learning, i.e., federated adversarial training (FAT), has been discussed in a series of
recent literature (Zizzo et al., 2020; Reisizadeh et al., 2020; Kerkouche et al., 2020). Zizzo et
al. (Zizzo et al., 2020) empirically evaluated the feasibility of practical FAT configurations (e.g., ratio
of adversarial samples) augmenting FedAvg with AT but only in iid and label-wise non-iid scenarios.
The adversarial attack in FAT was extended to a more general affine form, together with theoretical
guarantees of distributional robustness (Reisizadeh et al., 2020). It was found that in a communication-
constrained setting, a significant drop exists both in standard and robust accuracies, especially with
non-iid data (Shah et al., 2021). In addition to the challenges investigated above, this work studies
challenges imposed by hardware heterogeneity in FL, which was rarely discussed. Especially, when
only limited users have devices that afford AT, we strive to efficiently share robustness among users,
so that users without AT capabilities can also benefit from such robustness.
Robust federated optimization. Another line of related work focuses on the robust aggregation of
federated user updates (Kerkouche et al., 2020; Fu et al., 2019). Especially, Byzantine-robust federated
learning (Blanchard et al., 2017) aims to defend malicious users whose goal is to compromise
training, e.g., by model poisoning (Bhagoji et al., 2018; Fang et al., 2020) or inserting model
backdoor (Bagdasaryan et al., 2018). Various strategies aim to eliminate the malicious user updates
during federated aggregation (Chen et al., 2017; Blanchard et al., 2017; Yin et al., 2018; Pillutla et al.,
2020). However, most of them assume the normal users are from similar distributions with enough
samples such that the malicious updates can be detected as outliers. Therefore, these strategies could
be less effective on attacker detection given a finite dataset (Wu et al., 2020). Even though both the
proposed FRP and Byzantine-robust studies work with robustness, they have fundamental differences:
the proposed work focus on the robustness during inference, i.e., after the model is learned and
deployed, whereas Byzantine-robust work focus on the robust learning process. As such, the proposed
approach can combine with all Byzantine-robust techniques to provide training robustness.
3	Problem Setting: Federated Robustness Propagation (FRP)
In this section, we will review AT, present the unique challenges from hardware heterogeneity in FL
and formulate the problem of federated robustness propagation (FRP). In this paper, we assume that a
dataset D includes sampled pairs of images x ∈ Rd and labels y ∈ Rc from a distribution D. Though
we limit the data as images in this paper, our discussion could be generalized to other data forms. We
model a classifier, mapping from the Rd data/input space to classification logits f : Rd → Rc, by a
deep neural network (DNN). Whenever not causing confusing, we use the symbol of a model and
its parameters interchangeably. For brevity, We slightly abuse E[∙] for both empirical average and
expectation and use [N] to denote {1, . . . , N}.
3.1	Standard training and adversarial training
An adversarial attack applies a bounded noise δ : kδ k ≤ to an image x such that the perturbed
image A (x) , x + δ can mislead a Well-trained model to give a Wrong prediction. The norm
∣∣∙k can take a variety of forms, e.g., L∞-norm for constraining the maximal pixel scale. A model
f is said to be adversarially robust if it can predict labels correctly on a perturbed dataset D =
{(A(x), y)|(x, y) ∈ D}, and the standard accuracy on D should not be greatly impacted.
Consider the folloWing general learning objective:
minf L(f, D)=minf ɪ X [(1 - q) 'c(f ； x,y) + q'a(f ； x,y)],	(1)
|D|	(x,y)∈D
where 'c is a standard classification loss on clean images and 'a is an adversarial loss promoting
robustness. Eq. (1) performs standard training ifq = 0, and adversarial training if q ∈ (0, 1]. Without
loss of generality, we limit our discussion for q as 0 or 0.5. A popular instantiation of Eq. (1) is based
on PGD attack (Madry et al., 2018; Tsipras et al., 2019): 'c(f; x, y) = '(f (x), y), 'a(f; x, y)=
max∣∣δk≤e '(f (x+δ), y), where ∣∣∙∣ is the L∞-norm,' can be the cross-entropy loss, i.e., '(f (x), y)=
- Ptc=1 yt log(f(x)t) where t is the class index and f (x)t represents the t-th output logit.
3
Under review as a conference paper at ICLR 2022
3.2	Problem setup and challenges
We start with a typical FL setting: a finite set of distributions Di for i ∈ [C], from which a set of
datasets {Dk}kK=1 are sampled and distributed to K users’ devices. The users from distinct domains
related with Di expect to optimize objectives like Eq. (1). Some users can afford AT training (AT
users from group S with q = 0.5) when the remaining users cannot afford and use standard training
(ST users from group T with q = 0). If the two groups of users train models separately, the models
of ST users will be much less robust than those of AT ones. Note that data exchange among users
is forbidden according to the the FL setting for privacy concerns. The goal of federated robustness
propagation (FRP) is to transfer the robustness from AT users to ST users at minimal computation
and communication costs while preserve data locally. Formally, the FRP objective minimizes:
FRP({fk}; {Dk IDk 〜Di}) , X ∣-5-∣ X	'c(fk )
k∈T |Dk|	(x,y)∈Dk
+ X， ς T7f^∣ Xz	、 n j['c(fk) + 'a(fk)].	(2)
k∈S |Dk|	(x,y)∈Dk 2
Note that different from FAT (Zizzo et al., 2020), FRP assumes that Dk is sampled from different
distributions and that there are at least one zero entry in q. In the federated setting, each user’s model
is trained separately when initialized by a global model, and is aggregated to a global model at the end
of each epoch. A popular aggregation technique is FedAvg (McMahan et al., 2017), which averages
parameters by f =会 PK=I ak fk With normalization coefficients ak proportional to |Dk∣.
Remarkably, Eq. (2) formalizes two types of common user heterogeneity in FL. The first one is the
hardware heterogeneity Where users are divided into tWo groups of different computation budgets.
A node of tight computation budget, e.g., smartphone, may join FL in group T , While a poWerful
one, e.g., desktop computer in S (Hard et al., 2019). Besides, data heterogeneity is represented as Di
differing by i. We limit our discussion as the common feature distribution shift (on x) in contrast to
the label distribution shift (on y), as previously considered in (Li et al., 2020c). Such distribution shift
often happens When users are distributed across different environments, e.g., sensor data collected
indoor and outdoor.
New Challenges. We emphasize that jointly addressing the tWo types of heterogeneity in Eq. (2)
forms a neW challenge, distinct from either of them considered exclusively. First, the scarcity ofAT
group S Worsens the data heterogeneity as additional distribution shift in the hidden representations
from adversarial augmentation (Xie & Yuille, 2019). That means even if tWo users sample from the
same distribution, their classification layers may operate on different distributions.
Second, the data heterogeneity makes the transfer of robustness non-trivial (Shafahi et al., 2019b).
Hendrycks et al. discussed the transfer of models adversarially trained on multiple domains and
massive samples (Hendrycks et al., 2019). In (Shafahi et al., 2019b), Shafahi et al.firstly studied the
transferability of adversarial robustness from one data domain to another Without the data-hungry
problem. They proposed fine-tuning the robustness-sensitive layers in a neural netWork on a target
domain. Distinguished from Shafahi et al.’s Work, the FRP problem focuses on propagating robustness
from multiple AT users to multiple ST users Who have diverse distributions. Thus, fine-tuning all
source models in ST users is often not possible due to prohibitive computation costs.
4	Method: Federated Robust Batch-Normalization (FedRBN)
4.1	Robustness propagation by copying debiased BN layers
In centralized learning, an important observation is that robustness is highly correlated With the BN
statistics (Xie & Yuille, 2019). We extend this investigation to the FL setting, Where We assume all
other parameters are shared besides BN layers. There are significant differences in BN parameters
(mean and variance) betWeen ST and AT users from the same domain, as shoWn in Fig. 2a. This
observation indicates that directly using local BN statistics can hardly grant robustness to an ST user,
and suggests a possible Way to transfer robustness through leveraging the BN layers from AT users
in ST users upon predicting possible adversarial input images. HoWever, the distributions of users
from distinct domains can be quite different (Joaquin Quinonero-Candela et al., 2008), and therefore
directly copying BN among users can suffer from the distribution shift by domains. This motivates us
to develop a shift-aWare debiasing method.
4
Under review as a conference paper at ICLR 2022
(b) Correlation of relative statistics in the 2nd BN layer.
Figure 2: Models are trained with decoupled BN layers on Digits dataset. bn1 is the first BN layer in
the network. (a) Results on SVHN. (b) The relative statistics are compared on MNIST versus SVHN.
(a) Great difference of Clean/noise statistics (y-axis)
To capture the domain bias, we can leverage the BN layers as they are modeling local distributions.
Ideally, differentially modeling two users will yield BN statistics for the two corresponding distribu-
tions, separately. However, one challenge here is that the BN statistics in non-iid AT and ST users
are biased simultaneously by the domain difference and the adversarial noise. As such, directly
differential modeling will capture the mixture of both types of bias and therefore would not be
effective to infer the domain bias. Instead, we incrementally propose to simultaneously model clean
data and noise data by BN for all AT users, since clean data are also available during training. To do
so, we replace standard BN layers by ones that use the dual batch-normalization (DBN) structure (Xie
et al., 2020), which keep two sets of BN statistics: one for clean data and one for noise data. The
channel-wise mapping of DBN from the input layer x to output z is
Z = W [(1 - h) √⅛ + h√x-2^i + b,	(3)
σ2+0	σr2 +0
where μ and σ2 are the mean and variance over all non-channel dimensions, μr and σ2r are their
corresponding noised statistics, h serves as a model-wise switch, which is 0 for clean inputs or 1
for noised inputs, 0 is a small constant for the numerical stability. Different from prior work, e.g.,
(Xie et al., 2020; Xie & Yuille, 2019), we use a shared affine weights (w) and bias (b) for efficiency
considerations. In the user-side training, we explicitly choose the clean or noise BN based on the
input. Though we introduce DBN for a bias-inference purpose, the DBN still merits the model
performance as it normalize representations more precisely as priorly investigated (Xie et al., 2020).
Transfer robustness from a single user via copying BN layers. With the clean BN embedded in
DBN, We can estimate the distributional difference by an AT clean BN statistic tuple (μs, σ2) and an
ST (clean) BN statistic tuple (μt, σ2). Formally, we propose a debiased statistic estimation by
μt = μs + λ(μt- μS), σr2 = σS2 (σ2∕(σ2 + eO))λ,	⑷
where λ is a hyper-parameter in [0,1]. Note that when the distributions are matched, i.e., μt = μs,
then debiasing is not necessary and is automatically vanished. To justify the rationality of Eq. (4), we
contrast the μs - μt with μts - μrt in Fig. 2b (more results in Appendix D.3). The clean and noise
BN statistics are estimated during training DBN, and we observe a strong correlation of the relative
difference among domains both for the mean and variance.
To understand the debiasing method, we provide a principled analysis on a simplified one-dimensional
example. We assume the noised inputs to a BN layer in user s can be approximated by an affine-noise
model Xs = λxs + ν, Xt = λxt + V, where Xs 〜N(μs, σ2), Xt 〜N(μt, σ2) and V 〜 N(μ0,σ02) is
domain-independent noise. λ is a constant scalar. We further assume V is independent from Xs and Xt .
Taking expectation gives μls = λμs + μ0, μ = λμt + μ0; σj2 = λ2σ2 + σ02, σt2 = λ2σ2 + σ02. Due
to the invariance assumption of (μ0,σ0),we have: μs = μs+λ(μt-μs), σs = ∙vzσs2 + λ2(σ2 - σ2).
However, σs is meaningless when σss + λ2 (σ2 - σ2) < 0. To fix this, we use a division instead of
subtraction to represent the relative relation in Eq. (4). This simplified example by no means serves
as rigorous analysis, which is an open problem outside the scope of this paper.
4.2	FedRBN algorithm and its efficiency
We are now ready to present the proposed the two-stage Federated Robust Batch-Normalization
(FedRBN) algorithm. During training (Algorithm 1), we train models locally with decoupled clean
and noise BNs for each user. Locally trained models excluding BN are then aggregated by federated
5
Under review as a conference paper at ICLR 2022
parameter averaging. For ST users, they are free from heavy adversarial training. After training
(Algorithm 2), the server aggregates BN statistic debiasing from AT users which are broadcasted to
ST users for BN statistic estimation. For simplicity, We use μ and σ2 for parameters in all layers.
Algorithm 1 FedRBN: user training
Input: An initial model f from the server, adversary
A(∙), dataset D, user budget type (AT or ST)
1:	for mini-batch {(x, y)} in D do
2:	Set f to use clean BN by h - 0
3:	L - E(χ,y)['(f, (x,y))]
4:	if user budget type is AT then
5:	Perturb data X — A(X)
6:	Set f to use noise BN by h - 1
7:	L 一 2{L + E(x,y)['(f, (X,y))]}
8:	Update f by one-step gradient descent
9:	Upload parameters of layers except BN layers
Algorithm 2 FedRBN: post-training
Input: AT users S = {s} and ST user T = {t}
1:	for AT users s in S locally do	
2:	∆μs = μs ― λμs	
3:	∆ log σsr2 = log σsr2 - λ log σs2	
4:	Upload ∆μS, ∆ log σS2 and to server	
5:	∆μr = ⅛ Ps∈s AμS	
6:	δ log σr2 = ∣⅜ Ps∈S δ log σr2	
7:	for ST users t in T locally do	
8:	Download ∆μr, ∆ log σr 2	
9:	μt = λμt + ∆μr 2 2	「.一	C		τrl	
10	σ^r = exp λ log σ2 + ∆ log σr 2	
Inference-stage BN selection (parameter h). One issue of FedRBN is the choice ofBN at inference
time. To balance accuracy and robustness, We need a strategy to automatically select the right BN
for each sample, i.e., applying noise BN When the input is found to be an adversarial one. Although
the differences betWeen clean data and adversarial are subtle in raW images, recent advances have
shoWn promising results With the help of netWork structures. For example, in (Pang et al., 2018), the
authors find that the non-maximal entropy of their logits are quite distinct. In (Feinman et al., 2017),
their representations are separable under the similarity measurement of RBF-kernel (or K-density).
Formally, we first compute the logits by l = f (x) with h J 0. Then we use g(l) to predict whether
the image is noised. If g(l) indicates the image is noised, we assign h J g(l) and re-predict the
label by f (x). The details of locally training noise detectors is summarized in Algorithm 3. As only
logits are required as input for g(∙), the training is efficient. In Figs. 10 and 11, we visually show
that adversarial examples can be identified by a classifier g(∙) in the space of logits. More results are
provided in Appendix C. Our strategy is similar to the layer-wise gating Liu et al. (2020), but we
only use a gate function (namely noise detector) on model outputs, which can be efficiently trained
without repeated adversarial-sample generation.
Efficiency and privacy of BN operations. Since the BN statistics are only a small portion of any
networks and do not require back-propagation, an additional BN statistic will not significantly affect
the efficiency (Wang et al., 2020). During training, because users do not send out BN layers, the
communication cost is the same as a non-iid FL method (FedBN (Li et al., 2020c)) and less than other
fully-shared methods like FedAvg (McMahan et al., 2017). After the training is done, BN layers
will be copied to transfer robustness, and such one-time cost of transferring marginal components of
networks will be neglectable, compared to the total cost incurred in FL. Noticeably, AT users will
share the statistic difference (∆μ) instead of local statistics (μ) with the server, and the server will
send the averaged parameters with ST users only. Especially when local statistics may leak users’
data (Geiping et al., 2020), our method enjoys better privacy than traditional methods, like FedAvg. 5
5 Experiments
Datasets and models. To implement a non-iid scenario, we adopt a close-to-reality setting where
users’ datasets are sampled from different distributions. We used two multi-domain datasets for
the setting. The first is a subset (30%) of Digits, a benchmark for domain adaption (Peng et al.,
2019b). DIGITS has 28 × 28 images and serves as a commonly used benchmark for FL (Caldas et al.,
2019; McMahan et al., 2017; Li et al., 2020a). DIGITS includes 5 different domains: MNIST (MM)
(Lecun et al., 1998), SVHN (SV) (Netzer et al., 2011), USPS (US) (Hull, 1994), SynthDigits (SY)
(Ganin & Lempitsky, 2015), and MNIST-M (MM) (Ganin & Lempitsky, 2015). The second dataset is
DomainNet (Peng et al., 2019a) processed by (Li et al., 2020c), which contains 6 distinct domains
of large-size 256 × 256 real-world images: Clipart (C), Infograph (I), Painting (P), Quickdraw (Q),
Real (R), Sketch (S). For Digits, we use a convolutional network with BN (or DBN) layers following
each conv or linear layers. For the large-sized DomainNet, we use AlexNet (Krizhevsky et al.,
2012) extended with BN layers after each convolutional or linear layer (Li et al., 2020c). One more
large-sized image dataset is presented in Appendix C.5.
6
Under review as a conference paper at ICLR 2022
Training and evaluation. For AT users, we use n-step PGD (projected gradient descent) attack
(Madry et al., 2018) with a constant noise magnitude . Following (Madry et al., 2018), we use
= 8/255, n = 7, and attack inner-loop step size 2/255, for training, validation, and test. We
uniformly split the dataset for each domain into 10 subsets for DIGITS and 5 for DOMAINNET,
following (Li et al., 2020c), which are distributed to different users, respectively. Accordingly,
we have 50 users for DIGITS and 30 for DOMAINNET. Each user trains local model for one
epoch per communication round. We evaluate the federated performance by standard accuracy (SA),
classification accuracy on the clean test set, and robust accuracy (RA), classification accuracy on
adversarial images perturbed from the original test set. All metric values are averaged over users.
We defer other details of experimental setup such as hyper-parameters to Appendix C, and focus on
discussing the results.
5.1	Comprehensive study
To further understand the role of each component in FedRBN, we conduct a comprehensive study on
its properties. In experiments, we use three representative federated baselines combined with AT:
FedAvg (McMahan et al., 2017), FedProx (Li et al., 2020a), and FedBN (Li et al., 2020c). We use
FATAvg to denote the AT-augmented FedAvg, and similarly FATProx and FATBN. To implement
hardware heterogeneity, we let 20%-per-domain users from 3/5 domains (of DIGITS) conduct AT.
Ablation Study. We use FATBN as the base method and incrementally add FedRBN components:
+DBN, +detector (for adaptively BN selection), +copy BN statistics, and +debias copying.
Table 1 shows that even though DBN is a critical structure, simply adding DBN does not help unless
the noise detector is applied. Also, the most influential component is copying, supporting our key idea.
The +debias is more important in single AT domain case where domain gaps varies by different
ST domains. Other than +copy, all other components barely affect the SA.
Table 1: Ablation of different FedRBN components. Standard deviations are enclosed in brackets.
AT users	metric	base	+DBN	+detector	+copy	+debias
20% in 5/5 domains	RA	41.2(1.3)	38.8 (1.4)	42.4 (1.4)	55.7 (1.0)	55.7 (1.3)
	increment		-2.4	+3.6	+13.3	+0.0
	SA	86.4 (0.4)	86.5 (0.4)	86.2 (0.4)	85.2 (0.6)	85.2 (0.4)
100% in 1/5 domain	RA	36.5(1.8)	34.3 (2.0)	37.3 (1.8)	48.1 (1.6)	49.8 (1.6)
	increment		-2.2	+3.0	+10.8	+1.7
	SA	86.4 (0.4)	86.4 (0.4)	86.3 (0.4)	84.3 (0.5)	84.4 (0.4)
Impacts from Data Heterogeneity. To study the influence of different AT domains, we set up an
experiment where AT users only reside on one single domain. For simplicity, we let each domain
contains a single user as in (Li et al., 2020c) and utilize only 10% of Digits dataset. The single AT
domain plays the central role in gaining robustness from adversarial augmentation and propagating
to other domains. The task is hardened by the non-singleton of gaps between the AT domain and
multiple ST domains and a lack of the knowledge of domain relations. Results in Fig. 3a show
the superiority of the proposed FedRBN, which improves RA for more than 10% in all cases with
small drops in SA. We see that RA is the worst when MNIST serves as the AT domain, whereas
RA propagates better when the AT domain is SVHN or SynthDigits. A possible explanation is that
SVHN and SynthDigits are more visually different from the rest domains (see Fig. 6), forming larger
domain gaps at test.
Impacts from Hardware Heterogeneity. We vary the number of AT users in training from 1/N
(most heterogeneous) to N/N (homogeneous) to compare the robustness gain. Fig. 3b shows that
our method consistently improves the robustness. Even when all domains are noised, FedRBN is the
best due to the use of DBN. When not all domains are AT, our method only needs half of the users to
be noised such that the RA is close to the upper bound (fully noised case).
Other comprehensive studies including the parameter sensitivity and convergence are delayed to
Appendix C.2. The impact of total data sizes is included in Appendix C.3.
5.2	Comparison to baselines
To demonstrate the effectiveness of the proposed FedRBN, we compare it with baselines on two
benchmarks. We repeat each experiment for three times with different seeds. We introduce two more
baselines: personalized meta-FL extended with FAT (FATMeta) (Fallah et al., 2020) and federated
7
Under review as a conference paper at ICLR 2022
Table 2: Benchmarks of robustness propagation, where we measure the computation time (T ) by
counting ×1012 times of float add-or-multiplication operations (FLOPs).
AT users Metrics	Digits									DomainNet								
	All			20%			MNIST			All			20%			Real		
	RA	SA	T	RA	SA	T	RA	SA	T	RA	SA	T	RA	SA	T	RA	SA	T
FedRBN (ours)	66.7	87.3	2218	56.2	85.3	665	49.8	84.3	665	35.9	60.5	38490	24.2	61.5	11547	21.6	61.9	10425
FATBN	60.0	87.3	2211	41.2	86.4	663	36.5	86.4	663	35.2	60.2	38363	20.3	63.2	11509	15.7	64.7	10390
FATAvg	58.3	86.1	2211	42.6	84.6	663	38.4	84.1	663	24.6	47.4	38363	15.4	57.8	11509	10.7	57.9	10390
FATProx	58.5	86.3	2211	42.8	84.5	663	38.1	84.1	663	24.8	47.1	38363	14.5	57.3	11509	10.4	57.1	10390
FATMeta	43.6	71.6	2211	35.0	72.6	663	35.3	72.2	663	6.0	23.5	38363	0.0	37.2	11509	0.1	38.1	10390
FedRob	13.1	13.1	2211	20.6	59.3	1032	17.7	48.9	645	-	-	-	-	-	-	-	-	-
Table 3: Compare FedRBN versus
efficient federated AT on Digits.
	20% 3/5 AT domains		100% Free AT (Shafahi et al., 2019a)	
	FedRBN	FATAvg	FATAvg	FATBN
RA	56.1	44.9	47.1	46.3
SA	86.2	85.6	63.6	57.4
T	273	271	276	276
Table 4: Evaluation of RA with various attacks on Digits. n
and are the step number and the magnitude of attack.
Attack (n, )	PGD (20,16)	PGD (100,8)	MIA (20,16)	MIA (100,8)	AA (-, 8)	LSA (7, -)	SA -
FedRBN	42.8	54.5	39.9	52.2	48.3	73.5	84.2
FATBN	28.6	41.6	27.0	39.7	31.0	64.0	84.6
FATAvg	31.5	43.4	30.0	41.5	32.9	63.3	84.2
robust training (FedRob) (Reisizadeh et al., 2020). Because FedRob requires a project matrix of
the squared size of image and the matrix is up to 2562 × 2562 on DOMAINNET which does not
fit into a common GPU, we exclude it from comparison. Given the same setting, we constrain the
computation cost in the similar scale for cost-fair comparison. We evaluate methods on two FRP
settings. 1) Propagate from a single domain. In reality, a powerful computation center may join the
FL with many other users, e.g., mobile devices. Therefore, the computation center is an ideal node
for the computation-intensive AT. Due to limitations of data collection, the center may only have
access to a single domain, resulting gaps to most other users. We evaluate how well the robustness
can be propagated from the center to others. 2) Propagate from a few multi-domain AT users. In
this case, we assume that to reduce the total training time, ST users are exempted from the AT tasks
in each domain. Thus, an ST user wants to gain robustness from other same-domain users, but the
different-domain users may hinder the robustness due to the domain gaps in adversarial samples.
Table 2 shows that our method outperforms all baselines for all tasks, while it associates to only
marginal overhead (for fitting noise detector). Importantly, we show that only 20% users are enough
to achieve robustness comparable to the best fully-trained baseline. To fully evaluate the robustness,
we experiment with more attack methods, including MIA (Dong et al., 2018), AutoAttack (AA)
(Croce & Hein, 2020) and LSA (Narodytska & Kasiviswanathan, 2016). A strong score-based
blackbox attacks such as Square Attack (Andriushchenko et al., 2020) (included in AA) can avoid the
trip fake robustness due to obfuscated gradient Even evaluated by different attacks (see Table 4), our
method still outperforms others. With the concern that the attacker may bypass the noise detector and
lead to the trip of fake robustness (Athalye et al., 2018), we include joint-attack on noise detector
and the model prediction in Appendix C.8. Though, the accuracies of noise detectors are degraded
somehow, the overall RA or SA of our method are still outstanding.
Compare to full efficient AT. In Table 3, we show that when computation time is comparable, our
method can achieve both better RA and SA than full-AT baselines. For results to be comparable, we
train FedRBN for limited 150 epochs while Free AT for 300 epochs. Although Free AT improves
the robustness compared to FATAvg, it also greatly sacrifices SA performance. Thanks to stable
convergence and decoupled BN, FedRBN maintains both accurate and robust performance though
the AT is not ‘free’ for a few users.
More federated configurations. We also evaluate our method against FedBN with different feder-
ated configurations of local epochs E and batch size B. We constrain the parameters by E ∈ {1, 4, 8}
and B ∈ {10, 50, 100}. The 20% 3/5 domain FRP setting is adopted with DIGITS dataset. In
Table 5 (a longer one in Table 14), the competition results are consistent that our method significantly
promotes robustness over FedBN. We also observe that both our method and FedBN prefer a smaller
batch size and fewer local epochs for better RA and SA. In addition, our method drops less RA when
E is large or batch size increases.
8
Under review as a conference paper at ICLR 2022
50
45
40
35
30
MN MM SV SY US
AT domain
60
50
40
30
0.00 0.25 0.50 0.75 1.00
portion OfAT users per domain
I7l6i5l4
8 8 8 8
(造VS
portion ofAT users per domain
(a) FRP from a single AT domain
(b) FRP from partial AT users per domain
Figure 3:	Evaluating FRP performance with different FRP settings.
Evaluation with unequal dataset sizes. Sub-sampling the same number of data points for each user
may not be realistic in practice. Therefore, we study the experiment setting that sample sizes for
users are different. The experiment details are given in Appendix C.1. In Table 6, we summarize the
3-repetition-averaged comparison results on the 20% 3/5 domain FRP setting on the Digits dataset.
We see that our method is still most competitive with non-uniform dataset sizes.
Table 5: Evaluation with different FL configurations.
local epochs B	batch size E	method	RA	SA	Table 6: Comparison with unequal user- dataset sizes.		
10	1	FedBN	50.9	83.9			
	1	FedRBN	60.0	82.8		RA	SA
10	4	FedBN	42.0	75.8	FedRBN (ours)	53.1	84.4
	4	FedRBN	56.3	76.1	FedBN	37.3	85.7
50	1	FedBN	37.0	85.8	FedAvg	39.6	83.4
	1	FedRBN	53.2	84.5	FedProx	39.5	83.4
Domain-wise evaluation. We note that performance in different domains could vary a lot. Simply
comparing the performance averaged by users may not clearly present a fair comparison. As a
consequence, we conduct experiments to compare different methods domain by domain. The basic
settings follow Section 5.2. in Fig. 4a, one out of five domains is noised. Domains including SVHN,
USPS, SynthDigits, MNiST-M are not augmented with adversarial samples. Therefore robustness
is gained through federated propagation. Both in the easiest (USPS) and hardest (SVHN) cases,
FedRBN outperforms baselines with higher RA and similar SA. in Fig. 4b, 20% users are noised in
each domain. FedRBN improves the in-domain robustness propagation against FATBN by up to 20%
(USPS). in summary, the propagation efficiency of FedRBN is consistent across different domains.
40
20
C
MNIST SVHN	USPS SynthQIgItNN ISΓ-M
domain
MNIST SVHN	USPSSynthmg 倒 NIST
domain
MNIST SVHN USPS Synthmg 倒 NIST
domain
(b) 20% users are AT users
5
80
60
40
20
MNIST SVHN USPS Synthmgl明NlST-M
domain
(a) Users from the MNiST domain are AT users
Figure 4:	Comparison of robustness transfer approaches by domains.
6 Conclusion
in this paper, we investigate a novel problem setting, federate propagating robustness, and propose a
FedRBN algorithm that transfers robustness in FL through robust BN statistics. Extensive experiments
demonstrate the rationality and effectiveness of the proposed method, delivering both generalization
and robustness in FL. We believe such a client-wise efficient robust learning can broaden the
application scenarios of FL to users with diverse computation capabilities.
9
Under review as a conference paper at ICLR 2022
References
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efficient black-box adversarial attack via random search. arXiv:1912.00049 [cs, stat], July
2020.
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder-
ated learning with personalization layers. arXiv:1912.00818 [cs, stat], December 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, February 2018.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. International Conference on Machine Learning, July 2018.
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, November
2018.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with
adversaries: Byzantine tolerant gradient descent. Advances in Neural Information Processing
Systems, 30:119-129, 2017.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H. Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings.
arXiv:1812.01097 [cs, stat], December 2019.
Alvin Chan, Yi Tay, and Yew-Soon Ong. What it thinks is important is important: Robustness
transfers through input gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, March 2020.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing
Systems, 1(2):44:1-44:25, December 2017.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273-297,
September 1995.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of
diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206-2216.
PMLR, November 2020.
Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. In Advances in Neural Information Processing Systems, June 2020.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. arXiv:1710.06081 [cs, stat], March 2018.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-
learning approach. In Advances in Neural Information Processing Systems, June 2020.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Security
20), pp. 1605-1622, 2020.
Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting adversarial
samples from artifacts. arXiv:1703.00410 [cs, stat], November 2017.
Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen. Attack-resistant federated learning with residual-
based reweighting. September 2019.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International Conference on Machine Learning, pp. 1180-1189. PMLR, June 2015.
10
Under review as a conference paper at ICLR 2022
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients -
how easy is itto break privacy in federated learning? In Advances in Neural Information Processing
Systems, September 2020.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Frangoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv:1811.03604 [cs], February 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. Computer Vision and Pattern Recognition, 2016.
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In International Conference on Machine Learning, October 2019.
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 16(5):550-554, May 1994.
Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence.
Dataset Shift in Machine Learning | The MIT Press. MIT Press, 2008.
Raouf Kerkouche, Gergely Acs, and Claude Castelluccia. Federated learning in adversarial settings.
arXiv:2010.07808 [cs], October 2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolu-
tional neural networks. In Proceedings of the 25th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’12, pp. 1097-1105, Red Hook, NY, USA, December 2012.
Curran Associates Inc.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Conference on Systems and Machine
Learning Foundation (MLSys), April 2020a.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. International Conference on Learning Representations, June 2020b.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning
on non-iid features via local batch normalization. In International Conference on Learning
Representations, September 2020c.
Aishan Liu, Shiyu Tang, Xianglong Liu, Xinyun Chen, Lei Huang, Zhuozhuo Tu, Dawn Song, and
Dacheng Tao. Towards defending multiple adversarial perturbations via gated batch normalization.
arXiv:2012.01654 [cs], December 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. International Conference on
Learning Representations, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282, April 2017.
Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations for
deep networks. arXiv:1612.06299 [cs, stat], December 2016.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial
examples. Advances in Neural Information Processing Systems, November 2018.
11
Under review as a conference paper at ICLR 2022
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on
Computer Vision,pp. 1406-1415, 2019a.
Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko. Federated adversarial domain adaptation.
In International Conference on Learning Representations, September 2019b.
Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.
2020.
Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of affine distribution shifts. In Advances in Neural Information Processing
Systems, June 2020.
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias
Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances
in Neural Information Processing Systems, October 2020.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S.
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in Neural
Information Processing Systems, 2019a.
Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and
Tom Goldstein. Adversarially robust transfer learning. In International Conference on Learning
Representations, May 2019b.
Devansh Shah, Parijat Dube, Supriyo Chakraborty, and Ashish Verma. Adversarial training in
communication constrained federated learning. arXiv:2103.01319 [cs], March 2021.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems 30, pp. 4424-4434. Curran
Associates, Inc., 2017.
Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft. Improving the generalization of adver-
sarial training with domain adaptation. In International Conference on Learning Representations,
pp. 14, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. International Conference on Learning Representations,
September 2019.
Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, and Zhangyang Wang. Once-for-
all adversarial training: In-situ tradeoff between robustness and accuracy for free. Advances in
Neural Information Processing Systems, November 2020.
Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, September 2019.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B. Giannakis. Federated variance-reduced
stochastic gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal
Processing, 68:4583-4596, 2020.
Cihang Xie and Alan Yuille. Intriguing properties of adversarial training at scale. International
Conference on Learning Representations, December 2019.
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, and Quoc V. Le. Adversarial
examples improve image recognition. CVPR, April 2020.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning, pp.
5650-5659. PMLR, July 2018.
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation.
arXiv:2002.04758 [cs, stat], February 2020.
12
Under review as a conference paper at ICLR 2022
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. Advances in Neural Information
Processing Systems, pp. 12, 2019.
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous
federated learning. In Proceedings of the 38th International Conference on Machine Learning,
June 2021.
Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, and Beat Buesser. Fat: Federated adversarial training.
arXiv:2012.01791 [cs], December 2020.
A	Additional Related Work
This section reviews additional references in the areas of centralized adversarial learning and robust-
ness transfer.
Efficient centralized adversarial training. A line of work has been motivated by similar concerns
on the high time complexity of adversarial training. For example, Zhang et al. proposed to adversari-
ally train only the first layer of a network which is shown to be more influential for robustness Zhang
et al. (2019). Free AT Shafahi et al. (2019a) trades in some standard iterations (on different mini-
batches) for estimating a cached adversarial attack while keeping the total number of iterations
unchanged. Wong et al. proposed to randomly initialize attacks multiple times, which can improve
simpler attacks more efficiently Wong et al. (2019). Most of existing efforts above focus on speed
up the local training by approximated attacks that trade in either RA or SA for efficiency. Instead,
our method relocated the computation cost from budget-limited users to budget-sufficient users who
can afford the expansive standard AT. As result, the computation expense is indeed exempted for the
budget-limited users and their standard performance is not significantly influenced.
Robustness transferring. Our work is related to transferring robustness from multiple AT users to
ST users. For example, a new user can enjoy the transferrable robustness of a large model trained on
ImageNet Hendrycks et al. (2019). In order to improve the transferability, some researchers aim to
align the gradients between domains by criticizing their distributional difference Chan et al. (2020). A
similar idea was adopted for aligning the logits of adversarial samples between different domains Song
et al. (2019). By fine-tuning a few layers of a network, Shafahi et al. shows that robustness can be
transferred better than standard fine-tuning Shafahi et al. (2019b). Rather than a central algorithm
gathering all data or pre-trained models, our work considers a distributed setting when samples
or their corresponding gradients can not be shared for distribution alignment. Meanwhile, a large
transferrable model is not preferred in the setting, because of the huge communication cost associating
to transferring models between users. Because of the non-iid nature of users, it is also hard to pick a
proper user model, that works well on all source users, for fine-tuning on a target user.
Locally adapted models for data heterogeneity. In the sense of modeling data heterogeneity, some
prior work was done in adapting models for each local user (Smith et al., 2017; Arivazhagan et al.,
2019; Fallah et al., 2020; Dinh et al., 2020). For example, Smith et al. (2017) studied the linear
cases with regularization on the parameters, while we study a more general deep neural networks. In
addition, the work did not consider a data-dependent adversarial regularization for better robustness,
but a regularization that is independent from the data. Similarly, Dinh et al. (2020) regularizes the
local parameters similar to the global model in L2 distance, and Fallah et al. (2020) considers a
meta-learning strategy instead. A simpler method was proposed by Arivazhagan et al. (2019) to
only adapt the classifier head for different local tasks. Since all the above methods do not adapt the
robustness from global to local settings, we first study how the robustness can be propagated among
users in this work.
B	Additional Technical Details of FedRBN
The structure of FedRBN is illustrated in Fig. 5. The DBN layer has the same input/output interface
as an ordinary BN layer. Therefore, it can be plugged into most network structures whenever BN can
be adopted. Except for DBN layers, FedRBN can be extended to other dual normalization layers, for
instance, dual instance normalization Wang et al. (2020).
13
Under review as a conference paper at ICLR 2022
Figure 5: Illustration of the Dual-BN (DBN) layer and the copying operation for robustness propaga-
tion.
Algorithm 3 Train an adversarial noise detector
Input: Adversary A(∙), local validation dataset Dvai
1:	Da <— 0
2:	Set f to use clean BN by h — 0
3:	for mini-batch {(x, y)} in Dval do
4:	X — A(X)
5:	Da — Da ∪{(f(x), 0),(f(X),1)}
6:	Train a noise detector g(∙) on Da by minimizing the cross-entropy loss:
g = arg min E(l,y)∈Da [`(g(l), y)]
g
(5)
7:	Return noise detector g(∙)
Multiple-source propagation by weighted BN averaging. In FL with multiple AT users available,
we suggest a strategy to transfer robustness from multiple sources. Given N source BN statistics, we
use a weighted average to estimate the noise BN of a target ST user μ^r = PN αμ^t,si, where μ^t,si is
the estimated value from user Si by Eq.(4). Likewise, σ[ can be estimated. However, the difference
between the si-th adversarial distribution and the t-th counterpart is unknown. To tackle the issue, we
first present the following result:
Lemma B.1 (Informal). Suppose the the divergence between any data distribution D and its ad-
versarial distribution D is bounded, i.e., dH∆H (D, D) ≤ d where dH∆H is H∆H-divergence in
hypothesis space H. If a target model is formed by αi-weighted average of models from Dsi, the
summation i αidH∆H(Dsi , Dt) of divergence between a set of source standard datasets {Dsi}
and the target adversarial dataset Dt weighted by αi upper-bounds the generalization error of the
target adversarial data distribution Dt.
The lemma extends an existing bound for federated domain adaptation (Peng et al., 2019b), and
shows that the generalization error on the unseen target noised distribution Dt is bounded by the
αi -weighted distribution gaps. Motivated by the analysis, we set αi to be reversely proportional to the
divergence between Dsi and Dt . Specifically, we use a layer-averaged RBF-kernel to approximate
the weight, i.e., αi = LL PL=Iexp -γrbfdlW (Ds, Dt)/pl , where pl is the number of channels in
layer l. The distribution discrepancy can be approximately measured by Wasserstein distance as
dW(Ds,Dt) = ∣∣μSi - μt∣∣2 + ∣∣σS电—σl∣∣2. We use a large Yrbf, i.e., 100 X maxipl, to contrast
the in-distribution and out-distribution difference. Lastly, we normalize αi such that Pi αi = 1. The
formal analysis can be found in Appendix B.1.
As described in Algorithm 2, we propose to fit a support vector machine (SVM) (Cortes & Vapnik,
1995), denoted as g(∙), with RBF kernels on the validation set of each user. At inference, we predict
h in Eq. (3) by h = g (f (x)). The use of RBF kernels is partially inspired by (Feinman et al., 2017),
14
Under review as a conference paper at ICLR 2022
which used kernels on representations instead of logits. We use the most popular SVM instantiation,
libsvm, to implement the detector which will automatically sparsify the support samples. Thus, it can
successfully avoid the over-parameterization issue.
We are aware of efforts that eliminate the detected adversarial samples from test/inference, e.g., (Pang
et al., 2018). However, simply refusing to predict suspicious samples may break up the downstream
services, especially in the challenging scenarios when the detection becomes sensitive and many
samples are suspicious. Instead, our method detects and predicts the suspicious samples using the
robust model (with noise BN’s), and does not refuse any predictions.
The training and inference of noise detector. Introducing a noise detector results in an additional
training cost, but such overhead is marginal in practice. First, the training is only done once after
the network training is done. We only need to forward the whole network once for each sample to
obtain the logit required for training the noise detector. Therefore, the overall overhead for receiving
robustness is extremely efficient as compared to the AT overhead. Suppose adversarial samples are
collected at real time which could be private. Training a noise detector only requires approximately
a/T × 100% of the training adversarial samples where T is the number of epochs and a is the ratio
of validation set versus the training set.
B.1 Proof of Lemma B.1
In this section, we use the notation D for a dataset containing images and excluding labels. To
provide supervisions, we define a ground-truth labeling function g that returns the true labels given
images. So as for distribution D.
First, in Definition B.1, we define the H∆H-divergence that measures the discrepancy between two
distributions. Because the H∆H-divergence measures differences based on possible hypotheses (e.g.,
models), it can help relating model parameter differences and distribution shift.
Definition B.1. Given a hypothesis space H for input space X, the H-divergence between two
distributions D and D0 is dH (D, D0) , 2 supS∈S |PrD(S) - PrD0 (S)| where SH denotes the
collection of subsets of X that are the support of some hypothesis in H. The H∆H-divergence is
defined on the symmetric difference space H∆H，{f (x)㊉ h0(χ)∣h, h0 ∈ H} where ㊉ denotes the
XOR operation.
Then, we introduce Assumption B.1 to bound the distribution differences caused by adversarial noise.
The reason for introducing such an assumption is that the adversarial noise magnitude is bounded
and the resulting adversarial distribution should not differ from the original one too much. Since all
users are (or expected to be) noised by the same adversarial attacker Ae(∙) during training, We can
use d to universally bound the adversarial distributional differences for all users.
Assumption B.1. Let d be a non-negative constant governed by the adversarial magnitude .
For a distribution D, the divergence between D and its corresponding adversarial distribution
D , {Ae(x)∣x 〜D} is bounded as "hδh①,D) ≤ d&
-K T	1 ∙	1	.Λ	1 ∙	1~	Il 3	.Λ	. 1	∙ 1 1∙ , ∙1
Now, our goal is to analyze the generalization error of model ft on the target adversarial distribution
D, i.e., L(ft,Dt) = Ex〜D」|ft(x) - g(χ)∣]. Since We estimate ft by a weighted average, i.e.,
Pi aifsi where 鼠 is the robust model on Dsi, we can adapt the generalization error bound from
Peng et al. (2019b) for adversarial distributions. For consistency, we assume the AT users reside on
the source clean/noised domains while ST users reside on the target clean/noised domains. Without
loss of generality, we only consider one target domain and assume one user per domain.
Theorem B.1 (Restated from Theorem 2 in Peng et al. (2019b)). Let H be a hypothesis space of
V C -dimension d and {Dsi}iN=1, Dt be datasets induced by samples of size m drawn from {Dsi}iN=1
and Dt, respectively. Define the estimated hypothesis as ft ，PN=I aifsi. Then, ∀α ∈ RN,
PiN=1 αi = 1, with probability at least 1 - p over the choice of samples, for each f ∈ H,
N1
L(f, Dt) ≤ L(ft,D s) + Ei=I a 2d dH∆H(Dsi, Dt) + ξi∖ + C,	(6)
15
Under review as a conference paper at ICLR 2022
where C = 4 J2d log(2Nm+log^Tp), ξ% is the loss of the optimal hypothesis on the mixture of DSi
and Dt, and Ds is the mixture of all source samples with size Nm. dH∆H (Dsi, Dt) denotes the
divergence between domain si and t.
Based on Theorem B.1, we may choose a weighting strategy by ɑi H 1∕d,H∆H(Dsi, Dt). However,
the divergence cannot be estimated due to the lack of the target adversarial distribution D)t. Instead,
we provide a bound by clean-distribution divergence in Lemma B.2.
Lemma B.2 (Formal statement of Lemma B.1). Suppose Assumption B.1 holds. LetH be a hypothesis
space of V C -dimension d and {Dsi}iN=1, Dt be datasets induced by samples of size m drawn from
{Dsi}N=ι and Dt∙ Let an estimated target (robust) model be f = Ei aifsi where fsi is the robust
model trained on Dsi. Let D) S be the mixture of source samples from {D sJN=ι∙ Then, ∀α ∈ RN,
PiN=1 αi = 1, with probability at least 1 - p over the choice of samples, for each f ∈ H, the
following inequality holds:
N1
L(f, Dt) ≤ L(ft,D S) + de + y^i 1 αi(2 dH∆H(Dsi, Dt) + ξi) + C,
where C and ξi are defined in Theorem B.1. Ds is the mixture of all source samples with size Nm.
dH∆H (Dsi, Dt) is the divergence over clean distributions.
Proof. Notice that Eq. (6) is a loose bound as dH∆H (Dsi, Dt) is neither bounded nor predictable.
Differently, dH∆H(Dsi, Dt) can be estimated by clean samples which is available for all users. Thus,
we can bound dH∆H(Dsi, Dt) with dH∆H(Dsi, Dt). By Assumption B.1, it is easy to attain
dH∆H(Dsi, Dt) ≤ dH∆H(Dsi, Dsi) + dH∆H(Dsi, Dt) + dH∆H(Dt, Dt)
≤ 2d + dH∆H (Dsi, Dt),	(7)
where we used the triangle inequality in the space measured by dH∆H(∙, ∙). Substitute Eq. (7) into
Eq. (6), and we finish the proof.	□
TT	ɪʌ C	t ∙	ι.ι 1	1 i'	z»_n/z ι∙ι ι	ι ∙	,	, ∙	, ι ι .ι
In Lemma B.2, we discussed the bound for a f ∈ H (which also generalize to ft) estimated by the
linear combination of {fsi}i. In our algorithm, ft and fsi both represent the models with noise BN
layers, and they only differ by the BN layers. Therefore, Lemma B.2 guides us to re-weight BN
parameters according to the domain differences. Specifically, we should upweight BN statistics from
user si if dH∆H (Dsi, Dt) is large, vice versa. Since dH∆H (Dsi, Dt) is hard to estimate, we may use
the divergence over empirical distributions, i.e., dH∆H(Dsi, Dt) instead.
C Additional Empirical Study Results
In this section, we provide more details about our experiments and additional evaluation results.
C.1 Experiment details
Data. By default, we use 30% data of Digits for training. Datasets for all domains are truncated to
the same size following the minimal one. In addition, we leave out 50% (60% for DomainNet) of
the training set for validation for Digits. Test sets are preset according to the benchmarks in Li et al.
(2020c). Models are selected according to the validation accuracy. To be efficient, we validate robust
users with RA while non-robust users with SA. We use a large ratio of the training set for validation,
because the very limited sample size for each user will result in biased validation accuracy. When
selecting a subset of domains for AT users, we select the first n domains by the order: (MN, SV, US,
SY, MM) for Digits, and (R, C, I, P, Q, S) for DomainNet. Some samples are plotted in Fig. 6 to
show the visual difference between domains.
Hyper-parameters. We use a fixed parameters tuned by the DIGITS dataset and adopt it also for the
DOMAINNET dataset: λ = 0.5, C = 10 and γ = 1/10. The same tuning strategy is applied for other
baseline parameters.
16
Under review as a conference paper at ICLR 2022
Figure 6: Visualization of samples.
Table 7:	Network architecture for Digits dataset.
Layer ∣
Details
feature extractor
conv1
bn1
conv2
bn2
conv3
bn3
Conv2D(64, kernel size=5, stride=1, padding=2)
DBN2D, RELU, MaxPool2D(kernel size=2, stride=2)
Conv2D(64, kernel size=5, stride=1, padding=2)
DBN2D, ReLU, MaxPool2D(kernel size=2, stride=2)
Conv2D(128, kernel size=5, stride=1, padding=2)
DBN2D, ReLU
classifier
fc1	FC(2048)
bn4	DBN2D, ReLU
fc2	FC(512)
bn5	DBN1D, ReLU
fc3	FC(10)
Network architectures for DIGITS and DOMAINNET are listed in Tables 7 and 8. For the convolu-
tional layer (Conv2D or Conv1D), the first argument is the number channel. For a fully connected
layer (FC), we list the number of hidden units as the first argument.
Training. Following Li et al. (2020c), we conduct federated learning with 1 local epoch and batch
size 32, which means users will train multiple iterations and communicate less frequently. Without
specification, we let all users participant in the federated training at each round. Input images are
resized to 256 × 256 for DOMAINNET and 28 × 28 for DIGITS. SGD (Stochastic Gradient Descent)
is utilized to optimize models locally with a constant learning rate 10-2. Models are trained for 300
epochs by default. For FedMeta, we use the 0.001 learning rate for the meta-gradient descent and
0.02 for normal gradient descent following the published codes from Dinh et al. (2020). We fine-tune
the parameters for DomainNet such that the model can converge fast. FedMeta converges slower
than other methods, as it uses half of the batches to do the one-step meta-adaptation. We do not
let FedMeta fully converge since we have to limit the total FLOPs for a fair comparison. FedRob
fails to converge because locally estimated affine mapping is less stable with the large distribution
discrepancy.
Unequal users’ data sample. We assume a user samples a variable ratio of data, which follows
a Dirichlet distribution. We plot the different sample sizes for users in Fig. 8. Due to the varying
dataset sizes, we let each user run a fixed number of iterations which is calculated by the average
number of the per-epoch iterations of all users.
We implement our algorithm and baselines by PyTorch. The FLOPs are computed by thop package
in which the FLOPs of common network layers are predefined 1 . Then we compute the times of
forwarding (inference) and backward (gradient computing) in training. Accordingly, we compute the
total FLOPs of the algorithm. Because most other computation costs are relatively minor compared
to the network forward/backward, these costs are ignored in our reported results.
1Retrieve	the thop python package from https://github.com/Lyken17/
pytorch-OpCounter.
17
Under review as a conference paper at ICLR 2022
Table 8:	Network architecture for DomainNet dataset.
Layer ∣	Details
feature extractor
conv1
bn1
conv2
bn2
conv3
bn3
conv4
bn4
conv5
bn5
avgpool
Conv2D(64, kernel size=11, stride=4, padding=2)
DBN2D, ReLU, MaxPool2d(kernel size=3, stride=2)
Conv2D(192, kernel size=5, stride=1, padding=2)
DBN2D, ReLU, MaxPool2d(kernel size=3, stride=2)
Conv2D(384, kernel size=3, stride=1, padding=1)
DBN2D, ReLU
Conv2D(256, kernel size=3, stride=1, padding=1)
DBN2D, ReLU
Conv2D(256, kernel size=3, stride=1, padding=1)
DBN2D, ReLU, MaxPool2d(kernel size=3, stride=2)
AdaptiveAvgPool2d(6, 6)
classifier
fc1
bn6
fc2
bn7
fc3
FC(4096)
DBN1D, ReLU
FC(4096)
DBN1D, ReLU
FC(10)
Figure 7: The convergence curves and parameters sensitivity of λ, C and γ. C is for regularization
and γ is for RBF-kernel used in SVM whose performance is evaluated on Digits domains.
FATAvg
FATBN
FATProx
FedRBN
C.2 Convergence and hyper-parameters
Convergence. The first plot in Fig. 7 shows convergence
curves of different competing algorithms. Since FedRBN only
differs from FATBN by a DBN structure, FATBN and FedRBN
have similar convergence rates that are faster than others. We
see that FedRBN converges even faster than FATBN. A pos-
sible reason is that DBN decouples the normal and adversarial
samples, the representations after BN layers will be more con-
sistently distributed among non-iid users.
Parameter Sensitivity of the λ, C and γ. The second plot in
Fig. 7 shows a preferred λ is neither too small or too close to 1.
Since λ is critical when heterogeneity is severer, we evaluate
the sensitivity as only one domain (MNIST in Digits) is
adversarially trained. We find that a larger λ is more helpful
for the RA, as the estimation is closer to the true robust one.
The rest plots in Fig. 7 demonstrate the stability of the noise
detector when a choice of C = 10 and γ = 1/10 for SVM
generally works.
Figure 8: Dataset sizes for users
when the global seed is set as 1.
Larger circles indicate more train-
ing samples. The x-axis represents
the user index.
C.3 Impact of data size and validation ratio
To investigate the impact of data size, we conduct experiments with varying training dataset sizes
and validation ratios. Experiments follow previous protocols on the Digits dataset. Following the
training/testing split in Li et al. (2020c), we first sample a percentage of data for training. From the
training set, we randomly select a subset for validation. We denote the two subsampling ratios as
training percentage and validation ratio, respectively. When varying the training
percentage, we fix the validation ratio at 10%. When varying the validation ratio, we use 30%
18
Under review as a conference paper at ICLR 2022
training data. As shown in Fig. 9a, more training samples can improve the robustness and our method
outperforms baselines consistently. In Fig. 9b, the ratio of validation set is less influential for the
robustness performance of our FedRBN, but a larger validation ratio can reduce the time complexity
of training as less samples are used for gradient computation. Though baseline methods obtain higher
robust accuracies with smaller validation ratios, our method still introduces large gains in all cases.
(a) Varying the size of training set.
(b) Varying the ratio of validation data in training set.
Figure 9: Experiments with varying data size.
C.4 Logits of adversarial and clean examples
Visualization of logits by t-SNE is presented in Fig. 11 (DIGITS) and Fig. 10 (DOMAINNET). We
generally observe that the clean and adversarial logits are separable with generalizable decision
boundaries. Moreover, MNIST and USPS domains turn out to be the most separable cases as they
are easier classification tasks compared to the rest ones. Though some domains have a few mixed
samples in the visualization, their noise detection accuracies are mostly higher than 90%. Thus, it is
rational to fit a noise detector on the validation set for helping BN selection at test time.
DomaInNet-ClIpart
Figure 10: Logits of standard-trained models visualized by t-SNE on DomainNet.
C.5 Results on the Office-Caltech 1 0 Dataset
Following the same setting as Do-
mainNet experiments, we ex-
tend our experiments to a smaller
dataset, Office-Caltech10 dataset pre-
processed by Li et al. (2020c) with
images acquired by different cam-
eras. The dataset includes 4 domains:
Amazon, Caltech, DSLR, Webcam.
Because the dataset has very few sam-
ples, we only generate 2 users per do-
main such that each user has at least
Table 9: Comparison to baselines on the Office-Caltech10
dataset. Standard deviations are reported in brackets.
AT users metric	Amazon		All	
	RA	SA	RA	SA
FedRBN (ours)	9.2 (3.4)	62.9 (3.4)	29.1 (2.4)	68.7 (1.7)
FedBN	5.1 (1.1)	65.9 (2.4)	30.8 (2.5)	67.2 (2.1)
FedAvg	0.6 (0.5)	54.7 (3.8)	13.3 (2.3)	56.0 (2.6)
FedProx	0.6 (0.6)	55.3 (4.7)	13.6 (1.8)	56.2 (2.1)
19
Under review as a conference paper at ICLR 2022
□ Igits-mnist
DIgfcs-SVhn
Figure 11: Logits of standard-trained models visualized by t-SNE on Digits.
100 samples. In Table 9, we see that our method outperforms baselines as only one domain is
adversarially trained. As the training set is rather small, the RAs are generally worse than the ones on
Digits or DomainNet.
Table 10: Comparison to robustness transferring by fine-tuning (FT).
		# FT iterations	# freeze layers	RA	SA
FedRBN	-	0	53.1	84.4
FedAvg	-	0	44.7	85.7
FedAvg+FT	200	0	39.2	83.6
FedAvg+FT	200	3	31.6	78.2
FedAvg+FT	200	4	29.8	74.7
FedAvg+FT	200	5	31.5	66.1
FedAvg+FT	100	0	40.6	83.4
FedAvg+FT	100	3	32.0	77.5
FedAvg+FT	100	4	31.5	72.9
FedAvg+FT	100	5	31.5	64.5
FedAvg+FT	20	0	40.6	79.6
FedAvg+FT	20	3	33.4	73.8
FedAvg+FT	20	4	31.9	66.8
FedAvg+FT	20	5	31.9	62.2
C.6 Comparison to robustness transferring by fine-tuning
As an alternative to FRP, fine-tuning (FT) the federated-trained models on target users can enjoy even
better efficiency than FedRBN. Here, we first train AT users by FedAvg for 300 epochs. Note that we
do not adopt FedBN because FedBN will not output a single model for adapting to new users. Then,
the model is used for initializing the models for ST users. These ST users will be trained by FedAvg
for a given number of FT iterations. Still, we adopt the 20% 3/5 domain FRP setting on the DIGITS
dataset. In Table 10, we see that such a fine-tuning does not improve the robustness (RA).
C.7 Experiments in Fig. 1b
Though the results in Fig. 1b have been reported in previous experiments, we re-summarize the
results in Table 11 for ease of reading. The basic setting follows the previous experiments on the
Digits dataset. We construct different portions of AT users by in-domain or out-domain propagation
settings. When robustness is propagated in domains, we sample AT users in each domain by the
same portion and leave the rest as ST users. When robustness is propagated out of domains, all users
from the last two domains will not be adversarially trained and gain robustness from other domains.
Concretely, we add the FedRBN without copy propagation (FedRBN w/o prop) in the table, to
show the propagation effect. FedRBN w/o prop outperforms the baselines only when the AT-user
portion is more than 60%. Meanwhile, due to the lack of copy propagation, the RA is much worse
20
Under review as a conference paper at ICLR 2022
than the propagated FedRBN. Unless no AT user presents in the federated learning, FedRBN always
outperforms baselines.
Table 11: Results and detailed configurations of Fig. 1b on the 5-domain Digits dataset. FedAvg and
FedBN corresponds to FATAvg and FATBN in the figure.
AT-user ratio	propagation	method	RA	SA	# AT domain	per-domain AT ratio
0%	none	FedRBN (ours)	32.1	84.3	0	0.0
		FedRBN w/o prop	32.1	84.3	0	0.0
		FedAvg	35.3	82.0	0	0.0
		FedBN	32.1	84.3	0	0.0
12%	mix	FedRBN (ours)	55.1	84.6	3	20%
		FedRBN w/o prop	38.7	84.5	3	20%
		FedAvg	44.1	84.1	3	20%
		FedBN	42.4	86.0	3	20%
20%	in-domain	FedRBN (ours)	57.3	85.3	5	20%
		FedRBN w/o prop	46.1	86.1	5	20%
		FedAvg	45.9	84.7	5	20%
		FedBN	44.8	86.0	5	20%
60%	out-domain	FedRBN (ours)	61.6	85.0	3	100%
		FedRBN w/o prop	56.2	85.5	3	100%
		FedAvg	52.0	84.2	3	100%
		FedBN	53.0	85.5	3	100%
100%	none	FedRBN (ours)	65.7	85.9	5	100%
		FedRBN w/o prop	65.8	85.9	5	100%
		FedAvg	57.5	84.7	5	100%
		FedBN	59.1	85.9	5	100%
C.8 Joint attack on noise detector and model prediction
Furthermore, to check if the adversarial attacker can bypass the noise detector, we use PGD to jointly
maximize the sum of the cross-entropy losses of the noise detector and the inference model. We
show the results on the Digits and DomainNet dataset following the settings of Table 2. For ease of
comparison, we also include baseline results from Table 2. As observed in Table 12, our method
FedRBN outperforms baselines consistently in all benchmarks.
To show the JointPGD does fool the noise detectors, we present the detection accuracy of adversary
samples in Table 13. The detectors of FedRBN have lower accuracies due to the joint attack, compared
to FedRBN under PGD attack.
Table 12: The robust accuracy of FedRBN under joint PGD attacks. Baseline results are present with
PGD attacks. Standard deviations are included in brackets.
AT users	Digits			DomainNet		
	All	20%	MNIST	All	20%	Real
FedRBN (ours)	61.4 (0.5)	50.6 (1.3)	41.8(1.6)	30.2 (0.3)	23.7 (0.8)	17.3 (1.2)
FATBN	60.0	41.2	36.5	29.9	20.3	11.3
FATAvg	58.3	42.6	38.4	24.6	15.4	9.4
FATProx	58.5	42.6	38.1	24.8	14.5	9.4
21
Under review as a conference paper at ICLR 2022
Table 13: Detection accuracy by PGD and joint PGD
attacks.
Attack	Digits			DomainNet		
	All	20%	MNIST	All	20%	Real
PGD	79.2	81.4	79.5	56.2	63.8	65.8
Joint PGD	63.9	56.8	52.3	56.2	63.8	65.8
Table 14: Evaluation with different FL
configurations
B	E	method	RA	SA
10	1	FATBN	50.9	83.9
	1	FedRBN	60.0	82.8
10	4	FATBN	42.0	75.8
	4	FedRBN	56.3	76.1
10	8	FATBN	30.9	63.1
	8	FedRBN	53.4	68.4
50	1	FATBN	37.0	85.8
	1	FedRBN	53.2	84.5
100	1	FATBN	35.7	85.3
	1	FedRBN	53.0	83.8
22
Under review as a conference paper at ICLR 2022
D Additional Experiments for Rebuttal
D. 1 Partial participants
In reality, we can not expect that all users are available for training in each round. Therefore, it
is important to evaluate the federated performance when only a few users can contribute to the
learning. To simulate the scenario, we uniformly sample a number of users without replacement per
communication round. Only these users will train and upload models. In Fig. 12, RA and SA are
reported against the number of selected users. We observe that SA is barely affected by the partial
involvement, while RA increases by fewer users per round. Since the actual update steps in the view
of the global server are reduced with lower contact ratios, the result is consistent with Table 14, where
smaller batch sizes or fewer local steps lead to better robustness.
#USer per round
O 25	50	75	100 125 150 175 200
epoch
Figure 12: Vary the number of involved users per communication round. The validation accuracy is
computed by averaging users’ accuracy. For AT users, the RA is used while SA is used for ST users.
D.2 Scalability with more users
Since our method has the similar training/communication strategy as FATBN or FATAvg (except
switching and copying BN which are quite lightweight), the federation of FedRBN and its complexity
scale up to more users like FATBN or FATAvg who are widely used scalable implementations. To
empirically evaluate the scalability of our method versus FATBN and FATAvg, we experiment with
more clients given the Digits dataset. With the same total training samples, we re-distribute the data
to different numbers of clients in a non-uniform manner. In Fig. 13, we evaluate the RA and SA
by increasing the total number of users, including 25, 50, 150, 200. In each communication round,
50% randomly selected users will upload their trained models. The trend shows that both RA and
SA will be lower when samples are distributed to more clients. Despite the degradation, our method
maintains advantages in RA consistently.
In Fig. 13, we also demonstrate that
our method converges faster than
baselines either with fewer or more
users. The validation accuracy is
computed by averaging users’ accu-
racy when RA is used for AT users
and SA for ST users. As observed,
when data are more concentrated in
a few users (i.e. smaller numbers of
users), the convergence will be faster.
The result is natural for most non-iid
federated learning problems. For ex-
ample, Li et al. (2020b) proved that
more clients will result in worse final
losses and slower convergence.
Figure 13: Robustness and accuracy by the increasing total
number of users as 25, 50, 150, and 200. The larger scatter in
the left figure indicates more users.
D.3 More correlation plots of BN statistics
We provide more correlation plots of BN statistics in Fig. 14 as supplementary to Fig. 2b. In most
layers, the cross-domain difference of BN statistics is linearly correlated between clean and robust
23
Under review as a conference paper at ICLR 2022
ones. The linear correlation is weak in some shallow layers. One reason is that the coupling of
adversarial noise and data undermine the noise-independence assumption made for Eq. (4). The
decoupling is further discussed in Appendix D.5.
Figure 14: Correlation of statistic differences of mean (top) and log-variance (bottom) in BN layers.
Table 15: Robustness propagation using ResNet18.
D.4 Experiments with ResNet
Like Table 2, we conduct the same Domain- Net experiments but using ResNet18 (He	AT users Metrics	All		20%		Real	
		RA	SA	RA	SA	RA	SA
et al., 2016) in place of AlexNet. Most con-	FedRBN (ours)	59.1	61.6	49.6	63.1	42.9	56.4
figurations are the same but we use a cosine-	FATBN	37.3	60.4	25.5	62.8	13.6	57.4
annealing schedule of the learning rate from	FATAvg	46.6	57.1	36.5	61.6	18.8	49.0
0.05 to 0 within 600 epochs. Compared to
AlexNet, ResNet18 is significantly more robust in all three tasks. Consistent with the AlexNet-based
results, our method outperforms the two best baselines.
D.5 When does debiasing work?
On deriving Eq. (4), we made an assumption
of the independence of adversarial noise. By
investigating how the assumption holds up
in real datasets, we want to know when the
debiasing strategy could improve the robust
accuracy. For this purpose, we leverage Pear-
son coefficients (P) to estimate how depen-
dent the noise is on the input images. For
each user in the Digits dataset, we calcu-
late R(μr - μ) = LL PL=I(P(〃r-μι,μI) +
P(μr - μr,μr))/2, where μι, μ( are the
statistics of the layer l ∈ {1, . . . , L}. The
experiment protocol follows the 100% 1/5
domain setting in Table 1.
As reported in Fig. 15, we find that the
dependence weakens by layer and the RA
improvement brought by debiasing is aug-
Figure 15: Evaluation of the noise-independence as-
sumption (left two) and its effects on users’ RA im-
provement from debiasing (right two). Estimated by
the Pearson coefficient, R presents the degree to which
noise statistics (e.g., μr - μ) are correlated to data
statistics (e.g., μ).
4u9UJ9A0」dE- S
2.0-R β
mented by lower dependence, i.e., R. The phenomenon is related to the nature of adversarial noise.
Since adversarial noise is inherently so subtle to be decoupled from noised images, the linear modeling
of batch-normalization in shallow layers is less likely to extract the noise (for example, by μ — μι).
In deeper layers, the noised and clean features deviates from each other, for example, in the logit
layers (see Fig. 11). Therefore, the noise correlation is weaker for the noise-independence assumption
to hold better and our debiasing method is more effective in improving propagated robustness.
24