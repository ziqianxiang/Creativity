Under review as a conference paper at ICLR 2022
Spectral Multiplicity Entails Sample-wise
Multiple Descent
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the generalization risk of ridge and ridgeless linear regres-
sion. We assume that the data features follow a multivariate normal distribution
and that the spectrum of the covariance matrix consists of a given set of eigenval-
ues of proportionally growing multiplicity. We characterize the limiting bias and
variance when the dimension and the number of training samples tend to infinity
proportionally. Exact formulae for the bias and variance are derived using the ran-
dom matrix theory and convex Gaussian min-max theorem. Based on these formu-
lae, we study the sample-wise multiple descent phenomenon of the generalization
risk curve, i.e., with more data, the generalization risk can be non-monotone, and
specifically, can increase and then decrease multiple times with more training data
samples. We prove that sample-wise multiple descent occurs when the spectrum
of the covariance matrix is highly ill-conditioned. We also present numerical re-
sults to confirm the values of the bias and variance predicted by our theory and
illustrate the multiple descent of the generalization risk curve. Moreover, we the-
oretically show that the ridge estimator with optimal regularization can result in a
monotone generalization risk curve and thereby eliminate multiple descent under
some assumptions.
1	Introduction
The double/multiple descent phenomenon attracted recent research attention due to (Belkin et al.,
2019). This line of work focuses on the parameter-wise double/multiple descent phenomenon of
the risk curve (Bartlett et al., 2020; Tsigler & Bartlett, 2020; Belkin et al., 2019; 2020; Chen et al.,
2020a; Liang et al., 2020; Advani et al., 2020; Bos & Opper, 1998; Krogh & Hertz, 1992; Le CUn
et al., 1991; Mei & Montanari, 2019; Opper et al., 1990; Vallet et al., 1989; Watkin et al., 1993). The
classical learning theory shows that when the nUmber of parameters (which reflects the model com-
plexity) increases, the test error (generalization risk) first decreases dUe to more fitting power, and
then increases dUe to overfitting. The generalization risk attains a peak at the interpolation threshold
(the nUmber of parameters eqUals the nUmber of data points so that the model interpolates the data).
This resUlts in a U-shaped risk cUrve if we plot the test error versUs the nUmber of parameters. The
doUble descent risk cUrve posits that the risk will decrease (again) if one fUrther increases the model
complexity beyond the interpolation threshold (Belkin et al., 2019). ThUs there is a second descent
in addition to the first one in the U-shaped stage of the cUrve. Belkin et al. (2019) presented empiri-
cal resUlts and showed the existence of sUch doUble descent behavior in the random FoUrier featUres
model, the fUlly connected neUral network, and the random forest model. Prior to (Belkin et al.,
2019), earlier stUdies of the shape and featUres of the risk cUrve in a nUmber of contexts inclUde
(Vallet et al., 1989; Opper et al., 1990; Le Cun et al., 1991; Krogh & Hertz, 1992; Bos & Opper,
1998; Watkin et al., 1993; Advani et al., 2020). Loog et al. (2020) presented a prehistory of the
double descent phenomenon. Belkin et al. (2020) proved the double descent curve in the Gaussian
model and the Fourier series model. Mei & Montanari (2019) theoretically established the double
descent curve of the random features regression. Bartlett et al. (2020); Tsigler & Bartlett (2020)
characterized the conditions for ridgeless and ridge linear regression problems, respectively, under
which the minimum-norm interpolants achieve near-optimal generalization risk. Liang et al. (2020)
showed that the test error of the minimum-norm interpolator of data in reproducing kernel Hilbert
space is upper bounded by a multiple descent curve as the model complexity increases. They also
presented a numerical result supporting that the test error itself exhibits a multiple descent curve.
1
Under review as a conference paper at ICLR 2022
Chen et al. (2020a) proved that the multiple descent curve does exist for the minimum-norm inter-
polator in linear regression and that the curve can be even designed.
Following the parameter-wise double descent, research interest extended to epoch-wise and sample-
wise double descent (Nakkiran et al., 2020; Chen et al., 2020b; Min et al., 2021; Nakkiran et al.,
2021). Nakkiran et al. (2020) observed from their numerical result that the generalization risk ex-
periences a double descent as one keeps the model size fixed and increases the training time. They
called this observation epoch-wise double descent. Nakkiran et al. (2020) also noted sample-wise
non-monotonicity, which means that more data can hurt generalization. Nakkiran et al. (2021)
proved that for isotropic features, optimally regularized ridge regression yields a monotonic gener-
alization risk curve with more samples. Nakkiran et al. (2021) also showed that if the features are
formed by projecting high-dimensional isotropic data to a random low-dimensional space (say, d-
dimensional), the optimally regularized ridge regression has a monotonic generalization risk curve
with increasing d (the model size). Sample-wise non-monotonicity and double descent was also
observed in (Chen et al., 2020b; Min et al., 2021) in adversarially trained models. C ompared to
(Wu & Xu, 2020; ichi Amari et al., 2021; Dobriban & Wager, 2018; Richards et al., 2021), in what
follows, we highlight our contributions and the differences from them. First, our major contribu-
tion is providing a rigorous proof for the existence of sample-wise (test error vs. the number of
training samples) double and multiple descent in linear regression. However, (Richards et al., 2021)
only mentioned parameter-wise double descent (test error vs. model capacity) in their related work.
(ichi Amari et al., 2021) only mentioned epoch-wise (test error vs. training time) double descent
in Appendix A.2. Neither (Richards et al., 2021) nor (ichi Amari et al., 2021) mentioned multiple
descent. Second, we made and theoretically proved the observation that an ill-conditioned covari-
ance matrix is a sufficient condition for the existence of sample-wise multiple descent. To the best
of our knowledge, our work is the first paper that pointed this out. Third, we solved the Stieltjes
transform explicitly and derived explicit formulae for the risk and variance in our setup. In addition,
we also provided rigorous treatment to the ridgeless setting and also obtained explicit formulae for
it. Fourth, there is another difference between our paper and the papers that the reviewer mentioned.
(Wu & Xu, 2020; ichi Amari et al., 2021; Dobriban & Wager, 2018; Richards et al., 2021) assumed
a prior on the true linear model and takes expectation over the prior. In our paper, we do not assume
a prior on the true linear model and our risk does not take the expectation over a random true linear
model.
In the setting of generally anisotropic features, this paper gives an asymptotic characterization of the
generalization risk curve with more samples. The asymptotic regime is an approximation for large
n, d and can also shed light on practical machine learning problems. We first introduce our problem
setup.
1.1	Problem Setup
Data Distribution Let Σ ∈ Rd×d be a positive semi-definite matrix which is termed the covari-
ance matrix, and let θ* ∈ Rd. The eigenvalues of Σ are λι,..., λm with multiplicity di,... ,dm,
respectively. We have d = Pim=1 di. Assume that λ1, . . . , λm are fixed, distinct, all positive, and
do not depend on d (i.e., for all d, the eigenvalue of Σ are always λ1, . . . , λm). We assume the
following data distribution D for (x, y) ∈ Rd × R:
X 〜N(0, Σ), y = x>θ* + e ,
where X and e are independent and e 〜 N(0, σ2). In practice, there are natural random variables
x that satisfy our assumption. For example, assume that we want to use machine A to measure the
length of several objects and use machine B to measure their temperature. The measured lengths
and temperatures follow an i.i.d. Gaussian distribution. However, the variance of measurement of
machine A is different from that of machine B. Then we consider the random vector formed by the
measurements X = (l1, . . . , ln, t1, . . . , tn), where li and ti are the length and temperature of object i,
respectively. This results in a block-structured covariance matrix. When we measure more objects,
the size of the covariance matrix tends to infinity. Second, the motivation came from (Nakkiran
et al., 2021). (Nakkiran et al., 2021) observed empirically in their Figure 2 that when the covariance
matrix has a block structure (specifically, there are only two fixed different eigenvalues 10 and 1),
the expected excess risk exhibits multiple descent. We quantitatively studied this observation and
2
Under review as a conference paper at ICLR 2022
obtained the related formulae. The excess risk of an estimator θ ∈ Rd is given by
R(θ) = Eχ,y〜D [(y - x>θ)2 - (y - x>θ*)2].
Assume that the training data {(xi, yi)}in=1 ⊆ Rd × R is drawn i.i.d. from D. Write
(χ> ∖	( yι ∖
X =	: I ∈ Rn×d, y =	. I ∈ Rn.	(1)
x.n>	y.n
We have y = Xθ* + e, where e 〜N(0, σ2In).
Ridge Estimator and Minimum-Norm Estimator
Definition 1 (Ridge estimator). The ridge estimator θλn,d ∈ Rd (λ > 0) solves the following
minimization problem
mRd n kxθ -yk2+λ kθk2.
Definition 2 (Minimum-norm estimator). The minimum-norm estimator (also known as the ridge-
less estimator) θon,d ∈ Rd solves the following minimization problem
min kθk2 such that kXθ - yk2 = min kX θ - yk2 .
θ∈Rd	2	2 θ∈Rd	2
-c-r τ	∙	. 1 ∙ , 1	, 1	∙ 1 1' A	1 ∙ 1 ∙	1
We are interested in the expected excess risk of θλ,n,d, which is given by
Rλ,n,d = E hR (θλ,n,dj .
The expectation is taken over the randomness of the training data {(xi, yi)}in=1.
Asymptotic Regime Let Πi ∈ Rd×d be the orthogonal projection to the eigenspace of λi . This
paper focuses on the asymptotic behavior of the expected excess risk of θλ,n,d where n, di → +∞,
d；In → Zi (Zi is a fixed positive constant), and ∣∣∏iθ* |卜 → 力.In other words, We are interested in
lim	Rλ,n,d .
n,di →+∞
di /n→zi
k∏iθ*k2→η
1.2 Our Contributions
Our contributions are summarized as follows.
1.	We obtain the formulae for the limiting bias and variance, and thereby the limiting risk.
We use two methods to obtain these formulae. Specifically, we obtain the limiting bias and
variance by solving the Stieltjes transform and computing its derivatives and antideriva-
tives. We also use convex Gaussian min-max theorem (CGMT) (Thrampoulidis et al.,
2015) to compute the limiting variance. The advantage of the CGMT method is that it is
more mathematically tractable for the ridgeless estimator. Through the CGMT approach,
we obtain a closed-form formula for the variance in the underparameterized regime and
simplify the formula for the variance in the overparameterized regime. Moreover, based on
the simplified formula, we deduce a closed-form expression for the variance if the covari-
ance matrix of the data distribution has two different eigenvalues.
2.	We find and theoretically prove that sample-wise multiple descent happens when the co-
variance matrix has eigenvalues of very different orders of magnitude (thus the covariance
matrix is highly ill-conditioned).
3.	We show that if the true linear model θ* satisfies ∣∣∏iθ*∣∣2 = di^o, optimal regulariza-
tion (i.e., pick λ that minimizes the generalization risk of θλ,n,d) results in a monotone
generalization risk curve—in other words, with optimal regularization, more data samples
3
Under review as a conference paper at ICLR 2022
always improve generalization. Thus there is no sample-wise double or multiple descent.
This provides a theoretical proof of a phenomenon observed in (Nakkiran et al., 2021) that
optimal regularization can mitigate double descent for anisotropic data. Note that without
regularization, there will be a blow-up in expected excess risk when n = d (the linear
model exactly interpolates the data) and therefore, there is no samplewise descent across
the under- and over-parameterized regimes.
2 Preliminaries
Notation Write [m] for {1, 2, . . . , m}. Let i denote the imaginary unit. If x ∈ Rn and Σ ∈ Rn×n
is a positive Semidefinite matrix, write ∣∣xk∑，√x>Σx. For a vector x, let ∣∣∙∣k and ∣∣∙k2 denote
the `1 and `2 norm, respectively. Let denote the Hadamard (entry-wise) product between vectors.
Write k ∙ ∣∣2 and k ∙ ∣∣f for the spectral matrix norm and FrobeniUs matrix norm, respectively. Let
4 denotes the Loewner order. For two square matrices A and B of the same size, write A 4 B
if B - A is positive semidefinite. Define spec (A) as the set of all eigenvalUes of A. Let O(d) =
{A ∈ Rd×d | AA> = A>A = Id} denote the set of d X d orthogonal matrices. Define Sd-1(r)，
{x ∈ Rd | ∣x∣2 = r}. Denote almost sUre convergence by →a.s., and convergence in probability plim
and →P .
Ridge Estimator and Minimum-Norm Estimator We begin with the eqUivalent characteriza-
tions of the ridge and minimUm-norm estimator. An eqUivalent characterization of the ridge estima-
tor θλ,n,d is
θλ,n,d = (X>X + λnId)-1 X>y = X> (λnIn + XX>)-1 y .	(2)
The second equality in Equation (2) is because of the Sherman-Morrison-Woodbury formula. A
proof of EqUation (2) can be foUnd in (Tsigler & Bartlett, 2020).
An equivalent definition of the minimum-norm estimator θo,n,d
minimization problem
is that θo,n,d
solves the following
min ∣θ∣2 such that X>Xθ = X>y .
Thus we have
θo,n,d = (X>X)+ X>y = X> (XX>)+ y = X+y,
where A+ denotes the pseudo-inverse of A. The second and third equalities are because of the
identity X+ = (X>X)+ X> = X> (XX>) + . The minimum-norm estimator is the limit of the
ridge estimator θλ,n,d as λ → 0+:
θ0,n,d = lim θλ,n,d .
This is because of the identity	limλ→o+ (X >X + λnId) 1 X >	=
limλ→0+ X> (λnIn + XX>)-1 = X+.
Bias-Variance Decomposition of Expected Excess Risk We first show that the excess risk of an
estimator θ equals the norm of θ — θ*:
R⑹=E(χ,y)〜D [(y - χ>θ)2 - (y - x>θ*)i = Ex h(x> (θ* - θ))2i
=E [(θ* - θ)> Σ (θ* - θ)] = E h∣θ* - θ∣∑i .
For the ridge estimator, the expected excess risk is
Rλ,d,n =E [∣θ* - X>(nλIn + XX>)-1(Xθ* + e)k∑]
=E [k(Id - X>(nλIn + XX>)-1X)θ* - X>(nλIn + XX>)-1e∣∣∑]
=E [k(Id - X>(nλIn + XX>)-1X)θ*∣∣∑] + E h∣∣X>(nλIn + XX>)-屋|匕]
=E [k(Id - X>(nλIn + XX>)-1X)θ*k∑] + σ2Etr [XΣX>(nλIn + XX>)-2]
,Bλ,d,n + Vλ,d,n .	(3)
4
Under review as a conference paper at ICLR 2022
For the minimum-norm estimator, the expected excess risk is
Ro,d,n = E [kθ* — X +(Xθ* + e)k∑]
=E 1(Id - X+x) θ*- χ+e*i
=E hMId-X+X)叫 ∑i + E ”χ∕Σ]
=E [∣∣ (Id - X+X) θ* ∣∣∑i + σ2E tr [(X+)> ΣX+]
, B0,d,n + V0,d,n .	(4)
We call Bλ,d,n and B0,d,n the bias term, and call Vλ,d,n and V0,d,n the variance term. The bias and
variance for the minimum-norm estimator are the limit of their counterpart for the ridge estimator
as λ → 0+, i.e., limλ→0+ Bλ,d,n = B0,d,n and limλ→0+ Vλ,d,n = V0,d,n (this can be shown by
Lebesgue’s dominated convergence theorem, see our proof in Lemma 5 and Lemma 6, respectively).
3 Main Results
3.1 Limiting Risk and Sample-wise Multiple Descent
We study the limiting bias and variance for a linear regression problem in which the data distribution
follows a multivariate normal distribution, the spectrum of the covariance matrix exhibits a block
structure and tends to a discrete distribution. Thanks to the random matrix theory, we obtain the
formulae (presented in Theorem 1) for the limiting bias and variance, and thereby the total risk.
We use two methods to obtain these formulae. The first method is through the Stieltjes transform
of the matrix 1XX>. The central quantity for computing the limiting bias and variance through
the first method is the solution ρ* to the optimization problem Equation (5) in Item 1 of Theorem 1.
Item 1 guarantees the existence of a solution and determines its optimality condition Equation (6).
Item 2 computes the Jacobian matrix of ρ* with respect to λ% and provides a closed-form formula
to compute the Jacobian matrix. Equation (9) and Equation (10) in Item 4 give the formulae for
the limiting bias obtained by the first method. Equation (11) and Equation (12) give the limiting
variance.
The second method is through the convex Gaussian min-max theorem (CGMT) (Thrampoulidis
et al., 2015). The central quantity is the solution r* to the minimax optimization problem Equa-
tion (8) in Item 3. We use CGMT to obtain the formulae for the variance term. They are presented
in Equation (13) and Equation (14) in Item 4.
Theorem 1. The following statements hold:
1.
There exists a minimizer ρ ∈ R+m that solves
mm
λ+X λj Pj) + X (Pj-Zj(Iog zj+I)
j=1	j=1	zj
The minimizer ρ* satisfies
λ + Pm 1 λj Pj +1 - I =0 , ∀i ∈ m].
(5)
(6)
2.	Let Pj ∈ Rm be a minimizer of Equation (5) and J = d∂λ ∈ Rm ×m be the Jacobian
∂P
matrix Jij = ∂λ~. Then J Is g^ven by
J = (diag (λ) + (λ + λ>ρj) Im -(Z - Pj) λ>)	((Z - Pj) Pj> - diag (Pj))
and the matrix diag (λ) + λ + λ>Pj Im- (z - Pj) λ> is always invertible.
3.	Define r = (r1, . . . , rm), λ = (λ1, . . . , λm), and
”(rt, r,λ)
2rt /1 + X ri2 - 2rt X √ziri + X λ1 r2 - λr2
i∈[m]	i∈[m]	i∈[m]
(7)
5
Under review as a conference paper at ICLR 2022
For any Kt ≥ 2 and Ku ≥ "+(2+*), we have
max min 斤(rt, r, λ) = min max 斤(rt, r, λ) = max min 斤(rt, r, λ) = min max 斤(rt, r, λ)
0≤rt≤Kt0≤ri≤Ku	t	0≤ri≤Ku0≤rt≤Kt	t	rt≥0 ri≥0	t	ri≥0 rt≥0	t
(8)
and the above optimization problem has a solution.
4.	Let r* = (若,...,rm) solve Equation (8). Define q = (η2∕zι,..., η21 /zm)> and view
λ = (λ1, . . . , λm)> as a column vector. The limiting bias is given by
lim	Bλ,d,n =q>(λρ*+Jλ2),
n,di →+∞
di∕n→zi
k∏iθ*k2→η
lim	B0,d,n = lim q> (λ	ρ* + Jλ2) .
n,di→+∞	λ→0+
di∕n→zi
kΠi θ* k2 →ηi
The limiting variance is given by
lim	Vλ,d,n
n,di →+∞
di∕n→zi
2λ2>(ρ* + J>λ)
(λ + λ>ρ*)2
lim	V0,d,n = σ2 lim
n,di →+∞	λ→0+
di /n→zi
λθ2>(ρ* + J >λ)
(λ + λ>ρ*)2
lim	Vλ,d,n
n,di →+∞
di∕n→zi
m
σ2 X ri*2
i=1
lim	V0,d,n
n,di →+∞
di /n→zi
m
σ2 lim X ri*2
λ→0+	i
i=1
(9)
(10)
(11)
(12)
(13)
(14)
Figure 1 illustrates the theoretical and numerical values of the bias, variance, and total risk. We
observe a triple descent in Figure 1a where the covariance matrix has three blocks, and a quadruple
descent in Figure 1b where the covariance has four blocks. In the three-block example, we set λ3
λ2 λ1	(λ1 = 1, λ2 = 100, λ3 =	1000). In the four-block example, we set λ4 λ3 λ2 λ1
(λ1	= 1,	λ2 = 100, λ3 = 104, λ4	= 107). For the values of other parameters,	please	refer	to the
caption of Figure 1 Our findings provide an explanation for the occurrence of sample-wise multiple
descent: it occurs when the covariance matrix is highly ill-conditioned. Moreover, we find that the
generalization risk curve is continuous in ridge regression (λ > 0) while it blows up at n = d
in ridgeless regression (λ = 0). We can see the singularity (at n = d = 200) of the ridgeless
generalization risk curve in Figure 2a.
Following Theorem 1, we focus on the variance in the ridgeless case (λ = 0) and further study
the expressions in Equation (13) and Equation (14). We find that the variance exhibits sharply
different behaviors in the underparameterized and overparameterized regimes. Recall that we will
let n, di → +∞ and keep di/n → zi. Then d/n → Pi∈[m] zi. If lim d/n = Pi∈[m] zi > 1, we
are in the underparameterized regime. In this regime, the bias vanishes and therefore the risk equals
the variance. If lim d/n < 1, we are in the overparameterized regime.
Theorem 2. If d/n → Pi∈[m] zi > 1 and r* = (r1*, . . . , rm* ) solves
min ^X ɪr2 subject to
ri≥0i∈[m] i
JX Tr + 1
i∈[m]
E √ziri,
i∈[m]
then we have an optimality condition for r* :
r* _ λi	√ZiA* - r*
—=---------：---
Tj	λj	√zj A* - r*
i, j ∈ [m] ,
(15)
6
Under review as a conference paper at ICLR 2022
00500050
2 11
xsp03dx
50	100	150	200	250	300
-β- T. Bias (X = 0.01)
-6 t. Varl (λ = 0.01)
-o- T, Var2 (λ = 0.01)
-o- T, Risk ('=0.01)
N. Bias (λ = 0.01)
—N. Var (λ = 0.01)
-■-N. Risk (X = 0.01)
Number of Samples
(b) Sample-wise quadruple de-
scent
Number of Samples
(a) Sample-wise Triple Descent
Figure 1: Figure 1a and Figure 1b illustrate sample-wise triple and quadruple descent, respectively.
We specify the parameters that we used as follows. Figure 1a: There are 3 blocks. We set d1 = 60 ,
d2 = d3 = 40, λι = 1, λ2 = 100, λ3 = 1000, k∏1θ*∣∣2 = ∣∣Π3θ*k2 = 0.1 and ∣∣∏2θ*k2 = 1. The
three descents occur at n = 36, 80, 136, respectively. Figure 1b: There are 4 blocks. We set d1 =
d2 = d3 = d4 = 40, λι = 1, λ2 = 100, λ3 = 104, λ4 = 107, and ∣∣∏iθ* ∣2 = 0.01(i ∈ [4]). The
four descents occur at around n = 1, 37, 80, 120, 150, respectively. In the legend, the items starting
with “T.” are theoretical values predicted by Theorem 1. Items starting with “N.” are numerical
values. We plot two curves for the variance in Figure 1a. “T. Var1” is obtained by Equation (11) of
Theorem 1. “T. Var2” is obtained by Equation (13).
where A* = JPi∈[m] r产 + L Moreover, we have lim%di→+∞ V0,d,n = σ2 limλ→0+ Pm=I r*2∙
V	di∕n→zi
If d/n →	i∈[m] zi < 1, then we have
lim	V0,d,n
n,di →+∞
di∕n→Zi
σ2
i∈[m] Zi
1 -	i∈[m] zi
Corollary 1. If m = 1 and d/n → zι > 1, we have limn,di→+∞ V0,d,n = σ2 ^ɪ-ɪ.
di∕n→zi
Proof. In the case m = 1, we have r* solves minn ≥0 /r2 subject to y∕r+ + 1 = √Z1rι. The
equality constraint gives r*2 = 五-ɪ. Then by Theorem 2, the limiting variance is σ2r*2 = σ2 五-ɪ.
1 1 □
In Theorem 2, we find that in the underparameterized regime, r* solves an equality-constrained
minimization problem. In the proof of Theorem 2, we see that the equality constraint is feasible
in the underparameterized regime but infeasible in the overparameterized regime. Moreover, we
present an optimality condition for r*, which will be used in Theorem 3 to study the two-block
(m = 2) case. If the data distribution is isotropic (which means that the covariance matrix is a scalar
matrix), Collorary 1 shows that the limiting variance is σ2-1-τ, which agrees with (Hastie et al.,
z1-1
2019, Theorem 1).
In the overparameterized regime, however, we find that the limiting variance does not depend on the
spectrum {λ1, . . . , λm}of the covariance matrix and only depends on the noise intensity σ and the
ratios zi = lim di/n. This agrees with (Hastie et al., 2019, Proposition 2).
In Theorem 3, we study the case m = 2 and present a concrete closed-form formula for the limiting
variance in the overparameterized regime. Recall that the limiting variance in the underparameter-
ized regime has a closed-form σ2 ^Pi∈m] Zz for general m, as shown in Theorem 2.
1- i∈[m] zi
Theorem 3. Ifm = 2 and d/n → z1 + z2 > 1, we have
lim V0,d,n
n,di →+∞
di∕n→Zi
σ2___________q + 1___________
q2(z1 一 1) + 2q√zιZ2 + z2 一 1 .
7
Under review as a conference paper at ICLR 2022
25
T. Var (λ = 0)
N. Var (λ = 0)
20
oou-.ld> pocodx山
0，
0	100
200	300	400
Number of Samples
(a)
1	2345
ζ
(b)
9
J
Figure 2: Figure 2a: We illustrate sample-wise triple descent of the variance term in ridgeless
regression (λ = 0). There are 2 blocks. We set d1 = 80 , d2 = 120, λ1 = 1 and λ2 = 100.
The two descents occur at around n = 125, 200, respectively. In the legend, “T. Var” denotes
the theoretical values predicted by Theorem 3. “N. Var” denotes the numerical values. Figure 2b:
Function f(ζ) defined in Equation (17) with σ = 1.
where
q
λl (zi - 1) + λ2 (1 - Z2)+ ∙√(λι (zι - 1)+ λ2 (1 - Z2))2 +41112—1丁2
2λ2√ziz2
(16)
We illustrate the theoretical values predicted by Theorem 3 (overparameterized regime) and Theo-
rem 2 (underparameterized regime) in Figure 2a and compare it to the numerical values.
Corollary 2 (Triple descent in the two-block case). Assume m = 2, z1 = z2, d/n → ζ = 2z1, and
λ2∕λ1 = %. Define f%(Z) = limn,di →+∞ V0,d,n. We have
di∕n→zi
fσ 一
f (Z) , %→m∞fρ(Z)=	σ2	+ 2-ζ - 1
U ζ⅛
ζ<1,
1<ζ<2.
ζ>2
(17)
There exists	ζ1, ζ2, ζ3, ζ4	and %0 such that for all % >	%0,	we have f%0 (ζ1)	<	0,	f%0 (ζ2)	>	0,
f%0(ζ3) <0,andf%0(ζ4) <0.
Proof. The case ζ < 1 is already given in Theorem 2. In the sequel, assume ζ > 1. Define q as in
Equation (16). We have
Z + ,Z2(% +1)2 - 4Z(% - 1)2 + 4(% - 1)2 -(Z - 2)% 二 2
2Z%
Recall Theorem 3, we get
f%(Z)
lim	V0,d,n
n,di →+∞
di∕n→Zi
2 (q2 + 1)
Z (q +1)2-2(q2 + 1)
2σ2
(q+i)2
q2 + 1
-2
Direct calculation yields
lim
%→+∞
f%(Z)
Z
2
Z-2
+ 2-Z - 1
Z<1,
1<Z<2,
Z>2.
(σ2(Z⅛)2	Z< 1 ,
g(Z) , %→m∞f%(Z)= σ2d⅛⅛	1 <ζ<2 ,
tσ2(Z-⅛	Z> 2 .
8
Under review as a conference paper at ICLR 2022
The function g(Z) > 0 if Z ∈ (√2,2)and We have g(Z) < 0 if Z < √2 or Z > 2. Pick Zi > 2 >
Z2 > √2 > Z3 > 1 > Z4. Then We have g(Zi) < 0, g(Z2) > 0, g(Z3) < 0, and g(Z4) > 0. There
exists %o such that for all % > %0, we have f%(Z1) < 0, f%(Z2) > 0, f%(Z3) < 0, and f%(Z4) < 0. □
Collorary 2 theoretically proves that there exists triple descent When m = 2 and λ2 λ1 . Note
that a larger Z = lim d/n reflects a relatively smaller n. If f%0 (Z) < 0, then f%(Z) decreases on
a neighborhood of Z and therefore the limiting variance increases With a relatively larger n. As n
becomes relatively larger, We see an increasing stage, a decreasing stage, and finally an increasing
stage in order in the overparameterized regime (n < d). When We further increase n and enter
the underparameterized regime, We observe a decreasing stage. We illustrate f(Z) in Figure 2b. In
Figure 2b, We observe tWo singularities at Z = 1 and Z = 2.
3.2 Optimal Regularization Monotonizes Generalization Risk Curve
Recall the definition of the ridge estimator in Definition 1. Since this subsection concerns sample-
Wise monotonicity, We adda subscript n to X and y (they are defined by Equation (1) in Section 1.1)
to emphasize that they consist of n data items. Therefore We Write
θλ,n,d，argmin1 IIyn - Xnθ∣∣2 + λ ∣∣θ∣∣2 .
θn
In this subsection, under an assumption, We shoW that optimal regularization (i.e., pick λ that min-
imizes the generalization risk of θλ,n,d) results in a monotone generalization risk curve—in other
words, with optimal regularization, more data always reduces the generalization risk. The assump-
tion is that ∣∣∏iθ*k2 = ʌ/ʌdi, i.e., the squared norm of the projection of θ* onto each eigenspace of
the covariance matrix is proportional to the dimension of that eigenspace. (Nakkiran et al., 2021)
showed by numerical results that optimal regularization can mitigate double descent for anisotropic
data distribution. We give a partial theoretical proof of their observed phenomenon.
To ease the notation, we use γi , lim n/di rather than zi , limdi/n in Theorem 4 because a larger
γ reflects a relatively larger n (in the limit). Theorem 4 shows that with the optimal regularization,
the limiting risk is an increasing function of γ1 , . . . , γm .
Theorem 4 (Optimal regularization). If
k∏iθ*∣2 = d∣^,	(18)
then there exists a function g(γ1, . . . , γm) such that g(γ1, . . . , γm) is increasing in every γi and
lim inf EXn ,yn llθλ,n - θ*L = g(YI,…，Ym).
n,di→∞ λ>0	Σ
n/di →γi
4 Conclusion
We studied the generalization risk (test error) versus the number of training samples in ridgeless
regression. Under the assumption that the data distribution is Gaussian and the spectrum distribu-
tion of its covariance matrix converges to a discrete distribution, we obtained the exact formulae
for the limiting bias and variance terms using the random matrix theory when the dimension and
the number of training samples go to infinity in a proportional manner. Using these formulae, we
proved the sample-wise multiple descent phenomenon of the generalization risk curve.Moreover, we
theoretically showed that the ridge estimator with optimal regularization can result in a monotone
generalization risk curve and thereby eliminate multiple descent under some assumptions.
References
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks,132:428-446, 2020.
9
Under review as a conference paper at ICLR 2022
Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108-127. World Scientific, 2008.
Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices,
volume 20. Springer, 2010.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167-1180, 2020.
Siegfried Bos and Manfred Opper. Dynamics of batch training in a perceptron. Journal of Physics
A: Mathematical and General, 31(21):4835, 1998.
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve. arXiv preprint arXiv:2008.01036, 2020a.
Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization
gap between adversarially robust and standard models. In International Conference on Machine
Learning, pp. 1670-1680. PMLR, 2020b.
Donald L Cohn. Measure theory. Springer, 2013.
Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics, 46(1):247-279, 2018.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Shun ichi Amari, Jimmy Ba, Roger Baker Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu. When does preconditioning help or hurt generalization? In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
S724o4_WB3.
Anders Krogh and John A Hertz. Generalization in a linear perceptron in the presence of noise.
Journal of Physics A: Mathematical and General, 25(5):1135, 1992.
Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. In Conference on Learning Theory, pp.
2683-2711. PMLR, 2020.
F. Liese and K.J. Miescke. Statistical Decision Theory: Estimation, Testing, and Selection. Springer
Series in Statistics. Springer New York, 2008. ISBN 9780387731940.
Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. A brief prehistory of
double descent. Proceedings of the National Academy of Sciences, 117(20):10625-10626, 2020.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics,
2019.
Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More
data can help, double descend, or hurt generalization. In Conference on Uncertainty in Artificial
Intelligence, 2021.
10
Under review as a conference paper at ICLR 2022
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=7R7fAoUygoa.
M Opper, W Kinzel, J Kleinz, and R Nehl. On the ability of the optimal perceptron to generalise.
Journal of Physics A: Mathematical and General, 23(11):L581, 1990.
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less) regres-
sion under general source condition. In International Conference on Artificial Intelligence and
Statistics,pp. 3889-3897. PMLR, 2021.
Walter Rudin. Principles of mathematical analysis. New York, NY: McGraw-Hill, Inc., 3 edition,
1976.
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise
analysis of the estimation error. Proceedings of Machine Learning Research, 40:1683-1709,
2015.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286, 2020.
F Vallet, J-G Cailton, and Ph Refregier. Linear and nonlinear extension of the pseudo-inverse solu-
tion for learning boolean functions. EPL (Europhysics Letters), 9(4):315, 1989.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Timothy LH Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a rule.
Reviews of Modern Physics, 65(2):499, 1993.
Denny WU and Ji Xu. On the optimal weighted '_2 regularization in overparameterized linear re-
gression. arXiv preprint arXiv:2006.05800, 2020.
11
Under review as a conference paper at ICLR 2022
Notation		Comments
Σ = PAPτ	Rd×d-	Covariance matrix of data
P	O(d)	
A=diag(λ1Id1,...,λmIdm)	Rd×d	
θ0，Pτθ*	Rd	
X = ZTA1/2PT = (x1,...,Xn)T	Rn×d	Xi 〜N(0, ∑)
Z = (Z1, . . . , Zm)τ	Rd×n	Zi ∈ Rn×di. Each entry follows N(0, 1).
Table 1: Notation
A	Eigendecomposition and More Notation
Write Σ = PΛP>, where P is an orthogonal matrix and Λ = diag (λ1Id1 , . . . , λmIdm) ∈ Rd×d is
a diagonal matrix. Write λ- = mini∈[m] λi and λ+ = maxi∈[m] λi. We can generate x1 , . . . , xn
from standard normal random vector Zi 〜 N(0,Id) by setting Xi = PA1/2zi. Therefore, if Z =
( z1 . . . zn ) ∈ Rd×n,we get
X> = ( x1	. . . xn ) = P A1/2 ( z1 . . . zn ) = PA1/2Z .
Take the transpose gives X = Z>A1/2P>. Note that every entry of Z ∈ Rd×n follows i.i.d.
N(0, 1). Write Z in a row-partitioned form
(Z> ʌ
Z=I J
where Zi ∈ Rn×di . Write P in a column-partitioned form
P = ( P1 . . .	Pm ) ,
where Pi ∈ Rd×di. Recall that Πi ∈ Rd×d denotes the orthogonal projection to the eigenspace of
λi. We have ∏i = PiPiv. Define θ0，Pτθ* and write it in a row-partitioned form
( P>θ* ∖	( θ1 ∖
θ0 =	. I =..	(19)
∖ Pr>θ* J ∖ θm )
where θi ∈ Rdi. Then ∣∣θi k2 = ∣∣Pτθ*∣∣2 = IlPiPiT θ*∣∣2 = k∏i θ*k2. We summarize part of the
notation above in Table 1.
B Bias and Variance Under Eigendecomposition
Lemma 1 characterizes the smallest and largest eigenvalue of ZdZ (if n/d → γ < 1) and ZZ> (if
n/d → γ > 1). Recall that We study the asymptotic regime d%∕n → Zi. Therefore Y = P—1----------------.
j∈[m] zj
Lemma 1 ((Bai & Yin, 2008, Theorem 2)). Let Z ∈ Rd×n be a random matrix whose entries are
i.i.d. N(0, 1) random variables. As n, d → +∞, n/d → γ ∈ (0, 1), we have
lim λmin ( ZTZ
d
(1 - √Y)2 , lim λmax (ZdZ) =(1 + √Y)2
almost surely. If γ ∈ (1, ∞), as n, d → +∞, n/d → γ, we have
lim λmin
lim λmax
almost surely.
12
Under review as a conference paper at ICLR 2022
Lemma 2 (Corollary 5.35 (Vershynin, 2010)). Let A be an N × n matrix whose entries are in-
dependent standard normal random variables. Then for every t ≥ 0, with probability at least
1 - 2 exp -t2/2 one has
√N - √n - t ≤ Smin (A) ≤ SmaX(A) ≤ √N + √n + t ,
where smin(A) and smax(A) are the smallest and largest singular value of A.
Lemma 3. Let Z ∈ Rd×n be a random matrix whose entries are i.i.d. N (0, 1) random variables,
where d = d(n) satisfies limn→∞ dn) = γ. There exists universal positive constants C1,C2,N
such that for alln > N, we have
0 < C1 < —smmin(Z) ≤ —smmax(Z) < C2 ∙
nn
Proof. Since d(n) n, with loss of generality, we assume n/d → γ ∈ (0, 1). Take t =
Lemma 2, where c1 = 2 (√γ- 1) > 0. With probability at least 1 - 2e-c12n/2, we have
√d 一 n- - Ci√n ≤ Smin(Z) ≤ S
maX(Z) ≤ √d + √n+ci √n.
ciʌ/n in
Therefore, we deduce
(∖P 一 1 一 ci! ≤ ,sm®(Z) ≤，sm&x(Z) ≤ (r/ɪ+1+ci!.
n	n mn	n maX	n
Define Ci = ∣ (√ 一 1) > 0 and C2 = (√ + 1) . Then there exists a universal constant Ni
such that for all n > Ni, with probability at least 1 一 2e-c21n/2, we have
0 < Ci < —smmin(Z) ≤ —smmax(Z) < C2 .
n mn	n maX
Define event En = {g < iSmin(Z) ≤ iSmaX(Z) < C2}c. Thenwehave Pr {En] ≤ 2e-c2n/2.
Since Pn≥i Pr {En} ≤ Pn≥i 2e-c21n/2 < ∞, then the probability that infinitely many of En
occur is 0, i.e.,
Pr lim sup En = 0 .
Therefore, there exists a universal constant N2 such that for all n > N2 , En does not happen, in
θther WOrdS，	0 <Ci < 1 Smin(Z) ≤ IsmaX(Z) < C
n mn	n maX
holds.	□
Lemma 4. Let Z ∈ Rd×n be a random matrix whose entries are i.i.d. N (0, 1) random variables,
and let p be a fixed positive integer which is viewed as a constant and hidden in .. If n d, we
have E tr(ZZ>) X n2, E tr(ZZ>)2 X n3, and EkZkp . np/2.
Proof. We have
Etr (ZZ>) = EkZkF = X	Ezj = nd X n2 .
i∈[d],j∈[n]
(z> ʌ
Write Z =	. I, where Zi ∈ Rn and Zi 〜N(0, In). We have E (z>zi)2 = Ekzik2 =
zd>
n(n + 2). For i = j, we deduce E (z>zj)2 = E (∣∣Zi∣∣2 IIZjl2 u>v)2 where u,v 〜Unif(Sn-i)
and kZi k2 , kZj k2 , u, v are independent. Then we get
E (kzik kzjk S>Sj)2=EkZik2 kzjk2 1>叼)2=n2Eu2=n2 ∙ n=n∙
13
Under review as a conference paper at ICLR 2022
As a result, we have
Etr(ZZ>)2 = EllZZ>∣∣F = X E (z>zj)2 = dn(n + 2)+ (d2 — d) n X n3 .
i,j∈[d]
By (Vershynin, 2018), there exists a universal constant C > 0 such that for any t > 0,
P {∣∣Z∣∣2 > C (√n + √d +1) } < 2e-t2. Define K = C ^√n + √d). Then We have
P{kZk2 > K+t} < 2e-t2/C2 .	(20)
Recall Γ(z) = f∞ XzTe-xdx. Setting t = C√u in the equation below yields
Z e-t2/C2tp-1dt . Z e-uuP-1 du = Γ (P) X 1 .
Then we can bound the following integral
∞P{∣Z∣2 ≥ t}ptp-1dt
K
∞P{∣Z∣2 ≥ K+t}p(t+K)p-1dt
∞ e-t2/C2 (t + K)p-1 dt
=
0
.Z
0
.Z
0
=Z
0
∞ e-t2∕C2 (tP-1 + KPT) dt
e-t2/C2 tp-1 dt + κ p-1 / e-t2/C2 dt
.p-1
n 2
where the first inequality is because of Equation (20). We are in a position to bound E ∣Z∣2p:
E ∣Z∣p2
=Z∞P{∣Z∣2p≥u}du
0
= ∞P{∣Z∣2 ≥ t}ptp-1dt
0
= P{∣Z∣2 ≥ t}ptp-1dt +	P{∣Z∣2 ≥ t}ptp-1dt
0K
.np/2 + n(p-1)/2
.np/2,
where the first inequality is because
ZKP{∣Z∣2 ≥ t}ptp-1dt ≤ ZKptp-1dt = Kp . np/2 .
00
□
Lemma 5. The following equation for the bias term Bλ,d,n (defined in Equation (3)) holds
Bλ,d,n =E h∣Λ1/2 (id - A1/2Z (nλIn + Z>ΛZ)-1 Z>A1/2)。0|图	(21)
=E ∣∣Λ1/2 (Id + 人A1/2ZZ>A1/2) θ0k2 .	(22)
Moreover, we have ∣Bλ,d,n∣ . ∣∣θ*∣2 and limλ→o+ Bλ,d,n = Bo,d,n. ForallSUffiCientlylarge n and
d such that n/d → Y ∈ (0,1), we have 0 ≤ dλBλ,d,n . ∣∣θ*∣2. Therefore, {Bλ,d,n} is uniformly
bounded and uniformly equiContinuouS with reSpeCt to λ ∈ (0, ∞).
14
Under review as a conference paper at ICLR 2022
Proof. Introduce the shorthand notation M = Λ1/2ZZ>Λ1/2 ∈ Rd×d, A = Id + =M ∈ Rd×d,
N = nλIn + Z>ΛZ ∈ Rn×n, and Q = Id - A1/2ZNTZ>Λ1/2 ∈ Rd×d. Because
X>(nλIn + XX>)-1X = PA1/2Z (nλIn + Z>ΛZ)-1 Z>A1/2P> ,
we have
Bλ,d,n =E [k(Id - PA1/2Z (nλIn + Z>ΛZ)-1 Z>A1/2P>)θ*∣∣P八？>]
=E [|d1/2 (Id - A1/2Z (nλIn + Z>ΛZ)-1 Z>A1/2)。0|同
=E jkΛ1"k2i .
Using the Sherman-Morrison-Woodbury formula yields
N-1 =—In ——LZ>Λ1/2 (I + -1Λ1/2ZZ>A1/2)	A1/2Z
nλ	(nλ)2	nλ
=-1 (In- Z>Λ1/2 (nλId + A1/2ZZ>A1/2) -1 A1/2Z)
=nnλ (In - Z>Λ1/2 (nλId + M)-1 A1/2Z) .	(23)
It follows that
Q =Id - A1/2ZN-1Z>Λ1∕2
=Id -1A1/2Z (In - Z>Λ1/2 (nλId + M)-1 A1/2Z) Z>Λ1/2
=Id - nλ (Id - (nλId + M) 1 M)
=Id — nɪ (Id — (nλId + M) ɪ (nλId + M — nλId))
=Id -M(nλId+M)-1
=(Id+nλM)
=A-1 .
Therefore, we deduce
Bλ,d,n = E kΛ1/2 (Id + nχM)	Θ0k2 = E hkΛ"A-1θ0k2i .
Because ∣∣Λ1∕2∣∣2 . 1 and I(Id + nλΛ1∕2ZZ>Λ1∕2) 1∣∣ ≤ 1, we have
∣∣Λ1∕2A-1θ0∣∣2 . kθ0k2 = ∣∣θ*k2 .
Therefore ∣Bλ,d,n∣ . ∣∣θ*k2. Moreover, by the dominated convergence theorem,
lim Bλ,d,n = B0,d,n .
λ→0+	,,n	,,n
We compute the derivative of A-1:
dA-1 _	1 dA 1 _ MA-2
dλ	dλ	nλ2 .
The matrix Mn = 八1/2子>八1/2 ∈ Rd×d is positive semidefinite and its d 一 n smallest eigenvalues
are zeros. Its non-zero eigenvalues are the same as the non-zero eigenvalues of Z JZ. Because all
eigenvalues of Z>ΛZ are positive almost surely, the spectrum of MM consists of d 一 n zeros and the
15
Under review as a conference paper at ICLR 2022
spectrum of ZTZ. We study the range of the spectrum of ZTZ. Because λ-ZnZ 4 Z>ΛZ 4
λ+ ZnZ, we deduce
	λmin (Z>ΛZ) ≥ λ-λmin (ZnZ) → λ- (1 - √1∕Y) ∣	(24) λmax (z>Λz) ≤ Λ+Λmax (ZnZ) → λ+ (1 + B)∣ .	(25)
Define Li	=λ-λmin (ZnZ) and LI = λ+λmax (ZnZ) ∙	We get Iimn,d→+∞ LI = n	n	n	n	n∕d→γ<i 	、∣	∣	∣	、∣
λ- (1 - P	1∕γ 1 , limn,d→+∞ L∣ = λ+ (1 + ∙√1∕γ) and n	n∕d→γ<1	'	) spec (z>Λz) ⊆ [Lι,L∣].
We bound ∣	MAT∣∣ ∣∣MA-3∣∣∣=n MM (Id+M)	∣ ∣ 2 s =n max 	3 s∈sPec(M) (1 + SN s =n	max		ɜ s∈{Q}Uspec( z>Λz .) (1 + S/1) s =n	max		ŋ s∈spec(Z∑AZ .) (1 + S∕λ)3 S ≤n max 	ŋ . —s∈[L1,L2] (1 + S∕λ)3
We compute 今∣∣Λ1∕2A-1θzk∣:
	dλ Mv∣AT 叫 ∣ 0^∖ =Tθ,τ (A-1ΛMA-∣ + MA-∣ΛA-i) θ' =T (A-1θ0)τ (ΛMA-i + MA-1Λ) (A-1θ/)
Next, we bound ∣ 条 ∣∣Λ1∕2A-1θzk∣ ∣:
	9B dλBλ,d'n ≤nɪ! ∣∣MA-∣ΛA-i + ATΛMA-∣∣∣∣ ∣∣θ,∣∣ ≤ n⅛∣∣MA-IAAT ∣∣∣ ∣θ'k∣ =n⅛∣∣MA-3 AAAT∣∣∣ kθ0k∣ ≤ n⅛∣∣MA-3∣∣∣∣∣AAAT∣∣∣ ∣θ'k∣ .ɪ max 	S	o ∣∣θz∣∣ , λ∣ s∈[Lι,L2] (1 + S∕λ)3	∣
where the last inequality is because ∣∣AΛA-1^∣ = ∣∣Λ∣∣ ≤ λ+ < 1. Define f (S) =(1+：6)3.
Because f(s) = \]+—%),the function f is increasing on [0, λ∕2] and decreasing on [λ∕2, +∞).
16
Under review as a conference paper at ICLR 2022
If λ ≤ 2L1, we have
s
max ---------
s∈[L1,L2] (1 + s
LI
(I + LI/A)3
It follows that
ɪ---------L~3	= L1λ 3	≤ max L1λ 3	. ɪ
λ2	(1 + L1 /λ)3	(λ + L1)3	一 λ∈[0,2L1]	(λ + L1)3 Li
If λ ≥ 2L2, we get
ɪ max -----------s-----3	= ɪ	•----------3	≤ max ——3	. ɪ ≤ ɪ
λ2 s∈[L1,L2] (1	+ s∕λ)3 λ2	(1 + L2∕λ)3	一 λ∈[2L2,∞) (λ + L2)3 L2 — Li
If 2L1 < λ < 2L2, we obtain
ɪ max -----------S---3 . ɪ . ɪ .
λ s∈[Lι,L2] (1 + s∕λ) λ LI
In all three cases, We show that = maxs∈[L1,L2](1+：/1)3 .4.It follows that
ddλ |—〃2
dλ
By Lemma 3, there exists a universal constant n0 such that for all n > n0, one has
1
Thus we conclude that
疝 A" A-1θ0 k2
. kθ0k22 .
We can exchange differentiation and expectation and get
dλBλ,d,n = E 偿M/A-1θ0k2
dλ	dλ
and
ddλBλ,d,n = E [ldλ -叫2 l S kθ0k2 .
□
Lemma 6. The following equation for the variance term holds
Vλ,d,n =σ2EkΛ1/2 (λnIrf + Λ1/ZZ>A1/2)-1 Λ1∕2Z∣∣2
=σ2E∣∣ΛZ (λnIn + ZTAZ)-1 ∣∣2 .
Moreover, for all sufficiently large n and d such that n∕d → γ 6= 1, we have limλ→0+ Vλ,d,n =
Vo,d,n, ∣Vλ,d,n∣ . 1 and ∖ 余Vλ,d,n∣ . L Therefore, {Vλ,d,n) is uniformly bounded and uniformly
equicontinuous with respect to λ ∈ (0, ∞).
Proof. As in the proof of Lemma 5, define M = A1/2ZZ> A1/2 ∈ Rd×d and N = nλIn +Z > AZ ∈
Rn×n. Recalling Σ = PAP> and X = Z> A1/2P> , we have
Vλ,d,n =σ2Etr XΣX> (nλIn +XX> )-2
=σ2Etr Z> A2ZN-2
=σ2Etr N-1Z> A2ZN-1
=σ2EAZN-12F .
17
Under review as a conference paper at ICLR 2022
Recalling Equation (23) yields
ΛZN-1 =(AZ (In - Z>Λ1∕2 (nλId + M)-1 Λ1∕2Z)
=nλA1/2 (In - M (nλId + M)-1) A1/2Z
=A1/2 (nλId + M)-1 A1/2Z.
Define R = Λ1/2Z ∈ Rd×n. We get
(nλId + M)-1 A1/2Z = (nλId + RRT)T R.
Notice that if 0 < a <b, then aId + RRt 4 bId + RRt . We deduce
(bId + RRt)2 - (αId + RRt)2 = (b2 - a2) Id + 2(b - a) RRt < 0 .
Thus (bId + RRt)2 < (aId + RRt)2, which implies (bId + RRt) 2 4 (aId + RRt) 2. We
get
Rt (bId + RRt)-2 R 4 Rt (aId + RRT)-2 R,
tr(RT (bId + RRt)-2 R) ≤ tr(RT (aId + RRT)-2 R)
Let λo (∙) denote the smallest non-zero eigenvalue of a positive semidefinite matrix. We bound the
Frobenius norm
∣∣(nλId + M )-1 A1/2z[
=tr(RT (nλId + RRt)-1 R)
≤ tr Qlim+ Rt (nλId + RRT)-2 R)
=tr (RtR) +
= tr(Z T ΛZ) +
< tr (zTZ)+ .
It follows that
∣∣ΛZN-1 ∣∣F = ∣∣Λ"2 (nλId + M)-1 Λ/z[ < ∣∣(nλId + M)-1 Λ"2z[ < tr (ZtZ)+ = tr (ZZt)+ .
If n/d → γ < 1, the matrix ZtZ is full-rank almost surely. Then, using the formula for the mean
of inverse Wishart distribution, we have Etr (ZtZ)+ = tr E (ZtZ) 1 = tr ɑ-n-ə N 1. If
n/d → γ > 1, the matrix ZZT is full-rank almost surely. Similarly, we have Etr (ZZt)+ =
tr E (zzT) 1 N 1. By the dominated convergence theorem, we have limλ→o+ Vλ,d,n = V0,d,n.
Moreover, Vv,n < E ∣∣ΛZN-1∣∣F < 1.
Next we bound 余 V(θ). Because dd- = -N-1 塞N-1 = -nN-2, we deduce
-d- ∣∣ΛZN-1∣∣2 = -d- tr (N-1ZtΛ2ZN-1) = -2ntr (ZtΛ2ZN-3) ≤ 0 .
dλ	dλ
On the other hand, we have
tr (ZtΛ2ZN-3)
=tr (N-3/2ZTΛ2ZN-3∕2)
< tr (N-3/2ZTΛZN-3∕2)
=tr (ZtΛZN-3)
=X	—‰.
s∈speC⅛>ΛZ) (" + S)
18
Under review as a conference paper at ICLR 2022
Because the number of non-zero eigenvalues of Z>ΛZ equals rank Z>ΛZ = n ∧ d n, we get
ɪ ∣∣ΛZN-11∣2 N ntr (ZTΛ2ZN-3) . n2 max -------------------s 3 = max ------------------s——3
dλ	s∈spec(Z>ΛZ) (λn + S)	s∈specf z>λz \{0} (λ+ s)3
>
If Y < 1, the matrix nn is full-rank almost surely. By Equation (24) and Equation (25) in the
proof of Lemma 5, there exists universal positive constants Ci and C? such that spec (Z?Z) ⊆
[C1, C2] for all sufficiently large n and d such that n/d → γ < 1. Ifγ > 1, the non-zero eigenvalues
of Z>nΛZ and M are the same. The matrix M is full-rank almost surely. Thus spec (Z?Z) \ {0}=
spec (M). Because yτΛ-1y . yτy,
λmin (M) =min
n	x6=0
χT Λ1∕2ZZ>Λ1/2 X
min yTT y & min y'午 y = λmin
y6=0 yTΛ-1y	y6=0	yTy
ZZ>
Similarly, we get
. λmax
n
n
Therefore, there exists universal positive constants C1 and C2 such that
ZTΛZ
spec (	\ ∖{0} ⊆
, C2λmax
Thus in both cases, we have shown that there exists universal positive constants C1 and C2 such that
ZTΛZ
spec (	\ \{0} ⊆
, C2 λmax
Define Li = Ciλmi∏ (ZnZ ) and L? = C?λιmx (ZnZ). As a result, We get
QNT∣∣2	.
s
max ----------3 .
s∈[L1,L2] (λ + s)
Define f(S) =。+.尸.Because f0(s) =()+2*, the function f is increasing on [0,λ∕2] and
decreasing on [λ∕2, +∞). If λ ≥ 2L2 or λ ≤ 2Li, we get
S	Li	L2	1
max ---------T ≤ ---------T V---------T ≤ —o-.
s∈[L1,L2] (λ + S)	(λ + Li)	(λ + L2)	Li
If 2Li < λ < 2L2, we get
max --------------------------------------S-o .	.	.
s∈[L1,L2] (λ + S) λ LI
As a result, for all sufficiently large n, we have
-d ∣∣ΛZN-1∣∣2 = max —‰ . ɪ .——1~V . 1,
dλ 11	ll2	s∈[L1,L2] (λ + s)3	L2	λ2,n fZ>Z\
where the final inequality is because of Lemma 3. We can exchange the expectation and differenti-
ation and obtain
dλVλ,d,n=σ Edλ ∣∣λzn ∣∣2
and
dλVλ,d,n ≤ σ2E dλ ∣∣λzn-i∣∣2 . 1 .
dλ	dλ
□
19
Under review as a conference paper at ICLR 2022
C Lemmas on Stieltjes Transform
Definition 3 (Stieltjes transform). The Stieltjes transform of a distribution with cumulative distri-
bution function F is defined by
sF (z)
/三dF (λ)
( ∈H, { ∈ C | = > 0}).
Lemma 7 (Theorem 4.3 (Bai & Silverstein, 2010)). Suppose that the entries of Xn ∈ Cn×p are
complex random variables that are independent for each n and identically distributed for all n
and satisfy E |x11 -	Ex11 |2
1. Also, assume that Tn = diag(τ1 , . . . , τp), τi is real, and the
empirical distribution function of {τ1, . . . , τp} converges almost surely to a probability distribution
function H as n → ∞. The entries of both Xn and Tn may depend on n, which is suppressed for
brevity. Set Bn = An +，XnTnXn, where Xn is the conjugate transpose of Xn, An is Hermitian,
n × n satisfying F An →F A almost surely, where F A is a distribution function (possibly defective)
on the real line. Assume also that Xn, Tn, and An are independent. When p = p(n) with p/n →
y > 0 as n → ∞, then, almost surely, F Bn, the empirical spectral distribution of the eigenvalues
of Bn, converges vaguely, as n → ∞, to a (nonrandom) distribution function F	, where for any
∈	C+	= {z	∈ C	| =z	>	0},	its Stieltjes transform s = s(z	) is the unique solution in	C+	to the
equation
s = sA
τdH (T)
1 + Ts
where sA is the Stieltjes transform ofF A.
Lemma 8. If the functions fα, gα : I → R satisfy fα(x)- gα(x) → 0 uniformly as α → +∞, then
limα→+∞ (infx∈I f(x)-	infx∈Ig(x)) = 0.
Proof. Because fα(x)- gα(x) → 0 uniformly as α → +∞, we have for ∀ > 0, there exists N()
such that for ∀a > N(e) and ∀x ∈ I, it holds that ∣fα(x) - gα(x)∣ < e. Therefore, We get
gα(x)-	< fα(x) < gα(x) + .
Thus We obtain
inf fα (x) ≤ fα (x) < gα (x) + e
x∈I
inf gα(x) - e ≤ gα(x) - e < fα(x) ,
x∈I
Which in turn implies
inf fα(x) ≤ inf gα(x) + e
x∈I	x∈I
inf gα (x) - e ≤ inf fα (x) .
x∈I	x∈I
It follows that |infχ∈ι fa(x) - infχ∈ι ga(x) | ≤ e. In other words, we proved
lim	inf f(x) - inf g(x) = 0 .
α→+∞ x∈I	x∈I
□
Lemma 9. Define N = λnIn + Z> ΛZ. Then we have
lim tr (NT) =-^- inf
n,di→+∞ dλ ρ∈R+m
di /n→zi
lim — log det — = inf
n,di→+∞ n	n ρ∈R+m
di∕n→Zi
log λ + X λiPi	+ X (Pi - Zi (log ρi + l))
i∈[m]	i∈[m]	i
logλ + λiρi +	ρi-zi
i∈[m]	i∈[m]
P + 1
i
(26)
. (27)
- y
20
Under review as a conference paper at ICLR 2022
Proof. Proof of Equation (26). We apply Lemma 7 with An = 0n×n, Xn = Z> ∈ Rn×d,
Tn = Λ, and Bn = 1Z>ΛZ. The distribution function of 0n×n converges to 1t≤o and
its Stieltjes transform is sa(z) = J ʌ-zd1λ≤0 = -1. The empirical distribution function
of {λ1,...,λ1,...,λm .∙∙,λm} is Hn^i (t) = Pi∈[m] d 1t≤×i. Recall di/n → z%. Thus
×-------{----}	×---{-----}
d1	dm
di/d → zi/K, where d/n → y = Pj∈[m] zj . The empirical distribution function converges to
H(t) = Pi∈[m] Z 1t≤λi. Then the empirical spectral distribution of the eigenvalues of 1Z>ΛZ
converges vaguely to a nonrandom distribution function F and its Stieltjes transform is
S = s(z) = lim tr tr (—Z>ΛZ — ZIn)	= lim tr (Z>ΛZ — ZnIn) 1
n,di →+∞ n	n	n,di →+∞
di /n=zi	di /n=zi
(this is because of (Bai & Silverstein, 2010, Theorem B.9)). By Lemma 7, s(Z) is the unique solution
in C+ to the equation
1
which gives
s(Z) = sA
Z-y
TdH (τ))
1 + TS
λi Zi
i∈[m] 1+λis(z)
S(Z)
-
i∈[m]
λi Zi
1 + λis(z)
-1 .
Z
-
We want to prove Equation (26) first. The lefthand side of Equation (26) equals
lim
n,di →+∞
di /n=Zi
tr (λnIn + Z>ΛZ)-1
S(-λ) .
Because the matrix 1Z>ΛZ is positive semidefinite and thereby all of its eigenvalues are non-
negative, its limiting spectral distribution is supported on [0, ∞). The Stieltjes transform S(Z) of
the limiting spectral distribution can be continuously extended to (-∞, 0). Therefore, for ∀λ > 0,
S(-λ) is the unique solution to the following equation
s(-λ) 卜+X ι+λizi-λ)!=1.	(28)
We will verify that
dλρ∈nfm log (λ + XλjPj) + X(Pj-Zj(logρj + 1))j
satisfies Equation (28). Take a minimizer ρ* of Equation (5). Using the envelope theorem yields
dλ ρ∈Rfm	log (λ+X	λjP)+X	(ρj- zj (Iog ⅛+1))1=λ+Pml	λjPj.	(29)
Plugging the righthand side of Equation (29) into Equation (28), we get
1
λ+Pm=I λj Pj
m
λ+X
i=1
_______λiZi
1 + λi ∙ λ+pm= ι 入jPj
1.
Rewriting the above equation yields
m
X
i=1
λizi
1 + λi ∙ λ+Pm= 1 λj Pj
m
XλiPij.
i=1
It suffices to show that each summand on the lefthand side equals its counterpart on the righthand
side
λizi
1+ λi ∙ λ + Pm= 1 λj Pj
λiPij .
21
Under review as a conference paper at ICLR 2022
We need to show
zi	1
Pf = 1 + λi∙ λ + j λj Pj ,
which is equivalent to Equation (6) and therefore holds. Hence we have proved Equation (26).
Proof of Equation (27). We use α to denote the indices n, di. Define
(m	∖ m /
ʌ * XλjPj+X
Pj - Zj(log ρj + I)
j=1	j=1 zj
First, we want to show that limλ0→+∞ (h(λ0) - log λ0) = 0. Define
lλο (P) = log (1 + 1 X λjPj) + X (Pj-Zj (log ρj +1)),
q(ρ) = X (Pj-Zj (log Pj +1)).
The Hessian matrix of q(p) is diag (待，...，Pm), which is positive definite since z"i > 0. There-
fore, q(p) is convex and the minimum of q(P) on R+m is attained at P = z, where z = (Z1, . . . , Zm)>.
The minimum is infρ∈Rm q(P) = q(z) = 0. Because limkρk →+∞ lλ0 (P) = +∞, there ex-
ists a universal constant K1 > kzk2 > 0 such that lλ0 (P) > lλ0 (z) for all kPk2 > K1. De-
fine E = {p ∈ 股窣 | ∣∣Pk2 ≤ Kι}. We have Z ∈ E , infp∈e 1"。(P) = infρ∈Rm 1i°(p), and
inf ρ∈E q(P) = infρ∈R+m q(P) = 0. Therefore, we get
h(λ0) -logλ0 = inf lλ0(P) = inf lλ0(P) - inf q(P) .	(30)
ρ∈R+m	ρ∈E	ρ∈E
On E, there exists a universal constant K2 > 0 such that	j∈[m] λjPj < K2 . Thus on E, we
deduce
0 < lλo (P)- q(P) = Iog (1 + ( X λjPj) < log (1 +
The right-hand side log(1 + K) → 0 as λο → +∞. Thus limλ0→+∞ (1入。(p) - q")) = 0
uniformly for P ∈ E. By Lemma 8, we get
lim inf lλ0(P) - inf q(P) = 0 .
λ0→+∞ ρ∈E	ρ∈E
Recalling Equation (30) yields
lim (h(λ0) - logλ0) = 0 .	(31)
λ0 →+∞
Define fα(λ) = n log det N. Second, We want to show lima fα(λ) = h(λ), where lima means
limn,di→+∞. We have fα(λ) - fα(λ0) = Rλλ fα0 (x)dx for ∀λ, λ0 > 0. It follows that
di /n=zi
fa(λ) -	h(λ)1	≤	lfa(λ)	-	h(λ) +	h(λ0) -	fa(λ0) + fa(λ0) - log λ0 + log λ0 -	h(λ0)
≤	lfa(λ)	-	h(λ) +	h(λ0) -	fa(λ0)1 + lfa(λ0) - log λ0 | + |log λ0	- h(λ0)
= Zλλ0
fα0 (x)dx - (h(λ) - h(λ0))
+ lfa(λ0) - log λ0l + |log λ0 - h(λ0)1 .
Taking lim supα on both sides gives
limsup ∣fa(λ) - h(λ)∣ ≤ limsup I Z f!a(x)dx - (h(λ) - h(λο)) +limsup ∣fa(λο) - log λο∣ + ∣logλο - h(λ°)∣ .
α	α λ0	α
(32)
22
Under review as a conference paper at ICLR 2022
Recall f∖(λ) = tr N-1 and limα fa(λ) = h0(λ) (this is exactly Equation (26)). Because
∣tr N Tl = tr N-1 ≤ ɪ and 6 1 dx < +∞, by the dominated convergence theorem, we have
lim
a
/λ f0(x)
λ∙0
dx
∕λ
h( (x)dx
J入0
h(λ) — h(λo).
It follows that
IimSUP |，f0(x)dx — (h(λ) — h(λo))
λo
lim
a
∕λ f0(x)
λ0
dx — (h(λ) — h(λo))
(33)
0 .
Since
fα(》o)—log ʌo =n log det (λ0∕n + nZ>Λz) — n logdet(λoIn)=n log det (In + ηʌ Z>ΛZ
and the matrix n^ Z>ΛZ is positive semidefinite, we have
fα (λo) — log λo ≥ 0 .
We have
fα (λo) — log λo
=—log det (In +-- Z>ΛZ
n	n	nλo
≤ 1logdet (In+n+Z TZ)
≤log (1 + ⅛λmax (ZnZ),
令λm/纪Z ).
Then taking lim SUP , we get
limsup Ifa(λo) — log λo∣ = limsup(fα(λo) — log λo) ≤
a
a
where the last inequality is because lim supα λmax (⅛Z)
Using Equation (32), Equation (33) and Equation (34) gives
λ+ limsup λmax ( ^Z^L) . 1 ,
λo	a	nJ	λo
2	(34)
(1 + JY V 1)N 1 by Lemma 1.
limsup Ifa(X)- h(A)i . τ—k |logλo — h(λo)| .
λo
Then taking limλ0→+∞ and recalling Equation (31) yields
lim ∣fa(λ) — h(λ)∣ = limsup Ifa(λ) — h(λ)∣ =0 .
Therefore, we conclude limα fa(λ) = h(λ).
□
Lemma 10. Define N = XnIn + ZTΛZ = XnIn + 5∑i∈[m] λiZiZ>. Thefollowing equation holds
lim E
n,di→十∞
di∕n→zi
lim E
n,di→十∞
di∕n→zi
∂ 1	,，N
西 £logdet n.
∂ ∂2 1 ,…N
[∂λ^∂λ"n logdet n
得inf
∂Xi ρ∈Rm
log I X + ɪ2 XiPi I +
.∖	i∈[m])
需inf
∂Xj ∂Xi ρ∈Rm
log I X +	XiPi) +
i∈[m]
(35)
(36)
23
Under review as a conference paper at ICLR 2022
Proof. Proof of Equation (35). We use α to denote the indices n, di and use limα to denote
limn,di→+∞. Definefa(λi) = E [ 1 log det NN ] , fα (λi)=袅 E [ 1 log det NN ], and
di /n=zi
h(λi)
inf
ρ∈R+m
log I λ + ^X λipi	+ ^X (Pi - Zi (log ~ + 1
i∈[m]	i∈[m]	i
We have
ɪ log det —
nn
≤ — log det (λIn + λ+--) = log λ +— log det (In +—y Z> Z
n	n	n	nλ
By Lemma 3, there exists a universal constant C > 0 such that for all sufficiently large n,
Therefore, we get
n logdet (In + nλZ >Z) ≤log (1 + λ
—log det — ≤ log (λ + C).
n n
By the dominated convergence theorem and Lemma 9 (specifically, Equation (27)), we obtain
lim fα (λi) = h(λi) .
α
(37)
Because
^77Γ-log det — = — tr (Z>N-IZi) ≤ τ-2 tr (Z>Zi)
∂λi n	n n	λn2
and E [熹 tr(Z>Zi)] < +∞, We can interchange the differentiation and the expectation and get
fa(Ai) =万rE —logdet— = E 系—logdet— .	(38)
∂λi	n	n	∂λi n	n
Thus We deduce
^dE	1	logdet N	= E ɪ1 logdet N	≤ E ɪ1	logdet N	≤ E τ12	tr(Z>Zi)
∂λi	n	n	∂λi n	n	∂ λi n	n	λn2
By Lemma 4, E tr(Z>Zi) N n2 and therefore E [λn2 tr (Z>Zi)] . 1. The function sequence
{fa0 } is uniformly bounded.
Then We Want to shoW that {fa0 } is uniformly equicontinuous by shoWing that {fa00} is uniformly
bounded. Because
∂2 1 N
∂a2 n logdet^
〉r(Z>N-1Zi)2
1
≤ nλ2 tr
n
and E ]/tr (ZinZi)	< +∞, We can interchange the differentiation and the expectation and
get
[E [logdet N] = ɪE [ɪ 1logdet N] = E [《 1logdet N].
∂λi2 n	n ∂λi ∂λi n	n	∂λi2 n	n
Therefore, We deduce
ZirZi
n
Again, by Lemma 4, tr (ZinZi) N n. It follows that nɪɪE tr (ZinZi).9.Therefore {f∖} is
uniformly equicontinuous.
∂2	1 N	∂2 1 N 1
∂a2E[nlogdet nJ ≤e ∂a2nlogdetn ≤nλ2Etr(
We want to show lima fa0 (λi) = h0(λi) by contradiction. If it is not true, there exists > 0 and
a subsequence {fa0 } such that fa0 (λi) - h0(λi) ≥ . Let E = [a, b] 3 λi (b > a > 0) be
24
Under review as a conference paper at ICLR 2022
a closed interval that contains λi. The subsequence {fα0 k} is uniformly bounded and uniformly
equicontinuous. By the Arzela-Ascoli theorem, there exists a subsequence fα0	that converges
uniformly on λi ∈ E. Recall limα fα(λi) = h(λi) (Equation (37)). Thus limj fαk (λi) = h(λi).
By (Rudin, 1976, Theorem 7.17), for λi ∈ E, we have
lijm fα0 kj (λi) = h0(λi).
This is a contradiction. Hence, we have shown that limα fα0 (λi) = h0(λi), which is exactly Equa-
tion (35) (recall fα(λi)=袅E [ 1 log det NN] = E [袅 1 log det NN] in Equation (38)).
Proof of Equation (36). Define gα(λj∙)=悬E [ 1 log det NN] = E [昌1 log det N]. Then
g,α (λj) = ∂λ‰i E[ 1 logdet NN] = ∂λE h ∂λi n logdet Nn i. We have
∂2	1	N
∂λ^λ" n Iogdet n
=1 tr (ZiZ>NTZjZ>NT)
=Itr(Z>NτZjZ>NTZi)
=1ZNTZiUF
≤1 kZjk2 kZik2 IlNTIlF
≤λ2⅛ kZjk2 kZik2.
where the last inequality is because IIN-1IlF ≤ IlλnnInIlF = λ1n∙ If i 6= j , by Lemma 4, we have
λ21n2 EkZj k2 kZik2 = λ2⅛EkZjk2 国第12 . J .
If i = j , by Lemma 4, we have
λ2n2EkZik4 . λ⅛ ^ n2 = λ12.
As a result, we get
∂2	1	N 1	2	1	1
∂λ^∂λinlogdet	.n∙n ∙λ2n =
Thus We can interexchange 会 and expectation, and get g∖ (λj) = E [∂λ⅜λ"1 log det NN] ∙ Be-
cause | gα (λj )| ≤ E J &晨入.1 log det NJ . ⅛^, the function sequence {g' } is uniformly bounded
for λj .
25
Under review as a conference paper at ICLR 2022
Define L = Zj>N-1Zi andW = Zj>N-1Zj. We have
∂3
∂λ2∂λi
ɪ log det —
nn
2
=—tr (L>WL)
n
.λ-2 tr (L>Z> ZjL)
=J tr (ZlN- (ZjZ>)2 NTZ).
=力区勾NTZiUF
≤周ZjZ>口为2
≤焉区。"；区。2
=λ⅛区k4团已
where the first inequality is because W 4 * Zj Zj and the third inequality is because NT 4 λ1n In
and then IlN TlIF ≤ ∣∣ λ1n InIIF ≤ 忐.ByLemma 4, we have EkZjk4 . n2 and EkZik2 . n. If
i 6= j , then Zj and Zi are independent, and we deduce
x⅛ EkZjk2 kZik2.
1
If i = j , we have
λ⅛ EkZik4 kZik2 = λ⅛ EkZik6. E.
Asaresult, WededUce E h ∂λ2∂λi n ιogdet N i =最E h ∂λ‰ n 10gdet N i = g00 (λj).MOreover,
we have
∂3	1	N 1
lgα(λj)l≤ E ∂λpλ-n 10gdet n . λ3 .
Therefore {gα0} is uniformly equicontinuous.
Define
w(λj)
得inf
∂λi ρ∈R+m
1og I λ + ^X λi Pi	+ ^X (Pi - Zi (1og ~ + 1
i∈[m]	i∈[m]	i
We want to show by contradiction that 1imα gα0(λj) = w0(λj). Assume that it is not true. Then
there exists > 0 and a subsequence gα0 such that gα0 (λj) - w0(λj) > . Since gα0 is
uniformly bounded and uniformly equicontinuous, by the Arzela-Ascoli theorem, there is a subse-
quence gα0k	that converges uniformly on a closed interval E containing λj . Equation (35) shows
that 1imα gα(λj) = w(λj). It follows that 1imr gαk (λj) = w(λj). By (Rudin, 1976, Theorem
7.17), for λi ∈ E, we have
1irmgα0kr(λj) = w0(λj) ,
which is a contradiction. Therefore, we have shown that 1imα gα0(λj) = w0(λj), which is exactly
Equation (36).
□
26
Under review as a conference paper at ICLR 2022
D Proof of Theorem 1
D.1 PROOF OF ITEM 1
Define	g(ρ)	= log	(λ	+ Pm=I λjp)	+	Pm=I	(Pj-Zj(log Z +	1)).	The func-
tion g(p) is continuously differentiable on R'. The boundary of Rm is ∂R' =
{p ∈ Rm | (∀i ∈ [m],Pi ≥ 0)八(∃i ∈ [m],Pi = 0)}.	Because IimRm3ρ→ρ0∈∂>m g(p) =
IimRm3ρ→∞ g(p) = +∞, there exists a minimizer p* ∈ Rm of g(p).
Taking the derivative with respect to Pi gives
mm
λ + X λj PjJ + X
Setting it to zero gives Equation (6).
Pj-Zj (log ⅛ + 1))|= λ + P= 1 λj Pj +1 - Λ
D.2 Proof of Item 2
Recall Equation (6)
λ + SI λj Pj +1 - PI =0 , ∀i ∈ H.
Rewriting the above equation gives
(Zi-Pi) (λ + X λkPk) = λiPi ,	∀i ∈ [m].
Rewriting it in the linear algebraic form yields
(z - p*)(λ + λτ P*) = λ Θ P*.
Applying 袅 to both sides and using the implicit function theorem, we get
(z - P*) (p*τ + λτ J) - J (λ + λτpl) = diag (λ) J + diag (p*).
Arranging the above equation yields
(diag (λ) + (λ + λτpl) Im - (z - p*) λτ) J =(z - pl) p*τ - diag (p*).
Define a = λ + λτp*, A = diag (λ) + (λ + λτp*) Im = diag (λ) + αIm and B = diag (λ) +
(λ + λτp*) Im - (z - p*) λτ = A - (z - p*) λτ. The matrix determinant lemma gives
det (B) = (1 — λτA-1 (z — p*)) det (A).
Recall Equation (6) again and we have
Zia
λ∣ + a =——
Pi
We have
a - E λiPi(I-PiIZi
i∈[m]
λ + ∑ λip:	- ∑ λiPilQ-PiUZi)
i∈ [m]
i∈ [m]
=λ + X λi 匹 > 0.
Zi
i∈[m]
It follows that
Σi∈[m] λ∣Pi(1-Pi/Zi)	1
--------------------< 1 .
a
27
Under review as a conference paper at ICLR 2022
Then we compute λ>A-1 (z - ρi):
λ>A-1(z - Pi)
Σ
i∈ [m]
% (Zi- Pi)
ʌi + a
Σ
i∈ [m]
λi (Zi - Pi)
Σi∈[m]
Zia
Tr
λiρi (1 - ρi / Zi)
-----------------< 1 .
a
Thus we get 1 — λ>A-1 (z — Pi) > 0. Therefore, det B = 0 and the matrix B is invertible.
D.3 Proof OF ITEM 3
Lemma 11. Define N = λnIn + Z>ΛZ, Y = Pi∈[m] Zi, r = (ri,..., rm), λ = (λι,..., λm),
and
”(rt, r,λ) = 2rt ^ɪ +
r2- r2- 2rt E √ziri+ E
i∈ [m]
i∈ [m]
i∈ [m]
1-r2 - λr2
λi
For any Kt ≥ 2 and Ku ≥
2λ+(2+√γ)
lim	tr N-1
n,di→十∞
di∕n→Zi
max min
0<rt<Kt 0<ri<Ku
= lim
n,di →+∞
di∕n→Zi
”(rt, r,λ);
λ
E tr N
, we have
-1
min max ”(rt, r, λ) = max min ”(rt, r, λ)
0<ri<Ku 0≤rt≤Kt	rt≥0 ri≥0
min max
ri≥0 rt≥0
(39)
, r, λ) .
If ri is a solution to the optimization problem in Equation (39), then
2
rt
m
1+X Tri
j=i
r
m
X r*√z7 + Ar：
j=i
(40)
J1 + P 皂 j
=rt √zi -
λi
(41)
Moreover, we have
∂	“	、、
——max min ”(r, r, λ)
∂λi rtax m≥0 'r' r )
—
r*2
ri
用.
Proof. Let g 〜N(0, In) be a multivariate standard normal random vector. We have
tr N-1
=Eg g>N-1g
=Eg sup (2gτt — tτNt)
t∈Rn
=Eg sup (2gTt — tτZTΛZt — nλ ∣∣t∣∣2)
t∈Rn '
=Eg sup inf(2gTt — 2uτΛZt + uτΛu — nλ ∣∣t∣∣2)
t∈Rn U∈Rd、
=—2Eg inf sup (uτΛZt — gτt — LτΛu + ɪnʌ IItk2)
t∈Rn u∈Rd	2	2
=—2Eg inf sup (UTZt — gτt — JuTΛ-1u + ɪnʌ ∣∣t∣2 )
t∈Rn u∈Rd ∖	2	2	J
28
Under review as a conference paper at ICLR 2022
We view
inf sup 卜>Zt — g>t — gu>A-1u + ；nX ∣∣t∣∣2)	(42)
as the primal optimization (PO) problem in the convex Gaussian min-max theorem (CGMT) (Thram-
poulidis et al., 2015).
The KKT conditions for Equation (42) give
Z> u — g + nλt = 0 ,
Zt — ATu = 0.
Solving the above equations gives
t = N-1g ,	u = AZN-1g .
With probability at least 1 — 4 exp(-cn) (c > 0 is a universal constant), We have ∣∣g∣∣2 ≤ 2√n and
∣∣ZIl ≤ √d + 2√n ≤(2 + √γ) √n. Therefore, we get
tk2≤N-1kgk2
1 L 2
≤ 焉∙ 2√n = λ√n
IluIl2 ≤ λ+ IIZIlktk2 ≤ λ+ (2 + √γ) √n ∙ λ√= = -+~~\^^
λn	λ
/ u1 ∖
Write u =	. I, where	ui	∈	Rdi.	For all	Kt	≥ 2,	Ku	≥	2λ+(2+√γ),	the opti-
ξ um )
mal solutions	t* and U to Equation (42) satisfy	√n ∣∣t*∣∣2 ≤	Kt and	Iluill2 ≤	Ku for all
i ∈ [m] with	probability at least 1 — 4exp(-cn).	Define St =	{t ∈ Rn	| √n ∣∣t∣∣2	≤ Kt} and
Su	= u ∈	Rd	| Iui I	≤	Ku , ∀i	∈	[m]	. We use α to denote the indices n,di	and use limα	to
denote limn,di→+∞ . Define event
di /n=zi
Ea =	inf sup (u>Zt — g>t — -^u>A-1u + ɪnλ	|用|2 )	= inf sup	(u>Zt	— g>t — ^u>A-1u + Lλ |用|2
t∈Rn u∈Rd	2	2	2 t∈St u∈Su	2	2	2
Then with probability at least 1 — 4 exp(—cn), we have t* ∈ St and U ∈ Su. Therefore the event
Eα occurs with probability at least 1 — 4 exp(—cn), which yields
P {Eαc } ≤ 4 exp(—cn) .
Since	n≥1 4 exp(—cn) < +∞, by Borel-Cantelli lemma, we have
P lim sup Eαc = P nlim inf Eα o = 0 .
Then with probability 1, all but finitely many Eα occur. Then almost surely there exists n0 such that
for all n > n0 , Eα occurs.
The auxiliary optimization (AO) problem is
t∈St u∈up Qtb g>u + iui2 g>t - g>t - 2u>A-1u + 2nλ Ht"2
/	||g — /Pi∈[m]r2 g2
inf sup
0≤rt≤Kt 0≤ri≤Ku
inf sup
0≤rt≤Kt 0≤ri≤Ku
—
∖
2	Ig1,i I2	1	1 2	1	2
-rt+O F ri— d "2 λrt
i∈[m]	i∈[m]
I	2~,^^^^2 llg3I2 I	llg1,iI2	1	1 2 , 1 2 1
「JXrF~ rt+ *k ri — 2 2 ri+2 λrt)
where gι 〜N(0,Id), g2 〜N(0,In), and g3 〜N(0,In).
29
Under review as a conference paper at ICLR 2022
Taking n, di → +∞ with di /n → zi constant, the strong law of large numbers gives
J1+ X r2 kg3k2 驾 Q X『2 ,
i∈[m]	i∈[m]
kg1j k2 _ rd kg1j k2 a∙S∙ /—
丁丁 = V 瓦 7d	7z
Define
Xa (rt, r) = -∖∣1+ X『2%rt + rt X kg√⅛Ti
i∈[m]	i∈[m]
1X 卜2 + 1λr
i∈[m]
—
It is a stochastic process on (rt, r) ∈ [0, Kt] × [0, Ku]m. We have
lim Xα (rt,r) = X (rt,r) := -rt
α
11+ X r2 + Tt X √zri - 2 X λ1 r2 + 1 λr2
i∈[m]	i∈[m]	i∈[m] i
almost surely. Since √1+ x2 is convex and increasing and the function 伊|卜 is convex,
由说 q1+krk2 is ConVeX in r andthen -q1+Pi∈[m]r2 k√nk2 Tt = -qr+krk2 k√k2 Tt
is concave in r. Because -1 Pi∈[m] ±『2 is concave in r and rt Pi∈[m] kg√,nk2ri is lin-
ear in r, we deduce that Xα (Tt, r) is concave in r. By (Liese & Miescke, 2008, Lemma
7.75), suPr∈[o,κu]m ∣Xα (rt, r) - X (『t, r) | → 0 almost surely. Then for ∀e > 0, there exists
n0 (), d0,i(), δ0,i() such that for all n > n0(), di > d0,i(), |di/n -zi| < δ0,i() and for all
r ∈ [0, Ku]m, we have
X (Tt,r) - e < Xα(Tt, r) < X (Tt, r) + e .
Thus we obtain
X (Tt, r) - e < Xα (Tt, r) ≤ sup Xα (Tt, r)
r∈[0,Ku]m
Xα(Tt, r) < X (Tt,r) + e ≤ sup	X(Tt, r) + e ,
r∈[0,Ku]m
which in turn implies
sup	X (Tt, r) - e ≤ sup Xα (Tt, r)
r∈[0,Ku]m	r∈[0,Ku]m
sup	Xα(Tt, r) ≤ sup X(Tt, r) + e .
r∈[0,Ku]m	r∈[0,Ku]m
It follows that supr∈[0,Ku]m Xα(Tt, r) - supr∈[0,Ku]m X(Tt, r) ≤ e. In other words, we showed
∣γα (rt) - Y(Tt)∣→ o
almost surely, where Y (Tt) := supr∈[0,K ]m Xα(Tt, r) and Y (Tt) := supr∈[0,K ]m X(Tt, r).
Because Xα (Tt, r) is convex in Tt, then Y (Tt) = supr∈[0,Ku]m Xα(Tt, r) is convex in Tt. By (Liese
& Miesck , 2008, Lemma 7.75) again, suPrt∈[o,Kt] ∣Yα (Tt) - Y (Tt) | → 0 almost surely. A similar
argument shows that
inf Yα (Tt)	- inf Y (Tt)	= inf sup	Xα (Tt,	r) - inf sup	X(Tt, r)	→ 0
rt∈[0,Kt]	rt∈[0,Kt]	rt∈[0,Kt] r∈[0,Ku]m	rt∈[0,Kt] r∈[0,Ku]m
almost surely.
30
Under review as a conference paper at ICLR 2022
Therefore, we obtain
四部 Otk2 g1u+kuk2 g>t-g>t-2 UTATu+2 nλ ktk2
inf sup
0<rt<Kt 0<ri<Ku
(-∖PX*Tt + Tt X
∖ V i∈[m]	i∈[m]
kg1,ik2 r. - 1 X ɪr2 + 1 λr2
√n	'	2 4' + 2 t
a.s.
→ inf sup
0<rt<Kt 0<ri<Ku
-rt ∕ι+ E r2 + rt Σ √ZT'
i∈[m]	i∈[m]
-2 x
i∈ [m]
(43)
:μ .
Define event
Aa =	< I inf	sup	(UTZt	— gτt	— UUTA-IU + ɪnʌ	IItIl2)	—	μ	>	τ >	,
l∣t∈Rn	u∈Rd∖	2	2	"27	J
Ba =	< I inf	sup	UUTZt	— gτt	— JUTA-IU + ɪnʌ	∣∣tk2 )	—	μ	>	τ 1	,
Ut∈St	u∈Su∖	2	2	)	J
Ca ={ J tinft sup (∣t∣2 g^U + ∣∣U∣2 91t — gτt — 2UTATU + 2nλ |用。一μ > τ j .
Recall
Ea =	< inf	sup	(UTZt — gTt — JUTA-IU + ɪnʌ ∣∣t∣2 J	= inf sup (UTZt — gTt — JUTA-IU +	ɪnʌ	∣∣t∣2
t∈Rn u∈Rd∖	2	2	11 "27 t∈St u∈"∖	2	2	11 1l2
We have Aa ∩ Ea ⊆ Ba. Equation (43) gives lima P {Ca} = 0 for any τ > 0 because almost
sure convergence implies convergence in probability. By the convex Gaussian min-max theorem
(Thrampoulidis et al., 2015), we have
P {Ba} ≤ 2P {Ca}.
It follows that
P {Aa} ≤ P {Aa ∩ Ea} + P {E£} ≤ P {Ba} + P {E£} ≤ 2P {Ca} + P {Ea}.
Taking limSUPa on both sides, because lim SUPa P {Ba} ≤ 2 lim SUPa P {Ca} = 0, we get
lim sup P {Aa} ≤ lim sup P {Ea} ≤ P l lim sup Ec	= 0 ,
aa	a
where the second inequality is because of the reverse Fatou,s lemma. Thus
inf sup
t∈Rn u∈Rd
UTZt — gTt —
2UTATU + 2nλ ∣t∣2) → μ .
Therefore, we deduce
gτNTg 3 —2 inf sup
0<rt<Kt 0<ri<Ku
—rt A + X r2+ Tt X √zr — 2 x
∈ [m]	i∈ [m]	i∈ [m]
sup inf
0<rt<Kt 0<ri<Ku
∣2rt /1 + X r2 - 2rt X √Zr, + X '2
i∈ [m]	i∈ [m]	i∈ [m]
sup inf "(rt, r, λ).
0<rt<Kt 0<ri<Ku
(44)
BeCauSe I gTNTg∣ ≤ * ∣∣g∣2 and E白 B∣2 = 1 < ∞, by the dominated convergence theorem
for convergence in probability (Cohn, 2013, Proposition 3.1.6), we get
limtr N-1 = limEa |gTN-1g] = max min "(rt, r, λ).	(45)
a	a g	0<rt<Kt 0<ri<K ` t','	' '
31
Under review as a conference paper at ICLR 2022
Note that 2rt J1 + Pi∈[m]r2 is convex in r, -2rt Pi∈[m] √zri is linear in r, and Pi∈[m] λir2
is strongly
2rt Pi∈[m]
convex in r.
Thus H is strongly convex in r.
Note that 2rt J1 + Pi∈[m]r2 -
√Zifri is linear in rt and that -λr2 is strongly concave in r» Thus H is strongly concave
in rt. Then H has a unique saddle point (rt, r*) on [0, Kt] X [0, Ku]m that satisfies
max min H (rt, r) = min max H (rt, r) = H (rt, rt) ,	(46)
rt∈[0,Kt]r∈[0,Ku]m	r∈[0,Ku]m rt∈[0,Kt]	t
where the first equality is due to Sion’s minimax theorem.
Since Itr NTl ≤ ɪ, using the dominated convergence theorem and combining Equation (45) and
Equation (46) yields
limE tr N-1 =	max min H(rt, r, λ)	= min max H(rt, r, λ)	.
α	0≤rt≤Kt	0≤ri≤Ku	0≤ri≤Ku	0≤rt≤Kt
By the uniqueness of the limit, the right-hand side max0≤rt≤Kt min0≤ri≤Ku H(rt, r, λ) and
miηο≤ri≤κu maxο≤rt≤Kt H(rt, r, λ) do not depend on Kt and Ku as long as Kt ≥ 2andKu ≥
2λ+(2+√γ),
---∖	). Thus We have
limEtr N-1 = max min H(rt, r, λ) = min max H(rt, r, λ) .
α	rt≥0 ri≥0	t	ri≥0 rt≥0	t
If rt = 0, then H (0, rt) = mi□r∈[ο,κu]m Pi∈m] 十ri = 0. ThUs rt must be zero. However,
H (2λ, 0)= 4λ > H (0, rt). Therefore rt > 0. We compute the partial derivative
∂H	r	J-	r
=2rt-/	- 2rt√zi + 2 丁 .
∂ri	1 + P r2	λi
1	+	i∈[m] ri
If rit = 0, we have
IHI	= -2rt√zi <0.
∂ri Iri=0,rt=r;
Therefore, one can increase rit and make maxrt∈[0,Kt] minr∈[0,Ku]m H (rt, r) smaller, which results
in a contradiction. Thus rit > 0. Thus the minimax value is attained when rt, ri > 0 for all i ∈ [m].
To obtain the optimality condition, we compute the partial derivatives
∂H
∂rt
∂H
∂ri
2	.∕l+ X ri - 2 (X ri√Zi ) - 2λrt,
i∈[m]	i∈[m]
2rt	ri	- 2rt√Zi + 2?.
1 + Pi∈[m] ri2	λi
Setting them to zero gives the optimality condition for rtt , r1t , . . . , rmt and yields Equation (40) and
Equation (41).
Using the envelope theorem, we get
∂
——max min	H(rt, r, λ)
∂λi rt∈[0,Kt] r∈[0,Ku]m
= ∂H(rt, r*,λι,.∙∙,λm)
=	∂λi
t2
ri
-----
λi ∙
□
Lemma 12. Define N = λnIn + Z>ΛZ. The following equation holds
lim E
n,di →+∞
di∕n→zi
2	m	m	t2
西tr (N-1” = ∂λi∣λ Pinm log [λ+XNPjJ + X (Pj-Zj(Iog zj +1))j =-京,
32
Under review as a conference paper at ICLR 2022
where r* is a solution to SUPrt>0 infr1,...,rm>0 日(rt, ri,..., rm, λι,..., λm) and
^(rt,r1,...,rm,λ1,...,λm)
2rt /1+ X r2 - 2rt X √ziri+ X »2 - λr2
∈ [m]	i∈ [m]	i∈ [m]
Proof. Since
∂	,	-、	， τ C 、	1	, τ 、
西 tr (N-1) = tr (Z>N-2Zi) ≤ 昕"3>Zi)	(47)
and E(工：)2 tr(Z>Zi) N =(by Lemma 4), using the dominated convergence theorem gives
E [∂^ "(N-1)]=会 E tr(NT).
We use α to denote the indices n,d and use limα to denote limn,di→+∞. Define fa(λi) =
di ∕n= zi
E tr (N-1), g(%)=呆 infP∈Rm [l°g (λ + Pi∈[m] λiPi) + Pi∈[m] (PLz (lθg ρi + 1))],
and h(λi) = suprt>0inf∏,...,— >0E(rt,rι,... ,rm, λι,..., λm). Because ∣ tr (N-1) ∣	≤
tr In) ≤ 1 and limα tr (NT) = g(λi) (by Lemma 9), we have Lemma 9 limα fα(λi)=
limα E tr (NT) = g(λi). Lemma 11 shows limα fa(λi) = h(λi). Therefore limα fa(λi) =
g(λi) = h(λi).
Because of Equation (47), we have f[(λi)=衾E tr (N-1) and
∂	,	「∂	,	.、1	∂	,	..	1
Ifaa= KiEtr(NT) = E民tr(NT) ≤E Kitr(NT) <*
and therefore {f } is uniformly bounded for λi. Because
∂ 2	,	、、
可 tr(N一)
=2 tr (NTZiZir NTZiZir N-1)
<λ1n tr (N-1 (ZiZr)2 NT)
= "(ZiZ>N-2ZiZir)
λn
12
≤	3 tr (ZiZi ),
(λn)
and E J)3 tr (ZiZr)2 N 吉(by Lemma 4), using the dominated convergence theorem yields
E
1E 长tr(NT)
∂ 2
狷E [tr(N-1)]= f '(λi).
Moreover, we have
∂2
I f0(λi)I ≤ E -2 tr (N-1) <
网2
1
λ3.
Thus {fa} is uniformly equicontinuous for λi. We want to show that limα fj(λi) = g0(λi) by
contradiction. Assume that it is not true. Then there exists e > 0 and a subsequence {f^k} such
that ∣ f'a k(λi) — g'(λi) ∣ > e. Since f is uniformly bounded and uniformly equicontinuous for
λi ∈ E (E is any closed finite interval containing λi), by the Arzela-Ascoli theorem, there exists
a subsequence {f£ } that converges uniformly on E. Since limα fakr (λi) = g(λi), by ( udin,
1976, Thoerem 7.17), we have
lim f'akr(λi) = g,(λi),
which yields a contradiction. Therefore, we have lim f (λi) = g (λi). Recall g(λi) = h(λi)
for any λi > 0. Then by the final part of Lemma 11, we have lim f (λi) = g (λi) = h (λi) =
—
*2
Ti
K .
□
33
Under review as a conference paper at ICLR 2022
D.4 BIAS
Lemma 13. Suppose that U 〜 ㊉i∈[m] Unif (O (di)) and V are two independent d X d random
matrices such that V =d UVU>, where d = Pim=1 di. Let θ ∈ Rd be a fixed vector. Write
一 θι 1
θ 二 :	, where θi ∈ Rdi .Let
.
θm
Φ 〜M Unif(SdiT da))
i∈[m]
be a random vector independent of V and letΛ = diag (λ1Id1 , . . . , λmIdm) ∈ Rd×d. Then we have
E ]N2Vθ[= E ]卜1/2 vM∙
Proof. Recall UΛU> = Λ and noticing U>θ =d φ, we get
E
=E 卜1/2UVU>θ∣∣2j
=Eθ>UV>U>ΛUVU>θ
=Eθ>UV>ΛVU>θ
=E ||a1/2VU>θ∣∣2]
=E H
□
Lemma 14. Define Θ = diag(∣∣θ1 ∣∣2"ιldι,..., ∣∣θ°n∣∣2"mIdm)	and S =
A1/2Z (nλIn + Z>ΛZ) 1 Z>Λ1/2. Then we have
Bλ,d,n = kθ*k∑ - 2E tr (ΛSΘ) + E tr(SΛSΘ).
Proof. Recall Equation (22) in Lemma 5
Bλ,d,n = E
∣∣Λ1/2 (ld +、A1/2ZZ>A1/2) θ0k2
Let U 〜Li∈[m] Unif (O(di)) be a random matrix independent of Z. Because UZ = Z, we have
Id + -1Λ1/2ZZ>Λ1/2 = Id +2A1/2UZZ>U>Λ1/2 = U (Id +2A1/2ZZ>A1/2) U> .
nλ	nλ	nλ
Define θ 〜Li∈[m] Unif(Sdi-中阳日)).Lemma 13 gives
Bλ,d,n =E kΛ1/2 (Id + nλA1/2ZZ>Λ1/2
-1
θ∣2
=E [∣Λ1/2 (Id- S)训 2]
=E hk(Id- s"∣∣Λ]
=E m∣∣;—E [θ>Λsq — E [在>sλ0 + E [θ>sAsq.
34
Under review as a conference paper at ICLR 2022
Notice that
tr (SΘλ)
θ∣∣2 = ∣∣θ,kΛ and Θ = E R”]
Because Θ commutes with Λ, we have tr
=tr (ΛSΘ). In light of these, we deduce
SΛΘ
Bλ,d,n = ∣∣θ,∣Λ - Etr (ASΘ) - Etr (sΛθ) + Etr (sΛSθ)
=∣∣θ,∣Λ - 2Etr (asΘ) + Etr(SΛSΘ).
□
Lemma 14 expresses the bias Bλ,d,n as the sum of three terms.
Computing ||叫：Note that ||叫：=θ,τΛθ, = Pi∈m] λi Qk；. Therefore,
Iim	llθ,∣Λ = qτ(λ Θ Z).
n,di→十∞
di∕n→zi
k∏iθ*k2→ηi
Computing Etr (λsΘ) Define N = λnIn + ZτΛZ = λnIn + £山加 λiZiZτ. We have
E tr (λsΘ)
=Etr(ZτΛ1∕2θΛ3∕2Z (nλIn + ZτΛZ)-1)
=E tr(ZτΛ2ΘZNT)
=X λ lθ*2 E tr (ZiZirN-1)
--	dd
i∈[m]
Xλ2 帆 ∣2 nE ∣⅛ ： iogdetN
-∈[m]	L
X λikθik2^rT^-E ɪlogdet~
ʌ-	di ∂λi |_n	n
where the second inequality is because Θ commutes with Λ3/2 and the final equality is because of
Equation (38). Taking lim n,di→+∞ and using Lemma 10 gives
di∕n→zi
k∏iθ*k2→ηi
lim E tr(ASΘ) = X λiηi- ɪ inf log ∣ λ + X λiρi ∣ + X (Pi- & (log — + 1))
n,di→+∞	∖ z — Zi ∂λi ρ∈Rm	∖	—	/ — z	z Zi	7 J
di∕n→%	i∈[m] i	+ L ∖ 记[沱)川a	'	JJ
IIniθ* k2→η
Using the envelope theorem yields
ɪ inf
∂λi ρ∈Rm
log λ + ^X λiPi	+ ^X (Pi - zi (log 也 + 1))
i∈[m]	i∈[m] '	'	&	/ /
P↑ = Zi -窿
λ + Pi∈[m] λiρi	λi
where the final equality is because of Equation (6) in Item 1. Therefore, we deduce
lim Etr (ΛSΘ) = X λ2η2^P = X %褚(1 -邑)=qτ (λ Θ (z -ρ*)).
n,di→+∞	/ Zi	λi	it	Zv
di∕n→zi	i∈[m]	i∈[m]
ll∏iθ* k2→ηi
35
Under review as a conference paper at ICLR 2022
Computing E tr (S ΛSΘ) We have
E tr(SΛSΘ)
=Etr ∣Λ1∕2ZNTZ>Λ2ZNTZ>Λ1∕2θ]
=Etr [Z>A1/2SA1/2ZNTZτΛ2ZN-1 ]
=Etr [ZτΛθZNTZτΛ2ZNTi
=X λikθ0k2 X λ]E tr [ZiZjN-1ZjZjNT]
.∙,	di
i∈[m]	j∈ [m]
-X T X 印[$IlogdetNi
HIi	∣t√∕∖jt√∕∖i In	In
i∈[m]	j∈[m]	LJ	」
where the third equality is because Θ commutes with Λ1/2. Taking lim n,di→+∞ and using
di∕n→zi
∣∣∏iθ* k2→ηi
Lemma 10 gives
lim
n,di→十∞
di∕n→zi
k∏iθ*∣2→ηi
E tr(SΛSΘ) = - X 乎 X λ
i∈[m]	j∈ [m]
占inf
∂λj ∂λi ρ∈Rm
log ∣λ +∑ λ,ρJ + E Pl-z，
l∈[m]	l∈[m] '
Write λ = (λι,..., λm)τ and Z = (zι,..., zm)τ. Let ρ* ∈ Rm be a minimizer of Equation (5)
and J =褰 ∈ RmXm be the Jacobian matrix Jij =釜.Recall Item 2
(diag(λ) + (λ + λ>p*) Im -(Z - P* ) λ>) J = (z - P*) P*> - diag(p").
Using the envelope theorem, we have
ɪ inf log J λ + X	λιρι	j + X	(p，- zι	(log 巴 +	1))	=、工 P Pi-L =、工Pi τ	* .
a%ρ∈Rm[	1	ι∈⅛]	l	ι∈⅛]V	1	zι	))\	λ + ∑ι∈M λιρ*	λ + λτp
Recall Equation (6) yields
Pi	= Zi- Pi
λ + λτρ*	λi
Differentiating the above equation with respect to λj gives
a'a inf	log I λ + X λiρι I + X ρPl - ZI (log — + 1))
"mi、]<	ι∈m]	j	∈m^	V	ZI	/
=d Zi - P*
∂λj	λi
_ -λijij - (Zi - P* ) δij
=	λ	.
It follows that
lim E tr (SΛSΘ
n,di→+∞	∖	.
di∕n→zi
k∏iθ*∣2→ηi
X T X λ
i∈[m]	j∈[m]
λi jij + (Zi - ρi ) δij
λ
—(Jij+-)
=qτ(λ Θ (z - ρ*) + Jλθ2)
36
Under review as a conference paper at ICLR 2022
Putting all three terms together, we have
lim	Bλ,d,n =	q> (λ Θ	z)-2q>	(λ	Θ (Z — ρ*))+q> (ʌ Θ	(Z	— ρ*)	+ JAθ2) =	q>	(A Θ	ρ*	+	Jλθ2).
n,di→+∞
di∕n→zi
∣∣∏iθ* k2→ηi
Since {B∖,d,n} is uniformly bounded and uniformly equicontinuous for λ ∈ (0,1] by Lemma 5,
{Bλ,d,n} can be extended continuously to [0,1] and the family of extended functions is still uni-
formly bounded and uniformly equicontinuous for λ ∈ [0,1]. By the Arzela-Ascoli theorem,
{Bλ,d,n} converges uniformly to the limit. By the Moore-Osgood theorem, we can exchange the
two limits lim n,di→+∞ and limλ→0+ and get
di∕n→zi
k∏i θ*∣2→ηi
lim	B0,d,n = lim lim Bx,d,n = lim lim	Bx,d,n = q> (λ Θ ρ* + JAθ2) ∣λ=o .
n,di→+∞	n,di→+∞ λ→0+	λ→0+ n,di→十∞
di/n→zi	di/n→zi	di/n→zi
k∏iθ* ∣2→ηi	k∏iθ* ∣2→ηi	k∏iθ* ∣2→ηi
D.5 VARIANCE
Define N = nλIn + Z>ΛZ. Recalling Lemma 6 gives
Vλ,d,n
:σ2E∣∣ΛZN -1∣∣2
m
σ2 X A2etr (ZiZ>N-2)
i=1
m
-σ2
∑>2E U tr (n-1).
'	∂λi	_
i=1
Using Lemma 12, we get
lim Vχ,d,n
n,di→∞
di∕n→zi
m
-σ2∑ λ
i=1
lim E
n,di→∞
di∕n→zi
m
-σ2 X λ
i=1
lim E
n,di→∞
di∕n→zi
m	∂2
―σ2 X R ∂⅛ ρ∈⅛
i=1	+
m
λ + £ NPj
j=1
Using the envelope theorem, we deduce
∂ 削log (λ+X λj P)+ X (Pj-Zj (log Pj+ι)) = λ+PL j
Then we take 衾 and obtain
∂2	J (
E *[log C+
∂	1
log
m
+ X (Pj-Zj (log Q
mm
X λj Pj I + X(Pj - Zj (IOg Zj + 1)
dλi λ + Pm=I λj Pj
(λ + Pj∈[m] λjPj)
37
Under review as a conference paper at ICLR 2022
As a result,
lim	Vλ,d,n
n,di →∞
di∕n→Zi
σ2
m
Xλi2
i=1
P； + Σj∈[m] λ Jji
(λ + Pj∈[m] λjPi)
2 (λ'2)>(ρ* + JTλ
(λ + λ>ρ*)2
By Lemma 12, the variance is given by
m
lim	Vλ,d,n
n,di →∞
di∕n→zi
lim -σ2
n,di →∞
di∕n→zi
∑λ2 E ∂T "(N-1)
∂λi
i=1
m
σ2 XrJ
i=1
where r* solves
sup inf
rt >0 r1 ,...,rm >0
∣2rt ∕l + X r2 - 2rt X "，『，+ X (『2
i∈[m]	i∈[m]	i∈[m]
Since {Vλ,d,n} is uniformly bounded and uniformly equicontinuous with respect to λ∈ (0, 1] by
Lemma 5, {Vλ,d,n} can be extended continuously to [0, 1] and the family of extended functions
is still uniformly bounded and uniformly equicontinuous. By the Arzela-Ascoli theorem, {Vλ,d,n}
converges uniformly to the limit. By the Moore-Osgood theorem, we can exchange the two limits
limn,di→∞ and limλ→0+ and get
di∕n→zi
lim lim Vλ,d,n = lim lim Vλ,d,n
n,di→∞ λ→0+	λ→0+ n,di→∞
di ∕n→zi	di ∕n→zi
m
σ2 X 琮2 ∣λ=0 .
i=1
E Proof of Theorem 2
We use Theorem 1 to prove Theorem 2. As in Theorem 1, let r* solve min%≥o maxrt≥o H(『t, r, λ),
where H is defined in Equation (7). Note that H is a quadratic function of『t. Define A =
qPi∈[m] r2 + 1, B = Pi∈[m] √ziri, A* = qPi∈[m] r*2 + 1, and B* = Pi∈[m] √zr* ∙ Then
r* = a-b and We get
(A-B)2
min max H(『t, r, λ) = min	+
ri≥0 rt≥0	ri≥0	λ
X λ『2)= m≥n (T + X λ『2
i∈[m] i	i	i∈[m] i
Taking the partial derivative with respect to 『i gives
A (T+X E)=2 ∙ A-B( r√⅛+2 ∙ ⅞.
i	i∈[m] i	i
Setting it to zero gives the optimality condition for 『i* :
It follows that
T (?-W=-三
i ∈ [m] .
(48)
A^ - √zi _ r* /λi
—：-----------.
A -√j	6
i, j ∈ [m] .
Some algebraic manipulation in the above equation yields
『i _ λi	√z*A* -『i
—=—•—：------
『*	λ	√zj A* -『*
i, j ∈ [m] .
38
Under review as a conference paper at ICLR 2022
Define Z = (zι,...,zm). Then ∣∣z∣∣ι = ∑2i∈[m] z》By CaUchy-SchWarz inequality, if d/n →
i∈[m] zi < 1
B ≤ IX Zi krk2 < krk2 < Jkrk2 + 1 = A ∙
i∈[m]
Thus there does not exist r such that A = B. If d/n → Pi∈[m] zi > 1, then A = B is feasible for
r. For example, set
1
r =—,	:
P(kzkι- 1)kzk
=√z .
1
We have
B=E ,√=MLI
A=q1+krk2=1++
k^1 = B.
If kzk1
>
1, since A = B is feasible, then
lim min (A - B)) + X
λ→0+ ri≥0	λ
i∈[m]
λr2
λi
min
Ari=≥B0 i∈[m]
1	2
λi ri.
1
If kzk1
<
1, then A - B always holds. To be precise, we have
A — B ≥ (Jkrk2 + 1 -kr
1,then(1 - p∕kzk1) krk2 > 1-√kzk1∙ If krk2 ≤ 1,then Jkrk2 + 1 -krk2 ≥ √2-L
If krk2
>
Thus there exists a universal constant Co =(1 - ʌ/kzkj ∨ (√2 - 1)> 0 such that
A - B ≥ C0 .
Recall Equation (48). We have
(A* - B*)
—
λr*
七，i∈ [m].
Taking limλ→0+, since A* - B* ≥ C0 does not go to zero, we have
.*
A
i ∈ [m] .
Then we get
r*2
1+ pj∈[m]r*2 =zi, i∈ [m].
Summing all i ∈ [m] yields
P	r*2
kzk1 =1+pm] ir*2 .
1 +	i∈[m] ri
Therefore, we have
lim
n,di →+∞
di∕n→Zi
V0,d,n
σ2
lim X r*2 = σ2 j] Zi
λ→0+ i=1	1 - i∈[m] zi
39
Under review as a conference paper at ICLR 2022
F Proof of Theorem 3
Define A* = JPi∈[m] .♦ + 1 and B* = Pi∈[m] √iW∙ Equation (15) in Theorem 2 yields
若 _ λι √Z1A* - r*
—=----------：-----.
r λ2 √z2A* - r
Using the constraint A* = B*, we get
r* _ λι (√Z1B*-r*)
—=-----；---------.
r2	λ (√Z2B*-r2)
Define q = r⅛. We have the following equation
r2
λι (q(ZI - I) + √z1z2)
λ2 (q√Zl Z2 + Z2 - 1)
Solving the above equation yields
q
λ1 (ZI - 1) + λ2 (I - z2) + -∖∕(λ1 (z1 - 1) + λ2 (1 - z2)) 2 + 4λ1λ2z1z2
2λ2 √Z1Z2
(49)
Here we discard the negative root. Let x = r1*2 + r2*2 = r2*2 1 + q2 . ?? yields
1 + X = r22 (q√Zι + √Z2)2 = 1 ; q2 (q√Zι + √Z2)2 .
Solving x from the above equation gives
q2 + 1
x = -ɪ-----：-------——---------.
q2(zι - 1) + 2q√zιZ2 + Z2 - 1
Therefore,
lim	V0,d,n
n,di →+∞
di∕n→Zi
____________q2 + 1__________
q2(z1 - 1) + 2q√zιZ2 + Z2 - 1
G Proof of Theorem 4
Instead of considering the θ* specified in Equation (18), we first consider a Bayesian setting where
θ* 〜N(0, dId). Later, We will show that the setup in Equation (18) is asymptotically (as di → ∞)
equivalent to this Bayesian setting. The precise meaning of equivalence will also be presented later.
Our strategy can be divided into two steps. The first step is to show that the Bayes risk of the
Bayes estimator is monotonically decreasing in the sample size n. The second step is to translate
the sample-wise monotonicity of the Bayes estimator to the excess risk of the optimally regularized
estimator θλ,d,n in the setup of Equation (18).
Recall that since we are interested in sample-wise monotonicity, we add a subscript n to X and y
(they are defined by Equation (1) in Section 1.1) to emphasize that they consist of n data items. In
this Bayesian setting, the likelihood function of θ* is
Σ
L(θ* | Xn,yn) =	L(θ* |xi,yi)
i∈[n]
Z exp
i∈[n]
Iyi-g*
2σ2
exp
kXnθ*- ynk2
2σ2
The density of the prior of θ* is proportional to exp (-d ∣∣θ*∣∣2). Therefore, the posterior density
of θ * is given by
p (θ* | Xn, yn) Z exp
dkθ*k2
kXnθ*- ynk2
2σ2
2
—
40
Under review as a conference paper at ICLR 2022
As a result, the posterior distribution of θ* is Gaussian. The Bayes estimator is
% ∕ττ ∖	∙ τm	UCC *l∣2
θBayes (Xn,Yn) = arg min Eθ*〜p(θ* ∣Xn,y" ∣∣θ - θ ∣∣∑ .
θ
Taking the derivative with respect to θ gives
∂2
∂θ Eθ* 〜p(θ*Xn,yn) llθ — θ k∑ = 2夕(0 - θ ) .
Setting the above equation to zero yields Σ (©Bayes (Xn,yn) - Eθ*〜p(θ*∣xn,yn)θ*) = 0 and there-
fore
%	∕ττ ∖πn	八*	πn「八 *11 T	1	♦
θBayes (Xn,yn) = Eθ*〜p(θ* ∣Xn,yn)θ = E [θ | Xn, yn]=argmin
θ
The final equality is because the posterior mean of a Gaussian distribution equals its mode.
Define the Bayes risk
Rn , Eθ* 〜N (0, 11d ),Xn,yn ]SBayeS (Xn , yn) 一 θ
Write X = Rd and Y = R. Define
Rn , M 吗	Jθ* 〜N (0, d Id),Xn,ynllθ(Xn, yn)-θ*∣∣2.
θ:X n×Yn→R	11	iiς
(d∣θ*k2+ I"。2 ynk2
We have
Rn = θ:Xn×Yn→Rd Eθ*~N@ 1 Id),Xn,yn |口闯(羽，yn) - ^S*
2
2
θ:Xn×YLRd Eθ*~N(0, IId),B MS，yn) - ^S*
2
2
Eθ* 〜N (0, d Id),Xn,yn 胆[A4* । Xn yn] - Ne*
2
Eθ* ~N(0, d Id ),Xn,yn
Rn .
2
2
∑1分θBayes (Xn, An)- ∑1^θ*
2
where the third equality is because the conditional expectation minimizes the `2 loss. Next, we want
to show that Rn+1 ≤ Rn, i.e., the Bayes risk of the Bayes estimator is monotonically decreasing in
the sample size n.
Rn+1
inf
θ:X n+1×Yn+1→Rd
Eθ* 〜N (0, d Id),Xn + 1,yn+1
M(Xn+1, yn+1) - θ* 卜
inf
θ:X n×Y n→R
Eθ*~N(0, d Id),Xn+1,yn+1
∣∣θ(Xn, yn)-θ*ll2
≤
Then We want to show that Rn equals the Bayes risk of the optimally regularized estimator θλ,nd
Rn = λ≥0 Eθ* 〜N (0, d Id),Xn,yn
∣∣θλ,n,d-θ*∣∣∑.
SinCe Rn =inf ^:X n×γn→Rd Eθ * ,X“ ,y“ 1 ] θ(Xn, An) - θ*∣∣ς , We get
Rn ≤ λ≥0 Eθ*~N (0, d Id),Xn,yn
∣∣θλ,n,d-θ*∣∣∑.
41
Under review as a conference paper at ICLR 2022
On the other hand, recalling θBayes (Xn,yn) = argmi□θ (d∣∣θ∣∣2 + kXnWynk2) = θy nd and
n ,,
Rn = Eθ* 〜N (0,11d ),Xn,yn Mayes (Xn, yn) - θ* |[ , we deduce
Rn ≥ λ≥f0 Eθ* 〜N (0, d Id),Xn,yn
M,n,d-θ*∣∣∑.
Therefore we deduce Rn
infλ≥0 Eθ* 〜N (0, d Id),Xn,yn
,n,d
-啡
As a result, we establish
sample-wise monotonicity of the Bayes risk of optimal regularized θʌ,n,d:
2
Rn+1 = λ≥f0 Eθ*,Xη+ι,yη+ι ∣∣θλ,n+1,d - θ*
≤ λi≥0 Eθ*,Xn,yn M,n,d — θ*R = Rn .	W)
Σ
In What follows, We show that if θ* is given by Equation (18), the excess risk of θʌ,n,d is asymptoti-
cally equal to its Bayes risk when θ* 〜N(0, dId):
lim
di→∞
EXn,yn ∣∣θλ,n,d - θ*∣∣∑ - Eθ*〜N(0,1 Id)EXn,yn ∣∣θλ,n,d - θ*∣∣∑
We abuse the notation in the above equation. The θ* in Eχn,yn
θλ,n
-θ*∣∣Σ
0.
satisfies Equation (18),
while the θ* in E&* 〜N ”, 11d)EXn5 ∣∣θλ,n,d - θ*∣∣ follows a normal distribution N (0, d Id). By
Lemma 5 and Lemma 6, if Σ = PAP> and θ0 = P>θ* are as defined in Table 1 (where P is an
orthogonal matrix and A = diag(λιId1,..., λmIdm) ∈ Rd×d is a diagonal matrix), for fixed θ* we
have
-1
EXn,yn ∣∣θλ,n - θ* , =EXn,yn kA1/2 (Id + n1λA1/2ZZ>A1/2)	θ0k2
+ σ2Eχn,yn [∣∣AZ (λuIη + ZAZ)-1 k2i ,
where every entry of Z ∈ Rd×n follows i.i.d. N(0,1). If θ* 〜N(0, dId), we have θ0 〜N(0, dId).
Since the variance term σ2Eχn,yn [kAZ (λnIn + Z>AZ) 1 |同
variance terms cancel out and we get
does not depend on θ*, the two
EXn,yn ∣∣θλ,n,d — θj∣∑ — Eθ* 〜N (0, 1 Id)EXn,yn M,n,d - θ[∑
=Eχn,yn	kA1/2 (Id + nλA1/2ZZ>A1∕2) 1 θ0k2
-EXn,yn,θ0 〜N (0, 1 Id )	M1/2 (Id + nλ A1/2ZZ >A1/2)	θ0k2
For U 〜φi∈[m] Unif (O (di)), we have
(Id +±A1∕2ZZ>A1∕2)	= (Id +±A1∕2UZZ>U>A1/2)	= U (Id + 3A1∕2ZZ>A1/2)	U> .
nλ	nλ	nλ
By Lemma 13, for θ* (and thereby θ0) specified in Equation (18), we get
Eχn,yn	kA1/2	(Id +	n1λA1/2ZZ>A1∕2)	1 θ0k2	= Eχn,yn,φ	(Id	+ n^A^ZZ>A1∕2)	1 φk2	,
where
Φ 〜M Unif(Sdi-1 (kθik2)) = M Unif 卜di-1 (PdTd)).
i∈[m]	i∈[m]
42
Under review as a conference paper at ICLR 2022
In the Bayesian setting, if θ0 〜N(0, dId), then U>θ0 〜N(0,11d). We have
Eθ0〜N(0,dId),Xn,yn llθλ,n - θ*Hς = Eθ0〜N(0,11d),Xn,yn ]卜 1/2U (Id + nλA1∕2ZZ>A1/2)	U>θ0∣
EXn,yn,ψ	Λ1/ (Id + nλ Λ1/ZZ >A1/2) 1 ψ∣
where ψ = U>θ0 〜N(0, dId).
Next, We want to couple φ and ψ. Let Sii蚓 Unif (SdiT(1)), hi i.吟 χ2(di), and define
,hι∕dsι
ψ =	.
.
_ Vzhm∕dsm
We have kφk2 = 1 and
By the strong law of large numbers, limdi→+∞ hi∕di = 1 almost surely. Thus we get
limdi→+∞,di∕d→νi kΨk2 = VZPm=I Vi and limdi→+∞,di∕d→Vi kΦ - Ψk2 = 0 almost surely (re-
call that we will let di → +∞ and di ∕d → νi for some constant νi > 0. ). Because
∣∣Λ1∕2∣∣2 . 1 and ∣∣(Id + nλΛ1∕2ZZ>Λ1/2)-1∣∣ ≤ kIdk2 = 1, we bound the norm of
Q , A1/2 (Id + n1λA1/2ZZ>A1/2)-1 as follows
kQk2 ≤ ∣∣Λ1
Id + L1/2ZZ >A1/2)
nλ
.1.
2
It follows that
EXn,yn,φ hkQφk22i -EXn,yn,ψ hkQψk22i
≤EXn,yn,φ,ψ kQφk22 - kQψk22
= EXn,ynφ,Ψ (kQφk2 + IIQψk2) lkQφk2 - kQψk2l
.EXn,ynφ,ψ [(kφk2 + kψk2) kQ(φ - ψ)k2]
.EXn,ynφ,ψ Iφ- ψI2 ,
where the last inequality is because IφI2 + Iψ I2 . 1 for all sufficiently large di . We know that
limdi→+∞,di"→νi kΦ 一 Ψk2 = 0 almost surely. To apply Lebesgue,s dominated convergence
theorem, we need to find a dominating integrable random variable. In fact, 1 + Iψ I2 dominates
Iφ - ψI2:
Iφ - ψI2 ≤ IφI2 + IψI2 = 1 + IψI2 .
It is integrable because E ∣∣ψ∣2 = E Jjχ2dd)j ≤ JE[xd(d)] = 1.
dominated convergence theorem yields
Application of Lebesgue’s
lim	EXn,yn,φ hIQφI22i -EXn,yn,ψ hIQψI22i =0.
di—+8,di/dTVi I	L	」	L	」I
43
Under review as a conference paper at ICLR 2022
Therefore, we conclude that
lim
di →+∞,di /d→νi
EXn,yn llθλ,n - θ*U∑ - Eθ*〜N(0,11d)EXn,yn (θλ,n,d - θ]∑
0
and this convergence is uniform in n and λ ∈ (0, ∞). It follows that
lim
n,di →∞
n∕di→Yi
EXn»n lθλ,n,d - O*1 - Eθ* 〜N (0, d Id)EXn,yn Hθλ,n,d - θ*H∑
0
and this convergence is uniform in λ ∈ (0, ∞).
By Lemma 8 (the proof is similar when we replace α → +∞ by n, di → ∞, n/di → γi), we have
lim
inf EX
n,di →∞ λ>0
n∕di→Yi
一 n,ynlR,n,d-θ*llΣ - λ>0 Eθ* 〜N (0, d Id)EXn,ynllθλ,n,d-θ*llΣ∣=0.	(51)
Define fα(λ) = Eθ*〜N(0,11d)EXn,yn (θi,n,d - θ* ll£. We USe α to denote the indices n, di. By
Lemma 5 and Lemma 6, we have
Eθ* 〜N (0, d Id)EXn,yn llθλ,n,d - θ*
.Eθ*〜N(0,dId) kθ*k2 = 1.
Therefore {fα (λ)} is uniformly bounded for λ > 0. Since
Eθ*〜N(0,dId) kθ*k2 = 1,wehave
dλ Eθ* 〜N(0, d Id)EXn ,yn llθλ,n - θ*
dλ Exn,yn M,n - O*/
. kθ*k22 and
Eθ* 〜N (0,1 Id) dɪ EXn,yn lθλ,n - O1 j . 1 .
As a result, {fα(λ)} is uniformly equicontinuous for λ > 0, and in particular λ ∈ (0, M] for any
M > 0. Therefore {fα(λ)} can be extended continuously to [0, M] and the family of extended
functions is still uniformly bounded and uniformly equicontinuous. Recall that if θ* 〜 N(0,11d),
We have θ0 〜N(0, dId). As in Equation (19), write θ0 in a row-partitioned form
/ θ1 ∖
θ0
Om0
where θi ∈ Rdi. Then ∣∣∏iθ*∣∣2 = ∣∣θi∣∣2 〜ʌ/ɪɪ(fi) = JXddi) ∙ d → √V as n,d → +∞
and n/di → γi, where Vi = (Yi Pj∈[m] γj)	. By Theorem 1, {fα(λ)} converges pointwise, say,
to h(λ, γ1, . . . , γm ). By the Arzela-Ascoli theorem, limα fα(λ) = h(λ, γ1, . . . , γm ) uniformly on
λ ∈ [0, M]. Therefore, as n, di → ∞ and n/di → γi, by Lemma 8, we have
λ∈infM] Eθ*~N (0, IId)EXn,yn M,n - θ* k 一 λ∈in,M] h(λ, γι,..∙, Ym).
Recalling 0Bayes (Xn,yn) = argminθ (d ∣∣θ∣∣2 + kXnMynk2) = θσ2d,n and
Rn = Eθ* 〜N(0, 1 Id ),Xn,yn
es (Xn, yn) - θ*lll2Σ = λin≥f0
Eθ*~N(0, 1 Id),Xn,yn Ml，"” - θ*
For all M > CM := 2σ2 Pi∈[m] 1 ≥ σnd (recall n → Pi∈[m] 1), we have
λ∈inM]Eθ*~N (0, 1Id),Xn,yn llθλ,n,d - θ*∣lΣ = λ≥0 Eθ* 〜N (0, dId)EXn,yn llθλ,n,d - θ*
→ λ∈i[n0,fM]h(λ,γ1,...,γm).
2
Σ
2
Σ
2
Σ
2
Σ
44
Under review as a conference paper at ICLR 2022
The uniqueness of limits implies that infλ∈[0,M] h(λ, γ1 , . . . , γm) is independent of M as
long as M > σ2. As a result, if M > CM, we have infλ∈[0,M] h(λ, γ1 , . . . , γm) =
inf λ≥0 h(λ,γ1, . . . ,γm), which yields
λ≥0 Eθ* 〜N (0,11d ),Xn,yn M,n,d - θ*。→ λ≥0 h(λ, Yl,...,Ym) .	(52)
Equation (50) implies infλ≥0 h(λ, γ1, . . . , γm) is decreasing in every γi. Combining Equation (51)
and Equation (52) gives
λ≥0 Eχn,yn ∣∣θλ,n - θ*∣∣Σ → λ≥0 h(λ, Yl,...,Ym).
45