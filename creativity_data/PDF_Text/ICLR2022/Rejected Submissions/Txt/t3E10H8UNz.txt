Under review as a conference paper at ICLR 2022
Transferring Hierarchical Structure with
Dual Meta Imitation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Hierarchical Imitation learning (HIL) is an effective way for robots to learn sub-
skills from long-horizon unsegmented demonstrations. However, the learned hi-
erarchical structure lacks the mechanism to transfer across multi-tasks or to new
tasks, which makes them have to learn from scratch when facing a new situation.
Transferring and reorganizing modular sub-skills require fast adaptation ability
of the whole hierarchical structure. In this work, we propose Dual Meta Imita-
tion Learning (DMIL), a hierarchical meta imitation learning method where the
high-level network and sub-skills are iteratively meta-learned with model-agnostic
meta-learning. DMIL uses the likelihood of state-action pairs from each sub-skill
as the supervision for the high-level network adaptation, and use the adapted high-
level network to determine different data set for each sub-skill adaptation. We
theoretically prove the convergence of the iterative training process of DMIL and
establish the connection between DMIL and the Expectation-Maximization algo-
rithm. Empirically, we achieve state-of-the-art few-shot imitation learning perfor-
mance on the meta-world (Yu et al., 2019b) benchmark and comparable results on
the Kitchen environment.
1 Introduction
Imitation learning (IL) has shown promising results for intelligent robots to conveniently acquire
skills from expert demonstrations (Zhu et al., 2018; Peng et al., 2018). Nevertheless, acquiring skills
from long-horizon unsegmented demonstrations has been a challenge for IL algorithms, because of
the well-known issue of compounding errors (Ross et al., 2011). This is one of the crucial problems
for the application of IL methods to robots since plenty of manipulation tasks are long-horizon.
Hierarchical Imitation Learning (HIL) aims to tackle this problem by decomposing long-horizon
tasks with a hierarchical model, in which a set of sub-skills are employed to accomplish specific
parts of the long-horizon task, and a high-level network is responsible for determining the switching
of sub-skills along with the task. Such a hierarchical structure is usually modeled with Options
(Daniel et al., 2016; Krishnan et al., 2017; Jing et al., 2021) or goal-conditioned structure (Le et al.,
2018). HIL expresses the nature of how humans solve complex tasks, and has been considered to be
a valuable direction for IL algorithms (Osa et al., 2018).
However, most current HIL methods have no explicit mechanism to transfer previously learned skills
to new tasks with few-shot demonstrations. This requirement comes from that the learned hierarchi-
cal structure may conflict with discrepant situations in new tasks. As shown in figure 1(a), both the
high-level network and sub-skills need to be transferred to new forms to satisfy new requirements:
the high-level network needs new manners to schedule sub-skills in new tasks (for example, calling
different sub-skills at the same state), and each sub-skill needs to adapt to new specific forms in new
tasks (for example, grasping different kinds of objects). Quickly adapting to new tasks can signifi-
cantly increase the generalization ability of HIL methods and make them be applied to a wider range
of scenarios, thus an appropriate approach needs to be exploited to endow HIL with such kind of
ability with few-shot new task demonstrations.
1
Under review as a conference paper at ICLR 2022
(a) Illustration of the transfer problem of HIL in new tasks.
Adaptable Monolithic Network
(b) Comparison ofMIL and DMIL.
Figure 1: (a) Both the high-level network and sub-skills need to be transferred to new tasks. Above:
when the robot arm is over a half-open drawer, the task can be either opening or closing the drawer,
which requires the high-level network to call different sub-skills. Below: a same sub-skill pick-place
may exhibit different specific forms in new tasks. (b) DMIL aims to integrate MAML into HIL with
a novel iterative optimization procedure that meta-learns both the high-level network and sub-skills.
Recently, meta imitation learning (MIL) (Yu et al., 2018b; Finn et al., 2017b; Yu et al., 2018a)
employ model-agnostic meta-learning (MAML) (Finn et al., 2017a) to imitation learning procedure
to enable the learned policy to quickly adapt to new tasks with few shot demonstrations. MAML
first fine-tunes the policy network in the inner loop, then evaluates the fine-tuned network to update
its initial parameters with end-to-end gradient descent at the outer loop. This inspires us to integrate
MAML into HIL to transfer the hierarchical structure. However, most MIL methods meta-learn a
monolithic policy in an end-to-end fashion, which does not conform to the iterative training of the
bi-level structure in HIL, where the training supervision of each level comes from the other level.
Integrating MAML into such a self-supervised bi-level optimization procedure is a novel problem.
The main challenge comes from how to schedule the inner loops and outer loops of the meta-learning
process of bi-level modules in HIL to ensure convergence.
In this work, we propose a novel hierarchical meta imitation learning framework called dual meta
imitation learning (DMIL) to successfully incorporate MAML into the iterative training process of
HIL, as shown in figure 1(b). We make the high-level network and sub-skills as mutual supervision
during the bi-level MAML process: the likelihood of each state-action pair in few-shot demonstra-
tions of each sub-skill can provide the supervisions for the meta-training of high-level network, and
the fine-tuned high-level network in turn determines the data sets for the meta-training of each sub-
skill, like a general EM process. We further design an elaborate training procedure for DMIL that
first fine-tunes the high-level network and sub-skills in sequence at inner loops, then meta-updates
them simultaneously at outer loops. We theoretically prove the convergence of this special training
procedure by leveraging previous results from Ravi & Beatson (2019); Finn et al. (2018); Zou &
Lu (2020) to reframe both MAML and DMIL as hierarchical Bayes inference processes and get the
convergence of DMIL according to the convergence of MAML Fallah et al. (2020).
We test our method on the challenging meta-world benchmark environments (Yu et al., 2019b) and
the Kitchen environment of D4RL benchmarks (Fu et al., 2020). In our experiments, we successfully
acquire a set of meaningful sub-skills from a large scale of manipulation tasks, and achieve state-of-
the-art few-shot imitation learning abilities in the ML45 suite. In summary, the main contributions
of this paper are as follows:
•	We propose DMIL, a novel hierarchical meta imitation learning framework that meta-learns
both the high-level network and sub-skills from unsegmented multi-task demonstrations.
•	We propose a special EM-like training algorithm to schedule the meta-learning and hierar-
chical imitation learning processes in DMIL and theoretically guarantee its convergence.
•	We achieve state-of-the-art few-shot imitation learning performance on meta-world bench-
mark environments and comparable results in the Kitchen environment.
2
Under review as a conference paper at ICLR 2022
2	Related Work
2.1	Hierarchical Imitation Learning
Recovering inherent sub-skills contained in expert demonstrations and then reuse them with hier-
archical structures has long been an interesting topic in the hierarchical imitation learning (HIL)
domain. According to whether there are pretraining tasks, we can divide HIL methods into two cat-
egories. The first kind aims to manually design a set of simple pretraining tasks that could encourage
distinct skills or primitives, and then they learn a high-level network to master the switching of prim-
itives to accomplish complex tasks (Florensa et al., 2017; Peng et al., 2019; Liu & Hodgins, 2017;
Merel et al., 2019; Le et al., 2018). However, for unsegmented demonstrations where no pretraining
tasks are provided, which is the situation in our paper, these methods can not be applied.
The second kind of methods aim to learn sub-skills with unsupervised learning methods. Daniel
et al. (2016); Krishnan et al. (2017) acquire Options (Sutton et al., 1999) from demonstrations with
an Expectation-Maximization-like procedure and use the Baum-Welch algorithm to estimate the
parameters of different options. Henderson et al. (2018); Jing et al. (2021) integrate generative
adversarial networks into option discovery process. Li et al. (2017); Sharma et al. (2019); Lee & Seo
(2020) incorporate generative-adversarial imitation learning (Ho & Ermon, 2016) framework and an
information-theoretic metric (Chen et al., 2016) to simultaneously imitate the expert and maximize
the mutual-information between latent sub-skill categories and corresponding trajectories to acquire
decoupled sub-skills. There are some methods called mixture-of-expert (MoE) that compute the
weighted sum of all primitives to get the action rather than only using one of them at each time
step (Hausknecht & Stone, 2016; Neumann et al., 2009; Jacobs et al., 1991). Other methods aim to
seek an appropriate latent space that can map sub-skills into it and then condition a policy on the
latent variable to reuse sub-skills (Lynch et al., 2019; Haarnoja et al., 2018; Hausman et al., 2018;
Hakhamaneshi et al., 2021; Price & Boutilier, 2003).
For transferring learned sub-skills, some work fine-tune the whole structure in new tasks
(Hakhamaneshi et al., 2021). However, the performance of fine-tuning all depends on the gener-
alization of deep networks, which may vary among different tasks and network designs.
2.2	Meta Imitation Learning
Meta imitation learning, or one-shot imitation learning, leverages various meta-learning methods
and multi-task demonstrations to meta-learn a policy that can be quickly adapted to a new task with
few-shot new task demonstrations. Duan et al. (2017); Cachet et al. (2020) employ self-attention
modules to process the whole demonstration and the current observation to predict current action.
Yu et al. (2018a); Finn et al. (2017b); Yu et al. (2018b) use model-agnostic meta-learning (MAML)
(Finn et al., 2017a) to achieve one-shot imitation learning ability for various manipulation tasks with
robot or human visual demonstrations. Xu et al. (2019); Yu et al. (2019a) propose to meta-learn a
robust reward function that can be quickly adapted to new tasks and then use it to perform IRL
in new tasks. However, they need downstream inverse reinforcement learning after the adaptation
of reward functions, thus conflicts with our goal of few-shot adaptation. Most of above methods
only learn one monolithic policy, lacking the ability to model multiple sub-skills in long-horizon
tasks. Some works aim to tackle the multi-modal data problem in meta-learning by avoiding single
parameters initialization across all tasks (Vuorio et al., 2019; Alet et al., 2018; Frans et al., 2018;
Yao et al., 2019), but they lack the mechanism to schedule the switching of different sub-skills over
time. There are some works that also meta-learn a set of sub-skills in a hierarchical structure (Yu
et al., 2018a; Frans et al., 2018), but they either use manually designed pretraining tasks or relearn
the high-level network in new tasks, which is not appropriate in few-shot imitation learning settings.
3	Method
3.1	Preliminaries
We denote a discrete-time finite-horizon Markov decision process (MDP) as a tuple
(S , A, T, P, r, ρ0 ), where S is the state space, A is the action space, T is the time horizon,
3
Under review as a conference paper at ICLR 2022
% Multiply with
har hard selecting
---► Forward process
Sub-skill category
with the highest
probability
Sub-skill category
with low
probability
___M Meta-Iearning
process
Figure 2: The iterative meta-learning process of DMIL at each iteration. Left: the supervision of
high-level network (sub-skill categories) comes from the most accurate sub-skill (the green one,
sub-skill 1 here). Right: the sub-skill updated at current step (the green one, sub-skill 0 here) is
determined by the fine-tuned high-level network.
P : S × A × S → [0, 1] is the transition probability distribution, r : S × A → R is the reward
function, and ρ0 is the distribution of the initial state s0.
3.2	Formulation of Meta Imitation Learning Problem
We firstly introduce the general setting of the meta imitation learning problem. The goal of meta
imitation learning is to extract some common knowledge from a set of robot manipulation tasks {Ti}
that come from the same task distribution p(T), and adapt it to new tasks quickly with few shot new
task demonstrations. As in model-agnostic meta-learning algorithm (MAML) (Finn et al., 2017a),
we formalize the common knowledge as the initial parameter θ of the policy network πθ that can be
efficiently adapted with new task gradients.
For each task Ti 〜 p(T), a set of demonstrations Di is provided, where Di consists of N
demonstration trajectories: Di = {τij}jN=1, and τij consists of a sequence of state-action pairs:
τij = {(st, at)}tT=ij1, where Tij is the length of τij. Each Di is randomly split into support set Ditr
and query set Dival for meta-training and meta-testing respectively. During the training phase, we
sample m tasks from p(T), and in each task Ti, we use Ditr to fine-tune πθ to get the adapted task-
specific parameters λi with gradient descent, and then evaluate it with Dival to get the meta-gradient
ofTi, and we optimize the initial parameters θ with the average of meta-gradients from all m tasks.
As in Finn et al. (2017b); Duan et al. (2017), we use behavior cloning (Atkeson & Schaal, 1997)
loss as our metrics for meta-training and meta-testing. It aims to train a policy πθ : S → A that
maximizes the likelihood such that θ* = argmax& PN=Ilog ∏θ(ai∣Si), where N is the number of
provided state-action pairs. We denote the loss function of this optimization problem as LBC (θ, D),
and the general objective of meta imitation learning problem is:
m
min X LBC (λi,Dval),	⑴
θ
i=1
where λi = θ - αVθLBC(θ, Dtr), and a is a hyper-parameter which represents the inner-update
learning rate.
3.3	Dual Meta Imitation Learning (DMIL)
In this work we assume at each time step t, the robot may switch to different sub-skills to accom-
plish the task. We define the sub-skill category at each time step t as Zt = 1,…，K, where K
is the maximum number of sub-skills. We assume a successful trajectory τij of a task Ti is gen-
erated from several (at least one) sub-skill policies, i.e., Tij = PTjι{(st, ∏e(st∣zt))}, where ∏e
represents the expert policy. Our goal is to learn a hierarchical structure from multi-task demon-
strations {Dι,…,Dm} in an unsupervised fashion. In our model, a high-level network ∏θ. that
parameterized by θh determines the sub-skill category Zt at each time step t, and the Z-th sub-skill
among K different sub-skills ∏θπ , ∙∙∙ , ∏θ1k will be called to predict the corresponding action &t of
state st, where the hat symbol denotes the predicted result. We use λh and λιι,…，λik to represent
the adapted parameters of θh and θιι,…，θικ respectively. In order to achieve few-shot learning
ability in new tasks, we condition the high-level network only on states, i.e., Zt = ∏θ. (st), since at
the testing phase we only have access to states and have no access to action information.
4
Under review as a conference paper at ICLR 2022
DMIL aims to first fine-tune both ∏θh and ng1。, •…,∏θ1k and then meta-update them. In a new task,
πθh may not provide correct sub-skill categories as stated in the introduction. However, sub-skills
still retain the ability to give out supervision for the high-level network with knowledge learned from
previously learned tasks and few-shot demonstrations. This is because most robot manipulation tasks
are made up of a set of shared basis skills like reach, push and pick-place. As shown in the left side
of 2, the sub-skill that gives out the closet <¾ to at can be seen as supervision for the high-level
network to classify st into this sub-skill. On the other hand, the adapted high-level network can
classify each data point in provided demonstrations to different sub-skills for them to perform fine-
tuning, as shown in the right side of figure 2. In summary, DMIL contains four steps for one training
iteration. We call them High-Inner-Update (HI), Low-Inner-Update (LI), High-Outer-Update
(HO), and Low-Outer-Update (LO), which represents the fine-tuning and meta-updating process
of the bi-level networks respectively. The key problem is how to arrange these optimization orders to
ensure convergence. We first introduce these four steps formally here, then discuss how to schedule
them in the next section. The whole procedure is summarized in algorithm 1.
HI: For each sampled task Ti, we sample the first batch of trajectories {τi1} from Ditr. The principle
of this step is to use sub-skill that can predict the closest action to the expert action to provide self-
supervised category ground truths for the training of high-level network, which is a classifier in form.
We make every state-action pair passed directly to each sub-skill and compute LBC(θlk, τi1), k =
1,…，K, and choose the ground truth at each time step as the sub-skill category k that minimizes
LBC (θlk, (st, at)) :
p(z	= k) =	1, ifk = argminkLBC(θlk, (st, at))
Then we get predicted sub-skill categories from the high-level network: z^ιt
cross-entropy loss to train the high-level network:
(2)
πθh (st), and use a
Lh(θh,τii) = -T- XXp(ziit = k)logp(z^ιt = k).	(3)
Ti1 t=1 k=1
Finally we perform gradient descent on the high-level network and get λh = θh - αVθh Lh(θh,τQ.
Note θιι,…，θik are freezed here.
LI: We sample the second batch of trajectories {τi2} from Ditr. The adapted high level network πλh
will process each state in τi2 to get sub-skill category z^2t = ∏λh (St) at each time step, thus we get
K separate data sets for different sub-skills: D?k = {(si2t, ai2t)∣z^2t = k}, k = 1,…，K. Then we
compute the adaptation loss for each sub-skill with the corresponding dataset. In case of continuous
action space, we assume that actions belong to Gaussian distributions, so we have:
1 Nk
LBC (θlk, D2k ) =-寸 X(Ot - πθik (St))2,	(4)
Nk t=1
where Nk is the number of state-action pairs in D2k. Finally we perform gradient descent on sub-
skills and get λlk = θlk - αVθlk LB C (θlk, D2k). Note πλh is frozen in this process.
HO: We sample the third batch of trajectories {τi3} from Ditr and get L(λh, τi3) as in the HI process.
Then we compute meta-gradient for θh with L(λh, τi3) as follows:
vθh L(λh,τi3) = vλh L(λh,τi3)lλh = θh-αVθh L(θh,F) * vθh λh.	(5)
LO: we sample τi4 and get L(λik, D^), k = 1, ∙∙∙ ,K as in the LI process, then we compute
meta-gradient for θlk with L(λlk, D4k) as follows:
vθikL(Xlk, Qk)= vλikLSlk D4k)lλik = θik-αVθikL(θik,Dik) * V&ik%k∙	(6)
Note after the training of m tasks, we average all meta-gradients from m tasks and perform gra-
dient descents on the initial parameters together to update high-level parameters θh0 = θh -
β Pim=1 vθhL(λh, τi3) and sub-skill policies parameters θl0k = θlk - β Pim=1 vθlkL(λlk,τi4),
k = 1,…，K, i.e., we do not update them at step 5 and 6. This is crucial to ensure convergence.
For testing, although our method needs totally two batches of trajectories for one round of adapta-
tion, in practice we find only using one trajectory to perform HI and LI also works well in new tasks,
thus DMIL can satisfy the one-shot imitation learning requirement. Besides the above process, we
also add an auxiliary loss to better drive out meaningful sub-skills to avoid the excessively frequently
switching between different sub-skills along with time. Detailed information can be found in B.
5
Under review as a conference paper at ICLR 2022
m
≥ X{KL(q(Φi; λi) k p(φi∣Di,θ))
i=1
(7)
4 Theoretical Analysis
DMIL is a novel iterative hierarchical meta-learning procedure, and its convergence of DMIL needs
to be proved to ensure feasibility. As stated in the above section, what makes DMIL special is that
in HI and LI, we update parameters of each module immediately, but in LO and HO, we store
the gradients of each part and update them simultaneously. In this section, we show this can make
DMIL converge by rewriting both MAML and DMIL as hierarchical variational Bayes problems to
establish the equivalence between them, since the convergence of MAML can be proved in Fallah
et al. (2020). Proofs of all theorems are in Appendix C.
4.1	Hierarchical Variational Bayes Formulation of MAML
According to (Ravi & Beatson, 2019), MAML is a hierarchical variational Bayes inference process.
The general meta-learning objective 1 can be formulated as follows:
m
L(θ,λι,…，λm)=log YP (DM
i=1
+ Eq(φi>i)[logp(Di,φi∣θ) - log q(φi, λi)]},
where φi,i = 1, ∙∙∙ ,m represent the local latent variables for task Ti, and λι, ∙∙∙,》m are the
variational parameters of the approximate posteriors over φι, •…，Φm. We denote λi as λi(Di, θ)
and p(φi∣Di, θ) as p(φi∣Ditr, θ) to mean that λ% and φi are determined with prior parameters θ and
support data Dttr. First We need to minimize KL(q(φi; λi)kp(φi|Dtr, θ)) w.r.t. λir. According to
C.2, we have:
λi(Dtr ,θ) = arg max Eq(φig [log p(Dtr ∣φi)] - KL(q(Φi; λi)kp(φi∣θ)),	⑻
λi
and we can establish the connection between 8 and the fine-tuning process in MAML by the follow-
ing Lemma:
Lemma 11n case q(φi; λi) is a Dirac-delta function and choosing Gaussian prior forp(φi∖θ), equa-
tion 8 equals to the inner-update step of MAML, that is, maximizing logp(Ditr) w.r.t. λi by early-
stopping gradient-ascent with choosing μθ as initial point:
λi(Dtr; θ)= μθ + αVθlogp (Dtr∣θ) ∣θ=μ0.	(9)
Then we need to optimize L(θ, λι,…，Xm) w.r.t. θ. Since we evaluatep(Di∣λi(Dtr, θ)) with only
Dval, we assumep(Di∖λi(Dir, θ)) = P(Dval ∣λi(D^r, θ)). We give out the following theorem to es-
tablish the connection between the meta-update process and the optimization of L(θ, λι, ∙∙∙ , Xm):
Theorem 1 In case that Σθ → 0+, i.e., the uncertainty in the global latent variables θ is small, the
following equation holds:
M
VθL(θ, λι,…，Xm) = X Vλi logp(Dval∣λi) ∙Vθλi(Dtr,θ).	(10)
i=1
A general EM algorithm will first compute the distribution of latent variables (E-step), then optimize
the joint distribution of latent variable and trainable parameters (M-step), and the likelihood of data
can be proved to be monotone increasing to guarantee the convergence since the evidence lower
bound of likelihood is monotone increasing. Here φi,i = 1,…，M are the latent variables, and
θ corresponds to the trainable parameters. Lemma 1 and Theorem 1 correspond to the E-step and
M-step respectively. In the following part we establish the equivalence between 9 with 3 and 4, and
between 10 with 5 and 6 to prove the equivalence between DMIL and MAML.
4.2 Modeling DMIL with hierarchical variational Bayes framework
For simplicity, here we only derive in one specific task Ti , since derivatives of parameters from
multi-task can directly add up. We first establish the connection between the maximization of
logP(Dir ∣θh, θlι,…，θlκ) with the particular loss functions in DMIL:
6
Under review as a conference paper at ICLR 2022
Theorem 2 In case ofp(at∣st, θik)〜N(μθ^(§力,。2), We have:
Neh log p(D1τ∖θh,θιι,…，θικ ) = Neh Lh®, Di),	(11)
and
Nθik logp(Dtr∣θh, θιι,…，θικ) = NθikLBC(θik, D2k), k = 1,…，K.	(12)
Note in 12, D2k corresponds to data sets determined by the adapted high level netWork λh, and
this connects with 3 and 4 in DMIL. According to 8, finding λ% equals to maximize logP (D1tr ∣θ)
in specific conditions, and here in Theorem 2, we prove that maximize logp(Dir∣θh Θ11,…,θ∣κ)
corresponds to 3 and4 in DMIL. Thus theorem 2 corresponds to the E-step of DMIL, where we take
Tii and τi2 as Di, and optimize argmaxλi Eq@〜)[logp(Dtr∣Φi)] — KL(q(Φi; λi)kp(φi∣θ)) with
coordinate descent method, which can be proved to be equal to 9 in C.5.
For the M-step, we take τi3 and τi4 as Dival. According to Theorem 1, we can take the derivative of
λih λiiι, ∙∙∙ , λuκ to maximize thejoint distribution of latent variables and trainable parameters to
maximize the likelihood of dataset, so we have:
Nθh,θι logp(DVal∣λih,λii) = [Nλih logp(DVal∣λih) * Nθh入川,θ八),
Nλii logP(Dval∣λii) *Nθι%ι(Dtr,θι)]T
where θil = [θii,…，θiκ]t and λ%l = [λii,…，λ%κ]t. This is exactly the gradients computed
in HO and LO steps. Note this computation process can be automatically accomplished with stan-
dard deep learning libraries such as PyTorch (Paszke et al., 2019). To this end, we establish the
equivalence between DMIL and MAML, and the convergence of DMIL can be proved.
For a clearer comparison, MAML is an iterative process of θ → λ → θ0, and DMIL is an it-
erative process of θh , θl	→	λh,	θl	→	λh, λl	→	θh0 ,	θl0,	where the posterior estimation stages
θh, θl → λh, θl → λh, λl has no effect on parameters θh, θl, thus can be divided to two steps as
in DMIL. This decoupled fine-tuning fashion is exactly what we need to first adapt the high-level
network and then adapt sub-skills. If we end-to-end fine-tune parameters like θh, θl → λh, λl, sub-
skills will receive supervisions from an unadapted high-level network, which may provide incorrect
classifications. Different to this, the meta-updating process λh , λl → θh0 , θl0 must be done at the
same time, since if we update θh and θl successively, the later one will receive different derivative
(for example, Vq1 logp(Dval∣θ0h, λil) ) from derivatives in MAML (NeJogP(DiaI∣λih, λil)), and
the equivalence would not be proved.
5	Experiments
In experiments we aim to answer the following questions: (a) Can DMIL successfully transfer the
learned hierarchical structure to new tasks with few-shot new task demonstrations? (b) Can DMIL
achieve higher performance compared to other few-shot imitation learning methods? (c) What are
effects of different parts in DMIL, such as the skill number K, the bi-level meta-learning procedure,
and the continuity constraint? Video results are provided in supplementary materials.
5.1	Environments and Experiment Setups
We choose to evaluate DMIL on two representative robot manipulation environments. The first one
is Meta-world benchmark environments (Yu et al., 2019b), which contains 50 diverse robot manip-
ulation tasks, as shown in figure 6 and 7. We use both ML10 suite (10 meta-training tasks and 5
meta-testing tasks) and ML45 suite (45 meta-training tasks and 5 meta-testing tasks) to evaluate
our method, and collect 2K demonstrations for each task. We choose Meta-world since we think
a large scale of diverse manipulation tasks can help our method to learn semantic skills. We use
the following approaches for comparison in this environment: Option-GAIL: a hierarchical gener-
ative adversarial imitation learning method to discover options from unsegmented demonstrations
(Jing et al., 2021). We use Option-GAIL to evaluate the effect of meta-learning in DMIL. MIL: a
transformer-based meta imitation learning method (Cachet et al., 2020). We use MIL to evaluate the
effect of the hierarchical structure in DMIL. MLSH: the meta-learning shared hierarchies method
(Frans et al., 2018) that relearns the high-level network in every new task. We use MLSH to eval-
uate the effect of fine-tune (rather than relearn) the high-level network in new tasks. PEMIRL: a
contextual meta inverse RL method which transfers the reward function in the new tasks (Yu et al.,
2019a). We use PEMIRL to show DMIL can transfer to new tasks that have significantly different
reward functions.
7
Under review as a conference paper at ICLR 2022
Table 1: Success rates of different methods on Meta-world environments with K = 3. Each data
point comes from the success rate of 20 tests.
Methods	ML10				ML45			
	Meta-training		Meta-testing		Meta-training		Meta-testing	
	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot
OptionGAIL	0.455±0.011	0.952±0.016	0.241±0.042	0.640±0.025	0.506±0.008	0.715±0.006	0.220±0.013	0.481±0.010
MIL	0.776±0.025	0.869±0.029	0.361±0.040	0.689±0.032	0.584±0.011	0.745±0.017	0.205±0.024	0.510±0.005
PEMIRL	0.598±0.023	0.810±0.007	0.162±0.003	0.256±0.009	0.289±0.051	0.396±0.024	0.105±0.005	0.126±0.008
MLSH	0.506±0.134	0.725±0.021	0.106±0.032	0.135±0.009	0.235±0.093	0.295±0.021	0.050±0.000	0.050±0.000
DMIL	0.775±0.010	0.949±0.009	0.396±0.016	0.710±0.021	0.590±0.010	0.859±0.008	0.376±0.004	0.640±0.009
Table 2: Rewards of different methods on four unseen tasks in Kitchen environment with K = 4.
Boldface indicates excluded objects during training.
Task (Unseen)	FIST-no-FT	SPiRL	DMIL(ours)
Microwave, Kettle, Top Burner, Light Switch	2.0 ± 0.0	2.1 ± 0.48	1.5±0.48
Microwave, Bottom Burner, Light Switch, Slide Cabinet	0.0 ± 0.0	2.3 ± 0.49	2.35±0.39
Microwave, Kettle, Hinge Cabinet, Slide Cabinet	1.0 ± 0.0	1.9 ± 0.29	3.15±0.22
Microwave, Kettle, Hinge Cabinet, Slide Cabinet	2.0 ± 0.0	3.3 ± 0.38	2.95±0.44
The second one is the Kitchen environment of the D4RL benchmark (Fu et al., 2020), which con-
tains five different sub-tasks in the same kitchen environment. The accomplishment of each episode
requires sequentially completions of four specific sub-tasks, as shown in figure 9. We use an open
demonstration data set (Gupta et al., 2019) to train our method. During training, we exclude inter-
actions with selected objects and at test-time we provide demonstrations that involve manipulating
the excluded object to make them as unseen tasks. We choose this environment to show DMIL can
be used in long-horizon tasks and is robust across different environments. We use two approaches
for comparison in this experiment: SPiRL: an extension of the skill extraction methods to imita-
tion learning over skill space (Price & Boutilier, 2003); FIST: an algorithm that extracts skills from
offline data with an inverse dynamics model and a distance function (Hakhamaneshi et al., 2021).
We use fully-connected neuron networks for both the high-level network and sub-skills. Detailed de-
scriptions on the environment setup, demonstration collection procedure, hyper-parameters setting,
training process, and descriptions of different methods can be found in appendix E.
5.2	Results
Table 1 shows success rates of different methods in ML10 and ML45 suites with sub-skill number
K = 3. We perform 1-shot and 3-shot experiments respectively to show the progressive few-shot
performance of different methods. DMIL achieves the best results in ML10 testing suite and ML45
training and testing suites. This shows the superiority of our method for transferring across a large
scale of manipulation tasks. OptionGAIL achieves high success rates in both ML10 and ML45 train-
ing suites. These results come from their hierarchical structures that have adequate capacity to fit
potential multi-modal behaviors in multi-task demonstrations. MIL achieves comparable results for
all meta-testing tasks but is worse than DMIL. This shows the necessity of meta-learning processes.
Compared to them, PEMIRL and MLSH are mediocre among all suites. This comes from that the
reward functions across different tasks are difficult to transfer with few shot demonstrations, and the
relearned high-level network of MLSH damages previously learned knowledge. We also illustrate
the t-sne results of these methods in figure 4(a) to further analyze the methods in appendix D.2.
Table 2 shows the rewards of different methods on four unseen tasks in the Kitchen environment.
FIST-no-FT refers to a variant of FIST that does not use future conditioning, which makes the
comparison fairer. DMIL achieves higher rewards on two out of four tasks and comparable results
on the other two tasks, which exhibits the effectiveness of the bi-level meta-training procedure. The
poor performance of DMIL on the first task may come from the choice of skill number K or from
low-quality demonstrations. We perform ablation studies on K in the next section.
Figure 3 shows curves of sub-skill probabilities along time of two tasks window-close and peg-
insert-side of Meta-world and the microwave-kettle-top burner-light task in Kitchen environment.
We can see the activation of sub-skills shows a strong correlation to different stages of tasks. In
first two tasks, πθl0 activates when the robot is closing to something, πθl1 activates when the robot
is picking up something, and πθl2 activates when the robot is manipulating something. In the third
task, πθl2 activates when the robot is manipulating the microwave, πθl0 activates when the robot is
8
Under review as a conference paper at ICLR 2022
Figure 3: The iterative meta-learning process of DMIL at each iteration. Left: the supervision of
high-level network (sub-skill category) comes from the most accurate sub-skill. Right: the sub-skill
updated at current step is determined by the fine-tuned high-level network.
manipulating the kettle or the light switch, and πθl3 activates when the robot is manipulating the
burner switch. This shows that DMIL has the ability to learn semantic sub-skills from unsegmented
multi-task demonstrations.
Table 3: Success rates of different sub-skill number in
5.3 Ablation Studies
In this section we perform ablation stud-
ies on different components of DMIL to
provide effects of different parts. Due to
limited space, we put ablations on fine-
tuning steps, bi-level meta-learning pro-
Meta-world environments.
K	ML10				ML45			
	Meta-training		Meta-testing		Meta-training		Meta-testing	
	I-Shot	3-Shot	1-Shot	3-shot	1-shot	3-shot	1-shot	3-shot
2	0.76	0.955	0.32	0.72	0.563	0.818	0.44	0.67
3	0.775	0.949	0.396	0.71	0.59	0.859	0.376	0.64
5	0.795	0.94	0.52	0.57	0.713	0.92	0.21	0.48
10	0.8	0.975	0.38	0.62	0.736	0.931	0.34	0.64
cesses, continuity constraint and hard/soft EM choices in appendix D.
Effect of different skill number K : Table 3 shows the effect of different sub-skill number K in
Meta-world experiments. We can see that a larger K can lead to higher success rates on meta-
training tasks, but a smaller K can lead to better results on meta-testing tasks. This tells us that
an excessive number of sub-skills may result in over-fitting on training data, and a smaller K can
play the role of regularization. In Kitchen experiments, we can see similar phenomenons in table
5. It is worth noting in both environments, we did not encounter collapse problems, i.e., every
sub-skill gets well-trained even when K = 8 in kitchen environment or K = 10 in Meta-world
environments. This may come from that, in the meta-training stage, adding more sub-skills can help
the whole structure get lower loss, thus DMIL will use all of them for training. However, in our
supplementary videos, we can see sub-skills trained with a large K (for instance, K = 10 in Meta-
world environments) are not as semantic as sub-skills trained by a small K (for instance, K = 3 in
Meta-world environments) during the execution of a task.
6	Discussion and Future Works
In this work, we propose DMIL to meta-learn a hierarchical structure from unsegmented multi-task
demonstrations to endow it with fast adaptation ability to transfer to new tasks. We theoretically
proved its convergence by reframing MAML and DMIL as hierarchical Bayes inference processes
to get their equivalence. Empirically, we successfully acquire transferable hierarchical structures in
both Meta-world and Kitchen Environments.
The limitations of DMIL comes from several aspects, and future works can seek meaningful ex-
tensions in these perspectives. Firstly, DMIL models all tasks as bi-level structures. However, in
real-world situations, tasks may be multi-level structures. One can extend DMIL to multi-level hi-
erarchical structures like done in recent works (Shu et al., 2018). Secondly, DMIL does not capture
temporal information in demonstrations. Future state conditioning in Hakhamaneshi et al. (2021)
seems an effective tool to improve few-shot imitation learning performance in long-horizon tasks
such as in the Kitchen environments. Future works can employ temporal module such as trans-
former (Vaswani et al., 2017) as the high-level network of DMIL to improve its performance.
9
Under review as a conference paper at ICLR 2022
7	Reproducibility Statement
We provide video results in supplementary materials. We provide some basic codes to run few-shot
testing in meta-world environments in an anonymous repository 1. The complete training code will
be provided later. Details of experiments such as environments, hyper-parameter setting, model
setup, training procedure and data preprocessing can be found at E.
References
Ferran Alet, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Modular meta-learning. In 2nd
Annual Conference on Robot Learning, 2018.
Christopher G. Atkeson and Stefan Schaal. Robot learning from demonstration. In Proceedings of
the Fourteenth International Conference on Machine Learning, 1997.
T Cachet, J Perez, and S Kim. Transformer-based meta-imitation learning for robotic manipulation.
In Neural Information Processing Systems, Workshop on Robot Learning, 2020.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, 2016.
Christian Daniel, Herke van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for
determining options in reinforcement learning. Machine Learning, 2016.
Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever,
Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in Neural
Information Processing Systems, 2017.
David Duvenaud, Dougal Maclaurin, and Ryan Adams. Early stopping as nonparametric variational
inference. In Artificial Intelligence and Statistics. PMLR, 2016.
Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. On the convergence theory of gradient-
based model-agnostic meta-learning algorithms. In The 23rd International Conference on Artifi-
cial Intelligence and Statistics, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
2017a.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imita-
tion learning via meta-learning. In 1st Annual Conference on Robot Learning, 2017b.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Ad-
vances in Neural Information Processing Systems, 2018.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-
forcement learning. In 5th International Conference on Learning Representations, 2017.
Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared
hierarchies. In 6th International Conference on Learning Representations, 2018.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, and Richard S. Zemel. Smile: Scalable meta
inverse reinforcement learning through context-conditional policies. In Advances in Neural In-
formation Processing Systems 32:, 2019.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas L. Griffiths. Recasting
gradient-based meta-learning as hierarchical bayes. In 6th International Conference on Learn-
ing Representations, 2018.
1https://anonymous.4open.science/r/DMIL-6FCC
10
Under review as a conference paper at ICLR 2022
Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint
arXiv:1910.11956, 2019.
Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for
hierarchical reinforcement learning. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning, 2018.
Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, and Michael Laskin. Hierarchi-
cal few-shot imitation with skill transition models. 2021.
Matthew J. Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space.
In 4th International Conference on Learning Representations, 2016.
Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin A. Riedmiller.
Learning an embedding space for transferable robot skills. In 6th International Conference on
Learning Representations, 2018.
Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon, David Meger, Joelle Pineau, and Doina Precup.
Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforce-
ment learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, 2016.
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures
of local experts. Neural Computing, 1991.
Mingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao Kong, Chuang Gan, and Lei Li.
Adversarial option-aware hierarchical imitation learning. In Proceedings of the 38th International
Conference on Machine Learning, 2021.
Ilya Kostrikov, Kumar Krishna Agrawal, Sergey Levine, and Jonathan Tompson. Addressing sample
inefficiency and reward bias in inverse reinforcement learning. arXiv preprint arXiv:1809.02925,
2018.
Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. DDCO: discovery of deep continuous
options for robot learning from demonstrations. In 1st Annual Conference on Robot Learning,
2017.
Hoang Minh Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik, Yisong Yue, and Hal DaUme III.
Hierarchical imitation and reinforcement learning. In Proceedings of the 35th International Con-
ference on Machine Learning, 2018.
Sang-Hyun Lee and Seung-Woo Seo. Learning compound tasks without task-specific knowledge
via imitation and self-supervised learning. In Proceedings of the 37th International Conference
on Machine Learning, 2020.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual
demonstrations. In Advances in Neural Information Processing Systems, 2017.
Libin Liu and Jessica K. Hodgins. Learning to schedule control fragments for physics-based char-
acters using deep q-learning. ACM Trans. Graph., 2017.
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and
Pierre Sermanet. Learning latent plans from play. In 3rd Annual Conference on Robot Learning,
2019.
Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas
Heess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In 7th International
Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2022
Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? arXiv
preprint arXiv:1906.02629, 2019.
Gerhard Neumann, Wolfgang Maass, and Jan Peters. Learning complex motions by sequencing
simpler motion templates. In Andrea Pohoreckyj Danyluk, Leon Bottou, and Michael L. Littman
(eds.), Proceedings of the 26th Annual International Conference on Machine Learning, 2009.
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, and Jan Peters.
An algorithmic perspective on imitation learning. 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. Advances in neural information processing systems, 2019.
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: example-
guided deep reinforcement learning of physics-based character skills. ACM Trans. Graph., 2018.
Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. MCP: learning
composable hierarchical control with multiplicative compositional policies. In Advances in Neu-
ral Information Processing Systems, 2019.
Bob Price and Craig Boutilier. Accelerating reinforcement learning through implicit imitation. Jour-
nal of Artificial Intelligence Research ,19:569-629, 2003.
Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. In 7th International Conference
on Learning Representations, 2019.
StePhane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics, 2011.
Reginaldo J Santos. Equivalence of regularization and truncated iteration for general ill-posed prob-
lems. Linear algebra and its applications, 1996.
Mohit Sharma, Arjun Sharma, Nicholas Rhinehart, and Kris M. Kitani. Directed-info GAIL: learn-
ing hierarchical policies from unsegmented demonstrations using directed information. In 7th
International Conference on Learning Representations, 2019.
Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and interpretable skill acquisition in
multi-task reinforcement learning. In 6th International Conference on Learning Representations,
2018.
Richard S. Sutton, Doina Precup, and Satinder P. Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artif. Intell., 1999.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, 2017.
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J. Lim. Multimodal model-agnostic meta-
learning via task-aware modulation. In Advances in Neural Information Processing Systems,
2019.
Kelvin Xu, Ellis Ratner, Anca D. Dragan, Sergey Levine, and Chelsea Finn. Learning a prior
over intent via meta-inverse reinforcement learning. In Proceedings of the 36th International
Conference on Machine Learning, 2019.
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning.
In Proceedings of the 36th International Conference on Machine Learning, 2019.
Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse reinforcement learning with
probabilistic context variables. In Advances in Neural Information Processing Systems, 2019a.
Tianhe Yu, Pieter Abbeel, Sergey Levine, and Chelsea Finn. One-shot hierarchical imitation learning
of compound visuomotor tasks. arXiv preprint arXiv:1810.11043, 2018a.
12
Under review as a conference paper at ICLR 2022
Tianhe Yu, Chelsea Finn, Sudeep Dasari, Annie Xie, Tianhao Zhang, Pieter Abbeel, and Sergey
Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. In
Robotics: Science and Systems XIV, 2018b.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In 3rd Annual Conference on Robot Learning, 2019b.
Yuke Zhu, Ziyu Wang, Josh Merel, Andrei A. Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvu-
nakool, Janos Kramar, Raia HadselL Nando de Freitas, and Nicolas Heess. Reinforcement and
imitation learning for diverse visuomotor skills. In Robotics: Science and Systems XIV, 2018.
Yayi Zou and Xiaoqi Lu. Gradient-em bayesian meta-learning. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems, 2020.
13
Under review as a conference paper at ICLR 2022
A	Algorithm
Algorithm 1 Dual Meta Imitation Learning
Require: task distribution p(T), multi-task demonstrations {Di}, i = 1,…，m, initial param-
eters of high-level network θh and sub-skill policies θιι, ∙∙∙ , θικ, inner and outer learning rate
α, β.
while not done do
Sample batch of tasks T 〜p(T)
for all Ti do
Sample {τi1}, {τi2}, {τi3}, {τi4} from {Di}
Evaluate 口hLh(θh, 丁〃)according to 3 and f
Compute adapted parameters of high-level network: λ% = θh - αVθh Ch®, Tii)
Evaluate VθlkLBC(θik, D2k) according to 4 and e k =1,…，K
Compute adapted parameters of sub-skills: λik = θik -αVθ※ LBC (θik, D2k), k = 1,… ,K
Evaluate Vθh CTi (λhf3) and Vθ※ CTi (λik, D4k), k = 1,…，K
end for
UPdate θh J θh -尸口九 PTi〜P(T) CTi (λh, τi3)
UPdate θik - θlk -尸口1忆 PTi〜P(T) CTi (λlk, D4k ), k = * * * * * 1,…，K
end while
B Auxiliary Loss
We adopt an auxiliary loss for DMIL to better drive out meaningful sub-skills by punishing excessive
switching of sub-skills along the trajectory. This comes from an intuitive idea: each sub-skill should
be a temporal-extended macro-action, and the high-level policy only needs to switch to different
skills few times along a task, as the same idea of macro-action in MLSH Frans et al. (2018). We
denote:
1, if x = T rue
sign(x) =	0, ifx=False
and the auxiliary loss is:
T-1
Caux(T) = E Sign(Zt+1 = Zt) / len(τ).
t=0
(14)
(15)
Although this operation seems discrete, in practice we can use the operations in modern deep learn-
ing framework such as PyTorch (Paszke et al., 2019) to make it differentiable. We add this loss
function to the Ch(θh, τi1) and VθhC(λh, τi3) with a coefficient λ = 1. We also perform ablation
studies of Caux and results are in table 7.
C Proofs
C.1 Proof of Lemma 1
Lemma 11n case q(φi; λi) is a Dirac-delta function and choosing Gaussian prior forp(φi∣θ), equa-
tion 8 equals to the inner-update step of MAML, that is, maximizing logp(Ditr) w.r.t. λi by early-
stopping gradient-ascent with choosing μθ as initial point:
λi(Dtr； θ)= μθ + αVθ logP (Dtr∣θ) ∣θ=μ0.	(16)
Proof: in case of the conditions of Lemma 1, we have:
λi(Dtr； θ) = argmax[logP(Dtr |〃# -||〃人一μθk2∕2∑2],	(17)
λi
As stated in Grant et al. (2018), firstly in the case of linear models, early stopping of an iterative
gradient descent process of λ equals to the maximum posterior estimation (MAP) (Santos, 1996).
14
Under review as a conference paper at ICLR 2022
(19)
In our case the posterior distribution refers to q(φi∣λi), and MAML is a Bayes process to find the
MAP estimate as the point estimate of λi (Ditr; θ). In the nonlinear case, this point estimate is not
necessarily the global mode of the posterior, and we can refer to Duvenaud et al. (2016) for another
implicit posterior distribution over φi and making the early stopping procedure of MAML acting as
priors to get the similar result.
C.2 Proof of Equation 8
Equation 8 can be written as:
λi(Dtr,θ) = argminKL(q(φi; %)kp(φi∣Dtr,θ))
λi
=arg max Eq(ΦiA) [log p(Φi∣Dtr, θ) - log q(Φi λi)]	(18)
λi
=arg max Eq(φig [log P(Dtr ∣φi)] - KL(q(Φi; λi)kp(φi∣θ)),
λi
where in MAML we assume P(DiIr ∣φi) = p(Dttr ∣φi, θ), and use the joint distribution P(Dir ,φi∣θ)
to replacep(φi∣D1tr, θ) since We assume thatP(DiT) subjects to uniform distribution. Thus 8 can be
proved.
C.3 Proof of Theorem 1
Theorem 1 In case that Σθ → 0+, i.e., the uncertainty in the global latent variables θ is small, the
following equation holds:
M
VθL(θ,λι,…，λM) = X Vii logP(Dv叫") * Vθλi(Dtr,θ).
i=1
Proof:
M
VθL(θ, λι,…，λM) ≈ X{VθEq(φ"λi) [logP(Di, φi∣θ) - log q(φi %)]}
i=1
M
=X Vθ[logP(Dval∣λi(Dtr,θ)) - logP(λi(Dtr,θ)∣θ)]
i=1
M
≈ X Vθ logP(Dv叫λi(Dtr,θ))
i=1
M
=X Vλi logP(Dv叫λi) * Vθλi(Dtr,θ)
i=1
where the first approximate equal holds because the VI approximation error is small enough, and
the second approximate equal holds because that in case Σθ → 0+ and assuming λi be a neuron
network, logP(λi(Dtr,θ)∣θ) ≈ 0 holds almost everywhere, so Vθ logP(λi(Dtr,θ)∣θ) ≈ 0. Note
the condition of theorem 1 is usually satisfied since we are using MAML, and the initial parameters
θ are assumed to be deterministic.
From another perspective, the right side of above equation is the widely used
meta-gradient in MAML, and it is equal to m Pm=I (I - αV2LBC (θ, Dtr)) *
Vi/bc (θ 一 aVθLbc (θ, Dtr), Dval), which is proved to be converged by Fallah et al.
(2020).
C.4 Proof of Theorem 2
Theorem 2 In case ofP(at∣st, θik)〜N(μθ※(st),σ2), we have:
Vθh logP(DY∖θh,θιι,∙∙∙ ,θικ) = VθhLh(θh,Dtr),	(21)
15
(20)
Under review as a conference paper at ICLR 2022
and
Vθik log P(Dy∖θh,θιι,…，θικ ) = Vθik LBC (θik, D2k ),k = 1,…，K.	(22)
Proof： since p(D1∩θh,θιι,…，θικ) = QN=I p(at∖st,θh ,θιι,…，θικ )p(st∣θh,θιι, ∙∙∙ ,θικ) and
the second term is independent of θ, we consider the first conditional probability:
K
p(at∣st, θh,θιι,…，θικ) = Ep(Zk|st, θh)p(at∣st, θik).	(23)
k=1
In the HI step, p(at|st, θlk) is fixed, thus 23 becomes a convex optimization problem:
K
max y2p(zk∣St,θh)p(at∣St,θ
θh
h k=1
(24)
The solution of this problem is λh which satisfies p(zk |st, at, λh) = 1, k = argmaxk p(at∣st,θik).
This means that πθh needs to predict the sub-skill category at time step t as k, in which case πθlk
can maximizep(at|st, θlk). In case we choose πθh to be a classifier that employs a Softmax layer at
the end, minimizing the cross entropy loss 3 equals to maximize 24, thus 21 can be proved.
In the LI step, p(zk|st, λh) is fixed, and the data sets for optimizing θιι,…，θικ are also fixed
as D2k = {(sijt, aijt)∣z^2t = k}N=kι. Thus we need to maximize each p(at∖st,θik) With D?k. In
case of p(at∣st,θik) ~N(μθik(st),σ2) H exp[- (at-7^^(St)) ], we have maxθik p(at∖st,θik) ⇔
minθlk (at - πθlk (st))2, which leads to the loss function 4, thus 22 can be proved, which finishes the
prove of Theorem 2.
C.5 Proof of the E-step of DMIL
According to Theorem 1, we aim to maximize 17 w.r.t λi from the initial point θi with coordinate
gradient ascent. We here need to prove that in DMIL, we could also achieve the global maximum
point of λi as in MAML. We first give out the following Lemma:
Lemma 2 Let X be the solution found by coordinate gradient descent of f (x). Let Xi,i = 1,… ,n
be the n coordinate directions used in the optimization process. If f(x) can be decomposed as:
n
f(x) = g(x) +	hi(x),
i=1
(25)
where g(x) is a differentiable convex function, and each hi(x) is a convex function of the coordinate
direction xi, then x is the global minimum of f (x).
Proof： Let y be another arbitrary point, we have:
f(y) - f(x) = g(y) + h(y) - (g(x) + h(x))
≥ Vxg(x)T (y - x) + X hi (yi ) - hi (xi)
i=1
n
=	(Vig(x)(yi - xi) + hi(yi) - hi(xi))
i=1
≥ 0.
(26)
16
Under review as a conference paper at ICLR 2022
Now let’s consider our problem. Consider
T
log p(Dtr ∣θih,θii) = log X p(at∣St,θih,θii )p(st)
t=1
TK
=log£P(St) Ep(Zk|st, at,θih)p(at∣st, θii)
TK
≥ E[logp(st) + log£p(zk|st, at,θih)p(at|st, θii)]	(27)
TK
≥ E[logp(st) + £logp(zk∣st,at,θih)p(at|st, θii)]
t=1	k=1
TK	K
=£[log p(st) + £log p(zk ∣st,at,θih) + £log p(at∣st,θii)].
t=1	k=1	k=1
In our case, two coordinate directions are θih and θil. Let’s consider the terms inside the
brackets. According to Lemma 2, we can think logp(st) as g(x)(here it equals to constant),
PkK=1 logp(zk|st, at, θih) as h1 (x) and PkK=1 logp(at |st, θil) as h2(x). Thus the optimum can
be proved.
D	Additional Ablation Studies
D.1 Effects of the Bi-level Meta-learning Process
We use two variants of DMIL to see the effectiveness of bi-level meta-earning process. DMIL-
High: a variant that only meta-learns the high-level network, and DMIL-Low: a variant that only
meta-learns sub-skills. We use Option-GAIL as a comparison that does not meta-learn any level of
the hierarchical structure.
Table 4 shows the results of this ablation study. DMIL-High achieves close results with OptionGAIL
in all meta-training suites and better results in all meta-testing suites, but worse than DMIL in all
cases. This shows that meta-learning the high-level network can help the hierarchical structure adapt
to new tasks, but only transferring the high-level network is not enough for accomplish all kinds of
new tasks. DMIL-Low achieves poor results in all suites except in ML10 meta-training suites. This
shows that transferring the high-level network is necessary when training on a large scale of tasks or
testing in new tasks.
Table 4: Success rates of DMIL-High, DMIL-Low, DMIL and OptionGAIL on Meta-world envi-
ronments with K = 3. Each data point comes from the success rate of 20 tests.
Methods	ML10				ML45			
	Meta-training		Meta-testing		Meta-training		Meta-testing	
	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot
OptionGAIL	0.755±0.011	0.952±0.016	0.241±0.042	0.640±0.025	0.506±0.008	0.715±0.006	0.220±0.013	0.481±0.010
DMIL-High	0.634±0.001	0.914±0.011	0.298±0.012	0.670±0.015	0.495±0.007	0.735±0.006	0.280±0.016	0.551±0.011
DMIL-Low	0.746±0.011	0.943±0.006	0.291±0.024	0.666±0.021	0.511±0.005	0.765±0.009	0.266±0.010	0.492±0.014
DMIL	0.775±0.010	0.949±0.009	0.396±0.016	0.710±0.021	0.590±0.010	0.859±0.008	0.376±0.004	0.640±0.009
For better understand the effect of bi-level meta-learning process, we perform ablation study for
three DMIL variants in an manually-designed new task push-around-wall (figure 8). In this task,
the robot needs to grasp a cube and circle it around the wall. This is a brand new skill that is not
in the meta-world suite. The accomplishment of this new task requires quickly adapting abilities
of both the high-level network and sub-skills. We sample two demonstrations and use the first one
as few-shot data, and illustrate the sub-skill categories of the second demonstration given by the
high-level network at figure 4(b). Before adaptation, all variants give out approximately random
results. However, after one-shot adaptation, DMIL classifies almost every state into sub-skill 0 and
17
Under review as a conference paper at ICLR 2022
Before
Adaptation
After
Adaptation
MLSH	OptionGAIL	DMIL
DMIL-High
(a) T-sne results of demonstrations and sub-skill categories of several (b) Sub-skill categories of task push-
hierarchical models for meta-testing task hand-insert.	around-wall for ablation study.
Figure 4: T-sne results and ablation studies about the bi-level meta-learning process.
sub-skill 2, which indicates these two sub-skills in DMIL have been adapted to the new task, and the
high-level network has also been adapted with the supervision from adapted sub-skills. Compared
to DMIL, DMIL-Low still can not give out reasonable results after adaptation, since its high-level
network lacks the ability to quickly transfer to new tasks. Instead, DMIL-High gives out plausible
results after adaptation. This shows the high-level network has adapted to the new task according to
the supervision from adapted sub-skills, but no sub-skill can dominate for a long time period since
all sub-skills lack the quickly adaptation ability.
D.2 T-sne Results of Different Methods
For comparison of different methods, we illustrate the t-sne results of states of each sub-skill in an
ML45 meta-testing task hand-insert in figure 4(a). We use 3 demonstrations for adaptation, and
draw the t-sne results on another 16 demonstrations. This task is a meta-testing task, so no method
has ever been trained on this task before.
MLSH shows almost random clustering results no matter before and after adaptation, since its high-
level network is relearned in new tasks. OptionGAIL clusters to three sub-skills after adaptation.
Compared to them, DMIL clusters the data to only two sub-skills after adaptation. We believe fewer
categories reflect more meaningful sub-skills are developed in DMIL.
D.3 Effects of Sub-skill Number K in The Kitchen Environment
We perform ablation studies of sub-skill number K on the Kitchen environments and choose K =
2, 4, 8 respectively. Table 5 shows the results. We can see that a smaller number of sub-skills can
achieve better results on such four unseen results that a large number of sub-skills. This may indicate
that the sub-skill number K can work as a ’bottleneck’ like the middle layer in an auto-encoder.
Table 5: Ablations of sub-skill number K in Kitchen environments.
Task (Unseen)	∣	I K=2	K=4	K=8
Microwave, Kettle, Top Burner, Light Switch	1.9±0.43	1.5±0.48	1.7±0.22
Microwave, Bottom Burner, Light Switch, Slide Cabinet	2.15±0.19	2.35±0.39	2.0±0.37
Microwave, Kettle, Hinge Cabinet, Slide Cabinet	2.45±0.25	3.15±0.22	1.85±0.23
Microwave, Kettle, Hinge Cabinet, Slide Cabinet	2.01±0.24	2.95±0.44	2.44±0.47
18
Under review as a conference paper at ICLR 2022
D.4 Effects of Fine-tuning Steps
As all few-shot learning problems, the fine-tuning steps in new tasks to some extent determine the
performance of the trained model. It controls the balance between under-fitting and over-fitting. We
perform ablation studies of fine-tune steps in Meta-world benchmarks with K = 5 and lr=1e-2 in
table 6. Results are as follows:
Table 6: Ablation studies of the fine-tuning steps in Meat-world experiments with K = 5.
fine-tune steps	ML10				ML45			
	Meta-training		Meta-testing		Meta-training		Meta-testing	
	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot
10	0.5	0.575	0.28	0.24	0.374	0.49	0.05	0.17
30	0.69	0.915	0.25	0.31	0.602	0.85	0.13	0.32
50	0.695	0.895	0.27	0.39	0.583	0.87	0.12	0.32
100	0.665	0.905	0.23	0.41	0.614	0.872	0.07	0.43
300	0.66	0.845	0.28	0.44	0.605	0.876	0.12	0.44
500	0.63	0.91	0.25	0.39	0.584	0.867	0.13	0.42
Range	0.195	0.34	0.05	0.2	0.231	0.386	0.08	0.27
D.5 Effects of Continuity Regularization
We perform ablation studies of the effect of continuity regularization as following table 7. DMIL_nc
means no continuity regularization. We can see that the continuity constraint would damage meta-
training performance slightly, but increase the meta-testing performance greatly.
Table 7: K = 10
variants	ML10				ML45			
	Meta-training		Meta-testing		Meta-training		Meta-testing	
	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot
DMIL	0.795	0.94	0.52	0.57	0.713	0.92	0.21	0.48
DMIL_nc	0.788	0.96	0.32	0.56	0.703	0.927	0.17	0.35
Gap	0.007	-0.02	0.2	0.01	0.01	-0.007	0.04	0.13
D.6 Effects of Hard/Soft EM Choices
In DMIL, we use hard EM algorithm to train the high-level network. One may think about to use
soft cross entropy loss to train the high-level network to get better results. We perform this ablation
study in the following table 8. We can see that a soft cross entropy training won’t help increase the
whole success rates. This may comes from that, usually we use soft cross entropy (such as label
smoothing) to prevent over-fitting. However, in our situation, this may cause under-fitting, since
training on such a large scale of diverse manipulation tasks is already very difficult. Future works
can seek more comparisons about this choice.
Additionally, we found an interesting phenomena that the training loss of the high-level network with
a softmax shows a trend of rising first and then falling, as shown in . In our experiments, a softmax
loss may regularize the optimization process to make the high-level network be under-fitting on
training data. This may come from that, the experiment environment (Meta-world) contains a large
scale of manipulation tasks, in which the training of the high-level network can be difficult and
unstable. Thus a soft-max cross entropy loss cannot help that much here like how it works as a
regularizer to prevent over-fitting in the label smoothing (Muller et al., 2019).
19
Under review as a conference paper at ICLR 2022
Table 8: Ablation about hard/soft EM choices with K = 5 in the Meta-world environments.								
variants	ML10				ML45			
	Meta-training		Meta-testing		Meta-training		Meta-testing	
	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot	1-shot	3-shot
DMIL	0.795	0.94	0.52	0.57	0.713333	0.92	0.21	0.48
DMIL-Soft	0.37	0.65	0.33	0.46	0.235556	0.43	0.1	0.32
Gap	0.425	0.29	0.19	0.11	0.477778	0.49	0.11	0.16
Figure 5: The training loss of the high-level network with a softmax shows a trend of rising first and
then falling.
E Experiment Details
E.1 Environments
See figure 7,6,9 and 8.
Figure 6: The ML45 environment.
E.2 Model Setup
DMIL: The high-level network and each sub-skill is modeled with a 4-layer fully-connected neuron
network, with 512 ReLU units in each layer. We use Adam as the meta-optimizer. DMIL-High and
20
Under review as a conference paper at ICLR 2022
Figure 7: The ML10 environment.
Figure 8: Task push-around-wall.
Table 9: DMIL hyper-parameters.
Parameter	Value
α	5e-4
β	1e-4
fine-tune iterations	-3-
batch size (in trajectory)	-16-
λ	0.1
Table 10: MIL hyper-parameters.
Parameter	Value
nhead	-8-
	nlayer		-3-
dmodel	512
	dk		64
	dv		64
nposition	250
dropout	0.1
batch size (in state-action pair)	512
DMIL-Low use the same architecture with DMIL. Hyper-parameter settings are available in table
9.
MIL: We use a transformer (Vaswani et al., 2017) as the policy to perform MAML. The input of the
encoder is the whole one-shot demonstration or 3-shot demonstrations with concatenated state and
action. The input of the decoder is current state. The output is the predicted action. Hyper-parameter
settings are available in table 10.
MLSH: We use the same settings of network with DMIL here. The macro step of high-level network
is 3. Since our problem is not a reinforcement learning process, we use the behavior cloning variant
of MLSH. The pseudo reward is defined by the negative mean square loss of the predicted action
and the ground truth, and we perform pseudo reinforcement learning process with off-policy demon-
stration data. We use PPO as our reinforcement learning algorithm. Note in this way we ignore the
importance sampling weights that required by replacing the sampling process in the environments
with the demonstrations in the replay buffer, which has been shown to be effective in practice in
21
Under review as a conference paper at ICLR 2022
Figure 9: Kitchen environments.
Table 11: MLSH hyper-parameters.
Parameter	Value
high-level learning rate	1e-3
sub-skill learning rate	1e-4
PPOcliPthreshold	0.02
high-level warmup step	500
joint update step	1000
batch size (in state-action pair)	900
Table 12: PEMIRL hyper-parameters.
Parameter	Value
learning rate of all models	1e-4
PPO clip threshold	0.02
coefficient (Y) of hφ	-1-
β	0.1
batch size (in trajectory)	16
Kostrikov et al. (2018); Ghasemipour et al. (2019). Hyper-parameter settings are available in table
11.
PEMIRL: We use the same setting of the high-level network as the policy πω and the inference
model qψ in PEMIRL. We use PPO as our reinforcement learning algorithm. We use a 3-layer fully-
connected neuron network as the context-dependent disentangled reward estimator rθ (s, m) and the
context-dependent potential function hφ (s, m). Here we also use the behavior cloning variant of
PEMIRL to only train models on the off-policy data. Hyper-parameter settings are available in table
12.
E.3 Training Details
For fine-tuning, OptionGAIL, DMIL-Low and DMIL-High have no meta-learning mechanism for
(some parts of) the trained model. In the few-shot adaptation process, we have different fine-tune
method for these baselines:
OptionGAIL: We train OptionGAIL models on the provided few-shot demonstrations for a few
epochs.
DMIL-Low: We fix the high-level network and only fine-tune sub-skills with few-shot demonstra-
tions.
22
Under review as a conference paper at ICLR 2022
DMIL-High: We fix sub-skills and only fine-tune the high-level network with few-shot demonstra-
tions.
E.4 Detailed Results
We provided detailed success rates of each environments of different methods in figure 10 and 11
with K = 3.
23
Under review as a conference paper at ICLR 2022
ML-IO Maximum Per-Task Success Rates (N=20)
4U ① EUoLl >u 山 u-BJh-
aU ① EUo->u 山4js ①h-
Success Rate
Figure 10:	The ML10 results of different methods after 3-shot adaptation.
24
Under review as a conference paper at ICLR 2022
ML-45 Maximum Per-Task Success Rates (N=20)
DMIL
MIL
MLSH
OptionGAIL
PERMIRL
4ju ① EUoLl >u 山 ⊂-2π
disassembled -
door-close-v2 -
door-open-v2 -
drawer-close-v2 -I
d rawer-open-v2
faucet-open-v2
faucet-close-v2
hammer-v2
handle-press-side-v2
handle-press-v2 -
handle-pull-side-v2 -
handle-pull-v2 -
lever-pull-v2 -
peg-insert-side-v2 -
pick-place-wall-v2 -
pick-out-of-hole-v2 -
reach-v2 -
4ju ① EUo->u 山4js ① J.
push-back-v2 -
push-wall-v2 -
reach-wall-v2 -
shelf-place-v2 -
sweep-into-v2 -∖
sweep-v2
window-open-v2
window-close-v2
O 20	40	60	80	100
Success Rate
Figure 11:	The ML45 results of different methods after 3-shot adaptation.
25