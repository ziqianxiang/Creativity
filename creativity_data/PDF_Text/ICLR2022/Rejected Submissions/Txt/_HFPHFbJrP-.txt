Under review as a conference paper at ICLR 2022
Certified Adversarial Robustness
Under the Bounded Support Set
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks (DNNs) have revealed severe vulnerability to adversarial
perturbations, besides empirical adversarial training for robustness, the design of
provably robust classifiers attracts more and more attention. Randomized smooth-
ing method provides the certified robustness with agnostic architecture, which is
further extended to a provable robustness framework using f -divergence. While
these methods cannot be applied to smoothing measures with bounded support
sets such as uniform probability measures due to the use of likelihood ratio in
their certification methods. In this paper, we introduce a framework that is able to
deal with robustness properties of arbitrary smoothing measures including those
with bounded support set, by using Wasserstein distance as well as total vari-
ation distance. By applying our methodology to uniform probability measures
with support set B2 (O, r), we obtain certified robustness properties concerning
lp -perturbations. And by applying to uniform probability measures with support
set B∞(O, r), we obtain certified robustness properties with respect to l1 , l2, l∞-
perturbations. We present experimental results on CIFAR-10 dataset with ResNet
to validate our theory. It is worth mentioning that our certification procedure only
costs constant computation time, which is an improvement upon the state-of-the-
art methods in terms of the computation time.
1	Introduction
Vulnerability to adversarial samples is a major obstacle that various classifiers obtained by machine
learning algorithms, especially deep neural networks (DNNs), need to overcome (Szegedy et al.,
2013; Nguyen et al., 2015). For instance, in computer vision applications, deliberately adding some
subtle perturbation δ that humans cannot perceive to the input image x will cause DNNs to give a
wrong classification output with high probability. Many empirical adversarial defenses have been
proposed, among which adversarial training (Madry et al., 2018) is the most effective one (Athalye
et al., 2018), however, it still faces stronger or adaptive attacks to decrease its effectiveness to a
certain degree (Croce & Hein, 2020). This motivates research on certified robustness: algorithms
that are provably robust to the worst-case attacks.
Some works propose algorithms to learn DNNs that are provably robust against norm-bounded ad-
versarial perturbations by using some convex relaxation methods (Wong & Kolter, 2018; Weng et al.,
2018; Xiao et al., 2018; Zhang et al., 2019). However, these approaches are usually computationally
expensive and require extensive knowledge of classifier architecture. Besides, randomized smooth-
ing (first introduced in Cohen et al. (2019)) has received significant attention in recent years for
verifying the robustness of classifiers. Based on this method, several papers have studied which
smoothing strategies perform better for specific lp perturbations. Cohen et al. (2019) concludes that
randomized smoothing can be well understood for the l2 case by using Gaussian probability measure
for smoothing. It follows that several special cases of the conjecture have been proven forp < 2. Li
et al. (2018) show that l1 robustness can be achieved with the Laplacian distribution, and Lee et al.
(2019) show that l0 robustness can be achieved with a discrete distribution. Other papers start from
the opposite perspective and focus on studying under specific assumptions which perturbation is
provably difficult to handle and which smoothing methods are ineffective for particular disturbance.
As for the existence of a noise distribution that works for the case of p > 2, Blum et al. (2020)
and Kumar et al. (2020) show hardness results for random smoothing to achieve certified adversarial
robustness against attacks in the lp ball of radius . Nevertheless, since these works provide hard-
ness results for every possible base classifier f : Rd → Y including those unusual and even bizarre
ones, hardness results given by these papers might be attributed to taking into account classifiers that
1
Under review as a conference paper at ICLR 2022
will never appear in real-world applications. From this perspective, the order of difficulty restricted
within the common classifiers subset still remains unresolved.
Notably, based on randomized smoothing strategy, Dvijotham et al. (2020) introduce a provable ro-
bustness framework using f -divergence as their convex relaxation technique. However, due to the
use of likelihood ratio in their certification methods, the framework cannot be applied to smoothing
measures with bounded support sets such as uniform probability measures. In this paper, we intro-
duce a framework that is able to deal with robustness properties of arbitrary smoothing measures,
including those with bounded support set, by using Wasserstein distance as well as total variation
distance. Our contributions are summarized as follows:
•	By applying our methodology to uniform probability measures with support set B2(O, r),
we obtain certified robustness properties with respect to lp-perturbations. By applying
our methodology to uniform probability measures with support set B∞ (O, r), we obtain
certified robustness properties with respect to l1, l2, l∞-perturbations.
•	Furthermore, we analyze the cases when smoothing measure is taken as uniform proba-
bility measure with more general support set Bp(O, r) and show the unavoidable curse of
dimension for the usage of bounded support set smoothing measures.
•	We present experimental results on CIFAR-10 dataset with ResNet to validate our theory. It
is worth mentioning that our certification procedure only costs constant computation time,
which is an improvement upon the state-of-the-art methods in terms of the computation
time.
2	Problem Setting
Given a binary base classifier h : Rd → Y = {±1} and smoothing probability measure μ, the
randomly smoothed classifier hμ(χ) is defined as follows.
Definition 1 (smoothed classifier, smoothing measure). The smoothed version of a base binary
classifier h producing labels in set Y = {±1} is defined as
hμ(x) = argmaχ Pχ~χ+*[h(X) = y],
y∈Y
(1)
where μ ∈ P(X) is called smoothing measure.
Another way to understand this definition is to say that the smoothed classifier first scores point x
as hμ,y (x) = PX~χ+μ[h(X) = y] for each specific class y ∈ Y and then outputs the class y* with
the highest score. We want to study the robustness of the smoothed classifier hμ against adversarial
perturbations of size at most E with respect to a given norm || ∙ ||p. The question that whether a
bounded lp norm adversarial attack on a fixed input X satisfying hμ(χ) = +1 is successful or not
can be formulated as solving the optimization problem below:
min	Pχ~χo+μ[h(X ) = +1].	(2)
llx0-xllP≤e
The attack is successful if and only if the minimum value is smaller than 1. Since We know little
about the information of the black-box classifier h, we follow the approach introduced in Dvijotham
et al. (2020): rather than studying the adversarial attack in the input space X, we study it in the space
of probability measures defined on input space P(X),
min Pχ~χo+μ[h(X) = +1] = min Pχ~ν [h(X) = +1],	(3)
∣∣x0-x∣∣≤e	ν∈Dχ,e,q
where Dχ,e,q := {χ0 + μ : ||X - χ0 ||q ≤ e} represents an lq -norm-based constraint set of radius E for
smoothing measure μ centered at a particular sample point x. Then, we follow the full-information
robust certification framework established in Dvijotham et al. (2020) and analyze the generalization
of binary classifier h, which they called specification and denote it as φ : X ⊆ Rd → Z ⊆ R.
Besides, define reference measure P as X + μ and a collection of perturbed probability measures
D ⊆ P(X). Checking whether a given specification φ is robustly certified at ρ with respect to D or
not is equivalent to estimating the optimal value of following optimization problem is non-negative
or not:
OPT(φ,ρ, D) := minEχ~ν[Φ(X)]∙	(4)
ν∈D
And certifying lp robustness on input X with output of smoothed classifier hμ(x) = +1 is equivalent
to verify whether OPT(h, X + μ, Dχ,e,q) ≥ 0 or not.
2
Under review as a conference paper at ICLR 2022
3	Our Certification Procedures
Notice that the certification in Dvijotham et al. (2020) uses the likelihood ratio r(X) = P(XX), while
r(X) is well-defined only when the support set of ρ contains the support set of ν. Thus, when the
support set of reference measure ρ is bounded, and ν takes even a small translation of ρ, the support
set of ν will cross over the boundary of support set of ρ. Their certification is invalid in this case.
In this paper, by using Wasserstein distance as well as total variance distance, we provide analytical
techniques able to analyze bounded support set which is not covered by Dvijotham et al. (2020).
Since the set of measures Dχ,e,q constraint in optimization problem OPT(h, X + μ, Dχ,e,q) is in-
tractable to deal with, we consider relaxations of this by using Wasserstein distance as well as total
variance distance constraints between V and X + μ, i.e. Dχ,e,q ⊆ {ν : Wp(X + μ, ν) ≤ δ} := Dχ,δ,p
which represents Wp-distance-based constraint set of radius δ for smoothing measure μ centered at
sample point X and D ⊆ {ν : TV(X + μ, ν) ≤ ξ} := Dχ,ξ which represents TV-distance-based
constraint set of radius ξ for smoothing measure μ centered at sample point x. Combining the two
relaxations, we know Dx,,q ⊆ Dx,δ,p ∩ Dx,ξ and therefore
OPT(h, X + μ, Dχ,e,q) ≥ OPT(h, X + μ, Dχ,δ,p ∩Dχ,ξ).	(5)
And we can obtain tighter relaxation by combining multiple Wasserstein distance relaxation with
different p, i.e. Dx,,q ⊆ Ti∈I Dx,δi,pi ∩ Dx,ξ where I ⊆ R+ and therefore
OPT(h, X + μ, Dχ,e,q) ≥ OPT (h, X + μ, ( \ Dχ,δi,pJ ∩ Dχ,ξ) .	(6)
Thus, for a fixed input X, it suffices to consider the Wasserstein distance and total variance distance
relaxed problem and verify whether OPT(h, X + μ, Dχ,δ,p ∩ Dχ,ξ) ≥ 0 or not. The analysis of this
problem can be divided into three parts: computing the Wasserstein distance relaxation measure set,
computing the total variance distance relaxation measure set, and computing the Lagrange function
as well as dual problem of the relaxed optimization problem OPT(h, X + μ, Dχ,δ,p ∩ Dxξ). The
details are discussed in the following three sections.
3.1	Relaxation Using Wasserstein Distance
In this section, we show the following relaxation from norm-based constraint sets into W-distance-
based constraint sets for general smoothing measures as well as Gaussian smoothing measure.
Dχ,e,q =	{x0	+ μ :	||X -	X0IIq ≤	e}	⊆ {ν :	WP(X + μ,	V)	≤	δ}	=	Dχ,δ,p.	⑺
3.1.1	General Probability Measure
Here, we want to find a δq() such that
Dχ,e,q = {x0 + μ : ∣∣X - X0IIq ≤ e} ⊆ {ν : WP(X + μ, V) ≤ δq(e)} = Dχ,δq(e),p for all μ ∈ P(X).
(8)
Theorem 3.1.	For all X ∈ Rd, > 0, q > 0, norm-based constraint set Dx,,q can be relaxed
into W-distance-based constraint set Dxδq(e),p With radius δq(E) = max{e, Ed2-q } which can be
formulated as
Dχ,e,q ⊆ {ν : Wp(x + μ, V) ≤ max{E, Ed2 -1 }} := Da maχ {e ^1 - 1yp	⑼
And this relaxation radius max{E, Ed 1-1} WorkSfor any Wasserstein distance parameter P > 0 as
well as any smoothing measure μ.
Note that for lq(q ≤ 2) adversarial perturbations, the relaxed radius avoids the influence of dimen-
sion d, whereas for Iq(q > 2) adversarial perturbations, as q increases, 2 - 1 increases from 0 to 2
correspondingly. The fact that the radius of Wq-distance constraint set grows with order Θ(d1 - 1)
provides us with an intuition that it is increasingly harder to bound Dx,,q with larger q, therefore,
W-distance-relaxation works better for lq(q ≤ 2) norm perturbation. And this relaxation radius is
tight for W2 distance and Gaussian smoothing measures which is proved in the appendix D and
therefore shows that W2-distance-relaxation works well for Gaussian smoothing measure.
3
Under review as a conference paper at ICLR 2022
3.2 Relaxation Using Total Variance Distance
In this section, we show the following relaxation from norm-based constraint sets into TV-distance-
based constraint sets for Gaussian and uniform smoothing measures.
Dχ,e,q = {x0 + μ ： ||X - x0∣∣q ≤ e}⊆{ν : TV(X + μ, V) ≤ ξ} = Dχ,ξ	(10)
3.2.1	Gaussian Probability Measure
Here, We want to find a ξ(e) for Gaussian measure μ = N(0, σ2I) such that
Dx,e,q = {x0 + M ： ||X - x0Hq ≤ e}⊆{ν ： TV(X + M, ν) ≤ ξ(E)} = Dx,ξ(e)∙	(II)
The magnitude of ξ () is given by the following theorem.
Theorem 3.2.	For Gaussianprobability measure μ = N(0, σ2I) on Euclidean space Rd andfor all
X ∈ R, E > 0, q > 0, norm-based constraint set Dx,,q can be relaxed into TV-distance-based con-
1 — 1
StraintSet Dχ,ξ(e) With radius ξ(e) = 2G(max{'2：__q}) — ι where G is the cumulative distribution
function for standard normal distribution N(0, 1) which can be formulated as
Dx,,q ⊆
v : TV (x + μ, v) ≤ 2G ( max
(12)
This theorem theoretically shows that TV distance relaxation works effectively for lq(q ≤ 2) per-
turbation due to the irrelevance of the radius to dimension d and increasingly bad for lq (q > 2)
perturbation because of the dependence of the radius to dimension d as order Θ(d1 - 1).
3.2.2 Uniform Probability Measure
Here, we want to find a ξ(e) for uniform measure μ = U (K), where K is a specific convex compact
set, with density function fκ (x) = VOl(K)Iχ∈κ such that
Dχ,e,q = {x0 + μ ： ∣∣X — X0∣∣q ≤ ∈}⊆{v ： TV(x + μ, V) ≤ ξ(6)} = Dχ,ξ(e)∙	(13)
In this paper, we mainly focus on the case when K is lp -norm ball centered at original point O with
radius r, i.e., K = Bp(O, r). We give following theorems about special cases when p = 1, 2, ∞.
Theorem 3.3.	When K is an l1 norm ball centered at O with radius r, for uniform probability
measure U(K) on Euclidean space Rd, we have
Dχ,e,q \ {v ： TV(x + μ, v) ≤ 1 — δ} = 0 for all q > 1 and arbitrarily small δ > 0,	(14)
when E ≥ 2rd 1T. Note that E ≥ √rd which decays with order Θ(d-2) for q = 2 and E ≥ 2dr which
decays with order Θ(d-1)for q = ∞.
This theorem theoretically shows that for uniform smoothing measures with l1 ball support set, and
total variance distance failed to relax measure set Dx,,q effectively when q = 2, ∞. And this will
consequently lead to bad performance for l2 and l∞ robustness certification task, which can be seen
from the following section discussing the importance of TV-distance-based relaxation radius.
Theorem 3.4.	When K is an l2 (Euclidean) ball centered at O with radius r, for uniform
probability measure U(K) on Euclidean space Rd and for all X ∈ R, E > 0, q > 0, when
E > min{2r, 2rdq - 2 }, norm-based constraint set Dχ,e,q failed to be relaxed into TV-distance-
based constraint set which can be formulated as
Dχ,e,q \ {v ： TV(x + μ, v) ≤ 1 — δ} = 0 for all q > 1 and arbitrarily small δ > 0.	(15)
And when E ≤ min{2r, 2rd 1- 2 }, norm-based constraint set Dχ,e,q can be relaxed into valid TV-
1 - 1
max{,d 2 q }
「arccos (------------------------------------------------------------2r...-) - n
distance-baSedCOnStraintSet Dχξ(e) With radius ξ(c) = 1 —10-----R∏~^―[ʒ―SSn ( ) which Can
be formulated as
1 - 1
m max{e,ed 2 q } ∖
R	,	、	0arccos() sinn(t)dt]
Dχ,e,q ⊆ v v ： TV(x + μ, v) ≤ 1-------------π-------------------卜.	(16)
I	Ro2 sinn(t)dt	J
4
Under review as a conference paper at ICLR 2022
This theorem shows that for uniform smoothing measures with l2 ball support set, when q ≤ 2,
relaxation radius is independent of dimension d, whereas when q > 2 relaxation radius starts to be
bound up with dimension d and the impact of d grows as q increases. To put it another way, total
variance distance relaxation performs well for uniform smoothing measures with l2 ball support set
when q ≤ 2 and increasingly poor when q > 2.
Theorem 3.5. When K is an l∞ cube centered at O with radius r, for uniform probability measure
U(K) on Euclidean space Rd and for all x ∈ R, > 0, q > 0, when > 2r, norm-based constraint
set Dx,,q failed to be relaxed into TV-distance-based constraint set which can be formulated as
Dχ,e,q \ {ν : TV(X + μ, ν) ≤ 1 — δ} = 0 for all q > 0 and arbitrarily small δ > 0.	(17)
And when ≤ 2r, norm-based constraint set Dx,,q can be relaxed into valid TV-distance-based
constraint set Dχξ(e). When q = 1, ξ(e) Can be taken as 皋,which can beformulated as
Dχ,e,ι ⊆ {ν: TV(X + μ,ν) ≤ 2r}.	(18)
When q = 2, ξ(e) can be taken as 1 一(1 一 )d when 0 < e ≤ 2tnr where
tn approaches 1 at an exponential rate, which can be formulated as
≤ tn < 1 and
Dx,∈,2 ⊆ {ν ： TV(X + μ, V) ≤ 1 一 (1 一 2胪)O ≈ {ν : TV(X + μ,ν) ≤ 1 一 e 2rd2 O, (19)
ξ(e) can be taken as 1 一 (" 1+"(/r) d+1)	(1 √d( 2d) d+1) When 2tnr < e < 2r, which
can be formulated as
d d d TLV ,	dd - 1 + Pd( (2r)2 — d + 1、d-111 - Pd 2T)2 — d +1
Dx,e,2 ⊆ ν ν : TV(X + μ,ν) ≤ 1 一 (----------dr---------)	2d-------2d----------. ).
(20)
When q = ∞, ξ(e) can be taken as 1 —(1 一提),which can beformulated as
Dx,e,∞ ⊆ { ν ： TV(X + μ,ν) ≤ 1 一 (1 一 ^r) }.	QI)
As for uniform smoothing measures with l∞ cube support set, this theorem shows that the perfor-
mance towards lι perturbation turns out to be fine since TV distance relaxation radius * has nothing
to do with dimension and the dimensional curse is avoided. However, in this case, TV distance
relaxation shows incapability to cope with l2 and l∞ perturbation in some extent due to the rate of
1
increasing radius tending to 1 as Θ(ed2) and Θ(ed).
After discussing the special cases when K is an l∞ cube or an l2 Euclidean ball, we then consider
the general case when K is an lp ball centered at the original point with radius r and give a lower
bound for TV distance relaxation radius in the following theorem.
Theorem 3.6. When K is an lp ball centered at O with radius r, for uniform probability measure
U (K) on Euclidean space Rd and assume norm-based constraint set Dx,e,q can be relaxed into
TV-distance-based constraint set Dx,ξ(e), then
ξ(e) ≥ 2
edP
1
4r(pe) P Γ(1+p )
0
exp (- - e(2∕Γ(1 + ɪ))p) dX
pp
for all perturbation norm parameter q > 0 with high probability when d is sufficiently large, which
can be formulated as
1
：dP
Dχ,e,q \ { V : TV(x + μ, v) ≤ 2 /4r(pe) 1r(1+1
exp (- - e(2∕Γ(1 + ɪ))p)dX — δl = 0 (22)
p
p
for arbitrarily small δ > 0.
5
Under review as a conference paper at ICLR 2022
K/ɪ 					- X			
6
Under review as a conference paper at ICLR 2022
Smoothing
Measure
Perturbation Certification Objective
Prerequisite
U(B2(O,r))	lq(q ≤ 2) lq(q > 2)	B7 -	l^J√ VM _ 9 /		1一 1一	Rry )sinn(t)dt)	E ≤ 2r 1_1 E ≤ 2rdq 2
		jX 〜x+μ [VΛ∕∖ )∖ EX 〜χ + μ[φ(X )]	一2		RO2 sinn(t)dt	J ,arCCos (id2―q )	,、	\ RO	」	Sin (t)dt j	
					RO2 sinn (t)dt	J	
U(B∞(O,r))	l1	~~EX 〜x+μ[Φ(X )]	—ɪ r			E ≤ 2r
	l2	EX 〜x+μ[Φ(X )]	一2	1一	O-血 f)	E ≤ 2tnr
	l∞	EX 〜x + μ[φ(X )]	一2	1一	(1- W	E ≤ 2r
N (0, σ2I)	lq(q ≤ 2)	EX 〜x + μ[φ(X )]	一 2(2G(爵) 一 1)			-
	lq(q > 2)	EX 〜x+μ[Φ(X )]	一 2(2G(		修)一 1)	-
Table 1: Certification objectives and prerequisites.
where λ ≥ 0 is the dual variable w.r.t. constraint sup||f ||L ≤1 f (x)q(x)dx ≤ δ or constraint
suP∣∣f ∣∣l≤p(2R*)p-i J f (x)q(x)dx ≤ δp + (p — 1)(2R*)p-1 and C := δ when 0 < p ≤ 1 whereas
C := δp + (p 一 1)(2R*)p-1 whenP > 1.
Using the duality result, we know the optimal value in (23) can be obtained by computing
max EX〜x+μ[φ(X)] — ξ 一 λC = EX〜x+μ[φ(X)] — ξ,	(26)
λ≥0
which is only related to the radius ξ of TV distance relaxation set. We can see from this formula the
significance of TV distance relaxation radius. By plugging the TV distance relaxation radius given
in theorem 3.4, 3.5 and 3.2 in dual optimization problem, we obtain the certification objective in
Table 1 and we return certified for lp norm perturbation with magnitude if the objective function
has non-negative value.
3.4 Relationship with previous work
By applying our methodology to Gaussian probability measure, we miraculously obtain the same
certified robustness properties provided in Dvijotham et al. (2020) using as Hockey-stick divergence
with β = 1.
Theorem 3.10. When smoothing measure is taken as Gaussian probability measure, the certificate
EX 〜x+μ[Φ(X )]-2(2G( ɪ ) — 1)given in our paper is equivalent to the certificate E HS ,ι ≤ [θa-θb ]+
given in paper Dvijotham et al. (2020).
Therefore, when applying both methodologies to Gaussian measure, the formulas obtained are the-
oretically equivalent. Despite the similarity in analyzing Gaussian measure, our work covers cases
with bounded support sets, which is our main contribution.
4	Experiments
For adversarial robustness certification, we choose the test set certified accuracy as our metric of
interest, which is defined as the percentage of data points in the test set that can be correctly classified
and can also pass the robustness certification within an l2 ball of an assigned radius r. To pass the
robustness certification at data point x, the classification results of all points within an l2 distance to
the original point x must be consistent. For a model using the smoothing method, the classification
result of a data point is the class with the highest score in the smoothed data distribution, not to be
confused with the direct output of the base classifier at that data point.
In all experiments, the certification process on the test set with assigned perturbation l2 radius is
shown in the following Algorithm 1. Note that the cert(scorea, 1 一 scorea, r) function returns true
if the certification objective is non-negative, otherwise it returns false. Such objective is calculated
7
Under review as a conference paper at ICLR 2022
using formulas in Table 1 with l2 perturbation and corresponding smoothing distribution. Since our
method using Wasserstein distance does not involve iterations, our certification procedure only costs
constant computation time, which is much faster than Dvijotham et al. (2020).
Algorithm 1 Certification process
Input: T: test set, gt(x): true class of image x, f(x): base classifier, D(x): smoothing distribution,
n: sample amount, r: perturbation radius
Output: acc: test set certified accuracy
1: 2: 3:	CertCount J 0, allCount J 0 for all x ∈ T do S J {n samples from D(x)}
4: 5: 6: 7: 8: 9:	Countc J 0 for every class C for all x0 ∈ S do Countf (x0) J Countf (x0) + 1 end for SCoreC J CaunS) for every class C a J argmaxc{sCorec}
10:	if a = gt(x) ∧ Cert(sCorea, 1 - sCorea, r) then
11:	CertCount J CertCount + 1
12:	end if
13:	allC ount J allC ount + 1
14: 15:	end for return acc J— CeTtC°unt return acc J αιιcθunt
We achieve identical results when comparing our W-distance method with the F-divergence method
in Dvijotham et al. (2020) using Gaussian distribution and with specific metric parameter settings,
which is proved possible in Section 3.4. However, there is no previous work done yet to examine
the usage of uniform distribution when smoothing, so we mainly focus on comparing Gaussian, l2
and l∞ uniform distribution all using our W-distance method.
4.1	Setups
We choose CIFAR-10 as our dataset and ResNet-110 as our base classifier. We firstly train the base
classifier on the 50000 image training set without smoothing and achieve 89.6% prediction accuracy
on the 10000 image test set. Then we run the certification process on the test set with incremental
perturbation radius r. We test out different smoothing distributions as mentioned above, and we
change the parameters of such distributions to illustrate the effect of different distributions further.
We also try increasing the smoothing sample amount to examine the trade-off between performance
and accuracy improvement. All training, testing, and certification are run on an NVIDIA RTX 3090.
4.2	W-distance method with different smoothing distributions
We firstly implement our W-distance method with N(x, 0.05) as smoothing distribution and sample
amount n=100, and then we change the variance of the Gaussian distribution to 0.025 and 0.1. As
shown in Figure 2, there is a neat cut-off for each setting where the perturbation gets too big, and no
data point can pass the certification at this point. By changing the variance of the smoothing distri-
bution, we observe a clear trend that the increase of variance leads to a drop of initial certification
Figure 2: Results of different smoothing distributions using our W-distance method. Sigma stands
for variance for Gaussian distribution and the norm range for uniform distribution.
8
Under review as a conference paper at ICLR 2022
accuracy but also stronger robustness that can endure more significant perturbation, and the decrease
of the variance leads to the opposite change accordingly.
Next, for smoothing process, we substitute Gaussian distribution with l2 or l∞ uniform distribution,
with the norm range set to 0.025, 0.05 and 0.1. In Figure 2, both experiment results show identical
characteristics as with Gaussian distribution, but they bring along a critical issue, the mismatch of the
perturbation radius’s magnitude. Comparing the perturbation radius at the cut-off point, we find that
the radius of Gaussian distribution is about 50 times larger than that of two uniform distributions.
We assume that this phenomenon is caused by the lack of intersection of the smoothing distributions
before and after perturbation. For Gaussian distribution, there is always an intersection no matter
how big the perturbation radius gets, but two uniform distributions will separate quickly and become
disjoint under perturbation. Furthermore, the dimension of a 32 × 32 × 3 image is 3072, the square
root of which is around 55.4, very close to the cut-off radius’s 50 times ratio difference. Such
correlation may trace to the involvement of dimension when calculating the finite support set volume
of l2 and l∞ uniform distribution, while the support set volume of Gaussian distribution is infinitely
large. We conjecture that such deficiency is inherent when using the uniform distribution, which can
hardly be further improved.
4.3	W-distance method with different sampling amounts
When calculating scores for each class in the smoothing process, as we cannot classify all possible
data points, we shall only acquire approximate scores by sampling from the smoothed data distri-
bution. Thus such scores may differ in multiple runs due to the randomness of sampling. However,
through our experiments, we find that with a certain amount of samples, we can already obtain suf-
ficiently accurate scores, which cannot be significantly improved by increasing the sample amount.
Figure 3: Results on sample amounts with different smoothing distributions using W-distance.
We set the sample amount n to 100, 1000, and 10000 with three different smoothing distributions,
and they all obtain similar results: it takes only 10 minutes to run through the 10000 images test set
with 100 samples for each image, 30 minutes with 1000 samples and 3 hours with excessive 10000
samples. It is ten times faster than the 2 hours running time with the iteration-based method in
Dvijotham et al. (2020) using just 100 samples. It is also worth noting in Figure 3 that by increasing
the sample amount, no significant improvement is observed with Gaussian distribution. However,
there is minor progress made with both uniform distributions when the samples are getting overly
abundant. We assume that the extra samples make up for the lack of intersections of smoothing
uniform distributions before and after the perturbation, while Gaussian distribution has no such
issues.
5	Conclusion
We have introduced a framework based on Wasserstein distance and total variance distance relax-
ation as well as Lagrange duality. This methodology is able to deal with the analysis of bounded
support set smoothing measures, which is not covered by previous work. Moreover, we have ana-
lyzed the performance of specific smoothing measures, including Gaussian probability measure and
uniform probability measures with support set B2 (O, r), B∞ (O, r) theoretically and experimen-
tally, which shows the relative incapability of bounded support set smoothing measures compared
with Gaussian smoothing measure.
9
Under review as a conference paper at ICLR 2022
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, 2018.
Avrim Blum, Travis Dick, Naren Manoj, and Hongyang Zhang. Random smoothing might be unable
to certify l∞ robustness for high-dimensional images. Journal Of Machine Learning Research,
2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In ICML, 2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020.
D.C DoWson and B.V Landau. The fτechet distance between multivariate normal distributions.
Journal of Multivariate Analysis, 1982.
Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gy-
orgy, Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of
smoothed classifiers using f-divergences. In ICLR, 2020.
Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on
randomized smoothing for certifiable robustness. In ICML, 2020.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. arXiv preprint arXiv:1906.04948, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. arXiv preprint arXiv:1809.03113, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Matthew Thorpe. Introduction to optimal transport, 2018.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In ICML,
2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In ICML, 2018.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster
adversarial robustness verification via inducing relu stability. arXiv preprint arXiv:1809.03008,
2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks.
arXiv preprint arXiv:1906.06316, 2019.
10
Under review as a conference paper at ICLR 2022
A Optimal Transport Theory
Assume μ, ν ∈ P (Rd). Besides, assume μ, V are absolutely continuous w.r.t. LebesgUe measure λ
and let density functions be f and g .
Definition 2 (Push Forward). If T : Rd → Rd, then the distribution of T (X) is called the push-
forward of P, denoted by T#P. In other words,
T#P(A) = P(T (x) ∈ A) = P(T-1 (A))
Definition 3 (Optimal Distance, Optimal Transport Map). The Monge version of the optimal trans-
port distance is
T:Ti#nPf=Q	||x - T (x)||pdP (x)
(27)
A minimizer T*, ifone exists, is called the optimal transport map.
Definition 4 (Wasserstein Distance, Earth Mover Distance, Optimal Transport Plan). Let Γ(μ, V)
denote all joint distributions Y for (X, Y) that have marginals μ and V. Then the Wasserstein
distance is
Wp(μ,ν )=(	inf I ||x - y∣∣pdγ (x,y) ) where P ≥ 1
∖7∈Γ(μ,ν) J	)
(28)
When p = 1, this is also called the Earth Mover distance. The minimizer γ* (which does exist) is
called the optimal transport plan.
Lemma A.1 (Dual Formulation of Wasserstein Distance When p ≤ 1). It can be shown that
Wp(μ,ν) = sup / ψ(y)dν(y) - / φ(x)dμ(x)
ψ,φ
(29)
where ψ(y) - φ(x) ≤ ||x - y||p. In the special case when p = 1, we have the very simple represen-
tation
Wι(μ,ν) = sup / 夕(x)dμ(x)- / 夕(x)dν(X)= sup / 夕(x)d(μ-ν)(x) = SUP / 夕(x)(f-g)(x)dx
0∈F1 J	J	φ∈Fι J	φ∈Fι J
(30)
where F1 denotes all maps from Rd to R such that |f (x) - f(y)| ≤ ||x - y|| for all x, y. In the case
when 0 < P < 1, we have similar simple representation
Wp(μ,ν)= SUP / 夕(x)dμ(x)- / 夕(x)dν(X)= SUP / 夕(x)d(μ-ν)(x) = SUP / 夕(x)(f-g)(x)dx
φ∈Fp J	J	φ∈Fp J	φ∈Fp J
(31)
where Fp denotes all maps from Rd to R such that |f (x) - f (y)| ≤ ||x - y||p for all x, y.
Lemma A.2 (Dual Formulation of Wasserstein Distance When 1 < p < ∞). In the case when
1 < p < ∞ and the Support sets of measure μ and V are included in a convex compact set K.
Define R = supx∈K ||x||2 , then we have slightly different dual formulation
Wp(μ,ν) ≥(	SUp	[ ψ(y)d(ν - μ)(y)-(p - 1)(2R)PT)
∖ φ∈Lip(p(2R)p-1) J	)
=(	sup	[P(y)(g - f )(y)dy -(P - 1)(2R)PT
∖ φ∈Lip(p(2R)P-1) √
1
P
1
P
(32)
where Lip(P(2R)P-1) denotes all maps f from Rd to R such that |f (X) -f(y)| ≤ P(2R)P-1 ||X-y||
for all X, y ∈ K.
Definition 5 (Total Variation Distance). The total variation distance between two probability distri-
bution μ and V on Rd is defined by
∣∣μ - V∣∣τv = max {∣μ(A) - v(A)∣ : A ⊆ Rd}	(33)
where Rd is the set of all Borel subsets.
Lemma A.3. Let μ and V be two probability distributions on Rd and absolutely continuous w.r.t.
Lebesgue measure λ. Assume the density function of measure μ and V w.r.t. λ are f (x) and g(x).
Then,
∣∣μ - v∣∣tv = 1 /d ∣f(X)- g(X)IdX	(34)
11
Under review as a conference paper at ICLR 2022
B Proof of Lemma A.2
Recall the dual form of Wasserstein distance
Wp(μ,ν) =	SUp / ψ(y)dν(y) - / φ(x)dμ(x)
ψ,φ∈C(Rd)
(35)
where ψ(y) - φ(x) ≤ ||x - y||p.
For simplicity of the proof, consider equivalent form
Wp(μ, ν) =
sUp
ψ,φ∈C(Rd)
/ ψ(y)dν(y) + / φ(x)dμ(x)
(36)
where ψ(y) - φ(x) ≤ ||x - y||p. First, we introduce a theorem in Thorpe (2018)
Theorem B.1 (Existence of a Maximiser to the Dual Problem). Let μ ∈ P(X), V ∈ P(Y), where
X and Y are polish, and C : X X Y → [0, +∞). Assume that there exists CX ∈ L1 (μ), CY ∈ L1 (V)
such that c(x,y) ≤ CX (x) + CY(y) for μ-almost every X ∈ X and V-almost every y ∈ Y. In
addition, assume that
M:
/ CX (x)dμ(x) + / CY(y)dν(y) < ∞
XY
(37)
Then there exists (φ,ψ) ∈ Φc = {(夕,ψ) ∈ L1(μ) × L1(ν):夕(x) + ψ(y) ≤ c(x, y)} where the
inequality is understood to holdfor μ-almost every X ∈ X and V-almost every y ∈ Y such that
sup J = J(中,ψ)
Φc
(38)
where J is defined by J : L1(μ) × L1(ν) → R, J® ψ) = JX φdμ + JY ψdν. Futhermore we can
choose (夕，ψ) = (ηcc, ηc) for some η ∈ L1(μ). For η : X → R, the c-transforms ηc, ηcc are defined
by
ηc : Y → R, ηc(y) = inf (c(x, y) 一 η(x))	(39)
x∈X
ηcc : Y → R, ηcc(y) = inf (c(x, y) — ηc(x))	(40)
x∈X
Lemma B.1. For a, b ∈ R and 1 ≤ p < ∞,
|a+b|p ≤ 2p-1(|a|p + |b|p)	(41)
Proof. First, it’s easy to verify the cases when either of a = 0, b = 0, p = 1 holds. Then, Wlog,
assume a, b ∈ R+
|a+b|p ≤ 2p-1(|a|p + |b|p)
O (a + b)p ≤ 2p-1(ap + bp)
O2p-1(( o⅛ )p + ($)p)≥ 1
^⇒ 2p-1[xp + (1 — x)p] ≥ 1, ∀x ∈ (0,1)
where the last inequality is easy to verify.
□
In our case, C(X, y) = ||X — y||p ≤ (||X|| + ||y||)p ≤ 2p-1(||X||p + ||y||p) and the requirement that
M < ∞ is exactly the condition that μ and V have finite pth moments which is easy to verify by
noting that supp(μ) = SUPP(V) = K is compact set in Rd. Then, according to the theorem, there
exists η ∈ L1(μ) such that
wp(μ,V)=
SUp I ηc(y)dV(y) + I ηcc(x)dμ(x)
η∈L1(μ) J	J
(42)
Note that ηc possesses Lipschitz continuous property stated below
12
Under review as a conference paper at ICLR 2022
Lemma B.2. For η ∈ L1 (K) where K ⊆ Rd is a convex compact set, then ηcp is a p(2R)p-1 -
Lipschitzfunction where R := SUPxeK l∣x∣l and Cp(x, y) = ∖∖x — y∖∖p, i.e.,
∣∣ηcp(x) — ηcp (y)∣∣ ≤ p(2R)p-1∣∣x — y∣∣,	x,y ∈ K	(43)
Proof.
∣ηcp(X) — ηcp3)∣ = I inf (∣∣x — zι∣∣p — η(ZI)) — inf (∣∣y — z2∣∣p — η(Z2))|	(44)
I Z1∈K	Z2∈K	I
=I	inf	suP	((∣∣x —	zι∣∣p	—	∣∣y	— z2∣∣p)	—	(η(zι)	—	η(z⅛))) I
zι∈κ z2∈κ
≤ sup I (∣∣x — z∣∣p — η(z)) — (∣∣y — z∣∣p — η(z)) I	(45)
z∈K 1	1
=sup I ∣∣x — z∣∣p — ∣∣y — z∣∣p ∣
z∈K
where 44 is due to the definition of c-transform; 45 is obtained by taking a specific value of zι as z2.
Note that K is a compact set and I ∣∣x — z∣∣p — ∣∣y — z∣∣p I is a continuous function w.r.t. z, then there
exists a point z* such that I ∣∣x — z*∣∣p — ∣∣y — z*∣∣p I = supz∈κ I ∣∣x — z∣∣p — ∣∣y — z∣∣p I . According
to the first order condition, z* satisfies the equation below
Vz (∣∣x — Z∣∣p — Uy — Z∣∣p) = Vz ∣∣x — z∣∣p — Vz ∣∣y — z∣∣p = Vz ∣∣z — x∣∣p — Vz ∣∣z — x∣∣p
=p∣∣z — x∣∣2 T(Z — x)τ — p∣∣z — y∣∣P T(Z — y)τ = 0	(46)
=⇒∣∣z — x∣∣2 T(Z — x)τ = ∣∣z — y∣∣2 T(Z — y)τ
=⇒(∣∣z — x∣∣P T — ∣∣z — y∣∣2 T)ZT = ∣∣z — x∣∣2-1xτ — ∣∣z — y∣∣2 TyT
=⇒ =	∣∣z — x∣∣2 T__________________∣∣z — y∣∣p T_________
Z ∣∣Z — x∣∣P T — ∣∣Z — y∣∣2 Te	∣∣Z — x∣∣2 T — ∣∣z — y∣∣P Ty
where 46 is due to Vx∣∣x∣∣p = Vx(xτx)p = p(xτx)2TXT = p∣∣x∣∣2-1xτ. And this equation
shows that z* lie on the line determined by X and y but does not lies on the part between X and y,
which can be formulated as z* = λx + (1 — λ)y, λ ∈ R \ (0,1). Note that
sup I ∣∣x — (λx + (1 — λ)y)∣∣p — ∣∣y — (λx + (1 — λ)y)∣∣p I
λ∈R∖(0,1) 1	1
=sup I ∣∣(1 — λ)(x — y)∣∣p — HA(y — x)∣∣p1
λ∈R∖(0,1) 1	1
=sup I ∣1 —	λ∣p — ∣λ∣p I	∙∣∣x — y∣∣p	= ( sup I ∣1 —	λ∣∣p	— |λ|p I)	∙∣∣x — y∣∣p
λ∈R∖(0,1) 1	1	'λ∈R∖(0,1)∣	17
Then, we just need to optimize
sup I ∣1 — λ∣p — ∣λ∣p I
λ∈R∖(0,1) 1	1
s.t. λx + (1 — λ)y ∈ K
Note that we can relax the constraint as below
λx + (1 — λ)y ∈ K
^⇒ λ(x — y) + y = (1 — λ)(y — x) + X ∈ K
=⇒∣∣λ(x — y) + y∣∣ = ∣∣(1 — λ)(y — x) + x∣∣ ≤ R	(47)
=⇒∣∣λ(x — y)∣∣ ≤ R + ∣∣y∣∣,	∣∣(1 — λ)(y — x)∣∣ ≤ R + ∣∣x∣∣	(48)
=⇒∣λ∣ ∙ ∣∣x — y∣∣ ≤ 2r, ∣1 — λ∣ ∙ ∣∣x — y∣∣ ≤ 2R	(49)
2R	2R
=	∣∣x -y∣∣ ≤	≤ ∣∣x -y∣∣
where 47 and49 is due to the definition of R as supx∈κ ∣∣x∣∣; 48 is due to triangular inequality.
13
Under review as a conference paper at ICLR 2022
Using the relaxed constraint, we can show that when λ ≥ 1, ||1 - λ∣p - ∣λ∣p∣ = λp — (λ - 1)p is an
increasing function w.r.t. λ as P ≥ 1, then
P
P
111 - λ∣p - ∣λ∣p 1 = λp - (λ - 1)p ≤
2R
∣∣x - y∣∣
2R
∣∣x - y∣∣
-1
(50)
—
And when λ ≤ 0, ∣ ∣1 - λ∣p - ∣λ∣p ∣ = (1 - λ)p - (-λ)p is a decreasing function w.r.t λ as P ≥ 1,
then
∣∣1-λ∣p-∣λ∣p ∣=(1 - -p≤ (3 丫
Note that
i⅛-1 丫
(51)
—
(2R	∖p
(∣∣ χ - y ∣ ∣√
2R	p	2R
F-^ -1) =p(k •( F-^)+ (1-k) V
=p(τr2^1 + (k - 1))p 1≤ p(
∖∣∣x-y∣∣	)	∖
2R
p-1
X - y
(2R	YT
∖∣∣ X - y ∣ ∣√
(52)
where 52 is due to the Differential Mean Value Theorem where k ∈ (0,1).
Thus, we have
2R	p-1
F) ∙∣∣χ-y ∣∣p = p(2R)p-1 ∣∣χ-y∣∣
i.e. ηcp (x) is a p(2R)p-1 -Lipschitz function.
∣ ncp (χ) - ηcp (y) ∣ ≤ P(K
(53)
□
Using Lipschitz continuous property of ηc, we get
WP)(μ, v) = sup η ηc(y)dν(y) + / ηcc(x)dμ(x) ≤ sup
η∈L1(μ) J	J	0∈Lip(p(2R)p-1)
sup
W∈Lip(p(2R)pT)
/ ψ(y)dν(y) + / ^c(x)dμ(X)
/ ψ(y)dν(y) + / φc(y)dμ(y)
(54)
where Lip(p(2R)p-1) denotes the set of p(2R)p-1-Lipschitz functions. On the other hand, recall
that
Wp(μ,ν) =	sup / ψ(y)dν(y) + / φ(x)dμ(x)
ψ,φ∈C(Rd) J	J
(55)
where ψ(y) + φ(χ) ≤ ∣ ∣ x - y∣ ∣ p. Keeping ψ(x) fixed and optimizing w.r.t. φ(y), then we just
need to optimize / φ(y)dμ(y') under constraint φ(y) ≤ ∣ ∣ X - y∣ ∣ p - ψ(x). Then obviously we
have φ*(y) = infx∈κ (∣∣ X - y∣ ∣ p - ψ(x)) = ψcp(y) where cp(x,y) = ∣ ∣ X - y∣ ∣ p. The map
(φ, ψ) ∈ C(K)2 → (ψcp,ψ) ∈ C(K)2 replaces dual potentials by “better” ones improving the dual
objective Wp(μ, V).
Using c-transform, we can reformulate constrained problem into unconstrained convex problem over
a single potential
Wp(μ,ν) = sup I ψ(y)dν(y) + I ψcp (x)dμ(x)
ψ∈C(Rd) J	J
sup / Ψ(y)dν(y) + / ψcp(y)dμ(y)
ψ∈C(Rd) J	J
(56)
Combining 54 and 56, we know that when the support set of measure μ and V supp(μ) = supp(v)=
K where K is a convex compact set, we have
sup / ψ(y)dν(y)+/ ψcp(y)dμ(y) = WP)(μ, ν) ≤ sup / ^(y)dν(y)+/ ^c(y)dμ(y)
ψ∈C(K) J	J	φ∈Lip(p(2R)p-1) J	J
(57)
Note that Lipschitz function must be continuous and therefore Lip(p(2R)p-1) ⊆ C(K). Then, we
have
sup / θ(y)dv(y) + / 3c(y)dμ(y) ≤ sup / ψ(y)dv(y) + / ψcp(y)dμ(y) (58)
φ∈Lip(p(2R)p-1) J	J	φ∈C(K) J	J
—
14
Under review as a conference paper at ICLR 2022
Combining 57 and 58, we know the inequality in 57 changes into equality
Wp(μ,ν) =
sup / ψ(y)dν(y) + / φc(y)dμ(y)
φ∈Lip(p(2R)P-1) 7	J
(59)
Note that for f (x) = xp — p(2R)p-1x, x ∈ R+ achieves its minimum when f 0(x) = PXpT —
p(2R)p-1 = 0, i.e. x = 2R and the minimum is f(2R) = -(P — 1)(2R)P-1. Then,
φcp(y) = Jnf (l|x—y||p—o(x)) ≥ J∩f (||x-y||p—^(y)—p(2R)p-1||x—y∣θ = —^(y)—(p—1)(2R)p-1
(60)
Thus, we attain a lower bound of Wp(μ, V)
Wpp (μ,ν)=	sup	/ ψ(y)dν(y) + / φc(y)dμ(y)
φ∈Lip(p(2R)p-1) J	J
(61)
/
/
≥ sup
W∈Lip(p(2R)p-1)
ψ(y)dν(y) — / (^(y) + (p — 1)(2R)PT )d4(y)
(62)
sup
W∈Lip(p(2R)p-1)
ψ(y)dν (y) — / ψ(y)dμ(y) — (p — 1)(2R)PT
J ψ(y)d(ν — μ)(y) — (p — 1)(2R)PT
sup
φ∈Lip(p(2R)P-1
where 61 is due to 59 and 62 is due to 60.
C Proof of Theorem 3.1
Proof.
Dχ,e,q = {x0 + μ ： ||x — x0 ||q ≤ ∈}	(63)
Note that
sup Wp(μ, ν) = sup	WP(X + μ, x0 + μ) = sup Wp(μ, z + μ)	(64)
ν∈Dx,e,q	||X-X0||q ≤∈	|IZIq4
where the first equality is due to the definition of Dx,eq and the second equality is due to the trans-
lation invariance property of Wasserstein distance.
Then recall the Monge version of Wasserstein distance
Wp(μ,ν) ≤
inf	[ ||x - T(x)Hpdμ(x)^
T :T ^μ=νj	)
(65)
Noticing the inf operator in the Monge version definition of Wp, we can get an upper bound for
Wp(μ, V) by specializing a transport map T satisfying Tμ = V. In our case, we take T : Rd →
Rd, T : x → X + z, and it,s easy to verify that T#μ = Z + μ. Then we get the upper bound below
Wp(μ, z + μ) ≤
inf / ||x
T :T ^μ=z+μj
—T (x)Hpdμ(x)
1
||x - T(x)"pdμ(x))
||z||
(66)
where the last equality is due to μ is a probability measure. This provides us with an intuition that
the upper bound of Wp(μ, Z + μ) is determined by the Euclidean norm of displacement z. Using
this upper bound,
sup Wp(μ, z + μ) ≤ sup ||z||2	(67)
||z||q ≤e	||z||q ≤e
When 0 < q ≤ 2, using the lemma that when 0 < p < q < ∞, ||z||q ≤ ||z||p, ∀z ∈ Rd holds, we
have sup||z||q≤e ||z||2 ≤ sup||z||q≤e ||z||q = e. On the other hand, note that ||€e1||2 = ||€e1||q = e,
we have sup||z||q≤e ||z||2 = e. And when q > 2, recall Holder,s Inequality below
Lemma C.1 (Holder,s Inequality for Rn). For {αi}ι≤i≤n, {bi}ι≤i≤n ⊆ R, r > 1, we have
n	n	1 n	r-1
X |电晒号(X同，Y (X |ai|占)k
i=1	i=1	i=1
(68)
15
Under review as a conference paper at ICLR 2022
Thus, sup∣∣z∣∣q≤J∣z∣∣2 ≤ sup∣∣z∣∣q≤J∣x∣∣qd2- 1 = Cd2-1. On the other hand, note that
|| G Pd=1 ei||q = e, || W Pd=1 ei||2 = ed 2 1, we have sup∣∣z∣∣q≤e ∣∣z∣∣2 = cd2- 1. Combin-
nq	nq	q
ing the case when 0 < q ≤ 2 and q > 2, we have
(69)
(70)
{c when 0 < q ≤ 2	1工
i_i	= max{c, cd2 -q }
Cd2 - q when q > 2
(71)
□
D W2 DISTANCE RELAXATION IS TIGHT FOR GAUS S IAN PROBABILITY
Measure
Here, we show that W2 distance relaxation for Gaussian probability measure is tight.
Theorem D.1. When μ = N(0, σ2I) andP = 2, the relaxation in 9 is tight. In other words,
DXeq ⊆ D	1 -1	but Dχeq∖D	ι -1	= 0 for any sufficiently small δ > 0.
,,	x,max{e,ed2 q },2	,,	x,max{e,ed2 q }-δ,2
(72)
Proof. Note that Dowson & Landau (1982) established the formula of Wasserstein distance between
two Gaussian measures.
Theorem D.2. For Gaussian probability measures μ = N(μι, ∑ι) and V = N(μ2, ∑2), W2-
distance between μ and V have CloSedformformUla
W2(μ, V)2 = ∣∣μι - μ2∣l2 +tr(ςi + ς2 - 2N1 夕2)2)	(73)
Using above theorem, we yield following tight relaxation between norm-based constraint set Dx,e,q
and W2 -distance based constraint sets Dx,δ,2 for Gaussian smoothing measures centered at origin,
i.e. μ = N(0, σ2I)
sup W2(μ, V) = sup	W2(x + μ, x0 + μ) = sup W2(μ, z + μ)
ν∈Dx,,q	||x-x0 ||q ≤e	||z||q ≤e
= sup ∣∣z∣∣2 =max{c,cd2-q }	(74)
l∣z∣∣q ≤e
where 74 is due to theorem D.2 and equality 71. And generalization of above theorem when μ =
N(0, Σ) can be proved in the same way.	口
E	Proof of Theorem 3.2
Proof. First, we introduce the lemma below.
Lemma E.1. Let X be a random variable that follows d-dimensional Gaussian distribution with
density function
f (x; μ, Σ)=——1-re-1 (XW)TςT(X-μ)	(75)
'	'(2π)d∣Σ∣ 2
where x, μ ∈ Rd and Σ ∈ S+d +. Let H : xTw + b = 0 be a hyperplane in the d-dimensional
Euclidean space Rd, where w ∈ Rd and b ∈ R. The hyperplane H defines two half-spaces:
Ω+ =	{x	∈	RdIxTW + b ≥	0},	Ω-	= {x ∈	RdIxTW + b <	0}	(76)
16
Under review as a conference paper at ICLR 2022
Define the integral over half-space Ω+ as
P
I f(x; μ, Σ)dx = /
Ω++	Jω+
1
(2π)d ∣Σ∣2
e-1 (x-μ)τ s-1(x-μ)dχ
Since Σ is positive definite symmetric, there exist an orthogonal matrix U and a diagonal matrix
D with positive diagonal elements such that Σ = UtDU. Let xo
and hence P
RX∞ 志 e-2 χ2 dx.
μ> w+b
∖∖√DUw∖∖2
(The proof of this lemma is credit to https://math.stackexchange.com/questions/
55 697 7/gaussian-integrals-over-a-half-space.)
Recall the definition of lp-norm constraint set of probability measures
Dχ,e,q = {x0 + μ : ∣∣x - x0∣∣q ≤ ∈}	(77)
Note that
sup TV (μ, V) = sup	TV (x + μ, x0 + μ) = sup TV (μ, Z + μ)	(78)
ν∈Dx,e,q	∖∖χ-χ0∖∖q ≤∈	∖∖z∖∖q ≤∈
where the first equality is due to the definition of Dχ,e,q and the second equality is due to the trans-
lation invariance property of total variance distance.
Define hyperplane H1 : xτZ — ∖⅛∖^ = 0 and H2 : xτZ + ∖⅛∖^- = 0. The hyperplane H1 defines
two half-spaces: Ω+ = {x ∈ RdIXTZ — ∖⅛∖^ ≥ 0} and Ω- = {x ∈ RdIXTZ — ∖⅛∖^- < 0}. And
the hyperplane H2 defines two half-spaces: Ωj = {x ∈ Rd ∣xτZ + ∖⅛∖^- ≥ 0} and Ω- = {x ∈
Rd∣xτZ + ∖⅛∖^- < 0}. Applying lemma E.1 and lemma A.3, we know that
sup TV (μ, Z + μ)
∖∖z∖∖q≤e
sup
∖∖z∖∖q≤e
1 [ I 1	XTx
-I I----d- e-0
2 J ∣(2π)2 σd
1	(x-z)T (x-z)
—
dx
(79)
1	1	1	∣ - XTx - (X-Z)T(X-Z) I	/	1	∣ - XTx	- (X-Z)T(X-Z) ∣
-sup	----e——e 2σ2 — e	2σ2	dx +	----G——e 2σ2 — e	2σ2	dx
2 ∖∖z∖∖q ≤Jω+ (2π) 2 σd ∣	∣	Jω- (2π)2 σd ∣	∣
1	/	1	(—(X-Z)T(X-Z)	- XTx ∖	/	1	(— XTx	- (X-Z)T(X-Z)、
-sup	----e——e	2σ2	— e 2σ2 dx +	----e——e 2σ2 — e 2σ2 dx
2	∖∖z∖∖q≤Jω+ (2π) 2σd'	Ω	Jω- (2π)2σd'	)
(80)
1	sup / d(Z + μ) — / dμ + / dμ — /
2	∖∖z∖∖q≤e √Ω+	7ω+	Jω-	Jω-
⅝Ze L+
dμ — dμ + dμ — dμ
Jω1	7ωi	Jω2
十	—	—
d(Z + μ)
(81)
1	∞
9 SUP ll ll
2 ∖∖z∖∖q≤ J-2σh
—. e 2 X dx — /	—, e 2 X dx +
√2∏	√2∏
2σ
/ *
J -∞
1
--e e
√2∏
1 χ2 7
2 X dx —
/-淡
J -∞
√⅛e-1 χ2 dx
(82)

1 SUP (G( ⅛) - G( -⅛) +g( ») - G( —⅛))
2 ∖ ∖ z ∖ ∖ q≤e V V 2σ √	1 2σ ) V 2σ √	1 2σ 〃
(83)
=1 sup 2(2G(呼)-1)	(84)
2	∖ ∖ z∖∖ q ≤e ∖ v 2σ 7 J
=2G( maX{：；d 1 -1}) - 1	(85)
where 79 is due to lemma A.3; 80 is due to the consistency of sign of integrand function on Ω+ and
Ω-; 81 is due to the transformation formula of space coordinates; 82 is due to lemma E.1; 83 and
85 is due to the definition and central symmetry property of G as the cumulative density function of
standard normal distribution; 85 is due to71.	口
17
Under review as a conference paper at ICLR 2022
F Proof of Theorem 3.3
Proof. Recall the definition of lp -norm constraint set of probability measures
Dχ,e,q = {x0 + μ : ||x - x0∣∣q ≤ e}	(86)
Note that
sup TV(μ,ν) = sup	TV(X + μ, x0 + μ) = sup TV(μ, z + μ)	(87)
ν∈Dx,,q	||x-x0 ||q ≤	||z||q ≤
where the first equality is due to the definition of Dx,,q and the second equality is due to the trans-
lation invariance property of total variance distance. Next, compute the value of TV (μ, Z + μ).
Lemma F.1. K is a lι norm ball centered at original point of radius r, then K ∩ (Z + K) = 0 if
and only if ||z ||1 > 2r.
Proof. First, we prove the if part and assume ||Z||1 > 2r. Consider arbitrarily taken x ∈ (Z + K),
i.e. ||x - Z||1 ≤ r. According to the triangular inequality with respect to l1 norm, we have
||x||1 = ||Z - (x -Z)||1 ≥ ||Z||1 - ||x - Z||1 > 2r -r =r	(88)
which shows that x ∈/ K and therefore K ∩ (Z + K) = 0.
Then we prove the only if part by using reduction to absurdity and assume ∣∣z∣∣ι ≤ 2r. Take y = ɪ z,
then ∣∣y∣∣ι = 2∣∣z∣∣ι ≤ r and ||y - z||i = ɪ∣∣z∣∣ι ≤ r which shows that y ∈ K ∩ (z + K) and
therefore K ∩ (z + K) = 0 which leads to a contradiction.	□
According to lemma F.1, we know that when z||z||q ≤ , ||z||1 ≥ 2r 6= 0, we have
sup TV (μ, ν) = sup TV(μ, z + μ) = 1	(89)
ν∈Dx,,q	||z||q ≤
Define W= 2dr Pd=ι ei, and it’s easy to verify that ||洲 ι = 2r and ||洲q = 2rd 1 T for q > 1. Thus,
when E > 2rd 1 -1, We have
sup TV (μ, ν) = sup TV(μ, z + μ) = 1	(90)
ν∈Dx,,q	||z||q ≤
□
G Proof of Theorem 3.4
Proof. First, we introduce the lemmas below for the convenience of later proof.
Lemma G.1 (Volume Formula of d-dimensional spherical cap). The volume of a d-dimensional
hyperspherical cap of height h and radius r is given by:
V
∏ d-1 rd
,arCCOS( r-h )
0
(91)
where we define h as the value shown in figure 4 and Γ (the gamma funCtion) is given by Γ(z)
R0∞ tz-1e-tdt.
Lemma G.2 (Volume formula of d-dimensional Euclidean ball). The volume of d-dimensional Eu-
Clidean ball of radius r is given by
π d rd
γ( d + i)
(92)
Recall the definition of lp-norm constraint set of probability measures
Dχ,e,q = {x0 + μ : ||x - x0∣∣q ≤ e}	(93)
Note that
sup TV(μ, ν) = sup	TV(X + μ, x0 + μ) = sup TV(μ, z + μ)	(94)
ν∈Dx,,q	||x-x0 ||q ≤	||z||q ≤
where the first equality is due to the definition of Dx,,q and the second equality is due to the trans-
lation invariance property of total variance distance.
18
Under review as a conference paper at ICLR 2022
Figure 4: An example of a spherical cap in blue
Lemma G.3. K is a l2 norm ball centered at original point of radius r, then K ∩ (Z + K) = 0 if
and only if ||z ||2 > 2r.
According to this lemma, we know that when q ≤ 2 and > 2r, we have
1 ≥ sup TV(μ, z + μ) ≥ TV(μ, eeι + μ)=
l∣Z∣lq ≤
Vol(K∆(e1 + K))
2Vol(K)
(95)
1
where the last equality is due to ||ee1||2 = e > 2r and applying lemma G.3. And when q
1一1
e > 2rd q 2, we have
>
2 and
d
1 ≥ sup TV (μ, z + μ) ≥ TV(μ,二 ^X ei + μ)=
UzUq≤e	dq M
VoI(K∆(4 Pi=I ei + K))
______dq__________
2Vol(K)
(96)
1
where the last equality is due to || -⅛ Pi=ι ei∣∣2
d q	一
ing the results for q ≤ 2 and q > 2, we have
1_1
ed2 q > 2r and applying lemma G.3. Combin-
sup TV (μ, z + μ) = 1 when e > min{2r, 2rdq -1}
l∣Z∣lq ≤
(97)
Next, consider the case when e ≤ min{2r, 2rd1 -1}. Applying G.1, lemma G.2 and lemma A.3,
we have
sup TV (μ, Z + μ)
l∣z∣lq≤e
♦ 2∕l
1
VOi(Ky Ix∈K - Vol(K) 1x∈z+Kldx
(98)
||" 2V0l(K)
Vol(K∆(z+K))
J ʤ+K)dx =鹏e -2vol(K)一
Vol(K) -
sup
l∣z∣∣q≤e
d-1
2∏ 2 r
Γ(d+1)
;Rarccos(*) sind(t)dt
Vol(K)
2∏ 2 r
sup 1 -且亶
l∣Z∣lq ≤
d-1
F Rarccos(|>) sind(t)dt
Vol(K)
(99)
d-1
2∏ 2 r
Γ(d+1)
sup 1---------
l∣z∣∣q≤e
F Rarccos(||›) sind(t)dt
d
∏2 rd
Γ( 2 + 1)r
||z||2
2Γ( d + 1) ∕*arccos( F) J
SUP 1 —	1⅛d+π	sin (t)dt
ι∣z∣∣q≤e	∏2γ(d+1) 0
(100)
1 -1
max{,d 2 q }
2Γ( d + 1)	arccos( ——fτ-------)
1 -	(2 +1)	sind(t)dt
∏1 Γ( d+1) J0	' J
(101)
where 114 is due to lemma A.3; 99 is due to lemma G.1; 100 is due to lemma G.2; 101 is due to 71.
Because of the computation difficulty (overflow), We have to simplify the term ：(,+1).
γ( 2 )
19
Under review as a conference paper at ICLR 2022
When d is even, assume d = 2k, k ∈ N and note that Γ(1) = 1, Γ( ɪ) = π 1, then
Γ(d + 1) _ Γ(k + 1)
k!Γ(1)
k!
(2k)!!
"γ(⅛1T = W¥ = ∏k=ι(i - 1 )Γ( 1)=
Recall the Wallis integral lemma that When d is even
I sind(t)dt = ( cosd(t)dt = π
002
π 1 ∏k=ι(i — 2)	π 2 (2k — 1)!!
(d - 1)!!, d =2k ∈ N
d!!
(102)
(103)
Thus,
r( d + 1)
(2k)!!
Γ(d+1) = π1 (2k - 1)!! = 2π- 1 ∙(
1
π (2k-1)!! ∖
2 ∙	(2k)!! )
2π-1 J02 sin2k(t)dt	2π- 1 八2 sind(t)dt
(104)
When d is odd, assume d = 2k +1, k ∈ N and note that Γ(1) = 1, Γ( 1) = π2, then
Γ(d + 1) _ Γ(k + 32) _ ∏k=0(i + 1 )Γ( 1) _ π1 ∏k=0(i + 2) _ ∏2(2k + 1)!!
Γ( d++1) = Γ(k +1)
k!Γ(1)
k!
2 ∙(2k)!!
(105)
1
1
Recall the Wallis integral lemma that When d is odd
π	π
/ sind(t)dt = / cosd(t)dt =
(d -11)!!, d =2k + 1 ∈ N
d!!
(106)
Thus,
Γ(d + 1) _ ∏1 (2k + 1)!!
Γ(d+1) =	2 ∙ (2k)!!	= 2∏-2 ∙ ((2k)!!)
( 2 )	k 2	2π 2	( (2k+1)!!)
To sum up, for all d ∈ N, We have
γ( d + 1)
2π- 1 Ro2 sin2k+1(t)dt	2π-2 Ro2 sind(t)dt
(107)
γ(d-Q1)	2π- 1 Ro2 sind(t)dt
(108)
1
1
1
1
Then We avoid the computation of Γ(d + 1), Γ(d++1) and transfer it into the computation of an
integral. Applying formula 108, we have
sup TV(μ, z + μ) = 1 —
l∣Z∣lq≤
1
Ro2 sind(t)dt
1	1
———
farccos(…3；^ɪ)
sind(t)dt
0
(109)
□
H Proof of Theorem 3.5
Proof. Recall the definition of lp -norm constraint set of probability measures
Dχ,e,q = {x0 + μ : ||x - x0∣∣q ≤ e}	(110)
Note that
SuP TV(μ, ν) = SuP	TV(X + μ, x + μ) = SuP TV(μ, z + μ)	(111)
ν∈Dx,e,q	||x-x0||q ≤	||z||q ≤
Where the first equality is due to the definition of Dx,,q and the second equality is due to the trans-
lation invariance property of total variance distance.
When ≥ 2r,
1 ≥ SuP TV(μ, z + μ) ≥ TV(μ, eeɪ + μ) = 1	(112)
l∣Z∣lq ≤
20
Under review as a conference paper at ICLR 2022
where the first inequality is due to the fact that μ and Z + μ are probability measures; the second
inequality is due to supp(μ) ∩ SUPP(EeI + μ) = 0. Thus, in this case,
sup TV(μ, z + μ) = 1
l∣z∣∣q ≤e
When e < 2r,
sup TV (μ, z + μ)
l∣z∣∣q≤e
sup W I、丁	∖Zχ∈κ —	Zχ∈z+K∣dx
∣∣z∣∣q≤e 2 八 Vol(K)	2Vol(K)	I
(113)
(114)
Ifb	,	Vol(K∆(z + K))	1	Vol(K ∩ (z + K))
sup “ “小 I Iχ∈κ∆(z+κ)dx = sup ——“ ”小------------------= sup 1----------------------
∣∣z∣∣q≤e 2 Vol (K)J	∣∣z∣∣q ≤e	2Vol(K)	∣∣z∣∣q ≤e	VOI(K)
sup 1 —
∣∣z∣∣q≤e
∏ 幺ι(2r —|zi|)
(2r)d
(115)
First, we study typical cases when q = 1,2, ∞. When q = 1, we need to solve the following
optimization problem
inf Πd=1(2r — ∣zi∣)	(116)
∣∣z∣∣i≤e
Here we use mathematical induction to prove that
ll M ∏=ι(2r TZiI) = (2r)d-1(2r — e)	(117)
∣ ∣ z∣∣i≤e
When d = 2,
IliIf πd=1(2r — |zi l) = IIinf- (2r — |z1|)(2r Tz2I) = Iinf (2r — e + |z2|)(2r — |z2I)
∣ ∣ z∣∣ι≤e	∣ Zi ∣ +1 Z2 ∣≤e	∣ Z ∣≤e
= inf (2r — E + z2)(2r	— z2) = inf	z2(E	—	z2)	+	2r(2r	— e)	= 2r(2r	— E)
0≤Z2≤e	0≤Z2 ≤e
Thus, induction hypothesis holds for d = 2. Then, assume induction hypothesis holds for d = n.
When d = n + 1,
ifΠ "I1(2r — |zi|) = inf	Π"+11(2r — |zi|) = inf	(∏-=1(2r —|z#)(2r —际+”)
∣ ∣ z∣∣i≤e	Pn+i1∣ Zi ∣ ≤e	PNi ∣ Zi ∣ ≤e- ∣ zn+i∣
=inf (2r)n-1 (2r — e + |z„+i|)(2r —际+“)	(118)
∣ Zn+i ∣ ≤e
=(2r)n(2r — e) = (2r)d-1(2r — E)	(119)
where 118 is due to the induction hypothesis when d = n; 119 is due to the induction hypothesis
when d = 2. Therefore, we have already proved that
inf ∏=ι(2r — |&|) = (2r)d-1(2r — e),∀d ∈ N	(120)
∣ ∣ z∣∣i≤e 一
Plugging in this result, it follows that
sup TV (μ, z + μ) = sup 1 —
∣ ∣ z∣∣i≤e
∣ ∣ z∣∣i≤e
∏d=1(2r —|z/) _	(2r)d-1(2r — e)
------；--：~~7---- = 1 - --------T ：~~7-----
(2r)d	(2r)d
E
2r
(121)
When q = 2, we need to solve the following optimization problem
∣∣ zinL∏3(2rTZiI)
(122)
When d = 2,
l inf	∏d=ι(2r —	|zi|)	= l2	inf, 2(2r	—	|zi|)(2r	—	|z2|) = Iinf	(2r	—	(e2	—	z2)i )(2r	—	|z21)
∣ ∣ Z 11 2≤e	∣ Zi∣ 2 +1 Z2 ∣2≤e2	∣ Z2 ∣≤e、	)
=0⅛e (2一 (“ z2)2)(2r — z2)
21
Under review as a conference paper at ICLR 2022
Define f (z2) = ln(2r — (e2 — z2) 2)+ ln (2r — z2), then
f0(z )	= z¾(e2 — z2)-2	—	1	= 2rz¾(e2	—琰-2 — z2(e2 — Zr1 — 2r + (饪一以2
2r — (e2 — z2)2	2r —	z2	(2r — (e2 — zg)2 )(2r — z2)
ɪ	ɪ	ɪ	(123)
Define g(z2) = 2rz2(a —z2)-2 — z2(e2 — z2)-2 — 2r + (e2 — z2)2, then
g0(z2) = (2z3 — 3e2z2 + 2re2)(e2 — z2) 2	(124)
Define h(z2) = 2z3 — 3e2z2 + 2re2, then h0(z2) = 6z2 — 3e2 = 6(z2 —1)(z2 + 1).Thus,
when 0 ≤ z2 ≤ √2, h0(x) ≤ 0; when √ < z2 ≤ e, h0(x) > 0. Thus, the minimum value of h(x)
on interval [0, e] is h(√) = √2e2(√2r — e). Therefore, function f (z2) behaves differently when
0 < e ≤ √2r and when √2r < e < 2r.
When 0 < e ≤ √2r, h(z2) ≥ h(√⅛) = √2e2(√2r — e) ≥ 0 on interval [0,e] and therefore
g(z2) = h(z2)(e2 — z2)-2 ≥ 0. Note that g(0) = e — 2r < 0,g(1)=0,g(e-) = ∞ and
therefore f (z2) ≤ 0 when 0 ≤ z2 ≤ √ while f(z2) > 0 when √ < z2 ≤ e. Thus, f (z2) takes
its minimum when z2 = -‰. In this case,
2
OjnfCe (2r — (e2 — z2) 2 )(2r — z2)= (2r — -√2)2	(125)
When √2r	< e <	2r,	we have h(0) =	2re2	>	0, h(√⅛∣)	=	√2e2(√2r	—	e)	<	0, h(e)=
e2(2r — e)	> 0.	Assume h(t1) = h(t2) = 0,0 <	tι < √	< t2	< e, then when 0 ≤ z2	≤ tι
or t2 ≤ z2	≤ e,	h(z2) ≥ 0 and when tɪ < z2 <	t2, h(z2)	< 0.	Therefore, g(z2) ≥ 0	when
0 ≤ z2 ≤ tι or t2 ≤ z2 ≤ e; g0(z2) < 0 when tɪ < z2 < t2. Note that g(z2) = 0 Q⇒ (2z2 —
e2) (2(z2 —r)2+2r2 —e2) = 0, therefore when 0 ≤ z2 ≤ r — Je2 — r2 or √⅛∣ ≤ z2 ≤ r+ Je2 — r2,
g(z2) ≤ 0; when r — Je2 — r2 <z2 < √ or r + Je2 — r2 <z2 < e, g(z2) > 0. Thus, when
0 ≤ z2 ≤ r — Je2 — r2 or √ ≤ z? ≤ r + Je2 — r2, f0(x) ≤ 0; when r — Je2 — r2 < z2 < √
or r + ʌ/erɪr2 < z2 < e, f0(x) > 0. Thus, f (z2) takes its minimum when z2 = r — Je2 — r2
or z2 = r + JI二r2. In this case,
inf (2r — I — z2)2) (2r — z2)
0≤z2≤e ∖	/
e2
-—r2
2
e2
-----r2 ) = 2r2
2	)
e2
ɪ (126)
—
inf nd=1(2r—|zi|)=	inf	Π "J(2r—|ZiI)=	呼	(∏ 乜(2r—|% |))(2r—|z„+11)
∣∣z∣∣2≤e	P 鲁1 z2≤e2	pn=ι 瑶 ≤e2-z"ι '	’
(127)
By then, we have understand clearly the optimization problem when d = 2.
Then, consider the case when d = 3. When d = 3,
inf ∏d=1(2r —|zi|)=	inf	(2r — |zi|)(2r — |z2|)(2r —㈤)	(128)
∣∣z∣∣2≤e	z2+z2 + z2≤e2
When 0 < e ≤ √2r, assume the optimal point is z*. We will prove that each coordinate of z* has
the same value. Here we use reduction to absurdity, and wlog assume z* = z*. By fixing the value
of zɜ, the optimization problem 122 is equivalent to
2 JnL-(2r — |zi|)(2r—|z2|)	(129)
z2 + z2≤e2-(z*)2
And (z*, z*) should be an optimal point of above problem. Note that e2 — (zɜ)2 ≤ e2 ≤ 2r2 and
applying 125, we know that Z * = z* which is a contradiction. Thus, Z * = z* = z∙* = c. And
∣∣χf≤F 1(2r — |ZiI) =C吗(2r — c)3 = (2r - √3 )3	(130)
22
Under review as a conference paper at ICLR 2022
When √2r < € ≤ √3r, it,s obvious that the optimal point z* of optimization problem 128 must lie
on the boundary of feasible region, i.e. (z*)3 + (z*)3 + (zɜ)3 = €2. Wlog, assume (zɜ)3 ≥ 号 and
(z*)2 + (z*)2 ≤ 等 ≤ 2r3. By fixing the value of zɜ and following similar deduction procedure as
above we know that z* = z* = c*, where c* is the optimal point of following optimization problem.
inf (2r — c)2(2r —，€2 - 2c2)
0≤c≤ √3
(131)
Define f (x) = 2ln (2r — x) + ln (2r — √e2 — 2x2) where 0 ≤ x ≤ √∣, then
f0(x)
2(3x2 — 2rx — €2 + 2r√e2 — 2x2)
(x — 2r)(2r — √e2 — 2x2)√e2 — 2x2
(132)
It,s obvious that the denominator of f(x) is negative. As for the numerator, define g(x) = 3x2 —
2rx — €2 where 0 ≤ X ≤ -⅛. Note that
3
g(x) ≤ max {g(0),g(√3)} = max { — e2, — √∣} ≤ 0	(133)
Thus, we have the following equivalent relationship
3x2 — 2rx — €2 + 2r，€2 — 2x2 ≤ 0
Q⇒ 3x2 — 2rx — €2 ≤ —2rp€2 — 2x2 ≤ 0
Q⇒ (3x2 — 2rx — €2)2 ≥ ( — 2r√e2 — 2x2)2 ≥ 0
^⇒ (3x2 — e2)(3x2 — 4rx + 4r2 — €2) ≥ 0
Q⇒ 3x2 — 4rx + 4r2 — €2 ≤ 0
I0
^⇒ ∖ i___________ ?________________
I 2r — √3e2 - 8r2 VN < 2r + √3e2 - 8r2
when √2r < € ≤ 2y ∣r
when 2y ɪr < € ≤ √3r
3
3
where the last equivalent relationship is due to the discriminant of the quadratic equation 3x2 —
4rx + 4r2 — €2 is ∆ = 4(3e2 — 8r2). Therefore, when √2r < € ≤ 2^lIr, f0(x) ≤ 0,∀0 ≤ X ≤ -√^
and hence the optimal point c* in the optimization problem 131 takes value √, whereas when
2γ∕3r < € ≤ √3r, f0(x) ≤ 0 for 0 ≤ X ≤
2r- √3e2-8r2 2r+√3e2-8r2
for 2r-7*3-8r2 ≤ X ≤ 2r+v^2-8r2 and note that f (
,	3
2r-√3e2-8r2
3
≤ X ≤ √3 and f 0(x) > 0
)< f (√3) hence the optimal
3
point c* in the optimization problem takes value 2r-V^2-8r2. To sum up, when √2r < € ≤ 2 J3r,
the optimal point z* of optimization problem 128 satisfies z * = z* = z* = √. And when 22r <
€ ≤ √3r, the optimal point z* of optimization problem 128 satisfies z * = z* = 2r-√332 -8r2, z3 =
4r+√3f2-8r2 or one of its permutations.
When √3r < € < 2r, similarly we have (z *)3 + (z*)3 + (z*)3 = €2. If there exists 1 ≤ i ≤ 3 such
that (z*)2 ≥ €2 — 2r2, wlog assume (z3)2 ≥ €2 — 2r2. By substituting the value range of X from
[0, √3 ] into [0, r], following similar deduction procedure and noticing that f ( 2r-vT-8r2) < f (r),
we know that the optimal point z* in this case satisfies Z * = z* = 2r-√332-8r2, z3 = 4r+√332-8r2
or one of its permutations. On the other hand, if (z*)2 < €2 — 2r2 for all 1 ≤ i ≤ 3, then
(z *)2 + (z*)2 = €2 — (z3)2 > 2r2. Applying 126 and taking Z * = r — Jm-：3)2 — r2, z* =
r + J "jj — r2, we know the optimization problem is equivalent to
inf	(2r2 — ^2-z2) (2r - z3)	(134)
0≤Z3<√e2-2r2 '	2	/
23
Under review as a conference paper at ICLR 2022
According to monotonicity analysis of the cubic function above, the optimal point z3 is either
2r-√⅛2-8r2 or √e2 - 2r2. And it,s easy to verify that f (2r-√3f-8r2) < f (√e2 - 2r2) and
therefore z3 = 2r-y2-8r2. However, (Zg )2 > e2 - 2r2 which leads to a contradiction.
In summary, considering the case d = 3, when 0 < e ≤ 2 工r， the optimal point zg of original
optimization problem satisfies zg = ZB = z3 = √^ and the optimal value is (2r - √)3 and when
√3r < e < 2r, the optimal point zg, the optimal point Zg of original optimization problem satisfies
Zg = Zg = 2r-√3f-8r2, z3 = 4r+√3f-8r2 or one of its permutations.
Next, consider the general case when d = n ≥ 4. In the first place, we point out and prove two
useful properties of the optimal point Zg which help simplify our later discussion a lot.
•	All coordinates of optimal point Zg takes at most two different values.
•	If the coordinates of an optimal point Zg takes exactly two different values c1 and c2, then
the number of coordinates equal to c1 must be n - 1 or 1.
Proof. On one hand, by using reduction to absurdity, wlog assume Z1g , Z2g , Z3g take three different
values. Fixing the value of the other n - 3 coordinates, we know that (Z1g , Z2g , Z3g ) is the optimal
point of a special case of original problem when d = 3. And note that for all the optimal points of
d = 3, there must exist two coordinates taking the same value, which leads to a contradiction. Thus,
the first property is satisfied.
On the other hand, similarly, by applying reduction to absurdity, wlog assume Z1g = Z2g = c1 and
Z3g = Z4g = c2 where c1 6= c2. Fixing Z2g, Z4g and the value of the other n-4 coordinates and aware of
the fact that (z；)2 + (z3)2 ≤ 号 < 2r2, We know that (z；, z3 ) is the optimal point of a special case
of original problem when d = 2, e < √2r and therefore Zg = z3, which leads to a contradiction.
Thus, the second property is satisfied.	□
Using the two properties above, we know that the optimal point Zg has only two possible forms:
Zg = (√n, ∙∙∙ ,	√n)	and	Zg	=	(c, ∙∙∙ , c,	ʌ/e2	-	(n	- 1)c2)	or one of its permutations where
0 ≤ C ≤ √⅛,c = √n, which can be unified into one form: Zg = (c,…，c, ʌ/e2 - (n - 1)c2)
or one of its permutations where 0 ≤C ≤ √⅛. Thus, the original problem can be simplified into
following optimization problem with one degree of freedom:
inf	(2r — c)n-1(2r — ʌ/e2 - (n - 1)c2)
0≤c≤ √H
(135)
Define f (x) = (n — 1)ln (2r — x)+ln (2r — ʌ/e2 - (n - 1)x2) where 0 ≤ X ≤ √-, then
f0(x)
(n — 1) (nx2 — 2rx — e2 + 2r,c2 — (n — 1)x2)
(x — 2r)(2r — pe2 — (n — 1)x2) pe2 — (n — 1)x2
where 0 ≤ x < ,
n	√n-Γ
(136)
It’s obvious that the denominator of f0(x) is negative. As for the numerator, define g(x)
nx2 —
2rx — 2 where 0 ≤ x ≤
√n=~ι. Note that
g(x) ≤ min g(0), g
j E(E — 2ʌ/n — 1r)]
max 0 0, -----------匕 ≤ ≤ 0
(137)
24
Under review as a conference paper at ICLR 2022
where the last inequality is due to e < 2r < 2√n - 1r. Thus, when 0 ≤ x ≤ √-,
nx2 — 2rx — e2 + 2r，3一(n - 1)x2 ≤ 0
^⇒ nx2 — 2rx — e2 ≤ —2r//—(n _ 1)x2 ≤ 0
^⇒ (nx2 — 2rx — e2)2 ≥ ( — 2rpe2 — (n — 1)x2)2
^⇒ (nx2 — e2)(nx2 — 4rx + 4r2 — e2) ≥ 0
^⇒
^⇒
2r + -∖∕ne2 — 4(n — 1)r2
≤ X ≤ -----------H-------------------—
when 0 < e < 2
n
when 2a/n——-r ≤ e < 2r
n
where the last equivalence relationship is due to the discriminant of the quadratic equation nx2 —
4rx + 4r2 — e2 = 0 is
< 0 ^⇒ 0 < e < 2
(138)
Thus, if 0 < e <	2,n-1 r,	then f 0(x) ≥ 0 when	√^ ≤ X ≤ √-	and f 0(x)	< 0 when
0 ≤ x< 恭.Thus,f (X)takes	its minimum when X =恭 and therefore c*	= √n.
If 2,n-1 r ≤ e < 2r, then f0(x) ≥ 0 when 2r-ʌ/+-"”-1)* ≤ X ≤ 2r+'”声-4，”-1A2 or
√ ≤ x ≤』and f0(x) < 0 when 0 ≤ x < 2r-√ …(nf2 or 2r+J4(nT)r2 <
√n	√n-1	n	n
x < √n.In this case, f(X) takes its minimum when X = 2r-Vn'2-4(nT)T or X = √n. For the
convenience of analysis, assume t = *, Jn-1 ≤ t < 1 and it follows that
(139)
= (2r)n
(n — 1) + ʌ/nt2 — (n — 1)
n
1 — ʌ/nt2 — (n — 1)
n
We can prove that there exists tn ∈
(140)
such that C* =我 when 2 ^-r ≤ e ≤ 2tnr
and c* = 2r Vzne -4(n 1)r when 2tnr < e < 2r while tn converge to 1 at an exponential rate as
shown in figure 5.
In conclusion, for the case d = n ≥ 4, when 0 < e ≤ 2tnr, c* =号 and therefore
lli∏f ∏n=ι(2r — |zi|) = (2r — c*)n-1(2r — √e2 — (n - 1)(c*)2)
llzl∣2≤e
Plugging in this result, it follows that
(141)
sup TV (μ, z+μ) = sup 1 —
∏=ι(2r — |zi|)
(2r)d
(142)
25
Under review as a conference paper at ICLR 2022
Figure 5:	Graphs of functions f1 (t)	=	(1 — √tn )n,f2 (t)	=
((nT)+/；/-(n-1))	(1-√ntn-(nT)) when n = 4,16, 64 from left to right. Accord-
ing to the figure, on interval [ Jnn∙1, 1], f2(t) is greater than f1 (t) at first and then f2(t) exceeds
f1 (t). Furthermore, as n increases, the horizontal coordinate of the intersection point converge to 1,
which can be seen intuitively from the figure above.
and when 2tnr < e < 2r, c* =
n
and therefore
inf
llzl∣2 ≤C
∏n=1 (2r —|Zi |) = (2r — c* )n-1(2r — √e2 — (n — 1)(c* )2)
2(n — 1)r + ʌ/ne2 — 4(n — 1)r2 ∖n-1 ( 2r — ʌ/ne2 — 4(n — 1)r2
nn
2(d — 1)r + /de2 — 4(d — 1)r2 ∖n-1 ( 2r — ʌ/de2 — 4(d — 1)r2、
n	∖	d	，
Plugging in this result, it follows that
sup TV(μ, Z + 4) = sup 1 —
||z||2 ≤e	||z||2 ≤e
∏=1 (2r—|&|)
(2r)d
2(d-1)r+√de2-4(d-1)r2 d-1 2 2r-√de2-4(d-1)r2
d	)	(	d
(2r)d
d — 1 +，d(2r)2 — d +1 λd-1 (1 - ,d(2r)2 - d +1
d	) V	d
When q = ∞, it,s easy to verify that
Uinfj ∏d=1 (2r — |ZiD = (2r — e)d
HzH∞ ≤6
Plugging in the formula above, it follows
(143)
sup TV(μ, z + μ) = sup 1 —
||z||g ≤e
||z||g ≤e
∏d=1 (2r—|&D = 1 — (2r - "d = 1 — (I-工)d (144)
(2r)d = 1	(2r)d	Y 2r	()
=1 —
=1 —
□
I Proof of Theorem 3.6
Proof. Recall the definition of lp-norm constraint set of probability measures
Dχ,e,q = {X0 + ^ ： ||X - X0||q ≤ ∈}	(145)
Assume DXqq ⊆ Dχ,ξ(e), then
ξ(e) ≥	sup	TV(μ,	ν)	= sup	TV(x+μ,x0+μ)=	sup	TV(μ, z+μ)	≥ TV(μ,	ee1 +μ)
V ∈Dx,e,q	||X-X0||q ≤6	||z||q ≤C
(146)
26
Under review as a conference paper at ICLR 2022
which indicates that TV(μ,ee∖ + μ) provides a lower bound for ξ(e). Thus, We only need to estimate
the value of TV(μ, eeι + μ). According to lemma A.3, we have
Vol(K∆(eeι + K))	Vol(K ∩ (0 + K))
TV (μ, ee1+ 〃)= —2Vol(K- = 1----------------Vol(K)一
Note that
K∩(e1+K)
={x ∈ Rd∣∣xι∣p + …+ ∣xd∣p ≤ rp, ∣xι - e∣p + ∣X2∣p + …+ ∣Xd∣p ≤ rp}
={χ ∈	Rd	e - (rp	-	(∣χ2∣p +	…+ ∣χd∣p∣)) 1 ≤ xi ≤ (rp - (∣χ2∣p + …+ *∣p∣)) 1 }
={x ∈	Rd	e - (rp	-	(∣x2∣p +-+ ∣Xd∣p))1 ≤ xi ≤ ∣} ∪ {x∣擀 ≤ xi ≤ (rp - (∣x2∣p	+-+ ∣Xd∣p∣))1}
:=Ωι ∪ Ω2 where Ωι ∩ Ω2 = 0
It's easy to verify that Vol(Ω1) = Vol(Ω2) according to integration by substitution and therefore
Vol(K ∩ (eei + K)) = 2Vol(Ω2). To estimate the volume of Ω2, we first introduce several lemmas
below for the convenience of later discussion.
Lemma I.1 (olume formula of d-dimensional lp norm ball). The volume of d-dimensional lp ball
of radius r is given by
-	.Γ(1 + 1)d
V(d) = (2r)d τ(1+dγ	(147)
1
Lemma I.2. The d-dimensional lp ball ofvolume 1 has radius about-1dp-.
2(pe)P Γ(1+ P)
Proof. When dimension d is big enough, we can obtain an asymptotic volume estimation of lp norm
ball with radius r .
V(d) = (2r)d r(1 + p) ≈
P ( ) Γ(1 + d)
2∏ d
d = r-p~
、p V 2πd
2r(pe) 1 Γ(1 + 1 )∖d
(148)
j Γ(1 + 1)
(2r)d-~p~
P
d p
where the first equality is due to lemma I.1 and the approximate equality is due to Stirling’s formula
about the estimation of gamma function that Γ(z + 1) ≈ √2∏z (e) . Thus, when V(d) = 1, we
have
1
d P
r ≈-------1--------- (149)
2(pe)p Γ(1 + P)
□
Then we estimate the volume of lp norm ball cap by studying the asymptotic property of the mass
distribution of lp norm ball. To begin with, let’s estimate the (d - 1)-dimensional volume ofa slice
through the center of the lp ball of volume 1. Note that the ball has radius r = (V(d))- d. The slice
is an (d - 1)-dimensional ball of this radius, so its volume is
V (d-i)rd-i
pr
2d-i r(1 + P )d-i (2d r(1 + P )d Y 宁
Γ(1 + d-p)1	Γ(1 + d))
(150)
Using Stirling’s formula again, when d is sufficiently large, we have
V (d-P)
rd-P
2d-i r(1 + P )d-i (2d r(1 + P )d Y 号
Γ(1 + 号)1	Γ(1 + d))
2d-P
Γ(1 + P )d-i
d-1
d
1
/	1	、d-1 1 1
(1 + d⅛) K+2
(等)2d
≈ e p
27
Under review as a conference paper at ICLR 2022
Figure 6: Comparing the volume of a ball with that of its central slice
where the second equality is due to Stirling,s formula for Γ(1 + d--1) and Γ(1 + d); the third equality
just eliminate the exponential of 2 and Γ(1 + P). Thus, We conclude that the slice has volume about
1
eP when d is large.
Then, consider the (d - 1)-dimensional volumes of parallel slices. The slice at distance x from the
center is an (d - 1)-dimensional ball whose radius is (rp - Xp) 1, so the volume of the smaller slice
is about
1	d— 1
eP ( (Tr -TXp)P )	= e 1(1-(X)p) k
1
Since r is roughly-1dp-----, this is about
2(pe)P Γ(1+ P)
eP
2X(Pe) 1 γ(I + r jr) d-
1 /	pe(2xΓ(1 + 1 ))r ∖ dP1	/1	1 、
eP ( 1------r- )	≈ exp (——e(2xΓ(1+-))r )
d	pp
Thus, if we project the mass distribution of the lr ball of volume 1 onto a single coordinate
direction, we get a distribution with density function f(x) = exp( 1 - e(2xΓ(1 + p))r) =
eχp( r -e( r r( p ))pxp).
Thus, for an Ip ball centered at original point O with volume 1 and approximate radius-1dP-----,
2(re)P Γ(1+ 1)
then we can use the integral 2 Rs exp (p - e(2xΓ(1 + P))r) dx to estimate the volume between two
parallel slices at the same distance s from the center. Then the volume oflr ball cap corresponding to
the slice at distance s from the center can be approximated by 1 - Rs exp (p - e(2xΓ(1 + P))r) dx.
Note that the ratio k of slice,s distance d from center to radius r is about S /----1dP----- =
2 2(pe)P Γ(1+ P)
22SPeeP；(1+ P), i.e. s =----1d^-----. Thus, the volume of cap can be represented as
dP	2(pe)P Γ(1+ P )
1
kd P
1	2(Pe)P Γ(1+ 1)
2	Js
exp (- - e(2xΓ(1 + J))p)dx
pp
which is only related to the ratio k . Then, we can conclude that for a lr ball with radius r, when
dimension d is large enough and its cap corresponding to the slice at distance h form the center, then
the volume ratio of cap to ball is approximately
1
SdP___
1 - ∕2r(Pe)Pr(1+1) exp (1 - e(2xΓ(1 + 1))p[dx
20	p	p
Thus,
1
edP____
Vl(KM- ZL P r(1+1) exp (P - e(2xr(1 + p ))p)dx
(151)
(152)
28
Under review as a conference paper at ICLR 2022
and therefore
TV(μ, eeι + μ) = 1 —
Vol(K ∩ (e1 +K))
Vol(K)
1
：dp
2	2Vol(Ω2)
1 ------：--：-
Vol(K)
2 / 4r(Pe) P r(ι+P)
exp (- - e(2xΓ(1 + ɪ))pɔ dx
p
p
□
J Proof of theorem 3.7
Proof. Note that f (x) and g(χ) are respectively density functions of reference measure P = x + μ
and perturbed measure ν and q(x) is defined as g(x) - f (x). Therefore
Ex”[Φ(X )] = /
=Z
φ(x)g(x)dx = J φ(x)(g(x) — f(x))dx + J φ(x)f (x)dx = J φ(x)q(x)dx + J φ(x)f (x)dx
φ(x)q(x)dx + EX 〜x+μ[φ(X )]
Where the first term contains all the uncertainty in one functional variable q(x) and the second term
is a constant when sample point χ, smoothing measure μ and specification φ are fixed. And when
V ∈ Dχ,δ,p or equivalently Wp(ν, X + μ) ≤ δ, applying the dual form of Wp distance given in
formula 30 and 31, we have
Wι(v, x + μ) = SuP / 夕(x)(f - g)(x)dx = SuP / 夕(x)(g - f )(x)dx
φ∈F1 J	φ∈F1 J
= sup	f (x)q(x)dx ≤ δ
llfl∣L≤ι J
sup / 夕(x)q(x)dx
φ∈Fp J
(153)
And when V ∈ Dχ,ξ or equivalently TV(ν, X + μ) ≤ ξ, applying lemma A.3 for absolutely contin-
uous measure, we have
TV(ν,x + μ) = 1 / If (x) - g(x)∣dx
2 / Iq(X)Idx ≤ ξ
(154)
It follows that OPT(φ, X + μ, Dχ,δ,p ∩ Dxξ) is equivalent to minν∈Dx,δ,p∩Dx,ξ E[φ(X)] according
to the definition and therefore equivalent to optimization problem 23 Which iS obviously convex
according to 153 and 154.	□
K Proof of theorem 3.8
Recall the following result proved in the section before
Ex”[Φ(X )] = / φ(x)q(x)dx + EX 〜x+μ[φ(X )]
(155)
When v ∈ Dχ,δ,p or equivalently Wp(ν, X + μ) ≤ δ, applying the dual form of Wp distance given
in formula 32 and noticing that suPy∈spt(ν)∪spt(x+μ) ∣∣y∣∣2 = ∣∣x∣∣2 + R + max{e, Cd1 - 1 } := R*,
we have
( sup Z P(y)(g - f)(y)dy -(P-1)(2R*)p-1) ≤ Wp(v,x + μ) ≤ δ (156)
"∈Lip(p(2R*)pT) J	)
or equivalently
sup	[夕(y)(g - f )(y)dy =	SuP	f f (x)q(x)dx ≤ δp + (p - 1)(2R*)pT
0∈Lip(p(2R*)p-1) 1	||f ∣∣l≤p(2R* )p-1 J
(157)
WhereLiP (p(2R*)p-1) denotes all maps f from Rd to R SUchthat |f(x) - f (y)∣ ≤ p(2R*)p-1∣∣x -
y|| for all x,y ∈ K. Note OPT(φ,x + μ, Dχ,δ,p ∩Dχ,ξ) is equivalent to minν∈Dχ,δ,p∩Dχ,ξ E[Φ(X)]
according to the definition and therefore can be relaxed into optimization problem Which is obvioUsly
convex according to 157 and 154.
29
Under review as a conference paper at ICLR 2022
L Proof of theorem 3.9
Proof. For 0 < p ≤ 1, the optimization over q can be solved using Lagrangian duality as follows:
we dualize the constraints on q and obtain
L(λ) =	inf ( / φ(x)q(x)dx + EX〜χ+*[φ(X)] + λ( SUP / f(x)q(x)dx — δ))
∣ ∣ q∣∣ι≤2ξ U	'∣∣ f 11 l,p≤iJ	jJ
=EX〜χ+μ[φ(X)] +	inf sup	φ(x)q(x)dx + λ( sup / f(x)q(x)dx — δ
∣ ∣ q∣∣ι≤2ξ∣∣ f 11 l,p_	'∣∣ f 11 L≤1J	-
EX 〜χ+“[φ(X)] + inf sup
∣ ∣ q ∣ ∣ ι≤2ξ∣ ∣ f 11 l,p≤i
EX 〜x + μ[φ(X )]+	sup	IlliInfU
∣ ∣ f 11 l,p≤i∣∣ q ∣∣ι≤2ξ
/
/
(φ(x) + f (x)) q(x)dx — λδ
(φ(x) + f (x)) q(x)dx — λδ
EX 〜χ+”[φ(X)] +	sup inf
∣ ∣ f 11 l,p≤i∣ ∣ q ∣∣ι≤2ξ
EX〜χ+μ[φ(X )]
inf sup
f ∣∣l,p≤1 ∣∣q∣∣ι≤2ξ
— J ∣ (φ(x) + f (x))q(x) ∣ dx — λδ
J ∣ (φ(x) + f (x))q(x) ∣ dx — λδ
(158)
EX〜χ+μ[φ(X )]
EX〜χ+μ[φ(X )]
—2ξ∣∣∕∣in,fJ i φ(x)+f (x) i i∞ - λδ
—2ξ — λδ
(159)
(160)
—
where 158 is due to the choice of q(x) such that sgn(q(x)) = sgn(φ(x) + f (x)); 159 is due to
Holder inequality when q = 1,p = ∞; 160 is due to the fact that inf∣ ∣ 力∣ l≤1 11 φ(x) + f (x) 11 ∞ = 1
since the range of φ(x) is {±1} in applications and f cannot change suddenly when crossing the
decision region boundary of φ due to the Lipschitz constant constraint.
Similarly, for P > 1, we have
L(λ)
inf
∣ ∣ q∣ ∣ ι≤2ξ '
EX 〜x+μ
φ(x)q(x)dx + EX〜χ+μ[φ(X)] + λ( sup f f (x)q(x)dx — (δp + (p — 1)(2R*)p-1)
、∣ f ∣ ∣ l≤p(2R*)p-1 J
[Φ(X)]
+ inf	sup	φ(x)q(x)dx + λ( sup f f(x)q(x)dx — (δp + (p — 1)(2R*)p-1)
∣ ∣ q∣∣1≤2ξ ∣ ∣ f 11 l≤p(2R*)p-	' ∣∣ f 11 l≤p(2R* )p-1 J
EX5"[°(X )] + ∣∣ q in≤ 2ξ∣ ∣ f∣ ∣ l≤X)p-i
EX 〜χ+“[φ(X)] +	sup	inf
∣ ∣ f ∣ ∣ l≤p(2r*)p-1 ∣ ∣ q ∣∣ι≤2ξ
EX 〜χ+”[φ(X)] +	sup	inf
∣ ∣ f ∣ ∣ l≤p(2r*)p-1 ∣ ∣ q ∣∣ι≤2ξ
EX 〜χ + μ[φ(X )]
—
inf	sup
∣ ∣ 力 ∣ L≤p(2R*)p-1 ∣∣ q ∣∣ι≤2ξ
/ (φ(x) + f (x))q(x)dx — λ(δp + (p — 1)(2R*)p-1)
/ (φ(x) + f (x))q(x)dx — λ(δp + (p — 1)(2R*)p-1)
—/ I (φ(x) + f (x))q(x) I dx — λ(δp + (p — 1)(2R*)pT)
/ I (φ(x) + f (x))q(x) I dx — λ(δp + (p — 1)(2R*)pT)
EX 〜χ+μ[φ(X )]
EX 〜χ+μ[φ(X )]
-2ξ∣ ∣ f∣ ∣L≤%r*)p-i 1 lφ(X) + f (X) 1 L - λ(δp + (p - 1)(2R*)pT)
—2ξ — λ(δp + (p — 1)(2R*)P-1)
□
30
Under review as a conference paper at ICLR 2022
M Proof of theorem 3.10
Recall the certificate by using Hockey-stick divergence provided in table 4 in Dvijotham et al. (2020)
as below
HS,β ≤
β(θa - θb) - lβ - 1| i
2	1+
When β = 1, it follows that
θa - θb
eHS,1 ≤ [ -2 - J +
Besides, recall the relaxation radius using Hockey-stick divergence as below
eHS,1=G( 2σ)- G(-2σ)=2G( 2σ)- 1
And plug it in above inequality, we have
eHS,1=2G(2σ) - 1 ≤ hθa-θb i+
And recall the definition of θa and θb , we have
EX〜χ+μ[φ(X)] = θa 一 θb
Thus, our certificate EX〜χ+*[φ(X)] 一 2(2G() 一 1) ≥ 0 is equivalent to
2G(二)一 1 ≤ %-4
2σ	2
Thus, the equivalence relation holds.
31