Accelerating HEP simulations with Neural
Importance Sampling
Anonymous authors
Paper under double-blind review
Ab stract
Virtually all high-energy-physics (HEP) simulations for the LHC rely on Monte
Carlo using importance sampling by means of the VEGAS algorithm. How-
ever, complex high-precision calculations have become a challenge for the stan-
dard toolbox. As a result, there has been keen interest in HEP for modern ma-
chine learning to power adaptive sampling. Despite previous work proving that
normalizing-flow-powered neural importance sampling (NIS) sometimes outper-
forms VEGAS, existing research has still left major questions open, which we
intend to solve by introducing ZuNIS, a fully automated NIS library. We first
show how to extend the original formulation of NIS to reuse samples over multi-
ple gradient steps, yielding a significant improvement for slow functions. We then
benchmark ZuNIS over a range of problems and show high performance with
limited fine-tuning. The library can be used by non-experts with minimal effort,
which is crucial to become a mature tool for the wider HEP public.
1	Introduction
High-Energy-Physics (HEP) simulations are at the heart of the Large Hadron Collider (LHC) pro-
gram for studying the fundamental laws of nature. Most HEP predictins are expressed as expectation
values, evaluated numerically as Monte Carlo (MC) integrals. This permits both the integration of
the very complex functions and the reproduction of the data selection process by experiments.
Most HEP simulations tools (Alwall et al., 2014) perform MC integrals using importance sampling,
which allows to adaptively sample points to speed up convergence while keeping independent and
identically distributed samples, crucial to reproduce experimental analyses which can only ingest
uniformly-weighted data, typically produced by rejection sampling (see appendix A).
The most popular tool to optimize importance sampling in HEP is by far the VEGAS algorithm (Lep-
age, 1980; 2021), which fights the curse of dimensionality by assuming no correlations between the
variables. While this is rarely the case in general, a good understanding of the integrand func-
tion can help significantly. Indeed optimized parametrizations using multichannelling (Kleiss et al.,
1986; Ohl, 1999; Kleiss & Pittau, 1994) have become bread-and-butter tools for HEP event genera-
tion simulators, with good success for leading-order (LO) calculations. However, as simulations get
more complex, either by having more complex final states or by including higher orders in pertur-
bation theory, performance degrades fast.
While newer approaches to adaptive importance sampling have been developped, like Population
Monte Carlo (BUgano et al., 2017; 2015; CaPPe et al., 2004; Iba, 2001) and its extensions (CaPPe
et al., 2008; Koblents & Miguez, 2015; Elvira et al., 2017; Douc et al., 2007; Cornuet et al., 2012),
and have been successfully aPPlied to other fields, VEGAS remains the standard tool used by essen-
tially all modern simulation tools (Alwall et al., 2014; Bothmann et al., 2019; Bellm et al., 2016).
We susPect that the reasons for this are twofold. First of all the lack of exchange between the two
scientific communities is Probably at cause and should Probably not be neglected. Second of all
however, the imPetus in the HEP community is to move toward more "black box" aPProaches where
little is known of the structure of the integrand, which seems to be in tension with the situation with
PMC, which is sensitive to to the initial ProPosal density (Beaujean & Caldwell, 2013; CaPPe et al.,
2008). As the main Practical goal of this PaPer is to reduce the reliance of HEP simulations on the
careful tuning of integrators, we will focus on comParing our work with the de facto HEP standard:
the VEGAS algorithm.
1
There is much room for investing computational time into improving sampling (ATLAS Collabora-
tion, 2020): modern HEP theoretical calculations are taking epic proportions and can require hours
for a single function evaluation (Jones, 2018). Furthermore, unweighting samples can be extremely
inefficient, with upwards of 90% sampled points discarded (HEP Software Foundation, 2020). More
powerful importance sampling algorithms would therefore be a welcome improvement (Buckley,
2020; WG et al., 2021).
First attempts to use machine learning (ML) to address this challenge explored using classical neural
networks to sample (Bendavid, 2017; Klimek & Perelstein, 2020; Chen et al., 2021) but typically
suffer from excessive computational costs. Another avenue of research has been to leverage gener-
ative models successful in other fields such as generative adversarial networks (Butter et al., 2019;
Di Sipio et al., 2019; Butter et al., 2020; Ahdida et al., 2019; Hashemi et al., 2019; Carrazza &
Dreyer, 2019). While such approaches do improve sampling speed by a large factor, they have ma-
jor limitations. In particular, they have no theoretical guarantees of providing a correct answer on
average (Matchev et al., 2021) and poor control of uncertainties.
To avoid these disadvantages, our work exploits Neural Importance Sampling (NIS) (Muller et al.,
2019; Zheng & Zwicker, 2019), which relies on normalizing flows and has strong theoretical guar-
antees.
A number of exploratory papers have been published on using NIS for LHC simulations (Gao et al.,
2020b; Bothmann et al., 2020; Gao et al., 2020a), as well as closely related variations (Bellagente
et al., 2021; Stienen & Verheyen, 2021), but most studies have focused on preliminary investigation
of performance without much concern for the practical usability of the method. Indeed, training
requires function evaluations, which we are trying to minimize and data-efficiency training is there-
fore an important but under-appreciated concern. Furthermore, few authors have provided usable
open source code, making the adoption of the technique in the HEP community difficult.
Our contribution to improve this situation can be summarized in two items:
•	The introduction of a new loss function and an associated new training algorithm for NIS.
This permits the re-use of sampled points over multiple gradient descent steps, therefore
making NIS much more data efficient.
•	The introduction of ZuNIS, a PyTorch-based library providing robust and usable NIS
tools, usable by non-experts. It implements previously-developped ideas as well as our
new training procedure and is extensively documented.
2	Background
2.1	Importance sampling as an optimization problem
Importance sampling relies on the interpretation of integrals as expectation values. Indeed, let us
consider an integral over a finite volume:
I = J dxf(x), where V(Ω) = / dx is finite.
(1)
Let P be a strictly positive probability distribution over Ω, We can re-express our integral as the
expectation of a random variable
f (x) _一 G f (Xi)
I = Lp(X)dxP由=X匕 N i=1 P(Xi),
(2)
whose mean is indeed I and whose standard deviation is σ(fp), where σ(f, p) is the standard
√N	, j
deviation of f (X)∕p(X) for X 〜p:
(3)
The problem statement of importance sampling is to find the probability distribution function p that
minimizes the variance of our estimator for a given N. In Neural Importance Sampling, we rely on
2
Normalizing Flows to approximate the optimal distribution, which we can optimize using stochastic
gradient descent.
2.2	Normalizing flows and coupling cells
Normalizing flows (Tabak & Vanden-Eijnden, 2010; Tabak & Turner, 2013; Rippel & Adams, 2013;
Rezende & Mohamed, 2015) provide a way to generate complex probability distribution functions
from simpler ones using parametric changes of variables that can be learned to approximate a tar-
get distribution. As such, normalizing flows are diffeomorphisms: invertible, (nearly-everywhere)
differentiable mappings with a differentiable inverse.
Indeed, if U 〜p(u), then T(U)= X 〜q(χ) where
q(x = T (u)) = p(u) |JT(u)|-1,	(4)
where JT is the Jacobian determinant of T :
∂Ti
JT (U) = det ——(U).	(5)
∂Uj
In the world of machine learning, T is called a normalizing flow and is typically part of a parametric
family of diffeomorphisms (T(∙, θ)) such that gradients Vθ JT are tractable.
Coupling cell mappings perfectly satisfy this requirement (Dinh et al., 2015; 2017; Muller et al.,
2018): they are neural-network-parametrized bijections whose Jacobian factor can be obtained an-
alytically without backpropagation or expensive determinant calculation. As such, they provide a
good candidate for importance sampling as long as they can be trained to learn an unnormalized
target function, which is exactly what neural importance sampling proposes.
2.3	Neural importance sampling
Neural importance sampling was introduced in the context of computer graphics (Muller et al., 2018)
and proposes to use normalizing flows as a family of probability distributions over which to solve
the minimization problem of importance sampling.
L(θ) = Zω dxP¾⅜.
(6)
Of course, to actually do so, one needs to find a way to explicitly evaluate L(θ) and the original
neural importance sampling approach proposes to approximate it using importance sampling. One
needs to be careful that the gradient of the estimator of the loss need not be the estimator of the
gradient of the loss. The gradient of the loss can be expressed as
VθL(θ)
-Zω dxfB vθlog P(X ⑼,
(7)
for which an estimator is proposed as
VbθL(θ)
-X (pfX⅛ )2 vθ log p (Xi,θ),
Xi 〜p.
(8)
The authors also observed that other loss functions are possible which share the same global mini-
mum as the variance based loss: for example, the Kullback-Leibler divergence DKL between two
functions is also minimized when they are equal. Such alternative loss functions are not guaranteed
to work for importance sampling, but they prove quite successful in practice. After training to mini-
mize the loss estimator of eq. (8), the normalizing flows provides a tractable probability distribution
function from which to sample points and estimate the integral.
3	Concepts and algorithms
In this section we describe the original contributions of this paper. The major conceptual innova-
tion We provide in ZUNIS is a more flexible and data-efficient way of training normalizing flows in
3
the context of importance sampling. This relies on a more rigorous formulation of the connection
between the theoretical expression of ideal loss functions in terms of integrals and their practi-
cal realizations as random estimators than in previous literature. We describe this improvement in
section 3.1. We also give a high-level overview of the organization of the ZuNIS library, which
implements this new training procedure.
3.1	Efficient training for importance sampling
In this section, we describe how we train probability distributions within ZüNIS using gradient-
based optimizers. While the solution proposed in the original formulation of NIS defined eq. (8)
works, its main drawback is that it samples points from the same distribution that it tries to optimize.
As a result, new points Xi must be sampled from p after every gradient step, which is very inefficient
for slow integrands.
Our solution to this problem is to remember that the loss function is an integral, which can be
evaluated by importance sampling using any PDF, not only p. We will therefore define an auxiliary
probability distribution function q(x), independent from θ, from which we sample to estimate our
loss function:
/
f (x)2
dxfx⅛ = XEq p(x,θ)q(x).
(9)
This is the basis for the general method we use for training probability distributions within ZüNIS,
described in algorithm 2. Because the sampling distribution is separated from the model to train,
the same point sample can be reused for multiple training steps, which is not possible when using
eq. (8). This is particularly important for high-precision particle physics predictions that involve
high-order perturbative calculations or complex detector simulations because function evaluations
can be extremely costly. We show in section 4, in particular in fig. 4 that reusing data indeed has a
very significant impact on data efficiency.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 1: Backward sampling training in ZuNIS
Data: Parametric PDF p(x,θ)
Result: Trained PDF p(x,θ)
for M steps do
Sample a batch x1,. . . , xnbatch from q;
Compute the sampling PDF values q(xi );
Compute function values f(xi );
Start tracking gradients with respect toθ;
for N steps do
Compute the PDF values from the parametric PDF p(xi ,θ);
nbatch	2
1	f(xi )
EstJmatethe lossL= Rp(χ∕)q(χ);
C	. LZ 个 ♦ K T	, ∙
Compute VθL using backpropagation;
Set θ . θ 一 ηVθL;
Reset gradients;
end
end
return p(x, θ);
After training, q is discarded and the integral is estimated from the optimized p.
The only constraint on q is that it yields a good enough estimate so that gradient steps improve the
model. Much like in other applications of neural networks, we have found that stochastic gradient
descent can yield good results despite noisy loss estimates. We propose three schemes for q:
•	A uniform distribution (survey_strategy="flat")
4
•	A frozen copy of the model, which can be updated once in a while1 (survey_strategy=
"forward")
•	An adaptive scheme starting with a uniform distribution and switching to sampling
from a frozen model when it is expected to yield a more accurate loss estimate
(survey_strategy="adaptive_variance").
An important point to notice is that the original NIS formulation in eq. (8) can be restated as a
limiting case of our approach. Indeed, if we take q to be a frozen version of the model p(x, θ0),
which we update everytime we sample points (setting N = 1 in algorithm 2), the gradient update in
line 9 is
Vθ
f (x)2
_x~p3o) p(x,θ)p(x,θo )_
θ0=θ
f (x)2
E —，)、
X〜p(x,θ)p(x, θ)
Vθ log p(x, θ).
(10)
—
2
3.2 THE ZUNIS LIBRARY
On the practical side, ZuNIS is a PyTorch-based library which implements many ideas formulated
in previous work but organizes them in the form of a modular software library with an easy-to-use
interface and well-defined building blocks. We believe this structure will help non-specialist use it
without understanding all the nuts and bolts, while experts can easily add new features to responds
to their needs. The ZüNIS library relies on three layers of abstractions which steer the different
aspects of using normalizing flows to learn probability distributions from un-normalized functions
and compute their integrals:
•	Flows, which implement a bijective mapping which transforms points and computes the
corresponding Jacobian are described in appendix F.1
•	Trainers, which provide the infrastructure to perform training steps and sample from
flow models are described in appendix F.2
•	Integrators, which use trainers to steer the training of a model and compute integrals
are described in appendix F.3
4 Results
In this section, we evaluate ZüNIS on a variety of test functions to assess its performance and com-
pare it to the commonly used VEGAS algorithm (Peter Lepage, 1978; Ohl, 1999). We first produce
a few low dimensional examples for illustrative purposes, then move on to integrating paramet-
ric functions in various dimensions and finally evaluate performance on particle scattering matrix
elements.
4.1	Low-dimensional examples
Let us start by illustrating the effectiveness of ZüNIS in a low dimensional setting where we can
readily visualize results. We define three functions on the 2D unit hypercube which each illustrate
some failure mode of VEGAS (see appendix C).
We ran the ZüNIS Integrator with default arguments over ten repetitions for each function and
report the performance of the trained integral estimator compared to a flat estimator and to VEGAS
in table 1. Overall, ZüNIS Integrators learn to sample from their target function extremely
well: we outperform VEGAS by a factor 100 for the camel and the slashed circle functions and a
factor 30 for the sinusoidal function and VEGAS itself provides no advantage over uniform sampling
for the latter two.
We further illustrate the performance of our trained models by comparing the target functions and
density histogram for points sampled from the normalizing flows in fig. 1, which shows great quali-
tative agreement.
1This is inspired by deep-Q learning, where two copies of the value model are used: a frozen one used to
sample actions, and a trainable one used to estimate values in the loss function. Here the frozen copy is used to
sample points, and the trainable model is used to compute PDF values used in the loss function
5
Variance Reduction
vs. uniform
vs. VEGAS
Camel Slashed Circle Sinusoidal
1.8 ±	0.4	X	103^^8.9 ± 09	X	101 ^^2.0 ± 0.5 X 102
7.0 ±	1.4	×	102	8.8 ± 0.9	×	101	1.6 ± 0.5 × 102
Table 1: Variance reduction (high is good) for the camel, slashed circle and sinusoidal functions
compared to uniform sampling and to VEGAS over 10 repetitions.
Target Function	Target Function	Target Function
Sampled Points	Sampled Points	Sampled Points
(a) Camel function	(b) Sinusoidal function	(c) Slashed circle function
Figure 1: Comparison between target functions and point sampling densities for 1a the camel func-
tion, 1b the sinusoidal function, 1c the slashed circle function. Supplementary fig. 7 shows how
points are mapped from latent to target space.
4.2 Systematic benchmarks
Let US now take a more systematic approach to benchmarking ZuNIS. We compare ZUNIS Inte-
grators against uniform integration and VEGAS using the following metrics: integrand variance (a
measure of convergence speed, see section 2.1), unweighting efficiency (a measure of the efficiency
of exact sampling with rejection, see appendix A) and wall-clock training and sampling2.
ZoNIS improves convergence rate compared to VEGAS For this experiment, we focus on the
camel function defined in eq. (12) and scan a 35 configurations spanning from 2 to 32 dimensions
over function variances between 10-2 and 102 as shown in table 3.
Except in the low variance limit, ZüNIS can reduce the required number of points sampled to attain
a given precision on integral estimates without any parameter tuning, attaining speed-ups of up to
X1000 both compared to uniform sampling and VEGAS-based importance sampling, as shown in
fig. 2a-2b and table 4. Unweighting efficiencies are also boosted significantly, although more mildly
than variances, as shown in fig. 2c-2d, which we could attribute to PDF underestimation in regions
with low point density; the nature of the veto algorithm makes it very sensitive to afew bad behaving
points in the whole dataset.
ZüNIS is slower than VEGAS ZuNIS does not, however, outclass VEGAS on all metrics by
far: as shown in fig. 3, training is a few hundred times slower than VEGAS and sampling is 10-50
times slower, all while ZüNIS runs on GPUs. This is to be expected given the much increased com-
2We provide details on hardware in appendix G
6
(b)
(a)
Function Standard Deviation (σ(∕)∕E(∕))
Function Standard Deviation (σ(∕)∕E(∕))
(c)	(d)
Figure 2: Benchmarking ZuNIS against uniform sampling and VEGAS with default settings. In
(2a-2b), we show the sampling speed-up (ratio of integrand variance) as a function of the relative
standard deviation of the integrand, while we show the unweighting speed-up (ratio of unweighting
efficiencies) in (2c-2d).
putational complexity of normalizing flows compared to the VEGAS algorithm. As such, ZüNI S is
not a general replacement for VEGAS, but provides a clear advantage for integrating time-intensive
functions, where sampling is a negligible overhead, such as precision high-energy-physics simula-
tions.
(a)	(b)
Figure 3:	Comparison of the training and sampling speed of ZüNI S and VEGAS. As can be ex-
pected, ZüNI S is much slower than VEGAS, both for training and sampling, although larger batch
sizes can better leverage the power of hardware accelerators.
The new loss function introduced in ZuNIS improves data efficiency We have shown that
ZüNIS is a very performant importance sampling and event generation tool and provides significant
improvements over existing tools, while requiring little fine tuning from users. Another key result
is that the new approach to training we introduced in section 3.1 has a large positive impact on
7
performance. Indeed, as can be seen in fig. 4, re-using samples for training over multiple epochs
provides a 2- to 10-fold increase in convergence speed, making training much more data-efficient.
For this experiment, we use forward sampling, where the frozen model is used to sample a batch of
points which are then used for training over multiple epochs before resampling from an update of
the frozen model. As a result, we reproduce the original formulation of NIS in eq. (8) when we use
a single epoch as shown in eq. (10).
(a)	(b)
Figure 4:	Figure 4a: Effect of repeatedly training on the same sample of points over multiple epochs.
For all settings, there is a large improvement when going from one to moderate epoch counts, with
a peak around 5-10. Larger number of epochs lead to overfitting, which impacts performance nega-
tively. Figure 4b: Comparison between optimal data reuse (5 epochs) and the original NIS algorithm
(1 epoch).
4.3 MadGraph cross section integrals
Cross-sections are integrals of quantum transition matrix elements for a a scattering process such
as a LHC collision and express the probability that specific particles are produced. Matrix ele-
ments themselves are un-normalized probability distributions for the configuration of the outgoing
particles: it is therefore both valuable to integrate them to know the overall frequency of a given
scattering process, and to sample from them to understand how particles will be spatially distributed
as they fly off the collision point.
We study the performance of ZuNIS in comparison to VEGAS by studying three simple processes
at leading order in perturbation theory, e μ → e μ Via Z, dd → dd Via Z and Uc → Ucg (with 3-jet
cuts based on ∆R), see table 2 and fig. 5. We use the first process as a very easy reference while the
two other, quark-initiated processes are used to illustrate specific points. Indeed, both feature narrow
regions of their integration space with large enhancements, due respectiVely to Z -boson resonances
and infra-red diVergences.
(a)
Figure 5: Sample Feynman Diagrams for e-μ → e-μ via Z, dd → dd via Z and Uc → Ucg .
We eValuate the matrix elements for these three processes by using the Fortran standalone in-
terface of MadGraph5_aMC @ NLO (Alwall et al., 2014). The two hadronic processes are con-
volved with parton-distribution functions from LHAPDF6 (Buckley et al., 2015). We parametrize
8
	e-μ → e-μ via Z	dd → dd Via Z	UC → UCg
dimensions	2	4	7
normalized standard deviation	1.45 × 10-2	6.57 × 10-2	0.96
Table 2: Comparison of the three test processes.
phase space (the particle configuration space) using the RAMBO on diet algorithm (Platzer, 2013)
implemented for PyTorch in TORCHPS (Gotz, 2021).
We report benchmark results in fig. 6, in which we trained over 500,000 points for each process
using near-default configuration, scanning only over variance and Kullback-Leibler losses.
(a)
Figure 6: Average performance of ZuNIS over 20 runs relative to VEGAS, measured by the relative
speed-up in fig. 6a and by the relative unweighting efficiency in fig. 6b.
(b)
As previously observed, little convergence acceleration is achieved for low variance integrands like
e-μ → e-μ, but unweighting still benefits from NIS. The two hadronic processes illustrate typical
features for cross sections: training performance is variable and different processes are optimized
by different loss function choices3.
The performance ofdd → dd shows nice improvement with ZuNIS while that ofuc → ucg is more
limited. This can be understood by comparing to importance sampling (see appendix D.3): it is in
fact VEGAS, which performs significantly better on UC → UCg compared to dcT → dd because the
parametrization of RAMBO is based on splitting invariant masses, making them aligned with the
enhancements in the UCg phase space and allowing great VEGAS performance. This drives a key
conclusion for the potential role of ZüNIS in the HEP simulation landscape: not to replace VEGAS,
but to fill in the gaps where it fails due to inadequate parametrizations, as we illustrate here by using
non-multichanneled dd → dd as a proxy for more complex processes.
5	Conclusion
We have showed that ZüNIS can outperform VEGAS both in terms of integral convergence rate
and unweighting efficiency on specific cases, at the cost of a significant increase in training and
sampling time, which is an acceptable tradeoff for high-precision HEP computations with high costs.
In this context, the introduction of efficient training is a key element to making the most of the
power of neural importance sampling where function evaluation costs are a major concern. While
further testing is required to ascertain how far NIS can fill the gaps left by VEGAS for integrating
complex functions, there is already good evidence that ZüNIS can provide needed improvements
in specific cases. We hope that the publication of a usable toolbox for NIS such as ZüNIS will stir
a wider audience within the HEP community to apply the method so that the exact boundaries its
applicability can be more clearly ascertained.
3Generally, smoother functions are better optimized with the Kullback-Leibler loss while functions with
peaks benefit from using the variance loss. As we show in appendix D.4, choosing an adaptive strategy is
generally advisable whatever the loss
9
6	Reproducibility Statement
An anonymized version of the library is available on Github.
The recommended procedure to reproduce the experiments is to clone the repository and install the
Python requirements using pip install -r requirements.txt.
The data to reproduce the experiments can be generated using scripts provided in the repository
at experiments/benchmarks, in which Jupyter notebooks are also available to reproduce the
figures of the paper. The following scripts are available:
•	benchmarks_03/camel/run_benchmark_defaults.sh to generate camel integration
data
•	benchmarks_04/camel/run_benchmark_defaults.sh to generate camel sampling
speed data
•	benchmark_madgraph/ex_benchmark_emu.sh to generate e-μ → e-μ Via Z integra-
tion data
•	benchmark_madgraph/ex_benchmark_dd.sh to generate dd → dd Via Zintegration
data
•	benchmark_madgraph/ex_benchmark_ucg.sh to generate uc → ucg integration data
These scripts assume that 5 CUDA GPUs are aVailable and run 5 benchmarks in parallel. If fewer
GPUs are aVailable, it is recommended to modify the scripts to run the benchmarking scripts se-
quentially (by remoVing the ampersand) and to adapt the --cuda=N option.
References
C. Ahdida, R. Albanese, A. AlexandroV, A. Anokhina, S. Aoki, G. Arduini, E. Atkin, N. Azorskiy,
J.J. Back, A. Bagulya, and et al. Fast simulation of muons produced at the ship experiment using
generative adversarial networks. Journal OfInstrumentation,14(11):P11028-P11028, NoV 2019.
ISSN 1748-0221. doi: 10.1088/1748-0221/14/11/p11028. URL http://dx.doi.org/10.
1088/1748-0221/14/11/P11028.
J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H.-S. Shao, T. Stelzer,
P. Torrielli, and M. Zaro. The automated computation of tree-leVel and next-to-leading order
differential cross sections, and their matching to parton shower simulations. Journal of High
Energy Physics, 2014(7), Jul 2014. ISSN 1029-8479. doi: 10.1007/jhep07(2014)079. URL
http://dx.doi.org/10.1007/JHEP07(2014)079.
The ATLAS Collaboration. ATLAS HL-LHC Computing Conceptual Design Report. 2020.
Frederik Beaujean and Allen Caldwell. Initializing adaptiVe importance sampling with markoV
chains. arXiv preprint arXiv:1304.7808, 2013.
Marco Bellagente, Manuel Hauβmann, Michel Luchmann, and Tilman Plehn. Understanding Event-
Generation Networks Via Uncertainties. arXiv:2104.04543 [hep-ph], April 2021.
Johannes Bellm et al. Herwig 7.0/Herwig++ 3.0 release note. Eur. Phys. J. C, 76(4):196, 2016. doi:
10.1140/epjc/s10052-016-4018-8.
Joshua Bendavid. Efficient Monte Carlo Integration Using Boosted Decision Trees and Generative
Deep Neural Networks. 2017.
Enrico Bothmann, Timo Janβen, Max Knobbe, Tobias Schmale, and Steffen Schumann. Exploring
phase space with Neural Importance Sampling. SciPost Physics, 8(4):069, April 2020. ISSN
2542-4653. doi: 10.21468/SciPostPhys.8.4.069.
Enrico Bothmann et al. Event Generation with Sherpa 2.2. SciPost Phys., 7(3):034, 2019. doi:
10.21468/SciPostPhys.7.3.034.
10
Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estima-
tion. 2020.
Andy Buckley. Computational challenges for MC event generation. Journal of Physics: Conference
Series, 1525:012023, April 2020. ISSN 1742-6596. doi: 10.1088/1742-6596/1525/1/012023.
Andy Buckley, James Ferrando, StePhen Lloyd, Karl Nordstrom, Ben Page, Martin Rufe-
nacht, Marek Schonherr, and Graeme Watt. LHAPDF6: parton density access in the LHC
Precision era. The European Physical Journal C, 75(3), Mar 2015. ISSN 1434-6052.
doi: 10.1140/epjc/s10052-015-3318-8. URL http://dx.doi.org/10.1140/epjc/
s10052-015-3318-8.
Monica F. Bugallo, Victor Elvira, Luca Martino, David Luengo, Joaquin Miguez, and Petar M.
Djuric. Adaptive importance sampling: The past, the present, and the future. IEEE Signal Pro-
CessingMagazine, 34(4):60-79,2017. doi: 10.1109/MSP.2017.2699226.
M6nica F. Bugallo, Luca Martino, and Jukka Corander. Adaptive importance sampling in signal
processing. Digital Signal Processing, 47:36-49, 2015. ISSN 1051-2004. doi: https://doi.
org/10.1016/j.dsp.2015.05.014. URL https://www.sciencedirect.com/science/
article/pii/S1051200415001864. Special Issue in Honour of William J. (Bill) Fitzger-
ald.
Anja Butter, Tilman Plehn, and Ramon Winterhalder. How to gan lhc events. SciPost Physics, 7
(6), Dec 2019. ISSN 2542-4653. doi: 10.21468/scipostphys.7.6.075. URL http://dx.doi.
org/10.21468/SciPostPhys.7.6.075.
Anja Butter, Tilman Plehn, and Ramon Winterhalder. How to gan event subtraction. SciPost Physics
Core, 3(2), Nov 2020. ISSN 2666-9366. doi: 10.21468/scipostphyscore.3.2.009. URL http:
//dx.doi.org/10.21468/SciPostPhysCore.3.2.009.
Olivier Capp6, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Population monte carlo.
Journal of Computational and Graphical Statistics, 13(4):907-929, 2004.
Olivier Capp6, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Adaptive
importance sampling in general mixture classes. Statistics and Computing, 18(4):447-459, 2008.
Stefano Carrazza and FrederiC A. Dreyer. Lund jet images from generative and cycle-consistent
adversarial networks. The European Physical Journal C, 79(11), Nov 2019. ISSN 1434-
6052. doi: 10.1140/epjc/s10052-019-7501-1. URL http://dx.doi.org/10.1140/
epjc/s10052-019-7501-1.
I.-Kai Chen, Matthew D. Klimek, and Maxim Perelstein. Improved Neural Network Monte Carlo
Simulation. SciPost Physics, 10(1):023, January 2021. ISSN 2542-4653. doi: 10.21468/
SciPostPhys.10.1.023.
Jean-Marie Cornuet, Jean-Michel Marin, Antonietta Mira, and Christian Robert. Adaptive multiple
importance sampling. Scandinavian Journal of Statistics, 39(4):798-812, 2012. doi: https://doi.
org/10.1111/j.1467-9469.2011.00756.x. URL https://onlinelibrary.wiley.com/
doi/abs/10.1111/j.1467-9469.2011.00756.x.
Riccardo Di Sipio, Michele Faucci Giannelli, Sana Ketabchi Haghighat, and Serena Palazzo.
Dijetgan: a generative-adversarial network approach for the simulation of qcd dijet events
at the lhc. Journal of High Energy Physics, 2019(8), Aug 2019. ISSN 1029-8479. doi:
10.1007/jhep08(2019)110. URL http://dx.doi.org/10.1007/JHEP08(2019)110.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components
Estimation, 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP, 2017.
Randal Douc, Arnaud Guillin, J-M Marin, and Christian P Robert. Minimum variance importance
sampling via population monte carlo. ESAIM: Probability and Statistics, 11:427-447, 2007.
11
Victor Elvira, LUca Martino, David Luengo, and M6nica F. Bugallo. Improving population
monte carlo: Alternative weighting and resampling schemes. Signal Processing, 131:77-91,
2017. ISSN 0165-1684. doi: https://doi.org/10.1016/j.sigpro.2016.07.012. URL https:
//www.sciencedirect.com/science/article/pii/S0165168416301633.
Christina Gao, Stefan Hoche, Joshua Isaacson, Claudius Krause, and Holger Schulz. Event genera-
tion with normalizing flows. Physical Review D, 101(7):076002, April 2020a. ISSN 2470-0010,
2470-0029. doi: 10.1103/PhysRevD.101.076002.
Christina Gao, Joshua Isaacson, and Claudius Krause. I-flow: High-dimensional Integration and
Sampling with Normalizing Flows. Machine Learning: Science and Technology, 1(4):045023,
November 2020b. ISSN 2632-2153. doi: 10.1088/2632-2153/abab62.
Niklas Gotz. NGoetz/TorchPS:-v1.0.1, March 2021. URL https://doi.org/10.5281/
zenodo.4639109. Available at https://github.com/NGoetz/TorchPS/tree/
v1.0.1, version 1.0.1.
Bobak Hashemi, Nick Amin, Kaustuv Datta, Dominick Olivito, and Maurizio Pierini. Lhc analysis-
specific datasets with generative adversarial networks, 2019.
HEP Software Foundation. HL-LHC Computing Review: Common Tools and Community Soft-
ware. arXiv:2008.13636 [hep-ex, physics:physics], August 2020. doi: 10.5281/zenodo.4009114.
Yukito Iba. Population based Monte Carlo algorithms. Trans. Jap. Soc. Artif. Intell., 16:279, 2001.
doi: 10.1527/tjsai.16.279.
F.	James. Monte Carlo Theory and Practice. Rept. Prog. Phys., 43:1145, 1980. doi: 10.1088/
0034-4885/43/9/002.
S.P. Jones. Higgs Boson Pair Production: Monte Carlo Generator Interface and Parton Shower. Acta
Physica Polonica B Proceedings Supplement, 11(2):295, 2018. ISSN 1899-2358, 2082-7865. doi:
10.5506/APhysPolBSupp.11.295.
R. Kleiss and R. Pittau. Weight optimization in multichannel Monte Carlo. Computer Physics Com-
munications, 83(2-3):141-146, December 1994. ISSN 00104655. doi: 10.1016/0010-4655(94)
90043-4.
R Kleiss, W. J Stirling, and S. D Ellis. A new Monte Carlo treatment of multiparticle phase space at
high energies. Computer Physics Communications, 40(2):359-373, June 1986. ISSN 0010-4655.
doi: 10.1016/0010-4655(86)90119-0.
Matthew Klimek and Maxim Perelstein. Neural network-based approach to phase space integration.
SciPost Physics, 9(4):053, October 2020. ISSN 2542-4653. doi: 10.21468/SciPostPhys.9.4.053.
Eugenia Koblents and JoaqUn Miguez. A population monte carlo scheme with transformed weights
and its application to stochastic kinetic models. Statistics and Computing, 25(2):407-425, 2015.
G. Peter Lepage. VEGAS: AN ADAPTIVE MULTIDIMENSIONAL INTEGRATION PROGRAM.
March 1980.
G.	Peter Lepage. Adaptive Multidimensional Integration: VEGAS Enhanced. Journal of Computa-
tional Physics, 439:110386, August 2021. ISSN 00219991. doi: 10.1016/j.jcp.2021.110386.
Konstantin T. Matchev, Alexander Roman, and Prasanth Shyamsundar. Uncertainties associated
with GAN-generated datasets in high energy physics. arXiv:2002.06307 [hep-ex, physics:hep-
ph, physics:physics], June 2021.
Thomas Muller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novdk. Neural Im-
portance Sampling, 2018.
Thomas Muller, Brian Mcwilliams, Fabrice Rousselle, Markus Gross, and Jan Novdk. Neural Impor-
tance Sampling. ACM Transactions on Graphics, 38(5):1-19, November 2019. ISSN 0730-0301,
1557-7368. doi: 10.1145/3341156.
12
Thorsten Ohl. Vegas revisited: Adaptive Monte Carlo integration beyond factorization. Comput.
Phys. Commun. ,120:13-19,1999. doi:10.1016/S0010-4655(99)00209-X.
G Peter Lepage. A new algorithm for adaptive multidimensional integration. Journal of
Computational Physics, 27(2):192-203, 1978. ISSN 0021-9991. doi: https://doi.org/10.
1016/0021-9991(78)90004-9. URL https://www.sciencedirect.com/science/
article/pii/0021999178900049.
Simon Platzer. RAMBO on diet, 2013.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows.
32nd International Conference on Machine Learning, ICML 2015, 2:1530-1538, 2015. ISSN
9781510810587.
Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density
models. arXiv preprint arXiv:1302.5125, 2013.
Bob Stienen and Rob Verheyen. Phase Space Sampling and Inference from Weighted Events with
Autoregressive Flows. SciPost Physics, 10(2):038, February 2021. ISSN 2542-4653. doi: 10.
21468/SciPostPhys.10.2.038.
Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms.
Communications on Pure and Applied Mathematics, 66(2):145-164, 2013.
Esteban G Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood.
Communications in Mathematical Sciences, 8(1):217-233, 2010.
The HSF Physics Event Generator WG, Andrea Valassi, Efe Yazgan, Josh McFayden, Simone
Amoroso, Joshua Bendavid, Andy Buckley, Matteo Cacciari, Taylor Childers, Vitaliano Ciulli,
Rikkert Frederix, Stefano Frixione, Francesco Giuli, Alexander Grohsjean, Christian Gutschow,
Stefan Hoche, Walter Hopkins, Philip Ilten, Dmitri Konstantinov, Frank Krauss, Qiang Li, Leif
Lonnblad, Fabio Maltoni, Michelangelo Mangano, Zach Marshall, Olivier Mattelaer, Javier Fer-
nandez Menendez, Stephen Mrenna, Servesh Muralidharan, Tobias Neumann, Simon Platzer,
Stefan Prestel, Stefan Roiser, Marek Schonherr, Holger Schulz, Markus Schulz, Elizabeth Sexton-
Kennedy, Frank Siegert, Andrzej Si6dmok, and Graeme A. Stewart. Challenges in Monte Carlo
event generator software for High-Luminosity LHC. Computing and Software for Big Science, 5
(1):12, December 2021. ISSN 2510-2036, 2510-2044. doi: 10.1007/s41781-021-00055-1.
Quan Zheng and Matthias Zwicker. Learning to Importance Sample in Primary Sample Space.
Computer Graphics Forum, 38(2):169-179, 2019. ISSN 1467-8659. doi: 10.1111/cgf.13628.
A Event generation with the veto algorithm
Generating i.i.d points following an arbitrary probability distributions in high dimensions is not a
priori a trivial task. A straightforward way to obtain such data is to use rejection sampling, which can
be based on any distribution q from which We can sample. Given an i.i.d sample xi,... ,xn 〜q,
we can define weights w(xi) = p(xi)/q(xi) and keep/reject points with probability w(xi)/wmax.
The main metric for evaluating the performance of this algorithm is the unweighting efficiency: how
much data is kept from an original sample of size N on average, which is expressed as
eunw = E w(x).	(11)
x~q WmaX
B	Backward Sampling Algorithm
13
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 2: Backward sampling training in ZUNIS
Data: Parametric PDF p(x, θ)
Result: Trained PDF p(x, θ)
for M steps do
Sample a batch x1, . . . , xnbatch from q;
Compute the sampling PDF values q(xi);
Compute function values f(xi);
Start tracking gradients with respect to θ;
for N steps do
Compute the PDF values from the parametric PDF p(xi, θ);
1 nbatch	f(xi)2
Estimatetheloss L = N 3 PRθ)q^;
Z-X	. X—7	?	♦ F 1	. ∙
Compute Vθ L using backpropagation;
Set θ . θ 一 ηVθ L;
Reset gradients;
end
end
return p(x, θ);
C Fundamental limitations of the VEGAS algorithm
We define three functions over the two dimensional hypercube:
fcamel(x)
fe(x)
fsin(x) = cos(k ∙ x)2 ,
(12)
(13)
(14)
to which we refer respectively as the camel, slashed circle and sinusoidal target functions. We set
their parameters as follows
0.25	0.75
μ1= 10.25) , μ2 = [0.75)
k = (6) , σ = 0.1, σ0 = 0.1, r = 0.3
(15)
We chose these functions because they illustrate different failure modes of the VEGAS algorithm:
•	Because of the factorized PDF of VEGAS, the camel function leads to ’phantom peaks’ in
the off diagonal. This problem grows exponentially with the number of dimensions but can
be addressed by a change of variable to align the peaks with an axis of integration.
•	The sinuoidal function makes it nearly impossible for VEGAS to provide any improvement:
the marginal PDF over each variable of integration is nearly constant. Again this type of
issue can be addressed by a change of variable provided one knows how to perform it.
•	The slashed circle function is an example of a hard-for-VEGAS function that cannot be
improved significantly by a change of variables. One can instead use multi-channeling, but
this requires a lot of prior knowledge and has a computational costs since each channel is
its own integral.
D Supplementary Results
D. 1 Qualitative examples
14
Figure 7: Mapping between the uniform point density in the latent space and the target distribution
for the camel function, the sinusoidal function, the slashed circle function. Points are colored based
on their position in latent space.
D.2 Systematic benchmark details
σ1d rel. st.d
2
Dimension
4
8
16
σ.marap lema
2.00 X 10-2 8.93 X 10-2 2.04 X 10-1	3.61 X 10-1	5.06 X 10-1	0.001	14.1
6.32 X 10-2 1.64 X 10-1 3.14 X 10-1 4.64 X 10-1 6.06 X 10-1	0.01	4.41
2.21 X 10-1 3.78 X 10-1	5.23 X 10-1	6.67 X 10-1	8.25 X 10-1	0.1	1.22
4.51 X 10-1 5.93 X 10-1 7.43 X 10-1 9.11 X 10-1	1.10	0.3	0.56
6.44 X 10-1 7.99 X 10-1 9.74 X 10-1	1.18	1.41	0.5	0.32
8.62 X 10-1	1.05	1.26	1.51	1.81	0.7	0.19
1.21	1.45	1.73	2.07	2.47	1.0	0.10
Table 3: Setup of the 35 different camel functions considered to benchmark ZuNIS. We scan over
function relative standard deviations, which correspond to different σ parameters for each dimen-
sion(eq. (12)). We provide the corresponding width of a 1D gaussian (σ1d) with the same variance
for reference.
15
Dimension
4	8	16
32
σ1d rel. st.d
pu-deeps sAGEV
,, ,, ∕l∖
000000
1111110
×××××××
444453431122
...........................65
220000000000..
+-+-+-+-+-+-00
6 0 4 3 2 4+
...........................0
723612.
6.0+-11..20 × 102
5.8+-01..80 × 102
1.0+-00..11 × 102
2.0+-00..01 × 101
3.5+-00..12 × 100
5.3+-00..54 × 10-1
7.8+-00..65 × 10-2
4.3-+11..40 ×	103	9.0+-43..71 ×	102	3.5-+21..78	×	100
5.1-+00..89 ×	102	2.8+-00..33 ×	102	4.5-+00..68	×	101
3.5-+00..12 ×	101	1.9+-00..11 ×	101	1.2-+00..11	×	101
7.2-+00..33 ×	100	2.8+-00..11 ×	100	1.3-+00..10	×	100
9.1+-00..64 × 10-1	3.7-+00..22	×	10-1	2.0-+00..11	×	10-1
1.3+-00..11 × 10-1	5.5-+00..43	×	10-2	3.0-+00..12	×	10-2
1.6+-00..11 × 10-2 7.8-+00..86 × 10-3 4.8-+00..34 × 10-3
1126301
.1 4 2 5 3 2 1
......................
14 4 1 0 0 0 0
1
0 01 .1 3. .5 .7 .0
.0 . 0 0 0 0 1
00
2
Table 4: Variance reduction factor compared to VEGAS for each of the 35 different camel setups
defined in table 3.
D.3 Comparing ZuNIS with uniform sampling on matrix elements
(a)
Figure 8: Average performance of ZüNIS over 20 runs relative to flat sampling, measured by the
relative speed-up in 8a and by the relative unweighting efficiency in 8b.
Unweighting Efficiency ratio relative to Uniform Sampling
(b)
D.4 Effect of survey strategies
in the following, we want to investigate how the integration of the three example processes in 4.3
with ZüNIS behaves relative to VEGAs in dependence of the choice of the loss function, the survey
strategy and the number of epochs during training. For all other options, the default values are chosen
again, except for the number of points during the survey phase, which is set to 500,000.
Figure 9a shows that for a simple process like e-μ → e-μ Via Z , where no correlations exist, ZU-
Nis cannot reach the speed-up achieved by VEGAs. Variance loss seems to lead to higher variance
improVements than DKL loss. Contrary, 9b shows that ZuNis can greatly improVe the unweighting
efficiency for this process. The effect is again consistently stronger when using Variance loss. Using
a flat surVey strategy suffers for both loss functions from oVerfitting, whereas adaptiVe sampling in
aVerage performs slightly better and does not show oVerfitting.
dd → dd via Z presents a more realistic use case, as the parton distribution functions introduce
correlations between the integration Variables which present a challenge to the VEGAs algorithm.
For this process, both the speed-up and the unweighting efficiency ratio clearly favor the variance
loss again, which outperforms in both metrics the DKL loss when multiple epochs are used, as can
be seen in 10. The richer structure of the integrand reduces the effect of overfitting. Therefore, the
performance increases or stays approximately constant except for the combination of DKL loss and
the forward survey strategy. For the variance loss, it becomes apparent that the flat survey strategy
increases much slower in performance than alternative strategies as a function of the number of
epochs. However, for combination of losses and survey strategies, VEGAs could be outperformed
substantially in terms of integration speed.
16
Variance Loss	DKL Loss
Loss and Survey Strategy
(a)
Variance Loss	DKL Loss
Loss and Survey Strategy
(b)
Figure 9: Median of the performance of ZuNIS over 20 runs relative to VEGAS for the process
e-μ → e-μ via Z depending on the loss function, the survey strategy and number of epochs,
measured by the relative speed-up in 9a and by the relative unweighting efficiency in 9b.
Variance Loss
DKL Loss
Loss and Survey Strategy
-100
-50
(a)
Variance Loss
DKL Loss
Loss and Survey Strategy
(b)
Figure 10: Median of the performance of ZüNIS over 20 runs relative to VEGAS for the process
dd → dd via Z depending on the loss function, the survey strategy and number of epochs, measured
by the relative speed-up in 10a and by the relative unweighting efficiency in 10b.
r7.5
-5.0
-2.5
An opposite picture is drawn by the process uc → ucg in figure 11, for which, apart from the flat
survey strategy, DKL loss is in general favored both for integration speed as well as unweighting
efficiency ratio. The adaptive survey strategy is here giving the best results, although for a high
number of epochs causes overfitting for the unweighting efficiency.
17
s-50ls-Jo -QqEnN
Variance Loss	DKL Loss
Loss and Survey Strategy
(a)
s-50ls-Jo -QqEnN
Variance Loss	DKL Loss
Loss and Survey Strategy
(b)
Figure 11: Median of the performance of ZUNIS over 20 runs relative to VEGAS for the process
uc → ucg depending on the loss function, the survey strategy and number of epochs, measured by
the relative speed-up in 11a and by the relative unweighting efficiency in 11b.
The take-home message of this section is, one the one hand, that the flat survey strategy is in general
not recommended. Apart from this, the most important mean to improve the quality of importance
sampling are testing whether, independent of the survey strategy, the loss function should be chosen
differently.
E Exact minimzation of the neural importance sampling
ESTIMATOR VARIANCE
Let us show that the optimal probability distribution for importance sampling is the function itself.
Explicitly, as discussed in section 2.1, we want to find the probability distribution p defined over
some domain Ω which minimizes the variance of the estimator f (X)∕p(X), for X 〜 P(X). We
showed that this amounts to solving the following optimization problem:
minLs=Z dx”,
P	ω	p(x)
such that
dx p(x) = 1,
which we can encode using Lagrange multipliers
p = argmin L(p,λ)= / dxf(x)2 + λ (P(X)-
ω	P(X)
We can now solve this problem by finding extrema with functional derivatives
δL(P, λ) — ∖	f(X)2
= λ -
δP(X)	P(X)2
(16)
(17)
(18)
which indeed is zero if P(X) ɑ |f (x) |. Furthermore, this extremum is certainly a minimum because
the loss function is positive and unbounded. Indeed, if We separate Ω into two disjoint measurable
subdomains Ωι and Ω2, and define Pa(X) such that points are drawn uniformly over Ωι with prob-
ability α and uniformly over Ω2 with probability 1 - α, then the resulting loss function would be
L(Pα)
dXf (X)2 + 詈/ dXf (X)2,
α	Ωι	1 - α JΩ2
(19)
which can be made arbitrarily large by sending α to 0.
18
F High-level concepts of the ZUNIS API
F.1 Normalizing flows with FLOW classes
Flows map batches of points and their densities. The ZUNIS library implements normalizing
flows by specifying a general interface defined as a Python abstract class: GeneralFlow. All
flow models in ZüNIS are child classes of GeneralFlow, which itself inherits from the Pytorch
nn.Module interface.
As defined in section 2.2, a normalizing flow in ZUNIS is a bijective mapping between two d
dimensional spaces, which in practice are always the unit hypercube [0, 1]d or Rd, with a tractable
Jacobian so that it can be used to map probability distributions. To this end, the GeneralFlow
interface defines normalizing flows as a callable Python object which acts on batches of point drawn
from a known PDF p. A batch of points xi with their PDF values is encoded as a Pytorch Tensor
object X organized as follows
X= (X1,...,Xbatch) ∈ Rbatch ×Rd+1,	(20)
where each Xi corresponds to a points stacked with its negative log density
(xi,1	∖
.
Xi =	:	.	(21)
Xi,d
- log p(xi)
Encoding point densities by their negative logarithm makes their transformation under normalizing
flows additive. Indeed if We have a mapping Q with Jacobian determinant jq, then X 〜 p(χ) is
mapped to y = Q(χ)〜p(y) such that
-log p(y) = - log p(x) + log jQ(x).	(22)
Coupling Cells are flows defined by an element-wise transform and a mask. All flow mod-
els used in ZüNIS in practice are implemented as a sequence of coupling cell transformations
acting on a subset of the variables. The abstract class GeneralCouplingCell and its child In-
vertibleCouplingCell specifies the general organization of coupling cells as needing to be
instantiated with
•	a dimension d
•	a mask defined as a list of boolean specifying which coordinates are transformed or not
•	a transform that implements the mapping of the non-masked variables
In release v1.0 of ZüNIS two such classes are provided: PWLinearCoupling and
PWQuadraticCoupling, which respectively implement the piecewise linear and piecewise
quadratic coupling cells proposed in (Muller et al., 2018). New coupling cells can easily be im-
plemented, as explained in appendix F.4. Both existing classes rely on dense neural networks for the
prediction of the shape of their one-dimensional piecewise-polynomial mappings, whose parameters
are set at instantiation.
Here is how one can use a piecewise-linear coupling cell for sampling points
import torch
from zunis.models.flows.coupling_cells.piecewise_coupling.
Piecewise.quadratic import PWQuadraticCOupling
d=2
N_batch=10
mask = [True,False]
X = torch.zeros((N_batch,d+1))
# Sample the d first entries uniformly, keep 0. for the negative log
jacobian
x[...,:-1].uniform_()
print (x[0]) # [0.3377, 0.4362, 0.]
f = PWQuadraticCOuPling(d=d,mask=mask)
y = f(x)
print (y[0]) # [0.3377, 0.4411, -0.0314]
19
We provide further details of the use and possible parameters of flows in the documentation of
ZuNIS: https://zunis-anonymous.github.io/zunis/.
F.2 Training with the TRAINER class
The design of the ZüNIS library intentionally restricts Flow models to being bijective mappings
instead of being ways to evaluate and sample from PDFs so as not to restrict their applicability
(see Brehmer & Cranmer (2020) for an example). The specific application in which one uses a
normalizing flow, and in our case how precisely one samples from it, is intimately linked to how
such a model is trained. As a result, ZüNIS bundles together the low-level training tools for Flow
models together with sampling tools inside the Trainer classes.
The general blueprint for such classes is defined in the GenericTrainerAPI abstract class while
the main implementation for users is provided as StatefulTrainer. At instantiation, all trainers
expect a Flow model and flow_prior which samples point from a fixed PDF in latent space.
These two elements together define a probability distribution over the target space from which one
can sample.
There are two main ways one interacts with Trainers:
•	One can sample points from the PDF defined by the model and the prior using the sam-
ple_forward method.
•	One can train over a pre-sample batch of points, their sampling PDF and the corresponding
function values using train_on_batch(self, x, px, fx)
In practice, we expect that the main way users will use Trainers is for sampling pre-trained
models. In the context of particle physics simulations for example, unweighting is a common task,
which aims at sampling exactly from a known function f . A common approach is the Hit-or-miss
algorithm (James, 1980), whose efficiency is improved by sampling from a PDF approaching f .
This is how one would use a trainer trained on f :
#	[...]
#	import or train a trainer ‘pretrained_trainer ‘
import torch
#	Sampling points
Xj = Pretrained_trainer.sample_forward(100)
x = x[:, :-1]
px = (-x[:,-1]).exp()
fx = f(x)
#	Applying the veto algorithm
fmax = fx.max()
veto = (fx/fmax - torch.zeros_like(fx).uniform_(0.,1.)) > 0.
x_unweighted = x[veto]
#	x_unweighted follows the PDF obtained by normalizing f.
F.3 Integrating with the INTEGRATOR class
Integrators are intended as the main way for standard users to interact with ZüNIS. They provide a
high-level interface to the functionalities of the library and only optionally require users to know to
what lower levels of abstractions really entail and to what their options correspond. From a practical
point of view, the main interface of ZüNIS for integration is implemented as the Integrator,
which is a factory function that instantiates the appropriate integrator class based on a choice of
options.
All integrators follow the same pattern, defined in the SurveyRefineIntegratorAPI and
BaseIntegrator abstract classes. Integration start by performing a survey phase, in which it
optimizes the way it samples points and then a refine phase, in which it computes the integral by
using its learned sampler. Each phase proceeds through a number of steps, which can be set at
instantiation or when integrating:
20
#	Setting values at instantiation time
integrator = Integrator(d=d, f=f, n_iter_survey=3, n_iter_refine=5)
#	Override at integration time
integral_data = integrator.integrate(n_Survey=10, n_refine=10)
For both the survey and the refine phases, using multiple steps is useful to monitor the stability of
the training and of the integration process: if one step is not within a few standard deviations of the
next, either the sampling statistics are too low, or something is wrong. For the refine stage, this is
the main real advantage of using multiple steps. On the other hand, at each new survey step, a new
batch of points is re-sampled, which can be useful to mitigate overfitting.
By default, only the integral estimates obtained during the refine stage are combined to compute
the final integral estimate, and their combination is performed by taking their average. Indeed,
because the model is trained during the survey step, the points sampled during the refine stage are
correlated in an uncontrolled way with the points used during training. Ignoring the survey stage
makes all estimates used in the combination independent random variables, which permits us to
build a formally correct estimator of the variance of the final result.
F.4 Implementing new coupling cells
To implement a new invertible coupling cell inheriting from InvertibleCouplingCell, one
must provide an InvertibleTransform object and define a callable attribute T computing the
parameters of the transform. For example, consider a very simple linear coupling cell over R
y = Q(X) : { yB = exp (T(XA))X XB
(23)
where T(XA) is a scalar strictly positive value. This can be defined in the following way in ZuNIS
import torch
from zunis.models.flows.coupling_cells.general_coupling import
InvertibleCOuplingCell
from zunis.models.flows.coupling_cells.transforms import
InvertibleTransform
from zunis.models.layers.trainable import ArbitraryShapeRectangularDNN
class LinearTransform(InvertibleTransform):
def forward(self,x,T):
alpha = torch.exp(T)
logj = T*x.shape[-1]
return x*alpha, logj
def backward(self,x,T):
alpha = torch.exp(-T)
logj = -T*x.shape[-1]
return x*alpha, logj
21
class LinearCOuplingCell(InvertibleCOuplingCell):
def _____init__(self, d, mask, nn_width, nn_depth):
transform = LinearTranSform()
Super(LinearCOuplingCell, self).___init__(d=d, mask=mask,transform=
transform)
d_in = sum(mask)
self.T = ArbitraryShapeRectangularDNN(d_in=d_in,
out_shape=(1,),
d_hidden=nn_width,
n_hidden=nn_depth)
G Hardware setup details
The computations presented in this work were performed on a computing cluster using a Intel(R)
Xeon(R) Gold 5218 CPU @ 2.30GHz with 376 GB RAM. Processes which could be performed on
the GPU were done on a GeForce RTX 2080 having 12 GB memory and running on CUDA 11.0.
22