Under review as a conference paper at ICLR 2022
How to train RNNs on chaotic data?
Anonymous authors
Paper under double-blind review
Ab stract
Recurrent neural networks (RNNs) are wide-spread machine learning tools for
modeling sequential and time series data. They are notoriously hard to train be-
cause their loss gradients backpropagated in time tend to saturate or diverge during
training. This is known as the exploding and vanishing gradient problem. Previ-
ous solutions to this issue either built on rather complicated, purpose-engineered
architectures with gated memory buffers, or - more recently - imposed constraints
that ensure convergence to a fixed point or restrict (the eigenspectrum of) the re-
currence matrix. Such constraints, however, convey severe limitations on the ex-
pressivity of the RNN. Essential intrinsic dynamics such as multistability or chaos
are disabled. This is inherently at disaccord with the chaotic nature of many, if
not most, time series encountered in nature and society. Here we offer a compre-
hensive theoretical treatment of this problem by relating the loss gradients during
RNN training to the Lyapunov spectrum of RNN-generated orbits. We mathe-
matically prove that RNNs producing stable equilibrium or cyclic behavior have
bounded gradients, whereas the gradients of RNNs with chaotic dynamics always
diverge. Based on these analyses and insights, we adapt the old idea of teacher
forcing to yield an effective yet simple training technique for chaotic data, and
offer guidance on how to choose relevant hyperparameters according to the Lya-
punov spectrum.
1	Introduction
Recurrent neural networks (RNNs) are widely used across various fields in engineering and science
for learning sequential tasks or modeling and predicting time series (Lipton et al., 2015). Yet, they
struggle when long-term temporal dependencies, very slow, or hugely varying time scales are in-
volved (Hochreiter, 1991; Bengio et al., 1994; Schmidt et al., 2021; Li et al., 2018; Rusch & Mishra,
2021a). Time series or sequential data with such properties are, however, very common in fields
like climate physics (Thomson, 1990), neuroscience (Fusi et al., 2007; Russo & Durstewitz, 2017),
ecology (Turchin & Taylor, 1992), or language processing (Cho et al., 2014b). Training RNNs on
such data is hard because the loss gradients backpropagated in time easily saturate or diverge in this
process. This is commonly referred to as the exploding and vanishing gradient problem (EVGP)
(Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013).
One solution to the EVGP is based on specifically designed RNN architectures with gating mecha-
nisms, such as long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) or gated recur-
rent units (GRU) (Cho et al., 2014a). These architectures allow states at earlier time steps to more
easily influence activity much later through a kind of protected memory buffer, thus alleviating the
EVGP by structural design. In practice, such models need to be backed up by further techniques
like gradient clipping to keep the gradients in check (Pascanu et al., 2013). The relatively complex
architectural design of these networks impedes their mathematical analysis and requires reverse en-
gineering after training (Maheswaranathan et al., 2019; Monfared & Durstewitz, 2020a;b; Schmidt
et al., 2021). Partly to forego these complications, a variety of other solutions has been proposed re-
cently, imposing restrictions on the recurrence matrix to bound the gradients (Arjovsky et al., 2016;
Chang et al., 2019), or enforcing global stability by design or regularization (Erichson et al., 2021;
Kolter & Manek, 2019). Often these procedures dramatically curtail the expressivity of the RNN
(Kerg et al., 2019; Orhan & Pitkow, 2020; Schmidt et al., 2021); in particular, they rule out chaotic
dynamics (see further discussion below).
1
Under review as a conference paper at ICLR 2022
This is at odds with the plethora of chaotic phenomena in nature, engineering, and society. Chaotic
dynamics are commonplace, almost default in any complex physical or biological system. This in-
cludes scientific areas as diverse as neuroscience (Durstewitz & Gabriel, 2007; van Vreeswijk &
Sompolinsky, 1996), physiology (Kesmia et al., 2020), geophysics (Sivakumar, 2004), climate sys-
tems (Tziperman et al., 1997), astrophysics (Laskar & Robutel, 1993), ecology (Duarte et al., 2010),
chemical reactions (Field & Gyorgyi,1993), cell (Olsen & Degn, 1977) or population (May, 1987)
biology. Chaotic phenomena are also crucial for the understanding of societal and epidemiologi-
cal processes, such as the spread of diseases (Mangiarotti et al., 2020), or in economics (Faggini,
2014). They are further relevant in purely technical contexts such as electrical engineering (Tchit-
nga et al., 2019; Kamdjeu Kengne et al., 2021) or laser optics (Kantz et al., 1993). They have
even been suggested to play an up to now largely neglected, but potentially very significant role
in speech recognition (Sabanal & Nakagawa, 1996) and natural language processing (Inoue et al.,
2021). Hence, in almost any practical setting, chaotic phenomena abound. They cannot, in general,
be ignored when devising RNN training algorithms.
Here we offer a comprehensive theoretical treatment of the relation between RNN dynamics and the
behavior of the loss gradients during training. We find a close connection between an RNN’s loss
gradients and the largest Lyapunov exponent of its freely generated orbits. We mathematically prove
that RNNs producing stable fixed point or cyclic behavior have bounded gradients. Crucially, how-
ever, the loss gradients of RNNs producing chaotic dynamics always diverge. Hence, the chaotic
nature of many time series data induces a principle problem, and, despite significant efforts in the
past to solve the EVGP, training RNNs on such data remains an open issue. We illustrate the impli-
cations of our theory for RNN training on several simulated and empirical chaotic time series, and
adapt the old idea of sparsely forced BPTT as a simple yet effective remedy that enables to learn the
underlying dynamics despite exploding gradients.
2	Related works
Exploding and vanishing gradients. While ‘classical’ remedies of the EVGP (Hochreiter, 1991; Ben-
gio et al., 1994; Pascanu et al., 2013) rest on purpose-tailored architectures with gating mechanisms,
which safeguard information flow across longer temporal distances (Hochreiter & Schmidhuber,
1997; Cho et al., 2014a), the focus has recently shifted to simpler RNNs that address the EVGP by
restricting the recurrence matrix to be orthogonal (Henaff et al., 2016; Helfrich et al., 2018; Jing
et al., 2019), unitary (Arjovsky et al., 2016), or antisymmetric (Chang et al., 2019), or by ensur-
ing globally stable fixed point solutions (Kag et al., 2020; Kag & Saligrama, 2021a), for example
through co-trained Lyapunov functions (Kolter & Manek, 2019). However, all these approaches
impose strong limitations on the dynamical repertoire of the RNN, enforcing global convergence
to fixed points or simple cycles.1 In doing so, they drastically reduce the expressiveness of these
models (Kerg et al., 2019; Orhan & Pitkow, 2020). To address this problem, Erichson et al. (2021)
somewhat relaxed the constraints on the recurrence matrix by introducing a skew-symmetric decom-
position combined with a Lipschitz condition on the activation function. Another recent approach
discretizes oscillator ODEs to arrive at a stable system of coupled (Rusch & Mishra, 2021a) or
independent (Rusch & Mishra, 2021b) oscillators which increase the RNN’s expressiveness while
bounding its gradients. By design (and as acknowledged by the authors), neither of these architec-
tures is capable of producing chaotic dynamics, however, as the underlying ODEs do not allow for
exponential divergence of close-by trajectories (a prerequisite for chaos). Given these often prin-
ciple limitations of parametrically or dynamically strongly constrained models, a fruitful direction
may be to modify the training process itself, e.g. through modified or auxiliary loss functions (Trinh
et al., 2018; Schmidt et al., 2021), or special procedures for parameter updating (Kag & Saligrama,
2021b) or loss truncation (Williams & Zipser, 1989; Menick et al., 2021). Our empirical evalu-
ation will follow up on such ideas, but also highlight that simple loss truncation, windowing, or
architectural solutions like LSTMs are not sufficient.
Learning dynamical systems. Surprisingly disconnected from the work on the EVGP and learning
long-term dependencies, a huge and long-standing literature deals with training RNNs on nonlinear
dynamical systems (DS) (Pearlmutter, 1990; Trischler & D’Eleuterio, 2016; Vlachas et al., 2020),
including chaotic systems like the famous Lorenz equations (Lorenz, 1963) or chaotic turbulence in
fluid dynamics (Li et al., 2021). Teacher forcing (TF; Williams & Zipser (1989), Pearlmutter (1990),
1We make this point more formal in Appx. A.1.6.
2
Under review as a conference paper at ICLR 2022
Doya (1992), see also Goodfellow et al. (2016)) is one of the earliest techniques introduced to keep
RNN trajectories on track while training. The idea behind TF is to simply replace RNN states by
observations when available, thereby also effectively cutting off the gradients. TF essentially derives
from ideas in dynamical control theory, and adaptive schemes that increasingly hand over control to
the RNN throughout training have been devised (Abarbanel, 2013; Abarbanel et al., 2018; Bengio
et al., 2015). A related technique from the control theory literature is “multiple shooting” (Voss et al.,
2004): Here the whole observed time series is chopped into chunks, and for each chunk of trajectory
a new initial condition is estimated. Explicit constraints ensure continuity between the separate
trajectory bits during optimization. State space models and the Expectation-Maximization algorithm
became popular particularly in the 90es for uncovering the latent dynamics underlying a set of
time series observations (Ghahramani & Roweis, 1999), and remain an important tool until today
(Durstewitz, 2017; Koppe et al., 2019). Most recently, approaches based on variational inference
and the reparameterization trick (Kingma & Welling, 2014), like sequential variational autoencoders
(SVAE), gained in popularity for DS approximation (Hernandez et al., 2020). “Deterministic” RNNs
(i.e., with latent states not treated as random variables), like conventional LSTMs (Vlachas et al.,
2018), remain top choices for DS reconstruction, however.
Although connections between DS ideas and loss gradients have been drawn early on (Bengio et al.,
1994), so far only particular scenarios (like fixed point attractors) have been considered. Closest
to our work is recent work by Schmidt et al. (2021), where non-divergence of loss gradients is
established when RNNs converge to fixed points or cycles. However, this was done only for the
particular class of piecewise-linear RNNs (PLRNNs), more restrictive conditions for cycles were
imposed than assumed here, and - most importantly - the chaotic case on which we focus here was
not considered. Another recent study (Engelken et al., 2020) also points out the general connections
between Lyapunov exponents and loss gradients that we develop in sect. 3.1, but does not provide
any in-depth theoretical treatment, proofs, or empirical evaluation, as we do here. Thus, a systematic
theoretical framework that relates RNN dynamics more generally, and across a range of different
RNN architectures, to the behavior of its training gradients, is still lacking so far.
3	Theoretical analysis: Relation between RNN dynamics and
LOSS GRADIENTS
In our analysis, we will cover all major types of system dynamics (fixed points, cycles, chaos, and
quasi-periodicity), and mathematically investigate their implications for the loss gradients. We will
do this for all major classes of RNNs, including standard RNNs with largely arbitrary activation
function, LSTMs, GRUs, and PLRNNs. The next section will first develop and illustrate the basic
intuition behind the relations between RNN dynamics and loss gradients.
3.1	Preliminaries: RNN dynamics and loss gradients
Formally, all popular RNN architectures, including LSTMs, GRUs, or PLRNNs, are discrete time
DS, defined by a (first-order-Markovian) recursive prescription for the temporal evolution of the
latent states zt ∈ RM of the form
zt = Fθ(zt-1,st),
(1)
where st ∈ RN is the input at time t and θ are RNN parameters. For instance, for standard RNNs
we have Fθ(zt-1, st) = f W zt-1 + Bst + h , where f is an element-wise activation function
like tanh or a rectified linear unit (ReLU).
Assuming we start at some initial value z1 ∈ RM, and given a sequence of external inputs S = {st},
we can recursively rewrite eq. (1) as
zT = Fθ(Fθ(Fθ(...Fθ(z1, s2)...))) =: FθT -1 (z1, s2).
(2)
In DS theory, we characterize the long-term behavior of such sequences by its spectrum of Lyapunov
exponents. The Lyapunov exponents estimate the exponential growth rates in different local direc-
tions of the system’s state space, and the largest Lyapunov exponent gives the dominant exponential
behavior. Let us denote the system’s Jacobian at time t by
∂Fθ(zt-1, st)	∂zt
Jt := ----K--------= F—
(3)
∂zt-1	∂zt-1
3
Under review as a conference paper at ICLR 2022
For instance, for standard RNNs we would have Jt = W diag f0 W zt-1 + Bst + h , where
diag denotes a diagonal matrix for which the i-th diagonal entry is the derivative of f w.r.t. zi,t-1.
Then, the maximal Lyapunov exponent along an RNN trajectory {zι, z2,…，ZT,…} is defined as
λmaχ ：= lim ɪiθg
T→∞ T
T-2
Y JT-r
r=0
(4)
where k ∙ k denotes the spectral norm (or any subordinate norm) of a matrix. If λmaχ < 0 nearby
trajectories will ultimately converge to a fixed point or cycle, while for λmax > 0 (a necessary
condition for chaos) initially nearby trajectories will exponentially separate, i.e. we will have diver-
gence along one (or more) directions in state space. This accounts for the sensitive dependence on
initial conditions in chaotic systems.
Now let L(W, B, h) be some loss function employed for RNN training that decomposes in time
as L = PtT=1 Lt. Suppose we fancy BPTT as our training algorithm (similar derivations could be
performed for RTRL), we recursively develop the loss gradients w.r.t. some RNN parameter θ in
time (i.e., across layers of the RNN unrolled in time) as
∂L	T	∂Lt	∂Lt t	∂Lt	∂zt	∂+zr
—  7 -T  with -T 	7 ■————-—
∂θ	乙	∂θ	∂θ	乙	∂Zt	∂Zr ∂θ
t=1	r=1
(5)
and
∂Zt	∂Zt	∂Zt-1
∂Zr	∂Zt-1 ∂Zt-2
∂Zr+1
∂Zr
t-r-1
Y
k=0
∂Zt-k
∂Zt-k-1
t-r-1
Jt-k,
k=0
(6)
where ∂+ denotes the immediate derivative. Now observe that the behavior of the loss gradients
crucially depends on the product series of Jacobians in eqn. (6) : If the maximum absolute eigenval-
ues of the Jacobians Jt will, in the geometric mean, be larger than 1 (i.e.,
gradients will explode as T → ∞, while they will saturate if QrT=-02 JT-r
QrT=-02JT-r1/T>1),
1/T
< 1. Thus, the key
point to note is that the same terms that occur in the definition of the Lyapunov spectrum, eqn. (4),
resurface in the loss gradients, eqn. (5) & (6). This accounts for the tight links between system
dynamics and gradients.
3.2 Fixed points and cyclic dynamics
Let us start by considering the simplest types of dynamics that can occur in RNNs (or any discrete-
time DS): fixed points and cycles. In fact, by far most of the literature on global stability in RNNs
and on loss gradients focused on just fixed points (Chang et al., 2019; Kolter & Manek, 2019;
Erichson et al., 2021), with only few authors who recently started to also connect cyclic behavior
to loss gradients (Schmidt et al., 2021; Rusch & Mishra, 2021a). Recall that a fixed point of a
recursive map Zt = F(Zt-I) is defined as a point z* for which We have z* = F(z*).2 Likewise,
a k-cycle (k > 1) is a set of temporally consecutive periodic points Pk := {Zt1 , Zt2, . . . , Ztk} =
{Zt1 , F (Zt1 ), . . . , F k-1(Zt1)} that we obtain from recursive application of the map such that each of
the cyclic points Ztr ∈ Pk is a fixed point of the k times iterated map Fk (with k being the smallest
positive integer for which this holds). To simplify the subsequent treatment, we will collectively
refer to fixed points and cycles as k-cycles (k ≥ 1). Further recall that a fixed point or k-cycle is
called stable if the maximum absolute eigenvalue of the Jacobian evaluated at that point is smaller
than 1, neutrally stable if exactly 1, and unstable otherwise. Although the results we develop in this
and the following sections will hold more widely, we will restrict our attention to recursive maps Fθ
from the class of RNNs R = {standardRNN, LSTM, GRU, PLRNN} (see Appx. A.1 for details).
Based on the observations made in the previous sections we can state the following theorem that
links RNN dynamics and loss gradients:
Theorem 1. Consider an RNN Fθ ∈ R parameterized by θ, and assume that it converges to a stable
fixed point or k-cycle Γk (k ≥ 1) with BΓk as its basin of attraction3. Then for every Z1 ∈ BΓk (i)
2From here on we will suppress the explicit dependence on external inputs st notation-wise, see Remark 2.
3The basin of attraction is defined as the set of all points from which orbits converge to the resp. attractor.
4
Under review as a conference paper at ICLR 2022
the Jacobian ∂ZT exponentially vanishes as T → ∞; (ii) for Γk the tangent vectors dZT and thus
the gradient ofthe lossfunction, dLT, Willbeboundedfromabove, i.e. Willnotdivergefor T → ∞;
and (iii)for the PLRNN (27) both ∣∣ dZT ∣∣ and ∣∣ dLτ ∣∣ will remain bounded for every zι ∈ Brk as
T → ∞.
Proof. (i) Assume that Γk is a stable k-cycle (k ≥ 1) denoted by
γ k = {z1, z2, ∙ ∙ ∙ , ZT, ∙ ∙ ∙ } = {zt*k , zt*k — 1, ∙∙∙ , zt*k — (k — 1), zt*k , zt*k - 1, ∙∙∙ , zt*k -(k-1), ∙ ∙ ∙ }.
(7)
Then, the largest Lyapunov exponent of Γk is given by
λrk = t→m∞ t ln∣∣Jt* J-「
Jr∣∣ = jl→∞ j ln∣∣(γ Jt*k-s)
(8)
By assumption of stability of Γk We have λrk < 0 and also ρ( Qk-I Jt*k-s) < 1, which implies
k—1	j
1 …J2 = lim ( ɪɪ Jt*k-S)	= 0.
j→∞	s=0
(9)
Now suppose that OZ1 is an orbit of (1) converging to Γk, i.e. z1 ∈ BΓk. Since OZ1 and Γk have
the same largest Lyapunov exponent, we have
λOzι = Tim∞ Tln kJT JTT …J2 k = λrk <0,
(10)
and hence for z1 ∈ BΓk
lim
T→∞
∂ ZT
∂ Z1
lim k Jτ JT-1 ∙∙∙ J2 k = 0.
T→∞
(11)
(ii) & (iii) See Appx. A.2.1.
□
Remark 1. The result of Theorem 1 part (i) Will be generally true for any first-order-Markovian
recursive map (1), but the conclusions in part (ii) may hinge on its specific definition.
Remark 2. None ofthe results above and throughout sect. 3 require the dynamics to be autonomous,
the theory applies Whether there is external input or not. In fact, mathematically, non-autonomous
(externally forced) systems can alWays be reWritten as autonomous dynamical systems (Alligood
et al., 1996; Perko, 2001; Zhang et al., 2009), see Appx. A.1.1 for details.
The results above ensure that loss gradients will not diverge (explode) as T → ∞ in RNNs that are
“well-behaved” in the sense that they converge to a fixed point or cycle. This is a generalization of
the results given in Theorem 1 in Schmidt et al. (2021), where this was shown only a) for the specific
class of PLRNNs and b) for specific constraints imposed on the eigenvalue spectrum of the RNN’s
Jacobians which were relaxed in our theorem above.
While our treatment above is centered on the “exploding-gradients” case, various architectural mod-
ifications or regularization techniques can ensure that gradients do not vanish either, i.e. remain
bounded from below as well. This was established, for instance, in Schmidt et al. (2021) for
PLRNNs using ’manifold attractor regularization’. In Appx. A.2.1 we show that the results from
Theorem 2 from Schmidt et al. (2021) on doubly bounded (from below and above) loss gradients
can indeed be extended to the more general case covered by Theorem 1 above.
3.3 Chaotic dynamics
We will now consider the all-important chaotic case. Let F be a recursive map and OZ1 =
{z1, Z2, Z3, ∙…} be an orbit of F. The orbit is chaotic if (i) it is not asymptotically periodic and
(ii) has at least one positive Lyapunov exponent (Glendinning & Simpson, 2021; Meiss, 2007). If
the system’s invariant set is bounded, condition (ii) is considered a standard signature of chaos, as
in this case two nearby orbits separate exponentially fast, but at the same time their mutual sepa-
ration cannot go to infinity so that there are also folds. The following theorem states the sufficient
condition for exploding gradients:
5
Under review as a conference paper at ICLR 2022
Theorem 2. Suppose that an RNN Fθ ∈ R (parameterized by θ) has a chaotic attractor Γ* with
Br* as its basin of attraction. Then, for ^very orbit with zι ∈ B「*, (i) the Jacobians connecting
temporally distal states ZT and Zt ( T》t), ∂T, will exponentially explode for T → ∞, and (ii)
the tangent vector d∂T and so the gradients ofthe loss function, dLτ, will diverge as T → ∞.
Proof. Let the RNN Fθ ∈ R have a chaotic orbit denoted by Γ* = {z；, zg,…，ZT, ∙ ∙ ∙ }. Then,
denoting by JT the Jacobian of (1) at ZT ∈ Γ;, the largest Lyapunov exponent of Γ; is given by
λ = TimO TInIIJTJT-1 …J；|| .	(12)
Since Γ; is chaotic, so λ > 0. Hence, from (12), it is concluded that
lim
T →∞
-ι ∙ ∙
lim
T →∞
∂zT
∂zt
∞,
Tt.
(13)
Now, according to Oseledec’s multiplicative ergodic Theorem, nearly all the points in the basin of
attraction of Γ; have the same largest Lyapunov exponent λ. Thus, (13) holds for every zι ∈ Bγ* .
(ii) See Appx. A.2.2.
□
Remark 3. The first part of Theorem 2 holds for all first-order-Markovian recursive maps (1). Note
thatfor LSTMs, d∂-ZT (Z := (h, C)T) denotes thefull Jacobian ofboth hidden and cell states.
We collect some further mathematical results and remarks related to Theorem 2 in Appx. A.3.1.
Hence, the essential result is that for all popular RNNs R and activation functions, loss gradients
will inevitably diverge if the RNN latent states converge to a chaotic attractor.
3.4 Quasi-periodicity
Quasi-periodicity is a long-term behavior which occurs on a torus and, superficially, bears some
similarity to chaos in the sense that, strictly speaking, orbits are also aperiodic. That is, as T → ∞,
trajectories will never close up with themselves. Moreover, every trajectory becomes arbitrarily
close to any point on the torus, that is, it is dense. One important difference between quasi-periodic
and chaotic systems is, however, that in a quasi-periodic system, as time passes, two close initial
conditions are linearly diverging, while in a chaotic system the divergence is exponential.
Theorem 3. Assume that an RNN Fθ ∈ R (parameterized by θ) has a quasi-periodic attractor Γ
with BΓ as its basin of attraction. Then, for every Z1 ∈ BΓ
I ∂Z I
∀ 0 <e< 1 ∃ To > 1 s.t. ∀ T ≥ To =⇒ (1 — e)τ T < 产 < (1 + e)τ-1.	(14)
I ∂Z1 I
Proof. SeeAPPx. A.2.3.	口
According to Theorem 3, for every orbit converging to a quasi-periodic attractor, the Jacobians d∂-ZT-
may diverge or vanish as T → ∞, but this will not occur exponentially fast as T → ∞. Thus, even
for bounded non-chaotic RNNs we may sometimes stumble into the problem of diverging gradients.
Although this may be a less common scenario, we point out it may occur if we train RNNs on
real data from oscillatory systems with incommensurate frequencies, as for instance encountered in
electronic engineering.
In Appx. A.3.2 we have collected further mathematical results on the connection between RNN
dynamics and loss gradients that hold regardless of the RNN’s limiting behavior.
4	Empirical evaluation
Our theoretical results imply that chaotic time series pose a principle challenge for RNN training
that cannot easily be circumvented through specifically designed architectures, constraints, or regu-
larization criteria. If the underlying DS we aim to capture is chaotic, loss gradients propagated back
in time will inevitably explode. Here we will work out some implications for RNN training and
potential remedies empirically.
6
Under review as a conference paper at ICLR 2022
4.1	Training on systems with exploding gradients by sparse teacher forcing
To illustrate the connections between theory and RNN training, we revive the old idea of TF
(Williams & Zipser, 1989) as a mechanism for truncating error gradients while training. However,
we would like to do this such that important information about the system dynamics does not get
lost, for which Lyapunov theory offers some guidance. Specifically, we should not force the system
back onto the true trajectory all or most of the time (as in “classical TF”), but should effectively
“re-calibrate” it only at certain time points chosen wisely according to the system’s local divergence
rates. This procedure will be referred to as sparsely forced BPTT in the following.
Assume we want to train an RNN with hidden states zt ∈ RM and linear (or affine) output layer on
a time-series {xι, x2,…,XT} generated by a chaotic system.4 The linear output layer Xt = Bzt,
B ∈ RN×M, maps the RNN hidden states into the observation space. This allows us to modify the
original TF procedure by constructing a control series {Zι, Z2, ∙∙∙ , ZT} from the observations by
“inverting” the linear output mapping 5
Zt = (BTB)TBTXt.	B)
The idea is to supply this control signal only sparsely, separated by the learning interval τ between
consecutive forcings. Hence, defining T = {nτ + 1}n∈N0 as the set of all time points at which we
force the RNN onto the ‘true’ values, the RNN updates can be written as
IRNN(Zt)	ift ∈ T
RNN(Zt)	else
(16)
This forcing is applied after calculation of the loss, such that Lt = kXt - BZt k22 irrespective of
whether t is in T or not (and of course it is applied only during training, not at test time!). Replacing
hidden states Zt with their teacher-forced signals Zt simply breaks divergence between true and
predicted trajectories at time points t ∈ T, and also cuts off the Jacobians by breaking the temporal
contingency (for details see Appx. A.7). The learning interval τ hence controls how many time steps
are included in the gradient calculation and has to be chosen with care such as to balance the effects
of exploding gradients vs. those of loosing relevant time scales and long-term dependencies. While
it is general wisdom that an optimal batch size will facilitate training, the point here is thus much
more specific: Ideally τ should be chosen in accordance with the system’s Lyapunov spectrum, for
instance based on the predictability time (Bezruchko & Smirnov, 2010)
_ ln2
τpred = T
λmax
(17)
We emphasize that such a simple recipe for addressing the exploding gradient problem is based on
modifying the training routine, and is thus in principle applicable to any model architecture.6
4.2	Example 1: Lorenz system and externally forced Duffing oscillator in
CHAOTIC REGIME
Let us illustrate these ideas on two classical textbook examples of chaotic DS, the chaotic Lorenz
attractor as an autonomous system, and the chaotically forced Duffing oscillator as an example
with explicit external input (see Appx. A.4 for details). Trajectories were repeatedly drawn from
these systems, on which we trained a PLRNN, a vanilla RNN with tanh activation function, and a
LSTM by stochastic gradient descent (SGD) to minimize the MSE loss between predicted and actual
observations. As optimizer we used Adam (Kingma & Ba, 2015) from PyTorch (Paszke et al., 2017)
with a learning rate of 0.001. For all models, training proceeded solely by sparsely forced BPTT
and did not employ gradient clipping or any other technique that may interfere with optimal loss
truncation.
In nonlinear DS reconstruction, we are mainly interested in reproducing invariant properties of the
underlying system such as the attractor geometry (or topology; Takens (1981); Sauer et al. (1991)) or
4 Note that in DS reconstruction one usually considers the data as observations (unsupervised problem);
according to Remark 2, however, it is mainly a matter of the scientific question addressed whether we include
certain variables as explicit inputs or as observations.
5To ensure invertibility, one could add a regularizer λI to BT B in eqn. (15), as in ridge regression, but we
did not find this necessary in any of our examples.
6All code produced here is available at [placeholder].
7
Under review as a conference paper at ICLR 2022
the frequency composition (i.e., time-averaged properties), while measures like ahead-prediction er-
rors are less meaningful especially on chaotic time series (Wood, 2010; Koppe et al., 2019). Thus, in
evaluating training performance, here we follow Koppe et al. (2019) in using a Kullback-Leibler di-
vergence Dstsp to quantify the agreement between observed and generated probability distributions
across state-space to asses the overlap in attractor geometry (Appx. A.5). Moreover, we employ a
dimension-wise frequency correlation measure (PSC) to quantify the agreement of power-spectra of
the observed and generated time-series (Appx. A.5).
0j---------
τpred
100
200
Figure 1: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correlations
(P SC, higher = better) against learning interval τ for (a) the Lorenz and (b) the chaotically forced Duffing
oscillator. Continuous lines = sparsely forced BPTT. Dashed lines = classical BPTT with gradient clipping.
Prediction time indicated vertically in black.
Fig.	1 shows the dependence of the reconstruction quality on the learning interval τ for all RNN ar-
chitectures on (a) the Lorenz and (b) the externally forced Duffing system. Fig. 2 provides particular
examples of reconstructions for τ chosen too small, too large, or about right. For all models we find
a system-dependent range for the optimal learning interval that agrees well with the predictability
time defined in eqn. (17), where estimates for the maximal Lyapunov exponent were taken from the
literature (Rosenstein et al., 1993; Gilpin, 2021). As a reference, dashed lines represent the recon-
struction performance for all architectures when trained with classical BPTT and gradient clipping.
The training procedure was the same as for sparsely forced BPTT, except that instead of supplying a
control-signal, gradients were normalized to 1 prior to each parameter update. As evidenced by the
much worse performance, gradient clipping does not effectively address the EVGP, even for LSTMs.
As further shown in Fig. 9 in Appx. A.6.4, using the optimal window length τ but resetting the ini-
tial condition to zero (instead of its control value Zt) for each chunk equally destroys performance.
This suggests that neither mere gradient normalization nor simple windowing are sufficient, but will
wipe out essential information about the dynamics.
In Appx. A.6 We collect further results on the chaotic ROSSler attractor (Fig. 5), high-dimensional
Mackey-Glass equations (Fig. 7), and the Lorenz attractor with partial observations (Fig. 8).
Figure 2:	Lorenz attractor (blue) and example reconstructions by a LSTM (orange) trained With a learning
interval (a) chosen too small (τ = 5), (b) chosen optimally (τ = 30), and (c) chosen too large (τ = 200).
8
Under review as a conference paper at ICLR 2022
4.3 Example 2: Chaotic weather data
As for an empirical example, we trained all RNNs (vanilla RNN, PLRNN, LSTM) on a tempera-
ture time series recorded at the Weather Station at the Max Planck Institute for Biogeochemistry
in Jena, Germany. To expose the chaotic behavior and obtain a robust estimate of the maximal
Lyapunov exponent, trends and yearly cycles were removed, and nonlinear noise-reduction was per-
formed (Kantz et al. (1993); Appx. A.4). The maximal Lyapunov exponent was determined with
the TISEAN package (Hegger et al., 1999), as shown in Figure 3 (a). The value obtained is in close
agreement with the literature (Millan et al., 2010).
Figure 3 shows that also for these empirical data the optimal training interval τ agrees well with
the predictability time, eqn. (17), for all trained RNNs. Furthermore, as was the case for the DS
benchmarks, gradient clipping was not able to satisfactorily tackle the EVGP, even when paired
with architectures like LSTMs explicitly designed for alleviating this problem. Similar results are
reported for another real-world dataset, electroencephalogram (EEG) recordings, in Appx. A.11.
(a)
-1.5
0
50
100
m=6
—m=5
-	m=4
---Xmax =0.016
time-steps ≤
150
τpred	100	200
learning interval τ
Figure 3:	(a) The maximal Lyapunov exponent was determined as the slope of the average log-divergence of
nearest neighbors in embedding space (m = embedding dimension). (b) Reconstruction quality assessed by
attractor overlap (lower = better) and power-spectrum correlation (higher = better). Black vertical lines = τpred.
5 Discussion and conclusions
In this paper we proved that RNN dynamics and loss gradients are intimately related for all major
types of RNNs and activation functions. If the RNN is “well behaved” in the sense that its dynamics
converges to a fixed point or cycle, loss gradients will remain bounded, and established remedies
(Hochreiter & Schmidhuber, 1997; Schmidt et al., 2021) can be used to refrain them from vanishing.
However, if the dynamics are chaotic, gradients will always explode. This constitutes a principle
problem in RNN training that cannot easily be mastered through architectural design or gradient
clipping. It is furthermore a practically highly relevant one, as most time series we encounter in
nature, and many from man-made systems as well, are inherently chaotic. While we do not offer
a full solution to this problem here, we suggest it might be tackled in training by taking a system’s
local divergence rates as measured through the Lyapunov spectrum into account. Hence, rather than
conquering the EVGP by structural design or specific constraints or regularization terms, we recom-
mend to put the focus more on the training process itself. We illustrated this point empirically using
sparsely forced BPTT, a training technique that pulls trajectories back on track at times determined
by the maximal Lyapunov exponent. Doing so leads to optimal reconstruction results for a variety
of simulated and real-world benchmarks, regardless of the specific RNN architecture employed in
training. We stress that our goal above all was to provide a mathematically grounded perspective
on the problem, with the empirical section focused on elucidating the practical implications of the
theoretical results. Empirically, for instance, precise Lyapunov exponents may sometimes be hard to
obtain, although our empirical examples confirm that estimates based on log-divergence plots may
work sufficiently well.
Acknowledgments
Ethics statement
The current work deals with theoretical-mathematical aspects of RNN training and performs basic
research on limitations of training algorithms. As such it has no direct ethical implications. Since
many real world applications depend on accurate forward predictions of time series, however, this
9
Under review as a conference paper at ICLR 2022
paper may raise awareness for an important issue that will also be relevant in practical settings,
including sensitive domains like medical time series, weather forecasts, or traffic control.
Reproducibility statement
All theoretical results in this paper were carefully and thoroughly proven, with all proofs and de-
tailed derivations available in the Appendix. Likewise, we will make available all code used in the
empirical section in a way that will allow others to easily reproduce the results from this paper. This
means we will include everything, starting with the code for benchmark simulations and simulated
time series data used for evaluation, code for our model and training algorithms, up to the meta-
files that produce the actual figures in this work, on our lab github site. All of this will be clearly
documented.
References
Henry D. I. Abarbanel. Predicting the Future: Completing Models of Observed Complex Systems.
Understanding Complex Systems. Springer-Verlag, New York, 2013. ISBN 978-1-4614-7217-
9. doi: 10.1007/978-1-4614-7218-6. URL https://www.springer.com/gp/book/
9781461472179.
Henry D. I. Abarbanel, Paul J. Rozdeba, and Sasha Shirman. Machine Learning: Deepest Learning
as Statistical Data Assimilation Problems. Neural Computation, 30(8):2025-2055, August 2018.
ISSN 1530-888X. doi: 10.1162/neco_a_01094.
Kathleen T. Alligood, Tim D. Sauer, and James A. Yorke. Chaos: An Introduction to Dynamical
Systems. Springer, New York, NY, 1996.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
In Proceedings of the 33rd International Conference on International Conference on Machine
Learning - Volume 48, ICML’16, pp. 1120-1128. JMLR.org, 2016.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent Neural networks. In Proceedings of the 28th International Conference
on Neural Information Processing Systems - Volume 1, NIPS’15, pp. 1171-1179, Cambridge,
MA, USA, December 2015. MIT Press.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994. doi: 10.1109/
72.279181.
Boris P. Bezruchko and Dmitry A. Smirnov. Extracting Knowledge From Time Series: An In-
troduction to Nonlinear Empirical Modeling. Springer Series in Synergetics. Springer-Verlag,
Berlin Heidelberg, 2010. ISBN 978-3-642-12600-0. doi: 10.1007/978-3-642-12601-7. URL
https://www.springer.com/gp/book/9783642126000.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. AntisymmetricRNN: A dynamical system
view on recurrent neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=ryxepo0cFX.
KyUnghyUn Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103-111, Doha,
Qatar, October 2014a. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012.
URL https://aclanthology.org/W14-4012.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical ma-
chine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP
2014), 2014b.
Kenji Doya. Bifurcations in the learning of recurrent neural networks. In [Proceedings] 1992 IEEE
International Symposium on Circuits and Systems, volume 6, pp. 2777-2780 vol.6, May 1992.
doi: 10.1109/ISCAS.1992.230622.
10
Under review as a conference paper at ICLR 2022
Jorge Duarte, Cristina Januario, NUno Martins, and Josep Sardanyes. Quantifying chaos for
ecological stoichiometry. Chaos: An Interdisciplinary Journal of Nonlinear Science, 20(3):
033105, September 2010. ISSN 1054-1500. doi: 10.1063/1.3464327. URL https://aip.
scitation.org/doi/full/10.1063/1.3464327. Publisher: American Institute of
Physics.
Georg Duffing. Erzwungene SchWingUngen bei Veranderlicher eigenfrequenz, 1918.
Daniel Durstewitz. Advanced Data Analysis in Neuroscience: Integrating Statistical and Compu-
tational Models. Bernstein Series in Computational Neuroscience. Springer International Pub-
lishing, 2017. ISBN 978-3-319-59974-8. doi: 10.1007/978-3-319-59976-2. URL https:
//www.springer.com/de/book/9783319599748.
Daniel Durstewitz and Thomas Gabriel. Dynamical Basis of Irregular Spiking in NMDA-Driven
Prefrontal Cortex Neurons. Cerebral Cortex, 17(4):894-908, April 2007. ISSN 1460-2199,
1047-3211. doi: 10.1093/cercor/bhk044. URL https://academic.oup.com/cercor/
article-lookup/doi/10.1093/cercor/bhk044.
Rainer Engelken, Fred Wolf, and Larry F. Abbott. Lyapunov spectra of chaotic recurrent neural net-
works. arXiv:2006.02427 [nlin, q-bio], June 2020. URL http://arxiv.org/abs/2006.
02427. arXiv: 2006.02427.
N. Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W.
Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Repre-
sentations, 2021. URL https://openreview.net/forum?id=-N7PBXqOUJZ.
Marisa Faggini. Chaotic time series analysis in economics: Balance and perspectives. Chaos: An
Interdisciplinary Journal of Nonlinear Science, 24(4):042101, December 2014. ISSN 1054-1500.
doi: 10.1063/1.4903797. URL https://aip.scitation.org/doi/full/10.1063/
1.4903797. Publisher: American Institute of Physics.
Richard J Field and Laszlo Gyorgyi. Chaos in Chemistry and Biochemistry. WORLD SCIENTIFIC,
1993. doi: 10.1142/1706. URL https://www.worldscientific.com/doi/abs/10.
1142/1706.
Stefano Fusi, Wael F. Asaad, Earl K. Miller, and Xiao-Jing Wang. A neural circuit model of flexible
sensorimotor mapping: learning and forgetting on multiple timescales. Neuron, 54(2):319-333,
April 2007. ISSN 0896-6273. doi: 10.1016/j.neuron.2007.03.017.
Zoubin Ghahramani and Sam Roweis. Learning nonlinear dynamical systems using an em algo-
rithm. In M. Kearns, S. Solla, and D. Cohn (eds.), Advances in Neural Information Process-
ing Systems, volume 11. MIT Press, 1999. URL https://proceedings.neurips.cc/
paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf.
William Gilpin. Chaos as an interpretable benchmark for forecasting and data-driven modelling.
In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks
Track (Round 2), 2021. URL https://openreview.net/forum?id=enYjtbjYJrf.
L. Glass and M. C. Mackey. Pathological conditions resulting from instabilities in physiological
control systems. Annals of the New York Academy of Sciences, 316:214-235, 1979. ISSN 0077-
8923. doi: 10.1111/j.1749-6632.1979.tb29471.x.
Paul A. Glendinning and David J. W. Simpson. A constructive approach to robust chaos using
invariant manifolds and expanding cones. Discrete & Continuous Dynamical Systems, 41(7):
3367-3387, 2021.
A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov,
R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. Phys-
ioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for
complex physiologic signals.	Circulation, 101(23):e215-e220, 2000. Circulation
Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi:
10.1161/01.CIR.101.23.e215.
11
Under review as a conference paper at ICLR 2022
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Alex Graves, Greg Wayne, Malcolm Reynolds, and et al. Hybrid computing using a neural network
with dynamic external memory. Nature, 538:471—476, 2θ16. doi: 10.1038∕nature20101.
Rainer Hegger, Holger Kantz, and Thomas Schreiber. Practical implementation of nonlinear time
series methods: The TISEAN package. Chaos: An Interdisciplinary Journal of Nonlinear Sci-
ence, 9(2):413^35, June 1999. ISSN 1054-1500. doi: 10.1063/1.166424. URL https:
//aip.scitation.org/doi/citedby/10.1063/1.166424. Publisher: American
Institute of Physics.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal Recurrent Neural Networks with
Scaled Cayley Transform. In International Conference on Machine Learning, pp. 1969-1978.
PMLR, July 2018. URL http://proceedings.mlr.press/v80/helfrich18a.
html. ISSN: 2640-3498.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent Orthogonal Networks and Long-Memory
Tasks. In International Conference on Machine Learning, pp. 2034-2042. PMLR, June 2016.
URL http://proceedings.mlr.press/v48/henaff16.html. ISSN: 1938-7228.
Daniel Hernandez, Antonio Khalil Moretti, Ziqiang Wei, Shreya Saxena, John Cunningham, and
Liam Paninski. Nonlinear Evolution via Spatially-Dependent Linear Dynamics for Electrophysi-
ology and Calcium Data. arXiv preprint arXiv:1811.02459, 2020. URL http://arxiv.org/
abs/1811.02459.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen, 1991.
Sepp Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9
(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL
https://doi.org/10.1162/neco.1997.9.8.1735.
Katsuma Inoue, Soh Ohara, Yasuo Kuniyoshi, and Kohei Nakajima. Transient chaos in bert.
arXiv:2106.03181 [nlin], June 2021. URL http://arxiv.org/abs/2106.03181. arXiv:
2106.03181.
Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua
Bengio. Gated Orthogonal Recurrent Units: On Learning to Forget. Neural Computation, 31(4):
765-783,April2019. ISSN 1530-888X. doi: 10.1162/neco_a_01174.
Anil Kag and Venkatesh Saligrama. Time Adaptive Recurrent Neural Network. In 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15144-15153, Nashville,
TN, USA, June 2021a. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.01490.
URL https://ieeexplore.ieee.org/document/9578651/.
Anil Kag and Venkatesh Saligrama. Training recurrent neural networks via forward propagation
through time. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
5189-5200. PMLR, 18-24 Jul 2021b. URL https://proceedings.mlr.press/v139/
kag21a.html.
Anil Kag, Ziming Zhang, and Venkatesh Saligrama. Rnns incrementally evolving on an equilib-
rium manifold: A panacea for vanishing and exploding gradients? In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
HylpqA4FwS.
Leandre Kamdjeu Kengne, Justin R. Mboupda Pone, and Hilaire B. Fotsin. On the dynamics
of chaotic circuits based on memristive diode-bridge with variable symmetry: A case study.
Chaos, Solitons & Fractals, 145:110795, April 2021. ISSN 0960-0779. doi: 10.1016/j.chaos.
2021.110795. URL https://www.sciencedirect.com/science/article/pii/
S0960077921001478.
Holger Kantz and Thomas Schreiber. Nonlinear Time Series Analysis. Cambridge University Press,
2 edition, 2003. doi: 10.1017/CBO9780511755798.
12
Under review as a conference paper at ICLR 2022
Holger Kantz, Thomas Schreiber, Ingo Hoffmann, Thorsten Buzug, Gerd Pfister, Leci G. Flepp,
Josef Simonet, Remo Badii, and Ernst Brun. Nonlinear noise reduction: A case study on exper-
imental data. Physical Review E, 48(2):1529-1538, AUgUst 1993. doi: 10.1103∕PhysRevE.48.
1529. URL https://link.aps.org/doi/10.1103/PhysRevE.48.1529. Publisher:
American Physical Society.
Matthew B. Kennel, Reggie Brown, and Henry D. I. Abarbanel. Determining embedding dimension
for phase-space reconstrUction Using a geometrical constrUction. Phys. Rev. A, 45:3403-3411,
Mar 1992. doi: 10.1103/PhysRevA.45.3403. URL https://link.aps.org/doi/10.
1103/PhysRevA.45.3403.
Giancarlo Kerg, Kyle Goyette, Maximilian PUelma ToUzel, GaUthier Gidel, EUgene Vorontsov,
YoshUa Bengio, and GUillaUme Lajoie. Non-normal recUrrent neUral network (nnrnn):
learning long time dependencies while improving expressivity with transient dynamics.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volUme 32. CUrran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
9d7099d87947faa8d07a272dd6954b80-Paper.pdf.
MoUnira Kesmia, Soraya BoUghaba, and Sabir JacqUir. Control of continUoUs dynamical systems
modeling physiological states. Chaos, Solitons & Fractals, 136:109805, JUly 2020. ISSN 0960-
0779. doi: 10.1016/j.chaos.2020.109805. URL https://www.sciencedirect.com/
science/article/pii/S096007792030206X.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In YoshUa
Bengio and Yann LeCUn (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Diederik P. Kingma and Max Welling. AUto-Encoding Variational Bayes. In Proceedings of the 2nd
International Conference on Learning Representations, 2014. URL http://arxiv.org/
abs/1312.6114.
J. Zico Kolter and GaUrav Manek. Learning Stable Deep Dynamics Models. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volUme 32. CUrran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
0a4bbceda17a6253386bc9eb45240e25- Paper.pdf.
Georgia Koppe, Hazem ToUtoUnji, Peter Kirsch, Stefanie Lis, and Daniel DUrstewitz. Identifying
nonlinear dynamical systems via generative recUrrent neUral networks with applications to fMRI.
PLOS Computational Biology, 15(8):1-35, 08 2019. doi: 10.1371/joUrnal.pcbi.1007263. URL
https://doi.org/10.1371/journal.pcbi.1007263.
JacqUes Laskar and Philippe RobUtel. The chaotic obliqUity of the planets. Nature, 361(6413):
608-612, FebrUary 1993. ISSN 1476-4687. doi: 10.1038/361608a0. URL https://www.
nature.com/articles/361608a0.
ShUai Li, Wanqing Li, Chris Cook, Ce ZhU, and Yanbo Gao. Independently recUrrent neUral network
(indrnn): BUilding a longer and deeper rnn. In 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 5457-5466, 2018. doi: 10.1109/CVPR.2018.00572.
Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, BUrigede liU, KaUshik Bhat-
tacharya, Andrew StUart, and Anima AnandkUmar. FoUrier neUral operator for parametric partial
differential eqUations. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=c8P9NQVtmnO.
Zachary C. Lipton, John Berkowitz, and Charles Elkan. A critical review of recUrrent neUral net-
works for seqUence learning. arXiv:1506.00019 [cs], October 2015. URL http://arxiv.
org/abs/1506.00019. arXiv: 1506.00019.
13
Under review as a conference paper at ICLR 2022
Edward N. Lorenz. Deterministic nonperiodic flow. Journal of the Atmospheric Sciences, 20(2):
130-141, March 1963. ISSN 0022-4928, 1520-0469. doi: 10.1175/1520-0469(1963)020(0130:
DNFi2.0.CO;2. URL https://journals.ametsoc.org/view/journals/atsc/
20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml. Publisher: American Me-
teorological Society Section: Journal of the Atmospheric Sciences.
Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Re-
verse engineering recurrent networks for sentiment classification reveals line attractor dynam-
ics. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
d921c3c762b1522c475ac8fc0811bb0f- Paper.pdf.
Sylvain Mangiarotti, Matthieu Peyre, Yixiao Zhang, Maciej Huc, Friederike Roger, and Yvonne
Kerr. Chaos theory applied to the outbreak of COVID-19: an ancillary approach to decision
making in pandemic context. Epidemiology and Infection, 148:e95, May 2020. ISSN 0950-
2688. doi: 10.1017/S0950268820000990. URL https://www.ncbi.nlm.nih.gov/
pmc/articles/PMC7231667/.
Robert M. May. Chaos and the dynamics of biological populations. Proceedings of the Royal
Society of London. Series A, Mathematical and Physical Sciences, 413(1844):27-44, 1987. ISSN
00804630. URL http://www.jstor.org/stable/2398225.
James D. Meiss. Differential Dynamical Systems. Society for Industrial and Applied Mathematics,
2007.
Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, and Alex Graves.
Practical real time recurrent learning with a sparse approximation. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
q3KSThy2GwB.
HUmberto Millan, Behzad Ghanbarian-Alavijeh, and Ivan Garcia-Fornaris. Nonlinear dynamics
of mean daily temperature and dewpoint time series at Babolsar, Iran, 1961-2005. Atmo-
spheric Research, 98(1):89-101, October 2010. ISSN 0169-8095. doi: 10.1016/j.atmosres.
2010.06.001. URL https://www.sciencedirect.com/science/article/pii/
S0169809510001419.
Zahra Monfared and Daniel DUrstewitz. Existence of n-cycles and border-collision bi-
fUrcations in piecewise-linear continUoUs maps with applications to recUrrent neUral net-
works. Nonlinear Dynamics, 101(2):1037-1052, JUly 2020a. ISSN 0924-090X, 1573-
269X. doi: 10.1007/s11071-020-05841-x. URL http://link.springer.com/10.
1007/s11071-020-05841-x.
Zahra Monfared and Daniel DUrstewitz. Transformation of ReLU-based recUrrent neUral networks
from discrete-time to continUoUs-time. In International Conference on Machine Learning, pp.
6999-7009. PMLR, November 2020b. URL http://proceedings.mlr.press/v119/
monfared20a.html. ISSN: 2640-3498.
Lars Folke Olsen and Hans Degn. Chaos in an enzyme reaction. Nature, 267(5607):177-178,
May 1977. ISSN 1476-4687. doi: 10.1038/267177a0. URL https://www.nature.com/
articles/267177a0.
Emin Orhan and Xaq Pitkow. Improved memory in recUrrent neUral networks with seqUential
non-normal dynamics. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=ryx1wRNFvB.
Razvan PascanU, Tomas Mikolov, and YoshUa Bengio. On the difficUlty of training recUrrent neUral
networks. In Proceedings of the 30th International Conference on International Conference on
Machine Learning - Volume 28, ICML’13, pp. III-1310-III-1318. JMLR.org, 2013.
Adam Paszke, Sam Gross, SoUmith Chintala, Gregory Chanan, Edward Yang, Zach DeVito, Zeming
Lin, Alban Desmaison, LUca Antiga, and Adam Lerer. AUtomatic differentiation in pytorch. 2017.
URL https://openreview.net/forum?id=BJJsrmfCZ.
14
Under review as a conference paper at ICLR 2022
Barak Pearlmutter. Dynamic recurrent neural networks, 1990. URL https://kilthub.
cmu.edu/articles/journal_contribution/Dynamic_recurrent_neural_
networks/6605018/1.
Lawrence Perko. Differential Equations and Dynamical Systems, volume 7. Springer, New York,
NY, 2001.
Michael T. Rosenstein, James J. Collins, and Carlo J. De Luca. A practical method for calculating
largest Lyapunov exponents from small data sets. Physica D: Nonlinear Phenomena, 65(1):
117-134, May 1993. ISSN 0167-2789. doi: 10.1016∕0167-2789(93)90009-P. URL https:
//www.sciencedirect.com/science/article/pii/016727899390009P.
T. Konstantin Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (coRNN):
An accurate and (gradient) stable architecture for learning long time dependencies. In Interna-
tional Conference on Learning Representations, 2021a. URL https://openreview.net/
forum?id=F3s69XzWOia.
T. Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long
time dependencies. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
9168-9178. PMLR, 18-24 Jul 2021b. URL https://proceedings.mlr.press/v139/
rusch21a.html.
Eleonora Russo and Daniel Durstewitz. Cell assemblies at multiple time scales with arbitrary lag
constellations. eLife, 6:e19428, January 2017. ISSN 2050-084X. doi: 10.7554/eLife.19428.
URL https://doi.org/10.7554/eLife.19428. Publisher: eLife Sciences Publica-
tions, Ltd.
Otto E. Rossler. An equation for continuous chaos. Physics Letters A, 57(5):397-398, July
1976. ISSN 0375-9601. doi: 10.1016/0375-9601(76)90101-8. URL https://www.
sciencedirect.com/science/article/pii/0375960176901018.
Salvador Sabanal and Masahiro Nakagawa. The fractal properties of vocal sounds and their
application in the speech recognition model. Chaos, Solitons & Fractals, 7(11):1825-1843,
November 1996. ISSN 0960-0779. doi: 10.1016/S0960-0779(96)00043-4. URL https:
//www.sciencedirect.com/science/article/pii/S0960077996000434.
Tim Sauer, James A. Yorke, and Martin Casdagli. Embedology. Journal of Statistical Physics, 65
(3):579-616, November 1991. ISSN 1572-9613. doi: 10.1007/BF01053745. URL https:
//doi.org/10.1007/BF01053745.
Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R.
Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. IEEE trans-
actions on bio-medical engineering, 51(6):1034-1043, June 2004. ISSN 0018-9294. doi:
10.1109/TBME.2004.827072.
Dominik Schmidt, Georgia Koppe, Zahra Monfared, Max Beutelspacher, and Daniel Durste-
witz. Identifying nonlinear dynamical systems with multiple time scales and long-range de-
pendencies. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=_XYzwxPIQu6.
Bellie Sivakumar. Chaos theory in geophysics: past, present and future. Chaos,
Solitons & Fractals, 19(2):441-462, January 2004. ISSN 0960-0779. doi: 10.
1016/S0960-0779(03)00055-9. URL https://www.sciencedirect.com/science/
article/pii/S0960077903000559.
Floris Takens. Detecting strange attractors in turbulence. In David Rand and Lai-Sang Young
(eds.), Dynamical Systems and Turbulence, Warwick 1980, pp. 366-381, Berlin, Heidelberg,
1981. Springer Berlin Heidelberg. ISBN 978-3-540-38945-3.
15
Under review as a conference paper at ICLR 2022
Robert Tchitnga, B. Anicet Mezatio, Theophile Fonzin Fozin, Romanic Kengne, Patrick H.
Louodop Fotso, and Anaclet Fomethe. A novel hyperchaotic three-component oscillator oper-
ating at high frequency. Chaos, Solitons & Fractals, 118:166-180, January 2019. ISSN 0960-
0779. doi: 10.1016/j.chaos.2018.11.015. URL https://www.sciencedirect.com/
science/article/pii/S0960077918303047.
David J. Thomson. Time series analysis of Holocene climate data. Philosophical Transactions of
the Royal Society of London. Series A, Mathematical and Physical Sciences, 330(1615):601-616,
April 1990. doi: 10.1098/rsta.1990.0041. URL https://royalsocietypublishing.
org/doi/abs/10.1098/rsta.1990.0041. Publisher: Royal Society.
Trieu H. Trinh, Andrew M. Dai, Minh-Thang Luong, and Quoc V. Le. Learning longer-term de-
pendencies in rnns with auxiliary losses, 2018. URL https://openreview.net/forum?
id=Hy9xDwyPM.
Adam P. Trischler and Gabriele M.T. D’Eleuterio. Synthesis of recurrent neural networks for dy-
namical system simulation. Neural Networks, 80:67-78, 2016. ISSN 08936080. doi: 10.1016/
j.neunet.2016.04.001. URL https://linkinghub.elsevier.com/retrieve/pii/
S0893608016300314.
Peter Turchin and Andrew D. Taylor. Complex dynamics in ecological time series. Ecology, 73(1):
289-305, 1992. ISSN 1939-9170. doi: 10.2307/1938740.
Eli Tziperman, Harvey Scher, Stephen E. Zebiak, and Mark A. Cane. Controlling spatiotemporal
Chaos in a realistic El Nino prediction model. Physical Review Letters, 79(6):1034-1037, AU-
gust 1997. doi: 10.1103/PhysRevLett.79.1034. URL https://link.aps.org/doi/10.
1103/PhysRevLett.79.1034. Publisher: American Physical Society.
Carl van Vreeswijk and Haim Sompolinsky. Chaos in neuronal networks with balanced excita-
tory and inhibitory activity. Science, 274(5293):1724-1726, December 1996. ISSN 0036-8075,
1095-9203. doi: 10.1126/science.274.5293.1724. URL https://science.sciencemag.
org/content/274/5293/1724. Publisher: American Association for the Advancement of
Science Section: Reports.
Pantelis R. Vlachas, Wonmin Byeon, Zhong Y. Wan, Themistoklis P. Sapsis, and Petros Koumout-
sakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory
networks. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
474(2213):20170844, 2018. ISSN 1364-5021, 1471-2946. doi: 10.1098/rspa.2017.0844. URL
https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0844.
Pantelis R. Vlachas, Georgios Arampatzis, Caroline Uhler, and Petros Koumoutsakos. Learning the
Effective Dynamics of Complex Multiscale Systems. arXiv:2006.13431 [nlin, physics:physics],
July 2020. URL http://arxiv.org/abs/2006.13431. arXiv: 2006.13431.
Henning U. Voss, Jens Timmer, and JUrgen Kurths.	Nonlinear dynamical system identi-
fication from uncertain and indirect measurements.	International Journal of Bifurcation
and Chaos, 14(06):1905-1933, June 2004. ISSN 0218-1274, 1793-6551. doi: 10.1142/
S0218127404010345. URL https://www.worldscientific.com/doi/abs/10.
1142/S0218127404010345.
Joseph H. M. Wedderburn. Lectures on Matrices. New York: American mathematical society, New
York : Dover Publications, 1964.
Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural Computation, 1(2):270-280, June 1989. ISSN 0899-7667, 1530-888X.
doi: 10.1162/neco.1989.1.2.270. URL https://direct.mit.edu/neco/article/1/
2/270-280/5490.
Simon N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature,
466(7310), August 2010. ISSN 1476-4687. doi: 10.1038/nature09319. URL https://www.
nature.com/articles/nature09319.
Huaguang Zhang, Derong Liu, and Zhiliang Wang. Controlling Chaos. Springer, London, 2009.
16
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Theorems: Preliminaries
A.1.1 Transforming non-autonomous into autonomous discrete-time DS
Following (Zhang et al., 2009), and based on similar reasoning as for continuous time (ODE-based)
DS (Alligood et al., 1996; Perko, 2001), let us consider the non-autonomous discrete-time DS
xt+1 = F(xt, t),	x ∈ Rn.	(18)
Defining zt = (xt, t)T and G(zt) = (F(xt, t), t + 1)T , system (18) can be rewritten as the
autonomous system
zt+1 = G(zt),	z ∈ Rn+1.	(19)
Hence, in all our theoretical treatment we can confine our attention to systems of the form eqn. 18.
A.1.2 RNN derivatives
Considering the loss function L	PtT=1 Lt ofan RNN Fθ ∈ R parameterized by θ, We have	
	∂L	T ∂Lt
	——= ∂θ	= ∑iθ-,	QO) t=1
		
Where	∂Lt	-四吆	QI) 一∂Zt ∂θ .	()
	——— = ∂θ	
The tangent vector dZT has the form
dzT _	d+zτ	,	TX2	( Y J ʌ	d+zτ-t	(22、
~θθ~ =	~~∂θ~+	T	(JTjτ-r	~~∂θ~^	(22)
t=1	r=0
where ∂+ denotes the immediate partial derivative. Since for an RNN Fθ ∈ R the activation
function is element-wise, with θ the m-th element of a parameter vector θ (or belonging to the m-th
row of a parameter matrix θ), we have
d⅛T =(0 …0 ⅛t 0 …0)T.	(23)
For instance, let θ = W be a weight matrix, then
∂L
∂W
∂L
∂wιι
∂L
∂W21
∂L
∂W12
∂L
∂W22
∂L ∖
∂W1M
∂L
∂W2M
(24)
∖	∂L
∖∂WM1
∂L
∂WM2
∂L
dwMM
In this case, for the standard RNN we have
∂+Zτ
∂wmk
(0
0	zk,τ -1 ξmk (zτ-1) 0
0)T = 1(m,k) ξmk (zτ -1 ) zτ-1 ,	(25)
Where ξmk (ZT-I) = fW m,k (PM=I Wmj zj,T-1 + PM=I bmj sj,T + hm) , and fW m,k Stands for
the derivative of f with respect to wm,k.
Therefore, for standard RNNs, (22) becomes
∂zt
∂wmk
T-2	t-1
1(m,k) ξmk(zT-1) zT-1 + ∑ π JT-r 1(m,k) ξmk (zT -t-1 ) zT -t-1 .
t=1 r=0
(26)
17
Under review as a conference paper at ICLR 2022
A.1.3 Piecewise-linear RNN (PLRNN)
The PLRNN has the generic form (Koppe et al., 2019; Schmidt et al., 2021)
zt = F(zt-1) = A zt-1 + Wφ(zt-1) + Cst + h + εt,	(27)
where φ(zt-1) = max(zt-1, 0) is the element-wise rectified linear unit (ReLU) function, zt ∈ RM
is the neural state vector, A ∈ RM×M is a diagonal matrix of auto-regression weights, W ∈ RM×M
is a matrix of connection weights, h ∈ RM is the bias vector, st ∈ RK the external input weighted
byC ∈ RM×K,andεt 〜 N(0, Σ) a Gaussian noise term with diagonal covariance matrix Σ.
Equation (27) can be rewritten as
Zt = (A + WDΩ(t-I))Zt-1 + Cst + h + εt =: WΩ(t-1) zt-1 + Cst + h + εt,	(28)
where Dn(t)：= diag(dn(t)) is a diagonal matrix with d&(t)：= (d1,d2,…，&m) an indicator
vector such that dm(zm,t) =: dm = 1 whenever zm,t > 0, and zeros otherwise.
For the PLRNN (28) we have
Jt = /Zt = WΩ(t-1),	(29)
∂Zt-1
and∣∣WΩ(t-i)∣∣ ≤ ∣∣A∣∣ + ∣Wk∙
Furthermore, the derivatives (22) for the PLRNN (28) are
j-1
ɪɪ WΩ(T-i) ) 1(m,k)DΩ(T-j) zT-j.	(3O)
i=1
T-1
1(m,k)DΩ(T-1) ZT-1 + Σ
j=2
∂ ZT
dwmk
A.1.4 Long Short-Term Memory (LSTM)
The LSTM is defined by the equations
it = σ Wiist + Whiht-1 + bi
ft = σ(Wif st + Whf ht-1 + bf)
gt = tanh (Wigst + Whght-I + bg)
ot = σ(Wio st + Whoht-1 + bo)
ct = ft	ct-1 + it	gt
ht =	ot tanh	(ct)	(31)
where {st }	is the input sequence,	W	denotes	weight matrices,	b bias terms, it , ft, gt , ot	demon-
strate the input, forget, cell, and output gates, ht and ct are the hidden and cell states at time t
respectively, σ is the sigmoid activation function, and represents the element-wise (Hadamard)
product (see (Hochreiter & Schmidhuber, 1997; Graves et al., 2016; Vlachas et al., 2018) for further
information on LSTMs).
Defining Zt := (ht, ct)T, the LSTM (31) can be represented as the first-order recursive map
Zt = Fθ(Zt-1)
ot	tanh (ft	ct-1 + it
ft	ct-1 + it	gt
(32)
18
Under review as a conference paper at ICLR 2022
The term 磊 in (20) for some LSTM parameter θ can be written as
∂Lt	∂Lt ∂ht ∂Lt ∂zt ∂zr
———= >  ------:--:---:--:——
∂θ	∂h ∂ht ∂Zt ∂Zt ∂Zr ∂θ
r=1
(33)
A necessary condition for LSTMs to have a chaotic orbit is given by:
Proposition 1. Let the LSTM given by (31) have a chaotic attractor Γ* with Br* as its basin of
attraction. Thenfor every z1 = (h1, CI)T ∈ Br*
∂hτ
∂cι
∂cτ
∂cι
> 1.
(34)
Proof. The Jacobian matrix of (32) for t > 1 can be written in the block form
(∂ht
∂ht-i
dct
∂ht-ι
∂ht
∂ct-ι
∂ct
∂ct-ι
(35)
Further, due to the chain rule, we have
Jt Jt-I
∂ht	∂ht-ι	.	∂ht	∂ct-ι
∂ht-ι	∂ht-2	十	∂ct-ι	∂ht-2
∂ct	dht-1	I	∂ct	dct-1
∂ht-ι	∂ht-2	十	∂ct-ι	∂ht-2
∂ht	∂ht-ι	I	∂ht	∂ct-ι	,
∂ht-ι	∂ct-2	十	∂ct-ι	∂Ct-2
∂ct	dht-1	I	∂ct	dct-1
∂ht-1	∂ct-2	十	∂ct-1	∂ct-2.
∂ht ∖
∂Ct-2 ∖
∂ct	J
∂Ct-2 )
(36)
and by induction we obtain
∂ Zt
∂ Zi
Jt Jt-1 Jt-2 …J2
∂ht
∂h1
∂ct
∂h1
∂ht
∂c1
∂ct
∂C1
(37)
Now assume that (32) has a chaotic orbit given by
Γ* = {z；,对…、ZT, ∙∙∙}.	(38)
According to (37), the largest Lyapunov exponent of Γ; is given by
λr*
Tim∞ T InllJT JT-1 ∙ ∙
lim - ln
t→∞ T
舞J
耨川
Since Γ; is chaotic, so λr* > 0, which gives
lim
t →∞
(39)
Based on Oseledec,s multiplicative ergodic Theorem, (39) holds for every Zi ∈ Br*. This com-
pletes the proof.	口
19
Under review as a conference paper at ICLR 2022
A.1.5 Gated Recurrent Unit (GRU)
A GRU network is defined by the equations
Zt = σ(Wz st + Uz ht-1 + bz)
rt = σ Wr st + Urht-1 + br
ht = (1 — Zt)	Θ tanh	(Wh st +	Uh(r	Θ	ht-i)	+	bh)	+ Zt Θ	ht-i,	(40)
where rt represents the reset gate, zt the update gate, st and ht denote the inputs and the hidden state
respectively, Wz,Wr,Wh ∈ RM×N and Uz,Ur,Uh ∈ RM×M are weight matrices, bz,br,bh ∈
RM are bias vectors, and σ is the element-wise logistic sigmoid function (for more details about
GRUs see Cho et al. (2014a)).
A.1.6 Unitary evolution RNN (uRNN)
The uRNN, proposed in (Arjovsky et al., 2016), is defined as the nonlinear DS
Zt = σb(Wzt-i + Vst),	(41)
for which W ∈ U(M) is an unitary matrix, V ∈ CM×N, b ∈ RM is the bias parameter, st is the
real- or complex-valued input of dimension N , and
[σb(z)]i = KodReLu(z)]i =	+ 帅岗	+	“
0	if |zi | + bi < 0
(42)
Proposition 2. The uRNN given by (41) cannot have any chaotic orbit.
Proof. For any arbitrary orbit Oz1 of (41) we have
IIJT JT-1 …J2k
where Dt = diag σb0 W Zt-1
is concluded that
T-2
Y DT-kWT
k=0
T-2
Y DT-kWT
k=0
(43)
Since W is unitary and so a norm preserving matrix, it
T-2	T-2
≤ Y DT-kWT	= Y IDT-kI = 1,
k=0	k=0
(44)
which implies
λmax = Tl→∞ T ln kJT JT-1 …J2k ≤ 0.
(45)
This rules out the existence of chaos (since λmax > 0 is a necessary condition for Oz1 to be
chaotic).	□
Note that, more generally, any RNN which is constrained such as to exhibit global convergence
to a fixed point or cycle, by definition must have a maximum Lyapunov exponent λmax ≤ 0 (in
accordance with Theorem 1), hence cannot exhibit chaotic behavior by definition.
20
Under review as a conference paper at ICLR 2022
A.2 Theorems： Proofs
A.2.1 Proof of theorem 1, parts (ii) & (iii)
Proof. (ii) If J is the Jordan normal form of Qk-I Jt^k-s, then Qk-CJ Jt*j = PJP T
where
/Jm1 (λ1)	0	0	…	0	∖
0	jm2 (尢)0	…	0
J =	：	".	：	,	(46)
.	♦♦♦	♦♦♦	.
0	…	0	JmP-I (λp-1)	0
∖	0	........ 0	Jmp (λp)/
and m is the algebraic multiplicity of each eigenvalue λi. Since P(Qk-I Jt*^-s) < 1, so the
eigenvalue λ% associated with each Jordan block satisfies ∣λ∕ < 1 (i = 1, ∙∙∙ ,p). Moreover, every
mi × mi Jordan block has the form
Accordingly
	/λi	1	0	…	0 ∖
	0	λi	1	…	0
Jmi (λi)=	. . .	.	..	..	. .	.	.	.
	0	0	∙ ∙ ∙	Xi	1
	0	0	...	0	Xi)
k-1	j	
UnJt*% -		=IlPJjPTIl ≤ p I I Jj I I
s = 0
(47)
(48)
in which p = ∣∣P IIllP-11∣. Furthermore, for j ∈ N, Jj is a block diagonal matrix of the form
Jj =	(Jm ι (Xi)	0 0	Jm 2 W . . .	∙∙∙ 0	... 0	∙∙∙	0 0 . . . 0	Jmj p- ...	…	0	∖ ...	0 . . ...	. 1(Xp-1)	0 0	Jmj p (Xp)	,	(49)	
in which every mi × mi Jm i (Xi)=	Jordan block has the form (Xj	(1) XjT	(2) Xj-2 0	Xj	(1) Xj-i ..	..	.. ..	.		∙∙∙	(mj-i) Xj-mi + 1∖ (j ) ∖j-mi+2 ∙ ∙ ∙	mi-2 Xi ..	.. ..		.	(50)
In addition, for every blo	00 00 ck Jmj (Xi), we have	... ...	Xj	(j) Xj- 0	Xij		
IlJmi(Xi)Ii ≤ √mi iiJjji(Xi)Iι∞ = √miXI(Jmi(%))/
q=1
mi /	∙	∖	/	mi /	∙	∖
=√m X Iql Ji—+1 = ∣%j √m (∣λi∣1-miX Iql Jiw」
mi
≤ ∣λi∣j jmi √m ∣λi∣1-m<E ∣λi∣mi-q	=： ∣λi∣jjmiNλi∙	(51)
×	q=1	)
21
Under review as a conference paper at ICLR 2022
Moreover, for any 1 < r < 册,there exists some li such that j mi < rj for j ≥ li. This means
for j ≥ li
Jm,(")∣∣ ≤ Nιj%λiij,
(52)
SUChthat Iri λ/ = ri∣λi∣ < 1.
Besides, for Jj = Jm 1 (λ1)㊉ Jm?62 ㊉•一㊉ JmQP)
JjU = ImaxJm，(ʌi)ll =： Jm(刈卜
1 "≤ i "≤ P
Hence, from (48), (52) and (53), it is deduced that for j ≥ l
k-1	j
(∏ Jt*k-s)l ≤ pNλ ∣rλ∣j =: Prj,
(53)
(54)
in which r = ∣rλ∣ < 1.
Furthermore, let for Γfc
臀{JTk}
∩≤max-1{ kJt*k
rn,
∂+Zt*k-s
max
T ≥1
∂+Zt
∂θ
max
∩≤s≤k-1
∂θ
∣o
m≥x{ kzT k }
max
∩≤s≤k-1
Nt* k
q.
(55)
Hence, defining z∩ = 0, for this k-cycle
∂ ZT
∂+ Zt
∂θ
∂θ
T-2 /t-1	∖
X(Π JT-r)
t=1 r=∩
∂+Zt-t
∂θ
∂+Zt
∂θ
T-1 ∕t-1	、
X(Π JT-r)
t=1 r=∩
∂+Zt-t
∂θ
T-1
t-1
qξ
/ T —1 t-1
1 + X ∏ JT-
∖ t= 1	r = ∩
(56)
t=1
r=∩
≤
+
+
r
On the other hand, for T = kj , from (54) and (55) we have
T-1
t-1
E ∏JT-r
kj-1
t-1
k-1
t-1
t=1
r=∩
t=1
r=∩
Jkj-r
t=1
r=∩
Jkj-r
2k-1
+ X
t=k
t-1
∏ Jj-r
r=∩
3k-1
t-1
+	Jkj
(j-1)k-1
t=2k
k-1
t=1
r=∩
t-1
r=∩
-r
Jkj-r
+ ∙∙∙+ E
t=(j-2)k
j ik-1
t-1
t-1
Π Jj-r
r=∩
+	Jkj
-r
kj-1
t=(j-1)k
t-1
Π Jj-r
r=∩
i=2 t=(i-1)k
r=∩
≤ (m + m2 + ∙∙∙ + mk-1) + P (1 + m + n2
+ ∙∙∙ + mk-1) ri-1
i=2
(57)
+
j
22
Under review as a conference paper at ICLR 2022
Thus, considering (τm + m2 +----+ mk-1) = M, it is deduced that
lim dzT = lim dzkj ≤ q ξ(l + M + pr(1 + M)) = M < ∞,	(58)
T→∞ ∂θ	j→∞	∂θ	1 - r
which, by (21), implies d∂LT will be bounded for T → ∞.
(iii)	Consider the PLRNN given by (27), where for simplicity we ignore the external inputs and
noise terms. Let {zt1 , zt2, zt3, . . .} be an orbit which converges to Γk. Hence
lim d(ztn , Γk) = 0,	(59)
n→∞
which implies there exists a neighborhood U ofΓk and k sub-sequences {ztkm}m∞=1, {ztkm+1}m∞=1,
•…，{ztkm+(k-i) }∞=ι of the sequence {ztn }∞=ι such that all these sub-sequences belong to U and
a)	ztkm +S = Fk (Ztk(m-1) +S), S = 0, 1, 2,…，k - 1,
b)	lim Ztkm+S = Zfk-s, S = 0,1, 2,…,k - 1,
m→∞
c)	for every Ztn ∈ U there is some S ∈ {0,1, 2,…,k — 1} such that Ztn ∈ {ztkm+s }∞=ι.
In this case, for every Ztn ∈ U with Ztn ∈ {Ztkm+s }∞=ι, there exists some n ∈ N such that
Zt = N*- and lim zu^ɪ = Zt*k_.. Therefore, continuity of F results in
t,n	^κτ+-∖-s	~	t,kn, + s	t S	,	J
n-∞
Jim F (Ztkq = F (Zt*k-s)
n→∞
and so by (28)
nl→∞ (WΩ(tkft+s) Ztk^+s + h) = WQ(t*k-s)Zt*k-s + h,
(60)
(61)
which implies
n→∞ WQ(tkn+s)Zt^+s = WQ(t*k-s)Zt*k-s.	(62)
Assuming lim W^(tk--s)= L, since (62) holds for every Zt*k-s, substituting Zt*k-S = eτ =
n→∞	n+s,
(1,0, ∙∙∙ , 0)t in (62), we can prove that the first column of L equals the first column of WΩ(t*k-s).
Performing the same procedure for Zt*k-s = eτ, i = 2,3, ∙∙∙ ,M, yields
nl→∞ WΩ(tkn+s) = WΩ(t*k-s).	(63)
According to (59), U contains an infinite number of terms of the sequence {Ztn}n∞=1, i.e.
∃N ∈ N S.t. n ≥ N =⇒ Ztn ∈ U.	(64)
Suppose that Ztn ∈ U for some n ≥ N. Thus, there exists some S ∈ {0,1,2,…，k — 1} such that
Ztn ∈ {Ztkm-s }∞=ι. Without loss of generality let S = 0. Hence, there is some n ∈ N such that
23
Under review as a conference paper at ICLR 2022
Ztn =	Ztk汽 and lim Ztk汽=zv*k. In this case, moving forward in time gives n-∞		
Ztn =	Ztk 汽(Zt n ∈ {Ztkm }m=1),	lim Zt	= ~	IJkn■ n-∞	Zt*k ,
Ztn+1	=Ztk汽+1 (Ztn+1 ∈ {Ztkm+1 ?m=1)，	lim Zt ~	tkn + 1 n-∞	=Zt*k-1,
ztn+2	=Ztk 汽+2	(Ztn+2 ∈ {Ztkm+2}m=1)，	lim Zt ~	tk n + 2 n-∞	=Zt*k-2,
Zt +k-1	=Ztkn + k-1	(Ztn+(k-1) ∈ {Ztkm + k-1 }m=1),	lim Zt	= ~	tkn+k-1 n-∞	Zt*k-(k-1),
Zt	= t +k	Ztk(n+1)	(Ztn+k ∈ {Ztkm }^m=1 ),	lim Zt	= n→∞ tk(n+1)	Zt*k ,
Zt +k+1 .	=Ztk(n + 1) + 1	(Ztn+ k + 1 ∈ {Ztkm+1}m=1),	,lim Ztk(n+1) + 1 = n→∞	v 7	Zt*k-1,
. . Zt +2k-1	=Ztk(n+1)+k-1 (Ztn+2k-1 ∈ {Ztkm+k-1 }m=1),	nlim∞ Ztk(n+1)+k-1 = Zt*k-(k-1)	
Zt +2k	二 Ztk(n+2) (Ztn + 2k ∈ {Ztkm }∞=1),	lim Zt	= n→∞ tk(n+2)	Zt*k ,
(65)
Consequently, for n ≥ N and j ∈ N, we can write
kj-1
∏ Wω(……)
i=0
k	k	k
一(Y WQ(tk(汽+j)+k-i)) ( Y WΩ(tk(n十j-1)十k-i))…(Y WΩ(tk(n)十k-i))
j k
=YYWQ(tk(汽+j-i)+k-i >	(66)
On the other hand, in equation (28), there are different configurations for matrix D^(t-i)and
hence different forms for matrix WΩ(tk汽+s). In this case, the phase space of the system is divided
into different sub-regions by some borders; see (Monfared & Durstewitz, 2020a;b) for more details.
Also, since the system (28) is a linear map in each sub-region, the k periodic points of Γk must
belong to different sub-regions (at least two different sub-regions). Accordingly, based on (63) and
(65), there exists some N ∈ N such that for every n ≥ N both Ntk汽+s and zt*fc-s belong to the
same sub-region and so the matrices Wq*汽+s) and W∏(t*k-S) (S ∈ {0,1,2, ∙∙∙ ,k - 1}) are
identical. Hence, for n ≥ N, n ≥ N and j ∈ N, equation (66) becomes
kj-1	j k	∕k-1	∖j
Y WΩ(tn+kj-1-i) = Y Y WΩ(tk(")+k-i) = Y WΩ(t*k-s)	.	(67)
i=0	l=0 i=1	、s=0	,
Therefore, similar to the part (ii) ,we can prove for every z1 ∈ Bpfc, ¾T and dLT will also remain
bounded.	口
A.2.2 PROOF OF THEOREM 2, PART (II) Proof. (ii) Let for every T > 2 LT :二	二 JT JT-I …J^2.	(68)
24
Under review as a conference paper at ICLR 2022
{LT}T∈N, T>2 is a sequence of matrices LT = li(jT)1≤i,j≤M and, due to (13),
limT →∞ kLT k = ∞. Hence, there is at least one sub-sequence {l(mTkn) }Tn∈N, Tn>2 (for
some m,k ∈ {1, 2,…，M}) such that limτn→∞ I(Tn) = ∞.
On the other hand
∂zTT	∂ +ZTT
—: =—:-----
∂θ ∂θ
T-2
+X
t=1
d+zT-t
∂θ
(69)
Moreover, there exists some N > 2 such that (for t = T - N + 1)
d+zN-ι
-∂θ-
6= 0.
(70)
For θ as the k-th element of a parameter vector θ (or belonging to the k-th row of a parameter matrix
θ), the term
T-N
Y JT-
r=0
d+zN-ι
-∂θ-
(71)
is a vector in which the i-th element is l(T) d z∂θNT
Since limτn→∞ Imkn = ∞, due to (70) limTn→∞ l(Tn) ""∂θNT = ∞, which implies 喻 Will
∂L*
diverge as T → ∞. Similarly, by (21), we can prove -T^- is divergent for T → ∞.
By Oseledec,s multiplicative ergodic Theorem, the results also hold for every zι ∈ B「*.	□
A.2.3 Proof of theorem 3
Proof. Let Γ = {zι, z2,... ZT, ∙∙∙} be a quasi-periodic attractor. Then, the largest Lyapunov
exponent of Γ is
λ
T→∞ T ln kJT Jτ
-1 ∙
J2k
lim Ln
T→∞ T
∂zτ
∂zι
0.
(72)
We prove for every 0 < < 1
lim (1 - )T -1 < lim
T→∞	T→∞
∂zτ
∂zι
< lim (1 + )T-1.
T→∞
(73)
For this purpose, we show ∀ 0 < < 1
(I) limτ→∞(1 - e)τT < limτ→∞ Il⅛f∣∣,and
(II) limτ→∞ll 第
< limT→∞ (1 + )T-1.
Assume for the sake of contradiction that (I) does not hold. Then there exists some 0 < < 1 such
that
lim (1 — E)TT ≥ lim	^zT .	(74)
T→∞	T→∞ l ∂z1 l
Therefore
∃ To > 1 s.t. ∀T ≥ To =⇒ (1 — e)tT ≥ dzT ,	(75)
l ∂z1 l
25
Under review as a conference paper at ICLR 2022
and so
ZITl γτ>τ	In(I-E)TT	lnIl^dz1∣∣
∃ T0 > 1 St ∀ T ≥ To =⇒ ----——≥	—γl.	(76)
T -1 T -1
Consequently, due to (72), for T → ∞ we have ln(1 - E) ≥ 0. This implies E ≤ 0, which is a
contradiction.
Similarly if we assume (II) is not true, then there exists some 0 < E < 1 such that
Thereby
and thus
lim zT-∣ ≥ lim (1 + E)T-1.
T→∞ ∣ ∂z1 ∣	T→∞
∃T0 >1 s.t. ∀T ≥ To =⇒ Il ⅞τ I ≥(1+E)T-1,
∃ To > 1 s.t. ∀ T ≥ To =⇒
ln∣∣舞 I > ln(1 + E)T-1
T - 1	≥ T - 1
(77)
(78)
(79)
This means ln(1 + E) ≤ 0 as T → ∞, i.e. E ≤ 0, which is a contradiction.
Therefore (14) holds for Γ and also, according to Oseledec’s multiplicative ergodic Theorem, for
every zι in the basin of attraction of Γ.	口
A.3 Additional results on relation between dynamics and gradients
A.3.1 Further results and remarks related to Theorem 2
Remark 4. The result of Theorem 2 also holds for unstable orbits {zι, z2, z3, ∙∙∙} with positive
largest Lyapunov exponent. Trivially, for such orbits that diverge to infinity (unbounded latent states)
gradients of the loss function will explode as T → ∞.
Remark 5. For RNNs with ReLU activation functions there are finite compartments in the phase
space each with a different functional form. In such a case, to define the largest Lyapunov exponent
of Γ*, in the Proofof Theorem 2 we assume that Γ* never maps to the points Ofthe borders.
Based on Theorem 2, we can also formulate the necessary conditions for chaos and diverging gradi-
ents in standard RNNs with particular activation functions by considering the norms of their recur-
rence matrix, for which the following Corollary provides the basis:
Corollary 1. Let for a standard RNN
∣∣diag(f0(Wzt-i + Bst + h))∣∣ ≤ γ < ∞.	(80)
If the RNN is chaotic, then kW k γ > 1 .
Proof. Assume for the sake of contradiction that kW k γ ≤ 1 . From
Y W diagf (WZt-I + Bst + h)) ≤ Y ∣∣W diagf (WZt-I + Bst + h))∣∣
∣2<t≤T	∣	2<t≤T
≤ (kWk γ)T-2,	(81)
it is concluded that limT →∞ ∣Q2<t≤T Wdiag(f (Wzt-ι + Bst + h)) ∣∣ < ∞ , which con-
tradicts (13). This means kW k γ > 1 is a necessary condition for the standard RNN to be
Chaotic.	口
Remark 6. ForRNNwith the tanh and sigmoid activation functions Y = 1 and Y = 1, respectively.
Thus, by Corollary 1, the necessary conditions for chaos in these two cases are kW k > 1 and
kWk > 4 , respectively.
26
Under review as a conference paper at ICLR 2022
A.3.2 Other connections between dynamics and gradients
As sect. 3 elucidated, there is a direct link between the norms of the Jacobians of the RNN along
trajectories and the EVGP. By observing this link, we can formulate some general conditions that
will have implications for the behavior of the gradients regardless of the limiting behavior of the
RNN, as collected in the following theorem:
Theorem 4. Let Ozi = {zι, z2,... ZT, ∙∙∙} be a Sequence (orbit) generated by an RNN Fθ ∈ R
parameterized by θ, and PT := JT - I, T = 2,3,….
(i)	Assume that Ozi is an orbit for which ∣∣ d∂zT ∣∣ ≤ ξ ∀t ∙ If P∞=2 IlJT k < ∞, then the
Jacobian d∂zT, the tangent vector dzT and thus the gradient ofthe loss function, dLT, will
be bounded for T → ∞.
(ii)	If	T∞=2 IPT I < ∞, then the Jacobian d∂zT will neither vanish nor explode as T → ∞.
Proof. Let k.k be any matrix norm satisfying kA1A2 k ≤ kA1k kA2k.
∂+ z
(i)	By boundedness of ∂zτ We have
∂ ZT
~∂θ~
+	T-2 t-1
daF + X(Y JT-r
t=1 r=0
≤
ξ
∂+ZT-t
∂θ
T-2
t-1
T-2 t-1
1+ X∣∣∣YJT
t 1∣r 0
-r
t=1
r=0
T-2 t-1
≤ξ 1+	kJT-rk	.
t=1 r=0
(82)
Moreover,
T-2 t-1
1+XYkJT -r k ≤ 1 + X kJp k + X kJp k kJq k +
t=1 r=0
E k Jpkk Jq kkJrk + …
<q <r
p<q
p
T
=(1 + IJt k )(1 + IJt-ιk)…(1 + J2k) =: Y(1 + Bk). (83)
t=2
Since PT∞=2 kJTk converges, according to (Wedderburn, 1964), the infinite products QT∞=2 1 +
k Jt k) in (83) converge to a finite number K = 0. Consequently, by (82) and (83)
lim
T→∞
∂zt
~∂θ~
(84)
≤ K < ∞,
which implies d∂Lτ will be bounded for T → ∞.
∞
lim UT ≤ ΓΓ ∣∣Jtk := lim (
T→∞ ∣ ∂Z1 ∣	T→∞
1 T2
Furthermore
∞
≤ Y (1 + kJτ k) ≤ K,
T=2
(85)
which completes the proof.
(ii)	Since PT∞=1 kPTk < ∞, due to (Wedderburn, 1964) the infinite product
∞∞
Y(I + PT) = Y JT ：
T=2	T=2
lim JT JT-ι ∙
T→∞
J2,
converges to a matrix K 6= O, which implies
0 < lim
T→∞
∂zt
∂ Zi
kKk < ∞.
(86)
(87)
□
27
Under review as a conference paper at ICLR 2022
Part (i) of Theorem 4 relaxes some of the conditions required in Theorem 1 for bounded gradients
by imposing a Lipschitz condition on the immediate derivatives. Part (ii) generalizes conditions
satisfied, for instance, in orthogonal (unitary) RNNs (Arjovsky et al., 2016; Henaff et al., 2016) or
fully regularized PLRNNs (Schmidt et al., 2021).
Proposition 3. Let Ozi = {zι, z2,... ZT, ∙…} be an orbit generated by an RNN Fθ ∈ R (param-
eterized by θ), and ∣∣ Jt ∣∣ = 0, T ≥ 2. If P∞=2 ln IIJT k diverges to -∞ ,then the Jacobian d∂zT
vanishes as T tends to infinity.
Proof. For ∣JT ∣ 6= 0, T ≥ 2, we have
0 ≤	dzτ	≤	∣∣Jt k	∣Jt-i∣ ∙∙∙	∣∣J2k	=	eln kJTk eln kJT Tk ∙∙∙	eln kJ2k	= H=ln kJtk.
∂z1
(88)
Hence if PT∞=2 ln ∣JT∣ → -∞, then
∂zT
lim —— = O.	(89)
T→∞ ∂z1
□
A.4 Empirical evaluation: Datasets
Lorenz attractor The Lorenz system (Lorenz, 1963) is a simplified model for atmospheric con-
vection, given by
dx
dt
dy
dt
dz
σ(y - x),
x(ρ-z) -y,	(90)
xy - βz.
The system is of particular interest for its chaotic regime and was studied here for σ = 16, ρ = 45.92
and β = 4. For these parameters the Lorenz system is known to have a maximal Lyapunov exponent
λmax = 1.5 (Rosenstein et al., 1993). To generate a time series, the ODEs were integrated with
a step size ∆t = 0.01 using scipy.integrate. Accordingly, the prediction time is τpred =
∆tn(2)	= 46.2.
max
Duffing oscillator The Duffing oscillator (Duffing, 1918) is an example of a periodically forced
oscillator with nonlinear elasticity
x + δX + βx + αx3 = Y cos(ωt).	(91)
Note that this system is non-autonomous, that is externally forced due to the r.h.s. of eqn. 91. The
following parameters were chosen to arrive at a chaotically forced oscillator: α = 1.0, β = -1.0,
δ = 0.1, γ = 0.35, and ω = 1.4. For these parameters the Duffing oscillator has a maximum
Lyapunov exponent of λmax = 0.0995. The dataset used here was created with the code from
(Gilpin, 2021) as a three dimensional embedding with step size ∆t = 0.17. The prediction time is
τpred = 39.28.
Rossler system Another prime textbook example for a chaotic system is the Rossler system
(Rossler,1976) given by:
dx
dt
dy
dt
dz
dt
-y-z,
x+ay,
b + z(x - c).
(92)
28
Under review as a conference paper at ICLR 2022
For the parameters a = 0.15, b = 0.2 and c = 10, the maximal Lyapunov exponent is λmax =
0.09 (Rosenstein et al., 1993). To arrive at a time series, a step size of ∆t = 0.1 was chosen for
integration. This gives us a prediction time of τpred = 77.0 for this system.
Mackey-Glass equation The Mackey-Glass equation (Glass & Mackey, 1979) is a nonlinear time
delay differential equation
X = β 1 xp n - Yx With β,γ,p> 0.	(93)
1 + xρn
Here xρ represents the value of the variable x at time t - ρ (note that strictly, mathematically, this
makes the system infinite-dimensional). Choosing the parameters to be β = 2, γ = 1.0, n = 9.65,
and ρ = 2.0, leads to chaotic behavior With a maximum Lyapunov exponent of λmax = 0.21.
The dataset Was created as a 10-dimensional embedding With the code from (Gilpin, 2021) using
∆t = 0.04. This yields a prediction time of τpred = 82.2.
Empirical temperature time series This time series Was recorded at the Weather Station at the
Max Planck Institute for Biogeochemistry in Jena, Germany, spanning the time period betWeen 2009
and 2016, and reassembled by Francois Chonet for the book Deep Learning with Python. The data
set can be accessed at https://WWW.kaggle.com/pankrzysiu/Weather-archive-jena.
To expose the underlying chaotic dynamics of the time series, trends and yearly cycles Were re-
moved, and nonlinear noise-reduction Was performed (using ghkss from TISEAN, see also Kantz
et al. (1993)). Fig. 4 (a) shoWs a snippet of the temperature data in comparison With the de-
noised time-series. High-frequency noise Was further reduced through Gaussian kernel smoothing
(σ = 200), and the resulting time series Was sub-sampled (every 5th data point Was retained). Fig. 4
(b) clearly reveals a fractional dimension ofDeff = 2.8 for the de-noised and smoothed time-series.
This strongly suggests that the dynamics governing the time series are chaotic. We created a time
delay embedding (Kantz & Schreiber, 2003) With m = 5 (estimated by the false nearest neighbor
technique, see Kennel et al. (1992)) and delay ∆t = 500 (obtained as the first minimum of the
mutual information). The first three embedding dimensions are shoWn in Fig. 4(c). The maximal
Lyapunov exponent of this time series was determined with lyap_r from TISEAN (Hegger et al.,
1999) to be λmax = 0.016, see Fig. 3(a). This value is in close agreement with the literature (Millan
et al., 2010). The predictability time of this system is estimated to be τpred = 43.3.
2 0-2
河 əjn^,ɪəduɪə"
Figure 4: (a) Snippet of the original temperature data and de-noised time series. (b) Blue lines show the
local slopes of the correlation sums for embedding dimensions m ∈ {5, . . . , 10}. The convergence of these
estimates in m reveals a fractional dimension indicated by the plateau. (c) First three dimensions of the time-
delay embedding series as used for training.
All datasets used were standardized (i.e., centered with unit variance) prior to training.
A.5 Empirical evaluation: measures of reconstruction quality
Attractor overlap To asses the geometrical similarity of the chaotic attractor produced by the
RNN to the one underlying the observations, we calculate the Kullback-Leibler divergence of the
ground truth distribution ptrue(x) and the distribution pgen(x|z) generated by RNN simulation. To
do so in practice, we employ a binning approximation (see (Koppe et al., 2019))
K
Dstsp (PtrUe (X) ,Pgen (X | Z)) ≈ 工 p(kUe (X)Iog
k=1
Ue(X)
(x | Z)
29
Under review as a conference paper at ICLR 2022
where K is the total number of bins, and p)(ke (x) and Pgkn (X | Z) are estimates obtained as rela-
tive frequencies through sampling trajectories from the observed time-series and the trained RNN,
respectively.
Power-spectral correlations Since in DS reconstruction we aim to capture invariant (time-
independent) properties of the underlying system, besides the geometrical agreement, we compare
the similarity in true and RNN-reconstructed power spectra. To do so, we generate a time series
of length 100, 000 from the RNN and calculate its power spectrum using the fast Fourier transform
(scipy.fft). To reduce the influence of noise we apply Gaussian kernel smoothing and cut off
the long high-frequency tails of the spectra. The dimension-wise correlation between observed and
generated spectra are then averaged to give the P SC.
A.6 Further empirical evaluations
A.6.1 Reconstruction： ROSSLER System
Figure 5: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correlations
(PSC, higher = better) against learning interval T for the Rossler attractor. Continuous lines = sparsely forced
BPTT. Dashed lines = classical BPTT with gradient clipping. Prediction time indicated vertically in black.
Figure 6: The Rossler attractor (blue) and reconstruction by a LSTM (orange) trained with a learning interval
(a) chosen too small (τ = 5), (b) chosen optimally (τ = 30), and (c) chosen too large (τ = 200).
A.6.2 Reconstruction: High-dimensional Mackey-Glass system
Figure 7: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correla-
tions (PSC, higher = better) against learning interval τ for the 10d Mackey-Glass system. continuous lines =
sparsely forced BPTT. Dashed lines = classical BPTT with gradient clipping. Prediction time indicated verti-
cally in black.
30
Under review as a conference paper at ICLR 2022
A.6.3 Reconstruction: Partially observed Lorenz System
For this evaluation we trained models only on the variables {y, z} of the Lorenz system, eqn. 90.
In order to compute the attractor overlap (Dstsp) in the true state space, however, after training the
observation matrix B was recomputed by linearly regressing the first 10 latent states onto the first
10 observations from all three Lorenz variables in eqn. 90.
Figure 8: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correlations
(P SC, higher = better) against learning interval τ for the partially observed Lorenz system. Continuous lines
= sparsely forced BPTT. Dashed lines = classical BPTT with gradient clipping. Prediction time indicated
vertically in black.
A.6.4 Other initialization procedures: Standard batching with zero resetting
A common procedure in training RNNs is partitioning the time series into chunks (as we did based
on the Lyapunov spectrum), but then simply resetting the hidden states at the beginning of each
chunk (window) to 0. Formally this would mean that we do not force the trajectory back on track as
in our approach, but instead may kick it off the track. To illustrate this, here we trained an LSTM
on chunks (windows) with a length given by the optimal τ (τopt = 30 for the Lorenz system), but
then initialized the hidden states to 0 at the beginning of each window. The performance obtained
this way is indicated by the green dashed line in Fig. 9 below.
Figure 9: Overlap in attractor geometry (Dstsp, lower = better) and dimension-wise power-spectra correla-
tions (P SC, higher = better) against learning interval τ for the Lorenz system. Continuous lines = sparsely
forced BPTT. Dashed-dotted lines = windowing without forcing (choosing windows according to the optimal
prediction time, but resetting hidden states to zero rather than its TF control value). Prediction time indicated
vertically in black.
A.6.5 Electroencephalogram (EEG) data
We used EEG data recorded by Schalk et al. (2004) and provided on PhysioNet (Goldberger et al.,
2000), from which we took the baseline recording of the first patient for our analysis. Preprocessing
was performed as outlined above for the temperature time-series, i.e. we applied nonlinear
noise-reduction (see Fig.10 (a)) and Gaussian kernel smoothing (σ = 30). Fig. 10 (b) indicates a
fractional dimension Deff = 1.8 for the de-noised and smoothed times series. We created a time
delay embedding with an embedding dimension of m = 10 and a delay time of ∆t = 500. The
maximal Lyapunov exponent for this time series was determined to be λmax = 0.007, see Fig. 11
(a). To ease training, the time-series was sub-sampled and only every third data point was retained.
With this, we obtain a predictability time τpred = 33.01.
31
Under review as a conference paper at ICLR 2022
Figure 10:	(a) Snippet of the original EEG data and de-noised time series. (b) Blue lines show the local slopes
of the correlation sums for embedding dimensions m ∈ {5, . . . , 10}. The convergence of these estimates in m
reveals a fractional dimension indicated by the plateau. (c) First three dimensions of the time-delay embedding
series as used for training.
Full reconstruction of the dynamics from complex and noisy EEG signals is a very ambitious. Here
we therefore focused on the system’s short-term behavior and combined multiple shorter trajectory
bits (of length T = 70) in the calculation of our measure for geometrical agreement in state space.
Figure 11:	(a) The maximal Lyapunov exponent was determined as the slope of the average log-divergence
of nearest neighbors in embedding space (m = embedding dimension). (b) Reconstruction quality assessed by
attractor overlap (lower = better) and power-spectrum correlation (higher = better). Black vertical lines = τpred.
A.7 Sparsely forced BPTT
Loss truncation One implicit consequence of the teacher forcing, eqn. (16), is the interruption
of the hidden-to-hidden connections at these time points. More specifically, if the system is forced
at time t ∈ T, then there is no connection between zt and zt+1, that is
Jt+1
∂zt+ι _ ∂RNN(Zt)
∂zt
∂zt
0.
(94)
To see how these vanishing Jacobians truncate the loss gradients w.r.t to some parameter θ, let us
focus on the loss gradients immediately after the forcing,
d Lt+1 = d Lt+1
dθ — dZt+1 ⅛
∂zt+1 ∂+zk
∂zk ∂θ
∂ Lt+1 (∂+Zt+ι
( ∂θ
∂zt+1
t
+X
k=1
∂zt+1
∂Zk
1—{z-}
=0 , because of (94)
∂+Zk)
∂θ )
∂Lt+1 ∂+zt+1
∂zt+1	∂θ
(95)
Eqn. (95) shows that sparsely forced BPTT implicitly truncates the loss gradients because it in-
terrupts the hidden-to-hidden connection from zt to zt+1 for t ∈ T. More generally, defining
t := max{t0 ∈ T : t0 ≤ t}, the overall loss gradients are truncated to
∂ L _XX ∂Lt X
∂θ	∂zt ∂ Zt
t=1	t k=1
Tt
=X受X
t=1	k=et
∂Zt ∂+Zk
∂ Zk ∂θ
∂zt ∂+Zk
∂Zk ∂θ .
(96)
32