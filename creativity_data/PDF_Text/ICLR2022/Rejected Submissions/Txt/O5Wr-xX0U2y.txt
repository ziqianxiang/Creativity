Under review as a conference paper at ICLR 2022
Deep Reinforcement Learning for Dynamic Ex-
pectile Risk Measures: An Application to
Equal Risk Option Pricing and Hedging
Anonymous authors
Paper under double-blind review
Abstract
Motivated by the application of equal-risk pricing and hedging of a financial
derivative, where two operationally meaningful hedging portfolio policies needs
to be found that minimizes coherent risk measures, we propose in this paper a
novel deep reinforcement learning algorithm for solving risk-averse dynamic de-
cision making problems. Prior to our work, such hedging problems can either only
be solved based on static risk measures, leading to time-inconsistent policies, or
based on dynamic programming solution schemes that are impracticable in re-
alistic settings. Our work extends for the first time the deep deterministic policy
gradient algorithm, an off-policy actor-critic reinforcement learning (ACRL) algo-
rithm, to solving dynamic problems formulated based on time-consistent dynamic
expectile risk measure. Our numerical experiments confirm that the new ACRL
algorithm produces high quality solutions to equal-risk pricing and hedging prob-
lems and that its hedging strategy outperforms the strategy produced using a static
risk measure when the risk is evaluated at later points of time.
1	Introduction
This paper considers solving risk-averse dynamic decision making problems arising from applica-
tions where risk needs to be evaluated according to risk measures that are coherent. In particular, we
draw our motivation from the financial application of equal-risk pricing (ERP) and hedging (Guo &
Zhu (2017)), where two dynamic hedging problems need to be solved, one for the buyer and one for
the seller of a financial derivative (a.k.a option), for determining a fair transaction price that would
expose both parties to the same amount of hedging risk. The need to meaningfully model each
party’s best hedging decision in a financial market, namely that no arbitrage is allowed, and to have
a meaningful comparison between the two parties’ risk exposures, namely that the risks should be
measured in the same units, has led to the use of coherent risk measures for capturing both parties’
hedging risks in this application (see Marzban et al. (2020)).
To this date, most solution methods proposed for solving risk-averse dynamic decision making prob-
lems under a coherent risk measure have either relied on traditional dynamic programming (DP),
which suffers from the curse of dimensionality and assumes the knowledge of a stochastic model
that precisely captures the dynamics of the decision environment, or on the use of a static risk mea-
sure, i.e., that disregards the temporal structure of the random variable (e.g. Marzban et al. (2020),
Carbonneau & Godin (2020), and Carbonneau & Godin (2021) in the case of the ERP application).
The latter raises the serious issue that the resulting policy could be time inconsistent, i.e. that the ac-
tions prescribed by the policy may be considered significantly sub-optimal once the state is visited.
In an application such as ERP, this issue implies that policies obtained based on static risk measures
will not be implemented in practice, raising the need to consider dynamic risk measures.
Focusing on deep reinforcement learning (DRL) methods, while there has been a large number of
approaches proposed to address risk averse Markov decision processes (MDPs) using coherent risk
measures, to the best of our knowledge, all of them, except for two exceptions, consider a static
risk measure (see Prashanth & Ghavamzadeh (2013); Chow & Ghavamzadeh (2014); Castro et al.
(2019); Singh et al. (2020); Urpl et al. (2021)) and therefore suffer from time-inconsistency. The
1
Under review as a conference paper at ICLR 2022
two exceptions consist of Tamar et al. (2015) and Huang et al. (2021) who propose actor-critic re-
inforcement learning (ACRL) algorithms to deal with a general dynamic law-invariant coherent risk
measures. Unfortunately, the two algorithms respectively either assume that it is possible to generate
samples from a perturbed version of the dynamics, or rely on training three neural networks (namely
a state distribution reweighting network, a transition perturbation network, and a Lagrangean penal-
isation network) concurrently with the actor and critic networks. Furthermore, only Huang et al.
(2021) actually implemented their method. This was done on a toy tabular problem involving 12
states and 4 actions where it produced questionable performances1.
In this paper, we develop a new model-free ACRL algorithm for solving a time-consistent risk
averse MDP under a dynamic expectile risk measure.2 Overall, we may summarize the contribution
as follows:
•	Our ACRL algorithm is the first to naturally extend the popular model-free deep deter-
ministic policy gradient algorithm (DDPG) (see Lillicrap et al. (2015)) to a risk averse
setting where a time consistent coherent risk measure is used. Unlike the ACRL proposed
in Huang et al. (2021), which employs five neural networks, our algorithm will only re-
quire an actor and a critic network. While our policy network will be trained following
a stochastic gradient procedure similar to Silver et al. (2014), we are the first to leverage
the elicitability property of expectile risk measures to propose a procedure for training the
“risk-to-go” deep Q-network that is also based on stochastic gradient descent.
•	Our ACRL is the first model-free DRL-based algorithm capable of identifying optimal risk
averse option hedging strategies that are time-consistent with respect to a dynamic coherent
risk measure, and of computing their associated equal risk prices. A side benefit of time-
consistency will be that after training for an option with a given maturity, one obtains equal
risk prices and hedging strategies for any other shorter maturities. While our algorithm
certainly has a broader set of applications, we believe that ERP constitutes an original and
fertile application in which to develop and test new risk averse DRL methods.
•	We evaluate the training efficiency and the quality of solution, in terms of quality of option
hedging strategies and of estimated equal risk prices, obtained using our ACRL algorithm
on a synthetic multi-asset geometric Brownian motion market model. These experiments
constitute the first real application of a risk averse DRL algorithm that employs a dynamic
coherent risk measure.
The rest of this paper is organized as follows. Section 2 introduces equal risk pricing and its asso-
ciated DP equations. Section 3 proposes the new ACRL algorithm for general finite horizon risk
averse MDP with dynamic expectile measures. Finally, Section 4 presents and discusses our numer-
ical experiments. We note that a reader only interested in the ACRL algorithm can skip right ahead
to Section 3.
2	Application: Equal risk pricing and hedging under dynamic
EXPECTILE RISK MEASURES
As described in Marzban et al. (2020), the problem of ERP can be formalized as follows. Consider
a frictionless market, i.e. no transaction cost, tax, etc, that contains m risky assets, and a risk-free
bank account with zero interest rate. Let St : Ω → Rm denote the values of the risky assets adapted
to a filtered probability space (Ω, F, F := {Ft}T=Q, P), i.e. each St is Ft measurable. It is assumed
that St is a locally bounded real-valued semi-martingale process and that the set of equivalent local
martingale measures is non-empty (i.e. no arbitrage opportunity). The set of all admissible self-
financing hedging strategies with the initial capital p0 ∈ R is shown by X (p0):
X(p0)
{x : Ω → RT ∃{ξt}T=01
t-1
Xt = p0 + X ξt>0 ∆St0+1,
t0=0
∀t = 1, . . . , T
1At the time of writing this paper, the risk averse implementation of this algorithm reported in Huang et al.
(2021) is unable to recommend an optimal policy in a deterministic setting, while the risk neutral implementa-
tion produces policies that are outperformed by risk averse ones in a stochastic setting.
2Our ACRL algorithm exploits the elicitabilty property of expectile risk measures, which is the only elic-
itable coherent risk measure.
2
Under review as a conference paper at ICLR 2022
where ∆St+1 := St+1 - St, the hedging strategy ξt ∈ Rm is a vector of random variables adapted
to the filtration F and captures the number of shares of each risky asset held in the portfolio during
the period [t, t + 1], and Xt is the accumulated wealth.
Let F ({St}tT=1) denote the payoff of a derivative. Throughout this paper, we assume F ({St}tT=1)
admits the formulation ofF(ST, YT) where Yt is an auxiliary fixed-dimensional stochastic process
that is Ft-measurable. This class of payoff functions is common in the literature, (see for example
Bertsimas et al. (2001) and Marzban et al. (2020)). The problem of ERP is defined based on the
following two hedging problems that seek to minimize the risk of hedging strategies, one is for the
writer and the other is for the buyer of the derivative:
(Writer)	%w(p0)=	inf ρw(F(ST,YT) -XT)	(1)
X∈X(p0)
(Buyer)	%b(p0)=	inf	ρb(-F (ST , YT ) - XT ) ,	(2)
X∈X(-p0)
where ρw and ρb are two risk measures that capture respectively the writer and the buyer’s risk
aversion. In words, equation (1) describes a writer that is receiving p0 as the initial payment and
implements an optimal hedging strategy for the liability F(ST, YT). On the other hand, in (2) the
buyer is assumed to borrow p0 in order to pay for the option and then to manage a portfolio that
will minimize the risks associated to his final wealth. With equations (1) and (2), ERP defines a
fair price p0 as the value of an initial capital that leads to the same risk exposure to both parties, i.e.
%w (PM = %b(p0).
In particular, based on Proposition 3.1 and the examples presented in section 3.3 of Marzban et al.
(2020), together with the fact that both ρw and ρb are dynamic recursive law invariant risk measures,
a Markovian assumption allows us to conclude that the ERP can be calculated using two sets of
dynamic programming equations.
Assumption 2.1. [Markov property] There exists a sufficient statistic process ψt adapted to
F such that {(St, Yt ,ψt)}tT=0 is a Markov process relative to the filtration F. Namely,
P((St+s, Yt+s, ψt+s) ∈ A|Ft) = P((St+s, Yt+s, ψt+s) ∈ A|St, Yt, Ψt) for all t, for all S ≥ 0,
and all sets A.
Specifically, on the writer side, We can define VTw (ST, YT, ψτ) := F(ST, YT), and recursively
Vtw(St, Yt, ψt) := inf ρ(-ξ>∆St+ι + Vt+ι(St + ∆St+ι, Yt + ∆Yt+ι, ψt+ι)∣St, Yt, ψt),
ξt
where ρ(∙∣St, Yt, ψt) is a law invariant risk measure that uses P(∙∣St, Yt, ψt). This leads to
considering %w (0) = V0w (S0, Y0, ψ0). On the other hand, for the buyer we similarly define:
Vb(ST, Yt, Ψt)：= -F(St, YT) and
Vb(St, Yt, ψt) := inf ρ(-ξ>∆St+ι + V+ι(St + ∆St+ι, Yt + ∆Yt+ι, ψt+ι)∣St, Yt, ψt),
ξt
with %b(0) = V0b(S0, Y0,ψ0). The following lemma summarizes how DP can be used to compute
the ERP.
Lemma 2.1 (Marzban et al. (2020)). Under Assumption 2.1, the ERP that employs dynamic expec-
tile risk measure can be computed as: pɔ = (Vw (So, Yo, ψo) 一 Vb(So, Yo, ψo))∕2.
In what follows, we will further assume that the risk measure is a dynamic expectile risk measure.
Definition 2.1. A dynamic expectile risk measure takes the form: P(X) := po(pι(... PT-ι(X)))
where each p(∙) is an expectile risk measure that employs the conditional distribution based on Ft.
Namely, pt(Xt+ι) := argminq TE [(q — Xt+ι)+∣Ft] +(1 — T)E [(q — Xt+ι)-∣Ft] where Xt+ι
is a random liability measureable on Ft+1 .
Like conditional value at risk, the expectile measure (see Bellini & Bignozzi (2015)) covers the
range of risk attitudes from risk neutrality, when T = 1∕2, to worst-case risk, when T → 1.
3 Anovel actor-critic algorithm for risk averse MDP under a
DYNAMIC EXPECTILE RISK MEASURE
With the dynamic programming equations in hand, it now becomes apparent that each option hedg-
ing problem in ERP can be formulated as a finite horizon Markov Decision Process (MDP) described
3
Under review as a conference paper at ICLR 2022
with (S, A,r,P). In this regard, the agent (i.e. the writer or buyer) interacts with a stochastic en-
vironment by taking an action at ≡ ξt ∈ [-1, 1]m after observing the state st ∈S, which in-
cludes St, Yt, and ψt. Note that to simplify exposition, in this section we drop the reference to
the specific identity (i.e. w or b) of the agent in our notation. The action taken at each time t re-
sults in the immediate stochastic reward that takes the shape of the immediate hedging portfolio
return, i.e. rt(st, at, st+ι) := ξ>∆St+ι When t < T and otherwise of the option liability/PayoUt
rτ(ST,aτ,sτ +1) := F(ST, YT)(1 - 2 ∙ 1{agent=writer}), which is insensitive to ST +1. Fi-
nally, the Markovian exogeneous dynamics described in Assumption 2.1 are modeled using P as
P(st+ι∣st,at) = P(St+ι, Yt+ι, ψt+ι∣St, Yt, ψt). Overall, each of the two dynamic derivative
hedging problems presented in Section 2 reduce to a version of the following general risk averse
reinforcement learning problem:
%(0) = V0(S0, Y0, ψ0) = min Qn®, ∏0(s0)),
π
where S0 := (S0, Y0, ψ0) is the initial state in which the option is priced while Qn(st,at):=
p(-rt(st, at, st+ι) + Qt+1(st+1, ∏t+1(st+1))∣st), QT(ST3):= -rT(st,a，T, ST), and where P
is an expectile risk measure, i.e. P(X) := argminq E [τ (q - X )j + (1 - τ )(q - X)-]. Equipped
with these definitions, we can now motivate our proposed extension of the model-free off-policy
deterministic ACRL algorithm to the general finite horizon risk-averse MDP setting. In doing so,
we start with a proposition (see Appendix A.1 for a proof) that will provide the motivation for
a stochastic gradient scheme to optimize the actor network, while the optimization of the critic
network will follow from the elicitability property of the expectile risk measure.
Proposition 3.1. Let π be an arbitrary reference policy and μ an arbitrary distribution over the
initial state, such that there is a strictly positive probability of reaching all ofS for all t ≥ 1.3 * For
any π* that satisfies
π* ∈ argminE 口{0,...,T},so〜“ [Q∏(st, π¼))]	⑶
St+1~P G|st,nt(St))
where t is uniformly drawn, we necessarily have that π* ∈ arg min Qn (s0, ∏0(s0)) hence %(0)=
Q0* (s0, πo(s0)).
In the context of a deep reinforcement learning approach, we can employ a procedure based on off-
policy deterministic policy gradient (Silver et al., 2014) to optimize (3). Specifically, given a policy
network πθ, we wish to optimize:
minE 20,...,τ-i}	[Qnθ(st, πθ(sP)],
St+ι~P (∙∣St,∏t(st))
using a stochastic gradient algorithm. In doing so, we rely on the fact that:
RBE ]〜{0,…,T-1} [Qn (Sa πθ(S£))]
St+1^P (∙∣St,∏t(st))
=E A〜{0,…,T-1}	rθ Qn(Saa)I	θ + RaQn(Saa)rΘ πθ(%)∣
St+1 〜P(∙∣st,∏t(St)) L	∣a=nθ(%)	Ia=曜⑶)」
≈ E 干〜{0,…,T —1}	IVaQnθ (s⅛, a)Rθ∏θ(s⅛)I	θ...
St+1 〜P (∙∣st,nt(st)) L	∣a=n 工(S 工)」
Note that in the above equation, we have dropped the term that depends on VθQtnaθ as is commonly
done in off-policy deterministic gradient methods and usually motivated by a result of Degris et al.
(2012), who argue that this approximation preserves the set of local optima in a risk neutral setting,
i.e. ρ(∙) := E[∙]. While we do consider as an important subject of future research to extend this
motivation to more general risk measures, our numerical experiments (see Section 4.3) will confirm
empirically that the quality of this approximation permits the identification of nearly optimal hedging
policies.
3In our option hedging problem, given that st is entirely exogenous, the distribution of st+1 is unaffected
by π, which can therefore be chosen arbitrarily. Moreover, μ can put all the mass on 丽.
4
Under review as a conference paper at ICLR 2022
Given that We do not have access to an exact expression for Qn ⑸,a), this operator needs to be
estimated directly from the training data. Exploiting the fact that ρ is a utility-based shortfall risk
measure, We get that:
Qn (st, at) ∈ arg min Est+1 〜P (∙∣St,at)['(q + rt(st, at, 4 st+1) - Qπ+1(St+1, πt+1(St+1)))]
where '(y) := (τ 1{y > 0} - (1 - T)1{y ≤ 0})y2 is the score function associated to the T-expectile
risk measure. As explained in Theorem 3.2 of Shen et al. (2014), in a tabular MDP environment one
can apply the following stochastic gradient step:
Qt(st, at) — Qt(st, at) - αd'(Qt(St, at) + rt(st, at, st+1) -
A ( .	_	( .	∖ ∖
Qt+1(St+1, πt+1(St+1)) ,
where ∂'(y) := 2(τ max(0, y) - (1 - T) max(0, -y)) is the derivative of '(y), within a properly
designed Q-learning algorithm and have the guarantee that Q t(st,at) will almost surely converge to
Qtπ(St,at) for all t, St, and at.
In the non-tabular setting, we replace Qtπ (St, at) with two estimators: i.e. the “main” net-
work Qn (st,at∣θQ) for the immediate conditional risk and the “target” network Qn (st,at∣θQ0)
for the next period’s conditional risk. The procedure consists in iterating between a step that
attempts to make the main network Qn(st,at∣θQ) a good estimator of ρ(-r(st,at,st+ι) +
Qt+ι(st+ι,at+ι∣θQ )) and a step that replaces the target network Qn(st, at∣Θq ) with a network
more similar to the main one Q∏(St,at∣θQ). The former is achieved, similarly as with the policy
network, by searching for the optimal θQ according to:
mQnE 口{0,…,T-1} ['(Qn (st, πi(st)lθQ)+rt(si, ni(si),Si+i)-Qt+i(si+i, ∏+ι(si+ι)lθQj)],
st+ι^P (∙∣st,∏t(st))
which suggests a stochastic gradient update of the form θQ J θQ - α∆, where ∆ is
a`(Qn(Si, πi(si)ιθQ)+Tt(St, πi(si),si+I)-Qn+-"+I, πi+ι(si+ι)ιθQ0 ))vθQ Qn(Si,行(Si)斗).
These two types of updates are integrated in our proposed expectile-based actor-critic deep RL (a.k.a.
ACRL) algorithm. A first version, Algorithm 1, is designed for a simulation-based environment.
One may note that in each episode, the reference policy ∏t is updated to be a perturbed version of the
main policy network in order to focus the accuracy of the main critic network’s value and derivatives
on actions that are more likely to be produced by the main policy network. We also choose to update
the target networks using convex combinations operations as is done in Lillicrap et al. (2015) in
order to improve stability of learning. A second more general version of ACRL, which mimics the
original DDPG, by generating minibatches using a replay buffer can also be found in Appendix A.2.
We finally note that in our problem, P (St+1 |St,at)	=	P(St+1|St,a0t)	=
P(St+1, Yt+1,ψt+1 |St, Yt,ψt), meaning that the action is not affecting the distribution of
state in the next period. This is a direct consequence of using a translation invariant risk measure,
which eliminates the need to keep track of the accumulated wealth in the set of state variables as
explained in Marzban et al. (2020) and allows the reward function to provide an immediate signal
regarding the quality of implemented actions. In the context of our deep reinforcement learning
approach, we observed that convergence speed is significantly improved in training due to this
property (see Figure 4 in Appendix).
4 Experimental results
In this section we provide two different sets of experiments that are run over one vanilla and one
basket option. We will compare both algorithmic efficiency and quality, in terms of pricing and
hedging strategies, of the dynamic risk model (DRM), which employs a dynamic expectile risk
measure and is solved using our new ACRL algorithm, and the static risk model (SRM), which
employs a static expectile measure and is solved using an AORL algorithm similar to Carbonneau &
Godin (2021). All experiments are done using simulated price processes of five risky assets: AAPL,
AMZN, FB, JPM, and GOOGL. The price paths are simulated using correlated Brownian motions
considering the empirical mean, variance, and the correlation matrix of five reference stocks (AAPL,
AMZN, FB, KPM, and GOOGL) over the period that spans from January 2019 to January 2021. In
5
Under review as a conference paper at ICLR 2022
Algorithm 1: The actor-critic RL algorithm for the dynamic recursive expectile option hedging
problem (ACRL)
Randomly initialize the main actor and critic networks’ parameters θπ and θQ ;
Initialize the target actor, θπ0 J θπ, and critic, θQ0 J θQ, networks;
for j =1 : #Episodes do
Randomly select t ∈{0, 1,..., T - 1};
Sample a minibatch ofN triplets {(si, a；, st+1)}N=1 from P(∙∣st, ∏t(st)), where
∏t(st):= ∏t(st∣θπ) + N(0, σ);
Set the realized losses yi := -rt(St,Ot, s") + Qt+ι(s↑+ι, ∏t+ι(st+ι∣θπ0)∣θQ0);
Update the main critic network:
1N
θQ - θQ - αN E∂'(Qt(st,a"θQ) - yj)%QQt(st,ai∣θQ);
N i=1
Update the main actor network:
1N
θπ J θπ - α- I VaQt(St, a∣θQ)∣α=∏t(st∣θ∏)Vθ∏∏t(st∣θπ)；
Update the target networks:
θQ0 J αθQ+(1-α)θQ0,θπ0 J αθπ +(1-α)θπ0;	(4)
end
both experiments, the maturity of the option will be one year and the hedging portfolios will be
rebalanced on a monthly basis. Table 3 in the appendix provides the descriptive statistics of our
underlying hidden stochastic process.
In what follows, we first explain the architectures of our ACRL model. Then, the training pro-
cedure of the networks under the dynamic risk measurement is elaborated. Finally, the main nu-
merical results of the paper are presented for pricing and hedging a vanilla, where the precision
of our approach will be empirically demonstrated, and a basket option. All codes are available at
https://anonymous.4open.science/r/ERP-Dynamic-Expectile-RM-4BEA.
4.1 Actor and critic network architecture
Our implementation of the ACRL algorithm involves two simple networks presented in Figure 1.
Since the underlying assets follow a Brownian motion, the actor and critic networks can define
the input state as the logarithm of the cumulative returns of each asset and the time remaining to
maturity (i.e. dimension = m +1). The actor network is composed of three fully connected layers
where the number of neurons are considered to be k = 32 in the first two layers and then maps back
to the number of assets in the last layer to generate the investment policy accordingly for each asset.
The activation functions in our networks are considered to be tanh functions. In the last layer, this
implies that the actions will lie in [-1, 1]m. The critic network only concatenates the m dimensional
action information vector after its third layer. In the case of SRM, only the actor network is used.
4.2 Training procedure and learning curves
Recall that in an SRM setting, overfitting of any DRL algorithm can be controlled by measuring the
performance of the trained policy on a validation data set using an empirical estimate of the risk-
averse objective as validation score. Unfortunately, this is no longer possible in the case of DRMs
since the risk measure relies on conditional risk measurements of the trajectories produced by our
policy. In theory, estimates of such conditional measurements could be obtained by training a new
critic network using the validation set (while maintaining the policy fixed to the trained one). In
practice, this is highly computationally demanding to perform in the training stage and raises a new
6
Under review as a conference paper at ICLR 2022
G+≡
WelS
qxJ+≡)
P3EUU。。A--nu-
Figure 1: The architecture of the actor and critic networks in ACRL algorithm.
issue of how to control overfitting of the validation score estimate. Our solution for this problem is
to rely on using static risk measures as validation score, namely a set of static expectiles at risk levels
that are larger or equal to the risk level of the DRM. Figures 2 and 3 in the appendix show examples
of learning curves for the validation performance of DRM and SRM approaches on vanilla and
basket options at a risk level of τ = 90%, with a maturity T = 12. SRM appeared to have a faster
rate of convergence than DRM, due to its simpler architecture. Being a time-inconsistent model,
SRM must however be retrained whenever the maturity of the option is modified. When comparing
convergence rates between vanilla and basket options, we observed similar behavior, which indicates
that the training time might not be very sensitive to the number of assets, thus suffering less from
the curse of dimensionality. We finally note that both our training and validation sets included 1000
trajectories from the underlying geometric Brownian motion process, implying that the procedure
can be applied in settings where only historical data is available.
4.3 Vanilla option hedging and pricing
In our first set of experiments, we consider pricing and hedging an at-the-money vanilla call option
on AAPL. In this setting, it is possible to obtain (approximately) optimal solutions by dynamic
programming via discretization of the state space. The initial price of AAPL is set to 78.81 and
options with time to maturity ranging from one month to one year are considered. Both DRM and
SRM are trained using a one year maturity/horizon.
With the trained DRM and SRM policy networks, we can evaluate the writer and the buyer’s (out-
of-sample) risk exposure over a pre-specified time horizon so as to calculate the corresponding ERP.
We consider the following three metrics for measuring the realized risk under different hedging
policy and explain the methods used for calculating the metrics:
•	Out-of-sample static expectile risk: Given a trained policy, use the test data to calculate the
static expectile risk. This is the metric that should be minimized by the SRM.
•	RL based out-of-sample dynamic expectile risk estimation: Given the trained policy, use the
test data to only train a critic network using ACRL to produce an estimate of out-of-sample
dynamic expectile risk. This is an estimate of the metric minimized by the DRM.
•	DP based out-of-sample dynamic expectile risk estimation: Given a trained policy, evaluate
the “true” dynamic expectile risk by solving the dynamic programming equations using a
high precision discretization of the states, actions, and transitions.4 This serves as the true
metric minimized by the DRM.
We note that our RL based estimate of out-of-sample dynamic risk is a novel approach, which tackles
the important challenge of policy evaluation in RL with dynamic risk measures.
Table 1 summarizes the evaluations of out-of-sample dynamic risk for DRM policies trained for 1
year maturity at risk level τ = 90% then applied to options of different maturities ranging from 12
4Note that this metric is available neither for the case of basket option nor in a data-driven environment.
7
Under review as a conference paper at ICLR 2022
Table 1: The out-of-sample dynamic and static 90%-expectile risk imposed to the two sides of
vanilla at-the-money call options over AAPL.
		Time to maturity								
Policy	EStj	12	11	10	9	8	…	4	3	2	1
Dynamic 90%-expectile risk										
Writer’s DRM	RL	0.77	0.73	0.69	0.65	0.62	…	0.45	0.38	0.29	0.23
	DP	0.75	0.71	0.68	0.65	0.61	…	0.43	0.38	0.31	0.23
Buyer’s DRM	RL DP	-0.22 -0.23	-0.21 -0.22	-0.20 -0.21	-0.19 -0.20	-018∙^^ -0.18	…	-0.11 -0.12	-0.09 -0.11	-0.07 -0.08	-0.05 -0.06
Static 90%-expectile risk										
Writer’s SRM	ED	0.55	0.54	0.54	0.53	0.53	…	0.48	0.46	0.41	0.31
Writer’s DRM	ED	0.56	0.54	0.52	0.50	0.47	…	0.36	0.33	0.29	0.24
Buyer’s SRM	ED	-0.35	-0.33	-0.30	-0.27	-^-023~~∙^^^^	-0.09	-0.07	-0.07	-0.06
Buyer’s SRM	ED	-0.36	-0.34	-0.32	-0.30	-0.28	…	-0.18	-0.14	-0.11	-0.06
Equal risk prices with DRM										
True ERP		0.49	0.47	0.45	0.42	0.40	…	0.28	0.24	0.19	0.14
DRM	RL	0.50	0.47	0.45	0.42	^^040	∙^^^^	0.28	0.24	0.18	0.14
SRM	RL	0.49	0.46	0.44	0.43	0.40	…	0.30	0.27	0.24	0.22
t Estimation (Est.) is either made based on reinforcement learning (RL), discretized dynamic programming
(DP), or the empirical distribution (ED).
months to 1 months. One can observe that the risk of the writer decreases monotonically for options
of shorter maturities, whereas the risk of the buyer increases monotonically. This is consistent
with the fact that there is less uncertainty for a shorter hedging horizon, which favors the writer’s
risk exposure more than the buyer’s when considering an at-the-money option. This also provides
the evidence that the DRM policies, albeit only trained based on the longest time to maturity, i.e.
one year, can be well applied to hedge options with shorter time to maturity and be used to draw
consistent conclusion. Another important observation one can make is that the RL based out-of-
sample dynamic risk estimate is generally very close to the DP based estimate across all conditions.
Table 1 also reports the out-of-sample static risk for both SRM policies and DRM policies. The
results are interesting and perhaps surprising. First, the DRM policies outperform SRM policies in
terms of static risk exposure for short maturities, even though they were trained using a different
risk measure. Second, unlike with DRM, we observed at other risk levels (see Figure 6(e) and (f)
in Appendix) that the static risk of SRM policies for the seller (resp. buyer) can increase (resp.
decrease) when hedging an option with shorter maturity. The possibility that a seller’s policy may
actually increase risk when applied to an option with shorter maturity is clearly problematic here as
it is inconsistent with the fact that there is less uncertainty (and lower expected value) regarding the
payout of such options. Both observed phenomenon are consequences of the fact that SRM violates
the time consistency property. We suspect that the possibility that SRM policies may not account
properly for risk aversion at some future time point or for other range of option maturities should
seriously hinder their use in practice.
Finally, Table 1 reports the equal risk prices calculated based on RL based out-of-sample dynamic
risk estimate and based on the discretized DP (referred as True ERP).5 One first confirms that the RL
based estimate is of high quality, with a maximum approximation error of 0.01 over all maturities.
Moreover, we can see that the prices for the SRM polices are generally higher than the prices for
the DRM polices, perhaps due to the fact that it is the writer that benefits most from the improved
DRM policy than the buyer, as he is more exposed to tail risks in this transaction. We further refer
the reader to section B.2 of the appendix for additional results regarding the performance of SRM
and DRM in this vanilla option setting.
4.4 Basket option Hedging and Pricing
In our second set of experiments, we extend the application of ERP pricing framework to the case
of basket options where traditionnal DP solution schemes are not computationally tractable. In
5Note that in a real data-driven setting, the ERP could either be estimated using the in-sample trained critic
network, or by calculating our RL based estimate using some freshly reserved data to reduce statistical biases.
8
Under review as a conference paper at ICLR 2022
particular, we consider an at-the-money basket option with the strike price of 753$ on five underlying
assets: AAPL, AMZN, FB, JPM, and GOOGL, where the option payoff is determined by the average
price of the underlyings. In this section, dynamic risk is only estimated using the RL based estimator
defined in Section 4.3, given that exact DP resolution has become intractable.
Table 2: The out-of-sample dynamic and static 90%-expectile risk imposed to the two sides of basket
at-the-money call options. Associated ERPs under the DRM are also compared.
		Time to maturity								
Policy	EStJ	12	11	10	9	8	…	4	3	2	1
Dynamic 90%-expectile risk										
Writer’s DRM	RL	3.92	3.62	3.38	3.15	2.95	…	2.00	1.70	1.39	1.10
Buyer’s DRM	RL	-0.48	-0.49	-0.51	-0.52	-0.50	∙∙^~~	-0.47	-0.37	-0.33	-0.29
Static 90%-expectile risk										
Writer’s SRM	ED	2.43	2.36	2.28	2.16	2.08	…	1.61	1.45	1.26	0.94
Writer’s DRM	ED	2.38	2.28	2.18	2.06	1.96	…	1.51	1.39	1.20	0.92
Buyer’s SRM	ED	-1.31	-1.24	-1.15	-1.01	-094∙^^	-0.56	-0.48	-0.36	-0.22
Buyer’s SRM	ED	-1.39	-1.32	-1.24	-1.13	-1.07	…	-0.66	-0.56	-0.40	-0.23
Equal risk prices with DRM										
DRM	RL	2.20	2.06	1.95	1.84	1.73	…	1.24	1.04	0.86	0.70
SRM	RL	2.23	2.10	2.01	1.91	1.79	…	1.21	1.03	0.92	0.82
t Estimation (Est.) is either made based on reinforcement learning (RL), or the empirical distribution (ED).
Table 2 presents the dynamic risk obtained from training the DRM policy for a one year maturity
option and applying it on the test data for maturity ranging from 1 to 12 months. Similar to the
vanilla option case, the dynamic risk of the writer is monotonically decreasing as we get closer
to the maturity of the option, while for the writer the monotonic behavior seems to be slightly
perturbed by estimation error. The table also compares the static risk under DRM and SRM. One
can first recognize the same monotone convergence to zero of the two sides of the options. However,
contrary to the case of the vanilla option, the difference between the static risk performance of
DRM and SRM policies are rather similar for all maturity times. It therefore appears that in these
experiments with a basket option, both SRM and DRM produce more similar polices. One possible
reason could be that the range of “optimal” risk averse investment plans, whether using DRM or
SRM, is more limited. Indeed, while for the vanilla option, we observed that the optimal policies
generated investments in the range [0, 1] and [-1, 0] for the writer and the buyer respectively, for
the basket option we observed wealth allocations that are more concentrated around 0.20 (i.e. the
uniform portfolio known for its risk hedging properties) and -0.20 for each of the 5 assets asset
respectively. Finally, Table 2 presents the equal risk prices computed based on our RL based out-of-
sample dynamic risk estimator. Once again, the higher ERP price for the SRM policy are notable,
which again can be attributed to the better performing (in terms of dynamic risk) hedging policy
produced by ACRL for the DRM, compared to the policy produced by AORL for the SRM. Further
details are presented in section B.3 of the Appendix.
5 Conclusion
Motivated by the application of ERP, in this paper we considered solving risk averse MDP problems
formulated based on dynamic expectile risk measures, and proposed a novel ACRL algorithm that
extends the model-free off-policy deterministic ACRL algorithm to a general finite horizon risk-
averse MDP setting. In comparison to existing model-free deep RL methods for solving risk-averse
MDP formulated based on dynamic risk measures, our method is more amenable to practical im-
plementation, allowing for tackling real applications such as the ERP problem. Indeed, as a natural
risk-averse extension of the popular model-free DDPG, our method can easily accommodate any
finite horizon MDP applications solved by DDPG. More in-depth studies of these other applications
are left for future work. The extension of our method to an infinite horizon MDP setting is also
worth investigating further. Finally, the exploration of our method to accommodate other utility-
based shortfall risk measures should also be of great interest for future study.
9
Under review as a conference paper at ICLR 2022
References
Fabio Bellini and Valeria Bignozzi. On elicitable risk measures. Quantitative Finance, 15(5):725-
733, 2015.
Dimitris Bertsimas, Leonid Kogan, and Andrew W Lo. Hedging derivative securities and incomplete
markets: an ^-arbitrage approach. Operations research, 49(3):372-397, 2001.
Alexandre Carbonneau and Frederic Godin. Equal risk pricing of derivatives with deep hedging.
Quantitative Finance, pp. 1-16, 2020.
Alexandre Carbonneau and Frederic Godin. Deep equal risk pricing of financial derivatives with
multiple hedging instruments. arXiv preprint arXiv:2102.12694, 2021.
Dotan Di Castro, J. Oren, and Shie Mannor. Practical risk measures in reinforcement learning.
ArXiv, abs/1908.08379, 2019.
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR optimization in MDPs. Ad-
vances in neural infor- mation processing systems, abs/1406.3339:3509-3517, 2014.
Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. In Proceedings of the
29th International Coference on International Conference on Machine Learning, ICML’12, pp.
179-186, Madison, WI, USA, 2012. Omnipress.
Ivan Guo and Song-Ping Zhu. Equal risk pricing under convex trading constraints. Journal of
Economic Dynamics and Control, 76:136-151, 2017.
Audrey Huang, Liu Leqi, Zachary C. Lipton, and Kamyar Azizzadenesheli. On the convergence and
optimality of policy gradient for markov coherent risk, 2021.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Saeed Marzban, Erick Delage, and Jonathan Yumeng Li. Equal risk pricing and hedging of financial
derivatives with convex risk measures. arXiv preprint arXiv:2002.02876, 2020.
L.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs.
Advances in neural infor- mation processing systems, abs/1406.3339:252-260, 2013.
Alexander Shapiro. Interchangeability principle and dynamic equations in risk averse stochastic
programming. Operations Research Letters, 45(4):377-381, 2017.
Yun Shen, Michael J. Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement
learning. Neural Computation, 26(7):1298-1328, 2014.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387-395. PMLR, 2014.
Rahul Singh, Qinsheng Zhang, and Yongxin Chen. Improving robustness via risk averse distribu-
tional reinforcement learning. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A.
Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger (eds.), Proceedings of the 2nd
Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine
Learning Research, pp. 958-968, 2020.
Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for co-
herent risk measures. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
Nuria Armengol Urpi, Sebastian Curi, and Andreas Krause. Risk-averse offline reinforcement learn-
ing. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021.
10