Under review as a conference paper at ICLR 2022
Escaping Saddle Points in Nonconvex Minimax
Optimization via Cubic-Regularized Gradient
Descent-Ascent
Anonymous authors
Paper under double-blind review
Ab stract
The gradient descent-ascent (GDA) algorithm has been widely applied to solve
nonconvex minimax optimization problems. However, the existing GDA-type
algorithms can only find first-order stationary points of the envelope function of
nonconvex minimax optimization problems, which does not rule out the possibility
to get stuck at SuboPtimal saddle points. In this paper, We develop Cubic-GDA - the
first Newton-type GDA algorithm for escaping strict saddle points in nonconvex-
strongly-concave minimax optimization. Specifically, the algorithm uses gradient
ascent to estimate the second-order information of the minimax objective function,
and it leverages the cubic regularization technique to efficiently escape the strict
saddle points. Under standard smoothness assumptions on the objective function,
We shoW that Cubic-GDA admits an intrinsic potential function Whose value mono-
tonically decreases in the minimax optimization process. Such a property leads
to a desired global convergence of Cubic-GDA to a second-order stationary point
at a sublinear rate. Moreover, We analyze the convergence rate of Cubic-GDA in
the full spectrum of a gradient dominant-type nonconvex geometry. Our result
shoWs that Cubic-GDA achieves an orderWise faster convergence rate than the
standard GDA for a Wide spectrum of gradient dominant geometry. Our study
bridges minimax optimization With second-order optimization and may inspire neW
developments along this direction.
1 Introduction
Nonconvex minimax optimization is a popular optimization frameWork that has broad applications
in modern machine learning, including game theory (Ferreira et al., 2012), generative adversarial
netWorks (GoodfelloW et al., 2014), adversarial training (Sinha et al., 2017), reinforcement learning
(Qiu et al., 2020; Ho and Ermon, 2016; Song et al., 2018), etc. A standard nonconvex minimax
optimization problem is shoWn beloW, Where f is a smooth nonconvex function in x.
min max f(x, y).
x∈Rm y∈Rn
(P)
In the existing literature, many optimization algorithms have been developed to solve different types
of minimax problems. Among them, a simple and popular algorithm is the gradient descent-ascent
(GDA), Which alternates betWeen a gradient descent update on x and a gradient ascent update on
y in each iteration. Specifically, the global convergence of GDA has been established for minimax
problems under various types of global geometries, such as convex-concave-type geometry (f is
convex in X and concave in y) (Nedic and Ozdaglar, 2009; Du and Hu, 2019; Mokhtari et al.,
2020; Zhang and Wang, 2021), bi-linear geometry (Neumann, 1928; Robinson, 1951) and Polyak-
匕OjaSieWiCZ geometry (Nouiehed et al., 2019; Yang et al., 2020), yet these geometries are not satisfied
by general nonconvex minimax problems. Recently, many studies proved the convergence of GDA in
nonconvex minimax optimization for both nonconvex-concave problems (Lin et al., 2020; Nouiehed
et al., 2019; Xu et al., 2020d) and nonconvex-strongly-concave problems (Lin et al., 2020; Xu et al.,
2020d; Chen et al., 2021). In these studies, it has been shoWn that GDA converges sublinearly to a
stationary point Where the gradient of a certain envelope function of the minimax problem vanishes.
1
Under review as a conference paper at ICLR 2022
Although GDA can find first-order stationary points of nonconvex minimax problems, such a type
of convergence guarantee does not rule out the possibility that GDA may get stuck at suboptimal
saddle points of the envelope function, which are well known to be the major challenge for training
high-dimensional machine learning models (Dauphin et al., 2014; Jin et al., 2017; Zhou and Liang,
2018). On the other hand, while numerous algorithms have been developed for escaping saddle points
in conventional nonconvex optimization, e.g., first-order algorithms (Ge et al., 2015; Jin et al., 2017;
Carmon and Duchi, 2016; Liu and Yang, 2017) and second-order algorithms (Nesterov and Polyak,
2006; Agarwal et al., 2017; Yue et al., 2019; Zhou et al., 2018), such a type of algorithm has not been
developed for escaping saddle points in nonconvex minimax optimization. Therefore, we want to ask
the following fundamental questions.
• Q: How to develop a provably convergent Newton-type GDA algorithm that can effectively escape
saddle points in nonconvex minimax optimization? How fast it converges?
Developing and analyzing such an algorithm is nontrivial due to the following reasons: 1) we need to
have a good understanding and characterization of both the first-order and second-order information
of nonconvex minimax problems; 2) we need to develop a computationally feasible and efficient GDA
algorithm that can leverage the local curvature of the function to escape saddle points; 3) we aim to
develop a unified analysis framework that can characterize the convergence rate of this algorithm
under different types of nonconvex geometry of the minimax problem.
In this paper, we provide comprehensive answers to these questions. We develop the first Newton-type
GDA algorithm that escapes strict saddle points and converges to second-order stationary points
in nonconvex-strongly-concave minimax optimization. We also characterize the global and local
convergence rates of this algorithm under various types of nonconvex geometry. We summarize our
contributions as follows.
1.1	Our contributions
We consider the minimax optimization problem (P), where f is a twice-differentiable and nonconvex-
strongly-concave function, and its gradient and Jacobian matrices are Lipschitz continuous. Define an
envelope function Φ(x) := maxy∈Rn f (x, y). The existing GDA algorithms can only find first-order
stationary points that satisfy VΦ(χ*) = 0. In this paper, We develop a Newton-type GDA algorithm
that converges to second-order stationary points of the nonconvex minimax problem (P).
Specifically, We propose CUbic-GDA - a Newton-type GDA algorithm that leverages the classical
cubic regularization technique to escape saddle points. Different from the standard cubic regulariza-
tion algorithm that Uses the Hessian information of the fUnction, the Hessian of Φ(x) is not directly
available in nonconvex minimax optimization, and hence we develop a rigoroUs and compUtationally
feasible scheme in CUbic-GDA to estimate the Hessian.
We stUdy the global convergence property of CUbic-GDA in general nonconvex-strongly-concave
optimization. Specifically, we show that CUbic-GDA admits an intrinsic potential fUnction H(x, x0, y)
(see Proposition 2), which monotonically decreases along the trajectory of CUbic-GDA. Based on the
monotonicity of this potential fUnction, we show that every limit point of the parameter seqUence
{xt }t generated by CUbic-GDA is a second-order stationary point of the minimax problem.
We fUrther analyze the aymptotic convergence rates of CUbic-GDA Under a broad spectrUm of the
local nonconvex LOjaSieWiCz gradient geometry. In this case, we show that Cubic-GDA converges to
a UniqUe limit point, which is a second-order stationary point. Moreover, as the geometry parameter
increases (i.e., sharper local geometry), the convergence rate of Cubic-GDA accelerates from sub-
linear convergence up to super-linear convergence, as we summarize in Table 1 below. In particular,
these convergence rates are orderwise faster than those of the standard GDA under the same type of
nonconvex geometry (Chen et al., 2021) 1.
1Note that the geometry parameter θ in this paper corresponds to i-^ in (Chen et al., 2021).
2
Under review as a conference paper at ICLR 2022
Table 1: Comparison of potential function value gap H (Z) - H * convergence rates of CubiC-GDA and GDA under different parameterizations of LojasieWicz gradient geometry.		
Geometry parameter	GDA (Chen et al., 2021)	Cubic-GDA (This paper)
θ ∈ (2, +∞)	Super-linear convergence	Super-linear convergence
θ=2	Linear convergence	Super-linear convergence
θ ∈ (2,2)	Sub-linear convergence	Super-linear convergence
θ = 3 	IZ	2		Sub-linear convergence	Linear convergence
	θ ∈(1,2)	Sub-linear convergence	Sub-linear convergence
1.2	Related work
Deterministic GDA algorithms: Many studies characterized the convergence of GDA in nonconvex
minimax optimization. Specifically, Lin et al. (2020); Nouiehed et al. (2019); Xu et al. (2020d)
studied the convergence of GDA in the nonconvex-concave setting whereas Lin et al. (2020); Xu et al.
(2020d) focused on the nonconvex-strongly-concave setting. In these general nonconvex settings,
it is shown that GDA converges to a certain stationary point at a sublinear rate. Recently, Chen
et al. (2021) proved the parameter convergence of proximal-GDA in regularized nonconvex-strongly-
concave optimization under the KUrdyka-LojasieWicz geometry. The convergence rates obtained
there are orderwise slower than that of Cubic-GDA. Yang et al. (2020) studied an alternating gradient
descent-ascent (AGDA) algorithm in Which the gradient ascent step uses the current variable xt+1
instead of xt . Xu et al. (2020d) studied an alternating gradient projection algorithm Which applies
`2 regularizer to the local objective function of GDA folloWed by projection onto the constraint
sets. Daskalakis and Panageas (2018); Mokhtari et al. (2020); Zhang and Wang (2021) analyzed
optimistic gradient descent-ascent (OGDA) Which applies negative momentum to accelerate GDA.
Mokhtari et al. (2020) also studied an extra-gradient algorithm Which applies tWo-step GDA in
each iteration. Nouiehed et al. (2019) studied multi-step GDA Where multiple gradient ascent steps
are performed, and they also studied the momentum-accelerated version. Cherukuri et al. (2017);
Daskalakis and Panageas (2018); Jin et al. (2020) studied GDA in continuous time dynamics using
differential equations. Adolphs et al. (2019) analyzed a second-order variant of the GDA algorithm.
In a concurrent Work (Luo and Chen, 2021), the authors proposed and studied the same Cubic-GDA
algorithm. They characterize the computation complexity under a special type of inexactness that
approximates the inverse Jacobian using matrix Chebyshev polynomials. As a comparison, this study
focuses on analyzing the global and local convergence properties of Cubic-GDA.
Stochastic GDA algorithms: Lin et al. (2020); Yang et al. (2020) analyzed stochastic GDA and
stochastic AGDA, Which are direct extension of GDA and AGDA to the stochastic setting. Variance
reduction techniques have been applied to stochastic minimax optimization, including SVRG-based
(Du and Hu, 2019; Yang et al., 2020), SPIDER-based (Xu et al., 2020c), SREDA (Xu et al., 2020b),
STORM (Qiu et al., 2020) and its gradient free version (Huang et al., 2020). Xie et al. (2020) studied
the complexity loWer bound of first-order stochastic algorithms for finite-sum minimax problem.
Cubic regularization (CR): CR algorithm dates back to (GrieWank, 1981), Where global convergence
of the algorithm is established. In Nesterov and Polyak (2006), the authors analyzed the convergence
rate of CR to second-order stationary points for nonconvex optimization. In (Nesterov, 2008), the
authors established the sub-linear convergence of CR for solving convex smooth problems, and they
further proposed an accelerated version of CR With improved sub-linear convergence. Recently, Yue
et al. (2019) studied the asymptotic convergence properties of CR under the error bound condition,
and established the quadratic convergence of the iterates. Recently, Hallak and Teboulle (2020)
proposed a frameWork of tWo directional method for finding second-order stationary points in general
smooth nonconvex optimization. This main idea of the algorithm is to search for a feasible direction
toWard the solution and is not based on cubic regularization. Several other Works proposed different
methods to solve the cubic subproblem of CR, e.g., (AgarWal et al., 2017; Carmon and Duchi, 2016;
Cartis et al., 2011b). Another line of Work aimed at improving the computation efficiency of CR by
solving the cubic subproblem With inexact gradient and Hessian information. In particular, Ghadimi
et al. (2017) proposed an inexact CR for solving convex problem. Also, Cartis et al. (2011a) proposed
3
Under review as a conference paper at ICLR 2022
an adaptive inexact CR for nonconvex optimization, whereas Jiang et al. (2017) further studied
the accelerated version for convex optimization. Several studies explored subsampling schemes to
implement inexact CR algorithms, e.g., (Kohler and Lucchi, 2017; Xu et al., 2020a; Zhou and Liang,
2018; Wang et al., 2018).
2	Problem Formulation and Preliminaries
In this section, we introduce the problem formulation and present some preliminary results that will
be used in the analysis.
Notation: For notation simplicity, We denote Vιf, V2f as the gradients with respect to the first and
the second input arguments of f, respectively. We also denote V11 f, V22f as the Jacobian matrices
where the second-order derivatives are taken over the first and second arguments of f, respectively.
Moreover, we denote V12f as the Jacobian matrix where the second-order derivative is taken over
the first argument of f and followed by the second argument, and V21f is defined in a similar way.
We consider the minimax optimization problem (P) that satisfies the following standard assumptions.
Assumption 1. The minimax optimization problem (P) satisfies:
1.	Function f (∙, ∙) is Li -smooth and function f (x, ∙) is μ-strongly ConcaVefor all fixed X;
2.	The Jacobian matrices V11f, V12f, V21f, V22f are L2-Lipschitz;
3.	Function Φ is bounded below and has compact sub-leVel sets.
To elaborate, item 1 considers the class of nonconvex-strongly-concave functions f that has been
widely studied in the minimax optimization literature (Lin et al., 2020; Jin et al., 2020; Xu et al.,
2020d; Lu et al., 2020). Items 2 assumes that the block Jacobian matrices of f are Lipschitz, which
is a standard assumption for analyzing many second-order optimization algorithms (Nesterov and
Polyak, 2006; Agarwal et al., 2017; Yue et al., 2019). Moreover, item 3 guarantees that the minimax
problem has at least one solution. By strong concavity of f (x, ∙), it is clear that the maximizer
y*(x) := argmaXy∈Rn f (x, y) is unique for every X ∈ Rm. In particular, if x* is a second-order
stationary point of Φ(x), then (x*,y*(x*)) is the desired solution of the minimax problem (P).
Define an envelope function Φ(x) := maxy∈Rn f(x, y). Then the minimax problem (P) is equivalent
to the minimization problem. minx∈Rm Φ(x), where Φ(x) = maxy∈Rn f(x, y). As we show in item
2 of Proposition 1 later, this envelope function Φ(x) is smooth and nonconvex. The existing GDA
algorithms can only find first-order stationary points of the minimax problem that satisfy VΦ(x*) = 0.
In this paper, we aim to develop a provably convergent algorithm that can find second-order stationary
points x* of the function Φ(x) that satisfy the following set of conditions.
(Second-order stationary): VΦ(x*) = 0,	V2Φ(x*)	0.
In the existing literature, many optimization algorithms have been developed for finding second-
order stationary points in conventional nonconvex minimization problems. This includes first-order
algorithms (Ge et al., 2015; Jin et al., 2017; Carmon and Duchi, 2016; Liu and Yang, 2017) and
second-order algorithms (Nesterov and Polyak, 2006; Agarwal et al., 2017; Yue et al., 2019; Zhou
et al., 2018). However, these algorithms are not directly applicable to solve the problem (P’), as the
function Φ(x) involves a special maximization structure and hence its specific function form Φ as
well as the gradient VΦ and Hessian V2Φ are not available in practice. Instead, our algorithm design
can only leverage information of the bi-variate function f .
Next, we present some important properties regarding the gradient and Jacobian matrices of the
functions f (x, y) and Φ(x). Throughout, we denote K = Lι∕μ as the condition number.
Proposition 1. Let Assumption 1 hold. Then, the following statements hold.
1.	Mapping y* (x) is κ-Lipschitz continuous;
2.	Function Φ(x) is L1(1 + κ)-smooth and VΦ(x) = V1f(x, y* (x));
3.	Define G(x, y) = V11f(x, y) - V12f (x, y)[V22f (x, y)]-1V21f (x, y). Then, G is a Lipschitz
mapping with constant LG = L2(1 + κ)2, i.e., kG(x0, y0) - G(x, y)k ≤ LGk(x0, y0) - (x, y)k;
4
Under review as a conference paper at ICLR 2022
4.	The Hessian of Φ satisfies V2Φ(x) = G(x, y*(x)), which is Lipschitz continuous with constant
LΦ = LG(1 + κ) = L2(1 + κ)3.
The first two items characterize the gradient of the envelope function Φ in terms of the partial gradient
of the bi-variate objective function f . They are proved in the previous work (Lin et al., 2020) and we
include them for completeness. On the other hand, the last two items further characterize the Hessian
of Φ in terms of the block Jacobian matrices of f . As we present in the next section, the Lipschitz
continuous Hessian V2Φ(χ) allows Us to develop a cubic-regularization-based algorithm for finding
second-order stationary points. We also note that the proof of items 3 & 4 are not trivial. Specifically,
we need to first develop bounds for the spectrum norm of the block Jacobian matrices in Lemma 1
(see the first page of the appendix), which helps prove the Lipschitz continuity of the G mapping
in item 3. Moreover, we leverage the optimality condition of f (x, ∙) to derive an expression for the
maximizer mapping y* (x) (see (15) in the appendix), which is used to further prove item 4.
3 Cubic-GDA: Cubic-Regularized Gradient Descent-Ascent
In this section, we propose a new Gradient Descent-Ascent (GDA) algorithm that leverages the
cubic regularization technique (Nesterov and Polyak, 2006) to escape strict saddle points and find
second-order stationary points of the nonconvex minimax problem (P).
Our algorithm design is inspired by the conventional cubic regularization algorithm (Nesterov and
Polyak, 2006). Specifically, to find a second-order stationary point of the envelope function Φ(x), the
conventional cubic regularization algorithm would perform the following iterative update.
xt+1 ∈ arg min VΦ(xt)>(x — Xt) + 1(x — xt)> V2Φ(xt)(x — Xt) + 1— ∣∣x — xt∣∣3,	(1)
x	2	6ηx
where ηx > 0 is a proper learning rate. However, due to the special maximization structure of
Φ, its gradient and Hessian have complex formulas (see Proposition 1) that involve the mapping
y* (x), which cannot be computed exactly in practice. Hence, we aim to develop a new algorithm to
efficiently compute approximations of the gradient and Hessian of Φ and use them to perform the
cubic regularization update.
To perform the cubic regularization update in eq. (1), we need to compute VΦ(Xt) = V1f(Xt, y*(Xt))
and V2Φ(Xt) = G(Xt, y*(Xt)) (by Proposition 1), both of which depend on the maximizer y*(Xt)
of the function f (xt, ∙). Since f (xt, ∙) is strongly-concave, we can run Nt iterations of gradient
ascent to obtain an approximated maximizer yeNt ≈ y*(Xt), and then approximate VΦ(Xt), V2Φ(Xt)
using V1f(xt, yeNt) and G(xt, yeNt), respectively. Intuitively, these are good approximations due to
two reasons: (i) yeNt converges to y*(xt) at a fast linear convergence rate; and (ii) both V1f and
G are shown in Proposition 1 to be Lipschitz continuous in their second argument. We refer to
this algorithm as Cubic-Regularized Gradient Descent-Ascent (Cubic-GDA), and summarize its
update rule in Algorithm 1 below.
Algorithm 1 Cubic-Regularized Gradient Descent-Ascent (Cubic-GDA)
Input: Initialize x°, yo and learning rates η,,ηy.
for t = 0,1, 2, . . . , T — 1 do
Initialize ye0 = yt .
for k = 0, 1, 2, . . . , Nt — 1 do
I ek+ι = ek + ηyV2f(χt,ek).
end
Set yt+1 = yeNt and compute G(xt, yt+1) as follows:
G(xt,yt+1) = V11f(xt, yt+1) — V12f(xt, yt+1)[V22f(xt, yt+1)]-1V21f(xt, yt+1).
χt+ι ∈ argminχ v 1 f(χt, yt+1)>(χ — Xt) + 1 (χ — Xt)TG(Xt, yt+I)(X — Xt) + 6η1χ ∣∣χ — χt∣∣3.
end
Output: XT , yT .
We further comment on the implementation of Cubic-GDA. We note that the Cubic-GDA updates in
Algorithm 1 can be implemented in a computation efficient way. First, note that Cubic-GDA involves
5
Under review as a conference paper at ICLR 2022
an inner loop that performs gradient ascent updates. To guarantee the convergence of the algorithm, we
prove in the next section that the number of inner iterations Nt only needs to be kept at logarithm scale.
Therefore, a few number of inner iterations suffice to guarantee convergence in practice. Second, the
cubic regularization sub-problem can be efficiently solved by the gradient descent algorithm (Carmon
and Duchi, 2016), and it involves computation of only Jacobian-vector product that can be efficiently
computed by the existing machine learning platforms such as TensorFlow (Abadi et al., 2015) and
PyTorch (Paszke et al., 2019). In particular, to compute the Hessian-vector product G(x, y)v for
any vector v, one needs to compute the Jacobian-Vector product Vnf (x, y)v and the matrix-vector
product V12f (x, y)[V22f (x, y)]-1V21f (x, y)v. We note that this matrix-vector product term can
be computed as follows: first compute the Jacobian-vector product b = V21 f (x, y)v; Then, solve
the invertible linear system V22f(x, y)u = b using any standard solver (e.g., conjugate gradient
method), which involves iteratively computing Jacobian-vector products V22f(x, y)w for some
vector w; Finally, compute the Jacobian-vector product V12f(x, y)u. Hence, one can call multiple
Jacobian-vector product oracles to solve the cubic regularization sub-problem.
4 Global Convergence Properties of Cubic-GDA
In this section, we study the global convergence properties of Cubic-GDA. Importantly, our analysis
is based on characterizing an intrinsic potential function of the Cubic-GDA algorithm in nonconvex
minimax optimization.
Recall that our goal is to find a second-order stationary point of the function Φ(x). Our next result
shows that Cubic-GDA admits an intrinsic potential function that monotonically decreases in the
optimization process. The proof of is included in Appendix B.
Proposition 2. Let Assumption 1 hold. Define the following potential function
H(x,x0,y) := Φ(x) + L2κ3kx0 - xk3 +4L2∣∣y 一 y*(x)k3,
and denote Ht := H(xt, xt-1, yt+1). Choose Nt ≥
max(ln 2, ln[L1 k^2f (xt,yt) k/(L2μ)]-2 ln ∣∣xt-xt-ι k)
ln[κ/(κ-1)]
and learning rates η, ≤ 2^K, ηy ≤ L++μ. Then, the sequences {xt, yt}t generated by Cubic-GDA
satisfy, for all t = 0, 1, 2, ...
Ht+ι ≤ Ht-L2κ3kxt+ι — xtk3-L2kyt+ι — y*(Xt)Il3.	⑵
Consequently, it holds that
lim Ixt+1 - xt I = 0,
t→∞
lim kyt+ι - ytk = 0, lim kyt - y*(xt)k = 0.
t→∞	t→∞
Remark 1. We note that the above key result can also be established for an inexact version of Cubic-
GDA, which formulates the cubic subproblem with a general inexact gradient pt ≈ V1f(xt, yt+1)
and inexact Jacobian Pt ≈ G(xt, yt+1) that satisfy the conditions
Ipt	- V1f(xt,yt+1)I	≤ O(Ixt+1 - xtI2),	IPt	- G(xt,yt+1)I	≤ O(Ixt+1 - xtI).
These inexactness conditions are widely studied in the existing literature Cartis et al. (2011b;a).
Under these inexact conditions, our proof of the above proposition remains unchanged, except that
the coefficients of the term Ixt+1 - xtI3 would be slightly different.
Proposition 2 reveals that Cubic-GDA admits an intrinsic potential function H, which is the objective
function Φ(x) regularized by two cubic terms ∣∣x0 一 x∣∣3, ∣∣y — y*(x)∣∣3. SUCh a potential function is
closely connected to the optimization goal. Specifically, consider a desired case where xt converges
to a certain second-order stationary point x* and yt converges to y* (x*), it is clear that the potential
function Ht would converge to the corresponding function value Φ(x*). Hence, minimizing the
function Φ is equivalent to minimizing the potential function H . More importantly, Proposition 2
shows that this potential function is monotonically decreasing along the optimization path of Cubic-
GDA, implying that the algorithm continuously makes optimization progress. By leveraging this
property of the potential function, we are able to show that the parameter sequences generated by
Cubic-GDA are asymptotically stable, i.e., xt+1 一 xt → 0, yt → y* (xt).
Remark 2. In each outer iteration t, we set the total number of inner gradient ascent iterations
Nt based on ∣V2f(xt, yt)∣ and ∣xt 一 xt-1∣. Note that both of these two quantities can be easily
computed right after iteration t 一 1.
6
Under review as a conference paper at ICLR 2022
Based on Proposition 2, we are able to prove the convergence of {xt }t to a certain second-order
stationary point, which we formally present in the next theorem. The proof is included in Appendix C.
Theorem 1 (Global convergence). Under the same conditions as those of Proposition 2, the Cubic-
GDA has the following global convergence properties.
1.	Thefunction value SeqUence {Φ(xt)}t converges to a finite limit H * > 一∞;
2.	The generated sequences {xt}t, {yt}t are bounded and have a compact sets of limit points.
3.	Every limit point x* of {xt}t is a SeCOnd-Order stationary point of Φ, i.e., VΦ(x*) = 0,
V2Φ(x*)占 0, and satisfies Φ(x*) = H *.
The above theorem establishes the global convergence properties of Cubic-GDA. Specifically, item
1 shows that the function value sequence {Φ(xt)}t converges to a finite limit H*, which is also
the limit of the potential function sequence {Ht}t. Moreover, items 2 & 3 further show that all
the limit points of {xt}t are second-order stationary points of the minimax problem, at which the
function Φ achieves the constant value H*. These results show that Cubic-GDA is guaranteed to find
second-order stationary points in nonconvex minimax optimization.
By further leveraging the potential function characterized in Proposition 2, we obtain the following
global convergence rate of Cubic-GDA to a second-order stationary point. The proof is included
in Appendix D. Throughout, we adopt the following standard measure of second-order stationary
introduced in (Nesterov and Polyak, 2006).
μ(x) = max ]:
kVΦ(x)k
_________________________-λmin(V2Φ(x))[
1∕(2ηχ) + 5L2κ3 + 4L2κ2∕Lι, 1∕(2ηx) + 4L2κ2∕Lι J
Intuitively, a smaller μ(χ) means that the point X is closer to being second-order stationary.
Theorem 2 (Global convergence rate). Under the same conditions as those of Proposition 2, the
Cubic-GDA converges at the following rate Jorall T ≥ HoTL之KRm φ(x).
min μ(χt) ≤ (HO-^nfx∈Rm φ3)1/3.
o≤t≤τ-ιμ t	TL2κ3∕3
The above theorem shows that the first-order stationary measure kVΦ(xt)k converges at a sublinear
rate O(T-3), and the second-order stationary measure -λmi∏(V2Φ(χ)) converges at a sublinear
rate O(T-3). Both results match the convergence rates of the cubic regularization algorithm for
nonconvex minimization (Nesterov and Polyak, 2006). Therefore, by leveraging the curvature of the
approximated Hessian matrix G(xt, yt+1), Cubic-GDA is able to escape strict saddle points of Φ at a
fast rate.
We note that the proof of the global convergence results in Theorems 1 and 2 are critically based on
the intrinsic potential function H that we characterized in Proposition 2. We elaborate our technical
contribution as follows.
•	First, to identify the potential function, we need to characterize the per-iteration progress induced
by the cubic regularization step. However, the cubic subproblem in Cubic-GDA is constructed by
an inexact gradient V1f(xt, yt+1) and Hessian matrix G(xt, yt+1). Therefore, we need to properly
choose the number of inner gradient ascent iterations Nt to control the estimation error of both the
gradient and Hessian approximations at a desired level.
•	Due to the inexactness of the gradient and Hessian matrix, the cubic regularization update of
Cubic-GDA does not lead to a monotonically decreasing function value Φ(xt), as opposed to the
original cubic regularization algorithm in nonconvex minimization (which uses exact gradient and
Hessian). Hence, we construct and identify a decreasing potential function H instead.
5 Convergence Analysis under Local Nonconvex Geometry
The (2) of Proposition 2 shows that Cubic-GDA has a special optimization dynamics and therefore
its convergence rate is expected to be different from that of the vanilla GDA in nonconvex minimax
7
Under review as a conference paper at ICLR 2022
optimization. In this section, we explore the convergence rates of Cubic-GDA under a broad spectrum
of local nonconvex geometries characterized by the LojasieWicz gradient inequality.
We first introduce the Lojasiewicz gradient geometry of a function h. Throughout, the point-to-set
distance is denoted as disto(x) := infu∈ω Ilx - u∣∣∙
Definition 1. A differentiable function h is said to satisfy the LGjasieWicz gradient geometry if for
every compact set Ω ofcriticalpoints on which h takes a constant value h。∈ R, there exist ε,λ> 0
such thatfor all x ∈ {z ∈ Rm : disto(z) < ε,hΩ < h(z) < hΩ + λ}, thefollowing condition holds.
h(x)- hΩ ≤ c∣Vh(x)kθ,	(3)
where c > 0 is a universal constant and θ ∈ (1, +∞) is the geometry parameter.
Intuitively, the Lojasiewicz gradient geometry is a gradient-dominant-type geometry that characterizes
the local geometry of a nonconvex function around the set of critical points. In particular, it generalizes
the Polyak-Lojasiewicz (PL) geometry that corresponds to the special case θ = 2 Lojasiewicz (1963);
Karimi et al. (2016). In fact, a generalized version of the Lojasiewicz gradient geometry has
been shown to hold for a large class of functions including sub-analytic functions, exponential
functions and semi-algebraic functions, which cover most of the nonconvex functions encountered
in machine learning applications (Zhou et al., 2016; Yue et al., 2019; Zhou and Liang, 2017; Zhou
et al., 2018). For example, consider the class of robust machine learning problems that involve
the minimax problem minθ maxξi 1 Pn=1 '(hθ(ξi),yi)2 — 2 ∣∣ξi — ai∣∣2. Here (xi,yi) is the i-th
data sample that includes, e.g., an image xi and its label yi, ξi denotes the adversarial image, hθ
is a classification model parameterized by θ, and ` denotes the loss function. Such a problem is
nonconvex-stongly-concave when λ is large. In particular, as elaborated in the appendix of (Bolte
et al., 2014), the envelop function Φ(x) := maχξi n PZi '(hθ(ξi),yi)2 一 λ l∣ξi 一 aik2 satisfies
the local Lojasiewicz gradient geometry if it is semi-algebraic, which holds if every sample loss
f(θ,ξi) ：= '(hθ(ξi),yi)2 - 2∣∣ξi - aik2 is semi-algebraic.
By (2) of Proposition 2 and (30) (proved in Appendix E), we show that the potential function H of
Cubic-GDA satisfies the following special optimization dynamics.
Ht+1 - Ht ≤ -O(Ilxt+1 - xtk3),	(4)
IlVHtIl ≤ O(Ilxt- xt-i∣∣2 + IIxt-I- xt-2∣∣2).	(5)
The above dynamics of Cubic-GDA involves higher-order terms than the dynamics of GDA, which
takes the form Ht+i - Ht ≤ -O(Ixt+i - xtI2) and IVHtI ≤ O(Ixt+i - xtI) (Chen et al., 2021).
Thus, it is expected that Cubic-GDA achieves a faster convergence rate than GDA. On the other hand,
compare with the dynamics of the cubic regularization algorithm (Zhou et al., 2018), the gradient
dynamic of Cubic-GDA in (5) involves an additional term Ixt-i - xt-2 I2 that depends on the history,
which is due to the inexact gradient and Hessian used in the cubic regularization step.
With the optimization dynamics in (4) and (5) and by leveraging the Lojasiewicz gradient geometry,
we are able to prove the following strengthened convergence result of Cubic-GDA.
Theorem 3. Let Assumption 1 hold and assume that the potential function H satisfies the local
Lojasiewicz gradient geometry. Choose the hyperparameters Nt, ηx, ηy in the same way as Proposi-
tion 2. Then, the sequences {(xt, yt)}t generated by Cubic-GDA have a unique limit point, which is
a second-order stationary point of Φ.
Recall that Theorem 1 only proves that every limit point of {xt}t is a second-order stationary point
of the minimax problem. Theorem 3 further strengthens Theorem 1 by showing that Cubic-GDA
converges to a unique second-order stationary limit point under the Lojasiewicz gradient geometry.
Next, we further study the asymptotic convergence rates of Cubic-GDA under different parameter
ranges of the Lojasiewicz gradient geometry. We first obtain the following function value convergence
rate result, which strengthens the convergence rate result established in Theorem 2. Throughout, t0
denotes a sufficiently large positive integer and C0 is a universal positive constant defined as
Co = √2，笃-2/3 (10L2κ + 丁 + 4L2 + 2ηχKK2)
The proof is included in Appendix F.
8
Under review as a conference paper at ICLR 2022
Theorem 4 (Funtion value convergence rate). Under the same conditions as those of Theorem 3, the
sequence of potential function {Ht}t converges to the limit H * at the following rates.
1.	Ifthe geometry parameter θ ∈ (2, ∞) ,then Ht ] H * SuPer-Iinearly as
Ht
- H* ≤ O exp
∀t ≥ t0 ;
2.	Ifthe geometry parameter θ = 3, then Ht ] H * linearly as
Ht- H * ≤ (1 + C0/2)-t-2t0, Vt ≥ t0;
3.	Ifthe geometry parameter θ ∈ (1, 2)), then Ht ] H * sub-linearly as
Ht — H * ≤ O ((t —10)-熹)，Vt ≥ t0.
(6)
(7)
(8)
Remark 3. We note that ifthe Lojasiewicz gradient geometry holds globally, then the above asymp-
totic convergence rates become global convergence rates.
The above theorem characterizes the convergence rates of the potential function of Cubic-GDA in the
full spectrum of θ of the local Lojasiewicz gradient geometry. Specifically, it shows that a larger θ
implies that the local geometry is sharper and hence leads to a faster convergence rate. In particular, as
we summarize in Table 1 in the introduction section, the convergence rate of Cubic-GDA is orderwise
faster than that of the vanilla GDA for a wide range of the parameter of the Lojasiewicz gradient
geometry. This demonstrates the advantage of leveraging higher-order information in nonconvex
minimax-optimization.
As a byproduct, we also obtain the following asymptotic convergence rates of the parameter se-
quences generated by Cubic-GDA under the Lojasiewicz gradient geometry. The proof is included in
Appendix G.
Theorem 5 (Parameter convergence rate). Under the same conditions as those of Theorem 3, the
sequences {xt, yt}t generated by Cubic-GDA converge to their limits x*, y*(x*) respectively at the
following rates.
1.	Ifthe geometry parameter θ ∈ (2, ∞), then (xt,yt) → (x*,y* (x*)) super-linearly as
max {∣∣xt - x*k, kyt - y*(x*)k} ≤θ(exp (— 3 (2θ) 2	)),	Vt ≥ t0;	(9)
2.	Ifthe geometry parameter θ = 2, then (xt, yt) → (x* ,y*(x*)) linearly as
max {kxt- x*k,kyt-	y*(x*)k} ≤o((1 +	Ch-t-2t0),	Vt	≥ t0；	(10)
3.	Ifthe geometry parameter θ ∈ (1, 2), then (xt, yt) → (x*,y*(x*)) sub-linearly as
2(θ-1)	2θ
∣∣χt - χ*k ≤ O((t —10)-ɪɪ), kyt -y*(χt)k ≤。((t-10厂3(3-2θ)), Vt ≥ t0. (11)
It can be seen that, similar to the convergence rate results of the function value sequence, the
convergence rate of the parameter sequence is also affected by the parameterization of the local
geometry.
6 Conclusion
In this paper, we take one step further toward improving the convergence guarantee of GDA-type
algorithms in nonconvex minimax optimization. We develop a Cubic-GDA algorithm that leverages
the second-order information and the cubic regularization technique to effectively escape strict saddle
points in nonconvex minimax optimization. Our key observation is that Cubic-GDA has an intrinsic
potential function that monotonically decreases in the optimization process, and this leads to a
guaranteed global convergence of the algorithm. Moreover, our convergence analysis shows that
Cubic-GDA achieves a faster convergence rate than the standard GDA for a wide spectrum of gradient
dominant-type nonconvex geometries. In the future study, we will develop stochastic variants of
Cubic-GDA to further improve its computation efficiency. Another interesting direction is to apply
momentum techniques to further accelerate the convergence of this algorithm.
9
Under review as a conference paper at ICLR 2022
References
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz,
R., Kaiser, L., Kudlur, M., Levenberg, J., Man6, D., Monga, R., Moore, S., Murray, D., Olah, C.,
Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015).
TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from
tensorflow.org.
Adolphs, L., Daneshmand, H., Lucchi, A., and Hofmann, T. (2019). Local saddle point optimization:
A curvature exploitation approach. In Proc. International Conference on Artificial Intelligence and
Statistics (AISTATS), pages 486-495.
Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., and Ma, T. (2017). Finding approximate local
minima faster than gradient descent. In Proc. Annual ACM SIGACT Symposium on Theory of
Computing (STOC), pages 1195-1199.
Bolte, J., Sabach, S., and Teboulle, M. (2014). Proximal alternating linearized minimization for
nonconvex and nonsmooth problems. Mathematical Programming, 146(1-2):459-494.
Carmon, Y. and Duchi, J. C. (2016). Gradient descent efficiently finds the cubic-regularized non-
convex Newton step. ArXiv: 1612.00547.
Cartis, C., Gould, N. I. M., and Toint, P. (2011a). Adaptive cubic regularization methods for
unconstrained optimization. part ii: worst-case function- and derivative-evaluation complexity.
Mathematical Programming, 130(2):295-319.
Cartis, C., Gould, N. I. M., and Toint, P. L. (2011b). Adaptive cubic regularization methods for
unconstrained optimization. part i : motivation, convergence and numerical results. Mathematical
Programming.
Chen, Z., Zhou, Y., Xu, T., and Liang, Y. (2021). Proximal gradient descent-ascent: Variable
convergence under kl geometry. In Proc. International Conference on Learning Representations
(ICLR).
Cherukuri, A., Gharesifard, B., and Cortes, J. (2017). Saddle-point dynamics: conditions for
asymptotic stability of saddle points. SIAM Journal on Control and Optimization, 55(1):486-511.
Daskalakis, C. and Panageas, I. (2018). The limit points of (optimistic) gradient descent in min-max
optimization. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages
9236-9246.
Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying
and attacking the saddle point problem in high-dimensional non-convex optimization. In Proc.
Advances in Neural Information Processing Systems (NeurIPS), page 2933-2941.
Du, S. S. and Hu, W. (2019). Linear convergence of the primal-dual gradient method for convex-
concave saddle point problems without strong convexity. In Proc. International Conference on
Artificial Intelligence and Statistics (AISTATS), pages 196-205.
Ferreira, M. A. M., Andrade, M., Matos, M. C. P., Filipe, J. A., and Coelho, M. P. (2012). Minimax
theorem and nash equilibrium. International Journal of Latest Trends in Finance & Economic
Sciences.
Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015). Escaping from saddle points — online stochastic gra-
dient for tensor decomposition. In Proc. 28th Conference on Learning Theory (COLT), volume 40,
pages 797-842.
Ghadimi, S., Liu, H., and Zhang, T. (2017). Second-order methods with cubic regularization under
inexact information. ArXiv: 1710.05782.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,
and Bengio, Y. (2014). Generative adversarial nets. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), pages 2672-2680.
10
Under review as a conference paper at ICLR 2022
Griewank, A. (1981). The modification of newton’s method for unconstrained optimization by
bounding cubic terms. Technical Report.
Hallak, N. and Teboulle, M. (2020). Finding second-order stationary points in constrained min-
imization: A feasible direction approach. Journal of Optimization Theory and Applications,
186(2):480-503.
Ho, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pages 4565-4573.
Huang, F., Gao, S., Pei, J., and Huang, H. (2020). Accelerated zeroth-order momentum methods
from mini to minimax optimization. ArXiv:2008.08170.
Jiang, B., Lin, T., and Zhang, S. (2017). A unified scheme to accelerate adaptive cubic regularization
and gradient methods for convex optimization. ArXiv:1710.04788.
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points
efficiently. In Proc. International Conference on Machine Learning (ICML), volume 70, pages
1724-1732.
Jin, C., Netrapalli, P., and Jordan, M. I. (2020). What is local optimality in nonconvex-nonconcave
minimax optimization? In Proc. International Conference on Machine Learning (ICML).
Karimi, H., Nutini, J., and Schmidt, M. (2016). Linear convergence of gradient and proximal-gradient
methods under the Polyak-ICjasieWicz condition. Machine Learning and Knowledge DiscOvery in
Databases: European Conference, pages 795-811.
Kohler, J. M. and Lucchi, A. (2017). Sub-samPled cubic regularization for non-convex oPtimization.
In Proc. International Conference on Machine Learning (ICML), volume 70, Pages 1895-1904.
Lin, T., Jin, C., and Jordan, M. I. (2020). On gradient descent ascent for nonconvex-concave minimax
Problems. In Proc. International Conference on Machine Learning (ICML).
Liu, M. and Yang, T. (2017). On Noisy Negative Curvature Descent: ComPeting With Gradient
Descent for Faster Non-convex OPtimization. ArXiv:1709.08571v2.
Lojasiewicz, S. (1963). A topological property Cf real analytic subsets. Coll. du CNRS, Les equations
aux derivees partielles, Page 87-89.
Lu, S., Tsaknakis, I., Hong, M., and Chen, Y. (2020). Hybrid block successive approximation for
one-sided non-convex min-max problems: algorithms and applications. IEEE Transactions on
Signal Processing.
Luo, L. and Chen, C. (2021). Finding second-order stationary point for nonconvex-strongly-concave
minimax problem. arXiv:2110.04814.
Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2020). A unified analysis of extra-gradient and optimistic
gradient methods for saddle point problems: Proximal point approach. In Proc. International
Conference on Artificial Intelligence and Statistics (AISTATS), pages 1497-1507.
Nedic, A. and Ozdaglar, A. (2009). Subgradient methods for saddle-point problems. Journal of
optimization theory and applications, 142(1):205-228.
Nesterov, Y. (2008). Accelerating the cubic regularization of newton’s method on convex problems.
Mathematical Programming, 112(1):159-181.
Nesterov, Y. and Polyak, B. (2006). Cubic regularization of newton’s method and its global perfor-
mance. Mathematical Programming.
Neumann, J. v. (1928). Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320.
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J. D., and Razaviyayn, M. (2019). Solving a class of
non-convex min-max games using iterative first order methods. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pages 14934-14942.
11
Under review as a conference paper at ICLR 2022
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,
S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high-
performance deep learning library. In Proc. Advances in Neural Information Processing Systems
(NeuriPs), pages 8024-8035.
Qiu, S., Yang, Z., Wei, X., Ye, J., and Wang, Z. (2020). Single-timescale stochastic nonconvex-
concave optimization for smooth nonlinear td learning. ArXiv:2008.10103.
Robinson, J. (1951). An iterative method of solving a game. Annals of mathematics, 54(2):296-301.
Sinha, A., Namkoong, H., and Duchi, J. C. (2017). Certifying some distributional robustness with
principled adversarial training. In Proc. International Conference on Learning RePresentations
(ICLR).
Song, J., Ren, H., Sadigh, D., and Ermon, S. (2018). Multi-agent generative adversarial imitation
learning. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 7461-
7472.
Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2018). Sample Complexity of Stochastic Variance-
Reduced Cubic Regularization for Nonconvex Optimization. ArXiv:1802.07372v1.
Xie, G., Luo, L., Lian, Y., and Zhang, Z. (2020). Lower complexity bounds for finite-sum convex-
concave minimax optimization problems. In Proc. International Conference on Machine Learning
(ICML).
Xu, P., Roosta, F., and Mahoney, M. W. (2020a). Newton-type methods for non-convex optimization
under inexact hessian information. Mathematical Programming, 184(1):35-70.
Xu, T., Wang, Z., Liang, Y., and Poor, H. V. (2020b). Enhanced first and zeroth order variance
reduced algorithms for min-max optimization. ArXiv:2006.09361.
Xu, T., Wang, Z., Liang, Y., and Poor, H. V. (2020c). Gradient free minimax optimization: Variance
reduction and faster convergence. ArXiv:2006.09361.
Xu, Z., Zhang, H., Xu, Y., and Lan, G. (2020d). A unified single-loop alternating gradient projection
algorithm for nonconvex-concave and convex-nonconcave minimax problems. ArXiv:2006.02032.
Yang, J., Kiyavash, N., and He, N. (2020). Global convergence and variance reduction for a class of
nonconvex-nonconcave minimax problems. In Proc. Advances in Neural Information Processing
Systems (NeurIPS).
Yue, M.-C., Zhou, Z., and Man-Cho So, A. (2019). On the quadratic convergence of the cubic
regularization method under a local error bound condition. SIAM Journal on OPtimization,
29(1):904-932.
Zhang, G. and Wang, Y. (2021). On the suboptimality of negative momentum for minimax optimiza-
tion. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), pages
2098-2106.
Zhou, Y. and Liang, Y. (2017). Characterization of Gradient Dominance and Regularity Conditions
for Neural Networks. In NeuriPs OPtimization for Machine Learning WorkshoP.
Zhou, Y. and Liang, Y. (2018). Critical points of linear neural networks: Analytical forms and
landscape properties. In Proc. International Conference on Learning RePresentations (ICLR).
Zhou, Y., Wang, Z., and Liang, Y. (2018). Convergence of cubic regularization for nonconvex
optimization under kl property. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pages 3760-3769.
Zhou, Y., Zhang, H., and Liang, Y. (2016). Geometrical properties and accelerated gradient solvers
of non-convex phase retrieval. Proc. Annual Allerton Conference on Communication, Control, and
ComPuting (Allerton), pages 331-335.
12
Under review as a conference paper at ICLR 2022
Appendix
Table of Contents
A Proof of Proposition 1	13
B Proof of Proposition 2	14
C Proof of Theorem 1	16
D Proof of Theorem 2	17
E Proof of Theorem 3	18
F Proof of Theorem 4	20
G Proof of Theorem 5	22
In this supplementary material, we present the proof of all the results claimed in the paper. We first
prove the following auxiliary lemma that bounds the spectral norm of the Jacobian matrices.
Lemma 1. Let Assumption 1 hold. Then, for any x ∈ Rm and y ∈ Rn, the Jacobian matrices of
f (x, y ) satisfy the following bounds.
k[V22f(x,y)]-1k ≤ μ-1,	(12)
l∣V12f(x,y)k = kv2if(x,y)k ≤ L1.	(13)
Proof. We first prove eq. (12). Consider any x ∈ Rm and y ∈ Rn . By Assumption 1 we know that
f (x, ∙) is μ-strongy concave, which implies that -V22f (x, y)占 μI. Thus, We further conclude that
∣∣[V22f(x,y)]-1k = λmaχ([-V22f(x,y)]-1) = (λmin( - V22f(x,y)))	≤ μ-1.
Next, we prove eq. (13). Consider any x, u ∈ Rm and y ∈ Rn , we have
kV21f (x,y)uk =II It V2f (X + tu,y)∣^o∣∣
=Il limo 1 [V2f (x + tu, y) - V2f (x, y)]
=limo j∣lV2f(x + tu,y) - V2f(x,y)∣∣
≤ 1吧 L^∣∣tu∣∣ = LIkuk，
(14)
which implies that kV21f(x, y)k ≤ L1. Since f is twice differentiable and has continuous second-
order derivative, we have V12f (x, y)> = V21f (x, y), and hence eq. (13) follows.	□
A Proof of Proposition 1
Proposition 1. Let Assumption 1 hold. Then, the following statements hold.
1.	Mapping y*(x) is K-Lipschitz continuous;
2.	Function Φ(x) is Lι(1 + K)-Smooth and VΦ(x) = Vif (x, y*(x));
3.	Define G(x, y) = V11f(x, y) - V12f (x, y)[V22f (x, y)]-1V21f (x, y). Then, G is a Lipschitz
mapping with constant LG = L2(1 + K)2, i.e., kG(x0, y0) - G(x, y)k ≤ LGk(x0, y0) - (x, y)k;
13
Under review as a conference paper at ICLR 2022
4.	The Hessian of Φ satisfies V2Φ(x) = G(x,y* (x)), which is Lipschitz continuous with constant
LΦ = LG(1 + κ) = L2(1 + κ)3.
Proof. The items 1 & 2 are proved in Chen et al. (2021); Lin et al. (2020).
We first prove the item 3. Consider any x, x0 ∈ Rm and y, y0 ∈ Rn . For convenience we denote
z = (x, y) and z0 = (x0, y0). Then, by Assumption 1 and using the bounds of Lemma 1, we have that
kG(x0, y0) - G(x, y)k
≤ kV11f (x0, y0) - V11f(x, y)k + kV12f (x0, y0) - V12f (x, y)kk[V22f (x0, y0)]-1kkV21f (x0, y0)k
+kV12f(x,y)kk[V22f(x0,y0)]-1-[V22f(x,y)]-1kkV21f(x0,y0)k
+ kV12f(x,y)kk[V22f(x,y)-1]kkV21f(x0,y0) - V21f(x, y)k
≤ L2kz0 — zk + (L2kz0 — zk)μ-1Lι
+ L2k[V22f(x0,y0)]-lkkV22f(x,y) — V22f(x0,y0)∣∣∣∣[V22f(x,y)]T∣∣+ L1μ-l(L2kz0 -Zll)
≤ L2(1 + 2K)kz0 — zk + L1μ I(L2kz0 — Zk)μ 1
≤ L2(1 + κ)2 lz0 - zl.
Next, We prove the item 4. Consider any fixed X ∈ Rm, We know that f (x, ∙) achieves its maximum
at y*(x), where the gradient vanishes, i.e., V2f (x, y*(χ)) = 0. Thus, we further obtain that
0 = VχV2f (x,y*(x)) = V21f (x,y*(x)) + V22f (x,y*(x))Vy*(x),
which implies that
Vy*(x) = -[V22f (x,y*(x))]-1V21f (x,y*(x)).	(15)
With the above equation, we take derivative of VΦ(χ) = V1 f(χ, y* (x)) and obtain that
V2Φ(x) =V11f (x,y*(x)) + V12f (x,y*(x))Vy*(x)
=V11f (x,y*(x)) — V12f (x,y*(x))[V22f (x,y*(x))]-1V21f (x,y*(x))	(16)
=G(x,y*(x)).
Moreover, we have that
kV2Φ(x0) — V2Φ(x)k =kG(x0,y*(x0)) — G(x,y*(x))k
≤Lg [kχ0 — χk + ky*(χ0) — y*(χ)k]
≤LG(1 + κ)kx0 —xk,	(17)
where the last step uses the item 1. This proves the item 4.
□
B Proof of Proposition 2
Proposition 2. Let Assumption 1 hold. Define the following potential function
H(χ,χ0,y) ：= Φ(χ) + L2κ3kχ0 — χk3 + 4L2ky — y*(χ)k3,
and denote Ht := H(xt, xt-1, yt+1). Choose Nt ≥
max(ln 2, ln[Lι ∣∣^2f (xt,yt) k/(L2μ)]-2 ln ∣∣xt-xt-ι k)
ln[κ/(κ- 1)]
and learning rates ηx ≤ 28^1 K, ηy ≤ L+μ. Then, the sequences {xt, yt}t generated by Cubic-GDA
satisfy, for all t = 0, 1, 2, ...
Ht+1 ≤ Ht-L2κ3kxt+1 — xtk3—L2kyt+1 — y*(Xt)k3.	(2)
Consequently, it holds that
lim kxt+1 — xt k = 0,
t→∞
lim kyt+1 — ytk = 0, lim kyt — y*(xt)k = 0.
t→∞	t→∞
14
Under review as a conference paper at ICLR 2022
Proof. We first bound the term kyt+ι - y*(χt)k, which corresponds to the optimality gap of the
maximization problem. Note that y*(χj ∈ Rn is the unique maximizer of the strongly concave
function f(xt, y). Note that yt+1 is obtained by applying Nt gradient ascent steps starting from yt.
Hence, By the convergence rate of gradient ascent algorithm under strong concavity, we conclude
that with learning rate η = L2μ,
kyt+ι-y*(xt)k ≤(1-κT)Ntkyt-y*(xt)∣∣
≤~Γ^ kxt - xt-1k2,
L1
where the second inequality uses the hyperparameter choice
(18)
(19)
ln[L1kV2f (xt, yt)k/(L2μ)↑ - 2ln ∣∣xt - xt-1 k ≥ ln[L1 IIyt- y* (Xt)Il/L2] - 2ln ∣∣xt - xt-1k
ln[κ∕(κ - 1)]	—	ln[κ∕(κ - 1)]
as ∣∣V2f(Xt,yt)∣∣ = ∣∣V2f(Xt,yt) —V2f(Xt,y*(Xt))∣∣ ≥ μ∣∣yt - y*(Xt)∣∣. Moreover, eq. (18) and
the hyperparameter choice that Nt ≥ 二=/；：-[)] imply that
kyt+2 - y* (Xt+l)∣∣3 ≤(I-K-I )3NtIIyt+1 - y* (Xt+1)k3
(i)	1	1	3
≤l 2 a +2 M
(ii)	1	3	κ3	3
≤ 2 kyt+1 - y (Xt)Il + -2 HXt+1 - Xtk .
where (i) denotes that a := yt+1 - y*(χt), b ：= y*(χt) - y*(xt+ι), (ii) applies Jensen,s inequality
to the convex function ∣∣ ∙ ∣∣3.
Based on Theorem 10 and Proposition 1 of Nesterov and Polyak (2006), we know that
λmin [G(xt,yt+1)] ≥ -ʒ-kxt+1 - xt∣∣∙
2ηx
Since xt+1 minimizes the following cubic sub-problem
(20)
(21)
gyt+1 (x) ：= V1f(Xt,yt+1)>(X - Xt) + 1(x - Xt)>G(Xt,yt+1)(X - Xt) + 6n-∣∣X - Xtk3,
we obtain the following optimality condition
V 1f (xt, yt+1) + G(xt, yt+1 )(xt+1 - Xt) + 77—kxt+1 - xtk(xt+1 - Xt) = 0.	(22)
2ηx
Next, by the Lipschitz Hessian of Φ, we obtain that
Φ(Xt+1) - Φ(Xt)
≤ (vφ(Xt) - v 1f(Xt,yt+1)) (Xt+1 - Xt) + 2(Xt+1 - Xt)>(V2φ(Xt) - G(Xt, yt+1))(Xt+1 - Xt)
+ VIf(Xt, yt+1)>(Xt+1 - Xt) + 2(Xt+1 - Xt)>G(Xt, yt+1)(Xt+1 - Xt)+ 6~ HXt+1 - Xt『
(i)	LG
≤ LIIlyt+1 -y*(Xt)||||勺+1 -XtIl + -2~|lXt+1 -XtIl ∣∣yt+1 -y*(χt)k
-2(Xt+1- Xt) >G(Xt, yt+1)(Xt+1- Xt)+(^6^- 2n-) iiXt+1- Xt『
(ii)	2L3/2	/	3/2	L2κ3	3
≤ 3.1/2 3/2 ∣∣yt+1 - y (Xt)I	+	3~ llXt+1 - Xtk
+	^^2G h ^3^ llXt+1 - Xt||3 + 3κ2 ∣∣yt+1 - y*(Xt)『]
+	J-llXt+1 - Xt『+ (~φ - ʒ -) llXt+1 - Xt『	(23)
4ηx	6	2ηx
15
Under review as a conference paper at ICLR 2022
≤ 3κ3∕2 kxt - xt-1k3 + 3^2 kyt+1 - y*(xt)k3 + (3L2κ3 - 4 —) kxt+1 - xtk3,
(24)
where (i) uses Assumption 1, the item 4 of Proposition 1 and eq. (22), (ii) uses eq. (21) and
the inequality that ab = ((Ca)3/2(Ca)3/2(b/C)3)1/3 ≤ 2(Ca)3/2 + 3C3 for any a, b ≥ 0 and
C > 0 (based on AM-GM inequality), (iii) uses eq. (19) and LG = L2(1 + κ)2 ≤ 4L2κ2,
LΦ = L2(1 + κ)3 ≤ 8L2κ3.
Multiplying eq. (20) with 4L2 and adding it to eq. (24) yield that
Φ(χt+ι) 一 Φ(χt) + 4L2∣∣yt+2 — y (xt+1)k3
≤ 3κ3∕2 kxt - xt-1k3 +	3-2 kyt+1 - y*(Xt)k3 + (5L2κ3 — 4η~) kxt+1 - xtk3,
(i)
≤ L2κ Ilxt- χt-ι∣∣3 + 3L2kyt+ι — y*(χt)k —2L2κ kxt+ι — XtlE,
where (i) uses the condition that ηχ ≤ 28七屋∙ The above inequality implies eq. (2) by defining
Ht ：= Φ(xt) + L2κ3kxt — xt-1k3 + 4L2kyt+1 — y (xt)k3.
Next, summing eq. (2) over t = 0, 1, ..., T — 1, we obtain that for all T ≥ 1,
T-1	T-1
L2κ3 X kxt+1 - xtk3 + L2 X kyt+1 - y*(xt)k3
t=0	t=0
≤ H0	—	HT	≤ H0	—	Φ(xT )	≤	H0	— inf Φ(x)	< +∞.	(25)
x∈Rm
Letting T → ∞ yields that P∞=0 kxt+ι — xt∣3 < +∞ and P∞=0 ∣yt+ι — y*(xt)k3 < +∞, So
limt→∞ kxt+ι — xt∣= limt-∞ ∣∣yt+ι — y*(xt)∣ = 0. Hence, ∣yt — y*(xt)k ≤ ∣∣yt — y*(xt-ι)k +
ky*(xt) — y*(xt-ι)∣∣ → 0 using continuity of y*, and ∣∣yt+ι — yt∣ ≤ ∣∣yt+ι — y*(xt)∣∣ + ∣∣yt —
y*(xt)k → 0.
C Proof of Theorem 1
Theorem 1 (Global convergence). Under the same conditions as those of Proposition 2, the Cubic-
GDA has the following global convergence properties.
1.	Thefunction value SeqUence {Φ(xt)}t converges to a finite limit H * > —g;
2.	The generated sequences {xt}t, {yt}t are bounded and have a compact sets of limit points.
3.	Every limit point x* of {xt}t is a SeCOnd-Order stationary point of Φ, i.e., VΦ(x*) = 0,
V2Φ(x*)占 0, and satisfies Φ(x*) = H *.
Proof. We first prove item 1. We have shown in Proposition 2 that {Ht}t is monotonically decreasing.
Also, Assumption 1 says that Ht ≥ Φ(xt) ≥ infχ∈Rm Φ(x) > —g. Therefore, We conclude that
{Ht}t converges to a finite limit H * > —g. Since Proposition 2 proves that ∣xt — xt-ι∣, ∣yt+ι —
y*(xt)∣ →t 0, we further conclude that Φ(xt) →t H*, which proves item 1.
Next, we prove item 2. Note that Φ has compact sub-level sets. Moreover, for all t, we have
Φ(xt) ≤ Ht ≤ H0 < +g.	(26)
Hence, the sequence {xt}t is bounded and thus has a compact set of limit points. Since y* is Lipschitz
continuous by Proposition 1, {∣y* (xt)∣}t is also bounded. Consequently, as ∣yt — y* (xt)∣ → 0, we
conclude that {yt}t is also bounded and hence has a compact set of limit points. This proves item 2.
Next, we will prove item 3. Suppose that xtk →k x* along a certain sub-sequence {xtk}k. We have
that ytk+1 →t y* (x*) based on Proposition 2 and the continuity of y*. In addition, Proposition 1
implies that Φ, VΦ, V2Φ and G are continuous. Therefore, for every limit point x*,
Φ(x*) = lim Φ(xtk) = H*.
tk→∞
16
Under review as a conference paper at ICLR 2022
∣∣vφ(x*州= Iim ∣∣vφ(Xtk)Il = Jim IlVf(Xtk,y*(xtkDIl = Jim IlVf(Xtk,ytk+I)Il
tk→o	tk→o	tk→o
="⅛∞ IlG(Xtk ,ytk+ι)(Xtk+ι- xtk)+ 2η^ ι∣χtk+ι- Xtkk(Xtk+ι- xtk )∣∣ = °
λmin[V2Φ(x*)] = t Jimo λmin[V2Φ(X" )] = "→∞ λmi∏ [G(xtfc 4*(x加 ))]
(ii)	1
= Iim λmin [G(xtfc ,ytk +1)] ≥ -ʒ — Iim Ilxtk+ 1 - Xtk Il = 0
tk→∞	2ηx tk→∞
where (i) uses eq. (22), and (ii) uses eq. (21).
□
D Proof of Theorem 2
Theorem 2 (Global convergence rate). Under the same conditions as those of Proposition 2, the
Cubic-GDA converges at the following rateforall T ≥ HsKRm 以' ∙
min μ(Xt) < (H0 - inf…φ(X)广.
0≤t≤T-1 Kt	TL2κ3∕3
Before proving Theorem 2, we first prove the following auxiliary lemma.
Lemma 2. Forany SymmetriC matrices A, B ∈ Rn×n, we have ∣λmin(A) — λmin(B)∣ < IlA — B∣∣∙
ProofofLemma 2.
∣λmin(A) — λmin(B)∣ < I min UTAu — min UTBul
u: ∣∣u∣∣ = 1	u: ∣∣u∣∣ = 1
<	max IuTAu — uτBul
一u：kuk = 1 1	1
<	max1(35A - BllH) = IA - B∣∣.
u:kuk=1
□
Proofof Theorem 2. Note that (25) implies that
min
2≤t≤T-1
(IlXt+1 - Xt||3 + IlXt - Xt-Ill3∣∣ + IlXt-I
- Xt-2I3)
< Ho — infχ∈Rm Φ(x)
<	TL2κ3∕3
which further implies that there exists 1 < t0 < T - 1 such that
max (IlXt，+1 - X∣∣, ∣∣χt' 一 Xt，一1||, ||xt，一1 一 χt,-2∣∣) < (
On the other hand, equation (2.2) of Nesterov and Polyak (2006) implies that,
∣∣vφ(XtO)- V$(Xt0-1) - v2中(Xt0-I)(XtO- Xt0-1)|| < --"HXtz - Xt0-1||2.
Since VΦ(x) = Vf (x, /(x)), V2Φ(x) = G(x, /(x)), the above inequality implies that
一infx∈Rm φ(X)、1/3
"TL2K3∕3	).	()
∣∣VΦ(xto)∣∣
<	∣∣V1f(xt0-1,y*(xt0-1)) + G(XtO-1,y*(xt0-1))(xt0 - x"1)∣∣ + ？∣∣xto - xt0-1∣∣2
<	∣∣V1f(xt0-1,yto) + G(XtO-1,yt0)(xt0 — xt0-1)∣∣ + ∣∣V1f1 (xt0-1,y*(xt0-1)) - V1 f(xt0-1,yt0)∣∣
+ Il (G(Xt0-1, y*(XtO-I)) - G(Xt0-1, yto))(xt0 - XtO-I)Il +—2φllxt0 - xt0-1ll2
(i)	1
< llxtO - xtO-1||2 + LIIlytO - y*(xtO-I)Il + LGIlytO - y*(xtO-1 )llllxtO - xtO-1 Il
2ηχ
+ ~γllxtO - xtO-1『
17
Under review as a conference paper at ICLR 2022
(ii)	1	4L2 κ2
≤ (2---+ 4L2κ ) Ilxt0 - xt0-1∣∣2 + L2∣∣xt0-1 - xt0-2∣∣2 +-L	kxt0 - xt0-1∣∣2∣∣xt0 + 1 - xt0 k
(iii)	1	3	H0 - inf x∈Rm Φ(x) 2/3	4L22κ2 H0 - inf x∈Rm Φ(x)
≤ l2ηχ + 5L2κM—TL2κ3∕3—+	+ F	( —TL2K3/3—)
(iv)
≤
+ 5L2κ3 +
HO - infx∈Rm φ(X) A 2/3
TL2K3∕3	J
(28)
where (i) uses eq. (22), item 1 of Assumption 1 and item 3 of Proposition 1, (ii) uses eq. (19)
and LG = L2(1 + κ)2 ≤ 4L2κ2, LΦ = L2(1 + κ)3 ≤ 8L2κ3, (iii) uses eq. (27), and (iv) uses
T ≥ H0-infx 3Rmφχ). Also, note that
L2κ /3
一 λmin(V2φ(xt0 ))
(i)
≤ -λmin (G(XtO ,yt0+1)) + IlG(Xt0, y* (XtO))- G(XtO ,yt0+I)Il
Cii)	1
≤ ɔ一||Xt0 + 1 - Xt0 k + LGkyt0 + 1 一 y (XtO)k
2ηx
(iii)	1	L L
≤ --|lXt0+i 一 Xt0Il +—k—|lXt0 一 Xt0-4F
2ηx	L1
(≤)	1 / HO 一 inf χ∈Rm
一TL2κ3∕3
Φ(x)∖"3	LgL2 (HO — infχ∈Rm Φ(x)λ2/3
)+ l	TL2κ3∕3	)
L1
(v)
≤
+
Ho 一 infx∈Rm Φ(x) λ 1/3
TL2κ3∕3	)
(29)
where (i) uses Lemma 2, (ii) uses eq. (21) and item 3 of Proposition 1, (iii) uses eq. (19), (iv) uses
eq. (27), and (v) uses T ≥
imply Theorem 2.
Ho一infχ∈Rm Φ(x)
L2K373
and LG = L2(1 + κ)2 ≤ 4L2κ2. Equations (28) & (29)
□
E	Proof of Theorem 3
Theorem 3. Let Assumption 1 hold and assume that the potential function H satisfies the local
Lojasiewicz gradient geometry. Choose the hyperparameters Nt, ηχ, ηy in the same way as Proposi-
tion 2. Then, the sequences {(Xt, yt)}t generated by Cubic-GDA have a unique limit point, which is
a second-order stationary point of Φ.
Proof. We first derive a bound on ∣∣VH(X,X0,y)k := ,Pk=ι ∣∣VkH(X,X0,y)k2 as follows.
∣∣VH (Xt,Xt-i,yt+i)k
≤ ∣∣V1H(Xt,Xt-i,yt+ι)k + ∣∣V2H(Xt,Xt-i,yt+ι)k + ∣∣V3H(Xt,Xt-i,yt+ι)k
=∣∣VΦ(Xt) + 3L2κ3∣Xt - Xt-ιk(Xt — Xt-ι) + i2L2kyt+1 — y*(Xt)∣∣Vy*(x∕> [y"(Xt) — yt+ι] ∣∣
+ 卜L2K3||Xt - Xt-l||(Xt 一 Xt-1)∣∣ + ∣∣12L2∣yt+l — y"(Xt)k [yt+ι — y (Xt)] ∣∣
≤ kVl(Xt-1)+ V21(Xt —1)(Xt - Xt-1)k + (+ 6L2κ3) ||Xt - Xt-1 k2
+ 12L2∣yt+ι — y*(Xt )∣2(1 + |lV22f(X,y*(X))TkkV21f(X,y*(X))k)
(ii)
≤ ∣∣VΦ(Xt-1) + G(Xt-1,yt)(Xt — Xt-1)k + ∣∣G(Xt-1,y*(Xt-1)) — G(Xt-1,yt)k∣Xt 一 Xt-1∣
+ (~2^ + 6L2κ3) ||Xt - Xt-1∣2 + 12L2(1 + μ-1L1)∣yt+1 - y*(Xt)k2
(iii)	∣∣	1	∣∣
≤ ∣∣ V1 f(Xt —1, y* (Xt-I)) — VIf(Xt-1,yt) 一 ɔ- llXt 一 Xt-Ik(Xt - Xt-1) ∣∣
18
Under review as a conference paper at ICLR 2022
+ LGlIyt ― y*(Xt-I)IIllxt - xt-ιk + (+ 6L2κ3) kxt ― χt-ιk2 + 24L2κkyt+ι ― y*(Xt)Il2
(iv)	L	1
≤ LIIlyt - y*(Xt-I)Il + ( +6 + 6L2κ	+ --) kxt - xt-1k2
2	2ηx
+—L^^^Ilxt-I - xt-2∣∣2Ilxt - xt-ι∣∣+—L2	Ilxt - xt-ιk4
≤ L2∣∣xt-ι - xt-2∣∣2 + (10L2κ3 + 2η~)∣∣xt - xt-1∣∣2
4L22κ2	2	24L23κ	4
+	∣∣xt-ι - xt-2IlIlxt - xt-iIl +	2 Ilxt - xt-ι∣∣
L1	L1
(30)
where (i) uses eq. (2.2) of NesteroV and Polyak (2006), the fact that V2Φ is Lφ-Lipschitz, and eq.
(15), (ii) uses Lemma 1 and V2Φ(x) = G(x, y*(x)) from item 4 of Proposition 1, (iii) uses eq. (22),
VΦ(x) = Vif (x, y*(x)) from item 2 of Proposition 1 and the inequality that 1 + K ≤ 2κ, (iv) and
(V) use eq. (19), and (V) uses LG = L2(1 + κ)2 ≤ 4L2κ2, LΦ = L2(1 + κ)3 ≤ 8L2κ3.
Next, we prove the convergence of the sequence {xt}t under the assumption that H(x, x0, y) satisfies
the LOjaSieWicz gradient geometry. Recall that We have shown in the proof of Theorem 1 that: 1)
{Ht}t decreases monotonically to the finite limit H*; 2) for any limit point x*,y* of {xt}t, {yt}t,
Φ(x*) = H*. Hence, the Lojasiewicz gradient inequality (see Definition 1) holds after sufficiently
large number of iterations, i.e., there exists ti ∈ N+ such that for all t ≥ ti, H(xt, xt-i, yt+i) -
H * ≤ CIlVH (xt,xt-ι,yt+i)∣∣θ. Equivalently,
S(Ht- H*)∣∣VH(xt,xt-1,yt+1)∣∣ ≥ 1.
where we define the concave function that 夕(S) := ɪ-//θθs1-"θ(θ > 1,s > 0).
In addition, since Ixt - xt-iI →t 0 (Proposition 2), there exists t2 ∈ N+ such that for all t ≥ t2,
Ixt - xt-i I ≤ 1. Hence, rearranging the above inequality and utilizing eq. (30), we obtain that for
all t ≥ t0 := max(ti , t2 ),
,(Ht- H*)
≥ IVH(xt,xt-i,yt+i)I-i
≥
—+ L2) ∣∣xt-ι — xt-2∣∣2 + (10L2K3 + 24LL2 κ + A )∣∣xt — xt-ill2]
(31)
By concavity of the function 夕(s) := ɪ-//θθsi-i"(θ > 1,s > 0), we know that
夕(Ht— H *)—夕(Ht+i— H *)
≥，(Ht- H*)(Ht — Ht+i)
≥	2	L2K3Ixt+i — xtI3________________________ (32)
(~2~l^—+ L2)∣∣xt-i - xt-2∣∣2 + (10L2κ3 + -^22- + 2ηX) ∣∣xt - xt-i ∣∣2
≥)________________________L2κ3∣∣xt+i - xt∣∣3______________________
(q 4^2--+ L2 ιιxt-i- xt-2 Ii + q 10L2κ3 + ^L22- + 2ηiχ ∣∣xt — xt-iI)
where (i) uses Proposition 2 and eq. (31), (ii) uses the inequality that a2 + b2 ≤ (a + b)2 for any
a, b ≥ 0.
Rearranging the above inequality yields that
L2κ3Ixt+i — xtI3
≤ [2(Ht— H*)—夕(Ht+i— H*)]
4LL^+L ∣xt-i—xt-21+j10L2κ3+24L2κ+看 ∣xt—xt-i ∣!
19
Under review as a conference paper at ICLR 2022
1
≤——
一 27
C%Kt- H*) - ψ(Ht+ι - H*)] + CJL2
κ2
L1
+ L2 kxt-1 - xt-2 k
2 /… 3^^24L2K	F ll iΛ3
+C NIOL2κ+ F +2ηx kxt - xt-1k)
(33)
where the final step uses the AM-GM inequality that ab2 = [p(C2a)(b∕C)(b∕C)]3 ≤ 27(C2a +
C )3 for any a, b ≥ 0 and C > 0 (the value of C will be assigned later). Taking cubic root of both
sides of the above inequality and telescoping it over t = t0, . . . , T - 1, we obtain that
T-1
κ3/L E ∣∣χt+ι - XtIl
t=t0
C2	2	4L2κ2	T-1
≤	"3^[夕(HtO	- H ) -夕(HT	- H )] +	3C《	L-----+	L2	£ kxt-1	-	xt-2k
1	t=t0
2	I	:~24LfK	Γ~T-1 π	ll
+ 3CM	2K + F +2ηxS0kxt-χjk
≤ Cr 以 Ht0- H *)+3C 7
κ2
L1
T-1
+ L2	kxt-1 - xt-2 k
t=t0
2 Γ~~3~24LIK	FT-^ll	ll
+ 3C『。L2κ + F +2ηχ∑ kxt-xt-ιk
where the final step uses the facts that Ht - H * ≥ 0 and that 夕(S) is monotonically in-
creasing. Since the value of C > 。 is arbitrary, we can select large enough C such that
2	4L22 κ2
3c V lΓ~
plies that
+ L2, 3C J10L2k3 + 24Lrκ + 2ηχ <
K 33L2. Hence, the inequality above further im-
κ√3L2 ∑ kxt+ι-xtk≤C中(HtO- H*)
t=t0
+ K铲[kxt0-1
- xt0-2k + 2kxt0 - xt0 -1 k < +∞.	(34)
Letting T → ∞ concludes that
∞
∣xt+1 - xt ∣< + ∞.
t=1
Moreover, this implies that {xt}t is a Cauchy sequence and therefore converges to a certain limit,
i.e., xt →t x*. We have shown in Theorem 1 that any such limit point must be a second-order critical
point of Φ. Hence, we conclude that {xt}t converges to a certain second-order critical point x* of
Φ(x). Also, note that ∣y*(xt) - yt∣ →t 0, xt →t x* and y* is a Lipschitz mapping, so we conclude
that {yt}t converges to y* (x*). Finally, the item 3 of Theorem 1 implies that x* is a second-order
critical point of Φ(x).	□
F Proof of Theorem 4
Theorem 4 (Funtion value convergence rate). Under the same conditions as those of Theorem 3, the
sequence of potential function {Ht}t converges to the limit H* at the following rates.
1. Ifthe geometry parameter θ ∈ (2, ∞) ,then Ht ] H * Super-linearly as
O
Ht - H * ≤ O exp
∀t ≥ t0 ;
(6)
20
Under review as a conference paper at ICLR 2022
2.	Ifthe geometry parameter θ = 3, then Ht ] H * linearly as
Ht- H * ≤ (1 + C0/2)- t-2t0, Vt ≥ t0；
3.	Ifthe geometry parameter θ ∈ (1, ∣), then Ht ] H * sub-linearly as
Ht — H * ≤θ((t —10)-熹)，Vt ≥ t0.
(7)
(8)
Proof. Equation (31) implies that there exists t0 ∈ N+ such that for any t ≥ t0,
^(Ht - H*)-1 = c-"θ(Ht - H*)1/
≤ (联 + L2)kxt-1 - xt-2k2 + (i0L2κ3 + 24L2κ + A) kxt - xt-1k2	(35)
(i)
≤
+ L2κ-2
Ht-2 - Ht-1
L
)2/3
+
24L32
K + T27 +
L1κ
1	)( Ht-1- Ht)2/3
2ηx κ2 八	L J
where (i) uses proposition 2.
Defining	dt	:=	Ht	一	H *,	C1	:=	c1" L-2/3 (4L2	+	L2κ-2),	C2	：二
c1"L-2/3(i0L2κ + 泮 + Tr), and C0= 3√2c1∕θL-2/3(10L2K + 泮 + 4L2 + Tr),
2	2	L12 κ	2ηx κ2 ,	0	2	2	L21 κ	L1	2ηx κ2 ,
the above inequality further becomes
dt/ ≤C1(dt-2 - dt-1) / + C2 (dt-1 - dt) /	(36)
≤2 maχ(c1, C2) h2 (dt-2 - dt-1) / +2 (dt-1 - dt) i ^
(i)	1	2/3 (ii)
≤2 maχ(c1, C2) |_2(dt-2 - dt)	≤ C0(dt-2 - dt)	(37)
where (i) applies Jensen’s inequality to the concave function ξ(s) = s2/3, and (ii) uses the inequality
that 21/3 max(C1, C2) ≤ C0.
Next, we prove the convergence rates case by case.
(Case 1) If θ ∈ (3, +∞), since dt ≥ 0, eq. (36) implies that for t ≥ t0,
dt ≤c0d2-/3,
which is equivalent to that
3θ	-	3θ	-QAg
C02θ-3 dt ≤ [C02θ-3 dt-2]2θ/3	(38)
3θ
Since dt 1 0, C02θ-3 dt0 ≤ e-1 for sufficiently large t° ∈ N+. Hence, eq. (38) implies that for any
k ∈ N+
3θ
3θ
[(2θ⑶k]≤ eχp h - (2θ)ki.
Hence,
-3θ
d2k+t0+1 ≤ d2k+t0 ≤ C-K exp
h-(2θ )k ]
Note that θ ∈ (∣, +∞) implies that 挈 > 1, and thus the inequality above implies that Ht ] H* at
the super-linear rate given by eq. (6).
(Case 2) If θ = 3, eq. (37) implies that for t ≥ t0,
dt ≤ (1 +C03/2)-1dt-2.	(39)
21
Under review as a conference paper at ICLR 2022
Note that dt0 ≤ 1 for sufficiently large t°. Therefore, dt 1 0 (i.e., H(Zt) [ H*) at the linear rate
given by eq. (7).
(Case 3) If θ ∈ (1, 3), consider the following two subcases.
If dt-2 ≤ 2dt, denote ψ(s) = 322s1-23θ, then for any t ≥ t0,
/dt-2	fdt-2 _且	(i) - ɪ
ψ(dt) - ψ(dt-2) =	-ψ0(s)ds =	S-2θds ≥ dt-2 (dt-2 - dt)
dt	dt
(ii)	_ 3
≥ C-2
岛 (i≥ (2C0)-2
(40)
where (i) uses dt ≤ dt-2, and - 2 (1 — θ) < 0, (ii) uses the following inequality implied by eq. (37),
and (iii) uses ^l^ ≥ 2 and 另 ∈(1, 2).
If dt-2 > 2dt, then for any t ≥ t0,
2θ	3	3	2θ	3	,	3 r
ψ(dt) - Ψ(dt-2)	=3-2θ(d1	2θ- d1-22θ )	≥ 3—2θ	[d1	2θ	- (2dt)1-23θ]
2θ(1 - 21-2θ) 1-3 3
≥ ∖ - 2θ	d1	≥ (2C0)-2.	(41)
where we use 1 -岛 ∈ ( - 2,0), 322^ > 0 and dt ≤ dt° ≤ (2Co)3-2θ [ 2θ"匕左)]3 2θ for
sufficiently large t0 ∈ N+ .
3 ɪɪ
Since at least one of eqs. (40) & (41) holds, we have ψ(dt) - ψ(dt-2) ≥ (2Co)-2. Hence,
33
Ψ(d2k+t0+1) ≥ Ψ(d2k+t0) ≥ ψ(dto)+ k(2Co)-2 ≥ k(2Co)-2; k ∈ N,
which implies that ψ(dt) ≥ t-2t0(2Co)-33 By substituing the definition of ψ, the inequality above
implies that H(Zt) [ H* in a sub-linear rate given by eq. (8).	□
G Proof of Theorem 5
Theorem 5 (Parameter convergence rate). Under the same conditions as those of Theorem 3, the
sequences {xt, yt}t generated by Cubic-GDA converge to their limits x*, y*(x*) respectively at the
following rates.
1.	Ifthe geometry parameter θ ∈ (3, ∞), then (xt,yt) → (x*,y* (x*)) Super-Unearlyas
max {∣∣xt-x*k, kyt-y*(x*)k} ≤θ(exp (- 3 (2θ) 2	)), Vt ≥ to；	(9)
2.	Ifthe geometry parameter θ = 2, then (xt, yt) → (x* ,y*(x*)) linearly as
max {kxt- x*k, kyt - y*(x*)k} ≤o((1 + Ch-t-t0), Vt ≥ to；	(10)
3.	Ifthe geometry parameter θ ∈ (1, 2), then (xt, yt) → (x*,y*(x*)) sub-linearlyas
2(θ-1)	2θ
kxt-χ*k≤θ ((t-to)-…J, kyt - y*(χt)k≤θ ((t-to)-3™),	Vt ≥ to. (11)
Proof. Notice that eq. (34) still holds after increasing to, i.e., for T ≥ t ≥ to and large enough C
Wnrh that 2 4 ∕4L2κ2	LC 2 /10 LC «3 -I- 24L3κ ―~ / K √L2 . KP havp
SUChthat 3CV L~ + L2, 3C V10L2κ + F + 2ηX <	, we have
κ√3L2 ∑ kχs+ι - xsk ≤ 3(C Cr∖θ) (Ht -H *)1τθ
s=t
+----3 2[kxt-1 - xt-2k +2∣Ixt - xt-1k] < +∞.	(42)
22
Under review as a conference paper at ICLR 2022
Proposition 2 implies that
,(i)	,
kxt - Xlk ≤ L”S- Ht)1/3 ≤ L-1∕3κT(Ht-ι - H*)1/3,	(43)
where ⑴ uses Ht-ι ≥ Ht ≥ H*.
Therefore, for t ≥ t0, eqs. (42) & (43) imply that
T-1
Ilxt- x*k ≤IimSUp T l∣Xs+ι - XSk
T →∞ s=t
C 2c1∕θ
≤Km(I- 1∕θ) (Ht- H*)1-1/e + [(Ht-2 - H*)1/3 + 2(Ht-ι - H*)1/3] (44)
Next, we discuss case by case.
(Case 1) If θ ∈ (∣, +∞), then
kxt - x*k ≤θ[(Ht-2 - H *)1/3 + (Ht-I- H * )1/3 + (Ht- H * )1/3]
1 ∕2θ∖(t-to)/2-1]、
≤O(exp[- 3(Ir)	])
where the two ≤ use eqs. (44) & (6) respectively. Hence,
l∣yt - y*(Xt)k ≤kyt - y*(Xt-I)k + ky*(Xt-I)- y*(Xt)k
(i)	L2
≤	||Xt-1 - Xt-2 k2 + KkXt - Xt-Ik
L1
(ii)
≤θ[(Ht-2 - H*)2/3 + (Ht-ι - H*)1/3]
1 ∕2θ∖ (t-to-2)	1 2θ∖ (t-t0-1)/2
≤o(eχp[-£•	]+eχp [- Kir) D
1	2θ (t-t0 -1)/2
≤O(exp[- 3(— D,
where (i) uses eq. (19) and item 1 of Proposition 1, (ii) uses eq. (43) and (iii) uses eq. (6).
(45)
(46)
(47)
(Case 2) If θ = ∣, then the proof is similar to that of Case 2, and eqs. (45) & (47) still hold. The only
difference is to use the convergence rate of Ht - H* given by eq. (7) instead of eq. (6).
(Case 3) If θ ∈ (1, ∣), then similar to the proof of Case 2, we obtain from eq. (44) that
kXt - X*k ≤O[(Ht-2 - H*)1-1∕θ + (Ht-1 - H*)1-1∕θ + (Ht- H*)1-1/e]
2(θ-1) ∖
≤O((t - to)--^),
where the two ≤ use eqs. (43) & (6) respectively. Then, as eq. (47) still holds, we have
kyt-y*(Xt)k ≤θ((t-to)-3(3-⅛)∙
□
23