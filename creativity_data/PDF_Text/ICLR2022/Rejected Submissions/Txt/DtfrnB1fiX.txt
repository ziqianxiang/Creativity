Under review as a conference paper at ICLR 2022
Squeezing SGD Parallelization Performance
in Distributed Training Using Delayed Aver-
AGING
Anonymous authors
Paper under double-blind review
Ab stract
The state-of-the-art deep learning algorithms rely on distributed training systems
to tackle the increasing sizes of models and training data sets. Minibatch stochas-
tic gradient descent (SGD) algorithm requires workers to halt forward/back prop-
agations, to wait for gradients aggregated from all workers, and to receive weight
updates before the next batch of tasks. This synchronous execution model exposes
the overheads of gradient/weight communication among a large number of work-
ers in a distributed training system. We propose a new SGD algorithm, DaSGD
(Local SGD with Delayed Averaging), which parallelizes SGD and forward/back
propagations to hide 100% of the communication overhead. By adjusting the
gradient update scheme, this algorithm uses hardware resources more efficiently
and reduces the reliance on the low-latency and high-throughput inter-connects.
The theoretical analysis and the experimental results show its convergence rate
O(1∕√K), the same as SGD. The performance evaluation demonstrates it en-
ables a linear performance scale-up with the cluster size.
1	Introduction
Training deep learning models using data parallelism on a large-scale distributed cluster has become
an effective method for deep learning model training. The enormous training data set allows a huge
batch of training tasks on different data samples running in parallel. The pinnacle of this method
reduces the training time of the benchmark ResNet-50 from days to a couple of minutes (Goyal
et al., 2017; You et al., 2017b; Akiba et al., 2017; You et al., 2017a; Ying et al., 2018; Goodfellow
et al., 2016). During Mini-batch stochastic gradient descent (SGD), these workers have to halt, wait
for the computed gradients aggregated from all of the workers and receive a weight update before
starting the next batch. The wait time tends to worsen when the number of workers increases. As
the workloads are spread over a larger cluster, the computation time is greatly shortened and the
communication overheads take a larger portion of the overall cost.
System designers address this concern by improving inter-chip connects with higher throughput and
lower latency and refining network topology (Li et al., 2019), such as NVIDIA DGX-1 (NVI, 2017)
and NVIDIA DGX-2 (NVS, 2018). Additional care has been given to reduce the intermediate steps
that would increase communication latency. These methods effectively reduce the wait time during
Mini-batch SGD on a large-scale distributed system (Gau, 2019).
Communication efficient SGD algorithms (Lin et al., 2017; Wangni et al., 2018; Alistarh et al.,
2018; Dean et al., 2012; Recht et al., 2011; Zhang et al., 2015; Wang & Joshi, 2018; Wang & Joshi,
2018; Lin et al., 2018) are proposed to reduce the communication requirements. A successfully
modified SGD algorithm shows their convergence rates comparable to Mini-batch SGD through both
theoretical analysis and experimental results. Another challenge is to demonstrate good performance
evaluation results based on the common large distributed training system setups.
A modern data center design prefers cost-efficient hardware blocks and a balanced configuration
for the typical workloads (Barroso et al., 2018). Under these workloads, hardware resources are
utilized in a balanced fashion. A distributed training system works in the opposite manner. During
forward/back propagations, the computing resources are fully used while the system inter-connects
1
Under review as a conference paper at ICLR 2022
are completely idle. During SGD, the computing resources are mostly idle while the system inter-
connects are throttled at the peak throughputs. The performance of distributed training systems may
be improved in addition to better hardware. That is, training workloads may be re-structured for
balanced utilization of the hardware resources.
Inspired by the modern system design practices, we propose DaSGD, a new SGD method, enabling
SGD running parallelly with forward/back propagation and balanced utilization of the hardware re-
sources. It replaces a Mini-batch SGD with Local SGD iterations. In a Local SGD (Lin et al., 2018;
Wang & Joshi, 2018), each worker evolves a local model by performing sequential SGD updates, be-
fore synchronizing by averaging models among the workers. DaSGD uses Local SGD to add weight
synchronization points and allows weights to be updated between Local SGD iterations. Based on
the network throughput and the model size, it schedules delayed model averaging for a defined num-
ber of Local SGD iterations, which allows workers to compute the next batch while the weights are
transferred over a large distributed cluster. The theoretical analysis shows its convergence rate is
O(1∕√K), where K is the iteration step, same as Mini-batch SGD.
The main contributions of this paper are the following.
•	We present DaSGD as an algorithm-system co-design method for a large-scale distributed
training system. It enables designing a more balanced and better-utilized system, more
than a new variant of the SGD algorithm. The discussions and analyses in this paper are
organized around its equivalency to the traditional SGD and its benefit from the system
design perspective.
•	We provide the theoretical analysis of its convergence rate. It shows the convergence rate
at O(1/√K), the same as Mini-batch SGD.
•	Our experiments focus on the training progresses (in terms of loss and accuracy) at the
epoch level. It shows DaSGD allows the training converges at the same rate of SGD
epoch by epoch, which is a good indicator of statistical efficiency and the eventual time-to-
convergence. The experiments also explore the proper ranges of these parameters.
•	A performance evaluation of real-life systems measures many performance issues in a sys-
tem, such as the reduction algorithm, GPU interconnect topology and throughputs, network
throughputs, which are out of the scope of our discussion. Instead, we abstract an analytical
model from the key performance parameters of the system configuration and the training
setup. The analytical model demonstrates DaSGD introduces a linear performance scale-up
with the cluster size.
2	Background
2.1 Stochastic Gradient Descent
Stochastic Gradient Descent (SGD) is the backbone of numerous deep learning algorithms (Ghadimi
& Lan, 2013). Supervised deep learning demands massive training datasets. Training a deep learning
model needs many epochs for training to converge. A variant of classic SGD, synchronous mini-
batch SGD (Bottou, 2010), has become the mainstream due to a faster convergence rate., supported
by prevalent machine learning frameworks, such as Tensorflow (Abadi et al., 2016), Pytorch (Paszke
et al., 2019), MxNet (Chen et al., 2015).
Mini-batch SGD as a weight update process is shown in Eq. 1.
η2 * * * * * B
xk+ι = Xk - B X vF(Xk,skj))	⑴
B j=1
where x ∈ Rd is the weight of a model, η is the learning rate, B is the batch size, S is the training
dataset, Sfj ⊂ S is a random sample, VF (Xk, Sfj) is the stochastic gradient given the sample sj).
From a system perspective, a distributed training system may compute a batch of gradients on several
workers. At the end of a batch, a reduction operation is performed on the gradients on a worker first
and a worker sends out only a copy of local averaged gradients. Further reductions are performed on
gradients from different workers until a final copy of the averaging gradients is obtained. The above
2
Under review as a conference paper at ICLR 2022
Time
FP/BP LWU
I； FP/BP LWU
GWU
FP/BP LWU∣i FP/BP LWU∣i FP/BP LWU∣i FP/BP LWU
)"
)'
WorkerI
Worker2
Local
Batch Size &-
Number of Local Steps ` = 3
Delay Update
"=1
% Local Update Percentage
!!#) = #(!.(#) -%&.(#)) +(1-。2* !(&)
&‘(
Figure 1: A timing diagram of DaSGD.
equation may be rewritten as
xk + 1 = Xk - MM X g(xk,skj))
(2)
where M is the number of workers, g(xk , s(kj) ) is the stochastic gradient that worker j aggregates
locally for that batch.
,	(j), M M	(i),
g(Xk,Sk ) =  52 v(xk,sk )	(3)
B i=1
2.2 Communication Efficient SGD Algorithms
2.2.1	Asynchronous SGD
There are a few asynchronous training methods, such as Downpour SGD (Dean et al., 2012), Hog-
wild (Recht et al., 2011), Elastic Averaging SGD (Zhang et al., 2015). In these models, every worker
has its own copy of weights. A worker performs forward propagation and back propagation on its
partition of samples, and then sends the calculated gradients asynchronously to a pool of parame-
ter servers that manage a central copy of weights. The parameter servers update the central copy
and then send the new weights asynchronously to each worker. While each worker communicates
gradients at a different time and avoids congestions at worker inter-connects, the parameter servers
might be a performance bottleneck. For non-convex problems, ASGD requires that the staleness of
gradients is bounded (Lian et al., 2015) to match the convergence rate O(1∕√K) of synchronous
SGD, where K denotes the total Iteration steps.
2.2.2	LOCAL SGD
Another set of methods targets at reducing the frequency of inter-worker communication and is
called periodic averaging or Local SGD (Wang & Joshi, 2018; Wang & Joshi, 2018; Lin et al.,
2018). A worker performs SGD on its local copy of weights for τ times, where τ denotes the local
iteration steps. After T local updates, local copies are synchronously averaged across all workers
globally. Several works suggested that Local SGD incurs the same convergence rate O(1∕√K) as
SGD. The total number of steps to train a model remains similar but the total amount of inter-worker
communication is reduced by τ times. This has a similar effect as training with a large batch size,
where the number of synchronizations decreases with an increase of batch size. With Local SGD,
SGD and forward/back propagations are still blocking while system resources are unbalanced.
3 DASGD
In this paper, we propose a new algorithm, called Local SGD with Delayed Averaging, DaSGD for
short. It aggregates gradients and updates weights in a relaxed manner, which helps parallelize the
computation of forward/backward propagation with two other execution components: the execution
of global weight averaging and inter-worker data communication.
Our algorithm was initially inspired by the Local SGD algorithm (discussed in Section 2). Although
Local SGD was designed to reduce communication and synchronization overhead, it still involves
a significant amount of communication overhead. To further decrease communication overhead,
even to zero, the proposed algorithm exploits a delayed averaging approach that makes two novel
improvements based on Local SGD. First, in order to merge remote weights by other workers with
local in a deterministic way, DaSGD serializes forward propagations and back propagations for dif-
ferent samples. Second, workers start with local computations for the next local batch while waiting
for the aggregation and synchronization of global weights. In this way, the global communication
3
Under review as a conference paper at ICLR 2022
Figure 2: Loss landscape of (a) Mini-batch SGD, (b) Local SGD, and (c) DaSGD. 12 samples are
updated on two workers. The orange and green arrows represent the updated loss function of each
sample, and the blue arrows describe the location of the updated loss function on the global model.
and synchronization overhead are hidden or overlapped by local computations at the cost of a de-
layed update of local weights. However, theoretically we will prove that the convergence rate is the
same as Mini-batch SGD. Furthermore, DaSGD parameterizes the overlapping degree so that when
a large training cluster requires a longer time to synchronize, a worker may perform more iterations
of local computations.
Fig. 1 illustrates the proposed algorithm by showing a wall-clock time diagram of 6 training iter-
ations. There are 2 workers, dividing a batch into 2 local batches. Each worker computes 6 local
batches. Each local batch contains Bl samples. Each worker maintains a local copy of model. Ac-
cording to Local SGD, for a local batch, each worker operates Bl forward/backward propagations
and then updates the weights of its local model. After 3 local updates, a worker synchronizes local
weights with the other workers, resulting in an AllReduce operation being generated to average the
model weights. In Fig. 1, all workers wait for, at local iteration 3, the global synchronization to be
finished and then start to operate on the next local batch each, in the scenario of Local SGD.
DaSGD implements a key feature by imposing delay update on Local SGD. As shown in Fig. 1,
a worker, at local step 3, broadcasts its local weights to the wild and then immediately starts to
compute on the next local batch, without waiting for the global synchronization to be finished. Later,
at local iteration 4, the worker receives all the other workers’ weights and then updates its local
weights. This design very efficiently overlaps the communication of weights and forward/backward
propagations of the next local batch.
In DaSGD, we use τ to denote the number of local batches between two consecutive global syn-
chronizations. Therefore, τ is a controlling parameter that quantifies the number of propagations
between weight averaging globally. During the delay update, both local computation and the global
communication of weights are executed in parallel. As long as communication time is no more than
the computation time of d local iterations, the communication time can be hidden in the overall
model training time. Careful tuning of d and τ can realize full parallelism of global averaging and
local computations. Unlike Local SGD, τ does not have to be large, as it is not only used to reduce
inter-worker communication overhead (Lin et al., 2017).
In the following part of this section, in order to compare the proposed algorithm and traditional
SGDs, we start with the update framework of each algorithm, and then qualitatively analyze execu-
tion time. Finally, we discussed the updated rules and the convergence rate in detail.
3.1	Update Flow of Different SGD
Fig. 2 explains the mechanisms of weight update flows of Mini-batch SGD, Local SGD, and DaSGD
by taking an example of a 2-worker parallel training process that sets the batch size as 2 samples.
The 2 workers are distinguished by yellow and green arrows. In the Mini-batch SGD (as shown
in Fig. 2(a)), every worker updates its local weights once every mini-batch, which is computed as
the batch size divided by the number of workers. When both workers finish local updates for a
mini-batch, local weights are merged to compute their average (shown by blue arrows). Next, both
workers update their local weights with the average. Local SGD (shown in Fig. 2(b)) reduces the
weight aggregation times by letting every worker first update weights locally for continuous τ local
batches in a row before a global merge is made. Local batch in the context of Local SGD is just a
synonym of the mini-batch in the context of Mini-batch SGD.
Same as the regular periodic averaging method (i.e., Local SGD), in the proposed algorithm, each
worker updates local weights for τ local batches before a global aggregation. A novel change made
by the proposed algorithm is to delay weight update from global to local after the global averag-
ing. A worker may delay the update for d steps (i.e., samples) of local weight updates (d = 1 in
4
Under review as a conference paper at ICLR 2022
this example, as shown in Fig. 2(c)). With this novel algorithmic design, the time of global weight
averaging can be hidden by parallelizing it with local computation by a worker, i.e., forward prop-
agation, backward propagation, and local weight update. Large d can be set if the time of global
weight aggregation is very long in a large-scale distributed training to shorten overall training time.
3.2	Convergence Analysis
In this section, we provide a theoretical analysis of the DaSGD algorithm. We will prove the con-
vergence rate for DaSGD on non-convex problems and show that it converges at the same rate
O(1∕√K) as Mini-batch SGD and Local SGD. To facilitate the convergence analysis, We firstly
introduce the assumptions.
3.2.1	Assumptions
The convergence analysis is under the assumptions as the following, which are similar to the Local
SGD (Wang & Joshi, 2018):
•	Lipschitzian gradient: || 5 F(x) - 5F (y)|| ≤ L||x - y||
•	Unbiased gradients: ESk|x [g(x)] = 5F (x)
•	Lower bounder: F (x) ≥ Finf
•	Bounded variance: ESk|x||g(x) - 5F (x)||2 ≤ σ2
•	Independence: All random variables are independent to each other
•	Bounded age: The delay is bounded, d < τ
where S is the training dataset, Sk is set s(k1), ..., s(kM) of randomly sampled local batches, L is
the Lipschitz constant.
3.2.2	Update rule
x(km+)1
ξx(km) - ηξg x(km) +
,	(k+ 1)
(m)	(m)
xk	- ηg xk ,
(1-ξ) jMJxkj-d-ηg(xkj-d)]
M
mod τ = d
otherwise
(4)
where x(km) is the weights of worker m at k-th iteration, η the learning rate, M the number of
workers, andg(x(km)) the stochastic gradient of worker m. For every k that satisfies (k+1) mod τ =
d, a global average is updated to local weights. Besides, ξ is an auxiliary parameter to adjust the
weight of local weights in contrast to the global average when fusing them together.
From equation 4, we can define the average weight and the average gradient
MM
μk = M X xki), gk = M X "xki)).
i=1	i=1
Reformatting it, the update rule for the average weight is
μτ(k+1)+d = μτk+d - η
τ-1
ξ E gτk+d+i
i=τ -d
τ -1-d
+ E gτk+d+i
i=0
It is observed that the averaged weight 小丁(k+i)+d is performing a perturbed stochastic gradient
descent. Thus, we will focus on the convergence of the averaged weight μτ(k+i)+d, which is a
common approach in the literature of distributed optimization (Wang & Joshi, 2018; Wang & Joshi,
2018). SGD can converge to a local minimum or saddle point due to the non-convex objective
function F (x). Therefore, the expected gradient norm is used as an index of convergence.
3.2.3	Convergence Rate
The E-SUbOPtimal solution of the algorithm is Ek -K P ∣∣5F(μk)k2 ≤ U The learning rate is
k=1
usually set as a constant and is decayed only when the training process is saturated. Therefore, we
analyze the case of a fixed learning rate and study the lower limit of error at convergence.
5
Under review as a conference paper at ICLR 2022
1K
E - X k5F(μk)k2
K
Theorem (Convergence of DaSGD). Under assumptions, if the learning rate satisfies 2Lηdξ2 -
ξ + 眩 Lf-L^Jd + 6ξL2η2d ≤ 0 and 2Lη(τ - d) - ξ + Q叱-吕 η2(T-d) + 6ξL2η2d +
6L2η2(τ -d) ≤ 0 at the same time, the average-squared gradient norm after K iterations is bounded
as follows
2 [F (μi) - Finf ] + η 2Lσ2 hξ2d + T - di + 2 6L2(1 + ξ) X X σ2 + 2 6dξ2L2τσ2(1+ ξ)
ηK(ξd + τ - d)	M (ξd + τ - d)	" ξd + T - d ι=T-di=0	" (ξd + T - d)(1 - ξ2)
+ nɪ 12dσ2L2ξ2(τ - d)(1 + ξ) + 士 12L2dξ2(τ - d)(1 + ξ) S 口 F )口 2
+ K (ξd + τ - d)(1 - ξ2) + KM (ξd + τ - d)(1 - ξ2) i=ι k5 ( i)kF
where Xk = x1k, ..., xkm, k k2F is the Frobenius norm.All proofs are provided in the Appendix.
Corollary. Under assumptions, if the learning rate is η = MMV JK the average-squared gradient
norm after K iterations is bounded by
-1 K	21	2 [F (μI) - Finf ] + 2Lσ2 hξ2d + T - di	M 2( V、4 6L2(1 + ξ) U A 2
E — X k5F(μk)k2 ≤ —-----------,~f--------L--------+ ——(1+一) ——( + ξ) X X σ2
K £l	—	√MK(ξd + T - d)	K3l M ξd + T - d 士 di⅛
ι M2	I V、4 6dξ2 L2Tσ2(1 + ξ) IM 2U I V、4 12dσ2L2ξ2(T - d)(1 + ξ)
+K3 ( + M) (ξd+T-^)(Γ-ξ2) + K3 ( + M) -(ξd + T - d)(i - ξ2)-
l M /- V V 12L2dξ2 (T - d)(1 + ξ) MU 〃〜、口2
+ K G+ M) (ξd + T-d)(1-ξ2) ∑ k5F(Xi)kF
If the total iterations K is sufficiently large, then the average-squared gradient norm will be bounded
by
1K
E NX k5F(μk)k2
K
2 [F (μi) - Finf ] + 2Lσ2 hξ2d + T - di
≤
√MK(ξd + T - d)
Therefore, on non-convex objectives, for the total iterations K large enough, DaSGD converges
at rate O(1∕√K) consistented with the Mini-batch SGD and the Local SGD. Besides, it is worth
noting that these three introduced auxiliary parameters (τ, d and ξ) are mostly in the high-order term
O(1/K2) and O(1/K3), which has little influence on the convergence of DaSGD.
3.3 Guidelines for Using DaSGD
DaSGD is identical to Local SGD except that the global model is updated for delayed d local steps.
The tuning of the other parameters is the same as that of Local SGD. Therefore, in this paper, we
merely discuss how to set this key delay parameter.
In order to overlap communication with computation in DaSGD, the weight/gradient transfer time tc
across workers is required to be shorter than the time of d local iterations, that is, tc < d × tp, where
tp is the computation time of one iteration. For deep learning systems, the weight transfer time tc
across multiple workers is approximately calculated as tc = m × np/BW, where np denotes the
number of parameters of the model, m the number of workers, and BW the bandwidth of the device.
The computation time tp of one local batch is approximately calculated as tp = Bl × FLOP/FLOPS,
where FLOP denotes the floating-point operation counts of local computation, Bl the local batch
size, FLOPS denotes the computation speed of the device in terms of floating-point operations per
second (FLOPS).
In a nutshell, the delay is given by d > tc = m×np×FLOPS. Note that the delay is highly related to the
tp	Bl ×BW×FLOP
structure of neural network models, e.g., the number of parameters and FLOP, and the configurations
of deep learning training (local batch, worker number, bandwidth of the device and computation
speed). With the rapid improvement of the bandwidth and performance of distributed training, the
mode size of the neural network grows relatively slowly. As a result, in most cases, when the delay
is one single iteration, i.e. d = 1, the weight transfer can be processed completely in parallel with
local updates. In addition, as the worker number increases, the transfer time increases, and hence
the delay would increase moderately accordingly. The cooperative design of various parameters in
DaSGD and hardware is discussed in detail in the following sessions.
6
Under review as a conference paper at ICLR 2022
Table 1: Validation Accuracy of DaSGD, Mini-batch SGD and Local SGD on CIFAR-10.
Model	Validation accuracy after 50 epochs			
	Mini-batch SGD		2*Local SGD	2*DaSDG
	32 batch	1024 batch		
GoogleNet (Szegedy et al., 2015)	0.9467	0.9409	-0.9468-	0.9444
VGG-16 (Simonyan & Zisserman, 2014)	0.9362	0.9264	0.9330	0.9343
ResNet-50 (He et al.,2016)	0.9113	0.9037	0.9062	0.9088
ResNet-101	0.9116	0.9019	0.9061	0.9045
DenseNet-121(Huang etal., 2017)	0.9367	0.9332	0.9369	0.9357
MobileNetV2 (Sandler et al., 2018)	0.9230	0.9304	0.9241	0.9304
ResNeXt29 (Xie et al., 2017)	0.9475	0.9403	0.9424	0.9415
DPN-92 (Chen et al.,2017)	09507	0.9354	09513	0.9502
SSoi aj=∙,ll
-----Mini-batch SGD
-----Local SGD
SSoi a.!'=''」-
"∙ln""4
10	20	30	40
Epoch
0.5	0	0,5
0	10	20	30	40	50
0	=J 0,5
0	10	20	30	40	50
SSol aj=-"」l
0	20	30
Epoch
(a) GoogIeNet
10	20	30	40	50
SSol aj'='」l
",ln""q
(b) VGG-16
10	20	30	40
I - - I ( - f ---
LLQQQ
SSO^∣u-",ll
",ln""q
Epoch
(C) ReSNet-50
Epoch
(d) ReSNet-101
Epoch	Epoch	Epoch	Epoch
(e) DenSeNet-121	⑴ MobiIeNetV2	(g) ReSNeXt29	(h) DPN-92
Figure 3: Validation loss(dotted line), training loss, and validation accuracy(solid line) of different
models on the CIFAR-10 dataset.
4	Experimental Results
In this section, we will first introduce experimental settings, then the comparison of convergence for
different models, and last the effect of different parameters for DaSGD convergence.
4.1	Experimental Setup
We implemented our system based on the Fast.Ai (Howard, 2018) platform upon CIFAR-10 dataset.
The learning rate is linearly increased from 0.0001 to 0.01 in the first 30% epochs and then decreased
from 0.01 to 0.0001 in latter 70% epochs, by respecting One Cycle Policy (Smith, 2017). A larger
learning rate prevents the model from falling in the steep area of the loss function to find a flat
minimum, while a smaller learning rate prevents training from diverging and converging to a local
minimum. The proposed learning rate method assists in realizing high validation accuracy quickly
(in a few iterations) so that we could only set total training epochs as 50 for a comparison between
DaSGD and two other alternatives, Local SGD and Mini-batch SGD. The weight decay (commonly
called L2 regularization) is 0.01. And the momentum of SGD, which accelerates gradients vectors
drops in the right direction, is 0.9.
4.2	Convergence Rate and Accuracy Comparison
We compare DaSGD with Mini-batch SGD and Local SGD in terms of convergence rate and accu-
racy upon a set of neural network models that are trained for 50 epochs for CIFAR-10 dataset. All
models are trained with 32 workers. For the three alternatives, the batch size is 1024 and the total
number of iterations is 2450. For Mini-batch SGD, according to data parallelism, the mini-batch
size of each worker is 32. For Local SGD and DaSGD, the local batch size Bl is 32, which equals
to the mini-batch size of Mini-batch SGD 1. For Local SGD and DaSGD, the number of local steps
τ is 4. For DaSGD, the delayed update steps d is 1.
TABLE 1 first demonstrates that high validation accuracy up to 0.9513 can be obtained with only
2450 iterations with 1024 of batch size, even without tuning hyper-parameters. For Mini-batch SGD,
due to large batch size, the hyper-parameters need to be tuned carefully. Large batch size training
1The mini-batch size is a concept in Mini-batch SGD; the local batch size is a concept of Local SGD. They
equal to each other and are computed as the batch size divided by the number of workers.
7
Under review as a conference paper at ICLR 2022
leads to great accuracy loss. The hyper-parameter recipe for large-batch training is complex and
tedious, and the algorithms based on local update (Local SGD and DaSGD) overcome this problem,
since the batch size in local updating is small (B/m), which is B in Mini-batch SGD. Thus, without
any hyper-parameter adjustment for large-batch training, in addition to MobileNetV2, the validation
accuracy of Local SGD and DaSGD is higher than that of the Mini-batch SGD. Fig. 3 shows this
more clearly. At the beginning of the training, since the batch size is large, the algorithm based on
Mini-batch SGD is usually very unstable, and the validation accuracy fluctuates greatly. And, the
convergence rate of Mini-batch SGD is slower than that of the Local SGD and DaSGD. At the end of
the training, although the training loss of Mini-batch SGD is smaller, Local SGD and DaSGD have
small test loss and higher validation accuracy. In addition, compared with Local SGD, the validation
accuracy of DaSGD is slightly reduced due to delayed averaging.
4.3	Parameter Effects of DaSGD
We evaluate the effects of 5 parameters with respect to the convergence rate and validation accuracy
for the ResNet-50 model based on DaSGD in Fig. 4. 5 parameters are the number of workers m, the
local batch size Bl, the number of local steps τ, the local update percentage ξ and the delay factor
d. We study every parameter by only tuning one at a time, with the rest set as the baseline setting:
m = 32, τ = 4, d = 2, Bl = 32, and ξ = 0.25.
Worker number m. Fig. 4(a) suggests that DaSGD has a fast convergence rate and high validation
accuracy in general for any number of workers tested. As the number of workers increases from 2
to 256, the convergence rate becomes lower and validation accuracy degrades. Since the local batch
size remains unchanged as 32, when the worker number is 256, the total batch size has reached
8192 (32 × 256), which results in a decrease of validation accuracy of around 2% and an increase
of training loss of 0.31. Since the number of local steps is 4, DaSGD communicates once for a total
number of samples of 32K (32 × 256 × 4). The CIFAR-10 dataset has only 50000 training samples,
so the number of communications is just 2 for this large number of workers.
Local batch size Bl. Fig. 4(b) illustrates that DaSGD exhibits a good convergence rate with mod-
erate local batch sizes. When the local batch size is large as 256 or small as 8, the validation ac-
curacy is significantly reduced. Interestingly, the same observation is obtained in Mini-batch SGD.
The phenomena are attributed to that large local batch size, though reducing the overall iterations,
leads to poor generation ability, while small-batch size reduces the generalization error due to noise,
but requires a large number of iterations. Hence, the selection of the right local batch size is critical
to the performance of DaSGD. Based on our tests in Fig. 4(b), 32 or 64 of local batch size is the
setting of choice, resulting in the best validation accuracy and training loss when comparing to the
rest of values.
It is worth noting that the total batch size is computed as B = m × Bl. When the worker number is
32 and the local batch size is 256, the total batch size rises to 8k, which faces the same challenge of
tuning hyper-parameters of large-batch training discussed above. To obtain a good convergence rate
for such a large batch size needs careful coordination between multiple impactful hyper-parameters.
Number of local steps τ . Fig. 4(c) shows that as the number of local steps increases from 4 to
32, the validation accuracy decreases slightly and training loss increases. This concludes that for
DaSGD, the number of local steps should be as small as possible, as long as τ is large enough to
ensure parallel weight communication is totally overlapped by local computation.
Local update percentage. Fig. 4(d) shows that when combining the local weights with the average
global weights, different percentages of local updates have little impact on validation accuracy. From
the update rule equation 4, the local update proportion shares the same meaning as the momentum in
hyper-parameters. One cycle policy in Fast.Ai has shown that different momentum has little effect
on validation accuracy (Howard, 2018).
Delay factor d. Fig. 4(e) tunes the delay factor from 0 to 7 by fixing the number of local steps
as 8, since the delay factor is bounded by the number of local steps. Results show that the delay
factor imposes little effect on the convergence rate. When the delay factor increases from 0 to 7, the
convergence rate slows down and the validation accuracy slightly decreases. Local SGD is just as
the delay factor of 0. Thus, DaSGD incurs slightly worse validation accuracy when using the delay
factor of 1, with the same local steps.
8
Under review as a conference paper at ICLR 2022
Epoch
(a) Worker Number
Epoch
(b) Local Batch Size
Epoch
(C) Number of Local Steps
Epoch
(d) Delay in 8 Local Step
Figure 4: Effect of different parameters of DaSGD based on ResNet-50. The value in legend is the
validation accuracy after 50 epochs.
3.5
Top1
Mini-batch SGD
Local SGD
DaSGD
2.5
:0.3
0.5
O 0.5
^—2 local step 1 delay
3 local step 2 delay
^—4 local step 3 delay
—a local step 4 delay
3.5
>0.9
Top1
S 0.3
Epoch
(C) Local Step & Delay
W 0.5
Figure 5: Top-1/top-5 validation accuracy (left), training loss (right) and validation loss (right, dotted
line) based on ImageNet dataset.

4.4	ImageNet
We use Fast.Ai framework for training on ImageNet. The learning rate increases linearly from
0.0005 to 0.005 in the first 20% epochs and decreases linearly from 0.005 to 0.00005 in the last
80% epochs. The weight decay is set as 0.01, and the momentum of SGD is 0.9.
As shown in Fig. 5(a), three algorithms, which are Mini-batch SGD, Local SGD, and DaSGD, are
trained in 20 epochs to compare the convergence rate and top-1/top-5 validation accuracy in ResNet-
18. All models are trained on 8 workers and the batch size is 256. For Local SGD and DaSGD, the
number of local steps τ is 2. For DaSGD, the delayed update steps d is 1. The top-1/top-5 validation
accuracy of DaSGD (top-1 accuracy is 0.6461 and top-5 accuracy is 0.8582) is the same as that of
Local SGD after 20 epochs, and it is better than that of Mini-batch SGD. Although at the beginning
of training, the convergence rate of DaSGD is slower than that of Mini-batch SGD, after 15 epochs,
it is much faster. We also evaluate the effect of worker number in terms of the training/validation
loss and top-1/top-5 validation accuracy for the ResNet-18 model based on DaSGD in Fig. 5(b). It
can be found that the increase of worker number has almost no effect on the validation accuracy and
convergence rate, which also reflects the application potential of the DaSGD algorithm in large-scale
distributed training. Fig. 5(c) shows the results of DaSGD with different local steps and different
delays. The larger local steps and the larger delay converge faster at the beginning of training, but
the final accuracy is slightly poor.
5	Conclusion
In this work, we propose a new SGD algorithm called DaSGD, which parallelizes SGD and for-
ward/back propagation to hide communication time. Just adjusting the update schedule at the soft-
ware level, the DaSGD algorithm makes better use of distributed training systems and reduces the
reliance on low latency and high peak throughput communication hardware. Theoretical analysis
and experimental results clarify that its convergence rate is O (l∕√K), which is the same as the
mini-batch SGD.
9
Under review as a conference paper at ICLR 2022
References
Nvidia dgx-1 with tesla v100 system architecture. http://images.nvidia.com/content/
pdf/dgx1-v100-system-architecture-whitepaper.pdf, 2017.
Nvidia nvswitch.	https://images.nvidia.com/content/pdf/
nvswitch-technical-overview.pdf, April 2018.
Gaudi™ training platform white paper. https://habana.ai/wp-content/uploads/
2019/06/Habana-Gaudi-Training-Platform-whitepaper.pdf, 2019.
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning
on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd: Training resnet-
50 on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric
Renggli. The convergence of sparsified gradient methods. In Advances in Neural Information
Processing Systems, pp. 5973-5983, 2018.
Luiz Andre Barroso, Urs Holzle, and Parthasarathy Ranganathan. The datacenter as a computer:
Designing warehouse-scale machines. Synthesis Lectures on Computer Architecture, 13(3):i-189,
2018.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:11512.01274, 2015.
Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual
path networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
4467-4475. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7033-dual-path-networks.pdf.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In
Advances in neural information processing systems, pp. 1223-1231, 2012.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization 23(4):2341-2368, 2013.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Jeremy Howard. Now anyone can train imagenet in 18 minutes. https://www.fast.ai/
2018/08/10/fastai-diu-imagenet/, August 2018.
10
Under review as a conference paper at ICLR 2022
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Ang Li, Shuaiwen Leon Song, Jieyang Chen, Jiajia Li, Xu Liu, Nathan R Tallent, and Kevin J
Barker. Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect. IEEE
Transactions on Parallel and Distributed Systems, 31(1):94-110, 2019.
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for
nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 2737-2745,
2015.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Re-
ducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887,
2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. Advances in Neural Information Processing Systems 32:8024-8035, 2019.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in neural information processing systems,
pp. 693-701, 2011.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Confer-
ence on Applications of Computer Vision (WACV), pp. 464-472. IEEE, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Computer Vision and Pattern Recognition (CVPR), 2015. URL http://arxiv.org/abs/
1409.4842.
J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.
Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-runtime
trade-off in local-update sgd. arXiv preprint arXiv:1810.08313, 2018.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Advances in Neural Information Processing Systems, pp.
1299-1309, 2018.
S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated residual transformations for deep neural
networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
5987-5995, 2017.
Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng. Image classification at
supercomputer scale. arXiv preprint arXiv:1811.06992, 2018.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 6, 2017a.
11
Under review as a conference paper at ICLR 2022
Yang You, Zhao Zhang, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Imagenet training in 24
minutes. arXiv preprint arXiv:1709.05011, 2017b.
Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In
Advances in neural information processing Systems, pp. 685-693, 2015.
A Appendix
Convergence Analysis of DaSGD
A.1 Assumptions
We define some notations. S is the training dataset, Sk is set s(k1), ..., s(kM) of randomly sampled
local batches at M workers in k iteration, L is the Lipschitz constant, d is the number of local
iteration that global weight updates are delayed, τ is the number of local steps, x is the weight of
devices. The convergence analysis is conducted under the following assumptions:
•	Lipschitzian gradient: || 5 F(x) - 5F (y)|| ≤ L||x - y||
•	Unbiased gradients: ESk|x [g(x)] = 5F (x)
•	Lower bounder: F (x) ≥ Finf
•	Bounded variance: ESk|x||g(x) - 5F (x)||2 ≤ σ2
•	Independence: All random variables are independent to each other
•	Bounded age: The delay is bounded, d ≤ T
A.2 Update Rule
The update rule of DaSGD is given by
(m)
xk+1
x(km) - ηg x(km) ,
ξx(km) - ηξg x(km)
otherwise
+ 1M-ξ Plhxkj)d- ηg (xkj)d)i , (k +1 - d) mod T = 0
where x(km) is the weights at m worker in k iteration, η is the learning rate, M is the number of
workers, g(x(km)) is the stochastic gradient of worker m, ξ is the local update proportion, delayed
update is the case (k + 1 - d) mod τ = 0.
Matrix Representation. Define matrices Xk, Gk ∈ Rd×M that concatenate all local models and
gradients in k iteration:
Xk
, ..., xkm , Gk
g x(k1) , ...,g x(km)
Then, the update rule is
Xk+1
ξ(Xk-ηGk)+(1-ξ)(Xk-d-ηGk-d)J, (k+1-d) modτ=0	5
Xk - ηGk,	otherwise	(5)
Update Rule for the Averaged Model. The update rule of DaSGD is given by
(m)
xk+1
x(km) - ηg x(km) ,
ξx(km) - ηξg x(km)
otherwise
+ ⅛ξ Plhxkj-d - ηg (xkj)d)i, (k + 1 - d) mod T = 0
Here, we set
M
1	(i)
xk = Mrxk , gk
i=1
12
Under review as a conference paper at ICLR 2022
The average weight on different workers is obtained by
xk+1
Xk - ηgk,
ξχk + (1 - ξ)Xk-d - ηξgk - η(i - ξ)g(χk-d),
otherwise
(k + 1 - d)
mod T = 0
When Z = T(k + 1) for Z mod T = 0, we have
xτ(k + 1)+d = ξxτ(k+1)+d-1 + (1 - ξ)xτ(k + 1)-1 - ξηgτ(k + 1)+d-1 - (1 - ξ)ngτ(k + 1)-1
T-1	T-1-d
=ξxτk+d + (1 - ξ)xτk+d - ξη〉: gτk+d+i - (1 - ξ)η): gτk+d+i
i=0	i=0
Xτk+d - n
gτ-1-d	、	T-1-d
gτk+d+i - Σ gτk+d+i j + Σ gτk+d+i
i=0	i i=0
Xτk+d - n
-T-1
ξ E gτk+d+i
.i=τ-d
T-1-d	-
+ E gτk+d+i
i=0
If we set K(k) = τk + d
-T-1	T-1-d
xK(k+1) = xK(k) - n ξ E gK(k)+i + E gK(k)+i
i=τ -d	i=0
For the ease of writing, we first define some notations. Let Sk denote the set {skI),…，Skm)} of
mini-batches at m workers in iteration k. Besides, define averaged stochastic gradient and averaged
full batch gradient as follows:
1 M
GK (k) = ME
m=1
^ τ-1	τ-1-d
X ξg (xT*d+i)+ X g (xT7k+d+i
i=τ -d	i=0
HK(k)
-τ-1	τ-1-d
X ξ 5 F (XTk+d+i) + X 5F (XTT+d+i)
i=τ -d	i=0
M
1	(i)
μK(k) = Mx^XTk+d
(6)
⑺
(8)
Then we have
μK(k+1) = μK(k) - nGK(k)
A.3 Convergence Rate
Theorem (Convergence of DaSGD). When the learning rate satisfies the following two formulas
at the same time
2Lndξ2 ξ + 6ξL2n2d + 6L72(T-d) + 6hn2d ≤ 0
1 - ξ2
2Ln(τ - d) - ξ + 6ξL2n2d +6L2n2(T- d) + 6ξUn2d + 6L2η2(τ - d) ≤ 0
1	- ξ
Then the average-squared gradient norm after K iterations is bounded as
1K
EK(k) kΣ l∣5F(μK(k))∣∣2
k=1
≤ 2[F (〃1)- Finf ] + n2Lσ2 fd + T - d] + 2 6L2(1 + ξ) X X 2+ 2 6dξ2L2Tσ2(1 + ξ)
_ nK(ξd + T — d)	M(ξd + T — d)	ξd + T — d ( d~^i (ξd + T — d)(1 — ξ2)
+ nɪ 12dσ2L2ξ2(T - d)(1 + ξ)
+ K (ξd + T - d)(1 - ξ2)
n2 12L2dξ2(T - d)(1 + ξ)
KM (ξd + T - d)(1 - ξ2)
d-1
X k5F(Xi)kF
i=1
+
13
Under review as a conference paper at ICLR 2022
where μk = M PM=I XTik+d，kkF is the FrObeniUs norm.
Corollary. Under sumptions, if the learning rate is η = MMV yβK the average-squared gradient
norm after K iterations is boUnded by
EK(k
1K
K XII5F (μκ(k))ll2
k=1
≤ 2[F(μι) - Finf ] +	1	2Lσ2 [ξ2d + T - d
一√MK(ξd + τ — d)	√MK	(ξd + T - d)
+ M(1 + VY 6L2(1+ ξ) X Xσ + M(1 + VY 6dξ2L2τσ2(1+ ξ)
+ K3 V + M	ξd + τ - d 乙乙 + K3 V + MJ (ξd + τ - d)(1 - ξ2)
l=τ -d i=0
M2 (1	VY 12dσ2L2ξ2(τ - d)(1 + ξ)
+ K ( + M)	(ξd + T - d)(1 - ξ2)
M V4
+ K (1 + M)
12L2dξ2(τ - d)(1+ ξ)
(ξd + τ - d)(1 - ξ2)
d-1
X k5F (Xi )k2F
i=1
If the total iterations K is sufficiently large, then the average-squared gradient norm will be bounded
by
1K
K ∑k5F (μk )k2
E
2	[F(μI)- Finf ] + 2Lσ2 [ξ2d + T - d]
√MK(ξd + T - d)
A.4 Proof of Convergence Rate
Lemma 1. If the learning rate satisfies η ≤ min
are initialized at the same point, then the average-squared gradient after K iterations is bounded as
follows
,2L(ξ-d) } and all local model parameters
EK(k)
1K
K Xll5F(μK(k))l∣2
k=1
≤2 [F(μι) - Finf] + 2Lησ [ξ2d + T - d]
一ηK (ξd + T — d)	M (ξd + T — d)
2	K M	τ -1-d	2	τ-1
+ KM(ξd+	T-d)	XX X EK(k) HμK(k)	-	XTm+d+』+ ξ X	EK(k)	HμK(k)-
k=1 m=1 i=0	i=τ -d
X
(m)
τ k+d+i
2
Proof.
From the Lipschitzisan gradient assumption || 5 F(X) - 5F (y)|| ≤ L||X - y||, we have
F(xK(k+1)) - F(XK(k)) ≤〈5F(XK(k)), xK(k + 1) - XK(k)) + 2 llXK(k+1) - XK(k)『
=-η〈5F(XK(k力,GK(k)) + ~2~ HGK(k)H2
(9)
Taking expectation respect to SK(k) on both sides of equation 9, we have
EK(k) [F(XK(k+1) )] - F(XK(k)) ≤ - ηEK(k) [〈▽F(XK(k)), GK(k))] +---2-EK(k)[八GK(k) H
ha,bi = 2 (MF + 阳I2-Ila-"『)
14
Under review as a conference paper at ICLR 2022
we have
EK(k) [F(XK(k+1) )] — F (XK (k)) ≤ — ηEK(k) KVF(XK(k)} GK(k))] +-2- EK(k) [ 1 1 GK(k)|2j
Combining with Lemmas 4 and 5, we obtain
EK(k) [f(XK(k +1))] — f(XK(k))
≤ - ηEK(k) KVF(XK(k)), GK(k))] +-2~EK(k) [∣ I GK(k)∣∣2]
≤ - ηξd +2τ - d IIvf(XK(k))12 + LM^ [dξ2 + T - d]
+
+
—
-Lη2dξ2
M
τ -1
X kVF (χτk+d+i)IlF +
i=τ -d
-T-1-d
2MX X IIvF"H5F(：
m=1
i=0
Lη2(τ — d)
—
τ-1-d
X IVF (Xτk+d+i)IF
i=0
，	2	τ-1
xτk+d+i)II + ξ X ∣∣VF(XK(k)) -VF (xτk+d+i
i=τ -d
M
M
2
≤ — η
ξd + T — d
IlVF (μK(k))∣∣2 +	ηΜ- [dξ2+T- d]
2
—
-Lη2dξ2
M
τ -1
X IvF(Xτk+d+i)kF +
i=τ -d
τ -1-d
Lη2(τ — d)
—
τ-1-d
X IVF (Xτk+d+i)I2F
i=0
+ ηL2
+ 2M
X X ∣∣μK(k) - X
m=1
i=0
2 T-1	2
Im+d+1+ ξ X HμK(k)-xTm+d+1
i=τ -d
(10)
+
M
M
where equation 10 is due to the Lipschitzisan gradient assumption ||vF (X)-VF (y)|| ≤ L||x - y||.
After minor rearranging and according to the definition of Frobenius norm, it is easy to show
ηξd⅛^ UvF(d2
一	L ∣-π,	、i	Lη2σ2「…。	,-I
≤f(μK(k)) - EK(k) [F(μK(k+1))] +	M- [dξ + T - d]
—
一 Lη2dξ2
M
T -1
X kVF (χτk+d+i)IlF +
i=τ -d
τ -1-d
Lη2(τ — d)
—
τ-1-d
X IVF (Xτk+d+i)IF
i=0
+ ηL2
+ 2M
X X H〃K(k)-X
m=1
i=0
2 τ-1	2
:τm+d+iU+ ξ X HμK(k)- xTm+d+』
i=τ -d
+
M
M
Taking the total expectation and averaging over all iterates, we have
≤
ηRP± EK(k)
1 ʌ ,,	,,9
κ X ii VF(〃/))i i
F (μ1) - Finf	Lη2 σ
+ M
K
-Lη2dξ2
KM
k=1
2
—[dξ2 + T — d]
—
K T-1
X X IlVF(XTk+d+i)kF +
+ JLL
+ 2KM
k = 1 i=τ-d
Γτ-1-d
—
Lη2(τ — d)
-KM-
K τ-1-d
X X IlVF(XTk+d+i)kF
k=1 i=0
XX X
μK(k) — X
k=1 m=1
i=0
2	τ-1	2
:Tm+d+i∣∣ + ξ X ||〃K(k) -χTm+d+i∣∣
i=τ -d
+
K M
15
Under review as a conference paper at ICLR 2022
Then, we have
1K EK(k) K EU5F(μK(k))I∖2	≤ 2[F(μι)- Fin] , 2Lησ2 [ξ2d + τ - d] 一ηK (ξd + τ — d)	M (ξd + τ — d) + KM(ξdI T - d) X X EK(k) k5F(XTk+d+i)kF k=1 i=T-d
+
K τ -1-d
X X EK(k) k5F (Xτk+d+i)k2F
k=1 i=0
2Lη(τ - d) - ξ
KM (ξd + τ - d)
L2	K τ -1-d M
KM(ξd + T- d X X XIEK(WK(LxTm+ d+i
+
ξL2
KM (ξd + τ - d)
K	τ-1	M
X X X EK(k)
k=1 i=τ-d m=1
μκ(k)-
(m)
τ k+d+i
(11)
x
2
,2¼ξ-d) },then
If the learning rate satisfies η ≤ min
EK(k)	1K K X ∖5F(μK(k))∖2	≤ 2 [F(μl) - Finf] 一ηK (ξd + τ — d)	+	2Lησ2 [ξ2d + T — d] M (ξd + T - d)		
		L2 + KM (ξd + T -		K T -1-d M		2
			d)	X X X EK(k) k=1 i=0 m=1	∖μK(k)-	x(m)	II2 xTk+d+i I
		+—J + KM (ξd + T -	d)	K T-1 M X X X EK(k) k=1 i=T -d m=1	∣μK(k)-	x(m)	II2 xT k+d+i I
Recalling the definition μκ(k) = M PMI xTk+d = Xκ(k)lM/M and adding a positive term to the
RHS, one can get
τ-1 M
X X IMk) - xτm+d+i
i=τ -d m=1
2
τ-1
kXτk+dJ - Xτ k+d+i kF
i=τ -d
We have
1
EK(k)	KEU5F (μK(k))I∣2
≤2[F(μι)- Fin] 1 2Lησ2fd + T - d]
一ηK (ξd + τ — d)	M (ξd + τ — d)
L2	K τ -1-d
+ km(ξd + τ - d) E E EK(k) kXτk+dj - Xτk+d+ikF
(ξ +T - ) k=1 i=0
ξL2	K τ-1
+ KM (ξd + T-d) NC ∑∕K(k) kXτk+d J - Xτk+d+ikF
Lemma 2.
忸”)『≤ Mj- X II5F(Xτk+d+i)IlF +( M~~X X II5F(XTk+d+/IlF	(12)
i=τ -d	i=0
16
Under review as a conference paper at ICLR 2022
Proof.
IIHκ(⅛)l∣2
T-1 M
W XX 5F (
i=τ — d m=1
x(m)
xτk+d+i
τ—1—d M
+E X X 5F (xTm+d+i)
i=0 m=1
<
2dξ2
"M2
τ — 1
X
i=τ —d
M
X 5F
m=1
x(m)
xτk+d+i
2
+
2(T — d)
M2
T —1 — d
X
i=0
M
X 5F ("d+i)
m=1
(13)
τ τ τi) T —1	M	τ τ τ τ T —1 — d M	Q
≤ 寺 X X∣∣5F M+d+i)∣∣ +" X X∣∣5F /+d+i)∣∣
i=τ—d m=1	i=0 m=1
(14)
号 X ∣∣5F (Xτk+d+i)kF + 2⅛-τXd k5F (Xτk+d+i)kF
i=τ —d	i=0
where equation 13 is due to ∣∣α + b|『 ≤ 2 ∣∣α∣∣2 + 2 ∣∣b∣∣2, equation 14 comes from the convexity of
vector norm and Jensen,s inequality.
Lemma 3. Under assumptions ESk |x [g(x)] = 5F (x) and ESk |x||g(x) 一 5尸(x)||2 ≤ σ2, we have
the following variance bound for the averaged stochastic gradient:
EK(k) [∣ ∣ GK(k) 一 HK(k) ∣∣[ ≤ ^m^ [dξ2 + T 一 d]	(15)
Proof. According to the definition of equation 6, equation 7, and equation 8, we have
EK(k)
k)- Hκ(k)∣∣2]
)-5F 卜 τm+d+i)]∣∣
Il T — 1 M	T —1 — d M
MEκ(k) I ∣∣ξ X X[g (xTm+d+i) - 5F /")] + X X[g (xTm+d+i
∣ ∣ i=τ — d m=1	i=0 m=1
≤ MEK(k)1 ∣∣ξ X X Ig(XTm+d+) -5F (XTm+d+)]
∣ ∣	i=τ — d m=1
2
τ—1 — d M	∣ ∣ 2
X X % (χTm+d+i)―5F (χTm+d+J]∣∣
i=0 m=1
+
(16)
「	T — 1	M	2	τ — 1 — d M	2
MEK(k)	ξ2 X X	∣∣g (χTm+d+i)	一 5F (XTm+d+i)	∣∣ + X	X	∣∣g (xTm+d+i)	一 5F 卜Tm+d+i) ∣∣
L i=τ—d m=1	i=0	m=1
(17)
τ —1 M
+ ξ2 XXXTm+d+i) - 5f 卜 Tm+d+i) , "xT'k+d+J - 5F (xT'k+d+j)〉	(18)
j=i l=m
τ—1—d M
+ X X G(XTm+d+i) 一 5F (XTm+d+i) , "xT'k+d+j) 一 5F (XT'k+d+j)》	(19)
j=i l=m
9.2 τ — 1 M	2 O τ —1 — d M
M X	X	EK(k)	∣∣g 卜Tm+d+i)	一 5F 卜Tm+d+i)	∣∣ + M X	X	EK(k) ∣∣g 卜Tm+d+i) ― 5F (XTm+d+i
i=T —d	m=1	i=0	m=1
(20)
where equation 16 is due to ∣∣α + b∣2 ≤ 2 ∣∣ɑ∣2 + 2 ∣∣b∣2, equation 20 is due to Sk are independent
random variables and the assumption Esk∣x [g(x)] = 5F(x). Now, directly applying assumption
17
Under review as a conference paper at ICLR 2022
Esk∖x∖∖g(χ) -VF(x)ll2 ≤σ2 to equation 20. Then, we have
OC T-1 M	τ T-1-d M	2 2
≡κ(fe)	[∣∣Gκ(fc)	-Hκ(fc)∣∣2] ≤ M X X	σ2+而 X	X	σ2 = ɪ [dξ2+τ - d∖
i=τ — d m=1	i=0	m=1
Lemma 4. Under assumption ESk∖χ [g(x)] = VF(x), the expected inner product between stochas-
tic gradient and full batch gradient can be expanded as
EK(k) KVF(XK(k)), GK(k)}]
ξd + _ d	1	「 T-1	T-1-d
ξ +2T- IIvF(Xκ(k))∣∣2 + 而 ξ E kvF(Xτk+d+i)kF + E kvF(Xτk+d+i)kF
i=T -d	i=0
M 「t-1-d	2 τ-1	2
-由 X X ∣∣VF(Xκ(k))-vF(xτk+d,+i)∣∣ + ξ X ∣∣vF(χκ(k))-vF(xτk+d+i) ∣ ∣
m=1 L i=0	i=τ-d
Proof.
EK(k) [〈vF(Xκ(k)), GK(k))∖
EK(k)
KVF (XK (k)
τ-1 M
),ξM X Xg(
i=τ-d m=1
(m)
τk+d+i
T -1-d M
X X g(x
i=0 m=1
(m)
τk+d+i
1
+ -
+ M
τ-1 M
ξM X X(VF(XK(k)), VF (xτk+d+i)E
i=τ-dm=1
T -1-d M
+ M X X <vF(XK(k)), vF (xτk+d+i)E
i=0 m=1
T	τ —1	M -	2	2^
2M X X ∣∣vF(χK(k))∣∣2 + ∣∣vF (XTmL+i)∣∣ - ∣∣vF(XK(k))-VF (XTmL+i)∣∣
i=τ-d m=1 L	」
(21)
T-1-d M ∣-	2	2
+ 2M X X ∣∣vF(XK(k))『+1∣VFGTm)d+i)∣∣ -IVF(XK(k))- VF(XTm)d+J∣∣
i=0 m=1
(22)
ξd +	_	d	I	「T-1	T-1-d	-
ξ +2T-	∣∣vF(χK(k))∣∣2 + 而	ξ E	kvF(Xτk+d+i)kF	+ E	kvF(Xτk+d+i)kF
i=T -d	i=0
M 「t-1-d	2 τ-1	2
-2m X X ∣∣vF(XK(k))-vF(xτk⅛d⅛i)∣∣ + ξ X ∣∣vF(χK(k))-vF(xτk+d+i) ∣ ∣
m=1	i=0	i=T -d
where equation 21 and equation 22 come from(a, b) = ɪ (∖∖a∖∖2 + ∖∖b∖∖2 - ∖∖a - b∖∖2).
Lemma 5. Under assumptions Eξ∖x [g(x)] = VF(x) and Eξ∖x∖∖g(x) -VF(x)∖∖2 ≤ σ2,the squared
norm of stochastic gradient can be bounded as
EK(k) [∣ ∣ GK(k)∣∣2] ≤ Mf [dξ2 + T - d∖+ ^^M- X kvF (XTk+d+i)llF + ■-^~m~~X X l∣vF(XTk+d+i)llF
i=T -d	i=0
18
Under review as a conference paper at ICLR 2022
Proof.
EK(k) hGK(
EK(k) hGK(
k) — EK(k) [GK(k)]ll i + llEK(k) [GK(k)]ll
k) - HK(k)2 + HK(k)2
≤	^M^	[dξ2+T —	d]+ M~	X k5F(Xτk+d+i)∣∣F +(	M~~X	X	k5F(χτk+d+i)kF
i=τ -d
i=0
(23)
where equation 23 follows equation 12 and equation 15.
Theorem 1 (Convergence of SGD). Under assumptions, when the learning rate satisfies the follow-
ing two formulas at the same time
2Lηdξ2 - ξ+
6ξL2 η2 d + 6L2η2 (τ - d)
1 - ξ2
+ 6ξL2η2d ≤ 0
2Lη(τ - d) - ξ +
6ξL2η2d + 6L2η2(τ - d)
1-ξ2
+ 6ξL2η2d + 6L2η2(τ - d) ≤ 0
Then the average-squared gradient norm after K iterations is bounded as
EK(k)
1K
K X ∣l5F (μK(k))l∣2
k=1
≤ 2 [F(μi) - Finf] + η2Lσ2 Fd + T - d] + 2 6L2(1 + ξ) ∖X ^X 2 + 2 6dξ2L2R2(1 + ξ)
一ηK (ξd + τ — d)	M (ξd + T — d)	T ξd + T — d ʌZ ʌ/Cr n (ξd + T — d)(1 — ξ2)
l=τ -d i=0
+ η2 12dσ2L2ξ2(τ — d)(1 + ξ + Jn_ 12L2dξ2(T — d)(1 + ξ X k F(X )k2
+ K (ξd + T — d)(1 — ξ2)	+ KM (ξd + T — d)(1 — ξ2) 乙 k5 ( i)kF
where μk =焉 PM=I xTik+d,
Proof.
k k2F is the Frobenius norm.
Recall the intermediate result equation 11 in the proof of Lemma 1:
EK(k)
1K
K XII5F (μκ(k))ll2
≤2[FJi) — Finf] l 2Lησ2 [ξ2d + t — d]
一ηK (ξd + t — d)	M (ξd + T — d)
,2Lηdξ2 - ξ
+ KM (ξd + τ - d)
K τ-1
XX
EK(k) k5F (Xτ k+d+i)k2F
+ 2Lη(τ - d) - ξ
+ KM (ξd + τ - d)
L2
+ KM (ξd + T - d)
ξL2
k=1 i=τ -d
K τ -1-d
XX
k=1 i=0
K τ -1-d
XX
k=1 i=0
K τ-1
EK(k) k5F (Xτk+d+i)k2F
EK(k) kXτ k+dJ - Xτ k+d+i k2F
KM(ξd+τ -d)
Σ Σ EK(k) kXτ k+d J — Xτ k+d+ikF
k=1 i=τ -d
Our
K
P
(24)
goal is to provide an upper bound for the network error term
τ-1	2
EK(k) kXτ k+dJ — Xτk+d+ikF . First of all, let us derive a specific expression for
k=1 i=τ -d
+
19
Under review as a conference paper at ICLR 2022
XTk+dJ - XTk+d+i. According to the update rule equation 5, one can observe that
Xτk+dJ - x-τk,+d+i
i
Xτk+d(J - I) + η〉: Gτk+d+j
j=0
i
ξ (XTk+d-1 - ηGτk+d-I)(J - I) + (I - ξ) (XTk - ηGTk ) J(J - 1) + η ^X Gτk+d+j
j=0
τ — 1	i
ξXτ(k-i)+d(J -1) - ξ<jΓ GT (k—1)+d⅛i(J - I)+ η E Gτk+d+j
i=0	j=0
2 τ-1	i
ξ2Xτ (k-2)+d(J - I)- n£ Eξ" GT (k-j)+d+i (J - I) + η EGTk+d+j
j=1i=0	j=0
k τ -1	i
ξkXd(J - I)- η ^X ^X ξj Gτ(k-j) + d+i(J - I)+ η ^X Gτk+d+j
j=1i=0	j=0
k τ 一 1	i
ξk (Xd-I- ηGd-1) (J -I)- η XX ξjGτ(k-j)+d+i(J - I)+ η X Gτk+d+j
j=1i=0	j=0
d-1	k τ -1	i
ξkX1 (J - I) - ηξk X Gi (J - I) - η XX ξjGτ(k-j)+d+i (J - I) + η X Gτk+d+j
i=1	j=1 i=0	j=0
d-1	k τ -1	i
-ηξk X Gi(J -I)- η XX ξj GT(k-j)+d+i (J - I)+ η X GTk+d+j	(25)
i=1	j=1 i=0	j=0
where equation 25 follows the fact that all workers start from the same point at the beginning of each
local update period.
20
Under review as a conference paper at ICLR 2022
Accordingly, we have
τ-1
EK(k) kXτ k+d J - Xτ k+d+i kF
i=τ -d
l
2
τ-1
EK(k)
l=τ -d
≤3η2EK(k)
≤3η2EK(k)
d-1
k τ-1
-ηξk	Gi(J-I)-η	ξjGτ(k-j)+d+i(J - I) + η	Gτk+d+i
i=1
d-1
ξ2kd	Gi(J - I)
i=1
d-1
ξ2kd Gi + d
i=1
=3η2∑ Eκ(k)ξ2k d
m=1
=3η2 d
M
X EK(k)ξ2k
m=1
j=1 i=0
k τ-1
+ d	ξj Gτ (k-j)+d+i(J - I)
j=1 i=0
k τ-1
X X ξjGτ(k-j)+d+i
j=1 i=0
d-1
X g(xi(m))
i=1
d-1
X g(xi(m))
i=1
{z
T1
i=0
τ-1
+	Gτ
F	l=τ -d
i=0
k+d+i
+ dEK(k)
τ-1
+X
F l=τ -d
l
Gτk+d+i
i=0
(26)
k τ-1
XXξjg(x(τm(k)-j)+d+i)
j=1 i=0
M
+	EK(k)
m=1
k τ-1
X ξj X g(x(τm(k)-j)+d+i
j=1	i=0
{z
T2
τ-1
+ EK(k)
l=τ -d
l
X (x(m)
g(xτ k+d+i
i=0
1 M τ-1
+ d XX EK(k)
m=1 l=τ -d
X-----------------
l
X g(x(τmk+) d+i)
i=0
(27)
{z
T3
M
2
F
2
l
2
F
2
2
}
2
2
2
2
F
F
2
2
)
)
}
where the equation 26 is due to the operator norm of J - I is less than 1.
For T 2, we have
M
EK(k)
m=1
2
k τ-1
XξjXg(x(τm(k)-j)+d+i)
j=1	i=0
M
EK(k)
m=1
2
k	τ-1	k	τ-1
Xξj X hg(x(τm(k)-j)+d+i) - 5F(x(τm(k)-j)+d+i)i +XξjX5F(x(τm(k)-j)+d+i)
j=1	i=0	j=1	i=0
2
M
≤2 X EK(k)
m=1
k τ-1
XξjXhg(x(τm(k)-j)+d+i
j=1	i=0
) - 5F (x(τm(k)-j)+d+i)
M
+ 2 X EK(k)
m=1
} 、
k τ-1
XξjX5F(x(τm(k)-j)+d+i)
j=1	i=0
z
z
2
}
For the first term T4, since the stochastic gradients are unbiased, all cross terms are zero. Thus,
combining with Assumption of bounded variance, we have
M k	τ-1	2
T4= 2XXξ2jXEK(k) g(x(τm(k)-j)+d+i) - 5F (x(τm(k)-j)+d+i)
m=1 j=1	i=0
M k	τ-1
≤ 2XXξ2jXσ2≤
m=1 j=1	i=0
2Mτσ2ξ2
1 - ξ2
(28)
where equation 28 according to the summation formula of power
21
Under review as a conference paper at ICLR 2022
For the second term T5, we get
T5
k	τ-1	2
2Xξ2j XE 5F(X(τm(k)-j)+d+i)F
j=1	i=0
k-1	τ-1	2
2 X ξ2(k-r) X E 5F (X(τmr+) d+i)F
r=0	i=0
Substituting the bounds of T4 and T5 into T2 , we have
T2 ≤ 2M-σξJ2+2Xξ2(j)XEKF(XTm+d+i)∣∣F
- ξ	r=0	i=0	F
For T1, we have
M
X EK(k)ξ2k
m=1
M
= X EK(k)ξ2k
m=1
M
d1
X g(xi(m))-5F(xi(m)) +X5F(xi(m))
i=1
d1
i=1
2M
≤ 2 X EK(k)ξ2k X g(xi(m))-5F(xi(m))	+2XEK(k)ξ2k
m=1
、
i=1
{z
T6
m=1
} 、
d-1
X 5F(xi(m))
i=1
*{z
T7
2
2
2
}
For the first term T6, since the stochastic gradients are unbiased, all cross terms are zero. Thus,
combining with Assumption of bounded variance, we have
M d-1	2	M d-1
T6=2Xl * * * * * * * * XXEK(k)ξ2kg(xi(m))-5F(xi(m))2≤2ξ2kXXσ2 = 2ξ2k Mdσ2
m=1 i=1	m=1 i=1
For the second term T7, directly applying Jensen’s inequality, we get
M
T7= 2 X EK(k)ξ2k
m=1
d-1
X5F(xi(m))
i=1
2	M	d-1	2
≤ 2dXEK(k)ξ2kX5F(xi(m))
m=1	i=1
d-1
2dξ2k X k5F(Xi)k2F
i=1
Substituting the bounds of T6 and T7 into T1 , we have
d-1
T1 ≤ 2ξ2k Mdσ2+2dξ2k Xk5F(Xi)k2F
i=1
For T3 , we have
1 M τ-1
T3 = d X X EK(k)
m=1 l=τ -d
1 M τ-1
=d £ EK( EK(k)
m=1 l=τ -d
2 M τ-1
≤ d XX EK(k)
m=1 l=τ d
l2
Xg(x(τmk+)d+i)
i=0
Xl	g(x(τmk+) d+i) - 5F (x(τmk+) d+i)	+Xl	5F(x(τmk+)d+i)
i=0	i=0
2
l
i=0
x(τmk+)d+i)-5F(x(τmk+)d+i)
2 M	τ-1
+ d XX EK(k)
m=1 l=τ d
l
X 5F (x(τmk+) d+i)
i=0
M τ-1	l	2	M	τ-1	l	2
≤ d X X XEκ(∕∣g(χTm+d+i)-5F(XTm+d+i)∣∣ + d XX X旧“)卜下(XTm+dj∣
m=1 l=τ -d i=0	m=1 l=τ -d i=0
M T-1 l	M T-1 l	2
≤ dXXXσ2+dX X XEK(k)卜F(xTm+dJI
m=1 l=T d i=0	m=1 l=T d i=0
2M
d
T-1	l	M T-1 l	2
X Xσ2 + d X X XEK(k55F(XTm+d+,”
l=T d i=0	m=1 l=T d i=0
22
Under review as a conference paper at ICLR 2022
Substituting the bounds of T1, T2 and T3 into equation 27, we have
τ-1
2
F
E
K(k) kXτk+dJ - Xτ k+d+i k
i=τ -d
≤ 3η2d 2ξ2kMdσ2 + 2M-ξf +2dξ2kX k5F(Xi)kF + 2 X ξ2(I) X EKF(*以/|：
i=1	r=0	i=0
2M
τ-1 l
XXσ2
l=τ -d i=0
M τ-1 l
+ d XX X EK(k)
m=1 l=τ -d i=0
5F (x(τmk+) d+i)2
+才
And in the same way, we have
τ -1-d
2
F
E
K(k) kXτ k+dJ -Xτk+d+l k
l=0
≤ 3η2(τ - d) 2ξ2kMdσ2 + 2；R + 2dξ2k X ∣RF(Xi)kF + 2 X ξ2(I)X E 卜F(*以/|：
i=1	r=0	i=0
τ-1-d l	M τ-1-d l	2
+芝 X Xσ2 + 三 X X XEK(k) m(xTm+d+i)∣∣
τ-	τ-
l=0 i=0	m=1 l=0 i=0
Then, summing over all periods from k = 0 to k = K, where K is the total global iterations:
K τ-1
2
F
EK(k) kXτ k+dJ - Xτ k+d+i k
k=1 i=τ -d
K	2 2	d-1	k-1	τ-1	2
≤ 3η2dX 2ξ2kMdσ2 + 2MMTξ2 + 2dξ2k X ∣RF(Xi)kF + 2 Xξ2(I) XEKF(*几十』|
k=1	i=1	r=0	i=0	F
2M
τ-1 l
XXσ2
l=τ -d i=0
M τ-1 l
+ d X X X EK(k)
m=1 l=τ -d i=0
5F (x(τmk+) d+i)2
+才
≤ 6η2d ] §	2Mdσ2 + 2d ^Xk5F(Xi) kd + 6η2dTσ2ξ2MK +6η2MK X X σ2
i=1	l=τ -d i=0
K k-1	τ-1	2	K τ-1	l
+6η2dXXξ2(k-r)XE5F(X(τmr+)d+i)F+6η2XXXEK(k) k5F(Xτk+d+i)k2
k=1 r=0	i=0	k=1 l=τ -d i=0
(29)
Expanding the summation, we have
K k-1	τ-1	2
XX ξ2(k-r) X E 5F (X(τmr+) d+i)F
k=1 r=0	i=0
≤ XK	τX-1 E k5F(Xτr+d+i
r=1	i=0
≤ XK	τX-1 E k5F(Xτr+d+i
r=1	i=0
K τ-1
≤ 1⅛ XXE k5F (Xτk+d+i)k2F
- ξ k=1 i=0
(30)
23
Under review as a conference paper at ICLR 2022
And in the same way, we have
K τ-1 l	K τ-1
X X XE k5F(Xτk+d+i)k2F ≤ dXXE k5F(Xτk+d+i)k2F
k=1 l=τ -d i=0	k=1 i=0
(31)
Plugging equation 30 and equation 31 into equation 29,
K τ-1
EK(k) kXτ k+d J - Xτk+d+i k
k=1 i=τ -d
≤ 6η2d 1—ξ2
2Mdσ2 + 2dX k5F(Xi)kF + 6*^^"十 6〃2〃衣 X X02
.	i=1	_|	一 ξ	l=τ-di=0
+W⅛ XX
k=1 i=0
K τ-1
E k5F (Xτk+d+i)kF + 6η2d XX
E k5F (Xτ k+d+i)k2F
And in the same way, we have
K τ -1-d
Σ Σ EK(k) kXτ k+d J — Xτ k+d+l k2F
k=1 l=0
≤ 6η2(τ - d)ι-ξ2
d-1
2Mdσ2 + 2dX k5F(Xi)k2F
i=1
6η2(τ — d)τσ2ξ2 MK
1 — ξ2
τ-1	l
+ 6η2MK X Xσ2
l=τ -d i=0
6η2(τ — d)
1 — ξ2
K τ-1	K τ -1-d
XX
E k5F (Xτ k+d+i)kF + 6η2 (τ — d)XX
E k5F (Xτk+d+i)k2F
k=1 i=0	k=1 i=0
Recall the intermediate result equation 11 in the proof of Lemma 1:
EK(k)
1K
κ XU5F 8κ(k)”
k=1
≤ 2 [F(μ1 — Finf ] + η2Lσ2 Fd + T - d] + 2 6L2(1 + ξ) ∖X ^X 2 + 2 6dξ2L2τσ2(1 + ξ)
一 ηK (ξd + τ — d)	M (ξd + τ — d)	" ξd + T — d ʌZ ʌ/Cr n (ξd + T — d)(1 — ξ2)
l=τ -d i=0
η2 12dσ2L2 ξ2(τ — d)(1+ ξ)
+ K (ξd + T — d)(1 — ξ2)
η2 12L2dξ2(τ — d)(1 + ξ)
KM (ξd + T — d)(1 — ξ2)
d-1
X k5F (Xi )k2F
i=1
+
+
2
F
+
2Lηdξ2 — ξ + 6ξL2Tndx6L2η"dd + 6ξL2η2d 工 匚	2
+	KM(ξd + τ — d)	XXEK(k) k5F (XTk+d+i)kF
+
2Lη(τ — d) — ξ + 6ξL T d+-?2T (T-d) + 6ξL2η2d + 6L2η2(τ — d)
KM (ξd + T — d)
K T-1-d
X X EK(k) k5F (XTk+d+i)k2F
k=1 i=0
When the learning rate satisfies the following two formulas at the same time
2Lηdξ2 — ξ + 6ξL2η2d + 6L2η2(T-d) + 6ξL2η2d ≤ 0
1 — ξ 2
2Lη(τ — d) — ξ + 6ξL2η2d +6L2η2(T- d) + 6ξL2η2d + 6^^(- d) ≤ 0
1 — ξ 2
And
K T-1	K
XX
Eκ(k) k5F (Xτk+d+i)kF = Ek X ∣∣5F(μk)k2
k=1 i=0	k=1
24
Under review as a conference paper at ICLR 2022
Thus, we have
1K
EK(k) K Σ II5F(μK(k))l∣2
k=1
≤ 2 [F(μi) - Finf ]	+	η2Lσ2	Fd + T - d] +	2 6L2 (I +	ξ)	X-1	∙X∙	2 + 2	6dξ2L2τσ2(* 1 + ξ)
一 ηK(ξd + τ — d)	M(ξd + τ — d)	η ξd + τ — d ι d~^i η (ξd + T — d)(i — ξ2)
,η2 12dσ2L2ξ2(τ - d)(1 + ξ)
+ K (ξd + T — d)(1 — ξ2)
η2 i2L2dξ2(τ - d)(i + ξ)
KM (ξd + τ — d)(1 — ξ2)
d-1
X	k5F(Xi)k2F
i=1
+
Corollary 1. Under SssUmPtions, if the learning rate is η = MMV y M the average-squared gradient
norm after K iterations is bounded by
EK(k)
1K
K Xll * *5F(μK(k))U
k=1
≤ 2[F(μι) — Finf] +	1	2Lσ2 [ξ2d + T — d
一√MK(ξd + τ — d)	√MK	(ξd + T ― d)
+ M(1 + VY 6L2(1+ ξ) X Xσ2 + M(1 + VY 6dξ2L2Tσ2(1+ ξ)
+ K3 V + M ξd + T — d 乙乙	+ K3 ∖ MJ (ξd + T — d)(1 — ξ2)
l=τ d i=0
M2 (1	VY 12dσ2L2ξ2(T — d)(1 + ξ)
+ K3 I + M)	(ξd + T — d)(1 — ξ2)
M V4
+ K3 (1 + M)
12L2dξ2(T — d)(1+ ξ)
(ξd + T — d)(1 — ξ2)
d-1
X k5F(Xi)k2F
i=1
If the total iterations K is sufficiently large, then the average-squared gradient norm will be bounded
by
1
E 归并5F(μk)k2
2 [F(μi) — Finf]+ 2Lσ2 [ξ2d + T — d]
√MK (ξd + T — d)
25