Under review as a conference paper at ICLR 2022
Adaptive Learning of Tensor Network Struc-
TURES
Anonymous authors
Paper under double-blind review
Abstract
Tensor Networks (TN) offer a powerful framework to efficiently represent very
high-dimensional objects. TN have recently shown their potential for machine
learning applications and offer a unifying view of common tensor decomposition
models such as Tucker, tensor train (TT) and tensor ring (TR). However, identifying
the best tensor network structure from data for a given task is challenging. In this
work, we leverage the TN formalism to develop a generic and efficient adaptive
algorithm to jointly learn the structure and the parameters of a TN from data. Our
method is based on a simple greedy approach starting from a rank one tensor
and successively identifying the most promising tensor network edges for small
rank increments. Our algorithm can adaptively identify TN structures with small
number of parameters that effectively optimize any differentiable objective function.
Experiments on tensor decomposition, tensor completion and model compression
tasks demonstrate the effectiveness of the proposed algorithm. In particular, our
method outperforms the state-of-the-art evolutionary topology search introduced
in [19] for tensor decomposition of images (while being orders of magnitude faster)
and finds efficient structures to compress neural networks outperforming popular
TT based approaches [24].
1	Introduction
Matrix factorization is ubiquitous in machine learning and data science and forms the backbone of
many algorithms. Tensor decomposition techniques emerged as a powerful generalization of matrix
factorization. They are particularly suited to handle high-dimensional multi-modal data and have been
successfully applied in neuroimaging [44], signal processing [3, 33], spatio-temporal analysis [1, 31]
and computer vision [20]. Common tensor learning tasks include tensor decomposition (finding a
low-rank approximation of a given tensor), tensor regression (which extends linear regression to the
multi-linear setting), and tensor completion (inferring a tensor from a subset of observed entries).
Akin to matrix factorization, tensor methods rely on factorizing a high-order tensor into small factors.
However, in contrast with matrices, there are many different ways of decomposing a tensor, each
one giving rise to a different notion of rank, including CP, Tucker, Tensor Train (TT) and Tensor
Ring (TR). For most tensor learning problems, there is no clear way of choosing which decomposition
model to use, and the cost of model mis-specification can be high. It may even be the case that none
of the commonly used models is suited for the task, and new decomposition models would achieve
better tradeoffs between minimizing the number of parameters and minimizing a given loss function.
We propose an adaptive tensor learning algorithm which is agnostic to decomposition models. Our
approach relies on the tensor network formalism, which has shown great success in the many-body
physics community [28, 7, 6] and has recently demonstrated its potential in machine learning for
compressing models [24, 39, 8, 23, 14, 41], developing new insights into the expressiveness of deep
neural networks [4, 15], and designing novel approaches to supervised [35, 9] and unsupervised [34,
11, 22] learning. Tensor networks offer a unifying view of tensor decomposition models, allowing
one to reason about tensor factorization in a general manner, without focusing on a particular model.
In this work, we design a greedy algorithm to efficiently search the space of tensor network structures
for common tensor problems, including decomposition, completion and model compression. We
start by considering the novel tensor optimization problem of minimizing a loss over arbitrary tensor
network structures under a constraint on the number of parameters. To the best of our knowledge,
1
Under review as a conference paper at ICLR 2022
this is the first time that this problem is considered. The resulting problem is a bi-level optimization
problem where the upper level is a discrete optimization over tensor network structures, and the lower
level is a continuous optimization of a given loss function. We propose a greedy approach to optimize
the upper-level problem, which is combined with continuous optimization techniques to optimize
the lower-level problem. Starting from a rank one initialization, the greedy algorithm successively
identifies the most promising edge of a tensor network for a rank increment, making it possible to
adaptively identify from data the tensor network structure which is best suited for the task at hand.
The greedy algorithm we propose is conceptually simple, and experiments on tensor decomposition,
completion and model compression tasks showcase its effectiveness. Our algorithm significantly
outperforms a recent evolutionary algorithm [19] for tensor network decomposition on an image com-
pression task by discovering structures that require less parameters while simultaneously achieving
lower recovery errors. The greedy algorithm also outperforms CP, Tucker, TT and TR algorithms on
an image completion task and finds more efficient TN structures to compress fully connected layers
in neural networks than the TT based method introduced in [24].
Related work Adaptive tensor learning algorithms have been previously proposed, but they only
consider determining the rank(s) of a specific decomposition and are often tailored to a specific
tensor learning task (e.g., decomposition or regression). In [1], a greedy algorithm is proposed
to adaptively find the ranks of a Tucker decomposition for a spatio-temporal forecasting task, and
in [38] an adaptive Tucker based algorithm is proposed for background subtraction. In [42], the
authors present a Bayesian approach for automatically determining the rank of a CP decomposition.
In [2] an adaptive algorithm for tensor decomposition in the hierarchical Tucker format is proposed.
In [10] a stable rank-adaptive alternating least square algorithm is introduced for completion in the
TT format. The problem we consider is considerably more general since we do not assume a fixed
tensor network structure (e.g. Tucker, TT, CP, etc.). Exploring other decomposition relying on the
tensor network formalism has been sporadically explored. The work which is the most closely related
to our contribution is [19] where evolutionary algorithms are used to approximate the best tensor
network structure to exactly decompose a given target tensor. However, the method proposed in [19]
only searches for TN structures with uniform ranks (with the rank being a hyperparameter) and is
limited to the problem of tensor decomposition. In contrast, our method is the first to jointly explore
the space of structures and (non-uniform) ranks to minimize an arbitrary loss function over the space
of tensor parameters. Lastly, [12] proposes to explore the space of tensor network structures for
compressing neural networks, a rounding algorithm for general tensor networks is proposed in [21]
and the notions of rank induced by arbitrary tensor networks are studied in [40].
2	Preliminaries
In this section, we present notions of tensor algebra and tensor networks. We first introduce notations.
For any integer k, [k] denotes the set of integers from 1 to k. We use lower case bold letters for
vectors (e.g. v ∈ Rd1), upper case bold letters for matrices (e.g. M ∈ Rd1×d2) and bold calligraphic
letters for higher order tensors (e.g. T ∈ Rd1 ×d2 ×d3). The ith row (resp. column) of a matrix M will
be denoted by Mi,: (resp. M:,i). This notation is extended to slices of a tensor in the obvious way.
Tensors and tensor networks We first recall basic definitions of tensor algebra; more details
can be found in [17]. A tensor T ∈ Rd1× ×dp Can simply be Seen as a multidimensional array
(Tiι, ∙,ip : in ∈ [dn],n ∈ [p]). The inner product of two tensors is defined byhS, T)=
Pii …ip Siι ipTh ip and the Frobenius norm of a tensor is defined by ∣∣T∣∣F =(T, T). The
mode-n matrix product of a tensor T and a matrix X ∈ Rm×dn is a tensor denoted by T ×n X. It is
of size di X …X dn-ι X m X dn+ι X …X dp and is obtained by contracting the nth mode of T with
the second mode of X, e.g. for a 3rd order tensor T, we have (T×2 X)i1i2i3 = Pj Ti1ji3 Xi2j. The
nth mode matricization ofTis denoted by T(n) ∈ Rdn× i6=n di. Tensor network diagrams allow one
to represent complex operations on tensors in a graphical and intuitive way. A tensor network (TN) is
simply a graph where nodes represent tensors, and edges represent contractions between tensor modes,
i.e. a summation over an index shared by two tensors. In a tensor network, the arity of a vertex (i.e.
the number of legs of a node) corresponds to the order of the tensor (see Figure 1). Connecting
two legs in a tensor network represents a contraction over the corresponding indices. Consider
2
Under review as a conference paper at ICLR 2022
(Vd
m M n
Figure 1:	Tensor network representation of a vector v ∈ Rd, a matrix M ∈ Rm×n and a tensor
T ∈ Rd1 ×d2 ×d3 .
-AHB)- = AB
AS= = Tr(A)
T×3B
TξT = kτ kF
Figure 2:	Tensor network representation of common operation on matrices and tensors.
m — n -
the following simple tensor network With two nodes:  (A) (X. The first node represents a
matrix A ∈ Rm×n and the second one a vector X ∈ Rn. Since this tensor network has one dangling
leg (i.e. an edge which is not connected to any other node), it represents a first order tensor, i.e., a
vector. The edge between the second leg of A and the leg of X corresponds to a contraction between
the second mode of A and the first mode of X. Hence, the resulting tensor network represents the
classical matrix-vector product, which can be seen by calculating the ith component of this tensor
network: i -(A-(X) = Ej AijXj = (Ax)i . Other examples of tensor network representations
of common operations on matrices and tensors can be found in Figure 2. Lastly, it is worth mentioning
that disconnected tensor networks correspond to tensor products, e.g., -(U (v)- = uv> is the
outer product of U and V with components i-(U j = UiVj . Consequently, an edge of
dimension (or rank) 1 in a TN is equivalent to having no edge between the two nodes, e.g., if R =1
we have i -Ar(b-j = PR=I Ai,r Br,j = Ai,1 BIj = i-(A (B)-j.
Tensor decomposition and tensor rank We now briefly present the most common tensor decom-
position models, omitting the CP decomposition which cannot be described using the TN formalism
unless hyper-edges are allowed (which we do not consider in this work). For the sake of simplicity
we consider a fourth order tensor T ∈ Rd1 ×d2×d3×d4, each decomposition can be straightforwardly
extended to higher-order tensors. A Tucker decomposition [36] decomposes T as the product
of a core tensor G ∈ RR1 × R2 × R3 ×R4 with four factor matrices Ui ∈ Rdi×Ri for i = 1,…，4:
T = G ×1 U1 ×2 U2 ×3 U3 ×4 U4. The Tucker rank, or multilinear rank, ofT is the smallest
tuple (R1 ,R2 ,R3 ,R4 ) for which such a decomposition exists. The tensor ring (TR) decomposi-
tion [43, 25, 29] expresses each component ofT as the trace of a product of slices of four core
tensors G(1) ∈ RR0×d1×R1, G(2) ∈ RR1×d2×R2, G(3) ∈ RR2×d3×R3 and G(4) ∈ RR4×d4×R0:
Ti1,i2,i3,i4 = Tr(G:(,1i)1,:G:(,2i)2,:G:(,3i)3,:G:(,4i)4,:). The tensor train (TT) decomposition [26] (also known as
matrix product states in the physics community) is a particular case of the tensor ring decomposition
where R0 must be equal to 1 (R0 is thus omitted when referring to the rank of a TT decomposition).
Similarly to Tucker, the TT and TR decompositions naturally give rise to an associated notion of
rank: the TR rank (resp. TT rank) is the smallest tuple (R0,R1,R2,R3) (resp. (R1,R2,R3)) such
that a TR (resp. TT) decomposition exists.
Tensor networks offer a unifying view of tensor decomposition models: Figure 3 shows the TN
representation of common models. Each decomposition is naturally associated with the graph
topology of the underlying TN. For example, the Tucker decomposition corresponds to star graphs,
the TT decomposition corresponds to chain graphs, and the TR decomposition model corresponds
to cyclic graphs. The relation between the rank of a decomposition and its number of parameters is
different for each model. Letting p be the order of the tensor, d its largest dimension and R the rank
of the decomposition (assuming uniform ranks), the number of parameters is in O (Rp + pdR) for
Tucker, and O pdR2 for TT and TR. One can see that the Tucker decomposition is not well suited
for tensors of very high order since the size of the core tensor grows exponentially with p.
3
Under review as a conference paper at ICLR 2022
(WxJXl) ⅛ΛΛ⅛
Tucker	Tensor Train Tensor Ring Hierarchical Tucker
Figure 3:	Tensor network representation of common decomposition models for a 4th order tensor.
3	AGreedy Algorithm for Tensor Network Structure Learning
3.1	Tensor Network Optimization
We consider the problem of minimizing a loss function L : Rd1 × …× dp → 股+ w.r.t. a tensor W
efficiently parameterized as a tensor network (TN). We first introduce our notations for TN.
Without loss of generality, we consider TN having one factor per dimension of the parameter tensor
W ∈ Rd1× ×dp, where each of the factors has one dangling leg corresponding to one of the
dimensions di (we will discuss how this encompasses TN structures with internal nodes such as
Tucker at the end of this section). In this case, a TN structure is summarized by a collection of
ranks (Ri,j)1≤i<j≤p where each Ri,j ≥ 1 is the dimension of the edge connecting the ith and
jth nodes of the TN (for convenience, we assume Ri,j = Rj,i if i>j). If there is no edge
between nodes i and j in a TN, Ri,j is thus equal to 1 (see Section 2). A TN decomposition of
W ∈ Rd1× ×dp is then given by a collection of core tensors G(1),…,G(P) where each G(i) is of
size Rι,i X •一X Ri-ι,i × di × Ri,i+ι × •一X Ri,p. Each core tensor is of order P but some of its
dimensions may be equal to one (representing the absence of edge between the two cores in the TN
structure). We use TN(G(1), ∙∙∙ , G(P)) to denote the resulting tensor. Formally, for an order 4 tensor
we have
R1,2 R1,3	R3,4
TN(G ⑴,…，G(4))i1 i2i3i4 = XX …X G‰,j3,j4 G(j2),i2,j3,j4 G ^,3,4 G^4,4"
j12=1 j13=1	j34=1
This definition is straightforwardly extended to TN representing tensors of arbitrary orders.
As an illustration, for a TT decomposition the ranks of the tensor network representation would
be such that Ri,, 6=1if and only if j = i +1. The problem of finding a rank (r1, r2, r3) TT
decomposition of a target tensor T ∈ Rd1 ×d2×d3×d4 can thus be formalized as
min	L(TN(G(1), G(2), G(3), G(4)))	(1)
G(1) ∈Rd1 ×r1 ×1×1 ,G(2) ∈Rr1 ×d2 ×r2 ×1 ,
G(3) ∈R1×r2 ×d3 ×r3 ,G(4) ∈R1×1×r3 ×d4
where L(W) = kT - Wk2F. Other common tensor problems can be formalized in this manner. For
example, the tensor train completion problem would be formalized similarly with the loss function
being L(W) =者 £⑶,…(W iι,…,ip- T"∙∙∙,ip )2 Where ω ⊂ [di] X …X [dp]is the Setof
observed entries of T ∈ Rd1 × …× dp, and learning TT models for classification [35] and sequence
modeling [11] also falls within this general formulation by using the cross-entropy or log-likelihood
as a loss function.
We now explain how our formalism encompasses TN structure with internal nodes, such as the
Tucker format. Since a rank one edge in a TN is equivalent to having no edge, internal cores can
be represented as cores whose dangling leg have dimension 1. Consider for example the Tucker
decomposition T = G X1 U1 X2 U2 X3 U3 ∈ Rd1 ×d2×d3 of rank (r1,r2,r3). The tensor T
can naturally be seen as a fourth order tensor T ∈ R1×d1×d2×d3, G as G ∈ R1×r1×r2×r3, Ui
as U1 ∈ Rrι×dι×1×1, U2 as U2 ∈ Rr2×1×d2×1×1 and U3 as U3 ∈ Rr3×1×1×d3. With these
λ∙j
definitions, one can check that TN(G, Ui, U2, U3)=G Xi UiX2 U2 X3 U3 = T. More complex
TN structure with internal nodes such as hierarchical Tucker can be represented using our formalism
in a similar way. The assumption that each core tensor in a TN structure has one dangling leg
corresponding to each of the dimensions of the tensor T is thus without loss of generality, since it
suffices to augment T with singleton dimensions to represent TN structures with internal nodes.
4
Under review as a conference paper at ICLR 2022
3.2	Problem Statement
A large class of TN learning problems consist in optimizing a loss function w.r.t. the core tensors of
a fixed TN structure; this is, for example, the case of the TT completion problem: the rank of the
decomposition may be selected using, e.g., cross-validation, but the overall structure of the TN is
fixed a priori. In contrast, we propose to optimize the loss function simultaneously w.r.t. the core
tensors of the TN and the TN structure itself. This joint optimization problem can be formalized as
min min L(TN(G ⑴，…，G (P))) s.t. Size(G ⑴，∙∙∙，G(P)) ≤ C (2)
Ri,j, G⑴，…，G(p)
1≤i<j≤p
where L is a loss function, each core tensor G(i) is in R^R1，i×•••×Ri-1，i×di×Ri，i+1×•••×Ri，p, C is a
bound on the number of parameters, and Size(G ⑴,…，G(P)) is the number of parameters of the
TN, which is equal to PP= diRι,i ∙∙ ∙ Ri-1,iRi,i+1 …Ri,p. Note that if K is the maximum arity of
a node in a TN, its number of parameters is in O pdRK where d = maxi di and R = maxi,j Ri,j .
Problem 2 is a bi-level optimization problem where the upper level is a discrete optimization over
TN structures, and the lower level is a continuous optimization problem (assuming the loss function
is continuous). If it is possible to solve the lower level continuous optimization, an exact solution
can be found by enumerating the search space of the upper level, i.e. enumerating all TN structures
satisfying the constraint on the number of parameters, and selecting the one achieving the lower value
of the objective. This approach is, of course, not realistic since the search space is combinatorial in
nature, and its size will grow exponentially with p. Moreover, for most tensor learning problems,
the lower-level continuous optimization problem is NP-hard [13]. In the next section, we propose a
general greedy approach to tackle this problem.
3.3	Greedy Algorithm
We propose a greedy algorithm which consists in first optimizing the loss function L starting from a
rank one initialization of the tensor network, i.e. Ri,j is set to one for all i， j and each core tensor
G(i) ∈ RR1,i×…xRi—LixdixRi,讣1 ×…×Ri,p is initialized randomly. At each iteration of the greedy
algorithm, the most promising edge of the current TN structure is identified through some efficient
heuristic, the corresponding rank is increased, and the loss function is optimized w.r.t. the core
tensors of the new TN structure initialized through a weight transfer mechanism. In addition, at each
iteration, the greedy algorithm identifies nodes that can be split to create internal nodes in the TN
structure by analyzing the spectrum of matricizations of its core tensors.
The overall greedy algorithm, named Greedy-TN, is summarized in Algorithm 1. In the remaining
of this section, we describe the continuous optimization, weight transfer, best edge identification and
node splitting procedures. For Problem 2, a natural stopping criterion for the greedy algorithm is
when the maximum number of parameters is reached, but more sophisticated stopping criteria can
be used. For example, the algorithm can be stopped once a given loss threshold is reached, which
leads to an approximate solution to the problem of identifying the TN structure with the least number
of parameters achieving a given loss threshold. For learning tasks (e.g., TN classifiers or tensor
completion), the stopping criterion can be based on validation data (e.g., using early stopping).
Continuous Optimization Assuming that the loss function L is continuous and differentiable,
standard gradient-based optimization algorithms can be used to solve the inner optimization prob-
lem (line 12 of Algo. 1). E.g., in our experiments on compressing neural network layers (see
Section 4) we use Adam [16]. For particular losses, more efficient optimization methods can be
used: in our experiments on tensor completion and tensor decomposition, we use the Alternating
Least-Squares (ALS) [17, 5] algorithm which consists in alternatively solving the minimization
problem w.r.t. one of the core tensors while keeping the other ones fixed until convergence.
Weight Transfer A key idea of our approach is to restart the continuous optimization process from
the previous iteration of the greedy algorithm: we initialize the new slices of the two core tensors
connected by the incremented edge to values close to 0, while keeping all the other parameters of the
TN unchanged (line 8-10 of Algo. 1). E.g., for a tensor network of order 4, increasing the rank of
the edge (1， 2) by 1 is done by adding a slice of size d1 × R1,3 × R1,4 (resp. d2 × R2,3 × R2,4) to
the second mode of G(1) (resp. first mode of G(2)). After this operation, the new shape of G(1) will
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Greedy-TN: Greedy algorithm for tensor network structure learning.
Input: Loss function L : Rd1 × ×dp → R, splitting node threshold ε.
1:	// Initialize tensor network to a random rank one tensor and optimize loss function.
2:	Rij — 1 for 1 ≤ i <j ≤ P
3:	Initialize core tensors G(i) ∈ RR1,i× xRiiixdixRi,讣1 × ×Ri,p randomly
4:	(G(1),…，G(P)) 一 optimize L(TN(G⑴，…，G(P))) w.r.t. G⑴，…，G(P)
5:	repeat
6:	(i,j) - find-best-edge(L, (G(1),…,G(P)))
7:	// Weight transfer
8:	G(k)	一 G* (k) for k ∈ [p]∖{i,j}; Ri,j 一 Ri,j + 1
9:	G(i)	- add-slice(G(i),j) // add new slice to the jth mode of G(i)
10:	G(j)	- add-slice(G(j), i) // add new slice to the ith mode of G(j)
11:	// Optimize new tensor network structure
12:	(G⑴，…，G(P)) J optimize L(TN(G⑴，…，G(P))) from init. G⑴，…，G(P)
13:	// Add internal nodes if possible (number of cores p may be increased after this step)
14:	(G(I),…，G(P)) J Split-nodes((G⑴，…，G(P)), ε)
15:	until Stopping criterion
be d1 × (R1,2 + 1) × R1,3 × R1,4 and the one of G(2) will be (R1,2 + 1) × d2 × R2,3 × R2,4. The
following proposition shows that if these slices were initialized exactly to 0, the resulting TN would
represent exactly the same tensor as the original one. In practice, we initialize the slices randomly
with small values to break symmetries that could constrain the continuous optimization process.
Proposition 1. Let G(k) ∈ RR1，k ×…×Rkτ,k×dk×Rk,k+1×…×Rk,pfor k ∈ [p] bethe CoretenSorSofa
tensor network and let 1 ≤ i<j≤ p. Let Ri0j0 = Ri0,j0 +1if (i0,j0)=(i,j) and Ri0,j0 otherwise,
and define the core tensors G(k) ∈ RR1,k×…xRk-1，kxdkxRk，k+1 ×…×Rk,p for k ∈ [p] by
(G(i))(j) =	](G-；-j)，(G(j))(i)	=	(G-/(i)	and G(k) =	G(k) for k ∈	[p] \	{i,j}
where 0 denotes a row vector of zeros of the appropriate size in each block matrix.
(k)	(k)
Then， the core tensors G correspond to the same tensor network as the core tensors G(k)， i.e.，
TN(G⑴,…,G(P)) = TN(G⑴,…,G(P)).
The proof of the proposition can be found in Appendix A. The weight transfer mechanism leads to a
more efficient and robust continuous optimization by transferring the knowledge from each greedy
iteration to the next and avoiding re-optimizing the loss function from a random initialization at each
iteration. An ablation study showing the benefits of weight transfer is provided in Appendix C.3.
Best Edge Selection As mentioned previously, we propose to optimize the inner minimization
problem in Eq. 2 using iterative algorithms, namely gradient based algorithms or ALS depending on
the loss function L. In order to identify the most promising edge to increase the rank by 1 (line 6
of Algo. 1), a reasonable heuristic consists in optimizing the loss for a few epochs/iterations for
each possible edge and select the edge which led to the steepest decrease in the loss. One drawback
of this approach is its computational complexity: e.g., for ALS, each iteration requires solving p
least-squares problem with di Qk6=i Ri,k unknowns for i ∈ [p]. We propose to reduce the complexity
of the exploratory optimization in the best edge identification heuristic by only optimizing L w.r.t.
the new slices of the core tensors. Thus, at each iteration of the greedy algorithm, for each possible
edge to increase, we transfer the weights from the previous greedy iteration, optimize only w.r.t. the
new slices for a small number of iteration, and choose the edge which led to the steepest decrease of
the loss. For ALS, this reduces the complexity of each iteration to the one of solving 2 least-squares
problems with d 口破[回\口,n Ri,k and dj 口破[回\口,力 Ri,k unknowns, respectively, where (i,j) is
the edge being considered in the search. When using gradient-based optimization algorithms, the
same approach is used where the gradient is only computed for (and back-propagated through) the
6
Under review as a conference paper at ICLR 2022
new slices. It is worth mentioning that the greedy algorithm can seamlessly incorporate structural
constraints by restricting the set of edges considered when identifying the best edge for a rank
increment. For example, it can be used to adaptively select the ranks of a TT or TR decomposition.
Internal Nodes Lastly, we design a simple approach for the greedy algorithm to add internal
nodes to the TN structure relying on a common technique used in TN methods to split a node into
two new nodes using truncated SVD (see, e.g., Fig. 7.b in [35]). To illustrate this technique, let
M ∈ Rm1 ×m2×n1 ×n2 be the core tensor associated with a node in a TN we want to split into two
new nodes A ∈ Rm1 ×m2×r and B ∈ Rn1 ×n2×r: the first two legs of A (resp. B) will be connected
to the core tensors that were connected to M by its first two legs (resp. last two legs), and the third
leg of A and B will be connected together. This is achieved by taking the rank r truncated SVD of
(M)(1,2) ' UDV> ∈ Rm1m2×n1n2 (the matricization of M having modes 1 and 2 as rows and
modes 3 and 4 as columns), and letting A(3) = U> ∈ Rr×m1m2 and B(3) = DV> ∈ Rr×n1n2. If
the truncated SVD is exact, the resulting TN will represent exactly the same tensor as the one before
splitting the core M. This node splitting procedure is illustrated in the following TN diagram.
mi	ni	mi	r^r ni	mi	r ni
=M= ` =IU @ <V= = =IA <B=
m2	n2	m2	n2	m2	n2
In order to allow the greedy algorithm to learn TN structures with internal nodes, at the end of each
greedy iteration, we perform an SVD of each matricization of G(k) for k ∈ [p] (line 14 of Algo. 1).
For each matricization, we split the corresponding node only if there are enough singular values
below a given threshold ε in order for the new TN structure to have less parameters than the initial
one. While this approach may seem computationally heavy, the cost of these SVDs is negligible w.r.t.
the continuous optimization step which dominates the overall complexity of the greedy algorithm.
The details of all subroutines of Algorithm 1 as well as its complexity are given in the Appendix B.
4	Experiments
In this section we evaluate Greedy-TN on decomposition, completion, and model compression.
Tensor decomposition We first consider a tensor decomposition task. We randomly generate four
target tensors of size 7 × 7 × 7 × 7 × 7 with the following TN structures:
TT target tensor
S2S3S3Sk!
TR target tensor
Tucker target tensor
Triangle” target tensor
5
We run Greedy-TN until it recovers an almost exact decomposition (stopping when the relative
error falls below 10-6). We compare Greedy-TN with CP, Tucker and TT decomposition (using
the implementations from the TensorLy python package [18]) of increasing rank as baselines (we
use uniform ranks for Tucker and TT). We also include a simple random walk baseline based
on Greedy-TN, where the edge for the rank increment is chosen at random at each iteration.
Reconstruction errors averaged over 100 runs of this experiment are reported in Figure 4, where
we see that the greedy algorithm outperforms all baselines for the the four target tensors. Notably,
Greedy-TN outperforms TT/Tucker even on the TT/Tucker targets. This is because the rank of
the TT and Tucker targets are not uniform and Greedy-TN is able to adaptively set different ranks
to achieve the best compression ratio. Furthermore, Greedy-TN is able to recover the exact TN
structure of the triangle target tensor on almost every run. Lastly, we observe that the internal node
search of Greedy-TN is only beneficial on the Tucker target tensor, which is expected due to
the absence of internal nodes in the other target TN structures. As an illustration of the running
time, for the TR target, one iteration of Greedy-TN takes approximately 0.91 second on average
without the internal node search and 1.18 seconds with the search. The most common TN structures
recovered by Greedy-TN are shown in Appendix C.1. This experiment showcases the potential
cost of model mis-specification: both CP and Tucker struggle to efficiently approximate most target
tensors. Interestingly, even the random walk outperforms CP and Tucker on the TR target tensor.
Tensor completion We compare Greedy-TN with the TT and TR alternating least square algo-
rithms proposed in [37] and the CP and Tucker decomposition algorithms from Tensorly [18] on an
7
Under review as a conference paper at ICLR 2022
Figure 4: Evaluation of Greedy-TN on tensor decomposition. Curves represent the reconstruction
error averaged over 100 runs, shaded areas correspond to standard deviations and the vertical line
represents the number of parameters of the target TN. Greedy corresponds to Greedy-TN without
the search for internal nodes (line 14 of Algo. 1) while Greedy-int. includes this search.
Einstein Image Completion
Original
image
Observed CP (rank=366) TUcker (rank=20) TT (rank=20) TR (rank=18)
pixels	12.42% [20130]	15.98% [25209]	20.70% [12249]	10.83% [17820]
Greedy (iter=5) Greedy (iter=8) Greedy (iter=23) Greedy (iter=31) Greedy (iter=42) Greedy (iter=45)
31.28% [215]	25.19% [478]	12.88% [4690]	10.60% [10096]	9.45% [21375]	9.47% [24301]
Figure 5: Image completion with 10% of the entries randomly observed. (left) Relative reconstruction
error. (right) Best recovered images for CP, Tucker, TT and TR, and 6 recovered images at different
iteration of greedy (image title: RSE% [number of parameters]).
image completion task. We consider an experiment presented in [37]: the completion of an RGB
image of Albert Einstein reshaped into a 6 × 10 × 10 × 6 × 10 × 10 × 3 tensor (see [37] for details)
where 10% of entries are randomly observed. The ranks of methods other than Greedy-TN are
successively increased by one until the number of parameters gets larger than 25,000 (we use uniform
ranks for TT, TR and Tucker*). The relative errors as a function of number of parameters are reported
in Figure 5 (left) where we see that Greedy-TN outperforms all methods. The best recovered
images for all methods are shown in Figure 5 (right) along with the original image and observed
pixels. The best recovery error (9.45%) is achieved by Greedy-TN at iteration 42 with 21,375
parameters. The second best recovery error (10.83%) is obtained by TR-ALS at rank 18 with 17,820
parameters. At iteration 31, Greedy-TN already recovers an image with an error of 10.60% with
10,096 parameters, which is better than the best result of TR-ALS both in terms of parameters and
relative error. The images recovered at each iteration of Greedy-TN are shown in Appendix C.2. In
this experiment, the total running time of Greedy-TN is comparable to the one of TR-ALS (on the
order of hours), which is larger than the one of the other three methods.
Image compression In this experiment, we compare Greedy-TN with the genetic algorithm for
TN decomposition recently introduced in [19], denoted by GA(rank=6) and GA(rank=7) where the
rank is a hyper-parameter controlling the trade-off between accuracy and compression ratio (the
results of TT and TR, which are worst than GA, are available in Table 3 in [19]). Following [19],
we select 10 images of size 256 × 256 from the LIVE dataset [32], tensorize each image to an
order-8 tensor of size 48 and run Greedy-TN to decompose each tensor using a squared error loss.
Greedy-TN is stopped when the lowest RSE reported in [19] is reached. In Table 1, we report
the log compression ratio and root square error averaged over 50 random seeds. For all images,
*For Tucker, the completion is performed on the original image rather than the tensor reshaping since the
number of parameters of Tucker grows exponentially, leading to very poor results on the tensorized image.
8
Under review as a conference paper at ICLR 2022
Image	Log compression ratio CR↑ and (RSEJ) ±std		
	Greedy	GA(rank= 6)	GA(rank=7)
-0^^	0.715(0.105)10.152(0.005)	0.901(0.137)	0.660(0.115)
~L-	2.313(θ.15θ)±0.i89(0.005)	l.352(0.i58)	I.i59(0.i55)
	2.139(0.167)±o.127(0.004)	l.452(0.i76)	1.268(0.171)
	3.009(0.185)±0.088(0.002)	I.649(0.i93)	1.476(0.189)
4^	0.874(0.11l)±0.129(0.005)	0.859(0.152)	0.621(0.121)
-ɪ-	3.668(0.080)±0.i03(0.00i)	1.726(0.087)	1.548(0.083)
6~6^^	2.205(0.097)±0.171(0.004)	I.332(0.li0)	1.141(0.104)
J-	2.132(0.115)±0.202(0.002)	l.573(0.i26)	1.406(0.120)
-8-	3.634(0.080)±0.142(0.001)	1.679(0.085)	1.505(0.081)
9	1.669(0.i74)±0.202(0.002Γ	1.164(0.194)-	0.966(0.185)-
Table 1: Log compression ratio and RSE for 10 different
images selected from the LIVE dataset.
0.990 -
0.985 -
> 0.980 -
⅛ 0.975-
m 0.970 -
0.965 -
0.960 -
12000	13000	14000	15000
parameters
Figure 6: Train and test accuracies on the
MNIST dataset for different model sizes.
our method results in a higher compression ratio compared to GA(rank=7). Moreover, for images
1 to 9 our method even outperforms GA(rank=6) by achieving both higher compression ratios and
significantly lower RSE. For image 0, setting the greedy stopping criterion to the RSE of GA(rank=6),
Greedy-TN also achieves a higher compression ratio than GA(rank=6): 1.085(0.128). Our method
is also orders of magnitude faster—few minutes compared to several hours for GA.
Compressing neural networks Following [24], we apply our algorithm to compress a neural
network with one hidden layer on the MNIST dataset. The hidden layer is of size 1024 × 1024 which
We represent as a fifth-order tensor of size 16 X …X 16. We use Greedy-TN to train the core
tensors representing the hidden layer weight matrix alongside the output layer end-to-end. We select
the best edge for the rank increment using the validation performance on a separate random split
of the train dataset With 5, 000 images. In Figure 6, We report the train and test accuracies of the
TT based method introduced in [24] as Well as a TR tensorized model for uniform ranks 1 to 8 and
Greedy-TN (it is Worth noting that We use our oWn implementation of the TT method With dropout
and achieve higher accuracies than the ones reported in [24]). For every model size, our method
reaches higher accuracy. The best test accuracy of Greedy-TN is 98.74% With 15,050 parameters,
While TT reaches its best accuracy of 98.46% With 14,602 parameters, and TR achieves its best
accuracy of 98.42% With 14,154 parameters. At iteration 10, Greedy-TN already achieves an
accuracy of 98.46% With only 12,266 parameters. The running time of each iteration of Greedy-TN
is comparable With training one tensorized neural netWork With TT or TR.
Implementation details We use PyTorch [27] and the NCON function [30] to implement
Greedy-TN. For the continuous optimization step, We use the Adam [16] optimizer With a learning
rate of 10-3 and a batch size of 256 for 50 epochs for compressing neural netWork, and We use
ALS for the other three experiments (ALS is stopped When convergence is reached). The number
of iterations/epochs for the best edge identification is set to 2 for tensor decomposition, 5 for image
compression and 10 for image completion and compressing neural netWorks. The singular values
threshold for the internal node search is set to ε = 10-5. In all experiments except the tensor
decomposition on the Tucker target, the internal node search did not lead to any improvement of the
results. All experiments Were performed on a single 32GB V100 GPU.
5	Conclusion
We introduced a greedy algorithm to jointly optimize an arbitrary loss function and efficiently search
the space of TN structures and ranks to adaptively find parameter efficient TN structures from data.
Our experimental results shoW that Greedy-TN outperforms common methods tailored for specific
decomposition models on model compression and tensor decomposition and completion tasks. Even
though Greedy-TN is orders of magnitude faster than the genetic algorithm introduced in [19], its
computational complexity can still be limiting in some scenarios. In addition, the greedy algorithm
may converge to locally optimal TN structures. Future Work includes exploring more efficient discrete
optimization techniques to solve the upper-level discrete optimization problem and scaling up the
method to discover TN structures suited for efficient compression of larger neural netWork models.
9
Under review as a conference paper at ICLR 2022
References
[1]	Mohammad Taha Bahadori, Qi Rose Yu, and Yan Liu. Fast multivariate spatio-temporal
analysis via low rank tensor learning. In Advances in Neural Information Processing Systems,
pp. 3491-3499, 2014.
[2]	Jonas Ballani and Lars Grasedyck. Tree adaptive approximation in the hierarchical tensor
format. SIAM journal on scientific computing, 36(4):A1415-A1431, 2014.
[3]	A. Cichocki, R. Zdunek, A.H. Phan, and S.I. Amari. Nonnegative Matrix and Tensor Factor-
izations. Applications to Exploratory Multi-way Data Analysis and Blind Source Separation.
Wiley, 2009.
[4]	Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A
tensor analysis. In Conference on Learning Theory, pp. 698-728, 2016.
[5]	Pierre Comon, Xavier Luciani, and Andr6 LF De Almeida. Tensor decompositions, alternating
least squares and other tales. Journal of Chemometrics: A Journal of the Chemometrics Society,
23(7-8):393-405, 2009.
[6]	David Elieser Deutsch. Quantum computational networks. Proc. R. Soc. Lond. A, 425(1868):
73-90, 1989.
[7]	Richard P Feynman. Quantum mechanical computers. Foundations of physics, 16(6):507-531,
1986.
[8]	Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate ten-
sorization: compressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214,
2016.
[9]	Ivan Glasser, Nicola Pancotti, and J Ignacio Cirac. Supervised learning with generalized tensor
networks. arXiv preprint arXiv:1806.05964, 2018.
[10]	Lars Grasedyck and Sebastian Kramer. Stable als approximation in the tt-format for rank-
adaptive tensor completion. Numerische Mathematik, 143(4):855-904, 2019.
[11]	Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, and Pan Zhang. Unsupervised generative
modeling using matrix product states. Physical Review X, 8(3):031012, 2018.
[12]	Kohei Hayashi, Taiki Yamaguchi, Yohei Sugawara, and Shin-ichi Maeda. Einconv: Exploring
unexplored tensor decompositions for convolutional neural networks. In Advances in Neural
Information Processing Systems, 2019.
[13]	Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the
Association for Computing Machinery, 60(6), 2013.
[14]	Pavel Izmailov, Alexander Novikov, and Dmitry Kropotov. Scalable gaussian processes with
billions of inducing inputs via tensor train decomposition. In International Conference on
Artificial Intelligence and Statistics, pp. 726-735, 2018.
[15]	Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent
neural networks. In International Conference on Learning Representations, 2018.
[16]	Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd
International Conference on Learning Representations, ICLR, 2015.
[17]	Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51
(3):455-500, 2009.
[18]	Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. Tensorly: Tensor
learning in python. The Journal of Machine Learning Research, 20(1):925-930, 2019.
[19]	Chao Li and Zhun Sun. Evolutionary topology search for tensor network decomposition. In
International Conference on Machine Learning, 2020.
10
Under review as a conference paper at ICLR 2022
[20]	H. Lu, K.N. Plataniotis, and A. Venetsanopoulos. Multilinear Subspace Learning: Dimension-
ality Reduction of Multidimensional Data. CRC Press, 2013.
[21]	Oscar Mickelin and Sertac Karaman. Tensor ring decomposition. arXiv preprint
arXiv:1807.02513, 2018.
[22]	Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for language modeling.
arXiv preprint arXiv:2003.01039, 2020.
[23]	Alexander Novikov, Anton Rodomanov, Anton Osokin, and Dmitry Vetrov. Putting MRFs on a
tensor train. In International Conference on Machine Learning, pp. 811-819, 2014.
[24]	Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P. Vetrov. Tensorizing
neural networks. In Advances in Neural Information Processing Systems, pp. 442-450, 2015.
[25]	Romdn Orus. A practical introduction to tensor networks: Matrix product states and projected
entangled pair states. Annals of Physics, 349:117-158, 2014.
[26]	Ivan V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):
2295-2317, 2011.
[27]	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems
32, pp. 8024-8035. Curran Associates, Inc., 2019.
[28]	Roger Penrose. Applications of negative dimensional tensors. Combinatorial mathematics and
its applications, 1:221-244, 1971.
[29]	David Perez-Garcia, Frank Verstraete, Michael M Wolf, and J Ignacio Cirac. Matrix product
state representations. Quantum Information and Computation, 7(5-6):401-430, 2007.
[30]	Robert NC Pfeifer, Glen Evenbly, Sukhwinder Singh, and Guifre Vidal. Ncon: A tensor network
contractor for matlab. arXiv preprint arXiv:1402.0939, 2014.
[31]	Guillaume Rabusseau and Hachem Kadri. Low-rank regression with tensor responses. In
Advances in Neural Information Processing Systems, pp. 1867-1875, 2016.
[32]	H.R. Sheikh, M.F. Sabir, and A.C. Bovik. A statistical evaluation of recent full reference image
quality assessment algorithms. IEEE Transactions on Image Processing, 15(11):3440-3451,
2006. doi: 10.1109/TIP.2006.881959.
[33]	Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalex-
akis, and Christos Faloutsos. Tensor decomposition for signal processing and machine learning.
IEEE Transactions on Signal Processing, 65(13):3551-3582, 2017.
[34]	E. Miles Stoudenmire. Learning relevant features of data with multi-scale tensor networks.
Quantum Science and Technology, 3(3):034003, 2018.
[35]	Edwin Stoudenmire and David J. Schwab. Supervised learning with tensor networks. In
Advances in Neural Information Processing Systems, pp. 4799-4807, 2016.
[36]	Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31
(3):279-311, 1966.
[37]	Wenqi Wang, Vaneet Aggarwal, and Shuchin Aeron. Efficient low rank tensor ring completion.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 5697-5705,
2017.
[38]	Senlin Xia, Huaijiang Sun, and Beijia Chen. A regularized tensor decomposition method
with adaptive rank adjustment for compressed-sensed-domain background subtraction. Signal
Processing: Image Communication, 62:149-163, 2018.
11
Under review as a conference paper at ICLR 2022
[39]	Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for
video classification. arXiv preprint arXiv:1707.01786, 2017.
[40]	Ke Ye and Lek-Heng Lim. Tensor network ranks. arXiv preprint arXiv:1801.02662, 2018.
[41]	Rose Yu, Guangyu Li, and Yan Liu. Tensor regression meets gaussian processes. In International
Conference on Artificial Intelligence and Statistics,pp. 482-490, 2018.
[42]	Qibin Zhao, Liqing Zhang, and Andrzej Cichocki. Bayesian cp factorization of incomplete
tensors with automatic rank determination. IEEE transactions on pattern analysis and machine
intelligence, 37(9):1751-1763, 2015.
[43]	Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring
decomposition. arXiv preprint arXiv:1606.05535, 2016.
[44]	H. Zhou, L. Li, and H. Zhu. Tensor regression with applications in neuroimaging data analysis.
Journal of the American Statistical Association, 108(502):540-552, 2013.
12