Under review as a conference paper at ICLR 2022
SubMix:	Practical Private Prediction for
Large-scale Language Models
Anonymous authors
Paper under double-blind review
Ab stract
Recent data-extraction attacks have exposed that language models can memorize some
training samples verbatim. This is a vulnerability that can compromise the privacy of the
model’s training data. In this work, we introduce SubMix: a practical protocol for private
next-token prediction designed to prevent privacy violations by language models that were
fine-tuned on a private corpus after pre-training on a public corpus. We show that SubMix
limits the leakage of information that is unique to any individual user in the private corpus
via a relaxation of group differentially private prediction. Importantly, SubMix admits
a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing
data-extraction attacks while maintaining the utility of the language model. SubMix is
the first protocol that maintains privacy even when publicly releasing tens of thousands of
next-token predictions made by large transformer-based models such as GPT-2.
1	Introduction
The advent of transformers (Vaswani et al., 2017) has fostered a dramatic advancement in the
capabilities of generative neural language models (LMs), enabling large-scale models such as
GPT (Radford et al., 2019; Brown et al., 2020) to generate realistic, human-like text. Unfortunately,
these impressive capabilities also come at a cost to privacy, as the amount of excess parameters
in the LM enables it to memorize certain training samples. Consequently, several recent works
have demonstrated practical training-data extraction attacks that reproduce entire sentences from the
training dataset verbatim by querying the LM as an API (Carlini et al., 2019; 2020). These attacks
expose the privacy risks of large-scale LMs, especially when their training data contains sensitive
information such as addresses and personal ID numbers.
Existing solutions to data extraction attacks focus on using differential privacy (DP; Dwork et al.
(2014)), which provably protects against privacy attacks (Yeom et al., 2018). Techniques such as
DP-SGD (Abadi et al., 2016) have been applied to train differentially private neural networks on both
vision and language tasks (McMahan et al., 2017; Papernot et al., 2018). However, the threat model
in DP-SGD implicitly assumes that the adversary has full access to the private model’s parameters
and gradients during training, which results in pessimistic information leakage bounds that are
unreasonable for most models. Indeed, existing work only performs DP-SGD training of small
feedforward networks (Kerrigan et al., 2020) and RNNs (McMahan et al., 2017; Ramaswamy et al.,
2020), often with an unsatisfactory privacy-utility trade-off. Training large machine-learning models
with DP-SGD remains an open challenge (Jayaraman & Evans, 2019; Tramer & Boneh, 2020).
Our study deviates from prior work by, instead, considering the problem of private prediction (Dwork
& Feldman, 2018) using non-private language models fine-tuned on a private corpus. We propose
SubMix, a novel private prediction mechanism for answering next-token queries. Focusing on
private prediction affords SubMix three notable advantages: (1) Private prediction does not require
modification of the training algorithm, which makes use of large-scale LMs feasible. (2) Private
prediction allows us to leverage the probabilistic nature of next-token sampling for highly efficient
privacy accounting. (3) Private prediction allows us to leverage public pre-trained LMs1 to obtain
private predictive distributions that do not require noise addition to privatize the model’s predictions.
SubMix utilizes an ensemble of LMs fine-tuned on disjoint parts of the private corpus and privatizes
predictions by mixing the next-token distribution with that of a public pre-trained LM. The mixing
weight is adaptively tuned based on the degree of consensus among models in the ensemble. If
1We do not provide privacy guarantees or text extraction protection for the public corpus on which the LMs
are pre-trained; our privacy guarantees only on apply to the private corpus on which the LM is fine-tuned.
1
Under review as a conference paper at ICLR 2022
all models predict the same next-token distribution, then it is impossible for the next token to leak
sensitive information about any unique individual so no mixing is required. By contrast, if models
in the ensemble have high disagreement, SubMix will mix predictions with those of the public
pre-trained model to minimize privacy leakage. This allows SubMix to perform accurate next-token
prediction for most queries while preserving the privacy of the private corpus.
For any sequence of next-token queries issued to SubMix, we measure the amount of privacy leakage
in the response using Renyi divergence (Renyi, 1961). Our privacy notion, which We refer to as
operational privacy, is a sufficient condition for preventing samples that are unique to any user
from being generated by the SubMix mechanism. Importantly, operational privacy allows us to
perform tight data-dependent privacy accounting to upper bound the privacy loss of SubMix when
answering a variable-length query sequence. Concretely, when answering up to 1, 024 next-token
queries, SUBMIX realizes nearly 75% of the perplexity improvement that non-private fine-tuning
would have achieved on GPT-2 models (Radford et al., 2019), with privacy leakage as small as = 2.
We also show that SubMix can effectively prevent existing data extraction attacks against GPT-2.
2	Problem Formulation
We begin by setting up the problem of private next-token prediction and reviewing existing literature
on differential privacy. We then define and discuss the notion of operational privacy for SubMix.
2.1	Preliminaries
Let Σ denote a fixed finite vocabulary set. We use lower-case letters to denote single tokens (such as
X ∈ Σ) and use bold font to denote contexts or strings of tokens (such as X ∈ Σ*). A (causal) language
model h is a mapping from context strings to a distribution over next tokens: h : Σ* → Δiςi , where
Δiςi is the ∣Σ∣-dimensional probability simplex. For a particular context X ∈ Σ*, let h(x) denote
the next-token distribution vector obtained from evaluating h on context X, and let h(z|X) ∈ [0, 1]
denote the probability mass on a token z ∈ Σ.
User-level Corpus Let D denote a dataset of unstructured text, which is a set of token sequences
x ∈ Σ*. We assume that D is generated by a set of n distinct users, each holding a subset Di of the
full dataset D, i.e., D = Un=ι Di with Di ∩ Dj = 0 for i = j. We refer to each Di as a user-level
corpus for user i. As a concrete example, in the context of social media posts, Di would contain all
of the non-public posts made by user i. We aim to provide privacy guarantees for a model that is
non-privately fine-tuned on the dataset D.
Next-token Prediction One popular use case for language models is to perform next-token pre-
diction, that is, return a token z when queried with a context X. Such a query-answering API is
useful for applications such as smart keyboard for auto-correction and text completion (Mirowski &
Vlachos, 2015; Hertel, 2019). Typical approaches for next-token prediction involve sampling z from
the next-token distribution vector h(X); see Holtzman et al. (2019). Large transformers trained on
unstructured internet text have achieved remarkable success for this task, producing natural-looking
sentences via sequentially generating next tokens from a given prompt (Brown et al., 2020).
Text Extraction Attacks Recent studies have shown that it is possible for next-token prediction
APIs to reveal sensitive private information contained in the training dataset. Carlini et al. (2019)
defined κ-eidetic memorization to formalize the notion that extraction of strings that are uncommon
in the corpus can lead to violations of user privacy.
Definition 2.1 (κ-eidetic memorization (Carlini et al., 2019)). A string s is κ-eidetic memorized by
an LM h if s is extractable2 from h and s appears in at most κ examples in the training data D.
Carlini et al. (2019) showed that if the training dataset contains token sequences of the form: “My
social security number is --” where represents a digit of a user’s
2Carlini et al. (2019) define text extraction informally. In the supplement, we formalize it within the
framework of statistical hypothesis testing and show that differential privacy is sufficient to prevent eidetic
memorization.
2
Under review as a conference paper at ICLR 2022
social security number (SSN), then it is subtantially more likely for the LM trained on D to generate
the exact SSN appearing in D compared to a random SSN. As a result, it is possible to design an
efficient extraction attack that reproduces such unique sequences in the training dataset. Carlini
et al. (2020) further extended this attack to large transformer-based LMs such as GPT2 (Radford
et al., 2019), extracting memorized personal information such as name and address contained in the
model’s training dataset. Motivated by these shortcomings, this paper studies notions of privacy that
can prevent such text-extraction attacks while preserving the model’s utility.
2.2	Differential Privacy
Differential privacy (Dwork et al., 2014) is a powerful mathematical framework for privacy-preserving
data analysis. The underlying principle in differential privacy and all its variants is the notion of
indistinguishability. Informally, a mechanism M is private if, given two adjacent datasets D and
D0, the mechanism’s outputs M(D) and M(D0) are approximately indistinguishable. Hence by
observing the output of M, it is difficult for an adversary to discern the difference between D and D0 .
The above informal definition of privacy can be made mathematically precise by specifying: (1) the
notion of adjacency between datasets D and D0, and (2) the notion of approximate indistinguishability.
Differentially Private Training Prior work on private LM training (McMahan et al., 2017; Ra-
maswamy et al., 2020) adopted the definition of user-level adjacency: D and D0 are adjacent if they
differ in a single user’s data. Approximate indistinguishability is defined in terms of divergences and
is applied to the trained model: The private training algorithm M(D) induces a distribution over
models, and indistinguishability requires that D(M(D)||M(D0)) < for some divergence D and
small constant e > 0. Popular choices include the max divergence (DWork et al., 2014) and the Renyi
divergence oforder a (Renyi, 1961):
D∞(P ||Q) =	sup	log P (X)Tog Q(χ),	Da(P ||Q) =	1 1 log Ex 〜Q [P (χ)∕Q(χ)]α .
x∈supp(Q)	α - 1
Specializing to the choice of Renyi divergence, we define user-level Renyi differential privacy (RDP;
Mironov (2017)) for private training as folloWs.
Definition 2.2 (User-level RDP for private training). For a > 1, let Da denote the order-a Renyi
divergence. A private training algorithm M is an (α, )-RDP mechanism if for any D and D0 that
differ in only one user’s data Di, we have Da(M(D)||M(D0)) ≤ .
In order to satisfy the criteria in Definition 2.2 for neural language models, the standard approach is
to use DP-SGD (Abadi et al., 2016) to inject noise into the gradients computed at every iteration of
SGD training, and use composition theorems to bound the total privacy leakage across iterations.
Differentially Private Prediction Private prediction differs from private training in that the notion
of approximate indistinguishability applies to a sequence of predictions made by a private prediction
protocol P, rather than to a privately trained model. Formally, at each time step t, an adversary Adv
(potentially adaptively) issues a context string xt , and the private prediction protocol P responds
by generating a next token yt ∈ Σ. We let P T Adv denote the sequence of query-response pairs
between P and Adv up until time T: P T Adv = {xt, yt}tT=1. For a query sequence of length T,
approximate indistinguishability requires that for adjacent datasets D, D0 :
D P(D)	Adv ||P(D0)	Adv ≤ ,	(1)
for some divergence D and e > 0. We summarize the above discussion in the following Renyi-DP
variant of the definition for private prediction by Dwork & Feldman (2018).
Definition 2.3 (User-level RDP for private prediction). Let α > 1, > 0, and T ∈ Z+ . A prediction
protocol P is (α, , T)-RDP if for any adversary Adv and any D and D0 that differ in only one user’s
data Di , we have that Equation 1 holds.
It is well-known that differentially private models can be used for private prediction via the post-
processing theorem (Mironov, 2017): If h - M(D) isamodel obtained from an (α, e)-RDP training
mechanism M, then M0(x; D) = h(x) is an (α, , ∞)-RDP private prediction mechanism for any
sequence of queries (regardless of length). However, DP-SGD (Abadi et al., 2016)—the primary
3
Under review as a conference paper at ICLR 2022
mechanism for training private neural networks—makes an implicit assumption that the adversary
also observes additional information that is not accessible if h is used as a prediction API, and in
practice, it causes the accounting mechanism in DP-SGD to vastly overestimate the privacy leakage
parameter (Nasr et al., 2021). One alternative is the general-purpose subsample-and-aggregate
mechanism, which adds noise to an ensemble’s output in order to privatize it. This results in a trade-
off between the information leakage, , and the number of queries that can be answered, T (van der
Maaten & Hannun, 2020). For smaller T, the mechanism needs less noise to achieve a particular .
Conceptually, this is a step in the right direction, but the added noise greatly reduces utility and is
superfluous if we can leverage pre-trained public LMs to privatize the predictive distribution.
2.3 Operational Privacy for Private Prediction
To remedy the problems in user-level differentially private training and prediction, we propose a
different notion of privacy that is sufficient for preventing text extraction attacks, but admits more
specialized privacy mechanisms with tighter privacy accounting.
Let P(D) denote the power set of D. A partition Π ∈ P(D) ofD is a collection of sets π that satisfies
S∏∈∏ ∏ = D and that satisfies ∏ ∩ ∏0 = 0 for distinct ∏, ∏0 ∈ Π. We refer to the elements of Π as
parts. For some fixed ordering, we let Πi denote the i-th part. As a minor abuse of notation, we
let D \ π denote the usual element-wise subtraction and write Π \ π instead of Π \ {π} for brevity.
Recall the notion of the private prediction protocol P. We augment the protocol P with the capability
to terminate the query-response sequence at any time. With a slight abuse of notation, we denote by
T(P) the sequence length produced by P. We define Renyi operational privacy (ROP) as follows.
Definition 2.4 (Renyi operational privacy for private prediction). Let D be a dataset of user-level
corpora and let Π be a partition so that each user-level corpus Di is contained in some part Πj ∈ Π.
For α > 1 and > 0, a prediction protocol P is (α, )-ROP for partition Π of dataset D if for any
part Πi ∈ Π and adversary Adv, we have:
Dsym P (∏) = Adv ||P (∏ \ Πi) = Adv ≤ e.
α	T(P)	T(P)
Where Dsym(P||Q) = max{D0(P||Q), Dα(Q∖∖P)}. ROP differs from user-level RDP in Definition
2.3 in two aspects:
1.	We substitute user-level adjacency with partition-level adjacency. By definition, the partition
is constructed so that any user’s data belongs to a single part. Readers familiar with group
privacy (Dwork et al., 2014) may recognize partition-level adjacency as a formal relaxation
of the group-level adjacency used in Renyi group differential privacy. Partition-level RDP is
neither strictly weaker nor stronger than user-level RDP, and, at a cost, conversion is possible
(Appendix C).
2.	We allow variable-length query-response sequences by enabling the private-prediction protocol P
to terminate3 at will. ROP accounts for the privacy leakage in the responses made throughout the
prediction protocol’s operation lifetime, which is why we refer to it as operational. By allowing
for a variable-length sequence, we provide the mechanism with additional flexibility without
increasing susceptibility to text extraction. Non-sensitive queries often can be answered without
much privacy leakage, whereas sensitive queries may quickly exhaust the privacy budget, causing
the protocol to terminate early. The protocol’s decision to terminate at time T(P) may leak some
information about how sensitive the queried contexts are. However, this leakage is relatively
insignificant and can be upper bounded (allowing us to convert variable-length into fixed-length;
see Appendix C).
3 SubMix
We introduce SubMix, a private next-token prediction protocol that satisfies the operational pri-
vacy definition introduced above ( Figure 1). SubMix follows the design of the subsample-
and-aggregate mechanism by first forming a random partition Π of the training dataset, with
each user’s data belonging to a single random part Πi . For each part Πi, the protocol further
SPIitS ∏i into two Subparts ∏ and ∏i by randomly assigning users in ∏i to the two halves.
3After termination, the mechanism can technically continue to issue responses, but only in a way that is
entirely independent of the private corpus, e.g., by using a public pre-trained LM.
4
Under review as a conference paper at ICLR 2022
(a) Training. The corpus is a dataset comprised
of private user text. Each document represents
all of the text corresponding to a particular user.
At training time, SubMix learns an ensemble by:
(1) subsampling the dataset into non-overlapping
parts, (2) halving each part into two subparts, and
(3) fine-tuning the LM on each subpart using L.
(b) Prediction. The bottom-most network in the fig-
ure represents the pre-trained public model; the other
networks form the ensemble of model pairs obtained
after SubMix training. SubMix prediction produces
a mixing weight for each model. These weights are
aggregated and used to mix the ensemble predictions
with the predictions of the public model.
Figure 1: Overview of SUBMIX’s training protocol (left) and prediction protocol (right).
Fine-tuning a Pre-trained Model
Language models are often first trained
on vast internet crawls to develop
a general understanding of human
language, and then fine-tuned on a more
domain-specific dataset for the target
task (Dai & Le, 2015; Howard & Ruder,
2018). We treat language model training
as a black-box operation, and denote the
Algorithm 1 SUBMIX Training.
Inputs: User-level private corpus D, LM fine-tuning routine L
Outputs: Fine-tuned LMs hπi , hπ0 for i = 1, . . . , k
Hyperparameters: # of parts k
1:	Π — Random k-fold partition of D with ∣∏i| = |D|/k
2:	for i ∈ {1, ..., k} do
3:	(∏i, π0) — Randomly split part ∏i into two subparts.
4：	h∏i — L(∏i), h∏0 — L(∏0)
5: end for
training routine L(∙) as a function that returns a model hD = L(D). We assume access to a LM
pre-trained on public data, and use routine L to fine-tune the LM on the private user-level corpora.
Specifically, Algorithm 1 fine-tunes a public pre-trained LM on each subpart πi and πi0 to produce
LMs hπi = L(πi) and hπ0 = L(πi0). By convention, fine-tuning on the empty set returns the public
pre-trained model: h0 = L(0).
Next-token Distribution Given a query context xt , each part Πi is responsible for producing
a next-token probability mass function (pmf) by combining hπi (Xt) and hπ0(xt) into hi (Xt) =
(h∏i (Xt)+h∏0 (Xt))/2. SUBMIX mixes this pmf with the public pre-trained model h to add noise to the
prediction that hides private information. It does so by computing:
hi(xt) = λ hi(xt) + (1 - λ )h0(xt),
for a suitable choice of the mixing weight λ*. A value of λ* = 0 means the fine-tuned LMs hπi and
hπ0 are not used (no privacy loss), and λ* = 1 means no noise was added (no utility loss). We select
λ* based on how much information about the part Πi is contained in the pmfs hπi (xt) and hπo (xt).
Intuitively, since both πi and πi0 are random samples from the same data distribution, if the models
hπi and hπ0 did not memorize the query context xt then hπi (xt) and hπ0 (xt) will be similar. Hence,
the selected value of λ* should be close to 1. If either hπi or hπo memorized the context xt, then
hπi (xt) and hπ0 (xt) are dissimilar as πi and πi0 have no users in common. This suggests that mixing
with the pre-trained LM h is necessary for hiding the sensitive information in Πi, so λ* should be
close to 0. SUBMIX balances between these two extremes by computing a separate λi for each part
Πi . Specifically, it sets a target privacy leakage β > 0 and optimizes:
λi J
max {λ : Di (xt, λ) ≤ β},
λ∈[0,1]
(2)
where Di(Xt, λ) = Da (λh∏i(Xt) + (1 - λ)h0(Xt) || λh∏0(Xt) + (1 - λ)h0(Xt)). The final value
of λ* is obtained by averaging the λi values for i = 1,..., k, where k is the number of parts.
5
Under review as a conference paper at ICLR 2022
Algorithm 2 SUBMIX Prediction.
Inputs: Fine-tuned LMs hj∏i, h∏o for i = 1,..., k, privacy parameters e, time step t, query context Xt ∈ Σ*
Outputs: Next token response yt ∈ Σ
Hyperparameters: Renyi divergence order α, target leakage β
1:	if t = 1 then
2:	εi — e for i = 1,..., k
3:	else if STOP has been issued then
4:	return yt 〜h®(Xt)
5:	end if
6:	for i = 1, . . . , k do
7:	hi(xt) — 1 (h∏i(xt) + h∏0 (Xt))
8:	Compute λi using Equation 2.
9:	end for
10:	λ* - 1 Pk=I λi
11:	h(xt) - 1 Pk=I hi(xt)
12:	h(xt) — λ*h(xt) + (1 - λ*)h0(xt)
13:	for i = 1, . . . , k do * i * * 4 * * *
14:	λ-i — k-1 Pj = i λj
15:	h-i — k-1 Pj=i hj (Xt)
16:	h — λ*h(xt) + (1 - λ*)h0(xt)
17:	h0 - λ-ih-i(Xt) + (1 - λ-i)h0(xt))
18:	εi — εi - max{Dα(h∣∣h0),Dα(h0∣∣h)}
19:	end for
20:	if ∀i: εi > 0 then
21:	yt 〜h(xt)
22:	else
23:	Issue STOP signal.
24:	yt 〜h0(xt)
25:	end if
26:	return yt
Prediction and Privacy Accounting Given the next-token pmfs hi(xt) for i = 1, . . . , k, SUBMIX
computes the ensemble pmf, h(xt) = 1/k Pik=1 hi(xt), and samples from it to obtain a next-token
prediction. Our mechanism for selecting the mixing weight λ* can be shown to limit the privacy loss
of a sample from h(xt) under the operational privacy notion: Since each λi is determined entirely
by the part Πi, the next-token pmf after removal of Πi can be derived in closed form. This allows
us to compute the Renyi divergence in h(xt) for adjacent datasets Π \ ∏i. We present the SUBMIX
prediction protocol in Algorithm 2, and give its formal privacy analysis in the following proposition.
Proposition 3.1. SUBMIX is an (α, )-ROP prediction mechanism.
Proof. We use the adaptive sequential composition theorem for RDP (Mironov, 2017, Prop. 1). By
construction, Algorithm 2 ensures that maxi Pt Da (yt 〜P(∏) || yt 〜P(Π \ ∏i)) ≤ E for privacy
budget and divergence order α. Even if the query contexts {xt } are chosen adaptively by the
adversary, Proposition 1 in Mironov (2017) allows us to sum the Renyi divergences from each release:
T(P)
maxDa (P(∏) = Adv || P(Π \ ∏i) = Adv) ≤ max X Da (yt ~ P(∏) || Iyt ~ P(∏ \ ∏i))
ii
t=1
≤ max	εi (t) - εi(t - 1) ≤ max E = E.
it	i
To conclude the analysis, note that after SUBMIX issues the STOP signal, any subsequent queries are
answered by h@. Since h@ is not a function of Π, this does not leak any additional information. 口
4 Experiments
Datasets We evaluate SUBMIX by fine-tuning the pre-trained GPT-2 model from Hugging-
Face (Wolf et al., 2019) on two “private” datasets: (1) Wikitext-103 (Merity et al., 2016),
a collection of 103 million tokens scraped from Wikipedia; and (2) BigPatent-G (Sharma et al.,
2019), a collection of over 200,000 patents. We split the wikitext-103 corpus into blocks of
length 512 tokens and define each block as a (synthetic) user Di. We split BigPatent-G by patent
and define each user to be a single patent. This setup mimics settings in which users have distinct
data distributions within the text corpus.
Fine-Tuning & Evaluation We use standard hyperparameters for fine-tuning; see Appendix B for
details. To assess the quality ofLM predictions, we measure predictive perplexity:
PPh
Xl∙∙∙XL ~Dheldout
exp (- 1 X log h(x∕xι •…xi-i))]
6
Under review as a conference paper at ICLR 2022
where DheldOUt is the private held-out set and h(∙∣x1 ∙ ∙ ∙ xi-1) denotes the Pmf for the next token
given context x1 ∙ ∙ ∙ xi-1. In practice, We truncate the context window to a fixed maximum length
L = 512. Since each held-out sample consists of a block of L = 512 tokens, computing PPh for a
single sample requires L queries in total. Hence, the total number of queries B is a multiple of 512.
Following Geumlek et al. (2017), we report privacy loss in terms of a-R6nyi divergence. Note that
we can convert from (α, C)-RDP to (60, δ)-DP via F = e + %―Ib) (Mironov, 2017). In the paper,
we measure e using α = 2 Renyi divergence; we present results for other values of α in Appendix A.
4.1	Privacy-Utility Trade-off
Baseline Comparisons We first com-
pare SubMix to three privacy-preserving
mechanisms as baselines: (1) DP-
SGD (Abadi et al., 2016) for private
training; and (2) subsample-and-aggregate
(S&A, Dwork et al. (2014)) and (3) GN-
Max (Papernot et al., 2018) for private
prediction. See Appendix B for details
on adapting these mechanisms for private
next-token prediction. Figure 2 shows the
predictive perplexity of the private mecha-
nisms on the Wikitext-103 dataset for
ROP/RDP4 parameters α = 2 and e = 2.
The pre-trained GPT-2 model has a per-
plexity of 37.5 on Wikitext-103 and is
trivially private on that corpus. Fine-tuning
(Bus) (B=O)	(B = Io24)	(Bus) (B = I)	(B = I)
Figure 2: Perplexity for = 2, α = 2 of four privacy-
preserving mechanisms (SubMix, DP-SGD, S&A, and GN-
Max) using GPT-2 on Wikitext-103 with varying query
budgets B . Non-privately fine-tuning achieves a perplexity of
20.0 and the pre-trained public model achieves a perplexity
of 37.5 . Lower perplexity is better.
the LM non-privately achieves a perplexity of 20.0. SubMix achieves a perplexity of 26.9 at
B = 1, 024 queries, which is substantially below the perplexity of the pre-trained LM. By contrast,
the other mechanisms do not improve over the pre-trained baseline, even for a single query (B = 1).
subsection A.1 presents more detailed results on the privacy-utility trade-off of the baseline meth-
ods: all of them require extremely large e to improve over the pre-trained baseline, with DP-SGD
outperforming the private prediction baselines (S&A and GNMax).
Figure 3: Perplexity of SUBMIX (k = 8) on Wikitext-103 (left) and BigPatent-G (right) as
a function of ROP privacy loss e for three query budget values B . The perplexity of the pre-trained
model and (non-private) single model, ensemble, and fully fine-tuned models are shown for reference.
Varying the Privacy Loss In Figure 3, we show the trade-off between perplexity and ROP privacy
loss, e, of SUBMIX at α = 2. On both Wikitext-103 (left plot) and BigPatent-G (right plot),
SubMix substantially improves over the pre-trained GPT-2 model, even when the query budget
is increased to B = 10, 240 queries. As expected, SUBMIX matches the perplexity of non-private
LM at higher values of e. Interestingly, SUBMIX’s perplexity is even lower than that of a single
non-privately fine-tuned LM at high e. We surmise this is due to the performance gap between a single
fine-tuned LM and an LM ensemble. The effect is less pronounced on Wikitext-103 because
that corpus was split into users by block, as a result of which many LMs contain text blocks from the
same Wikipedia article. This reduces the positive effects of ensembling on predictive perplexity.
4Privacy loss is measured under RDP for DP-SGD, S&A, and GNMax; and under ROP for SubMix.
7
Under review as a conference paper at ICLR 2022
Figure 4: Perplexity of SUBMIX (k = 8) on Wikitext-103 (left) and BigPatent-G (right) as
a function of query budget B for different ROP privacy losses . The perplexity of the pre-trained
model and (non-private) single model, ensemble, and fully fine-tuned models are shown for reference.
Varying the Query Budget Figure 4 shows the trade-off between perplexity and the number of
queries B . The results in the figure were obtained by tuning the target leakage to obtain the desired
budget. As expected, answering more queries using SubMix increases the average perplexity for each
next-token query at a given . However, SUBMIX attains a surprisingly low perplexity for a moderate
number of queries (e.g., B = 210) at all values on both Wikitext-103 and BigPatent-G.
Varying the Number of Parts A key hy-
perparameter of interest in SubMix is the
size of the partition Π. Intuitively, a smaller
number of partitions, allows each part to
train a better quality LM at the cost of a
larger Renyi divergence when a part is re-
moved. Figure 5 shows the trade-off be-
tween perplexity and ROP privacy loss, ,
for varying partition sizes, k . We observe
the key trend that as decreases, perplexity Figure 5: Perplexity of SUBMIX on Wikitext-103 as a
increases more rapidly for smaller k be- function of ROP privacy loss for different partition sizes k.
cause the privacy budget is exhausted more
quickly when each part has a greater relative contribution to the ensemble. The optimal value for k is
generally around 16 for all values of interest, although this may depend on the data distribution and
design choices such as model architecture and training hyperparameters.
4.2	Text Extraction Attacks
To empirically validate that SubMix pre-
vents text extraction attacks, we perform
a random code text-extraction experiment
in the style of Ramaswamy et al. (2020);
Shi et al. (2021). We randomly gener-
ate m codes with each code being an `-
digit number (for example, representing a
user’s age, ZIP-code, phone number, SSN,
etc.). The fine-tuning dataset is constructed
so that each user’s text is single sentence:
Di = “My number is: <random
`-digit number here>”. We then
fine-tune on this dataset and make private
predictions using SubMix for the query
Figure 6: Hit rate of text extraction attacks on SUBMIX for
varying lengths of code. The # of parts is k = 3 and the # of
codes generated is g = 100. The non-privately fine-tuned LM
has a hit rate of ≥ 0.9 for all lengths.
context “My number is:” to test whether SUBMIX prevents text extraction (and if so, at what
). As a baseline, we fine-tune GPT-2 on this dataset for 1000 iterations. This results in the LM
memorizing all ` codes, achieving a perplexity of less than 0.5 and ≥ 90% recall when prompted
with context. We apply SUBMIX with k = m/2 parts so that each model in the ensemble strongly
memorizes one number, achieving near 100% recall when prompted with the context.
8
Under review as a conference paper at ICLR 2022
For the text extract attack, the figure-of-merit is the hit rate of the g generations, defined as the number
of generated codes that exactly match a secret code divided by g . Figure 6 shows the hit rate of the
text extraction attack. For all code lengths ` = 2, . . . , 5, SUBMIX succeeds in preventing the attack at
= 102. For longer code lengths, even higher values of suffice for preventing this random-sampling
text extraction. Intuitively, extraction becomes more difficult as the code lengths increase. This
experiment shows that the mechanism for limiting the release of sensitive information via solving
Equation 2 is effective, and the privacy accounting in SubMix meaningfully measures the amount of
privacy loss. We also ran these attacks for varying values of K and m, and made the same qualitative
observations. In addition, we performed experiments in which we varied g ∈ {10, 100, 1000}. We
found that this does not affect SubMix’s hit rate.
5 Discussion & Related Work
Related Work McMahan et al. (2017) was the first to study the training of differentially private
language models by using DP-SGD to train a small recurrent neural network. However, the resulting
LMs have far fewer parameters than modern transformers and attain much higher perplexities. More
recently, Shi et al. (2021) explored an alternative approach called selective differential privacy,
where the privacy guarantee only applies to blocks of text in the training corpus that are deemed
sensitive, e.g., addresses and phone numbers. Unfortunately, this approach is difficult to scale to
large unstructured text corpora because it requires annotating all text in the corpus with a privacy
sensitivity level.
SubMix has conceptual similarities to PATE (Papernot et al., 2016; 2018) for private semi-supervised
learning. Both SubMix and PATE make predictions using an ensemble of models trained (or fine-
tuned, in the case of SubMix) on private data, and employ a data-dependent and query-dependent
privacy accounting mechanism at prediction time. The central idea in both methods is that privacy
loss is small when models trained on different parts of the data agree on a prediction. However, PATE
is more natural in discriminative or classification tasks because it return a distribution’s noisy argmax.
On the other hand, SubMix is more natural in generative tasks because it returns a sample from the
distribution.
SubMix is also related to prior work on private posterior sampling (Geumlek et al., 2017; Dimi-
trakakis et al., 2017), where the randomness in the privacy mechanism comes from releasing a sample
from a distribution defined by the private data. In particular, SubMix uses a privacy accounting
methodology based on Renyi divergences similar to that of GeUmlek et al. (2017).
Mireshghallah et al. (2021) propose adding a privacy regularizer to language model training to
redUce its memorization of sensitive text. They empirically showed that the regUlarizer redUces the
effectiveness of text extraction attacks, bUt it does not satisfy any formal privacy gUarantee sUch as
DP. Other related works exploring privacy in natUral langUage processing inclUde (Gopi et al., 2020;
LyU et al., 2020; XU et al., 2020; Li et al., 2018; Kim et al., 2021).
Limitations & Future Directions One limitation of the SUBMIX protocol as presented here is
that it only sUpports decoding from the ensemble of LMs by sampling directly from the predicted
pmf. This type of sampling is known to prodUce UnnatUral and incoherent text (KUlikov et al., 2018;
Holtzman et al., 2019). Better decoding methods sUch as top-k sampling (Fan et al., 2018) and nUcleUs
sampling (Holtzman et al., 2019) exist, bUt they reqUire modifications to the protocol that may caUse
additional privacy leakage. However, we note that a close alternative to top-k and nUcleUs sampling is
temperature decoding (Holtzman et al., 2019), which scales the predicted pmf by a temperatUre term
to decrease its entropy. SubMix readily sUpports this decoding method by applying temperatUre
scaling as a post-processing step. In fUtUre work, we aim to extend oUt work by designing protocols
that can sUpport different types of decoding strategies as well.
Another limitation of SubMix is that the Use ofan ensemble sUbstantially increases the compUtational
and storage reqUirements. OUr experiments sUggests that an overhead factor of 8 is needed to attain a
non-vacUoUs trade-off between privacy and Utility. One potential solUtion to redUce the compUtational
reqUirements of SubMix may be to fine-tUne only the top few transformer layers closest to the
prediction head. This woUld allow the evalUation of the bottom transformer layers to be shared
between models in the ensemble, thereby redUcing both compUtation and storage reqUirements. We
leave the exploration of sUch efficiency improvements for fUtUre work.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308-318, 2016.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:
Evaluating and testing unintended memorization in neural networks. In 28th {USENIX} Security
Symposium ({USENIX} Security 19), pp. 267-284, 2019.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data
from large language models. arXiv preprint arXiv:2012.07805, 2020.
Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. Advances in neural information
processing systems, 28:3079-3087, 2015.
Christos Dimitrakakis, Blaine Nelson, Zuhe Zhang, Aikateirni Mitrokotsa, and Benjamin Rubinstein.
Differential privacy for bayesian inference through posterior sampling. Journal of machine learning
research, 18(11):1-39, 2017.
Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. In Conference On Learning
Theory, pp. 1693-1702. PMLR, 2018.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3-4):211-407, 2014.
Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint
arXiv:1805.04833, 2018.
Joseph Geumlek, Shuang Song, and Kamalika Chaudhuri. R\’enyi differential privacy mechanisms
for posterior sampling. arXiv preprint arXiv:1710.00892, 2017.
Sivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad Shokouhi, and
Sergey Yekhanin. Differentially private set union. In International Conference on Machine
Learning, pp. 3627-3636. PMLR, 2020.
Matthias Hertel. Neural language models for spelling correction. Master’s thesis, Albert-Ludwigs-
Universitat Freiburg im Breisgau, 2019.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751, 2019.
Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.
arXiv preprint arXiv:1801.06146, 2018.
Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice.
In 28th {USENIX} Security Symposium ({USENIX} Security 19), pp. 1895-1912, 2019.
Gavin Kerrigan, Dylan Slack, and Jens Tuyls. Differentially private language models benefit from
public pre-training. arXiv preprint arXiv:2009.05886, 2020.
Kunho Kim, Sivakanth Gopi, Janardhan Kulkarni, and Sergey Yekhanin. Differentially private n-gram
extraction. arXiv preprint arXiv:2108.02831, 2021.
Ilia Kulikov, Alexander H Miller, Kyunghyun Cho, and Jason Weston. Importance of search and
evaluation strategies in neural dialogue modeling. arXiv preprint arXiv:1811.00907, 2018.
Yitong Li, Timothy Baldwin, and Trevor Cohn. Towards robust and privacy-preserving text represen-
tations. arXiv preprint arXiv:1805.06093, 2018.
10
Under review as a conference paper at ICLR 2022
Lingjuan Lyu, Xuanli He, and Yitong Li. Differentially private representation for nlp: Formal
guarantee and an empirical study on privacy and fairness. arXiv preprint arXiv:2010.01285, 2020.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. arXiv preprint arXiv:1710.06963, 2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Fatemehsadat Mireshghallah, Huseyin A Inan, Marcello Hasegawa, Victor RUhle, Taylor Berg-
Kirkpatrick, and Robert Sim. Privacy regularization: Joint privacy-utility optimization in language
models. arXiv preprint arXiv:2103.07567, 2021.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263-275. IEEE, 2017.
Piotr Mirowski and Andreas Vlachos. Dependency recurrent neural language models for sentence
completion. In Proc. IJCNLP, pp. 511—-517, 2015.
Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adver-
sary instantiation: Lower bounds for differentially private machine learning. arXiv preprint
arXiv:2101.04535, 2021.
Nicolas Papernot, Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-
supervised knowledge transfer for deep learning from private training data. arXiv preprint
arXiv:1610.05755, 2016.
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar
Erlingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and
FranCOiSe Beaufays. Training production language models without memorizing user data. arXiv
preprint arXiv:2009.10031, 2020.
Alfred Renyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics, pp. 547-561. University of California Press, 1961.
Philippe Rigollet. 18. s997: High dimensional statistics. Lecture Notes), Cambridge, MA, USA: MIT
Open-CourseWare, 2015.
Eva Sharma, Chen Li, and Lu Wang. Bigpatent: A large-scale dataset for abstractive and coherent
summarization. arXiv preprint arXiv:1906.03741, 2019.
Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu. Selective differential privacy for language
modeling. arXiv preprint arXiv:2108.12944, 2021.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more
data). arXiv preprint arXiv:2011.11660, 2020.
Laurens van der Maaten and Awni Hannun. The trade-offs of private prediction. arXiv preprint
arXiv:2007.05089, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingface,s transformers:
State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
11
Under review as a conference paper at ICLR 2022
Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, and Nathanael Teissier. A differentially private
text perturbation method using a regularized mahalanobis metric. arXiv preprint arXiv:2010.11947,
2020.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:
Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations
Symposium (CSF), pp. 268-282. IEEE, 2018.
12