Under review as a conference paper at ICLR 2022
Local Calibration: Metrics and
Recalibration
Anonymous authors
Paper under double-blind review
Ab stract
Probabilistic classifiers output confidence scores along with their predictions, and
these confidence scores should be calibrated, i.e., they should reflect the reliability
of the prediction. Confidence scores that minimize standard metrics such as the
expected calibration error (ECE) accurately measure the reliability on average
across the entire population. However, it is in general impossible to measure the
reliability of an individual prediction. In this work, we propose the local calibration
error (LCE) to span the gap between average and individual reliability. For each
individual prediction, the LCE measures the average reliability of a set of similar
predictions, where similarity is quantified by a kernel function on a pretrained
feature space and by a binning scheme over predicted model confidences. We
show theoretically that the LCE can be estimated sample-efficiently from data, and
empirically find that it reveals miscalibration modes that are more fine-grained than
the ECE can detect. Our key result is a novel local recalibration method LoRe,
to improve confidence scores for individual predictions and decrease the LCE.
Experimentally, we show that our recalibration method produces more accurate
confidence scores, which improves downstream fairness and decision making on
classification tasks with both image and tabular data.
1	Introduction
Uncertainty estimation is extremely important in high stakes decision-making tasks. For example,
a patient wants to know the probability that a medical diagnosis is correct; an autonomous driving
system wants to know the probability that a pedestrian is correctly identified. Uncertainty estimates
are usually achieved by predicting a probability along with each classification. Ideally, we want to
achieve individual calibration, i.e., we want to predict the probability that each sample is misclassified.
However, each sample is observed only once for most datasets (e.g., image classification datasets
do not contain identical images), making it impossible to estimate, or even define, the probability
of incorrect classification for individual samples. Because of this, commonly used metrics such as
the expected calibration error (ECE) measure the gap between a classifier’s confidence and accuracy
averaged across the entire dataset. Consequently, ECE can be accurately estimated but does not
measure the reliability of individual predictions.
In this work, we propose the local calibration error (LCE), a calibration metric that spans the gap
between fully global (e.g., ECE) and fully individual calibration. Motivated by the success of
kernel-based locality in other fields such as fairness (where similar individuals should be treated
similarly) (Dwork et al., 2012; Pleiss et al., 2017) and causal inference (where matching techniques
are used to find similar neighboring samples) (Stuart, 2010), we approximate the probability of
misclassification for an individual sample by computing the average classification error over similar
samples, where similarity is measured by a kernel function in a pre-trained feature space and a
binning scheme over predicted confidences. Intuitively, two samples are similar if they are close in a
pretrained feature space and have similar predicted confidence scores. By choosing the bandwidth
of the kernel function, we can trade off estimation accuracy and individuality: when the bandwidth
is very large, we recover existing global calibration metrics; when the bandwidth is small, we
approximate individual calibration. We choose an intermediate bandwidth, so our metric can be
accurately estimated, and provides some measurement on the reliability of individual predictions.
1
Under review as a conference paper at ICLR 2022
Theoretically, we show that the LCE can be estimated with polynomially many samples if the kernel
function is bounded. Empirically, we also show that for intermediate values of the bandwidth, the
LCE can be accurately estimated and reveals modes of miscalibration that global metrics (such as
ECE) fail to uncover.
In addition, we introduce a non-parametric, post-hoc localized recalibration method (LoRe), for
lowering the LCE. Empirically, LoRe improves fairness by achieving low calibration error on all
potentially sensitive subsets of the data, such as racial groups. Notably, it can do so without any prior
knowledge of those groups, and is more effective than global methods at this task. In addition, our
recalibration method improves decision making when there is a “safe” action that is selected whenever
the predicted confidence is low. For example, an automated system which classifies tissue samples
as cancerous should request a human expert opinion whenever it is unsure about a classification. In
a simulation on an image classification dataset, we show that recalibrated prediction models more
accurately choose whether to use the “safe” action, which improves the overall utility.
In summary, the contributions of our paper are as follows. (1) We introduce a local calibration metric,
the LCE, that is both easy to compute and can estimate the reliability of individual predictions. (2)
We introduce a post-hoc localized recalibration method LoRe, that transforms a model’s confidence
predictions to improve the local calibration. (3) We empirically evaluate LoRe on several downstream
tasks and observe that LoRe improves fairness and decision-making more than existing baselines.
2	Background and Related Work
2.1	Global Calibration Metrics
Consider a classification task that maps from some input domain (e.g., images) X to a finite set of
labels Y = {1, ∙∙∙ ,m}. A classifier is a pair (f,p) where f : X → Y maps each input X ∈ X
to a label y ∈ Y and p: X → [0,1] maps each input X to a confidence value c. Let Pr be a joint
distribution on X × Y (e.g., from which training or test data pairs (x, y) are drawn). The classifier
(f,p) is perfectly calibrated (G GUo et al, 201 ) with respect to Pr if for all C ∈ [0,1]
Pr[f (X)= Y | p(X) = c] = c.	(1)
To numerically measure how well a classifier is calibrated, the most commonly used metric is
the expected calibration error (ECE) (Naeini et al., 2015; Guo et al., 2017), which measures the
average absolute deviation from Eq. 1 over the domain. In practice, given a finite dataset, the ECE is
approximated by binning. The predicted confidences P are partitioned into bins Bi,..., Bk, and then
a weighted average is taken of the absolute difference between the average confidence conf(Bi) and
average accuracy acc(Bi) for each bin Bi:
k
ECE(f,P) := X 牛∣conf(Bi) - acc(Bi)∣.	⑵
i=1 N
Similarly, the maximum calibration error (MCE) (Naeini et al., 2015; Guo et al., 2017) measures the
average deviation from Eq. 1 in the bin with the highest calibration error, and is defined as
MCE(f,p) := max ∣conf(Bɪ) — acc(BJ∣.	(3)
i
2.2	Existing G lobal Recalibration Methods
Many existing methods apply a post-hoc adjustment that changes a model’s confidence predictions
to improve global calibration, including Platt scaling (Platt, 1999), temperature scaling (Guo et al.,
2017), isotonic regression (Zadrozny & Elkan, 2002), and histogram binning (Zadrozny & Elkan,
2001). These methods all learn a simple transformation from the original confidence predictions to
new confidence predictions, and aim to decrease the expected calibration error (ECE). Platt scaling
fits a logistic regression model; temperature scaling learns a single temperature parameter to rescale
confidence scores for all samples simultaneously; isotonic regression learns a piece-wise constant
monotonic function; histogram binning partitions confidence scores into bins {[0, e), [e, 2e),…，[1 —
e, 1]} and sorts each validation sample into a bin based on its confidence p(χ); it then resets the
confidence level of all samples in the bin to match the classification accuracy of that bin.
2
Under review as a conference paper at ICLR 2022
2.3	Local Calibration
Two notions of calibration that address some of the deficits of global calibration are class-wise
calibration and group-wise calibration. Class-wise calibration groups samples by their true class
label (Kull et al., 2019; Nixon et al., 2019) and measures the average class ECE, while group-wise
calibration uses pre-specified groupings (e.g., race or gender) (Kleinberg et al., 2016; Pleiss et al.,
2017) and measures the average group-wise ECE or maximum group-wise MCE.
A few recalibration methods have been proposed for these notions of calibration as well. Dirichlet
calibration (Kull et al., 2019) achieves calibration for groups defined by class labels, but does not
generalize well to settings with many classes (Zhao et al., 2021). Multicalibration (Hebert-Johnson
et al., 2017) achieves calibration for any group that can be represented by a polynomial sized circuit,
but lacks a tractable algorithm. If the groups are known a priori, one can also apply global calibration
methods within each group; however, this is impractical in many situations where the groups are not
known for new examples at inference time.
At an even more local level, Zhao et al. (2020) look at individual calibration in the regression setting
and conclude that individual calibration is impossible to verify with a deterministic forecaster, and
thus there is no general method to achieve individual calibration.
2.4	Kernel-Based Calibration Metrics
Kumar et al. (2018) introduce the maximum mean calibration error (MMCE), a kernel-based quantity
that replaces the hard binning of the standard ECE estimator with a kernel similarity k(p(x),p(x0))
between the confidence of two examples. They further propose to optimize the MMCE directly in
order to achieve better model calibration globally. Widmann et al. (2019) extend their work and
propose the more general kernel calibration error. Zhang et al. (2020) and Gupta et al. (2020) also
consider kernel-based calibration. However, these methods only consider the similarity between
model confidences p(χ),p(χ0), rather than the inputs x, χ0 themselves.
3	The Local Calibration Error
Recall that commonly used metrics for calibration, such as the ECE or the MCE, are global in nature
and thus only measure an aggregate reliability over the entire dataset, making them insufficient for
many applications. An ideal calibration metric would instead measure calibration at an individual
level; however, doing so is impossible without making assumptions about the ground truth distribu-
tion (Zhao et al., 2020). A localized calibration metric represents an adjustable balance between these
two extremes. Ideally, such a metric should measure calibration at a local level (where the extent of
the local neighborhood can be chosen by the user) and group similar data points together.
In this section, we introduce the local calibration error (LCE), a kernel-based metric that allows
us to measure the calibration locally around a prediction. Our metric leverages learned features to
automatically group similar samples into a soft neighborhood, and allows the neighborhood size to
be set with a hyperparameter γ. We also consider only points with a similar model confidence as the
prediction, so that similarity is defined in terms of distance both in the feature space and in model
confidence. Thus, the LCE effectively creates soft groupings that depend on the feature space; with a
semantically meaningful feature space, these groupings correspond to useful subsets of the data. We
then mention a few design choices and visualize LCE maps over a 2D feature space to show that we
can use our metric to diagnose regions of local miscalibration.
3.1	Local Calibration Error Metric
We propose a metric to measure calibration locally around a given prediction. The calibration of
similar samples should be similar, so we use a kernel similarity function kγ : X × X → R+, which
provides similarity scores, to define soft local neighborhoods. kγ (x, x0) has bandwidth γ > 0, which
determines the extent of the local neighborhood — as γ increases, the neighborhood grows. Less
similar (i.e., further away) samples x0 have less influence on the local calibration metric at x. Also,
as with the ECE and MCE (Eqs. 2 and 3), we use binning and consider only the points in the same
3
Under review as a conference paper at ICLR 2022
confidence bin as x. Thus, the samples that influence the local calibration metric at x are similar to x
in both features and model confidences.
More formally, let φ : X → Rd be a feature map that transforms an input to a feature vector, and let
kγ be parameterized as kγ (x, x0) = g((φ(x) - φ(x0))∕γ) for some LiPschitz function g : Rd → 股十.
Then given a data point X ∈ X and a classifier (f,p), the local calibration error (LCE) of the model
at x is the expected difference between the model’s confidence and accuracy on a randomly sampled
data point x0 〜Pr, weighted by the kernel similarity kγ(x, x0).
We say a probabilistic classifier (P, f) is perfectly locally calibrated with respect to kγ if
SUp	E(χ0,y0)〜Prh(P(x0)	-	1[f(X0)	=	y0])kγ(X, X0)	I P(X0)	= P(x)i	=	0.
x∈supp(Pr)、一	L	{Z	I	」,
=LCEY (x;f,p)
Similar to perfect calibration, perfect local calibration is achieved by the Bayes-optimal classifier. In
general, perfect local calibration is a much stricter notion than perfect calibration due to localizing to
each indiviudal data point X, and reduces to perfect calibration if kγ (X, X0) ≡ 1 is a trivial kernel.
To define LCE on a finite dataset, we perform an additional binning on the confidence to deal with the
conditioning. Let D = ((χι,yι),..., (XN, yN)) be a dataset, and let β(χ) = {i : P(Xi) ∈ B(P(X))}
be the set of indices of the points in D occupying the same confidence bin as X. Then we can compute
the LCE by
Td , 一、	Pi∈β(x)(P(Xi)- 1[f(Xi) = yi])k(X,Xi)
LCEY(x; f,P)=————Q-----------------T7——;-------------.	(4)
I	i∈β (x) k(X, Xi)	I
Note that the quantity (P(Xi) - 1 [f (Xi) = yj) is simply the difference between the confidence and
the accuracy for sample Xi , and the denominator is a normalization term.
We then define the maximum local calibration error (MLCE) as
MLCEY(f,P) := maxLCEγ(x; f,P).	(5)
x
Intuitively, the LCE considers a neighborhood about a sample X (as defined by the kernel kY and
the confidence bin B), and computes the kernel-weighted average of the difference between the
confidence and accuracy for each sample in that neighborhood. Note that by changing the bandwidth γ,
we can interpolate the LCE between an individualized calibration metric (as γ → 0) and a global one
(as γ → ∞). Lemma 1 makes this more concrete under the assumption that limY→∞ kY (X, X0) = 1
(proof in Appendix C). For example, the Laplacian and Gaussian kernels satisfy this condition.
Lemma 1. As γ → ∞, the MLCE converges to the MCE.
Theorem 1 shows that under certain regularity conditions, the finite-sample estimator LCE(X)
converges uniformly and sample-efficiently to its true expected value LCE* (x):
Theorem 1. (Informal) Leta ≤ infχ∈χ E [kγ(X, x)1[P(X) ∈ B(P(X))]] be a lower bound on the
expectation of the kernel, and d be the dimension of the kernel’s feature space. If the sample size is at
least O(d∕α42) where > 0 is a target accuracy level, then with probability at least 1 - δ we have
SUp ∣LCEγ (x; f,P) - LCEY(x; f,P)∣ ≤ e.
x∈X
Here, O hides log factors of the form log(1∕αγδe). In practice, α depends inversely on γ.
To summarize, the MLCE measures a worst-case individual calibration error as γ → 0 (i.e., the
effective neighborhood is very small) and converges to the global MCE metric as γ → ∞ (i.e., the
effective neighborhood is very large). In practice, one must pick intermediate values of γ to balance a
more local notion of calibration error with the sample efficiency of its estimation. A more formal
statement and full proof of Theorem 1 can be found in Appendix D.
3.2	Choice of Kernel and Feature Map
In this work, we compute the LCE using 15 equal-width bins and use the Laplacian kernel
kγ (X,χ0)=exp(-kφ(X) - φ(x0)kι )
dγ
4
UlIderreVieW as a COnferenCe PaPer at ICLR 2022
BeCaUSe dis-ances in a high—dimensionals∙pur SPaCe (eg)image dara) may nor be meaningful On
their OWn)We evalua-e the kernel On a fea≡re representaro∙n Of 0 rather than On 0 irself∙ FearUreS
Ieanled from IleU邑 Ile-WOrkShaVe PrOVeII USefUI for a Wide range Of tasks⅛d they have been ShOWII
8 CaP - Ure USefUI SemanHC feaɛres Of 一heirB∙pss (HUh et aΓ22'6; Chen et aΓ2020; Li et aΓ2。2。).
The kernel SimiIariry -ermin the LCE thus leverages learned fea-ures -O antomaticaUy
CaP≡re rich SUbgrOUPS Ofrhe data∙ FOr image daa we ChOSe an InCeP 一O∙nlv3 model as OUr feaBre
map)SmCe IIlCeP 一o∙n features are Widely accepted as USefBand represneB∙many areas (eg)for
generative models (SalimanS et al∙2 2Oi6))二 60Ughsher neu 邑 fea≡res Can also be USed (APPendix
B) ∙ FOr tabular data) we USed rhe Hnal hidden Iayer Ofthe neural ne-work trained for dass≡caro∙n∙
In generaLWe also USe r—SNE S PCAε
reduce the dimension Of the feature space,
FOr exampp-he 2048—D IiICePrO-H—V3 em—
bedding is S£1 Very high—dimenso-naL We
report results USingt—SNE to reduce the di—
menso∙n to 2 Or 3&S WelI as PCAtO reduce
the dimension -O reduce the dimension -O
50 for image data and 20 for Bbular dara∙
ThUS rhe OVerallrePreSen 一 aro∙n func 一o∙n is
B(S) H 02(01H)L where SlmaPS from
thes∙pursεthe neural fea≡re∞and S2
reduces rhe feature SPaCe dimension,
FigUre 1 pors the MLCE as a function Of
一he kernel bandwid一h for an ImageNet ClaS—
S≡caro∙n rask∙ NOre -har When q is SmalL
一he MLCE is 1 (a Worsrcase individual Ca丁
ibraro∙n errorL and When q is large〉the
MLCE approaches the global MCE∙ To Ob—
Bin a SingIe SUmmary SrariSro∙describing
一heOCaI calibration error〉We CaII VieW 一his
Por and PiCk a VaIUe Of q berween -he Iim—
≡ng behavo∙rs∙ We Hnd thar q = 0.2 and q
and 50—D PCA feaBres〉respectively CFiguri
MLCE
Kerne- BandWidth y
FigUreL MLCE Of a ReSner—50 CIaSSiner On the ImageNer
Ssr SPFas a function OfrhekenIeI bandwidth / We USe
a LaPlaCian kernel Wirh feature map e2 Oe1“ Where el -
t →¾04s -S rhe InCePrO∙nlv3 model∞hidden IayeK Blue:
02 ：为2048 为 3 isfrSNE; Orange- e2 ..另2048 ^ 5。-S
PCA; greem e2 (Z) H ZiS the iden≡y∙
=0.4 are goods∙rermediare ps.nrs for the 3—D r—SNE
J∙
3∙3 LoCAL CALIBRATIoN ERRoR VISUALIZATIoNS
LCE(X)5∙Bin 5/15 (Besf Goba - Ca=braf5-m
LCE(X二 n Binl2/15 (WorSf Goba - Ca=braf5-m
CDFof Lc-x)in Bin 5/15 {BesG°ba - Ca=braf°m
FigUre 2- We ViSUaHZe LCEO∙2(jifor a ReSNe'50 CIaSS≡er (∖0) Pre-rained On ImageNeL for every
image 2rhe ImageNer validation SeL We focus On rhe binS Wirh rhe beand WorSr global CaHbrarion error
Ho PrOvide more intuition for the LCEn We Wi= now ViSUaHZe SOme examples Ofthe LCE metric OVer
a 2—D feature embedding。We COnSider a ReSNet—50 model pre—trained On ImageNet as OUr ClaSSifier
Under review as a conference paper at ICLR 2022
(f,p^), and pre-trained IncePtion-v3 features as a feature map φι : X → R2048. φ2 : R2048 → R2
then reduces the 2048-D feature vectors with t-SNE to two dimensions for ease of visualization in the
LCE landscapes, so our overall representation function is φ = φ2 (φ1(x)). Figure 2 visualizes the
landscape of LCE0.2(χ; f,p^) as a function of φ(χ) for the entire ImageNet validation set, as well as
the marginal CDF of LCE0.2(x, f,p). We show these visualizations for the two confidence bins with
the best and worst global calibration.
In the bin with the best global calibration, Figure 2 (top) shows that the landscape of the LCE is
highly non-uniform, and the CDF of the LCE lies almost entirely to the right of the bin’s average
calibration error. Numerically, the bin’s average calibration error is 0.0022, while its average LCE
is 0.0259. This implies that the regions where the model is underconfident and overconfident are
spatially clustered within the bin. Because global calibration metrics solely consider the average
accuracy and average confidence within a bin, confidence predictions that are too high and too low
are averaged out to obtain a low overall error value; they fail to capture this localized miscalibration.
In the bin with the worst global calibration, Figure 2 (bottom) clearly shows that the LCE still has
high variance, even though the average calibration error of the bin (0.0455) is much closer to its
average LCE (0.0515). The CDF plot provides more evidence that the landscape is not flat — there
is no sharp rise at the bin calibration error. However, in this case the regions that are underconfident
and overconfident are not clustered spatially.
4	LCE Recalib ration
In this section, we introduce local recalibration (LoRe), a non-parametric recalibration method
that adjusts a model’s output confidences to achieve better local calibration. Our method improves
the LCE more than existing recalibration methods, and using our method improves performance
on both downstream fairness tasks and downstream decision-making tasks. Specifically, we can
leverage the kernel similarity to achieve strong calibration for all sensitive subgroups of a population,
without knowing those groups a priori. As long as the feature space is semantically meaningful,
LoRe provides utility for downstream tasks without needing subgroup labels for the samples. If the
subgroups are known, one can recover standard group-wise recalibration methods (and metrics) by
using the improper kernel k(x, x0) = 1[x, x0 in same group].
The idea behind our method is simple: we can compute the kernel-weighted accuracy for each point
x of all the points that are in the same confidence bin as x, and then reset the confidence of x to
this kernel-weighted accuracy value. Note that using the kernel function to compute this value is
intuitively like taking a weighted average of the accuracy of the points in the local neighborhood of x.
Thus, LoRe can be considered a local analogue to histogram binning.
More formally, given a trained classifier (f,p), a recalibration dataset D = ((x1,y1),..., (XN ,yN)),
and a fixed point X ∈ X, let β(χ) = {i : P(Xi) ∈ B(P(X))} be the set of indices of the points in D
occupying the same confidence bin as x. Then, we compute the recalibrated confidence as
p0(x)
i∈β(x) kγ(X, Xi)1[f(Xi) = yi]
(6)
i∈β(x)kγ(X,Xi)
Equation 6 represents the kernel-weighted average accuracy of all points in the same confidence bin
as x. In the limit as the kernel bandwidth Y → ∞, P(X) → Pi∈β(χ) 1[f (Xi) = yi]∕∣β(χ)∣ recovers
histogram binning. As Y → 0, p^0(x) → 1[f (x* ) = y%*], where i = argmini∈β(χ) kγ(x,xJ, thus
recovering a nearest-neighbor method. For intermediate γ, our method interpolates between the
two extremes. Throughout this work, we used used Y = 0.2 for LoRe with tSNE and Y = 0.4 for
LoRe with PCA throughout this work, since these represent intermediate points between the limiting
behaviors of the LCE (e.g., see Fig. 1).
5	Experiments
In this section, we show empirically that LoRe substantially improves LCE values, and that these
lower LCE values lead to better performance on downstream fairness and decision-making tasks.
In particular, we evaluate the local calibration through the MLCE, because we are interested in
6
Under review as a conference paper at ICLR 2022
∙0∙8∙64∙2∙0
Iooooo
山ɔiw
10-3 IO-2 lθ-ɪ IO0 IO1 IO2
Kernel Bandwidth γ
Figure 3: MLCE vs. kernel bandwidth γ for ImageNet.
LoRe (with t-SNE and γ = 0.2) achieves the lowest
MLCE for a wide range of γ . This suggests that LoRe
leads to lower LCE values across the whole dataset.
-1500
-1000
peMωα
Ooo
Ooo
5 0 5
1 1
Figure 4: Reward attained vs. reward ratio for the
ImageNet dataset (higher is better). LoRe achieves
the highest rewards across a wide range of reward
ratios.

understanding a model’s worst-case local miscalibration. On each task, we compare the performance
of LoRe to no recalibration (‘Original’), temperature scaling (‘TS’) (Guo et al., 2017), histogram
binning (‘HB’) (Zadrozny & Elkan, 2001), isotonic regression (‘IR’) (Zadrozny & Elkan, 2002), and
direct MMCE optimization (‘MMCE’) (Kumar et al., 2018), all strong global recalibration methods.
We first run extensive experiments on three datasets to demonstrate that LoRe outperforms all
baselines and achieves the lowest MLCE over a wide range of γ values. We then evaluate the
performance of our method on a fairness task, where it is important that a model is well-calibrated
for all sensitive subgroups of a given population, and we demonstrate that it achieves the lowest
group-wise MCE. Notably, we find that the MLCE is well-correlated with the group-wise MCE across
all experimental settings, and thus achieving low MLCE is a good indicator that a model has good
group-wise calibration. Finally, we compare the performance of our method against the baselines on
a cost-sensitive decision-making task, where there is a low cost for a prediction of “unsure” but a
high cost for an incorrect prediction, and show that our method achieves the lowest cost.
5.1	Datasets
ImageNet dataset (Deng et al., 2009): A large-scale dataset of natural scene images with 1000
classes; over 1.3 million images total. The training/validation/test split is 1.3mil / 25,000 / 25,000.
UCI Communities and Crime dataset (Dua & Graff, 2017): This tabular dataset contains a
number of attributes about American neighborhoods (e.g., race, age, employment, housing, etc.).
The task is to predict the neighborhood’s violent crime rate. The training/validation/test split is
1494 / 500 / 500. We randomize the training/validation/test split over multiple trials.
CelebA dataset (Liu et al., 2015): A large-scale dataset of face images with 40 attribute anno-
tations (e.g., glasses, hair color, etc.); 202,599 images total. The training/validation/test split is
162,770 / 19,867 / 19,962.
5.2	Recalibration Performance
LoRe substantially improves the LCE values. In Figure 3, we plot the MLCE as a function of γ.
We can see that our method outperforms all baselines (strong global calibration methods) across a
wide range of γ values on ImageNet. This is true despite the fact that we only implement LoRe for a
single γ. Appendix B provides similar results on the Communities & Crime, CelebA, CIFAR-10, and
CIFAR-100 datasets. Note that LoRe works well regardless of the feature map and the dimensionality
reduction method (see Section 5.3 for results with both t-SNE and PCA). Although the results shown
in this section use Inception-v3 features, we show similar results in Appendix B with AlexNet
(Krizhevsky, 2014), DenseNet121 (Huang et al., 2018), and ResNet101 (He et al., 2015) features.
Recall that as γ gets large, the MLCE recovers the MCE; because LoRe does well even at large γ,
our method also works well at minimizing global calibration errors. The fact that LoRe lowers the
worst-case LCE suggests that it leads to lower LCE values across the entire dataset.
7
Under review as a conference paper at ICLR 2022
5.3	Downstream Fairness Performance
Experimental Setup In many fairness-related applications, it is important to show that a model
is well-calibrated for all sensitive subgroups of a given population. For example, when predicting
the crime rate of a neighborhood, a model should not be considered well-calibrated if it consistently
underestimates the crime rate for neighborhoods of one demographic, while overestimating the
crime rate for neighborhoods of a different demographic. Therefore, in this section, we examine the
worst-case group-wise miscalibration of a classifier, as measured by the maximum group-wise MCE
when evaluated only on sensitive sub-groups. We consider the following experimental settings:
1.	UCI Communities and Crime: Predict whether a neighborhood’s crime rate is higher than the
median; group neighborhoods by their plurality race (White, Black, Asian, Indian, Hispanic).
60 random seeds for model training.
2.	CelebA: Predict a person’s hair color (bald, black, blond, brown, gray, other); group people
by hair type (bald, receding hairline, bangs, straight, wavy, other). 20 random seeds for
model training.
3.	CelebA: Predict a person’s hair type; group people by their hair color; inverse of Setting 2.
20 random seeds for model training.
For each task, we train a classifier (see Appendix A for full details) and recalibrate its output
confidences using each of the recalibration methods.
Results Table 1 reports the
maximum group-wise MCE for
each of the recalibration methods
on each of the three tasks. LoRe
outperforms the other baselines,
achieving an average 49% reduc-
tion over no recalibration and an
average 23% improvement over
the next best global recalibration
method. (Note in Figures 5, 6,
and 7 in Appendix B that LoRe
is the most effective method of
Recalibration method	Setting 1	Setting 2	Setting 3
No recalibration	0.588±0.107	0.407 ± 0.087	0.446 ± 0.083
Temperature scaling	0.521±0.092	0.532±0.089	0.441±0.079
Histogram binning	0.515±0.081	0.218±0.056	0.268 ± 0.067
Isotonic regression	0.596±0.063	0.615±0.100	0.716±0.082
MMCE optimization	0.526±0.172	0.429 ± 0.079	0.475 ± 0.079
Group temp. scaling	0.423 ± 0.066	0.673 ± 0.075	0.329±0.108
Group hist. binning	0.542±0.083	0.260 ± 0.053	0.352±0.068
LoRe (tSNE) (ours)	0.351 ± 0.084	0.165 ± 0.055	0.235±0.063
LoRe (PCA) (ours)	0.392±0.071	0.167±0.013	0.154 ± 0.082
Table 1: Performance on downstream fairness, as measured by maximum
group-wise MCE (lower is better). Experimental settings as described in
Section 5.3. Mean and standard deviations are computed over 60 random
seeds for setting 1, and 20 for settings 2 and 3. Best results are bold.
lowering the MLCE over a wide
range of γ). Notably, LoRe is
robust to the feature map used
(tSNE vs. PCA). It even outper-
forms global methods applied to each individual group, implying that correcting local calibration
errors is a robust way to improve group calibration that generalizes better than naive alternatives.
Moreover, Table 2 shows that the maximum
group-wise MCE is well-correlated with the
MLCE, and it is in fact much better correlated
with MLCE than with global calibration metrics
Taken together, our results indicate that lowering
the LCE has positive implications in fairness set-
tings that cannot be achieved by simply lowering
global metrics like the ECE. For reference, we
also include the performance of all recalibration
methods on various global calibration metrics
in Table 3, which shows that LoRe is able to
improve worst-case group-wise calibration with-
out meaningfully sacrificing (and in some cases
improving) average-case global calibration.
		Setting 1	Setting 2	Setting 3
ECE	0.102	-0.061	-0.195
MCE	0.233	0.439	0.281
NLL	0.542	0.045	-0.287
Brier	0.101	0.144	-0.280
MLCE0.2 (tSNE)	0.642	0.801	0.591
MLCE0.4 (PCA)	0.639	0.659	0.778
Table 2: Pearson correlation between max group-wise
MCE and other calibration metrics (higher is better).
Experimental settings as described in Section 5.3. Best
results in bold. MLCE is better-correlated with the max
group-wise MCE than any of the global metrics.
5.4	Downstream Decision-Making
Experimental Setup Machine learning predictions are often used to make decisions, and in many
situations an agent must select a best action in expectation. As an example, suppose there is a low
8
Under review as a conference paper at ICLR 2022
Recalibration method	ECE(%)	Setting 1 NLL	Brier	ECE(%)	Setting 2 NLL	Brier	ECE(%)	Setting 3 NLL	Brier
No recalibration	15.12.7	.96.25	.17.02	1.80.3	.617.004	.641.004	1.1o.3	.782.006	.571.006
Temperature scaling	4.9i.7	.43.03	.14.01	2.00.3	.619.004	.622.003	1.0O.2	.781.006	.569.002
Histogram binning	3.3ι.ι	.48.03	.150.1	2.5o.2	.619.004	.614.003	2.5o.4	.788.006	.552.002
Isotonic regression	30.62.3	.79.05	.30.02	2.60.2	.618.004	.615.003	2.4o.2	.785.006	.553.002
MMCE optimization	4.4i.3	.43.03	.14.01	3.8o.7	.646.014	.679.012	5.4o.8	.808.009	.619.009
LoRe (tSNE) (ours)	3.5ι.ι	.42.02	.13.01	2.80.2	.623.004	.613.003	2.60.3	.792.006	.551.002
LoRe (PCA) (ours)	4.5i.4	.44.02	.14.01	3.1o.2	.628.004	.606.003	2.80.4	.792.007	.538.002
Table 3: Performance on global calibration metrics, formatted as meansd. Lower is better. Experimental settings
as described in Section 5.3. Best results are bold. Across all settings, LoRe generally achieves a global
calibration error that is comparable to the baselines.
cost u associated with returning “unsure” and a high cost w associated with returning an incorrect
classification (e.g., in situations such as autonomous driving, being unsure incurs only the small cost
of calling a human operator, but making an incorrect classification incurs a high cost). An agent
with good uncertainty quantification can make a more optimal decision about whether to return a
classification or return “unsure”; for a calibrated model, it would be optimal for the agent to return
“unsure” below the confidence threshold of 1 - u/w, and return a prediction above this threshold.
Following this policy — i.e., returning “unsure” when the confidence is below this threshold and
returning a prediction when the confidence is above it, we used a ResNet-50 model to make predictions
on ImageNet, and recalibrated the predictions with each of the recalibration methods. For each method,
we then calculated the total reward attained under various reward ratios w/u, as well as the prediction
rejection area ratio (PRR) (Malinin et al., 2020), which measures the quality of the rejection curve
when a model can choose to not provide any prediction. Note that a PRR of 1.0 indicates optimal
rejection, and a PRR of 0.0 indicates “random” rejection; a higher PRR is better.
Results In Figure 4, we show the improvement in the total reward over	Recalibration method	PRR	ECE	NLL	Brier
					
the original classifier (i.e., no recali-	No recalibration	0.566	0.037	0.959	40.64
bration) as a function of the reward	Temperature scaling	0.561	0.022	0.948	40.60
ratio w/u (the ratio of the cost of an	Histogram binning	0.461	0.012	0.952	40.59
incorrect classification to the cost of	Isotonic regression	0.570	0.011	0.945	40.59
being unsure). Across a wide range of	MMCE optimization	0.571	0.061	0.965	40.67
	LoRe (ours)	0.575	0.007	0.955	40.58
reward ratios, LoRe achieves the high-					
est reward. The MLCE curves for this					
task are shown in Figure 3; note that	Table 4: Performance on	downstream decision-making on Ima-			
LoRe also achieves lower LCE values	geNet, as measured by the PRR (higher is better). We also report				
than the global recalibration methods. Table 4 also reports the PRR achieved	the global calibration metrics ECE, NLL, and Brier score (lower is better). Best results in bold.				
with each method under a cost function of u = -1, w = -10, as well as several global calibration
metrics; LoRe generally outperforms the other baselines in terms of both PRR and global calibration
metrics. These results indicate that our recalibration method most effectively lowers LCE values
without sacrificing (and indeed often improving) average-case global calibration, and that these lower
LCE values correspond to better performance on this decision-making task.
6	Conclusion
In this paper, we introduce the local calibration error (LCE), a metric that measures calibration in
a localized neighborhood around a prediction. The LCE spans the gap between fully global and
fully individualized calibration error, with an effective neighborhood size that can be set with a
bandwidth parameter γ. We also introduce LoRe, a recalibration method that greatly improves the
local calibration. Finally, we demonstrate that achieving lower LCE values leads to better performance
on downstream fairness and decision-making tasks. In future work, we hope to explore better feature
spaces to define similarity, since the quality of our metric depends on the quality of the feature space.
9
Under review as a conference paper at ICLR 2022
References
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daume In and Aarti Singh (eds.), Proceedings
of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp.1597-1607. PMLR, 13-18 Jul 2020. URL http://Proceedings.
mlr.press/v119/chen20j.html.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,
ITCS ’12, pp. 214-226, New York, NY, USA, 2012. Association for Computing Machinery. ISBN
9781450311151. doi: 10.1145/2090236.2090255.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1321-1330, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/guo17a.html.
Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu,
and Richard Hartley. Calibration of neural networks using splines. 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2015.
Ursula Hebert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Calibration for the
(computationally-identifiable) masses, 2017. URL http://arxiv.org/abs/1711.08513.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. 2018.
Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer
learning?, 2016. URL http://arxiv.org/abs/1608.08614.
Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair
determination of risk scores, 2016. URL http://arxiv.org/abs/1609.05807.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. CoRR,
abs/1404.5997, 2014. URL http://arxiv.org/abs/1404.5997.
Meelis Kull, Miquel Perello Nieto, Markus Kangsepp, Telmo Silva Filho, Hao Song, and Peter Flach.
Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet
calibration. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32, pp. 12316-12326. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
file/8ca01ea920679a0fe3728441494041b9-Paper.pdf.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 2805-2814, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/kumar18a.html.
Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence
embeddings from pre-trained language models. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pp. 9119-9130. Association for
Computational Linguistics, November 2020. doi: 10.18653/v1/2020.emnlp-main.733. URL
https://www.aclweb.org/anthology/2020.emnlp-main.733.
10
Under review as a conference paper at ICLR 2022
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
A. Malinin, Bruno Mlodozeniec, and M. Gales. Ensemble distribution distillation. ArXiv,
abs/1905.00076, 2020.
Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In AAAI,, pp. 2901-2907, 2015.
Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibration in deep learning. ArXiv, abs/1904.01685, 2019.
John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin Classifiers, pp. 61-74. MIT Press, 1999.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness
and calibration. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30, pp. 5680-
5689. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/
2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7- Paper.pdf.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In NIPS, 2016.
Elizabeth A. Stuart. Matching methods for causal inference: A review and a look forward. Statistical
science : a review journal of the Institute of Mathematical Statistics, 25(1):1-21, 2010. doi:
10.1214/09-STS313.
David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classification:
A unifying framework. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In In Proceedings of the Eighteenth International Conference on
Machine Learning, pp. 609-616. Morgan Kaufmann, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probabil-
ity estimates. In SIGKDD, 2002.
Jize Zhang, Bhavya Kailkhura, and T. Yong-Jin Han. Mix-n-match: Ensemble and compositional
methods for uncertainty calibration in deep learning. 2020.
Shengjia Zhao, Tengyu Ma, and S. Ermon. Individual calibration with randomized forecasting. In
ICML, 2020.
Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating
predictions to decisions: A novel approach to multi-class calibration, 2021.
11
Under review as a conference paper at ICLR 2022
A Model Architecture, Training, and Other Hyperparameters
For ImageNet and CelebA, we compute the ECE, MCE, and LCE using 15 equal-width confidence
bins. For the UCI communities and crime dataset, we use 5 equal-width bins because the dataset is
much smaller (500 datapoints for recalibration). These numbers of bins represent a good tradeoff
between bias and variance in estimating the relevant calibration errors. We also ran some initial
experiments with equal-mass binning, but found that the results were very similar to those obtained
with equal-width binning.
A.1 ImageNet
For all experiments with the ImageNet dataset, we used the pre-trained ResNet-50 model from the
PyTorch torchvision package as our classifier. To calculate the LCE and apply LoRe, we used
pre-trained Inception-v3 features, applying either t-SNE to reduce their dimension to 3 or PCA to
reduce their dimension to 50, as a feature representation for the kernel.
A.2 UCI Communities and Crime
For all experiments with the UCI communities and crime dataset, we used a 3-hidden-layer dense
neural network as our base classifier. Each hidden layer had a width of 100 and was followed by a
Leaky ReLU activation. We applied dropout with probability 0.4 after the final hidden layer. We
trained the model using the Adam optimizer with a batch size of 64 and a learning rate of 3 × 10-4
until the validation accuracy stopped improving. All other hyperparameters were PyTorch defaults.
Training was done locally on a laptop CPU. We trained 60 different models with different random
seeds to perform the experiments described in Section 5.3 and Figure 5. To calculate the LCE and
apply LoRe, we used the final hidden layer representation learned by our model, applying t-SNE to
reduce the dimension to 2 or PCA to reduce their dimension to 20, as a feature representation for the
kernel.
A.3 CelebA
For all experiments with the CelebA dataset, we trained a ResNet50 model and used it as our base
classifier. We applied standard data augmentation to our training data (random crops & random
horizontal flips), and trained all models for 10 epochs using the Adam optimizer with a learning rate
of 1 × 10-3 and a batch size of 256. All other hyperparameters were PyTorch defaults. Training
was distributed over 4 GPUs, and training a single model took about 30 minutes. For both Setting
2 and Setting 3 (described in Section 5.3), we trained 20 models with different random seeds to
perform the experiments shown in Figures 6 and 7. To calculate the LCE and apply LoRe, we used
pre-trained Inception-v3 features, applying t-SNE to reduce their dimension to 2 or PCA to reduce
their dimension to 50, as a feature representation for the kernel.
B Additional Experimental Results
In Figures 5, 6, and 7, we visualize the MLCE achieved by all recalibration methods for the three
experimental settings evaluated in Section 5.3. Figure 3 in the main paper shows the same visualization
for all methods on ImageNet. In Figure 8, we plot the MLCE achieved by all recalibration methods
for CIFAR-100, and in Figure 9, we do the same for CIFAR-10. Across all settings and datasets, our
method LoRe is the most effective at minimizing MLCE across a wide range of γ, even accounting
for variations between runs.
12
Under review as a conference paper at ICLR 2022
0.2
-°-8-6
loo
山ɔiw
10-3 IO-2 lθ-ɪ IO0 IO1 IO2
Kernel Bandwidth γ
-°-8-6
loo
山 ɔl
10-3 IO-2 lθ-ɪ IO0 IO1 IO2
Kernel Bandwidth γ
0.2
∙0∙8∙64
Iooo
山ɔiw
0.0
io'-3......io'-2......io'-ɪ......ib0 .....io1 ......io2
Kernel Bandwidth γ
Figure 5: MLCE vs. kernel bandwidth γ for all methods on task 1 of Section 5.3, predicting whether a
neighborhood’s crime rate is higher than the median. LoRe achieves the best (or competitive) MLCE for most γ.
Left: 2D t-SNE features. Right: 20D PCA features.
∙0∙8∙64
Iooo
山 ɔl
10^3	10-2 io-ɪ IO0 IO1 IO2
Kernel Bandwidth γ
0.2
∙0∙8∙64
Iooo
山ɔiw
10~3	10-2	10^1 IO0 IO1 IO2
Kernel Bandwidth γ
Figure 7: MLCE vs. kernel bandwidth for all methods on task 3 of Section 5.3, predicting hair type on CelebA.
LoRe achieves the best MLCE for all γ < 1 and is tied with histogram binning for γ > 1. Left: 2D t-SNE
features. Right: 50D PCA features.
0.2
Figure 6: MLCE vs. kernel bandwidth γ for all methods on task 2 of Section 5.3, predicting hair color on CelebA.
LoRe achieves the best MLCE for virtually all values of γ. Left: 2D t-SNE features. Right: 50D PCA features.
∙0∙8∙64
Iooo
山 ɔl
10~3	10-2	10^1 IO0 IO1 IO2
Kernel Bandwidth γ
In these figures, “Original” represents no recalibration, “TS” represents temperature scaling, “HB”
represents histogram binning, “IR” represents isotonic regression, “MMCE” represents direct MMCE
optimization, and “LoRe” is our method.
Next, we examine the influence of the specific feature map used. In Figures 10, 11, 12, and 13, we
plot the MLCE achieved by all recalibration methods for ImageNet using Inception-v3, AlexNet,
DenseNet121, and ResNet101 features. In Figures 14 and 15, we plot the MLCE achieved by all
recalibration methods for ImageNet when the features used to calculate the MLCE are different from
the features used by LoRe. For completeness, in Figures 16, 17, 18, and 19, we also visualize the
average LCE for all experimental settings. All plots show similar results: LoRe performs best over a
wide range of γ .
13
Under review as a conference paper at ICLR 2022
IO-3	Io-2	lθ-ɪ IO0	IO1	IO2
Kernel Bandwidth γ
Figure 8: MLCE vs. kernel bandwidth γ for all recali-
bration methods for CIFAR-100 (3D t-SNE features).
LoRe achieves lower MLCE for most γ.
---Original
——TS
——HB
——IR
——MMCE
8 6 4 2
Cicicici
IO-3	Io-2	io-ɪ IO0 ioɪ ιo2
Kernel Bandwidth γ
Figure 9: MLCE vs. kernel bandwidth γ for all recali-
bration methods for CIFAR-10 (3D t-SNE features).
LoRe achieves lower MLCE for most γ .

ι.o
0.8-
0.6-
0.4-
0.2-
00 10^3	IO-2	io-ɪ IO0 IO1 IO2
Kernel Bandwidth γ
IO-3	Io-2	lθ-ɪ IO0	IO1	IO2
Kernel Bandwidth γ
Figure 11: MLCE vs. kernel bandwidth γ for all recal-
ibration methods on ImageNet using AlexNet features.
LoRe achieves the best MLCE for most γ.
Figure 10: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using Inception-v3
features. LoRe achieves the best MLCE for most γ.
---Original
——TS
——HB
——IR
——MMCE
—LoRe
8 6 4
Cicici
山：TIiAl
---Original
——TS
——HB
——IR
——MMCE
—LoRe
Figure 12: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using DenseNet121
features. LoRe achieves the best MLCE for most γ.
Figure 13: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using ResNet101
features. LoRe achieves the best MLCE for most γ.
14
Under review as a conference paper at ICLR 2022
Figure 14: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using Inception-v3
features to calculate the MLCE and AlexNet features
for applying LoRe.
Figure 15: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using DenseNet121
features to calculate the MLCE and AlexNet features
for applying LoRe.
Figure 16: Average LCE vs. kernel bandwidth γ for
all recalibration methods on ImageNet (3D t-SNE
features). LoRe gets lower average LCE for most γ.
Figure 17: Average LCE vs. kernel bandwidth γ for
all recalibration methods in task 1 (crime data, 2D
t-SNE features). LoRe gets lower average LCE for
most γ.
Figure 18: Average LCE vs. kernel bandwidth γ for
all recalibration methods in task 2 (CelebA, 2D t-SNE
features). LoRe gets lower average LCE for most γ.
Figure 19: Average LCE vs. kernel bandwidth γ for
all recalibration methods in task 3 (CelebA, 2D t-SNE
features). LoRe gets lower average LCE for most γ.
15
Under review as a conference paper at ICLR 2022
C Proof of Lemma 1
We restate Lemma 1 below, and provide the proof:
Lemma 2. Assume that limγ→∞ kγ (x, x0) = 1 for all x, x0 ∈ X. Then, as γ → ∞, the MLCE
converges to the MCE.
Proof. Since limγ→∞ kγ(x, x0) = 1 identically,
1
lim maxLCEγ(x; f,p) = max
γ→∞ X	X ∣β(x)∣
E P(Xi)- 1 [f(Xi) = yi]
i∈β(x)
1
max
k |Bk |
E P(Xi)- 1 [f (χi) = yi]
i∈Bk
max |conf (Bk ) - acc(Bk )|
k
MCE(x; f,P)
□
D Formal Statement and Proof of Theorem 1
Let B1, . . . , BN denote a set of bins that partition [0, 1], and B(P) denote the bin that a particular
p ∈ [0,1] belongs to. Let af(x, y) = 1 [f (x) = y] indicate the accuracy of a the classifier (f,p) on
an input X. We consider the signed local calibration error (SLCE):
SLCE(Tr- E[(P(X) - af (X, Y ))kγ (X, X | P(X) ∈ B(P(X))]
Y( ;f，P) :=	E[kγ(X,χ) I P(X) ∈ B(P(X))]
=E[(P(X)- af (X,Y))kγ(X,x)1 [P(X) ∈ B(P(X))]]
E[kY (X,x)1 [P(X ) ∈ B(P(X))]]	.
D. 1 Assumptions and Formal Statement of Theorem
We make the following assumptions:
Assumption A (Lipschitz kernel). The kernel kγ takes the form
kγ(X,X0) = g( φ(X)1)),
where φ: X → Rd is a representation function, and g : Rd → [0, 1] is L-Lipschitz with respect to
some norm ∣∣∙k.
Note this definition may require an implicit rescaling (for example, We can take Φ(x) — φfeature (x) /d
for a d-dimensional feature map φfeature and take g(z) = exp(-∣z∣1), which corresponds to the
Laplacian kernel We used in Section 3.2).
Assumption B (Binning-aWare covering number). For any > 0, the range of the representation
function φ(X) := {Φ(x) : X ∈ X} has an e -cover in the ∣∣∙∣ -norm of size (C∕e)d for some absolute
constant C > 0: There exists a set N ∈ X with |N| ≤ (C/)d such that for any X ∈ X, there exists
some X0 ∈ N such that ∣∣≠(x) 一 Φ(x0)∣ ≤ e and B(P(X)) = B(P(X0)).
Assumption C (LoWer bound on expectation of kernel Within bin). We have
inf E[kγ(X, x)1 [P(X) ∈ B(P(X))]] ≥ α
X∈X
for some constant α ∈ (0, 1).
The constant α characterizes the hardness of estimating the SLCE from samples. Intuitively, With a
smaller α, the denominator in SLCE gets smaller and We desire a higher accuracy in estimating both
the numerator and the denominator. Also note that in practice the value of α typically depends on γ.
16
Under review as a conference paper at ICLR 2022
We analyze the following estimator of the SLCE using n samples:
-----
SLCEY (x; f,p)
1 Pn=I(P(Xi ) - af (Xi ,yi))kY (Xi, X)I AxQ ∈ B(P(X))]
1 Pi=I kY(Xi, X)I [p(Xi) ∈ B(P(X))]
(7)
Theorem 2. Under Assumptions A, B, and C, Suppose the sample size n ≥ O(d∕α4e2) where e > 0
is a target accuracy level, then with probability at least 1 - δ we have
sup ∣SLCEγ (x; f,p) — SLCEγ(x; f,p^) I ≤ e,
x∈X
where O hides log factors of the form log(L∕γeδα).
Theorem 2 shows that Oe(d∕e2α4) samples is sufficient to estimate the SLCE simultaneously for
all x ∈ X. When α = Ω(1), this sample complexity only depends polynomially in terms of the
representation dimension d and logarithmically in other constants (such as L, γ, and the failure
probability δ).
D.2 Proof of Theorem 2
P sup
x∈N
Step 1. We first study the estimation at finitely many X’s. Let N ⊆ X be a finite set of X’s with
|N| = N. Since kγ ∈ [0,1] and |p(x) — af (x, y)| ≤ 1 are bounded variables, by the Hoeffding
inequality and a union bound, we have
1n
n E(P(Xi) —af (Xi,yi))kγ(xi,x)1 [p(xi) ∈ B(P(X))]
n i=1
—E[(P(X) — af(X,Y))kγ(X,x)1 [P(X) ∈ B(P(X))]] > αe∕10
≤ exp(—cna2e2 + logN).
Therefore, as long as n ≥ O(log(N∕δ)∕e2α2 ) samples, the above probability is bounded by δ. In
other words, with probability at least 1 — δ, we have simultaneously
∣1n
-E(P(Xi) —af(Xi,yi))kγ(xi,x)1 [P(xi) ∈ B(P(X))]
i=1
'---------------------{z----------------------}
：=A(X)
—E[(P(X) — af(X,Y))kγ(X,x)1 [P(X) ∈ B(P(X))]]
、-----------------------{---------------------}
：=A(x)
≤ αe∕10.
for all X ∈ N. Similarly, when n ≥ O(log(N∕δ)∕e2α4), we also have (with probability at least
1 — δ)
1n	∣
一	kγ(xi,	x)1	[P(Xi)	∈ B(P(X))]	—	E[kγ(X, x)1	[P(X)	∈ B(P(X))]]	≤ α2e∕10
n i=1	'	{7"	}
{: = B(X)
：=B(X)
On these concentration events, we have for any X ∈ N that
SLCEY(x; f, P) — SLCEγ(x; f, P) I = I 黑—Bl)
≤ e.
1	1
B(X) - B(X)
— A(X)II
a2 e/10
α(α — α2e∕10)
+ ɪ ∙ αe∕10
≤
≤
1 •
17
Under review as a conference paper at ICLR 2022
Step 2. We now extend the bound to all x ∈ X using the covering argument. By Assumption B,
We can take an a2eγ∕(10L)-CoVering of φ(X) with cardinality N ≤ (10CL∕02eγ)d. Let N ⊂ X
denote the covering set (in the X space). This means that for any x ∈ X, there exists x0 ∈ N such
that kφ(x) - φ(x0)k ≤ α2eγ∕(10L) amd B(P(X)) = B(P(Xy), which implies that for any e ∈ X
we haVe
∣k(e,x) - k(x,χ0)∣ = f ( 9 - °(x)) - f (φ(e)-φ(χ')) J
≤ L kφ(x) - φ(x0)k
γ
≤ α2e∕10,
where we haVe used the Lipschitzness assumption of g (Assumption A). This further implies
JA(X) — A(x0) J =
1n
-E(P(Xi) -af(xi,yi))kγ(Xi,x)1 [P(xi) ∈ B(P(X))]
i=1
1n	J
-n E(P(Xi) - af (Xi,yi))kγ(Xi,x0)1 [P(Xi) ∈ B(P(X0))]
i=1
1n	J
E(P(Xi) - af (Xi,yi))[kγ(x^x) - kγ(x^x )]1 [pE) ∈ B(P(X))]
n i=1
1n
≤ — £ IP(χi) - af (χi,yi)l ∙ ∣kγ(χi,χ) - kγ(χi,χ0)∣∙ 1 [P(xi) ∈ B(P(X))]
n i=1
≤ α2e∕10.
Similarly, we have |A(x) — A(x0)∣ ≤ α2e∕10, |B(x) — B(x0)∣ ≤ α2e∕10, and |B(x) — B(x0)∣ ≤
α2e∕10. This means that the estimation error at X is close to that at X0 ∈ N and consequently also
bounded by e:
JSLCEY(x; f,P) - SLCEγ(x; f,P)J =
≤
A(X)	A(x) J
而-BXyJ
A(X)	A(x0) J	J A(x0)	A(x0)
B(X)	B(XO)J	J B(x0)	B(x0)
+ j A(χO) _ AHJ
+ J B(x0)	B(x) J
≤ 3 1∙
α2e∕10
α(α - α2e∕10)
+ ɪ ∙ α2e∕10
≤ e.
N≥O
d log 10CL∕α2eγ + log(1∕δ)
α4e2
O(d∕α4e2),
Therefore, taking this N in step 1, we know that as long as the sample size
iog(∣N∣∕δ) ʌ =O
e2α4	J =
we have with probability at least 1 - δ that
sup JSLCEγ(x; f,P) - SLCEγ(x; f,P)J ≤ e.
x∈X
This is the desired result.
□
18