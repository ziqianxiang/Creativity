Under review as a conference paper at ICLR 2022
Effective Uncertainty Estimation with Evi-
dential Models for Open-World Recognition
Anonymous authors
Paper under double-blind review
Ab stract
Reliable uncertainty estimation is crucial when deploying a classifier in the wild.
In this paper, we tackle the challenge of jointly quantifying in-distribution and
out-of-distribution (OOD) uncertainties. To this end, we leverage the second-
order uncertainty representation provided by evidential models and we introduce
KLoS, a KUllback-Leibler divergence criterion defined on the class-probability
simplex. By keeping the full distributional information, KLoS captures class con-
fUsion and lack of evidence in a single score. A crUcial property of KLoS is to
be a class-wise divergence measUre bUilt from in-distribUtion samples and to not
reqUire OOD training data, in contrast to cUrrent second-order Uncertainty mea-
sUres. We fUrther design an aUxiliary neUral network, KLoSNet, to learn a refined
criterion directly aligned with the evidential training objective. In the realistic
context where no OOD data is available dUring training, oUr experiments show
that KLoSNet oUtperforms first-order and second-order Uncertainty measUres to
simUltaneoUsly detect misclassifications and OOD samples. When training with
OOD samples, we also observe that existing measUres are brittle to the choice of
the OOD dataset, whereas KLoS remains more robUst.
1	Introduction
Safety is a major concern in visUal-recognition applications sUch as aUtonomoUs driving (McAl-
lister et al., 2017) and medical imaging (Heckerman et al., 1992). However, modern neUral net-
works (NNs) strUggle to detect their own misclassifications (Hendrycks & Gimpel, 2017). In addi-
tion, when exposed to oUt-of-distribUtion (OOD) samples, NNs have been shown to provide over-
confident predictions instead of abstaining (Hein et al., 2019; NgUyen et al., 2015). Obtaining reli-
able estimates of the predictive Uncertainty is thUs necessary to safely deploy models in open-world
conditions (Bendale & BoUlt, 2015).
Notable progress has been made in NN Uncertainty estimation with the renewal of Bayesian neU-
ral networks (Gal & Ghahramani, 2016; Maddox et al., 2019) and ensembling (Lakshminarayanan
et al., 2017; Ovadia et al., 2019b). These techniqUes prodUce a probability density over the predic-
tive categorical distribUtion obtained from sampling. A recent class of models, coined evidential
(Malinin & Gales, 2018; Sensoy et al., 2018), proposes instead to explicitly learn the concentration
parameters of a Dirichlet distribUtion over first-order probabilities. They have been shown to im-
prove generalisation (Joo et al., 2020), OOD detection (Nandy et al., 2020) and adversarial attack
detection (Malinin & Gales, 2019).
Based on the sUbjective logic framework (Josang, 2016), evidential models enrich Uncertainty rep-
resentation with evidence information and enable to represent different soUrces of Uncertainty. Con-
flicting evidence, e.g., class confUsion, is characterized by the expectation of the second-order
Dirichlet distribUtion while the distribUtion spread on the simplex expresses the amoUnt of evidence
in a prediction (Shi et al., 2020). These soUrces of Uncertainty are also known as data and model
Uncertainty in the machine learning literatUre (Malinin & Gales, 2019).
For open-world recognition, a model shoUld be eqUipped with an Uncertainty measUre that accoUnts
both for class confUsion and lack of evidence to detect misclassifications and OOD samples. The pre-
dictive entropy and the maximum class probability (MCP) targeting total Uncertainty actUally redUce
probability distribUtions on the simplex to their expected valUe and compUte first-order Uncertainty
measUres (Joo et al., 2020; Sensoy et al., 2018). This caUses a significant loss of information, as
1
Under review as a conference paper at ICLR 2022

Concentrations
Cat
(b) Outlier with same class confusion
MCP = 0.50 , entropy = 0.97, KLoS = 104.71
Figure 1: Limitations of first-order uncertainty measures and their handling with KLoS. (a) An
in-distribution image with conflicting evidence between dog and wolf. (b) An outlier with same class
confusion but a lower amount of evidence. An evidential neural network (ENN) outputs class-wise
evidence information as concentration parameters of a Dirichlet density (visualized on the simplex)
over 3-class distributions. Although this density is flatter for the second input, the predictive entropy
and MCP, only based on first-order statistics, are equal for both inputs. In contrast, the proposed
measure, KLoS, captures both class confusion and lack of evidence, hence correctly reflecting the
larger uncertainty for the latter sample.
(a) In-distribution image
MCP = 0.50 , entropy = 0.97, KLoS = 97.85
shown in Fig. 1: the resulting measures are invariant to the spread of the distribution, whereas un-
certainty caused by class confusion and lack of evidence should be cumulative, a property naturally
fulfilled by the predictive variance in Bayesian regression (Murphy, 2012).
Existing OOD detection methods with evidential models (Malinin
& Gales, 2018; 2019; Nandy et al., 2020) use second-order un-
certainty measures assuming that the Dirichlet distribution spread
is larger for OOD than for in-distribution (ID) samples, e.g., pre-
cision α0 or mutual information. They also rely on using auxil-
iary OOD data during training to enforce higher distribution spread
on OOD inputs. However, this assumption is not always fulfilled
in absence of OOD training data, as noted by Charpentier et al.
(2020); Sensoy et al. (2020). As shown in Fig. 2 for a model
trained on CIFAR-10, α0 values largely overlap between IDs and
OODs when no OOD training data is used, limiting the effective-
ness of existing second-order uncertainty measures. Consequently,
neither current first-order nor second-order uncertainty measures
appear to be suited for open-world settings.
Precision a0
Figure 2: Precision densities
for ID (CIFAR-10) and OOD
(TinyImageNet) samples when
no OOD training data is used.
Contributions. In this paper, we introduce KLoS, a measure that accounts for both in-distribution
and out-of-distribution sources of uncertainty and that is effective even without having access to
auxiliary OOD data at train time. KLoS computes the KUllbaCk-Leibler (KL) divergence between
model’s predicted Dirichlet distribution and a specifically designed class-wise prototype Dirichlet
distribution. By leveraging the second-order uncertainty representation that evidential models pro-
vide, KLoS captures both class confusion and lack of evidence in a single score. Prototype distri-
butions are designed with concentration parameters shared with in-distribution training data, which
enables to detect OOD samples without assuming any restrictive behavior, e.g., having low preci-
sion α0 . KLoS naturally reflects the training objective used in evidential models and we propose to
learn an auxiliary model, named KLoSNet, to regress the values of a refined objective for training
samples and to improve uncertainty estimation. To assess the quality of uncertainty estimates in
open-world recognition, we design the new task of simultaneous detection of misclassifications and
OOD samples. Extensive experiments show the benefits of KLoSNet on various image datasets and
model architectures. In presence of OOD training data, we also found that our proposed measure is
more robust to the choice of OOD samples while previous measures may perform poorly. Finally,
we show that KLoS can be successfully combined with ensembling to improve performance.
2	Capturing In-Distribution and OOD Uncertainties
Section 2.2 presents our measure to capture class confusion and lack of evidence with evidential
models. We propose a confidence learning approach to enhance in-distribution uncertainty estima-
tion in Section 2.3. Beforehand, we review evidential models and their learning in Section 2.1 to put
in perspective the benefits of the proposed approach.
2
Under review as a conference paper at ICLR 2022
2.1	Background: Evidential Neural Networks
Let us consider a training dataset D of N i.i.d. samples x with label y ∈ J1, CK drawn from an un-
known joint distribution p(x, y). Bayesian models and ensembling methods approximate the poste-
rior predictive distribution P(y = c∣x*, D) by marginalizing over the network's parameters thanks
to sampling. But this comes at the cost of multiple forward passes. Evidential Neural Networks
(ENNs) propose instead to model explicitly the posterior distribution over categorical probabilities
p(π |x, y) by a variational Dirichlet distribution,
qθ(∏∣x)= Dir(∏∣α(x, θ))=
C
「(劭(X, θ))	Y ∏αc(x,θ)-1
Qc=Ir(αc(x, θ)) U C
(1)
whose concentration parameters α(x, θ) = exp f(x, θ) are output by a network f with parameters
θ; Γ is the Gamma function and α0(x, θ) = PcC=1 αc(x, θ). Precision α0 controls the sharpness of
the density with more mass concentrating around the mean as α0 grows. By conjugate property, the
predictive distribution for a new point x* is P(y = c|x*, D) ≈ Eq®(∏∣χ*)[∏c] = PCeXPfc(X R g ,
k=1 exp k x ,
which is the usual output of a network f with softmax activation.
The concentration parameters α can be interpreted as pseudo-counts representing the amount of
evidence in each class. For instance, in Fig. 1a, the a's output by the ENN indicates that the image
is almost equally likely to be classified as wolf or as dog. More interestingly, it also distinguishes
this in-distribution images from the OOD sample in Fig. 1b via the total amount of evidence α0.
Training Objective. The ENN training is formulated as a variational approximation to minimize
the KL divergence between q®(π∣x) and the true posterior distribution p(π∣x, y):
Lvar(θ; D) = E(χ,y)〜p(χ,y) [KL(qθ(∏∣x) k p(∏∣χ, y))]	⑵
(X Nn X -(ψ(αy(x, θ))-ψ(αo(x, θ))) + KL(qe(π∣x) ∣∣ p(∏∣x)),	(3)
(x,y)∈D
where ψ is the digamma function. Following Joo et al. (2020), we use the non-informative uniform
prior p(π∣x) = Dir(π∣l), where 1 is the all-one vector, and weigh the KL divergence term with
λ>01:	1
Lvar(θ;D) = N X —(Ψ(αy) - ψ(αo)) + λKL(Dir(π∣α) ∣∣ Dir(π∣1)).	(4)
(x,y)∈D
In particular, minimizing loss Eq. (4) enforce training sample’s precision α0 to remain close to
C + λ-1 value (Malinin & Gales, 2019).
2.2	A Kullback-Leibler Divergence Measure on THE Simplex
By explicitly learning a distribution of the categorical probabilities π, evidential models provide a
second-order uncertainty representation where the expectation of the Dirichlet distribution relates
to class confusion and its spread to the amount of evidence. While originally used to measure the
total uncertainty, the predictive entropy H[y|x, θ] and the maximum class probability McP(x, θ) =
maxc P(y = c|x, θ) only account for the position on the simplex. These measures are invariant
to the dispersion of the Dirichlet distribution that generates the categorical probabilities. This can
be problematic, as illustrated in Fig. 1. To capture uncertainties due to class confusion and lack of
evidence, an effective measure should account for the sharpness of the Dirichlet distribution and its
location on the simplex.
We introduce a novel measure, named KLoS for “KL on Simplex”, that computes the KL divergence
between the model's output and a class-wise prototype Dirichlet distribution with concentrations Yy
focused on the predicted class y:
KLoS(x)，KL(Dir(π∣α(x,θ)) ∣∣ Dir(π∣γy)),	(5)
where α(x, θ) = exp f (x, θ) are model's output and Yy = (1,..., 1, τ, 1,..., 1) are the uniform
concentration parameters except for the predicted class with τ = 1+ λ-1.
1For conciseness, we denote αc = αc(x, θ), ∀c hereafter.
3
Under review as a conference paper at ICLR 2022
Figure 3: KLoS and KLoS* on the probability simplex. Given the input sample, the blue region
represents the distribution predicted by the evidential model and the orange region represents the
prototype DiriChlet distribution with parameters Yy = (1, ∙ ∙ ∙ , 1,τ, 1,..., 1) focused on the pre-
dicted class y. Illustration of the behavior of KLoS in absence of uncertainty (a), in case of class
confusion (b) and in case of a different amount of evidence, either lower (c) or higher (d).
The lower KLoS is, the more certain the prediction is. Correct predictions will have Dirich-
let distributions similar to the prototype Dirichlet distribution Yy and will thus be associated
with a low uncertainty score (Fig. 3a). Samples with high class confusion will present an ex-
pected probability distributions closer to simplex’s center than the expected class-wise prototype
Py = (K-1+τ,…，K-1+τ,…，K-1+τ), resulting in a higher KLoS score (Fig. 3b). Similarly,
KLoS also penalizes samples having a different precision α0 than the precision α0* = τ+C-1 of
the prototype γy. Samples with smaller (Fig. 3c) and higher (Fig. 3d) amount of evidence than α*
receive a larger KLoS score.
Effective measure without OOD training data. Since in-distribution samples are enforced to have
precision close to α0* during training, the class-wise prototypes are fine estimates of the concentra-
tion parameters of training data for each class. Hence, KLoS is a divergence-based metric, which
only needs in-distribution data during training to compute its prototypes. This behavior is illustrated
in Section 4.1. The proposed measure will be effective to detect various types of OOD samples
whose precision is far from α0*. In contrast, second-order uncertainty measures, e.g., mutual infor-
mation, assume that OOD samples have smaller α0 , a property difficult to fulfill for models trained
only with in-distribution samples (see Fig. 2). In Section 4.3, we explore more in-depth the impact
of the choice of OOD training data on the actual α0 values for OOD samples.
By approximating the digamma function ψ (see Appendix B), KLoS can also be decomposed as:
KLOS(X) ≈ -(T -I)Iog (Iy) + (-(T - 1)(2o^ - 2017) + KL(DiKnIa) k DiKnII))).⑹
The first term is the standard log-likelihood and relates only to expected probabilities, hence to the
class confusion. The ratio α^∕α0 makes it invariant to any scaling of the concentration parameters
vector α. The other terms take into account the spread of the distribution by measuring how close
α0 is to (T+C-1), and measure the amount of evidence.
2.3	Improving Uncertainty Estimation with Confidence Learning
When the model misclassifies an example, i.e., the predicted class y differs
from the ground truth y, KLoS measures the distance between the ENN’s
output and the wrongly estimated posterior p(π∣x, y). This may result in an
arbitrarily high confidence / low KL divergence value. Measuring instead the
distance to the true posterior distribution p(πIx, y) (Fig. 4) would more likely
yield a greater value, reflecting the fact that the classifier made an error. Thus,
a better measure for misclassification detection would be:
Figure 4: KLoS*
KLoS*(x,y)，KL(Dir(π∣α(x, θ)) ∣∣ Dir(π∣γy)),	(7)
where Yy corresponds to the uniform concentrations except for the true class y with T = 1 + λ-1 .
Connecting KLoS* with Evidential Training Objective. Choosing such value for T results in
KLoS* matching the objective function in Eq. (4). This means that KLoS* is explicitly minimized
by the evidential model during training for in-distribution samples. By mimicking the evidential
4
Under review as a conference paper at ICLR 2022
training objective, We reflect the fact that the model is confident about its prediction if KLoS * is close
to zero. In addition, minimizing the KL divergence between the variational distribution qθ(π∣x)
and the posterior p(π∣x, y) is equivalent to maximizing the evidence lower bound (ELBO). Hence,
a small KLoS* value corresponds to a high ELBO, which is coherent with the common assumption
in variational inference that higher ELBO corresponds to “better” models (Gal, 2016).
Obviously, the true class of an output is not available when estimating confidence on test samples.
We propose to learn KLoS* by introducing an auxiliary confidence neural network, KLoSNet, with
parameters ω, which outputs a confidence prediction C(x, ω). KLoSNet consists ofa small decoder,
composed of several dense layers attached to the penultimate layer of the original classification
network. During training, we seek ω such that C(x, ω) is close to KLoS*(x, y), by minimizing
LKLOSNet(ω;D) = NN X ∣∣c(x,ω) - KLoS*(x, y)∣∣2.	(8)
(x,y)∈D
KLoSNet can be further improved by endowing it with its own feature extractor. Initialized with
the encoder of the classification network, which must remain untouched for not affecting its perfor-
mance, the encoder of KLoSNet can be fine-tuned along with its regression head. This amounts to
minimizing Eq. (8) w.r.t. to both sets of parameters.
The training set for confidence learning is the one used for classification training. In the experiments,
we observe a slight performance drop when using a validation set instead. Indeed, when dealing
with models with high predictive performance and small validation sets, we end up with fewer
misclassification examples than in the train set. At test time, we now directly use KLoSNet’s scalar
output C(x, ω0 ) as our uncertainty estimate. As previously, the lower the output value, the more
confident the prediction.
3	Related Work
Misclassification Detection. Several works (Jiang et al., 2018; COrbiere et al., 2019; Moon et al.,
2020) aim to improve the standard MCP baseline (Hendrycks & Gimpel, 2017) in misclassification
detection with NNs. In particular, COrbiere et al. (2019) design an auxiliary NN to predict a con-
fidence criterion on training points. We adapt their high-level idea for evidential models to tackle
jointly the detection of misclassified in-distribution samples and of OOD samples. With evidential
models, Shi et al. (2020) use dissonance in their active learning framework. In contrast to other
second-uncertainty measures, dissonance relates to class confusion, but does not acknowledge for
the amount of evidence.
Out-of-Distribution Detection. Bayesian neural networks (Neal, 1996) and ensembling (Laksh-
minarayanan et al., 2017) offer a principled approach for uncertainty estimation in which a second-
order measure can be derived by measuring the dispersion between individual probabilities vectors.
But this comes at the expense ofan increased computational cost. Evidential models emulate an en-
semble of models using a single network, but usually require OOD samples during training (Malinin
& Gales, 2018; 2019; Nandy et al., 2020), which may be unrealistic in many applications. KLoS
alleviates this constraint and remains effective without OOD in training. Another range of methods
proposes to improve OOD detection on any pre-trained model. ODIN (Liang et al., 2018) mitigates
over-confidence by post-processing logits with temperature scaling and by adding inverse adver-
sarial perturbations. Lee et al. (2018) proposes a confidence score based on the class-conditional
Mahalanobis distance, with the assumption of tied covariance. Although effective, both approaches
need OOD data to tune hyperparameters, which might not generalize to other OOD datasets (Shafaei
et al., 2019). Finally, Liu et al. (2020) interpret a pre-trained NN as an energy-based model and
compute the energy score to detect OOD samples. Interestingly, this score corresponds to the log
precision log α0, which is similar to the EPKL measure Malinin & Gales (2019) used in ENNs.
4	Experiments
In this section, we assess our method against existing baseline uncertainty measures on synthetic data
and we conduct extensive experiments across various image datasets, architectures and settings.
Experimental Setup. Uncertainty measures are derived from an evidential model (Eq. (4)) with
λ = 0-2 as in (Joo et al., 2020; Malinin & Gales, 2019), except for second-order metrics where
5
Under review as a conference paper at ICLR 2022
(a) Toy dataset (b) Corr./Err. (c) Entropy (d) Mut. Inf. (e) Mahalanobis (f) KLoS
Figure 5: Comparison of various uncertainty measures for a given evidential classifier on a toy
dataset. (a) Training samples from 3 input Gaussian distributions with large overlap (hence class
confusion) and OOD test samples (blue); (b) Correct (yellow) and erroneous (red) class predictions
on in-domain test samples; (c-f) Visualisation of different uncertainty measures derived from the
evidential model trained on the toy dataset. Yellow (resp. purple) indicates high (resp. low) certainty.
we found that setting λ = 10-3 improves performance. We rely on the learned classifier to train our
auxiliary confidence model KLoSNet, using the same training set and following loss Eq. (8). All
details about architectures, training algorithms and datasets are available in Appendix C.
Baselines. We evaluate our approach against: first-order uncertainty metrics (Maximum Class prob-
ability (MCP) and predictive entropy (Entropy)), second-order metrics (mutual information (Mut.
Inf.) and dissonance), post-training methods for OOD detection (ODIN and Mahalanobis) and for
misclassification detection (ConfidNet). Except in Section 4.3, we consider setups where no OOD
data is available for training. Consequently, the results reported for ODIN and Mahalanobis are
obtained without adversarial perturbations, which is also the best configuration for the considered
tasks. We indeed show in Appendix D.6 that these perturbations degrade misclassification detection.
4.1	Synthetic Experiment
We analyse the behavior of the KLoS measure and the limitations of existing first- and second-order
uncertainty metrics on a 2D synthetic dataset composed of three Gaussian-distributed classes with
equidistant means and identical isotropic variance (Fig. 5). This constitutes a scenario with high
first-order uncertainty due to class overlap. OOD samples are drawn from a ring around the in-
distribution dataset and are only used for evaluation. Fig. 5c shows that Entropy correctly assigns
large uncertainty along decision boundaries, which is convenient to detect misclassifications, but
yields low uncertainty for points far from the distribution. Mut. Inf. (Fig. 5d) have the opposite
behavior than desired by decreasing when moving away from the training data. This is due to
the linear nature of the toy dataset where models assign higher concentration parameters far from
decision boundaries, hence smaller spread on the simplex, as also noted in (Charpentier et al., 2020).
Additionally, Mut. Inf. does not reflect the uncertainty caused by class confusion along decision
boundaries. Neither Entropy nor Mut. Inf. is suitable to detect OOD samples in this synthetic
experiment. In contrast, KLoS allows discriminating both misclassifications and OOD samples from
correct predictions as uncertainty increases far from in-distribution samples for each class (Fig. 5f).
KLoS measures a distance between the model’s output and a class-wise prototype distribution. Here,
we can observe that it acts as a divergence-based measure for each class.
We extend the comparison to include Mahalanobis (Fig. 5e), which is a distance-based measure by
assuming Gaussian class conditionals on latent representations, here in the input space. However,
Mahalanobis does not discriminate points close to the decision boundaries from points with a similar
distance to the origin. Hence, it may be less suited to detect misclassifications than KLoS. Addi-
tionally, KLoS does not assume Gaussian distributions in the latent space nor tied covariance, which
may be a strong assumption when dealing with high-dimension latent space. In Appendix D.1, a
complementary quantitative evaluation on this toy problem confirms our findings regarding the inad-
equacy of first-order uncertainty measures such as MCP and Entropy, and the improvement provided
by KLoS over Mahalanobis on misclassification detection.
4.2	Comparative Experiments
The task of detecting both in-distribution misclassifications and OOD samples gives the opportunity
to jointly evaluate in-distribution and out-of-distribution uncertainty representations of a method.
In this binary classification problem, correct predictions are considered as positive samples while
6
Under review as a conference paper at ICLR 2022
Table 1: Comparative experiments on CIFAR-10 and CIFAR-100. Misclassification (Mis.), out-
of-distribution (OOD) and simultaneous (Mis+OOD) detection results (mean % AUROC and std.
over 5 runs). Bold type indicates significantly best performance (p<0.05) according to paired t-test.
9'o°A
01-RAFIC
8'sNSox
01-RAFIC
9'o°A
001-RAFIC
8'sNSox
001-RAFI
		LSUN		TinyImageNet		STL-10	
Method	Mis.	OOD	Mis+OOD	OOD	Mis+OOD	OOD	Mis+OOD
MCP	87.6 ±1.6	79.7 ±1.1	84.9 ±1.1	80.3 ±1.5	85.2 ±1.5	60.3 ±1.2	75.2 ±1.4
Entropy	83.5 ±2.4	83.8 ±0.3	87.9 ±0.2	82.3 ±0.4	87.2 ±0.4	60.1 ±1.2	75.0 ±1.4
ConfidNet	90.2 ±0.8	82.1 ±1.5	87.6 ±1.1	83.5 ±0.6	88.3 ±0.7	61.5 ±1.6	77.2 ±1.1
Dissonance	91.9 ±0.2	84.8 ±0.3	90.1 ±0.1	84.2 ±0.2	89.7 ±0.1	64.1 ±0.1	79.6 ±0.1
Mut. Inf.	84.1 ±1.5	84.6 ±0.6	85.1 ±1.0	80.6 ±0.8	83.4 ±1.1	61.3 ±0.8	65.0 ±2.5
Diss.+Mut. Inf.	92.0 ±0.2	86.5 ±0.3	89.8 ±0.2	83.6 ±0.3	89.5 ±0.3	63.6 ±0.5	79.4 ±0.4
ODIN	86.0 ±2.0	79.5 ±1.2	83.8 ±1.5	79.6 ±1.9	84.0 ±2.0	54.7 ±1.5	65.0 ±2.6
Mahalanobis	91.2 ±0.3	88.9 ±0.2	91.3 ±0.1	86.4 ±0.2	90.2 ±0.1	63.4 ±0.2	78.8 ±0.3
KLoSNet (Ours)	92.5 ±0.6	87.6 ±0.9	91.7 ±0.9	86.6 ±0.9	91.2 ±0.8	67.7 ±1.4	81.8 ±0.9
MCP	84.9 ±0.8	79.6 ±1.0	83.0 ±0.9	77.2 ±0.7	81.8 ±0.7	58.5 ±1.2	72.5 ±0.4
Entropy	84.6 ±0.8	79.6 ±1.1	82.8 ±0.9	77.2 ±0.7	81.6 ±0.7	58.4 ±1.2	72.2 ±0.4
ConfidNet	90.7 ±0.4	84.6 ±1.1	88.6 ±0.6	83.5 ±1.1	88.0 ±0.6	63.2 ±1.2	77.9 ±0.5
Dissonance	92.9 ±0.4	90.3 ±0.4	92.7 ±0.4	87.7 ±0.3	91.4 ±0.3	67.3 ±0.5	81.2 ±0.4
Mut. Inf	80.6 ±0.6	77.0 ±1.2	79.4 ±0.9	74.3 ±0.8	78.0 ±0.7	56.4 ±1.0	69.1 ±0.2
Diss.+Mut. Inf.	92.4 ±0.5	86.7 ±1.0	90.1 ±0.8	84.3 ±0.5	88.8 ±0.6	65.2 ±0.7	80.3 ±0.4
ODIN	83.7 ±0.7	78.9 ±1.0	81.9 ±0.9	76.5 ±0.7	80.7 ±0.7	57.9 ±1.2	71.5 ±0.4
Mahalanobis	91.2 ±0.4	90.7 ±0.4	91.8 ±0.3	87.6 ±0.4	90.3 ±0.4	66.8 ±0.5	80.0 ±0.3
KLoSNet (Ours)	93.9 ±0.4	93.1 ±1.1	94.4 ±0.3	90.6 ±0.6	93.2 ±0.2	68.5 ±0.3	82.3 ±0.2
MCP	82.9 ±0.8	62.8 ±1.3	77.6 ±0.9	72.0 ±0.5	81.8 ±0.7	69.7 ±0.7	80.9 ±0.7
Entropy	82.2 ±0.8	63.2 ±1.4	77.2 ±1.0	72.5 ±0.6	81.5 ±0.8	70.1 ±0.8	80.6 ±0.7
ConfidNet	84.4 ±0.6	65.3 ±2.0	80.0 ±1.3	73.8 ±0.6	83.7 ±0.7	71.5 ±0.6	82.7 ±0.3
Dissonance	84.1 ±0.4	62.5 ±1.4	78.7 ±0.8	70.3 ±0.4	82.5 ±0.4	69.3 ±0.4	82.2 ±0.4
Mut. Inf.	78.9 ±0.8	65.6 ±0.7	76.2 ±0.9	71.8 ±0.2	79.1 ±0.4	70.1 ±0.6	78.5 ±0.6
Diss.+Mut. Inf.	84.2 ±0.6	65.1 ±0.3	80.1 ±0.4	70.1 ±0.3	82.5 ±0.5	69.5 ±0.3	82.3 ±0.5
ODIN	82.1 ±0.8	62.9 ±1.4	77.1 ±1.0	71.9 ±0.6	81.3 ±0.8	69.6 ±0.8	80.3 ±0.7
Mahalanobis	84.0 ±0.2	71.1 ±1.0	82.4 ±0.5	77.0 ±0.5	84.9 ±0.3	75.4 ±0.3	84.3 ±0.5
KLoSNet (Ours)	86.7 ±0.4	68.4 ±1.1	83.0 ±0.6	76.4 ±0.4	86.4 ±0.4	75.0 ±0.5	86.0 ±0.4
MCP	84.0 ±0.4	70.4 ±0.9	81.0 ±0.3	76.6 ±0.5	83.6 ±0.4	75.4 ±0.5	83.1 ±0.2
Entropy	83.7 ±0.4	70.4 ±0.9	80.8 ±0.3	76.9 ±0.5	83.5 ±0.3	75.7 ±0.5	83.0 ±0.3
ConfidNet	87.1 ±0.2	73.0 ±1.4	84.5 ±0.6	79.1 ±0.3	86.8 ±0.3	78.5 ±0.8	86.6 ±0.5
Dissonance	86.7 ±0.4	72.3 ±0.4	84.0 ±0.2	75.0 ±0.4	85.3 ±0.4	74.7 ±0.3	85.2 ±0.2
Mut. Inf	82.6 ±0.4	70.2 ±1.1	80.0 ±0.4	76.4 ±0.6	82.6 ±0.3	75.1 ±0.5	82.1 ±0.3
Diss.+Mut. Inf.	86.5 ±0.4	71.8 ±0.8	83.6 ±0.5	76.1 ±0.3	84.7 ±0.4	75.2 ±0.5	84.6 ±0.3
ODIN	83.7 ±0.4	70.3 ±0.9	80.8 ±0.3	76.6 ±0.5	83.5 ±0.3	75.4 ±0.5	83.0 ±0.3
Mahalanobis	85.9 ±0.4	75.2 ±0.6	84.5 ±0.1	78.4 ±0.5	85.9 ±0.3	77.5 ±0.4	85.6 ±0.3
KLoSNet (Ours)	86.9 ±0.3	73.1 ±0.4	84.4 ±0.1	80.8 ±0.2	87.3 ±0.2	79.0 ±0.2	86.7 ±0.3
misclassified inputs and OOD examples constitute negative samples. Following standard practices
(Hendrycks & Gimpel, 2017), we use the area under the ROC curve (AUROC) to evaluate threshold-
independent performance. Results for other relevant metrics are available in Appendix D.
The models used in the experiments present high predictive performances. Most often, there are
much fewer misclassifications in the test set than considered OOD samples. Hence, joint detection
performances might be dominated by the evaluation of the quality of OOD detection. To mitigate
this unbalance, we propose to consider the following scheme based on oversampling. Let AM be the
subset of in-distribution test examples that are misclassified by the observed model and AO the set of
OOD test samples. We randomly sample κ∣Ao | points in AM, with K = 1. Supposing |Ao | ≥ |Am |,
this corresponds to oversampling the set of misclassifications. This over-sampled set is then added
to the OOD set to form the negative examples for detection training. The set of correct predictions
remains the same. We observed that the variance in AUROC due to this sampling is negligible and
we report only the mean hereafter.
Experiments are conducted with VGG-16 (Simonyan & Zisserman, 2015) and ResNet-18 (He et al.,
2016) architectures on CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). The OOD datasets
used for evaluation are LSUN (Yu et al., 2015), TinyImageNet and STL-10 (Coates et al., 2011).
Along with simultaneous detection results, we provide separate results for misclassifications detec-
tion and OOD detection respectively in Table 1. On OOD detection, Mahalanobis and KLoSNet
outperform other methods, including second-order measures. ODIN also fails to deliver here as
7
Under review as a conference paper at ICLR 2022
Table 2: Impact of confidence learning. Comparison of detection performances between KLoS
and KLoSNet for CIFAR-10 and CIFAR-100 experiments with VGG-16 architecture.
Method		Mis.	LSUN		TinyImageNet		STL-10	
			OOD	Mis+OOD	OOD	Mis+OOD	OOD	Mis+OOD
CIFAR-10	KLoS	92.1 ±0.3	86.5 ±0.3	91.2 ±0.2	85.4 ±0.3	90.4 ±0.2	64.1 ±0.3	79.6 ±0.3
VGG-16	KLoSNet	92.5 ±0.6	87.6 ±0.9	91.7 ±0.9	86.6 ±0.9	91.2 ±0.8	67.7 ±1.4	81.8 ±0.9
CIFAR-100	KLoS	85.4 ±0.2	65.1 ±1.1	81.3 ±0.6	74.5 ±0.4	85.4 ±0.4	72.7 ±0.3	84.8 ±0.4
VGG-16	KLoSNet	86.7 ±0.4	68.4 ±1.1	83.0 ±0.6	76.4 ±0.4	86.4 ±0.4	75.0 ±0.5	86.0 ±0.4
(a) SVHN
Precision σ0
Figure 6: Effect of OOD training data on precision α0. Den-
sity plots for CIFAR-10/TinyImageNet benchmark: (a,b) with in-
appropriate OOD samples (SVHN, LSUN); (c) with close OOD
samples (CIFAR-100).
Oversampling Factor
Figure 7: Impact of the over-
sampling factor κ (CIFAR-
10/TinyImageNet).
logits are small due to regularization in the evidential training objective. Mut. Inf. and other spread-
based second-order uncertainty measures (see Appendix D.2) fall short to detect correctly OOD.
Indeed, for settings where OOD training data is not available, there is no guarantee that every OOD
sample will result in lower predicted concentration parameters as previously shown by the density
plot of precision α0 in Fig. 2. This stresses the importance of class-wise divergence-based measure.
While Mahalanobis may sometimes be slightly better than KLoSNet for OOD detection, it performs
significantly less well in misclassification detection, in line with the behavior shown in synthetic
experiments. As a result, KLoSNet appears to be the best measure in every simultaneous detection
benchmark. For instance, for CIFAR-10/STL-10 with VGG-16, KLoSNet achieves 81.8% AUROC
while the second best, Mahalanobis, scores 78.8%. We also observe that KLoSNet improves sig-
nificantly misclassification detection, even compared to dedicated methods such as ConfidNet or
second-order measures related to class confusion, e.g., dissonance. Another baseline could be to
combine two measures specialized respectively for class confusion and lack of evidence, such as
Dissonance+Mut.Inf. But it still performs less well than KLoSNet. In Appendix E, we evaluate
our approach on the task of selective classification when inputs are subject to corruptions (which
can be seen as OOD samples close to the input distribution, hence particularly challenging), and we
observe similar results.
Impact of Confidence Learning. To evaluate the effect of the uncertainty measure KLoS and
of the auxiliary confidence network KLoSNet, we report a detailed ablation study in Table 2. We
can notice that KLoSNet improves misclassification over KLoS but also OOD detection in every
benchmark. We intuit that learning to improve misclassification detection also helps to spot some
OOD inputs that share similar characteristics.
Oversampling Factor. When deploying a model in the wild, it is difficult to know beforehand the
proportions of misclassifications and OOD samples it will have to handle. We vary the oversampling
factor κ in [0.01; 100] for CIFAR10/TinyImageNet in Fig. 7 to assess the robustness of our approach.
KLoSNet consistently outperforms all other measures, with a larger gain when κ increases.
Combining KLoS with Ensembling. Aggregating predictions from an ensemble of neural net-
works not only improves generalization (Lakshminarayanan et al., 2017; Rame & Cord, 2021) but
also uncertainty estimation (Ovadia et al., 2019a). We train an ensemble of ten evidential models
on CIFAR-10 and evaluate the performance of KLoS obtained from averaged concentration param-
eters. On CIFAR-10/TinyImageNet benchmark, misclassification and OOD detection performances
are improved by respectively +1.8 points and +1.9 points, resulting in a +1.1 points gain on the
joint detection task.
8
Under review as a conference paper at ICLR 2022
CIFAR-100 STL-IO LSUN SVHN None
CIFAR-100 STL-IO LSUN SVHN None
CIFAR-100 STL-IO LSUN SVHN None
(a)	Mis. detection
(b)	OOD detection
(c)	Mis.+OOD detection
Figure 8: Comparative detection results with different OOD training datasets. While using
OOD samples in training improves performance in general, the gain value varies widely, sometimes
even being negative for inappropriate OOD samples e.g., SVHN. KLoS remains the best measure in
every setting. Experiment with VGG-16 architecture on CIFAR-10 dataset.
4.3 Effect of Training with Out-of-Distribution Samples
Previous results demonstrate that simultaneous detection of misclassifications and OOD samples
can be significantly improved by KLoSNet. We now investigate settings where OOD samples are
available. We train an evidential model to minimize the reverse KL-divergence (Malinin & Gales,
2019) between the model output and a sharp Dirichlet distribution focused on the predictive class for
in-distribution samples, and between the model output and a uniform Dirichlet distribution for OOD
samples. This loss induces low concentration parameters for OOD data and improves second-order
uncertainty measures such as Mut. Inf
The literature on evidential models only deals with an OOD training set somewhat related to the in-
distribution dataset, e.g. CIFAR-100 for models trained on CIFAR-10. Despite semantic differences,
CIFAR-10 and CIFAR-100 images were collected the same way, which might explain the general-
isation to other OOD samples in evaluation. Contrarily, CIFAR-10 objects and SVHN street-view
numbers differ more for instance. In Fig. 8, we vary the OOD training set and compare the uncer-
tainty metrics taken from the resulting models. As expected, using CIFAR-100 as training OOD data
improves performance for every measure (MCP, Mut. Inf. and KLoS). However, the boost provided
by training with OOD samples depends highly on the chosen dataset: The performance of Mut. Inf.
decreases from 92.6% AUC with CIFAR-100 to 82.9% when switching to LSUN, and even becomes
worse with SVHN (78.5%) compared to using no OOD data (80.6%). Indeed, Fig. 6 shows that only
the CIFAR-100 dataset seems to be effective to enforce low α0 on unseen OOD samples.
We also note that KLoS outperforms or is on par with MCP and Mut. Inf. in every setting. These
results confirm the adequateness of KLoS for simultaneous detection and extend our findings to
settings where OOD data is available at train time. Most importantly, using KLoS on models with-
out OOD training data yields better detection performance than other measures taken from models
trained with inappropriate OOD samples, here being every OOD dataset other than CIFAR-100.
5	Discussion
We propose KLoSNet, an auxiliary model to estimate the uncertainty of a classifier for both in-
domain and out-of-domain inputs. Based on evidential models, KLoSNet is trained to predict the
KLoS* value of a prediction. By design, KLoSNet encompasses both class confusion and evidence
information, which is necessary for open-world recognition. Our experiments extensively demon-
strate its effectiveness across various architectures, datasets and configurations, and reveal its class-
wise divergence-based behavior. Many real-world applications, e.g., based on active learning or
domain adaptation, necessitate to correctly detect both sources of uncertainty. It will be interesting
to apply KLoSNet in these contexts. We also show that, far from being the panacea, using training
OOD samples depends critically on the choice of these samples for existing uncertainty measures.
KLoS, on the other hand, is more robust to this choice and can alleviate their use altogether. While
finding suitable OOD samples may be easy for some academic datasets, it may turn more problem-
atic in real-world applications, with the risk of degrading performance with an inappropriate choice.
How to build suitable OOD training sets is an important open problem to attack.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
To better understand the link between the evidential training objective and KLoS* criterion stated
in Appendix A.2, we provide a detailed derivation in Appendix A. We also elaborate on the the-
oretical decomposition of KLoS (Eq. (6)) in Appendix B. Regarding experiments reproducibility,
Appendix C describes extensively how we generate data in the synthetic experiment (Section 4.1),
the image classification datasets used to evaluate our approach including in-distribution and OOD
datasets, the implementation and the hyperparameters of the experiments shown in Section 4.2. In
particular, we took care in our experiments to perform multiple runs and present mean and std re-
sults. Giving access to the code has yet to be authorized internally, but we plan to release the code
in the future.
References
Abhijit Bendale and Terrance Boult. Towards open world recognition. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2015. 1
Bertrand Charpentier, Daniel Zugner, and Stephan Gunnemann. Posterior network: Uncertainty esti-
mation without ood samples via density-based pseudo-counts. In Advances in Neural Information
Processing Systems, 2020. 2, 6, 16
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the International Conference on Artificial Intelligence and
Statistics, 2011. 7
Charles Corbiere, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Perez. Addressing
failure prediction by learning model confidence. In Advances in Neural Information Processing
Systems, 2019. 5
Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of
Machine Learning Research, 2010. 21
Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016. 5
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model un-
certainty in deep learning. In Proceedings of the International Conference on Machine Learning,
2016. 1
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2016. 7
David Heckerman, Eric Horvitz, and Bharat Nathwani. Toward normative expert systems: Part
i the pathfinder project. Methods of information in medicine, 31, 07 1992. doi: 10.1055/
s-0038-1634867. 1
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence predictions far away from the training data and how to mitigate the problem. In
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 1
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In Proceedings of International Conference on Learning Representa-
tions, 2019. 21
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In Proceedings of International Conference on Learning Represen-
tations, 2017. 1, 5,7
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classifier. In
Advances in Neural Information Processing Systems, 2018. 5
Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being bayesian about categorical probability. In
Proceedings of the International Conference on Machine Learning, 2020. 1, 3, 5, 13
10
Under review as a conference paper at ICLR 2022
Audun Josang. Subjective Logic: A Formalism for Reasoning Under Uncertainty. Springer, 2016. 1
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-
subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, Jure Leskovec,
Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A bench-
mark of in-the-wild distribution shifts. arxiv, 2020. 21
A Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department
of Computer Science, University of Toronto, 2009. 7, 16
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, 2017. 1, 5, 8
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-
ing Systems, 2018. 5, 19
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detec-
tion in neural networks. In Proceedings of International Conference on Learning Representations,
2018. 5, 19
Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detec-
tion. In Advances in Neural Information Processing Systems, 2020. 5
Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson.
A simple baseline for Bayesian uncertainty in deep learning. In Advances in Neural Information
Processing Systems 32, 2019. 1
Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In Advances
in Neural Information Processing Systems, 2018. 1, 2, 5
Andrey Malinin and Mark Gales. Reverse kl-divergence training of prior networks: Improved uncer-
tainty and adversarial robustness. In Advances in Neural Information Processing Systems, 2019.
1, 2, 3, 5, 9, 16
Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, Amar Shah, Roberto Cipolla, and
Adrian Weller. Concrete problems for autonomous vehicle safety: Advantages of Bayesian deep
learning. In Proceedings of the International Joint Conference on Artificial Intelligence, 2017. 1
Jooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang. Confidence-aware learning
for deep neural networks. In Proceedings of the International Conference on Machine Learning,
2020. 5
Kevin P. Murphy. Machine learning : A Probabilistic Perspective. MIT Press, 2012. 2
Jay Nandy, Wynne Hsu, and Mong-Li Lee. Towards maximizing the representation gap between in-
domain and out-of-distribution examples. In Advances in Neural Information Processing Systems,
2020. 1,2,5, 16
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, 1996. 5
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011. 16, 18
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2015. 1
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating
predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems,
2019a. 8
11
Under review as a conference paper at ICLR 2022
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating
predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems,
2019b. 1
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems, 2019. 16
Alexandre Rame and Matthieu Cord. Dice: Diversity in deep ensembles via conditional redundancy
adversarial estimation. In International Conference on Learning Representations, 2021. 8
Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshmi-
narayanan. A simple fix to mahalanobis distance for improving near-ood detection, 2021. 19
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classifica-
tion uncertainty. In Advances in Neural Information Processing Systems, 2018. 1
Murat Sensoy, Lance Kaplan, Federico Cerutti, and Maryam Saleki. Uncertainty-aware deep classi-
fiers using generative models. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial
Intelligence, 2020. 2
Alireza Shafaei, Mark Schmidt, and James J. Little. A less biased evaluation of out-of-distribution
sample detectors. In BMVC, 2019. 5
W. Shi, Xujiang Zhao, Feng Chen, and Qi Yu. Multifaceted uncertainty estimation for label-efficient
deep learning. In Advances in Neural Information Processing Systems, 2020. 1, 5
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proceedings of the International Conference on Learning Representations, 2015.
7, 16
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction
of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015. 7, 16
12
Under review as a conference paper at ICLR 2022
A Link between KLoS* and Evidential Training Objective
In this section, we detail the connection between KLoS* and the evidential training objective. For
conciseness, we denote αc = αc(x, θ), ∀c ∈ J0, CK hereafter.
A.1 Deriving the Evidential Training Objective
We first show the equivalence between Eq. (2) and Eq. (4). Starting from Eq. (2),
Lvar(θ; D)= E(x,y)〜p(x,y) [KL(qθ (∏∣x) k P(∏∣X, y))]	(9)
=17 X h [ qθ(∏∣χ)log qθ(n|x'i	(10)
N	L /	p(π∣x, y) -I
(x,y)∈D	,
=事(χ%h∕qθ (∏∣χ)iog Py≡≡⅛ i	(11)
=N X ∣Eqθ(∏∣x)[- logp(y∣∏,χ)] + KL(qθ(∏∣χ) k p(∏∣χ)) + logp(y∣χ)],
(x,y)∈D
(12)
where N = card(D). As the log-likelihood log p(y|x) does not depend on parameters θ,
min Lvar(θ; D) = min N X	忸q0(∏∣χ) [ — logp(y∣∏, x)] + KL(qe(∏∣x) k p(∏∣x))]. (13)
(x,y)∈D
For a sample (x,y), the reverse cross-entropy term amounts to En〜q®(∏∣χ)[- logp(y∣∏, x)] =
- ψ(αy) -ψ(α0) where ψ is the digamma function. Hence, we recover Eq.(3) of the main paper:
minLvar(θ;D) = N X -Ψ(αy)-ψ(α0)) + KL(qθ(π∣x) k p(∏∣x)).	(14)
(x,y)∈D
Considering that most of the training inputs x are associated with only one observation y in D, we
should choose small concentrations parameters β for the prior p(π∣x) = Dir(π∣β) to prevent the
resulting posterior distribution p(π∣x) = Dir(π∣βι,..., βy + 1,..., βc) from being dominated by
the prior. However, this causes exploding gradients in the small-value regimes due to the digamma
function, e.g. ψ0(0.01) > 10-4.
To stabilize the optimization, we follow Joo et al. (2020) and use the non-informative uniform prior
distribution p(π∣x) = Dir(π∣l) where 1 is the all-one vector, and We weight the KL-divergence
term with λ > 0:
Lλar(θ; D) = N X -(ψ(αy) - ψ(α0)) + λKL(Dir(π∣α(x, θ)) k Dir(π∣1)). (15)
(x,y)∈D
While Lvλar(θ; D) slightly differs from Lvar(θ; D), both functions lead to the same optima. Indeed,
by considering their gradient, we can show that a local optimum of Lvar(θ; D) is achieved for a
sample X when α* = (βι,…,βy + 1,…,βc) and a local optimum of Lλar(θ; D) is α∙ = (1,…,1 +
1∕λ,…，1). Hence, their respective ratio between each element is equal:
∀i,j ∈ J1,cκg = α∙.	(16)
αj αj
A.2 Link with KLoS *
Let us remind the definition of KLoS* as a KL divergence between the model’s output and a class-
wise prototype Dirichlet distribution with concentrations Yy focused on the true class y:
KLoS*(x,y)，KL(Dir(π∣α(x, θ)) k Dir(π%)),	(17)
13
Under review as a conference paper at ICLR 2022
where α(x, θ) = exp f(x, θ) is the model’s output and γy = (1, . . . , 1, τ, 1, . . . , 1) the vector of
uniform concentration parameters except for the true class with T =1∕λ + 1.
The KL divergence between two Dirichlet distributions can be obtained in closed form and KLoS*
can be calculated as:
KLoS*(x,y) = KL(Dir(π∣α) ∣∣ Dir(π∣γy))	(18)
C
=logΓ(α0)-logΓ(C-1+1∕λ)+logΓ(1+1∕λ)-XlogΓ(αc)
c=1
+ ^X (αc - 1)(ψ(αc) - ψ(α0)) + (αy - (I + 1"A(ψ9y) - ψ(α0)) (19)
On the other hand, the KL-divergence between the model’s output and an uniform Dirichlet distri-
bution Dir π |1 reads:
CC
KL(Dir(π∣α) ∣∣ Dir(π∣l)) = logΓ(α°)-logΓ(C)—XlogΓ(αJ+X (αc-l)(ψ(αc)-ψ(ao)).
c=1	c=1
(20)
Hence, KLoS* can be written as:
KLoS*(x,y) = —1 (ψ(αy) — ψ(α°)) + KL(Dir(π∣α) ∣∣ Dir(π∣1))
λ
+ (logΓ(1 + 1∕λ) — logΓ(C — 1 + 1∕λ) — logΓ(C)).	(21)
Let Us decompose £击(。;D)= 得 P(x y)∈D lVλar(x,y, θ). We retrieve that KLoS"(x) a
lλar(x, y, θ) + r where r = — (logΓ(1 + 1∕λ) — logΓ(C — 1 + 1∕λ) — logΓ(C)) does not depend
on the model parameters θ.
In summary, minimizing the evidential training objective Lvλar(θ; D) is equivalent to minimizing the
KLoS" value of each training point x.
B Decomposition of KLoS
This section provides details of the derivation of Eq. (6). Let us remind the definition of KLoS as
a KL divergence between the model’s output and a class-wise prototype Dirichlet distribution with
concentrations Yy focused on the predicted class y:
KLoS(x)，KL(Dir(π∣α(x, θ)) ∣∣ Dir(π∣γy)),	(22)
where α(x, θ) = exp f (x, θ) is the model,s output and Yy = (1,..., τ,..., 1) the vector of uniform
concentration parameters except for the predicted class with τ > 1. For conciseness, we denote
αc = αc(x, θ), ∀c ∈ J0, CK hereafter.
Similar to the derivation done in Appendix A.2, KLoS can be derived as:
KLoS(x) = — (τ — 1)(ψ(αy) — ψ(αo)) + KL(Dir(π∣α) ∣∣ Dir(π∣1)),	(23)
where 1 is the all-one vector.
By considering the asymptotic series approximation to the digamma function, ψ(x) = log X —贵 +
O( X12), the previous expression can be approximated by:
KLoS(x) ≈ —(τ — 1)log(V)+ ( — (τ — 1)(2^ — 2^Γ))+ KL(Dir(∏∣α) ∣∣ Dir(π∣1))) (24)
To gain further intuition about the decomposition, we provide
illustrations of the first term (negative log-likelihood, NLL)
and the second term in Fig. 9, similar to those shown in Fig. 3
of the paper. We also present quantitative results in Table 3.
We observe that the NLL term, which is equivalent to MCP
14
Method	Mis.(↑)	OOD (↑)
NLL term	80.2 ±1.1	15.9 ±0.7
Second term	48.2 ±4.3	98.4 ±0.1
KLoS	79.4 ±1.2	98.8 ±0.3
Table 3: Results of KLoS decompo-
sition on synth. data.
Under review as a conference paper at ICLR 2022
measure, helps to detect misclassifications while the second
term denotes increasing uncertainty as we move either far
away from training data. Hence, by using either the NLL term
or the second term, one could distinguish the source of uncertainty if needed.
In particular, we can show that the second term reaches its minimum for α0 = τ + C - 1 thanks to
the following lemma.
(a) KLoS
(b) NLL term
(c) Second term
Figure 9:	Visualisation of the different terms of KLoS, derived from the evidential model trained on
the same toy dataset than in the main paper.
Lemma B.1. A local minimum of g : a0 → -(T 一 1)(六 一 ^^)) + KL(Dir(π∣α) ∣∣ Dir(π∣1))
is reached for α0 = τ + C - 1.
Proof. Let Us denote μ ∈ RC such as ∀c, a。= μc ∙ α0. By the definition of the KL divergence
between two Dirichlet distributions, g reads:
C
C
g ： ao → 一(T -1)
1 一 μy 1
2μy	a0
+ logΓ(ao) -E logΓ(μcao) + E(μcao - 1)(ψ(μca°) - ψ(a°)).
c=1
c=1
By taking the derivative:
(25)
d ,	、	,	、1 一 μy 1
而g(a0)=(T- 1)^μ- a2
CC
+ ψ(ao) - J^μcψ(μca0) + £小。(ψ(μca°) - ψ(a0))
C
+ E(μca0 - 1)(μcψ(1)(μcao) - ψ(1)(ao)),
c=1
(26)
where ψ(1)(x) = 今ψ(x) is the trigamma function. As PC=I μc = 1, the previous equation
simplifies to:
-7d-g(α0) = (T - 1)1 - μy = + X(μca0 - 1)(μcψ(1)(μca0)-砂⑴⑺。)).	(27)
da0	2μy α0
We consider the asymptotic series approximation to the trigamma function ψ(1)(χ) = ɪ + + +
O(表).Given this approximation, the derivative reads:
白 gm。)≈(T- 1)11μμy∙ ɪ + X(μcα0- 1)(μc(μc1a0+ 2μ∣a2 - (2 - ⅛^)
(28)
(29)
C
(T- 1)⅛f ⅛+X葭- 1)⅛μc a
The derivative vanishes when ∀c = y, μcα0 = 1 and μya° = T, hence a local minimum of g is
reached for a0 = T + C - 1.	□
15
Under review as a conference paper at ICLR 2022
C Experimental Setup
In this section, we provide comprehensive details about the datasets, the implementation and the
hyperparameters of the experiments shown in Section 4 of the main paper.
Synthetic Data. The training dataset (Fig. 3 of the paper) consists of 1,000 samples (x, y) from a
distribution p(x, y) over R2 × {1, 2, 3} defined as:
P(X = χ, y = y) = 3 N (X = χ∣μy ,σ2i2×2),	(30)
where μι = (0, √3∕2), μ2 = (-1, -√3∕2), μ3 = (1, -√3∕2) and σ = 4. The marginal distribu-
tion of X is a Gaussian mixture with three equally weighted components having equidistant centers
and equal spherical covariance matrices. The test dataset consists of 1,000 other samples from this
distribution. Finally, we construct an out-of-distribution (OOD) dataset following Malinin & Gales
(2019), by sampling 100 points in R2 such that they form a ‘ring’ with large noise around the train-
ing points. Some OOD samples will be close to the in-distribution while others will be very far (see
Fig. 3 of the paper). The number of OOD samples has been chosen so that it amounts approximately
to the number of test points misclassified by the classifier. The classification is performed by a
simple logistic regression.
A set of five models is trained for 200 epochs using the evidential training objective (Eq. 6 of the
paper) with regularization parameter λ = 5e-2 and Adam optimizer with learning rate 0.02. Un-
certainty metrics - MCP, Entropy, Mut. Inf., Malahanobis and KLoS - are computed from these
models.
Image Classification Datasets. In Sections 4.2 and 4.3 of the main paper, the experiments are
conducted using CIFAR-10 and CIFAR-100 datasets Krizhevsky (2009). They consist in32×32
natural images featuring 10 object classes for CIFAR-10 and 100 classes for CIFAR-100. Both
datasets are composed with 50,000 training samples and 10,000 test samples. We further randomly
split the training set to create a validation set of 10,000 images.
OOD datasets are TinyImageNet2 - a subset of ImageNet (10,000 test images with 200 classes) -,
LSUN Yu et al. (2015) - a scene classification dataset (10,000 test images of 10 scenes) -, STL-10
- a dataset similar to CIFAR-10 but with different classes, and SVHN Netzer et al. (2011) - an RGB
dataset of 28 × 28 house-number images (73,257 training and 26,032 test images with 10 digits) -.
We downsample each image of TinyImageNet, LSUN and STL-10 to size 32 ×32.
Training Details. We implemented in PyTorch Paszke et al. (2019) a VGG-16 architecture Si-
monyan & Zisserman (2015) in line with the previous works of Charpentier et al. (2020); Malinin
& Gales (2019); Nandy et al. (2020), with fully-connected layers reduced to 512 units. Models are
trained for 200 epochs with a batch size of 128 images, using a stochastic gradient descent with Nes-
terov momentum of 0.9 and weight decay 5e-4. The learning rate is initialized at 0.1 and reduced
by a factor of 10 at 50% and 75% of the training progress. Images are randomly horizontally flipped
and shifted by ±4 pixels as a form of data augmentation.
Balancing Misclassification and OOD Detection. Most neural
networks used in our experiments tend to overfit, which leaves very
few training errors available. We provide accuracies on training, val-
idation and test sets in Table 4. With such high predictive perfor-
mances, the number of misclassifications is usually lower than the
number of OOD samples (〜10,000). Hence, the oversampling ap-
proach proposed in the paper helps to better balance misclassifica-
tion detection performances and OOD detection performances in the
reported metrics.
	CIFAR-10	CIFAR-100
Train	99.0 ±0.1	91.2 ±0.2
Val	93.6 ±0.1	70.6 ±0.3
Test	93.0 ±0.3	70.1 ±0.4
Table 4: Mean accuracies
(%) and std. over five runs.
KLoSNet. We start from the pre-trained evidential model described above. As detailed in Section
3.2 of the main paper, KLoSNet consists of a small decoder attached to the penultimate layer of
2https://tiny-imagenet.herokuapp.com/
16
Under review as a conference paper at ICLR 2022
Table 6: Other baselines results on comparative experiments on CIFAR-10 and CIFAR-100.
Bold type indicates significantly best performance (p<0.05) according to paired t-test.
Method		Mis.	LSUN		TinyImageNet		STL-10	
			OOD	Mis+OOD	OOD	Mis+OOD	OOD	Mis+OOD
CIFAR-10 VGG-16	Diff. Ent.	86.8 ±1.0	85.6 ±0.5	87.2 ±0.7	82.7 ±0.7	85.8 ±0.8	62.0 ±1.0	75.4 ±1.3
	EPKL	83.9 ±1.5	84.5 ±0.7	85.1 ±1.0	80.4 ±0.8	83.2 ±1.2	61.3 ±0.8	73.8 ±1.1
	KLoSNet (Ours)	92.5 ±0.6	87.6 ±0.9	91.7 ±0.9	86.6 ±0.9	91.2 ±0.8	67.7 ±1.4	81.8 ±0.9
CIFAR-10 ResNet-18	Diff. Ent	82.7 ±0.6	78.3 ±1.2	81.1 ±0.9	75.9 ±0.8	79.9 ±0.7	57.5 ±1.1	70.8 ±0.3
	EPKL	80.2 ±0.6	76.8 ±1.3	79.0 ±0.9	74.1 ±0.8	77.7 ±0.7	56.2 ±1.0	68.9 ±0.3
	KLoSNet (Ours)	93.9 ±0.4	93.1 ±1.1	94.4 ±0.3	90.6 ±0.6	93.2 ±0.2	68.5 ±0.3	82.3 ±0.2
CIFAR-100 VGG-16	Diff. Ent.	80.2 ±0.8	65.6 ±0.9	77.2 ±0.8	72.7 ±0.3	80.4 ±0.4	71.0 ±0.5	79.7 ±0.5
	EPKL	78.8 ±0.8	65.2 ±1.0	76.1 ±0.9	71.6 ±0.2	78.9 ±0.4	70.0 ±0.6	78.3 ±0.6
	KLoSNet (Ours)	86.7 ±0.4	68.4 ±1.1	83.0 ±0.6	76.4 ±0.4	86.4 ±0.4	75.0 ±0.5	86.0 ±0.4
CIFAR-100 ResNet-18	Diff. Ent	83.0 ±0.4	70.1 ±1.1	80.2 ±0.4	76.8 ±0.5	83.0 ±0.3	75.6 ±0.5	82.5 ±0.3
	EPKL	82.5 ±0.4	70.2 ±1.1	80.0 ±0.4	76.3 ±0.6	82.5 ±0.3	75.0 ±0.5	82.0 ±0.2
	KLoSNet (Ours)	86.9 ±0.3	73.1 ±0.4	84.4 ±0.1	80.8 ±0.2	87.3 ±0.2	79.0 ±0.2	86.7 ±0.3
the main network. In CIFAR experiments, this corresponds to VGG-16’s fc1 layer of size 512.
This auxiliary neural network is composed of five fully-connected layers of size 400, except for
the last layer obviously. KLoSNet decoder’s weights ω are trained for 100 epochs with `2 loss
(Eq. 8 in the main paper) and with Adam optimizer with learning rate 1e-4. As KLoS* ranges from
zero to large positive values (>1000), one may encounter some issues when training KLoSNet.
Consequently, we apply a sigmoid function, σ(x)= 订I-X, after computing the KL-divergence
between the NN’s output and γy . To prevent over-fitting, training is stopped when validation AUC
metric for misclassification detection starts decreasing. Then, a second training step is performed
by initializing new encoder E0 such that θE0 = θE initially and by optimizing weights (θE0 , ω) for
30 epochs with Adam optimizer with learning rate 1e-6. We stop training once again based on the
validation AUC metric.
D Additional Results
D.1 Detailed Results for Synthetic Experiments
We detail in Table 5 the quantitative results for
the task of simultaneous detection of misclas-
sifications and of OOD samples for the syn-
thetic experiment presented in Section 4.1 of
the paper. First-order uncertainty measures
such as MCP and Entropy perform obviously
well on the first task with 80.2% AUC for
MCP. However, their OOD performance drops
to 〜15% AUC on this dataset. On the other
hand, Mahalanobis is adapted to detect OOD
samples but not as good for misclassifications.
KLoS achieves comparable performances to
best methods in misclassification detection and
Method	Mis.(↑)	OOD (↑)	Mis+OOD (↑)
MCP	80.2 ±1.1	15.9 ±0.7	48.6 ±1.9
Entropy	78.4 ±1.5	11.0 ±0.3	45.7 ±1.0
Mut. Inf.	75.0 ±2.3	2.2 ±0.2	38.8 ±1.2
Diff. Ent.	74.2 ±2.7	1.9 ±1.0	38.0 ±1.3
Mahalanobis	51.5 ±2.8	98.5 ±0.3	75.0 ±1.4
KLoS	79.4 ±1.2	98.8 ±0.3	89.2 ±0.5
Table 5: Synthetic experiment: misclassifica-
tion (Mis.), out-of-distribution detection (OOD)
and simultaneous detection (Mis+OOD) (mean %
AUC and std. over 5 runs). Bold type indicates
significant top performance (p < 0.05) according
to paired t-test.
in OOD detection (79.4% for Mis. and 98.8%
for OOD). As a result, when detecting both in-
puts simultaneously, KLoS improves all baselines, reaching 89.2% AUC.
D.2 Results with other second-uncertainty measures results
In Table 6, we presents results with other second-uncertainty measures results: differential entropy
(Diff. Ent.) and EPKL. As observed with Mut. Inf. their performances remains below KLoSNet.
17
Under review as a conference paper at ICLR 2022
Table 7: AUPR results for experiments with VGG-16 architecture on CIFAR-10 and CI-
FAR100: Misclassification (Mis.), out-of-distribution (OOD) and simultaneous (Mis.+OOD) de-
tection results (% mean and std. over 5 runs).
Method		CIFAR-10 Mis. (↑)	LSUN		TinyImageNet		STL-10	
			OOD (↑)	Mis+OOD (↑)	OOD (↑)	Mis+OOD(↑)	OOD (↑)	Mis+OOD(↑)
	MCP	44.8 ±2.8	80.8 ±0.6	92.7 ±0.4	80.6 ±0.8	92.7 ±0.6	57.3 ±0.5	85.6 ±0.6
	Entropy	44.2 ±3.1	82.1 ±0.7	92.8 ±0.5	81.6 ±0.8	92.7 ±0.6	57.5 ±0.5	85.5 ±0.7
	ConfidNet	45.1 ±3.2	81.2 ±1.3	93.4 ±0.4	82.1 ±0.4	93.7 ±0.3	58.3 ±0.8	86.4 ±0.5
CIFAR-10 VGG-16	Mut. Inf	39.1 ±2.1	85.9 ±0.4	93.0 ±0.4	82.4 ±0.5	92.0 ±0.5	57.6 ±0.6	84.4 ±0.6
	Diff. Ent	40.8 ±0.3	86.5 ±0.3	93.7 ±0.3	83.6 ±0.4	93.0 ±0.3	58.4 ±0.4	85.3 ±0.5
	EPKL	39.0 ±2.1	85.7 ±0.4	92.9 ±0.4	82.2 ±0.5	91.9 ±0.5	57.6 ±0.6	84.3 ±0.6
	ODIN	43.4 ±3.2	82.1 ±0.8	92.5 ±0.6	81.3 ±1.0	92.4 ±0.7	57.2 ±0.6	85.0 ±0.8
	Mahalanobis	43.7 ±3.5	86.6 ±0.4	94.9 ±0.3	84.9 ±0.3	94.4 ±0.2	59.5 ±0.2	86.9 ±0.4
	KLoSNet (Ours)	47.6 ±2.5	82.2 ±1.8	94.8 ±0.2	83.1 ±0.7	94.7 ±0.2	61.7 ±0.6	88.4 ±0.6
	MCP	68.2 ±1.6	61.4 ±1.1	90.6 ±0.6	70.3 ±0.7	92.5 ±0.4	62.5 ±0.6	90.3 ±0.4
	Entropy	67.1 ±1.6	62.1 ±1.2	90.4 ±0.6	71.2 ±0.6	92.3 ±0.4	63.1 ±0.6	90.1 ±0.4
	ConfidNet	68.5 ±1.8	61.7 ±0.6	91.0 ±0.4	71.2 ±0.4	92.9 ±0.4	63.7 ±0.5	90.8 ±0.3
CIFAR-100 VGG-16	Mut. Inf	63.1 ±1.4	63.9 ±0.8	89.9 ±0.5	70.8 ±0.4	91.3 ±0.3	63.1 ±0.6	89.0 ±0.4
	Diff. Ent	64.6 ±1.4	63.8 ±0.7	90.3 ±0.4	71.5 ±0.4	91.8 ±0.3	63.8 ±0.5	89.6 ±0.3
	EPKL	62.9 ±1.4	63.9 ±0.8	89.8 ±0.5	70.7 ±0.4	91.2 ±0.3	63.0 ±0.6	88.9 ±0.4
	ODIN	67.6 ±1.7	61.6 ±1.2	90.4 ±0.6	70.5 ±0.7	92.3 ±0.4	62.6 ±0.6	90.1 ±0.4
	Mahalanobis	65.7 ±1.4	68.0 ±1.5	91.8 ±0.5	73.1 ±0.6	92.9 ±0.3	66.0 ±0.6	91.0 ±0.4
	KLoSNet (Ours)	69.9. ±1.4	64.3 ±1.3	92.2 ±0.5	72.6 ±0.5	93.7 ±0.3	65.6 ±0.7	92.0 ±0.3
	(a) CIFAR-10 With VGG-16				
Method	CIFAR-10 Mis. (↑)	SVHN	
		OOD (↑)	Mis+OOD (↑)
MCP	87.6 ±1.6	87.3 ±2.2	88.9 ±0.5
Entropy	83.5 ±2.4	85.5 ±2.3	86.9 ±1.9
ConfidNet	90.2 ±0.8	89.0 ±3.1	91.0 ±1.1
Mut. Inf.	84.1 ±1.5	80.0 ±3.9	83.2 ±1.7
Diff. Ent.	86.8 ±1.0	86.0 ±2.0	87.6 ±0.9
EPKL	83.9 ±1.5	79.4 ±4.2	82.8 ±1.9
ODIN	86.0 ±2.0	86.8 ±2.2	87.7 ±1.0
Mahalanobis	91.2 ±0.3	89.1 ±2.8	91.5 ±1.1
KLoSNet (Ours)	92.5 ±0.6	89.8 ±3.0	92.7 ±1.2
(b) CIFAR-10 With ResNet18			
Method	CIFAR-100 Mis. (↑)	SVHN	
		OOD (↑)	Mis+OOD (↑)
MCP	84.9 ±0.8	79.6 ±1.0	83.0 ±0.9
Entropy	84.6 ±0.8	79.6 ±1.1	82.8 ±0.9
ConfidNet	90.7 ±0.4	84.6 ±1.1	88.6 ±0.6
Mut. Inf	80.6 ±0.6	77.0 ±1.2	79.4 ±0.9
Diff. Ent	82.7 ±0.6	78.3 ±1.2	81.1 ±0.9
EPKL	80.2 ±0.6	76.8 ±1.3	79.0 ±0.9
ODIN	83.7 ±0.7	78.9 ±1.0	81.9 ±0.9
Mahalanobis	91.2 ±0.4	90.7 ±0.4	91.8 ±0.3
KLoSNet (Ours)	93.9 ±0.4	93.1 ±1.1	94.4 ±0.3
(c) CIFAR-100 with VGG-16
(d) CIFAR-100 with ResNet18
Method	CIFAR-10 Mis. (↑)	SVHN	
		OOD (↑)	Mis+OOD (↑)
MCP	82.9 ±0.8	70.8 ±3.9	81.3 ±2.0
Entropy	82.2 ±0.8	72.9 ±3.9	81.5 ±2.0
ConfidNet	84.4 ±0.6	68.0 ±3.4	80.8 ±2.0
Mut. Inf.	78.9 ±0.8	72.7 ±4.9	79.5 ±2.5
Diff. Ent.	80.2 ±0.8	72.4 ±4.9	80.2 ±2.5
EPKL	78.8 ±0.8	72.7 ±4.8	79.4 ±2.4
ODIN	82.1 ±0.8	72.0 ±3.8	81.3 ±1.9
Mahalanobis	84.0 ±0.2	73.4 ±5.6	83.2 ±2.5
KLoSNet (Ours)	86.7 ±0.4	70.4 ±5.7	83.5 ±2.8
Method	CIFAR-100 Mis. (↑)	SVHN	
		OOD (↑)	Mis+OOD (↑)
MCP	84.9 ±0.8	79.6 ±1.0	83.0 ±0.9
Entropy	84.6 ±0.8	79.6 ±1.1	82.8 ±0.9
ConfidNet	90.7 ±0.4	84.6 ±1.1	88.6 ±0.6
Mut. Inf	80.6 ±0.6	77.0 ±1.2	79.4 ±0.9
Diff. Ent	82.7 ±0.6	78.3 ±1.2	81.1 ±0.9
EPKL	80.2 ±0.6	76.8 ±1.3	79.0 ±0.9
ODIN	83.7 ±0.7	78.9 ±1.0	81.9 ±0.9
Mahalanobis	91.2 ±0.4	90.7 ±0.4	91.8 ±0.3
KLoSNet (Ours)	93.9 ±0.4	93.1 ±1.1	94.4 ±0.3
Figure 10:	Results with SVHN as OOD dataset (% mean AUROC and std. over 5 runs).
D.3 AUPR results with VGG-16 architecture
Along with the AUROC metric, we also present results with the area under the precision-recall
curve (AUPR) metric in Table 7. The precision-recall (PR) curve is the graph of the precision =
TP/(TP +FP) as a function of the recall = TP/(TP + FN). This metric is threshold-independent
but in contrast to AUROC, the AUPR adjusts for the different positive and negative base rates. In
our experiments, misclassifications+OOD samples are used as the positive detection class. These
results confirm that KLoSNet is the best suited measure to detect simultaneously misclassifications
and OOD samples.
D.4 Results with SVHN as OOD test dataset
We report in Fig. 10 all the results when evaluating with SVHN Netzer et al. (2011) as OOD dataset.
Along with simultaneous detection results, we also provide separate results for misclassifications
18
Under review as a conference paper at ICLR 2022
Table 8: Comparative experiments including relative Mahalanobis (RelMaha) and Maha-
lanobis (Maha) baselines on CIFAR-10 and CIFAR-100. Misclassification (Mis.), out-of-
distribution (OOD) and simultaneous (Mis+OOD) detection results (mean % AUROC and std. over
5 runs). Bold type indicates significantly best performance (p<0.05) according to paired t-test.
LSUN	TinyImageNet	STL-10
Method		Mis.	OOD	Mis+OOD	OOD	Mis+OOD	OOD	Mis+OOD
CIFAR-10 VGG-16	Maha	91.2 ±0.3	88.9 ±0.2	91.3 ±0.1	86.4 ±0.2	90.2 ±0.1	63.4 ±0.2	78.8 ±0.3
	Rel-Maha	91.3 ±0.4	88.8 ±0.2	91.4 ±0.2	86.4 ±0.2	90.2 ±0.1	63.3 ±0.2	78.8 ±0.2
	KLoSNet	92.5 ±0.6	87.6 ±0.9	91.7 ±0.9	86.6 ±0.9	91.2 ±0.8	67.7 ±1.4	81.8 ±0.9
CIFAR-100 VGG-16	Maha	84.0 ±0.2	71.1 ±1.0	82.4 ±0.5	77.0 ±0.5	84.9 ±0.3	75.4 ±0.3	84.3 ±0.5
	Relzaha	83.5 ±0.4	70.5 ±1.0	81.9 ±0.6	77.0 ±0.5	84.6 ±0.4	75.4 ±0.3	84.1 ±0.5
	KLoSNet	86.7 ±0.4	68.4 ±1.1	83.0 ±0.6	76.4 ±0.4	86.4 ±0.4	75.0 ±0.5	86.0 ±0.4
detection and OOD detection respectively. Similarly to the comparative results in the main paper,
KLoSNet outperforms all the baselines in every simultaneous detection benchmark, with Maha-
lanobis being second.
D.5 Comparison with Relative Mahalanobis
In Table 8, we include results with the relative Mahalanobis distance proposed in Ren et al. (2021).
Results show no improvement with the simultaneous detection performances, and even a slight de-
crease for the task of misclassification detection. In the original workshop paper, the authors evaluate
their method with near-OOD samples, such as CIFAR-10 vs. CIFAR-100, while we evaluate here
with far OOD datasets e.g. TinyImageNet.
D.6 Impact of Adversarial Perturbations
In the original papers, ODIN and Mahalanobis preprocess inputs by adding small inverse adver-
sarial perturbations to reinforce networks in their prediction; this has also the observed benefit to
make in-distribution and out-of-distribution samples more separable. The tuning of the adversarial
noise’s magnitude depends on the evaluated OOD data. In Figure 11a, we plot the AUC of each de-
tection task with different values of perturbation magnitude ε with ODIN, Mahalanobis and KLoS,
using SVHN as OOD dataset. Even though there exists a particular noise value for improved OOD
detection (Fig. 11a, middle), increasing noise magnitude deteriorates performances in misclassifica-
tion detection (Fig. 11a, left) for each method. The best results on the simultaneous detection task
(Fig. 11a, right) correspond to ε = 0, as done in experiments presented in the main paper.
Worse, except with SVHN, adversarial perturbations are detrimental even to OOD detection. We
report the AUC results of varying adversarial perturbations on CIFAR-10 dataset when using LSUN
(Fig. 11b), TinyImageNet (Fig. 11c) and STL-10 (Fig. 11d) as OOD datasets. The best results on
each considered task correspond to ε = 0 and KLoS outperforms both Mahalanobis and ODIN. As
opposed to results with SVHN as OOD dataset, we did not observe improvements on any method
(ODIN, Mahalanobis and KLoS) when using inverse adversarial perturbations for OOD detection
with LSUN, TinyImageNet and STL-10 datasets. Similar results are observed in Liang et al. (2018)
(Appendix B, Fig. 8) when using WideResNet architectures. Regarding Mahalanobis in Lee et al.
(2018), the authors only provided ablation for SVHN dataset.
19
Under review as a conference paper at ICLR 2022
——ODlN
一 Mahalanobis
⑻ Mis, detection
(b) OoD detection
(C) MiS.+OOD detection
0.00 0.01 0.02 0.03 0.04 0.05
Noise magnitude ε
0.00 0.01 0.02 0.03 0.04 0.05
Noise magnitude ε
0.00 0.01 0.02 0.03 0.04 0.05
Noise magnitude ε
(a) CIFAR-10 / SVHN
(b) CIFAR-10/LSUN
(b) Oe)D detection
0.00 0.01 0.02 0.03 0.04 0.05
Noise magnitude ε
0.00 0.01 0.02 0.03 0.04 0.05
Noise magnitude ε
(a) Mis. detection
0.00 0.01 0.02 0.03 0.04 0.05
Noise magnitude ε
(C) MiS.+OOD detection
⑻ Mis, detection
(C) CIFAR-10 / TinyImageNet
(b) OoD detection
(C) MiS.+OOD detection
(d) CIFAR-10 / STL-10
Figure 11: Effect of inverse adversarial perturbations on OOD-designed measures and KLoS for
misclassification detection, OOD detection and simultaneous detection with VGG-16 architecture.
20
Under review as a conference paper at ICLR 2022
E S elective Clas sification in Presence of Distribution Shifts
Classification with a reject option, also known as selective classification El-Yaniv & Wiener (2010),
consists in a scenario where a classifier can abstain on points where its confidence is below a certain
threshold. This is appropriate for applications where the system can hand over to human experts
or users. Performance can be measured on risk-coverage curves. The coverage is the probability
mass of the non-rejected region in X and can be empirically estimated by the percentage of the non-
rejected samples. The risk of a selective classifier is the average loss on the accepted samples. Given
a chosen coverage, good selective classifiers correlate with low risk. Averaged performances are
evaluated on risk-coverage curves with a threshold-independent area-under-curve metric, denoted
here AURC. The lower the AURC, the better the selective classifier.
Previous works evaluate the performance on in-distribution data. However, a classifier may en-
counter data drawn from a different distribution when deployed in the wild. Following Koh et al.
(2020), we extend selective classification by penalizing non-rejected OOD samples. If a sample is
drawn from the in-distribution, we compute the 0/1 cost function as usual. For OOD samples, we
apply the maximum cost of 1, whatever the prediction. As for simultaneous detection, we rely on
oversampling to mitigate the unbalance between misclassifications and OOD samples.
Experiments are conducted with previously trained VGG-16 networks on CIFAR-100. We mea-
sure their selective classification when subject to distribution shifts by considering CIFAR-100C
Hendrycks & Dietterich (2019) as OOD dataset. This dataset is constructed by corrupting the orig-
inal CIFAR-100 test set. There is a total of 15 types of corruptions, which can be grouped into five
families, namely noise, blur, weather and digital. Each corruption comes with five different levels of
severity. While this dataset is commonly used to measure robustness to distribution shift, we focus
here on models’ ability to reject these samples along with misclassifications made on the original
CIFAR-100 test set.
The results are reported by corruption family (noise, blur, weather and digital) in Fig. 13a and de-
tailed in Tables 13b and 12b. One common observation regardless of the criterion is that selective
classification is harder when subject to noise perturbations than other types of perturbation. In each
case, KLoSNet and ConfidNet obtain the best performances. For instance, for weather perturbations
on CIFAR-10-C, KloSNet achieves 42.7% AURC and ConfidNet 43.4% AURC. In particular, KloS-
Net outperforms every other method for blur, weather and digital perturbations of CIFAR-100-C.
Hence, when subject to an unforeseen distribution shift, a model equipped with KLoSNet provides
more accurate uncertainty estimates without sacrificing predictive performances. Note that for noise
corruptions, the results depend widely on the run, which makes interpretation more difficult.
We present the results of a similar experiment on CIFAR10-C in Figure 12a and its detailed results
in Table 12b. Here, KLoSNet obtains state-of-the-art results in every corruption and also on average
with 41.0% AURC.
21
Under review as a conference paper at ICLR 2022
585654腕50的4β444240
(％)。中？
Method	Clean	Gaussian	Noise Shot	Impulse	Defocus	Blur		Zoom	Weather				Digital				Mean
						Glass	Motion		Snow	Frost	Fog	Bright	Contrast	Elastic	Pixel	JPEG	
MCP	48.3%	46.7%	48.0%	43.7%	48.6%	45.7%	45.4%	43.4%	44.4%	42.5%	40.4%	46.5%	43.3%	43.3%	43.0%	1.9%	42.2%
Entropy	48.8%	47.2%	48.5%	43.9%	49.1%	45.9%	45.6%	43.6%	44.7%	42.7%	40.6%	46.9%	43.5%	43.7%	43.2%	2.0%	42.5%
ConfidNet	47.4%	45.8%	47.1%	42.9%	48.1%	45.2%	44.9%	42.5%	43.5%	41.6%	39.1%	45.9%	42.6%	42.0%	42.1%	1.3%	41.4%
Mut. Inf.	53.6%	52.3%	53.2%	48.3%	53.0%	49.3%	49.1%	48.8%	49.8%	47.8%	46.9%	51.5%	47.9%	49.8%	48.2%	4.8%	47.1%
ODIN	49.3%	47.7%	48.9%	44.3%	49.5%	46.3%	46.0%	44.1%	45.2%	43.2%	41.2%	47.3%	43.9%	44.3%	43.7%	2.2%	42.9%
Mahalanobis	48.9%	46.9%	48.6%	42.5%	49.7%	45.3%	44.8%	43.1%	44.1%	41.4%	38.6%	45.8%	42.6%	42.9%	42.4%	1.0%	41.8%
KLoSNet	47.0%	45.3%	46.9%	42.3%	48.0%	45.0%	44.6%	42.2%	43.1%	41.1%	38.1%	45.2%	42.4%	41.8%	42.0%	0.9%	41.0%
(b) Detailed results
Figure 12:	Selective classification on CIFAR10-C. Comparative performance in AURC (%) of
classification with the option to reject misclassified test samples and samples from shifted distribu-
tions. Results are average on 5 runs (mean ± std.).
MCP	ODIN
(a) Aggregated results by corruption type
Method
MCP
Entropy
ConfidNet
Mut. Inf.
ODIN
Mahalanobis
Noise
Clean Gaussian Shot
53.3%	52.7%	55.2%
53.5%	53.0%	55.8%
53.5%	52.8%	55.0%
54.5%	54.0%	57.1%
53.4%	52.9%	55.5%
54.5%	53.9%	56.7%
Impulse DefOCUS
50.8%
51.3%
50.1%
53.1%
51.2%
50.9%
54.0%
54.8%
53.7%
56.2%
54.5%
55.5%
Blur
Glass Motion Zoom
52.8%
53.3%
52.3%
54.9%
53.2%
52.8%
52.5%
53.1%
51.9%
54.7%
52.9%
52.9%
51.3%
51.8%
50.8%
53.5%
51.7%
52.2%
Snow
52.0%
52.6%
51.6%
54.3%
52.4%
52.5%
Weather		Bright	Contrast	Digital	
Frost	Fog			Elastic	Pixel
50.5%	47.9%	52.1%	50.7%	50.4%	51.3%
51.0%	48.2%	52.7%	51.1%	50.9%	51.7%
50.0%	47.2%	51.7%	50.3%	49.8%	50.9%
52.6%	50.0%	54.4%	52.4%	53.0%	53.2%
50.9%	48.2%	52.5%	51.0%	50.8%	51.7%
50.7%	47.8%	52.4%	51.0%	51.4%	51.9%
JPEG Mean
13.1%
13.5%
11.7%
16.5%
13.7%
11.0%
49.4%
49.9%
49.0%
51.5%
49.8%
49.9%
KLoSNet I 54.0% ∣ 53.1%	55.5%	49.5% ∣ 53.6%	51.6%	51.4%	50.7% ∣ 51.1%
49.2% 46.5% 51.3% ∣ 49.9%	49.7% 50.7%	9.7% ∣ 48.6%
(b) Detailed results
Figure 13:	Selective classification on CIFAR100-C. Comparative performance in AURC (%) of
classification with the option to reject misclassified test samples and samples from shifted distribu-
tions. Results are average on 5 runs (mean ± std.).
22