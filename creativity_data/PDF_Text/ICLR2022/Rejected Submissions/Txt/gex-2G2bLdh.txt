Under review as a conference paper at ICLR 2022
Hinge Policy Optimization: Rethinking
Policy Improvement and Reinterpreting PPO
Anonymous authors
Paper under double-blind review
Ab stract
Policy optimization is a fundamental principle for designing reinforcement learn-
ing algorithms, and one example is the proximal policy optimization algorithm
with a clipped surrogate objective (PPO-clip), which has been popularly used in
deep reinforcement learning due to its simplicity and effectiveness. Despite its su-
perior empirical performance, PPO-clip has not been justified via theoretical proof
up to date. This paper proposes to rethink policy optimization and reinterpret the
theory of PPO-clip based on hinge policy optimization (HPO), called to improve
policy by hinge loss in this paper. Specifically, we first identify sufficient condi-
tions of state-wise policy improvement and then rethink policy update as solving a
large-margin classification problem with hinge loss. By leveraging various types
of classifiers, the proposed design opens up a whole new family of policy-based
algorithms, including the PPO-clip as a special case. Based on this construct, we
prove that these algorithms asymptotically attain a globally optimal policy. To our
knowledge, this is the first ever that can prove global convergence to an optimal
policy for a variant of PPO-clip. We corroborate the performance of a variety of
HPO algorithms through experiments and an ablation study.
1 Introduction
Reinforcement learning (RL) has served as a powerful framework for achieving optimal sequen-
tial decision making by directly interacting with the environment and learning from the underlying
random process. Policy optimization, as a fundamental design principle of RL algorithms, itera-
tively searches for an optimal policy by alternating between a policy evaluation step and a policy
improvement subroutine. Most of the popular policy optimization approaches, including the policy
gradient methods (Sutton et al., 1999; Mnih et al., 2016; Silver et al., 2014; Lillicrap et al., 2016),
Trust Region Policy Optimization (Schulman et al., 2015), and Proximal Policy Optimization (PPO)
(Schulman et al., 2017), aim to achieve policy improvement in terms of the expected total reward
via gradient-based parameter updates. Despite the empirical success of the above approaches, these
policy improvement schemes inherently lead to the following fundamental issues: (i) Global conver-
gence is usually difficult to establish due to the non-convexity of the underlying objective function
(e.g., total discounted reward) and the use of gradient methods, even for the tabular cases; (ii) The
policy update direction for improvement is tightly coupled with the true discounted state visitation
distribution, which is usually intractable to obtain and needs to be addressed via sampling in practice.
To address the above issues, we propose to rethink policy improvement in RL from the perspective
of state-wise policy improvement, which aims for improvement directly in the policy space through a
partial ordering of the policies, as adopted by policy iteration for the optimal control of Markov deci-
sion processes (MDPs). We start by identifying two useful sufficient conditions for state-wise policy
improvement and thereafter pinpoint that improvement can be achieved based solely on the sign of
the advantage function. Based on this insight, we propose Hinge Policy Optimization (HPO) by con-
necting state-wise policy improvement to solving a large-margin classification problem, where we
regard the process of policy improvement as training a binary classifier with hinge loss via empirical
risk minimization. As policy improvement of HPO is not enforced in the expected total reward but
in a state-wise manner, the policy update of HPO is completely decoupled from the state visitation
distribution, which is by contrast required by many existing popular policy optimization methods.
By leveraging various types of classifiers, the proposed HPO framework opens up a whole new
family of policy-based RL algorithms. Interestingly, the popular PPO algorithm with a surrogate
1
Under review as a conference paper at ICLR 2022
clipped objective (PPO-clip) can be shown to be a special case of HPO with a specific type of clas-
sifier. Given the policy improvement property of the proposed classification-based scheme, we are
able to establish global convergence to an optimal policy for the HPO algorithms. To the best of our
knowledge, our analysis provides the first global convergence guarantee for a variant of PPO-clip.
This paper is meant to provide a clear picture of the RL algorithms in the new HPO family with
global convergence guarantees and thereby introduce a new perspective for reinterpreting the popular
PPO-clip algorithm. The main contributions of this paper can be summarized as follows:
•	We propose HPO, a new policy optimization framework where the policy update is built on state-
wise policy improvement via hinge loss. The members in the HPO family share a generic loss
function, and their differences lie in the choice of the margin and the classifier. We also show that
the widely-used PPO-clip algorithm can be viewed as a special case in this family.
•	This paper is the first ever that can prove state-wise policy improvement and global convergence
for a variant of PPO-clip algorithm1 . Specifically, we first present a variant of the PPO algo-
rithm with an adaptive clipped objective, which can be viewed as an HPO algorithm with adaptive
margin, called HPO-AM. We prove the convergence to an optimal policy for HPO-AM, and pro-
ceed to generalize the global convergence result to some other HPO-AM like algorithms equipped
with other classifiers. We also empirically validate the proposed theoretical framework through
experiments and thereby corroborate the performance of various HPO algorithms with different
classifiers and margins (with the experimental results provided in Appendices E and F).
2 Main Results
In this section, we first review the theoretical background about RL and view PPO-clip update as a
classification problem. Then, we establish global convergence for the proposed HPO-AM.
2.1 Background
A discounted Markov Decision Process (MDP) is defined by the tuple (S, A, P, r, γ), where S is a
finite state space, A is a finite action space, P : S × A × S → [0, 1] is the state transition probability
matrix, r : S × A → R is the reward function, and γ ∈ (0, 1) is the discounted factor. A stochastic
policy π : S → ∆(A) specifies the action distribution based on the current state, where ∆(A) is
the probability simplex over A, i.e., ∆(A) = {x ∈ R|A| | x0 + ... + x|A|-1 = 1, xi ≥ 0, ∀i =
0, ..., |A| - 1}. Given any policy π, the state value function V π, the state-action value function Qπ,
and the advantage function Aπ are defined as follows.
∞
Vπ(s) := E	X γtr(st, at)s0 = s
at 〜π(Tst),	t=o	I
st+ι^P(Tst,at) L	-
(1)
∞
Qπ(s, a) :=	E	γtr(st, at)s0 = s, a0 = a ,	(2)
at 〜WTsj	t=o	1
st + 1~P(∙∣st,at) L
Aπ (s, a) := Qπ (s, a) - Vπ(s).	(3)
Define dsπ(s0) := (1 - γ) Pt∞=0 γt Pr(st = s0|s0 = s, π) as the normalized discounted state vis-
itation frequency, which represents the probability of visiting state s0 in a trajectory of π, given
that s0 = s. With the above definition, (Kakade & Langford, 2002) quantified the difference in
performance between two policies as follows. Given policies π1 and π2 ,
Vπ1(s) - Vπ2(s) = —γ X dπ1(s0) X ∏ι(a∣s0)Aπ2 (s0, a).	(4)
γ s0∈S	a∈A
1Regarding the convergence of PPO, (Liu et al., 2019) proves global convergence in expected total reward
for a neural variant of PPO with adaptive Kullback-Leibler penalty (PPO-KL). Given the salient algorithmic
difference between PPO-KL and PPO-clip, to the best of our knowledge, there remains no proof of global
convergence to an optimal policy for PPO-clip.
2
Under review as a conference paper at ICLR 2022
The TRPO algorithm proceeds to maximize the expected value of surrogate function of (4) over the
initial state distribution under the constraint of KL divergence. State-wise policy improvement is
formalized based on the following partial ordering relation.
Definition 1 (Partial ordering over policies). Let π1 and π2 be two policies. Then, π1 ≥ π2, called
π1 improves upon π2, if and only if Vπ1 (s) ≥ Vπ2 (s), ∀s ∈ S. Moreover, we say π1 > π2, called
π1 strictly improves upon π2, if and only if π1 ≥ π2 and there exists at least one state s such that
Vπ1(s) > V π2 (s).
Definition 2 (An optimal policy). A policy π* is said to be an optimal policy if π* ≥ π0, for ^very
policy π0. Moreover we let V*(s) denote the optimal value Ofeach state s, i.e., V*(s) = Vπ (S).
We first introduce two sufficient conditions for state-wise policy improvement in Propositions 1-2.
These two conditions have been identified by (Hu et al., 2020, Section 3.1).
Proposition 1. Given policies π1 and π2, π1 improves upon π2 if the following condition holds:
^X ∏ι(a∣s)Aπ2 (s, a) ≥ 0, ∀s ∈ S.	(5)
a∈A
Proposition 2. Given policies π1 and π2, π1 improves upon π2 if the following condition holds:
(∏ι(a∣s) — ∏2(a∣s))Aπ2(s, a) ≥ 0, ∀(s, a) ∈ S ×A.	(6)
Proposition 1 holds for the following reason: Since all dsπ1 (s0) are non-negative, all values in (4)
are non-negative, and hence π1 improves upon π2 . Proposition 2 can be derived directly from
Proposition 1 and the fact that P°∈a ∏2(a∣s)Aπ2(s,a) = 0, ∀s ∈ S.
Notably, Proposition 2 offers a useful insight that state-wise policy improvement can be achieved
by determining the sign of the advantage of each state-action pair (regardless of its magnitude)
and adjusting the action probabilities accordingly. In this way, no additional constraints, such as
the KL divergence constraint used in TRPO (Schulman et al., 2015), are needed to ensure policy
improvement. This also naturally motivates the design of using the signs of the advantage function
as labels in determining the direction of the policy update. More specifically, we can draw an analogy
between (6) in Proposition 2 and the training of a linear classifier: (i) The state-action pair serves as
the feature vector of a training sample; (ii) The sign of Aπ2 (s, a) plays the role of a binary label; (iii)
∏ι(a∣s) — ∏2(a∣s) resembles the prediction of a linear classifier. In the next section, We substantiate
this insight and present the proposed HPO framework.
In the rest of this paper, our analysis relies on the folloWing assumptions:
Assumption 1 (Bounded reWard). To avoid trivial cases, we assume not all rewards are zero. Since
both state and action spaces are finite, it holds naturally that there exists a positive constant R =
suP(s,a)∈S×A |r(s,a)| > 0.
Assumption 2 (Tabular policies). Policies are parameterized by π(a∣s) = θs,a, where θs ∈ ∆(A)
refers to the vector of θs.∙ for some fixed State S, and θ ∈ ∆(A)lSl, i.e., θ is SubjeCt to θs,a ≥ 0 and
Pa∈A θs,a = 1, ∀s ∈ S, ∀a ∈ A.
Notations. Throughout this paper, we letha, b〉and a ◦ b denote the inner product and the Hadamard
product of tWo real vectors a, b, respectively.
2.2	Hinge Policy Optimization and PPO-Clip
In this section, we build on the insight offered by Proposition 2 and formally present the HPO
framework. To better describe HPO, we use PPO-clip as an exemplary algorithm by reinterpreting
the policy update of PPO-clip as a large-margin classification problem.
PPO-clip. Since the TRPO algorithm can be costly, e.g., requiring the computation of multiple
Hessian-vector product, (Schulman et al., 2017) proposes two versions of PPO, PPO with KL
penalty (PPO-KL) and PPO with clipped surrogate objective (PPO-clip). PPO-KL replaces the KL
constraints of TRPO with KL penalty and a certain surrogate objective forms a lower bound on the
performance of the policy, while PPO-clip further drops the KL penalty and instead directly clips the
probability ratio. As a result, the proximal term becomes inherent in the clip function. PPO-KL and
3
Under review as a conference paper at ICLR 2022
TRPO have been guaranteed to monotonically improve the expected total reward (Liu et al., 2019)
over a given initial state distribution. However, there remains no theoretical analysis for PPO-clip.
The original objective function of PPO-clip (Schulman et al., 2017) is to maximize the following:
Lclip(θ) = Es〜d∏0,。〜∏(∙∣s) [min{Ps,a(θ)Aπ(s, a), CIiP(Ps,a(θ), 1 - e, 1 + e)Aπ(s, a)}]
=X dμo (S) X ∏(a∣s)min{ρs,α(θ)Aπ(s,a), CIiP(Ps,a(θ), 1 - e, 1 + e)Aπ(s,a)}, (7)
s∈S	a∈A
where ρs,a(θ) denotes the probability ratio "(累),μo the initial state distribution, π(a∣s) the old
policy, ∏θ (a|s) the new policy parameterized by θ, dμ0 (s) the normalized discounted state visitation
frequenCy, and e the CliPPing range. The funCtion CliP(Ps,a (θ), 1 - e, 1 + e) in (7) modifies the
objective by removing the incentive for moving the ratio Ps,a (θ) outside of the interval [1 - e, 1 + e].
Connecting PPO-clip and Hinge Loss. In PPO-clip, the policy stops being updated when the
probability ratio is out of the clipping range. That is, the subgradient method keeps “pushing” the
new policy away from the old one until it “improves” the old policy by a margin. This behavior
coincides with the large-margin classification problem where the classifier intends to “push” the
predicted label out of a margin. (Pi et al., 2020) has shown that maximizing each term in the
empirical average objective function of PPO-clip is equivalent to minimizing a hinge loss times the
magnitude of advantage as follows. Specifically, the equivalence actually refers to their gradients
instead of the functions themselves. The gradient of the original clipped objective is indeed the
negative of the gradient of the hinge loss objective. Their objective functions differ from each other
by a constant2, but their policy update rules derived by the gradient methods are exactly the same.
∂
min{ρs,a(θ)A (s,a), CIiP(Ps,a(θ), 1 - e, 1 + e)A (s, a)}
∂θ
-而 lAπ (S, a)| '(Sign(An (s, a)), ρs,a(θ) - 1, e).
∂θ
(8)
Here, '(yi, fθ (xi), e) is a hinge loss defined as
max{0, e - yi × fθ(xi)},	(9)
where e is the margin, yi ∈ {-1, 1} the label corresponding to the data xi, and fθ(xi) serves as
the binary classifier. Once yifθ(Xi) is larger than the margin, '(yi, fθ(xi), e) will equal zero, which
reflects the sample clipping mechanism in PPO-clip. Note that hinge loss has been commonly used
for large-margin classification, most notably for support vector machines (Freund & Schapire, 1999).
From the above, in the tabular settings maximizing the objective function in (7) can be rewritten as
minimizing the following loss function,
L(θ) = X dμo (s) X ∏(a∣s)∣Aπ (s,a)∣ '(sign(Aπ (s, a)),ρs,a (θ) - 1,e).	(10)
s∈S	a∈A
In practice, using the sample average to approximately maximize the above loss function as
L(θ) ≈ L(θ) = E
(s,a)∈Dπ
∣Aπ(s,a)∣ '(sign(Aπ(s,a)),ρs,a(θ) - 1,e)
∣D∏I
(11)
where Dπ is a batch of samples drawn under π and IAπ (s, a)I can be interpreted as the weight or
cost associated with each sample in large-margin classification.
Remark 1. For classification problems, if one is able to find a solution (i.e., classifier) that can
classify all of the training data correctly, then the “costs” are relatively unimportant compared to
the label. Therefore, in the hinge loss function form, this implies that the magnitude of advantage
is not important. Note that we do not take the state-action pair with Aπ(s, a) = 0 into account.
As shown later in the experiments in Appendix E, algorithms with unweighted loss function also
perform well. The “costs” in the cost-sensitive classification matter when the parameter space is not
complete or when the hinge loss is not reduced to (nearly) zero. In those cases, one can consider it
as a cost-sensitive classification problem and extend the analysis.
2Please see Appendix G for the detailed comparison of the two objectives.
4
Under review as a conference paper at ICLR 2022
Hinge Policy Optimization. Based on the the similarity of PPO-clip objective as that to minimize
a hinge loss drawn by (Pi et al., 2020), we propose a new family of algorithms called Hinge Policy
Optimization (HPO), where the general form of the loss function for policy improvement is
L(θ) = ɪ X weight X '(label, classifier, margin).
|	| (s,a)∈D
(12)
Different combinations of classifiers, margins, and weights lead to different loss functions, and
hence represents different algorithms in this new HPO family.
2.3	HPO with an Adaptive Margin: Algorithm and Global Convergence
In this subsection, we propose an HPO algorithm with an adaptive margin, called HPO-AM. In
principle, HPO-AM resembles the PPO-clip algorithm, except for that we represent the objective
function in a form of hinge loss, as described in (10) instead of (7), and that is dynamic for all
states s during the training process, as follows. Let Aπ (s, a) denote the estimated advantage of a
state-action pair (s, a) under policy π. Then, we define
e∏ := α ∙ min {l, Pa∈Iπ- .(：卜) }, 0 <α< 1, Vs ∈S,	(13)
a∈lS+ π(als)
whereI∏+ := {a ∈ A∣Aπ(s,a) > 0}, andI∏- := {a ∈ A∣Aπ(s,a) < 0}. ThenotationI∏+ (In-)
represents the set of actions of s leading to positive (negative) estimated advantages. The adaptive
clipping range is designed for the theoretical proof to “ensure the existence of an improved policy.”
The ratio in (13) reflects how much the current policy can be improved since HPO is designed to
increase the probability of the actions with positive advantages and decrease those with negative
advantages. Notably, the adaptive clipping range is in fact smaller than the constant clipping in the
original PPO-clip as the adaptive clipping range is the constant α times a number no greater than 1.
Our HPO-AM is shown in Algorithm 1 below: Like PPO-clip, HPO-AM proceeds iteratively and, in
each iteration t, runs an old policy (denoted by π(t) or π(θ(t)) parameterized by θ(t)) obtained from
the previous iteration t - 1, and then updates the new policy into π(t+1) for the next iteration. For
simplicity, let ρ(st,)a(θ), e(st), Is(t)+, and Is(t)- respectively denote the above ρs,a(θ), esπ, Isπ+, and Isπ-
used during iteration t. Moreover, let V(t)(s), Q(t)(s, a), A(t)(s, a), and A(t) (s, a) denote Vπ(t) (s),
Qπ(t) (s, a), Aπ(t) (s, a), and Aπ(t) (s, a), respectively. Define the loss function of HPO-AM as
Kτ
L(t)(θ)= E £|A(t)(sk,ak)∣'(Sign(A⑴(sk,ak))/，鼠(θ)-".
(14)
τ∈Dt k=0
In HPO-AM, we consider the weighted hinge loss with classifier ρs,a (θ) - 1. In each iteration,
HPO-AM updates the policy by the entropic mirror descent algorithm (EMDA) (Beck & Teboulle,
2003) to achieve a sufficiently small loss. Let δSt) := mina∈∕(t)+∪∕(t)- |A(t)(s, a)∣ and set δSt) = ∞
if Is(t)+ ∪ Is(t)- is empty. To specify the termination condition of EMDA, define δ(t) := mins∈S δs(t)
and e(t) := ming∈{g0∈s∣e(t) >0} eSt), and choose the threshold as (1 一 Z)δ(t)e(t), with Z ∈ (0,1).
Remark 2. As shown in Algorithm 1, EMDA is applied to find a proper θ(t+1) that achieves a
sufficiently small loss L(t)(θ). While there are alternative ways to minimize the loss L(t)(θ) over
∆(A)lSl (e.g., the projected subgradient method), We leverage the exponentiated gradient scheme
of EMDA to ensure that π(t) remains strictly positive for all state-action pairs in each iteration t.
Before obtaining our main theorem, we make the following assumptions.
Assumption 3 (Training data and strict positivity of distributions). Suppose that the sampled train-
ing data set during each iteration of HPO-AM is sufficiently large to contain all possible state-action
pairs. Strict positivity of the initial state distribution μο and the initial policy π(0) is a necessary
condition for this assumption.
Assumption 4 (Correctness of the sign of the advantage function). We assume that at each iteration
t, for each state-action pair, the sign of the estimated advantage is the same as that of the true
advantage, i.e., Sign(A(t)(s, a)) = Sign(A(t)(s, a)), for all (s, a).
5
Under review as a conference paper at ICLR 2022
Algorithm 1: HPO with an adaptive margin
1
2
3
4
5
6
Result: Learned policy π(∞)
Initialize policy π(0) = π(θ(0)), initial state 1
distribution μo, α ∈ (0,1), step sizes of
EMDA {ηk}, threshold ζ ∈ (0, 1);
for t = 0,1,…do	2
Collect a set of trajectories τ ∈ Dt under	3
policy π(t) = π(θ(t) );	4
Estimate A(t) by any policy evaluation 5
algorithms;	6
Compute L(t)(θ), δ(t), e(t) based on A(t)	1
and the collected samples in Dt ;
Update the policy by	8
θ(t+1) = EMDA(L(t)(θ), {ηk}, (1 -	9
ζ)δ(t)e(t), θ(t))	10
end	11
Algorithm 2: EMDA(L(θ), {ηk}, ξ, θinit)
Result: Learned parameter θ
Input: Objective L(θ), step sizes {ηk},
threshold ξ, and initial parameter θinit;
Initialize θe(1) = θinit, θe= θinit, and k = 1;
while L(θe) > ξ do
for each state s do
Find gs,a = ∂L(θ) lθ=e(k), for each a;
s,a
Let Ws = (e-ηkgs,1,…，e-ηkgs,∣A∣);
e(k+1) = hw¼(Ws ◦ e(k));
hws , s i
end
θ = argminθ∈{e"ι≤i≤k+i} L(θ);
Increment k by 1;
end
7
Remark 3. As pointed out by Assumption 4, the global convergence of HPO requires only the
correct signs of the advantage values, as will be shown in Theorem 1 below. This is one major
difference from the recent global convergence results of policy gradient methods (Agarwal et al.,
2019; Bhandari & Russo, 2019; Mei et al., 2020), which require the true policy gradient and hence
the true advantage function. To address the potentially incorrect signs of the estimated advantage
values, one could consider the “label-robust” version of HPO by leveraging the loss functions in the
robust classification literature, e.g., (Bertsimas et al., 2019; Natarajan et al., 2013). As this paper is
meant to take the first step toward establishing the theoretical foundation of the proposed new policy
optimization framework and PPO-clip, we leverage Assumption 3 in order to rigorously establish
the global convergence property of HPO.
Theorem 1 (Global convergence of HPO-AM). Suppose the step size of EMDA is diminishing and
non-summable. UnderHPO-AM, we have V ((J (s) → V * (s') as t → ∞, ∀s ∈ S
The proof of Theorem 1 is provided in the next section. Notably, the main difference between HPO-
AM and PPO-clip is the choice of margin, and HPO-AM is ensured to monotonically increase all
state values, not just the expected total reward as in PPO-KL and TRPO. Moreover, Theorem 1
ensures that all of the state values converge to the optimal value eventually.
Inspired by (Agarwal et al., 2020), based on the limits A(∞)(s, a) which can be shown to exist via
Theorem 1, we divide all actions into the following sets for each state s:
I+ :=	{a ∈ A∣A(∞)(s,a)	> 0},	(15)
I0 :=	{a ∈ A∣A(∞)(s,a)	=0},	(16)
I-	:=	{a ∈ A∣A(∞)(s,a)	< 0}.	(17)
Note that π(() converges to an optimal policy if Is+ is empty for all s. As a result, Is+ will play an
important role in the proof of Theorem 1 in Section 3.2. Theorem 1 is followed by the two notable
Corollaries 1-2, and their proofs are given in Appendix B.
Corollary 1. Consider state s ∈ S. For each a ∈ Is-, the probability for our agent to take action a
under state s converges to zero. For a ∈ Is0, the probability sum of taking these actions under state
s converges to one. That is, π(() (a|s) → 0 for all a ∈ Is- and Pa∈I0 π(() (a|s) → 1 as t → ∞.
Corollary 2. Consider state s ∈ S, if there is only one action a ∈ Is0, i.e., the optimal policy is
deterministic, then we have e(s() → 0 as t → ∞.
Remark 4 (Multiple optimal actions). In the case that a state has one or more actions with the
largest Q-value, any policy that assigns nonzero probability only to these actions with one in total
is an optimal policy. From Corollary 1, we know that the probability sum of actions belong to Is0
approaches one. As a result, we presume that the actions described above form the set Is0 .
6
Under review as a conference paper at ICLR 2022
Remark 5 (Label quality). In Theorem 1, we obtain a similar state-wise convergence result as
Theorem 5.1 in (Agarwal et al., 2020), which proves asymptotic state-wise global convergence of
policy gradient for softmax parameterization. In (Agarwal et al., 2020), for the convergence result to
be true, it requires true gradient, which includes true advantage values. In addition, the calculation
for true gradient is an expectation over state visitation frequency, so it needs to follow a specific
distribution and sum over every state-action pair. In contrast, as stated in Remark 1 and Assumptions
3-4, our method only requires true advantage signs and visiting all state action pairs, and there is no
need to approximate the state visitation frequency. Obviously, the requirement of accurate advantage
estimate is more demanding than that of accurate advantage signs.
3 Proof of Global Convergence of HPO-AM
3.1	Supporting Lemmas for the Proof of Theorem 1
In this section, we present the important properties of HPO-AM, which would serve as useful lem-
mas for proving Theorem 1. Define the state-wise loss function for each state s in each iteration t as
L St)(θs )= £|A㈤(s,a)∣ '(sign(A㈤Ga)),Ps,a(θs)- 1,e"	(18)
a∈A
Based on Assumption 2, where θs ∈ ∆(A), we can use ρs,a(θs) to represent ρs,a (θ) and further
construct the above function of θs. Regardless of the size of the state space, the loss function can be
viewed as optimizing policies on a per-state basis. The reason for this lies in that the definition of
state-wise loss function is valid as long as the policy parameterization is “separable by state.” That is,
the policy parameters under different states are independent, e.g., the direct tabular parameterization
and the Softmax parameterization. As a result, We know L(t) (θ) = 0 if and only if LSt)(θs )=0 for
all s ∈ S . Next, we introduce the supporting lemmas, whose proofs are in Appendix A.
Lemma 1 (Strict improvement and strict positivity of policy under HPO-AM). Suppose π(t) is
strictly positive in all state-action pairs, i.e., π(t)(a∣s) > 0 ,forall (s, a). UnderHPO-AM, ifEMDA
takes diminishing non-summable step sizes, i.e., ηk → 0 and Pk∞=1 ηk = ∞, then π(t+1) satisfies
that π(t+1) > π(t) and π(t+1) (a|s) > 0, for all (s, a).
Lemma 2 (Monotonicity in π(t)(a∣s)). There exist To and Ti such thatfor all a ∈ I+, π(t)(a∣s) is
strictly increasing for t > T0;for all a ∈ Is-, π(t) (a|s) is strictly decreasing for t > T1.
Lemma 3 (Lower Bound of (st)). If Is+ is not an empty set, then there exists a positive constant c
such that (st) is lower bounded by c, for all t > T1.
Lemma 4. Pa∈I+∪I- (st)π(t) (a|s)|A(t) (s, a)| → 0 as t → ∞.
3.2	Proof of Theorem 1
We first prove that each state value increases monotonically. Then, under the bounded reward set-
ting, we have that each state value is bounded and thus converges. For convergence to the global
optimal, we prove that the algorithm won’t stop until there is no action with positive advantage.
The same analytical framework has also been used by (Agarwal et al., 2020) to establish the global
convergence of the vanilla policy gradient method. Despite the high-level resemblance, we reach the
above three proof steps in a totally different way as the two approaches are fundamentally different.
From Lemma 1, we know that in each iteration t, π(t+1) and π(t) satisfy the sufficient condition for
state-wise policy improvement provided by Proposition 2. This implies that the sequence of state
values is point-wise monotonically increasing, i.e., V (t+1)(s) ≥ V (t)(s), ∀s ∈ S. Moreover, by
iR ≤ V(t)(s) ≤ 1RY
Assumption 1 and the discounted setting, we have -
. The above monotone
increasing property and boundedness imply convergence, i.e., V(t) (s) → V(∞) (s). Similarly, it can
be derived that Q(t)(s, a) → Q(∞)(s, a), and thus A(t)(s, a) → A(∞)(s, a). The above sets Is+, Is0
and Is- are well-defined, based on the limit A(∞) (s, a).
7
Under review as a conference paper at ICLR 2022
To establish that π(t) converges to an optimal policy, it is sufficient to show that Is+ is an empty set
for all s. The proof proceeds by contradiction as follows: Suppose Is+ is non-empty with a+ ∈ Is+.
From Lemma 2, We have that π(t)(a+ |s) is bounded below by π(T0)(a+∣s) for some T0 ∈ N for all
t > T0. From Lemma 3, there exists a positive lower bound of (st). However, Lemma 4 shows that
convergence ofV (t)(s) implies that either (st) converges to zero or the sum of probability mass over
all actions in Is+ ∪ Is- converges to zero. This leads to a contraction, and thus completes the proof.
4 HPO-AM With Alternative Classifiers
In the previous sections, we propose the HPO update scheme and prove the global convergence for
a particular case, HPO-AM, in Theorem 1. In this section, we go beyond HPO-AM as follows:
HPO-AM with Alternative Classifiers. Notably, the HPO update scheme can be readily extended
to various other classifiers. In the following Theorem 2, we provide a further result that global
convergence also holds for HPO-AM with some other classifiers. The proof is given in Appendix C.
Theorem 2. Theorem 1 is also satisfied for HPO-AM with the following classifiers and the corre-
sponding margins with 0 < α < 1:
(i)	HPO-AM with classifier log(∏θ(a∣s)) — log(π(a∣s)) (HPO-AM-log): Let
e； = log(1 + α ∙ min{1, pa∈I∏ ［。* }).
α∈IS+ WaIS)
(ii)	HPO-AMwith classifier Pρs,°(θ) — 1 (HPO-AM-root): Let
∏ 二 .rι Pα∈ιπ- n(a|sL,	1
es = 1 +α α ∙ min{1,	~~s_rrʒ} — L
a∈lS+ WaIS)
(19)
(20)
Remark 6. The classifiers in Theorem 2 also leverage the probability ratio ρs,a(θ), but in a way
different from that in HPO-AM in (14). Given the generality of the HPO framework, one can design
HPO-AM with other customized classifiers that either does not directly involve the probability ratio
or considers the power of the ratio to be larger than one, such as ∏θ(a|s) — ∏(a∣s) (called HPO-
AM-sub) or (ρs,a(θ))2 — 1 (called HPO-AM-square). It remains an interesting open problem how to
establish global convergence for HPO with these classifiers. We provide more detailed explanations
for the above two unproven classifiers in Appendix D.
5 Practical Implementation and Experimental Results
Recall that HPO-AM improves the policy by minimizing the loss L√(t)(θ) in (14). Therefore, like
PPO-clip, in practice HPO-AM can be readily combined with neural network parameterization and
end-to-end training. We implement HPO-AM and its variants on top of the open-source RL Base-
lines 3 Zoo framework (Raffin, 2020). We empirically evaluate the HPO-AM algorithms in MinAtar
(Young & Tian, 2019), a modified version of Atari games for efficient evaluation, and conduct an
ablation study to better understand the candidate design choices of HPO. Due to the space limitation,
the ablation study is provided in Appendix E. We compare HPO-AM variants with PPO-clip, A2C,
and Rainbow in Asterix, Breakout and Freeway with 5 million training steps under 5 random seeds.
For PPO-clip and A2C, we use the tuned hyperparameters suggested by Stable Baselines 3, and
we disable the entropy regularization for a fair comparison. For Rainbow, we use the open-source
implementation from (Obando-Ceron & Castro, 2021).
Figure 1 shows the training curves of the HPO algorithms and multiple popular benchmark RL
methods in three different games. The results of A2C and Rainbow are consistent with their perfor-
mance in the benchmark (Obando-Ceron & Castro, 2021). In (Kaiser et al., 2020), for Atari games,
Rainbow is better than PPO-clip in Asterix, but worse in Breakout and Freeway, which is similar
to our results in MinAtar. Figure 1 indicates that each classifier has its own strengths. Notably,
HPO-AM and HPO-AM-root achieve comparable or better performance than PPO-clip in the three
environments. HPO-AM-log gets similar rewards as PPO-clip in Breakout and Freeway, but it gets
fewer rewards than PPO-clip in Asterix. HPO-AM-square is close to A2C in Asterix and Breakout,
but its performance is on par with PPO-clip in Freeway. Overall, the above results demonstrate that
the HPO algorithms are indeed applicable in high-dimensional tasks.
8
Under review as a conference paper at ICLR 2022
Ooooooo
6 5 4 3 2 1
----HPO-AM
----HPO-AM-Iog
----HPO-AM-root
——HPO-AM-SUb
----HPO-AM-square
(a) Breakout	(b) Freeway	(c) Asterix
Figure 1: Experimental results in the MinAtar environments.
6	Related Work
Global Convergence of Policy Gradient Methods. One related line of recent research is on the
global convergence of the policy gradient methods. (Agarwal et al., 2019; 2020) establishes global
convergence results of various policy gradient approaches, including the vanilla policy gradient
(with both tabular and softmax policy parametrizations) and the natural policy gradient method
(with a softmax policy parametrization). Concurrently, (Bhandari & Russo, 2019) provides an alter-
native analysis of global optimality of the policy gradient method. (Wang et al., 2019) provides the
global optimality guarantees for both the vanilla policy gradient and natural policy gradient methods
under the overparameterized two-layer neural parameterization. (Mei et al., 2020) establishes the
convergence rates of both vanilla softmax policy gradient and the entropy-regularized policy gradi-
ent. (Liu et al., 2020) further establishes the global convergence rates of various variance-reduced
policy gradient methods. Inspired by the analysis of (Agarwal et al., 2019), we establish the global
convergence result of the proposed HPO-AM algorithms.
Global Convergence of TRPO and PPO. Regarding TRPO, (Shani et al., 2020) presents the global
convergence rates of an adaptive TRPO, which is established by connecting TRPO and the mirror
descent method. (Liu et al., 2019) proves global convergence in expected total reward for a neural
variant of PPO with adaptive Kullback-Leibler penalty (PPO-KL). To the best of our knowledge,
(Liu et al., 2019) appears to be the only global convergence result for PPO-KL. By contrast, our
focus is PPO-clip. Given the salient algorithmic difference between PPO-KL and PPO-clip, there
remains no proof of global convergence to an optimal policy for PPO with a clipped objective. In
this paper, we rigorously provide the first global convergence guarantee for a variant of PPO-clip.
RL as Classification. Regarding the general idea of casting RL as a classification problem, it has
been investigated by the existing literature (Lagoudakis & Parr, 2003; Lazaric et al., 2010; Farah-
mand et al., 2014), which view the one-step greedy update (e.g. in Q-learning) as a binary classifica-
tion problem. However, a major difference is the labeling: Classification-based Approximate Policy
Iteration labels the action with the largest Q value as positive; HPO labels the actions with positive
advantage as positive. Despite the high-level resemblance, our paper is fundamentally different from
the prior works (Lagoudakis & Parr, 2003; Lazaric et al., 2010; Farahmand et al., 2014) as our paper
is meant to study the theoretical foundation of PPO-clip, from the perspective of HPO.
7	Concluding Remarks
In this paper, we propose to rethink policy optimization methods from the perspective of state-
wise policy improvement. Specifically, we propose a classification-based policy update scheme
leveraging hinge loss for policy improvement, and thereby propose anew family of algorithms called
Hinge Policy Optimization (HPO). This paper proves global convergence of HPO with an adaptive
margin (HPO-AM), which can be viewed as a PPO-clip algorithm with an adaptive clipping range.
Tabular parameterization enables us to perform policy updates for each state separately; viewing
each state-action pair as a training sample allows checking state-wise policy improvement, and the
adaptive margin ensures the existence of improved policy. We also conduct experiments in both
tabular and more complex environments for several variants of HPO-AM algorithms equipped with
different classifiers and margins to corroborate their convergence behavior and performance.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64-66. PMLR, 2020.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Dimitris Bertsimas, Jack Dunn, Colin Pawlowski, and Ying Daisy Zhuo. Robust classification.
INFORMS Journal on Optimization, 1(1):2-34, 2019.
Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv
preprint arXiv:1906.01786, 2019.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Amir-massoud Farahmand, Doina Precup, Andre Barreto, and Mohammad Ghavamzadeh.
Classification-based approximate policy iteration: Experiments and extended discussions. arXiv
preprint arXiv:1407.0449, 2014.
Yoav Freund and Robert E. Schapire. Large margin classification using the perceptron algorithm.
Machine Learning, 37(3):277-296, 1999.
Kai-Chun Hu, Ping-Chun Hsieh, Ting Han Wei, and I-Chen Wu. Rethinking deep policy gradients
via state-wise policy improvement. In ”I Can’t Believe It’s Not Better!”NeurIPS 2020 workshop,
2020.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for
Atari. In International Conference on Learning Representations, 2020.
Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In International Conference on Machine Learning, pp. 267-274, 2002.
Michail G Lagoudakis and Ronald Parr. Reinforcement learning as classification: Leveraging mod-
ern classifiers. In International Conference on Machine Learning, pp. 424-431, 2003.
Alessandro Lazaric, Mohammad Ghavamzadeh, and Remi Munos. Analysis ofa classification-based
policy iteration algorithm. In International Conference on Machine Learning, pp. 607-614, 2010.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), International Conference on Learning Representations, 2016.
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimiza-
tion attains globally optimal policy. Advances in Neural Information Processing Systems, 32:
10565-10576, 2019.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
policy gradient and natural policy gradient methods. Advances in Neural Information Processing
Systems, 33, 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820-6829, 2020.
10
Under review as a conference paper at ICLR 2022
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K RavikUmar, and AmbUj Tewari. Learning with
noisy labels. Advances in neural information processing systems, 26:1196-1204, 2013.
Johan S Obando-Ceron and Pablo SamUel Castro. Revisiting Rainbow: Promoting more insightfUl
and inclUsive deep reinforcement learning research. In International Conference on Machine
Learning, 2021.
Chen-HUan Pi, Kai-ChUn HU, Stone Cheng, and I-Chen WU. Low-level aUtonomoUs control and
tracking of qUadrotor Using reinforcement learning. Control Engineering Practice, 95:104222,
2020. ISSN 0967-0661. doi: https://doi.org/10.1016/j.conengprac.2019.104222.
Antonin Raffin. RL Baselines 3 Zoo. Available at https://github.com/DLR-RM/
rl-baselines3-zoo, 2020.
Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are
loss fUnctions all the same? Neural computation, 16(5):1063-1076, 2004.
John SchUlman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. TrUst region
policy optimization. In International Conference on Machine Learning, volUme 37, pp. 1889-
1897, 2015.
John SchUlman, Filip Wolski, PrafUlla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trUst region policy optimization: Global
convergence and faster rates for regUlarized mdps. In AAAI Conference on Artificial Intelligence,
volUme 34, pp. 5668-5675, 2020.
David Silver, GUy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning,
volUme 32, pp. 387-395, 2014.
Richard S. SUtton. Learning to predict by the methods of temporal differences. Machine Learning,
3:9-44, 1988.
Richard S. SUtton, David A. McAllester, Satinder P. Singh, and Yishay MansoUr. Policy gradient
methods for reinforcement learning with fUnction approximation. In Advances in Neural Infor-
mation Processing Systems, pp. 1057-1063, 1999.
Lingxiao Wang, Qi Cai, ZhUoran Yang, and Zhaoran Wang. NeUral policy gradient methods: Global
optimality and rates of convergence. In International Conference on Learning Representations,
2019.
Kenny YoUng and Tian Tian. MinAtar: An Atari-Inspired Testbed for ThoroUgh and ReprodUcible
Reinforcement Learning Experiments. arXiv preprint arXiv:1903.03176, 2019.
11
Under review as a conference paper at ICLR 2022
Appendix
A	Proofs of the S upporting Lemmas for Theorem 1
A.1 Additional Supporting Lemmas
Lemma 5 (Loss value and policy improvement). Suppose that π(t) (a|s) > 0, for all state-action
Pairs(S, a). Given any Z ∈ (0,1), if LSt)(θs) ≤ (1—Z)δ(t"St) forall S ∈ S, then we have ∏θ > π⑶.
Proof. Recall that
LSt)(θs)= X|A㈤(s,a)∣ '(sign(A㈤(s,a)),Ps,α(θs)- 1,eSt))	(21)
a∈A
=X|A㈤(s,a)l '(sign(A㈤(s,a)),ρs,α(θs)- 1,β^)	(22)
a∈A
≥ X δS(t) max{0, (St) — sign(A(t) (S, a))(ρS,a(θS) — 1)},	(23)
a∈A
where (22) holds by Assumption 4, and (23) holds by the definition of δS(t) and the non-negativity of
hinge loss. Then, our goal is to show the following claim: for each state s, L St)(θs) ≤ (1 — Z)δSt" St)
implies that
∏θ(a|s) ≥ (1 + ZESt))∏(t)(a∣s), a ∈ lSt)+,	(24)
πθ (a|S) ≤ (1 — ζ(St))π(t) (a|S), a ∈ IS(t)- .	(25)
We can prove the above claim by contradiction. If there exists an action a ∈ IS(t)+ such that
∏θ(a|s) < (1 + ZESt))π⑴(a|s), then We know LSt)(θs) > (1 — Z)δ(t"St) by (23). Similarly,
if there exists an action a ∈ Istt- such that ∏θ(a|s) > (1 — ZESt)Wt)(a|s), then we know
LS"(Θs) > (1 — Z)δSt)Est) by (23). Therefore, by (24)-(25) and the sufficient condition for pol-
icy improvement in Proposition 1, we know ∏θ ≥ π(t).	□
Lemma 6 (Existence of a policy with zero loss). Given that the tabular policy parameterization
is complete, i.e., any stochastic policy can be represented in the class, in each iteration t, we can
always find a policy ∏θ such that Lst) (Θs) = 0, for all S ∈ S.
Proof. Recall that ISt)+ := {a ∈ A|A(t)(s,a) > 0} and Ist)- := {a ∈ A|A(t) (s, a) < 0}. Notice
that we ignore the actions with zero advantage. We will prove that there always exists a policy πθ
such that
Sign(A(t)(s, a))(ρs,a(θ) — 1) ≥ e»	(26)
for all a ∈ IS(t)+ ∪ IS(t)-, i.e., actions such that Aπ(S, a) 6= 0, for all S ∈ S.
For state S, take
∏θ(a|s) = θs,a =(1 + α ∙三a∈Is)-----)∏(a∣s),∀a ∈ Ist)+,	(27)
'	a∈at+)+ π(aιs)√
∏θ(a|s) = θs,a = (1 — α)π(a∣s),∀a ∈ Ist)-.	(28)
We consider two cases, depending on whether the probability sum of actions with positive advantage
is bigger than those with negative advantage, under the current state.
P	(t)- n(a|s)	(t)
Case 1.	If P S---≥-1 ≥ 1, then 苴)=α.
Ea∈*)+π(alS)—
12
Under review as a conference paper at ICLR 2022
For a ∈ Is(t)+, we have
πθ⑷S) = (1 + α ∙ Pa∈Ist) ∏(a∣s) )π⑷S)
Σα∈lSt)+ WaIS)
≥ (1 + α)π(a∣s) = (1 + Est))π(a∣s).	(29)
On the other hand, for a ∈ Is(t)-, we have
∏θ (a∣s) = (1 — α)π(a∣s) = (1 — Est))π(a∣s).	(30)
P uτ(t)- πSIs)	(t)	P U √t)- n(aIs)
Case 2.	If Pa^——< 1, then Est) = α ∙ P^——.
I(t)+ π(aIs)	,	s	I(t)+ π(aIs)
a∈ s	a∈ s
For a ∈ Is(t)+, we have
πθ ⑷S)=(I+α •三 a∈Is)__--U )n(a|S) = (I+Est))n(a|S).	(31)
Σα∈lSt)+ n(a|S)
Similarly, for a ∈ Is(t)-, we know
∏θ(a∣S) = (1 — α)π(a∣S) ≤ (1 — α • 三a∈Is-----) j ) )π(a∣S)
一	∑α∈ι(t)+ WaIS)
= (I-Est))n(a|S).	(32)
Therefore, in both cases we have πθ satisfy (26) for all a ∈ Is(t)+ ∪ Is(t)- and for all S ∈ S, i.e.,
L(t)(θ) = 0.	□
A.2 Proof of Lemma 1
For convenience, we restate Lemma 1 as follows.
Lemma (Strict improvement and strict positivity of policy under HPO-AM). Suppose π(t) is strictly
positive in all state-action pairs, i.e., π(t) (aIS) > 0, for all (S, a). Under HPO-AM, if EMDA takes
diminishing non-summable step sizes, i.e., ηk → 0 and Pk∞=1 ηk = ∞, then π(t+1) satisfies that
π(t+1) > π(t) and π (t+1) (aIS) > 0, for all (S, a).
Proof. We start by showing that the objective loss function L(t) (θ) is convex and LiPschitz Contin-
uous. From (18), we have
Lst)(θs) = XIA⑴(S, a)I '(sign(A⑴(S, a)),ρs,a(θs)- 1, Est)),	(33)
a∈A
=X ∣A(t)(S, a)∣ max{0, Est)-Sign(A㈤(S,a))(ρs,α(θs) — 1)}.	(34)
a∈A
By the fact that ρs,a(θs) — 1 is linear and hinge loss is convex (Rosasco et al., 2004), their com-
position '(sign(Aπ(s, a)), ρs,α(θs) — 1, Est)) preserves convexity, for each a ∈ A. Being the non-
negative weighted sum of the above convex functions, Lst)(θs) is also convex. Hence, L(t)(θ) is
also convex.
13
Under review as a conference paper at ICLR 2022
For LiPschitz continuity, it follows from the fact that for any θ,θ ∈ ∆(A)lSl,
∣max{0, Es - Sign(A(t)(s, a))(ρs,α(θs) - 1)} - max{0, Es -Sign(A(t)(s, a))(ps,a(θs) - 1)}|
≤∣(est) -sign(A(t)(s,a))(ρs,a(θs) - 1)) - (Est)-Sign(A(t)(s,a))(ρs,a(θs) - 1))| = lρs,a(θs) - ρs,a (θs)1 =∏(t)(a∣s) lπθ(Ws) - n8(a|S)I =∏(t)(a∣s) lθs,a - "s，a| ≤ (t)( I 、kθs -夕sk. π(t)(a∣s)	(35) (36) (37) (38) (39)
Thus, |A(t) (s, a)| max{0, Est) - Sign(A(t)(s, a))(ρs,α(θs) - 1)} is Lipschitz continuous with a Lip-
schitz constant "；)(：,：)|, for each a ∈ A. As a result, the summation Lst)(θs) over all actions is
2R
M-Lipschitz continuous with M = min{∏-Y(a∣s)} < ∞ (since ∏(t)(a∣s) > 0 for all s, a). Hence,
L(t)(θ) is Lipschitz continuous as well.
Therefore, under a diminishing non-summable step size, the convergence of L(t)(θ) to its minimum,
which is zero by Lemma 6, follows directly from the standard convergence result of EMDA shown
by (Beck & Teboulle, 2003, Theorem 4.1). Moreover, by the termination condition of EMDA spec-
ified in Algorithm 1, We know that the θ returned by EMDA must satisfy L(t) (θ) ≤ (1 — Z)δ(t)E(t),
which implies that Lst)(θ) ≤ (1 - Z)δst)Est), for every state S ∈ S. Hence, by Lemma 5, We have
π(t+1) > π(t).
Next, by the strict positivity of π(t), we also have δs(t) > 0 and E(st) > 0, for all s. Then, by the
termination condition of EMDA specified in Algorithm 1 and the convergence of EMDA, we know
that under HPO-AM, EMDA would terminate in finitely many iterations (i.e., finite k in Algorithm
2), for any t. Therefore, it is easy to verify that π(t+1) (a|s) > 0 for all (s, a) by the exponentiated
gradient update scheme of EMDA and the strict positivity of ∏(t).	□
A.3 Proof of Lemma 2
For ease of exposition, we restate Lemma 2 as follows.
Lemma (Monotonicity in π(t)(a∣s)). There exist To and Ti such that for all a ∈ I+, π(t)(a∣s) is
strictly increasing for t > T0;for all a ∈ Is-, π(t) (a|s) is strictly decreasing for t > T1.
Proof. Consider a ∈ Is+. Since A(∞)(s, a) > 0, there exists a T0 ∈ N such that A(t)(s, a) > 0 for
all t > T0 . This implies that
π(t+1) (a|s) - π(t) (a|s) > E(st)π(t) (a|s) > 0,	(40)
i.e., π(t)(a∣s) is strictly increasing for t > To.
The proof of the second claim for a ∈ I- is similar.	□
A.4 Proof of Lemma 3
For ease of exposition, we restate Lemma 3 as follows.
Lemma (Lower Bound ofE(st)). If Is+ is not an empty set, then there exists a positive constant c such
that E(st) is lower bounded by c, for all t > T1 .
14
Under review as a conference paper at ICLR 2022
Proof. From the fact that
£ ∏O(a∣S)AC)(S, a) =0;	(41)
a∈A
X π(∞)(a∣s)A(∞)(s, a) = 0,	(42)
a∈Is0
we have
∏(∞ π(∞)(a∣s)A(∞)(s,a) = E π(∞)(a∣s)∣A(∞)(s, a)|.	(43)
a∈Is	a∈Is
By Assumption 1, We have	|V(∞)(s)∣	≤	ιRγ,	∣Q(∞)(s,	a)|	≤	ιRγ and thus ∣A(∞)(s,	a)|	=
∣Q(∞)(s, a) - V(∞)(s)∣ ≤ IyY. Therefore, by Holder,s inequality, We have
E ∏(∞)(a∣s) ≥
a∈Is-
Pa∈I+ N∞)⑷S)AC)(S,a)
-2R-
1-Y
(44)
Define ∆ := mina∈I+ A(∞)(S, a) > 0 and p := Pa∈I+ π(∞) (a|S). Since We knoW that p =
Pa∈ι+ ∏(∞)(a∣s) > Pa∈ι+ ∏(T0)(a∣s) by Lemma 2, we have P > 0. Moreover, by Lemma 2, we
have Is(t)- ⊇ Is- and π(t+1) (a|S) < π(t) (a|S) for all a ∈ Is- and t > T1. This implies
X π(t)(WS) > X π(t)(alS) > X π(∞)(alS) ≥ 姜	(45)
a∈lSt)-	a∈I-	a∈I-	1-γ
for all t > T1 .
Therefore, by the definition of (st), we have
for all t > T1 .
(t)	. r1 Pa∈lSt)- π() ⑷S)I
eS ) = α ∙ min口,	--------(t” I 、｝
Σa∈I(t)+ n(t)(a|S)
Pa∈I(t)- n(t)(a|S)
≥ a . -ɪs------------
一Pa∈lSt)+ n(t)(a|S)
≥ α ∙ E ∏(t)(a∣S) (= E ∏(t)(a∣S) ≤ 1)
a∈Is(t)-	a∈Is(t)+
p∆
≥ α ∙ -R- (From inequality (45))
1-Y
(46)
(47)
(48)
(49)
□
A.5 Proof of Lemma 4
For ease of exposition, we restate Lemma 4 as follows.
Lemma. Pa∈I+∪I- e(st)π(t) (a|S)|A(t) (S, a)| → 0 as t → ∞.
Proof. Convergence of V(t) (S) implies that V (t+1) (S) - V(t) (S) → 0, and by (4), the difference
between the value functions at t and t + 1 can be written as
V (t+1) (S) - V(t) (S)
= T-- X dSt+1)(S0) X ∏(t+1)(a∣S0)A(t)(S0,a).	(50)
γ s0∈S	a∈A
Following the update rule given by our classification-based scheme, π(t+1) satisfies both Proposi-
tion 1 and Proposition 2. That is,
X π(t+1) (a|S0)A(t) (S0, a) ≥ 0 ∀S0 ∈ S.	(51)
a∈A
15
Under review as a conference paper at ICLR 2022
This implies that for convergence of V (t)(s), it is sufficient to show that
dSt+1)(s0) X ∏(t+1)(a∣s0)A(t)(s0 ,a) → 0, for alls0 ∈ S.	(52)
a∈A
Let us focus on s0 = s. Simply considering the first term of the discounted state visitation frequency,
we obtain
∞
Xγt0 Pr(st0 = s|s0 = s,π(t+1))
t0=0
≥ γ0 Pr(s0 = s|s0 = s,π(t+1)) = 1,	(53)
and thus
∞
d(st+1)(s) = (1 - γ) X γt Pr(st0 = s|s0 = s, π(t+1))
t0=0
≥ 1 - γ.	(54)
From (52) and (54) we have
X^ π(t+1)(a∣s)A(t)(s, a) → 0.	(55)
a∈A
Combining the above result and the fact that P°∈a ∏(t)(a∣s)A(t)(s, a) = 0, We have
X (π(t+1) (a|s) - π(t) (a|s))A(t) (s, a) → 0.	(56)
a∈A
We split the summation over all actions into three parts, according to limits of the advantage function,
i.e.,
^X(π(t+1)(a∣s) - π(t)(a∣s))A(t)(s, a)
a∈A
=^X (π(t+1)(a∣s) — π(t)(a∣s))A(t)(s, a)+
a∈Is0
(57)
ɪ2 (∏(t+ )(a∣s) — ∏(t)(a∣s))A(t)(s, a)+
a∈Is+
X (π(t+1)(a|s) — π(t)(a∣s))A(t)(s, a)
a∈Is-
Let T = max{T0, T1}, Where T0, T1 are the constants defined in Lemma 2. Then, for a ∈ Is+ ∪ Is-,
A(t)(s, a) is of the same sign as A(∞)(s, a), for all t > T.
After moving the term of summation over Is0 in (57) to the left hand side, by our classification-based
scheme, We have
X (π(t+1) (a|s) — π(t) (a|s))A(t) (s, a) — X (π(t+1) (a|s) — π(t) (a|s))A(t) (s, a)	(58)
a∈A	a∈Is0
=^X (π(t+1)(a∣s) — π(t)(a∣s))A(t)(s, a) + ^X (π(t+1)(a∣s) — π(t)(a∣s))A(t)(s, a)	(59)
a∈Is+	a∈Is-
=^X (π(t+1)(a∣s) — π(t)(a∣s))A(t)(s, a) — ^X (π(t+1)(a∣s) — π(t)(a∣s))∣A(t)(s, a)|	(60)
a∈Is+	a∈Is-
≥ X ZeSt)∏(t)(a∣s)A(t)(s,a) + X Ze't)∏㈤(a|s)|A⑴(s, a)|	(61)
a∈Is+	a∈Is-
= X	Zest)π⑴(a|s)|A㈤(s,a)∣ > 0,	(62)
a∈Is+∪Is-
16
Under review as a conference paper at ICLR 2022
for all t > T. Note that (61) follows from the same argument as (24)-(25). By the fact that
^X(π(t+1)(a∣s) — π(t)(a∣s))A(t)(s, a) → 0	(63)
a∈A
and
^X (π(t+1)(a∣s) — π(t)(a∣s))A(t)(s, a) → 0,	(64)
a∈Is0
we have (58) converges to zero. Finally, by the sandwich theorem, we have
X	(st)π(t) (a|s)|A(t) (s, a)| → 0.	(65)
a∈Is+ ∪Is-
□
B Proofs of Corollaries 1 and 2
B.1	Proof of Corollary 1
From Theorem 1, we know that Is+ is an empty set, for all s ∈ S . Therefore, the result of Lemma 4
becomes
X (st)π(t) (a|s)|A(t) (s, a)| → 0.	(66)
a∈Is-
This implies that either Est) → 0 or Pa∈∕- ∏(t)(a∣s) → 0. By Lemma 2, Ist)- ⊇ I-, for all
t > Ti. This implies that Pa∈ι(t)- ∏(t)(a∣s) > P°∈∕- ∏(t)(a∣s), for all t > Ti. Moreover, We
have that Est) → 0 implies PaaE-[(t)- π(t)(a∣s) → 0 and thereby implies Pa∈∕- ∏(t)(a∣s) → 0, i.e.,
Pa∈ι- π(t)(a∣s) → 0 is a necessary condition for Est) → 0. Hence, we know
Xl π(t)(a∣s) → 0	(67)
a∈Is-
regardless of the behavior of E(st).
By combining Pa∈∕- ∏(t)(a∣s) → 0 and the fact that I+ is empty, we have
X π(t) (a|s) → 1.	(68)
a∈Is0
□
B.2	Proof of Corollary 2
Since there is only one action a ∈ Is0 and Is+ is empty, for all a0 ∈ A \ {a}, we have a0 ∈ Is-. By
Lemma 2, there exists Ti such that for all t > Ti, A(t)(s, a0) < 0, for all a0 ∈ A\ {a}. This implies
that A(t) (s, a) > 0, for all t > Ti, i.e., for all t > Ti, we have a ∈ Is(t)+ and a0 ∈ Is(t)-, for all
a0∈A∖{a}.
Then, by Corollary 1, as t → ∞
	E π(t)(Ols) = a∈Is(t)+	π(t) (als) → 1 a∈Is0	(69)
	X π(t)(als) =	X π(t)(als) → 0.	(70)
	a∈Is(t)-	a∈Is-	
Hence, by definition, E(st)	二 ɑ ∙ min{1, Pa*1(" a∈Is(t)+	π(t) (a|s) ∙∏ww }→ 0 as t →∞.	□
17
Under review as a conference paper at ICLR 2022
C Proof of Theorem 2
For ease of exposition, we restate Theorem 2 as follows.
Theorem. Theorem 1 is also satisfied for the following classifiers with corresponding margins with
0 < α < 1:
(i)	For classifier log(∏θ (a∣s)) 一 log(π(a∣s)) ,let
π
s
log(1 + α ∙ min{1,

Ea∈"- π⑷S)
Pa∈"+∏(a∣s) })
(ii)	For classifier ∕ρs,a(θ) — 1, let
π _ 二	.ʃ] Pa∈IS- n(aIS)I	1
Cs =	1 + α ∙ min{1,	一 1.
V	Ea∈"+ n(aIS)
(71)
(72)
Proof. The proof of Theorem 2 follows exactly the same procedure as that of Theorem 1. We
sketch the proof for the two different classifiers: To begin with, we extend the supporting lemmas to
HPO-AM with these two alternative classifiers:
•	Notice that by using exactly the same argument as Lemma 5, we have that under these two clas-
Sifiers, strict policy improvement ∏θ > ∏(t) is guaranteed if Ls(θs) ≤ (1 — Z)δst)cst) for all s,
where ζ ∈ (0, 1).
•	Regarding Lemma 6, the policy πθ we construct in the proof of Lemma 6 also achieves zero loss
for the alternative loss functions induced by the two alternative classifiers.
•	Regarding Lemma 1, since both the logarithm function and the square root function are concave,
and hinge loss is convex and non-increasing, their compositions preserve convexity (Boyd et al.,
2004). Moreover, by the clipping property of hinge loss, we also know that in each iteration t,
Ls(θs) is Lipschitz continuous under the two classifiers log(ρs,a(θs)) and	(ρs,a(θs)) 一 1 if
the policy ∏(t) is strictly positive in all state-action pairs. Hence, L(θ) also remains LiPschitz
continuous. By using the same argument of convergence of EMDA as that in Lemma 1, it is easy
to verify that π(t+1) > π(t) and π(t+1)(aIS) > 0 for all (S, a) if EMDA takes diminishing and
non-summable step sizes.
•	Based on the extended version of Lemma 5 and Lemma 1 for these two alternative classifiers and
Assumptions 3-4, we know that under the design of HPO-AM, strict policy improvement (i.e.,
π(t+1) > π(t)) and strict positivity of π(t) are satisfied in every iteration t. Therefore, the limits
V (∞), Q(∞), and A(∞) exist by monotone convergence theorem.
•	Lemma 2 does not use the specific form of the classifier and hence still holds under the two
alternative classifiers.
•	For the two alternative classifiers, Lemma 3 can be readily extended by adapting the argument in
(46)-(49) to the Csπ defined in (19) and (20). Similarly, Lemma 4 can be extended by adapting the
inequalities in (61)-(62) to the corresponding Csπ defined in (19) and (20).
Since all the lemmas hold for these alternative classifiers and their corresponding Csπ , we can com-
plete the proof by showing a contradiction similar to that in Theorem 1.
□
D Unproven Classifiers
In Remark 6, We identify two other classifiers, ∏(a∣s) — π(a∣s) and (ρs,α(θ))2 — 1, which We cannot
directly apply similar proof as Theorem 1.
18
Under review as a conference paper at ICLR 2022
First, an intuitive reason for the classifier ∏θ (a∣s)-∏(a∣s) is explained as follows. To ensure the exis-
tence of improved policy, we must set Cs = mina∈A{∏(α∣s)}∙α∙min{1,
Ea∈ιπ- n(a|s)
Pa ∈I∏+ n(aIs)
a∈Is
0<α< 1.
Note that there is an additional term in front, compared to the margin we use in Theorem 1 before.
Due to multiplying this additional term, the margin is bounded by the minimum probability of
action. We cannot use Lemma 3 to give a lower bound for cst), since mina∈A{π(t)(a∣s)} will prob-
ably approach zero as t → ∞. To be more specific, it makes the probability of all the other actions
increase/decrease slowly, when there exists an action with extremely small probability. Without
Lemma 3, we are not able to prove the global convergence by contradiction.
Second, for the classifier (ρs,a(θ))2 - 1, although we can prove the existence of improved policy by
defining the margin as Cs = (1 + α ∙ min{1, Pa∈Iπ十n；|； })2 - 1, our loss function is no longer
convex. Without convexity, the EMDA method is nsot guaranteed to converge to the optimal value.
We leave it as an interesting open question of whether or not this problem can be solved by using a
non-convex optimization method.
E Ablation Study of HPO-AM via Experiments in a Tabular
Environment
E.1 Gridworld Environment
For a principled discussion, we use a 4 × 4 gridworld similar to that in (Sutton, 1988, Example 4.1),
where an optimal policy and all the value functions can be analytically obtained. Regarding the
gridworld environment, there are four available actions (up, down, right, left) and a terminal state
located at the top-left corner. At each non-terminal state, the reward is either 1.0 or -1.2 with equal
probability, for all the actions. All the state transitions are deterministic.
E.2 Experimental Settings
In the gridworld environment, all the policies in our experiments are trained for 40000 episodes un-
der 10 fixed random seeds. All the source code is in supplementary.zip of the supplementary
materials. Our code of the HPO algorithms is extended from an open-source PyTorch example3, and
the gridworld environment is modified from an open-source project, gym-gridworld4.
E.3 Neural Network Model
Our neural network model is composed of three layers: two fully connected layers and one output
layer, which includes two output heads for the value and the action. The two fully connected layers
with 32 hidden units are shared for the output heads. The learning rate and the discount factor (γ)
are set to be 0.001 and 0.99, respectively. The input of the model is a one-hot vector to represent the
grid position.
E.4 Ablations for the weight and margin
Recall that the loss function of HPO in (12) involves three critical components: a classifier, a weight,
and a margin. Regarding the classifier, we evaluate five variants of HPO-AM with the classifiers ,
including:⑴ ρs,a(θ) - 1, (the standard HPO-AM); (ii) log(∏θ(a∣s)) - log(π(a∣s)) (HPO-AM-log);
(iii) PPTW- 1 (HPO-AM-root); (iv) ∏(a|s) - π(a∣s) (HPO-AM-sub); (V)(Ps,a(θ))2 - 1 (HPO-
AM-square). Regarding the weight and the margin, we consider four different scenarios, including:
(i) adaptive Cs and ∣^l(t)(sk,α+ )| as the weight (denoted as WAE); (ii) adaptive Cs and a constant
weight of 1 (denoted as AE); (iii) constant Cs = 0.1 and ∣√l(t)(sk,aQ∣ as the weight (denoted as
WCE); (iv) constant Cs = 0.1 and a constant weight of 1 (denoted as CE). For the constant margin,
we follow the popular setting of PPO-clip to set C = 0.1. Those methods are implemented with a
3https://github.com/pytorch/examples/blob/master/reinforcement_
learning/actor_critic.py
4https://github.com/podondra/gym-gridworlds
19
Under review as a conference paper at ICLR 2022
tabular policy or a neural network policy. Under all the algorithms, the policy is trained for 40000
episodes under 10 random seeds.
E.5 Training curves
We show the L1 norm between HPO-AM policy with the optimal policy in Figure 2 and Figure 3.
First, we evaluate the five variants of HPO-AM under tabular parameterization with true advantage
values in Figure 2. This experiment is meant to verify the convergence behavior of HPO-AM.
HPO-AM
HPO-AM-Iog
HPO-AM-root
HPO-AM-SUb
H PO-AM-square
N2
5k IOk 15k 20k 25k 30k 35k 40k
Training Steps
5k IOk 15k 20k 25k 30k 35k 40k
Training Steps
HPO-AM
HPO-AM-Iog
HPO-AM-root
HPO-AM-sub
H PO-AM-square
HPO-AM
HPO-AM-Iog
HPO-AM-root
HPO-AM-sub
H PO-AM-square
N2
HPO-AM
HPO-AM-Iog
HPO-AM-root
HPO-AM-sub
H PO-AM-square
5k IOk 15k 20k 25k 30k 35k 40k
Training Steps
5k IOk 15k 20k 25k 30k 35k 40k
Training Steps
4
4
(a) WAE
(b) AE
(c) WCE
(d) CE
Figure 2: Experimental results of tabular policies in the gridworld environments.
We further implement the five variants of HPO-AM using neural networks with estimated advantage
values. Figure 3 reveals the mean value and the standard deviation of L1 norm. Most of the five
algorithms can converge within a reasonable number of training episodes, while some of HPO-AM-
sub with WAE and WCE settings cannot converge.
——HPO-AM
——HPO-AM-Iog
——HPO-AM-root
—— HPO-AM-sub
--- HPO-AM-square
——HPO-AM
——HPO-AM-Iog
——HPO-AM-root
—— HPO-AM-sub
--- HPO-AM-square
——HPO-AM
—— HPO-AM-Iog
——HPO-AM-root
——HPO-AM-sub
--- HPO-AM-square
——HPO-AM
—— HPO-AM-Iog
——HPO-AM-root
—— HPO-AM-sub
--- HPO-AM-square
(a) WAE
(b) AE
(c) WCE
(d) CE
Figure 3: Experimental results of neural network policies in the gridworld environments.
E.6 Discussions
We measure the L1 norm between the policy learned under each variant of HPO-AM with the op-
timal policy as the performance criterion. First, we evaluate the five variants of HPO-AM under
tabular parameterization with true advantage values. This experiment is meant to verify the con-
vergence behavior of HPO-AM. As expected, HPO-AM, HPO-AM-log and HPO-AM-root indeed
converge to the optimal policy (in a small number of training episodes). Interestingly, HPO-AM-sub
and HPO-AM-square, which are based on the two weak classifiers without theoretical guarantees as
discussed in the previous section, still converges. These results serve as empirical insights into the
convergence analysis for these two classifiers. We further implement the five variants of HPO-AM
using neural networks with estimated advantage values. Most of the five algorithms can converge
within a reasonable number of training episodes. We also observe that the HPO-AM algorithms
appear to converge slightly faster than PPO-clip. This manifests the potential of the proposed HPO
framework in opening up a new class of promising RL algorithms.
F Detailed Configurations of the Experiments in MinAtar
F.1 Experimental Settings
For the experiments in the MinAtar environments (Young & Tian, 2019), we implement HPO-AM
and its variants with WAE setting on top of the open-source RL Baselines 3 Zoo framework (Raf-
fin, 2020). Recall the classifier is a critical component for the loss function of HPO in (12), we
evaluate five variants of HPO-AM with the classifiers, including: (i) ρs,a (θ) - 1, (the standard
20
Under review as a conference paper at ICLR 2022
HPO-AM); (ii) log(∏θ(a∣s)) - log(π(a∣s)) (HPO-AM-log); (iii) √ρs,α(θ) - 1 (HPO-AM-root);
(iv) ∏θ(a|s) - π(a∣s) (HPO-AM-sub); (V)(Ps,α(θ))2 - 1 (HPO-AM-square). InMinAtar, the obser-
vation is encoded by 10 × 10 × n state representations, where each of the n channels corresponds to a
game-specific object. For example, in Breakout, the game-specific objects are ball, paddle and brick.
We compare HPO-AM Variants with PPO-clip, A2C, and Rainbow in Asterix, Breakout and Freeway
with 5 million training steps among 5 random seeds. The source code is in supplementary.zip
of the supplementary materials.
F.2 Neural Network Model
The network model of HPO-AM is composed of two parts: a shared feature extractor and an output
layer (including two sets of output heads, one for the Q-Value and the other for the policy). For
the MinAtar experiments, the feature extractor consists of one 2D conVolution layer (with 3 × 3
kernel and 16 output features) and one fully connected layer with 128 hidden units. Regarding
A2C and PPO-clip, we set the entropy coefficient to 0 to disable the entropy regularization for a
fair comparison and follow the configuration suggested by RL Baseline3 Zoo5 for the rest of the
hyperparameters. Table 1 summarize the hyperparameters for MinAtar and all HPO-AM Variants
use the same setting. For each MiniAtar game, we run the experiments under 5 random seeds with
5,000,000 training steps for all methods.
Table 1: Parameters for the MinAtar experiments (lin_2.5e-4 means that the learning rate decays
linearly from 2.5 × 10-4 to 0).
Hyperparameters	HPO-AM	PPO-clip	A2C
n_envs	8	8	16
n_steps	64	128	-5-
batch_size	-256-	512	80
n_epochs	10	4	-
learning_rate	lin_2.5e-4	lin25e-4	7e-4
vf_coef	08	0.5	0.5
α	0.5	-	-
G Comparison of the clipped objective in PPO-clip and the hinge
loss objective of HPO
Recall that the original objectiVe of PPO-clip is
LCIip(θ)= Es〜dμ0,α〜∏(∙∣s) [min{ρs,α(θ)Aπ(s, a),CliP(Ps,。(θ), 1 - e, 1 + e)Aπ(s, a)}],	(73)
where ρs,α(θ) = *(*). In practice, Lclip(θ) is approximated by the sample average as
LCliP(θ) ≈ LCliP(θ) = ɪ X min{ρs,a(θ)Aπ(s,a), clip(ρs,a(θ), 1 - e, 1 + e)Aπ(s,a)}
| π | (s,a)∈Dπ
(74)
1
|Dn |
V" ∣Aπ(s, a)∣ ∙ min{ρs,α(θ) Sign(Aπ(s, a)), CliP(Ps,。(θ), 1 - e, 1 + e) Sign(An(s, a))}.
X---------------------------------------------------------------------------------------------}
^^{z^^^-
Wscl,iap (θ)
(s,a)∈Dπ
(75)
5https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/benchmark.md
21
Under review as a conference paper at ICLR 2022
Note that Wsc,laip (θ) can be further written as
	1 + e	, if Aπ(s, a) > 0 and Ps,a(θ) ≥ 1 + e
	Ps,a(θ)	, if Aπ(s,α) > 0 and ρs,α(θ) < 1 + e
W黑 θ)= <	-ρs,a (θ)	, if Aπ (s, a) < 0 and ρs,a (θ) > 1 - e
	-(1 — e) , if Aπ(s, a) < 0 and ρs,a(θ) ≤ 1 — e
	0	, otherwise
Recall from (11) that the loss function of HPO takes the form as	
1 L(θ) ≈ L(O) = ∣^5^^∣ lDn |	X ∣Aπ(s, a)l ∙ max {0, e - (ρs,a(θ) - 1)sign(Aπ(s, a))} .	(76) |_	- -	一}
	…DD	=Ws0(^
Similarly, Ws,a(θ) can be further written as	
	0	, if Aπ(s, a) > 0 and ρs,a(θ) ≥ 1 + e
	-ps,a(θ) + (1 + e)	, if Aπ(s, a) > 0 and ps,a(θ) < 1 + e
Ws,a ⑹=<	ρs,a(θ) - (1 - e)	, if Aπ (s, a) < 0 and ρs,a (θ) > 1 - e
	0	, if Aπ(s,a) < 0 and ρs,a(θ) ≤ 1 - e
	e	, otherwise
Therefore, it is easy to verify that Lclip(θ) and -L(θ) only differ by a constant with respect to θ.
___ ^ .ι∙ . , -、	__ ^ ,..
This also implies that VθLcIiP (θ) = -▽&L(θ).
22