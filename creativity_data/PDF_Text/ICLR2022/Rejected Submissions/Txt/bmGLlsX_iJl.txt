Under review as a conference paper at ICLR 2022
EMFlow: Data Imputation in Latent Space via
EM and Deep Flow Models
Anonymous authors
Paper under double-blind review
Ab stract
The presence of missing values within high-dimensional data is an ubiquitous
problem for many applied sciences. A serious limitation of many available data
mining and machine learning methods is their inability to handle partially miss-
ing values and so an integrated approach that combines imputation and model
estimation is vital for down-stream analysis. A computationally fast algorithm,
called EMFlow, is introduced that performs imputation in a latent space via an
online version of Expectation-Maximization (EM) algorithm by using a normal-
izing flow (NF) model which maps the data space to a latent space. The proposed
EMFlow algorithm is iterative, involving updating the parameters of online EM
and NF alternatively. Extensive experimental results for high-dimensional multi-
variate and image datasets are presented to illustrate the superior performance of
the EMFlow compared to a couple of recently available methods in terms of both
predictive accuracy and speed of algorithmic convergence.
1	Introduction
Missing values are often encountered for real-world datasets and are known to adversely impact
the validity of down-stream analysis. Most machine learning (ML) algorithms and statistical tools
often ignore the problem of partially observed features by simply dropping entire cases with miss-
ing values, which could result into biased estimation and underestimation of parameter uncertainty
(Schmitt et al., 2015; Somasundaram & Nedunchezhian, 2011; Lall, 2016).
There have been some recent attempts to develop ML methods that properly handle missing values.
Early works in this field performed imputation in a supervised manner where a complete training
set is needed to learning the correlation between missing and observed entries (e.g. Garcla-Laencina
et al., 2010; Rezende et al., 2014; Bertalmio et al., 2000; Xie et al., 2012; Yeh et al., 2016). How-
ever, it is common that a large collection of fully observed data is hard to acquire, which makes those
data-greedy supervised algorithms less effective. On the other hand, early attempts in unsupervised
imputation methods, such as the collaborative filtering (Sarwar et al., 2001), usually learn the corre-
lation across dimensions by embedding the data into a lower dimensional latent space linearly (e.g.
Little & Rubin, 2019; Audigier et al., 2016). To improve the limited representation power of those
linear models, kernel-based methods are also proposed but at the cost of excessive computational
load when dealing with large datasets (e.g. Sanguinetti & Lawrence, 2006; Liu et al., 2016).
More recently, several imputation methods based on deep generative models have been proposed,
providing much more accurate estimates of missing values especially for high-dimensional image
datasets. In this work, we introduce EMFlow that integrates the normalizing flow (NF) (Dinh et al.,
2016; Rezende & Mohamed, 2015; Dinh et al., 2014; Kingma & Dhariwal, 2018) with an online
version of Expectation-Maximization (EM) algorithm (CaPPe & Moulines, 2009). The proposed
framework is motivated by the strength and weakness of EM. As a class of iterative algorithm de-
signed for latent variable models, EM can be applied to data imputation in an interpretable way
(Little & Rubin, 2019). Additionally, the learning of EM is usually numerically stable and its con-
vergence property has been studied extensively (e.g. Ma et al., 2000; Zhao et al., 2020; Meng et al.,
1994; Wu, 1983). However, the E-step and M-step are only traceable with simple underlying dis-
tributions including multivariate Gaussian or Student’s t distributions as well as their mixtures (e.g.
Di Zio et al., 2007; xian Wang et al., 2004). On the other hand, NF is capable of latent representa-
tion and efficient data sampling, which makes it a convenient bridge between the data space and the
1
Under review as a conference paper at ICLR 2022
latent space. Therefore, we let EM perform imputation in the latent space where simple inter-feature
dependency is assumed (i.e. the underlying distribution is multivariate Gaussian). Meanwhile, NF
is used to recover the mapping between the complex inter-feature dependency in the data space and
the one in the latent space learned by EM.
The inference of EMFlow adopts an iterative learning strategy that has been widely used in model-
based multiple imputation methods where the initial naive imputation is refined step by step until
convergence (e.g. Gondara & Wang, 2017; Buuren & Groothuis-Oudshoorn, 2010; Stekhoven &
Buhlmann, 2012). Specifically, three steps are performed alternatively:
•	update the density estimation of complete data including the observed and current imputed
values;
•	update the base distribution (i.e. μ and Σ) in the latent space by EM; and
•	update the imputation in the data space.
Note that the first update corresponds to optimizing the complete data likelihood as well as the
reconstruction error computed on the observed entries. We also derive an online version ofEM such
that it only consumes a batch of data at a time for parameter updates and thus can work with deep
generative models smoothly. We will show that such learning schema has simpler implementation
and leads to faster convergence compared to other competing methods.
The main contributions of this work are
(i)	an imputation framework combining an online version of Expectation-Maximization (EM)
algorithm and the normalizing flow;
(ii)	an iterative learning schema that alternatively updates the inter-feature dependency in the
latent space (i.e. the base distribution) and density estimation in the data space;
(iii)	a derivation of online EM in the context of missing data imputation; and
(iv)	extensive experiments on multiple image and tabular datasets to demonstrate the imputation
quality and convergence speed of the proposed framework.
2	Related work
Recently, the applications of deep generative models like Generative Adversarial Networks (GAN)
(Goodfellow et al., 2014) have been extended to the field of missing data imputation under the as-
sumption of Missing at Random (MAR) or Missing Completely at Random (MCAR). GAIN (Yoon
et al., 2018) designs a compete data generator that performs imputation, and a discriminator to dif-
ferentiate imputed and observed components with the help ofa hint mechanism. MisGAN (Li et al.,
2019) introduces another pair of generator-discriminator that aims to learn the distribution of miss-
ing mask. However, training GAN-based models is a notoriously challenging for its excessively
complex structure and non-convex objectives. For example, MisGAN optimizes three objectives
jointly involving six neural networks for imputation. Furthermore, GAN-based models do not ob-
tain explicit density estimation that could be critical for down-stream analysis (e.g. Ferdosi et al.,
2011; Zambom & Ronaldo, 2013).
Some imputation techniques based on Variational Autoencoders (VAE) (Kingma & Welling, 2013)
are also developed. For example, MIWAE Mattei & Frellsen (2019) leverages the importance-
weighted autoencoder (Burda et al., 2015) and optimizes a lower bound of the likelihood of observed
data. But the zero imputation adopted by it can be problematic since observed entries can also be
zero. It also needs quite large computational power to make the bound to be tight. EDDI (Ma
et al., 2018) avoids zero imputation by introducing a permutation invariant encoder. Imputation
under Missing not at Random (MNAR) has also been explored by explicitly modeling the missing
mechanism (Ipsen et al., 2020; Collier et al., 2020). However, all VAE-based approaches only permit
approximate density estimation no matter how expressive the inference model is.
Our proposed framework builds on the work of MCFlow (Richardson et al., 2020) that utilizes NF
to learn exact density estimation on incomplete data via an iterative learning schema. The core
component of MCFlow is a feed forward network that operates in the latent space and attempts to
find the likeliest embedding vector in a supervised manner. Although MCFlow achieves impressive
performance compared to other state-of-art methods, it remains ambiguous that how the feed forward
2
Under review as a conference paper at ICLR 2022
network exploits the correlation in the latent space. A key difference between the MCFlow and
our proposed method is that we exploit the correlation in the latent space which in turn induces
correlation in ambient space via NF. We also find that the inference of MCFlow often has slow
convergence speed and can be unstable.
3	Approach
3.1	Problem Definition
We define the complete dataset X as a collection of p-dimensional vectors {x1 , . . . , xn} that are
independent and identically distributed (i.i.d.) samples drawn fromPX (∙; θ) in ap-dimensional data
space X. The missing pattern of xi is described by a binary mask mi ∈ {0, 1}p such that xij is
missing if mij = 1, and xij is observed if mij = 0.
Let xo and xm be the observed and missing parts of x, and pM (m|x) = pM (m|xo, xm) be the
conditional distribution of the mask. Based on dependency between m and (xo, xm), the missing
mechanism can be classified into three classes (Little & Rubin, 2019):
•	MCAR: pM(m|xo, xm) = pM (m)
•	MAR: pM (m|xo, xm) = pM (m|xo)
•	MNAR: The probability of missing depends on both xo and xm .
Throughout this paper, we only focus on MCAR or MAR where the missing mechanism can be
safely ignored. The typical objective of learning a latent model is to maximize the observed data
log-likelihood defined as
Lobs
n
(θ) = Xlog
i=1
pX (xio, xim; θ) dxim.
(1)
However, our goal in this work is to estimate pX from incomplete data as well as obtain accurate
imputation under pX. Therefore, we attempt to learn the reconstructed data Xb = (xb1, . . . , xbn)T and
the estimated model parameter θ via
n
/O R∖	V^1	/ IC∖	,7
(X,θ) = argmax> log PX (x∕θ)	(2)
xi∈Xi0,θ i=1
where Xi0 ⊆ X is the search space for xi where the observed locations have fixed values.
3.2	Normalizing Flows
To make it possible to optimize equation 2, PX (x; θ) needs to be specified in a parametric way that
should be expressive enough, as the density is potentially complex and high-dimensional. To this
end, we use NF to modelPX(x; θ) as an invertible transformation fψ of a base distributionPZ(z; φ)
in the latent space Z . Under the change of variables theorem, the complete data log-density is
specified as
(3)
where θ = (ψ, φ).
fψ is usually composed by a sequence of relatively simple transformations to approximate arbi-
trarily complex distributions with high representation power. In this work, we choose Real NVP
(Dinh et al., 2016) based on affine coupling transformations as the flow model1. Recently, Teshima
1See appendix A for the details of Real NVP.
3
Under review as a conference paper at ICLR 2022
et al. (2020) has shown that flow models constructed from affine coupling layers can be universal
distributional approximators.
Data imputation relies on the inter-feature dependency that is usually intractable and hard to capture
in the data space X with the presence of missing entries. Therefore, we make the following two
assumptions related to NF.
Assumption 1: The inter-feature dependency in the latent space Z is simple and can be character-
ized by a multivariate Gaussian density, that is:
PZ (z; φ) = N(z; μ, ∑)	(4)
where φ = (μ, Σ) consists of latent parameters; mean vector μ and covariance matrix Σ.
Note that the base distribution is usually chosen as a standard Gaussian distribution (with μ = 0 and
Σ = Ip) in the literature for simplicity. However, the covariance Σ is essential in our imputation
work to represent the inter-feature dependency.
Assumption 2: Given that the transformations involved in NF are feature-wise (e.g. Dinh et al.,
2016; 2014), we expect NF to learn the mapping between the complex inter-feature dependency in
the data space X and the simple one in the latent space Z.
3.3	ONLINE EM
EM is a class of iterative algorithms for latent variable models including missing data imputa-
tion(Little & Rubin, 2019). In EMFlow, it works in the latent space Z where the underlying dis-
tribution is N(z; μ, Σ). Given the embedding vectors {zι,..., Zn} where Zi = f-1(xi), and the
corresponding missing mask {mι,..., mn}, EM aims to estimate (μ, Σ) in an iterative way:
μ(t+1) = g μ(μ(t), ∑(t); {zi, mi}n=ι)
∑(t+1) = g∑(μ(t), ∑⑴;{zi,mi}n=ι)
where (μb(t), Σ(t)) are the estimates at the tth iteration, and {g“(∙), gχ(∙)} denote the mappings
between two consecutive iterations 2.
Given estimates (μ, Σ), the missing part Zm is imputed by its conditional mean given the observed
part zio :
-1
维=E(Zm |z0； μ, ∑) = μmi + ∑miθi (∑OiOi)	(Zo - MoJ	(6)
where Oi is the observed mask (i.e. the complement of m/ and the subscripts of (μ, Σ) denote the
slicing indexes.
When processing datasets of large volume, EM becomes impractical because it needs to read the
whole data into the memory for each iteration. Following the framework introduced by CaPPe &
Moulines (2009), we derive an online version ofEM algorithm in the context of data imputation. Let
B ⊂ {1, . . . , n} denote a mini-batch of sample indexes, the online EM first obtains local estimates
from a batch of data:
μlocal = gμ(μ'" , £("； {zi, mi}i∈B )
Σlocal = g∑(μ(t), ∑(t)； {Zi, mi}i∈B)
The global estimates are then updated in the fashion of weighted average:
μ(t+I) = pt+ιμiocai + (1 — pt+ι)μ(t)
∑ (t+1) = Pt+1∑ local + (1 — Pt+1)∑(t)
2See appendix B for the details of gμ and g∑.
(7)
(8)
4
Under review as a conference paper at ICLR 2022
Latent SPaCe 2
Figure 1: EMFlow architecture.
where (ρ1, ρ2, . . .) are a sequence of step sizes satisfying
∞∞
0 < ρt < 1,	ρi = ∞ and	ρi2 < ∞	(9)
In this work, we use a step size schedule defined by
ρt = Ct-γ, t= 1,2,...	(10)
where C is a positive constant and γ ∈ (0.5, 1].
3.4 Architecture and Inference
EMFlow is a composite framework that combines NF and online EM. As illustrated in Fig 1, NF is
the bidirectional tunnel between the data space and the latent space, aiming to learn the complete
data density pX . In the latent space, the online EM estimates the inter-feature dependency of the
embedding vectors and performs imputation. To address the issue that NF needs complete data
vectors for computation, the incomplete data Xin are imputed naively (e.g. median imputation
for tabular datasets) at the very beginning to get the initial current imputed data Xb . Afterwards,
the objective in equation 2 is optimized in an iterative schema, where each iteration consists of a
training phase and a re-imputation phase.
Training Phase At this phase, the current imputed data Xb stay fixed, while the parameter estimates
^
^
of NF (i.e. ψ) and base distribution (i.e. μb and Σ) are updated in different ways.
τ-<∙	, r 11	♦ ,ι	, J ,	， ι	τ , ∙ι	n r/ ^ S∖	ι ι f	ι	ι
First of all, given the current estimated base distribution N(∙; μ, Σ), the flow model fψ are learned
by minimizing the negative log-likelihood of a batch of the current imputed data Xb B :
1
Lι(ψ) = —|B| £logPX(Xi；ψ,μ,ς)
|B| i∈B
(11)
where |B| denotes the batch size.
The computation of L1 requires exact likelihood evaluation that is equipped by NF. Once the flow
model parameters get updated, we obtain the embedding vectors in the latent space:
zi=fψ-b1(Xbi),	i∈B
(12)
5
Under review as a conference paper at ICLR 2022
Although the embedding vectors are complete, they are treated as incomplete using the missing
masks in the data space {mi}i∈B, given that the invertible mapping parameterized by fψ is feature-
wise. Therefore, the online EM imputes the missing parts of the embedding vectors with the current
global estimates (μ, Σ):
b=E(zm∣zθ;μ,∑), i ∈ B
(13)
which results in new embedding vectors {bz}i∈B where zbi consists of the observed part zio and the
imputed part bm. After the imputation, the global estimates (μ, Σ) are also updated following
equation 7 and equation 8.
Since the base distribution has been changed, it’s necessary to update the flow model fψ again by
optimizing a composite loss:
L2(ψ)
-∏Bi X [logPX(Xi； ψ, μ, Σ) - αLrec(ei, bi, m，)]
|B| i∈B
(14)
where xei = fψ(bzi), and Lrec(xei, xbi, mi) is the reconstruction error only for non-missing values:
p
Lrec(xei, xbi, mi) =	(1- mij)(xeij - xbij)2	(15)
j=1
In this composite loss, the first term forces the reconstructed data vectors {xe}i∈B to have high
likelihood in the data space, while the second term encourages {xe}i∈B to match the observed parts
of the incomplete training data. And since {xe}i∈B are transformed from {bz}i∈B via fψ, both terms
are conditioned on the inter-feature dependency learned by EM. A pseudocode for this training phase
is presented in Algorithm 1, and the implementation details that aim to make the training phase more
stable is presented in appendix D.
Algorithm 1 Training Phase
1:	Input: Current imputation: Xb = (xb1, . . . , xbn)T, missing masks M = (m1, . . . , mn)T, initial
estimates of the base distribution (μ(0), Σ(0)), online EM step size sequence: pi, ρ2,...,ρt,...
2:	for t = 1to Tepoch do
3:	Get a mini-batch Xb B = {xbi}i∈B
4:	# update the flow model
5:	Compute L1 in equation 11
6:	Update ψ via gradient descent
7:	# update the base distribution
8:	zi = fψ-1(xbi),	i ∈ B
9:	Impute in the latent space with (fb(t-i'), Σ(t-i)) to get {bi}i∈B via equation 13
10:	Obtain updated (μb(t), Σ(t)) via equation 7 and equation 8
11:	# update the flow model again
12:	xei = fψ (bzi), i ∈ B
13:	Compute L2 via equation 14
14:	Update ψ via gradient descent
Re-Imputation Phase After the training phase, the re-imputation phase is executed to update the
current imputation Xb .
This procedure is similar to that in the training phase, except that all model
parameters are kept fixed. As shown by the last line of Algorithm 2, the missing parts of Xb are
replaced with those of the reconstructed data vectors, while the observed parts are kept the same.
6
Under review as a conference paper at ICLR 2022
Algorithm 2 Re-imputation Phase
1:	Input: Current imputation: Xb = (xb1, . . . , xbn)T, missing masks M = (m1, . . . , mn)T, esti-
mates of the base distribution (μ, Σ) from the the previous training phase
2:	for i = 1 to n do
3:	zi = fψ-1(xbi)
4:	Impute in the latent space With (μ, Σ) to get Zi Via equation 13
5:	xei = fψ (bzi )
6:	# update current imputation
7:	xbi = xbi	oi + xei	mi
Table 1: Imputation results on UCI datasets - RMSE (loWer is better).
Data		 MCAR 					 MAR				
	EMFlow	MCFlow	GAIN	EMFlow	MCFlow	GAIN
News	.139 ±.001	.167 ± .0015	.197 ± .005	.172 ±.000	.181 ± .001	.271 ± .039
Air	.097 ±.005	.111 ± .004	.127 ± .005	.040 ±.001	.055 ± .001	.061 ± .023
Letter	.111 ± .001	.121 ± .000	.127 ± .001	.110 ±.001	.127 ± .001	.166 ± .040
Concrete	.147 ± .004	.233 ± .007	.194 ± .008	.133 ±.006	.198 ± .010	.184 ± .012
Review	.229 ±.003	.234 ± .004	.278 ± .005	.194 ±.004	.191 ± .004	.234 ± .014
Credit	.125 ±.001	.135 ± .002	.131 ± .002	.024 ±.001	.028 ± .002	.029 ± .002
Energy	.086 ±.001	.092 ± .001	.110 ± .002	.175 ±.002	.176 ± .002	.250 ± .019
CTG	.104 ±.006	.140 ± .005	.143 ± .008	.105 ±.001	.153 ± .003	.165 ± .006
Song	.025 ±.000	.030 ± .006	.034 ± .002	.024 ±.000	.031 ± .014	.028 ± .001
Wine	.076 ± .001	.098 ± .002	.097 ± .002	.102 ±.002	.124 ± .003	.135 ± .007
Web	.001 ± .000	.002 ± .000-	.003 ± .003	.002 ±.004	.006 ± .009一	.006 ± .010
山SWB
SSo-I 6lnu-e-ll-
Online News Popularity
0 5 0 5 0 5
5 2 2 5 7
0	250	500	750	1000
0.250
0.225
0.200
0.175
0.150
0	250	500	750	1000
Elapsed Time /s
0	100	200	300	400
Elapsed Time / s
0	200	400	600	800
Elapsed Time / s
Figure 2: Comparison of convergence speed in terms of the training loss and test set RMSE on three
UCI datasets.
4	Experiments
In this section, we evaluate the performance of EMFlow on multivariate and image datasets in terms
of the imputation quality and and the speed of model training. Its performance is compared to that
of MCFlow, the most related competitor that has been shown to be superior to other state of art
methods (Richardson et al., 2020).
7
Under review as a conference paper at ICLR 2022
Table 2: Imputation results on image datasets - RMSE (lower is better)
	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9
	GAIN	.1029	.1184	.1399	.1495	.1723	.1794	.2167	.2200	.2710
MNIST	MisGAN	.1083	.1117	.1184	.1227	.1311	.1388	.1512	.1906	.2621
	MCFloW	.0835	.0879	.0894	.0941	.1027	.1119	.1251	.1463	.2020
	EMFloW	.0726	.0775	.0832	.0901	.0986	.1100	.1260	.1504	.1951
	GAIN	.1025	.1090	.1103	.1073	.1094	.1202	.1217	.1426	.5388
CIFAR-10	MisGAN	.1577	.1434	.1478	.1326	.1588	.1824	.2036	.2660	.3011
	MCFlow	.1083	.1112	.1179	.1273	.1340	.1387	.1466	.1552	.1702
	EMFloW	.0444	.0479	.0525	.0575	.0619	.0689	.0782	.0926	.1188
Table 3: Classification accuracy on imputed image datasets (higher is better)
	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9
MNIST	MCFloW	7.9894	.9878	.9878	.9871	.9840	.9806	.9659	7933Γ	T7732^
	EMFlow	.9894	.9884	.9882	.9878	.9860	.9824	.9696	.9253	.7502
CIFAR-10	MCFloW	.8352	.7081	.5525	.4166	.3406	.2820	.2476	.2194	.1875
	EMFloW	.9085	.8974	.8783	.8535	.8116	.7446	.6214	.4868	.3127
To make the comparison more objective, both models use the same normalizing flow with six affine
coupling layers. We also follow the authors’ suggestion for the the hyperparameter selection of
MCFlow throughout this section. Additionally, we also present the benchmarks of other state-of-art
models including GAIN (Yoon et al., 2018) and MisGAN (Li et al., 2019) for more comprehensive
comparisons.
4.1	Multivariate Datasets
Ten multivariate datasets from the UCI repository (Dheeru & Taniskidou, 2017) are used for evalua-
tion. For all of them, each feature is scaled to fit inside the interval [0, 1] via min-max normalization.
We simulate MCAR with a missing rate of 0.2 by removing each value independently according to
a Bernoulli distribution. We also simulate MAR scenario where the missing probability of the last
30% features depends on the values of the first 70% features3 .
The initial imputation is performed by randomly sampling from the observed entries of each feature.
All experiments are conducted using five-fold cross validation where the test set only goes through
the re-imputation phase in each iteration. The choices of hyperparameters are detailed in appendix
F, where we also show that EMFlow is not sensitive to the choice of hyperparameters.
Results The imputation performace is evaluated by calculating the Root Mean Squared Error
(RMSE) between the imputed and true values. As shown in Table 1, EMFlow performs constantly
better than MCFlow under both MCAR and MAR settings for nearly all datasets.
Additionally, We trained EMFlow and MCFlow on the same machine with the same learning rate
and batch size to compare the convergence speed. Figure 2 shows the training loss and the test set
RMSE over time on three UCI datasets. It shows that EMFlow converges significantly faster than
MCFlow. In fact, EMFlow converges within three iterations for most of the UCI datasets.
4.2	Image Datasets
We also evaluate EMFlow on MNIST and CIFAR-10. MNIST is a dataset of 28×28 grayscale images
of handwritten digits (LeCun et al., 1998), and CIFAR-10 is a dataset of 32×32 colorful images from
10 classes (Krizhevsky et al., 2009). For both datasets, the pixel values of each image are scaled to
3See appendix E for the details of how MAR is simulated.
8
Under review as a conference paper at ICLR 2022
[0, 1]. In this section, we simulate MCAR where each pixel is independently missing with various
probabilities from 0.1 to 0.9.
The initial imputation is performed by nearest-neighbor sampling where a missing pixel is filled
by one of its nearest observed neighbors. In our experiments, the standard 60,000/10,000 and
50,000/10,000 training-test set partitions are used for MNIST and CIFAR-10 respectively. The
choices of hyperparameters are detailed in appendix F.
Results Table 2 shows the RMSE of all considered methods on both image datasets. In the case of
MNIST, EMFlow and MCFlow have similar RMSE and outperform other methods, while MCFlow
starts to gain slight advantage under high missing rates. In the case of CIFAR-10, EMFlow achieve
much lower RMSE than all competing methods.
To further demonstrate the efficiency of EMFlow, We also compare EMFlow and MCFlow with
respect to the accuracy of post-imputation classification. For this purpose, a LeNet-based model and
a VGG19 model were trained on the original training sets of MNIST and CIFAR-10 respectively.
These models then made predictions on the imputed test sets under different missing rates. Table
3 shows that EMFlow yields slightly better post-imputation prediction accuracy than MCFlow on
MNIST, while the improvement is much more significant on CIFAR-10. We note that these findings
are in good agreement with the RMSE results.
To qualitatively compare the imputation quality of EMFlow and MCFlow, Figure 3 shows sample
imputed images from CIFAR-10 with MCAR missing rates at 0.5 and 0.9. The first row includes
the (complete) ground truth images for reference, while the second row includes the (incomplete)
observed images on which the models were trained. The last two rows showcase the reconstructed
images by MCFlow and EMFlow, respectively. It’s clear that EMFlow performs better than MCFlow
by recovering more details and displaying sharper boundaries and cleaner background.
5	Conclusion
We propose a novel architecture EMFlow for missing data imputation. It combines the strength of
the online EM and the normalizing flow to learn the density estimation in the presence of incom-
plete data while performing imputation. Various experiments with multivariate and image datasets
show that EMFlow significantly outperforms its state-of-art competitor with respect to imputation
accuracy as well as the convergence speed under a wide range of missing rates and different missing
mechanisms. The accuracy of post-imputation classification on image datasets also demonstrates
the superior EMFlow’s ability of recovering semantic structure from incomplete data.
Reproducibility Statement
The code and data to reproduce the results in this work are included in the supplementary materials.
9
Under review as a conference paper at ICLR 2022
References
Vincent Audigier, Francois Husson, and Julie Josse. Multiple imputation for continuous variables
using a bayesian principal component analysis. Journal of statistical computation and simulation,
86(11):2140-2156, 2016.
Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting.
In Proceedings of the 27th annual conference on Computer graphics and interactive techniques,
pp. 417-424, 2000.
AE Brockwell. Universal residuals: A multivariate transformation. Statistics & probability letters,
77(14):1473-1478, 2007.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Stephen Burgess, Ian R White, Matthieu Resche-Rigon, and Angela M Wood. Combining multiple
imputation and meta-analysis with individual participant data. Statistics in medicine, 32(26):
4499-4514, 2013.
S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations
in r. Journal of statistical software, pp. 1-68, 2010.
Olivier CaPPe and Eric Moulines. On-line expectation-maximization algorithm for latent data mod-
els. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593-613,
2009.
Yilun Chen, Ami Wiesel, and Alfred O Hero. Robust shrinkage estimation of high-dimensional
covariance matrices. IEEE Transactions on Signal Processing, 59(9):4097-4107, 2011.
Mark Collier, Alfredo Nazabal, and Christopher KI Williams. Vaes in the presence of missing data.
arXiv preprint arXiv:2006.05301, 2020.
Dua Dheeru and E Karra Taniskidou. Uci machine learning repository. 2017.
Marco Di Zio, Ugo Guarnera, and Orietta Luzi. Imputation through finite gaussian mixture models.
Computational Statistics & Data Analysis, 51(11):5305-5316, 2007.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Jianqing Fan, Yingying Fan, and Jinchi Lv. High dimensional covariance matrix estimation using a
factor model. Journal of Econometrics, 147(1):186-197, 2008.
BJ Ferdosi, H Buddelmeijer, SC Trager, MHF Wilkinson, and JBTM Roerdink. Comparison of
density estimation methods for astronomical datasets. Astronomy & Astrophysics, 531:A114,
2011.
Pedro J Garcla-Laencina, JOSe-LUiS Sancho-Gomez, and Anlbal R Figueiras-Vidal. Pattern classifi-
cation with missing data: a review. Neural Computing and Applications, 19(2):263-282, 2010.
Lovedeep Gondara and Ke Wang. Multiple imputation using deep denoising autoencoders. arXiv
preprint arXiv:1705.02737, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Niels Bruun Ipsen, Pierre-Alexandre Mattei, and Jes Frellsen. not-miwae: Deep generative mod-
elling with missing not at random data. arXiv preprint arXiv:2006.12871, 2020.
10
Under review as a conference paper at ICLR 2022
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Ranjit LalL HoW multiple imputation makes a difference. Political Analysis, 24(4):414—433, 2016.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Steven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin. Misgan: Learning from incomplete data
With generative adversarial netWorks. arXiv preprint arXiv:1902.09599, 2019.
Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793. John
Wiley & Sons, 2019.
Xinyue Liu, Chara AggarWal, Yu-Feng Li, Xiaugnan Kong, Xinyuan Sun, and Saket Sathe. Kernel-
ized matrix factorization for collaborative filtering. In Proceedings of the 2016 SIAM International
Conference on Data Mining, pp. 378-386. SIAM, 2016.
Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebastian
NoWozin, and Cheng Zhang. Eddi: Efficient dynamic discovery of high-value information With
partial vae. arXiv preprint arXiv:1809.11142, 2018.
JinWen Ma, Lei Xu, and Michael I Jordan. Asymptotic convergence rate of the em algorithm for
gaussian mixtures. Neural Computation, 12(12):2881-2907, 2000.
Pierre-Alexandre Mattei and Jes Frellsen. MiWae: Deep generative modelling and imputation of
incomplete data sets. In International Conference on Machine Learning, pp. 4413-4423. PMLR,
2019.
Xiao-Li Meng. Multiple-imputation inferences With uncongenial sources of input. Statistical Sci-
ence, pp. 538-558, 1994.
Xiao-Li Meng et al. On the rate of convergence of the ecm algorithm. The Annals of Statistics, 22
(1):326-339, 1994.
Danilo Rezende and Shakir Mohamed. Variational inference With normalizing floWs. In Interna-
tional Conference on Machine Learning, pp. 1530-1538. PMLR, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International conference on machine learning,
pp. 1278-1286. PMLR, 2014.
Trevor W Richardson, Wencheng Wu, Lei Lin, Beilei Xu, and Edgar A Bernal. McfloW: Monte
carlo floW models for data imputation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 14205-14214, 2020.
Guido Sanguinetti and Neil D LaWrence. Missing data in kernel pca. In European Conference on
Machine Learning, pp. 751-758. Springer, 2006.
Badrul SarWar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th international conference on World Wide
Web, pp. 285-295, 2001.
Peter Schmitt, Jonas Mandel, and Mickael Guedj. A comparison of six methods for missing data
imputation. Journal of Biometrics & Biostatistics, 6(1):1, 2015.
RS Somasundaram and R Nedunchezhian. Evaluation of three simple imputation methods for en-
hancing preprocessing of data With missing values. International Journal of Computer Applica-
tions, 21(10):14-19, 2011.
11
Under review as a conference paper at ICLR 2022
Daniel J Stekhoven and Peter Buhlmann. Missforest-non-parametric missing value imputation for
mixed-type data. Bioinformatics, 28(1):112-118, 2012.
Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama.
Coupling-based invertible neural networks are universal diffeomorphism approximators. arXiv
preprint arXiv:2006.11469, 2020.
CF Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pp.
95-103, 1983.
Hai xian Wang, Quan bing Zhang, Bin Luo, and Sui Wei. Robust mixture modelling using multivari-
ate t-distribution with missing information. Pattern Recognition Letters, 25(6):701-710, 2004.
Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising and inpainting with deep neural net-
works. Advances in neural information processing systems, 25:341-349, 2012.
Raymond Yeh, Chen Chen, Teck Yian Lim, Mark Hasegawa-Johnson, and Minh N Do. Semantic
image inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539, 2(3),
2016.
Jinsung Yoon, James Jordon, and Mihaela Schaar. Gain: Missing data imputation using generative
adversarial nets. In International Conference on Machine Learning, pp. 5689-5698. PMLR, 2018.
Adriano Z Zambom and Dias Ronaldo. A review of kernel density estimation with applications to
econometrics. International Econometric Review, 5(1):20-42, 2013.
Ruofei Zhao, Yuanzhi Li, Yuekai Sun, et al. Statistical convergence of the em algorithm on gaussian
mixture models. Electronic Journal of Statistics, 14(1):632-660, 2020.
A	Affine Coupling Layers
The basic idea behind normalizing flow is to find a transformation (of X) that would be able to
represent a complex density function as a function of simpler ones, such as a multivariate Gaussian
random variable (to be denoted by Z). The quest for an invertible transformation f, such that Z =
f T(X)〜Np(0, I) can be challenging, but in theory such a transformation does exist. Brockwell
(2007) showed the existence of such a transformation which converts any p-dimensional random
variable X to P independent uniform random variables Uj = gj(X)〜U(0,1) for j = 1,...,p,
which are known as universal residuals. Thus, choosing Zj = f-1(X) ≡ Φ-1(gj(X))〜N(0,1),
where Φ(∙) denotes the cumulative distribution function of a standard normal distribution, We can
show the existence of Z = f(X), where f-1(x) = (f1-1(x), . . . , fp-1(x)) that enables to transform
X to Z. However, finding such nonlinear transform based on observed data is challenging and
so we use sequence of simpler transforms in the spirit of universal residuals, for simplicity and
computational efficiency.
Following the framework of Real NVP (Dinh et al., 2016), the NF used throughout this work consists
of a sequence of affine coupling layers faff : x 7→ y defined as
y1:d = x1:d
yd+1:p = xd+1:p	exp (s (x1:d)) + t (x1:d)
(16)
where the scale and shift parameters s(∙) and t(∙) are usually implemented by neural networks, and
is the element-wise product.
Therefore, the input x are split into two parts:the first d dimensions stay the same, while the other
dimensions undergo an affine transformation whose parameters are the functions of the first d di-
mensions.
12
Under review as a conference paper at ICLR 2022
B	Online EM for Missing Data imputation
In this section, the steps of vanilla EM in the context of missing data imputation are reviewed. We
then show how the online EM framework proposed by CaPPe & Moulines (2009) can be easily
applied here.
B.1	Vanilla EM
Given n i.i.d data points {zi = (zθ, zm)}n=ι distributed under N(μ, Σ), the E-step of each EM
iteration evaluates the conditional expectation of the complete data likelihood:
nn
Q(φ; b(t))= n X Qi(Φ; b(t))= n X Eφ(t) [log N(Zi Φ)∣zo]	(17)
i=1
i=1
where φ = (μ, Σ), and φ(t) = (μ(t), Σ(t)) are the estimates at the tth iteration. We also follow the
treatment by CaPPe & Moulines (2009) to normalize the conditional expectation by 1/n for easier
transition to the online version of EM.
To calculate conditional expectation inside the summation, we can use the fact that the conditional
distribution of zim given zio is still Gaussian:
zmizo； b⑴〜N(μm⑴,ς m(t))
(18)
where
m(t)
μi
-1	zio
Σe m(t)
Σb(mt)imi - Σb(mt)ioi	Σb(oti)oi-1Σb(oti)mi,
(19)
μmi+$肾巴(* Woi
—
mi and Oi are the missing and observed masks respectively, and the subscripts of (μ(t), Σ(t)) rep-
resent the slicing indexes.
Then it’s easily easy to find
Qi(φ; b(t))= -1 ](b(t) - * ∑-1 W) - μ
+Tr ([(£-%mJ ςm(t))i - 2 log I2∏ςI
(20)
where Tr(∙) denotes the matrix trace, and zit) is in fact the imputed data vector whose missing part
is replaced by the conditional mean μm(t).
When it comes to the M-step, the derivatives of Q(φ; φ(t)) are calculated as
-μ)τ+2
n
ς -1X ς (t)
(21)
where Σe i(t) is p × p matrix satisfying Σe i(,tm) m
m(t)
Σi	and all other elements equal to 0.
Therefore, the maximizer of Q(φ; φb(t)) are
13
Under review as a conference paper at ICLR 2022
n
μ(t+1) = 1 Xb(t),
i=1
1n
∑ (t+1) = 1 X
n
i=1
-μ(t+1)	- μ(t+1)yT +
(22)
Here we provide a remark that helps to motivate the optimization of EMFlow described in Section
3.4.
Remark. Given an incomplete data point Z = (zo, Zm)〜N(μ, Σ), its likelihood is maximized at
zm = E [zm |zo].
To prove it, first note that the log-likelihood of Z can be written as
T
1m	m
logN(zm,zo∣μ,∑)=-2 [Zo-μom] ∑	[ Zo-μom]+c	(23)
Denote the conditional mean and covariance of zm as μ = μm + ∑mo (∑oo)-1 (zo - μo) and
Σe = Σmm - Σmo (Σoo)-1 Σom. The matrix block-wise inversion gives
Σ-1
Σmm
Σom
Σmo
Σoo
-1
∑1ι
v,*
Σ21
Σ*
12
v,*
Σ22
(24)
where
Σ
*
11
-1
Σ*
12
-Σe-1ΣmoΣo-o1
Σ
*	-1	e-1
Σ21 = -Σoo ΣmoΣ
Σ2*2 = Σo-o1 + Σo-o1ΣmoΣe-1ΣmoΣo-o1.
After a series of linear algebra, we have
log N (zm, zo∣μ, Σ)
=-2 (zm - μ)T ∑T(Zm - μ) - 2(zo - μo)T ∑-1 谊-〃o) + C
(25)
(26)
A ɪ-i一1 ♦	∙ . ∙	1 r∙	∙ . ∙ F ♦	. 1 ..1 IF IT 1	1	.1	♦	. τr>	~
As Σ 1 is positive definite, it is obvious that the likelihood reaches the maximum at zm = μ.
B.2	Online EM
It’s clear that the EM algorithm needs to process the whole dataset at each iteration, which becomes
impractical when dealing with huge datasets. To address this problem, CaPPe & Moulines (2009)
propose to replace the reestimation functional in the E-step by a stochastic approximation step:
Q(t+1)(φ) = (I-Pt+1)Q(t)(φ) + Pt+ι ∙ 7jB7 X Eb(t) [log f (Zi, φ) | zo]	(27)
|B | i∈B
where f is the probability density of complete data, {ρi}i∞=1 are a series of step sizes satisfying the
conditions specified in equation 9, B ⊂ {1, . . . , n} denotes the indexes of a mini-batch of samples,
and |B | is the batch size.
Under some mild regulations such as the complete data model belongs to exponential family CaPPe
& Moulines (2009), equation 27 boils down to a stochastic approximation of the estimates of suffi-
cient statistics:
14
Under review as a conference paper at ICLR 2022
b(t+1) = (1 - Pt+ι)b(t) + Pt+ιLBI XEφ(t [S(z)∣zO]
|B| i∈B
(28)
where S(z) is the sufficient statistic of f.
Note that the sufficient statistics for multivariate Gaussian are just sample mean and sample covari-
ance. Therefore, equation 8 follows immediately form equation 28.
C Interpret the Optimization
As described in Section 3.1, the estimated model parameters and the imputed data are supposed
to maximize the complete data likelihood (only one data point is considered here for illustration
purpose), namely,
(b,ψ, μ, Σ) = arg max logPX (x∣ψ, μ, Σ)	(29)
x∈X0 ,ψ,μ,Σ
While it’s hard to optimize such objective simultaneously, it’s possible to optimize it alternatively.
For example, learning L1(ψ) in equation 11 corresponds to updating ψ while keeping other variables
unchanged.
The operations performed in the latent space needs more explanation. First of all, it is shown in
appendix B that the imputation increases the complete likelihood in the latent space, i.e.,
-,. ^. - . ^.
N(b;μ,ς) ≥N(z;μ,∑)	(30)
where z and bz are the embedding vectors before and after the imputation, respectively.
However, the increase of likelihood in the latent space doesn’t guarantee the same thing in the data
space. Therefore, the flow model is updated again by optimizing L2(ψ) in equation 14 to ensure
that xe = fψ(bz) has a high likelihood in the data space X. Note that although xe doesn’t necessarily
belong to X0, the reconstruction penalty Lrec defined in equation 15 forces xe to be close to X0.
Finally, in the re-imputation phase, the imputed data xb are updated by projecting xe onto X0.
D	Implementation Details
Model Initialization Each inference iteration consists of a training phase and a re-imputation
phase. And since the latter phase updates the current imputation, the parameters of the flow model
fψ are reinitialized after each iteration to learn the new data density faster.
The base distribution estimate N(z; μ, Σ) is initialized at the very beginning when the online EM
encounters the first batch of embedding vectors. Specifically μ and Σ are initialized to be the sample
mean and sample covariance. By default, the base distribution is also reinitialized along with the
flow model after each iteration.
Stabilization of Online EM Online EM can be unstable in some cases where the covariance
estimate Σ becomes ill-conditioned during the inference. Such instability can be traced back to
two primary sources: (i) the initial naive imputation has really bad quality and the inter-feature
dependency is distorted significantly; (ii) the batch size is close to or less than the number of features,
which makes it hard to estimate the covariance directly (e.g. Fan et al., 2008; Chen et al., 2011).
Fortunately, those two issues can be solved easily. To reduce the impact of the initial naive imputa-
tion and make the covariance estimation more robust, we enlarge the diagonal entries of the original
covariance estimate from equation 8 proportionally:
ΣRob = Σ + β ∙ Diag(Σ)	(31)
15
Under review as a conference paper at ICLR 2022
where Diag(Σ) is a diagonal matrix with the same diagonal entries as Σ, and β a positive hyper-
parameter. In practice, β would be decreased gradually to 0 during the first a few iterations as the
quality of current imputation gets better and better.
To address the second issue, the training phase in Algorithm 1 can be modified to make the online
EM see more data points. When updating the base distribution, the current batch of embedding
vectors can be concatenated with previous ones to form a super-batch with a maximum size Ssuper. If
the maximum size is exceeded, the most previous embedding vectors would be excluded. And since
the previous batches are already imputed, the super-batch brings nearly no additional computational
overhead.
E Simulate MAR on UCI Datasets
To simulate MAR, the first 70% features of each data point xi is retained, and the remaining 30%
features are removed with probability:
b0.7pc
sigmoid	xij
j=1
(32)
where [∙[ denotes the integer part of a number.
Note that different data sets would exhibit different missing rates under our MAR setting. Table
shows the dimensions of UCI data sets used in our experiment as well as their MAR missing rates
for the last 30% features.
Table 4: Information of UCI datasets.
Data	Features	Samples	MAR missing rate
News	-60-	39644	0.35
Air	13	9357	0.62
Letter	16-	19999	0.57
Concrete	9	-1030	0.50
Review	24	5454	0.49
Credit	-23	30000	0.30
Energy	-28	19735	0.39
CTG	21	-2126-	0.29
Song	-90-	92743	0.32
Wine	12	6497	0.26
Web	550	58638	0.12
F	Hyperparameter Robustness
EMFlow has a relatively simpler architecture than GAN-based models, and there are just two most
important two hyperparameters specific to it. One is the power index γ in the step size schedule
of online EM (see equation 10), while the other is the strength of the reconstruction error α in the
objective (see equation 14). From our extensive experiments, γ ∈ [0.6, 0.9] often leads to consistent
results. On the other hand, a practical guide for choosing α is to make the magnitudes of first and
second term of equation 14 don’t differ too much.
Figure 4 shows RMSE on the test set of Online News Popularity dataset versus different values of
γ and α, while all other hyperparameters are kept fixed. The RMSEs in all configurations are well
below that obtained by MCFlow, and their fluctuations are almost negligible.
16
Under review as a conference paper at ICLR 2022
IO5
106
a
—∙-- EMFIow
---MCFIow
IO7
Figure 4: RMSE between true and imputed values on the test set of Online News Popularity dataset
with different choices of γ and α.
F.1 Hyperparameters for UCI Datasets
We use α = 106 and choose Pt = 0.99 ∙ t-0.8 as the step size schedule for all UCI datasets.
During the training of EMFlow, the batch size is 256 and the learning rate is 1 × 10-4. Compared to
MCAR, the initial imputation can be more difficult under MAR where the marginal observed density
is distorted more. Therefore, the robust covariance estimation in equation 31 is adopted by default
under MAR. Specifically, we use β = 10-2 for the first two iterations, β = 10-3 for the next two
iterations, and β = 0 for the remaining ones.
F.2 Hyperparameters for Image Datasets
For all experiments with the image datasets, the step size schedule of online EM is again Pt =
0.99 ∙ t-0.8. For MNIST and CIFAR-10, We choose a to be 5 X 108 and 1 X 106 respectively. During
the training of EMFlow, the batch size is 512 and the learning rate is 1 × 10-3. Since image datasets
have much larger dimensions than UCI datasets, the super-batch approach With a size of 3000 is
adopted. Additionally, similar robust covariance estimation used by UCI datasets is also applied to
CIFAR-10.
G	Additional Experiment
G.1 Model Congeniality
We also consider the congeniality of the imputation model that measures its ability to preserve
the feature-label relationship (Meng, 1994; Burgess et al., 2013). To quantify hoW the feature-
label relationship is changed after imputation, We calculated the euclidean distance betWeen the
coefficients of tWo linear or logistic regression models, one learned from a complete test set and the
other learned from the corresponding imputed version. Three datasets With continuous or discrete
targets Were used in our experiments, and the results are shoWn in Table 5. It shoWs that the gaps in
the measured congeniality actually coincide With those in the measured RMSE.
Table 5: Congeniality of imputation models (loWer is better).
	Credit	Letter	News
GAIN	1.3310 ± 0.1378	4.2957 ± 02212	0.0575 ± 0.0762
MCFlow	1.1080 ± 01363	3.8157 ± 0.2580	00541 ± 00699
EMFlow	0.9141 ± 0.1112	3.7611 ± 0.1910	0.0229 ± 0.0240
17
Under review as a conference paper at ICLR 2022
Table 6: Imputation results of MIWAE on UCI datasets - RMSE (lower is better).
Data		MCAR				MAR		
	EMFloW	MIWAE	EMFlow	MIWAE
NeWs	-.139 ± .001-	-.203 ± .004	-.172 ± .000-	-.195 ± .005
Air	-.097 ± .005-	-.101 ± .008	-.040 ± .001-	.034 ± .001
Letter	-.111 ± .001-	.078 ± .001	-.110 ± .001-	.085 ± .001
Concrete	-.147 ± .004-	-.151 ± .009	-.133 ± .006-	-.138 ± .009
Review	-.229 ± .003-	-.234 ± .007	-.194 ± .004-	.192 ± .007
Credit	-.125 ± .001-	-.130 ± .001-	-.024 ± .001-	-.026 ± .002
Energy	-.086 ± .001-	-.101 ± .011-	-.175 ± .002-	.167 ± .002
CTG	-.104 ± .006-	.102 ± .005	-.105 ± .001-	-.108 ± .002
-Song	-.025 ± .000-	-.031 ± .001-	-.024 ± .000-	-.025 ± .000
-Wine-	-.076 ± .001-	.072 ± .002	-.102 ± .002-	.089 ± .002
Web	.0006 ± .0003-	.0022 ± .00T0-	.0022 ± .0042^	.0066 ± .0109-
G.2 Comparison to MIWAE
In this section, we compare EMFlow with MIWAE that introduces the importance-weighted autoen-
coder for imputation tasks. For UCI datasets, we follow the implementation provided by the author
4. On the other hand, the implementation of MIWAE on image datasets are not provided, and thus
we tried our best to tune MIWAE on MNIST.
As shown in Table 6, EMFlow and MIWAE have mixed performance on UCI datasets, and the
differences in most cases are quite small. Table 7 and 8 shows that EMFlow outperforms MIWAE
on MNIST in terms of the RMSE on test set and post-imputation accuracy. We also note that
EMFlow still converges much faster than MIWAE.
Table 7: Imputation results of MIWAE on MNIST - RMSE (lower is better)
	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9
MNIST	-MIWAE-	.1022	.1057	.1120	.1185	.1194	.1233	.1409	.1672	.2275
	EMFloW	.0726	.0775	.0832	.0901	.0986	.1100	.1260	.1504	.1951
Table 8: Classification accuracy of MIWAE on imputed MNIST (higher is better)
	Missing Rate	.1	.2	.3	.4	.5	.6	.7	.8	.9
MNIST	-MIWAE-	.9792	.9878	.9683	.9672	.9521	.9469	.9237	.9331	.7005
	EMFloW	.9894	.9884	.9882	.9878	.9860	.9824	.9696	.9253	.7502
G.3 Effect of Initial Imputation
EMFlow is an iterative imputation framework, and the nearest-neighbor (NN) imputation is per-
formed for image datasets at the beginning as warm-start in all the previous experiments. In this
section, we compare NN imputation and zero imputation as the starting point of EMFlow and shows
the convergence of the training loss, the RMSE on the test set, and the Frobenius norm of the co-
variance matrix in the latent space. As shown in Figure, 5, the difference between NN imputation
and zero imputation is negligible and the convergence under both cases is fast.
4https://github.com/pamattei/miwae
18
Under review as a conference paper at ICLR 2022
----NN Imputation
Zero Imputation
----NN Imputation
Zero Imputation
----NN Imputation
Zero Imputation
Epoch
----NN Imputation
一 Zero Imputation
Q 5 Q 5 Q 5 Q
E-ION sɔu ① qoli:
Epoch
Figure 5: The convergence of training loss, test set RMSE and Frobenius norm of the latent covari-
ance matrix using NN or zero imputation as the the start on MNIST. The first and second rows of
plots correspond to the missing rates of 0.5 and 0.9.
19