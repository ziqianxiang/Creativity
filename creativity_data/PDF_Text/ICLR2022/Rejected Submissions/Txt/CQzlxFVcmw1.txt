Under review as a conference paper at ICLR 2022
Message	Function	Search	for	Hyper-
relational Knowledge Graph
Anonymous authors
Paper under double-blind review
Ab stract
Recently, the hyper-relational knowledge graph (HKG) has attracted much attention
due to its widespread existence and potential applications. The pioneer works have
adapted powerful graph neural networks (GNNs) to embed HKGs by proposing
domain-specific message functions. These message functions for HKG embedding
are utilized to learn relational representations and capture the correlation between
entities and relations of HKGs. However, these works often manually design and
fix structures and operators of message functions, which makes them difficult to
handle complex and diverse relational patterns in various HKGs (i.e., data patterns).
To overcome these shortcomings, we plan to develop a method to dynamically
search suitable message functions that can adapt to patterns of the given HKG.
Unfortunately, it is not trivial to design an expressive search space to enable the
powerful message functions being searched, especially the space cannot be too
large for the sake of search efficiency. In this paper, we first unify a search space of
message functions to search both structures and operators. The message functions
of existing GNNs and some classic KG/HKG models can be instantiated as special
cases of the proposed search space. Then, we leverage a search algorithm to
search the message function and other GNN components for any given HKGs. We
empirically show that the searched message functions are data-dependent, and can
achieve leading performance in link/relation prediction tasks on benchmark HKGs.
1	Introduction
Knowledge base (KB) (Auer et al., 2007) is an important tool to organize and explore human
knowledge, thereby promoting a series of applications, e.g., question answering (Lukovnikov et al.,
2017) and recommendation system (Cao et al., 2019). Generally, the KB stores the n-ary fact
r(eι, ∙∙∙ ,en) (n is arity) that represents the relation r ∈ R between real-world entities ei ∈ E. To
manipulate large scale KBs, KB embedding (Nickel et al., 2015; Wang et al., 2017) proposes to encode
the set of relations R and entities E into a d-dimensional vector space R ∈ RlRl×d, E ∈ RlEl×d.
In last decades, the research community mainly focuses on embedding knowledge graphs (KGs)
that only contain binary facts {r(e1, e2)}, e.g., isCaptialOf(Beijing,China). Among
kinds of KG embedding models (Rossi et al., 2021), tensor models (Lacroix et al., 2018; Balazevic
et al., 2019) propose to represent a KG into a 3-order tensor and decompose tensors into R and
E, which achieve outstanding empirical performance and theoretical guarantees. Recent studies
start to learn embeddings from n-ary facts (n ≥ 2) because n-ary facts are widespread in KBs, e.g.,
playCharacterIn(LeonardNimoy,Spock,StarTrek1). For example, more than 30%
of entities in Freebase (Bollacker et al., 2008) involve facts with higher arity (Wen et al., 2016).
Therefore, it is necessary to investigate the more general case of KGs, facts with mixed arities
S = {r(e1, . . . , en) : n ∈ {2, . . . , N}} named as hyper-relational KGs (HKGs).
Unfortunately, it is hard to extend powerful tensor models from the case of fixed arity (e.g., KG)
to the case of mixed arities (i.e., HKG). That is because a tensor can only model a set of facts
under the same arity. Instead, some pioneer works (Yadati, 2020; Galkin et al., 2020) demonstrate
that the multi-relational hypergraph (MRHG) could be a more natural way to model HKGs (see
Appx. A for more details). Let entities E and relations R be nodes and edge types in the MRHG
G(E, R, S), respectively. The length of MRHG’s edge (e1, . . . , en) (hyperedge) could be variant,
which can represent facts with various arities n. And hyperedges can be labeled by multiple edge
types r ∈ R like r(e1, . . . , en) ∈ S. Under the MRHG modeling, these works adapt powerful graph
1
Under review as a conference paper at ICLR 2022
neural networks (GNNs) (Kipf & Welling, 2016; Hamilton et al., 2017) to embed HKGs (Yadati,
2020; Galkin et al., 2020). Generally, GNNs learn node embeddings by passing messages (e.g., the
node features) from adjacent nodes to the center node. But in scenarios of HKGs, it is important to
know the type of edge (relation) that connects several entities. Therefore, existing GNNs for HKG
embedding design several domain-specific message functions to learn relational representations R by
capturing the interaction between entities E and relations R.
Existing works manually design and fix the structures and operators of message functions. However,
such rigid message function designs are not conducive to pursuing high empirical performance, as
relations usually have distinct patterns in various KGs/HKGs. For example, the message function of
G-MPNN (Yadati, 2020) adopts the inner product way like DistMult (Yang et al., 2015) to compute
the correlation between entities and relations, which has been proven to only cover symmetric
relations (Kazemi & Poole, 2018). Its performance may not be good if there are many non-symmetric
relations existed (see Appx. B.1). It may be a potential solution to design a universal message function
to cover as many relational patterns as possible. But covering a certain pattern does not mean that
the model can reach good performance on it (Meilicke et al., 2018; Rossi et al., 2021). Moreover, a
pioneer work AutoSF (Zhang et al., 2020) shows that designing data-aware models can consistently
achieve high performance on any given KGs. Thus, dynamically searching message functions could
be an effective way to capture the data patterns of the given HKG and pursue high performance.
Unfortunately, the searching method AutoSF is strictly restricted to bilinear KG models (Yang et al.,
2015; Kazemi & Poole, 2018), which is not applicable to message function design and the HKG
scenario. Besides, although neural architecture search (NAS) (Elsken et al., 2019) has been introduced
to search GNN architectures, current GNN search spaces (Zhang et al., 2021) are not well regularized
to tasks on HKGs. Specifically, the existing search spaces of message function follow the classic way
to simply aggregate node neighbors to learn node embeddings, while ignoring edge embeddings to
represent relations. Thus, they cannot capture the correlation between entities and relations of HKGs.
In summary, rigid message function designs for HKGs are not conducive to consistently pursuing
high performance on different data sets, while existing searching methods are not applicable for
HKGs. To bridge this research gap, we propose the Message function SEArch for any given HKGs,
named as MSeaHKG. However, it is non-trivial to design an expressive search space to enable the
powerful message functions being searched, especially the space cannot be too large for the sake of
search efficiency. Thus, we identify the necessary computation operators that are domian-specific
designs for HKGs and propose to search the interaction between these operators for capturing the
relational patterns. Moreover, except for the message function search, we also incorporate other
GNN components (e.g., aggregation function) in the MSeaHKG search space for more performance
improvements. Then, we formulate the discrete HKG models with probabilistic modelings to enable
an efficient NAS algorithm working on our scenario. The main contributions are listed as:
•	Previous GNN searching methods generally ignore the edge representations, which fails to handle
semantic meaningful relations on HKGs. Besides, their message functions cannot capture complex
interactions between entities with relations. In this paper, we propose a searching method to
dynamically design a suitable GNN that can achieve high performance on the given HKG.
•	Inspired by rigid message function designs, we define a novel search space of message functions for
HKGs, which enables the message function to be flexibly searched on the given HKG. Especially,
the message function designs of existing GNNs for HKGs and some classic KG/HKG models can
be instantiated as special cases of the proposed space.
•	We conduct experiments on benchmark HKGs for the link prediction and relation prediction
tasks. Experimental results demonstrate that MSeaHKG can consistently achieve state-of-the-art
performance by designing data-aware message functions. Besides, we also transfer MSeaHKG to
other graph-based tasks and further investigate its capability.
2	Related Work
2.1	One-shot Neural Architecture Search in Graph Neural Network
To avoid manual efforts on neural architecture designs, NAS (Hutter et al., 2018; Yao & Wang, 2018)
aims to automatically search suitable neural architectures for the given data and task. Generally, search
2
Under review as a conference paper at ICLR 2022
Table 1: Overview of Existing Works. NC, GC, LP, RP denote node classification, graph classification,
link prediction, relation prediction, respectively. DP(∙) is dropout; BN(∙) is batch normalization;
θ(∙) outputs the hidden representation after summarizing mixed operations (Zhao et al., 2021); hK
denotes the output from i-th operator of O in K-th layer of message function (see Sec. 3.1).
Type	Model	Scenarios		Message Function
		Task	# Type/Length of edge	
NAS for GNNs	You et al. (2020)	NC/GC/LP	=1/ = 2	DP(BN (Wej + b))-
	-GraPhNAS	NC		aijconcat(ei, ej∙)
	AGNN			a，ij Wej
	SANE		≥ 1/ = 2	W "{ej}e, ∈n (a))
	NAS-GCN	GC		aij MLP (hij )ej-
GNNs for KGs/HKGs	R-GCN	LP/RP	≥ 1/ = 2	Wr ej-
	COmPGCN~~			Wλ(r)φ(ej, r)
	StarE		≥ 1/ ≥ 2	Wλ(r)φr(eo,γ(r, hr))
	G-MPNN			TP(e) * Qi Pei * ei ,
Ours	MSeaHKG	LP/RP	≥ 1 ≥ 2	M LP&concat ({hK } iO1)
space, search algorithm, and evaluation measurement are three important components in NAS (Elsken
et al., 2019). Search space defines what network architectures in principle should be searched. The
search algorithm performs an efficient search over the search space and finds architectures that achieve
good performance. Evaluation measurement decides how to evaluate the searched architectures during
the search. Classical NAS methods are computationally consuming because candidate architectures
are evaluated by the stand-alone way, i.e., evaluating the performance of architecture after training
it to convergence. More recently, one-shot NAS (Pham et al., 2018) proposes the weight sharing
mechanism to share network weights across different candidate architectures and evaluate them on
the shared weights, which can extremely reduce the search cost.
Some pioneer works have explored NAS for GNNs, such as You et al. (2020), GraphNAS (Gao et al.,
2020), AGNN (Zhou et al., 2019), NAS-GCN (Jiang & Balaprakash, 2020). And the one-shot NAS
also has been introduced to search GNN architectures recently, e.g., SANE (Zhao et al., 2021). As
shown in the left part of Fig. 1, most GNN searching methods follow the message passing neural
networks (MPNNs) (Gilmer et al., 2017) to unify two steps of the GNN framework in one layer:
step1 : mi J agg({mgc(ei, ej)}jj∈n3)), step2 : ei J act(comb(ei, mi)),	(1)
where ei ∈ Rd represents the embedding of node ei , mi is the intermediate embeddings of ei
gathered from its neighborhood N(ei). The search space of operators in Eq. 1 are summarized into:
•	Message Function mgc(∙): The message function decides the way to gather information from
a neighborhood ej of the center node ei . Zhang et al. (2021) summarizes the typical message
in existing GNN searching methods as mgc(ei, ej) = aijWej, where aij denotes the attention
scores between nodes ei with ej. Besides, we present more message function designs in Tab. 1.
•	Aggregation Function agg(∙): It controls the way to aggregate message from nodes, neigh-
borhood. Usually agg ∈ {sum, mean, max}, where sum(∙) = Eq.∈nS)mgc(ei, ej),
mean(∙) = PeQ∈nS)mgc(ei, ej∙)/|N(v)|, and max(∙) denotes channel-wise maximum.
•	Combination Function comb(∙): It determines the way to merge messages between neighborhood
and node itself. In literature, COmb is selected from {concɑt, add, mlp}, where COnCat。=
[ei, mi], add(∙) = ei + mi, and mlp(∙) = MLP(ei + mi) (MLP is Multi-layer Perceptron).
•	Activation Function act(∙): [identity, sigmoid, tanh, relu, elu] are some of the most commonly
used activation functions (Gao et al., 2020).
Overall, mgc(∙) in Eq. 1 only learns node embeddings, which cannot encode the semantic meaningful
edge types (i.e., relations in HKGs). Note that NAS-GCN (Jiang & Balaprakash, 2020) takes the edge
feature hij between ei and ej as input without learning edge embeddings, and recent AutoGEL (Zhili
et al., 2021) simply extends the message function mgc(∙) to learn edge embeddings without studying
the interactions between entities and relations, thereby failing to handle the LP/RP tasks on HKGs.
3
Under review as a conference paper at ICLR 2022
entity relation role/PoSition
OOD
MPNN
COmPGCN	StarE	G-MPNN
Figure 1: The framework of several GNN frameworks. The green box refers to the message function.
Note that COmPGCN and StarE select sum(∙) as agg(∙) and omit comb(∙), while G-MPNN uses
Concat(∙) as comb(∙).
2.2	Graph Neural Networks for KG/HKG Embedding
As introduced in Sec. 1 and Sec. 2.1, message functions in classic GNNS simply aggregate messages
from adjacent nodes (see Tab. 1 and Eq. 1). But in scenarios of KGs and HKGs, it is important to know
the type of edge (relation) that connects several entities. To capture relations, R-GCN (Schlichtkrull
et al., 2018) proposes to model relations R in KGS through relation-specific weight matrix Wr ∈
Rd×d for r ∈ R in message functions, which is instantiated as:
ei = act(sum({Wr ej }r(ei,ej )∈I(ei))),	⑵
where I(ej = {r(ei, ej) ∈ S : r ∈ R, ej ∈ E} is the set of relational edges incident on e〃 But such
relation modeling may lead to the over-parameterization issue because there are many relations in
KGs. Therefore, CompGCN (Vashishth et al., 2020) utilizes the embedding vector r to represent the
relation instead of weigh matrix:
ei = act(sum({Wλ(r)φ(ej, r)}r(eτ,e3)∈i(ei))), r = Wr	⑶
where λ(r) records the directional information of edges. The entity-relation composition operator
set {sub, mult, corr} is inspired by classical scoring function design in existing KG embedding
models, such as element-wise subtraction sub(∙) = ej - r (Bordes et al., 2013), inner product
mult(∙) = ej- * r (Yang et al., 2015), circular correlation corr(∙) = ej ◦ r (Nickel et al., 2016).
Subsequently, StarE and G-MPNN extend the applications of GNNS from KGS to HKGs.
StarE (Galkin et al., 2020) requires the role information % of entity ej∙ and assumes a hyper-
relational fact r(eι,...,en) is composed by a base triplet r(e§, e°) with a set of role-value pairs
{(r%, ej)} such that r(e§, e0, {(底,，ej∙)}). Then StarE takes base relation with the role-value pairs as
a hyper-relation and performs the composition operator φr (∙) on the hyper-relation with base entity:
es = act (Sum({wλ(r)φr (e o, Y (r, hr )) }r(es ,e0,{(r0j ,ej )})∈I(es))), hr = W Sum({φo(rθj, ej )}j ),
where hr is the hidden representation aggregated from the role-value pairs, γ(∙) is a concatenate
operator to output the representation of hyper-relation. The update of r in StarE is similar to
CompGCN. Instead of requiring role information, G-MPNN (Yadati, 2020) proposes to model
positional information in GNNs:
ei = act([ei, agg({rs,P (s) *	pej,s * ej}r(e1,...,en)∈I(ei))]),	(4)
j∈{1,...,n}
where S represents r(e1, . . . , en) ∈ S, P(S) : S → {1, . . . , np} is a positional mapping (np ≤ |E|),
and pej,s is the positional embedding vector of ej on fact S. To intuitively check the difference of
message functions, we plot the framework of general MPPNs and GNNs for KGs/HKGs in Fig. 1.
4
Under review as a conference paper at ICLR 2022
3	MSEAHKG
In this section, we first propose a search space, especially the space of message functions, which
enables the powerful GNNs to be searched for any given HKGs. Then, we formulate our search
problem on the proposed space and solve it by leveraging an efficient search algorithm.
3.1	GNN Search Space Design for HKG Embedding
As discussed in Sec. 1, designing the proper message function is more conducive to capturing
relational patterns of given HKGs and pursuing high empirical performance. Therefore, the first task
is to design an expressive search space so that message functions covering various relational patterns
can be searched. However, the space of existing GNN searching methods (see Eq. 1 and Fig. 1) does
not apply to HKGs and cannot cover the message functions of GNNs for KGs/HKGs (see Sec. 2.2).
Thus, we focus on the search space design of message functions in this paper.
Recently, various message functions have been proposed for KGs/HKGs as presented in Sec. 2.2.
From Fig. 1, we can observe that those message functions are mainly different in these two aspects:
1) the operators (e.g., W, φ, γ) for computing hidden representations, 2) the structure of message
functions that decides how computational operators are connected. For the operator selection, existing
works manually tune them on different data sets, such as φ in ComPGCN, φo,φr,γ in StarE, agg(∙)
in G-MPNN. Moreover, the structure of message functions for HKGs (StarE and G-MPNN) tends
to be deePer and more comPlex than those for KGs (e.g., ComPGCN). This is because the message
function needs to Process more information (e.g., more entities/roles) when facing the facts with
higher arity. These observations motivate us to build sPace of oPerators and structures for message
function search. Furthermore, we investigate more about the relationshiP between oPerators and
relational Patterns. Generally, the relational Pattern can be rePresented as a certain correlation among
r(pemu(e1, . . . , en)) (check more in APPx. B.2), where pemu denotes the Permutation. For examPle,
r is symmetry if r(ej, ei) must be true when r(ei, ej) is true. Therefore, the message function in the
search sPace must be able to handle such correlation in the form of r(pemu(e1, . . . , en)). In the next,
we first introduce the sPace of oPerators, and then discuss how they deal with r(pemu(e1, . . . , en)).
•	Positional Transformation Matrix WP(e): The Position of entity in a fact can largely determine
the Plausibility of the fact. For examPle, isCaptialOf(Beijing,China) is true while
isCaptialOf(China,Beijing) is false. G-MPNN utilizes the Positional embedding pe,s
and re,P (e) to encode the Position of entity e and relation r in different facts, which requires the
model comPlexity O(d|S|(|E| + N)). However, the training data set is very sParse in KBs. Such
over-Parameterization may make the training insufficient. Instead, we adoPt the way to transform
one entity e to N Possible Positions. Let the Positional maPPing be P (e) : e → {1, . . . , N}, then
the positional matrix is able to transform e to the permutation position in pemu(∙) by WP(e)e,
where WP (e) consumes O(N d2) (|S|	d in Practice).
•	Concatenate Operator γ(∙): It mainly determines the concatenation way between embedding
vectors. In this paper, we set Oγ = {concat, mult, wsum} (Galkin et al., 2020), where wsum
is the weighted sum. In general, γ(∙) can concatenate embeddings after the positional transform
matrix WP(e), i.e., encoding pemu(e1, . . . , en).
•	Role Embedding ro: Unlike the positional information of entities encoded by WP(e), the role em-
bedding is utilized to model the semantic information of entities (Liu et al., 2021). For example, the
roles in 2nd position of facts playCharacerIn(actor:ZacharyQuinto,character
:Spock,movie:StarTrek) and actorAwardIn(actor:ZacharyQuinto,award:
BC-BSFC,movie:StarTrek) are different though other entities and roles are same. Thus, the
model should be able to capture the role of candidate entities after using WP(e), e.g., BC-BSFC is
unlike to be the 2nd entity of playCharacerIn since its role is award instead of character.
•	Composition Operator φ(∙): Following CompGCN, we utilize composition operator φ(∙) to
capture message between the node and edge embeddings before aggregation step. Note that
φ actually encodes the interaction between r and pemu(e1, . . . , en). While CompGCN and
StarE empirically selects the most proper φ(∙), we include this operator into the space X. Thus,
MSeaHKG is able to automatically search for the most suitable φ(∙) in a more efficient way. We
combine the settings of CompGCN and StarE to set Oφ = {sub, mult, corr, rotat} (rotat (Sun
5
Under review as a conference paper at ICLR 2022
(a) Framework.
(b) One-layer MPNN.
Wl
O1
eι
Wlel
引...
~¾≡
ejv
WN
h0
θ1
(c) Message Function Space.
Figure 2: The framework OfMSeaHKG. Fig. (b) represents concrete framework of MPNN layer in
Fig. (a), and Fig. (c) is detailed formulation of mg(∙; θ) in Fig. (b). The operator layer Cnt enables
the connectivity of different layers (Jiang & Balaprakash, 2020).

…晅匾A
申伞卷0
et al., 2019), see others in Sec 2.2). Besides, φ not only occurs between primary relations with
entities, but also captures the correlation between roles with entities.
•	Others: (1) The idt(h) = h operation allows inputs to skip one layer in the message function; (2)
Unlike WP(e), the transform matrix W processes the hidden representations.
Among above components, we fix the role embedding and positional transform matrix WP (e) in the
message function (see Fig. 2 (c)) at the initial layer, which are specific designs to HKG problems.
And we include others into the space of message function O = {W, φ, γ, idt} for searching. As
shown in Fig. 2 (c), we denote the node oik as i-th operator of O in k-th layer and hik be the hidden
representation outputted by oik . Then we have:
{hi0}i2=n+1 1 = {WP(e1)e1, . . . , WP(en)en,ro1, . . . ,ron,r},	(5)
hk = ok({θjhk-1}j=1),for k ∈{1,...,K} and i ∈ {1,..., |O|},	(6)
where	θikj	∈	{0,	1} controls the connection between	oik	with	ojk-1.	Note that	{hi0}in=+11	=
{WP(e1)e1, . . . , WP(en)en, r} if role information is not available in the given HKG. From Fig. 2 (c),
we can observe that the structure of message functions is controlled by {θikj}.
To avoid manual operation selection, we also search for concrete operations of two operators φ and
γ. Given the operator set Oφ and Oγ, let θiφk , θiγk ∈ {0, 1} records the selection i-th operation
oi ∈ Oφ , Oγ at k-layer respectively. Then, φ and γ perform the computation in Eq. 6 could be
φk(h) = P θiφkoi(h) and γk(h) = P θiγkoi(h). Note that Pi θiφk = 1 and Pi θiγk = 1. Let
θmg = {θikj} ∪ {θiφk} ∪ {θiγk}. The message function parameterized by θmg is defined as:
mg(r(eι,..., en); θmg) = MLP &concat({hK }，=]),	(7)
where we simplify role embedding ro for simplicity and hiK is outputted by the last layer of Eq. 6.
Intuitively, existing GNNs for KGs/HKGs are contained in the MSeaHKG space (compare Fig. 1 and
Fig. 2). Moreover, we include more discussions about the instantiations of other KG/HKGs models
in the MSeaHKG space and the capability of handling relational patterns in Appx. B.2.
In addition to message function search, we also search for other operators (e.g., agg, comb, act) like
existing GNN search methods as shown in Fig. 2 (a) and (b). Let Θ = {θmg, θagg, . . . } be parameter
set for all operators selection in our MPNN framework. Then, an architecture can be represented as
Xθ = {mg(∙; θmg),agg(∙; θagg), .…} (check other functions in Appx. C.1). And existing GNNs
for KG/HKG embeddings actually can be represented by different instantiations of Θ. Overall, a
GNN model XΘ encodes the given HKG G into embedding space ω = {E, R}, i.e., ω = XΘ(G).
6
Under review as a conference paper at ICLR 2022
3.2	Search Algorithm Design
In this subsection, we introduce how to select the GNN XΘ that can achieve high performance on the
given G. First, we evaluate performance of XΘ based on the HKG embedding ω = {E, R} since
XΘ is utilized to encode G(E, R, S) into ω, i.e., ω = XΘ (G). In the KG/HKG embedding, the
scoring function f(s; ω) is to verify the plausibility of fact s = r(e1, . . . , en) by decoding ω (check
more about scoring functions in Appx. C.2). Generally, the good embedding ω can make f (s; ω) to
distinguish true or false for a given fact s. Thus, we build evaluation of XΘ on f (s; ω). Taking link
prediction task (predict the missing entity in a fact, e.g., r(?, e2, . . . , en)) as example, let a scoring
function f(s; ω) decode ω and output a score matrix ps ∈ [0, 1]|E|, where pse is the probability score
of e ∈ E may be the ground truth of the missing entity. Then, we follow Dettmers et al. (2018) to
construct the cross entropy loss `(f (s; ω)) = Pe∈E yes log pse, where yes = 1 if e is the ground truth
otherwise 0. Subsequently, we denote L(XΘ, ω; G) = Ps∈S `(f (s; ω)) to calculate the overall loss
ofa GNN model XΘ with the HKG embedding ω on G(E, R, S). Formally the GNN search problem
for a given HKG G is formulated as:
min L(XΘ, ω; G).	(8)
Θ,ω
Solving Eq. 8 is a non-trivial task because XΘ (e.g., XΘ = {agg : sum, act : relu, . . . }) is from
a large space. For example, only the structure space size {θikj } of message function reaches to
O(2K|O|2+(2N+1)|O|). AndX
Θ is discrete, indicating the gradient-based optimization cannot be
employed since Vxθ L(∙) does not exist. To enable an efficient search, We first relax the parameters of
GNN model Θ from a discrete space into a continuous and probabilistic space Θ. More specifically,
θj ∈ {0,1} restrictively controls the connectivity between ok with oj-1, while Gk ∈ [0,1] is the
probability that o is connected with oj-1 Then, let X 〜p® (X) represent a GNN model X being
sampled from the distribution p® (X). We reformulate the problem in Eq. 8 into:
min EX〜pθ(x)[L(X, ω; G)],	(9)
Θ,ω
where E[∙] is the expectation. To compute the gradient w.r.t. Θ, we first utilize the reparameterization
trick X = g® (U) (Maddison et al., 2016), where U is sampled from a uniform distribution P(U).
Then the gradient w.r.t. ΘG and ω is computed as (check full derivation in Appx. C.4):
V®EX〜p®(x)[L(X, ω; G)] = EU〜p(u)[L0(g®(U),ω; D)V®g®(U)],	(10)
VωEX〜pθ(X)[L(X, ω; G)] = EX〜pθ(X)[VωL(X, ω; G)].	(11)
Note that V®g® (U) can be computed if g® (U) is differentiable. Inspired by SNAS (Xie et al.,
2018), we leverage Maddison et al. (2016); Jang et al. (2016) to instantiate g® (U) (see Appx. C.4).
4	Experiments
4.1	Experimental Setup
The experiments are implemented on top of PyTorch (Paszke et al., 2019) and performed on one
single RTX 2080 Ti GPU. Appx. D.1.1 introduces the details of hyper-parameters.
Data Sets. The details of data sets are summarized into Tab. 7 in Appx. D.1.2. For experiments on
HKGs (facts with mixed arities), we employ 2 benchmark data sets: (1) Wiki-People (Guan et al.,
2019) is extracted from wiki-data, which mainly concerns the entities that belong to the human type.
(2) JF17K (Zhang et al., 2018) is extracted from Freebase (Bollacker et al., 2008). For experiments
on facts with fixed arities, we utilize several data sets with fixed arities and n > 2: WikiPeople-3,
JF17k-3, WikiPeople-4, and JF17k-4. Note that GETD (Liu et al., 2020) filters out the 3-ary and
4-ary facts from WikiPeople and JF17K to construct WikiPeople-n and JF17k-n, respectively.
Tasks and Evaluation Metrics. In this paper, we mainly compare HKG embedding models on the
link and relation prediction task in the transductive setting. The link prediction task is to predict
the missing entity in the given fact at n possible positions, e.g., predicting the first missing entity
r(?, e2,…,en). The relation classification task needs to predict the missing relation in a fact when
all entities are known, i.e., ?(ei,…，en). We employ Mean Reciprocal Ranking (MRR) (Voorhees,
1999) and Hits@{1, 3, 10}. All reports are in the “filtered” setting (see more in Appx. D.1.3).
7
Under review as a conference paper at ICLR 2022
Table 2: The model comparison of the link prediction task on HKGs. The results of NNs and
multi-linear baselines are copied from Liu et al. (2020), those of Geo and S2S are copied from Di
et al. (2021). And GNN baselines are re-implemented due to the task variance.
type	model		WikiPeople					JF17K			
		MRR	Hit@1	Hit@3	Hit@10	MRR	Hit@1	Hit@3	Hit@10
Geo	-RAE	0.172	0.102	0.182	0.320	0.310	0.219	0.334	0.504
	-NaLP	0.338	0.272	0.364	0.466	0.366	0.290	0.391	0.516
NNs	HINGE	0.333	0.259	0.361	0.477	0.473	0.397	0.490	0.618
	NeUInfer	0.350	0.282	0.381	0.467	0.517	0.436	0.553	0.675
Multi-	HyPE	0.292	0.162	0.375	0.502	0.507	0.421	0.550	0.669
linear	RAM	0.380	0.279	0.445	0.539	0.539	0.463	0.573	0.690
GNNs	StarE	0.378	0.265	0.452	0.542	0.542	0.454	0.580	0.685
	G-MPNN	0.367	0.258	0.439	0.526	0.530	0.459	0.572	0.688
Search	^S2S	0.372	0.277	0.439	0.533	0.528	0.457	0.570	0.690
	MSeaHKG	0.395	0.291	0.470	0.554	0.577	0.481	0.599	0.711
Table 3: The model comparison of the relation prediction task on HKGs. The results of NNs are
copied from Guan et al. (2020), others are based on our implementations.
tyPe	model		WikiPeople					JF17K			
		MRR	Hit@1	Hit@3	Hit@10	MRR	Hit@1	Hit@3	Hit@10
NNs	-NaLP	0.735	0.595	0.852	0.938	0.825	0.762	0.873	0.927
	NeuInfer	0.765	0.686	0.877	0.900	0.861	0.832	0.885	0.910
GNNs	-StarE	0.800	0.753	0.936	0.951	0.901	0.884	0.929	0.963
	G-MPNN	0.777	0.694	0.905	0.912	0.864	0.842	0.883	0.917
Search	^S2S	0.813	0.744	0.928	0.960	0.912	0.877	0.932	0.951
	MSeaHKG	0.831	0.787	0.955	0.972	0.933	0.894	0.950	0.972
Baselines. Except for GNNs in Sec. 2.2, we present key functions of most adopted baselines in Tab. 5.
For tasks on HKGs, we mainly compare the proposed method with advanced HKG embedding models:
(1) Geometric model: RAE (Zhang et al., 2018), which is upgrade version of m-TransH (Wen et al.,
2016) that extended from TransH (Wang et al., 2014); (2) GNN-based models: G-MPNN (Yadati,
2020) and StarE (Galkin et al., 2020) (note that we re-implement and tune them because G-MPNN
is under inductive settings and StarE only tests the performance of main triplets in hyper-relational
facts); (3) Other NN-based models: NaLP (Guan et al., 2019), HINGE (Rosso et al., 2020), and
NeuInfer (Guan et al., 2020); (4) Multi-linear models: The final score of HypE (Fatemi et al., 2020)
and RAM (Liu et al., 2021) is computed by multi-way inner product which is extended from bilinear
KG embedding models (Yang et al., 2015). (5) Search method: S2S (Di et al., 2021) proposes a search
space for tensor decomposition models and searches for sparse core tensors. It shares embeddings to
jointly learn from facts with mixed arities to alleviate the issue of tensor modeling.
Except for the above methods on HKGs, we also include the tensor decomposition models (n-CP,
n-TuckER, GETD) that work well on the scenario of facts with fixed arity. n-CP (Lacroix et al., 2018)
leverages CANDECOMP/PARAFAC decomposition (Hitchcock, 1927), while n-TuckER (Balazevic
et al., 2019) is based on Tucker decomposition (Tucker, 1966). GETD (Liu et al., 2020) proposes to
reduce the model complexity by tensor ring decomposition.
4.2	Main Experimental Results
The main results on HKGs (i.e., WikiPeople and JF17K) have been summarized into Tab. 2 and Tab. 3.
Compared with Geometric and classic NN-based methods (e.g., NaLP), GNNs methods (e.g., StarE)
achieve outstanding performance, which demonstrates the power of GNNs on the graph tasks. And
StarE generally is better than G-MPNN in GNNs methods because the inner product way in G-MPNN
cannot handle several relational patterns as mentioned in Sec. 1. Besides, although the multi-linear
method RAM utilizes the simple inner product as its scoring function, it carefully models the role
semantic information and interaction patterns, thus achieving good performance. Another searching
method S2S alleviates the extension issue of tensor decomposition models from the fixed to the mixed
scenario. It is still slightly inferior compared with other state-of-the-art methods. Overall, all existing
methods cannot consistently achieve the leading performance on different tasks and data sets. In this
8
Under review as a conference paper at ICLR 2022
Table 4: The model comparison of the link prediction task on facts with fixed arity. The results of
tensor models are copied from Liu et al. (2020), others are copied from Di et al. (2021).
type	model	WikiPeople-3		JF17K-3		WikiPeople-4		JF17K-4	
		MRR	H@10	MRR	H@10	MRR	H@10	MRR	H@10
Geo	-RAE	0.239	0.379	0.505	0.644	0.150	0.273	0.707	0.835
	NaLP	0.301	0.508	0.515	0.679	0.342	0.540	0.719	0.805
NNs	HINGE	0.338	0.508	0.587	0.738	0.352	0.557	0.745	0.842
	NeUInfer	0.355	0.521	0.622	0.770	0.361	0.566	0.765	0.871
	-n-CP	0.330	0.496	0.700	0.827	0.265	0.445	0.787	0.890
Tensor	n-TuckER	0.365	0.548	0.727	0.852	0.362	0.570	0.804	0.902
	GETD	0.373	0.558	0.732	0.856	0.386	0.596	0.810	0.913
	-S2S	0.386	0.559	0.740	0.860	0.391	0.600	0.822	0.924
Search	MSeaHKG	0.405	0.583	0.757	0.892	0.412	0.628	0.834	0.940
paper, MSeaHKG pursues the high model performance by dynamically designing the most suitable
message function for the given HKG and task. The searched message functions can capture data-level
properties (see case study in Appx. D.2), thereby showing the leading performance. Especially,
the search space of another search method S2S is based on the tensor modeling. As discussed in
Sec. 1 and Appx. A, the MRHG could be a more natural way to represent HKGs, thereby MSeaHKG
benefits from building a message function search space in GNNs under the MRHG modeling.
We show experiments on facts with fixed arity in Tab. 4. As discussed in Sec. 1, we can indeed
observe that classic tensor decomposition models (n-CP, n-TuckER, GETD) perform better than
Geometric and NN-based methods on the scenario of fixed arity. Then, S2S proposes to dynamically
sparsify the core tensor of tensor decomposition models for the given data and further improve the
performance of tensor models. Moreover, we note that MSeaHKG still performs better than S2S
even in the scenario of fixed arity. That is because S2S simply assumes 3 relationships between
entities and relations in the search space: positive, irrelevant, and negative. But the message function
space in Sec. 3.1 could characterize more complex interactions between entities and relations, thereby
achieving improvements.
4.3	More Insights via Empirical S tudy
Due to the space limitation, we include more experimental results in Appx. D to provide more insights
and verify several claims in this paper. First, we demonstrate case studies in Appx. D.2 that the
searched message functions are data-dependent and can adapt to the given data set. Second, we
conduct ablation studies to analyze the components of the proposed method in Appx. D.3. Specifically,
we show several variants of the proposed search space to demonstrate: (1) with a simple extension of
message function, the GNN searching method cannot work well on HKGs (e.g., AutoGEL discussed
in Sec. 2.2), (2) automatic operation selection can improve the performance built on the manual
operation tuning, (3) the structure design of message functions is important to improve performance
on HKGs. Then, we implement two more popular NAS algorithms (Liu et al., 2018; Akimoto et al.,
2019) to compare the searching effectiveness and efficiency of one-shot NAS search algorithms in
our scenarios. Third, we transfer MSeaHKG to more variety of graph-based tasks in Appx. D.4.
5	Conclusion
In this paper, we propose a new message function searching method for HKGs, named MSeaHKG.
First, we present a novel search space of message functions in MPNNs, which enables both structure
search and operation selection in message functions. With our expressive message function design,
some classic KGs/HKGs models and existing message functions for HKGs could be instantiated
as special cases of the proposed space. Then, we develop an efficient one-shot NAS algorithm to
search the message function and other GNN components for the given HKG. The empirical study
demonstrates that the searched message functions are data-dependent and can adapt to the data
patterns of the given HKGs. Overall, MSeaHKG has shown its effectiveness and efficiency on
benchmark data sets.
9
Under review as a conference paper at ICLR 2022
References
Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari, Kento Uchida, Shota Saito, and Kouhei
Nishida. Adaptive stochastic natural gradient method for one-shot neural architecture search. In
ICML, 2019.
Soren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.
DbPedia: A nucleus for a Web of open data. In The Semantic Web, pp. 722-735. Springer, 2007.
I.	Balazevic, C. Allen, and T. Hospedales. Tucker: Tensor factorization for knowledge graph
completion. In EMNLP, pp. 5188-5197, 2019.
J.	Bergstra, D. Yamins, and D. D. Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. 2013.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabo-
ratively created graph database for structuring human knowledge. In SIGMOD, pp. 1247-1250,
2008.
A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for
modeling multi-relational data. In NIPS, pp. 2787-2795, 2013.
Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. Unifying knowledge graph
learning and recommendation: Towards a better understanding of user preferences. In WWW, pp.
151-161,2019.
Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. Fair darts: Eliminating unfair advantages
in differentiable architecture search. In ECCV, pp. 465U80. Springer, 2020.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D
knowledge graph embeddings. In AAAI, 2018.
Shimin Di, Quanming Yao, and Lei Chen. Searching to sparsify tensor decomposition for n-ary
relational data. In WebConf, pp. 4043T054, 2021.
Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al. Neural architecture search: A survey. J.
Mach. Learn. Res., 20(55):1-21, 2019.	一
Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimiza-
tion at scale. In ICML, pp. 1437-1446. PMLR, 2018.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In WWW, pp. 417T26, 2019.
Bahare Fatemi, Perouz Taslakian, David Vazquez, and David Poole. Knowledge hypergraphs:
Prediction beyond binary relations. 2020.
Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Message
passing for hyper-relational knowledge graphs. In EMNLP, 2020.
Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search. In
IJCAL volume 20, pp. 1403-1409, 2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, pp. 1263-1272. PMLR, 2017.
Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational
data. In WWW, pp. 583-593, 2019.
Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng. Neuinfer: Knowledge
inference on n-ary facts. In ACL, pp. 6141-6151, 2020.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NeUrIPS, pp. 1025-1035, 2017.
10
Under review as a conference paper at ICLR 2022
Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. JOUrnal of
MathematiCS and Physics, 6(1-4):164-189, 1927.
F. Hutter, L. Kotthoff, and J. Vanschoren. AUtOmated MaChine Learning: Methods, Systems,
Challenges. Springer, 2018.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
PrePrint arXiv:1611.01144, 2016.
Shengli Jiang and Prasanna Balaprakash. Graph neural network architecture search for molecular
property prediction. In IEEE Big Data, pp. 1346-1353. IEEE, 2020.
S.	Kazemi and D. Poole. Simple embedding for link prediction in knowledge graphs. In NeUrIPS, pp.
4284T295, 2018.
D.P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In ICLR, 2016.
T.	Lacroix, N. Usunier, and G. Obozinski. Canonical tensor decomposition for knowledge base
completion. InICML, pp. 2863-2872, 2018.
H. Liu, K. Simonyan, and Y. Yang. DARTS: Differentiable architecture search. In ICLR, 2018.
Yu Liu, Quanming Yao, and Yong Li. Generalizing tensor decomposition for n-ary relational
knowledge bases. In WebConf, pp. 1104—1114, 2020.
Yu Liu, Quanming Yao, and Yong Li. Role-aware modeling for n-ary relational knowledge bases. In
WebConf, pp. 2660-2671, 2021.
Denis Lukovnikov, Asja Fischer, Jens Lehmann, and Soren Auer. Neural network-based question
answering over knowledge graphs on word and character level. In WWW, pp. 1211-1220, 2017.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv PrePrint arXiv:1611.00712, 2016.
Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla, and Heiner
Stuckenschmidt. Fine-grained evaluation of rule-and embedding-based systems for knowledge
graph completion. In ISWC, pp. 3-20, 2018.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33, 2015.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge
graphs. In AAAI, 2016.
A.	Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeUrIPS,
pp. 8024-8035, 2019.
H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Efficient neural architecture search via parameter
sharing. InICML, pp. 4092T101, 2018.
Andrea Rossi, Denilson Barbosa, Donatella Firmani, Antonio Matinata, and Paolo Merialdo. Knowl-
edge graph embedding for link prediction: A comparative analysis. TKDD, 15(2):1T9, 2021.
Paolo Rosso, Dingqi Yang, and Philippe Cudre-Mauroux. Beyond triplets: hyper-relational knowledge
graph embedding for link prediction. In WebConf, pp. 1885-1896, 2020.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In ESWC, pp. 593-607.
Springer, 2018.
11
Under review as a conference paper at ICLR 2022
DI Shimin, YAO Quanming, Yongqi ZHANG, and CHEN Lei. Efficient relation-aware scoring
function search for knowledge graph embedding. In ICDE, pp. 1104-1115. IEEE, 2021.
Z. Sun, Z. Deng, J. Nie, and J. Tang. RotatE: Knowledge graph embedding by relational rotation in
complex space. In ICLR, 2019.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by
relational rotation in complex space. In ICLR, 2018.
K. Toutanova and D. Chen. Observed versus latent features for knowledge base and text inference. In
WorkshoP on CVSMC, pp. 57-66, 2015.
T. Trouillon, Christopher R., E. Gaussier, J. Welbl, S. Riedel, and G. Bouchard. Knowledge graph
completion via complex tensor factorization. JMLR, 18(1):4735^772, 2017.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):
279-311, 1966.
Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based multi-
relational graph convolutional networks. In ICLR, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998-6008, 2017.
Ellen Voorhees. The trec-8 question answering track report. In TREC, volume 99, pp. 77-82, 1999.
Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge graph embedding: A survey of approaches and
applications. TKDE, 29(12):2724-2743, 2017.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In AAAI, 2014.
Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. Pooling architecture search for
graph classification. In PrOCeedingS of the 30th ACM International COnferenCe on InfOrmatiOn &
Knowledge Management, pp. 2091-2100, 2021.
Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and
embedding of knowledge bases beyond binary relations. In IJCAI, 2016.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. ML, 8(3-4):229-256, 1992.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search.
arXiv PrePrint arXiv:1812.09926, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? InICLR, 2018.
Naganand Yadati. Neural message passing for multi-relational ordered and recursive hypergraphs.
AdvanCeS in NeuraI InfOrmatiOn PrOCeSSing Systems, 33, 2020.
B.	Yang, W. Yih, X. He, J. Gao, and L. Deng. Embedding entities and relations for learning and
inference in knowledge bases. In ICLR, 2015.
Quanming Yao and Mengshuo Wang. Taking human out of learning applications: A survey on
automated machine learning. Technical report, arXiv:1810.13306, 2018.
Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. AdvanCeS in
NeUraI InfOrmatiOn PrOCeSSing Systems, 33, 2020.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Un-
derstanding and robustifying differentiable architecture search. arXiv PrePrint arXiv:1909.09656,
2019.
12
Under review as a conference paper at ICLR 2022
(a) KG With Multi-relational Graph Modeling.
Figure 3: The illustration to KGs and HKGs. The binary facts are simplified in (b) to make the 3-ary
facts look more prominent.
(b) HKG with Multi-relational Hypergraph Modeling.
Richong Zhang, Junpeng Li, Jiajie Mei, and Yongyi Mao. Scalable instance reconstruction in
knowledge bases via relatedness affiliated embedding. In WWW, pp. 1185-1194, 2018.
Yongqi Zhang, Quanming Yao, Wenyuan Dai, and Lei Chen. AutoSF: Searching scoring functions
for knowledge graph embedding. In ICDE, pp. 433U44, 2020.
Ziwei Zhang, Xin Wang, and Wenwu Zhu. Automated machine learning on graphs: A survey. arXiv
PrePrint arXiv:2103.00742, 2021.
Huan Zhao, Quanming Yao, and Weiwei Tu. Search to aggregate neighborhood for graph neural
network. arXiv PrePrint arXiv:2104.06608, 2021.
Wang Zhili, Di Shimin, and Chen Lei. Autogel: An automated graph neural network with explicit
link information. In NeUrIPS, 2021.
Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of
graph neural networks. arXiv PrePrint arXiv:1909.03184, 2019.
A DISCUSSION ON THE MODELING OF HKG
Given a set of facts with the fixed arity {r(eι,..., en)}, tensor models (e.g., CP (Lacroix et al., 2018)
and TuckER (Balazevic et al., 2019)) use a (n +1)-dimensional tensor Gn+1 ∈ {0, l}lRl×lEl×…×lEl
to represent facts, where Gn,+,1..,n = 1 represents the fact r(eι,...,en) existed otherwise Gn,+,1..,n =
0. Then, tensor models leverage the tensor decomposition techniques to decompose Gn+1 into
embeddings R and E with a core tensor Zn+1 ∈ Rd× ×d:
Gn+1 = Zn+1 ×1 R ×2 E ×3 ∙∙∙×n+ι E.
Obviously, a tensor Gn+1 can only model facts with the fixed arity n. Thus, we have to build
{G2,..., GN+1} to model the facts with mixed arities S = {r(eι,...,en) : n ∈ {2,..., N}} and
decompose them into N - 1 sets of entity and relation embeddings, such as {En, Rn}N=2. But KBs
are known to have the data sparse issue, thus learning multiple set of embeddings could lead to severe
problems.
As mentioned in Sec. 1, multi-relational hypergraphs (MRHGs) G(E, R, S) could be a more natural
way to represent HKGs. We illustrate examples of KGs and HKGs in Fig. 3 (a) and (b), respectively.
Given a fact with high arity (e.g., playCharacterIn(e1 ,e3,e5 )), MRHG can first store the
correspondence between multiple entities by a hyperedge (e1, e3, e5), then label this hyperedge with
the relation playCharacterIn. Moreover, Fig. 3 (b) demonstrates that the facts with mixed
arities (n ∈ {2, 3} in this example) can be represented by a MRHG. Thus, MRHG allows us to
directly encode a given HKG into entity and relation embeddings, where every embedding vector is
jointly learned from low and high arities. However, tensor modeling leads the model to learn multiple
sets of embeddings.
13
Under review as a conference paper at ICLR 2022
(a) HolE (Nickel et al.,
2016).
(b) RotatE (Sun et al.,
2019).
(c) n-CP (Lacroix et al.,	(d) n-DistMult (Yang
2018).	etal.,2015).
Figure 4: Several instantiation cases of MSeaHKG message function space. ◦ denotes circular
correlation cοrr(∙) (Nickel et al., 2016), * denotes inner product mult(∙) (Yang et al., 2015),-
denotes the substraction sυb(∙) (Bordes et al., 2013), ? is from RotatE (Sun et al., 2018).
Besides, HKGs generally contain more complete information compared with KGs. For example, it is
hard to answer “who plays Spock in StarTrek-3?” with the KG example (Fig. 3 (a)). That is because
KGs suffer from the information loss of correspondence among multiple entities. Generally, the
constructing procedure of KG, named Star-to-Clique Conversion, has been verified to be irreversible
and caused the information loss in the facts with high arities (Wen et al., 2016). Therefore, we believe
that HKG could be a potential solution to solve complex Q&A scenarios.
B	Discussion on DATA-LEVEL Properties of KGs/HKGs and Message
Function Design
B.1	Data-level Properties in KGs: relational Patterns
In the past decades, the research community has found that the relations of KGs exhibit differ-
ent patterns. For example, we can infer that neighbοr0f(lοcatiοn2,lοcatiοn1)
must be true as long as neighbοr0f(lοcatiοn1,lοcatiοn2) is true, like
neighborOf(ShenZhen ,HongKong). But for other relations (e.g., IargerThan), we know
largerThan(Value2,valυe1) must be false if largerThan(Value1,valυe2) is true.
Many relational patterns in KGs have been found and discussed (Kazemi & Poole, 2018; Rossi et al.,
2021), such as symmetry, anti-symmetry, asymmetry, inversion, composition, hierarchy, intersection.
However, various KGs usually contain different relational patterns with different proportions of
facts. To handle diverse relational patterns on different KGs (i.e., data-level properties), kinds of
scoring functions have been proposed to cover as many relational patterns as possible, i.e., pursuing
expressiveness. Given the learned embeddings ω = {E, R}, the scoring function f (∙) is proposed to
evaluate whether the fact is true or false (check Appx. C.2 and Tab. 5 for more details about scoring
function). For the sake of understanding, we here present the requirements of several relational
patterns on scoring functions:
•	Symmetry: r is symmetry if r(ej, e. must be true when r(ei, e§) is true, i.e., f (r(ei, ej))=
f(r(ej, ei))
•	Anti-symmetry: r is anti-symmetric if r(ej,ei) must be false when r(ei,ej) is true, i.e.,
f(r(ei, ej)) = -f(r(ej, ej).
•	Asymmetry: The plausibility of r(ej,ei) is unknown when r(ei,ej) is true, f (r(ei, ej)) =
f(r(ej, ei)).
•	inverse: r1 and r2 are inverse relations if r1 (ei, ej) is true and r2(ej, ei) is true. Then,
f(r1(ei,ej)) = f(r2(ej,ei))
As mentioned in sec. 1, the message function of G-MPNN may not be able to infer the non-symmetric
relations. suppose that s1 = r(e1, e2) is true and r is one of any non-symmetric relations (e.g., anti-
symmetry s2 = r(e2, e1) must be false). Let hi be the entity feature and hr be the relation feature.
According to the message function of G-MPNN in Eq. 4, we can learn the hidden embeddings of mi :
m1 = m2 = hr * h1 * pe1,s * h2 * pe2,s Then ei are updated with mi. With the scoring function
14
Under review as a conference paper at ICLR 2022
Table 5: Scoring Functions for Facts with High-order Arities.
Type	Model	Scoring Function Design f (s; ω)	Notes
Geo	m-TransH RAE	IIPn=1 aj (ei - v>eivr) + rl∣2 IlPn=I aj (ei - v>ei vr ) + r∣l2 + Pij FC ([ei, ej ])	vr is relation dependent. FC(∙) is fully connected layer.
Tensor	n-CP n-TuckER GETD	r * e11) * e22) * ∙∙∙ * enn) Z ×1 r ×2 e1 ×3 …×n+1 en TR(W 1, ..., W k) ×1 e1 ×2 e2 ×3 …×n+1 en	ei(j) is for ei in j-th po- sition. Z ∈ Rd×…×d, Xi is ten- sor product in i-th mode. TR(∙) is tensor ring de- composition Wi is a 3- order tensor.
GNNs	G-MPNN	rs,P (S) * IL。∈Seen(s) Pe°,s * ej	S een(s) returns the seen entities ej in s
Multi- linear	HypE RAM	r * Conv(eι) * C0nν(e2) *∙∙∙* Conv(en) Pn=I ri * Pr (1, :)el * …* Pr (n, :)en	Conv(∙) is the convolu- tional filter. ri is embedding for i-th role of r, ej∙ ∈ Rm×d is embedding matrix of e
Search	S2S	Pjr,jl,...,js Zn ×1 rjr ×2 e11 ×3 …×n+1 enn	Zin is the searched sparse core tensor, i denotes (jr,j1, . . . ,js).
of G-MPNN (multiple inner product), We can compute f (r(eι, e2)) = hr * Peι,s * eι * Pe2,s * e2.
Because e1, e2 are seen in s1, G-MPNN will assign the positional embeddings e1, e2 in s1 when
computing the score of s2, thereby leading f(r(e1, e2)) = f(r(e2, e1)). Thus, We argue that the
fixed message function is not flexible enough to capture the relational patterns of the given HKGs,
especially its performance may not be good if some uncovered relational patterns exist in the given
data set.
B.2	Discussion on the Capability of Message Functions to Cover Relational
Patterns on HKGs
As mentioned in Appx. B.1, pursuing expressiveness is one Way to cover relational patterns on KGs.
Such potential solution is also discussed in Sec. 1. HoWever, Meilicke et al. (2018); Rossi et al. (2021)
report that being expressive does not mean achieving good performance even the model can cover
those relational patterns. Thus, AutoSF (Zhang et al., 2020) proposes to search bilinear-based scoring
functions for KGs and consistently achieve good performance. Inspired by literature, MSeaHKG
proposes to search proper message functions for the pursuit of high model performance on any given
HKGs.
Generally, the relational patterns on HKGs have not been explored before. Inspired by literature on
KGs, We roughly summarize a high level representations of relational patterns in the scenario of high
arity as:
f(r1(pemu(e1,...,en)); ω)? f (r2(pemu(e1,..., en)); ω),
where r1,r2 may be same relation or different, Pemuq) represents the permutation of n
entities {ei}in=1, and ? ∈ {=, >, 6=, }. For example, f(r1(pemu(e1, . . . , en)); ω) =
f(r1(pemu(e1, . . . , en)); ω) indicates r1 is a symmetry. In this paper, since our scoring func-
tion design is a linear transformation (see Appx. C.2), the design of message function in MPNNs (i.e.,
ω = XΘ(G)) will be the crucial component to capture relational patterns. To capture the permutation
information pemu(∙), we first utilize WP⑻ to encode the positional information, which transforms
entity embeddings into corresponding positions. Then, the operator γ(∙) concatenates the entity
embeddings after encoding positional information. Lastly, the operator φ(∙) computes the interaction
between r with ei. In other words, WP⑹ and γ(∙) are employed to represent pemu(∙), and φ(∙) is
utilized to represent the correlation between r andpemu(∙). Within such design, most classic scoring
function designs (including Geometric models (Bordes et al., 2013; Nickel et al., 2016; Sun et al.,
2019), Bilinear models (Yang et al., 2015) and Tensor models (Lacroix et al., 2018)) and GNNs
15
Under review as a conference paper at ICLR 2022
for KGs/HKGs can be instantiated as special cases in the MSeaHKG space. We have demonstrated
several examples of classic scoring function designs in Fig. 4. Note that the expressiveness of these
models has been fully investigated in literature. Every model can cover one or multiple relational
patterns, which can make our message function space expressive enough to handle most HKGs. In
other words, the search space of MSeaHKG has the potential to return a GNN model that can adapt
to the relational patterns of the given HKG.
C S upplementary of Search Algorithm
C.1 The Parameterization of Other GNN Components
Generally, the relaxation of other GNN components (e.g., aggregation and activation) is consistent
with φk(h) = P θiφk oi (h) and γk(h) = P θiγkoi(h) as presented in Sec. 3.1. Here we illustrate
more details.
•	Let Oagg = {sum, mean, max} be the set of candidate aggregation functions and
mg(r(eι,..., e。)； θmg) be the output from Eq. 7. Then, the aggregation function agg(∙; θagg)
parameterized by θagg can be defined as:
mi = agg({mg(r(e1, .. ., en); θmg)}r(e1,...,en)∈I(ei); θagg)
=X j ∙ Oj ({mg(r(eι,..., e。)； θmg )}「( )∈i(e"
oj ∈Oagg
•	Let Oact = {identity, sigmOid, tanh} be the set of candidate activation functions. Then, the
activation function act(∙; θact) parameterized by θact can be defined as:
ei = act(comb(ei, mi); θact) = ɪ2 θact ∙ Oj(Comb(ei, mJ)
oj ∈Oact
Then, as presented in Sec. 3.2, We can relax θagg and θact into continuous space 8agg and θact by
leveraging the Gumbel-Softmax technique.
C.2 Scoring Function Formulation
As mentioned in Sec. 3.2, the GNN model X embeds the HKG G(E, R, S) into the loW-dimensional
vector space ω = {E, R}, i.e., ω = X(G). Besides, it is also important to design a scoring function
f(s; ω) to interprets the plausibility of the fact s based on the learned ω (Rossi et al., 2021; Zhang
et al., 2020; Shimin et al., 2021). Many promising scoring functions have been proposed in the
past decades, including geometric models (e.g., TransE (Bordes et al., 2013) and RotatE (Sun et al.,
2018)), neural netWork models (e.g., ConvE (Dettmers et al., 2018)), tensor decomposition models
(e.g., CP (Lacroix et al., 2018), TuckER (Balazevic et al., 2019)). Inspired by the success of literature,
We regard the scoring function design as a component of implementations. In this paper, We focus on
those scoring functions for facts With high arity (n > 2) and summarize the popular ones in Tab. 5.
Note that StarE (Galkin et al., 2020) employs the Transformer (VasWani et al., 2017) as its scoring
function, thus it is not included in Tab. 5.
In principle, MSeaHKG can implement most of existing scoring functions as its decoder. But We
argue that the poWer of the searched message function could Well model the interaction betWeen
entities and relations in the encoding step. Within a poWerful encoder, it may be unnecessary to
introduce a complex scoring function like CompGCN (Vashishth et al., 2020) and StarE (Galkin et al.,
2020) to decode embeddings. In this paper, We simply concatenate the embeddings of knoWn entities
and relations in a fact and feed it into the linear transformation With a softmax operator.
C.3 Loss Function w.r.t. Relation Prediction
Note that the loss function L(X, ω; G) introduced in Sec. 3.2 is under the scenario of link prediction
task. In the relation classification task, the probability score is formed to ps ∈ [0, 1]|R|, i.e., predicting
16
Under review as a conference paper at ICLR 2022
the ground truth r ∈ R when the relation is missing in ?(e1, . . . , e2). The loss function is defined as:
L(X, ω; G) = X X yrs log prs,
s∈S r∈R
where yrs = 1 if r is the ground truth relation in s.
C.4 Full Derivation involved in Sec. 3.2
First, we present the full derivation of Eq. 10 and Eq. 11, and their approximation based on Monte-
Carlo sampling:
V<ΘEX〜PΘ(X)[L(X,ω; G)] = v⅛EU〜P(U)[L(g<θ(U),ω; G)] = V⅛ /P(U)L(g⅛(U), ω; G)dU
/P(U)VθL(gΘ (U), ω; G)dU = EU〜P(U)[V®L(gΘ (U), ω; G)]
EU〜P(U) [L0(3®(U), ω; G)vθgθ(U)],
VωEX〜pθ(X)[L(X, ω; G)]
Vω ∕pθ(X)L(X, ω; G)dX
/PΘ(X)VωL(X, ω; G)dX
EX 〜pθ (X) [Vω L(X, ω; G)].
Second, We build the reparameterization trick X = g® (U) (mentioned in Sec. 3.2) based on Gumbel-
Softmax (Jang et al., 2016) or Concrete distribution (Maddison et al., 2016). For simplicity, we
simplify Θ to the parameter θ for a specific operator space O:
X =	(U) =	exp((log θo - log(-log(Uo)))/t)
o = gθ( )= Po0∈o exp((log θo0 - log(-lοg(Uo0)))∕τ),
(12)
where T is the temperature of softmax, and Uo 〜 Uniforma 1)∙ It has been proven that
p(limτ→o Xo = 1) = θo∕ Poο三ο θoο making the stochastic differentiable relaxation unbiased
once converged (Xie et al., 2018). And the details of V®g® (U) can refer to Xie et al. (2018).
D More Experiments
D.1 Experimental Setup
D.1.1 Hyper-parameter Settings
We have summarized the hyper-parameters of this paper in Tab. 6. The hyper-parameter set includes
Adam optimizer (Kingma & Ba, 2014), learning rate ∈ {0.1, 0.01, 0.001, 0.0001, 0.00001}, # MPNN
layers ∈ {1, 2, 3, 4} (see left part of Fig. 2), # layers in message functions K ∈ {1, 2, 3, 4, 5} (see
right part of Fig. 2), batch size ∈ {64, 128, 256, 512}, embedding dimension d ∈ {64, 128, 256, 512},
dropout ratio ∈ {0, 0.05, 0.1, 0.15, . . . , 0.5}, label smoothing ratio T ∈ {0, 0.1, 0.2, . . . , 0.9}. Note
that the label smoothing ratio T is employed to relax the one-hot label vector y. In practical, we
set yi = 1 一 τ for the ground truth entity/relation, while y% = t∕∣e∣-i for link prediction and
yi = τ∕∣R∣-ι for relation prediction. That is because there are usually a large candidate space for
relations and entities, while Using one-hot vector is quite restrictive. All hyper-parameters are tuned
with the help of optuna.samplers.TPESampler (Bergstra et al., 2013; Falkner et al., 2018)1.
D.1.2 Data Statistics
Here we present the statistic summary of the benchmark data sets in this paper. And the links for
download them.
1https://optuna.readthedocs.io/en/stable/reference/generated/optuna.
samplers.TPESampler.html
17
Under review as a conference paper at ICLR 2022
Table 6: List of hyper-parameters in main experiments. W and J are abbreviations of WikiPeople and
JF17K, respectively.
Hyperparameters		Link Prediction							Relation Prediction	
	WikiPeople	JF17K	W-3	J-3	W-4	J-4	WikiPeople	JF17K
Learning rate	0.0001-	0.001	0.0001	OOr	0.0001	0.0001	0.0001-	0.001
# MPNN layers	2	-2-	-1-		1	-1-	2	-2-
# Layers in mg(∙) K	4	4	3	2	2	3	4	4
Batch size	256	128	128	^T28^	256	128	256	128
Embedding dim d	256	256	128	128	256	128	128	128
Dropout ratio	-0T5-	0.2	0.1	~n~	0.05	0.15	02	0.15
Label smoothing ratio τ	0.3	0.8	0.7	0.5	0.8	0.7	0.1	0.1
Table 7: The statistical summary on data sets.
type	data set	# all facts	# facts (n > 2)	max arity N	# ent	# rel	train	valid	test
Mixed	-JF17K-	100,947	46,320	6	28,645	^32∑	76,379	-	24,568
	WikiPeoPle	382,229	44,315	9	47,765	707	305,725	38,223	38,281
	WN18RR	93,003	0	2	40,943	~TΓ	86,835	3,034	3,134
	FB15k237	310,116	0	2	14,541	237	272,115	17,535	20,466
Fixed	JF17K-3	34,544	34,544	3	11,541	104	27,635	3,454	3,455
	JF17K-4	9,509	9,509	4	6,536	23	7,607	951	951
	WikiPeople-3	25,820	25,820	3	12,270	66	20,656	2,582	2,582
	WikiPeoPle-4	15,188	15,188	4	9,528	50	12,150	1,519	1,519
D.1.3 Evaluation Measurement
Let ranki,n denote the rank of ground truth entity at position n of i-th fact s = (r, e1, . . . , eN),
defined as:
ranki,n = ∣{e0 ∈ E ∖{en} : f (s0; ω) > f (s; ω)}∣ + 1,
where s0 = (r, e1, . . . , en-1, e0, . . . , eN). As in (Bordes et al., 2013; Wang et al., 2014), we report
performance under the “filtered” setting, i.e., evaluating the rank of test fact after removing all
corrupted facts that exist in the train, valid, and test data set. That is because true facts in data set
should be not considered as faults when evaluating a test fact. Correspondingly, ranki,n under filtered
setting is formed as:
ranki,n = |{e0 ∈ E \{en} : f(s0; ω) > f(s; ω) ∩ s0 ∈/ S}| + 1.
Then we adopt the classical metrics (Bordes et al., 2013; Wang et al., 2014):
•	Mean Reciprocal Ranking (MRR): 1/N |S| P|iS=|1 PnN=1 1/ranki,n ;
•	Hit@1,Hit@3, andHit@10, WhereHit@kis given by 1∕∣s∣ P；=1 PN=II(ranki,n ≤k) andI(∙) is
the indicator function.
Note that the higher MRR and Hit@k values mean better embedding quality.
D.2 Case S tudy
The relational patterns on KGs (see Appx. B) have been fully investigated (Kazemi & Poole, 2018;
Rossi et al., 2021) in existing Works. Thus, We first shoW the case studies on tWo benchmark KGs
WN18RR (Dettmers et al., 2018) and FB15k237 (Toutanova & Chen, 2015) (see Tab. 7 for data
statistics and Tab. 8 for experimental results), Which have removed the duplicate and inverse relations
of WN18 and FB15k (Bordes et al., 2013; Dettmers et al., 2018). As presented in Rossi et al. (2021),
the irreflexive, anti-symmetric, and symmetric relations account for a major proportion of these tWo
data sets, especially facts With the symmetric relations reach 37% in WN18RR. We found that the
message function searched on WN18RR (Fig. 5 (a)) is the exact RotatE model (Sun et al., 2019),
Which has been proven to cover symmetric relations. As for the message function searched on
FB15k237 (Fig. 5 (b)), it first feeds the relation embedding into a transformation matrix W. We infer
18
Under review as a conference paper at ICLR 2022
(c) FB15k237.
Figure 5: Several message functions searched by MSeaHKG on different data sets.
⑶ WN18RR.
(b) FB15k237.
Table 8: The model comparison of the link prediction task on KGs. The results of R-GCN and
ComPGCN are copied from Vashishth et al. (2020), and others are copied from Rossi et al. (2021).
type	model		FB15k237					WN18RR			
		MRR	Hit@1	Hit@10		MRR Hit@1		Hit@10
	TransE	0.310	0.217	0.497	0.206	0.028	0.495
Geometric	RotatE	0.336	0.238	0.531	0.475	0.426	0.574
Bilinear	DistMult	0.313	0.224~~	0.490	0.433	0.397^^	0.502
Tensor	TuckER	0.352	0.259	0.536	0.459	0.430	0.514
Π-NTNTo	R-GCN	0.248	0.151	0.417	-	一	一
GNNS	CompGCN	0.355	0.264	0.535	0.479	0.443	0.546
Search	MSeaHKG 一	0.360	0.267	0.545	0.485	0.446	0.554
that is mainly because FB15k237 has 237 relations, which is more than 11 relations on WN18RR. For
the message function searched on facts with high arity, we show an example on JF17K-3 in Fig. 5 (c).
It demonstrates that the message function tends to be deeper and complex in the high arity case. Note
that the full version of message functions searched on WikiPeople (N = 9) and JF17K (N = 6) is
too large to put it on paper. Overall, we can observe that the message functions are data-dependent
from Fig. 5.
D.3 Ablation Study
Except for main experimental results, here we report the performance of several variants ofMSeaHKG
(see Tab. 9) to investigate some key designs in this paper, including MSeaHKGWr, MSeaHKGop,
MSeaHKGst for the search space, MSeaHKGdarts and MSeaHKGrl for the search algorithm.
D.3.1 Search Space
We first present the configuration of variants:
•	MSeaHKGWr basically enables current GNN searching methods working on HKGs. Inspired by
R-GCN (see Eq. 2), we first replace the transform matrix W in mgc(∙) (see Eq. 1) to Wr.. Then, we
concatenate the entity embeddings as h = concat(e1, . . . , en, 0, . . . , 0). Note that the number of
zero embeddings 0 is equal to nmax - n. We utilize the message function mgc(r(e1, . . . , en)) =
Wrh to replace Eq. 7. Other steps are same with original version.
•	MSeaHKGop only searches operations of operators φ and Y in mg(∙; θ) (i.e., θ = {θφk} ∪ {θγk}),
while keeping the structure of StarE’s message functions. Other steps are same with original
version.
•	MSeaHKGst only searches structures of the proposed message function mg(∙; θ), and sets φ, Y
to corr, wsum respectively (i.e., θ = {θikj }). The fixed operations are selected based on better
empirical performance. Other steps are same with original version.
19
Under review as a conference paper at ICLR 2022
Table 9: The comparison of variants of MSeaHKG in the link prediction task on HKGs.
Type	Model		WikiPeople						JF17K				
		MRR	Hit@1	Hit@3	Hit@10	MRR	Hit@1	Hit@3	Hit@10
GNNs	^StarE	■0.378	0.265	0.452	0.542	0.542-	0.454	0.580	0.685
	G-MPNN	0.367	0.258	0.439	0.526	0.530	0.459	0.572	0.688
Search	^S2S	0.372-	0.277	0.439	0.533	0.528-	0.457	0.570	0.690
	MSeaHKG	0.395	0.291	0.470	0.554	0.577	0.481	0.599	0.711
Variants of	MSeaHKGWr	0.354	0.233	0.431	0.520	0.512	0.445	0.553	0.671
	MSeaHKGop	0.385	0.274	0.460	0.548	0.554	0.468	0.591	0.699
space	MSeaHKGst	0.391	0.278	0.465	0.552	0.563	0.475	0.602	0.706
Variants of	MSeaHKGdarts	0.373	0.275	0.445	0.535	0.554	0.460	0.588	0.697
algorithm	MSeaHKGrl	0.380	0.281	0.457	0.542	0.563	0.472	0.593	0.701
From Tab. 9, we observe that the simple extension version MSeaHKGWr even cannot achieve as
good performance as existing GNNs (e.g., StarE and G-MPNN). This verifies the claim that the
simple message function in the existing GNN searching method (e.g., AutoGEL (Zhili et al., 2021)
discussed in Sec. 2.2) may not be able to handle the complex correlations between relations and
entities on HKGs. Moreover, MSeaHKGop keeps the same message function structure with StarE
but searches suitable operations. Differ from manually tuning operations in StarE, the automatic
way is more powerful so that MSeaHKGop achieves a minor improvement compared with StarE.
As for MSeaHKGst , it can search for more flexible structures of message functions for the given
HKG and achieve the best performance among several variants. It can illustrate that the message
function design is important to HKG embedding. However, MSeaHKGst is still slightly inferior
compared with the original version of MSeaHKG. This shows that the best structure and operations
are dependent. Simply fixing operations to search the structure may lead to the sub-optimum.
D.3.2 Search Algorithm
In this paper, we mainly focus on one-shot NAS search algorithms due to their searching efficiency
(see more discussion in Sec. 2.1). Existing one-shot NAS algorithms can be roughly categorized
into: stochastic differentiable method (e.g., SNAS (Xie et al., 2018)), deterministic differentiable
method (e.g., DARTS (Liu et al., 2018)), and policy gradient-based method (e.g., ENAS (Pham
et al., 2018), ASNG (Akimoto et al., 2019)). Here, we implement two more variants of MSeaHKG,
MSeaHKGdarts based on DARTS and MSeaHKGrl based on ASNG, to investigate the performance
of other two kinds of NAS search algorithms in our application scenario.
•	MSeaHKGdarts follows DARTS to directly relax Θ to learnable parameters. Then, the computa-
tion in DAG (see Eq. 6) will be reformed to:
|O|	θk
hk = X αkj ∙ ok(hk-1),αkj = POj-.	(13)
j =1	j0=1 θikj0
Then, We are able to minimize the loss L(Θ, ω; G) through the gradient VθL(∙).
• MSeaHKGrl follows the policy gradient-based NAS search algorithm to derive:
VθEX〜pΘ(X)[L(X, ω; G)] = Vθ /pθ(X)L(X, ω; G)dX
/ L(X, ω; G)Vθ pθ (X )dX
/ L(X, ω; G)Vθ logPθ(X) ∙ PΘ(X)dX	(14)
VθEX〜p&(X) [L(X, ω; G V logPΘ(X)],
where V®pθ (X) = log pθ (X )pθ (X) is the policy gradient trick (Williams, 1992). In practical,
We utilize ASNG (Akimoto et al., 2019) to instantiate Eq. 14, Which implements the fisher
information matrix for fast convergence and adaptive learning rate for robustness.
We here analyze the model comparison between original MSeaHKG with its variants MSeaHKGrl
and MSeaHKGdarts. From Tab. 9 and Tab. 10, we observe that MSeaHKG and MSeaHKGrl have
20
Under review as a conference paper at ICLR 2022
Table 10: The training time comparison (in hours) of GNN-based models in the link prediction task
on HKGs.
Type	Model	HKGS		Arity=3		Arity=4	
		WikiPeople	JF17K	W-3	J-3	W-4	J-4
GNNs	G-MPNN	15.4 ± 2.6	4.1 ± 0.3	1.2 ± 0.1	0.6 ± 0.1	0.5 ± 0.1	0.6 ± 0.1
	StarE	137.5 ± 6.3	16.7 ± 2.2	2.6 ± 0.4	4.7 ± 0.8	2.3 ± 0.4	1.9 ± 0.2
Search	MSeaHKG	30.9 ± 4.7	7.7 ± 1.8	1.8 ± 0.4	1.4 ± 0.2	0.9 ± 0.1	1.1 ± 0.2
Variants of	MSeaHKGdarts	40.2 ± 2.5	11.5 ± 2.2	2.5 ± 0.5	2.1 ± 0.3	1.5 ± 0.2	1.3 ± 0.1
algorithm	MSeaHKGrl	35.1 ± 3.0	10.3 ± 1.7	2.3 ± 0.3	1.5 ± 0.2	1.4 ± 0.2	1.1 ± 0.1
better performance than MSeaHKGdarts in both effectiveness and efficiency comparisons. First,
DARTS aims to train a supernet by mixing all candidate operations during the searching phase, then
it will derive a discrete architecture after finishing the search. But the weights αik in Eq. 13 cannot
cannot converge to a one-hot vector, which lead to performance collapse after removing |O - 1|
operations in O (Zela et al., 2019; Chu et al., 2020). Second, it will consume more computational
resources when maintaining all operations during the search. Instead, MSeaHKG and MSeaHKGrl
are to train discrete architectures in searching (the bounded discreteness of MSeaHKG is discussed
in Appx. C.4), which avoids performance collapse and large computational overhead. As for the
comparison between MSeaHKG with MSeaHKGrl , MSeaHKG achieves slight improvements. That
is mainly because MSeaHKG directly calculates the gradient w.r.t. Θ from the loss L(∙), while
MSeaHKGrl takes the loss L(∙) as a reward to feed ittoa RL controller.
From Tab. 10, we also can observe the efficiency comparison between GNN-based models (G-MPNN,
StarE, and MSeaHKG). Like other NAS methods, MSeaHKG requires two training phases, searching
architecture and training the searched architecture from the scratch. Thus, it needs more running time
compared with G-MPNN. Moreover, both G-MPNN and MSeaHKG utilizes the simple decoders
(i.e., scoring function discussed in Appx. C.2), while StarE adopts the complex transformer as its
decoder. Thus, StarE is less efficient.
D.4 MSeaHKG can be transferred to other graph-based tasks.
Since many configurations of MSeaHKG are inspired by GNNs and GNN searching methods, we
transfer MSeaHKG to other GNN-related tasks to further investigate its capability.
First, we conduct an extensive experiment on social recommendation (Fan et al., 2019), where
the recommendation data sets are formed as multi-relational graphs. In the data set Ciao 2, nodes
represent the users and items, edges have two main types: 1) 5 level of ratings {1, 2, 3, 4, 5} between
users and items, 2) connections among users. The goal of the task is to predict the unknown
ratings of items given by users. After treating the 5 ratings as 5 edge types, the recommendation
task is converted to the relation prediction task, i.e., predict the rating r given ?(user, item). To
compare MSeaHKG with literature more conveniently, we follow Fan et al. (2019) to utilize the
Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as the evaluation metrics, and
60% as training data. We let MSeaHKG compute top-3 scores like a = score(2(user, item)), b =
score(4(user, item)), c = score(5(user, item)), then adopt weighted sum to output the final rating
like (a ∙ 2 + b ∙ 4 + C ∙ 5)/(a + b + c). The experimental report in Tab. 11a shows MSeaHKG has a
good generalized ability to the social recommendation task, which is consistent with the performance
on KGs (i.e., multi-relational graphs).
Second, we extend MSeaHKG to graph-level tasks. We incorporate one more essential component at
the final layer OfMPNNs, i.e., readout funcion rd(∙). General MPNNS employ rd(∙) to output the
representation of a whole graph G(E, R, S) by aggregating the node embeddings as:
hG = rd({ei}ei∈E).
Due to the modeling of edge representations, MSeaHKG slightly adjusts the readout function as:
hG = rd({[e1, r, e2]}r(e1,e2)∈S).
2https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm
21
Under review as a conference paper at ICLR 2022
Table 11: The model performance of extending MSeaHKG to other tasks.
(b) The comparison on the graph classification task. The results of
(a) The comparison on the rating pre- diction task of social recommendation. The results of baselines are copied from Fan et al. (2019).			baselines on PROTEINS and IMDB-M are copied from PAS (Wei et al., 2021), and those on MUTAG are copied from GIN (Xu et al., 2018).				
			type	model	PROTEINS	IMDB-M	MUTAG
model	Ciao			GCN	-07484^^	0.5040	0.8560
	MAE	RMSE	GNNs	GraphSAGE	0.7375	0.4853	0.8510
PMF	0.9520	1.1967		GIN	0.7620	0.5230	0.8940
TrustMF	0.7681	1.0543		GraPhNAS	-07520^^	0.4827	-
NeuMF	0.8251	1.0824	NAS	SNAG	0.7233	0.5000	-
GraphRec	0.7540	1.0093	for	You et al. (2020)	0.7390	0.4780	-
MSeaHKG	0.7511	1.0021	GNNs	PAS	0.7664	0.5220	-
				MSeaHKG	0.7724	0.5317	0.8922
We implement the choices of readout function as {global「mean, global_max, global_sum}. We
report the accuracy in Tab. 11b. Among 3 data sets, the graphs in MUTAG are multi-relational, thus
most of GNN searching methods do not include it in empirical study. We can observe that MSeaHKG
achieves not bad performance on these data sets.
22