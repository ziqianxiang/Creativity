Under review as a conference paper at ICLR 2022
Optimized Separable Convolution:
Yet Another Efficient Convolution Operator
Anonymous authors
Paper under double-blind review
Ab stract
The convolution operation is the most critical component in recent surge of deep
learning research. Conventional 2D convolution needs O(C2K2) parameters to
represent, where C is the channel size and K is the kernel size. The amount of
parameters has become really costly considering that these parameters increased
tremendously recently to meet the needs of demanding applications. Among var-
ious implementations of the convolution, separable convolution has been proven
to be more efficient in reducing the model size. For example, depth separable
convolution reduces the complexity to O(C ∙ (C + K2)) while spatial separable
convolution reduces the complexity to O(C2K). However, these are considered
ad hoc designs which cannot ensure that they can in general achieve optimal sep-
aration. In this research, we propose a novel and principled operator called opti-
mized separable convolution by optimal design for the internal number of groups
and kernel sizes for general separable convolutions can achieve the complexity of
O(C2 K). When the restriction in the number of separated convolutions can be
lifted, an even lower complexity at O(C ∙ log(CK2)) can be achieved. Experi-
mental results demonstrate that the proposed optimized separable convolution is
able to achieve an improved performance in terms of accuracy-#Params trade-offs
over both conventional, depth-wise, and depth/spatial separable convolutions.
1	Introduction
Tremendous progresses have been made in recent years towards more accurate image analysis tasks,
such as image classification, with deep convolutional neural networks (DCNNs) (Krizhevsky et al.,
2012; Srivastava et al., 2015; He et al., 2016; Real et al., 2019; Tan & Le, 2019; Dai et al., 2020).
However, the complexity of state-of-the-art DCNN models has also become increasingly high. This
can significantly deter their deployment to real-world applications, such as mobile platforms and
robotics, where the resources and networks are highly constrained (Howard et al., 2017; Dai et al.,
2020).
The most resource-consuming building block of a DCNN is the convolutional layer. There have
been many previous works aiming at reducing the amount of parameters in the convolutional layer.
Network pruning (Han et al., 2015) strategies are developed to reduce redundant parameters that are
not sensitive to performances. Quantization and binarization (Gong et al., 2014; Courbariaux et al.,
2016) techniques are introduced to compress the original network by reducing the number of bits re-
quired to represent each parameter. Low-rank factorization methods (Jaderberg et al., 2014; Ioannou
et al., 2015) are designed to approximate the original weights using matrix decomposition. Knowl-
edge distillation (Hinton et al., 2015) is applied to train a compact network with distilled knowledge
from a large ensemble model. However, all these existing methods start from a pre-trained model.
Besides, they mainly focus on network compression and have limited or no improvements in terms
of network acceleration.
In this research, we study how to design a separable convolution to achieve an optimal implemen-
tation in terms of model size (representational complexity). Enabling convolution to be separable
has been proven to be an efficient way to reduce the representational complexity (Sifre & Mallat,
2014; Howard et al., 2017; Szegedy et al., 2016). Comparing to the network compression related
approaches, a well-designed separable convolution shall be more efficient in both storage and com-
putation and shall not require a pre-trained model to begin with.
1
Under review as a conference paper at ICLR 2022
Table 1. A comparison of the number of parameters and computational complexity of the proposed optimized
separable convolution and existing approaches. The proposed optimized separable convolution is much more
efficient in both #Params and FLOPs. In this table, C represents the channel size of convolution, K is the
kernel size, H and W are the output height and width, g is the number of groups. “Vol. RF” represents whether
the corresponding convolution satisfies the proposed volumetric receptive field condition.
	Conventional Conv2D	Grouped Conv2D	Depth-wise Conv2D	Point-wise Conv2D	Depth Separable Conv2D	Spatial Separable Conv2D	Optimized Separable Conv2D(N = 2)	Optimized Separable Conv2D (Optimized N)
#ParamS	C2K2	C2K2∕g	CK2	C2	C(C + K2)	2C2K	2C 2 K	eC log(CK2)
FLOPS	C2K2HW	C2K2HW∕g	CK2HW	C2 HW	CHW (C + K2)	2C2KHW	3 2C 2 KHW	eCHW log(CK2)
Vol. RF	✓	X	X	X	✓	✓	✓	✓
Note	-	-	g = C	K = 1	Depth-wise + Point-wise	K2 → 2K	-	e = 2.71828 …
3x3	1x1
O(C2K2)
Conv2D
3x3
O
O
O
O
0
O
O
0
O(C(C + K2))
Depth Separable
3x3	1x1
oʌo⅛4o
o5^o‰
3
O(C5 K)
Optimal Separable
(b)
Figure 1. Volumetric receptive field and the proposed optimized separable convolution. (a) The volumetric
receptive field (RF) of a convolution is the Cartesian product of its (spatial) RF and channel RF. (b) Illustrations
of the channel connections for conventional, depth separable, and the proposed optimized separable convolu-
tions. Optimized separable convolution is sparse-connected, whereas it can be efficiently implemented using a
channel shuffle operation.
In the DCNN research, the two most well-known separable convolutions are depth separable (Sifre
& Mallat, 2014) and spatial separable (Szegedy et al., 2016) convolutions. Both are able to reduce
the complexity of a convolution. The representational complexity of a conventional 2D convolu-
tion is quadratic with two hyper-parameters: number of channels (C) and kernel size (K), and its
representational complexity is actually O(C2K2). Depth separable convolution is constructed as
a depth-wise convolution followed by a point-wise convolution, where depth-wise convolution is a
group convolution with its number of groups g = C and point-wise convolution is a 1 × 1 convo-
lution. Spatial separable convolution replaces a K × K kernel with a K × 1 and a 1 × K kernel.
Different types of convolutions and their complexities are summarized in Table 1. From this table,
we can see that, for all convolutions, their computational complexities equal to the corresponding
representational complexity times a constant. We can also verify that depth separable convolution
has a complexity of O(C ∙(C+K 2)) and spatial separable convolution has a complexity of O(C 2K).
Both depth and spatial separable convolutions follow an ad hoc design mode and are non-principled.
They are able to reduce the complexity to some degree but normally cannot achieve an optimal sepa-
ration. A separable convolution in general has three sets of hyperparameters: the internal number of
groups, channel size, and kernel size of each separated convolution. Instead of setting these hyper-
parameters in an ad hoc (manual) fashion, we design a novel and principled (auto) scheme to achieve
an optimal separation. The resulting separable convolution is called optimized separable convolu-
tion in this research. The proposed scheme in general performs better than the other convolution
operator counterparts and it also enriches the separable convolution family.
To prevent the proposed optimized separable convolution from being degenerated, we assume that
the internal channel size is in an order ofO(C) and propose the following volumetric receptive field
condition. As illustrated in Fig. 1a, similar to the receptive field (RF) of a convolution which is
defined as the region in the input space that a particular CNN’s feature is looking at (or affected by)
(Lindeberg, 2013), we define the volumetric RF ofa convolution to be the volume in the input space
that affects CNN’s output. The volumetric RF condition requires a properly decomposed separable
convolution to maintain the same volumetric RF as the original convolution before decomposition.
Hence, the proposed optimized separable convolution will be equivalent to optimizing the internal
number of groups and kernel sizes to achieve the target objective (measured in #Params) while
2
Under review as a conference paper at ICLR 2022
satisfying the proposed volumetric RF condition. Formally, the objective function is defined by
Equation (2) under the constraints defined by Equations (3)-(6). The solution to this optimization
problem will be elaborated in Section 2.
We shall show that the proposed optimized separable convolution can be represented with the order
of O(C2 K). This is at least a factor of √C more efficient than the depth and spatial separable
convolutions. The proposed optimized separable convolution is able to be generalized into an N-
separable case, where the number of separated convolutions N can be optimized further. In such a
generalized case, an even lower complexity at O(C ∙ log(CK2)) may be achieved.
Extensive experiments have been carried out to demonstrate the effectiveness of the proposed opti-
mized separable convolution over other alternatives, including conventional, depth-wise, depth and
spatial separable convolutions (Fig. 3(c) and Fig. 4(c)). As further illustrated in Fig. 3 and Fig.
4, on the CIFAR10 and CIFAR100 datasets (Krizhevsky et al., 2009), the proposed optimized sep-
arable convolution achieves a better Pareto-frontier1 than both conventional and depth separable
convolutions using the ResNet (He et al., 2016) architecture. To demonstrate that the proposed op-
timized separable convolution generalizes well to other DCNN architectures, we adopt the DARTS
(Liu et al., 2018) architecture by replacing the depth separable convolution with the proposed op-
timized separable convolution. The accuracy is improved from 97.24% to 97.67% with reduced
representational complexity. On the ImageNet dataset (Deng et al., 2009), the proposed optimized
separable convolution also achieves improved performance. For the DARTS architecture, the pro-
posed approach achieves 74.2% top1 accuracy with only 4.5 million parameters. For MobileNet, the
proposed approach achieves 71.1% top1 accuracy with only 3.0 million parameters.
2	The Proposed Approach
2.1	Convolution and its Complexity
A convolutional layer takes an input tensor Bl-1 of shape (Cl-1 , Hl-1, Wl-1) and produces an
output tensor Bl of shape (C1,H1, Wι), where C*, H*, W* are input and output channels, feature
heights and widths. The convolutional layer is parameterized with a convolutional kernel of shape
(Cl, Cl-1, KlH, KlW ), where Kl* are the kernel sizes, and the superscript indicates whether it is
aligned with the features in height or width. In this research, we take C* = O(C), H* = O(H),
W* = O(W), and K*H|W = O(K) for complexity analysis. Formally, we have
Bl(cl, hl, wl) =	Bl-1(cl-1, hl-1, wl-1)∙ Fι(cι,cι-ι,kH,kW),	(1)
cl-1 klH klW
where hι = hι-1 + kιH and wι = wι-1 + kιW. Hence, the number of parameters for convolution is
CιCι-1KιHKιW and its representational complexity is O(C2K2). The number of FLOPs (multiply-
adds) for convolution is Cι HιWι ∙Cι-ι KH KW and its computational complexity is O(C2 K2 HW).
For a group convolution, We have g convolutions with kernels of shape (Cι∕g, Cι-ι∕g, KH, KW).
Hence, it has O(C2K2/g) parameters and O(C2K2HW/g) FLOPs, where g is the number of
groups. A depth-wise convolution is equivalent to a group convolution with g = C* = C. A point-
wise convolution is a 1 × 1 convolution. A depth separable convolution is composed ofa depth-wise
convolution and a point-wise convolution. A spatial separable convolution replaces a K × K kernel
with K × 1 and 1 × K kernels. Different types of convolutions are summarized in Table 1. From
this table, their number of parameters and FLOPs can be easily verified. It can also be seen that, for
a convolution, its representational complexity is equivalent to its computational complexity for up
to a constant (HW).
2.2	Rethinking Convolution and the Volumetric Receptive Field Condition
Separable convolution has been proven to be efficient in reducing the representational demand in
convolution. However, existing approaches including both depth and spatial separable convolutions
follow an ad hoc design and are non-principled. They are able to reduce the complexity to some
extent but will not normally achieve an optimal separation. In this research, we shall design an
1In multi-objective optimization, a Pareto-frontier is the set of parameterizations (allocations) that are all
Pareto-optimal. An allocation is Pareto-optimal if there is no alternative allocation where improvement can be
made to one participant’s well-being without sacrificing any other’s. Here, Pareto-frontier represents the curve
of the accuracies we are able to achieve for different #Params (or FLOPs).
3
Under review as a conference paper at ICLR 2022
efficient convolution operator capable of achieving the representational objective by optimal design
of its internal hyper-parameters. The resulting operator is called optimized separable convolution.
The proposed optimized separable convolution is called principled as it optimizes the representa-
tional complexity under the following volumetric receptive field condition. As illustrated in Fig. 1a,
the receptive field (RF) of a convolution is defined to be the region in the input space that a particular
CNN’s feature is affected by (Lindeberg, 2013). We define the channel RF to be the channels that
affect CNN’s output and define the volumetric RF to be the Cartesian product of the RF and channel
RF of this convolution. The volumetric RF of a convolution actually represents the volume in the
input space that affects CNN’s output. The volumetric RF condition requires that a properly decom-
posed separable convolution at least maintains the same volumetric RF as the original convolution
before decomposition. Hence, the proposed optimized separable convolution will be equivalent to
optimizing its internal parameters while satisfying the volumetric RF condition. Formally, we shall
have the objective function defined by Equation (2) and the volumetric RF constraints defined by
Equations (3)-(6).
The volumetric RF of a convolution needs to be maintained for technical, conceptual, and experi-
mental reasons. Technically, if we do not pose any restriction to a separable convolution, optimizing
the representational complexity will resulting in a separable convolution being equivalent to a de-
generated channel scaling operator2. The composition of such operators is not meaningful because
the composition itself is equivalent to a single channel scaling operator. Conceptually, maintain-
ing the volumetric RF encourages the fusion of channel information, which shall contribute to the
good performance of a DCNN. In fact, all modern DCNNs are designed following this rule. With-
out this channel information exchange, the performance of a DCNN shall be significantly degraded
(depth-wise vs depth separable convolutions in Section 3). Finally, the necessity of maintaining
the volumetric RF is experimentally verified. We shall quantize the degree of necessity as overlap
coefficient (γ) in Section 2.3 and elaborate the experimental results in Section 3.
2.3	Optimized Separable Convolution
In this section, for ease of simplicity, we first discuss the case of two-separable convolution. Suppose
that the shape of the original convolutional kernel is (Cout, Cin, KH, KW ), where Cin, Cout are
the input and output channels, and (KH, KW) is the kernel size. Let C1 = Cin, and C3 = Cout.
For the proposed optimized separable convolution, we optimize the representational complexity as
objective while maintaining the original convolution’s volumetric RF. Formally, the representational
demand of the proposed separable convolution is
f(g1,g2 C," ) = CCKHKW + CCKHKW
g1	g2
(2)
In order to satisfy the volumetric RF condition, the following three conditions need to be satisfied:
K1H + K2H - 1 = KH	(Receptive F ield C ondition)	(3)
K1W + K2W - 1 = K W	(4)
C1 C2
gi ∙ g2 ≤ C2∕γ ⇔ ——∙——≥ γC1	(Channel Condition)	(5)
g1 g2
min(Cl, Cl+1) ≥ gl	(Group Convolution C ondition)	(6)
The channel condition (5) means the product C ∙ C needs to occupy each node in the input channel
C1 = Cin to maintain the volumetric receptive field. This is further explained for the channel
condition general case (15) in Section 2.4. In order to study the necessity of the proposed volumetric
RF condition, an overlap coefficient γ is introduced to encourage channel information fusion. It can
be verified that, if γ ≥ 1, the channel RF shall be maintained, otherwise, it shall be not. By default,
we set γ = 1 in this research unless the behavior of γ is particularly concerned.
We have three sets of parameters: the number of groups g1, g2, the internal channel size C2, and the
internal kernel sizes KH1W. In this research, We assume that the internal channel size C2 is in an
order of O(C) and is preset according to a given policy. Otherwise, g1 = g2 = C2 = 1 will be a
trivial solution. This could lead the separable convolution to be over-simplified and not applicable
2From Table 1, let g = C and K = 1, a convolution Will have C parameters and CHW FLOPs. This is in
fact a channel scaling operator.
4
Under review as a conference paper at ICLR 2022
in practice. Typical policies of presetting C2 include C2 = min(C1, C3) (normal architecture),
C2 = (C1 + C3)/2 (linear architecture), C2 = max(C1, C3)/4 (bottleneck architecture (He et al.,
2016)), or C2 = 4 min(C1, C3) (inverted residual architecture (Sandler et al., 2018)).
The solution to the proposed optimized separable prob-
lem shall be given in Theorem 1 in Section 2.4. By setting
N = 2 and Y = 1,we shall have
gι=SCCiHKWW ~ √C, g2=C2/gi	⑺
V C3K2 K2
(KH,KH) = (Kh, 1) or (1,KH)	(8)
(KW,KW) = (Kw,1) or (1,KW)	⑼
and	_________________
min f = 2 ∙ JCIC2C3KH KW KH KW =O(C 3 K).
(10)
FLOPS
3600
3400
3200
3000
2B00
n'产
Figure 2. Given channels C1 = C2 =
C3 = 64, and kernel sizes KH = KW = 5
in Equation (2), by setting f0(g1) = 0,
f0(K1) = 0. The solution g1 = 8, K1 = 3
is a saddle point.
One interesting fact is that if we set g2 = C2/g1,
f0(g1) = 0, and f0(K1) = 0, assume that kernel sizes
aligned in height and width are equal, one can derive that
gι is the same as Equation (7) and Ki = K2 = K2+1.
Substituting them into Equation (10), one can get f (gi, Ki)=O(C3K2). This results in a higher
3
complexity than O(C2K). In fact, the solution to f0(g1) = 0 and f0(K1) = 0 is a saddle point,
which is illustrated in Fig. 2.
2.4	Optimized Separable Convolution (General Case)
In this section, we shall generalize the proposed optimized separable convolution from N = 2 to an
optimal N. For ease of analysis, we first introduce the notation channels per group n = C, which
simply means: channels per group × number of groups = the number of channels.
Theorem 1 (Optimized Separable Convolution: General Case). Suppose that the shape of the orig-
inal convolutional kernel is (Cout, Cin, KH, KW). Let Ci = Cin, and CN+i = Cout. The repre-
sentational demand of an N -separable convolution is
f({g*},{KHlW }) = X Cl+1ClKlH KlW	(11)
gl
l=i	gl
or
N
f({n*},{KHIW })= X Cl+ιnlKH KlW	(12)
l=i
Under the proposed volumetric RF condition, we will have:
KH + KH + …=KH + (N - 1)	(Receptive Field Condition)	(13)
KW + KW + …=KW + (N - 1)
/ C2…CN
ni …nN ≥ YCi ⇔ gi …gN ≤ -
γ
(14)
(C hannel C ondition) (15)
nl ≥ max(1, ɑl+1) ⇔ gl ≤ min(Cl, Cl+i).	(Group Convolution Condition)
Cl
(16)
Assume that C* = Ο(C) and KHIW = O(K). The solution to this constrained optimization
problem (the proposed optimized separable convolution problem) is given by
nl
<YnN+iCinN=IKH ∏N=iKW
------------TT-TTT-----〜
Cl+iKH KW
(17)
KlH0 = KH, KlH = 1 (l 6= l0)	(18)
KlW = KW, KlW = 1 (l 6= li)	(19)
and its corresponding representational complexity is
min f ({n*}, {KH|W })=O(NY N Ci+N K N).	(20)
5
Under review as a conference paper at ICLR 2022
Cenventlonal (baseline)
Depthwlse (dw-)
Depth SepaEbie W-)
Spatial Separable (j-)
Optimal Sepanible (ə-)
Spatial Optimal Separable (κ>-)
M川
ReSNet20
ReSNet32
ReSNet56
ResNetiio
(c)
Figure 3. Experimental results on CIFAR10 for the ResNet architecture (best viewed in color). The proposed
optimized separable convolution (o-ResNet) achieves improved (a) accuracy-#ParamS and (b) accuracy-FLOPs
Pareto-frontiers than both the conventional (ResNet) and depth separable (d-ResNet) convolutions. (c) A com-
parison for performances of different convolution schemes.
Table 2. Experimental results on CIFAR10 for different overlap coefficients (γ). If Y ≥ 1, the volumetric RF
is maintained, otherwise it is not. Each row of o-ResNet is channel multiplied. When γ < 1, the performance
hurts due to discourage of channel information fusion.
Overlap Coef. (γ)	ε (Depthwise)	1/16	1/4	1	4	16	64	+ ∞ (Conventional)
Vol. RF	X	X	X	✓	✓	✓	✓	✓
o-ResNet20	90.2	91.45	92.56	93.37	92.84	93.06	92.16	91.25
o-ResNet32	89.82	92.31	92.8	93.69	93.65	93.03	93.07	92.49
o-ResNet56	89.88	92.71	92.88	93.81	93.8	93.49	92.73	93.03
o-ResNet110	90.26	94.04	94.85	94.88	94.83	94.41	93.95	93.39
Furthermore, if the number of separated convolutions N can be optimized, we will have
N = log(γCK2)	(21)
and
minf({n*},{KHlW}) = O(C ∙ log(γCK2)).	(22)
In Theorem 1, We keep both notations gι and n(. This is because, for the channel condition, it is
intuitive to see nι …nN ≥ Ci means that the product of nι …nN needs to occupy each node in
the input channel Ci = Cin. This is equivalent to the less intuitive condition gi ∙∙∙ gN ≤ C2 •一CN.
Similarly, for the group convolution condition, gi ≤ min(Cι, C1+1) means the number of groups can
not exceed the input and output channels of this group convolution, while n ≥ max(1, C+1) is less
intuitive. A sketch of proof of Theorem 1 is given in Appendix A.
Equations (18) and (19) mean that one of the internal kernel sizes should take KH or KW and
the remaining ones take 1. Hence, the proposed optimized separable convolution shall have a spatial
separable configuration: a single kernel takes (KH, KW) or two kernels take (KH, 1) and (1, KW).
The implementation details of the proposed optimized separable convolution scheme is described in
Algorithm 1 (Appendix B). Finally, the proposed optimized separable convolution is sparse con-
nected. The hyperparameters of each separated convolution are given by Equations (17)-(19) and
the proposed scheme can be efficiently implemented using a channel shuffle operation (Fig. 1b).
3	Experimental Results
In this section, We carry out extensive experiments on benchmark datasets to demonstrate the effec-
tiveness of the proposed optimized separable convolution scheme. In the proposed experiments, we
use a prefix dw-, d-, s-, o- or So- to indicate that the conventional or depth separable convolutions
in the baseline networks are replaced with depth-wise, depth separable (dsep), spatial separable,
the proposed optimized separable (osep), or the proposed spatial optimized separable convolutions.
In this research, we set the number of separated convolutions N = 2. The details of the training
settings for the proposed experiments are described in Appendix C.
3.1	Experimental Results on CIFAR10
CIFAR10 (Krizhevsky et al., 2009) is a dataset consist of 50,000 training images and 10,000 testing
images. These images are with a resolution of 32 X 32 and are categorized into 10 object classes. In
6
Under review as a conference paper at ICLR 2022
the proposed experiments, we use ResNet (He et al., 2016) as baselines and replace the conventional
convolutions in ResNet with dsep and osep convolutions, resulting in d-ResNet and o-ResNet.
The proposed osep scheme can
significantly reduce the representa-
tional complexity. In Section 2, we
state that this reduction factor can
be √CK in theory3. As illustrated
by the solid lines in Fig. 3(a), the
orange solid curve lies in a region
with significantly smaller x-values
than the blue solid curve. This indi-
cates that o-ResNet shall have sig-
nificantly less parameters than the
ResNet baseline. For example, the
110-layered o-ResNet110 has even
fewer parameters (0.180 million vs
Table 3. Experimental results on CIFAR10 for DARTS. The pro-
posed optimized separable convolution (o-DARTS) generalizes well
to the DARTS architecture, and achieves improved accuracy with ap-
proximately the same FLOPs and fewer parameters. DARTS uses
depth separable convolution and an optional d- is prefixed.
Net Arch	#Params	FLOPs	Accuracy	Error Rate
	(million)	(billion)	(%)	(%)
(d-)DARTS (Liu et al., 2018)	3.35	0.528	97.24%	2.76%
o-DARTS	3.25	0.572	97.67%	2.33%
P-DARTS (Chen et al., 2019)	3.43	0.532	97.50%	2.50%
PC-DARTS (Xu et al., 2019)	3.63	0.557	97.43%	2.57%
GOLD-DARTS (Bi et al., 2020)	3.67	0.546	97.47%	2.53%
0.270 million) than the 20-layered ResNet20, yet with noticeable higher accuracy (92.15% vs
91.25%). This demonstrates that the proposed osep scheme could significantly reduce the repre-
sentational complexity for convolutions. For dsep, this reduction factor is [/小；]/。, which is
bounded by K2. For 3 × 3 kernels, this reduction can be at most 9. Whereas for the proposed osep
scheme, no such bounds exist. The advantage of the proposed osep scheme over dsep is illustrated
in Fig. 3(a) by the orange and green solid curves. From this, we can see the proposed osep scheme
is more efficient with smaller x-values. We further plot accuracy-FLOPs curves in Fig. 3(b) for
reference, where similar conclusions can be drawn.
The proposed o-ResNets can have 10x-18x fewer parameters than the ResNet baselines in the pro-
posed experiments. For fair comparisons, we introduce the channel multiplier in order to approxi-
mately match the #ParamS (or FLOPS)4. We use the suffix "_m<multiplier>” to indicate the
channel multiplier. As illustrated in Fig. 3(a), from which we can see, the proposed osep scheme
is much more efficient than conventional convolutions. The orange curve, including both solid and
dashed parts, achieved a better accuracy-#Params Pareto-frontier than the blue curve. Such repre-
sentation efficiency could result in a more regularized network with fewer parameters to prevent
over-fitting and possibly contribute to the final performance. In Fig. 3(a), we also present the d-
ResNet curves in dashed green by replacing the conventional convolutions with dsep convolutions.
As can be seen, d-ResNet achieves good accuracy-#Params balances for small networks (e.g. d-
ResNet20 and d-ResNet32), but performs comparable or no better than conventional convolutions
for large ones (e.g. d-ResNet56 and d-ResNet110). In summary, the proposed osep scheme achieves
better accuracy-#Params (and also accuracy-FLOPs as illustrated in Fig. 3(b)) Pareto-frontiers than
both conventional and dsep convolutions.
Other Conv2D Types: Besides conventional and depth separable (d-) convolutions, we compare
the proposed osep (o-) scheme against the other convolution types, including depth-wise (dw-) and
spatial separable (s-) convolutions. In the following, we shall omit the suffix of channel multiplier
for simplicity, which shall be clear from the context. From Algorithm 1 (Appendix B), the proposed
osep scheme also has a spatial separable (so-) variant. A comparison of all these convolutions for
the ResNet architecture is illustrated Fig. 3(c). From this figure, we can conclude that the proposed
osep scheme is more efficient than all other alternatives (the orange bar is highest).
Channel Information Fusion: We discuss more about dw-ResNet in Fig. 3(c). Recall from Table 1
that a depth separable convolution is a depth-wise convolution followed by a pointwise convolution.
dw-ResNet allows no channel information exchange while d-ResNet does. Fig. 3(c) demonstrates
that dw-ResNet performs much worse than d-ResNet. In fact, dw-ResNet is the only one that does
not maintain the volumetric RF and performs worst of all these six convolution schemes. This
3For optimized separable, √CK = C；K；. For depth separable, 1∕κ2+1∕c = CCK22)< K2∙
4We match both #Params and FLOPs here. If this is not allowed, we approximately match for one and make
sure the other not to exceed. In Appendix D, we present experimental results of matching #Params only in Fig.
5 and Fig. 6. The conclusions we reached in this Section is the same.
7
Under review as a conference paper at ICLR 2022
ReSNet20 ResNet32	ReSNet56	ResNetiiO
Conventional (baseline)	Spatial Separable (s-)
Depthwlse (dw-)	Optimal Sepeebie (σ-)
Depth Separable W")	Spatial Optimal Separable (m-)
(c)
Figure 4. Experimental results on CIFAR100 for the ResNet architecture (best viewed in color). The proposed
optimized separable convolution (o-ResNet) achieves improved (a) accuracy-#ParamS and (b) accuracy-FLOPs
Pareto-frontiers than both the conventional (ResNet) and depth separable (d-ResNet) convolutions. (c) A com-
parison for performances of different convolution schemes.
Table 4. Experimental results on CIFAR100 for different overlap coefficients (γ). If Y ≥ 1, the volumetric RF
is maintained, otherwise it is not. Each row of o-ResNet is channel multiplied. When γ < 1, the performance
hurts due to discourage of channel information fusion.
Overlap Coef. (γ)	ε (Depthwise)	1/16	1/4	1	4	16	64	+ ∞ (Conventional)
Vol. RF	X	X	X	✓	✓	✓	✓	✓
o-ResNet20	65.46	68.24	70.96	71.03	71.12	70.8	70.08	67.38
o-ResNet32	66.42	70.59	71.05	72.75	70.89	71.91	70.76	68.21
o-ResNet56	65.39	69.23	69.87	73.55	72.4	71.98	70.98	69.34
o-ResNet110	65.98	73.30	74.00	75.48	74.74	73.62	73.01	71.68
suggests that channel information fusion could be critical for the good performance of a DCNN, and
hence validates our proposed volumetric receptive field condition.
Overlap Coefficient: We carry out an ablation study on the overlap coefficient γ. For Y ≥ 1, the
volumetric RF is maintained, otherwise, it is not. From Table 2, We can see that, a good-performing
γ takes values 1 ≤ Y ≤ 4 and Y = 1 achieves the best. It is reasonable to conjecture that, for γ < 1,
the volumetric RF is not maintained and the channel information fusion is compromised, leading to
bad performance. For γ > 4, the representation efficiency is also slightly lower. We argue that this
is because the channel information has already been fused sufficiently. For larger γ, more overlap
introduces more cost yet no additional fusion, hence the efficiency has been degraded accordingly.
In Table 2, We also include the results for dw-ResNet and ResNet, as they can be roughly viewed
as the limit cases of Y to be infinitely small (ε) or infinitely large (+∞). The ablation study on the
overlap coefficient in Table 2 clearly demonstrates that we should satisfy the proposed volumetric
RF condition.
Generalization to DARTS: To demonstrate that the proposed osep scheme generalizes well to
other DCNN architectures, we adopt the DARTS (V2) (Liu et al., 2018) network as the baseline.
The DARTS evaluation network has 20 cells and 36 initial channels, we increase the initial channels
to 42 to match the FLOPs. By replacing the dsep convolutions in DARTS with the proposed osep
convolutions, as illustrated in Table 3, the resulting o-DARTS improved the accuracy from 97.24%
to 97.67%, but with fewer parameters (3.25 million vs 3.35 million). It is worth noting that it is very
hard to significantly improve the DARTS search space. In Table 3, we also include three variants of
DARTS, i.e. P-DARTS (Chen et al., 2019), PC-DARTS (Xu et al., 2019), and GOLD-DARTS (Bi
et al., 2020), with more advanced search strategies for comparison. As can be seen, o-DARTS can
achieve even higher accuracies than these advanced network architectures.
3.2	Experimental Results on CIFAR 1 00
CIFAR100 (Krizhevsky et al., 2009) is a dataset consist of 50,000 training images and 10,000 testing
images. These images are with a resolution of 32×32 and are categorized into 100 object classes. We
carry out similar experiments on CIFAR100 as those on CIFAR10, from which similar conclusions
can be drawn.
From Fig. 4(a) and (b), we can conclude that the proposed optimized separable convolution scheme
achieves better accuracy-#Params and accuracy-FLOPs Pareto-frontiers than both conventional and
depth separable convolutions. From Fig. 4(c), we can see that the proposed osep scheme is more
8
Under review as a conference paper at ICLR 2022
efficient than the other alternative Conv2D types, including depth-wise, spatial separable, and the
proposed spatial optimized separable convolutions. In Fig. 4(c), dw-ResNet is the only one that
does not maintain the volumetric RF and performs significantly worse than the other counterparts.
In Table 4, the experimental results indicate the best overlap coefficient takes value γ = 1. These
latter two observations also demonstrate the validity of the proposed volumetric RF condition.
3.3	Experimental Results on ImageNet
We evaluate the proposed optimized Table 5. Experimental results on full ImageNet for the DARTS ar-
separable convolution scheme on the chitecture. The proposed o-DARTS achieves 74.2% top1 accuracy
benchmark ImageNet (Deng et al., with only 4.5 million parameters and the proposed o-MobileNet
2009) dataset, which contains 1.28 achieves 70.8% top1 accuracy with only 3.0 million parameters.
million training images and 50,000 testing images.	Net Arch	#Params (million)	FLOPs (billion)	Top1 (%)	Top1 Error (%)
	(d-)DARTS (Liu et al., 2018)	4.72	0.530	73.3%	26.7%
3.3.1 ImageNet40	o-DARTS	4.50	0.554	74.2%	25.8%
Because carrying out experiments di-	(d-)MobileNet (Howard, 2017)	4.20	0.575	70.6%	29.4%
rectly on the ImageNet dataset can	o-MobileNet	3.00	0.564	71.1%	28.9%
be resource- and time-consuming, We
resized all the images into 40 × 40 pixels. Due to space limitations, we present the experimental
results on ImageNet40 in Appendix F and Table 7. We can conclude that the proposed o-ResNet
achieved 4-5% (e.g. 49.97% vs 44.93% for 56-layer and 50.72% vs 46.74% for 110-layer) perfor-
mance gains comparing against the ResNet baselines.
3.3.2 Full ImageNet
Similar to the experiments on CIFAR10, We replace the dsep convolutions in the DARTS (V2)
netWork With the proposed osep convolutions to demonstrate that the proposed approach is able to
generalize to other netWork architectures. The experiment is carried out on the full ImageNet dataset.
The DARTS evaluation netWork has 14 cells and 48 initial channels, We increase the initial channel
size to 56 to match the original neural net. The resulting netWork is called o-DARTS. Experimental
results are illustrated in Table 5. It can be seen that, With feWer parameters (4.50 million vs 4.72
million), the proposed o-DARTS netWork achieved higher accuracies in both top1 (74.2% vs 73.3%)
and top5 (91.9% vs 91.3%) accuracies than the DARTS baseline. Finally, We replace the dsep
convolution in MobileNet (HoWard et al., 2017) to the proposed osep convolution. Using only
3.0 million parameters, the proposed o-MobileNet is able to achieve 71.1% top1 accuracy on the
ImageNet dataset. This is a great gain comparing against the original MobileNet With 4.2 million
parameters. We can conclude that the proposed osep is able to achieve better accuracy-FLOPs and
accuracy-#Params balances than dsep convolutions.
4	Conclusions
In this paper, We have presented yet another novel convolution scheme called optimized separable
convolution to improve efficiency. Conventional convolution took a costly complexity at O(C2K2).
The proposed optimized separable convolution scheme is able to achieve its complexity at O(C3 K),
which is even lower than that of depth separable convolution at O(C ∙ (C + K2)). Hence, the pro-
posed optimized separable convolution has the full potential to replace the usage of depth separable
convolutions in a DCNN. Examples considered include ResNet, DARTS, and MobileNet architec-
tures. The proposed optimized separable convolution also has a spatial separable configuration. A
generalized N-separable case can achieve better performance at O(C ∙ log(CK2)).
We believe the proposed optimized separable convolution also has a potential impact on the AutoML
community. The proposed novel operator is able to increase the neural architecture search space.
In a multi-objective optimization formulation, where both accuracy and #Params (or FLOPs) are
optimized, we expect a more efficient network architecture can be discovered in the future using
the proposed optimized separable convolution operator. In the future, we also expect to carry out
experiments on more neural network architectures, e.g. EfficientNet, etc.
9
Under review as a conference paper at ICLR 2022
References
Kaifeng Bi, Lingxi Xie, Xin Chen, Longhui Wei, and Qi Tian. Gold-nas: Gradual, one-level, differ-
entiable. arXiv preprint arXiv:2007.03331, 2020. 3, 3.1
Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging
the depth gap between search and evaluation. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pp. 1294-1303, 2019. 3, 3.1, C
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016. 1, G
Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong
Tian, Matthew Yu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe search using neural
acquisition function. arXiv preprint arXiv:2006.02049, 2020. 1
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009. 1, 3.3
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional net-
works using vector quantization. arXiv preprint arXiv:1412.6115, 2014. 1, G
Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for
efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. 1, G
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016. 1, 1, 2.3, 3.1
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015. 1, G
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Pro-
ceedings of the IEEE International Conference on Computer Vision, pp. 1314-1324, 2019. G
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1, 3.3.2, G
Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training
cnns with low-rank filters for efficient image classification. arXiv preprint arXiv:1511.06744,
2015. 1,G
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. 1, G
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009. 1, 3.1, 3.2
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012. 1
Tony Lindeberg. A computational theory of visual receptive fields. Biological cybernetics, 107(6):
589-635, 2013. 1, 2.2
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018. 1, 3, 3.1, 5, C, G
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV), pp. 116-131, 2018. E
10
Under review as a conference paper at ICLR 2022
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
volume 33,pp. 4780-4789, 2019. 1
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018. 2.3, G
LaUrent Sifre and Stephane Mallat. Rigid-motion scattering for image classification. Ph. D. thesis,
2014. 1
RuPesh K Srivastava, KlaUs Greff, and JUrgen Schmidhuber. Training very deep networks. In
Advances in neural information processing systems, pp. 2377-2385, 2015. 1
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
G
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016. 1, G
Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. arXiv preprint arXiv:1905.11946, 2019. 1, G
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong.
Pc-darts: Partial channel connections for memory-efficient architecture search. In International
Conference on Learning Representations, 2019. 3, 3.1
11
Under review as a conference paper at ICLR 2022
Algorithm 1 The Algorithm for Optimized Separable Convolution
Input: Input channel C1 = Cin, output channel CN+1 = Cout, kernel size (KH, KW), number of sepa-
rated convolutions N
Optional Input: internal kernel sizes (optional, preset), internal number of groups (optional, masked values),
spatial separable (True or False), overlap coefficient (γ = 1).
Output: internal channel sizes C2,…,CN, internal kernel sizes KHIW,…,KHIW, internal number of
groups gι, •…,gN
Calculate internal channel sizes C2, ∙…,Cn as min(Cin, Cout), max(Cin, Cout)14, or 4 min(Cin, Cout),
etc. according to a preset policy.
if internal kernel sizes KHIW,… ,KHIW are not given then
if spatial separable
Set KbHN/2c =
else
SetKHIW =
Set KbN/2c =
then
KH, KbWN/2+1c = KW and all other internal kernel sizes to 1.
KHIW and all other internal kernel sizes to 1.
end if
end if
Calculate internal channels per group nl according to nl
NqlnN+1CinN=IKH πN=ikW
Cl + 1KH KW
Let gl = min(dCl/nle, Cl, Cl+1). If Cl/nl < 1 or Cl/nl > min(Cl, Cl+1) for certain l, re-optimize gl
With a masked number of groups by pre-setting gl = 1 for l ∈ {l : Cl/nl < 1}, gl = min(Cl, Cl+1) for
l ∈ {l : Cl/nl > min(Cl,Cl+1)}.
Return C2,…，Cn; KHIW,…，KHlW; gι,…，gN
.Because nι 〜 NZC, for large channel sizes, We rarely need to re-optimize.
A S ketch of Proof for Theorem 1
Sketch of Proof for Theorem 1. For Equation (12), after applying an arithmetic-geometric mean in-
equality, We can get
f({n*},{KHlW})≥N NrCIC2…CNCN+1KH…KNKW∙∙∙KW	(23)
J'l *j ,ι * J J— V	g1∙∙∙gN	\	/
≥N N:YCr~CN+1KH~KHKW∙KW
The equality holds if and only if C2n1KHKW =…=CN+1nNKNKW. Let n =
βι = CC2KHHKW . Let β = ∏βi, we can solve nι = NqYeF = "CCKHKHnKW and
Cl+1 Kl Kl	β	C2K1 K1
IN YnN+1CinN=IKH ∏N=1KW
Cι+1KH KW
(24)
βln1, where
(25)
Note that the inequality (24) holds for arbitrary KHIW. We need to further optimize KH1W. Again,
from the arithmetic-geometric mean inequality again, we can get KH …KN ≤ (KI +N+KN )n =
(KHNV-1 )nand the equality holds if and only if KH = … = KH = KHNN-1. However, we
want the inequality reversed, instead of finding the maximum of this product, we expect to find its
minimum. This still gives us a hint, the maximum is achieved when the internal kernel sizes are as
even as possible, so the minimum should be achieved when the internal kernel sizes are as diverse as
possible. In the extreme case, one of the internal kernel sizes should take KHand all the rest takes
1, i.e. Equations (18) and (19). A formal proof of this claim can be derived. Hence, we have
f ({n*}, {KHlW}) ≥ N PγC1 …CN+1KHKW	(26)
=O(NY N C 1+ N K N).	(27)
By setting f0(N) = 0, we can derive that
N = log(YCK2),	(28)
and
minf({n*}, {KHlW}) = eCHW ∙ log(γCK2)	(29)
=O(CHW ∙ log(γCK2)),	(30)
where e = 2.71828... is the natural logarithm constant.	□
12
Under review as a conference paper at ICLR 2022
ReSNet20
Cenventlonal (baseline)
Depthwlse (dw-)
Depth SepaEbie W-)
Spatial Separable (j-)
Optimal Sepanible (ə-)
Spatial Optimal Separable (κ>-)

ReSNet32
ReSNet56
ResNetiio
(c)
Figure 5. Experimental results on CIFAR10 for the ResNet architecture (best viewed in color, same as Fig.
3 except networks are channel multiplied to match #ParamS). The proposed optimized separable convolution
(o-ResNet) achieves improved (a) accuracy-#ParamS and (b) accuracy-FLOPs Pareto-frontiers than both the
conventional (ResNet) and depth separable (d-ResNet) convolutions. (c) A comparison for performances of
different convolution schemes.
Cenventlonal (baseline)
Depthwlse (dw-)
Depth Separable (d-)
SPatbl SePaBble (j-)
Optimal Sepanible (0-)
Spatial Optimal Separable (κ>-)
JLIhii
ReSNet20
ReSNet32
ReSNet56
ResNetiio
(c)
Figure 6. Experimental results on CIFAR100 for the ResNet architecture (best viewed in color, same as Fig.
4 except networks are channel multiplied to match #ParamS). The proposed optimized separable convolution
(o-ResNet) achieves improved (a) accuracy-#ParamS and (b) accuracy-FLOPs Pareto-frontiers than both the
conventional (ResNet) and depth separable (d-ResNet) convolutions. (c) A comparison for performances of
different convolution schemes.
B The Proposed Optimized Separable Convolution Algorithm
A detailed implementation of the proposed optimized separable convolution algorithm is illustration
in Algorithm 1.
C Training Settings
Experiments on CIFAR10 and CIFAR100 for the ResNet architecture The images are padded
with 4 pixels and randomly cropped into 32 X 32 to feed into the network. A random horizontal flip
with a probability of 0.5 is also applied. All the networks are trained with a standard SGD optimizer
for 200 epochs. The initial learning rate is set to 0.1, with a decay of 0.1 at the 100 and 150 epochs.
The batch size is 128. A weight decay of 0.0001 and a momentum of 0.9 are used.
Experiments on CIFAR10 for the DARTS architecture We follow the same training settings in
(Liu et al., 2018): the network is trained with a standard SGD optimizer for 600 epochs with a batch
size of 96. The initial learning rate is set to 0.025 with a cosine learning rate scheduler. A weight
decay of 0.0003 and a momentum of 0.9 are used. Additional enhancements include cutout, path
dropout of probability 0.2, and auxiliary towers with weight 0.4.
Experiments on ImageNet40 for the ResNet architecture Each network is trained with a stan-
dard SGD optimizer for 20 epochs with the initial learning rate set to 0.1, and a decay of 0.1 at the
10 and 15 epochs. The batch size is 256, the weight decay is 0.0001 and the momentum is 0.9.
Experiments on full ImageNet for the DARTS architecture We follow the training settings in
(Chen et al., 2019) for multi-GPU training: the images are random resized crop into 224 × 224
13
Under review as a conference paper at ICLR 2022
Table 6. Experimental results on CIFAR10 for the ResNet with inference time on a Windows 10 Intel i5-8250
CPU or Nvidia GeForce RTX 2080 Ti GPU.
Net Arch	Channel Multiplier	#Params (million)	FLOPs (billion)	Accuracy (%)	CPU Infer Time (s)	GPU Infer Time (s)
ResNet20	-	0.270	0.04055	91.25	0.0310	0.0057
o-ResNet20	3.625	0.221	0.04070	93.37	0.0468	0.0120
d-ResNet20	2.72	0.250	0.0400	92.66	0.0468	0.0060
ResNet32	-	0.464	0.06886	92.49	0.0469	0.0067
o-ResNet32	3.75	0.367	0.06726	93.69	0.0937	0.0185
d-ResNet32	2.75	0.429	0.06565	92.98	0.1154	0.0092
ResNet56	-	0.853	0.12548	93.03	0.0938	0.0101
o-ResNet56	3.875	0.682	0.12574	93.81	0.1562	0.0330
d-ResNet56	2.75	0.786	0.11890	92.69	0.1875	0.0181
ResNet110	-	1.728	0.25289	93.39	0.1563	0.0178
o-ResNet110	3.875	1.337	0.24763	94.88	0.3216	0.0671
d-ResNet110	2.75	1.590	0.23870	93.40	0.3462	0.0317
patches with a random scale in [0.08, 1.0] and a random aspect ratio in [0.75, 1.33]. Random
horizontal flip and color jitter are also applied. The network is trained from scratch for 250 epochs
with batch size 1024 on 8 GPUs. An SGD optimizer with an initial learning rate of 0.5, a momentum
of 0.9, and a weight decay of 3e-5. The learning rate is decayed linearly after each epoch. Additional
enhancements include label smoothing with weight 0.1 and auxiliary towers with weight 0.4.
Experiments on full ImageNet for the MobileNet architecture The images are random resized
crop into 224 × 224 patches with a random scale in [0.08, 1.0] and a random aspect ratio in [0.75,
1.33]. Random horizontal flip is also applied (no color jitter). The network is trained from scratch
for 200 epochs with batch size 1024 on 8 GPUs. An SGD optimizer with an initial learning rate of
0.064, a momentum of 0.9, and a weight decay of 1e-5. The learning rate is decayed with a rate of
0.973 for every 0.8 epoch.
D Experimental Results for Matching #Params
In this section, we report experimental results of matching #Params only, instead of matching both
#Params and FLOPs. In Fig. 5 and Fig. 6, we illustrate the experimental results on CIFAR10
and CIFAR100 for the ResNet architecture. As can be seen, the observations in these two figures
are consistent with those in Fig. 3 and Fig. 4. Hence, the conclusions we reached in Section 3.1
and Section 3.2 are not affected. Please refer to these two sections for a detailed description of the
experimental results.
E	Inference Time for the Proposed Optimized S eparab le
Convolution
In this research, we focus on the representational efficiency of the proposed optimized separable
scheme. The representational complexity is measured with the number of parameters (#Params) and
is hardware-independent. For the proposed experiments, we included both #Params and FLOPs. In
this section, we further report the wall-clock inference time of the proposed optimized separable
convolution scheme for reference reasons. It is important to keep in mind that FLOPs measures the
theoretical speed we are able to achieve. The wall-clock time reported in this section is hardware
dependent. Slowness can occur due to an inefficient hardware implementation.
From Table 6, we can see that, under approximately the same FLOPs, both o-ResNet and d-ResNet
take a longer inference time than conventional ResNet. This is because the current implementation
of grouped convolution in PyTorch is not optimized. From Table 6, we can also conclude that, under
approximately the same FLOPs, o-ResNet is slightly faster than d-ResNet (e.g. o-ResNet32 takes
0.0937s while d-ResNet32 takes 0.1154s) on a CPU and yet the former is about twice slower than
the latter (e.g. o-ResNet32 takes 0.0185s while d-ResNet32 takes 0.0092s) on a GPU. The better
wall-clock timing of the proposed osep scheme over dsep on a CPU may suggest that it also has
14
Under review as a conference paper at ICLR 2022
Table 7. Experimental results on ImageNet40 for the ResNet architecture. The proposed optimized separable
convolution (o-ResNet) achieves 4-5% performance gain over the ResNet baseline.
Net Arch	Channel Multiplier	#Params (million)	FLOPs (billion)	Accuracy (%)	Error Rate (%)
ResNet20	-	4.58	0.162	40.28	59.72
o-ResNet20	5.375	5.13	0.160	44.94	55.06
ResNet32	-	7.68	0.275	42.98	57.02
o-ResNet32	5.75	7.78	0.278	47.88	52.12
ResNet56	-	13.88	0.502	44.93	55.07
o-ResNet56	6.0	12.55	0.497	49.97	50.03
ResNet110	-	27.83	1.012	46.74	53.26
o-ResNet110	6.25	23.79	1.027	50.72	49.28
advantages for ARM CPU architectures. Hence, the proposed osep scheme could be more efficient
for mobile applications.
There are good reasons for the slowness of the proposed osep on a GPU. In fact, there are two extra
copies of blobs in our current Python implementation of the proposed osep convolution (one for
group convolution if the number of groups does not divide the input or output channels, and the
other one for the channel shuffle operation). These two extra copies of blobs can actually be avoided
for an efficient implementation. However, optimizing this code shall require a careful tweak of
the CUDNN library. It is known that on a GPU, the memory access cost can dominate over the
computational cost (Ma et al., 2018). Hence, the slowness of the proposed osep scheme shall occur.
On a CPU, the computational cost dominates over the memory access cost. Hence, the proposed
osep is faster than dsep. In the future, we expect the bottleneck of memory access for a GPU could
be addressed and the wall-clock timing of the proposed osep scheme could be greatly sped up.
F	Experimental Results on ImageNet40
Because carrying out experiments directly on the ImageNet dataset can be resource- and time-
consuming, we resized all the images into 40 × 40 pixels. A 32 × 32 patch is randomly cropped
and a random horizontal flip with a probability of 0.5 is applied before feeding into the network.
No extra data augmentation strategies are used. The baseline ResNet architecture is a modified
version of that used on the CIFAR10 dataset, except that the channel sizes are set to be 4× larger,
the features are calculated on scales of [16, 8, 4], and the last fully-connected (FC) layer outputs
1000 categories for classification. We make this modification because the ImageNet dataset has sig-
nificantly more training samples than the CIFAR10 dataset. Experimental results are illustrated in
Table 7, as can be seen, by substituting conventional convolutions with the proposed optimized sep-
arable convolutions, the resulting o-ResNet achieved 4-5% (e.g. 49.97% vs 44.93% for 56-layer and
50.72% vs 46.74% for 110-layer) performance gains comparing against the ResNet baselines. This
demonstrates that the proposed optimized separable convolution scheme is much more efficient. For
o-ResNet56 and o-ResNet110, they also have fewer parameters which could contribute to a more
regularized model. For o-ResNet20 and o-ResNet32, they have slightly more parameters because
the last FC layer accounts for a great portion of overhead for 1000 classes.
G	Related Work
There have been many previous works aiming at reducing the amount of parameters in convolution.
Network pruning (Han et al., 2015) strategies are developed to reduce redundant parameters that are
not sensitive to performances. Quantization and binarization (Gong et al., 2014; Courbariaux et al.,
2016) techniques are introduced to compress the original network by reducing the number of bits re-
quired to represent each parameter. Low-rank factorization methods (Jaderberg et al., 2014; Ioannou
et al., 2015) are designed to approximate the original weights using matrix decomposition. Knowl-
edge distillation (Hinton et al., 2015) is applied to train a compact network with distilled knowledge
from a large ensemble model. However, all these existing methods start from a pre-trained model.
Besides, they mainly focus on network compression and have limited or no improvements in terms
of network acceleration.
15
Under review as a conference paper at ICLR 2022
Among various implementations of convolution, separable convolution has been proven to be more
efficient in reducing the representational demand. Depth separable convolution is explored exten-
sively in modern DCNNs (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019; Liu et al.,
2018; Tan & Le, 2019). It reduces the representational cost of a conventional convolution from
O(C2K2) to O(C ∙ (C + K2)). However, the proposed optimized separable convolution is even
more efficient than depth separable convolution. It can be represented at O(C3 K) and has the full
potential to replace the usage of depth separable convolutions. A second advantage of the proposed
optimized separable convolution is that it can be applied to fully connected layers if we view them
as 1 × 1 convolutional layers, whereas depth separable convolution cannot. Further, depth separable
convolution requires the middle channel size to be equal to the input channel size, whereas for the
proposed optimized separable convolution, the middle channel size can be freely set.
Spatial separable convolution was originally developed to speed up image processing operations.
For example, a Sobel kernel is a 3 X 3 kernel and can be written as (1,2, I)T ∙ (-1,0,1). Spatial
separable will require 6 instead of 9 parameters while doing the same operation. Spatial separable
convolution is also adopted in the design of modern DCNNs. For example, in (Szegedy et al., 2016),
the authors introduce spatial separation to the GoogLeNet (Szegedy et al., 2015) architecture. For
the proposed optimized separable convolution, there is also a spatial separable configuration.
In the body of literature, separable convolution is also referred as factorized convolution or convolu-
tion decomposition. In this research, the proposed scheme is called optimized separable convolution
following the naming conventions of depth and spatial separable convolutions.
16