Under review as a conference paper at ICLR 2022
Coherent and Consistent Relational Trans-
fer Learning with Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Human defined concepts are inherently transferable, but it is not clear under what
conditions they can be modelled effectively by non-symbolic artificial learners.
This paper argues that for a transferable concept to be learned, the system of re-
lations that define it must be coherent across domains. This is to say that the
learned concept-specific relations ought to be consistent with respect to a the-
ory that constrains their semantics and that such consistency must extend beyond
the representations encountered in the source domain. To demonstrate this, we
first present formal definitions for consistency and coherence, and a proposed Dy-
namic Comparator relation-decoder model designed around these principles. We
then perform a proposed Partial Relation Transfer learning task on a novel data
set, using a neural-symbolic autoencoder architecture that joins sub-symbolic rep-
resentations with modular relation-decoders. By comparing against several exist-
ing relation-decoder models, our experiments show that relation-decoders which
maintain consistency over unobserved regions of representational space retain co-
herence across domains, whilst achieving better transfer learning performance.
1	Introduction
Humans are capable of learning concepts such that they can be applied to many different scenar-
ios (Inhelder & Piaget, 1964; Piaget, 2005; Lake et al., 2017). An important characteristic is that
human-like concepts remain coherent across contexts, whereby their logical consistency in one con-
text is retained in another (Nye et al., 2021). As an example, consider the concept of ordinality
which permits comparison over ordered sets, e.g. “A is larger than B”, and pertains to a multitude of
properties: position, size, volume, reach, etc. So long as one of these properties can be attributed to
an object, a set of objects can be compared on that basis; in this sense ordinality generalises between
objects. All in all, if the concept of ordinality were to be learned in its most general form, it should
be coherent across properties and objects.
In this paper, we seek to define the conditions that allow a learned concept to transfer well across
properties and objects in the case of sub-symbolic learners (d’Avila Garcez & Lamb, 2020; Santoro
et al., 2021; Greff et al., 2020). We define consistency and coherence of sub-symbolic learners bor-
rowing from analogous definitions from symbolic AI. We propose a neural-symbolic autoencoder
architecture consisting of a neural encoder for objects coupled with modular relation-decoders (Ser-
afini & Garcez, 2016; Donadello et al., 2017; Badreddine et al., 2020; Wang et al., 2017; Nickel
et al., 2016a; Dai et al., 2020), and we show that this architecture is capable of achieving an im-
proved transfer learning performance by being coherent across properties and objects.
We therefore claim that retaining consistency across domains dictates concept coherence, offering
a more fine-grained measure of transfer learning than accuracy alone. The proposed architecture
is a practical instantiation of this formalisation and is evaluated in this paper on a new Partial Re-
lation Transfer (PRT) task and data set. We begin by expressing the symbolic application of a set
of relations to some domain of interest as a model-theoretic structure, such as A is larger than B,
and defining an analogous soft-structure for non-symbolic learners where relations are modelled by
relation-decoders that compute beliefs. We then propose formal definitions for consistency and co-
herence of soft-structures which provide a practical consistency score calculation to the evaluation of
autoencoders. Finally, we present a benchmark PRT learning task with the use of a new BlockStacks
data set derived from the CLEVR data set rendering agent. We then compare our proposed archi-
1
Under review as a conference paper at ICLR 2022
tecture with several existing relation-decoder models on transfer learning tasks from the MNIST
data set to BlockStacks. Our experiments show that relation-decoders which maintain consistency
over unobserved regions of representational space retain coherence across domains whilst achieving
better transfer learning performance. The contributions of this paper are:
•	A formal definition of coherence and consistency for sub-symbolic learners with a practical
evaluation score.
•	A neural model and learning task for partial relation transfer including a new data set to
evaluate concept coherence.
•	A comprehensive critical evaluation of results in comparison with multiple state-of-the-art
relation-decoder models.
In Section 2 we provide the required background, Section 3 introduces soft-structures and formally
defines coherence and consistency, Section 4 provides a practical consistency loss and Section 5
then outlines our neural-symbolic architecture. After detailing the PRT task in Section 6, we present
results in Section 7 and complete the paper in Section 8 with a discussion and concluding remarks,
including limitations and future work. We provide related work in Appendix A.
2	Preliminaries
Notations: We reserve uppercase calligraphic letters to denote sets and lowercase versions of the
same letter to denote their elements, e.g., S = {s1, . . . , sn} is a set S of n elements si. We indicate
with |S | = n the cardinality of S. We use uppercase roman letters to denote a random variable e.g.,
S, and use the uppercase calligraphic version of the same letter (S) to denote the set from which the
random variable takes values according to some corresponding probability distribution, pS , over the
elements of the set, such that P|iS=|1 pS(si) = 1 for a discrete S. For brevity, we may write pS(si) as
p(si), where the random variable is implied by the argument. We use bold font lowercase letters to
denote vector elements, e.g., si ∈ Rd is an d-dimensional vector element from the set S = Rd.
Logic and model-theoretic background: We assume a formal language L composed of variables,
predicates (i.e. relations), logical connectives  (negation), ∨ (disjunction) and ∧ (conjunction), →
(implication) and universal quantification ∀ (for all) with their conventional meaning (see Shapiro
& Kouri Kissel (2021)). Relations express relational knowledge over elements of a domain. For
instance, r(s1 , s2) states that elements s1 and s2 are related through the binary relation r. The
meaning of relations is defined by an interpretation, ISσ over elements of an non-empty domain S .
Together a S and ISσ form a structure Sσ .
Definition 1 (Signature, Interpretation, Structure) The signature of a language L is σ = {r ∈
L : r is a relation}, whose elements have arity given by ar : σ → N, where N is the set of natural
numbers. For each r ∈ σ, ar(r) denotes the arity of r. Given a signature σ anda non-empty domain
S, an interpretation ISσ ofσ over elements ofS assigns to each relation r ∈ σ a set ISσ (r) ⊆ Sar(r).
A structure is a tuple Sσ = (S, ISσ).
Note that for a fixed domain S and signature σ, different interpretations yield different structures.
We construct universally quantified first-order formulae (called sentences) using the signature σ of
L, whose truth-value is defined with respect to a given structure Sσ . To do so, we first consider
ground instances of a formula. These are given by replacing all the variables in the formula with
elements from the domain S. For instance, r(s1, s2), where s1 and s2 are elements of S, is a
ground instance of an atomic formula r(i, j) where i and j are variables in L. Given a structure
Sσ = (S, ISσ) a relation r and a tuple (s1, . . . , sar(r)) ∈ Sar(r), a ground instance r(s1, . . . , sar(r))
is true in the structure Sσ if and only if (s1, . . . , sar(r)) ∈ ISσ (r). The truth value of a sentence in
a given structure Sσ depends on the truth value of its respective ground instances. Specifically, a
sentence is true in a structure Sσ if and only if all of its ground instances are true in Sσ . When a
sentence, τ , is true in a structure, Sσ , we say that the structure satisfies τ , denoted as Sσ |= τ . A set
of sentences form a theory, T. A model of T is a structure that satisfies every sentence in T.
Definition 2 (Model of a theory) Let T be a theory written in a language L and let Sσ = (S, ISσ)
be a structure, where σ is the signature of L. Sσ is a model of T if and only if Sσ |= τ for every
sentence τ ∈ T.
2
Under review as a conference paper at ICLR 2022
Example 1 Suppose we have the structure Sσ = (S, ISσ), where S is a domain of images of hand-
written digits and σ the signature of binary relations σ = {isGreater, isEqual, isLess, isSucces-
sor, isPredecessor}, or for short σ = {G, E, L, S, P}. Let T be the theory that defines ordinality
including, for instance, the sentence ∀i,j. G(i,j) → -E(i,j) (ifa digit is greater than another then
they are not equal). Any structure Sσ = (S, ISσ) with interpretations ISσ ofσ that captures a total
order over the elements of S is a model of T.
3	Approximating structures that have real-world domains
In this section we turn our attention to the challenge of learning a model over a real-world do-
main, given a signature and theory. Here a learner must determine an appropriate interpretation
over real-world data, such as images or other perceptions. This can be challenging because, firstly,
we may only have a partial description of the interpretation, and secondly data may be noisy and
contain information that is not relevant to the theory. For instance MNIST, a relatively simple data
set by current standards, consists of stylistic details such as line thickness and digit skew (Chen
& Batmanghelich, 2020), which are irrelevant to the notion of ordinality, which makes obtaining
the structure from Example 1 more complicated. Nevertheless, statistical machine learning models
are able to discover commonalities in data which help to infer the underlying semantics (i.e. inter-
pretation) and disregard the noise. Following the convention in disentanglement literature (Bengio
et al., 2013; Kingma & Welling, 2014; Higgins et al., 2017; 2018), we take the assumption that real-
world observations S are drawn from some conditional distribution pS|Z, where Z is a latent random
variable, itself drawn from prior pZ. It is therefore useful to define a domain encoding of the form,
ψS : S → Z,	(1)
tasked with approximating the conditioned expectation of the posterior, i.e. ψS (s) = E[pZ|S(Z|s)].
Since obtaining an interpretation from domain encodings, for a given signature, may require dealing
with noise, we express the interpretation of relations over real-world data by belief functions over
the space Z (Paris & Vencovska, 2015; Paris, 1994), and refer to these as relation-decoders:
φr : Zar(r) → (0, 1)	(2)
with φ = {φr : r ∈ σ}. Concretely, for a binary relation r and ordered pair (si, sj ) ∈ S2,
φr(ψS(si), ψS (sj)) describes the belief that (si, sj) ∈ ISσ (r). A belief φr (ψS (si), ψS (sj)) ≈ 1
signifies a strong belief that (si, sj) ∈ ISσ (r) and φr(ψS(si), ψS (sj)) ≈ 0 signifies a strong belief
that (si, sj) ∈/ ISσ (r). Together, ψS and φ allow us to define a belief-based analogue to a structure.
Definition 3 (Soft-Structure/Soft-Substructure) Given signature σ, a possibly infinite set Z and
relation-decoders φ, a soft-structure is a tuple Zσ = (Z, φ). For (finite) domain S and encoding
ψS : S → Z, Sσ = (ψS (S), φ) is a (finite) soft-substructure of Zσ, with sub-domain ψS (S) =
{ψS (s)|s ∈ S} ⊆ Z.
A soft-structure can be used to learn a structure over a real-world domain through learning ψS and
φ. Clearly, a finite soft-substructure is a soft-structure. To determine the degree to which a soft-
structure supports any given structure we introduce the following measure:
p(Sσ ∣Sσ ) = Y Y (Φr(Ψs (O)))YO，S。(1— Φr(Ψs (O)))1-γr,sσ	⑶
r∈σ O∈Sar(r)
where γOr ,S	= 1 if O ∈ ISσ (r), and 0 otherwise; we use φr (ψS (O)) as shorthand for
φr(ψS(s1), .. . , ψS(sn)) for n = ar(r). Eqn. 3 expresses the assumption that, given a finite soft-
structure, the beliefs in what constitutes the (different) interpretations of (different) relations are
independent of one another. It is straightforward to show that ESb P(Sσ ∣Sσ) = 1 (summed over all
possible structures with domain S and signature σ) and so it can be treated as a probability measure,
where p(Sσ ∣Sσ) ≈ 1 means that there is a high probability that the interpretation sampled from Sσ
will be ISσ . If we have a theory T over σ then it is natural to ask with what weight Sσ supports any
f
given structure that is a model of T. In the following, we use model weight, ΓTSσ, to describe the
support given by Sσ to models of T:
ΓTσ =	E p(Sσ ∣Sσ)	(4)
Sσ∈MTS
3
Under review as a conference paper at ICLR 2022
where MTS is the set of all structures with domain S that are models of T. This lets us compare
soft-structures, wherein a good soft-structure will be one that has a high model weight:
f
Definition 4 (e-Consistency of Soft-Structure) Given a finite soft-structure S。, if 1 - ΓT ≤ e
then we say that the soft-structure is -consistent with theory T.
We propose e-consistency as an appropriate quantified measure of the notion of consistency pre-
sented in (Nye et al., 2021). A consistent soft-structure Sσ ensures that φ gives high belief only to
interpretations that satisfy, i.e., are consistent with, T. However, this expression is limited to the
domain encodings of Sσ, i.e. ψS (S). Going a step further, for a concept to be learned in a manner
comparable to what a human might learn, we would expect that this consistency carries over to new
domains and their corresponding soft-structures, as defined in what follows.
3.1	Coherence between soft-structures
In this section we define the notion of coherence between soft-structures, which aims to characterise
what it means for a concept to be learned in a human-like manner. As a motivating case, consider
a situation where we have already learned a soft-structure that has high model weight with models
from Example 1. Now suppose we are given anew domain of images, Y, showing single block stacks
of differing height, and let us again use the signature of ordinal relations and T from Example 1.
Lastly, let IYσ be a corresponding interpretation that orders images according to block stack height
and is a model of T. We can summarise this with the following two structures:
Xσ = (X,IXσ) ∈MTX	and	Yσ = (Y,IYσ) ∈MTY,	(5)
where Xσ is the structure from Example 1 with a domain of handwritten digits and Yσ is our new
structure, with a domain of block stack images. These can be modelled by soft-structures:
~ . ... ~ . ...
Xσ = (Ψx(X ),Φ)	and	Yσ = (Ψγ (Y ),Φ),	(6)
which use domain-specific encoders, ψX and ψY, but share the same relation-decoders. As we know
that Xσ has a high model weight and since φ is shared with Yσ, a natural question to ask is: under
what conditions will a φ that is consistent over domain-encodings ψX (X ) also be consistent over
ψY(Y )? If this is the case, then we know that the high model weight in Xσ is reciprocated for Yσ.
Concretely, we are interested in when the following coherence condition holds.
Definition 5 (e-Coherence across soft-structures) Two soft-structures, X。 and Yσ that share
relation-decoders φ, are said to be e-coherent with respect to a theory T, if they are both at least
e-consistent with T.
Coherence between X。 and Y。 as defined above means that the concept of ordinality that applies
to digit ordering can also be applied to block stack height ordering. It is desirable that learning
ordinality on the domain of digits produces a coherent concept of ordinality with respect to other
ordinal properties, such as height. Since it is possible that ψS (X ) and ψS (Y ) produce unique
encodings, coherence relies on φ's ability to generalise over possibly disjoint subsets of Z 1.
4	A PRACTICAL CONSISTENCY LOSS AND -PROXY
Calculating Eqn. 4 can quickly become intractable as it involves computing φ beliefs for every
grounding and comparing these with every interpretation that is a model of the theory of interest.
We therefore want to derive an efficient consistency loss and a calculable e-proxy, that can act as
a proxy estimate for a soft-structure’s e-consistency/coherence with a given theory, without needing
access to every model and without requiring an exhaustive calculation over every grounding. In this
section, we present the e-proxy derivation outline and defer the expanded derivation to Appendix H.
Suppose we have a fixed domain S and a theory T, whose sentences use relations from a signature
σ. A structure S。 will be a model of T if and only if each ground instance of each formula of
1If soft-structure Zσ (defined over the full space Z) is consistent, then coherence is guaranteed between all
possible soft-substructures.
4
Under review as a conference paper at ICLR 2022
Figure 1: Network architecture used for PRT task, With YO=φr (ψ∣nc(O)). The figure shows how
relational learning is performed on the source MNIST data set (to learn e.g. that digit 5 is greater than
3). As discussed in Section 6, moving to the target domain (to learn that a stack of blocks is greater
than another) involves training a new ^^nc/dec together with a subset of the φr relation-decoders
(with fixed parameters), where the rest are held out as zero-shot transferred relation-decoders.
T is satisfied. Let k ∈ {1, ..., K0} be the index associated with each unique grounding of domain
elements to the variables of T. Further, take BT to be a Boolean random variable denoting the truth-
value ofT, so that the probability of the theory being either satisfied (bT = 1) or violated (bT = 0)
across ground instances, under a soft-structure Sσ can be expressed as p(bT&, k). Notably, by the
definition of a theory, ground instances of formulae will always hold as true for any model of T,
i.e. p(bT = 1∣Sσ, k) = 1 if Sσ ∈ MT. Similarly, when Sσ is consistent with T then we should
also find p(bτ = 1∣Sσ, k) ≈ 1. We thus define our consistency loss as an expectation of the binary
cross entropy between P(BT∣Sσ, k) and P(BT∣Sσ, k), which, given p(bτ = 0∣Sσ, k) = 0 for any
k grounding, simplifies to the expected negative log-likelihood of satisfying T under a randomly
sampled grounding,
L(T,Sσ) = Ek〜p(k) [- lnp(bT = 1∣Sσ,k)]∙	⑺
where p(k) = K is taken to be uniform over the possible unique groundings. However, we still
require an -proxy measure based on this loss, to enable practical evaluation of concept coherence.
SS
To achieve this, we define ΓTSσ = exp(-L(T, Sσ)) and use its relationship with ΓTSσ to define a
proxy bound,
1
ln1-1 ≥ L(T,Sσ)	(8)
where e ≥ 1 — ΓT. In our results, we take e-proxy coherence to be the uppermost bound of ln ι--≡
between source and target domains.
5	Neural model
The critical components of a soft-structure, Sσ, are its domain-encoder ψs and modular relation-
decoders φ. Together these form an autoencoding architecture which, given a domain of images
S ⊂ RW ×H and with d-dimensional latent space Z = Rd, converts sub-symbolic encodings from
ψS into a modular relational representation via decodings for each φr , r ∈ σ. Additionally, to retain
information in Z pertaining to S which is beyond the requirements of φ, we include an additional
domain-decoder, which produces domain reconstructions S. The overall neural model is depicted
by Figure 1, where we use ψSenc to refer to the domain-encoder and ψSdec for the domain-decoder. To
train the neural model, we assume a ground truth interpretation ISσ is given, allowing us to directly
maximise Eqn. 3 via negative log-likelihood loss:
L Sσ = -log p(Sσ ∣Sσ ),	⑼
5
Under review as a conference paper at ICLR 2022
To obtain informative latent representations for S , we use a Variational AutoEncoder (VAE), specif-
ically the β-VAE, given its simplicity and demonstrated ability to separate distinct factors in the
latent representation (Higgins et al., 2017; Burgess et al., 2017; Kingma & Welling, 2014). The
β-VAE achieves this by optimising the ELBO objective, LEβL-VBAOE, but with an additional β scalar
hyperparameter that can be thought of as a disentanglement pressure, forcing distinct explanatory
factors to align with different axes of the latent space. We provide the full ELBO loss, with a detailed
explanation, in Appendix C. We combine losses over each model component to give the following
aggregate objective:
Ljoint = LeLVAE - λLSσ	(10)
where λ is a scalar weighting parameter. Together with the LEβL-VBAOE, the choice of relation-decoder
model will shape how the domain-encodings are structured (GUtierrez-BasUltO & Schockaert, 2018).
Our evaluation considers a selection of relation-decoder models that cover a range of representa-
tional flexibility. Amongst these is our proposed low-complexity, but nonetheless expressive, Dy-
namic Comparator (DC) model. The overall DC model is composed of two modes, a distance-based
measure, φf, that measures the distance between two inputs relative to a reference point, and a step-
like function, φr, that determines the sign of the difference between two points, optionally with an
offset. Although we can use any functions that have the required characteristics for φt and φ*, in
this paper we use the following implementation:
φDC(zi, Zj) = ar,o∙φr + ar,ι ∙ φr where	(11)
φr = fo(—ηr,θ(ku Θ	(Zi	—	Zj +	. )∣∣2))	and	φr	= fl(ηr,1∙	U> (Zi	- Zj + ^)).	(12)
Here ar = Softmax(Ar) ∈ (0, 1)2 is an attention weighting between the two modes, f0 and f1
are an exponential and sigmoid function, respectively ; ur = Softmax(Ur) ∈ (0, 1)m is an at-
tention mask which is applied to m-dimensional embeddings; b, b ∈ Rm are learnable bias terms
that enables an offset to each mode; and ηr,0 ∈ R+ are non-negative and ηr,1 ∈ R any-valued scalar
terms, respectively. Lastly, Θ denotes the Hadamard product and ∣∣ ∙ ∣∣2 is the L2-norm. The key
innovation behind DC is its ability to model each of the ordinal relations whilst encouraging gener-
alised consistency across the full latent subspace, as defined by each ur . This is achieved without
explicit weight sharing, wherein relation-decoders discover parametric relationships between rela-
tions from the data. We provide an additional depiction and analysis ofDC in Appendix D.1, which
further illustrates these effects.
6	Experimental design
In this section we describe an experimental design used to compare soft-structure coherence when
choosing different relation-decoder implementations.
Partial Relation Transfer (PRT): The evaluation involves a proposed PRT task across two soft-
structures Xσ and Yσ . Each shares a common signature σ and relation-decoders φ but have disjoint
domains X and Y, respectively. The experimental procedure involves first learning φ on source
domain X , together with its domain-specific autoencoder. In the second phase, we train a new
domain-specific autoencoder on the target domain, Y, alongside a selection of the now learned φ
relation-decoders but with fixed-parameters. The selected relation-decoders act as training guides
for ψYenc, whilst held-out relation-decoders can be evaluated against zero-shot transfer performance.
For domain X we employ the MNIST handwritten digits data set (LeCun & Cortes, 2010), and
for domain Y we use a proposed BlockStacks data set, which includes singular multi-colored cube
stacks of differing height, each containing one randomly positioned red cube (see Appendix B for
further details and examples). The shared signature includes the ordinal relations, i.e. σ ={G, E,
L, S, P}, and is applied to digit ordering in MNIST and red cube position ordering in BlockStacks.
We provide results against a theory of ordinality, as explored in Example 1 - we provide a formal
specification of this theory in Appendix G. When guiding ψYenc to perform a similar mapping to
ψXenc, i.e. from domain to a similar ordinal subspace as defined by φ, we could use the full φ set
of relation-decoders. However, this is not necessary from a logical standpoint, as our system of
relations can all be expressed in terms of isSuccessor. We therefore only employ the isSuccessor
relation-decoder as a fixed-parameter guide for ψYenc.
Neural model components: Together with DC, existing relation-decoder models compared here
are: TransR (Lin et al., 2015), HolE (Nickel et al., 2016b), NTN (Socher et al., 2013). We addition-
6
Under review as a conference paper at ICLR 2022
Figure 2: [Top] Relation-decoder prediction accuracy per relation and model, in the source (left) and
target domains. Relations are abbreviated on the x-axis by {S: isSuccessor, P: isPredecessor,
E: isEqual, G: isGreater, L: isLess}, with a red highlight identifying a relation included as a
guide for ψYenc . [Bottom] Impact of different values of β for each relation-decoder (mean across
all relations in the source domain (left) and mean for held-out relations only in the target domain
(right). Notably, it can be seen that our model (DC) is not impacted while all other models show a
decrease of accuracy in the target domain.
ally include a basic neural-network baseline, NN. To produce domain-encodings, all experiments
use a β-VAE. We provide further details for all models in Appendix D. Due to a convergence issue
when using a pretrained DC with fixed parameters, a flexible fitting procedure was necessary, in
which we enable the DC parameters to train in the target domain, but with the additional loss term
∣∣ρ* - ρk, between pretrained ρ* and untrained parameters ρ, respectively. In all cases We evaluated
the final parameter values in the target domain and found them to be approximately equivalent to the
ρ*. We did not apply this method to the other models as they were all able to fit the isSuccessor
relation in the target domain.
Hyperparameters: In the source domain we explore β values between {1, 4, 8, 12}, and set
λ = 103 and in the target domain we first normalise losses (see Appendix D.4) and set β = 10-4
and λ = 10-2 as these produced good reconstructions whilst also ensuring optimisation against
J
LYσ . In all experiments, we fix Z = R10.
7	Key Results
In this section, Figure 2 firstly shows standard PRT prediction accuracies per relation in both the
source and target domain. Figure 3 then presents consistency losses for three color-coded data splits:
data-embeddings (blue), where all inputs are encodings of a domain’s test data; interpolation (green),
where we obtain an empirical mean and variance for the domain’s data-embeddings and sample
from a corresponding Gaussian distribution; and extrapolation (red), where we sample from regions
strictly outside the smallest, axis aligned hyper-rectangle that encloses all data-points. Finally, Table
1 concludes with a clear -proxy coherence comparison between relation-decoders2.
Relation-decoder PRT accuracy performance: Figure 2-top provides relation-decoder prediction
accuracy in both the source MNIST (left), and target BlockStacks (right), domains. Key observations
are that DC produces excellent PRT performance, whilst NN, NTN and HolE all see some degrada-
tion from their source accuracies on relations other than isSuccessor. TransR seems to maintain an
target accuracy profile similar to its performance in the source domain, but this is significantly below
the performance of other models in the source domain.We include β ’s impact on these performances
in Figure 2-bottom. Barring DC which has little discernible change in either domain, PRT perfor-
mance is significantly impacted by β in all models, but has little effect in the source domain. TransR
shows a strong positive correlation between target domain accuracy and β , whereas the remaining
models produce their best PRT performances with intermediate disentanglement pressure.
2We take φr prediction values above 0.5 to signify a truth prediction and those below 0.5 to signify falsity.
7
Under review as a conference paper at ICLR 2022
Figure 3: Modified [Top] Con-A values for each relation-decoder model, referenced to source (left)
and target (right) domains (lower values better). [Bottom] Con-I values (lower values better) for
each relation-decoder model referenced to source (left) and target (right) domains, where stacked
bars are for formula: transitivity (white), asymmetry (magenta) and reflexivity (black). In all plots,
darker color shades denote higher values of β, corresponding to greater disentanglement pressure
from the β-VAE. In top-left and bottom plots, blue, green and red groups show results for data-
embeddings, interpolation and extrapolation embeddings respectively (see main text for details).
Consistency Across (Con-A): To interrogate further how β affects each model, Figure 3-top
presents consistency losses against formulae that constrains truth value assignments across rela-
tions, under a theory of ordinality, referred to as Con-A3. Results are referenced to both source
(left) and target (right) domain embeddings. With reference to the source domain, we note that DC
shows excellent Con-A in all regions. Most other models have worse interpolation and extrapolation
consistency. Increasing β appears to improve interpolation and extrapolation performance for mod-
els NN, NTN and TransR, but there are indications that this trend does not persist into the largest
β = 12 value. On the other hand, HolE shows a negative correlation between β and Con-A per-
formance, across all data-splits. Although DC sustains impressive Con-A results for target domain
data-embeddings (right), results for all other models are notably worse with respect to their source
data-embeddings performances and are instead comparable with their interpolation or extrapolation
results in the source domain. It may therefore be possible to anticipate/diagnose poor transfer per-
formance by evaluating interpolation and extrapolation consistency, which suggests that DC’s strong
consistency generalisation indeed enables it to learn a transferable concept.
Consistency Individual (Con-I): Figure 3-bottom presents stacked consistency losses, for formula:
transitivity (white), asymmetry (magenta) and reflexivity (black); results are averaged over indi-
vidual relations and are together grouped under label Con-I, given that they refer to constraints on
individual relations. Losses are again partitioned between source domain (left) and target domain
(right). We firstly observe that DC and NN share the best overall Con-I performance profiles, with
TransR following closely. DC and TransR both show comparable data-embedding versus interpola-
tion/extrapolation performance, whereas NN, NTN and HolE suffer from degradation across these
splits. Interestingly, these results show that: DC only suffers on transitivity, NN and TransR mainly
struggle to model transitivity but show additional loss for asymmetry and HolE demonstrates diffi-
CUlty in modelling each of the Con-I sub-stack. With regards to β ’s impact, it is not possible to deter-
mine a correlation for DC. However, NN and NTN demonstrate a negative correlation of β against
overall Con-I, with comparable response for each underlying sub-stack. TransR shows a significant
Con-I extrapolation improvement with increased β and HolE is for the most part adversely impacted
as β is increased. Similar trends can be seen for target Con-I performance. However, notably, many
models show improvements, in particular in Con-I (asymmetry). This could be due to more precise
target domain data-embeddings, as a result of using a single pretrained relation-decoder.
-proxy coherence: Table 1 provides a comparison between optimal -proxy coherences achieved
for each relation-decoder model, as defined in Section 4. Results are partitioned according to each
consistency type and an Aggr(egate) value, which gives best summed consistency, together with
best β = β* values. DC clearly outperforms all other models in e-proxy coherence across all types.
NN achieves strong aggregated -coherence compared with NTN, HolE and TransR outperforming
across Con-A and Con-I-tr. Although NTN and HolE have similar aggregate e-proxy coherence,
3Truth tables for each consistency formula is given in Appendix G
8
Under review as a conference paper at ICLR 2022
Table 1: -proxy coherence comparison, with respect to source and target data-embedding con-
Sistency levels. Results are reported with the corresponding β = β* setting (in parenthesis).
The consistency loss abbreviations refer to: (A)cross, (tr)ansitivity, (asym)metry, (refl)exivity and
(Aggr)egate, which gives the best obtained aggregate (summed) consistencies.
φ	Aggr.	(β*)	Con-A	(β*)	Con-I-tr	(β*)	Con-I-asym	(β*)	Con-I-refl (β*)
TransR	90.33	(8)	44.34	(12)	35.30	(8)	9.94	(8)	0.55	(8)
HolE	82.06	(8)	41.18	(8)	32.15	(4)	5.96	(1)	0.07	(8)
NTN	79.54	(8)	38.91	(8)	30.08	(12)	4.49	(12)	0.09	(12)
NN	34.09	(8)	24.78	(8)	7.24	(8)	3.88	(8)	0.04	(4)
DC	0.34	(1)	0.07	(1)	0.18	(1) 一	0.00	(1T	0.09	(1)
TransR performs generally worse. This may be caused by TransR producing weaker beliefs in
comparison to other models, as this can result in a worse overall consistency level. Looking at β*
profiles, we see that most models achieve optimum aggregate -proxy coherence at β = 8, other than
DC which performs better at β = 1. Overall, this is in agreement with the β profiles given by Figure
2-bottom (right). However, We can see that β* profiles for Con-A based e-proxy coherence are in
more direct agreement - as TransR achieves its best at β = 12 - suggesting that Con-A invariance is
more important to concept transfer.
8	Discussion and concluding remarks
In this work, we introduced the notion ofa soft-structure, which can learn a structure over real-world
domains, through the use of a domain-encoder coupled with modular relation-decoders. We subse-
quently provided formal definitions, defining what it means for a concept to be coherent in terms of
domain-invariance of soft-structure consistency with respect to a theory. We then outlined a neural
model and experimental procedure that together allowed us to investigate how concept coherence
differs when choosing different implementations for underlying relation-decoders and its impact on
concept transfer. Our results suggest that increasing regularisation over relation-decoder models,
either in the form of disentanglement pressure or relation-decoder model capacity, seems to improve
their ability to learn coherent concepts. Firstly, strong PRT transfer for DC and NN (given an appro-
priately high β setting) showed that both relation-decoder models are able to minimise Eqn. 9 in the
source domain and retain good performance in the target domain. Consistency profiles over partial
theories (subsets of the sentences that comprise the overall theory of ordinality), covering multiple
data-splits, then further suggested that a relation-decoder’s ability to retain consistency over inter-
polated/extrapolated regions with respect to the observed data-encodings (during training) greatly
impact concept coherence. Finally, an e-proxy coherence comparison showed that DC achieved ex-
cellent coherence with an aggregated e-proxy of 0.34, which mirrors its strong PRT performance.
NN achieved a score of 34.09, which, although significantly worse than DC, is a marked improve-
ment over the remaining models. All in all, the empirical analysis in this work provides strong
evidence towards the hypothesis that the transferability of a concept depends on its coherence, as
measured by the retention of consistency across domains.
Limitations and future work: Firstly, this work only considered binary relations, and in particular
unary relations, such as digit classification, are not considered. Additionally, we have only consid-
ered a fixed signature which is learned “all at once” in a source domain. In practical applications,
however, it is quite possible that ordinality would be discovered gradually, either through incre-
mental learning of the relations that form a ‘complete’ signature for ordinality, or through gradual
refinement of pre-learned relations after being progressively exposed to different contexts in which
ordinality applies. This necessitates a continual learning procedure which has not been explored
in this work and would be useful as an addition in future work. Further, even in a single domain,
ordinality can be applied to multiple properties (e.g. for BlockStacks we have: block stack height,
block size, position of stacks, etc.) and future work can explore our framework’s ability to initialize
multiple instances of our signature, each applied to a different ordinal property. Lastly, we have
only explored a signature for ordinality, whereas other fundamental properties are easy to find, such
as periodic (e.g. rotation) and unordered categorical (e.g. shape) properties. These aspects are not
explored in this work and would certainly be interesting for future work.
9
Under review as a conference paper at ICLR 2022
9	Ethics Statement
The authors declare that this work does not include any of the following: involvement of human
subjects, sensitive data, harmful insights, methodologies and applications. The results, data sets and
methodologies are objectively nondiscriminatory, unbiased and fair. This work does not breach any
privacy or security guidelines or laws, nor any other legal restrictions. The authors declare that there
are no conflicts of interest and/or external motivations through sponsorship.
10	Reproducibility S tatement
To ensure reproducibility of this work, the experimental code has been open sourced, together with
the proposed BlockStacks data set and rendering code;4 references for all other employed data sets
are provided in the main text. Hyperparameter configurations are specified in both Section 6 and
Appendix D and the experimental procedure is detailed in Section 6.
References
Ralph Abboud, IsmaiI IIkan Ceylan, Thomas Lukasiewicz, and Tommaso Salvatori.
Boxe: A box embedding model for knowledge base completion. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
6dbbe6abe5f14af882ff977fc3f35501- Abstract.html.
Masataro Asai. Photo-Realistic Blocksworld Dataset. arXiv preprint arXiv:1812.01818, 2018.
Samy Badreddine, Artur d’Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor
networks. CoRR, abs/2012.13635, 2020. URL https://arxiv.org/abs/2012.13635.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on PatternAnalysis andMachine Intelligence, 35(8):1798-1828,
2013. ISSN 01628828. doi: 10.1109/TPAMI.2013.50.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating Embeddings for Modeling Multi-relational Data. In C J C Burges, L Bottou,
M Welling, Z Ghahramani, and K Q Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 26: 27th Annual Conference on Neural Information Processing Systems, pp.
2787-2795. Curran Associates, Inc., Lake Tahoe, USA, 2013.
Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in β-VAE. In Advances in Neu-
ral Information Processing Systems 30, number Nips, Long Beach, CA, USA, 2017. URL
http://arxiv.org/abs/1804.03599.
Junxiang Chen and Kayhan Batmanghelich. Robust ordinal VAE: employing noisy pairwise compar-
isons for disentanglement. CoRR, abs/1910.05898, 2019. URL http://arxiv.org/abs/
1910.05898.
Junxiang Chen and Kayhan Batmanghelich. Weakly Supervised Disentanglement by Pairwise Sim-
ilarities. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, AAAI, New York,
NY, USA, 2020.
Ricky T Q Chen, Xuechen Li, Roger B. Grosse, and David Duvenaud. Isolating Sources of Disen-
tanglement in Variational Autoencoders. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems, pp. 2615—-2625, Montreal,
Quebec, Canada, 2018.
4Pending acceptance
10
Under review as a conference paper at ICLR 2022
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. In-
fogan: Interpretable representation learning by information maximizing generative adversarial
nets. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp.
2172-2180, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
7c9d0b1f96aebd7b5eca8c3edaa19ebb- Abstract.html.
Yuanfei Dai, Shiping Wang, Neal N Xiong, and Wenzhong Guo. A Survey on Knowledge Graph
Embedding: Approaches, Applications and Benchmarks. Electronics, 9(5):1-29, 2020. ISSN
20799292. doi: 10.3390/electronics9050750.
ArtUr d,Avila Garcez and Luis C. Lamb. Neurosymbolic AI: the 3rd wave. CoRR, abs/2012.05876,
2020. URL https://arxiv.org/abs/2012.05876.
Ivan Donadello, Luciano Serafini, and Artur d’Avila Garcez. Logic Tensor Networks for Seman-
tic Image Interpretation. In Proceedings of the Twenty-Sixth International Joint Conference on
Artificial Intelligence, pp. 1596—-1602, 2017.
Cian Eastwood and Christopher K I Williams. A framework for the quantitative evaluation of disen-
tangled representations. In 6th International Conference on Learning Representations, {ICLR},
Vancouver, BC, Canada, 2018.
Klaus Greff, Sjoerd van Steenkiste, and JUrgen Schmidhuber. On the binding problem in artificial
neural networks. CoRR, abs/2012.05208, 2020. URL https://arxiv.org/abs/2012.
05208.
Victor GUtierrez-BaSUIto and Steven Schockaert. From Knowledge Graph Embedding to Ontology
Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules.
2018. doi: 1805.10461. URL http://arxiv.org/abs/1805.10461.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a
Constrained Variational Framework. In 5th International Conference on Learning Representa-
tions, {ICLR}, Toulon, France, 2017.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a Definition of Disentangled Representations. arXiv preprint
arXiv:1812.02230, 2018. doi: arXiv:1812.02230v1. URL http://arxiv.org/abs/1812.
02230.
B. Inhelder and J. Piaget. The early growth of logic in the child: classification and seriation.
Routledge and Kegan Paul, London, 1964.
Theofanis Karaletsos, Serge Belongie, and Gunnar Ratsch. When crowds hold privileges: Bayesian
unsupervised representation learning with oracle constraints. In 4th International Conference on
Learning Representations, {ICLR}, pp. 1-16, San Juan, Puerto Rico, 2016.
Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs.
Advances in Neural Information Processing Systems, 2018-December(Nips):4284-4295, 2018.
ISSN 10495258.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the
2nd International Conference on Learning Representations, Banff, Alberta, Canada, 2014. ISBN
1312.6114v10. doi: 10.1051/0004-6361/201527329.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentan-
gled latent concepts from unlabeled observations. In 6th International Conference on Learning
Representations, {ICLR}, Vancouver, BC, Canada, 2018.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
Machines That Learn and Think Like People. Behavioral and Brain Sciences, 40, 2017. ISSN
14691825. doi: 10.1017/S0140525X16001837.
11
Under review as a conference paper at ICLR 2022
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation em-
beddings for knowledge graph completion. In Blai Bonet and Sven Koenig (eds.), Proceedings of
the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas,
USA, pp. 2181-2187. AAAI Press, 2015. URL http://www.aaai.org/ocs/index.
php/AAAI/AAAI15/paper/view/9571.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R{\”{a}}tsch, Sylvain Gelly, Bernhard
Sch{\”{o}}lkopf, and Olivier Bachem. Challenging Common Assumptions in the Unsupervised
Learning of Disentangled Representations. In Proceedings of the 36th International Conference
on Machine Learning,{ICML}, pp. 4114—-4124, Long Beach, California, USA, 2019.
Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael
Tschannen. Weakly-Supervised Disentanglement Without Compromises. CoRR, abs/2002.0,
2020.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on Machine Learning ICML, pp. 807-814,
2010.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33, 2016a. ISSN
00189219. doi: 10.1109/JPROC.2015.2483592.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. Holographic embeddings of knowl-
edge graphs. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the Thirti-
eth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA,
pp. 1955-1961. AAAI Press, 2016b. URL http://www.aaai.org/ocs/index.php/
AAAI/AAAI16/paper/view/12484.
Maxwell I. Nye, Michael Henry Tessler, Joshua B. Tenenbaum, and Brenden M. Lake. Improving
coherence and consistency in neural sequence models with dual-system, neuro-symbolic reason-
ing. CoRR, abs/2107.02794, 2021.
J. B. Paris. The Uncertain Reasoner’s Companion: A Mathematical Perspective. Cambridge Uni-
versity Press, 1994. ISBN 0-521-46089-1.
Jeffrey Paris and Alena Vencovska. Pure Inductive Logic. Perspectives in Logic. Cambridge Uni-
versity Press, 2015. doi: 10.1017/CBO9781107326194.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Jean Piaget. The Psychology of Intelligence. Routledge and Kegan Paul, 2005. ISBN 0521781604.
doi: 10.1093/acprof:oso/9780195150100.001.0001.
Ievgen Redko, Amaury Habrard, Emilie Morvant, Marc Sebban, and YoUneS Bennani. Advances in
Domain Adaptation Theory. Elsevier, 2019. ISBN 978-1-78548-236-6.
Karl Ridgeway and Michael C Mozer. Learning Deep Disentangled Embeddings With the F-Statistic
Loss. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems, pp. 185—-194, Montreal, Quebec, Canada, 2018.
Adam Santoro, Andrew K. Lampinen, Kory Mathewson, Timothy P. Lillicrap, and David Raposo.
Symbolic behaviour in artificial intelligence. CoRR, abs/2102.03406, 2021.
Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and
Max Welling. Modeling Relational Data with Graph Convolutional Networks. Lecture Notes
in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
Notes in Bioinformatics), 10843 LNCS(1):593-607, 2018. ISSN 16113349. doi: 10.1007/
978-3-319-93417-4.38.
12
Under review as a conference paper at ICLR 2022
Luciano Serafini and Artur D.Avila Garcez. Logic tensor networks: Deep learning and logical
reasoning from data and knowledge. In Proceedings of the 11th International Workshop on
Neural-Symbolic Learning and Reasoning (NeSy’16) co-located with the Joint Multi-Conference
on Human-Level Artificial Intelligence {(HLAI} 2016), New York, NY, USA, 2016.
Stewart Shapiro and Teresa Kouri Kissel. Classical Logic. In Edward N. Zalta (ed.), The Stan-
ford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2021
edition, 2021.
Richard Socher, Danqi Chen, Christopher Manning, Danqi Chen, and Andrew Ng. Reasoning With
Neural Tensor Networks for Knowledge Base Completion. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems, pp.
926-934, 2013.
Xander Steenbrugge, Sam Leroux, Tim Verbelen, and Bart Dhoedt. Improving Generalization
for Abstract Reasoning Tasks Using Disentangled Feature Representations. In Neural Informa-
tion Processing Systems (NeurIPS) Workshop on Relational Representation Learning, Montreal,
Canada, 2018. doi: http://arxiv.org/abs/1811.04784.
Theo Trouillon, Johannes WelbL Sebastian Riedel, Enc GaUssier, and GUillaUme Bouchard. Com-
plex Embeddings for Simple Link Prediction. In Proceedings of the 33nd International Con-
ference on Machine Learning, {ICML}, pp. 2071-2080, New York, NY, USA, 2016. ISBN
9781510829008.
Theo Trouillon, Enc GaUssier, Christopher R. Dance, and Guillaume Bouchard. On inductive abili-
ties of latent factor models for relational learning. Journal of Artificial Intelligence Research, 64:
21-53, 2019. ISSN 10769757. doi: 10.1613/jair.1.11305.
Sjoerd van Steenkiste, Francesco Locatello, JUrgen Schmidhuber, and Olivier Bachem. Are Disen-
tangled Representations Helpful for Abstract Visual Reasoning? In Advances in Neural Informa-
tion Processing Systems 32: Annual Conference on Neural Information Processing Systems, pp.
14222—-14235, Vancouver, BC, Canada, 2019.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of
approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):
2724—-2743, 2017. ISSN 10414347. doi: 10.1109/TKDE.2017.2754499.
A Related Work
Relational representations play a prominent role in Knowledge Graph Embedding, wherein sets
of relation-decoders are jointly learned in order to obtain a semantic latent representation for data
points (Socher et al., 2013; Trouillon et al., 2016; 2019; Bordes et al., 2013; Nickel et al., 2016a;
Wang et al., 2017; Dai et al., 2020; Kazemi & Poole, 2018; Abboud et al., 2020). Although these
typically do not use a shared autoencoder as we do in this paper, Schlichtkrull et al. (2018) did adopt
an autoencoding framework, where a graph neural network is used as the encoder, however they
did not work with visual data and the model was only applied to single data sets. Similarly, disen-
tanglement is also concerned with semantic representation learning (Bengio et al., 2013) , and has
been explored using a variety of methods including both Generative Adversarial Networks (Chen
et al., 2016) and VAEs (Burgess et al., 2017; Higgins et al., 2017; Chen et al., 2018; Ridgeway &
Mozer, 2018; Eastwood & Williams, 2018; Kumar et al., 2018; Locatello et al., 2019). Disentan-
gled representations have been evaluated in terms of there transferability in (van Steenkiste et al.,
2019; Steenbrugge et al., 2018; Locatello et al., 2020). A bridge between these two fields, wherein
relation-decoders are employed as a semi-supervision to VAEs can be found in (Karaletsos et al.,
2016; Chen & Batmanghelich, 2020; 2019), where (Karaletsos et al., 2016) use multiple relation-
decoders but compute a triplet comparison based query and (Chen & Batmanghelich, 2020; 2019)
only include a single binary relation and use function forms that are not sufficient to model the full
set of relations that we include in this work. Neither presents a comprehensive analysis of resulting
concept coherence. Lastly, we note that our experimental setup is most remnant of domain adapta-
tion (Redko et al., 2019). To the best of our knowledge, no work has compared relation-decoders in
their ability to learn coherent concepts, as measured by their consistency across domains.
13
Under review as a conference paper at ICLR 2022
Figure 4: Example of two BlockStacks data set images.
B BlockStacks dataset description
The BlockStacks dataset consists of 12,000 images (200×200 pixels but resized in code to 128 × 128)
of individual block stacks, of varying height (between 1-10 blocks), block colors (uniformly sampled
from options: { gray, blue, green, brown, purple, cyan, yellow}) and position (uniformly sampled
from x, y range (-3,-3) to (3,3)), but with the requirement that each instance consists of a single red
block at a random height (see Figure 4 for example images). These were rendered using the CLEVR
rendering agent with the help of code from (Asai, 2018). The dataset is divided into 9000:1500:1500
train, validation and test splits.
C EXPLANATION OF THE β-VAE
The VAE is derived by introducing an approximate posterior qa(Z|X), from which a lower bound
(commonly referred to as the Evidence LOwer Bound (ELBO)) on the true marginal logpθ(X)
can be obtained by using Jensen’s inequality (Kingma & Welling, 2014). The VAE maximises the
log-probability by maximising this lower bound, given by:
LβL盟=Eqa(z∣x)[logPΘ(X|Z)] - βDκL(qα(ZIX)kpθ(Z)),
(13)
where qa(Z|X) is typically modelled as a neural-network encoder with parameters a. Similarly
Pθ(X|Z) is often modelled as a neural-network decoder with parameters θ and is calculated as
a Monte Carlo estimation. A reparameterization trick is used to enable differentiation through an
otherwise undifferentiable sampling from qa(Z|X) (see (Kingma & Welling, 2014)). In the β-VAE
(Higgins et al., 2017; Burgess et al., 2017), an additional β scalar hyperparameter was added as it
was found to influence disentanglement through stronger distribution matching pressure with respect
to the prior pθ(Z), where this prior is typically set to an isotropic zero-mean Gaussian N(0, I)).
When β = 1 we obtain the standard VAE objective (Kingma & Welling, 2014).
D Model Descriptions
In this section we firstly present an in-depth analysis of the key innovations presented by DC which
provides insight into how it can learn a coherent notion of ordinality. We then provide model details
for each of the compared relation-decoders in the main results and the β-VAE architecture that we
employ for each data set.
14
Under review as a conference paper at ICLR 2022
Figure 5: Depiction of a set of DC relation-decoders for binary relations isGreater, isLess, isE-
qual, isSuccessor and isPredecessor. Each DC relation-decoder (for each relation) has a one-hot
mask, ur (that is in this example the same across relations), which ensures only the zeroth dimen-
sions of the embedding arguments are compared, giving zi,0 and zj,0 .
D.1 Dynamic Comparator Analysis
Figure 5 depicts how DC is able to learn the isGreater, isLess, isEqual, isSuccessor and is-
Predecessor family of binary ordinal relations, assuming each corresponding relation-decoder has
learned a common one-hot mask on the zeroth dimension i.e. uG = uE = . . . = uP = [1, . . . , 0],
such that activations only depend on the zi,0 - zi,1 difference. An important capability of DC is
its ability to select, Via ar an appropriate functional mode, either φ∖ or φr, depending on the type
of relation it needs to model. As shown by Figure 5, isEqual exhibits its reflexive, symmetric and
transitiVe characteristics, whilst isGreater and isLess both carry transitiVity but are asymmetric
and irreflexiVe. Furthermore, the use of a subtraction between zi and zj (which, Via mask u ends
up only being a subtraction between their zeroth dimensions) leads to a relatiVe comparison, not an
absolute comparison, which generalises to arbitrary zi and zj sampled from anywhere in Z .
Note that there is no built in parameter sharing, meaning each relation-decoder (for each indiVidual
relation r) is trained independently and has its own set of ar, Ur, ηr,0, ηr,1, br and br parameters.
HoweVer, our experiments show that DC reliably obtains settings such that e.g. uG = uE, or aG =
QL = [0,1], or bG = -b： and so on. DC is thus able to discover the interdependencies between
families of relations. By learning to indirectly ‘tie’ together parameters in this way, whilst still being
expressive enough to model each type of relation, DC can facilitate a data-driven binding between
relation-decoder outputs. This helps ensure consistent generalisation across a latent subspace, as
defined by the common/overlapped ur masks.
D.2 Relation-Decoder implementations
TransR (Lin et al., 2015):
φrTransR(zi,zj) = khr +r - trk22
with,
hr = Mrzi and tr = Mrzj .
where for zi, zj ∈ Rdz vectors, Mr ∈ Rdz ×dz and r ∈ Rdz. As we want to obtain a [0,1] output,
we modify TransR through φrTransR+ = σ(c- φrTransR), where σ is a sigmoid function and c is a scalar
that ensures that at φrTransR(zi, zj) = 0, then φrTransR+ (zi, zj) ≈ 1. In all experiments we set c = 10.
15
Under review as a conference paper at ICLR 2022
NTN (modified version of (Socher et al., 2013) from (Donadello et al., 2017; Serafini & Garcez,
2016)):
φr(zi,..., Zn) = σ(u> [tanh(zc>Mr Zc + Vr Zc + br )])
(14)
where Ur ∈ Rk, Mr ∈ Rn∙dz×ndz ×k, Vr ∈ Rk×n∙dz) and b ∈ Rk. The only hyperparameter to
consider is k, which controls the NTN’s capacity - in all experiments, we set this to 1. If k > 1,
Zc> MrZc produces a k-dimension vector by applying the bilinear operation to each of the k Mr
slices. Here Zc ∈ Rn∙dz is a concatenation of the inputs zι,..., zn which was introduced in
(Donadello et al., 2017; Serafini & Garcez, 2016). In contrast, the original NTN (see (Socher et al.,
2013)) is only applicable to binary relations and does not include the outer sigmoid.
HolE (Nickel et al., 2016b):
φrHolE(Zi,Zj) = σ(r> (Zi ?Zj))
where r ∈ Rdz and ? : Rdz × Rdz → Rd denotes the circular correlation operator and is given by,
d-1
[Zi?Zj]k =
zi,mzj,(k+m) mod d
m=0
NN: a simple four-layer neural-network with layer sizes lin = 2dz , l1 = 2dz and l2 = dz, with
ReLU activations (Nair & Hinton, 2010). The final output layer, lout, is a single value passed through
a sigmoid function, to bound the output within (0,1).
D.3 β-VAE CONFIGURATION
The model configurations used for both MNIST and BlockStacks data sets are given in Table 2.
D.4 Ljoint CONFIGURATION
In the source domain, we vary β values between {1, 4, 8, 12} and fix λ = 103. In the target domain,
we fix β to 10-4 and λ = 10-2 and normalise the LβE-LVABEO reconstruction term by dividing by
a factor √^W C, for height H, width W and color channels C, and normalize the distribution
matching term by a factor 六,for latent representation size dz.
To train relation-decoders over a given domain S, it is necessary to supervise estimates of
φr(ψSenc(O)), O ∈ S2, against corresponding ground-truth labels, γOr ,S . However, doing so for
every O ∈ S2 can easily become intractable and we instead only sample a subset of possible S2
tuples. Our sampling strategy involves first selecting a ratio R = |SB| where B ⊂ S2 is a set of O
tuples. We then sample relation-decoder specific subsets Br where ∣Br | = |B|, to ensure a balanced
distribution of tuples between relation-decoders. Furthermore, we ensure that each Br contains a
balanced ratio of γOr ,S = 1 versus γOr ,S = 0 instances. We found that each |Br| set can be small
without jeopardising the final relation-decoder performance level, allowing us to use R = 1 for
MNIST experiments and R = 3 for BlockStacks experiments.
Finally, in all experiments we use a β-VAE trained for up to 300,000 steps, following accepted
practice from (Locatello et al., 2019; Steenbrugge et al., 2018), together with any included relation-
decoders. However, to ensure computation efficiency across experiments, we employ an early stop-
ping procedure, where if the validation score does not increase over 30 and 120 training epochs for
MNIST and Blockstacks experiments, respectively, we end the training early.
E S upplementary Results
β effect on intrinsic relation-decoder characteristics: We have seen how β impacts PRT ac-
curacy but it is not clear how this is facilitated. To understand the way in which β affects each
relation-decoder we produce a gradient-conformity evaluation, based on the intuition that collections
of relation-decoder outputs will have to shift together in order to maintain consistency, facilitated
16
Under review as a conference paper at ICLR 2022
Table 2: Specification of our β-VAE encoder and decoder model parameters, for both 28×28 (top)
and 128×128 (bottom) size input data. I: Input channels, O: Output channels, K: Kernel size, S:
Stride, P: Padding, A: Activation
Encoder Input: 28 × 28 × NC = 1	Decoder Input: R10	
LayerID ;I;O;K;S;P;A Conv2d_1 ; NC ;32; 4 X 4 ;2;1; ReLU Conv2d_2 ;32;32; 4 X 4 ;2;1; ReLU Conv2d_3 ;32;64; 3 X 3 ;2;1; ReLU Conv2d_4 ;64;64; 2 X 2 ;2;1; ReLU	LayerJD ; Num Nodes : In - Out; A FCN ; 10 - 144 ; ReLU FC_z_mu ; 144 - 576 ; ReLU	
	LayerJD ;I;O;K;S;P;A UPConv2d_1 ;64;64; 2 X 2 ;2; UPConv2dN ;64;32; 3 X 3 ;2; UPConv2d_3 ;32;32; 4 X 4 ;2; UPConv2d_4 ;32; NC ; 4 X 4 ;2	1 ; ReLU
LayerJD ; Num Nodes : In - Out; A FCz ; 576 - 144 ; ReLU FC_z_mu ; 144 - 10 ; None FC_z」ogvar ; 144 - 10 ; None		1 ; ReLU 1 ; ReLU ; 1 ; Sigmoid
		
Encoder Input: 128 X 128 X NC = 3	Decoder InPut: R10	
LayerJD ;I;O;K;S;P;A Conv2d_1 ; NC ;32; 4 X 4 ;2;1; ReLU Conv2d_2 ;32;32; 4 X 4 ;2;1; ReLU Conv2d_3 ;32;64; 4 X 4 ;2;1; ReLU Conv2d_4 ;32;64; 4 X 4 ;2;1; ReLU Conv2d_5 ;64;64; 4 X 4 ;2;1; ReLU	LayerJD ; Num Nodes : In - Out; A FC_z ; 10 - 256 ; ReLU FC_z_mu ; 256 - 1024 ; ReLU	
	LayerJD ;I;O;K;S;P;A UPConv2d_1 ;64;64; 4 X 4 ;2; UPConv2d_2 ;64;32; 4 X 4 ;2; UPConv2d_3 ;32;32; 4 X 4 ;2; UPConv2d_4 ;32;32; 4 X 4 ;2; UPConv2d_5 ;32; NC ; 4 X 4 ;2	1 ; ReLU 1 ; ReLU
LayerJD ; Num Nodes : In - Out; A FCz ; 1024 - 256 ; ReLU FC_z_mu ; 256 - 10 ; None FC_z_logvar ; 256 - 10 ; None		1 ; ReLU 1 ; ReLU ; 1 ; Sigmoid
via a certain conformity in their gradients of input against output. For instance, suppose we have
xi, xj ∈ X such that φG(xi, xj) ≈ 0 and φE(xi, xj) ≈ 1. If we then take xi, xk ∈ X such
that φG(xi, xj) ≈ 1, then we must have φE(xi, xj) ≈ 0. In fact, any time φG outputs close 1,
we require φL and φE output close to 0, since only one of these three relations can be true for any
common arguments. At the decision boundaries, we require that their collective outputs conform
such that they are each modified appropriately, ensuring that they continue to satisfy any applicable
constraints. We therefore compute a gradient based analysis for arbitrary relation-decoder inputs
against the overall T belief state, which should be consistently ≈ 1, to see how much this varies as
β is increased. Gradient-conformity (GC) is calculated as:
GC
dτ dj
|田|2 ||dj ||2
where di
dφri
dzc
zc=znc
and dj
dφrj
dzc
cc
=zn
∀i 6= j	(15)
z
where znc is the concatenation of the nth sample of zi and zj from the latent space (and, more
specifically, a particular data split). Figure 6 presents source domain referenced GC measures for
each model, with the same data split schematic as in Figure 3. We see that for DC, GC is close to 1
for all β with no discernible change. All other models show a weaker GC with positive correlation
between GC and β . TransR and NN achieve significantly higher GC than NTN and HolE. It appears
that models that achieve a GC greater than 0.5 perform better at the overall PRT learning task.
17
Under review as a conference paper at ICLR 2022
GC for different data-splits referenced on source domain
Figure 6: GC values (higher values better), for each relation-decoder model referenced to source
domain. Darker color shades denote higher values of β, corresponding to greater disentanglement
pressure from the β-VAE. Blue, green and red groups show results for data-embeddings, interpola-
tion and extrapolation embeddings respectively (see main text for further details regarding the data
splits).
Figure 7: Analysis of domain-specific information retention by the β-VAE when using different
relation-decoders for ordinality relation decoding. We attempt to predict the overall BlockStacks
stack height on the final fixed embeddings obtained after isSuccessor relation-decoder alignment.
F How does each model impact the retention of
domain-dependent information
Figure 7 shows results for BlockStacks overall block height prediction accuracy when training on
fixed encodings of each block stack, after isSuccessor relation-decoder alignment as been applied
(using a pretrained fixed-parameter relation-decoder). Note β is fixed in the target domain, so the
only moving part are the choices of pretrained models which have been previously trained with var-
ied source β values. Note also that DC has an unfair advantage here, as the steered fitting approach
allows more flexibility to the VAE learning phase - for this reason the result is only included in the
appendix. Since we are interested in capturing general representations that encode both domain-
dependent and domain-independent information, we use each target encoder ψetnc obtained from
each PRT experiment and produce encodings for the full BlockStacks test set. The resulting en-
codings are then divided into a new train and test subset, used to train both a Sci-Kit Learn Linear
regressor and Support Vector Machine regressor with a RBF kernel (Pedregosa et al., 2011). We
present the resulting Mean Squared Errors (MSE) in Figure 7, with Ordinary Least Squares (OLS)
(a) and Support Vector Regression (SVR) (b).
There are a number of noteworthy details: firstly, DC shows no dependence on β and leads to
a lower MSE across all settings; second, excluding DC, for all models we observe an optimum
MSE at β = 8, with TransR reaching DC MSE performance for OLS and NN doing the same for
SVR. These results indicate that lower MSE can be obtained by using non-linear regression, which
indicates that to some degree, the block stack height factor is not encoded linearly, regardless of
selected model. Next, by contrasting with Figure 6, these results suggest that models with higher
GC lead to embeddings that are more amenable to domain-specific factor prediction. However,
the parabolic trend, where increasing β to 12 leads to an increase in error, is in agreement with
Figure 2-bottom-right, which showed that most models do not improve at PRT for the largest β.
This is perhaps due to a loss of mutual information between input and latent representation, as the
distribution matching loss outweighs reconstruction in the LEβL-VBAOE.
G Specification for theory of ordinality
18
Under review as a conference paper at ICLR 2022
To support our claim that we can use only the isSuccessor relation as the target encoder guide due
to its logical relationship the remaining relations, we include here the logical clauses:
∀i, j, k (isSuccessor(i, j) ∧ isSuccessor(k, j) → isEqual(i, k))
∀i, j (isSuccessor(i, j) → isGreater(i, j))
∀i, j, k (isSuccessor(i, j) ∧ isGreater(j, k) → isGreater(i, k))
∀i,j (isSuccessor(i,j) - isPredecessor(j,i))
∀i, j (isPredecessor(i, j) → isLess(i, j ))
∀i, j, k (isPredecessor(i, j) ∧ isLess(j, k) → isLess(i, k)).
Therefore, by knowing all of the successor relations between data instances, it should be possible to
infer the remaining relationships that they share.
For completeness, we provide the truth tables for each of the sub-theories that our consistency losses
evaluate against. We only include configurations that are valid under the constraints, indicated by
⊂ T = T, where this notation highlights the fact each incomplete set of constraints form a subset
of the overall theory T.
Firstly, the truth-table that describes constraints shared between relation truth-values is given by the
following, ∀i, j :
G(i,j)	E(i,j)	L(i,j)	S(i,j)	P(i,j)	I ⊂TI
-T^~	F	-^F^~	F	F	~^Γ~
T	F	F	T	F	T
F	T	F	F	F	T
F	F	T	F	F	T
F	F	T	F	T	T
where we use the same relation abbreviations as in the main text results.
Next, we provide each of the three consistency individual (Con-I) truth-tables. These are referred
to as being “individual” due to the fact that they describe constraints applied to the truth-state of a
single relation. For transitivity, given by the rule e.g. G(i, j) ∧ G(j, k) → G(i, k), we have that
∀i, j :
G(i,j)	G(j,k)	G(i, k)	I ⊂TI	
F	-^F^^	F	~^Γ~	
F	F	T	T	
T	F	F	T	(16)
T	F	T	T	
F	T	F	T	
F	T	T	T	
T	T	T	T	
For asymmetry, where S(i,j) → -S(j,i), we have ∀i,j:
S(i,j)	S(j,i)	I ⊂TI
-F^~	-^F-	~^Γ~
T	F	T
F	T	T
(17)
Finally, for reflexivity, given by E(i, i) → > (in this case describing that an object is always equal
to itself) we have ∀i:
E(i,i)	⊂T
F T
(18)
Truth-table matrices for each of the above truth-tables can be obtained by replacing T with 1 and F
with 0. We provide the full set of individual constraints that are applicable to each relation covered
in this paper are given by Table 3.
19
Under review as a conference paper at ICLR 2022
Table 3: Characteristic properties of ordinal relations.
Relation ∣ asymmetric		transitive	reflexive
G	Y	Y	N
E	N	Y	Y
L	Y	Y	N
S	Y	N	N
P	Y	N	N
H	EXPANDED -PROXY DERIVATION
In this section, We present the expandedjustification for reporting - ln e consistency and coherence
as a proxy for -consistency/coherence as defined in Section 3. For notational clarity, in the following
We omit ψS, such that φr(ψS (O)) is abbreviated to φr(O).
In the folloWing, We make no assumptions about the sizes of domain S, signature σ and arities of
each r ∈ σ. Further, We take T to be an arbitrary theory over σ consisting of universally quantified
formula, and the validity of each ground instances of atomic formula With respect to T, can be
expressed by a single ground truth-table matrix, T ∈ {0, 1}K0×K1 ×K2, Wherein each slice, Tk,:,:
gives a unique grounding of domain objects to the variables, v, required by T. For each grounding
of the K0 = |S ||v| possible groundings, there are K1 = 2l unique truth-assignments to the l atomic
formulae that constitute T, giving K2 = l + 1 assignments per Tk,t,: roW - one per atomic formulae
and an additional value to denotes Whether the particular roW satisfies T. T can be obtained by
taking any truth-table from the previous section and sWitching true (T) for 1 and false (F) for 0, and
producing K0 copies for each assignment of domain elements to the variables. Given this truth-table
matrix, notice that a structure Sσ can be composed by selecting a single roW ofT for each grounding
(kth slice), giving a vector ckt = Tk,t,1:l. If the structure is a model of T, i.e. Sσ ∈ MTS , then only
roWs With Tk,t,K2 = 1 are alloWed. Taking t+ to be the set of roWs such that Tk,t,K2 = 1 (Which is
3
identical for each k) for t ∈ t+, We can then reWrite ΓTSσ in terms of samples from T:
rr
ΓTSσ =	φr(O)γO,Sσ (1 - φr(O))1-γO,Sσ	(Eqn. 3)
Sσ ∈MTS r∈σ O∈Sar(r)
K0	l
= X Y X 1t0k,S (t) Y f(φrm, Okm, cktm)N(φrm,Okm,cktm,Sσ)-1	(19)
Sσ ∈MTS k=1 t∈t+	m=1
With
f (φrm , Okm, cktm) = φrm (Okm)cktm (1 - φrm (Okm)) -cktm .	(20)
In the above, 1t0	(t) is an indicator function Which equals 1 if t = t0k,S and 0 otherWise, for
active roW t0k,S under structure Sσ and grounding k. 1t0	(t) has the role of only including the
single summand Where t corresponds With t0k,S . N(φrm, Okm, cktm, Sσ) is a function that counts
the number of repeat products of term f (φrm , Okm, cktm), such that the appropriate root can be
applied. We use rm to denote the relation for atomic formula at column m and Okm its correspond-
ing arguments, under grounding k; and We use cktm to denote the truth-assignment of the atomic
formula for column m, as designated by roW t.
3
At this point, We are left With an expression for ΓTSσ in terms of truth-table matrix T entries, Which
is more reminiscent of L(T, Sσ) as defined in Section 4. HoWever, We must go further to expose
3
the relationship between ΓT and L(T, S0) for arbitrary T expressed by T. We Will now show that
the consistency loss L(T, Sσ) gives the negative log-likelihood of satisfying T given a grounding
3
k ∈ {1, . . . , K0}, which can be further seen as a relaxation of ΓTSσ to sum over all rows t ∈ t+ and
without normalising via the N(φrm, Okm, cktm, Sσ)-1 exponent. With Boolean random variable
BT denoting whether T is (bT = 1) or is not (bT = 0) satisfied, the consistency loss for a soft-
structure Sσ against theory T is given by,
L(T, Sσ) = Ek〜U[{i,...,Ko}][H(p(BτS,k),P(BTS,k))]	Eqn. 7 base
20
Under review as a conference paper at ICLR 2022
which can be expanded to,
K0	1
L(T, Sσ) = - X	P(bT = 1lSσ,k)lnP(bT = 1lSσ,k)	QI)
k=1 K0
， ，- ・--、、- ，- . ~
+ (1 - p(bτ = 1∣Sσ ,k))ln1 - p(bτ = 1∣Sσ ,k).
where Sσ ∈ MT. Given Sσ ∈ MT, then p(bτ = 1∣Sσ,k) = 1 always holds, which means the
negative case in Eqn. 21 can be ignored, yielding the following simplified form:
K0
1
L(T, Sσ ) = -E k lnP(bT = 1lSσ, k)
k=1 K0
=-Ek〜U[1,...,Ko][lnP(bT = 1lSσ, k)].	Eqn. 7
and so L(T, Sσ) is simply the negative log-likelihood of sampling a satisfied theory (bT = 1)
from soft-structure Sτ, for randomly sampled grounding k. Next, we show the similarities between
一~ 一~
SS
L(T, Sσ) and Γ^σ by looking at the likelihood p(bτ = 1∣Sσ, k). First, we define Γ^σ by isolating
the likelihood:
K0
exp(-L(T,Sσ)) = Y p(bτ = 1&,k) K0
k=1
二C
=Γ T	(22)
We then expand p(bτ = 1|S ,k) to:
K1
p(bτ = 1∣Sσ, k) = Xp(bτ = 1∣Ckt)p(ct∣Sσ, k)
t=1
=X P(Ckt Bσ,k)	(23)
t∈t+
where t+ is defined as before. For all other t 6= t+, p(bT = 1|ckt) = 0 and so this acts as a filter,
yielding:
K0
Γ Tσ = YXp(ckt∣Sσ,k) K0.	(24)
k=1 t∈t+
p(ckt∣Sσ, k) is calculated by evaluating the belief of each relation-decoder against the expected
truth-assignment as defined by truth-table row ckt:
l
p(Ckt∣S^σ ,k) = Y Φrm (Okm)Cktm(1 -。产(Okm))「cktm
m=1
= f(φrm, Okm, cktm)
where rm is the relation for atomic formula associated with column m (which is the same for each k
slice and t row) and Okm is the grounding of this entry for slice k (which is the same across rows).
Putting it all back together, we finally have that:
K0	l
ΓT = YXY f(φrm ,Okm,Cktm) K0 ,	(25)
k=1 t∈t+ m=1
3	f
which makes the similarities between ΓTσ and ΓT clear and exposes their relationship.In partic-
3
ular, for the special case where |MTS | = 1, the outer sum for ΓTSσ can be removed, and the re-
33
maming differences between ΓT and ΓT are the sum over t+ rows and difference in exponent
3
over f (φrm,Okm, Cktm). For ΓTσ to be maximised, through p(Sσ ∣Sσ) ≈ 1, we would find that
21
Under review as a conference paper at ICLR 2022
W
Sσ maximally supports only the rows associated With 5。for each k grounding. Notice that ΓTσ Is
WW
again bound to (0,1) and achieves ΓSσ ≈ 1 when ΓSσ ≈ 1. We use the correspondence between
WW
ΓSσ and ΓSσ to define a practical e-proxy consistency measure as follows. We firstly re-express
~
S
e-consistency/coherence but for ΓSσ and a different e. We then trace this back to L(T, S。) so a
bound in terms of the consistency loss can be reported as the overall e-proxy. Together this yields
the following:
二C
e ≥ 1 — Γ Sσ
T
1W
ln i--∑ ≥ - ln(ΓSσ)
_ , 一 ~ .
≥ L(T, Sσ)	(26)
and we arrive at an e-proxy of the form ln 亡,which is reported in the main text.
22