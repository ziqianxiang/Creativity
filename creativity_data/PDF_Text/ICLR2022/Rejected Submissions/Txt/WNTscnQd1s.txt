Sparsistent Model Discovery
Anonymous authors
Paper under double-blind review
Ab stract
Discovering the partial differential equations underlying spatio-temporal datasets
from very limited and highly noisy observations is of paramount interest in many
scientific fields. However, it remains an open question to know when model dis-
covery algorithms based on sparse regression can actually recover the underlying
physical processes. In this work, we show the design matrices used to infer the
equations by sparse regression can violate the irrepresentability condition (IRC)
of the Lasso, even when derived from analytical PDE solutions (i.e. without addi-
tional noise). Sparse regression techniques which can recover the true underlying
model under violated IRC conditions are therefore required, leading to the intro-
duction of the randomised adaptive Lasso. We show once the latter is integrated
within the deep learning model discovery framework DeepMod1, a wide variety
of nonlinear and chaotic canonical PDEs can be recovered: (1) up to O(2) higher
noise-to-sample ratios than state-of-the-art algorithms, (2) with a single set of hy-
perparameters, which paves the road towards truly automated model discovery.
1	Introduction
Mathematical models are central in modelling complex dynamical processes such as climate change,
the spread of an epidemic or in designing aircrafts. To derive such models, conservation laws, phys-
ical principles and phenomenological behaviors are key. However, some systems are too complex to
model with a purely bottom up approach. In such situations, and when observational data is present,
automated model discovery tools are becoming increasingly more useful to derive partial differential
equations (PDEs) directly from the data. For example, to model the ocean dynamics in Sanchez-Pi
et al. (2020) and for embryo patterning in Maddu et al. (2020). From the mathematical point of
view, model discovery of PDEs consists in finding F such that,
ut = F(1, u, ux, uxx, ...),
where ut is the temporal derivative of the field u and u, ux, uxx, ... are higher order spatial deriva-
tives. Usually F is identified based on an experiment consisting of n samples of the field u, see
Brunton et al. (2016); Rudy et al. (2017); Schaeffer (2017); Raissi et al. (2017). Some recent ap-
proaches use symbolic regression to find F, see Maslyaev et al. (2019), but so far the most popular
approach to perform model discovery is by linear regression which was first introduced in Rudy
et al. (2017) and consists in considering F as a linear combination of some candidate terms,
ut = θ ∙ ξ,
where each column in Θ is a candidate term for the underlying equation, typically a combination
of polynomial and spatial derivative functions (e.g. u, ux, uux). In order to obtain a parsimonious
PDE, many of the coefficients ξ must be zero, motivating the use of a sparse regression method to
infer the equation,
£= argξmin ∣∣ut - Θ ∙ ξ∣∣2 + λXX∣∣ξikρ.
The best subset selection is obtained for ρ = 0, Hastie et al. (2015). However it requires solving a
nonconvex and combinatorial optimisation problem. The Lasso, Tibshirani (1996), is special in the
sense that ρ = 1 is the smallest value of ρ which leads to a convex constraint region and hence a
1Data, code and results shared on: https://anonymous.4open.science/r/sparsistent_
model_disco-56F8/
1
convex optimisation problem - for which very efficient solvers exist. Furthermore, there is a large
corpus of theoretical work available for the Lasso from which the model discovery community could
benefit. It is anecdotaly known in the model discovery community that the Lasso does not perform
very well compared to other relaxations of the original problem, see Rudy et al. (2017), Li et al.
(2019), Rudy et al. (2019) and Maddu et al. (2019), however it has never been studied why. We
trace back this lack of performance to the potential variable selection inconsistency of the Lasso.
Let us focus on variable selection consistency or sparsistency, a property defined as: when the
ʌ
number of samples n -→ ∞, the estimated vector ξ contains the same nonzero terms as the true vec-
tor ξ. By first revisiting a geometric interpretation of the irrepresentability condition (IRC) of the
Lasso in the context of model discovery, we provide some theoretical insights on Lasso’s inconsis-
tency but also on the design of the libraries generated for sparse regression based model discovery.
In addition, these libraries have to be estimated using methods that introduce some deterministic
noise into the sparse regression problem. We show when and why the adaptive Lasso Zou (2006)
design matrices might have more chances of satisfying the IRC and introduce the randomised adap-
tive Lasso to perform variable selection under violated IRC. The latter is tuned by stability selection
MeinshaUsen & Buhlmann (2010), which has been used in the past for model discovery in pure
sparse regression based approaches Li et al. (2019); Maddu et al. (2019) but without any variable
selection error control.
Furthermore, purely relying on sparse regression such as Rudy et al. (2017), Li et al. (2019) and
Maddu et al. (2019) heavily limits to low noise and dense data sets, due to the differentiation method
used to build the library (typically numerical differentiation or splines). Purely relying on deep learn-
ing, see Raissi et al. (2017), to discover F will result in hardly interpretable equations. In Both et al.
(2021a) and Chen et al. (2020), the two problems are tackled by concurrently learning a solution of
the PDE using a physics informed neural network and inferring an explicit equation by performing
sparse regression on a library built by automatic differentiation. However, deep learning model dis-
covery frameworks typically require manually tuning many hyperparameters which are sensitive to
the input data: (1) the original DeepMod Both et al. (2021b) requires to tune a threshold to prune
coefficients with small magnitudes, (2) PiDL Chen et al. (2020) introduces a couple of multipliers
to parameter the amount of physics informed regularisation and the amount of regularisation in the
sparsity estimator. Our results show how once the randomised adaptive Lasso with stability selec-
tion is integrated within the deep learning model discovery framework DeepMod, a single set of
hyperparameters can be used to recover a wide variety of PDEs.
Contributions
•	We show the design matrices used to infer the equations by sparse regression can violate
the irrepresentability condition (IRC) of the Lasso, even when derived from analytical PDE
solutions, i.e. without additional noise. This implies any sparse regression based model
discovery framework needs to deal with highly correlated irrelevant variables with relevant
ones, no matter the differentiation method used to compute the library.
•	We introduce a randomised adaptive Lasso (rAdaLasso) with stability selection and error
control algorithm, to recover the true underlying PDE in the presence of design matrices
that are highly correlated and violate the IRC.
•	By integrating rAdaLasso within the deep learning model discovery framework DeepMod,
we show a wide variety of nonlinear and chaotic canonical PDEs can be recovered: (1)
at higher noise-to-sample ratios than state-of-the-art algorithms, (2) with a single set of
hyperparameters, paving the road towards truly automated model discovery.
2	Theory
Sparse regression based model discovery sometimes fails to discover the correct underlying PDE
from a data set, even when the model is present in the library and contains little noise. To illustrate
this, we present a two-soliton solution2 of the Korteweg-de-Vries (KdV) equation in figure 1(a).
Considering a library Θ ofp =12 terms, the Lasso fails to select the correct terms of the underlying
2Obtained from an analytical solution, see details in Appendix D.
2
(a) a noiseless library
Figure 1: Example of PDE term selection inconsistency using the Lasso - from a noiseless library of
a two-soliton analytical solution of the Korteweg-de-Vries (KdV) equation: ut = -6uux - uxxx. In
(a) the terms ux and uux are highly correlated. In (b) no matter the regularisation, the Lasso selects
the spurious term ux . On the right hand side, with a proper choice of λ, the adaptive Lasso might
select the true model.
(b) coefficient paths
PDE, see figure 1(b), even in the absence of noise. In this section, we explain why it occurs and
introduce the irrepresentable condition to identify the cause.
2.1	On Lasso’ s inconsistency
The Lasso is known to be variable selection consistent under the ir-
representable condition (IRC), see3 Hastie et al. (2015), which re-
quires the existence of η > 0 such that,
maax ιι(θT θT )-1θT θF,j 111 < 1-n,
(1)
where ΘT is the subset of the design matrix Θ that contains the true
model and ΘF,j a column of the subset of the design matrix that con-
tains the rest of vectors. Conversely, the Lasso will not consistently
select the true variables for any design matrix, even if they are present
in the library.
Geometric interpretation it can be remarked that
(ΘTT ΘT )-1 ΘTT ΘF,j is the least squares solution of,
ΘF,j = ΘTξ + ,	(2)
with Gaussian noise, see figure 2(a). The IRC will be violated if
any of the projections of ΘF,j onto the column space of ΘT is larger
than 1, meaning they lie outside the unit sphere see figure 2(b). The
IRC will be trivially satisfied when ΘF,j is orthogonal to the column
space of ΘT, in which case η = 1: meaning irrelevant vectors are
orthogonal to the relevant vectors Hastie et al. (2015). Furthermore,
it becomes clear from the example on figure 2, that if the relevant
vectors are orthogonal to each other then the projections of irrelevant
terms onto the space of relevant vectors will be exactly the corre-
lations and the IRC will be less likely to be violated. While it is
obvious the IRC will more likely be violated in the case the data is
highly correlated: correlations among irrelevant vectors will not lead
to a violation of the IRC.
(a)
relevant
vectors space
(b)
Figure 2: Example of projec-
tion that lies outside the unit
sphere: the IRC is violated.
3Many formulations of the IRC can be found in literature such as the seminal work from Zhao & Yu (2006)
and MeinshaUsen & Buhlmann (2010) - We do not pretend to be exhaustive on the matter but rather choose the
one that better suits our purpose.
3
A diagnostics metric ∆ based on the IRC we introduce,
δ" )=ιmax ll(θT θT )-1θT θF,j ll1,
(3)
where T are the vectors of the support set S and F on its complementary set Sc . In practice, the true
ʌ
support S is unknown. Its estimation S can be obtained for example by taking for granted the result
ʌ
from a first variable selection. ∆(Θ, T) can help us determining if our library Θ is sufficiently well
ʌ ʌ
designed: if ∆ < 1 we know a Lasso can distinguish S from Sc, otherwise the variable selection
result should be taken with caution. It is worth pointing out, that by no means if ∆(Θ, T) < 1 we
can claim T = T.
2.2	On the adaptive Lasso’ s sparsistency
We have seen in the previous section that the Lasso might not be variable selection consistent when
∆(Θ, T) > 1, meaning that even if the true model is present in the library it might not be selected.
Instead, we propose to use the adaptive Lasso which is known to preserve the consistency, see Zou
(2006). In figure 1 we illustrate that applying it, results in the correct underlying KdV equation to
be recovered. Let us give insights of why it might perform better on model discovery problems. The
adaptive Lasso is a two-step estimation procedure where first an initial estimation of the coefficients
is obtained to derive a weight vector Wi = 1∕∣ξi∣γ. We fix Y = 2 throughout this work. ξi is
preferably obtained using a Ridge regression to handle multicollinearity. In the second step, a Lasso
is applied on the weighted coefficient, W%, penalising the terms with their respective weights, i.e.,
f=argmin (ɪ∣∣∂tu - Θf∣∣2 + λX ∣∣ξi∣∣ι),	(4)
ξ	v2n	i=ι	/
∙-v	∙-v
where Θi = Θi∕W% and ξi = Wiξ%. Let US consider the impact of the transformation on the design
matrix by the adaptive weights.
Proposition 1. by assuming all relevant coefficients are larger than the irrelevant ones in magnitude
and γ ≥ 1, then as a result of the transformation, the projection of the j th irrelevant vector onto the
k h relevant vector Will shrink by afactor ∣ξF,j∕ξτ,k ∣γ and,
∙-v
∆(Θ,T) ≤ ∆(Θ,T).	(5)
∙-v
The design matrix Θ will therefore have more chances to verify the IRC than Θ.
See proof on appendix A. By coming back to the example presented on figure 1, ∆(Θ, T) = 1.69
while ∆(Θ, T) = 2e-14, which illustrates inequality 5. It is worth insisting the correlation matrices
∙-v
of Θ and Θ are identical. However, the projection of the irrelevant vector ux into the space of
relevant vectors (uux, uxx) becomes very small, and small enough in this case for the IRC not to be
∙-v
violated anymore (by replacing Θ with Θ). This gives insights why the adaptive Lasso manages to
identify the true underlying model when the Lasso would not.
2.3	What happens with more realistic libraries
In practice, the design matrix Θ cannot be observed, nor measured, nor derived from analytical
solutions, as we did on figure 1, and has to be estimated. In classic model discovery the library
Θ is typically built using splines and/or numerical differentiation. In neural network based model
discovery, there will also be a non-random misfit between the output of the neural network and
the data. As a result, these methods introduce non-random approximations of the higher order
derivatives which can be seen as additional deterministic noise δ on top of the ground truth,
Ut = (Θ + δ) ∙ ξ	(6)
We see in practice that if the correlations introduced by δ are not too large, the true PDE can still be
discovered by choosing the correct amount of regularisation for the randomised adaptive Lasso. To
see the effect of the interpolation and differentiation methods, we compute ∆ from 2 libraries based
on a numerical solution of the chaotic Kuramoto-Sivashinsky (KS) equation (see Rudy et al. (2017))
4
AdaLasSo
≈∙qo.Id uo-ɔ-əs
10-8
λ
10-6
randomised AdaLasso
10-8	10-6
λ
rAdaLasso + error control
0 8 6 4 2 0
------
Iooooo
∙qo.Id uoɔəs
10-7
λ
10-9
(a) stability plots
(b) selection error control
Figure 3: Randomising the adaptive Lasso to perform model selection under violated IRC. The
library comes from Rudy et al. (2017), and was obtained by polynomial interpolation and numerical
differentiation from a numerical solution of the Kuramoto-Sivashinksy equation (ut = -uux -
uxx - uxxxx) with additional 1% Gaussian white noise. Even Θ violates the IRC: ∆(Θ, T) = 1.77.
In (a) only the randomised adaptive Lasso allows to disentangle relevant from irrelevant PDE terms.
In (b), contours represent the upper bound on the selection error. The relevant PDE terms can be
found by the rAdaLasso by a proper choice of the maximum number of expected false positives
EVmax = 2 (at fixed minimum probability of being selected πthr = 0.9).
with varying noise levels. Both libraries contain with p = 36 potential terms and are derived by poly-
nomial interpolations and numerical differentiation. When no noise is added, ∆(Θ0% , T) = 1.33
and ∆(Θo%,T) = 7e-3 meaning the adaptive weights used by the adaptive Lasso help Cast-
ing better the design matrix. However as soon as 1% noise is added, ∆(Θ1%, T) = 1.38 and
∆(Θ1%, T) = 1.77: the Lasso nor the adaptive Lasso would be able to select the true model. In
this case, the interpolation and differentiation methods introduce enough correlations for any design
matrix to violate the IRC. On figure 3, the stability plots show the adaptive Lasso would not be able
to select the true model even if present in the library, no matter the amount of regularisation; this
motivates the introduction of a supplementary ingredient.
2.4	Randomised adaptive Lasso (rAdaLasso)
To work under violated IRC due to correlations that are due to the underlying physical process
itself and/or the interpolation and/or the differentiation method, we introduce a randomised adaptive
Lasso. It is inspired by the random LaSSo presented in Meinshausen & Buhlmann (2010),
p
ξ = argmin (2n ∣∣dtu - θ ξ∣∣2 + λ X -W-1)	⑺
ξ	i=1	i
where Wi is randomly selected from a beta distribution, W 〜β(1,2) to promote weights close to 0.
Such randomisation of the regularisation is equivalent to a random rescaling of each column of the
design matrix followed by an adaptive Lasso, it is therefore straightforward to solve4. Empirically,
we can see the randomisation breaks the correlations and allows to disentangle the group of relevant
from the group of irrelevant variables in a stability selection loop: see the stability plots without and
with randomisation on figure 3.
Controlling the selection error The determination of λ can be done using stability selection, see
Meinshausen & Buhlmann (2010), which has been used in the past for model discovery Li et al.
(2019); Maddu et al. (2019). However, error control has not been used and is in our opinion under-
utilised: as we will see in the experimental results its hyperparameters can become data insensitive.
For the sake of completeness we revisit the details of stability selection on Appendix B and recall
4It is (slightly) more computationally expensive than the adaptive Lasso with stability selection as the ran-
domness prevents from the benefit of warm starts.
5
main results in the following lines. The set of variables to be selected SsΛtable is determined using
two hyperparameters: πthr is the minimum probability of being selected and an upper bound on the
expected number of false positives EVmax ,
S$abie = {k such that maxΠΠk ≥ ∏thr for λ ∈ Λ*0	(8)
where the regularisation path is restricted by an upper bound,
Λ* = lλ ∈ Λ such that, E(V) ≤。次 ∣、 ≤ EVmax)	(9)
(2πthr - 1)p
where qΛ is the average of selected variables. On figure 3(b), the upper bounds on the selec-
tion errors are represented using contours on top of the stability plot. The set of variables to be
selected S*ie is given by the terms inside a given contour line while being above the threshold ∏th.
To conclude this section, in the context of model discovery, sparse regression is usually performed
on highly correlated data, due to the data itself and/or to the differentiation method used to estimate
the library, which will tend to violate the IRC. This means that even if the true model is present in the
library it might not be selected by the Lasso. As a mitigation, we introduce a randomised adaptive
Lasso and show once in a stability selection loop with error control, the underlying true model can
still be recovered.
2.5	DeepMod integration
Neural network based model discovery improves the quality of the library with respect to numerical
differentiation based methods, see Both et al. (2021b) . We can therefore expect the deterministic
noise δ to be much smaller. To leverage such capability, we implement the randomised adaptive
Lasso with stability selection and error control in the deep learning model discovery framework
DeepMod5, Both & Kusters (2020). The framework combines a function approximator of u, typi-
cally a deep neural network which is trained with the following loss,
L = 1 l∣u - u∣l2 + 1 l∣∂tU - Θ(ξ ∙ M)||2
n}
L mse
(10)
}
The first term Lmse learns the data mapping (x, t) → u, while the second term Lreg constrains the
ʌ
function approximator to solutions of the partial differential equation given by ∂tu, Θ and (ξ ∙ M).
The terms to be selected in the PDEs are determined using a mask M derived from the result of the
randomised adaptive Lasso with stability selection and error control,
Mi = [ 1 ifξi ∈ Sstable
0 otherwise
(11)
where i ∈ [1,p] is the index of a potential term and SΛable is determined by equation (8). The
ʌ
coefficients ξ in front of the potential terms are computed using a Ridge regression on the masked
library (Θ ∙ M). During training, if Lmse on the test set does not vary anymore or if it increases,
the sparsity estimator is triggered periodically. As a result, the PDE terms are selected iteratively by
the dynamic udpate of the mask M during the training. In practice, this promotes the discovery of
parsimonious PDEs.
3	Experiments
In this section, we first show how the randomised adaptive Lasso compares with state-of-the-art
sparsity estimators. Second, once within DeepMod, we compare it to the original DeepMod frame-
work.
5The randomised adaptive Lasso promoted here, uses the Ridge and Lasso implementations from scikit-
learn, Pedregosa et al. (2011). DeepMod is implemented in JAX, Bradbury et al. (2018)
6
Table 1: Known challenging cases from literature. When polynomial interpolation is used to com-
pute higher order derivatives from noisy data, it is known that the quality of the library is going to be
poor - making it challenging to discover the underlying PDE by sparse regression. For both libraries
∆ > 1 revealing the Lasso would not be able to recover the true support. *KS: Kuramoto-Sivashinsky.
#	PDE	Noise	Terms	Deriv. Order	n	source	∆
1	KS*	1%	36	5	250k	Rudy et al. (2017)	1.38
2	Burgers	4%	19	4	20k	Maddu et al. (2019)	1.23
Table 2: Success in recovering the ground truth PDE terms for table 1 cases. Here we reproduced
the results from Rudy et al. (2017), Maddu et al. (2019) (h stands for heuristic) and report an addi-
tional results using the Lasso, adaptive Lasso and randomised adaptive Lasso. In case 1, PDE-FIND
does find the correct terms, while it does not in case 2. In the latter, PDE-STRIDE and a randomised
adaptive Lasso do, see figure 5.
regularisation Case 1 Case 2
Lasso	l1	X	X
randomised Lasso	l1	-	X
PDE-FIND (STRidge)	h	✓	X
PDE-STRIDE (IHT)	l0	-	✓
adaptive Lasso	l1	X	X
randomised adaptive Lasso	l1	✓	✓
Comparing with state-of-the art sparsity estimators In order to get an idea of the performance
of the randomised adaptive Lasso with stability selection and error control, we compare it to two
pure sparse regression based model discovery approaches: PDE-FIND Rudy et al. (2017) and PDE-
STRIDE Maddu et al. (2019). While the first is a heuristic, the latter solves a relaxation of the best
subset selection (l0 regularisation) using an Iterative Hard Thresholding algorithm. To make sure the
comparison is fair, we compare our approach with the ones from literature using the data from the
original authors of those approaches. Furthermore, we restrict ourselves to cases where the original
authors have tuned their algorithms and present the cases as being hard ones, see table 1. In these
cases, ∆(Θ, T) > 1, meaning they violate the IRC, see table 1. The results from the benchmark are
∙-v	∙-v
presented in table 2. For case 1, ∆(Θ, T ) ≈ 1.77 and for case 2, ∆(Θ, T) ≈ 19 explaining why
the adaptive Lasso alone will not work in those cases. The result for case 1 is presented on figure
3. From figure 56, with proper tuning both the randomised adaptive Lasso as well as the Iterative
Hard Thresholding (IHT) algorithm can recover the true underlying PDE of case 2. However, the
computational cost of the IHT is much higher (×100) than the one of the randomised adaptive Lasso
(rAdaLasso), which solves a convex optimisation problem.
Impact of rAdaLasso in DeepMod To quantify the impact of the proposed sparsity estimator
within DeepMod we compare DeepMod with rAdaLasso and a baseline (the original DeepMod).
The latter leverages a thresholded Lasso with a preset threshold of 0.1 (to cut-off small terms) and
λ found by cross validation on 5 folds. We simulate model discoveries for the Burgers, Kuramoto-
Sivashinsky (KS) and two additional PDEs that introduce different nonlinearities and derivative
orders: Kortweg-de-Vries (KdV), ut = -6uux -uxxx and Newell-Whitehead (NW), ut = 10uxx +
u(1 - u2) - 0.4. A single set of hyperparameters is used in all cases see Appendix C. The results
are reported on figure 47 . Our approach allows to recover all 4 PDEs without overfitting while the
original DeepMod would for all, except for the KdV equation. The stability plot obtained on figure
4(b) for the KS equation can be compared to the one presented on figure 3(b): the combination
6The computational cost reported here is obtained by running the code with both the data and hyperparam-
eters from the authors of the original work.
7In terms of computational cost, an epoch takes in average around 0.04s (with 2k samples) on a GeForce
RTX 2070 GPU from NVIDIA: discovering the KS equation takes around 90k epochs and around 1 hour.
7
102	103	104
epoch
102	103
epoch
104
103
epoch
105	103	105
epoch
(a)	MSE on test set, recovered PDEs and term selection error during learning
(U ∙qojd Uo-ɔ-os
10-6	10-4	10-2
λ
10-6	10-4	10-2
λ
10-6	10-4	10-2
λ
10-6	10-4	10-2
λ
(b)	rAdaLasso stability plots after DeepMod converged
Figure 4: With and without rAdaLasso within DeepMod. In (a), all true underlying equations are
recovered when using the proposed rAdaLasso within DeepMod , from n = 2k samples for varying
α% Gaussian white noise levels. The original DeepMod leverages a thresholded LassoCV sparsity
estimator which selects spurious terms (except for the KdV example) and results in poorly general-
isable PDEs (the MSE on the test set increases). In (b), the stability plots show the selected terms
for all the examples have become independent (after DeepMod has converged) from the hyperpa-
rameters πthr and EVmax .
of rAdaLasso and DeepMod allow to recover the chaotic equation with greater confidence as the
probability of selecting irrelevant terms is null.
Benchmarking noise-to-sample ratios We compare the ratios Ψ = α∕n where α is the GaUssian
white noise expressed in percent of successful model discoveries of additional frameworks for the
two most investigated cases in literatUre: BUrgers and KS eqUations. The compared frameworks
are PiDL(deep learning based thats Uses the sparsity estimator of PDE-FIND, Chen et al. (2020)),
S3d (sparse bayesian learning, YUan et al. (2019)), SNAPE (basis fUnction approximations based,
Bhowmick & Nagarajaiah (2021)) and R-DLGA (symbolic regression based, XU & Zhang (2021)).
The resUlts can be foUnd on table 3. Symbolic regression based approaches sUch as R-DLGA are
more general than sparse regression based approaches in the sense they do need a predefined library,
bUt do not perform as well. For the BUrgers eqUation, the approach proposed in this work can
perform the discovery at the same noise-to-sample ratio than PiDL bUt with a larger library (×2)
and a mUch higher noise-to-sample ratio (×100) for the KS eqUation.
Some limitations of oUr approach are presented on Appendix F.
8
Table 3: Noise-to-sample ratios (Ψ) of successful PDE discoveries from state-of-the-art frame-
works. In parenthesis, the sparsity estimator and the library size (p) are specified when applicable.
framework (spar. est.)	Burgers (p)	KS (P)	source
PDE-FIND (STRidge)	4e-5 (16)	4e-6 (36)	Rudy et al. (2017)
PDE-STRIDE (IHT)	2e-5 (19)	-	Maddu et al. (2019)
S3d (SBL)	-	2e-5(36)	Yuan et al. (2019)
PiDL (STRidge)	2e-2 (16)	3e-4 (36)	Chen et al. (2020)
SNAPE (NA)	3e-3(NA)	-	Bhowmick & Nagarajaiah (2021)
R-DLGA (NA)	-	8e-5(NA)	Xu & Zhang (2021)
DeepMod (rAdaLasso)	2e-2 (36)	2.5e-2 (36)	this work
4 Conclusion
In this paper, we show that no matter the method used to compute the derivatives (numerical or auto-
matic differentiation), the design matrices used for PDE discovery can violate the irrepresentability
conditions (IRC) of the Lasso. This means irrelevant variables might be highly correlated with rel-
evant ones. To perform model variable selection under violated IRC, we introduce a randomised
adaptive Lasso (rAdaLasso). In addition, it allows to preserve a convex optimization problem and
experimental results show it can select the true model in challenging cases at much lower computa-
tional cost than state-of-the-art approaches. Furthermore, once integrated in the deep learning model
discovery framework DeepMod, the rAdaLasso allows to recover a wide variety of nonlinear and
chaotic canonical PDEs up to O(2) higher noise-to-sample ratios than state-of-the-art algorithms.
Finally, the hyperparameters used to perform the model discoveries are identical across experiments.
These contributions pave the road towards truly automated model discovery.
Future work will focus on (1) performing discoveries by leveraging the data from several datasets
using multitask learning and (2) including an evolutionary approach in the proposed framework to
build the library Θ automatically.
Acknowledgments
Acknowledgements will be included in the final version.
Reproducibility S tatement
As reported in the abstract of the paper, we share the data and code on a public repository to repro-
duce our results. In addition,
•	for figure 1, Appendix D.1 recalls the expression of the analytical solution.
•	for figure 3, the repository allows to reproduce it.
•	for figure 4, Appendix D.3 gives more details about the input data for DeepMod. The
hyperparameters are identical across experiment and shared in Appendix C. Our repository
also contains files with the requirements to reproduce the experiments using python.
The notebooks shared on the repository contain supplementary figures where one can see how well
or not the modified version of DeepMod interpolates the data (especially for the chaotic Kuramoto-
Sivashinsky equation).
References
Sutanu Bhowmick and Satish Nagarajaiah. Data-driven theory-guided learning of partial differential
equations using simultaneous basis function approximation and parameter estimation (snape).
arXiv preprint arXiv:2109.07471, 2021.
9
Gert-Jan Both and Remy Kusters. Sparsely constrained neural networks for model discovery of
pdes. arXiv preprint arXiv:2011.04336, 2020.
Gert-Jan Both, Subham Choudhury, Pierre Sens, and Remy Kusters. Deepmod: Deep learning for
model discovery in noisy data. Journal of Computational Physics, 428:109985, 2021a.
Gert-Jan Both, Georges Tod, and Remy Kusters. Model discovery in the sparse sampling regime.
arXiv preprint arXiv:2105.00400, 2021b.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data
by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of
sciences,113(15):3932-3937, 2016.
Zhao Chen, Yang Liu, and Hao Sun. Deep learning of physical laws from scarce data. arXiv preprint
arXiv:2005.03448, 2020.
Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the
lasso and generalizations. CRC press, 2015.
XiUting Li, Liang Li, Zuogong Yue, XiaoqUan Tang, Henning U Voss, Jurgen Kurths, and Ye Yuan.
Sparse learning of partial differential equations with structured dictionary matrix. Chaos: An
Interdisciplinary Journal of Nonlinear Science, 29(4):043130, 2019.
W.X. Ma and B. Fuchssteiner. Explicit and exact solutions to a kolmogorov-petrovskii-piskunov
equation. International Journal of Non-Linear Mechanics, 31(3):329-338, May 1996. ISSN
0020-7462. doi: 10.1016/0020-7462(95)00064-x. URL http://dx.doi.org/10.1016/
0020-7462(95)00064-X.
Suryanarayana Maddu, Bevan L. Cheeseman, Ivo F. Sbalzarini, and Christian L. Muller. Stability
selection enables robust learning of partial differential equations from limited noisy data. arXiv
preprint arXiv:1907.07810, 2019.
Suryanarayana Maddu, Bevan L. Cheeseman, Christian L. Muller, and Ivo F. Sbalzarini. Learn-
ing physically consistent mathematical models from data using group sparsity. arXiv preprint
arXiv:2012.06391, 2020.
Mikhail Maslyaev, Alexander Hvatov, and Anna Kalyuzhnaya. Data-driven partial derivative equa-
tions discovery with evolutionary approach. Computational Science - ICCS 2019, pp. 635-641,
2019. ISSN 1611-3349. doi: 10.1007/978-3-030-22750-0_61. URL http://dx.doi.org/
10.1007/978-3-030-22750-0_61.
Nicolai Meinshausen and Peter Buhlmann. Stability selection. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 72(4):417-473, 2010.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
M Raissi, P Perdikaris, and GE Karniadakis. Physics informed deep learning (part ii): Data-
driven, discovery of nonlinear partial differential equations,”, arxiv e-prints, p. arXiv preprint
arXiv:1711.10566, 2017.
Samuel Rudy, Alessandro Alla, Steven L Brunton, and J Nathan Kutz. Data-driven identification
of parametric partial differential equations. SIAM Journal on Applied Dynamical Systems, 18(2):
643-660, 2019.
Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of
partial differential equations. Science Advances, 3(4):e1602614, 2017.
10
Nayat Sanchez-Pi, LUis Marti, Andre Abreu, Olivier Bernard, Colomban de Vargas, Damien Eveil-
lard, Alejandro Maass, Pablo A Marquet, Jacques Sainte-Marie, Julien Salomon, et al. Artificial
intelligence, machine learning and modeling for understanding the oceans and climate change. In
NeurIPS 2020 Workshop-Tackling Climate Change with Machine Learning, 2020.
Hayden Schaeffer. Learning partial differential equations via data discovery and sparse optimiza-
tion. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473
(2197):20160446, 2017.
Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267-288,1996.
Hao Xu and Dongxiao Zhang. Robust discovery of partial differential equations in complex situa-
tions. arXiv preprint arXiv:2106.00008, 2021.
Ye Yuan, Junlin Li, Liang Li, Frank Jiang, Xiuchuan Tang, Fumin Zhang, Sheng Liu, Jorge
Goncalves, Henning U Voss, Xiuting Li, et al. Machine discovery of partial differential equa-
tions from spatiotemporal data. arXiv preprint arXiv:1909.06730, 2019.
Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning
Research, 7:2541-2563, 2006.
Hui Zou. The adaptive lasso and its oracle properties. Journal of the American statistical associa-
tion, 101(476):1418-1429, 2006.
A Proof of Proposition 1
Proof. by definition for i ∈ [1,p], Wi = 1∕∣ξi∣γ. We denote Θτ,k a vector of the subset of the
∙-v	∙-v
design matrix Θ that contains the relevant vectors and ΘF,j a vector of the subset of the design
~	r∖	〜
matrix that contains the irrelevant vectors, then, Θτ,k = Θτ,k/Wk = ∣ξτ,k ∣γ ∙ Θτ,k and Θp,j =
ʌ 〜〜
θF,j/Wj = ∣ξF,j∣γ ∙ θF,j where k is some column index of Θt and j of Θf. The projection of
an irrelevant vector onto the space of relevant vectors in the least-squares sense ΘF,j = ΘTξ +
can be decomposed into the space of relevant vectors, ΘF,j = Pk ξF,j,kΘT,k + , where ξF,j,k are
the projections of a given irrelevant vector onto the relevant space. Using such decomposition the
irrepresentability condition becomes, maxj Pk ∣ξFj,k | < 1 - η. By applying the transformation
due to the adaptive weights We get, Θpj = ∣ξFj ∣γ Ek ξFj,k ∙ ∣^^ ∙ Θτ,k + 〜∣ξFj ∣γ, which
Ll ξp
can be	reduced to Θpj = Ek l ξFj∣	ξFj,k	∙ Θτ,k +	E ∙ ∣ξFj ∣γ.	Now if we assume that since
ʌ ʌ ʌ ʌ
ξτ,k is a relevant term and ξFj is an irrelevant one: ∣ξτ,k| > ∣ξFj |. If in addition, Y ≥ 1, then,
ʌ ʌ
0 ≤ 归F,j∕ξτ,k ∣γ < 1, which leads to,
X । IFj l lξF,j,k | ≤ X lξF,j,k |	(12)
k lξT,k l	k
which is equivalent to the inequality 5.	口
B	S tab ility selection with error control
With stability selection, variables are chosen according to their probabilities of being selected with a
warranty on the selection error, Meinshausen & Buhlmann (2010). The first step consists in finding
the probability of a variable k of being selected under a data perturbation: let Ib be one of B random
11
sub-samples of half the size of the training data drawn without replacement. For a given λ an
estimation of the probability of k being selected is given by,
Inλ = 1 XX J 1 if fλ(Ib) > 0
k	B 乙 I 0 otherwise
b=1
(13)
where fλ(Ib) = ∣∣ξλ(Ib)IIi for the randomised adaptive Lasso. Second, by computing the proba-
bilities of being selected over a given range of λ's the stability paths can be obtained, see figure 3.
That range is denoted Λ = [λmax ; λmax], where is the path length and λmax is the regularisation
ʌ
parameter where all coefficients ξ are null. In Meinshausen & Buhlmann (2010) derive an upper
bound on the expected number of false positives E(V ), that can help determining a smaller Λ re-
gion where a control on the selection error can be warrantied. By fixing this bound to EVmax , the
regularisation region becomes, see figure 3,
Λ* = (λ ∈ Λ such that, E(V) ≤ 0 次 ∣、 ≤ EVmax)	(14)
(2πthr - 1)p
where πthr is the minimum probability threshold to be selected and qΛ is the average of selected
variables. We propose here to approximate qχ by ^λ = EbBSbI = Pk Π∏λ∙ Finally, the set of stable
variables with an upper bound on the expected number of false positives is,
SAabIe = {k such that maxΠk ≥ ∏thr for λ ∈ Λ*}
(15)
C A single set of hyperparameters
Stability selection expected number of false positives upper bound EVmax = 3, number of re-
samples B = 40 and the minimum probability to be selected πthr = 0.9.
Library consists of polynomials and partial derivatives up to the fifth order leading to a library size
ofp =36 potential terms: {1,ux,uxx, ...,uxxxxx, u,uux, ..., uuxxxxx, ..., u5, u5ux, ..., u uxxxxx}.
Neural network architecture & optimiser NNs are 4 layers deep with 65 neurons per layer and
sinus activation functions with a specific initialisation strategy, see Sitzmann et al. (2020). The NNs
are trained by an Adam optimiser with a learning rate of 5 ∙ 10-5 and β = (0.99,0.99).
Randomness Seeds are identical across datasets: (1) to initialise the NNs and (2) generate the
noise vectors.
D About the data
This appendix provides data source details to reproduce the examples of this paper.
D.1 Analytical library of the KdV equation
The Kortweg-de-Vries (KdV) ut = -6uux - uxxx PDE analytical solution for 2 travelling solitons
is,
u(x,t) = 2(c1-c2)∙
Ci cosh(√⅞ξ2∕2)2 + C2 sinh(√CTξι∕2)2
(√ci - √C2) cosh[(√C1ξι + √C2ξ2)∕2] + (√C1 + √C2) cosh[(√C1ξi - √C½)∕2])
where	ci	>	c2	>	0,	ξi	= x -	cit	and	ξ2	= x -	c2t.	40 points equally dis-
tributed such that x ∈ [-5, 12], 50 points equally distributed such that t ∈ [-1, 2]
and ci = 5, c2 = 2. By automatic differentiation we obtain the 12 terms library:
{1, ux , uxx , uxxx , u, uux, uuxx , uuxxx , u , u ux , u uxx , u uxxx}.
12
D.2 Libraries from splines/numerical differentiation
Burgers, ut = νuxx - uux, shared on the github repository mentionned in Maddu et al. (2019).
The solution here is very similar to the one obtained using the analytical expression below using
Dirac delta initial conditions.
Kuramoto-Sivashinky (KS), ut = -uux - uxx - uxxxx, shared on the github repository men-
tionned in Rudy et al. (2017).
D.3 Input data for deep learning experiments
We generate numerical solutions from several equations, on top of which we add α Gaussian white
noise,
Unoisy = U + α ∙ σ(u) ∙ Z	(16)
where Z 〜N(0,1). The following PDES are considered:
Burgers, initial condition: Dirac delta, analytical solution,
,	. V7	(e2ν - 1)e-Xt
乜 x"	Mnt 1 + 2(e2ν - 1)erfc(√4νt
)
where A is a constant and ν is the viscosity, ν = 0.1, A = 1 and 40 points equally distributed such
that x ∈ [-2, 3], 50 points equally distributed such that t ∈ [0.5, 5].
Kortweg-de-Vries (KdV), see subsection D.1.
Newell-Whitehead (NW), Ut = 10Uxx + U(1 - U2) - 0.4, numerical solution using a finite
differences solver and the following initial condition:
3
U(x, 0) =	αi sin(βiπx)
i=1
where αi and βi are constants. 40 points equally distributed such that x ∈ [0, 39], 50 points equally
distributed such that t ∈ [0, 1.96] and α1 = 0.2, α2 = 0.8, α3 = 0.4, β1 = 12, β2 = 5, β3 = 10.
Kuramoto-Sivashinky (KS), see subsection D.2. 2000 samples are randomly drawn from a subset
of the dataset, details can be found on our github repository, see note 1.
E Additional Results
Stability plots for case 2 comparison In this case the performance of PDE-STRIDE and
rAdaLasso are compared on figure 5.
DeepMod interpolations for the experiments see figure 6.
F	Limitations of the approach
Incomplete library An obvious limitation of the approach is that the library of potential PDE
terms must contain the true underlying PDE terms. Typically this can be diagnosed as we report
(in TensorboardX) the mean square error on a test set which allows to verify if the discovered PDE
generalizes well or not.
13
PDE-STRIDE
≈-=IqEqOJd UO-ɔ-①S
≈-三qBqoJd UO-ɔ-①s
randomised AdaLasso


λ
λ
a

Figure 5: Comparing PDE-STRIDE and the randomised adaptive Lasso selection performance on a
challenging case: recovering the Burgers’ equation from a library built using polynomial interpola-
tion from a dataset with 4% noise Maddu et al. (2019). In (a), PDE-STRIDE solves a relaxation of
the best subset selection (l0 regularisation) using an Iterative Hard Thresholding algorithm. In (b),
the stability plot for the randomised adaptive Lasso. The true underlying PDE can be recovered by
both methods by a proper tuning of the error selection: EVmax = 2. However, the computational
cost to run PDE-STRIDE is a couple orders of magnitude higher (≈ 122s) compared to the one of
for the randomised adaptive Lasso (≈ 1.30s).
Figure 6: DeepMod interpolations for the experiments described in the main text.
14
Non-unique solutions If the discovery problem has non unique solutions, our approach will pro-
pose one that might not be the one we were looking for. So far, we have devised two cases in which
the solutions are not unique and we present them in the next paragraphs. It is known that the solution
of a single soliton from the KdV equation is also a solution of the simpler travelling wave equation:
ut = -cux, see Rudy et al. (2017). So trying to discover the underlying equation from an analytical
of KdV with a single soliton like,
c
u(x, t)
2cosh(√c x-2ct)
(17)
where c > 0, will result in the discovery of ut = -cux. We obtained a similar result with our
approach while trying to find the underlying equation from data generated by a solution of Fishers
equation (ut = uxx + u(1 - u)) from Ma & Fuchssteiner (1996),
u(x,t) = (l + (√2 - 1)e-σ(x+λt))-2
(18)
where σ = λ - √λ2 - 1 and λ = 2√6. For both analytical solutions X and t play a symmetric role
explaining why Ut = k ∙ ux, where k is some constant.
Coefficient bias in chaotic systems for the successful discoveries of non chaotic PDEs, the co-
efficient mean errors are typically below (< 2%). For the chaotic Kuramoto-Sivashinsky PDE, we
show that with 50% noise and a fraction of the data we can recover the terms of the PDE with half
the mean error on the coefficients magnitudes with respect to PDE-FIND. However, chaotic systems
are sensitive to their initial conditions and the coefficients errors (20%) of the discovered PDE are
very large with respect to the ground truth. As a result, the discovered PDE cannot be used for
predictions but can be used to identify the underlying physical processes.
15