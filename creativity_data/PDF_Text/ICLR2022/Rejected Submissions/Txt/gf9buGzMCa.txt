Under review as a conference paper at ICLR 2022
Expressiveness of Neural Networks Having
Width Equal or Below the Input Dimension
Anonymous authors
Paper under double-blind review
Ab stract
The understanding about the minimum width of deep neural networks needed to
ensure universal approximation for different activation functions has progressively
been extended (Park et al., 2021). In particular, with respect to approximation
on general compact sets in the input space, a network width less than or equal
to the input dimension excludes universal approximation. In this work, we focus
on network functions of width less than or equal to the latter critical bound. We
prove a maximum principle from which we conclude that for all continuous and
monotonic activation functions, universal approximation of arbitrary continuous
functions is impossible on sets that coincide with the boundary of an open set plus
an inner point. Conversely, we prove that in this regime, the exact fit of partially
constant functions on disjoint compact sets is still possible for ReLU network
functions under some conditions on the mutual location of these components.
1	Introduction
In the course of the increasing popularity of deep neural networks for applications in technical,
ecological and many other fields, there has been huge progress in the research on understanding
the mathematical properties of the mathematical mapping implemented by a deep neural network.
The approximation properties, or expressiveness, of neural network functions have attracted intense
interest. The central result in this field is the classic universal approximation theorem, which states
that any continuous function can be approximated with arbitrary accuracy (in terms of uniform
approximation or Lp norms) by neural network functions that have only one hidden layer for nearly
every activation function (Cybenko, 1989; Hornik, 1991). In recent years, these kind of results have
become more precise, for instance, in terms of estimates on the order of magnitude of parameters
needed to achieve an approximation of a certain accuracy (Petersen & Voigtlaender, 2018; Yarotsky,
2017; 2018), or by investigating the number off piece-wise linear regions (Montufar et al., 2014;
Serra et al., 2018; Hanin & Rolnick, 2019); we refer to (DeVore et al., 2020) for an overview on
recent developments. Since empirical observations showed that depth has a significant impact on the
performance of neural networks, a lot of research has been dedicated to the effect of depth on the
expressive power of neural networks (Telgarsky, 2016; Mhaskar & Poggio, 2016; Montufar et al.,
2014; Raghu et al., 2017; Rolnick & Tegmark, 2018; Lin et al., 2017; Cohen et al., 2016). Besides this,
the role of width in expressiveness of network functions has been investigated (Lu et al., 2017; Hanin
& Sellke, 2017; Hanin, 2019; Johnson, 2018; Kidger & Lyons, 2020; Park et al., 2021). An important
result states that a width larger than the input dimension is needed to allow universal approximation
on arbitrary compact sets. It has also been shown that the capability to learn disconnected or bounded
decision regions depends on whether the width is larger than the input dimension (Nguyen et al.,
2018; Beise et al., 2021).
In a further line of research, the capabilities of neural networks to memorize finite sample sets is
investigated (Yun et al., 2019; Vershynin, 2020; Bubeck et al., 2020; Park et al., 2020), which provides
mathematical foundations to the surprising findings that neural networks can interpolate the training
data (Zhang et al., 2017) in practise.
In this work, we investigate what kind of subsets M ⊂ Rn admit universal approximation by neural
network functions with a maximum layer width less than or equal to the input dimension, i.e. network
functions that violate the necessary condition for universal approximation given in Hanin (2019) and
Park et al. (2021) for arbitrary compact sets.
1
Under review as a conference paper at ICLR 2022
Let_Us introduce the following notation: For a set D ⊂ Rn We denote by DO the set of interior points,
by D its closure and by ∂D = D \ DO the boundary of D. By ∣∣∙k we mean the Euclidean norm. For
a linear subspace U ⊂ Rn, we denote by dim(U) its dimension and by PU : Rn → U the orthogonal
linear projection on U. We denote by ej the unit vector of the j-th coordinate axis in Rn. For some
v ∈ Rn and j ∈ {1, ..., n}, we write v(j) for the j-th component of v.
For some depth L ∈ N, we consider neural network functions of some , F : Rn0 → RnL where n0 is
called the input dimension and nL the output dimension. Our network functions have the following
form
F := WL(AL-1 ◦ ... ◦ A1) + bL	(1)
where Aj (x) = σ(Wjx + bj) with Wj ∈ Rnj ×nj-1 (weights), bj ∈ Rnj (bias), where j = 1, ..., L,
and σ : R → R the activation function. The application of σ and the inverse image σ-1 are
understood to be applied element wise to vectors or subsets of Rn . The widely used activation
function rectified linear unit, shortly ReLU, is defined by t 7→ max{t, 0}. As a shorthand notation of
the function implemented by the first k layers of a network function, we set
Fk = Wk (Ak-1 ◦ ... ◦ A1) + bk,	(2)
for k ∈ {1, ..., L - 1}. We call nj the width of layer j = 1, ..., L. The width of the network is defined
as ω(F) = max{nj : j = 1, ..., L} and L is called the depth of the network. For m, L ∈ N and an
activation function σ, NNkσ(m, L) is the set of network functions F : Rn0 → Rk of the form (1)
with activation σ and of maximum width m, i.e. ω(F) ≤ m, and depth L, i.e. L layers including
the final linear layer. In case that Rk = R we omit the latter and write NNσ (m, L) instead, and
in case the the depth is not specified, meaning that arbitrary depth is allowed, we write NNkσ(m),
NNσ (m), respectively.
We say that a compact subset M ⊂ Rn0 admits universal uniform approximation of functions in
some function class of mappings from M to R by network function ofNNσ(m, L) orNNσ(m), if
for every f : M → R in that class and every ε > 0, there exists some F ∈ NNσ (m, L), respectively
F ∈ NNσ(m), such that
max|f (x) - F (x)| < ε.	(3)
2	Related work
Approximation properties of width bounded neural networks have been studied in several papers. A
common goal in these lines of research is to provide upper and lower bounds on the minimum width
of the network, i.e. the minimum number of neurons per layers, here denoted by ωmin, needed for
universal approximation in C(M, RnL), i.e. the space of continuous functions from a compact set M
to RnL endowed with the norm of uniform convergence, and Lp(M, RnL) with some set M ⊂ Rn0,
i.e. the space of functions f = (f1, ..., fnL) from M to RnL such that every |fj |p is integrable over
M , endowed with the usual Lp norm. Since in this work we are interested in universal approximation
in C(M, R), we refer to Lu et al. (2017), Kidger & Lyons (2020) and Park et al. (2021) for the case
of Lp approximation. For the case of C(M, RnL), it was proven in Hanin & Sellke (2017) that
n0 + 1 ≤ ωmin ≤ n0 + nL , for ReLU activation,	(4)
which has been tightened in Park et al. (2021) to
ωmin ≥ max{n0 + 1, nL } for ReLU and STEP activation,	(5)
where STEP refers to the threshold activation that maps to 1 on {x ≥ 0} and 0 otherwise. It has been
shown at the same time in Johnson (2018) and Beise et al. (2021) that
ωmin ≥ n0 + 1 for ReLU + injective continuous activation.	(6)
The activation functions allowed in Johnson (2018) are slightly more general, as it is only required that
they admit arbitrary accurate uniform approximation by injective continuous functions on arbitrary
compact subset of R. In Kidger & Lyons (2020) the above upper bounds are extended to further
classes of activation functions. For these results and a general overview on recent developments in
this field, we refer to Park et al. (2021). As we also formulate results for finite sets, our work partially
exhibits relations to the research on finite sample memorization (Yun et al., 2019; Vershynin, 2020;
Bubeck et al., 2020; Park et al., 2020).
2
Under review as a conference paper at ICLR 2022
In this work, we only consider width less than or equal to n0 and investigate uniform approximation
on certain compacts subsets of the input space. According to the above result (6), we cannot expect
universal approximation for all compact M ⊂ Rn0 . However, a common assumption in applications
is that approximation is only needed on a certain subset. We derive some topological conditions on
M that allow or exclude a kind of universal approximation under these circumstances. Although
narrow neural networks, as they are considered in this work, are not common in practical applications,
many networks exhibit a decaying layer width at the later layers. From this perspective, our results
give theoretical insights on the kind feature extraction earlier layers need to implement in order to
allow the later layers to solve a given machine learning task. Our main contributions are as follows:
1.	A maximum principle is given for F ∈ NNσ(n0) with σ continuous and monotonic. This
allows to conclude that the lower bound ωmin ≥ n0 + 1 in (6) is sharp for a wide range of
subsets M, e.g. when M = ∂D ∪ {c} for some D with non-empty interior and C ∈ D◦.
2.	We show that for the case of two disjoint compact sets, the existence of a cone-like sector
that contains one of these sets and does not intersect with the other one, is sufficient to allow
exact fit of functions that take constant values on each of these sets by network functions
from NN ReLU (n0, 4). A weaker result is concluded for the case of multiple pairwise
compact components.
3 Maximum principle
Let us recall thatNNσ(n0) designates the set of network functions with activation function σ, having
maximum width n0 and arbitrary depth. In this section, we will prove a maximum principle for
network functions ofNNσ(n0) for a wide class of activation functions. This principle can be viewed
as a root cause why universal approximation with functions from NNσ(n0) on arbitrary compact
sets is impossible as shown in Johnson (2018) and Beise et al. (2021), see (6). This also immediately
leads to a topological condition on subsets of Rn0 that do not admit universal approximation by
functions in NN σ (n0).
Theorem 1 (Maximum Principle). Let M be some compact subset of Rn0 and σ a continuous,
monotonic activation function. Then every F ∈ NNσ(n0) takes its maximum value at the boundary
∂M.
Note that, considering -F instead of F, the latter result also implies that the minimum value is taken
on ∂M . This implies that universal uniform approximation of continuous functions is impossible on
compact sets with non-empty interior. Even more, it can be concluded that:
Corollary 1. Let M be some compact subset of Rn0 such that ∂D ⊂ M and M ∩ Do = 0 for some
non-empty open set D ⊂ Rn. Then M does not admit universal uniform approximation of continuous
function by network functions from NNσ(n0).
The latter results naturally lead to the following question: Is it possible to uniformly approximate
arbitrary continuous functions on the unit sphere (in Rn0 ) with arbitrary accuracy by network
functions as they are considered in Theorem 1? Some positive approximation results are given in the
next section, but a gap to an answer to the question remains.
It is not clear to us whether the conditions of Theorem 1 can be weakened in a way that it also applies
to network function having width larger than the input dimensions under certain circumstances. The
following might be a natural question in this context: Let F : Rn0 → R be a neural network function
of arbitrary width but with weight matrices having rank less than or equal to n0 . Does a maximum
principle similar to Theorem 1 apply in this case? The following example shows that the maximum
principle in Theorem 1 does not hold in this case.
Example 1. Let M = [-1, 1] and let W1 ∈ R2×1 and b1 ∈ R2 implement the following linear affine
mapping
W1x+b1 = x+x 1
Then M1 = ReLU(W1M + b1) is given by M1 = M1,1 ∪ M1,2 where
M1,1 := {(x + 1, 0)T : x ∈ M, and x < 0}
3
Under review as a conference paper at ICLR 2022
M1,2 := {(x + 1, x)T : xT ∈ M, and x ≥ 0}.
Now, let v = (1, 1/2)T ∈ R2, W2 ∈ R2×2 and b2 ∈ R2 such that x 7→ W2x + b2 implements
the orthogonal projection onto the hypersurface V := {x ∈ R2 : xTv = (1, 0)v}. That is, the
hypersurface orthogonal to v which intersects (1, 0)T. Then both, M1,1 and M1,2 are mapped to the
line M2 := {y ∈ R2 : y =(1,0) + λ(-1/2,1), 0 ≤ λ ≤ 1/2} and since those vectors are contained
in the first quadrant we have
M2 = ReLU(W2(ReLU(W1M + b1) + b1).
It can further easily be verified that, under the latter two layer network function, 0, which is an inner
point of M, is the only vector that is mapped to the extreme point (1, 0) of M2. Now it is easy to
find a weight W3 ∈ R2 such that the linear map x 7→ W3 x as a mapping from M2 to R takes its
minimum or maximum value at (1, 0) only.
By concatenation of additional components to the input dimension such that M = [-1, 1] × [0, 1]d,
d ∈ N, and adaptation of the weights W1, W2 and bias b1, b2 in a way that these additional
components are mapped identically under the action of the two layer network function x 7→
ReLU(W2(ReLU(W1x + b1) + b1), the above example can be extended to higher dimensions.
The following proposition provides the main observation for the proof of Theorem 1.
Proposition 1. Let M be a compact subset of Rn0, σ a continuous, monotonic activation function
and F ∈ NNσn0 (n0) such that all weight matrices Wj, j = 1, ..., L, are square and have full rank.
Iffor some X ∈ M◦ the image F(X) is a boundary point of F(M), i.e. F(X) ∈ ∂F(M), then there
is an X ∈ ∂M such that F(X) = F(x).
Proof. In case that σ is injective, the whole network function F is injective since the weight matrices
are assumed to be square and of full rank. Hence, the invariance of the domain theorem gives
that no X ∈ M◦ can be mapped to ∂F(M). The remaining case is that σ is partially constant.
Then, given an X ∈ M◦ with F(x) ∈ ∂F(M), let k be the smallest k ∈ {1,…，L - 1} such that
σ(F"(x)) ∈ ∂σ(F"(M)). Notice that such a k must exist since, according to the invariance of the
domain theorem, the last injective, linear affine mapping X 7→ WLX cannot map inner points to
boundary points. Then, necessarily for at least one component of Fk (x), a partially constant part of
σ is active. More explicitly, say (Fk(X))(j) ∈ [aj,bj] ⊂ R with σ(t) = Cj, where Cj is the constant
value taken for all t ∈ [aj, bj], for some indices j ∈ I ⊂ {1, ..., n0}. In case that one of those
intervals can be enlarged arbitrarily to either [aj, ∞) or (-∞, bj], which would be the case for ReLU
activation, we can find a t ∈ R such that Xkk = Fkk(X) + tej ∈ ∂Fkk(M) and with σ(Xkk) = σ(Fkk(X)).
In a next step we show that such an Xkk can also be found in cases where the length of the intervals
[aj , bj] are upper bounded. In those cases we may assume that the intervals [aj , bj] are maximum
in the sense that at their left and right end σ is not constant any more, and I is maximum in the
sense that for every other index, the activation function σ is injective in a neighbourhood. Indeed,
if such components would not exist, the component wise application of σ would be injective in
a small neighbourhood of Fkk (X) and the invariance of the domain theorem would exclude that
σ(Fk(x)) ∈ ∂σ(Fk(M)). By the selection of k, Fk (X)IS still an inner point of Fk(M). The fact that
the inner point Fk (x) ∈ (Fk (M))◦ is mapped to a boundary point by the element wise application of
σ implies that there is some non-empty I ⊂ I with corresponding tj ∈ R such that
tj ∈ [-(Fkk(X))(j)+aj,-(Fkk(X))(j)+bj], j∈Ik	(7)
Xkk := Fkk(X) + Xtjej ∈ ∂Fkk(M).	(8)
j∈Ik
Indeed, otherwise,
n0
Fk(x) + Xtjej ∈ (Fk(M))°
j=1
where for some small ε > 0 and all j ∈ I, tj may be chosen arbitrarily in (-(Fkk(X))(j) + aj -
ε, -(Fkk(X))(j) + bj + ε) and ε > |tj| > 0 for j ∈/ I. By the definition of I and the corresponding
4
Under review as a conference paper at ICLR 2022
intervals [aj, bj], this would imply that σ (Fk (x)) is an inner point of σ (Fk (M)). Now, (8) implies
that there is a X ∈ M such that
n0
Fk (X) = Fk(X) + X tjej ∈ dFk(M),
j=1
with tj as in (7). Considering that Wk is square and has full rank, the inverse image of Fk (X) under
X 7→ WkkX + bkk is a boundary point of σ(Fkk-1(M)) in case that k > 1 and a boundary point of M in
case when k = 1. In the last case we are done. In the first case, the same reasoning can be applied to
X instead of x, and the iterative application finally leads to a boundary point of M that is mapped to
σ (Fk (X)) = σ (Fk (x)) and hence follows the same trajectory when passed through the last layers
from k to L.	□
Proof. (Theorem 1) We may assume that M◦ is non-empty and that every weight matrix Wj,
j = 1, ..., L - 1, is square and of full rank. Otherwise the result is trivial, because F(M) = ∂F(M).
Hence the assumption of Proposition 1 holds for F = AL-1 ◦ ... ◦ A1. Since the linear mapping X 7→
WLX from RnL-1 to R takes its maximum and minimum values on ∂F(M), the latter proposition
also implies that F takes its maximum value on ∂M.	□
4	Approximation properties on sub sets
In this section we give some approximation results that show that, despite the limitations induced
by Theorem 1, the network functions considered in this work still allow some weaker kind of
universal approximation on subsets. The first result in this section follows from existing results on
approximation by network functions. The first point can be deduced from Hanin & Sellke (2017,
Theorem 1). The second point can be derived from Hanin & Sellke (2017, Proposition 2) or from
Hardt & Ma (2017, proof of Theorem 3.2.).
Proposition 2.
1.	Let M be some compact subset of Rn0. If there exists a subspace U ⊂ Rn0 with dim(U) <
n0 such that PU is injective as a mapping from M to U, then M admits universal uniform
approximation of continuous function by network functions in N N ReLU (dim(U) + 1).
2.	Let M = {X1, ..., Xm} ⊂ Rn0 be a finite set and f : M → R some mapping. Then there
exists a network function F ∈ NNReLU(2) such that F(X) = f(X)for all X ∈ M.
The proof is almost straightforward, given the results from the aforementioned works. A proof can be
found in the appendix.
We next extend the second statement of the latter proposition to the case of two disjointn0-dimensional
compact sets.Theorem 1 shows that arbitrary uniform approximation is not admitted on such domains.
However, exact fit is possible for functions that are constant on each of these sets under certain
geometrical properties on their mutual location. The approximation of such functions arises naturally
in classification tasks where each point in a particular disjoint compact set belongs to the same class,
i.e. these sets constitute different disjoint clusters of the data.
Theorem 2. Let M = K1 ∪ K2, with disjoint compact sets K1, K2 ⊂ Rn0 such that for some
c ∈ Rn0 and some linearly independent v1, ..., vn0 ∈ Rn0 the set K1 is contained in the sector
n0
S := {X ∈ Rn0 : X = c + X λjvj , λj > 0}
j=1
and K ⊂ Rn0 \ S. Thenfor everyfunction f : M → R that takes constant values on Ki and K,
respectively, there exists a network function F ∈ NN ReLU (n0, 4) such that
f(X) = F(X)
for every X ∈ M.
5
Under review as a conference paper at ICLR 2022
The conditions of Theorem 2 are verified and inspected by numerical experiments in Section 6. How-
ever, to us it is unclear whether the assumptions on the mutual location of the compact components in
Theorem 2 can be significantly weakened. Increasing the depth will certainly allow more complex
configurations. However, from Theorem 1 is clear that K2 cannot completely enclose K1 when the
maximum width is upper bounded by n0, no matter how many layers the network contains. We think
that closing the described gap could entail important insights for the theory of neural networks.
For the proof of the latter result we need the following.
Proposition 3. Let K, M be two compact sets in Rn0 that are strictly separable by a linear hyper
surface. Then there exists an F ∈ NN nR0eLU (n0, 2) such that F(K) is a single vector in Rn0 \ M
and F (x) = x for all x ∈ M. Moreover, F can be arranged in a way that given ε > 0
minkF (K) - xk < ε.
x∈K
As a consequence of Proposition 3 and Proposition 2, we obtain the following for the case of more
than two distinct sets under considerably more restrictive condition as in Theorem 2.
Corollary 2. Let M = Sjd=1 Kj with pairwise disjoint compact sets Kj ⊂ Rn0, j = 1, ..., d such
that for every Kj there exist vj ∈ Rn0 and qj ∈ R such that
vjTx > qj for all x ∈ Kj, and vjTx < qj for all x ∈ M \ Kj,
i.e. there are linear hypersurfaces separating each Kj from the remaining Kl, l 6= j. Then for every
function f : M → R that takes constant values on every Kj, j = 1, ..., d, there exists a network
function F ∈ NN ReLU (n0) with
f(x) = F(x)
for every x ∈ M .
Proof. (Corollary 2) By means of Proposition 3, we find an F1 ∈ NNnR0eLU(n0) such that F1(x) = x
for x ∈ Sjd=2 Kj and a1 := F1(K1) is a single vector in Rn0 \ Sjd=2 Kj. Proposition 3 also allows
us to choose F1 in a way that a1 is sufficiently close to K1 that the conditions of Corollary 2 still
apply to Sjd=2 Kj ∪ {a1}. We can iteratively apply Proposition 3 to find F1, ..., Fd ∈ NN nR0eLU (n0)
such that
Fd ◦ ... ◦ F1 (Kj) = aj, j = 1, ..., d
for pairwise distinct a1, ..., ad ∈ Rn0. Now, for a given f : M → R with f(x) = yj for all x ∈ Kj,
j = 1, ..., d, Proposition 2 yields an F ∈ NN ReLU (2) with F(aj) = yj for all j = 1, ..., d. The
desired network function is then obtained by F := F ◦ Fd ◦ ... ◦ Fi.	口
Proof. (Proposition 3) Let us remind that e1, ..., en0 are the standard basis vectors in Rn0. By
assumption, there is a v ∈ Rn0, with kvk = 1 and a q ∈ R such that vTx > q for x ∈ K and
vTx < q for x ∈ M. For a given ε > 0 we may assume that the linear hyper surface defined by v, q,
H := {x ∈ Rn0 : vTx = q} is so near to K that for some a ∈ K that realizes the minimum distance
of K to H we have
VT a = q + ε∕2.	(9)
Indeed, otherwise we can shift the hyper surface accordingly by increasing q. Then, by the fact that
∣∣vk = 1, (9) means that a has a distance equal to ε∕2 to H and a := a - ε∕2 V ∈ H is the unique
vector in H that realizes this distance. Let V1 ∈ Rn0×n0 such that x 7→ V1x implements a length
preserving rotation that maps V to -e1. Then x 7→ V1x - V1aa maps H to span(e2, ..., en0) and
K1 := V1 K - V1 aa is contained in the half space of vectors with negative first component and with
ai := V1a - Via = -ε∕2 ei, and Mi := ViM — Via is contained in the half space of vectors with
positive first component. By compactness of K1 and M1, we can find u1, ..., un0 ∈ span(e2, ..., en0),
such that for a δ > 0, -ui - δei, ..., -un0 - δei are linearly independent and such that with δ
sufficiently small
n0
Ki ⊂ S- := {x ∈ Rn0 : x = X λj (-uj - δei), λj > 0}
j=i
n0
Mi ⊂ S+ := {x ∈ Rn0 : x = X λj (uj + δei), λj > 0}.
j=i
6
Under review as a conference paper at ICLR 2022
Let V2 ∈ Rn0×n0 such that x 7→ V2x maps -uj - δe1 to -ej for j = 1, ..., n0. Then, by linearity,
every uj + δe1 is mapped to ej for j = 1, ..., n0. Thus S- is mapped to the cone of vectors having
negative components only, and S+ is mapped to the cone of vectors having positive components only.
Hence, the application of ReLU maps all of V2K1 to 0 and maps V2M1 identically. We can now
apply the inverse of the linear affine mappings which will map V2M1 back to M, i.e. for
F(X)= VTVTReLU(V2(V1x - V1a)) + a.
With this F We have F(M) = M and F(K) = F (a) = a. Note that by (9) and the definition of a
min∣∣x — F(a)k = ε∕2 < ε
x∈K
which concludes the proof.	□
Proof. (Theorem 2) Let V be the matrix that results from the concatenation of the columns v1, ..., vn0
and set	W1	:= -V	-1	and	b1	:=	W1(-c).	Then the mapping x 7→	W1x	maps the	vj	to	-ej,
j = 1, ..., n0 and W1K1 + b1 is a subset of
n0
S- := {x ∈ Rn0 : x = Xλj(-ej), λj > 0}
j=1
and W1K2 + bi ⊂ Rn0 \ S-. Thus, the one layer network function Fι(x) := ReLU(WIx + bi)
maps all x ∈ K1 to 0 and x ∈ K2 are mapped to
n0
{x ∈ Rn0 : x = Xλjej, λj ≥ 0}\ {0}.
j=i
Hence, with a sufficiently small q > 0, Fi(Ki) and Fi(K2) are strictly separated by the linear hyper
surface
{x ∈ Rn0 : (1, 1,  , 1)x = q}.
The twofold application of Proposition 3 yields two network functions that can be concatenated to
a three layer network function F2 ∈ NNnR0eLU(n0, 3) with F2(x) = W4 ReLU(W3 ReLU(W2x +
b2) + b3) + b4 and such that F2(Fi(Ki)) and F2(Fi(K2)) are two distinct vectors ui, u2 in Rn0,
respectively. For a given f : M → R that takes constant values on Ki and K2 , say ai , a2 ∈ R,
respectively, we attach a linear affine layer x 7→ wTx + b5, where w ∈ Rn0 , b5 ∈ R such that
wTui + b5 = ai and wTu2 + b5 = a2. This mapping is integrated in the final layer of F2 ◦ Fi and
finally gives
F(x) = wTW4 ReLU(W3 ReLU(W2 ReLU(Wix + bi) +b2) + b3) +wTb4 +b5.
□
We conclude this section with a comment on implications of our results on generalisation. In Zhang
et al. (2017) the researchers made the observation that deep neural networks can interpolate randomly
annotated data, which shows the enormous capabilities of large networks to memorize data. In
Proposition 2, Theorem 2 and Corollary 3 we show that network functions of width less than or equal
than to input dimension in many configurations also implement an exact fit of the training data. At
the same time these network functions are restricted by the maximum principle given in Theorem 1,
which may contradict the underlying distribution of the data at hand.
5 Uniqueness
A maximum principle similar to Theorem 1 also applies for harmonic and holomorphic functions,
that is, the null space of the Laplace operator and the null space of ∂z := 1∕2(∂χ + i∂y) (x real
part, y imaginary part), respectively (Rudin, 2006). As a consequence, a uniqueness theorem applies
which states that two harmonic, respectively holomorphic, functions coincide on the interior of a
given set when they coincide on its boundary, respectively. In view of Theorem 1, it is hence natural
to ask whether a similar result applies in the case of network functions in NNσ(n0). However, the
proof for the case of harmonic and holomorphic functions relies on the fact that these functions form
a vector space, which is not the case for the set NNσ(n0). The following examples show that a
uniqueness theorem does not hold, in general, for the network functions with (strictly) monotonic
activation functions considered in this work.
7
Under review as a conference paper at ICLR 2022
1α
X → σɑ I (1 + α) σα(x) +	+ &) ) and x → σα
Example 2.	Let σα(x) be the leaky ReLU function defined by σα(x) = x for x ≥ 0 andσα(x) = αx
for x < 0 with α > 0. Then
(I σα(X + 1))
both map —1 to 0 and 1 to 1, but the first one maps 0 to α∕(1 + α), while the Second one maps 0 to
1/2 and hence do not coincide unless α = 1.
Example 3.	A similar example can be obtained for analytic activation functions. Indeed, let σ be the
sigmoid function. For x 7→ σ(ax + b), one can arrange a > 0, and b ∈ R, such that in one case only
a concave and in a second case a convex excerpt of sigmoid is active forx in [0,1], respectively, and
such that both functions coincide at 0 and 1. As the first one is concave and the second one is convex
on [0,1] and both are non-linear, they do not coincide on the interior of [0,1].
A simple uniqueness result follows from the following observation.
Proposition 4. Let M be a compact subset of Rn0, F ∈ NNR；Lu(no) and B = FT ((F(M))◦)
the inverse image of the inner points ofF(M). Then F is linear affine and bijective as a mapping
from B to (F(M))◦.
A proof of Proposition 4 is given in the appendix. As a consequence we have.
Corollary 3. Let F, G ∈ NNnR0eLU(n0) and M ⊂ Rn0 be some compact set. IfF and G coincide
on a set of vectors v1, ..., vn0+1 in
K := FT ((F(M))◦) ∩ GT ((G(M))◦),
such that vj — vn0+1, j = 1, ..., n0 are linearly independent, then F(x) = G(x) for all x ∈ K.
It should be noted that in the latter corollary, the fact that F, G coincide on K does not imply that F
and G are the same network functions as functions on Rn0.
6	Numerical experiment
We provide numerical results to illustrate that neural networks learn transformations similar to the
construction of our theoretical derivations in the proof of Theorem 2. We formulate a toy example
consisting of several (6 or 8) pair-wise disjoint balls of the same class encasing a center ball of
a different class. The border balls use a radius of 0.125 and a subset of the centres (0.25, 0.25),
(0.5, 0.25), (0.75, 0.25), (0.25, 0.5), (0.75, 0.5), (0.25, 0.75), (0.5, 0.75) and (0.75, 0.75) while the
center ball uses a radius of 0.01 and the center (0.5, 0.5). We generated randomly and uniformly 2000
data points for each border ball, however, as the classes would hence be unbalanced, we generated as
many data points for the center ball as there are in total for the border balls. We used a multilayer
perceptron (MLP) consisting of4 layers, all of width 2, and the ReLU activation function (as stated in
the theorem). For illustration purposes, we used an input dimension of 2 such that the neural network
functions are of the form F ∈ NNReLU(2). We used a batch size of 16, the Adam optimizer with
a learning rate of 0.001 and we trained each model for 500 epochs using the mean-squared error
(MSE) as our cost function. The different datasets and results are reported in Fig. 1. We rate an
experiment as successful when the universal uniform approximation condition (3) tends to zero. It can
be observed in Fig. 1 (a) that, when the conditions of Theorem 2 are fulfilled, the neural network can
approximate the function correctly and the different layers are learning transformations similar to the
constructions in the proof. When the conditions are violated, as is shown in Fig. 1 (b), the universal
uniform approximation condition does not tend to zero and the intermediate layer transformations
need to violate the constructions in the proof. Additional results are reported in Fig. 2 in the appendix.
7	Conclusion
We identified a maximum principle that holds in the case of width less than or equal to the input
dimension for all common activation functions, which can be interpreted as a root cause why universal
approximation is not possible under the aforementioned conditions. On the positive side we proved
8
Under review as a conference paper at ICLR 2022
(a)	7 border balls - center ball radius: 0.01 - MSE: 0.0 - UUAC: 0.0
(b)	8 border balls - center ball radius: 0.01 - MSE: 0.01135 - UUAC: 1.08494
Figure 1: Datasets (a) and (b) together with the learned transformations by each activation function
and layer. When the conditions of Theorem 2 are fulfilled (a), the transformations are similar to the
construction in the proof. Otherwise (b), the construction in the proof is violated. We report the final
decision regions, the mean-square error (MSE) and the universal uniform approximation condition
(UUAC). Each subplot’s title describes the function which was applied to the previous subplot’s data.
that, although being not sufficient for universal approximation in the general sense, ReLU network
functions of width less than or equal to the input dimension are sufficient to implement functions
that give zero or arbitrary small training error in some machine learning tasks. In particular, we
have shown that for the case of two disjoint compact sets, exact fit in a two class classification task
is feasible with 4 layers, provided the sets can be separated by a cone like sector. However, there
remains a gap between our positive results and limitation implied by the maximum principle. Our
numerical experiments show that neural networks learn transformations similar to our theoretical
derivations.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
This paper is a contribution to the theory of neural networks, which includes some numerical examples
to illustrate our findings. For this purpose, we work only with artificial two-dimensional data and
simple network architectures, each of which is described in detail and is therefore easily reproducible.
Further, we provide a code implementation in the supplementary material to reproduce the results of
our paper. A proof is given for all the theorems, propositions and lemma either in the main paper or
in the appendix.
10
Under review as a conference paper at ICLR 2022
References
Hans-Peter Beise, Steve Dias Da Cruz, and Udo Schroder. On decision regions of narrow deep neural
networks. Neural Networks,140:121-129, 2021.
SebaStien Bubeck, Ronen Eldan, Yin Tat Lee, and Dan Mikulincer. Network size and weights size
for memorization with two-layers neural networks. arXiv preprint arXiv:2006.02855, 2020.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on Learning Theory (COLT), pp. 698-728, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, 1989.
Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. arXiv preprint
arXiv:2012.14501, 2020.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and ReLU
activations. Mathematics, 7(10):992, 2019.
Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. 2019.
Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on
Learning Representations (ICLR), 2017.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):
251-257, 1991.
Jesse Johnson. Deep, skinny neural networks are not universal approximators. In International
Conference on Learning Representations (ICLR), 2018.
Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Conference
on Learning Theory (COLT), pp. 2306-2327, 2020.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
Journal of Statistical Physics, 168(6):1223-1247, 2017.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 6232-6240, 2017.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14(06):829-848, 2016.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
2014.
Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. Neural networks should be wide
enough to learn disconnected decision regions. In International Conference on Machine Learning
(ICML), 2018.
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin. Provable memorization via deep neural
networks using sub-linear parameters. arXiv preprint arXiv:2010.13363, 2020.
Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approximation.
In International Conference on Learning Representations (ICLR), 2021.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using
deep ReLU neural networks. Neural Networks, 108:296-330, 2018.
11
Under review as a conference paper at ICLR 2022
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In International Conference on Machine Learning
(ICLR),pp. 2847-2854. PMLR, 2017.
David Rolnick and Max Tegmark. The power of deeper networks for expressing natural functions. In
International Conference on Learning Representations (ICLR), 2018.
Walter Rudin. Real and complex analysis. Tata McGraw-hill education, 2006.
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. In ICML, 2018.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory (COLT),
pp. 1517-1539, 2016.
Roman Vershynin. Memory capacity of neural networks with threshold and rectified linear unit
activations. SIAM Journal on Mathematics of Data Science, 2020.
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:
103-114, 2017.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In
Conference On Learning Theory (COLT), pp. 639-649, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a
tight analysis of memorization capacity. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR), 2017.
12
Under review as a conference paper at ICLR 2022
A Appendix
Proof Proposition 2
Proof. We start with the proof of (1). It is clear that PU followed by a linear isomorphism E : U →
Rm with m = dim(U) < n0 can be implemented by x 7→ W1x + b1 with some W1 ∈ Rm×n0,
b1 ∈ Rm. As M is compact and hence bounded, one can further arrange b1 in a way that W1x+b1 ≥ 0
holds (component wise) for every x ∈ M, which implies that ReLU(W1x + b1) = W1x + b1 for all
x ∈ M. Thus g(x) = ReLU(W1x + b1) maps M bijectively to M1 := ReLU(W1M + b1) ⊂ Rm.
Now, uniform approximation of a given continuous function f from M to R amounts to uniform
approximation of continuous function from M1 to R. By Tietze’s Extension Theorem (Rudin, 2006)
we can apply Theorem 1 from Hanin & Sellke (2017) which yields arbitrary accurate uniform
approximation of continuous function on rectangular domains in Rm by network functions in
NN ReLU (m + 1).
To show (2), let W1 ∈ R1×n0 and b1 ∈ R such that yj = W1xj + b1, j = 1, ..., m, are distinct points
M1 = {y1, ..., ym} ⊂ [0, ∞). It is then clear that also F0(x) := ReLU(W1x + b1) maps M to M1.
Now, for some function f : M → R (which is automatically continuous as M is finite) we have to
find a network function F1 ∈ NN ReLU (2) such that F1(yj) = f(xj) for j = 1, ..., m. To this end,
we observe that the mapping yj 7→ f(xj) j = 1, ..., m can be interpolated by a max-min string, see
Hanin & Sellke (2017, Definition 1) for a definition, so that Hanin & Sellke (2017, Proposition
2) yields the desired F1 . Alternatively, one could follow the proof of Hardt & Ma (2017, Theorem
3.2.), observing that on the non-negative yj, j = 1, ..., m, the residual ReLU network function
constructed in the latter can be written as a ReLU network function in the sense of this work with
width two in our case. Finally, we have that F = F1 ◦ F0 ∈ NN ReLU (2) and F(xj) = f(xj) for all
j = 1,…，m.	□
Proof Proposition 4
Proof. It is clear that the existence of inner points in F (M) requires that every weight matrix of F
is square and of full rank. Indeed, otherwise the range of M is reduced to a manifold of dimension
less than or equal to n0 - 1 after a layer that violates this condition. From then on the range remains
without inner points. If L = 1, then we are done, since in this case F reduces to an isomorphism
on Rn0. Otherwise, let Mj := Aj (Mj-1) for j = 1, ..., L and M0 := M. Hence F (M) = ML.
Since WL is non-singular, uL ∈ ML is an inner point of ML if and only if uL = WLuL-1 for some
unique point ul-i ∈ ML-ι∙ This implies that component wise ul-i > 0 and further, by the fact
that Wl-i is non-singular, ul-i = WL-IuL-2 + bL-ι for some unique point ul-i ∈ ML-2. The
exact same reasoning can now iteratively be applied down to the first layer. This gives a sequence of
uj ∈ Mj, j = 1,…，L so that component wise uj > 0 holds for every j. Hence, uj = Wjuj-ι + bj
for j = 1,…,L. Thus, for the inverse image B = FT (F(M)o) ⊂ Mo, the mapping F reduces to
an bijective linear affine mapping.	□
13
Under review as a conference paper at ICLR 2022
Numerical Experiment
(a) 6 border balls - center ball radius: 0.01 - MSE: 0.0 - UUAC: 0.0
(b) 6 border balls - center ball radius: 0.05 - MSE: 0.00026 - UUAC: 0.99092
Figure 2: Additional results for Fig. 1 of the main paper. Different input datasets (a) and (b) together
with the resulting transformations by each activation function and each layer learned by the neural
network after 500 epochs. We also report the final decision regions, the mean-square error (MSE)
and the universal uniform approximation condition (UUAC). When the conditions of Theorem 2 are
fulfilled (a), the learned transformations are similar to the construction in the proof of Theorem 2.
When the condition of Theorem is violated (b), the intermediate layers also violate the construction in
the proof. The title of each subplot describes the function which is applied to the data of the previous
subplot to form the data of the current subplot.
14