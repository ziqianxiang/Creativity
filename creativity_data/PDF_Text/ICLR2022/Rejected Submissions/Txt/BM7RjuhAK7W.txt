Model-Invariant	State Abstractions	for
Model-Based Reinforcement Learning
Anonymous authors
Paper under double-blind review
Abstract
Accuracy and generalization of dynamics models is key to the success of model-
based reinforcement learning (MBRL). As the complexity of tasks increases, learn-
ing accurate dynamics models becomes increasingly sample inefficient. However,
many complex tasks also exhibit sparsity in dynamics, i.e., actions only have a local
effect on the system dynamics. In this paper, we exploit this property with a causal
invariance perspective in the single-task setting, introducing a new type of state
abstraction called model-invariance. Unlike previous forms of state abstractions,
model-invariant state abstraction leverages causal sparsity over state variables.
This allows for compositional generalization to unseen states, something that non-
factored forms of state abstractions cannot do. We prove that an optimal policy
can be learned over exact model-invariance state abstraction and show improved
generalization in a simple toy domain. Next, we propose a practical method to
approximately learn a model-invariant representation for complex domains and
validate our approach by showing improved modelling performance over standard
maximum likelihood approaches on challenging tasks, such as the MuJoCo-based
Humanoid. Finally, within the MBRL setting we show strong performance gains
with respect to sample efficiency across a host of continuous control tasks.
1	Introduction
Model-based reinforcement learning or MBRL [4, 15] is a popular framework for data-efficient
learning of control policies. At the core of MBRL is learning an environmental dynamics model and
using it to: 1) fully plan [14, 11], 2) augment the data used by a model-free solver [51, 54], or 3)
use as an auxiliary task while training [35, 58]. However, learning a dynamics model — similar to
other supervised learning problems — suffers from the issue of generalization, since the data we
train on is not necessarily the data we test on. This is a persistent issue that is worsened in MBRL as
even a small inaccuracy in the dynamics model or changes in the control policy can result in visiting
completely unexplored parts of the state space [1]. This advocates for the need to learn models
capable of generalizing well. Various workarounds for this issue have been explored in the past; for
example, combining local but simple models with global, more expressive models [36, 22], using an
ensemble of models to handle uncertainty in estimates [11, 33] and coupling the model and policy
learning processes [34] so that the model is always accurate to a certain threshold. However, these
approaches do not stem from a representation learning viewpoint and thus fail to leverage special
structure in dynamics for better generalization.
This paper studies how to improve generalization capabilities through careful state abstraction. In
particular, we leverage two existing concepts to define a new kind of state abstraction. The first
concept is that many real world problems exhibit sparsity in the local dynamics — given a set of state
variables, each variable at timestep t +1only depends on a small subset (i.e., local) of all variables
in the previous timestep t (see Figure 1). The second concept is the principle of causal invariance,
which dictates that given a set of candidate features, we should aim to build representations that
comprise only those features that are consistently necessary for predicting the target variable of interest
across different interventions [46]. In the MBRL context, we can cast model learning with causal
invariance as a supervised objective where the target variables are the next state variables and input
features are the current state and action variables (the probable set of causal predictors of the target).
Intuitively, since we learn a predictor that only takes into ac-
count invariant features that consistently predict the target vari-
able well, it is likely to contain the true causal features, and
therefore, will generalize well to all possible shifts in the data
distribution. The two concepts of sparsity and causality are
intertwined in that they both are forms of inductive biases that
surround the agent dynamics [23]. This paper shows how causal
invariance tools can be effectively used to exploit sparsity in dy-
namics, leading to improved model generalization. Given basic
exploratory assumptions, we analyze this question theoretically
and show empirically that we can learn a model that generalizes
well on state distributions induced by policies distinct from the
ones used while learning it. To do this, we introduce a new
state abstraction, model-invariance, which leverages sparsity
over state variables. We connect this abstraction viewpoint to a
concrete problem of generalization in model-based RL, that of
……
("1)—►("&")
Figure 1: Graphical model of spar-
sity across state variables. The state
variable xt3+1 (shaded in blue) only de-
pends on two variables xt3 and xt2 (in
the blue box). Predicting xt3+1 using
the other variables (say xt1+1 ) can re-
sult in spurious correlations which lead
to poor generalization.
arising spurious correlations in the dynamics model, even when all state variables are task relevant.
Having built enough intuition, we then introduce a practical method that approximates learning
a model-invariant representation for more complex domains that use function approximation (i.e.,
neural networks (NNs)). We empirically observe that model invariance leads to better model general-
ization for domains such as the MuJoCo-based Humanoid. Our method is simple to implement and
shows strong performance on multiple MuJoCo tasks, outperforming state of the art model-based
methods in the low-data regime.
2	Preliminaries
RL Setup. We consider the agent’s interaction with the environment as a discrete time γ-discounted
Markov Decision Process (MDP) [47] M = (X, A, P, R, Y, μ0), where X ⊆ Rd is a finite but
arbitrarily large state space and A is the and action space; P ≡ P x0 |x, a is the transition kernel;
R ≡ r(x, a) is the reward function with the maximum value of Rmax; γ ∈ [0, 1) is the discount
factor; and μo is the initial state distribution. Let π : X → △/ be a stationary memoryless policy,
where ∆A is the set of probability distributions on A. The value function of a policy π at a state
x ∈Xis defined as Vπ(x) ≡ E[P ≥0 γtr(xt, at)|x0 = x, π]. Similarly, the action-value function
of π is defined as Qπ(x, a)=E[P ≥0 γtr(xt, at)|x0 = x, a0 = a, π]. The Bellman optimality
operator T : RIX×Al → RIX×Al is defined as TQ(x, a) = r(x, a) + Y〈P(∙∣x, a), max。，Q(∙, a0)).
In this work, we assume that all d state variables x1, x2,..., xd are useful for the task in hand, i.e., we
are given the full state. Furthermore, we assume that the transition dynamics over the full state are
factorized. More formally:
Assumption 1. (Transition Factorization) For given full state vectors xt,xt+1 ∈X, action a ∈A,
and xi denoting the ith dimension of state x we have P xt+1 |xt, a = P xit+1 |xt,a .
Note that this is a weaker assumption than factored MDPs [31, 24] as we do not assume a correspond-
ing factorization of the reward function.
Invariant Causal Prediction. Invariant causal prediction (ICP) [46] considers the problem of
learning an invariant representation w.r.t. spurious correlations that arise due to noise in the underlying
causal model (unknown) describing a given system. The key observation is that if one considers the
direct causal parents of a response/target variable of interest (Y ), then the conditional distribution
of Y given these direct causes PA(Y ) does not change across interventions on any variable except
Y . Therefore, ICP suggests collecting data into different environments (corresponding to different
interventions), and to output the set of variables Xi for which a learned predictor of Y remains the
same given Xi across the multiple environments with high probability.
State Abstractions. State abstractions allow us to map behaviorally-equivalent states into a single
abstract state, thus simplifying the learning problem, which then makes use of the (potentially
much smaller set of) abstract states instead of the original states [5]. In principle, any function
approximation architecture can act as an abstraction, since it attempts to group similar states together.
Therefore, exploring the properties of a representation learning scheme as a state abstraction help
develop stronger intuition for building practical algorithms. In the next section, we build our
theory based on this connection. A well known state abstraction is bisimulation [19, 48, 37].
Formally, an abstraction φ : X 7→ S is a bisimulation if for any two states x1,x2 and next state
x ∈X, abstract state s ∈S, a ∈ A where φ(x1) = φ(x2), we have R(x1,a)=R(x2,a), and
x∈φ-1(s)P x|x1,a =	x∈φ-1(s)P x|x2 ,a . Since an exact equivalence is not practical, prior
work deals with approximate variants through the notion of e-closeness [29].
3	Model Invariance and Abstractions
Current state abstractions such as bisimulation do not provide support for sparse structures. Instead,
closeness in abstract states is defined based on probabilities of all state variables together, i.e., x.To
remedy this, we introduce a new state abstraction, called model-invariance, which is specific to each
state variable xi . Formally, an invariant abstraction φi is one which has the same transition probability
of the ith next state variable xi for any two given states x1 and x2, i.e., P xi|x1,a = P xi|x2, a .
Note that if we assume factored rewards, we can define a corresponding reward-based invariant
abstraction that parallels the bisimulation abstraction more closely, but we focus here on the reward-
free setting. Since it is impractical to ensure this equivalence exactly, we can use an approximate
definition which ensures an e-closeness.
Definition 1. (Approximate Model Invariance) φ is ei,P -model-invariant if for each index i,
sup	∣∣P(xi∣xι, a) — P(Xi∣X2, a) J] ≤ e*p.
a∈A,X,Xl ,X2 ∈X,φ(xι )： = 0(X2 )
φ is eR-model-invariant ifeR = suPa∈A,χ1,χ2∈X,φ(x1)=φ(x2) IR(Xba) - R(x2,a)∣ .
Based on the definition, it intuitively seems like applying the abstraction over the original transition
distribution should be close to the transition distribution over abstract states. This can be precisely
written in the following lemma, which is a necessary result in ensuring that an optimal policy can still
be learnt over a model-invariant abstraction:
Lemma 1. (Model Error Bound) Let φ be an ei,P -approximate model-invariant abstraction on MDP
M. Given any distributions ps : si ∈ φi(X) where ps is supported on φ-1(si) and ps = Qd=1 ps ,
we define Mφ = (φ(X), A,Pφ,Rφ, Y) where Pφ(s,a) = Ex〜PS [P(∙∣x,a)] and Rφ = Ex〜PS
R(X, a) . Then for any X ∈X, a ∈A,
kPφ(s, a) — ΦP (X, a)k ≤ Xei,P,
i=1
where ΦP denotes the lifted version of P, where we take the next-step transition distribution from
observation space X and lift it to latent space S (Proof in Appendix 2). Pφ refers to the transition
probability of a MDP that acts on the states Φ(X), rather than the original MDP. Finally, note that
we are particularly concerned with the case where each Xi is atomic in nature, i.e., it is not further
divisible. Such a property ensures that model-invariance does not collapse to bisimulation1.
4	Causal Invariance in Model Learning
Having defined model-invariant abstractions, we are now ready to provide connections between
causal invariance and model learning in RL. For simplicity, we can assume that there exists a linear
structural equation model [45] that consists of the d state variables and action a as the features X and
the next state variable Xit+1 as the target Y , for each index i. Similar to the ICP setting(Section 2),
we can define the different environments as follows:
Assumption 2. (ICP Environments) For each e ∈E: the experimental setting e arises due to
one or several interventions on variables from X1,...,Xd,at but not on Xi+1; here, we allow for
do-interventions [44] or soft-interventions [18].
1In the simplest case index i describes each state dimension, i.e., being atomic. However in general the index
could be a grouping of different dimensions as well. Consider the case where there is only one index over which
we build a model invariance abstraction. Now x1 would then correspond to the entire state x. Since Definition 1
would be satisfied trivially, we do not gain anything on the sparsity level, thus collapsing to bisimulation.
For our purposes, each intervention corresponds to a change in the current state and action distribution.
This change in distribution can be realized as a change in the policy. Therefore, each policy π defines
an ICP environment e. It can be shown that under Assumption 2 the direct causes, i.e., parents of
xt+i, define a valid support over invariant predictors, namely S* = PA(xit+1 ). 2. The key idea
therefore is to make sure that in predicting each next state variable xit+1 we use only its set of invariant
predictors and not all state variables and actions (see Figure 1). With this intuition, it becomes clearer
why our original model learning problem is inherently tied with learning better representations, in
that having access to a representation that discards excess information for each state variable (more
formally, a causally invariant representation), would be more robust to spurious correlations and
thus, at least in principle, lead to improved generalization performance across different parts of the
state space. In fact, such a causally invariant representation obeys the properties of a model-invariant
abstraction. Formally,
Proposition 1. For the abstraction φi (x) = [x]S , where Si = PA(xti+1), φi is model-invariant.
Proof is provided in Appendix 2. It now becomes easy to see that sparsity in dynamics is central to
what we have discussed so far, since if we do not have sparsity, the causally invariant representation
trivially reduces to the original state x, thus resulting in no state aggregation. However, when sparsity
is present, following the above definition leads to a representation φ such that any two components
φi and φj are decorrelated, since each encodes information about a different subset of state variables.
Finally, we show that learning a transition model over a model-invariant abstraction φ and then
planning over this model is optimal.
Theorem 1. (Value bound) If φ is an er, ei,p approximate model-invariant abstraction on MDP M,
and Mφ is the abstract MDP formed using φ, then we can bound the loss in the optimal state action
value function in both the MDPs as:
[QMφ ]M - QM IL " ≤ Y~^ l∣[QMφ ]M - T [QMφ ]M∣∣2 J
kQMφ]M - T[QMφ]M∣∣2 μ ≤ eR + Y(X ei,p) 2(Rmaxγ),
where C refers to the concentrability coefficient as defined in [9]. Here, an admissible distribution
ν refers to any distribution that can be realized in the given MDP by following a policy for some
timesteps, while μ refers to the distribution from which the data is generated. Proof and all details
surrounding the theoretical results are provided in Appendix 2. This result is important because
We can follow this with standard sample complexity arguments that will have a logarithmic in M
dependence, thus guaranteeing that learning in this abstract MDP is faster and only incurs the above
described sub-optimality.
So far, we have embedded the RL model learning problem in the ICP framework. This highlights the
connection between generalization issues in MBRL and arising spurious correlations in identifying
the true causal predictors of each state variable. Furthermore, we have explored the state abstraction
properties of the causally invariant representations which avoid spurious correlations. In the next
section, we now show how ICP can be used as a sub-routine in learning a more generalized model in
the simple setting of a linear MDP.
5	Linear Case: Certainty Equivalence
In the simpler setting of tabular RL, estimating the model using transition samples and then planning
over the learned model is referred to as certainty equivalence [6]. Particularly for estimating the
transition model, it considers the case where we are provided with n transition samples per state-action
pair, (xt ,at ) in the dataset Dx,a, and estimate the model as
P(xt+i∣xt,at) = 1 X I(X = xt+i).
lx∈ Dx,a
2The proof follows directly by applying Proposition 1 of Peters et al. [46] (which itself follows from
construction) to each state variable indexed by dimension i
Now, when next state components do not depend on each other given the previous state and action
(i.e., Assumption 1), we can re-write P xt+1|xt,at as Q P xi+1 |xt,at . Furthermore, ifwe know
the causal parents of xi+1, we can instead empirically estimate the true transition probabilities as
P(xt+1lxt,at) = P(Xt+1|PA(Xt+J,at) = . X I(X = xt+1) ,	(I)
x∈D
k
where D =	Dx,a, X ∈ φ-1(x). From Proposition 1, We know that such an invariant solution
i=1
exists, and is defined by the causal parents of each state variable. Therefore, in the linear dynamics
case, given data from multiple environments (different policies), we can use ICP to learn the causal
parents of each state variable and then estimate the probability of a certain transition using Eq. 13 4 5 6 7 8 9. We
refer to this as the invariant model learner and detail the procedure in Algorithm 1. On the other hand,
if we do extract the causal parent variables and instead use all state variables in Eq. 1, we would get
the standard maximum likelihood (MLE) learner.
Algorithm 1 Linear Model-Invariant MBRL
1: Input Replay buffer D containing data from multi-
ple policies/envs, confidence parameter α;
2: for state variable i =1,...,ddo
3:	Si - ICP(i, D, α); (Appendix ??)
4:	Estimate Pi from D using Eq. 1;
5: end for
6: Estimate transition probability kernel P — ∏iPi
7: for state sj ,j=1,...,N do
8:	∏R(sj) — Plan(R, P, Sj)
9: end for
The invariance based solution avoids spurious
correlations more than the MLE learner, result-
ing in better generalization. To see this, con-
sider a simple linear MDP with three state vari-
ables (X1 , X2, X3), each depending only on its
own (PA(Xi+1) = Xit), and taking integer val-
ues between [-10, 10]. The exact details of the
MDP are described in Appendix 3.1. We con-
sider three different distributions corresponding
to three different policies, each describing an
ICP environment. In Table 1, we compare our
invariant learner with a standard MLE learner
and show how their error with the true probability of transition varies as the number of samples
grows.
Samples	MLE	Model-Invariance
100	-± -	9.3 ± 1.3
200	24.6 ± 9.23	5.7 ± 1.5
500	14.5 ± 2.75	2.7 ± 0.7
2000	9.6 ± 1.72	1.8 ± 0.3
5000	6.4 ± 1.46	1.6 ± 0.3
Table 1: Linear MDP Transition Prediction Error.
Consider the simple linear MDP from Appendix 3.1.
We compare the estimated transition probability of our
invariant learner with the MLE learner (lower is better).
The invariant learner converges faster and more stably
to the true solution. Mean and std. err. over 15 random
seeds. Order of magnitude of the errors is 1e-3.
Note that Table 1 represents the results for one
environment, specified by a fixed policy that is
used for data collection. If the policy changes,
it would result in a different environment as de-
scribed in Section 3. Our ideal scenario is to
find a predictive model that is closest to the true
model for all environments. We find that the
invariant learner quickly converges to approxi-
mately the same solution across all training envi-
ronments, with just a few data samples. On the
other hand, the standard MLE learner results in
different solutions for each training environment
in the low data regime. The solution provided at
test time in such a case is an average of all such
solutions found during training, which is clearly off the true probability (higher error in Table 1).
It is worth noting that this example assumes linearity in dynamics, which allows us to use the ICP
procedure from Peters et al. [46]. In the general non-linear case, this is not possible. To that end, in
the next section we will describe a practical method that leverages ideas from self-supervised learning
to exploit sparsity in an end-to-end manner.
6	Non-linear Case: Learning Practical Model-Invariant
Representations
We now introduce a practical algorithm for learning model-invariant representations where we relax
the following assumptions: linearity in dynamics, having access to data from multiple environments,
3Using Theorem 1, we know that performing planning over this estimated dynamics model would be
e-optimal.
Algorithm 2 Non-linear Model-Invariant MBRL
1:	Input Replay buffer D = 0; Value and policy
network parameters θQ, θπ, model parameters θr,
θf for any MBRL algorithm;
2:	for environment steps t =1,...,T do
3:	Take action at 〜 π(∙∣xt), observe rt and xt+ι,
and add to the replay buffer D;
4:	for Mmodel-free updates do
5:	Sample batch {xj, aj, rj, xj+1}jN=1 from D;
6:	Run gradient update for the model free com-
ponents of the algorithm (e.g., θπ , θQ etc.);
7:	end for
8:	for Mmodel updates do
9:	Sample batch {xj, aj, rj, xj+1}jN=1 from D;
10:	Update reward model (θr);
11:	Update invariant dynamics model: θf —
invariant_update(θf, Vθy Lf) (Appendix 3.2
or Eq. 2);
12:	end for
13:	end for
Figure 2: Architecture for learning model-invariant
representations. Model-invariance uses two (or more)
transition dynamics models, denoted by f and h, over a
common representation φ. The critic g provides a score
for a chosen dimension/state variable of the output of
f and h models.
and being in the strictly batch setting. Having connected the abstraction viewpoint to the problem of
spurious correlations arising in dynamics models, we wish to come up with NN representations that
abstract away irrelevant information on a per-state-variable level (i.e., xi, not xt) and learn them in an
end-to-end manner. To that end, we view the task of learning such representations as a self-supervised
objective where we want to be invariant to models that exhibit spurious correlations. Consider two
(or more) randomly initialized and independently trained (on different samples) dynamics models
attached to a common representation φ. We can view these models as augmented versions of the true
dynamics models since each model captures different spurious correlations. We must be invariant to
augmentations in the model space on a per-state-variable level (since spurious correlations arise for
individual state variable dynamics).
To instantiate this idea, we take inspiration from a recent method called ReLIC [40], which uses a
contrastive loss [41] over augmented views of the same data sample and an invariance loss to enforce
consistency. ReLIC was shown to be closely connected to doing causal interventions over input
variables and enforcing invariance over certain functions (like data augmentations). Here we are
exploiting the same connection, with additional modifications. Instead of using augmentations over
inputs, we induce augmentations over models and then define the invariance loss for a particular
dimension — the goal is to eliminate the contribution of spurious features when predicting a particular
state variable. Note that doing exact causal interventions is not possible in the large state-action
spaces in RL but we can still make strong connections to the causal literature and also use it in simpler
settings, as we do in Section 5 (linear case) where we deploy exact invariance tools.
After randomizing two (or more) identical models at the start of training, a model is sampled
randomly and is used for minimizing the standard MLE model predictive loss at each optimization
step. Simultaneously, an invariance loss defined over the predictions of both models augments
the MLE loss. The invariance loss enforces consistency in the prediction of both (all) models by
minimizing the difference in similarities between the prediction of one model w.r.t. the prediction
of the other model and vice versa (Eq. 2). This similarity is computed for a single (randomly
selected) state variable at each optimization step, with the specifics being borrowed from the ReLIC
objective Mitrovic et al. [40]. Finally, further consistency is encouraged because all models have a
common representation with a a bottleneck structure. Our implementation is detailed in Figure 2
and as pseudocode in Appendix 3.2. Although this method is designed to be robust to spurious
correlations in dynamics models, in our experiments we will show that the representations learnt
for each index i, φi actually end up being sparse, a property that motivated the model invariance
abstraction back in Definition 1. The overall model invariance loss is:
Lf = Ex 〜D
h(f (xt,at)-xt+ι)2+ KL (pfh,Phf )],
Pfh = Softmax(ψi(f, h)),	(2)
^^^^{^^^^™
MLE Loss
} |-----------{---------}
Invariance Loss
|
w/o model-inv.
w/ model-inv.
Figure 4: Invariant Model Learning on Humanoid-v2. Left plot: test model learning error for different
horizon values. Mean and std. err. over 10 random seeds. Middle plot: modelling error when we enforce
per-state vs per-dim invariance. Table: mean correlation metric; order of magnitude is 103.
mean corr. in φi
1.63 ± 0.48
1.25 ± 0.53
where
ψi(f, h) = (g(fi(Xt,at)), g(hi(Xt,at)))
is the cosine similarity between the predictions for
the models f and h for the state variable indexed by i. The output of the inner product is normalized
for each sample and thus the resulting vector g fi (Xt ,at) is a probability distribution. In our
implementation, the function g is a fully connected neural network and is used to project the outputs
of the models f and h. It is often called the critic network in self-supervised learning losses [10].
Note that the matrix ψi is not symmetric since the values at any two symmetric indices are different
(they are computed for different samples) — the KL loss remains well-defined.
We will use the invariant model learner described above within a model-based RL algorithm and
compare its policy performance to a standard MLE based model learner. A general framework that
uses an invariant model learner is outlined in Algorithm 2. For the purposes of this paper, we employ a
simple actor-critic setup where the model is used to compute multi-step estimates of the Q value used
by the actor learner. A specific instantiation of this idea of model value expansion is the SAC-SVG
algorithm proposed in Amos et al. [2] (see Appendix 3.3 for details). It is important to note that the
proposed version of model-invariance can be used in combination with any MBRL method and with
any type of dynamics model architecture, such as ensembles or recurrent architectures.
7 Experiments
Presence of Spurious Correlations. We first test the pres-
ence of spurious correlations in the Humanoid-v2 [53] task
by choosing to predict a single dimension (the knee joint)
in two contrasting settings; 1) No_Mask: when all state
and action variables are provided as input and, 2) Mask:
when the state variables that are likely uncorrelated to the
knee joint are masked (see Appendix 3.5). Having trained
different models for the two settings, we observe that 1)
No_Mask: performs worse than 2) Mask:, for both hori-
zon values in {3, 5} (see Figure 3). This verifies that there
indeed is an invariant, causal set of parents among the state
dimensions and that there could be some interference due
to spurious correlations in 1), and thus, it performs worse
than case 2). Furthermore, when the dimensions that are
likely to be useful in predicting the knee joint are masked,
(a) Horizon=3
(b) Horizon=5
Figure 3: Effect of spurious correlation on
model learning test loss of Humanoid-v2 for
a single dimension (knee joint) with two set-
tings: Mask and No_mask. 10 seeds, 1 std.
dev. shaded. Y-axis magnitude order is 1e-3.
then the model error is the highest (worst; not shown in figure). Note that since all variables are
necessary for the task in hand, simply discarding some of them is a not a solution to the spurious
correlation problem.
Invariant Model Learning on Humanoid-v2. We compare the invariant model learner to a standard
MLE learner for the Humanoid-v2 task. To observe the effect of the invariance loss clearly, we
decouple the model learning component from the policy optimization component by testing the model
on data coming from the replay buffer of a pre-trained model-free SAC agent. Such a setup ensures
that the changes in state distribution according to changes in policy are still present but any changes
in the model do not actively affect this distribution, thus clearly reflecting the effect of model learning
only.
We observe that our invariant model learner performs much better than the standard model learner,
especially when the number of samples available is low, i.e., around the 200K to 500K mark (see
Figure 4, left plot). As the number of samples increases, the performance between both models
converges, just as observed in the tabular case. This is expected since in the infinite data regime, both
solutions (MLE and invariance based) approach the true model. Furthermore, we observe that the
number of samples it takes for convergence between the standard and the invariant model learners
increases as the rollout horizon (H in Figure 4) of the model learner is increased. Next, we plot
the test error when the invariance loss is computed for all state variables and compare it to when it
computed for a particular state variable (Figure 4, middle plot). We see that the per-state version
performs worse than the per-state-variable or per-dim version, showing the importance of enforcing
invariance at the state variable level. Finally, we test the mean correlation in the dimensions of the
learnt representation φ with and without model-invariance (Figure 4, right table). We see that the
mean correlation is lower with model-invariance than without, suggesting that the practical algorithm
does induce the sparsity property discussed in the earlier part of the paper.
POPLIN	Cheetah	Walker	Hopper	Ant
*PETS	2288 ±510	-282 ± 250	114 ± 311	1165 ± 113
*POPLIN-A	1562 ±568	-105 ± 125	202 ± 481	1148 ± 219
*POPLIN-P	4235 ±566	597 ± 239	2055 ± 206	2330 ± 160
* METRPO	2283 ± 450	-1609 ±328	1272 ± 250	282 ± 9
*SAC	4035 ± 134	-382 ± 424	2020 ± 346	836 ± 34
SAC-SVG H-3	6530 ±382	80 ± 472	1108 ±263	2293 ± 397
Ours H-3	7067 ±269	-150 ± 556	1724 ± 271	3124 ± 199
Table 2: POPLIN Invariant MBRL performance Invariant MBRL performance reported at 200k steps on
four MuJoCo based domains from POPLIN [55]. * represents performance reported by POPLIN. Standard
error with 10 seeds reported. We bold the scores with larger mean values.
MBPO	Cheetah	Walker	Hopper	Ant	Humanoid
SAC-SVG H-3	8957 ±532	3795 ±503	3201 ± 101	3997 ± 153	1712 ± 415
OURS H-3	9109 ±334	3961 ±239	3382 ± 84	4546 ± 286	2443 ± 561
SAC-SVG H-4	8327 ±870	3494 ±392	3291 ±232	4470 ± 307	2404 ± 495
OURS H-4	9663 ± 487	4347 ±136	3489 ± 19	4565 ±221	3506 ± 538
SAC-SVG H-5	5710 ±1329	2773 ±492	3059 ± 276	3808 ±531	3190 ±601
OURS H-5	5796 ±855	3326 ±430	3207 ± 210	3817 ±488	3446 ± 481
Table 3: MBPO Invariant MBRL performance Invariant MBRL performance reported at 200k steps on five
MuJoCo based domains from MBPO [28]. Standard error with 10 seeds reported. We bold the scores with
larger mean values.
Invariant Model-based Reinforcement Learning. Finally, we evaluate the invariant model learner
within the policy optimization setting of SAC-SVG [2]. We compare the difference in performance to
SAC-SVG when the horizon length is varied (see MBPO environments in Table 3 and Appendix 3.4)
and then compare the performance of our method against multiple model based methods including
PETS [11], POPLIN [55], METRPO [32], and the model free SAC [27] algorithm (see POPLIN
environments in Table 2 and Appendix 3.4). The results in Table 3 show a consistent improvement
in performance when the invariant model learner (Ours) is used instead of the standard model
learner (SAC-SVG) across most tasks, including the Humanoid-v2. Furthermore, we see that as the
horizon length is increased, the difference in performance between the invariant and standard learners
increases for most tasks. A comparison of model training errors for both cases is provided in Table 5
in Appendix. We see a consistently higher training loss for the invariant learner, indicating less over
fitting. Finally, in Table 2 we see that the invariant learner outperforms all six baselines on 3 out 4
tasks, with the exception of Walker, where most methods fail to reach a positive score. Combining our
invariant model learner with other policy optimization algorithms is therefore a promising direction
for future investigation.
8	Related Work
Planning based on structural assumptions on the underlying MDP have been explored in significant
detail in the past [7]. The most closely related setting is of factored MDPs, but approaches that
build on the factored MDP assumption have predominantly also assumed a known graph structure
for the transition factorization [31, 50, 42]. The factored MDPs literature has carried two distinct
ideas all along, one of sparsity and the other of factorized transitions/rewards. This paper focuses
predominantly on sparsity. We rely on factorization for some theoretical results but the practical
algorithm does not require factorization to work. Sparsity leads to the abstraction viewpoint introduced
in this work, which allows us to scale our formalism to practical methods. It allows us to connect the
initial formalism to the concrete problem of neural network generalization (without an abstraction-
based formalism, it does not make sense to use neural networks).
On a separate axis, papers that deal with learning the graph structure and then doing RL [50, 16, 26],
do so by constructing close to optimal transition functions, and then planning. In our practical method,
we focus on only avoiding spurious correlations, which is computationally more efficient since by
the time an exact model of the MDP can be learnt (usually done by checking all possible parent set
candidates), most model-free methods can just learn the reward function and perform much better.
Degris et al. [13] learn decision tree based models online while we focus on learning NN based
models. Extending such exact methods to NN models remains an open problem. Guestrin et al. [25]
state a similar idea to this work but simply assume that one is given basis functions that have limited
scope (model-invariant abstractions).
As mentioned, other forms of state abstraction such as bisimulation [37, 58] do not consider sparsity
whereas model-invariance does. Therefore, both forms of abstraction are in fact orthogonal to each
other, and therefore can be combined on top of one another. In similar essence, a lot of works
have proposed value-aware model learning objectives, which only model the minimal information
required to predict the value function of the task [20, 17, 12]. This is again a complimentary idea to
model-invariance, since we consider states spaces that are already minimal w.r.t. the value function,
i.e., no task irrelevant information is present.
9	Conclusion and Future Directions
This paper introduced a new type of state abstraction for MBRL that exploits the inherent sparsity
present in many complex tasks. We first showed that a representation that only depends on the
causal parents of each state variable follows this definition and is provably optimal. Following, we
introduced a novel approach for learning model-invariant abstractions in practice, which can be
plugged in any given MBRL method. Experimental results show that this approach measurably
improves the generalization ability of the learnt models. This stands as an important first step to
building more advanced algorithms with improved generalization for systems that possess sparse
dynamics.
In terms of future work, there remain multiple exciting directions and open questions. First, to
enable model-invariance, we could also look at other kind of approaches proposed recently such
as the AND mask [43]. The AND mask specifically requires the data to be separated into multiple
environments, and thus naturally suits the offline RL setting. Second, moving to pixel based input,
the representation learning task becomes two-fold, including learning to abstract away the irrelevant
information present in the pixels and then learning a model-invariant representation. Third, note
that our theoretical results do not involve an explicit dependence on a sparsity measure, for example,
the maximum number of parents any state variable could have. Including such a dependence would
ensure tighter bounds. Fourth, it is worth asking how such an explicit constraint on model-invariance
can perform as a standalone representation learning objective, considering the strong progress made
by self-supervised RL.
References
[1]	Pieter Abbeel, Morgan Quigley, and Andrew Y Ng. Using inaccurate models in reinforcement
learning. In Proceedings ofthe 23rd international conference on Machine learning, pp. 1-8,
2006.
[2]	Brandon Amos, Samuel Stanton, Denis Yarats, and Andrew Gordon Wilson. On the
model-based stochastic value gradient for continuous reinforcement learning. arXiv preprint
arXiv:2008.12775, 2020.
[3]	Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-
mization. arXiv preprint arXiv:1907.02893, 2019.
[4]	Christopher G Atkeson and Juan Carlos Santamaria. A comparison of direct and model-based
reinforcement learning. In Proceedings of international conference on robotics and automation,
volume 4, pp. 3557-3564. IEEE, 1997.
[5]	D. P. Bertsekas and D. A. Castanon. Adaptive aggregation methods for infinite horizon dynamic
programming. IEEE Transactions on Automatic Control, 34(6):589-598, 1989. doi: 10.1109/9.
24227.
[6]	Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 1st
edition, 1995. ISBN 1886529124.
[7]	Craig Boutilier, Thomas Dean, and Steve Hanks. Decision-theoretic planning: Structural
assumptions and computational leverage. Journal of Artificial Intelligence Research, 11:1-94,
1999.
[8]	Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[9]	Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement
learning. In International Conference on Machine Learning, pp. 1042-1051. PMLR, 2019.
[10]	Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International Conference on Machine
Learning (ICML), pp. 1597-1607, 2020.
[11]	Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems (NIPS), pp. 4754-4765, 2018.
[12]	Brandon Cui, Yinlam Chow, and Mohammad Ghavamzadeh. Control-aware representations for
model-based reinforcement learning. arXiv preprint arXiv:2006.13408, 2020.
[13]	Thomas Degris, Olivier Sigaud, and Pierre-Henri Wuillemin. Learning the structure of factored
markov decision processes in reinforcement learning problems. In Proceedings of the 23rd
international conference on Machine learning, pp. 257-264, 2006.
[14]	Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efficient approach to
policy search. In International Conference on Machine Learning (ICML), pp. 465-472, 2011.
[15]	Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen. Gaussian processes for
data-efficient learning in robotics and control. IEEE transactions on pattern analysis and
machine intelligence, 37(2):408-423, 2013.
[16]	Carlos Diuk, Lihong Li, and Bethany R Leffler. The adaptive k-meteorologists problem and its
application to structure learning and feature selection in reinforcement learning. In Proceedings
of the 26th Annual International Conference on Machine Learning, pp. 249-256, 2009.
[17]	Pierluca D’Oro, Alberto Maria Metelli, Andrea Tirinzoni, Matteo Papini, and Marcello Restelli.
Gradient-aware model-based policy search. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 3801-3808, 2020.
[18]	Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy of
science,74(5):981-995, 2007.
[19]	Eyal Even-Dar and Yishay Mansour. Approximate equivalence of markov decision processes.
In Learning Theory and Kernel Machines, pp. 581-594. Springer, 2003.
[20]	Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for
model-based reinforcement learning. In Artificial Intelligence and Statistics, pp. 1486-1494.
PMLR, 2017.
[21]	Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey
Levine. Model-based value expansion for efficient model-free reinforcement learning. In
International Conference on Machine Learning (ICML), 2018.
[22]	Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep
spatial autoencoders for visuomotor learning. In 2016 IEEE International Conference on
Robotics and Automation (ICRA), pp. 512-519. IEEE, 2016.
[23]	Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.
arXiv preprint arXiv:2011.15091, 2020.
[24]	Carlos Guestrin, Daphne Koller, and Ronald Parr. Max-norm projections for factored MDPs. In
International Joint Conferences on Artificial Intelligence (IJCAI), volume 1, pp. 673-682, 2001.
[25]	Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution
algorithms for factored mdps. Journal of Artificial Intelligence Research, 19:399-468, 2003.
[26]	Zhaohan Daniel Guo and Emma Brunskill. Sample efficient feature selection for factored mdps.
arXiv preprint arXiv:1703.03454, 2017.
[27]	Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. In International
Conference on Machine Learning, pp. 1861-1870. PMLR, 2018.
[28]	Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. In Advances in Neural Information Processing Systems, pp.
12519-12530, 2019.
[29]	Nan Jiang. Notes on state abstractions, 2018. URL http://nanjiang.cs.illinois.
edu/files/cs598/note4.pdf.
[30]	Anders Jonsson and Andrew Barto. Causal graph based decomposition of factored mdps. J.
Mach. Learn. Res., 7:2259-2301, December 2006. ISSN 1532-4435.
[31]	Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored MDPs. In
International Joint Conferences on Artificial Intelligence (IJCAI), volume 16, pp. 740-747,
1999.
[32]	Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
[33]	Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474,
2016.
[34]	Nathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra. Objective mismatch in
model-based reinforcement learning. Learning for Dynamics and Control (L4DC), pp. 761-770,
2020. URL http://arxiv.org/abs/2002.04523.
[35]	Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953,
2019.
[36]	Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
[37]	Lihong Li. A unifying framework for computational reinforcement learning theory. PhD thesis,
Rutgers University-Graduate School-New Brunswick, 2009.
[38]	Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state
abstraction and provably efficient rich-observation reinforcement learning. In International
Conference on Machine Learning (ICML), pp. 6961-6971, 2020.
[39]	Dipendra Misra, Qinghua Liu, Chi Jin, and John Langford. Provable rich observation
reinforcement learning with combinatorial latent states. In International Conference on
Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=
hx1IXFHAw7R.
[40]	Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles Blundell. Repre-
sentation learning via invariant causal mechanisms. arXiv preprint arXiv:2010.07922, 2020.
[41]	Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[42]	Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps. In
Advances in Neural Information Processing Systems, pp. 604-612, 2014.
[43]	Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard
Scholkopf. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020.
[44]	Judea Pearl. Causality. Cambridge university press, 2009.
[45]	Judea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress,
19, 2000.
[46]	Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference using invariant
prediction: identification and confidence intervals. arXiv preprint arXiv:1501.01332, 2015.
[47]	Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.
John Wiley & Sons, 2014.
[48]	Balaraman Ravindran and Andrew G Barto. An algebraic approach to abstraction in reinforce-
ment learning. PhD thesis, University of Massachusetts at Amherst, 2004.
[49]	Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimiza-
tion. arXiv preprint arXiv:2010.05761, 2020.
[50]	Alexander L Strehl, Carlos Diuk, and Michael L Littman. Efficient structure learning in
factored-state mdps. In AAAI, volume 7, pp. 645-650, 2007.
[51]	Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM
Sigart Bulletin, 2(4):160-163, 1991.
[52]	Yi Tian, Jian Qian, and Suvrit Sra. Towards minimax optimal reinforcement learning in factored
markov decision processes. Advances in Neural Information Processing Systems, 33, 2020.
[53]	Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.
5026-5033. IEEE, 2012.
[54]	Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in
reinforcement learning? Advances in Neural Information Processing Systems, 32:14322-14333,
2019.
[55]	Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv
preprint arXiv:1906.08649, 2019.
[56]	Ziping Xu and Ambuj Tewari. Near-optimal reinforcement learning in factored mdps: Oracle-
efficient algorithms for the non-episodic setting. 2020.
[57]	Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau,
Yarin Gal, and Doina Precup. Invariant causal prediction for block MDPs. In International
Conference on Machine Learning (ICML), pp.11214-11224,13-18 JUl 2020.
[58]	Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
Invariant representations for reinforcement learning withoUt reconstrUction. In International
Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/
forum?id=-2FCwDKRREu.