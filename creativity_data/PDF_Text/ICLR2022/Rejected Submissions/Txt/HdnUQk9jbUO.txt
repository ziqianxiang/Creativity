Under review as a conference paper at ICLR 2022
Linear Convergence of SGD on
Overparametrized Shallow Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Despite the non-convex landscape, first-order methods can be shown to reach global
minima when training overparameterized neural networks, where the number
of parameters far exceed the number of training data. In this work, we prove
linear convergence of stochastic gradient descent when training a two-layer neural
network with smooth activations. While the existing theory either requires a high
degree of overparameterization or non-standard initialization and training strategies,
e.g., training only a single layer, we show that a subquadratic scaling on the width
is sufficient under standard initialization and training both layers simultaneously if
the minibatch size is sufficiently large and it also grows with the number of training
examples. Via the batch size, our results interpolate between the state-of-the-art
subquadratic results for gradient descent and the quadratic results in the worst case.
1	Introduction
Our understanding of the optimization landscape of supervised learning with neural networks has
vastly improved in recent years. This is in part due to the observation that overparameterization is
key to overcome the pitfalls of first-order methods in general non-convex problems (Soltanolkotabi
et al., 2019). Under this assumption, a line of research has established convergence of first-order
methods such as gradient descent (GD) to global optimality, (Allen-Zhu et al., 2019; Kawaguchi &
Huang, 2019; Du et al., 2019; Du & Lee, 2018; Zou & Gu, 2019; Brutzkus & Globerson, 2017; Song
& Yang, 2019; Oymak & Soltanolkotabi, 2020), a phenomenon that has been confirmed in practice.
Empirically, as long as the width of a network scales linearly with the size of the training data (mild
overparameterization), stochastic gradient descent (SGD) enjoys fast convergence to global optimality
(Livni et al., 2014; Safran & Shamir, 2018; Oymak & Soltanolkotabi, 2020; Kawaguchi & Huang,
2019). Can we explain such behavior theoretically? Sadly, the available characterizations require
a larger degree of overparameterization, or imposes additional assumptions, which do not hold for
the algorithms that are used in practice. For example, if GD is applied exclusively to the last layer,
Kawaguchi & Huang (2019) show that an ideal linear scaling of the width is sufficient to guarantee
convergence. Song & Yang (2019) prove quadratic scaling when GD is applied only to the first layer.
For two-layer neural networks, when both layers are trained with GD simultaneously, state-of-the-art
results show that subquadratic (not linear) scaling is enough to converge to global optimality (Anony-
mous). Despite being close to the ideal linear rate of overparameterization, due to computational
constraints, GD is rarely used in modern applications involving huge datasets. Hence, closing the gap
between theory and practice requires studying scalable first-order algorithms such as SGD. Our work
focuses on mini-batch SGD, which is one of the most common algorithms for training deep models.
We study convergence of SGD when it is applied to train both layers of a neural network, which is
initialized with standard initialization schemes.
Our contributions:
•	We show that under proper initialization and choice of learning rate, the iterates of SGD
converge to a global minimum with high probability and exponentially fast for a general
non-convex problem assuming that the loss function satisfies a growth condition.
•	For the special case a two-layer neural network, we show that a subquadratic scaling on the
width is sufficient under standard initialization and training both layers simultaneously, if the
1
Under review as a conference paper at ICLR 2022
Reference	Algorithm	Activation	Setting	Scaling
Oymak & Soltanolkotabi (2020)	SGD on layer 1	ReLU	QL	ΩΩ(n2)
Song & Yang (2019)	GD on layer 1	ReLU	SD	Ω(n2)
Kawaguchi & Huang (2019)	GD on layer 2	ReLU	CLL	~ ,. Ω(n)
Du et al. (2019)	GD	ReLU	SD+QL	ΩΩ(n6)
Zou & Gu (2019)	GD	ReLU	SD+QL	Ω(n8)
Anonymous	GD	smooth	QL	ΩΩ(n2) or ΩΩ(n2)
Allen-Zhu et al. (2019)	SGD	ReLU	SD+QL	Ω(n24)
This paper	SGD	smooth	QL	ΩΩ(n2) or ΩΩ(n2)
Table 1: Required degree of Overparameterization for training shallow networks with global convergence
，	dr	t . ∙ -1	z-ιτ τ	t τ ∙	1 ♦， ι	γλ-rʌ	ι ι t . n-ɪi	. . ∙ 入∙
guarantees. QL=quadratic loss, CLL=ConVex and LiPSchitZ loss, SD=SeParable data. The notation Ω ignores
logarithmic factors.
minibatch siZe is sufficiently large and it also grows with the number of training examPles.
For constant batch siZe, we show that quadratic oVerParametriZation is sufficient. Our results
interPolate between subquadratic and quadratic scalings dePending on the batch siZe.
Related work. The majority of the existing literature on oVerParameteriZation focuses on GD (Du
et al., 2019; Du & Lee, 2018; Allen-Zhu et al., 2019; Zou & Gu, 2019). Allen-Zhu et al. (2019)
ProVided theoretical bounds for deeP networks trained with SGD. HoweVer, their results require an
oVerParameteriZation degree that is too large, comPared to what can be achieVed for GD. In contrast,
we study SGD and how the batch siZe affects the required degree of oVerParameteriZation. Chen
et al. (2021) establish generaliZation guarantees and sufficient network width when SGD trains deeP
ReLU networks for binary classification, which is a different setting comPared to our PaPer.
We study the case where SGD uPdates all the Parameters of a shallow neural network. In contrast,
a number of existing literature assume that only the Parameters corresPonding to some layers are
uPdated throughout training (Oymak & Soltanolkotabi, 2020; Kawaguchi & Huang, 2019; Song &
Yang, 2019). When SGD is aPPlied only to the first layer, Oymak & Soltanolkotabi (2020) showed
that quadratic scaling is sufficient for conVergence with linear rate. DesPite being an interesting
theoretical setuP, such algorithmic choice rarely haPPens in Practice.
There are also differences regarding the choice of actiVation function. While ReLU can be considered
as the default actiVation function when studying deeP neural networks, its non-smoothness may
be the reason why results for ReLU networks require substantially more number of Parameters or
additional assumPtions on the data (like seParability) to guarantee conVergence to a global minimum.
MoreoVer, backProPagation on ReLU networks does not correctly calculate the gradient at all Points
of differentiability (Kakade & Lee, 2018; Bolte & Pauwels, 2021), which raises major technical
issues. In contrast, we assume a smooth actiVation, similar to Anonymous, which aVoids such issues
and achieVes lower oVerParameteriZation degrees.
The authors of (Anonymous) established subquadratic scaling when GD trains a shallow neural
network. In this PaPer, we focus on SGD, which results in substantial technical challenges. ComPared
to the results in (Anonymous), controlling the length of the trajectory is more inVolVed in this PaPer,
which requires a new analysis technique that bounds the length of the trajectory with high Probability.
We consider the effect of mini-batch SGD, which shows an interPolation between subquadratic
and quadratic scaling. We also imProVe the estimates at initialiZation and show that more relaxed
assumPtions are sufficient to establish sufficient oVerParameteriZation degree.
We summariZe such recent results in the overparametrization literature in Table 1.
Lazy Training. ProVing fast conVergence to global oPtimality is not a comPlete answer. It has been
shown that desPite fast conVergence, it is Possible that an algorithm tends towards a solution with
Poor Performance on test data, if the training falls in the so-called Lazy Training regime (ChiZat
et al., 2019). Thus, any useful algorithmic framework for learning neural networks should aVoid
this regime, usually through careful initialiZation schemes. For examPle, desPite requiring only
linear oVerParametriZation for GD, the initialiZation studied by Nguyen & Mondelli (2020) leads to
2
Under review as a conference paper at ICLR 2022
the lazy regime. This is the reason why we omit such result from Table 1. In this work, we show
global convergence and achieve subquadratic scaling under standard initialization schemes, which
empirically perform well on test data.
Polylogarithmic width is enough to obtain convergence for neural networks of arbitrary depth,
according to Ji & Telgarsky (2020); Chen et al. (2021). However, in those work, convergence is
understood in an ergodic sense. This is a weaker notion than strict convergence with high probability,
which is the one we consider, and which better matches practical applications.
Given perfect knowledge about the underlying function that generates the labels and under the
assumption that such target function has low-rank approximation, Su & Yang (2019) showed that GD
achieves zero-approximation. This is different from the problem considered in our paper.
For a binary classification problem, Daniely (2020) showed that near linear network size is sufficient
for SGD to memorize random examples under a variant of Xavier initialization, which is a different
setting compared to our paper. For a deep neural network with pyramidal structure and smooth
activations, Nguyen & Mondelli (2020) showed that subquadratic scaling is sufficient for global
convergence of GD under a restrictive initialization scheme. In this paper, we establish global
convergence for SGD under standard initialization.
A recent line of work uses mean-field analysis to approximate the target distribution of the weights in
a neural network via their empirical distribution (Mei et al., 2019; Lu et al., 2020). Nevertheless,
such results do not provide useful overparametrization degree bounds in terms of the number of
samples. In contrast, our work does not require such approximations and we focus on deriving
explicit sufficient overparametrization rates for global optimality of SGD.
Notation. We use ∣∣ ∙ ∣∣ to denote the Euclidean norm of a vector and FrobeniUs norm of a matrix.
We use V to represent the Jacobian of a vector-valued and gradient of a scalar-valued function. We
use Θ and 0 to represent the entry-wise Hadamard product and Kronecker product, respectively. We
use lower-case bold font to denote vectors. We use calligraphic and standard fonts to represent sets
and scalars, respectively. We use σmin (T) and σmax(T) to denote the smallest and largest singular
values of a linear map T. We use [n] to represent {1,…，n} for an integer n. We use O and Ω to
hide logarithmic factors and use . to ignore terms up to constant and logarithmic factors.
2	A General Global Convergence Result for SGD
In this section, we consider a general non-convex minimization problem and show that for a certain
choice of learning rate and careful initialization, the iterates of SGD converge to a global minimum
with high probability and exponentially fast. In Section 3, we extend our consideration to the training
of a shallow neural network and find the hidden layer size, which is sufficient for SGD to converge to
a global minimum i.e., its overparameterization degree.
Definition 1 (Smoothness). Let βψ > 0. A function ψ : Rd1 → Rd2 is βψ -smooth, if for all
u, v ∈ Rd1 , we have
σmax(Vψ(u) - Vψ(v)) ≤ βψ∣u - v∣.	(1)
Definition 2 (PL condition (Bolte et al., 2017)). A function ψ : Rd1 → R satisfies the PL condition
if there exists αψ > 0 such that, for all u ∈ Rd1 , we have
ψ(u) ≤T.	⑵
2αψ
We are now ready to state our finite-sum compositional optimization problem:
min
w∈Rd
m
h(w) := f(Φ(w)) = —X fj(Φ(w))
m j=1
Φ : Rd → Rq	f,fj : Rd → R+ (3)
where m denotes the number of training examples.
Assumption 1. The functions introduced in Eq. (3) satisfy the following properties: (i) Φ is twice-
differentiable and βΦ-smooth (Definition 1), (ii) f is twice-differentiable and βf -smooth and (iii) f
satisfies the PL condition with some αf > 0 (Definition 2).
3
Under review as a conference paper at ICLR 2022
We study the iterates of the stochastic gradient descent (SGD) algorithm when applied to the objective
function h in Eq. (3). For i ≥ 0, let Ii denote a random minibatch at iteration i drawn uniformly at
random, independent of all previous draws. Let b ∈ [m] denote the minibatch size, i.e., |Ii| = b for
all i. The SGD iterates are defined by a random variable w0, referred to as the initialization, and the
update rule:
wi+1 = Wi —入；^X Vhj (Wi),
j∈Ii
(4)
where λ > 0 is the learning rate and hj (W) := fj (Φ(W)) ∀j, W.
An important feature of the SGD iterates is that b Pj∈ii Vhj(Φ(wi)) in Eq. (4) is an unbiased
estimator of the gradient Vh(Wi) given wi, i.e., E [b Pj∙∈zi Vhj(Φ(wi))∣wi] = Vh(wi). NeVer-
theless, this is not enough for SGD to converge to the first-order optimality. In addition, we will
assume that f in Eq. (3) satisfies the growth condition (Schmidt & Roux, 2013; Vaswani et al., 2019;
Cevher & Vu, 2019):
Definition 3 (Growth condition). A function ψ : Rd → R with a finite-sum structure satisfies the
growth condition with minibatch size b if there exists ηψ > 0 such that, for all u ∈ Rd, we have
Ehll 1 XVψj(u)「≤ ηψkVψ(u)k2,
where the expectation is over the random choice of set I.
Assumption 2. In Eq. (3), f satisfies the growth condition (Definition 3) for some ηf > 0.
(5)
We are now ready to state the main result of this section.
Theorem 1. Let Assumptions 1 and 2 hold and let ζ > 1. Suppose that at initialization,
0 < μφ ≤ σmin(VΦ*(w0)) ≤ σmaχ(VΦ*(w0)) ≤ Vφ,
h(w0) = O
af μφ	'
ζβΦ ηf νΦJ .
(6)
Then, for a sufficiently small learning rate
λ . min ( —~…	德---------：---------........... ― μ ----------------------?
ηf(βΦνΦkVf(φ(w0))k + βfνΦ + βfμΦνΦ) Z√ηfv®GMVf&(w°))k + %叫〃小)/
(7)
the iterates of SGD {wi}i≥0 (4) converge to a global minimizer of h (3) with the optimal value of
zero, exponentially fast and with probability at least 1 一 1/Z. The rate ofconvergence is given by
E[h(wi)] ≤ (1 — Cλafμφ)i ∙ h(w0)
for a universal constant C.
Remark 1. The second item in Eq. (6) suggests initializing close to a global minimum of the non-
convex optimization problem. This feature has precedence in the related literature, e.g., in matrix
factorization (Chi et al., 2019).
The proof of Theorem 1 is deferred to Appendix B. However, in the remaining of this section we
provide a sketch of the main arguments that lead to the result. The first condition in Eq. (6) is central
to our arguments, and we will refer to it as the near-isometry property.
Definition 4 (Near-isometry). A linear map T : Rd1 → Rd2 is (μ, ν)-near-isometry if there exist
0 < μ ≤ V such that
μ ≤ σmin(T) ≤ σmaχ(T) ≤ V.
(8)
Let W denote the limit point when the SGD algorithm is run with some learning rate and let VΦ* (W)
denote the adjoint operator of VΦ(w). Convergence of SGD is ensured with high probability due to
the strong growth condition (Definition 3) along with proper learning rate and initialization. We note
that W is a first-order stationary point of h. Hence we have:
0 = Vh(W) = VΦ*(W)Vf (Φ(W))
(9)
4
Under review as a conference paper at ICLR 2022
Note that if VΦ*(w) is nonsingular, it would follow that Vf (Φ(w)) = 0. The PL condition
(Definition 2) would then imply that Φ(w) is a global minimizer of f and hence, a global minimizer
of h. With this fact in mind, our proof can be summarized in three steps: first, a careful choice of
initialization will ensure that VΦ* is nonsingular for all elements within a certain distance of w0.
Second, we show that under small enough learning rate, the iterates of SGD remain close to the
initialization w0, with high probability regardless of the number of iterations.
The third and final step will use the non-singularity of VΦ* at convergence and Eq. (9) to conclude
global optimality.
This is akin to the arguments in (Anonymous), however, in our case the stochasticity in the SGD
updates poses a challenge for controlling the distance to initialization. We use concentration bounds
on the length of the path and show that the SGD trajectory remains in the region where VΦ* is
non-singular, with high probability.
A crucial result for the first step of our proof has already been established in by (Anonymous). It
shows that a smooth function that is near-isometry at initialization will preserve such property for all
points within a certain distance.
Lemma 1 (Anonymous). Let Φ be βφ-smooth and VΦ* (w0) be a (μφ, νφ)-near-isometry. Then
forall W such that ∣∣w 一 w°k ≤ 2βΦ-,	μφ ≤ σma(VΦ"(w)) ≤ σmaχ(VΦ*(w)) ≤ 3Vφ (10)
The second step in the proof of Theorem 1 is to compute the expected length of the SGD trajectory
which is spent inside the ball defined in Eq. (10). We find an upper bound on this expected length
depending on the initialization and learning rate, but independent of the number of iterations. Hence,
under some proper initialization and learning rate, we can control the expected length of the trajectory
for which Lemma 1 holds. In particular, we have
Proposition 1 (Expected length of trajectory). Let Assumptions 1 and 2 hold and let ζ > 1. Let the
random variable I denote the first iteration of SGD (Eq. (4)) such that
wI ∈/ B := ball(w0, ρΦ) := {w : ∣w 一 w0 ∣ ≤ ρΦ}	(11)
or I = ∞ if the trajectory does not leave B. Suppose that w0 satisfies Eq. (6) and SGD is executed
with sufficiently small learning rate, which satisfies Eq. (7). An upper bound on the expected length
of the SGD trajectory is given by
E['(I )]≤ 备=ρφ.	(12)
We provide the sketch of the proof (see Appendix A for the complete proof). We first find an upper
bound on the expected length of the trajectory in terms of the norm of gradients of f . With a proper
learning rate, we find an upper bound on the norm of the gradient in terms of the expected decent
of f in two consecutive iterates, which are inside the ball. We also ensure that the learning rate is
sufficiently small such that E[∣wI 一 wI-1 ∣] is bounded. Finally, under proper initialization, we
obtain an upper bound on the expected length of the trajectory for the iterates inside the ball, i.e.,
E[PiI=-01 ∣wi 一 wi-1∣].
Remark 2. A similar phenomenon that shows bounded length of the trajectory has been observed
in various settings mainly for gradient descent (Du et al., 2019; Oymak & Soltanolkotabi, 2019;
Anonymous). In this paper, we focus on a compositional non-convex problem trained with SGD,
which is more challenging to analyze.
Using the upper bound (12) on the expected length of the trajectory spent inside B = ball(w0, ρΦ),
we can bound the probability that the SGD iterates leaves the ball B. Indeed, in order for the process
to leave B starting from w0, it is required that the length of the trajectory spent inside B satisfies
l(I) ≥ Pφ. Hence, using bound (12) on E['(I)] together with a concentration bound (Markov
inequality in our case), we can upper bound the probability of SGD iterates leaving B. Finally, under
the event that the SGD iterates remain in B, an upper bound on E['(I)] implies the convergence of
the iterates.
Remark 3. With a more involved analysis on the concentration properties of the random variable
'(I), it may be possible to greatly improve the dependence of the initialization and SteP size on ζ.
5
Under review as a conference paper at ICLR 2022
Indeed, the current analysis assumes the worst-case scenario, where the SGD iterates either remain
at the initialization, or directly leave the ball B in a straight line (this scenario indeed maximizes the
probability that the process leaves the ball, given a bound on E['(I)]).
Although '(I) is obtained as a sum ofrandom variables, the difficulty Ofobtaining better concentration
boundsfor '(I) comesfrom the high level ofdependence between all the variables involved. A better
analysis would thus need to better understand how the trajectories behave inside the ball B, e.g., by
bounding the variance of '(I).
In the following section, we specify our result to the special case of shallow neural networks. We
will show that, in the case of quadratic loss, the strong growth condition naturally holds, with a
constant depending on the batch size. Moreover, using Gaussian initialization for the neural network
parameters, we can control the initial smoothness and near-isometry parameters involved in Theorem 1
with high probability.
3	Global Optimality of Neural Networks trained with SGD
Setup. We will consider the problem of training a shallow neural network with one hidden layer,
input dimension d0, d1 hidden nodes, output dimension d2 , and quadratic loss. We denote the data
and label matrices as X ∈ Rd0×m and Y ∈ Rd2×m, respectively.
Let W ∈ Rd1×d0 and V ∈ Rd2×d1 denote the parameters of the first and second layers of the network,
respectively. We collect both parameters in a variable Θ =(W, V ) ∈ Rd1 ×d0 × Rd2×d1. In order to
fit the supervised training of the network to the template studied in Section 2 (Eq. (3)) we define:
Φ(Θ) := V ∙ φ(WX) ∈ Rd2×m, fj(Z) = ∣∣Zj - Yjk2 ∈ R+	(13)
where Zj denotes the j -th column of a matrix Z and φ : R → R is the activation function, which is
applied entry-wise. We can now write the problem as a the finite-sum:
(1	1 m
h(⑼=f (Φ(Θ)) = m-∣Vφ(WX) - Yk2 = m- E ∣Vφ(WXj) - Y∙k2	(14)
m	m j=1	
We will make an assumption on the Hermite norm (Definition 5) of the activation function. Our
assumptions on the activation function are summarized as Assumption 3 below.
Definition 5 (Hermite norm (Olver et al., 2010)). Let φ : R → R. The Hermite norm of φ is given
by ∣φ∣H = a∕Σ∞=o C2 where Ci denotes the i-th Hermite coefficients of φ given by:
Ci = hφ,qiiH = √2∏ / φ(X)qi(X) eχp ( — x~) dX
and qi : R → R is the i-th Hermite polynomial (probabilist’s convention) for i ≥ 0.
Assumption 3. φ IS twιce-dιfferentιable, φ(0) = 0, SuPa ∣φ(a)∣ = φmaχ < ∞, SuPa ∣φ(a)∣ =
φmax < ∞, and ∣φ∣H < ∞.
The popular ReLU does not satisfy the twice-differentiability assumption. However, smooth ap-
proximations of ReLU such as the Gaussian error Linear Units (GeLU) and softplus (Hendrycks &
Gimpel, 2020; Nguyen & Mondelli, 2020) have been shown to outperform ReLU in several settings
and are commonly used in practice (Clevert et al., 2016; Gulrajani et al., 2017; Kumar et al., 2017;
Kim et al., 2018; Xu et al., 2020). In addition, smoothened functions by a Gaussian kernel uniformly
approximate the ReLU function (Nguyen & Mondelli, 2020).
Assumption 4. For all j ∈ [m], ∣Xj ∣ ≤ 1. ∣Y ∣ ≤ 1.
Remark 4. The assumption on the data is mild and common in the overparameterization literature
(Li & Liang, 2018; Ji & Telgarsky, 2020). It can be enforced by data normalization.
Initialization. The initial iterate of SGD will be chosen in the following way:
W0 〜N(0, J) , V0 〜N(0,小.	Θ0 := (W0, V0)	(15)
6
Under review as a conference paper at ICLR 2022
Remark 5. The initialization in Eq. (15) matches popular initialization schemes such as LeCun
(LeCun et al., 2012) and He (He et al., 2015) initializations.
We now proceed to estimate with high probability the value of h(Θ0), near-isometry constants
(μΦ, vφ) of V*Φ(Θ0) and smoothness parameter βφ, which are required in Theorem 1.
Lemma 2 (Estimation of h(Θ0),μφ, νΦ , βΦ (Anonymous)). Let Assumptions 3 and 4 hold, and
suppose that Θ0 follows the initialization distribution in Eq. (15). Let t be a positive integer such that
m ` d0 and X *t ∈ Rd0 ×m be the matrix whose a -th column defined as Vec(Xa 0∙∙∙0 Xa) ∈ Rd0.
For some constants δ1, δ2, δ3, k1, and k2 independent of d0, d1 and m, with probability at least
1 - ψ it holds that:
h(θ0) ≤ δ3k2k2σmaX(X)
m
Vφ = max {∣C0∣p(1 + δ2)d1m, ωι J(1 + δ2)(c2 + c∞)dισmax(X)}
μΦ = (J- - δ1)t!d1σmin(X*t)
(16)
The precise expression for ψ is provided in Appendix E, ci is the i-th Hermite coefficients of φ
(Definition 5) and c2∞ = Pi∞=2 ci2/i!.
Moreover, the map Φ restricted to the set {(V, W) : σmaX(V ) ≤ χmaX} is smooth with constant
βΦ = √2 σmaX(X) φmaX + φmaXχmaX .	(17)
Although the mapping Φ is not globally smooth, Lemma 2 shows that it is smooth in a region where
the largest singular value of V remains bounded. In the following lemma, we show that we can indeed
bound the smoothness constant of Φ restricted to a neighbourhood of V 0 as required in Theorem 1.
Lemma 3. Let Assumption 3 hold. Let V0, W0 be arbitrary matrices and μφ be as in (16). Let
工
βφ ：= √2σmax (X )(Φ max + bmax(V 0)) + ^mx, PΦ ：=兽	(18)
2φmaX	2βΦ
The function Φ is βΦ-smooth over the set:
Bρφ(V 0 ,W 0) := {(V,W) : PkV - V 0k2 + kW - W 0k2 ≤ Pφ}	(19)
Proof. Let
χmax := σmax (V ) +
______μΦ______
2√2σmax(X )Φmax
~ - ，_________ _________ 、
BXmax= {(V，W) : σmax(V) ≤ χmax}	(20)
Lemma 2 then implies that Φ restricted to BXmaX is βφ-smooth, following Eq. (17). With this
choice of XmaX we show that Bρφ(V0, W0) ⊆ BXmax, which implies the result. Note that βφ ≥
√2σmaX(X)φmax, hence
Xmax ≥ σmax (V ) +
μΦ
2βΦ
σmax(V0) + ρΦ .
Suppose that (V, W) ∈ BρΦ (V0, W0). By Eq. (19) this implies kV - V0k ≤ ρΦ. Then,
σmax(V) ≤
σmax(V-V0)+
σmax
≤kV -V0k+σmax(V0)
≤ ρΦ + σmax (V0 )
≤ Xmax.
□
7
Under review as a conference paper at ICLR 2022
In our case, as f is the quadratic loss, the growth condition (Definition 3) is satisfied with ηf = Zb.
This is precisely the quantity that will reveal the impact of the minibatch size on the global convergence
of SGD. Moreover, the quadratic loss satisfies the PL condition (Definition 2) and is smooth. All
things considered, Assumptions 1 and 2 hold for our shallow neural network training setting, ensuring
that Theorem 1 is valid.
We are now ready to integrate Lemma 2 and Lemma 3 together with the convergence guarantees in
Theorem 1 to arrive at the sufficient degree of overparameterization required for the convergence
of SGD. The following theorem finally concludes that for the shallow neural network described in
Section 3, for a sufficient degree of overparameterization SGD converges to a global minimum with
high probability.
Theorem 2 (Shallow network with SGD). Suppose that Assumptions 3 and 4 hold, and that that
(W0, V0) is randomly initialized as in (15). Suppose that the hidden layer width d1 satisfies
di = Ω (ξ(Cδ,t,φ, {ci}i≥0,ζ) σmax.Xm )	(21)
σ31in(x *t) Vb
where Cδ is a set of constants, ξ is a term independent of d0,m. The SGD iterates converge to a global
minimum exponentially fast with probability at least 1 - ψ(φ, ξ, d0, d1, d2, X, ζ). See Appendix C
for the exact expressions of ξ and ψ and the proof.
Finally, we provide an order analysis to understand how the sufficient overall overparameterization
degree directly depends on the minibatch size. Intuitively, the sufficient overparameterization degree
improves (is lower) as the minibatch size increases.
3.1 Impact of the Minibatch size on the Overparameterization degree
For t = 1, the analysis requires m ` do, which is not a common setting in practice. For t ≥ 2,
we suppose that m ` d0, which is the case in practice. We estimate that σmaχ(X) ` ʌ/m/do
and σmin(X*t) ` m/d/d' ` 1, along the lines of (Oymak & Soltanolkotabi, 2020, Section 2.1).
Substituting σmax(X) and σmin(X*t) into (21), we have
2
m
di & H .	(22)
bd0
Therefore, the overall overparameterization degree becomes dodi ` Ω(/2/Vb), which is sufficient
for SGD to find a global minimum at a linear rate except with an arbitrary small probability. This fact
will let us understand more clearly the effect minibatch size on the overparameterization degree.
If b = Ω(m), similar to gradient descent, a subquadratic scaling on the network width, d°di `
3
Ω(m 2), IS sufficient. In that case, an optimal linear scaling di ` O(m) IS sufficient when the number
of input features is sufficiently large do ` Ω(ʌ/m).
On the other hand, when the batch size is small b = O(1), we recover the standard quadratic
3	3	3	3 3 3.	3	3	3
scaling on the network width. Our analysis provides an interpolation between dodi ` Ω(m2) and
dodi ` Ω(m2) depending on b. As long as the batch size b ` Ω(ma) for some a > 0, we achieve a
subquadratic scaling.
4 Conclusions and future work
In this work, we prove linear convergence of stochastic gradient descent for training over-
parameterized two-layer neural networks with smooth activation functions, using classical initial-
ization strategies, and where both layers are trained simultaneously. We provide a lower bound on
the required over-parameterization degree for our result to hold, depending on the batch size b used
to compute the stochastic gradients. More precisely, we show that using a number of parameters
dodi = Ω(m2∕√b) is sufficient to obtain linear convergence with high probability, providing sub-
quadratic over-parameterization degree as long as the batch size increases with the number of data
points.
8
Under review as a conference paper at ICLR 2022
In future work, we would like to relax the smoothness condition on the activation function, in order
to encapsulate non-smooth activation functions such as ReLU. In addition, we would like to improve
the high probability bound by analyzing more deeply the concentration properties of the random
variable '(I), characterizing the length of the trajectory spent in a neighborhood of the initialization.
Finally, an important step would be to analyze the generalization properties of SGD through the lens
of the proposed approach, in particular by analyzing in which case it leads to lazy training.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning (ICML), 2019.
Anonymous. Anonymous. Anonymous.
Jerome Bolte and Edouard Pauwels. Conservative set valued fields, automatic differentiation,
stochastic gradient methods and deep learning. Mathematical Programming, 188(1):19-51, 2021.
Jerome Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W Suter. From error bounds to the
complexity of first-order descent methods for convex functions. Mathematical Programming, 165:
471-507, 2017.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with Gaussian
inputs. In International Conference on Machine Learning (ICML), 2017.
Volkan Cevher and Bang Cong Vu. On the linear convergence of the stochastic gradient method with
constant step-size. Optimization Letters, 13:1177-1187, 2019.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is
sufficient to learn deep ReLU networks? In International Conference on Learning Representations
(ICLR), 2021.
Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization:
An overview. IEEE Transactions on Signal Processing (TSP), 67:5239-5269, 2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in neural information processing systems (NeurIPS), 2019.
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learn-
ing by exponential linear units (ELUs). In International Conference on Learning Representations
(ICLR), 2016.
Amit Daniely. Neural networks learning and memorization with (almost) no over-parameterization.
In Advances in neural information processing systems (NeurIPS), 2020.
Simon S. Du and Jason D. Lee. On the power of over-parametrization in neural networks with
quadratic activation. In International Conference on Machine Learning (ICML), 2018.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations
(ICLR), 2019.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in neural information processing systems
(NeurIPS), 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2015.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELU). arXiv preprint
arXiv:1606.08415v4, 2020.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of
American Statistical Association, 58:13-30, 1963.
9
Under review as a conference paper at ICLR 2022
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow ReLU networks. In International Conference on Learning
Representations (ICLR), 2020.
Sham Kakade and Jason D Lee. Provably correct automatic subdifferentiation for qualified programs.
arXiv preprint arXiv:1809.08530, 2018.
Kenji Kawaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep
neural networks of practical sizes. In Annual Allerton Conference on Communication, Control,
and Computing, 2019.
Youngjin Kim, Minjung Kim, and Gunhee Kim. Memorization precedes generation: Learning unsu-
pervised GANs with memory networks. In International Conference on Learning Representations
(ICLR), 2018.
Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with GANs:
Manifold invariance with improved inference. In Advances in neural information processing
systems (NeurIPS), 2017.
Yann A. LeCun, Leon Bottou, Genevieve B. Orr, and KlaUs-Robert Muller. Efficient BackProp. In
Neural networks: Tricks of the Trade. Springer, 2012.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in neural information processing systems (NeurIPS), 2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Proceedings of the 27th International Conference on Neural Information Processing
Systems - Volume 1, NIPS'14,pp. 855-863, Cambridge, MA, USA, 2014. MIT Press.
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean field analysis of deep resnet
and beyond: Towards provably optimization via overparameterization from depth. In International
Conference on Machine Learning (ICML), 2020.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, 2019.
Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer
followed by pyramidal topology. In Advances in neural information processing systems (NeurIPS),
2020.
Frank W. J. Olver, Daniel W. Lozier, Ronald F. Boisvert, and Charles W. Clark. NIST Handbook of
Mathematical Functions Paperback and CD-ROM. Cambridge University Press, 2010.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In International Conference on Machine Learning (ICML), 2019.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 1:84-105, 2020.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4433-4441. PMLR, 2018.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition. arXiv preprint arXiv:1308.6370, 2013.
Mahdi Soltanolkotabi, Adel Javanmard, and J. Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65:742-769, 2019.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix Chernoff bound.
arXiv preprint arXiv:1906.03593v2, 2019.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approxi-
mation perspective. In Advances in neural information processing systems (NeurIPS), 2019.
10
Under review as a conference paper at ICLR 2022
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. In International Conference on Artificial
Intelligence and Statistics (AISTATS), 2019.
Roman Vershynin. Introduction to the Non-asymptotic Analysis of Random Matrices. Cambridge
University Press, 2012.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. arXiv preprint arXiv:1505.00853v2, 2020.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in neural information processing systems (NeurIPS), 2019.
A Proof of Proposition 1
According to the definition of I, Wi ∈ B for i = 0,…，I 一 1. We first find an upper bound on the
expected length of the trajectory in terms of the norm of gradients of f . With a proper learning rate,
we find an upper bound on the norm of the gradient in terms of the expected decent of function in two
consecutive iterates. We also ensure that the learning rate is sufficiently small such that kWI 一 WI-1 k
is bounded. Finally, under proper initialization, the final bound is established.
The “length” of the trajectory traced by the SGD iterates {Wi }iI=0 is
I-1
'(I) := X kwi+1 - Wik
i=0
I-1	1
=λ∑ k b EVhj (wi)k
i=0	j∈Ii
I-1 1
.λνφ∑ kb EVfj(Zi)Il
i=0	j∈Ii
(A.1)
where zi = Φ(wi) for all i.
The expected length of this segment of the trajectory is therefore bounded as
「IT 1	1
E['(I)] . λνφE区k b∑Vfj (zi)kl
i=0	j∈Ii	(A.2)
I-1
≤ λ√ηfvφE X kVf (Zi) k
i=0
where the last inequality holds thanks to the growth condition and Jensen’s inequality.
We now develop a “descent inequality’ and establish a lower bound on f(Zi) - E[f (Zi+1)|wi]. First,
consider two consecutive SGD iterates i, i + 1 such that wi , wi+1 ∈ B. Then, we have
f (zi) — f (zi+1) ≥ hzi — zi+1, Vf(Zi)i — β2f kzi+1 - Zik2
=hΦ(wi) - Φ(wi+1), Vf (zi)i - βf kΦ(wi+1) - Φ(wi)k2
={VΦ(wi) {wi - wi+1} , Vf (zi)i - βf kΦ(wi+1) - Φ(wi)k2
-hΦ(wi+1) - Φ(wi) - VΦ(wi) {wi+1 - wi} , Vf (zi)i
≥ hVΦ(wi) {wi - wi+1} , Vf (zi)i - βf kΦ(wi+1) - Φ(wi)k2
11
Under review as a conference paper at ICLR 2022
-βφkwi+1 - wik2kVf(zi)k
≥ hVΦ(wi) {wi - wi+1}, Vf(Zi)i- 2kwi+1 - wik2 (βφ∣∣Vf(zi)k +
1	λ2 1
=λhVΦ(wi){b E Vhj(Wi)}, Vf(Zi)i - -kb E Vhj(Wi)k2 (βφ∣∣Vf(zi)k +
j∈Ii	j∈Ii
(A.3)
Hence, taking expectation over the selected batch ni at iteration i, we find that
f(Zi) - E[f(Zi+1)|Wi]
≥ E ( λhVΦ(wi){b X Vhj(wi)}, Vf(zi)i - λ2kb X Vhj(wi)k (βφ∣∣Vf(zi)k + 9β4ν2)] Wi
j∈Ii	j∈Ii
=卜VΦ(wi ){Vh(wi)}, Vf (zi)i - λ2 E k b XVhj (wi)k2∣w] (βφ∣∣Vf(zi)k + 9β4ν2 ))
=(λ∣∣Vh(wi)k2 - λ2E k 1 XVhj(wi)k2∣wi (βφ∣∣Vf(zi)k + 9β4νφ))
≥ (λ∣∣Vh(wi)k2 - λfφ kVf(zi)k2 (βφ∣∣Vf(zi)k + 9β4ν2))
=λμφkVf(zi )k2 (1 - fβφP⅛ - 9ηff4)	(chain rule and Lemma 1)
2μφ	8μφ )
& λμφkVf(zi)k2
where the final inequality holds if the learning rate is small enough such that
λ.
μΦ
ηf βφVφ maxi ∣∣Vf (zi)k + nf βfνφ '
(A.4)
We also note that:
fJW- - √E[f (zi+1)∣wi]
(f(zi)- E[f(zi+1)∣wi])
PfP7+ PE[f (zi+1)∣wi]
&	λμφ∣Vf (zi)k2
〜Pf(ziJ+ pE[f(zi+1)Wi]
≥ λμφ∣Vf(zi)k2
_	2p7(z)
(A.5)
λ√αf μΦkVf (Zi )k2
√2∣Vf (zi)k
λ√√fμφ kVf(zi)k.
2
Substituting the above back into (A.2), and using the fact that Wi ∈ B ∀i = 0, . . . , I - 1, an upper
bound on E['(I)] is given by:
I-1
E['(I)] ≤ λ√nfVφE X kVf(zi)k
i=0
12
Under review as a conference paper at ICLR 2022
.√ηfνι
〜√afμφ
E XI-2
f(zi) - PE[f(zi+1)∣wi]) + λ√ηfVφE [kv∕(zI-1)k]
√η? νΦ
≤	Γ~—. 2
√α7 μΦ
i=0
E XXXp PTp) - Pf (zi+1)j + λ√ηfVφE [∣Vf(zI-1)k]
(Jensen’s inequality)
√η? νΦ
√αf μΦ
+ λ√ηf vφE [kvf (zI-1)k]
≤√ηf√f)+ λ√ηf 3kVf(ZIT)k].
(A.6)
Suppose that, in addition to the previous bound on the learning rate, we have
μΦ
λ ≤ 4Z√ηfVφβφ maxi ∣∣V∕(zi)k,
(A.7)
and that the initialization satisfies
af μφ
Z2β2ηfνΦ.
(A.8)
for some ζ > 1. Then, we obtain (12).:
W )]≤ 裁=ρφ
We note that the local Lipschitz constant off is given by
max ∣vf(zi)∣ ≤
i
kvf(z0)
kvf(z0)
kvf(z0)
+ max ∣Vf(zi) - Vf(z0)∣
i
+ βf max ∣zi - z0 ∣
i
+ βf max ∣Φ(wi) - Φ(w0)∣
kvf(z0)
kvf(z0)
i
3βf νΦ
2
3βf νΦ
2
max kwi - w0 k
i
• PΦ
(A.9)
kvf(z0)
+ 3βfμφVφ
4βΦ
≤
≤
+
+
Substituting (A.9) into (A.4) and (A.7), an upper bound on the learning rate of SGD is given by
λ . min
μφ
μΦ
ηf(βφνφ∣∣Vf (z0)k + βfνφ + βfμφνφ), Z√ηfVφ(βφ∣∣Vf (z0)k + βfVφμφ”.
This completes the proof of Proposition 1.
B Proof Theorem 1
Let Z > 1. Using Proposition 1, under proper initialization and choice of learning rate, the expected
length of SGD trajectory is bounded above by:
Ew(I)]≤ 裁
ρΦ
T
Thus, using Markov inequality on '(I), We have
Pr{'(I) ≥ Pφ}
1
Z
≤
13
Under review as a conference paper at ICLR 2022
Therefore, with probability greater than 1 - 1, we have '(I) < ρφ, meaning that the SGD trajectory
never leaves the ball B, i.e., I = ∞. Moreover, conditioned on this event, '(I) corresponds to the
length of the full trajectory. Since We have a bound on E['(I)], it implies that the trajectory length is
finite, i.e., that the SGD iterates converge.
Finally, we show the linear convergence to the optimal limit point:
E[h(wi+1)|wi] = E[h(wi+1)|wi] - h(wi) + h(wi)
= E[f(zi+1)|wi] - f(zi) + h(wi)
≤ -CλμφkVf(zi)k2 + h(wi)	(A.10)
≤ -Cλαfμφf (zi) + h(wi)
=(1 - Cλαfμφ)h(wi),
where C denotes constants and zi = Φ(wi) for all i. Taking expectation of both sides of (A.10) w.r.t.
n0,…，ni-1 completes the proof of Theorem 1.
C Proof of Theorem 2
For the case of quadratic loss functions, we have ɑf = βf = -m. In the following, we first find the
growth condition parameter:
Recall fj(Z) = kZj - Yj k2. Although Vfj(Z) is a matrix is a matrix with the same size as Z,
i.e., Vf(Z) ∈ Rd2×m , fj only depends on the jth column of Z, and hence only the jth column of
Vfj(Z) will be non zero. More precisely, we have
2
(Vfj(Z))LE(Zj- Yj).
and (Vfj(Z))∙k = 0 for k = j, where (Vfj(Z))∙k denotes the kth column of Vfj(Z). We thus have
that
Eι,∣ι∣=b kb X Vfj(Z)k2 =b E1,lIl=b b X kZj-Y k2
j∈I	j∈I
4
=b E[fj (Z)]
4
=bf(Z).
Similarly, we have ∣∣Vf (Z)k2 = ∖f (Z). Hence, the quadratic loss satisfies growth condition with
m
ηf =万.
Suppose there exists some constant t ≥ 1 such that m ' dt0 .
Using Lemma 2 and Lemma 3, we can estimate the parameters Vφ,μφ, βφ and establish an upper
bound on h(Θ0):
h(Θ0) ≤ ^δ3k2k2σmaχ(X)
m	max
Vφ = max {∣C0|p(1 + δ2)d1m, ωι J(1 + δ2)(c2 + c∞)diσmaχ(X)}
μΦ = J(I - δ1)才d1 σmin(X*t)
工 ，一
βφ = √2σmax(X )(Φ max + Omax(V 0)) + ^mx^
2φmax
(A.11)
where δ1, δ2 , δ3, k1, and k2 are all constants and independent of d0, d1, and m; the term t is
a constant such that m ` d0, and X*t ∈ Rd0 ×m is derived from Khatri-Rao product with its
a-th column defined as Vec(Xa ③•一③ Xa) ∈ Rd0, with probability at least 1 - ψ where ψ =
14
Under review as a conference paper at ICLR 2022
2
2
δiσmin(EM0])
δ2σmaχ(E[M0])
d-Cδ4 d0 -d-Cδ4d2 -e ∖ 4lφmax σ2ιax( X ) δ4 √d0 log d1	-g ∖ 4φmaxσ2ιaχ(x)δ4 √d01ogdΓ
-e-Cd1 -e-Cδ32
δ4 = max(k1, k2), M0 = φ(X>W0>)φ(W0X), C is a constant, and


c2
ω2 dl K*n(X	) .	σmin(EM0])	≤	σmax(E[M0])	.	d1	(mc2	+ ω2 (C2	+ 心》2^(X)).
We denote Cδ = {δ1, δ2, δ3, δ4}.
We highlight the main condition for the linear convergence of SGD in (6):
h(Θ0) = O
αf μφ
ζβΦηfνΦ	.
The convergence happens with probability at least 1 - 1.
Suppose |co| is sufficiently large such that |co|，(1 + δ2)dιm becomes the dominating term in νφ.1
We note that the order of σmaχ(X) and σmin(X*t) play significant roles for the overparameterization
order analysis. For t = 1, it requires m ' d0, which is not a common setting in practice. In the
following, we focus on t ≥ 2.
Substituting the parameters obtained in (A.11) into (6), a sufficient condition to satisfy (6) is given by
dl & √ξ(Cδ,t,φ, {Ci}i≥0,Z) ∙ mσmax(X))、.
b	√bσmm(X *t)
where
I 八	S Zδ3 C2(1 + δ2)ω4t+4 δ4((φ max + σmaχ(V 0))2t!3
ξ(Cδ, t, φ, {ci}i≥0,ζ) = W-------------(ι - δ1 )3c6-------------.
Since ξ(Cδ, t, φ, {ci}i≥0, ζ) is constant w.r.t. d0, d1, and m. The sufficient condition can be expressed
as:
d1
〜.
Ω(
2
mσm2 ax
max
(X) )
√bσ3lm(X ”.
(A.12)
(X) ` √I
Finally, along the lines of (Oymak & Soltanolkotabi, 2020)[Section 2.1], we have σmax
and σmin(X*t) ` 西、` 1.
Replacing σmaχ(X) and σmin(X*t) into (A.12), the overall overparameterization degree becomes
dodι ` Ω(m2∕√b).Finally using the union bound on the events corresponding to random initializa-
tion and random SGD iterates, the linear convergence is established with probability at least 1 - ψ
where
ψ = d-cδ4d0 + d-cδ4d2 + e-Cd1 + e-Cδ2 + 1
-(	διbmin(EM0D	ʌ 2	- (	δ2σmaχ(E[M0])
+ e 1 4φmaxσ2ιax(X)δ4√d0 log d1	+ £ 1 4φJax σ2ιax(X) δ4 √d0 log d1
(A.13)
D	Proof of Lemma 1
The content of this proof is originally due to (Anonymous) and provided here for the completeness of
our paper.
Intuitively, if VΦ*(w0) is a (μφ, νφ)-near-isometry, then one would expect VΦ* to remain near-
isometry for all nearby points. Formally, let A, B ∈ Rm×n and let singular values of a matrix are
ordered such that σi(A) ≥ σj (A) and σi(B) ≥ σj(B) for 1 ≤ i ≤ j ≤ min{m, n}. Using Weyl’s
inequality and for i + j - 1 ≤ min{m, n}, we have:
1The same scaling holds if the other term dominates.
15
Under review as a conference paper at ICLR 2022
σi+j-1(A + B) ≤ σi(A) + σj(B).
More formally, suppose that w ∈ Rd satisfies
kw 一 w0k≤ 2βΦ=pφ
(A.14)
(A.15)
If VΦ* (w0) is (μφ, νφ)-isometry in the sense of Definition 4, then applying Weyl's inequality (A.14)
along with using smoothness and (A.15), we have
σmin(VΦ*(w)) ≥ σmin (VΦ*(w0)) — σmaχ(VΦ*(w) - VΦ*(w0))
≥ μΦ 一 βΦ∣∣w — w0k
≥”
2
Using a similar argument, We establish an upper bound σmaχ(VΦ* (w)):
σmaχ(VΦ*(w)) ≤ σmaχ(VΦ*(w0))+ bmaχ(VΦ*(w) — VΦ*(w0)) ≤ VΦ + μφ ≤ 3Vφ.
E Proof of Lemma 2
Lemma A.4 (Estimation of h(Θ0), μφ, νφ, βφ (Anonymous)). Suppose that the shallow neural
network satisfies Assumption 3. Then we have
h(OO) ≤ -1 δ3k2k2σmaχ(X)
m
Vφ = max {∣C0|p(1 + δ2)d1m, ωιʌ/(ɪ + δ2)(c2 + c∞)dισmaχ(X)}
μΦ = 4(1 - δ1) 木d1 σmin (X*t)
βΦ = √2σmaX(X) (φmax + φmaxχmax .
(A.16)
where δ1, δ2, δ3, k1, and k2 are all constants and independent of d0, d1, and m; βΦ is the smoothness
constant of the map Φ restricted to the set {(V, W) : σmaX(V ) ≤ χmaX}; the term t is a constant
such that m ` d0, and X *t ∈ Rd0×m is derived from Khatri-Rao product with its a -th column
defined as VeC(Xa ③…③ Xa) ∈ Rd0, with probability at least 1 — ψ where ψ = d-cδ4d0 —
-(	δι σmm(E[M0])	[2	- (	δ2 σmaχ(E[M0])	ʌ 2
d-cδ4d2 一 e 14φmaχσmaχ(X)δ4√d0 Iog d1 ) 一 g 1 4φ9ax σ2ιaχ(X)δ4 √d0 Iog d1 ) 一 g-Cd1 一 e-Cδ2 , where
δ4 = max(k1, k2), M0 = φ(X>W0>)φ(W0X), C is a constant, and
c2
ω1td1 才4in(X*') . σmin(E[M0]) ≤ σmax(E[M0]) . d1 (mc2 + ω2 (c2 + c∞)σ21aX(X)),
. We denote Cδ = {δ1, δ2, δ3, δ4}.
The content of this proof is originally due to (Anonymous) and provided here for the completeness of
our paper.
We first obtain the expression for adjoint operator VΦ*(Θ) : Rd2 ×m → Rd1×d0 X Rd2 ×d1. Let
∆W ∈ Rd1 ×d0, ∆V ∈ Rd2 ×d1, and ∆ ∈ Rd2×m. We expand Φ as folloW:
Φ(W +∆W,V) ≈ Φ(W,V) +VWΦ(∆W),
Φ(W, V +∆V) ≈ Φ(W,V) +VVΦ(∆V)
Where
VWΦ(∆w) = V (φ(WX) Θ ∆wX), VVΦ(∆v) = ∆vφ(WX),
(A.17)
16
Under review as a conference paper at ICLR 2022
Θ stands for the Hadamard (entry-wise) product, and φ(WX) is the derivative of φ calculated at each
entry of the matrix WX. The 叩erator VΦ(Θ) is given by (AW, Ay) → VwΦ(∆w) + VyΦ(∆y).
Using the cyclic property of the trace operator and trace ((A Θ B)C) = trace ((A Θ C T)BT), we
have
(△, VWΦ(Aw))= DG(WX) Θ VTA) Xt, AWE ,
(△, VVΦ(Ay)〉=〈Ay, Aφ (XtWt)).
Substituting (A.18), the adjoint operator is given by
VΦ*(⑼：A → ((φ(WX) Θ Vta) Xt, Aφ (XtWt)).
Suppose that there exist φmax, φmax < ∞ such that
sup ∣φ(α)∣ ≤ φmax, sup ∣φ(α)∣ ≤ φmaχ.
a	a
Lemma A.5. Let A ∈ Rm×n and B ∈ Rn×k. Then, we have
σmin(A)∣∣B∣∣ ≤ IlABlI ≤ σmax(A)∣∣B∣∣.
Using Lemma A.5 and triangular inequality, we note that
∣∣VΦ*(Θ, A)k ≤ ∣∣(φ(WX) Θ (V tA)) X t ∣ ∣ + ∣ ∣ Aφ(X tW T)Il
≤ φmaxσmax(X)^max(V)∣∣∆∣∣ + bmax(φ(WX))∣∣∆∣∣.
Similarly, we have this lower bound:
∣∣VΦ*(Θ,∆)k ≥ σmin(φ(WX))∣∣∆∣∣.
Substituting Θ0 = (W0, %) into (A.21) and (A.22), μφ and νφ are given by:
σmax(vφ (Θ0)) ≤ φmaxσmax(X)σmax(V0) + σmax(φ(^^θX)) =： νΦ,
σmin(VΦ*(Θ0)) ≥ σmin(φ(W0X)) =： μφ.
Lemma A.6. Let A ∈ Rm×n and B ∈ Rn×k. Then, for p, q ≥ 1, we have
IlABIlq ≤ ∣∣A∣∣p∣B∣∣q.
(A.18)
(A.19)
(A.20)
(A.21)
(A.22)
(A.23)
In the following, we find the smoothness parameter e中 in (1). Let Θ, Θ ∈ Rd1 ×d0 × Rd2 ×d1. We
note that ∣VΦ(Θ, A) - VΦ(Θ, ∆)∣ ≤ U1 + U2 where
U1 = ∣V(φ(WtX) Θ (∆WX)) - V(φ(WtX) Θ (∆WX))k
U2 = IlAVφ(WtX) - AVφ(WTX)∣.
(A.24)
Let us denote
Gmax(V) ≤ Xmax.	(A.25)
An upper bound on Ui in (A.24) is given by:
U1 ≤ I(V - V)(φ(Wtx) Θ (∆WX))I + ∣∣V(φ(Wtx) Θ (∆WX) - Vφ(WtX) Θ (AWX))I
≤ φ maxGmax(X )∣∣V - V ^Aw I + Gmax(X )bmax(V)∣∣φ(W TX ) - φ(W TX )∣∣∞∣∣AWk
, . . .. ^............. .. ∙∙ . .. ^ . .. ^ .................... ..
≤ φ maxGmax(X )∣∣V - V IIIlAW ∣∣ + φmax∙max(X )∣∣X h-max(V)||W - W IIIlAW I
≤ φmaxσmaX(X)||V - VIIkAWI∣ + φmaxXmaxσmaX(X)||W - WkkAWIl
where the second last inequality is due to Lemma A.6.
17
Under review as a conference paper at ICLR 2022
An upper bound on U2 in (A.24) is given by:
U ≤ φ maχσmaχ(X )∣∣W - WllIl∆v ∣∣.
Substituting the upper bounds on U and U2, an upper bound on σmaχ(VΦ(Θ) 一 VΦ((Θ)) is given by
σmax (vφ(θ) - vφ(θD ≤ σmaχ(X ) (φ max + φmaxXmaX)IlW - WI∣ + σmax(X )φ maxk V - V k
≤ √2σmax(X) φmax + φmaxχmax IΘ 一 ΘI
where the last inequality holds since
kW - Wk + kV 一 Vk ≤ √2√kW 一 Wk2 + kV - Vk2.
Finally, βΦ in (1) is given by
βφ = √2σmax(X ) φmax + φmaxχ
max .	(A.26)
E.1 ESTIMATING μφ,Vφ
We now estimate the random quantities μφ, vφ in our neural network setting. They key quantities to
estimate are σmin(φ(W0X)) and σmax (φ(W0X)). To that end, we consider Hermite decomposition
of the activation function φ.
We start with the basic definition of Hermite polynomial and its properties. Let i ≥ 0 and let
qi : R → R denote the i-th Hermite polynomial. Note that qi ’s form an orthogonal basis for the
Hilbert space of functions.:
H
U : R → R | / u2(x) exp
<∞
which is equipped with the inner product
hu,viH = √2∏ / u(x)v(x)exp (—x2~) dX
for u, v ∈ H. We consider probabilist’s convention of Hermite polynomial. Specifically, for i, j ≥ 0,
we have
hqi,qjiH= i0! ii 6== jj,.
(A.27)
Using the above orthogonal basis to decompose φ(W0X), we have
∞
Φ(W0X) = X Ci ∙ qi (W0X)	(A.28)
i=0
where ci = hφ, qiiH and each matrix qi(W0X) ∈ Rd1 ×m is formed by applying qi entry-wise to the
matrix W0X . Let us denote
M0 := φ(X>W0>)φ(W0X).
In the following, we first obtain E[MM0] = E[φ(X> W0,)φ(W0X)] with W0 〜N(0,1) and then
obtain a lower bound on σmin(E[M 0]) and an upper bound on σmin(E[M 0]) by appropriately scaling
the data matrix X .
Applying Hermite decomposition (A.28) and taking expectation, we have
E[MM0] = E .(X>W0>)φ(W0X)]
∞
=X cj!E[qi(X>W0>)qj(W0x)]
i,j=0 i!j!
(A.29)
18
Under review as a conference paper at ICLR 2022
where the expectation is w.r.t. the random matrix W0. Let Xa ∈ Rd0 denote the a-th column of the
training data X . Each summand in (A.29) is an m × m matrix where
d1
I	∕*v -1-	I	I	I
Mqi(X>W0>)qj (W0X)〕L = X E [%(X>Wo,c,→)qj (W>,,→Xb)],
(A.30)
〜
〜
where W0,c,→ is the c-th row of W0 for a, b ∈ [m].
In SUmmand on the RHS of (A.30), We note that there is a linear combination of WW0'S elements
inside of each Hermite polynomial.
We Use the properties of Hermite polynomials (Olver et al., 2010)[§18.18.11]:
(a2 +------+ 川
i
2 〜/aɪXi + …+ a.Xr
i!
qi
(a2 +----+ a.)1
)=X _ ：：!..；!星1 (XI) …叱(Xr)	(A.31)
s1+-Hsr =i
where q's form an orthogonal basis, equipped with the inner product hu, ViH =
√∏ / u(x)v(x) exp(-x2)dx. This basis follows the physicist,s convention of Hermite polynomial.
Since 或 and qi are rescalings of the other, we can replace qis into (A.31). Note that we have
kXa k2 = 1 for all a ∈ [n]. Then we have
xS1 • • • xSd0
qi(χ>W0,c,→) = i! X ⅛⅛f
sι + ∙∙∙ + sdo =i	d0'


qS 1 (W0,c,1) ∙ ∙ ∙ qSdO(W0,c,d0 )
(A.32)


where xα,k and Wo,c,k are k-th entry of Xa and Wo,c,→ for k ∈ [do]. Using the expansion in (A.32),
We expand (A.30) as folloWs:
ζi,j(a, b) = i!j!
s1+…+sd0 =is1 +…+sdo =j
xS1	xSd0	xS1
xa,1	xa,do	XbJ
Si!…Sdo !	Si
xS0d0
TtPs,s，(W0,c,→)
• • • Sd0!
2 P	(xa,1xb,1)
(i ) 乙 Sl+…+Sd0 =i
0
Fj∙(xa,d0 xb,d0 )sd0
s1!…sdθ
i = j,
i 6= j
(A.33)
i! Psι+∙∙∙+sd0 =i (si,-、。) (XajXb,1)S1 …(xa,doxb,do )sd0
0
i = j,
i 6= j
where ζi,j'(a, b) = E1(x>w0,c,→)qj(W>c,→
Ps,s0 (W0,c,→) = E [qsi (W0,c,1) ∙
s = [s1, • • • ,sd0], and s0 = [s01, • • • , s0d0].
• ∙ qSdO (WO,c,do ) ∙ qs0ι (W0,c,1) ∙ ∙ ∙ qsdo
(W0,c,do )],
To simplify the expression in (A.33), we define X*i ∈ Rd0 ×m where the a-th column is given by
Xai = Vec(Xa ③…③ Xa) ∈ Rd0 ,
which is also called Khatri-Rao product. For i = 0, we use the convention that X*0 = 11> ∈ Rm×m
We can rewrite (A.33) as follows:
ζi,j (a, b) =
i!hχai,χbii	i=j
i 6= j.
(A.34)
(
(
Σ
0


Substituting (A.34) back into (A.30), we find that
[E[qi(X>W0>)qj(W0X)], ° = X E [%(X>Wo,c,→)qj(W.,→Xb)]
a,	c=1
_ ∫d1i!(Xαiχii i = j
(A.35)
0
i 6= j.
19
Under review as a conference paper at ICLR 2022
Substituting (A.35) into (A.29), we have
∞2
C011> + c2X>X + X 卡(X*i)>X*i .	(A.36)
We now establish an upper bound on σmaχ (P∞=2 ci!(X*i)>X*i)：
X∞c2	∞ c2
才(X*i)>χ*i ≤ X 才σmaχ((X*i)>χ*i)
i=2	(A.37)
≤ c2 σ2 (X)
∞ max
where c∞ is given by
∞2
c∞∞ = X ⅜,
i=2
which is finite provided that kφkH is bounded.
Using (A.37), We now establish an upper bound on σmaχ(E[MM0]):
σmax(EM0]) . di (mc0 + (c2 + c∞Jσ21aχ(X))
2
Moreover, suppose there exists some t such that σma(X*t) > 0. Then we have di 才σ21jn(X*t).
σmin(E[M0]). This requires to have d0 ≥ m. Putting together the lower bound on σmin(E[MM0])
and the upper bound on σmin(E[M 0]), noting W0 = ωiW 0, and scaling X accordingly to take into
account the coefficient ωi , we have
c2
ω2td1 才σ21in(X*t) . σmin(E[M0]) ≤ σmaχ(E[M0]) . di (mc2 + ω2 (C2 + -)/2^(X)).
(A.38)
E.2 CONCENTRATION OF THE RANDOM MATRIX M 0
To see how well the random matrix M 0 concentrates about its expectation, note that
M 0 = φ(X>W0>)φ(W0X)
d1
= X φ(X>W0>,i,→)φ(W0,i,→X)
i=i
d1
= X Ai
i=i
(A.39)
where {Ai}id=1 i ⊂ Rm×m are independent random matrices.
Consider the event Ei that
max ∣∣W0,i,→k2 . k1ω1 √ do log di,	max ∣∣%,i,j∣2 . k2ω2 Vz d2 log di	(A.40)
i∈[d1]	i∈[d1]
where Vo,i,ψ is the i-th column of V0. Note that Wo,i,→ ∈ Rd0 and V0,ij ∈ Rd2 are random zero-
mean Gaussian vectors whose entries’ variances are ωi2 and ω22, respectively. Therefore, with an
application of the scalar Bernstein inequality (Vershynin, 2012, Proposition 5.16), followed by the
union bound, we observe that the event Ei happens except with a probability of at most
pi := di-Ck1d0 + di-Ck2d2 ,
(A.41)
for a universal constant C with sufficiently large ki, k2.
20
Under review as a conference paper at ICLR 2022
Let i ∈ [d1]. Conditioned on the event E1, an upper bound on kφ(X>W0,i,→)k2 is given by:
kΦ(X>W0,i,→)k2 . φmaχσmaχ(X)k 1 ω 1 Pd0Iogd1.	(A.42)
Moreover, we have
σmax(Ai) = kφ(X>W0,i,→)k22
= kφ(X>W0,i,→) - φ(0)k22	(A.43)
.φmnaxσ2riax(X)k2ω2d0 log d1∙
We now focus on the concentration of σmin(M0) and σmax(M0). We use a concentration property,
which provides the tail bound of f(W) = φ(X> W>)φ(WX) with multivariate Gaussian input W.
In the following lemma, we show that f is a Lipschitz function, and its Lipschitz constant explains
how f (W ) concentrates around its mean.
LemmaA.7. Let f(W) = φ(X>W>)φ(WX). Suppose W satisfies (A.40). Then f is K-Lipschitz
function with constant K = 4φ需&乂。2^(X)k1ω1 ʌ/do log di. So we have
kf(W) - f(W 0)k < 4φ maχσmaχ(X )k1ω1 Pdiogd ∙kW - W 0k∙
Proof. Note that f (W0) = M0 and f can be represented as
d1
f(x ) = X fi(Wi,→)
i=1
where fi is given by fi(Wi,→) = φ(X>Wi>,→)φ(Wi,→X). We prove that each fi is K-Lipschitz,
which implies that f is also K-Lipschitz.
We note that fi’s can be expressed as a composition of three functions:
fi (v) = (g1 ◦ g2 ◦ g3)(v)
where g1 , g2 , and g3 are given by
g1 (v) = vv> , f2(v) = φ(v), f3(v) = vX.	(A.44)
It is clear that g2 is φmaχ-Lipschitz, and g3 is σmaχ(X)-Lipschitz from their definitions. Lipschitz
constant of g1 comes from the domain bound as follows:
kg1 (v + δv) - g1 (v)k = kδvv> + vδv> + δvδv> k
≤ 2kδvv> k + kδvδv> k	(A.45)
≤ ⑵Wk + kδvk) ∙kδvk.
A bound on (2kvk + kδvk) is obtained in (A.42). Then g1 is K1-Lipschitz function with
Ki = 4φmaχσmaχ(X)kiωi√d01og^d1. Therefore, all gi, g2 and g3 are Lipschitz function, so
their composition f is also Lipschitz function with constant K = 4φmaχσ21aχ(X)k1ω1√d0 log di,
which completes the proof.	□
Lemma A.8. Let z ∈ Rd denote a Gaussian random vector. Then we have Pr{kz - E[z]k >
t |E2 } . exp(-t2 ) where E2 is the event that kzk is bounded.
-c-r τ	C	.1	,	∙1 T , ∙1	1' Tl ʃ	J*∕TT7- ∖ ɪ τ ∙ ɪ	A ∣-t	1 A rɪ	1
We can focus on the tail distribution of M0 = f(W0). Using Lemmas A.7 and A.8, we have
Pr{kM0-E[M0]k > t |Ei}. exp(-k32)	(A.46)
where t = k34φm&乂。2^(%)k1ω1√d0 log di with some constant k3.
21
Under review as a conference paper at ICLR 2022
Using (A.46), we now establish a tail bound on σmin (M0):
Pr{σmm(M0) ≤ (1 - δι)σmm(E[M0])∣Eι} ≤ Pr{∣σmin(M0) - σmm(E[M,0])| ≥ δισmm(E[M0])∣Eι}
≤ Pr{σmin(M0 - E[M0]) ≥ δισmin(E[M0])∣Eι}
≤ Pr{σmaχ(M0 - E[M0]) ≥ SGn(E[M0])∣Eι}
≤ Pr{kM 0-E[M 0]k ≥ δισmin(E[M 0])∣EJ
. p2
where
P2= exp (- ( .	δ"(EM0])— Y].
4φ⅛aχσmaχ(X)k1ω1√d0 log d1 J
Similarly, we obtain
Pr{σmaχ(M0) ≥ (1+ δ2)σmaχ(E[M0])∣Eι} . P3
where
p3 = exp
δ2σmax(E[M 0])
4φmaxσiLx(x)k1ω1√d0 log d
Putting these bounds together with (A.38), we have :
4(1 - δ1)木d1 σmin(X*t) ≤
σmin(φ(W0X))
ω
t
1
σmaχ(φ(W0X)) ≤ p(1 + δ2)(ω1 ʌ/(c2 + c∞)dισmaχ(X) + ∣c0∣p⅛m)
(A.47)
except with a probability of at most p1 + p2 + p3 .
With establishing the bounds on σmin(φ(W0X)) and σmax (φ(W 0 X)), we can finally estimate
μΦ, νφ as follows:
E.3 LOWER BOUND ON μφ
A lower bound on μφ is given by
ωt J(1 - δι)c!diσmin(X*t) ≤ σmin(φ(W0X)) = μφ,	(A.48)
except with a probability of at most p1 + p2 .
E.4 UPPER BOUND ON νΦ
Since vφ = φmaχσmaχ(X)σmaχ(V0) + σmaχ(φ(W0X)), we obtain a bound on σmaχ(V0):
Since V0 is a Gaussian random matrix, we have
σmaχ(V0) ≤ ω2(2Pdl + pd2) . ω2Pdl	(A.49)
except with a probability of at most p4 = exp(-Cd1) where C is a universal constant (Vershynin,
2012)[Corollary 5.35].
Combining (A.49) with the upper bound on σmax (φ(W 0 X)), we have
νΦ = φmax σmax (X)σmax (V ) + σmax (φ(W0 X))
. ω2 φmax σmax(X) pd1 + ωι ʌ/(l + δ2)(c2 + c∞)dισ max (X) + c0 p(1 + δ2)d1m
except with a probability of at most p1 + p3 + p4 .
22
Under review as a conference paper at ICLR 2022
E.5 UPPER BOUND ON h(Θ0)
In this section, we bound h(Θ0). Using ka + bk22 ≤ 2kak22 + 2kbk22, we have
h(Θ0) = -1 ∣∣V0φ(W0X) - Yk2
m2	2	(A.50)
≤ -kV0φ(W0X)k2 + -kYk2.
mm
To upper bound the random norm in (A.50), we first decompose V0φ(W0X) into terms including
Wo,i,→ ∈ Rd0 and ¾,ψ ∈ Rd2 as follows:
d1
V0φ(W0X) = XBi
i=1
(A.51)
where Bi = Voi,χΦ(W>i→X) ∈ Rd2 ×m's are independent random matrices for i ∈ [dι].
Conditioned on the event E1 defined in (A.40), we bound ∣Bi ∣:
∣Bik = k¾d∣2kΦ(W>i,→X )∣2
≤ kV0,i,ψk2 ∙ φmaχσmaχ(X)kiωι，d。log d	(A.52)
≤ ω1ω2φmaχσmaχ(X)k#2 VdOd2 log d
for i ≤ d1 .
Substituting the upper bound in A.51 into A.52 and applying the Hoeffding inequality (Hoeffding,
1963), we have
Pr{∣ V0φ(W0X)k & u(do, di, d2)∣E1} = Pr{∣V0φ(W0X) - E[V0φ(W0X))∣Eι]∣ & u(do, di, d2)∣E1}
≤ Pr Xd1 ∣Bi -E[Bi]∣ & u(d0,d1,d2)|E1
≤ p5
where
u(d0,d1,d2) = δ3ω1ω2φmaχklk2 √d0dld2σmaχ(X)lθg di
and p5 = exp(-Cδ32) with δ3 ≥ 0 and a universal constant C.
Therefore, under the event Ei , we have
22
h(Θ0) ≤ — ∣∣V0φ(W0X)k2 + — ∣∣Yk2
(A.53)
.Jδ3ω2ω2φmaxk2k2d0dld"max(X) lθg2 d1 + J kYk2
except with a probability of at most pi + p5. It is natural to assume that d2 = o(di). We also have
∣Y ∣ ≤ 1.
Suppose that
1
ω1ω2 . —	____
Φmax √d0d1 log di
(A.54)
Substituting (A.54) into (A.53), we have
h(Θ0) ≤ -1 δ2k2k∣σmaχ(X)	(A.55)
m
where δ3, ki, and k2 are all constants and independent of d0, di, and m.
23