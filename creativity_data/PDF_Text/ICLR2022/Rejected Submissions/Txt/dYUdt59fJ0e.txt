Under review as a conference paper at ICLR 2022
Yformer: U-Net Inspired Transformer
Architecture for Far Horizon
Time Series Forecasting
Anonymous authors
Paper under double-blind review
Ab stract
Time series data is ubiquitous in research as well as in a wide variety of industrial
applications. Effectively analyzing the available historical data and providing in-
sights into the far future allows us to make effective decisions. Recent research has
witnessed the superior performance of transformer-based architectures, especially
in the regime of far horizon time series forecasting. However, the current state
of the art sparse Transformer architectures fail to couple down- and upsampling
procedures to produce outputs in a similar resolution as the input. We propose
the Yformer model, based on a novel Y-shaped encoder-decoder architecture that
(1) uses direct connection from the downscaled encoder layer to the correspond-
ing upsampled decoder layer in a U-Net inspired architecture, (2) Combines the
downscaling/upsampling with sparse attention to capture long-range effects, and
(3) stabilizes the encoder-decoder stacks with the addition of an auxiliary recon-
struction loss. Extensive experiments have been conducted with relevant baselines
on four benchmark datasets, demonstrating an average improvement of 19.82,
18.41 percentage MSE and 13.62, 11.85 percentage MAE in comparison to the
current state of the art for the univariate and the multivariate settings respectively.
1	Introduction
In the most simple case, time series forecasting deals with a scalar time-varying signal and aims
to predict or forecast its values in the near future; for example, countless applications in finance,
healthcare, production automatization, etc. (Carta et al., 2021; Cao et al., 2018; Sagheer & Kotb,
2019) can benefit from an accurate forecasting solution. Often not just a single scalar signal is
of interest, but multiple at once, and further time-varying signals are available and even known
for the future. For example, suppose one aims to forecast the energy consumption of a house, it
likely depends on the social time that one seeks to forecast for (such as the next hour or day), and
also on features of these time points (such as weekday, daylight, etc.), which are known already
for the future. This is also the case in model predictive control (Camacho & Alba, 2013), where
one is interested to forecast the expected value realized by some planned action, then this action is
also known at the time of forecast. More generally, time series forecasting, nowadays deals with
quadruples (x, y, x0 , y0) of known past predictors x, known past targets y, known future predictors
x0 and sought future targets y0 . (Figure 3 in appendix section A provides a simple illustration)
Time series problems can often be addressed by methods developed initially for images, treating
them as 1-dimensional images. Especially for time-series classification many typical time series
encoder architectures have been adapted from models for images (Wang et al., 2017; Zou et al.,
2019). Time series forecasting then is closely related to image outpainting (Van Hoorick, 2019),
the task to predict how an image likely extends to the left, right, top or bottom, as well as to the
more well-known task of image segmentation, where for each input pixel, an output pixel has to be
predicted, whose channels encode pixel-wise classes such as vehicle, road, pedestrian say for road
scenes. Time series forecasting combines aspects from both problem settings: information about
targets from shifted positions (e.g., the past targets y as in image outpainting) and information about
other channels from the same positions (e.g., the future predictors x0 as in image segmentation).
One of the most successful, principled architectures for the image segmentation task are U-Nets
introduced in Ronneberger et al. (2015), an architecture that successively downsamples / coarsens
1
Under review as a conference paper at ICLR 2022
its inputs and then upsamples / refines the latent representation with deconvolutions also using the
latent representations of the same detail level, tightly coupling down- and upsampling procedures
and thus yielding latent features on the same resolution as the inputs.
Following the great success in Natural Language Processing (NLP) applications, attention-based,
esp. transformer-based architectures (Vaswani et al., 2017) that model pairwise interactions between
sequence elements have been recently adapted for time series forecasting. One of the significant
challenges, is that the length of the time series, are often one or two magnitudes of order larger than
the (sentence-level) NLP problems. Plenty of approaches aim to mitigate the quadratic complexity
O(T 2) in the sequence/time series length T to at most O(T logT). For example, the Informer archi-
tecture (Zhou et al., 2020), arguably one of the most accurate forecasting models researched so far,
adapts the transformer by a sparse attention mechanism and a successive downsampling/coarsening
of the past time series. As in the original transformer, only the coarsest representation is fed into the
decoder. Possibly to remedy the loss in resolution by this procedure, the Informer feeds its input a
second time into the decoder network, this time without any coarsening.
While forecasting problems share many commonalities with image segmentation problems,
transformer-based architectures like the Informer do not involve coupled down- and upscaling pro-
cedures to yield predictions on the same resolution as the inputs. Thus, we propose a novel Y-shaped
architecture called Yformer that
1.	Couples downscaling/upscaling to leverage both, coarse and fine-grained features for time
series forecasting,
2.	Combines the coupled scaling mechanism with sparse attention modules to capture long-
range effects on all scale levels, and
3.	Stabilizes encoder and decoder stacks by reconstructing the recent past.
2	Related Work
Deep Learning Based Time Series Forecasting: While Convolutional Neural Network (CNN)
and Recurrent Neural network (RNN) based architectures (Salinas et al., 2020; Rangapuram et al.,
2018) outperform traditional methods like ARIMA (Box & Jenkins, 1968) and exponential smooth-
ing methods (Hyndman & Athanasopoulos, 2018), the addition of attention layers (Vaswani et al.,
2017) to model time series forecasting has proven to be very beneficial across different problem
settings (Fan et al., 2019; Qin et al., 2017; Lai et al., 2018). Attention allows direct pair-wise in-
teraction with eccentric events (like holidays) and can model temporal dynamics inherently unlike
RNN’s and CNN’s that fail to capture long-range dependencies directly. Recent work like Reformer
(Kitaev et al., 2020), Linformer (Wang et al., 2020) and Informer (Zhou et al., 2020) have focused on
reducing the quadratic complexity of modeling pair-wise interactions to O(T logT) with the intro-
duction of restricted attention layers. Consequently, they can predict for longer forecasting horizons
but are hindered by their capability of aggregating features and maintaining the resolution required
for far horizon forecasting.
U-Net: The Yformer model is inspired by the famous U-Net architecture introduced in Ronneberger
et al. (2015) originating from the field of medical image segmentation. The U-net architecture is
capable of compressing information by aggregating over the inputs and up-sampling embeddings to
the same resolutions as that of the inputs from their compressed latent features. Current transformer
architectures like the Informer (Zhou et al., 2020) do not utilize up-sampling techniques even though
the network produces intermediate multi resolution feature maps. Our work aims to capitalize on
these multi resolution feature maps and use the U-net shape effectively for the task of time series
forecasting. Previous works like Stoller et al. (2019) and Perslev et al. (2019) have successfully ap-
plied U-Net architecture for the task of sequence modeling and time series segmentation, illustrating
superior results in the respective tasks. These work motivate the use of a U-Net-inspired architec-
ture for time series forecasting as current methods fail to couple sparse attention mechanism with
the U-Net shaped architecture. Additional related works section is decoupled from the main text and
is presented in the appendix section B.
2
Under review as a conference paper at ICLR 2022
3	Problem Formulation
By a time series x with M channels, we mean a finite sequence of vectors in RM , denote their
space by R*×M ：= UT∈n RT×M, and their length by |x| := T (for X ∈ RT×M, M ∈ N). We write
(x, y) ∈ R*×(M+O) to denote two time series of same length with M and O channels, respectively.
We model a time series forecasting instance as a quadruple (χ,y,χ0,y0) ∈ R*×(M+O) X
R*×(M+O), where χ,y denote the past predictors and targets until a reference time point T and
x0, y0 denote the future predictors and targets from the reference point T to the next τ time steps.
Here, τ = |x0 | is called the forecast horizon.
For a Time Series Forecasting Problem, given (i) a sample D := {(x1, y1, x01, y10 ),
. . . , (xN, yN, x0N, yN0 )} from an unknown distribution p of time series forecasting instances and (ii)
a function ' : R*×(O+O)→ R called loss, we attempt to find a function y : R*×(M +O) × R*×m →
R*×o (with ∣y(x,y,x0)∣ = |x0|) with minimal expected loss
E(x,y,x0,y0)~p '(y , y(X, y, X ))
(1)
The loss ` usually is the mean absolute error (MAE) or mean squared error (MSE) averaged over
future time points:
1	|y0| 1	1	|y0| 1
'mae(y0,y)：=的 EO∙I∣y0 -yt∣∣ι,	'mse(y0,y) ：= ME OI∣y0 - yt∣l2
(2)
Furthermore, if there is only one target channel and no predictor channels (O = 1, M = 0), the time
series forecasting problem is called univariate, otherwise multivariate.
4	Background
Our work incorporates restricted attention based Transformer in a U-Net inspired architecture. For
this reason, we base our work on the current state of the art sparse attention model Informer, in-
troduced in Zhou et al. (2020). We provide a brief overview of the sparse attention mechanism
(ProbSparse) and the encoder block (Contracting ProbSparse Self-Attention Blocks) used in the
Informer model for completeness.
ProbSparse Attention: The ProbSparse attention mechanism restricts the canonical attention
(Vaswani et al., 2017) by selecting a subset u of dominant queries having the largest variance across
all the keys. Consequently, the query Q ∈ RLQ×d in the canonical attention is replaced by a sparse
query matrix Q ∈ RLQ ×d consisting of only the U dominant queries. ProbSParse attention can
hence be defined as:
APropSparse (Q, K, V)
Softmax(
(3)
where d denotes the input dimension to the attention module. For more details on the ProbSParse
attention mechanism, we refer the reader to Zhou et al. (2020).
Contracting ProbSparse Self-Attention Blocks: The Informer model uses Contracting ProbSParse
Self-Attention Blocks to distill out redundant information from the long history input sequence
(X, y) in a pyramid structure motivated from the image domain (Lin et al., 2017). The sequence
of operations within a block begins with a ProbSParse self-attention that takes as input the hidden
representation hi from the ith block and projects the hidden representation into query, key and value
for self-attention. This is followed by multiple layers of convolution (Conv1d), and finally the
MaxPool operation reduces the latent dimension by effectively distilling out redundant information
at each block. We refer the reader to Algorithm 2 in the appendix section C where these operations
are presented in an algorithmic structure for a comprehensive overview.
3
Under review as a conference paper at ICLR 2022
5	Methodology
Past sequence
past sequence
-Y-Past Fncnder
Y-FiJtue Encoder ∙
Past sequence # Future Predictors
l^token/ Vtoken/ % J
(a) Informer Architecture
(b) Yformer Architecture
Figure 1: Comparison of Informer and Yformer architecture. (1) The Informer architecture process
part of the past input data (x, y) within the decoder as (xtoken, ytoken) along with the future predictors
(x0). The Yformer avoids this redundant reprocessing and uses a masked self-attention network for
embedding the future predictors (x0). (2) The Informer uses the final encoder embedding as the input
to the decoder. The Yformer passes a concatenated ( ++ ) representation (ei) of the ith Y-Past and
Y-Future Encoder embedding to the I - ith layer of the Y-Decoder, forming a U-Net connection
(represented in red) between the encoder and the decoder. (3) The Yformer architecture predicts
both the input reconstruction ypast and future predictions yfut, to encourage learning embeddings that
can produce similar output distribution as the inputs.
The Yformer model is a Y-shaped (Figure 1b) symmetric encoder-decoder architecture that is specif-
ically designed to take advantage of the multi-resolution embeddings generated by the Contracting
ProbSparse Self-Attention Blocks. The fundamental design consideration is the adoption of U-Net-
inspired connections to extract encoder features at multiple resolutions and provide direct connection
to the corresponding symmetric decoder block (simple illustration provided in Figure 4, appendix
section A). Furthermore, the addition of reconstruction loss helps the model learn generalized em-
beddings that better approximate the data generating distribution.
The Y-Past Encoder of the Yformer is designed using a similar encoder structure as that of the
Informer. The Y-Past Encoder embeds the past sequence (x, y) into a scalar projection along with the
addition of positional and temporal embeddings. Multiple Contracting ProbSparse Self-Attention
Blocks are used to generate encoder embeddings at various resolutions. The Informer model uses
the final low-dimensional embedding as the input to the decoder (Figure 1a) whereas, the Yformer
retains the embeddings at multiple resolutions to be passed on to the decoder. This allows the
Yformer to use high-dimensional lower-level embeddings effectively.
The Y-Future Encoder of the Yformer mitigates the issue of the redundant reprocessing of parts
of the past sequence (x, y) used as tokens (xtoken, ytoken) in the Informer architecture. The Informer
model uses only the coarsest representation from the encoder embedding, leading to a loss in resolu-
tion and forcing the Informer to pass part of the past sequence as tokens (xtoken, ytoken) to the decoder
(Figure 1a). The Yformer separates the future predictors and the past sequence (x, y) bypassing the
future predictors (x0) through a separate encoder and utilizing the multi-resolution embeddings to
dismiss the need for tokens entirely. Unlike the Y-Past Encoder, the attention blocks in the Y-Future
encoder are based on masked canonical self-attention mechanism (Vaswani et al., 2017). Mask-
ing the attention ensures that there is no information leak from the future time steps to the past.
Moreover, a masked canonical self-attention mechanism helps reduce the complexity, as half of the
query-key interactions are restricted by design. Thus, the Y-Future Encoder is designed by stacking
4
Under review as a conference paper at ICLR 2022
multiple Contracting ProbSparse Self-Attention Blocks where the ProbSparse attention is replaced
by the Masked Attention. We name these blocks Contracting Masked Self-Attention Blocks (Algo-
rithm 3 appendix section C).
The Yformer processes the past inputs and the future predictors separately within its encoders. How-
ever, considering the time steps, the future predictors are a continuation of the past time steps. For
this reason, the Yformer model concatenates (represented by the symbol ++ ) the past encoder em-
bedding and the future encoder embedding along the time dimension after each encoder block,
preserving the continuity between the past input time steps and the future time steps. Let i represent
the index of an encoder block, then eip+as1t and efiu+t 1 represent the output from the past encoder and
the future encoder respectively. The final concatenated encoder embedding (ei+1) is calculated as,
eip+as1t = ContractingProbSparseSelfAttentionBlock(eipast)
efiu+t 1 = ContractingMaskedSelfAttentionBlock(efiut)
past fut
ei+1 = ei+1 ++ ei+1
(4)
The encoder embeddings represented by E = [e0, . . . , eI] (where I is the number of encoder layers)
contain the combination of past and future embeddings at multiple resolutions.
The Y-Decoder of the Yformer consists of two parts. The first part takes as input the final concate-
nated low-dimensional embedding (eI) and performs a multi-head canonical self-attention mech-
anism. Here, the past encoder embedding (eIpast) is allowed to attend to itself as well as the fu-
ture encoder embedding (efIut) in an unrestricted fashion. The encoder embedding (eI) is the low-
dimensional distilled embedding, and skipping query-key interaction within these low-dimensional
embeddings might deny the model useful pair-wise interactions. Therefore, it is by design that this is
the only part of the Yformer model that uses canonical self-attention in comparison to the Informer
that uses canonical attention within its repeating decoder block, as shown in Figure 1a. Since the
canonical self-attention layer is separated from the repeating attention blocks within the decoder, the
Yformer complexity from this full attention module does not increase with an increase in the number
of decoder blocks. The U-Net architecture inspires the second part of the Y-Decoder. Consequently,
the decoder is structured in a symmetric expanding path identical to the contracting encoder. We
realize this architecture by introducing upsampling on the ProbSparse attention mechanism using
Expanding ProbSparse Cross-Attention Block.
The Expanding ProbSparse Cross-Attention Block within the Yformer decoder performs two
tasks: (1) upsample the compressed encoder embedding eI and (2) perform restricted cross atten-
tion between the expanding decoder embedding dI-i and the corresponding encoder embedding
ei (represented in Figure 4 appendix section A). We accomplish both the tasks by introducing an
Expanding ProbSparse Cross-Attention Block as illustrated in Algorithm 1.
Algorithm 1 Expanding ProbSparse Cross-Attention Block
Input : dI-i , ei
Output : dI-i+1
dɪ-i+ι - ProbSParseCrossAttn(dι-i,ei)
dɪ-i+ι - ConvId(di-i+ι)
dɪ-i+ι - ConvId(di-i+ι)
dɪ-i+ι J LayerNorm(dι-i+ι)
dɪ-i+ι J ELU(ConvTransPose1d(dι-i+ι)))
The Expanding ProbSparse Cross-Attention Blocks within the Yformer decoder uses a
ProbSParseCrossAttn to construct direct connections between the lower levels of the encoder and
the corresponding symmetric higher levels of the decoder. Direct connections from the encoder
to the decoder are an essential component for majority of the models within the image domain.
For example, ResNet (He et al., 2016), and DenseNet (Huang et al., 2017) have demonstrated that
direct connections between previous feature maps, strengthen feature propagation, reduce parame-
ters, mitigate vanishing gradients and encourage feature reuse. However, current transformer-based
architectures like the Informer fail to utilize such direct connections. The ProbSParseCrossAttn
5
Under review as a conference paper at ICLR 2022
takes in as input the decoder embedding from the previous layer dI-i as queries and the correspond-
ing encoder embedding ei as keys. The Yformer uses the ProbSparse restricted attention so that the
model is scalable with an increase in the number of decoder blocks.
We utilize ConvTranspose1d or popularly known as Deconvolution for incrementally increasing
the embedding space. The famous U-Net architecture uses a symmetric expanding path using such
Deconvolution layers. This property enables the model to not only aggregate over the input but also
upscale the latent dimensions, improving the overall expressivity of the architecture. The decoder
of Yformer follows a similar strategy by employing Deconvolution to expand the embedding space
of the encoded output. We describe the different operators used in the appendix section C.
A fully connected layer (LinearLayer) predicts the future time steps yfut from the final decoder layer
(di) and additionally reconstructs the past input targets ypast.
[ypast, yfut] = LinearLayer(dɪ)
(5)
The addition of reconstruction loss to the Yformer as an auxiliary loss, serves two significant pur-
poses. Firstly, the reconstruction loss acts as a data-dependent regularization term that reduces
overfitting by learning embeddings that are more general (Ghasedi Dizaji et al., 2017; Jarrett &
van der Schaar, 2020). Secondly, the reconstruction loss helps in producing future-output in a sim-
ilar distribution as the inputs (Bank et al., 2020). For far horizon forecasting, we are interested in
learning a future-output distribution. However, the future-output distribution and the past-input dis-
tribution arise from the same data generating process. Therefore having an auxiliary reconstruction
loss would direct the gradients to a better approximate of the data generating process. The Yformer
model is trained on the combined loss `,
' =α'mse(y,ypast) + (1 - α) 'mse(y0,yfut)
(6)
where the first term tries to learn the past targets y and the second term learns the future targets y0 .
We use the reconstruction factor (α) to vary the importance of reconstruction and future prediction
and tune this as a hyperparameter.
6	Experiments
6.1	Datasets
To evaluate our proposed YFormer architecture, we use two real-world public datasets and one
public benchmark to compare the experiment results with the published results by the Informer.
ETT (Electricity Transformer Temperature1): This real-world dataset for the electric power deploy-
ment introduced by Zhou et al. 2020 combines short-term periodical patterns, long-term periodical
patterns, long-term trends, and irregular patterns. The data consists of load and temperature readings
from two transformers and is split into two hourly subsets ETTh1 and ETTh2. The ETTm1 dataset
is generated by splitting ETTh1 dataset into 15-minute intervals. The dataset has six features and
70,080 data points in total. For easy comparison, we kept the splits for train/val/test consistent with
the published results in Zhou et al. 2020, where the available 20 months of data is split as 12/4/4.
ECL (Electricity Consuming Load2): This electricity dataset represents the electricity consumption
from 2011 to 2014 of 370 clients recorded in 15-minutes periods in Kilowatt (kW). We split the data
into 15/3/4 months for train, validation, and test respectively as in Zhou et al. 2020.
6.2	Experimental Setup
Baseline: Our main baseline is the the Informer architecture by Zhou et al. 2020. The results
presented in the paper were reported to have a data scaling issue3, and the authors have updated
1Available under https:// github.com/zhouhaoyi/ETDataset.
2Available under https://archive.ics.uci.edu/ml/ datasets/ElectricityLoadDiagrams20112014
3https://github.com/zhouhaoyi/Informer2020/issues/41
6
Under review as a conference paper at ICLR 2022
Table 1: MAE for the univariate time series forecasting task. The best result is highlighted in bold
and the second-best in italic and red. Informer* here represents a modified version of the standard
informer, which uses the canonical attention module.
Dataset	Horizon (T)	Methods					Improvement %
		LogTrans	LSTnet	Informer*	Informer	Yformer	
ETThI	24	-0.259-	0.280	-0246-	-0.247-	0.230	6.50
	48	-0.328-	0.327	-0.322-	-0.319	0.308	3.45
	168	-0.375-	0.422	-0.355-	-0.346	0.268-	22:54
	336	-0.398-	0.552	-0369-	-0.387-	0.365	1.08
	720	-0.463-	0.707	-0421-	-0.435-	0.394-	6.41
ETTh2	24	-0.255-	0.263	-0.241-	-0.240	0.221-	7.92
	48	-0.348-	0.341	-0317 -	0.314-	0.334-	-6.37
	168	-0.422-	0.414	-0.390-	-0.389	0.337	13.37
	336	-0.437-	0.607	-0.423-	-0.417	0.391-	6.24
	720	-0.493-	0.58-	-0.442-	-0431-	0.382-	1ΓT37
ETTml	24	-0.202-	0.243	-0.160-	-0.137	0.118	13.87
	48	-0.220-	0.362	-0194-	-0.203-	0.173-	10.82
	96	-0.386-	0.496	-0.384-	-0.372	0.311-	16.40
	288	-0.572-	0.795	-0548-	-0.554-	0.316	42.34
	672	-0.702-	1.352	-0.664-	-0644 -	-0.476-	26.09
ECL	48	-0.429-	0357	-0.368-	-0.359-	0.322-	9.80
	168	-0.529-	0436	-0.514-	-0.503-	0.361-	17.20
	336	-0.563-	0519	-0.552-	-0.528-	0.375	2775
	720	-0.609-	0.595	-0.578-	-0571-	0.479	19.50
	960	―	-0.645-	0.683	-0.638-	-0608	0.573-	16.11
# wins per method		0	0	0	1	19	
						Avg	13.62
their results in the official GitHub repository4. Therefore we compare against these updated results.
Recently, the Query Selector (Klimek et al., 2021) utilize the Informer framework for far horizon
forecasting, however they report results from the Informer paper before the bug fix and hence we
avoid comparison with this work. As a second baseline, we also compare the second-best performing
model in Zhou et al. 2020 which is the Informer that uses canonical attention module. It is repre-
sented as Informer* in the tables. Furthermore, we also compare against DeepAR (Salinas et al.,
2020), LogTrans (Li et al., 2019), and LSTnet (Lai et al., 2018) architectures as they outperformed
the Informer baseline for certain forecasting horizons. For a quick analysis, we present the percent
improvement achieved by the Yformer over the current best results.
For easy comparison, we choose two commonly used metrics for time series forecasting to evaluate
the Yformer architecture, the MAE and MSE in Equation 2. We report the results for the MAE in
the paper and provide the MSE results in the appendix section D. We performed our experiments on
GeForce RTX 2080 Ti GPU nodes with 32 GB ram and provide results as an average of three runs.
6.3	Results and Analysis
This section compares our results with the results reported in the Informer baseline both in uni- and
multivariate settings for the multiple datasets and horizons. The best-performing and the second-best
models (lowest MAE) are highlighted in bold and red, respectively.
Univariate: The proposed Yformer model is able to outperform the current state of the art Informer
baseline in 19 out of the 20 available tasks across different datasets and horizons by an average of
13.62 % of MAE. The Table 1 illustrates that the superiority of the Yformer is not just limited to afar
horizon but even for the shorter horizons and in general across datasets. Considering the individual
datasets, the Yformer surpasses the current state of the art by 8, 6.8, 21.9, and 18.1% of MAE for
the ETTh1, ETTh2, ETTm1, and ECL datasets respectively as seen in Table 1. We also report MSE
scores in the supplementary appendix section D, which illustrates an improvement of 16.7, 12.6,
34.8 and 15.2% for the ETTh1, ETTh2, ETTm1, and ECL datasets respectively. We observe that the
MAE for the model is greater at horizon 48 than the MAE at horizon 168 for the ETTh1 dataset. This
may be a case where the reused hyperparameters from the Informer paper are far from optimal for
4Commit 702fb9bbc69847ecb84a8bb205693089efb41c6
7
Under review as a conference paper at ICLR 2022
Table 2: MAE for the multivariate time series forecasting task. The best result is highlighted in bold
and the second-best in italic and red. Informer* here represents a modified version of the standard
informer, which uses the canonical attention module.
Dataset	Horizon (T)	Methods					Improvement %
		LogTrans	LSTnet	Informer*	Informer	Yformer	
ETThI	24	-0.604-	0.901	-0.577-	-0.549	0.492-	10:38
	48	-0.757-	0.960	-0.671-	-0.625-	0.537	14.08
	168	-0.846-	1.214	-0.797-	-0752	0.684-	9.04
	336	-0.952-	1.369	-0813-	-0.873-	0.803-	1.23
	720	-1.291-	1.380	-0.9T7-	-0.896	0.803-	10:38
ETTh2	24	-0.750-	1.457	-0.727-	-0.665	0.498	25711
	48	-1.034-	1.687	-1.077-	-1001-	0.865	13.59
	168	-1.681-	2.513	-1.6T2-	-1.515	1.218	19.60
	336	-1763-	2.591	-1.285-	-1.340-	-1.283-	OT6
	720	-1.552-	3.709	-1.495-	-1.473	1.337	923
ETTmI	24	-0.412-	1.170	-0.371-	-0.369	0.363-	1.63
	48	-0.583-	1.215	-0470-	-0.503-	0.457	277
	96	-0.792-	1.542	-0612-	-0.614-	0.567	735
	288	-1.320-	2.076	-0.879-	-0.786	0.593-	24.55
	672	-1.461-	2.941	-1ΓT03-	-0926-	0.656-	29TT6
ECL	48	~04Γ8-	0.445	-0.399-	-0.393	0.390	076
	168	-0.432-	0.476	-0420-	-0.424-	0.387	786
	336	-0.439-	0.477	-0.439-	-0431 -	0.394-	8.58
	720	-0.454-	0.565	-0438-	-0.443-	0.384-	12:33
	960	-0.589-	0.599	-0.550-	-0.548	0.388	29.20
# wins per method	0	0	0	0	0	20	
						Avg	11.85
the Yformer. The other results show consistent behavior of increasing error with increasing horizon
length τ . Additionally, this behavior is also observed in the Informer baseline for ETTh2 dataset
(Table 2), where the loss is 1.340 for horizon 336 and 1.515 for a horizon of 168.
Figure 2 differentiates the improvement of Y-former architecture from the baseline Informer and
the reconstruction loss. We notice that the Yformer architecture without the reconstruction loss is
able to outperform the Informer architecture across different datasets and horizons. Additionally,
the optimal value for the loss weighting hyperparameter α is always greater than zero as shown in
Table 3, confirming the assumption that the addition of the auxiliary reconstruction loss term helps
the model to generalize well to the future distribution.
Multivariate: We observe a similar trend in the multivariate setting. Here the Yformer model
outperforms the current state of the art method in all of the 20 tasks across the four datasets by a
margin of 11.85% of MAE. There is a clear superiority of the proposed approach in longer horizons
across the different datasets. Across the different datasets, the Yformer improves the current state of
the art MAE results by 9, 13.5, 13.1, and 11.7% of MAE and 12, 26.3, 13.9, and 17.1% of MSE for
the ETTh1, ETTh2, ETTm1, and ECL datasets respectively (table in the supplementary appendix
section D). We attribute the improvement in performance to superior architecture and the ability to
approximate the data distribution for the multiple targets due to the reconstruction loss.
7	Ablation study
We performed additional experiments on the ETTh2 and ETTm1 datasets to study the impact of the
Yformer model architecture and the effect of the reconstruction loss separately.
7.1	Y-former architecture
In this section we attempt to understand (1) How much of an improvement is brought about by the
Y-shaped model architecture? and (2) How much impact did the reconstruction loss have? From
Figure 2, it is clear that the Yformer architecture performs better or is comparable to the Informer
throughout the entire horizon range (without the reconstruction loss α = 0). Moreover, we can
notice that for the larger horizons, the Yformer architecture (with α = 0) seems to have a clear
8
Under review as a conference paper at ICLR 2022
α	Horizon T			count
	short	medium	long	
0~o~	-0-	0	-1-	-1-
-0Γ^	-0-	1	-2-	-3-
-03-	-1-	2	-1-	-4-
-05-	-1-	0	-0-	-1-
-0Γ^	-5-	2	-	-10-
~τ~	0	0	0	0
ɑ	Horizon T			count
	short	medium	long	
o~0~	-1-	0	-1-	-2-
-0Γ^	-0-	0	-1-	-1-
-03-	-1-	0	-1-	-2-
-03-	-1-	1	-1-	-3-
-0Γ^	-5-	4	-2-	-∏-
~τ~	0	0	1	1
Table 3: Impact of the α parameter on
univariate (top) and multivariate (bot-
tom) problem settings for the Yformer
architecture. The horizons τ are 0-50,
51-300, and > 300 for short, medium,
and long horizons respectively.
Figure 2: Impact of the Yformer architecture and re-
construction loss on univariate (top) and multivariate
(bottom) for ETTh2 (left) and ETTm1 (right) datasets.
X-axis shows the forecast horizons (τ), Y-axis shows
the MAE. The colours blue, orange, green represents
the Informer, the Yformer without reconstruction loss,
the Yformer respectively.
advantage over in Informer baseline. We attribute this improvement in performance to the direct U-
Net inspired connections within the Yformer architecture. Using feature maps at multiple resolutions
offers a clear advantage by eliminating vanishing gradients and encouraging feature reuse.
The reconstruction loss seems to have a significant impact in reducing the loss for the ETTm1
multivariate case (right bottom graph Figure 2). And, in general, it helps to improve the results
for the Yformer architecture as the green curve in the Figure 2 is almost always below the blue
(Informer) and orange (Yformer with α = 0) curve. The significance of the loss depends on the
dataset but, in general, the auxiliary loss helps to improve the overall performance.
7.2	Reconstruction Factor
How impactful is the reconstruction factor α from the proposed loss in Eq. 6? We analyzed the opti-
mal value for α across different forecasting horizons and summarized them in Table 3. Interestingly,
α = 0.7 seems to be the predominant optimal setting, implying a high weight for the reconstruction
loss helps the Yformer to achieve a lower loss for the future targets. Additionally, we can observe a
trend that α is on average larger for short forecasting horizons signifying that the input reconstruc-
tion loss is also important for the short horizon forecast.
8	Conclusion
Time series forecasting is an important business and research problem that has a broad impact in
today’s world. This paper proposes a novel Y-shaped architecture, specifically designed for the far
horizon time series forecasting problem. The study shows the importance of direct connections
from the multi-resolution encoder to the decoder and reconstruction loss for the task of time series
forecasting. The Yformer couples the U-Net architecture from the image segmentation domain on a
sparse transformer model to achieve state of the art results in 39 out of the 40 tasks across various
settings as presented in the Tables 1 and 2.
9	Reproducibility Statement
All the experimental datasets used to evaluate the Yformer architecture are publicly available and,
we provide the source code for the proposed architecture with the supplementary materials for re-
producibility. The hyperparameters needed to reproduce the reported results on the different datasets
are presented in the appendix section E.
9
Under review as a conference paper at ICLR 2022
References
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,
abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.
Dor Bank, Noam Koenigstein, and Raja Giryes. Autoencoders. arXiv preprint arXiv:2003.05991,
2020.
G. E. P. Box and G. M. Jenkins. Some recent advances in forecasting and control. Journal of
the Royal Statistical Society. Series C (Applied Statistics),17(2):91-109,1968. ISSN00359254,
14679876. URL http://www.jstor.org/stable/2985674.
Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer science &
business media, 2013.
Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent
imputation for time series. arXiv preprint arXiv:1805.10572, 2018.
Salvatore Carta, Anselmo Ferreira, Alessandro Sebastian Podda, Diego Reforgiato Recupero,
and Antonio Sanna. Multi-dqn: An ensemble of deep q-learning agents for stock market
forecasting. Expert Systems with Applications, 164:113820, 2021. ISSN 0957-4174. doi:
https://doi.org/10.1016/j.eswa.2020.113820. URL https://www.sciencedirect.com/
science/article/pii/S0957417420306321.
Chenyou Fan, Yuze Zhang, Yi Pan, Xiaoyue Li, Chi Zhang, Rong Yuan, Di Wu, Wensheng Wang,
Jian Pei, and Heng Huang. Multi-horizon time series forecasting with temporal attention learning.
In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, KDD ’19, pp. 25272535, New York, NY, USA, 2019. Association for Computing
Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330662. URL https://doi.
org/10.1145/3292500.3330662.
Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai, and Heng Huang. Deep
clustering via joint convolutional autoencoder embedding and relative entropy minimization. In
Proceedings of the IEEE international conference on computer vision, pp. 5736-5745, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.
Daniel Jarrett and Mihaela van der Schaar. Target-embedding autoencoders for supervised rep-
resentation learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=BygXFkSYDH.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Nikita Kitaev, Eukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv
preprint arXiv:2001.04451, 2020.
Jacek Klimek, Jakub Klimek, Witold Kraskiewicz, and Mateusz Topolewski. Long-term series fore-
casting with query selector-efficient model of sparse attention. arXiv preprint arXiv:2107.08687,
2021.
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term
temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference
on Research & Development in Information Retrieval, pp. 95-104, 2018.
10
Under review as a conference paper at ICLR 2022
Lei Le, Andrew Patterson, and Martha White. Supervised autoencoders: Improving generalization
performance with unsupervised regularizers. Advances in neural information processing systems,
31:107-117, 2018.
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng
Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series
forecasting. Advances in Neural Information Processing Systems, 32:5243-5253, 2019.
TsUng-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
Feature pyramid networks for object detection. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 2117-2125, 2017.
Mathias Perslev, Michael Hejselbak Jensen, Sune Darkner, Poul J0rgen Jennum, and Christian IgeL
U-time: A fUlly convolUtional network for time series segmentation applied to sleep staging.
arXiv preprint arXiv:1910.11162, 2019.
Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison Cottrell. A
dual-stage attention-based recurrent neural network for time series prediction. arXiv preprint
arXiv:1704.02971, 2017.
Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang
Wang, and Tim Januschowski. Deep state space models for time series forecasting.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
5cf68969fb67aa6082363a6d4e6468e2- Paper.pdf.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
Alaa Sagheer and Mostafa Kotb. Time series forecasting of petroleum production using deep lstm
recurrent networks. Neurocomputing, 323:203-213, 2019.
David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic fore-
casting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181-
1191, 2020.
Daniel Stoller, Mi Tian, Sebastian Ewert, and Simon Dixon. Seq-u-net: A one-dimensional causal
u-net for efficient sequence modelling. arXiv preprint arXiv:1911.06393, 2019.
Basile Van Hoorick. Image outpainting and harmonization using generative adversarial networks.
arXiv preprint arXiv:1912.10960, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768, 2020.
Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep
neural networks: A strong baseline. In 2017 International joint conference on neural networks
(IJCNN), pp. 1578-1585. IEEE, 2017.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efficient transformer for long sequence time-series forecasting. arXiv preprint
arXiv:2012.07436, 2020.
Xiaowu Zou, Zidong Wang, Qi Li, and Weiguo Sheng. Integration of residual network and con-
volutional neural network along with various activation functions and global pooling for time
series classification. Neurocomputing, 367:39-45, 2019. ISSN 0925-2312. doi: https://doi.org/
10.1016/j.neucom.2019.08.023. URL https://www.sciencedirect.com/science/
article/pii/S0925231219311506.
11
Under review as a conference paper at ICLR 2022
A Appendix : Additional Figures
Figure 3: General time series setting illustrating the quadruples (x, y, x0, y0) denoting the past pre-
dictors, past targets, future predictors and future targets respectively. Given the history information
(x, y) and the future predictors (x0) (in green) until time t = T , time series forecasting predicts the
target y0 (in red) for the next τ time steps. In the figure, O and M represents the respective channels
of the targets and the predictors.
Figure 4: U-Net connections for effectively utilizing embeddings at multiple resolutions in the
Yformer. The Y-Past Encoder embeddings and the Y-Future Encoder embeddings are concatenated
within the Yformer encoder. A direct connection is allowed between the contracting encoder embed-
ding (ei) and the corresponding expanding decoder embedding (dI-i). (++ denotes concatenation)
B Appendix : Additional Related Works
Time-series Forecasting: Time Series forecasting has been a well-established research topic with
steadily growing applications in a real-world environment. Traditionally, ARIMA Box & Jenkins
(1968) and exponential smoothing methods Hyndman & Athanasopoulos (2018) have been used for
time series forecasting. However, limited scalability and the inability to model non-linear patterns
restrict their use for time series forecasting.
Reconstruction Loss The usage of a reconstruction loss originated in the domain of variational
autoencoders Kingma & Welling (2013) to reconstruct the inputs during the generative process.
Forecasting for a long horizon in a single forward pass can be considered as generating a distribution
given the past time steps as the input distribution. A reconstruction loss would hence be beneficial as
12
Under review as a conference paper at ICLR 2022
the forecast distribution is similar to the past input distribution. Moreover, a recent study by Le et al.
2018 has shown that the addition of the reconstruction term to any loss function generally provides
uniform stability and bounds on the generalization error, therefore leading to a more robust model
overall with no negative effect on the performance.
Our work tries to combine the different ideas of U-Net from image segmentation with a sparse atten-
tion network for modeling time series that also utilizes the additional guidance from reconstruction
loss.
C Appendix : Definition
C.1 Operators
ProbSparseAttn: Attention module that uses the ProbSParse method introduced in ZhoU et al.
(2020). The query matrix Q ∈ RLQ ×d denotes the sparse query matrix with U dominant queries.
APropSparse (Q, K, V)
Softmax(
(7)
MaskedAttn: Canonical self-attention with masking to prevent positions from attending to subse-
quent positions in the future (Vaswani et al., 2017).
Conv1d: Given N batches of 1D array of length L and C number of channels/dimensions. A
convolution operation produces an output:
Cin-1
out(Ni, Coutj) = bias(Coutj) +	weight(Coutj, k) ?input(Ni,k)	(8)
k=0
For further reference please visit pytorch Conv1D page
LayerNorm: Layer Normalization introduced in Ba et al. (2016), normalizes the inputs across
channels/dimensions. LayerNorm is the default normalization in common transformer architectures
(Vaswani et al., 2017). Here, γ and β are learnable affine transformations.
ou^ *)=inpPVXutENnputT] * γ+β
(9)
MaxPool: Given N batches of 1D array of length L, and C number of channels/dimensions. A
MaxPool operation produces an output.
out(Ni, Cj, k) = max	input(Ni, Cj, stride × k + m)
m=0,…,kernel_size — 1
(10)
For further reference please visit pytorch MaxPool1D page
ELU: Given an input x, the ELU applies element-wise non linear activation function as shown.
ELU(x)
x,
α * (exp(x) - 1),
if x > 0
if x ≤ 0
(11)
ConvTranspose1d: Also known as deconvolution or fractionally strided convolution, uses convolu-
tion on padded input to produce upsampled outputs (see pytorch ConvTranspose1d page).
C.2 Contracting Prob S pars e Self-Attention Blocks
The Informer model uses Contracting ProbSparse Self-Attention Blocks to distil out redundant in-
formation from the long history input sequence (x, y) in a pyramid structure similar to Lin et al.
13
Under review as a conference paper at ICLR 2022
(2017). The sequence of operations within a block begins with a ProbSparse self-attention that
takes as input the hidden representation hi from the ith block and projects the hidden representa-
tion into query, key and value for self-attention. This is followed by multiple layers of convolution
(Conv1d), and finally a MaxPool operation is performed to reduce the latent dimension at each
block, after applying non-linearity with an ELU() activation function . The LayerNorm operation,
regularizes the model using Layer Normalization (Ba et al., 2016). Algorithm 2 shows the multiple
operations performed within a Contracting ProbSparse Self-Attention Block that takes an input hi
and produces the hidden representation hi+1 for the i + 1th block.
Algorithm 2 Contracting ProbSparse Self-Attention Block
Input : hi
Output : hi+1
hi+ι — ProbSParseAttn(hi, hi)
hi+1 - ConvId(hi+ι)
hi+1 - ConvId(hi+ι)
hi+1 - LayerNorm(hi+ι)
hi+1 - MaxPool(ELU(Conv1d(hi+ι)))
C.3 Contracting Masked Self-Attention Blocks
The Y-Future Encoder, uses multiple blocks of Contracting Masked Self-Attention Blocks that re-
places the ProbSparseAttn in the Contracting ProbSparse Self-Attention Blocks with a masked at-
tention. Masking attention for the Y-Future Encoder, avoids any future information leak architec-
turally. In our experiments, the addition of restricted attention like the ProbSparse on an already
masked attention resulted in performance loss. This could be the result of missing query-key in-
teraction brought about by the ProbSparse on an already masked attention. Algorithm 3 shows the
multiple operations performed within a Contracting Masked Self-Attention Block that takes an input
hi and produces the hidden representation hi+1 for the i + 1th block.
Algorithm 3 Contracting Masked Self-Attention Block
Input : hi
Output : hi+1
hi+1 - MaskedAttn(hi, h)
hi+1 - ConvId(hi+ι)
hi+1 - ConvId(hi+。
hi+1 - LayerNorm(hi+1)
hi+1 - MaxPool(ELU(Conv1d(hi+Q))
14
Under review as a conference paper at ICLR 2022
D Appendix : MSE Results
Table 4: MSE for the univariate time series forecasting task with the Yformer architecture. The
best result is highlighted in bold and the second best in italic and red. Informer* here is representing
a modified version of the standard informer which is using the canonical attention module.
Dataset	Horizon (T)	Methods					Improvement %
		LogTrans	LSTnet	Informer*	Informer	Yformer	
ETTHI	24	-0.103-	0.107	-0092-	-0.098-	0.082-	10:87
	48	-0.167-	0.162	-0ΓT6I-	-0.158	0.139-	12.03
	168	-0.207-	0.239	-0.187-	-0183-	0.111-	39:34
	336	-0.230-	0.445	-0215-	-0.222-	0.195-	930
	720	-0.273-	0.658	-0257-	-0.269-	0.226-	12:06
ETTH2	24	-0.102-	0.098	-0.099-	-0093-	0.082-	1183
	48	-0.169-	0.163	-0159-	0.155 -	0.172-	-10.97
	168	-0.246-	0.255	-0.235-	-0232-	0.174-	25.00
	336	-0.267-	0.604	-0258-	-0.263-	0.224-	13:18
	720	-0.303-	0.429	-0.285-	-0277	0.211-	23.83
ETTmI	24	-0.065-	0.091	-0.034-	-0030	0.024-	20.00
	48	-0.078-	0.219	-0066-	-0.069-	0.048-	27.27
	96	-0.199-	0.364	-0187-	-0.194-	0.143-	23.53
	288	-0.411-	0.948	-0409-	-0401 -	0.150-	62.59
	672	-0.598-	2.437	-0.519-	-0512-	0.305-	40.43
ECL	48	-0.280-	0.204	-0.238-	-0.239-	0.194-	4.90
	168	-0.454-	0.315	-0442-	-0.447-	0.260-	17.46
	336	-0.514-	0.414	-0.501-	-0.489-	0.269-	35.02
	720	-0.558-	0.563	-0.543-	-0.540	0.427-	20.93
	960	-0.624-	0.657	-0.594-	0.582-	-0.595	-2.23
# wins per method	0	0	0	0	2	18	
						Avg	19.82
Table 5: MSE for the multivariate time series forecasting task with the Yformer architecture. The
best result is highlighted in bold and the second best in italic and red. Informer* here is representing
a modified version of the standard informer which is using the canonical attention module.
Dataset	Horizon (T)	Methods					Improvement %
		LogTrans	LSTnet	Informer*	Informer	Yformer	
ETTH1	24	-0.686-	1.293	-0.620-	-0577	0.485-	15.94
	48	-0.766-	1.456	-0.692-	-0.685-	0.530-	22.63
	168	-1.002-	1.997	-0.947-	-0931-	0.866-	6.98
	336	-1.362-	2.655	-1094-	-17128-	1.041-	4.84
	720	-1.397-	2.143	-1.241-	-1.215	1.098-	9.63
ETTH2	24	-0.828-	2.742	-0.753-	-072-	0.412-	42.78
	48	-1.806-	3.567	-1.461-	-1.457	1.171-	19.63
	168	-4.070-	3.242	-3.485-	-3.489-	2.171-	33.04
	336	-3.875-	2.544	-2.626-	-2.723-	2.260-	1Γ716
	720	~39Γ3-	4.625	-3.548-	-3.467	2.595-	25715
ETTm1	24	-0.4T9-	1.968	-0306-	-0.323-	0.289-	556
	48	-0.507-	1.999	-0.465-	-0.494-	0.486-	-452
	96	-0.768-	2.762	-0.681-	-0.678	0.569-	16.08
	288	-1.462-	1.257	-17Γ62-	-1.056	0.649-	38.54
	672	-1.669-	1.917	-1.231-	-1192-	0.772-	35.23
ECL	48	-0.355-	0.369	-0334-	-0.344-	0.306-	8.38
	168	-0.368-	0.394	-0353-	-0.368-	0.317 -	10.20
	336	-0.373-	0.419	-0.381-	-0381-	0.323-	15.22
	720	-0.409-	0.556	-0391-	-0.406-	0.312-	20.20
	960	-0.477-	0.605	-0.492-	-0.460	0.315 -	31.52
# wins per method	0	0	0	1	0	19	
						Avg	18.41
15
Under review as a conference paper at ICLR 2022
E Appendix : hyperparameters
E.1 Hyperparameter Search Space
For a fair comparison, we retain the design choices from the Informer baseline like the history input
length (T) for a particular forecast length (τ), so that any performance improvement can exclusively
be attributed to the architecture of the Yformer model and not to an increased history input length.
We performed a grid search for learning rates of {0.001, 0.0001}, α-values of {0, 0.3, 0.5, 0.7, 1},
number of encoder and decoder blocks I = {2, 3, 4} while keeping all the other hyperparameters
the same as the Informer. Furthermore, Adam optimizer was used for all experiments, and we
employed an early stopping criterion with a patience of three epochs. To counteract overfitting,
we tried dropout with varying ratios but interestingly found the effect to be minimal in the results.
Therefore, we adopt weight-decay for our experiments with factors {0, 0.02, 0.05} for additional
regularization. We select the optimal hyperparameters based on the lowest validation loss and will
publish the code upon acceptance.
E.2 Optimal Hyperparameters
Table 6: Optimal hyperparameters across different horizon and datasets for the univariate setting.
All the remaining hyperparameters are retained from the Informer Model.
Dataset	Horizon T	History Length	Weight Decay	Learning Rate	Reconstruction Factor ɑ	Batch Size	Encoder Blocks
ETThI	24	720	0	0.0001-	0.7	32	2
	48	720	0	0.0001-	0.7	16	4
	168	720	0	0.001	0.7	32	4
	336	720	0.05	0.0001-	0∏	32	4
	720	720	0.05	0.0001-	0.7	16	2
ETTh2	24	48	0	0.0001-	0.7	32	2
	48	96	0.02	0.0001-	0.3	32	4
	168	336	0.02	0.001	0.3	32	2
	336	336	0.09	0.0001-	0	32	2
	720	336	0.09	0.0001-	0.7	16	2
ETTmI	24	96	0.02	0.0001-	0.7	32	4
	48	96	0.02	0.0001-	0.7	32	4
	96	384	0.02	0.0001-	071	32	4
	288	384	0.02	0.001	0.7	16	2
	672	384	007	0.001	0.3	16	2
ECL	48	168	0	0.0001-	0.7	16	2
	168	168	0.01	0.0001-	0.3	16	2
	336	168	0.01	0.0001-	0.7	16	2
	720	168	0	0.0001-	071	16	2
	960	48	0	-	0.0001~~	0.5	16	4
Table 7: Optimal hyperparameters across different horizon and datasets for the multivariate setting.
All the remaining hyperparameters are retained from the Informer Model.
Dataset	Horizon T	History Length	Weight Decay	Learning Rate	Reconstruction Factor ɑ	Batch Size	Encoder Blocks
ETTh1	24	48	0	0.0001-	0.7	32	3
	48	96	0.02	0.001	0.5	32	2
	168	168	0.02	0.001	0.7	32	2
	336	168	0	0.0001-	0.7	32	4
	720	336	0.05	0.0001-	1	16	2
ETTh2	24	48	0	0.0001-	0.7	32	2
	48	96	0.02	0.001	0	32	4
	168	336	0.09	0.001	0.7	32	2
	336	336	0.07	0.001	0.3	32	2
	720	336	0	0.0001-	0	16	2
ETTm1	24	672	0	0.0001-	0.7	32	2
	48	96	0	0.0001-	0.7	32	4
	96	384	0.05	0.0001-	0.7	32	4
	288	672	0.02	0.001	0.5	16	2
	672	672	0.02	0.0001-	0.3	16	2
ECL	48	24	0	0.0001-	0.7	16	3
	168	48	0	0.0001-	0.7	16	3
	336	24	0	0.0001-	0.5	16	2
	720	48	0	0.0001-	0.7	16	2
	960	336	0	-	0.0001~~	0.7	16	2
16