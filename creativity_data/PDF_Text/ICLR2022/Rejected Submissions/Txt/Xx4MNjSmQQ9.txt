Under review as a conference paper at ICLR 2022
Robust Generalization of Quadratic Neural
Networks via Function Identification
Anonymous authors
Paper under double-blind review
Ab stract
A key challenge facing deep learning is that neural networks are often not robust
to shifts in the underlying data distribution. We study this problem from the
perspective of the statistical concept of parameter identification. Generalization
bounds from learning theory often assume that the test distribution is close to the
training distribution. In contrast, if we can identify the “true” parameters, then
the model generalizes to arbitrary distribution shifts. However, neural networks
are typically over-parameterized, making parameter identification impossible. We
show that for quadratic neural networks, we can identify the function represented
by the model even though we cannot identify its parameters. Thus, we can obtain
robust generalization bounds even in the over-parameterized setting. We leverage
this result to obtain new bounds for contextual bandits and transfer learning with
quadratic neural networks. Overall, our results suggest that we can improve
robustness of neural networks by designing models that can represent the true
data generating process. In practice, the true data generating process is often very
complex; thus, we study how our framework might connect to neural module
networks, which are designed to break down complex tasks into compositions
of simpler ones. We prove robust generalization bounds when individual neural
modules are identifiable.
1	Introduction
Recent work has demonstrated that neural networks are not robust to shifts in the underlying data,
including both distribution shifts (i.e., where the data comes from anew distribution independent of the
neural network parameters) (Hendrycks & Dietterich, 2019; Taori et al., 2020) as well as adversarial
shifts (i.e., where the shift can depend on the neural network parameters) (Szegedy et al., 2013).
Accordingly, there has been a great deal of interest in better understanding why neural networks fail
to be robust (Tsipras et al., 2018; Ilyas et al., 2019) and on improving robustness (Goodfellow et al.,
2014; Raghunathan et al., 2018; Cohen et al., 2019).
From the perspective of learning theory, there is little reason to expect neural networks to be robust,
since generalization bounds typically assume that the test examples are from the same distribution as
the training examples. PAC-Bayesian generalization bounds allow for a limited amount of robustness,
but only if the support of the target distribution q is contained in that of the source distribution p,
since it requires that the KL divergence DKL(q k p) is finite. Yet, distribution shifts (Hendrycks &
Dietterich, 2019) often shift probability mass to inputs completely outside the source distribution.
Instead, the reason we might expect neural networks to be robust to these shifts is that humans are
robust to them; for instance, small pixel-level shifts considered in adversarial examples are typically
unnoticeable to humans, yet these shifts can move the image completely off of the distribution of
natural images. This fact indicates a gap in our theoretical understanding of neural networks. In
particular, the key question is understanding settings under which we may expect neural networks to
be robust to distribution shifts that are “large” (e.g., in terms of KL divergence).
We study a strategy for closing this gap based on the statistical concept of identifiability (Hsu et al.,
2012). At a high level, this concept assumes that the true model belongs to the model family; then, in
the limit of infinite training data, the learning algorithm can exactly recover the parameters of the true
model. For instance, in linear regression, the data is generated according to the model y = (θ* ,x)+ ξ,
where ξ is σ-subgaussian noise. Then, under mild assumptions on the training data Z = (X, Y ), the
1
Under review as a conference paper at ICLR 2022
ι ∙	1	.	/ /、ɪ γ-a∖ J . A / rz∖	,1 ,	,	∙	♦ ,1 —	c ♦,
ordinary least squares (OLS) estimator θ(Z) recovers the true parameter—i.e., in the limit of infinite
F , ∕λ / Γ7∖	/1* -«1 τ∙ ,1 C ♦,	1	zʌɪ C	1 ∙ 1	KK ∙ 1 ∙ ,	,	C , 1 C
data, θ(Z) = θ . With finite samples,OLS satisfies high-probability convergence rates of the form
..^ .. ...
kθ(Z)-θ*∣∣2 ≤ e.	(1)
The connection to robustness is that if (1) holds, then for any input x such that kxk2 ≤ xmax, we have
^
^
... . . . .. ... . ... ..
∣hθ(Z),xi-hθ*,xi∣ ≤ kθ(Z) - θ*k2 ∙kx∣∣2 ≤ e∙x∣
(2)
max.
CrVI	C	1 ∙ , ∙1	/ ∖ ∙ .Λ	.	T-l /zʌ	∖	C _ T? I II II /	1 A / Γ7∖ 1 ,
Thus, for any distribution q(x) with support on B2(0, xmax) = {x ∈ X | kxk2 ≤ xmax}, θ(Z) obtains
bounded error-i.e., We have Eq(X) [(hθ(Z), Xi - (θ*, x))2] ≤ e2xmιax with high probability.
Thus, a natural question is whether we can obtain similar kinds of parameter identification bounds for
neural networks. A key complication is that practical neural networks are often over-parameterized,
possibly to facilitate optimization (Du & Lee, 2018; Jacot et al., 2018). In this setting, identification
is impossible, since multiple parameters can yield the same model. Nevertheless, it may be possible
to obtain bounds of the form (2)—in particular, even if we do not recover the true parameters θ*, we
can still recover the function fθ* (x). We refer to this notion as function identification. Furthermore,
we show that quadratic neural networks satisfy function identification bounds under mild conditions.
To demonstrate its utility, we show how function identification can be leveraged to obtain regret
guarantees for a bandit (Rusmevichientong & Tsitsiklis, 2010) where each arm is a quadratic neural
network. Linear bandits fundamentally involve covariate shift since their “covariates” are arms, which
are adaptively chosen through the learning process as a function of past observations; thus, existing
approaches have all operated in the setting where there is a unique and identifiable global minimizer.
Similarly, we build on recent work proving bounds on transfer learning in the setting of bounded
label shift and unbounded covariate shift (Bastani, 2020; Xu et al., 2021); again, we show that we can
leverage function identification to easily transfer learn quadratic neural networks.
Our results suggest that one strategy for improving robustness of neural networks is to design models
that can represent the true data generating process. However, doing so can be challenging due to the
complexity of most real-world data generating processes mapping covariates to labels—e.g., mapping
natural language to semantic meaning or images to object detections and labels. As a consequence,
we study how our results can connect to neural module networks (Andreas et al., 2016), which are
designed to break down complex tasks into smaller ones that can each be solved by a neural network.
For instance, we may break down the task “count the number of red balls in image x” into (i) detecting
balls, (ii) detecting red objects, and (iii) intersecting the two sets, and (iv) summing the results.
Intuitively, neural modules can generalize more robustly since (i) it is more likely that an individual
neural module designed to solve a simple task can be identified from training data, and (ii) even if
module composition is not itself identifiable, shifts in the compositional structure tend to be smaller
than shifts in the underlying data distribution. We study a simplified form of neural module networks,
where modules are quadratic neural networks composed in sequence according to a given input; for
simplicity, we assume they can be trained in a supervised way, ensuring robust generalization. Then,
we show that under certain conditions, compositions of these models are also robust, including the
case where there are shifts in the distribution over compositions.
Related work. Prior work has connected misspecification (i.e., the true model is in the model family)
and robustness to covariate shift (Shimodaira, 2000; Wen et al., 2014); however, having a correctly
specified model is insufficient if the true parameters are not identifiable—e.g., in linear regression, if
the covariance matrix Σ = Ep(x) [xx>] is singular, then θ is not identifiable; thus, the estimated model
may not be robust. Quadratic neural networks cannot be identified even if the model is correctly
specified since the parameters have a continuous symmetry (i.e., orthogonal transformations).
Recent work has studied learning under adversarial examples (Goodfellow et al., 2014; Raghunathan
et al., 2018; Cohen et al., 2019) and corrupted training data (Steinhardt et al., 2017; Diakonikolas
et al., 2019). In contrast, we are interested in robustness to covariate shift; there has been recent work
empirically showing that neural networks are sensitive to distribution shift (Hendrycks & Dietterich,
2019; Taori et al., 2020; Ruis et al., 2020; Ribeiro et al., 2020; Koh et al., 2020). Distributionally
robust optimization enables training of models robust to small shifts (Duchi & Namkoong, 2018), but
we are interested in potentially large shifts. Unsupervised domain adaptation (Ben-David et al., 2007;
Blitzer et al., 2008) learns a model on a covariate shifted target distribution; however, they rely on
unlabeled examples from the target domain, whereas we do not. There has been recent theory on
2
Under review as a conference paper at ICLR 2022
robustness to adversarial perturbations—e.g., showing there may be a tradeoff between robustness
and on-distribution generalization (Tsipras et al., 2018), and that non-robust algorithms tend to learn
predictive but brittle representations compared to adversarially robust ones (Ilyas et al., 2019). In
contrast, we show that these tradeoffs are mitigated when the true model function can be identified
despite over-parameterization. Furthermore, adversarial shifts are typically bounded (e.g., small '∞
norm), whereas the shifts we consider may be large.
There has been a great deal of recent work on deep learning theory, including on quadratic neural
networks; however, it has largely focused on optimization (Ge et al., 2017b; Jacot et al., 2018;
Du et al., 2019; Gao et al., 2019; Soltanolkotabi et al., 2018; Li et al., 2018), and on-distribution
generalization (Neyshabur et al., 2017; Du & Lee, 2018; Jacot et al., 2018; Arora et al., 2018; Long
& Sedghi, 2019). In contrast, we are interested in out-of-distribution generalization.
We discuss additional related work on matrix factorization and multi-armed bandits in Appendix A,
as well as a discussion of the novelty of our results.
2	Problem Formulation
We consider a model fθ : X → Y, with covariates X ⊆ Rd , labels Y ⊆ R, and parameters
θ ∈ Θ ⊆ Rm. A generalization bound from learning theory typically has the form
Pp(Z)[Lp(θ(Z)) ≤ e] ≥ 1 - δ where	Lp(θ) = Ep.)[(fθ(X)- fθ*(x))2],	(3)
where e,δ ∈ R>o, Z = {(xι, yι), ∙∙∙, (xn, yn)} ⊆X ×Y with y = fθ*(xi) + ξi isa training set of
i.i.d. observations from a distribution P (i.e., P(Z) = p(χι,yι) ∙... ∙p(xn, yn)), ξi is bounded random
noise independent of Xi with ∣ξi | ≤ ξmax, θ* ∈ Θ are the true parameters, and
1n
θ(Z) = arg min L(θ; Z)	where	L(θ; Z)=n J^(fθ (Xi)- yi)2
is an estimator based on the training data Z.1 In particular, they assume that the training inputs
Xi 〜p are i.i.d. samples from the same distribution as the test example X 〜p.
Definition 2.1. The model fθ and distribution P satisfy function identification if for any e, δ ∈ R>0,
we have Pp(Z) [∀x ∈ X . (f§(z)(x) - fθ* (x))2 ≤ e] ≥ 1 - δ for n = |Z| sufficiently large.
Function identification implies generalization bounds even when the test data comes from a different
distribution q. In particular, we say fθ robustly generalizes if for any q with support on X, we have
Pp(z)[Lq(θ(Z)) ≤ e] ≥ 1 - δ,	(4)
where the difference from (3) has been highlighted in red. It is easy to see that function identification
implies (4). Note that the true model fθ* does not change, so there is no label shift.
3	Function Identification of Quadratic Neural Networks
Traditional statistical bounds on parameter identification can provide guarantees for arbitrary covariate
shift. In particular, suppose we have a bound of the form
Pp(Z) [kθ(Z)-θ*k2 ≤ e] ≥ 1 - δ,	(5)
and assume that the model family fθ is K-Lipschitz continuous in θ; then, we have
Lq(θ(Z)) ≤ K2 ∙kθ(Z)-θ*k2 ≤ K2e2	(6)
with probability at least 1 - δ according to P(Z). In particular, this bound holds for any covariate
distribution q. Our goal is to extend these techniques to quadratic neural networks, which are
over-parameterized so we cannot identify the true parameters θ*—i.e., (5) does not hold.
1Note that in (3), the loss Lp omits the label errors ξ; including it would result in an additive constant to Lp .
This choice ensures that the optimal parameters have zero loss—i.e., Lq (θ*) = 0 for any q.
3
Under review as a conference paper at ICLR 2022
3.1	Quadratic Neural Networks
We consider a neural network fθ, where θ ∈ Rd×k, with a single hidden layer with k neurons—i.e.,
fθ(x) = pk=ι aj ∙ σ(hθj,xi). We consider the over-parameterization case where k can be much
larger than d. Following prior work (Du & Lee, 2018), we assume that fθ has quadratic activations
and output weights equal to one—i.e., σ(z) = z2 and aj = 1 for each j ∈ [k], so we have
k
fθ(x) = Xhθj,xi2.
We assume the true (training) loss is the mean-squared error Lp(θ) = Ep(X) [(fθ(x) 一 fθ* (χ))2], and
we consider a model trained using an empirical estimate of this loss on the training dataset:
1n
θ(Z) = arg min L(θ; Z)	where	L(θ; Z) = — ɪ2(fθ (xi) - Iyi)2.
θ∈Θ	ni=1
Now, our goal is to obtain a bound of the form (6); to this end, we assume the following:
Assumption 3.1. kxk2 ≤ xmax and kθkF ≤ θmax.
Assumption 3.2. There exists α ∈ R>0 such that Ep(x) [(x>∆x)2] ≥ αk∆k2F for any symmetric
∆ ∈ Rd×d.
Our second assumption is standard; in particular, it is closely related to the assumption in linear
regression that the minimum eigenvalue of the covariance matrix is lower bounded—i.e., Σ =
Ep(x) [xx>]	0. As an example, when x is i.i.d. uniform in each component, e.g., p(x) =
Qid=1 Uniform(xi; [一1/2, 1/2]), then we can take α = 1/180; we give a proof in Appendix B.1.
3.2 Robust Generalization
Our approach leverages the fact that fθ(x) = x> (θθ>)x; thus, fθ resembles a matrix factorization
model. Recent work has leveraged this connection to translate matrix factorization theory to quadratic
neural networks (Du & Lee, 2018). We let g(θ) = θθ> and fφ(χ) = χ>φχ, where φ ∈ Φ ⊆ Rd×d, in
which case fθ(x) = fg(θ)(χ); in addition, we define Lp(φ) = Ep(X) [(fφ(x) 一 fφ* (x))2], where φ =
ʌ ʌ
~	.	-1	_ 1∙V.	,	~	,	.C	~	,	.	.	ʌ	~	,	,	.
g(θ*), and L(φ; Z) = n-1 Pi=ι(fφ(xi) - Vi) , so Lp(θ) = Lp(g(θ)) and L(θ; Z) = L(g(θ); Z).
Finally, we assume that kφkF ≤ φmax; in general, we have φmax ≤ θm2 ax by Assumption 3.1.
We begin by stating several lemmas establishing the properties needed for function identification.
Our first lemma says that the loss is strongly convex in φ.
Lemma 3.3. Under Assumption 3.2, the loss Lp(φ) is 2α-strongly convex in φ.
We give a proof in Appendix B.2. Our next lemma says that our model family is Lipschitz in φ.
Lemma 3.4. Under Assumptions 3.1 & 3.2, fφ and L are K -Lipschitz in φ, where K = 4φmaxx4max.
We give a proof in Appendix B.3. Our final lemma says that our estimate of the loss function is a
uniformly good approximation of the true loss.
Lemma 3.5. Under Assumptions 3.1 & 3.2, for any δ ∈ R>0, we have
Pp(Z) SUP lLp(θ) - L(θ; Z)- σ(Z)I ≤ E ≥ 1 一 δ,
θ∈Θ
where σ(Z) = n-1 En=I ξ2，and letting 'max = 2/"3φmαx be an upper bound on ∣fθ(x) — fθ* (x)∣,
(7)
We give a proof in Appendix B.4. Note that E → 0 as n → ∞. Next, we prove our main result, which
says that quadratic neural networks can be functionally identified.
4
Under review as a conference paper at ICLR 2022
Theorem 3.6. Under Assumptions 3.1 & 3.2, we have
2K2
pp(z) ∀x ∈ X. (fθ(Z)(X) - fθ*(X)) ≤ -α- ≥ 1 - δ∙
Proof. By Lemma 3.3, and since VφL(g(θ*)) = 0, We have
Lp(θ(Z)) - Lp(θ*) = Lp(g(θ(Z)))- Lp(g(θ*)) ≥ α∣∣g(θ(Z)) - g(θ*)∣∣F.	(8)
TL T . Λ T	Ck	1.<	∕'	.A ∙∙	T /C Γ7∖	1
Next, by Lemma 3.5 and the fact that θ minimizes L(θ; Z), We have
ʌ , ʌ ʌ ʌ , ..
Lp(θ(Z))	≤ L(θ; Z)	+ E — σ(Z)	≤ L(θ*;	Z)	+ E — σ(Z)	≤ Lp(θ*)	+ 2e	(9)
With probability at least 1 - δ. Combining (8) and (9), We have
kg(θ(Z)) - g(θ*)kF ≤∖/B
α
With probability at least 1 - δ. Finally, by Lemma 3.4, We have
(f^(z)(x) - fθ*(x))2 = (fg(θ(Z))(X)- fg(θ*)(x))2 ≤ K2kg(θ(Z)) - g(θ*)k2 ≤ 与(∀x ∈ X)
with probability at least 1 一 δ, as claimed.	□
As a result, We provide a robust generalization error bound for quadratic neural netWorks With
potential distribution shifts.
Corollary 3.7. Under Assumptions 3.1 & 3.2, for any distribution q(X) with support on B2(0, Xmax),
2K2E
Pp(Z) Lq(θ(Z)) ≤ —α一 ≥1 - δ∙
Finally, we also prove that gradient descent can find the global minima of L(θ; Z), which ensures that
gradient descent can perform function identification in practice; we give a proof in Appendix B.5.
Proposition 3.8. All local minima of L(θ; Z) are also global minima.
4 Quadratic Neural Bandits
A key application of robust generalization bounds is to parametric bandits; this is because, in bandit
learning, the distribution of inputs X used to estimate θ ≈ θ can differ from the distribution under
which f@ is used. Thus, generalization bounds based on notions such as Rademacher complexity
cannot be used. Unlike prior literature in bandits, we consider an over-parameterized function that
does not admit a unique solution; in contrast, recent work on neural tangent kernel bandits (Zhou
et al., 2020) assumes that there is a unique, identifiable solution. Note that this assumption cannot
hold for quadratic neural networks because they are invariant to transformations such as rotations.
We consider a standard linear bandit (Rusmevichientong & Tsitsiklis, 2010; Abbasi-Yadkori et al.,
2011) with a fixed horizon T ∈ N, but where the expected reward is parameterized by a quadratic
neural network instead of a linear function. At each time step t, the algorithm chooses among a
continuum of actions Xt ∈ X, and receives a reward
k
yt = fθ* (Xt) + ξt = Xhθ;, Xt i2 + ξt,	(IO)
j=1
where θ* ∈ Rd×k is an unknown parameter matrix, and ξt are bounded i.i.d. random variables. For
simplicity, we assume that X = B2(0, 1) is the unit ball. Then, our goal is to bound the regret
T
R(T) = X(Ep(ξt)[yt] - y*)	where y* = maχ fθ* (x).
x∈X
t=1
We make the following assumption, which says that φ* = θ* θ*> has a gap in its top eigenvalue:
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Explore-Then-Commit Algorithm for Quadratic Neural Network Bandit
procedure QUADRATICNEURALBANDIT
Initialize Z J 0
Let m be as in (11)
for t ∈ {1, ..., m} do
Sample i.i.d. action Xt 〜p, where P is as in (12)
Take action xt and obtain reward yt as in (10)
Update Z J Z ∪ {(xt, yt)}
end for
Compute θ = argmin& L(θ; Z), where L(θ; Z) = m-1 Pm=ι(fθ(Xi) 一 yi)2
Compute X = arg maXχ∈χ f^(x)
for t ∈ {m + 1, ..., T } do
Take action Xt = X and obtain reward yt as in (10)
end for
end procedure
Assumption 4.1. Let φ* = θ*θ*>, and let λι ≥ λ2 ≥ ... ≥ λd be the eigenvalues of φ*. There
exists a constant M ∈ R>0 such that λ1 一 λ2 ≥ 4/M.
This assumption ensures that the eigenvectors of φ* to be stable under perturbations. The eigenvectors
of φ* correspond to the optimal action x* = arg maXχ∈χ fθ* (x) since fθ* (x) = x>Φ*x; thus, this
assumption ensures that if θ ≈ θ*, then the optimal action X = arg maXχ∈χ f@(X)satisfies X ≈ x*.
Next, we describe our algorithm, summarized in Algorithm 1. We consider an explore-then-commit
strategy for simplicity, since it already achieves the asymptotically optimal regret rate (Rusmevichien-
tong & Tsitsiklis, 2010); our approach can similarly be applied to more sophisticated algorithms
such as UCB (Abbasi-Yadkori et al., 2011) and Thompson sampling (Agrawal & Goyal, 2013).
Our algorithm proceeds in two stages: (i) the exploration stage (for t ∈ {1, ..., m}), and (ii) the
exploitation stage (for t ∈ {m + 1, ..., T}), where
m
135M ('max + ξmax)2d3T Vzlog(3 + 8。皿醒长7/编空)
φmax
(11)
In the exploration stage, We randomly choose actions Xt 〜p, where
d
p(X) =	Uniform Xi ;
i=1
(12)
Note that kXk2 ≤ 1 for X in the support ofp, so Xt ∈ X. Following the discussion in Section 3, for
this choice ofp, Assumption 3.2 holds for the dataset Z with α = 4/(45d2).
Next, we compute an estimate θ of θ* based on the data Z collected so far, and compute the optimal
action X assuming θ are the true parameters. Then, in the exploitation stage, We always use action x.
The key challenge providing theoretical guarantees using traditional generalization bounds is handling
the optimization problem over X ∈ X used to compute X. Since X is not sampled from the distribution
p, traditional bounds do not provide any guarantees about the accuracy of fj(X) compared to fθ* (X).
In contrast, Theorem 3.6 provides a uniform guarantee, and can therefore be used to bound the regret.
Theorem 4.2. Under Assumptions 3.1, 3.2 & 4.1, the expected regret of Algorithm 1 is
R(T) ≤ Co + Ci ∙ T2/3 (log(3 + 8φ*KT ''I",
where C0 and C1 do not depend on T (see Appendix C).
In particular, we have R(T) = O(T2/3); we give a proof in Appendix C. Note that this is worse than
the usual O(√T) regret because Theorem 3.6 only admits a n1/4 convergence rate.
6
Under review as a conference paper at ICLR 2022
5 Transfer Learning of Quadratic Neural Networks
So far, we have considered shifts in the covariate distribution but not in the label distribution. Now,
we consider a transfer learning problem where there is additionally a small shift in the labels. In
particular, we assume we have proxy data Zp ⊆ X × Y from the source domain of the form
yp,i = fθp (xp,i) + ξp,i (for i ∈ [np]), where θp ∈ Θ are the proxy parameters and p(xp) is the
source covariate distribution, along with gold data Zg ⊆ X × Y from the target domain of the form
yg,i = fθ* (xg,i) + ξg,i (for i ∈ [ng]), where θg ∈ Θ are the gold parameters and q(xg) is the target
covariate distribution. We are interested in the setting n》n§, and where ∣∣θg 一 θp∣∣F ≤ B is small.
We consider a two-stage estimator (Bastani, 2020) that first computes an estimate of the proxy
parameters θp = arg minθ∈Θ L(θ; Zp), and then computes an estimate of the gold parameters in a
way that is constrained towards the proxy parameters. First, note that we have
Pp(Z) ∖lqq(θp) ≤ 2K2^p] ≥ 1 — δ,
where
ep = S 18..7一 + ξmaɔ (d2 max {1, log (1 + 刖*%)} + log g),
where we have highlighted the differences from in (7) in red. Next, we make a technical assumption:
Assumption 5.1. For some σ° ∈ R>o, σm⅛(θp) ≥ σ0, where σm⅛(θ) is the dth singular value of θ.
Equivalently, the minimum eigenvalue of g(θp) is positive; intuitively, this assumption ensures a good
estimate of θpθp> implies a good estimate of θp (UP to an orthogonal transformation). Then, letting
B = B + ^ j2ep
σ0	α
be an expanded radius to account for error in our estimate of θp , we use the estimator
θg = arg min L(θ; Zg) where B2(θp,B) = {θ ∈ Θ | ∣θ 一 ©PkF ≤ B}.
. .ʌ ʌ.
θ∈B2(θp,B)
Note that we have assume B is known; in practice, this constraint can be included as an additive
regularization term. Intuitively, this formulation mirrors transfer learning algorithms based on fine-
tuning—i.e., initializing the parameters to the proxy data θp and then taking a small number of steps
of stochastic gradient descent (SGD) on the gold data θg . In particular, SGD can be interpreted as L2
regularization on the parameters (Ali et al., 2020), so fine-tuning L2-regularizes θg towards θp.
Theorem 5.2. Under Assumptions 3.1, 3.2 & 5.1, for any q(x) with support on B2(0, xmax),
PP(Z) Lq (θg) ≤	aɪ	≥ 1 一 δ,
where
^	M2(K2B2 + ξmax)(4	∫1 1	8φmax"g、[4	4:
eg = B T-----ng------- d2m max 1 1，log (1 +	+ i +log δ}
Thus, if B is small and n is large, f« is accurate even if n is small; we give a proof in Appendix D.
6 Generalization Bounds for Neural Module Networks
While function identification enables robust generalization, many data generating processes are
too complex to be identifiable. Neural module networks are designed to break complex prediction
problems into smaller tasks that are individually easier to solve. These models take two kinds of input:
(i) a sequence of tokens w (e.g., word embeddings) indicating the correct composition of modules,
7
Under review as a conference paper at ICLR 2022
and (ii) the input x to the modules. Then, the model predicts the sequence of modules j1 ...jT based
on w, and runs the modules in sequence to obtain output x0 = fjT (...(fj1 (x))...).
We study conditions under which neural module networks can robustly generalize. Rather than study
arbitrary distribution shifts, we consider two separate shifts:
•	Module inputs: We assume that the individual modules are identifiable; as a consequence, we
assume the shift to the module input x can be arbitrary.
•	Module composition: We consider shifts to the token sequence w. If the model mapping w to
j1...jT is identifiable, then the entire model is identifiable. Instead, we show that when this model
is not identifiable, compositional structure can still aid generalization. Intuitively, we show that
while small shifts in the compositional structure can cause large shifts in the distribution p(w),
models that leverage the structure of p(w) can still generalize well.
In more detail, consider a simplified neural module network f, which includes (i) a set of neural
modules {fj : X → X}jk=1, and (ii) a parser g : ZT → [k]T, where Z ⊆ Rr, with model class
g ∈ G. We assume each component of fj (x) is computed by a separate quadratic neural network;
we discuss the architecture of g below. Then, given an input x ∈ X ⊆ Rd and w ∈ W = ZT, the
corresponding neural module network f : X × W → X is defined by
f(x, w) = (fjT ◦ ... ◦ fj1)(x) = fjT (...(fj1 (x))...)	where	j1...jT = g(w).
We assume that g has compositional structure■—i.e., for some g : [k] ×Z → [k], We have
g(w)= j1...jT	where	jt = {θ(zt,jt-ι) othe=w0ise,
Where w = z1 ...zT . Intuitively, w is a sequence of Word vectors; then, the current neural module
jt = g(zt,jt-ι) depends both on the current word vector Zt and the previous neural module jt-ι.
First, we assume that the individual modules have been functionally identified.
We assume we have fully labeled data we can use to train the neural modules—i.e., for each input x
and sequence w, we have both the desired sequence j1...jT of neural modules, as well as the entire
execution x0, x1, ..., xT, where x0 = x and xt+1 = fjt (xt) otherwise. Thus, we can use supervised
learning to train the neural modules;2 in particular, we can construct labeled examples (jt-1, zt,jt)
used to train the parser g, and labeled examples (xt, xt+i) to train the modules /九.For simplicity,
we assume we have a uniform lower bound n on the number of training examples for the parser and
for each module. Then, we have the following straightforward result:
Lemma 6.1. Under Assumptions 3.1 & 3.2, with probability at least 1 - dkδ, for each j ∈ [k],
kfj(x)- fj(x)k2 ≤ y -α- =: Cf	(∀x ∈X),
where fjis the estimated module and fj is the ground truth module.
This result follows straightforwardly from Theorem 3.6 along with a union bound. In contrast, we
do not assume the parsing model robustly generalizes, but only on distribution. For the subsequent
analysis, we can use any neural network models that satisfy the statement of Lemma 6.1.
Lemma 6.2. Under Assumptions 3.1 & 3.2, with probability at least 1 - δ, we have
Pp(Zj) [θ(z,j) = gj(z, j)i ≤ 4Rn(G) + j2bg(2/J =: eg,
where Rn (G) is the Rademacher complexity of G (including its loss function), where p(z, j)
T-1 PtT=1 pt(z, j), and where
1	Jl(j = 0) ∙p(z)
pt(z,j)= ∣Pko=ι R l(j = gj(z0 ,j0)) ∙ p(z | Z0) ∙ pt-1(z0,j 0)dz0
ift=1
otherwise.
2Neural modules are often trained with only partial supervision (Andreas et al., 2016); we leave an analysis
of this strategy to future work since our focus is on understanding generalization rather than learning dynamics.
8
Under review as a conference paper at ICLR 2022
This result is a standard Rademacher generalization bound (Bartlett & Mendelson, 2002). Note that
we have also assumed that the distribution over token sequences is structured, which is necessary for
our compositional implementation of g to generalize, even on distribution. Intuitively, the distribution
over (z, j) consists of both a unigram model over the word vectors:
T
p(zi, ..., ZT) = ɪɪp(zt | Zt-l),
t=1
where We define p(z` | z0) = p(zι), as well as a unigram model over neural modules:
T
p(j1 …jT | z1,…，zT) = Y Ijt = g*(zt,jt-1)).
t=1
Next, we consider a shifted distribution q(z | z0), which is close to p(z | z0).
Assumption 6.3. We have ∣∣q(∙ | z0) -p(∙ | z0)∣∣tv ≤ α.
Importantly, despite this assumption, the shift between the overall distributions p(z1, ..., zT) and
q(z1 , ..., zT) can still be large since it compounds exponentially across the steps t ∈ [T].
Proposition 6.4. There existP and q that satisfy Assumption 6.3, but ∣p — q∣ tv = 2(1 — (1 — a∕2)T).
That is, even if the single step probabilities p(z | z0) and q(z | z0) have total variation (TV) distance
bounded as in Assumption 6.3, the overall distributions p and q can have TV distance exponentially
close to the maximum possible distance of 2 in T; we give a proof in Appendix E.1.
We show that neural module networks generalize since g leverages the compositional structure of p.
First, we show that under Assumption 6.3, the overall shift in the input distribution ofgq is bounded:
Lemma 6.5. Under Assumptions 3.1, 3.2 & 6.3, we have ∣qq — pq∣TV ≤ Tα, where pq is defined in
Lemma 6.2 and qq is defined in Assumption 6.3.
That is, while the shift can compound across steps t, it does so only linearly; we give a proof in
Appendix E.2. Next, we show that as a consequence, the error of g is bounded.
Lemma 6.6. Under Assumptions 3.1, 3.2 & 6.3, and assuming that Pp(z,j)[g(z, j) = g"z,j)] ≤ Cg,
we have that Pp(W) [g(w) = g*(w)] ≤ Teg.
We give a proof in Appendix E.3. Finally, we have our main result.
Theorem 6.7. Under Assumptions 3.1, 3.2 & 6.3, with probability at least 1 — (dk + 1)δ, we have
Pq(w) [k∕(x, w) — f *(x,w)∣2 ≤ Tef ∙ max{KT-1,1}] ≥ 1 — Teg - T2α.
We give a proof in Appendix E.4. Intuitively, Theorem 6.7 says that the error of the neural module
network is linear in T as long as K ≤ 1. Note that even if there is no distribution shift, its error is
Pp(w) [kf(x,w) — f *(x,w)∣2 ≤ Tef ∙ max{KT-1,1}] ≥ 1 — Teg,
by the same argument as the proof of Theorem 6.7. The exponential dependence on K is unavoidable
since K > 1 says that the modules fj can expand the input, which leads to exponential blowup in the
magnitude of the output as a function of T, which also makes the estimation error exponential in T.
Thus, the only cost to the distribution shift from p to q is the additional error probability T2α.
7 Conclusion
We have presented a number of results demonstrating that over-parameterization does not funda-
mentally harm learning models that are robust to arbitrary distribution shifts. In particular, even
though we can no longer identify the true parameters for quadratic neural networks, we show that
we can identify the true function, thereby enabling us to prove new results in bandits and transfer
learning. Finally, we provide preliminary analysis extending these results to neural module networks
to handle complex data generating processes. A limitation of our work is that it is specialized to
quadratic neural networks; a key direction for future work is to explore how our results extend to
other activation functions and deeper architectures.
9
Under review as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In NIPS, volume 11,pp. 2312-2320, 2011.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International Conference on Machine Learning, pp. 127-135. PMLR, 2013.
Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, pp. 233-244. PMLR,
2020.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 39-48, 2016.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep
nets via a compression approach. In International Conference on Machine Learning, pp. 254-263.
PMLR, 2018.
Francis Bach, Julien Mairal, and Jean Ponce. Convex sparse matrix factorizations. arXiv preprint
arXiv:0812.1869, 2008.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Hamsa Bastani. Predicting with proxies: Transfer learning in high dimension. Management Science,
2020.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning
bounds for domain adaptation. In Advances in neural information processing systems, pp. 129-136,
2008.
Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a
minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57
(4):2342-2359, 2011.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high-dimensions without the computational intractability. SIAM Journal on
Computing, 48(2):742-864, 2019.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. In International Conference on Machine Learning, pp. 1329-1338. PMLR, 2018.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-1685.
PMLR, 2019.
John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally
robust optimization. arXiv preprint arXiv:1810.08750, 2018.
Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with
regression oracles. In International Conference on Machine Learning, pp. 3199-3210. PMLR,
2020.
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence
of adversarial training in overparametrized neural networks. Advances in Neural Information
Processing Systems, 32:13029-13040, 2019.
10
Under review as a conference paper at ICLR 2022
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. In International Conference on Machine Learning, pp. 1233-1242.
PMLR, 2017a.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017b.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
Daniel Hsu, Sham M Kakade, and Percy Liang. Identifiability and unmixing of latent parse trees.
arXiv preprint arXiv:1206.3137, 2012.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175,
2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Maryia Kabanava, Richard Kueng, Holger Rauhut, and Ulrich Terstiege. Stable low-rank matrix
recovery via null space properties. Information and Inference: A Journal of the IMA, 5(4):405-441,
2016.
Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. An introduction to computational
learning theory. MIT press, 1994.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-
subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A
benchmark of in-the-wild distribution shifts. arXiv preprint arXiv:2012.07421, 2020.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory,
pp. 2-47. PMLR, 2018.
Philip M Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks.
arXiv preprint arXiv:1905.12600, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. arXiv preprint arXiv:1706.08947, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy:
Behavioral testing of nlp models with checklist. arXiv preprint arXiv:2005.04118, 2020.
Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. A
benchmark for systematic generalization in grounded language understanding. arXiv preprint
arXiv:2003.05161, 2020.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395-411, 2010.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
11
Under review as a conference paper at ICLR 2022
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks.
arXiv preprint arXiv:1706.03691, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig
Schmidt. Measuring robustness to natural distribution shifts in image classification. arXiv
preprint arXiv:2007.00644, 2020.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Book Draft
(Working Publication), 2016. URL https://www.stat.berkeley.edu/~wainwrig/
nachdiplom/Chap2_Sep10_2015.pdf.
Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions:
Relating covariate shift to model misspecification. In International Conference on Machine
Learning,pp. 631-639. PMLR, 2014.
Kan Xu, Xuanyi Zhao, Hamsa Bastani, and Osbert Bastani. Group-sparse matrix factorization for
transfer learning of word embeddings. In International Conference on Machine Learning, 2021.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis-kahan theorem for
statisticians. Biometrika, 102(2):315-323, 2015.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, pp. 11492-11502. PMLR, 2020.
12
Under review as a conference paper at ICLR 2022
A Additional Related Work
Low-rank matrix factorization. Our notion of functional identification for quadratic neural network
is related to the low-rank matrix factorization literature. However, they impose a low-rank structure
on the matrix to recover, and hence typically require extra conditions to identify the matrix—e.g., the
restricted isometry property (RIP) (Candes & Plan, 2011; Ge et al., 2017a), or bounded `2 norm of
noise vector (Kabanava et al., 2016). In contrast, we consider a more general case and do not assume
any underlying structure of the matrix; in particular, since our goal is to capture over-parameterization
of neural networks, our matrix is usually decomposed as φ = θθ>, where θ ∈ Rd×k and k ≥ d (and
φ is not necessarily low-rank). Also, we study the prediction error of neural networks in the presence
of distribution shifts, whereas the goal of the low-rank literature is to recover the true matrix.
Multi-armed bandits. Prior literature on parameterized bandits has considered a number of func-
tional forms, ranging from linear (Abbasi-Yadkori et al., 2011; Rusmevichientong & Tsitsiklis, 2010)
to neural tangent kernels (Zhou et al., 2020). Most of this work makes a realizability assumption that
the model family contains the true model;3 implicitly, they consider model families where there is a
unique, identifiable true parameter. These assumptions are necessary precisely due to the fact that the
test and training distributions are different; thus, much of the bandit literature has focused on proving
parameter identification results to enable learning. In contrast, the identifiability assumption does
not hold for quadratic neural networks because they are invariant to parameter transformations. To
the best of our knowledge, we consider the first over-parameterized bandit problem that considers a
model that is not parameter-identifiable; we find that similar regret results hold as long as the function
represented by the model can be identified. Separately, Foster & Rakhlin (2020) makes a general
connection between online regression oracles and the regret of a bandit algorithm; however, their
approach only provides good guarantees when the regression oracle returns a model that generalizes
off-distribution. Finally, recent work on UCB with neural tangent kernels (Zhou et al., 2020) provides
general regret bounds, but their bound is only sublinear under conditions such as the true reward
function having small RKHS norm (see Remark 4.8 in their paper), which amounts to assuming they
can recover the true parameters.
Novelty. We briefly discuss the novelty of our results compared to existing work. First, the results in
Section 3 are novel. To the best of our knowledge, the proof strategy in our main result, Theorem 3.6,
is novel, though we note that the preceding lemmas are based on standard arguments—e.g., bounding
the convexity of Lp(φ) (Lemma 3.3) and the Lipschitz constant (Lemma 3.4) of fφ; also, Lemma 3.5
relies on a standard covering number argument. For our applications to bandits and transfer learning,
our key novel results are Lemma C.3 for bandits, which proves smoothed bounded response for
quadratic neural networks, and Lemma D.1 for transfer learning. Finally, to the best of our knowledge,
our arguments in Section 6 are novel.
3Slightly different from realizability in PAC learning (Kearns et al., 1994), which says there is a model with
zero true loss.
13
Under review as a conference paper at ICLR 2022
B Proofs for Section 3
B.1 Proof of Minimum Eigenvalue for Uniform Distribution
In this section, we prove the claim that Assumption 3.2 holds for the covariate distribution where xi
is an i.i.d. random variable with distribution Uniform(xi; [-1/2, 1/2]). To this end, note that
∖( d	∖2'
Ep(x)[(X δX) ] = EP(X) I I x x xixj ∆ij I
i,j=1
= Ep(X)I X x4∆2i + X x2χ2∆ii∆jj + 2 X χ2χ2∆2j
i	i6=j	i6=j
i	i6=j	i6=j
≥ ɪ∣∣∆∣∣2,
-180UUF,
as claimed. □
B.2 Proof of Lemma 3.3
We use the notation U : V2f (φ) : V to denote the matrix inner producthU, V2f (φ)(V))for
U, V ∈ Rd×d. The Hessian V2 f (φ) can be viewed as a d2 X d2 matrix. As everything here is
bounded, we can exchange the expectation and differentiation. Therefore, the Hessian of our loss
function has for any symmetric matrix ∆
∆ : V2Lp(φ) : △ = 2Ep(χ)[(x>∆x)2] ≥ 2αk∆kF,
where the last inequality uses Assumption 3.2.	□
B.3 Proof of Lemma 3.4
By our definition, for any φ, φ0 ∈ Φ,
lfφ(χ) - fφo(χ)l = l(χ>(Φ — Φ0)χ)2l ≤ XmLxkO — Φ0kF.
Given our quadratic loss function, we have
l(fφ(χ) - fφ*(χ))2-(fφo(χ)- fφ*(χ))2l
.~ , , ~ , ~ ~ . . . ~ , ~ , ..
≤ lfφ(χ) - fφ*(χ) + fφ0(χ) - fφ*(χ)llfφ(χ) - fφo(χ)l
≤ 4φmaxX4max kφ - φ0 kF .
Next, the true loss satisfies
|Lp(Φ) - Lp(Φ )| ≤ Ep(x)[∣(fφ(x) - fφ* (x))2 - (fφo (X)- fφ* (x))2∣] ≤ 4φmaxXm1axkΦ - Φ l∣F.
Finally, the empirical loss satisfies
n
1
|L(。； Z)-	L(φ	; Z)I =	~ E2[(fφ(xi)	-	yi)	-	(fφ0 (Xi)-	yi)	]
n i=1
i=1
1n
+■— E 归小 |fXXi) - fφ0 (Xi)I
n i=1
≤ (4φmax Xmax + 2ξmax Xmax)kφ - φ kF ,
as claimed. □
14
Under review as a conference paper at ICLR 2022
B.4 Proof of Lemma 3.5
First, we have the following results:
Lemma B.1 (Covering Number of Euclidean Ball). For a Euclidean ball in Rn1 ×n2 with radius R
with respect to Frobenius norm, there exists an -net E such that
|E| ≤
1+
n1n2
Proof. This claim follows by a direct application of Proposition 4.2.12 in Vershynin (2018). 口
Lemma B.2 (Hoeffding’s Inequality for Subgaussian Random Variables). Letting {zi}in=1 be a set
of independent σ-subgaussian random variables, then for all t ≥ 0, we have
Pr
1n
-Vzi ≥ t
n i=1
2nt2
≤ exp (-卞)∙
Proof. See Proposition 2.1 of Wainwright (2016).
□
Now, we prove Lemma 3.5. Consider an /(4K)-net E. Then, for any φ ∈ Φ, there exists φ0 ∈ E
such that
ʌ	ʌ	V
l(-L(φ;Z)- Lp(O))-(L(O;Z)- Lp(φ ))| ≤ 2Kllφ - φ IIf ≤ 2∙
Therefore, we have
Pp(Z)SuP ∣L(g(θ); Z)- Lp(g(θ)) - σ(Z)∣ ≥ e
θ
=Pp(Z) Sup lL(φ; Z)- Lp(O)- σ(Z)I ≥ e
φ∈Φ
≤ Pp(Z) maxIL(O; Z)- Lp(O)- σ(Z)I ≥ 2
≤ X Pp(Z) h|L(O; Z)- LP(O)- σ(Z)I ≥ 2i ∙	(13)
φ∈E
Now, defining
nn
L(O; Z) = £X(fφ(xi)- fφ*(xi))2	and	η(O; Z) = - X(f⅛(xi) - fφ*(xi))ξi,
n i=1	n i=1
and recalling that σ(Z) = n-1 Pin=1 ξi2, then we have
ʌ —
~ , . ~ , , , . ,
L(O; Z) = L(O; Z) + 2η(O; Z) + σ(Z)∙
Thus, continuing from (13), we have
X Pp(Z) h|L(O; Z)- LP(O)- σ(Z)I ≥ 2i
φ∈E
≤	XPp(Z) [iL(O; Z) - Lp(O)I + 2Iη(O; Z)I ≥ j]
φ∈E
≤	X(Pp(Z) [iL(O; z) - Lp(O)I ≥ 6i + Pp(Z) hIη(O; Z)I ≥ jD ∙	(14)
φ∈E
For the first term in (14), note that I(fφ(x) - fφ*(x))2I ≤ 'mm^, So (fφ(x) - fφ*(x))2 is 'mm^-
subgaussian; thus, by Lemma B.2, we have
X Pp(Z) hIL(O; Z) - Lp(O)I ≥ ji ≤ 2IEI∙ exp 卜 181-
(15)
15
Under review as a conference paper at ICLR 2022
~
~
~
~
Next, for the second term in (14), note that ∣(fφ(xi)-fφ* (xi))ξi∣ ≤ 'maxξmax, so (fφ(xi)-fφ* (xi))ξi
is 'maxξmax-subgaussian; thus, by Lemma B.2, we have
X Pp(Z) [∣η(φ; Z)1 ≥ ∣] ≤ 2∣E∣∙ exp (-
φ∈E	6
2
nE2
18'maxξm ax
(16)
Combining (15) & (16), continuing from (14), we have
X(Pp(Z) h∣L(Φ; Z)- Lp(φ)∣ ≥ 6i + Pp(Z) h∣η(φ; Z)| ≥ 6i)
φ∈E
≤ 4|E| ∙ exp -
2
nE2
i8'max('max+ξm α
/ n ∕ι i 8φmaxK∖"	(
≤2 1+∙exp -
2
nE2
2 exp -
2
nE2
18'2 ax('2ax + ξ⅛ax)
max max max
18'max('max + ξm ax”
+ d2 log (ι + 8φmxK
(17)

where for the first inequality, we have used max{'max, ξ22ax} ≤ 'mmax + ξmmax, and the second inequality
follows since by Lemma B.1, the covering number of the E-net E of Φ satisfies
d2
∣E∣≤ 1 +
Finally, We choose E so that (17) is smaller than δ——in particular, letting
E = j18'm∕'max + ξmaX) (d2 max {ι, log (ι + ^maxKn)} +	2).
then continuing (17), we have
2exp (- i8'max ('ll+ξm ax)+d2log (1 + 8⅛κ)) ≤ δ,
as claimed. 口
B.5 Proof of Proposition 3.8
^
L(φ; Z) is twice differentiable and convex inφ. Note that the minimization problem of L(θ; Z) is
ʌ
♦ 1 . . .1 . i' T / /C' Γ7∖ 5 T	♦ t .	.	∙ t . 1	1	Zi t	1 T
equivalent to that of L(g(θ); Z). We consider two cases. First, consider the case where θ has rank d.
ʌ
EI r∙	t	t∙ .∙	X—7 T / ∕^t Γ7∖	Γ∖ ∙ .1	X—7 T / /C' Γ7∖	Γ∖ Λ ∙ ) ♦
The first order condition VL(θ; Z)=0 is the same as VL(g(θ); Z) = 0, which gives
ʌ
~ , ʌ . ʌ
VL(φ; Z)θ = 0.
(18)
As θ is of full row rank, there exists a matrix θ* ∈ Rk×d such that θθ* = I (e.g. θ 飞=θ>(θθ>)-1).
We can right multiply the above equation by θ* and obtain that
ʌ
~ , ʌ
VL(φ; Z)=0.
ʌ . ʌ
k I / I Γ7∖ ♦	∙	； . )	Λ	∙	-I ∙	!	/ Zi∖ ♦	11 1 ∙	i' T / I Γ7∖ rτ-><	Γ∙
As L(φ; Z) is convex inφ, the above implies φ = g(θ) is a global minimum of L(φ; Z). Therefore,
θ is a global minimum of L(θ; Z). Next, consider the case where the rank of θ is smaller than d. In
this case, we follow the proof strategy in Proposition 4 in Bach et al. (2008); we provide here for
completeness. In this case, Equation (18) still holds, which implies
ʌ . .	ʌ .
~ , ʌ	. ʌ ʌɪ	~ , ʌ . ʌ
0 = VL(φ; Z)θθ> = VL(φ; Z)φ.	(19)
The Hessian of L(g(θ); Z) has
ʌ ʌ ʌ . .
rɪ ~ . . ʌ,	~ . ʌ	-I-	rɪ ~ . ʌ	. ʌ -I-	人-I- ʌ -I-	人-ι-
V2L(g(θ); Z)(∆, ∆) = 2<VL(φ; Z), ∆∆>i + V2L(φ; Z)(θ∆> + ∆θ>, θ∆> + ∆θ>).
16
Under review as a conference paper at ICLR 2022
As θR is also a local minimum for any orthogonal matrix R (i.e., RR> = R>R = I), We can find a
θ With the last column being 0 by right multiplying certain R. Then, consider any ∆ With the first
k 一 1 columns being 0 and the last column being any U ∈ Rd. With this choice of ∆ and θ, θ∆> = 0.
Therefore,
ʌ ʌ
rɪ ~ , ʌ.	.	.	.	.	-I-	~ , ʌ	.
V2L(g(θ); Z)(∆, ∆) = 2u>VL(φ; Z)u.
ʌ ʌ
…	ʌ . 一 一	_ ≈ ,	, O.	.	.	_C ≈ ,	, O. ...... _ 一 .一.
Since θ is a local minimum of L(g(θ); Z), it holds that V2L(g(θ); Z)(∆, ∆) ≥ 0, which implies
ʌ
__~ , ʌ__
VL(φ; Z)占 0.	(20)
Equation (19) and (20) together comprise the first order conditions of the convex minimization
^
problem min@占0 L(φ; Z). Thus, θ is also a global minimum. 口
C Proofs for Section 4
First, we provide the full statement of Theorem 4.2 (including constants).
Theorem C.1. The expected regret of Algorithm 1 is
R(T) ≤ Co + Cl ∙ T2/3
where
C _ ____________________64(φmax) 2d2-1
0 =	2d2 + 2	3d2
(135M ('max + ξmax )2d3) 2d2-1 (8φ max K/%ax ) 2d2-1
Cl = 162d2(M 2('max + ξmax )4 Φmax)1/3.
Before we prove Theorem C.1, we first prove a preliminary result establishing an analog of the
smooth best arm response property Rusmevichientong & Tsitsiklis (2010) to our setting. First, we
have the following useful result:
Lemma C.2. Let φ, φ0 ∈ Rd×d be symmetric matrices, let x, x0 ∈ Rd be eigenvectors of φ, φ0
corresponding to their top eigenvalue, such that kxk2 = kx0k2 = 1, and let λ1 ≥ λ2 ≥ ... ≥ λd be
the eigenvalues of φ0. Suppose that hx, x0i ≥ 0. Then, we have
kx-x0k2 ≤ 2"2kφ- φ0k2 .
λ1 一 λ2
Proof. See Corollary 3ofYuet al. (2015).	口
Next, let X : Rd×d → 2x denote the subset of reward-maximizing arms for g(θ) = θθ>—i.e.,
χ(φ) = arg maxx>φx,
x∈X
where the argmax returns the set of all optimal values. Then, we have the following analog of smooth
best arm response:
Lemma C.3. For any φ ∈ Rd×d, there exists X ∈ χ(φ) and x* ∈ χ(φ*) such that
kx 一 X*k2 ≤ Mkφ 一 Φ*kF.
Proof. First, note that x, x* are eigenvectors of φ, φ* corresponding to their top eigenvalues, re-
spectively. Next, note that if x* ∈ χ(φ*), then we also have -x* ∈ χ(φ*); thus, without loss of
generality, we can assume that hx*, xi ≥ 0. Also, note that kxk2 = kx* k2 = 1 since the optimizer
maximizes the magnitude of x. Thus, we have
kx - x*k2 ≤ 23T- φ*k2 ≤ Mkφ 一 φ*kF,
λ1 一 λ2
where the second inequality follows by by Lemma C.2, and the third inequality follows by Assump-
tion 4.1, as claimed.	口
17
Under review as a conference paper at ICLR 2022
Now, we prove Theorem C.1. The cumulative regret R(T) of a horizon of T has that
T
R(T) = E X(fθ*(χ*)- fθ*(χt))
t=1
mT
=E X(fθ*(χ*)- fθ*(xt))+ X (fθ*(x*)- fθ*(xt))
t=1	t=m+1
TT
≤ 2mφmax + E X hg(θ) - g(θ*),xx> - x*x*>i + X hg(θ),x*x*> - XX>i ,
t=m+1	t=m+1
(21)
where θ is an estimator that minimizes the empirical loss of the first m samples, X ∈ χ(g(θ))
maximizes the estimated expected reward fj(x), and hφ, φ0i = Pdj=I φijφj is the matrix inner
product. Since X is a maximizer of fa(x) = hg(θ), χχ>i,we have hg(θ),χ*χ*> - XX>i ≤ 0. Thus,
continuing from (21), we have
T
R(T) ≤ 2mφmax + E	X hg(θ) - g(θ*), XX> - x*x*>i
t=m+1
≤ 2mφmax + (T - m)E [∣∣g(θ) - g(θ*)IIF∣∣XX> - x*x*>∣∣Fi .	(22)
To bound the second term in (22), note that
∣∣XX> — x*x*>∣f ≤ ∣∣XX> — Xx*>∣f + ∣∣Xx*> — x*x*>∣f ≤ 2M∣∣g(θ) — g(θ*)∣∣F,
where the last step follows by Lemma C.3. Next, by Theorem 3.6, we have
"-"山 ≤ V?
=d (453'm*m+ξm ax) (d2 max (ɪ, log (i+8φmaxKm)}+log δ))1/4
with probability at least 1 - δ. Now, defining the event
G={∣g(θ)- g(θ*)∣F ≤ ∏},
letting
δ = 2 exp -d2 max 1, log 1 +
8φmaxKm
'max
and continuing from (22), we have
R(T) ≤ 2mφmax + T ∙ E [∣g(θ) - g(θ*)∣F∣XX> - x*x*>∣FI(G)] + 4Tφmax ∙ P(Gc)
≤ 2mφmax + 2MT ∙ E [∣g(θ) - g(θ*)∣F ∣ Gi + 4Tφmax ∙ P(Gc)
≤ 2mφmax + 270M('max + ξmax)2d3Tylog(3 + 8φmxKT∕'max) +
8Tφmax
(8φmaxKm∕'max)d2 .
(23)
The third term in inequality (23) is smaller than the second term when
m≥
1/(d2-1/2)
4φmax	∖
135M('max + ξmax)2d3(8φmaxK/'max)d plθg(3 + 8φmaxKT∕'max))
(24)
18
Under review as a conference paper at ICLR 2022
For a choice of m satisfying (24), continuing from (23), we have
R(T) ≤ 2mφmax + 540M('lmχ + ξlraχ)2d3T∖∕log(3 + 8φmaχκτ∕'maχ).	(25)
m
Next, we choose m to minimize the upper bound in (24) for sufficiently large T—in particular,
_____________________ 2 -
135M('maχ + ξmaχ)2d3T,lθg(3 + 8φmaχKT∕'mα) 3
φmax	)
(26)
With this choice of m, we have
R(T) ≤ 162(M2 ('max + ξmax)4φmax) 3 d2T2/3 (lθg(3 + 8φmlaχKT∕'mιa)) 3 .
Finally, note that (24) holds under the choice of m in (26) for T satisfying
T√log(3 + 8φmaxKT∕'max) ≥
2d2 + 2
64(φmaχ) 2d2-1
2d2 + 2
3d2
(135M('maχ + ξmaχ)2d3) 2d2-1 (8。皿至长/琮亚)2d2-1
The claim follows. □
D	Proof of Theorem 5.2
First, we have the following key result:
Lemma D.1. Let θ, θ0 ∈ Rd×k, and let φ = θθ> and φ0 = θ0θ0>. Assume that kφ - φ0kF ≤ η, and
that σmin(θ) ≥ σ0 > 0, where σmin(θ) is the minimum singular value of θ (more precisely, the dth
largest singular value). Then, there exist orthogonal matrices R, R0 ∈ Rk×k such that
kθR - θ0R0kF ≤ -η.	(27)
σ0
Proof. Consider the SVDs θ = UΣV> and θ0 = U0Σ0V0>, where U,U0 ∈ Rd×d, Σ,Σ0 ∈ Rd×d,
and V, V 0 ∈ Rk×d; then, we have φ = UΣ2 U> and φ0 = U0Σ02U0>. Then, we claim that the choices
R = V U> and R0 = V 0U0> satisfy (27). In particular, note that θR = UΣU> and θ0R0 = U0Σ0U0>,
since V >V = V 0>V 0 = Id since k ≥ d, where Id ∈ Rd×d is the d-dimensional identity matrix.
Thus, it suffices to show that
σ0kUΣU> - U0Σ0U0>kF ≤η. (28)
To this end, note that
η≥ kφ - φ0kF = kUΣ2U> - U0Σ02U0>kF = kU0>UΣ2 - Σ02U0>UkF,	(29)
where in the last step, we have multiplied the expression inside the Frobenius norm by U0> on the
left and by U on the right, using the fact that the Frobenius norm is invariant under multiplication by
orthogonal matrices. Defining W = U0>U, note that
(W Σ)ij	d WikΣkj = k=1 d	WijΣjj	(30)
(ΣW)ij	X Σ0ik Wkj =	WijΣ0ii	(31)
	k=1 d		
(WΣ2)ij	X Wik (Σ2)kj	= Wij Σj2j	(32)
	k=1 d		
(Σ2W)ij	X(Σ02)ikWkj k=1	= Wij Σ0i2i .	(33)
19
Under review as a conference paper at ICLR 2022
Then, continuing from (29), we have
d
η2≥ kWΣ2-Σ02Wk2F = X Wi2j(Σj2j-Σ0i2i)2
i,j=1
d
= X Wi2j(Σjj -Σ0ii)2(Σjj+Σ0ii)2
i,j=1
d
≥ X Wi2j(Σjj - Σ0ii)2σ02
i,j=1
=σ02kWΣ-Σ0Wk2F
= σ02kU0>UΣ - Σ0U0>Uk2F
= σ02kUΣU> - U0Σ0U0>k2F,
where on the first line, we have used (32) & (33), on the third line we have used Σjj ≥ σ0, on the
fourth line we have used (30) & (31), and on the last line we have multiplied on by U0 on the left
U > on the right, again using the fact that the Frobenius norm is invariant under multiplication by
orthogonal matrices. Thus, We have shown (28), so the claim follows.	□
We note here that our result provides an analog of Lemma 6 in Ge et al. (2017a) for quadratic neural
networks.
Now, we prove Theorem 5.2. First, by directly applying the arguments in the proof of Theorem 3.6,
we have
kg(θp) - g(θp)kF ≤ y -α^
• .1	1	1 ∙ι ∙ .	. 1 ,r	¢- /rʌ T T	A ∙ .	1 Γ∙	. 1	1	. Zi⅛ T	1	1
with probability at least 1 - δ∕2. However, θp itself may not be close to θp. Instead, applying
T	1 ʌ 1	∙ .1 Zi	久	1	∕∖l	Zi-J=	t ♦ .t	/rʌ 7 .t	∙	.	.t	t	.	∙
Lemma D.1 with θ = θp and θ0 = θp, and with η = vz2ep∕α, there exists an orthogonal matrix
Rp = R R> that aligns θp with θp, yielding
k"p-θpRpkF ≤ σ v20p,
1	∙	.1	∙	1	1	Γ∙八士 TL T	1	. P⅛	Zi-J= I >	1	.	.1	. .1 ∙	111
where σ0 is the minimum singular value of θp. Now, let θg = θgRp, and note that this is a global
minimizer (i.e., g(θg) = g(θg)), since Rp is orthogonal. Then, we have
≈ ʌ
O
..一	,, ... ..,,
kθg -	θpkF	≤ kθgRp	- θpRpkF	+ kθpRp	- θpkF
≤kθg -θpkF+σ r2αρ
≤ B + ɪ 产
σ0 α
(34)
with probability at least 1 - δ∕2. In other words, an alternative global mmimizer θg exists within a
ʌ
ʌ
small FrObeniUS norm of our proxy estimator θp, even if θp is not close to θp.
L* 1 1	.1	,,1, /C 八 Ill	,,1, C Zi _ 7-1 / Zi -r∖∖	1	. 1	1 .	. ∙	1	1
Finally, on the event that (34) holds, note that for θ ∈ B2(θp, B), we have the alternative upper bound
, . , ... .... ............................................. ʌ
lfθ(X)- fθg(x))l ≤ Kkg(θ) -g(θg)kF ≤ KB,
1	.1 Γ` . ∙	1 ∙ .	1111 T	ry t , 1	,	1	/1	TT -f∖ ml	. 1	..1
where the first inequality holds by Lemma 3.4; thus, we can take 'max = KB. Thus, on the event that
(34) holds, by Theorem 3.6, we have
Pp(Z)
Lq (θg ) ≤ 乎
≥ 1- 2，
so the claim follows by a union bound. □
20
Under review as a conference paper at ICLR 2022
E Proofs for Section 6
E.1 Proof OF Proposition 6.4
Suppose that Zt ∈ {0,1} is binary, z0 = 0, and
P(Z I z0) = {0
if Z = Z0
otherwise.
In particular, since z0 = 0, P(W) = I(W = w0) places all weight on the zero sequence w0 = 0...0.
Next, consider the shifted distribution
∩	if z = z0 = 1
q(zt ∣ zt-i) = < 1 - α∕2 if Z = z0 = 0
1α∕2	otherwise.
Note that ∣∣p(∙ ∣ z0) - q(∙ ∣ z0)∣∣tv ≤ α, so Assumption 6.3 is satisfied. Note that
T
q(WO) = Y q(O I O) = (I- a/2)T.
t=1
As a consequence, we have
IlP - q∣∣τV = E |p(w) - q(W)I
w∈W
=IP(WO) - q(wo)∣ + E	q(W)
W∈W∖{wo}
= (I-(I- a/2)T) + (I-(I- a/2)T)
= 2(1 - (1 - α∕2)T),
as claimed. □
E.2 Proof of Lemma 6.5
Note that
kqt -PtIlTV
k C
=X / 1五(Zj)-Pt(Z,j)|dZ
j=i'
k k 「
=XX	Iej	=g*(Z0,j0))	∙Iq(Z	i	z)a-1(Z0,j0) —P(Z	i	ZyPt-I(Zm\dZdZ
j = 1 3' = ij
k
=X I |q(Z i z)7t-i(Z0,j0) -P(Z i Z0)Pt-i(Z0,j0)|dZ0dZ
j0=1 J
k
≤ X ∖虱Z i z) -P(Z i z0)| ∙ 7t-i(Z0,j0) + P(Z i Z) ∙ |7t-i(Z0, j0) -Pt-I(Z0,j0)|dZ0dZ
j=∖J
k
≤ X / α ∙比-1(z0, j0) + @-i(Z0,j0)— Pt-I(z0,j0)|dz0
j0=11
≤ α + 府t-1 -Pt-IIlTV.
Since qo(z, j) = Po(z, j) for all Z ∈ Z and j ∈ [k], by induction, ∣qt - PtIlTV ≤ ta. Thus, we have
1T
l∣q -PkTV ≤ T Ekqt-Ptk ≤ T«,
t=1
as claimed. □
21
Under review as a conference paper at ICLR 2022
E.3 Proof of Lemma 6.6
First, we prove the following lemma.
Lemma E.1. We have Pt(Ztjt-1)=	X Z j1,...,jt-2j Proof. For the base case, we have k	G p2(Z2,j1)= X、 jo = 1j k =X八 j0=i J =∕ljι as claimed. For the inductive case, k Pt(Ztjt-I)= X I l(jt-	(Y ɪ(jT = g*(zτ,jτT))) ∙p(zi,…,Zt)dzi…dzt-1. 1(j1 = g*(z1,j0)) ∙ P(z2 | Z1) ∙ P1(z1,j0)dz1 1(j1 = g"(z1,j0)) ∙ IP(Z2 | Z1) ∙ l(j = 0) ∙ P(z1)dz1 =<7*(z1,j0)) ∙P(z1,z2)dz1, we have -1 = g*(zt-1,jt-2)) ∙P(zt | zt-1) ∙Pt-I(Zt-1,jt-2)dzt-1
jt-2 = 1 k	t-1 =	X	/(Y 1(jτ-1 = g*(zτ-1,jτ-2))卜 P j1,∙∙∙,jt-2 = 1	∖τ =1	) as claimed. Now, we prove Lemma 6.6. First, note that for each t ∈ [T], we have Pp(W) (O(W)t = g*(w)t)八(^ g(W)T = g"(W)TU =/ l(g(w)t = g*(w)t) ∙ (Y l(g(w)τ = g*(w)τ)) ∙P(W)dw k	t-1 = X	/ l(g(w)t = g*(w)t) ∙ I Y l(g(w)τ = g*(W)T) I ∙p( j1,∙∙∙,jt-1 = 1	∖τ =1	) k	t-1 =	X / i(g(w)t = g*(W)t) ∙ ∙ (Y i(g(W)T = g*(W)T))∙ jι,…,jt-1 = 1 J	∖τ=1	) ∙ P(W)dW k	t-1 =	X	/ l(g(zτ, jτ-1)= 0*(ZT, jτ-1)) ∙ (Y l(g(zτ, jτ-1)	(Z1, ..., Zt)dz1...dzt-1. □ j1...jt-1 | w) ∙P(w)dw Y l(jτ = g*(zτ, jτ-1)) =g"f(zτ, jτ-1)))
j1,…,jt-1=1	τ= =1 ∙ (Y 1(jτ = 0*(ZT, jT-1))) ∙ P(w)dw k	t-1 ≤	X	/ l(g(zτ, jτ-1) = 0*(ZT, jτ-1)) ∙ ( Y l(jτ = j1,…,jt-1=1 J	∖τ =1 k	t-1 =	X	/ l(g(zτ, jτ-1) = 0*(ZT, jτ-1)) ∙ (Y l(jτ = j1,∙∙∙,jt-1 = 1	∖τ =1 =Ppt(Zj) [g(zτ,jτ-I) = g*(zτ,jT-1 )],	二 g*(Zτ,jτ-1))) ∙ P(w)dw 二 g*(zτ,jτT))) ∙ P(Z1, ...,Zt)dz1...dzt
22
Under review as a conference paper at ICLR 2022
where the last step follows from Lemma E.1. Now, note that
T
Pp(W) [g(w) = g*(w)] = X Pp(W)
t=1
(g(w)t = g*(w)t)八
't-1
^ g(W)T = g*(W)
τ=1
))
T
≤ X Ppt(z,j) ∖gzzjj) = g*(Z,j)]
t=1
≤ Teg,
as claimed. 口
J l(g(z,j)= g(Zj ∙ Ig(Zj)-P(Zj)IdZ
E.4 Proof of Theorem 6.7
First, we show that Pq(Zj) [g(z, j) = g* (z, j)] ≤ eg + Ta. To this end, note that
_	-入，	、	,	,	、 r
Pq(Z,j)[g(z,j ) = g (z,j )]
=Pp(Z,j)[g(z,j)= g (z,j)] + Pq(Z,j)[g(Zj) = g (Z,j)] — Pp(Z,j)[g(zj) = g IZj)]
k
≤ eg + X
j=ι
≤ eg + Ilg 一训TV
≤ eg + Tα.
Next, by Lemma 6.6 with q in place of P and eg + Ta in place of eg, we have Pq(w)[g(w) = g* (w)] ≤
Teg + T2α. Then, assuming that g(w) = g* (w), we have
11/*(x,w) - f(x, w)k2
..,	.... ,ʌ ʌ ..............
=k(/jT o …o /ι)(X)- (/t o …o fjι)(X)k2
T
≤ E k(∕τ o …o /*t+1 o fj o fjt-1 o ... o fj )(x) - (/t o ... O fjt+ι o fjt o fjt-1 o ... o /ι)(x)∣2
t=1
T
X
K - ∙ k(fj o f jt-1 o ... o fjI)(X)- (fjt o f jt-1 o ... o fjI)(X)k2
t=1
T
≤ X KTTef
t=1
≤ Tef ∙ max{KT-1,1}.
The claim follows by a union bound. 口
23