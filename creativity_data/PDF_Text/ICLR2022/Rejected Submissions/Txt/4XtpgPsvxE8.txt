Under review as a conference paper at ICLR 2022
Multi-Objective Model Selection for Time Se-
ries Forecasting
Anonymous authors
Paper under double-blind review
Ab stract
Research on time series forecasting has predominantly focused on developing
methods that improve accuracy. However, other criteria such as training time
or latency are critical in many real-world applications. We therefore address the
question of how to choose an appropriate forecasting model for a given dataset
among the plethora of available forecasting methods when accuracy is only one
of many criteria. For this, our contributions are two-fold. First, we present a
comprehensive benchmark, evaluating 7 classical and 6 deep learning forecasting
methods on 44 heterogeneous, publicly available datasets. The benchmark code
is open-sourced along with evaluations and forecasts for all methods. These eval-
uations enable us to answer open questions such as the amount of data required
for deep learning models to outperform classical ones. Second, we leverage the
benchmark evaluations to learn good defaults that consider multiple objectives
such as accuracy and latency. By learning a mapping from forecasting models
to performance metrics, we show that our method ParetoSelect is able to ac-
curately select models from the Pareto front — alleviating the need to train or
evaluate many forecasting models for model selection. To the best of our knowl-
edge, ParetoSelect constitutes the first method to learn default models in a
multi-objective setting.
1	Introduction
For decades, businesses have been using time series forecasting to drive strategic decision-making
(Simchi-Levi et al., 2013; Hyndman & Athanasopoulos, 2018; Petropoulos et al., 2020). Analysts
leverage forecasts to gauge resource requirements, retailers forecast future product demand to opti-
mize their supply chains, cloud providers predict future web traffic to scale server fleets, and in the
energy sector, forecasting plays a crucial role e.g. to predict load and energy prices. In domains like
these and many other, more precise predictions directly translate into an increase in profit. Thus, it
is no surprise that research on forecasting methods has historically focused on improving accuracy.
In addition to more classical local forecasting methods which fit a model per time series, global
forecasting models such as deep learning and tree-based models have demonstrated state-of-the-art
forecasting accuracy (Wen et al., 2017; Oreshkin et al., 2019; Salinas et al., 2020a; Smyl, 2020) when
sufficient training data is available (Makridakis et al., 2018; Januschowski et al., 2020). This research
has led to a large variety of different forecasting models and hyperparameter choices (Hutter et al.,
2019), where different models exhibit vastly different characteristics, including accuracy, training
time, model size, and inference latency.
While this variety of forecasting models is a great resource, it introduces challenging questions.
On the one hand, researchers would like to understand patterns in the performance of different
models and to benchmark new models against existing methods. On the other hand, practitioners
are interested in understanding which model performs best for a particular dataset or application.
In this work, we address both of these problems: we release one of the most comprehensive publicly
available evaluations of forecasting models across 44 datasets. Using this benchmark dataset, we
develop a novel method for learning good defaults for forecasting models on previously unseen
datasets. Importantly, we adopt a multi-objective perspective that allows us to select forecasting
models that are simultaneously accurate and satisfy constraints such as inference latency, training
time, or model size.
1
Under review as a conference paper at ICLR 2022
The main contributions of this work can be summarized as follows:
•	We release the evaluations of 13 forecasting methods on 44 public datasets with respect to mul-
tiple performance criteria (different forecast accuracy metrics, inference latency, training time,
and model size). This constitutes, by far, the most comprehensive publicly available evaluation
of forecasting methods. Those evaluations can be leveraged, for example, to assess the relative
performance of future forecasting methods with little effort.
•	As an example application of this benchmark, we use the data to perform a statistical analysis
which shows that only a few thousands observations are required for deep learning methods to
outperform classical methods. In addition, we investigate the benefit of ensembling forecasting
models.
•	We propose a novel method that can leverage offline evaluations to learn good default models
for unseen datasets. Default models are selected to optimize for multiple objectives.
•	We introduce a technique for ensembling models in a multi-objective setting where the resulting
ensemble is not only highly accurate but also optimized for other objectives, such as a low
inference latency.
2	Related Work
Time Series Forecasting. Time series forecasting has seen a recent surge in attention by the aca-
demic community that has started to reflect its relevance in business applications. Traditionally,
univariate, so-called local models that consider time series individually have dominated (Hyndman
& Athanasopoulos, 2018). However, in modern applications, methods that learn globally across a
set of time series can be more accurate (Oreshkin et al., 2019; Salinas et al., 2020a; Lim et al., 2021;
Montero-Manso & Hyndman, 2021) — in particular, methods that rely on deep learning. With a
considerable choice of models available, it is unclear which methods should perform best on which
forecasting dataset, unlike in other machine learning domains where dominant approaches exist.
In domains such as computer vision or neural architecture search (NAS), offline computations have
been harvested and leveraged successfully to perform extensive model comparisons or compare
different NAS strategies (Ying et al., 2019; Dong & Yang, 2020; Pfisterer et al., 2021). However, to
the best of our knowledge, no comprehensive set of model evaluations has been released in the realm
of forecasting. Consequently, some questions remain open: how much data is required for global
deep learning methods to outperform classical local methods such as ARIMA or ETS (Hyndman
& Athanasopoulos, 2018)? What is the impact on accuracy when ensembling different forecasting
models? While some recent methods incorporate ensembling (Oreshkin et al., 2019; Jeon & Seong,
2021), comparisons are often made against individual models which may cloud the benefit of the
method proposed versus the sole benefit of ensembling.
Learning Default Models. Finding the best model or set of hyperparameters is often performed
via Bayesian optimization given its theoretical regret guarantees (Srinivas et al., 2012). However,
even with early-stopping techniques (Golovin et al., 2017; Li et al., 2017), practitioners often restrict
the search to a single model due to the large cost of training many models. One technique to dras-
tically speed up the model/hyperparameter search is to reuse offline evaluations of related datasets
via transfer learning. For instance, Wistuba et al. (2015); Winkelmolen et al. (2020); Pfisterer et al.
(2021) leverage offline evaluations to alleviate the need for training many models by learning a
small list of n good defaults that provide a small joint error when evaluated on all datasets. This
approach bears a lot of similarity with ours. Key differences are that we consider multiple objec-
tives and model ensembles. In the time series domain, Shah et al. (2021) also considers the task of
automatically choosing from a large pool of forecasting models the one performing best on a partic-
ular dataset but also only optimizes for a single objective and relies on potentially expensive model
training for model selection.
3	Benchmarking Forecasting Methods
In this section, we formally introduce the problem of time series forecasting and subsequently pro-
vide an overview of the benchmark evaluations that we release. By evaluating 13 forecasting meth-
2
Under review as a conference paper at ICLR 2022
ods along with different hyperparameter choices on all 44 benchmark datasets and for two random
seeds, the benchmark comprises 4,708 training runs and amasses over 1 TiB of forecast data. At the
end of this section, we demonstrate how this benchmark data can be used to perform an extensive
comparison of contemporary forecasting methods.
3.1	Time Series Forecasting
The goal of time series forecasting is to predict the future values of one or more time series based on
historical observations. Formally, we consider a set Z = {z1(i:T) i }iK=1 of K univariate, equally-spaced
time series of lengths Ti , where za(i:)b = (za(i), za(i+) 1,..., zb(i))
denotes the vector of observations for
the i-th time series in the time interval a ≤ t ≤ b. In probabilistic time series forecasting, a model
then estimates the probability distribution across the forecast horizon τ, i.e. the distribution over the
τ future values, from the historical observations:
P( ZT)+1 :Ti+」z (": Ti)	(1)
Models typically approximate the joint distribution of Eq. (1) via Monte Carlo sampling (Salinas
et al., 2020a) or learn to directly predict a set of distribution quantiles for multiple time steps using
quantile regression (Wen et al., 2017; Lim et al., 2021).
3.2	Methods
We consider 13 models in total that can be distinguished into 7 local methods (estimating parameters
individually from each time series) and 6 global methods (learning from all available time series
jointly). Details about models can be found in Appendix A.
Local Methods. We use Seasonal NaTve (Hyndman & Athanasopoulos, 2018) and NPTS (Ranga-
puram et al., 2021) as simple, non-parametric baselines. Further, we consider ARIMA and ETS (Hyn-
dman & Athanasopoulos, 2018) as well-known statistical methods and STL-AR (Talagala, 2021)
and Theta (Assimakopoulos & Nikolopoulos, 2000) as inexpensive alternatives. Lastly, we include
Prophet (Taylor & Letham, 2018), an interpretable model that has received plenty of attention.
Global Methods. All global methods that we use are deep learning models, namely: Simple Feed-
forward (Alexandrov et al., 2020), MQ-CNN and MQ-RNN (Wen et al., 2017), DeepAR (Salinas
et al., 2020a), N-BEATS (Oreshkin et al., 2019), and TFT (Lim et al., 2021). For each model (ex-
cept MQ-RNN), we consider three hyperparameter settings: this includes the default set provided by
their implementation as well as hyperparameter sets that roughly halve and double the default model
capacity (see Table 2 in the appendix). Additionally, we consider three different context lengths that
govern the length of the time series that predictions are conditioned on.
Model Training. Model training is only required for deep learning models since parametric lo-
cal methods estimate parameters at prediction time. Deep learning models are trained for a fixed
duration depending on the size of the dataset. Further details can be found in Appendix C.
3.3	Datasets
Our benchmark provides 44 heterogeneous public datasets in total. The datasets greatly differ in
the number of time series (from 8 to ≈170,000), their mean length (from ≈22 to ≈500,000), their
frequency (minutely, hourly, daily, weekly, monthly, quarterly, yearly), and the forecast horizon
(from 6 to 60). Thus, we expect them to cover a wide range of datasets encountered in practice.
Datasets are taken from various forecasting competitions, the UCI (Dua & Graff, 2017), and the
Monash time series forecasting repository (Godahewa et al., 2021). Dataset sources, descriptions,
basic statistics, and an explanation of the data preparation procedure can be found in Appendix B.
3.4	Metrics
To measure the accuracy of forecasting models, we employ the normalized continuous ranked prob-
ability score (nCRPS) (Matheson & Winkler, 1976; Gneiting & Raftery, 2007) whose definition
3
Under review as a conference paper at ICLR 2022
Table 1: The average performance of various forecasting methods across all datasets with respect to
relative latency (compared to Seasonal Naive) and nCRPS. The table shows classical methods (top),
deep learning methods (middle, left), hyper-ensembles using all hyperparameter configurations of
deep learning models (middle, right), and latency-constrained ensembles (bottom). Best values
across within each group are displayed in bold, best values across all models are starred.
	Rel. Latency	nCRPS Rank
ARIMA	31100.33 土 11739.26	13.86 ± 5.83
ETS	1183.26 ± 321.88	15.45 ± 6.87
NPTS	133.64 ± 20.82	17.82 ± 7.35
Prophet	2635.38 ± 393.48	17.82 ± 5.52
Seasonal Naive	*1.00 ± 0.00	19.86 ± 4.22
STL-AR	76.12 ± 7.44	15.57 ± 6.73
Theta	25.15 ± 1.55	15.59 ± 6.19
DeepAR	21.55 土 2.33/310.62 土 34.26	9.91 ± 5.79 / 6.73 ± 5.24
MQ-CNN	1.78 ±0.15/ 17.48 ± 1.60	15.86 ± 5.75 / 13.07 ± 5.98
MQ-RNN	2.01 ± 0.18 / 6.53 ± 0.63	22.07 ± 4.30 / 21.02 ± 4.92
N-BEATS	3.33 ± 0.21 / 34.29 ± 2.25	18.32 ± 3.45 / 16.59 ± 3.71
Simple Feedforward	1.57 ± 0.07 / 14.12 ± 0.63	12.32 ± 4.24 / 9.45 ± 4.16
TFT	4.66 ± 0.31 / 44.08 ± 3.06	8.70 ± 4.71 / 6.75 ± 4.61
Constrained Ensemble (1 ms)	2.29 土 0.16	13.77 ± 5.01
Constrained Ensemble (5 ms)	11.99 ± 1.09	9.45 ± 5.20
Constrained Ensemble (10 ms)	24.20 ± 2.18	8.30 ± 4.97
Constrained Ensemble (50 ms)	107.68 ± 9.44	6.57 ± 5.04
Constrained Ensemble (100 ms)	157.75 ± 14.86	5.55 ± 4.28
Unconstrained Ensemble	196.41 ± 20.44	*4.36 ± 3.37
is given in Appendix A. The benchmark dataset that we release contains four additional forecast
accuracy metrics (MAPE, sMAPE, NRMSE, ND). Besides forecasting accuracy metrics, we store
inference latency, model size (i.e. number of parameters) and training time for deep learning mod-
els. Latency is measured by dividing the time taken to generate predictions for all test time series in
the dataset by their number.
3.5	Benchmark Analysis
Having outlined the benchmark, we now want to demonstrate the usefulness of the evaluations for
research. For this, we want to analyze how local (classical) and global (deep learning) forecast-
ing methods compare against each other. In fact, this comparison has been the object of heated
discussions in the forecasting community (Makridakis et al., 2018; Januschowski et al., 2020).
Method Comparison. In order to compare methods, we consider their latency and accuracy (in
terms of nCRPS) across all datasets. Table 1 shows each method’s relative latency compared to
Seasonal Naive as well as the the methods’ nCRPS rank1. As far as latency is concerned, global
methods, once trained, allow to generate forecasts considerably faster than local methods (except
for Seasonal Naive). The reason is simple: once trained, they simply need to run a forward pass. In
contrast, the implementations for the local methods chosen here do not differentiate between training
and inference, but rather estimate their parameters at prediction time. Across datasets, the relative
latency of all considered deep learning models improves between 15% and 95% upon the latency
of the fastest local method (excluding Seasonal Naive). As far as accuracy is concerned, complex
deep learning models — namely DeepAR and TFT — generally perform best across the benchmark
datasets. Except for MQ-RNN, global methods compare favorably against local methods.
As a measure of relative model stability, Table 1 additionally shows the standard deviation of the
nCRPS rank across all benchmark datasets. Not only is TFT the method which performs best on
average, its rank is also comparatively consistent compared to other methods. Especially,
1Ranks are computed over all methods and ensembles, including the constrained ensembles of Section 4.4.
4
Under review as a conference paper at ICLR 2022
0.8
0.6
0.2
0.0
∣o∙4
IO4
IO6
IO8
IO5
IO7
Number of Observations
Figure 1: The p-values of the null hypothesis
that statistical methods perform equal to or bet-
ter than deep learning methods, evaluated for
datasets of different sizes. The red line displays
the significance level α = 5%.
Figure 2: The relative improvement of the best
deep learning method versus the best classical
method on all benchmark datasets. The red line
is fitted on the visualized datapoints via linear re-
gression in the log-space.
Table 1 also indicates the performance of ensembles obtained by averaging the predictions of
n = 9 (n = 3 for MQ-RNN) deep learning models trained with different hyperparameters (hyper-
ensembles) as done for N-BEATS (Oreshkin et al., 2019) or in M5 competition’s third-ranking
method (Jeon & Seong, 2021). While generally bearing a significant cost in latency, the benefit
of ensembling is significant. For instance, the Simple Feedforward hyper-ensemble yields a com-
petitive model that outperforms DeepAR in terms of both accuracy and latency. Lastly, DeepAR and
TFT hyper-ensembles result in the most competitive ensembles.
Comparison of Classical and Deep Learning Methods. We further conduct a statistical analysis
to compare the classical time series models with the deep learning models listed in Section 3.2. For
this, we construct the null hypothesis that the accuracy of classical models is equal to or better than
the accuracy of deep learning methods. This is a claim put forward, for example, by Makridakis et al.
(2018). More formally, we write H0 : Qclass ≤ Qdeep where Qclass and Qdeep describe the distribution
of nCRPS values of classical and deep learning models, respectively. Samples of the distributions
are derived from the respective single-model evaluations in our benchmark. Qclass and Qdeep are
continuous and belong to unknown families of distributions with unequal variances. Hence, we
choose the nonparametric two-sample Kolmogorov-Smirnov test (Gibbons & Chakraborti, 2003) to
compute the p-value, i.e. the probability of H0 holding true, on each individual dataset.
We evaluate the null hypothesis H0 on all 44 benchmark datasets and plot the p-value for the dif-
ferent dataset sizes in Figure 1. At a significance level of α = 5%, H0 can be rejected for 30 out
of 44 datasets. In fact, deep learning models exhibit competitive performance for almost all datasets
and regularly outperform classical methods. Further, they tend to perform comparatively better with
increasing dataset size — even if for some large datasets, classical models are indistinguishable —
and, nonetheless, show competitive performance for small datasets. The latter is of particular inter-
est since it contradicts tribal knowledge that large datasets are needed to outperform local/classical
methods and disputes the claim presented by Makridakis et al. (2018) since only a few thousands
observations seem sufficient to outperform the classical models considered.
These statements are further supported by Figure 2 where we compare the best deep learning model
xdeep against the best classical model xclass on each dataset. We compute the relative improvement
given by deep learning models as nCRPS(：deeP) - 1. Except for few outliers, the best deep learning
model outperforms all classical models on 40 out of 44 datasets (see Appendix F for a further
analysis). Fitting a linear regression model on the relative improvements shows that the improvement
by using deep learning models is only slightly amplified as the datasets grow in size.
We note, that we conflate deep learning and global models in our above discussion. Montero-
Manso & Hyndman (2021) show the general superiority of global models over local models both
theoretically and empirically and our results confirm their findings further.
4	Learning Multi-Objective Defaults
The benchmark introduced in the previous section outlined how to acquire offline evaluations of
forecasting methods on various datasets. However, in the presence of multiple conflicting objectives
5
Under review as a conference paper at ICLR 2022
such as accuracy and latency, itis not clear how one could choose the best models. In this section, we
first formalize the problem of learning good defaults from these evaluations for a single objective.
Afterwards, we formally introduce multi-objective optimization and show how learning defaults can
be extended to account for multiple objectives. In the end, we report experimental results obtained
by using our method.
4.1	Prerequisites
In our setting, we consider an objective function f : X → Rm for a given task (in our case, a task
is a dataset). The objective function maps any time series model x ∈ X to m objectives (such as
nCRPS, latency, etc.) that ought to be minimized. In addition, we assume that model evaluations on
T different but related tasks with objective functions f(1), . . . , f(T) are available. The set of offline
model evaluations is then given as
TN
D = [ nxi(j), yi(j)o with yi(j) = f (j)(xi(j)) ∈ Rm	(2)
j=1	i=1
where Nj evaluations are available for task j and yi(j) denotes the evaluation of xi(j) on task j.
Learning Defaults. To learn a set of n good default models for unseen datasets in the single-
objective setting, Pfisterer et al. (2021) propose to pick the models that minimize the joint error ob-
tained when evaluating them on all datasets. This amount to pick the set of models {x1, . . . , xn} ⊂
X which minimize the following objective:
T
min	min f(j) (xi)
{x1,...,xn}⊂X	1≤i≤n k i
j=1
(3)
where k denotes a single fixed objective (for instance, the classification error). Since the problem
is NP-complete, a greedy heuristic with a provable approximation guarantee is used: it iteratively
adds the model minimizing Eq. 3 to the current selection. However, this approach only works for
a single objective of choice fk rather than taking all objectives f (such as accuracy and latency)
into account. In addition, it does not easily support the selection of ensembles since objectives can
interact in differently when models are ensembled: while accuracy will likely increase by a small
amount, the latencies of ensemble members need to be added up. Taking into account all possible
ensembles of a given size n would also blow up the combinatoric search when optimizing Eq. 3.
Multi-Objective Optimization. In multi-objective optimization, there generally exists no singular
solution. Thus, when considering the objective function f, there exists no single optimal model, but
rather a set of optimal models (Emmerich & Deutz, 2018). A model x ∈ X dominates another
model χ0 ∈ X (x Yf χ0) if f (x) ≤ f (χ0) for all i and there exists some i such that f (x) V f (x0).
The set of all non-dominated solutions (i.e. models) is denoted as the Pareto front Pf (X) whose
members are commonly referred to as Pareto-optimal solutions:
Pf (X) = {x ∈ X I —∃x0 ∈ X : x0 Yf x}	(4)
To quantify the quality ofa set S = {x1, . . . , xn} ⊂ X of selected models, we use the hypervolume
error (Zitzler & Thiele, 1998; Li & Yao, 2019). Given a set of points Y ⊂ Rm and a reference point
r ∈ Rm, we first define the hypervolume as the Lebesgue measure Λ of the dominated space:
H(Y) = Λ({q ∈Rm | ∃p∈ Y :p ≤ q∧q ≤ r})	(5)
In turn, the hypervolume error ε of the set S is defined as the difference in hypervolume compared
to the true Pareto front Pf (X):
ε(S) = H ({f(x) | x∈Pf(X)})-H({f(x1),...,f(xn))}) ≥0	(6)
Thus, the hypervolume error ε(S) reaches its minimum when S ⊇ Pf (X). To account for different
scales in the optimization objectives, we further standardize all objectives by quantile normalization
(Bolstad et al., 2003) such that they follow a uniform distribution U(0, 1) — this is feasible as X is
finite. While the normalization makes our comparisons robust to monotonic transformations of the
objectives (Binois et al., 2020), it also allows us to choose the reference point for Eq. 5 as r = 1m.
6
Under review as a conference paper at ICLR 2022
Learning Multi-Objective Defaults. Using the hypervolume error, we can extend the minimiza-
tion problem of Eq. 3 to learn defaults in the multi-objective setting. For this, we propose the
following minimization problem:
T
min	ε({x1,...,xn})	(7)
{x1,...,xn}⊂X
j=1
In words, we seek to find a set of complementary model configurations {x1, . . . , xn} ⊂ X that
provide a good approximation of the Pareto front Pf(j) (X) across tasks j ∈ {1, . . . , T }.
4.2 ParetoSelect
We now introduce ParetoSelect which tackles the minimization of Eq. 7. On a high-level,
∙-v
PARETOSELECT works as follows: first, it fits a parametric surrogate model fθ that predicts the
performances of model configurations by leveraging the evaluations ofD. Then, it uses the surrogate
∙-v
fθ to estimate the objectives for all models in X and applies the non-dominated sorting algorithm
on the objectives to select a list of default models.
∙-v
The surrogate model fθ is trained by attempting to correctly rank model configurations within each
∙-v
task in the available offline evaluations D. For each model x ∈ X, fθ outputs a vector y ∈ Rm for
m optimization objectives. Models can then be ranked for each objective independently. In order to
∙-v
fit fθ on the data D, we use listwise ranking (Xia et al., 2008) and apply linear discounting to focus
on correctly identifying the top configurations similar to Zugner et al. (2020). The corresponding
loss function can be found in Appendix D.
∙-v
For the parametric surrogate model fθ, we choose a simple MLP similar to Salinas et al. (2020b).
We use two hidden layers of size 32 with LeakyReLU activations after each hidden layer and ap-
ply a weight decay of 0.01 as regularization. Importantly, we do not tune these hyperparameters.
Predictive performances for every model can eventually be computed as
∙-v	∙-v
U = {fθ(X) I X ∈X}	(8)
We note that for minimizing Eq. 7, either regression or ranking objectives can be used: since we
apply quantile normalization to compute the hypervolume, the magnitude of the values predicted by
∙-v
fθ is irrelevant. However, we find the ranking setting to be superior to regression for finding good
default models (see Appendix D for more details).
Multi-Objective Sorting. The “best” configurations are then determined by applying the non-
dominated sorting algorithm (NDSA) — an extension of sorting to the multi-dimensional setting
∙-v
(Srinivas & Deb, 1994; Emmerich & Deutz, 2018) — to the predictive performances U. Figure 3
provides an illustration of the sorting procedure. NDSA can be described as a two-level procedure.
First, it partitions the configurations to be sorted in multiple layers by iteratively computing the
Pareto fronts, then it applies a sorting function sort to rank the configurations in each layer:
X1 = sort (Pf (X)) , Xi +1 = sort 卜fθ (X ∖ Li XJ j	(9)
We choose sort to compute an -net (Clarkson, 2006) for which there exists a simple greedy algo-
rithm. When sorting a set X, the -net sorting operation yields an order sort(X) = [X1, . . . , X|X|].
The first element X1 is chosen randomly from X and Xi+1 is defined iteratively as the item which is
farthest from the current selection in terms of Euclidean distance, formally:
Xi+1 = argmaxx∈X min kf (X) - f (Xj)k2	(10)
j≤i
The final ordering is eventually obtained by concatenating the sorted layers. While previous work
followed different approaches for the sort operation in the NDSA algorithm (Srinivas & Deb,
1994; Deb et al., 2002), we leverage the -net since Salinas et al. (2021) highlighted its theoretical
guarantees and Schmucker et al. (2021) provided empirical evidence for its good performance.
The pseudo-code of ParetoSelect is given in Algorithm 1 in the appendix. In the case where
predictions of the surrogate are error-free, the recommendations of the algorithm can be guaranteed
to be perfect with zero hypervolume error.
7
Under review as a conference paper at ICLR 2022

Layer 1
Layer 2
Layer 3
Layer 4
162
8 12
Obj ective #2	Objective #2
2317
21 ∙
%
*
18
Objective #2	Objective #2
9-：
13 -。％
Figure 3: Illustration of non-dominated sorting. The layers show the partitioning of the data in
Pareto fronts. The numbers depict the overall rank by computing the -net within each layer.
0.9 -
0.8 -
J θ∙7 -
I 0.6 -
卷 0.5-
10∙4^
R 0.3 -
H
0.2 -
0.1 -
0.0
2
Single-Objective
ParetoSelect
Random
Model-Free
4	6	8
Number of Models
4×10-'
× ×
3 2
-----Pareto Front
M ARIMA
舞 ETS
/ NPTS
M Prophet
M Seasonal Naave
* STL-AR
嬲 Theta
舞 DeepAR
MQ-CNN
能 MQ-RNN
簿 N-BEATS
* Simple Feedforward
舞 TFT
★ ParetoSelect
10
IOT
10°	ιo1	ιo2
Latency (in ms)

Figure 4: Hypervolume error
when iteratively choosing mod-
els via different model selection
approaches. Errors are averaged
over all 44 benchmark datasets
and five random seeds.
Figure 5: Visualization of forecast latency and nCRPS for mod-
els trained and evaluated on the “M4 Yearly” dataset. Each color
describes a family of models with dots representing different hy-
perparameter configurations and training times (checkpoints for
deep learning models). Crosses show the default hyperparame-
ter configuration and the maximum training time for the dataset.
Proposition 1. Assume that n ≥ |Pf (X) | and for all x ∈ X, fθ (x) = f (x).	Then,
ε({x1, . . . , xn}) = 0 where x1, . . . , xn are the first n models selected by our method.
Proof. ByEq. 9, the first k recommendations ofPareto Select are such that {x 1,... ,xk } = Pf§ (X)
where k = ∣Pf (X) |. Since for all x ∈ X, fθ(x) = f(x), we get that Pf§ (X) = Pf (X).
It follows that for any k ≤ n, Pf (X) = {x1, . . . , xk} ⊂ {x1, . . . , xn} and consequently
ε({x1, . . . , xn}) = 0 since the hypervolume error of any set of points containing the Pareto front is
zero.	□
4.3	Results
In order to evaluate our approach, we leverage the data collected with the benchmark presented in
Section 3. For evaluation, we perform leave-one-out-cross-validation where we use each dataset one
after the other as the test dataset and estimate the parameters θ of our parametric surrogate model
fθ on the remaining 43 datasets. The multi-objective setting in the following experiments aims to
minimize latency as well as nCRPS.
In Figure 4, we report the average hypervolume obtained for ParetoSelect and several baselines,
averaged over all 44 test datasets. ParetoSelect picks the top elements of the non-dominated sort
on the surrogate’s predictive performances. As baselines, we consider a variant of ParetoSelect
which considers the nCRPS as the single objective (Single-Objective), the approach of Pfisterer
et al. (2021) which greedily picks models to minimize the joint nCRPS on the training data (Greedy
Model-Free), and random search over the entire model space of size |X | = 247 (Random). The
hypervolume obtained by PARETOSELECT with 10 default models is small (≈ 0.06) compared to
the best baseline that reaches ≈ 0.20. Further, random search requires a total of 27 models to be on
par with the hypervolume of only 10 models selected via our method.
8
Under review as a conference paper at ICLR 2022
+60%
§ +50%
§
2 +40%
f
W +30%
S +20%
U
S +ιo%
+0%
-----Pareto Front
Ensemble Pareto Front	×
•	Constrained Ensemble	×
☆	UneOnStrained Ensemble	×
All Models	×
DeepAR
MQ-CNN
N-BEATS
Simple Feedforward
TFT
-100%
-80%
-60%
-40%
-20%
Rel. Latency Improvement
0%	455%
Figure 6: Comparison of individual forecasting models, hyper-ensembles and latency-constrained
ensembles. The axes show the average relative latency and nCRPS compared to the unconstrained
ensemble. Results are averaged across all 44 benchmark datasets. Light crosses show default config-
urations of deep learning models, bold crosses show their corresponding hyper-ensembles. Classical
models are not colored as most can only be found (far) beyond the range of the plot.
Figure 5 depicts the top n = 10 recommendations produced by PARETOSELECT when predicting
default models on a single dataset. The default models cover the true Pareto front well: our method is
able to pick models with low latency, low nCRPS, and a good trade-off between these two objectives.
4.4	Multi-Objective Ensembling
As shown in Section 3.5, ensembling models yields significant gains in accuracy but comes at the
cost of considerable latency. Since a massively large ensemble model is of no practical use, it is
critical to be able to select an ensemble with a latency that is acceptable for an application. Thus,
we consider ensembles under different latency constraints.
For a latency constraint c, we build an ensemble with n ≤ 10 members x1, . . . , xn by iteratively
picking the model with best nCRPS such that the ensemble latency constrain c is still met. To gen-
erate ensemble predictions, we combine the forecasts of the base models x1 , . . . , xn by averaging
all 10-quantiles {0.1, . . . , 0.9} for each time series and time step independently. Albeit the perfor-
mances of the base models might differ significantly, our experiments showed that more compli-
cated, non-uniform weighting schemes yielded less accurate forecasts on average. This aligns with
current literature showing that simple averaging is often most effective (Petropoulos et al., 2020).
Figure 6 shows the performance of different latency-constrained ensembles, relative to the perfor-
mance of an unconstrained ensemble that picks the n = 10 models with lowest predicted nCRPS.
As expected, higher latency constraints c allows the ensemble to perform better. For c ≥ 100 ms, the
constrained ensemble recovers the Pareto front and outperforms all individual models x ∈ X as well
as ensembles. The small gap to the Pareto front for constraints c ≤ 50 ms can be attributed to multi-
ple effects. First, the nCRPS values predicted by the surrogate fθ are not optimal. Second, models
may not satisfy c on every dataset although they have a lower latency on average (e.g. the default
TFT configuration violates c = 2.5 ms on 17/44 datasets). Table 1 additionally reports performance
metrics of latency-constrained ensembles: for instance, the ensemble for c = 10 ms outperforms
all individual base models in terms of average nCRPS rank while the ensemble for c = 100 ms
outperforms all hyper-ensembles as well.
5	Conclusion
In this work, we have presented (i) a new benchmark for evaluating forecasting methods in terms
of multiple metrics and (ii) ParetoSelect, a new algorithm that can learn default model config-
urations in a multi-objective setting. In future work, we will consider applying those techniques to
other domains such as computer vision where offline evaluations of multiple datasets were recently
made available by Dong & Yang (2020). Additionally, future research may extend the presented
predictive surrogate model to the case of ensembles: this would enable ParetoSelect to be used
directly to select members for ensembles of any latency.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility
Public datasets and method implementations were used to ensure the reproducibility of our experi-
ments. In Appendix A, we provide all hyperparameters used for training the benchmark forecasting
models via GluonTS. The source and processing of each dataset as well as the procedure for gen-
erating data splits is detailed in Appendix B. We also provide details on the surrogate model used
and a comparison with other choices in Appendix D. The pseudocode of ParetoSelect is given
in Appendix E. As a source of truth, we share the entire code used to run the benchmark and its
evaluation with the submission. This code as well as the generated evaluations will be released with
the paper.
References
AEMO. Market Data NEMWeb, 2020. URL http://www.nemweb.com.au/.
Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan
Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper
Schulz, Lorenzo Stella, Ali Caner Turkmen, and Yuyang Wang. GlUonTS: Probabilistic and
Neural Time Series Modeling in Python. Journal of Machine Learning Research, 21(116):1-6,
2020.
Vassilis Assimakopoulos and Konstantinos Nikolopoulos. The Theta Model: A Decomposition
Approach to Forecasting. International Journal of Forecasting, 16(4):521-530, 2000.
George Athanasopoulos, Rob J Hyndman, Haiyan Song, and Doris C Wu. The tourism forecasting
competition. International Journal of Forecasting, 27(3):822-844, 2011.
Ron Bekkerman, Hang Zhang, and Jeong-Yoon Lee. KDD Cup 2018, 2018. URL https://www.
kdd.org/kdd2018/kdd-cup.
Mickael Binois, Victor Picheny, Patrick Taillandier, and Abderrahmane HabbaL The kalai-
smorodinsky solution for many-objective bayesian optimization. J. Mach. Learn. Res., 21(150):
1-42, 2020.
Benjamin M Bolstad, Rafael A Irizarry, Magnus Astrand, and Terence P. Speed. A comparison of
normalization methods for high density oligonucleotide array data based on variance and bias.
Bioinformatics, 19(2):185-193, 2003.
Caltrans. Caltrans performance measurement system, 2020. URL https://pems.dot.ca.
gov/.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. MXNet: A Flexible and Efficient Machine Learning Library
for Heterogeneous Distributed Systems. arXiv preprint arXiv:1512.01274, 2015.
City of Melbourne. Pedestrian counting system - monthly (counts per hour),
2017.	URL https://data.melbourne.vic.gov.au/Transport/
Pedestrian- Counting- System- Monthly- counts- per- hour/b2ak- trbp.
Kenneth L Clarkson. Nearest-Neighbor Searching and Metric Space Dimensions. Nearest-Neighbor
Methods for Learning and Vision: Theory and Practice, pp. 15-59, 2006.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A Fast and Elitist Multi-
objective Genetic Algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6(2):
182-197, 2002.
Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track
COVID-19 in real time. The Lancet Infectious Diseases, 20(5):533-534, 2020.
Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture
search, 2020.
10
Under review as a conference paper at ICLR 2022
Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. URL http://archive.
ics.uci.edu/ml.
Michael TM Emmerich and Andre H Deutz. A Tutorial on MUltiobjective Optimization: Funda-
mentals and Evolutionary Methods. Natural Computing,17(3):585-609, 2018.
Corporacidn Favorita. Corporacidn Favorita Grocery Sales Forecasting, 2017. URL https://
www.kaggle.com/c/favorita-grocery-sales-forecasting.
Andrew Flowers and Reuben Fischer-Baum. Uber TLC FOIL Response, 2015. URL https:
//github.com/fivethirtyeight/uber-tlc-foil-response.
Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas,
Valentin Flunkert, and Tim Januschowski. Probabilistic Forecasting with Spline Quantile Func-
tion RNNs. In International Conference on Artificial Intelligence and Statistics, pp. 1901-1910,
2019.
Jean Dickinson Gibbons and Subhabrata Chakraborti. Nonparametric Statistical Inference. Marcel
Dekker, 2003.
Tilmann Gneiting and Adrian E Raftery. Strictly Proper Scoring Rules, Prediction, and Estimation.
Journal of the American Statistical Association, 102(477):359-378, 2007.
Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman, and Pablo Montero-
Manso. Monash Time Series Forecasting Archive. arXiv preprint arXiv:2105.06643, 2021.
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John El-
liot Karro, and D. Sculley (eds.). Google Vizier: A Service for Black-Box Op-
timization,	2017. URL http://www.kdd.org/kdd2017/papers/view/
google-vizier-a-service- for-black-box-optimization.
Recruit Holdings. Recruit Restaurant Visitor Forecasting, 2017. URL https://www.kaggle.
com/c/recruit-restaurant-visitor-forecasting.
Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated Machine Learning: Methods,
Systems, Challenges. Springer Nature, 2019.
Rob Hyndman, George Athanasopoulos, Christoph Bergmeir, Gabriel Caceres, Leanne Chhay,
Mitchell O’Hara-Wild, Fotios Petropoulos, Slava Razbash, Earo Wang, and Farah Yasmeen. fore-
cast: Forecasting functions for time series and linear models, 2021. URL https://pkg.
robjhyndman.com/forecast/. R package version 8.15.
Rob J Hyndman. expsmooth: Data Sets from "Forecasting with Exponential Smoothing", 2015.
URL https://cran.r-project.org/web/packages/expsmooth/.
Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice. OTexts, 2018.
James M. Kilts Center. Dominick’s Dataset, 2020. URL https://www.chicagobooth.edu/
research/kilts/datasets/dominicks.
Tim Januschowski, Jan Gasthaus, Yuyang Wang, David Salinas, Valentin Flunkert, Michael Bohlke-
Schneider, and Laurent Callot. Criteria for classifying forecasting methods. International Journal
of Forecasting, 36(1):167-177, 2020.
D. Jean-Michael. Smart meters in London, 2019. URL https://www.kaggle.com/
jeanmidev/smart-meters-in-london.
Yunho Jeon and Sihyeon Seong. Robust recurrent network model for intermittent time-series fore-
casting. International Journal of Forecasting, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Under review as a conference paper at ICLR 2022
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling Long- and Short-Term
Temporal Patterns with Deep Neural Networks. In International ACM SIGIR Conference on
Research & DeveloPment in Information Retrieval,pp. 95-104, 2018.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A Novel Bandit-Based Approach to Hyperparameter Optimization. Journal of Machine Learning
Research, 18(1):6765-6816, 2017.
Miqing Li and Xin Yao. Quality Evaluation of Solution Sets in Multiobjective Optimisation: A
Survey. ACM ComPuting Surveys, 52(2):1-38, 2019.
Bryan Lim, Sercan O Ank, Nicolas Loeff, and Tomas Pfister. Temporal Fusion Transformers for
Interpretable Multi-Horizon Time Series Forecasting. International Journal of Forecasting, 2021.
Tie-Yan Liu. Learning to Rank for Information Retrieval. Foundations and Trends in Information
Retrieval, 3(3):225331, 2009.
S Makridakis, E Spiliotis, and V Assimakopoulos. The M5 accuracy competition: Results, findings
and conclusions. International Journal of Forecasting, 2020a.
Spyros Makridakis and Michele Hibon. The M3-Competition: results, conclusions and implications.
International Journal of Forecasting, 16(4):451-476, 2000.
Spyros Makridakis, Allan Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf
Lewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The Accuracy of Extrap-
olation (lime Series) Methods: Results of a Forecasting Competit ion. Journal of Forecasting, 1
(2):111-153, 1982.
Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. Statistical and Machine
Learning Forecasting Methods: Concerns and Ways Forward. PLOS One, 13(3), 2018.
Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4 Competition:
100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36(1):
54-74, 2020b.
James E. Matheson and Robert L. Winkler. Scoring rules for continuous probability distributions.
Management Science, 22(10):1087-1096, 1976.
Michael W McCracken and Serena Ng. FRED-MD: A Monthly Database for Macroeconomic Re-
search. Journal of Business & Economic Statistics, 34(4):574-589, 2016.
Pablo Montero-Manso and Rob J Hyndman. Principles and Algorithms for Forecasting groups of
Time Series: Locality and Globality. International Journal of Forecasting, 2021.
NYC Taxi and Limousine Commission. TLC Trip Record Data, 2015. URL https://www1.
nyc.gov/site/tlc/about/tlc-trip-record-data.page.
Mitchell O’Hara-Wild, Rob Hyndman, and Earo Wang. tsibbledata: Diverse Datasets for ‘tsibble’,
2021. URL https://tsibbledata.tidyverts.org/.
Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural Basis
Expansion Analysis for Interpretable Time Series Forecasting. arXiv PrePrint arXiv:1905.10437,
2019.
Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied Babai, Devon K
Barrow, Christoph Bergmeir, Ricardo J Bessa, John E Boylan, Jethro Browell, Claudio Carnevale,
et al. Forecasting: Theory and Practice. arXiv PrePrint arXiv:2012.03854, 2020.
Florian Pfisterer, Jan N. van Rijn, Philipp Probst, Andreas C. Muller, and Bernd BiSchL Learning
Multiple Defaults for Machine Learning Algorithms. In Proceedings of the Genetic and Evolu-
tionary ComPutation Conference ComPanion, pp. 241-242, 2021.
Syama Sundar Rangapuram, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Bernie
Wang, and Tim Januschowski. Deep Non-Parametric Time Series Forecaster. to be Published,
2021.
12
Under review as a conference paper at ICLR 2022
Rossmann. Rossmann Store Sales, 2015. URL https://www.kaggle.com/c/
rossmann-store-sales.
David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus.
High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes. arXiv
preprint arXiv:1910.03002, 2019.
David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic
Forecasting with Autoregressive Recurrent Networks. International Journal of Forecasting, 36
(3):1181-1191,2020a.
David Salinas, Huibin Shen, and Valerio Perrone. A quantile-based approach for hyperparameter
transfer learning. In Hal DaUme In and Aarti Singh (eds.), Proceedings ofthe 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
8438-8448. PMLR, 13-18 Jul 2020b. URL https://Proceedings.mlr.ρress∕v119∕
salinas20a.html.
David Salinas, Valerio Perrone, Olivier Cruchant, and Cedric Archambeau. A Multi-Objective Per-
spective on Jointly Tuning Hardware and Hyperparameters. In ICLR Workshop on Neural Archi-
tecture Search, 2021.
Robin Schmucker, Michele Donini, Muhammad Bilal Zafar, David Salinas, and Cedric Archam-
beau. Multi-Objective Asynchronous Successive Halving. arXiv preprint arXiv:2106.12639,
2021.
Syed Yousaf Shah, Dhaval Patel, Long Vu, Xuan-Hong Dang, Bei Chen, Peter Kirchner, Horst
Samulowitz, David Wood, Gregory Bramble, Wesley M Gifford, et al. AutoAI-TS: AutoAI for
Time Series Forecasting. arXiv preprint arXiv:2102.12347, 2021.
David Simchi-Levi, Xin Chen, and Julien Bramel. The Logic of Logistics: Theory, Algorithms, and
Applications for Logistics Management. Springer, 2013.
Slawek Smyl. A hybrid method of Exponential Smoothing and Recurrent Neural Networks for time
series forecasting. International Journal of Forecasting, 36(1):75-85, 2020.
Adam Sparks. bomrang: Australian Government Bureau of Meteorology (BOM) Data
Client, 2021. URL https://www.rdocumentation.org/packages/bomrang/
versions/0.7.4.
Nidamarthi Srinivas and Kalyanmoy Deb. Multiobjective Optimization Using Nondominated Sort-
ing in Genetic Algorithms. Evolutionary Computation, 2(3):221-248, 1994.
Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Information-
theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transac-
tions on Information Theory, 58(5):32503265, May 2012. ISSN 1557-9654. doi: 10.1109/tit.
2011.2182033. URL http://dx.doi.org/10.1109/TIT.2011.2182033.
Martin Stepnicka and Michal Burda. On the Results and Observations of the Time Series Forecasting
Competition CIF 2016. In IEEE International Conference on Fuzzy Systems, pp. 1-6, 2017.
Souhaib Ben Taieb, Gianluca Bontempi, Amir F Atiya, and Antti Sorjamaa. A review and com-
parison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting
competition. Expert Systems with Applications, 39(8):7067-7083, 2012.
Thiyanga Talagala. STL-AR Method, 2021. URL https://rdrr.io/cran/seer/man/
stlar.html.
Sean J Taylor and Benjamin Letham. Forecasting at Scale. The American Statistician, 72(1):37-45,
2018.
Walmart. Walmart Recruiting - Store Sales Forecasting, 2014. URL https://www.kaggle.
com/c/walmart-recruiting-store-sales-forecasting.
13
Under review as a conference paper at ICLR 2022
Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A Multi-Horizon
Quantile Recurrent Forecaster. arXiv preprint arXiv:1711.11053, 2017.
Fela Winkelmolen, Nikita Ivkin, H Furkan Bozkurt, and Zohar Karnin. Practical and sample efficient
zero-shot HPO. arXiv preprint arXiv:2007.13382, 2020.
Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimiza-
tion initializations. In 2015 IEEE International Conference on Data Science and Advanced Ana-
Iytics(DSAA),pp.1-10,2015. doi:10.1109/DSAA.2015.7344817.
Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise Approach to Learning to
Rank: Theory and Algorithm. In International Conference on Machine learning, pp. 1192-1199,
2008.
Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, and Frank Hutter. Nas-
bench-101: Towards reproducible neural architecture search, 2019.
Yingchen Zhang. Solar Power Data for Integration Studies, 2006. URL https://www.nrel.
gov/grid/solar-power-data.html.
Eckart Zitzler and Lothar Thiele. Multiobjective Optimization Using Evolutionary Algorithms — A
Comparative Case Study. In International Conference on Parallel Problem Solving from Nature,
pp. 292-301, 1998.
Daniel Zugner, Oliver Borchert, Amir Akbarnejad, and Stephan Gunnemann. Adversarial attacks
on graph neural networks: Perturbations and their patterns. ACM Transactions on Knowledge
Discovery from Data, 14(5):1-31, 2020.
14
Under review as a conference paper at ICLR 2022
Table 2: Overview of all models considered and their associated hyperparameters. The upper seven models do not require training. The lower six models are deep learning models. For all deep learning			
models, three context lengths are considered as well	as three hyperparameter configurations (except		
for MQ-RNN) that result in small, medium and large models, respectively. Hyperparameters that are			
not augmented keep their default values. The hyperparameters of the		“default” models correspond	
to the medium size.			
	Context Lengths	Hyperparameters	
ARIMA (Hyndman & Athanasopoulos, 2018)	—	—	
ETS (Hyndman & Athanasopoulos, 2018)	—	—	
NPTS (Rangapuram et al., 2021)	—	—	
Prophet (Taylor & Letham, 2018)	—	—	
Seasonal Naive (Hyndman & Athanasopoulos, 2018)	—	—	
STL-AR (Talagala, 2021)	—	—	
Theta (Assimakopoulos & Nikolopoulos, 2000)	—	—	
		# Layers	# Cells
DeepAR (Salinas et al., 2020a)	1,2,4	1	20
		2	40
		4	80
		# Filters	Kernel Sizes
MQ-CNN (Wen et al., 2017)	2, 4, 8	20	3, 3, 2
		30	7, 3, 3
		40	14,7,3
MQ-RNN (Wen et al., 2017)	2, 4, 8	—	
		# Stacks	# Blocks
N-BEATS (Oreshkin et al., 2019)	1,2,4	4	5
		30	1
		30	2
		Hidden Dim	# Layers
Simple Feedforward	1,2,4	30	1
		40	2
		80	3
		Hidden Dim	# Heads
Temporal Fusion Transformer (Lim et al., 2021)	1,2,4	16	2
		32	4
		64	8
A Models
Table 2 provides an overview over all benchmark models along with hyperparameters considered.
In addition to three hyperparameter settings per deep learning model (except for MQ-RNN), three
different context lengths l are considered. These represent multiples of the forecast horizon τ and are
thus dataset-dependent. Eventually, this results in 9 model configurations per deep learning model
(and 3 for MQ-RNN).
The implementation of all models are taken from GluonTS (Alexandrov et al., 2020). All deep
learning models are implemented in MXNet (Chen et al., 2015). ARIMA, ETS, STL-AR, and Theta
are available in GluonTS as well but forward computations to the R forecast package (Hyndman
et al., 2021).
Model Evaluation. To measure the accuracy of forecasting models, we employ the continuous
ranked probability score (CRPS) (Matheson & Winkler, 1976; Gneiting & Raftery, 2007). Given
the quantile function (Ft(i))-1 of the forecast probability distribution in Eq. (1) for a time series z(i)
15
Under review as a conference paper at ICLR 2022
at time step t, the CRPS is defined as
CRPS(Ft(i))-1,zt(i) =Z01Λα (Ft(i))-1,zt(i) dα	(11)
where Λα is the quantile loss (or pinball loss) defined over a quantile level 0 < α < 1:
Λα (F-1 ,z) = (α - I[Z < F-1(α)]) (Z - F-1(α))	(12)
Here, I is the indicator function. To compute an aggregated score for a model forecasting multiple
time series and time steps, we compute a normalized CRPS (nCRPS):
We approximate the nCRPS using the 10-quantiles α ∈ {0.1, 0.2, 0.3, . . . , 0.9}. While we focus
on nCRPS for the evaluations in this paper, the benchmark dataset that we release contains four
additional forecast accuracy metrics (MAPE, sMAPE, NRMSE, ND).
B	Datasets
Our benchmark provides 44 datasets which were obtained from multiple sources. 16 datasets were
obtained though GluonTS (Alexandrov et al., 2020), 24 datasets are taken from the Monash Time
Series Forecasting Repository (Godahewa et al., 2021), and the remaining 4 datasets were originally
published as part of Kaggle2 forecasting competitions. Table 3 provides basic statistics about all
datasets.
B.1 Dataset Descriptions
In the following, we provide brief descriptions of all datasets and provide their sources. We link
the original source (if possible) along with any work which pre-processed the data (if applicable).
The datasets “Corporacidn Favorita"，“Restaurant"，“Rossmann"，and “Walmart" 一 which were
obtained from Kaggle — are processed by ourselves.
Australian Electricity Demand (O’Hara-Wild et al.， 2021; Godahewa et al.， 2021) provides the 30-
minute electricity demand of five Australian states (New South Wales， Queensland， South Australia，
Tasmania， Victoria). Time series range from January 2002 to April 2015.
Bitcoin (Godahewa et al.， 2021) provides over a dozen of potential influencers of the price of Bitcoin.
Among others， these influencers are daily hash rate， block size， or mining difficulty. We exclude one
time series from the original dataset as its absolute values are ≥ 1018. Data is provided from January
2009 to July 2021.
CIF 2016 (Stepnicka & Burda, 2017; GodaheWa et al., 2021) is the dataset from the “Computational
Intelligence in Forecasting" competition in 2016. One third of the time series originate from the
banking sector while the remaining two thirds are generated artificially. In the competition， the time
series have two different prediction horizons， however， GluonTS forces us to adopt only a single
horizon (for which we choose the most common one).
COVID Deaths (Dong et al.， 2020; Godahewa et al.， 2021) provides the daily COVID-19 death
counts of various countries between January 22 and August 21 in the year 2020.
Car Parts (Hyndman， 2015; Godahewa et al.， 2021) provides intermittent time series of car parts
sold monthly by aUS car company between January 1998 and April 2002. Periods in which no parts
are sold have a value of zero.
Corporacion Favorita (Favorita, 2017) contains the daily unit sales of over 4,000 items at 54 differ-
ent stores of the supermarket company Corporacion Favorita in Ecuador. Unit sales were manually
set to 0 on days where there was no data available. Data ranges from January 2013 to August 2017.
All covariates that are provided in the Kaggle competition are discarded.
2https://www.kaggle.com/
16
Under review as a conference paper at ICLR 2022
Table 3: Statistics of the benchmark datasets. Frequencies read as follows: MIN - minutely, H -
hourly, D - daily, B - business daily, W - weekly, M - monthly, Q - quarterly, Y - yearly. Number
of observations are calculated from the training data.
Freq. Horizon I # Series Avg. Length # Observations
Australian Electricity Demand 30 MIN
Bitcoin	D
CIF 2016	M
COVID Deaths	D
Car Parts	M
Corporacidn Favorita	D
Dominick	W
Electricity	H
Exchange Rate	B
Fred-MD	M
Hospital	M
KDD 2018	H
London Smart Meters	30 MIN
48
30
12
30
12
16
8
24
30
12
12
48
48
M1 Monthly
M1 Quarterly
M1 Yearly
M3 Monthly
M3 Other
M3 Quarterly
M3 Yearly
M4 Daily
M4 Hourly
M4 Monthly
M4 Quarterly
M4 Weekly
M4 Yearly
M5
NN5
Pedestrian Count
Restaurant
Rideshare
Rossmann
San Francisco Traffic
MQYMQQYDHMQWYDDHDHDHH
18861888614481881362856
Solar
Taxi	30 MIN
Temperature Rain	D
Tourism Monthly	M
Tourism Quarterly	Q
Tourism Yearly	Y
Vehicle Trips	D
Walmart	W
Weather	D
Wiki	D
Wind Farms	MIN
24
30
24
8
4
30
39
30
30
60
5	231,005	1,155,024
17	4,134	70,279
72	87	6,244
227	182	41,314
2,504	39	97,656
171,091	1,089	186,310,518
115,163	157	18,137,043
321	21,044	6,755,124
8	6,071	48,568
107	716	76,612
767	72	55,224
270	10,850	2,929,404
5,559	29,903	166,231,248
617	73	44,892
203	41	8,320
181	19	3,429
1,428	99	141,858
174	69	11,933
756	41	30,956
645	22	14,449
4,227	2,357	9,964,658
414	854	353,500
48,000	216	10,382,411
24,000	92	2,214,108
359	1,022	366,912
22,974	31	707,265
30,490	1,885	57,473,650
111	735	81,585
66	47,412	3,129,178
823	321	263,817
2,304	493	1,135,872
1,115	864	963,689
862	17,496	15,081,552
137	7,009	960,233
1,214	1,488	1,806,432
32,053	695	22,276,835
366	275	100,496
427	92	39,128
518	21	10,685
314	100	31,361
2,934	102	298,509
3,010	14,266	42,941,700
9,535	762	7,265,670
313	513,442	160,707,330
Dominick (James M. Kilts Center, 2020; Godahewa et al., 2021) contains time series with the
weekly profits of numerous stock keeping units (SKUs). The data is obtained from the grocery
store company Dominick’s over a period of seven years.
Electricity (Dua & Graff, 2017; Alexandrov et al., 2020) is comprised of the hourly electricity
consumption (in kWh) of hundreds of households between January 2012 and June 2014.
Exchange Rate (Lai et al., 2018; Alexandrov et al., 2020) provides the daily exchange rates (on
weekdays) between the US dollar and the currencies of eight countries (Australia, Great Britain,
Canada, Switzerland, China, Japan, New Zealand, Singapore) in the period from 1990 to 2013.
17
Under review as a conference paper at ICLR 2022
Fred-MD (McCracken & Ng, 2016; Godahewa et al., 2021) contains monthly time series of macro-
economic indicators (e.g. interest rates, employment rates, . . . ) from the Federal Reserve Bank
since January 1959 (until September 2019).
Hospital (Hyndman, 2015; Godahewa et al., 2021) provides the monthly patient counts for various
products that are related to medical problems between 2000 and 2007.
KDD 2018 (Bekkerman et al., 2018; Godahewa et al., 2021) is the dataset used by the KDD Cup
2018. It contains hourly time series with air quality levels (represented by various chemical com-
pounds) measured by stations in Beijing and London between January 2017 and April 2018.
London Smart Meters (Jean-Michael, 2019; Godahewa et al., 2021) was originally published
on Kaggle and contains the half-hourly (electrical) energy consumption (in kWh) of thousands of
households in the period from November 2011 to February 2014.
M1 (Makridakis et al., 1982; Godahewa et al., 2021) datasets are taken from the M1 competition in
1982. Time series have varying start- and end dates and cover a wide semantic spectrum of domains
(demographics, micro, macro, industry).
M3 (Makridakis & Hibon, 2000; Alexandrov et al., 2020) datasets are obtained from the M3 com-
petition. Across frequencies, time series were collected from six different domains: demographics,
micro, macro, industry, finance, and other. For the time series with no specified frequency, we
assume a quarterly frequency (as this is the default in GluonTS).
M4 (Makridakis et al., 2020b; Alexandrov et al., 2020) datasets are taken from the prestigious M4
competition. The time series are collected from the same domains as in the M3 competition.
M5 (Makridakis et al., 2020a; Alexandrov et al., 2020) is the dataset from the M5 competition and
contains daily unit sales of 3,049 products across 10 Walmart stores located in California, Texas,
and Wisconsin. While covariates are available (e.g. product prices or special events), none of the
forecasting methods we consider makes use of them. Sales data is provided from January 2011 to
April 2016.
NN5 (Taieb et al., 2012; Godahewa et al., 2021) data was used in the NN5 competition run in 2008.
The time series provide daily amounts of cash withdrawals at dozens of ATMs in the UK from March
1996 to May 1998.
Pedestrian Count (City of Melbourne, 2017; Godahewa et al., 2021) provides hourly pedestrian
counts in Melbourne from May 2009 to May 2020. The counts are captured by sensors scattered in
the city.
Restaurant (Holdings, 2017) provides the number of daily visitors of hundreds of restaurants in
Japan between January 2016 and April 2017, as tracked by the AirREGI system. As dates for which
restaurants are closed do not provide visitor numbers, we impute zeros for these missing values.
The Kaggle competition where this dataset is obtained from also provides data about reservations
but this data remains unused for our purposes.
Rideshare (Godahewa et al., 2021) is comprised of hourly time series representing various attributes
related to services offered by Uber in Lyft in New York. Time series include e.g. minimum, max-
imum, and average price, or the maximum distance traveled — grouped by starting location and
provider. Data is available for the period of a month, starting in late November 2018.
Rossmann (Rossmann, 2015) provides daily sales counts at hundreds of different stores of the Ross-
mann drug store chain. Just like for the M5 dataset, covariates are available (e.g. distance to com-
petition, special events, or discounts) but not used by any method. Data is available from January
2013 until August 2015.
San Francisco Traffic (Caltrans, 2020; Godahewa et al., 2021) contains hourly occupancy rates of
freeways in the San Francisco Bay area. Data is available for two full years, starting from January
1, 2015.
Solar (Zhang, 2006; Alexandrov et al., 2020) is comprised of the hourly power consumption of
dozens of photovoltaic power stations in the state of Alabama in the year 2006.
Taxi (NYC Taxi and Limousine Commission, 2015; Salinas et al., 2019; Alexandrov et al., 2020)
contains the number of taxi rides in hundreds of locations around New York in 30 minute windows.
18
Under review as a conference paper at ICLR 2022
Training data contains data from January 2015 while test data contains data from January in the
subsequent year.
Temperature Rain (Godahewa et al., 2021) provides the daily temperature observations and rain
forecasts gather from 422 weather stations across Australia. Data ranges from May 2015 through
April 2017.
Tourism (Athanasopoulos et al., 2011; Godahewa et al., 2021) datasets come from a Kaggle fore-
casting competition where time series were supplied by tourism bodies from Australia, Hong Kong,
and New Zealand as well as academics. Monthly data ranges from 1979 through 2007, quarterly
data from from 1975 through 2007, and yearly data from 1960 through 2008.
Vehicle Trips (Flowers & Fischer-Baum, 2015; Godahewa et al., 2021) contains the daily number
of trips served by hundreds of for-hire vehicle (FHV) companies (such as Uber). Data ranges from
January to September 2015.
Walmart (Walmart, 2014) contains data about Walmart store sales similarly to the M5 competition.
Data is provided as the weekly sales (in USD) across 45 stores and 81 departments in different
regions and ranges from February 2010 to November 2012. Just like for many of the other Kaggle
competitions, covariates are available, yet unused.
Weather (Sparks, 2021; Godahewa et al., 2021) is comprised of daily data collected from hundreds
of weather stations in Australia. Collected data is the amount of rain, the minimum and maximum
temperature, and the amount of solar radiation.
Wiki (Gasthaus et al., 2019; Alexandrov et al., 2020) similarly provides daily pages views for several
thousand Wikipedia pages in the period from January 2012 to July 2014.
Wind Farms (AEMO, 2020; Godahewa et al., 2021) contains time series with the minutely power
production of hundreds of wind farms in Australia. Data is available for a period of one year, starting
in August 2019.
B.2 Data Preparation
For all of the datasets in Table 3 except for “Taxi”, we perform similar preprocessing. Let
Z = {z1(i:)Ti }iK=1 be the set of K univariate time series of lengths Ti . Then, we run the follow-
ing preprocessing steps:
1.	Remove all time series which are constant to always be able to compute the MASE metric.
Z 1 = {z (i) ∈Z I -∃c ∈ R : Z (i) = C ∙ 1 Ti}	(14)
2.	Remove all time series which are too short. We exclude all time series of length
T ≤ (p + 1)τ where τ is the prediction horizon and p is a dataset-specific integer which
governs over how many prediction lengths we want to perform testing3. We are using
p = 1 for all datasets but “Electricity” (p = 7), “Exchange Rate” (p = 3), “Solar” (p = 4),
“Traffic” (p = 7) and “Wiki” (p = 3).
Z2 = {z(i) ∈ Z1 | Ti > (p+ 1)τ}	(15)
3.	Split time series into training, validation and test data. For each time series, we split off the
prediction horizon p - 1 times to obtain time series for the test data. Then, we split off the
prediction horizon another time to obtain a validation time series per training time series.
Eventually, we construct the following sets:
Ztest = [ n {Z(：Ti , ..., z(:) Ti-(P-1) T) } | N (i) ∈ Z20	(16)
ZVal= nZ(.(Ti-PT) I Z(() ∈Z20	(17)
Ztrain = nzi：)Ti-(p +1)T) | N(() ∈ Z20	(18)
3Assume p = 2 and a time series z ∈ RT from a dataset with prediction horizon τ . Then, we perform
testing on time series z(1) = z1:T and z(2) = z1:(T -τ) while z(3) = z1:(T -2τ) is being trained on.
19
Under review as a conference paper at ICLR 2022
For the “Taxi” dataset, we need to slightly augment the preprocessing to allow for discontinuities in
the available time series. Essentially, each time series z ∈ Z consists of an old part zold and a new
part znew (data throughout January of two successive years). We then obtain the training data from
zold and construct the validation data by cutting the prediction horizon from zold. The test dataset is
constructed from znew as in bullet point 3 with p = 56.
C Training Details
This section describes in detail how the deep learning models from Table 2 are trained on the datasets
outlined in the previous section. As outlined in Section 3.2, parametric local methods directly es-
timate their parameters at training time. While deep learning models are trained on Ztrain such that
models at different checkpoints can be compared using Zval, parameters of parametric local methods
are estimated using Zval.
C.1 Training Time
For all deep learning models, we run training for a predefined duration depending on the dataset
size. For n total observations in the training data, we run training for d hours where
d =2η With η = min {max {[log 10 (ɪo^)] , 0} , 3}.	(19)
Training therefore always runs between one and eight hours (refer to Table 3 for training times on
individual datasets). Even on small datasets, We train for an hour to ensure that the trained model
receives sufficiently many gradient updates.
C.2 Probabilistic Forecasts
Forecasts of all methods are required to be probabilistic to store the 10-quantiles q0.1, q0.2, . . . , q0.9.
While some methods forecast the quantiles directly (e.g. MQ-CNN, TFT), other methods provide
samples of the output distribution (e.g. DeepAR) or point forecasts (e.g. N-BEATS). For sampling-
based models, We simply compute the empirical 10-quantiles. For the point forecasts, We interpret
forecasts as Dirac distributions centered around the forecasted value y. Thus, each quantile can be
set to the value y.
C.3 Learning Rate Scheduling
All deep learning models are trained using the Adam optimizer (Kingma & Ba, 2014) With an initial
learning rate of ρ = 10-3. During training, We decrease the learning rate three times by a factor of
λ = 2 using a linear schedule. When training for a duration d, We decrease the learning rate after
durations D = {d, d,苧}. This results in a final learning rate of λ3P = 1.25 ∙ 10—4. The decay is
alWays applied after the first batch exceeding one of the durations in D.
C.4 Validation and Testing
During training, We save the model and compute its loss L as Well as the nCRPS Q on the validation
data for a total of 11 times. Fora model being trained for a duration t, We perform these computations
after durations D = H∪ {k9 ∣ k ∈ {1,…,9}} with intervals I = {t ∙ 3-k | k ∈ {0,..., 4}}. For
each d ∈ D, We run validation on Zval after the first batch Where the total training time exceeds d
and compute the losses Ld and Qd.
Note that We do not add the time taken to run validation to the training time to evaluate Whether it
exceeds d. We argue that validation can be run sWiftly. If necessary, it could also be run on a subset
of all available time series for further speedup.
For testing, We choose 5 of the 11 models that We saved during training. For each d ∈ I, We
choose the model With the loWest nCRPS that Was encountered up to d. Note that this may result
in choosing the same model multiple times if continued training did not yield any improvements on
the validation data.
20
Under review as a conference paper at ICLR 2022
C.5 Infrastructure and Training Statistics
All training jobs were scheduled on AWS Sagemaker4 using CPU-only ml.c5.2xlarge instances
(8 CPUs, 16 GiB memory). In few cases, we ran jobs on other instance types:
•	ml.m5.2xlarge instances (8 CPUs, 32 GiB memory) to counteract out-of-memory-
issues. Used for N-BEATS on “Corporacidn Favorita” and “Weather"，DeePAR on “Cor-
Poraci6n Favorita”, and Prophet on “Wind Farms”.
•	ml.m5.4xlarge instances (16 CPUs， 64 GiB memory) to allow for even more memory-
intensive models. Used for N-BEATS on “London Smart Meters”.
•	ml.c5.18xlarge instances (72 CPUs, 144 GiB memory) to sPeed uP Predictions by
Parallelization. Used for ARIMA on “Taxi”.
In total, we ran 4,708 training jobs. Including validation and testing, this amounted to roughly
684 days of total training time (〜3 hours and 30 minutes per training job). Parallelized over 200
instances, actual training time for our benchmark was roughly 4 days and amounts to aPProximately
$7,300 of compute costs on AWS.
D Surrogate Model
In the following, we discuss the choice of the surrogate model. First, we outline why a parametric
surrogate model is desirable. Then, we describe how model configurations are vectorized to be used
by parametric surrogate models. Afterwards, we list the ranking loss with linear discounting that
we mention in Section 4.2 and eventually compare the ranking performance of different surrogate
models.
D. 1 Choice of Surrogate Model
Intuitively, a nonparametric surrogate model might appear to be optimal for ranking models. How-
ever, a parametric surrogate has several advantages. First, it can interpolate between neighboring
configurations, which is essential when the configurations differ between different datasets or when
the objective is noisy. Second, a parametric model allows for suggesting new configurations that
∙-v
e.g. maximize capacity under a given constraint. Third, a parametric surrogate model fθ may also
be conditioned on the dataset by taking as input dataset features (such as number of time series or
∙-v
observations in the task at hand) by concatenating features to the input vector of fθ — although we
do not consider this possibility here.
D.2 Input Vectorization
When using XGBoost or an MLP as surrogate model for predicting model performance, models must
be represented as feature vectors. For this, we proceed as follows to encode model configurations:
•	The model type (i.e. ARIMA, DeepAR, . . . ) is encoded via a one-hot encoding, yielding a
13-dimensional vector.
•	Every model hyperparameter (across models) defines a new real-valued feature. Hyper-
parameters that are shared among deep learning models (context length multiple, training
time, learning rate5) are re-used across models. This results in 15 additional features which
are all standardized to have zero mean and unit variance.
Inputs to the MLP are, thus, 28-dimensional. Features that are missing (e.g. all classical models do
not provide a context length multiple) are imputed by the features’ means (resulting in zeros due to
the standardization).
4https://aws.amazon.com/sagemaker/
5Although never altered, this provides a potential hyperparameter.
21
Under review as a conference paper at ICLR 2022
D.3 Ranking Loss with Linear Discounting
∙-v
Section 4.2 outlines that the MLP surrogate model fθ is trained via listwise ranking using linear
∙-v
discounting. For training on the offline evaluations D, fθ minimizes the loss L via SGD. As ranking
is performed for each objective k ∈ {1, . . . , m} and task j ∈ {1, . . . , T } independently, L can be
written as follows:
mT
L (fθ)= ΣΣLkj (fθ ； k)	(20)
k=1 j=1
∙-v	∙-v
where fθ；k yields the surrogate model's outputs for objective k and Lkj (fθ；k) is defined as follows
(where we ignore indices k and j in the formula for notational convenience):
N1	N
Lkj (fθ ； k) = X if (Xr (i)) + log X exp (-fθ (Xr (i))) )	(21)
i=1	l=i
Here, N provides the number of available evaluations for task j and r(i) defines the index of the
configuration X that is ranked at position i with respect to objective k among all configurations for
task j. The ranks of the configurations X1, . . . , XN with respect to an objective k can be computed
from the corresponding evaluations y1;k, . . . , yN;k. Note that rank i = 1 provides the index for the
∙-v
“best” configuration and fθ outputs a smaller value for configurations that have a smaller rank (i.e.
are “better”).
D.4 Comparison of Different Models
The following compares the MLP surrogate trained via listwise ranking introduced in Section 4 to
other possible choices for the surrogate model. Much like the approach described in Section 4.3,
we evaluate surrogate models by performing LOOCV: surrogates are evaluated on each of the 44
benchmark datasets one after the other, being trained on the remaining 43 datasets. Surrogates are
evaluated with respect to different ranking metrics that give an overview of the models’ ability to
rank the nCRPS values of model configurations on different datasets. As metrics for measuring the
ranking performance on a single dataset, we consider the following which yield metrics between 0
and 1:
•	Mean Reciprocal Rank (MRR) (Liu, 2009): Computed as one divided by the predicted
rank of the model with the lowest true nCRPS.
•	Precision@k (Liu, 2009): The fraction of the k models with the lowest true nCRPS cap-
tured in the top k predictions.
•	Normalized Discounted Cumulative Gain (NDCG) (Liu, 2009): A measure for the over-
all ranking performance. It first computes the discounted cumulative gain (DCG) by sum-
ming the discounted relevance scores 1, 0.9, . . . , 0.1 of the 10 models with the lowest true
nCRPS scores — more specifically, a relevance score π(i) is discounted by log2 k + 1
where k is the predicted rank of the model with true rank i. Then, it normalizes the DCG
by using the ideal DCG (iDCG) to obtain the NDCG.
As a comparison to the ranking MLP surrogate with linear discount (MLP Ranking + Discounting),
we first consider a random surrogate which predicts nCRPS by sampling fromU(0, 1) as a baseline
(Random). Then, we use a nonparametric model which predicts the nCRPS as the average among
all training datasets (Nonparametric Regression) and one that predicts the average rank of the
nCRPS across training datasets (Nonparametric Ranking). Additionally, we consider XGBoost
surrogates which are trained via regression (XGBoost Regression) and pairwise ranking6 (XGBoost
Ranking), respectively. Lastly, we consider an MLP with the same architecture but trained via
regression instead of listwise ranking (MLP Regression) and one without any discounting (MLP
Ranking).
Table 4 clearly shows that the MLP with listwise ranking and linear discounting markedly outper-
forms all other choices for the surrogate model. Especially, it is much more capable of identifying
6XGBoost does not provide a working version of listwise ranking.
22
Under review as a conference paper at ICLR 2022
Table 4: Comparison of different surrogate models with respect to their ability to rank model config-
urations according to their nCRPS. Ranking metrics are averaged across all benchmark datasets and
multiplied by 100. Metrics for non-deterministic surrogates (random and MLP) are further averaged
by running the entire evaluation over five random seeds. Best values for each metric are displayed
in bold.
	MRR	NDCG	Precision@5	Precision@10	Precision@20
Random	2.17	25.36	2.45	4.68	8.27
Nonparametric Regression	6.88	35.80	13.18	20.91	32.73
Nonparametric Ranking	6.78	35.87	10.00	20.91	33.07
XGBoost Regression	5.68	35.34	13.18	22.05	32.61
XGBoost Ranking	6.64	35.63	12.27	21.59	32.84
MLP Regression	3.86	32.06	7.73	13.73	25.23
MLP Ranking	5.00	34.59	10.27	21.00	33.34
MLP Ranking + Discounting	16.41	43.74	23.82	28.32	34.39
the top configurations, stressed by the very high values of MRR and Precision@5 compared to the
other surrogate models. This can likely be attributed to the linear discounting that encourages the
model to focus on ranking the top configurations correctly. Thus, the loss formulation is more stable
in the presence of outliers.
E ParetoSelect pseudo code
Algorithm 1: Overview of PARETOSELECT
Data: Time series models X, offline evaluations D, number of default models n
Result: A set {x1, . . . , xn} ⊂ X of model defaults
function non_dominated_sort(X, fθ):
Λ= []
while X = 0 do
PX fθ = {x ∈ X | -∃x0 ∈ X : X Yf x}	// Compute the Pareto front
X = X \ PX fθ	// Remove Pareto front
∕~v
extend (A, Compute_epsilon_net (PX f§, fθ))
return A
function Compute_epsilon_net(X, fθ):
λ = [pop(X)]	// Choose and remove a random element
while X 6= 0 do
D = {}	// Mapping from remaining elements to minimum distanCes
for x ∈ X do	// Compute minimum distanCe to all Chosen elements
L∕~v	∕~v
D[x] = minx∈λ ∣∣jΓθ(X) — fθ(X)∣∣2
// Append the element furthest away from the Current seleCtion
append (λ, pop (X, argmaxx∈χ D[x]))
return λ
fθ =train_surrogate_model(D)
// Multi-objeCtive sorting aCCording to prediCtions
x1 , . . . , x|X| = non_dominated_sort (X, fθ)
returnx1,. . . , xmin(n,|X|)
23
Under review as a conference paper at ICLR 2022
F	Analysis of datasets characteristics
In this section, we try to answer the following question: are there some simple dataset characteristics
that can help to know which model is going to perform best? In particular, we analyze in details the
results of Section 3.5 where we showed that, on some datasets, (1) statistical methods outperformed
deep learning methods and that (2) the performance of statistical and deep learning methods could
not be distinguished.
We start by analyzing more in depth the four out 44 datasets where deep learning models do
not outperform classical methods — Figure 2 showed that the best deep learning method outper-
forms the best classical methods on all but four of our benchmark datasets. On these four datasets
(“KDD2018”, “M3 Yearly”, “M4 Quarterly”, “Taxi”), the summary statistics differ wildly as can
be seen from Table 3 both in terms of the number of observations, the average length, and other
characteristics. Further, the classical method that outperforms the best deep learning method is dif-
ferent for each of these datasets (“KDD2018”: NPTS, “M3 Yearly”: Theta, “M4 Quarterly”: ETS,
“Taxi”: STL-AR). Notably, this does not include the local method that performs best on average (i.e.
ARIMA, see Table 1).
Figure 1 showed that for 14 out of the 44 benchmark datasets, the performance of classical methods
is indistinguishable from the performance of deep learning methods. To further study these datasets,
we plot in Figure 7 the correlation matrix of all k benchmark methods’ nCRPS ranks across the
datasets (using only the default configuration for the deep learning models). Notably, for twelve
datasets (“Exchange Rate” through “M1 Yearly” on the y-axis), we observe a very low correlation
with a large number of datasets. On these datasets, all models perform similarly due to strong
stochasticity (e.g. “Exchange Rate”, “Bitcoin”, “M1 Quarterly”, ...) or seasonality (e.g. “Tourism”,
“Ride share”, ...) and predicting their ranking is therefore hard. Notably, 11 of these datasets overlap
with the 14 datasets where classical methods are indistinguishable from deep learning methods.
In line with these observations, we found that adding simple dataset grouping features such as do-
main type (electricity, retail, finance) as an input for a surrogate model predicting the performance
of forecasting methods does not have much effect. Our conclusion is that the ranks of the best
performing models seem to be relatively hard to identify only from simple dataset grouping rules.
Nonetheless, we hope that by releasing the benchmark data, we help future research to identify
dominant methods from dataset characteristics.
24
Under review as a conference paper at ICLR 2022
■
0.5
-0.0
TburismYearly
WndFanns
M4 Daily
M4 %arly
Tburism Quarterly
M3 Other
M4 Weekly
M3%arly
M3 Monthly
M3 Quarterly
M4 Monthly
M4 Quarterly
Exchange Rate
Hospital
Tburism Monthly
Bitcoiii
Fred-MD
Ml Monthly
CIF 2016
Rideshare
ævm Deaths
M4 Hourly
Ml Quarterly
Ml %arly
Weather
CarParts
Dominick
San FranCis∞ TrafiBc
Solar
NN5
Vehicle T⅛ips
Restaurant
Rossmann
Corporacidn Favorita
London. Smart Meters
TemperatureRain.
Pedestrian Count
Wki
Australian Electricity Demand
Electricity
I⅛xi
MS
KDD 2018
Walmart
≡sαs
SW
XJP8
⅛CSHUQ XJJ9μpŋeɪɪeJtSnV
P≡
“moɔ σeμιsspad
aπqwdαι⅝L
FoAeH spejodjoɔ
SSSnnQU
一σejnss^
sdμj, ul⅛
SNN
-WloS
09sσeJH αα∞
ɔpɪɑɪɑIoQ
sped 3
uu≡3⅛
APB¾UW
ΛSI0 «s
AlJnoH 3
≡⅛3Q0≥8
SJeqSSPa
90
XiQS «s
i⅛E⅛
ŋɪoəjIH
OlSjmO
ZIdSOH
0sαeq9x
ΛSI03
XiQSl
XiQSS
ApBSfAEW
A≡⅛⅛wi
UBSS
APSPmlð HB⅛5H
ApBSfAWi
Xn
≡ps⅛⅛l∙
Figure 7: Correlation matrix of all benchmark methods’ nCRPS ranks across all benchmark datasets.
For the computation of the ranks, all classical methods and the default configuration of deep learning
models are considered.
25