Under review as a conference paper at ICLR 2022
Improving Fairness via federated learning
Anonymous authors
Paper under double-blind review
Ab stract
Recently, lots of algorithms have been proposed for learning a fair classifier from
centralized data. However, how to privately train a fair classifier on decentralized
data has not been fully studied yet. In this work, we first propose a new theoretical
framework, with which we analyze the value of federated learning in improving
fairness. Our analysis reveals that federated learning can strictly boost model
fairness compared with all non-federated algorithms. We then theoretically and
empirically show that the performance tradeoff of FedAvg-based fair learning
algorithms is strictly worse than that of a fair classifier trained on centralized
data. To resolve this, we propose FedFB, a private fair learning algorithm on
decentralized data with a modified FedAvg protocol. Our extensive experimental
results show that FedFB significantly outperforms existing approaches, sometimes
achieving a similar tradeoff as the one trained on centralized data.
1 Introduction
As machine learning is now used to make critical decisions that affect human life, culture, and rights,
fair learning has recently received increasing attention. Various fairness notions have been introduced
in the past few years (Dwork et al., 2012; Hardt et al., 2016; Zafar et al., 2017b;a; Kearns et al.,
2018; Friedler et al., 2016). Among various fairness notions, group fairness is the most studied
one (Hardt et al., 2016; Zafar et al., 2017a). Group fairness requires the classifier to treat different
groups similarly, where groups are defined with respect to sensitive attributes such as gender and race.
One of the most commonly used group fairness notions is demographic parity, which requires that
different groups are equally likely to receive desirable outcomes.
There has been a large amount of work in training fair classifiers (Zafar et al., 2017c; Hardt et al.,
2016; Roh et al., 2021), and almost all of these studies assume that the learner has access to the entire
training data. Unfortunately, this is not the case in many critical applications. To see this, consider a
scenario where multiple data owners (e.g. courts or financial institutions) have their own private data.
Even if they are willing to coordinate with the other institutions to obtain a single model that works
well on the combined data, they cannot directly share their data with the others due to the privacy
act. This precisely sets the core question We aim to answer in this paper - how can we privately
train a fair classifier on decentralized data? To answer this, we first study three existing approaches:
Unfederated Fair Learning (UFL), Federated Fair Learning via FedAvg (FFL via FedAvg), and
Centralized Fair Learning (CFL). See Fig. 1 for illustration.
Unfederated Fair Learning (UFL) and Centralized Fair Learning (CFL) UFL is the most
naive yet most private approach. As the name indicates, this strategy refers to a scenario where
multiple data owners simply decide to not coordinate. Instead, each of them learns a fair model on
its local data to serve its own users. This approach is completely private as the participating data
owners share nothing with the others. However, the overall performance of UFL is expected to be
poor, because each data owner may have a highly biased view of the entire data distribution, making
their locally trained classifiers fair only on a biased subset of the data, but not on the entire data.
To evaluate the performance of this approach, we consider the randomized classifier that makes
a prediction using a randomly chosen local classifier. Note that this can be viewed as a random
customer model, i.e., a user drawn from the overall data distribution picks and visits one of the
institutions, uniformly at random. Another extreme approach is CFL, where a fair model is trained on
the pooled data. We expect CFL to achieve the best performance tradeoff, at the cost of no privacy.
Federated Fair Learning via FedAvg (FFL via FedAvg) FFL via FEDAVG applies federated
learning (Konecny et al., 2017) together with existing fair learning algorithms. Federated learning is
1
Under review as a conference paper at ICLR 2022
-<-------------------------------------more .private -----------------------------------------------
------------------------------------------fairer ---------------------------------------------------*
Figure 1: A high-level illustration of various approaches to fair learning on decentralized data and our
contributions. Assuming two data owners, from left to right, we show UFL (Unfederated Fair Learning), FFL
via FedAvg (Federated Fair Learning via FedAvg), FedFB (ours), and CFL (Centralized Fair Learning).
In UFL, each data owner trains a locally fair model on its own data, and the customer picks one of them
at random. FFL via FedAvg applies FedAvg together with off-the-shelf fair training algorithms for local
training. Our proposed solution FedFB consists of a modified FedAvg protocol with a custom-designed fair
learning algorithm. CFL is the setting where a fair model is trained on the pooled data. In this work, we
theoretically characterize the strict ordering between the existing approaches and empirically demonstrate the
superior performance of FedFB.
a distributed learning framework, using which many data owners can collaboratively train a model
under the orchestration of a central server while keeping their data decentralized. For instance, under
FedAvg, the standard aggregation protocol for federated learning, the central server periodically
computes a weighted average of the locally trained model parameters. If each data owner runs a fair
learning algorithm on its own data and these locally trained models are aggregated via FedAvg, then
one might hope to obtain a model that is accurate and fair on the overall data distribution. We call
this approach Federated Fair Learning via FEDAVG (FFL via FEDAVG).
Goal and Main Contributions The performances of these approaches have not been rigorously
analyzed in the literature. In the first place, it has been unknown whether there is any strict perfor-
mance gap between UFL and FFL via FedAvg. This makes it unclear whether or not federated
learning is necessary at all for decentralized fair learning. The performance comparison between FFL
via FedAvg and CFL also remains unclear. Can FFL via FedAvg always match the performance
of CFL? If not, can we develop a better federated learning approach for decentralized fair learning?
Inspired by these open questions, this work rigorously analyzes the performance of the existing
approaches and proposes a new solution to decentralized fair learning. Our major contributions can
be summarized as follows:
•	We develop a theoretical framework for analyzing various approaches for decentralized fair learning.
Using this, we prove the strict ordering between the existing approaches, i.e., under some mild
conditions, UFL < FFL via FEDAVG < CFL, w.r.t. their fairness-accuracy tradeoffs.
•	Improving upon the state-of-the-art algorithm for (centralized) fair learning (Roh et al., 2021), we
design FedFB, a novel approach to learning fair classifiers via federated learning.
•	Via extensive experiments, we show that (1) our theoretical findings hold under more general
settings, and (2) FedFB significantly outperforms the existing approaches on various datasets and
achieves similar performance as CFL.
To the best of our knowledge, our work is the first theoretical performance comparison of various
approaches to fair learning on decentralized data. Moreover, it characterizes the necessity of federated
learning for improved fairness-accuracy tradeoff, and we expect this to expedite the adoption of feder-
ated learning-based approaches. Our proposed solution FedFB achieves state-of-the-art performance
on many datasets, sometimes achieving a similar tradeoff as the one trained on centralized data.
2	Related work
Model Fairness Among various algorithms for fair training (Zemel et al., 2013; Jiang & Nachum,
2020; Zafar et al., 2017c;a; Hardt et al., 2016; Roh et al., 2021; 2020), the current state-of-the-art is
FairBatch (Roh et al., 2021), which reweights the samples by solving a bi-level optimization problem,
whose inner optimizer is the standard training algorithm and outer optimizer aims to find the best
weights attached to groups of samples for the sake of model fairness.
Federated Learning Unlike traditional, centralized machine learning approaches, federated learning
keeps the data decentralized throughout training, reducing the privacy risks involved in traditional
approaches (Konecny et al., 2017; McMahan et al., 2017). FEDAVG (McMahan et al., 2017) is the
2
Under review as a conference paper at ICLR 2022
Privacy
∙UFL
•FFL via FedAvg FedFB
AgnosticFair
■
______________________CFL-
ACCUracy-fairness performance
Figure 2: Compar-
ison of various fair
learning methods..
first and most widely used federated learning algorithm. The idea is to iteratively compute a weighted
average of the local model parameters, with the weights proportional to the local datasets’ sizes. Prior
work (Li et al., 2020b) has shown that FedAvg provably converges under some mild conditions. The
design of our proposed algorithm FedFB is also based on that of FedAvg.
Federated Fair Learning for Client Parity There have been only a few attempts in achieving
fairness under the federated setting. Moreover, the definition of “fairness” used in the existing
federated learning work is slightly different from the standard notion used in the centralized setting.
One popular definition of fairness in the federated setting is that all clients (i.e. data owners) achieve
similar accuracies (or loss values), which we call client parity, and several algorithms have been
proposed to achieve this goal (Li et al., 2021; 2020a; Mohri et al., 2019; Yue et al., 2021; Zhang et al.,
2020a). To compare our methods with existing federated fair learning algorithms designed for client
parity, we also extend our FedFB such that it can also achieve client parity instead of the standard
notion of group fairness. In Sec. 5, we will show that FedFB can achieve as good client parity as the
existing algorithms, though FedFB is not specifically designed for client parity.
Federated Fair Learning for Group Fairness A few very recent stud-
ies (Ezzeldin et al., 2021; RodrIgUez-Galvez et al., 2021; ChU et al., 2021;
Du et al., 2021; Cui et al., 2021), conducted concurrently with our work, also
aim at achieving groUp fairness Under federated learning. In particUlar, DU
et al. (2021), RodrIgUez-Galvez et al. (2021) and Chu et al. (2021) mimic
the centralized fair learning setting by exchanging information for each local
Update. In contrast, oUr FedFB reqUires mUch fewer commUnication roUnds,
ensUring higher privacy and lower commUnication costs. Simlar to FedFB,
Ezzeldin et al. (2021) employs FedAvg and a reweighting mechanism to
achieve groUp fairness. However, FairFed only applies to the case with one single binary sensitive
attribute, while RodrIgUez-GalVez et al. (2021) and Chu et al. (2021) are not applicable to demo-
graphic parity. Therefore, we sUmmarize the comparison of UFL, FFL via FedAvg, CFL, FedFB
and AgnosticFair (Du et al., 2021) in terms of performance and privacy in Fig. 2. There is also
work that aims at achieving local fairness for each data owner (Cui et al., 2021). This is in contrast to
our work, which instead focuses on achieving global fairness in the overall data distribution. Our
setting is more appropriate in domains such as criminal justice and social welfare.
3	Performance Analysis of UFL, FFL via FedAvg and CFL
In Sec. 3.1, we first show the necessity of federation by proving that FFL via FedAvg can achieve
strictly higher fairness than UFL. We then prove the limitation of FFL via FedAvg by comparing its
performance with an oracle bound of CFL in Sec. 3.2. These two results together imply that federated
learning is necessary, but there exists a limit on what can be achieved by FedAvg-based approaches.
We will present informal theoretical statements, deferring the formal versions and proofs to Sec. A.
Problem Setting Denote [N] := {0, 1, . . . , N - 1} for any N ∈ Z+. We assume I clients, which
have the same amount of data. We further assume a simple binary classification setting with a
binary sensitive attribute, i.e., x ∈ X = R is the input feature, y ∈ Y = [1] is the outcome and
a ∈ A = [A] = [1] is the binary sensitive attribute. Assume x is a continuous variable. The
algorithm we will develop later in Sec. 4 will be applicable to general settings.
We now introduce parameters for describing the data distribution. Let y | X 〜Bern(η(x)) for all
client i, where η(∙) : X → [0,1] is a strictly monotone increasing function. Assume X | a = a, i =
i 〜Pai), a | i = i 〜 Bern(qi), where i is the index of the client, Pa(i) is a distribution, and qi ∈ [0, 1]
for a = 0, 1, i ∈ [I]. Let F = {f : X × A → [0, 1]}. Given f ∈ F and data sample (X, a), we
consider the following randomized classifier: ^ | x, a 〜Bern(f (x, a)).
Using these definitions, we now define demographic parity, specialized for a binary sensitive attribute:
Definition 1 (Demographic Parity (binary cases)). P(^ = 1 | a = 0) = P(^ = 1 | a = 1).
To measure how unfair a classifier is with respect to demographic parity (DP), we measure DP
disparity, i.e. the absolute difference between the two positive prediction rates:
DP Disp(f ) = ∣P(^ =1 | a = 0) — P(^=1 | a= 1)|.
3
Under review as a conference paper at ICLR 2022
Figure 3: Fundamental limitation of UFL in terms of fairness range. We visualize the DP disparity of UFL
as a function of local unfairness budgets on three simple Gaussian distributions. The blue horizontal plane is
of perfect fairness. The pink horizontal plane visualizes the value of δ (the lowest DP disparity that UFL can
achieve). (a) (Example 1) On a distribution that satisfies the conditions of Corollary 2. (b, c) On distributions
that do not satisfy the conditions of Corollary 2. In all three cases, the UFL cannot achieve perfect fairness, i.e.,
δ > 0.
3.1	Necessity of Federation: FFL via FedAvg is strictly better than UFL
UFL Optimization We now present the optimization problem solved in the UFL scenario. Here,
for analytical tractability, we will assume the population limit, i.e., the true data distribution is used in
optimization. Recall that clients do not coordinate in this scenario, and each of them solves their own
optimization problem to train a locally fair classifier. In particular, each client i ∈ [I] first sets its
own local fairness constraint εi ∈ [0, 1], and solves the following constrained optimization problem:
minP(^ = y | i = i), s.t. ∣P(^ =1 | a = 0, i = i) - P(^=1 | a=1, i = i)| ≤ ε八 (UFL(i,εi))
f∈F
We denote by fiεi the solution to UFL(i, εi). Recall that the overall performance of UFL is defined as
the performance of the following mixture of I classifiers:
y | x, a 〜Bern(fεi(x, a)), w∙P∙ 1/I = Bern ( X fεi(x, a)/l) = Bern(fUFL),
i∈[I]
where fεUFL := Pi∈[I] fiεi (x, a)/I, with ε = (ε0, . . . ,εI-1).
The question here is whether the resulting classifier fεUFL obtained by UFL can achieve an arbitrary
level of fairness. The following lemma shows that UFL cannot achieve a high enough fairness level
beyond a certain threshold.
Lemma 1 ((Informal) Achievable fairness range of UFL). Let qi = q ∈ (0, 1) for all i ∈
[I]. Under certain conditions, there exists a certain DP disparity threshold δ > 0 such that
minε∈[0,1]I DPDisp(fεUFL) > δ.
A critical condition of Lemma 1 is that the distribution of insensitive attribute x | a is highly
heterogeneous on different clients. (See the detailed statements in Sec. A.2.2 and Sec. A.4.) Therefore,
Lemma 1 implies that UFL fails to achieve strict fairness requirements even without data heterogeneity
on the distribution of sensitive attribute a. The following corollary provides an example satisfying the
conditions of Lemma 1.
Corollary 2 (Informal). Let I = 2 and qo = qι = 0.5, η(x) = ɪ+I-X, Pai) = N(μ!”,σS)2), where
i = 0, 1. Under certain assumptions, if one client has much larger variance than the other, there
exists δ > 0 such that minε0,ε1∈[0,1] DP Disp(fεUFL) > δ.
Note that the condition that one client has a much larger variance than the other contributes to the
“high data heterogeneity” requirement. We provide the explicit form of δ in Corollary 9 in Sec. A.2.2.
Corollary 2 implies that under a limiting case of Gaussian distribution, UFL cannot achieve high
fairness requirements. In Sec. 5.1, we will numerically demonstrate the same claim holds for more
general cases. Next, we give a specific example that satisfies the conditions of Corollary 2, which is
visualized in Fig. 3(a).
Example 1. Let μ00) = μ10) = 0, μ01) = 3, μf) = —1, σ ⑼=70 and σ ⑴=L Then, δ ≈ 0.21.
FFL via FedAvg Optimization FFL via FEDAVG enables federated training of a fair classifier
on decentralized data. In Sec. A.3.2, we show that the classifier obtained by FFL via FedAvg is
equivalent to fεFFL via FedAvg, the solution to the following constrained optimization problem.
fmF P(V = y)
(FFL via FEDAVG(ε))
s.t.	∣P(y=1 | a = 0, i = i) — P(^=1 | a = 1, i = i) | ≤ ε%.
The following theorem asserts that FFL via FEDAVG can achieve a strictly higher fairness than UFL.
4
Under review as a conference paper at ICLR 2022
higher data heterogeneity
Qo 0.5, g,ι = 0.5 Qq = 0.4, q↑ = 0.6	% = 0.3, q↑ = 0.7 q0 = 0.2,3=0.8
0.85
0.80
0.75
Figure 4: Accuracy-fairness tradeoff curves of CFL, FFL via FedAvg, and UFL for two clients cases.
Here qi denotes the proportion of group 1 in client i ∈ {0, 1}, so |q1 - q0 | captures the data heterogeneity of
the distribution. The green dotted vertical line describes the lower bound on DP disparity FFL via FedAvg can
achieve, and the orange dotted vertical line describes the lower bound on DP disparity UFL can achieve. As
predicted in Thm. 3 and Lemma 4, FFL via FedAvg’s maximum fairness is strictly higher than that of UFL but
strictly lower than that of CFL. Moreover, the tradeoff curves are strictly ordered in the same order.
Theorem 3 ((Informal) Fundamental values of federated learning for fairness). Let qi =
q ∈ (0, 1) for all i ∈ [I]. Under certain conditions, minε∈[0,1]I DP Disp(fεUFL) >
minε∈[0,1]I DP Disp(fεFFL via FedAvg) =0.
Similar to Lemma 1, the technical assumptions include high enough heterogeneity of x | a across
the clients. Thm. 3 shows that even with just two clients (I = 2), a non-trivial gap exists between
non-federated algorithms and federated algorithms in their fairness performances. More details are
provided in Sec. A.4. The theorem asserts that, under certain distributional assumptions, by using the
optimal local fairness budgets εi , FFL via FEDAVG can achieve perfect fairness, while UFL cannot.
Remark 1. Extending Thm. 3 to general cases where qi are not all the same remains open. In
particular, the analysis of FFL via FEDAVG for those cases remains open. However, we conjecture
that our lemma holds for more general cases, and we numerically support our conjecture in Sec. 5.
Remark 2. There is a stark difference between this phenomenon and the well-known gain of federated
learning due to an increased sample size, which is almost negligible with a few number of clients.
Our finding on this untapped gain in fairness can better support the need for federated learning even
between a small number of clients, which is the case for most cross-silo federated learning scenarios.
3.2	FFL via FedAvg is strictly worse than CFL
While we showed that FFL via FedAvg can achieve perfect fairness on certain distributions, it is
still unclear whether or not this is the case for every distribution. In this section, we first present the
optimization problem for CFL, whose achievable fairness region can serve as an upper bound on that
of all other federated learning algorithms. We then show the existence of data distributions on which
FFL via FedAvg achieves a strictly worse fairness performance than CFL. This implies a strict gap
between the performance tradeoff of FFL via FedAvg and that of CFL.
CFL Optimization We consider the same problem setting as Sec. 3.1. We now model the CFL
scenario as the following constrained optimization problem:
minP(^ = y), s.t. |P(y = 1 | a = 0) — P(^ = 1 | a = 1)| ≤ ε.
f∈F
(CFL(ε))
Denote the solution to CFL(ε) as fεCFL. It is clear that fεCFL achieves the best accuracy-fairness
tradeoff, at the cost of no privacy. The following lemma shows that there exists some distribution
such that FFL via FEDAVG is strictly worse than CFL when the distribution of sensitive attribute a is
heterogeneous (qi are not all the same).
Lemma 4 ((Informal) A strict gap between FFL via FEDAVG and CFL). When there exist i 6=
j ∈ [I] s.t. qi 6= qj, there exist a distribution such that minε∈[0,1]I DP Disp(fεFFL via FedAvg) >
minε DP Disp(fεCFL ) = 0.
Remark 3. A strict gap exists for certain distributions, but not for all distributions.
3.3	Numerical Comparisons of Accuracy-fairness tradeoffs
One limitation of our current theoretical results is that they only compare the maximum achievable
fairness. Note that such analysis reveals how the tradeoff of accuracy and fairness behaves as the
fairness level increases, but it fails at fully characterizing the entire tradeoff curve. Extending our
theoretical results to fully characterize such tradeoffs is highly non-trivial, so we leave it as future
5
Under review as a conference paper at ICLR 2022
work. Instead, we numerically solve each of the optimization problems and visualize the tradeoff
curves achieved by different algorithms.
Shown in Fig. 4 are the tradeoff curves for two clients cases. Let X | a = 0, i = 0 〜N(3,1),x|
a = 1, i = 0 〜N(5,1), x | a = 0, i = 1 〜N(1,1), X | a = 1, i = 1 〜N( —1,1), a | i = 0 〜
Bern(qo), a | i = 1 〜Bern(qι), η(x)=[+；-., and vary the values of qo and qι. Note that ∣qι - qo|
captures the heterogeneity of the sensitive data a, which increases from left to right. First, one can
observe that UFL<FFL via FEDAVG< CFL in terms of the achievable fairness range, as predicted by
our theory. Furthermore, we also observe an increasing gap between the tradeoff curves as the data
heterogeneity increases. Theoretical understanding of this phenomenon remains open.
4	FedFB for Improved Federated Fair Learning
Our findings in the previous section imply that federated learning is necessary, but the current
FedAvg-based approach might not be the best approach. Can we design a federated learning
algorithm that is strictly better than FedAvg-based approaches? In this section, we propose a new
federated learning algorithm for fair learning, which we dub FedFB (short for Federated FairBatch).
Our approach is based on the state-of-the-art (centralized) fair learning algorithm FB (short for
FairBatch) (Roh et al., 2021) and has a few desirable theoretical guarantees. Later in Sec. 5, we
empirically show that FedFB outperforms FFL via FedAvg and closely matches the CFL’s tradeoff
on various datasets.
Algorithm 1: FEDFB algorithm
ClientUpdate(i, w, λ):
Update w(i) according to the sample
weights λ;
Lyi,a J P(i,y,a) = (i,y,a) '@ Y； W),
∀(y, a);
Send w(i), L(yi,)a(w) for all (y, a) to
_ server via a SecAgg protocol;
ServerExecutes:
for each iteration do
Clients perform updates;
w J SecAgg({w(i)});
Ly,a J SecAgg({L(yi,)a}), ∀(y, a);
λ J Update(λ, Ly,a);
Broadcast W and λ to clients;
output: W
it decreases the weights for the samples whose y
whose γ
1,a
1 so that after retraining, P(^ = 1 |
increases. And vice versa for the other case.
We first provide a brief review of the FB algorithm.
FB solves a bi-level optimization problem to learn
a fair classifier on centralized data. The inner opti-
mization problem solves a weighted empirical risk
minimization problem where samples from differ-
ent groups are reweighted by different weights. The
outer optimization problem optimizes the weights
used for the inner problem, with the goal of min-
imizing the unfairness of the classifier. FB works
for various group fairness definitions including de-
mographic parity, equalized odds, and equalized
opportunity. For the case of demographic parity,
the algorithm reduces to the following simple yet
intuitive algorithm. The algorithm starts with equal
weights for two different groups. After training a
model with the initial weights, it computes the sign
of the difference between the two positive predic-
tion rates P(^ = 1 | a = 0) - P(γ = 1 | a = 1).
If this quantity is zero, then DP Disp is zero, so the
sample weights are not updated. If this is positive,
1, a = 0 and increases the weights for the samples
a = 0) decreases and P(^ = 1 | a = 1)
FedFB is a simple modification of the original FB, which closely simulates the centralized FB
applied to the entire data. Recall that under the FedAvg protocol, clients periodically share their
locally trained model parameters with the server. Our modification is based on the following simple
observation: the bi-level structure of FB naturally fits the hierarchical structure of federated learning.
More specifically, note that if the clients also share their group-specific positive prediction rates, then
the centralized server can immediately reconstruct the difference between the two positive prediction
rates, measured on the entire data distribution. Therefore, the update rules for the outer optimization
of the original FB algorithm can be implemented at the central server given the extra information.
Then, the central server can broadcast the updated group weights with the clients, which can then
locally train models with the newly reweighted samples.
This precisely describes the essence of FedFB, and shown in Alg. 1 is the pseudocode of the
FEDFB framework. Note that the update rule for group weights (denoted by λ in the pseudocode)
only requires the sum of the group losses, which enables secure aggregation. In Sec. B.1, we
present the detailed description of the algorithm, which consists of the local training algorithm with
6
Under review as a conference paper at ICLR 2022
Table 1: Comparison of accuracy and DP disparity on the synthetic, Adult, COMPAS, and Bank datasets.
FedFB significantly outperforms the other approaches on all the tested datasets, sometimes nearly matching the
performance of CFL. Note that FFL via FedAvg sometimes gets a strictly worse performance than FedAvg.
This can be explained by noting that the average of two fair models may not be fair at all.
Synthetic	Adult	COMPAS	Bank
Method	ACC.(↑)	DP DISP.(φ)	ACC.(↑)	DP Disp(J)	ACC.(↑)	DP DISP.(J)	ACC.(↑)	D P DISP. (J)
FedAvg	.886±.003	.406±.009	.829±.012	.153±.022	.655±.009	.167±.037	.898±.001	.026±.003
UFL	.727±.194	.248±.194	.825±.008	.034±.028	.620±.019	.088±.055	.892±.002	.014±.006
FFL via FedAvg	.823±.102	.305±.131	.801±.043	.123±.071	.595±.005	.059±.009	.893±.000	.017±.001
FedFB (Ours)	.613±.007	.011±.009	.765±.001	.001±.001	.542±.001	.001±.001	.883±.000	.000±.000
CFL	.726±.009	.028±.016	.816±.010	.045±.024	.616±.033	.036±.028	.883±.000	.000±.000
reweighted samples, the model/loss aggregation protocol, the group-weight update algorithm, and
the model/group-weight distribution protocol. We highlight a few advantages of FedFB. First, it
provably converges under some mild technical conditions. We proved it by leveraging the analysis
tools for federated learning and FB - See Thm. 23 for more details. Second, our algorithm is a strict
improvement of FB even in centralized data cases. The original FB algorithm was not applicable if
the sensitive attributes are not binary. We made appropriate changes to the algorithm (with theoretical
guarantees) so that it can also handle more general cases. Thus, we use our version of FB by default
for fair learning in the rest of this paper.
One can note that under FedFB, clients exchange additional information with the server by com-
municating real-valued loss values in addition to the model parameters. To limit the information
leakage, we also consider a variant of FedFB, which exchanges the quantized loss values. For
instance, “FedFB(10bits)” means each loss value is uniformly quantized using 10 bits. Such a loss
quantization scheme limits the amount of additional information shared between the clients and the
server, at the cost of potentially inaccurate group weight updates.
5	Experiments
In this section, we numerically study the performance of UFL, FFL via FedAvg, and CFL for more
general cases, and evaluate the empirical performance of FedFB. We investigate the fundamental
limitation of UFL under general Gaussian distribution. We compare the accuracy-fairness tradeoff
of UFL, FFL via FEDAVG, and CFL by numerically solving UFL(i, εi), FFL via FEDAVG(ε),
and CFL(ε). More specifically, we first characterize the solutions to these problems up to an unknown
scalar, which can be numerically optimized. See Sec. A for more details. Moreover, we evaluate
FedFB on both demographic parity and client parity. In each simulation study, we report the summary
statistics across five replications. Similar to the experimental settings used in (Roh et al., 2020), we
train all algorithms using a two-layer ReLU neural network with four hidden neurons to evaluate
the performance of FedFB for the non-convex case. The results for logistic regression are provided
in Sec. C. We also investigate the empirical relationship between the performance of FedFB and
the number of clients and incorporate differential privacy to further strengthen the power of FedFB.
More implementation details are included in Sec. C.
5.1	Limitation of UFL on general cases
The first experiment examines the fairness range of UFL under a more general Gaussian distribution,
which does not satisfy the conditions of Corollary 2. For instance, if the variance of two clients is
similar, then the conditions do not hold. However, we still conjecture that the same phenomenon
holds for more general distributions, and we corroborate our conjecture with numerical experiments.
Shown in Fig. 3(b,c) are the numerically computed lower bound on UFL’s achievable fairness. In
particular, for (b), we let X | a = 0, i = 0 〜N(10,0.22),X | a = 1, i = 0 〜N(9.8,0.22),X |
a = 0, i = 1 〜N(0.2,0.22), x | a = 1, i = 1 〜N(0,0.22), a 〜Bern(0.2), and for (c), We let
x | a = 0, i = 0 〜N(3,1), X | a = 1, i = 0 〜N(5,1), X | a = 0, i = 1 〜N(1,1), X | a =
1,i = 1 〜 N (-1, 1),a 〜 Bern(0.5). For both cases, we set η(x)= ［十；—.. It is easy to check
that these distributions do not satisfy the conditions of Corollary 2. In particular, the distribution (b)
corresponds to the case that the same group is favored on both clients, and the positive rates of each
group in different clients are distinctive. The distribution (c) represents the case that different groups
are favored on two clients. In both cases, we can see that UFL fails to achieve perfect fairness, i.e.,
δ > 0. We also observe that δ is large on the distribution (c), where different groups are favored on
two clients. This supports our conjecture that UFL’s fairness performance is strictly limited not only
on certain data distributions but also on more general ones.
5.2	Accuracy-fairness tradeoffs of UFL, FFL via FedAvg and CFL
The second experiment extends the experiments conducted in Sec. 3.3. We assess the relationship
between the data heterogeneity and the gap between the three fair learning scenarios with three clients.
7
Under review as a conference paper at ICLR 2022
Table 2: Comparison of accuracy and DP disparity on the synthetic dataset with varying heterogeneity.
FedFB achieves good performance on all the tested levels of heterogeneity. This is because by design, FedFB
closely matches the operation of CFL, whose performance is independent of data heterogeneity.
Low Data Heterogeneity	Medium Data Heterogeneity	High Data Heterogeneity
Method	ACC.(↑)	DPDISP.(J)	ACC.(↑)	DP DISP.(J)	ACC.(↑)	DP DISP.(J)
FedFB (Ours)	.669±.040	.058±.042	.613±.007	.011±.009	.627±.019	.030±.026
CFL	.726±.009	.028±.016	.726±.009	.028±.016	.726±.009	.028±.016
As shown in Fig. 8, FFL via FedAvg is observed to achieve a strictly worse tradeoff than CFL and a
strictly higher maximum fairness value than UFL. The results corroborate the benefit and limitation of
FedAvg-based federated learning in improving fairness. A very interesting observation is that UFL
is observed to obtain a strictly higher accuracy than FFL via FedAvg. Indeed, this could be attributed
to the fact that the average of locally fair models might not be fair to any sub-distribution, while UFL
at least ensures that each component of the mixture classifier is fair on some sub-distribution.
5.3	FedFB evaluation on demographic parity
We assess the empirical performance of FedFB for both convex and non-convex cases on four
datasets and the performance of FedFB under different data heterogeneity. We focus on demographic
parity and report DP disparity = maxa∈[* ∣P(^ = 1 | a = a) - P(^ = 1)|, where A is the number
of groups. Note that this is slightly different from the definition we used in the previous sections,
which was used specifically for the case of one binary sensitive attribute.
Baselines We employ three types of baselines: (1) decentralized non-fair training (FEDAVG); (2)
decentralized fair training (UFL, FFL via FedAvg); (3) centralized fair training (AgnosticFair,
CFL). Here, for all the algorithms that involve fair training, we use our improved version of FB,
which is the state-of-the-art fair learning algorithm on centralized data. for fairer comparison and
better performance, the implementation of UFL, FFL via FedAvg, and CFL are all based on FB.
Note that UFL is absolutely private, CFL violates the privacy policy, FedAvg, FFL via FedAvg,
and FedFB share some information at communication rounds without directly sharing the data, and
AgnosticFair exchanges information for each local update. To have a fairer comparison between
FFL via FedAvg and FedFB, we also equalize their differential privacy guarantees (Dwork, 2008)
and compare their performances. See Sec. C for more details. We also report the performance of
FairFed, a recently proposed algorithm for achieving demographic parity for binary sensitive groups
in the federated setting (Ezzeldin et al., 2021), and AgnosticFair in Sec. C.
Datasets (synthetic) We follow Roh et al. (2021) for data generation, but with a slight modifica-
tion to make the dataset more unbalanced. To study the empirical relationship between accuracy,
fairness, and data heterogeneity, we split the dataset in different ways to obtain desired levels of
data heterogeneity. More details are given in Sec. C.2. (real) We use three benchmark datasets:
Adult (Dua & Graff, 2017) with 48,842 samples, COMPAS (ProPublica, 2021) with 7,214 samples,
and Bank (Moro et al., 2014) with 45,211 samples. We follow Du et al. (2021)’s method to preprocess
and split Adult into two clients and Jiang & Nachum (2020)’s method to preprocess COMPAS and
Bank. Then, we split COMPAS into two clients based on age and split Bank into three clients based
on the loan decision. Note that all the datasets are split in heterogeneous ways.
Results We present the results for two-layer ReLU neural networks in
Table 1 and leave the results for logistic regression in Sec. C. Table 1
reports the test accuracy and DP disparity of four baselines and FedFB.
We see a substantial fairness improvement obtained by FedFB. As ex-
pected, the resulting fairness level of FedFB is close to that of CFL and
AgnosticFair. Besides, we observe the poor performance of UFL and
FFL via FedAvg, which is due to the fundamental limitation of UFL
and FFL via FedAvg. Table 2 reports the accuracy and fairness of each
method under different data heterogeneity. FedFB is observed to be robust
to data heterogeneity. This agrees with our expectation as FedFB mimics
the operation of CFL - which is not affected by data heterogeneity - as
0.8
0.7
0.0	0.5
DP Disparity
Figure 5:	Accuracy-
fairness tradeoff curves
on the synthetic dataset.
FedFB nearly matches the
performance of CFL.
much as possible by design. We make a more thorough comparison between CFL and FedFB by
plotting the accuracy-fairness tradeoff curves in Fig. 5. To demonstrate the performance gain does
not come at the cost of privacy loss, we restrict FedFB to only exchange 10 bits of information per
communication round. Fig. 5 showcases the benefit of FedFB in terms of accuracy, fairness and
privacy. In Table 7 and Table 8 in Sec. C, we compare FedFB and FairFed. One can observe that
FedFB can achieve a strictly improved fairness than FairFed. Also, FedFB is observed to be more
8
Under review as a conference paper at ICLR 2022
CP Disparity
--q- q-FFL
■■■<■ Ditto
----GIFAIR
―**— FedFB
—∙— FedFB(IObits)
Figure 6: Comparison of accuracy and Client Parity (CP) disparity on the synthetic, Adult, COMPAS,
and Bank datasets. Even though our algorithm is not specifically designed for CP, it closely matches the
performance of the state-of-the-art fair federated learning algorithms designed for CP.
robust to data heterogeneity. Table 9 in Sec. C shows that FedFB achieves similar performance as
AgnosticFair, at much lower cost of privacy.
5.4	FedFB evaluation on client parity
We evaluate the performance of FedFB in achieving client parity (CP) and compare it with the
state-of-the-art algorithms for CP. We will measure CP disparity = maxi6=j∈[I] |L(i) - L(j) |. Here
L(i) is the loss in ith client, see Sec. B.4 for more detail.
Baselines We consider GIFAIR (Yue et al., 2021), Q-FFL (Li et al., 2020a), DITTO (Li et al., 2021),
and the unconstrained baseline FedAvg (McMahan et al., 2017). GIFAIR and q-FFL are the most
similar ones to FedFB. Similar to FedFB, both GIFAIR and q-FFL propose a modified aggregation
protocol, under which clients share some additional information with the central server, which then
accordingly adjust the objective function used for the next round of training. The key difference is
that while FEDFB optimizes the coefficients for the primary objective terms (i.e. sample reweighting)
by solving a bi-level optimization problem, GIFAIR updates the coefficient for the penalty terms,
and q-FFL implicitly updates the weights on each objective term based on nonlinear behaviors of
polynomial functions, which is equivalent to the α-fairness algorithm used in networking (Mo &
Walrand, 2000). The Ditto algorithm combines multitask learning with federated learning to learn a
personalized classifier for each client, improving the accuracy of the clients with low accuracy.
Datasets We use the same datasets as Sec. 5.3, but split the datasets according to their sensitive
attributes to simulate the same setting as assumed by GIFAIR, q-FFL, and Ditto.
Results Fig. 6 shows that FEDFB offers competitive and stable performances in mitigating the
model bias, especially in the high fairness region. Although q-FFL achieves better accuracy and
fairness on the synthetic data, under strict fairness constraint, FedFB and its private variant nearly
achieves the highest accuracy on the other three datasets.
6	Conclusions
Summary We have investigated how one can achieve group fairness under a decentralized setting.
For the first time in the literature, we developed a theoretical framework for decentralized fair learning
algorithms and analyzed the performance of UFL, FFL via FedAvg, and CFL. As a result, we provide
novel insights that (1) federated learning can significantly boost model fairness even with only a
handful number of participating clients, and (2) FedAvg-based federated fair learning algorithms
are strictly worse than the oracle upper bound of CFL. To close the gap between FedAvg-based
fair learning algorithms and CFL, we propose FedFB, a new federated fair learning algorithm. The
key idea behind FedFB is that each client shares extra information about the unfairness of its local
classifier with the server, which then computes the optimal samples weights that need to be used
for the following round of local training. Our extensive experimental results demonstrate that our
proposed solution FedFB achieves state-of-the-art performance, while still ensuring data privacy.
Open questions (Theory) While we characterized some fundamental limits on tradeoffs of various
approaches, there still remains a large number of open questions. First, as we briefly mentioned in
Sec. 3.3, full theoretical characterization of accuracy-fairness tradeoff still remains open. Our current
theoretical results only study the extreme ends of the tradeoff curves. Moreover, as shown in Sec. 5.2,
some of our experimental results reveal a highly nontrivial phenomenon. Studying this phenomenon
and identifying the exact relationship between various learning algorithms is an interesting open
problem. Furthermore, a three-way tradeoff between accuracy, fairness, and privacy remains widely
open. (Algorithm) It remains open whether or not our proposed solution FEDFB can be applied
for achieving different fairness notions used in the federated setting. In particular, proportional
fairness, i.e., clients who contribute more should receive more rewards (Zhang et al., 2020b; Lyu
et al., 2020b;a), is another popular notion of fairness used in the federated setting, and our current
FedFB cannot handle it. Extending FedFB to handle proportional fairness is one future research
direction.
9
Under review as a conference paper at ICLR 2022
Ethics statement
This work will improve the well-being of individuals in our society by solving ethical issues of AI,
such as the implicit discrimination in machine learning algorithms and the privacy of sensitive data.
Our theoretical and empirical findings assert that a fair machine learning model can be reliably trained
on decentralized data without compromising much privacy.
Reproducibility statement
We have released our implementation in anonymous github1, which contains the code for all the
experiments, including the datasets we use and the implementation of data processing steps. For the
theoretical results, we provide all the necessary assumptions and proof in the Appendix.
References
Lingyang Chu, Lanjun Wang, Yanjie Dong, Jian Pei, Zirui Zhou, and Yong Zhang. Fedfair: Training
fair models in cross-silo federated learning. arXiv preprint arXiv:2109.05662, 2021.
Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang. Addressing algorithmic
disparity and performance inconsistency in federated learning. In Thirty-Fifth Conference on
Neural Information Processing Systems, 2021.
Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong. Fairness-aware agnostic federated learning.
In Proceedings ofthe 2021 SIAM International Conference on Data Mining (SDM), pp. 181-189,
2021.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and
applications of models of computation, pp. 1-19, 2008.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,
ITCS ’12, pp. 214-226, 2012. ISBN 9781450311151. doi: 10.1145/2090236.2090255. URL
https://doi.org/10.1145/2090236.2090255.
Yahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and Salman Avestimehr. Fairfed:
Enabling group fairness in federated learning. arXiv preprint arXiv:2110.00857, 2021.
Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility of
fairness, 2016.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances
in Neural Information Processing Systems (NIPS), volume 29, pp. 3315-3323, 2016.
Heinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In
Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,
volume 108 of Proceedings of Machine Learning Research, pp. 702-712, 2020. URL http:
//proceedings.mlr.press/v108/jiang20a.html.
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering:
Auditing and learning for subgroup fairness. In Proceedings of the 35th International Conference
on Machine Learning (ICML), volume 80 of Proceedings of Machine Learning Research, pp.
2564-2572, 2018. URL http://proceedings.mlr.press/v80/kearns18a.html.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2017.
1https://anonymous.4open.science/r/Improving-Fairness-via-Data-Federation-7B68.
10
Under review as a conference paper at ICLR 2022
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. In International Conference on Learning Representations (ICLR), 2020a. URL https:
//openreview.net/forum?id=ByexElSYDr.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization. In Proceedings of the 38th International Conference on Machine
Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 6357-6368,
2021. URL http://proceedings.mlr.press/v139/li21h.html.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations (ICLR), 2020b.
URL https://openreview.net/forum?id=HJxNAnVtDS.
L. Lyu, J. Yu, K. Nandakumar, Y. Li, X. Ma, J. Jin, H. Yu, and K. Ng. Towards fair and privacy-
preserving federated deep models. IEEE Transactions on Parallel & Distributed Systems, 31(11):
2524-2541, 2020a. ISSN 1558-2183. doi: 10.1109/TPDS.2020.2996273.
Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. Collaborative fairness in federated learning. In
Federated Learning, pp. 189-204. Springer, 2020b.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Areas.
Communication-efficient learning of deep networks from decentralized data. In International Con-
ference on Artificial Intelligence and Statistics (AISTATS), volume 54 of Proceedings of Machine
Learning Research, pp. 1273-1282, 2017. URL http://proceedings.mlr.press/v54/
mcmahan17a.html.
Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In
Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of
Proceedings of Machine Learning Research, pp. 107-118, 2018. URL http://proceedings.
mlr.press/v81/menon18a.html.
Jeonghoon Mo and Jean Walrand. Fair end-to-end window-based congestion control. IEEE/ACM
Transactions on networking, 8(5):556-567, 2000.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Proceed-
ings of the 36th International Conference on Machine Learning (ICML), volume 97 of Proceedings
of Machine Learning Research, pp. 4615-4625, 2019. URL http://proceedings.mlr.
press/v97/mohri19a.html.
Sergio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of
bank telemarketing. Decision Support Systems, 62:22-31, 2014. ISSN 0167-9236. doi: https://
doi.org/10.1016/j.dss.2014.03.001. URL https://www.sciencedirect.com/science/
article/pii/S016792361400061X.
ProPublica. Compas recidivism risk score data and analysis. 2021.
URL	https://www.propublica.org/datastore/dataset/
compas-recidivism-risk-score-data-and-analysis.
Borja Rodrlguez-Galvez, Filip Granqvist, Rogier van Dalen, and Matt SeigeL Enforcing fairness
in private federated learning via the modified method of differential multipliers. arXiv preprint
arXiv:2109.08604, 2021.
Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. FR-train: A mutual information-
based approach to fair and robust training. In Proceedings of the 37th International Conference
on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pp.
8147-8157, 2020. URL http://proceedings.mlr.press/v119/roh20a.html.
Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection
for model fairness. In International Conference on Learning Representations (ICLR), 2021. URL
https://openreview.net/forum?id=YNnpaAKeCfx.
Xubo Yue, Maher Nouiehed, and Raed Al Kontar. Gifair-fl: An approach for group and individual
fairness in federated learning. arXiv preprint arXiv:2108.02741, 2021.
11
Under review as a conference paper at ICLR 2022
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness
beyond disparate treatment & disparate impact: Learning classification without disparate mistreat-
ment. In Proceedings of the 26th International Conference on World Wide Web, pp. 1171-1180,
2017a.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi, and Adrian
Weller. From parity to preference-based notions of fairness in classification. In Advances in Neural
Information Processing Systems (NIPS), NIPS’17, pp. 228-238, 2017b. ISBN 9781510860964.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P. Gummadi. Fair-
ness Constraints: Mechanisms for Fair Classification. In International Conference on Artificial
Intelligence and Statistics (AISTATS), volume 54, pp. 962-970, 2017c.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International conference on machine learning, pp. 325-333. PMLR, 2013.
Daniel Yue Zhang, Ziyi Kou, and Dong Wang. Fairfl: A fair federated learning approach to reducing
demographic bias in privacy-sensitive classification models. In 2020 IEEE International Conference
on Big Data (Big Data), pp. 1051-1060, 2020a. doi: 10.1109/BigData50022.2020.9378043.
Jingfeng Zhang, Cheng Li, Antonio Robles-Kelly, and Mohan Kankanhalli. Hierarchically fair
federated learning. arXiv preprint arXiv:2004.10386, 2020b.
12
Under review as a conference paper at ICLR 2022
Symbol	Meaning	Symbol	Meaning	Symbol	Meaning
x	feature	-H-	indicator function	εi	bias in client i
a	sensitive attribute	f	randomized classifier	q	a 〜Bern(q)
i	client index	Pa(i)	distribution	DP DisP(f)	unfairness of f
y	Predicted class	η(x)	P(y = 1|x = x)	fUFL fε0 ,ε1	UFL classifier
Table 3: Commonly used notations.
A Appendix - UFL, FFL via FedAvg, CFL analysis
In this section, we provide the concrete analysis for UFL, FFL via FedAvg, and CFL. For illustration
purposes, we will start by discussing two clients cases, and then extend the analysis into more clients
cases in Sec. A.4. We begin with the analysis of CFL in Sec. A.1. Then we analyze UFL and FFL via
FedAvg. To be specific, in Sec. A.2, we analyze the limitation of UFL and present the formal version
of Lemma 1 under two clients cases and Corollary 2. In Sec. A.3, we analyze FFL via FedAvg
and compare it with UFL and CFL, then we present the formal two-client version of Thm. 3 and
Lemma 4. All the multi-client statements are included in Sec. A.4 We summarize the commonly used
notations in Table 3 .
A. 1 CFL analysis
In this section, we analyze the CFL classifier fεCFL given in CFL(ε). We mainly derive the solution
of CFL(ε) in Lemma 5. In Lemma 6 and Lemma 7 we summarize the properties of fεCFL.
Lemma 5. Let q ∈ (0,1). Define g(∙) : [— max(q, 1 — q), max(q, 1 — q)] → [—1,1] as
g(λ) =
[[-~(1 1 - 2(i-q) ), + ∞]
dP0 —
J[n-1( 1+2λq ),+∞]
dP1,
then fCFL = {Js(x,a) > 0』+ αJs(x,a) 二 0』:α ∈ [0,1]}, where s(x, 0) = 2η(x) — 1 + i-q,
s(x, 1) = 2η(x) — 1 — q, λ = g-1(sign(g(0))min{ε, ∣g(0)∣}). Here we denote the indicator
function as JE』 : JE』 = 1 if E is true, zero otherwise.
Proof. The proof is similar as Menon & Williamson (2018). To solve CFL(ε), we first write the error
rate and the fairness constraint as a linear function of f. Let pa(∙) be the Pdf of Pa, where a = 0,1.
Denote the joint distribution of x and a as px,a(x, a). Note that
P(^ = y)
=	[f (x, a)(1 — η(x)) + (1 — f(x, a))η(x)] px,a(x, a) dx
X a∈A
=Ex,af(x,a)(1 — 2η(x)) +P(y= 1)
and
P(^= 1 | a = 0) - P(^= 1 | a= 1)
f(x, 0)p0(x) dx —	f(x, 1)p1(x) dx
ZX XJa = 0Κf (x, 0)P⅛ dx -ZX XJa = 1Kf (x, 1) J dx
=ExJf (x, 0)铝K — f(x,1) Ja≤Γ.
1—q	q
Consequently, our goal becomes solving
minf∈F Ex,af(x, a)(1 — 2η(x)) +P(y= 1)
s.
≤ ε.
(1)
13
Under review as a conference paper at ICLR 2022
Denote the function that minimizes the error rate (ERM) as f ∈ F. It is easy to see that,
~ ,	,	,	∙r	-，、	-	-
f (X) ∈ {Jn(X) > 1/2K + αJη(x) = 1/2K： α ∈ [0,1]}.
TL T	♦∙>,∙> Cll ♦	.1	T	. •	1	♦•>,∙>	∕' ∕' I-H « iʌ / 7∖ I ，	1
Next, consider the following three cases. In particular, we provide the proof for |MD(f)| ≤ ε and
MD(f)> ε. The proof for MD(『)< -ε is similar as the proof for MD(『)> ε.
Case 1.	|MD(f)| ≤ ε: ERMisalreadyfair
一 一 —	_	_____.... . . τ
The solution to (1) and CFL(ε) is f.
Case 2.	MD(f) > ε: ERM is favoring group 0 over group 1.
We will show that solving (1) is equivalent to solving an unconstrained optimization problem.
First, we will prove by contradiction that the solution f? ∈ F to (1) satisfies MD(f?) = ε. We
use f? ∈ F to denote the solution of (1). Suppose the above claim does not hold. Then we have
MD(f ? ) < ε. To show the contradiction, we construct a f0 ∈ F that satisfies the fairness constraint
and has a lower error rate than that of f?. Let f0 be a linear combination of f? and f:
f0 = af? + (1- a)f,
Where a = MDMDfD：f*) ∈ (O, I). Then We Obtain
,... ,.,, , 、 ,~,
MD(f0) = aMD(f?) + (1 - a)MD(f)
, ,~. ,....
ε(MD(f) - MD(f?)) = ε
MD(f) - MD(f?)一〜
Denote the error rate P {^ = y} as e : F → [0,1]. Then the error rate of f 0 is
e(f0)= ae(f*) + (1- a)e(f) <e(f?),
Which is inconsistent to the optimality assumption of f?. Therefore, MD(f?) = ε.
NoW, solving CFL(ε) is equivalent to solving
minf∈FEx,af(x,a)(1 - 2η(x)) +P(y = 1)
s∙t∙ |Ex,a [f(x, O) Ja_0K - f(x, I) JaqIK i | = ε
Furthermore, the optimization problem above is also equivalent to
minf∈F Eχ,af(x, a)(1 - 2n(x)) - λEχ,a (f(x, 0) Ja = OK - f (x, 1) Ja = IK)	(2)
1-q	q
s.t. |Ex,a [f (x, 0) P - f(x,1) Ja≤> 1 | = ε,	⑶
1-q	q
for all λ ∈ R.
Next, our goal is to select a suitable λ such that the constrained optimization problem above becomes
an unconstrained problem, i.e., We Will select a suitable λ such that the minimizer to the unconstrained
optimization problem (2) satisfies equality constraint (3).
Note that
Eχ,af (x, a)(1 - 2n(x)) - λEχ,a (f (x, 0)J∣=∣K - f (x, 1)Ja=≡)
=Eχ,af (x, a)(1 - 2n(x) - λJa-I + λ—),
then the solution to unconstrained optimization problem (2) is
f ∈ {Js(x, a) > 0K + αJs(x, a) = 0』：α ∈ [0, 1]},
14
Under review as a conference paper at ICLR 2022
τm∙	r<L ι ∙	,♦	ι~ / ∖ ∖	1 ɪ / ∖ ∖ ɪ Xt τι	∕c∖ x ʃ-rʌ /了\、	~,,ι	t ∙	∖ ι' .t 1	.
Figure 7: Visualization of g(λ) and ∣g(λ)∣. When g(0) = MD(f) > ε ≥ 0, the corresponding λ of the best
classifier is λ = g-1() < 0.
where
s(x, a) =
∫-1 + 2η(x) + 1-q
[-i + 2η(χ)- q
a=0
a=1
Since the range of η(x) is [0,1], f with λ > max(q, 1 - q) is no different from f with λ =
max(q, 1 — q); f with λ < — max(q, 1 — q) is no different from f with λ = — max(q, 1 — q).
Therefore, the only thing left is to find the λ ∈ [— max(q, 1 — q), max(q, 1 — q)] such that f satisfies
the constraint (3).
Now consider the mean difference between the positive rate of two groups:
MD(∕) = P(^= 1 | a = 0) — P(y=1 | a = 1)
Z+∞	1 λ	+∞	1 λ
∞ Jn(X) > 2 - 2(T-q) K * L Jn(X) > 2 + 2qK 叫
=	dP0 —	dP1
[n-1( ⅛ - 2(i—q) }+∞]	[n-1( 2+2λq }+∞]
= g(λ).
Note that g(∙) : [— max(q, 1 — q), max(q, 1 - q)] → [—1,1] is a strictly monotone increasing function.
Consequently, if and only if λ = g-1(ε), f satisfies (3). Recall that optimization problem (2) with
constraint (3) is equivalent as CFL(ε). Thus, let λ = g-1(ε) and f is the solution of CFL(ε).
Case 3. MD (f) < —ε: ERM is favoring group 1 over group 0.
Similarly, like Case 2, we obtain that the solution CFL(ε) is
f ∈ {Js(x, a) > 0K + αJs(x, a) = 0K : α ∈ [0, 1]},
where
s(X, a) =
[—1 + 2n(χ) + 合
1—1 + 2n(χ) — q
a=0
a=1
and λ = g-1(—ε). Combining all the cases above yields the desired conclusion. The proof is now
complete.	□
Remark 4. Select α = 0, then the solution to CFL(ε) can be written as
f(x, a) =
1-2 1-2
>>
))
XX
((
nn
JJ
-2(i-q) K
+ 2q K
a=0
a=1
(4)
Therefore, Lemma 5 implies that the best classifier of the CFL problem is equivalent to simply
applying a constant threshold to the class-probabilities for each value of the sensitive feature.
15
Under review as a conference paper at ICLR 2022
Lemma 5 suggests the following property of the solution to CFL(ε).
Lemma 6. If f and g are two solutions to CFL(ε), then f = g almost everywhere.
For illustration purposes, we denote
λεCFL = g-1(sign(g(0)) min{ε, |g(0)|}).	(5)
Below we summarize some useful properties of λεCFL.
Lemma 7. The sign of λεCFL and MD(fεCFL) are determined by g(0).
1.	If ε < |g(0)|, then MD(fεCFL) = λεCFL 6= 0, and g(λεCFL) = sign(g(0))ε. If |g(0)| ≤ ε, then
λεCFL = 0 and MD(fεCFL) = g(λεCFL) = g(0).
2.	If g(0) > 0 or g(0) < 0, then for any ε ≥ 0, we have λ ≤ 0 or λ ≥ 0, respectively.
Proof. The first property follows directly from the definition of λεCFL. Next, we prove the second
property.
When ε > |g(0)|, we have λ = 0 and the first property holds. When g(0) > ε > 0, by the
definition of λεCFL, we have λ = g-1 (ε) < g-1 (g(0)) = 0. When g(0) < -ε < 0, we have
λ = g-1(-ε) > g-1 (g(0)) = 0. Combining all the cases above yields the desired conclusion. □
A.2 UFL analysis
With the analysis of CFL in Sec. A.1, in this section, we analyze the UFL classifier UFL(i, εi) for the
case of two clients. For illustration purpose, with I = 2, we denote f^* = fUFL = (f00 + f ；1)/2.
In Sec. A.2.1 we introduce some notations for the UFL classifier fεU0F,εL1 that follows from Lemma 5.
In Sec. A.2.2 we analyze the limitation of fεUF,εL as stated in Sec. 3.1. To be more specific, we present
the two clients’ version of Lemma 1, formal version of Corollary 2 and conclude their proof. In
Sec. A.2.3, we analyze the performance gap between fεU0F,εL1 and CFL classifier fεCFL.
A.2.1 Problem setting
By Lemma 5, the solution to UFL(i, εi) is
fiεP,a)=<η(X) > 1 - λ⅛K
[Jη(χ) > 2 + ⅛K
where the associated λUFLi is defined as
εi
a=0
a=
and
λUFLi
εi
gi-1(sign(gi(0))min(εi, |gi(0)|))
(6)
gi(λ) = [η-11
2 - 2(1-q)
dP0(i) -	dP1(i) .
),+∞)	[[-I(11 + 2q ),+∞)
1
λ
Note that gi(λ) is the mean difference on ith client Ex 〜户⑶ f (x, 0) - Ex 〜户⑸ f (x, 1) of the classifier
of the form (4). Now, the demographic disparity for fεUF,εL can be written as
1
1	1
DPDiSP(fUFLI)= -]T[P(^=1 |
a = 0, i = i) — P(^= 1 | a = 1, i = i)]
i=0
1 1
=W X 艮〜P0i)疗(χ, 0) - %pf) fjεj (χ, 1)]
i,j=0
=I- (go(λ UFL 0) + go(λUFLI) + gι(λUFL0)+ gι (λUFLI))∣.
For ease of notation, we define local mean difference on ith client as MDi(f) = P(^ = 1 ∣
a = 0, i = i) - P(^ = 1 ∣ a = 1, i = i) and local demographic disparity on ith client as
16
Under review as a conference paper at ICLR 2022
DP Dispi(f) = |MDi(f)|, where i = 0, 1. Since the overall distribution of the data samples is
X | a = a 〜 Pa =。!0)/2 + Fa1 /2 a = 0,1, g (See the definition of g in Lemma 5) and go, g1 has
the following relation:
g(λ) = 1 g0(λ) + 1 g1(λ).
A.2.2 Limitation of UFL
In this section, we mainly analyze the limitation of UFL in Lemma 8, which shows that fεUF,εL can
not achieve 0 demographic disparity in certain cases. Corollary 9 is a specific example of Lemma 8.
Lemma 8 (Formal version of Lemma 1 under two clients cases). Let q ∈ (0, 1). Let c =
min{|g0(0)|, |g1(0)|}. Define ψ : [0, c] × [0, c] → [-1, 1] as
ψ(ε0,εI) = MD (fUFLi) = 4 g0(g-1(sign(g1 (O))εI)) + 4 g1(g-1(Sign(g0(O))εO))
14	1	4	(7)
+ 4 Sign(g0(0))ε0 + 4 Sign(g1(0))ε1.
If g0(0)g1(0) < 0 and ψ(ε0, ε1)(g0(0) + g1(0)) > 0 for all ε0, ε1 ∈ [0, c], then for all ε0, ε1 ∈ [0, 1],
DPDisP(fU0F,ε 1) ≥ δ = min{∣ψ(ε0,ε1)∣ : ε0,ε1 ∈ [0,c]} > 0.
Proof. Define δ = min{∣ψ(ε0, ε1)∣ : ε0, ε1 ∈ [0, c]}. The goal is to show that the demographic
disparity has a positive lower bound. Note that the mean difference can be expressed as
MD(fUFLI) = 4 (g0(λ*L0) + g0(λUFLI) + g1(λUFLθ) + g1(λUFLI )).	(8)
In the following proof, we will show, the mean difference cannot reach 0.
Without any loss of generality, assume |g0(0)| < |g1(0)|. First we consider g1(0) > 0. We will
discuss g1(0) < 0 later. By g0(0)g1(0) < 0 and ψ(ε0, ε1)(g0(0) + g1(0)) > 0 for all ε0, ε1 ∈ [0, c],
we have g0(0) < 0 and ψ(ε0, ε1) > 0 for all ε0, ε1 ∈ [0, c].
First, we will prove that UFL achieves its lowest mean difference when ε0, ε1 ∈ [0, c]. In what
follows, we consider five different cases to derive the desired result.
Case 1. ε0 > |g0(0)|, ε1 > |g1(0)|: ERM is fair on both clients.
By (6), We have λUFL0 = λUFL1 = O. Recall gi(∙) is a monotone increasing function, We combine
g1(0) > 0 and Lemma 7 to have g0(g1-1(0)) < g0(0) < 0. Applying the above conclusion yields
44	444
(8)	= 2g0(O) + 2g1(0) > 2 (4g0(O) + 4g1(0) + 4g0(g11(O))I = 2ψ(g0(0),0) ≥ δ∙
Case 2.	ε0 ≤ |g0(0)|, ε1 > |g1(0)|: ERM is unfair on client 0, but fair on client 1.
Applying (6) results in λUFL1 = O. By the fact that gi(∙) is a strictly monotone increasing function,
We have λεU0FL0 = g0-1(-ε0) > g0-1(g0(O)) = O. Applying the above conclusion yields
(8)=- 4ε0+4g1(O)+4g0(O)+4g1(λUFLO) (λεoFL0 > 0,g1(λUFLO) > g1(O),g0(O) < -ε0)
>2g0(O) + 2g1(O) > 2ψ(g0(o),O) ≥ δ∙
Case 3.	ε0 ≤ |g0(O)|, ε1 ≤ |g1(O)|: ERM is unfair on both client 0 and client 1.
Applying (6) We have λεU0FL0 = g0-1(-ε0), λεU1FL1 = g1-1(ε1). Then We have
(8) = 4 (-ε0 + ε1 + g0(g-1 (εI)) + g1(g-1(-ε0)))
≥ 4 (-ε0 + ε0 + g0 (g-1 (ε0)) + g1(g-1(-ε0 ))) = ψ(ε0, ε0) ≥ δ∙
17
Under review as a conference paper at ICLR 2022
Case 4. ε0 > |g0(0)|, ε1 ≤ |g0(0)|: ERM is fair on client 0 and very unfair on client 1.
By (6), we have λεUFL0 = 0, λεUFL1 = g1-1(ε1) > g1-1(0). Then we obtain
(8)	= 4 (g0(O) + goCUFL1) + g1 (O) + ει)
> (g0(0) + g0(g1-1(0)) + g1(0))/4 = ψ(g0(0), 0) ≥δ.
Case 5. ε0 > |g0(O)|, |g0(O)| ≤ ε1 < |g1(O)|: ERM is fair on client 0 and unfair on client 1.
Applying (6) implies λεU1FL1 = g1-1(ε1) > g1-1(O). Therefore,
(8)	=4 (g0(O) + g0(g-1(ε1)) + ε1 + g1 (O))
> (go(O) + gι(O) + go(g-1(O)) + ει)∕4 > ψ(go(O), O) ≥ δ.
Combining all the cases above, we conclude that when g1 (O) > O DP Disp(fεU0F,εL1) ≥ δ =
min{∣ψ(εo, ει)| : ε0, ει ∈ [O, c]} > O for all ε0, ει ∈ [0,1].
Now we consider g1(O) < O, by the setting |g0(O)| < |g1(O)| and the assumption g0(O)g1(O) < O
and ψ(ε0, ε1)(g0(O) + g1(O)) > O, we have g0(O) > O and ψ(ε0, ε1) < O for all ε0, ε1 ∈ [O, c].
Following similar computation above, Case 1 - Case 5 become:
Case 1.	εo > ∣go(O)∣,ει > ∣gι(O)∣. Now we have O < go (O) < go(g-1(O)), thus
(8)	< 2 G go (O) + 1 gι (O) + 1 go(g-1(O))) = 2ψ(go(O), O) ≤ -δ.
Case 2.	ε0 ≤ |g0(O)|, ε1 > |g1(O)|. Now we have g0(O) > εo, g1 (λεU0FL0) < g1(O), thus
(8) < 2go(O) + 2gι(O) < 2ψSo(O), O) ≤ -δ.
Case 3.	In this case we have
(8)	= 4 (ε0 - ε1 + g0(g1 1(-ε1)) + g1(g0 1(ε0))) = ψ(ε0,ε1) ≤ -δ∙
Case 4.	Now we have λεU1FL1 = g1-1(-ε1) < g1-1(O), thus
(8) < (go(O) + go(g1-1(O)) + g1(O))∕4 = ψ(go(O), O) ≤ -δ.
Case 5. Now we have λεUFL1 = g1-1(-ε1) < g1-1(O), thus
(8) = (go(O) + g1(O) + go(g1-1(O)) - ε1)∕4 < ψ(go(O), O) ≤ -δ.
Then we conclude the proof.
□
Remark 5. Note that c is the smallest local demographic disparity the ERM achieves on clients.
The condition go(O)g1(O) < O implies that the ERM is favoring different groups in different clients.
The condition ψ(εo, ε1)(go(O) + g1(O)) > O for all εo, ε1 ∈ [O, c] implies that fεU0F,εL1 favors the same
group as ERM when the constraint is very tight. If the conditions above hold, Lemma 8 suggests that
there exists a lower bound of all the demographic disparity that UFL can achieve. In particular, if the
conditions above hold, UFL fails to achieve perfect demographic parity.
Among the conditions of Lemma 8, go(O)g1(O) < O can be satisfied by the distribution with high
data heterogeneity. To demonstrate the condition ψ(εo, ε1)(go(O) + g1(O)) > O for all εo, ε1 ∈ [O, c]
can be satisfied, we consider a limiting Gaussian case. The following corollary serves as an example
that satisfies the conditions of Lemma 8, and provides a more explicit expression of the lowest
demographic disparity UFL can reach in the Gaussian case.
18
Under review as a conference paper at ICLR 2022
Corollary 9 (Formal version of Corollary 2). Let q = 0.5, η(x) = "；-X, Pai) = N(仙!"，。(')2)
where (μ00) — μ10))(μ01) — μ11)) < OThenDPDisP(fUOFLJ ≥ δ ≈ 1 ∣go(0) + gι(0)∣ > 0 for all
ε0, ε1 ∈ [0, 1] if one of the following condition holds:
1.	σ(0)》σ(1), ∣μ00)∣, ∣μ10)∣, ∣μ01)∣, ∣μ11)∣ and μ01) > μ11): client 0 has much larger variance
than client 1, and client 1 is favoring grouP 0;
2.	σ⑴》σ(0), ∣μ00)∣, ∣μ10)∣, ∣μ01)∣, ∣μ11)∣ and μ00) > μ10): client 1 has much larger variance
than client 0, and client 0 is favoring grouP 0.
Proof. In this example, note that local mean difference function of λ can be written as:
gi(λ) = Φ( η-1(2 ：；))-a ) - Φ( η-1(1 -S . μ ),	(9)
where Φ(∙) is the CDF of the standard Gaussian distribution.
We only provide the proof for condition 1, and the proof for condition 2 is similar. Assume condition
1 holds. By (μ00) - μ10))(μ01) - μf)) < 0 and μ01) - μ1) > 0, we have μ00) - μ,) < 0. By (9),
we have g0(0) < 0 and g1(0) > 0. Consequently, combining Lemma 7 and (7) yields
ψ(ε0,εI) = 4 (g0(gl 1 (EI)) + g1(g0 1(-εO)) - ε0 + εl).
First we show that ψ(ε0, ε1) reachs its minimum either at (c, 0) or (0, c) by taking the derivative of
ψ, where c = min{|g0(0)|, |g1(0)|} is the smallest local demographic disparity. And then, we will
estimate the minimum of ψ on [0, 1] × [0, 1].
We take the derivative of ψ with respect to εi and get
∂ψ(	、	.	/	, g1-i(g-1 (Sign(gi(0))εi)八"
钎(εo,ε1) = sig"%(O)) 1 + - -1,.―/ 小、、、 /4.
∂εi	gi0(gi-1(Sign(gi(0))εi))
(0)	(0)
By condition 1,wehave go(0) = Φ(-3)-Φ(-翡)≈ 0, thus ∣go(0)∣ ≪ ∣gι(0)∣ and C = |go(0)|.
Since gi are increasing function, i = 0,1, we have gi(∙) > 0, and thus ∂dψ < 0, ∂dψ > 0. Therefore,
ψ reaches its extreme value at (0, c) and (c, 0).
Now, we evaluate
ψ(0, c) = (g0(g1-1(c)) + g1(g0-1(0)) - g0(0))/4 >
g1(O) - go(O) + go(g1 I(C))
4
where the inequality comes from go(0) < 0 to have g1(go-1(0)) > g1(0). And at (C, 0), we have
ψ(C, 0) = (go(g1-1(0)) + g1(0) + go(0))/4.
In what follows, we will show that min {ψ(c, 0), ψ(0, c)} ≈ δ = g1(0)+g0(o) by proving that
0 >go(g1-1(C)) >go(g1-1(0)) ≈0.
Consider ψ(C, 0) and ψ(0, C). Since g1(0) > C, go(0) < 0, we have
go(g1-1(0)) < go(g1-1(C)) < go(0) < 0.
Therefore, the only thing left is to show go(g1-1(0)) ≈ 0. We divide the rest of the proof into the
following three cases.
Case 1. μ11) < μ01) < 0: on client 1, the local classifier isfavoring group 0 over group 1, and the
positive rate of both groups are under 2.
19
Under review as a conference paper at ICLR 2022
Clearly, under this case, We have Rn-I(L)∞ dP01) = /0 ∞ dP01) < 2. We select λ0 < 0 such
that η-1( 1 + λO) = μ11). Then we have Rn-1( 1 +λ0),∞) dP(1) = R[μii),∞) dP(1) = 1, while
八-vι 入0 、 d dP01) < 0. Thus we get gι(λ0) < 0. Combining gι(0) > 0 and intermediate
J[n (2- 2(1-q) ),∞) U	J …/	J"' '
value theorem results in λ0 < g1-1(0) < 0. Then we obtain
μ11)= η-1(1 + λ0) <η-1(1+ g-1(0)) < 0
21	12	1	.	(10)
-μ11) = η-1(2 - T) > η-1(2 - g-1(O)) > 0	(η(x) - 2 is Odd)
By plugging (10) into (9), we have the other side of g0(g1-1(0)) < 0 is bounded by g0(λ0) =
Φ(μ1 σ-μ1 ') — Φ( -μ1σ(-μ0 '). Since σ⑴》∣μf)∣,向叫,weget go(g-1(0)) ≈ 0.
CaSe 2. 0 < μ11) < μ01): on client 1, the local classifier is favoring group 0 over group 1, and the
positive rate of both groups are above 2.
This proof of this case is similar to Case 1.
Case 3. μf) < 0 < μ01): with respect to the local classifier trained by client 1, the positive rate of
group 0 is above ɪ while that of group 1 is under 2.
Without any loss of generality, we assume ∣μf) | < ∣μ01) |. Select λ0 < 0 such that η( ɪ 一 λ00) = μ01).
Clearly R[n-1( 1 -λ00),+∞) dP01) = R[μ01),+∞) dP01) = 2, while
/	dP(1) = [	dP(I) >/	dP(I) > 1.
，[n-1( 2+λ00),+∞)	^[-μ01),+∞)	^[-μ11),+∞)	2
Consequently, we get g1(λ00) < 0 and λ00 < g1-1(0) < 0. Then we draw the same conclusion as (10).
Therefore, g0(g1-1(0)) ≈ 0.
Combining all three cases above, we get δ > 0. Then applying Lemma 8 we complete the proof. □
A.2.3 Comparison between UFL and CFL
In this section, we compare the performance of UFL and CFL. In Lemma 10 and Lemma 11, we
illustrate the conditions for UFL to have the same performance as CFL. In Lemma 12, Lemma 13
and Lemma 14, we illustrate the scenarios when CFL outperforms UFL.
To do the comparison, first we introduce some additional notations. Define the accuracy of a classifier
f as
Acc(f) = P(y = y)= P(y = 0)Eχ,α∣y=o[1 — f (x, a)] + P(y = 1)Eχ,a∣y=of (x, a).
Given the required global demographic disparity ε, define the performance of fεUF,εL as:
UFLMει; ε)=[ ACCcCLI)DPDiSPCCLI) ≤ ε,
0	o.w.
and define performance of fεCFL as:
CFL(ε) = Acc(fεCFL).
Now we are able to compare the performance between UFL and CFL with the metric UFL(ε0, ε1; ε)
and CFL(ε). In particular, we will show that, under some mild conditions, maxε0,ε1 UFL(ε0, ε1; ε) <
CFL(ε), which implies the gap between UFL and CFL is inevitable.
We begin with the following two lemmas, which describe the cases that UFL(ε0, ε1; ε) = CFL(ε).
20
Under review as a conference paper at ICLR 2022
Lemma 10. Let q ∈ (0, 1). Given an UFL classifier fεU0F,εL1 such that DP Disp(fεU0F,εL1) ≤ ε and a
CFL classifier fεCFL, we have UFL(ε0, ε1; ε) ≤ CFL(ε). The equality holds if and only if λεUFL0 =
λεUFL1 = λεCFL, where λεCFL is defined in (5), λεUFL0, λεUFL1 are defined in (6).
Proof. The proof is straightforward. Clearly, since fεCFL is the optimizer to CFL(ε), we have
UFL(ε0, ε1; ε) ≤ CFL(ε). By Lemma 5, according to the form of the solution to CFL(ε), fεU0F,εL1 is
the solution to CFL(ε) if and only if λεU0FL0 = λεU1FL1 = λεCFL. Thus complete the proof.
□
Lemma 11. Let q ∈ (0, 1). If the ERM is already fair, i.e., ε ≥ |g(0)|, then
max UFL(ε0, ε1; ε) = CFL(ε).
ε0,ε1
Proof. Since the ERM is already fair, fεCFL is ERM= Jη(x) > 1/2K. Therefore, we take ε0 = ε1 = 1,
and fεU0F,εL1 also equals to ERM. Thus we conclude the lemma.
□
The next two lemmas describes the cases that UFL(ε0, ε1 ; ε) < CFL(ε).
Lemma 12. Let q ∈ (0, 1). If g0(0)g1(0) < 0, maxε0,ε1 UFL(ε0, ε1; ε) < CFL(ε) for all ε <
|g(0)|.
Proof. In this proof, we only consider the case that g1(0) > 0, |g1(0)| ≥ |g0(0)|. The proof for
g1(0) > 0 or |g1(0)| ≥ |g0(0)| is similar. Next, we divide the proof into two cases.
Case 1.	maxε0,ε1 UFL(ε0, ε1; ε) = 0: UFL cannot achieve ε global demographic disparity.
The conclusion holds.
Case 2.	maxε0,ε1 UFL(ε0, ε1; ε) > 0: UFL can achieve ε global demographic disparity.
Since ε < |g(0)|, by Lemma 7, we have λεCFL 6= 0. Next, we solve fεU0F,εL1 by solving the local
version of CFL(ε). Combining Lemma 7, g1 (0) > 0 and g0(0) < 0 yields λεUFL0 ≥ 0, λεUFL1 ≤ 0.
If λUFL0 = λUFL1, then λUFL0 = λUFL1 = 0 = λCFL. Thus, We conclude the lemma by applying
Lemma 10.	□
Remark 6. Lemma 12 implies that if ERM is favoring different groups in different clients, there
exists an inevitable gap between the performance of UFL and that of CFL.
Lemma 13. Letq ∈ (0,1). LetT = min {∣g(0)∣, max{sign(g(0))g(g-1(0)), sign(g(0))g(g-1(0))}}.
If g0(0)g1(0) > 0, we have
max ufl (ε0,ε1;ε) {=CL (ε)
for all ε ≥ τ
o.W.
Proof. Without any loss of generality, assume g0(0), g1(0) > 0. Then by Lemma 7, We have
λεU0FL0 ≤ 0, λεU1FL1 ≤0forallε0,ε1 ∈ [0, 1], and g(0) = (g0(0)+g1(0))/2 >0.
To study the performance of UFL When g0(0)g1(0) < 0, recall that We use fiεi to denote the local
classifier trained by client i in UFL analysis. Therefore, fi0 is the local classifier trained by client i
that achieves perfect local fairness.
Next, We discuss tWo cases to prove the result. In Case 1, We Will shoW that τ = 0, and then prove
that maxε0,ε1 UFL(ε0, ε1; ε) = CFL(ε); in Case 2, We Will shoW that τ > 0, and then prove that
maxε0,ε1 UFL(ε0, ε1; ε) < CFL(ε) When ε < τ.
Case 1.	f00 = f10: the two local classifiers that achieve perfect local fairness are equal.
21
Under review as a conference paper at ICLR 2022
When ε ≥ g(0), the conclusion holds by directly applying Lemma 11. Therefore, in what follows,
we focus on the case that ε < g(0).
Next, we will first, show that when ε = 0, λ0UFL0 = λ0UFL1 = λ0CFL , which implies that f00 = f10
f0CFL.
Since f00 = f10, we have λ0UFL0 = λ0UFL1, and thus g0-1(0) = g1-1(0). Consequently, we get
g(λUFL0) = g(λUFLι)= g(g-1(0)) = g0(g-1(0)) +g1(g-1(0)) = 0 = g(λCFL).
By the monotonicity of g, we have λ0UFL0 = λ0UFL1 = λ0CFL and τ = 0. Next, we will show
that, for ε 6= 0, there also exists ε0, ε1 ∈ [0, 1] such that λεUFL0 = λεUFL1 = λεCFL, which implies
f UFL0 = f UFL1 = f CFL.
Consider ε 6= 0. Select εi = gi (λεCFL), i = 0, 1. By Lemma 5 and the monotonicity of g, we
have λ0CFL < λεCFL < 0. Therefore, εi = gi (λεCFL) > gi(λ0CFL) = 0. By Lemma 5, we get
λεUiFLi = gi-1 (εi) for i = 0, 1. By the selection of εi, we have λεCFL = gi-1(εi). Therefore,
λεCFL = λεUFL0 = λεUFL1 when ε 6= 0.
Combining all the discussion above yields fεU0F,εL1 = fεCFL for all ε < g(0), thus we conclude
maxε0,ε1 UFL(ε0, ε1; ε) = CFL(ε) for all ε < g(0). Consequently, the lemma holds under Case 1.
Case 2.	f00 6= f10: the two local classifiers that achieve perfect local fairness are different.
The key idea of this proof is: when MD0(fεCFL), MD1(fεCFL) ≥ 0, then we can always se-
lect ε0 =	g0(λεCFL) = MD0(fεCFL) >	0, ε1 = g1(λεCFL) =	MD1(fεCFL)	>	0 such that
λεU0FL0 =	g0-1(ε0) = λεCFL and λεU1FL1	= g1-1(ε1) = λεCFL,	thus fεU0F,εL1	=	fεCFL; when
MD0(fεCFL)MD1(fεCFL) = g0(λεCFL)g1(λεCFL) < 0, however, by Lemma 7, for all ε0, ε1 ∈
[0, 1] × [0, 1], we have g0(λεU0FL0)g1(λεU1FL1) > 0 > g0(λεCFL)g1(λεCFL), thus there exist i ∈ {0, 1}
such that λεUiFLi 6= λεCFL and fεU0F,εL1 6= fεCFL. Next, we will give rigorous proof.
Since f00 6= f10, we have λ0UFL0 6= λ0UFL1 . By Lemma 5, we get g0-1(0) 6= g1-1(0). Without any
loss of generality, assume g0-1(0) < g1-1(0), which implies g1(g0-1(0)) < g1(g1-1(0)) = 0 and
g0(g0-1(0)) = 0 < g0(g1-1(0)). Thus we get
g1(g0-1(0)) <0 < g0(g1-1(0)).
Combining the inequality above and g = g0+g1 yields
g(g0-1(0)) =g1(g0-1(0)) < 0 = g(g-1(0)) <g0(g1-1(0)) = g(g1-1(0)).	(11)
Thus we have
τ = max{sign(g(0))g(g0-1(0)), sign(g(0))g(g1-1(0))} = g(g1-1(0)).
When ε ≥ |g(0)|, by Lemma 11, clearly we have maxε0,ε1 UFL(ε0, ε1; ε) = CFL(ε).
For the other case ε < |g(0)|, by Lemma 7 we have λ = g-1(ε). Similar to Case 1, in order to
achieve λεU0FL0 = λεU1FL1 = λεCFL, we select εi = gi (λεCFL).
When ε < g(g1-1(0)),
g1(λεCFL) = g1(g-1(ε)) < g1(g-1(g(g1-1(0)))) = 0.
Since g1(0) > 0, by Lemma 7 we have g1(λεUFL1 ) ≥ 0, from the monotonicity of g1 we conclude
λεU1FL1 6= λεCFL. By Lemma 10, we have maxε0,ε1 UFL(ε0, ε1; ε) < CFL(ε) for all ε ≤ τ.
When ε ≥ g(g1-1(0)), applying (11) we have
gi(0) > εi = gi(λ) = gi(g-1(ε)) ≥ gi(g-1(g(gi-1(0)))) = 0,
where the first inequality comes from Case 1. Thus, λi = g-1(εi) = λ as desired, and we obtain
f UFL = f CFL
fε0,ε1 = fε .
Combining both cases above yields the desired conclusion.	口
22
Under review as a conference paper at ICLR 2022
Remark 7. Recall that we use fiεi to denote the local classifier trained by client i in UFL analysis.
In the expression of τ :
min {∣g(O)l, max{sign(g(0))g(g-1(0)), sign(g(0))g(g-1(0))}},
|g(0)| is the demographic disparity of ERM, sign(g(0))g(g0-1(0)) is the demographic disparity of
local classifier f00, and sign(g(0))g(g1-1 (0)) is the demographic disparity of local classifier f10.
According to the proof of Lemma 13, we obtain max{sign(g(0))g(g0-1(0)), sign(g(0))g(g1-1(0))} >
0 if and only if two local classifiers which achieves perfect local fairness is equal, i.e., f00 = f10.
Therefore, Lemma 13 implies that, if the ERM is favoring the same group on different clients and the
two local classifiers which achieve perfect local fairness are unequal, then UFL performs strictly
worse than CFL when the required demographic disparity is smaller than a certain value.
So far We assume a 〜Bern(q) for both client 0 and client 1. When both clients do not share the
same q, we can conclude that CFL outperforms UFL in the following lemma.
Lemma 14. Assume a 〜 Bern(qi) in client i, and qo = qι	∈	(0,1). Then
maxε0,ε1 UFL(ε0, ε1; ε) < CFL(ε) for all ε < |g(0)|.
Proof. We assemble the dataset from two clients to have X | a = a 〜Pa = ^ɪ^ Pθ0 + ^ɪ^Pa1,
a = 0,1. We let f;CFL be the solution to CFL(ε), with q = q0++ q1:
fεCFL = Js(x, a) > 0K,
where s(x, 0) = 2η(x) - 1 +
λCFL
s(x, 1) = 2η(x) - 1----ε—
Given an UFL classifier f^U^1 = (ff0 + fε1 )/2 such that DP DispfUFLj ≤ ε, the solution reads
fiεi = Jsi (x, a) > 0K,
λUFLi	λUFLi
where si(x, 0) = 2η(x) — 1 + —i—, si(x, 1) = 2η(x) — 1 — —i—.
1	- qi	qi
We prove the lemma by contradiction argument. If Acc(fεU0F,εL1 ) = CFL(ε), then fεU0F,εL1 is a solution
to CFL(ε) with q = q0++ q1. Since ε < |g(0)|, by Lemma 7 we have λCFL = 0. Without any loss of
generality, assume λεCFL < 0. Below we discuss three cases.
Case 1.	λεUFL0 = λεUFL1 = 0: the UFL classifier is ERM.
In this case,	f00	=	ff1.	We have	fU* (x,	0) = 1 for	η(x)	>	ɪ,	and	fCFL	= 0 for	η(x)	<
(1 — λCFL∕(1 — q))∕2. By Lemma 6, fUL is not a solution to CFL(ε).
Case 2.	λεU0FL0 6= 0 or λεU1FL1 6= 0, and λεU0FL0 λεU1FL1 = 0: the UFL classifier is not ERM, but one of
the local classifier is ERM.
Without any loss of generality, let λUFL0 = 0, λUFL1 < 0. Then f00 (x, 0) = 1 for η(x) > ɪ, while
f1ε1 (x, 0) = 0 for η(x) < (1 — λεU0FL0 (1 — q1))/2. Thus we get
fUFLi(x, 0) = 2for 2 < η(X) < (I- λUFL1(1 — qi))/2.
By Lemma 6, fεU0F,Lε1 is not a solution to CFL(ε).
Case 3.	λεU0FL0 λεU1FL1 6= 0: The local classifiers are not ERM.
λUFL0	λUFL1	λUFL0	λUFL1
When ɪ-0^ = ι-q^, without loss of generality, let ɪ-0^- > ɪ-^. Then by the same argument in
Case 2, we have
UFL	UFL
-	1	1 — λε0-	1 —工
fUFL1 (x, 0)= 2 for —产 < η(x) < —2—1
23
Under review as a conference paper at ICLR 2022
By Lemma 6, fεU0F,Lε1 is not a solution to CFL(ε).
λUFL0	λUFL1
When -0^- = -1^-, similarly, fUFLι is not a solution to CFL(ε).
λUFL0
When S
λUFL1	λUFL0	λUFL1
Hqh and + = ι⅛，since λUFL0 λUFL1 羊 0, we have
which leads to -U^
λε0 0
q0	q1	1 - q0	1 - q1
------------------ ------------------
∖UFLo	∖UFLι ,	∖UFL0	∖UFLι ,
λε0	λε1	λε0	λε1
-u⅛lt and thus qo = q1. This contradicts with the assumption that qo = q1.
λε1 1
Combining all three cases yields desired conclusion.
□
A.3 FFL via FedAvg analysis
In this section, we analyze FFL via FedAvg for the case of two clients. For purpose of illustration,
with I = 2, we denote fεF0F,Lε1via FedAvg = fεFFL via FedAvg to be the solution to FFL via FEDAVG(ε). In
Sec. A.3.1, we present a formal version of Thm. 3 and show that compared to UFL, FFL via FedAvg
has strictly higher fairness. In Sec. A.3.2 we derive the solution to FFL via FEDAVG(ε), and show it
is equivalent to FFL via FedAvg. In Sec. A.3.3, we analyze the limitation of FFL via FedAvg and
present a formal version of Lemma 4.
A.3.1 Improve fairnes s via federated learning
Different to UFL, FFL via FEDAVG can reach any ε demographic disparity:
Theorem 15 (Formal version of Thm. 3 under two clients cases). Let q ∈ (0, 1). For all ε ∈ [0, 1],
there exists εo, ε1 ∈ [0, 1] such that DP Disp(fεF0F,Lε1via FedAvg) ≤ ε. Thus under the condition in
Lemma 8, we have
min DPDisp(fεU0F,εL1)> min DP Disp(fεF0F,Lε1via FedAvg) = 0.
ε0,ε1∈[o,1]	ε0,ε1 ∈[o,1]
Proof. For any ε ∈ [0, 1], let εo = ε1 = ε. Then the global DP disparity becomes
DP Disp(fεF0F,Lε1via FedAvg) = |Ex|a=of(x,0)-Ex|a=1f(x,1)|
= |(Z f (x, 0) dPo(o) +Z f (x, 0) dPo(1))/2
XX
-(Z f(x,1)dP1(o)+Z f (x, 1) dP1(1))/2|
XX
=MDofFLviaFedAVg )/2 + MDι(fFF,1εViaFedAVg)/2|
≤ DP Dispo(fεF0F,Lε1via FedAvg)/2 + DP Disp1(fεF0F,Lε1via FedAvg)/2
≤ (ε0 + ε1)/2 = ε∙
□
A.3.2 The best classifier of FFL via FedAvg
For FFL Via FedAvg, we directly consider multi-client cases. To Visualize the gap between UFL,
FFL Via FedAvg, and CFL, in our numerical experiments, we draw finite samples from Gaussian
distribution, and then we optimize the empirical risk with the fairness constraint to obtain the classifier
trained by FFL Via FEDAVG. The following lemma proVides the solution to FFL Via FEDAVG(ε)
when X is finite.
Lemma 16. For finite X, the solution to FFL via FEDAVG(ε) is given by
f(x, a) = JX si(x,a)p(ai)(x) > 0K,
i∈[I]
where Si (x, a) = 2η(x) 一 1 + Iλ% Ja=OK 一 Iλ% Ja=IK ,for certain λo,... λι-ι ∈ R.
24
Under review as a conference paper at ICLR 2022
Proof. This proof is based on Menon & Williamson (2018). The key idea of this proof is to use
the Lagrangian approach. Before we applying the Lagrangian approach, we will show that FFL via
FEDAVG(ε) is expressible as a linear program, and thus the strong duality holds.
Since X is finite, f is a vector of finite dimension. Based on the proof of Lemma 5, the error rate can
be written as
P(^ = y)= X f(x, a)(1- 2η(x))P(x = x, a = a) + P(y=1),
x∈X ,a∈A
and the fairness constraints can be written as
P(y=1 | a = 0, i = i) - P(y=1 | a=1, i = i)
L ∖((	∩∖P(X = X | a = 0, i = i) " ι∖P(X = X |	a=1, i = i厂
=工 f(x，0) —P(α=o)-----------------f (x，1) —PF=I-------------J，
for	i	∈	[I]. Let u(X, a)	= (1 - 2η(X))P(x = X, a = a), u0 = P(y =	1), and vi(X, a)	=	(Ja	=
oK	-	Ja	=	1K)P(x = X |	a = a, i = i)/P(a = a), for X ∈ X, a ∈ A, i ∈	[I]. Note that u,	v0,	v1	are
vectors of the same dimension of f . For ease of notation, we allow ≤ to be applied to pairs of vectors
in an element-wise manner. Therefore, the optimization is
min u> f + u0
s.t. vi>f ≤ εi
o≤f≤1,
which is a linear objective with linear constraint. Therefore, the strong duality holds for FFL via
FEDAVG(ε). Next, we apply Lagrangian approach to solve the FFL via FEDAVG(ε).
Recall that
P(^ = y) = I EaEx 〜Pai) f (x, a)(1 - 2η(χ)) + P(y=1),
and
P(^=1 | a = 0, i = i) - P(^=1 | a=1, i = i)
=EaEx〜Pa，)]f(x, O) j⅛0K - /MD —:
By strong duality, for λ00 , . . . , λ02I-1 ≥ o, the corresponding Lagrangian version of FFL via FE-
DAVG(ε) is
芈乎P(^ = y)- X [λ2∕P(^=1 | a = 0, i = i) - P(^ = 1 | a=1, i = i) - j]	(12)
f∈F	i∈[I]
+ λ2i+ι[εi - P(^=1 | a = 0, i = i) - P(^ = 1 | a = 1, i =川
Let λi = λ02i - λ02i+1, i ∈ [I], then we get
(12)	=mF EaEx 〜Pai) If(x, a)(1 - 2η(x)) - X (λif(x, 0) JI=-0K - λif (χ, 1)Ja = IK)]
=m∈mɪ x - 1 f(χ,a)[ XSi(X,a)pαi)(χ)]dχ.
where si is defined in Lemma 16. Thus the above equation reaches its minimum at
f(X, a) = JX Si(X,a)p(ai)(X) > 0K.
i∈[I]
□
25
Under review as a conference paper at ICLR 2022
Remark 8. Based on the proof of Lemma 16, FFL via FEDAVG(ε) is equivalent to solving
I-1
minP(^ = y) - X%(P(^=1 | a = 0, i = i) - P(^ = 1 | a = 1, i = i)).
f∈	i=0
Under certain conditions (assumptions 1 to 4 in Li et al. (2020b)), we have solving FFL via
FEDAVG(ε) is equivalent to minimizing
P(^ = y | i = i) - λi(P(^ = 1 | a = 0, i = i) - P(^ = 1 | a = 1, i = i))
locally and applying FEDAVG (Theorem 1 in Li et al. (2020b)).
A.3.3 Comparison of FFL via FedAvg and CFL
Lemma 17 (Formal version of Lemma 4 under two clients cases). When a | i = 0 〜Bern(0), a |
i = 1 〜Bern(DandDPDisp(fCFL) > 0, we have
min DP Disp(fεFFLε via FedAvg) = DPDisp(f1CFL) > minDPDisp(fεCFL) = 0.
ε0,ε1	0, 1	ε
Proof. Since a | i = 0 〜Bern(0), a | i = 1 〜 Bern(1), the constraints in FFL via FEDAVG(ε)
vanish. When ε = 1, the constraint in CFL(ε) always holds and thus also vanishes. Thus in such
scenario the solution to FFL via FEDAVG(ε) becomes f1CFL. Then from the assumption we have
DPDisp(fεFF,LεviaFedAvg) = DP Disp(f1CFL) > DPDisp(f0CFL) = 0.
□
A.4 Extension to multi-client cases
In this subsection, we perform the analysis of UFL and FFL via FedAvg for the multi-client cases.
We present a more general version of Lemma 8, Thm. 15 and Lemma 17.
The following lemma shows the fundamental limitation of UFL:
Lemma 18 (Formal version of Lemma 1). Let q ∈ (0, 1). Consider a partition which divides I
clients into two subsets. Denote the mixture distribution ofeach subset as X | a = a, j = j 〜 钝j,
where j is the index of the subset, and Paj) is a distribution, for a, j = 0,1. Similar to two clients
case, define gj (λ)=九——— λ )+∞) dP0j) — 九—1(1 + λ)+∞) dP(j). Consider the case that
qi = qfor all i ∈ [I] and q ∈ (0, 1). Denote the proportion of the two subset as J0 and J1, where
T T	C	J T . T	-t T	♦	r∣~∕c∖ll~∕c∖l1 I ʌ /`	I 「c 1.. 「c 1	Γ -t 11
J0,J1 > 0 and Jo + Ji = 1. Let C = min {∣go(0)∣, ∣gι(0)∣}. Define ψ : [0, c] X [0, c] → [-1,1] as
ψ(ε⅛,巨 i) = Jo JMo(g-i(sign(gι(0))包))+ Jo Jιgι(g-i(sign(go(0))巨 o))
+ J2 sign(go(0))εo + J2 sign(gi(0))巨i.
Ifthere exists a partition such that go(0)gι(0) < 0 and ψ(εo, ει)(go(0) + gι(0)) > 0 for all εo,ει ∈
[0, c], then for all ɛo, ɛi ∈ [0,1], DP Disp (fUFL) ≥ δ = min{∣ψ(ε⅞, ɛi)l : ɛo, ɛi ∈ [0, c]} > 0.
Proof. By Lemma 8, we conclude the achievable fairness range of UFL is strictly smaller than that
of CFL. Therefore, pooling the datasets in one subset and perform fair learning can clearly achieve a
wider range of fairness than perform fair learning on each client individually. Thus, we can consider
the two subsets as two clients with uneven amounts of data, which is almost the same case Lemma 8
considers. Therefore, we follow the same proof idea as Lemma 8 to prove our claim.
Denote the assembled classifier trained from two pooled datasets as f~^ι. Note that the mean
difference can be expressed as
MD(fUFLI) = J2go(NU:L0) + JoJigo(NUFLI) + Jo Jigi OUFL0) + J2gi(λUFLI).	(13)
In the following proof, we will show, the mean difference cannot reach 0.
26
Under review as a conference paper at ICLR 2022
Without any loss of generality, assume ∣g0 (0)| < ∣gι(0)∣ and %(0) > 0. By g0(0)g1(0) < 0 and
~	,	,	.	，一	-r	，、	~	,	..
ψ(ε0, ε1)(g0(0) + 91 (0)) > 0 for all ε0, ει ∈ [0, c], we have go(0) < 0 and ψ(ε0, ει) > 0 for all
ε0,ε1 ∈ [0, c]. Without any loss of generality, assume J0 ≤ Ji.
First, we will prove that UFL achieves its lowest mean difference when ε0, ɛi ∈ [0, c]. In what
follows, we consider five different cases to derive the desired result.
Case 1. £0 > ∣g0(0)∣, ɛi > ∣(∕ι(0)∣: ERM isfair on both clients.
By (6), we have 入UFLO = AUFL1 = 0. Recall gi(∙) is a monotone increasing function, combine
gι(0) > 0 and Lemma 7, and thus g0(g-1(0)) < 加(0) < 0. Applying the above conclusion yields
(13)	= j0g0(O) + JIOI(O) > ~r (J200(O) + j2gι(O) + j0j1g0(OII(O)))=〒包加(O),0) ≥ δ∙
Ji	Ji
Case 2. O0 ≤ ∣g0(0)∣,O1 > ∣Oι(0)∣: ERM is unfairon client 0, but fair on client 1.
Applying (6) results in AUFL1 = 0. By the fact that Oi(∙) is a strictly monotone increasing function,
we have OUFLO = g-1(-O0) > g-1(g0(0)) = 0. Applying the above conclusion yields
(13)= - J2O0 + J2g1(0) + J0 J1g0(0) + J0J1O1(OUFLO )
1~,,、、
>J0O0(0) + J1O1(0) > 〒Ψ(O0(0), 0) ≥ O.
J1
In the first equality we used OUFLO > 0,g1(λUFLO) > O1(0), O0(0) < -O0.
Case 3. O0 ≤ ∣g0(0)∣,O1 ≤ ∣O1(0)∣: ERM is unfairon both client 0 and client 1.
Applying (6) we have OUFLO = g-1(-O0), OUFLI = g-1(O1). Then we have
(13) = - joo0 + J2O1 + J0 J100(gr1(O1)) + J0 J1g1(g0-1(-00))
≥ - j000 + J2O0 + JOJ1O0(g-1(OO)) + j0 j1g1(g0^1(-00)) = ψ(00,00)≥ °.
Case 4. Oo > ∣go(0)∣, O1 ≤ ∣Oo(0)∣: ERM isfair on client 0 and very unfairon client 1.
By (6), we have OUFLO = 0, AUFL1 = O-1 (£1) > g-1(0). Then we obtain
(13)	= J2gO(0) + JOJ10o(oUFLI) + JOJIg1(0) + J2O1
> J2go(0) + Jo J1Oo(g-1(0)) + J2O1(0) = ψ(go(0),0) ≥ 0.
Case 5. Oo > ∣go(0)∣, ∣go(0)∣ ≤ O1 < ∣O1(0)∣: ERM isfairon client 0 and unfair on client 1.
Applying (6) implies OUFLI = g-1(O1) > g-1(0). Therefore,
(13)	= JO°O(0) + jOj1gθ(gι 1(OI)) + J2O1 + jO j1g1(0)
> J2go(0) + JoJ1O1(0) + J0J1g0(O-1 (0)) + J2O1 > ψ(go(0),0) ≥ 0.
Combining all the cases above, we conclude that when g1(0) > 0, DPDiSP(想FLI) ≥ 0 =
mιn{∣ψ(εo, ε1 )∣ : εo, ε1 ∈ [0, c]} > 0 for all εo, ε1 ∈ [0,1].
□
Remark 9. Based on the proof above, we can conclude, for the cases with multiple clients, the
fundamental limitation of UFL still exists.
The following theorem shows that FFL via FEDAVG can reach any ε DP disparity:
Theorem 19 (Generalized version of Thm. 15). Let /=q ∈ (0,1) forall i ∈ [I]. Forall ε ∈ [0,1],
let £i ≤ ε for all i ∈ [I], then DP Disp(fFFL via FedAvg) ≤ ε. Thus under the condition in Lemma 18,
we have
mɪnʃ DP Disp(fUFL) > min ʃ DP Disp(fFFL VUaFedAVg) = 0.
27
Under review as a conference paper at ICLR 2022
Proof. When εi ≤ ε the global DP disparity becomes
DP Disp(fεFFL via FedAvg) = |Ex|a=0f(x,0)-Ex|a=1f(x,1)|
I-1	I-1
= |(X	f(x,0)dP0(i))/I- (X
i=0 X	i=0 X
f(x,1)dP1(i))/I|
I-1	I-1	I-1
=| XMDi(fFFLviaFedAvg)/I| ≤ XDPDisPi(fFFLviaFedAVg)/I ≤ X ε"I = ε.
i=0	i=0	i=0
□
The following theorem shows the limitation of FFL via FedAvg:
Lemma 20 (Generalized version of Lemma 17). Let a | i = i 〜Bern(0) or a | i = i 〜Bern(I) for
all i ∈ [I]. When DP Disp(f1CFL) > 0, we have
min DP Disp(fεFFL via FedAvg) = DP Disp(f1CFL) > min DP Disp(fεCFL) = 0.
ε∈[0,1]I	ε	1	ε	ε
Proof. Since a | i = i 〜Bern(0) or a | i = i 〜Bern(1), the constraints in (FFL via FEDAVG(ε))
vanish. When ε = 1, the constraint in CFL(ε) always holds and thus also vanishes. Thus in such
scenario the solution to (FFL via FEDAVG(ε)) becomes f1CFL. Then from the assumPtion we have
DP DisP(fεFFL via FedAvg) = DP DisP(f1CFL) > DPDisP(f0CFL) = 0.
□
B Appendix - FedFB analysis and algorithm description
In this section, we Provide our bi-level oPtimization formulation for FedFB for four fairness notions:
demograPhic Parity, equal oPPortunity, equalized odds and client Parity, and design the corresPonding
uPdate rule. This develoPment can also be aPPlied to centralized case. Then, we Provides more
details of how we incorPorate FB with federated learning.
To exPlain how to oPtimize the weights of different grouPs, we introduce some necessary notations
first. Denote the kth samPle as (xk, yk, ak, ik), k = 1, . . . , n. Here xk ∈ X is the inPut feature,
yk ∈ {0, 1} is the label, ak ∈ A = [A] is the sensitive attribute and ik ∈ [I] is the index of the
client that the samPle belongs to. A rePresents the total amount of sensitive attribute and I rePresents
the total amount of clients. Denote the number of samPles in grouP a as n?,a := |{k : ak =
a}|. Let ny,a := |{k : yk = y, ak = a}| be the number of samPles in grouP a of label y, and
n(yi,)a := |{k : yk = y, ak = a, ik = i}| be the number of samPles belong to client i of label y
and sensitive attribute a. Define the loss function as '(y, y). Let Ly,a(w) be the empirical risk
aggregated over samples subject to y = y, a = a, i.e., Ly,a(w) := Pfc：yfc=y,afc=a '(yk, Vk)∕ny,a,
where ny,a := |{k : yk = y, ak = a}|. We then define the local version of Ly,a as L(yi,)a(w) :=
Pk:yfc = y,afc = a,ifc=i '(yk，"n!"
B.1 FEDFB w.r.t DEMOGRAPHIC PARITY
To extend Roh et al. (2021) to multiple groups cases, we propose a different bi-level optimization
problem w.r.t demographic parity. This development can also be applied to the centralized setting.
The following proposition gives a necessary sufficient condition for demographic parity, which can
be directly obtained from Roh et al. (2021). For completeness, we also include the proof here.
Proposition 21 (Necessary sufficient condition for demographic parity, Proposition 2 in Roh et al.
(2021)). Consider 0-1 loss: '(y, ^) = Jy = ^K. Let Ly,°(w) := nya Ly,a(w) ,then
-L0,θ(w) + L1,θ(w) + L0,a(w) - L1,a (W) +	00-----0a = 0	(14)
0,0	1,0	0,a	1,a	n?,0	n?,a
for all a ∈ [A] is a necessary sufficient condition for demographic parity.
28
Under review as a conference paper at ICLR 2022
Proof. We denote by P the empirical probability. The demographic parity is satisfied when P(^ =
1 | a = 0) = P(^ = 1 | a = a) holds for all a ∈ [A]. Thus,
P(^ = 1, y = 0 | a = 0) + P(^ = 1, y = 1 | a = 0)
=P(^ = 1, y = 0 | a = a) + P(^ = 1, y = 1 | a = a)
For 0-1 loss, we have '(∣1 一 y|, ∙) = 1 一 '(y, ∙), thus
— X (1 一' (yk, ^k)) + — X	' (yk, yk)
n? 0	n? 0
, k:yk =0,ak =0	, k:yk=1,ak=0
=L X (1 -' (yk, ^k)) +， X	' (yk, ^k).
n?a	n?a
, k:yk=0,ak=a	, k:yk =1,ak =a
ByrePlacing Pk：yk =y,ak =a ' (yk , Vk ) = ny,aLy,a (W), WehaVe
—00 (1 一 L0,0 (W)) +---120L1,0(W)
n?,0	n?,0
=-0a (1 一 L0,a(W)) +----12a- L1,a(w)∙
n?,a	n?,a
□
Remark 10. Proposition 21 provides usa way to measure demographic disparity using group-specific
losses. However, since 0-1 loss is noncontinuous, in practice, we use the “continuous surrogate” of it,
which is cross entropy.
The necessary sufficient condition for demographic parity (14) inspires us to achieve demographic
parity by connecting one group with all the other groups for non-binary sensitive attribute settings.
To achieve demographic parity, we introduce another parameter: λ = (λ0, . . . , λA-1), the weights
attached to samples. We formalize the reweighting task into the following bi-level optimization
problem, where the outer objective function captures the demographic parity criterion:
A-1
mλ∈iΛn Fdp(λ):= mλ∈iΛn X
a=1
一刀0,0(蛆入) + L1,0(wλ) + L0,a(wλ) 一 L1,a(wλ) +
(15)outer
Wλ := arg min L(W, λ)
w
(15)
A-1
argmWn X 卜。L0,a(W) + (2詈 一 λa)L1,a(w)],
a=0
(15)inner
where A = [0,2nn0] × …× [0, 2n?A-I].
We make the following assumption to our loss function to have the decreasing direction in Lemma 22.
Assumption 1./；,。(.)is twice diferentiabIefor all y ∈ {0,1}, a ∈ [A], and
A-1
^X [λaV2L0,a(W) + (2 -η。一 λa W"]。W)] A 0 for all λ ∈ A∙	(16)
。=0
If Ly,a(Wλ) is convex for all y ∈ {0,1}, a ∈ [A], the condition (16) holds unless for all a,
L0 a(∙),L1 a(∙) share their stationary points, which is very unlikely (see Remark 1 in Roh et al.
(2021)).	,
The following lemma provides a decreasing direction of the outer objective function Fdp , which
inspired us to design the update rule of λ.
Lemma 22 (Decreasing direction of Fdp). If Assumption 1 holds, then on the direction μ(λ)=
(μ0(λ),...,μA-1(λ)) where
A-1	n n
μ0(λ) = 一 X ( 一刀0,0(如入)+ l1,0(wa) + l0,。(蛆入)一刀1,。(如入)H ------------^a ),
。=1	0,0	1,0	0,。	1,。	n?,0	n?,。	(17)
μo(λ)= 一刀0,0(如入)+ l1,0(wa) + l0,。(如入)一刀1,。(如入)H	00----0。
,	,	,。	,。	n?,0	n?,。
29
Under review as a conference paper at ICLR 2022
forall a ∈ {1,...,A — 1}, we have μ(λ) ∙ VFdp(λ) ≤ 0, and the equality holds ifonly if μ(λ) = 0.
Proof. We compute the derivative as
-：；()=2 X [( — L0,0(wλ) + L1,0(wλ) + L0,a(wλ) — L1,a(wλ) + ɪɪ00 —
∂λj	a=1	n?,0
(—vl0,0(Wa + vl1,0 (wλ) + VL0,a(Wa — VL1,a(Wλ Di ∂λ .
no,a∖
na (18)
Note that wλ is the minimizer to (15)inner, we have
A-1
X 卜aVL0,a(Wλ) + (2nna — λa)VLl,a(wλ)] = 0.
a=0
We take the λj derivative to the above equation and have
A-1
VL0j (wλ) — VLIj (Wλ) + X 卜 aV2L0,a(Wλ) + (2 nna — λa)V2 L1,
a=0
W=0.
Thus we get
A-1
∂λj
a=0
λaV2L0,a(Wλ) + (2 nna — λa)V2Ll,a (Wλ)])-1 [VL[j (wλ) - VL0j (wλ)].
(19)
Then on the direction μ(λ) given by (17), we combine (18) and (19) to have
μ(λ) ∙V%(λ)
=2( X [( — L0,0(wλ) + L1,0(wλ) + L0,a(wλ) — L1,a(wλ) +	00 —
a=1	n?,0
( — VL00,0(wλ ) + VL01,0(wλ ) + VL00,a(wλ ) — VL01,a(wλ )i
A-1	-1
(X [XaV2L0,a (Wa + (2^na -Xa)V2 L1,a (S)D
a=0
A-1	n
(—X [ ( — L0,0(wλ) + L1,0(wλ) + L0,a (wλ) — L1,a(wλ) + -00 —
a=1	n?,0
( — VL00,0(wλ ) + VL01,0(wλ ) + VL00,a(wλ ) — VL01,a(wλ )i ≤ 0
no,a
n?,a
no,a
n?,a
where We have used Assumption 1, and the equality holds only when μ(λ) = 0.
□
Inspired by Lemma 22, in each communication round t = 0, 1, . . . , we design update rule for λ as:
λat+1) = λat) + kμ(λ(tt)他 μa(λ(t)), for a ∈ [A],	(20)
where αt is the step size.
Now we introduce how clients collaborate to solve the bi-level optimization problem (15).
First, we focus on the outer objective function (15)outer and introduce how clients collaborate to
update the weight λ. Note that the central server can compute L0y,a (Wλ(t) ) by weight-averaging the
local group loss L(yi,)a(Wλ(t) ) sent from clients at communication rounds as
n(i)
Ly,a(Wλ(t)) = X ny^- Lya(W λ(t)),
i∈[I] n?,a
30
Under review as a conference paper at ICLR 2022
thereby obtain μ(λ(t)) and update λ(t) by (20).
Next, we focus on the inner objective function (15)inner and introduce how clients collaborate to update
the model parameters wλ using FEDAVG. Note that we can decompose the objective function L(w, λ)
into L(W, λ) = Pi∈[i] Li)(W,λ), Where L⑴(WD)= PA=oDanOiaLJyn?,。+ (2nna-
1。)n1：：刀1：：/九？,。] is the client objective function of client i. The global objective can be seen as a
Weighted sum of the client objective function. Therefore, We can use FedAvg to solve the inner
optimization problem.
We present the pseudocode of FEDFB w.r.t demographic parity in Algorithm 2.
Algorithm 2: FEDFB w.r.t Demographic Parity
Server executes:
input : Learning rate {αt}t∈N;
Initialize λa as nn,a for all a ∈ [A]\ {0};
for each iteration t = 1, 2, . . . do
Clients perform updates;
Wλ J SecAgg {w(i)} for all i;
Ly,a J SeCAgg {Lyi,a} for all (y, a);
Ly,aJ n?aLy,。for all y ∈ {0,1}, a ∈ [A];
μ0 J- PA-II (-LO,0 + L1,0 + L0,a - L1,a + n?0 - n0a) ;
μa J----L0,0 + L1,0 + L0,a - L1,a + n0-0 - n^, a ∈ [A]\{0};
λa J λa + |激2 μa, for all a ∈ [A];
Broadcast Wλ and λ to clients;
end
output : Wλ
ClientUpdate(i, W, λ):
W(i) J Gradient descent w.r.t objective function
PA=01 [λaL0(,i? (W) + (2nT -入。)赏。(W)i ;
Send w(i), L%*w), LIia(W) for all a ∈ [A] to server via a SecAgg protocol;
Next, We analyze the convergence performance of FedFB. We need to make the folloWing assump-
tions on the objective function L(i) (W, λ), i ∈ [I]. For simplicity, We drop the λ here and use the
notations L(i) (W) and L(W) instead. We use Wt(i) to denote the model parameters at t-th iteration in
i-th client. The assumptions beloW are proposed by Work Li et al. (2020b):
Assumption 2 (Strong convexity, Assumption 1 in Li et al. (2020b)). L(i)(W) is μ-strongly convex
for i ∈ [I], i.e., for all v and W, W, L(i) (v) ≥ L(i) (W) + (v ——)>▽£(Z)(W) + 2 ∣∣v — W∣∣2.
Assumption 3 (Smoothness, Assumption 2 in Li et al. (2020b)). L(i) (W) is L-smooth for i ∈ [I],
i.e.,forall V and W, L(i)(V) ≤ L(i)(W) + (v — w)>VL(z)(w) + LL ∣∣v — W∣2.
Assumption 4 (Bounded variance, Assumption 3 in Li et al. (2020b)). Let ξt(i) be sampled
from i-th device’s local data uniformly at random, where t ∈ [T], and T is the total num-
ber of every client’s SGDs. The variance of stochastic gradients in each device is bounded:
EVL(i)(Wt(i),ξt(i)) - VL(i)(Wt(i))2 < ∞fori ∈ [I].
Assumption 5 (Bounded gradients, Assumption 4 in Li et al. (2020b)). The expected squared norm
of stochastic gradients is uniformly bounded, i.e., E VL(i) (Wt(i), ξt(i)) < ∞ for all i ∈ [I], t ∈ [T].
In FedAvg, first, the central server broadcasts the lastest model to all clients, then, every client
performs local updates for a number of iterations, last, the central server aggregates the local models
to produce the neW global model(see Algorithm Description in Li et al. (2020b) for more detailed
explanation).
31
Under review as a conference paper at ICLR 2022
We denote E as the number local iterations performed in a client between two communications, and
R be the total number of every client's iteration. Thus R is the number of communications.
With the above assumptions, the following theorem shows the convergence of FedFB in the case of
two clients.
Theorem 23. Consider the case of A = 2. Let Assumption 2, 3, 4, 5 on {L(i)(∙,入)}记口 and
Assumption 1 on {Ly,α(∙)}y,a∈{o,i} hold. Choose {ɑt}∞=ι such that limt→∞ at = 0, E∞=ι at = ∞.
Suppose we use FEDAVG with R and E satisfying E → 0 to solve (15)inner between two λ update
rounds. We can find sufficiently large T such that with high probability, applying update rule (20)
leads to ∣λ!T) - λ?| ≤ max {尾0) - λ?| - PT=at, aT → 0, where λ? is the local minimizer on
direction μ(λ).
Proof. We first derive the update rule in the case of A = 2. From (20), We have μo = 一μι. Denote
f(λ⑴)=-L0,o(wλ(t)) + L1,o(w(t)) + L0,ι(wf)) - Ll,ι(w,) + n00 - n01.
n?,0	n?,1
Then the update rule becomes
λ0t+1) = λ0t)-√2 ɑtsign(f(λ㈤))
22	(2i)
λ1t+1) = λ1t) + ɪ atsign(f(λ㈤)).
Then We apply FEDAVG With R number of total iterations to solve minw L(w; λ(t)), and obtain
wR(t). Then in the update round from λ(t) to λ(t+1), by Thm. 1 in Li et al. (2020b), We have
E[L(wR); λ(t))] - L(wλ(t) ； λ⑴)=O(E),
EllwR)- wλ(t)f = O( ER),
Where wλ(t) = arg minL(w; λ(t)). By Markov’s inequality, With probability 1 - δ,
w
L(wR); λ(t)) - L(w*t);入⑴)=O(ER),
IlwR)- wλ(t)ll = O( ER).
Then taking the union bound over T updating iterations of λ(t), the conclusions above hold With
probability at least 1 一 Τδ for all λ(t),t = 1,2,…，T. Therefore, with sufficiently large R =
R(δ) such that ER → 0, for all λ(t) in the T iteration, ∣L(w(t); λ(t)) 一 L(w〃(t); λ(t))∣《1,
kwR(t) - wλ(t)k 1.
By Lemma 22, FdP(λ) has a local minimizer λ? on direction μ(λ). By update rule (21), we can find
large T > 0 to have
∣λaT) - λa∣ ≤ maχ {∣λa°) -1?i - X at,aT} →0,
where a ∈ {0, 1}.
□
Remark 11. Note that Thm. 23 assumes FEDFB does not update λ in each communication round and
there are infinite rounds of aggregations between two λ updating round. However, for computation
efficiency, we update λ at every communication round in practice.
32
Under review as a conference paper at ICLR 2022
B.2 FEDFB w.r.t EQUAL OPPORTUNITY
Similar to Proposition 21, we design the following bi-level optimization problems to capture equal
opportunity:
A-1
min Feo(λ) = min	(L1,a(wλ) - L1,0 (wλ))2
a=1
A-1	A-1
Wλ = arg min X λ°Lι,a(w) + (上一X λ°)Lι,o(W) + 0?LLo,?(w).	(22)
wn	n
a=1	a=1
Here Λ = {(λι,..., λa-ι) : λι +-+ λa-ι ≤ nn,?, λa ≥ 0 for all a = 1,...,A - 1}.
For equal opportunity, we make the following assumption:
Assumption 6. Ly,a(∙) is twice differentiablefor all y ∈ {0,1}, a ∈ [A], and
A-1	A-1
X λa V2Ll,a (wλ) + ( ~n? - X λa)V2 L1,0(wλ) +-n? V2L0,*(wλ) » 0
a=1	a=1
for all λ ∈ Λ.
With the above assumption, the following lemma provides the update rule:
Lemma 24. If Assumption 6 holds, then on the direction
μ(λ) = (L1,1(wλ) — Lι,o(wλ),...,Lι,A-ι(wλ) — Lι,o (wλ)),	(23)
we have μ(λ) ∙ VFeo(λ) ≤ 0, and the equality holds ifonly if μ(λ) = 0.
Update rule for equal opportunity:
λ(at+1) = λ(at) +
W⅛(Lι,a (Wλ(t"-
L1,0(wλ(t))).
Proof of Lemma 24. We compute the derivative as
—N ) = 2 X (L1,a(wλ) — L1,0(wλ))(vL1,a(wλ) — vL1,0(wλ)) 内 J .
∂λj	a=1	∂λj
(24)
Note that Wλ is the minimizer to (22), we have
A-1	A-1
^X λavL1,a(wλ) + ( 1,? — ^X λa)vL1,0(wλ) +-?vL0,*(wλ) = 0∙
nn
a=1	a=1
We take the λj derivative to the above equation and have
A-1	A-1
[X λav2Ll,a(Wλ) + (nn? — X λa)v2Ll,o(Wλ) +	v2Lo,*(Wλ)]
a=1	a=1
∂Wλ
vLι,j (wλ) — vL1,0(wλ) + ɪ- = 0.
∂λj
Thus we get
A-1
∂Wλ=hX
a=1
λav2L1,a(wλ) + ( ~n^
A-1
—SXJ λa)v2L1,0(wλ) H---n? v2L0,? (wλ)i
a=1
(25)
[vL1,0(Wλ) — vL1,j(Wλ)].
33
Under review as a conference paper at ICLR 2022
Then on the direction μ(λ) given by (23), We combine (24) and (25) to have
μ(λ) ∙VFeo(λ)
A-1
= 2 X(L1,a(wλ) - L1,0(wλ))(VL1,a(wλ) - VL1,0(wλ))
a=1
A-1	A-1	-1
[X λaV2Ll,a(Wλ) + (nnn? - X λa)V2Ll,o(Wλ) + 号V2Lo,*(Wλ)]-
a=1	a=1
A-1
X(L1,a(wλ) - L1,0(wλ))(VL1,0(wλ) - VL1,a(wλ)) ≤ 0,
a=1
where we have used Assumption 6, the equality holds only when μ(λ) = 0.
□
We present the FEDFB algorithm w.r.t equal opportunity in Algorithm 3.
Algorithm 3: FEDFB w.r.t Equal Opportunity
Server executes:
input : Learning rate {αt}t∈N;
Initialize λ° as nna for all a ∈ [A]\ {0};
for each iteration t = 1, 2, . . . do
Clients perform updates;
Wλ J SecAgg {w(i)} for all i;
Ly,a J SeCAgg {Lyi,a 0 for all y ∈ {0,1},a ∈ [A]∖{0};
μa J L1,a - L1,0, a ∈ [A]\ {0};
λa J λa + ɪθkk2μa, for all a ∈ [A]\ {0};
Broadcast wλ and λ to clients;
end
output : wλ
ClientUpdate(i, w, λ):
w(i) J Gradient descent w.r.t objective function
PA-I λaLiia(w)+( nn? - PA-I λa)Lii,0(w) + nn? L0i?(w);
Send w(i), L(0i,)a(w), L(1i,)a(w) for all a ∈ [A] to server via a SecAgg protocol;
B.3 FEDFB w.r.t EQUALIZED ODDS
For equalized odd, we design the following bi-level optimization problem:
Here	A-1 min Feod(λ) = min	(L1,a(wλ) - L1,0(wλ))2 + (L0,a(wλ) - L0,0(wλ))2 a=1 A-1 wλ = arg mwin	(λ0,aL0,a (w) + λ1,aL1,a (w)) a=1 A-1	A-1 + ( —0? - ^X λ0,a )L0,0(W) + ( —1? - ^X λ1,a)L1,0(w).	(26) nn a=1	a=1
Λ=	A-1	A-1 二{(λ0,1,..∙, λ0,A-1, λ1,1, ..., λ1,A-1) : ^X λ0,a ≤ "? ,	^X λ1,a ≤	"* , nn a=1	a=1
λ0,a, λ1,a ≥ 0, for all a = 1, . . . ,A - 1}.
34
Under review as a conference paper at ICLR 2022
For equalized odds, we make the following assumption:
Assumption 7. Ly,a(∙) is twice differentiablefor all y ∈ {0,1}, a ∈ [A], and
A-1	A-1
X h(λ1,aV2Ll,a(wλ) + λ0,a^i2 L0,a(wλ) + ( -^2? - X λ0,a ) V2 L0,0 (wλ)
a=1	n	a=1
A-1
+ (」？ - X λ1,a)V2Lι,0(wλ))] » 0
n	a=1
for all λ ∈ Λ.
With the above assumption, the following lemma provides the update rule:
Lemma 25. If Assumption 7 holds, then on the direction
μ(λ) = (μo,ι(λ),...,μo,A-i(λ),μι,ι(λ),...,μι,A-i(λ)), With
μy,a(λ) = Ly,a(wλ) - Ly,0(wλ),	y ∈ {0, 1}, a∈ [A],	(27)
we have μ(λ) ∙ VFeod(λ) ≤ 0, and the equality holds ifonly if μ(λ) = 0.
Update rule for equalized odd:
λy+1) = λyt,a + kμ(λ1))k2 (Ly,a(W入⑴ ) - Ly,0(wλ(t) )) for y ∈ {0, 1}, a ∈ [A].
Proof of Lemma 25. We compute the derivative as
∂Feod(λ)
dλy,j
A-1
2 X (L1,a(wλ) - L1,0(wλ))(VL1,a(wλ) - VL1,0(wλ))
a=1
+ (L0,a(wλ) - L0,0(wλ))(VL0,a(wλ) - VL0,0(wλ))
∂ Wλ
dλy,j
(28)
Note that wλ is the minimizer to (26), we have
A-1
(λ0,aVL0,a(wλ) + λ1,aVL1,a(wλ))
a=1
A-1	A-1
+(—o? - X λ0,a)vL0,0(wλ)+(—1n,? - X λ1,a)vL1,0(wλ)=0.
a=1	a=1
We take the λy,j derivative to the above equation and have
A-1
vLy,j(wλ) - vLy,0 (wλ) + X(λ0,av2L0,a(wλ) + λ1,av2L1,a(wλ))
a=1
A-1	A-1
+ (-0? - X λ0,a)v2L0,0(wλ) + (^,? - X λ1,a )v2L1,0(wλ)i ∂λ λ = 0.
n	n	∂λ
a=1	a=1	y,j
Thus we get
∂Wλ
dλy,j
A-1
=	X(λ0,av2L0,a(wλ) + λ1,av2L1,a(wλ))
a=1
A-1	A-1
+ (等 - X λ0,a)V2L0,0(Wλ) + (詈 - X λι,a)V2Ll,o(Wλ)]
a=1	a=1
[vLy,0(wλ) - vLy,j(wλ)].
(29)
35
Under review as a conference paper at ICLR 2022
Then on the direction μ(λ) given by (27), We combine (28) and (29) to have
μ(λ) ∙ VFeod (λ)
A-1
= 2 X [(L1,a(wλ) - L1,0(wλ))(VL1,a(wλ) - VL1,0(wλ))
a=1
+ (L0,a(wλ) - L0,0(wλ))(VL0,a(wλ) - VL0,0(wλ))]
A-1
X(λ0,aV2L0,a(wλ)+λ1,aV2L1,a(wλ))
a=1
A-1	A-1	-1
+ (—0? — ^X λ0,a)v2L0,0(wλ) + (-1,? — ^X λ1,a)V2Ll,0(Wλ)]
n	a=1	n	a=1
A-1
X [(L1,a(wλ) — L1,0(wλ))(VL1,0(wλ) — VL1,a(wλ))
a=1
+ (L0,a(wλ) — L0,0(wλ))(VL0,0(wλ) — VL0,a(wλ))]i ≤ 0
where we have used Assumption 7, and the equality holds only when μ(λ) = 0.
□
The full procedure is described in Algorithm 4.
Algorithm 4: FEDFB w.r.t Equalized Odds
Server executes:
input : Learning rate {αt}t∈N;
Initialize λy,a as nna for all y ∈ {0,1} , a ∈ [A]\ {0};
for each iteration t = 1, 2, . . . do
Clients perform updates;
Wλ J SecAgg {w(i)} for all i;
Ly,a J SeCAgg {Lyi,a O for all y ∈ {0,1},a ∈ [A]∖{0};
μy,a J Ly,a - Ly,0 for all Iy ∈ {0, 1} , a ∈ [A]\{0};
λy,a J λy,a + kμkk2μy,a, for all a ∈ [A\ {0};
Broadcast wλ and λ to clients;
end
output : wλ
ClientUpdate(i, w, λ):
w(i) J Gradient descent w.r.t objective function PaA=-11 λ0,aL(0i,)a(w) + λ1,aL(1i,)a(w) +
(nn? - PA=II λ0,a)L0i0(w) + (nn? - PA=11 λ1,a)Lii,0(w);
Send w(i), L(0i,)a(w), L(1i,)a(w) for all a ∈ [A] to server via a SecAgg protocol;
B.4 FEDFB w.r.t CLIENT PARITY
For client parity, we slightly abuse the notation and define the loss over client i as L(i) (w) :=
Pik=i '(yk, ^k)/n(i), with n(i) = |{k : ik = i}|. Note that the L(i) (w) here is different from the
one in Sec. B.1. We design the following bi-level optimization problem:
min Fcp (λ) = min X L(i) (wλ) — L(0) (wλ)2 ,
∈	∈ i=1
I1	I1
wλ
arg min	λ(i)L(i) (w) + (1 —	λ(i))L(0) (w).
(30)
w
i=1
i=1
36
Under review as a conference paper at ICLR 2022
Here
I-1
Λ = {(λ⑴,…,λ(IT)) : 0 ≤ λ⑴ ≤ 1,	Xλ⑴ ≤ 1}.
i=1
For client parity, we make the following assumption:
Assumption 8. L(i)(∙) is twice differentiablefor all i = 1,...,I 一 1, and
I-1	I-1
X λ⑴V2L⑴(wλ) + (1 一 X λ(i))V2L⑼(wλ) A 0.
i=1	i=1
With the above assumption, the following lemma provides the update rule:
Lemma 26. If Assumption 8 holds, then on the direction
μ(λ) = (L⑴(wλ) - L(0)(wλ),…，L(I-1)(wλ) - L(0)(wλ)),	(31)
we have μ(λ) ∙ VFcp(λ) ≤ 0, and the equality holds if and only if μ(λ) = 0.
Then we update λ(t) = (λ(1)(t), . . . , λ(I -1)(t)) as follows:
Update rule for client parity:
λ⑺(t+1) = λ⑴(t) +	a(mn (L(i)(Wλ(t)) 一 L(0)(Wλ(t))) for i = 1,∙∙∙,I 一 L
kμ(Xt))k2
Proof of Lemma 26. We compute the derivative as
dFpjλ) =(2X[L⑴(Wλ) - L⑼(wλ)][VL⑺(Wλ) - VL⑼(Wλ)])弟.	(32)
i=1
Note that wλ is the minimizer to (30), we have
I-1	I-1
Xλ(i)VL(i)(wλ)+(1 一 X λ(i))VL(0)(wλ) =0.
i=1	i=1
We take the λ(j) derivative to the above equation and have
I-1	I-1
VLj)(wλ) + X λ⑴V2L⑴(wλ)d∂Wλ 一 VL(0)(wλ) + (1 一 X λ⑺)V2L⑼(wλ)警=0.
∂ λ	∂ λ
i=1	j	i=1	j
Thus we get
I-1	I-1	-1
渭=[XX(i)V2L⑴(wλ) + (1-Xλ⑺)V2L⑼(wλ)]	[VL⑼(wλ-VLj)(W入)].(33)
i=1
i=1
Then on the direction given by (31), we combine (32) and (33) to have
μ(λ) ∙VFcp(λ)
I-1
=2(X[L⑴(wλ) - L(0)(wλ)][VL⑴(wλ) - VL⑼(wλ)])
i=1
I-1	I-1	-1
hXλ(i)V2L(i)(wλ) + (1 一 Xλ(i))V2L(0)(wλ)i
i=1	i=1
I-1
X ([L⑴(wλ) - L⑼(wλ)][VL⑼(wλ) - VL(i)(wλ)]) ≤ 0,
i=1
where We have used Assumption 8, and the equality holds only when μ(λ) = 0.
□
The Algorithm 5 gives the full description.
37
Under review as a conference paper at ICLR 2022
Algorithm 5: FEDFB w.r.t Client Parity
Server executes:
input : Learning rate {αt}t∈N;
Initialize λ(i) as 誓 for all i ∈ [I]\ {0};
for each iteration t = 1, 2, . . . do
Clients perform updates;
wλ J SecAgg w(i) for all i;
μ(i) J L⑴一L(0),i ∈ [I]∖{0};
λ(i)
J λ⑴ + 诜〃⑴，for all i ∈ [I]∖{0};
Broadcast wλ and λ to clients;
end
output : wλ
ClientUpdate(i, w, λ):
w(i) J Gradient descent w.r.t objective function
Ji 6= 0Kλ(i)L(i)(w) + Ji = 0K(1 - PjI=-11 λ(ij)L(0)(w);
Send w(i) to server via a SecAgg protocol; Send L(i)(w) to server;
C Appendix - Experiments
We continue from Sec. 5 and provide more details on experimental settings, and other supplementary
experiment results.
C.1 Experiment setting
The reported statistics are computed on the test set, and we set 10 communication rounds and 30 local
epochs for all federated learning algorithms except AgnosticFair. We run 300 epochs for other
methods. For all tasks, we randomly split data into a training set and a testing set at a ratio of 7:3. The
batch size is set to be 128. For all methods, we choose learning rate froom {0.001, 0.005, 0.01}. We
solve FEDFB with sample weight learning rates α ∈ {0.001, 0.05, 0.08, 0.1, 0.2, 0.5, 1, 2} in parallel
and select the α which achieves the highest fairness. All benchmark models are tuned according to
the hyperparameter configuration suggested in their original works. We perform cross-validation on
the training sets to find the best hyperparameter for all algorithms.
C.2 Synthetic dataset generation
We generate a synthetic dataset of 5,000 examples with two non-sensitive attributes (x1, x2), a binary
sensitive a, and a binary label y. A tuple (x1, x2, y) is randomly generated based on the two Gaussian
distributions: (xι,x2) | y 〜N([-2; -2], [10,1; 1, 3]) and (xι, x2) | y = 1 〜N([2;2],[5,1]),
where y 〜Bern(0.6). For the sensitive attribute a, We generate biased data using an unfair scenario
px1 ,x2 ((x01 , x02 ) | y = 1)/[px1 ,x2 ((x01, x02) | y = 0) + px1 ,x2 ((x01 , x02 ) | y = 1)], where px1 ,x2 is
the pdf of (x1 , x2 ). We split the data into three clients in a non-iid way. We randomly assign
50%, 30%, 20% of the samples from group 0 and 20%, 40%, 40% of the samples from group 1 to
1st, 2nd, and 3rd client, respectively. To study the empirical relationship between the performance
of FedFB and data heterogeneity, we split the dataset into other ratios to obtain the desired level
of data heterogeneity. To get the dataset with low data heterogeneity, we draw samples from each
group into three clients in a ratio of 33%, 33%, 34% and 33%, 33%, 34%. For dataset with high data
heterogeneity, the ratio we choose is 70%, 10%, 20% and 10%, 80%, 10%.
C.3 Supplementary experiment results
Fig. 8 shows the accuracy versus fairness violation for the 3 clients’ cases. We see a clear advantage
of FFL via FedAvg over the UFL in both 2 clients’ cases and 3 clients’ cases. The achievable
fairness range of FFL via FedAvg is much wider than that of UFL, though the accuracy of FedAvg
is not guaranteed when the data heterogeneity increases.
38
Under review as a conference paper at ICLR 2022
DP Disparity
Figure 8: Accuracy-Fairness tradeoff curves of CFL, FFL via FedAvg, and UFL for three clients cases.
The data heterogeneity is increasing from left to right. The green dotted vertical line describes the lower bound
of unfairness FFL via FedAvg can achieve, and and the orange dotted vertical line describes the lower bound
of unfairness UFL can achieve. Here the distribution setting is X | a = 0, i = 0 〜N(3,1),x|a=1,i=
0 〜N(5,1), x | a = 0, i = 1 〜N(1,1), X | a = 1, i = 1 〜N(—1,1), X | a = 0, i = 2 〜N(1,1), X |
a = 1, i = 2 〜N(2,1),a| i = i 〜Bern(qi) for i = 0,1, 2. The data heterogeneity here is captured by
|q2 — q0|. (a) q0 = q1 = q2 = 0.5. (b) q0 = 0.4, q1 = 0.5, q2 = 0.6. (c): q0 = 0.3, q1 = 0.5, q2 = 0.7. (d):
q0 = 0.2, q1 = 0.5, q2 = 0.8.
Table 4: Comparison of accuracy and fairness in the synthetic, Adult, COMPAS, and Bank datasets w.r.t
demographic parity (DP) on logistic regression. The implementation of UFL, FFL via FEDAVG, and CFL are
all based on FB.
Method	Synthetic		Adult		COMPAS		Bank	
	ACC.(↑)	DP DISP.(φ)	ACC.(↑)	DP DISP(J)	ACC.(↑)	DP DISP.(J)	ACC.(↑)	DP DISP.(J)
FedAvg	.884±.001	.419±.006	.837±.007	.144±.015	.658±.006	.149±.022	.900±.000	.026±.001
UFL	.712±.198	.266±.175	.819±006.	.032±.031	.606±.018	.089±.058	.888±.005	.008±.007
FFL via FedAvg	.789±.138	.301±.192	.828±.002	.098±.008	.560±.000	.030±.002	.892±.000	.013±.000
FedFB	.756±.001	.085±.001	.820±.000	.002±.001	.550±.000	.009±.000	.890±.001	.011±.002
AgnosticFair	.622±.051	.028±.008	.768±.000	.003±.000	.568±.018	.034±.023	.883±.000	.000±.000
CFL	.662±.039	.077±.020	.810±.009	.054±.027	.587±.003	.032±.002	.883±.000	.000±.000
We also compare FEDAVG, UFL, FFL via FEDAVG, FEDFB, AGNOSTICFAIR and CFL on logistic
regression. Table 4 shows that our method outperforms UFL and FFL via FedAvg, while achieving
similar performance as AgnosticFair and CFL but ensuring higher privacy.
C.4 Evaluation of differentially private FedFB
Since FedFB exchanges more information than FFL via FedAvg, we employ differential privacy to
decrease information leakage. We apply the Laplace mechanism to make the information exchanged
in each communication round ε-differentially private. We report the test accuracy and DP disparity
of FFL via FedAvg and FedFB with different differential privacy levels in Table 5. Interestingly,
we observe that a higher level of differential privacy even helps to improve fairness (see FFL via
FedAvg column), though the accuracy is decreased. This phenomenon is to be expected since larger
noise helps to protect sensitive information, thereby improving fairness and lower accuracy. Still,
Table 5 implies that FedFB still outperforms FFL via FedAvg with restrictions on the information
exchange.
C.5 Empirical relationship between accuracy, fairnes s, and the number of
CLIENTS
We investigate the empirical relationship between accuracy, fairness, and the number of clients. We
generate three synthetic datasets of 3,333, 5,000, and 6,667 samples, and split them into two, three,
and four clients, respectively. Table 6 shows that our method outperforms under the three cases.
39
Under review as a conference paper at ICLR 2022
Table 5: Performance comparison under ε-differential private information exchange in each communication
round on synthetic dataset.
FFL via FedAvg	FedFB
ε	Acc.(↑)	DP Disp.(1)	ACC.(↑)	DP Disp.(1)
0.1	.583±.258	.185±.254	.517±.180	.086±.192
1	.582±.261	. 1 89±.249	.508±.191	.103±.188
10	.645±.315	.373±.217	.616±.169	.091±.204
Table 6: Comparison of accuracy and fairness in the synthetic datasets with different numbers of clients
w.r.t demographic parity (DP). The implementation of UFL, FFL via FEDAVG, and CFL are all based on FB.
I Method	TWO CLIENTS		THREE CLIENTS		FOUR CLIENTS	
	ACC.(↑)	DP Disp.(()	ACC. (↑)	DP Disp.(()	ACC.(↑)	DP Disp.(()
FedAvg	.879±.004	.360±.013	.883±.003	.402±.01 8	.879±.003	.382±.005
UFL	.780±.090	.161±.157	.729土 」95	.256±.193	.720±.192	.246±.180
FFL via FedAvg	.866±.002	.429±.017	.746±.307	.478±.079	.608±.367	.491±.102
FEDFB	.679±.007	.047±.012	.613±.007	.011±.009	.705±.004	.005±.003
CFL	.668±.028	.063±.035	.693±.030	.051±.020	.670±.035	.064±.020
C.6 Comparison with FairFed
Table 7: Comparison of accuracy and fairness in the synthetic and Adult dataset w.r.t demographic parity (DP)
on logistic regression.
Method	Synthetic		Adult	
	ACC.(↑)	DP Disp.(Φ)	ACC.(↑)	DPDISP&)
FairFed	.509±.092	.092±.127	.757±.003	.105±.009
FedFB (Ours)	.756±.001	.085±.001	.820±.000	.002±.001
As suggested in Ezzeldin et al. (2021), we use logistic regression to compare our approach with
FairFed. Since FairFed is only applicable to single binary sensitive attribute cases, we report the
performance of FairFed on synthetic and Adult datasets in Table 7. We observe that our FedFB
outperforms FairFed in terms of both accuracy and fairness, while FairFed also outperforms FFL
via FedAvg thanks to the additional client reweighting step.
Table 8: Comparison of accuracy and fairness under the same setting as Ezzeldin et al. (2021). The
statistics of FairFed are from Ezzeldin et al. (2021).
Method	ADULT	COMPAS HETEROGENEITY LEVEL α	HETEROGENEITY LEVEL α 0.1	0.2	0.5	10	5000	0.1	0.2	0.5	10	5000
FairFed ACC.(↑) FEDFB (OURS)	.775 .794 .819 .824	.824	.594 .586 .608 .636	.640 .764 .761 .762 .764	.759	.668 .655 .541 .666	.666
ISPDl(I) FAIRFED ISPDN) FEDFB(OURS)	.021 .037 .061 .065	.065	.048 .040 .072 .108	.115 .003 .003 .003 .000	.009	.009 .017 .000 .050	.006
To make fairer comparison, we follow the exact same setting as Ezzeldin et al. (2021) to re-split Adult
and COMPAS dataset, employ |SPD| = ∣P(^ =1 | a = 0) - P(y =1 | a = 1) | as unfairness metric
and present the results in Table 8. We observe that FedFB achieves higher fairness than FairFed
while being robust to data heterogeneity.
C.7 Comparison with AgnosticFair
Lastly, we compare our method with AgnosticFair (Du et al., 2021), which exchanges the model
parameters and the other information after every gradient update to mimic the performance of CFL
with FairnessConstraint implementation (Zafar et al., 2017c). Table 9 shows that our FedFB
achieves similar performance as AgnosticFair, at much lower cost of privacy.
40
Under review as a conference paper at ICLR 2022
Table 9: Comparison of accuracy and fairness in the synthetic, Adult, COMPAS and Bank datasets w.r.t
demographic parity (DP) on multilayer perceptron.
Method	Synthetic		Adult		COMPAS		Bank	
	ACC.(↑)	DP DlSP.(φ)	ACC.(↑)	DP DlSP.(φ)	ACC.(↑)	DP DlSP.(φ)	ACC.(↑)	DP DISP.(J)
FedFB (Ours)	.613±.007	.011±.009	.765±.001	.001±.001	.542±.001	.001±.001	.883±.000	.000±.000
AgnosticFair	.657±.029	.032±.044	.767±.004	.003±.005	.541±.000	.000±.000	.883±.000	.000±.000
41