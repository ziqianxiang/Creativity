Under review as a conference paper at ICLR 2022
Gaussian Differential Privacy Transforma-
tion: from identification to application
Anonymous authors
Paper under double-blind review
Ab stract
Gaussian differential privacy (GDP) is a single-parameter family of privacy no-
tions that provides coherent guarantees to avoid the exposure of individuals from
machine learning models. Relative to traditional (, δ)-differential privacy (DP),
GDP is more interpretable and tightens the bounds given by standard DP composi-
tion theorems. In this paper, we start with an exact privacy profile characterization
of (, δ)-DP and then define an efficient, tractable, and visualizable tool, called the
Gaussian differential privacy transformation (GDPT). With theoretical property
of the GDPT, we develop an easy-to-verify criterion to characterize and identify
GDP algorithms. Based on our criterion, an algorithm is GDP if and only if an
asymptotic condition on its privacy profile is met. By development of numerical
properties of the GDPT, we give a method to narrow down possible values of an
optimal privacy measurement μ with an arbitrarily small and quantifiable margin
of error. As applications of our newly developed tools, we revisit some established
(, δ)-DP algorithms and find that their utility can be improved. We additionally
make a comparison between two single-parameter families of privacy notions, -
DP and μ-GDP. Lastly, We use the GDPT to examine the effect of subsampling
under the GDP framework.
1 Introduction
Recent years have seen explosive growth in the research and application of data-driven machine
learning. While data fuels advancement in this unprecedented age of “big data”, concern for indi-
vidual privacy has deepened with the continued mining, transportation, and exchange of this new
resource. While expressions of privacy concerns can be traced back as early as 1969 (Miller, 1969),
the concept of privacy is often perceived as “vague and difficult to get into a right perspective”
(Shils, 1966). Through its alluring convenience and promise of societal prosperity, the use of ag-
gregated data has long outstripped the capabilities of privacy protection measures. Indeed, early
privacy protection protocols relied on the ad hoc enforcement of anonymization and offered little
to no protection against the exposure of individual data, as evidenced by the AOL search log and
Netflix Challenge dataset controversies (Narayanan & Shmatikov, 2006; 2008; Barbaro & T. Zeller,
2006).
Differential privacy (DP) first gained traction as it met the urgent need for rigour and quantifiability
in privacy protection (Dwork et al., 2006). In short, DP bounds the change in the distribution of
outputs of a query made on a dataset under an alteration of one data point. The following definition
formalizes this notion.
Definition 1.1 (Dwork et al., 2006) A randomized algorithm A, taking a dataset consisting of indi-
viduals as its input, is (, δ)-differentially private if, for any pair of datasets S and S0 that differ in
the record of a single individual and any event E,
P [A(S) ∈ E] ≤ eP [A(S0) ∈ E]+δ.
When δ = 0, A is called -differentially private (-DP).
While the notion of (, δ)-DP has wide applications (Dankar & El Emam, 2012; Erlingsson et al.,
2014; Cormode et al., 2018; Hassan et al., 2019), there are a few notable drawbacks to this frame-
work. One is the poor interpretability of (, δ)-DP: unlike other concepts in machine learning, DP
1
Under review as a conference paper at ICLR 2022
should not remain a black box. Privacy guarantees are intended for human interpretation and so must
be understandable by the users it affects and by regulatory entities. A second drawback is (e, δ)-DP's
inferior composition properties and lack of versatility. Here, “composition” refers to the ability for
DP properties to be inherited when DP algorithms are combined and used as building blocks. As
an example, the training of deep learning models involves gradient evaluations and weight updates:
each of these steps can be treated as a building block. It is natural to expect that a DP learning
algorithm can be built using differentially-private versions of these components. However, the DP
composition properties cannot generally be well characterized within the framework of (, δ)-DP,
leading to very loose composition theorems.
To overcome the drawbacks of (, δ)-DP, numerous variants have been developed, including the
hypothesis-testing-based f-DP (Wasserman & Zhou, 2010; Dong et al., 2019), the moments-
accountant-based Renyi DP (Mironov, 2017), as well as concentrated DP and its variants (BUn
& Steinke, 2016; Bun et al., 2018). Despite their very different perspectives, all of these DP vari-
ants can be fully characterized by an infinite union of (, δ)-DP guarantees. In particular, there is
a two-way embedding between f-DP and the infinite union of (, δ)-DP guarantees: any guarantee
provided by an infinite union of (, δ)-DP can be fully characterized by f-DP and vice visa (Dong
et al., 2019). Consequently, f-DP has the versatility to treat all of the above notions as special cases.
In addition to its versatility, f-DP is more interpretable than other DP paradigms because it considers
privacy protection from an attacker’s perspective. Under f -DP, an attacker is challenged with the
hypothesis-testing problem
H0 : the underlying dataset is S versus H1 : the underlying dataset is S0
and given an output of an algorithm A, where S and S0 are neighbouring datasets. The harder this
testing problem is, the less privacy leakage A has. To see this, consider the dilemma that the attacker
is facing. The attacker must reject either H0 or H1 based on the given output of A: this means the
attacker must select a subset R0 of Range(A) and reject H0 if the sampled output is in R0 (or must
otherwise reject H1). The attacker is more likely to incorrectly reject H0 (in a type I error) when
R0 is large. Conversely, if R0 is small, the attacker is more likely to incorrectly reject H1 (in a type
II error). We say that an algorithm A is f-DP if, for any α ∈ [0, 1], no attacker can simultaneously
bound the probability of type I error below α and bound the probability of type II error below f(α).
The function f is called a trade-off function and controls the strength of the privacy protection.
The versatility afforded by f can be unwieldy in practice. Although f-DP is capable of handling
composition and can embed other notions of differential privacy, it is not convenient for repre-
senting safety levels as a curve amenable to human interpretation. Gaussian differential privacy
(GDP), as a parametric family of f-DP guarantees, provides a balance between interpretability and
versatility. GDP guarantees are parameterized by a single value μ and use the trade-off function
f(α) = Φ Φ-1(1 -α) - μ), where Φ is the cumulative distribution function of the standard nor-
mal distribution. With this choice of f, the hypothesis-testing problem faced by the attacker is as
hard as distinguishing between N(0,1) and N(μ, 1) on the basis of a single observation. Aside
from its visual interpretation, GDP also has unique composition theorems: the composition of a
μι- and μ2-GDP algorithm is, as expected, ∙∖∕μ2 + μ2-GDP ThiS property can be easily gener-
alized to n-fold composition. GDP also has has a special central limit theorem implying that all
hypothesis-testing-based definitions of privacy converge to GDP in terms ofa limit in the number of
compositions. Readers are referred to Dong et al. (2019) for more information.
The benefits of DP come with a price. As outlined in the definition of DP, any DP algorithm must be
randomized. This randomization is usually achieved by perturbing the intermediate step or the final
output via the injection of random noise. Because of the noise, a DP algorithm cannot faithfully
output the truth like its non-DP counterpart. To provide a higher level of privacy protection, a
stronger utility compromise should be made. This leads to the paramount problem of the “privacy-
utility trade-off”. Under the (, δ)-DP framework, this trade-off is often characterized in a form
of σ = f(, δ): to achieve (, δ)-DP, the utility parameter (usually the scale of noise) needs to be
chosen as f(, δ). We will further discuss this formulation with examples in the next section.
1.1	Our contribution
The general goal of this paper is to provide both a deeper theoretical understanding of and powerful
practical tools for the GDP framework. To this end, we start with the analysis of privacy profiles
2
Under review as a conference paper at ICLR 2022
under the condition of traditional (, δ)-DP, where we point out an often-overlooked partial order
on (, δ)-DP conditions via implication. We choose this as the starting point for our work because
ignoring this partial order will lead to problematic asymptotic analysis and compromised utility.
Next, we broken down the GDP into two parts: a head condition and a tail condition and define
an efficient, tractable, and visualizable tool, called the Gaussian differential privacy transformation
(GDPT). We first used the GDPT on the identification of GDP algorithms. Through the intermediate
property of GDPT that we develop, we find an easy-to-verify criterion that can distinguish GDP
mechanisms from non-GDP mechanisms. For GDP algorithms, this criterion provides a lower bound
for the privacy protection parameter μ. This criterion can help researchers widen the set of available
GDP algorithms and also gives an interesting characterization of GDP without an explicit reference
to the Gaussian distribution.
Following the identification of a GDP algorithm, the logical next step is to measure the exact value
of μ. By development of numerical properties of the GDPT, We give a method to narrow down
possible values of an optimal μ with an arbitrarily small and quantifiable margin of error.
Lastly, we give three more applications of our newly developed tools. In the first, we revisit some
established (, δ)-DP algorithms and improve their utility by accounting for the overlooked partial
order on (, δ)-DP conditions provided by logical implication. In the second application, we make a
comparison between E-DP and μ-GDP and find that any E-DP algorithm must be also μ-GDP. In the
last application, we discuss the effect of subsampling using the GDPT.
2	Background
An algorithm can be (E, δ)-DP for multiple pairs of E and δ: the union of all such pairs provides a
complete image of the algorithm under the (E, δ)-DP framework. In particular, an (E, δ)-DP mecha-
nism A is also (E0, δ0)-DP for any E0 ≥ E and any δ0 ≥ δ. The infinite union of (E, δ) pairs can thus be
represented as the smallest δ associated with each E. This intuition is formulated as a privacy profile
in Balle & Wang (2018). The privacy profile corresponding to a collection of (e, δ)-DP guarantees Ω
is defined as the curve in [0, ∞) × [0, 1] separating the space of privacy parameters into two regions,
one of which contains exactly the pairs in Ω. The privacy profile provides as much information as Ω
itself. Many privacy guarantees and privacy notions, including (e, δ)-DP, Renyi DP, f -DP, GDP, and
concentrated DP, can be embedded into a family of privacy profile curves and fully characterized
(Balle et al., 2020a). A privacy profile can be provided or derived by an algorithm’s designer or
users.
Before proceeding with detailed discussions, we first give three examples of DP algorithms that are
used throughout the paper. The first example we consider is the Laplace mechanism, a classical
DP mechanism whose prototype is discussed in the paper that originally defined the concept of
differential privacy (Dwork et al., 2006). The level of privacy that the Laplace mechanism can
provide is determined by the scale b of the added Laplacian noise. Given a global sensitivity ∆, the
value of b needs to be chosen as f (e, 0) = Δ∕e in order to provide an (e, 0)-DP guarantee. Despite
its long history, the Laplace mechanism has remained in use and study in recent years (Phan et al.,
2017; Hu et al., 2019; Xu et al., 2020; Li & Clifton, 2021).
Our second example is a family of algorithms in which a noise parameter has the form σ =
AET ,iog(B∕δ). Examples include
•	the goodness offit algorithm (Gaboardi et al., 2016),
•	noisy stochastic gradient descent and its variants (Bassily et al., 2014; Abadi et al., 2016;
Feldman et al., 2018), and
•	the one-shot spectral method and the one-shot Laplace algorithm (Qiao et al., 2021).
Our third example comes from the field of federated learning: given n users and the number of
messages m, the invisibility cloak encoder algorithm (ICEA) from (Ishai et al., 2006) is (E, δ)-DP if
m > 10 log(n∕(Eδ)) (Ghazi et al., 2019). See also (Balle et al., 2020b; Ghazi et al., 2020) for other
analysis of ICEA.
For figures and numerical demonstrations in this paper, we use b = 2∕∆ for the Laplace mechanism;
A = 2, B = 1, andσ = 2 for the second example, which we refer to as SGD; andm = 20 andn = 4
3
Under review as a conference paper at ICLR 2022
for the ICEA. We omit the internal details of these methods and focus on their privacy guarantees:
other than for the classical Laplace mechanism, whose privacy profile is known (Balle et al., 2020a),
privacy guarantees are given in the form of a privacy-utility trade-off equation σ = f (e, δ). Given
σ, it is tempting to derive the privacy profile by inverting the trade-off equation (i.e., as δA() =
min{δ | σ = f(, δ)}). However, in most cases, a privacy profile naively derived in this way is
not tight and will lead to a problematic asymptotic analysis, especially near the origin, because of a
frequently overlooked partial order between (, δ)-DP conditions, which we discuss below.
3	PRIVACY PROFILES AND AN EXACT PARTIAL ORDER ON (, δ)-DP
CONDITIONS
An (0, δ0)-DP algorithm is trivially (, δ)-DP for any ≥ 0 and δ ≥ δ0. However, this statement
does not give a full picture of the relationships between different (, δ)-DP conditions.
Theorem 3.1 Assume that 0 ≥ 0 and 0 ≤ δ0 < 1. The (0, δ0)-DP condition implies (, δ)-DP if
and only if δ ≥ δ0 + (1 - δ0)(e0 - e)+ /(1 + e0 ).
Corollary 3.1 Assume that ≥ 0 ≥ 0 and δ ≤ [(1 + e)δ0 - e + e0]/(1 + e0). An (, δ)-DP
algorithm is (0, δ0)-DP.
Corollary 3.2 Let A be an (0, δ0)-DP algorithm with the privacy profile δA. Then
X r w ʌ -u (I - δ0)(ee0 - ee)+
δA(C) ≤ δ0+-----------Γ+^10---------
(1)
We remark that the bound given in (1) is tight in the following sense: there is a specific (0, δ0)-DP
algorithm A such that δA() is exactly δ0 + (1 - δ0)(e0 - e)+ /(1 + e0).
Corollary 3.1 states the exact partial order of logical implication on (, δ)-DP conditions. Taking
this partial order into account, the privacy profile derived from the naive inversion of the trade-off
function can be refined into
λ ( ( ./Ll (1	( ( A1 - X _L (1 - δ0)(ee0 - e6)+ 1∖
δ∕(c) = min ( jδ | σ = f (co, δo) and δ ≥ δ0 +-----------1+^^-----------.),
Intuitively, the refined privacy profile not only considers (C, δ)-DP provided directly by the trade-off
function but also takes all pairs (c, δ) inferred by Corollary 3.1. Theorem 3.2 can itself be used
as a tool to improve some DP results: we will discuss this improvement in Section 5. With this
approach, we can derive the privacy profile of our second example to be δSGD(c) = min(δ2 + (1 -
δ2)(e2 - e)+/(1 + e2 ), exp{-c2}), where c2 ≈ 1.187 and δ2 ≈ 0.244. The privacy profile of
our third example is δICEA(c) = min(δ3 + (1 - δ3)(e3 - e)+/(1 + e3), K/c}), where K = 4/e2,
c3 ≈ 1.159, and δ3 ≈ 0.468. Notice that δSGD and δICEA share a similar form: both are the smaller
of two terms, one derived from Corollary 3.1 from a particular (c, δ)-DP pair and the other from
inverting the trade-off function. This is no coincidence. We leave the derivation to Appendix B.
We next show the connection between GDP and the privacy profile: briefly, Gaussian differential
privacy can be characterized as an infinite union of (c, δ)-DP conditions.
Theorem 3.2 A mechanism is μ-GDP ifand only ifit is (c, δμ(e))-DPfor all C ≥ 0, where
δ"(e) = φ (-μ + 2) - eeφ (-μ - 2).
(2)
This result follows from properties of f-DP and is formulated as Corollary 2.13 in Dong et al.
(2019). Prior to this general form, a similar expression for a special case appeared in Balle & Wang
(2018). From the definition of the privacy profile, it follows immediately that an algorithm A with
the privacy profile δ/ is μ-GDP if and only if δμ(c) ≥ δ∕(c) for all non-negative c. However, this
observation does not automatically lead to a meaningful way to identify μ-GDP algorithms.
Before proceeding with an analysis of privacy profiles, we give a few visual examples in Figure
1. The left pane illustrates the privacy profiles of our examples. That of the Laplace mechanism
4
Under review as a conference paper at ICLR 2022
is derived in Balle et al. (2020a) as Theorem 3: given a noise parameter b and a global sensitivity
∆, the privacy profile of the Laplace mechanism is δ(e) = max(1 - exp{ε∕2 - ∆∕(2b)}, 0). For
the second and the third examples, we compare the naive privacy profiles obtained by inverting the
trade-off function and the refined privacy profiles. The refined and naive privacy profiles take on
notably different values around e = 0. The inverted trade-off functions suggest that (0, δ) cannot be
achieved by any choice of parameter σ. However, this is clearly not true, considering Theorem 3.1.
As shown in the right pane of Figure 1, the Laplace mechanism’s privacy profile is below the 2-GDP
and 4-GDP curves but crosses the 1-GDP curve, indicating that the Laplace mechanism in this case
is 2-GDP and 4-GDP but not 1-GDP. The ICEA curve intersects all of the displayed GDP curves,
so the algorithm is not μ-GDP for μ ∈ {1,2,4}. It is hard to tell whether or not the SGD curve
crosses the 1-GDP curve and we cannot say ifit will cross the 2-GDP or even the 4-GDP curve at a
large value of e. These examples illustrate that we cannot draw conclusions simply by looking at a
graph. A privacy profile is defined on [0, ∞), so it is hard to tell if an inequality is maintained as e
increases. Previous failures ofad hoc attempts at privacy have taught that privacy must be protected
via tractable and objective means (Narayanan & Shmatikov, 2006; 2008; Barbaro & T. Zeller, 2006).
Figure 1: (Left) Examples of privacy profiles obtained by inverting the trade-off function (naive)
and by Theorem 3.1 (refined). (Right) Comparison of 1-GDP and 2-GDP privacy profiles against
those for our three examples.
Performing this check via numerical evaluation yields similar problems: we cannot consider all
values of e on an infinite interval (or even a finite one, for that matter). Turning to closed forms
for privacy profiles and δμ is also difficult: even if a given privacy profile is easy to handle, δμ
presents some technical hurdles. The profile δ* and Φ are transcendental with different asymptotic
behaviors for different values of μ and e. This is clear from the right pane of Figure 1: near e = 0,
δμ is concave for μ = 4 but convex for μ = 1. As a further complication, both the first and
second terms in the definition of δ* converge to 1 as e → ∞, but the difference between them
vanishes. Subtracting good approximations of two nearby numbers may cause a phenomenon called
catastrophic cancellation and lead to very bad approximations (Malcolm, 1971; Cuyt et al., 2001).
Due to the risk of catastrophic cancellation, a good approximation of Φ does not guarantee a good
approximation of the GDP privacy profile. These problems make it difficult to tightly bound δμ by
a function with a simple form.
To address the problem of differing asymptotic behaviours, we define the following two notions.
Definition 3.1 (Head condition) An algorithm A with the privacy profile δ/ is (e%, μ)-head GDP if
and only if δ∕(e) ≤ δμ(e) when e ≤ e八.
Definition 3.2 (Tail condition) An algorithm A with the privacy profile δ/ is (et, μ)-tail GDP if
and only if δ∕(e) ≤ δμ(e) when e > et. In particular, we define A as (+∞, μ)-tail GDP if A is
(et, μ)-tail GDP for some et < +∞ and define A as (et, +∞)-tail GDP if A is (et, μ) for some
μ < +∞.
5
Under review as a conference paper at ICLR 2022
The head condition checks the μ-GDP condition for E near zero and the tail condition checks the μ-
GDP condition for E far away from zero. As such, the combination of (e, μ)-head GDP and (e, μ)-tail
GDP is equivalent to μ-GDP.
4 The Gaussian Differential Privacy Transformation
In this section, we propose a new tool called the Gaussian Differential privacy transformation
(GDPT). We start by establishing a link between the GDPT and the head and tail conditions and
discuss how the GDPT can identify GDP algorithms and measure the value of μ.
Definition 4.1 (GDPT) Let f be a non-increasing, non-negative function defined on [0, +∞) satis-
fying f(0) ≤ 1. The Gaussian differential privacy transformation (GDPT) of f is the function Gf
mapping [0, ∞) to [0, ∞) such that Gf (E) = μGDP (e, f (E)), where μGDP (x, y) is the implicitfunction
defined by the equation δμ(x) = y.
We highlight two critical features of the GDPT.
•	The GDPT is order preserving: if f(E) ≥ g(E), then Gf (E) ≥ Gg(E).
•	The GDPT of δμ is Gδμ (E) = μ, a constant function.
The first of these two features derives from the monotonicity of 6*(e). Given a fixed μ, 6*(e) is a
strictly decreasing continuous function of e. Given a fixed e, E*(e) is a strictly increasing continuous
function of μ. Therefore, μGDP (x, y) is an increasing function of y: this leads to the order-preserving
property. The second property follows immediately from the definition of μGDP.
By taking advantage of the order-preserving property, direct comparisons between δμ and δ/ are
no longer necessary: instead, it is sufficient to compare their corresponding GDPTs. Furthermore,
appealing to the second property above, We need only compare GδA to the constant function μ. The
following theorems formalize this insight.
Theorem 4.1	An algorithm A with the privacy profile δ/ is (e%, μ)-head GDP if and only if μ ≥
sup({GδA (E) | E ∈ [0, Eh]).
Theorem 4.2	An algorithm A with the privacy profile δ/ is (Et, μ)-tail GDP if and only if μ ≥
sup({GδA (E) | E ∈ (Et, ∞)).
Corollary 4.1 An algorithm A with the privacy profile δ/ is μ-GDP if and only if μ ≥
sup({GδA (E) | E ∈ [0, ∞)}).
Without the above results, we would be forced to search through an uncountably-large family of
functions for a single δμ that never crosses δ/ anywhere on [0, ∞) and has μ as small as possible.
Now, with Theorem 4.1, we need only consider one function: the GDPT of δ/. The tightest value μ
is sup{GδA (E)}.
Using the GDPT framework, a condition for identifying GDP algorithms (that does not specify the
exact value of μ) is simple to formulate. We do so in the following theorems.
Theorem 4.3	An algorithm A is GDP if and only ifA is (+∞, +∞)-tail GDP.
Theorem 4.4	Let f be a non-increasing, non-negative function defined on [0, +∞) satisfying
f(0) ≤ 1. Then
lim Gf (e)
→+∞ f
M -≡.
E2
Theorems 4.3 and 4.4 give a useful criterion characterizing GDP and deepen our understanding of
GDP. Putting the exact value of μ aside, a GDP algorithm must provide an infinite union of (e, δ)-DP
conditions, where δ must be O(e-2) as E → ∞. Refer to Appendices A.3 and A.4 for proofs of
Theorems 4.3 and 4.4, respectively.
6
Under review as a conference paper at ICLR 2022
Using these newly-developed tools, We revisit our previous three examples for which the limit in
Theorem 4.4 is 0,，1/2, and +∞, respectively. From these evaluations, we can conclude that the
Laplace mechanism and SGD are GDP for some μ and that the privacy profile of the ICEA algorithm
crosses every μ-GDP curve regardless of how large μ is, indicating that the ICEA algorithm is not
GDP.
Figure 2: (Left) Examples of GDPTs. (Right) Plot of G+ and G- with different values of D.
The left pane of Figure 2 shows the GDPTs of the three examples considered in this paper. All
three GDPTs converge to a finite value as → 0+ . This can be attributed to the fact that any
algorithm providing some non-trivial (, δ)-DP guarantee is (0, δ)-DP for some δ ∈ [0, 1). For
larger values of , the GDPT of the Laplace mechanism takes on a constant value of 0, the GDPT of
SGD converges to a value that is approximately 0.7, and the GDPT of the ICEA seems to diverging.
These observations are consistent with the values of 0,，1/2, and ∞ obtained from Theorem 4.4.
Nonetheless, plots are only good for visualization and are not sufficient proof when verifying GDP.
We still need objective and tractable methods for obtaining bounds on GDPTs.
Once an algorithm is confirmed to be GDP via Theorems 4.3 and 4.4, it is natural to be interested in
the exact level of privacy protection, quantified by μ. Following the intuition outlined by definition
3.1 and 3.2, we decompose the GDP condition into head and tail conditions and first focus on finding
μ such that A is (e, μ)-head GDP.
4.1 Measuring the head
Without additional knowledge, finding sup{GδA (e) | e ∈ [0, eh]}, even for a finite eh, seems com-
putationally infeasible as there are uncountably many real numbers in any non-empty interval and it
is impossible to iterate through an infinite set. To solve this problem, we take advantage of the fact
that μGDP has a bounded partial derivative.
Theorem 4.5	One of the partial derivatives of μGDP is uniformly bounded. Specifically, 0 ≤
dμGDP (e,6 V √2π
∂	≤ ~7Γ.
Theorem 4.6	Given eh ≥ 0, let D = eh/n and xi = iD + eh for i ∈ {0, . . . , n + 1}. The GDPT
ofA, denoted by GA(e), is bounded between the two staircase functions
n+1
G-A (e) =	μGDP (xi, δA(xi+1)) ×1∈[xi ,xi+1 )
i=0
and
n+1
g+a (e) =	μGDP (xi+1，δA(xi)) × 1 ∈[xi,xi+1).
i=0
Specifically,
max G- (Xi)	≤ max	G∕(e)	≤ max G+ (Xi)	≤ max G- (Xi)	+ v2πD. (3)
i∈{0,...,n} δA i ∈[0,h] A	i∈{0,...,n+1} δA i i∈{0,...,n} δA i
7
Under review as a conference paper at ICLR 2022
Refer to Appendix A.5 and A.6 for proofs of Theorem 4.5 and 4.6, respectively.
For any h < +∞, we can now bound any GDPT GδA to any precision on [0, h] without full point-
wise evaluation because GδA is bounded between Gδ+ and Gδ- and each staircase function takes
on only finitely many values. The inequalities in (3) provide a viable way to estimate max GδA ():
maxi∈{0,...,n} Gδ- (xi) and maxi∈{0,...,n+1} Gδ+ (xi) can be computed by choosing the maximum
value among 2n + 2 evaluations of μGDp. For the completeness, We provide a detailed description of
an efficient algorithm with a time complexity of O(h/D + log2 (D) log(h)) in Appendix D.
4.2 Cutting the tail
With Theorem 4.6, one can verify ("，μ)-head GDP conditions for arbitrarily large Eh and an arbi-
trarily precise approximation of μ. While the error in μ can be quantified by D, one gap remains:
Eh can be arbitrarily large but can never truly be +∞. In this subsection, We discuss the gap be-
tween ("，μ)-head GDP and true GDP (which is equivalent to (+∞, μ)-head GDP). Before giving
a solution, we intuitively illustrate the gap between ("，μ)-head GDP and true GDP. Consider the
following two cases:
•	GDP with catastrophic failure, where with probability 1 - p, A1 functions properly and
exact μ-GDP is guaranteed, but with probability p, Ai malfunctions and discloses the entire
dataset; and
•	head-GDP with e-DP, where A2 is both (eh μ)-head GDP and (ch 0)-DP.
The μ-GDP privacy guarantee lies strictly between those of Ai and A2: specifically, δ∕ι (e) <
δ“(e) <δA2 (e).
In practice, μ is rarely above six in GDP and e is rarely above 10 in e-DP because more-extreme
values provide almost no privacy protection (Dong et al., 2019). Ifwe verify the head condition up
to eh = 100 (which is not difficult because the time required for verification grows linearly) and
take μ = 6, then P = δμ(eh) will be on the order of 10-43. If eh, is increased to 200, then P will be
smaller than 10-202. Hence, we conclude that the gap won’t make any notable difference in practice
with a proper choice of μ and eh
However, in some cases, one may still wish to fully mend the gap theoretically. This can also be
achieved under the GDPT framework. A GDPT is a fixed function that inherits smoothness proper-
ties from the corresponding privacy profile. The maximum value of a GDPT can be analysed using
standard functional tools. We give an example of such an analysis in Section 5. If the privacy profile
is not given in a manageable closed form or the analysis is infeasible, we provide the following “clip
and retify” procedure that can turn any (eh μ)-head GDP algorithm into a μ-GDP one at the cost of
some utility.
Theorem 4.7 Let A be a (eh μ)-head GDP algorithm with a numerical output. Assume that -∞ <
y- < y+ < +∞. Define C(y) = max(min(y, y+), y-) and R(z) = z + v, where v is sampled
from LaPlace(b) with b = (y+ — y-)/eh. Then RoCoA is μ-GDP
Refer to Appendix A.7 for proof of Theorem 4.7. We remark that, in order to minimize the utility
loss, the bounds y- and y+ should be properly chosen and the head condition should be verified to
an eh that is as large as possible.
5	Applications
5.1	GDPT OF e-DP ALGORITHMS AND THE LAPLACE MECHANISM
By our previous analysis of the GDPT, we know that being GDP means that a privacy profile has a
quickly vanishing tail (i.e., δ(e) must be O(e-2 )). It is remarkable that another single parameter
family of DP conditions, the e-DP conditions, is also a property that pertains to the tail of privacy
profiles. For any e0-DP algorithm, the privacy profile must be exactly 0 after e0. This suggests that
e0 -DP is stronger than GDP. Next, we will quantify this intuition using the tools developed in this
paper.
8
Under review as a conference paper at ICLR 2022
By Theorem 3.2, we know if A is 0-DP, then in the worst case, δA() = (e0 - e)+/(1 + e0).
We consider the GDPT of δA, denoted by GδA . It is easy to see that, for ≥ 0,
GδA(E) = 0: We need only consider E ∈ [0, 6o). Let GδA(e) be denoted by μe. Us-
ing using the partial derivative of G§a derived in Appendix A.5, we know that 品GδA(e) =
√2π exp n (μ2 + 2E)2 / (8μ2)0 hφ(-μ2+2e ) - φ( -2μ0 )i . Then Sign( ∂ GδA(e)) = Sign(Me - μ0 -
2e∕μo). We can conclude that μe ≤ μo and, further, that GδA(E) is strictly decreasing on [0, eo).
By Theorem 4.1, we know that A is μo-DP. This finding can be more generally formulated as the
following theorem.
Theorem 5.1 Any (e, 0)-DP algorithm is also μ-GDPfor μ ≥ —2Φ-1 (1/(1 + ee)).
Dong et al. (2019) pointed out that the DP guarantees of the Laplace mechanism are stronger than
those correspondingly provided by E-DP. We reaffirm this difference by showing that it still exists
under the GDP framework. The Laplace mechanism satisfies μ-GDP for μ smaller than the bound
given in Theorem 5.1. The GDPTs presented in Appendix E.1 illustrate this difference.
5.2	Utility refinement
There is an interesting byproduct or the privacy profile refinement. Theoretically, the privacy profile
refinement can also be used to improve an algorithm’s utility. For example, the projected noisy SGD
algorithm in Feldman et al. (2018) is (e, δ)-DP and the trade-off function is σ = —C log(δo)∕Eo. To
achieve (0.2, e-2)-DP, it appears that σ needs to be chosen as —C log(e-2)/0.2 = 10C. By Corol-
lary 3.1, (E, δ)-DP implies (0.2, e-2)-DP when δ + (1 — δ)(ee — e0.2)+ /(1 + ee) = e-2. Numerical
methods suggest that, by choosing E ≈ 0.334 and δ ≈ 0.067, (E, δ)-DP implies (0.2, e-2)-DP but
σ = —C log(δ)∕E ≈ 8.086C < 10C. Therefore, the desired level of DP can be achieved with a
lower noise parameter. However, this type of refinement majorly affects privacy profile around the
origin and therefore minor in practice. For the general case and details of this derivation, refer to
Appendix C.
5.3	Effect of subsampling
It is well known that a mechanism can be improved from a privacy perspective by adding extra
steps to perturb its input or output. One popular approach is the subsampling technique (Balle et al.,
2020a). We omit the details of this method and use the following result from Balle et al. (2020a).
Let Abe an (E, δ)-DP algorithm and Sγ the Poisson subsampling procedure wherein each data point
is retained with probability γ. Then A ◦ Sγ is (log(1 — γ + γee), γδ)-DP when two datasets are
considered differ in one element if one dataset equals to the other dataset plus one additional element.
Combining this result with the partial derivatives of Mgdp, the Poisson subsampling procedure can
significantly decrease the value of μ around E = 0 but has no effect on the GDPT,s tail. Therefore,
in the worst case, a subsampling procedure does not improve the value of μ for a GDP algorithm.
For detailed plots, refer to Appendix E.2.
6	Conclusion and discussion
In this paper, we provided both an analytical understanding of and engineering tools for the GDP
framework. Using the new notions we proposed, we gave solutions to two problems in GDP: iden-
tification and measurement. We further provided many applications of the proposed tools.
Our developments in this paper suggest numerous interesting directions for future work. The “clip
and retify” procedure can be extended to provide more versatility to methods for privacy amplifica-
tions by utilizing more advanced composition and post-processing results. GDPT is generalizable
to other parameterized DP notions such as RDP and cDP and enriches the DP literature with its
tractability and visualizability.
9
Under review as a conference paper at ICLR 2022
Ethics Statement. Our work is majorly development of mathematical tools for the differential
privacy framework. We cannot foresee any immediate real-world impact and believe those mathe-
matical tools will contribute to the developments of privacy-aware technologies.
Reproducibility Statement. For all our theoretical results, we confirm that all necessary assump-
tions are properly addressed, and all proofs are provided unless trivial. No datasets and interactive
virtual environments are involved in this paper. There are no experimental results in this work.
Figures are only for demonstrative reasons as main results are supported by proofs.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308-318, 2016.
Milton Abramowitz, Irene A Stegun, and Robert H Romer. Handbook of mathematical functions
with formulas, graphs, and mathematical tables, 1988.
Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Ana-
lytical calibration and optimal denoising. In International Conference on Machine Learning, pp.
394-403. PMLR, 2018.
Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy profiles and amplification by subsampling.
Journal of Privacy and Confidentiality, 10(1), 2020a.
Borja Balle, James Bell, Adria Gascon, and Kobbi Nissim. Private summation in the multi-message
shuffle model. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Commu-
nications Security, pp. 657-676, 2020b.
M. Barbaro and Jr. T. Zeller. A face is exposed for aol searcher no. 4417749. New York Times (Aug,
9, 2006), 2006.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473. IEEE, 2014.
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and
lower bounds. volume 9985, pp. 635-658, 11 2016. ISBN 978-3-662-53640-7. doi: 10.1007/
978-3-662-53641-4-24.
Mark Bun, Cynthia Dwork, Guy N Rothblum, and Thomas Steinke. Composable and versatile
privacy via truncated cdp. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
of Computing, pp. 74-86, 2018.
Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao Wang.
Privacy at scale: Local differential privacy in practice. In Proceedings of the 2018 International
Conference on Management of Data, pp. 1655-1658, 2018.
Annie Cuyt, Brigitte Verdonk, Stefan Becuwe, and Peter Kuterna. A remarkable example of catas-
trophic cancellation unraveled. Computing, 66(3):309-320, 2001.
Fida Kamal Dankar and Khaled El Emam. The application of differential privacy to health data. In
Proceedings of the 2012 Joint EDBT/ICDT Workshops, pp. 158-166, 2012.
Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. To appear in Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 2019.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, pp. 486-503. Springer, 2006.
10
Under review as a conference paper at ICLR 2022
UJlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable
privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on
computer and communications security, pp. 1054-1067, 2014.
Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by
iteration. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),
pp. 521-532. IEEE, 2018.
Marco Gaboardi, Hyun Lim, Ryan Rogers, and Salil Vadhan. Differentially private chi-squared
hypothesis testing: Goodness of fit and independence testing. In International Conference on
Machine Learning, pp. 2111-2120. PMLR, 2016.
Badih Ghazi, Rasmus Pagh, and Ameya Velingker. Scalable and differentially private distributed
aggregation in the shuffled model. arXiv preprint arXiv:1906.08320, 2019.
Badih Ghazi, Pasin Manurangsi, Rasmus Pagh, and Ameya Velingker. Private aggregation from
fewer anonymous messages. Advances in CryPtology-EUROCRYPT2020, 12106:798, 2020.
Muneeb Ul Hassan, Mubashir Husain Rehmani, and Jinjun Chen. Differential privacy techniques
for cyber physical systems: a survey. IEEE Communications Surveys & Tutorials, 22(1):746-789,
2019.
Yaochen Hu, Peng Liu, Linglong Kong, and Di Niu. Learning privately over distributed features:
An admm sharing approach. arXiv preprint arXiv:1907.07735, 2019.
Yuval Ishai, Eyal Kushilevitz, Rafail Ostrovsky, and Amit Sahai. Cryptography from anonymity. In
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 239-
248. IEEE, 2006.
Tao Li and Chris Clifton. Differentially private imaging via latent space manipulation. arXiv preprint
arXiv:2103.05472, 2021.
Michael A Malcolm. On accurate floating-point summation. Communications of the ACM, 14(11):
731-736, 1971.
Arthur R Miller. Personal privacy in the computer age: The challenge of a new technology in an
information-oriented society. Michigan Law Review, 67(6):1089-1246, 1969.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Sym-
posium (CSF), pp. 263-275. IEEE, 2017.
Arvind Narayanan and Vitaly Shmatikov. How to break anonymity of the netflix prize dataset. arXiv
preprint cs/0610105, 2006.
Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. In 2008
IEEE Symposium on Security and Privacy (sp 2008), pp. 111-125. IEEE, 2008.
NhatHai Phan, Xintao Wu, Han Hu, and Dejing Dou. Adaptive laplace mechanism: Differential
privacy preservation in deep learning. In 2017 IEEE International Conference on Data Mining
(ICDM), pp. 385-394. IEEE, 2017.
Gang Qiao, Weijie Su, and Li Zhang. Oneshot differentially private top-k selection. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 8672-8681. PMLR, 18-24 Jul
2021.
Edward Shils. Privacy: Its constitution and vicissitudes. Law and Contemporary Problems, 31(2):
281-306, 1966.
Larry Wasserman and Shuheng Zhou. A statistical framework for differential privacy. Journal of
the American Statistical Association, 105(489):375-389, 2010.
Xuefeng Xu, Yanqing Yao, and Lei Cheng. Deep learning algorithms design and implementation
based on differential privacy. In International Conference on Machine Learning for Cyber Secu-
rity, pp. 317-330. Springer, 2020.
11
Under review as a conference paper at ICLR 2022
A Appendix： Proofs
Proof A.1 ProofofTheorem 3.1:
Sufficiency:
When E ≥ €0, the sufficiency is trivial as δ = δo.
When € < €o, given that A is (∈o, δo) -DP, by the definition, for any pair of datasets S and S0 that
differ in the record of a single individual and any event E,
P[A(S) ∈ E] ≤ ee°P [A (S0) ∈ E] + δo.
When P [A (S0) ∈ E] ≤ II-⅛ := co,
P[A(S) ∈ E] ≤ ee°P [A (S0) ∈ E] + δo
≤ (ee° + e - ee)P [A (S0) ∈ E]+ δo + δ - δ
≤ ee°P [A (S0) ∈ E] + δ +(ee° - ee)co + δo - δ
≤ e'P [A (S0) ∈ E] + δ + (e'° - e')co - (I -	- 吟
1 + ee°
≤ eeP [A (S0) ∈ E] + δ.
When co ≤ P [A (S0) ∈ E] ≤ 1,
P[A(S) ∈ E] = 1 - P[A(S) ∈ Ec]
≤ 1 - e-e° (P [A (S0) ∈ Ec] - δo)
=1 - e-e° (1 - P [A (S0) ∈ E] - δo)
=1 - e-e° + e-e°P [A (S0) ∈ E] + e-e° δo
=1 - e-e° + e-e°δo + δ - δ + (e-e° + ee - ee)P [A (S0) ∈ E]
=eeP [A (S0)	∈	E]	+ δ + 1 -	e-e°	+ e-δo - δ + (e-e°	- ee)P [A (S0)	∈ E]
≤ eeP [A (S0)	∈	E]	+ δ + 1 -	e-e°	+ e-δo - δ + (e-e°	- ee)co
.	.	-	e-e° - ee
=eeP [A (S0) ∈ E] + δ +(1 - δo)(	e° - e-°) + 1 - δ
1+e
e-° — ee	ee - ee°
≤ e'P [A(S0) ∈ E ] + δ +(1 - δo)( F-O--e-° + 1 + K/)
=eeP [A (S0) ∈ E] + δ.
Necessity:
We prove the necessity by giving a specific (∈o, δo)-DP algorithm A such that δ∕(c) is exactly δo +
(1-δ°)e°-e，)十
1⅛ee°	.
Define Ωe = {1, 2, 3,4} and Ωs = {0,1}. Let € ≥ 0, 0 ≤ δo ≤ 1 and denote 1+ee° as ao. Let A
be a randomized algorithm that take a single point from Ωs and generate output as follows:
(P(A(S) = 1 | S = 0) = δo,
J P(A(S) = 2 | S = 0)=0,
∣P (A(S ) = 3 | S = 0) = (1 - δo)&o,
IP(A(S) = 4 | S = 0) = (1 - δo)(1 - αo),
By definition, δ(c) is the smallest δ such that P(A(S) U E | S = S) ≤ eeP(A(S) U E | S =
1 — s) + δ holds true for all E U Ωe and S ∈ Ωs . By checking all 64 combinations, we can conclude
that δA(E) = δo + (I-δ°M0°-e')十.
P (A(S)=	1|S=	1)	0,
P (A(S)=	2|S=	1)	=δo,
P (A(S)=	3|S=	1)	=(1 - δo)(1 - 0o),
P (A(S)=	4|S=	1)	=(1 — δo)αo.
12
Under review as a conference paper at ICLR 2022
We present a key lemma underlying much of the theoretical analysis and may be of use for future
developments.
Lemma A.1 Define M(E) =	,where a = — μ + μ. Itfollows that ］邛 ∣μ(j) = 1.
Proof A.2 ProofofLemma A.1:
It is well known that (Abramowitz et al., 1988), for t < 0:
-t + √t2 + 4
< v2exp
①⑴< —
-t + tjt+ + π
1
Let a = (—μ + 2)and b = (一μ
μ
2
lim δμ(e)
e→∞
lim Φ (a)
e-∞
-eeΦ(b)
lim δμ(e)
e-∞
exp (二
-a+
-α2
exp (-2—+ E)
-b + √b2+4.
α2 + I
—
-b + √b2Γ4 厂
lim Φ (a) — eeΦ (b)
e-∞
〉
〉
≤
2τim
∏ e-∞
—
1
0.
Therefore,
lim δμ(e) = 0.
e→∞
(4)
It is easy to see that,
〜	“e-α2/2
㈣而㈤』√2∏a^=0
(5)
13
Under review as a conference paper at ICLR 2022
By L,Hospital,s rule:
~ ,.
1. W)
lim ~Γ~Γ∖
e→∞ δμ(e)
~
.δ,μ (e)
lim τ7TT
c→∞ δμ(e)
e—吟(α2 + 2)
lim------ʌ=-----
c→∞	√2πa3
eeΦ(b)
lim
e→∞
_ b2 ɪ ∕j ʌ
e ɪΦ(b)
lim
b→ι∞
√2πb
e—咚 Φ(b)
1.
√2πb
Proof A.3 ProofofTheorem 4.3:
It is easy to see that A is (+∞, +∞)-tail GDP if and only if lim Gδ. (e) < +∞.
e→十∞
Sufficiency:
If A is μ -GDP. Then lim G^λ (e) ≤ lim G§ (e) = μ.
C→ + ∞	C→ + ∞	μ
Necessity:
If lim GδA (e) = μ < +∞, there must be a €t > 0 such that A is (et, μ0 + 1)-tail GDP.
C→ 十 ∞
Notice that lim δμ(et) = 1, we can pick μι > μ0 large enough such that δμι (et) > δ/ (0).
This is possible because by Theorem 3.1, δ∕(0) < 1. Then for € ∈ [0, et), δ∕(c) ≤ δ∕(0) ≤
δμι (Et) ≤ δμι(€). A is both(€t, μ)-head and tail GDPfor μ = μ0 + μι + 1. A is GDP as desired.
Proof A.4 ProofofTheorem 4.4:
Let lim Gf (e) = μt.
First we show that lim
e→∞
e2
—2 log δA(e)
By the definition the limit, for any μ0 > μt, for sufficient large €, Gf(€)< μ0 and further δ∕(c) ≤
δμo (e). Hence, lim ? A(R ≤ 1. ByLemmaA.1, lim δA(f∖ ≤ 1.
e→∞ δμ0 (C)一	e→∞ δμ0 (C)
Then lim
e→∞
e2
—2log δA(e) ≤ eli→∞
e2
—2 log δμ0 (C)
=μ2.
lim
e-∞
e2
—2 log 6λ(c)
≤ μt as desired as we take μo → μt.
Next we show that lim
e→∞
e2
—2 log 6λ(c)
e2
μ0 < μ2, then by Lemma A.1,
lim
e→∞
€2
€2
----：---T--:~~ - ----：---T--:~~
-2 log δ∕(e)	-2 log δμt (e)
lim
e→∞
€2
-urn——匕—
c→∞ -2log δμt (e)
If㈣
—2 log δA(c)
-2 log δ∕(e)
< μ0 - μ2
Thenfor a sufficiently large €0,
---：---T--:---- - --：---T--:---
-2 log δA(e0) -2 log δμ0 (e0)
< 0.
Since log is an increasing function, it follows that δ∕(e0) < δμ0 (e0). Then lim Gf (e) ≤ μ0 < μt,
C→ 十 ∞
which is a contradiction.
14
Under review as a conference paper at ICLR 2022
Proof A.5 ProofofTheorem 4.5:
Let Gμ(e) = F(e, δμ(e)) and F(x,y) = μGDP(x,y)∙
By definition of μgdp，Gμ (E) = μ.
On one hand,
dG μ(O
∂E
dG*(E)
∂μ
生法
，.
O 1
==
(∂Gμ(E) = ∂F + ∂F ∂δμ(E)
C--T 一 τ∙,I ∂e ∂x ∂y ∂e
On the other hand, by chain rule,
I ∂Gμ(e) = ∂F ∂δμ(E)
I ∂μ ∂y ∂μ '
(∂F =( ∂δ*(E)	I
Therefore J 的 如
f , I ∂F	∂δμ(e))-1 ∂δ”(E)
I ∂x = ( ∂μ ) ∂e .
Using the close forms, dδμ(e) and 吗：，can be directly computed:
3 = -eeφ(-W +2E )
∂e	(	2μ )
(μj)2
∂δμ(E) _ e	早-
--二---------. ---.
∂μ	√2π
Hence,
√2∏e¾1 Φ(-^2+2) ≤ √2∏e端Φ(-μ) ≤ √2π,
(μ2-2e)2
√2πe 8μ2	> 0.
____ (μ2+2e)	2	/-
Notice that 某=√2πe 8μ	Φ(-μ 2+2e) > 0, combined with the fact that 某 ≤ =In, we can
COnClude that 0 ≤ dμGG3 f ≤ √2π. By 瑞 > 0, we can see GDPT is order preserving.
Proof A.6 ProofofTheorem 4.6:
We now consider the gap between maxi∈{0,…，n}{G- (Xi)} andmaxi∈{0,∙∙∙ ,n+i!{Gf+ (xi)} bound
the length of [μ-, μ+ ] in two cases.
Case 1:	If maxi∈{o,…,n+i}{G+A(xi)} = G+A(xo), then maxi∈{o,…,n+i}{G+A(xi)}
G+A (XO) = μ GDP(D，δA(0)) ≤ μ GDP(0, δA(0)) + v⅜d .Therefore,
maχ1 g(e) ≤ g+4 (xo) ≤ {g-λ (xo)} +
e∈ [0,e%]
√2πD
2
Case 2:	If maxi∈{o,…，n+i}{Gf+ (xi)} = G+(xo), then by the order preserving property,
the optimal μ lies in [μ-, μ+], where μ- = max(μh, maxi∈{o,…,n}{G- (xi)}) and μ+ =
max(μh, maxi∈{i,…，n+i}{G" (xi)}). Notice that
max {Ga , (Xi)} = max {μgdp (Xi，δA(Xi+i))} = max {μgdp (Xi—I，δA(Xi))}
i∈{o,…，n} A	i∈{o,…，n}	i∈{i,…，n+i}
≥ max	{μGDP(xi+i，δ∕(xi)) - √2πD}
i∈{i,…，n+i}
≥ max	{G+ (xi)} — √2πD.
i∈{i,…，n+i}	A
15
Under review as a conference paper at ICLR 2022
In both cases the gap is no greater than √2πD as desired.
Proof A.7 Proof of Theorem 4.7:
By the definition of C, C ◦ A is bounded in [y-, y+]. Therefore the global sensitivity of C ◦ A is no
greater than y + -y-. Then R ◦ C ◦ A is a special case of the Laplace mechanism. By Balle et al.
(2020a), RoCoA is Eh-DP. Then 6roCoA(e) = 0 < δμ(e) for any E ≥ Eh.
In addition, because of the post-processing property, δR◦c◦A(E') ≤ 6∕(e) < 6*(e) forany E < Eh.
Therefore, RoCoA is μ-GDP.
B Appendix: Refining the privacy profile
Given a trade-off function σ = f(E, δ) and a fixed parameter σ. From definition of the trade-off
function it is instant that the for any (e, δ)∈ Ω = {(e, δ) | σ = f (e, δ)}, (e, δ)-DP is guaranteed.
Then, (e, δ)-DP is also guaranteed if there is a (eo, δo) ∈ Ω such that (eo, δo)-DP implies (e, δ)-DP.
Therefore,
(1 - δ0)(e0 - e)+
6∕(e) = mm {δ | σ = f (e0, δo) and δ ≥ δo +-----------------}.
1 + e0
Notice that by Corollary 3.2, (E0, δ0)-DP implies (E, δ) with δ < δ0 only if E < E0, we rewrite the
δA(E) as:
δA(E) =	inf g(E, E0),
0∈[,∞)
where g(E, eo) := (1 一 δ∕(Eo))	+ 6/(eo) and δ/ is the naive privacy profile defined implicitly
by σ = f(E0, δ0). For continuously differentiable f, the minimum value of the right-hand side can
be found be take the derivative:
叫[E0 = J++1。)2 hδA (EO) + ee0(I 一 δA(EO) + δA (EO))].
E0	+e
We remark that the sign of dδA(e) does not depend on e. For both of our example 2 and 3, We
both find a particular value Ei such that Sign("啜：0)) = —Sign(E 一 Ei). This means for E ≥ Ei,
δA(E) = δA(E) and otherwise δA(E) equals to the δ value derived from (E, δA(E)).
C APPENDIX: REFINING THE σ
Given a trade-off function σ = f(E, δ) and desired (E, δ). From definition of the trade-off function,
(E, δ)-DP is guaranteed when σ = f(E, δ). We treat the σ as the noise parameter and without loss of
generality we assume a smaller value of σ is preferred. Notice that (E, δ)-DP is also guaranteed if
there is a (eo, δo) ∈ Ω such that (eo, δo)-DP implies (e, δ)-DP. Therefore, σ can be chosen as,
σ- = min ({f (eo, δo) | δ ≥ δo + (I - 'ʌe C- e)}).
1 + ee0
Notice that by Corollary 3.2, (Eo, δo)-DP implies (E, δ) with δ < δo only if E < Eo, we rewrite the
σ- as:
σ- =	inf g(E, Eo),
e0∈[e,∞)
where g(E, eo) := f (eo, (1 — δ∕(Eo))黑+： + δ∕(Eo)) and δ/ is the naive privacy profile defined
implicitly by σ = f(Eo, δo). For continuously differentiable f, the minimum value of the right-hand
side can be found be take the derivative:
彻『0) = dI。、2	hδA	(EO)	+	ee0 (1 一	δ∕(EO)	+ δA	(E0))] IC |e=eo ,δ=δo	+ ∂f le=e0,δ=δ0	∙
Eo	( + e )	E
16
Under review as a conference paper at ICLR 2022
We remark that the sign of dδA(e) does not depend on E and the zero point can be effectively solved
numerically.
D A self-contained efficient head measurement algorithm
First We formalise the binary search algorithm to find Mgdp：
Algorithm 1: Binary search
Input： e, δ, μmax, b ;
Output: μ-, μ+ (lower and upper bound of μ).;
μ- i— 0;
μ+《-μmax;
while μ+ - μ- > b do
〃 _ μ++μ-.
μ 2	;
if δμ(e) > δ then
I μ+ J μo;
else
I μ- J μo;
end
end
return μ+, μ—.
Given a proper searching range [0, μmax], the algorithm 1 yields μ+ and μ- such that μ- ≤
Mgdp (g δ) ≤ μ+ and μ+ — μ- < b. We denote μ+ as 仙++^ (e,δ, μmax, b) and μ- as μt-jP 亿 δ,μmax, b).
When searching for μ via privacy profiles, the μmax can chosen as μG□p(μt, 6a(0)) because Mgdp will
not exceed this value on [0, j] (refer to Theorem 4.3 for the value of μt). Alternatively, μmax can be
set to a large constant for convenience, for example 10. In this case, if the outputted μ+ equals to
the preset value (10) then the privacy profile fails to imply 10-GDP. In practice, GDP with μ ≥ 6
provides almost no privacy protection Dong et al. (2019).
With the formal definition of binary search we use, the exhaustive iteration method to
bound the staircase functions outlined in Theorem 4.6 can be formally written as follows：
Algorithm 2: Finding μ with privacy profiles (Naive).
Input: δ/, Eh, μt, c, μmax. (Privacy profile, the end point of searching /, μ given by Theorem
4.3, reciprocal of error margin, searching range);
Output: μ-, μ+ (lower and upper bound of μ).;
i J 0;
n J d√8c∏6he + 1;
D J%；
n-1
μ- J maχ(μh, μt);
μ+ j- μmax;
while i ≤ n + 1 do
x- J iD + Eh;
x+ J (i + 1)D + Eh;
μ+ J maχ(μ+,μ+p(χ-,δA(χ+),μmax, 2c));
μ- J max(μ-,μGDP (x+ ),δA(x-,μmax, 2c));
iJi+1
end
return μ+, μ—.
Algorithm 2 naively go thorough all Gδ- (xi) and Gδ+ (xi). By the choice of D, the true gap
between max G- (E) and max G+ (E) is less than 2c and the binary search estimate the Mgdp with
an error less than 2c. Combined together, the overall gap between estimated μ+ and μ- is less than
C. However, there is still room for optimization:
17
Under review as a conference paper at ICLR 2022
We take μ+ J max(μ+,μ++jp(x-, δ∕(x+), μmaχ, 21c)) for example, same optimization can be
applied to μ- J max(μ-,μsp(x+ ,δ∕(x-,μmaχ, 21c))) as well. The naive operation, μ+ J
max(μ+,μt+jp(x-, δ∕(x+),小皿醒，；2^)) can be optimized into ”If δ*+ (x-) < δ∕(x+), then μ+ J
小黑(X-, δ∕(x+), μmax, 2ιc)). To see this, We list all three possibilities as follows:
•	CaSeL μ+ < μGDP(χ-,δA(x+)) ≤ μGDP(χ-, δA(x+), μmax, 21c))∙
•	Case2: μGDP(χ-,δA(x+)) ≤ μ+ ≤ μGDP(χ-, δA(x+), μmax, 21c)).
•	Case3: μGDP(χ-,δA(x+)) ≤ μGDP(χ-,δA(x+ ),μmax, 21c)) < μ+.
In case 1, both of the naive operation and the optimized operation will update μ+ to
μ+DP (x- ,δA(χ+),μmax, 21c)).
In case 2, the optimized operation will do nothing, because the test δμ+ (x-) < δ∕(x+) will fail.
The naive operation will update μ+ due to the error of binary search, which should be avoided.
In case 3, the optimized operation will do nothing, because the test δμ+ (x-) < δ∕(x+) will fail.
The naive operation will also do nothing because the max operator will choose μ+.
To sum up, the optimized operation always give a more accurate update.
Besides, we want to avoid case 1 because only in that case a binary search is needed. Notice that
case 1 happens only if δμ+ (x-) < δ∕(x+), which is equivalent to μ+ < Mgdp(x-, δ∕(x+)). In
the k + 1 round of loop, the condition μ+ < Mgdp(x-,6/(x+)) holds true only if for all j ∈
{0,…，k}, μGDP(x-, δA(x+)) < Mgdp(x-, δ∕(x+)),where X- and x+ are the values of X- and χ+
in the round j . This inspire us to shuffle xi before iteration because after shuffling, the probability
of ”Mgdp(x-, δA(χ+)) < Mgdp (x-, δ∕(x+)) for all j ∈ {0,…，k}" will be 告.The expected
occurrence of case 1 will be Pn+1 k+1 = O(log(n)). We present the complete optimized algorithm
as follows:
Algorithm 3: Finding μ with privacy profiles (optimized).
Input: δ/, ∈h, μt, c, μmax. (Privacy profile, the end point of searching /, μ given by Theorem
4.3, reciprocal of error margin, searching range);
Output: μ-, μ+ (lower and upper bound of μ).;
i J 0;
n J d√Sc∏6h] + 1;
D J ^hT;
n-1
μ- J maχ(μh, μt);
μ+ j- μmax;
S = [0, 1,…，n + 1];
Shuffle S;
while i ≤ n + 1 do
x- J S[i]D + h;
x+ J S[i + 1]D + h;
if δμ+ (x-) < δ∕(x+) then
I μ+ J μ+Dp(χ-,δA(χ+),μmax, 21c));
end
if δμ- (x+) < δ∕(x-) then
I μ- J μ-Dp (x+ ,δA(χ-),μmax, 21c));
end
iJi+1
end
return μ+, μ-.
The time complexity of shuffling S is O(n) = O(hc). Each binary search has a time complexity of
O(log(c)) and the expected number of binary searches is O(log(hc)). The overall time complexity
of the optimized algorithm is therefore O(hc + log2 (c) log(h)).
18
Under review as a conference paper at ICLR 2022
E Appendix: Plots
E.1 The Laplace mechanism under GDP
Figure 3: The plot of GDPT of -DP privacy profiles and the Laplace mechanisms with the same
-DP guarantee.
19
Under review as a conference paper at ICLR 2022
E.2 The effect of subsampling
Figure 4: (Left) GDPT of the Laplace mechanism for various of γ. (Right) GDPT of the SGD for
various of γ .
Figure 5: (Left) GDPT of the ICEA for various of Y. (Right) GDPT of the δμ for various of γ.
20