Under review as a conference paper at ICLR 2022
Langevin Autoencoders for Learning Deep
Latent Variable Models
Anonymous authors
Paper under double-blind review
Ab stract
Markov chain Monte Carlo (MCMC), such as Langevin dynamics, is valid for
approximating intractable distributions. However, its usage is limited in the con-
text of deep latent variable models since it is not scalable to data size owing to its
datapoint-wise iterations and slow convergence. This paper proposes the amor-
tized Langevin dynamics (ALD), wherein datapoint-wise MCMC iterations are
entirely replaced with updates of an inference model that maps observations into
latent variables. Since it no longer depends on datapoint-wise iterations, ALD
enables scalable inference from large-scale datasets. Despite its efficiency, it re-
tains the excellent property of MCMC; we prove that ALD has the target posterior
as a stationary distribution under some assumptions. Furthermore, ALD can be
extended to sampling from an unconditional distribution such as an energy-based
model, enabling more flexible generative modeling by applying it to the prior
distribution of the latent variable. Based on ALD, we construct a new deep la-
tent variable model named the Langevin autoencoder (LAE). LAE uses ALD for
autoencoder-like posterior inference and sampling from the latent space EBM.
Using toy datasets, we empirically validate that ALD can properly obtain samples
from target distributions in both conditional and unconditional cases, and ALD
converges significantly faster than traditional LD. We also evaluate LAE on the
image generation task using three datasets (SVHN, CIFAR-10, and CelebA-HQ).
Not only can LAE be trained faster than non-amortized MCMC methods, but LAE
can also generate better samples in terms of the Frechet Inception Distance (FID)
compared to AVI-based methods, such as the variational autoencoder1.
1	Introduction
Variational inference (VI) and Markov chain Monte Carlo (MCMC) are two practical tools to ap-
proximate intractable distributions. Recently, VI has been dominantly used in deep latent variable
models (DLVMs) to approximate the posterior distribution over the latent variable z given the ob-
servation x, i.e., p (z | x). At the core of the success of VI is the invention of amortized variational
inference (AVI) (Kingma et al., 2014; An & Cho, 2015; Su et al., 2018; Eslami et al., 2018; Kumar
et al., 2018), which replaces optimization of datapoint-wise variational parameters with an inference
model that predicts latent variables from observations. The advantage of AVI over traditional VI is
that minibatch training can be used for its optimization, which enables efficient posterior inference
on large-scale datasets. In addition, we can also leverage the optimized inference model to per-
form inference for new data in test time. However, the approximation power of AVI (or VI itself)
is limited because it relies on distributions with tractable densities for approximations. Although
there have been attempts to improve their flexibility (e.g., normalizing flows (Rezende & Mohamed,
2015; Kingma et al., 2016; Van Den Berg et al., 2018; Huang et al., 2018)), such methods typically
have architectural constraints (e.g., invertibility in normalizing flows).
Compared to VI, MCMC can approximate complex distributions because it does not rely on any
tractable distributions. Instead, MCMC repeats sampling from the target distribution and uses ob-
tained samples to approximate the posterior distribution. Langevin dynamics is a typical example of
MCMC for sampling from a continuous distribution. However, despite its high approximation abil-
ity, MCMC has received relatively little attention in learning DLVMs. It is because MCMC methods
1The implementation is available at https://bit.ly/2Swow0F
1
Under review as a conference paper at ICLR 2022
(A)	(B-1) Traditional LD
(B-2) Proposed Method (ALD)
Figure 1: (A) Directed graphical model under consideration. (B-1) In traditional Langevin dynam-
ics, the samples are directly updated in the latent space. (B-2) Our amortized Langevin dynamics
replace the update of latent samples with an inference model fz|x that maps the observation x into
the latent variable z.
take a long time to converge, making it difficult to be used in the training of DLVMs. When learning
DLVMs with MCMC, we need to run MCMC iterations for sampling from each posterior per data
point, i.e., p z | x(i) (i = 1, . . . , n), where n is the number of training data, as shown in Figure
1 (B-1). It is problematic, mainly when training with a large-scale (n > 10K) dataset, because it
is time-consuming to run massive MCMC iterations for all data points. Furthermore, we need to
re-run the sampling procedure when we obtain new observations in test time.
As in VI, there have been some attempts to introduce the concept of amortized inference to MCMC.
For example, Hoffman (2017) initializes MCMC sampling using an inference model that predicts
latent variables from observations. However, as they use inference models only for the initialization
of MCMC, these methods still rely on datapoint-wise sampling iterations. Not only is it time-
consuming, but implementations of such partially amortized methods also tend to be complicated
compared to the simplicity of AVI. To make MCMC more suitable for the inference of DLVMs, a
more straightforward and sophisticated framework of amortization is needed.
This paper proposes the amortized Langevin dynamics (ALD), which replace datapoint-wise MCMC
iterations with updates of an inference model that maps observations into latent variables (Figure 1
(B-2)). Since latent variables depend on the inference model, the updates of the inference model can
be regarded as implicit updates of latent variables, which enables us to perform posterior inference
without datapoint-wise MCMC iterations. Notably, our ALD treats outputs of the inference model
themselves as samples from the target distribution, whereas existing amortization methods use the
outputs only as initialization of MCMC. Therefore, ALD can be implemented straightforwardly like
AVI. Moreover, despite the simplicity, we can theoretically guarantee that ALD has the true posterior
as a stationary distribution under some assumptions, which is a critical requirement for valid MCMC
algorithms.
Although we have introduced ALD as a posterior sampling algorithm, the application of ALD is not
limited to sampling from posterior distributions. Recent studies have demonstrated that applying an
energy-based model (EBM) into the prior distribution over the latent variable enables more flexible
generative modeling for DLVMs. When we train an EBM, we have to obtain samples from the
EBM by running costly MCMC iterations. By extending our ALD to sampling from unconditional
distributions, we can apply ALD into sampling from such EBMs. When sampling from an EBM
with ALD, we prepare a function that maps fixed inputs into latent variables, and updates of EBM
samples are replaced with updates of the function’s parameters. In the same way with the posterior
case, the updates of the sampler function can be regarded as implicit updates of samples from EBMs.
Using our ALD for sampling from both the posterior and the EBM over the latent variable, we derive
a novel framework of learning DLVMs, which we refer to as the Langevin autoencoder (LAE). In-
terestingly, the learning algorithm of LAE naturally takes the combined form of an autoencoder-like
architecture and adversarial training. Our experiments show that ALD can properly obtain samples
from target distributions using toy datasets. Subsequently, we perform numerical experiments of the
image generation task using the SVHN, CIFAR-10, and CelebA-HQ datasets. Not only can LAE
2
Under review as a conference paper at ICLR 2022
be trained faster than non-amortized MCMC methods, but LAE can also generate better samples in
terms of the Frechet Inception Distance compared to AVI-based methods, such as VAE.
2	Preliminaries
2.1	Problem Definition
Consider a probabilistic model with the dx-dimensional observation x, the dz -dimensional continu-
ous latent variable z, and the model parameter Θ, as described by the probabilistic graphical model
shown in Figure 1 (A). Although the posterior distribution over the latent variable is proportional to
the prior and the likelihood: p (z | x) = p (z) p (x | z) /p (x), this is intractable due to the normal-
izing constant p (x) = p (z) p (x | z) dz. This study aims to approximate the posterior p (z | x)
for all n observations x(1) , . . . x(n) efficiently by obtaining samples from it.
2.2	Langevin Dynamics
Langevin dynamics (LD) (Neal, 2011) is a sampling algorithm based on the following Langevin
equation:
dz = -VzU (x, Z) dt + p2β-1dB,	(1)
where U is a Lipschitz continuous potential function that satisfies an appropriate growth condition,
β is an inverse temperature parameter, and Bis a Brownian motion. This stochastic differential
equation has pβ (Z | x) 8 exp(一βU (x, Z)) as its equilibrium distribution. We set β = 1 and
define the potential as follows to obtain the target posterior p (z | x) as its equilibrium:
U(x,Z) = -logp(Z) -logp(x | Z) .	(2)
We can obtain samples from the posterior by simulating Eq. (1) using the Euler-Maruyama method
(Kloeden & Platen, 2013) as follows:
Z0 〜N (z0; Z — ηVzU (x, Z), 2ηI),	(3)
where η is a step size for discretization. When the step size is sufficiently small, the samples asymp-
totically move to the target posterior by repeating this sampling iteration. LD can be applied to
any posterior inference problems for continuous latent variables, provided the potential energy is
differentiable on the latent space. However, to obtain the posterior samples for all observations
x(1), . . . x(n), we should perform iterations of Eq. (3) per data point, as shown in Figure 1 (B-1). It
is inefficient, mainly if the dataset is large. In addition, we need to re-run the time-consuming itera-
tions for new observations in test time. In the next section, we demonstrate a method that addresses
the inefficiency by amortization.
3	Amortized Langevin Dynamics
3.1	General Idea
As an alternative to the direct simulation of latent dynamics, we define an inference model fz|x ,
which maps the observation into the latent variable. Formally, the dynamics of its parameter Φ is
n
dΦ = — X VφU (X⑴，fz|x (X⑴；φ)) dt + √2dB.	(4)
i=1
Because the function fz|x outputs latent variables, the stochastic dynamics on the parameter space
induce dynamics on the latent space. The main idea of our amortized Langevin dynamics (ALD) is
to regard the transition on this induced dynamics as a sampling procedure for the posterior distribu-
tions, as shown in Figure 1 (B-2).
We can use the Euler-Maruyama method to simulate Eq. (4) like traditional LD:
φ0 〜N (φ0; φ — ηX VφU (x(i),fz|x (x(i)； φ)) , 2ηi) .	(5)
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Amortized Langevin dynamics
Φ J Initialize parameters
Z(1),...,Z(n) J- 0	. Initialize SampIe sets for all n data points
repeat
Φ j Φ0 〜N (Φ0; Φ - η Pn=ι Vφ U (X ⑴,Zei) = fz∣χ (χ(i); φ)) , 2ηφi)
Z(I),..., Z(n)J Z(1)∪ {fz∣χ (X ⑴；Φ)} ,..., Z(N) ∪ {fz∣χ (x(n); Φ)}	. Add samples
until convergence of parameters
return Z(1), . . . , Z(n)
Through the iterations, the posterior sampling is implicitly performed by collecting outputs of the
inference model for each data point as described in Algorithm 1. Note that Z(i) denotes a set of
samples of the posterior for the i-th data (i.e., p z | X(i) ) obtained using ALD. When we perform
inference for new test data, the trained inference model can be used to initialize an MCMC method
(e.g., traditional LD) because it is expected that the trained inference model can map data into the
high-density area of the posteriors.
By this amortization, we replace the direct update of latent variables (z(1), . . . , z(n)) with the update
of the global parameter Φ. A significant advantage of amortization is that the cost of MCMC can be
reduced by using minibatch training. For minibatch training, we substitute the minibatch statistics
of m data points for the derivative for all n data.
nm
X VφU (χ(i),fz∣χ (x(i); φ)) ≈ - X VφU (χ(i),fz∣χ (x(i))).
i=1	m i=1
We refer to the minibatch version of ALD as stochastic gradient amortized Langevin dynamics
(SGALD). SGALD enables us to sample from posteriors ofa massive dataset efficiently. Moreover,
in the context of stochastic gradient LD (SGLD), it is known that adaptive preconditioning effec-
tively improves convergence compared to the naive SGLD (Li et al., 2016)2. This preconditioning
technique is also applicable to our SGALD, and we employ it throughout our experiments.
3.2	Theoretical Analysis
To justify our ALD as a posterior sampling algorithm, we provide a theoretical analysis of the
stationary distribution of our ALD algorithm. Here, X and Z denote matrices with X(i) and z(i) in
rows Xi,: and Zi,:, respectively. Our main result is as follows:
Theorem 1. Let q (Z | X) be a stationary distribution of the latent variables induced by Eq.
(4). When the mapping fz∣χ meets the following conditions, q (Z | X) satisfies q (Z | X) H
exp (-U (X, Z)) := exp (- Pn=I U (x(i), z(i))).
1.	The mapping has the form of fz|x (X; Φ) = Φg (X), where Φ is a dz × d matrix, g is a
mapping from Rdx to Rd, and d is the dimensionality ofg (X).
2.	The rank ofG is n, where G is a matrix with g X(i) in row Gi,:.
See Appendix A for the proof. Theorem 1 suggests that samples obtained by ALD asymptotically
converge to the true posterior when we construct the inference model fz|x with an appropriate form.
Practically, we can implement such a function using a neural network whose parameters are fixed
except for the last linear layer. In this implementation, the last linear layer takes a role of the
parameter Φ, and the preceding feature extractor takes a role of the function g. In our experiments,
we randomly initialize weights of the feature extractor and freeze them throughout the training.
In addition, the dimensionality of the last linear layer should be sufficiently large to meet the second
condition. Therefore, the second condition derives a trade-off between approximation quality and
2The relationship between the naive SGLD and the preconditioned version is almost identical to the naive
stochastic gradient descent and RMSProp.
4
Under review as a conference paper at ICLR 2022
computational costs. It is worth noting that a similar trade-off is also known in the context of AVI.
In AVI, the approximation quality is influenced by the capacity of the inference model, and the gap
between the optimal variational distribution and the amortized distribution is often denoted as the
amortization gap (Cremer et al., 2018). In experiments, we confirm that preserving the condition
does not become a significant computational overhead in practice. In addition, we should note that
decaying the step size η is needed to ensure convergence when we perform the simulation with
discretization as described by Welling & Teh (2011).
3.3	Extension to Unconditional Cases
Currently, we have introduced ALD as a sampling algorithm for conditional posterior distributions.
We can also apply ALD to sampling from unconditional unnormalized distributions, namely energy-
based models (EBMs). Consider an unconditional distribution over a random variable z defined by
an energy function fz as follows:
P (z) (X exp(-fz (Z)),	(6)
where fz maps the variable z into a scalar value. To obtain samples from this EBM using ALD,
we prepare a sampler function fz|u (u; Ψ) that maps its input u into the variable z. Here, the input
vector u is fixed, whereas observations are used in the posterior case. To run multiple MCMC
chains in parallel, we prepare k fixed inputs u(1), . . . , u(k) and update the parameter of the sampler
function as follows:
Ψ0 〜N (ψ - ηXX Vψfz (fz∣u (U⑺;ψ)) , 2ηi).	⑺
Typically, the fixed input vectors u(1), . . . , u(k) are chosen from a standard Gaussian distribution.
As in the posterior case, we can guarantee the stationary distribution matches the EBM by choosing
an appropriate form for the function fz|u, and such a function can be implemented using a neural
network whose parameters are fixed except for the last linear layer. For minibatch training, we can
substitute the gradient for all k chains with the stochastic gradient of m minibatch chains:
km
X Vψfz (fz|u (U(A ψ)) ≈ mm X Vψfz (fz|u (U⑴;ψ)).	(8)
The advantage of using amortization in the unconditional case is that we can run massive chains
parallel using minibatch training.
4 Langevin Autoencoders
Using ALD for sampling from both the posterior and the energy-based prior, we derive a novel
framework for learning DLVMs. We here consider a latent variable model defined as follows:
P (z | Θ) X exp (-fz (z; Θ)), P (X | z, Θ) = N (x; fχ∣z (z; Θ), diag(σ)) ,	(9)
where σ is a variance parameter of a Gaussian distribution3. To learn the latent variable model, we
need to estimate both the model parameter Θ and the latent variable z. In Bayesian learning, the
estimation is represented as the joint posterior distribution P Θ, z(1), . . . , z(n) | x(1), . . . , x(n) 4.
To obtain samples from the posterior, we can combine traditional LD and ALD as follows:
Θ0 〜N (Θ0; Θ - ηVθU (X,fz∣χ (X； Φ), Θ) , 2ηI) ,	(10)
Φ0 〜N (Φ0; Φ - ηVφU (X,fz∣χ (X; Φ), Θ) , 2ηI) ,	(11)
n
U(X,Z,Θ) := -logP(Θ) -X logP z(i) | Θ +logP x(i) | z(i),Θ ,	(12)
i=1
where fz|x (X; Φ) is a matrix with fz|x x(i); Φ in its i-th row. If we omit the Gaussian noise
injection in Eq. (10), it corresponds to gradient ascent for maximum a posteriori (MAP) estimation
3We also include the variance σ into Θ and treat it as a learnable parameter.
4We also provide the learning algorithm of maximum likelihood in Appendix B.
5
Under review as a conference paper at ICLR 2022
Algorithm 2 Langevin Autoencoders
Θ, Φ, Ψ J Initialize parameters
repeat
Θ J Θ0 〜N (Θ0; Θ — ηVθL (Θ, Φ, Ψ), 2ηI)
Φ J Φ0 〜N(Φ0; Φ — ηVφL (Θ, Φ, Ψ), 2ηi)
ψ J ψ0 〜N (Ψ0; ψ + ηVψL (Θ, φ, Ψ), 2ηi)
until convergence of parameters
return Θ, Φ, Ψ
. Update the generative model
. Update the inference model
. Update the sampler model
of Θ; if we additionally use a flat prior for p (Θ), it yields the maximum likelihood estimation
(MLE). In this study, we assume a flat prior forp (Θ) and omit the notation for simplicity.
In Eq. (10), we cannot calculate the derivative of the potential function VΘ U in a closed-form
because the latent prior p (z | Θ) is defined using an unnormalized energy function. However, we
can obtain the unbiased estimator of the derivative by obtaining samples from the prior as follows:
VΘU (X, Z, Θ)
nk
≈ X Vθfz (Z⑺；θ) - Vθ logP (X⑺ | Z⑴,θ) - k X Vθfz (z(j)； θ) ,	(13)
i=1	j=1
where Z(1) * * * 5 &,... Z(n) are sampled from the latent prior P (Z | Θ) (see Appendix C for the derivation).
To get samples from the latent prior, we can also use ALD by preparing a sampler function fz|u that
takes fixed inputs u(1), . . . , u(k), as described in Section 3.3. Here, we set the number of chains
equal to the number of data points for simplicity, i.e., k = n.
In summary, the encoder fz|x, the decoder fx|z, and the latent energy function fz are trained by min-
imizing the following loss function L, whereas the latent sampler fz|u is trained by maximizing it,
while stochastic noise of the Brownian motion is injected in their update in order to avoid shrinking
to MAP estimates:
L(Θ,Φ,Ψ)	(14)
n
=X fz (fz|x (x(i)； φ) ； θ) - fz (fz|u (U⑺；ψ) ； θ) - logP(X⑴ | fz|x (X⑴；φ) , θ).
i=1
We refer to this framework of learning DLVMs as the Langevin autoencoder (LAE). We summarize
the algorithm of the LAE in Algorithm 2. LAE is closely related to the traditional autoencoder (AE)
and other deep generative models, such as the variational autoencoder (VAE) and the generative
adversarial network (GAN). We discuss the relationship in the next section in detail.
5 Related Works
Amortized inference is well-investigated in the context of variational inference. It is often re-
ferred to as amortized variational inference (AVI) (Rezende & Mohamed, 2015; Shu et al., 2018).
The basic idea of AVI is to replace the optimization of the datapoint-wise variational parameters
with the optimization of shared parameters across all data points by introducing an inference model
that predicts latent variables from observations. The AVI is commonly used in generative models
(Kingma & Welling, 2013), semi-supervised learning (Kingma et al., 2014), anomaly detection (An
& Cho, 2015), machine translation (Su et al., 2018), and neural rendering (Eslami et al., 2018; Ku-
mar et al., 2018). However, in the MCMC literature, there are few works on such amortization.
Han et al. (2017) use traditional LD to obtain samples from posteriors to train deep latent variable
models. Such Langevin-based algorithms for deep latent variable models are called alternating
back-propagation (ABP) and are applied in several fields (Xie et al., 2019; Zhang et al., 2020; Xing
et al., 2018; Zhu et al., 2019). However, ABP requires datapoint-wise Langevin iterations, causing
slow convergence. Moreover, when we perform inference for new data in test time, ABP requires
6
Under review as a conference paper at ICLR 2022
MCMC iterations from randomly initialized samples again. Although Li et al. (2017) and Hoff-
man (2017) propose amortization methods for MCMC, they only amortize the initialization cost in
MCMC by using an inference model. Therefore, they do not entirely remove datapoint-wise MCMC
iterations.
Autoencoders (AEs) (Hinton & Salakhutdinov, 2006) can be seen as a particular case of LAEs,
wherein the Gaussian noise injection to the update of the inference model (encoder) and the gener-
ative model (decoder) is omitted in Eqs. (10) and (11), and a flat prior is used forp (z | Θ). When a
different distribution is used as a latent prior, it is known as sparse autoencoders (SAEs) (Ng et al.,
2011). In these cases, the dynamics in Eqs. (10) and (11) are dominated by gradient VU; hence,
both the latent variables and the model parameter converge to MLE or MAP estimates (or other
stationary points). Therefore, AEs (and SAEs) can be considered MLE (and MAP) algorithms for
the parameter Θ and the latent variables Z.
Variational Autoencoders (VAEs) are based on AVI, wherein an inference model (encoder) is de-
fined as a variational distribution q (z | x; Φ) using a neural network. Its parameter Φ is optimized
by maximizing the evidence lower bound. Interestingly, there is a contrast between VAE and LAE
when stochastic noise is used in posterior inference. In VAE, noise is used to sample from the
stochastic inference model in calculating the potential U, i.e., in the forward calculation. However,
in LAE, the inference model itself is deterministic, and stochastic noise is used for its parameter
update along with the gradient calculation Vφ U, i.e., in the backward calculation. The advantage
of LAE over VAE is that LAE can flexibly approximate complex posteriors by obtaining samples,
whereas VAE’s approximation ability is limited by choice of variational distribution q (z | x; Φ)
because it requires a tractable density function. Although there are several considerations in the
improvement of the approximation flexibility, these methods typically have architectural constraints
(e.g., invertibility and ease of Jacobian calculation in normalizing flows (Rezende & Mohamed,
2015; Kingma et al., 2016; Van Den Berg et al., 2018; Huang et al., 2018; Titsias & Ruiz, 2019)), or
they incur more computational costs (e.g., MCMC sampling for the reverse conditional distribution
in unbiased implicit variational inference (Titsias & Ruiz, 2019)).
Energy-based Models’ training is challenging, and many researchers have been studying method-
ology for its stable and practical training. A significant challenge is that it requires MCMC sampling
from EBMs, which is challenging to perform in high dimensional space. Our LAE avoids this dif-
ficulty by defining the energy function in latent space rather than data space. A similar approach is
taken in several works (Pang et al., 2020a;b), but they use traditional LD to obtain latent samples
without amortization.
Generative adversarial networks (GANs) are closely related to our LAE because both are trained
using adversarial loss functions. For a detailed discussion, see Appendix D.
6	Experiment
In our experiment, we first test our ALD algorithm on toy examples to investigate its behavior, then
we show the results of its application to the training of deep generative models.
6.1	Toy Examples
We perform numerical simulation using toy examples to demonstrate that our ALD can properly ob-
tain samples from target distributions in conditional and unconditional cases. First, we use examples
where the posterior density can be derived in a closed-form. We initially generate three synthetic
data x1, x2, x3, where each xi is sampled from a bivariate Gaussian distribution as follows:
P (Z) = N (z; μz, ∑z), P (X | Z) = N (x; Z Σχ).
In this case, we can calculate the exact posterior as follows:
P(z I X) = N (z; (∑z1 + ∑-1)-1 (∑z1μz + ∑x 1χ), (∑z1 + ∑x 1)T),
0
0
In this experiment, We set μz
10
Σz =	0 1 , and Σx
0.7
0.7
0.6
0.8
. We simulate
our ALD algorithm for this setting to obtain samples from the posterior. We use a neural netWork
7
Under review as a conference paper at ICLR 2022
(A) Conditional Case
(B) Unconditional Case
Figure 2: Visualization of ground truth density (left) and samples by ALD (right) in the conditional
case (A) and the unconditional case (B) in toy examples.
Figure 3: Evolution of sample values across MCMC iterations for traditional LD and our SGALD
in univariate Gaussian examples. The black lines denote the ground truth posteriors (the solid lines
show the mean values, and the dashed lines show the standard deviation).
of three fully connected layers of 128 units with ReLU activation for the inference model fz|x ;
setting the step size to 4 × 10-4, and update the parameters for 3,000 steps. We omit the first 1,000
samples as burn-in steps and use the remaining 2,000 samples for qualitative evaluation. The result
is shown in Figure 2 (A). ALD produces samples that match the shape of the target distributions
well, even though ALD does not perform direct updates of samples in the latent space. We also
performed a similar experiment in a univariate setting to see the convergence speed of our SGALD,
the minibatch version of ALD (see Appendix F.1 for the detailed experimental setting). Figure 3
shows the evolution of obtained sample values by traditional LD and our SGALD. It can be observed
that SGALD’s samples converge much faster than traditional LD.
In addition to the simple conjugate Gaussian example, we
experiment with a complex posterior, wherein the likeli-
hood is defined with a randomly initialized neural net-
work. For comparison, we also implement the amortized
variational inference (AVI) method, in which the pos-
terior is approximated with a Gaussian distribution pa-
rameterized by a neural network (see Appendix F.2 for
more experimental details). Figure 4 shows a typical ex-
ample, which characterizes the difference between AVI
and ALD. The advantage of our ALD over AVI is the
flexibility of posterior approximation. AVI methods typ-
ically approximate posteriors using variational distribu-
GT	AVI ALD
Figure 4: Visualizations of a ground
truth posterior (left), an approximation
by AVI (center), and samples by ALD
(right) in the neural likelihood example.
tions, which have tractable density functions. Hence, their approximation power is limited by the
choice of variational distribution family, and they often fail to approximate such complex posteri-
ors. On the other hand, ALD can capture such posteriors well. The results in other examples are
summarized in Figure 6 in the appendix.
Furthermore, we also test our ALD for sampling from an unconditional distribution. In this experi-
ment, we use a mixture distribution of eight Gaussians and obtain samples using ALD, as shown in
Figure 2 (B). We can observe that ALD adequately captures the actual density’s multimodality and
works well in the unconditional case.
8
Under review as a conference paper at ICLR 2022
Table 1: Quantitative results of the image generation for SVHN, CIFAR-10, and CelebA-HQ. We
report the mean and standard deviation of the Frechet Inception Distance in three different seeds.
Description		SVHN	CIFAR-10	CelebA-HQ	
		32 X 32	32 X 32	32 X 32	64 X 64
VAE	VI + amortization	47.19 ± 0.96	106.0 ± 0.5	102.6 ± 1.4	174.9 ± 0.9
VAE-floW	VI + amortization + floW	46.69 ± 0.69	105.5 ± 1.0	101.2 ± 1.1	174.5 ± 0.8
ABP	LD	46.90 ± 1.07	105.6 ± 0.2	99.27 ± 2.42	135.7 ± 2.0
DLGM	LD + amortized init.	46.86 ± 1.04	102.3 ± 1.76	73.64 ± 1.81	139.9 ± 3.4
LEBM	LD + EBM	38.79 ± 2.48	97.02 ± 0.36	32.59 ± 0.30	53.31 ± 2.26
LAE	LD + EBM + amortization	46.66 ± 1.33	95.85 ± 1.06	40.33 ± 1.33	61.38 ± 1.20
6.2	Image Generation
To demonstrate the applicability of our LAE to the generative model training, we experiment on
image generation tasks using SVHN, CIFAR10, and CelebA-HQ datasets. Note that our goal here
is not to provide the state-of-the-art results on image generation benchmarks but to verify the ef-
fectiveness of our ALD as a method of approximate inference in deep latent variable models. For
this aim, we compare our LAE with five baseline methods, as shown in Table 1. VAE (Kingma &
Welling, 2013) is one of the most popular deep latent variable models in which the posterior distri-
bution is approximated using the AVI. VAE-flow is an extension of VAE in which the flexibility of
AVI is improved using normalizing flows. In addition to AVI-based methods, we use three methods
based on Langevin dynamics (LD). The alternating back-propagation (ABP) uses traditional LD to
approximate the posterior, and the deep latent Gaussian model (DLGM) uses a VAE-like inference
model to initialize LD. The latent energy-based model (LEBM) uses an EBM for the latent prior,
and the EBM and posterior sampling is performed via traditional LD. LEBM can be regarded as a
non-amortization version of our LAE.
We apply a commonly used convolutional neural network-based architecture for all models and a
multi-layer perceptron for an energy-based model in the latent space of LAE and LEBM. Please
refer to Appendix F.3 for more detailed experimental settings. For quantitative evaluation of the
sample quality, We report the FreChet Inception Distance (FID) (HeUSel et al., 2017).
The results are summarized in Table 1. It can be observed that LAE outperforms VI-based methods
in terms of FID, although it does not reach LEBM’s performance in SVHN and CelebA-HQ. In
training speed, LAE takes 34.12 seconds per epoch on average to train With CIFAR-10, While LEBM
and DLGM take 84.29 and 60.66 seconds per epoch, respectively. This result shoWs that LAE
is approximately 2.47 times faster than the non-amortized LEBM and 1.78 times faster than the
partially amortized DLGM.
7	Conclusion
This paper proposed amortized Langevin dynamics (ALD), an efficient MCMC method for deep
latent variable models. The ALD amortizes the cost of datapoint-Wise iterations by using inference
models. We shoWed that our ALD algorithm could accurately approximate posteriors With both
theoretical and empirical studies. Using ALD, We derived a novel scheme of deep generative models
called the Langevin autoencoder (LAE). We demonstrated that our LAE performs better than VI-
based methods in sample quality and can be trained faster than non-amortized LD methods.
This study Will be the first step to further Work on efficient MCMC for latent variable models With
large-scale datasets. For instance, deriving a Metropolis-Hastings rejection step for ALD and algo-
rithms based on Hamiltonian Monte Carlo methods is an exciting direction of future Work. More-
over, developing more sophisticated Way of choosing the feature extractor of the inference model
is also important. In our experiments, We used a randomly initialized neural netWork that is fixed
throughout the training, but there could be a better Way to improve the performance of LAE.
9
Under review as a conference paper at ICLR 2022
References
Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruc-
tion probability. Special Lecture on IE, 2(1), 2015.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. In International Conference on Machine Learning, pp. 1078-1086. PMLR, 2018.
SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Gar-
nelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene
representation and rendering. Science, 360(6394):1204-1210, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Alternating back-propagation for generator
network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.
Hao He, Hao Wang, Guang-He Lee, and Yonglong Tian. Probgan: Towards probabilistic gan with
theoretical guarantees. In International Conference on Learning Representations, 2018.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Matthew D Hoffman. Learning deep latent gaussian models with markov chain monte carlo. In
International conference on machine learning, pp. 1510-1519. PMLR, 2017.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In International Conference on Machine Learning, pp. 2078-2087. PMLR, 2018.
Rie Johnson and Tong Zhang. Composite functional gradient learning of generative adversarial
models. In International Conference on Machine Learning, pp. 2371-2379. PMLR, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems, pp.
3581-3589, 2014.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in neural information
processing systems, pp. 4743-4751, 2016.
Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations, vol-
ume 23. Springer Science & Business Media, 2013.
Ananya Kumar, SM Eslami, Danilo J Rezende, Marta Garnelo, Fabio Viola, Edward Lockhart,
and Murray Shanahan. Consistent generative query networks. arXiv preprint arXiv:1807.02033,
2018.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic
gradient langevin dynamics for deep neural networks. In Thirtieth AAAI Conference on Artificial
Intelligence, 2016.
Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised mcmc. arXiv
preprint arXiv:1702.08343, 2017.
10
Under review as a conference paper at ICLR 2022
Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo,
pp. 113, 2011.
Andrew Ng et al. Sparse autoencoder. CS294A Lecture notes,72(2011):1-19, 2011.
Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space
energy-based prior model. arXiv preprint arXiv:2006.08205, 2020a.
Bo Pang, Erik Nijkamp, Jiali Cui, Tian Han, and Ying Nian Wu. Semi-supervised learning by latent
space energy-based model of symbol-vector coupling. arXiv preprint arXiv:2010.09359, 2020b.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. arXiv
preprint arXiv:1710.05941, 7:1, 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional conference on machine learning, pp. 1530-1538. PMLR, 2015.
Yunus Saatci and Andrew Wilson. Bayesian gans. In Advances in neural information processing
systems, pp. 3624-3633, 2017.
Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized infer-
ence regularization. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 4398-4407, 2018.
Jinsong Su, Shan Wu, Deyi Xiong, Yaojie Lu, Xianpei Han, and Biao Zhang. Variational recurrent
neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In The 22nd Inter-
national Conference on Artificial Intelligence and Statistics, pp. 167-176. PMLR, 2019.
Rianne Van Den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester nor-
malizing flows for variational inference. In 34th Conference on Uncertainty in Artificial In-
telligence 2018, UAI 2018, pp. 393-402. Association For Uncertainty in Artificial Intelligence
(AUAI), 2018.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Learning dynamic
generator model by alternating back-propagation through time. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 33, pp. 5498-5507, 2019.
Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, and Ying Nian Wu. Deformable gen-
erator network: Unsupervised disentanglement of appearance and geometry. arXiv preprint
arXiv:1806.06298, 2018.
Jing Zhang, Jianwen Xie, and Nick Barnes. Learning noise-aware encoder-decoder from noisy
labels by alternating back-propagation for saliency detection. arXiv preprint arXiv:2007.12211,
2020.
Yizhe Zhu, Jianwen Xie, Bingchen Liu, and Ahmed Elgammal. Learning feature-to-feature trans-
lator by alternating back-propagation for generative zero-shot learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 9844-9854, 2019.
A Proof of Theorem 1
First, we prepare some lemmas.
11
Under review as a conference paper at ICLR 2022
Lemma 2. Let h : Rdz ×d T RdZ ×n be a linear map defined by h (Φ) := ΦG. When the rank of G
is n, there exists an orthogonal linear map τ : Rdz ×d → Rdz ×d such that τ (Φ) = [Φ⑴，Φ⑵]
satisfies ker h = span ∣0dz×n, Φ ⑵],where h := h o τ-1, Φ ⑴：=[φι,..., φj, Φ ⑵
[φn+ι,... φd] and φi ∈ RdZ for i = 1,..., d.
Proof. The singular value decomposition of G is represented by TGTT
~
Λ
0(d-n×n)
,where
Λ is a n × n diagonal matrix Λ = dιag (λ1,..., λn), and T and T are orthogonal matrices. Since
the rank of G is n, λ1,..., λn are non-zero. When we set T (Φ) := ΦT>, we obtain
h Φ := h
τ-1
〜
Φ TG
λ T t.
0(d-n×n)
(15)
From the above equation, ker h = span [0d∕×n, Φ(2)] holds.
Lemma 3. For Φ ∈ Rdz ×d, let Φ satisfy:
Φ=他⑴母(2)] = T (Φ).
A map h(1) : Rdz ×n T Rdz ×n defined by h(1) (Φ(I)):=
h(1) (Φ(I)) and is linear isomorphic.
h ( Φ(1), 0 ) satisfies ΦG
Proof. From the definition, we have
ΦG = T-1 Φ G = ΦTG
Φ (TGTT) T
∖Φ(I),Φ⑵][
Λ
0(d-n×n)
〜
〜
Φ
~
T
□
俾Z 0] [ 0(dl×n)
τ-1 (∣^Φ(I), 0]) G
~
T
h o T-1 ([Φ(I), 0
h(1) (φ(I)).
By the definition, h(1) is linear. Here, h(1) is injective, since ker h = span [0d∕×n,吊⑵],and
hence, dim (Im h(1)) ≥ dz × n. Since Im h(1) U Rdz ×n, h(1) is surjective.	□
Lemma 4. For V : RD 3 φ → V (Φ) := U (X, ∕z∣x (X; Φ)) ∈ R, Φ = [Φ⑴,Φ叫：=T (Φ),
V := V o τ-1 and V(I) (Φ(I)) := V 0Φ(I), 0dz×(d-n)0, Eq. (4) is equivalent to
dΦ⑴=-V4(1)V⑴(Φ(I)) dt + √2dB,
dΦ ⑵=√2dB.
(16)
(17)
12
Under review as a conference paper at ICLR 2022
Proof. By direct calculation, we obtain
V ([φ ⑴,φ ⑵D=V ◦ τ-ι ([φ ⑴,φ ⑵D
=U (X,fz|x(X; TT ([Φ⑴母⑵D))
=U(X,h (τ-1 (但⑴,Φ⑵D))
=U (X ,h O T-1 ([φ (1), 0]) + h。T-1 ([0母(2)]))
=U(X ,h(τ-1 ([φ(1), 0])))
=U (X ,fz∣χ(X; TT ([Φ(I), 0])))
=V OT-1 ([Φ(I), 0])
=V ([Φ(I), 0]) .	(18)
Then, the following equivalence holds:
dΦ = -VφV (Φ) dt + √2dB,
⇔ dτ-1 (Φ) = -TT (vφ V (Φ)) dt + √2dB
⇔ dΦ = -τ 0 τT (vi,V (Φ)) dt + √2dτ (B)
=-VφV (Φ) dt + √2dB,
where we used T 0 TT = id because T is orthogonal. From Eq. (18), the dynamics in Eq. (19) is
equivalent to Eq. (16) and Eq. (17).	□
In the following, we prove Theorem 1 using the above lemmas. The latent variables Z :=
h⑴ (φ⑴)is independent of Φ⑵，and the probability distribution q (Z | X) of Z is given by
the pushforward measure (h(1))	(PT)) (Z) of the probability distribution p(1) of Φ by h(1). The
amortized Langevin dynamics has q (Z | G) as its stationary distribution of Z. Then, we have
q (Z | X )= (h(I)) # (p!I)) (Z)
p(
det叱尸
dZ
(Z), 0	G
exp I —U I X, T
= exp(-U (X, Z)),
where we used that %；； = h(1) because of the linearity of h(1) and is constant with respect to
Z. The last equation is derived as follows. From Lemma 3, ΦG = h(1) (Φ⑴)holds when
13
Under review as a conference paper at ICLR 2022
Algorithm 3 Amortized Langevin dynamics (test time)
Z J fz∣χ (x; Φ*)	. Initialize a sample using a trained inference model
Z J 0	. Initialize a sample set
repeat
z J z0 〜N (z0; z 一 NzU (x, Z), 2ηI)	. Update the sample using traditional LD
Z J Z ∪ {z}	. Add samples
until convergence of parameters
return Z
(φ (I)) = ΦG
□
B Maximum Likelihood Training for LAE
In Section 4, we derive Bayesian learning algorithm of LAE, where the whole model is formulated
as a Bayesian neural network. We can also think of the frequentist approach, where the training is
defined as maximum likelihood. In this case, the objective is to maximize the evidence logp (x | Θ),
and its derivative is as follows.
Vθ logP (X | Θ)
=Vθ log /p (x, z | Θ) dz
=pτx⅛ / vθp (X，Z |θ)dz
=/ P (χ, z 吸) Vθ log P (x, Z | Θ) dz
P(X | Θ)
= Ep(z∣x,Θ) [VΘlogP(X,z | Θ)]
= Ep(z∣x,Θ) [VΘlogP(X | z, Θ) + VΘlogP(z | Θ)]
= Ep(z∣x,Θ) [VΘlogP(X | z, Θ) 一 VΘfz (z; Θ)] + Ep(z∣Θ) [VΘfz (z; Θ)] .
(20)
(21)
(22)
(23)
(24)
(25)
(26)
We can approximate the derivative by obtaining samples from the posterior P (z | X, Θ) and the
latent prior P (z | Θ) using LAE as in Eq. (5) and (7). Hence, the maximum likelihood version of
LAE is summarized as shown in Algorithm 4.
C Derivation of Eq. (13)
VΘU(X,Z,Θ)
n
=X Vθfz (Z⑴；θ) -Vθ logp(X(i) | Z⑺，θ) + Vθ log C (Θ),
i=1
14
Under review as a conference paper at ICLR 2022
Algorithm 4 Maximum likelihood version of Langevin Autoencoders
Θ, Φ, Ψ J Initialize parameters
repeat
repeat
Φ J Φ0 〜N (Φ0; Φ 一 ηVφL (Θ, Φ, Ψ), 2ηI)	. Update the inference model
Ψ J Ψ0 〜N (Ψ0; Ψ + ηVψL (Θ, Φ, Ψ), 2ηI)	. Update the sampler model
until convergence of Φ and Ψ
Θ J Θ 一 ηVΘL (Θ, Φ, Ψ)	. Update the generative model
until convergence of Θ
return Θ, Φ, Ψ
where C (Θ) = J exp (-fz (z; Θ)) dz. By direct calculation, We obtain
Vθ log C (Θ) = C(Θ) VθC (Θ)	(27)
=C1θ) V Vθ exp(-fz (z; Θ)) dz	(28)
e exp(-fz (z； Θ))	C∖，	Ce
=-J -------C(Θ)------Vθfz (z; Θ) dz	(29)
= 一	p (z | Θ) VΘfz (z; Θ) dz	(30)
=-Ez〜p(z∣θ) [Vθfz (z; Θ)]	(31)
1k
≈- k X Vθfz (z(j); θ) ,	(32)
j=1
where Z* (I) * * * * &,... z(n) are samples drawn fromP (z | Θ).
D Additional Related Work
Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is similar to LAE in that both
are trained by minimax game between two functions (i.e., the energy function and the sampler func-
tion in LAE; the discriminator and the generator in GAN). However, there are some differences be-
tween them. First, the minimax game is performed in the latent space in LAE, while it is performed
in the observation space in GAN. In other words, the latent variable is identical to the observation
(i.e., p (x | z) = 1x=z) in GAN. Note that the latent variable z is different from the input of GAN’s
generators. Here, the input of the GAN’s generators is denoted as u for the analogy with LAE.
Second, the loss function is slightly different. In GAN, the loss function is as follows:
n
LGAN (Θ, Ψ) = - X log fx (X⑺;θ) + log (1 - fx (fx|u (U⑴;ψ) ; θ)) ,	(33)
i=1
where fx|u denotes the generator that maps its inputs u into the observation space, and fx denotes
the discriminator that maps from the observation space into (0,1), and u(i)〜 N (u; 0, I). The
discriminator is trained to minimize this loss function, whereas the generator is trained to maximize
it. The main difference to the loss function of LAE is the second term. When we substitute it with
- log fx fx|u u(i); Ψ ; Θ , it becomes more similar. This modification is often used to alleviate
gradient vanishing and stabilize the training of GAN’s generator (Goodfellow et al., 2014; Johnson
& Zhang, 2018). In this formulation, the counter parts of the energy function and the sampler
function are - log fx (∙; Θ) and fχ∣u (∙; Ψ), respectively.
Another difference between LAE and GAN is that the input vector of the sampler function is fixed
through the training in LAE, whereas the input of the generator changes per iteration by sampling
from N (u; 0, I) in GAN. Furthermore, LAE is trained using noise injected gradient, whereas GAN
15
Under review as a conference paper at ICLR 2022
Ground Truth
d>n
Figure 5: Analysis on amortization gap.
d<n
is trained with a standard stochastic optimization method like SGD. In GAN, the discriminator is
also trained with standard stochastic optimization, which corresponds to the MLE case where noise
injection is omitted. Although there are some investigations to apply Bayesian approach to GAN
(Saatci & Wilson, 2017; He et al., 2018), their discriminators are not defined as energy functions.
Wasserstein GANs (WGANs) (Arjovsky et al., 2017) also have a loss function similar to LAE’s:
n
LWGAN (Θ, Ψ) = - X fχ (χ(i); θ) - fχ & (U⑴;Ψ) ； θ) ,	(34)
i=1
where D denotes the discriminator of WGANs that maps from the observation space into the real
space R. In this case, the counter part of the energy function is -D (x; Θ), although D has a
constraint of 1-Lipschitz continuity, which the energy function of LAE does not has.
E	Additional Experiment
We perform an additional experiment to investigate the amortization gap when the capacity of the
inference model is not enough to meet the second condition of Theorem 1. We use the same experi-
mental setting with the bivariate Gaussian example in Section 6.1, and change the dimensionality of
the last linear layer of the inference model from 2 (< d) to 128 (> d). The results are summarized
in Figure 5. It can be observed that the sample quality is good when the dimensionality of the last
linear layer is equal to or greater than the number of data points (i.e., d ≥ n). When the dimension-
ality is smaller than the number of data points, the samples for some data points shrink to a small
area, while good samples are obtained for the remaining data points.
F Experimental Settings
F.1 Conjugate Univariate Gaussian Example
In the experiment of conjugate univariate Gaussian example, we initially generate 100 synthetic data
x(1), . . . , x(100), where each x(i) is sampled from a univariate Gaussian distribution as follows:
P (Z) = N (z; μz,σ2
p (x | z) = N x; z, σx2
In this experiment, We set μz = 0,σ2 = 1,σ2
posterior as follows:
0.01. In this case, We can calculate the exact
p (z | x)
N卜?⅛
In this experiment, We obtain 20,000 samples using SGALD. We use four fully-connected layers of
128 units With tanh activation for the inference model and set the step size ηφ to 0.001. We set the
batch size to 10.
16
Under review as a conference paper at ICLR 2022
Table 2: Neural network architectures
Encoder	Decoder
x ∈ Rdx → Conv3x1x2x64 → Conv4x2x2x128 → Conv4x2x2x256 → Conv4x2x2x512 → Conv6x1x0xn → Linear dz	Z ∈ Rdz → Deconv6x1x0x512 → DeConv4x2x0x256 → DeConv4x2x0x128 → DeConv4x2x0x64 → Conv3x1x0xdx
Energy	Sampler
z ∈ Rdz → Linear2048 → Linear1	U ∈ Rdu → Linear2048 → Linear n → Linear dz
F.2 Neural likelihood example
We perform an experiment with a complex posterior, wherein the likelihood is defined with a ran-
domly initialized neural network fθ. Particularly, we parameterize fθ by four fully-connected layers
of 128 units with ReLU activation and two dimensional outputs like p (x | z) = N fθ (z) , σx2I .
We initialize the weight and bias parameters with N (0, 0.2I) and N (0, 0.1I), respectively. In ad-
dition, we set the observation variance σx to 0.25. We used the same neural network architecture for
the inference model fφ. Other settings are same as the previous conjugate Gaussian experiment.
The results are shown in Figure 6. The left three columns show the density visualizations of the
ground truth or approximation posteriors of AVI methods; the right two columns show the visual-
izations of 2D histograms and samples obtained using ALD. For AVI method, we use two different
models. One uses diagonal Gaussians, i.e., N (μ (x; φ), diag (σ2 (x; φ))), for the variational dis-
tribution, and the oher uses Gaussians with full covariance N (μ (x; φ), Σ(x; φ)). From the den-
sity visualization of GT, the true posterior is multimodal and skewed; this leads to the failure of the
Gaussian AVI methods notwithstanding considering covariance. In contrast, the samples of ALD
accurately capture such a complex distribution, because ALD does not need to assume any tractable
distributions for approximating the true posteriors. The samples of ALD capture well the multimodal
and skewed posterior, while Gaussian AVI methods fail it even when considering covariance.
17
Under review as a conference paper at ICLR 2022
Figure 7: Generated images by LAE.
F.3 Image Generation
Our implementation of baseline models is based on the official public code of Pang et al. (2020a)
which is available at https://github.com/bpucla/latent-space-EBM-prior. The
architecture of neural networks for LAE is summarized in Table 2. Conv k x s x p x c denotes
a convolutional layer with k × k kernel, s × s stride, p × p padding, and c output channels.
Deconv k x s x p x c denotes a transposed convolutional layer with k × k kernel, s × s stride,
p × p padding, and c output channels. Linear d is a fully connected layer of output dimension
d. We apply the swish function (Ramachandran et al., 2017) as activation after each convolution,
transposed convolution or linear layer except the last one. dx , dz and du are the dimensionality of
x, z and u respectively. We fix the parameters in the encoder and the sampler except for the last
linear layer to meet the condition of Theorem 1. For all datasets, we set the minibatch size m to
100. The latent dimensionality dz is set to 64 for SVHN and 128 for CIFAR10 and CelebA-HQ.
We set du = 2048 throughout the experiments. The training of LAE is performed via the Langevin
sampling iterations as explained in Section 4. We use preconditioned stochastic gradient Langevin
dynamics (pSGLD) (Li et al., 2016) as the sampling algorithm. The implementation is available at
https://bit.ly/2Swow0F
18