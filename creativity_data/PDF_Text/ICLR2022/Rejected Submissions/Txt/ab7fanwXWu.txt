Under review as a conference paper at ICLR 2022
Accelerating	Optimization	using	Neural
Reparametrization
Anonymous authors
Paper under double-blind review
Ab stract
We tackle the problem of accelerating non-linear non-convex optimization prob-
lems. We discover that reparametrizing the optimization variables as the output
of a neural network can lead to significant speedup. We examine the dynamics of
gradient flow of such neural reparametrization. We find that to obtain the maxi-
mum speed up, the neural network architecture needs to be a specially designed
graph convolutional network (GCN). The aggregation function of the GCN is con-
structed from the gradients of the loss function and reduces to the Hessian in early
stages of the optimization. We show the utility of our method on two optimization
problems: network synchronization and persistent homology optimization, and
find an impressive speedup, with our method being 4 〜80 × faster.
1	Introduction
Gradient-based optimization lies at the heart of machine learning. For non-linear, non-convex large-
scale optimization problems, gradient-based optimization can suffer from slow convergence and
significant computational bottleneck. In linear systems, preconditioning Axelsson (1996); Saad &
Van Der Vorst (2000) can accelerating convergence by multiplying a symmetric positive-definite
preconditioner matrix to the original problem. In effect, preconditioning re-scales the loss landscape
to have more circle-like level sets, hence changing the condition number of the system. Adaptive
gradient optimizers such as AdaGrad Duchi et al. (2011) and Adam Kingma & Ba (2014) uses the
same idea but updates the preconditioner during iterative optimization with adaptive step size, which
leads to great improvement in convergence speed.
Our goal is to improve the convergence of non-convex optimization further. First, we examine the
dynamics of the gradient flow and identify the condition number that dictates the speed of conver-
gence. We discover that changing the condition number naturally requires the reparametrization
to be a type of graph neural networks (GNN). Based on the analysis, we introduce a novel neu-
ral reparametrization scheme, which generalizes preconditioning to non-linear systems. Note that
our method is complementary to existing gradient-based optimization algorithms and can be used
in conjunction with any adaptive gradient optimizer (e.g. Adam). We test our hypothesis on two
highly nonlinear optimization problems and obtain impressive speedups.
A few earlier work Sosnovik & Oseledets (2019); Hoyer et al. (2019) have demonstrated that convo-
lutional neural networks can serve as priors to improve the parameterization of a class of structural
optimization problems. But the theoretical foundation behind these techniques are not well under-
stood. Concurrently, the implicit acceleration of over-parametrization in linear neural networks have
been analyzed in Arora et al. (2018); Tarmoun et al. (2021). Specifically, Arora et al. (2018) shows
that reparametrizing the linear weights with deep linear networks impose a pre-conditioning scheme
on gradient descent. Tarmoun et al. (2021) further exploits the conservation laws in gradient flow
and derive convergence rate for two-layer linear networks.
In contrast, our method applies to highly nonlinear problems, guided by theoretical insights. Our
contributions can be summarized as follows
1.	We analyze the dynamics of the gradient flow and derive how a neural reparametrization
(NR) can lead to a speedup.
2.	We prove that maximum speedup is achieved if the NR is a special GCN which uses gradi-
ent of the loss in its aggregation function.
1
Under review as a conference paper at ICLR 2022
3.	For early steps, we show that the NR should use the Hessian of the loss, leading to an
efficient hybrid implementation.
4.	We showcase NP on two optimization tasks: network synchronization and persistent ho-
mology and observe consistent speedup.
Our method also demonstrates a novel usage and unique perspective for GNN, specifically GCN
(Kipf & Welling, 2016). The majority of literature treat GNNs as a representation learning tool for
graph-structure data (see surveys and the references in Bronstein et al. (2017); Zhang et al. (2018);
Wu et al. (2019); Goyal & Ferrara (2018)). However, our analysis shows that GNN can be used to
modify optimization dynamics in problems without explicit graph data.
2	Methodology
We consider the problem of minimizing a smooth, lower-bounded loss function L(w) ∈ R on
abounded variable w ∈ Rn :
argminwL(w)
When the optimization variables are matrices or tensors, w represents a flattened vector containing
all variables. w can also represent all trainable parameters in a deep neural network. For supervised
learning with a dataset Z = {(xi, yi)iN=1}, the loss function L(w) becomes L(w; Z). The dataset
and the model class (e.g. architecture of a neural network) together define a loss landscape as a
function of the optimization variables w.
Gradient-based optimization updates the variables w by taking repeated steps in the direction of the
steepest descent wt+1 = Wt 一 ε ∂WL for a learning rate ε. With very small time steps δt, we examine
the dynamics of w(t) ∈ Rn during the optimization process, treating t as a continuous time. Using
steepest descent for the optimization, dw/dt follows the gradient flow:
dwi	∂L
dt	εii ∂wi，
(1)
where denote the learning rate ε as a diagonal matrix of learning rates, because modern optimizers
such as RMSProp (Tieleman et al., 2012) or Adam (Kingma & Ba, 2014) choose ε adaptively for
each wi . Specifically, in Adam and RMSProp, which we use in experiments, we have
η
εij(VL,t)≈
√Et [(ViL)2]+ ξ
(2)
where δij is the Kronecker delta, and η andξ are small numbers. The expected value E is calculated
using stochastic samples for Vi L over multiple iteration steps, with more weights given to more
recent steps. Equation 1 is also an ordinary differential equation (ODE). The local minima of L(w)
correspond to the stationary solutions of the ODE, i.e. dw/dt = 0.
Convergence rate We are interested in the expected convergence rate E[dL/dt] of the loss func-
tion. This expectation can be either approximated in a fashion similar to adaptive gradient optimizers
(i.e. exponential time average) or be over random initializations under a distribution P (w(0)). We
will assume the expectation value for ε and E[dL/dt] are evaluated with the same method, meaning
we assume E[ε] = ε. The convergence rate is then given by
E
dL(w)
dt
X∂L dwi
∂ Wi dt
i
一 Tr [εM],
— JdL ∂L
Mij ≡ E [∂W- ∂Wj
(3)
E
where we used our assumption to get E[εM] = εE[M], with Mij = ViLVj L being the “square
gradients” matrix and M = E[M]. With this definition, equation 2 becomes ε
Note that M is time-dependent and can change during optimization.
=η/ Jdiag[M] + ξ.
Estimating M If We know the distribution P(w(t)) for some t, We can approximate M. One way
to do this is similar to Adam as a weighted average over the past few steps. Also, if we know the
distribution of the initialization P(w(0)), we can evaluate M for early stages, which is what we
2
Under review as a conference paper at ICLR 2022
b
a	Original
NNeWOk _L(W)
♦
— I data J
W
Neural Reparametrization
W
L(w())
Figure 1: Original (a) vs Neural Reparametrization (b) of the optimization problem.
will use in the current paper. When using Xavier initialization (Glorot & Bengio, 2010), we have
w(0)〜N(0, n-1/2). In this case We can show (See appendix A.1)
—	一	∂2L	∂2L	CI. Crl	C
Mij(t → O) = EEwkwι]τ^~~不—+—+— + O(n 2) = - [H2]iJ	+ O(n 2),	⑷
∂wk∂wi ∂wl∂wj	n ij w→0
k,l
where Hij (W) ≡ ∂2L(w)∕∂wi∂Wj is the Hessian of the loss at t = 0.
The eigenvectors of M with zero eigenvalue are modes that do not evolve during optimization. As
gradients are zero in these directions, GD can never find and explore those directions. Therefore, we
may exclude them from the beginning and claim that M is full rank.
The rate dWi ∕dt can be different along different directions of the parameter space, with fast and
slow modes. Next, we quantify this difference in rates and suggest ways for acceleration.
2.1 Fast and slow Dynamics of Gradient Flow
M is positive semi-definite because for any vector V
∈ Rn we have VTMV = E [(vτ▽£)[ ≥ 0.
Thus, M is Hermitian with a spectral expansion M = Pi miψiψT. Using equation 1, we can show
that the dynamics of W along different ψi are orthogonal to each other
ε2ψT Mψj = SiSj mi δij.
(5)
IfS is constant and not adaptive, W evolves faster along modes ψi with larger eigenvalues mi, since
if mi > mj we have
>E
S2 mj
(6)
This is precisely what optimizers such as Adam try to address. Ideally, having εi = η/√mi would
make up for the rate difference, resulting in a uniform GD where no mode ψi is evolving more slowly
than others. However, calculating mi is O(n3) and can be quite expensive during optimization,
hence the approximate version equation 2 is used. We show here that in many cases it is worthwhile
to do a full correction to GD using M, at least in early stages of the optimization.
The eigenvalues of M determine the rate of evolution of the overlaps along each of its eigenvectors.
In particular, if some eigenvalue mslow meani[mi] is much smaller than the mean, the evolution
of W along ψslow will be much slower than other directions. Therefore, we will refer to all ψslow
as the slow modes of the optimization problem. Conversely, the ψfast where mf ast meani [mi]
will be referred to as the fast modes.
When running GD, the maximum change in W is bounded to ensure numerical stability. Because of
the orthogonality in equation 5, we can enforce a numerical bound η via
T dW 2
max E	ψi 版
≤ η ⇒ SmaX ≤ -/	:
mmax
≤ η2
mi
mmax
(7)
where mmax = maxi mi is the largest eigenvalue of M . Therefore, the learning rate S is bounded
by the largest eigenvalues (fastest modes). Thus, in order to speed up convergence, we need to focus
on the slow modes, which can be achieve with a reparametrization of W .
3
Under review as a conference paper at ICLR 2022
2.2 Neural Parametrization of Optimization Problems
Using neural networks to reparametrize optimization problems have been suggested in (Hoyer et al.,
2019; Sosnovik & Oseledets, 2019). The idea is that the optimization variable w can be defined as
the output of a deep neural network w(θ) ≡ f(θ) with trainable parameters θ. Then, rather than
optimizing over the original variables w directly, we can instead optimize over the neural network
parameters θ. We will take this idea further and analyze the type of neural network architectures that
can accelerate the optimization.
Modified convergence rate The neural reparametrization of the variables w(θ) leads the follow-
ing updating rule for θ:
dθa	∂L	∂L ∂wi
-a = -εa 布=L 套	(8)
dt	∂θa	∂wi ∂θa
i
The convergence rate is then given by
-L ∂L ∂wi dθα	∂L ∂L	∂wi ∂wj
-t ∂w ∂wi ∂θa dt ∂w ∂wi ∂wj ɪ› εa ∂θα ∂θa
=-X Mij Kij = - Tr [MKT] ,	Kij ≡ X εa dwi 等.	⑼
∂θα ∂θα
i,j	a
Here K is the neural tangent kernel (NTK) (Jacot et al., 2018) of w(θ), and Mij is the squared
gradients. Note, We have absorbed ɛɑ into K for convenience of notation.
Expected convergence rates The expected convergence rate E[dL/dt] equation 3 depended on
M = E[M], the reparametrized one from equation 9 depends on E[MK]. In order to quantify any
improvements in E[dL/dt] from equation 9, We need to be able to express E[MK] in terms ofE[M],
Which is not possible for a general K.
If We assume M and K to be independent from each other, i.e. E[MK] becomes E[M]E[K]. Since
M is a function of w, K must not be a function of w. One Way is to have w = σ(Aθ + b) or any
other similar neural netWork. We Will discuss explicit choices for w beloW in sec. 2.3. In summary,
an important constraint that We Will put on w(θ) is that it yields a K independent of w
choose w(θ) s.t.:	E[MK] = E[M]E[K]	(10)
NoW We can compute the expected rate of change of w(θ) along the previous fast and sloW mode
eigenvectors ψi of M, similar to equation 6. Using equation 9 and equation 8 we have
dw	∂w dθa	∂w	∂wj ∂L	∂L
ψT蕨=ψTX 既aαma = -ψTXεa< X 就西=-ψTKdL (∈R)	(II)
a	aj
where ∂L∕∂w = Nw L is the gradient vector. Multiplying equation 11 by its transpose, denoting
K ≡ E[K], and using equation 10 the expected value becomes
E ]QT -w J = ΨT KMKT Ψi= X mj (ΨT KΨj )2 .	(12)
which shows how the neural reparametrization may speed up the convergence.
First, note that NR changes the rate of evolution of modes ψi in equation 7. We can quantify this
change when [K, M] = 0. In this case, K and M can be diagonalized simultaneously with shared
eigenvectors ψi. Plugging the spectral expansion K = Pi kiψiψiT into equation 12 and using
ε < η/√mmax (equation 7) we have
if: [K,M =0	⇒
E
(13)
Thus, one way to speed up the convergence, is to have [K, M] = 0 and have K increase the slowest
rates kψsiowdw/dt k, while keeping the fastest rate mostly unchanged. As a result, we have:
4
Under review as a conference paper at ICLR 2022
Theorem 1. With a neural reparametrization yielding [K, M] = 0, to make all modes ψi reach the
ideal maximum rate maxi ∣∣ψτdw/dtk < η, we need to have
K ≈
M
mmax
-1/2
(14)
Proof. This is because plugging ki2 ≈ mmax/(mi + ξ) (ξ	1) into equation 13 makes the r.h.s.
equal to η2.	□
Next, We discuss how We can achieve this in practice. Specifically, We derive What architecture for
the neural network w(θ) = f (θ) will result in a K satisfying the condition in equation 14.
2.3 Practical Implementation
To find a neural network model that satisfies equation 14, we need to consider the fact that M(w(t))
changes during optimization. To minimize the overhead from constructing w(θ) (and hence K) as
a function of M, the architecture has to be as simple as possible and involve steps which do not
increase the time complexity of each iteration significantly. We will now discuss when such neural
reparametrization is feasible and how to implement equation 14 in practice.
Hybrid optimization To use equation 14 at points other than the initial steps, we need to dynami-
cally estimate and update M. Though estimating M(t) efficiently with ideas from Adam is possible,
in the current work we focus on the speeding up at early stages using equation 4. Thus, our main
proposal is to use a two stage hybrid optimization: 1) use the Hessian to reparametrize w(θ) in early
stages for some number of iterations and then 2) switch to the original optimization over w .
Simple implementation Because M may depend on w(θ), as in equation 10, we need a K that
is independent of θ to ensure E[MK] = E[M]E[K]. The simplest architecture is a linear model
Wi = Pa κiaθa, which yields K = KKT ≈ (M∕mmaχ)-1/2. We need K to be a full rank matrix,
meaning that K needs to be at least n X n. Since we need the least computationally expensive K, we
choose θ ∈ Rn, and let K be a symmetric n X n matrix. This way, K = KK = (M∕mmaχ)-1/4.
For the initial stages, where M ≈ H2/n we have K = (H/hmax)-1/2, with H being the Hessian.
Per-step time complexity Since w ∈ Rn, the complexity of computing L(w) is O(np) where p
depends on the sparsity structure of L. M is an n X n matrix and computing M 1/2 is O(n3),
which is very expensive when n 1. The eigenvalue mmax can be estimated iteratively in O(qn2).
Hence, computing K by exactly following equation 14 is not feasible, unless M is fixed and can
be calculated offline. Even then, evaluating M 1/2 only makes sense if the expected GD steps
for convergence is more than O(n), otherwise we don’t gain any speedup. We may instead try to
approximate M -1/2 and mmax iteratively with complexity O(qn2) where q n is the number
of iterations. Hence we note that using equation 14 to speedup the optimization may generally be
useful if the the per step complexity of GD on L has a time complexity of least O(qn2).
Implementation constraints In summary, we have the following constraints on the structure of
the neural network w(θ)
1.	Computing w(θ) ∈ Rn must be at most O(qn2) with q	n
2.	To have K independent of M, ∂w∕∂θ must be independent of W and hence, only linear
maps w(θ) = Kθ are allowed, with K independent ofw.
3.	θ ∈ Rm with m ≥ n. We choose m = n, which yields K = √K = (M∕mmaχ)-1/4
4.	Instead of K = (M∕mmaχ)-1/4, which is O(n3), we should use O(qn2) approximations.
5
Under review as a conference paper at ICLR 2022
Relation to GCN Since both θ ∈ Rn and w ∈ Rn, our linear w = κθ architecture is essentially
a Graph Convolutional Network (GCN) (KiPf & Welling, 2016) with the aggregation rule K =
(M/mmax )-1/4, or a weighted adjacency matrix. In fact, all of our derivations remain unchanged
if w ∈ Rn×d and θ ∈ Rn×m (i.e. if there are d and m features per node in w and θ). Thus, our
claim is that we can speed up the optimization process by reparametrizing w using a GCN with
linear activation, as w = κθ, where θ are trainable and κ is the aggregation function derived from
the loss gradients. However, evaluating this κ during GD can be quite expensive (O(n3)) for large
n (GD steps 〜 O(n2)). Ideally We want an approximation for K which is O(qn2) with q 〜 O⑴.
Computationally efficient implementation We will focus on speeding up the early stages, in
which we use the Hessian H in M ≈ H2/n. Define H ≡ (1 一 ξ)H∕hmax, where hmax = √mmOX
and ξ《 1. To get an O(qn2) approximation for K = (M/mmax)-1/4 = H-1/2 we can take the
first q terms in the binomial expansion (see Appendix A.2). A rough estimate would be
13
H-1/2 ≈ I - 2(I - H)- 4(I - H)2.	(15)
Since H is positive semi-definite and its largest eigenvalue is 1 一 ξ < 1, the sum in equation 15
can be truncated after q 〜O⑴ terms. The first q terms of equation 25 can be implemented as a
q layer GCN with aggregation function f (M) = I 一 Mo and residual connections. We choose q
to be as small as 1 or 2, as larger q may actually slow down theoptimization. Note that computing
f (Mo)qθ is O(qn2) because we do not need to first compute f (M)q (which is O(qn3)) and instead
use Vi+1f (M)vi (O(n2)) q times with v1 = θ.
Lastly, to evaluate M0 we need to estimate the leading eigenvalue mmax. Given any vector v ∈ Rn
and using spectral expansion, we have
HqV = X mq∕2(ΨTv)ψi	⇒	hmax ≈ 1 TH 1 = Pj 2	(16)
i	1TH1	ij Hij
where, since H is positive semi-definite, for q > 1 the leading eigenvector ψmax quickly dominates
the sum and we have Hqv ≈ hqmax(ψmaxv)ψmax. Here we chose q = 2 to get a crude approx-
imation for hmax. The generalized Perron-Frobenius theorem (Berman & Plemmons, 1994), state
the components of the leading eigenvector ψmax should be mostly positive. Therefore, we chose
v = 1∕√n, where 1 = (1,..., 1), which should be close to the actual ψmax.
If we think of M as the weighted adjacency matrix of a graph with n vertices, the vector component
Di = [H1]i = Pj Hij is the weighted degree of node i. Hence equation 16 states H/mmax ≈
(Pi Di∕kDk2) H. This is similar to the graph diffusion operator D-1H and D-1/2HD-1/2 which
are used as the aggregation functions in GCN (here Dij = Di δij is the degree matrix).
When using modern optimizers, the adaptove learning rates is a term within K. This will interfere
with the optimal choice for K and hence our GCN aggregation function. Therefore, in practice, we
do not fix the coefficients for H and H2 in our GCN. Instead, we use single or two layer GCN with
aggregation function f(H) = D-1/2HD-1/2 .
3 Experiments
We showcase the acceleration effect of neural reparametrization on two non-convex optimization
problems: network of Kuramoto oscillators which are widely-used for modeling synchronization
phenomena; and persistent homology, a mathematical tool that computes the topology features of
data that persist across scales . We use Adam Kingma & Ba (2014) optimizer and compare different
reparameterization models. We implemented all models with Pytorch.
3.1	Network of Kuramoto oscillators
Kuramoto model Kuramoto (1975; 1984) are widely used for synchronization problems, which have
profound impact on engineering, physics and social systems (Pikovsky et al., 2003). As shown in
Fig. 3(right), Kuramoto model describes the behavior of a large set of coupled oscillators. Each
6
Under review as a conference paper at ICLR 2022
Figure 2: Optimizing Kuramoto model on a 25 × 25 square lattice (a) Loss over run time in seconds
for different methods. (b) Evolution of the phase variables over iterations. (c) Level of synchroniza-
tion, measured by global order parameter over time. Neural reparameterization with GCN achieves
the highest speedup.
oscillator is defined by an angle θi = ωit + φi , where ωi is the frequency and φi is the phase. We
consider the case where ωi = 0. The coupling strength is represented by a graph adjacency matrix
with elements Aij ∈ R. Defining ∆ij ≡ φi - φj , the dynamics of the phases φi (t) in the Kuramoto
model follows
^dti = -ε X Aji sin Zj,
j=1
n
L(φ) =	Ajicos∆ij.
i,j=1
(17)
Hopf-Kuramoto (HK) model Lauter et al. (2015) is a more general version of the Kuramoto model.
It includes second neighbor interactions. HK has a rich phase space with qualitatively different
solutions. The phase space includes regions where simulations becomes slow and difficult. We use
the HK model to showcase our method in more complex and nonlinear scenarios. The HK model’s
dynamics follows
dφti = C X Aji [cos ∆ij - Si sin ∆ij]
j
+ s2	(Aij Aj k [sin (∆ji + ∆jk) - sin (∆ji - ∆jk)] + Aij Aik sin (∆ji + ∆ki)) . (18)
k,j
with the following loss function
L = ε ^X Aji [sin ∆ij + s1 cos ∆ij] + 2ε ^X AijAjk [cos (∆ji + ∆jk) + cos (∆ji - ∆jk)]
i,j
i,k,j
(19)
Our goal is to minimize the phase drift dφi /dt to synchronize the oscillators. Existing numerical
methods directly optimize the loss L(φ) with gradient-based algorithms, which we refer to as linear.
We apply our method to reparametrize the phase variables φ and speed up convergence towards
synchronization.
Implementation. For early stages, we use a GCN with the aggregation function derived from the
Hessian which for the Kuramoto model simply becomes Hij (0) = ∂2L∕∂φi∂φj | φ→0 = Aij —
k Aik δij = -Lij, where L = D - A is the graph Laplacian ofA. We found that NR in the early
stages of the optimization gives more speed up. We implemented the hybrid optimization described
earlier, where we reparemtrize the problem in the first 100 iterations and then switch to the original
linear optimization for the rest of the optimization.
We experimented with three Kuramoto oscillator systems with different coupling structures: square
lattice, circle graph, and tree graph. For each system, the phases are randomly initialized between
0 and 2π from uniform distribution. We let the different models run until the loss converges (10
patience steps for early stopping, 10-15 loss fluctuation limit).
Results of Kuramoto Model. Figure 2 shows the results of Kuramoto model on a square lattice.
Additional results on circle graph, and tree graph can be found in Appendix B. Figure 2 (a) shows
that NR with one-layer GCN (GCN-1) and GCN with residual connection (RGCN-1) achieves sig-
nificant speedup. In particular, we found 3.6 ± .5 speed improvement for the lattice, 6.1 ± .1 for the
7
Under review as a conference paper at ICLR 2022
Figure 3: Hopf-Kuramoto model on a square lattice (50 × 50). a) Speedup in the final loss value
difference function. Points color correspond to the regions of the phase diagram (b), also, the number
above each phase pattern are the global order parameters. We fixed c = 1 for the simplicity. c)
Coupled oscillator system.
ψ
circle graph and 2.7 ± .3 for tree graphs. We also experimented with two layer (GCN/RGCN-2) and
three layer (GCN/RGCN-3) GCNs. As expected, the overhead of deeper GCN models slows down
optimization and offsets the NR speedup gains. Figure 2 (b) visualizes the evolution of the oscillator
phase φi on a square lattice over iterations. We can see that even though different reparametrization
models reach the same loss value, the final solutions are quite different. The linear version (without
NR) arrives at the final solution smoothly, while GCN models form dense clusters at the initial steps
and reach an organized state before 100 steps. To quantify the level of synchronization, we measure
a quantity P known as the “global order parameter” (Sarkar & GUPte (2021)): P = N^ ∣ Pj eiφj ∣. Fig-
ure 2 (c) shows the convergence of the global order parameter over time. We can see that one-layer
GCN and RGCN gives the highest amount of acceleration, driving the system to synchronization.
Results of Hopf-Kuramoto Model. We report the comparison of NR models on synchronizing
Hopf-Kuramoto dynamics. According to the Lauter et al. (2015) paper, we identify two different
main patterns on the phase diagram Fig. 9(b): ordered (small s2/c, smooth patterns) and disordered
(large s2/c, noisy) phases. In all experiments we use the same lattice size 50 × 50, with the the same
stopping criteria (10 patience steps and 10-10 loss error limit) and switch between the Linear and
GCN model after 100 iteration steps. Fig. 3 (a) shows the loss at convergence versus the speedup.
We compare different GCN models and observe that GCN with A2 as the propagation rule achieves
the highest speedup. Also, we can see that we have different speedups in each region, especially in
the disordered phases. Furthermore, we observed that the Linear and GCN models converge into a
different minimum state in a few cases. However, the patterns remain the same. (b) shows how the
level of ordering changes region by region. If the global order parameter is closer to 0, we have more
of a disordered phase while the parameter is closer to 1, meaning it is a more organized pattern.
We know that running dynamics in a large lattice are computationally expensive, especially close to
the phase transition points where difficult to separate the slow and fast modes.
3.2	Persistent homology
Persistent homology Edelsbrunner et al. (2008) is an algebraic tool for measuring topological fea-
tures of shapes and functions. It provides precise definition of qualitative features, is computable
with linear algebra and robust to perturbation of input data (Otter et al., 2017), see more details in
Appendix B.3. An example of persistent homology is point cloud optimization Gabrielsson et al.
(2020); Carriere et al. (2021). Given a random point cloud w that lacks any observable characteris-
tics, we aim to produce persistent homological features by optimizing the position of data points.
n
L(w) = - X kp-π∆(p)k2∞ + X kwi - rk2	(20)
where p ∈ D = {(bi , di)}i∈Ik denotes the homological features in the the persistence diagram
D, consisting of all the pairs of birth bi and death di filtration values of the set of k-dimensional
homological features Ik . π∆ is the projection onto the diagonal ∆ and Pi d(φi , S) constrains the
points within the a square centered at the origin with length 2r (denote r as range of the point cloud).
Carriere et al. (2021) optimizes the point cloud positions directly with gradient-based optimization.
8
Under review as a conference paper at ICLR 2022
Figure 4: Speedup: a,b) Training and total time speedup; c) GCN speeds up convergence.
GCN Model
Speed Up
r=4.0
Figure 5: a) Loss vs speed-up of GCN model. The converged loss value is determined by point cloud
range only, while the speed-up is affected by both point cloud range and size. b) Initial random point
cloud and GCN-optimized point cloud.
100 pts
We will refer theirs as the baseline linear model. In contrast, we integrate GCN and reparameterize
the point cloud positions, and will refer ours as the GCN model.
Implementation. To measure the speed-up of GCN model, we used the same Gudhi library for
computing persistence diagram as Gabrielsson et al. (2020); Carriere et al. (2021). The runtime
of learning persistent homology is dominated by computing persistence diagram in every iteration,
which has the time complexity of O(n3). Thus, the runtime per iteration for GCN model and
linear model are very similar, and we demonstrate that the GCN model can reduce convergence time
by a factor of 〜4. We ran the experiments for point cloud of 100,200,300 points, with ranges
of 0.5,1.0,2.0,4.0. The hyperparameters of the GCN model are kept constant, including network
dimensions. The result for each setting is averaged from 5 consecutive runs.
Results. Fig. 4 and 5(a) show that the speedup of the GCN model is related to point cloud density.
Training converges faster as the point cloud becomes more sparse, but the speedup gain saturates
as point cloud density decreases. On the other hand, time required for initial point cloud fitting
increases significantly with the range of point cloud. Consequently, the overall speedup peaks when
the range of point cloud is around 4 times larger than what is used be in Gabrielsson et al. (2020);
Carriere et al. (2021), which spans over an area 16 times larger. Further increase in point cloud
range causes the speedup to drop as the extra time of initial point cloud fitting outweighs the reduced
training time. The loss curve plot in Fig. 4 shows the convergence of training loss of the GCN model
and the baseline model in one of the settings when GCN is performing well. Fig. 5 shows the initial
random point cloud and the output from the GCN model. In the Appendix B.3, we included the
results of GCN model hyperparameter search and a runtime comparison of the GCN model under
all experiment settings.
Conclusion We propose a neural reparametrization scheme to accelerate a large class of optimiza-
tion problems. Our method is grounded on analysis that the dynamics of gradient flow are related
to the condition number of the system. By reparametrizing the optimization problem with a graph
convolutional network, we can modify the condition number and obtain the maximum speed up.
The aggregation function of the GCN is constructed from the gradients of the loss function and re-
duces to the Hessian in early stages of the optimization. We demonstrate our method on optimizing
synchronization problems and persistent homology of pointclouds. Depending on the experiment,
we obtain a best case speedup that ranges from 4 to 80.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning, pp.
244-253. PMLR, 2018.
Owe Axelsson. Iterative solution methods. Cambridge university press, 1996.
Abraham Berman and Robert J Plemmons. Nonnegative matrices in the mathematical sciences.
SIAM, 1994.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
MathieU Carriere, Frederic ChazaL Marc Glisse, Yuichi Ike, Hariprasad Kannan, and Yuhei Umeda.
Optimizing persistent homology based functions. In International Conference on Machine Learn-
ing, pp. 1294-1303. PMLR, 2021.
Nima Dehmamy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the representation power of
graph neural networks in learning graph topology. Advances in Neural Information Processing
Systems, 2019.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Herbert Edelsbrunner, John Harer, et al. Persistent homology-a survey. Contemporary mathematics,
453:257-282, 2008.
Rickard BrUel Gabrielsson, Bradley J Nelson, Anjan Dwaraknath, and Primoz Skraba. A topology
layer for machine learning. In International Conference on Artificial Intelligence and Statistics,
pp. 1553-1563. PMLR, 2020.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A
survey. Knowledge-Based Systems, 151:78-94, 2018.
Stephan Hoyer, Jascha Sohl-Dickstein, and Sam Greydanus. Neural reparameterization improves
structural optimization. arXiv preprint arXiv:1909.04240, 2019.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
John Michael Kosterlitz and David James Thouless. Ordering, metastability and phase transitions
in two-dimensional systems. Journal of Physics C: Solid State Physics, 6(7):1181, 1973.
Yoshiki Kuramoto. Self-entrainment of a population of coupled non-linear oscillators. In Interna-
tional symposium on mathematical problems in theoretical physics, pp. 420-422. Springer, 1975.
Yoshiki Kuramoto. Chemical turbulence. In Chemical Oscillations, Waves, and Turbulence, pp.
111-140. Springer, 1984.
Roland Lauter, Christian Brendel, Steven JM Habraken, and Florian Marquardt. Pattern phase di-
agram for two-dimensional arrays of coupled limit-cycle oscillators. Physical Review E, 92(1):
012902, 2015.
10
Under review as a conference paper at ICLR 2022
Nina Otter, Mason A Porter, Ulrike Tillmann, Peter Grindod, and Heather A Harrington. A roadmap
for the computation of persistent homology. EPJ Data Science, 6(17), 2017.
Arkady Pikovsky, Jurgen Kurths, Michael Rosenblum, and Jurgen Kurths. Synchronization: a Uni-
versal concept in nonlinear sciences. Number 12. Cambridge university press, 2003.
Yousef Saad and Henk A Van Der Vorst. Iterative solution of linear systems in the 20th century.
Journal of Computational and Applied Mathematics,123(1-2):1-33, 2000.
Mrinal Sarkar and Neelima Gupte. Phase synchronization in the two-dimensional kuramoto model:
Vortices and duality. Physical Review E, 103(3):032204, 2021.
Ivan Sosnovik and Ivan Oseledets. Neural networks for topology optimization. Russian Journal of
Numerical Analysis and Mathematical Modelling, 34(4):215-223, 2019.
Salma Tarmoun, Guilherme Franca, Benjamin D Haeffele, and Rene Vidal. Understanding the
dynamics of gradient flow in overparameterized linear models. In International Conference on
Machine Learning, pp. 10153-10161. PMLR, 2021.
Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. arXiv preprint
arXiv:1812.04202, 2018.
11